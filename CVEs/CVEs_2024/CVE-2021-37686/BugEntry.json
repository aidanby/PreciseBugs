{"buggy_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <algorithm>\n#include <cmath>\n#include <cstdlib>\n#include <functional>\n#include <iterator>\n#include <limits>\n#include <random>\n#include <string>\n#include <vector>\n\n#include <gtest/gtest.h>\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/test_util.h\"\n\nnamespace tflite {\nnamespace {\n\n// Runs the reference and optimized AveragePool functions and asserts the values\n// are the same.\nvoid RunOneAveragePoolTest(const PoolParams& params,\n                           const RuntimeShape& input_shape,\n                           const int8* input_data,\n                           const RuntimeShape& output_shape) {\n  const int buffer_size = output_shape.FlatSize();\n  std::vector<int8> optimized_averagePool_output(buffer_size);\n  std::vector<int8> reference_averagePool_output(buffer_size);\n\n  reference_integer_ops::AveragePool(params, input_shape, input_data,\n                                     output_shape,\n                                     reference_averagePool_output.data());\n  optimized_integer_ops::AveragePool(params, input_shape, input_data,\n                                     output_shape,\n                                     optimized_averagePool_output.data());\n\n  for (int i = 0; i < buffer_size; i++) {\n    EXPECT_TRUE(reference_averagePool_output[i] ==\n                optimized_averagePool_output[i]);\n  }\n}\n\n// Creates random input shape (batch, height, width, depth), then computes\n// output shape based on value of `padding_same`:\n// `padding_same` == true, calculate output with padding == \"SAME\"\n// `padding_same` == false, calculate output with padding == \"VALID\"\n// With input/output shapes computed, fills the input data and calls the\n// test function.\nvoid CreateDataAndRunAveragePool(bool padding_same) {\n  const int batch = UniformRandomInt(1, 2);\n  const int input_depth = UniformRandomInt(1, 700);\n  const int output_depth = input_depth;\n  const int input_width_offset = UniformRandomInt(1, 30);\n  const int input_height_offset = UniformRandomInt(1, 30);\n  const int stride_width = UniformRandomInt(1, 10);\n  const int stride_height = UniformRandomInt(1, 10);\n  const int filter_width = UniformRandomInt(1, 10);\n  const int filter_height = UniformRandomInt(1, 10);\n  const int input_width = input_width_offset + filter_width;\n  const int input_height = input_height_offset + filter_height;\n  const int output_width =\n      padding_same ? (input_width + stride_width - 1) / stride_width\n                   : (input_width - filter_width + stride_width) / stride_width;\n  const int output_height =\n      padding_same\n          ? (input_height + stride_height - 1) / stride_height\n          : (input_height - filter_height + stride_height) / stride_height;\n\n  auto input_shape =\n      RuntimeShape({batch, input_height, input_width, input_depth});\n  auto output_shape =\n      RuntimeShape({batch, output_height, output_width, output_depth});\n  const int buffer_size = input_shape.FlatSize();\n  std::vector<int8> input_data(buffer_size);\n  FillRandom(&input_data);\n\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.quantized_activation_min =\n      static_cast<int8_t>(std::numeric_limits<int8_t>::lowest());\n  params.quantized_activation_max =\n      static_cast<int8_t>(std::numeric_limits<int8_t>::max());\n  auto compute_padding = [](int stride, int in_size, int filter_size,\n                            int out_size) {\n    int padding = ((out_size - 1) * stride + filter_size - in_size) / 2;\n    return padding > 0 ? padding : 0;\n  };\n  params.padding_values.width =\n      compute_padding(stride_width, input_width, filter_width, output_width);\n  params.padding_values.height = compute_padding(stride_height, input_height,\n                                                 filter_height, output_height);\n  RunOneAveragePoolTest(params, input_shape, input_data.data(), output_shape);\n}\n\nTEST(TestAveragePool, SymmetricQuantAveragePool) {\n  const int kTestsToRun = 10;\n  for (int i = 0; i < kTestsToRun; i++) {\n    CreateDataAndRunAveragePool(/*padding_same=*/true);\n    CreateDataAndRunAveragePool(/*padding_same=*/false);\n  }\n}\n\n// Creates random input shape (batch, height, width, depth), then computes\n// output shape based on value of `padding_same`:\n// `padding_same` == true, calculate output with padding == \"SAME\"\n// `padding_same` == false, calculate output with padding == \"VALID\"\n// With input/output shapes computed, fills the input data and calls the\n// test function.\nvoid CreateExtremalDataAndRunAveragePool(bool padding_same) {\n  const int batch = UniformRandomInt(1, 2);\n  const int input_depth = UniformRandomInt(1, 700);\n  const int output_depth = input_depth;\n  const int input_width_offset = UniformRandomInt(1, 30);\n  const int input_height_offset = UniformRandomInt(1, 30);\n  const int stride_width = UniformRandomInt(1, 128);\n  const int stride_height = UniformRandomInt(1, 128);\n  const int filter_width = UniformRandomInt(1, 28);\n  const int filter_height = UniformRandomInt(1, 28);\n  if (filter_width * filter_height > 64) {\n    std::cout << \"should test 32 version\" << std::endl;\n  }\n  const int input_width = input_width_offset + filter_width;\n  const int input_height = input_height_offset + filter_height;\n  const int output_width =\n      padding_same ? (input_width + stride_width - 1) / stride_width\n                   : (input_width - filter_width + stride_width) / stride_width;\n  const int output_height =\n      padding_same\n          ? (input_height + stride_height - 1) / stride_height\n          : (input_height - filter_height + stride_height) / stride_height;\n\n  auto input_shape =\n      RuntimeShape({batch, input_height, input_width, input_depth});\n  auto output_shape =\n      RuntimeShape({batch, output_height, output_width, output_depth});\n\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.quantized_activation_min =\n      static_cast<int8_t>(std::numeric_limits<int8_t>::lowest());\n  params.quantized_activation_max =\n      static_cast<int8_t>(std::numeric_limits<int8_t>::max());\n  auto compute_padding = [](int stride, int in_size, int filter_size,\n                            int out_size) {\n    int padding = ((out_size - 1) * stride + filter_size - in_size) / 2;\n    return padding > 0 ? padding : 0;\n  };\n  params.padding_values.width =\n      compute_padding(stride_width, input_width, filter_width, output_width);\n  params.padding_values.height = compute_padding(stride_height, input_height,\n                                                 filter_height, output_height);\n\n  const int buffer_size = input_shape.FlatSize();\n  std::vector<int8> input_data(buffer_size);\n\n  // Test small values\n  int8 min = std::numeric_limits<int8>::min();\n  int8 max = std::numeric_limits<int8>::min() + 10;\n  FillRandom(&input_data, min, max);\n  RunOneAveragePoolTest(params, input_shape, input_data.data(), output_shape);\n\n  // Test large values\n  min = std::numeric_limits<int8>::max() - 10;\n  max = std::numeric_limits<int8>::max();\n  FillRandom(&input_data, min, max);\n  RunOneAveragePoolTest(params, input_shape, input_data.data(), output_shape);\n}\n\nTEST(TestAveragePool, SymmetricQuantExtremalAveragePool) {\n  CreateExtremalDataAndRunAveragePool(/*padding_same=*/true);\n  CreateExtremalDataAndRunAveragePool(/*padding_same=*/false);\n}\n\n}  // namespace\n}  // namespace tflite\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_INTEGER_OPS_POOLING_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_INTEGER_OPS_POOLING_H_\n\n#include <string.h>\n\n#include <algorithm>\n\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/cppmath.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/im2col_utils.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/neon_check.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/strided_slice_logic.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\nnamespace optimized_integer_ops {\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const int8* input_data, const RuntimeShape& output_shape,\n                    int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaxPool/8bit\");\n\n  // Here, and in other pooling ops, in order to maintain locality of reference,\n  // to minimize some recalculations, and to load into NEON vector registers, we\n  // use an inner loop down the depth. Since depths can be large and hence we\n  // would need arbitrarily large temporary storage, we divide the work up into\n  // depth tranches just within the batch loop.\n  static constexpr int kPoolingAccTrancheSize = 256;\n\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  int8 acc[kPoolingAccTrancheSize];\n  for (int batch = 0; batch < batches; ++batch) {\n    // We proceed through the depth in tranches (see comment above). The\n    // depth_base is the depth at the beginning of the tranche. The\n    // tranche_depth is the depth dimension of the tranche.\n    for (int depth_base = 0; depth_base < depth;\n         depth_base += kPoolingAccTrancheSize) {\n      const int tranche_depth =\n          std::min(depth - depth_base, kPoolingAccTrancheSize);\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          memset(acc, params.quantized_activation_min,\n                 tranche_depth * sizeof(acc[0]));\n          const int8* input_ptr =\n              input_data + depth_base +\n              depth * (in_x_origin +\n                       input_width * (in_y_origin + input_height * batch));\n          for (int fy = filter_y_start; fy < filter_y_end; fy++) {\n            const int8* input_row_ptr =\n                input_ptr + depth * (fy * input_width + filter_x_start);\n            for (int fx = filter_x_start; fx < filter_x_end; fx++) {\n              const int8* input_channel_ptr = input_row_ptr;\n              int channel = 0;\n#ifdef USE_NEON\n              for (; channel <= tranche_depth - 16; channel += 16) {\n                int8x16_t acc_reg = vld1q_s8(acc + channel);\n                int8x16_t input_reg = vld1q_s8(input_channel_ptr);\n                input_channel_ptr += 16;\n                acc_reg = vmaxq_s8(acc_reg, input_reg);\n                vst1q_s8(acc + channel, acc_reg);\n              }\n\n              for (; channel <= tranche_depth - 8; channel += 8) {\n                int8x8_t acc_reg = vld1_s8(acc + channel);\n                int8x8_t input_reg = vld1_s8(input_channel_ptr);\n                input_channel_ptr += 8;\n                acc_reg = vmax_s8(acc_reg, input_reg);\n                vst1_s8(acc + channel, acc_reg);\n              }\n#endif\n              for (; channel < tranche_depth; ++channel) {\n                acc[channel] = std::max(acc[channel], *input_channel_ptr++);\n              }\n              input_row_ptr += depth;\n            }\n          }\n          int8* output_ptr = output_data + Offset(output_shape, batch, out_y,\n                                                  out_x, depth_base);\n          int channel = 0;\n#ifdef USE_NEON\n          for (; channel <= tranche_depth - 16; channel += 16) {\n            int8x16_t a = vld1q_s8(acc + channel);\n            a = vminq_s8(a, vdupq_n_s8(params.quantized_activation_max));\n            a = vmaxq_s8(a, vdupq_n_s8(params.quantized_activation_min));\n            vst1q_s8(output_ptr + channel, a);\n          }\n          for (; channel <= tranche_depth - 8; channel += 8) {\n            int8x8_t a = vld1_s8(acc + channel);\n            a = vmin_s8(a, vdup_n_s8(params.quantized_activation_max));\n            a = vmax_s8(a, vdup_n_s8(params.quantized_activation_min));\n            vst1_s8(output_ptr + channel, a);\n          }\n#endif\n          for (; channel < tranche_depth; ++channel) {\n            int8 a = acc[channel];\n            a = std::max<int8>(a, params.quantized_activation_min);\n            a = std::min<int8>(a, params.quantized_activation_max);\n            output_ptr[channel] = static_cast<int8>(a);\n          }\n        }\n      }\n    }\n  }\n}\n\ninline void AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape, const int8* input_data,\n                        const RuntimeShape& output_shape, int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"AveragePool/8bitWith32bitAccumulator\");\n\n  // Here, and in other pooling ops, in order to maintain locality of reference,\n  // to minimize some recalculations, and to load into NEON vector registers, we\n  // use an inner loop down the depth. Since depths can be large and hence we\n  // would need arbitrarily large temporary storage, we divide the work up into\n  // depth tranches just within the batch loop.\n  static constexpr int kPoolingAccTrancheSize = 256;\n\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  int32 acc[kPoolingAccTrancheSize];\n  for (int batch = 0; batch < batches; ++batch) {\n    // We proceed through the depth in tranches (see comment above). The\n    // depth_base is the depth at the beginning of the tranche. The\n    // tranche_depth is the depth dimension of the tranche.\n    for (int depth_base = 0; depth_base < depth;\n         depth_base += kPoolingAccTrancheSize) {\n      const int tranche_depth =\n          std::min(depth - depth_base, kPoolingAccTrancheSize);\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          const int filter_count =\n              (filter_x_end - filter_x_start) * (filter_y_end - filter_y_start);\n          memset(acc, 0, tranche_depth * sizeof(acc[0]));\n          const int8* input_ptr =\n              input_data + depth_base +\n              depth * (in_x_origin +\n                       input_width * (in_y_origin + input_height * batch));\n          for (int fy = filter_y_start; fy < filter_y_end; fy++) {\n            const int8* input_row_ptr =\n                input_ptr + depth * (fy * input_width + filter_x_start);\n            for (int fx = filter_x_start; fx < filter_x_end; fx++) {\n              const int8* input_channel_ptr = input_row_ptr;\n              int channel = 0;\n#ifdef USE_NEON\n              for (; channel <= tranche_depth - 16; channel += 16) {\n                int16x4_t acc_reg[4];\n                int8x16_t input_reg = vld1q_s8(input_channel_ptr);\n                input_channel_ptr += 16;\n                acc_reg[0] = vget_low_s16(vmovl_s8(vget_low_s8(input_reg)));\n                acc_reg[1] = vget_high_s16(vmovl_s8(vget_low_s8(input_reg)));\n                acc_reg[2] = vget_low_s16(vmovl_s8(vget_high_s8(input_reg)));\n                acc_reg[3] = vget_high_s16(vmovl_s8(vget_high_s8(input_reg)));\n                for (int i = 0; i < 4; i++) {\n                  vst1q_s32(\n                      acc + channel + 4 * i,\n                      vaddw_s16(vld1q_s32(acc + channel + 4 * i), acc_reg[i]));\n                }\n              }\n              for (; channel <= tranche_depth - 8; channel += 8) {\n                int16x4_t acc_reg[2];\n                int16x8_t input_reg = vmovl_s8(vld1_s8(input_channel_ptr));\n                input_channel_ptr += 8;\n                acc_reg[0] = vget_low_s16(input_reg);\n                acc_reg[1] = vget_high_s16(input_reg);\n                for (int i = 0; i < 2; i++) {\n                  vst1q_s32(\n                      acc + channel + 4 * i,\n                      vaddw_s16(vld1q_s32(acc + channel + 4 * i), acc_reg[i]));\n                }\n              }\n#endif\n              for (; channel < tranche_depth; ++channel) {\n                acc[channel] += *input_channel_ptr++;\n              }\n              input_row_ptr += depth;\n            }\n          }\n          int8* output_ptr = output_data + Offset(output_shape, batch, out_y,\n                                                  out_x, depth_base);\n          int channel = 0;\n#ifdef USE_NEON\n          for (; channel <= tranche_depth - 8; channel += 8) {\n            int16 buf[8];\n            for (int i = 0; i < 8; i++) {\n              buf[i] =\n                  acc[channel + i] > 0\n                      ? (acc[channel + i] + filter_count / 2) / filter_count\n                      : (acc[channel + i] - filter_count / 2) / filter_count;\n            }\n            int8x8_t buf8 = vqmovn_s16(vld1q_s16(buf));\n            buf8 = vmin_s8(buf8, vdup_n_s8(params.quantized_activation_max));\n            buf8 = vmax_s8(buf8, vdup_n_s8(params.quantized_activation_min));\n            vst1_s8(output_ptr + channel, buf8);\n          }\n#endif\n          for (; channel < tranche_depth; ++channel) {\n            int16 a = acc[channel] > 0\n                          ? (acc[channel] + filter_count / 2) / filter_count\n                          : (acc[channel] - filter_count / 2) / filter_count;\n            a = std::max<int16>(a, params.quantized_activation_min);\n            a = std::min<int16>(a, params.quantized_activation_max);\n            output_ptr[channel] = static_cast<int8>(a);\n          }\n        }\n      }\n    }\n  }\n}\n\n}  // namespace optimized_integer_ops\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_INTEGER_OPS_POOLING_H_\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_LEGACY_OPTIMIZED_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_LEGACY_OPTIMIZED_OPS_H_\n\n#include <stdint.h>\n#include <sys/types.h>\n\n#include \"public/gemmlowp.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/fully_connected.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/resize_bilinear.h\"\n#include \"tensorflow/lite/kernels/internal/reference/legacy_reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\nnamespace optimized_ops {\n\n// Unoptimized reference ops:\nusing reference_ops::Broadcast4DSlowGreater;\nusing reference_ops::Broadcast4DSlowGreaterEqual;\nusing reference_ops::Broadcast4DSlowGreaterEqualWithScaling;\nusing reference_ops::Broadcast4DSlowGreaterWithScaling;\nusing reference_ops::Broadcast4DSlowLess;\nusing reference_ops::Broadcast4DSlowLessEqual;\nusing reference_ops::Broadcast4DSlowLessEqualWithScaling;\nusing reference_ops::Broadcast4DSlowLessWithScaling;\nusing reference_ops::BroadcastAdd4DSlow;\nusing reference_ops::BroadcastGreater;\nusing reference_ops::BroadcastGreaterEqual;\nusing reference_ops::BroadcastLess;\nusing reference_ops::BroadcastLessEqual;\nusing reference_ops::BroadcastMul4DSlow;\nusing reference_ops::BroadcastSubSlow;\nusing reference_ops::Concatenation;\nusing reference_ops::ConcatenationWithScaling;\nusing reference_ops::DepthConcatenation;\nusing reference_ops::Div;\nusing reference_ops::FakeQuant;\nusing reference_ops::Gather;\nusing reference_ops::Greater;\nusing reference_ops::GreaterEqual;\nusing reference_ops::GreaterEqualWithScaling;\nusing reference_ops::GreaterWithScaling;\nusing reference_ops::Less;\nusing reference_ops::LessEqual;\nusing reference_ops::LessEqualWithScaling;\nusing reference_ops::LessWithScaling;\nusing reference_ops::Mean;\nusing reference_ops::RankOneSelect;\nusing reference_ops::Relu1;\nusing reference_ops::Relu6;\nusing reference_ops::ReluX;\nusing reference_ops::Select;\nusing reference_ops::SpaceToBatchND;\nusing reference_ops::Split;\nusing reference_ops::TensorFlowSplit;\n\nstatic constexpr int kDepthwiseReverseShift = -1;\n\ntemplate <typename Scalar, int N>\nVectorMap<Scalar> MapAsVector(Scalar* data, const Dims<N>& dims) {\n  const int size = FlatSize(dims);\n  return VectorMap<Scalar>(data, size, 1);\n}\n\ntemplate <typename Scalar, int N>\nMatrixMap<Scalar> MapAsMatrixWithFirstDimAsRows(Scalar* data,\n                                                const Dims<N>& dims) {\n  const int rows = dims.sizes[0];\n  int cols = 1;\n  for (int d = 1; d < N; d++) {\n    cols *= dims.sizes[d];\n  }\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar, int N>\nMatrixMap<Scalar> MapAsMatrixWithLastDimAsCols(Scalar* data,\n                                               const Dims<N>& dims) {\n  const int cols = dims.sizes[N - 1];\n  int rows = 1;\n  for (int d = 0; d < N - 1; d++) {\n    rows *= dims.sizes[d];\n  }\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar, int N>\nArrayMap<Scalar> MapAsArrayWithFirstDimAsRows(Scalar* data,\n                                              const Dims<N>& dims) {\n  const int rows = dims.sizes[0];\n  int cols = 1;\n  for (int d = 1; d < N; d++) {\n    cols *= dims.sizes[d];\n  }\n  return ArrayMap<Scalar>(data, rows, cols);\n}\n\n// TODO(b/62193649): this function is only needed as long\n// as we have the --variable_batch hack.\ntemplate <typename Scalar, int N>\nMatrixMap<Scalar> MapAsMatrixWithGivenNumberOfRows(Scalar* data,\n                                                   const Dims<N>& dims,\n                                                   int rows) {\n  const int flatsize = FlatSize(dims);\n  TFLITE_DCHECK((flatsize % rows) == 0);\n  const int cols = flatsize / rows;\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ninline bool AreSameDims(const Dims<4>& dims1, const Dims<4>& dims2) {\n  for (int i = 0; i < 4; i++) {\n    if (dims1.sizes[i] != dims2.sizes[i]) {\n      return false;\n    }\n  }\n  return true;\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  const RuntimeShape output_shape = DimsToShape(output_dims);\n  const int output_height = output_shape.Dims(1);\n\n  DepthwiseConvImpl(op_params, DimsToShape(input_dims), input_data,\n                    DimsToShape(filter_dims), filter_data,\n                    DimsToShape(bias_dims), bias_data, output_shape,\n                    output_data, CpuFlags(), /*thread_start=*/0,\n                    /*thread_end=*/output_height, /*thread_dim=*/1);\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, 1, 1, pad_width,\n                pad_height, depth_multiplier, output_activation_min,\n                output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, float* output_data,\n                   const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, pad_width, pad_height,\n                depth_multiplier, output_activation_min, output_activation_max,\n                output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   float* output_data, const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n                    bias_dims, stride, stride, pad_width, pad_height,\n                    depth_multiplier, output_data, output_dims);\n}\n\ntemplate <DepthwiseConvOutputRounding kOutputRounding>\ninline void LegacyDepthwiseConvWithRounding(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, int thread_start, int thread_end, int thread_dim) {\n  ruy::profiler::ScopeLabel label(\"DepthwiseConv/8bit\");\n  const int depth_multiplier = params.depth_multiplier;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  TFLITE_DCHECK_GE(dilation_width_factor, 1);\n  TFLITE_DCHECK_GE(dilation_height_factor, 1);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);\n  const int input_depth = input_shape.Dims(3);\n  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);\n\n// Enable for arm64 except for the Nvidia Linux 4 Tegra (L4T) running on\n// Jetson TX-2. This compiler does not support the offsetof() macro.\n#if defined(__aarch64__) && !defined(GOOGLE_L4T)\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  const int output_shift = params.output_shift;\n\n  // Call kernel optimized for depthwise convolutions using 3x3 filters if\n  // parameters are supported.\n  if (depthwise_conv::Fast3x3FilterKernelSupported(\n          input_shape, filter_shape, stride_width, stride_height,\n          dilation_width_factor, dilation_height_factor, pad_width, pad_height,\n          depth_multiplier, output_shape, output_shift)) {\n    ruy::profiler::ScopeLabel specialized_label(\"DepthwiseConv/8bit/3x3\");\n    depthwise_conv::DepthwiseConv3x3Filter<kOutputRounding>(\n        params, input_shape, input_data, filter_shape, filter_data, bias_shape,\n        bias_data, output_shape, output_data, thread_start, thread_end,\n        thread_dim);\n    return;\n  }\n#endif\n\n  ruy::profiler::ScopeLabel specialized_label(\"DepthwiseConv/8bit/General\");\n  depthwise_conv::DepthwiseConvGeneral(params, input_shape, input_data,\n                                       filter_shape, filter_data, bias_shape,\n                                       bias_data, output_shape, output_data,\n                                       thread_start, thread_end, thread_dim);\n}\n\ninline void LegacyDepthwiseConvImpl(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, int thread_start, int thread_end, int thread_dim) {\n  return LegacyDepthwiseConvWithRounding<\n      DepthwiseConvOutputRounding::kAwayFromZero>(\n      params, input_shape, input_data, filter_shape, filter_data, bias_shape,\n      bias_data, output_shape, output_data, thread_start, thread_end,\n      thread_dim);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kDepthwiseReverseShift * output_shift;\n\n  const RuntimeShape output_shape = DimsToShape(output_dims);\n  const int output_height = output_shape.Dims(1);\n\n  LegacyDepthwiseConvImpl(\n      op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n      filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n      output_data, /*thread_start=*/0,\n      /*thread_end=*/output_height, /*thread_dim=*/1);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, 1, 1, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, int32 output_offset,\n                   int32 output_multiplier, int output_shift,\n                   int32 output_activation_min, int32 output_activation_max,\n                   uint8* output_data, const Dims<4>& output_dims) {\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   int32 output_offset, int32 output_multiplier,\n                   int output_shift, int32 output_activation_min,\n                   int32 output_activation_max, uint8* output_data,\n                   const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, input_offset, filter_data,\n                    filter_dims, filter_offset, bias_data, bias_dims, stride,\n                    stride, pad_width, pad_height, depth_multiplier,\n                    output_offset, output_multiplier, output_shift,\n                    output_activation_min, output_activation_max, output_data,\n                    output_dims);\n}\n\ntemplate <typename T, typename TS>\nstruct LegacyDepthwiseConvWorkerTask : public gemmlowp::Task {\n  LegacyDepthwiseConvWorkerTask(\n      const DepthwiseParams& params, const RuntimeShape& input_shape,\n      const T* input_data, const RuntimeShape& filter_shape,\n      const T* filter_data, const RuntimeShape& bias_shape, const TS* bias_data,\n      const RuntimeShape& output_shape, T* output_data, int thread_start,\n      int thread_end, int thread_dim)\n      : params_(params),\n        input_shape_(input_shape),\n        input_data_(input_data),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        thread_start_(thread_start),\n        thread_end_(thread_end),\n        thread_dim_(thread_dim) {}\n\n  void Run() override {\n    LegacyDepthwiseConvImpl(params_, input_shape_, input_data_, filter_shape_,\n                            filter_data_, bias_shape_, bias_data_,\n                            output_shape_, output_data_, thread_start_,\n                            thread_end_, thread_dim_);\n  }\n\n private:\n  const DepthwiseParams& params_;\n  const RuntimeShape& input_shape_;\n  const T* input_data_;\n  const RuntimeShape& filter_shape_;\n  const T* filter_data_;\n  const RuntimeShape& bias_shape_;\n  const TS* bias_data_;\n  const RuntimeShape& output_shape_;\n  T* output_data_;\n  int thread_start_;\n  int thread_end_;\n  int thread_dim_;\n};\n\ninline int HowManyConvThreads(const RuntimeShape& output_shape,\n                              const RuntimeShape& filter_shape,\n                              int thread_dim) {\n  constexpr int kMinMulPerThread = 8;\n  const int output_units = output_shape.Dims(thread_dim);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int num_mul_per_unit =\n      FlatSizeSkipDim(output_shape, thread_dim) * filter_height * filter_width;\n  const int min_units_per_thread = kMinMulPerThread / num_mul_per_unit + 1;\n  int thread_count = output_units / min_units_per_thread;\n  return thread_count;\n}\n\ninline void DepthwiseConv(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext* gemmlowp_context = nullptr) {\n  ruy::profiler::ScopeLabel label(\"DepthwiseConv\");\n\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int output_batches = output_shape.Dims(0);\n  const int output_rows = output_shape.Dims(1);\n  int thread_count_batch = HowManyConvThreads(output_shape, filter_shape, 0);\n  int thread_count_row = HowManyConvThreads(output_shape, filter_shape, 1);\n  int thread_dim, thread_count, thread_dim_size;\n  if (thread_count_batch > thread_count_row) {\n    thread_dim = 0;\n    thread_dim_size = output_batches;\n    thread_count = thread_count_batch;\n  } else {\n    thread_dim = 1;\n    thread_dim_size = output_rows;\n    thread_count = thread_count_row;\n  }\n\n  const int max_threads =\n      gemmlowp_context ? gemmlowp_context->max_num_threads() : 1;\n  thread_count = std::max(1, std::min(thread_count, max_threads));\n\n  if (thread_count == 1) {\n    LegacyDepthwiseConvImpl(params, input_shape, input_data, filter_shape,\n                            filter_data, bias_shape, bias_data, output_shape,\n                            output_data, /*thread_start=*/0,\n                            /*thread_end=*/output_rows, /*thread_dim=*/1);\n  } else {\n    std::vector<gemmlowp::Task*> tasks(thread_count);\n    int thread_start = 0;\n    for (int i = 0; i < thread_count; ++i) {\n      int thread_end =\n          thread_start + (thread_dim_size - thread_start) / (thread_count - i);\n      tasks[i] = new LegacyDepthwiseConvWorkerTask<uint8, int32>(\n          params, input_shape, input_data, filter_shape, filter_data,\n          bias_shape, bias_data, output_shape, output_data, thread_start,\n          thread_end, thread_dim);\n      thread_start = thread_end;\n    }\n    gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n  }\n}\n\ntemplate <typename T, typename TS>\nstruct LegacyPerChannelDepthwiseConvWorkerTask : public gemmlowp::Task {\n  LegacyPerChannelDepthwiseConvWorkerTask(\n      const DepthwiseParams& params, const int32* output_multiplier,\n      const int32* output_shift, const RuntimeShape& input_shape,\n      const T* input_data, const RuntimeShape& filter_shape,\n      const T* filter_data, const RuntimeShape& bias_shape, const TS* bias_data,\n      const RuntimeShape& output_shape, T* output_data, int thread_start,\n      int thread_end, int thread_dim)\n      : params_(params),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        input_shape_(input_shape),\n        input_data_(input_data),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        thread_start_(thread_start),\n        thread_end_(thread_end),\n        thread_dim_(thread_dim) {}\n\n  void Run() override {\n    CpuBackendContext backend_context;\n    optimized_integer_ops::DepthwiseConvImpl(\n        params_, output_multiplier_, output_shift_, input_shape_, input_data_,\n        filter_shape_, filter_data_, bias_shape_, bias_data_, output_shape_,\n        output_data_, thread_start_, thread_end_, thread_dim_, backend_context);\n  }\n\n private:\n  const DepthwiseParams& params_;\n  const int32* output_multiplier_;\n  const int32* output_shift_;\n  const RuntimeShape& input_shape_;\n  const T* input_data_;\n  const RuntimeShape& filter_shape_;\n  const T* filter_data_;\n  const RuntimeShape& bias_shape_;\n  const TS* bias_data_;\n  const RuntimeShape& output_shape_;\n  T* output_data_;\n  int thread_start_;\n  int thread_end_;\n  int thread_dim_;\n};\n\ninline void DepthwiseConvPerChannel(\n    const DepthwiseParams& params, const int32* output_multiplier,\n    const int32* output_shift, const RuntimeShape& input_shape,\n    const int8* input_data, const RuntimeShape& filter_shape,\n    const int8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape, int8* output_data,\n    gemmlowp::GemmContext* gemmlowp_context = nullptr) {\n  ruy::profiler::ScopeLabel label(\"DepthwiseConvInt8\");\n\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int output_batches = output_shape.Dims(0);\n  const int output_rows = output_shape.Dims(1);\n  int thread_count_batch = HowManyConvThreads(output_shape, filter_shape, 0);\n  int thread_count_row = HowManyConvThreads(output_shape, filter_shape, 1);\n  int thread_dim, thread_count, thread_dim_size;\n  if (thread_count_batch > thread_count_row) {\n    thread_dim = 0;\n    thread_dim_size = output_batches;\n    thread_count = thread_count_batch;\n  } else {\n    thread_dim = 1;\n    thread_dim_size = output_rows;\n    thread_count = thread_count_row;\n  }\n\n  const int max_threads =\n      gemmlowp_context ? gemmlowp_context->max_num_threads() : 1;\n  thread_count = std::max(1, std::min(thread_count, max_threads));\n\n  if (thread_count == 1) {\n    CpuBackendContext backend_context;\n    optimized_integer_ops::DepthwiseConvImpl(\n        params, output_multiplier, output_shift, input_shape, input_data,\n        filter_shape, filter_data, bias_shape, bias_data, output_shape,\n        output_data, /*thread_start=*/0,\n        /*thread_end=*/output_rows, /*thread_dim=*/1, backend_context);\n  } else {\n    std::vector<gemmlowp::Task*> tasks(thread_count);\n    int thread_start = 0;\n    for (int i = 0; i < thread_count; ++i) {\n      int thread_end =\n          thread_start + (thread_dim_size - thread_start) / (thread_count - i);\n      tasks[i] = new LegacyPerChannelDepthwiseConvWorkerTask<int8, int32>(\n          params, output_multiplier, output_shift, input_shape, input_data,\n          filter_shape, filter_data, bias_shape, bias_data, output_shape,\n          output_data, thread_start, thread_end, thread_dim);\n      thread_start = thread_end;\n    }\n    gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n  }\n}\n\ninline void DepthwiseConv(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  DepthwiseConvImpl(params, input_shape, input_data, filter_shape, filter_data,\n                    bias_shape, bias_data, output_shape, output_data,\n                    CpuFlags(),\n                    /*thread_start=*/0,\n                    /*thread_end=*/output_shape.Dims(1), /*thread_dim=*/1);\n}\n\ninline void AddBiasAndEvalActivationFunction(const float* bias_data,\n                                             const Dims<4>& bias_dims,\n                                             float* array_data,\n                                             const Dims<4>& array_dims,\n                                             float output_activation_min,\n                                             float output_activation_max) {\n  AddBiasAndEvalActivationFunction(output_activation_min, output_activation_max,\n                                   DimsToShape(bias_dims), bias_data,\n                                   DimsToShape(array_dims), array_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AddBiasAndEvalActivationFunction(const float* bias_data,\n                                      const Dims<4>& bias_dims,\n                                      float* array_data,\n                                      const Dims<4>& array_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  AddBiasAndEvalActivationFunction(bias_data, bias_dims, array_data, array_dims,\n                                   output_activation_min,\n                                   output_activation_max);\n}\n\ntemplate <typename Lhs, typename Rhs, typename Result>\nvoid Gemm(const Eigen::MatrixBase<Lhs>& lhs, const Eigen::MatrixBase<Rhs>& rhs,\n          Eigen::MatrixBase<Result>* result) {\n  if (rhs.cols() == 1) {\n    ruy::profiler::ScopeLabel label(\"GEMV\");\n    result->col(0).noalias() = lhs * rhs.col(0);\n  } else {\n    ruy::profiler::ScopeLabel label(\"GEMM\");\n    result->noalias() = lhs * rhs;\n  }\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& bias_shape,\n    const float* optional_bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected\");\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n\n  // TODO(b/62193649): this convoluted shape computation (determining\n  // input_rows from the weights_dims, then MapAsMatrixWithGivenNumberOfRows)\n  // is because the current --variable_batch hack consists in overwriting the\n  // 3rd dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  // When that is fixed, this should become:\n  // const auto input_matrix_map =\n  //     MapAsMatrixWithFirstDimAsRows(input_data, input_dims);\n  const int dims_count = weights_shape.DimensionsCount();\n  const int input_rows = weights_shape.Dims(dims_count - 1);\n  const auto input_matrix_map =\n      MapAsMatrixWithGivenNumberOfRows(input_data, input_shape, input_rows);\n  const auto filter_matrix_map =\n      MapAsMatrixWithLastDimAsRows(weights_data, weights_shape);\n  auto output_matrix_map =\n      MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n\n  Gemm(filter_matrix_map.transpose(), input_matrix_map, &output_matrix_map);\n\n  if (optional_bias_data != nullptr) {\n    AddBiasAndEvalActivationFunction(\n        output_activation_min, output_activation_max, bias_shape,\n        optional_bias_data, output_shape, output_data);\n  } else {\n    const int flat_size = output_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      output_data[i] = ActivationFunctionWithMinMax(\n          output_data[i], output_activation_min, output_activation_max);\n    }\n  }\n}\n\ninline void FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                           const float* weights_data,\n                           const Dims<4>& weights_dims, const float* bias_data,\n                           const Dims<4>& bias_dims,\n                           float output_activation_min,\n                           float output_activation_max, float* output_data,\n                           const Dims<4>& output_dims) {\n  tflite::FullyConnectedParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(weights_dims), weights_data,\n                 DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                    const float* weights_data, const Dims<4>& weights_dims,\n                    const float* bias_data, const Dims<4>& bias_dims,\n                    float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  FullyConnected(input_data, input_dims, weights_data, weights_dims, bias_data,\n                 bias_dims, output_activation_min, output_activation_max,\n                 output_data, output_dims);\n}\n\nstruct GemmlowpOutputPipeline {\n  typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n      ColVectorMap;\n  typedef std::tuple<gemmlowp::OutputStageBiasAddition<ColVectorMap>,\n                     gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent,\n                     gemmlowp::OutputStageClamp,\n                     gemmlowp::OutputStageSaturatingCastToUint8>\n      Pipeline;\n  static Pipeline MakeExp(const int32* bias_data, int output_rows,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_left_shift, int32 output_activation_min,\n                          int32 output_activation_max) {\n    ColVectorMap bias_vector(bias_data, output_rows);\n    gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n    bias_addition_stage.bias_vector = bias_vector;\n    gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent quantize_down_stage;\n    quantize_down_stage.result_offset_after_shift = output_offset;\n    quantize_down_stage.result_fixedpoint_multiplier = output_multiplier;\n    quantize_down_stage.result_exponent = output_left_shift;\n    gemmlowp::OutputStageClamp clamp_stage;\n    clamp_stage.min = output_activation_min;\n    clamp_stage.max = output_activation_max;\n    gemmlowp::OutputStageSaturatingCastToUint8 saturating_cast_stage;\n    return std::make_tuple(bias_addition_stage, quantize_down_stage,\n                           clamp_stage, saturating_cast_stage);\n  }\n};\n\nstruct GemmlowpOutputPipelineInt8 {\n  typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n      ColVectorMap;\n  typedef std::tuple<gemmlowp::OutputStageBiasAddition<ColVectorMap>,\n                     gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent,\n                     gemmlowp::OutputStageClamp,\n                     gemmlowp::OutputStageSaturatingCastToInt8>\n      Pipeline;\n  static Pipeline MakeExp(const int32* bias_data, int output_rows,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_left_shift, int32 output_activation_min,\n                          int32 output_activation_max) {\n    ColVectorMap bias_vector(bias_data, output_rows);\n    gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n    bias_addition_stage.bias_vector = bias_vector;\n    gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent quantize_down_stage;\n    quantize_down_stage.result_offset_after_shift = output_offset;\n    quantize_down_stage.result_fixedpoint_multiplier = output_multiplier;\n    quantize_down_stage.result_exponent = output_left_shift;\n    gemmlowp::OutputStageClamp clamp_stage;\n    clamp_stage.min = output_activation_min;\n    clamp_stage.max = output_activation_max;\n    gemmlowp::OutputStageSaturatingCastToInt8 saturating_cast_stage;\n    return std::make_tuple(bias_addition_stage, quantize_down_stage,\n                           clamp_stage, saturating_cast_stage);\n  }\n};\n\n#ifdef USE_NEON\ninline void LegacyFullyConnectedAsGEMVWorkerImpl(\n    const RuntimeShape& input_shape, const uint8* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const uint8* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    uint8* output_data, int row_start, int row_end) {\n  ruy::profiler::ScopeLabel label(\"FullyConnectedAsGEMV/8bit\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kPeel = 4;\n  const bool shift_left = (output_shift > 0);\n  for (int k = 0; k < input_size; k += 64) {\n    optimized_ops_preload_l1_stream(input_data + k);\n  }\n  for (int k = 0; k < kPeel * input_size; k += 64) {\n    optimized_ops_preload_l1_stream(filter_data + k);\n  }\n\n  TFLITE_DCHECK_GE(row_end - row_start, kPeel);\n\n  for (int out = row_start; out < row_end; out += kPeel) {\n    out = std::min(out, row_end - kPeel);\n    int32x4_t acc0 = vdupq_n_s32(0);\n    int32x4_t acc1 = acc0;\n    int32x4_t acc2 = acc0;\n    int32x4_t acc3 = acc0;\n    const int16x8_t input_offset_vec = vdupq_n_s16(input_offset);\n    const int16x8_t filter_offset_vec = vdupq_n_s16(filter_offset);\n    int in = 0;\n    for (; in <= input_size - 16; in += 16) {\n      const uint8x16_t input_val_u8 = vld1q_u8(input_data + in);\n      const uint8* filter_ptr = filter_data + in + out * input_size;\n      uint8x16_t filter_val_u8_0 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      filter_ptr += input_size;\n      uint8x16_t filter_val_u8_1 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      filter_ptr += input_size;\n      uint8x16_t filter_val_u8_2 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      filter_ptr += input_size;\n      uint8x16_t filter_val_u8_3 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      int16x8_t input_val_0, input_val_1;\n      uint8x8_t low = vget_low_u8(input_val_u8);\n      uint8x8_t high = vget_high_u8(input_val_u8);\n      input_val_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      input_val_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      input_val_0 = vaddq_s16(input_val_0, input_offset_vec);\n      input_val_1 = vaddq_s16(input_val_1, input_offset_vec);\n      low = vget_low_u8(filter_val_u8_0);\n      high = vget_high_u8(filter_val_u8_0);\n      int16x8_t filter_val_0_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_0_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_0_0 = vaddq_s16(filter_val_0_0, filter_offset_vec);\n      filter_val_0_1 = vaddq_s16(filter_val_0_1, filter_offset_vec);\n      low = vget_low_u8(filter_val_u8_1);\n      high = vget_high_u8(filter_val_u8_1);\n      int16x8_t filter_val_1_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_1_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_1_0 = vaddq_s16(filter_val_1_0, filter_offset_vec);\n      filter_val_1_1 = vaddq_s16(filter_val_1_1, filter_offset_vec);\n      low = vget_low_u8(filter_val_u8_2);\n      high = vget_high_u8(filter_val_u8_2);\n      int16x8_t filter_val_2_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_2_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_2_0 = vaddq_s16(filter_val_2_0, filter_offset_vec);\n      filter_val_2_1 = vaddq_s16(filter_val_2_1, filter_offset_vec);\n      low = vget_low_u8(filter_val_u8_3);\n      high = vget_high_u8(filter_val_u8_3);\n      int16x8_t filter_val_3_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_3_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_3_0 = vaddq_s16(filter_val_3_0, filter_offset_vec);\n      filter_val_3_1 = vaddq_s16(filter_val_3_1, filter_offset_vec);\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_0),\n                       vget_low_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_0),\n                       vget_low_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_0),\n                       vget_low_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_0),\n                       vget_low_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_1),\n                       vget_low_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_1),\n                       vget_low_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_1),\n                       vget_low_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_1),\n                       vget_low_s16(input_val_1));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_0),\n                       vget_high_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_0),\n                       vget_high_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_0),\n                       vget_high_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_0),\n                       vget_high_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_1),\n                       vget_high_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_1),\n                       vget_high_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_1),\n                       vget_high_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_1),\n                       vget_high_s16(input_val_1));\n    }\n    for (; in <= input_size - 8; in += 8) {\n      const uint8x8_t input_val_u8 = vld1_u8(input_data + in);\n      const uint8* filter_ptr = filter_data + in + out * input_size;\n      uint8x8_t filter_val_u8_0 = vld1_u8(filter_ptr);\n      filter_ptr += input_size;\n      uint8x8_t filter_val_u8_1 = vld1_u8(filter_ptr);\n      filter_ptr += input_size;\n      uint8x8_t filter_val_u8_2 = vld1_u8(filter_ptr);\n      filter_ptr += input_size;\n      uint8x8_t filter_val_u8_3 = vld1_u8(filter_ptr);\n      int16x8_t input_val = vreinterpretq_s16_u16(vmovl_u8(input_val_u8));\n      input_val = vaddq_s16(input_val, input_offset_vec);\n      int16x8_t filter_val_0 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_0));\n      filter_val_0 = vaddq_s16(filter_val_0, filter_offset_vec);\n      int16x8_t filter_val_1 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_1));\n      filter_val_1 = vaddq_s16(filter_val_1, filter_offset_vec);\n      int16x8_t filter_val_2 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_2));\n      filter_val_2 = vaddq_s16(filter_val_2, filter_offset_vec);\n      int16x8_t filter_val_3 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_3));\n      filter_val_3 = vaddq_s16(filter_val_3, filter_offset_vec);\n      acc0 =\n          vmlal_s16(acc0, vget_low_s16(filter_val_0), vget_low_s16(input_val));\n      acc1 =\n          vmlal_s16(acc1, vget_low_s16(filter_val_1), vget_low_s16(input_val));\n      acc2 =\n          vmlal_s16(acc2, vget_low_s16(filter_val_2), vget_low_s16(input_val));\n      acc3 =\n          vmlal_s16(acc3, vget_low_s16(filter_val_3), vget_low_s16(input_val));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0),\n                       vget_high_s16(input_val));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1),\n                       vget_high_s16(input_val));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2),\n                       vget_high_s16(input_val));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3),\n                       vget_high_s16(input_val));\n    }\n    if (in < input_size) {\n      int32 buf[16];\n      vst1q_s32(buf + 0, acc0);\n      vst1q_s32(buf + 4, acc1);\n      vst1q_s32(buf + 8, acc2);\n      vst1q_s32(buf + 12, acc3);\n      for (; in < input_size; in++) {\n        int lane = (in + 8 - input_size) % 4;\n        const int32 input_val = input_data[in] + input_offset;\n        for (int k = 0; k < kPeel; k++) {\n          int32 filter_val =\n              filter_data[in + (out + k) * input_size] + filter_offset;\n          buf[lane + 4 * k] += filter_val * input_val;\n        }\n      }\n      acc0 = vld1q_s32(buf + 0);\n      acc1 = vld1q_s32(buf + 4);\n      acc2 = vld1q_s32(buf + 8);\n      acc3 = vld1q_s32(buf + 12);\n    }\n\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(acc0), vget_high_s32(acc0));\n    int32x2_t pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(acc1), vget_high_s32(acc1));\n    int32x2_t pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(acc2), vget_high_s32(acc2));\n    int32x2_t pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(acc3), vget_high_s32(acc3));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    int32x4_t bias_vec = vld1q_s32(bias_data + out);\n    reduced = vaddq_s32(reduced, bias_vec);\n    if (shift_left) {\n      const int32 multiplier_power_of_two = 1 << output_shift;\n      reduced = vmulq_n_s32(reduced, multiplier_power_of_two);\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n    } else {\n      // Multiply by the fixed-point multiplier.\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n      // Rounding-shift-right.\n      using gemmlowp::RoundingDivideByPOT;\n      reduced = RoundingDivideByPOT(reduced, -output_shift);\n    }\n    // Add the output offset.\n    const int32x4_t output_offset_vec = vdupq_n_s32(output_offset);\n    reduced = vaddq_s32(reduced, output_offset_vec);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    // Narrow values down to 8 bit unsigned, saturating.\n    uint8x8_t res8 = vqmovun_s16(vcombine_s16(res16, res16));\n    // Apply the clamping from the activation function\n    res8 = vmax_u8(res8, vdup_n_u8(output_activation_min));\n    res8 = vmin_u8(res8, vdup_n_u8(output_activation_max));\n    // Store results to destination.\n    vst1_lane_u8(output_data + out + 0, res8, 0);\n    vst1_lane_u8(output_data + out + 1, res8, 1);\n    vst1_lane_u8(output_data + out + 2, res8, 2);\n    vst1_lane_u8(output_data + out + 3, res8, 3);\n  }\n}\n\nstruct LegacyFullyConnectedAsGEMVWorkerTask : public gemmlowp::Task {\n  LegacyFullyConnectedAsGEMVWorkerTask(\n      const RuntimeShape& input_shape, const uint8* input_data,\n      int32 input_offset, const RuntimeShape& filter_shape,\n      const uint8* filter_data, int32 filter_offset,\n      const RuntimeShape& bias_shape, const int32* bias_data,\n      int32 output_offset, int32 output_multiplier, int output_shift,\n      int32 output_activation_min, int32 output_activation_max,\n      const RuntimeShape& output_shape, uint8* output_data, int row_start,\n      int row_end)\n      : input_shape_(input_shape),\n        input_data_(input_data),\n        input_offset_(input_offset),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        filter_offset_(filter_offset),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_offset_(output_offset),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_activation_min_(output_activation_min),\n        output_activation_max_(output_activation_max),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        row_start_(row_start),\n        row_end_(row_end) {}\n\n  void Run() override {\n    LegacyFullyConnectedAsGEMVWorkerImpl(\n        input_shape_, input_data_, input_offset_, filter_shape_, filter_data_,\n        filter_offset_, bias_shape_, bias_data_, output_offset_,\n        output_multiplier_, output_shift_, output_activation_min_,\n        output_activation_max_, output_shape_, output_data_, row_start_,\n        row_end_);\n  }\n\n  const RuntimeShape& input_shape_;\n  const uint8* input_data_;\n  int32 input_offset_;\n  const RuntimeShape& filter_shape_;\n  const uint8* filter_data_;\n  int32 filter_offset_;\n  const RuntimeShape& bias_shape_;\n  const int32* bias_data_;\n  int32 output_offset_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int32 output_activation_min_;\n  int32 output_activation_max_;\n  const RuntimeShape& output_shape_;\n  uint8* output_data_;\n  int row_start_;\n  int row_end_;\n};\n\ninline void FullyConnectedAsGEMV(\n    const RuntimeShape& input_shape, const uint8* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const uint8* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kKernelRows = 4;\n  const int thread_count = gemmlowp::HowManyThreads<kKernelRows>(\n      gemmlowp_context->max_num_threads(), output_rows, batches, input_size);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    LegacyFullyConnectedAsGEMVWorkerImpl(\n        input_shape, input_data, input_offset, filter_shape, filter_data,\n        filter_offset, bias_shape, bias_data, output_offset, output_multiplier,\n        output_shift, output_activation_min, output_activation_max,\n        output_shape, output_data, 0, output_rows);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<gemmlowp::Task*> tasks(thread_count);\n  const int kRowsPerWorker = gemmlowp::RoundUp<kKernelRows>(\n      gemmlowp::CeilQuotient(output_rows, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; ++i) {\n    int row_end = std::min(output_rows, row_start + kRowsPerWorker);\n    tasks[i] = new LegacyFullyConnectedAsGEMVWorkerTask(\n        input_shape, input_data, input_offset, filter_shape, filter_data,\n        filter_offset, bias_shape, bias_data, output_offset, output_multiplier,\n        output_shift, output_activation_min, output_activation_max,\n        output_shape, output_data, row_start, row_end);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_rows);\n  gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n}\n#endif  // USE_NEON\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/8bit\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n#ifdef USE_NEON\n  if (batches == 1) {\n    const int output_size = MatchingDim(filter_shape, filter_dim_count - 2,\n                                        output_shape, output_dim_count - 1);\n    if (output_size >= 4) {\n      return FullyConnectedAsGEMV(\n          input_shape, input_data, input_offset, filter_shape, filter_data,\n          filter_offset, bias_shape, bias_data, output_offset,\n          output_multiplier, output_shift, output_activation_min,\n          output_activation_max, output_shape, output_data, gemmlowp_context);\n    }\n  }\n#endif  // USE_NEON\n  const int filter_rows = filter_shape.Dims(filter_dim_count - 2);\n  const int filter_cols = filter_shape.Dims(filter_dim_count - 1);\n  TFLITE_DCHECK_EQ(filter_shape.FlatSize(), filter_rows * filter_cols);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, output_rows, filter_cols, filter_cols);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, filter_cols, batches, filter_cols);\n  gemmlowp::MatrixMap<uint8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, batches, output_rows);\n  const auto& output_pipeline = GemmlowpOutputPipeline::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, output_shift,\n      output_activation_min, output_activation_max);\n  gemmlowp::GemmWithOutputPipeline<uint8, uint8,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\n#ifdef GEMMLOWP_NEON\n// In the common case of batch size 1, a fully-connected node degenerates\n// to a matrix*vector product. LSTM cells contain a fully-connected node;\n// when quantized, this becomes a special type of GEMV operation where\n// the output is 16bit-quantized, thus needs its own special path.\ninline void GEMVForLstmCell(const RuntimeShape& input_shape,\n                            const uint8* input_data,\n                            const RuntimeShape& weights_shape,\n                            const uint8* weights_data, uint8 weights_zero_point,\n                            const RuntimeShape& bias_shape,\n                            const int32* bias_data, int32 accum_multiplier,\n                            int accum_shift, const RuntimeShape& output_shape,\n                            int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"GEMVForLstmCell\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  const int output_size = MatchingDim(weights_shape, weights_dim_count - 2,\n                                      output_shape, output_dim_count - 1);\n  // This special fast path for quantized LSTM cells does not try to support\n  // odd sizes that we haven't encountered in any LSTM cell, that would\n  // require special code (that would go untested until any LSTM cell\n  // exercises it). We just guard our assumptions about size evenness with\n  // the following assertions.\n  TFLITE_DCHECK(!(output_size % 4));\n  TFLITE_DCHECK(!(input_size % 8));\n  const int32* bias_ptr = bias_data;\n  int16* output_ptr = output_data;\n  for (int out = 0; out < output_size; out += 4) {\n    int32x4_t acc_0 = vdupq_n_s32(0);\n    int32x4_t acc_1 = vdupq_n_s32(0);\n    int32x4_t acc_2 = vdupq_n_s32(0);\n    int32x4_t acc_3 = vdupq_n_s32(0);\n    const int16x8_t input_offset_vec = vdupq_n_s16(-128);\n    const int16x8_t weights_offset_vec = vdupq_n_s16(-weights_zero_point);\n    int in = 0;\n    // Handle 16 levels of depth at a time.\n    for (; in <= input_size - 16; in += 16) {\n      const uint8x16_t input_val_u8 = vld1q_u8(input_data + in);\n      const uint8* weights_ptr = weights_data + in + out * input_size;\n      uint8x16_t weights_val_u8_0 = vld1q_u8(weights_ptr + 0 * input_size);\n      uint8x16_t weights_val_u8_1 = vld1q_u8(weights_ptr + 1 * input_size);\n      uint8x16_t weights_val_u8_2 = vld1q_u8(weights_ptr + 2 * input_size);\n      uint8x16_t weights_val_u8_3 = vld1q_u8(weights_ptr + 3 * input_size);\n      int16x8_t input_val_0, input_val_1;\n      const uint8x8_t low = vget_low_u8(input_val_u8);\n      const uint8x8_t high = vget_high_u8(input_val_u8);\n      input_val_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      input_val_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      input_val_0 = vaddq_s16(input_val_0, input_offset_vec);\n      input_val_1 = vaddq_s16(input_val_1, input_offset_vec);\n      int16x8_t weights_val_0_0, weights_val_1_0, weights_val_2_0,\n          weights_val_3_0;\n      int16x8_t weights_val_0_1, weights_val_1_1, weights_val_2_1,\n          weights_val_3_1;\n      weights_val_0_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_0))),\n          weights_offset_vec);\n      weights_val_0_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_0))),\n          weights_offset_vec);\n      weights_val_1_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_1))),\n          weights_offset_vec);\n      weights_val_1_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_1))),\n          weights_offset_vec);\n      weights_val_2_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_2))),\n          weights_offset_vec);\n      weights_val_2_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_2))),\n          weights_offset_vec);\n      weights_val_3_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_3))),\n          weights_offset_vec);\n      weights_val_3_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_3))),\n          weights_offset_vec);\n      acc_0 = vmlal_s16(acc_0, vget_low_s16(weights_val_0_0),\n                        vget_low_s16(input_val_0));\n      acc_1 = vmlal_s16(acc_1, vget_low_s16(weights_val_1_0),\n                        vget_low_s16(input_val_0));\n      acc_2 = vmlal_s16(acc_2, vget_low_s16(weights_val_2_0),\n                        vget_low_s16(input_val_0));\n      acc_3 = vmlal_s16(acc_3, vget_low_s16(weights_val_3_0),\n                        vget_low_s16(input_val_0));\n      acc_0 = vmlal_s16(acc_0, vget_high_s16(weights_val_0_0),\n                        vget_high_s16(input_val_0));\n      acc_1 = vmlal_s16(acc_1, vget_high_s16(weights_val_1_0),\n                        vget_high_s16(input_val_0));\n      acc_2 = vmlal_s16(acc_2, vget_high_s16(weights_val_2_0),\n                        vget_high_s16(input_val_0));\n      acc_3 = vmlal_s16(acc_3, vget_high_s16(weights_val_3_0),\n                        vget_high_s16(input_val_0));\n      acc_0 = vmlal_s16(acc_0, vget_low_s16(weights_val_0_1),\n                        vget_low_s16(input_val_1));\n      acc_1 = vmlal_s16(acc_1, vget_low_s16(weights_val_1_1),\n                        vget_low_s16(input_val_1));\n      acc_2 = vmlal_s16(acc_2, vget_low_s16(weights_val_2_1),\n                        vget_low_s16(input_val_1));\n      acc_3 = vmlal_s16(acc_3, vget_low_s16(weights_val_3_1),\n                        vget_low_s16(input_val_1));\n      acc_0 = vmlal_s16(acc_0, vget_high_s16(weights_val_0_1),\n                        vget_high_s16(input_val_1));\n      acc_1 = vmlal_s16(acc_1, vget_high_s16(weights_val_1_1),\n                        vget_high_s16(input_val_1));\n      acc_2 = vmlal_s16(acc_2, vget_high_s16(weights_val_2_1),\n                        vget_high_s16(input_val_1));\n      acc_3 = vmlal_s16(acc_3, vget_high_s16(weights_val_3_1),\n                        vget_high_s16(input_val_1));\n    }\n    // Handle 8 levels of depth at a time.\n    for (; in < input_size; in += 8) {\n      const uint8x8_t input_val_u8 = vld1_u8(input_data + in);\n      const uint8* weights_ptr = weights_data + in + out * input_size;\n      uint8x8_t weights_val_u8_0 = vld1_u8(weights_ptr + 0 * input_size);\n      uint8x8_t weights_val_u8_1 = vld1_u8(weights_ptr + 1 * input_size);\n      uint8x8_t weights_val_u8_2 = vld1_u8(weights_ptr + 2 * input_size);\n      uint8x8_t weights_val_u8_3 = vld1_u8(weights_ptr + 3 * input_size);\n      int16x8_t input_val;\n      input_val = vreinterpretq_s16_u16(vmovl_u8(input_val_u8));\n      input_val = vaddq_s16(input_val, input_offset_vec);\n      int16x8_t weights_val_0, weights_val_1, weights_val_2, weights_val_3;\n      weights_val_0 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_0)),\n                    weights_offset_vec);\n      weights_val_1 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_1)),\n                    weights_offset_vec);\n      weights_val_2 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_2)),\n                    weights_offset_vec);\n      weights_val_3 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_3)),\n                    weights_offset_vec);\n      acc_0 = vmlal_s16(acc_0, vget_low_s16(weights_val_0),\n                        vget_low_s16(input_val));\n      acc_1 = vmlal_s16(acc_1, vget_low_s16(weights_val_1),\n                        vget_low_s16(input_val));\n      acc_2 = vmlal_s16(acc_2, vget_low_s16(weights_val_2),\n                        vget_low_s16(input_val));\n      acc_3 = vmlal_s16(acc_3, vget_low_s16(weights_val_3),\n                        vget_low_s16(input_val));\n      acc_0 = vmlal_s16(acc_0, vget_high_s16(weights_val_0),\n                        vget_high_s16(input_val));\n      acc_1 = vmlal_s16(acc_1, vget_high_s16(weights_val_1),\n                        vget_high_s16(input_val));\n      acc_2 = vmlal_s16(acc_2, vget_high_s16(weights_val_2),\n                        vget_high_s16(input_val));\n      acc_3 = vmlal_s16(acc_3, vget_high_s16(weights_val_3),\n                        vget_high_s16(input_val));\n    }\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,\n        pairwise_reduced_acc_2, pairwise_reduced_acc_3;\n    pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(acc_0), vget_high_s32(acc_0));\n    pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(acc_1), vget_high_s32(acc_1));\n    pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(acc_2), vget_high_s32(acc_2));\n    pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(acc_3), vget_high_s32(acc_3));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    int32x4_t bias_vec = vld1q_s32(bias_ptr);\n    bias_ptr += 4;\n    reduced = vaddq_s32(reduced, bias_vec);\n    int left_shift = accum_shift > 0 ? accum_shift : 0;\n    int right_shift = accum_shift > 0 ? 0 : -accum_shift;\n    reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));\n    // Multiply by the fixed-point multiplier.\n    reduced = vqrdmulhq_n_s32(reduced, accum_multiplier);\n    // Rounding-shift-right.\n    using gemmlowp::RoundingDivideByPOT;\n    reduced = RoundingDivideByPOT(reduced, right_shift);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    vst1_s16(output_ptr, res16);\n    output_ptr += 4;\n  }\n}\n#endif\n\n#ifdef GEMMLOWP_NEON\ninline void GEMVForLstmCellWithSymmetricRange(\n    const RuntimeShape& input_shape, const uint8* input_data,\n    const RuntimeShape& weights_shape, const uint8* weights_data,\n    const RuntimeShape& bias_shape, const int32* bias_data,\n    int32 accum_multiplier, int accum_shift, const RuntimeShape& output_shape,\n    int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"GEMVForLstmCellWithSymmetricRange\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  const int output_size = MatchingDim(weights_shape, weights_dim_count - 2,\n                                      output_shape, output_dim_count - 1);\n  // This special fast path for quantized LSTM cells does not try to support\n  // odd sizes that we haven't encountered in any LSTM cell, that would\n  // require special code (that would go untested until any LSTM cell\n  // exercises it). We just guard our assumptions about size evenness with\n  // the following assertions.\n  TFLITE_DCHECK(!(output_size % 4));\n  TFLITE_DCHECK(!(input_size % 64));\n  const int32* bias_ptr = bias_data;\n  int16* output_ptr = output_data;\n  const uint8x16_t signbit = vdupq_n_u8(0x80);\n  for (int in = 0; in < input_size; in += 32) {\n    optimized_ops_preload_l1_keep(input_data + in);\n  }\n  const int left_shift = accum_shift > 0 ? accum_shift : 0;\n  const int right_shift = accum_shift > 0 ? 0 : -accum_shift;\n  for (int out = 0; out < output_size; out += 4) {\n    // Load the bias values\n    int32x4_t bias_vec = vld1q_s32(bias_ptr);\n    bias_ptr += 4;\n\n    // Clear accumulators. We use 2 accumulator registers per row,\n    // for 4 rows. row_accumRN is the N-th accumulator for row R.\n    int32x4_t row_accum00 = vdupq_n_s32(0);\n    int32x4_t row_accum01 = vdupq_n_s32(0);\n    int32x4_t row_accum10 = vdupq_n_s32(0);\n    int32x4_t row_accum11 = vdupq_n_s32(0);\n    int32x4_t row_accum20 = vdupq_n_s32(0);\n    int32x4_t row_accum21 = vdupq_n_s32(0);\n    int32x4_t row_accum30 = vdupq_n_s32(0);\n    int32x4_t row_accum31 = vdupq_n_s32(0);\n\n    // kReadAhead parametrizes how far ahead we prefetch weights into L1 cache.\n    const int kReadAhead = 512;\n    // Prefetch the first weights values.\n    for (int k = 0; k < kReadAhead; k += 64) {\n      optimized_ops_preload_l1_stream(weights_data + (out + 0) * input_size +\n                                      k);\n      optimized_ops_preload_l1_stream(weights_data + (out + 1) * input_size +\n                                      k);\n      optimized_ops_preload_l1_stream(weights_data + (out + 2) * input_size +\n                                      k);\n      optimized_ops_preload_l1_stream(weights_data + (out + 3) * input_size +\n                                      k);\n    }\n    // Loop along the rows, handling 64 bytes per iteration because that's\n    // cache line size on most current ARM-architecture CPUs.\n    for (int in = 0; in < input_size; in += 64) {\n      // Prefetch some future weights values.\n      optimized_ops_preload_l1_stream(weights_data + (out + 0) * input_size +\n                                      in + kReadAhead);\n      optimized_ops_preload_l1_stream(weights_data + (out + 1) * input_size +\n                                      in + kReadAhead);\n      optimized_ops_preload_l1_stream(weights_data + (out + 2) * input_size +\n                                      in + kReadAhead);\n      optimized_ops_preload_l1_stream(weights_data + (out + 3) * input_size +\n                                      in + kReadAhead);\n\n      // We will use 2 local 16-bit accumulators per row, for 2 rows.\n      // See below (*) for the rationale of processing only 2 rows at a time.\n      // local_accumRN is the N-th local accumulator for row R.\n      int16x8_t local_accum00;\n      int16x8_t local_accum01;\n      int16x8_t local_accum10;\n      int16x8_t local_accum11;\n\n      // Load 64 bytes of input activations values. Convert to signed int8\n      // by flipping the sign bit (i.e. subtracting 128, the required\n      // zero_point value).\n      int8x16_t input0 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 0)));\n      int8x16_t input1 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 1)));\n      int8x16_t input2 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 2)));\n      int8x16_t input3 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 3)));\n\n      // Beginning of the core accumulation. Notice how while we have 4\n      // rows to process, this code is taking care of only 2 rows at a time,\n      // thus being divided into two parts looking similar (\"Rows 0 and 1\" and\n      // \"Rows 2 and 3\").\n      //\n      // (*) The rationale for handling only 2 rows at a time is to avoid\n      // cache aliasing issues on 4-way set-associative L1-cache CPUs, such\n      // as Cortex-A53. With sufficiently large, power-of-two matrix dimensions,\n      // we may find ourselves in a situation where rows alias each other in\n      // the L1 cache, and moreover may also mutually alias with the input\n      // activations. If we try to load 4 rows at a time, together with the\n      // input activations, that may be 5 mutually-aliasing vectors, resulting\n      // in constant mutual eviction from L1 cache. Handling 2 rows at a time\n      // here largely mitigates these issues, and seems at least to be very\n      // effective on Cortex-A53:\n      //                          Before       After\n      // big (Cortex-A73)         2.85 ms      2.85 ms\n      // little (Cortex-A53)      11.0 ms      5.16 ms\n\n      // Rows 0 and 1:\n      // Load 64 bytes of weights values from each row. Convert to signed int8\n      // by flipping the sign bit (i.e. subtracting 128, the required\n      // zero_point value).\n      int8x16_t weights00 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 0)));\n      int8x16_t weights01 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 1)));\n      int8x16_t weights02 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 2)));\n      int8x16_t weights03 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 3)));\n      int8x16_t weights10 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 0)));\n      int8x16_t weights11 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 1)));\n      int8x16_t weights12 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 2)));\n      int8x16_t weights13 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 3)));\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights00), vget_low_s8(input0));\n      local_accum01 = vmull_s8(vget_low_s8(weights01), vget_low_s8(input1));\n      local_accum10 = vmull_s8(vget_low_s8(weights10), vget_low_s8(input0));\n      local_accum11 = vmull_s8(vget_low_s8(weights11), vget_low_s8(input1));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights00),\n                               vget_high_s8(input0));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights01),\n                               vget_high_s8(input1));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights10),\n                               vget_high_s8(input0));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights11),\n                               vget_high_s8(input1));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum00 = vpadalq_s16(row_accum00, local_accum00);\n      row_accum01 = vpadalq_s16(row_accum01, local_accum01);\n      row_accum10 = vpadalq_s16(row_accum10, local_accum10);\n      row_accum11 = vpadalq_s16(row_accum11, local_accum11);\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights02), vget_low_s8(input2));\n      local_accum01 = vmull_s8(vget_low_s8(weights03), vget_low_s8(input3));\n      local_accum10 = vmull_s8(vget_low_s8(weights12), vget_low_s8(input2));\n      local_accum11 = vmull_s8(vget_low_s8(weights13), vget_low_s8(input3));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights02),\n                               vget_high_s8(input2));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights03),\n                               vget_high_s8(input3));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights12),\n                               vget_high_s8(input2));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights13),\n                               vget_high_s8(input3));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum00 = vpadalq_s16(row_accum00, local_accum00);\n      row_accum01 = vpadalq_s16(row_accum01, local_accum01);\n      row_accum10 = vpadalq_s16(row_accum10, local_accum10);\n      row_accum11 = vpadalq_s16(row_accum11, local_accum11);\n\n      // Rows 2 and 3:\n      // Load 64 bytes of weights values from each row. Convert to signed int8\n      // by flipping the sign bit (i.e. subtracting 128, the required\n      // zero_point value).\n      weights00 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 0)));\n      weights01 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 1)));\n      weights02 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 2)));\n      weights03 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 3)));\n      weights10 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 0)));\n      weights11 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 1)));\n      weights12 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 2)));\n      weights13 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 3)));\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights00), vget_low_s8(input0));\n      local_accum01 = vmull_s8(vget_low_s8(weights01), vget_low_s8(input1));\n      local_accum10 = vmull_s8(vget_low_s8(weights10), vget_low_s8(input0));\n      local_accum11 = vmull_s8(vget_low_s8(weights11), vget_low_s8(input1));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights00),\n                               vget_high_s8(input0));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights01),\n                               vget_high_s8(input1));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights10),\n                               vget_high_s8(input0));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights11),\n                               vget_high_s8(input1));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum20 = vpadalq_s16(row_accum20, local_accum00);\n      row_accum21 = vpadalq_s16(row_accum21, local_accum01);\n      row_accum30 = vpadalq_s16(row_accum30, local_accum10);\n      row_accum31 = vpadalq_s16(row_accum31, local_accum11);\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights02), vget_low_s8(input2));\n      local_accum01 = vmull_s8(vget_low_s8(weights03), vget_low_s8(input3));\n      local_accum10 = vmull_s8(vget_low_s8(weights12), vget_low_s8(input2));\n      local_accum11 = vmull_s8(vget_low_s8(weights13), vget_low_s8(input3));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights02),\n                               vget_high_s8(input2));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights03),\n                               vget_high_s8(input3));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights12),\n                               vget_high_s8(input2));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights13),\n                               vget_high_s8(input3));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum20 = vpadalq_s16(row_accum20, local_accum00);\n      row_accum21 = vpadalq_s16(row_accum21, local_accum01);\n      row_accum30 = vpadalq_s16(row_accum30, local_accum10);\n      row_accum31 = vpadalq_s16(row_accum31, local_accum11);\n    }\n\n    row_accum00 = vaddq_s32(row_accum00, row_accum01);\n    row_accum10 = vaddq_s32(row_accum10, row_accum11);\n    row_accum20 = vaddq_s32(row_accum20, row_accum21);\n    row_accum30 = vaddq_s32(row_accum30, row_accum31);\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,\n        pairwise_reduced_acc_2, pairwise_reduced_acc_3;\n    pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(row_accum00), vget_high_s32(row_accum00));\n    pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(row_accum10), vget_high_s32(row_accum10));\n    pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(row_accum20), vget_high_s32(row_accum20));\n    pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(row_accum30), vget_high_s32(row_accum30));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    reduced = vaddq_s32(reduced, bias_vec);\n    reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));\n    // Multiply by the fixed-point multiplier.\n    reduced = vqrdmulhq_n_s32(reduced, accum_multiplier);\n    // Rounding-shift-right.\n    using gemmlowp::RoundingDivideByPOT;\n    reduced = RoundingDivideByPOT(reduced, right_shift);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    vst1_s16(output_ptr, res16);\n    output_ptr += 4;\n  }\n}\n#endif\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data_int32, const RuntimeShape& output_shape,\n    int16* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/Uint8Int16\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  // This is a copy of the reference implementation. We do not currently have a\n  // properly optimized version.\n  (void)gemmlowp_context;  // only used in properly optimized code.\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  TFLITE_DCHECK_EQ(output_offset, 0);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);\n\n  // Implementation of the fully connected node suited to the inside of an LSTM\n  // cell. The operands are 8-bit integers, the accumulators are internally\n  // 32bit integers, and the output is 16-bit fixed-point with 3 integer bits so\n  // the output range is [-2^3, 2^3] == [-8, 8]. The rationale for that\n  // is explained in the function comment above.\n#ifdef GEMMLOWP_NEON\n  if (batches == 1 && input_offset == -128 && output_activation_min == -32768 &&\n      output_activation_max == 32767) {\n    if (filter_offset == -128 && !(output_depth % 4) && !(accum_depth % 64)) {\n      GEMVForLstmCellWithSymmetricRange(\n          input_shape, input_data, filter_shape, filter_data, bias_shape,\n          bias_data_int32, output_multiplier, output_shift, output_shape,\n          output_data);\n      return;\n    }\n    if (!(output_depth % 4) && !(accum_depth % 8)) {\n      GEMVForLstmCell(input_shape, input_data, filter_shape, filter_data,\n                      filter_offset, bias_shape, bias_data_int32,\n                      output_multiplier, output_shift, output_shape,\n                      output_data);\n      return;\n    }\n  }\n#endif\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> weights_matrix(\n      filter_data, output_depth, accum_depth);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, accum_depth, batches);\n  gemmlowp::MatrixMap<int16, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_depth, batches);\n  typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n      ColVectorMap;\n  ColVectorMap bias_vector(bias_data_int32, output_depth);\n  gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n  bias_addition_stage.bias_vector = bias_vector;\n  gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent scale_stage;\n  scale_stage.result_offset_after_shift = 0;\n  scale_stage.result_fixedpoint_multiplier = output_multiplier;\n  // Note that this shift is negated wrt ordinary FC.\n  scale_stage.result_exponent = output_shift;\n  gemmlowp::OutputStageClamp clamp_stage;\n  clamp_stage.min = output_activation_min;\n  clamp_stage.max = output_activation_max;\n  gemmlowp::OutputStageSaturatingCastToInt16 saturating_cast_int16_stage;\n  auto output_pipeline =\n      std::make_tuple(bias_addition_stage, scale_stage, clamp_stage,\n                      saturating_cast_int16_stage);\n  gemmlowp::GemmWithOutputPipeline<uint8, int16,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, weights_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\ninline void FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                           int32 input_offset, const uint8* filter_data,\n                           const Dims<4>& filter_dims, int32 filter_offset,\n                           const int32* bias_data, const Dims<4>& bias_dims,\n                           int32 output_offset, int32 output_multiplier,\n                           int output_shift, int32 output_activation_min,\n                           int32 output_activation_max, uint8* output_data,\n                           const Dims<4>& output_dims,\n                           gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\ninline void FullyConnected(\n    const uint8* input_data, const Dims<4>& input_dims, int32 input_offset,\n    const uint8* filter_data, const Dims<4>& filter_dims, int32 filter_offset,\n    const int32* bias_data_int32, const Dims<4>& bias_dims, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, int16* output_data, const Dims<4>& output_dims,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data_int32, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_offset, const uint8* filter_data,\n                    const Dims<4>& filter_dims, int32 filter_offset,\n                    const int32* bias_data, const Dims<4>& bias_dims,\n                    int32 output_offset, int32 output_multiplier,\n                    int output_shift, int32 output_activation_min,\n                    int32 output_activation_max, uint8* output_data,\n                    const Dims<4>& output_dims,\n                    gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  FullyConnected(input_data, input_dims, input_offset, filter_data, filter_dims,\n                 filter_offset, bias_data, bias_dims, output_offset,\n                 output_multiplier, output_shift, output_activation_min,\n                 output_activation_max, output_data, output_dims,\n                 gemmlowp_context);\n}\n\n#ifdef USE_NEON\ninline void LegacyInt8FullyConnectedAsGEMVWorkerImpl(\n    const RuntimeShape& input_shape, const int8_t* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const int8_t* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    int8_t* output_data, int row_start, int row_end) {\n  ruy::profiler::ScopeLabel label(\"FullyConnectedAsGEMVInt8/8bit\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kPeel = 4;\n  const bool shift_left = (output_shift > 0);\n  TFLITE_DCHECK_GE(row_end - row_start, kPeel);\n\n  for (int out = row_start; out < row_end; out += kPeel) {\n    out = std::min(out, row_end - kPeel);\n    int32x4_t acc0 = vdupq_n_s32(0);\n    int32x4_t acc1 = acc0;\n    int32x4_t acc2 = acc0;\n    int32x4_t acc3 = acc0;\n    const int16x8_t input_offset_vec = vdupq_n_s16(input_offset);\n    const int16x8_t filter_offset_vec = vdupq_n_s16(filter_offset);\n    int in = 0;\n    for (; in <= input_size - 16; in += 16) {\n      const int8x16_t input_val_s8 = vld1q_s8(input_data + in);\n      const int8_t* filter_ptr = filter_data + in + out * input_size;\n      int8x16_t filter_val_s8_0 = vld1q_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x16_t filter_val_s8_1 = vld1q_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x16_t filter_val_s8_2 = vld1q_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x16_t filter_val_s8_3 = vld1q_s8(filter_ptr);\n      int16x8_t input_val_0, input_val_1;\n      int8x8_t low = vget_low_s8(input_val_s8);\n      int8x8_t high = vget_high_s8(input_val_s8);\n      input_val_0 = vmovl_s8(low);\n      input_val_1 = vmovl_s8(high);\n      input_val_0 = vaddq_s16(input_val_0, input_offset_vec);\n      input_val_1 = vaddq_s16(input_val_1, input_offset_vec);\n      low = vget_low_s8(filter_val_s8_0);\n      high = vget_high_s8(filter_val_s8_0);\n      int16x8_t filter_val_0_0 = vmovl_s8(low);\n      int16x8_t filter_val_0_1 = vmovl_s8(high);\n      filter_val_0_0 = vaddq_s16(filter_val_0_0, filter_offset_vec);\n      filter_val_0_1 = vaddq_s16(filter_val_0_1, filter_offset_vec);\n      low = vget_low_s8(filter_val_s8_1);\n      high = vget_high_s8(filter_val_s8_1);\n      int16x8_t filter_val_1_0 = vmovl_s8(low);\n      int16x8_t filter_val_1_1 = vmovl_s8(high);\n      filter_val_1_0 = vaddq_s16(filter_val_1_0, filter_offset_vec);\n      filter_val_1_1 = vaddq_s16(filter_val_1_1, filter_offset_vec);\n      low = vget_low_s8(filter_val_s8_2);\n      high = vget_high_s8(filter_val_s8_2);\n      int16x8_t filter_val_2_0 = vmovl_s8(low);\n      int16x8_t filter_val_2_1 = vmovl_s8(high);\n      filter_val_2_0 = vaddq_s16(filter_val_2_0, filter_offset_vec);\n      filter_val_2_1 = vaddq_s16(filter_val_2_1, filter_offset_vec);\n      low = vget_low_s8(filter_val_s8_3);\n      high = vget_high_s8(filter_val_s8_3);\n      int16x8_t filter_val_3_0 = vmovl_s8(low);\n      int16x8_t filter_val_3_1 = vmovl_s8(high);\n      filter_val_3_0 = vaddq_s16(filter_val_3_0, filter_offset_vec);\n      filter_val_3_1 = vaddq_s16(filter_val_3_1, filter_offset_vec);\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_0),\n                       vget_low_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_0),\n                       vget_low_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_0),\n                       vget_low_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_0),\n                       vget_low_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_1),\n                       vget_low_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_1),\n                       vget_low_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_1),\n                       vget_low_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_1),\n                       vget_low_s16(input_val_1));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_0),\n                       vget_high_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_0),\n                       vget_high_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_0),\n                       vget_high_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_0),\n                       vget_high_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_1),\n                       vget_high_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_1),\n                       vget_high_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_1),\n                       vget_high_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_1),\n                       vget_high_s16(input_val_1));\n    }\n    for (; in <= input_size - 8; in += 8) {\n      const int8x8_t input_val_s8 = vld1_s8(input_data + in);\n      const int8_t* filter_ptr = filter_data + in + out * input_size;\n      int8x8_t filter_val_s8_0 = vld1_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x8_t filter_val_s8_1 = vld1_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x8_t filter_val_s8_2 = vld1_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x8_t filter_val_s8_3 = vld1_s8(filter_ptr);\n      int16x8_t input_val = vmovl_s8(input_val_s8);\n      input_val = vaddq_s16(input_val, input_offset_vec);\n      int16x8_t filter_val_0 = vmovl_s8(filter_val_s8_0);\n      filter_val_0 = vaddq_s16(filter_val_0, filter_offset_vec);\n      int16x8_t filter_val_1 = vmovl_s8(filter_val_s8_1);\n      filter_val_1 = vaddq_s16(filter_val_1, filter_offset_vec);\n      int16x8_t filter_val_2 = vmovl_s8(filter_val_s8_2);\n      filter_val_2 = vaddq_s16(filter_val_2, filter_offset_vec);\n      int16x8_t filter_val_3 = vmovl_s8(filter_val_s8_3);\n      filter_val_3 = vaddq_s16(filter_val_3, filter_offset_vec);\n      acc0 =\n          vmlal_s16(acc0, vget_low_s16(filter_val_0), vget_low_s16(input_val));\n      acc1 =\n          vmlal_s16(acc1, vget_low_s16(filter_val_1), vget_low_s16(input_val));\n      acc2 =\n          vmlal_s16(acc2, vget_low_s16(filter_val_2), vget_low_s16(input_val));\n      acc3 =\n          vmlal_s16(acc3, vget_low_s16(filter_val_3), vget_low_s16(input_val));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0),\n                       vget_high_s16(input_val));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1),\n                       vget_high_s16(input_val));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2),\n                       vget_high_s16(input_val));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3),\n                       vget_high_s16(input_val));\n    }\n    if (in < input_size) {\n      int32 buf[16];\n      vst1q_s32(buf + 0, acc0);\n      vst1q_s32(buf + 4, acc1);\n      vst1q_s32(buf + 8, acc2);\n      vst1q_s32(buf + 12, acc3);\n      for (; in < input_size; in++) {\n        int lane = (in + 8 - input_size) % 4;\n        const int32 input_val = input_data[in] + input_offset;\n        for (int k = 0; k < kPeel; k++) {\n          int32 filter_val =\n              filter_data[in + (out + k) * input_size] + filter_offset;\n          buf[lane + 4 * k] += filter_val * input_val;\n        }\n      }\n      acc0 = vld1q_s32(buf + 0);\n      acc1 = vld1q_s32(buf + 4);\n      acc2 = vld1q_s32(buf + 8);\n      acc3 = vld1q_s32(buf + 12);\n    }\n\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(acc0), vget_high_s32(acc0));\n    int32x2_t pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(acc1), vget_high_s32(acc1));\n    int32x2_t pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(acc2), vget_high_s32(acc2));\n    int32x2_t pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(acc3), vget_high_s32(acc3));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    int32x4_t bias_vec = vld1q_s32(bias_data + out);\n    reduced = vaddq_s32(reduced, bias_vec);\n    if (shift_left) {\n      const int32 multiplier_power_of_two = 1 << output_shift;\n      reduced = vmulq_n_s32(reduced, multiplier_power_of_two);\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n    } else {\n      // Multiply by the fixed-point multiplier.\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n      // Rounding-shift-right.\n      using gemmlowp::RoundingDivideByPOT;\n      reduced = RoundingDivideByPOT(reduced, -output_shift);\n    }\n    // Add the output offset.\n    const int32x4_t output_offset_vec = vdupq_n_s32(output_offset);\n    reduced = vaddq_s32(reduced, output_offset_vec);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    // Narrow values down to 8 bit signed, saturating.\n    int8x8_t res8 = vqmovn_s16(vcombine_s16(res16, res16));\n    // Apply the clamping from the activation function\n    res8 = vmax_s8(res8, vdup_n_s8(output_activation_min));\n    res8 = vmin_s8(res8, vdup_n_s8(output_activation_max));\n    // Store results to destination.\n    vst1_lane_s8(output_data + out + 0, res8, 0);\n    vst1_lane_s8(output_data + out + 1, res8, 1);\n    vst1_lane_s8(output_data + out + 2, res8, 2);\n    vst1_lane_s8(output_data + out + 3, res8, 3);\n  }\n}\n\nstruct LegacyInt8FullyConnectedAsGEMVWorkerTask : public gemmlowp::Task {\n  LegacyInt8FullyConnectedAsGEMVWorkerTask(\n      const RuntimeShape& input_shape, const int8_t* input_data,\n      int32 input_offset, const RuntimeShape& filter_shape,\n      const int8_t* filter_data, int32 filter_offset,\n      const RuntimeShape& bias_shape, const int32* bias_data,\n      int32 output_offset, int32 output_multiplier, int output_shift,\n      int32 output_activation_min, int32 output_activation_max,\n      const RuntimeShape& output_shape, int8_t* output_data, int row_start,\n      int row_end)\n      : input_shape_(input_shape),\n        input_data_(input_data),\n        input_offset_(input_offset),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        filter_offset_(filter_offset),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_offset_(output_offset),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_activation_min_(output_activation_min),\n        output_activation_max_(output_activation_max),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        row_start_(row_start),\n        row_end_(row_end) {}\n\n  void Run() override {\n    LegacyInt8FullyConnectedAsGEMVWorkerImpl(\n        input_shape_, input_data_, input_offset_, filter_shape_, filter_data_,\n        filter_offset_, bias_shape_, bias_data_, output_offset_,\n        output_multiplier_, output_shift_, output_activation_min_,\n        output_activation_max_, output_shape_, output_data_, row_start_,\n        row_end_);\n  }\n\n  const RuntimeShape& input_shape_;\n  const int8_t* input_data_;\n  int32 input_offset_;\n  const RuntimeShape& filter_shape_;\n  const int8_t* filter_data_;\n  int32 filter_offset_;\n  const RuntimeShape& bias_shape_;\n  const int32* bias_data_;\n  int32 output_offset_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int32 output_activation_min_;\n  int32 output_activation_max_;\n  const RuntimeShape& output_shape_;\n  int8_t* output_data_;\n  int row_start_;\n  int row_end_;\n};\n\ninline void LegacyInt8FullyConnectedAsGEMV(\n    const RuntimeShape& input_shape, const int8_t* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const int8_t* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    int8_t* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kKernelRows = 4;\n  const int thread_count = gemmlowp::HowManyThreads<kKernelRows>(\n      gemmlowp_context->max_num_threads(), output_rows, batches, input_size);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    LegacyInt8FullyConnectedAsGEMVWorkerImpl(\n        input_shape, input_data, input_offset, filter_shape, filter_data,\n        filter_offset, bias_shape, bias_data, output_offset, output_multiplier,\n        output_shift, output_activation_min, output_activation_max,\n        output_shape, output_data, 0, output_rows);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<LegacyInt8FullyConnectedAsGEMVWorkerTask> tasks;\n  // TODO(b/131746020) don't create new heap allocations every time.\n  // At least we make it a single heap allocation by using reserve().\n  tasks.reserve(thread_count);\n  const int kRowsPerWorker = gemmlowp::RoundUp<kKernelRows>(\n      gemmlowp::CeilQuotient(output_rows, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; ++i) {\n    int row_end = std::min(output_rows, row_start + kRowsPerWorker);\n    tasks.emplace_back(input_shape, input_data, input_offset, filter_shape,\n                       filter_data, filter_offset, bias_shape, bias_data,\n                       output_offset, output_multiplier, output_shift,\n                       output_activation_min, output_activation_max,\n                       output_shape, output_data, row_start, row_end);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_rows);\n  gemmlowp_context->workers_pool()->Execute(tasks.size(), tasks.data());\n}\n#endif  // USE_NEON\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const int8* input_data, const RuntimeShape& filter_shape,\n    const int8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape, int8* output_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnectedInt8/8bit\");\n\n#ifdef USE_NEON\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  if (batches == 1) {\n    const int output_size = MatchingDim(filter_shape, filter_dim_count - 2,\n                                        output_shape, output_dim_count - 1);\n    if (output_size >= 4) {\n      return LegacyInt8FullyConnectedAsGEMV(\n          input_shape, input_data, input_offset, filter_shape, filter_data,\n          filter_offset, bias_shape, bias_data, output_offset,\n          output_multiplier, output_shift, output_activation_min,\n          output_activation_max, output_shape, output_data, gemmlowp_context);\n    }\n  }\n#endif  // USE_NEON\n\n#ifdef GEMMLOWP_NEON\n  const int filter_rows = filter_shape.Dims(filter_dim_count - 2);\n  const int filter_cols = filter_shape.Dims(filter_dim_count - 1);\n  TFLITE_DCHECK_EQ(filter_shape.FlatSize(), filter_rows * filter_cols);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n  gemmlowp::MatrixMap<const int8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, output_rows, filter_cols, filter_cols);\n  gemmlowp::MatrixMap<const int8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, filter_cols, batches, filter_cols);\n  gemmlowp::MatrixMap<int8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, batches, output_rows);\n  const auto& output_pipeline = GemmlowpOutputPipelineInt8::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, output_shift,\n      output_activation_min, output_activation_max);\n\n  gemmlowp::GemmWithOutputPipeline<\n      int8, int8, gemmlowp::SignedL8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n  return;\n#endif  // GEMMLOWP_NEON\n\n  // If both GEMMLOWP_NEON && NEON paths are skipped, fallback to reference\n  // implementation.\n  reference_integer_ops::FullyConnected(params, input_shape, input_data,\n                                        filter_shape, filter_data, bias_shape,\n                                        bias_data, output_shape, output_data);\n}\n\nstruct LegacyShuffledFullyConnectedWorkerTask : gemmlowp::Task {\n  LegacyShuffledFullyConnectedWorkerTask(const uint8* input_data,\n                                         const int8* shuffled_weights_data,\n                                         int batches, int output_depth,\n                                         int output_stride, int accum_depth,\n                                         const int32* bias_data,\n                                         int32 output_multiplier,\n                                         int output_shift, int16* output_data)\n      : input_data_(input_data),\n        shuffled_weights_data_(shuffled_weights_data),\n        batches_(batches),\n        output_depth_(output_depth),\n        output_stride_(output_stride),\n        accum_depth_(accum_depth),\n        bias_data_(bias_data),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_data_(output_data) {}\n\n  void Run() override {\n    ShuffledFullyConnectedWorkerImpl(\n        input_data_, shuffled_weights_data_, batches_, output_depth_,\n        output_stride_, accum_depth_, bias_data_, output_multiplier_,\n        output_shift_, output_data_);\n  }\n\n  const uint8* input_data_;\n  const int8* shuffled_weights_data_;\n  int batches_;\n  int output_depth_;\n  int output_stride_;\n  int accum_depth_;\n  const int32* bias_data_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int16* output_data_;\n};\n\ninline void ShuffledFullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& weights_shape,\n    const uint8* shuffled_weights_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"ShuffledFullyConnected/8bit\");\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  (void)gemmlowp_context;  // only used in optimized code.\n  TFLITE_DCHECK_EQ(output_activation_min, -32768);\n  TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(weights_shape, weights_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = weights_shape.Dims(weights_dim_count - 1);\n  TFLITE_DCHECK((accum_depth % 16) == 0);\n  TFLITE_DCHECK((output_depth % 4) == 0);\n  // Shuffled weights have had their sign bit (0x80) pre-flipped (xor'd)\n  // so that just reinterpreting them as int8 values is equivalent to\n  // subtracting 128 from them, thus implementing for free the subtraction of\n  // the zero_point value 128.\n  const int8* int8_shuffled_weights_data =\n      reinterpret_cast<const int8*>(shuffled_weights_data);\n\n  // Shuffling and xoring of input activations into the workspace buffer\n  if (batches == 1) {\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (int i = 0; i < accum_depth; i += 16) {\n      uint8x16_t val = vld1q_u8(input_data + i);\n      val = veorq_u8(val, signbit);\n      vst1q_u8(shuffled_input_workspace_data + i, val);\n    }\n#else\n    for (int i = 0; i < accum_depth; i++) {\n      shuffled_input_workspace_data[i] = input_data[i] ^ 0x80;\n    }\n#endif\n  } else if (batches == 4) {\n    uint8* shuffled_input_workspace_ptr = shuffled_input_workspace_data;\n    int c = 0;\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (c = 0; c < accum_depth; c += 16) {\n      const uint8* src_data_ptr = input_data + c;\n      uint8x16_t val0 = vld1q_u8(src_data_ptr + 0 * accum_depth);\n      uint8x16_t val1 = vld1q_u8(src_data_ptr + 1 * accum_depth);\n      uint8x16_t val2 = vld1q_u8(src_data_ptr + 2 * accum_depth);\n      uint8x16_t val3 = vld1q_u8(src_data_ptr + 3 * accum_depth);\n      val0 = veorq_u8(val0, signbit);\n      val1 = veorq_u8(val1, signbit);\n      val2 = veorq_u8(val2, signbit);\n      val3 = veorq_u8(val3, signbit);\n      vst1q_u8(shuffled_input_workspace_ptr + 0, val0);\n      vst1q_u8(shuffled_input_workspace_ptr + 16, val1);\n      vst1q_u8(shuffled_input_workspace_ptr + 32, val2);\n      vst1q_u8(shuffled_input_workspace_ptr + 48, val3);\n      shuffled_input_workspace_ptr += 64;\n    }\n#else\n    for (c = 0; c < accum_depth; c += 16) {\n      for (int b = 0; b < 4; b++) {\n        const uint8* src_data_ptr = input_data + b * accum_depth + c;\n        for (int j = 0; j < 16; j++) {\n          uint8 src_val = *src_data_ptr++;\n          // Flip the sign bit, so that the kernel will only need to\n          // reinterpret these uint8 values as int8, getting for free the\n          // subtraction of the zero_point value 128.\n          uint8 dst_val = src_val ^ 0x80;\n          *shuffled_input_workspace_ptr++ = dst_val;\n        }\n      }\n    }\n#endif\n  } else {\n    TFLITE_DCHECK(false);\n    return;\n  }\n\n  static constexpr int kKernelRows = 4;\n  const int thread_count = gemmlowp::HowManyThreads<kKernelRows>(\n      gemmlowp_context->max_num_threads(), output_depth, batches, accum_depth);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    ShuffledFullyConnectedWorkerImpl(\n        shuffled_input_workspace_data, int8_shuffled_weights_data, batches,\n        output_depth, output_depth, accum_depth, bias_data, output_multiplier,\n        output_shift, output_data);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<gemmlowp::Task*> tasks(thread_count);\n  const int kRowsPerWorker = gemmlowp::RoundUp<kKernelRows>(\n      gemmlowp::CeilQuotient(output_depth, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; i++) {\n    int row_end = std::min(output_depth, row_start + kRowsPerWorker);\n    tasks[i] = new LegacyShuffledFullyConnectedWorkerTask(\n        shuffled_input_workspace_data,\n        int8_shuffled_weights_data + row_start * accum_depth, batches,\n        row_end - row_start, output_depth, accum_depth, bias_data + row_start,\n        output_multiplier, output_shift, output_data + row_start);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_depth);\n  gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n}\n\ninline void ShuffledFullyConnected(\n    const uint8* input_data, const Dims<4>& input_dims,\n    const uint8* shuffled_weights_data, const Dims<4>& weights_dims,\n    const int32* bias_data, const Dims<4>& bias_dims, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    int16* output_data, const Dims<4>& output_dims,\n    uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  ShuffledFullyConnected(op_params, DimsToShape(input_dims), input_data,\n                         DimsToShape(weights_dims), shuffled_weights_data,\n                         DimsToShape(bias_dims), bias_data,\n                         DimsToShape(output_dims), output_data,\n                         shuffled_input_workspace_data, gemmlowp_context);\n}\n\ntemplate <typename T>\ninline void ExtractPatchIntoBufferColumn(\n    const Dims<4>& input_dims, int w, int h, int b, int kheight, int kwidth,\n    int stride_width, int stride_height, int pad_width, int pad_height,\n    int in_width, int in_height, int in_depth, int single_buffer_length,\n    int buffer_id, const T* in_data, T* conv_buffer_data, uint8 zero_byte) {\n  ExtractPatchIntoBufferColumn(\n      DimsToShape(input_dims), w, h, b, kheight, kwidth, stride_width,\n      stride_height, pad_width, pad_height, in_width, in_height, in_depth,\n      single_buffer_length, buffer_id, in_data, conv_buffer_data, zero_byte);\n}\n\ntemplate <typename T>\nvoid DilatedIm2col(const T* input_data, const Dims<4>& input_dims,\n                   const Dims<4>& filter_dims, int stride_width,\n                   int stride_height, int dilation_width_factor,\n                   int dilation_height_factor, int pad_width, int pad_height,\n                   const Dims<4>& output_dims, uint8 zero_byte,\n                   T* im2col_data) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n\n  DilatedIm2col(op_params, zero_byte, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), DimsToShape(output_dims),\n                im2col_data);\n}\n\ntemplate <typename T>\nvoid Im2col(const T* input_data, const Dims<4>& input_dims, int stride_width,\n            int stride_height, int pad_width, int pad_height, int kheight,\n            int kwidth, uint8 zero_byte, T* output_data,\n            const Dims<4>& output_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = 1;\n  op_params.dilation_height_factor = 1;\n\n  Im2col(op_params, kheight, kwidth, zero_byte, DimsToShape(input_dims),\n         input_data, DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <typename T>\nvoid Im2col(const T* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int kheight, int kwidth,\n            uint8 zero_byte, T* output_data, const Dims<4>& output_dims) {\n  Im2col(input_data, input_dims, stride, stride, pad_width, pad_height, kheight,\n         kwidth, zero_byte, output_data, output_dims);\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const float* input_data, const RuntimeShape& filter_shape,\n                 const float* filter_data, const RuntimeShape& bias_shape,\n                 const float* bias_data, const RuntimeShape& output_shape,\n                 float* output_data, const RuntimeShape& im2col_shape,\n                 float* im2col_data) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  (void)im2col_data;\n  (void)im2col_shape;\n  ruy::profiler::ScopeLabel label(\"Conv\");\n\n  // NB: the float 0.0f value is represented by all zero bytes.\n  const uint8 float_zero_byte = 0x00;\n  const float* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    DilatedIm2col(params, float_zero_byte, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    Im2col(params, filter_height, filter_width, float_zero_byte, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    // TODO(aselle): We need to make sure to not send im2col if it is not\n    // needed.\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  // The following code computes matrix multiplication c = a * transponse(b)\n  // with CBLAS, where:\n  // * `a` is a matrix with dimensions (m, k).\n  // * `b` is a matrix with dimensions (n, k), so transpose(b) is (k, n).\n  // * `c` is a matrix with dimensions (m, n).\n  // The naming of variables are aligned with CBLAS specification here.\n  const float* a = gemm_input_data;\n  const float* b = filter_data;\n  float* c = output_data;\n  const int gemm_input_dims = gemm_input_shape->DimensionsCount();\n  int m = FlatSizeSkipDim(*gemm_input_shape, gemm_input_dims - 1);\n  int n = output_shape.Dims(3);\n  int k = gemm_input_shape->Dims(gemm_input_dims - 1);\n\n#if defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n  // The stride of matrix a, b and c respectively.\n  int stride_a = k;\n  int stride_b = k;\n  int stride_c = n;\n\n  cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans, m, n, k, 1.0f, a,\n              stride_a, b, stride_b, 0.0f, c, stride_c);\n#else\n  // When an optimized CBLAS implementation is not available, fall back\n  // to using Eigen.\n  typedef Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>\n      Matrix;\n  typedef Eigen::Map<Matrix> MatrixRef;\n  typedef Eigen::Map<const Matrix> ConstMatrixRef;\n\n  MatrixRef matrix_c(c, m, n);\n  ConstMatrixRef matrix_a(a, m, k);\n  ConstMatrixRef matrix_b(b, n, k);\n\n  // The following special casing for when a or b is a vector is required\n  // as Eigen seem to fail to make this optimization on its own.\n  if (n == 1) {\n    ruy::profiler::ScopeLabel label(\"GEMV\");\n    matrix_c.col(0).noalias() = matrix_a * matrix_b.row(0).transpose();\n  } else if (m == 1) {\n    ruy::profiler::ScopeLabel label(\"GEMV\");\n    matrix_c.row(0).noalias() = matrix_a.row(0) * matrix_b.transpose();\n  } else {\n    ruy::profiler::ScopeLabel label(\"GEMM\");\n    matrix_c.noalias() = matrix_a * matrix_b.transpose();\n  }\n\n#endif  //  defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n\n  optimized_ops::AddBiasAndEvalActivationFunction(\n      output_activation_min, output_activation_max, bias_shape, bias_data,\n      output_shape, output_data);\n}\n\ninline void Conv(const float* input_data, const Dims<4>& input_dims,\n                 const float* filter_data, const Dims<4>& filter_dims,\n                 const float* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 float output_activation_min, float output_activation_max,\n                 float* output_data, const Dims<4>& output_dims,\n                 float* im2col_data, const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data);\n}\n\ninline void HybridConv(const int8_t* input_data, const Dims<4>& input_dims,\n                       const int8_t* filter_data, const Dims<4>& filter_dims,\n                       const float* bias_data, const Dims<4>& bias_dims,\n                       int stride_width, int stride_height, int pad_width,\n                       int pad_height, float* scaling_factors_ptr,\n                       float output_activation_min, float output_activation_max,\n                       int32_t* scratch_data, const Dims<4>& scratch_dims,\n                       float* output_data, const Dims<4>& output_dims,\n                       int8_t* im2col_data, const Dims<4>& im2col_dims,\n                       CpuBackendContext* context) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  HybridConv(op_params, scaling_factors_ptr, DimsToShape(input_dims),\n             input_data, DimsToShape(filter_dims), filter_data,\n             DimsToShape(bias_dims), bias_data, DimsToShape(scratch_dims),\n             scratch_data, DimsToShape(output_dims), output_data,\n             DimsToShape(im2col_dims), im2col_data, context);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int dilation_width_factor,\n          int dilation_height_factor, int pad_width, int pad_height,\n          float* output_data, const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, dilation_width_factor,\n       dilation_height_factor, pad_width, pad_height, output_activation_min,\n       output_activation_max, output_data, output_dims, im2col_data,\n       im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, 1, 1, pad_width, pad_height,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  Conv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n           bias_dims, stride, stride, 1, 1, pad_width, pad_height, output_data,\n           output_dims, im2col_data, im2col_dims);\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const uint8* input_data, const RuntimeShape& filter_shape,\n                 const uint8* filter_data, const RuntimeShape& bias_shape,\n                 const int32* bias_data, const RuntimeShape& output_shape,\n                 uint8* output_data, const RuntimeShape& im2col_shape,\n                 uint8* im2col_data, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"Conv/8bit\");\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const uint8* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    DilatedIm2col(params, input_zero_point, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    Im2col(params, filter_height, filter_width, input_zero_point, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  const int gemm_input_rows = gemm_input_shape->Dims(3);\n  // Using FlatSizeSkipDim causes segfault in some contexts (see b/79927784).\n  // The root cause has not yet been identified though. Same applies below for\n  // the other calls commented out. This is a partial rollback of cl/196819423.\n  // const int gemm_input_cols = FlatSizeSkipDim(*gemm_input_shape, 3);\n  const int gemm_input_cols = gemm_input_shape->Dims(0) *\n                              gemm_input_shape->Dims(1) *\n                              gemm_input_shape->Dims(2);\n  const int filter_rows = filter_shape.Dims(0);\n  // See b/79927784.\n  // const int filter_cols = FlatSizeSkipDim(filter_shape, 0);\n  const int filter_cols =\n      filter_shape.Dims(1) * filter_shape.Dims(2) * filter_shape.Dims(3);\n  const int output_rows = output_shape.Dims(3);\n  // See b/79927784.\n  // const int output_cols = FlatSizeSkipDim(output_shape, 3);\n  const int output_cols =\n      output_shape.Dims(0) * output_shape.Dims(1) * output_shape.Dims(2);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, gemm_input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, gemm_input_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n#ifdef USE_NEON\n  if (gemm_input_cols == 1 && output_rows >= 4) {\n    RuntimeShape fc_filter_shape{\n        filter_shape.Dims(0),\n        filter_shape.Dims(filter_shape.DimensionsCount() - 1)};\n\n    return FullyConnectedAsGEMV(\n        *gemm_input_shape, gemm_input_data, input_offset, fc_filter_shape,\n        filter_data, filter_offset, bias_shape, bias_data, output_offset,\n        output_multiplier, output_shift, output_activation_min,\n        output_activation_max, output_shape, output_data, gemmlowp_context);\n  }\n#endif\n\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, filter_rows, filter_cols);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      gemm_input_data, gemm_input_rows, gemm_input_cols);\n  gemmlowp::MatrixMap<uint8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, output_cols);\n  const auto& output_pipeline = GemmlowpOutputPipeline::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, output_shift,\n      output_activation_min, output_activation_max);\n  gemmlowp::GemmWithOutputPipeline<uint8, uint8,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 int32 output_offset, int32 output_multiplier, int output_shift,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims,\n                 uint8* im2col_data, const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data, gemmlowp_context);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height, 1, 1,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const uint8* input_data, const Dims<4>& input_dims,\n          int32 input_offset, const uint8* filter_data,\n          const Dims<4>& filter_dims, int32 filter_offset,\n          const int32* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, int32 output_offset,\n          int32 output_multiplier, int output_shift,\n          int32 output_activation_min, int32 output_activation_max,\n          uint8* output_data, const Dims<4>& output_dims, uint8* im2col_data,\n          const Dims<4>& im2col_dims, gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride, stride, pad_width,\n       pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid Im2col(const T* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int kheight, int kwidth,\n            uint8 zero_byte, T* output_data, const Dims<4>& output_dims) {\n  Im2col(input_data, input_dims, stride, stride, pad_width, pad_height, kheight,\n         kwidth, zero_byte, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid ConvAsGemm(const float* input_data, const Dims<4>& input_dims,\n                const float* filter_data, const Dims<4>& filter_dims,\n                const float* bias_data, const Dims<4>& bias_dims,\n                float* output_data, const Dims<4>& output_dims) {\n  ruy::profiler::ScopeLabel label(\"ConvAsGemm\");\n\n  const auto input_matrix_map =\n      MapAsMatrixWithFirstDimAsRows(input_data, input_dims);\n  const auto filter_matrix_map =\n      MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);\n  auto output_matrix_map =\n      MapAsMatrixWithFirstDimAsRows(output_data, output_dims);\n\n  Gemm(filter_matrix_map.transpose(), input_matrix_map, &output_matrix_map);\n\n  AddBiasAndEvalActivationFunction<Ac>(bias_data, bias_dims, output_data,\n                                       output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid ConvAsGemm(const uint8* input_data, const Dims<4>& input_dims,\n                int32 input_offset, const uint8* filter_data,\n                const Dims<4>& filter_dims, int32 filter_offset,\n                const int32* bias_data, const Dims<4>& bias_dims,\n                int32 output_offset, int32 output_multiplier, int output_shift,\n                int32 output_activation_min, int32 output_activation_max,\n                uint8* output_data, const Dims<4>& output_dims,\n                gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"ConvAsGemm/8bit\");\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  const int input_rows = input_dims.sizes[0];\n  const int input_cols = FlatSizeSkipDim(input_dims, 0);\n  const int filter_rows = filter_dims.sizes[3];\n  const int filter_cols = FlatSizeSkipDim(filter_dims, 3);\n  const int output_rows = output_dims.sizes[0];\n  const int output_cols = FlatSizeSkipDim(output_dims, 0);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, input_rows);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[0], output_rows);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[1], 1);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[2], 1);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[3], 1);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, output_rows, filter_cols, filter_cols);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, filter_cols, output_cols, filter_cols);\n  gemmlowp::MatrixMap<uint8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, output_cols, output_rows);\n  const auto& output_pipeline = GemmlowpOutputPipeline::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, -output_shift,\n      output_activation_min, output_activation_max);\n  gemmlowp::GemmWithOutputPipeline<uint8, uint8,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\ninline void TransposeConv(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& output_shape,\n    float* output_data, const RuntimeShape& im2col_shape, float* im2col_data) {\n  ruy::profiler::ScopeLabel label(\"TransposeConv\");\n  // Note we could use transposed weights with forward conv for unstrided\n  // cases. But we are already getting good performance with this code as-is.\n  TFLITE_DCHECK(im2col_data);\n  TransposeIm2col(params, 0, input_shape, input_data, filter_shape,\n                  output_shape, im2col_data);\n\n  const auto im2col_matrix_map =\n      MapAsMatrixWithLastDimAsRows(im2col_data, im2col_shape);\n  const auto filter_matrix_map =\n      MapAsMatrixWithFirstDimAsCols(filter_data, filter_shape);\n  auto output_matrix_map =\n      MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n\n  Gemm(filter_matrix_map.transpose(), im2col_matrix_map, &output_matrix_map);\n}\n\ninline void TransposeConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, float* output_data,\n                          const Dims<4>& output_dims, float* im2col_data,\n                          const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n\n  TransposeConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data, DimsToShape(output_dims),\n                output_data, DimsToShape(im2col_dims), im2col_data);\n}\n\ninline void TransposeConvV2(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& hwoi_ordered_filter_shape,\n    const float* hwoi_ordered_filter_data, const RuntimeShape& output_shape,\n    float* output_data, const RuntimeShape& col2im_shape, float* col2im_data,\n    CpuBackendContext* cpu_backend_context) {\n  TransposeConvV2(params, input_shape, input_data, hwoi_ordered_filter_shape,\n                  hwoi_ordered_filter_data, /*bias_shape*/ RuntimeShape(),\n                  /*bias_data*/ nullptr, output_shape, output_data,\n                  col2im_shape, col2im_data, cpu_backend_context);\n}\n\ntemplate <typename T>\nvoid TransposeIm2col(const T* input_data, const Dims<4>& input_dims,\n                     const Dims<4>& filter_dims, int stride_width,\n                     int stride_height, int pad_width, int pad_height,\n                     const Dims<4>& output_dims, uint8 zero_byte,\n                     T* im2col_data) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n\n  TransposeIm2col(op_params, zero_byte, DimsToShape(input_dims), input_data,\n                  DimsToShape(filter_dims), DimsToShape(output_dims),\n                  im2col_data);\n}\n\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const float* input_data, const RuntimeShape& unextended_prev_activ_shape,\n    const float* prev_activ_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& unextended_bias_shape,\n    const float* bias_data, const RuntimeShape& unextended_prev_state_shape,\n    const float* prev_state_data,\n    const RuntimeShape& unextended_output_state_shape, float* output_state_data,\n    const RuntimeShape& unextended_output_activ_shape, float* output_activ_data,\n    const RuntimeShape& unextended_concat_temp_shape, float* concat_temp_data,\n    const RuntimeShape& unextended_activ_temp_shape, float* activ_temp_data) {\n  ruy::profiler::ScopeLabel label(\"LstmCell\");\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  MatchingDim(  // batches\n      input_shape, 0, prev_activ_shape, 0, prev_state_shape, 0,\n      output_state_shape, 0, output_activ_shape, 0);\n  MatchingDim(  // height\n      input_shape, 1, prev_activ_shape, 1, prev_state_shape, 1,\n      output_state_shape, 1, output_activ_shape, 1);\n  MatchingDim(  // width\n      input_shape, 2, prev_activ_shape, 2, prev_state_shape, 2,\n      output_state_shape, 2, output_activ_shape, 2);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n\n  // Concatenate prev_activ and input data together\n  std::vector<float const*> concat_input_arrays_data;\n  std::vector<RuntimeShape const*> concat_input_arrays_shapes;\n  concat_input_arrays_data.push_back(input_data);\n  concat_input_arrays_data.push_back(prev_activ_data);\n  concat_input_arrays_shapes.push_back(&input_shape);\n  concat_input_arrays_shapes.push_back(&prev_activ_shape);\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = concat_input_arrays_data.size();\n  Concatenation(concat_params, &(concat_input_arrays_shapes[0]),\n                &(concat_input_arrays_data[0]), concat_temp_shape,\n                concat_temp_data);\n\n  // Fully connected\n  tflite::FullyConnectedParams fc_params;\n  fc_params.float_activation_min = std::numeric_limits<float>::lowest();\n  fc_params.float_activation_max = std::numeric_limits<float>::max();\n  FullyConnected(fc_params, concat_temp_shape, concat_temp_data, weights_shape,\n                 weights_data, bias_shape, bias_data, activ_temp_shape,\n                 activ_temp_data);\n\n  // Map raw arrays to Eigen arrays so we can use Eigen's optimized array\n  // operations.\n  ArrayMap<float> activ_temp_map =\n      MapAsArrayWithLastDimAsRows(activ_temp_data, activ_temp_shape);\n  auto input_gate_sm = activ_temp_map.block(0 * output_depth, 0, output_depth,\n                                            activ_temp_map.cols());\n  auto new_input_sm = activ_temp_map.block(1 * output_depth, 0, output_depth,\n                                           activ_temp_map.cols());\n  auto forget_gate_sm = activ_temp_map.block(2 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  auto output_gate_sm = activ_temp_map.block(3 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  ArrayMap<const float> prev_state_map =\n      MapAsArrayWithLastDimAsRows(prev_state_data, prev_state_shape);\n  ArrayMap<float> output_state_map =\n      MapAsArrayWithLastDimAsRows(output_state_data, output_state_shape);\n  ArrayMap<float> output_activ_map =\n      MapAsArrayWithLastDimAsRows(output_activ_data, output_activ_shape);\n\n  // Combined memory state and final output calculation\n  ruy::profiler::ScopeLabel label2(\"MemoryStateAndFinalOutput\");\n  output_state_map =\n      input_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          new_input_sm.tanh() +\n      forget_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          prev_state_map;\n  output_activ_map =\n      output_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n      output_state_map.tanh();\n}\n\ninline void LstmCell(const float* input_data, const Dims<4>& input_dims,\n                     const float* prev_activ_data,\n                     const Dims<4>& prev_activ_dims, const float* weights_data,\n                     const Dims<4>& weights_dims, const float* bias_data,\n                     const Dims<4>& bias_dims, const float* prev_state_data,\n                     const Dims<4>& prev_state_dims, float* output_state_data,\n                     const Dims<4>& output_state_dims, float* output_activ_data,\n                     const Dims<4>& output_activ_dims, float* concat_temp_data,\n                     const Dims<4>& concat_temp_dims, float* activ_temp_data,\n                     const Dims<4>& activ_temp_dims) {\n  tflite::LstmCellParams op_params;\n  // Float LSTM cell does not need parameters to be set: leave untouched.\n\n  LstmCell(op_params, DimsToShape(input_dims), input_data,\n           DimsToShape(prev_activ_dims), prev_activ_data,\n           DimsToShape(weights_dims), weights_data, DimsToShape(bias_dims),\n           bias_data, DimsToShape(prev_state_dims), prev_state_data,\n           DimsToShape(output_state_dims), output_state_data,\n           DimsToShape(output_activ_dims), output_activ_data,\n           DimsToShape(concat_temp_dims), concat_temp_data,\n           DimsToShape(activ_temp_dims), activ_temp_data);\n}\n\ntemplate <int StateIntegerBits>\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const uint8* input_data_uint8,\n    const RuntimeShape& unextended_prev_activ_shape,\n    const uint8* prev_activ_data_uint8, const RuntimeShape& weights_shape,\n    const uint8* weights_data_uint8, const RuntimeShape& unextended_bias_shape,\n    const int32* bias_data_int32,\n    const RuntimeShape& unextended_prev_state_shape,\n    const int16* prev_state_data_int16,\n    const RuntimeShape& unextended_output_state_shape,\n    int16* output_state_data_int16,\n    const RuntimeShape& unextended_output_activ_shape,\n    uint8* output_activ_data_uint8,\n    const RuntimeShape& unextended_concat_temp_shape,\n    uint8* concat_temp_data_uint8,\n    const RuntimeShape& unextended_activ_temp_shape,\n    int16* activ_temp_data_int16, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\n      \"LstmCell/quantized (8bit external, 16bit internal)\");\n  int32 weights_zero_point = params.weights_zero_point;\n  int32 accum_multiplier = params.accum_multiplier;\n  int accum_shift = params.accum_shift;\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  // Gather dimensions information, and perform consistency checks.\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int outer_size = MatchingFlatSizeSkipDim(\n      input_shape, 3, prev_activ_shape, prev_state_shape, output_state_shape,\n      output_activ_shape);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n  const int fc_batches = FlatSizeSkipDim(activ_temp_shape, 3);\n  const int fc_output_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, activ_temp_shape, 3);\n  const int fc_accum_depth = total_input_depth;\n  TFLITE_DCHECK_EQ(fc_output_depth, 4 * output_depth);\n\n  // Depth-concatenate prev_activ and input data together.\n  uint8 const* concat_input_arrays_data[2] = {input_data_uint8,\n                                              prev_activ_data_uint8};\n  const RuntimeShape* concat_input_arrays_shapes[2] = {&input_shape,\n                                                       &prev_activ_shape};\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = 2;\n  Concatenation(concat_params, concat_input_arrays_shapes,\n                concat_input_arrays_data, concat_temp_shape,\n                concat_temp_data_uint8);\n\n  // Implementation of the fully connected node inside the LSTM cell.\n  // The operands are 8-bit integers, the accumulators are internally 32bit\n  // integers, and the output is 16-bit fixed-point with 3 integer bits so\n  // the output range is [-2^3, 2^3] == [-8, 8]. The rationale for that\n  // is explained in the function comment above.\n  bool gemm_already_performed = false;\n#ifdef GEMMLOWP_NEON\n  if (fc_batches == 1 && !(fc_output_depth % 4) && !(fc_accum_depth % 8)) {\n    GEMVForLstmCell(concat_temp_shape, concat_temp_data_uint8, weights_shape,\n                    weights_data_uint8, weights_zero_point, bias_shape,\n                    bias_data_int32, accum_multiplier, accum_shift,\n                    activ_temp_shape, activ_temp_data_int16);\n    gemm_already_performed = true;\n  }\n#endif\n  if (!gemm_already_performed) {\n    gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor>\n        weights_matrix(weights_data_uint8, fc_output_depth, fc_accum_depth);\n    gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n        concat_temp_data_uint8, fc_accum_depth, fc_batches);\n    gemmlowp::MatrixMap<int16, gemmlowp::MapOrder::ColMajor> output_matrix(\n        activ_temp_data_int16, fc_output_depth, fc_batches);\n    typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n        ColVectorMap;\n    ColVectorMap bias_vector(bias_data_int32, fc_output_depth);\n    gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n    bias_addition_stage.bias_vector = bias_vector;\n    gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent scale_stage;\n    scale_stage.result_offset_after_shift = 0;\n    scale_stage.result_fixedpoint_multiplier = accum_multiplier;\n    scale_stage.result_exponent = accum_shift;\n    gemmlowp::OutputStageSaturatingCastToInt16 saturating_cast_int16_stage;\n    auto output_pipeline = std::make_tuple(bias_addition_stage, scale_stage,\n                                           saturating_cast_int16_stage);\n    gemmlowp::GemmWithOutputPipeline<\n        uint8, int16, gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n        gemmlowp_context, weights_matrix, input_matrix, &output_matrix,\n        -weights_zero_point, -128, output_pipeline);\n  }\n\n  // Rest of the LSTM cell: tanh and logistic math functions, and some adds\n  // and muls, all done in 16-bit fixed-point.\n  const int16* input_gate_input_ptr = activ_temp_data_int16;\n  const int16* input_modulation_gate_input_ptr =\n      activ_temp_data_int16 + output_depth;\n  const int16* forget_gate_input_ptr = activ_temp_data_int16 + 2 * output_depth;\n  const int16* output_gate_input_ptr = activ_temp_data_int16 + 3 * output_depth;\n  const int16* prev_state_ptr = prev_state_data_int16;\n  int16* output_state_data_ptr = output_state_data_int16;\n  uint8* output_activ_data_ptr = output_activ_data_uint8;\n\n  for (int b = 0; b < outer_size; ++b) {\n    int c = 0;\n#ifdef GEMMLOWP_NEON\n    for (; c <= output_depth - 8; c += 8) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<int16x8_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<int16x8_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(vld1q_s16(input_gate_input_ptr));\n      input_gate_input_ptr += 8;\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(vld1q_s16(input_modulation_gate_input_ptr));\n      input_modulation_gate_input_ptr += 8;\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(vld1q_s16(forget_gate_input_ptr));\n      forget_gate_input_ptr += 8;\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(vld1q_s16(output_gate_input_ptr));\n      output_gate_input_ptr += 8;\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(vld1q_s16(prev_state_ptr));\n      prev_state_ptr += 8;\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      vst1q_s16(output_state_data_ptr, new_state.raw());\n      output_state_data_ptr += 8;\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16x8_t rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int8x8_t int8_output_activ = vqmovn_s16(rescaled_output_activ);\n      uint8x8_t uint8_output_activ =\n          vadd_u8(vdup_n_u8(128), vreinterpret_u8_s8(int8_output_activ));\n      vst1_u8(output_activ_data_ptr, uint8_output_activ);\n      output_activ_data_ptr += 8;\n    }\n#endif\n    for (; c < output_depth; ++c) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<std::int16_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<std::int16_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(*input_gate_input_ptr++);\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(*input_modulation_gate_input_ptr++);\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(*forget_gate_input_ptr++);\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(*output_gate_input_ptr++);\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(*prev_state_ptr++);\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      *output_state_data_ptr++ = new_state.raw();\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16 rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int16 clamped_output_activ =\n          std::max<int16>(-128, std::min<int16>(127, rescaled_output_activ));\n      *output_activ_data_ptr++ = 128 + clamped_output_activ;\n    }\n    input_gate_input_ptr += 3 * output_depth;\n    input_modulation_gate_input_ptr += 3 * output_depth;\n    forget_gate_input_ptr += 3 * output_depth;\n    output_gate_input_ptr += 3 * output_depth;\n  }\n}\n\ntemplate <int StateIntegerBits>\nvoid LstmCell(const uint8* input_data_uint8, const Dims<4>& input_dims,\n              const uint8* prev_activ_data_uint8,\n              const Dims<4>& prev_activ_dims, const uint8* weights_data_uint8,\n              const Dims<4>& weights_dims, const int32* bias_data_int32,\n              const Dims<4>& bias_dims, const int16* prev_state_data_int16,\n              const Dims<4>& prev_state_dims, int16* output_state_data_int16,\n              const Dims<4>& output_state_dims, uint8* output_activ_data_uint8,\n              const Dims<4>& output_activ_dims, uint8* concat_temp_data_uint8,\n              const Dims<4>& concat_temp_dims, int16* activ_temp_data_int16,\n              const Dims<4>& activ_temp_dims, int32 weights_zero_point,\n              int32 accum_multiplier, int accum_shift,\n              gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::LstmCellParams op_params;\n  op_params.weights_zero_point = weights_zero_point;\n  op_params.accum_multiplier = accum_multiplier;\n  op_params.accum_shift = accum_shift;\n\n  LstmCell<StateIntegerBits>(\n      op_params, DimsToShape(input_dims), input_data_uint8,\n      DimsToShape(prev_activ_dims), prev_activ_data_uint8,\n      DimsToShape(weights_dims), weights_data_uint8, DimsToShape(bias_dims),\n      bias_data_int32, DimsToShape(prev_state_dims), prev_state_data_int16,\n      DimsToShape(output_state_dims), output_state_data_int16,\n      DimsToShape(output_activ_dims), output_activ_data_uint8,\n      DimsToShape(concat_temp_dims), concat_temp_data_uint8,\n      DimsToShape(activ_temp_dims), activ_temp_data_int16, gemmlowp_context);\n}\n\ntemplate <typename T>\nvoid BroadcastDiv(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastDivSlow(op_params, DimsToShape(input1_dims), input1_data,\n                   DimsToShape(input2_dims), input2_data,\n                   DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const RuntimeShape& input_shape,\n                     float* output_data, const RuntimeShape& output_shape) {\n  static_assert(Ac == FusedActivationFunctionType::kNone, \"\");\n  tflite::L2NormalizationParams op_params;\n  // No params need to be set for float, but reserved in signature for future\n  // activations.\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ninline void L2Normalization(const uint8* input_data,\n                            const RuntimeShape& input_shape,\n                            int32 input_zero_point, uint8* output_data,\n                            const RuntimeShape& output_shape) {\n  tflite::L2NormalizationParams op_params;\n  op_params.input_zero_point = input_zero_point;\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  L2Normalization<Ac>(input_data, DimsToShape(input_dims), output_data,\n                      DimsToShape(output_dims));\n}\n\ninline void L2Normalization(const uint8* input_data, const Dims<4>& input_dims,\n                            int32 input_zero_point, uint8* output_data,\n                            const Dims<4>& output_dims) {\n  L2Normalization(input_data, DimsToShape(input_dims), input_zero_point,\n                  output_data, DimsToShape(output_dims));\n}\n\ninline void Relu(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Relu(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(int left_shift, const uint8* input1_data,\n                const Dims<4>& input1_dims, int32 input1_offset,\n                int32 input1_multiplier, int input1_shift,\n                const uint8* input2_data, const Dims<4>& input2_dims,\n                int32 input2_offset, int32 input2_multiplier, int input2_shift,\n                int32 output_offset, int32 output_multiplier, int output_shift,\n                int32 output_activation_min, int32 output_activation_max,\n                uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const int32* input1_data, const Dims<4>& input1_dims,\n         const int32* input2_data, const Dims<4>& input2_dims,\n         int32* output_data, const Dims<4>& output_dims) {\n  ruy::profiler::ScopeLabel label(\"Add/int32\");\n  TFLITE_DCHECK(Ac == FusedActivationFunctionType::kNone);\n\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = std::numeric_limits<int32>::min();\n  op_params.quantized_activation_max = std::numeric_limits<int32>::max();\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAdd(int left_shift, const uint8* input1_data,\n                         const Dims<4>& input1_dims, int32 input1_offset,\n                         int32 input1_multiplier, int input1_shift,\n                         const uint8* input2_data, const Dims<4>& input2_dims,\n                         int32 input2_offset, int32 input2_multiplier,\n                         int input2_shift, int32 output_offset,\n                         int32 output_multiplier, int output_shift,\n                         int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAddFivefold(\n    int y0, int y1, int y2, int y3, int y4, int left_shift,\n    const uint8* input1_data, const Dims<4>& input1_dims, int32 input1_offset,\n    int32 input1_multiplier, int input1_shift, const uint8* input2_data,\n    const Dims<4>& input2_dims, int32 input2_offset, int32 input2_multiplier,\n    int input2_shift, int32 output_offset, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  tflite::ArithmeticParams op_params;\n  op_params.broadcast_category =\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.broadcast_shape[4] = y0;\n  op_params.broadcast_shape[3] = y1;\n  op_params.broadcast_shape[2] = y2;\n  op_params.broadcast_shape[1] = y3;\n  op_params.broadcast_shape[0] = y4;\n  BroadcastAddFivefold(op_params, DimsToShape(input1_dims), input1_data,\n                       DimsToShape(input2_dims), input2_data,\n                       DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T* output_data, const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  BroadcastAdd(input1_data, input1_dims, input2_data, input2_dims,\n               output_activation_min, output_activation_max, output_data,\n               output_dims);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(const int16* input1_data, const Dims<4>& input1_dims,\n                int input1_shift, const int16* input2_data,\n                const Dims<4>& input2_dims, int input2_shift,\n                int16 output_activation_min, int16 output_activation_max,\n                int16* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, -32768);\n    TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Sub(const float* input1_data, const Dims<4>& input1_dims,\n                const float* input2_data, const Dims<4>& input2_dims,\n                float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(FusedActivationFunctionType::kNone,\n                      &output_activation_min, &output_activation_max);\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid Sub(const T* input1_data, const Dims<4>& input1_dims, const T* input2_data,\n         const Dims<4>& input2_dims, T* output_data,\n         const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(FusedActivationFunctionType::kNone,\n                      &output_activation_min, &output_activation_max);\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n  op_params.input1_offset = input1_offset;\n  op_params.input2_offset = input2_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  BroadcastMul(input1_data, input1_dims, input1_offset, input2_data,\n               input2_dims, input2_offset, output_offset, output_multiplier,\n               output_shift, output_activation_min, output_activation_max,\n               output_data, output_dims);\n}\n\ninline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int kwidth, int kheight,\n                        float output_activation_min,\n                        float output_activation_max, float* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  AveragePool(params, DimsToShape(input_dims), input_data,\n              DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const float* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int kwidth, int kheight, float* output_data,\n                 const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n              pad_height, kwidth, kheight, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, float* output_data,\n                 const Dims<4>& output_dims) {\n  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n                  filter_width, filter_height, output_data, output_dims);\n}\n\ninline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int filter_width, int filter_height,\n                        int32 output_activation_min,\n                        int32 output_activation_max, uint8* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  AveragePool(params, DimsToShape(input_dims), input_data,\n              DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int filter_width, int filter_height,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n              pad_height, filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims) {\n  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n                  filter_width, filter_height, output_activation_min,\n                  output_activation_max, output_data, output_dims);\n}\n\ninline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int kwidth, int kheight,\n                    float output_activation_min, float output_activation_max,\n                    float* output_data, const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int kwidth, int kheight, float* output_data,\n             const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, kwidth, kheight, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             float* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_data, output_dims);\n}\n\ninline void MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int filter_width, int filter_height,\n                    int32 output_activation_min, int32 output_activation_max,\n                    uint8* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int filter_width, int filter_height, int32 output_activation_min,\n             int32 output_activation_max, uint8* output_data,\n             const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, filter_width, filter_height, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             int32 output_activation_min, int32 output_activation_max,\n             uint8* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\ninline void L2Pool(const float* input_data, const Dims<4>& input_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int filter_width, int filter_height,\n                   float output_activation_min, float output_activation_max,\n                   float* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  L2Pool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n         output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims,\n            int stride_width, int stride_height, int pad_width, int pad_height,\n            int filter_width, int filter_height, float* output_data,\n            const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  L2Pool(input_data, input_dims, stride_width, stride_height, pad_width,\n         pad_height, filter_width, filter_height, output_activation_min,\n         output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int filter_width, int filter_height,\n            float* output_data, const Dims<4>& output_dims) {\n  L2Pool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n             filter_width, filter_height, output_data, output_dims);\n}\n\ninline void Softmax(const SoftmaxParams& params,\n                    const RuntimeShape& input_shape, const uint8* input_data,\n                    const RuntimeShape& output_shape, uint8* output_data) {\n  const int32 input_beta_multiplier = params.input_multiplier;\n  const int32 input_beta_left_shift = params.input_left_shift;\n  const int diff_min = params.diff_min;\n  // The representation chosen for the input to the exp() function is Q5.26.\n  // We need to leave extra space since values that we skip might be as large as\n  // -32 before multiplying by input_beta_multiplier, and therefore as large as\n  // -16 afterwards.  Note that exp(-8) is definitely not insignificant to\n  // accumulation, but exp(-16) definitely is.\n  static const int kScaledDiffIntegerBits = 5;\n  static const int kAccumulationIntegerBits = 12;\n  using FixedPointScaledDiff =\n      gemmlowp::FixedPoint<int32, kScaledDiffIntegerBits>;\n  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;\n  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n\n  ruy::profiler::ScopeLabel label(\"Softmax/8bit\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  for (int b = 0; b < outer_size; ++b) {\n    const uint8* input_data_ptr = input_data + b * depth;\n    uint8* output_data_ptr = output_data + b * depth;\n\n    // Determine the largest entry in the current row\n    uint8 max_in_row = 0;\n    {\n      int c = 0;\n#ifdef USE_NEON\n      uint8x16_t max16_0 = vdupq_n_u8(0);\n      uint8x16_t max16_1 = vdupq_n_u8(0);\n      for (; c <= depth - 32; c += 32) {\n        max16_0 = vmaxq_u8(max16_0, vld1q_u8(input_data_ptr + c + 0));\n        max16_1 = vmaxq_u8(max16_1, vld1q_u8(input_data_ptr + c + 16));\n      }\n      uint8x16_t max16 = vmaxq_u8(max16_0, max16_1);\n      if (c <= depth - 16) {\n        max16 = vmaxq_u8(max16, vld1q_u8(input_data_ptr + c));\n        c += 16;\n      }\n      uint8x8_t max8 = vmax_u8(vget_low_u8(max16), vget_high_u8(max16));\n      if (c <= depth - 8) {\n        max8 = vmax_u8(max8, vld1_u8(input_data_ptr + c));\n        c += 8;\n      }\n      uint8x8_t max4 = vmax_u8(max8, vext_u8(max8, max8, 4));\n      uint8x8_t max2 = vmax_u8(max4, vext_u8(max4, max4, 2));\n      uint8x8_t max1 = vpmax_u8(max2, max2);\n      max_in_row = vget_lane_u8(max1, 0);\n#endif\n      for (; c < depth; ++c) {\n        max_in_row = std::max(max_in_row, input_data_ptr[c]);\n      }\n    }\n\n#ifdef USE_NEON\n    using FixedPointAccumInt32x4 =\n        gemmlowp::FixedPoint<int32x4_t, kAccumulationIntegerBits>;\n    using FixedPointScaledDiffInt32x4 =\n        gemmlowp::FixedPoint<int32x4_t, kScaledDiffIntegerBits>;\n    using FixedPoint0Int32x4 = gemmlowp::FixedPoint<int32x4_t, 0>;\n    FixedPoint0Int32x4 input_beta_multiplier_f0 =\n        FixedPoint0Int32x4::FromScalarRaw(input_beta_multiplier);\n    int16x8_t max_in_row_s16 = vdupq_n_s16(max_in_row);\n#endif\n\n    // Compute the sum of exponentials of the differences of entries in the\n    // current row from the largest entry in the current row.\n    FixedPointAccum sum_of_exps = FixedPointAccum::Zero();\n    {\n      int c = 0;\n#ifdef USE_NEON\n      int32x4_t diff_min_s32 = vdupq_n_s32(diff_min);\n      FixedPointAccumInt32x4 sum_of_exps_0 = FixedPointAccumInt32x4::Zero();\n      FixedPointAccumInt32x4 sum_of_exps_1 = FixedPointAccumInt32x4::Zero();\n      FixedPointAccumInt32x4 zeros = FixedPointAccumInt32x4::Zero();\n      for (; c <= depth - 8; c += 8) {\n        uint16x8_t input_u16 = vmovl_u8(vld1_u8(input_data_ptr + c));\n        int16x8_t input_diff_s16 =\n            vsubq_s16(vreinterpretq_s16_u16(input_u16), max_in_row_s16);\n        int32x4_t input_diff_s32_0 = vmovl_s16(vget_low_s16(input_diff_s16));\n        int32x4_t input_diff_s32_1 = vmovl_s16(vget_high_s16(input_diff_s16));\n        int32x4_t mask_0 =\n            gemmlowp::MaskIfGreaterThanOrEqual(input_diff_s32_0, diff_min_s32);\n        int32x4_t mask_1 =\n            gemmlowp::MaskIfGreaterThanOrEqual(input_diff_s32_1, diff_min_s32);\n        FixedPointScaledDiffInt32x4 scaled_diff_0 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_0, input_beta_left_shift));\n        FixedPointScaledDiffInt32x4 scaled_diff_1 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_1, input_beta_left_shift));\n        FixedPointAccumInt32x4 exps_0 =\n            gemmlowp::Rescale<kAccumulationIntegerBits>(\n                exp_on_negative_values(scaled_diff_0));\n        FixedPointAccumInt32x4 exps_1 =\n            gemmlowp::Rescale<kAccumulationIntegerBits>(\n                exp_on_negative_values(scaled_diff_1));\n        FixedPointAccumInt32x4 masked_exps_0 =\n            SelectUsingMask(mask_0, exps_0, zeros);\n        FixedPointAccumInt32x4 masked_exps_1 =\n            SelectUsingMask(mask_1, exps_1, zeros);\n        sum_of_exps_0 = sum_of_exps_0 + masked_exps_0;\n        sum_of_exps_1 = sum_of_exps_1 + masked_exps_1;\n      }\n      int32x4_t sum_of_exps_reduced_4 = (sum_of_exps_0 + sum_of_exps_1).raw();\n      int32x2_t sum_of_exps_reduced_2 =\n          vadd_s32(vget_low_s32(sum_of_exps_reduced_4),\n                   vget_high_s32(sum_of_exps_reduced_4));\n      int32x2_t sum_of_exps_reduced_1 =\n          vpadd_s32(sum_of_exps_reduced_2, sum_of_exps_reduced_2);\n      sum_of_exps =\n          FixedPointAccum::FromRaw(vget_lane_s32(sum_of_exps_reduced_1, 0));\n#endif\n      for (; c < depth; ++c) {\n        int32 input_diff = static_cast<int32>(input_data_ptr[c]) - max_in_row;\n        if (input_diff >= diff_min) {\n          const int32 input_diff_rescaled =\n              MultiplyByQuantizedMultiplierGreaterThanOne(\n                  input_diff, input_beta_multiplier, input_beta_left_shift);\n          const FixedPointScaledDiff scaled_diff_f8 =\n              FixedPointScaledDiff::FromRaw(input_diff_rescaled);\n          sum_of_exps =\n              sum_of_exps + gemmlowp::Rescale<kAccumulationIntegerBits>(\n                                exp_on_negative_values(scaled_diff_f8));\n        }\n      }\n    }\n\n    // Compute the fixed-point multiplier and shift that we need to apply to\n    // perform a division by the above-computed sum-of-exponentials.\n    int num_bits_over_unit = 0;\n    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(\n        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));\n\n    // Compute the quotients of exponentials of differences of entries in the\n    // current row from the largest entry, over the previously-computed sum of\n    // exponentials.\n    {\n      int c = 0;\n#ifdef USE_NEON\n      int16x8_t diff_min_s16 = vdupq_n_s16(diff_min);\n      for (; c <= depth - 8; c += 8) {\n        uint16x8_t input_u16 = vmovl_u8(vld1_u8(input_data_ptr + c));\n        int16x8_t input_diff_s16 =\n            vsubq_s16(vreinterpretq_s16_u16(input_u16), max_in_row_s16);\n        int32x4_t input_diff_s32_0 = vmovl_s16(vget_low_s16(input_diff_s16));\n        int32x4_t input_diff_s32_1 = vmovl_s16(vget_high_s16(input_diff_s16));\n        uint8x8_t mask = vmovn_u16(vcgeq_s16(input_diff_s16, diff_min_s16));\n        FixedPointScaledDiffInt32x4 scaled_diff_0 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_0, input_beta_left_shift));\n        FixedPointScaledDiffInt32x4 scaled_diff_1 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_1, input_beta_left_shift));\n        FixedPoint0Int32x4 exp_0 = exp_on_negative_values(scaled_diff_0);\n        FixedPoint0Int32x4 exp_1 = exp_on_negative_values(scaled_diff_1);\n        int32x4_t output_s32_0 = gemmlowp::RoundingDivideByPOT(\n            vqrdmulhq_n_s32(exp_0.raw(), shifted_scale.raw()),\n            num_bits_over_unit + 31 - 8);\n        int32x4_t output_s32_1 = gemmlowp::RoundingDivideByPOT(\n            vqrdmulhq_n_s32(exp_1.raw(), shifted_scale.raw()),\n            num_bits_over_unit + 31 - 8);\n        int16x8_t output_s16 =\n            vcombine_s16(vqmovn_s32(output_s32_0), vqmovn_s32(output_s32_1));\n        uint8x8_t output_u8 = vqmovun_s16(output_s16);\n        uint8x8_t masked_output = vbsl_u8(mask, output_u8, vdup_n_u8(0));\n        vst1_u8(output_data_ptr + c, masked_output);\n      }\n#endif\n      for (; c < depth; ++c) {\n        int32 input_diff = static_cast<int32>(input_data_ptr[c]) - max_in_row;\n        if (input_diff >= diff_min) {\n          const int32 input_diff_rescaled =\n              MultiplyByQuantizedMultiplierGreaterThanOne(\n                  input_diff, input_beta_multiplier, input_beta_left_shift);\n          const FixedPointScaledDiff scaled_diff_f8 =\n              FixedPointScaledDiff::FromRaw(input_diff_rescaled);\n\n          FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);\n          int32 unsat_output = gemmlowp::RoundingDivideByPOT(\n              (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);\n\n          output_data_ptr[c] = std::max(std::min(unsat_output, 255), 0);\n\n        } else {\n          output_data_ptr[c] = 0;\n        }\n      }\n    }\n  }\n}\n\ninline void Softmax(const float* input_data, const RuntimeShape& input_shape,\n                    float beta, float* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.beta = beta;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Softmax(const float* input_data, const Dims<4>& input_dims,\n                    float beta, float* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), beta, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void Softmax(const uint8* input_data, const RuntimeShape& input_shape,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_beta_multiplier;\n  params.input_left_shift = input_beta_left_shift;\n  params.diff_min = diff_min;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\ninline void Softmax(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), input_beta_multiplier,\n          input_beta_left_shift, diff_min, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const float* input_data, const RuntimeShape& input_shape,\n                       float* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  // No params currently used for float LogSoftmax.\n  LogSoftmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void LogSoftmax(const float* input_data, const Dims<4>& input_dims,\n                       float* output_data, const Dims<4>& output_dims) {\n  LogSoftmax(input_data, DimsToShape(input_dims), output_data,\n             DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const uint8* input_data, const RuntimeShape& input_shape,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  params.reverse_scaling_divisor = reverse_scaling_divisor;\n  params.reverse_scaling_right_shift = reverse_scaling_right_shift;\n  params.diff_min = diff_min;\n  reference_ops::LogSoftmax(params, input_shape, input_data, output_shape,\n                            output_data);\n}\n\ninline void LogSoftmax(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const Dims<4>& output_dims) {\n  reference_ops::LogSoftmax(\n      input_data, DimsToShape(input_dims), input_multiplier, input_left_shift,\n      reverse_scaling_divisor, reverse_scaling_right_shift, diff_min,\n      output_data, DimsToShape(output_dims));\n}\n\ninline void Logistic(const LogisticParams& params,\n                     const RuntimeShape& input_shape, const uint8* input_data,\n                     const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic/Uint8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int input_left_shift = params.input_left_shift;\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n#ifdef USE_NEON\n  // Handle 16 values at a time\n  for (; c <= size - 16; c += 16) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    uint8x16_t input_val_u8 = vld1q_u8(input_data + c);\n    int16x8_t input_val_centered_0 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n    int16x8_t input_val_centered_1 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint16x8_t mask_rightclamp_0 =\n        vcgtq_s16(input_val_centered_0, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_rightclamp_1 =\n        vcgtq_s16(input_val_centered_1, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_leftclamp_0 =\n        vcgeq_s16(input_val_centered_0, vdupq_n_s16(-input_range_radius));\n    uint16x8_t mask_leftclamp_1 =\n        vcgeq_s16(input_val_centered_1, vdupq_n_s16(-input_range_radius));\n    uint8x16_t mask_rightclamp = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                                             vshrn_n_u16(mask_rightclamp_1, 8));\n    uint8x16_t mask_leftclamp = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                                            vshrn_n_u16(mask_leftclamp_1, 8));\n\n    // This performs what is expressed in the scalar code as\n    // const int32 input_val_rescaled =\n    //     MultiplyByQuantizedMultiplierGreaterThanOne(\n    //         input_val_centered, input_multiplier, input_left_shift);\n    int32x4_t input_val_rescaled_0 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_1 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_2 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_3 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    input_val_rescaled_0 =\n        vqrdmulhq_n_s32(input_val_rescaled_0, input_multiplier);\n    input_val_rescaled_1 =\n        vqrdmulhq_n_s32(input_val_rescaled_1, input_multiplier);\n    input_val_rescaled_2 =\n        vqrdmulhq_n_s32(input_val_rescaled_2, input_multiplier);\n    input_val_rescaled_3 =\n        vqrdmulhq_n_s32(input_val_rescaled_3, input_multiplier);\n\n    // Invoke gemmlowp::logistic on FixedPoint wrapping int32x4_t\n    using FixedPoint4 = gemmlowp::FixedPoint<int32x4_t, 4>;\n    using FixedPoint0 = gemmlowp::FixedPoint<int32x4_t, 0>;\n    const FixedPoint4 input_val_f4_0 =\n        FixedPoint4::FromRaw(input_val_rescaled_0);\n    const FixedPoint4 input_val_f4_1 =\n        FixedPoint4::FromRaw(input_val_rescaled_1);\n    const FixedPoint4 input_val_f4_2 =\n        FixedPoint4::FromRaw(input_val_rescaled_2);\n    const FixedPoint4 input_val_f4_3 =\n        FixedPoint4::FromRaw(input_val_rescaled_3);\n    const FixedPoint0 output_val_f0_0 = gemmlowp::logistic(input_val_f4_0);\n    const FixedPoint0 output_val_f0_1 = gemmlowp::logistic(input_val_f4_1);\n    const FixedPoint0 output_val_f0_2 = gemmlowp::logistic(input_val_f4_2);\n    const FixedPoint0 output_val_f0_3 = gemmlowp::logistic(input_val_f4_3);\n\n    // Divide by 2^23 as in the scalar code\n    using gemmlowp::RoundingDivideByPOT;\n    int32x4_t output_val_s32_0 = RoundingDivideByPOT(output_val_f0_0.raw(), 23);\n    int32x4_t output_val_s32_1 = RoundingDivideByPOT(output_val_f0_1.raw(), 23);\n    int32x4_t output_val_s32_2 = RoundingDivideByPOT(output_val_f0_2.raw(), 23);\n    int32x4_t output_val_s32_3 = RoundingDivideByPOT(output_val_f0_3.raw(), 23);\n\n    // Cast output values to uint8, saturating\n    int16x8_t output_val_s16_0 = vcombine_s16(vqmovn_s32(output_val_s32_0),\n                                              vqmovn_s32(output_val_s32_1));\n    int16x8_t output_val_s16_1 = vcombine_s16(vqmovn_s32(output_val_s32_2),\n                                              vqmovn_s32(output_val_s32_3));\n    uint8x16_t output_val_u8 = vcombine_u8(vqmovun_s16(output_val_s16_0),\n                                           vqmovun_s16(output_val_s16_1));\n\n    // Perform the bit-masking with the bit masks computed at the beginning,\n    // see the comment there.\n    output_val_u8 = vorrq_u8(output_val_u8, mask_rightclamp);\n    output_val_u8 = vandq_u8(output_val_u8, mask_leftclamp);\n\n    // Store back to memory\n    vst1q_u8(output_data + c, output_val_u8);\n  }\n#endif\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int32 input_val_centered =\n        static_cast<int32>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      const int32 input_val_rescaled =\n          MultiplyByQuantizedMultiplierGreaterThanOne(\n              input_val_centered, input_multiplier, input_left_shift);\n      using FixedPoint4 = gemmlowp::FixedPoint<int32, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int32 output_val_s32 = RoundingDivideByPOT(output_val_f0.raw(), 23);\n      if (output_val_s32 == 256) {\n        output_val_s32 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s32, 0);\n      TFLITE_DCHECK_LE(output_val_s32, 255);\n      output_val = static_cast<uint8>(output_val_s32);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Logistic(const uint8* input_data, const RuntimeShape& input_shape,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const RuntimeShape& output_shape) {\n  LogisticParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  Logistic(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n           output_data);\n}\n\ninline void Logistic(const uint8* input_data, const Dims<4>& input_dims,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const Dims<4>& output_dims) {\n  Logistic(input_data, DimsToShape(input_dims), input_zero_point,\n           input_range_radius, input_multiplier, input_left_shift, output_data,\n           DimsToShape(output_dims));\n}\n\ninline void Logistic(const RuntimeShape& input_shape, const int16* input_data,\n                     const RuntimeShape& output_shape, int16* output_data) {\n  LogisticParams params;\n  // No params currently needed by int16 Logistic.\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const int16* input_data, const RuntimeShape& input_shape,\n                     int16* output_data, const RuntimeShape& output_shape) {\n  LogisticParams params;\n  // No params currently needed by int16 Logistic.\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const int16* input_data, const Dims<4>& input_dims,\n                     int16* output_data, const Dims<4>& output_dims) {\n  Logistic(input_data, DimsToShape(input_dims), output_data,\n           DimsToShape(output_dims));\n}\n\ninline void Tanh(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Tanh(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Tanh(const TanhParams& params, const RuntimeShape& input_shape,\n                 const uint8* input_data, const RuntimeShape& output_shape,\n                 uint8* output_data) {\n  // Note that this is almost the exact same code as in Logistic().\n  ruy::profiler::ScopeLabel label(\"Tanh\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int input_left_shift = params.input_left_shift;\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n  int32_t output_zero_point = 128;\n#ifdef USE_NEON\n  // Handle 16 values at a time\n  for (; c <= size - 16; c += 16) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    uint8x16_t input_val_u8 = vld1q_u8(input_data + c);\n    int16x8_t input_val_centered_0 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n    int16x8_t input_val_centered_1 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint16x8_t mask_rightclamp_0 =\n        vcgtq_s16(input_val_centered_0, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_rightclamp_1 =\n        vcgtq_s16(input_val_centered_1, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_leftclamp_0 =\n        vcgeq_s16(input_val_centered_0, vdupq_n_s16(-input_range_radius));\n    uint16x8_t mask_leftclamp_1 =\n        vcgeq_s16(input_val_centered_1, vdupq_n_s16(-input_range_radius));\n    uint8x16_t mask_rightclamp = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                                             vshrn_n_u16(mask_rightclamp_1, 8));\n    uint8x16_t mask_leftclamp = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                                            vshrn_n_u16(mask_leftclamp_1, 8));\n\n    // This performs what is expressed in the scalar code as\n    // const int32 input_val_rescaled =\n    //     MultiplyByQuantizedMultiplierGreaterThanOne(\n    //         input_val_centered, input_multiplier, input_left_shift);\n    int32x4_t input_val_rescaled_0 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_1 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_2 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_3 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    input_val_rescaled_0 =\n        vqrdmulhq_n_s32(input_val_rescaled_0, input_multiplier);\n    input_val_rescaled_1 =\n        vqrdmulhq_n_s32(input_val_rescaled_1, input_multiplier);\n    input_val_rescaled_2 =\n        vqrdmulhq_n_s32(input_val_rescaled_2, input_multiplier);\n    input_val_rescaled_3 =\n        vqrdmulhq_n_s32(input_val_rescaled_3, input_multiplier);\n\n    // Invoke gemmlowp::tanh on FixedPoint wrapping int32x4_t\n    using FixedPoint4 = gemmlowp::FixedPoint<int32x4_t, 4>;\n    using FixedPoint0 = gemmlowp::FixedPoint<int32x4_t, 0>;\n    const FixedPoint4 input_val_f4_0 =\n        FixedPoint4::FromRaw(input_val_rescaled_0);\n    const FixedPoint4 input_val_f4_1 =\n        FixedPoint4::FromRaw(input_val_rescaled_1);\n    const FixedPoint4 input_val_f4_2 =\n        FixedPoint4::FromRaw(input_val_rescaled_2);\n    const FixedPoint4 input_val_f4_3 =\n        FixedPoint4::FromRaw(input_val_rescaled_3);\n    const FixedPoint0 output_val_f0_0 = gemmlowp::tanh(input_val_f4_0);\n    const FixedPoint0 output_val_f0_1 = gemmlowp::tanh(input_val_f4_1);\n    const FixedPoint0 output_val_f0_2 = gemmlowp::tanh(input_val_f4_2);\n    const FixedPoint0 output_val_f0_3 = gemmlowp::tanh(input_val_f4_3);\n\n    // Divide by 2^24 as in the scalar code\n    using gemmlowp::RoundingDivideByPOT;\n    int32x4_t output_val_s32_0 = RoundingDivideByPOT(output_val_f0_0.raw(), 24);\n    int32x4_t output_val_s32_1 = RoundingDivideByPOT(output_val_f0_1.raw(), 24);\n    int32x4_t output_val_s32_2 = RoundingDivideByPOT(output_val_f0_2.raw(), 24);\n    int32x4_t output_val_s32_3 = RoundingDivideByPOT(output_val_f0_3.raw(), 24);\n\n    // Add the output zero point\n    int32x4_t output_zero_point_s32 = vdupq_n_s32(output_zero_point);\n    output_val_s32_0 = vaddq_s32(output_val_s32_0, output_zero_point_s32);\n    output_val_s32_1 = vaddq_s32(output_val_s32_1, output_zero_point_s32);\n    output_val_s32_2 = vaddq_s32(output_val_s32_2, output_zero_point_s32);\n    output_val_s32_3 = vaddq_s32(output_val_s32_3, output_zero_point_s32);\n\n    // Cast output values to uint8, saturating\n    int16x8_t output_val_s16_0 = vcombine_s16(vqmovn_s32(output_val_s32_0),\n                                              vqmovn_s32(output_val_s32_1));\n    int16x8_t output_val_s16_1 = vcombine_s16(vqmovn_s32(output_val_s32_2),\n                                              vqmovn_s32(output_val_s32_3));\n    uint8x16_t output_val_u8 = vcombine_u8(vqmovun_s16(output_val_s16_0),\n                                           vqmovun_s16(output_val_s16_1));\n\n    // Perform the bit-masking with the bit masks computed at the beginning,\n    // see the comment there.\n    output_val_u8 = vorrq_u8(output_val_u8, mask_rightclamp);\n    output_val_u8 = vandq_u8(output_val_u8, mask_leftclamp);\n\n    // Store back to memory\n    vst1q_u8(output_data + c, output_val_u8);\n  }\n#endif\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int32 input_val_centered =\n        static_cast<int32>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      const int32 input_val_rescaled =\n          MultiplyByQuantizedMultiplierGreaterThanOne(\n              input_val_centered, input_multiplier, input_left_shift);\n      using FixedPoint4 = gemmlowp::FixedPoint<int32, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::tanh(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int32 output_val_s32 = RoundingDivideByPOT(output_val_f0.raw(), 24);\n      output_val_s32 += output_zero_point;\n      if (output_val_s32 == 256) {\n        output_val_s32 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s32, 0);\n      TFLITE_DCHECK_LE(output_val_s32, 255);\n      output_val = static_cast<uint8>(output_val_s32);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Tanh(const uint8* input_data, const RuntimeShape& input_shape,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_zero_point,\n       input_range_radius, input_multiplier, input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ninline void Tanh(const int16* input_data, const RuntimeShape& input_shape,\n                 int input_left_shift, int16* output_data,\n                 const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const int16* input_data, const Dims<4>& input_dims,\n                 int input_left_shift, int16* output_data,\n                 const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ntemplate <typename T>\ninline void DepthToSpace(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::DepthToSpaceParams op_params;\n  op_params.block_size = block_size;\n\n  DepthToSpace(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void SpaceToDepth(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::SpaceToDepthParams op_params;\n  op_params.block_size = block_size;\n\n  SpaceToDepth(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ninline void Mul(const float* input1_data, const Dims<4>& input1_dims,\n                const float* input2_data, const Dims<4>& input2_dims,\n                float output_activation_min, float output_activation_max,\n                float* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Mul(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  Mul(input1_data, input1_dims, input2_data, input2_dims, output_activation_min,\n      output_activation_max, output_data, output_dims);\n}\n\ninline void Mul(const int32* input1_data, const Dims<4>& input1_dims,\n                const int32* input2_data, const Dims<4>& input2_dims,\n                int32 output_activation_min, int32 output_activation_max,\n                int32* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Mul(const int32* input1_data, const Dims<4>& input1_dims,\n         const int32* input2_data, const Dims<4>& input2_dims,\n         int32* output_data, const Dims<4>& output_dims) {\n  TFLITE_DCHECK(Ac == FusedActivationFunctionType::kNone);\n  tflite::ArithmeticParams op_params;\n  // No parameters needed.\n\n  MulNoActivation(op_params, DimsToShape(input1_dims), input1_data,\n                  DimsToShape(input2_dims), input2_data,\n                  DimsToShape(output_dims), output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int16* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  // No parameters needed.\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int32 output_offset, int32 output_activation_min,\n                int32 output_activation_max, uint8* output_data,\n                const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.output_offset = output_offset;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastMul(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// For compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastMul(const float* input1_data, const Dims<4>& input1_dims,\n                         const float* input2_data, const Dims<4>& input2_dims,\n                         float* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  float float_activation_min;\n  float float_activation_max;\n  GetActivationMinMax(Ac, &float_activation_min, &float_activation_max);\n  SetActivationParams(float_activation_min, float_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ninline void LocalResponseNormalization(const float* input_data,\n                                       const Dims<4>& input_dims, int range,\n                                       float bias, float alpha, float beta,\n                                       float* output_data,\n                                       const Dims<4>& output_dims) {\n  tflite::LocalResponseNormalizationParams op_params;\n  op_params.range = range;\n  op_params.bias = bias;\n  op_params.alpha = alpha;\n  op_params.beta = beta;\n\n  LocalResponseNormalization(op_params, DimsToShape(input_dims), input_data,\n                             DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename SrcT, typename DstT>\nvoid Cast(const SrcT* input_data, const Dims<4>& input_dims, DstT* output_data,\n          const Dims<4>& output_dims) {\n  Cast(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Floor(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Floor(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ninline void ResizeBilinear(const float* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, float* output_data,\n                           const Dims<4>& output_dims, bool align_corners) {\n  tflite::ResizeBilinearParams op_params;\n  op_params.align_corners = align_corners;\n  op_params.half_pixel_centers = false;\n  ResizeBilinear(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(output_size_dims), output_size_data,\n                 DimsToShape(output_dims), output_data);\n}\n\ninline void ResizeBilinear(const uint8* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, uint8* output_data,\n                           const Dims<4>& output_dims, bool align_corners) {\n  tflite::ResizeBilinearParams op_params;\n  op_params.align_corners = align_corners;\n  op_params.half_pixel_centers = false;\n  ResizeBilinear(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(output_size_dims), output_size_data,\n                 DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ninline void ResizeBilinear(const float* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, float* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear(input_data, input_dims, output_size_data, output_size_dims,\n                 output_data, output_dims, /*align_corners=*/false);\n}\n\n// legacy, for compatibility with old checked-in code\ninline void ResizeBilinear(const uint8* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, uint8* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear(input_data, input_dims, output_size_data, output_size_dims,\n                 output_data, output_dims, /*align_corners=*/false);\n}\n\ntemplate <typename T>\ninline void BatchToSpaceND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* crops_data, const Dims<4>& crops_dims,\n                           T* output_data, const Dims<4>& output_dims) {\n  BatchToSpaceND(DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(crops_dims), crops_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// Legacy signature, function covered both Pad and PadV2.\ntemplate <typename T>\ninline void PadV2(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& left_paddings,\n                  const std::vector<int>& right_paddings, T* output_data,\n                  const Dims<4>& output_dims, const T pad_value) {\n  TFLITE_DCHECK_EQ(left_paddings.size(), 4);\n  TFLITE_DCHECK_EQ(right_paddings.size(), 4);\n  tflite::PadParams op_params;\n  op_params.left_padding_count = 4;\n  op_params.right_padding_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.left_padding[i] = left_paddings[3 - i];\n    op_params.right_padding[i] = right_paddings[3 - i];\n  }\n  const T pad_value_copy = pad_value;\n\n  Pad(op_params, DimsToShape(input_dims), input_data, &pad_value_copy,\n      DimsToShape(output_dims), output_data);\n}\n\n// Old Pad that calls legacy PadV2.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims, const int32_t pad_value) {\n  const T converted_pad_value = static_cast<T>(pad_value);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, converted_pad_value);\n}\n\n// Old Pad that only padded with 0.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims) {\n  const T pad_value = static_cast<T>(0);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, pad_value);\n}\n\ntemplate <typename T>\ninline void Slice(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& begin, const std::vector<int>& size,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::SliceParams op_params;\n  op_params.begin_count = 4;\n  op_params.size_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.begin[i] = begin[3 - i];\n    op_params.size[i] = size[3 - i];\n  }\n\n  Slice(op_params, DimsToShape(input_dims), input_data,\n        DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid TensorFlowMinimum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Minimum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid TensorFlowMaximum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Maximum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ninline void Dequantize(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 zero_point, double scale, float* output_data,\n                       const Dims<4>& output_dims) {\n  tflite::DequantizationParams op_params;\n  op_params.zero_point = zero_point;\n  op_params.scale = scale;\n\n  Dequantize(op_params, DimsToShape(input_dims), input_data,\n             DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid Transpose(const T* input, const Dims<4>& input_dims, T* output,\n               const Dims<4>& output_dims, const int* permuted_axes) {\n  TransposeParams params;\n  params.perm_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    params.perm[i] = 3 - permuted_axes[3 - i];\n  }\n  Transpose(params, DimsToShape(input_dims), input, DimsToShape(output_dims),\n            output);\n}\n\ntemplate <typename T>\ninline void StridedSlice(const T* input_data, const Dims<4>& input_dims,\n                         int begin_mask, int end_mask, int shrink_axis_mask,\n                         const std::vector<int>& start_indices,\n                         const std::vector<int>& stop_indices,\n                         const std::vector<int>& strides, T* output_data,\n                         const Dims<4>& output_dims) {\n  TFLITE_DCHECK_EQ(start_indices.size(), 4);\n  auto op_params = strided_slice::BuildStridedSliceParams(\n      begin_mask, end_mask, shrink_axis_mask, start_indices, stop_indices,\n      strides);\n  reference_ops::StridedSliceReverseIndices(&op_params);\n\n  StridedSlice(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const T3* axis, const T1* input_data,\n            const tflite::Dims<4>& input_dims, T2* output_data,\n            const tflite::Dims<4>& output_dims) {\n  // Assumes the input always has 4 dimensions, and therefore,\n  // output always has three dimensions.\n  auto output_shape = RuntimeShape(\n      {output_dims.sizes[2], output_dims.sizes[1], output_dims.sizes[0]});\n  // Another way to interpret this is that output_dims.sizes[4] is always 1.\n  TFLITE_DCHECK_EQ(output_shape.FlatSize(),\n                   DimsToShape(output_dims).FlatSize());\n  // Legacy path only supported this.\n  TFLITE_DCHECK_EQ(axis[0], 3);\n  ArgMinMax(DimsToShape(input_dims), input_data, axis, output_shape,\n            output_data, /*is_arg_max=*/true);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMinMax(const T3* axis, const T1* input_data, const Dims<4>& input_dims,\n               T2* output_data, const Dims<4>& output_dims,\n               const bool is_arg_max) {\n  ArgMinMax(axis, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n            output_data, is_arg_max);\n}\n\n}  // namespace optimized_ops\n}  // namespace tflite\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_LEGACY_OPTIMIZED_OPS_H_\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_OPTIMIZED_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_OPTIMIZED_OPS_H_\n\n#include <assert.h>\n#include <stdint.h>\n#include <sys/types.h>\n\n#include <algorithm>\n#include <cmath>\n#include <cstdint>\n#include <limits>\n#include <memory>\n#include <tuple>\n#include <type_traits>\n\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/reference/add.h\"\n#include \"tensorflow/lite/kernels/internal/reference/resize_nearest_neighbor.h\"\n\n#if defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n#include <Accelerate/Accelerate.h>\n#endif\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"fixedpoint/fixedpoint.h\"\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_gemm.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_gemm_params.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_threadpool.h\"\n#include \"tensorflow/lite/kernels/internal/cppmath.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/im2col_utils.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/strided_slice_logic.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/kernels/internal/transpose_utils.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\n#if __aarch64__ && __clang__\n#define TFLITE_SOFTMAX_USE_UINT16_LUT\n#endif\n\nnamespace tflite {\nnamespace optimized_ops {\n\n// Unoptimized reference ops:\nusing reference_ops::Broadcast4DSlowGreater;\nusing reference_ops::Broadcast4DSlowGreaterEqual;\nusing reference_ops::Broadcast4DSlowGreaterEqualWithScaling;\nusing reference_ops::Broadcast4DSlowGreaterWithScaling;\nusing reference_ops::Broadcast4DSlowLess;\nusing reference_ops::Broadcast4DSlowLessEqual;\nusing reference_ops::Broadcast4DSlowLessEqualWithScaling;\nusing reference_ops::Broadcast4DSlowLessWithScaling;\nusing reference_ops::BroadcastAdd4DSlow;\nusing reference_ops::BroadcastMul4DSlow;\nusing reference_ops::BroadcastSub16POTSlow;\nusing reference_ops::BroadcastSubSlow;\nusing reference_ops::Concatenation;\nusing reference_ops::ConcatenationWithScaling;\nusing reference_ops::DepthConcatenation;\nusing reference_ops::Div;\nusing reference_ops::Elu;\nusing reference_ops::FakeQuant;\nusing reference_ops::Fill;\nusing reference_ops::Gather;\nusing reference_ops::Greater;\nusing reference_ops::GreaterEqual;\nusing reference_ops::GreaterEqualWithScaling;\nusing reference_ops::GreaterWithScaling;\nusing reference_ops::LeakyRelu;\nusing reference_ops::Less;\nusing reference_ops::LessEqual;\nusing reference_ops::LessEqualWithScaling;\nusing reference_ops::LessWithScaling;\nusing reference_ops::Mean;\nusing reference_ops::ProcessBroadcastShapes;\nusing reference_ops::RankOneSelect;\nusing reference_ops::Relu1;\nusing reference_ops::Relu6;\nusing reference_ops::ReluX;\nusing reference_ops::Round;\nusing reference_ops::Select;\nusing reference_ops::SpaceToBatchND;\nusing reference_ops::Split;\nusing reference_ops::Sub16;\n\n// TODO(b/80247582) Remove this constant.\n// This will be phased out as the shifts are revised with more thought. Use of a\n// constant enables us to track progress on this work.\n//\n// Used to convert from old-style shifts (right) to new-style (left).\nstatic constexpr int kReverseShift = -1;\n\n// Make a local VectorMap typedef allowing to map a float array\n// as a Eigen vector expression. The std::conditional here is to\n// construct the suitable Eigen type for the constness of the\n// data. Indeed, for const data, we need to produce\n//    Eigen::Map<const Eigen::Matrix<float, ...>>\n// and not the more straightforward\n//    Eigen::Map<Eigen::Matrix<const float, ...>>\ntemplate <typename Scalar>\nusing VectorMap = typename std::conditional<\n    std::is_const<Scalar>::value,\n    Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,\n                                   Eigen::Dynamic, 1>>,\n    Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>>::type;\n\ntemplate <typename Scalar>\nVectorMap<Scalar> MapAsVector(Scalar* data, const RuntimeShape& shape) {\n  const int size = shape.FlatSize();\n  return VectorMap<Scalar>(data, size, 1);\n}\n\n// Make a local VectorMap typedef allowing to map a float array\n// as a Eigen matrix expression. The same explanation as for VectorMap\n// above also applies here.\ntemplate <typename Scalar>\nusing MatrixMap = typename std::conditional<\n    std::is_const<Scalar>::value,\n    Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,\n                                   Eigen::Dynamic, Eigen::Dynamic>>,\n    Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;\n\ntemplate <typename Scalar>\nMatrixMap<Scalar> MapAsMatrixWithLastDimAsRows(Scalar* data,\n                                               const RuntimeShape& shape) {\n  const int dims_count = shape.DimensionsCount();\n  const int rows = shape.Dims(dims_count - 1);\n  const int cols = FlatSizeSkipDim(shape, dims_count - 1);\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar>\nMatrixMap<Scalar> MapAsMatrixWithFirstDimAsCols(Scalar* data,\n                                                const RuntimeShape& shape) {\n  const int cols = shape.Dims(0);\n  const int rows = FlatSizeSkipDim(shape, 0);\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar>\nusing ArrayMap = typename std::conditional<\n    std::is_const<Scalar>::value,\n    Eigen::Map<const Eigen::Array<typename std::remove_const<Scalar>::type,\n                                  Eigen::Dynamic, Eigen::Dynamic>>,\n    Eigen::Map<Eigen::Array<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;\n\ntemplate <typename Scalar>\nArrayMap<Scalar> MapAsArrayWithLastDimAsRows(Scalar* data,\n                                             const RuntimeShape& shape) {\n  const int dims_count = shape.DimensionsCount();\n  const int rows = shape.Dims(dims_count - 1);\n  const int cols = FlatSizeSkipDim(shape, dims_count - 1);\n  return ArrayMap<Scalar>(data, rows, cols);\n}\n\n// Copied from tensorflow/core/framework/tensor_types.h\ntemplate <typename T, int NDIMS = 1, typename IndexType = Eigen::DenseIndex>\nstruct TTypes {\n  // Rank-1 tensor (vector) of scalar type T.\n  typedef Eigen::TensorMap<Eigen::Tensor<T, 1, Eigen::RowMajor, IndexType>,\n                           Eigen::Aligned>\n      Flat;\n  typedef Eigen::TensorMap<\n      Eigen::Tensor<const T, 2, Eigen::RowMajor, IndexType>>\n      UnalignedConstMatrix;\n};\n\n// TODO(b/62193649): this function is only needed as long\n// as we have the --variable_batch hack.\ntemplate <typename Scalar>\nMatrixMap<Scalar> MapAsMatrixWithGivenNumberOfRows(Scalar* data,\n                                                   const RuntimeShape& shape,\n                                                   int rows) {\n  const int flatsize = shape.FlatSize();\n  TFLITE_DCHECK_EQ(flatsize % rows, 0);\n  const int cols = flatsize / rows;\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename ElementwiseF, typename ScalarBroadcastF, typename T>\ninline void BinaryBroadcastFiveFold(const ArithmeticParams& unswitched_params,\n                                    const RuntimeShape& unswitched_input1_shape,\n                                    const T* unswitched_input1_data,\n                                    const RuntimeShape& unswitched_input2_shape,\n                                    const T* unswitched_input2_data,\n                                    const RuntimeShape& output_shape,\n                                    T* output_data, ElementwiseF elementwise_f,\n                                    ScalarBroadcastF scalar_broadcast_f) {\n  ArithmeticParams switched_params = unswitched_params;\n  switched_params.input1_offset = unswitched_params.input2_offset;\n  switched_params.input1_multiplier = unswitched_params.input2_multiplier;\n  switched_params.input1_shift = unswitched_params.input2_shift;\n  switched_params.input2_offset = unswitched_params.input1_offset;\n  switched_params.input2_multiplier = unswitched_params.input1_multiplier;\n  switched_params.input2_shift = unswitched_params.input1_shift;\n\n  const bool use_unswitched =\n      unswitched_params.broadcast_category ==\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n\n  const ArithmeticParams& params =\n      use_unswitched ? unswitched_params : switched_params;\n  const T* input1_data =\n      use_unswitched ? unswitched_input1_data : unswitched_input2_data;\n  const T* input2_data =\n      use_unswitched ? unswitched_input2_data : unswitched_input1_data;\n\n  // Fivefold nested loops. The second input resets its position for each\n  // iteration of the second loop. The first input resets its position at the\n  // beginning of the fourth loop. The innermost loop is an elementwise add of\n  // sections of the arrays.\n  T* output_data_ptr = output_data;\n  const T* input1_data_ptr = input1_data;\n  const T* input2_data_reset = input2_data;\n  // In the fivefold pattern, y0, y2 and y4 are not broadcast, and so shared\n  // between input shapes. y3 for input 1 is always broadcast, and so the\n  // dimension there is 1, whereas optionally y1 might be broadcast for\n  // input 2. Put another way, input1.shape.FlatSize = y0 * y1 * y2 * y4,\n  // input2.shape.FlatSize = y0 * y2 * y3 * y4.\n  int y0 = params.broadcast_shape[0];\n  int y1 = params.broadcast_shape[1];\n  int y2 = params.broadcast_shape[2];\n  int y3 = params.broadcast_shape[3];\n  int y4 = params.broadcast_shape[4];\n  if (y4 > 1) {\n    // General fivefold pattern, with y4 > 1 so there is a non-broadcast inner\n    // dimension.\n    for (int i0 = 0; i0 < y0; ++i0) {\n      const T* input2_data_ptr = nullptr;\n      for (int i1 = 0; i1 < y1; ++i1) {\n        input2_data_ptr = input2_data_reset;\n        for (int i2 = 0; i2 < y2; ++i2) {\n          for (int i3 = 0; i3 < y3; ++i3) {\n            elementwise_f(y4, params, input1_data_ptr, input2_data_ptr,\n                          output_data_ptr);\n            input2_data_ptr += y4;\n            output_data_ptr += y4;\n          }\n          // We have broadcast y4 of input1 data y3 times, and now move on.\n          input1_data_ptr += y4;\n        }\n      }\n      // We have broadcast y2*y3*y4 of input2 data y1 times, and now move on.\n      input2_data_reset = input2_data_ptr;\n    }\n  } else {\n    // Special case of y4 == 1, in which the innermost loop is a single\n    // element and can be combined with the next (y3) as an inner broadcast.\n    //\n    // Note that this handles the case of pure scalar broadcast when\n    // y0 == y1 == y2 == 1. With low overhead it handles cases such as scalar\n    // broadcast with batch (as y2 > 1).\n    //\n    // NOTE The process is the same as the above general case except\n    // simplified for y4 == 1 and the loop over y3 is contained within the\n    // AddScalarBroadcast function.\n    for (int i0 = 0; i0 < y0; ++i0) {\n      const T* input2_data_ptr = nullptr;\n      for (int i1 = 0; i1 < y1; ++i1) {\n        input2_data_ptr = input2_data_reset;\n        for (int i2 = 0; i2 < y2; ++i2) {\n          scalar_broadcast_f(y3, params, *input1_data_ptr, input2_data_ptr,\n                             output_data_ptr);\n          input2_data_ptr += y3;\n          output_data_ptr += y3;\n          input1_data_ptr += 1;\n        }\n      }\n      input2_data_reset = input2_data_ptr;\n    }\n  }\n}\n\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n\n// Looks up each element of <indices> in <table>, returns them in a vector.\ninline uint8x16_t aarch64_lookup_vector(const uint8x16x4_t table[4],\n                                        uint8x16_t indices) {\n  // Look up in 1st quarter of the table: top 2 bits of indices == 00\n  uint8x16_t output1 = vqtbl4q_u8(table[0], indices);\n  // Look up in 2nd quarter of the table: top 2 bits of indices == 01\n  uint8x16_t output2 =\n      vqtbl4q_u8(table[1], veorq_u8(indices, vdupq_n_u8(0x40)));\n  // Look up in 3rd quarter of the table: top 2 bits of indices == 10\n  uint8x16_t output3 =\n      vqtbl4q_u8(table[2], veorq_u8(indices, vdupq_n_u8(0x80)));\n  // Look up in 4th quarter of the table: top 2 bits of indices == 11\n  uint8x16_t output4 =\n      vqtbl4q_u8(table[3], veorq_u8(indices, vdupq_n_u8(0xc0)));\n\n  // Combine result of the 4 lookups.\n  return vorrq_u8(vorrq_u8(output1, output2), vorrq_u8(output3, output4));\n}\n\n#endif\n\ninline void AddBiasAndEvalActivationFunction(float output_activation_min,\n                                             float output_activation_max,\n                                             const RuntimeShape& bias_shape,\n                                             const float* bias_data,\n                                             const RuntimeShape& array_shape,\n                                             float* array_data) {\n  BiasAndClamp(output_activation_min, output_activation_max,\n               bias_shape.FlatSize(), bias_data, array_shape.FlatSize(),\n               array_data);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& bias_shape,\n    const float* optional_bias_data, const RuntimeShape& output_shape,\n    float* output_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected\");\n  const int dims_count = weights_shape.DimensionsCount();\n  const int input_rows = weights_shape.Dims(dims_count - 1);\n  cpu_backend_gemm::MatrixParams<float> rhs_params;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.rows = input_rows;\n  rhs_params.cols = input_shape.FlatSize() / input_rows;\n  rhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.rhs_cacheable);\n  TFLITE_DCHECK_EQ(input_shape.FlatSize(), rhs_params.rows * rhs_params.cols);\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.cols = weights_shape.Dims(dims_count - 1);\n  lhs_params.rows = FlatSizeSkipDim(weights_shape, dims_count - 1);\n  lhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.lhs_cacheable);\n  cpu_backend_gemm::MatrixParams<float> dst_params;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.rows = output_shape.Dims(output_shape.DimensionsCount() - 1);\n  dst_params.cols =\n      FlatSizeSkipDim(output_shape, output_shape.DimensionsCount() - 1);\n  cpu_backend_gemm::GemmParams<float, float> gemm_params;\n  gemm_params.bias = optional_bias_data;\n  gemm_params.clamp_min = params.float_activation_min;\n  gemm_params.clamp_max = params.float_activation_max;\n  cpu_backend_gemm::Gemm(lhs_params, weights_data, rhs_params, input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/8bit\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int filter_rows = filter_shape.Dims(filter_dim_count - 2);\n  const int filter_cols = filter_shape.Dims(filter_dim_count - 1);\n  TFLITE_DCHECK_EQ(filter_shape.FlatSize(), filter_rows * filter_cols);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  if (bias_data) {\n    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n  }\n\n  cpu_backend_gemm::MatrixParams<uint8> lhs_params;\n  lhs_params.rows = filter_rows;\n  lhs_params.cols = filter_cols;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.zero_point = -filter_offset;\n  lhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.lhs_cacheable);\n  cpu_backend_gemm::MatrixParams<uint8> rhs_params;\n  rhs_params.rows = filter_cols;\n  rhs_params.cols = batches;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.zero_point = -input_offset;\n  rhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.rhs_cacheable);\n  cpu_backend_gemm::MatrixParams<uint8> dst_params;\n  dst_params.rows = filter_rows;\n  dst_params.cols = batches;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.zero_point = output_offset;\n  cpu_backend_gemm::GemmParams<int32, uint8> gemm_params;\n  gemm_params.bias = bias_data;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  gemm_params.multiplier_fixedpoint = output_multiplier;\n  gemm_params.multiplier_exponent = output_shift;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data_int32, const RuntimeShape& output_shape,\n    int16* output_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/Uint8Int16\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  TFLITE_DCHECK_EQ(output_offset, 0);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);\n\n  cpu_backend_gemm::MatrixParams<uint8> lhs_params;\n  lhs_params.rows = output_depth;\n  lhs_params.cols = accum_depth;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.zero_point = -filter_offset;\n  lhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.lhs_cacheable);\n  cpu_backend_gemm::MatrixParams<uint8> rhs_params;\n  rhs_params.rows = accum_depth;\n  rhs_params.cols = batches;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.zero_point = -input_offset;\n  rhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.rhs_cacheable);\n  cpu_backend_gemm::MatrixParams<int16> dst_params;\n  dst_params.rows = output_depth;\n  dst_params.cols = batches;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.zero_point = 0;\n  cpu_backend_gemm::GemmParams<int32, int16> gemm_params;\n  gemm_params.bias = bias_data_int32;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  gemm_params.multiplier_fixedpoint = output_multiplier;\n  gemm_params.multiplier_exponent = output_shift;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\n// Internal function doing the actual arithmetic work for\n// ShuffledFullyConnected.\n// May be called either directly by it (single-threaded case) or may be used\n// as the 'task' for worker threads to run (multi-threaded case, see\n// ShuffledFullyConnectedWorkerTask below).\ninline void ShuffledFullyConnectedWorkerImpl(\n    const uint8* shuffled_input_workspace_data,\n    const int8* shuffled_weights_data, int batches, int output_depth,\n    int output_stride, int accum_depth, const int32* bias_data,\n    int32 output_multiplier, int output_shift, int16* output_data) {\n#if defined USE_NEON\n  const int8* shuffled_weights_ptr = shuffled_weights_data;\n  if (batches == 1) {\n    const int right_shift = output_shift > 0 ? 0 : -output_shift;\n    const int left_shift = output_shift > 0 ? output_shift : 0;\n    for (int c = 0; c < output_depth; c += 4) {\n      // Accumulation loop.\n      int32x4_t row_accum0 = vdupq_n_s32(0);\n      int32x4_t row_accum1 = vdupq_n_s32(0);\n      int32x4_t row_accum2 = vdupq_n_s32(0);\n      int32x4_t row_accum3 = vdupq_n_s32(0);\n      for (int d = 0; d < accum_depth; d += 16) {\n        int8x16_t weights0 = vld1q_s8(shuffled_weights_ptr + 0);\n        int8x16_t weights1 = vld1q_s8(shuffled_weights_ptr + 16);\n        int8x16_t weights2 = vld1q_s8(shuffled_weights_ptr + 32);\n        int8x16_t weights3 = vld1q_s8(shuffled_weights_ptr + 48);\n        shuffled_weights_ptr += 64;\n        int8x16_t input =\n            vreinterpretq_s8_u8(vld1q_u8(shuffled_input_workspace_data + d));\n        int16x8_t local_accum0 =\n            vmull_s8(vget_low_s8(weights0), vget_low_s8(input));\n        int16x8_t local_accum1 =\n            vmull_s8(vget_low_s8(weights1), vget_low_s8(input));\n        int16x8_t local_accum2 =\n            vmull_s8(vget_low_s8(weights2), vget_low_s8(input));\n        int16x8_t local_accum3 =\n            vmull_s8(vget_low_s8(weights3), vget_low_s8(input));\n        local_accum0 =\n            vmlal_s8(local_accum0, vget_high_s8(weights0), vget_high_s8(input));\n        local_accum1 =\n            vmlal_s8(local_accum1, vget_high_s8(weights1), vget_high_s8(input));\n        local_accum2 =\n            vmlal_s8(local_accum2, vget_high_s8(weights2), vget_high_s8(input));\n        local_accum3 =\n            vmlal_s8(local_accum3, vget_high_s8(weights3), vget_high_s8(input));\n        row_accum0 = vpadalq_s16(row_accum0, local_accum0);\n        row_accum1 = vpadalq_s16(row_accum1, local_accum1);\n        row_accum2 = vpadalq_s16(row_accum2, local_accum2);\n        row_accum3 = vpadalq_s16(row_accum3, local_accum3);\n      }\n      // Horizontally reduce accumulators\n      int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,\n          pairwise_reduced_acc_2, pairwise_reduced_acc_3;\n      pairwise_reduced_acc_0 =\n          vpadd_s32(vget_low_s32(row_accum0), vget_high_s32(row_accum0));\n      pairwise_reduced_acc_1 =\n          vpadd_s32(vget_low_s32(row_accum1), vget_high_s32(row_accum1));\n      pairwise_reduced_acc_2 =\n          vpadd_s32(vget_low_s32(row_accum2), vget_high_s32(row_accum2));\n      pairwise_reduced_acc_3 =\n          vpadd_s32(vget_low_s32(row_accum3), vget_high_s32(row_accum3));\n      const int32x2_t reduced_lo =\n          vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n      const int32x2_t reduced_hi =\n          vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n      int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n      // Add bias values.\n      int32x4_t bias_vec = vld1q_s32(bias_data + c);\n      reduced = vaddq_s32(reduced, bias_vec);\n      reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));\n      // Multiply by the fixed-point multiplier.\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n      // Rounding-shift-right.\n      using gemmlowp::RoundingDivideByPOT;\n      reduced = RoundingDivideByPOT(reduced, right_shift);\n      // Narrow values down to 16 bit signed.\n      const int16x4_t res16 = vqmovn_s32(reduced);\n      vst1_s16(output_data + c, res16);\n    }\n  } else if (batches == 4) {\n    const int right_shift = output_shift > 0 ? 0 : -output_shift;\n    const int left_shift = output_shift > 0 ? output_shift : 0;\n    for (int c = 0; c < output_depth; c += 4) {\n      const int8* shuffled_input_ptr =\n          reinterpret_cast<const int8*>(shuffled_input_workspace_data);\n      // Accumulation loop.\n      int32x4_t row_accum00 = vdupq_n_s32(0);\n      int32x4_t row_accum10 = vdupq_n_s32(0);\n      int32x4_t row_accum20 = vdupq_n_s32(0);\n      int32x4_t row_accum30 = vdupq_n_s32(0);\n      int32x4_t row_accum01 = vdupq_n_s32(0);\n      int32x4_t row_accum11 = vdupq_n_s32(0);\n      int32x4_t row_accum21 = vdupq_n_s32(0);\n      int32x4_t row_accum31 = vdupq_n_s32(0);\n      int32x4_t row_accum02 = vdupq_n_s32(0);\n      int32x4_t row_accum12 = vdupq_n_s32(0);\n      int32x4_t row_accum22 = vdupq_n_s32(0);\n      int32x4_t row_accum32 = vdupq_n_s32(0);\n      int32x4_t row_accum03 = vdupq_n_s32(0);\n      int32x4_t row_accum13 = vdupq_n_s32(0);\n      int32x4_t row_accum23 = vdupq_n_s32(0);\n      int32x4_t row_accum33 = vdupq_n_s32(0);\n      for (int d = 0; d < accum_depth; d += 16) {\n        int8x16_t weights0 = vld1q_s8(shuffled_weights_ptr + 0);\n        int8x16_t weights1 = vld1q_s8(shuffled_weights_ptr + 16);\n        int8x16_t weights2 = vld1q_s8(shuffled_weights_ptr + 32);\n        int8x16_t weights3 = vld1q_s8(shuffled_weights_ptr + 48);\n        shuffled_weights_ptr += 64;\n        int8x16_t input0 = vld1q_s8(shuffled_input_ptr + 0);\n        int8x16_t input1 = vld1q_s8(shuffled_input_ptr + 16);\n        int8x16_t input2 = vld1q_s8(shuffled_input_ptr + 32);\n        int8x16_t input3 = vld1q_s8(shuffled_input_ptr + 48);\n        shuffled_input_ptr += 64;\n        int16x8_t local_accum0, local_accum1, local_accum2, local_accum3;\n#define TFLITE_SHUFFLED_FC_ACCUM(B)                                           \\\n  local_accum0 = vmull_s8(vget_low_s8(weights0), vget_low_s8(input##B));      \\\n  local_accum1 = vmull_s8(vget_low_s8(weights1), vget_low_s8(input##B));      \\\n  local_accum2 = vmull_s8(vget_low_s8(weights2), vget_low_s8(input##B));      \\\n  local_accum3 = vmull_s8(vget_low_s8(weights3), vget_low_s8(input##B));      \\\n  local_accum0 =                                                              \\\n      vmlal_s8(local_accum0, vget_high_s8(weights0), vget_high_s8(input##B)); \\\n  local_accum1 =                                                              \\\n      vmlal_s8(local_accum1, vget_high_s8(weights1), vget_high_s8(input##B)); \\\n  local_accum2 =                                                              \\\n      vmlal_s8(local_accum2, vget_high_s8(weights2), vget_high_s8(input##B)); \\\n  local_accum3 =                                                              \\\n      vmlal_s8(local_accum3, vget_high_s8(weights3), vget_high_s8(input##B)); \\\n  row_accum0##B = vpadalq_s16(row_accum0##B, local_accum0);                   \\\n  row_accum1##B = vpadalq_s16(row_accum1##B, local_accum1);                   \\\n  row_accum2##B = vpadalq_s16(row_accum2##B, local_accum2);                   \\\n  row_accum3##B = vpadalq_s16(row_accum3##B, local_accum3);\n\n        TFLITE_SHUFFLED_FC_ACCUM(0)\n        TFLITE_SHUFFLED_FC_ACCUM(1)\n        TFLITE_SHUFFLED_FC_ACCUM(2)\n        TFLITE_SHUFFLED_FC_ACCUM(3)\n\n#undef TFLITE_SHUFFLED_FC_ACCUM\n      }\n      // Horizontally reduce accumulators\n\n#define TFLITE_SHUFFLED_FC_STORE(B)                                           \\\n  {                                                                           \\\n    int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,                 \\\n        pairwise_reduced_acc_2, pairwise_reduced_acc_3;                       \\\n    pairwise_reduced_acc_0 =                                                  \\\n        vpadd_s32(vget_low_s32(row_accum0##B), vget_high_s32(row_accum0##B)); \\\n    pairwise_reduced_acc_1 =                                                  \\\n        vpadd_s32(vget_low_s32(row_accum1##B), vget_high_s32(row_accum1##B)); \\\n    pairwise_reduced_acc_2 =                                                  \\\n        vpadd_s32(vget_low_s32(row_accum2##B), vget_high_s32(row_accum2##B)); \\\n    pairwise_reduced_acc_3 =                                                  \\\n        vpadd_s32(vget_low_s32(row_accum3##B), vget_high_s32(row_accum3##B)); \\\n    const int32x2_t reduced_lo =                                              \\\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);            \\\n    const int32x2_t reduced_hi =                                              \\\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);            \\\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);                 \\\n    int32x4_t bias_vec = vld1q_s32(bias_data + c);                            \\\n    reduced = vaddq_s32(reduced, bias_vec);                                   \\\n    reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));                    \\\n    reduced = vqrdmulhq_n_s32(reduced, output_multiplier);                    \\\n    using gemmlowp::RoundingDivideByPOT;                                      \\\n    reduced = RoundingDivideByPOT(reduced, right_shift);                      \\\n    const int16x4_t res16 = vqmovn_s32(reduced);                              \\\n    vst1_s16(output_data + c + B * output_stride, res16);                     \\\n  }\n\n      TFLITE_SHUFFLED_FC_STORE(0);\n      TFLITE_SHUFFLED_FC_STORE(1);\n      TFLITE_SHUFFLED_FC_STORE(2);\n      TFLITE_SHUFFLED_FC_STORE(3);\n\n#undef TFLITE_SHUFFLED_FC_STORE\n    }\n  } else {\n    TFLITE_DCHECK(false);\n    return;\n  }\n#else\n  if (batches == 1) {\n    int16* output_ptr = output_data;\n    // Shuffled weights have had their sign bit (0x80) pre-flipped (xor'd)\n    // so that just reinterpreting them as int8 values is equivalent to\n    // subtracting 128 from them, thus implementing for free the subtraction of\n    // the zero_point value 128.\n    const int8* shuffled_weights_ptr =\n        reinterpret_cast<const int8*>(shuffled_weights_data);\n    // Likewise, we preshuffled and pre-xored the input data above.\n    const int8* shuffled_input_data =\n        reinterpret_cast<const int8*>(shuffled_input_workspace_data);\n    for (int c = 0; c < output_depth; c += 4) {\n      // Internal accumulation.\n      // Initialize accumulator with the bias-value.\n      int32 accum[4] = {0};\n      // Accumulation loop.\n      for (int d = 0; d < accum_depth; d += 16) {\n        for (int i = 0; i < 4; i++) {\n          for (int j = 0; j < 16; j++) {\n            int8 input_val = shuffled_input_data[d + j];\n            int8 weights_val = *shuffled_weights_ptr++;\n            accum[i] += weights_val * input_val;\n          }\n        }\n      }\n      for (int i = 0; i < 4; i++) {\n        // Add bias value\n        int acc = accum[i] + bias_data[c + i];\n        // Down-scale the final int32 accumulator to the scale used by our\n        // (16-bit, typically 3 integer bits) fixed-point format. The quantized\n        // multiplier and shift here have been pre-computed offline\n        // (e.g. by toco).\n        acc =\n            MultiplyByQuantizedMultiplier(acc, output_multiplier, output_shift);\n        // Saturate, cast to int16, and store to output array.\n        acc = std::max(acc, -32768);\n        acc = std::min(acc, 32767);\n        output_ptr[c + i] = acc;\n      }\n    }\n  } else if (batches == 4) {\n    int16* output_ptr = output_data;\n    // Shuffled weights have had their sign bit (0x80) pre-flipped (xor'd)\n    // so that just reinterpreting them as int8 values is equivalent to\n    // subtracting 128 from them, thus implementing for free the subtraction of\n    // the zero_point value 128.\n    const int8* shuffled_weights_ptr =\n        reinterpret_cast<const int8*>(shuffled_weights_data);\n    // Likewise, we preshuffled and pre-xored the input data above.\n    const int8* shuffled_input_data =\n        reinterpret_cast<const int8*>(shuffled_input_workspace_data);\n    for (int c = 0; c < output_depth; c += 4) {\n      const int8* shuffled_input_ptr = shuffled_input_data;\n      // Accumulation loop.\n      // Internal accumulation.\n      // Initialize accumulator with the bias-value.\n      int32 accum[4][4];\n      for (int i = 0; i < 4; i++) {\n        for (int b = 0; b < 4; b++) {\n          accum[i][b] = 0;\n        }\n      }\n      for (int d = 0; d < accum_depth; d += 16) {\n        for (int i = 0; i < 4; i++) {\n          for (int b = 0; b < 4; b++) {\n            for (int j = 0; j < 16; j++) {\n              int8 input_val = shuffled_input_ptr[16 * b + j];\n              int8 weights_val = shuffled_weights_ptr[16 * i + j];\n              accum[i][b] += weights_val * input_val;\n            }\n          }\n        }\n        shuffled_input_ptr += 64;\n        shuffled_weights_ptr += 64;\n      }\n      for (int i = 0; i < 4; i++) {\n        for (int b = 0; b < 4; b++) {\n          // Add bias value\n          int acc = accum[i][b] + bias_data[c + i];\n          // Down-scale the final int32 accumulator to the scale used by our\n          // (16-bit, typically 3 integer bits) fixed-point format. The\n          // quantized multiplier and shift here have been pre-computed offline\n          // (e.g. by toco).\n          acc = MultiplyByQuantizedMultiplier(acc, output_multiplier,\n                                              output_shift);\n          // Saturate, cast to int16, and store to output array.\n          acc = std::max(acc, -32768);\n          acc = std::min(acc, 32767);\n          output_ptr[b * output_stride + c + i] = acc;\n        }\n      }\n    }\n  } else {\n    TFLITE_DCHECK(false);\n    return;\n  }\n#endif\n}\n\n// Wraps ShuffledFullyConnectedWorkerImpl into a Task class\n// to allow using gemmlowp's threadpool.\nstruct ShuffledFullyConnectedWorkerTask : cpu_backend_threadpool::Task {\n  ShuffledFullyConnectedWorkerTask(const uint8* input_data,\n                                   const int8* shuffled_weights_data,\n                                   int batches, int output_depth,\n                                   int output_stride, int accum_depth,\n                                   const int32* bias_data,\n                                   int32 output_multiplier, int output_shift,\n                                   int16* output_data)\n      : input_data_(input_data),\n        shuffled_weights_data_(shuffled_weights_data),\n        batches_(batches),\n        output_depth_(output_depth),\n        output_stride_(output_stride),\n        accum_depth_(accum_depth),\n        bias_data_(bias_data),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_data_(output_data) {}\n\n  void Run() override {\n    ShuffledFullyConnectedWorkerImpl(\n        input_data_, shuffled_weights_data_, batches_, output_depth_,\n        output_stride_, accum_depth_, bias_data_, output_multiplier_,\n        output_shift_, output_data_);\n  }\n\n  const uint8* input_data_;\n  const int8* shuffled_weights_data_;\n  int batches_;\n  int output_depth_;\n  int output_stride_;\n  int accum_depth_;\n  const int32* bias_data_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int16* output_data_;\n};\n\ninline void ShuffledFullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& weights_shape,\n    const uint8* shuffled_weights_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, uint8* shuffled_input_workspace_data,\n    CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"ShuffledFullyConnected/8bit\");\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_EQ(output_activation_min, -32768);\n  TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(weights_shape, weights_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = weights_shape.Dims(weights_dim_count - 1);\n  TFLITE_DCHECK((accum_depth % 16) == 0);\n  TFLITE_DCHECK((output_depth % 4) == 0);\n  // Shuffled weights have had their sign bit (0x80) pre-flipped (xor'd)\n  // so that just reinterpreting them as int8 values is equivalent to\n  // subtracting 128 from them, thus implementing for free the subtraction of\n  // the zero_point value 128.\n  const int8* int8_shuffled_weights_data =\n      reinterpret_cast<const int8*>(shuffled_weights_data);\n\n  // Shuffling and xoring of input activations into the workspace buffer\n  if (batches == 1) {\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (int i = 0; i < accum_depth; i += 16) {\n      uint8x16_t val = vld1q_u8(input_data + i);\n      val = veorq_u8(val, signbit);\n      vst1q_u8(shuffled_input_workspace_data + i, val);\n    }\n#else\n    for (int i = 0; i < accum_depth; i++) {\n      shuffled_input_workspace_data[i] = input_data[i] ^ 0x80;\n    }\n#endif\n  } else if (batches == 4) {\n    uint8* shuffled_input_workspace_ptr = shuffled_input_workspace_data;\n    int c = 0;\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (c = 0; c < accum_depth; c += 16) {\n      const uint8* src_data_ptr = input_data + c;\n      uint8x16_t val0 = vld1q_u8(src_data_ptr + 0 * accum_depth);\n      uint8x16_t val1 = vld1q_u8(src_data_ptr + 1 * accum_depth);\n      uint8x16_t val2 = vld1q_u8(src_data_ptr + 2 * accum_depth);\n      uint8x16_t val3 = vld1q_u8(src_data_ptr + 3 * accum_depth);\n      val0 = veorq_u8(val0, signbit);\n      val1 = veorq_u8(val1, signbit);\n      val2 = veorq_u8(val2, signbit);\n      val3 = veorq_u8(val3, signbit);\n      vst1q_u8(shuffled_input_workspace_ptr + 0, val0);\n      vst1q_u8(shuffled_input_workspace_ptr + 16, val1);\n      vst1q_u8(shuffled_input_workspace_ptr + 32, val2);\n      vst1q_u8(shuffled_input_workspace_ptr + 48, val3);\n      shuffled_input_workspace_ptr += 64;\n    }\n#else\n    for (c = 0; c < accum_depth; c += 16) {\n      for (int b = 0; b < 4; b++) {\n        const uint8* src_data_ptr = input_data + b * accum_depth + c;\n        for (int j = 0; j < 16; j++) {\n          uint8 src_val = *src_data_ptr++;\n          // Flip the sign bit, so that the kernel will only need to\n          // reinterpret these uint8 values as int8, getting for free the\n          // subtraction of the zero_point value 128.\n          uint8 dst_val = src_val ^ 0x80;\n          *shuffled_input_workspace_ptr++ = dst_val;\n        }\n      }\n    }\n#endif\n  } else {\n    TFLITE_DCHECK(false);\n    return;\n  }\n\n  static constexpr int kKernelRows = 4;\n  const int thread_count =\n      LegacyHowManyThreads<kKernelRows>(cpu_backend_context->max_num_threads(),\n                                        output_depth, batches, accum_depth);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    ShuffledFullyConnectedWorkerImpl(\n        shuffled_input_workspace_data, int8_shuffled_weights_data, batches,\n        output_depth, output_depth, accum_depth, bias_data, output_multiplier,\n        output_shift, output_data);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<ShuffledFullyConnectedWorkerTask> tasks;\n  // TODO(b/131746020) don't create new heap allocations every time.\n  // At least we make it a single heap allocation by using reserve().\n  tasks.reserve(thread_count);\n  const int kRowsPerWorker =\n      RoundUp<kKernelRows>(CeilQuotient(output_depth, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; i++) {\n    int row_end = std::min(output_depth, row_start + kRowsPerWorker);\n    tasks.emplace_back(shuffled_input_workspace_data,\n                       int8_shuffled_weights_data + row_start * accum_depth,\n                       batches, row_end - row_start, output_depth, accum_depth,\n                       bias_data + row_start, output_multiplier, output_shift,\n                       output_data + row_start);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_depth);\n  cpu_backend_threadpool::Execute(tasks.size(), tasks.data(),\n                                  cpu_backend_context);\n}\n\n#ifdef USE_NEON\n\ninline int32x4_t RoundToNearest(const float32x4_t input) {\n#if defined(__aarch64__) || defined(__SSSE3__)\n  // Note: vcvtnq_s32_f32 is not available in ARMv7\n  return vcvtnq_s32_f32(input);\n#else\n  static const float32x4_t zero_val_dup = vdupq_n_f32(0.0f);\n  static const float32x4_t point5_val_dup = vdupq_n_f32(0.5f);\n  static const float32x4_t minus_point5_val_dup = vdupq_n_f32(-0.5f);\n\n  const uint32x4_t mask = vcltq_f32(input, zero_val_dup);\n  const float32x4_t round =\n      vbslq_f32(mask, minus_point5_val_dup, point5_val_dup);\n  return vcvtq_s32_f32(vaddq_f32(input, round));\n#endif  // defined(__aarch64__) || defined(__SSSE3__)\n}\n\ninline uint32x4_t RoundToNearestUnsigned(const float32x4_t input) {\n#if defined(__aarch64__)\n  // Note that vcvtnq_u32_f32 is not available in ARMv7 or in arm_neon_sse.h.\n  return vcvtnq_u32_f32(input);\n#else\n  static const float32x4_t point5_val_dup = vdupq_n_f32(0.5f);\n\n  return vcvtq_u32_f32(vaddq_f32(input, point5_val_dup));\n#endif  // defined(__aarch64__)\n}\n\n#endif  // USE_NEON\n\ninline void MeanImpl(const tflite::MeanParams& op_params,\n                     const RuntimeShape& input_shape, const uint8_t* input_data,\n                     int32 multiplier, int32 shift, int32 bias,\n                     const RuntimeShape& output_shape, uint8_t* output_data,\n                     int start_depth, int end_depth) {\n  ruy::profiler::ScopeLabel label(\"Mean4D/Uint8/MeanImpl\");\n\n  // Current implementation only supports dimension equals 4 and simultaneous\n  // reduction over width and height.\n  const int output_batch = output_shape.Dims(0);\n  const int output_height = output_shape.Dims(2);\n  const int output_width = output_shape.Dims(2);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n\n  TFLITE_CHECK_EQ(op_params.axis_count, 2);\n  TFLITE_CHECK((op_params.axis[0] == 1 && op_params.axis[1] == 2) ||\n               (op_params.axis[0] == 2 && op_params.axis[1] == 1));\n  TFLITE_CHECK_EQ(output_height, 1);\n  TFLITE_CHECK_EQ(output_width, 1);\n\n  constexpr int32_t kMinValue = std::numeric_limits<uint8_t>::min();\n  constexpr int32_t kMaxValue = std::numeric_limits<uint8_t>::max();\n\n#ifdef USE_NEON\n  const int32x4_t bias_dup = vdupq_n_s32(bias);\n  const int32x4_t min_dup = vdupq_n_s32(kMinValue);\n  const int32x4_t max_dup = vdupq_n_s32(kMaxValue);\n#endif  // USE_NEON\n\n  for (int out_b = 0; out_b < output_batch; ++out_b) {\n    int out_d = start_depth;\n#ifdef USE_NEON\n\n    for (; out_d <= end_depth - 16; out_d += 16) {\n      int32x4x4_t temp_sum;\n      temp_sum.val[0] = vdupq_n_s32(0);\n      temp_sum.val[1] = vdupq_n_s32(0);\n      temp_sum.val[2] = vdupq_n_s32(0);\n      temp_sum.val[3] = vdupq_n_s32(0);\n      for (int in_h = 0; in_h < input_height; ++in_h) {\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          const uint8_t* input_data_ptr =\n              input_data + Offset(input_shape, out_b, in_h, in_w, out_d);\n          uint8x16_t input_data_val = vld1q_u8(input_data_ptr);\n\n          int16x8_t input_data_low_shift =\n              vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(input_data_val)));\n          int16x8_t input_data_high_shift =\n              vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(input_data_val)));\n\n          int32x4_t input_low_low =\n              vmovl_s16(vget_low_s16(input_data_low_shift));\n          int32x4_t input_high_low =\n              vmovl_s16(vget_high_s16(input_data_low_shift));\n          int32x4_t input_low_high =\n              vmovl_s16(vget_low_s16(input_data_high_shift));\n          int32x4_t input_high_high =\n              vmovl_s16(vget_high_s16(input_data_high_shift));\n\n          temp_sum.val[0] = vaddq_s32(temp_sum.val[0], input_low_low);\n          temp_sum.val[1] = vaddq_s32(temp_sum.val[1], input_high_low);\n          temp_sum.val[2] = vaddq_s32(temp_sum.val[2], input_low_high);\n          temp_sum.val[3] = vaddq_s32(temp_sum.val[3], input_high_high);\n        }\n      }\n\n      temp_sum =\n          MultiplyByQuantizedMultiplier4Rows(temp_sum, multiplier, shift);\n\n      temp_sum.val[0] = vaddq_s32(temp_sum.val[0], bias_dup);\n      temp_sum.val[1] = vaddq_s32(temp_sum.val[1], bias_dup);\n      temp_sum.val[2] = vaddq_s32(temp_sum.val[2], bias_dup);\n      temp_sum.val[3] = vaddq_s32(temp_sum.val[3], bias_dup);\n\n      temp_sum.val[0] = vminq_s32(vmaxq_s32(temp_sum.val[0], min_dup), max_dup);\n      temp_sum.val[1] = vminq_s32(vmaxq_s32(temp_sum.val[1], min_dup), max_dup);\n      temp_sum.val[2] = vminq_s32(vmaxq_s32(temp_sum.val[2], min_dup), max_dup);\n      temp_sum.val[3] = vminq_s32(vmaxq_s32(temp_sum.val[3], min_dup), max_dup);\n\n      uint16x4_t narrowed_low_low =\n          vmovn_u32(vreinterpretq_u32_s32(temp_sum.val[0]));\n      uint16x4_t narrowed_high_low =\n          vmovn_u32(vreinterpretq_u32_s32(temp_sum.val[1]));\n      uint16x4_t narrowed_low_high =\n          vmovn_u32(vreinterpretq_u32_s32(temp_sum.val[2]));\n      uint16x4_t narrowed_high_high =\n          vmovn_u32(vreinterpretq_u32_s32(temp_sum.val[3]));\n\n      uint16x8_t combined_low =\n          vcombine_u16(narrowed_low_low, narrowed_high_low);\n      uint16x8_t combined_high =\n          vcombine_u16(narrowed_low_high, narrowed_high_high);\n\n      uint8x8_t narrowed_low = vmovn_u16(combined_low);\n      uint8x8_t narrowed_high = vmovn_u16(combined_high);\n\n      uint8x16_t combined_output = vcombine_u8(narrowed_low, narrowed_high);\n\n      uint8_t* output_data_ptr =\n          output_data + Offset(output_shape, out_b, 0, 0, out_d);\n      vst1q_u8(output_data_ptr, combined_output);\n    }\n#endif  // USE_NEON\n\n    for (; out_d < end_depth; ++out_d) {\n      int acc = 0;\n      for (int in_h = 0; in_h < input_height; ++in_h) {\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          acc += input_data[Offset(input_shape, out_b, in_h, in_w, out_d)];\n        }\n      }\n\n      acc = MultiplyByQuantizedMultiplier(acc, multiplier, shift);\n      acc += bias;\n      acc = std::min(std::max(acc, kMinValue), kMaxValue);\n      output_data[Offset(output_shape, out_b, 0, 0, out_d)] =\n          static_cast<uint8_t>(acc);\n    }\n  }\n}\n\nstruct MeanWorkerTask : cpu_backend_threadpool::Task {\n  MeanWorkerTask(const tflite::MeanParams& op_params,\n                 const RuntimeShape& input_shape, const uint8_t* input_data,\n                 int32 multiplier, int32 shift, int32 bias,\n                 const RuntimeShape& output_shape, uint8_t* output_data,\n                 int start_height, int end_height)\n      : op_params(op_params),\n        input_shape(input_shape),\n        input_data(input_data),\n        multiplier(multiplier),\n        shift(shift),\n        bias(bias),\n        output_shape(output_shape),\n        output_data(output_data),\n        start_height(start_height),\n        end_height(end_height) {}\n\n  void Run() override {\n    MeanImpl(op_params, input_shape, input_data, multiplier, shift, bias,\n             output_shape, output_data, start_height, end_height);\n  }\n\n private:\n  const tflite::MeanParams& op_params;\n  const RuntimeShape& input_shape;\n  const uint8_t* input_data;\n  int32 multiplier;\n  int32 shift;\n  int32 bias;\n  const RuntimeShape& output_shape;\n  uint8_t* output_data;\n  int start_height;\n  int end_height;\n};\n\ninline void Mean(const tflite::MeanParams& op_params,\n                 const RuntimeShape& unextended_input_shape,\n                 const uint8_t* input_data, int32 input_zero_point,\n                 float input_scale, const RuntimeShape& unextended_output_shape,\n                 uint8_t* output_data, int32 output_zero_point,\n                 float output_scale, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"Mean4D/Uint8\");\n  // Current implementation only supports dimension equals 4 and simultaneous\n  // reduction over width and height.\n  TFLITE_CHECK_EQ(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_CHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_depth = output_shape.Dims(3);\n\n  TFLITE_CHECK_EQ(op_params.axis_count, 2);\n  TFLITE_CHECK((op_params.axis[0] == 1 && op_params.axis[1] == 2) ||\n               (op_params.axis[0] == 2 && op_params.axis[1] == 1));\n  TFLITE_CHECK_EQ(output_height, 1);\n  TFLITE_CHECK_EQ(output_width, 1);\n\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const float num_elements_in_axis = input_width * input_height;\n\n  float temp = input_zero_point * input_scale / output_scale;\n  temp = temp > 0 ? temp + 0.5f : temp - 0.5f;\n  int32_t bias = output_zero_point - static_cast<int32_t>(temp);\n  float real_scale = input_scale / (num_elements_in_axis * output_scale);\n\n  int32 multiplier, shift;\n  QuantizeMultiplier(real_scale, &multiplier, &shift);\n\n  constexpr int kMinDepthPerThread = 8;\n  int thread_count = output_depth / kMinDepthPerThread;\n  thread_count = thread_count > 0 ? thread_count : 1;\n  const int capped_thread_count =\n      std::min(thread_count, cpu_backend_context->max_num_threads());\n\n  if (capped_thread_count == 1) {\n    MeanImpl(op_params, input_shape, input_data, multiplier, shift, bias,\n             output_shape, output_data, 0, output_depth);\n  } else {\n    // Instead parallel for batch, we loop for the output_depth since batch\n    // is typical 1.\n    std::vector<MeanWorkerTask> tasks;\n    // TODO(b/131746020) don't create new heap allocations every time.\n    // At least we make it a single heap allocation by using reserve().\n    tasks.reserve(capped_thread_count);\n    int depth_start = 0;\n    for (int i = 0; i < capped_thread_count; ++i) {\n      // Try to distribute the tasks as even as possible.\n      int depth_end = depth_start +\n                      (output_depth - depth_start) / (capped_thread_count - i);\n      tasks.emplace_back(op_params, input_shape, input_data, multiplier, shift,\n                         bias, output_shape, output_data, depth_start,\n                         depth_end);\n      depth_start = depth_end;\n    }\n    cpu_backend_threadpool::Execute(tasks.size(), tasks.data(),\n                                    cpu_backend_context);\n  }\n}\n\ntemplate <typename T, typename U>\ninline bool MeanGeneral(const T* input_data, const int* input_dims,\n                        const int input_num_dims, T* output_data,\n                        const int* output_dims, const int output_num_dims,\n                        const int* axis, const int num_axis_dimensions,\n                        bool keep_dims, int* temp_index, int* resolved_axis,\n                        U* temp_sum) {\n  return reference_ops::Mean(input_data, input_dims, input_num_dims,\n                             output_data, output_dims, output_num_dims, axis,\n                             num_axis_dimensions, keep_dims, temp_index,\n                             resolved_axis, temp_sum);\n}\n\ntemplate <>\ninline bool MeanGeneral<float, float>(\n    const float* input_data, const int* input_dims, const int input_num_dims,\n    float* output_data, const int* output_dims, const int output_num_dims,\n    const int* axis, const int num_axis_dimensions, bool keep_dims,\n    int* temp_index, int* resolved_axis, float* temp_sum) {\n  // Handle reduce_mean for the last dimensions.\n  if (num_axis_dimensions == 1 && axis[0] == (input_num_dims - 1)) {\n    ruy::profiler::ScopeLabel label(\"MeanLastDim/Float\");\n    int output_size = 1;\n    for (int i = 0; i < input_num_dims - 1; ++i) {\n      output_size *= input_dims[i];\n    }\n    const int last_input_dim = input_dims[axis[0]];\n\n    // TODO(b/152563685): Consider use eigen to cover more general cases.\n    const MatrixMap<const float> in_mat(input_data, last_input_dim,\n                                        output_size);\n    VectorMap<float> out(output_data, output_size, 1);\n    out = (in_mat.array().colwise().sum()) / static_cast<float>(last_input_dim);\n    return true;\n  }\n\n  return reference_ops::Mean(input_data, input_dims, input_num_dims,\n                             output_data, output_dims, output_num_dims, axis,\n                             num_axis_dimensions, keep_dims, temp_index,\n                             resolved_axis, temp_sum);\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const float* input_data, const RuntimeShape& filter_shape,\n                 const float* filter_data, const RuntimeShape& bias_shape,\n                 const float* bias_data, const RuntimeShape& output_shape,\n                 float* output_data, const RuntimeShape& im2col_shape,\n                 float* im2col_data, CpuBackendContext* cpu_backend_context) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  ruy::profiler::ScopeLabel label(\"Conv\");\n\n  // NB: the float 0.0f value is represented by all zero bytes.\n  const uint8 float_zero_byte = 0x00;\n  const float* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    DilatedIm2col(params, float_zero_byte, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    Im2col(params, filter_height, filter_width, float_zero_byte, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  const int gemm_input_dims = gemm_input_shape->DimensionsCount();\n  int m = FlatSizeSkipDim(*gemm_input_shape, gemm_input_dims - 1);\n  int n = output_shape.Dims(3);\n  int k = gemm_input_shape->Dims(gemm_input_dims - 1);\n\n#if defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n  // The following code computes matrix multiplication c = a * transponse(b)\n  // with CBLAS, where:\n  // * `a` is a matrix with dimensions (m, k).\n  // * `b` is a matrix with dimensions (n, k), so transpose(b) is (k, n).\n  // * `c` is a matrix with dimensions (m, n).\n  // The naming of variables are aligned with CBLAS specification here.\n  const float* a = gemm_input_data;\n  const float* b = filter_data;\n  float* c = output_data;\n  // The stride of matrix a, b and c respectively.\n  int stride_a = k;\n  int stride_b = k;\n  int stride_c = n;\n\n  cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans, m, n, k, 1.0f, a,\n              stride_a, b, stride_b, 0.0f, c, stride_c);\n  optimized_ops::AddBiasAndEvalActivationFunction(\n      output_activation_min, output_activation_max, bias_shape, bias_data,\n      output_shape, output_data);\n#else\n  // When an optimized CBLAS implementation is not available, fall back\n  // to using cpu_backend_gemm.\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = n;\n  lhs_params.cols = k;\n  cpu_backend_gemm::MatrixParams<float> rhs_params;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.rows = k;\n  rhs_params.cols = m;\n  cpu_backend_gemm::MatrixParams<float> dst_params;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.rows = n;\n  dst_params.cols = m;\n  cpu_backend_gemm::GemmParams<float, float> gemm_params;\n  gemm_params.bias = bias_data;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, gemm_input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n#endif  //  defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n}\n\ninline void HybridConv(const ConvParams& params, float* scaling_factors_ptr,\n                       const RuntimeShape& input_shape,\n                       const int8_t* input_data,\n                       const RuntimeShape& filter_shape,\n                       const int8_t* filter_data,\n                       const RuntimeShape& bias_shape, const float* bias_data,\n                       const RuntimeShape& accum_scratch_shape,\n                       int32_t* accum_scratch, const RuntimeShape& output_shape,\n                       float* output_data, const RuntimeShape& im2col_shape,\n                       int8_t* im2col_data, CpuBackendContext* context) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int batch_size = input_shape.Dims(0);\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n\n  const int input_zero_point = 0;\n  const int8_t* gemm_input_data = nullptr;\n  int num_input;\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n\n  if (need_dilated_im2col) {\n    DilatedIm2col(params, input_zero_point, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    num_input = im2col_shape.FlatSize();\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    // symmetric quantization assumes zero point of 0.\n\n    Im2col(params, filter_height, filter_width, input_zero_point, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    num_input = im2col_shape.FlatSize();\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    num_input = input_shape.FlatSize();\n  }\n\n  // Flatten 4D matrices into 2D matrices for matrix multiplication.\n\n  // Flatten so that each filter has its own row.\n  const int filter_rows = filter_shape.Dims(0);\n  const int filter_cols = FlatSizeSkipDim(filter_shape, 0);\n\n  // In MatrixBatchVectorMultiplyAccumulate, each output value is the\n  // dot product of one row of the first matrix with one row of the second\n  // matrix. Therefore, the number of cols in each matrix are equivalent.\n  //\n  // After Im2Col, each input patch becomes a row.\n  const int gemm_input_cols = filter_cols;\n  const int gemm_input_rows = num_input / gemm_input_cols;\n\n  const int output_cols = output_shape.Dims(3);\n  const int output_rows = FlatSizeSkipDim(output_shape, 3);\n  TFLITE_DCHECK_EQ(output_cols, filter_rows);\n  TFLITE_DCHECK_EQ(output_rows, gemm_input_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_cols);\n\n  // MatrixBatchVectorMultiplyAccumulate assumes that each row of the second\n  // input matrix has its own scale factor. This code duplicates the scale\n  // factors for each row in the same batch.\n  const int rows_per_batch = gemm_input_rows / batch_size;\n  for (int i = gemm_input_rows - 1; i >= 0; --i) {\n    scaling_factors_ptr[i] = scaling_factors_ptr[i / rows_per_batch];\n  }\n\n  std::fill_n(output_data, output_rows * output_cols, 0.0f);\n\n  // The scratch buffer must have the same size as the output.\n  TFLITE_DCHECK_EQ(accum_scratch_shape.FlatSize(), output_shape.FlatSize());\n  tensor_utils::MatrixBatchVectorMultiplyAccumulate(\n      filter_data, filter_rows, filter_cols, gemm_input_data,\n      scaling_factors_ptr, /*n_batch=*/gemm_input_rows, accum_scratch,\n      output_data, context);\n  AddBiasAndEvalActivationFunction(output_activation_min, output_activation_max,\n                                   bias_shape, bias_data, output_shape,\n                                   output_data);\n}\n\ninline void HybridConvPerChannel(\n    const ConvParams& params, float* scaling_factors_ptr,\n    const RuntimeShape& input_shape, const int8_t* input_data,\n    const RuntimeShape& filter_shape, const int8_t* filter_data,\n    const RuntimeShape& bias_shape, const float* bias_data,\n    const RuntimeShape& output_shape, float* output_data,\n    const RuntimeShape& im2col_shape, int8_t* im2col_data,\n    const float* per_channel_scale, int32_t* input_offset,\n    const RuntimeShape& scratch_shape, int32_t* scratch, int32_t* row_sums,\n    bool* compute_row_sums, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"ConvHybridPerChannel\");\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int8* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n\n  const int batch_size = input_shape.Dims(0);\n\n  if (need_dilated_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    optimized_ops::DilatedIm2col(params, input_shape, input_data, filter_shape,\n                                 output_shape, im2col_data, input_offset,\n                                 batch_size);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    Im2col(params, filter_height, filter_width, input_offset, batch_size,\n           input_shape, input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  const int filter_rows = filter_shape.Dims(0);\n  const int filter_cols = FlatSizeSkipDim(filter_shape, 0);\n\n  const int gemm_input_rows = gemm_input_shape->Dims(3);\n  const int gemm_input_cols = FlatSizeSkipDim(*gemm_input_shape, 3);\n  const int output_rows = output_shape.Dims(3);\n  const int output_cols =\n      output_shape.Dims(0) * output_shape.Dims(1) * output_shape.Dims(2);\n\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, gemm_input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, gemm_input_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n  TFLITE_DCHECK_EQ(scratch_shape.FlatSize(), output_shape.FlatSize());\n  if (!compute_row_sums || *compute_row_sums) {\n    tensor_utils::ReductionSumVector(filter_data, row_sums, filter_rows,\n                                     filter_cols);\n    if (compute_row_sums) {\n      *compute_row_sums = false;\n    }\n  }\n\n  cpu_backend_gemm::MatrixParams<int8> lhs_params;\n  lhs_params.rows = filter_rows;\n  lhs_params.cols = filter_cols;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n\n  cpu_backend_gemm::MatrixParams<int8> rhs_params;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.rows = gemm_input_rows;\n  rhs_params.cols = gemm_input_cols;\n\n  cpu_backend_gemm::MatrixParams<int32> dst_params;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.rows = output_rows;\n  dst_params.cols = output_cols;\n\n  // TODO(b/149003801): Use hybrid gemm once supported in Ruy.\n  cpu_backend_gemm::GemmParams<int32_t, int32_t> gemm_params;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, gemm_input_data,\n                         dst_params, scratch, gemm_params, cpu_backend_context);\n\n  MatrixMap<float> out_mat(output_data, filter_rows, output_cols);\n  MatrixMap<int32_t> in_mat(scratch, filter_rows, output_cols);\n  VectorMap<const float> bias_data_vec(bias_data, filter_rows, 1);\n  VectorMap<int32_t> row_sums_vec(row_sums, filter_rows, 1);\n  VectorMap<const float> per_channel_scale_vec(per_channel_scale, filter_rows,\n                                               1);\n  const int cols_per_batch = output_cols / batch_size;\n  for (int c = 0; c < output_cols; c++) {\n    const int b = c / cols_per_batch;\n    const float input_scale = scaling_factors_ptr[b];\n    const int32_t zero_point = input_offset[b];\n    out_mat.col(c) =\n        (((in_mat.col(c) - (row_sums_vec * zero_point))\n              .cast<float>()\n              .cwiseProduct((per_channel_scale_vec * input_scale))) +\n         bias_data_vec)\n            .cwiseMin(params.float_activation_max)\n            .cwiseMax(params.float_activation_min);\n  }\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const uint8* input_data, const RuntimeShape& filter_shape,\n                 const uint8* filter_data, const RuntimeShape& bias_shape,\n                 const int32* bias_data, const RuntimeShape& output_shape,\n                 uint8* output_data, const RuntimeShape& im2col_shape,\n                 uint8* im2col_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"Conv/8bit\");\n\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const uint8* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    DilatedIm2col(params, input_zero_point, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    Im2col(params, filter_height, filter_width, input_zero_point, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  const int gemm_input_rows = gemm_input_shape->Dims(3);\n  // Using FlatSizeSkipDim causes segfault in some contexts (see b/79927784).\n  // The root cause has not yet been identified though. Same applies below for\n  // the other calls commented out. This is a partial rollback of cl/196819423.\n  // const int gemm_input_cols = FlatSizeSkipDim(*gemm_input_shape, 3);\n  const int gemm_input_cols = gemm_input_shape->Dims(0) *\n                              gemm_input_shape->Dims(1) *\n                              gemm_input_shape->Dims(2);\n  const int filter_rows = filter_shape.Dims(0);\n  // See b/79927784.\n  // const int filter_cols = FlatSizeSkipDim(filter_shape, 0);\n  const int filter_cols =\n      filter_shape.Dims(1) * filter_shape.Dims(2) * filter_shape.Dims(3);\n  const int output_rows = output_shape.Dims(3);\n  // See b/79927784.\n  // const int output_cols = FlatSizeSkipDim(output_shape, 3);\n  const int output_cols =\n      output_shape.Dims(0) * output_shape.Dims(1) * output_shape.Dims(2);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, gemm_input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, gemm_input_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n  cpu_backend_gemm::MatrixParams<uint8> lhs_params;\n  lhs_params.rows = filter_rows;\n  lhs_params.cols = filter_cols;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.zero_point = -filter_offset;\n  cpu_backend_gemm::MatrixParams<uint8> rhs_params;\n  rhs_params.rows = gemm_input_rows;\n  rhs_params.cols = gemm_input_cols;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.zero_point = -input_offset;\n  cpu_backend_gemm::MatrixParams<uint8> dst_params;\n  dst_params.rows = output_rows;\n  dst_params.cols = output_cols;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.zero_point = output_offset;\n  cpu_backend_gemm::GemmParams<int32, uint8> gemm_params;\n  gemm_params.bias = bias_data;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  gemm_params.multiplier_fixedpoint = output_multiplier;\n  gemm_params.multiplier_exponent = output_shift;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, gemm_input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\ntemplate <typename T>\ninline void DepthToSpace(const tflite::DepthToSpaceParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const T* input_data,\n                         const RuntimeShape& unextended_output_shape,\n                         T* output_data) {\n  ruy::profiler::ScopeLabel label(\"DepthToSpace\");\n\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  const int input_depth = input_shape.Dims(3);\n  const int input_width = input_shape.Dims(2);\n  const int input_height = input_shape.Dims(1);\n\n  const int output_depth = output_shape.Dims(3);\n  const int batch_size = output_shape.Dims(0);\n\n  // Number of continuous values that we can copy in one interation.\n  const int stride = op_params.block_size * output_depth;\n\n  for (int batch = 0; batch < batch_size; ++batch) {\n    for (int in_h = 0; in_h < input_height; ++in_h) {\n      const T* input_ptr = input_data + Offset(input_shape, batch, in_h, 0, 0);\n      for (int offset_h = 0; offset_h < op_params.block_size; ++offset_h) {\n        const T* src = input_ptr;\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          memcpy(output_data, src, stride * sizeof(T));\n          output_data += stride;\n          src += input_depth;\n        }\n        input_ptr += stride;\n      }\n    }\n  }\n}\n\ntemplate <typename T>\ninline void SpaceToDepth(const tflite::SpaceToDepthParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const T* input_data,\n                         const RuntimeShape& unextended_output_shape,\n                         T* output_data) {\n  ruy::profiler::ScopeLabel label(\"SpaceToDepth\");\n\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  const int output_depth = output_shape.Dims(3);\n  const int output_width = output_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n\n  const int input_depth = input_shape.Dims(3);\n  const int batch_size = input_shape.Dims(0);\n\n  // Number of continuous values that we can copy in one interation.\n  const int stride = op_params.block_size * input_depth;\n\n  for (int batch = 0; batch < batch_size; ++batch) {\n    for (int out_h = 0; out_h < output_height; ++out_h) {\n      T* output_ptr = output_data + Offset(output_shape, batch, out_h, 0, 0);\n      for (int offset_h = 0; offset_h < op_params.block_size; ++offset_h) {\n        T* dst = output_ptr;\n        for (int out_w = 0; out_w < output_width; ++out_w) {\n          memcpy(dst, input_data, stride * sizeof(T));\n          input_data += stride;\n          dst += output_depth;\n        }\n        output_ptr += stride;\n      }\n    }\n  }\n}\n\ninline void Relu(const RuntimeShape& input_shape, const float* input_data,\n                 const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Relu (not fused)\");\n\n  const auto input = MapAsVector(input_data, input_shape);\n  auto output = MapAsVector(output_data, output_shape);\n  output = input.cwiseMax(0.0f);\n}\n\ninline void L2Normalization(const tflite::L2NormalizationParams& op_params,\n                            const RuntimeShape& input_shape,\n                            const float* input_data,\n                            const RuntimeShape& output_shape,\n                            float* output_data, float epsilon = 1e-6) {\n  ruy::profiler::ScopeLabel label(\"L2Normalization\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n  for (int i = 0; i < outer_size; ++i) {\n    float squared_l2_norm = 0;\n    for (int c = 0; c < depth; ++c) {\n      const float val = input_data[c];\n      squared_l2_norm += val * val;\n    }\n    float l2_norm = std::sqrt(squared_l2_norm);\n    l2_norm = std::max(l2_norm, epsilon);\n    for (int c = 0; c < depth; ++c) {\n      *output_data = *input_data / l2_norm;\n      ++output_data;\n      ++input_data;\n    }\n  }\n}\n\ninline void L2Normalization(const tflite::L2NormalizationParams& op_params,\n                            const RuntimeShape& input_shape,\n                            const uint8* input_data,\n                            const RuntimeShape& output_shape,\n                            uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"L2Normalization/8bit\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int32 input_zero_point = op_params.input_zero_point;\n  for (int i = 0; i < outer_size; ++i) {\n    int32 square_l2_norm = 0;\n    for (int c = 0; c < depth; c++) {\n      // Note that input_data advances by depth in the second pass below.\n      int32 diff = input_data[c] - input_zero_point;\n      square_l2_norm += diff * diff;\n    }\n    // TODO(b/29395854): add clamping to TOCO and TF Lite kernel\n    // for all zero tensors in the input_data\n    int32 inv_l2norm_multiplier;\n    int inv_l2norm_shift;\n    GetInvSqrtQuantizedMultiplierExp(square_l2_norm, kReverseShift,\n                                     &inv_l2norm_multiplier, &inv_l2norm_shift);\n\n    for (int c = 0; c < depth; c++) {\n      int32 diff = *input_data - input_zero_point;\n      int32 rescaled_diff = MultiplyByQuantizedMultiplierSmallerThanOneExp(\n          128 * diff, inv_l2norm_multiplier, inv_l2norm_shift);\n      int32 unclamped_output_val = 128 + rescaled_diff;\n      int32 output_val = std::min(255, std::max(0, unclamped_output_val));\n      *output_data = static_cast<uint8>(output_val);\n      ++input_data;\n      ++output_data;\n    }\n  }\n}\n\ninline void AddElementwise(int size, const ArithmeticParams& params,\n                           const float* input1_data, const float* input2_data,\n                           float* output_data) {\n  int i = 0;\n\n#ifdef USE_NEON\n  const auto activation_min = vdupq_n_f32(params.float_activation_min);\n  const auto activation_max = vdupq_n_f32(params.float_activation_max);\n  for (; i <= size - 16; i += 16) {\n    auto a10 = vld1q_f32(input1_data + i);\n    auto a11 = vld1q_f32(input1_data + i + 4);\n    auto a12 = vld1q_f32(input1_data + i + 8);\n    auto a13 = vld1q_f32(input1_data + i + 12);\n    auto a20 = vld1q_f32(input2_data + i);\n    auto a21 = vld1q_f32(input2_data + i + 4);\n    auto a22 = vld1q_f32(input2_data + i + 8);\n    auto a23 = vld1q_f32(input2_data + i + 12);\n    auto x0 = vaddq_f32(a10, a20);\n    auto x1 = vaddq_f32(a11, a21);\n    auto x2 = vaddq_f32(a12, a22);\n    auto x3 = vaddq_f32(a13, a23);\n    x0 = vmaxq_f32(activation_min, x0);\n    x1 = vmaxq_f32(activation_min, x1);\n    x2 = vmaxq_f32(activation_min, x2);\n    x3 = vmaxq_f32(activation_min, x3);\n    x0 = vminq_f32(activation_max, x0);\n    x1 = vminq_f32(activation_max, x1);\n    x2 = vminq_f32(activation_max, x2);\n    x3 = vminq_f32(activation_max, x3);\n    vst1q_f32(output_data + i, x0);\n    vst1q_f32(output_data + i + 4, x1);\n    vst1q_f32(output_data + i + 8, x2);\n    vst1q_f32(output_data + i + 12, x3);\n  }\n  for (; i <= size - 4; i += 4) {\n    auto a1 = vld1q_f32(input1_data + i);\n    auto a2 = vld1q_f32(input2_data + i);\n    auto x = vaddq_f32(a1, a2);\n    x = vmaxq_f32(activation_min, x);\n    x = vminq_f32(activation_max, x);\n    vst1q_f32(output_data + i, x);\n  }\n#endif  // NEON\n\n  for (; i < size; i++) {\n    auto x = input1_data[i] + input2_data[i];\n    output_data[i] = ActivationFunctionWithMinMax(\n        x, params.float_activation_min, params.float_activation_max);\n  }\n}\n\ninline void Add(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const float* input1_data,\n                const RuntimeShape& input2_shape, const float* input2_data,\n                const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Add\");\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  AddElementwise(flat_size, params, input1_data, input2_data, output_data);\n}\n\n// Element-wise add that can often be used for inner loop of broadcast add as\n// well as the non-broadcast add.\ninline void AddElementwise(int size, const ArithmeticParams& params,\n                           const uint8* input1_data, const uint8* input2_data,\n                           uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"AddElementwise/8bit\");\n  int i = 0;\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n#ifdef USE_NEON\n  const uint8x8_t output_activation_min_vector =\n      vdup_n_u8(params.quantized_activation_min);\n  const uint8x8_t output_activation_max_vector =\n      vdup_n_u8(params.quantized_activation_max);\n  for (; i <= size - 8; i += 8) {\n    const uint8x8_t input1_val_original = vld1_u8(input1_data + i);\n    const uint8x8_t input2_val_original = vld1_u8(input2_data + i);\n    const int16x8_t input1_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input1_val_original));\n    const int16x8_t input2_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input2_val_original));\n    const int16x8_t input1_val =\n        vaddq_s16(input1_val_s16, vdupq_n_s16(params.input1_offset));\n    const int16x8_t input2_val =\n        vaddq_s16(input2_val_s16, vdupq_n_s16(params.input2_offset));\n    const int16x4_t input1_val_high = vget_high_s16(input1_val);\n    const int16x4_t input1_val_low = vget_low_s16(input1_val);\n    const int16x4_t input2_val_high = vget_high_s16(input2_val);\n    const int16x4_t input2_val_low = vget_low_s16(input2_val);\n    int32x4_t x11 = vmovl_s16(input1_val_low);\n    int32x4_t x12 = vmovl_s16(input1_val_high);\n    int32x4_t x21 = vmovl_s16(input2_val_low);\n    int32x4_t x22 = vmovl_s16(input2_val_high);\n    const int32x4_t left_shift_dup = vdupq_n_s32(params.left_shift);\n    x11 = vshlq_s32(x11, left_shift_dup);\n    x12 = vshlq_s32(x12, left_shift_dup);\n    x21 = vshlq_s32(x21, left_shift_dup);\n    x22 = vshlq_s32(x22, left_shift_dup);\n    x11 = vqrdmulhq_n_s32(x11, params.input1_multiplier);\n    x12 = vqrdmulhq_n_s32(x12, params.input1_multiplier);\n    x21 = vqrdmulhq_n_s32(x21, params.input2_multiplier);\n    x22 = vqrdmulhq_n_s32(x22, params.input2_multiplier);\n    const int32x4_t input1_shift_dup = vdupq_n_s32(params.input1_shift);\n    const int32x4_t input2_shift_dup = vdupq_n_s32(params.input2_shift);\n    x11 = vshlq_s32(x11, input1_shift_dup);\n    x12 = vshlq_s32(x12, input1_shift_dup);\n    x21 = vshlq_s32(x21, input2_shift_dup);\n    x22 = vshlq_s32(x22, input2_shift_dup);\n    int32x4_t s1 = vaddq_s32(x11, x21);\n    int32x4_t s2 = vaddq_s32(x12, x22);\n    s1 = vqrdmulhq_n_s32(s1, params.output_multiplier);\n    s2 = vqrdmulhq_n_s32(s2, params.output_multiplier);\n    using gemmlowp::RoundingDivideByPOT;\n    s1 = RoundingDivideByPOT(s1, -params.output_shift);\n    s2 = RoundingDivideByPOT(s2, -params.output_shift);\n    const int16x4_t s1_narrowed = vmovn_s32(s1);\n    const int16x4_t s2_narrowed = vmovn_s32(s2);\n    const int16x8_t s = vaddq_s16(vcombine_s16(s1_narrowed, s2_narrowed),\n                                  vdupq_n_s16(params.output_offset));\n    const uint8x8_t clamped =\n        vmax_u8(output_activation_min_vector,\n                vmin_u8(output_activation_max_vector, vqmovun_s16(s)));\n    vst1_u8(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    const int32 input1_val = params.input1_offset + input1_data[i];\n    const int32 input2_val = params.input2_offset + input2_data[i];\n    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);\n    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);\n    const int32 scaled_input1_val =\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            shifted_input1_val, params.input1_multiplier, params.input1_shift);\n    const int32 scaled_input2_val =\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            shifted_input2_val, params.input2_multiplier, params.input2_shift);\n    const int32 raw_sum = scaled_input1_val + scaled_input2_val;\n    const int32 raw_output =\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            raw_sum, params.output_multiplier, params.output_shift) +\n        params.output_offset;\n    const int32 clamped_output =\n        std::min(params.quantized_activation_max,\n                 std::max(params.quantized_activation_min, raw_output));\n    output_data[i] = static_cast<uint8>(clamped_output);\n  }\n}\n\n// Scalar-broadcast add that can be used for inner loop of more general\n// broadcast add, so that, for example, scalar-broadcast with batch will still\n// be fast.\ninline void AddScalarBroadcast(int size, const ArithmeticParams& params,\n                               uint8 input1_data, const uint8* input2_data,\n                               uint8* output_data) {\n  using gemmlowp::RoundingDivideByPOT;\n\n  ruy::profiler::ScopeLabel label(\"AddScalarBroadcast/8bit\");\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n\n  int i = 0;\n\n#ifdef USE_NEON\n  const int32x4_t left_shift_dup = vdupq_n_s32(params.left_shift);\n  const uint8x8_t output_activation_min_vector =\n      vdup_n_u8(params.quantized_activation_min);\n  const uint8x8_t output_activation_max_vector =\n      vdup_n_u8(params.quantized_activation_max);\n\n  // Process broadcast scalar.\n  const uint8x8_t input1_val_original = vdup_n_u8(input1_data);\n  const int16x8_t input1_val_s16 =\n      vreinterpretq_s16_u16(vmovl_u8(input1_val_original));\n  const int16x8_t input1_val =\n      vaddq_s16(input1_val_s16, vdupq_n_s16(params.input1_offset));\n  const int16x4_t input1_val_high = vget_high_s16(input1_val);\n  const int16x4_t input1_val_low = vget_low_s16(input1_val);\n  int32x4_t x11 = vmovl_s16(input1_val_low);\n  int32x4_t x12 = vmovl_s16(input1_val_high);\n  x11 = vshlq_s32(x11, left_shift_dup);\n  x12 = vshlq_s32(x12, left_shift_dup);\n  x11 = vqrdmulhq_n_s32(x11, params.input1_multiplier);\n  x12 = vqrdmulhq_n_s32(x12, params.input1_multiplier);\n  const int32x4_t input1_shift_dup = vdupq_n_s32(params.input1_shift);\n  x11 = vshlq_s32(x11, input1_shift_dup);\n  x12 = vshlq_s32(x12, input1_shift_dup);\n\n  for (; i <= size - 8; i += 8) {\n    const uint8x8_t input2_val_original = vld1_u8(input2_data + i);\n    const int16x8_t input2_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input2_val_original));\n    const int16x8_t input2_val =\n        vaddq_s16(input2_val_s16, vdupq_n_s16(params.input2_offset));\n    const int16x4_t input2_val_high = vget_high_s16(input2_val);\n    const int16x4_t input2_val_low = vget_low_s16(input2_val);\n    int32x4_t x21 = vmovl_s16(input2_val_low);\n    int32x4_t x22 = vmovl_s16(input2_val_high);\n    x21 = vshlq_s32(x21, left_shift_dup);\n    x22 = vshlq_s32(x22, left_shift_dup);\n    x21 = vqrdmulhq_n_s32(x21, params.input2_multiplier);\n    x22 = vqrdmulhq_n_s32(x22, params.input2_multiplier);\n    const int32x4_t input2_shift_dup = vdupq_n_s32(params.input2_shift);\n    x21 = vshlq_s32(x21, input2_shift_dup);\n    x22 = vshlq_s32(x22, input2_shift_dup);\n    int32x4_t s1 = vaddq_s32(x11, x21);\n    int32x4_t s2 = vaddq_s32(x12, x22);\n    s1 = vqrdmulhq_n_s32(s1, params.output_multiplier);\n    s2 = vqrdmulhq_n_s32(s2, params.output_multiplier);\n    s1 = RoundingDivideByPOT(s1, -params.output_shift);\n    s2 = RoundingDivideByPOT(s2, -params.output_shift);\n    const int16x4_t s1_narrowed = vmovn_s32(s1);\n    const int16x4_t s2_narrowed = vmovn_s32(s2);\n    const int16x8_t s = vaddq_s16(vcombine_s16(s1_narrowed, s2_narrowed),\n                                  vdupq_n_s16(params.output_offset));\n    const uint8x8_t clamped =\n        vmax_u8(output_activation_min_vector,\n                vmin_u8(output_activation_max_vector, vqmovun_s16(s)));\n    vst1_u8(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  if (i < size) {\n    // Process broadcast scalar.\n    const int32 input1_val = params.input1_offset + input1_data;\n    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);\n    const int32 scaled_input1_val =\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            shifted_input1_val, params.input1_multiplier, params.input1_shift);\n\n    for (; i < size; ++i) {\n      const int32 input2_val = params.input2_offset + input2_data[i];\n      const int32 shifted_input2_val = input2_val * (1 << params.left_shift);\n      const int32 scaled_input2_val =\n          MultiplyByQuantizedMultiplierSmallerThanOneExp(\n              shifted_input2_val, params.input2_multiplier,\n              params.input2_shift);\n      const int32 raw_sum = scaled_input1_val + scaled_input2_val;\n      const int32 raw_output =\n          MultiplyByQuantizedMultiplierSmallerThanOneExp(\n              raw_sum, params.output_multiplier, params.output_shift) +\n          params.output_offset;\n      const int32 clamped_output =\n          std::min(params.quantized_activation_max,\n                   std::max(params.quantized_activation_min, raw_output));\n      output_data[i] = static_cast<uint8>(clamped_output);\n    }\n  }\n}\n\n// Scalar-broadcast add that can be used for inner loop of more general\n// broadcast add, so that, for example, scalar-broadcast with batch will still\n// be fast.\ninline void AddScalarBroadcast(int size, const ArithmeticParams& params,\n                               float broadcast_value, const float* input2_data,\n                               float* output_data) {\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t output_activation_min_vector =\n      vdupq_n_f32(params.float_activation_min);\n  const float32x4_t output_activation_max_vector =\n      vdupq_n_f32(params.float_activation_max);\n  const float32x4_t broadcast_value_dup = vdupq_n_f32(broadcast_value);\n  for (; i <= size - 4; i += 4) {\n    const float32x4_t input2_val_original = vld1q_f32(input2_data + i);\n\n    const float32x4_t output =\n        vaddq_f32(input2_val_original, broadcast_value_dup);\n\n    const float32x4_t clamped =\n        vmaxq_f32(output_activation_min_vector,\n                  vminq_f32(output_activation_max_vector, output));\n    vst1q_f32(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    auto x = broadcast_value + input2_data[i];\n    output_data[i] = ActivationFunctionWithMinMax(\n        x, params.float_activation_min, params.float_activation_max);\n  }\n}\n\ninline void Add(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const uint8* input1_data,\n                const RuntimeShape& input2_shape, const uint8* input2_data,\n                const RuntimeShape& output_shape, uint8* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  ruy::profiler::ScopeLabel label(\"Add/8bit\");\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n  AddElementwise(flat_size, params, input1_data, input2_data, output_data);\n}\n\ninline void Add(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Add/Int16\");\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n\n  const int input1_shift = params.input1_shift;\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  const int16 output_activation_min = params.quantized_activation_min;\n  const int16 output_activation_max = params.quantized_activation_max;\n\n  TFLITE_DCHECK(input1_shift == 0 || params.input2_shift == 0);\n  TFLITE_DCHECK_LE(input1_shift, 0);\n  TFLITE_DCHECK_LE(params.input2_shift, 0);\n  const int16* not_shift_input = input1_shift == 0 ? input1_data : input2_data;\n  const int16* shift_input = input1_shift == 0 ? input2_data : input1_data;\n  const int input_right_shift =\n      input1_shift == 0 ? -params.input2_shift : -input1_shift;\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 input_ready_scaled = F0::FromRaw(not_shift_input[i]);\n    F0 scaled_input = F0::FromRaw(\n        gemmlowp::RoundingDivideByPOT(shift_input[i], input_right_shift));\n    F0 result = gemmlowp::SaturatingAdd(scaled_input, input_ready_scaled);\n    const int16 raw_output = result.raw();\n    const int16 clamped_output = std::min(\n        output_activation_max, std::max(output_activation_min, raw_output));\n    output_data[i] = clamped_output;\n  }\n}\n\ntemplate <typename T>\ninline typename std::enable_if<is_int32_or_int64<T>::value, void>::type Add(\n    const ArithmeticParams& params, const RuntimeShape& input1_shape,\n    const T* input1_data, const RuntimeShape& input2_shape,\n    const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Add/int32or64\");\n\n  T activation_min, activation_max;\n  GetActivationParams(params, &activation_min, &activation_max);\n\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  if (input1_shape == input2_shape) {\n    output_map.array() = (input1_map.array() + input2_map.array())\n                             .cwiseMax(activation_min)\n                             .cwiseMin(activation_max);\n  } else if (input2_shape.FlatSize() == 1) {\n    auto scalar = input2_data[0];\n    output_map.array() = (input1_map.array() + scalar)\n                             .cwiseMax(activation_min)\n                             .cwiseMin(activation_max);\n  } else if (input1_shape.FlatSize() == 1) {\n    auto scalar = input1_data[0];\n    output_map.array() = (scalar + input2_map.array())\n                             .cwiseMax(activation_min)\n                             .cwiseMin(activation_max);\n  } else {\n    reference_ops::BroadcastAdd4DSlow<T>(params, input1_shape, input1_data,\n                                         input2_shape, input2_data,\n                                         output_shape, output_data);\n  }\n}\n\ntemplate <typename T>\ninline void BroadcastAddDispatch(\n    const ArithmeticParams& params, const RuntimeShape& input1_shape,\n    const T* input1_data, const RuntimeShape& input2_shape,\n    const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return BroadcastAdd4DSlow(params, input1_shape, input1_data, input2_shape,\n                              input2_data, output_shape, output_data);\n  }\n\n  BinaryBroadcastFiveFold(\n      params, input1_shape, input1_data, input2_shape, input2_data,\n      output_shape, output_data,\n      static_cast<void (*)(int, const ArithmeticParams&, const T*, const T*,\n                           T*)>(AddElementwise),\n      static_cast<void (*)(int, const ArithmeticParams&, T, const T*, T*)>(\n          AddScalarBroadcast));\n}\n\ninline void BroadcastAddFivefold(const ArithmeticParams& unswitched_params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const uint8* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const uint8* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 uint8* output_data) {\n  BroadcastAddDispatch(unswitched_params, unswitched_input1_shape,\n                       unswitched_input1_data, unswitched_input2_shape,\n                       unswitched_input2_data, output_shape, output_data);\n}\n\ninline void BroadcastAddFivefold(const ArithmeticParams& params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const float* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const float* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 float* output_data) {\n  BroadcastAddDispatch(params, unswitched_input1_shape, unswitched_input1_data,\n                       unswitched_input2_shape, unswitched_input2_data,\n                       output_shape, output_data);\n}\n\ninline void MulElementwise(int size, const ArithmeticParams& params,\n                           const float* input1_data, const float* input2_data,\n                           float* output_data) {\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n\n  int i = 0;\n#ifdef USE_NEON\n  const auto activation_min = vdupq_n_f32(output_activation_min);\n  const auto activation_max = vdupq_n_f32(output_activation_max);\n  for (; i <= size - 16; i += 16) {\n    auto a10 = vld1q_f32(input1_data + i);\n    auto a11 = vld1q_f32(input1_data + i + 4);\n    auto a12 = vld1q_f32(input1_data + i + 8);\n    auto a13 = vld1q_f32(input1_data + i + 12);\n    auto a20 = vld1q_f32(input2_data + i);\n    auto a21 = vld1q_f32(input2_data + i + 4);\n    auto a22 = vld1q_f32(input2_data + i + 8);\n    auto a23 = vld1q_f32(input2_data + i + 12);\n    auto x0 = vmulq_f32(a10, a20);\n    auto x1 = vmulq_f32(a11, a21);\n    auto x2 = vmulq_f32(a12, a22);\n    auto x3 = vmulq_f32(a13, a23);\n\n    x0 = vmaxq_f32(activation_min, x0);\n    x1 = vmaxq_f32(activation_min, x1);\n    x2 = vmaxq_f32(activation_min, x2);\n    x3 = vmaxq_f32(activation_min, x3);\n    x0 = vminq_f32(activation_max, x0);\n    x1 = vminq_f32(activation_max, x1);\n    x2 = vminq_f32(activation_max, x2);\n    x3 = vminq_f32(activation_max, x3);\n\n    vst1q_f32(output_data + i, x0);\n    vst1q_f32(output_data + i + 4, x1);\n    vst1q_f32(output_data + i + 8, x2);\n    vst1q_f32(output_data + i + 12, x3);\n  }\n  for (; i <= size - 4; i += 4) {\n    auto a1 = vld1q_f32(input1_data + i);\n    auto a2 = vld1q_f32(input2_data + i);\n    auto x = vmulq_f32(a1, a2);\n\n    x = vmaxq_f32(activation_min, x);\n    x = vminq_f32(activation_max, x);\n\n    vst1q_f32(output_data + i, x);\n  }\n#endif  // NEON\n\n  for (; i < size; i++) {\n    auto x = input1_data[i] * input2_data[i];\n    output_data[i] = ActivationFunctionWithMinMax(x, output_activation_min,\n                                                  output_activation_max);\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const float* input1_data,\n                const RuntimeShape& input2_shape, const float* input2_data,\n                const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul\");\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  MulElementwise(flat_size, params, input1_data, input2_data, output_data);\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int32* input1_data,\n                const RuntimeShape& input2_shape, const int32* input2_data,\n                const RuntimeShape& output_shape, int32* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/int32/activation\");\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = ActivationFunctionWithMinMax(\n        input1_data[i] * input2_data[i], output_activation_min,\n        output_activation_max);\n  }\n}\n\ninline void MulNoActivation(const ArithmeticParams& params,\n                            const RuntimeShape& input1_shape,\n                            const int32* input1_data,\n                            const RuntimeShape& input2_shape,\n                            const int32* input2_data,\n                            const RuntimeShape& output_shape,\n                            int32* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/int32\");\n\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  if (input1_shape == input2_shape) {\n    output_map.array() = input1_map.array() * input2_map.array();\n  } else if (input2_shape.FlatSize() == 1) {\n    auto scalar = input2_data[0];\n    output_map.array() = input1_map.array() * scalar;\n  } else if (input1_shape.FlatSize() == 1) {\n    auto scalar = input1_data[0];\n    output_map.array() = scalar * input2_map.array();\n  } else {\n    reference_ops::BroadcastMul4DSlow(params, input1_shape, input1_data,\n                                      input2_shape, input2_data, output_shape,\n                                      output_data);\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/Int16/NoActivation\");\n  // This is a copy of the reference implementation. We do not currently have a\n  // properly optimized version.\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 unclamped_result =\n        F0::FromRaw(input1_data[i]) * F0::FromRaw(input2_data[i]);\n    output_data[i] = unclamped_result.raw();\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/Int16Uint8\");\n  // This is a copy of the reference implementation. We do not currently have a\n  // properly optimized version.\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  const int32 output_offset = params.output_offset;\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 unclamped_result =\n        F0::FromRaw(input1_data[i]) * F0::FromRaw(input2_data[i]);\n    int16 rescaled_result =\n        gemmlowp::RoundingDivideByPOT(unclamped_result.raw(), 8);\n    int16 clamped_result =\n        std::min<int16>(output_activation_max - output_offset, rescaled_result);\n    clamped_result =\n        std::max<int16>(output_activation_min - output_offset, clamped_result);\n    output_data[i] = output_offset + clamped_result;\n  }\n}\n\n// Element-wise mul that can often be used for inner loop of broadcast Mul as\n// well as the non-broadcast Mul.\ninline void MulElementwise(int size, const ArithmeticParams& params,\n                           const uint8* input1_data, const uint8* input2_data,\n                           uint8* output_data) {\n  int i = 0;\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n  TFLITE_DCHECK_GT(params.output_offset, -256);\n  TFLITE_DCHECK_LT(params.output_offset, 256);\n#ifdef USE_NEON\n  const auto input1_offset_vector = vdupq_n_s16(params.input1_offset);\n  const auto input2_offset_vector = vdupq_n_s16(params.input2_offset);\n  const auto output_offset_vector = vdupq_n_s16(params.output_offset);\n  const auto output_activation_min_vector =\n      vdup_n_u8(params.quantized_activation_min);\n  const auto output_activation_max_vector =\n      vdup_n_u8(params.quantized_activation_max);\n  const int left_shift = std::max(0, params.output_shift);\n  const int right_shift = std::max(0, -params.output_shift);\n  const int32x4_t left_shift_vec = vdupq_n_s32(left_shift);\n  for (; i <= size - 8; i += 8) {\n    // We load / store 8 at a time, multiplying as two sets of 4 int32s.\n    const auto input1_val_original = vld1_u8(input1_data + i);\n    const auto input2_val_original = vld1_u8(input2_data + i);\n    const auto input1_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input1_val_original));\n    const auto input2_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input2_val_original));\n    const auto input1_val = vaddq_s16(input1_val_s16, input1_offset_vector);\n    const auto input2_val = vaddq_s16(input2_val_s16, input2_offset_vector);\n\n    const auto input1_val_low = vget_low_s16(input1_val);\n    const auto input1_val_high = vget_high_s16(input1_val);\n    const auto input2_val_low = vget_low_s16(input2_val);\n    const auto input2_val_high = vget_high_s16(input2_val);\n\n    auto p1 = vmull_s16(input2_val_low, input1_val_low);\n    auto p2 = vmull_s16(input2_val_high, input1_val_high);\n\n    p1 = vshlq_s32(p1, left_shift_vec);\n    p2 = vshlq_s32(p2, left_shift_vec);\n    p1 = vqrdmulhq_n_s32(p1, params.output_multiplier);\n    p2 = vqrdmulhq_n_s32(p2, params.output_multiplier);\n    using gemmlowp::RoundingDivideByPOT;\n    p1 = RoundingDivideByPOT(p1, right_shift);\n    p2 = RoundingDivideByPOT(p2, right_shift);\n\n    const auto p1_narrowed = vqmovn_s32(p1);\n    const auto p2_narrowed = vqmovn_s32(p2);\n    const auto p =\n        vaddq_s16(vcombine_s16(p1_narrowed, p2_narrowed), output_offset_vector);\n    const auto clamped =\n        vmax_u8(output_activation_min_vector,\n                vmin_u8(output_activation_max_vector, vqmovun_s16(p)));\n    vst1_u8(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    const int32 input1_val = params.input1_offset + input1_data[i];\n    const int32 input2_val = params.input2_offset + input2_data[i];\n    const int32 unclamped_result =\n        params.output_offset +\n        MultiplyByQuantizedMultiplier(input1_val * input2_val,\n                                      params.output_multiplier,\n                                      params.output_shift);\n    const int32 clamped_output =\n        std::min(params.quantized_activation_max,\n                 std::max(params.quantized_activation_min, unclamped_result));\n    output_data[i] = static_cast<uint8>(clamped_output);\n  }\n}\n\n// Broadcast mul that can often be used for inner loop of broadcast Mul.\ninline void MulSimpleBroadcast(int size, const ArithmeticParams& params,\n                               const uint8 broadcast_value,\n                               const uint8* input2_data, uint8* output_data) {\n  const int16 input1_val = params.input1_offset + broadcast_value;\n\n  int i = 0;\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n  TFLITE_DCHECK_GT(params.output_offset, -256);\n  TFLITE_DCHECK_LT(params.output_offset, 256);\n#ifdef USE_NEON\n  const auto input2_offset_vector = vdupq_n_s16(params.input2_offset);\n  const auto output_offset_vector = vdupq_n_s16(params.output_offset);\n  const auto output_activation_min_vector =\n      vdup_n_u8(params.quantized_activation_min);\n  const auto output_activation_max_vector =\n      vdup_n_u8(params.quantized_activation_max);\n  const int left_shift = std::max(0, params.output_shift);\n  const int right_shift = std::max(0, -params.output_shift);\n  const int32x4_t left_shift_vec = vdupq_n_s32(left_shift);\n  for (; i <= size - 8; i += 8) {\n    // We load / store 8 at a time, multiplying as two sets of 4 int32s.\n    const auto input2_val_original = vld1_u8(input2_data + i);\n    const auto input2_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input2_val_original));\n    const auto input2_val = vaddq_s16(input2_val_s16, input2_offset_vector);\n\n    const auto input2_val_low = vget_low_s16(input2_val);\n    const auto input2_val_high = vget_high_s16(input2_val);\n\n    auto p1 = vmull_n_s16(input2_val_low, input1_val);\n    auto p2 = vmull_n_s16(input2_val_high, input1_val);\n\n    p1 = vshlq_s32(p1, left_shift_vec);\n    p2 = vshlq_s32(p2, left_shift_vec);\n    p1 = vqrdmulhq_n_s32(p1, params.output_multiplier);\n    p2 = vqrdmulhq_n_s32(p2, params.output_multiplier);\n    using gemmlowp::RoundingDivideByPOT;\n    p1 = RoundingDivideByPOT(p1, right_shift);\n    p2 = RoundingDivideByPOT(p2, right_shift);\n\n    const auto p1_narrowed = vmovn_s32(p1);\n    const auto p2_narrowed = vmovn_s32(p2);\n    const auto p =\n        vaddq_s16(vcombine_s16(p1_narrowed, p2_narrowed), output_offset_vector);\n    const auto clamped =\n        vmax_u8(output_activation_min_vector,\n                vmin_u8(output_activation_max_vector, vqmovun_s16(p)));\n    vst1_u8(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    const int32 input2_val = params.input2_offset + input2_data[i];\n    const int32 unclamped_result =\n        params.output_offset +\n        MultiplyByQuantizedMultiplier(input1_val * input2_val,\n                                      params.output_multiplier,\n                                      params.output_shift);\n    const int32 clamped_output =\n        std::min(params.quantized_activation_max,\n                 std::max(params.quantized_activation_min, unclamped_result));\n    output_data[i] = static_cast<uint8>(clamped_output);\n  }\n}\n\n// Broadcast mul that can often be used for inner loop of broadcast Mul.\n// This function will handle scalar_value (LHS) * vector_values (RHS).\n// Since it's a float function, input params does not matter here.\ninline void MulSimpleBroadcast(int size, const ArithmeticParams& params,\n                               const float broadcast_value,\n                               const float* input2_data, float* output_data) {\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t output_activation_min_vector =\n      vdupq_n_f32(params.float_activation_min);\n  const float32x4_t output_activation_max_vector =\n      vdupq_n_f32(params.float_activation_max);\n  const float32x4_t broadcast_value_dup = vdupq_n_f32(broadcast_value);\n  for (; i <= size - 4; i += 4) {\n    const float32x4_t input2_val_original = vld1q_f32(input2_data + i);\n\n    const float32x4_t output =\n        vmulq_f32(input2_val_original, broadcast_value_dup);\n\n    const float32x4_t clamped =\n        vmaxq_f32(output_activation_min_vector,\n                  vminq_f32(output_activation_max_vector, output));\n    vst1q_f32(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    float x = broadcast_value * input2_data[i];\n    output_data[i] = ActivationFunctionWithMinMax(\n        x, params.float_activation_min, params.float_activation_max);\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const uint8* input1_data,\n                const RuntimeShape& input2_shape, const uint8* input2_data,\n                const RuntimeShape& output_shape, uint8* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  ruy::profiler::ScopeLabel label(\"Mul/8bit\");\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  MulElementwise(flat_size, params, input1_data, input2_data, output_data);\n}\n\ntemplate <typename T>\ninline void BroadcastMulDispatch(\n    const ArithmeticParams& params, const RuntimeShape& input1_shape,\n    const T* input1_data, const RuntimeShape& input2_shape,\n    const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return BroadcastMul4DSlow(params, input1_shape, input1_data, input2_shape,\n                              input2_data, output_shape, output_data);\n  }\n\n  BinaryBroadcastFiveFold(\n      params, input1_shape, input1_data, input2_shape, input2_data,\n      output_shape, output_data,\n      static_cast<void (*)(int, const ArithmeticParams&, const T*, const T*,\n                           T*)>(MulElementwise),\n      static_cast<void (*)(int, const ArithmeticParams&, T, const T*, T*)>(\n          MulSimpleBroadcast));\n}\n\ninline void BroadcastMulFivefold(const ArithmeticParams& unswitched_params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const uint8* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const uint8* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 uint8* output_data) {\n  BroadcastMulDispatch(unswitched_params, unswitched_input1_shape,\n                       unswitched_input1_data, unswitched_input2_shape,\n                       unswitched_input2_data, output_shape, output_data);\n}\n\ninline void BroadcastMulFivefold(const ArithmeticParams& params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const float* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const float* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 float* output_data) {\n  BroadcastMulDispatch(params, unswitched_input1_shape, unswitched_input1_data,\n                       unswitched_input2_shape, unswitched_input2_data,\n                       output_shape, output_data);\n}\n\n// TODO(jiawen): We can implement BroadcastDiv on buffers of arbitrary\n// dimensionality if the runtime code does a single loop over one dimension\n// that handles broadcasting as the base case. The code generator would then\n// generate max(D1, D2) nested for loops.\n// TODO(benoitjacob): BroadcastDiv is intentionally duplicated from\n// reference_ops.h. Once an optimized version is implemented and NdArrayDesc<T>\n// is no longer referenced in this file, move NdArrayDesc<T> from types.h to\n// reference_ops.h.\ntemplate <typename T, int N = 5>\nvoid BroadcastDivSlow(const ArithmeticParams& params,\n                      const RuntimeShape& unextended_input1_shape,\n                      const T* input1_data,\n                      const RuntimeShape& unextended_input2_shape,\n                      const T* input2_data,\n                      const RuntimeShape& unextended_output_shape,\n                      T* output_data) {\n  ruy::profiler::ScopeLabel label(\"BroadcastDivSlow\");\n  T output_activation_min;\n  T output_activation_max;\n  GetActivationParams(params, &output_activation_min, &output_activation_max);\n\n  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), N);\n\n  NdArrayDesc<N> desc1;\n  NdArrayDesc<N> desc2;\n  NdArrayDesc<N> output_desc;\n  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,\n                                      unextended_input2_shape, &desc1, &desc2);\n  CopyDimsToDesc(RuntimeShape::ExtendedShape(N, unextended_output_shape),\n                 &output_desc);\n\n  // In Tensorflow, the dimensions are canonically named (batch_number, row,\n  // col, channel), with extents (batches, height, width, depth), with the\n  // trailing dimension changing most rapidly (channels has the smallest stride,\n  // typically 1 element).\n  //\n  // In generated C code, we store arrays with the dimensions reversed. The\n  // first dimension has smallest stride.\n  //\n  // We name our variables by their Tensorflow convention, but generate C code\n  // nesting loops such that the innermost loop has the smallest stride for the\n  // best cache behavior.\n  auto div_func = [&](int indexes[N]) {\n    output_data[SubscriptToIndex(output_desc, indexes)] =\n        ActivationFunctionWithMinMax(\n            input1_data[SubscriptToIndex(desc1, indexes)] /\n                input2_data[SubscriptToIndex(desc2, indexes)],\n            output_activation_min, output_activation_max);\n  };\n  NDOpsHelper<N>(output_desc, div_func);\n}\n\n// BroadcastDiv is intentionally duplicated from reference_ops.h.\n// For more details see the comment above the generic version of\n// BroadcastDivSlow.\ntemplate <int N = 5>\ninline void BroadcastDivSlow(const ArithmeticParams& params,\n                             const RuntimeShape& unextended_input1_shape,\n                             const uint8* input1_data,\n                             const RuntimeShape& unextended_input2_shape,\n                             const uint8* input2_data,\n                             const RuntimeShape& unextended_output_shape,\n                             uint8* output_data) {\n  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), N);\n\n  NdArrayDesc<N> desc1;\n  NdArrayDesc<N> desc2;\n  NdArrayDesc<N> output_desc;\n  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,\n                                      unextended_input2_shape, &desc1, &desc2);\n  CopyDimsToDesc(RuntimeShape::ExtendedShape(N, unextended_output_shape),\n                 &output_desc);\n\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n  TFLITE_DCHECK_GT(params.output_offset, -256);\n  TFLITE_DCHECK_LT(params.output_offset, 256);\n\n  auto div_func = [&](int indexes[N]) {\n    const int32 input1_val =\n        params.input1_offset + input1_data[SubscriptToIndex(desc1, indexes)];\n    const int32 input2_val =\n        params.input2_offset + input2_data[SubscriptToIndex(desc2, indexes)];\n    TFLITE_DCHECK_NE(input2_val, 0);\n    int recip_shift;\n    const int32 input2_inv =\n        (input2_val > 0) ? GetReciprocal(input2_val, 31, &recip_shift)\n                         : -GetReciprocal(-input2_val, 31, &recip_shift);\n    const int headroom = CountLeadingSignBits(input1_val);\n    const int32 unscaled_quotient = MultiplyByQuantizedMultiplierGreaterThanOne(\n        input1_val, input2_inv, headroom);\n    const int total_shift = params.output_shift - recip_shift - headroom;\n    const int32 unclamped_result =\n        params.output_offset +\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            unscaled_quotient, params.output_multiplier, total_shift);\n    const int32 clamped_output =\n        std::min(params.quantized_activation_max,\n                 std::max(params.quantized_activation_min, unclamped_result));\n    output_data[SubscriptToIndex(output_desc, indexes)] =\n        static_cast<uint8>(clamped_output);\n  };\n  NDOpsHelper<N>(output_desc, div_func);\n}\n\ntemplate <typename T>\ninline void SubWithActivation(\n    const ArithmeticParams& params, const RuntimeShape& input1_shape,\n    const T* input1_data, const RuntimeShape& input2_shape,\n    const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"SubWithActivation_optimized\");\n  TFLITE_DCHECK_EQ(input1_shape.FlatSize(), input2_shape.FlatSize());\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  T activation_min, activation_max;\n  GetActivationParams(params, &activation_min, &activation_max);\n  output_map.array() = (input1_map.array() - input2_map.array())\n                           .cwiseMin(activation_max)\n                           .cwiseMax(activation_min);\n}\n\ninline void SubNonBroadcast(const ArithmeticParams& params,\n                            const RuntimeShape& input1_shape,\n                            const float* input1_data,\n                            const RuntimeShape& input2_shape,\n                            const float* input2_data,\n                            const RuntimeShape& output_shape,\n                            float* output_data) {\n  ruy::profiler::ScopeLabel label(\"SubNonBroadcast\");\n  SubWithActivation<float>(params, input1_shape, input1_data, input2_shape,\n                           input2_data, output_shape, output_data);\n}\n\ntemplate <typename T>\nvoid Sub(const ArithmeticParams& params, const RuntimeShape& input1_shape,\n         const T* input1_data, const RuntimeShape& input2_shape,\n         const T* input2_data, const RuntimeShape& output_shape,\n         T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Sub\");\n\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  if (input1_shape == input2_shape) {\n    output_map.array() = input1_map.array() - input2_map.array();\n  } else if (input1_shape.FlatSize() == 1) {\n    auto scalar = input1_data[0];\n    output_map.array() = scalar - input2_map.array();\n  } else if (input2_shape.FlatSize() == 1) {\n    auto scalar = input2_data[0];\n    output_map.array() = input1_map.array() - scalar;\n  } else {\n    BroadcastSubSlow(params, input1_shape, input1_data, input2_shape,\n                     input2_data, output_shape, output_data);\n  }\n}\n\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const float* input_data, const RuntimeShape& unextended_prev_activ_shape,\n    const float* prev_activ_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& unextended_bias_shape,\n    const float* bias_data, const RuntimeShape& unextended_prev_state_shape,\n    const float* prev_state_data,\n    const RuntimeShape& unextended_output_state_shape, float* output_state_data,\n    const RuntimeShape& unextended_output_activ_shape, float* output_activ_data,\n    const RuntimeShape& unextended_concat_temp_shape, float* concat_temp_data,\n    const RuntimeShape& unextended_activ_temp_shape, float* activ_temp_data,\n    CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"LstmCell\");\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  MatchingDim(  // batches\n      input_shape, 0, prev_activ_shape, 0, prev_state_shape, 0,\n      output_state_shape, 0, output_activ_shape, 0);\n  MatchingDim(  // height\n      input_shape, 1, prev_activ_shape, 1, prev_state_shape, 1,\n      output_state_shape, 1, output_activ_shape, 1);\n  MatchingDim(  // width\n      input_shape, 2, prev_activ_shape, 2, prev_state_shape, 2,\n      output_state_shape, 2, output_activ_shape, 2);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n\n  // Concatenate prev_activ and input data together\n  std::vector<float const*> concat_input_arrays_data;\n  std::vector<RuntimeShape const*> concat_input_arrays_shapes;\n  concat_input_arrays_data.push_back(input_data);\n  concat_input_arrays_data.push_back(prev_activ_data);\n  concat_input_arrays_shapes.push_back(&input_shape);\n  concat_input_arrays_shapes.push_back(&prev_activ_shape);\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = concat_input_arrays_data.size();\n  Concatenation(concat_params, &(concat_input_arrays_shapes[0]),\n                &(concat_input_arrays_data[0]), concat_temp_shape,\n                concat_temp_data);\n\n  // Fully connected\n  tflite::FullyConnectedParams fc_params;\n  fc_params.float_activation_min = std::numeric_limits<float>::lowest();\n  fc_params.float_activation_max = std::numeric_limits<float>::max();\n  fc_params.lhs_cacheable = false;\n  fc_params.rhs_cacheable = false;\n  FullyConnected(fc_params, concat_temp_shape, concat_temp_data, weights_shape,\n                 weights_data, bias_shape, bias_data, activ_temp_shape,\n                 activ_temp_data, cpu_backend_context);\n\n  // Map raw arrays to Eigen arrays so we can use Eigen's optimized array\n  // operations.\n  ArrayMap<float> activ_temp_map =\n      MapAsArrayWithLastDimAsRows(activ_temp_data, activ_temp_shape);\n  auto input_gate_sm = activ_temp_map.block(0 * output_depth, 0, output_depth,\n                                            activ_temp_map.cols());\n  auto new_input_sm = activ_temp_map.block(1 * output_depth, 0, output_depth,\n                                           activ_temp_map.cols());\n  auto forget_gate_sm = activ_temp_map.block(2 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  auto output_gate_sm = activ_temp_map.block(3 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  ArrayMap<const float> prev_state_map =\n      MapAsArrayWithLastDimAsRows(prev_state_data, prev_state_shape);\n  ArrayMap<float> output_state_map =\n      MapAsArrayWithLastDimAsRows(output_state_data, output_state_shape);\n  ArrayMap<float> output_activ_map =\n      MapAsArrayWithLastDimAsRows(output_activ_data, output_activ_shape);\n\n  // Combined memory state and final output calculation\n  ruy::profiler::ScopeLabel label2(\"MemoryStateAndFinalOutput\");\n  output_state_map =\n      input_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          new_input_sm.tanh() +\n      forget_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          prev_state_map;\n  output_activ_map =\n      output_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n      output_state_map.tanh();\n}\n\ntemplate <int StateIntegerBits>\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const uint8* input_data_uint8,\n    const RuntimeShape& unextended_prev_activ_shape,\n    const uint8* prev_activ_data_uint8, const RuntimeShape& weights_shape,\n    const uint8* weights_data_uint8, const RuntimeShape& unextended_bias_shape,\n    const int32* bias_data_int32,\n    const RuntimeShape& unextended_prev_state_shape,\n    const int16* prev_state_data_int16,\n    const RuntimeShape& unextended_output_state_shape,\n    int16* output_state_data_int16,\n    const RuntimeShape& unextended_output_activ_shape,\n    uint8* output_activ_data_uint8,\n    const RuntimeShape& unextended_concat_temp_shape,\n    uint8* concat_temp_data_uint8,\n    const RuntimeShape& unextended_activ_temp_shape,\n    int16* activ_temp_data_int16, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\n      \"LstmCell/quantized (8bit external, 16bit internal)\");\n  int32 weights_zero_point = params.weights_zero_point;\n  int32 accum_multiplier = params.accum_multiplier;\n  int accum_shift = params.accum_shift;\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  // Gather dimensions information, and perform consistency checks.\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int outer_size = MatchingFlatSizeSkipDim(\n      input_shape, 3, prev_activ_shape, prev_state_shape, output_state_shape,\n      output_activ_shape);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n  const int fc_batches = FlatSizeSkipDim(activ_temp_shape, 3);\n  const int fc_output_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, activ_temp_shape, 3);\n  const int fc_accum_depth = total_input_depth;\n  TFLITE_DCHECK_EQ(fc_output_depth, 4 * output_depth);\n\n  // Depth-concatenate prev_activ and input data together.\n  uint8 const* concat_input_arrays_data[2] = {input_data_uint8,\n                                              prev_activ_data_uint8};\n  const RuntimeShape* concat_input_arrays_shapes[2] = {&input_shape,\n                                                       &prev_activ_shape};\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = 2;\n  Concatenation(concat_params, concat_input_arrays_shapes,\n                concat_input_arrays_data, concat_temp_shape,\n                concat_temp_data_uint8);\n\n  // Implementation of the fully connected node inside the LSTM cell.\n  // The operands are 8-bit integers, the accumulators are internally 32bit\n  // integers, and the output is 16-bit fixed-point with 3 integer bits so\n  // the output range is [-2^3, 2^3] == [-8, 8]. The rationale for that\n  // is explained in the function comment above.\n  cpu_backend_gemm::MatrixParams<uint8> lhs_params;\n  lhs_params.rows = fc_output_depth;\n  lhs_params.cols = fc_accum_depth;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.zero_point = weights_zero_point;\n  cpu_backend_gemm::MatrixParams<uint8> rhs_params;\n  rhs_params.rows = fc_accum_depth;\n  rhs_params.cols = fc_batches;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.zero_point = 128;\n  cpu_backend_gemm::MatrixParams<int16> dst_params;\n  dst_params.rows = fc_output_depth;\n  dst_params.cols = fc_batches;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.zero_point = 0;\n  cpu_backend_gemm::GemmParams<int32, int16> gemm_params;\n  gemm_params.bias = bias_data_int32;\n  gemm_params.multiplier_fixedpoint = accum_multiplier;\n  gemm_params.multiplier_exponent = accum_shift;\n  cpu_backend_gemm::Gemm(\n      lhs_params, weights_data_uint8, rhs_params, concat_temp_data_uint8,\n      dst_params, activ_temp_data_int16, gemm_params, cpu_backend_context);\n\n  // Rest of the LSTM cell: tanh and logistic math functions, and some adds\n  // and muls, all done in 16-bit fixed-point.\n  const int16* input_gate_input_ptr = activ_temp_data_int16;\n  const int16* input_modulation_gate_input_ptr =\n      activ_temp_data_int16 + output_depth;\n  const int16* forget_gate_input_ptr = activ_temp_data_int16 + 2 * output_depth;\n  const int16* output_gate_input_ptr = activ_temp_data_int16 + 3 * output_depth;\n  const int16* prev_state_ptr = prev_state_data_int16;\n  int16* output_state_data_ptr = output_state_data_int16;\n  uint8* output_activ_data_ptr = output_activ_data_uint8;\n\n  for (int b = 0; b < outer_size; ++b) {\n    int c = 0;\n#ifdef GEMMLOWP_NEON\n    for (; c <= output_depth - 8; c += 8) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<int16x8_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<int16x8_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(vld1q_s16(input_gate_input_ptr));\n      input_gate_input_ptr += 8;\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(vld1q_s16(input_modulation_gate_input_ptr));\n      input_modulation_gate_input_ptr += 8;\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(vld1q_s16(forget_gate_input_ptr));\n      forget_gate_input_ptr += 8;\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(vld1q_s16(output_gate_input_ptr));\n      output_gate_input_ptr += 8;\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(vld1q_s16(prev_state_ptr));\n      prev_state_ptr += 8;\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      vst1q_s16(output_state_data_ptr, new_state.raw());\n      output_state_data_ptr += 8;\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16x8_t rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int8x8_t int8_output_activ = vqmovn_s16(rescaled_output_activ);\n      uint8x8_t uint8_output_activ =\n          vadd_u8(vdup_n_u8(128), vreinterpret_u8_s8(int8_output_activ));\n      vst1_u8(output_activ_data_ptr, uint8_output_activ);\n      output_activ_data_ptr += 8;\n    }\n#endif\n    for (; c < output_depth; ++c) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<std::int16_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<std::int16_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(*input_gate_input_ptr++);\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(*input_modulation_gate_input_ptr++);\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(*forget_gate_input_ptr++);\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(*output_gate_input_ptr++);\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(*prev_state_ptr++);\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      *output_state_data_ptr++ = new_state.raw();\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16 rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int16 clamped_output_activ =\n          std::max<int16>(-128, std::min<int16>(127, rescaled_output_activ));\n      *output_activ_data_ptr++ = 128 + clamped_output_activ;\n    }\n    input_gate_input_ptr += 3 * output_depth;\n    input_modulation_gate_input_ptr += 3 * output_depth;\n    forget_gate_input_ptr += 3 * output_depth;\n    output_gate_input_ptr += 3 * output_depth;\n  }\n}\n\ninline int NodeOffset(int b, int h, int w, int height, int width) {\n  return (b * height + h) * width + w;\n}\n\ninline void AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const float* input_data,\n                        const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"AveragePool\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  // TODO(benoitjacob) make this a proper reference impl without Eigen!\n  const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n  auto out_mat = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n  // TODO(benoitjacob) get rid of the dynamic memory allocation here!\n  Eigen::VectorXf out_count(out_mat.cols());\n  out_count.setZero();\n  // Prefill the output to 0.\n  out_mat.setZero();\n  for (int b = 0; b < batches; ++b) {\n    for (int h = 0; h < input_height; ++h) {\n      for (int w = 0; w < input_width; ++w) {\n        // (h_start, h_end) * (w_start, w_end) is the range that the input\n        // vector projects to.\n        int hpad = h + params.padding_values.height;\n        int wpad = w + params.padding_values.width;\n        int h_start = (hpad < params.filter_height)\n                          ? 0\n                          : (hpad - params.filter_height) / stride_height + 1;\n        int h_end = std::min(hpad / stride_height + 1, output_height);\n        int w_start = (wpad < params.filter_width)\n                          ? 0\n                          : (wpad - params.filter_width) / stride_width + 1;\n        int w_end = std::min(wpad / stride_width + 1, output_width);\n        // compute elementwise sum\n        for (int ph = h_start; ph < h_end; ++ph) {\n          for (int pw = w_start; pw < w_end; ++pw) {\n            int out_offset = NodeOffset(b, ph, pw, output_height, output_width);\n            out_mat.col(out_offset) +=\n                in_mat.col(NodeOffset(b, h, w, input_height, input_width));\n            out_count(out_offset)++;\n          }\n        }\n      }\n    }\n  }\n  // Divide the output by the actual number of elements being averaged over\n  TFLITE_DCHECK_GT(out_count.minCoeff(), 0);\n  out_mat.array().rowwise() /= out_count.transpose().array();\n\n  const int flat_size = output_shape.FlatSize();\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = ActivationFunctionWithMinMax(output_data[i],\n                                                  params.float_activation_min,\n                                                  params.float_activation_max);\n  }\n}\n\ninline void AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const uint8* input_data,\n                        const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"AveragePool/8bit\");\n\n  // Here, and in other pooling ops, in order to maintain locality of reference,\n  // to minimize some recalculations, and to load into NEON vector registers, we\n  // use an inner loop down the depth. Since depths can be large and hence we\n  // would need arbitrarily large temporary storage, we divide the work up into\n  // depth tranches just within the batch loop.\n  static constexpr int kPoolingAccTrancheSize = 256;\n\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  uint32 acc[kPoolingAccTrancheSize];\n  for (int batch = 0; batch < batches; ++batch) {\n    // We proceed through the depth in tranches (see comment above). The\n    // depth_base is the depth at the beginning of the tranche. The\n    // tranche_depth is the depth dimension of the tranche.\n    for (int depth_base = 0; depth_base < depth;\n         depth_base += kPoolingAccTrancheSize) {\n      const int tranche_depth =\n          std::min(depth - depth_base, kPoolingAccTrancheSize);\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          const int filter_count =\n              (filter_x_end - filter_x_start) * (filter_y_end - filter_y_start);\n          memset(acc, 0, tranche_depth * sizeof(acc[0]));\n          const uint8* input_ptr =\n              input_data + depth_base +\n              depth * (in_x_origin +\n                       input_width * (in_y_origin + input_height * batch));\n          for (int fy = filter_y_start; fy < filter_y_end; fy++) {\n            const uint8* input_row_ptr =\n                input_ptr + depth * (fy * input_width + filter_x_start);\n            for (int fx = filter_x_start; fx < filter_x_end; fx++) {\n              const uint8* input_channel_ptr = input_row_ptr;\n              int channel = 0;\n#ifdef USE_NEON\n              for (; channel <= tranche_depth - 16; channel += 16) {\n                uint16x4_t acc_reg[4];\n                uint8x16_t input_reg = vld1q_u8(input_channel_ptr);\n                input_channel_ptr += 16;\n                acc_reg[0] = vget_low_u16(vmovl_u8(vget_low_u8(input_reg)));\n                acc_reg[1] = vget_high_u16(vmovl_u8(vget_low_u8(input_reg)));\n                acc_reg[2] = vget_low_u16(vmovl_u8(vget_high_u8(input_reg)));\n                acc_reg[3] = vget_high_u16(vmovl_u8(vget_high_u8(input_reg)));\n                for (int i = 0; i < 4; i++) {\n                  vst1q_u32(\n                      acc + channel + 4 * i,\n                      vaddw_u16(vld1q_u32(acc + channel + 4 * i), acc_reg[i]));\n                }\n              }\n              for (; channel <= tranche_depth - 8; channel += 8) {\n                uint16x4_t acc_reg[2];\n                uint16x8_t input_reg = vmovl_u8(vld1_u8(input_channel_ptr));\n                input_channel_ptr += 8;\n                acc_reg[0] = vget_low_u16(input_reg);\n                acc_reg[1] = vget_high_u16(input_reg);\n                for (int i = 0; i < 2; i++) {\n                  vst1q_u32(\n                      acc + channel + 4 * i,\n                      vaddw_u16(vld1q_u32(acc + channel + 4 * i), acc_reg[i]));\n                }\n              }\n#endif\n              for (; channel < tranche_depth; ++channel) {\n                acc[channel] += *input_channel_ptr++;\n              }\n              input_row_ptr += depth;\n            }\n          }\n          uint8* output_ptr = output_data + Offset(output_shape, batch, out_y,\n                                                   out_x, depth_base);\n          int channel = 0;\n#ifdef USE_NEON\n#define AVGPOOL_DIVIDING_BY(FILTER_COUNT)                               \\\n  if (filter_count == FILTER_COUNT) {                                   \\\n    for (; channel <= tranche_depth - 8; channel += 8) {                \\\n      uint16 buf[8];                                                    \\\n      for (int i = 0; i < 8; i++) {                                     \\\n        buf[i] = (acc[channel + i] + FILTER_COUNT / 2) / FILTER_COUNT;  \\\n      }                                                                 \\\n      uint8x8_t buf8 = vqmovn_u16(vld1q_u16(buf));                      \\\n      buf8 = vmin_u8(buf8, vdup_n_u8(params.quantized_activation_max)); \\\n      buf8 = vmax_u8(buf8, vdup_n_u8(params.quantized_activation_min)); \\\n      vst1_u8(output_ptr + channel, buf8);                              \\\n    }                                                                   \\\n  }\n          AVGPOOL_DIVIDING_BY(9)\n          AVGPOOL_DIVIDING_BY(15)\n#undef AVGPOOL_DIVIDING_BY\n          for (; channel <= tranche_depth - 8; channel += 8) {\n            uint16 buf[8];\n            for (int i = 0; i < 8; i++) {\n              buf[i] = (acc[channel + i] + filter_count / 2) / filter_count;\n            }\n            uint8x8_t buf8 = vqmovn_u16(vld1q_u16(buf));\n            buf8 = vmin_u8(buf8, vdup_n_u8(params.quantized_activation_max));\n            buf8 = vmax_u8(buf8, vdup_n_u8(params.quantized_activation_min));\n            vst1_u8(output_ptr + channel, buf8);\n          }\n#endif\n          for (; channel < tranche_depth; ++channel) {\n            uint16 a = (acc[channel] + filter_count / 2) / filter_count;\n            a = std::max<uint16>(a, params.quantized_activation_min);\n            a = std::min<uint16>(a, params.quantized_activation_max);\n            output_ptr[channel] = static_cast<uint8>(a);\n          }\n        }\n      }\n    }\n  }\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const float* input_data, const RuntimeShape& output_shape,\n                    float* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaxPool\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n  auto out_mat = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n  // Prefill the output to minimum representable float value\n  out_mat.setConstant(std::numeric_limits<float>::lowest());\n  for (int b = 0; b < batches; ++b) {\n    for (int h = 0; h < input_height; ++h) {\n      for (int w = 0; w < input_width; ++w) {\n        // (h_start, h_end) * (w_start, w_end) is the range that the input\n        // vector projects to.\n        int hpad = h + params.padding_values.height;\n        int wpad = w + params.padding_values.width;\n        int h_start = (hpad < params.filter_height)\n                          ? 0\n                          : (hpad - params.filter_height) / stride_height + 1;\n        int h_end = std::min(hpad / stride_height + 1, output_height);\n        int w_start = (wpad < params.filter_width)\n                          ? 0\n                          : (wpad - params.filter_width) / stride_width + 1;\n        int w_end = std::min(wpad / stride_width + 1, output_width);\n        // compute elementwise sum\n        for (int ph = h_start; ph < h_end; ++ph) {\n          for (int pw = w_start; pw < w_end; ++pw) {\n            int out_offset = NodeOffset(b, ph, pw, output_height, output_width);\n            out_mat.col(out_offset) =\n                out_mat.col(out_offset)\n                    .cwiseMax(in_mat.col(\n                        NodeOffset(b, h, w, input_height, input_width)));\n          }\n        }\n      }\n    }\n  }\n  const int flat_size = output_shape.FlatSize();\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = ActivationFunctionWithMinMax(output_data[i],\n                                                  params.float_activation_min,\n                                                  params.float_activation_max);\n  }\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const uint8* input_data, const RuntimeShape& output_shape,\n                    uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaxPool/8bit\");\n\n  // Here, and in other pooling ops, in order to maintain locality of reference,\n  // to minimize some recalculations, and to load into NEON vector registers, we\n  // use an inner loop down the depth. Since depths can be large and hence we\n  // would need arbitrarily large temporary storage, we divide the work up into\n  // depth tranches just within the batch loop.\n  static constexpr int kPoolingAccTrancheSize = 256;\n\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  uint8 acc[kPoolingAccTrancheSize];\n  for (int batch = 0; batch < batches; ++batch) {\n    // We proceed through the depth in tranches (see comment above). The\n    // depth_base is the depth at the beginning of the tranche. The\n    // tranche_depth is the depth dimension of the tranche.\n    for (int depth_base = 0; depth_base < depth;\n         depth_base += kPoolingAccTrancheSize) {\n      const int tranche_depth =\n          std::min(depth - depth_base, kPoolingAccTrancheSize);\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          memset(acc, 0, tranche_depth * sizeof(acc[0]));\n          const uint8* input_ptr =\n              input_data + depth_base +\n              depth * (in_x_origin +\n                       input_width * (in_y_origin + input_height * batch));\n          for (int fy = filter_y_start; fy < filter_y_end; fy++) {\n            const uint8* input_row_ptr =\n                input_ptr + depth * (fy * input_width + filter_x_start);\n            for (int fx = filter_x_start; fx < filter_x_end; fx++) {\n              const uint8* input_channel_ptr = input_row_ptr;\n              int channel = 0;\n#ifdef USE_NEON\n              for (; channel <= tranche_depth - 16; channel += 16) {\n                uint8x16_t acc_reg = vld1q_u8(acc + channel);\n                uint8x16_t input_reg = vld1q_u8(input_channel_ptr);\n                input_channel_ptr += 16;\n                acc_reg = vmaxq_u8(acc_reg, input_reg);\n                vst1q_u8(acc + channel, acc_reg);\n              }\n\n              for (; channel <= tranche_depth - 8; channel += 8) {\n                uint8x8_t acc_reg = vld1_u8(acc + channel);\n                uint8x8_t input_reg = vld1_u8(input_channel_ptr);\n                input_channel_ptr += 8;\n                acc_reg = vmax_u8(acc_reg, input_reg);\n                vst1_u8(acc + channel, acc_reg);\n              }\n#endif\n              for (; channel < tranche_depth; ++channel) {\n                acc[channel] = std::max(acc[channel], *input_channel_ptr++);\n              }\n              input_row_ptr += depth;\n            }\n          }\n          uint8* output_ptr = output_data + Offset(output_shape, batch, out_y,\n                                                   out_x, depth_base);\n          int channel = 0;\n#ifdef USE_NEON\n          for (; channel <= tranche_depth - 16; channel += 16) {\n            uint8x16_t a = vld1q_u8(acc + channel);\n            a = vminq_u8(a, vdupq_n_u8(params.quantized_activation_max));\n            a = vmaxq_u8(a, vdupq_n_u8(params.quantized_activation_min));\n            vst1q_u8(output_ptr + channel, a);\n          }\n          for (; channel <= tranche_depth - 8; channel += 8) {\n            uint8x8_t a = vld1_u8(acc + channel);\n            a = vmin_u8(a, vdup_n_u8(params.quantized_activation_max));\n            a = vmax_u8(a, vdup_n_u8(params.quantized_activation_min));\n            vst1_u8(output_ptr + channel, a);\n          }\n#endif\n          for (; channel < tranche_depth; ++channel) {\n            uint8 a = acc[channel];\n            a = std::max<uint8>(a, params.quantized_activation_min);\n            a = std::min<uint8>(a, params.quantized_activation_max);\n            output_ptr[channel] = static_cast<uint8>(a);\n          }\n        }\n      }\n    }\n  }\n}\n\ninline void L2Pool(const PoolParams& params, const RuntimeShape& input_shape,\n                   const float* input_data, const RuntimeShape& output_shape,\n                   float* output_data) {\n  ruy::profiler::ScopeLabel label(\"L2Pool\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  // Actually carry out L2 Pool. Code is written in forward mode: we go through\n  // the input values once, and write to all the pooled regions that it maps to.\n  const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n  auto out_mat = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n  Eigen::VectorXf in_square(in_mat.rows());\n  Eigen::VectorXf out_count(out_mat.cols());\n  out_count.setZero();\n  // Prefill the output to 0.\n  out_mat.setZero();\n  for (int b = 0; b < batches; ++b) {\n    for (int h = 0; h < input_height; ++h) {\n      for (int w = 0; w < input_width; ++w) {\n        // (h_start, h_end) * (w_start, w_end) is the range that the input\n        // vector projects to.\n        const int hpad = h + params.padding_values.height;\n        const int wpad = w + params.padding_values.width;\n        const int h_start =\n            (hpad < params.filter_height)\n                ? 0\n                : (hpad - params.filter_height) / stride_height + 1;\n        const int h_end = std::min(hpad / stride_height + 1, output_height);\n        const int w_start =\n            (wpad < params.filter_width)\n                ? 0\n                : (wpad - params.filter_width) / stride_width + 1;\n        const int w_end = std::min(wpad / stride_width + 1, output_width);\n        // pre-compute square\n        const int in_offset = w + input_width * (h + input_height * b);\n        in_square =\n            in_mat.col(in_offset).array() * in_mat.col(in_offset).array();\n        // compute elementwise sum of squares\n        for (int ph = h_start; ph < h_end; ++ph) {\n          for (int pw = w_start; pw < w_end; ++pw) {\n            const int out_offset = pw + output_width * (ph + output_height * b);\n            out_mat.col(out_offset) += in_square;\n            out_count(out_offset)++;\n          }\n        }\n      }\n    }\n  }\n\n  out_count = out_count.array().inverse();\n  out_mat =\n      (out_mat.array().rowwise() * out_count.transpose().array()).cwiseSqrt();\n\n  const int flat_size = output_shape.FlatSize();\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = ActivationFunctionWithMinMax(output_data[i],\n                                                  params.float_activation_min,\n                                                  params.float_activation_max);\n  }\n}\n\ninline void LocalResponseNormalization(\n    const tflite::LocalResponseNormalizationParams& op_params,\n    const RuntimeShape& input_shape, const float* input_data,\n    const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"LocalResponseNormalization\");\n  MatchingFlatSize(input_shape, output_shape);\n\n  const auto data_in = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n  auto data_out = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n\n  // Carry out local response normalization, vector by vector.\n  // Since the data are stored column major, making row-wise operation\n  // probably not memory efficient anyway, we do an explicit for loop over\n  // the columns.\n  const int double_range = op_params.range * 2;\n  Eigen::VectorXf padded_square(data_in.rows() + double_range);\n  padded_square.setZero();\n  const float bias = op_params.bias;\n  for (int r = 0; r < data_in.cols(); ++r) {\n    // Do local response normalization for data_in(:, r)\n    // first, compute the square and store them in buffer for repeated use\n    padded_square.block(op_params.range, 0, data_in.rows(), 1) =\n        data_in.col(r).cwiseProduct(data_in.col(r)) * op_params.alpha;\n    // Then, compute the scale and writes them to data_out\n    float accumulated_scale = 0;\n    for (int i = 0; i < double_range; ++i) {\n      accumulated_scale += padded_square(i);\n    }\n    for (int i = 0; i < data_in.rows(); ++i) {\n      accumulated_scale += padded_square(i + double_range);\n      data_out(i, r) = bias + accumulated_scale;\n      accumulated_scale -= padded_square(i);\n    }\n  }\n\n  // In a few cases, the pow computation could benefit from speedups.\n  if (op_params.beta == 1) {\n    data_out.array() = data_in.array() * data_out.array().inverse();\n  } else if (op_params.beta == 0.5f) {\n    data_out.array() = data_in.array() * data_out.array().sqrt().inverse();\n  } else {\n    data_out.array() = data_in.array() * data_out.array().pow(-op_params.beta);\n  }\n}\n\ninline void SoftmaxImpl(const SoftmaxParams& params,\n                        const RuntimeShape& input_shape,\n                        const float* input_data,\n                        const RuntimeShape& output_shape, float* output_data,\n                        int start_batch, int end_batch) {\n  ruy::profiler::ScopeLabel label(\"Softmax/Impl\");\n  MatchingFlatSize(input_shape, output_shape);\n\n  const int logit_size = input_shape.Dims(input_shape.DimensionsCount() - 1);\n  const MatrixMap<const float> in_mat(input_data + logit_size * start_batch,\n                                      logit_size, end_batch - start_batch);\n  MatrixMap<float> out_mat(output_data + logit_size * start_batch, logit_size,\n                           end_batch - start_batch);\n  // Compute the exponential first, removing the max coefficient for numerical\n  // stability.\n  out_mat =\n      (in_mat.rowwise() - in_mat.colwise().maxCoeff()).array() * params.beta;\n  // We are separating out the exp function so that exp can be vectorized.\n  out_mat = out_mat.array().exp();\n  // Normalize to get the activations.\n  Eigen::Array<float, 1, Eigen::Dynamic> scale =\n      out_mat.array().colwise().sum().inverse();\n  out_mat.array().rowwise() *= scale;\n}\n\nstruct SoftmaxWorkerTask : cpu_backend_threadpool::Task {\n  SoftmaxWorkerTask(const SoftmaxParams& params,\n                    const RuntimeShape& input_shape, const float* input_data,\n                    const RuntimeShape& output_shape, float* output_data,\n                    int start_batch, int end_batch)\n      : params(params),\n        input_shape(input_shape),\n        input_data(input_data),\n        output_shape(output_shape),\n        output_data(output_data),\n        start_batch(start_batch),\n        end_batch(end_batch) {}\n  void Run() override {\n    SoftmaxImpl(params, input_shape, input_data, output_shape, output_data,\n                start_batch, end_batch);\n  }\n\n private:\n  const tflite::SoftmaxParams& params;\n  const RuntimeShape& input_shape;\n  const float* input_data;\n  const RuntimeShape& output_shape;\n  float* output_data;\n  int start_batch;\n  int end_batch;\n};\n\ninline void Softmax(const SoftmaxParams& params,\n                    const RuntimeShape& input_shape, const float* input_data,\n                    const RuntimeShape& output_shape, float* output_data,\n                    CpuBackendContext* cpu_backend_context = nullptr) {\n  ruy::profiler::ScopeLabel label(\"Softmax\");\n\n  // We picture softmax input as a 2-D matrix while the last dim is the logit\n  // dim, and the rest dims will be the batch dim for the 2-D matrix.\n  const int batch_size =\n      FlatSizeSkipDim(input_shape, input_shape.DimensionsCount() - 1);\n  constexpr int kMinBatchPerThread = 8;\n  int thread_count = batch_size / kMinBatchPerThread;\n  thread_count = thread_count > 0 ? thread_count : 1;\n  const int capped_thread_count =\n      cpu_backend_context == nullptr\n          ? 1\n          : std::min(thread_count, cpu_backend_context->max_num_threads());\n  if (capped_thread_count == 1) {\n    SoftmaxImpl(params, input_shape, input_data, output_shape, output_data, 0,\n                batch_size);\n  } else {\n    std::vector<SoftmaxWorkerTask> tasks;\n    // TODO(b/131746020) don't create new heap allocations every time.\n    // At least we make it a single heap allocation by using reserve().\n    tasks.reserve(capped_thread_count);\n    int batch_start = 0;\n    for (int i = 0; i < capped_thread_count; ++i) {\n      // Try to distribute the tasks as even as possible.\n      int batch_end =\n          batch_start + (batch_size - batch_start) / (capped_thread_count - i);\n      tasks.emplace_back(params, input_shape, input_data, output_shape,\n                         output_data, batch_start, batch_end);\n      batch_start = batch_end;\n    }\n    cpu_backend_threadpool::Execute(tasks.size(), tasks.data(),\n                                    cpu_backend_context);\n  }\n}\n\ntemplate <typename T>\ninline int32_t QuantizeSoftmaxOutput(float prob_rescaled, int32_t zero_point) {\n  const int32_t prob_rnd = static_cast<int32_t>(std::round(prob_rescaled));\n  return prob_rnd + zero_point;\n}\n\n#if !__aarch64__\n// With ARM64, rounding is faster than add + truncation.\ntemplate <>\ninline int32_t QuantizeSoftmaxOutput<uint8_t>(float prob_rescaled,\n                                              int32_t zero_point) {\n  return static_cast<int32_t>(prob_rescaled + 0.5f);\n}\n#endif\n\ninline void PopulateSoftmaxLookupTable(SoftmaxParams* data, float input_scale,\n                                       float beta) {\n  const float scale = -input_scale * beta;\n  const int32_t max_uint8 = std::numeric_limits<uint8_t>::max();\n  for (int32_t val = 0; val <= max_uint8; ++val) {\n    data->table[max_uint8 - val] = expf(scale * val);\n  }\n}\n\ntemplate <typename In, typename Out>\ninline void Softmax(const SoftmaxParams& params,\n                    const RuntimeShape& input_shape, const In* input_data,\n                    const RuntimeShape& output_shape, Out* output_data) {\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int excluding_last_dim =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int last_dim =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  const int32_t clamp_max = std::numeric_limits<Out>::max();\n  const int32_t clamp_min = std::numeric_limits<Out>::min();\n  for (int i = 0; i < excluding_last_dim; ++i) {\n    int32_t max_val = std::numeric_limits<In>::min();\n    // Find max quantized value.\n    for (int j = 0; j < last_dim; ++j) {\n      max_val = std::max(max_val, static_cast<int32_t>(input_data[j]));\n    }\n\n    float sum_exp = 0.0f;\n    const int32_t max_uint8 = std::numeric_limits<uint8_t>::max();\n    const float* table_offset = &params.table[max_uint8 - max_val];\n    // Calculate normalizer sum(exp(x)).\n    for (int j = 0; j < last_dim; ++j) {\n      sum_exp += table_offset[input_data[j]];\n    }\n\n    const float inv_sum_exp = 1.0f / (sum_exp * params.scale);\n    // Normalize and quantize probabilities.\n    for (int j = 0; j < last_dim; ++j) {\n      const float prob_rescaled = table_offset[input_data[j]] * inv_sum_exp;\n      const int32_t prob_quantized =\n          QuantizeSoftmaxOutput<Out>(prob_rescaled, params.zero_point);\n      output_data[j] = static_cast<Out>(\n          std::max(std::min(clamp_max, prob_quantized), clamp_min));\n    }\n    input_data += last_dim;\n    output_data += last_dim;\n  }\n}\n\n// Here's the softmax LUT optimization strategy:\n// For softmax, we can do some mathmetically equivalent transformation:\n//\n// softmax(x) = e^x / sum(e^x, 0...n)  ===> equals to\n// softmax(x) = e^(x - CONST) / sum(e^(x - CONST), 0...n)\n//\n// For quantization, `x` in our case is (input_q - input_zp) * input_s\n// For uint8 case (int8 can be handled similarly), the range is [0, 255]\n//\n// so if we let\n// CONST = (255 - input_zp) * input_s\n// then we will have:\n// softmax(x) = e^((input_q - 255) * input_s) --------- (1)\n//         /\n// sum(e^(input_q - 255) * input_s, 0...n)   -------- (2)\n//\n// the good thing about (1) is it's within the range of (0, 1), so we can\n// approximate its result with uint16.\n//  (1) = uint8_out * 1 / 2^16.\n//\n// so (1) is lookup_uint8_table(input_zp) * 1 / 2^16.\n// then (2) is essentially the following:\n// sum(lookup_uint8_table(input_zp), 0...n) / 2^16.\n//\n// since (output_q - output_zp) * output_s = softmax(x)\n// output_q = lookup_uint8_table(input_zp)\n//            /\n// (sum(lookup_uint8_table(input_zp), 0...n) * output_s)\n//             +\n//   output_zp\n//\n// We can actually further improve the performance by using uint8 instead of\n// uint16. But that we may lose some accuracy, so we need to pay attention\n// to that.\ninline void PopulateSoftmaxUInt8LookupTable(SoftmaxParams* data,\n                                            float input_scale, float beta) {\n  const float scale = input_scale * beta;\n  const int32_t max_uint8 = std::numeric_limits<uint8_t>::max();\n  const int32_t max_uint16 = std::numeric_limits<uint16_t>::max();\n\n  for (int32_t val = 0; val <= max_uint8; ++val) {\n    float input_to_exp = scale * (val - max_uint8);\n    int32_t temp = static_cast<int>(expf(input_to_exp) * max_uint16 + 0.5);\n    temp = std::min(max_uint16, temp);\n    uint8_t part1 = temp >> 8;\n    uint8_t part2 = temp & 0xff;\n    data->uint8_table1[val] = static_cast<uint8_t>(part1);\n    data->uint8_table2[val] = static_cast<uint8_t>(part2);\n  }\n}\n\ninline int FindMaxValue(int size, const uint8_t* input_data, uint8_t offset) {\n  int32_t max_val = std::numeric_limits<uint8_t>::min();\n  int j = 0;\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n  uint8x16_t max_val_dup = vdupq_n_u8(max_val);\n  uint8x16_t offset_dup = vdupq_n_u8(offset);\n  for (; j <= size - 16; j += 16) {\n    uint8x16_t input_value = vld1q_u8(input_data + j);\n    input_value = veorq_u8(input_value, offset_dup);\n    max_val_dup = vmaxq_u8(input_value, max_val_dup);\n  }\n  max_val = std::max(max_val, static_cast<int32>(vmaxvq_u8(max_val_dup)));\n#endif\n\n  for (; j < size; ++j) {\n    max_val = std::max(max_val, static_cast<int32_t>(input_data[j] ^ offset));\n  }\n  return max_val;\n}\n\n#ifdef USE_NEON\n// Value_to_store layout:\n// [high_high, high_low, low_high, low_low].\ninline void StoreValue(int32x4x4_t value_to_store, int8_t* output) {\n  const int16x8_t result_1 = vcombine_s16(vqmovn_s32(value_to_store.val[1]),\n                                          vqmovn_s32(value_to_store.val[0]));\n  const int16x8_t result_2 = vcombine_s16(vqmovn_s32(value_to_store.val[3]),\n                                          vqmovn_s32(value_to_store.val[2]));\n  const int8x16_t result =\n      vcombine_s8(vqmovn_s16(result_2), vqmovn_s16(result_1));\n  vst1q_s8(output, result);\n}\n\n// Value_to_store layout:\n// [high_high, high_low, low_high, low_low].\ninline void StoreValue(int32x4x4_t value_to_store, uint8_t* output) {\n  const uint16x8_t result_1 =\n      vcombine_u16(vqmovn_u32(vreinterpretq_u32_s32(value_to_store.val[1])),\n                   vqmovn_u32(vreinterpretq_u32_s32(value_to_store.val[0])));\n  const uint16x8_t result_2 =\n      vcombine_u16(vqmovn_u32(vreinterpretq_u32_s32(value_to_store.val[3])),\n                   vqmovn_u32(vreinterpretq_u32_s32(value_to_store.val[2])));\n  const uint8x16_t result =\n      vcombine_u8(vqmovn_u16(result_2), vqmovn_u16(result_1));\n  vst1q_u8(output, result);\n}\n\n#endif\n\ntemplate <typename In, typename Out>\ninline void SoftmaxInt8LUT(const SoftmaxParams& params,\n                           const RuntimeShape& input_shape,\n                           const In* input_data,\n                           const RuntimeShape& output_shape, Out* output_data) {\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int excluding_last_dim =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int last_dim =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  const int32_t clamp_max = std::numeric_limits<Out>::max();\n  const int32_t clamp_min = std::numeric_limits<Out>::min();\n\n  // Offset is used to interpret the input data \"correctly\".\n  // If the input is uint8, the data will be unchanged.\n  // If the input is int8, since it will be reinterpret as uint8.\n  // e.g.,\n  // int8 127 will be applied \"offset\" to become 255 in uint8.\n  uint8_t offset = 0;\n  if (std::is_same<In, int8>::value) {\n    offset = 0x80;\n  }\n\n  const uint8_t* input_data_uint = reinterpret_cast<const uint8_t*>(input_data);\n\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n  // This code uses ARM64-only instructions.\n  // TODO(b/143709993): Port to ARMv7\n\n  // Load the tables into registers. (4*4 128-bit registers)\n  uint8x16x4_t table1[4];\n  table1[0] = vld1q_u8_x4(params.uint8_table1 + 16 * 4 * 0);\n  table1[1] = vld1q_u8_x4(params.uint8_table1 + 16 * 4 * 1);\n  table1[2] = vld1q_u8_x4(params.uint8_table1 + 16 * 4 * 2);\n  table1[3] = vld1q_u8_x4(params.uint8_table1 + 16 * 4 * 3);\n\n  uint8x16x4_t table2[4];\n  table2[0] = vld1q_u8_x4(params.uint8_table2 + 16 * 4 * 0);\n  table2[1] = vld1q_u8_x4(params.uint8_table2 + 16 * 4 * 1);\n  table2[2] = vld1q_u8_x4(params.uint8_table2 + 16 * 4 * 2);\n  table2[3] = vld1q_u8_x4(params.uint8_table2 + 16 * 4 * 3);\n#endif\n\n  for (int i = 0; i < excluding_last_dim; ++i) {\n    // Find max quantized value.\n    int32_t max_val = FindMaxValue(last_dim, input_data_uint, offset);\n\n    int32 sum_exp = 0;\n    const int32_t max_uint8 = std::numeric_limits<uint8_t>::max();\n    const uint8_t table_offset = max_uint8 - max_val;\n\n    // Calculate normalizer sum(exp(x)).\n    int sum_j = 0;\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n    uint8x16_t table_offset_dup = vdupq_n_u8(table_offset);\n    uint8x16_t offset_dup = vdupq_n_u8(offset);\n    uint32x4_t sum_4 = vdupq_n_u32(0);\n    const int multiplier_shift = 8;\n    for (; sum_j <= last_dim - 16; sum_j += 16) {\n      uint8x16_t input_value = vld1q_u8(input_data_uint + sum_j);\n      input_value = veorq_u8(input_value, offset_dup);\n      input_value = vaddq_u8(input_value, table_offset_dup);\n\n      const uint8x16_t output1 = aarch64_lookup_vector(table1, input_value);\n      const uint8x16_t output2 = aarch64_lookup_vector(table2, input_value);\n\n      uint16x8_t exp_value1 =\n          vshll_n_u8(vget_high_u8(output1), multiplier_shift);\n      uint16x8_t exp_value2 =\n          vshll_n_u8(vget_low_u8(output1), multiplier_shift);\n\n      exp_value1 = vaddw_u8(exp_value1, vget_high_u8(output2));\n      exp_value2 = vaddw_u8(exp_value2, vget_low_u8(output2));\n\n      sum_4 = vpadalq_u16(sum_4, exp_value1);\n      sum_4 = vpadalq_u16(sum_4, exp_value2);\n    }\n    int temp = vgetq_lane_u32(sum_4, 0) + vgetq_lane_u32(sum_4, 1) +\n               vgetq_lane_u32(sum_4, 2) + vgetq_lane_u32(sum_4, 3);\n    sum_exp += temp;\n\n#endif\n    for (; sum_j < last_dim; ++sum_j) {\n      const uint8_t index = (input_data_uint[sum_j] ^ offset) + table_offset;\n\n      uint8_t part1 = params.uint8_table1[index];\n      uint8_t part2 = params.uint8_table2[index];\n      sum_exp += ((part1 << 8) + part2);\n    }\n\n    const float inv_sum_exp = 1.0f / (sum_exp * params.scale);\n\n    int32 multiplier, shift;\n    QuantizeMultiplier(inv_sum_exp, &multiplier, &shift);\n\n    // Normalize and quantize probabilities.\n    int j = 0;\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n    const int32x4_t output_zp_dup = vdupq_n_s32(params.zero_point);\n    const int32x4_t max_val_dup = vdupq_n_s32(clamp_max);\n    const int32x4_t min_val_dup = vdupq_n_s32(clamp_min);\n\n    for (; j <= last_dim - 16; j += 16) {\n      uint8x16_t input_value = vld1q_u8(input_data_uint + j);\n      input_value = veorq_u8(input_value, offset_dup);\n      input_value = vaddq_u8(input_value, table_offset_dup);\n\n      const uint8x16_t output1 = aarch64_lookup_vector(table1, input_value);\n      const uint8x16_t output2 = aarch64_lookup_vector(table2, input_value);\n\n      uint16x8_t exp_value1 =\n          vshll_n_u8(vget_high_u8(output1), multiplier_shift);\n      uint16x8_t exp_value2 =\n          vshll_n_u8(vget_low_u8(output1), multiplier_shift);\n\n      exp_value1 = vaddw_u8(exp_value1, vget_high_u8(output2));\n      exp_value2 = vaddw_u8(exp_value2, vget_low_u8(output2));\n\n      int32x4x4_t output_value;\n      output_value.val[0] =\n          vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(exp_value1)));\n      output_value.val[1] =\n          vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(exp_value1)));\n      output_value.val[2] =\n          vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(exp_value2)));\n      output_value.val[3] =\n          vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(exp_value2)));\n\n      int32x4x4_t temp_val =\n          MultiplyByQuantizedMultiplier4Rows(output_value, multiplier, shift);\n\n      temp_val.val[0] = vaddq_s32(temp_val.val[0], output_zp_dup);\n      temp_val.val[1] = vaddq_s32(temp_val.val[1], output_zp_dup);\n      temp_val.val[2] = vaddq_s32(temp_val.val[2], output_zp_dup);\n      temp_val.val[3] = vaddq_s32(temp_val.val[3], output_zp_dup);\n\n      temp_val.val[0] =\n          vmaxq_s32(vminq_s32(temp_val.val[0], max_val_dup), min_val_dup);\n      temp_val.val[1] =\n          vmaxq_s32(vminq_s32(temp_val.val[1], max_val_dup), min_val_dup);\n      temp_val.val[2] =\n          vmaxq_s32(vminq_s32(temp_val.val[2], max_val_dup), min_val_dup);\n      temp_val.val[3] =\n          vmaxq_s32(vminq_s32(temp_val.val[3], max_val_dup), min_val_dup);\n\n      StoreValue(temp_val, output_data + j);\n    }\n#endif\n    for (; j < last_dim; ++j) {\n      const uint8_t index = (input_data_uint[j] ^ offset) + table_offset;\n      const uint8_t part1 = params.uint8_table1[index];\n      const uint8_t part2 = params.uint8_table2[index];\n      const int32_t exp_value = (part1 << 8) + part2;\n      const int32_t output_value =\n          MultiplyByQuantizedMultiplier(exp_value, multiplier, shift);\n\n      output_data[j] = static_cast<Out>(std::max(\n          std::min(clamp_max, output_value + params.zero_point), clamp_min));\n    }\n    input_data_uint += last_dim;\n    output_data += last_dim;\n  }\n}\n\ninline void LogSoftmax(const SoftmaxParams& params,\n                       const RuntimeShape& input_shape, const float* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"LogSoftmax\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  for (int i = 0; i < outer_size; ++i) {\n    VectorMap<const float> block_input(input_data + i * depth, depth, 1);\n    VectorMap<float> block_output(output_data + i * depth, depth, 1);\n    // Find max element value which we'll use to ensure numerical stability\n    // taking advantage of the following equality:\n    // log(exp(x[i])/sum(exp(x[i]))) == log(exp(x[i]+C)/sum(exp(x[i]+C)))\n    const float max = block_input.maxCoeff();\n    const float log_sum = std::log((block_input.array() - max).exp().sum());\n    block_output = block_input.array() - max - log_sum;\n  }\n}\n\n// Backwards compatibility. Less optimized than below version.\ninline void LogSoftmax(const SoftmaxParams& params,\n                       const RuntimeShape& input_shape, const uint8* input_data,\n                       const RuntimeShape& output_shape, uint8* output_data) {\n  reference_ops::LogSoftmax(params, input_shape, input_data, output_shape,\n                            output_data);\n}\n\n// Compute LogSoftmax as (x - x_max) - ln(sum(e^(x_i - x_max)...)\n// as done in tf.nn.log_softmax to prevent underflow and overflow.\n// This is in contrast to just log(softmax(x))\n//\n// To handle quantization, first dequantize the inputs (from doing\n// e^(input scale * val) where we ignore the zero point since it cancels\n// out during subtraction due to the ln) and do a rescale at the end to int8.\n//\n// Notably this makes use of float and is intended as the optimized\n// form for quantized execution on CPU. For a fully integer version,\n// see the reference op.\n//\n// TODO(tflite): notes for optimization:\n// 1) See if e^ is also bottleneck in the reference fully-integer\n// version and apply lookup there and compare.\ntemplate <typename T>\ninline void LogSoftmax(const SoftmaxParams& params, float input_scale,\n                       const RuntimeShape& input_shape, const T* input_data,\n                       const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"LogSoftmax\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int excluding_last_dim =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int last_dim =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  const int32_t clamp_max = std::numeric_limits<T>::max();\n  const int32_t clamp_min = std::numeric_limits<T>::min();\n\n  for (int i = 0; i < excluding_last_dim; ++i) {\n    T max_val = std::numeric_limits<T>::min();\n    // Find max quantized value.\n    for (int j = 0; j < last_dim; ++j) {\n      max_val = std::max(max_val, input_data[j]);\n    }\n\n    float sum_exp = 0.0f;\n    const int32_t max_uint8 = std::numeric_limits<uint8>::max();\n    // Offset into table to compute exp(scale*(x - xmax)) instead of\n    // exp(scale*(x)) to prevent overflow.\n    const float* table_offset = &params.table[max_uint8 - max_val];\n    // Calculate sum(exp(scale*(x - x_max))).\n    for (int j = 0; j < last_dim; ++j) {\n      sum_exp += table_offset[input_data[j]];\n    }\n    const float log_sum_exp = std::log(sum_exp);\n\n    // params.scale is the output scale.\n    const float scale = input_scale / params.scale;\n    const float precomputed =\n        (input_scale * max_val + log_sum_exp) / params.scale;\n    for (int j = 0; j < last_dim; ++j) {\n      // Equivalent to (input_scale * (input_data[j] - max_val) - log_sum_exp) /\n      // output_scale.\n      const float log_prob = scale * input_data[j] - precomputed;\n\n      // TODO(tflite): look into better solution.\n      // Use std::rint over std::round (which is used in\n      // FakeQuant) since it's multiple times faster on tested arm32.\n      const int32_t prob_quantized = std::rint(log_prob) + params.zero_point;\n      output_data[j] = static_cast<T>(\n          std::max(std::min(clamp_max, prob_quantized), clamp_min));\n    }\n    input_data += last_dim;\n    output_data += last_dim;\n  }\n}\n\ninline void Logistic(const RuntimeShape& input_shape, const float* input_data,\n                     const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() =\n      input_map.array().unaryExpr(Eigen::internal::scalar_logistic_op<float>());\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// uniform between data types.\ninline void Logistic(const LogisticParams&, const RuntimeShape& input_shape,\n                     const float* input_data, const RuntimeShape& output_shape,\n                     float* output_data) {\n  // Drop params: not needed.\n  Logistic(input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const LogisticParams& params,\n                     const RuntimeShape& input_shape, const int16* input_data,\n                     const RuntimeShape& output_shape, int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic/Int16\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n  }\n\n  int c = 0;\n  const int16* input_data_ptr = input_data;\n  int16* output_data_ptr = output_data;\n#ifdef GEMMLOWP_NEON\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<int16x8_t, 3>;\n\n    for (; c <= flat_size - 16; c += 16) {\n      F3 input0 = F3::FromRaw(vld1q_s16(input_data_ptr));\n      F3 input1 = F3::FromRaw(vld1q_s16(input_data_ptr + 8));\n      F0 output0 = gemmlowp::logistic(input0);\n      F0 output1 = gemmlowp::logistic(input1);\n      vst1q_s16(output_data_ptr, output0.raw());\n      vst1q_s16(output_data_ptr + 8, output1.raw());\n\n      input_data_ptr += 16;\n      output_data_ptr += 16;\n    }\n    for (; c <= flat_size - 8; c += 8) {\n      F3 input = F3::FromRaw(vld1q_s16(input_data_ptr));\n      F0 output = gemmlowp::logistic(input);\n      vst1q_s16(output_data_ptr, output.raw());\n\n      input_data_ptr += 8;\n      output_data_ptr += 8;\n    }\n  }\n#endif\n#ifdef GEMMLOWP_SSE4\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<gemmlowp::int16x8_m128i, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<gemmlowp::int16x8_m128i, 3>;\n\n    for (; c <= flat_size - 16; c += 16) {\n      F3 input0 = F3::FromRaw(gemmlowp::to_int16x8_m128i(\n          _mm_loadu_si128(reinterpret_cast<const __m128i*>(input_data_ptr))));\n      F3 input1 = F3::FromRaw(gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n          reinterpret_cast<const __m128i*>(input_data_ptr + 8))));\n      F0 output0 = gemmlowp::logistic(input0);\n      F0 output1 = gemmlowp::logistic(input1);\n      _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                       output0.raw().v);\n      _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr + 8),\n                       output1.raw().v);\n      input_data_ptr += 16;\n      output_data_ptr += 16;\n    }\n    for (; c <= flat_size - 8; c += 8) {\n      F3 input = F3::FromRaw(gemmlowp::to_int16x8_m128i(\n          _mm_loadu_si128(reinterpret_cast<const __m128i*>(input_data_ptr))));\n      F0 output = gemmlowp::logistic(input);\n      _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                       output.raw().v);\n      input_data_ptr += 8;\n      output_data_ptr += 8;\n    }\n  }\n#endif\n\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<std::int16_t, 3>;\n\n    for (; c < flat_size; ++c) {\n      F3 input = F3::FromRaw(*input_data_ptr);\n      F0 output = gemmlowp::logistic(input);\n      *output_data_ptr = output.raw();\n\n      ++input_data_ptr;\n      ++output_data_ptr;\n    }\n  }\n}\n\ninline void Tanh(const RuntimeShape& input_shape, const float* input_data,\n                 const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Tanh\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() = input_map.array().tanh();\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// uniform between data types.\ninline void Tanh(const TanhParams&, const RuntimeShape& input_shape,\n                 const float* input_data, const RuntimeShape& output_shape,\n                 float* output_data) {\n  // Drop params: not needed.\n  Tanh(input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const TanhParams& params, const RuntimeShape& input_shape,\n                 const int16* input_data, const RuntimeShape& output_shape,\n                 int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Tanh/Int16\");\n  const int input_left_shift = params.input_left_shift;\n  // Support for shifts is limited until we have a parameterized version of\n  // SaturatingRoundingMultiplyByPOT().\n  TFLITE_DCHECK_GE(input_left_shift, 0);\n  TFLITE_DCHECK_LE(input_left_shift, 1);\n\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n  const int16* input_data_ptr = input_data;\n  int16* output_data_ptr = output_data;\n#ifdef GEMMLOWP_NEON\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<int16x8_t, 3>;\n\n    if (input_left_shift == 0) {\n      for (; c <= flat_size - 16; c += 16) {\n        F3 input0 = F3::FromRaw(vld1q_s16(input_data_ptr));\n        F3 input1 = F3::FromRaw(vld1q_s16(input_data_ptr + 8));\n        F0 output0 = gemmlowp::tanh(input0);\n        F0 output1 = gemmlowp::tanh(input1);\n        vst1q_s16(output_data_ptr, output0.raw());\n        vst1q_s16(output_data_ptr + 8, output1.raw());\n\n        input_data_ptr += 16;\n        output_data_ptr += 16;\n      }\n      for (; c <= flat_size - 8; c += 8) {\n        F3 input = F3::FromRaw(vld1q_s16(input_data_ptr));\n        F0 output = gemmlowp::tanh(input);\n        vst1q_s16(output_data_ptr, output.raw());\n\n        input_data_ptr += 8;\n        output_data_ptr += 8;\n      }\n    } else {\n      for (; c <= flat_size - 16; c += 16) {\n        F3 input0 = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            vld1q_s16(input_data_ptr)));\n        F3 input1 = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            vld1q_s16(input_data_ptr + 8)));\n        F0 output0 = gemmlowp::tanh(input0);\n        F0 output1 = gemmlowp::tanh(input1);\n        vst1q_s16(output_data_ptr, output0.raw());\n        vst1q_s16(output_data_ptr + 8, output1.raw());\n\n        input_data_ptr += 16;\n        output_data_ptr += 16;\n      }\n      for (; c <= flat_size - 8; c += 8) {\n        F3 input = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            vld1q_s16(input_data_ptr)));\n        F0 output = gemmlowp::tanh(input);\n        vst1q_s16(output_data_ptr, output.raw());\n\n        input_data_ptr += 8;\n        output_data_ptr += 8;\n      }\n    }\n  }\n#endif\n#ifdef GEMMLOWP_SSE4\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<gemmlowp::int16x8_m128i, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<gemmlowp::int16x8_m128i, 3>;\n\n    if (input_left_shift == 0) {\n      for (; c <= flat_size - 16; c += 16) {\n        F3 input0 = F3::FromRaw(gemmlowp::to_int16x8_m128i(\n            _mm_loadu_si128(reinterpret_cast<const __m128i*>(input_data_ptr))));\n        F3 input1 = F3::FromRaw(gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n            reinterpret_cast<const __m128i*>(input_data_ptr + 8))));\n        F0 output0 = gemmlowp::tanh(input0);\n        F0 output1 = gemmlowp::tanh(input1);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                         output0.raw().v);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr + 8),\n                         output1.raw().v);\n\n        input_data_ptr += 16;\n        output_data_ptr += 16;\n      }\n      for (; c <= flat_size - 8; c += 8) {\n        F3 input = F3::FromRaw(gemmlowp::to_int16x8_m128i(\n            _mm_loadu_si128(reinterpret_cast<const __m128i*>(input_data_ptr))));\n        F0 output = gemmlowp::tanh(input);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                         output.raw().v);\n        input_data_ptr += 8;\n        output_data_ptr += 8;\n      }\n    } else {\n      for (; c <= flat_size - 16; c += 16) {\n        F3 input0 = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n                reinterpret_cast<const __m128i*>(input_data_ptr)))));\n        F3 input1 = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n                reinterpret_cast<const __m128i*>(input_data_ptr + 8)))));\n        F0 output0 = gemmlowp::tanh(input0);\n        F0 output1 = gemmlowp::tanh(input1);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                         output0.raw().v);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr + 8),\n                         output1.raw().v);\n\n        input_data_ptr += 16;\n        output_data_ptr += 16;\n      }\n      for (; c <= flat_size - 8; c += 8) {\n        F3 input = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n                reinterpret_cast<const __m128i*>(input_data_ptr)))));\n        F0 output = gemmlowp::tanh(input);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                         output.raw().v);\n        input_data_ptr += 8;\n        output_data_ptr += 8;\n      }\n    }\n  }\n#endif\n\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<std::int16_t, 3>;\n\n    if (input_left_shift == 0) {\n      for (; c < flat_size; ++c) {\n        F3 input = F3::FromRaw(*input_data_ptr);\n        F0 output = gemmlowp::tanh(input);\n        *output_data_ptr = output.raw();\n\n        ++input_data_ptr;\n        ++output_data_ptr;\n      }\n    } else {\n      for (; c < flat_size; ++c) {\n        F3 input = F3::FromRaw(\n            gemmlowp::SaturatingRoundingMultiplyByPOT<1>(*input_data_ptr));\n        F0 output = gemmlowp::tanh(input);\n        *output_data_ptr = output.raw();\n\n        ++input_data_ptr;\n        ++output_data_ptr;\n      }\n    }\n  }\n}\n\ntemplate <typename SrcT, typename DstT>\ninline void Cast(const RuntimeShape& input_shape, const SrcT* input_data,\n                 const RuntimeShape& output_shape, DstT* output_data) {\n  ruy::profiler::ScopeLabel label(\"Cast\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() = input_map.array().template cast<DstT>();\n}\n\ninline void Floor(const RuntimeShape& input_shape, const float* input_data,\n                  const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Floor\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() = Eigen::floor(input_map.array());\n}\n\ninline void Ceil(const RuntimeShape& input_shape, const float* input_data,\n                 const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Ceil\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() = Eigen::ceil(input_map.array());\n}\n\n// Helper methods for BatchToSpaceND.\n// `spatial_index_dim` specifies post-crop offset index in this spatial\n// dimension, i.e. spatial offset introduced by flattening batch to spatial\n// dimension minus the crop size at beginning. `block_shape_dim` is the block\n// size in current dimension. `input_dim` and `output_dim` are input and output\n// size of BatchToSpaceND operation in current dimension.\n// Output start index is inclusive and end index is exclusive.\ninline void GetIndexRange(int spatial_index_dim, int block_shape_dim,\n                          int input_dim, int output_dim, int* start_index,\n                          int* end_index) {\n  // (*start_index) * block_shape_dim is effectively rounded up to the next\n  // multiple of block_shape_dim by the integer division.\n  *start_index =\n      std::max(0, (-spatial_index_dim + block_shape_dim - 1) / block_shape_dim);\n  // Similarly, (*end_index) * block_shape_dim is rounded up too (note that\n  // end_index is exclusive).\n  *end_index = std::min(\n      input_dim,\n      (output_dim - spatial_index_dim + block_shape_dim - 1) / block_shape_dim);\n}\n\ntemplate <typename T>\ninline void BatchToSpaceND(\n    const RuntimeShape& unextended_input1_shape, const T* input1_data,\n    const RuntimeShape& unextended_input2_shape, const int32* block_shape_data,\n    const RuntimeShape& unextended_input3_shape, const int32* crops_data,\n    const RuntimeShape& unextended_output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"BatchToSpaceND\");\n\n  TFLITE_DCHECK_GE(unextended_input1_shape.DimensionsCount(), 3);\n  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(unextended_input1_shape.DimensionsCount(),\n                   unextended_output_shape.DimensionsCount());\n\n  // Extends the input/output shape from 3D to 4D if needed, NHC -> NH1C.\n  auto extend_shape = [](const RuntimeShape& shape) {\n    if (shape.DimensionsCount() == 4) {\n      return shape;\n    }\n    RuntimeShape new_shape(4, 1);\n    new_shape.SetDim(0, shape.Dims(0));\n    new_shape.SetDim(1, shape.Dims(1));\n    new_shape.SetDim(3, shape.Dims(2));\n    return new_shape;\n  };\n  const RuntimeShape input1_shape = extend_shape(unextended_input1_shape);\n  const RuntimeShape output_shape = extend_shape(unextended_output_shape);\n\n  const int output_width = output_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_batch_size = output_shape.Dims(0);\n\n  const int depth = input1_shape.Dims(3);\n  const int input_width = input1_shape.Dims(2);\n  const int input_height = input1_shape.Dims(1);\n  const int input_batch_size = input1_shape.Dims(0);\n\n  const int block_shape_height = block_shape_data[0];\n  const int block_shape_width =\n      unextended_input1_shape.DimensionsCount() == 4 ? block_shape_data[1] : 1;\n  const int crops_top = crops_data[0];\n  const int crops_left =\n      unextended_input1_shape.DimensionsCount() == 4 ? crops_data[2] : 0;\n\n  for (int in_batch = 0; in_batch < input_batch_size; ++in_batch) {\n    const int out_batch = in_batch % output_batch_size;\n    const int spatial_offset = in_batch / output_batch_size;\n\n    int in_h_start = 0;\n    int in_h_end = 0;\n    // GetIndexRange ensures start and end indices are in [0, output_height).\n    GetIndexRange(spatial_offset / block_shape_width - crops_top,\n                  block_shape_height, input_height, output_height, &in_h_start,\n                  &in_h_end);\n\n    for (int in_h = in_h_start; in_h < in_h_end; ++in_h) {\n      const int out_h = in_h * block_shape_height +\n                        spatial_offset / block_shape_width - crops_top;\n      TFLITE_DCHECK_GE(out_h, 0);\n      TFLITE_DCHECK_LT(out_h, output_height);\n\n      int in_w_start = 0;\n      int in_w_end = 0;\n      // GetIndexRange ensures start and end indices are in [0, output_width).\n      GetIndexRange(spatial_offset % block_shape_width - crops_left,\n                    block_shape_width, input_width, output_width, &in_w_start,\n                    &in_w_end);\n\n      for (int in_w = in_w_start; in_w < in_w_end; ++in_w) {\n        const int out_w = in_w * block_shape_width +\n                          spatial_offset % block_shape_width - crops_left;\n        TFLITE_DCHECK_GE(out_w, 0);\n        TFLITE_DCHECK_LT(out_w, output_width);\n        T* out = output_data + Offset(output_shape, out_batch, out_h, out_w, 0);\n        const T* in =\n            input1_data + Offset(input1_shape, in_batch, in_h, in_w, 0);\n        memcpy(out, in, depth * sizeof(T));\n      }\n    }\n  }\n}\n\ntemplate <typename T>\nvoid TypedMemset(void* ptr, T value, size_t num) {\n  // Optimization for common cases where memset() will suffice.\n  if (value == 0 || std::is_same<T, uint8_t>::value) {\n    memset(ptr, value, num * sizeof(T));\n  } else {\n    // Default implementation for cases where memset() will not preserve the\n    // bytes, e.g., typically when sizeof(T) > sizeof(uint8_t).\n    char* pos = static_cast<char*>(ptr);\n    for (size_t i = 0; i < num; ++i) {\n      memcpy(pos, &value, sizeof(T));\n      pos = pos + sizeof(T);\n    }\n  }\n}\n\n// This makes heavy use of Offset, along with conditional branches. There may be\n// opportunities for improvement.\n//\n// There are two versions of pad: Pad and PadV2.  In PadV2 there is a second\n// scalar input that provides the padding value.  Therefore pad_value_ptr can be\n// equivalent to a simple input1_data.  For Pad, it should point to a zero\n// value.\n//\n// Note that two typenames are required, so that T=P=int32 is considered a\n// specialization distinct from P=int32.\ntemplate <typename T, typename P>\ninline void PadImpl(const tflite::PadParams& op_params,\n                    const RuntimeShape& input_shape, const T* input_data,\n                    const P* pad_value_ptr, const RuntimeShape& output_shape,\n                    T* output_data) {\n  ruy::profiler::ScopeLabel label(\"PadImpl\");\n  const int max_supported_dims = 5;\n  const RuntimeShape ext_input_shape =\n      RuntimeShape::ExtendedShape(max_supported_dims, input_shape);\n  const RuntimeShape ext_output_shape =\n      RuntimeShape::ExtendedShape(max_supported_dims, output_shape);\n  TFLITE_DCHECK_LE(op_params.left_padding_count, max_supported_dims);\n  TFLITE_DCHECK_LE(op_params.right_padding_count, max_supported_dims);\n\n  // Pad kernels are limited to max 4 dimensions. Copy inputs so we can pad them\n  // to 4 dims (yes, we are \"padding the padding\").\n  std::vector<int> left_padding_copy(max_supported_dims, 0);\n  const int left_padding_extend =\n      max_supported_dims - op_params.left_padding_count;\n  for (int i = 0; i < op_params.left_padding_count; ++i) {\n    left_padding_copy[left_padding_extend + i] = op_params.left_padding[i];\n  }\n  std::vector<int> right_padding_copy(max_supported_dims, 0);\n  const int right_padding_extend =\n      max_supported_dims - op_params.right_padding_count;\n  for (int i = 0; i < op_params.right_padding_count; ++i) {\n    right_padding_copy[right_padding_extend + i] = op_params.right_padding[i];\n  }\n\n  const int output_batch = ext_output_shape.Dims(0);\n  const int output_spatial_dim1 = ext_output_shape.Dims(1);\n  const int output_spatial_dim2 = ext_output_shape.Dims(2);\n  const int output_spatial_dim3 = ext_output_shape.Dims(3);\n  const int output_channel = ext_output_shape.Dims(4);\n\n  const int left_b_padding = left_padding_copy[0];\n  const int left_s1_padding = left_padding_copy[1];\n  const int left_s2_padding = left_padding_copy[2];\n  const int left_s3_padding = left_padding_copy[3];\n  const int left_c_padding = left_padding_copy[4];\n\n  const int right_b_padding = right_padding_copy[0];\n  const int right_s1_padding = right_padding_copy[1];\n  const int right_s2_padding = right_padding_copy[2];\n  const int right_s3_padding = right_padding_copy[3];\n  const int right_c_padding = right_padding_copy[4];\n\n  const int input_depth = ext_input_shape.Dims(4);\n  const T pad_value = *pad_value_ptr;\n\n  if (left_b_padding != 0) {\n    TypedMemset<T>(output_data, pad_value,\n                   left_b_padding * output_spatial_dim1 * output_spatial_dim2 *\n                       output_spatial_dim3 * output_channel);\n  }\n  for (int out_b = left_b_padding; out_b < output_batch - right_b_padding;\n       ++out_b) {\n    if (left_s1_padding != 0) {\n      TypedMemset<T>(output_data + Offset(ext_output_shape, out_b, 0, 0, 0, 0),\n                     pad_value,\n                     left_s1_padding * output_spatial_dim2 *\n                         output_spatial_dim3 * output_channel);\n    }\n    for (int out_p = left_s1_padding;\n         out_p < output_spatial_dim1 - right_s1_padding; ++out_p) {\n      if (left_s2_padding != 0) {\n        TypedMemset<T>(\n            output_data + Offset(ext_output_shape, out_b, out_p, 0, 0, 0),\n            pad_value, left_s2_padding * output_spatial_dim3 * output_channel);\n      }\n      for (int out_h = left_s2_padding;\n           out_h < output_spatial_dim2 - right_s2_padding; ++out_h) {\n        if (left_s3_padding != 0) {\n          TypedMemset<T>(\n              output_data + Offset(ext_output_shape, out_b, out_p, out_h, 0, 0),\n              pad_value, left_s3_padding * output_channel);\n        }\n        for (int out_w = left_s3_padding;\n             out_w < output_spatial_dim3 - right_s3_padding; ++out_w) {\n          if (left_c_padding != 0) {\n            TypedMemset<T>(output_data + Offset(ext_output_shape, out_b, out_p,\n                                                out_h, out_w, 0),\n                           pad_value, left_c_padding);\n          }\n\n          T* out = output_data + Offset(ext_output_shape, out_b, out_p, out_h,\n                                        out_w, left_c_padding);\n          const T* in = input_data +\n                        Offset(ext_input_shape, out_b - left_b_padding,\n                               out_p - left_s1_padding, out_h - left_s2_padding,\n                               out_w - left_s3_padding, 0);\n          memcpy(out, in, input_depth * sizeof(T));\n\n          if (right_c_padding != 0) {\n            TypedMemset<T>(\n                output_data + Offset(ext_output_shape, out_b, out_p, out_h,\n                                     out_w, output_channel - right_c_padding),\n                pad_value, right_c_padding);\n          }\n        }\n        if (right_s3_padding != 0) {\n          TypedMemset<T>(\n              output_data + Offset(ext_output_shape, out_b, out_p, out_h,\n                                   output_spatial_dim3 - right_s3_padding, 0),\n              pad_value, right_s3_padding * output_channel);\n        }\n      }\n      if (right_s2_padding != 0) {\n        TypedMemset<T>(\n            output_data + Offset(ext_output_shape, out_b, out_p,\n                                 output_spatial_dim2 - right_s2_padding, 0, 0),\n            pad_value, right_s2_padding * output_spatial_dim3 * output_channel);\n      }\n    }\n    if (right_s1_padding != 0) {\n      TypedMemset<T>(\n          output_data + Offset(ext_output_shape, out_b,\n                               output_spatial_dim1 - right_s1_padding, 0, 0, 0),\n          pad_value,\n          right_s1_padding * output_spatial_dim2 * output_spatial_dim3 *\n              output_channel);\n    }\n  }\n  if (right_b_padding != 0) {\n    TypedMemset<T>(\n        output_data + Offset(ext_output_shape, output_batch - right_b_padding,\n                             0, 0, 0, 0),\n        pad_value,\n        right_b_padding * output_spatial_dim1 * output_spatial_dim2 *\n            output_spatial_dim3 * output_channel);\n  }\n}\n\ntemplate <typename T, typename P>\ninline void Pad(const tflite::PadParams& op_params,\n                const RuntimeShape& input_shape, const T* input_data,\n                const P* pad_value_ptr, const RuntimeShape& output_shape,\n                T* output_data) {\n  PadImpl(op_params, input_shape, input_data, pad_value_ptr, output_shape,\n          output_data);\n}\n\n// The second (pad-value) input can be int32 when, say, the first is uint8.\ntemplate <typename T>\ninline void Pad(const tflite::PadParams& op_params,\n                const RuntimeShape& input_shape, const T* input_data,\n                const int32* pad_value_ptr, const RuntimeShape& output_shape,\n                T* output_data) {\n  const T converted_pad_value = static_cast<T>(*pad_value_ptr);\n  PadImpl(op_params, input_shape, input_data, &converted_pad_value,\n          output_shape, output_data);\n}\n\n// This version avoids conflicting template matching.\ntemplate <>\ninline void Pad(const tflite::PadParams& op_params,\n                const RuntimeShape& input_shape, const int32* input_data,\n                const int32* pad_value_ptr, const RuntimeShape& output_shape,\n                int32* output_data) {\n  PadImpl(op_params, input_shape, input_data, pad_value_ptr, output_shape,\n          output_data);\n}\n\n// TODO(b/117643175): Optimize. (This is an introductory copy of standard Pad.)\n//\n// This pad requires that (a) left and right paddings are in the 4D patterns\n// {0, h_pad, w_pad, 0}, and (b) memset can be used: *pad_value_ptr == 0 and/or\n// T is uint8.\n//\n// There are two versions of pad: Pad and PadV2.  In PadV2 there is a second\n// scalar input that provides the padding value.  Therefore pad_value_ptr can be\n// equivalent to a simple input1_data.  For Pad, it should point to a zero\n// value.\n//\n// Note that two typenames are required, so that T=P=int32 is considered a\n// specialization distinct from P=int32.\ntemplate <typename T, typename P>\ninline void PadImageStyleMemset(const tflite::PadParams& op_params,\n                                const RuntimeShape& input_shape,\n                                const T* input_data, const P* pad_value_ptr,\n                                const RuntimeShape& output_shape,\n                                T* output_data) {\n  ruy::profiler::ScopeLabel label(\"PadImageStyle\");\n  const RuntimeShape ext_input_shape =\n      RuntimeShape::ExtendedShape(4, input_shape);\n  const RuntimeShape ext_output_shape =\n      RuntimeShape::ExtendedShape(4, output_shape);\n  TFLITE_DCHECK_LE(op_params.left_padding_count, 4);\n  TFLITE_DCHECK_LE(op_params.right_padding_count, 4);\n\n  // Pad kernels are limited to max 4 dimensions. Copy inputs so we can pad them\n  // to 4 dims (yes, we are \"padding the padding\").\n  std::vector<int> left_padding_copy(4, 0);\n  const int left_padding_extend = 4 - op_params.left_padding_count;\n  for (int i = 0; i < op_params.left_padding_count; ++i) {\n    left_padding_copy[left_padding_extend + i] = op_params.left_padding[i];\n  }\n  std::vector<int> right_padding_copy(4, 0);\n  const int right_padding_extend = 4 - op_params.right_padding_count;\n  for (int i = 0; i < op_params.right_padding_count; ++i) {\n    right_padding_copy[right_padding_extend + i] = op_params.right_padding[i];\n  }\n  // The following padding restrictions are contractual requirements, and\n  // embody what it means for a padding op to be \"image-style\".\n  TFLITE_DCHECK_EQ(left_padding_copy[0], 0);\n  TFLITE_DCHECK_EQ(left_padding_copy[3], 0);\n  TFLITE_DCHECK_EQ(right_padding_copy[0], 0);\n  TFLITE_DCHECK_EQ(right_padding_copy[3], 0);\n\n  const int batch = MatchingDim(ext_input_shape, 0, ext_output_shape, 0);\n  const int output_height = ext_output_shape.Dims(1);\n  const int output_width = ext_output_shape.Dims(2);\n  const int input_height = ext_input_shape.Dims(1);\n  const int input_width = ext_input_shape.Dims(2);\n  const int depth = MatchingDim(ext_input_shape, 3, ext_output_shape, 3);\n\n  const int left_h_padding = left_padding_copy[1];\n  const int left_w_padding = left_padding_copy[2];\n  const int right_h_padding = right_padding_copy[1];\n  const int right_w_padding = right_padding_copy[2];\n\n  TFLITE_DCHECK_EQ(output_height,\n                   input_height + left_h_padding + right_h_padding);\n  TFLITE_DCHECK_EQ(output_width,\n                   input_width + left_w_padding + right_w_padding);\n\n  const T pad_value = *pad_value_ptr;\n  const int top_block_size = left_h_padding * output_width * depth;\n  const size_t num_top_block_bytes = top_block_size * sizeof(T);\n  const int bottom_block_size = right_h_padding * output_width * depth;\n  const size_t num_bottom_block_bytes = bottom_block_size * sizeof(T);\n  const int left_blocks_size = left_w_padding * depth;\n  const size_t num_left_block_bytes = left_blocks_size * sizeof(T);\n  const int right_blocks_size = right_w_padding * depth;\n  const size_t num_right_block_bytes = right_blocks_size * sizeof(T);\n  const int inner_line_size = input_width * depth;\n  const size_t num_inner_line_bytes = inner_line_size * sizeof(T);\n\n  if (input_height == 0) {\n    memset(output_data, pad_value,\n           num_top_block_bytes + num_bottom_block_bytes);\n  } else {\n    for (int i = 0; i < batch; ++i) {\n      // For each image in the batch, apply the top padding, then iterate\n      // through rows, then apply the bottom padding.\n      //\n      // By unwinding one iteration, we can combine the first left-margin\n      // padding with the top padding, and the last right-margin padding with\n      // the bottom padding.\n      memset(output_data, pad_value,\n             num_top_block_bytes + num_left_block_bytes);\n      output_data += top_block_size + left_blocks_size;\n      memcpy(output_data, input_data, num_inner_line_bytes);\n      input_data += inner_line_size;\n      output_data += inner_line_size;\n      // One iteration unwound.\n      // Unwinding this loop affords the opportunity to reorder the loop work\n      // and hence combine memset() calls.\n      //\n      // Before unwinding:\n      // for (int j = 0; j < input_height; ++j) {\n      //   // Pad on left, copy central data, pad on right.\n      //   memset(output_data, pad_value, num_left_block_bytes);\n      //   output_data += left_blocks_size;\n      //   memcpy(output_data, input_data, num_inner_line_bytes);\n      //   input_data += inner_line_size;\n      //   output_data += inner_line_size;\n      //   memset(output_data, pad_value, num_right_block_bytes);\n      //   output_data += right_blocks_size;\n      // }\n      for (int j = 1; j < input_height; ++j) {\n        memset(output_data, pad_value,\n               num_right_block_bytes + num_left_block_bytes);\n        output_data += right_blocks_size + left_blocks_size;\n        memcpy(output_data, input_data, num_inner_line_bytes);\n        input_data += inner_line_size;\n        output_data += inner_line_size;\n      }\n      memset(output_data, pad_value,\n             num_right_block_bytes + num_bottom_block_bytes);\n      output_data += right_blocks_size + bottom_block_size;\n    }\n  }\n}\n\ntemplate <typename T, typename P>\ninline void PadImageStyle(const tflite::PadParams& op_params,\n                          const RuntimeShape& input_shape, const T* input_data,\n                          const P* pad_value_ptr,\n                          const RuntimeShape& output_shape, T* output_data) {\n  reference_ops::PadImageStyle(op_params, input_shape, input_data,\n                               pad_value_ptr, output_shape, output_data);\n}\n\ntemplate <typename P>\ninline void PadImageStyle(const tflite::PadParams& op_params,\n                          const RuntimeShape& input_shape,\n                          const uint8* input_data, const P* pad_value_ptr,\n                          const RuntimeShape& output_shape,\n                          uint8* output_data) {\n  PadImageStyleMemset(op_params, input_shape, input_data, pad_value_ptr,\n                      output_shape, output_data);\n}\n\ntemplate <typename P>\ninline void PadImageStyle(const tflite::PadParams& op_params,\n                          const RuntimeShape& input_shape,\n                          const float* input_data, const P* pad_value_ptr,\n                          const RuntimeShape& output_shape,\n                          float* output_data) {\n  const float converted_pad_value = static_cast<float>(*pad_value_ptr);\n  if (converted_pad_value == 0.0f) {\n    PadImageStyleMemset(op_params, input_shape, input_data, pad_value_ptr,\n                        output_shape, output_data);\n  } else {\n    PadImpl(op_params, input_shape, input_data, pad_value_ptr, output_shape,\n            output_data);\n  }\n}\n\ntemplate <typename T>\ninline void Slice(const tflite::SliceParams& op_params,\n                  const RuntimeShape& input_shape,\n                  const RuntimeShape& output_shape,\n                  SequentialTensorWriter<T>* writer) {\n  ruy::profiler::ScopeLabel label(\"Slice\");\n  const RuntimeShape ext_shape = RuntimeShape::ExtendedShape(5, input_shape);\n  TFLITE_DCHECK_LE(op_params.begin_count, 5);\n  TFLITE_DCHECK_LE(op_params.size_count, 5);\n  const int begin_count = op_params.begin_count;\n  const int size_count = op_params.size_count;\n  // We front-pad the begin and size vectors.\n  std::array<int, 5> start;\n  std::array<int, 5> stop;\n  for (int i = 0; i < 5; ++i) {\n    int padded_i = 5 - i;\n    start[i] =\n        begin_count < padded_i ? 0 : op_params.begin[begin_count - padded_i];\n    stop[i] =\n        (size_count < padded_i || op_params.size[size_count - padded_i] == -1)\n            ? ext_shape.Dims(i)\n            : start[i] + op_params.size[size_count - padded_i];\n  }\n\n  for (int i0 = start[0]; i0 < stop[0]; ++i0) {\n    for (int i1 = start[1]; i1 < stop[1]; ++i1) {\n      for (int i2 = start[2]; i2 < stop[2]; ++i2) {\n        for (int i3 = start[3]; i3 < stop[3]; ++i3) {\n          const int len = stop[4] - start[4];\n          if (len > 0)\n            writer->WriteN(Offset(ext_shape, i0, i1, i2, i3, start[4]), len);\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename T>\ninline void Slice(const tflite::SliceParams& op_params,\n                  const RuntimeShape& input_shape, const T* input_data,\n                  const RuntimeShape& output_shape, T* output_data) {\n  SequentialTensorWriter<T> writer(input_data, output_data);\n  return Slice(op_params, input_shape, output_shape, &writer);\n}\n\ntemplate <typename T>\ninline void Slice(const tflite::SliceParams& op_params,\n                  const RuntimeShape& input_shape, const TfLiteTensor* input,\n                  const RuntimeShape& output_shape, TfLiteTensor* output) {\n  SequentialTensorWriter<T> writer(input, output);\n  return Slice(op_params, input_shape, output_shape, &writer);\n}\n\n// Note: This implementation is only optimized for the case where the inner\n// stride == 1.\ntemplate <typename T>\ninline void StridedSlice(const tflite::StridedSliceParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const RuntimeShape& unextended_output_shape,\n                         SequentialTensorWriter<T>* writer) {\n  using strided_slice::LoopCondition;\n  using strided_slice::StartForAxis;\n  using strided_slice::StopForAxis;\n\n  ruy::profiler::ScopeLabel label(\"StridedSlice\");\n\n  // Note that the output_shape is not used herein.\n  tflite::StridedSliceParams params_copy = op_params;\n\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 5);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(5, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(5, unextended_output_shape);\n\n  // Reverse and pad to 5 dimensions because that is what the runtime code\n  // requires (ie. all shapes must be 5D and are given backwards).\n  strided_slice::StridedSlicePadIndices(&params_copy, 5);\n\n  const int start_0 = StartForAxis(params_copy, input_shape, 0);\n  const int stop_0 = StopForAxis(params_copy, input_shape, 0, start_0);\n  const int start_1 = StartForAxis(params_copy, input_shape, 1);\n  const int stop_1 = StopForAxis(params_copy, input_shape, 1, start_1);\n  const int start_2 = StartForAxis(params_copy, input_shape, 2);\n  const int stop_2 = StopForAxis(params_copy, input_shape, 2, start_2);\n  const int start_3 = StartForAxis(params_copy, input_shape, 3);\n  const int stop_3 = StopForAxis(params_copy, input_shape, 3, start_3);\n  const int start_4 = StartForAxis(params_copy, input_shape, 4);\n  const int stop_4 = StopForAxis(params_copy, input_shape, 4, start_4);\n  const bool inner_stride_is_1 = params_copy.strides[4] == 1;\n\n  for (int offset_0 = start_0 * input_shape.Dims(1),\n           end_0 = stop_0 * input_shape.Dims(1),\n           step_0 = params_copy.strides[0] * input_shape.Dims(1);\n       !LoopCondition(offset_0, end_0, params_copy.strides[0]);\n       offset_0 += step_0) {\n    for (int offset_1 = (offset_0 + start_1) * input_shape.Dims(2),\n             end_1 = (offset_0 + stop_1) * input_shape.Dims(2),\n             step_1 = params_copy.strides[1] * input_shape.Dims(2);\n         !LoopCondition(offset_1, end_1, params_copy.strides[1]);\n         offset_1 += step_1) {\n      for (int offset_2 = (offset_1 + start_2) * input_shape.Dims(3),\n               end_2 = (offset_1 + stop_2) * input_shape.Dims(3),\n               step_2 = params_copy.strides[2] * input_shape.Dims(3);\n           !LoopCondition(offset_2, end_2, params_copy.strides[2]);\n           offset_2 += step_2) {\n        for (int offset_3 = (offset_2 + start_3) * input_shape.Dims(4),\n                 end_3 = (offset_2 + stop_3) * input_shape.Dims(4),\n                 step_3 = params_copy.strides[3] * input_shape.Dims(4);\n             !LoopCondition(offset_3, end_3, params_copy.strides[3]);\n             offset_3 += step_3) {\n          // When the stride is 1, the inner loop is equivalent to the\n          // optimized slice inner loop. Otherwise, it is identical to the\n          // strided_slice reference implementation inner loop.\n          if (inner_stride_is_1) {\n            const int len = stop_4 - start_4;\n            if (len > 0) {\n              writer->WriteN(offset_3 + start_4, len);\n            }\n          } else {\n            for (int offset_4 = offset_3 + start_4, end_4 = offset_3 + stop_4;\n                 !LoopCondition(offset_4, end_4, params_copy.strides[4]);\n                 offset_4 += params_copy.strides[4]) {\n              writer->Write(offset_4);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename T>\ninline void StridedSlice(const tflite::StridedSliceParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const T* input_data,\n                         const RuntimeShape& unextended_output_shape,\n                         T* output_data) {\n  SequentialTensorWriter<T> writer(input_data, output_data);\n  StridedSlice<T>(op_params, unextended_input_shape, unextended_output_shape,\n                  &writer);\n}\n\ntemplate <typename T>\ninline void StridedSlice(const tflite::StridedSliceParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const TfLiteTensor* input,\n                         const RuntimeShape& unextended_output_shape,\n                         TfLiteTensor* output) {\n  SequentialTensorWriter<T> writer(input, output);\n  StridedSlice<T>(op_params, unextended_input_shape, unextended_output_shape,\n                  &writer);\n}\n\ntemplate <typename T>\nvoid Minimum(const RuntimeShape& input1_shape, const T* input1_data,\n             const T* input2_data, const RuntimeShape& output_shape,\n             T* output_data) {\n  ruy::profiler::ScopeLabel label(\"TensorFlowMinimum\");\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  auto min_value = input2_data[0];\n  output_map.array() = input1_map.array().min(min_value);\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T>\ninline void Minimum(const RuntimeShape& input1_shape, const T* input1_data,\n                    const RuntimeShape&, const T* input2_data,\n                    const RuntimeShape& output_shape, T* output_data) {\n  // Drop shape of second input: not needed.\n  Minimum(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename T>\nvoid Maximum(const RuntimeShape& input1_shape, const T* input1_data,\n             const T* input2_data, const RuntimeShape& output_shape,\n             T* output_data) {\n  ruy::profiler::ScopeLabel label(\"TensorFlowMaximum\");\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  auto max_value = input2_data[0];\n  output_map.array() = input1_map.array().max(max_value);\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T>\ninline void Maximum(const RuntimeShape& input1_shape, const T* input1_data,\n                    const RuntimeShape&, const T* input2_data,\n                    const RuntimeShape& output_shape, T* output_data) {\n  // Drop shape of second input: not needed.\n  Maximum(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename T>\nvoid TransposeIm2col(const ConvParams& params, uint8 zero_byte,\n                     const RuntimeShape& input_shape, const T* input_data,\n                     const RuntimeShape& filter_shape,\n                     const RuntimeShape& output_shape, T* im2col_data) {\n  ruy::profiler::ScopeLabel label(\"TransposeIm2col\");\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK(im2col_data);\n\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  MatchingDim(output_shape, 3, filter_shape, 0);  // output_depth\n\n  // Construct the MxN sized im2col matrix.\n  // The rows M, are sub-ordered B x H x W\n  const RuntimeShape row_shape({1, batches, output_height, output_width});\n  // The columns, N, are sub-ordered Kh x Kw x Din\n  const RuntimeShape col_shape({1, filter_height, filter_width, input_depth});\n  // Use dimensions M and N to construct dims for indexing directly into im2col\n  const RuntimeShape im2col_shape(\n      {1, 1, row_shape.FlatSize(), col_shape.FlatSize()});\n\n  // Build the im2col matrix by looping through all the input pixels,\n  // computing their influence on the output, rather than looping through all\n  // the output pixels. We therefore must initialize the im2col array to zero.\n  // This is potentially inefficient because we subsequently overwrite bytes\n  // set here. However, in practice memset is very fast and costs negligible.\n  memset(im2col_data, zero_byte, im2col_shape.FlatSize() * sizeof(T));\n\n  // Loop through the output batches\n  for (int batch = 0; batch < batches; ++batch) {\n    // Loop through input pixels one at a time.\n    for (int in_y = 0; in_y < input_height; ++in_y) {\n      for (int in_x = 0; in_x < input_width; ++in_x) {\n        // Loop through the output pixels it will influence\n        const int out_x_origin = (in_x * stride_width) - pad_width;\n        const int out_y_origin = (in_y * stride_height) - pad_height;\n        for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n          const int out_y = out_y_origin + filter_y;\n          // Is output pixel within height bounds?\n          if ((out_y >= 0) && (out_y < output_height)) {\n            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n              const int out_x = out_x_origin + filter_x;\n              // Is output pixel within width bounds?\n              if ((out_x >= 0) && (out_x < output_width)) {\n                // Copy the input elements of this pixel\n                T const* src =\n                    input_data + Offset(input_shape, batch, in_y, in_x, 0);\n                int row_offset = Offset(row_shape, 0, batch, out_y, out_x);\n                int col_offset = Offset(col_shape, 0, filter_y, filter_x, 0);\n                T* dst = im2col_data +\n                         Offset(im2col_shape, 0, 0, row_offset, col_offset);\n                memcpy(dst, src, input_depth * sizeof(T));\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n// Returns in 'im_data' (assumes to be zero-initialized) image patch in storage\n// order (height, width, depth), constructed from patches in 'col_data', which\n// is required to be in storage order (out_height * out_width, filter_height,\n// filter_width, in_depth).  Implementation by Yangqing Jia (jiayq).\n// Copied from //tensorflow/core/kernels/conv_grad_input_ops.cc\ntemplate <typename T>\nvoid Col2im(const T* col_data, const int depth, const int height,\n            const int width, const int filter_h, const int filter_w,\n            const int pad_t, const int pad_l, const int pad_b, const int pad_r,\n            const int stride_h, const int stride_w, T* im_data) {\n  ruy::profiler::ScopeLabel label(\"Col2im\");\n  int height_col = (height + pad_t + pad_b - filter_h) / stride_h + 1;\n  int width_col = (width + pad_l + pad_r - filter_w) / stride_w + 1;\n  int h_pad = -pad_t;\n  for (int h = 0; h < height_col; ++h) {\n    int w_pad = -pad_l;\n    for (int w = 0; w < width_col; ++w) {\n      T* im_patch_data = im_data + (h_pad * width + w_pad) * depth;\n      for (int ih = h_pad; ih < h_pad + filter_h; ++ih) {\n        for (int iw = w_pad; iw < w_pad + filter_w; ++iw) {\n          if (ih >= 0 && ih < height && iw >= 0 && iw < width) {\n            // TODO(andydavis) Vectorize this loop (if compiler does not).\n            for (int i = 0; i < depth; ++i) {\n              im_patch_data[i] += col_data[i];\n            }\n          }\n          im_patch_data += depth;\n          col_data += depth;\n        }\n        // Jump over remaining number of depth.\n        im_patch_data += depth * (width - filter_w);\n      }\n      w_pad += stride_w;\n    }\n    h_pad += stride_h;\n  }\n}\n\n// TODO(b/188008864) Optimize this function by combining outer loops.\ntemplate <typename T>\nvoid BiasAdd(T* im_data, const T* bias_data, const int batch_size,\n             const int height, const int width, const int depth) {\n  if (bias_data) {\n    for (int n = 0; n < batch_size; ++n) {\n      for (int h = 0; h < height; ++h) {\n        for (int w = 0; w < width; ++w) {\n          for (int d = 0; d < depth; ++d) {\n            im_data[d] += bias_data[d];\n          }\n          im_data += depth;\n        }\n      }\n    }\n  }\n}\n\n// TransposeConvV2 expect the weights in HWOI order.\ninline void TransposeConvV2(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& hwoi_ordered_filter_shape,\n    const float* hwoi_ordered_filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* const output_data, const RuntimeShape& col2im_shape,\n    float* col2im_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"TransposeConvV2/float\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(hwoi_ordered_filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK(col2im_data);\n  TFLITE_DCHECK(hwoi_ordered_filter_data);\n\n  const int batch_size = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_image_size = input_shape.Dims(1) * input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_image_size = output_height * output_width;\n  const int input_depth =\n      MatchingDim(input_shape, 3, hwoi_ordered_filter_shape, 3);\n  const int output_depth =\n      MatchingDim(output_shape, 3, hwoi_ordered_filter_shape, 2);\n  const int input_offset = input_image_size * input_depth;\n  const int output_offset = output_image_size * output_depth;\n\n  const int filter_height = hwoi_ordered_filter_shape.Dims(0);\n  const int filter_width = hwoi_ordered_filter_shape.Dims(1);\n  const int padding_top = params.padding_values.height;\n  const int padding_bottom =\n      params.padding_values.height + params.padding_values.height_offset;\n  const int padding_left = params.padding_values.width;\n  const int padding_right =\n      params.padding_values.width + params.padding_values.width_offset;\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  const int hwoi_ordered_filter_total_size =\n      filter_height * filter_width * output_depth;\n\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = hwoi_ordered_filter_total_size;\n  lhs_params.cols = input_depth;\n  float* output_data_p = output_data;\n  std::fill_n(output_data, output_offset * batch_size, 0.0f);\n  for (int i = 0; i < batch_size; ++i) {\n    cpu_backend_gemm::MatrixParams<float> rhs_params;\n    rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n    rhs_params.rows = input_depth;\n    rhs_params.cols = input_image_size;\n    cpu_backend_gemm::MatrixParams<float> dst_params;\n    dst_params.order = cpu_backend_gemm::Order::kColMajor;\n    dst_params.rows = hwoi_ordered_filter_total_size;\n    dst_params.cols = input_image_size;\n    cpu_backend_gemm::GemmParams<float, float> gemm_params;\n    cpu_backend_gemm::Gemm(lhs_params, hwoi_ordered_filter_data, rhs_params,\n                           input_data + input_offset * i, dst_params,\n                           col2im_data, gemm_params, cpu_backend_context);\n\n    Col2im(col2im_data, output_depth, output_height, output_width,\n           filter_height, filter_width, padding_top, padding_left,\n           padding_bottom, padding_right, stride_height, stride_width,\n           output_data_p);\n    output_data_p += output_offset;\n  }\n  output_data_p = output_data;\n  BiasAdd(output_data_p, bias_data, batch_size, output_height, output_width,\n          output_depth);\n}\n\ninline void Quantize(int32_t multiplier, int32_t shift, int32_t total_size,\n                     int32_t output_zp, int32_t* scratch, uint8_t* output) {\n  ruy::profiler::ScopeLabel label(\"Quantize/uint8\");\n  int i = 0;\n  const int32_t output_min = std::numeric_limits<uint8_t>::min();\n  const int32_t output_max = std::numeric_limits<uint8_t>::max();\n\n#ifdef USE_NEON\n  const int32x4_t output_zp_dup = vdupq_n_s32(output_zp);\n  const int32x4_t max_val_dup = vdupq_n_s32(output_max);\n  const int32x4_t min_val_dup = vdupq_n_s32(output_min);\n\n  using gemmlowp::RoundingDivideByPOT;\n  using gemmlowp::SaturatingRoundingDoublingHighMul;\n\n  for (; i <= total_size - 16; i += 16) {\n    int32x4x4_t scratch_val;\n    scratch_val.val[0] = vld1q_s32(scratch + i);\n    scratch_val.val[1] = vld1q_s32(scratch + i + 4);\n    scratch_val.val[2] = vld1q_s32(scratch + i + 8);\n    scratch_val.val[3] = vld1q_s32(scratch + i + 12);\n\n    int32x4x4_t temp_val =\n        MultiplyByQuantizedMultiplier4Rows(scratch_val, multiplier, shift);\n\n    temp_val.val[0] = vaddq_s32(temp_val.val[0], output_zp_dup);\n    temp_val.val[1] = vaddq_s32(temp_val.val[1], output_zp_dup);\n    temp_val.val[2] = vaddq_s32(temp_val.val[2], output_zp_dup);\n    temp_val.val[3] = vaddq_s32(temp_val.val[3], output_zp_dup);\n\n    temp_val.val[0] =\n        vmaxq_s32(vminq_s32(temp_val.val[0], max_val_dup), min_val_dup);\n    temp_val.val[1] =\n        vmaxq_s32(vminq_s32(temp_val.val[1], max_val_dup), min_val_dup);\n    temp_val.val[2] =\n        vmaxq_s32(vminq_s32(temp_val.val[2], max_val_dup), min_val_dup);\n    temp_val.val[3] =\n        vmaxq_s32(vminq_s32(temp_val.val[3], max_val_dup), min_val_dup);\n\n    const uint16x8_t result_1 =\n        vcombine_u16(vqmovn_u32(vreinterpretq_u32_s32(temp_val.val[0])),\n                     vqmovn_u32(vreinterpretq_u32_s32(temp_val.val[1])));\n    const uint16x8_t result_2 =\n        vcombine_u16(vqmovn_u32(vreinterpretq_u32_s32(temp_val.val[2])),\n                     vqmovn_u32(vreinterpretq_u32_s32(temp_val.val[3])));\n    const uint8x16_t result =\n        vcombine_u8(vqmovn_u16(result_1), vqmovn_u16(result_2));\n    vst1q_u8(output + i, result);\n  }\n#endif\n  for (; i < total_size; ++i) {\n    int32_t temp = MultiplyByQuantizedMultiplier(scratch[i], multiplier, shift);\n    temp += output_zp;\n    if (temp > output_max) {\n      temp = output_max;\n    }\n    if (temp < output_min) {\n      temp = output_min;\n    }\n    output[i] = static_cast<uint8_t>(temp);\n  }\n}\n\ninline void Quantize(const int32_t* multiplier, const int32_t* shift,\n                     int32_t channel_size, int32_t total_size,\n                     int32_t output_zp, int32_t output_min, int32_t output_max,\n                     int32_t* scratch, int8_t* output) {\n  ruy::profiler::ScopeLabel label(\"Quantize/int8\");\n\n  // Here we're trying to quantize the raw accumulators:\n  //        output_channels\n  //       data data data data data\n  // rows  data data data data data\n  //       data data data data data\n  //          ....\n  //\n  // In order to minimize the reload of the multipliers & shifts, once we load\n  // the multipliers & shifts, we load & quantize the raw accumulators for every\n  // row.\n#ifdef USE_NEON\n  const int32x4_t output_offset_vec = vdupq_n_s32(output_zp);\n  const int32x4_t output_activation_min_vec = vdupq_n_s32(output_min);\n  const int32x4_t output_activation_max_vec = vdupq_n_s32(output_max);\n  const int32x4_t zeros = vdupq_n_s32(0);\n#endif\n\n  TFLITE_DCHECK_EQ(total_size % channel_size, 0);\n  const int32_t rows = total_size / channel_size;\n\n  int c = 0;\n\n#ifdef USE_NEON\n  using gemmlowp::RoundingDivideByPOT;\n  for (; c <= channel_size - 8; c += 8) {\n    int32x4_t out_shift_1 = vld1q_s32(shift + c);\n    int32x4_t out_shift_2 = vld1q_s32(shift + c + 4);\n    int32x4_t left_shift_1 = vmaxq_s32(out_shift_1, zeros);\n    int32x4_t left_shift_2 = vmaxq_s32(out_shift_2, zeros);\n\n    // Right shift will be performed as left shift with negative values.\n    int32x4_t right_shift_1 = vminq_s32(out_shift_1, zeros);\n    int32x4_t right_shift_2 = vminq_s32(out_shift_2, zeros);\n\n    int32x4_t out_mul_1 = vld1q_s32(multiplier + c);\n    int32x4_t out_mul_2 = vld1q_s32(multiplier + c + 4);\n    for (int n = 0; n < rows; ++n) {\n      int loc = n * channel_size + c;\n      int32x4_t acc_1 = vld1q_s32(scratch + loc);\n      int32x4_t acc_2 = vld1q_s32(scratch + loc + 4);\n\n      // Saturating Rounding Doubling High Mul.\n      acc_1 = vshlq_s32(acc_1, left_shift_1);\n      acc_1 = vqrdmulhq_s32(acc_1, out_mul_1);\n      acc_2 = vshlq_s32(acc_2, left_shift_2);\n      acc_2 = vqrdmulhq_s32(acc_2, out_mul_2);\n\n      // Rounding Dividing By POT.\n      acc_1 = vrshlq_s32(acc_1, right_shift_1);\n      acc_2 = vrshlq_s32(acc_2, right_shift_2);\n\n      // Add the output offset.\n      acc_1 = vaddq_s32(acc_1, output_offset_vec);\n      acc_2 = vaddq_s32(acc_2, output_offset_vec);\n\n      // Apply the activation function.\n      acc_1 = vmaxq_s32(acc_1, output_activation_min_vec);\n      acc_1 = vminq_s32(acc_1, output_activation_max_vec);\n      acc_2 = vmaxq_s32(acc_2, output_activation_min_vec);\n      acc_2 = vminq_s32(acc_2, output_activation_max_vec);\n\n      // Saturating cast to int8 and store to destination.\n      const int16x4_t acc_s16_1 = vqmovn_s32(acc_1);\n      const int16x4_t acc_s16_2 = vqmovn_s32(acc_2);\n      const int16x8_t res_s16 = vcombine_s16(acc_s16_1, acc_s16_2);\n      const int8x8_t res_s8 = vqmovn_s16(res_s16);\n      vst1_s8(output + loc, res_s8);\n    }\n  }\n\n#endif  // USE_NEON\n  // Handle leftover values, one by one. This is very slow.\n  for (; c < channel_size; c++) {\n    for (int n = 0; n < rows; ++n) {\n      int loc = n * channel_size + c;\n      int32 acc = scratch[loc];\n      acc = MultiplyByQuantizedMultiplier(acc, multiplier[c], shift[c]);\n      acc += output_zp;\n      acc = std::max(acc, output_min);\n      acc = std::min(acc, output_max);\n      output[loc] = static_cast<int8>(acc);\n    }\n  }\n}\n\n// TransposeConvV2 expect the weights in HWOI order.\ninline void TransposeConvV2(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const uint8_t* input_data, const RuntimeShape& hwoi_ordered_filter_shape,\n    const uint8_t* hwoi_ordered_filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8_t* output_data, const RuntimeShape& col2im_shape,\n    int32_t* col2im_data, int32_t* scratch_data,\n    CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"TransposeConvV2/uint8\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(hwoi_ordered_filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK(col2im_data);\n  TFLITE_DCHECK(hwoi_ordered_filter_data);\n\n  const int batch_size = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_image_size = input_shape.Dims(1) * input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_image_size = output_height * output_width;\n  const int input_depth =\n      MatchingDim(input_shape, 3, hwoi_ordered_filter_shape, 3);\n  const int output_depth =\n      MatchingDim(output_shape, 3, hwoi_ordered_filter_shape, 2);\n  const int input_offset = input_image_size * input_depth;\n  const int output_offset = output_image_size * output_depth;\n\n  const int filter_height = hwoi_ordered_filter_shape.Dims(0);\n  const int filter_width = hwoi_ordered_filter_shape.Dims(1);\n  const int padding_top = params.padding_values.height;\n  const int padding_bottom =\n      params.padding_values.height + params.padding_values.height_offset;\n  const int padding_left = params.padding_values.width;\n  const int padding_right =\n      params.padding_values.width + params.padding_values.width_offset;\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  const int hwoi_ordered_filter_total_size =\n      filter_height * filter_width * output_depth;\n\n  cpu_backend_gemm::MatrixParams<uint8_t> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = hwoi_ordered_filter_total_size;\n  lhs_params.cols = input_depth;\n  lhs_params.zero_point = -params.weights_offset;\n\n  int32_t* scratch_data_p = scratch_data;\n  std::fill_n(scratch_data, output_offset * batch_size, static_cast<int32>(0));\n  for (int i = 0; i < batch_size; ++i) {\n    cpu_backend_gemm::MatrixParams<uint8_t> rhs_params;\n    rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n    rhs_params.rows = input_depth;\n    rhs_params.cols = input_image_size;\n    rhs_params.zero_point = -params.input_offset;\n\n    cpu_backend_gemm::MatrixParams<int32_t> dst_params;\n    dst_params.order = cpu_backend_gemm::Order::kColMajor;\n    dst_params.rows = hwoi_ordered_filter_total_size;\n    dst_params.cols = input_image_size;\n\n    cpu_backend_gemm::GemmParams<int32_t, int32_t> gemm_params;\n    cpu_backend_gemm::Gemm(lhs_params, hwoi_ordered_filter_data, rhs_params,\n                           input_data + input_offset * i, dst_params,\n                           col2im_data, gemm_params, cpu_backend_context);\n\n    Col2im(col2im_data, output_depth, output_height, output_width,\n           filter_height, filter_width, padding_top, padding_left,\n           padding_bottom, padding_right, stride_height, stride_width,\n           scratch_data_p);\n\n    scratch_data_p += output_offset;\n  }\n  scratch_data_p = scratch_data;\n  BiasAdd(scratch_data_p, bias_data, batch_size, output_height, output_width,\n          output_depth);\n\n  Quantize(params.output_multiplier, params.output_shift,\n           output_shape.FlatSize(), params.output_offset, scratch_data,\n           output_data);\n}\n\n// Integer-only version of ResizeNearestNeighbor. Since scales are represented\n// in fixed-point and thus approximated, |in_x| or |in_y| may differ from the\n// reference version. Debug checks are in place to test if this occurs.\n// NOTE: If align_corners or half_pixel_centers is true, we use the reference\n// version.\ninline void ResizeNearestNeighbor(\n    const tflite::ResizeNearestNeighborParams& op_params,\n    const RuntimeShape& unextended_input_shape, const uint8* input_data,\n    const RuntimeShape& output_size_shape, const int32* output_size_data,\n    const RuntimeShape& unextended_output_shape, uint8* output_data) {\n  if (op_params.align_corners || op_params.half_pixel_centers) {\n    // TODO(b/149823713): Add support for align_corners & half_pixel_centers in\n    // this kernel.\n    reference_ops::ResizeNearestNeighbor(\n        op_params, unextended_input_shape, input_data, output_size_shape,\n        output_size_data, unextended_output_shape, output_data);\n    return;\n  }\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  int32 batches = MatchingDim(input_shape, 0, output_shape, 0);\n  int32 input_height = input_shape.Dims(1);\n  int32 input_width = input_shape.Dims(2);\n  int32 depth = MatchingDim(input_shape, 3, output_shape, 3);\n\n  // The Tensorflow version of this op allows resize on the width and height\n  // axis only.\n  TFLITE_DCHECK_EQ(output_size_shape.FlatSize(), 2);\n  int32 output_height = output_size_data[0];\n  int32 output_width = output_size_data[1];\n\n  // Convert scales to fixed-point with 16 fractional bits. We add 1 as an\n  // error factor and to avoid zero scales. For example, with input_height = 1,\n  // output_height = 3, the float scaling factor would be non-zero at 1/3.\n  // With fixed-point, this is zero.\n  int32 height_scale = (input_height << 16) / output_height + 1;\n  int32 width_scale = (input_width << 16) / output_width + 1;\n\n  const int col_offset = input_shape.Dims(3);\n  const int row_offset = input_shape.Dims(2) * col_offset;\n  const int batch_offset = input_shape.Dims(1) * row_offset;\n\n  const uint8* input_ptr = input_data;\n  uint8* output_ptr = output_data;\n  for (int b = 0; b < batches; ++b) {\n    for (int y = 0; y < output_height; ++y) {\n      int32 in_y = std::min((y * height_scale) >> 16, input_height - 1);\n      // Check offset calculation is the same as the reference version. See\n      // function comment for details. We check using a non-float version of:\n      // TFLITE_DCHECK_EQ(in_y, std::floor(y * (static_cast<float>(input_height)\n      //                                            / output_height)));\n      TFLITE_DCHECK_LT(y * input_height, output_height + in_y * output_height);\n      TFLITE_DCHECK_GE(y * input_height, in_y * output_height);\n      const uint8* y_input_ptr = input_ptr + in_y * row_offset;\n      for (int x = 0; x < output_width; ++x) {\n        int32 in_x = std::min((x * width_scale) >> 16, input_width - 1);\n        // Check offset calculation is the same as the reference version. See\n        // function comment for details. We check using a non-float version of:\n        // TFLITE_DCHECK_EQ(in_y,\n        //                  std::floor(y * (static_cast<float>(input_width)\n        //                                      / output_width)));\n        TFLITE_DCHECK_LT(x * input_width, output_width + in_x * output_width);\n        TFLITE_DCHECK_GE(x * input_width, in_x * output_width);\n        const uint8* x_input_ptr = y_input_ptr + in_x * col_offset;\n        memcpy(output_ptr, x_input_ptr, depth);\n        output_ptr += depth;\n      }\n    }\n    input_ptr += batch_offset;\n  }\n}\n\ntemplate <typename input_type, typename output_type>\ninline void Requantize(const input_type* input_data, int32_t size,\n                       int32_t effective_scale_multiplier,\n                       int32_t effective_scale_shift, int32_t input_zeropoint,\n                       int32_t output_zeropoint, output_type* output_data) {\n  reference_ops::Requantize(input_data, size, effective_scale_multiplier,\n                            effective_scale_shift, input_zeropoint,\n                            output_zeropoint, output_data);\n}\n\ntemplate <>\ninline void Requantize<int8_t, uint8_t>(const int8_t* input_data, int32_t size,\n                                        int32_t effective_scale_multiplier,\n                                        int32_t effective_scale_shift,\n                                        int32_t input_zeropoint,\n                                        int32_t output_zeropoint,\n                                        uint8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Requantize/Int8ToUint8\");\n\n  static constexpr int32_t kMinOutput = std::numeric_limits<uint8_t>::min();\n  static constexpr int32_t kMaxOutput = std::numeric_limits<uint8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  // Constants.\n  const int32x4_t input_zero_point_dup = vdupq_n_s32(-input_zeropoint);\n  const int32x4_t output_zero_point_dup = vdupq_n_s32(output_zeropoint);\n  const int32x4_t min_val_dup = vdupq_n_s32(kMinOutput);\n  const int32x4_t max_val_dup = vdupq_n_s32(kMaxOutput);\n\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input_vec = vld1q_s8(input_data + i);\n    const int16x8_t first_half = vmovl_s8(vget_low_s8(input_vec));\n    const int16x8_t second_half = vmovl_s8(vget_high_s8(input_vec));\n    int32x4x4_t input;\n    input.val[0] = vmovl_s16(vget_low_s16(first_half));\n    input.val[1] = vmovl_s16(vget_high_s16(first_half));\n    input.val[2] = vmovl_s16(vget_low_s16(second_half));\n    input.val[3] = vmovl_s16(vget_high_s16(second_half));\n    input.val[0] = vaddq_s32(input.val[0], input_zero_point_dup);\n    input.val[1] = vaddq_s32(input.val[1], input_zero_point_dup);\n    input.val[2] = vaddq_s32(input.val[2], input_zero_point_dup);\n    input.val[3] = vaddq_s32(input.val[3], input_zero_point_dup);\n\n    int32x4x4_t result = MultiplyByQuantizedMultiplier4Rows(\n        input, effective_scale_multiplier, effective_scale_shift);\n\n    result.val[0] = vaddq_s32(result.val[0], output_zero_point_dup);\n    result.val[1] = vaddq_s32(result.val[1], output_zero_point_dup);\n    result.val[2] = vaddq_s32(result.val[2], output_zero_point_dup);\n    result.val[3] = vaddq_s32(result.val[3], output_zero_point_dup);\n    result.val[0] =\n        vmaxq_s32(vminq_s32(result.val[0], max_val_dup), min_val_dup);\n    result.val[1] =\n        vmaxq_s32(vminq_s32(result.val[1], max_val_dup), min_val_dup);\n    result.val[2] =\n        vmaxq_s32(vminq_s32(result.val[2], max_val_dup), min_val_dup);\n    result.val[3] =\n        vmaxq_s32(vminq_s32(result.val[3], max_val_dup), min_val_dup);\n\n    const uint32x4_t result_val_1_unsigned =\n        vreinterpretq_u32_s32(result.val[0]);\n    const uint32x4_t result_val_2_unsigned =\n        vreinterpretq_u32_s32(result.val[1]);\n    const uint32x4_t result_val_3_unsigned =\n        vreinterpretq_u32_s32(result.val[2]);\n    const uint32x4_t result_val_4_unsigned =\n        vreinterpretq_u32_s32(result.val[3]);\n\n    const uint16x4_t narrowed_val_1 = vqmovn_u32(result_val_1_unsigned);\n    const uint16x4_t narrowed_val_2 = vqmovn_u32(result_val_2_unsigned);\n    const uint16x4_t narrowed_val_3 = vqmovn_u32(result_val_3_unsigned);\n    const uint16x4_t narrowed_val_4 = vqmovn_u32(result_val_4_unsigned);\n    const uint16x8_t output_first_half =\n        vcombine_u16(narrowed_val_1, narrowed_val_2);\n    const uint16x8_t output_second_half =\n        vcombine_u16(narrowed_val_3, narrowed_val_4);\n    const uint8x8_t narrowed_first_half = vqmovn_u16(output_first_half);\n    const uint8x8_t narrowed_second_half = vqmovn_u16(output_second_half);\n    const uint8x16_t narrowed_result =\n        vcombine_u8(narrowed_first_half, narrowed_second_half);\n    vst1q_u8(output_data + i, narrowed_result);\n  }\n\n#endif\n  for (; i < size; ++i) {\n    const int32_t input = input_data[i] - input_zeropoint;\n    const int32_t output =\n        MultiplyByQuantizedMultiplier(input, effective_scale_multiplier,\n                                      effective_scale_shift) +\n        output_zeropoint;\n    const int32_t clamped_output =\n        std::max(std::min(output, kMaxOutput), kMinOutput);\n    output_data[i] = static_cast<uint8_t>(clamped_output);\n  }\n}\n\ntemplate <>\ninline void Requantize<uint8_t, int8_t>(const uint8_t* input_data, int32_t size,\n                                        int32_t effective_scale_multiplier,\n                                        int32_t effective_scale_shift,\n                                        int32_t input_zeropoint,\n                                        int32_t output_zeropoint,\n                                        int8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Requantize/Uint8ToInt8\");\n\n  static constexpr int32_t kMinOutput = std::numeric_limits<int8_t>::min();\n  static constexpr int32_t kMaxOutput = std::numeric_limits<int8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  // Constants.\n  const int32x4_t input_zero_point_dup = vdupq_n_s32(-input_zeropoint);\n  const int32x4_t output_zero_point_dup = vdupq_n_s32(output_zeropoint);\n  const int32x4_t min_val_dup = vdupq_n_s32(kMinOutput);\n  const int32x4_t max_val_dup = vdupq_n_s32(kMaxOutput);\n\n  for (; i <= size - 16; i += 16) {\n    const uint8x16_t input_vec = vld1q_u8(input_data + i);\n    const uint16x8_t first_half = vmovl_u8(vget_low_u8(input_vec));\n    const uint16x8_t second_half = vmovl_u8(vget_high_u8(input_vec));\n    int32x4x4_t input;\n    input.val[0] = vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(first_half)));\n    input.val[1] = vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(first_half)));\n    input.val[2] = vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(second_half)));\n    input.val[3] = vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(second_half)));\n    input.val[0] = vaddq_s32(input.val[0], input_zero_point_dup);\n    input.val[1] = vaddq_s32(input.val[1], input_zero_point_dup);\n    input.val[2] = vaddq_s32(input.val[2], input_zero_point_dup);\n    input.val[3] = vaddq_s32(input.val[3], input_zero_point_dup);\n\n    int32x4x4_t result = MultiplyByQuantizedMultiplier4Rows(\n        input, effective_scale_multiplier, effective_scale_shift);\n\n    result.val[0] = vaddq_s32(result.val[0], output_zero_point_dup);\n    result.val[1] = vaddq_s32(result.val[1], output_zero_point_dup);\n    result.val[2] = vaddq_s32(result.val[2], output_zero_point_dup);\n    result.val[3] = vaddq_s32(result.val[3], output_zero_point_dup);\n    result.val[0] =\n        vmaxq_s32(vminq_s32(result.val[0], max_val_dup), min_val_dup);\n    result.val[1] =\n        vmaxq_s32(vminq_s32(result.val[1], max_val_dup), min_val_dup);\n    result.val[2] =\n        vmaxq_s32(vminq_s32(result.val[2], max_val_dup), min_val_dup);\n    result.val[3] =\n        vmaxq_s32(vminq_s32(result.val[3], max_val_dup), min_val_dup);\n\n    const int16x4_t narrowed_val_1 = vqmovn_s32(result.val[0]);\n    const int16x4_t narrowed_val_2 = vqmovn_s32(result.val[1]);\n    const int16x4_t narrowed_val_3 = vqmovn_s32(result.val[2]);\n    const int16x4_t narrowed_val_4 = vqmovn_s32(result.val[3]);\n    const int16x8_t output_first_half =\n        vcombine_s16(narrowed_val_1, narrowed_val_2);\n    const int16x8_t output_second_half =\n        vcombine_s16(narrowed_val_3, narrowed_val_4);\n    const int8x8_t narrowed_first_half = vqmovn_s16(output_first_half);\n    const int8x8_t narrowed_second_half = vqmovn_s16(output_second_half);\n    const int8x16_t narrowed_result =\n        vcombine_s8(narrowed_first_half, narrowed_second_half);\n    vst1q_s8(output_data + i, narrowed_result);\n  }\n\n#endif\n  for (; i < size; ++i) {\n    const int32_t input = input_data[i] - input_zeropoint;\n    const int32_t output =\n        MultiplyByQuantizedMultiplier(input, effective_scale_multiplier,\n                                      effective_scale_shift) +\n        output_zeropoint;\n    const int32_t clamped_output =\n        std::max(std::min(output, kMaxOutput), kMinOutput);\n    output_data[i] = static_cast<int8_t>(clamped_output);\n  }\n}\n\ntemplate <>\ninline void Requantize<int8_t, int8_t>(const int8_t* input_data, int32_t size,\n                                       int32_t effective_scale_multiplier,\n                                       int32_t effective_scale_shift,\n                                       int32_t input_zeropoint,\n                                       int32_t output_zeropoint,\n                                       int8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Requantize/Int8ToInt8\");\n\n  static constexpr int32_t kMinOutput = std::numeric_limits<int8_t>::min();\n  static constexpr int32_t kMaxOutput = std::numeric_limits<int8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  // Constants.\n  const int32x4_t input_zero_point_dup = vdupq_n_s32(-input_zeropoint);\n  const int32x4_t output_zero_point_dup = vdupq_n_s32(output_zeropoint);\n  const int32x4_t min_val_dup = vdupq_n_s32(kMinOutput);\n  const int32x4_t max_val_dup = vdupq_n_s32(kMaxOutput);\n\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input_vec = vld1q_s8(input_data + i);\n    const int16x8_t first_half = vmovl_s8(vget_low_s8(input_vec));\n    const int16x8_t second_half = vmovl_s8(vget_high_s8(input_vec));\n    int32x4x4_t input;\n    input.val[0] = vmovl_s16(vget_low_s16(first_half));\n    input.val[1] = vmovl_s16(vget_high_s16(first_half));\n    input.val[2] = vmovl_s16(vget_low_s16(second_half));\n    input.val[3] = vmovl_s16(vget_high_s16(second_half));\n\n    input.val[0] = vaddq_s32(input.val[0], input_zero_point_dup);\n    input.val[1] = vaddq_s32(input.val[1], input_zero_point_dup);\n    input.val[2] = vaddq_s32(input.val[2], input_zero_point_dup);\n    input.val[3] = vaddq_s32(input.val[3], input_zero_point_dup);\n\n    int32x4x4_t result = MultiplyByQuantizedMultiplier4Rows(\n        input, effective_scale_multiplier, effective_scale_shift);\n\n    result.val[0] = vaddq_s32(result.val[0], output_zero_point_dup);\n    result.val[1] = vaddq_s32(result.val[1], output_zero_point_dup);\n    result.val[2] = vaddq_s32(result.val[2], output_zero_point_dup);\n    result.val[3] = vaddq_s32(result.val[3], output_zero_point_dup);\n    result.val[0] =\n        vmaxq_s32(vminq_s32(result.val[0], max_val_dup), min_val_dup);\n    result.val[1] =\n        vmaxq_s32(vminq_s32(result.val[1], max_val_dup), min_val_dup);\n    result.val[2] =\n        vmaxq_s32(vminq_s32(result.val[2], max_val_dup), min_val_dup);\n    result.val[3] =\n        vmaxq_s32(vminq_s32(result.val[3], max_val_dup), min_val_dup);\n\n    const int16x4_t narrowed_val_1 = vqmovn_s32(result.val[0]);\n    const int16x4_t narrowed_val_2 = vqmovn_s32(result.val[1]);\n    const int16x4_t narrowed_val_3 = vqmovn_s32(result.val[2]);\n    const int16x4_t narrowed_val_4 = vqmovn_s32(result.val[3]);\n    const int16x8_t output_first_half =\n        vcombine_s16(narrowed_val_1, narrowed_val_2);\n    const int16x8_t output_second_half =\n        vcombine_s16(narrowed_val_3, narrowed_val_4);\n    const int8x8_t narrowed_first_half = vqmovn_s16(output_first_half);\n    const int8x8_t narrowed_second_half = vqmovn_s16(output_second_half);\n    const int8x16_t narrowed_result =\n        vcombine_s8(narrowed_first_half, narrowed_second_half);\n    vst1q_s8(output_data + i, narrowed_result);\n  }\n\n#endif\n  for (; i < size; ++i) {\n    const int32_t input = input_data[i] - input_zeropoint;\n    const int32_t output =\n        MultiplyByQuantizedMultiplier(input, effective_scale_multiplier,\n                                      effective_scale_shift) +\n        output_zeropoint;\n    const int32_t clamped_output =\n        std::max(std::min(output, kMaxOutput), kMinOutput);\n    output_data[i] = static_cast<int8_t>(clamped_output);\n  }\n}\n\ntemplate <>\ninline void Requantize<uint8_t, uint8_t>(\n    const uint8_t* input_data, int32_t size, int32_t effective_scale_multiplier,\n    int32_t effective_scale_shift, int32_t input_zeropoint,\n    int32_t output_zeropoint, uint8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Requantize/Uint8ToUint8\");\n\n  static constexpr int32_t kMinOutput = std::numeric_limits<uint8_t>::min();\n  static constexpr int32_t kMaxOutput = std::numeric_limits<uint8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  // Constants.\n  const int32x4_t input_zero_point_dup = vdupq_n_s32(-input_zeropoint);\n  const int32x4_t output_zero_point_dup = vdupq_n_s32(output_zeropoint);\n  const int32x4_t min_val_dup = vdupq_n_s32(kMinOutput);\n  const int32x4_t max_val_dup = vdupq_n_s32(kMaxOutput);\n\n  for (; i <= size - 16; i += 16) {\n    const uint8x16_t input_vec = vld1q_u8(input_data + i);\n    const uint16x8_t first_half = vmovl_u8(vget_low_u8(input_vec));\n    const uint16x8_t second_half = vmovl_u8(vget_high_u8(input_vec));\n    int32x4x4_t input;\n    input.val[0] = vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(first_half)));\n    input.val[1] = vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(first_half)));\n    input.val[2] = vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(second_half)));\n    input.val[3] = vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(second_half)));\n    input.val[0] = vaddq_s32(input.val[0], input_zero_point_dup);\n    input.val[1] = vaddq_s32(input.val[1], input_zero_point_dup);\n    input.val[2] = vaddq_s32(input.val[2], input_zero_point_dup);\n    input.val[3] = vaddq_s32(input.val[3], input_zero_point_dup);\n\n    int32x4x4_t result = MultiplyByQuantizedMultiplier4Rows(\n        input, effective_scale_multiplier, effective_scale_shift);\n\n    result.val[0] = vaddq_s32(result.val[0], output_zero_point_dup);\n    result.val[1] = vaddq_s32(result.val[1], output_zero_point_dup);\n    result.val[2] = vaddq_s32(result.val[2], output_zero_point_dup);\n    result.val[3] = vaddq_s32(result.val[3], output_zero_point_dup);\n    result.val[0] =\n        vmaxq_s32(vminq_s32(result.val[0], max_val_dup), min_val_dup);\n    result.val[1] =\n        vmaxq_s32(vminq_s32(result.val[1], max_val_dup), min_val_dup);\n    result.val[2] =\n        vmaxq_s32(vminq_s32(result.val[2], max_val_dup), min_val_dup);\n    result.val[3] =\n        vmaxq_s32(vminq_s32(result.val[3], max_val_dup), min_val_dup);\n\n    const uint32x4_t result_val_1_unsigned =\n        vreinterpretq_u32_s32(result.val[0]);\n    const uint32x4_t result_val_2_unsigned =\n        vreinterpretq_u32_s32(result.val[1]);\n    const uint32x4_t result_val_3_unsigned =\n        vreinterpretq_u32_s32(result.val[2]);\n    const uint32x4_t result_val_4_unsigned =\n        vreinterpretq_u32_s32(result.val[3]);\n\n    const uint16x4_t narrowed_val_1 = vqmovn_u32(result_val_1_unsigned);\n    const uint16x4_t narrowed_val_2 = vqmovn_u32(result_val_2_unsigned);\n    const uint16x4_t narrowed_val_3 = vqmovn_u32(result_val_3_unsigned);\n    const uint16x4_t narrowed_val_4 = vqmovn_u32(result_val_4_unsigned);\n    const uint16x8_t output_first_half =\n        vcombine_u16(narrowed_val_1, narrowed_val_2);\n    const uint16x8_t output_second_half =\n        vcombine_u16(narrowed_val_3, narrowed_val_4);\n    const uint8x8_t narrowed_first_half = vqmovn_u16(output_first_half);\n    const uint8x8_t narrowed_second_half = vqmovn_u16(output_second_half);\n    const uint8x16_t narrowed_result =\n        vcombine_u8(narrowed_first_half, narrowed_second_half);\n    vst1q_u8(output_data + i, narrowed_result);\n  }\n\n#endif\n  for (; i < size; ++i) {\n    const int32_t input = input_data[i] - input_zeropoint;\n    const int32_t output =\n        MultiplyByQuantizedMultiplier(input, effective_scale_multiplier,\n                                      effective_scale_shift) +\n        output_zeropoint;\n    const int32_t clamped_output =\n        std::max(std::min(output, kMaxOutput), kMinOutput);\n    output_data[i] = static_cast<uint8_t>(clamped_output);\n  }\n}\n\ninline void HardSwish(const RuntimeShape& input_shape, const float* input_data,\n                      const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"HardSwish/Float\");\n  auto size = MatchingFlatSize(input_shape, output_shape);\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t zero = vdupq_n_f32(0.0f);\n  const float32x4_t three = vdupq_n_f32(3.0f);\n  const float32x4_t six = vdupq_n_f32(6.0f);\n  const float32x4_t one_sixth = vdupq_n_f32(1.0f / 6.0f);\n\n  for (; i <= size - 16; i += 16) {\n    // 4x partially unrolled version of the loop below. Refer to its comments.\n    const float32x4_t in_0 = vld1q_f32(input_data + i + 0);\n    const float32x4_t in_1 = vld1q_f32(input_data + i + 4);\n    const float32x4_t in_2 = vld1q_f32(input_data + i + 8);\n    const float32x4_t in_3 = vld1q_f32(input_data + i + 12);\n    const float32x4_t in_scaled_0 = vmulq_f32(in_0, one_sixth);\n    const float32x4_t in_scaled_1 = vmulq_f32(in_1, one_sixth);\n    const float32x4_t in_scaled_2 = vmulq_f32(in_2, one_sixth);\n    const float32x4_t in_scaled_3 = vmulq_f32(in_3, one_sixth);\n    const float32x4_t in_reluish_0 =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in_0, three)));\n    const float32x4_t in_reluish_1 =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in_1, three)));\n    const float32x4_t in_reluish_2 =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in_2, three)));\n    const float32x4_t in_reluish_3 =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in_3, three)));\n    const float32x4_t product_0 = vmulq_f32(in_scaled_0, in_reluish_0);\n    const float32x4_t product_1 = vmulq_f32(in_scaled_1, in_reluish_1);\n    const float32x4_t product_2 = vmulq_f32(in_scaled_2, in_reluish_2);\n    const float32x4_t product_3 = vmulq_f32(in_scaled_3, in_reluish_3);\n    vst1q_f32(output_data + i + 0, product_0);\n    vst1q_f32(output_data + i + 4, product_1);\n    vst1q_f32(output_data + i + 8, product_2);\n    vst1q_f32(output_data + i + 12, product_3);\n  }\n  for (; i <= size - 4; i += 4) {\n    // The expression to be computed is:\n    //   out = one_sixth * in * min(six, max(zero, (in + three)))\n    // We structure the AST to have two roughly balanced, independent branches:\n    //  - Multiplication: in_scaled = one_sixth * in.\n    //  - Addition and clamping: in_reluish = min(six, max(zero, (in + three))).\n    // Then the remaining multiplication at the root of the tree.\n    const float32x4_t in = vld1q_f32(input_data + i);\n    const float32x4_t in_scaled = vmulq_f32(in, one_sixth);\n    const float32x4_t in_reluish =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in, three)));\n    const float32x4_t product = vmulq_f32(in_scaled, in_reluish);\n    vst1q_f32(output_data + i, product);\n  }\n#endif\n  for (; i < size; i++) {\n    const float in = input_data[i];\n    output_data[i] =\n        in * std::min(6.0f, std::max(0.0f, in + 3.0f)) * (1.0f / 6.0f);\n  }\n}\n\n#ifdef USE_NEON\ninline void SaturateAndStore(int16x8_t src, std::uint8_t* dst) {\n  // Narrow values down to 8 bit unsigned, saturating.\n  uint8x8_t res8 = vqmovun_s16(src);\n  // Store results to destination.\n  vst1_u8(dst, res8);\n}\n\ninline void SaturateAndStore(int16x8_t src, std::int8_t* dst) {\n  // Narrow values down to 8 bit unsigned, saturating.\n  int8x8_t res8 = vqmovn_s16(src);\n  // Store results to destination.\n  vst1_s8(dst, res8);\n}\n#endif\n\ntemplate <typename T>\ninline void HardSwish(const HardSwishParams& params,\n                      const RuntimeShape& input_shape, const T* input_data,\n                      const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"HardSwish/Quantized\");\n\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int i = 0;\n  // This code heavily uses NEON saturating left shifts (vqshl*) with shift\n  // amounts that can be zero, in which case we rely on the correct behavior\n  // of a left shift by zero returning just its first operand unmodified.\n  // Unfortunately, the Intel arm_neon_sse.h implementation of vqshl* is\n  // buggy in the case of zero shift amounts, see b/137199585. That is why\n  // this NEON code path is restricted to true ARM NEON, excluding\n  // arm_neon_sse.h. Anyway, the arm_neon_sse.h implementation of saturating\n  // left shifts is slow scalar code, so there may not be much benefit in\n  // running that over just plain reference code.\n  //\n  // TODO(b/137199585): revisit when this is fixed.\n#ifdef __ARM_NEON\n  const int16x8_t positive_reluish_multiplier_exponent_minus_one =\n      vdupq_n_s16(std::max(0, params.reluish_multiplier_exponent - 1));\n  const int16x8_t positive_reluish_multiplier_exponent_last_bit =\n      vdupq_n_s16(params.reluish_multiplier_exponent > 0 ? 1 : 0);\n  const int16x8_t negative_reluish_multiplier_exponent =\n      vdupq_n_s16(std::min(0, params.reluish_multiplier_exponent));\n  const int16x8_t constant_32767 = vdupq_n_s16(32767);\n  const int16x8_t output_multiplier_exponent =\n      vdupq_n_s16(params.output_multiplier_exponent);\n  const int16x8_t output_zero_point = vdupq_n_s16(params.output_zero_point);\n  // 4x unrolled version of the below NEON loop. Read that first.\n  for (; i <= flat_size - 32; i += 32) {\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_value_0_1 =\n        Load16AndSubtractZeroPoint(input_data + i, params.input_zero_point);\n    const int16x8x2_t input_value_2_3 = Load16AndSubtractZeroPoint(\n        input_data + i + 16, params.input_zero_point);\n    const int16x8_t input_value_on_hires_input_scale_0 =\n        vshlq_n_s16(input_value_0_1.val[0], 7);\n    const int16x8_t input_value_on_hires_input_scale_1 =\n        vshlq_n_s16(input_value_0_1.val[1], 7);\n    const int16x8_t input_value_on_hires_input_scale_2 =\n        vshlq_n_s16(input_value_2_3.val[0], 7);\n    const int16x8_t input_value_on_hires_input_scale_3 =\n        vshlq_n_s16(input_value_2_3.val[1], 7);\n    const int16x8_t input_value_on_preshift_output_scale_0 =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale_0,\n                        params.output_multiplier_fixedpoint_int16);\n    const int16x8_t input_value_on_preshift_output_scale_1 =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale_1,\n                        params.output_multiplier_fixedpoint_int16);\n    const int16x8_t input_value_on_preshift_output_scale_2 =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale_2,\n                        params.output_multiplier_fixedpoint_int16);\n    const int16x8_t input_value_on_preshift_output_scale_3 =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale_3,\n                        params.output_multiplier_fixedpoint_int16);\n    int16x8_t reluish_value_0 = input_value_on_hires_input_scale_0;\n    int16x8_t reluish_value_1 = input_value_on_hires_input_scale_1;\n    int16x8_t reluish_value_2 = input_value_on_hires_input_scale_2;\n    int16x8_t reluish_value_3 = input_value_on_hires_input_scale_3;\n    reluish_value_0 = vqshlq_s16(\n        reluish_value_0, positive_reluish_multiplier_exponent_minus_one);\n    reluish_value_1 = vqshlq_s16(\n        reluish_value_1, positive_reluish_multiplier_exponent_minus_one);\n    reluish_value_2 = vqshlq_s16(\n        reluish_value_2, positive_reluish_multiplier_exponent_minus_one);\n    reluish_value_3 = vqshlq_s16(\n        reluish_value_3, positive_reluish_multiplier_exponent_minus_one);\n    reluish_value_0 = vqrdmulhq_n_s16(\n        reluish_value_0, params.reluish_multiplier_fixedpoint_int16);\n    reluish_value_1 = vqrdmulhq_n_s16(\n        reluish_value_1, params.reluish_multiplier_fixedpoint_int16);\n    reluish_value_2 = vqrdmulhq_n_s16(\n        reluish_value_2, params.reluish_multiplier_fixedpoint_int16);\n    reluish_value_3 = vqrdmulhq_n_s16(\n        reluish_value_3, params.reluish_multiplier_fixedpoint_int16);\n    reluish_value_0 = vqshlq_s16(reluish_value_0,\n                                 positive_reluish_multiplier_exponent_last_bit);\n    reluish_value_1 = vqshlq_s16(reluish_value_1,\n                                 positive_reluish_multiplier_exponent_last_bit);\n    reluish_value_2 = vqshlq_s16(reluish_value_2,\n                                 positive_reluish_multiplier_exponent_last_bit);\n    reluish_value_3 = vqshlq_s16(reluish_value_3,\n                                 positive_reluish_multiplier_exponent_last_bit);\n    reluish_value_0 =\n        vrshlq_s16(reluish_value_0, negative_reluish_multiplier_exponent);\n    reluish_value_1 =\n        vrshlq_s16(reluish_value_1, negative_reluish_multiplier_exponent);\n    reluish_value_2 =\n        vrshlq_s16(reluish_value_2, negative_reluish_multiplier_exponent);\n    reluish_value_3 =\n        vrshlq_s16(reluish_value_3, negative_reluish_multiplier_exponent);\n    reluish_value_0 = vrhaddq_s16(reluish_value_0, constant_32767);\n    reluish_value_1 = vrhaddq_s16(reluish_value_1, constant_32767);\n    reluish_value_2 = vrhaddq_s16(reluish_value_2, constant_32767);\n    reluish_value_3 = vrhaddq_s16(reluish_value_3, constant_32767);\n    const int16x8_t preshift_output_value_0 =\n        vqdmulhq_s16(reluish_value_0, input_value_on_preshift_output_scale_0);\n    const int16x8_t preshift_output_value_1 =\n        vqdmulhq_s16(reluish_value_1, input_value_on_preshift_output_scale_1);\n    const int16x8_t preshift_output_value_2 =\n        vqdmulhq_s16(reluish_value_2, input_value_on_preshift_output_scale_2);\n    const int16x8_t preshift_output_value_3 =\n        vqdmulhq_s16(reluish_value_3, input_value_on_preshift_output_scale_3);\n    int16x8_t output_value_0 =\n        vrshlq_s16(preshift_output_value_0, output_multiplier_exponent);\n    int16x8_t output_value_1 =\n        vrshlq_s16(preshift_output_value_1, output_multiplier_exponent);\n    int16x8_t output_value_2 =\n        vrshlq_s16(preshift_output_value_2, output_multiplier_exponent);\n    int16x8_t output_value_3 =\n        vrshlq_s16(preshift_output_value_3, output_multiplier_exponent);\n    output_value_0 = vaddq_s16(output_value_0, output_zero_point);\n    output_value_1 = vaddq_s16(output_value_1, output_zero_point);\n    output_value_2 = vaddq_s16(output_value_2, output_zero_point);\n    output_value_3 = vaddq_s16(output_value_3, output_zero_point);\n    SaturateAndStore(output_value_0, output_data + i);\n    SaturateAndStore(output_value_1, output_data + i + 8);\n    SaturateAndStore(output_value_2, output_data + i + 16);\n    SaturateAndStore(output_value_3, output_data + i + 24);\n  }\n  // NEON version of reference_ops::HardSwish. Read that first.\n  for (; i <= flat_size - 8; i += 8) {\n    using cpu_backend_gemm::detail::Load8AndSubtractZeroPoint;\n    const int16x8_t input_value =\n        Load8AndSubtractZeroPoint(input_data + i, params.input_zero_point);\n    const int16x8_t input_value_on_hires_input_scale =\n        vshlq_n_s16(input_value, 7);\n    const int16x8_t input_value_on_preshift_output_scale =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale,\n                        params.output_multiplier_fixedpoint_int16);\n    int16x8_t reluish_value = input_value_on_hires_input_scale;\n    reluish_value = vqshlq_s16(reluish_value,\n                               positive_reluish_multiplier_exponent_minus_one);\n    reluish_value = vqrdmulhq_n_s16(reluish_value,\n                                    params.reluish_multiplier_fixedpoint_int16);\n    reluish_value = vqshlq_s16(reluish_value,\n                               positive_reluish_multiplier_exponent_last_bit);\n    reluish_value =\n        vrshlq_s16(reluish_value, negative_reluish_multiplier_exponent);\n    reluish_value = vrhaddq_s16(reluish_value, constant_32767);\n    const int16x8_t preshift_output_value =\n        vqdmulhq_s16(reluish_value, input_value_on_preshift_output_scale);\n    int16x8_t output_value =\n        vrshlq_s16(preshift_output_value, output_multiplier_exponent);\n    output_value = vaddq_s16(output_value, output_zero_point);\n    SaturateAndStore(output_value, output_data + i);\n  }\n#endif\n  // TODO(b/137208495): revisit when unit tests cover reference code.\n  // Fall back to reference_ops::HardSwish. In general we have preferred\n  // to duplicate such scalar code rather than call reference code to handle\n  // leftovers, thinking that code duplication was not a big concern.\n  // However, most of our unit tests happen to test only optimized code,\n  // and the quantized HardSwish implementation is nontrivial enough that\n  // I really want test coverage for the reference code.\n  if (i < flat_size) {\n    const RuntimeShape leftover_shape{flat_size - i};\n    reference_ops::HardSwish(params, leftover_shape, input_data + i,\n                             leftover_shape, output_data + i);\n  }\n}\n\ntemplate <typename T>\ninline void IntegerExponentPow(const ArithmeticParams& params,\n                               const RuntimeShape& unextended_base_shape,\n                               const T* base_data, const int exponent,\n                               const RuntimeShape& unextended_output_shape,\n                               T* output_data) {\n  TFLITE_DCHECK_GE(exponent, 1);\n  if (exponent == 1) {\n    // copy data over.\n    std::memcpy(output_data, base_data,\n                unextended_base_shape.FlatSize() * sizeof(T));\n  } else {\n    IntegerExponentPow(params, unextended_base_shape, base_data, exponent / 2,\n                       unextended_output_shape, output_data);\n    Mul(params, unextended_base_shape, output_data, unextended_base_shape,\n        output_data, unextended_output_shape, output_data);\n    if (exponent % 2 == 1) {\n      Mul(params, unextended_base_shape, base_data, unextended_base_shape,\n          output_data, unextended_output_shape, output_data);\n    }\n  }\n}\n\ntemplate <typename T>\ninline void BroadcastPow4D(const RuntimeShape& unextended_input1_shape,\n                           const T* input1_data,\n                           const RuntimeShape& unextended_input2_shape,\n                           const T* input2_data,\n                           const RuntimeShape& unextended_output_shape,\n                           T* output_data) {\n  ruy::profiler::ScopeLabel label(\"PowBroadcast\");\n\n  if (unextended_input2_shape.FlatSize() == 1) {\n    static const float epsilon = 1e-5;\n    const T exponent = input2_data[0];\n    const int int_exponent = static_cast<int>(std::round(exponent));\n    if ((std::abs(input2_data[0] - int_exponent) < epsilon) &&\n        (int_exponent >= 1)) {\n      ArithmeticParams params;\n      if (std::is_same<T, float>::value) {\n        params.float_activation_max = std::numeric_limits<float>::max();\n        params.float_activation_min = std::numeric_limits<float>::lowest();\n      } else if (std::is_same<T, int>::value) {\n        params.quantized_activation_max = std::numeric_limits<int>::max();\n        params.quantized_activation_min = std::numeric_limits<int>::lowest();\n      }\n      IntegerExponentPow(params, unextended_input1_shape, input1_data,\n                         int_exponent, unextended_output_shape, output_data);\n      return;\n    }\n  }\n  reference_ops::BroadcastPow4DSlow(unextended_input1_shape, input1_data,\n                                    unextended_input2_shape, input2_data,\n                                    unextended_output_shape, output_data);\n}\n\n#ifdef USE_NEON\n\ninline void ScaleWithNewZeroPoint(const int32x4_t input,\n                                  const float32x4_t scale_dup,\n                                  const float32x4_t zero_times_scale_dup,\n                                  float32x4_t* output) {\n#ifdef __ARM_FEATURE_FMA\n  *output = vfmaq_f32(zero_times_scale_dup, vcvtq_f32_s32(input), scale_dup);\n#else\n  *output = vaddq_f32(vmulq_f32(vcvtq_f32_s32(input), scale_dup),\n                      zero_times_scale_dup);\n#endif\n}\n\n#endif  // USE_NEON\n\ninline void Dequantize(const tflite::DequantizationParams& op_params,\n                       const RuntimeShape& input_shape,\n                       const uint8_t* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Dequantize/Uint8\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = op_params.scale;\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t scale_dup = vdupq_n_f32(static_cast<float>(scale));\n  const float32x4_t zero_times_scale_dup =\n      vdupq_n_f32(static_cast<float>(-zero_point * scale));\n  for (; i <= flat_size - 8; i += 8) {\n    const uint8x8_t input_u8 = vld1_u8(input_data + i);\n    const uint16x8_t input_u16 = vmovl_u8(input_u8);\n    const int16x8_t input_s16 = vreinterpretq_s16_u16(input_u16);\n    const int16x4_t input_s16_low = vget_low_s16(input_s16);\n    const int16x4_t input_s16_high = vget_high_s16(input_s16);\n    const int32x4_t val_low = vmovl_s16(input_s16_low);\n    const int32x4_t val_high = vmovl_s16(input_s16_high);\n\n    float32x4_t result_low, result_high;\n    ScaleWithNewZeroPoint(val_low, scale_dup, zero_times_scale_dup,\n                          &result_low);\n    ScaleWithNewZeroPoint(val_high, scale_dup, zero_times_scale_dup,\n                          &result_high);\n\n    vst1q_f32(output_data + i, result_low);\n    vst1q_f32(output_data + i + 4, result_high);\n  }\n#endif  // NEON\n  for (; i < flat_size; ++i) {\n    const int32 val = input_data[i];\n    const float result = static_cast<float>(scale * (val - zero_point));\n    output_data[i] = result;\n  }\n}\n\ninline void Dequantize(const tflite::DequantizationParams& op_params,\n                       const RuntimeShape& input_shape,\n                       const int8_t* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Dequantize/Int8\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = op_params.scale;\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t scale_dup = vdupq_n_f32(static_cast<float>(scale));\n  const float32x4_t zero_times_scale_dup =\n      vdupq_n_f32(static_cast<float>(-zero_point * scale));\n  for (; i <= flat_size - 8; i += 8) {\n    const int8x8_t input_s8 = vld1_s8(input_data + i);\n    const int16x8_t input_s16 = vmovl_s8(input_s8);\n    const int16x4_t input_s16_low = vget_low_s16(input_s16);\n    const int16x4_t input_s16_high = vget_high_s16(input_s16);\n    const int32x4_t val_low = vmovl_s16(input_s16_low);\n    const int32x4_t val_high = vmovl_s16(input_s16_high);\n\n    float32x4_t result_low, result_high;\n    ScaleWithNewZeroPoint(val_low, scale_dup, zero_times_scale_dup,\n                          &result_low);\n    ScaleWithNewZeroPoint(val_high, scale_dup, zero_times_scale_dup,\n                          &result_high);\n\n    vst1q_f32(output_data + i, result_low);\n    vst1q_f32(output_data + i + 4, result_high);\n  }\n#endif  // NEON\n  for (; i < flat_size; ++i) {\n    const int32 val = input_data[i];\n    const float result = static_cast<float>(scale * (val - zero_point));\n    output_data[i] = result;\n  }\n}\n\ninline void Dequantize(const tflite::DequantizationParams& op_params,\n                       const RuntimeShape& input_shape,\n                       const int16_t* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Dequantize/Int16\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = op_params.scale;\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t scale_dup = vdupq_n_f32(static_cast<float>(scale));\n  const float32x4_t zero_times_scale_dup =\n      vdupq_n_f32(static_cast<float>(-zero_point * scale));\n  for (; i <= flat_size - 8; i += 8) {\n    const int16x4_t input_s16_low = vld1_s16(input_data + i);\n    const int16x4_t input_s16_high = vld1_s16(input_data + i + 4);\n    const int32x4_t val_low = vmovl_s16(input_s16_low);\n    const int32x4_t val_high = vmovl_s16(input_s16_high);\n\n    float32x4_t result_low, result_high;\n    ScaleWithNewZeroPoint(val_low, scale_dup, zero_times_scale_dup,\n                          &result_low);\n    ScaleWithNewZeroPoint(val_high, scale_dup, zero_times_scale_dup,\n                          &result_high);\n\n    vst1q_f32(output_data + i, result_low);\n    vst1q_f32(output_data + i + 4, result_high);\n  }\n#endif  // NEON\n  for (; i < flat_size; ++i) {\n    const int32 val = input_data[i];\n    const float result = static_cast<float>(scale * (val - zero_point));\n    output_data[i] = result;\n  }\n}\n\ninline void Dequantize(const RuntimeShape& input_shape,\n                       const Eigen::half* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  reference_ops::Dequantize(input_shape, input_data, output_shape, output_data);\n}\n\ntemplate <typename T>\ninline void AffineQuantize(const tflite::QuantizationParams& op_params,\n                           const RuntimeShape& input_shape,\n                           const float* input_data,\n                           const RuntimeShape& output_shape, T* output_data) {\n  reference_ops::AffineQuantize(op_params, input_shape, input_data,\n                                output_shape, output_data);\n}\n\ntemplate <>\ninline void AffineQuantize(const tflite::QuantizationParams& op_params,\n                           const RuntimeShape& input_shape,\n                           const float* input_data,\n                           const RuntimeShape& output_shape,\n                           int8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantize/Int8\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = static_cast<double>(op_params.scale);\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  static constexpr int32 min_val = std::numeric_limits<int8_t>::min();\n  static constexpr int32 max_val = std::numeric_limits<int8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t reverse_scale_dup = vdupq_n_f32(1.0f / scale);\n  const int32x4_t zero_point_dup = vdupq_n_s32(zero_point);\n  const int32x4_t min_val_dup = vdupq_n_s32(min_val);\n  const int32x4_t max_val_dup = vdupq_n_s32(max_val);\n\n  for (; i <= flat_size - 8; i += 8) {\n    const float* src_data_ptr = input_data + i;\n    float32x4_t input_val_0 = vld1q_f32(src_data_ptr);\n    float32x4_t input_val_1 = vld1q_f32(src_data_ptr + 4);\n\n    input_val_0 = vmulq_f32(input_val_0, reverse_scale_dup);\n    input_val_1 = vmulq_f32(input_val_1, reverse_scale_dup);\n\n    int32x4_t casted_val_0 = RoundToNearest(input_val_0);\n    int32x4_t casted_val_1 = RoundToNearest(input_val_1);\n\n    casted_val_0 = vaddq_s32(casted_val_0, zero_point_dup);\n    casted_val_1 = vaddq_s32(casted_val_1, zero_point_dup);\n\n    // Clamp the values to fit the target type's range.\n    casted_val_0 = vmaxq_s32(casted_val_0, min_val_dup);\n    casted_val_1 = vmaxq_s32(casted_val_1, min_val_dup);\n    casted_val_0 = vminq_s32(casted_val_0, max_val_dup);\n    casted_val_1 = vminq_s32(casted_val_1, max_val_dup);\n\n    const int16x4_t narrowed_val_0 = vmovn_s32(casted_val_0);\n    const int16x4_t narrowed_val_1 = vmovn_s32(casted_val_1);\n    const int16x8_t combined_val = vcombine_s16(narrowed_val_0, narrowed_val_1);\n    const int8x8_t combined_val_narrowed = vmovn_s16(combined_val);\n    vst1_s8(output_data + i, combined_val_narrowed);\n  }\n#endif  // NEON\n\n  for (; i < flat_size; ++i) {\n    const float val = input_data[i];\n    const int32 unclamped =\n        static_cast<int32>(TfLiteRound(val / scale)) + zero_point;\n    const int32 clamped = std::min(std::max(unclamped, min_val), max_val);\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <>\ninline void AffineQuantize(const tflite::QuantizationParams& op_params,\n                           const RuntimeShape& input_shape,\n                           const float* input_data,\n                           const RuntimeShape& output_shape,\n                           uint8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantize/Uint8\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = static_cast<double>(op_params.scale);\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  static constexpr int32 min_val = std::numeric_limits<uint8_t>::min();\n  static constexpr int32 max_val = std::numeric_limits<uint8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t reverse_scale_dup = vdupq_n_f32(1.0f / scale);\n  const int32x4_t zero_point_dup = vdupq_n_s32(zero_point);\n  const int32x4_t min_val_dup = vdupq_n_s32(min_val);\n  const int32x4_t max_val_dup = vdupq_n_s32(max_val);\n\n  for (; i <= flat_size - 8; i += 8) {\n    const float* src_data_ptr = input_data + i;\n    float32x4_t input_val_0 = vld1q_f32(src_data_ptr);\n    float32x4_t input_val_1 = vld1q_f32(src_data_ptr + 4);\n\n    input_val_0 = vmulq_f32(input_val_0, reverse_scale_dup);\n    input_val_1 = vmulq_f32(input_val_1, reverse_scale_dup);\n\n    int32x4_t casted_val_0 = RoundToNearest(input_val_0);\n    int32x4_t casted_val_1 = RoundToNearest(input_val_1);\n\n    casted_val_0 = vaddq_s32(casted_val_0, zero_point_dup);\n    casted_val_1 = vaddq_s32(casted_val_1, zero_point_dup);\n\n    // Clamp the values to fit the target type's range.\n    casted_val_0 = vmaxq_s32(casted_val_0, min_val_dup);\n    casted_val_1 = vmaxq_s32(casted_val_1, min_val_dup);\n    casted_val_0 = vminq_s32(casted_val_0, max_val_dup);\n    casted_val_1 = vminq_s32(casted_val_1, max_val_dup);\n\n    const uint16x4_t narrowed_val_0 = vqmovun_s32(casted_val_0);\n    const uint16x4_t narrowed_val_1 = vqmovun_s32(casted_val_1);\n    const uint16x8_t combined_val =\n        vcombine_u16(narrowed_val_0, narrowed_val_1);\n    const uint8x8_t combined_val_narrowed = vmovn_u16(combined_val);\n    vst1_u8(output_data + i, combined_val_narrowed);\n  }\n#endif  // NEON\n\n  for (; i < flat_size; ++i) {\n    const float val = input_data[i];\n    const int32 unclamped =\n        static_cast<int32>(TfLiteRound(val / scale)) + zero_point;\n    const int32 clamped = std::min(std::max(unclamped, min_val), max_val);\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <>\ninline void AffineQuantize(const tflite::QuantizationParams& op_params,\n                           const RuntimeShape& input_shape,\n                           const float* input_data,\n                           const RuntimeShape& output_shape,\n                           int16_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantize/Int16\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = static_cast<double>(op_params.scale);\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  static constexpr int32 min_val = std::numeric_limits<int16_t>::min();\n  static constexpr int32 max_val = std::numeric_limits<int16_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t reverse_scale_dup = vdupq_n_f32(1.0f / scale);\n  const int32x4_t zero_point_dup = vdupq_n_s32(zero_point);\n  const int32x4_t min_val_dup = vdupq_n_s32(min_val);\n  const int32x4_t max_val_dup = vdupq_n_s32(max_val);\n\n  for (; i <= flat_size - 8; i += 8) {\n    const float* src_data_ptr = input_data + i;\n    float32x4_t input_val_0 = vld1q_f32(src_data_ptr);\n    float32x4_t input_val_1 = vld1q_f32(src_data_ptr + 4);\n\n    input_val_0 = vmulq_f32(input_val_0, reverse_scale_dup);\n    input_val_1 = vmulq_f32(input_val_1, reverse_scale_dup);\n\n    int32x4_t casted_val_0 = RoundToNearest(input_val_0);\n    int32x4_t casted_val_1 = RoundToNearest(input_val_1);\n\n    casted_val_0 = vaddq_s32(casted_val_0, zero_point_dup);\n    casted_val_1 = vaddq_s32(casted_val_1, zero_point_dup);\n\n    // Clamp the values to fit the target type's range.\n    casted_val_0 = vmaxq_s32(casted_val_0, min_val_dup);\n    casted_val_1 = vmaxq_s32(casted_val_1, min_val_dup);\n    casted_val_0 = vminq_s32(casted_val_0, max_val_dup);\n    casted_val_1 = vminq_s32(casted_val_1, max_val_dup);\n\n    const int16x4_t narrowed_val_0 = vmovn_s32(casted_val_0);\n    const int16x4_t narrowed_val_1 = vmovn_s32(casted_val_1);\n    vst1_s16(output_data + i, narrowed_val_0);\n    vst1_s16(output_data + i + 4, narrowed_val_1);\n  }\n#endif  // NEON\n\n  for (; i < flat_size; ++i) {\n    const float val = input_data[i];\n    const int32 unclamped =\n        static_cast<int32>(TfLiteRound(val / scale)) + zero_point;\n    const int32 clamped = std::min(std::max(unclamped, min_val), max_val);\n    output_data[i] = clamped;\n  }\n}\n\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n\ninline int16x8x4_t SaturatingRounding(\n    int16x8_t input_val_0, int16x8_t input_val_1, int16x8_t input_val_2,\n    int16x8_t input_val_3, int input_left_shift, int input_multiplier) {\n  // This performs what is expressed in the scalar code as\n  // const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n  //      static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n  //      static_cast<int16>(input_multiplier));\n  const int16x8_t left_shift_dup = vdupq_n_s16(input_left_shift);\n  const int16x8_t input_val_shifted_0 = vshlq_s16(input_val_0, left_shift_dup);\n  const int16x8_t input_val_shifted_1 = vshlq_s16(input_val_1, left_shift_dup);\n  const int16x8_t input_val_shifted_2 = vshlq_s16(input_val_2, left_shift_dup);\n  const int16x8_t input_val_shifted_3 = vshlq_s16(input_val_3, left_shift_dup);\n  int16x8x4_t result;\n  result.val[0] = vqrdmulhq_n_s16(input_val_shifted_0, input_multiplier);\n  result.val[1] = vqrdmulhq_n_s16(input_val_shifted_1, input_multiplier);\n  result.val[2] = vqrdmulhq_n_s16(input_val_shifted_2, input_multiplier);\n  result.val[3] = vqrdmulhq_n_s16(input_val_shifted_3, input_multiplier);\n  return result;\n}\n\n// 4-bit fixed point is enough for tanh since tanh(16) is almost same with one,\n// considering 7 digits under zero.\ninline int16x8x4_t FixedPoint4Logistic(int16x8x4_t input_val) {\n  // Invoke gemmlowp::logistic on FixedPoint wrapping int16x8_t\n  using FixedPoint4 = gemmlowp::FixedPoint<int16x8_t, 4>;\n  using FixedPoint0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n  const FixedPoint4 input_val_f4_0 = FixedPoint4::FromRaw(input_val.val[0]);\n  const FixedPoint4 input_val_f4_1 = FixedPoint4::FromRaw(input_val.val[1]);\n  const FixedPoint4 input_val_f4_2 = FixedPoint4::FromRaw(input_val.val[2]);\n  const FixedPoint4 input_val_f4_3 = FixedPoint4::FromRaw(input_val.val[3]);\n\n  // TODO(b/134622898) Implement a low accuracy version of logistic. In this\n  // method, gemmlowp::tanh spends about 80% of the execution times. The\n  // current implementation is rougly 12-bit accurate in the 16-bit fixed\n  // point case. Until reaching to error bounds, there are rooms for\n  // improvements.\n  const FixedPoint0 output_val_f0_0 = gemmlowp::logistic(input_val_f4_0);\n  const FixedPoint0 output_val_f0_1 = gemmlowp::logistic(input_val_f4_1);\n  const FixedPoint0 output_val_f0_2 = gemmlowp::logistic(input_val_f4_2);\n  const FixedPoint0 output_val_f0_3 = gemmlowp::logistic(input_val_f4_3);\n\n  // Divide by 2^7 as in the scalar code\n  int16x8x4_t result;\n  result.val[0] = vrshrq_n_s16(output_val_f0_0.raw(), 7);\n  result.val[1] = vrshrq_n_s16(output_val_f0_1.raw(), 7);\n  result.val[2] = vrshrq_n_s16(output_val_f0_2.raw(), 7);\n  result.val[3] = vrshrq_n_s16(output_val_f0_3.raw(), 7);\n  return result;\n}\n\n// 4-bit fixed point is enough for tanh since tanh(16) is almost same with one,\n// considering 11 digits under zero at least.\ninline int16x8x4_t FixedPoint4Tanh(int16x8x4_t input_val) {\n  // Invoke gemmlowp::logistic on FixedPoint wrapping int16x8_t\n  using FixedPoint4 = gemmlowp::FixedPoint<int16x8_t, 4>;\n  using FixedPoint0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n  const FixedPoint4 input_val_f4_0 = FixedPoint4::FromRaw(input_val.val[0]);\n  const FixedPoint4 input_val_f4_1 = FixedPoint4::FromRaw(input_val.val[1]);\n  const FixedPoint4 input_val_f4_2 = FixedPoint4::FromRaw(input_val.val[2]);\n  const FixedPoint4 input_val_f4_3 = FixedPoint4::FromRaw(input_val.val[3]);\n\n  // TODO(b/134622898) Implement a low accuracy version of logistic. In this\n  // method, gemmlowp::tanh spends about 80% of the execution times. The\n  // current implementation is rougly 12-bit accurate in the 16-bit fixed\n  // point case. Until reaching to error bounds, there are rooms for\n  // improvements.\n  const FixedPoint0 output_val_f0_0 = gemmlowp::tanh(input_val_f4_0);\n  const FixedPoint0 output_val_f0_1 = gemmlowp::tanh(input_val_f4_1);\n  const FixedPoint0 output_val_f0_2 = gemmlowp::tanh(input_val_f4_2);\n  const FixedPoint0 output_val_f0_3 = gemmlowp::tanh(input_val_f4_3);\n\n  // Divide by 2^7 as in the scalar code\n  int16x8x4_t result;\n  result.val[0] = vrshrq_n_s16(output_val_f0_0.raw(), 8);\n  result.val[1] = vrshrq_n_s16(output_val_f0_1.raw(), 8);\n  result.val[2] = vrshrq_n_s16(output_val_f0_2.raw(), 8);\n  result.val[3] = vrshrq_n_s16(output_val_f0_3.raw(), 8);\n  return result;\n}\n\ninline uint8x16x2_t CalculateUnsignedClampingWithRangeBitMasks(\n    int16x8x2_t input_val, int16x8_t range_radius_dup,\n    int16x8_t neg_range_radius_dup) {\n  const uint16x8_t mask_rightclamp_0 =\n      vcgtq_s16(input_val.val[0], range_radius_dup);\n  const uint16x8_t mask_rightclamp_1 =\n      vcgtq_s16(input_val.val[1], range_radius_dup);\n\n  const uint16x8_t mask_leftclamp_0 =\n      vcgeq_s16(input_val.val[0], neg_range_radius_dup);\n  const uint16x8_t mask_leftclamp_1 =\n      vcgeq_s16(input_val.val[1], neg_range_radius_dup);\n\n  uint8x16x2_t result;\n  result.val[0] = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                              vshrn_n_u16(mask_leftclamp_1, 8));\n  result.val[1] = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                              vshrn_n_u16(mask_rightclamp_1, 8));\n  return result;\n}\n\ninline uint8x16x2_t CalculateSignedClampingWithRangeBitMasks(\n    int16x8x2_t input_val, int16x8_t range_radius_dup,\n    int16x8_t neg_range_radius_dup) {\n  const uint16x8_t mask_rightclamp_0 =\n      vcgtq_s16(input_val.val[0], range_radius_dup);\n  const uint16x8_t mask_rightclamp_1 =\n      vcgtq_s16(input_val.val[1], range_radius_dup);\n\n  const uint16x8_t mask_leftclamp_0 =\n      vcltq_s16(input_val.val[0], neg_range_radius_dup);\n  const uint16x8_t mask_leftclamp_1 =\n      vcltq_s16(input_val.val[1], neg_range_radius_dup);\n\n  uint8x16x2_t result;\n  result.val[0] = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                              vshrn_n_u16(mask_leftclamp_1, 8));\n  result.val[1] = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                              vshrn_n_u16(mask_rightclamp_1, 8));\n  return result;\n}\n\ninline void ClampWithRangeAndStore(uint8_t* output_dst, uint8x16_t input_val,\n                                   uint8x16x2_t masks_clamp) {\n  // Store back to memory\n  vst1q_u8(output_dst, vandq_u8(vorrq_u8(input_val, masks_clamp.val[1]),\n                                masks_clamp.val[0]));\n}\n\ninline void ClampWithRangeAndStore(int8_t* output_dst, int8x16_t input_val,\n                                   uint8x16x2_t masks_clamp) {\n  static const int8x16_t max_dup = vdupq_n_s8(127);\n  static const int8x16_t min_dup = vdupq_n_s8(-128);\n  // Store back to memory\n  vst1q_s8(output_dst,\n           vbslq_s8(masks_clamp.val[1], max_dup,\n                    vbslq_s8(masks_clamp.val[0], min_dup, input_val)));\n}\n\n#endif  // GEMMLOWP_NEON\n\ninline void Tanh16bitPrecision(const TanhParams& params,\n                               const RuntimeShape& input_shape,\n                               const uint8* input_data,\n                               const RuntimeShape& output_shape,\n                               uint8* output_data) {\n  // Note that this is almost the exact same code as in Logistic().\n  ruy::profiler::ScopeLabel label(\"Tanh/Uint8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int16 input_multiplier = static_cast<int16>(params.input_multiplier);\n  const int16 input_left_shift = static_cast<int16>(params.input_left_shift);\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n  int16_t output_zero_point = 128;\n\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n  const int16x8_t range_radius_dup = vdupq_n_s16(input_range_radius);\n  const int16x8_t neg_range_radius_dup = vdupq_n_s16(-input_range_radius);\n  const int16x8_t output_zero_point_s16 = vdupq_n_s16(output_zero_point);\n\n  // Handle 32 values at a time\n  for (; c <= size - 32; c += 32) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_val_centered_0_1 =\n        Load16AndSubtractZeroPoint(input_data + c, input_zero_point);\n    const int16x8x2_t input_val_centered_2_3 =\n        Load16AndSubtractZeroPoint(input_data + c + 16, input_zero_point);\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint8x16x2_t masks_clamp_0_1 = CalculateUnsignedClampingWithRangeBitMasks(\n        input_val_centered_0_1, range_radius_dup, neg_range_radius_dup);\n    uint8x16x2_t masks_clamp_2_3 = CalculateUnsignedClampingWithRangeBitMasks(\n        input_val_centered_2_3, range_radius_dup, neg_range_radius_dup);\n\n    int16x8x4_t input_val_rescaled = SaturatingRounding(\n        input_val_centered_0_1.val[0], input_val_centered_0_1.val[1],\n        input_val_centered_2_3.val[0], input_val_centered_2_3.val[1],\n        input_left_shift, input_multiplier);\n\n    int16x8x4_t output_val_s16 = FixedPoint4Tanh(input_val_rescaled);\n\n    // Add the output zero point\n    output_val_s16.val[0] =\n        vaddq_s16(output_val_s16.val[0], output_zero_point_s16);\n    output_val_s16.val[1] =\n        vaddq_s16(output_val_s16.val[1], output_zero_point_s16);\n    output_val_s16.val[2] =\n        vaddq_s16(output_val_s16.val[2], output_zero_point_s16);\n    output_val_s16.val[3] =\n        vaddq_s16(output_val_s16.val[3], output_zero_point_s16);\n\n    // Cast output values to uint8, saturating\n    uint8x16_t output_val_u8_0_1 = vcombine_u8(\n        vqmovun_s16(output_val_s16.val[0]), vqmovun_s16(output_val_s16.val[1]));\n    uint8x16_t output_val_u8_2_3 = vcombine_u8(\n        vqmovun_s16(output_val_s16.val[2]), vqmovun_s16(output_val_s16.val[3]));\n\n    ClampWithRangeAndStore(output_data + c, output_val_u8_0_1, masks_clamp_0_1);\n    ClampWithRangeAndStore(output_data + c + 16, output_val_u8_2_3,\n                           masks_clamp_2_3);\n  }\n#endif  // GEMMLOWP_NEON\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int16 input_val_centered =\n        static_cast<int16>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      using gemmlowp::SaturatingRoundingDoublingHighMul;\n      const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n          static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n          static_cast<int16>(input_multiplier));\n      using FixedPoint4 = gemmlowp::FixedPoint<int16, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int16, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::tanh(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int16 output_val_s16 = RoundingDivideByPOT(output_val_f0.raw(), 8);\n      output_val_s16 += output_zero_point;\n      if (output_val_s16 == 256) {\n        output_val_s16 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s16, 0);\n      TFLITE_DCHECK_LE(output_val_s16, 255);\n      output_val = static_cast<uint8>(output_val_s16);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Tanh16bitPrecision(const TanhParams& params,\n                               const RuntimeShape& input_shape,\n                               const int8* input_data,\n                               const RuntimeShape& output_shape,\n                               int8* output_data) {\n  // Note that this is almost the exact same code as in Logistic().\n  ruy::profiler::ScopeLabel label(\"Tanh/Int8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int16 input_multiplier = static_cast<int16>(params.input_multiplier);\n  const int16 input_left_shift = static_cast<int16>(params.input_left_shift);\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n  const int16x8_t range_radius_dup = vdupq_n_s16(input_range_radius);\n  const int16x8_t neg_range_radius_dup = vdupq_n_s16(-input_range_radius);\n\n  // Handle 32 values at a time\n  for (; c <= size - 32; c += 32) {\n    // Read input int8 values, cast to int16 and subtract input_zero_point\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_val_centered_0_1 =\n        Load16AndSubtractZeroPoint(input_data + c, input_zero_point);\n    const int16x8x2_t input_val_centered_2_3 =\n        Load16AndSubtractZeroPoint(input_data + c + 16, input_zero_point);\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = -128;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 127;\n    //   } else {\n    //     ...\n    uint8x16x2_t masks_clamp_0_1 = CalculateSignedClampingWithRangeBitMasks(\n        input_val_centered_0_1, range_radius_dup, neg_range_radius_dup);\n    uint8x16x2_t masks_clamp_2_3 = CalculateSignedClampingWithRangeBitMasks(\n        input_val_centered_2_3, range_radius_dup, neg_range_radius_dup);\n\n    int16x8x4_t input_val_rescaled = SaturatingRounding(\n        input_val_centered_0_1.val[0], input_val_centered_0_1.val[1],\n        input_val_centered_2_3.val[0], input_val_centered_2_3.val[1],\n        input_left_shift, input_multiplier);\n\n    int16x8x4_t output_val_s16 = FixedPoint4Tanh(input_val_rescaled);\n\n    // Cast output values to uint8, saturating\n    int8x16_t output_val_s8_0_1 = vcombine_s8(\n        vqmovn_s16(output_val_s16.val[0]), vqmovn_s16(output_val_s16.val[1]));\n    int8x16_t output_val_s8_2_3 = vcombine_s8(\n        vqmovn_s16(output_val_s16.val[2]), vqmovn_s16(output_val_s16.val[3]));\n\n    ClampWithRangeAndStore(output_data + c, output_val_s8_0_1, masks_clamp_0_1);\n    ClampWithRangeAndStore(output_data + c + 16, output_val_s8_2_3,\n                           masks_clamp_2_3);\n  }\n#endif  // GEMMLOWP_NEON\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const int8 input_val_s8 = input_data[c];\n    const int16 input_val_centered =\n        static_cast<int16>(input_val_s8) - input_zero_point;\n    int8 output_val;\n    if (input_val_centered <= -input_range_radius) {\n      output_val = -128;\n    } else if (input_val_centered >= input_range_radius) {\n      output_val = 127;\n    } else {\n      using gemmlowp::SaturatingRoundingDoublingHighMul;\n      const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n          static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n          static_cast<int16>(input_multiplier));\n      using FixedPoint4 = gemmlowp::FixedPoint<int16, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int16, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::tanh(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int16 output_val_s16 = RoundingDivideByPOT(output_val_f0.raw(), 8);\n      if (output_val_s16 == 128) {\n        output_val_s16 = 127;\n      }\n      TFLITE_DCHECK_GE(output_val_s16, -128);\n      TFLITE_DCHECK_LE(output_val_s16, 127);\n      output_val = static_cast<int8>(output_val_s16);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Logistic16bitPrecision(const LogisticParams& params,\n                                   const RuntimeShape& input_shape,\n                                   const uint8* input_data,\n                                   const RuntimeShape& output_shape,\n                                   uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic/Uint8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int16 input_left_shift = static_cast<int16>(params.input_left_shift);\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n  const int16x8_t range_radius_dup = vdupq_n_s16(input_range_radius);\n  const int16x8_t neg_range_radius_dup = vdupq_n_s16(-input_range_radius);\n\n  // Handle 32 values at a time\n  for (; c <= size - 32; c += 32) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_val_centered_0_1 =\n        Load16AndSubtractZeroPoint(input_data + c, input_zero_point);\n    const int16x8x2_t input_val_centered_2_3 =\n        Load16AndSubtractZeroPoint(input_data + c + 16, input_zero_point);\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint8x16x2_t masks_clamp_0_1 = CalculateUnsignedClampingWithRangeBitMasks(\n        input_val_centered_0_1, range_radius_dup, neg_range_radius_dup);\n    uint8x16x2_t masks_clamp_2_3 = CalculateUnsignedClampingWithRangeBitMasks(\n        input_val_centered_2_3, range_radius_dup, neg_range_radius_dup);\n\n    int16x8x4_t input_val_rescaled = SaturatingRounding(\n        input_val_centered_0_1.val[0], input_val_centered_0_1.val[1],\n        input_val_centered_2_3.val[0], input_val_centered_2_3.val[1],\n        input_left_shift, input_multiplier);\n\n    int16x8x4_t output_val_s16 = FixedPoint4Logistic(input_val_rescaled);\n\n    // Cast output values to uint8, saturating\n    uint8x16_t output_val_u8_0_1 = vcombine_u8(\n        vqmovun_s16(output_val_s16.val[0]), vqmovun_s16(output_val_s16.val[1]));\n    uint8x16_t output_val_u8_2_3 = vcombine_u8(\n        vqmovun_s16(output_val_s16.val[2]), vqmovun_s16(output_val_s16.val[3]));\n\n    ClampWithRangeAndStore(output_data + c, output_val_u8_0_1, masks_clamp_0_1);\n    ClampWithRangeAndStore(output_data + c + 16, output_val_u8_2_3,\n                           masks_clamp_2_3);\n  }\n#endif  // GEMMLOWP_NEON\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int16 input_val_centered =\n        static_cast<int16>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      using gemmlowp::SaturatingRoundingDoublingHighMul;\n      const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n          static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n          static_cast<int16>(input_multiplier));\n      using FixedPoint4 = gemmlowp::FixedPoint<int16, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int16, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int16 output_val_s16 = RoundingDivideByPOT(output_val_f0.raw(), 7);\n      if (output_val_s16 == 256) {\n        output_val_s16 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s16, 0);\n      TFLITE_DCHECK_LE(output_val_s16, 255);\n      output_val = static_cast<uint8>(output_val_s16);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Logistic16bitPrecision(const LogisticParams& params,\n                                   const RuntimeShape& input_shape,\n                                   const int8* input_data,\n                                   const RuntimeShape& output_shape,\n                                   int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic/Int8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int16 input_left_shift = static_cast<int16>(params.input_left_shift);\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n  const int16 output_zero_point = 128;\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n  const int16x8_t range_radius_dup = vdupq_n_s16(input_range_radius);\n  const int16x8_t neg_range_radius_dup = vdupq_n_s16(-input_range_radius);\n  const int16x8_t output_zero_point_dup = vdupq_n_s16(output_zero_point);\n\n  // Handle 32 values at a time\n  for (; c <= size - 32; c += 32) {\n    // Read input int8 values, cast to int16 and subtract input_zero_point\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_val_centered_0_1 =\n        Load16AndSubtractZeroPoint(input_data + c, input_zero_point);\n    const int16x8x2_t input_val_centered_2_3 =\n        Load16AndSubtractZeroPoint(input_data + c + 16, input_zero_point);\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = -128;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 127;\n    //   } else {\n    //     ...\n    uint8x16x2_t masks_clamp_0_1 = CalculateSignedClampingWithRangeBitMasks(\n        input_val_centered_0_1, range_radius_dup, neg_range_radius_dup);\n    uint8x16x2_t masks_clamp_2_3 = CalculateSignedClampingWithRangeBitMasks(\n        input_val_centered_2_3, range_radius_dup, neg_range_radius_dup);\n\n    int16x8x4_t input_val_rescaled = SaturatingRounding(\n        input_val_centered_0_1.val[0], input_val_centered_0_1.val[1],\n        input_val_centered_2_3.val[0], input_val_centered_2_3.val[1],\n        input_left_shift, input_multiplier);\n\n    int16x8x4_t output_val_s16 = FixedPoint4Logistic(input_val_rescaled);\n\n    // Substract output zero point.\n    output_val_s16.val[0] =\n        vsubq_s16(output_val_s16.val[0], output_zero_point_dup);\n    output_val_s16.val[1] =\n        vsubq_s16(output_val_s16.val[1], output_zero_point_dup);\n    output_val_s16.val[2] =\n        vsubq_s16(output_val_s16.val[2], output_zero_point_dup);\n    output_val_s16.val[3] =\n        vsubq_s16(output_val_s16.val[3], output_zero_point_dup);\n\n    // Cast output values to int8, saturating\n    int8x16_t output_val_s8_0_1 = vcombine_s8(\n        vqmovn_s16(output_val_s16.val[0]), vqmovn_s16(output_val_s16.val[1]));\n    int8x16_t output_val_s8_2_3 = vcombine_s8(\n        vqmovn_s16(output_val_s16.val[2]), vqmovn_s16(output_val_s16.val[3]));\n\n    ClampWithRangeAndStore(output_data + c, output_val_s8_0_1, masks_clamp_0_1);\n    ClampWithRangeAndStore(output_data + c + 16, output_val_s8_2_3,\n                           masks_clamp_2_3);\n  }\n#endif  // GEMMLOWP_NEON\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const int8 input_val_s8 = input_data[c];\n    const int16 input_val_centered =\n        static_cast<int16>(input_val_s8) - input_zero_point;\n    int8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = -128;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 127;\n    } else {\n      using gemmlowp::SaturatingRoundingDoublingHighMul;\n      const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n          static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n          static_cast<int16>(input_multiplier));\n      using FixedPoint4 = gemmlowp::FixedPoint<int16, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int16, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int16 output_val_s16 = RoundingDivideByPOT(output_val_f0.raw(), 7);\n      output_val_s16 -= output_zero_point;\n      if (output_val_s16 == 128) {\n        output_val_s16 = 127;\n      }\n      TFLITE_DCHECK_GE(output_val_s16, -128);\n      TFLITE_DCHECK_LE(output_val_s16, 127);\n      output_val = static_cast<int8>(output_val_s16);\n    }\n    output_data[c] = output_val;\n  }\n}\n\n// Transpose2D only deals with typical 2D matrix transpose ops.\n// Perform transpose by transposing 4x4 blocks of the input, proceeding from\n// left to right (down the rows) of the input, and then from top to bottom.\ntemplate <typename T>\ninline void Transpose2D(const RuntimeShape& input_shape, const T* input_data,\n                        const RuntimeShape& output_shape, T* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);\n\n  const int d0 = input_shape.DimsData()[0];\n  const int d1 = input_shape.DimsData()[1];\n  const int kLines = 4;\n  const int kSkipSize = (kLines - 1) * d1;\n\n  const T* input = input_data;\n\n  int i = 0;\n  for (; i <= d0 - kLines; i += kLines) {\n    T* output = output_data + i;\n\n    const T* input_ptr = input;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n\n    int j = 0;\n    for (; j <= d1 - kLines; j += kLines) {\n      input_ptr = input;\n      const T a00 = input_ptr[0];\n      const T a01 = input_ptr[1];\n      const T a02 = input_ptr[2];\n      const T a03 = input_ptr[3];\n      input_ptr += d1;\n      const T a10 = input_ptr[0];\n      const T a11 = input_ptr[1];\n      const T a12 = input_ptr[2];\n      const T a13 = input_ptr[3];\n      input_ptr += d1;\n      const T a20 = input_ptr[0];\n      const T a21 = input_ptr[1];\n      const T a22 = input_ptr[2];\n      const T a23 = input_ptr[3];\n      input_ptr += d1;\n      const T a30 = input_ptr[0];\n      const T a31 = input_ptr[1];\n      const T a32 = input_ptr[2];\n      const T a33 = input_ptr[3];\n\n      output[0] = a00;\n      output[1] = a10;\n      output[2] = a20;\n      output[3] = a30;\n      output += d0;\n\n      output[0] = a01;\n      output[1] = a11;\n      output[2] = a21;\n      output[3] = a31;\n      output += d0;\n\n      output[0] = a02;\n      output[1] = a12;\n      output[2] = a22;\n      output[3] = a32;\n      output += d0;\n\n      output[0] = a03;\n      output[1] = a13;\n      output[2] = a23;\n      output[3] = a33;\n      output += d0;\n\n      input += kLines;\n    }\n    if (j == d1) {\n      input += kSkipSize;\n    } else {\n      for (int p = 0; p < kLines; ++p) {\n        for (int q = 0; q < d1 - j; ++q) {\n          *(output + q * d0 + p) = *(input + p * d1 + q);\n        }\n      }\n      input += (d1 - j) + kSkipSize;\n    }\n  }\n  for (; i < d0; ++i) {\n    T* output = output_data + i;\n    for (int j = 0; j < d1; ++j) {\n      *output = *input;\n      output += d0;\n      ++input;\n    }\n  }\n}\n\ntemplate <>\ninline void Transpose2D(const RuntimeShape& input_shape,\n                        const int32_t* input_data,\n                        const RuntimeShape& output_shape,\n                        int32_t* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);\n\n  const int d0 = input_shape.DimsData()[0];\n  const int d1 = input_shape.DimsData()[1];\n#ifdef USE_NEON\n  const int kLines = 4;\n  const int kSkipSize = (kLines - 1) * d1;\n#endif\n\n  const int32_t* input = input_data;\n\n  int i = 0;\n#ifdef USE_NEON\n  for (; i <= d0 - kLines; i += kLines) {\n    int32_t* output = output_data + i;\n\n    const int32_t* input_ptr = input;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n\n    int j = 0;\n    for (; j <= d1 - kLines; j += kLines) {\n      input_ptr = input;\n      int32x4_t a0 = vld1q_s32(input);\n      input_ptr += d1;\n      int32x4_t a1 = vld1q_s32(input_ptr);\n      input_ptr += d1;\n      int32x4_t a2 = vld1q_s32(input_ptr);\n      input_ptr += d1;\n      int32x4_t a3 = vld1q_s32(input_ptr);\n\n      int32x4x2_t tmp1 = vuzpq_s32(a0, a2);\n      int32x4x2_t tmp2 = vuzpq_s32(a1, a3);\n      int32x4x2_t tmp3 = vtrnq_s32(tmp1.val[0], tmp2.val[0]);\n      int32x4x2_t tmp4 = vtrnq_s32(tmp1.val[1], tmp2.val[1]);\n\n      vst1q_s32(output, tmp3.val[0]);\n      output += d0;\n      vst1q_s32(output, tmp4.val[0]);\n      output += d0;\n      vst1q_s32(output, tmp3.val[1]);\n      output += d0;\n      vst1q_s32(output, tmp4.val[1]);\n      output += d0;\n      input += kLines;\n    }\n    if (j == d1) {\n      input += kSkipSize;\n    } else {\n      for (int p = 0; p < kLines; ++p) {\n        for (int q = 0; q < d1 - j; ++q) {\n          *(output + q * d0 + p) = *(input + p * d1 + q);\n        }\n      }\n      input += (d1 - j) + kSkipSize;\n    }\n  }\n#endif\n  for (; i < d0; ++i) {\n    int32_t* output = output_data + i;\n    for (int j = 0; j < d1; ++j) {\n      *output = *input;\n      output += d0;\n      ++input;\n    }\n  }\n}\n\n// TODO(b/173718660): see if we can reduce the number\n// of lines of code in branching without affecting latency.\ntemplate <typename T>\ninline void Transpose3D(const TransposeParams& params,\n                        const RuntimeShape& input_shape, const T* input_data,\n                        const RuntimeShape& output_shape, T* output_data) {\n  int s1, s2, s3;\n  s1 = input_shape.Dims(0);\n  s2 = input_shape.Dims(1);\n  s3 = input_shape.Dims(2);\n\n  int p1, p2, p3;\n  if (params.perm[0] == 2) {\n    p1 = 1;\n  } else if (params.perm[1] == 2) {\n    p2 = 1;\n  } else {\n    p3 = 1;\n  }\n\n  if (params.perm[0] == 1) {\n    p1 = s3;\n  } else if (params.perm[1] == 1) {\n    p2 = s3;\n  } else {\n    p3 = s3;\n  }\n\n  if (params.perm[0] == 0) {\n    p1 = s2 * s3;\n  } else if (params.perm[1] == 0) {\n    p2 = s2 * s3;\n  } else {\n    p3 = s2 * s3;\n  }\n\n  int o_s[3];\n  o_s[0] = input_shape.Dims(params.perm[0]);\n  o_s[1] = input_shape.Dims(params.perm[1]);\n  o_s[2] = input_shape.Dims(params.perm[2]);\n\n  for (int i1 = 0; i1 < o_s[0]; ++i1) {\n    for (int i2 = 0; i2 < o_s[1]; ++i2) {\n      for (int i3 = 0; i3 < o_s[2]; ++i3) {\n        const int i = i1 * p1 + i2 * p2 + i3 * p3;\n        const int o = i1 * o_s[1] * o_s[2] + i2 * o_s[2] + i3;\n        output_data[o] = input_data[i];\n      }\n    }\n  }\n}\n\ntemplate <typename T, int N>\nvoid TransposeImpl(const TransposeParams& params,\n                   const RuntimeShape& input_shape, const T* input_data,\n                   const RuntimeShape& output_shape, T* output_data) {\n  const int dims_cnt = input_shape.DimensionsCount();\n\n  int dim0, dim1;\n  if (transpose_utils::IsTranspose2DApplicable(params, input_shape, &dim0,\n                                               &dim1)) {\n    Transpose2D(RuntimeShape({dim0, dim1}), input_data,\n                RuntimeShape({dim1, dim0}), output_data);\n    return;\n  }\n\n  // TODO(b/141217325): notably Eigen is better suited for\n  // larger inputs whereas Transpose3D is generally\n  // better for smaller ones.\n  //\n  // E.g. on Nexus 5, Eigen is better for size 96^3 and up\n  // and Transpose3D is better for 72^3 and down.\n  //\n  // 96^3 is not mobile-friendly for certain usecases\n  // (e.g. model used in beam search for seq2seq) but is in others.\n  // Consider tradeoffs.\n  if (dims_cnt == 3) {\n    Transpose3D(params, input_shape, input_data, output_shape, output_data);\n    return;\n  }\n\n  // Reroute to the reference version if an optimized method for the given data\n  // is not available.\n  reference_ops::Transpose<T, N>(params, input_shape, input_data, output_shape,\n                                 output_data);\n}\n\ntemplate <typename T, int N = 5>\nvoid Transpose(const TransposeParams& unshrinked_params,\n               const RuntimeShape& unshrinked_input_shape, const T* input_data,\n               const RuntimeShape& unshrinked_output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Transpose\");\n\n  const int output_size = unshrinked_output_shape.DimensionsCount();\n  TFLITE_DCHECK_LE(unshrinked_input_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(output_size, N);\n  TFLITE_DCHECK_EQ(output_size, unshrinked_params.perm_count);\n\n  RuntimeShape shrinked_input_shape = RuntimeShape(unshrinked_input_shape);\n  RuntimeShape shrinked_output_shape = RuntimeShape(unshrinked_output_shape);\n  TransposeParams shrinked_params = unshrinked_params;\n\n  // Reduce any dimensions that have one size. Lower transpose op usually\n  // performs better since memory access patterns will be improved.\n  transpose_utils::RemoveOneSizeDimensions(\n      &shrinked_input_shape, &shrinked_output_shape, &shrinked_params);\n\n  // Handle identity cases.\n  // TODO(b/140779653): Add an optimization pass in the conversion process to\n  // remove transpose op nodes where they do nothing like the below one.\n  bool identical = true;\n  for (int i = 0; i < shrinked_params.perm_count; ++i) {\n    if (shrinked_params.perm[i] != i) {\n      identical = false;\n      break;\n    }\n  }\n  if (identical) {\n    memcpy(output_data, input_data,\n           unshrinked_input_shape.FlatSize() * sizeof(T));\n    return;\n  }\n\n  // Reduce dimensions by flattening.\n  if (shrinked_params.perm[0] == 0 && output_size >= 3) {\n    RuntimeShape non_flatten_input_shape;\n    RuntimeShape non_flatten_output_shape;\n    TransposeParams non_flatten_params;\n    const int total_size = shrinked_input_shape.FlatSize();\n    const int non_flatten_size = transpose_utils::Flatten(\n        shrinked_input_shape, shrinked_output_shape, shrinked_params,\n        &non_flatten_input_shape, &non_flatten_output_shape,\n        &non_flatten_params);\n    TFLITE_DCHECK_NE(non_flatten_params.perm[0], 0);\n\n    for (int i = 0; i < total_size; i += non_flatten_size) {\n      TransposeImpl<T, N>(non_flatten_params, non_flatten_input_shape,\n                          input_data + i, non_flatten_output_shape,\n                          output_data + i);\n    }\n    return;\n  }\n\n  // Call non-flattened case.\n  TransposeImpl<T, N>(shrinked_params, shrinked_input_shape, input_data,\n                      shrinked_output_shape, output_data);\n}\n\n// Assume input1 & input2 have the same scale & zero point.\ninline void MaximumElementwise(int size, const ArithmeticParams& params,\n                               const int8* input1_data, const int8* input2_data,\n                               int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaximumElementwiseInt8/8bit\");\n  int i = 0;\n#ifdef USE_NEON\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input1_val_original = vld1q_s8(input1_data + i);\n    const int8x16_t input2_val_original = vld1q_s8(input2_data + i);\n    const int8x16_t max_data =\n        vmaxq_s8(input1_val_original, input2_val_original);\n    vst1q_s8(output_data + i, max_data);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const int8 input1_val = input1_data[i];\n    const int8 input2_val = input2_data[i];\n    output_data[i] = std::max(input1_val, input2_val);\n  }\n}\n\ninline void MaximumScalarBroadcast(int size, const ArithmeticParams& params,\n                                   int8 input1_data, const int8* input2_data,\n                                   int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaximumScalarBroadcastInt8/8bit\");\n  int i = 0;\n\n#ifdef USE_NEON\n  const int8x16_t input1_val_original = vdupq_n_s8(input1_data);\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input2_val_original = vld1q_s8(input2_data + i);\n    const int8x16_t max_data =\n        vmaxq_s8(input1_val_original, input2_val_original);\n    vst1q_s8(output_data + i, max_data);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const int8 input2_val = input2_data[i];\n    output_data[i] = std::max(input1_data, input2_val);\n  }\n}\n\n// Assume input1 & input2 have the same scale & zero point.\ninline void MinimumElementwise(int size, const ArithmeticParams& params,\n                               const int8* input1_data, const int8* input2_data,\n                               int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MinimumElementwiseInt8/8bit\");\n  int i = 0;\n#ifdef USE_NEON\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input1_val_original = vld1q_s8(input1_data + i);\n    const int8x16_t input2_val_original = vld1q_s8(input2_data + i);\n    const int8x16_t min_data =\n        vminq_s8(input1_val_original, input2_val_original);\n    vst1q_s8(output_data + i, min_data);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const int8 input1_val = input1_data[i];\n    const int8 input2_val = input2_data[i];\n    output_data[i] = std::min(input1_val, input2_val);\n  }\n}\n\ninline void MinimumScalarBroadcast(int size, const ArithmeticParams& params,\n                                   int8 input1_data, const int8* input2_data,\n                                   int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MinimumScalarBroadcastInt8/8bit\");\n  int i = 0;\n\n#ifdef USE_NEON\n  const int8x16_t input1_val_original = vdupq_n_s8(input1_data);\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input2_val_original = vld1q_s8(input2_data + i);\n    const int8x16_t min_data =\n        vminq_s8(input1_val_original, input2_val_original);\n    vst1q_s8(output_data + i, min_data);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const int8 input2_val = input2_data[i];\n    output_data[i] = std::min(input1_data, input2_val);\n  }\n}\n\ntemplate <typename Op>\ninline void BroadcastMaximumDispatch(const ArithmeticParams& params,\n                                     const RuntimeShape& input1_shape,\n                                     const int8* input1_data,\n                                     const RuntimeShape& input2_shape,\n                                     const int8* input2_data,\n                                     const RuntimeShape& output_shape,\n                                     int8* output_data, Op op) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return reference_ops::MaximumMinimumBroadcastSlow(\n        input1_shape, input1_data, input2_shape, input2_data, output_shape,\n        output_data, op);\n  }\n\n  BinaryBroadcastFiveFold(params, input1_shape, input1_data, input2_shape,\n                          input2_data, output_shape, output_data,\n                          MaximumElementwise, MaximumScalarBroadcast);\n}\n\ntemplate <typename Op>\ninline void BroadcastMinimumDispatch(const ArithmeticParams& params,\n                                     const RuntimeShape& input1_shape,\n                                     const int8* input1_data,\n                                     const RuntimeShape& input2_shape,\n                                     const int8* input2_data,\n                                     const RuntimeShape& output_shape,\n                                     int8* output_data, Op op) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return reference_ops::MaximumMinimumBroadcastSlow(\n        input1_shape, input1_data, input2_shape, input2_data, output_shape,\n        output_data, op);\n  }\n\n  BinaryBroadcastFiveFold(params, input1_shape, input1_data, input2_shape,\n                          input2_data, output_shape, output_data,\n                          MinimumElementwise, MinimumScalarBroadcast);\n}\n\ntemplate <typename T>\nvoid CumsumImpl(const T* input_data, const RuntimeShape& shape, int axis,\n                bool exclusive, bool reverse, T* output_data) {\n  Eigen::array<Eigen::DenseIndex, 3> dims = {1, 1, 1};\n\n  for (int i = 0; i < axis; ++i) {\n    dims[0] *= shape.Dims(i);\n  }\n  dims[1] = shape.Dims(axis);\n  for (int i = axis + 1; i < shape.DimensionsCount(); ++i) {\n    dims[2] *= shape.Dims(i);\n  }\n\n  typedef Eigen::TensorMap<\n      Eigen::Tensor<const T, 3, Eigen::RowMajor, Eigen::DenseIndex>,\n      Eigen::Aligned>\n      ConstTensor;\n  typedef Eigen::TensorMap<\n      Eigen::Tensor<T, 3, Eigen::RowMajor, Eigen::DenseIndex>, Eigen::Aligned>\n      Tensor;\n  ConstTensor input(input_data, dims);\n  Tensor output(output_data, dims);\n\n  if (reverse) {\n    Eigen::array<bool, 3> reverse_idx = {false, true, false};\n    output =\n        input.reverse(reverse_idx).cumsum(1, exclusive).reverse(reverse_idx);\n  } else {\n    output = input.cumsum(1, exclusive);\n  }\n}\n\ntemplate <typename T>\nvoid CumSum(const T* input_data, const RuntimeShape& shape, int axis,\n            bool exclusive, bool reverse, T* output_data) {\n  const int dim = shape.DimensionsCount();\n  TFLITE_DCHECK_GE(dim, 1);\n  CumsumImpl<T>(input_data, shape, axis, exclusive, reverse, output_data);\n}\n\ninline void PReluScalarBroadcast(int size, const ArithmeticParams& params,\n                                 float alpha, const float* input_data,\n                                 float* output_data) {\n  ruy::profiler::ScopeLabel label(\"PreluScalarBroadcast/float\");\n  int i = 0;\n\n#ifdef USE_NEON\n  const float32x4_t zero_dup = vdupq_n_f32(0.0f);\n  const float32x4_t alpha_dup = vdupq_n_f32(alpha);\n  for (; i <= size - 16; i += 16) {\n    const float32x4_t input1 = vld1q_f32(input_data + i);\n    const float32x4_t input2 = vld1q_f32(input_data + i + 4);\n    const float32x4_t input3 = vld1q_f32(input_data + i + 8);\n    const float32x4_t input4 = vld1q_f32(input_data + i + 12);\n\n    const float32x4_t temp1 = vmulq_f32(input1, alpha_dup);\n    const float32x4_t temp2 = vmulq_f32(input2, alpha_dup);\n    const float32x4_t temp3 = vmulq_f32(input3, alpha_dup);\n    const float32x4_t temp4 = vmulq_f32(input4, alpha_dup);\n\n    const uint32x4_t mask1 = vcgeq_f32(input1, zero_dup);\n    const uint32x4_t mask2 = vcgeq_f32(input2, zero_dup);\n    const uint32x4_t mask3 = vcgeq_f32(input3, zero_dup);\n    const uint32x4_t mask4 = vcgeq_f32(input4, zero_dup);\n\n    const float32x4_t result1 = vbslq_f32(mask1, input1, temp1);\n    vst1q_f32(output_data + i, result1);\n    const float32x4_t result2 = vbslq_f32(mask2, input2, temp2);\n    vst1q_f32(output_data + i + 4, result2);\n    const float32x4_t result3 = vbslq_f32(mask3, input3, temp3);\n    vst1q_f32(output_data + i + 8, result3);\n    const float32x4_t result4 = vbslq_f32(mask4, input4, temp4);\n    vst1q_f32(output_data + i + 12, result4);\n  }\n\n  for (; i <= size - 4; i += 4) {\n    const float32x4_t input = vld1q_f32(input_data + i);\n    const float32x4_t temp = vmulq_f32(input, alpha_dup);\n    const uint32x4_t mask = vcgeq_f32(input, zero_dup);\n    const float32x4_t result = vbslq_f32(mask, input, temp);\n    vst1q_f32(output_data + i, result);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const float input = input_data[i];\n    output_data[i] = input >= 0.f ? input : input * alpha;\n  }\n}\n\ninline void PReluElementWise(int flat_size, const ArithmeticParams& params,\n                             const float* alpha_data, const float* input_data,\n                             float* output_data) {\n  ruy::profiler::ScopeLabel label(\"PreluElementWise/float\");\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t zero_dup = vdupq_n_f32(0.0f);\n  for (; i <= flat_size - 16; i += 16) {\n    const float32x4_t input1 = vld1q_f32(input_data + i);\n    const float32x4_t alpha1 = vld1q_f32(alpha_data + i);\n    const float32x4_t input2 = vld1q_f32(input_data + i + 4);\n    const float32x4_t alpha2 = vld1q_f32(alpha_data + i + 4);\n    const float32x4_t input3 = vld1q_f32(input_data + i + 8);\n    const float32x4_t alpha3 = vld1q_f32(alpha_data + i + 8);\n    const float32x4_t input4 = vld1q_f32(input_data + i + 12);\n    const float32x4_t alpha4 = vld1q_f32(alpha_data + i + 12);\n\n    const float32x4_t temp1 = vmulq_f32(input1, alpha1);\n    const float32x4_t temp2 = vmulq_f32(input2, alpha2);\n    const float32x4_t temp3 = vmulq_f32(input3, alpha3);\n    const float32x4_t temp4 = vmulq_f32(input4, alpha4);\n\n    const uint32x4_t mask1 = vcgeq_f32(input1, zero_dup);\n    const uint32x4_t mask2 = vcgeq_f32(input2, zero_dup);\n    const uint32x4_t mask3 = vcgeq_f32(input3, zero_dup);\n    const uint32x4_t mask4 = vcgeq_f32(input4, zero_dup);\n\n    const float32x4_t result1 = vbslq_f32(mask1, input1, temp1);\n    vst1q_f32(output_data + i, result1);\n    const float32x4_t result2 = vbslq_f32(mask2, input2, temp2);\n    vst1q_f32(output_data + i + 4, result2);\n    const float32x4_t result3 = vbslq_f32(mask3, input3, temp3);\n    vst1q_f32(output_data + i + 8, result3);\n    const float32x4_t result4 = vbslq_f32(mask4, input4, temp4);\n    vst1q_f32(output_data + i + 12, result4);\n  }\n\n  for (; i <= flat_size - 4; i += 4) {\n    const float32x4_t input = vld1q_f32(input_data + i);\n    const float32x4_t alpha = vld1q_f32(alpha_data + i);\n\n    const float32x4_t temp = vmulq_f32(input, alpha);\n    const uint32x4_t mask = vcgeq_f32(input, zero_dup);\n    const float32x4_t result = vbslq_f32(mask, input, temp);\n    vst1q_f32(output_data + i, result);\n  }\n#endif  // USE_NEON\n  for (; i < flat_size; ++i) {\n    const float input = input_data[i];\n    const float alpha = alpha_data[i];\n    output_data[i] = input >= 0.f ? input : input * alpha;\n  }\n}\n\ninline void BroadcastPReluDispatch(\n    const ArithmeticParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& alpha_shape,\n    const float* alpha_data, const RuntimeShape& output_shape,\n    float* output_data, float (*func)(float, float)) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return reference_ops::BroadcastBinaryFunction4DSlow<float, float, float>(\n        input_shape, input_data, alpha_shape, alpha_data, output_shape,\n        output_data, func);\n  }\n\n  BinaryBroadcastFiveFold(params, input_shape, input_data, alpha_shape,\n                          alpha_data, output_shape, output_data,\n                          PReluElementWise, PReluScalarBroadcast);\n}\n\n// Returns the index with minimum value within `input_data`.\n// If there is a tie, returns the smaller index.\ntemplate <typename T>\ninline int ArgMinVector(const T* input_data, int size) {\n  T min_value = input_data[0];\n  int min_index = 0;\n  for (int i = 1; i < size; ++i) {\n    const T curr_value = input_data[i];\n    if (curr_value < min_value) {\n      min_value = curr_value;\n      min_index = i;\n    }\n  }\n  return min_index;\n}\n\n// Returns the index with maximum value within `input_data`.\n// If there is a tie, returns the smaller index.\ntemplate <typename T>\ninline int ArgMaxVector(const T* input_data, int size) {\n  T max_value = input_data[0];\n  int max_index = 0;\n  for (int i = 1; i < size; ++i) {\n    const T curr_value = input_data[i];\n    if (curr_value > max_value) {\n      max_value = curr_value;\n      max_index = i;\n    }\n  }\n  return max_index;\n}\n\ntemplate <>\ninline int ArgMinVector(const float* input_data, int size) {\n  int32_t min_index = 0;\n  float min_value = input_data[0];\n  int32_t i = 1;\n#ifdef USE_NEON\n  if (size >= 4) {\n    float32x4_t min_value_f32x4 = vld1q_f32(input_data);\n    const int32_t index_init[4] = {0, 1, 2, 3};\n    int32x4_t min_index_s32x4 = vld1q_s32(index_init);\n    int32x4_t index_s32x4 = min_index_s32x4;\n    int32x4_t inc = vdupq_n_s32(4);\n    for (i = 4; i <= size - 4; i += 4) {\n      // Increase indices by 4.\n      index_s32x4 = vaddq_s32(index_s32x4, inc);\n      float32x4_t v = vld1q_f32(&input_data[i]);\n      uint32x4_t mask = vcltq_f32(v, min_value_f32x4);\n      min_value_f32x4 = vminq_f32(min_value_f32x4, v);\n      min_index_s32x4 = vbslq_s32(mask, index_s32x4, min_index_s32x4);\n    }\n    // Find min element within float32x4_t.\n#ifdef __aarch64__\n    min_value = vminvq_f32(min_value_f32x4);\n#else\n    float32x2_t min_value_f32x2 = vpmin_f32(vget_low_f32(min_value_f32x4),\n                                            vget_high_f32(min_value_f32x4));\n    min_value_f32x2 = vpmin_f32(min_value_f32x2, min_value_f32x2);\n    min_value = vget_lane_f32(min_value_f32x2, 0);\n#endif  // __aarch64__\n    // Mask indices of non-min values with max int32_t.\n    float32x4_t fill_min_value_f32x4 = vdupq_n_f32(min_value);\n    uint32x4_t mask = vceqq_f32(min_value_f32x4, fill_min_value_f32x4);\n    int32x4_t all_set = vdupq_n_s32(std::numeric_limits<int>::max());\n    min_index_s32x4 = vbslq_s32(mask, min_index_s32x4, all_set);\n    // Find min index of min values.\n#ifdef __aarch64__\n    min_index = vminvq_s32(min_index_s32x4);\n#else\n    int32x2_t min_index_s32x2 = vpmin_s32(vget_low_s32(min_index_s32x4),\n                                          vget_high_s32(min_index_s32x4));\n    min_index_s32x2 = vpmin_s32(min_index_s32x2, min_index_s32x2);\n    min_index = vget_lane_s32(min_index_s32x2, 0);\n#endif  // __aarch64__\n  }\n#endif  // USE_NEON\n  // Leftover loop.\n  for (; i < size; ++i) {\n    const float curr_value = input_data[i];\n    if (curr_value < min_value) {\n      min_value = curr_value;\n      min_index = i;\n    }\n  }\n  return min_index;\n}\n\ntemplate <>\ninline int ArgMaxVector(const float* input_data, int size) {\n  int32_t max_index = 0;\n  float max_value = input_data[0];\n  int32_t i = 1;\n#ifdef USE_NEON\n  if (size >= 4) {\n    float32x4_t max_value_f32x4 = vld1q_f32(input_data);\n    const int32_t index_init[4] = {0, 1, 2, 3};\n    int32x4_t max_index_s32x4 = vld1q_s32(index_init);\n    int32x4_t index_s32x4 = max_index_s32x4;\n    int32x4_t inc = vdupq_n_s32(4);\n    for (i = 4; i <= size - 4; i += 4) {\n      // Increase indices by 4.\n      index_s32x4 = vaddq_s32(index_s32x4, inc);\n      float32x4_t v = vld1q_f32(&input_data[i]);\n      uint32x4_t mask = vcgtq_f32(v, max_value_f32x4);\n      max_value_f32x4 = vmaxq_f32(max_value_f32x4, v);\n      max_index_s32x4 = vbslq_s32(mask, index_s32x4, max_index_s32x4);\n    }\n    // Find max element within float32x4_t.\n#ifdef __aarch64__\n    max_value = vmaxvq_f32(max_value_f32x4);\n#else\n    float32x2_t max_value_f32x2 = vpmax_f32(vget_low_f32(max_value_f32x4),\n                                            vget_high_f32(max_value_f32x4));\n    max_value_f32x2 = vpmax_f32(max_value_f32x2, max_value_f32x2);\n    max_value = vget_lane_f32(max_value_f32x2, 0);\n#endif  // __aarch64__\n    // Mask indices of non-max values with max int32_t.\n    float32x4_t fill_max_value_f32x4 = vdupq_n_f32(max_value);\n    uint32x4_t mask = vceqq_f32(max_value_f32x4, fill_max_value_f32x4);\n    int32x4_t all_set = vdupq_n_s32(std::numeric_limits<int>::max());\n    max_index_s32x4 = vbslq_s32(mask, max_index_s32x4, all_set);\n    // Find min index of max values.\n#ifdef __aarch64__\n    max_index = vminvq_s32(max_index_s32x4);\n#else\n    int32x2_t max_index_s32x2 = vpmin_s32(vget_low_s32(max_index_s32x4),\n                                          vget_high_s32(max_index_s32x4));\n    max_index_s32x2 = vpmin_s32(max_index_s32x2, max_index_s32x2);\n    max_index = vget_lane_s32(max_index_s32x2, 0);\n#endif  // __aarch64__\n  }\n#endif  // USE_NEON\n  // Leftover loop.\n  for (; i < size; ++i) {\n    const float curr_value = input_data[i];\n    if (curr_value > max_value) {\n      max_value = curr_value;\n      max_index = i;\n    }\n  }\n  return max_index;\n}\n\ntemplate <>\ninline int ArgMaxVector(const int8_t* input_data, int size) {\n  int32_t max_index = 0;\n  int8_t max_value = input_data[0];\n  int32_t i = 0;\n#ifdef USE_NEON\n  constexpr int VECTOR_SIZE = 16;\n  if (size >= VECTOR_SIZE) {\n    int8x16_t max_value_s8x16;\n    for (; i <= size - VECTOR_SIZE; i += VECTOR_SIZE) {\n      max_value_s8x16 = vld1q_s8(input_data + i);\n      int8_t max_from_vec;\n#ifdef __aarch64__\n      max_from_vec = vmaxvq_s8(max_value_s8x16);\n#else   // 32 bit\n      int8x8_t max_val_s8x8 =\n          vpmax_s8(vget_low_s8(max_value_s8x16), vget_high_s8(max_value_s8x16));\n      max_val_s8x8 = vpmax_s8(max_val_s8x8, max_val_s8x8);\n      max_val_s8x8 = vpmax_s8(max_val_s8x8, max_val_s8x8);\n      max_val_s8x8 = vpmax_s8(max_val_s8x8, max_val_s8x8);\n      max_from_vec = vget_lane_s8(max_val_s8x8, 0);\n#endif  // __aarch64__\n      if (max_from_vec > max_value) {\n        max_value = max_from_vec;\n        max_index = i;\n      }\n    }\n  }\n  for (int start_idx = max_index; start_idx < max_index + VECTOR_SIZE;\n       start_idx++) {\n    if (input_data[start_idx] == max_value) {\n      max_index = start_idx;\n      break;\n    }\n  }\n\n#endif  // USE_NEON\n  // Leftover loop.\n  for (; i < size; ++i) {\n    const int8_t curr_value = input_data[i];\n    if (curr_value > max_value) {\n      max_value = curr_value;\n      max_index = i;\n    }\n  }\n\n  return max_index;\n}\n\ntemplate <>\ninline int ArgMaxVector(const uint8_t* input_data, int size) {\n  int32_t max_index = 0;\n  uint8_t max_value = input_data[0];\n  int32_t i = 0;\n#ifdef USE_NEON\n  constexpr int VECTOR_SIZE = 16;\n  if (size >= VECTOR_SIZE) {\n    uint8x16_t max_value_u8x16;\n    for (; i <= size - VECTOR_SIZE; i += VECTOR_SIZE) {\n      max_value_u8x16 = vld1q_u8(input_data + i);\n      uint8_t max_from_vec;\n#ifdef __aarch64__\n      max_from_vec = vmaxvq_u8(max_value_u8x16);\n#else   // 32 bit\n      uint8x8_t max_val_u8x8 =\n          vpmax_u8(vget_low_u8(max_value_u8x16), vget_high_u8(max_value_u8x16));\n      max_val_u8x8 = vpmax_u8(max_val_u8x8, max_val_u8x8);\n      max_val_u8x8 = vpmax_u8(max_val_u8x8, max_val_u8x8);\n      max_val_u8x8 = vpmax_u8(max_val_u8x8, max_val_u8x8);\n      max_from_vec = vget_lane_u8(max_val_u8x8, 0);\n#endif  // __aarch64__\n      if (max_from_vec > max_value) {\n        max_value = max_from_vec;\n        max_index = i;\n      }\n    }\n  }\n  for (int start_idx = max_index; start_idx < max_index + VECTOR_SIZE;\n       start_idx++) {\n    if (input_data[start_idx] == max_value) {\n      max_index = start_idx;\n      break;\n    }\n  }\n\n#endif  // USE_NEON\n  // Leftover loop.\n  for (; i < size; ++i) {\n    const uint8_t curr_value = input_data[i];\n    if (curr_value > max_value) {\n      max_value = curr_value;\n      max_index = i;\n    }\n  }\n\n  return max_index;\n}\n\n// Specializes ArgMinMax function with axis=dims-1.\n// In this case, ArgMinMax reduction is applied on contiguous memory.\ntemplate <typename T1, typename T2, bool is_arg_max>\ninline void ArgMinMaxLastAxis(const RuntimeShape& input_shape,\n                              const T1* input_data,\n                              const RuntimeShape& output_shape,\n                              T2* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_EQ(input_shape.Dims(0), output_shape.Dims(0));\n\n  int outer_size = input_shape.Dims(0);\n  int axis_size = input_shape.Dims(1);\n  for (int outer = 0; outer < outer_size; ++outer) {\n    if (is_arg_max) {\n      output_data[outer] = static_cast<T2>(\n          ArgMaxVector<T1>(input_data + outer * axis_size, axis_size));\n    } else {\n      output_data[outer] = static_cast<T2>(\n          ArgMinVector<T1>(input_data + outer * axis_size, axis_size));\n    }\n  }\n}\n\ntemplate <typename T1, typename T2, typename T3>\ninline void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,\n                      const T3* input2_data, const RuntimeShape& output_shape,\n                      T2* output_data, const bool is_arg_max) {\n  ruy::profiler::ScopeLabel label(\"ArgMinMax\");\n\n  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);\n  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,\n                   output_shape.DimensionsCount());\n  int axis = input2_data[0];\n  if (axis < 0) {\n    axis += input1_shape.DimensionsCount();\n  }\n  const int axis_size = input1_shape.Dims(axis);\n\n  int outer_size = 1;\n  for (int i = 0; i < axis; ++i) {\n    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));\n    outer_size *= input1_shape.Dims(i);\n  }\n\n  int inner_size = 1;\n  const int dims_count = input1_shape.DimensionsCount();\n  for (int i = axis + 1; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));\n    inner_size *= input1_shape.Dims(i);\n  }\n\n  // Call specialized function when axis=dims-1. So far, only float32 is\n  // optimized so reroute to specialized function only when T1 is float32.\n  if (inner_size == 1 &&\n      (std::is_same<T1, float>::value || std::is_same<T1, int8_t>::value ||\n       std::is_same<T1, uint8_t>::value)) {\n    if (is_arg_max) {\n      ArgMinMaxLastAxis<T1, T2, /*is_arg_max=*/true>(\n          {outer_size, axis_size}, input1_data, {outer_size}, output_data);\n    } else {\n      ArgMinMaxLastAxis<T1, T2, /*is_arg_max=*/false>(\n          {outer_size, axis_size}, input1_data, {outer_size}, output_data);\n    }\n    return;\n  }\n\n  reference_ops::ArgMinMax(input1_shape, input1_data, input2_data, output_shape,\n                           output_data, is_arg_max);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const RuntimeShape& input1_shape, const T1* input1_data,\n            const T3* input2_data, const RuntimeShape& output_shape,\n            T2* output_data) {\n  ArgMinMax(input1_shape, input1_data, input2_data, output_shape, output_data,\n            /*is_arg_max=*/true);\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\n// For backward compatibility, reference_ops has ArgMax function.\ntemplate <typename T1, typename T2, typename T3>\ninline void ArgMax(const RuntimeShape& input1_shape, const T1* input1_data,\n                   const RuntimeShape& input2_shape, const T3* input2_data,\n                   const RuntimeShape& output_shape, T2* output_data) {\n  // Drop shape of second input: not needed.\n  ArgMax(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ninline void Conv3D(const Conv3DParams& params, const RuntimeShape& input_shape,\n                   const float* input_data, const RuntimeShape& filter_shape,\n                   const float* filter_data, const RuntimeShape& bias_shape,\n                   const float* bias_data, const RuntimeShape& output_shape,\n                   float* output_data, const RuntimeShape& im2col_shape,\n                   float* im2col_data,\n                   const RuntimeShape& transposed_filter_shape,\n                   float* transposed_filter_data,\n                   CpuBackendContext* cpu_backend_context) {\n  const int stride_depth = params.stride_depth;\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  const int dilation_depth_factor = params.dilation_depth;\n  const int dilation_height_factor = params.dilation_height;\n  const int dilation_width_factor = params.dilation_width;\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n\n  ruy::profiler::ScopeLabel label(\"Conv3D\");\n\n  // NB: the float 0.0f value is represented by all zero bytes.\n  const uint8 float_zero_byte = 0x00;\n  const float* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_depth = filter_shape.Dims(0);\n  const bool need_dilated_im2col = dilation_width_factor != 1 ||\n                                   dilation_height_factor != 1 ||\n                                   dilation_depth_factor != 1;\n  const bool need_im2col = stride_depth != 1 || stride_height != 1 ||\n                           stride_width != 1 || filter_depth != 1 ||\n                           filter_height != 1 || filter_width != 1;\n\n  if (need_dilated_im2col) {\n    DilatedIm2col3D(params, filter_depth, filter_height, filter_width,\n                    float_zero_byte, input_shape, input_data, im2col_shape,\n                    im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    Im2col3D(params, filter_depth, filter_height, filter_width, float_zero_byte,\n             input_shape, input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  // Transpose the filter tensor.\n  TransposeParams transpose_params;\n  transpose_params.perm_count = 5;\n  transpose_params.perm[0] = 4;\n  transpose_params.perm[1] = 0;\n  transpose_params.perm[2] = 1;\n  transpose_params.perm[3] = 2;\n  transpose_params.perm[4] = 3;\n  Transpose<float, 5>(transpose_params, filter_shape, filter_data,\n                      transposed_filter_shape, transposed_filter_data);\n\n  const int gemm_input_dims = gemm_input_shape->DimensionsCount();\n  int m = FlatSizeSkipDim(*gemm_input_shape, gemm_input_dims - 1);\n  int n = output_shape.Dims(4);\n  int k = gemm_input_shape->Dims(gemm_input_dims - 1);\n\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = n;\n  lhs_params.cols = k;\n  cpu_backend_gemm::MatrixParams<float> rhs_params;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.rows = k;\n  rhs_params.cols = m;\n  cpu_backend_gemm::MatrixParams<float> dst_params;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.rows = n;\n  dst_params.cols = m;\n  cpu_backend_gemm::GemmParams<float, float> gemm_params;\n  gemm_params.bias = bias_data;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  cpu_backend_gemm::Gemm(lhs_params, transposed_filter_data, rhs_params,\n                         gemm_input_data, dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\n// Returns in 'im_data' (assumed to be zero-initialized) image patch in storage\n// order (planes, height, width, channel), constructed from patches in\n// 'col_data', which is required to be in storage order (out_planes * out_height\n// * out_width, filter_planes, filter_height, filter_width, in_channel).\n//\n// This function is copied from tensorflow/core/kernels/conv_grad_ops_3d.cc\n// authored by Eugene Zhulenev(ezhulenev).\ntemplate <typename T>\nvoid Col2im(const T* col_data, const int channel, const int planes,\n            const int height, const int width, const int filter_p,\n            const int filter_h, const int filter_w, const int pad_pt,\n            const int pad_t, const int pad_l, const int pad_pb, const int pad_b,\n            const int pad_r, const int stride_p, const int stride_h,\n            const int stride_w, T* im_data) {\n  const int planes_col = (planes + pad_pt + pad_pb - filter_p) / stride_p + 1;\n  const int height_col = (height + pad_t + pad_b - filter_h) / stride_h + 1;\n  const int width_col = (width + pad_l + pad_r - filter_w) / stride_w + 1;\n  int p_pad = -pad_pt;\n  for (int p = 0; p < planes_col; ++p) {\n    int h_pad = -pad_t;\n    for (int h = 0; h < height_col; ++h) {\n      int w_pad = -pad_l;\n      for (int w = 0; w < width_col; ++w) {\n        T* im_patch_data =\n            im_data +\n            (p_pad * height * width + h_pad * width + w_pad) * channel;\n        for (int ip = p_pad; ip < p_pad + filter_p; ++ip) {\n          for (int ih = h_pad; ih < h_pad + filter_h; ++ih) {\n            for (int iw = w_pad; iw < w_pad + filter_w; ++iw) {\n              if (ip >= 0 && ip < planes && ih >= 0 && ih < height && iw >= 0 &&\n                  iw < width) {\n                for (int i = 0; i < channel; ++i) {\n                  im_patch_data[i] += col_data[i];\n                }\n              }\n              im_patch_data += channel;\n              col_data += channel;\n            }\n            // Jump over remaining number of channel.\n            im_patch_data += channel * (width - filter_w);\n          }\n          // Jump over remaining number of (channel * width).\n          im_patch_data += (channel * width) * (height - filter_h);\n        }\n        w_pad += stride_w;\n      }\n      h_pad += stride_h;\n    }\n    p_pad += stride_p;\n  }\n}\n\ntemplate <typename T>\nvoid BiasAdd3D(T* im_data, const T* bias_data, const RuntimeShape& input_shape,\n               float float_activation_min, float float_activation_max) {\n  if (bias_data) {\n    const int outer_size = input_shape.Dims(0) * input_shape.Dims(1) *\n                           input_shape.Dims(2) * input_shape.Dims(3);\n    const int num_channels = input_shape.Dims(4);\n    for (int n = 0; n < outer_size; ++n) {\n      for (int c = 0; c < num_channels; ++c) {\n        im_data[c] = ActivationFunctionWithMinMax(im_data[c] + bias_data[c],\n                                                  float_activation_min,\n                                                  float_activation_max);\n      }\n      im_data += num_channels;\n    }\n  } else {\n    const int flat_size = input_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      im_data[i] = ActivationFunctionWithMinMax(\n          im_data[i], float_activation_min, float_activation_max);\n    }\n  }\n}\n\ninline void Conv3DTranspose(\n    const Conv3DTransposeParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* const output_data, const RuntimeShape& col2im_shape,\n    float* col2im_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"Conv3DTranspose/float\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK(col2im_data);\n\n  const int batch_size = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_channel = MatchingDim(input_shape, 4, filter_shape, 4);\n  const int output_channel = MatchingDim(output_shape, 4, filter_shape, 3);\n  const int input_spatial_size =\n      input_shape.Dims(1) * input_shape.Dims(2) * input_shape.Dims(3);\n  const int output_spatial_size =\n      output_shape.Dims(1) * output_shape.Dims(2) * output_shape.Dims(3);\n\n  const int output_spatial_dim_1 = output_shape.Dims(1);\n  const int output_spatial_dim_2 = output_shape.Dims(2);\n  const int output_spatial_dim_3 = output_shape.Dims(3);\n  const int input_offset = input_spatial_size * input_channel;\n  const int output_offset = output_spatial_size * output_channel;\n\n  const int filter_spatial_dim_1 = filter_shape.Dims(0);\n  const int filter_spatial_dim_2 = filter_shape.Dims(1);\n  const int filter_spatial_dim_3 = filter_shape.Dims(2);\n\n  const int spatial_dim_1_padding_before = params.padding_values.depth;\n  const int spatial_dim_1_padding_after =\n      params.padding_values.height + params.padding_values.depth_offset;\n  const int spatial_dim_2_padding_before = params.padding_values.height;\n  const int spatial_dim_2_padding_after =\n      params.padding_values.height + params.padding_values.height_offset;\n  const int spatial_dim_3_padding_before = params.padding_values.width;\n  const int spatial_dim_3_padding_after =\n      params.padding_values.width + params.padding_values.width_offset;\n  const int spatial_dim_1_stride = params.stride_depth;\n  const int spatial_dim_2_stride = params.stride_height;\n  const int spatial_dim_3_stride = params.stride_width;\n  const int filter_total_size = filter_spatial_dim_1 * filter_spatial_dim_2 *\n                                filter_spatial_dim_3 * output_channel;\n\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = filter_total_size;\n  lhs_params.cols = input_channel;\n  float* output_data_p = output_data;\n  std::fill_n(output_data, output_offset * batch_size, 0.0f);\n  for (int i = 0; i < batch_size; ++i) {\n    cpu_backend_gemm::MatrixParams<float> rhs_params;\n    rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n    rhs_params.rows = input_channel;\n    rhs_params.cols = input_spatial_size;\n    cpu_backend_gemm::MatrixParams<float> dst_params;\n    dst_params.order = cpu_backend_gemm::Order::kColMajor;\n    dst_params.rows = filter_total_size;\n    dst_params.cols = input_spatial_size;\n    cpu_backend_gemm::GemmParams<float, float> gemm_params;\n    cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params,\n                           input_data + input_offset * i, dst_params,\n                           col2im_data, gemm_params, cpu_backend_context);\n\n    Col2im(col2im_data, output_channel, output_spatial_dim_1,\n           output_spatial_dim_2, output_spatial_dim_3, filter_spatial_dim_1,\n           filter_spatial_dim_2, filter_spatial_dim_3,\n           spatial_dim_1_padding_before, spatial_dim_2_padding_before,\n           spatial_dim_3_padding_before, spatial_dim_1_padding_after,\n           spatial_dim_2_padding_after, spatial_dim_3_padding_after,\n           spatial_dim_1_stride, spatial_dim_2_stride, spatial_dim_3_stride,\n           output_data_p);\n    output_data_p += output_offset;\n  }\n  output_data_p = output_data;\n  BiasAdd3D(output_data_p, bias_data, output_shape, params.float_activation_min,\n            params.float_activation_max);\n}\n\n}  // namespace optimized_ops\n}  // namespace tflite\n\n#if defined OPTIMIZED_OPS_H__IGNORE_DEPRECATED_DECLARATIONS\n#undef OPTIMIZED_OPS_H__IGNORE_DEPRECATED_DECLARATIONS\n#pragma GCC diagnostic pop\n#endif\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_OPTIMIZED_OPS_H_\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_POOLING_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_POOLING_H_\n\n#include <limits>\n#include \"tensorflow/lite/kernels/internal/common.h\"\n\nnamespace tflite {\nnamespace reference_integer_ops {\n\ninline void AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const int8_t* input_data,\n                        const RuntimeShape& output_shape, int8_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int32_t acc = 0;\n          int filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              acc +=\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              filter_count++;\n            }\n          }\n          // Round to the closest integer value.\n          acc = acc > 0 ? (acc + filter_count / 2) / filter_count\n                        : (acc - filter_count / 2) / filter_count;\n          acc = std::max(acc, params.quantized_activation_min);\n          acc = std::min(acc, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<int8_t>(acc);\n        }\n      }\n    }\n  }\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const int8_t* input_data, const RuntimeShape& output_shape,\n                    int8_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_GE(params.quantized_activation_min,\n                   std::numeric_limits<int8_t>::min());\n  TFLITE_DCHECK_LE(params.quantized_activation_max,\n                   std::numeric_limits<int8_t>::max());\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int8_t max = std::numeric_limits<int8_t>::lowest();\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              max = std::max(\n                  max,\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);\n            }\n          }\n          max = std::max<int8_t>(max, params.quantized_activation_min);\n          max = std::min<int8_t>(max, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<int8_t>(max);\n        }\n      }\n    }\n  }\n}\n\ninline void AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const int16_t* input_data,\n                        const RuntimeShape& output_shape,\n                        int16_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int32_t acc = 0;\n          int filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              acc +=\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              filter_count++;\n            }\n          }\n          // Round to the closest integer value.\n          acc = acc > 0 ? (acc + filter_count / 2) / filter_count\n                        : (acc - filter_count / 2) / filter_count;\n          acc = std::max(acc, params.quantized_activation_min);\n          acc = std::min(acc, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<int16_t>(acc);\n        }\n      }\n    }\n  }\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const int16_t* input_data, const RuntimeShape& output_shape,\n                    int16_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_GE(params.quantized_activation_min,\n                   std::numeric_limits<int16_t>::min());\n  TFLITE_DCHECK_LE(params.quantized_activation_max,\n                   std::numeric_limits<int16_t>::max());\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int16_t max = std::numeric_limits<int16_t>::lowest();\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              max = std::max(\n                  max,\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);\n            }\n          }\n          max = std::max<int16_t>(max, params.quantized_activation_min);\n          max = std::min<int16_t>(max, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<int16_t>(max);\n        }\n      }\n    }\n  }\n}\n\n}  // namespace reference_integer_ops\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_POOLING_H_\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEGACY_REFERENCE_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEGACY_REFERENCE_OPS_H_\n\n#include <stdint.h>\n#include <sys/types.h>\n\n#include \"public/gemmlowp.h\"\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/legacy_types.h\"\n#include \"tensorflow/lite/kernels/internal/reference/conv.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/reference/tanh.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\n\nnamespace reference_ops {\n\nstatic constexpr int kDepthwiseReverseShift = -1;\n\ninline void ShapeFromDims(const tflite::Dims<4>& dims, RuntimeShape* shape) {\n  shape->BuildFrom(\n      {dims.sizes[3], dims.sizes[2], dims.sizes[1], dims.sizes[0]});\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  DepthwiseConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                bias_data, DimsToShape(output_dims), output_data);\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, 1, 1, pad_width,\n                pad_height, depth_multiplier, output_activation_min,\n                output_activation_max, output_data, output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, float* output_data,\n                   const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, pad_width, pad_height,\n                depth_multiplier, output_activation_min, output_activation_max,\n                output_data, output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   float* output_data, const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n                    bias_dims, stride, stride, pad_width, pad_height,\n                    depth_multiplier, output_data, output_dims);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kDepthwiseReverseShift * output_shift;\n\n  DepthwiseConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                bias_data, DimsToShape(output_dims), output_data);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, 1, 1, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, int32 output_offset,\n                   int32 output_multiplier, int output_shift,\n                   int32 output_activation_min, int32 output_activation_max,\n                   uint8* output_data, const Dims<4>& output_dims) {\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   int32 output_offset, int32 output_multiplier,\n                   int output_shift, int32 output_activation_min,\n                   int32 output_activation_max, uint8* output_data,\n                   const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, input_offset, filter_data,\n                    filter_dims, filter_offset, bias_data, bias_dims, stride,\n                    stride, pad_width, pad_height, depth_multiplier,\n                    output_offset, output_multiplier, output_shift,\n                    output_activation_min, output_activation_max, output_data,\n                    output_dims);\n}\n\ninline void Conv(const float* input_data, const Dims<4>& input_dims,\n                 const float* filter_data, const Dims<4>& filter_dims,\n                 const float* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 float output_activation_min, float output_activation_max,\n                 float* output_data, const Dims<4>& output_dims,\n                 float* im2col_data, const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int dilation_width_factor,\n          int dilation_height_factor, int pad_width, int pad_height,\n          float* output_data, const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, dilation_width_factor,\n       dilation_height_factor, pad_width, pad_height, output_activation_min,\n       output_activation_max, output_data, output_dims, im2col_data,\n       im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, 1, 1, pad_width, pad_height,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  Conv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n           bias_dims, stride, stride, 1, 1, pad_width, pad_height, output_data,\n           output_dims, im2col_data, im2col_dims);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 int32 output_offset, int32 output_multiplier, int output_shift,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims,\n                 uint8* im2col_data, const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data, gemmlowp_context);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height, 1, 1,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const uint8* input_data, const Dims<4>& input_dims,\n          int32 input_offset, const uint8* filter_data,\n          const Dims<4>& filter_dims, int32 filter_offset,\n          const int32* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, int32 output_offset,\n          int32 output_multiplier, int output_shift,\n          int32 output_activation_min, int32 output_activation_max,\n          uint8* output_data, const Dims<4>& output_dims, uint8* im2col_data,\n          const Dims<4>& im2col_dims, gemmlowp::GemmContext* gemmlowp_context) {\n  Conv<Ac>(input_data, input_dims, input_offset, filter_data, filter_dims,\n           filter_offset, bias_data, bias_dims, stride, stride, pad_width,\n           pad_height, output_offset, output_multiplier, output_shift,\n           output_activation_min, output_activation_max, output_data,\n           output_dims, im2col_data, im2col_dims, gemmlowp_context);\n}\n\ninline void TransposeConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, float* output_data,\n                          const Dims<4>& output_dims, float* im2col_data,\n                          const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n\n  TransposeConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data,\n                /*bias_shape*/ RuntimeShape(), /*bias*/ nullptr,\n                DimsToShape(output_dims), output_data, DimsToShape(im2col_dims),\n                im2col_data);\n}\n\ninline void TransposeConv(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& output_shape,\n    float* output_data, const RuntimeShape& im2col_shape, float* im2col_data) {\n  TransposeConv(params, input_shape, input_data, filter_shape, filter_data,\n                /*bias_shape*/ RuntimeShape(), /*bias*/ nullptr, output_shape,\n                output_data, im2col_shape, im2col_data);\n}\n\ninline void FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                           const float* weights_data,\n                           const Dims<4>& weights_dims, const float* bias_data,\n                           const Dims<4>& bias_dims,\n                           float output_activation_min,\n                           float output_activation_max, float* output_data,\n                           const Dims<4>& output_dims) {\n  tflite::FullyConnectedParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(weights_dims), weights_data,\n                 DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                    const float* weights_data, const Dims<4>& weights_dims,\n                    const float* bias_data, const Dims<4>& bias_dims,\n                    float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  FullyConnected(input_data, input_dims, weights_data, weights_dims, bias_data,\n                 bias_dims, output_activation_min, output_activation_max,\n                 output_data, output_dims);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext*) {\n  FullyConnected(params, input_shape, input_data, filter_shape, filter_data,\n                 bias_shape, bias_data, output_shape, output_data);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, gemmlowp::GemmContext*) {\n  FullyConnected(params, input_shape, input_data, filter_shape, filter_data,\n                 bias_shape, bias_data, output_shape, output_data);\n}\n\ninline void FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                           int32 input_offset, const uint8* filter_data,\n                           const Dims<4>& filter_dims, int32 filter_offset,\n                           const int32* bias_data, const Dims<4>& bias_dims,\n                           int32 output_offset, int32 output_multiplier,\n                           int output_shift, int32 output_activation_min,\n                           int32 output_activation_max, uint8* output_data,\n                           const Dims<4>& output_dims,\n                           gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\ninline void FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                           int32 input_offset, const uint8* filter_data,\n                           const Dims<4>& filter_dims, int32 filter_offset,\n                           const int32* bias_data, const Dims<4>& bias_dims,\n                           int32 output_offset, int32 output_multiplier,\n                           int output_shift, int32 output_activation_min,\n                           int32 output_activation_max, int16* output_data,\n                           const Dims<4>& output_dims,\n                           gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\ninline void ShuffledFullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& weights_shape,\n    const uint8* shuffled_weights_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext*) {\n  ShuffledFullyConnected(params, input_shape, input_data, weights_shape,\n                         shuffled_weights_data, bias_shape, bias_data,\n                         output_shape, output_data,\n                         shuffled_input_workspace_data);\n}\n\ninline void ShuffledFullyConnected(\n    const uint8* input_data, const Dims<4>& input_dims,\n    const uint8* shuffled_weights_data, const Dims<4>& weights_dims,\n    const int32* bias_data, const Dims<4>& bias_dims, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    int16* output_data, const Dims<4>& output_dims,\n    uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  ShuffledFullyConnected(op_params, DimsToShape(input_dims), input_data,\n                         DimsToShape(weights_dims), shuffled_weights_data,\n                         DimsToShape(bias_dims), bias_data,\n                         DimsToShape(output_dims), output_data,\n                         shuffled_input_workspace_data, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_offset, const uint8* filter_data,\n                    const Dims<4>& filter_dims, int32 filter_offset,\n                    const int32* bias_data, const Dims<4>& bias_dims,\n                    int32 output_offset, int32 output_multiplier,\n                    int output_shift, int32 output_activation_min,\n                    int32 output_activation_max, uint8* output_data,\n                    const Dims<4>& output_dims,\n                    gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  FullyConnected(input_data, input_dims, input_offset, filter_data, filter_dims,\n                 filter_offset, bias_data, bias_dims, output_offset,\n                 output_multiplier, output_shift, output_activation_min,\n                 output_activation_max, output_data, output_dims,\n                 gemmlowp_context);\n}\n\ninline void LstmCell(const float* input_data, const Dims<4>& input_dims,\n                     const float* prev_activ_data,\n                     const Dims<4>& prev_activ_dims, const float* weights_data,\n                     const Dims<4>& weights_dims, const float* bias_data,\n                     const Dims<4>& bias_dims, const float* prev_state_data,\n                     const Dims<4>& prev_state_dims, float* output_state_data,\n                     const Dims<4>& output_state_dims, float* output_activ_data,\n                     const Dims<4>& output_activ_dims, float* concat_temp_data,\n                     const Dims<4>& concat_temp_dims, float* activ_temp_data,\n                     const Dims<4>& activ_temp_dims) {\n  tflite::LstmCellParams op_params;\n  // Float LSTM cell does not need parameters to be set: leave untouched.\n\n  LstmCell(op_params, DimsToShape(input_dims), input_data,\n           DimsToShape(prev_activ_dims), prev_activ_data,\n           DimsToShape(weights_dims), weights_data, DimsToShape(bias_dims),\n           bias_data, DimsToShape(prev_state_dims), prev_state_data,\n           DimsToShape(output_state_dims), output_state_data,\n           DimsToShape(output_activ_dims), output_activ_data,\n           DimsToShape(concat_temp_dims), concat_temp_data,\n           DimsToShape(activ_temp_dims), activ_temp_data);\n}\n\ntemplate <int StateIntegerBits>\nvoid LstmCell(const uint8* input_data_uint8, const Dims<4>& input_dims,\n              const uint8* prev_activ_data_uint8,\n              const Dims<4>& prev_activ_dims, const uint8* weights_data_uint8,\n              const Dims<4>& weights_dims, const int32* bias_data_int32,\n              const Dims<4>& bias_dims, const int16* prev_state_data_int16,\n              const Dims<4>& prev_state_dims, int16* output_state_data_int16,\n              const Dims<4>& output_state_dims, uint8* output_activ_data_uint8,\n              const Dims<4>& output_activ_dims, uint8* concat_temp_data_uint8,\n              const Dims<4>& concat_temp_dims, int16* activ_temp_data_int16,\n              const Dims<4>& activ_temp_dims, int32 weights_zero_point,\n              int32 accum_multiplier, int accum_shift,\n              gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::LstmCellParams op_params;\n  op_params.weights_zero_point = weights_zero_point;\n  op_params.accum_multiplier = accum_multiplier;\n  op_params.accum_shift = accum_shift;\n\n  LstmCell<StateIntegerBits>(\n      op_params, DimsToShape(input_dims), input_data_uint8,\n      DimsToShape(prev_activ_dims), prev_activ_data_uint8,\n      DimsToShape(weights_dims), weights_data_uint8, DimsToShape(bias_dims),\n      bias_data_int32, DimsToShape(prev_state_dims), prev_state_data_int16,\n      DimsToShape(output_state_dims), output_state_data_int16,\n      DimsToShape(output_activ_dims), output_activ_data_uint8,\n      DimsToShape(concat_temp_dims), concat_temp_data_uint8,\n      DimsToShape(activ_temp_dims), activ_temp_data_int16, gemmlowp_context);\n}\n\ntemplate <typename T>\nvoid BroadcastDiv(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastDivSlow(op_params, DimsToShape(input1_dims), input1_data,\n                   DimsToShape(input2_dims), input2_data,\n                   DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Div(const T* input1_data, const Dims<4>& input1_dims,\n                const T* input2_data, const Dims<4>& input2_dims,\n                T output_activation_min, T output_activation_max,\n                T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  Div(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac, typename Scalar>\ninline void Concatenation(int concat_dim, const Scalar* const* input_data,\n                          const Dims<4>* const* input_dims, int inputs_count,\n                          Scalar* output_data, const Dims<4>& output_dims) {\n  // For now we don't have a model with a Concatenation with fused activation.\n  TFLITE_DCHECK_EQ(Ac, FusedActivationFunctionType::kNone);\n\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::ConcatenationParams op_params;\n  op_params.axis = 3 - concat_dim;\n  op_params.inputs_count = inputs_count;\n\n  Concatenation(op_params, input_shapes_indirect.data(), input_data,\n                DimsToShape(output_dims), output_data);\n}\n\ninline void Concatenation(int concat_dim, const uint8* const* input_data,\n                          const Dims<4>* const* input_dims,\n                          const int32* input_zeropoint,\n                          const float* input_scale, int inputs_count,\n                          uint8* output_data, const Dims<4>& output_dims,\n                          const int32 output_zeropoint,\n                          const float output_scale) {\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::ConcatenationParams op_params;\n  op_params.axis = 3 - concat_dim;\n  op_params.input_zeropoint = input_zeropoint;\n  op_params.input_scale = input_scale;\n  op_params.inputs_count = inputs_count;\n  op_params.output_zeropoint = output_zeropoint;\n  op_params.output_scale = output_scale;\n\n  ConcatenationWithScaling(op_params, input_shapes_indirect.data(), input_data,\n                           DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac, typename Scalar>\nvoid DepthConcatenation(const Scalar* const* input_data,\n                        const Dims<4>* const* input_dims, int inputs_count,\n                        Scalar* output_data, const Dims<4>& output_dims) {\n  // For now we don't have a model with a Concatenation with fused activation.\n  TFLITE_DCHECK_EQ(Ac, FusedActivationFunctionType::kNone);\n\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::ConcatenationParams op_params;\n  op_params.inputs_count = inputs_count;\n\n  DepthConcatenation(op_params, input_shapes_indirect.data(), input_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename Scalar>\nvoid TensorFlowSplit(const Scalar* input_data, const Dims<4>& input_dims,\n                     int axis, int outputs_count, Scalar* const* output_data,\n                     const Dims<4>* const* output_dims) {\n  std::vector<RuntimeShape> output_shapes(outputs_count);\n  std::vector<const RuntimeShape*> output_shapes_indirect(outputs_count);\n  for (int i = 0; i < outputs_count; ++i) {\n    ShapeFromDims(*output_dims[i], &output_shapes[i]);\n    output_shapes_indirect[i] = &output_shapes[i];\n  }\n  tflite::SplitParams op_params;\n  op_params.axis = 3 - axis;\n  op_params.num_split = outputs_count;\n\n  Split(op_params, DimsToShape(input_dims), input_data,\n        output_shapes_indirect.data(), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac, typename Scalar>\nvoid TensorFlowSplit(const Scalar* input_data, const Dims<4>& input_dims,\n                     int outputs_count, Scalar* const* output_data,\n                     const Dims<4>* const* output_dims) {\n  TFLITE_DCHECK_GE(outputs_count, 1);\n  for (int i = 0; i < outputs_count; i++) {\n    /* batches = */ MatchingArraySize(*output_dims[i], 3, input_dims, 3);\n    /* height = */ MatchingArraySize(*output_dims[i], 2, input_dims, 2);\n    /* width = */ MatchingArraySize(*output_dims[i], 1, input_dims, 1);\n  }\n  // For now we don't have a model with a Split with fused activation.\n  TFLITE_DCHECK_EQ(Ac, FusedActivationFunctionType::kNone);\n\n  TensorFlowSplit(input_data, input_dims, /*axis=*/0, outputs_count,\n                  output_data, output_dims);\n}\n\ninline void Softmax(const float* input_data, const RuntimeShape& input_shape,\n                    float beta, float* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.beta = beta;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Softmax(const uint8* input_data, const RuntimeShape& input_shape,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_beta_multiplier;\n  params.input_left_shift = input_beta_left_shift;\n  params.diff_min = diff_min;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void LogSoftmax(const float* input_data, const RuntimeShape& input_shape,\n                       float* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  // No params currently used for float LogSoftmax.\n  LogSoftmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void LogSoftmax(const uint8* input_data, const RuntimeShape& input_shape,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  params.reverse_scaling_divisor = reverse_scaling_divisor;\n  params.reverse_scaling_right_shift = reverse_scaling_right_shift;\n  params.diff_min = diff_min;\n  LogSoftmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const LogisticParams& params,\n                     const RuntimeShape& input_shape, const uint8* input_data,\n                     const RuntimeShape& output_shape, uint8* output_data) {\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int input_left_shift = params.input_left_shift;\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    const uint8 input_val_u8 = input_data[i];\n    const int32 input_val_centered =\n        static_cast<int32>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered <= -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered >= input_range_radius) {\n      output_val = 255;\n    } else {\n      const int32 input_val_rescaled =\n          MultiplyByQuantizedMultiplierGreaterThanOne(\n              input_val_centered, input_multiplier, input_left_shift);\n      using FixedPoint4 = gemmlowp::FixedPoint<int32, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      // Convert from Q0.31 to Q23.8.\n      using gemmlowp::RoundingDivideByPOT;\n      int32 output_val_s32 = RoundingDivideByPOT(output_val_f0.raw(), 23);\n      if (output_val_s32 == 256) {\n        output_val_s32 = 255;\n      }\n      // Reinterpret as U0.8.\n      TFLITE_DCHECK_GE(output_val_s32, 0);\n      TFLITE_DCHECK_LE(output_val_s32, 255);\n      output_val = static_cast<uint8>(output_val_s32);\n    }\n    output_data[i] = output_val;\n  }\n}\n\ninline void Logistic(const uint8* input_data, const RuntimeShape& input_shape,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const RuntimeShape& output_shape) {\n  LogisticParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const RuntimeShape& input_shape, const int16* input_data,\n                     const RuntimeShape& output_shape, int16* output_data) {\n  LogisticParams params;\n  // No params currently needed by int16 Logistic.\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const uint8* input_data, const RuntimeShape& input_shape,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const int16* input_data, const RuntimeShape& input_shape,\n                 int input_left_shift, int16* output_data,\n                 const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Dequantize(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 zero_point, double scale, float* output_data,\n                       const Dims<4>& output_dims) {\n  tflite::DequantizationParams op_params;\n  op_params.zero_point = zero_point;\n  op_params.scale = scale;\n\n  Dequantize(op_params, DimsToShape(input_dims), input_data,\n             DimsToShape(output_dims), output_data);\n}\n\ninline void FakeQuant(const float* input_data, const Dims<4>& input_dims,\n                      float rmin, float rmax, int num_bits, float* output_data,\n                      const Dims<4>& output_dims) {\n  tflite::FakeQuantParams op_params;\n  op_params.num_bits = num_bits;\n  op_params.minmax.min = rmin;\n  op_params.minmax.max = rmax;\n\n  FakeQuant(op_params, DimsToShape(input_dims), input_data,\n            DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Gather(const T* input_data, const Dims<4>& input_dims,\n                   int input_rank, const int32* coords_data,\n                   const Dims<4>& coords_dims, T* output_data,\n                   const Dims<4>& output_dims) {\n  tflite::GatherParams op_params;\n  op_params.axis = 4 - input_rank;\n  op_params.batch_dims = 0;\n\n  Gather(op_params, DimsToShape(input_dims), input_data,\n         DimsToShape(coords_dims), coords_data, DimsToShape(output_dims),\n         output_data);\n}\n\ninline uint32 LegacyReverseBits32(uint32 n) {\n  n = ((n >> 1) & 0x55555555) | ((n & 0x55555555) << 1);\n  n = ((n >> 2) & 0x33333333) | ((n & 0x33333333) << 2);\n  n = ((n >> 4) & 0x0F0F0F0F) | ((n & 0x0F0F0F0F) << 4);\n  return (((n & 0xFF) << 24) | ((n & 0xFF00) << 8) | ((n & 0xFF0000) >> 8) |\n          ((n & 0xFF000000) >> 24));\n}\n\ninline void StridedSliceReverseIndices(tflite::StridedSliceParams* p) {\n  TFLITE_CHECK_EQ(p->start_indices_count, p->stop_indices_count);\n  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);\n\n  std::reverse(p->start_indices, p->start_indices + p->start_indices_count);\n  std::reverse(p->stop_indices, p->stop_indices + p->stop_indices_count);\n  std::reverse(p->strides, p->strides + p->strides_count);\n\n  p->begin_mask = LegacyReverseBits32(static_cast<uint32>(p->begin_mask)) >>\n                  (32 - p->start_indices_count);\n  p->ellipsis_mask =\n      LegacyReverseBits32(static_cast<uint32>(p->ellipsis_mask)) >>\n      (32 - p->start_indices_count);\n  p->end_mask = LegacyReverseBits32(static_cast<uint32>(p->end_mask)) >>\n                (32 - p->start_indices_count);\n  p->new_axis_mask =\n      LegacyReverseBits32(static_cast<uint32>(p->new_axis_mask)) >>\n      (32 - p->start_indices_count);\n  p->shrink_axis_mask =\n      LegacyReverseBits32(static_cast<uint32>(p->shrink_axis_mask)) >>\n      (32 - p->start_indices_count);\n}\n\ntemplate <typename T>\ninline void StridedSlice(const T* input_data, const Dims<4>& input_dims,\n                         int begin_mask, int end_mask, int shrink_axis_mask,\n                         const std::vector<int>& start_indices,\n                         const std::vector<int>& stop_indices,\n                         const std::vector<int>& strides, T* output_data,\n                         const Dims<4>& output_dims) {\n  TFLITE_DCHECK_EQ(start_indices.size(), 4);\n  auto op_params = strided_slice::BuildStridedSliceParams(\n      begin_mask, end_mask, shrink_axis_mask, start_indices, stop_indices,\n      strides);\n  StridedSliceReverseIndices(&op_params);\n\n  StridedSlice(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Mean(const T* input_data, const Dims<4>& input_dims,\n                 const std::vector<int>& reduction_indices, T* output_data,\n                 const Dims<4>& output_dims) {\n  tflite::MeanParams op_params;\n  op_params.axis_count = reduction_indices.size();\n  for (int i = 0; i < op_params.axis_count; ++i) {\n    op_params.axis[i] = reduction_indices[op_params.axis_count - 1 - i];\n  }\n\n  Mean(op_params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ntemplate <typename T>\nvoid Transpose(const T* input, const Dims<4>& input_dims, T* output,\n               const Dims<4>& output_dims, const int* permuted_axes) {\n  TransposeParams params;\n  params.perm_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    params.perm[i] = 3 - permuted_axes[3 - i];\n  }\n  Transpose(params, DimsToShape(input_dims), input, DimsToShape(output_dims),\n            output);\n}\n\ntemplate <typename T, ComparisonFn<T> F>\ninline void Comparison(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, const Dims<4>& input2_dims,\n                       bool* output_data, const Dims<4>& output_dims) {\n  ComparisonParams op_params;\n  // No parameters needed.\n  ComparisonImpl<T, F>(op_params, DimsToShape(input1_dims), input1_data,\n                       DimsToShape(input2_dims), input2_data,\n                       DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, ComparisonFn<int32> F>\ninline void Comparison(int left_shift, const T* input1_data,\n                       const Dims<4>& input1_dims, int32 input1_offset,\n                       int32 input1_multiplier, int input1_shift,\n                       const T* input2_data, const Dims<4>& input2_dims,\n                       int32 input2_offset, int32 input2_multiplier,\n                       int input2_shift, bool* output_data,\n                       const Dims<4>& output_dims) {\n  tflite::ComparisonParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input2_shift = kReverseShift * input2_shift;\n\n  ComparisonWithScaling<T, F>(op_params, DimsToShape(input1_dims), input1_data,\n                              DimsToShape(input2_dims), input2_data,\n                              DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, ComparisonFn<T> F>\ninline void BroadcastComparison(const T* input1_data,\n                                const Dims<4>& input1_dims,\n                                const T* input2_data,\n                                const Dims<4>& input2_dims, bool* output_data,\n                                const Dims<4>& output_dims) {\n  ComparisonParams op_params;\n  // No parameters needed.\n  BroadcastComparison4DSlowImpl<T, F>(op_params, DimsToShape(input1_dims),\n                                      input1_data, DimsToShape(input2_dims),\n                                      input2_data, DimsToShape(output_dims),\n                                      output_data);\n}\n\ntemplate <typename T, ComparisonFn<int32> F>\ninline void BroadcastComparison(int left_shift, const T* input1_data,\n                                const Dims<4>& input1_dims, int32 input1_offset,\n                                int32 input1_multiplier, int input1_shift,\n                                const T* input2_data,\n                                const Dims<4>& input2_dims, int32 input2_offset,\n                                int32 input2_multiplier, int input2_shift,\n                                bool* output_data, const Dims<4>& output_dims) {\n  ComparisonParams op_params;\n\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input2_shift = kReverseShift * input2_shift;\n\n  BroadcastComparison4DSlowWithScaling<T, F>(\n      op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\n#define TFLITE_LEGACY_COMPARISON_OP(name)                                     \\\n  template <typename T>                                                       \\\n  inline void name(const T* input1_data, const Dims<4>& input1_dims,          \\\n                   const T* input2_data, const Dims<4>& input2_dims,          \\\n                   bool* output_data, const Dims<4>& output_dims) {           \\\n    ruy::profiler::ScopeLabel label(#name);                                   \\\n    Comparison<T, name##Fn>(input1_data, input1_dims, input2_data,            \\\n                            input2_dims, output_data, output_dims);           \\\n  }                                                                           \\\n  template <typename T>                                                       \\\n  inline void name(                                                           \\\n      int left_shift, const T* input1_data, const Dims<4>& input1_dims,       \\\n      int32 input1_offset, int32 input1_multiplier, int input1_shift,         \\\n      const T* input2_data, const Dims<4>& input2_dims, int32 input2_offset,  \\\n      int32 input2_multiplier, int input2_shift, bool* output_data,           \\\n      const Dims<4>& output_dims) {                                           \\\n    ruy::profiler::ScopeLabel label(#name \"/8bit\");                           \\\n    Comparison<T, name##Fn>(left_shift, input1_data, input1_dims,             \\\n                            input1_offset, input1_multiplier, input1_shift,   \\\n                            input2_data, input2_dims, input2_offset,          \\\n                            input2_multiplier, input2_shift, output_data,     \\\n                            output_dims);                                     \\\n  }                                                                           \\\n  template <typename T>                                                       \\\n  inline void Broadcast##name(                                                \\\n      const T* input1_data, const Dims<4>& input1_dims, const T* input2_data, \\\n      const Dims<4>& input2_dims, bool* output_data,                          \\\n      const Dims<4>& output_dims) {                                           \\\n    ruy::profiler::ScopeLabel label(\"Broadcast\" #name);                       \\\n    BroadcastComparison<T, name##Fn>(input1_data, input1_dims, input2_data,   \\\n                                     input2_dims, output_data, output_dims);  \\\n  }                                                                           \\\n  template <typename T>                                                       \\\n  inline void Broadcast##name(                                                \\\n      int left_shift, const T* input1_data, const Dims<4>& input1_dims,       \\\n      int32 input1_offset, int32 input1_multiplier, int input1_shift,         \\\n      const T* input2_data, const Dims<4>& input2_dims, int32 input2_offset,  \\\n      int32 input2_multiplier, int input2_shift, bool* output_data,           \\\n      const Dims<4>& output_dims) {                                           \\\n    ruy::profiler::ScopeLabel label(\"Broadcast\" #name \"/8bit\");               \\\n    BroadcastComparison<T, name##Fn>(left_shift, input1_data, input1_dims,    \\\n                                     input1_offset, input1_multiplier,        \\\n                                     input1_shift, input2_data, input2_dims,  \\\n                                     input2_offset, input2_multiplier,        \\\n                                     input2_shift, output_data, output_dims); \\\n  }\nTFLITE_LEGACY_COMPARISON_OP(Equal);\nTFLITE_LEGACY_COMPARISON_OP(NotEqual);\nTFLITE_LEGACY_COMPARISON_OP(Greater);\nTFLITE_LEGACY_COMPARISON_OP(GreaterEqual);\nTFLITE_LEGACY_COMPARISON_OP(Less);\nTFLITE_LEGACY_COMPARISON_OP(LessEqual);\n#undef TFLITE_LEGACY_COMPARISON_OP\n\ntemplate <typename D, typename T>\ninline void Select(const D* input_condition_data,\n                   const Dims<4>& input_condition_dims, const T* input_x_data,\n                   const Dims<4>& input_x_dims, const T* input_y_data,\n                   const Dims<4>& input_y_dims, T* output_data,\n                   const Dims<4>& output_dims) {\n  Select(DimsToShape(input_condition_dims), input_condition_data,\n         DimsToShape(input_x_dims), input_x_data, DimsToShape(input_y_dims),\n         input_y_data, DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename D, typename T>\ninline void RankOneSelect(const D* input_condition_data,\n                          const Dims<4>& input_condition_dims,\n                          const T* input_x_data, const Dims<4>& input_x_dims,\n                          const T* input_y_data, const Dims<4>& input_y_dims,\n                          T* output_data, const Dims<4>& output_dims) {\n  RankOneSelect(DimsToShape(input_condition_dims), input_condition_data,\n                DimsToShape(input_x_dims), input_x_data,\n                DimsToShape(input_y_dims), input_y_data,\n                DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, typename TI>\ninline void SparseToDense(const std::vector<std::vector<TI>>& indices,\n                          const T* values, T default_value, T* output_data,\n                          const Dims<4>& output_dims, bool value_is_scalar) {\n  SparseToDense(indices, values, default_value, value_is_scalar,\n                DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename Scalar>\nvoid Pack(int dim, const Scalar* const* input_data,\n          const Dims<4>* const* input_dims, int inputs_count,\n          Scalar* output_data, const Dims<4>& output_dims) {\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::PackParams op_params;\n  op_params.axis = 3 - dim;\n  op_params.inputs_count = inputs_count;\n\n  Pack(op_params, input_shapes_indirect.data(), input_data,\n       DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename Scalar>\nvoid Unpack(int axis, const Scalar* input_data, const Dims<4>& input_dims,\n            int dimensions, int outputs_count, Scalar* const* output_datas,\n            const Dims<4>& output_dims) {\n  tflite::UnpackParams op_params;\n  op_params.axis = 3 - axis;\n  op_params.num_split = outputs_count;\n\n  Unpack(op_params, DimsToShape(input_dims), input_data,\n         DimsToShape(output_dims), output_datas);\n}\n\ntemplate <typename Scalar>\nvoid Pack(int dim, const Scalar* const* input_data,\n          const Dims<4>* const* input_dims, const int32* input_zeropoint,\n          const float* input_scale, int inputs_count, Scalar* output_data,\n          const Dims<4>& output_dims, const int32 output_zeropoint,\n          const float output_scale) {\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::PackParams op_params;\n  op_params.axis = 3 - dim;\n  op_params.input_zeropoint = input_zeropoint;\n  op_params.input_scale = input_scale;\n  op_params.inputs_count = inputs_count;\n  op_params.output_zeropoint = output_zeropoint;\n  op_params.output_scale = output_scale;\n\n  PackWithScaling(op_params, input_shapes_indirect.data(), input_data,\n                  DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const RuntimeShape& input_shape,\n                     float* output_data, const RuntimeShape& output_shape) {\n  static_assert(Ac == FusedActivationFunctionType::kNone, \"\");\n  tflite::L2NormalizationParams op_params;\n  // No params need to be set for float.\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ninline void L2Normalization(const uint8* input_data,\n                            const RuntimeShape& input_shape,\n                            int32 input_zero_point, uint8* output_data,\n                            const RuntimeShape& output_shape) {\n  tflite::L2NormalizationParams op_params;\n  op_params.input_zero_point = input_zero_point;\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  L2Normalization<Ac>(input_data, DimsToShape(input_dims), output_data,\n                      DimsToShape(output_dims));\n}\n\ninline void L2Normalization(const uint8* input_data, const Dims<4>& input_dims,\n                            int32 input_zero_point, uint8* output_data,\n                            const Dims<4>& output_dims) {\n  L2Normalization(input_data, DimsToShape(input_dims), input_zero_point,\n                  output_data, DimsToShape(output_dims));\n}\n\ninline void Relu(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Relu(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Relu1(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Relu1(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ninline void Relu6(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Relu6(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ninline void ReluX(uint8 min_value, uint8 max_value, const uint8* input_data,\n                  const RuntimeShape& input_shape, uint8* output_data,\n                  const RuntimeShape& output_shape) {\n  tflite::ActivationParams params;\n  params.quantized_activation_max = max_value;\n  params.quantized_activation_min = min_value;\n  ReluX(params, input_shape, input_data, output_shape, output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(int left_shift, const uint8* input1_data,\n                const Dims<4>& input1_dims, int32 input1_offset,\n                int32 input1_multiplier, int input1_shift,\n                const uint8* input2_data, const Dims<4>& input2_dims,\n                int32 input2_offset, int32 input2_multiplier, int input2_shift,\n                int32 output_offset, int32 output_multiplier, int output_shift,\n                int32 output_activation_min, int32 output_activation_max,\n                uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const int32* input1_data, const Dims<4>& input1_dims,\n         const int32* input2_data, const Dims<4>& input2_dims,\n         int32* output_data, const Dims<4>& output_dims) {\n  ruy::profiler::ScopeLabel label(\"Add/int32\");\n  TFLITE_DCHECK(Ac == FusedActivationFunctionType::kNone);\n\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = std::numeric_limits<int32>::min();\n  op_params.quantized_activation_max = std::numeric_limits<int32>::max();\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAdd(int left_shift, const uint8* input1_data,\n                         const Dims<4>& input1_dims, int32 input1_offset,\n                         int32 input1_multiplier, int input1_shift,\n                         const uint8* input2_data, const Dims<4>& input2_dims,\n                         int32 input2_offset, int32 input2_multiplier,\n                         int input2_shift, int32 output_offset,\n                         int32 output_multiplier, int output_shift,\n                         int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAddFivefold(\n    int y0, int y1, int y2, int y3, int y4, int left_shift,\n    const uint8* input1_data, const Dims<4>& input1_dims, int32 input1_offset,\n    int32 input1_multiplier, int input1_shift, const uint8* input2_data,\n    const Dims<4>& input2_dims, int32 input2_offset, int32 input2_multiplier,\n    int input2_shift, int32 output_offset, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  tflite::ArithmeticParams op_params;\n  op_params.broadcast_category =\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.broadcast_shape[4] = y0;\n  op_params.broadcast_shape[3] = y1;\n  op_params.broadcast_shape[2] = y2;\n  op_params.broadcast_shape[1] = y3;\n  op_params.broadcast_shape[0] = y4;\n  BroadcastAddFivefold(op_params, DimsToShape(input1_dims), input1_data,\n                       DimsToShape(input2_dims), input2_data,\n                       DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T* output_data, const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  BroadcastAdd(input1_data, input1_dims, input2_data, input2_dims,\n               output_activation_min, output_activation_max, output_data,\n               output_dims);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(const int16* input1_data, const Dims<4>& input1_dims,\n                int input1_shift, const int16* input2_data,\n                const Dims<4>& input2_dims, int input2_shift,\n                int16 output_activation_min, int16 output_activation_max,\n                int16* output_data, const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, -32768);\n    TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Sub(const float* input1_data, const Dims<4>& input1_dims,\n                const float* input2_data, const Dims<4>& input2_dims,\n                float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(FusedActivationFunctionType::kNone,\n                      &output_activation_min, &output_activation_max);\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid Sub(const T* input1_data, const Dims<4>& input1_dims, const T* input2_data,\n         const Dims<4>& input2_dims, T* output_data,\n         const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = std::numeric_limits<T>::min();\n  op_params.quantized_activation_max = std::numeric_limits<T>::max();\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int kwidth, int kheight,\n                        float output_activation_min,\n                        float output_activation_max, float* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  AveragePool(params, DimsToShape(input_dims), input_data,\n              DimsToShape(output_dims), output_data);\n}\n\n// Transitional version that will be moved shortly to legacy_reference_ops, as\n// part of RuntimeShape revisions.\ninline void BroadcastMul4DSlow(const uint8* input1_data,\n                               const Dims<4>& input1_dims, int32 input1_offset,\n                               const uint8* input2_data,\n                               const Dims<4>& input2_dims, int32 input2_offset,\n                               int32 output_offset, int32 output_multiplier,\n                               int output_shift, int32 output_activation_min,\n                               int32 output_activation_max, uint8* output_data,\n                               const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n  op_params.input1_offset = input1_offset;\n  op_params.input2_offset = input2_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = output_shift;\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  BroadcastMul4DSlow(\n      input1_data, input1_dims, input1_offset, input2_data, input2_dims,\n      input2_offset, output_offset, output_multiplier,\n      //\n      kReverseShift * output_shift,\n      //\n      output_activation_min, output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  BroadcastMul(input1_data, input1_dims, input1_offset, input2_data,\n               input2_dims, input2_offset, output_offset, output_multiplier,\n               output_shift, output_activation_min, output_activation_max,\n               output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const float* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int kwidth, int kheight, float* output_data,\n                 const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n              pad_height, kwidth, kheight, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, float* output_data,\n                 const Dims<4>& output_dims) {\n  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n                  filter_width, filter_height, output_data, output_dims);\n}\n\ninline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int filter_width, int filter_height,\n                        int32 output_activation_min,\n                        int32 output_activation_max, uint8* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  AveragePool(params, DimsToShape(input_dims), input_data,\n              DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int filter_width, int filter_height,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n              pad_height, filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims) {\n  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n                  filter_width, filter_height, output_activation_min,\n                  output_activation_max, output_data, output_dims);\n}\n\ninline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int kwidth, int kheight,\n                    float output_activation_min, float output_activation_max,\n                    float* output_data, const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int kwidth, int kheight, float* output_data,\n             const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, kwidth, kheight, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             float* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_data, output_dims);\n}\n\ninline void MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int filter_width, int filter_height,\n                    int32 output_activation_min, int32 output_activation_max,\n                    uint8* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int filter_width, int filter_height, int32 output_activation_min,\n             int32 output_activation_max, uint8* output_data,\n             const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, filter_width, filter_height, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             int32 output_activation_min, int32 output_activation_max,\n             uint8* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\ninline void L2Pool(const float* input_data, const Dims<4>& input_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int filter_width, int filter_height,\n                   float output_activation_min, float output_activation_max,\n                   float* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  L2Pool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n         output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims,\n            int stride_width, int stride_height, int pad_width, int pad_height,\n            int filter_width, int filter_height, float* output_data,\n            const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  L2Pool(input_data, input_dims, stride_width, stride_height, pad_width,\n         pad_height, filter_width, filter_height, output_activation_min,\n         output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int filter_width, int filter_height,\n            float* output_data, const Dims<4>& output_dims) {\n  L2Pool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n             filter_width, filter_height, output_data, output_dims);\n}\n\ninline void Softmax(const float* input_data, const Dims<4>& input_dims,\n                    float beta, float* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), beta, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void Softmax(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), input_beta_multiplier,\n          input_beta_left_shift, diff_min, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const float* input_data, const Dims<4>& input_dims,\n                       float* output_data, const Dims<4>& output_dims) {\n  LogSoftmax(input_data, DimsToShape(input_dims), output_data,\n             DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const Dims<4>& output_dims) {\n  LogSoftmax(input_data, DimsToShape(input_dims), input_multiplier,\n             input_left_shift, reverse_scaling_divisor,\n             reverse_scaling_right_shift, diff_min, output_data,\n             DimsToShape(output_dims));\n}\n\ninline void Logistic(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  Logistic(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n           output_data);\n}\n\ninline void Logistic(const uint8* input_data, const Dims<4>& input_dims,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const Dims<4>& output_dims) {\n  Logistic(input_data, DimsToShape(input_dims), input_zero_point,\n           input_range_radius, input_multiplier, input_left_shift, output_data,\n           DimsToShape(output_dims));\n}\n\ninline void Logistic(const int16* input_data, const Dims<4>& input_dims,\n                     int16* output_data, const Dims<4>& output_dims) {\n  Logistic(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n           output_data);\n}\n\ninline void Tanh(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Tanh(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Tanh(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_zero_point,\n       input_range_radius, input_multiplier, input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ninline void Tanh(const int16* input_data, const Dims<4>& input_dims,\n                 int input_left_shift, int16* output_data,\n                 const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ntemplate <typename T>\ninline void DepthToSpace(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::DepthToSpaceParams op_params;\n  op_params.block_size = block_size;\n\n  DepthToSpace(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void SpaceToDepth(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::SpaceToDepthParams op_params;\n  op_params.block_size = block_size;\n\n  SpaceToDepth(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Mul(const T* input1_data, const Dims<4>& input1_dims,\n                const T* input2_data, const Dims<4>& input2_dims,\n                T output_activation_min, T output_activation_max,\n                T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Mul(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastMul(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid BroadcastMul(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T* output_data, const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int16* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  // No params in this version.\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int32 output_offset, int32 output_activation_min,\n                int32 output_activation_max, uint8* output_data,\n                const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.output_offset = output_offset;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void LocalResponseNormalization(const float* input_data,\n                                       const Dims<4>& input_dims, int range,\n                                       float bias, float alpha, float beta,\n                                       float* output_data,\n                                       const Dims<4>& output_dims) {\n  tflite::LocalResponseNormalizationParams op_params;\n  op_params.range = range;\n  op_params.bias = bias;\n  op_params.alpha = alpha;\n  op_params.beta = beta;\n\n  LocalResponseNormalization(op_params, DimsToShape(input_dims), input_data,\n                             DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename SrcT, typename DstT>\nvoid Cast(const SrcT* input_data, const Dims<4>& input_dims, DstT* output_data,\n          const Dims<4>& output_dims) {\n  Cast(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Floor(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Floor(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ntemplate <typename T>\ninline void ResizeBilinear(const T* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, T* output_data,\n                           const Dims<4>& output_dims, bool align_corners) {\n  tflite::ResizeBilinearParams op_params;\n  op_params.align_corners = align_corners;\n  op_params.half_pixel_centers = false;\n  ResizeBilinear(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(output_size_dims), output_size_data,\n                 DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ninline void ResizeBilinear(const float* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, float* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear<float>(input_data, input_dims, output_size_data,\n                        output_size_dims, output_data, output_dims,\n                        /*align_corners=*/false);\n}\n\ninline void ResizeBilinear(const uint8* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, uint8* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear<uint8>(input_data, input_dims, output_size_data,\n                        output_size_dims, output_data, output_dims,\n                        /*align_corners=*/false);\n}\n\ntemplate <typename T>\ninline void SpaceToBatchND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* paddings_data,\n                           const Dims<4>& paddings_dims, T* output_data,\n                           const Dims<4>& output_dims,\n                           const int32_t pad_value) {\n  tflite::SpaceToBatchParams op_params;\n  op_params.output_offset = pad_value;\n\n  SpaceToBatchND(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(paddings_dims), paddings_data,\n                 DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void SpaceToBatchND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* paddings_data,\n                           const Dims<4>& paddings_dims, T* output_data,\n                           const Dims<4>& output_dims) {\n  tflite::SpaceToBatchParams op_params;\n  op_params.output_offset = 0;\n\n  SpaceToBatchND(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(paddings_dims), paddings_data,\n                 DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void BatchToSpaceND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* crops_data, const Dims<4>& crops_dims,\n                           T* output_data, const Dims<4>& output_dims) {\n  BatchToSpaceND(DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(crops_dims), crops_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// Legacy signature, function covered both Pad and PadV2.\ntemplate <typename T>\ninline void PadV2(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& left_paddings,\n                  const std::vector<int>& right_paddings, T* output_data,\n                  const Dims<4>& output_dims, const T pad_value) {\n  TFLITE_DCHECK_EQ(left_paddings.size(), 4);\n  TFLITE_DCHECK_EQ(right_paddings.size(), 4);\n  tflite::PadParams op_params;\n  op_params.left_padding_count = 4;\n  op_params.right_padding_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.left_padding[i] = left_paddings[3 - i];\n    op_params.right_padding[i] = right_paddings[3 - i];\n  }\n  // SetFloatOrInt(pad_value, &op_params.pad_value);\n  const T pad_value_copy = pad_value;\n\n  Pad(op_params, DimsToShape(input_dims), input_data, &pad_value_copy,\n      DimsToShape(output_dims), output_data);\n}\n\n// Old Pad that calls legacy PadV2.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims, const int32_t pad_value) {\n  const T converted_pad_value = static_cast<T>(pad_value);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, converted_pad_value);\n}\n\n// Old Pad that only padded with 0.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims) {\n  const T pad_value = static_cast<T>(0);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, pad_value);\n}\n\ntemplate <typename T>\nvoid TensorFlowMinimum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Minimum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid TensorFlowMaximum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Maximum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, typename Op>\nvoid TensorFlowMaximumMinimum(const T* input1_data, const Dims<4>& input1_dims,\n                              const T* input2_data, const Dims<4>& input2_dims,\n                              T* output_data, const Dims<4>& output_dims,\n                              Op op) {\n  MaximumMinimumBroadcastSlow(DimsToShape(input1_dims), input1_data,\n                              DimsToShape(input2_dims), input2_data,\n                              DimsToShape(output_dims), output_data, op);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const T3* axis, const T1* input_data,\n            const tflite::Dims<4>& input_dims, T2* output_data,\n            const tflite::Dims<4>& output_dims) {\n  // Assumes the input always has 4 dimensions, and therefore,\n  // output always has three dimensions.\n  auto output_shape = RuntimeShape(\n      {output_dims.sizes[2], output_dims.sizes[1], output_dims.sizes[0]});\n  // Another way to interpret this is that output_dims.sizes[4] is always 1.\n  TFLITE_DCHECK_EQ(output_shape.FlatSize(),\n                   DimsToShape(output_dims).FlatSize());\n  // Legacy path only supported this.\n  TFLITE_DCHECK_EQ(axis[0], 3);\n  ArgMinMax(DimsToShape(input_dims), input_data, axis, output_shape,\n            output_data, std::greater<T1>());\n}\n\ntemplate <typename T1, typename T2, typename T3, typename Cmp>\nvoid ArgMinMax(const T3* axis, const T1* input_data, const Dims<4>& input_dims,\n               T2* output_data, const Dims<4>& output_dims, const Cmp& cmp) {\n  ArgMinMax(axis, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n            output_data, cmp);\n}\n\ntemplate <typename T>\ninline void Pow(const T* input1_data, const Dims<4>& input1_dims,\n                const T* input2_data, const Dims<4>& input2_dims,\n                T* output_data, const Dims<4>& output_dims) {\n  Pow(DimsToShape(input1_dims), input1_data, DimsToShape(input2_dims),\n      input2_data, DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void BroadcastPow(const T* input1_data, const Dims<4>& input1_dims,\n                         const T* input2_data, const Dims<4>& input2_dims,\n                         T* output_data, const Dims<4>& output_dims) {\n  BroadcastPow4DSlow(DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// R: Result type. T1: Input 1 type. T2: Input 2 type.\ntemplate <typename R, typename T1, typename T2>\ninline void BroadcastBinaryFunction(const T1* input1_data,\n                                    const Dims<4>& input1_dims,\n                                    const T2* input2_data,\n                                    const Dims<4>& input2_dims, R* output_data,\n                                    const Dims<4>& output_dims,\n                                    R (*func)(T1, T2)) {\n  BroadcastBinaryFunction(DimsToShape(input1_dims), input1_data,\n                          DimsToShape(input2_dims), input2_data,\n                          DimsToShape(output_dims), output_data, func);\n}\n\n// R: Result type. T1: Input 1 type. T2: Input 2 type.\ntemplate <typename R, typename T1, typename T2>\ninline void BinaryFunction(const T1* input1_data, const Dims<4>& input1_dims,\n                           const T2* input2_data, const Dims<4>& input2_dims,\n                           R* output_data, const Dims<4>& output_dims,\n                           R (*func)(T1, T2)) {\n  BinaryFunction(DimsToShape(input1_dims), input1_data,\n                 DimsToShape(input2_dims), input2_data,\n                 DimsToShape(output_dims), output_data, func);\n}\n\ntemplate <typename T>\ninline void Slice(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& begin, const std::vector<int>& size,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::SliceParams op_params;\n  op_params.begin_count = 4;\n  op_params.size_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.begin[i] = begin[3 - i];\n    op_params.size[i] = size[3 - i];\n  }\n\n  Slice(op_params, DimsToShape(input_dims), input_data,\n        DimsToShape(output_dims), output_data);\n}\n\n}  // namespace reference_ops\n}  // namespace tflite\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEGACY_REFERENCE_OPS_H_\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING_H_\n\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/cppmath.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\nnamespace reference_ops {\n\ninline void AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const float* input_data,\n                        const RuntimeShape& output_shape, float* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          float total = 0.f;\n          float filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              total +=\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              filter_count++;\n            }\n          }\n          const float average = total / filter_count;\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              ActivationFunctionWithMinMax(average, params.float_activation_min,\n                                           params.float_activation_max);\n        }\n      }\n    }\n  }\n}\n\ninline void AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const uint8_t* input_data,\n                        const RuntimeShape& output_shape,\n                        uint8_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int32_t acc = 0;\n          int filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              acc +=\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              filter_count++;\n            }\n          }\n          acc = (acc + filter_count / 2) / filter_count;\n          acc = std::max(acc, params.quantized_activation_min);\n          acc = std::min(acc, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<uint8_t>(acc);\n        }\n      }\n    }\n  }\n}\n\ninline void L2Pool(const PoolParams& params, const RuntimeShape& input_shape,\n                   const float* input_data, const RuntimeShape& output_shape,\n                   float* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          float sum_squares = 0.f;\n          int filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              const float val =\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              sum_squares += val * val;\n              filter_count++;\n            }\n          }\n          const float l2pool_result = std::sqrt(sum_squares / filter_count);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              ActivationFunctionWithMinMax(l2pool_result,\n                                           params.float_activation_min,\n                                           params.float_activation_max);\n        }\n      }\n    }\n  }\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const float* input_data, const RuntimeShape& output_shape,\n                    float* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          float max = std::numeric_limits<float>::lowest();\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              max = std::max(\n                  max,\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);\n            }\n          }\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              ActivationFunctionWithMinMax(max, params.float_activation_min,\n                                           params.float_activation_max);\n        }\n      }\n    }\n  }\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const uint8_t* input_data, const RuntimeShape& output_shape,\n                    uint8_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);\n  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          uint8_t max = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              max = std::max(\n                  max,\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);\n            }\n          }\n          max = std::max<uint8_t>(max, params.quantized_activation_min);\n          max = std::min<uint8_t>(max, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<uint8_t>(max);\n        }\n      }\n    }\n  }\n}\n}  // namespace reference_ops\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING_H_\n", "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/pooling.h\"\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <cstdlib>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/reference/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/padding.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace pooling {\n\n// This file has two implementation of each pooling op.\nenum KernelType {\n  kReference,\n  kGenericOptimized,\n};\n\nenum PoolType {\n  kAverage,\n  kMax,\n  kL2,\n};\n\nstruct OpData {\n  TfLitePaddingValues padding;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  // This is a builtin op, so we don't use the contents in 'buffer', if any.\n  // Instead, we allocate a new object to carry information from Prepare() to\n  // Eval().\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\ntemplate <PoolType pool_type>\nTfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  int batches = input->dims->data[0];\n  int height = input->dims->data[1];\n  int width = input->dims->data[2];\n  int channels_out = input->dims->data[3];\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n\n  // Prevent division by 0 in optimized pooling implementations\n  TF_LITE_ENSURE(context, params->stride_height > 0);\n  TF_LITE_ENSURE(context, params->stride_width > 0);\n\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width, 1, 1, height, width,\n      params->filter_height, params->filter_width, padding, &out_height,\n      &out_width);\n\n  if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n    if (pool_type == kAverage || pool_type == kMax) {\n      TFLITE_DCHECK_LE(std::abs(input->params.scale - output->params.scale),\n                       1.0e-6);\n      TFLITE_DCHECK_EQ(input->params.zero_point, output->params.zero_point);\n    }\n    if (pool_type == kL2) {\n      // We currently don't have a quantized implementation of L2Pool\n      TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);\n    }\n  }\n\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = batches;\n  output_size->data[1] = out_height;\n  output_size->data[2] = out_width;\n  output_size->data[3] = channels_out;\n  return context->ResizeTensor(context, output, output_size);\n}\n\ntemplate <KernelType kernel_type>\nvoid AverageEvalFloat(TfLiteContext* context, TfLiteNode* node,\n                      TfLitePoolParams* params, OpData* data,\n                      const TfLiteTensor* input, TfLiteTensor* output) {\n  float activation_min, activation_max;\n  CalculateActivationRange(params->activation, &activation_min,\n                           &activation_max);\n#define TF_LITE_AVERAGE_POOL(type)                                       \\\n  tflite::PoolParams op_params;                                          \\\n  op_params.stride_height = params->stride_height;                       \\\n  op_params.stride_width = params->stride_width;                         \\\n  op_params.filter_height = params->filter_height;                       \\\n  op_params.filter_width = params->filter_width;                         \\\n  op_params.padding_values.height = data->padding.height;                \\\n  op_params.padding_values.width = data->padding.width;                  \\\n  op_params.float_activation_min = activation_min;                       \\\n  op_params.float_activation_max = activation_max;                       \\\n  type::AveragePool(op_params, GetTensorShape(input),                    \\\n                    GetTensorData<float>(input), GetTensorShape(output), \\\n                    GetTensorData<float>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_AVERAGE_POOL(reference_ops);\n  } else {\n    TF_LITE_AVERAGE_POOL(optimized_ops);\n  }\n#undef TF_LITE_AVERAGE_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid AverageEvalQuantizedUint8(TfLiteContext* context, TfLiteNode* node,\n                               TfLitePoolParams* params, OpData* data,\n                               const TfLiteTensor* input,\n                               TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                          &activation_min, &activation_max);\n#define TF_LITE_AVERAGE_POOL(type)                                         \\\n  tflite::PoolParams op_params;                                            \\\n  op_params.stride_height = params->stride_height;                         \\\n  op_params.stride_width = params->stride_width;                           \\\n  op_params.filter_height = params->filter_height;                         \\\n  op_params.filter_width = params->filter_width;                           \\\n  op_params.padding_values.height = data->padding.height;                  \\\n  op_params.padding_values.width = data->padding.width;                    \\\n  op_params.quantized_activation_min = activation_min;                     \\\n  op_params.quantized_activation_max = activation_max;                     \\\n  type::AveragePool(op_params, GetTensorShape(input),                      \\\n                    GetTensorData<uint8_t>(input), GetTensorShape(output), \\\n                    GetTensorData<uint8_t>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_AVERAGE_POOL(reference_ops);\n  } else {\n    TF_LITE_AVERAGE_POOL(optimized_ops);\n  }\n#undef TF_LITE_AVERAGE_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid AverageEvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,\n                              TfLitePoolParams* params, OpData* data,\n                              const TfLiteTensor* input, TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n\n  (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                          &activation_min, &activation_max);\n#define TF_LITE_AVERAGE_POOL(type)                                        \\\n  tflite::PoolParams op_params;                                           \\\n  op_params.stride_height = params->stride_height;                        \\\n  op_params.stride_width = params->stride_width;                          \\\n  op_params.filter_height = params->filter_height;                        \\\n  op_params.filter_width = params->filter_width;                          \\\n  op_params.padding_values.height = data->padding.height;                 \\\n  op_params.padding_values.width = data->padding.width;                   \\\n  op_params.quantized_activation_min = activation_min;                    \\\n  op_params.quantized_activation_max = activation_max;                    \\\n  type::AveragePool(op_params, GetTensorShape(input),                     \\\n                    GetTensorData<int8_t>(input), GetTensorShape(output), \\\n                    GetTensorData<int8_t>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_AVERAGE_POOL(reference_integer_ops);\n  } else {\n    TF_LITE_AVERAGE_POOL(optimized_integer_ops);\n  }\n#undef TF_LITE_AVERAGE_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid AverageEvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,\n                               TfLitePoolParams* params, OpData* data,\n                               const TfLiteTensor* input,\n                               TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  CalculateActivationRangeQuantized(context, params->activation, output,\n                                    &activation_min, &activation_max);\n#define TF_LITE_AVERAGE_POOL(type)                                         \\\n  tflite::PoolParams op_params;                                            \\\n  op_params.stride_height = params->stride_height;                         \\\n  op_params.stride_width = params->stride_width;                           \\\n  op_params.filter_height = params->filter_height;                         \\\n  op_params.filter_width = params->filter_width;                           \\\n  op_params.padding_values.height = data->padding.height;                  \\\n  op_params.padding_values.width = data->padding.width;                    \\\n  op_params.quantized_activation_min = activation_min;                     \\\n  op_params.quantized_activation_max = activation_max;                     \\\n  type::AveragePool(op_params, GetTensorShape(input),                      \\\n                    GetTensorData<int16_t>(input), GetTensorShape(output), \\\n                    GetTensorData<int16_t>(output))\n  TF_LITE_AVERAGE_POOL(reference_integer_ops);\n#undef TF_LITE_AVERAGE_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid MaxEvalFloat(TfLiteContext* context, TfLiteNode* node,\n                  TfLitePoolParams* params, OpData* data,\n                  const TfLiteTensor* input, TfLiteTensor* output) {\n  float activation_min, activation_max;\n  CalculateActivationRange(params->activation, &activation_min,\n                           &activation_max);\n#define TF_LITE_MAX_POOL(type)                                                 \\\n  tflite::PoolParams op_params;                                                \\\n  op_params.stride_height = params->stride_height;                             \\\n  op_params.stride_width = params->stride_width;                               \\\n  op_params.filter_height = params->filter_height;                             \\\n  op_params.filter_width = params->filter_width;                               \\\n  op_params.padding_values.height = data->padding.height;                      \\\n  op_params.padding_values.width = data->padding.width;                        \\\n  op_params.float_activation_min = activation_min;                             \\\n  op_params.float_activation_max = activation_max;                             \\\n  type::MaxPool(op_params, GetTensorShape(input), GetTensorData<float>(input), \\\n                GetTensorShape(output), GetTensorData<float>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_MAX_POOL(reference_ops);\n  } else {\n    TF_LITE_MAX_POOL(optimized_ops);\n  }\n#undef TF_LITE_MAX_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid MaxEvalQuantizedUInt8(TfLiteContext* context, TfLiteNode* node,\n                           TfLitePoolParams* params, OpData* data,\n                           const TfLiteTensor* input, TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                          &activation_min, &activation_max);\n#define TF_LITE_MAX_POOL(type)                                         \\\n  tflite::PoolParams op_params;                                        \\\n  op_params.stride_height = params->stride_height;                     \\\n  op_params.stride_width = params->stride_width;                       \\\n  op_params.filter_height = params->filter_height;                     \\\n  op_params.filter_width = params->filter_width;                       \\\n  op_params.padding_values.height = data->padding.height;              \\\n  op_params.padding_values.width = data->padding.width;                \\\n  op_params.quantized_activation_min = activation_min;                 \\\n  op_params.quantized_activation_max = activation_max;                 \\\n  type::MaxPool(op_params, GetTensorShape(input),                      \\\n                GetTensorData<uint8_t>(input), GetTensorShape(output), \\\n                GetTensorData<uint8_t>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_MAX_POOL(reference_ops);\n  } else {\n    TF_LITE_MAX_POOL(optimized_ops);\n  }\n#undef TF_LITE_MAX_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid MaxEvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,\n                          TfLitePoolParams* params, OpData* data,\n                          const TfLiteTensor* input, TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                          &activation_min, &activation_max);\n#define TF_LITE_MAX_POOL(type)                                        \\\n  tflite::PoolParams op_params;                                       \\\n  op_params.stride_height = params->stride_height;                    \\\n  op_params.stride_width = params->stride_width;                      \\\n  op_params.filter_height = params->filter_height;                    \\\n  op_params.filter_width = params->filter_width;                      \\\n  op_params.padding_values.height = data->padding.height;             \\\n  op_params.padding_values.width = data->padding.width;               \\\n  op_params.quantized_activation_min = activation_min;                \\\n  op_params.quantized_activation_max = activation_max;                \\\n  type::MaxPool(op_params, GetTensorShape(input),                     \\\n                GetTensorData<int8_t>(input), GetTensorShape(output), \\\n                GetTensorData<int8_t>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_MAX_POOL(reference_integer_ops);\n  } else {\n    TF_LITE_MAX_POOL(optimized_integer_ops);\n  }\n#undef TF_LITE_MAX_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid MaxEvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,\n                           TfLitePoolParams* params, OpData* data,\n                           const TfLiteTensor* input, TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  CalculateActivationRangeQuantized(context, params->activation, output,\n                                    &activation_min, &activation_max);\n#define TF_LITE_MAX_POOL(type)                                         \\\n  tflite::PoolParams op_params;                                        \\\n  op_params.stride_height = params->stride_height;                     \\\n  op_params.stride_width = params->stride_width;                       \\\n  op_params.filter_height = params->filter_height;                     \\\n  op_params.filter_width = params->filter_width;                       \\\n  op_params.padding_values.height = data->padding.height;              \\\n  op_params.padding_values.width = data->padding.width;                \\\n  op_params.quantized_activation_min = activation_min;                 \\\n  op_params.quantized_activation_max = activation_max;                 \\\n  type::MaxPool(op_params, GetTensorShape(input),                      \\\n                GetTensorData<int16_t>(input), GetTensorShape(output), \\\n                GetTensorData<int16_t>(output))\n  TF_LITE_MAX_POOL(reference_integer_ops);\n#undef TF_LITE_MAX_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid L2EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                 TfLitePoolParams* params, OpData* data,\n                 const TfLiteTensor* input, TfLiteTensor* output) {\n  float activation_min, activation_max;\n  CalculateActivationRange(params->activation, &activation_min,\n                           &activation_max);\n#define TF_LITE_L2_POOL(type)                                                 \\\n  tflite::PoolParams op_params;                                               \\\n  op_params.stride_height = params->stride_height;                            \\\n  op_params.stride_width = params->stride_width;                              \\\n  op_params.filter_height = params->filter_height;                            \\\n  op_params.filter_width = params->filter_width;                              \\\n  op_params.padding_values.height = data->padding.height;                     \\\n  op_params.padding_values.width = data->padding.width;                       \\\n  op_params.float_activation_min = activation_min;                            \\\n  op_params.float_activation_max = activation_max;                            \\\n  type::L2Pool(op_params, GetTensorShape(input), GetTensorData<float>(input), \\\n               GetTensorShape(output), GetTensorData<float>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_L2_POOL(reference_ops);\n  } else {\n    TF_LITE_L2_POOL(optimized_ops);\n  }\n#undef TF_LITE_L2_POOL\n}\n\n#undef TF_LITE_KERNEL_TYPE_DISPATCH\n\ntemplate <KernelType kernel_type>\nTfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  switch (input->type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      AverageEvalFloat<kernel_type>(context, node, params, data, input, output);\n      break;\n    case kTfLiteUInt8:\n      AverageEvalQuantizedUint8<kernel_type>(context, node, params, data, input,\n                                             output);\n      break;\n    case kTfLiteInt8:\n      AverageEvalQuantizedInt8<kernel_type>(context, node, params, data, input,\n                                            output);\n      break;\n    case kTfLiteInt16:\n      AverageEvalQuantizedInt16<kernel_type>(context, node, params, data, input,\n                                             output);\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Type %s not currently supported.\",\n                         TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  switch (input->type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      MaxEvalFloat<kernel_type>(context, node, params, data, input, output);\n      break;\n    case kTfLiteUInt8:\n      MaxEvalQuantizedUInt8<kernel_type>(context, node, params, data, input,\n                                         output);\n      break;\n    case kTfLiteInt8:\n      MaxEvalQuantizedInt8<kernel_type>(context, node, params, data, input,\n                                        output);\n      break;\n    case kTfLiteInt16:\n      MaxEvalQuantizedInt16<kernel_type>(context, node, params, data, input,\n                                         output);\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Type %s not currently supported.\",\n                         TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus L2Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  switch (input->type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      L2EvalFloat<kernel_type>(context, node, params, data, input, output);\n      break;\n    case kTfLiteUInt8:\n    // We don't have a quantized implementation, so just fall through to the\n    // 'default' case.\n    default:\n      context->ReportError(context, \"Type %d not currently supported.\",\n                           input->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace pooling\n\nTfLiteRegistration* Register_AVERAGE_POOL_REF() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kAverage>,\n                                 pooling::AverageEval<pooling::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_MAX_POOL_REF() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kMax>,\n                                 pooling::MaxEval<pooling::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_L2_POOL_REF() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kL2>,\n                                 pooling::L2Eval<pooling::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_AVERAGE_POOL_GENERIC_OPT() {\n  static TfLiteRegistration r = {\n      pooling::Init, pooling::Free, pooling::GenericPrepare<pooling::kAverage>,\n      pooling::AverageEval<pooling::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_MAX_POOL_GENERIC_OPT() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kMax>,\n                                 pooling::MaxEval<pooling::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_L2_POOL_GENERIC_OPT() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kL2>,\n                                 pooling::L2Eval<pooling::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_AVERAGE_POOL_2D() {\n  return Register_AVERAGE_POOL_GENERIC_OPT();\n}\n\nTfLiteRegistration* Register_MAX_POOL_2D() {\n  return Register_MAX_POOL_GENERIC_OPT();\n}\n\nTfLiteRegistration* Register_L2_POOL_2D() {\n  return Register_L2_POOL_GENERIC_OPT();\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "fixing_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <algorithm>\n#include <cmath>\n#include <cstdlib>\n#include <functional>\n#include <iterator>\n#include <limits>\n#include <random>\n#include <string>\n#include <vector>\n\n#include <gtest/gtest.h>\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/test_util.h\"\n\nnamespace tflite {\nnamespace {\n\n// Runs the reference and optimized AveragePool functions and asserts the values\n// are the same.\nvoid RunOneAveragePoolTest(const PoolParams& params,\n                           const RuntimeShape& input_shape,\n                           const int8* input_data,\n                           const RuntimeShape& output_shape) {\n  const int buffer_size = output_shape.FlatSize();\n  std::vector<int8> optimized_averagePool_output(buffer_size);\n  std::vector<int8> reference_averagePool_output(buffer_size);\n\n  bool reference_success = reference_integer_ops::AveragePool(\n      params, input_shape, input_data, output_shape,\n      reference_averagePool_output.data());\n  bool optimized_success = optimized_integer_ops::AveragePool(\n      params, input_shape, input_data, output_shape,\n      optimized_averagePool_output.data());\n  EXPECT_TRUE(reference_success);\n  EXPECT_TRUE(optimized_success);\n\n  for (int i = 0; i < buffer_size; i++) {\n    EXPECT_TRUE(reference_averagePool_output[i] ==\n                optimized_averagePool_output[i]);\n  }\n}\n\n// Creates random input shape (batch, height, width, depth), then computes\n// output shape based on value of `padding_same`:\n// `padding_same` == true, calculate output with padding == \"SAME\"\n// `padding_same` == false, calculate output with padding == \"VALID\"\n// With input/output shapes computed, fills the input data and calls the\n// test function.\nvoid CreateDataAndRunAveragePool(bool padding_same) {\n  const int batch = UniformRandomInt(1, 2);\n  const int input_depth = UniformRandomInt(1, 700);\n  const int output_depth = input_depth;\n  const int input_width_offset = UniformRandomInt(1, 30);\n  const int input_height_offset = UniformRandomInt(1, 30);\n  const int stride_width = UniformRandomInt(1, 10);\n  const int stride_height = UniformRandomInt(1, 10);\n  const int filter_width = UniformRandomInt(1, 10);\n  const int filter_height = UniformRandomInt(1, 10);\n  const int input_width = input_width_offset + filter_width;\n  const int input_height = input_height_offset + filter_height;\n  const int output_width =\n      padding_same ? (input_width + stride_width - 1) / stride_width\n                   : (input_width - filter_width + stride_width) / stride_width;\n  const int output_height =\n      padding_same\n          ? (input_height + stride_height - 1) / stride_height\n          : (input_height - filter_height + stride_height) / stride_height;\n\n  auto input_shape =\n      RuntimeShape({batch, input_height, input_width, input_depth});\n  auto output_shape =\n      RuntimeShape({batch, output_height, output_width, output_depth});\n  const int buffer_size = input_shape.FlatSize();\n  std::vector<int8> input_data(buffer_size);\n  FillRandom(&input_data);\n\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.quantized_activation_min =\n      static_cast<int8_t>(std::numeric_limits<int8_t>::lowest());\n  params.quantized_activation_max =\n      static_cast<int8_t>(std::numeric_limits<int8_t>::max());\n  auto compute_padding = [](int stride, int in_size, int filter_size,\n                            int out_size) {\n    int padding = ((out_size - 1) * stride + filter_size - in_size) / 2;\n    return padding > 0 ? padding : 0;\n  };\n  params.padding_values.width =\n      compute_padding(stride_width, input_width, filter_width, output_width);\n  params.padding_values.height = compute_padding(stride_height, input_height,\n                                                 filter_height, output_height);\n  RunOneAveragePoolTest(params, input_shape, input_data.data(), output_shape);\n}\n\nTEST(TestAveragePool, SymmetricQuantAveragePool) {\n  const int kTestsToRun = 10;\n  for (int i = 0; i < kTestsToRun; i++) {\n    CreateDataAndRunAveragePool(/*padding_same=*/true);\n    CreateDataAndRunAveragePool(/*padding_same=*/false);\n  }\n}\n\n// Creates random input shape (batch, height, width, depth), then computes\n// output shape based on value of `padding_same`:\n// `padding_same` == true, calculate output with padding == \"SAME\"\n// `padding_same` == false, calculate output with padding == \"VALID\"\n// With input/output shapes computed, fills the input data and calls the\n// test function.\nvoid CreateExtremalDataAndRunAveragePool(bool padding_same) {\n  const int batch = UniformRandomInt(1, 2);\n  const int input_depth = UniformRandomInt(1, 700);\n  const int output_depth = input_depth;\n  const int input_width_offset = UniformRandomInt(1, 30);\n  const int input_height_offset = UniformRandomInt(1, 30);\n  const int stride_width = UniformRandomInt(1, 128);\n  const int stride_height = UniformRandomInt(1, 128);\n  const int filter_width = UniformRandomInt(1, 28);\n  const int filter_height = UniformRandomInt(1, 28);\n  if (filter_width * filter_height > 64) {\n    std::cout << \"should test 32 version\" << std::endl;\n  }\n  const int input_width = input_width_offset + filter_width;\n  const int input_height = input_height_offset + filter_height;\n  const int output_width =\n      padding_same ? (input_width + stride_width - 1) / stride_width\n                   : (input_width - filter_width + stride_width) / stride_width;\n  const int output_height =\n      padding_same\n          ? (input_height + stride_height - 1) / stride_height\n          : (input_height - filter_height + stride_height) / stride_height;\n\n  auto input_shape =\n      RuntimeShape({batch, input_height, input_width, input_depth});\n  auto output_shape =\n      RuntimeShape({batch, output_height, output_width, output_depth});\n\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.quantized_activation_min =\n      static_cast<int8_t>(std::numeric_limits<int8_t>::lowest());\n  params.quantized_activation_max =\n      static_cast<int8_t>(std::numeric_limits<int8_t>::max());\n  auto compute_padding = [](int stride, int in_size, int filter_size,\n                            int out_size) {\n    int padding = ((out_size - 1) * stride + filter_size - in_size) / 2;\n    return padding > 0 ? padding : 0;\n  };\n  params.padding_values.width =\n      compute_padding(stride_width, input_width, filter_width, output_width);\n  params.padding_values.height = compute_padding(stride_height, input_height,\n                                                 filter_height, output_height);\n\n  const int buffer_size = input_shape.FlatSize();\n  std::vector<int8> input_data(buffer_size);\n\n  // Test small values\n  int8 min = std::numeric_limits<int8>::min();\n  int8 max = std::numeric_limits<int8>::min() + 10;\n  FillRandom(&input_data, min, max);\n  RunOneAveragePoolTest(params, input_shape, input_data.data(), output_shape);\n\n  // Test large values\n  min = std::numeric_limits<int8>::max() - 10;\n  max = std::numeric_limits<int8>::max();\n  FillRandom(&input_data, min, max);\n  RunOneAveragePoolTest(params, input_shape, input_data.data(), output_shape);\n}\n\nTEST(TestAveragePool, SymmetricQuantExtremalAveragePool) {\n  CreateExtremalDataAndRunAveragePool(/*padding_same=*/true);\n  CreateExtremalDataAndRunAveragePool(/*padding_same=*/false);\n}\n\n}  // namespace\n}  // namespace tflite\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_INTEGER_OPS_POOLING_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_INTEGER_OPS_POOLING_H_\n\n#include <string.h>\n\n#include <algorithm>\n\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/cppmath.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/im2col_utils.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/neon_check.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/strided_slice_logic.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\nnamespace optimized_integer_ops {\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const int8* input_data, const RuntimeShape& output_shape,\n                    int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaxPool/8bit\");\n\n  // Here, and in other pooling ops, in order to maintain locality of reference,\n  // to minimize some recalculations, and to load into NEON vector registers, we\n  // use an inner loop down the depth. Since depths can be large and hence we\n  // would need arbitrarily large temporary storage, we divide the work up into\n  // depth tranches just within the batch loop.\n  static constexpr int kPoolingAccTrancheSize = 256;\n\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  int8 acc[kPoolingAccTrancheSize];\n  for (int batch = 0; batch < batches; ++batch) {\n    // We proceed through the depth in tranches (see comment above). The\n    // depth_base is the depth at the beginning of the tranche. The\n    // tranche_depth is the depth dimension of the tranche.\n    for (int depth_base = 0; depth_base < depth;\n         depth_base += kPoolingAccTrancheSize) {\n      const int tranche_depth =\n          std::min(depth - depth_base, kPoolingAccTrancheSize);\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          memset(acc, params.quantized_activation_min,\n                 tranche_depth * sizeof(acc[0]));\n          const int8* input_ptr =\n              input_data + depth_base +\n              depth * (in_x_origin +\n                       input_width * (in_y_origin + input_height * batch));\n          for (int fy = filter_y_start; fy < filter_y_end; fy++) {\n            const int8* input_row_ptr =\n                input_ptr + depth * (fy * input_width + filter_x_start);\n            for (int fx = filter_x_start; fx < filter_x_end; fx++) {\n              const int8* input_channel_ptr = input_row_ptr;\n              int channel = 0;\n#ifdef USE_NEON\n              for (; channel <= tranche_depth - 16; channel += 16) {\n                int8x16_t acc_reg = vld1q_s8(acc + channel);\n                int8x16_t input_reg = vld1q_s8(input_channel_ptr);\n                input_channel_ptr += 16;\n                acc_reg = vmaxq_s8(acc_reg, input_reg);\n                vst1q_s8(acc + channel, acc_reg);\n              }\n\n              for (; channel <= tranche_depth - 8; channel += 8) {\n                int8x8_t acc_reg = vld1_s8(acc + channel);\n                int8x8_t input_reg = vld1_s8(input_channel_ptr);\n                input_channel_ptr += 8;\n                acc_reg = vmax_s8(acc_reg, input_reg);\n                vst1_s8(acc + channel, acc_reg);\n              }\n#endif\n              for (; channel < tranche_depth; ++channel) {\n                acc[channel] = std::max(acc[channel], *input_channel_ptr++);\n              }\n              input_row_ptr += depth;\n            }\n          }\n          int8* output_ptr = output_data + Offset(output_shape, batch, out_y,\n                                                  out_x, depth_base);\n          int channel = 0;\n#ifdef USE_NEON\n          for (; channel <= tranche_depth - 16; channel += 16) {\n            int8x16_t a = vld1q_s8(acc + channel);\n            a = vminq_s8(a, vdupq_n_s8(params.quantized_activation_max));\n            a = vmaxq_s8(a, vdupq_n_s8(params.quantized_activation_min));\n            vst1q_s8(output_ptr + channel, a);\n          }\n          for (; channel <= tranche_depth - 8; channel += 8) {\n            int8x8_t a = vld1_s8(acc + channel);\n            a = vmin_s8(a, vdup_n_s8(params.quantized_activation_max));\n            a = vmax_s8(a, vdup_n_s8(params.quantized_activation_min));\n            vst1_s8(output_ptr + channel, a);\n          }\n#endif\n          for (; channel < tranche_depth; ++channel) {\n            int8 a = acc[channel];\n            a = std::max<int8>(a, params.quantized_activation_min);\n            a = std::min<int8>(a, params.quantized_activation_max);\n            output_ptr[channel] = static_cast<int8>(a);\n          }\n        }\n      }\n    }\n  }\n}\n\ninline bool AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape, const int8* input_data,\n                        const RuntimeShape& output_shape, int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"AveragePool/8bitWith32bitAccumulator\");\n\n  // Here, and in other pooling ops, in order to maintain locality of reference,\n  // to minimize some recalculations, and to load into NEON vector registers, we\n  // use an inner loop down the depth. Since depths can be large and hence we\n  // would need arbitrarily large temporary storage, we divide the work up into\n  // depth tranches just within the batch loop.\n  static constexpr int kPoolingAccTrancheSize = 256;\n\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  int32 acc[kPoolingAccTrancheSize];\n  for (int batch = 0; batch < batches; ++batch) {\n    // We proceed through the depth in tranches (see comment above). The\n    // depth_base is the depth at the beginning of the tranche. The\n    // tranche_depth is the depth dimension of the tranche.\n    for (int depth_base = 0; depth_base < depth;\n         depth_base += kPoolingAccTrancheSize) {\n      const int tranche_depth =\n          std::min(depth - depth_base, kPoolingAccTrancheSize);\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          const int filter_count =\n              (filter_x_end - filter_x_start) * (filter_y_end - filter_y_start);\n          if (filter_count == 0) return false;\n          memset(acc, 0, tranche_depth * sizeof(acc[0]));\n          const int8* input_ptr =\n              input_data + depth_base +\n              depth * (in_x_origin +\n                       input_width * (in_y_origin + input_height * batch));\n          for (int fy = filter_y_start; fy < filter_y_end; fy++) {\n            const int8* input_row_ptr =\n                input_ptr + depth * (fy * input_width + filter_x_start);\n            for (int fx = filter_x_start; fx < filter_x_end; fx++) {\n              const int8* input_channel_ptr = input_row_ptr;\n              int channel = 0;\n#ifdef USE_NEON\n              for (; channel <= tranche_depth - 16; channel += 16) {\n                int16x4_t acc_reg[4];\n                int8x16_t input_reg = vld1q_s8(input_channel_ptr);\n                input_channel_ptr += 16;\n                acc_reg[0] = vget_low_s16(vmovl_s8(vget_low_s8(input_reg)));\n                acc_reg[1] = vget_high_s16(vmovl_s8(vget_low_s8(input_reg)));\n                acc_reg[2] = vget_low_s16(vmovl_s8(vget_high_s8(input_reg)));\n                acc_reg[3] = vget_high_s16(vmovl_s8(vget_high_s8(input_reg)));\n                for (int i = 0; i < 4; i++) {\n                  vst1q_s32(\n                      acc + channel + 4 * i,\n                      vaddw_s16(vld1q_s32(acc + channel + 4 * i), acc_reg[i]));\n                }\n              }\n              for (; channel <= tranche_depth - 8; channel += 8) {\n                int16x4_t acc_reg[2];\n                int16x8_t input_reg = vmovl_s8(vld1_s8(input_channel_ptr));\n                input_channel_ptr += 8;\n                acc_reg[0] = vget_low_s16(input_reg);\n                acc_reg[1] = vget_high_s16(input_reg);\n                for (int i = 0; i < 2; i++) {\n                  vst1q_s32(\n                      acc + channel + 4 * i,\n                      vaddw_s16(vld1q_s32(acc + channel + 4 * i), acc_reg[i]));\n                }\n              }\n#endif\n              for (; channel < tranche_depth; ++channel) {\n                acc[channel] += *input_channel_ptr++;\n              }\n              input_row_ptr += depth;\n            }\n          }\n          int8* output_ptr = output_data + Offset(output_shape, batch, out_y,\n                                                  out_x, depth_base);\n          int channel = 0;\n#ifdef USE_NEON\n          for (; channel <= tranche_depth - 8; channel += 8) {\n            int16 buf[8];\n            for (int i = 0; i < 8; i++) {\n              buf[i] =\n                  acc[channel + i] > 0\n                      ? (acc[channel + i] + filter_count / 2) / filter_count\n                      : (acc[channel + i] - filter_count / 2) / filter_count;\n            }\n            int8x8_t buf8 = vqmovn_s16(vld1q_s16(buf));\n            buf8 = vmin_s8(buf8, vdup_n_s8(params.quantized_activation_max));\n            buf8 = vmax_s8(buf8, vdup_n_s8(params.quantized_activation_min));\n            vst1_s8(output_ptr + channel, buf8);\n          }\n#endif\n          for (; channel < tranche_depth; ++channel) {\n            int16 a = acc[channel] > 0\n                          ? (acc[channel] + filter_count / 2) / filter_count\n                          : (acc[channel] - filter_count / 2) / filter_count;\n            a = std::max<int16>(a, params.quantized_activation_min);\n            a = std::min<int16>(a, params.quantized_activation_max);\n            output_ptr[channel] = static_cast<int8>(a);\n          }\n        }\n      }\n    }\n  }\n  return true;\n}\n\n}  // namespace optimized_integer_ops\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_INTEGER_OPS_POOLING_H_\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_LEGACY_OPTIMIZED_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_LEGACY_OPTIMIZED_OPS_H_\n\n#include <stdint.h>\n#include <sys/types.h>\n\n#include \"public/gemmlowp.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/fully_connected.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/resize_bilinear.h\"\n#include \"tensorflow/lite/kernels/internal/reference/legacy_reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\nnamespace optimized_ops {\n\n// Unoptimized reference ops:\nusing reference_ops::Broadcast4DSlowGreater;\nusing reference_ops::Broadcast4DSlowGreaterEqual;\nusing reference_ops::Broadcast4DSlowGreaterEqualWithScaling;\nusing reference_ops::Broadcast4DSlowGreaterWithScaling;\nusing reference_ops::Broadcast4DSlowLess;\nusing reference_ops::Broadcast4DSlowLessEqual;\nusing reference_ops::Broadcast4DSlowLessEqualWithScaling;\nusing reference_ops::Broadcast4DSlowLessWithScaling;\nusing reference_ops::BroadcastAdd4DSlow;\nusing reference_ops::BroadcastGreater;\nusing reference_ops::BroadcastGreaterEqual;\nusing reference_ops::BroadcastLess;\nusing reference_ops::BroadcastLessEqual;\nusing reference_ops::BroadcastMul4DSlow;\nusing reference_ops::BroadcastSubSlow;\nusing reference_ops::Concatenation;\nusing reference_ops::ConcatenationWithScaling;\nusing reference_ops::DepthConcatenation;\nusing reference_ops::Div;\nusing reference_ops::FakeQuant;\nusing reference_ops::Gather;\nusing reference_ops::Greater;\nusing reference_ops::GreaterEqual;\nusing reference_ops::GreaterEqualWithScaling;\nusing reference_ops::GreaterWithScaling;\nusing reference_ops::Less;\nusing reference_ops::LessEqual;\nusing reference_ops::LessEqualWithScaling;\nusing reference_ops::LessWithScaling;\nusing reference_ops::Mean;\nusing reference_ops::RankOneSelect;\nusing reference_ops::Relu1;\nusing reference_ops::Relu6;\nusing reference_ops::ReluX;\nusing reference_ops::Select;\nusing reference_ops::SpaceToBatchND;\nusing reference_ops::Split;\nusing reference_ops::TensorFlowSplit;\n\nstatic constexpr int kDepthwiseReverseShift = -1;\n\ntemplate <typename Scalar, int N>\nVectorMap<Scalar> MapAsVector(Scalar* data, const Dims<N>& dims) {\n  const int size = FlatSize(dims);\n  return VectorMap<Scalar>(data, size, 1);\n}\n\ntemplate <typename Scalar, int N>\nMatrixMap<Scalar> MapAsMatrixWithFirstDimAsRows(Scalar* data,\n                                                const Dims<N>& dims) {\n  const int rows = dims.sizes[0];\n  int cols = 1;\n  for (int d = 1; d < N; d++) {\n    cols *= dims.sizes[d];\n  }\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar, int N>\nMatrixMap<Scalar> MapAsMatrixWithLastDimAsCols(Scalar* data,\n                                               const Dims<N>& dims) {\n  const int cols = dims.sizes[N - 1];\n  int rows = 1;\n  for (int d = 0; d < N - 1; d++) {\n    rows *= dims.sizes[d];\n  }\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar, int N>\nArrayMap<Scalar> MapAsArrayWithFirstDimAsRows(Scalar* data,\n                                              const Dims<N>& dims) {\n  const int rows = dims.sizes[0];\n  int cols = 1;\n  for (int d = 1; d < N; d++) {\n    cols *= dims.sizes[d];\n  }\n  return ArrayMap<Scalar>(data, rows, cols);\n}\n\n// TODO(b/62193649): this function is only needed as long\n// as we have the --variable_batch hack.\ntemplate <typename Scalar, int N>\nMatrixMap<Scalar> MapAsMatrixWithGivenNumberOfRows(Scalar* data,\n                                                   const Dims<N>& dims,\n                                                   int rows) {\n  const int flatsize = FlatSize(dims);\n  TFLITE_DCHECK((flatsize % rows) == 0);\n  const int cols = flatsize / rows;\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ninline bool AreSameDims(const Dims<4>& dims1, const Dims<4>& dims2) {\n  for (int i = 0; i < 4; i++) {\n    if (dims1.sizes[i] != dims2.sizes[i]) {\n      return false;\n    }\n  }\n  return true;\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  const RuntimeShape output_shape = DimsToShape(output_dims);\n  const int output_height = output_shape.Dims(1);\n\n  DepthwiseConvImpl(op_params, DimsToShape(input_dims), input_data,\n                    DimsToShape(filter_dims), filter_data,\n                    DimsToShape(bias_dims), bias_data, output_shape,\n                    output_data, CpuFlags(), /*thread_start=*/0,\n                    /*thread_end=*/output_height, /*thread_dim=*/1);\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, 1, 1, pad_width,\n                pad_height, depth_multiplier, output_activation_min,\n                output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, float* output_data,\n                   const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, pad_width, pad_height,\n                depth_multiplier, output_activation_min, output_activation_max,\n                output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   float* output_data, const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n                    bias_dims, stride, stride, pad_width, pad_height,\n                    depth_multiplier, output_data, output_dims);\n}\n\ntemplate <DepthwiseConvOutputRounding kOutputRounding>\ninline void LegacyDepthwiseConvWithRounding(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, int thread_start, int thread_end, int thread_dim) {\n  ruy::profiler::ScopeLabel label(\"DepthwiseConv/8bit\");\n  const int depth_multiplier = params.depth_multiplier;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  TFLITE_DCHECK_GE(dilation_width_factor, 1);\n  TFLITE_DCHECK_GE(dilation_height_factor, 1);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);\n  const int input_depth = input_shape.Dims(3);\n  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);\n\n// Enable for arm64 except for the Nvidia Linux 4 Tegra (L4T) running on\n// Jetson TX-2. This compiler does not support the offsetof() macro.\n#if defined(__aarch64__) && !defined(GOOGLE_L4T)\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  const int output_shift = params.output_shift;\n\n  // Call kernel optimized for depthwise convolutions using 3x3 filters if\n  // parameters are supported.\n  if (depthwise_conv::Fast3x3FilterKernelSupported(\n          input_shape, filter_shape, stride_width, stride_height,\n          dilation_width_factor, dilation_height_factor, pad_width, pad_height,\n          depth_multiplier, output_shape, output_shift)) {\n    ruy::profiler::ScopeLabel specialized_label(\"DepthwiseConv/8bit/3x3\");\n    depthwise_conv::DepthwiseConv3x3Filter<kOutputRounding>(\n        params, input_shape, input_data, filter_shape, filter_data, bias_shape,\n        bias_data, output_shape, output_data, thread_start, thread_end,\n        thread_dim);\n    return;\n  }\n#endif\n\n  ruy::profiler::ScopeLabel specialized_label(\"DepthwiseConv/8bit/General\");\n  depthwise_conv::DepthwiseConvGeneral(params, input_shape, input_data,\n                                       filter_shape, filter_data, bias_shape,\n                                       bias_data, output_shape, output_data,\n                                       thread_start, thread_end, thread_dim);\n}\n\ninline void LegacyDepthwiseConvImpl(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, int thread_start, int thread_end, int thread_dim) {\n  return LegacyDepthwiseConvWithRounding<\n      DepthwiseConvOutputRounding::kAwayFromZero>(\n      params, input_shape, input_data, filter_shape, filter_data, bias_shape,\n      bias_data, output_shape, output_data, thread_start, thread_end,\n      thread_dim);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kDepthwiseReverseShift * output_shift;\n\n  const RuntimeShape output_shape = DimsToShape(output_dims);\n  const int output_height = output_shape.Dims(1);\n\n  LegacyDepthwiseConvImpl(\n      op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n      filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n      output_data, /*thread_start=*/0,\n      /*thread_end=*/output_height, /*thread_dim=*/1);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, 1, 1, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, int32 output_offset,\n                   int32 output_multiplier, int output_shift,\n                   int32 output_activation_min, int32 output_activation_max,\n                   uint8* output_data, const Dims<4>& output_dims) {\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   int32 output_offset, int32 output_multiplier,\n                   int output_shift, int32 output_activation_min,\n                   int32 output_activation_max, uint8* output_data,\n                   const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, input_offset, filter_data,\n                    filter_dims, filter_offset, bias_data, bias_dims, stride,\n                    stride, pad_width, pad_height, depth_multiplier,\n                    output_offset, output_multiplier, output_shift,\n                    output_activation_min, output_activation_max, output_data,\n                    output_dims);\n}\n\ntemplate <typename T, typename TS>\nstruct LegacyDepthwiseConvWorkerTask : public gemmlowp::Task {\n  LegacyDepthwiseConvWorkerTask(\n      const DepthwiseParams& params, const RuntimeShape& input_shape,\n      const T* input_data, const RuntimeShape& filter_shape,\n      const T* filter_data, const RuntimeShape& bias_shape, const TS* bias_data,\n      const RuntimeShape& output_shape, T* output_data, int thread_start,\n      int thread_end, int thread_dim)\n      : params_(params),\n        input_shape_(input_shape),\n        input_data_(input_data),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        thread_start_(thread_start),\n        thread_end_(thread_end),\n        thread_dim_(thread_dim) {}\n\n  void Run() override {\n    LegacyDepthwiseConvImpl(params_, input_shape_, input_data_, filter_shape_,\n                            filter_data_, bias_shape_, bias_data_,\n                            output_shape_, output_data_, thread_start_,\n                            thread_end_, thread_dim_);\n  }\n\n private:\n  const DepthwiseParams& params_;\n  const RuntimeShape& input_shape_;\n  const T* input_data_;\n  const RuntimeShape& filter_shape_;\n  const T* filter_data_;\n  const RuntimeShape& bias_shape_;\n  const TS* bias_data_;\n  const RuntimeShape& output_shape_;\n  T* output_data_;\n  int thread_start_;\n  int thread_end_;\n  int thread_dim_;\n};\n\ninline int HowManyConvThreads(const RuntimeShape& output_shape,\n                              const RuntimeShape& filter_shape,\n                              int thread_dim) {\n  constexpr int kMinMulPerThread = 8;\n  const int output_units = output_shape.Dims(thread_dim);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int num_mul_per_unit =\n      FlatSizeSkipDim(output_shape, thread_dim) * filter_height * filter_width;\n  const int min_units_per_thread = kMinMulPerThread / num_mul_per_unit + 1;\n  int thread_count = output_units / min_units_per_thread;\n  return thread_count;\n}\n\ninline void DepthwiseConv(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext* gemmlowp_context = nullptr) {\n  ruy::profiler::ScopeLabel label(\"DepthwiseConv\");\n\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int output_batches = output_shape.Dims(0);\n  const int output_rows = output_shape.Dims(1);\n  int thread_count_batch = HowManyConvThreads(output_shape, filter_shape, 0);\n  int thread_count_row = HowManyConvThreads(output_shape, filter_shape, 1);\n  int thread_dim, thread_count, thread_dim_size;\n  if (thread_count_batch > thread_count_row) {\n    thread_dim = 0;\n    thread_dim_size = output_batches;\n    thread_count = thread_count_batch;\n  } else {\n    thread_dim = 1;\n    thread_dim_size = output_rows;\n    thread_count = thread_count_row;\n  }\n\n  const int max_threads =\n      gemmlowp_context ? gemmlowp_context->max_num_threads() : 1;\n  thread_count = std::max(1, std::min(thread_count, max_threads));\n\n  if (thread_count == 1) {\n    LegacyDepthwiseConvImpl(params, input_shape, input_data, filter_shape,\n                            filter_data, bias_shape, bias_data, output_shape,\n                            output_data, /*thread_start=*/0,\n                            /*thread_end=*/output_rows, /*thread_dim=*/1);\n  } else {\n    std::vector<gemmlowp::Task*> tasks(thread_count);\n    int thread_start = 0;\n    for (int i = 0; i < thread_count; ++i) {\n      int thread_end =\n          thread_start + (thread_dim_size - thread_start) / (thread_count - i);\n      tasks[i] = new LegacyDepthwiseConvWorkerTask<uint8, int32>(\n          params, input_shape, input_data, filter_shape, filter_data,\n          bias_shape, bias_data, output_shape, output_data, thread_start,\n          thread_end, thread_dim);\n      thread_start = thread_end;\n    }\n    gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n  }\n}\n\ntemplate <typename T, typename TS>\nstruct LegacyPerChannelDepthwiseConvWorkerTask : public gemmlowp::Task {\n  LegacyPerChannelDepthwiseConvWorkerTask(\n      const DepthwiseParams& params, const int32* output_multiplier,\n      const int32* output_shift, const RuntimeShape& input_shape,\n      const T* input_data, const RuntimeShape& filter_shape,\n      const T* filter_data, const RuntimeShape& bias_shape, const TS* bias_data,\n      const RuntimeShape& output_shape, T* output_data, int thread_start,\n      int thread_end, int thread_dim)\n      : params_(params),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        input_shape_(input_shape),\n        input_data_(input_data),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        thread_start_(thread_start),\n        thread_end_(thread_end),\n        thread_dim_(thread_dim) {}\n\n  void Run() override {\n    CpuBackendContext backend_context;\n    optimized_integer_ops::DepthwiseConvImpl(\n        params_, output_multiplier_, output_shift_, input_shape_, input_data_,\n        filter_shape_, filter_data_, bias_shape_, bias_data_, output_shape_,\n        output_data_, thread_start_, thread_end_, thread_dim_, backend_context);\n  }\n\n private:\n  const DepthwiseParams& params_;\n  const int32* output_multiplier_;\n  const int32* output_shift_;\n  const RuntimeShape& input_shape_;\n  const T* input_data_;\n  const RuntimeShape& filter_shape_;\n  const T* filter_data_;\n  const RuntimeShape& bias_shape_;\n  const TS* bias_data_;\n  const RuntimeShape& output_shape_;\n  T* output_data_;\n  int thread_start_;\n  int thread_end_;\n  int thread_dim_;\n};\n\ninline void DepthwiseConvPerChannel(\n    const DepthwiseParams& params, const int32* output_multiplier,\n    const int32* output_shift, const RuntimeShape& input_shape,\n    const int8* input_data, const RuntimeShape& filter_shape,\n    const int8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape, int8* output_data,\n    gemmlowp::GemmContext* gemmlowp_context = nullptr) {\n  ruy::profiler::ScopeLabel label(\"DepthwiseConvInt8\");\n\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int output_batches = output_shape.Dims(0);\n  const int output_rows = output_shape.Dims(1);\n  int thread_count_batch = HowManyConvThreads(output_shape, filter_shape, 0);\n  int thread_count_row = HowManyConvThreads(output_shape, filter_shape, 1);\n  int thread_dim, thread_count, thread_dim_size;\n  if (thread_count_batch > thread_count_row) {\n    thread_dim = 0;\n    thread_dim_size = output_batches;\n    thread_count = thread_count_batch;\n  } else {\n    thread_dim = 1;\n    thread_dim_size = output_rows;\n    thread_count = thread_count_row;\n  }\n\n  const int max_threads =\n      gemmlowp_context ? gemmlowp_context->max_num_threads() : 1;\n  thread_count = std::max(1, std::min(thread_count, max_threads));\n\n  if (thread_count == 1) {\n    CpuBackendContext backend_context;\n    optimized_integer_ops::DepthwiseConvImpl(\n        params, output_multiplier, output_shift, input_shape, input_data,\n        filter_shape, filter_data, bias_shape, bias_data, output_shape,\n        output_data, /*thread_start=*/0,\n        /*thread_end=*/output_rows, /*thread_dim=*/1, backend_context);\n  } else {\n    std::vector<gemmlowp::Task*> tasks(thread_count);\n    int thread_start = 0;\n    for (int i = 0; i < thread_count; ++i) {\n      int thread_end =\n          thread_start + (thread_dim_size - thread_start) / (thread_count - i);\n      tasks[i] = new LegacyPerChannelDepthwiseConvWorkerTask<int8, int32>(\n          params, output_multiplier, output_shift, input_shape, input_data,\n          filter_shape, filter_data, bias_shape, bias_data, output_shape,\n          output_data, thread_start, thread_end, thread_dim);\n      thread_start = thread_end;\n    }\n    gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n  }\n}\n\ninline void DepthwiseConv(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  DepthwiseConvImpl(params, input_shape, input_data, filter_shape, filter_data,\n                    bias_shape, bias_data, output_shape, output_data,\n                    CpuFlags(),\n                    /*thread_start=*/0,\n                    /*thread_end=*/output_shape.Dims(1), /*thread_dim=*/1);\n}\n\ninline void AddBiasAndEvalActivationFunction(const float* bias_data,\n                                             const Dims<4>& bias_dims,\n                                             float* array_data,\n                                             const Dims<4>& array_dims,\n                                             float output_activation_min,\n                                             float output_activation_max) {\n  AddBiasAndEvalActivationFunction(output_activation_min, output_activation_max,\n                                   DimsToShape(bias_dims), bias_data,\n                                   DimsToShape(array_dims), array_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AddBiasAndEvalActivationFunction(const float* bias_data,\n                                      const Dims<4>& bias_dims,\n                                      float* array_data,\n                                      const Dims<4>& array_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  AddBiasAndEvalActivationFunction(bias_data, bias_dims, array_data, array_dims,\n                                   output_activation_min,\n                                   output_activation_max);\n}\n\ntemplate <typename Lhs, typename Rhs, typename Result>\nvoid Gemm(const Eigen::MatrixBase<Lhs>& lhs, const Eigen::MatrixBase<Rhs>& rhs,\n          Eigen::MatrixBase<Result>* result) {\n  if (rhs.cols() == 1) {\n    ruy::profiler::ScopeLabel label(\"GEMV\");\n    result->col(0).noalias() = lhs * rhs.col(0);\n  } else {\n    ruy::profiler::ScopeLabel label(\"GEMM\");\n    result->noalias() = lhs * rhs;\n  }\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& bias_shape,\n    const float* optional_bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected\");\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n\n  // TODO(b/62193649): this convoluted shape computation (determining\n  // input_rows from the weights_dims, then MapAsMatrixWithGivenNumberOfRows)\n  // is because the current --variable_batch hack consists in overwriting the\n  // 3rd dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  // When that is fixed, this should become:\n  // const auto input_matrix_map =\n  //     MapAsMatrixWithFirstDimAsRows(input_data, input_dims);\n  const int dims_count = weights_shape.DimensionsCount();\n  const int input_rows = weights_shape.Dims(dims_count - 1);\n  const auto input_matrix_map =\n      MapAsMatrixWithGivenNumberOfRows(input_data, input_shape, input_rows);\n  const auto filter_matrix_map =\n      MapAsMatrixWithLastDimAsRows(weights_data, weights_shape);\n  auto output_matrix_map =\n      MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n\n  Gemm(filter_matrix_map.transpose(), input_matrix_map, &output_matrix_map);\n\n  if (optional_bias_data != nullptr) {\n    AddBiasAndEvalActivationFunction(\n        output_activation_min, output_activation_max, bias_shape,\n        optional_bias_data, output_shape, output_data);\n  } else {\n    const int flat_size = output_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      output_data[i] = ActivationFunctionWithMinMax(\n          output_data[i], output_activation_min, output_activation_max);\n    }\n  }\n}\n\ninline void FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                           const float* weights_data,\n                           const Dims<4>& weights_dims, const float* bias_data,\n                           const Dims<4>& bias_dims,\n                           float output_activation_min,\n                           float output_activation_max, float* output_data,\n                           const Dims<4>& output_dims) {\n  tflite::FullyConnectedParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(weights_dims), weights_data,\n                 DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                    const float* weights_data, const Dims<4>& weights_dims,\n                    const float* bias_data, const Dims<4>& bias_dims,\n                    float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  FullyConnected(input_data, input_dims, weights_data, weights_dims, bias_data,\n                 bias_dims, output_activation_min, output_activation_max,\n                 output_data, output_dims);\n}\n\nstruct GemmlowpOutputPipeline {\n  typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n      ColVectorMap;\n  typedef std::tuple<gemmlowp::OutputStageBiasAddition<ColVectorMap>,\n                     gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent,\n                     gemmlowp::OutputStageClamp,\n                     gemmlowp::OutputStageSaturatingCastToUint8>\n      Pipeline;\n  static Pipeline MakeExp(const int32* bias_data, int output_rows,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_left_shift, int32 output_activation_min,\n                          int32 output_activation_max) {\n    ColVectorMap bias_vector(bias_data, output_rows);\n    gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n    bias_addition_stage.bias_vector = bias_vector;\n    gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent quantize_down_stage;\n    quantize_down_stage.result_offset_after_shift = output_offset;\n    quantize_down_stage.result_fixedpoint_multiplier = output_multiplier;\n    quantize_down_stage.result_exponent = output_left_shift;\n    gemmlowp::OutputStageClamp clamp_stage;\n    clamp_stage.min = output_activation_min;\n    clamp_stage.max = output_activation_max;\n    gemmlowp::OutputStageSaturatingCastToUint8 saturating_cast_stage;\n    return std::make_tuple(bias_addition_stage, quantize_down_stage,\n                           clamp_stage, saturating_cast_stage);\n  }\n};\n\nstruct GemmlowpOutputPipelineInt8 {\n  typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n      ColVectorMap;\n  typedef std::tuple<gemmlowp::OutputStageBiasAddition<ColVectorMap>,\n                     gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent,\n                     gemmlowp::OutputStageClamp,\n                     gemmlowp::OutputStageSaturatingCastToInt8>\n      Pipeline;\n  static Pipeline MakeExp(const int32* bias_data, int output_rows,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_left_shift, int32 output_activation_min,\n                          int32 output_activation_max) {\n    ColVectorMap bias_vector(bias_data, output_rows);\n    gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n    bias_addition_stage.bias_vector = bias_vector;\n    gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent quantize_down_stage;\n    quantize_down_stage.result_offset_after_shift = output_offset;\n    quantize_down_stage.result_fixedpoint_multiplier = output_multiplier;\n    quantize_down_stage.result_exponent = output_left_shift;\n    gemmlowp::OutputStageClamp clamp_stage;\n    clamp_stage.min = output_activation_min;\n    clamp_stage.max = output_activation_max;\n    gemmlowp::OutputStageSaturatingCastToInt8 saturating_cast_stage;\n    return std::make_tuple(bias_addition_stage, quantize_down_stage,\n                           clamp_stage, saturating_cast_stage);\n  }\n};\n\n#ifdef USE_NEON\ninline void LegacyFullyConnectedAsGEMVWorkerImpl(\n    const RuntimeShape& input_shape, const uint8* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const uint8* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    uint8* output_data, int row_start, int row_end) {\n  ruy::profiler::ScopeLabel label(\"FullyConnectedAsGEMV/8bit\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kPeel = 4;\n  const bool shift_left = (output_shift > 0);\n  for (int k = 0; k < input_size; k += 64) {\n    optimized_ops_preload_l1_stream(input_data + k);\n  }\n  for (int k = 0; k < kPeel * input_size; k += 64) {\n    optimized_ops_preload_l1_stream(filter_data + k);\n  }\n\n  TFLITE_DCHECK_GE(row_end - row_start, kPeel);\n\n  for (int out = row_start; out < row_end; out += kPeel) {\n    out = std::min(out, row_end - kPeel);\n    int32x4_t acc0 = vdupq_n_s32(0);\n    int32x4_t acc1 = acc0;\n    int32x4_t acc2 = acc0;\n    int32x4_t acc3 = acc0;\n    const int16x8_t input_offset_vec = vdupq_n_s16(input_offset);\n    const int16x8_t filter_offset_vec = vdupq_n_s16(filter_offset);\n    int in = 0;\n    for (; in <= input_size - 16; in += 16) {\n      const uint8x16_t input_val_u8 = vld1q_u8(input_data + in);\n      const uint8* filter_ptr = filter_data + in + out * input_size;\n      uint8x16_t filter_val_u8_0 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      filter_ptr += input_size;\n      uint8x16_t filter_val_u8_1 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      filter_ptr += input_size;\n      uint8x16_t filter_val_u8_2 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      filter_ptr += input_size;\n      uint8x16_t filter_val_u8_3 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      int16x8_t input_val_0, input_val_1;\n      uint8x8_t low = vget_low_u8(input_val_u8);\n      uint8x8_t high = vget_high_u8(input_val_u8);\n      input_val_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      input_val_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      input_val_0 = vaddq_s16(input_val_0, input_offset_vec);\n      input_val_1 = vaddq_s16(input_val_1, input_offset_vec);\n      low = vget_low_u8(filter_val_u8_0);\n      high = vget_high_u8(filter_val_u8_0);\n      int16x8_t filter_val_0_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_0_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_0_0 = vaddq_s16(filter_val_0_0, filter_offset_vec);\n      filter_val_0_1 = vaddq_s16(filter_val_0_1, filter_offset_vec);\n      low = vget_low_u8(filter_val_u8_1);\n      high = vget_high_u8(filter_val_u8_1);\n      int16x8_t filter_val_1_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_1_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_1_0 = vaddq_s16(filter_val_1_0, filter_offset_vec);\n      filter_val_1_1 = vaddq_s16(filter_val_1_1, filter_offset_vec);\n      low = vget_low_u8(filter_val_u8_2);\n      high = vget_high_u8(filter_val_u8_2);\n      int16x8_t filter_val_2_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_2_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_2_0 = vaddq_s16(filter_val_2_0, filter_offset_vec);\n      filter_val_2_1 = vaddq_s16(filter_val_2_1, filter_offset_vec);\n      low = vget_low_u8(filter_val_u8_3);\n      high = vget_high_u8(filter_val_u8_3);\n      int16x8_t filter_val_3_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_3_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_3_0 = vaddq_s16(filter_val_3_0, filter_offset_vec);\n      filter_val_3_1 = vaddq_s16(filter_val_3_1, filter_offset_vec);\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_0),\n                       vget_low_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_0),\n                       vget_low_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_0),\n                       vget_low_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_0),\n                       vget_low_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_1),\n                       vget_low_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_1),\n                       vget_low_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_1),\n                       vget_low_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_1),\n                       vget_low_s16(input_val_1));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_0),\n                       vget_high_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_0),\n                       vget_high_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_0),\n                       vget_high_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_0),\n                       vget_high_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_1),\n                       vget_high_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_1),\n                       vget_high_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_1),\n                       vget_high_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_1),\n                       vget_high_s16(input_val_1));\n    }\n    for (; in <= input_size - 8; in += 8) {\n      const uint8x8_t input_val_u8 = vld1_u8(input_data + in);\n      const uint8* filter_ptr = filter_data + in + out * input_size;\n      uint8x8_t filter_val_u8_0 = vld1_u8(filter_ptr);\n      filter_ptr += input_size;\n      uint8x8_t filter_val_u8_1 = vld1_u8(filter_ptr);\n      filter_ptr += input_size;\n      uint8x8_t filter_val_u8_2 = vld1_u8(filter_ptr);\n      filter_ptr += input_size;\n      uint8x8_t filter_val_u8_3 = vld1_u8(filter_ptr);\n      int16x8_t input_val = vreinterpretq_s16_u16(vmovl_u8(input_val_u8));\n      input_val = vaddq_s16(input_val, input_offset_vec);\n      int16x8_t filter_val_0 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_0));\n      filter_val_0 = vaddq_s16(filter_val_0, filter_offset_vec);\n      int16x8_t filter_val_1 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_1));\n      filter_val_1 = vaddq_s16(filter_val_1, filter_offset_vec);\n      int16x8_t filter_val_2 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_2));\n      filter_val_2 = vaddq_s16(filter_val_2, filter_offset_vec);\n      int16x8_t filter_val_3 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_3));\n      filter_val_3 = vaddq_s16(filter_val_3, filter_offset_vec);\n      acc0 =\n          vmlal_s16(acc0, vget_low_s16(filter_val_0), vget_low_s16(input_val));\n      acc1 =\n          vmlal_s16(acc1, vget_low_s16(filter_val_1), vget_low_s16(input_val));\n      acc2 =\n          vmlal_s16(acc2, vget_low_s16(filter_val_2), vget_low_s16(input_val));\n      acc3 =\n          vmlal_s16(acc3, vget_low_s16(filter_val_3), vget_low_s16(input_val));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0),\n                       vget_high_s16(input_val));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1),\n                       vget_high_s16(input_val));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2),\n                       vget_high_s16(input_val));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3),\n                       vget_high_s16(input_val));\n    }\n    if (in < input_size) {\n      int32 buf[16];\n      vst1q_s32(buf + 0, acc0);\n      vst1q_s32(buf + 4, acc1);\n      vst1q_s32(buf + 8, acc2);\n      vst1q_s32(buf + 12, acc3);\n      for (; in < input_size; in++) {\n        int lane = (in + 8 - input_size) % 4;\n        const int32 input_val = input_data[in] + input_offset;\n        for (int k = 0; k < kPeel; k++) {\n          int32 filter_val =\n              filter_data[in + (out + k) * input_size] + filter_offset;\n          buf[lane + 4 * k] += filter_val * input_val;\n        }\n      }\n      acc0 = vld1q_s32(buf + 0);\n      acc1 = vld1q_s32(buf + 4);\n      acc2 = vld1q_s32(buf + 8);\n      acc3 = vld1q_s32(buf + 12);\n    }\n\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(acc0), vget_high_s32(acc0));\n    int32x2_t pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(acc1), vget_high_s32(acc1));\n    int32x2_t pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(acc2), vget_high_s32(acc2));\n    int32x2_t pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(acc3), vget_high_s32(acc3));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    int32x4_t bias_vec = vld1q_s32(bias_data + out);\n    reduced = vaddq_s32(reduced, bias_vec);\n    if (shift_left) {\n      const int32 multiplier_power_of_two = 1 << output_shift;\n      reduced = vmulq_n_s32(reduced, multiplier_power_of_two);\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n    } else {\n      // Multiply by the fixed-point multiplier.\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n      // Rounding-shift-right.\n      using gemmlowp::RoundingDivideByPOT;\n      reduced = RoundingDivideByPOT(reduced, -output_shift);\n    }\n    // Add the output offset.\n    const int32x4_t output_offset_vec = vdupq_n_s32(output_offset);\n    reduced = vaddq_s32(reduced, output_offset_vec);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    // Narrow values down to 8 bit unsigned, saturating.\n    uint8x8_t res8 = vqmovun_s16(vcombine_s16(res16, res16));\n    // Apply the clamping from the activation function\n    res8 = vmax_u8(res8, vdup_n_u8(output_activation_min));\n    res8 = vmin_u8(res8, vdup_n_u8(output_activation_max));\n    // Store results to destination.\n    vst1_lane_u8(output_data + out + 0, res8, 0);\n    vst1_lane_u8(output_data + out + 1, res8, 1);\n    vst1_lane_u8(output_data + out + 2, res8, 2);\n    vst1_lane_u8(output_data + out + 3, res8, 3);\n  }\n}\n\nstruct LegacyFullyConnectedAsGEMVWorkerTask : public gemmlowp::Task {\n  LegacyFullyConnectedAsGEMVWorkerTask(\n      const RuntimeShape& input_shape, const uint8* input_data,\n      int32 input_offset, const RuntimeShape& filter_shape,\n      const uint8* filter_data, int32 filter_offset,\n      const RuntimeShape& bias_shape, const int32* bias_data,\n      int32 output_offset, int32 output_multiplier, int output_shift,\n      int32 output_activation_min, int32 output_activation_max,\n      const RuntimeShape& output_shape, uint8* output_data, int row_start,\n      int row_end)\n      : input_shape_(input_shape),\n        input_data_(input_data),\n        input_offset_(input_offset),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        filter_offset_(filter_offset),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_offset_(output_offset),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_activation_min_(output_activation_min),\n        output_activation_max_(output_activation_max),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        row_start_(row_start),\n        row_end_(row_end) {}\n\n  void Run() override {\n    LegacyFullyConnectedAsGEMVWorkerImpl(\n        input_shape_, input_data_, input_offset_, filter_shape_, filter_data_,\n        filter_offset_, bias_shape_, bias_data_, output_offset_,\n        output_multiplier_, output_shift_, output_activation_min_,\n        output_activation_max_, output_shape_, output_data_, row_start_,\n        row_end_);\n  }\n\n  const RuntimeShape& input_shape_;\n  const uint8* input_data_;\n  int32 input_offset_;\n  const RuntimeShape& filter_shape_;\n  const uint8* filter_data_;\n  int32 filter_offset_;\n  const RuntimeShape& bias_shape_;\n  const int32* bias_data_;\n  int32 output_offset_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int32 output_activation_min_;\n  int32 output_activation_max_;\n  const RuntimeShape& output_shape_;\n  uint8* output_data_;\n  int row_start_;\n  int row_end_;\n};\n\ninline void FullyConnectedAsGEMV(\n    const RuntimeShape& input_shape, const uint8* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const uint8* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kKernelRows = 4;\n  const int thread_count = gemmlowp::HowManyThreads<kKernelRows>(\n      gemmlowp_context->max_num_threads(), output_rows, batches, input_size);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    LegacyFullyConnectedAsGEMVWorkerImpl(\n        input_shape, input_data, input_offset, filter_shape, filter_data,\n        filter_offset, bias_shape, bias_data, output_offset, output_multiplier,\n        output_shift, output_activation_min, output_activation_max,\n        output_shape, output_data, 0, output_rows);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<gemmlowp::Task*> tasks(thread_count);\n  const int kRowsPerWorker = gemmlowp::RoundUp<kKernelRows>(\n      gemmlowp::CeilQuotient(output_rows, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; ++i) {\n    int row_end = std::min(output_rows, row_start + kRowsPerWorker);\n    tasks[i] = new LegacyFullyConnectedAsGEMVWorkerTask(\n        input_shape, input_data, input_offset, filter_shape, filter_data,\n        filter_offset, bias_shape, bias_data, output_offset, output_multiplier,\n        output_shift, output_activation_min, output_activation_max,\n        output_shape, output_data, row_start, row_end);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_rows);\n  gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n}\n#endif  // USE_NEON\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/8bit\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n#ifdef USE_NEON\n  if (batches == 1) {\n    const int output_size = MatchingDim(filter_shape, filter_dim_count - 2,\n                                        output_shape, output_dim_count - 1);\n    if (output_size >= 4) {\n      return FullyConnectedAsGEMV(\n          input_shape, input_data, input_offset, filter_shape, filter_data,\n          filter_offset, bias_shape, bias_data, output_offset,\n          output_multiplier, output_shift, output_activation_min,\n          output_activation_max, output_shape, output_data, gemmlowp_context);\n    }\n  }\n#endif  // USE_NEON\n  const int filter_rows = filter_shape.Dims(filter_dim_count - 2);\n  const int filter_cols = filter_shape.Dims(filter_dim_count - 1);\n  TFLITE_DCHECK_EQ(filter_shape.FlatSize(), filter_rows * filter_cols);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, output_rows, filter_cols, filter_cols);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, filter_cols, batches, filter_cols);\n  gemmlowp::MatrixMap<uint8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, batches, output_rows);\n  const auto& output_pipeline = GemmlowpOutputPipeline::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, output_shift,\n      output_activation_min, output_activation_max);\n  gemmlowp::GemmWithOutputPipeline<uint8, uint8,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\n#ifdef GEMMLOWP_NEON\n// In the common case of batch size 1, a fully-connected node degenerates\n// to a matrix*vector product. LSTM cells contain a fully-connected node;\n// when quantized, this becomes a special type of GEMV operation where\n// the output is 16bit-quantized, thus needs its own special path.\ninline void GEMVForLstmCell(const RuntimeShape& input_shape,\n                            const uint8* input_data,\n                            const RuntimeShape& weights_shape,\n                            const uint8* weights_data, uint8 weights_zero_point,\n                            const RuntimeShape& bias_shape,\n                            const int32* bias_data, int32 accum_multiplier,\n                            int accum_shift, const RuntimeShape& output_shape,\n                            int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"GEMVForLstmCell\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  const int output_size = MatchingDim(weights_shape, weights_dim_count - 2,\n                                      output_shape, output_dim_count - 1);\n  // This special fast path for quantized LSTM cells does not try to support\n  // odd sizes that we haven't encountered in any LSTM cell, that would\n  // require special code (that would go untested until any LSTM cell\n  // exercises it). We just guard our assumptions about size evenness with\n  // the following assertions.\n  TFLITE_DCHECK(!(output_size % 4));\n  TFLITE_DCHECK(!(input_size % 8));\n  const int32* bias_ptr = bias_data;\n  int16* output_ptr = output_data;\n  for (int out = 0; out < output_size; out += 4) {\n    int32x4_t acc_0 = vdupq_n_s32(0);\n    int32x4_t acc_1 = vdupq_n_s32(0);\n    int32x4_t acc_2 = vdupq_n_s32(0);\n    int32x4_t acc_3 = vdupq_n_s32(0);\n    const int16x8_t input_offset_vec = vdupq_n_s16(-128);\n    const int16x8_t weights_offset_vec = vdupq_n_s16(-weights_zero_point);\n    int in = 0;\n    // Handle 16 levels of depth at a time.\n    for (; in <= input_size - 16; in += 16) {\n      const uint8x16_t input_val_u8 = vld1q_u8(input_data + in);\n      const uint8* weights_ptr = weights_data + in + out * input_size;\n      uint8x16_t weights_val_u8_0 = vld1q_u8(weights_ptr + 0 * input_size);\n      uint8x16_t weights_val_u8_1 = vld1q_u8(weights_ptr + 1 * input_size);\n      uint8x16_t weights_val_u8_2 = vld1q_u8(weights_ptr + 2 * input_size);\n      uint8x16_t weights_val_u8_3 = vld1q_u8(weights_ptr + 3 * input_size);\n      int16x8_t input_val_0, input_val_1;\n      const uint8x8_t low = vget_low_u8(input_val_u8);\n      const uint8x8_t high = vget_high_u8(input_val_u8);\n      input_val_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      input_val_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      input_val_0 = vaddq_s16(input_val_0, input_offset_vec);\n      input_val_1 = vaddq_s16(input_val_1, input_offset_vec);\n      int16x8_t weights_val_0_0, weights_val_1_0, weights_val_2_0,\n          weights_val_3_0;\n      int16x8_t weights_val_0_1, weights_val_1_1, weights_val_2_1,\n          weights_val_3_1;\n      weights_val_0_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_0))),\n          weights_offset_vec);\n      weights_val_0_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_0))),\n          weights_offset_vec);\n      weights_val_1_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_1))),\n          weights_offset_vec);\n      weights_val_1_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_1))),\n          weights_offset_vec);\n      weights_val_2_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_2))),\n          weights_offset_vec);\n      weights_val_2_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_2))),\n          weights_offset_vec);\n      weights_val_3_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_3))),\n          weights_offset_vec);\n      weights_val_3_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_3))),\n          weights_offset_vec);\n      acc_0 = vmlal_s16(acc_0, vget_low_s16(weights_val_0_0),\n                        vget_low_s16(input_val_0));\n      acc_1 = vmlal_s16(acc_1, vget_low_s16(weights_val_1_0),\n                        vget_low_s16(input_val_0));\n      acc_2 = vmlal_s16(acc_2, vget_low_s16(weights_val_2_0),\n                        vget_low_s16(input_val_0));\n      acc_3 = vmlal_s16(acc_3, vget_low_s16(weights_val_3_0),\n                        vget_low_s16(input_val_0));\n      acc_0 = vmlal_s16(acc_0, vget_high_s16(weights_val_0_0),\n                        vget_high_s16(input_val_0));\n      acc_1 = vmlal_s16(acc_1, vget_high_s16(weights_val_1_0),\n                        vget_high_s16(input_val_0));\n      acc_2 = vmlal_s16(acc_2, vget_high_s16(weights_val_2_0),\n                        vget_high_s16(input_val_0));\n      acc_3 = vmlal_s16(acc_3, vget_high_s16(weights_val_3_0),\n                        vget_high_s16(input_val_0));\n      acc_0 = vmlal_s16(acc_0, vget_low_s16(weights_val_0_1),\n                        vget_low_s16(input_val_1));\n      acc_1 = vmlal_s16(acc_1, vget_low_s16(weights_val_1_1),\n                        vget_low_s16(input_val_1));\n      acc_2 = vmlal_s16(acc_2, vget_low_s16(weights_val_2_1),\n                        vget_low_s16(input_val_1));\n      acc_3 = vmlal_s16(acc_3, vget_low_s16(weights_val_3_1),\n                        vget_low_s16(input_val_1));\n      acc_0 = vmlal_s16(acc_0, vget_high_s16(weights_val_0_1),\n                        vget_high_s16(input_val_1));\n      acc_1 = vmlal_s16(acc_1, vget_high_s16(weights_val_1_1),\n                        vget_high_s16(input_val_1));\n      acc_2 = vmlal_s16(acc_2, vget_high_s16(weights_val_2_1),\n                        vget_high_s16(input_val_1));\n      acc_3 = vmlal_s16(acc_3, vget_high_s16(weights_val_3_1),\n                        vget_high_s16(input_val_1));\n    }\n    // Handle 8 levels of depth at a time.\n    for (; in < input_size; in += 8) {\n      const uint8x8_t input_val_u8 = vld1_u8(input_data + in);\n      const uint8* weights_ptr = weights_data + in + out * input_size;\n      uint8x8_t weights_val_u8_0 = vld1_u8(weights_ptr + 0 * input_size);\n      uint8x8_t weights_val_u8_1 = vld1_u8(weights_ptr + 1 * input_size);\n      uint8x8_t weights_val_u8_2 = vld1_u8(weights_ptr + 2 * input_size);\n      uint8x8_t weights_val_u8_3 = vld1_u8(weights_ptr + 3 * input_size);\n      int16x8_t input_val;\n      input_val = vreinterpretq_s16_u16(vmovl_u8(input_val_u8));\n      input_val = vaddq_s16(input_val, input_offset_vec);\n      int16x8_t weights_val_0, weights_val_1, weights_val_2, weights_val_3;\n      weights_val_0 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_0)),\n                    weights_offset_vec);\n      weights_val_1 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_1)),\n                    weights_offset_vec);\n      weights_val_2 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_2)),\n                    weights_offset_vec);\n      weights_val_3 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_3)),\n                    weights_offset_vec);\n      acc_0 = vmlal_s16(acc_0, vget_low_s16(weights_val_0),\n                        vget_low_s16(input_val));\n      acc_1 = vmlal_s16(acc_1, vget_low_s16(weights_val_1),\n                        vget_low_s16(input_val));\n      acc_2 = vmlal_s16(acc_2, vget_low_s16(weights_val_2),\n                        vget_low_s16(input_val));\n      acc_3 = vmlal_s16(acc_3, vget_low_s16(weights_val_3),\n                        vget_low_s16(input_val));\n      acc_0 = vmlal_s16(acc_0, vget_high_s16(weights_val_0),\n                        vget_high_s16(input_val));\n      acc_1 = vmlal_s16(acc_1, vget_high_s16(weights_val_1),\n                        vget_high_s16(input_val));\n      acc_2 = vmlal_s16(acc_2, vget_high_s16(weights_val_2),\n                        vget_high_s16(input_val));\n      acc_3 = vmlal_s16(acc_3, vget_high_s16(weights_val_3),\n                        vget_high_s16(input_val));\n    }\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,\n        pairwise_reduced_acc_2, pairwise_reduced_acc_3;\n    pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(acc_0), vget_high_s32(acc_0));\n    pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(acc_1), vget_high_s32(acc_1));\n    pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(acc_2), vget_high_s32(acc_2));\n    pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(acc_3), vget_high_s32(acc_3));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    int32x4_t bias_vec = vld1q_s32(bias_ptr);\n    bias_ptr += 4;\n    reduced = vaddq_s32(reduced, bias_vec);\n    int left_shift = accum_shift > 0 ? accum_shift : 0;\n    int right_shift = accum_shift > 0 ? 0 : -accum_shift;\n    reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));\n    // Multiply by the fixed-point multiplier.\n    reduced = vqrdmulhq_n_s32(reduced, accum_multiplier);\n    // Rounding-shift-right.\n    using gemmlowp::RoundingDivideByPOT;\n    reduced = RoundingDivideByPOT(reduced, right_shift);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    vst1_s16(output_ptr, res16);\n    output_ptr += 4;\n  }\n}\n#endif\n\n#ifdef GEMMLOWP_NEON\ninline void GEMVForLstmCellWithSymmetricRange(\n    const RuntimeShape& input_shape, const uint8* input_data,\n    const RuntimeShape& weights_shape, const uint8* weights_data,\n    const RuntimeShape& bias_shape, const int32* bias_data,\n    int32 accum_multiplier, int accum_shift, const RuntimeShape& output_shape,\n    int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"GEMVForLstmCellWithSymmetricRange\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  const int output_size = MatchingDim(weights_shape, weights_dim_count - 2,\n                                      output_shape, output_dim_count - 1);\n  // This special fast path for quantized LSTM cells does not try to support\n  // odd sizes that we haven't encountered in any LSTM cell, that would\n  // require special code (that would go untested until any LSTM cell\n  // exercises it). We just guard our assumptions about size evenness with\n  // the following assertions.\n  TFLITE_DCHECK(!(output_size % 4));\n  TFLITE_DCHECK(!(input_size % 64));\n  const int32* bias_ptr = bias_data;\n  int16* output_ptr = output_data;\n  const uint8x16_t signbit = vdupq_n_u8(0x80);\n  for (int in = 0; in < input_size; in += 32) {\n    optimized_ops_preload_l1_keep(input_data + in);\n  }\n  const int left_shift = accum_shift > 0 ? accum_shift : 0;\n  const int right_shift = accum_shift > 0 ? 0 : -accum_shift;\n  for (int out = 0; out < output_size; out += 4) {\n    // Load the bias values\n    int32x4_t bias_vec = vld1q_s32(bias_ptr);\n    bias_ptr += 4;\n\n    // Clear accumulators. We use 2 accumulator registers per row,\n    // for 4 rows. row_accumRN is the N-th accumulator for row R.\n    int32x4_t row_accum00 = vdupq_n_s32(0);\n    int32x4_t row_accum01 = vdupq_n_s32(0);\n    int32x4_t row_accum10 = vdupq_n_s32(0);\n    int32x4_t row_accum11 = vdupq_n_s32(0);\n    int32x4_t row_accum20 = vdupq_n_s32(0);\n    int32x4_t row_accum21 = vdupq_n_s32(0);\n    int32x4_t row_accum30 = vdupq_n_s32(0);\n    int32x4_t row_accum31 = vdupq_n_s32(0);\n\n    // kReadAhead parametrizes how far ahead we prefetch weights into L1 cache.\n    const int kReadAhead = 512;\n    // Prefetch the first weights values.\n    for (int k = 0; k < kReadAhead; k += 64) {\n      optimized_ops_preload_l1_stream(weights_data + (out + 0) * input_size +\n                                      k);\n      optimized_ops_preload_l1_stream(weights_data + (out + 1) * input_size +\n                                      k);\n      optimized_ops_preload_l1_stream(weights_data + (out + 2) * input_size +\n                                      k);\n      optimized_ops_preload_l1_stream(weights_data + (out + 3) * input_size +\n                                      k);\n    }\n    // Loop along the rows, handling 64 bytes per iteration because that's\n    // cache line size on most current ARM-architecture CPUs.\n    for (int in = 0; in < input_size; in += 64) {\n      // Prefetch some future weights values.\n      optimized_ops_preload_l1_stream(weights_data + (out + 0) * input_size +\n                                      in + kReadAhead);\n      optimized_ops_preload_l1_stream(weights_data + (out + 1) * input_size +\n                                      in + kReadAhead);\n      optimized_ops_preload_l1_stream(weights_data + (out + 2) * input_size +\n                                      in + kReadAhead);\n      optimized_ops_preload_l1_stream(weights_data + (out + 3) * input_size +\n                                      in + kReadAhead);\n\n      // We will use 2 local 16-bit accumulators per row, for 2 rows.\n      // See below (*) for the rationale of processing only 2 rows at a time.\n      // local_accumRN is the N-th local accumulator for row R.\n      int16x8_t local_accum00;\n      int16x8_t local_accum01;\n      int16x8_t local_accum10;\n      int16x8_t local_accum11;\n\n      // Load 64 bytes of input activations values. Convert to signed int8\n      // by flipping the sign bit (i.e. subtracting 128, the required\n      // zero_point value).\n      int8x16_t input0 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 0)));\n      int8x16_t input1 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 1)));\n      int8x16_t input2 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 2)));\n      int8x16_t input3 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 3)));\n\n      // Beginning of the core accumulation. Notice how while we have 4\n      // rows to process, this code is taking care of only 2 rows at a time,\n      // thus being divided into two parts looking similar (\"Rows 0 and 1\" and\n      // \"Rows 2 and 3\").\n      //\n      // (*) The rationale for handling only 2 rows at a time is to avoid\n      // cache aliasing issues on 4-way set-associative L1-cache CPUs, such\n      // as Cortex-A53. With sufficiently large, power-of-two matrix dimensions,\n      // we may find ourselves in a situation where rows alias each other in\n      // the L1 cache, and moreover may also mutually alias with the input\n      // activations. If we try to load 4 rows at a time, together with the\n      // input activations, that may be 5 mutually-aliasing vectors, resulting\n      // in constant mutual eviction from L1 cache. Handling 2 rows at a time\n      // here largely mitigates these issues, and seems at least to be very\n      // effective on Cortex-A53:\n      //                          Before       After\n      // big (Cortex-A73)         2.85 ms      2.85 ms\n      // little (Cortex-A53)      11.0 ms      5.16 ms\n\n      // Rows 0 and 1:\n      // Load 64 bytes of weights values from each row. Convert to signed int8\n      // by flipping the sign bit (i.e. subtracting 128, the required\n      // zero_point value).\n      int8x16_t weights00 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 0)));\n      int8x16_t weights01 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 1)));\n      int8x16_t weights02 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 2)));\n      int8x16_t weights03 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 3)));\n      int8x16_t weights10 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 0)));\n      int8x16_t weights11 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 1)));\n      int8x16_t weights12 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 2)));\n      int8x16_t weights13 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 3)));\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights00), vget_low_s8(input0));\n      local_accum01 = vmull_s8(vget_low_s8(weights01), vget_low_s8(input1));\n      local_accum10 = vmull_s8(vget_low_s8(weights10), vget_low_s8(input0));\n      local_accum11 = vmull_s8(vget_low_s8(weights11), vget_low_s8(input1));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights00),\n                               vget_high_s8(input0));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights01),\n                               vget_high_s8(input1));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights10),\n                               vget_high_s8(input0));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights11),\n                               vget_high_s8(input1));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum00 = vpadalq_s16(row_accum00, local_accum00);\n      row_accum01 = vpadalq_s16(row_accum01, local_accum01);\n      row_accum10 = vpadalq_s16(row_accum10, local_accum10);\n      row_accum11 = vpadalq_s16(row_accum11, local_accum11);\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights02), vget_low_s8(input2));\n      local_accum01 = vmull_s8(vget_low_s8(weights03), vget_low_s8(input3));\n      local_accum10 = vmull_s8(vget_low_s8(weights12), vget_low_s8(input2));\n      local_accum11 = vmull_s8(vget_low_s8(weights13), vget_low_s8(input3));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights02),\n                               vget_high_s8(input2));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights03),\n                               vget_high_s8(input3));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights12),\n                               vget_high_s8(input2));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights13),\n                               vget_high_s8(input3));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum00 = vpadalq_s16(row_accum00, local_accum00);\n      row_accum01 = vpadalq_s16(row_accum01, local_accum01);\n      row_accum10 = vpadalq_s16(row_accum10, local_accum10);\n      row_accum11 = vpadalq_s16(row_accum11, local_accum11);\n\n      // Rows 2 and 3:\n      // Load 64 bytes of weights values from each row. Convert to signed int8\n      // by flipping the sign bit (i.e. subtracting 128, the required\n      // zero_point value).\n      weights00 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 0)));\n      weights01 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 1)));\n      weights02 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 2)));\n      weights03 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 3)));\n      weights10 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 0)));\n      weights11 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 1)));\n      weights12 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 2)));\n      weights13 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 3)));\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights00), vget_low_s8(input0));\n      local_accum01 = vmull_s8(vget_low_s8(weights01), vget_low_s8(input1));\n      local_accum10 = vmull_s8(vget_low_s8(weights10), vget_low_s8(input0));\n      local_accum11 = vmull_s8(vget_low_s8(weights11), vget_low_s8(input1));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights00),\n                               vget_high_s8(input0));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights01),\n                               vget_high_s8(input1));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights10),\n                               vget_high_s8(input0));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights11),\n                               vget_high_s8(input1));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum20 = vpadalq_s16(row_accum20, local_accum00);\n      row_accum21 = vpadalq_s16(row_accum21, local_accum01);\n      row_accum30 = vpadalq_s16(row_accum30, local_accum10);\n      row_accum31 = vpadalq_s16(row_accum31, local_accum11);\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights02), vget_low_s8(input2));\n      local_accum01 = vmull_s8(vget_low_s8(weights03), vget_low_s8(input3));\n      local_accum10 = vmull_s8(vget_low_s8(weights12), vget_low_s8(input2));\n      local_accum11 = vmull_s8(vget_low_s8(weights13), vget_low_s8(input3));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights02),\n                               vget_high_s8(input2));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights03),\n                               vget_high_s8(input3));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights12),\n                               vget_high_s8(input2));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights13),\n                               vget_high_s8(input3));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum20 = vpadalq_s16(row_accum20, local_accum00);\n      row_accum21 = vpadalq_s16(row_accum21, local_accum01);\n      row_accum30 = vpadalq_s16(row_accum30, local_accum10);\n      row_accum31 = vpadalq_s16(row_accum31, local_accum11);\n    }\n\n    row_accum00 = vaddq_s32(row_accum00, row_accum01);\n    row_accum10 = vaddq_s32(row_accum10, row_accum11);\n    row_accum20 = vaddq_s32(row_accum20, row_accum21);\n    row_accum30 = vaddq_s32(row_accum30, row_accum31);\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,\n        pairwise_reduced_acc_2, pairwise_reduced_acc_3;\n    pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(row_accum00), vget_high_s32(row_accum00));\n    pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(row_accum10), vget_high_s32(row_accum10));\n    pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(row_accum20), vget_high_s32(row_accum20));\n    pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(row_accum30), vget_high_s32(row_accum30));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    reduced = vaddq_s32(reduced, bias_vec);\n    reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));\n    // Multiply by the fixed-point multiplier.\n    reduced = vqrdmulhq_n_s32(reduced, accum_multiplier);\n    // Rounding-shift-right.\n    using gemmlowp::RoundingDivideByPOT;\n    reduced = RoundingDivideByPOT(reduced, right_shift);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    vst1_s16(output_ptr, res16);\n    output_ptr += 4;\n  }\n}\n#endif\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data_int32, const RuntimeShape& output_shape,\n    int16* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/Uint8Int16\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  // This is a copy of the reference implementation. We do not currently have a\n  // properly optimized version.\n  (void)gemmlowp_context;  // only used in properly optimized code.\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  TFLITE_DCHECK_EQ(output_offset, 0);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);\n\n  // Implementation of the fully connected node suited to the inside of an LSTM\n  // cell. The operands are 8-bit integers, the accumulators are internally\n  // 32bit integers, and the output is 16-bit fixed-point with 3 integer bits so\n  // the output range is [-2^3, 2^3] == [-8, 8]. The rationale for that\n  // is explained in the function comment above.\n#ifdef GEMMLOWP_NEON\n  if (batches == 1 && input_offset == -128 && output_activation_min == -32768 &&\n      output_activation_max == 32767) {\n    if (filter_offset == -128 && !(output_depth % 4) && !(accum_depth % 64)) {\n      GEMVForLstmCellWithSymmetricRange(\n          input_shape, input_data, filter_shape, filter_data, bias_shape,\n          bias_data_int32, output_multiplier, output_shift, output_shape,\n          output_data);\n      return;\n    }\n    if (!(output_depth % 4) && !(accum_depth % 8)) {\n      GEMVForLstmCell(input_shape, input_data, filter_shape, filter_data,\n                      filter_offset, bias_shape, bias_data_int32,\n                      output_multiplier, output_shift, output_shape,\n                      output_data);\n      return;\n    }\n  }\n#endif\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> weights_matrix(\n      filter_data, output_depth, accum_depth);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, accum_depth, batches);\n  gemmlowp::MatrixMap<int16, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_depth, batches);\n  typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n      ColVectorMap;\n  ColVectorMap bias_vector(bias_data_int32, output_depth);\n  gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n  bias_addition_stage.bias_vector = bias_vector;\n  gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent scale_stage;\n  scale_stage.result_offset_after_shift = 0;\n  scale_stage.result_fixedpoint_multiplier = output_multiplier;\n  // Note that this shift is negated wrt ordinary FC.\n  scale_stage.result_exponent = output_shift;\n  gemmlowp::OutputStageClamp clamp_stage;\n  clamp_stage.min = output_activation_min;\n  clamp_stage.max = output_activation_max;\n  gemmlowp::OutputStageSaturatingCastToInt16 saturating_cast_int16_stage;\n  auto output_pipeline =\n      std::make_tuple(bias_addition_stage, scale_stage, clamp_stage,\n                      saturating_cast_int16_stage);\n  gemmlowp::GemmWithOutputPipeline<uint8, int16,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, weights_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\ninline void FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                           int32 input_offset, const uint8* filter_data,\n                           const Dims<4>& filter_dims, int32 filter_offset,\n                           const int32* bias_data, const Dims<4>& bias_dims,\n                           int32 output_offset, int32 output_multiplier,\n                           int output_shift, int32 output_activation_min,\n                           int32 output_activation_max, uint8* output_data,\n                           const Dims<4>& output_dims,\n                           gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\ninline void FullyConnected(\n    const uint8* input_data, const Dims<4>& input_dims, int32 input_offset,\n    const uint8* filter_data, const Dims<4>& filter_dims, int32 filter_offset,\n    const int32* bias_data_int32, const Dims<4>& bias_dims, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, int16* output_data, const Dims<4>& output_dims,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data_int32, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_offset, const uint8* filter_data,\n                    const Dims<4>& filter_dims, int32 filter_offset,\n                    const int32* bias_data, const Dims<4>& bias_dims,\n                    int32 output_offset, int32 output_multiplier,\n                    int output_shift, int32 output_activation_min,\n                    int32 output_activation_max, uint8* output_data,\n                    const Dims<4>& output_dims,\n                    gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  FullyConnected(input_data, input_dims, input_offset, filter_data, filter_dims,\n                 filter_offset, bias_data, bias_dims, output_offset,\n                 output_multiplier, output_shift, output_activation_min,\n                 output_activation_max, output_data, output_dims,\n                 gemmlowp_context);\n}\n\n#ifdef USE_NEON\ninline void LegacyInt8FullyConnectedAsGEMVWorkerImpl(\n    const RuntimeShape& input_shape, const int8_t* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const int8_t* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    int8_t* output_data, int row_start, int row_end) {\n  ruy::profiler::ScopeLabel label(\"FullyConnectedAsGEMVInt8/8bit\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kPeel = 4;\n  const bool shift_left = (output_shift > 0);\n  TFLITE_DCHECK_GE(row_end - row_start, kPeel);\n\n  for (int out = row_start; out < row_end; out += kPeel) {\n    out = std::min(out, row_end - kPeel);\n    int32x4_t acc0 = vdupq_n_s32(0);\n    int32x4_t acc1 = acc0;\n    int32x4_t acc2 = acc0;\n    int32x4_t acc3 = acc0;\n    const int16x8_t input_offset_vec = vdupq_n_s16(input_offset);\n    const int16x8_t filter_offset_vec = vdupq_n_s16(filter_offset);\n    int in = 0;\n    for (; in <= input_size - 16; in += 16) {\n      const int8x16_t input_val_s8 = vld1q_s8(input_data + in);\n      const int8_t* filter_ptr = filter_data + in + out * input_size;\n      int8x16_t filter_val_s8_0 = vld1q_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x16_t filter_val_s8_1 = vld1q_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x16_t filter_val_s8_2 = vld1q_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x16_t filter_val_s8_3 = vld1q_s8(filter_ptr);\n      int16x8_t input_val_0, input_val_1;\n      int8x8_t low = vget_low_s8(input_val_s8);\n      int8x8_t high = vget_high_s8(input_val_s8);\n      input_val_0 = vmovl_s8(low);\n      input_val_1 = vmovl_s8(high);\n      input_val_0 = vaddq_s16(input_val_0, input_offset_vec);\n      input_val_1 = vaddq_s16(input_val_1, input_offset_vec);\n      low = vget_low_s8(filter_val_s8_0);\n      high = vget_high_s8(filter_val_s8_0);\n      int16x8_t filter_val_0_0 = vmovl_s8(low);\n      int16x8_t filter_val_0_1 = vmovl_s8(high);\n      filter_val_0_0 = vaddq_s16(filter_val_0_0, filter_offset_vec);\n      filter_val_0_1 = vaddq_s16(filter_val_0_1, filter_offset_vec);\n      low = vget_low_s8(filter_val_s8_1);\n      high = vget_high_s8(filter_val_s8_1);\n      int16x8_t filter_val_1_0 = vmovl_s8(low);\n      int16x8_t filter_val_1_1 = vmovl_s8(high);\n      filter_val_1_0 = vaddq_s16(filter_val_1_0, filter_offset_vec);\n      filter_val_1_1 = vaddq_s16(filter_val_1_1, filter_offset_vec);\n      low = vget_low_s8(filter_val_s8_2);\n      high = vget_high_s8(filter_val_s8_2);\n      int16x8_t filter_val_2_0 = vmovl_s8(low);\n      int16x8_t filter_val_2_1 = vmovl_s8(high);\n      filter_val_2_0 = vaddq_s16(filter_val_2_0, filter_offset_vec);\n      filter_val_2_1 = vaddq_s16(filter_val_2_1, filter_offset_vec);\n      low = vget_low_s8(filter_val_s8_3);\n      high = vget_high_s8(filter_val_s8_3);\n      int16x8_t filter_val_3_0 = vmovl_s8(low);\n      int16x8_t filter_val_3_1 = vmovl_s8(high);\n      filter_val_3_0 = vaddq_s16(filter_val_3_0, filter_offset_vec);\n      filter_val_3_1 = vaddq_s16(filter_val_3_1, filter_offset_vec);\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_0),\n                       vget_low_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_0),\n                       vget_low_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_0),\n                       vget_low_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_0),\n                       vget_low_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_1),\n                       vget_low_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_1),\n                       vget_low_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_1),\n                       vget_low_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_1),\n                       vget_low_s16(input_val_1));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_0),\n                       vget_high_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_0),\n                       vget_high_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_0),\n                       vget_high_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_0),\n                       vget_high_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_1),\n                       vget_high_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_1),\n                       vget_high_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_1),\n                       vget_high_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_1),\n                       vget_high_s16(input_val_1));\n    }\n    for (; in <= input_size - 8; in += 8) {\n      const int8x8_t input_val_s8 = vld1_s8(input_data + in);\n      const int8_t* filter_ptr = filter_data + in + out * input_size;\n      int8x8_t filter_val_s8_0 = vld1_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x8_t filter_val_s8_1 = vld1_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x8_t filter_val_s8_2 = vld1_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x8_t filter_val_s8_3 = vld1_s8(filter_ptr);\n      int16x8_t input_val = vmovl_s8(input_val_s8);\n      input_val = vaddq_s16(input_val, input_offset_vec);\n      int16x8_t filter_val_0 = vmovl_s8(filter_val_s8_0);\n      filter_val_0 = vaddq_s16(filter_val_0, filter_offset_vec);\n      int16x8_t filter_val_1 = vmovl_s8(filter_val_s8_1);\n      filter_val_1 = vaddq_s16(filter_val_1, filter_offset_vec);\n      int16x8_t filter_val_2 = vmovl_s8(filter_val_s8_2);\n      filter_val_2 = vaddq_s16(filter_val_2, filter_offset_vec);\n      int16x8_t filter_val_3 = vmovl_s8(filter_val_s8_3);\n      filter_val_3 = vaddq_s16(filter_val_3, filter_offset_vec);\n      acc0 =\n          vmlal_s16(acc0, vget_low_s16(filter_val_0), vget_low_s16(input_val));\n      acc1 =\n          vmlal_s16(acc1, vget_low_s16(filter_val_1), vget_low_s16(input_val));\n      acc2 =\n          vmlal_s16(acc2, vget_low_s16(filter_val_2), vget_low_s16(input_val));\n      acc3 =\n          vmlal_s16(acc3, vget_low_s16(filter_val_3), vget_low_s16(input_val));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0),\n                       vget_high_s16(input_val));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1),\n                       vget_high_s16(input_val));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2),\n                       vget_high_s16(input_val));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3),\n                       vget_high_s16(input_val));\n    }\n    if (in < input_size) {\n      int32 buf[16];\n      vst1q_s32(buf + 0, acc0);\n      vst1q_s32(buf + 4, acc1);\n      vst1q_s32(buf + 8, acc2);\n      vst1q_s32(buf + 12, acc3);\n      for (; in < input_size; in++) {\n        int lane = (in + 8 - input_size) % 4;\n        const int32 input_val = input_data[in] + input_offset;\n        for (int k = 0; k < kPeel; k++) {\n          int32 filter_val =\n              filter_data[in + (out + k) * input_size] + filter_offset;\n          buf[lane + 4 * k] += filter_val * input_val;\n        }\n      }\n      acc0 = vld1q_s32(buf + 0);\n      acc1 = vld1q_s32(buf + 4);\n      acc2 = vld1q_s32(buf + 8);\n      acc3 = vld1q_s32(buf + 12);\n    }\n\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(acc0), vget_high_s32(acc0));\n    int32x2_t pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(acc1), vget_high_s32(acc1));\n    int32x2_t pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(acc2), vget_high_s32(acc2));\n    int32x2_t pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(acc3), vget_high_s32(acc3));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    int32x4_t bias_vec = vld1q_s32(bias_data + out);\n    reduced = vaddq_s32(reduced, bias_vec);\n    if (shift_left) {\n      const int32 multiplier_power_of_two = 1 << output_shift;\n      reduced = vmulq_n_s32(reduced, multiplier_power_of_two);\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n    } else {\n      // Multiply by the fixed-point multiplier.\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n      // Rounding-shift-right.\n      using gemmlowp::RoundingDivideByPOT;\n      reduced = RoundingDivideByPOT(reduced, -output_shift);\n    }\n    // Add the output offset.\n    const int32x4_t output_offset_vec = vdupq_n_s32(output_offset);\n    reduced = vaddq_s32(reduced, output_offset_vec);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    // Narrow values down to 8 bit signed, saturating.\n    int8x8_t res8 = vqmovn_s16(vcombine_s16(res16, res16));\n    // Apply the clamping from the activation function\n    res8 = vmax_s8(res8, vdup_n_s8(output_activation_min));\n    res8 = vmin_s8(res8, vdup_n_s8(output_activation_max));\n    // Store results to destination.\n    vst1_lane_s8(output_data + out + 0, res8, 0);\n    vst1_lane_s8(output_data + out + 1, res8, 1);\n    vst1_lane_s8(output_data + out + 2, res8, 2);\n    vst1_lane_s8(output_data + out + 3, res8, 3);\n  }\n}\n\nstruct LegacyInt8FullyConnectedAsGEMVWorkerTask : public gemmlowp::Task {\n  LegacyInt8FullyConnectedAsGEMVWorkerTask(\n      const RuntimeShape& input_shape, const int8_t* input_data,\n      int32 input_offset, const RuntimeShape& filter_shape,\n      const int8_t* filter_data, int32 filter_offset,\n      const RuntimeShape& bias_shape, const int32* bias_data,\n      int32 output_offset, int32 output_multiplier, int output_shift,\n      int32 output_activation_min, int32 output_activation_max,\n      const RuntimeShape& output_shape, int8_t* output_data, int row_start,\n      int row_end)\n      : input_shape_(input_shape),\n        input_data_(input_data),\n        input_offset_(input_offset),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        filter_offset_(filter_offset),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_offset_(output_offset),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_activation_min_(output_activation_min),\n        output_activation_max_(output_activation_max),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        row_start_(row_start),\n        row_end_(row_end) {}\n\n  void Run() override {\n    LegacyInt8FullyConnectedAsGEMVWorkerImpl(\n        input_shape_, input_data_, input_offset_, filter_shape_, filter_data_,\n        filter_offset_, bias_shape_, bias_data_, output_offset_,\n        output_multiplier_, output_shift_, output_activation_min_,\n        output_activation_max_, output_shape_, output_data_, row_start_,\n        row_end_);\n  }\n\n  const RuntimeShape& input_shape_;\n  const int8_t* input_data_;\n  int32 input_offset_;\n  const RuntimeShape& filter_shape_;\n  const int8_t* filter_data_;\n  int32 filter_offset_;\n  const RuntimeShape& bias_shape_;\n  const int32* bias_data_;\n  int32 output_offset_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int32 output_activation_min_;\n  int32 output_activation_max_;\n  const RuntimeShape& output_shape_;\n  int8_t* output_data_;\n  int row_start_;\n  int row_end_;\n};\n\ninline void LegacyInt8FullyConnectedAsGEMV(\n    const RuntimeShape& input_shape, const int8_t* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const int8_t* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    int8_t* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kKernelRows = 4;\n  const int thread_count = gemmlowp::HowManyThreads<kKernelRows>(\n      gemmlowp_context->max_num_threads(), output_rows, batches, input_size);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    LegacyInt8FullyConnectedAsGEMVWorkerImpl(\n        input_shape, input_data, input_offset, filter_shape, filter_data,\n        filter_offset, bias_shape, bias_data, output_offset, output_multiplier,\n        output_shift, output_activation_min, output_activation_max,\n        output_shape, output_data, 0, output_rows);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<LegacyInt8FullyConnectedAsGEMVWorkerTask> tasks;\n  // TODO(b/131746020) don't create new heap allocations every time.\n  // At least we make it a single heap allocation by using reserve().\n  tasks.reserve(thread_count);\n  const int kRowsPerWorker = gemmlowp::RoundUp<kKernelRows>(\n      gemmlowp::CeilQuotient(output_rows, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; ++i) {\n    int row_end = std::min(output_rows, row_start + kRowsPerWorker);\n    tasks.emplace_back(input_shape, input_data, input_offset, filter_shape,\n                       filter_data, filter_offset, bias_shape, bias_data,\n                       output_offset, output_multiplier, output_shift,\n                       output_activation_min, output_activation_max,\n                       output_shape, output_data, row_start, row_end);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_rows);\n  gemmlowp_context->workers_pool()->Execute(tasks.size(), tasks.data());\n}\n#endif  // USE_NEON\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const int8* input_data, const RuntimeShape& filter_shape,\n    const int8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape, int8* output_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnectedInt8/8bit\");\n\n#ifdef USE_NEON\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  if (batches == 1) {\n    const int output_size = MatchingDim(filter_shape, filter_dim_count - 2,\n                                        output_shape, output_dim_count - 1);\n    if (output_size >= 4) {\n      return LegacyInt8FullyConnectedAsGEMV(\n          input_shape, input_data, input_offset, filter_shape, filter_data,\n          filter_offset, bias_shape, bias_data, output_offset,\n          output_multiplier, output_shift, output_activation_min,\n          output_activation_max, output_shape, output_data, gemmlowp_context);\n    }\n  }\n#endif  // USE_NEON\n\n#ifdef GEMMLOWP_NEON\n  const int filter_rows = filter_shape.Dims(filter_dim_count - 2);\n  const int filter_cols = filter_shape.Dims(filter_dim_count - 1);\n  TFLITE_DCHECK_EQ(filter_shape.FlatSize(), filter_rows * filter_cols);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n  gemmlowp::MatrixMap<const int8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, output_rows, filter_cols, filter_cols);\n  gemmlowp::MatrixMap<const int8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, filter_cols, batches, filter_cols);\n  gemmlowp::MatrixMap<int8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, batches, output_rows);\n  const auto& output_pipeline = GemmlowpOutputPipelineInt8::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, output_shift,\n      output_activation_min, output_activation_max);\n\n  gemmlowp::GemmWithOutputPipeline<\n      int8, int8, gemmlowp::SignedL8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n  return;\n#endif  // GEMMLOWP_NEON\n\n  // If both GEMMLOWP_NEON && NEON paths are skipped, fallback to reference\n  // implementation.\n  reference_integer_ops::FullyConnected(params, input_shape, input_data,\n                                        filter_shape, filter_data, bias_shape,\n                                        bias_data, output_shape, output_data);\n}\n\nstruct LegacyShuffledFullyConnectedWorkerTask : gemmlowp::Task {\n  LegacyShuffledFullyConnectedWorkerTask(const uint8* input_data,\n                                         const int8* shuffled_weights_data,\n                                         int batches, int output_depth,\n                                         int output_stride, int accum_depth,\n                                         const int32* bias_data,\n                                         int32 output_multiplier,\n                                         int output_shift, int16* output_data)\n      : input_data_(input_data),\n        shuffled_weights_data_(shuffled_weights_data),\n        batches_(batches),\n        output_depth_(output_depth),\n        output_stride_(output_stride),\n        accum_depth_(accum_depth),\n        bias_data_(bias_data),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_data_(output_data) {}\n\n  void Run() override {\n    ShuffledFullyConnectedWorkerImpl(\n        input_data_, shuffled_weights_data_, batches_, output_depth_,\n        output_stride_, accum_depth_, bias_data_, output_multiplier_,\n        output_shift_, output_data_);\n  }\n\n  const uint8* input_data_;\n  const int8* shuffled_weights_data_;\n  int batches_;\n  int output_depth_;\n  int output_stride_;\n  int accum_depth_;\n  const int32* bias_data_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int16* output_data_;\n};\n\ninline void ShuffledFullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& weights_shape,\n    const uint8* shuffled_weights_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"ShuffledFullyConnected/8bit\");\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  (void)gemmlowp_context;  // only used in optimized code.\n  TFLITE_DCHECK_EQ(output_activation_min, -32768);\n  TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(weights_shape, weights_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = weights_shape.Dims(weights_dim_count - 1);\n  TFLITE_DCHECK((accum_depth % 16) == 0);\n  TFLITE_DCHECK((output_depth % 4) == 0);\n  // Shuffled weights have had their sign bit (0x80) pre-flipped (xor'd)\n  // so that just reinterpreting them as int8 values is equivalent to\n  // subtracting 128 from them, thus implementing for free the subtraction of\n  // the zero_point value 128.\n  const int8* int8_shuffled_weights_data =\n      reinterpret_cast<const int8*>(shuffled_weights_data);\n\n  // Shuffling and xoring of input activations into the workspace buffer\n  if (batches == 1) {\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (int i = 0; i < accum_depth; i += 16) {\n      uint8x16_t val = vld1q_u8(input_data + i);\n      val = veorq_u8(val, signbit);\n      vst1q_u8(shuffled_input_workspace_data + i, val);\n    }\n#else\n    for (int i = 0; i < accum_depth; i++) {\n      shuffled_input_workspace_data[i] = input_data[i] ^ 0x80;\n    }\n#endif\n  } else if (batches == 4) {\n    uint8* shuffled_input_workspace_ptr = shuffled_input_workspace_data;\n    int c = 0;\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (c = 0; c < accum_depth; c += 16) {\n      const uint8* src_data_ptr = input_data + c;\n      uint8x16_t val0 = vld1q_u8(src_data_ptr + 0 * accum_depth);\n      uint8x16_t val1 = vld1q_u8(src_data_ptr + 1 * accum_depth);\n      uint8x16_t val2 = vld1q_u8(src_data_ptr + 2 * accum_depth);\n      uint8x16_t val3 = vld1q_u8(src_data_ptr + 3 * accum_depth);\n      val0 = veorq_u8(val0, signbit);\n      val1 = veorq_u8(val1, signbit);\n      val2 = veorq_u8(val2, signbit);\n      val3 = veorq_u8(val3, signbit);\n      vst1q_u8(shuffled_input_workspace_ptr + 0, val0);\n      vst1q_u8(shuffled_input_workspace_ptr + 16, val1);\n      vst1q_u8(shuffled_input_workspace_ptr + 32, val2);\n      vst1q_u8(shuffled_input_workspace_ptr + 48, val3);\n      shuffled_input_workspace_ptr += 64;\n    }\n#else\n    for (c = 0; c < accum_depth; c += 16) {\n      for (int b = 0; b < 4; b++) {\n        const uint8* src_data_ptr = input_data + b * accum_depth + c;\n        for (int j = 0; j < 16; j++) {\n          uint8 src_val = *src_data_ptr++;\n          // Flip the sign bit, so that the kernel will only need to\n          // reinterpret these uint8 values as int8, getting for free the\n          // subtraction of the zero_point value 128.\n          uint8 dst_val = src_val ^ 0x80;\n          *shuffled_input_workspace_ptr++ = dst_val;\n        }\n      }\n    }\n#endif\n  } else {\n    TFLITE_DCHECK(false);\n    return;\n  }\n\n  static constexpr int kKernelRows = 4;\n  const int thread_count = gemmlowp::HowManyThreads<kKernelRows>(\n      gemmlowp_context->max_num_threads(), output_depth, batches, accum_depth);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    ShuffledFullyConnectedWorkerImpl(\n        shuffled_input_workspace_data, int8_shuffled_weights_data, batches,\n        output_depth, output_depth, accum_depth, bias_data, output_multiplier,\n        output_shift, output_data);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<gemmlowp::Task*> tasks(thread_count);\n  const int kRowsPerWorker = gemmlowp::RoundUp<kKernelRows>(\n      gemmlowp::CeilQuotient(output_depth, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; i++) {\n    int row_end = std::min(output_depth, row_start + kRowsPerWorker);\n    tasks[i] = new LegacyShuffledFullyConnectedWorkerTask(\n        shuffled_input_workspace_data,\n        int8_shuffled_weights_data + row_start * accum_depth, batches,\n        row_end - row_start, output_depth, accum_depth, bias_data + row_start,\n        output_multiplier, output_shift, output_data + row_start);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_depth);\n  gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n}\n\ninline void ShuffledFullyConnected(\n    const uint8* input_data, const Dims<4>& input_dims,\n    const uint8* shuffled_weights_data, const Dims<4>& weights_dims,\n    const int32* bias_data, const Dims<4>& bias_dims, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    int16* output_data, const Dims<4>& output_dims,\n    uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  ShuffledFullyConnected(op_params, DimsToShape(input_dims), input_data,\n                         DimsToShape(weights_dims), shuffled_weights_data,\n                         DimsToShape(bias_dims), bias_data,\n                         DimsToShape(output_dims), output_data,\n                         shuffled_input_workspace_data, gemmlowp_context);\n}\n\ntemplate <typename T>\ninline void ExtractPatchIntoBufferColumn(\n    const Dims<4>& input_dims, int w, int h, int b, int kheight, int kwidth,\n    int stride_width, int stride_height, int pad_width, int pad_height,\n    int in_width, int in_height, int in_depth, int single_buffer_length,\n    int buffer_id, const T* in_data, T* conv_buffer_data, uint8 zero_byte) {\n  ExtractPatchIntoBufferColumn(\n      DimsToShape(input_dims), w, h, b, kheight, kwidth, stride_width,\n      stride_height, pad_width, pad_height, in_width, in_height, in_depth,\n      single_buffer_length, buffer_id, in_data, conv_buffer_data, zero_byte);\n}\n\ntemplate <typename T>\nvoid DilatedIm2col(const T* input_data, const Dims<4>& input_dims,\n                   const Dims<4>& filter_dims, int stride_width,\n                   int stride_height, int dilation_width_factor,\n                   int dilation_height_factor, int pad_width, int pad_height,\n                   const Dims<4>& output_dims, uint8 zero_byte,\n                   T* im2col_data) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n\n  DilatedIm2col(op_params, zero_byte, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), DimsToShape(output_dims),\n                im2col_data);\n}\n\ntemplate <typename T>\nvoid Im2col(const T* input_data, const Dims<4>& input_dims, int stride_width,\n            int stride_height, int pad_width, int pad_height, int kheight,\n            int kwidth, uint8 zero_byte, T* output_data,\n            const Dims<4>& output_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = 1;\n  op_params.dilation_height_factor = 1;\n\n  Im2col(op_params, kheight, kwidth, zero_byte, DimsToShape(input_dims),\n         input_data, DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <typename T>\nvoid Im2col(const T* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int kheight, int kwidth,\n            uint8 zero_byte, T* output_data, const Dims<4>& output_dims) {\n  Im2col(input_data, input_dims, stride, stride, pad_width, pad_height, kheight,\n         kwidth, zero_byte, output_data, output_dims);\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const float* input_data, const RuntimeShape& filter_shape,\n                 const float* filter_data, const RuntimeShape& bias_shape,\n                 const float* bias_data, const RuntimeShape& output_shape,\n                 float* output_data, const RuntimeShape& im2col_shape,\n                 float* im2col_data) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  (void)im2col_data;\n  (void)im2col_shape;\n  ruy::profiler::ScopeLabel label(\"Conv\");\n\n  // NB: the float 0.0f value is represented by all zero bytes.\n  const uint8 float_zero_byte = 0x00;\n  const float* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    DilatedIm2col(params, float_zero_byte, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    Im2col(params, filter_height, filter_width, float_zero_byte, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    // TODO(aselle): We need to make sure to not send im2col if it is not\n    // needed.\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  // The following code computes matrix multiplication c = a * transponse(b)\n  // with CBLAS, where:\n  // * `a` is a matrix with dimensions (m, k).\n  // * `b` is a matrix with dimensions (n, k), so transpose(b) is (k, n).\n  // * `c` is a matrix with dimensions (m, n).\n  // The naming of variables are aligned with CBLAS specification here.\n  const float* a = gemm_input_data;\n  const float* b = filter_data;\n  float* c = output_data;\n  const int gemm_input_dims = gemm_input_shape->DimensionsCount();\n  int m = FlatSizeSkipDim(*gemm_input_shape, gemm_input_dims - 1);\n  int n = output_shape.Dims(3);\n  int k = gemm_input_shape->Dims(gemm_input_dims - 1);\n\n#if defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n  // The stride of matrix a, b and c respectively.\n  int stride_a = k;\n  int stride_b = k;\n  int stride_c = n;\n\n  cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans, m, n, k, 1.0f, a,\n              stride_a, b, stride_b, 0.0f, c, stride_c);\n#else\n  // When an optimized CBLAS implementation is not available, fall back\n  // to using Eigen.\n  typedef Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>\n      Matrix;\n  typedef Eigen::Map<Matrix> MatrixRef;\n  typedef Eigen::Map<const Matrix> ConstMatrixRef;\n\n  MatrixRef matrix_c(c, m, n);\n  ConstMatrixRef matrix_a(a, m, k);\n  ConstMatrixRef matrix_b(b, n, k);\n\n  // The following special casing for when a or b is a vector is required\n  // as Eigen seem to fail to make this optimization on its own.\n  if (n == 1) {\n    ruy::profiler::ScopeLabel label(\"GEMV\");\n    matrix_c.col(0).noalias() = matrix_a * matrix_b.row(0).transpose();\n  } else if (m == 1) {\n    ruy::profiler::ScopeLabel label(\"GEMV\");\n    matrix_c.row(0).noalias() = matrix_a.row(0) * matrix_b.transpose();\n  } else {\n    ruy::profiler::ScopeLabel label(\"GEMM\");\n    matrix_c.noalias() = matrix_a * matrix_b.transpose();\n  }\n\n#endif  //  defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n\n  optimized_ops::AddBiasAndEvalActivationFunction(\n      output_activation_min, output_activation_max, bias_shape, bias_data,\n      output_shape, output_data);\n}\n\ninline void Conv(const float* input_data, const Dims<4>& input_dims,\n                 const float* filter_data, const Dims<4>& filter_dims,\n                 const float* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 float output_activation_min, float output_activation_max,\n                 float* output_data, const Dims<4>& output_dims,\n                 float* im2col_data, const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data);\n}\n\ninline void HybridConv(const int8_t* input_data, const Dims<4>& input_dims,\n                       const int8_t* filter_data, const Dims<4>& filter_dims,\n                       const float* bias_data, const Dims<4>& bias_dims,\n                       int stride_width, int stride_height, int pad_width,\n                       int pad_height, float* scaling_factors_ptr,\n                       float output_activation_min, float output_activation_max,\n                       int32_t* scratch_data, const Dims<4>& scratch_dims,\n                       float* output_data, const Dims<4>& output_dims,\n                       int8_t* im2col_data, const Dims<4>& im2col_dims,\n                       CpuBackendContext* context) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  HybridConv(op_params, scaling_factors_ptr, DimsToShape(input_dims),\n             input_data, DimsToShape(filter_dims), filter_data,\n             DimsToShape(bias_dims), bias_data, DimsToShape(scratch_dims),\n             scratch_data, DimsToShape(output_dims), output_data,\n             DimsToShape(im2col_dims), im2col_data, context);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int dilation_width_factor,\n          int dilation_height_factor, int pad_width, int pad_height,\n          float* output_data, const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, dilation_width_factor,\n       dilation_height_factor, pad_width, pad_height, output_activation_min,\n       output_activation_max, output_data, output_dims, im2col_data,\n       im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, 1, 1, pad_width, pad_height,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  Conv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n           bias_dims, stride, stride, 1, 1, pad_width, pad_height, output_data,\n           output_dims, im2col_data, im2col_dims);\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const uint8* input_data, const RuntimeShape& filter_shape,\n                 const uint8* filter_data, const RuntimeShape& bias_shape,\n                 const int32* bias_data, const RuntimeShape& output_shape,\n                 uint8* output_data, const RuntimeShape& im2col_shape,\n                 uint8* im2col_data, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"Conv/8bit\");\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const uint8* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    DilatedIm2col(params, input_zero_point, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    Im2col(params, filter_height, filter_width, input_zero_point, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  const int gemm_input_rows = gemm_input_shape->Dims(3);\n  // Using FlatSizeSkipDim causes segfault in some contexts (see b/79927784).\n  // The root cause has not yet been identified though. Same applies below for\n  // the other calls commented out. This is a partial rollback of cl/196819423.\n  // const int gemm_input_cols = FlatSizeSkipDim(*gemm_input_shape, 3);\n  const int gemm_input_cols = gemm_input_shape->Dims(0) *\n                              gemm_input_shape->Dims(1) *\n                              gemm_input_shape->Dims(2);\n  const int filter_rows = filter_shape.Dims(0);\n  // See b/79927784.\n  // const int filter_cols = FlatSizeSkipDim(filter_shape, 0);\n  const int filter_cols =\n      filter_shape.Dims(1) * filter_shape.Dims(2) * filter_shape.Dims(3);\n  const int output_rows = output_shape.Dims(3);\n  // See b/79927784.\n  // const int output_cols = FlatSizeSkipDim(output_shape, 3);\n  const int output_cols =\n      output_shape.Dims(0) * output_shape.Dims(1) * output_shape.Dims(2);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, gemm_input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, gemm_input_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n#ifdef USE_NEON\n  if (gemm_input_cols == 1 && output_rows >= 4) {\n    RuntimeShape fc_filter_shape{\n        filter_shape.Dims(0),\n        filter_shape.Dims(filter_shape.DimensionsCount() - 1)};\n\n    return FullyConnectedAsGEMV(\n        *gemm_input_shape, gemm_input_data, input_offset, fc_filter_shape,\n        filter_data, filter_offset, bias_shape, bias_data, output_offset,\n        output_multiplier, output_shift, output_activation_min,\n        output_activation_max, output_shape, output_data, gemmlowp_context);\n  }\n#endif\n\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, filter_rows, filter_cols);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      gemm_input_data, gemm_input_rows, gemm_input_cols);\n  gemmlowp::MatrixMap<uint8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, output_cols);\n  const auto& output_pipeline = GemmlowpOutputPipeline::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, output_shift,\n      output_activation_min, output_activation_max);\n  gemmlowp::GemmWithOutputPipeline<uint8, uint8,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 int32 output_offset, int32 output_multiplier, int output_shift,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims,\n                 uint8* im2col_data, const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data, gemmlowp_context);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height, 1, 1,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const uint8* input_data, const Dims<4>& input_dims,\n          int32 input_offset, const uint8* filter_data,\n          const Dims<4>& filter_dims, int32 filter_offset,\n          const int32* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, int32 output_offset,\n          int32 output_multiplier, int output_shift,\n          int32 output_activation_min, int32 output_activation_max,\n          uint8* output_data, const Dims<4>& output_dims, uint8* im2col_data,\n          const Dims<4>& im2col_dims, gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride, stride, pad_width,\n       pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid Im2col(const T* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int kheight, int kwidth,\n            uint8 zero_byte, T* output_data, const Dims<4>& output_dims) {\n  Im2col(input_data, input_dims, stride, stride, pad_width, pad_height, kheight,\n         kwidth, zero_byte, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid ConvAsGemm(const float* input_data, const Dims<4>& input_dims,\n                const float* filter_data, const Dims<4>& filter_dims,\n                const float* bias_data, const Dims<4>& bias_dims,\n                float* output_data, const Dims<4>& output_dims) {\n  ruy::profiler::ScopeLabel label(\"ConvAsGemm\");\n\n  const auto input_matrix_map =\n      MapAsMatrixWithFirstDimAsRows(input_data, input_dims);\n  const auto filter_matrix_map =\n      MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);\n  auto output_matrix_map =\n      MapAsMatrixWithFirstDimAsRows(output_data, output_dims);\n\n  Gemm(filter_matrix_map.transpose(), input_matrix_map, &output_matrix_map);\n\n  AddBiasAndEvalActivationFunction<Ac>(bias_data, bias_dims, output_data,\n                                       output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid ConvAsGemm(const uint8* input_data, const Dims<4>& input_dims,\n                int32 input_offset, const uint8* filter_data,\n                const Dims<4>& filter_dims, int32 filter_offset,\n                const int32* bias_data, const Dims<4>& bias_dims,\n                int32 output_offset, int32 output_multiplier, int output_shift,\n                int32 output_activation_min, int32 output_activation_max,\n                uint8* output_data, const Dims<4>& output_dims,\n                gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"ConvAsGemm/8bit\");\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  const int input_rows = input_dims.sizes[0];\n  const int input_cols = FlatSizeSkipDim(input_dims, 0);\n  const int filter_rows = filter_dims.sizes[3];\n  const int filter_cols = FlatSizeSkipDim(filter_dims, 3);\n  const int output_rows = output_dims.sizes[0];\n  const int output_cols = FlatSizeSkipDim(output_dims, 0);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, input_rows);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[0], output_rows);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[1], 1);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[2], 1);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[3], 1);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, output_rows, filter_cols, filter_cols);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, filter_cols, output_cols, filter_cols);\n  gemmlowp::MatrixMap<uint8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, output_cols, output_rows);\n  const auto& output_pipeline = GemmlowpOutputPipeline::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, -output_shift,\n      output_activation_min, output_activation_max);\n  gemmlowp::GemmWithOutputPipeline<uint8, uint8,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\ninline void TransposeConv(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& output_shape,\n    float* output_data, const RuntimeShape& im2col_shape, float* im2col_data) {\n  ruy::profiler::ScopeLabel label(\"TransposeConv\");\n  // Note we could use transposed weights with forward conv for unstrided\n  // cases. But we are already getting good performance with this code as-is.\n  TFLITE_DCHECK(im2col_data);\n  TransposeIm2col(params, 0, input_shape, input_data, filter_shape,\n                  output_shape, im2col_data);\n\n  const auto im2col_matrix_map =\n      MapAsMatrixWithLastDimAsRows(im2col_data, im2col_shape);\n  const auto filter_matrix_map =\n      MapAsMatrixWithFirstDimAsCols(filter_data, filter_shape);\n  auto output_matrix_map =\n      MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n\n  Gemm(filter_matrix_map.transpose(), im2col_matrix_map, &output_matrix_map);\n}\n\ninline void TransposeConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, float* output_data,\n                          const Dims<4>& output_dims, float* im2col_data,\n                          const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n\n  TransposeConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data, DimsToShape(output_dims),\n                output_data, DimsToShape(im2col_dims), im2col_data);\n}\n\ninline void TransposeConvV2(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& hwoi_ordered_filter_shape,\n    const float* hwoi_ordered_filter_data, const RuntimeShape& output_shape,\n    float* output_data, const RuntimeShape& col2im_shape, float* col2im_data,\n    CpuBackendContext* cpu_backend_context) {\n  TransposeConvV2(params, input_shape, input_data, hwoi_ordered_filter_shape,\n                  hwoi_ordered_filter_data, /*bias_shape*/ RuntimeShape(),\n                  /*bias_data*/ nullptr, output_shape, output_data,\n                  col2im_shape, col2im_data, cpu_backend_context);\n}\n\ntemplate <typename T>\nvoid TransposeIm2col(const T* input_data, const Dims<4>& input_dims,\n                     const Dims<4>& filter_dims, int stride_width,\n                     int stride_height, int pad_width, int pad_height,\n                     const Dims<4>& output_dims, uint8 zero_byte,\n                     T* im2col_data) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n\n  TransposeIm2col(op_params, zero_byte, DimsToShape(input_dims), input_data,\n                  DimsToShape(filter_dims), DimsToShape(output_dims),\n                  im2col_data);\n}\n\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const float* input_data, const RuntimeShape& unextended_prev_activ_shape,\n    const float* prev_activ_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& unextended_bias_shape,\n    const float* bias_data, const RuntimeShape& unextended_prev_state_shape,\n    const float* prev_state_data,\n    const RuntimeShape& unextended_output_state_shape, float* output_state_data,\n    const RuntimeShape& unextended_output_activ_shape, float* output_activ_data,\n    const RuntimeShape& unextended_concat_temp_shape, float* concat_temp_data,\n    const RuntimeShape& unextended_activ_temp_shape, float* activ_temp_data) {\n  ruy::profiler::ScopeLabel label(\"LstmCell\");\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  MatchingDim(  // batches\n      input_shape, 0, prev_activ_shape, 0, prev_state_shape, 0,\n      output_state_shape, 0, output_activ_shape, 0);\n  MatchingDim(  // height\n      input_shape, 1, prev_activ_shape, 1, prev_state_shape, 1,\n      output_state_shape, 1, output_activ_shape, 1);\n  MatchingDim(  // width\n      input_shape, 2, prev_activ_shape, 2, prev_state_shape, 2,\n      output_state_shape, 2, output_activ_shape, 2);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n\n  // Concatenate prev_activ and input data together\n  std::vector<float const*> concat_input_arrays_data;\n  std::vector<RuntimeShape const*> concat_input_arrays_shapes;\n  concat_input_arrays_data.push_back(input_data);\n  concat_input_arrays_data.push_back(prev_activ_data);\n  concat_input_arrays_shapes.push_back(&input_shape);\n  concat_input_arrays_shapes.push_back(&prev_activ_shape);\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = concat_input_arrays_data.size();\n  Concatenation(concat_params, &(concat_input_arrays_shapes[0]),\n                &(concat_input_arrays_data[0]), concat_temp_shape,\n                concat_temp_data);\n\n  // Fully connected\n  tflite::FullyConnectedParams fc_params;\n  fc_params.float_activation_min = std::numeric_limits<float>::lowest();\n  fc_params.float_activation_max = std::numeric_limits<float>::max();\n  FullyConnected(fc_params, concat_temp_shape, concat_temp_data, weights_shape,\n                 weights_data, bias_shape, bias_data, activ_temp_shape,\n                 activ_temp_data);\n\n  // Map raw arrays to Eigen arrays so we can use Eigen's optimized array\n  // operations.\n  ArrayMap<float> activ_temp_map =\n      MapAsArrayWithLastDimAsRows(activ_temp_data, activ_temp_shape);\n  auto input_gate_sm = activ_temp_map.block(0 * output_depth, 0, output_depth,\n                                            activ_temp_map.cols());\n  auto new_input_sm = activ_temp_map.block(1 * output_depth, 0, output_depth,\n                                           activ_temp_map.cols());\n  auto forget_gate_sm = activ_temp_map.block(2 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  auto output_gate_sm = activ_temp_map.block(3 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  ArrayMap<const float> prev_state_map =\n      MapAsArrayWithLastDimAsRows(prev_state_data, prev_state_shape);\n  ArrayMap<float> output_state_map =\n      MapAsArrayWithLastDimAsRows(output_state_data, output_state_shape);\n  ArrayMap<float> output_activ_map =\n      MapAsArrayWithLastDimAsRows(output_activ_data, output_activ_shape);\n\n  // Combined memory state and final output calculation\n  ruy::profiler::ScopeLabel label2(\"MemoryStateAndFinalOutput\");\n  output_state_map =\n      input_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          new_input_sm.tanh() +\n      forget_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          prev_state_map;\n  output_activ_map =\n      output_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n      output_state_map.tanh();\n}\n\ninline void LstmCell(const float* input_data, const Dims<4>& input_dims,\n                     const float* prev_activ_data,\n                     const Dims<4>& prev_activ_dims, const float* weights_data,\n                     const Dims<4>& weights_dims, const float* bias_data,\n                     const Dims<4>& bias_dims, const float* prev_state_data,\n                     const Dims<4>& prev_state_dims, float* output_state_data,\n                     const Dims<4>& output_state_dims, float* output_activ_data,\n                     const Dims<4>& output_activ_dims, float* concat_temp_data,\n                     const Dims<4>& concat_temp_dims, float* activ_temp_data,\n                     const Dims<4>& activ_temp_dims) {\n  tflite::LstmCellParams op_params;\n  // Float LSTM cell does not need parameters to be set: leave untouched.\n\n  LstmCell(op_params, DimsToShape(input_dims), input_data,\n           DimsToShape(prev_activ_dims), prev_activ_data,\n           DimsToShape(weights_dims), weights_data, DimsToShape(bias_dims),\n           bias_data, DimsToShape(prev_state_dims), prev_state_data,\n           DimsToShape(output_state_dims), output_state_data,\n           DimsToShape(output_activ_dims), output_activ_data,\n           DimsToShape(concat_temp_dims), concat_temp_data,\n           DimsToShape(activ_temp_dims), activ_temp_data);\n}\n\ntemplate <int StateIntegerBits>\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const uint8* input_data_uint8,\n    const RuntimeShape& unextended_prev_activ_shape,\n    const uint8* prev_activ_data_uint8, const RuntimeShape& weights_shape,\n    const uint8* weights_data_uint8, const RuntimeShape& unextended_bias_shape,\n    const int32* bias_data_int32,\n    const RuntimeShape& unextended_prev_state_shape,\n    const int16* prev_state_data_int16,\n    const RuntimeShape& unextended_output_state_shape,\n    int16* output_state_data_int16,\n    const RuntimeShape& unextended_output_activ_shape,\n    uint8* output_activ_data_uint8,\n    const RuntimeShape& unextended_concat_temp_shape,\n    uint8* concat_temp_data_uint8,\n    const RuntimeShape& unextended_activ_temp_shape,\n    int16* activ_temp_data_int16, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\n      \"LstmCell/quantized (8bit external, 16bit internal)\");\n  int32 weights_zero_point = params.weights_zero_point;\n  int32 accum_multiplier = params.accum_multiplier;\n  int accum_shift = params.accum_shift;\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  // Gather dimensions information, and perform consistency checks.\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int outer_size = MatchingFlatSizeSkipDim(\n      input_shape, 3, prev_activ_shape, prev_state_shape, output_state_shape,\n      output_activ_shape);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n  const int fc_batches = FlatSizeSkipDim(activ_temp_shape, 3);\n  const int fc_output_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, activ_temp_shape, 3);\n  const int fc_accum_depth = total_input_depth;\n  TFLITE_DCHECK_EQ(fc_output_depth, 4 * output_depth);\n\n  // Depth-concatenate prev_activ and input data together.\n  uint8 const* concat_input_arrays_data[2] = {input_data_uint8,\n                                              prev_activ_data_uint8};\n  const RuntimeShape* concat_input_arrays_shapes[2] = {&input_shape,\n                                                       &prev_activ_shape};\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = 2;\n  Concatenation(concat_params, concat_input_arrays_shapes,\n                concat_input_arrays_data, concat_temp_shape,\n                concat_temp_data_uint8);\n\n  // Implementation of the fully connected node inside the LSTM cell.\n  // The operands are 8-bit integers, the accumulators are internally 32bit\n  // integers, and the output is 16-bit fixed-point with 3 integer bits so\n  // the output range is [-2^3, 2^3] == [-8, 8]. The rationale for that\n  // is explained in the function comment above.\n  bool gemm_already_performed = false;\n#ifdef GEMMLOWP_NEON\n  if (fc_batches == 1 && !(fc_output_depth % 4) && !(fc_accum_depth % 8)) {\n    GEMVForLstmCell(concat_temp_shape, concat_temp_data_uint8, weights_shape,\n                    weights_data_uint8, weights_zero_point, bias_shape,\n                    bias_data_int32, accum_multiplier, accum_shift,\n                    activ_temp_shape, activ_temp_data_int16);\n    gemm_already_performed = true;\n  }\n#endif\n  if (!gemm_already_performed) {\n    gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor>\n        weights_matrix(weights_data_uint8, fc_output_depth, fc_accum_depth);\n    gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n        concat_temp_data_uint8, fc_accum_depth, fc_batches);\n    gemmlowp::MatrixMap<int16, gemmlowp::MapOrder::ColMajor> output_matrix(\n        activ_temp_data_int16, fc_output_depth, fc_batches);\n    typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n        ColVectorMap;\n    ColVectorMap bias_vector(bias_data_int32, fc_output_depth);\n    gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n    bias_addition_stage.bias_vector = bias_vector;\n    gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent scale_stage;\n    scale_stage.result_offset_after_shift = 0;\n    scale_stage.result_fixedpoint_multiplier = accum_multiplier;\n    scale_stage.result_exponent = accum_shift;\n    gemmlowp::OutputStageSaturatingCastToInt16 saturating_cast_int16_stage;\n    auto output_pipeline = std::make_tuple(bias_addition_stage, scale_stage,\n                                           saturating_cast_int16_stage);\n    gemmlowp::GemmWithOutputPipeline<\n        uint8, int16, gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n        gemmlowp_context, weights_matrix, input_matrix, &output_matrix,\n        -weights_zero_point, -128, output_pipeline);\n  }\n\n  // Rest of the LSTM cell: tanh and logistic math functions, and some adds\n  // and muls, all done in 16-bit fixed-point.\n  const int16* input_gate_input_ptr = activ_temp_data_int16;\n  const int16* input_modulation_gate_input_ptr =\n      activ_temp_data_int16 + output_depth;\n  const int16* forget_gate_input_ptr = activ_temp_data_int16 + 2 * output_depth;\n  const int16* output_gate_input_ptr = activ_temp_data_int16 + 3 * output_depth;\n  const int16* prev_state_ptr = prev_state_data_int16;\n  int16* output_state_data_ptr = output_state_data_int16;\n  uint8* output_activ_data_ptr = output_activ_data_uint8;\n\n  for (int b = 0; b < outer_size; ++b) {\n    int c = 0;\n#ifdef GEMMLOWP_NEON\n    for (; c <= output_depth - 8; c += 8) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<int16x8_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<int16x8_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(vld1q_s16(input_gate_input_ptr));\n      input_gate_input_ptr += 8;\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(vld1q_s16(input_modulation_gate_input_ptr));\n      input_modulation_gate_input_ptr += 8;\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(vld1q_s16(forget_gate_input_ptr));\n      forget_gate_input_ptr += 8;\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(vld1q_s16(output_gate_input_ptr));\n      output_gate_input_ptr += 8;\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(vld1q_s16(prev_state_ptr));\n      prev_state_ptr += 8;\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      vst1q_s16(output_state_data_ptr, new_state.raw());\n      output_state_data_ptr += 8;\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16x8_t rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int8x8_t int8_output_activ = vqmovn_s16(rescaled_output_activ);\n      uint8x8_t uint8_output_activ =\n          vadd_u8(vdup_n_u8(128), vreinterpret_u8_s8(int8_output_activ));\n      vst1_u8(output_activ_data_ptr, uint8_output_activ);\n      output_activ_data_ptr += 8;\n    }\n#endif\n    for (; c < output_depth; ++c) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<std::int16_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<std::int16_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(*input_gate_input_ptr++);\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(*input_modulation_gate_input_ptr++);\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(*forget_gate_input_ptr++);\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(*output_gate_input_ptr++);\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(*prev_state_ptr++);\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      *output_state_data_ptr++ = new_state.raw();\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16 rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int16 clamped_output_activ =\n          std::max<int16>(-128, std::min<int16>(127, rescaled_output_activ));\n      *output_activ_data_ptr++ = 128 + clamped_output_activ;\n    }\n    input_gate_input_ptr += 3 * output_depth;\n    input_modulation_gate_input_ptr += 3 * output_depth;\n    forget_gate_input_ptr += 3 * output_depth;\n    output_gate_input_ptr += 3 * output_depth;\n  }\n}\n\ntemplate <int StateIntegerBits>\nvoid LstmCell(const uint8* input_data_uint8, const Dims<4>& input_dims,\n              const uint8* prev_activ_data_uint8,\n              const Dims<4>& prev_activ_dims, const uint8* weights_data_uint8,\n              const Dims<4>& weights_dims, const int32* bias_data_int32,\n              const Dims<4>& bias_dims, const int16* prev_state_data_int16,\n              const Dims<4>& prev_state_dims, int16* output_state_data_int16,\n              const Dims<4>& output_state_dims, uint8* output_activ_data_uint8,\n              const Dims<4>& output_activ_dims, uint8* concat_temp_data_uint8,\n              const Dims<4>& concat_temp_dims, int16* activ_temp_data_int16,\n              const Dims<4>& activ_temp_dims, int32 weights_zero_point,\n              int32 accum_multiplier, int accum_shift,\n              gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::LstmCellParams op_params;\n  op_params.weights_zero_point = weights_zero_point;\n  op_params.accum_multiplier = accum_multiplier;\n  op_params.accum_shift = accum_shift;\n\n  LstmCell<StateIntegerBits>(\n      op_params, DimsToShape(input_dims), input_data_uint8,\n      DimsToShape(prev_activ_dims), prev_activ_data_uint8,\n      DimsToShape(weights_dims), weights_data_uint8, DimsToShape(bias_dims),\n      bias_data_int32, DimsToShape(prev_state_dims), prev_state_data_int16,\n      DimsToShape(output_state_dims), output_state_data_int16,\n      DimsToShape(output_activ_dims), output_activ_data_uint8,\n      DimsToShape(concat_temp_dims), concat_temp_data_uint8,\n      DimsToShape(activ_temp_dims), activ_temp_data_int16, gemmlowp_context);\n}\n\ntemplate <typename T>\nvoid BroadcastDiv(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastDivSlow(op_params, DimsToShape(input1_dims), input1_data,\n                   DimsToShape(input2_dims), input2_data,\n                   DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const RuntimeShape& input_shape,\n                     float* output_data, const RuntimeShape& output_shape) {\n  static_assert(Ac == FusedActivationFunctionType::kNone, \"\");\n  tflite::L2NormalizationParams op_params;\n  // No params need to be set for float, but reserved in signature for future\n  // activations.\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ninline void L2Normalization(const uint8* input_data,\n                            const RuntimeShape& input_shape,\n                            int32 input_zero_point, uint8* output_data,\n                            const RuntimeShape& output_shape) {\n  tflite::L2NormalizationParams op_params;\n  op_params.input_zero_point = input_zero_point;\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  L2Normalization<Ac>(input_data, DimsToShape(input_dims), output_data,\n                      DimsToShape(output_dims));\n}\n\ninline void L2Normalization(const uint8* input_data, const Dims<4>& input_dims,\n                            int32 input_zero_point, uint8* output_data,\n                            const Dims<4>& output_dims) {\n  L2Normalization(input_data, DimsToShape(input_dims), input_zero_point,\n                  output_data, DimsToShape(output_dims));\n}\n\ninline void Relu(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Relu(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(int left_shift, const uint8* input1_data,\n                const Dims<4>& input1_dims, int32 input1_offset,\n                int32 input1_multiplier, int input1_shift,\n                const uint8* input2_data, const Dims<4>& input2_dims,\n                int32 input2_offset, int32 input2_multiplier, int input2_shift,\n                int32 output_offset, int32 output_multiplier, int output_shift,\n                int32 output_activation_min, int32 output_activation_max,\n                uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const int32* input1_data, const Dims<4>& input1_dims,\n         const int32* input2_data, const Dims<4>& input2_dims,\n         int32* output_data, const Dims<4>& output_dims) {\n  ruy::profiler::ScopeLabel label(\"Add/int32\");\n  TFLITE_DCHECK(Ac == FusedActivationFunctionType::kNone);\n\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = std::numeric_limits<int32>::min();\n  op_params.quantized_activation_max = std::numeric_limits<int32>::max();\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAdd(int left_shift, const uint8* input1_data,\n                         const Dims<4>& input1_dims, int32 input1_offset,\n                         int32 input1_multiplier, int input1_shift,\n                         const uint8* input2_data, const Dims<4>& input2_dims,\n                         int32 input2_offset, int32 input2_multiplier,\n                         int input2_shift, int32 output_offset,\n                         int32 output_multiplier, int output_shift,\n                         int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAddFivefold(\n    int y0, int y1, int y2, int y3, int y4, int left_shift,\n    const uint8* input1_data, const Dims<4>& input1_dims, int32 input1_offset,\n    int32 input1_multiplier, int input1_shift, const uint8* input2_data,\n    const Dims<4>& input2_dims, int32 input2_offset, int32 input2_multiplier,\n    int input2_shift, int32 output_offset, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  tflite::ArithmeticParams op_params;\n  op_params.broadcast_category =\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.broadcast_shape[4] = y0;\n  op_params.broadcast_shape[3] = y1;\n  op_params.broadcast_shape[2] = y2;\n  op_params.broadcast_shape[1] = y3;\n  op_params.broadcast_shape[0] = y4;\n  BroadcastAddFivefold(op_params, DimsToShape(input1_dims), input1_data,\n                       DimsToShape(input2_dims), input2_data,\n                       DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T* output_data, const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  BroadcastAdd(input1_data, input1_dims, input2_data, input2_dims,\n               output_activation_min, output_activation_max, output_data,\n               output_dims);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(const int16* input1_data, const Dims<4>& input1_dims,\n                int input1_shift, const int16* input2_data,\n                const Dims<4>& input2_dims, int input2_shift,\n                int16 output_activation_min, int16 output_activation_max,\n                int16* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, -32768);\n    TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Sub(const float* input1_data, const Dims<4>& input1_dims,\n                const float* input2_data, const Dims<4>& input2_dims,\n                float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(FusedActivationFunctionType::kNone,\n                      &output_activation_min, &output_activation_max);\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid Sub(const T* input1_data, const Dims<4>& input1_dims, const T* input2_data,\n         const Dims<4>& input2_dims, T* output_data,\n         const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(FusedActivationFunctionType::kNone,\n                      &output_activation_min, &output_activation_max);\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n  op_params.input1_offset = input1_offset;\n  op_params.input2_offset = input2_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  BroadcastMul(input1_data, input1_dims, input1_offset, input2_data,\n               input2_dims, input2_offset, output_offset, output_multiplier,\n               output_shift, output_activation_min, output_activation_max,\n               output_data, output_dims);\n}\n\ninline bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int kwidth, int kheight,\n                        float output_activation_min,\n                        float output_activation_max, float* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  return AveragePool(params, DimsToShape(input_dims), input_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nbool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int kwidth, int kheight, float* output_data,\n                 const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  return AveragePool(input_data, input_dims, stride_width, stride_height,\n                     pad_width, pad_height, kwidth, kheight,\n                     output_activation_min, output_activation_max, output_data,\n                     output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nbool AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, float* output_data,\n                 const Dims<4>& output_dims) {\n  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n                         pad_height, filter_width, filter_height, output_data,\n                         output_dims);\n}\n\ninline bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int filter_width, int filter_height,\n                        int32 output_activation_min,\n                        int32 output_activation_max, uint8* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  return AveragePool(params, DimsToShape(input_dims), input_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nbool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int filter_width, int filter_height,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  return AveragePool(input_data, input_dims, stride_width, stride_height,\n                     pad_width, pad_height, filter_width, filter_height,\n                     output_activation_min, output_activation_max, output_data,\n                     output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nbool AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims) {\n  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n                         pad_height, filter_width, filter_height,\n                         output_activation_min, output_activation_max,\n                         output_data, output_dims);\n}\n\ninline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int kwidth, int kheight,\n                    float output_activation_min, float output_activation_max,\n                    float* output_data, const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int kwidth, int kheight, float* output_data,\n             const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, kwidth, kheight, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             float* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_data, output_dims);\n}\n\ninline void MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int filter_width, int filter_height,\n                    int32 output_activation_min, int32 output_activation_max,\n                    uint8* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int filter_width, int filter_height, int32 output_activation_min,\n             int32 output_activation_max, uint8* output_data,\n             const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, filter_width, filter_height, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             int32 output_activation_min, int32 output_activation_max,\n             uint8* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\ninline void L2Pool(const float* input_data, const Dims<4>& input_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int filter_width, int filter_height,\n                   float output_activation_min, float output_activation_max,\n                   float* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  L2Pool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n         output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims,\n            int stride_width, int stride_height, int pad_width, int pad_height,\n            int filter_width, int filter_height, float* output_data,\n            const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  L2Pool(input_data, input_dims, stride_width, stride_height, pad_width,\n         pad_height, filter_width, filter_height, output_activation_min,\n         output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int filter_width, int filter_height,\n            float* output_data, const Dims<4>& output_dims) {\n  L2Pool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n             filter_width, filter_height, output_data, output_dims);\n}\n\ninline void Softmax(const SoftmaxParams& params,\n                    const RuntimeShape& input_shape, const uint8* input_data,\n                    const RuntimeShape& output_shape, uint8* output_data) {\n  const int32 input_beta_multiplier = params.input_multiplier;\n  const int32 input_beta_left_shift = params.input_left_shift;\n  const int diff_min = params.diff_min;\n  // The representation chosen for the input to the exp() function is Q5.26.\n  // We need to leave extra space since values that we skip might be as large as\n  // -32 before multiplying by input_beta_multiplier, and therefore as large as\n  // -16 afterwards.  Note that exp(-8) is definitely not insignificant to\n  // accumulation, but exp(-16) definitely is.\n  static const int kScaledDiffIntegerBits = 5;\n  static const int kAccumulationIntegerBits = 12;\n  using FixedPointScaledDiff =\n      gemmlowp::FixedPoint<int32, kScaledDiffIntegerBits>;\n  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;\n  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n\n  ruy::profiler::ScopeLabel label(\"Softmax/8bit\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  for (int b = 0; b < outer_size; ++b) {\n    const uint8* input_data_ptr = input_data + b * depth;\n    uint8* output_data_ptr = output_data + b * depth;\n\n    // Determine the largest entry in the current row\n    uint8 max_in_row = 0;\n    {\n      int c = 0;\n#ifdef USE_NEON\n      uint8x16_t max16_0 = vdupq_n_u8(0);\n      uint8x16_t max16_1 = vdupq_n_u8(0);\n      for (; c <= depth - 32; c += 32) {\n        max16_0 = vmaxq_u8(max16_0, vld1q_u8(input_data_ptr + c + 0));\n        max16_1 = vmaxq_u8(max16_1, vld1q_u8(input_data_ptr + c + 16));\n      }\n      uint8x16_t max16 = vmaxq_u8(max16_0, max16_1);\n      if (c <= depth - 16) {\n        max16 = vmaxq_u8(max16, vld1q_u8(input_data_ptr + c));\n        c += 16;\n      }\n      uint8x8_t max8 = vmax_u8(vget_low_u8(max16), vget_high_u8(max16));\n      if (c <= depth - 8) {\n        max8 = vmax_u8(max8, vld1_u8(input_data_ptr + c));\n        c += 8;\n      }\n      uint8x8_t max4 = vmax_u8(max8, vext_u8(max8, max8, 4));\n      uint8x8_t max2 = vmax_u8(max4, vext_u8(max4, max4, 2));\n      uint8x8_t max1 = vpmax_u8(max2, max2);\n      max_in_row = vget_lane_u8(max1, 0);\n#endif\n      for (; c < depth; ++c) {\n        max_in_row = std::max(max_in_row, input_data_ptr[c]);\n      }\n    }\n\n#ifdef USE_NEON\n    using FixedPointAccumInt32x4 =\n        gemmlowp::FixedPoint<int32x4_t, kAccumulationIntegerBits>;\n    using FixedPointScaledDiffInt32x4 =\n        gemmlowp::FixedPoint<int32x4_t, kScaledDiffIntegerBits>;\n    using FixedPoint0Int32x4 = gemmlowp::FixedPoint<int32x4_t, 0>;\n    FixedPoint0Int32x4 input_beta_multiplier_f0 =\n        FixedPoint0Int32x4::FromScalarRaw(input_beta_multiplier);\n    int16x8_t max_in_row_s16 = vdupq_n_s16(max_in_row);\n#endif\n\n    // Compute the sum of exponentials of the differences of entries in the\n    // current row from the largest entry in the current row.\n    FixedPointAccum sum_of_exps = FixedPointAccum::Zero();\n    {\n      int c = 0;\n#ifdef USE_NEON\n      int32x4_t diff_min_s32 = vdupq_n_s32(diff_min);\n      FixedPointAccumInt32x4 sum_of_exps_0 = FixedPointAccumInt32x4::Zero();\n      FixedPointAccumInt32x4 sum_of_exps_1 = FixedPointAccumInt32x4::Zero();\n      FixedPointAccumInt32x4 zeros = FixedPointAccumInt32x4::Zero();\n      for (; c <= depth - 8; c += 8) {\n        uint16x8_t input_u16 = vmovl_u8(vld1_u8(input_data_ptr + c));\n        int16x8_t input_diff_s16 =\n            vsubq_s16(vreinterpretq_s16_u16(input_u16), max_in_row_s16);\n        int32x4_t input_diff_s32_0 = vmovl_s16(vget_low_s16(input_diff_s16));\n        int32x4_t input_diff_s32_1 = vmovl_s16(vget_high_s16(input_diff_s16));\n        int32x4_t mask_0 =\n            gemmlowp::MaskIfGreaterThanOrEqual(input_diff_s32_0, diff_min_s32);\n        int32x4_t mask_1 =\n            gemmlowp::MaskIfGreaterThanOrEqual(input_diff_s32_1, diff_min_s32);\n        FixedPointScaledDiffInt32x4 scaled_diff_0 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_0, input_beta_left_shift));\n        FixedPointScaledDiffInt32x4 scaled_diff_1 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_1, input_beta_left_shift));\n        FixedPointAccumInt32x4 exps_0 =\n            gemmlowp::Rescale<kAccumulationIntegerBits>(\n                exp_on_negative_values(scaled_diff_0));\n        FixedPointAccumInt32x4 exps_1 =\n            gemmlowp::Rescale<kAccumulationIntegerBits>(\n                exp_on_negative_values(scaled_diff_1));\n        FixedPointAccumInt32x4 masked_exps_0 =\n            SelectUsingMask(mask_0, exps_0, zeros);\n        FixedPointAccumInt32x4 masked_exps_1 =\n            SelectUsingMask(mask_1, exps_1, zeros);\n        sum_of_exps_0 = sum_of_exps_0 + masked_exps_0;\n        sum_of_exps_1 = sum_of_exps_1 + masked_exps_1;\n      }\n      int32x4_t sum_of_exps_reduced_4 = (sum_of_exps_0 + sum_of_exps_1).raw();\n      int32x2_t sum_of_exps_reduced_2 =\n          vadd_s32(vget_low_s32(sum_of_exps_reduced_4),\n                   vget_high_s32(sum_of_exps_reduced_4));\n      int32x2_t sum_of_exps_reduced_1 =\n          vpadd_s32(sum_of_exps_reduced_2, sum_of_exps_reduced_2);\n      sum_of_exps =\n          FixedPointAccum::FromRaw(vget_lane_s32(sum_of_exps_reduced_1, 0));\n#endif\n      for (; c < depth; ++c) {\n        int32 input_diff = static_cast<int32>(input_data_ptr[c]) - max_in_row;\n        if (input_diff >= diff_min) {\n          const int32 input_diff_rescaled =\n              MultiplyByQuantizedMultiplierGreaterThanOne(\n                  input_diff, input_beta_multiplier, input_beta_left_shift);\n          const FixedPointScaledDiff scaled_diff_f8 =\n              FixedPointScaledDiff::FromRaw(input_diff_rescaled);\n          sum_of_exps =\n              sum_of_exps + gemmlowp::Rescale<kAccumulationIntegerBits>(\n                                exp_on_negative_values(scaled_diff_f8));\n        }\n      }\n    }\n\n    // Compute the fixed-point multiplier and shift that we need to apply to\n    // perform a division by the above-computed sum-of-exponentials.\n    int num_bits_over_unit = 0;\n    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(\n        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));\n\n    // Compute the quotients of exponentials of differences of entries in the\n    // current row from the largest entry, over the previously-computed sum of\n    // exponentials.\n    {\n      int c = 0;\n#ifdef USE_NEON\n      int16x8_t diff_min_s16 = vdupq_n_s16(diff_min);\n      for (; c <= depth - 8; c += 8) {\n        uint16x8_t input_u16 = vmovl_u8(vld1_u8(input_data_ptr + c));\n        int16x8_t input_diff_s16 =\n            vsubq_s16(vreinterpretq_s16_u16(input_u16), max_in_row_s16);\n        int32x4_t input_diff_s32_0 = vmovl_s16(vget_low_s16(input_diff_s16));\n        int32x4_t input_diff_s32_1 = vmovl_s16(vget_high_s16(input_diff_s16));\n        uint8x8_t mask = vmovn_u16(vcgeq_s16(input_diff_s16, diff_min_s16));\n        FixedPointScaledDiffInt32x4 scaled_diff_0 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_0, input_beta_left_shift));\n        FixedPointScaledDiffInt32x4 scaled_diff_1 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_1, input_beta_left_shift));\n        FixedPoint0Int32x4 exp_0 = exp_on_negative_values(scaled_diff_0);\n        FixedPoint0Int32x4 exp_1 = exp_on_negative_values(scaled_diff_1);\n        int32x4_t output_s32_0 = gemmlowp::RoundingDivideByPOT(\n            vqrdmulhq_n_s32(exp_0.raw(), shifted_scale.raw()),\n            num_bits_over_unit + 31 - 8);\n        int32x4_t output_s32_1 = gemmlowp::RoundingDivideByPOT(\n            vqrdmulhq_n_s32(exp_1.raw(), shifted_scale.raw()),\n            num_bits_over_unit + 31 - 8);\n        int16x8_t output_s16 =\n            vcombine_s16(vqmovn_s32(output_s32_0), vqmovn_s32(output_s32_1));\n        uint8x8_t output_u8 = vqmovun_s16(output_s16);\n        uint8x8_t masked_output = vbsl_u8(mask, output_u8, vdup_n_u8(0));\n        vst1_u8(output_data_ptr + c, masked_output);\n      }\n#endif\n      for (; c < depth; ++c) {\n        int32 input_diff = static_cast<int32>(input_data_ptr[c]) - max_in_row;\n        if (input_diff >= diff_min) {\n          const int32 input_diff_rescaled =\n              MultiplyByQuantizedMultiplierGreaterThanOne(\n                  input_diff, input_beta_multiplier, input_beta_left_shift);\n          const FixedPointScaledDiff scaled_diff_f8 =\n              FixedPointScaledDiff::FromRaw(input_diff_rescaled);\n\n          FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);\n          int32 unsat_output = gemmlowp::RoundingDivideByPOT(\n              (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);\n\n          output_data_ptr[c] = std::max(std::min(unsat_output, 255), 0);\n\n        } else {\n          output_data_ptr[c] = 0;\n        }\n      }\n    }\n  }\n}\n\ninline void Softmax(const float* input_data, const RuntimeShape& input_shape,\n                    float beta, float* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.beta = beta;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Softmax(const float* input_data, const Dims<4>& input_dims,\n                    float beta, float* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), beta, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void Softmax(const uint8* input_data, const RuntimeShape& input_shape,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_beta_multiplier;\n  params.input_left_shift = input_beta_left_shift;\n  params.diff_min = diff_min;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\ninline void Softmax(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), input_beta_multiplier,\n          input_beta_left_shift, diff_min, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const float* input_data, const RuntimeShape& input_shape,\n                       float* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  // No params currently used for float LogSoftmax.\n  LogSoftmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void LogSoftmax(const float* input_data, const Dims<4>& input_dims,\n                       float* output_data, const Dims<4>& output_dims) {\n  LogSoftmax(input_data, DimsToShape(input_dims), output_data,\n             DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const uint8* input_data, const RuntimeShape& input_shape,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  params.reverse_scaling_divisor = reverse_scaling_divisor;\n  params.reverse_scaling_right_shift = reverse_scaling_right_shift;\n  params.diff_min = diff_min;\n  reference_ops::LogSoftmax(params, input_shape, input_data, output_shape,\n                            output_data);\n}\n\ninline void LogSoftmax(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const Dims<4>& output_dims) {\n  reference_ops::LogSoftmax(\n      input_data, DimsToShape(input_dims), input_multiplier, input_left_shift,\n      reverse_scaling_divisor, reverse_scaling_right_shift, diff_min,\n      output_data, DimsToShape(output_dims));\n}\n\ninline void Logistic(const LogisticParams& params,\n                     const RuntimeShape& input_shape, const uint8* input_data,\n                     const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic/Uint8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int input_left_shift = params.input_left_shift;\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n#ifdef USE_NEON\n  // Handle 16 values at a time\n  for (; c <= size - 16; c += 16) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    uint8x16_t input_val_u8 = vld1q_u8(input_data + c);\n    int16x8_t input_val_centered_0 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n    int16x8_t input_val_centered_1 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint16x8_t mask_rightclamp_0 =\n        vcgtq_s16(input_val_centered_0, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_rightclamp_1 =\n        vcgtq_s16(input_val_centered_1, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_leftclamp_0 =\n        vcgeq_s16(input_val_centered_0, vdupq_n_s16(-input_range_radius));\n    uint16x8_t mask_leftclamp_1 =\n        vcgeq_s16(input_val_centered_1, vdupq_n_s16(-input_range_radius));\n    uint8x16_t mask_rightclamp = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                                             vshrn_n_u16(mask_rightclamp_1, 8));\n    uint8x16_t mask_leftclamp = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                                            vshrn_n_u16(mask_leftclamp_1, 8));\n\n    // This performs what is expressed in the scalar code as\n    // const int32 input_val_rescaled =\n    //     MultiplyByQuantizedMultiplierGreaterThanOne(\n    //         input_val_centered, input_multiplier, input_left_shift);\n    int32x4_t input_val_rescaled_0 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_1 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_2 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_3 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    input_val_rescaled_0 =\n        vqrdmulhq_n_s32(input_val_rescaled_0, input_multiplier);\n    input_val_rescaled_1 =\n        vqrdmulhq_n_s32(input_val_rescaled_1, input_multiplier);\n    input_val_rescaled_2 =\n        vqrdmulhq_n_s32(input_val_rescaled_2, input_multiplier);\n    input_val_rescaled_3 =\n        vqrdmulhq_n_s32(input_val_rescaled_3, input_multiplier);\n\n    // Invoke gemmlowp::logistic on FixedPoint wrapping int32x4_t\n    using FixedPoint4 = gemmlowp::FixedPoint<int32x4_t, 4>;\n    using FixedPoint0 = gemmlowp::FixedPoint<int32x4_t, 0>;\n    const FixedPoint4 input_val_f4_0 =\n        FixedPoint4::FromRaw(input_val_rescaled_0);\n    const FixedPoint4 input_val_f4_1 =\n        FixedPoint4::FromRaw(input_val_rescaled_1);\n    const FixedPoint4 input_val_f4_2 =\n        FixedPoint4::FromRaw(input_val_rescaled_2);\n    const FixedPoint4 input_val_f4_3 =\n        FixedPoint4::FromRaw(input_val_rescaled_3);\n    const FixedPoint0 output_val_f0_0 = gemmlowp::logistic(input_val_f4_0);\n    const FixedPoint0 output_val_f0_1 = gemmlowp::logistic(input_val_f4_1);\n    const FixedPoint0 output_val_f0_2 = gemmlowp::logistic(input_val_f4_2);\n    const FixedPoint0 output_val_f0_3 = gemmlowp::logistic(input_val_f4_3);\n\n    // Divide by 2^23 as in the scalar code\n    using gemmlowp::RoundingDivideByPOT;\n    int32x4_t output_val_s32_0 = RoundingDivideByPOT(output_val_f0_0.raw(), 23);\n    int32x4_t output_val_s32_1 = RoundingDivideByPOT(output_val_f0_1.raw(), 23);\n    int32x4_t output_val_s32_2 = RoundingDivideByPOT(output_val_f0_2.raw(), 23);\n    int32x4_t output_val_s32_3 = RoundingDivideByPOT(output_val_f0_3.raw(), 23);\n\n    // Cast output values to uint8, saturating\n    int16x8_t output_val_s16_0 = vcombine_s16(vqmovn_s32(output_val_s32_0),\n                                              vqmovn_s32(output_val_s32_1));\n    int16x8_t output_val_s16_1 = vcombine_s16(vqmovn_s32(output_val_s32_2),\n                                              vqmovn_s32(output_val_s32_3));\n    uint8x16_t output_val_u8 = vcombine_u8(vqmovun_s16(output_val_s16_0),\n                                           vqmovun_s16(output_val_s16_1));\n\n    // Perform the bit-masking with the bit masks computed at the beginning,\n    // see the comment there.\n    output_val_u8 = vorrq_u8(output_val_u8, mask_rightclamp);\n    output_val_u8 = vandq_u8(output_val_u8, mask_leftclamp);\n\n    // Store back to memory\n    vst1q_u8(output_data + c, output_val_u8);\n  }\n#endif\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int32 input_val_centered =\n        static_cast<int32>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      const int32 input_val_rescaled =\n          MultiplyByQuantizedMultiplierGreaterThanOne(\n              input_val_centered, input_multiplier, input_left_shift);\n      using FixedPoint4 = gemmlowp::FixedPoint<int32, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int32 output_val_s32 = RoundingDivideByPOT(output_val_f0.raw(), 23);\n      if (output_val_s32 == 256) {\n        output_val_s32 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s32, 0);\n      TFLITE_DCHECK_LE(output_val_s32, 255);\n      output_val = static_cast<uint8>(output_val_s32);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Logistic(const uint8* input_data, const RuntimeShape& input_shape,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const RuntimeShape& output_shape) {\n  LogisticParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  Logistic(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n           output_data);\n}\n\ninline void Logistic(const uint8* input_data, const Dims<4>& input_dims,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const Dims<4>& output_dims) {\n  Logistic(input_data, DimsToShape(input_dims), input_zero_point,\n           input_range_radius, input_multiplier, input_left_shift, output_data,\n           DimsToShape(output_dims));\n}\n\ninline void Logistic(const RuntimeShape& input_shape, const int16* input_data,\n                     const RuntimeShape& output_shape, int16* output_data) {\n  LogisticParams params;\n  // No params currently needed by int16 Logistic.\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const int16* input_data, const RuntimeShape& input_shape,\n                     int16* output_data, const RuntimeShape& output_shape) {\n  LogisticParams params;\n  // No params currently needed by int16 Logistic.\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const int16* input_data, const Dims<4>& input_dims,\n                     int16* output_data, const Dims<4>& output_dims) {\n  Logistic(input_data, DimsToShape(input_dims), output_data,\n           DimsToShape(output_dims));\n}\n\ninline void Tanh(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Tanh(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Tanh(const TanhParams& params, const RuntimeShape& input_shape,\n                 const uint8* input_data, const RuntimeShape& output_shape,\n                 uint8* output_data) {\n  // Note that this is almost the exact same code as in Logistic().\n  ruy::profiler::ScopeLabel label(\"Tanh\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int input_left_shift = params.input_left_shift;\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n  int32_t output_zero_point = 128;\n#ifdef USE_NEON\n  // Handle 16 values at a time\n  for (; c <= size - 16; c += 16) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    uint8x16_t input_val_u8 = vld1q_u8(input_data + c);\n    int16x8_t input_val_centered_0 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n    int16x8_t input_val_centered_1 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint16x8_t mask_rightclamp_0 =\n        vcgtq_s16(input_val_centered_0, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_rightclamp_1 =\n        vcgtq_s16(input_val_centered_1, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_leftclamp_0 =\n        vcgeq_s16(input_val_centered_0, vdupq_n_s16(-input_range_radius));\n    uint16x8_t mask_leftclamp_1 =\n        vcgeq_s16(input_val_centered_1, vdupq_n_s16(-input_range_radius));\n    uint8x16_t mask_rightclamp = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                                             vshrn_n_u16(mask_rightclamp_1, 8));\n    uint8x16_t mask_leftclamp = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                                            vshrn_n_u16(mask_leftclamp_1, 8));\n\n    // This performs what is expressed in the scalar code as\n    // const int32 input_val_rescaled =\n    //     MultiplyByQuantizedMultiplierGreaterThanOne(\n    //         input_val_centered, input_multiplier, input_left_shift);\n    int32x4_t input_val_rescaled_0 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_1 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_2 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_3 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    input_val_rescaled_0 =\n        vqrdmulhq_n_s32(input_val_rescaled_0, input_multiplier);\n    input_val_rescaled_1 =\n        vqrdmulhq_n_s32(input_val_rescaled_1, input_multiplier);\n    input_val_rescaled_2 =\n        vqrdmulhq_n_s32(input_val_rescaled_2, input_multiplier);\n    input_val_rescaled_3 =\n        vqrdmulhq_n_s32(input_val_rescaled_3, input_multiplier);\n\n    // Invoke gemmlowp::tanh on FixedPoint wrapping int32x4_t\n    using FixedPoint4 = gemmlowp::FixedPoint<int32x4_t, 4>;\n    using FixedPoint0 = gemmlowp::FixedPoint<int32x4_t, 0>;\n    const FixedPoint4 input_val_f4_0 =\n        FixedPoint4::FromRaw(input_val_rescaled_0);\n    const FixedPoint4 input_val_f4_1 =\n        FixedPoint4::FromRaw(input_val_rescaled_1);\n    const FixedPoint4 input_val_f4_2 =\n        FixedPoint4::FromRaw(input_val_rescaled_2);\n    const FixedPoint4 input_val_f4_3 =\n        FixedPoint4::FromRaw(input_val_rescaled_3);\n    const FixedPoint0 output_val_f0_0 = gemmlowp::tanh(input_val_f4_0);\n    const FixedPoint0 output_val_f0_1 = gemmlowp::tanh(input_val_f4_1);\n    const FixedPoint0 output_val_f0_2 = gemmlowp::tanh(input_val_f4_2);\n    const FixedPoint0 output_val_f0_3 = gemmlowp::tanh(input_val_f4_3);\n\n    // Divide by 2^24 as in the scalar code\n    using gemmlowp::RoundingDivideByPOT;\n    int32x4_t output_val_s32_0 = RoundingDivideByPOT(output_val_f0_0.raw(), 24);\n    int32x4_t output_val_s32_1 = RoundingDivideByPOT(output_val_f0_1.raw(), 24);\n    int32x4_t output_val_s32_2 = RoundingDivideByPOT(output_val_f0_2.raw(), 24);\n    int32x4_t output_val_s32_3 = RoundingDivideByPOT(output_val_f0_3.raw(), 24);\n\n    // Add the output zero point\n    int32x4_t output_zero_point_s32 = vdupq_n_s32(output_zero_point);\n    output_val_s32_0 = vaddq_s32(output_val_s32_0, output_zero_point_s32);\n    output_val_s32_1 = vaddq_s32(output_val_s32_1, output_zero_point_s32);\n    output_val_s32_2 = vaddq_s32(output_val_s32_2, output_zero_point_s32);\n    output_val_s32_3 = vaddq_s32(output_val_s32_3, output_zero_point_s32);\n\n    // Cast output values to uint8, saturating\n    int16x8_t output_val_s16_0 = vcombine_s16(vqmovn_s32(output_val_s32_0),\n                                              vqmovn_s32(output_val_s32_1));\n    int16x8_t output_val_s16_1 = vcombine_s16(vqmovn_s32(output_val_s32_2),\n                                              vqmovn_s32(output_val_s32_3));\n    uint8x16_t output_val_u8 = vcombine_u8(vqmovun_s16(output_val_s16_0),\n                                           vqmovun_s16(output_val_s16_1));\n\n    // Perform the bit-masking with the bit masks computed at the beginning,\n    // see the comment there.\n    output_val_u8 = vorrq_u8(output_val_u8, mask_rightclamp);\n    output_val_u8 = vandq_u8(output_val_u8, mask_leftclamp);\n\n    // Store back to memory\n    vst1q_u8(output_data + c, output_val_u8);\n  }\n#endif\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int32 input_val_centered =\n        static_cast<int32>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      const int32 input_val_rescaled =\n          MultiplyByQuantizedMultiplierGreaterThanOne(\n              input_val_centered, input_multiplier, input_left_shift);\n      using FixedPoint4 = gemmlowp::FixedPoint<int32, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::tanh(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int32 output_val_s32 = RoundingDivideByPOT(output_val_f0.raw(), 24);\n      output_val_s32 += output_zero_point;\n      if (output_val_s32 == 256) {\n        output_val_s32 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s32, 0);\n      TFLITE_DCHECK_LE(output_val_s32, 255);\n      output_val = static_cast<uint8>(output_val_s32);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Tanh(const uint8* input_data, const RuntimeShape& input_shape,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_zero_point,\n       input_range_radius, input_multiplier, input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ninline void Tanh(const int16* input_data, const RuntimeShape& input_shape,\n                 int input_left_shift, int16* output_data,\n                 const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const int16* input_data, const Dims<4>& input_dims,\n                 int input_left_shift, int16* output_data,\n                 const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ntemplate <typename T>\ninline void DepthToSpace(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::DepthToSpaceParams op_params;\n  op_params.block_size = block_size;\n\n  DepthToSpace(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void SpaceToDepth(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::SpaceToDepthParams op_params;\n  op_params.block_size = block_size;\n\n  SpaceToDepth(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ninline void Mul(const float* input1_data, const Dims<4>& input1_dims,\n                const float* input2_data, const Dims<4>& input2_dims,\n                float output_activation_min, float output_activation_max,\n                float* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Mul(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  Mul(input1_data, input1_dims, input2_data, input2_dims, output_activation_min,\n      output_activation_max, output_data, output_dims);\n}\n\ninline void Mul(const int32* input1_data, const Dims<4>& input1_dims,\n                const int32* input2_data, const Dims<4>& input2_dims,\n                int32 output_activation_min, int32 output_activation_max,\n                int32* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Mul(const int32* input1_data, const Dims<4>& input1_dims,\n         const int32* input2_data, const Dims<4>& input2_dims,\n         int32* output_data, const Dims<4>& output_dims) {\n  TFLITE_DCHECK(Ac == FusedActivationFunctionType::kNone);\n  tflite::ArithmeticParams op_params;\n  // No parameters needed.\n\n  MulNoActivation(op_params, DimsToShape(input1_dims), input1_data,\n                  DimsToShape(input2_dims), input2_data,\n                  DimsToShape(output_dims), output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int16* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  // No parameters needed.\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int32 output_offset, int32 output_activation_min,\n                int32 output_activation_max, uint8* output_data,\n                const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.output_offset = output_offset;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastMul(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// For compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastMul(const float* input1_data, const Dims<4>& input1_dims,\n                         const float* input2_data, const Dims<4>& input2_dims,\n                         float* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  float float_activation_min;\n  float float_activation_max;\n  GetActivationMinMax(Ac, &float_activation_min, &float_activation_max);\n  SetActivationParams(float_activation_min, float_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ninline void LocalResponseNormalization(const float* input_data,\n                                       const Dims<4>& input_dims, int range,\n                                       float bias, float alpha, float beta,\n                                       float* output_data,\n                                       const Dims<4>& output_dims) {\n  tflite::LocalResponseNormalizationParams op_params;\n  op_params.range = range;\n  op_params.bias = bias;\n  op_params.alpha = alpha;\n  op_params.beta = beta;\n\n  LocalResponseNormalization(op_params, DimsToShape(input_dims), input_data,\n                             DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename SrcT, typename DstT>\nvoid Cast(const SrcT* input_data, const Dims<4>& input_dims, DstT* output_data,\n          const Dims<4>& output_dims) {\n  Cast(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Floor(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Floor(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ninline void ResizeBilinear(const float* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, float* output_data,\n                           const Dims<4>& output_dims, bool align_corners) {\n  tflite::ResizeBilinearParams op_params;\n  op_params.align_corners = align_corners;\n  op_params.half_pixel_centers = false;\n  ResizeBilinear(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(output_size_dims), output_size_data,\n                 DimsToShape(output_dims), output_data);\n}\n\ninline void ResizeBilinear(const uint8* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, uint8* output_data,\n                           const Dims<4>& output_dims, bool align_corners) {\n  tflite::ResizeBilinearParams op_params;\n  op_params.align_corners = align_corners;\n  op_params.half_pixel_centers = false;\n  ResizeBilinear(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(output_size_dims), output_size_data,\n                 DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ninline void ResizeBilinear(const float* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, float* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear(input_data, input_dims, output_size_data, output_size_dims,\n                 output_data, output_dims, /*align_corners=*/false);\n}\n\n// legacy, for compatibility with old checked-in code\ninline void ResizeBilinear(const uint8* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, uint8* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear(input_data, input_dims, output_size_data, output_size_dims,\n                 output_data, output_dims, /*align_corners=*/false);\n}\n\ntemplate <typename T>\ninline void BatchToSpaceND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* crops_data, const Dims<4>& crops_dims,\n                           T* output_data, const Dims<4>& output_dims) {\n  BatchToSpaceND(DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(crops_dims), crops_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// Legacy signature, function covered both Pad and PadV2.\ntemplate <typename T>\ninline void PadV2(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& left_paddings,\n                  const std::vector<int>& right_paddings, T* output_data,\n                  const Dims<4>& output_dims, const T pad_value) {\n  TFLITE_DCHECK_EQ(left_paddings.size(), 4);\n  TFLITE_DCHECK_EQ(right_paddings.size(), 4);\n  tflite::PadParams op_params;\n  op_params.left_padding_count = 4;\n  op_params.right_padding_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.left_padding[i] = left_paddings[3 - i];\n    op_params.right_padding[i] = right_paddings[3 - i];\n  }\n  const T pad_value_copy = pad_value;\n\n  Pad(op_params, DimsToShape(input_dims), input_data, &pad_value_copy,\n      DimsToShape(output_dims), output_data);\n}\n\n// Old Pad that calls legacy PadV2.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims, const int32_t pad_value) {\n  const T converted_pad_value = static_cast<T>(pad_value);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, converted_pad_value);\n}\n\n// Old Pad that only padded with 0.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims) {\n  const T pad_value = static_cast<T>(0);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, pad_value);\n}\n\ntemplate <typename T>\ninline void Slice(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& begin, const std::vector<int>& size,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::SliceParams op_params;\n  op_params.begin_count = 4;\n  op_params.size_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.begin[i] = begin[3 - i];\n    op_params.size[i] = size[3 - i];\n  }\n\n  Slice(op_params, DimsToShape(input_dims), input_data,\n        DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid TensorFlowMinimum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Minimum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid TensorFlowMaximum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Maximum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ninline void Dequantize(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 zero_point, double scale, float* output_data,\n                       const Dims<4>& output_dims) {\n  tflite::DequantizationParams op_params;\n  op_params.zero_point = zero_point;\n  op_params.scale = scale;\n\n  Dequantize(op_params, DimsToShape(input_dims), input_data,\n             DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid Transpose(const T* input, const Dims<4>& input_dims, T* output,\n               const Dims<4>& output_dims, const int* permuted_axes) {\n  TransposeParams params;\n  params.perm_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    params.perm[i] = 3 - permuted_axes[3 - i];\n  }\n  Transpose(params, DimsToShape(input_dims), input, DimsToShape(output_dims),\n            output);\n}\n\ntemplate <typename T>\ninline void StridedSlice(const T* input_data, const Dims<4>& input_dims,\n                         int begin_mask, int end_mask, int shrink_axis_mask,\n                         const std::vector<int>& start_indices,\n                         const std::vector<int>& stop_indices,\n                         const std::vector<int>& strides, T* output_data,\n                         const Dims<4>& output_dims) {\n  TFLITE_DCHECK_EQ(start_indices.size(), 4);\n  auto op_params = strided_slice::BuildStridedSliceParams(\n      begin_mask, end_mask, shrink_axis_mask, start_indices, stop_indices,\n      strides);\n  reference_ops::StridedSliceReverseIndices(&op_params);\n\n  StridedSlice(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const T3* axis, const T1* input_data,\n            const tflite::Dims<4>& input_dims, T2* output_data,\n            const tflite::Dims<4>& output_dims) {\n  // Assumes the input always has 4 dimensions, and therefore,\n  // output always has three dimensions.\n  auto output_shape = RuntimeShape(\n      {output_dims.sizes[2], output_dims.sizes[1], output_dims.sizes[0]});\n  // Another way to interpret this is that output_dims.sizes[4] is always 1.\n  TFLITE_DCHECK_EQ(output_shape.FlatSize(),\n                   DimsToShape(output_dims).FlatSize());\n  // Legacy path only supported this.\n  TFLITE_DCHECK_EQ(axis[0], 3);\n  ArgMinMax(DimsToShape(input_dims), input_data, axis, output_shape,\n            output_data, /*is_arg_max=*/true);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMinMax(const T3* axis, const T1* input_data, const Dims<4>& input_dims,\n               T2* output_data, const Dims<4>& output_dims,\n               const bool is_arg_max) {\n  ArgMinMax(axis, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n            output_data, is_arg_max);\n}\n\n}  // namespace optimized_ops\n}  // namespace tflite\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_LEGACY_OPTIMIZED_OPS_H_\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_OPTIMIZED_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_OPTIMIZED_OPS_H_\n\n#include <assert.h>\n#include <stdint.h>\n#include <sys/types.h>\n\n#include <algorithm>\n#include <cmath>\n#include <cstdint>\n#include <limits>\n#include <memory>\n#include <tuple>\n#include <type_traits>\n\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/reference/add.h\"\n#include \"tensorflow/lite/kernels/internal/reference/resize_nearest_neighbor.h\"\n\n#if defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n#include <Accelerate/Accelerate.h>\n#endif\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"fixedpoint/fixedpoint.h\"\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_gemm.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_gemm_params.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_threadpool.h\"\n#include \"tensorflow/lite/kernels/internal/cppmath.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/im2col_utils.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/strided_slice_logic.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/kernels/internal/transpose_utils.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\n#if __aarch64__ && __clang__\n#define TFLITE_SOFTMAX_USE_UINT16_LUT\n#endif\n\nnamespace tflite {\nnamespace optimized_ops {\n\n// Unoptimized reference ops:\nusing reference_ops::Broadcast4DSlowGreater;\nusing reference_ops::Broadcast4DSlowGreaterEqual;\nusing reference_ops::Broadcast4DSlowGreaterEqualWithScaling;\nusing reference_ops::Broadcast4DSlowGreaterWithScaling;\nusing reference_ops::Broadcast4DSlowLess;\nusing reference_ops::Broadcast4DSlowLessEqual;\nusing reference_ops::Broadcast4DSlowLessEqualWithScaling;\nusing reference_ops::Broadcast4DSlowLessWithScaling;\nusing reference_ops::BroadcastAdd4DSlow;\nusing reference_ops::BroadcastMul4DSlow;\nusing reference_ops::BroadcastSub16POTSlow;\nusing reference_ops::BroadcastSubSlow;\nusing reference_ops::Concatenation;\nusing reference_ops::ConcatenationWithScaling;\nusing reference_ops::DepthConcatenation;\nusing reference_ops::Div;\nusing reference_ops::Elu;\nusing reference_ops::FakeQuant;\nusing reference_ops::Fill;\nusing reference_ops::Gather;\nusing reference_ops::Greater;\nusing reference_ops::GreaterEqual;\nusing reference_ops::GreaterEqualWithScaling;\nusing reference_ops::GreaterWithScaling;\nusing reference_ops::LeakyRelu;\nusing reference_ops::Less;\nusing reference_ops::LessEqual;\nusing reference_ops::LessEqualWithScaling;\nusing reference_ops::LessWithScaling;\nusing reference_ops::Mean;\nusing reference_ops::ProcessBroadcastShapes;\nusing reference_ops::RankOneSelect;\nusing reference_ops::Relu1;\nusing reference_ops::Relu6;\nusing reference_ops::ReluX;\nusing reference_ops::Round;\nusing reference_ops::Select;\nusing reference_ops::SpaceToBatchND;\nusing reference_ops::Split;\nusing reference_ops::Sub16;\n\n// TODO(b/80247582) Remove this constant.\n// This will be phased out as the shifts are revised with more thought. Use of a\n// constant enables us to track progress on this work.\n//\n// Used to convert from old-style shifts (right) to new-style (left).\nstatic constexpr int kReverseShift = -1;\n\n// Make a local VectorMap typedef allowing to map a float array\n// as a Eigen vector expression. The std::conditional here is to\n// construct the suitable Eigen type for the constness of the\n// data. Indeed, for const data, we need to produce\n//    Eigen::Map<const Eigen::Matrix<float, ...>>\n// and not the more straightforward\n//    Eigen::Map<Eigen::Matrix<const float, ...>>\ntemplate <typename Scalar>\nusing VectorMap = typename std::conditional<\n    std::is_const<Scalar>::value,\n    Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,\n                                   Eigen::Dynamic, 1>>,\n    Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>>::type;\n\ntemplate <typename Scalar>\nVectorMap<Scalar> MapAsVector(Scalar* data, const RuntimeShape& shape) {\n  const int size = shape.FlatSize();\n  return VectorMap<Scalar>(data, size, 1);\n}\n\n// Make a local VectorMap typedef allowing to map a float array\n// as a Eigen matrix expression. The same explanation as for VectorMap\n// above also applies here.\ntemplate <typename Scalar>\nusing MatrixMap = typename std::conditional<\n    std::is_const<Scalar>::value,\n    Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,\n                                   Eigen::Dynamic, Eigen::Dynamic>>,\n    Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;\n\ntemplate <typename Scalar>\nMatrixMap<Scalar> MapAsMatrixWithLastDimAsRows(Scalar* data,\n                                               const RuntimeShape& shape) {\n  const int dims_count = shape.DimensionsCount();\n  const int rows = shape.Dims(dims_count - 1);\n  const int cols = FlatSizeSkipDim(shape, dims_count - 1);\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar>\nMatrixMap<Scalar> MapAsMatrixWithFirstDimAsCols(Scalar* data,\n                                                const RuntimeShape& shape) {\n  const int cols = shape.Dims(0);\n  const int rows = FlatSizeSkipDim(shape, 0);\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar>\nusing ArrayMap = typename std::conditional<\n    std::is_const<Scalar>::value,\n    Eigen::Map<const Eigen::Array<typename std::remove_const<Scalar>::type,\n                                  Eigen::Dynamic, Eigen::Dynamic>>,\n    Eigen::Map<Eigen::Array<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;\n\ntemplate <typename Scalar>\nArrayMap<Scalar> MapAsArrayWithLastDimAsRows(Scalar* data,\n                                             const RuntimeShape& shape) {\n  const int dims_count = shape.DimensionsCount();\n  const int rows = shape.Dims(dims_count - 1);\n  const int cols = FlatSizeSkipDim(shape, dims_count - 1);\n  return ArrayMap<Scalar>(data, rows, cols);\n}\n\n// Copied from tensorflow/core/framework/tensor_types.h\ntemplate <typename T, int NDIMS = 1, typename IndexType = Eigen::DenseIndex>\nstruct TTypes {\n  // Rank-1 tensor (vector) of scalar type T.\n  typedef Eigen::TensorMap<Eigen::Tensor<T, 1, Eigen::RowMajor, IndexType>,\n                           Eigen::Aligned>\n      Flat;\n  typedef Eigen::TensorMap<\n      Eigen::Tensor<const T, 2, Eigen::RowMajor, IndexType>>\n      UnalignedConstMatrix;\n};\n\n// TODO(b/62193649): this function is only needed as long\n// as we have the --variable_batch hack.\ntemplate <typename Scalar>\nMatrixMap<Scalar> MapAsMatrixWithGivenNumberOfRows(Scalar* data,\n                                                   const RuntimeShape& shape,\n                                                   int rows) {\n  const int flatsize = shape.FlatSize();\n  TFLITE_DCHECK_EQ(flatsize % rows, 0);\n  const int cols = flatsize / rows;\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename ElementwiseF, typename ScalarBroadcastF, typename T>\ninline void BinaryBroadcastFiveFold(const ArithmeticParams& unswitched_params,\n                                    const RuntimeShape& unswitched_input1_shape,\n                                    const T* unswitched_input1_data,\n                                    const RuntimeShape& unswitched_input2_shape,\n                                    const T* unswitched_input2_data,\n                                    const RuntimeShape& output_shape,\n                                    T* output_data, ElementwiseF elementwise_f,\n                                    ScalarBroadcastF scalar_broadcast_f) {\n  ArithmeticParams switched_params = unswitched_params;\n  switched_params.input1_offset = unswitched_params.input2_offset;\n  switched_params.input1_multiplier = unswitched_params.input2_multiplier;\n  switched_params.input1_shift = unswitched_params.input2_shift;\n  switched_params.input2_offset = unswitched_params.input1_offset;\n  switched_params.input2_multiplier = unswitched_params.input1_multiplier;\n  switched_params.input2_shift = unswitched_params.input1_shift;\n\n  const bool use_unswitched =\n      unswitched_params.broadcast_category ==\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n\n  const ArithmeticParams& params =\n      use_unswitched ? unswitched_params : switched_params;\n  const T* input1_data =\n      use_unswitched ? unswitched_input1_data : unswitched_input2_data;\n  const T* input2_data =\n      use_unswitched ? unswitched_input2_data : unswitched_input1_data;\n\n  // Fivefold nested loops. The second input resets its position for each\n  // iteration of the second loop. The first input resets its position at the\n  // beginning of the fourth loop. The innermost loop is an elementwise add of\n  // sections of the arrays.\n  T* output_data_ptr = output_data;\n  const T* input1_data_ptr = input1_data;\n  const T* input2_data_reset = input2_data;\n  // In the fivefold pattern, y0, y2 and y4 are not broadcast, and so shared\n  // between input shapes. y3 for input 1 is always broadcast, and so the\n  // dimension there is 1, whereas optionally y1 might be broadcast for\n  // input 2. Put another way, input1.shape.FlatSize = y0 * y1 * y2 * y4,\n  // input2.shape.FlatSize = y0 * y2 * y3 * y4.\n  int y0 = params.broadcast_shape[0];\n  int y1 = params.broadcast_shape[1];\n  int y2 = params.broadcast_shape[2];\n  int y3 = params.broadcast_shape[3];\n  int y4 = params.broadcast_shape[4];\n  if (y4 > 1) {\n    // General fivefold pattern, with y4 > 1 so there is a non-broadcast inner\n    // dimension.\n    for (int i0 = 0; i0 < y0; ++i0) {\n      const T* input2_data_ptr = nullptr;\n      for (int i1 = 0; i1 < y1; ++i1) {\n        input2_data_ptr = input2_data_reset;\n        for (int i2 = 0; i2 < y2; ++i2) {\n          for (int i3 = 0; i3 < y3; ++i3) {\n            elementwise_f(y4, params, input1_data_ptr, input2_data_ptr,\n                          output_data_ptr);\n            input2_data_ptr += y4;\n            output_data_ptr += y4;\n          }\n          // We have broadcast y4 of input1 data y3 times, and now move on.\n          input1_data_ptr += y4;\n        }\n      }\n      // We have broadcast y2*y3*y4 of input2 data y1 times, and now move on.\n      input2_data_reset = input2_data_ptr;\n    }\n  } else {\n    // Special case of y4 == 1, in which the innermost loop is a single\n    // element and can be combined with the next (y3) as an inner broadcast.\n    //\n    // Note that this handles the case of pure scalar broadcast when\n    // y0 == y1 == y2 == 1. With low overhead it handles cases such as scalar\n    // broadcast with batch (as y2 > 1).\n    //\n    // NOTE The process is the same as the above general case except\n    // simplified for y4 == 1 and the loop over y3 is contained within the\n    // AddScalarBroadcast function.\n    for (int i0 = 0; i0 < y0; ++i0) {\n      const T* input2_data_ptr = nullptr;\n      for (int i1 = 0; i1 < y1; ++i1) {\n        input2_data_ptr = input2_data_reset;\n        for (int i2 = 0; i2 < y2; ++i2) {\n          scalar_broadcast_f(y3, params, *input1_data_ptr, input2_data_ptr,\n                             output_data_ptr);\n          input2_data_ptr += y3;\n          output_data_ptr += y3;\n          input1_data_ptr += 1;\n        }\n      }\n      input2_data_reset = input2_data_ptr;\n    }\n  }\n}\n\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n\n// Looks up each element of <indices> in <table>, returns them in a vector.\ninline uint8x16_t aarch64_lookup_vector(const uint8x16x4_t table[4],\n                                        uint8x16_t indices) {\n  // Look up in 1st quarter of the table: top 2 bits of indices == 00\n  uint8x16_t output1 = vqtbl4q_u8(table[0], indices);\n  // Look up in 2nd quarter of the table: top 2 bits of indices == 01\n  uint8x16_t output2 =\n      vqtbl4q_u8(table[1], veorq_u8(indices, vdupq_n_u8(0x40)));\n  // Look up in 3rd quarter of the table: top 2 bits of indices == 10\n  uint8x16_t output3 =\n      vqtbl4q_u8(table[2], veorq_u8(indices, vdupq_n_u8(0x80)));\n  // Look up in 4th quarter of the table: top 2 bits of indices == 11\n  uint8x16_t output4 =\n      vqtbl4q_u8(table[3], veorq_u8(indices, vdupq_n_u8(0xc0)));\n\n  // Combine result of the 4 lookups.\n  return vorrq_u8(vorrq_u8(output1, output2), vorrq_u8(output3, output4));\n}\n\n#endif\n\ninline void AddBiasAndEvalActivationFunction(float output_activation_min,\n                                             float output_activation_max,\n                                             const RuntimeShape& bias_shape,\n                                             const float* bias_data,\n                                             const RuntimeShape& array_shape,\n                                             float* array_data) {\n  BiasAndClamp(output_activation_min, output_activation_max,\n               bias_shape.FlatSize(), bias_data, array_shape.FlatSize(),\n               array_data);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& bias_shape,\n    const float* optional_bias_data, const RuntimeShape& output_shape,\n    float* output_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected\");\n  const int dims_count = weights_shape.DimensionsCount();\n  const int input_rows = weights_shape.Dims(dims_count - 1);\n  cpu_backend_gemm::MatrixParams<float> rhs_params;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.rows = input_rows;\n  rhs_params.cols = input_shape.FlatSize() / input_rows;\n  rhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.rhs_cacheable);\n  TFLITE_DCHECK_EQ(input_shape.FlatSize(), rhs_params.rows * rhs_params.cols);\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.cols = weights_shape.Dims(dims_count - 1);\n  lhs_params.rows = FlatSizeSkipDim(weights_shape, dims_count - 1);\n  lhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.lhs_cacheable);\n  cpu_backend_gemm::MatrixParams<float> dst_params;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.rows = output_shape.Dims(output_shape.DimensionsCount() - 1);\n  dst_params.cols =\n      FlatSizeSkipDim(output_shape, output_shape.DimensionsCount() - 1);\n  cpu_backend_gemm::GemmParams<float, float> gemm_params;\n  gemm_params.bias = optional_bias_data;\n  gemm_params.clamp_min = params.float_activation_min;\n  gemm_params.clamp_max = params.float_activation_max;\n  cpu_backend_gemm::Gemm(lhs_params, weights_data, rhs_params, input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/8bit\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int filter_rows = filter_shape.Dims(filter_dim_count - 2);\n  const int filter_cols = filter_shape.Dims(filter_dim_count - 1);\n  TFLITE_DCHECK_EQ(filter_shape.FlatSize(), filter_rows * filter_cols);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  if (bias_data) {\n    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n  }\n\n  cpu_backend_gemm::MatrixParams<uint8> lhs_params;\n  lhs_params.rows = filter_rows;\n  lhs_params.cols = filter_cols;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.zero_point = -filter_offset;\n  lhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.lhs_cacheable);\n  cpu_backend_gemm::MatrixParams<uint8> rhs_params;\n  rhs_params.rows = filter_cols;\n  rhs_params.cols = batches;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.zero_point = -input_offset;\n  rhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.rhs_cacheable);\n  cpu_backend_gemm::MatrixParams<uint8> dst_params;\n  dst_params.rows = filter_rows;\n  dst_params.cols = batches;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.zero_point = output_offset;\n  cpu_backend_gemm::GemmParams<int32, uint8> gemm_params;\n  gemm_params.bias = bias_data;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  gemm_params.multiplier_fixedpoint = output_multiplier;\n  gemm_params.multiplier_exponent = output_shift;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data_int32, const RuntimeShape& output_shape,\n    int16* output_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/Uint8Int16\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  TFLITE_DCHECK_EQ(output_offset, 0);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);\n\n  cpu_backend_gemm::MatrixParams<uint8> lhs_params;\n  lhs_params.rows = output_depth;\n  lhs_params.cols = accum_depth;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.zero_point = -filter_offset;\n  lhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.lhs_cacheable);\n  cpu_backend_gemm::MatrixParams<uint8> rhs_params;\n  rhs_params.rows = accum_depth;\n  rhs_params.cols = batches;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.zero_point = -input_offset;\n  rhs_params.cache_policy =\n      cpu_backend_gemm::DefaultCachePolicy(params.rhs_cacheable);\n  cpu_backend_gemm::MatrixParams<int16> dst_params;\n  dst_params.rows = output_depth;\n  dst_params.cols = batches;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.zero_point = 0;\n  cpu_backend_gemm::GemmParams<int32, int16> gemm_params;\n  gemm_params.bias = bias_data_int32;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  gemm_params.multiplier_fixedpoint = output_multiplier;\n  gemm_params.multiplier_exponent = output_shift;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\n// Internal function doing the actual arithmetic work for\n// ShuffledFullyConnected.\n// May be called either directly by it (single-threaded case) or may be used\n// as the 'task' for worker threads to run (multi-threaded case, see\n// ShuffledFullyConnectedWorkerTask below).\ninline void ShuffledFullyConnectedWorkerImpl(\n    const uint8* shuffled_input_workspace_data,\n    const int8* shuffled_weights_data, int batches, int output_depth,\n    int output_stride, int accum_depth, const int32* bias_data,\n    int32 output_multiplier, int output_shift, int16* output_data) {\n#if defined USE_NEON\n  const int8* shuffled_weights_ptr = shuffled_weights_data;\n  if (batches == 1) {\n    const int right_shift = output_shift > 0 ? 0 : -output_shift;\n    const int left_shift = output_shift > 0 ? output_shift : 0;\n    for (int c = 0; c < output_depth; c += 4) {\n      // Accumulation loop.\n      int32x4_t row_accum0 = vdupq_n_s32(0);\n      int32x4_t row_accum1 = vdupq_n_s32(0);\n      int32x4_t row_accum2 = vdupq_n_s32(0);\n      int32x4_t row_accum3 = vdupq_n_s32(0);\n      for (int d = 0; d < accum_depth; d += 16) {\n        int8x16_t weights0 = vld1q_s8(shuffled_weights_ptr + 0);\n        int8x16_t weights1 = vld1q_s8(shuffled_weights_ptr + 16);\n        int8x16_t weights2 = vld1q_s8(shuffled_weights_ptr + 32);\n        int8x16_t weights3 = vld1q_s8(shuffled_weights_ptr + 48);\n        shuffled_weights_ptr += 64;\n        int8x16_t input =\n            vreinterpretq_s8_u8(vld1q_u8(shuffled_input_workspace_data + d));\n        int16x8_t local_accum0 =\n            vmull_s8(vget_low_s8(weights0), vget_low_s8(input));\n        int16x8_t local_accum1 =\n            vmull_s8(vget_low_s8(weights1), vget_low_s8(input));\n        int16x8_t local_accum2 =\n            vmull_s8(vget_low_s8(weights2), vget_low_s8(input));\n        int16x8_t local_accum3 =\n            vmull_s8(vget_low_s8(weights3), vget_low_s8(input));\n        local_accum0 =\n            vmlal_s8(local_accum0, vget_high_s8(weights0), vget_high_s8(input));\n        local_accum1 =\n            vmlal_s8(local_accum1, vget_high_s8(weights1), vget_high_s8(input));\n        local_accum2 =\n            vmlal_s8(local_accum2, vget_high_s8(weights2), vget_high_s8(input));\n        local_accum3 =\n            vmlal_s8(local_accum3, vget_high_s8(weights3), vget_high_s8(input));\n        row_accum0 = vpadalq_s16(row_accum0, local_accum0);\n        row_accum1 = vpadalq_s16(row_accum1, local_accum1);\n        row_accum2 = vpadalq_s16(row_accum2, local_accum2);\n        row_accum3 = vpadalq_s16(row_accum3, local_accum3);\n      }\n      // Horizontally reduce accumulators\n      int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,\n          pairwise_reduced_acc_2, pairwise_reduced_acc_3;\n      pairwise_reduced_acc_0 =\n          vpadd_s32(vget_low_s32(row_accum0), vget_high_s32(row_accum0));\n      pairwise_reduced_acc_1 =\n          vpadd_s32(vget_low_s32(row_accum1), vget_high_s32(row_accum1));\n      pairwise_reduced_acc_2 =\n          vpadd_s32(vget_low_s32(row_accum2), vget_high_s32(row_accum2));\n      pairwise_reduced_acc_3 =\n          vpadd_s32(vget_low_s32(row_accum3), vget_high_s32(row_accum3));\n      const int32x2_t reduced_lo =\n          vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n      const int32x2_t reduced_hi =\n          vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n      int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n      // Add bias values.\n      int32x4_t bias_vec = vld1q_s32(bias_data + c);\n      reduced = vaddq_s32(reduced, bias_vec);\n      reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));\n      // Multiply by the fixed-point multiplier.\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n      // Rounding-shift-right.\n      using gemmlowp::RoundingDivideByPOT;\n      reduced = RoundingDivideByPOT(reduced, right_shift);\n      // Narrow values down to 16 bit signed.\n      const int16x4_t res16 = vqmovn_s32(reduced);\n      vst1_s16(output_data + c, res16);\n    }\n  } else if (batches == 4) {\n    const int right_shift = output_shift > 0 ? 0 : -output_shift;\n    const int left_shift = output_shift > 0 ? output_shift : 0;\n    for (int c = 0; c < output_depth; c += 4) {\n      const int8* shuffled_input_ptr =\n          reinterpret_cast<const int8*>(shuffled_input_workspace_data);\n      // Accumulation loop.\n      int32x4_t row_accum00 = vdupq_n_s32(0);\n      int32x4_t row_accum10 = vdupq_n_s32(0);\n      int32x4_t row_accum20 = vdupq_n_s32(0);\n      int32x4_t row_accum30 = vdupq_n_s32(0);\n      int32x4_t row_accum01 = vdupq_n_s32(0);\n      int32x4_t row_accum11 = vdupq_n_s32(0);\n      int32x4_t row_accum21 = vdupq_n_s32(0);\n      int32x4_t row_accum31 = vdupq_n_s32(0);\n      int32x4_t row_accum02 = vdupq_n_s32(0);\n      int32x4_t row_accum12 = vdupq_n_s32(0);\n      int32x4_t row_accum22 = vdupq_n_s32(0);\n      int32x4_t row_accum32 = vdupq_n_s32(0);\n      int32x4_t row_accum03 = vdupq_n_s32(0);\n      int32x4_t row_accum13 = vdupq_n_s32(0);\n      int32x4_t row_accum23 = vdupq_n_s32(0);\n      int32x4_t row_accum33 = vdupq_n_s32(0);\n      for (int d = 0; d < accum_depth; d += 16) {\n        int8x16_t weights0 = vld1q_s8(shuffled_weights_ptr + 0);\n        int8x16_t weights1 = vld1q_s8(shuffled_weights_ptr + 16);\n        int8x16_t weights2 = vld1q_s8(shuffled_weights_ptr + 32);\n        int8x16_t weights3 = vld1q_s8(shuffled_weights_ptr + 48);\n        shuffled_weights_ptr += 64;\n        int8x16_t input0 = vld1q_s8(shuffled_input_ptr + 0);\n        int8x16_t input1 = vld1q_s8(shuffled_input_ptr + 16);\n        int8x16_t input2 = vld1q_s8(shuffled_input_ptr + 32);\n        int8x16_t input3 = vld1q_s8(shuffled_input_ptr + 48);\n        shuffled_input_ptr += 64;\n        int16x8_t local_accum0, local_accum1, local_accum2, local_accum3;\n#define TFLITE_SHUFFLED_FC_ACCUM(B)                                           \\\n  local_accum0 = vmull_s8(vget_low_s8(weights0), vget_low_s8(input##B));      \\\n  local_accum1 = vmull_s8(vget_low_s8(weights1), vget_low_s8(input##B));      \\\n  local_accum2 = vmull_s8(vget_low_s8(weights2), vget_low_s8(input##B));      \\\n  local_accum3 = vmull_s8(vget_low_s8(weights3), vget_low_s8(input##B));      \\\n  local_accum0 =                                                              \\\n      vmlal_s8(local_accum0, vget_high_s8(weights0), vget_high_s8(input##B)); \\\n  local_accum1 =                                                              \\\n      vmlal_s8(local_accum1, vget_high_s8(weights1), vget_high_s8(input##B)); \\\n  local_accum2 =                                                              \\\n      vmlal_s8(local_accum2, vget_high_s8(weights2), vget_high_s8(input##B)); \\\n  local_accum3 =                                                              \\\n      vmlal_s8(local_accum3, vget_high_s8(weights3), vget_high_s8(input##B)); \\\n  row_accum0##B = vpadalq_s16(row_accum0##B, local_accum0);                   \\\n  row_accum1##B = vpadalq_s16(row_accum1##B, local_accum1);                   \\\n  row_accum2##B = vpadalq_s16(row_accum2##B, local_accum2);                   \\\n  row_accum3##B = vpadalq_s16(row_accum3##B, local_accum3);\n\n        TFLITE_SHUFFLED_FC_ACCUM(0)\n        TFLITE_SHUFFLED_FC_ACCUM(1)\n        TFLITE_SHUFFLED_FC_ACCUM(2)\n        TFLITE_SHUFFLED_FC_ACCUM(3)\n\n#undef TFLITE_SHUFFLED_FC_ACCUM\n      }\n      // Horizontally reduce accumulators\n\n#define TFLITE_SHUFFLED_FC_STORE(B)                                           \\\n  {                                                                           \\\n    int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,                 \\\n        pairwise_reduced_acc_2, pairwise_reduced_acc_3;                       \\\n    pairwise_reduced_acc_0 =                                                  \\\n        vpadd_s32(vget_low_s32(row_accum0##B), vget_high_s32(row_accum0##B)); \\\n    pairwise_reduced_acc_1 =                                                  \\\n        vpadd_s32(vget_low_s32(row_accum1##B), vget_high_s32(row_accum1##B)); \\\n    pairwise_reduced_acc_2 =                                                  \\\n        vpadd_s32(vget_low_s32(row_accum2##B), vget_high_s32(row_accum2##B)); \\\n    pairwise_reduced_acc_3 =                                                  \\\n        vpadd_s32(vget_low_s32(row_accum3##B), vget_high_s32(row_accum3##B)); \\\n    const int32x2_t reduced_lo =                                              \\\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);            \\\n    const int32x2_t reduced_hi =                                              \\\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);            \\\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);                 \\\n    int32x4_t bias_vec = vld1q_s32(bias_data + c);                            \\\n    reduced = vaddq_s32(reduced, bias_vec);                                   \\\n    reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));                    \\\n    reduced = vqrdmulhq_n_s32(reduced, output_multiplier);                    \\\n    using gemmlowp::RoundingDivideByPOT;                                      \\\n    reduced = RoundingDivideByPOT(reduced, right_shift);                      \\\n    const int16x4_t res16 = vqmovn_s32(reduced);                              \\\n    vst1_s16(output_data + c + B * output_stride, res16);                     \\\n  }\n\n      TFLITE_SHUFFLED_FC_STORE(0);\n      TFLITE_SHUFFLED_FC_STORE(1);\n      TFLITE_SHUFFLED_FC_STORE(2);\n      TFLITE_SHUFFLED_FC_STORE(3);\n\n#undef TFLITE_SHUFFLED_FC_STORE\n    }\n  } else {\n    TFLITE_DCHECK(false);\n    return;\n  }\n#else\n  if (batches == 1) {\n    int16* output_ptr = output_data;\n    // Shuffled weights have had their sign bit (0x80) pre-flipped (xor'd)\n    // so that just reinterpreting them as int8 values is equivalent to\n    // subtracting 128 from them, thus implementing for free the subtraction of\n    // the zero_point value 128.\n    const int8* shuffled_weights_ptr =\n        reinterpret_cast<const int8*>(shuffled_weights_data);\n    // Likewise, we preshuffled and pre-xored the input data above.\n    const int8* shuffled_input_data =\n        reinterpret_cast<const int8*>(shuffled_input_workspace_data);\n    for (int c = 0; c < output_depth; c += 4) {\n      // Internal accumulation.\n      // Initialize accumulator with the bias-value.\n      int32 accum[4] = {0};\n      // Accumulation loop.\n      for (int d = 0; d < accum_depth; d += 16) {\n        for (int i = 0; i < 4; i++) {\n          for (int j = 0; j < 16; j++) {\n            int8 input_val = shuffled_input_data[d + j];\n            int8 weights_val = *shuffled_weights_ptr++;\n            accum[i] += weights_val * input_val;\n          }\n        }\n      }\n      for (int i = 0; i < 4; i++) {\n        // Add bias value\n        int acc = accum[i] + bias_data[c + i];\n        // Down-scale the final int32 accumulator to the scale used by our\n        // (16-bit, typically 3 integer bits) fixed-point format. The quantized\n        // multiplier and shift here have been pre-computed offline\n        // (e.g. by toco).\n        acc =\n            MultiplyByQuantizedMultiplier(acc, output_multiplier, output_shift);\n        // Saturate, cast to int16, and store to output array.\n        acc = std::max(acc, -32768);\n        acc = std::min(acc, 32767);\n        output_ptr[c + i] = acc;\n      }\n    }\n  } else if (batches == 4) {\n    int16* output_ptr = output_data;\n    // Shuffled weights have had their sign bit (0x80) pre-flipped (xor'd)\n    // so that just reinterpreting them as int8 values is equivalent to\n    // subtracting 128 from them, thus implementing for free the subtraction of\n    // the zero_point value 128.\n    const int8* shuffled_weights_ptr =\n        reinterpret_cast<const int8*>(shuffled_weights_data);\n    // Likewise, we preshuffled and pre-xored the input data above.\n    const int8* shuffled_input_data =\n        reinterpret_cast<const int8*>(shuffled_input_workspace_data);\n    for (int c = 0; c < output_depth; c += 4) {\n      const int8* shuffled_input_ptr = shuffled_input_data;\n      // Accumulation loop.\n      // Internal accumulation.\n      // Initialize accumulator with the bias-value.\n      int32 accum[4][4];\n      for (int i = 0; i < 4; i++) {\n        for (int b = 0; b < 4; b++) {\n          accum[i][b] = 0;\n        }\n      }\n      for (int d = 0; d < accum_depth; d += 16) {\n        for (int i = 0; i < 4; i++) {\n          for (int b = 0; b < 4; b++) {\n            for (int j = 0; j < 16; j++) {\n              int8 input_val = shuffled_input_ptr[16 * b + j];\n              int8 weights_val = shuffled_weights_ptr[16 * i + j];\n              accum[i][b] += weights_val * input_val;\n            }\n          }\n        }\n        shuffled_input_ptr += 64;\n        shuffled_weights_ptr += 64;\n      }\n      for (int i = 0; i < 4; i++) {\n        for (int b = 0; b < 4; b++) {\n          // Add bias value\n          int acc = accum[i][b] + bias_data[c + i];\n          // Down-scale the final int32 accumulator to the scale used by our\n          // (16-bit, typically 3 integer bits) fixed-point format. The\n          // quantized multiplier and shift here have been pre-computed offline\n          // (e.g. by toco).\n          acc = MultiplyByQuantizedMultiplier(acc, output_multiplier,\n                                              output_shift);\n          // Saturate, cast to int16, and store to output array.\n          acc = std::max(acc, -32768);\n          acc = std::min(acc, 32767);\n          output_ptr[b * output_stride + c + i] = acc;\n        }\n      }\n    }\n  } else {\n    TFLITE_DCHECK(false);\n    return;\n  }\n#endif\n}\n\n// Wraps ShuffledFullyConnectedWorkerImpl into a Task class\n// to allow using gemmlowp's threadpool.\nstruct ShuffledFullyConnectedWorkerTask : cpu_backend_threadpool::Task {\n  ShuffledFullyConnectedWorkerTask(const uint8* input_data,\n                                   const int8* shuffled_weights_data,\n                                   int batches, int output_depth,\n                                   int output_stride, int accum_depth,\n                                   const int32* bias_data,\n                                   int32 output_multiplier, int output_shift,\n                                   int16* output_data)\n      : input_data_(input_data),\n        shuffled_weights_data_(shuffled_weights_data),\n        batches_(batches),\n        output_depth_(output_depth),\n        output_stride_(output_stride),\n        accum_depth_(accum_depth),\n        bias_data_(bias_data),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_data_(output_data) {}\n\n  void Run() override {\n    ShuffledFullyConnectedWorkerImpl(\n        input_data_, shuffled_weights_data_, batches_, output_depth_,\n        output_stride_, accum_depth_, bias_data_, output_multiplier_,\n        output_shift_, output_data_);\n  }\n\n  const uint8* input_data_;\n  const int8* shuffled_weights_data_;\n  int batches_;\n  int output_depth_;\n  int output_stride_;\n  int accum_depth_;\n  const int32* bias_data_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int16* output_data_;\n};\n\ninline void ShuffledFullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& weights_shape,\n    const uint8* shuffled_weights_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, uint8* shuffled_input_workspace_data,\n    CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"ShuffledFullyConnected/8bit\");\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_EQ(output_activation_min, -32768);\n  TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(weights_shape, weights_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = weights_shape.Dims(weights_dim_count - 1);\n  TFLITE_DCHECK((accum_depth % 16) == 0);\n  TFLITE_DCHECK((output_depth % 4) == 0);\n  // Shuffled weights have had their sign bit (0x80) pre-flipped (xor'd)\n  // so that just reinterpreting them as int8 values is equivalent to\n  // subtracting 128 from them, thus implementing for free the subtraction of\n  // the zero_point value 128.\n  const int8* int8_shuffled_weights_data =\n      reinterpret_cast<const int8*>(shuffled_weights_data);\n\n  // Shuffling and xoring of input activations into the workspace buffer\n  if (batches == 1) {\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (int i = 0; i < accum_depth; i += 16) {\n      uint8x16_t val = vld1q_u8(input_data + i);\n      val = veorq_u8(val, signbit);\n      vst1q_u8(shuffled_input_workspace_data + i, val);\n    }\n#else\n    for (int i = 0; i < accum_depth; i++) {\n      shuffled_input_workspace_data[i] = input_data[i] ^ 0x80;\n    }\n#endif\n  } else if (batches == 4) {\n    uint8* shuffled_input_workspace_ptr = shuffled_input_workspace_data;\n    int c = 0;\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (c = 0; c < accum_depth; c += 16) {\n      const uint8* src_data_ptr = input_data + c;\n      uint8x16_t val0 = vld1q_u8(src_data_ptr + 0 * accum_depth);\n      uint8x16_t val1 = vld1q_u8(src_data_ptr + 1 * accum_depth);\n      uint8x16_t val2 = vld1q_u8(src_data_ptr + 2 * accum_depth);\n      uint8x16_t val3 = vld1q_u8(src_data_ptr + 3 * accum_depth);\n      val0 = veorq_u8(val0, signbit);\n      val1 = veorq_u8(val1, signbit);\n      val2 = veorq_u8(val2, signbit);\n      val3 = veorq_u8(val3, signbit);\n      vst1q_u8(shuffled_input_workspace_ptr + 0, val0);\n      vst1q_u8(shuffled_input_workspace_ptr + 16, val1);\n      vst1q_u8(shuffled_input_workspace_ptr + 32, val2);\n      vst1q_u8(shuffled_input_workspace_ptr + 48, val3);\n      shuffled_input_workspace_ptr += 64;\n    }\n#else\n    for (c = 0; c < accum_depth; c += 16) {\n      for (int b = 0; b < 4; b++) {\n        const uint8* src_data_ptr = input_data + b * accum_depth + c;\n        for (int j = 0; j < 16; j++) {\n          uint8 src_val = *src_data_ptr++;\n          // Flip the sign bit, so that the kernel will only need to\n          // reinterpret these uint8 values as int8, getting for free the\n          // subtraction of the zero_point value 128.\n          uint8 dst_val = src_val ^ 0x80;\n          *shuffled_input_workspace_ptr++ = dst_val;\n        }\n      }\n    }\n#endif\n  } else {\n    TFLITE_DCHECK(false);\n    return;\n  }\n\n  static constexpr int kKernelRows = 4;\n  const int thread_count =\n      LegacyHowManyThreads<kKernelRows>(cpu_backend_context->max_num_threads(),\n                                        output_depth, batches, accum_depth);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    ShuffledFullyConnectedWorkerImpl(\n        shuffled_input_workspace_data, int8_shuffled_weights_data, batches,\n        output_depth, output_depth, accum_depth, bias_data, output_multiplier,\n        output_shift, output_data);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<ShuffledFullyConnectedWorkerTask> tasks;\n  // TODO(b/131746020) don't create new heap allocations every time.\n  // At least we make it a single heap allocation by using reserve().\n  tasks.reserve(thread_count);\n  const int kRowsPerWorker =\n      RoundUp<kKernelRows>(CeilQuotient(output_depth, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; i++) {\n    int row_end = std::min(output_depth, row_start + kRowsPerWorker);\n    tasks.emplace_back(shuffled_input_workspace_data,\n                       int8_shuffled_weights_data + row_start * accum_depth,\n                       batches, row_end - row_start, output_depth, accum_depth,\n                       bias_data + row_start, output_multiplier, output_shift,\n                       output_data + row_start);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_depth);\n  cpu_backend_threadpool::Execute(tasks.size(), tasks.data(),\n                                  cpu_backend_context);\n}\n\n#ifdef USE_NEON\n\ninline int32x4_t RoundToNearest(const float32x4_t input) {\n#if defined(__aarch64__) || defined(__SSSE3__)\n  // Note: vcvtnq_s32_f32 is not available in ARMv7\n  return vcvtnq_s32_f32(input);\n#else\n  static const float32x4_t zero_val_dup = vdupq_n_f32(0.0f);\n  static const float32x4_t point5_val_dup = vdupq_n_f32(0.5f);\n  static const float32x4_t minus_point5_val_dup = vdupq_n_f32(-0.5f);\n\n  const uint32x4_t mask = vcltq_f32(input, zero_val_dup);\n  const float32x4_t round =\n      vbslq_f32(mask, minus_point5_val_dup, point5_val_dup);\n  return vcvtq_s32_f32(vaddq_f32(input, round));\n#endif  // defined(__aarch64__) || defined(__SSSE3__)\n}\n\ninline uint32x4_t RoundToNearestUnsigned(const float32x4_t input) {\n#if defined(__aarch64__)\n  // Note that vcvtnq_u32_f32 is not available in ARMv7 or in arm_neon_sse.h.\n  return vcvtnq_u32_f32(input);\n#else\n  static const float32x4_t point5_val_dup = vdupq_n_f32(0.5f);\n\n  return vcvtq_u32_f32(vaddq_f32(input, point5_val_dup));\n#endif  // defined(__aarch64__)\n}\n\n#endif  // USE_NEON\n\ninline void MeanImpl(const tflite::MeanParams& op_params,\n                     const RuntimeShape& input_shape, const uint8_t* input_data,\n                     int32 multiplier, int32 shift, int32 bias,\n                     const RuntimeShape& output_shape, uint8_t* output_data,\n                     int start_depth, int end_depth) {\n  ruy::profiler::ScopeLabel label(\"Mean4D/Uint8/MeanImpl\");\n\n  // Current implementation only supports dimension equals 4 and simultaneous\n  // reduction over width and height.\n  const int output_batch = output_shape.Dims(0);\n  const int output_height = output_shape.Dims(2);\n  const int output_width = output_shape.Dims(2);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n\n  TFLITE_CHECK_EQ(op_params.axis_count, 2);\n  TFLITE_CHECK((op_params.axis[0] == 1 && op_params.axis[1] == 2) ||\n               (op_params.axis[0] == 2 && op_params.axis[1] == 1));\n  TFLITE_CHECK_EQ(output_height, 1);\n  TFLITE_CHECK_EQ(output_width, 1);\n\n  constexpr int32_t kMinValue = std::numeric_limits<uint8_t>::min();\n  constexpr int32_t kMaxValue = std::numeric_limits<uint8_t>::max();\n\n#ifdef USE_NEON\n  const int32x4_t bias_dup = vdupq_n_s32(bias);\n  const int32x4_t min_dup = vdupq_n_s32(kMinValue);\n  const int32x4_t max_dup = vdupq_n_s32(kMaxValue);\n#endif  // USE_NEON\n\n  for (int out_b = 0; out_b < output_batch; ++out_b) {\n    int out_d = start_depth;\n#ifdef USE_NEON\n\n    for (; out_d <= end_depth - 16; out_d += 16) {\n      int32x4x4_t temp_sum;\n      temp_sum.val[0] = vdupq_n_s32(0);\n      temp_sum.val[1] = vdupq_n_s32(0);\n      temp_sum.val[2] = vdupq_n_s32(0);\n      temp_sum.val[3] = vdupq_n_s32(0);\n      for (int in_h = 0; in_h < input_height; ++in_h) {\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          const uint8_t* input_data_ptr =\n              input_data + Offset(input_shape, out_b, in_h, in_w, out_d);\n          uint8x16_t input_data_val = vld1q_u8(input_data_ptr);\n\n          int16x8_t input_data_low_shift =\n              vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(input_data_val)));\n          int16x8_t input_data_high_shift =\n              vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(input_data_val)));\n\n          int32x4_t input_low_low =\n              vmovl_s16(vget_low_s16(input_data_low_shift));\n          int32x4_t input_high_low =\n              vmovl_s16(vget_high_s16(input_data_low_shift));\n          int32x4_t input_low_high =\n              vmovl_s16(vget_low_s16(input_data_high_shift));\n          int32x4_t input_high_high =\n              vmovl_s16(vget_high_s16(input_data_high_shift));\n\n          temp_sum.val[0] = vaddq_s32(temp_sum.val[0], input_low_low);\n          temp_sum.val[1] = vaddq_s32(temp_sum.val[1], input_high_low);\n          temp_sum.val[2] = vaddq_s32(temp_sum.val[2], input_low_high);\n          temp_sum.val[3] = vaddq_s32(temp_sum.val[3], input_high_high);\n        }\n      }\n\n      temp_sum =\n          MultiplyByQuantizedMultiplier4Rows(temp_sum, multiplier, shift);\n\n      temp_sum.val[0] = vaddq_s32(temp_sum.val[0], bias_dup);\n      temp_sum.val[1] = vaddq_s32(temp_sum.val[1], bias_dup);\n      temp_sum.val[2] = vaddq_s32(temp_sum.val[2], bias_dup);\n      temp_sum.val[3] = vaddq_s32(temp_sum.val[3], bias_dup);\n\n      temp_sum.val[0] = vminq_s32(vmaxq_s32(temp_sum.val[0], min_dup), max_dup);\n      temp_sum.val[1] = vminq_s32(vmaxq_s32(temp_sum.val[1], min_dup), max_dup);\n      temp_sum.val[2] = vminq_s32(vmaxq_s32(temp_sum.val[2], min_dup), max_dup);\n      temp_sum.val[3] = vminq_s32(vmaxq_s32(temp_sum.val[3], min_dup), max_dup);\n\n      uint16x4_t narrowed_low_low =\n          vmovn_u32(vreinterpretq_u32_s32(temp_sum.val[0]));\n      uint16x4_t narrowed_high_low =\n          vmovn_u32(vreinterpretq_u32_s32(temp_sum.val[1]));\n      uint16x4_t narrowed_low_high =\n          vmovn_u32(vreinterpretq_u32_s32(temp_sum.val[2]));\n      uint16x4_t narrowed_high_high =\n          vmovn_u32(vreinterpretq_u32_s32(temp_sum.val[3]));\n\n      uint16x8_t combined_low =\n          vcombine_u16(narrowed_low_low, narrowed_high_low);\n      uint16x8_t combined_high =\n          vcombine_u16(narrowed_low_high, narrowed_high_high);\n\n      uint8x8_t narrowed_low = vmovn_u16(combined_low);\n      uint8x8_t narrowed_high = vmovn_u16(combined_high);\n\n      uint8x16_t combined_output = vcombine_u8(narrowed_low, narrowed_high);\n\n      uint8_t* output_data_ptr =\n          output_data + Offset(output_shape, out_b, 0, 0, out_d);\n      vst1q_u8(output_data_ptr, combined_output);\n    }\n#endif  // USE_NEON\n\n    for (; out_d < end_depth; ++out_d) {\n      int acc = 0;\n      for (int in_h = 0; in_h < input_height; ++in_h) {\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          acc += input_data[Offset(input_shape, out_b, in_h, in_w, out_d)];\n        }\n      }\n\n      acc = MultiplyByQuantizedMultiplier(acc, multiplier, shift);\n      acc += bias;\n      acc = std::min(std::max(acc, kMinValue), kMaxValue);\n      output_data[Offset(output_shape, out_b, 0, 0, out_d)] =\n          static_cast<uint8_t>(acc);\n    }\n  }\n}\n\nstruct MeanWorkerTask : cpu_backend_threadpool::Task {\n  MeanWorkerTask(const tflite::MeanParams& op_params,\n                 const RuntimeShape& input_shape, const uint8_t* input_data,\n                 int32 multiplier, int32 shift, int32 bias,\n                 const RuntimeShape& output_shape, uint8_t* output_data,\n                 int start_height, int end_height)\n      : op_params(op_params),\n        input_shape(input_shape),\n        input_data(input_data),\n        multiplier(multiplier),\n        shift(shift),\n        bias(bias),\n        output_shape(output_shape),\n        output_data(output_data),\n        start_height(start_height),\n        end_height(end_height) {}\n\n  void Run() override {\n    MeanImpl(op_params, input_shape, input_data, multiplier, shift, bias,\n             output_shape, output_data, start_height, end_height);\n  }\n\n private:\n  const tflite::MeanParams& op_params;\n  const RuntimeShape& input_shape;\n  const uint8_t* input_data;\n  int32 multiplier;\n  int32 shift;\n  int32 bias;\n  const RuntimeShape& output_shape;\n  uint8_t* output_data;\n  int start_height;\n  int end_height;\n};\n\ninline void Mean(const tflite::MeanParams& op_params,\n                 const RuntimeShape& unextended_input_shape,\n                 const uint8_t* input_data, int32 input_zero_point,\n                 float input_scale, const RuntimeShape& unextended_output_shape,\n                 uint8_t* output_data, int32 output_zero_point,\n                 float output_scale, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"Mean4D/Uint8\");\n  // Current implementation only supports dimension equals 4 and simultaneous\n  // reduction over width and height.\n  TFLITE_CHECK_EQ(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_CHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_depth = output_shape.Dims(3);\n\n  TFLITE_CHECK_EQ(op_params.axis_count, 2);\n  TFLITE_CHECK((op_params.axis[0] == 1 && op_params.axis[1] == 2) ||\n               (op_params.axis[0] == 2 && op_params.axis[1] == 1));\n  TFLITE_CHECK_EQ(output_height, 1);\n  TFLITE_CHECK_EQ(output_width, 1);\n\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const float num_elements_in_axis = input_width * input_height;\n\n  float temp = input_zero_point * input_scale / output_scale;\n  temp = temp > 0 ? temp + 0.5f : temp - 0.5f;\n  int32_t bias = output_zero_point - static_cast<int32_t>(temp);\n  float real_scale = input_scale / (num_elements_in_axis * output_scale);\n\n  int32 multiplier, shift;\n  QuantizeMultiplier(real_scale, &multiplier, &shift);\n\n  constexpr int kMinDepthPerThread = 8;\n  int thread_count = output_depth / kMinDepthPerThread;\n  thread_count = thread_count > 0 ? thread_count : 1;\n  const int capped_thread_count =\n      std::min(thread_count, cpu_backend_context->max_num_threads());\n\n  if (capped_thread_count == 1) {\n    MeanImpl(op_params, input_shape, input_data, multiplier, shift, bias,\n             output_shape, output_data, 0, output_depth);\n  } else {\n    // Instead parallel for batch, we loop for the output_depth since batch\n    // is typical 1.\n    std::vector<MeanWorkerTask> tasks;\n    // TODO(b/131746020) don't create new heap allocations every time.\n    // At least we make it a single heap allocation by using reserve().\n    tasks.reserve(capped_thread_count);\n    int depth_start = 0;\n    for (int i = 0; i < capped_thread_count; ++i) {\n      // Try to distribute the tasks as even as possible.\n      int depth_end = depth_start +\n                      (output_depth - depth_start) / (capped_thread_count - i);\n      tasks.emplace_back(op_params, input_shape, input_data, multiplier, shift,\n                         bias, output_shape, output_data, depth_start,\n                         depth_end);\n      depth_start = depth_end;\n    }\n    cpu_backend_threadpool::Execute(tasks.size(), tasks.data(),\n                                    cpu_backend_context);\n  }\n}\n\ntemplate <typename T, typename U>\ninline bool MeanGeneral(const T* input_data, const int* input_dims,\n                        const int input_num_dims, T* output_data,\n                        const int* output_dims, const int output_num_dims,\n                        const int* axis, const int num_axis_dimensions,\n                        bool keep_dims, int* temp_index, int* resolved_axis,\n                        U* temp_sum) {\n  return reference_ops::Mean(input_data, input_dims, input_num_dims,\n                             output_data, output_dims, output_num_dims, axis,\n                             num_axis_dimensions, keep_dims, temp_index,\n                             resolved_axis, temp_sum);\n}\n\ntemplate <>\ninline bool MeanGeneral<float, float>(\n    const float* input_data, const int* input_dims, const int input_num_dims,\n    float* output_data, const int* output_dims, const int output_num_dims,\n    const int* axis, const int num_axis_dimensions, bool keep_dims,\n    int* temp_index, int* resolved_axis, float* temp_sum) {\n  // Handle reduce_mean for the last dimensions.\n  if (num_axis_dimensions == 1 && axis[0] == (input_num_dims - 1)) {\n    ruy::profiler::ScopeLabel label(\"MeanLastDim/Float\");\n    int output_size = 1;\n    for (int i = 0; i < input_num_dims - 1; ++i) {\n      output_size *= input_dims[i];\n    }\n    const int last_input_dim = input_dims[axis[0]];\n\n    // TODO(b/152563685): Consider use eigen to cover more general cases.\n    const MatrixMap<const float> in_mat(input_data, last_input_dim,\n                                        output_size);\n    VectorMap<float> out(output_data, output_size, 1);\n    out = (in_mat.array().colwise().sum()) / static_cast<float>(last_input_dim);\n    return true;\n  }\n\n  return reference_ops::Mean(input_data, input_dims, input_num_dims,\n                             output_data, output_dims, output_num_dims, axis,\n                             num_axis_dimensions, keep_dims, temp_index,\n                             resolved_axis, temp_sum);\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const float* input_data, const RuntimeShape& filter_shape,\n                 const float* filter_data, const RuntimeShape& bias_shape,\n                 const float* bias_data, const RuntimeShape& output_shape,\n                 float* output_data, const RuntimeShape& im2col_shape,\n                 float* im2col_data, CpuBackendContext* cpu_backend_context) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  ruy::profiler::ScopeLabel label(\"Conv\");\n\n  // NB: the float 0.0f value is represented by all zero bytes.\n  const uint8 float_zero_byte = 0x00;\n  const float* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    DilatedIm2col(params, float_zero_byte, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    Im2col(params, filter_height, filter_width, float_zero_byte, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  const int gemm_input_dims = gemm_input_shape->DimensionsCount();\n  int m = FlatSizeSkipDim(*gemm_input_shape, gemm_input_dims - 1);\n  int n = output_shape.Dims(3);\n  int k = gemm_input_shape->Dims(gemm_input_dims - 1);\n\n#if defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n  // The following code computes matrix multiplication c = a * transponse(b)\n  // with CBLAS, where:\n  // * `a` is a matrix with dimensions (m, k).\n  // * `b` is a matrix with dimensions (n, k), so transpose(b) is (k, n).\n  // * `c` is a matrix with dimensions (m, n).\n  // The naming of variables are aligned with CBLAS specification here.\n  const float* a = gemm_input_data;\n  const float* b = filter_data;\n  float* c = output_data;\n  // The stride of matrix a, b and c respectively.\n  int stride_a = k;\n  int stride_b = k;\n  int stride_c = n;\n\n  cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans, m, n, k, 1.0f, a,\n              stride_a, b, stride_b, 0.0f, c, stride_c);\n  optimized_ops::AddBiasAndEvalActivationFunction(\n      output_activation_min, output_activation_max, bias_shape, bias_data,\n      output_shape, output_data);\n#else\n  // When an optimized CBLAS implementation is not available, fall back\n  // to using cpu_backend_gemm.\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = n;\n  lhs_params.cols = k;\n  cpu_backend_gemm::MatrixParams<float> rhs_params;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.rows = k;\n  rhs_params.cols = m;\n  cpu_backend_gemm::MatrixParams<float> dst_params;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.rows = n;\n  dst_params.cols = m;\n  cpu_backend_gemm::GemmParams<float, float> gemm_params;\n  gemm_params.bias = bias_data;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, gemm_input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n#endif  //  defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n}\n\ninline void HybridConv(const ConvParams& params, float* scaling_factors_ptr,\n                       const RuntimeShape& input_shape,\n                       const int8_t* input_data,\n                       const RuntimeShape& filter_shape,\n                       const int8_t* filter_data,\n                       const RuntimeShape& bias_shape, const float* bias_data,\n                       const RuntimeShape& accum_scratch_shape,\n                       int32_t* accum_scratch, const RuntimeShape& output_shape,\n                       float* output_data, const RuntimeShape& im2col_shape,\n                       int8_t* im2col_data, CpuBackendContext* context) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int batch_size = input_shape.Dims(0);\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n\n  const int input_zero_point = 0;\n  const int8_t* gemm_input_data = nullptr;\n  int num_input;\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n\n  if (need_dilated_im2col) {\n    DilatedIm2col(params, input_zero_point, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    num_input = im2col_shape.FlatSize();\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    // symmetric quantization assumes zero point of 0.\n\n    Im2col(params, filter_height, filter_width, input_zero_point, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    num_input = im2col_shape.FlatSize();\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    num_input = input_shape.FlatSize();\n  }\n\n  // Flatten 4D matrices into 2D matrices for matrix multiplication.\n\n  // Flatten so that each filter has its own row.\n  const int filter_rows = filter_shape.Dims(0);\n  const int filter_cols = FlatSizeSkipDim(filter_shape, 0);\n\n  // In MatrixBatchVectorMultiplyAccumulate, each output value is the\n  // dot product of one row of the first matrix with one row of the second\n  // matrix. Therefore, the number of cols in each matrix are equivalent.\n  //\n  // After Im2Col, each input patch becomes a row.\n  const int gemm_input_cols = filter_cols;\n  const int gemm_input_rows = num_input / gemm_input_cols;\n\n  const int output_cols = output_shape.Dims(3);\n  const int output_rows = FlatSizeSkipDim(output_shape, 3);\n  TFLITE_DCHECK_EQ(output_cols, filter_rows);\n  TFLITE_DCHECK_EQ(output_rows, gemm_input_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_cols);\n\n  // MatrixBatchVectorMultiplyAccumulate assumes that each row of the second\n  // input matrix has its own scale factor. This code duplicates the scale\n  // factors for each row in the same batch.\n  const int rows_per_batch = gemm_input_rows / batch_size;\n  for (int i = gemm_input_rows - 1; i >= 0; --i) {\n    scaling_factors_ptr[i] = scaling_factors_ptr[i / rows_per_batch];\n  }\n\n  std::fill_n(output_data, output_rows * output_cols, 0.0f);\n\n  // The scratch buffer must have the same size as the output.\n  TFLITE_DCHECK_EQ(accum_scratch_shape.FlatSize(), output_shape.FlatSize());\n  tensor_utils::MatrixBatchVectorMultiplyAccumulate(\n      filter_data, filter_rows, filter_cols, gemm_input_data,\n      scaling_factors_ptr, /*n_batch=*/gemm_input_rows, accum_scratch,\n      output_data, context);\n  AddBiasAndEvalActivationFunction(output_activation_min, output_activation_max,\n                                   bias_shape, bias_data, output_shape,\n                                   output_data);\n}\n\ninline void HybridConvPerChannel(\n    const ConvParams& params, float* scaling_factors_ptr,\n    const RuntimeShape& input_shape, const int8_t* input_data,\n    const RuntimeShape& filter_shape, const int8_t* filter_data,\n    const RuntimeShape& bias_shape, const float* bias_data,\n    const RuntimeShape& output_shape, float* output_data,\n    const RuntimeShape& im2col_shape, int8_t* im2col_data,\n    const float* per_channel_scale, int32_t* input_offset,\n    const RuntimeShape& scratch_shape, int32_t* scratch, int32_t* row_sums,\n    bool* compute_row_sums, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"ConvHybridPerChannel\");\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int8* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n\n  const int batch_size = input_shape.Dims(0);\n\n  if (need_dilated_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    optimized_ops::DilatedIm2col(params, input_shape, input_data, filter_shape,\n                                 output_shape, im2col_data, input_offset,\n                                 batch_size);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    Im2col(params, filter_height, filter_width, input_offset, batch_size,\n           input_shape, input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  const int filter_rows = filter_shape.Dims(0);\n  const int filter_cols = FlatSizeSkipDim(filter_shape, 0);\n\n  const int gemm_input_rows = gemm_input_shape->Dims(3);\n  const int gemm_input_cols = FlatSizeSkipDim(*gemm_input_shape, 3);\n  const int output_rows = output_shape.Dims(3);\n  const int output_cols =\n      output_shape.Dims(0) * output_shape.Dims(1) * output_shape.Dims(2);\n\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, gemm_input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, gemm_input_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n  TFLITE_DCHECK_EQ(scratch_shape.FlatSize(), output_shape.FlatSize());\n  if (!compute_row_sums || *compute_row_sums) {\n    tensor_utils::ReductionSumVector(filter_data, row_sums, filter_rows,\n                                     filter_cols);\n    if (compute_row_sums) {\n      *compute_row_sums = false;\n    }\n  }\n\n  cpu_backend_gemm::MatrixParams<int8> lhs_params;\n  lhs_params.rows = filter_rows;\n  lhs_params.cols = filter_cols;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n\n  cpu_backend_gemm::MatrixParams<int8> rhs_params;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.rows = gemm_input_rows;\n  rhs_params.cols = gemm_input_cols;\n\n  cpu_backend_gemm::MatrixParams<int32> dst_params;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.rows = output_rows;\n  dst_params.cols = output_cols;\n\n  // TODO(b/149003801): Use hybrid gemm once supported in Ruy.\n  cpu_backend_gemm::GemmParams<int32_t, int32_t> gemm_params;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, gemm_input_data,\n                         dst_params, scratch, gemm_params, cpu_backend_context);\n\n  MatrixMap<float> out_mat(output_data, filter_rows, output_cols);\n  MatrixMap<int32_t> in_mat(scratch, filter_rows, output_cols);\n  VectorMap<const float> bias_data_vec(bias_data, filter_rows, 1);\n  VectorMap<int32_t> row_sums_vec(row_sums, filter_rows, 1);\n  VectorMap<const float> per_channel_scale_vec(per_channel_scale, filter_rows,\n                                               1);\n  const int cols_per_batch = output_cols / batch_size;\n  for (int c = 0; c < output_cols; c++) {\n    const int b = c / cols_per_batch;\n    const float input_scale = scaling_factors_ptr[b];\n    const int32_t zero_point = input_offset[b];\n    out_mat.col(c) =\n        (((in_mat.col(c) - (row_sums_vec * zero_point))\n              .cast<float>()\n              .cwiseProduct((per_channel_scale_vec * input_scale))) +\n         bias_data_vec)\n            .cwiseMin(params.float_activation_max)\n            .cwiseMax(params.float_activation_min);\n  }\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const uint8* input_data, const RuntimeShape& filter_shape,\n                 const uint8* filter_data, const RuntimeShape& bias_shape,\n                 const int32* bias_data, const RuntimeShape& output_shape,\n                 uint8* output_data, const RuntimeShape& im2col_shape,\n                 uint8* im2col_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"Conv/8bit\");\n\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const uint8* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    DilatedIm2col(params, input_zero_point, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    Im2col(params, filter_height, filter_width, input_zero_point, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  const int gemm_input_rows = gemm_input_shape->Dims(3);\n  // Using FlatSizeSkipDim causes segfault in some contexts (see b/79927784).\n  // The root cause has not yet been identified though. Same applies below for\n  // the other calls commented out. This is a partial rollback of cl/196819423.\n  // const int gemm_input_cols = FlatSizeSkipDim(*gemm_input_shape, 3);\n  const int gemm_input_cols = gemm_input_shape->Dims(0) *\n                              gemm_input_shape->Dims(1) *\n                              gemm_input_shape->Dims(2);\n  const int filter_rows = filter_shape.Dims(0);\n  // See b/79927784.\n  // const int filter_cols = FlatSizeSkipDim(filter_shape, 0);\n  const int filter_cols =\n      filter_shape.Dims(1) * filter_shape.Dims(2) * filter_shape.Dims(3);\n  const int output_rows = output_shape.Dims(3);\n  // See b/79927784.\n  // const int output_cols = FlatSizeSkipDim(output_shape, 3);\n  const int output_cols =\n      output_shape.Dims(0) * output_shape.Dims(1) * output_shape.Dims(2);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, gemm_input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, gemm_input_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n  cpu_backend_gemm::MatrixParams<uint8> lhs_params;\n  lhs_params.rows = filter_rows;\n  lhs_params.cols = filter_cols;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.zero_point = -filter_offset;\n  cpu_backend_gemm::MatrixParams<uint8> rhs_params;\n  rhs_params.rows = gemm_input_rows;\n  rhs_params.cols = gemm_input_cols;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.zero_point = -input_offset;\n  cpu_backend_gemm::MatrixParams<uint8> dst_params;\n  dst_params.rows = output_rows;\n  dst_params.cols = output_cols;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.zero_point = output_offset;\n  cpu_backend_gemm::GemmParams<int32, uint8> gemm_params;\n  gemm_params.bias = bias_data;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  gemm_params.multiplier_fixedpoint = output_multiplier;\n  gemm_params.multiplier_exponent = output_shift;\n  cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params, gemm_input_data,\n                         dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\ntemplate <typename T>\ninline void DepthToSpace(const tflite::DepthToSpaceParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const T* input_data,\n                         const RuntimeShape& unextended_output_shape,\n                         T* output_data) {\n  ruy::profiler::ScopeLabel label(\"DepthToSpace\");\n\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  const int input_depth = input_shape.Dims(3);\n  const int input_width = input_shape.Dims(2);\n  const int input_height = input_shape.Dims(1);\n\n  const int output_depth = output_shape.Dims(3);\n  const int batch_size = output_shape.Dims(0);\n\n  // Number of continuous values that we can copy in one interation.\n  const int stride = op_params.block_size * output_depth;\n\n  for (int batch = 0; batch < batch_size; ++batch) {\n    for (int in_h = 0; in_h < input_height; ++in_h) {\n      const T* input_ptr = input_data + Offset(input_shape, batch, in_h, 0, 0);\n      for (int offset_h = 0; offset_h < op_params.block_size; ++offset_h) {\n        const T* src = input_ptr;\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          memcpy(output_data, src, stride * sizeof(T));\n          output_data += stride;\n          src += input_depth;\n        }\n        input_ptr += stride;\n      }\n    }\n  }\n}\n\ntemplate <typename T>\ninline void SpaceToDepth(const tflite::SpaceToDepthParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const T* input_data,\n                         const RuntimeShape& unextended_output_shape,\n                         T* output_data) {\n  ruy::profiler::ScopeLabel label(\"SpaceToDepth\");\n\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  const int output_depth = output_shape.Dims(3);\n  const int output_width = output_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n\n  const int input_depth = input_shape.Dims(3);\n  const int batch_size = input_shape.Dims(0);\n\n  // Number of continuous values that we can copy in one interation.\n  const int stride = op_params.block_size * input_depth;\n\n  for (int batch = 0; batch < batch_size; ++batch) {\n    for (int out_h = 0; out_h < output_height; ++out_h) {\n      T* output_ptr = output_data + Offset(output_shape, batch, out_h, 0, 0);\n      for (int offset_h = 0; offset_h < op_params.block_size; ++offset_h) {\n        T* dst = output_ptr;\n        for (int out_w = 0; out_w < output_width; ++out_w) {\n          memcpy(dst, input_data, stride * sizeof(T));\n          input_data += stride;\n          dst += output_depth;\n        }\n        output_ptr += stride;\n      }\n    }\n  }\n}\n\ninline void Relu(const RuntimeShape& input_shape, const float* input_data,\n                 const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Relu (not fused)\");\n\n  const auto input = MapAsVector(input_data, input_shape);\n  auto output = MapAsVector(output_data, output_shape);\n  output = input.cwiseMax(0.0f);\n}\n\ninline void L2Normalization(const tflite::L2NormalizationParams& op_params,\n                            const RuntimeShape& input_shape,\n                            const float* input_data,\n                            const RuntimeShape& output_shape,\n                            float* output_data, float epsilon = 1e-6) {\n  ruy::profiler::ScopeLabel label(\"L2Normalization\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n  for (int i = 0; i < outer_size; ++i) {\n    float squared_l2_norm = 0;\n    for (int c = 0; c < depth; ++c) {\n      const float val = input_data[c];\n      squared_l2_norm += val * val;\n    }\n    float l2_norm = std::sqrt(squared_l2_norm);\n    l2_norm = std::max(l2_norm, epsilon);\n    for (int c = 0; c < depth; ++c) {\n      *output_data = *input_data / l2_norm;\n      ++output_data;\n      ++input_data;\n    }\n  }\n}\n\ninline void L2Normalization(const tflite::L2NormalizationParams& op_params,\n                            const RuntimeShape& input_shape,\n                            const uint8* input_data,\n                            const RuntimeShape& output_shape,\n                            uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"L2Normalization/8bit\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int32 input_zero_point = op_params.input_zero_point;\n  for (int i = 0; i < outer_size; ++i) {\n    int32 square_l2_norm = 0;\n    for (int c = 0; c < depth; c++) {\n      // Note that input_data advances by depth in the second pass below.\n      int32 diff = input_data[c] - input_zero_point;\n      square_l2_norm += diff * diff;\n    }\n    // TODO(b/29395854): add clamping to TOCO and TF Lite kernel\n    // for all zero tensors in the input_data\n    int32 inv_l2norm_multiplier;\n    int inv_l2norm_shift;\n    GetInvSqrtQuantizedMultiplierExp(square_l2_norm, kReverseShift,\n                                     &inv_l2norm_multiplier, &inv_l2norm_shift);\n\n    for (int c = 0; c < depth; c++) {\n      int32 diff = *input_data - input_zero_point;\n      int32 rescaled_diff = MultiplyByQuantizedMultiplierSmallerThanOneExp(\n          128 * diff, inv_l2norm_multiplier, inv_l2norm_shift);\n      int32 unclamped_output_val = 128 + rescaled_diff;\n      int32 output_val = std::min(255, std::max(0, unclamped_output_val));\n      *output_data = static_cast<uint8>(output_val);\n      ++input_data;\n      ++output_data;\n    }\n  }\n}\n\ninline void AddElementwise(int size, const ArithmeticParams& params,\n                           const float* input1_data, const float* input2_data,\n                           float* output_data) {\n  int i = 0;\n\n#ifdef USE_NEON\n  const auto activation_min = vdupq_n_f32(params.float_activation_min);\n  const auto activation_max = vdupq_n_f32(params.float_activation_max);\n  for (; i <= size - 16; i += 16) {\n    auto a10 = vld1q_f32(input1_data + i);\n    auto a11 = vld1q_f32(input1_data + i + 4);\n    auto a12 = vld1q_f32(input1_data + i + 8);\n    auto a13 = vld1q_f32(input1_data + i + 12);\n    auto a20 = vld1q_f32(input2_data + i);\n    auto a21 = vld1q_f32(input2_data + i + 4);\n    auto a22 = vld1q_f32(input2_data + i + 8);\n    auto a23 = vld1q_f32(input2_data + i + 12);\n    auto x0 = vaddq_f32(a10, a20);\n    auto x1 = vaddq_f32(a11, a21);\n    auto x2 = vaddq_f32(a12, a22);\n    auto x3 = vaddq_f32(a13, a23);\n    x0 = vmaxq_f32(activation_min, x0);\n    x1 = vmaxq_f32(activation_min, x1);\n    x2 = vmaxq_f32(activation_min, x2);\n    x3 = vmaxq_f32(activation_min, x3);\n    x0 = vminq_f32(activation_max, x0);\n    x1 = vminq_f32(activation_max, x1);\n    x2 = vminq_f32(activation_max, x2);\n    x3 = vminq_f32(activation_max, x3);\n    vst1q_f32(output_data + i, x0);\n    vst1q_f32(output_data + i + 4, x1);\n    vst1q_f32(output_data + i + 8, x2);\n    vst1q_f32(output_data + i + 12, x3);\n  }\n  for (; i <= size - 4; i += 4) {\n    auto a1 = vld1q_f32(input1_data + i);\n    auto a2 = vld1q_f32(input2_data + i);\n    auto x = vaddq_f32(a1, a2);\n    x = vmaxq_f32(activation_min, x);\n    x = vminq_f32(activation_max, x);\n    vst1q_f32(output_data + i, x);\n  }\n#endif  // NEON\n\n  for (; i < size; i++) {\n    auto x = input1_data[i] + input2_data[i];\n    output_data[i] = ActivationFunctionWithMinMax(\n        x, params.float_activation_min, params.float_activation_max);\n  }\n}\n\ninline void Add(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const float* input1_data,\n                const RuntimeShape& input2_shape, const float* input2_data,\n                const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Add\");\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  AddElementwise(flat_size, params, input1_data, input2_data, output_data);\n}\n\n// Element-wise add that can often be used for inner loop of broadcast add as\n// well as the non-broadcast add.\ninline void AddElementwise(int size, const ArithmeticParams& params,\n                           const uint8* input1_data, const uint8* input2_data,\n                           uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"AddElementwise/8bit\");\n  int i = 0;\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n#ifdef USE_NEON\n  const uint8x8_t output_activation_min_vector =\n      vdup_n_u8(params.quantized_activation_min);\n  const uint8x8_t output_activation_max_vector =\n      vdup_n_u8(params.quantized_activation_max);\n  for (; i <= size - 8; i += 8) {\n    const uint8x8_t input1_val_original = vld1_u8(input1_data + i);\n    const uint8x8_t input2_val_original = vld1_u8(input2_data + i);\n    const int16x8_t input1_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input1_val_original));\n    const int16x8_t input2_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input2_val_original));\n    const int16x8_t input1_val =\n        vaddq_s16(input1_val_s16, vdupq_n_s16(params.input1_offset));\n    const int16x8_t input2_val =\n        vaddq_s16(input2_val_s16, vdupq_n_s16(params.input2_offset));\n    const int16x4_t input1_val_high = vget_high_s16(input1_val);\n    const int16x4_t input1_val_low = vget_low_s16(input1_val);\n    const int16x4_t input2_val_high = vget_high_s16(input2_val);\n    const int16x4_t input2_val_low = vget_low_s16(input2_val);\n    int32x4_t x11 = vmovl_s16(input1_val_low);\n    int32x4_t x12 = vmovl_s16(input1_val_high);\n    int32x4_t x21 = vmovl_s16(input2_val_low);\n    int32x4_t x22 = vmovl_s16(input2_val_high);\n    const int32x4_t left_shift_dup = vdupq_n_s32(params.left_shift);\n    x11 = vshlq_s32(x11, left_shift_dup);\n    x12 = vshlq_s32(x12, left_shift_dup);\n    x21 = vshlq_s32(x21, left_shift_dup);\n    x22 = vshlq_s32(x22, left_shift_dup);\n    x11 = vqrdmulhq_n_s32(x11, params.input1_multiplier);\n    x12 = vqrdmulhq_n_s32(x12, params.input1_multiplier);\n    x21 = vqrdmulhq_n_s32(x21, params.input2_multiplier);\n    x22 = vqrdmulhq_n_s32(x22, params.input2_multiplier);\n    const int32x4_t input1_shift_dup = vdupq_n_s32(params.input1_shift);\n    const int32x4_t input2_shift_dup = vdupq_n_s32(params.input2_shift);\n    x11 = vshlq_s32(x11, input1_shift_dup);\n    x12 = vshlq_s32(x12, input1_shift_dup);\n    x21 = vshlq_s32(x21, input2_shift_dup);\n    x22 = vshlq_s32(x22, input2_shift_dup);\n    int32x4_t s1 = vaddq_s32(x11, x21);\n    int32x4_t s2 = vaddq_s32(x12, x22);\n    s1 = vqrdmulhq_n_s32(s1, params.output_multiplier);\n    s2 = vqrdmulhq_n_s32(s2, params.output_multiplier);\n    using gemmlowp::RoundingDivideByPOT;\n    s1 = RoundingDivideByPOT(s1, -params.output_shift);\n    s2 = RoundingDivideByPOT(s2, -params.output_shift);\n    const int16x4_t s1_narrowed = vmovn_s32(s1);\n    const int16x4_t s2_narrowed = vmovn_s32(s2);\n    const int16x8_t s = vaddq_s16(vcombine_s16(s1_narrowed, s2_narrowed),\n                                  vdupq_n_s16(params.output_offset));\n    const uint8x8_t clamped =\n        vmax_u8(output_activation_min_vector,\n                vmin_u8(output_activation_max_vector, vqmovun_s16(s)));\n    vst1_u8(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    const int32 input1_val = params.input1_offset + input1_data[i];\n    const int32 input2_val = params.input2_offset + input2_data[i];\n    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);\n    const int32 shifted_input2_val = input2_val * (1 << params.left_shift);\n    const int32 scaled_input1_val =\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            shifted_input1_val, params.input1_multiplier, params.input1_shift);\n    const int32 scaled_input2_val =\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            shifted_input2_val, params.input2_multiplier, params.input2_shift);\n    const int32 raw_sum = scaled_input1_val + scaled_input2_val;\n    const int32 raw_output =\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            raw_sum, params.output_multiplier, params.output_shift) +\n        params.output_offset;\n    const int32 clamped_output =\n        std::min(params.quantized_activation_max,\n                 std::max(params.quantized_activation_min, raw_output));\n    output_data[i] = static_cast<uint8>(clamped_output);\n  }\n}\n\n// Scalar-broadcast add that can be used for inner loop of more general\n// broadcast add, so that, for example, scalar-broadcast with batch will still\n// be fast.\ninline void AddScalarBroadcast(int size, const ArithmeticParams& params,\n                               uint8 input1_data, const uint8* input2_data,\n                               uint8* output_data) {\n  using gemmlowp::RoundingDivideByPOT;\n\n  ruy::profiler::ScopeLabel label(\"AddScalarBroadcast/8bit\");\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n\n  int i = 0;\n\n#ifdef USE_NEON\n  const int32x4_t left_shift_dup = vdupq_n_s32(params.left_shift);\n  const uint8x8_t output_activation_min_vector =\n      vdup_n_u8(params.quantized_activation_min);\n  const uint8x8_t output_activation_max_vector =\n      vdup_n_u8(params.quantized_activation_max);\n\n  // Process broadcast scalar.\n  const uint8x8_t input1_val_original = vdup_n_u8(input1_data);\n  const int16x8_t input1_val_s16 =\n      vreinterpretq_s16_u16(vmovl_u8(input1_val_original));\n  const int16x8_t input1_val =\n      vaddq_s16(input1_val_s16, vdupq_n_s16(params.input1_offset));\n  const int16x4_t input1_val_high = vget_high_s16(input1_val);\n  const int16x4_t input1_val_low = vget_low_s16(input1_val);\n  int32x4_t x11 = vmovl_s16(input1_val_low);\n  int32x4_t x12 = vmovl_s16(input1_val_high);\n  x11 = vshlq_s32(x11, left_shift_dup);\n  x12 = vshlq_s32(x12, left_shift_dup);\n  x11 = vqrdmulhq_n_s32(x11, params.input1_multiplier);\n  x12 = vqrdmulhq_n_s32(x12, params.input1_multiplier);\n  const int32x4_t input1_shift_dup = vdupq_n_s32(params.input1_shift);\n  x11 = vshlq_s32(x11, input1_shift_dup);\n  x12 = vshlq_s32(x12, input1_shift_dup);\n\n  for (; i <= size - 8; i += 8) {\n    const uint8x8_t input2_val_original = vld1_u8(input2_data + i);\n    const int16x8_t input2_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input2_val_original));\n    const int16x8_t input2_val =\n        vaddq_s16(input2_val_s16, vdupq_n_s16(params.input2_offset));\n    const int16x4_t input2_val_high = vget_high_s16(input2_val);\n    const int16x4_t input2_val_low = vget_low_s16(input2_val);\n    int32x4_t x21 = vmovl_s16(input2_val_low);\n    int32x4_t x22 = vmovl_s16(input2_val_high);\n    x21 = vshlq_s32(x21, left_shift_dup);\n    x22 = vshlq_s32(x22, left_shift_dup);\n    x21 = vqrdmulhq_n_s32(x21, params.input2_multiplier);\n    x22 = vqrdmulhq_n_s32(x22, params.input2_multiplier);\n    const int32x4_t input2_shift_dup = vdupq_n_s32(params.input2_shift);\n    x21 = vshlq_s32(x21, input2_shift_dup);\n    x22 = vshlq_s32(x22, input2_shift_dup);\n    int32x4_t s1 = vaddq_s32(x11, x21);\n    int32x4_t s2 = vaddq_s32(x12, x22);\n    s1 = vqrdmulhq_n_s32(s1, params.output_multiplier);\n    s2 = vqrdmulhq_n_s32(s2, params.output_multiplier);\n    s1 = RoundingDivideByPOT(s1, -params.output_shift);\n    s2 = RoundingDivideByPOT(s2, -params.output_shift);\n    const int16x4_t s1_narrowed = vmovn_s32(s1);\n    const int16x4_t s2_narrowed = vmovn_s32(s2);\n    const int16x8_t s = vaddq_s16(vcombine_s16(s1_narrowed, s2_narrowed),\n                                  vdupq_n_s16(params.output_offset));\n    const uint8x8_t clamped =\n        vmax_u8(output_activation_min_vector,\n                vmin_u8(output_activation_max_vector, vqmovun_s16(s)));\n    vst1_u8(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  if (i < size) {\n    // Process broadcast scalar.\n    const int32 input1_val = params.input1_offset + input1_data;\n    const int32 shifted_input1_val = input1_val * (1 << params.left_shift);\n    const int32 scaled_input1_val =\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            shifted_input1_val, params.input1_multiplier, params.input1_shift);\n\n    for (; i < size; ++i) {\n      const int32 input2_val = params.input2_offset + input2_data[i];\n      const int32 shifted_input2_val = input2_val * (1 << params.left_shift);\n      const int32 scaled_input2_val =\n          MultiplyByQuantizedMultiplierSmallerThanOneExp(\n              shifted_input2_val, params.input2_multiplier,\n              params.input2_shift);\n      const int32 raw_sum = scaled_input1_val + scaled_input2_val;\n      const int32 raw_output =\n          MultiplyByQuantizedMultiplierSmallerThanOneExp(\n              raw_sum, params.output_multiplier, params.output_shift) +\n          params.output_offset;\n      const int32 clamped_output =\n          std::min(params.quantized_activation_max,\n                   std::max(params.quantized_activation_min, raw_output));\n      output_data[i] = static_cast<uint8>(clamped_output);\n    }\n  }\n}\n\n// Scalar-broadcast add that can be used for inner loop of more general\n// broadcast add, so that, for example, scalar-broadcast with batch will still\n// be fast.\ninline void AddScalarBroadcast(int size, const ArithmeticParams& params,\n                               float broadcast_value, const float* input2_data,\n                               float* output_data) {\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t output_activation_min_vector =\n      vdupq_n_f32(params.float_activation_min);\n  const float32x4_t output_activation_max_vector =\n      vdupq_n_f32(params.float_activation_max);\n  const float32x4_t broadcast_value_dup = vdupq_n_f32(broadcast_value);\n  for (; i <= size - 4; i += 4) {\n    const float32x4_t input2_val_original = vld1q_f32(input2_data + i);\n\n    const float32x4_t output =\n        vaddq_f32(input2_val_original, broadcast_value_dup);\n\n    const float32x4_t clamped =\n        vmaxq_f32(output_activation_min_vector,\n                  vminq_f32(output_activation_max_vector, output));\n    vst1q_f32(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    auto x = broadcast_value + input2_data[i];\n    output_data[i] = ActivationFunctionWithMinMax(\n        x, params.float_activation_min, params.float_activation_max);\n  }\n}\n\ninline void Add(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const uint8* input1_data,\n                const RuntimeShape& input2_shape, const uint8* input2_data,\n                const RuntimeShape& output_shape, uint8* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  ruy::profiler::ScopeLabel label(\"Add/8bit\");\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n  AddElementwise(flat_size, params, input1_data, input2_data, output_data);\n}\n\ninline void Add(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Add/Int16\");\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n\n  const int input1_shift = params.input1_shift;\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  const int16 output_activation_min = params.quantized_activation_min;\n  const int16 output_activation_max = params.quantized_activation_max;\n\n  TFLITE_DCHECK(input1_shift == 0 || params.input2_shift == 0);\n  TFLITE_DCHECK_LE(input1_shift, 0);\n  TFLITE_DCHECK_LE(params.input2_shift, 0);\n  const int16* not_shift_input = input1_shift == 0 ? input1_data : input2_data;\n  const int16* shift_input = input1_shift == 0 ? input2_data : input1_data;\n  const int input_right_shift =\n      input1_shift == 0 ? -params.input2_shift : -input1_shift;\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 input_ready_scaled = F0::FromRaw(not_shift_input[i]);\n    F0 scaled_input = F0::FromRaw(\n        gemmlowp::RoundingDivideByPOT(shift_input[i], input_right_shift));\n    F0 result = gemmlowp::SaturatingAdd(scaled_input, input_ready_scaled);\n    const int16 raw_output = result.raw();\n    const int16 clamped_output = std::min(\n        output_activation_max, std::max(output_activation_min, raw_output));\n    output_data[i] = clamped_output;\n  }\n}\n\ntemplate <typename T>\ninline typename std::enable_if<is_int32_or_int64<T>::value, void>::type Add(\n    const ArithmeticParams& params, const RuntimeShape& input1_shape,\n    const T* input1_data, const RuntimeShape& input2_shape,\n    const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Add/int32or64\");\n\n  T activation_min, activation_max;\n  GetActivationParams(params, &activation_min, &activation_max);\n\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  if (input1_shape == input2_shape) {\n    output_map.array() = (input1_map.array() + input2_map.array())\n                             .cwiseMax(activation_min)\n                             .cwiseMin(activation_max);\n  } else if (input2_shape.FlatSize() == 1) {\n    auto scalar = input2_data[0];\n    output_map.array() = (input1_map.array() + scalar)\n                             .cwiseMax(activation_min)\n                             .cwiseMin(activation_max);\n  } else if (input1_shape.FlatSize() == 1) {\n    auto scalar = input1_data[0];\n    output_map.array() = (scalar + input2_map.array())\n                             .cwiseMax(activation_min)\n                             .cwiseMin(activation_max);\n  } else {\n    reference_ops::BroadcastAdd4DSlow<T>(params, input1_shape, input1_data,\n                                         input2_shape, input2_data,\n                                         output_shape, output_data);\n  }\n}\n\ntemplate <typename T>\ninline void BroadcastAddDispatch(\n    const ArithmeticParams& params, const RuntimeShape& input1_shape,\n    const T* input1_data, const RuntimeShape& input2_shape,\n    const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return BroadcastAdd4DSlow(params, input1_shape, input1_data, input2_shape,\n                              input2_data, output_shape, output_data);\n  }\n\n  BinaryBroadcastFiveFold(\n      params, input1_shape, input1_data, input2_shape, input2_data,\n      output_shape, output_data,\n      static_cast<void (*)(int, const ArithmeticParams&, const T*, const T*,\n                           T*)>(AddElementwise),\n      static_cast<void (*)(int, const ArithmeticParams&, T, const T*, T*)>(\n          AddScalarBroadcast));\n}\n\ninline void BroadcastAddFivefold(const ArithmeticParams& unswitched_params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const uint8* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const uint8* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 uint8* output_data) {\n  BroadcastAddDispatch(unswitched_params, unswitched_input1_shape,\n                       unswitched_input1_data, unswitched_input2_shape,\n                       unswitched_input2_data, output_shape, output_data);\n}\n\ninline void BroadcastAddFivefold(const ArithmeticParams& params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const float* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const float* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 float* output_data) {\n  BroadcastAddDispatch(params, unswitched_input1_shape, unswitched_input1_data,\n                       unswitched_input2_shape, unswitched_input2_data,\n                       output_shape, output_data);\n}\n\ninline void MulElementwise(int size, const ArithmeticParams& params,\n                           const float* input1_data, const float* input2_data,\n                           float* output_data) {\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n\n  int i = 0;\n#ifdef USE_NEON\n  const auto activation_min = vdupq_n_f32(output_activation_min);\n  const auto activation_max = vdupq_n_f32(output_activation_max);\n  for (; i <= size - 16; i += 16) {\n    auto a10 = vld1q_f32(input1_data + i);\n    auto a11 = vld1q_f32(input1_data + i + 4);\n    auto a12 = vld1q_f32(input1_data + i + 8);\n    auto a13 = vld1q_f32(input1_data + i + 12);\n    auto a20 = vld1q_f32(input2_data + i);\n    auto a21 = vld1q_f32(input2_data + i + 4);\n    auto a22 = vld1q_f32(input2_data + i + 8);\n    auto a23 = vld1q_f32(input2_data + i + 12);\n    auto x0 = vmulq_f32(a10, a20);\n    auto x1 = vmulq_f32(a11, a21);\n    auto x2 = vmulq_f32(a12, a22);\n    auto x3 = vmulq_f32(a13, a23);\n\n    x0 = vmaxq_f32(activation_min, x0);\n    x1 = vmaxq_f32(activation_min, x1);\n    x2 = vmaxq_f32(activation_min, x2);\n    x3 = vmaxq_f32(activation_min, x3);\n    x0 = vminq_f32(activation_max, x0);\n    x1 = vminq_f32(activation_max, x1);\n    x2 = vminq_f32(activation_max, x2);\n    x3 = vminq_f32(activation_max, x3);\n\n    vst1q_f32(output_data + i, x0);\n    vst1q_f32(output_data + i + 4, x1);\n    vst1q_f32(output_data + i + 8, x2);\n    vst1q_f32(output_data + i + 12, x3);\n  }\n  for (; i <= size - 4; i += 4) {\n    auto a1 = vld1q_f32(input1_data + i);\n    auto a2 = vld1q_f32(input2_data + i);\n    auto x = vmulq_f32(a1, a2);\n\n    x = vmaxq_f32(activation_min, x);\n    x = vminq_f32(activation_max, x);\n\n    vst1q_f32(output_data + i, x);\n  }\n#endif  // NEON\n\n  for (; i < size; i++) {\n    auto x = input1_data[i] * input2_data[i];\n    output_data[i] = ActivationFunctionWithMinMax(x, output_activation_min,\n                                                  output_activation_max);\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const float* input1_data,\n                const RuntimeShape& input2_shape, const float* input2_data,\n                const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul\");\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  MulElementwise(flat_size, params, input1_data, input2_data, output_data);\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int32* input1_data,\n                const RuntimeShape& input2_shape, const int32* input2_data,\n                const RuntimeShape& output_shape, int32* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/int32/activation\");\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = ActivationFunctionWithMinMax(\n        input1_data[i] * input2_data[i], output_activation_min,\n        output_activation_max);\n  }\n}\n\ninline void MulNoActivation(const ArithmeticParams& params,\n                            const RuntimeShape& input1_shape,\n                            const int32* input1_data,\n                            const RuntimeShape& input2_shape,\n                            const int32* input2_data,\n                            const RuntimeShape& output_shape,\n                            int32* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/int32\");\n\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  if (input1_shape == input2_shape) {\n    output_map.array() = input1_map.array() * input2_map.array();\n  } else if (input2_shape.FlatSize() == 1) {\n    auto scalar = input2_data[0];\n    output_map.array() = input1_map.array() * scalar;\n  } else if (input1_shape.FlatSize() == 1) {\n    auto scalar = input1_data[0];\n    output_map.array() = scalar * input2_map.array();\n  } else {\n    reference_ops::BroadcastMul4DSlow(params, input1_shape, input1_data,\n                                      input2_shape, input2_data, output_shape,\n                                      output_data);\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/Int16/NoActivation\");\n  // This is a copy of the reference implementation. We do not currently have a\n  // properly optimized version.\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 unclamped_result =\n        F0::FromRaw(input1_data[i]) * F0::FromRaw(input2_data[i]);\n    output_data[i] = unclamped_result.raw();\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/Int16Uint8\");\n  // This is a copy of the reference implementation. We do not currently have a\n  // properly optimized version.\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  const int32 output_offset = params.output_offset;\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 unclamped_result =\n        F0::FromRaw(input1_data[i]) * F0::FromRaw(input2_data[i]);\n    int16 rescaled_result =\n        gemmlowp::RoundingDivideByPOT(unclamped_result.raw(), 8);\n    int16 clamped_result =\n        std::min<int16>(output_activation_max - output_offset, rescaled_result);\n    clamped_result =\n        std::max<int16>(output_activation_min - output_offset, clamped_result);\n    output_data[i] = output_offset + clamped_result;\n  }\n}\n\n// Element-wise mul that can often be used for inner loop of broadcast Mul as\n// well as the non-broadcast Mul.\ninline void MulElementwise(int size, const ArithmeticParams& params,\n                           const uint8* input1_data, const uint8* input2_data,\n                           uint8* output_data) {\n  int i = 0;\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n  TFLITE_DCHECK_GT(params.output_offset, -256);\n  TFLITE_DCHECK_LT(params.output_offset, 256);\n#ifdef USE_NEON\n  const auto input1_offset_vector = vdupq_n_s16(params.input1_offset);\n  const auto input2_offset_vector = vdupq_n_s16(params.input2_offset);\n  const auto output_offset_vector = vdupq_n_s16(params.output_offset);\n  const auto output_activation_min_vector =\n      vdup_n_u8(params.quantized_activation_min);\n  const auto output_activation_max_vector =\n      vdup_n_u8(params.quantized_activation_max);\n  const int left_shift = std::max(0, params.output_shift);\n  const int right_shift = std::max(0, -params.output_shift);\n  const int32x4_t left_shift_vec = vdupq_n_s32(left_shift);\n  for (; i <= size - 8; i += 8) {\n    // We load / store 8 at a time, multiplying as two sets of 4 int32s.\n    const auto input1_val_original = vld1_u8(input1_data + i);\n    const auto input2_val_original = vld1_u8(input2_data + i);\n    const auto input1_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input1_val_original));\n    const auto input2_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input2_val_original));\n    const auto input1_val = vaddq_s16(input1_val_s16, input1_offset_vector);\n    const auto input2_val = vaddq_s16(input2_val_s16, input2_offset_vector);\n\n    const auto input1_val_low = vget_low_s16(input1_val);\n    const auto input1_val_high = vget_high_s16(input1_val);\n    const auto input2_val_low = vget_low_s16(input2_val);\n    const auto input2_val_high = vget_high_s16(input2_val);\n\n    auto p1 = vmull_s16(input2_val_low, input1_val_low);\n    auto p2 = vmull_s16(input2_val_high, input1_val_high);\n\n    p1 = vshlq_s32(p1, left_shift_vec);\n    p2 = vshlq_s32(p2, left_shift_vec);\n    p1 = vqrdmulhq_n_s32(p1, params.output_multiplier);\n    p2 = vqrdmulhq_n_s32(p2, params.output_multiplier);\n    using gemmlowp::RoundingDivideByPOT;\n    p1 = RoundingDivideByPOT(p1, right_shift);\n    p2 = RoundingDivideByPOT(p2, right_shift);\n\n    const auto p1_narrowed = vqmovn_s32(p1);\n    const auto p2_narrowed = vqmovn_s32(p2);\n    const auto p =\n        vaddq_s16(vcombine_s16(p1_narrowed, p2_narrowed), output_offset_vector);\n    const auto clamped =\n        vmax_u8(output_activation_min_vector,\n                vmin_u8(output_activation_max_vector, vqmovun_s16(p)));\n    vst1_u8(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    const int32 input1_val = params.input1_offset + input1_data[i];\n    const int32 input2_val = params.input2_offset + input2_data[i];\n    const int32 unclamped_result =\n        params.output_offset +\n        MultiplyByQuantizedMultiplier(input1_val * input2_val,\n                                      params.output_multiplier,\n                                      params.output_shift);\n    const int32 clamped_output =\n        std::min(params.quantized_activation_max,\n                 std::max(params.quantized_activation_min, unclamped_result));\n    output_data[i] = static_cast<uint8>(clamped_output);\n  }\n}\n\n// Broadcast mul that can often be used for inner loop of broadcast Mul.\ninline void MulSimpleBroadcast(int size, const ArithmeticParams& params,\n                               const uint8 broadcast_value,\n                               const uint8* input2_data, uint8* output_data) {\n  const int16 input1_val = params.input1_offset + broadcast_value;\n\n  int i = 0;\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n  TFLITE_DCHECK_GT(params.output_offset, -256);\n  TFLITE_DCHECK_LT(params.output_offset, 256);\n#ifdef USE_NEON\n  const auto input2_offset_vector = vdupq_n_s16(params.input2_offset);\n  const auto output_offset_vector = vdupq_n_s16(params.output_offset);\n  const auto output_activation_min_vector =\n      vdup_n_u8(params.quantized_activation_min);\n  const auto output_activation_max_vector =\n      vdup_n_u8(params.quantized_activation_max);\n  const int left_shift = std::max(0, params.output_shift);\n  const int right_shift = std::max(0, -params.output_shift);\n  const int32x4_t left_shift_vec = vdupq_n_s32(left_shift);\n  for (; i <= size - 8; i += 8) {\n    // We load / store 8 at a time, multiplying as two sets of 4 int32s.\n    const auto input2_val_original = vld1_u8(input2_data + i);\n    const auto input2_val_s16 =\n        vreinterpretq_s16_u16(vmovl_u8(input2_val_original));\n    const auto input2_val = vaddq_s16(input2_val_s16, input2_offset_vector);\n\n    const auto input2_val_low = vget_low_s16(input2_val);\n    const auto input2_val_high = vget_high_s16(input2_val);\n\n    auto p1 = vmull_n_s16(input2_val_low, input1_val);\n    auto p2 = vmull_n_s16(input2_val_high, input1_val);\n\n    p1 = vshlq_s32(p1, left_shift_vec);\n    p2 = vshlq_s32(p2, left_shift_vec);\n    p1 = vqrdmulhq_n_s32(p1, params.output_multiplier);\n    p2 = vqrdmulhq_n_s32(p2, params.output_multiplier);\n    using gemmlowp::RoundingDivideByPOT;\n    p1 = RoundingDivideByPOT(p1, right_shift);\n    p2 = RoundingDivideByPOT(p2, right_shift);\n\n    const auto p1_narrowed = vmovn_s32(p1);\n    const auto p2_narrowed = vmovn_s32(p2);\n    const auto p =\n        vaddq_s16(vcombine_s16(p1_narrowed, p2_narrowed), output_offset_vector);\n    const auto clamped =\n        vmax_u8(output_activation_min_vector,\n                vmin_u8(output_activation_max_vector, vqmovun_s16(p)));\n    vst1_u8(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    const int32 input2_val = params.input2_offset + input2_data[i];\n    const int32 unclamped_result =\n        params.output_offset +\n        MultiplyByQuantizedMultiplier(input1_val * input2_val,\n                                      params.output_multiplier,\n                                      params.output_shift);\n    const int32 clamped_output =\n        std::min(params.quantized_activation_max,\n                 std::max(params.quantized_activation_min, unclamped_result));\n    output_data[i] = static_cast<uint8>(clamped_output);\n  }\n}\n\n// Broadcast mul that can often be used for inner loop of broadcast Mul.\n// This function will handle scalar_value (LHS) * vector_values (RHS).\n// Since it's a float function, input params does not matter here.\ninline void MulSimpleBroadcast(int size, const ArithmeticParams& params,\n                               const float broadcast_value,\n                               const float* input2_data, float* output_data) {\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t output_activation_min_vector =\n      vdupq_n_f32(params.float_activation_min);\n  const float32x4_t output_activation_max_vector =\n      vdupq_n_f32(params.float_activation_max);\n  const float32x4_t broadcast_value_dup = vdupq_n_f32(broadcast_value);\n  for (; i <= size - 4; i += 4) {\n    const float32x4_t input2_val_original = vld1q_f32(input2_data + i);\n\n    const float32x4_t output =\n        vmulq_f32(input2_val_original, broadcast_value_dup);\n\n    const float32x4_t clamped =\n        vmaxq_f32(output_activation_min_vector,\n                  vminq_f32(output_activation_max_vector, output));\n    vst1q_f32(output_data + i, clamped);\n  }\n#endif  // NEON\n\n  for (; i < size; ++i) {\n    float x = broadcast_value * input2_data[i];\n    output_data[i] = ActivationFunctionWithMinMax(\n        x, params.float_activation_min, params.float_activation_max);\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const uint8* input1_data,\n                const RuntimeShape& input2_shape, const uint8* input2_data,\n                const RuntimeShape& output_shape, uint8* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  ruy::profiler::ScopeLabel label(\"Mul/8bit\");\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  MulElementwise(flat_size, params, input1_data, input2_data, output_data);\n}\n\ntemplate <typename T>\ninline void BroadcastMulDispatch(\n    const ArithmeticParams& params, const RuntimeShape& input1_shape,\n    const T* input1_data, const RuntimeShape& input2_shape,\n    const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return BroadcastMul4DSlow(params, input1_shape, input1_data, input2_shape,\n                              input2_data, output_shape, output_data);\n  }\n\n  BinaryBroadcastFiveFold(\n      params, input1_shape, input1_data, input2_shape, input2_data,\n      output_shape, output_data,\n      static_cast<void (*)(int, const ArithmeticParams&, const T*, const T*,\n                           T*)>(MulElementwise),\n      static_cast<void (*)(int, const ArithmeticParams&, T, const T*, T*)>(\n          MulSimpleBroadcast));\n}\n\ninline void BroadcastMulFivefold(const ArithmeticParams& unswitched_params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const uint8* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const uint8* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 uint8* output_data) {\n  BroadcastMulDispatch(unswitched_params, unswitched_input1_shape,\n                       unswitched_input1_data, unswitched_input2_shape,\n                       unswitched_input2_data, output_shape, output_data);\n}\n\ninline void BroadcastMulFivefold(const ArithmeticParams& params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const float* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const float* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 float* output_data) {\n  BroadcastMulDispatch(params, unswitched_input1_shape, unswitched_input1_data,\n                       unswitched_input2_shape, unswitched_input2_data,\n                       output_shape, output_data);\n}\n\n// TODO(jiawen): We can implement BroadcastDiv on buffers of arbitrary\n// dimensionality if the runtime code does a single loop over one dimension\n// that handles broadcasting as the base case. The code generator would then\n// generate max(D1, D2) nested for loops.\n// TODO(benoitjacob): BroadcastDiv is intentionally duplicated from\n// reference_ops.h. Once an optimized version is implemented and NdArrayDesc<T>\n// is no longer referenced in this file, move NdArrayDesc<T> from types.h to\n// reference_ops.h.\ntemplate <typename T, int N = 5>\nvoid BroadcastDivSlow(const ArithmeticParams& params,\n                      const RuntimeShape& unextended_input1_shape,\n                      const T* input1_data,\n                      const RuntimeShape& unextended_input2_shape,\n                      const T* input2_data,\n                      const RuntimeShape& unextended_output_shape,\n                      T* output_data) {\n  ruy::profiler::ScopeLabel label(\"BroadcastDivSlow\");\n  T output_activation_min;\n  T output_activation_max;\n  GetActivationParams(params, &output_activation_min, &output_activation_max);\n\n  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), N);\n\n  NdArrayDesc<N> desc1;\n  NdArrayDesc<N> desc2;\n  NdArrayDesc<N> output_desc;\n  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,\n                                      unextended_input2_shape, &desc1, &desc2);\n  CopyDimsToDesc(RuntimeShape::ExtendedShape(N, unextended_output_shape),\n                 &output_desc);\n\n  // In Tensorflow, the dimensions are canonically named (batch_number, row,\n  // col, channel), with extents (batches, height, width, depth), with the\n  // trailing dimension changing most rapidly (channels has the smallest stride,\n  // typically 1 element).\n  //\n  // In generated C code, we store arrays with the dimensions reversed. The\n  // first dimension has smallest stride.\n  //\n  // We name our variables by their Tensorflow convention, but generate C code\n  // nesting loops such that the innermost loop has the smallest stride for the\n  // best cache behavior.\n  auto div_func = [&](int indexes[N]) {\n    output_data[SubscriptToIndex(output_desc, indexes)] =\n        ActivationFunctionWithMinMax(\n            input1_data[SubscriptToIndex(desc1, indexes)] /\n                input2_data[SubscriptToIndex(desc2, indexes)],\n            output_activation_min, output_activation_max);\n  };\n  NDOpsHelper<N>(output_desc, div_func);\n}\n\n// BroadcastDiv is intentionally duplicated from reference_ops.h.\n// For more details see the comment above the generic version of\n// BroadcastDivSlow.\ntemplate <int N = 5>\ninline void BroadcastDivSlow(const ArithmeticParams& params,\n                             const RuntimeShape& unextended_input1_shape,\n                             const uint8* input1_data,\n                             const RuntimeShape& unextended_input2_shape,\n                             const uint8* input2_data,\n                             const RuntimeShape& unextended_output_shape,\n                             uint8* output_data) {\n  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), N);\n\n  NdArrayDesc<N> desc1;\n  NdArrayDesc<N> desc2;\n  NdArrayDesc<N> output_desc;\n  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,\n                                      unextended_input2_shape, &desc1, &desc2);\n  CopyDimsToDesc(RuntimeShape::ExtendedShape(N, unextended_output_shape),\n                 &output_desc);\n\n  TFLITE_DCHECK_GT(params.input1_offset, -256);\n  TFLITE_DCHECK_LT(params.input1_offset, 256);\n  TFLITE_DCHECK_GT(params.input2_offset, -256);\n  TFLITE_DCHECK_LT(params.input2_offset, 256);\n  TFLITE_DCHECK_GT(params.output_offset, -256);\n  TFLITE_DCHECK_LT(params.output_offset, 256);\n\n  auto div_func = [&](int indexes[N]) {\n    const int32 input1_val =\n        params.input1_offset + input1_data[SubscriptToIndex(desc1, indexes)];\n    const int32 input2_val =\n        params.input2_offset + input2_data[SubscriptToIndex(desc2, indexes)];\n    TFLITE_DCHECK_NE(input2_val, 0);\n    int recip_shift;\n    const int32 input2_inv =\n        (input2_val > 0) ? GetReciprocal(input2_val, 31, &recip_shift)\n                         : -GetReciprocal(-input2_val, 31, &recip_shift);\n    const int headroom = CountLeadingSignBits(input1_val);\n    const int32 unscaled_quotient = MultiplyByQuantizedMultiplierGreaterThanOne(\n        input1_val, input2_inv, headroom);\n    const int total_shift = params.output_shift - recip_shift - headroom;\n    const int32 unclamped_result =\n        params.output_offset +\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            unscaled_quotient, params.output_multiplier, total_shift);\n    const int32 clamped_output =\n        std::min(params.quantized_activation_max,\n                 std::max(params.quantized_activation_min, unclamped_result));\n    output_data[SubscriptToIndex(output_desc, indexes)] =\n        static_cast<uint8>(clamped_output);\n  };\n  NDOpsHelper<N>(output_desc, div_func);\n}\n\ntemplate <typename T>\ninline void SubWithActivation(\n    const ArithmeticParams& params, const RuntimeShape& input1_shape,\n    const T* input1_data, const RuntimeShape& input2_shape,\n    const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"SubWithActivation_optimized\");\n  TFLITE_DCHECK_EQ(input1_shape.FlatSize(), input2_shape.FlatSize());\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  T activation_min, activation_max;\n  GetActivationParams(params, &activation_min, &activation_max);\n  output_map.array() = (input1_map.array() - input2_map.array())\n                           .cwiseMin(activation_max)\n                           .cwiseMax(activation_min);\n}\n\ninline void SubNonBroadcast(const ArithmeticParams& params,\n                            const RuntimeShape& input1_shape,\n                            const float* input1_data,\n                            const RuntimeShape& input2_shape,\n                            const float* input2_data,\n                            const RuntimeShape& output_shape,\n                            float* output_data) {\n  ruy::profiler::ScopeLabel label(\"SubNonBroadcast\");\n  SubWithActivation<float>(params, input1_shape, input1_data, input2_shape,\n                           input2_data, output_shape, output_data);\n}\n\ntemplate <typename T>\nvoid Sub(const ArithmeticParams& params, const RuntimeShape& input1_shape,\n         const T* input1_data, const RuntimeShape& input2_shape,\n         const T* input2_data, const RuntimeShape& output_shape,\n         T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Sub\");\n\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  if (input1_shape == input2_shape) {\n    output_map.array() = input1_map.array() - input2_map.array();\n  } else if (input1_shape.FlatSize() == 1) {\n    auto scalar = input1_data[0];\n    output_map.array() = scalar - input2_map.array();\n  } else if (input2_shape.FlatSize() == 1) {\n    auto scalar = input2_data[0];\n    output_map.array() = input1_map.array() - scalar;\n  } else {\n    BroadcastSubSlow(params, input1_shape, input1_data, input2_shape,\n                     input2_data, output_shape, output_data);\n  }\n}\n\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const float* input_data, const RuntimeShape& unextended_prev_activ_shape,\n    const float* prev_activ_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& unextended_bias_shape,\n    const float* bias_data, const RuntimeShape& unextended_prev_state_shape,\n    const float* prev_state_data,\n    const RuntimeShape& unextended_output_state_shape, float* output_state_data,\n    const RuntimeShape& unextended_output_activ_shape, float* output_activ_data,\n    const RuntimeShape& unextended_concat_temp_shape, float* concat_temp_data,\n    const RuntimeShape& unextended_activ_temp_shape, float* activ_temp_data,\n    CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"LstmCell\");\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  MatchingDim(  // batches\n      input_shape, 0, prev_activ_shape, 0, prev_state_shape, 0,\n      output_state_shape, 0, output_activ_shape, 0);\n  MatchingDim(  // height\n      input_shape, 1, prev_activ_shape, 1, prev_state_shape, 1,\n      output_state_shape, 1, output_activ_shape, 1);\n  MatchingDim(  // width\n      input_shape, 2, prev_activ_shape, 2, prev_state_shape, 2,\n      output_state_shape, 2, output_activ_shape, 2);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n\n  // Concatenate prev_activ and input data together\n  std::vector<float const*> concat_input_arrays_data;\n  std::vector<RuntimeShape const*> concat_input_arrays_shapes;\n  concat_input_arrays_data.push_back(input_data);\n  concat_input_arrays_data.push_back(prev_activ_data);\n  concat_input_arrays_shapes.push_back(&input_shape);\n  concat_input_arrays_shapes.push_back(&prev_activ_shape);\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = concat_input_arrays_data.size();\n  Concatenation(concat_params, &(concat_input_arrays_shapes[0]),\n                &(concat_input_arrays_data[0]), concat_temp_shape,\n                concat_temp_data);\n\n  // Fully connected\n  tflite::FullyConnectedParams fc_params;\n  fc_params.float_activation_min = std::numeric_limits<float>::lowest();\n  fc_params.float_activation_max = std::numeric_limits<float>::max();\n  fc_params.lhs_cacheable = false;\n  fc_params.rhs_cacheable = false;\n  FullyConnected(fc_params, concat_temp_shape, concat_temp_data, weights_shape,\n                 weights_data, bias_shape, bias_data, activ_temp_shape,\n                 activ_temp_data, cpu_backend_context);\n\n  // Map raw arrays to Eigen arrays so we can use Eigen's optimized array\n  // operations.\n  ArrayMap<float> activ_temp_map =\n      MapAsArrayWithLastDimAsRows(activ_temp_data, activ_temp_shape);\n  auto input_gate_sm = activ_temp_map.block(0 * output_depth, 0, output_depth,\n                                            activ_temp_map.cols());\n  auto new_input_sm = activ_temp_map.block(1 * output_depth, 0, output_depth,\n                                           activ_temp_map.cols());\n  auto forget_gate_sm = activ_temp_map.block(2 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  auto output_gate_sm = activ_temp_map.block(3 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  ArrayMap<const float> prev_state_map =\n      MapAsArrayWithLastDimAsRows(prev_state_data, prev_state_shape);\n  ArrayMap<float> output_state_map =\n      MapAsArrayWithLastDimAsRows(output_state_data, output_state_shape);\n  ArrayMap<float> output_activ_map =\n      MapAsArrayWithLastDimAsRows(output_activ_data, output_activ_shape);\n\n  // Combined memory state and final output calculation\n  ruy::profiler::ScopeLabel label2(\"MemoryStateAndFinalOutput\");\n  output_state_map =\n      input_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          new_input_sm.tanh() +\n      forget_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          prev_state_map;\n  output_activ_map =\n      output_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n      output_state_map.tanh();\n}\n\ntemplate <int StateIntegerBits>\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const uint8* input_data_uint8,\n    const RuntimeShape& unextended_prev_activ_shape,\n    const uint8* prev_activ_data_uint8, const RuntimeShape& weights_shape,\n    const uint8* weights_data_uint8, const RuntimeShape& unextended_bias_shape,\n    const int32* bias_data_int32,\n    const RuntimeShape& unextended_prev_state_shape,\n    const int16* prev_state_data_int16,\n    const RuntimeShape& unextended_output_state_shape,\n    int16* output_state_data_int16,\n    const RuntimeShape& unextended_output_activ_shape,\n    uint8* output_activ_data_uint8,\n    const RuntimeShape& unextended_concat_temp_shape,\n    uint8* concat_temp_data_uint8,\n    const RuntimeShape& unextended_activ_temp_shape,\n    int16* activ_temp_data_int16, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\n      \"LstmCell/quantized (8bit external, 16bit internal)\");\n  int32 weights_zero_point = params.weights_zero_point;\n  int32 accum_multiplier = params.accum_multiplier;\n  int accum_shift = params.accum_shift;\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  // Gather dimensions information, and perform consistency checks.\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int outer_size = MatchingFlatSizeSkipDim(\n      input_shape, 3, prev_activ_shape, prev_state_shape, output_state_shape,\n      output_activ_shape);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n  const int fc_batches = FlatSizeSkipDim(activ_temp_shape, 3);\n  const int fc_output_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, activ_temp_shape, 3);\n  const int fc_accum_depth = total_input_depth;\n  TFLITE_DCHECK_EQ(fc_output_depth, 4 * output_depth);\n\n  // Depth-concatenate prev_activ and input data together.\n  uint8 const* concat_input_arrays_data[2] = {input_data_uint8,\n                                              prev_activ_data_uint8};\n  const RuntimeShape* concat_input_arrays_shapes[2] = {&input_shape,\n                                                       &prev_activ_shape};\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = 2;\n  Concatenation(concat_params, concat_input_arrays_shapes,\n                concat_input_arrays_data, concat_temp_shape,\n                concat_temp_data_uint8);\n\n  // Implementation of the fully connected node inside the LSTM cell.\n  // The operands are 8-bit integers, the accumulators are internally 32bit\n  // integers, and the output is 16-bit fixed-point with 3 integer bits so\n  // the output range is [-2^3, 2^3] == [-8, 8]. The rationale for that\n  // is explained in the function comment above.\n  cpu_backend_gemm::MatrixParams<uint8> lhs_params;\n  lhs_params.rows = fc_output_depth;\n  lhs_params.cols = fc_accum_depth;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.zero_point = weights_zero_point;\n  cpu_backend_gemm::MatrixParams<uint8> rhs_params;\n  rhs_params.rows = fc_accum_depth;\n  rhs_params.cols = fc_batches;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.zero_point = 128;\n  cpu_backend_gemm::MatrixParams<int16> dst_params;\n  dst_params.rows = fc_output_depth;\n  dst_params.cols = fc_batches;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.zero_point = 0;\n  cpu_backend_gemm::GemmParams<int32, int16> gemm_params;\n  gemm_params.bias = bias_data_int32;\n  gemm_params.multiplier_fixedpoint = accum_multiplier;\n  gemm_params.multiplier_exponent = accum_shift;\n  cpu_backend_gemm::Gemm(\n      lhs_params, weights_data_uint8, rhs_params, concat_temp_data_uint8,\n      dst_params, activ_temp_data_int16, gemm_params, cpu_backend_context);\n\n  // Rest of the LSTM cell: tanh and logistic math functions, and some adds\n  // and muls, all done in 16-bit fixed-point.\n  const int16* input_gate_input_ptr = activ_temp_data_int16;\n  const int16* input_modulation_gate_input_ptr =\n      activ_temp_data_int16 + output_depth;\n  const int16* forget_gate_input_ptr = activ_temp_data_int16 + 2 * output_depth;\n  const int16* output_gate_input_ptr = activ_temp_data_int16 + 3 * output_depth;\n  const int16* prev_state_ptr = prev_state_data_int16;\n  int16* output_state_data_ptr = output_state_data_int16;\n  uint8* output_activ_data_ptr = output_activ_data_uint8;\n\n  for (int b = 0; b < outer_size; ++b) {\n    int c = 0;\n#ifdef GEMMLOWP_NEON\n    for (; c <= output_depth - 8; c += 8) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<int16x8_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<int16x8_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(vld1q_s16(input_gate_input_ptr));\n      input_gate_input_ptr += 8;\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(vld1q_s16(input_modulation_gate_input_ptr));\n      input_modulation_gate_input_ptr += 8;\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(vld1q_s16(forget_gate_input_ptr));\n      forget_gate_input_ptr += 8;\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(vld1q_s16(output_gate_input_ptr));\n      output_gate_input_ptr += 8;\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(vld1q_s16(prev_state_ptr));\n      prev_state_ptr += 8;\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      vst1q_s16(output_state_data_ptr, new_state.raw());\n      output_state_data_ptr += 8;\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16x8_t rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int8x8_t int8_output_activ = vqmovn_s16(rescaled_output_activ);\n      uint8x8_t uint8_output_activ =\n          vadd_u8(vdup_n_u8(128), vreinterpret_u8_s8(int8_output_activ));\n      vst1_u8(output_activ_data_ptr, uint8_output_activ);\n      output_activ_data_ptr += 8;\n    }\n#endif\n    for (; c < output_depth; ++c) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<std::int16_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<std::int16_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(*input_gate_input_ptr++);\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(*input_modulation_gate_input_ptr++);\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(*forget_gate_input_ptr++);\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(*output_gate_input_ptr++);\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(*prev_state_ptr++);\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      *output_state_data_ptr++ = new_state.raw();\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16 rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int16 clamped_output_activ =\n          std::max<int16>(-128, std::min<int16>(127, rescaled_output_activ));\n      *output_activ_data_ptr++ = 128 + clamped_output_activ;\n    }\n    input_gate_input_ptr += 3 * output_depth;\n    input_modulation_gate_input_ptr += 3 * output_depth;\n    forget_gate_input_ptr += 3 * output_depth;\n    output_gate_input_ptr += 3 * output_depth;\n  }\n}\n\ninline int NodeOffset(int b, int h, int w, int height, int width) {\n  return (b * height + h) * width + w;\n}\n\ninline bool AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const float* input_data,\n                        const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"AveragePool\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  if (stride_height == 0) return false;\n  if (stride_width == 0) return false;\n\n  // TODO(benoitjacob) make this a proper reference impl without Eigen!\n  const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n  auto out_mat = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n  // TODO(benoitjacob) get rid of the dynamic memory allocation here!\n  Eigen::VectorXf out_count(out_mat.cols());\n  out_count.setZero();\n  // Prefill the output to 0.\n  out_mat.setZero();\n  for (int b = 0; b < batches; ++b) {\n    for (int h = 0; h < input_height; ++h) {\n      for (int w = 0; w < input_width; ++w) {\n        // (h_start, h_end) * (w_start, w_end) is the range that the input\n        // vector projects to.\n        int hpad = h + params.padding_values.height;\n        int wpad = w + params.padding_values.width;\n        int h_start = (hpad < params.filter_height)\n                          ? 0\n                          : (hpad - params.filter_height) / stride_height + 1;\n        int h_end = std::min(hpad / stride_height + 1, output_height);\n        int w_start = (wpad < params.filter_width)\n                          ? 0\n                          : (wpad - params.filter_width) / stride_width + 1;\n        int w_end = std::min(wpad / stride_width + 1, output_width);\n        // compute elementwise sum\n        for (int ph = h_start; ph < h_end; ++ph) {\n          for (int pw = w_start; pw < w_end; ++pw) {\n            int out_offset = NodeOffset(b, ph, pw, output_height, output_width);\n            out_mat.col(out_offset) +=\n                in_mat.col(NodeOffset(b, h, w, input_height, input_width));\n            out_count(out_offset)++;\n          }\n        }\n      }\n    }\n  }\n  // Divide the output by the actual number of elements being averaged over\n  TFLITE_DCHECK_GT(out_count.minCoeff(), 0);\n  out_mat.array().rowwise() /= out_count.transpose().array();\n\n  const int flat_size = output_shape.FlatSize();\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = ActivationFunctionWithMinMax(output_data[i],\n                                                  params.float_activation_min,\n                                                  params.float_activation_max);\n  }\n\n  return true;\n}\n\ninline bool AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const uint8* input_data,\n                        const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"AveragePool/8bit\");\n\n  // Here, and in other pooling ops, in order to maintain locality of reference,\n  // to minimize some recalculations, and to load into NEON vector registers, we\n  // use an inner loop down the depth. Since depths can be large and hence we\n  // would need arbitrarily large temporary storage, we divide the work up into\n  // depth tranches just within the batch loop.\n  static constexpr int kPoolingAccTrancheSize = 256;\n\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  uint32 acc[kPoolingAccTrancheSize];\n  for (int batch = 0; batch < batches; ++batch) {\n    // We proceed through the depth in tranches (see comment above). The\n    // depth_base is the depth at the beginning of the tranche. The\n    // tranche_depth is the depth dimension of the tranche.\n    for (int depth_base = 0; depth_base < depth;\n         depth_base += kPoolingAccTrancheSize) {\n      const int tranche_depth =\n          std::min(depth - depth_base, kPoolingAccTrancheSize);\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          const int filter_count =\n              (filter_x_end - filter_x_start) * (filter_y_end - filter_y_start);\n          if (filter_count == 0) return false;\n          memset(acc, 0, tranche_depth * sizeof(acc[0]));\n          const uint8* input_ptr =\n              input_data + depth_base +\n              depth * (in_x_origin +\n                       input_width * (in_y_origin + input_height * batch));\n          for (int fy = filter_y_start; fy < filter_y_end; fy++) {\n            const uint8* input_row_ptr =\n                input_ptr + depth * (fy * input_width + filter_x_start);\n            for (int fx = filter_x_start; fx < filter_x_end; fx++) {\n              const uint8* input_channel_ptr = input_row_ptr;\n              int channel = 0;\n#ifdef USE_NEON\n              for (; channel <= tranche_depth - 16; channel += 16) {\n                uint16x4_t acc_reg[4];\n                uint8x16_t input_reg = vld1q_u8(input_channel_ptr);\n                input_channel_ptr += 16;\n                acc_reg[0] = vget_low_u16(vmovl_u8(vget_low_u8(input_reg)));\n                acc_reg[1] = vget_high_u16(vmovl_u8(vget_low_u8(input_reg)));\n                acc_reg[2] = vget_low_u16(vmovl_u8(vget_high_u8(input_reg)));\n                acc_reg[3] = vget_high_u16(vmovl_u8(vget_high_u8(input_reg)));\n                for (int i = 0; i < 4; i++) {\n                  vst1q_u32(\n                      acc + channel + 4 * i,\n                      vaddw_u16(vld1q_u32(acc + channel + 4 * i), acc_reg[i]));\n                }\n              }\n              for (; channel <= tranche_depth - 8; channel += 8) {\n                uint16x4_t acc_reg[2];\n                uint16x8_t input_reg = vmovl_u8(vld1_u8(input_channel_ptr));\n                input_channel_ptr += 8;\n                acc_reg[0] = vget_low_u16(input_reg);\n                acc_reg[1] = vget_high_u16(input_reg);\n                for (int i = 0; i < 2; i++) {\n                  vst1q_u32(\n                      acc + channel + 4 * i,\n                      vaddw_u16(vld1q_u32(acc + channel + 4 * i), acc_reg[i]));\n                }\n              }\n#endif\n              for (; channel < tranche_depth; ++channel) {\n                acc[channel] += *input_channel_ptr++;\n              }\n              input_row_ptr += depth;\n            }\n          }\n          uint8* output_ptr = output_data + Offset(output_shape, batch, out_y,\n                                                   out_x, depth_base);\n          int channel = 0;\n#ifdef USE_NEON\n#define AVGPOOL_DIVIDING_BY(FILTER_COUNT)                               \\\n  if (filter_count == FILTER_COUNT) {                                   \\\n    for (; channel <= tranche_depth - 8; channel += 8) {                \\\n      uint16 buf[8];                                                    \\\n      for (int i = 0; i < 8; i++) {                                     \\\n        buf[i] = (acc[channel + i] + FILTER_COUNT / 2) / FILTER_COUNT;  \\\n      }                                                                 \\\n      uint8x8_t buf8 = vqmovn_u16(vld1q_u16(buf));                      \\\n      buf8 = vmin_u8(buf8, vdup_n_u8(params.quantized_activation_max)); \\\n      buf8 = vmax_u8(buf8, vdup_n_u8(params.quantized_activation_min)); \\\n      vst1_u8(output_ptr + channel, buf8);                              \\\n    }                                                                   \\\n  }\n          AVGPOOL_DIVIDING_BY(9)\n          AVGPOOL_DIVIDING_BY(15)\n#undef AVGPOOL_DIVIDING_BY\n          for (; channel <= tranche_depth - 8; channel += 8) {\n            uint16 buf[8];\n            for (int i = 0; i < 8; i++) {\n              buf[i] = (acc[channel + i] + filter_count / 2) / filter_count;\n            }\n            uint8x8_t buf8 = vqmovn_u16(vld1q_u16(buf));\n            buf8 = vmin_u8(buf8, vdup_n_u8(params.quantized_activation_max));\n            buf8 = vmax_u8(buf8, vdup_n_u8(params.quantized_activation_min));\n            vst1_u8(output_ptr + channel, buf8);\n          }\n#endif\n          for (; channel < tranche_depth; ++channel) {\n            uint16 a = (acc[channel] + filter_count / 2) / filter_count;\n            a = std::max<uint16>(a, params.quantized_activation_min);\n            a = std::min<uint16>(a, params.quantized_activation_max);\n            output_ptr[channel] = static_cast<uint8>(a);\n          }\n        }\n      }\n    }\n  }\n  return true;\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const float* input_data, const RuntimeShape& output_shape,\n                    float* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaxPool\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n  auto out_mat = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n  // Prefill the output to minimum representable float value\n  out_mat.setConstant(std::numeric_limits<float>::lowest());\n  for (int b = 0; b < batches; ++b) {\n    for (int h = 0; h < input_height; ++h) {\n      for (int w = 0; w < input_width; ++w) {\n        // (h_start, h_end) * (w_start, w_end) is the range that the input\n        // vector projects to.\n        int hpad = h + params.padding_values.height;\n        int wpad = w + params.padding_values.width;\n        int h_start = (hpad < params.filter_height)\n                          ? 0\n                          : (hpad - params.filter_height) / stride_height + 1;\n        int h_end = std::min(hpad / stride_height + 1, output_height);\n        int w_start = (wpad < params.filter_width)\n                          ? 0\n                          : (wpad - params.filter_width) / stride_width + 1;\n        int w_end = std::min(wpad / stride_width + 1, output_width);\n        // compute elementwise sum\n        for (int ph = h_start; ph < h_end; ++ph) {\n          for (int pw = w_start; pw < w_end; ++pw) {\n            int out_offset = NodeOffset(b, ph, pw, output_height, output_width);\n            out_mat.col(out_offset) =\n                out_mat.col(out_offset)\n                    .cwiseMax(in_mat.col(\n                        NodeOffset(b, h, w, input_height, input_width)));\n          }\n        }\n      }\n    }\n  }\n  const int flat_size = output_shape.FlatSize();\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = ActivationFunctionWithMinMax(output_data[i],\n                                                  params.float_activation_min,\n                                                  params.float_activation_max);\n  }\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const uint8* input_data, const RuntimeShape& output_shape,\n                    uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaxPool/8bit\");\n\n  // Here, and in other pooling ops, in order to maintain locality of reference,\n  // to minimize some recalculations, and to load into NEON vector registers, we\n  // use an inner loop down the depth. Since depths can be large and hence we\n  // would need arbitrarily large temporary storage, we divide the work up into\n  // depth tranches just within the batch loop.\n  static constexpr int kPoolingAccTrancheSize = 256;\n\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  uint8 acc[kPoolingAccTrancheSize];\n  for (int batch = 0; batch < batches; ++batch) {\n    // We proceed through the depth in tranches (see comment above). The\n    // depth_base is the depth at the beginning of the tranche. The\n    // tranche_depth is the depth dimension of the tranche.\n    for (int depth_base = 0; depth_base < depth;\n         depth_base += kPoolingAccTrancheSize) {\n      const int tranche_depth =\n          std::min(depth - depth_base, kPoolingAccTrancheSize);\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          memset(acc, 0, tranche_depth * sizeof(acc[0]));\n          const uint8* input_ptr =\n              input_data + depth_base +\n              depth * (in_x_origin +\n                       input_width * (in_y_origin + input_height * batch));\n          for (int fy = filter_y_start; fy < filter_y_end; fy++) {\n            const uint8* input_row_ptr =\n                input_ptr + depth * (fy * input_width + filter_x_start);\n            for (int fx = filter_x_start; fx < filter_x_end; fx++) {\n              const uint8* input_channel_ptr = input_row_ptr;\n              int channel = 0;\n#ifdef USE_NEON\n              for (; channel <= tranche_depth - 16; channel += 16) {\n                uint8x16_t acc_reg = vld1q_u8(acc + channel);\n                uint8x16_t input_reg = vld1q_u8(input_channel_ptr);\n                input_channel_ptr += 16;\n                acc_reg = vmaxq_u8(acc_reg, input_reg);\n                vst1q_u8(acc + channel, acc_reg);\n              }\n\n              for (; channel <= tranche_depth - 8; channel += 8) {\n                uint8x8_t acc_reg = vld1_u8(acc + channel);\n                uint8x8_t input_reg = vld1_u8(input_channel_ptr);\n                input_channel_ptr += 8;\n                acc_reg = vmax_u8(acc_reg, input_reg);\n                vst1_u8(acc + channel, acc_reg);\n              }\n#endif\n              for (; channel < tranche_depth; ++channel) {\n                acc[channel] = std::max(acc[channel], *input_channel_ptr++);\n              }\n              input_row_ptr += depth;\n            }\n          }\n          uint8* output_ptr = output_data + Offset(output_shape, batch, out_y,\n                                                   out_x, depth_base);\n          int channel = 0;\n#ifdef USE_NEON\n          for (; channel <= tranche_depth - 16; channel += 16) {\n            uint8x16_t a = vld1q_u8(acc + channel);\n            a = vminq_u8(a, vdupq_n_u8(params.quantized_activation_max));\n            a = vmaxq_u8(a, vdupq_n_u8(params.quantized_activation_min));\n            vst1q_u8(output_ptr + channel, a);\n          }\n          for (; channel <= tranche_depth - 8; channel += 8) {\n            uint8x8_t a = vld1_u8(acc + channel);\n            a = vmin_u8(a, vdup_n_u8(params.quantized_activation_max));\n            a = vmax_u8(a, vdup_n_u8(params.quantized_activation_min));\n            vst1_u8(output_ptr + channel, a);\n          }\n#endif\n          for (; channel < tranche_depth; ++channel) {\n            uint8 a = acc[channel];\n            a = std::max<uint8>(a, params.quantized_activation_min);\n            a = std::min<uint8>(a, params.quantized_activation_max);\n            output_ptr[channel] = static_cast<uint8>(a);\n          }\n        }\n      }\n    }\n  }\n}\n\ninline void L2Pool(const PoolParams& params, const RuntimeShape& input_shape,\n                   const float* input_data, const RuntimeShape& output_shape,\n                   float* output_data) {\n  ruy::profiler::ScopeLabel label(\"L2Pool\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  // Actually carry out L2 Pool. Code is written in forward mode: we go through\n  // the input values once, and write to all the pooled regions that it maps to.\n  const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n  auto out_mat = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n  Eigen::VectorXf in_square(in_mat.rows());\n  Eigen::VectorXf out_count(out_mat.cols());\n  out_count.setZero();\n  // Prefill the output to 0.\n  out_mat.setZero();\n  for (int b = 0; b < batches; ++b) {\n    for (int h = 0; h < input_height; ++h) {\n      for (int w = 0; w < input_width; ++w) {\n        // (h_start, h_end) * (w_start, w_end) is the range that the input\n        // vector projects to.\n        const int hpad = h + params.padding_values.height;\n        const int wpad = w + params.padding_values.width;\n        const int h_start =\n            (hpad < params.filter_height)\n                ? 0\n                : (hpad - params.filter_height) / stride_height + 1;\n        const int h_end = std::min(hpad / stride_height + 1, output_height);\n        const int w_start =\n            (wpad < params.filter_width)\n                ? 0\n                : (wpad - params.filter_width) / stride_width + 1;\n        const int w_end = std::min(wpad / stride_width + 1, output_width);\n        // pre-compute square\n        const int in_offset = w + input_width * (h + input_height * b);\n        in_square =\n            in_mat.col(in_offset).array() * in_mat.col(in_offset).array();\n        // compute elementwise sum of squares\n        for (int ph = h_start; ph < h_end; ++ph) {\n          for (int pw = w_start; pw < w_end; ++pw) {\n            const int out_offset = pw + output_width * (ph + output_height * b);\n            out_mat.col(out_offset) += in_square;\n            out_count(out_offset)++;\n          }\n        }\n      }\n    }\n  }\n\n  out_count = out_count.array().inverse();\n  out_mat =\n      (out_mat.array().rowwise() * out_count.transpose().array()).cwiseSqrt();\n\n  const int flat_size = output_shape.FlatSize();\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = ActivationFunctionWithMinMax(output_data[i],\n                                                  params.float_activation_min,\n                                                  params.float_activation_max);\n  }\n}\n\ninline void LocalResponseNormalization(\n    const tflite::LocalResponseNormalizationParams& op_params,\n    const RuntimeShape& input_shape, const float* input_data,\n    const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"LocalResponseNormalization\");\n  MatchingFlatSize(input_shape, output_shape);\n\n  const auto data_in = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n  auto data_out = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n\n  // Carry out local response normalization, vector by vector.\n  // Since the data are stored column major, making row-wise operation\n  // probably not memory efficient anyway, we do an explicit for loop over\n  // the columns.\n  const int double_range = op_params.range * 2;\n  Eigen::VectorXf padded_square(data_in.rows() + double_range);\n  padded_square.setZero();\n  const float bias = op_params.bias;\n  for (int r = 0; r < data_in.cols(); ++r) {\n    // Do local response normalization for data_in(:, r)\n    // first, compute the square and store them in buffer for repeated use\n    padded_square.block(op_params.range, 0, data_in.rows(), 1) =\n        data_in.col(r).cwiseProduct(data_in.col(r)) * op_params.alpha;\n    // Then, compute the scale and writes them to data_out\n    float accumulated_scale = 0;\n    for (int i = 0; i < double_range; ++i) {\n      accumulated_scale += padded_square(i);\n    }\n    for (int i = 0; i < data_in.rows(); ++i) {\n      accumulated_scale += padded_square(i + double_range);\n      data_out(i, r) = bias + accumulated_scale;\n      accumulated_scale -= padded_square(i);\n    }\n  }\n\n  // In a few cases, the pow computation could benefit from speedups.\n  if (op_params.beta == 1) {\n    data_out.array() = data_in.array() * data_out.array().inverse();\n  } else if (op_params.beta == 0.5f) {\n    data_out.array() = data_in.array() * data_out.array().sqrt().inverse();\n  } else {\n    data_out.array() = data_in.array() * data_out.array().pow(-op_params.beta);\n  }\n}\n\ninline void SoftmaxImpl(const SoftmaxParams& params,\n                        const RuntimeShape& input_shape,\n                        const float* input_data,\n                        const RuntimeShape& output_shape, float* output_data,\n                        int start_batch, int end_batch) {\n  ruy::profiler::ScopeLabel label(\"Softmax/Impl\");\n  MatchingFlatSize(input_shape, output_shape);\n\n  const int logit_size = input_shape.Dims(input_shape.DimensionsCount() - 1);\n  const MatrixMap<const float> in_mat(input_data + logit_size * start_batch,\n                                      logit_size, end_batch - start_batch);\n  MatrixMap<float> out_mat(output_data + logit_size * start_batch, logit_size,\n                           end_batch - start_batch);\n  // Compute the exponential first, removing the max coefficient for numerical\n  // stability.\n  out_mat =\n      (in_mat.rowwise() - in_mat.colwise().maxCoeff()).array() * params.beta;\n  // We are separating out the exp function so that exp can be vectorized.\n  out_mat = out_mat.array().exp();\n  // Normalize to get the activations.\n  Eigen::Array<float, 1, Eigen::Dynamic> scale =\n      out_mat.array().colwise().sum().inverse();\n  out_mat.array().rowwise() *= scale;\n}\n\nstruct SoftmaxWorkerTask : cpu_backend_threadpool::Task {\n  SoftmaxWorkerTask(const SoftmaxParams& params,\n                    const RuntimeShape& input_shape, const float* input_data,\n                    const RuntimeShape& output_shape, float* output_data,\n                    int start_batch, int end_batch)\n      : params(params),\n        input_shape(input_shape),\n        input_data(input_data),\n        output_shape(output_shape),\n        output_data(output_data),\n        start_batch(start_batch),\n        end_batch(end_batch) {}\n  void Run() override {\n    SoftmaxImpl(params, input_shape, input_data, output_shape, output_data,\n                start_batch, end_batch);\n  }\n\n private:\n  const tflite::SoftmaxParams& params;\n  const RuntimeShape& input_shape;\n  const float* input_data;\n  const RuntimeShape& output_shape;\n  float* output_data;\n  int start_batch;\n  int end_batch;\n};\n\ninline void Softmax(const SoftmaxParams& params,\n                    const RuntimeShape& input_shape, const float* input_data,\n                    const RuntimeShape& output_shape, float* output_data,\n                    CpuBackendContext* cpu_backend_context = nullptr) {\n  ruy::profiler::ScopeLabel label(\"Softmax\");\n\n  // We picture softmax input as a 2-D matrix while the last dim is the logit\n  // dim, and the rest dims will be the batch dim for the 2-D matrix.\n  const int batch_size =\n      FlatSizeSkipDim(input_shape, input_shape.DimensionsCount() - 1);\n  constexpr int kMinBatchPerThread = 8;\n  int thread_count = batch_size / kMinBatchPerThread;\n  thread_count = thread_count > 0 ? thread_count : 1;\n  const int capped_thread_count =\n      cpu_backend_context == nullptr\n          ? 1\n          : std::min(thread_count, cpu_backend_context->max_num_threads());\n  if (capped_thread_count == 1) {\n    SoftmaxImpl(params, input_shape, input_data, output_shape, output_data, 0,\n                batch_size);\n  } else {\n    std::vector<SoftmaxWorkerTask> tasks;\n    // TODO(b/131746020) don't create new heap allocations every time.\n    // At least we make it a single heap allocation by using reserve().\n    tasks.reserve(capped_thread_count);\n    int batch_start = 0;\n    for (int i = 0; i < capped_thread_count; ++i) {\n      // Try to distribute the tasks as even as possible.\n      int batch_end =\n          batch_start + (batch_size - batch_start) / (capped_thread_count - i);\n      tasks.emplace_back(params, input_shape, input_data, output_shape,\n                         output_data, batch_start, batch_end);\n      batch_start = batch_end;\n    }\n    cpu_backend_threadpool::Execute(tasks.size(), tasks.data(),\n                                    cpu_backend_context);\n  }\n}\n\ntemplate <typename T>\ninline int32_t QuantizeSoftmaxOutput(float prob_rescaled, int32_t zero_point) {\n  const int32_t prob_rnd = static_cast<int32_t>(std::round(prob_rescaled));\n  return prob_rnd + zero_point;\n}\n\n#if !__aarch64__\n// With ARM64, rounding is faster than add + truncation.\ntemplate <>\ninline int32_t QuantizeSoftmaxOutput<uint8_t>(float prob_rescaled,\n                                              int32_t zero_point) {\n  return static_cast<int32_t>(prob_rescaled + 0.5f);\n}\n#endif\n\ninline void PopulateSoftmaxLookupTable(SoftmaxParams* data, float input_scale,\n                                       float beta) {\n  const float scale = -input_scale * beta;\n  const int32_t max_uint8 = std::numeric_limits<uint8_t>::max();\n  for (int32_t val = 0; val <= max_uint8; ++val) {\n    data->table[max_uint8 - val] = expf(scale * val);\n  }\n}\n\ntemplate <typename In, typename Out>\ninline void Softmax(const SoftmaxParams& params,\n                    const RuntimeShape& input_shape, const In* input_data,\n                    const RuntimeShape& output_shape, Out* output_data) {\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int excluding_last_dim =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int last_dim =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  const int32_t clamp_max = std::numeric_limits<Out>::max();\n  const int32_t clamp_min = std::numeric_limits<Out>::min();\n  for (int i = 0; i < excluding_last_dim; ++i) {\n    int32_t max_val = std::numeric_limits<In>::min();\n    // Find max quantized value.\n    for (int j = 0; j < last_dim; ++j) {\n      max_val = std::max(max_val, static_cast<int32_t>(input_data[j]));\n    }\n\n    float sum_exp = 0.0f;\n    const int32_t max_uint8 = std::numeric_limits<uint8_t>::max();\n    const float* table_offset = &params.table[max_uint8 - max_val];\n    // Calculate normalizer sum(exp(x)).\n    for (int j = 0; j < last_dim; ++j) {\n      sum_exp += table_offset[input_data[j]];\n    }\n\n    const float inv_sum_exp = 1.0f / (sum_exp * params.scale);\n    // Normalize and quantize probabilities.\n    for (int j = 0; j < last_dim; ++j) {\n      const float prob_rescaled = table_offset[input_data[j]] * inv_sum_exp;\n      const int32_t prob_quantized =\n          QuantizeSoftmaxOutput<Out>(prob_rescaled, params.zero_point);\n      output_data[j] = static_cast<Out>(\n          std::max(std::min(clamp_max, prob_quantized), clamp_min));\n    }\n    input_data += last_dim;\n    output_data += last_dim;\n  }\n}\n\n// Here's the softmax LUT optimization strategy:\n// For softmax, we can do some mathmetically equivalent transformation:\n//\n// softmax(x) = e^x / sum(e^x, 0...n)  ===> equals to\n// softmax(x) = e^(x - CONST) / sum(e^(x - CONST), 0...n)\n//\n// For quantization, `x` in our case is (input_q - input_zp) * input_s\n// For uint8 case (int8 can be handled similarly), the range is [0, 255]\n//\n// so if we let\n// CONST = (255 - input_zp) * input_s\n// then we will have:\n// softmax(x) = e^((input_q - 255) * input_s) --------- (1)\n//         /\n// sum(e^(input_q - 255) * input_s, 0...n)   -------- (2)\n//\n// the good thing about (1) is it's within the range of (0, 1), so we can\n// approximate its result with uint16.\n//  (1) = uint8_out * 1 / 2^16.\n//\n// so (1) is lookup_uint8_table(input_zp) * 1 / 2^16.\n// then (2) is essentially the following:\n// sum(lookup_uint8_table(input_zp), 0...n) / 2^16.\n//\n// since (output_q - output_zp) * output_s = softmax(x)\n// output_q = lookup_uint8_table(input_zp)\n//            /\n// (sum(lookup_uint8_table(input_zp), 0...n) * output_s)\n//             +\n//   output_zp\n//\n// We can actually further improve the performance by using uint8 instead of\n// uint16. But that we may lose some accuracy, so we need to pay attention\n// to that.\ninline void PopulateSoftmaxUInt8LookupTable(SoftmaxParams* data,\n                                            float input_scale, float beta) {\n  const float scale = input_scale * beta;\n  const int32_t max_uint8 = std::numeric_limits<uint8_t>::max();\n  const int32_t max_uint16 = std::numeric_limits<uint16_t>::max();\n\n  for (int32_t val = 0; val <= max_uint8; ++val) {\n    float input_to_exp = scale * (val - max_uint8);\n    int32_t temp = static_cast<int>(expf(input_to_exp) * max_uint16 + 0.5);\n    temp = std::min(max_uint16, temp);\n    uint8_t part1 = temp >> 8;\n    uint8_t part2 = temp & 0xff;\n    data->uint8_table1[val] = static_cast<uint8_t>(part1);\n    data->uint8_table2[val] = static_cast<uint8_t>(part2);\n  }\n}\n\ninline int FindMaxValue(int size, const uint8_t* input_data, uint8_t offset) {\n  int32_t max_val = std::numeric_limits<uint8_t>::min();\n  int j = 0;\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n  uint8x16_t max_val_dup = vdupq_n_u8(max_val);\n  uint8x16_t offset_dup = vdupq_n_u8(offset);\n  for (; j <= size - 16; j += 16) {\n    uint8x16_t input_value = vld1q_u8(input_data + j);\n    input_value = veorq_u8(input_value, offset_dup);\n    max_val_dup = vmaxq_u8(input_value, max_val_dup);\n  }\n  max_val = std::max(max_val, static_cast<int32>(vmaxvq_u8(max_val_dup)));\n#endif\n\n  for (; j < size; ++j) {\n    max_val = std::max(max_val, static_cast<int32_t>(input_data[j] ^ offset));\n  }\n  return max_val;\n}\n\n#ifdef USE_NEON\n// Value_to_store layout:\n// [high_high, high_low, low_high, low_low].\ninline void StoreValue(int32x4x4_t value_to_store, int8_t* output) {\n  const int16x8_t result_1 = vcombine_s16(vqmovn_s32(value_to_store.val[1]),\n                                          vqmovn_s32(value_to_store.val[0]));\n  const int16x8_t result_2 = vcombine_s16(vqmovn_s32(value_to_store.val[3]),\n                                          vqmovn_s32(value_to_store.val[2]));\n  const int8x16_t result =\n      vcombine_s8(vqmovn_s16(result_2), vqmovn_s16(result_1));\n  vst1q_s8(output, result);\n}\n\n// Value_to_store layout:\n// [high_high, high_low, low_high, low_low].\ninline void StoreValue(int32x4x4_t value_to_store, uint8_t* output) {\n  const uint16x8_t result_1 =\n      vcombine_u16(vqmovn_u32(vreinterpretq_u32_s32(value_to_store.val[1])),\n                   vqmovn_u32(vreinterpretq_u32_s32(value_to_store.val[0])));\n  const uint16x8_t result_2 =\n      vcombine_u16(vqmovn_u32(vreinterpretq_u32_s32(value_to_store.val[3])),\n                   vqmovn_u32(vreinterpretq_u32_s32(value_to_store.val[2])));\n  const uint8x16_t result =\n      vcombine_u8(vqmovn_u16(result_2), vqmovn_u16(result_1));\n  vst1q_u8(output, result);\n}\n\n#endif\n\ntemplate <typename In, typename Out>\ninline void SoftmaxInt8LUT(const SoftmaxParams& params,\n                           const RuntimeShape& input_shape,\n                           const In* input_data,\n                           const RuntimeShape& output_shape, Out* output_data) {\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int excluding_last_dim =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int last_dim =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  const int32_t clamp_max = std::numeric_limits<Out>::max();\n  const int32_t clamp_min = std::numeric_limits<Out>::min();\n\n  // Offset is used to interpret the input data \"correctly\".\n  // If the input is uint8, the data will be unchanged.\n  // If the input is int8, since it will be reinterpret as uint8.\n  // e.g.,\n  // int8 127 will be applied \"offset\" to become 255 in uint8.\n  uint8_t offset = 0;\n  if (std::is_same<In, int8>::value) {\n    offset = 0x80;\n  }\n\n  const uint8_t* input_data_uint = reinterpret_cast<const uint8_t*>(input_data);\n\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n  // This code uses ARM64-only instructions.\n  // TODO(b/143709993): Port to ARMv7\n\n  // Load the tables into registers. (4*4 128-bit registers)\n  uint8x16x4_t table1[4];\n  table1[0] = vld1q_u8_x4(params.uint8_table1 + 16 * 4 * 0);\n  table1[1] = vld1q_u8_x4(params.uint8_table1 + 16 * 4 * 1);\n  table1[2] = vld1q_u8_x4(params.uint8_table1 + 16 * 4 * 2);\n  table1[3] = vld1q_u8_x4(params.uint8_table1 + 16 * 4 * 3);\n\n  uint8x16x4_t table2[4];\n  table2[0] = vld1q_u8_x4(params.uint8_table2 + 16 * 4 * 0);\n  table2[1] = vld1q_u8_x4(params.uint8_table2 + 16 * 4 * 1);\n  table2[2] = vld1q_u8_x4(params.uint8_table2 + 16 * 4 * 2);\n  table2[3] = vld1q_u8_x4(params.uint8_table2 + 16 * 4 * 3);\n#endif\n\n  for (int i = 0; i < excluding_last_dim; ++i) {\n    // Find max quantized value.\n    int32_t max_val = FindMaxValue(last_dim, input_data_uint, offset);\n\n    int32 sum_exp = 0;\n    const int32_t max_uint8 = std::numeric_limits<uint8_t>::max();\n    const uint8_t table_offset = max_uint8 - max_val;\n\n    // Calculate normalizer sum(exp(x)).\n    int sum_j = 0;\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n    uint8x16_t table_offset_dup = vdupq_n_u8(table_offset);\n    uint8x16_t offset_dup = vdupq_n_u8(offset);\n    uint32x4_t sum_4 = vdupq_n_u32(0);\n    const int multiplier_shift = 8;\n    for (; sum_j <= last_dim - 16; sum_j += 16) {\n      uint8x16_t input_value = vld1q_u8(input_data_uint + sum_j);\n      input_value = veorq_u8(input_value, offset_dup);\n      input_value = vaddq_u8(input_value, table_offset_dup);\n\n      const uint8x16_t output1 = aarch64_lookup_vector(table1, input_value);\n      const uint8x16_t output2 = aarch64_lookup_vector(table2, input_value);\n\n      uint16x8_t exp_value1 =\n          vshll_n_u8(vget_high_u8(output1), multiplier_shift);\n      uint16x8_t exp_value2 =\n          vshll_n_u8(vget_low_u8(output1), multiplier_shift);\n\n      exp_value1 = vaddw_u8(exp_value1, vget_high_u8(output2));\n      exp_value2 = vaddw_u8(exp_value2, vget_low_u8(output2));\n\n      sum_4 = vpadalq_u16(sum_4, exp_value1);\n      sum_4 = vpadalq_u16(sum_4, exp_value2);\n    }\n    int temp = vgetq_lane_u32(sum_4, 0) + vgetq_lane_u32(sum_4, 1) +\n               vgetq_lane_u32(sum_4, 2) + vgetq_lane_u32(sum_4, 3);\n    sum_exp += temp;\n\n#endif\n    for (; sum_j < last_dim; ++sum_j) {\n      const uint8_t index = (input_data_uint[sum_j] ^ offset) + table_offset;\n\n      uint8_t part1 = params.uint8_table1[index];\n      uint8_t part2 = params.uint8_table2[index];\n      sum_exp += ((part1 << 8) + part2);\n    }\n\n    const float inv_sum_exp = 1.0f / (sum_exp * params.scale);\n\n    int32 multiplier, shift;\n    QuantizeMultiplier(inv_sum_exp, &multiplier, &shift);\n\n    // Normalize and quantize probabilities.\n    int j = 0;\n#ifdef TFLITE_SOFTMAX_USE_UINT16_LUT\n    const int32x4_t output_zp_dup = vdupq_n_s32(params.zero_point);\n    const int32x4_t max_val_dup = vdupq_n_s32(clamp_max);\n    const int32x4_t min_val_dup = vdupq_n_s32(clamp_min);\n\n    for (; j <= last_dim - 16; j += 16) {\n      uint8x16_t input_value = vld1q_u8(input_data_uint + j);\n      input_value = veorq_u8(input_value, offset_dup);\n      input_value = vaddq_u8(input_value, table_offset_dup);\n\n      const uint8x16_t output1 = aarch64_lookup_vector(table1, input_value);\n      const uint8x16_t output2 = aarch64_lookup_vector(table2, input_value);\n\n      uint16x8_t exp_value1 =\n          vshll_n_u8(vget_high_u8(output1), multiplier_shift);\n      uint16x8_t exp_value2 =\n          vshll_n_u8(vget_low_u8(output1), multiplier_shift);\n\n      exp_value1 = vaddw_u8(exp_value1, vget_high_u8(output2));\n      exp_value2 = vaddw_u8(exp_value2, vget_low_u8(output2));\n\n      int32x4x4_t output_value;\n      output_value.val[0] =\n          vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(exp_value1)));\n      output_value.val[1] =\n          vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(exp_value1)));\n      output_value.val[2] =\n          vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(exp_value2)));\n      output_value.val[3] =\n          vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(exp_value2)));\n\n      int32x4x4_t temp_val =\n          MultiplyByQuantizedMultiplier4Rows(output_value, multiplier, shift);\n\n      temp_val.val[0] = vaddq_s32(temp_val.val[0], output_zp_dup);\n      temp_val.val[1] = vaddq_s32(temp_val.val[1], output_zp_dup);\n      temp_val.val[2] = vaddq_s32(temp_val.val[2], output_zp_dup);\n      temp_val.val[3] = vaddq_s32(temp_val.val[3], output_zp_dup);\n\n      temp_val.val[0] =\n          vmaxq_s32(vminq_s32(temp_val.val[0], max_val_dup), min_val_dup);\n      temp_val.val[1] =\n          vmaxq_s32(vminq_s32(temp_val.val[1], max_val_dup), min_val_dup);\n      temp_val.val[2] =\n          vmaxq_s32(vminq_s32(temp_val.val[2], max_val_dup), min_val_dup);\n      temp_val.val[3] =\n          vmaxq_s32(vminq_s32(temp_val.val[3], max_val_dup), min_val_dup);\n\n      StoreValue(temp_val, output_data + j);\n    }\n#endif\n    for (; j < last_dim; ++j) {\n      const uint8_t index = (input_data_uint[j] ^ offset) + table_offset;\n      const uint8_t part1 = params.uint8_table1[index];\n      const uint8_t part2 = params.uint8_table2[index];\n      const int32_t exp_value = (part1 << 8) + part2;\n      const int32_t output_value =\n          MultiplyByQuantizedMultiplier(exp_value, multiplier, shift);\n\n      output_data[j] = static_cast<Out>(std::max(\n          std::min(clamp_max, output_value + params.zero_point), clamp_min));\n    }\n    input_data_uint += last_dim;\n    output_data += last_dim;\n  }\n}\n\ninline void LogSoftmax(const SoftmaxParams& params,\n                       const RuntimeShape& input_shape, const float* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"LogSoftmax\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  for (int i = 0; i < outer_size; ++i) {\n    VectorMap<const float> block_input(input_data + i * depth, depth, 1);\n    VectorMap<float> block_output(output_data + i * depth, depth, 1);\n    // Find max element value which we'll use to ensure numerical stability\n    // taking advantage of the following equality:\n    // log(exp(x[i])/sum(exp(x[i]))) == log(exp(x[i]+C)/sum(exp(x[i]+C)))\n    const float max = block_input.maxCoeff();\n    const float log_sum = std::log((block_input.array() - max).exp().sum());\n    block_output = block_input.array() - max - log_sum;\n  }\n}\n\n// Backwards compatibility. Less optimized than below version.\ninline void LogSoftmax(const SoftmaxParams& params,\n                       const RuntimeShape& input_shape, const uint8* input_data,\n                       const RuntimeShape& output_shape, uint8* output_data) {\n  reference_ops::LogSoftmax(params, input_shape, input_data, output_shape,\n                            output_data);\n}\n\n// Compute LogSoftmax as (x - x_max) - ln(sum(e^(x_i - x_max)...)\n// as done in tf.nn.log_softmax to prevent underflow and overflow.\n// This is in contrast to just log(softmax(x))\n//\n// To handle quantization, first dequantize the inputs (from doing\n// e^(input scale * val) where we ignore the zero point since it cancels\n// out during subtraction due to the ln) and do a rescale at the end to int8.\n//\n// Notably this makes use of float and is intended as the optimized\n// form for quantized execution on CPU. For a fully integer version,\n// see the reference op.\n//\n// TODO(tflite): notes for optimization:\n// 1) See if e^ is also bottleneck in the reference fully-integer\n// version and apply lookup there and compare.\ntemplate <typename T>\ninline void LogSoftmax(const SoftmaxParams& params, float input_scale,\n                       const RuntimeShape& input_shape, const T* input_data,\n                       const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"LogSoftmax\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int excluding_last_dim =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int last_dim =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  const int32_t clamp_max = std::numeric_limits<T>::max();\n  const int32_t clamp_min = std::numeric_limits<T>::min();\n\n  for (int i = 0; i < excluding_last_dim; ++i) {\n    T max_val = std::numeric_limits<T>::min();\n    // Find max quantized value.\n    for (int j = 0; j < last_dim; ++j) {\n      max_val = std::max(max_val, input_data[j]);\n    }\n\n    float sum_exp = 0.0f;\n    const int32_t max_uint8 = std::numeric_limits<uint8>::max();\n    // Offset into table to compute exp(scale*(x - xmax)) instead of\n    // exp(scale*(x)) to prevent overflow.\n    const float* table_offset = &params.table[max_uint8 - max_val];\n    // Calculate sum(exp(scale*(x - x_max))).\n    for (int j = 0; j < last_dim; ++j) {\n      sum_exp += table_offset[input_data[j]];\n    }\n    const float log_sum_exp = std::log(sum_exp);\n\n    // params.scale is the output scale.\n    const float scale = input_scale / params.scale;\n    const float precomputed =\n        (input_scale * max_val + log_sum_exp) / params.scale;\n    for (int j = 0; j < last_dim; ++j) {\n      // Equivalent to (input_scale * (input_data[j] - max_val) - log_sum_exp) /\n      // output_scale.\n      const float log_prob = scale * input_data[j] - precomputed;\n\n      // TODO(tflite): look into better solution.\n      // Use std::rint over std::round (which is used in\n      // FakeQuant) since it's multiple times faster on tested arm32.\n      const int32_t prob_quantized = std::rint(log_prob) + params.zero_point;\n      output_data[j] = static_cast<T>(\n          std::max(std::min(clamp_max, prob_quantized), clamp_min));\n    }\n    input_data += last_dim;\n    output_data += last_dim;\n  }\n}\n\ninline void Logistic(const RuntimeShape& input_shape, const float* input_data,\n                     const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() =\n      input_map.array().unaryExpr(Eigen::internal::scalar_logistic_op<float>());\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// uniform between data types.\ninline void Logistic(const LogisticParams&, const RuntimeShape& input_shape,\n                     const float* input_data, const RuntimeShape& output_shape,\n                     float* output_data) {\n  // Drop params: not needed.\n  Logistic(input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const LogisticParams& params,\n                     const RuntimeShape& input_shape, const int16* input_data,\n                     const RuntimeShape& output_shape, int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic/Int16\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n  }\n\n  int c = 0;\n  const int16* input_data_ptr = input_data;\n  int16* output_data_ptr = output_data;\n#ifdef GEMMLOWP_NEON\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<int16x8_t, 3>;\n\n    for (; c <= flat_size - 16; c += 16) {\n      F3 input0 = F3::FromRaw(vld1q_s16(input_data_ptr));\n      F3 input1 = F3::FromRaw(vld1q_s16(input_data_ptr + 8));\n      F0 output0 = gemmlowp::logistic(input0);\n      F0 output1 = gemmlowp::logistic(input1);\n      vst1q_s16(output_data_ptr, output0.raw());\n      vst1q_s16(output_data_ptr + 8, output1.raw());\n\n      input_data_ptr += 16;\n      output_data_ptr += 16;\n    }\n    for (; c <= flat_size - 8; c += 8) {\n      F3 input = F3::FromRaw(vld1q_s16(input_data_ptr));\n      F0 output = gemmlowp::logistic(input);\n      vst1q_s16(output_data_ptr, output.raw());\n\n      input_data_ptr += 8;\n      output_data_ptr += 8;\n    }\n  }\n#endif\n#ifdef GEMMLOWP_SSE4\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<gemmlowp::int16x8_m128i, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<gemmlowp::int16x8_m128i, 3>;\n\n    for (; c <= flat_size - 16; c += 16) {\n      F3 input0 = F3::FromRaw(gemmlowp::to_int16x8_m128i(\n          _mm_loadu_si128(reinterpret_cast<const __m128i*>(input_data_ptr))));\n      F3 input1 = F3::FromRaw(gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n          reinterpret_cast<const __m128i*>(input_data_ptr + 8))));\n      F0 output0 = gemmlowp::logistic(input0);\n      F0 output1 = gemmlowp::logistic(input1);\n      _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                       output0.raw().v);\n      _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr + 8),\n                       output1.raw().v);\n      input_data_ptr += 16;\n      output_data_ptr += 16;\n    }\n    for (; c <= flat_size - 8; c += 8) {\n      F3 input = F3::FromRaw(gemmlowp::to_int16x8_m128i(\n          _mm_loadu_si128(reinterpret_cast<const __m128i*>(input_data_ptr))));\n      F0 output = gemmlowp::logistic(input);\n      _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                       output.raw().v);\n      input_data_ptr += 8;\n      output_data_ptr += 8;\n    }\n  }\n#endif\n\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<std::int16_t, 3>;\n\n    for (; c < flat_size; ++c) {\n      F3 input = F3::FromRaw(*input_data_ptr);\n      F0 output = gemmlowp::logistic(input);\n      *output_data_ptr = output.raw();\n\n      ++input_data_ptr;\n      ++output_data_ptr;\n    }\n  }\n}\n\ninline void Tanh(const RuntimeShape& input_shape, const float* input_data,\n                 const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Tanh\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() = input_map.array().tanh();\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// uniform between data types.\ninline void Tanh(const TanhParams&, const RuntimeShape& input_shape,\n                 const float* input_data, const RuntimeShape& output_shape,\n                 float* output_data) {\n  // Drop params: not needed.\n  Tanh(input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const TanhParams& params, const RuntimeShape& input_shape,\n                 const int16* input_data, const RuntimeShape& output_shape,\n                 int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Tanh/Int16\");\n  const int input_left_shift = params.input_left_shift;\n  // Support for shifts is limited until we have a parameterized version of\n  // SaturatingRoundingMultiplyByPOT().\n  TFLITE_DCHECK_GE(input_left_shift, 0);\n  TFLITE_DCHECK_LE(input_left_shift, 1);\n\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n  const int16* input_data_ptr = input_data;\n  int16* output_data_ptr = output_data;\n#ifdef GEMMLOWP_NEON\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<int16x8_t, 3>;\n\n    if (input_left_shift == 0) {\n      for (; c <= flat_size - 16; c += 16) {\n        F3 input0 = F3::FromRaw(vld1q_s16(input_data_ptr));\n        F3 input1 = F3::FromRaw(vld1q_s16(input_data_ptr + 8));\n        F0 output0 = gemmlowp::tanh(input0);\n        F0 output1 = gemmlowp::tanh(input1);\n        vst1q_s16(output_data_ptr, output0.raw());\n        vst1q_s16(output_data_ptr + 8, output1.raw());\n\n        input_data_ptr += 16;\n        output_data_ptr += 16;\n      }\n      for (; c <= flat_size - 8; c += 8) {\n        F3 input = F3::FromRaw(vld1q_s16(input_data_ptr));\n        F0 output = gemmlowp::tanh(input);\n        vst1q_s16(output_data_ptr, output.raw());\n\n        input_data_ptr += 8;\n        output_data_ptr += 8;\n      }\n    } else {\n      for (; c <= flat_size - 16; c += 16) {\n        F3 input0 = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            vld1q_s16(input_data_ptr)));\n        F3 input1 = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            vld1q_s16(input_data_ptr + 8)));\n        F0 output0 = gemmlowp::tanh(input0);\n        F0 output1 = gemmlowp::tanh(input1);\n        vst1q_s16(output_data_ptr, output0.raw());\n        vst1q_s16(output_data_ptr + 8, output1.raw());\n\n        input_data_ptr += 16;\n        output_data_ptr += 16;\n      }\n      for (; c <= flat_size - 8; c += 8) {\n        F3 input = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            vld1q_s16(input_data_ptr)));\n        F0 output = gemmlowp::tanh(input);\n        vst1q_s16(output_data_ptr, output.raw());\n\n        input_data_ptr += 8;\n        output_data_ptr += 8;\n      }\n    }\n  }\n#endif\n#ifdef GEMMLOWP_SSE4\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<gemmlowp::int16x8_m128i, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<gemmlowp::int16x8_m128i, 3>;\n\n    if (input_left_shift == 0) {\n      for (; c <= flat_size - 16; c += 16) {\n        F3 input0 = F3::FromRaw(gemmlowp::to_int16x8_m128i(\n            _mm_loadu_si128(reinterpret_cast<const __m128i*>(input_data_ptr))));\n        F3 input1 = F3::FromRaw(gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n            reinterpret_cast<const __m128i*>(input_data_ptr + 8))));\n        F0 output0 = gemmlowp::tanh(input0);\n        F0 output1 = gemmlowp::tanh(input1);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                         output0.raw().v);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr + 8),\n                         output1.raw().v);\n\n        input_data_ptr += 16;\n        output_data_ptr += 16;\n      }\n      for (; c <= flat_size - 8; c += 8) {\n        F3 input = F3::FromRaw(gemmlowp::to_int16x8_m128i(\n            _mm_loadu_si128(reinterpret_cast<const __m128i*>(input_data_ptr))));\n        F0 output = gemmlowp::tanh(input);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                         output.raw().v);\n        input_data_ptr += 8;\n        output_data_ptr += 8;\n      }\n    } else {\n      for (; c <= flat_size - 16; c += 16) {\n        F3 input0 = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n                reinterpret_cast<const __m128i*>(input_data_ptr)))));\n        F3 input1 = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n                reinterpret_cast<const __m128i*>(input_data_ptr + 8)))));\n        F0 output0 = gemmlowp::tanh(input0);\n        F0 output1 = gemmlowp::tanh(input1);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                         output0.raw().v);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr + 8),\n                         output1.raw().v);\n\n        input_data_ptr += 16;\n        output_data_ptr += 16;\n      }\n      for (; c <= flat_size - 8; c += 8) {\n        F3 input = F3::FromRaw(gemmlowp::SaturatingRoundingMultiplyByPOT<1>(\n            gemmlowp::to_int16x8_m128i(_mm_loadu_si128(\n                reinterpret_cast<const __m128i*>(input_data_ptr)))));\n        F0 output = gemmlowp::tanh(input);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_data_ptr),\n                         output.raw().v);\n        input_data_ptr += 8;\n        output_data_ptr += 8;\n      }\n    }\n  }\n#endif\n\n  {\n    // F0 uses 0 integer bits, range [-1, 1].\n    // This is the return type of math functions such as tanh, logistic,\n    // whose range is in [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n    // F3 uses 3 integer bits, range [-8, 8], the input range expected here.\n    using F3 = gemmlowp::FixedPoint<std::int16_t, 3>;\n\n    if (input_left_shift == 0) {\n      for (; c < flat_size; ++c) {\n        F3 input = F3::FromRaw(*input_data_ptr);\n        F0 output = gemmlowp::tanh(input);\n        *output_data_ptr = output.raw();\n\n        ++input_data_ptr;\n        ++output_data_ptr;\n      }\n    } else {\n      for (; c < flat_size; ++c) {\n        F3 input = F3::FromRaw(\n            gemmlowp::SaturatingRoundingMultiplyByPOT<1>(*input_data_ptr));\n        F0 output = gemmlowp::tanh(input);\n        *output_data_ptr = output.raw();\n\n        ++input_data_ptr;\n        ++output_data_ptr;\n      }\n    }\n  }\n}\n\ntemplate <typename SrcT, typename DstT>\ninline void Cast(const RuntimeShape& input_shape, const SrcT* input_data,\n                 const RuntimeShape& output_shape, DstT* output_data) {\n  ruy::profiler::ScopeLabel label(\"Cast\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() = input_map.array().template cast<DstT>();\n}\n\ninline void Floor(const RuntimeShape& input_shape, const float* input_data,\n                  const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Floor\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() = Eigen::floor(input_map.array());\n}\n\ninline void Ceil(const RuntimeShape& input_shape, const float* input_data,\n                 const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Ceil\");\n  auto input_map = MapAsVector(input_data, input_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  output_map.array() = Eigen::ceil(input_map.array());\n}\n\n// Helper methods for BatchToSpaceND.\n// `spatial_index_dim` specifies post-crop offset index in this spatial\n// dimension, i.e. spatial offset introduced by flattening batch to spatial\n// dimension minus the crop size at beginning. `block_shape_dim` is the block\n// size in current dimension. `input_dim` and `output_dim` are input and output\n// size of BatchToSpaceND operation in current dimension.\n// Output start index is inclusive and end index is exclusive.\ninline void GetIndexRange(int spatial_index_dim, int block_shape_dim,\n                          int input_dim, int output_dim, int* start_index,\n                          int* end_index) {\n  // (*start_index) * block_shape_dim is effectively rounded up to the next\n  // multiple of block_shape_dim by the integer division.\n  *start_index =\n      std::max(0, (-spatial_index_dim + block_shape_dim - 1) / block_shape_dim);\n  // Similarly, (*end_index) * block_shape_dim is rounded up too (note that\n  // end_index is exclusive).\n  *end_index = std::min(\n      input_dim,\n      (output_dim - spatial_index_dim + block_shape_dim - 1) / block_shape_dim);\n}\n\ntemplate <typename T>\ninline void BatchToSpaceND(\n    const RuntimeShape& unextended_input1_shape, const T* input1_data,\n    const RuntimeShape& unextended_input2_shape, const int32* block_shape_data,\n    const RuntimeShape& unextended_input3_shape, const int32* crops_data,\n    const RuntimeShape& unextended_output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"BatchToSpaceND\");\n\n  TFLITE_DCHECK_GE(unextended_input1_shape.DimensionsCount(), 3);\n  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(unextended_input1_shape.DimensionsCount(),\n                   unextended_output_shape.DimensionsCount());\n\n  // Extends the input/output shape from 3D to 4D if needed, NHC -> NH1C.\n  auto extend_shape = [](const RuntimeShape& shape) {\n    if (shape.DimensionsCount() == 4) {\n      return shape;\n    }\n    RuntimeShape new_shape(4, 1);\n    new_shape.SetDim(0, shape.Dims(0));\n    new_shape.SetDim(1, shape.Dims(1));\n    new_shape.SetDim(3, shape.Dims(2));\n    return new_shape;\n  };\n  const RuntimeShape input1_shape = extend_shape(unextended_input1_shape);\n  const RuntimeShape output_shape = extend_shape(unextended_output_shape);\n\n  const int output_width = output_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_batch_size = output_shape.Dims(0);\n\n  const int depth = input1_shape.Dims(3);\n  const int input_width = input1_shape.Dims(2);\n  const int input_height = input1_shape.Dims(1);\n  const int input_batch_size = input1_shape.Dims(0);\n\n  const int block_shape_height = block_shape_data[0];\n  const int block_shape_width =\n      unextended_input1_shape.DimensionsCount() == 4 ? block_shape_data[1] : 1;\n  const int crops_top = crops_data[0];\n  const int crops_left =\n      unextended_input1_shape.DimensionsCount() == 4 ? crops_data[2] : 0;\n\n  for (int in_batch = 0; in_batch < input_batch_size; ++in_batch) {\n    const int out_batch = in_batch % output_batch_size;\n    const int spatial_offset = in_batch / output_batch_size;\n\n    int in_h_start = 0;\n    int in_h_end = 0;\n    // GetIndexRange ensures start and end indices are in [0, output_height).\n    GetIndexRange(spatial_offset / block_shape_width - crops_top,\n                  block_shape_height, input_height, output_height, &in_h_start,\n                  &in_h_end);\n\n    for (int in_h = in_h_start; in_h < in_h_end; ++in_h) {\n      const int out_h = in_h * block_shape_height +\n                        spatial_offset / block_shape_width - crops_top;\n      TFLITE_DCHECK_GE(out_h, 0);\n      TFLITE_DCHECK_LT(out_h, output_height);\n\n      int in_w_start = 0;\n      int in_w_end = 0;\n      // GetIndexRange ensures start and end indices are in [0, output_width).\n      GetIndexRange(spatial_offset % block_shape_width - crops_left,\n                    block_shape_width, input_width, output_width, &in_w_start,\n                    &in_w_end);\n\n      for (int in_w = in_w_start; in_w < in_w_end; ++in_w) {\n        const int out_w = in_w * block_shape_width +\n                          spatial_offset % block_shape_width - crops_left;\n        TFLITE_DCHECK_GE(out_w, 0);\n        TFLITE_DCHECK_LT(out_w, output_width);\n        T* out = output_data + Offset(output_shape, out_batch, out_h, out_w, 0);\n        const T* in =\n            input1_data + Offset(input1_shape, in_batch, in_h, in_w, 0);\n        memcpy(out, in, depth * sizeof(T));\n      }\n    }\n  }\n}\n\ntemplate <typename T>\nvoid TypedMemset(void* ptr, T value, size_t num) {\n  // Optimization for common cases where memset() will suffice.\n  if (value == 0 || std::is_same<T, uint8_t>::value) {\n    memset(ptr, value, num * sizeof(T));\n  } else {\n    // Default implementation for cases where memset() will not preserve the\n    // bytes, e.g., typically when sizeof(T) > sizeof(uint8_t).\n    char* pos = static_cast<char*>(ptr);\n    for (size_t i = 0; i < num; ++i) {\n      memcpy(pos, &value, sizeof(T));\n      pos = pos + sizeof(T);\n    }\n  }\n}\n\n// This makes heavy use of Offset, along with conditional branches. There may be\n// opportunities for improvement.\n//\n// There are two versions of pad: Pad and PadV2.  In PadV2 there is a second\n// scalar input that provides the padding value.  Therefore pad_value_ptr can be\n// equivalent to a simple input1_data.  For Pad, it should point to a zero\n// value.\n//\n// Note that two typenames are required, so that T=P=int32 is considered a\n// specialization distinct from P=int32.\ntemplate <typename T, typename P>\ninline void PadImpl(const tflite::PadParams& op_params,\n                    const RuntimeShape& input_shape, const T* input_data,\n                    const P* pad_value_ptr, const RuntimeShape& output_shape,\n                    T* output_data) {\n  ruy::profiler::ScopeLabel label(\"PadImpl\");\n  const int max_supported_dims = 5;\n  const RuntimeShape ext_input_shape =\n      RuntimeShape::ExtendedShape(max_supported_dims, input_shape);\n  const RuntimeShape ext_output_shape =\n      RuntimeShape::ExtendedShape(max_supported_dims, output_shape);\n  TFLITE_DCHECK_LE(op_params.left_padding_count, max_supported_dims);\n  TFLITE_DCHECK_LE(op_params.right_padding_count, max_supported_dims);\n\n  // Pad kernels are limited to max 4 dimensions. Copy inputs so we can pad them\n  // to 4 dims (yes, we are \"padding the padding\").\n  std::vector<int> left_padding_copy(max_supported_dims, 0);\n  const int left_padding_extend =\n      max_supported_dims - op_params.left_padding_count;\n  for (int i = 0; i < op_params.left_padding_count; ++i) {\n    left_padding_copy[left_padding_extend + i] = op_params.left_padding[i];\n  }\n  std::vector<int> right_padding_copy(max_supported_dims, 0);\n  const int right_padding_extend =\n      max_supported_dims - op_params.right_padding_count;\n  for (int i = 0; i < op_params.right_padding_count; ++i) {\n    right_padding_copy[right_padding_extend + i] = op_params.right_padding[i];\n  }\n\n  const int output_batch = ext_output_shape.Dims(0);\n  const int output_spatial_dim1 = ext_output_shape.Dims(1);\n  const int output_spatial_dim2 = ext_output_shape.Dims(2);\n  const int output_spatial_dim3 = ext_output_shape.Dims(3);\n  const int output_channel = ext_output_shape.Dims(4);\n\n  const int left_b_padding = left_padding_copy[0];\n  const int left_s1_padding = left_padding_copy[1];\n  const int left_s2_padding = left_padding_copy[2];\n  const int left_s3_padding = left_padding_copy[3];\n  const int left_c_padding = left_padding_copy[4];\n\n  const int right_b_padding = right_padding_copy[0];\n  const int right_s1_padding = right_padding_copy[1];\n  const int right_s2_padding = right_padding_copy[2];\n  const int right_s3_padding = right_padding_copy[3];\n  const int right_c_padding = right_padding_copy[4];\n\n  const int input_depth = ext_input_shape.Dims(4);\n  const T pad_value = *pad_value_ptr;\n\n  if (left_b_padding != 0) {\n    TypedMemset<T>(output_data, pad_value,\n                   left_b_padding * output_spatial_dim1 * output_spatial_dim2 *\n                       output_spatial_dim3 * output_channel);\n  }\n  for (int out_b = left_b_padding; out_b < output_batch - right_b_padding;\n       ++out_b) {\n    if (left_s1_padding != 0) {\n      TypedMemset<T>(output_data + Offset(ext_output_shape, out_b, 0, 0, 0, 0),\n                     pad_value,\n                     left_s1_padding * output_spatial_dim2 *\n                         output_spatial_dim3 * output_channel);\n    }\n    for (int out_p = left_s1_padding;\n         out_p < output_spatial_dim1 - right_s1_padding; ++out_p) {\n      if (left_s2_padding != 0) {\n        TypedMemset<T>(\n            output_data + Offset(ext_output_shape, out_b, out_p, 0, 0, 0),\n            pad_value, left_s2_padding * output_spatial_dim3 * output_channel);\n      }\n      for (int out_h = left_s2_padding;\n           out_h < output_spatial_dim2 - right_s2_padding; ++out_h) {\n        if (left_s3_padding != 0) {\n          TypedMemset<T>(\n              output_data + Offset(ext_output_shape, out_b, out_p, out_h, 0, 0),\n              pad_value, left_s3_padding * output_channel);\n        }\n        for (int out_w = left_s3_padding;\n             out_w < output_spatial_dim3 - right_s3_padding; ++out_w) {\n          if (left_c_padding != 0) {\n            TypedMemset<T>(output_data + Offset(ext_output_shape, out_b, out_p,\n                                                out_h, out_w, 0),\n                           pad_value, left_c_padding);\n          }\n\n          T* out = output_data + Offset(ext_output_shape, out_b, out_p, out_h,\n                                        out_w, left_c_padding);\n          const T* in = input_data +\n                        Offset(ext_input_shape, out_b - left_b_padding,\n                               out_p - left_s1_padding, out_h - left_s2_padding,\n                               out_w - left_s3_padding, 0);\n          memcpy(out, in, input_depth * sizeof(T));\n\n          if (right_c_padding != 0) {\n            TypedMemset<T>(\n                output_data + Offset(ext_output_shape, out_b, out_p, out_h,\n                                     out_w, output_channel - right_c_padding),\n                pad_value, right_c_padding);\n          }\n        }\n        if (right_s3_padding != 0) {\n          TypedMemset<T>(\n              output_data + Offset(ext_output_shape, out_b, out_p, out_h,\n                                   output_spatial_dim3 - right_s3_padding, 0),\n              pad_value, right_s3_padding * output_channel);\n        }\n      }\n      if (right_s2_padding != 0) {\n        TypedMemset<T>(\n            output_data + Offset(ext_output_shape, out_b, out_p,\n                                 output_spatial_dim2 - right_s2_padding, 0, 0),\n            pad_value, right_s2_padding * output_spatial_dim3 * output_channel);\n      }\n    }\n    if (right_s1_padding != 0) {\n      TypedMemset<T>(\n          output_data + Offset(ext_output_shape, out_b,\n                               output_spatial_dim1 - right_s1_padding, 0, 0, 0),\n          pad_value,\n          right_s1_padding * output_spatial_dim2 * output_spatial_dim3 *\n              output_channel);\n    }\n  }\n  if (right_b_padding != 0) {\n    TypedMemset<T>(\n        output_data + Offset(ext_output_shape, output_batch - right_b_padding,\n                             0, 0, 0, 0),\n        pad_value,\n        right_b_padding * output_spatial_dim1 * output_spatial_dim2 *\n            output_spatial_dim3 * output_channel);\n  }\n}\n\ntemplate <typename T, typename P>\ninline void Pad(const tflite::PadParams& op_params,\n                const RuntimeShape& input_shape, const T* input_data,\n                const P* pad_value_ptr, const RuntimeShape& output_shape,\n                T* output_data) {\n  PadImpl(op_params, input_shape, input_data, pad_value_ptr, output_shape,\n          output_data);\n}\n\n// The second (pad-value) input can be int32 when, say, the first is uint8.\ntemplate <typename T>\ninline void Pad(const tflite::PadParams& op_params,\n                const RuntimeShape& input_shape, const T* input_data,\n                const int32* pad_value_ptr, const RuntimeShape& output_shape,\n                T* output_data) {\n  const T converted_pad_value = static_cast<T>(*pad_value_ptr);\n  PadImpl(op_params, input_shape, input_data, &converted_pad_value,\n          output_shape, output_data);\n}\n\n// This version avoids conflicting template matching.\ntemplate <>\ninline void Pad(const tflite::PadParams& op_params,\n                const RuntimeShape& input_shape, const int32* input_data,\n                const int32* pad_value_ptr, const RuntimeShape& output_shape,\n                int32* output_data) {\n  PadImpl(op_params, input_shape, input_data, pad_value_ptr, output_shape,\n          output_data);\n}\n\n// TODO(b/117643175): Optimize. (This is an introductory copy of standard Pad.)\n//\n// This pad requires that (a) left and right paddings are in the 4D patterns\n// {0, h_pad, w_pad, 0}, and (b) memset can be used: *pad_value_ptr == 0 and/or\n// T is uint8.\n//\n// There are two versions of pad: Pad and PadV2.  In PadV2 there is a second\n// scalar input that provides the padding value.  Therefore pad_value_ptr can be\n// equivalent to a simple input1_data.  For Pad, it should point to a zero\n// value.\n//\n// Note that two typenames are required, so that T=P=int32 is considered a\n// specialization distinct from P=int32.\ntemplate <typename T, typename P>\ninline void PadImageStyleMemset(const tflite::PadParams& op_params,\n                                const RuntimeShape& input_shape,\n                                const T* input_data, const P* pad_value_ptr,\n                                const RuntimeShape& output_shape,\n                                T* output_data) {\n  ruy::profiler::ScopeLabel label(\"PadImageStyle\");\n  const RuntimeShape ext_input_shape =\n      RuntimeShape::ExtendedShape(4, input_shape);\n  const RuntimeShape ext_output_shape =\n      RuntimeShape::ExtendedShape(4, output_shape);\n  TFLITE_DCHECK_LE(op_params.left_padding_count, 4);\n  TFLITE_DCHECK_LE(op_params.right_padding_count, 4);\n\n  // Pad kernels are limited to max 4 dimensions. Copy inputs so we can pad them\n  // to 4 dims (yes, we are \"padding the padding\").\n  std::vector<int> left_padding_copy(4, 0);\n  const int left_padding_extend = 4 - op_params.left_padding_count;\n  for (int i = 0; i < op_params.left_padding_count; ++i) {\n    left_padding_copy[left_padding_extend + i] = op_params.left_padding[i];\n  }\n  std::vector<int> right_padding_copy(4, 0);\n  const int right_padding_extend = 4 - op_params.right_padding_count;\n  for (int i = 0; i < op_params.right_padding_count; ++i) {\n    right_padding_copy[right_padding_extend + i] = op_params.right_padding[i];\n  }\n  // The following padding restrictions are contractual requirements, and\n  // embody what it means for a padding op to be \"image-style\".\n  TFLITE_DCHECK_EQ(left_padding_copy[0], 0);\n  TFLITE_DCHECK_EQ(left_padding_copy[3], 0);\n  TFLITE_DCHECK_EQ(right_padding_copy[0], 0);\n  TFLITE_DCHECK_EQ(right_padding_copy[3], 0);\n\n  const int batch = MatchingDim(ext_input_shape, 0, ext_output_shape, 0);\n  const int output_height = ext_output_shape.Dims(1);\n  const int output_width = ext_output_shape.Dims(2);\n  const int input_height = ext_input_shape.Dims(1);\n  const int input_width = ext_input_shape.Dims(2);\n  const int depth = MatchingDim(ext_input_shape, 3, ext_output_shape, 3);\n\n  const int left_h_padding = left_padding_copy[1];\n  const int left_w_padding = left_padding_copy[2];\n  const int right_h_padding = right_padding_copy[1];\n  const int right_w_padding = right_padding_copy[2];\n\n  TFLITE_DCHECK_EQ(output_height,\n                   input_height + left_h_padding + right_h_padding);\n  TFLITE_DCHECK_EQ(output_width,\n                   input_width + left_w_padding + right_w_padding);\n\n  const T pad_value = *pad_value_ptr;\n  const int top_block_size = left_h_padding * output_width * depth;\n  const size_t num_top_block_bytes = top_block_size * sizeof(T);\n  const int bottom_block_size = right_h_padding * output_width * depth;\n  const size_t num_bottom_block_bytes = bottom_block_size * sizeof(T);\n  const int left_blocks_size = left_w_padding * depth;\n  const size_t num_left_block_bytes = left_blocks_size * sizeof(T);\n  const int right_blocks_size = right_w_padding * depth;\n  const size_t num_right_block_bytes = right_blocks_size * sizeof(T);\n  const int inner_line_size = input_width * depth;\n  const size_t num_inner_line_bytes = inner_line_size * sizeof(T);\n\n  if (input_height == 0) {\n    memset(output_data, pad_value,\n           num_top_block_bytes + num_bottom_block_bytes);\n  } else {\n    for (int i = 0; i < batch; ++i) {\n      // For each image in the batch, apply the top padding, then iterate\n      // through rows, then apply the bottom padding.\n      //\n      // By unwinding one iteration, we can combine the first left-margin\n      // padding with the top padding, and the last right-margin padding with\n      // the bottom padding.\n      memset(output_data, pad_value,\n             num_top_block_bytes + num_left_block_bytes);\n      output_data += top_block_size + left_blocks_size;\n      memcpy(output_data, input_data, num_inner_line_bytes);\n      input_data += inner_line_size;\n      output_data += inner_line_size;\n      // One iteration unwound.\n      // Unwinding this loop affords the opportunity to reorder the loop work\n      // and hence combine memset() calls.\n      //\n      // Before unwinding:\n      // for (int j = 0; j < input_height; ++j) {\n      //   // Pad on left, copy central data, pad on right.\n      //   memset(output_data, pad_value, num_left_block_bytes);\n      //   output_data += left_blocks_size;\n      //   memcpy(output_data, input_data, num_inner_line_bytes);\n      //   input_data += inner_line_size;\n      //   output_data += inner_line_size;\n      //   memset(output_data, pad_value, num_right_block_bytes);\n      //   output_data += right_blocks_size;\n      // }\n      for (int j = 1; j < input_height; ++j) {\n        memset(output_data, pad_value,\n               num_right_block_bytes + num_left_block_bytes);\n        output_data += right_blocks_size + left_blocks_size;\n        memcpy(output_data, input_data, num_inner_line_bytes);\n        input_data += inner_line_size;\n        output_data += inner_line_size;\n      }\n      memset(output_data, pad_value,\n             num_right_block_bytes + num_bottom_block_bytes);\n      output_data += right_blocks_size + bottom_block_size;\n    }\n  }\n}\n\ntemplate <typename T, typename P>\ninline void PadImageStyle(const tflite::PadParams& op_params,\n                          const RuntimeShape& input_shape, const T* input_data,\n                          const P* pad_value_ptr,\n                          const RuntimeShape& output_shape, T* output_data) {\n  reference_ops::PadImageStyle(op_params, input_shape, input_data,\n                               pad_value_ptr, output_shape, output_data);\n}\n\ntemplate <typename P>\ninline void PadImageStyle(const tflite::PadParams& op_params,\n                          const RuntimeShape& input_shape,\n                          const uint8* input_data, const P* pad_value_ptr,\n                          const RuntimeShape& output_shape,\n                          uint8* output_data) {\n  PadImageStyleMemset(op_params, input_shape, input_data, pad_value_ptr,\n                      output_shape, output_data);\n}\n\ntemplate <typename P>\ninline void PadImageStyle(const tflite::PadParams& op_params,\n                          const RuntimeShape& input_shape,\n                          const float* input_data, const P* pad_value_ptr,\n                          const RuntimeShape& output_shape,\n                          float* output_data) {\n  const float converted_pad_value = static_cast<float>(*pad_value_ptr);\n  if (converted_pad_value == 0.0f) {\n    PadImageStyleMemset(op_params, input_shape, input_data, pad_value_ptr,\n                        output_shape, output_data);\n  } else {\n    PadImpl(op_params, input_shape, input_data, pad_value_ptr, output_shape,\n            output_data);\n  }\n}\n\ntemplate <typename T>\ninline void Slice(const tflite::SliceParams& op_params,\n                  const RuntimeShape& input_shape,\n                  const RuntimeShape& output_shape,\n                  SequentialTensorWriter<T>* writer) {\n  ruy::profiler::ScopeLabel label(\"Slice\");\n  const RuntimeShape ext_shape = RuntimeShape::ExtendedShape(5, input_shape);\n  TFLITE_DCHECK_LE(op_params.begin_count, 5);\n  TFLITE_DCHECK_LE(op_params.size_count, 5);\n  const int begin_count = op_params.begin_count;\n  const int size_count = op_params.size_count;\n  // We front-pad the begin and size vectors.\n  std::array<int, 5> start;\n  std::array<int, 5> stop;\n  for (int i = 0; i < 5; ++i) {\n    int padded_i = 5 - i;\n    start[i] =\n        begin_count < padded_i ? 0 : op_params.begin[begin_count - padded_i];\n    stop[i] =\n        (size_count < padded_i || op_params.size[size_count - padded_i] == -1)\n            ? ext_shape.Dims(i)\n            : start[i] + op_params.size[size_count - padded_i];\n  }\n\n  for (int i0 = start[0]; i0 < stop[0]; ++i0) {\n    for (int i1 = start[1]; i1 < stop[1]; ++i1) {\n      for (int i2 = start[2]; i2 < stop[2]; ++i2) {\n        for (int i3 = start[3]; i3 < stop[3]; ++i3) {\n          const int len = stop[4] - start[4];\n          if (len > 0)\n            writer->WriteN(Offset(ext_shape, i0, i1, i2, i3, start[4]), len);\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename T>\ninline void Slice(const tflite::SliceParams& op_params,\n                  const RuntimeShape& input_shape, const T* input_data,\n                  const RuntimeShape& output_shape, T* output_data) {\n  SequentialTensorWriter<T> writer(input_data, output_data);\n  return Slice(op_params, input_shape, output_shape, &writer);\n}\n\ntemplate <typename T>\ninline void Slice(const tflite::SliceParams& op_params,\n                  const RuntimeShape& input_shape, const TfLiteTensor* input,\n                  const RuntimeShape& output_shape, TfLiteTensor* output) {\n  SequentialTensorWriter<T> writer(input, output);\n  return Slice(op_params, input_shape, output_shape, &writer);\n}\n\n// Note: This implementation is only optimized for the case where the inner\n// stride == 1.\ntemplate <typename T>\ninline void StridedSlice(const tflite::StridedSliceParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const RuntimeShape& unextended_output_shape,\n                         SequentialTensorWriter<T>* writer) {\n  using strided_slice::LoopCondition;\n  using strided_slice::StartForAxis;\n  using strided_slice::StopForAxis;\n\n  ruy::profiler::ScopeLabel label(\"StridedSlice\");\n\n  // Note that the output_shape is not used herein.\n  tflite::StridedSliceParams params_copy = op_params;\n\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 5);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(5, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(5, unextended_output_shape);\n\n  // Reverse and pad to 5 dimensions because that is what the runtime code\n  // requires (ie. all shapes must be 5D and are given backwards).\n  strided_slice::StridedSlicePadIndices(&params_copy, 5);\n\n  const int start_0 = StartForAxis(params_copy, input_shape, 0);\n  const int stop_0 = StopForAxis(params_copy, input_shape, 0, start_0);\n  const int start_1 = StartForAxis(params_copy, input_shape, 1);\n  const int stop_1 = StopForAxis(params_copy, input_shape, 1, start_1);\n  const int start_2 = StartForAxis(params_copy, input_shape, 2);\n  const int stop_2 = StopForAxis(params_copy, input_shape, 2, start_2);\n  const int start_3 = StartForAxis(params_copy, input_shape, 3);\n  const int stop_3 = StopForAxis(params_copy, input_shape, 3, start_3);\n  const int start_4 = StartForAxis(params_copy, input_shape, 4);\n  const int stop_4 = StopForAxis(params_copy, input_shape, 4, start_4);\n  const bool inner_stride_is_1 = params_copy.strides[4] == 1;\n\n  for (int offset_0 = start_0 * input_shape.Dims(1),\n           end_0 = stop_0 * input_shape.Dims(1),\n           step_0 = params_copy.strides[0] * input_shape.Dims(1);\n       !LoopCondition(offset_0, end_0, params_copy.strides[0]);\n       offset_0 += step_0) {\n    for (int offset_1 = (offset_0 + start_1) * input_shape.Dims(2),\n             end_1 = (offset_0 + stop_1) * input_shape.Dims(2),\n             step_1 = params_copy.strides[1] * input_shape.Dims(2);\n         !LoopCondition(offset_1, end_1, params_copy.strides[1]);\n         offset_1 += step_1) {\n      for (int offset_2 = (offset_1 + start_2) * input_shape.Dims(3),\n               end_2 = (offset_1 + stop_2) * input_shape.Dims(3),\n               step_2 = params_copy.strides[2] * input_shape.Dims(3);\n           !LoopCondition(offset_2, end_2, params_copy.strides[2]);\n           offset_2 += step_2) {\n        for (int offset_3 = (offset_2 + start_3) * input_shape.Dims(4),\n                 end_3 = (offset_2 + stop_3) * input_shape.Dims(4),\n                 step_3 = params_copy.strides[3] * input_shape.Dims(4);\n             !LoopCondition(offset_3, end_3, params_copy.strides[3]);\n             offset_3 += step_3) {\n          // When the stride is 1, the inner loop is equivalent to the\n          // optimized slice inner loop. Otherwise, it is identical to the\n          // strided_slice reference implementation inner loop.\n          if (inner_stride_is_1) {\n            const int len = stop_4 - start_4;\n            if (len > 0) {\n              writer->WriteN(offset_3 + start_4, len);\n            }\n          } else {\n            for (int offset_4 = offset_3 + start_4, end_4 = offset_3 + stop_4;\n                 !LoopCondition(offset_4, end_4, params_copy.strides[4]);\n                 offset_4 += params_copy.strides[4]) {\n              writer->Write(offset_4);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename T>\ninline void StridedSlice(const tflite::StridedSliceParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const T* input_data,\n                         const RuntimeShape& unextended_output_shape,\n                         T* output_data) {\n  SequentialTensorWriter<T> writer(input_data, output_data);\n  StridedSlice<T>(op_params, unextended_input_shape, unextended_output_shape,\n                  &writer);\n}\n\ntemplate <typename T>\ninline void StridedSlice(const tflite::StridedSliceParams& op_params,\n                         const RuntimeShape& unextended_input_shape,\n                         const TfLiteTensor* input,\n                         const RuntimeShape& unextended_output_shape,\n                         TfLiteTensor* output) {\n  SequentialTensorWriter<T> writer(input, output);\n  StridedSlice<T>(op_params, unextended_input_shape, unextended_output_shape,\n                  &writer);\n}\n\ntemplate <typename T>\nvoid Minimum(const RuntimeShape& input1_shape, const T* input1_data,\n             const T* input2_data, const RuntimeShape& output_shape,\n             T* output_data) {\n  ruy::profiler::ScopeLabel label(\"TensorFlowMinimum\");\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  auto min_value = input2_data[0];\n  output_map.array() = input1_map.array().min(min_value);\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T>\ninline void Minimum(const RuntimeShape& input1_shape, const T* input1_data,\n                    const RuntimeShape&, const T* input2_data,\n                    const RuntimeShape& output_shape, T* output_data) {\n  // Drop shape of second input: not needed.\n  Minimum(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename T>\nvoid Maximum(const RuntimeShape& input1_shape, const T* input1_data,\n             const T* input2_data, const RuntimeShape& output_shape,\n             T* output_data) {\n  ruy::profiler::ScopeLabel label(\"TensorFlowMaximum\");\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  auto max_value = input2_data[0];\n  output_map.array() = input1_map.array().max(max_value);\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T>\ninline void Maximum(const RuntimeShape& input1_shape, const T* input1_data,\n                    const RuntimeShape&, const T* input2_data,\n                    const RuntimeShape& output_shape, T* output_data) {\n  // Drop shape of second input: not needed.\n  Maximum(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename T>\nvoid TransposeIm2col(const ConvParams& params, uint8 zero_byte,\n                     const RuntimeShape& input_shape, const T* input_data,\n                     const RuntimeShape& filter_shape,\n                     const RuntimeShape& output_shape, T* im2col_data) {\n  ruy::profiler::ScopeLabel label(\"TransposeIm2col\");\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK(im2col_data);\n\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  MatchingDim(output_shape, 3, filter_shape, 0);  // output_depth\n\n  // Construct the MxN sized im2col matrix.\n  // The rows M, are sub-ordered B x H x W\n  const RuntimeShape row_shape({1, batches, output_height, output_width});\n  // The columns, N, are sub-ordered Kh x Kw x Din\n  const RuntimeShape col_shape({1, filter_height, filter_width, input_depth});\n  // Use dimensions M and N to construct dims for indexing directly into im2col\n  const RuntimeShape im2col_shape(\n      {1, 1, row_shape.FlatSize(), col_shape.FlatSize()});\n\n  // Build the im2col matrix by looping through all the input pixels,\n  // computing their influence on the output, rather than looping through all\n  // the output pixels. We therefore must initialize the im2col array to zero.\n  // This is potentially inefficient because we subsequently overwrite bytes\n  // set here. However, in practice memset is very fast and costs negligible.\n  memset(im2col_data, zero_byte, im2col_shape.FlatSize() * sizeof(T));\n\n  // Loop through the output batches\n  for (int batch = 0; batch < batches; ++batch) {\n    // Loop through input pixels one at a time.\n    for (int in_y = 0; in_y < input_height; ++in_y) {\n      for (int in_x = 0; in_x < input_width; ++in_x) {\n        // Loop through the output pixels it will influence\n        const int out_x_origin = (in_x * stride_width) - pad_width;\n        const int out_y_origin = (in_y * stride_height) - pad_height;\n        for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n          const int out_y = out_y_origin + filter_y;\n          // Is output pixel within height bounds?\n          if ((out_y >= 0) && (out_y < output_height)) {\n            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n              const int out_x = out_x_origin + filter_x;\n              // Is output pixel within width bounds?\n              if ((out_x >= 0) && (out_x < output_width)) {\n                // Copy the input elements of this pixel\n                T const* src =\n                    input_data + Offset(input_shape, batch, in_y, in_x, 0);\n                int row_offset = Offset(row_shape, 0, batch, out_y, out_x);\n                int col_offset = Offset(col_shape, 0, filter_y, filter_x, 0);\n                T* dst = im2col_data +\n                         Offset(im2col_shape, 0, 0, row_offset, col_offset);\n                memcpy(dst, src, input_depth * sizeof(T));\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n// Returns in 'im_data' (assumes to be zero-initialized) image patch in storage\n// order (height, width, depth), constructed from patches in 'col_data', which\n// is required to be in storage order (out_height * out_width, filter_height,\n// filter_width, in_depth).  Implementation by Yangqing Jia (jiayq).\n// Copied from //tensorflow/core/kernels/conv_grad_input_ops.cc\ntemplate <typename T>\nvoid Col2im(const T* col_data, const int depth, const int height,\n            const int width, const int filter_h, const int filter_w,\n            const int pad_t, const int pad_l, const int pad_b, const int pad_r,\n            const int stride_h, const int stride_w, T* im_data) {\n  ruy::profiler::ScopeLabel label(\"Col2im\");\n  int height_col = (height + pad_t + pad_b - filter_h) / stride_h + 1;\n  int width_col = (width + pad_l + pad_r - filter_w) / stride_w + 1;\n  int h_pad = -pad_t;\n  for (int h = 0; h < height_col; ++h) {\n    int w_pad = -pad_l;\n    for (int w = 0; w < width_col; ++w) {\n      T* im_patch_data = im_data + (h_pad * width + w_pad) * depth;\n      for (int ih = h_pad; ih < h_pad + filter_h; ++ih) {\n        for (int iw = w_pad; iw < w_pad + filter_w; ++iw) {\n          if (ih >= 0 && ih < height && iw >= 0 && iw < width) {\n            // TODO(andydavis) Vectorize this loop (if compiler does not).\n            for (int i = 0; i < depth; ++i) {\n              im_patch_data[i] += col_data[i];\n            }\n          }\n          im_patch_data += depth;\n          col_data += depth;\n        }\n        // Jump over remaining number of depth.\n        im_patch_data += depth * (width - filter_w);\n      }\n      w_pad += stride_w;\n    }\n    h_pad += stride_h;\n  }\n}\n\n// TODO(b/188008864) Optimize this function by combining outer loops.\ntemplate <typename T>\nvoid BiasAdd(T* im_data, const T* bias_data, const int batch_size,\n             const int height, const int width, const int depth) {\n  if (bias_data) {\n    for (int n = 0; n < batch_size; ++n) {\n      for (int h = 0; h < height; ++h) {\n        for (int w = 0; w < width; ++w) {\n          for (int d = 0; d < depth; ++d) {\n            im_data[d] += bias_data[d];\n          }\n          im_data += depth;\n        }\n      }\n    }\n  }\n}\n\n// TransposeConvV2 expect the weights in HWOI order.\ninline void TransposeConvV2(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& hwoi_ordered_filter_shape,\n    const float* hwoi_ordered_filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* const output_data, const RuntimeShape& col2im_shape,\n    float* col2im_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"TransposeConvV2/float\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(hwoi_ordered_filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK(col2im_data);\n  TFLITE_DCHECK(hwoi_ordered_filter_data);\n\n  const int batch_size = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_image_size = input_shape.Dims(1) * input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_image_size = output_height * output_width;\n  const int input_depth =\n      MatchingDim(input_shape, 3, hwoi_ordered_filter_shape, 3);\n  const int output_depth =\n      MatchingDim(output_shape, 3, hwoi_ordered_filter_shape, 2);\n  const int input_offset = input_image_size * input_depth;\n  const int output_offset = output_image_size * output_depth;\n\n  const int filter_height = hwoi_ordered_filter_shape.Dims(0);\n  const int filter_width = hwoi_ordered_filter_shape.Dims(1);\n  const int padding_top = params.padding_values.height;\n  const int padding_bottom =\n      params.padding_values.height + params.padding_values.height_offset;\n  const int padding_left = params.padding_values.width;\n  const int padding_right =\n      params.padding_values.width + params.padding_values.width_offset;\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  const int hwoi_ordered_filter_total_size =\n      filter_height * filter_width * output_depth;\n\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = hwoi_ordered_filter_total_size;\n  lhs_params.cols = input_depth;\n  float* output_data_p = output_data;\n  std::fill_n(output_data, output_offset * batch_size, 0.0f);\n  for (int i = 0; i < batch_size; ++i) {\n    cpu_backend_gemm::MatrixParams<float> rhs_params;\n    rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n    rhs_params.rows = input_depth;\n    rhs_params.cols = input_image_size;\n    cpu_backend_gemm::MatrixParams<float> dst_params;\n    dst_params.order = cpu_backend_gemm::Order::kColMajor;\n    dst_params.rows = hwoi_ordered_filter_total_size;\n    dst_params.cols = input_image_size;\n    cpu_backend_gemm::GemmParams<float, float> gemm_params;\n    cpu_backend_gemm::Gemm(lhs_params, hwoi_ordered_filter_data, rhs_params,\n                           input_data + input_offset * i, dst_params,\n                           col2im_data, gemm_params, cpu_backend_context);\n\n    Col2im(col2im_data, output_depth, output_height, output_width,\n           filter_height, filter_width, padding_top, padding_left,\n           padding_bottom, padding_right, stride_height, stride_width,\n           output_data_p);\n    output_data_p += output_offset;\n  }\n  output_data_p = output_data;\n  BiasAdd(output_data_p, bias_data, batch_size, output_height, output_width,\n          output_depth);\n}\n\ninline void Quantize(int32_t multiplier, int32_t shift, int32_t total_size,\n                     int32_t output_zp, int32_t* scratch, uint8_t* output) {\n  ruy::profiler::ScopeLabel label(\"Quantize/uint8\");\n  int i = 0;\n  const int32_t output_min = std::numeric_limits<uint8_t>::min();\n  const int32_t output_max = std::numeric_limits<uint8_t>::max();\n\n#ifdef USE_NEON\n  const int32x4_t output_zp_dup = vdupq_n_s32(output_zp);\n  const int32x4_t max_val_dup = vdupq_n_s32(output_max);\n  const int32x4_t min_val_dup = vdupq_n_s32(output_min);\n\n  using gemmlowp::RoundingDivideByPOT;\n  using gemmlowp::SaturatingRoundingDoublingHighMul;\n\n  for (; i <= total_size - 16; i += 16) {\n    int32x4x4_t scratch_val;\n    scratch_val.val[0] = vld1q_s32(scratch + i);\n    scratch_val.val[1] = vld1q_s32(scratch + i + 4);\n    scratch_val.val[2] = vld1q_s32(scratch + i + 8);\n    scratch_val.val[3] = vld1q_s32(scratch + i + 12);\n\n    int32x4x4_t temp_val =\n        MultiplyByQuantizedMultiplier4Rows(scratch_val, multiplier, shift);\n\n    temp_val.val[0] = vaddq_s32(temp_val.val[0], output_zp_dup);\n    temp_val.val[1] = vaddq_s32(temp_val.val[1], output_zp_dup);\n    temp_val.val[2] = vaddq_s32(temp_val.val[2], output_zp_dup);\n    temp_val.val[3] = vaddq_s32(temp_val.val[3], output_zp_dup);\n\n    temp_val.val[0] =\n        vmaxq_s32(vminq_s32(temp_val.val[0], max_val_dup), min_val_dup);\n    temp_val.val[1] =\n        vmaxq_s32(vminq_s32(temp_val.val[1], max_val_dup), min_val_dup);\n    temp_val.val[2] =\n        vmaxq_s32(vminq_s32(temp_val.val[2], max_val_dup), min_val_dup);\n    temp_val.val[3] =\n        vmaxq_s32(vminq_s32(temp_val.val[3], max_val_dup), min_val_dup);\n\n    const uint16x8_t result_1 =\n        vcombine_u16(vqmovn_u32(vreinterpretq_u32_s32(temp_val.val[0])),\n                     vqmovn_u32(vreinterpretq_u32_s32(temp_val.val[1])));\n    const uint16x8_t result_2 =\n        vcombine_u16(vqmovn_u32(vreinterpretq_u32_s32(temp_val.val[2])),\n                     vqmovn_u32(vreinterpretq_u32_s32(temp_val.val[3])));\n    const uint8x16_t result =\n        vcombine_u8(vqmovn_u16(result_1), vqmovn_u16(result_2));\n    vst1q_u8(output + i, result);\n  }\n#endif\n  for (; i < total_size; ++i) {\n    int32_t temp = MultiplyByQuantizedMultiplier(scratch[i], multiplier, shift);\n    temp += output_zp;\n    if (temp > output_max) {\n      temp = output_max;\n    }\n    if (temp < output_min) {\n      temp = output_min;\n    }\n    output[i] = static_cast<uint8_t>(temp);\n  }\n}\n\ninline void Quantize(const int32_t* multiplier, const int32_t* shift,\n                     int32_t channel_size, int32_t total_size,\n                     int32_t output_zp, int32_t output_min, int32_t output_max,\n                     int32_t* scratch, int8_t* output) {\n  ruy::profiler::ScopeLabel label(\"Quantize/int8\");\n\n  // Here we're trying to quantize the raw accumulators:\n  //        output_channels\n  //       data data data data data\n  // rows  data data data data data\n  //       data data data data data\n  //          ....\n  //\n  // In order to minimize the reload of the multipliers & shifts, once we load\n  // the multipliers & shifts, we load & quantize the raw accumulators for every\n  // row.\n#ifdef USE_NEON\n  const int32x4_t output_offset_vec = vdupq_n_s32(output_zp);\n  const int32x4_t output_activation_min_vec = vdupq_n_s32(output_min);\n  const int32x4_t output_activation_max_vec = vdupq_n_s32(output_max);\n  const int32x4_t zeros = vdupq_n_s32(0);\n#endif\n\n  TFLITE_DCHECK_EQ(total_size % channel_size, 0);\n  const int32_t rows = total_size / channel_size;\n\n  int c = 0;\n\n#ifdef USE_NEON\n  using gemmlowp::RoundingDivideByPOT;\n  for (; c <= channel_size - 8; c += 8) {\n    int32x4_t out_shift_1 = vld1q_s32(shift + c);\n    int32x4_t out_shift_2 = vld1q_s32(shift + c + 4);\n    int32x4_t left_shift_1 = vmaxq_s32(out_shift_1, zeros);\n    int32x4_t left_shift_2 = vmaxq_s32(out_shift_2, zeros);\n\n    // Right shift will be performed as left shift with negative values.\n    int32x4_t right_shift_1 = vminq_s32(out_shift_1, zeros);\n    int32x4_t right_shift_2 = vminq_s32(out_shift_2, zeros);\n\n    int32x4_t out_mul_1 = vld1q_s32(multiplier + c);\n    int32x4_t out_mul_2 = vld1q_s32(multiplier + c + 4);\n    for (int n = 0; n < rows; ++n) {\n      int loc = n * channel_size + c;\n      int32x4_t acc_1 = vld1q_s32(scratch + loc);\n      int32x4_t acc_2 = vld1q_s32(scratch + loc + 4);\n\n      // Saturating Rounding Doubling High Mul.\n      acc_1 = vshlq_s32(acc_1, left_shift_1);\n      acc_1 = vqrdmulhq_s32(acc_1, out_mul_1);\n      acc_2 = vshlq_s32(acc_2, left_shift_2);\n      acc_2 = vqrdmulhq_s32(acc_2, out_mul_2);\n\n      // Rounding Dividing By POT.\n      acc_1 = vrshlq_s32(acc_1, right_shift_1);\n      acc_2 = vrshlq_s32(acc_2, right_shift_2);\n\n      // Add the output offset.\n      acc_1 = vaddq_s32(acc_1, output_offset_vec);\n      acc_2 = vaddq_s32(acc_2, output_offset_vec);\n\n      // Apply the activation function.\n      acc_1 = vmaxq_s32(acc_1, output_activation_min_vec);\n      acc_1 = vminq_s32(acc_1, output_activation_max_vec);\n      acc_2 = vmaxq_s32(acc_2, output_activation_min_vec);\n      acc_2 = vminq_s32(acc_2, output_activation_max_vec);\n\n      // Saturating cast to int8 and store to destination.\n      const int16x4_t acc_s16_1 = vqmovn_s32(acc_1);\n      const int16x4_t acc_s16_2 = vqmovn_s32(acc_2);\n      const int16x8_t res_s16 = vcombine_s16(acc_s16_1, acc_s16_2);\n      const int8x8_t res_s8 = vqmovn_s16(res_s16);\n      vst1_s8(output + loc, res_s8);\n    }\n  }\n\n#endif  // USE_NEON\n  // Handle leftover values, one by one. This is very slow.\n  for (; c < channel_size; c++) {\n    for (int n = 0; n < rows; ++n) {\n      int loc = n * channel_size + c;\n      int32 acc = scratch[loc];\n      acc = MultiplyByQuantizedMultiplier(acc, multiplier[c], shift[c]);\n      acc += output_zp;\n      acc = std::max(acc, output_min);\n      acc = std::min(acc, output_max);\n      output[loc] = static_cast<int8>(acc);\n    }\n  }\n}\n\n// TransposeConvV2 expect the weights in HWOI order.\ninline void TransposeConvV2(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const uint8_t* input_data, const RuntimeShape& hwoi_ordered_filter_shape,\n    const uint8_t* hwoi_ordered_filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8_t* output_data, const RuntimeShape& col2im_shape,\n    int32_t* col2im_data, int32_t* scratch_data,\n    CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"TransposeConvV2/uint8\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(hwoi_ordered_filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK(col2im_data);\n  TFLITE_DCHECK(hwoi_ordered_filter_data);\n\n  const int batch_size = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_image_size = input_shape.Dims(1) * input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_image_size = output_height * output_width;\n  const int input_depth =\n      MatchingDim(input_shape, 3, hwoi_ordered_filter_shape, 3);\n  const int output_depth =\n      MatchingDim(output_shape, 3, hwoi_ordered_filter_shape, 2);\n  const int input_offset = input_image_size * input_depth;\n  const int output_offset = output_image_size * output_depth;\n\n  const int filter_height = hwoi_ordered_filter_shape.Dims(0);\n  const int filter_width = hwoi_ordered_filter_shape.Dims(1);\n  const int padding_top = params.padding_values.height;\n  const int padding_bottom =\n      params.padding_values.height + params.padding_values.height_offset;\n  const int padding_left = params.padding_values.width;\n  const int padding_right =\n      params.padding_values.width + params.padding_values.width_offset;\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n\n  const int hwoi_ordered_filter_total_size =\n      filter_height * filter_width * output_depth;\n\n  cpu_backend_gemm::MatrixParams<uint8_t> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = hwoi_ordered_filter_total_size;\n  lhs_params.cols = input_depth;\n  lhs_params.zero_point = -params.weights_offset;\n\n  int32_t* scratch_data_p = scratch_data;\n  std::fill_n(scratch_data, output_offset * batch_size, static_cast<int32>(0));\n  for (int i = 0; i < batch_size; ++i) {\n    cpu_backend_gemm::MatrixParams<uint8_t> rhs_params;\n    rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n    rhs_params.rows = input_depth;\n    rhs_params.cols = input_image_size;\n    rhs_params.zero_point = -params.input_offset;\n\n    cpu_backend_gemm::MatrixParams<int32_t> dst_params;\n    dst_params.order = cpu_backend_gemm::Order::kColMajor;\n    dst_params.rows = hwoi_ordered_filter_total_size;\n    dst_params.cols = input_image_size;\n\n    cpu_backend_gemm::GemmParams<int32_t, int32_t> gemm_params;\n    cpu_backend_gemm::Gemm(lhs_params, hwoi_ordered_filter_data, rhs_params,\n                           input_data + input_offset * i, dst_params,\n                           col2im_data, gemm_params, cpu_backend_context);\n\n    Col2im(col2im_data, output_depth, output_height, output_width,\n           filter_height, filter_width, padding_top, padding_left,\n           padding_bottom, padding_right, stride_height, stride_width,\n           scratch_data_p);\n\n    scratch_data_p += output_offset;\n  }\n  scratch_data_p = scratch_data;\n  BiasAdd(scratch_data_p, bias_data, batch_size, output_height, output_width,\n          output_depth);\n\n  Quantize(params.output_multiplier, params.output_shift,\n           output_shape.FlatSize(), params.output_offset, scratch_data,\n           output_data);\n}\n\n// Integer-only version of ResizeNearestNeighbor. Since scales are represented\n// in fixed-point and thus approximated, |in_x| or |in_y| may differ from the\n// reference version. Debug checks are in place to test if this occurs.\n// NOTE: If align_corners or half_pixel_centers is true, we use the reference\n// version.\ninline void ResizeNearestNeighbor(\n    const tflite::ResizeNearestNeighborParams& op_params,\n    const RuntimeShape& unextended_input_shape, const uint8* input_data,\n    const RuntimeShape& output_size_shape, const int32* output_size_data,\n    const RuntimeShape& unextended_output_shape, uint8* output_data) {\n  if (op_params.align_corners || op_params.half_pixel_centers) {\n    // TODO(b/149823713): Add support for align_corners & half_pixel_centers in\n    // this kernel.\n    reference_ops::ResizeNearestNeighbor(\n        op_params, unextended_input_shape, input_data, output_size_shape,\n        output_size_data, unextended_output_shape, output_data);\n    return;\n  }\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  int32 batches = MatchingDim(input_shape, 0, output_shape, 0);\n  int32 input_height = input_shape.Dims(1);\n  int32 input_width = input_shape.Dims(2);\n  int32 depth = MatchingDim(input_shape, 3, output_shape, 3);\n\n  // The Tensorflow version of this op allows resize on the width and height\n  // axis only.\n  TFLITE_DCHECK_EQ(output_size_shape.FlatSize(), 2);\n  int32 output_height = output_size_data[0];\n  int32 output_width = output_size_data[1];\n\n  // Convert scales to fixed-point with 16 fractional bits. We add 1 as an\n  // error factor and to avoid zero scales. For example, with input_height = 1,\n  // output_height = 3, the float scaling factor would be non-zero at 1/3.\n  // With fixed-point, this is zero.\n  int32 height_scale = (input_height << 16) / output_height + 1;\n  int32 width_scale = (input_width << 16) / output_width + 1;\n\n  const int col_offset = input_shape.Dims(3);\n  const int row_offset = input_shape.Dims(2) * col_offset;\n  const int batch_offset = input_shape.Dims(1) * row_offset;\n\n  const uint8* input_ptr = input_data;\n  uint8* output_ptr = output_data;\n  for (int b = 0; b < batches; ++b) {\n    for (int y = 0; y < output_height; ++y) {\n      int32 in_y = std::min((y * height_scale) >> 16, input_height - 1);\n      // Check offset calculation is the same as the reference version. See\n      // function comment for details. We check using a non-float version of:\n      // TFLITE_DCHECK_EQ(in_y, std::floor(y * (static_cast<float>(input_height)\n      //                                            / output_height)));\n      TFLITE_DCHECK_LT(y * input_height, output_height + in_y * output_height);\n      TFLITE_DCHECK_GE(y * input_height, in_y * output_height);\n      const uint8* y_input_ptr = input_ptr + in_y * row_offset;\n      for (int x = 0; x < output_width; ++x) {\n        int32 in_x = std::min((x * width_scale) >> 16, input_width - 1);\n        // Check offset calculation is the same as the reference version. See\n        // function comment for details. We check using a non-float version of:\n        // TFLITE_DCHECK_EQ(in_y,\n        //                  std::floor(y * (static_cast<float>(input_width)\n        //                                      / output_width)));\n        TFLITE_DCHECK_LT(x * input_width, output_width + in_x * output_width);\n        TFLITE_DCHECK_GE(x * input_width, in_x * output_width);\n        const uint8* x_input_ptr = y_input_ptr + in_x * col_offset;\n        memcpy(output_ptr, x_input_ptr, depth);\n        output_ptr += depth;\n      }\n    }\n    input_ptr += batch_offset;\n  }\n}\n\ntemplate <typename input_type, typename output_type>\ninline void Requantize(const input_type* input_data, int32_t size,\n                       int32_t effective_scale_multiplier,\n                       int32_t effective_scale_shift, int32_t input_zeropoint,\n                       int32_t output_zeropoint, output_type* output_data) {\n  reference_ops::Requantize(input_data, size, effective_scale_multiplier,\n                            effective_scale_shift, input_zeropoint,\n                            output_zeropoint, output_data);\n}\n\ntemplate <>\ninline void Requantize<int8_t, uint8_t>(const int8_t* input_data, int32_t size,\n                                        int32_t effective_scale_multiplier,\n                                        int32_t effective_scale_shift,\n                                        int32_t input_zeropoint,\n                                        int32_t output_zeropoint,\n                                        uint8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Requantize/Int8ToUint8\");\n\n  static constexpr int32_t kMinOutput = std::numeric_limits<uint8_t>::min();\n  static constexpr int32_t kMaxOutput = std::numeric_limits<uint8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  // Constants.\n  const int32x4_t input_zero_point_dup = vdupq_n_s32(-input_zeropoint);\n  const int32x4_t output_zero_point_dup = vdupq_n_s32(output_zeropoint);\n  const int32x4_t min_val_dup = vdupq_n_s32(kMinOutput);\n  const int32x4_t max_val_dup = vdupq_n_s32(kMaxOutput);\n\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input_vec = vld1q_s8(input_data + i);\n    const int16x8_t first_half = vmovl_s8(vget_low_s8(input_vec));\n    const int16x8_t second_half = vmovl_s8(vget_high_s8(input_vec));\n    int32x4x4_t input;\n    input.val[0] = vmovl_s16(vget_low_s16(first_half));\n    input.val[1] = vmovl_s16(vget_high_s16(first_half));\n    input.val[2] = vmovl_s16(vget_low_s16(second_half));\n    input.val[3] = vmovl_s16(vget_high_s16(second_half));\n    input.val[0] = vaddq_s32(input.val[0], input_zero_point_dup);\n    input.val[1] = vaddq_s32(input.val[1], input_zero_point_dup);\n    input.val[2] = vaddq_s32(input.val[2], input_zero_point_dup);\n    input.val[3] = vaddq_s32(input.val[3], input_zero_point_dup);\n\n    int32x4x4_t result = MultiplyByQuantizedMultiplier4Rows(\n        input, effective_scale_multiplier, effective_scale_shift);\n\n    result.val[0] = vaddq_s32(result.val[0], output_zero_point_dup);\n    result.val[1] = vaddq_s32(result.val[1], output_zero_point_dup);\n    result.val[2] = vaddq_s32(result.val[2], output_zero_point_dup);\n    result.val[3] = vaddq_s32(result.val[3], output_zero_point_dup);\n    result.val[0] =\n        vmaxq_s32(vminq_s32(result.val[0], max_val_dup), min_val_dup);\n    result.val[1] =\n        vmaxq_s32(vminq_s32(result.val[1], max_val_dup), min_val_dup);\n    result.val[2] =\n        vmaxq_s32(vminq_s32(result.val[2], max_val_dup), min_val_dup);\n    result.val[3] =\n        vmaxq_s32(vminq_s32(result.val[3], max_val_dup), min_val_dup);\n\n    const uint32x4_t result_val_1_unsigned =\n        vreinterpretq_u32_s32(result.val[0]);\n    const uint32x4_t result_val_2_unsigned =\n        vreinterpretq_u32_s32(result.val[1]);\n    const uint32x4_t result_val_3_unsigned =\n        vreinterpretq_u32_s32(result.val[2]);\n    const uint32x4_t result_val_4_unsigned =\n        vreinterpretq_u32_s32(result.val[3]);\n\n    const uint16x4_t narrowed_val_1 = vqmovn_u32(result_val_1_unsigned);\n    const uint16x4_t narrowed_val_2 = vqmovn_u32(result_val_2_unsigned);\n    const uint16x4_t narrowed_val_3 = vqmovn_u32(result_val_3_unsigned);\n    const uint16x4_t narrowed_val_4 = vqmovn_u32(result_val_4_unsigned);\n    const uint16x8_t output_first_half =\n        vcombine_u16(narrowed_val_1, narrowed_val_2);\n    const uint16x8_t output_second_half =\n        vcombine_u16(narrowed_val_3, narrowed_val_4);\n    const uint8x8_t narrowed_first_half = vqmovn_u16(output_first_half);\n    const uint8x8_t narrowed_second_half = vqmovn_u16(output_second_half);\n    const uint8x16_t narrowed_result =\n        vcombine_u8(narrowed_first_half, narrowed_second_half);\n    vst1q_u8(output_data + i, narrowed_result);\n  }\n\n#endif\n  for (; i < size; ++i) {\n    const int32_t input = input_data[i] - input_zeropoint;\n    const int32_t output =\n        MultiplyByQuantizedMultiplier(input, effective_scale_multiplier,\n                                      effective_scale_shift) +\n        output_zeropoint;\n    const int32_t clamped_output =\n        std::max(std::min(output, kMaxOutput), kMinOutput);\n    output_data[i] = static_cast<uint8_t>(clamped_output);\n  }\n}\n\ntemplate <>\ninline void Requantize<uint8_t, int8_t>(const uint8_t* input_data, int32_t size,\n                                        int32_t effective_scale_multiplier,\n                                        int32_t effective_scale_shift,\n                                        int32_t input_zeropoint,\n                                        int32_t output_zeropoint,\n                                        int8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Requantize/Uint8ToInt8\");\n\n  static constexpr int32_t kMinOutput = std::numeric_limits<int8_t>::min();\n  static constexpr int32_t kMaxOutput = std::numeric_limits<int8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  // Constants.\n  const int32x4_t input_zero_point_dup = vdupq_n_s32(-input_zeropoint);\n  const int32x4_t output_zero_point_dup = vdupq_n_s32(output_zeropoint);\n  const int32x4_t min_val_dup = vdupq_n_s32(kMinOutput);\n  const int32x4_t max_val_dup = vdupq_n_s32(kMaxOutput);\n\n  for (; i <= size - 16; i += 16) {\n    const uint8x16_t input_vec = vld1q_u8(input_data + i);\n    const uint16x8_t first_half = vmovl_u8(vget_low_u8(input_vec));\n    const uint16x8_t second_half = vmovl_u8(vget_high_u8(input_vec));\n    int32x4x4_t input;\n    input.val[0] = vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(first_half)));\n    input.val[1] = vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(first_half)));\n    input.val[2] = vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(second_half)));\n    input.val[3] = vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(second_half)));\n    input.val[0] = vaddq_s32(input.val[0], input_zero_point_dup);\n    input.val[1] = vaddq_s32(input.val[1], input_zero_point_dup);\n    input.val[2] = vaddq_s32(input.val[2], input_zero_point_dup);\n    input.val[3] = vaddq_s32(input.val[3], input_zero_point_dup);\n\n    int32x4x4_t result = MultiplyByQuantizedMultiplier4Rows(\n        input, effective_scale_multiplier, effective_scale_shift);\n\n    result.val[0] = vaddq_s32(result.val[0], output_zero_point_dup);\n    result.val[1] = vaddq_s32(result.val[1], output_zero_point_dup);\n    result.val[2] = vaddq_s32(result.val[2], output_zero_point_dup);\n    result.val[3] = vaddq_s32(result.val[3], output_zero_point_dup);\n    result.val[0] =\n        vmaxq_s32(vminq_s32(result.val[0], max_val_dup), min_val_dup);\n    result.val[1] =\n        vmaxq_s32(vminq_s32(result.val[1], max_val_dup), min_val_dup);\n    result.val[2] =\n        vmaxq_s32(vminq_s32(result.val[2], max_val_dup), min_val_dup);\n    result.val[3] =\n        vmaxq_s32(vminq_s32(result.val[3], max_val_dup), min_val_dup);\n\n    const int16x4_t narrowed_val_1 = vqmovn_s32(result.val[0]);\n    const int16x4_t narrowed_val_2 = vqmovn_s32(result.val[1]);\n    const int16x4_t narrowed_val_3 = vqmovn_s32(result.val[2]);\n    const int16x4_t narrowed_val_4 = vqmovn_s32(result.val[3]);\n    const int16x8_t output_first_half =\n        vcombine_s16(narrowed_val_1, narrowed_val_2);\n    const int16x8_t output_second_half =\n        vcombine_s16(narrowed_val_3, narrowed_val_4);\n    const int8x8_t narrowed_first_half = vqmovn_s16(output_first_half);\n    const int8x8_t narrowed_second_half = vqmovn_s16(output_second_half);\n    const int8x16_t narrowed_result =\n        vcombine_s8(narrowed_first_half, narrowed_second_half);\n    vst1q_s8(output_data + i, narrowed_result);\n  }\n\n#endif\n  for (; i < size; ++i) {\n    const int32_t input = input_data[i] - input_zeropoint;\n    const int32_t output =\n        MultiplyByQuantizedMultiplier(input, effective_scale_multiplier,\n                                      effective_scale_shift) +\n        output_zeropoint;\n    const int32_t clamped_output =\n        std::max(std::min(output, kMaxOutput), kMinOutput);\n    output_data[i] = static_cast<int8_t>(clamped_output);\n  }\n}\n\ntemplate <>\ninline void Requantize<int8_t, int8_t>(const int8_t* input_data, int32_t size,\n                                       int32_t effective_scale_multiplier,\n                                       int32_t effective_scale_shift,\n                                       int32_t input_zeropoint,\n                                       int32_t output_zeropoint,\n                                       int8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Requantize/Int8ToInt8\");\n\n  static constexpr int32_t kMinOutput = std::numeric_limits<int8_t>::min();\n  static constexpr int32_t kMaxOutput = std::numeric_limits<int8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  // Constants.\n  const int32x4_t input_zero_point_dup = vdupq_n_s32(-input_zeropoint);\n  const int32x4_t output_zero_point_dup = vdupq_n_s32(output_zeropoint);\n  const int32x4_t min_val_dup = vdupq_n_s32(kMinOutput);\n  const int32x4_t max_val_dup = vdupq_n_s32(kMaxOutput);\n\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input_vec = vld1q_s8(input_data + i);\n    const int16x8_t first_half = vmovl_s8(vget_low_s8(input_vec));\n    const int16x8_t second_half = vmovl_s8(vget_high_s8(input_vec));\n    int32x4x4_t input;\n    input.val[0] = vmovl_s16(vget_low_s16(first_half));\n    input.val[1] = vmovl_s16(vget_high_s16(first_half));\n    input.val[2] = vmovl_s16(vget_low_s16(second_half));\n    input.val[3] = vmovl_s16(vget_high_s16(second_half));\n\n    input.val[0] = vaddq_s32(input.val[0], input_zero_point_dup);\n    input.val[1] = vaddq_s32(input.val[1], input_zero_point_dup);\n    input.val[2] = vaddq_s32(input.val[2], input_zero_point_dup);\n    input.val[3] = vaddq_s32(input.val[3], input_zero_point_dup);\n\n    int32x4x4_t result = MultiplyByQuantizedMultiplier4Rows(\n        input, effective_scale_multiplier, effective_scale_shift);\n\n    result.val[0] = vaddq_s32(result.val[0], output_zero_point_dup);\n    result.val[1] = vaddq_s32(result.val[1], output_zero_point_dup);\n    result.val[2] = vaddq_s32(result.val[2], output_zero_point_dup);\n    result.val[3] = vaddq_s32(result.val[3], output_zero_point_dup);\n    result.val[0] =\n        vmaxq_s32(vminq_s32(result.val[0], max_val_dup), min_val_dup);\n    result.val[1] =\n        vmaxq_s32(vminq_s32(result.val[1], max_val_dup), min_val_dup);\n    result.val[2] =\n        vmaxq_s32(vminq_s32(result.val[2], max_val_dup), min_val_dup);\n    result.val[3] =\n        vmaxq_s32(vminq_s32(result.val[3], max_val_dup), min_val_dup);\n\n    const int16x4_t narrowed_val_1 = vqmovn_s32(result.val[0]);\n    const int16x4_t narrowed_val_2 = vqmovn_s32(result.val[1]);\n    const int16x4_t narrowed_val_3 = vqmovn_s32(result.val[2]);\n    const int16x4_t narrowed_val_4 = vqmovn_s32(result.val[3]);\n    const int16x8_t output_first_half =\n        vcombine_s16(narrowed_val_1, narrowed_val_2);\n    const int16x8_t output_second_half =\n        vcombine_s16(narrowed_val_3, narrowed_val_4);\n    const int8x8_t narrowed_first_half = vqmovn_s16(output_first_half);\n    const int8x8_t narrowed_second_half = vqmovn_s16(output_second_half);\n    const int8x16_t narrowed_result =\n        vcombine_s8(narrowed_first_half, narrowed_second_half);\n    vst1q_s8(output_data + i, narrowed_result);\n  }\n\n#endif\n  for (; i < size; ++i) {\n    const int32_t input = input_data[i] - input_zeropoint;\n    const int32_t output =\n        MultiplyByQuantizedMultiplier(input, effective_scale_multiplier,\n                                      effective_scale_shift) +\n        output_zeropoint;\n    const int32_t clamped_output =\n        std::max(std::min(output, kMaxOutput), kMinOutput);\n    output_data[i] = static_cast<int8_t>(clamped_output);\n  }\n}\n\ntemplate <>\ninline void Requantize<uint8_t, uint8_t>(\n    const uint8_t* input_data, int32_t size, int32_t effective_scale_multiplier,\n    int32_t effective_scale_shift, int32_t input_zeropoint,\n    int32_t output_zeropoint, uint8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Requantize/Uint8ToUint8\");\n\n  static constexpr int32_t kMinOutput = std::numeric_limits<uint8_t>::min();\n  static constexpr int32_t kMaxOutput = std::numeric_limits<uint8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  // Constants.\n  const int32x4_t input_zero_point_dup = vdupq_n_s32(-input_zeropoint);\n  const int32x4_t output_zero_point_dup = vdupq_n_s32(output_zeropoint);\n  const int32x4_t min_val_dup = vdupq_n_s32(kMinOutput);\n  const int32x4_t max_val_dup = vdupq_n_s32(kMaxOutput);\n\n  for (; i <= size - 16; i += 16) {\n    const uint8x16_t input_vec = vld1q_u8(input_data + i);\n    const uint16x8_t first_half = vmovl_u8(vget_low_u8(input_vec));\n    const uint16x8_t second_half = vmovl_u8(vget_high_u8(input_vec));\n    int32x4x4_t input;\n    input.val[0] = vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(first_half)));\n    input.val[1] = vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(first_half)));\n    input.val[2] = vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(second_half)));\n    input.val[3] = vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(second_half)));\n    input.val[0] = vaddq_s32(input.val[0], input_zero_point_dup);\n    input.val[1] = vaddq_s32(input.val[1], input_zero_point_dup);\n    input.val[2] = vaddq_s32(input.val[2], input_zero_point_dup);\n    input.val[3] = vaddq_s32(input.val[3], input_zero_point_dup);\n\n    int32x4x4_t result = MultiplyByQuantizedMultiplier4Rows(\n        input, effective_scale_multiplier, effective_scale_shift);\n\n    result.val[0] = vaddq_s32(result.val[0], output_zero_point_dup);\n    result.val[1] = vaddq_s32(result.val[1], output_zero_point_dup);\n    result.val[2] = vaddq_s32(result.val[2], output_zero_point_dup);\n    result.val[3] = vaddq_s32(result.val[3], output_zero_point_dup);\n    result.val[0] =\n        vmaxq_s32(vminq_s32(result.val[0], max_val_dup), min_val_dup);\n    result.val[1] =\n        vmaxq_s32(vminq_s32(result.val[1], max_val_dup), min_val_dup);\n    result.val[2] =\n        vmaxq_s32(vminq_s32(result.val[2], max_val_dup), min_val_dup);\n    result.val[3] =\n        vmaxq_s32(vminq_s32(result.val[3], max_val_dup), min_val_dup);\n\n    const uint32x4_t result_val_1_unsigned =\n        vreinterpretq_u32_s32(result.val[0]);\n    const uint32x4_t result_val_2_unsigned =\n        vreinterpretq_u32_s32(result.val[1]);\n    const uint32x4_t result_val_3_unsigned =\n        vreinterpretq_u32_s32(result.val[2]);\n    const uint32x4_t result_val_4_unsigned =\n        vreinterpretq_u32_s32(result.val[3]);\n\n    const uint16x4_t narrowed_val_1 = vqmovn_u32(result_val_1_unsigned);\n    const uint16x4_t narrowed_val_2 = vqmovn_u32(result_val_2_unsigned);\n    const uint16x4_t narrowed_val_3 = vqmovn_u32(result_val_3_unsigned);\n    const uint16x4_t narrowed_val_4 = vqmovn_u32(result_val_4_unsigned);\n    const uint16x8_t output_first_half =\n        vcombine_u16(narrowed_val_1, narrowed_val_2);\n    const uint16x8_t output_second_half =\n        vcombine_u16(narrowed_val_3, narrowed_val_4);\n    const uint8x8_t narrowed_first_half = vqmovn_u16(output_first_half);\n    const uint8x8_t narrowed_second_half = vqmovn_u16(output_second_half);\n    const uint8x16_t narrowed_result =\n        vcombine_u8(narrowed_first_half, narrowed_second_half);\n    vst1q_u8(output_data + i, narrowed_result);\n  }\n\n#endif\n  for (; i < size; ++i) {\n    const int32_t input = input_data[i] - input_zeropoint;\n    const int32_t output =\n        MultiplyByQuantizedMultiplier(input, effective_scale_multiplier,\n                                      effective_scale_shift) +\n        output_zeropoint;\n    const int32_t clamped_output =\n        std::max(std::min(output, kMaxOutput), kMinOutput);\n    output_data[i] = static_cast<uint8_t>(clamped_output);\n  }\n}\n\ninline void HardSwish(const RuntimeShape& input_shape, const float* input_data,\n                      const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"HardSwish/Float\");\n  auto size = MatchingFlatSize(input_shape, output_shape);\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t zero = vdupq_n_f32(0.0f);\n  const float32x4_t three = vdupq_n_f32(3.0f);\n  const float32x4_t six = vdupq_n_f32(6.0f);\n  const float32x4_t one_sixth = vdupq_n_f32(1.0f / 6.0f);\n\n  for (; i <= size - 16; i += 16) {\n    // 4x partially unrolled version of the loop below. Refer to its comments.\n    const float32x4_t in_0 = vld1q_f32(input_data + i + 0);\n    const float32x4_t in_1 = vld1q_f32(input_data + i + 4);\n    const float32x4_t in_2 = vld1q_f32(input_data + i + 8);\n    const float32x4_t in_3 = vld1q_f32(input_data + i + 12);\n    const float32x4_t in_scaled_0 = vmulq_f32(in_0, one_sixth);\n    const float32x4_t in_scaled_1 = vmulq_f32(in_1, one_sixth);\n    const float32x4_t in_scaled_2 = vmulq_f32(in_2, one_sixth);\n    const float32x4_t in_scaled_3 = vmulq_f32(in_3, one_sixth);\n    const float32x4_t in_reluish_0 =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in_0, three)));\n    const float32x4_t in_reluish_1 =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in_1, three)));\n    const float32x4_t in_reluish_2 =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in_2, three)));\n    const float32x4_t in_reluish_3 =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in_3, three)));\n    const float32x4_t product_0 = vmulq_f32(in_scaled_0, in_reluish_0);\n    const float32x4_t product_1 = vmulq_f32(in_scaled_1, in_reluish_1);\n    const float32x4_t product_2 = vmulq_f32(in_scaled_2, in_reluish_2);\n    const float32x4_t product_3 = vmulq_f32(in_scaled_3, in_reluish_3);\n    vst1q_f32(output_data + i + 0, product_0);\n    vst1q_f32(output_data + i + 4, product_1);\n    vst1q_f32(output_data + i + 8, product_2);\n    vst1q_f32(output_data + i + 12, product_3);\n  }\n  for (; i <= size - 4; i += 4) {\n    // The expression to be computed is:\n    //   out = one_sixth * in * min(six, max(zero, (in + three)))\n    // We structure the AST to have two roughly balanced, independent branches:\n    //  - Multiplication: in_scaled = one_sixth * in.\n    //  - Addition and clamping: in_reluish = min(six, max(zero, (in + three))).\n    // Then the remaining multiplication at the root of the tree.\n    const float32x4_t in = vld1q_f32(input_data + i);\n    const float32x4_t in_scaled = vmulq_f32(in, one_sixth);\n    const float32x4_t in_reluish =\n        vminq_f32(six, vmaxq_f32(zero, vaddq_f32(in, three)));\n    const float32x4_t product = vmulq_f32(in_scaled, in_reluish);\n    vst1q_f32(output_data + i, product);\n  }\n#endif\n  for (; i < size; i++) {\n    const float in = input_data[i];\n    output_data[i] =\n        in * std::min(6.0f, std::max(0.0f, in + 3.0f)) * (1.0f / 6.0f);\n  }\n}\n\n#ifdef USE_NEON\ninline void SaturateAndStore(int16x8_t src, std::uint8_t* dst) {\n  // Narrow values down to 8 bit unsigned, saturating.\n  uint8x8_t res8 = vqmovun_s16(src);\n  // Store results to destination.\n  vst1_u8(dst, res8);\n}\n\ninline void SaturateAndStore(int16x8_t src, std::int8_t* dst) {\n  // Narrow values down to 8 bit unsigned, saturating.\n  int8x8_t res8 = vqmovn_s16(src);\n  // Store results to destination.\n  vst1_s8(dst, res8);\n}\n#endif\n\ntemplate <typename T>\ninline void HardSwish(const HardSwishParams& params,\n                      const RuntimeShape& input_shape, const T* input_data,\n                      const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"HardSwish/Quantized\");\n\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int i = 0;\n  // This code heavily uses NEON saturating left shifts (vqshl*) with shift\n  // amounts that can be zero, in which case we rely on the correct behavior\n  // of a left shift by zero returning just its first operand unmodified.\n  // Unfortunately, the Intel arm_neon_sse.h implementation of vqshl* is\n  // buggy in the case of zero shift amounts, see b/137199585. That is why\n  // this NEON code path is restricted to true ARM NEON, excluding\n  // arm_neon_sse.h. Anyway, the arm_neon_sse.h implementation of saturating\n  // left shifts is slow scalar code, so there may not be much benefit in\n  // running that over just plain reference code.\n  //\n  // TODO(b/137199585): revisit when this is fixed.\n#ifdef __ARM_NEON\n  const int16x8_t positive_reluish_multiplier_exponent_minus_one =\n      vdupq_n_s16(std::max(0, params.reluish_multiplier_exponent - 1));\n  const int16x8_t positive_reluish_multiplier_exponent_last_bit =\n      vdupq_n_s16(params.reluish_multiplier_exponent > 0 ? 1 : 0);\n  const int16x8_t negative_reluish_multiplier_exponent =\n      vdupq_n_s16(std::min(0, params.reluish_multiplier_exponent));\n  const int16x8_t constant_32767 = vdupq_n_s16(32767);\n  const int16x8_t output_multiplier_exponent =\n      vdupq_n_s16(params.output_multiplier_exponent);\n  const int16x8_t output_zero_point = vdupq_n_s16(params.output_zero_point);\n  // 4x unrolled version of the below NEON loop. Read that first.\n  for (; i <= flat_size - 32; i += 32) {\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_value_0_1 =\n        Load16AndSubtractZeroPoint(input_data + i, params.input_zero_point);\n    const int16x8x2_t input_value_2_3 = Load16AndSubtractZeroPoint(\n        input_data + i + 16, params.input_zero_point);\n    const int16x8_t input_value_on_hires_input_scale_0 =\n        vshlq_n_s16(input_value_0_1.val[0], 7);\n    const int16x8_t input_value_on_hires_input_scale_1 =\n        vshlq_n_s16(input_value_0_1.val[1], 7);\n    const int16x8_t input_value_on_hires_input_scale_2 =\n        vshlq_n_s16(input_value_2_3.val[0], 7);\n    const int16x8_t input_value_on_hires_input_scale_3 =\n        vshlq_n_s16(input_value_2_3.val[1], 7);\n    const int16x8_t input_value_on_preshift_output_scale_0 =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale_0,\n                        params.output_multiplier_fixedpoint_int16);\n    const int16x8_t input_value_on_preshift_output_scale_1 =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale_1,\n                        params.output_multiplier_fixedpoint_int16);\n    const int16x8_t input_value_on_preshift_output_scale_2 =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale_2,\n                        params.output_multiplier_fixedpoint_int16);\n    const int16x8_t input_value_on_preshift_output_scale_3 =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale_3,\n                        params.output_multiplier_fixedpoint_int16);\n    int16x8_t reluish_value_0 = input_value_on_hires_input_scale_0;\n    int16x8_t reluish_value_1 = input_value_on_hires_input_scale_1;\n    int16x8_t reluish_value_2 = input_value_on_hires_input_scale_2;\n    int16x8_t reluish_value_3 = input_value_on_hires_input_scale_3;\n    reluish_value_0 = vqshlq_s16(\n        reluish_value_0, positive_reluish_multiplier_exponent_minus_one);\n    reluish_value_1 = vqshlq_s16(\n        reluish_value_1, positive_reluish_multiplier_exponent_minus_one);\n    reluish_value_2 = vqshlq_s16(\n        reluish_value_2, positive_reluish_multiplier_exponent_minus_one);\n    reluish_value_3 = vqshlq_s16(\n        reluish_value_3, positive_reluish_multiplier_exponent_minus_one);\n    reluish_value_0 = vqrdmulhq_n_s16(\n        reluish_value_0, params.reluish_multiplier_fixedpoint_int16);\n    reluish_value_1 = vqrdmulhq_n_s16(\n        reluish_value_1, params.reluish_multiplier_fixedpoint_int16);\n    reluish_value_2 = vqrdmulhq_n_s16(\n        reluish_value_2, params.reluish_multiplier_fixedpoint_int16);\n    reluish_value_3 = vqrdmulhq_n_s16(\n        reluish_value_3, params.reluish_multiplier_fixedpoint_int16);\n    reluish_value_0 = vqshlq_s16(reluish_value_0,\n                                 positive_reluish_multiplier_exponent_last_bit);\n    reluish_value_1 = vqshlq_s16(reluish_value_1,\n                                 positive_reluish_multiplier_exponent_last_bit);\n    reluish_value_2 = vqshlq_s16(reluish_value_2,\n                                 positive_reluish_multiplier_exponent_last_bit);\n    reluish_value_3 = vqshlq_s16(reluish_value_3,\n                                 positive_reluish_multiplier_exponent_last_bit);\n    reluish_value_0 =\n        vrshlq_s16(reluish_value_0, negative_reluish_multiplier_exponent);\n    reluish_value_1 =\n        vrshlq_s16(reluish_value_1, negative_reluish_multiplier_exponent);\n    reluish_value_2 =\n        vrshlq_s16(reluish_value_2, negative_reluish_multiplier_exponent);\n    reluish_value_3 =\n        vrshlq_s16(reluish_value_3, negative_reluish_multiplier_exponent);\n    reluish_value_0 = vrhaddq_s16(reluish_value_0, constant_32767);\n    reluish_value_1 = vrhaddq_s16(reluish_value_1, constant_32767);\n    reluish_value_2 = vrhaddq_s16(reluish_value_2, constant_32767);\n    reluish_value_3 = vrhaddq_s16(reluish_value_3, constant_32767);\n    const int16x8_t preshift_output_value_0 =\n        vqdmulhq_s16(reluish_value_0, input_value_on_preshift_output_scale_0);\n    const int16x8_t preshift_output_value_1 =\n        vqdmulhq_s16(reluish_value_1, input_value_on_preshift_output_scale_1);\n    const int16x8_t preshift_output_value_2 =\n        vqdmulhq_s16(reluish_value_2, input_value_on_preshift_output_scale_2);\n    const int16x8_t preshift_output_value_3 =\n        vqdmulhq_s16(reluish_value_3, input_value_on_preshift_output_scale_3);\n    int16x8_t output_value_0 =\n        vrshlq_s16(preshift_output_value_0, output_multiplier_exponent);\n    int16x8_t output_value_1 =\n        vrshlq_s16(preshift_output_value_1, output_multiplier_exponent);\n    int16x8_t output_value_2 =\n        vrshlq_s16(preshift_output_value_2, output_multiplier_exponent);\n    int16x8_t output_value_3 =\n        vrshlq_s16(preshift_output_value_3, output_multiplier_exponent);\n    output_value_0 = vaddq_s16(output_value_0, output_zero_point);\n    output_value_1 = vaddq_s16(output_value_1, output_zero_point);\n    output_value_2 = vaddq_s16(output_value_2, output_zero_point);\n    output_value_3 = vaddq_s16(output_value_3, output_zero_point);\n    SaturateAndStore(output_value_0, output_data + i);\n    SaturateAndStore(output_value_1, output_data + i + 8);\n    SaturateAndStore(output_value_2, output_data + i + 16);\n    SaturateAndStore(output_value_3, output_data + i + 24);\n  }\n  // NEON version of reference_ops::HardSwish. Read that first.\n  for (; i <= flat_size - 8; i += 8) {\n    using cpu_backend_gemm::detail::Load8AndSubtractZeroPoint;\n    const int16x8_t input_value =\n        Load8AndSubtractZeroPoint(input_data + i, params.input_zero_point);\n    const int16x8_t input_value_on_hires_input_scale =\n        vshlq_n_s16(input_value, 7);\n    const int16x8_t input_value_on_preshift_output_scale =\n        vqrdmulhq_n_s16(input_value_on_hires_input_scale,\n                        params.output_multiplier_fixedpoint_int16);\n    int16x8_t reluish_value = input_value_on_hires_input_scale;\n    reluish_value = vqshlq_s16(reluish_value,\n                               positive_reluish_multiplier_exponent_minus_one);\n    reluish_value = vqrdmulhq_n_s16(reluish_value,\n                                    params.reluish_multiplier_fixedpoint_int16);\n    reluish_value = vqshlq_s16(reluish_value,\n                               positive_reluish_multiplier_exponent_last_bit);\n    reluish_value =\n        vrshlq_s16(reluish_value, negative_reluish_multiplier_exponent);\n    reluish_value = vrhaddq_s16(reluish_value, constant_32767);\n    const int16x8_t preshift_output_value =\n        vqdmulhq_s16(reluish_value, input_value_on_preshift_output_scale);\n    int16x8_t output_value =\n        vrshlq_s16(preshift_output_value, output_multiplier_exponent);\n    output_value = vaddq_s16(output_value, output_zero_point);\n    SaturateAndStore(output_value, output_data + i);\n  }\n#endif\n  // TODO(b/137208495): revisit when unit tests cover reference code.\n  // Fall back to reference_ops::HardSwish. In general we have preferred\n  // to duplicate such scalar code rather than call reference code to handle\n  // leftovers, thinking that code duplication was not a big concern.\n  // However, most of our unit tests happen to test only optimized code,\n  // and the quantized HardSwish implementation is nontrivial enough that\n  // I really want test coverage for the reference code.\n  if (i < flat_size) {\n    const RuntimeShape leftover_shape{flat_size - i};\n    reference_ops::HardSwish(params, leftover_shape, input_data + i,\n                             leftover_shape, output_data + i);\n  }\n}\n\ntemplate <typename T>\ninline void IntegerExponentPow(const ArithmeticParams& params,\n                               const RuntimeShape& unextended_base_shape,\n                               const T* base_data, const int exponent,\n                               const RuntimeShape& unextended_output_shape,\n                               T* output_data) {\n  TFLITE_DCHECK_GE(exponent, 1);\n  if (exponent == 1) {\n    // copy data over.\n    std::memcpy(output_data, base_data,\n                unextended_base_shape.FlatSize() * sizeof(T));\n  } else {\n    IntegerExponentPow(params, unextended_base_shape, base_data, exponent / 2,\n                       unextended_output_shape, output_data);\n    Mul(params, unextended_base_shape, output_data, unextended_base_shape,\n        output_data, unextended_output_shape, output_data);\n    if (exponent % 2 == 1) {\n      Mul(params, unextended_base_shape, base_data, unextended_base_shape,\n          output_data, unextended_output_shape, output_data);\n    }\n  }\n}\n\ntemplate <typename T>\ninline void BroadcastPow4D(const RuntimeShape& unextended_input1_shape,\n                           const T* input1_data,\n                           const RuntimeShape& unextended_input2_shape,\n                           const T* input2_data,\n                           const RuntimeShape& unextended_output_shape,\n                           T* output_data) {\n  ruy::profiler::ScopeLabel label(\"PowBroadcast\");\n\n  if (unextended_input2_shape.FlatSize() == 1) {\n    static const float epsilon = 1e-5;\n    const T exponent = input2_data[0];\n    const int int_exponent = static_cast<int>(std::round(exponent));\n    if ((std::abs(input2_data[0] - int_exponent) < epsilon) &&\n        (int_exponent >= 1)) {\n      ArithmeticParams params;\n      if (std::is_same<T, float>::value) {\n        params.float_activation_max = std::numeric_limits<float>::max();\n        params.float_activation_min = std::numeric_limits<float>::lowest();\n      } else if (std::is_same<T, int>::value) {\n        params.quantized_activation_max = std::numeric_limits<int>::max();\n        params.quantized_activation_min = std::numeric_limits<int>::lowest();\n      }\n      IntegerExponentPow(params, unextended_input1_shape, input1_data,\n                         int_exponent, unextended_output_shape, output_data);\n      return;\n    }\n  }\n  reference_ops::BroadcastPow4DSlow(unextended_input1_shape, input1_data,\n                                    unextended_input2_shape, input2_data,\n                                    unextended_output_shape, output_data);\n}\n\n#ifdef USE_NEON\n\ninline void ScaleWithNewZeroPoint(const int32x4_t input,\n                                  const float32x4_t scale_dup,\n                                  const float32x4_t zero_times_scale_dup,\n                                  float32x4_t* output) {\n#ifdef __ARM_FEATURE_FMA\n  *output = vfmaq_f32(zero_times_scale_dup, vcvtq_f32_s32(input), scale_dup);\n#else\n  *output = vaddq_f32(vmulq_f32(vcvtq_f32_s32(input), scale_dup),\n                      zero_times_scale_dup);\n#endif\n}\n\n#endif  // USE_NEON\n\ninline void Dequantize(const tflite::DequantizationParams& op_params,\n                       const RuntimeShape& input_shape,\n                       const uint8_t* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Dequantize/Uint8\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = op_params.scale;\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t scale_dup = vdupq_n_f32(static_cast<float>(scale));\n  const float32x4_t zero_times_scale_dup =\n      vdupq_n_f32(static_cast<float>(-zero_point * scale));\n  for (; i <= flat_size - 8; i += 8) {\n    const uint8x8_t input_u8 = vld1_u8(input_data + i);\n    const uint16x8_t input_u16 = vmovl_u8(input_u8);\n    const int16x8_t input_s16 = vreinterpretq_s16_u16(input_u16);\n    const int16x4_t input_s16_low = vget_low_s16(input_s16);\n    const int16x4_t input_s16_high = vget_high_s16(input_s16);\n    const int32x4_t val_low = vmovl_s16(input_s16_low);\n    const int32x4_t val_high = vmovl_s16(input_s16_high);\n\n    float32x4_t result_low, result_high;\n    ScaleWithNewZeroPoint(val_low, scale_dup, zero_times_scale_dup,\n                          &result_low);\n    ScaleWithNewZeroPoint(val_high, scale_dup, zero_times_scale_dup,\n                          &result_high);\n\n    vst1q_f32(output_data + i, result_low);\n    vst1q_f32(output_data + i + 4, result_high);\n  }\n#endif  // NEON\n  for (; i < flat_size; ++i) {\n    const int32 val = input_data[i];\n    const float result = static_cast<float>(scale * (val - zero_point));\n    output_data[i] = result;\n  }\n}\n\ninline void Dequantize(const tflite::DequantizationParams& op_params,\n                       const RuntimeShape& input_shape,\n                       const int8_t* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Dequantize/Int8\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = op_params.scale;\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t scale_dup = vdupq_n_f32(static_cast<float>(scale));\n  const float32x4_t zero_times_scale_dup =\n      vdupq_n_f32(static_cast<float>(-zero_point * scale));\n  for (; i <= flat_size - 8; i += 8) {\n    const int8x8_t input_s8 = vld1_s8(input_data + i);\n    const int16x8_t input_s16 = vmovl_s8(input_s8);\n    const int16x4_t input_s16_low = vget_low_s16(input_s16);\n    const int16x4_t input_s16_high = vget_high_s16(input_s16);\n    const int32x4_t val_low = vmovl_s16(input_s16_low);\n    const int32x4_t val_high = vmovl_s16(input_s16_high);\n\n    float32x4_t result_low, result_high;\n    ScaleWithNewZeroPoint(val_low, scale_dup, zero_times_scale_dup,\n                          &result_low);\n    ScaleWithNewZeroPoint(val_high, scale_dup, zero_times_scale_dup,\n                          &result_high);\n\n    vst1q_f32(output_data + i, result_low);\n    vst1q_f32(output_data + i + 4, result_high);\n  }\n#endif  // NEON\n  for (; i < flat_size; ++i) {\n    const int32 val = input_data[i];\n    const float result = static_cast<float>(scale * (val - zero_point));\n    output_data[i] = result;\n  }\n}\n\ninline void Dequantize(const tflite::DequantizationParams& op_params,\n                       const RuntimeShape& input_shape,\n                       const int16_t* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Dequantize/Int16\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = op_params.scale;\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t scale_dup = vdupq_n_f32(static_cast<float>(scale));\n  const float32x4_t zero_times_scale_dup =\n      vdupq_n_f32(static_cast<float>(-zero_point * scale));\n  for (; i <= flat_size - 8; i += 8) {\n    const int16x4_t input_s16_low = vld1_s16(input_data + i);\n    const int16x4_t input_s16_high = vld1_s16(input_data + i + 4);\n    const int32x4_t val_low = vmovl_s16(input_s16_low);\n    const int32x4_t val_high = vmovl_s16(input_s16_high);\n\n    float32x4_t result_low, result_high;\n    ScaleWithNewZeroPoint(val_low, scale_dup, zero_times_scale_dup,\n                          &result_low);\n    ScaleWithNewZeroPoint(val_high, scale_dup, zero_times_scale_dup,\n                          &result_high);\n\n    vst1q_f32(output_data + i, result_low);\n    vst1q_f32(output_data + i + 4, result_high);\n  }\n#endif  // NEON\n  for (; i < flat_size; ++i) {\n    const int32 val = input_data[i];\n    const float result = static_cast<float>(scale * (val - zero_point));\n    output_data[i] = result;\n  }\n}\n\ninline void Dequantize(const RuntimeShape& input_shape,\n                       const Eigen::half* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  reference_ops::Dequantize(input_shape, input_data, output_shape, output_data);\n}\n\ntemplate <typename T>\ninline void AffineQuantize(const tflite::QuantizationParams& op_params,\n                           const RuntimeShape& input_shape,\n                           const float* input_data,\n                           const RuntimeShape& output_shape, T* output_data) {\n  reference_ops::AffineQuantize(op_params, input_shape, input_data,\n                                output_shape, output_data);\n}\n\ntemplate <>\ninline void AffineQuantize(const tflite::QuantizationParams& op_params,\n                           const RuntimeShape& input_shape,\n                           const float* input_data,\n                           const RuntimeShape& output_shape,\n                           int8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantize/Int8\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = static_cast<double>(op_params.scale);\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  static constexpr int32 min_val = std::numeric_limits<int8_t>::min();\n  static constexpr int32 max_val = std::numeric_limits<int8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t reverse_scale_dup = vdupq_n_f32(1.0f / scale);\n  const int32x4_t zero_point_dup = vdupq_n_s32(zero_point);\n  const int32x4_t min_val_dup = vdupq_n_s32(min_val);\n  const int32x4_t max_val_dup = vdupq_n_s32(max_val);\n\n  for (; i <= flat_size - 8; i += 8) {\n    const float* src_data_ptr = input_data + i;\n    float32x4_t input_val_0 = vld1q_f32(src_data_ptr);\n    float32x4_t input_val_1 = vld1q_f32(src_data_ptr + 4);\n\n    input_val_0 = vmulq_f32(input_val_0, reverse_scale_dup);\n    input_val_1 = vmulq_f32(input_val_1, reverse_scale_dup);\n\n    int32x4_t casted_val_0 = RoundToNearest(input_val_0);\n    int32x4_t casted_val_1 = RoundToNearest(input_val_1);\n\n    casted_val_0 = vaddq_s32(casted_val_0, zero_point_dup);\n    casted_val_1 = vaddq_s32(casted_val_1, zero_point_dup);\n\n    // Clamp the values to fit the target type's range.\n    casted_val_0 = vmaxq_s32(casted_val_0, min_val_dup);\n    casted_val_1 = vmaxq_s32(casted_val_1, min_val_dup);\n    casted_val_0 = vminq_s32(casted_val_0, max_val_dup);\n    casted_val_1 = vminq_s32(casted_val_1, max_val_dup);\n\n    const int16x4_t narrowed_val_0 = vmovn_s32(casted_val_0);\n    const int16x4_t narrowed_val_1 = vmovn_s32(casted_val_1);\n    const int16x8_t combined_val = vcombine_s16(narrowed_val_0, narrowed_val_1);\n    const int8x8_t combined_val_narrowed = vmovn_s16(combined_val);\n    vst1_s8(output_data + i, combined_val_narrowed);\n  }\n#endif  // NEON\n\n  for (; i < flat_size; ++i) {\n    const float val = input_data[i];\n    const int32 unclamped =\n        static_cast<int32>(TfLiteRound(val / scale)) + zero_point;\n    const int32 clamped = std::min(std::max(unclamped, min_val), max_val);\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <>\ninline void AffineQuantize(const tflite::QuantizationParams& op_params,\n                           const RuntimeShape& input_shape,\n                           const float* input_data,\n                           const RuntimeShape& output_shape,\n                           uint8_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantize/Uint8\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = static_cast<double>(op_params.scale);\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  static constexpr int32 min_val = std::numeric_limits<uint8_t>::min();\n  static constexpr int32 max_val = std::numeric_limits<uint8_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t reverse_scale_dup = vdupq_n_f32(1.0f / scale);\n  const int32x4_t zero_point_dup = vdupq_n_s32(zero_point);\n  const int32x4_t min_val_dup = vdupq_n_s32(min_val);\n  const int32x4_t max_val_dup = vdupq_n_s32(max_val);\n\n  for (; i <= flat_size - 8; i += 8) {\n    const float* src_data_ptr = input_data + i;\n    float32x4_t input_val_0 = vld1q_f32(src_data_ptr);\n    float32x4_t input_val_1 = vld1q_f32(src_data_ptr + 4);\n\n    input_val_0 = vmulq_f32(input_val_0, reverse_scale_dup);\n    input_val_1 = vmulq_f32(input_val_1, reverse_scale_dup);\n\n    int32x4_t casted_val_0 = RoundToNearest(input_val_0);\n    int32x4_t casted_val_1 = RoundToNearest(input_val_1);\n\n    casted_val_0 = vaddq_s32(casted_val_0, zero_point_dup);\n    casted_val_1 = vaddq_s32(casted_val_1, zero_point_dup);\n\n    // Clamp the values to fit the target type's range.\n    casted_val_0 = vmaxq_s32(casted_val_0, min_val_dup);\n    casted_val_1 = vmaxq_s32(casted_val_1, min_val_dup);\n    casted_val_0 = vminq_s32(casted_val_0, max_val_dup);\n    casted_val_1 = vminq_s32(casted_val_1, max_val_dup);\n\n    const uint16x4_t narrowed_val_0 = vqmovun_s32(casted_val_0);\n    const uint16x4_t narrowed_val_1 = vqmovun_s32(casted_val_1);\n    const uint16x8_t combined_val =\n        vcombine_u16(narrowed_val_0, narrowed_val_1);\n    const uint8x8_t combined_val_narrowed = vmovn_u16(combined_val);\n    vst1_u8(output_data + i, combined_val_narrowed);\n  }\n#endif  // NEON\n\n  for (; i < flat_size; ++i) {\n    const float val = input_data[i];\n    const int32 unclamped =\n        static_cast<int32>(TfLiteRound(val / scale)) + zero_point;\n    const int32 clamped = std::min(std::max(unclamped, min_val), max_val);\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <>\ninline void AffineQuantize(const tflite::QuantizationParams& op_params,\n                           const RuntimeShape& input_shape,\n                           const float* input_data,\n                           const RuntimeShape& output_shape,\n                           int16_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantize/Int16\");\n  const int32 zero_point = op_params.zero_point;\n  const double scale = static_cast<double>(op_params.scale);\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  static constexpr int32 min_val = std::numeric_limits<int16_t>::min();\n  static constexpr int32 max_val = std::numeric_limits<int16_t>::max();\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t reverse_scale_dup = vdupq_n_f32(1.0f / scale);\n  const int32x4_t zero_point_dup = vdupq_n_s32(zero_point);\n  const int32x4_t min_val_dup = vdupq_n_s32(min_val);\n  const int32x4_t max_val_dup = vdupq_n_s32(max_val);\n\n  for (; i <= flat_size - 8; i += 8) {\n    const float* src_data_ptr = input_data + i;\n    float32x4_t input_val_0 = vld1q_f32(src_data_ptr);\n    float32x4_t input_val_1 = vld1q_f32(src_data_ptr + 4);\n\n    input_val_0 = vmulq_f32(input_val_0, reverse_scale_dup);\n    input_val_1 = vmulq_f32(input_val_1, reverse_scale_dup);\n\n    int32x4_t casted_val_0 = RoundToNearest(input_val_0);\n    int32x4_t casted_val_1 = RoundToNearest(input_val_1);\n\n    casted_val_0 = vaddq_s32(casted_val_0, zero_point_dup);\n    casted_val_1 = vaddq_s32(casted_val_1, zero_point_dup);\n\n    // Clamp the values to fit the target type's range.\n    casted_val_0 = vmaxq_s32(casted_val_0, min_val_dup);\n    casted_val_1 = vmaxq_s32(casted_val_1, min_val_dup);\n    casted_val_0 = vminq_s32(casted_val_0, max_val_dup);\n    casted_val_1 = vminq_s32(casted_val_1, max_val_dup);\n\n    const int16x4_t narrowed_val_0 = vmovn_s32(casted_val_0);\n    const int16x4_t narrowed_val_1 = vmovn_s32(casted_val_1);\n    vst1_s16(output_data + i, narrowed_val_0);\n    vst1_s16(output_data + i + 4, narrowed_val_1);\n  }\n#endif  // NEON\n\n  for (; i < flat_size; ++i) {\n    const float val = input_data[i];\n    const int32 unclamped =\n        static_cast<int32>(TfLiteRound(val / scale)) + zero_point;\n    const int32 clamped = std::min(std::max(unclamped, min_val), max_val);\n    output_data[i] = clamped;\n  }\n}\n\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n\ninline int16x8x4_t SaturatingRounding(\n    int16x8_t input_val_0, int16x8_t input_val_1, int16x8_t input_val_2,\n    int16x8_t input_val_3, int input_left_shift, int input_multiplier) {\n  // This performs what is expressed in the scalar code as\n  // const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n  //      static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n  //      static_cast<int16>(input_multiplier));\n  const int16x8_t left_shift_dup = vdupq_n_s16(input_left_shift);\n  const int16x8_t input_val_shifted_0 = vshlq_s16(input_val_0, left_shift_dup);\n  const int16x8_t input_val_shifted_1 = vshlq_s16(input_val_1, left_shift_dup);\n  const int16x8_t input_val_shifted_2 = vshlq_s16(input_val_2, left_shift_dup);\n  const int16x8_t input_val_shifted_3 = vshlq_s16(input_val_3, left_shift_dup);\n  int16x8x4_t result;\n  result.val[0] = vqrdmulhq_n_s16(input_val_shifted_0, input_multiplier);\n  result.val[1] = vqrdmulhq_n_s16(input_val_shifted_1, input_multiplier);\n  result.val[2] = vqrdmulhq_n_s16(input_val_shifted_2, input_multiplier);\n  result.val[3] = vqrdmulhq_n_s16(input_val_shifted_3, input_multiplier);\n  return result;\n}\n\n// 4-bit fixed point is enough for tanh since tanh(16) is almost same with one,\n// considering 7 digits under zero.\ninline int16x8x4_t FixedPoint4Logistic(int16x8x4_t input_val) {\n  // Invoke gemmlowp::logistic on FixedPoint wrapping int16x8_t\n  using FixedPoint4 = gemmlowp::FixedPoint<int16x8_t, 4>;\n  using FixedPoint0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n  const FixedPoint4 input_val_f4_0 = FixedPoint4::FromRaw(input_val.val[0]);\n  const FixedPoint4 input_val_f4_1 = FixedPoint4::FromRaw(input_val.val[1]);\n  const FixedPoint4 input_val_f4_2 = FixedPoint4::FromRaw(input_val.val[2]);\n  const FixedPoint4 input_val_f4_3 = FixedPoint4::FromRaw(input_val.val[3]);\n\n  // TODO(b/134622898) Implement a low accuracy version of logistic. In this\n  // method, gemmlowp::tanh spends about 80% of the execution times. The\n  // current implementation is rougly 12-bit accurate in the 16-bit fixed\n  // point case. Until reaching to error bounds, there are rooms for\n  // improvements.\n  const FixedPoint0 output_val_f0_0 = gemmlowp::logistic(input_val_f4_0);\n  const FixedPoint0 output_val_f0_1 = gemmlowp::logistic(input_val_f4_1);\n  const FixedPoint0 output_val_f0_2 = gemmlowp::logistic(input_val_f4_2);\n  const FixedPoint0 output_val_f0_3 = gemmlowp::logistic(input_val_f4_3);\n\n  // Divide by 2^7 as in the scalar code\n  int16x8x4_t result;\n  result.val[0] = vrshrq_n_s16(output_val_f0_0.raw(), 7);\n  result.val[1] = vrshrq_n_s16(output_val_f0_1.raw(), 7);\n  result.val[2] = vrshrq_n_s16(output_val_f0_2.raw(), 7);\n  result.val[3] = vrshrq_n_s16(output_val_f0_3.raw(), 7);\n  return result;\n}\n\n// 4-bit fixed point is enough for tanh since tanh(16) is almost same with one,\n// considering 11 digits under zero at least.\ninline int16x8x4_t FixedPoint4Tanh(int16x8x4_t input_val) {\n  // Invoke gemmlowp::logistic on FixedPoint wrapping int16x8_t\n  using FixedPoint4 = gemmlowp::FixedPoint<int16x8_t, 4>;\n  using FixedPoint0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n  const FixedPoint4 input_val_f4_0 = FixedPoint4::FromRaw(input_val.val[0]);\n  const FixedPoint4 input_val_f4_1 = FixedPoint4::FromRaw(input_val.val[1]);\n  const FixedPoint4 input_val_f4_2 = FixedPoint4::FromRaw(input_val.val[2]);\n  const FixedPoint4 input_val_f4_3 = FixedPoint4::FromRaw(input_val.val[3]);\n\n  // TODO(b/134622898) Implement a low accuracy version of logistic. In this\n  // method, gemmlowp::tanh spends about 80% of the execution times. The\n  // current implementation is rougly 12-bit accurate in the 16-bit fixed\n  // point case. Until reaching to error bounds, there are rooms for\n  // improvements.\n  const FixedPoint0 output_val_f0_0 = gemmlowp::tanh(input_val_f4_0);\n  const FixedPoint0 output_val_f0_1 = gemmlowp::tanh(input_val_f4_1);\n  const FixedPoint0 output_val_f0_2 = gemmlowp::tanh(input_val_f4_2);\n  const FixedPoint0 output_val_f0_3 = gemmlowp::tanh(input_val_f4_3);\n\n  // Divide by 2^7 as in the scalar code\n  int16x8x4_t result;\n  result.val[0] = vrshrq_n_s16(output_val_f0_0.raw(), 8);\n  result.val[1] = vrshrq_n_s16(output_val_f0_1.raw(), 8);\n  result.val[2] = vrshrq_n_s16(output_val_f0_2.raw(), 8);\n  result.val[3] = vrshrq_n_s16(output_val_f0_3.raw(), 8);\n  return result;\n}\n\ninline uint8x16x2_t CalculateUnsignedClampingWithRangeBitMasks(\n    int16x8x2_t input_val, int16x8_t range_radius_dup,\n    int16x8_t neg_range_radius_dup) {\n  const uint16x8_t mask_rightclamp_0 =\n      vcgtq_s16(input_val.val[0], range_radius_dup);\n  const uint16x8_t mask_rightclamp_1 =\n      vcgtq_s16(input_val.val[1], range_radius_dup);\n\n  const uint16x8_t mask_leftclamp_0 =\n      vcgeq_s16(input_val.val[0], neg_range_radius_dup);\n  const uint16x8_t mask_leftclamp_1 =\n      vcgeq_s16(input_val.val[1], neg_range_radius_dup);\n\n  uint8x16x2_t result;\n  result.val[0] = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                              vshrn_n_u16(mask_leftclamp_1, 8));\n  result.val[1] = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                              vshrn_n_u16(mask_rightclamp_1, 8));\n  return result;\n}\n\ninline uint8x16x2_t CalculateSignedClampingWithRangeBitMasks(\n    int16x8x2_t input_val, int16x8_t range_radius_dup,\n    int16x8_t neg_range_radius_dup) {\n  const uint16x8_t mask_rightclamp_0 =\n      vcgtq_s16(input_val.val[0], range_radius_dup);\n  const uint16x8_t mask_rightclamp_1 =\n      vcgtq_s16(input_val.val[1], range_radius_dup);\n\n  const uint16x8_t mask_leftclamp_0 =\n      vcltq_s16(input_val.val[0], neg_range_radius_dup);\n  const uint16x8_t mask_leftclamp_1 =\n      vcltq_s16(input_val.val[1], neg_range_radius_dup);\n\n  uint8x16x2_t result;\n  result.val[0] = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                              vshrn_n_u16(mask_leftclamp_1, 8));\n  result.val[1] = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                              vshrn_n_u16(mask_rightclamp_1, 8));\n  return result;\n}\n\ninline void ClampWithRangeAndStore(uint8_t* output_dst, uint8x16_t input_val,\n                                   uint8x16x2_t masks_clamp) {\n  // Store back to memory\n  vst1q_u8(output_dst, vandq_u8(vorrq_u8(input_val, masks_clamp.val[1]),\n                                masks_clamp.val[0]));\n}\n\ninline void ClampWithRangeAndStore(int8_t* output_dst, int8x16_t input_val,\n                                   uint8x16x2_t masks_clamp) {\n  static const int8x16_t max_dup = vdupq_n_s8(127);\n  static const int8x16_t min_dup = vdupq_n_s8(-128);\n  // Store back to memory\n  vst1q_s8(output_dst,\n           vbslq_s8(masks_clamp.val[1], max_dup,\n                    vbslq_s8(masks_clamp.val[0], min_dup, input_val)));\n}\n\n#endif  // GEMMLOWP_NEON\n\ninline void Tanh16bitPrecision(const TanhParams& params,\n                               const RuntimeShape& input_shape,\n                               const uint8* input_data,\n                               const RuntimeShape& output_shape,\n                               uint8* output_data) {\n  // Note that this is almost the exact same code as in Logistic().\n  ruy::profiler::ScopeLabel label(\"Tanh/Uint8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int16 input_multiplier = static_cast<int16>(params.input_multiplier);\n  const int16 input_left_shift = static_cast<int16>(params.input_left_shift);\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n  int16_t output_zero_point = 128;\n\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n  const int16x8_t range_radius_dup = vdupq_n_s16(input_range_radius);\n  const int16x8_t neg_range_radius_dup = vdupq_n_s16(-input_range_radius);\n  const int16x8_t output_zero_point_s16 = vdupq_n_s16(output_zero_point);\n\n  // Handle 32 values at a time\n  for (; c <= size - 32; c += 32) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_val_centered_0_1 =\n        Load16AndSubtractZeroPoint(input_data + c, input_zero_point);\n    const int16x8x2_t input_val_centered_2_3 =\n        Load16AndSubtractZeroPoint(input_data + c + 16, input_zero_point);\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint8x16x2_t masks_clamp_0_1 = CalculateUnsignedClampingWithRangeBitMasks(\n        input_val_centered_0_1, range_radius_dup, neg_range_radius_dup);\n    uint8x16x2_t masks_clamp_2_3 = CalculateUnsignedClampingWithRangeBitMasks(\n        input_val_centered_2_3, range_radius_dup, neg_range_radius_dup);\n\n    int16x8x4_t input_val_rescaled = SaturatingRounding(\n        input_val_centered_0_1.val[0], input_val_centered_0_1.val[1],\n        input_val_centered_2_3.val[0], input_val_centered_2_3.val[1],\n        input_left_shift, input_multiplier);\n\n    int16x8x4_t output_val_s16 = FixedPoint4Tanh(input_val_rescaled);\n\n    // Add the output zero point\n    output_val_s16.val[0] =\n        vaddq_s16(output_val_s16.val[0], output_zero_point_s16);\n    output_val_s16.val[1] =\n        vaddq_s16(output_val_s16.val[1], output_zero_point_s16);\n    output_val_s16.val[2] =\n        vaddq_s16(output_val_s16.val[2], output_zero_point_s16);\n    output_val_s16.val[3] =\n        vaddq_s16(output_val_s16.val[3], output_zero_point_s16);\n\n    // Cast output values to uint8, saturating\n    uint8x16_t output_val_u8_0_1 = vcombine_u8(\n        vqmovun_s16(output_val_s16.val[0]), vqmovun_s16(output_val_s16.val[1]));\n    uint8x16_t output_val_u8_2_3 = vcombine_u8(\n        vqmovun_s16(output_val_s16.val[2]), vqmovun_s16(output_val_s16.val[3]));\n\n    ClampWithRangeAndStore(output_data + c, output_val_u8_0_1, masks_clamp_0_1);\n    ClampWithRangeAndStore(output_data + c + 16, output_val_u8_2_3,\n                           masks_clamp_2_3);\n  }\n#endif  // GEMMLOWP_NEON\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int16 input_val_centered =\n        static_cast<int16>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      using gemmlowp::SaturatingRoundingDoublingHighMul;\n      const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n          static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n          static_cast<int16>(input_multiplier));\n      using FixedPoint4 = gemmlowp::FixedPoint<int16, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int16, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::tanh(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int16 output_val_s16 = RoundingDivideByPOT(output_val_f0.raw(), 8);\n      output_val_s16 += output_zero_point;\n      if (output_val_s16 == 256) {\n        output_val_s16 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s16, 0);\n      TFLITE_DCHECK_LE(output_val_s16, 255);\n      output_val = static_cast<uint8>(output_val_s16);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Tanh16bitPrecision(const TanhParams& params,\n                               const RuntimeShape& input_shape,\n                               const int8* input_data,\n                               const RuntimeShape& output_shape,\n                               int8* output_data) {\n  // Note that this is almost the exact same code as in Logistic().\n  ruy::profiler::ScopeLabel label(\"Tanh/Int8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int16 input_multiplier = static_cast<int16>(params.input_multiplier);\n  const int16 input_left_shift = static_cast<int16>(params.input_left_shift);\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n  const int16x8_t range_radius_dup = vdupq_n_s16(input_range_radius);\n  const int16x8_t neg_range_radius_dup = vdupq_n_s16(-input_range_radius);\n\n  // Handle 32 values at a time\n  for (; c <= size - 32; c += 32) {\n    // Read input int8 values, cast to int16 and subtract input_zero_point\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_val_centered_0_1 =\n        Load16AndSubtractZeroPoint(input_data + c, input_zero_point);\n    const int16x8x2_t input_val_centered_2_3 =\n        Load16AndSubtractZeroPoint(input_data + c + 16, input_zero_point);\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = -128;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 127;\n    //   } else {\n    //     ...\n    uint8x16x2_t masks_clamp_0_1 = CalculateSignedClampingWithRangeBitMasks(\n        input_val_centered_0_1, range_radius_dup, neg_range_radius_dup);\n    uint8x16x2_t masks_clamp_2_3 = CalculateSignedClampingWithRangeBitMasks(\n        input_val_centered_2_3, range_radius_dup, neg_range_radius_dup);\n\n    int16x8x4_t input_val_rescaled = SaturatingRounding(\n        input_val_centered_0_1.val[0], input_val_centered_0_1.val[1],\n        input_val_centered_2_3.val[0], input_val_centered_2_3.val[1],\n        input_left_shift, input_multiplier);\n\n    int16x8x4_t output_val_s16 = FixedPoint4Tanh(input_val_rescaled);\n\n    // Cast output values to uint8, saturating\n    int8x16_t output_val_s8_0_1 = vcombine_s8(\n        vqmovn_s16(output_val_s16.val[0]), vqmovn_s16(output_val_s16.val[1]));\n    int8x16_t output_val_s8_2_3 = vcombine_s8(\n        vqmovn_s16(output_val_s16.val[2]), vqmovn_s16(output_val_s16.val[3]));\n\n    ClampWithRangeAndStore(output_data + c, output_val_s8_0_1, masks_clamp_0_1);\n    ClampWithRangeAndStore(output_data + c + 16, output_val_s8_2_3,\n                           masks_clamp_2_3);\n  }\n#endif  // GEMMLOWP_NEON\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const int8 input_val_s8 = input_data[c];\n    const int16 input_val_centered =\n        static_cast<int16>(input_val_s8) - input_zero_point;\n    int8 output_val;\n    if (input_val_centered <= -input_range_radius) {\n      output_val = -128;\n    } else if (input_val_centered >= input_range_radius) {\n      output_val = 127;\n    } else {\n      using gemmlowp::SaturatingRoundingDoublingHighMul;\n      const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n          static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n          static_cast<int16>(input_multiplier));\n      using FixedPoint4 = gemmlowp::FixedPoint<int16, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int16, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::tanh(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int16 output_val_s16 = RoundingDivideByPOT(output_val_f0.raw(), 8);\n      if (output_val_s16 == 128) {\n        output_val_s16 = 127;\n      }\n      TFLITE_DCHECK_GE(output_val_s16, -128);\n      TFLITE_DCHECK_LE(output_val_s16, 127);\n      output_val = static_cast<int8>(output_val_s16);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Logistic16bitPrecision(const LogisticParams& params,\n                                   const RuntimeShape& input_shape,\n                                   const uint8* input_data,\n                                   const RuntimeShape& output_shape,\n                                   uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic/Uint8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int16 input_left_shift = static_cast<int16>(params.input_left_shift);\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n  const int16x8_t range_radius_dup = vdupq_n_s16(input_range_radius);\n  const int16x8_t neg_range_radius_dup = vdupq_n_s16(-input_range_radius);\n\n  // Handle 32 values at a time\n  for (; c <= size - 32; c += 32) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_val_centered_0_1 =\n        Load16AndSubtractZeroPoint(input_data + c, input_zero_point);\n    const int16x8x2_t input_val_centered_2_3 =\n        Load16AndSubtractZeroPoint(input_data + c + 16, input_zero_point);\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint8x16x2_t masks_clamp_0_1 = CalculateUnsignedClampingWithRangeBitMasks(\n        input_val_centered_0_1, range_radius_dup, neg_range_radius_dup);\n    uint8x16x2_t masks_clamp_2_3 = CalculateUnsignedClampingWithRangeBitMasks(\n        input_val_centered_2_3, range_radius_dup, neg_range_radius_dup);\n\n    int16x8x4_t input_val_rescaled = SaturatingRounding(\n        input_val_centered_0_1.val[0], input_val_centered_0_1.val[1],\n        input_val_centered_2_3.val[0], input_val_centered_2_3.val[1],\n        input_left_shift, input_multiplier);\n\n    int16x8x4_t output_val_s16 = FixedPoint4Logistic(input_val_rescaled);\n\n    // Cast output values to uint8, saturating\n    uint8x16_t output_val_u8_0_1 = vcombine_u8(\n        vqmovun_s16(output_val_s16.val[0]), vqmovun_s16(output_val_s16.val[1]));\n    uint8x16_t output_val_u8_2_3 = vcombine_u8(\n        vqmovun_s16(output_val_s16.val[2]), vqmovun_s16(output_val_s16.val[3]));\n\n    ClampWithRangeAndStore(output_data + c, output_val_u8_0_1, masks_clamp_0_1);\n    ClampWithRangeAndStore(output_data + c + 16, output_val_u8_2_3,\n                           masks_clamp_2_3);\n  }\n#endif  // GEMMLOWP_NEON\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int16 input_val_centered =\n        static_cast<int16>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      using gemmlowp::SaturatingRoundingDoublingHighMul;\n      const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n          static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n          static_cast<int16>(input_multiplier));\n      using FixedPoint4 = gemmlowp::FixedPoint<int16, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int16, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int16 output_val_s16 = RoundingDivideByPOT(output_val_f0.raw(), 7);\n      if (output_val_s16 == 256) {\n        output_val_s16 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s16, 0);\n      TFLITE_DCHECK_LE(output_val_s16, 255);\n      output_val = static_cast<uint8>(output_val_s16);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Logistic16bitPrecision(const LogisticParams& params,\n                                   const RuntimeShape& input_shape,\n                                   const int8* input_data,\n                                   const RuntimeShape& output_shape,\n                                   int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic/Int8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int16 input_left_shift = static_cast<int16>(params.input_left_shift);\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n  const int16 output_zero_point = 128;\n// TODO(b/139252020): Replace GEMMLOWP_NEON with USE_NEON when the bug is fixed.\n// The converted versions of gemmlowp::tanh and gemmlowp::logistic, done by\n// arm_sse_2_neon.h, produce incorrect results with int16x8_t data types.\n#ifdef GEMMLOWP_NEON\n  const int16x8_t range_radius_dup = vdupq_n_s16(input_range_radius);\n  const int16x8_t neg_range_radius_dup = vdupq_n_s16(-input_range_radius);\n  const int16x8_t output_zero_point_dup = vdupq_n_s16(output_zero_point);\n\n  // Handle 32 values at a time\n  for (; c <= size - 32; c += 32) {\n    // Read input int8 values, cast to int16 and subtract input_zero_point\n    using cpu_backend_gemm::detail::Load16AndSubtractZeroPoint;\n    const int16x8x2_t input_val_centered_0_1 =\n        Load16AndSubtractZeroPoint(input_data + c, input_zero_point);\n    const int16x8x2_t input_val_centered_2_3 =\n        Load16AndSubtractZeroPoint(input_data + c + 16, input_zero_point);\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = -128;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 127;\n    //   } else {\n    //     ...\n    uint8x16x2_t masks_clamp_0_1 = CalculateSignedClampingWithRangeBitMasks(\n        input_val_centered_0_1, range_radius_dup, neg_range_radius_dup);\n    uint8x16x2_t masks_clamp_2_3 = CalculateSignedClampingWithRangeBitMasks(\n        input_val_centered_2_3, range_radius_dup, neg_range_radius_dup);\n\n    int16x8x4_t input_val_rescaled = SaturatingRounding(\n        input_val_centered_0_1.val[0], input_val_centered_0_1.val[1],\n        input_val_centered_2_3.val[0], input_val_centered_2_3.val[1],\n        input_left_shift, input_multiplier);\n\n    int16x8x4_t output_val_s16 = FixedPoint4Logistic(input_val_rescaled);\n\n    // Substract output zero point.\n    output_val_s16.val[0] =\n        vsubq_s16(output_val_s16.val[0], output_zero_point_dup);\n    output_val_s16.val[1] =\n        vsubq_s16(output_val_s16.val[1], output_zero_point_dup);\n    output_val_s16.val[2] =\n        vsubq_s16(output_val_s16.val[2], output_zero_point_dup);\n    output_val_s16.val[3] =\n        vsubq_s16(output_val_s16.val[3], output_zero_point_dup);\n\n    // Cast output values to int8, saturating\n    int8x16_t output_val_s8_0_1 = vcombine_s8(\n        vqmovn_s16(output_val_s16.val[0]), vqmovn_s16(output_val_s16.val[1]));\n    int8x16_t output_val_s8_2_3 = vcombine_s8(\n        vqmovn_s16(output_val_s16.val[2]), vqmovn_s16(output_val_s16.val[3]));\n\n    ClampWithRangeAndStore(output_data + c, output_val_s8_0_1, masks_clamp_0_1);\n    ClampWithRangeAndStore(output_data + c + 16, output_val_s8_2_3,\n                           masks_clamp_2_3);\n  }\n#endif  // GEMMLOWP_NEON\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const int8 input_val_s8 = input_data[c];\n    const int16 input_val_centered =\n        static_cast<int16>(input_val_s8) - input_zero_point;\n    int8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = -128;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 127;\n    } else {\n      using gemmlowp::SaturatingRoundingDoublingHighMul;\n      const int16 input_val_rescaled = SaturatingRoundingDoublingHighMul(\n          static_cast<int16>(input_val_centered * (1 << input_left_shift)),\n          static_cast<int16>(input_multiplier));\n      using FixedPoint4 = gemmlowp::FixedPoint<int16, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int16, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int16 output_val_s16 = RoundingDivideByPOT(output_val_f0.raw(), 7);\n      output_val_s16 -= output_zero_point;\n      if (output_val_s16 == 128) {\n        output_val_s16 = 127;\n      }\n      TFLITE_DCHECK_GE(output_val_s16, -128);\n      TFLITE_DCHECK_LE(output_val_s16, 127);\n      output_val = static_cast<int8>(output_val_s16);\n    }\n    output_data[c] = output_val;\n  }\n}\n\n// Transpose2D only deals with typical 2D matrix transpose ops.\n// Perform transpose by transposing 4x4 blocks of the input, proceeding from\n// left to right (down the rows) of the input, and then from top to bottom.\ntemplate <typename T>\ninline void Transpose2D(const RuntimeShape& input_shape, const T* input_data,\n                        const RuntimeShape& output_shape, T* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);\n\n  const int d0 = input_shape.DimsData()[0];\n  const int d1 = input_shape.DimsData()[1];\n  const int kLines = 4;\n  const int kSkipSize = (kLines - 1) * d1;\n\n  const T* input = input_data;\n\n  int i = 0;\n  for (; i <= d0 - kLines; i += kLines) {\n    T* output = output_data + i;\n\n    const T* input_ptr = input;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n\n    int j = 0;\n    for (; j <= d1 - kLines; j += kLines) {\n      input_ptr = input;\n      const T a00 = input_ptr[0];\n      const T a01 = input_ptr[1];\n      const T a02 = input_ptr[2];\n      const T a03 = input_ptr[3];\n      input_ptr += d1;\n      const T a10 = input_ptr[0];\n      const T a11 = input_ptr[1];\n      const T a12 = input_ptr[2];\n      const T a13 = input_ptr[3];\n      input_ptr += d1;\n      const T a20 = input_ptr[0];\n      const T a21 = input_ptr[1];\n      const T a22 = input_ptr[2];\n      const T a23 = input_ptr[3];\n      input_ptr += d1;\n      const T a30 = input_ptr[0];\n      const T a31 = input_ptr[1];\n      const T a32 = input_ptr[2];\n      const T a33 = input_ptr[3];\n\n      output[0] = a00;\n      output[1] = a10;\n      output[2] = a20;\n      output[3] = a30;\n      output += d0;\n\n      output[0] = a01;\n      output[1] = a11;\n      output[2] = a21;\n      output[3] = a31;\n      output += d0;\n\n      output[0] = a02;\n      output[1] = a12;\n      output[2] = a22;\n      output[3] = a32;\n      output += d0;\n\n      output[0] = a03;\n      output[1] = a13;\n      output[2] = a23;\n      output[3] = a33;\n      output += d0;\n\n      input += kLines;\n    }\n    if (j == d1) {\n      input += kSkipSize;\n    } else {\n      for (int p = 0; p < kLines; ++p) {\n        for (int q = 0; q < d1 - j; ++q) {\n          *(output + q * d0 + p) = *(input + p * d1 + q);\n        }\n      }\n      input += (d1 - j) + kSkipSize;\n    }\n  }\n  for (; i < d0; ++i) {\n    T* output = output_data + i;\n    for (int j = 0; j < d1; ++j) {\n      *output = *input;\n      output += d0;\n      ++input;\n    }\n  }\n}\n\ntemplate <>\ninline void Transpose2D(const RuntimeShape& input_shape,\n                        const int32_t* input_data,\n                        const RuntimeShape& output_shape,\n                        int32_t* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 2);\n\n  const int d0 = input_shape.DimsData()[0];\n  const int d1 = input_shape.DimsData()[1];\n#ifdef USE_NEON\n  const int kLines = 4;\n  const int kSkipSize = (kLines - 1) * d1;\n#endif\n\n  const int32_t* input = input_data;\n\n  int i = 0;\n#ifdef USE_NEON\n  for (; i <= d0 - kLines; i += kLines) {\n    int32_t* output = output_data + i;\n\n    const int32_t* input_ptr = input;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n    input_ptr += d1;\n    optimized_ops_preload_l1_keep(input_ptr);\n\n    int j = 0;\n    for (; j <= d1 - kLines; j += kLines) {\n      input_ptr = input;\n      int32x4_t a0 = vld1q_s32(input);\n      input_ptr += d1;\n      int32x4_t a1 = vld1q_s32(input_ptr);\n      input_ptr += d1;\n      int32x4_t a2 = vld1q_s32(input_ptr);\n      input_ptr += d1;\n      int32x4_t a3 = vld1q_s32(input_ptr);\n\n      int32x4x2_t tmp1 = vuzpq_s32(a0, a2);\n      int32x4x2_t tmp2 = vuzpq_s32(a1, a3);\n      int32x4x2_t tmp3 = vtrnq_s32(tmp1.val[0], tmp2.val[0]);\n      int32x4x2_t tmp4 = vtrnq_s32(tmp1.val[1], tmp2.val[1]);\n\n      vst1q_s32(output, tmp3.val[0]);\n      output += d0;\n      vst1q_s32(output, tmp4.val[0]);\n      output += d0;\n      vst1q_s32(output, tmp3.val[1]);\n      output += d0;\n      vst1q_s32(output, tmp4.val[1]);\n      output += d0;\n      input += kLines;\n    }\n    if (j == d1) {\n      input += kSkipSize;\n    } else {\n      for (int p = 0; p < kLines; ++p) {\n        for (int q = 0; q < d1 - j; ++q) {\n          *(output + q * d0 + p) = *(input + p * d1 + q);\n        }\n      }\n      input += (d1 - j) + kSkipSize;\n    }\n  }\n#endif\n  for (; i < d0; ++i) {\n    int32_t* output = output_data + i;\n    for (int j = 0; j < d1; ++j) {\n      *output = *input;\n      output += d0;\n      ++input;\n    }\n  }\n}\n\n// TODO(b/173718660): see if we can reduce the number\n// of lines of code in branching without affecting latency.\ntemplate <typename T>\ninline void Transpose3D(const TransposeParams& params,\n                        const RuntimeShape& input_shape, const T* input_data,\n                        const RuntimeShape& output_shape, T* output_data) {\n  int s1, s2, s3;\n  s1 = input_shape.Dims(0);\n  s2 = input_shape.Dims(1);\n  s3 = input_shape.Dims(2);\n\n  int p1, p2, p3;\n  if (params.perm[0] == 2) {\n    p1 = 1;\n  } else if (params.perm[1] == 2) {\n    p2 = 1;\n  } else {\n    p3 = 1;\n  }\n\n  if (params.perm[0] == 1) {\n    p1 = s3;\n  } else if (params.perm[1] == 1) {\n    p2 = s3;\n  } else {\n    p3 = s3;\n  }\n\n  if (params.perm[0] == 0) {\n    p1 = s2 * s3;\n  } else if (params.perm[1] == 0) {\n    p2 = s2 * s3;\n  } else {\n    p3 = s2 * s3;\n  }\n\n  int o_s[3];\n  o_s[0] = input_shape.Dims(params.perm[0]);\n  o_s[1] = input_shape.Dims(params.perm[1]);\n  o_s[2] = input_shape.Dims(params.perm[2]);\n\n  for (int i1 = 0; i1 < o_s[0]; ++i1) {\n    for (int i2 = 0; i2 < o_s[1]; ++i2) {\n      for (int i3 = 0; i3 < o_s[2]; ++i3) {\n        const int i = i1 * p1 + i2 * p2 + i3 * p3;\n        const int o = i1 * o_s[1] * o_s[2] + i2 * o_s[2] + i3;\n        output_data[o] = input_data[i];\n      }\n    }\n  }\n}\n\ntemplate <typename T, int N>\nvoid TransposeImpl(const TransposeParams& params,\n                   const RuntimeShape& input_shape, const T* input_data,\n                   const RuntimeShape& output_shape, T* output_data) {\n  const int dims_cnt = input_shape.DimensionsCount();\n\n  int dim0, dim1;\n  if (transpose_utils::IsTranspose2DApplicable(params, input_shape, &dim0,\n                                               &dim1)) {\n    Transpose2D(RuntimeShape({dim0, dim1}), input_data,\n                RuntimeShape({dim1, dim0}), output_data);\n    return;\n  }\n\n  // TODO(b/141217325): notably Eigen is better suited for\n  // larger inputs whereas Transpose3D is generally\n  // better for smaller ones.\n  //\n  // E.g. on Nexus 5, Eigen is better for size 96^3 and up\n  // and Transpose3D is better for 72^3 and down.\n  //\n  // 96^3 is not mobile-friendly for certain usecases\n  // (e.g. model used in beam search for seq2seq) but is in others.\n  // Consider tradeoffs.\n  if (dims_cnt == 3) {\n    Transpose3D(params, input_shape, input_data, output_shape, output_data);\n    return;\n  }\n\n  // Reroute to the reference version if an optimized method for the given data\n  // is not available.\n  reference_ops::Transpose<T, N>(params, input_shape, input_data, output_shape,\n                                 output_data);\n}\n\ntemplate <typename T, int N = 5>\nvoid Transpose(const TransposeParams& unshrinked_params,\n               const RuntimeShape& unshrinked_input_shape, const T* input_data,\n               const RuntimeShape& unshrinked_output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Transpose\");\n\n  const int output_size = unshrinked_output_shape.DimensionsCount();\n  TFLITE_DCHECK_LE(unshrinked_input_shape.DimensionsCount(), N);\n  TFLITE_DCHECK_LE(output_size, N);\n  TFLITE_DCHECK_EQ(output_size, unshrinked_params.perm_count);\n\n  RuntimeShape shrinked_input_shape = RuntimeShape(unshrinked_input_shape);\n  RuntimeShape shrinked_output_shape = RuntimeShape(unshrinked_output_shape);\n  TransposeParams shrinked_params = unshrinked_params;\n\n  // Reduce any dimensions that have one size. Lower transpose op usually\n  // performs better since memory access patterns will be improved.\n  transpose_utils::RemoveOneSizeDimensions(\n      &shrinked_input_shape, &shrinked_output_shape, &shrinked_params);\n\n  // Handle identity cases.\n  // TODO(b/140779653): Add an optimization pass in the conversion process to\n  // remove transpose op nodes where they do nothing like the below one.\n  bool identical = true;\n  for (int i = 0; i < shrinked_params.perm_count; ++i) {\n    if (shrinked_params.perm[i] != i) {\n      identical = false;\n      break;\n    }\n  }\n  if (identical) {\n    memcpy(output_data, input_data,\n           unshrinked_input_shape.FlatSize() * sizeof(T));\n    return;\n  }\n\n  // Reduce dimensions by flattening.\n  if (shrinked_params.perm[0] == 0 && output_size >= 3) {\n    RuntimeShape non_flatten_input_shape;\n    RuntimeShape non_flatten_output_shape;\n    TransposeParams non_flatten_params;\n    const int total_size = shrinked_input_shape.FlatSize();\n    const int non_flatten_size = transpose_utils::Flatten(\n        shrinked_input_shape, shrinked_output_shape, shrinked_params,\n        &non_flatten_input_shape, &non_flatten_output_shape,\n        &non_flatten_params);\n    TFLITE_DCHECK_NE(non_flatten_params.perm[0], 0);\n\n    for (int i = 0; i < total_size; i += non_flatten_size) {\n      TransposeImpl<T, N>(non_flatten_params, non_flatten_input_shape,\n                          input_data + i, non_flatten_output_shape,\n                          output_data + i);\n    }\n    return;\n  }\n\n  // Call non-flattened case.\n  TransposeImpl<T, N>(shrinked_params, shrinked_input_shape, input_data,\n                      shrinked_output_shape, output_data);\n}\n\n// Assume input1 & input2 have the same scale & zero point.\ninline void MaximumElementwise(int size, const ArithmeticParams& params,\n                               const int8* input1_data, const int8* input2_data,\n                               int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaximumElementwiseInt8/8bit\");\n  int i = 0;\n#ifdef USE_NEON\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input1_val_original = vld1q_s8(input1_data + i);\n    const int8x16_t input2_val_original = vld1q_s8(input2_data + i);\n    const int8x16_t max_data =\n        vmaxq_s8(input1_val_original, input2_val_original);\n    vst1q_s8(output_data + i, max_data);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const int8 input1_val = input1_data[i];\n    const int8 input2_val = input2_data[i];\n    output_data[i] = std::max(input1_val, input2_val);\n  }\n}\n\ninline void MaximumScalarBroadcast(int size, const ArithmeticParams& params,\n                                   int8 input1_data, const int8* input2_data,\n                                   int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MaximumScalarBroadcastInt8/8bit\");\n  int i = 0;\n\n#ifdef USE_NEON\n  const int8x16_t input1_val_original = vdupq_n_s8(input1_data);\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input2_val_original = vld1q_s8(input2_data + i);\n    const int8x16_t max_data =\n        vmaxq_s8(input1_val_original, input2_val_original);\n    vst1q_s8(output_data + i, max_data);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const int8 input2_val = input2_data[i];\n    output_data[i] = std::max(input1_data, input2_val);\n  }\n}\n\n// Assume input1 & input2 have the same scale & zero point.\ninline void MinimumElementwise(int size, const ArithmeticParams& params,\n                               const int8* input1_data, const int8* input2_data,\n                               int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MinimumElementwiseInt8/8bit\");\n  int i = 0;\n#ifdef USE_NEON\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input1_val_original = vld1q_s8(input1_data + i);\n    const int8x16_t input2_val_original = vld1q_s8(input2_data + i);\n    const int8x16_t min_data =\n        vminq_s8(input1_val_original, input2_val_original);\n    vst1q_s8(output_data + i, min_data);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const int8 input1_val = input1_data[i];\n    const int8 input2_val = input2_data[i];\n    output_data[i] = std::min(input1_val, input2_val);\n  }\n}\n\ninline void MinimumScalarBroadcast(int size, const ArithmeticParams& params,\n                                   int8 input1_data, const int8* input2_data,\n                                   int8* output_data) {\n  ruy::profiler::ScopeLabel label(\"MinimumScalarBroadcastInt8/8bit\");\n  int i = 0;\n\n#ifdef USE_NEON\n  const int8x16_t input1_val_original = vdupq_n_s8(input1_data);\n  for (; i <= size - 16; i += 16) {\n    const int8x16_t input2_val_original = vld1q_s8(input2_data + i);\n    const int8x16_t min_data =\n        vminq_s8(input1_val_original, input2_val_original);\n    vst1q_s8(output_data + i, min_data);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const int8 input2_val = input2_data[i];\n    output_data[i] = std::min(input1_data, input2_val);\n  }\n}\n\ntemplate <typename Op>\ninline void BroadcastMaximumDispatch(const ArithmeticParams& params,\n                                     const RuntimeShape& input1_shape,\n                                     const int8* input1_data,\n                                     const RuntimeShape& input2_shape,\n                                     const int8* input2_data,\n                                     const RuntimeShape& output_shape,\n                                     int8* output_data, Op op) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return reference_ops::MaximumMinimumBroadcastSlow(\n        input1_shape, input1_data, input2_shape, input2_data, output_shape,\n        output_data, op);\n  }\n\n  BinaryBroadcastFiveFold(params, input1_shape, input1_data, input2_shape,\n                          input2_data, output_shape, output_data,\n                          MaximumElementwise, MaximumScalarBroadcast);\n}\n\ntemplate <typename Op>\ninline void BroadcastMinimumDispatch(const ArithmeticParams& params,\n                                     const RuntimeShape& input1_shape,\n                                     const int8* input1_data,\n                                     const RuntimeShape& input2_shape,\n                                     const int8* input2_data,\n                                     const RuntimeShape& output_shape,\n                                     int8* output_data, Op op) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return reference_ops::MaximumMinimumBroadcastSlow(\n        input1_shape, input1_data, input2_shape, input2_data, output_shape,\n        output_data, op);\n  }\n\n  BinaryBroadcastFiveFold(params, input1_shape, input1_data, input2_shape,\n                          input2_data, output_shape, output_data,\n                          MinimumElementwise, MinimumScalarBroadcast);\n}\n\ntemplate <typename T>\nvoid CumsumImpl(const T* input_data, const RuntimeShape& shape, int axis,\n                bool exclusive, bool reverse, T* output_data) {\n  Eigen::array<Eigen::DenseIndex, 3> dims = {1, 1, 1};\n\n  for (int i = 0; i < axis; ++i) {\n    dims[0] *= shape.Dims(i);\n  }\n  dims[1] = shape.Dims(axis);\n  for (int i = axis + 1; i < shape.DimensionsCount(); ++i) {\n    dims[2] *= shape.Dims(i);\n  }\n\n  typedef Eigen::TensorMap<\n      Eigen::Tensor<const T, 3, Eigen::RowMajor, Eigen::DenseIndex>,\n      Eigen::Aligned>\n      ConstTensor;\n  typedef Eigen::TensorMap<\n      Eigen::Tensor<T, 3, Eigen::RowMajor, Eigen::DenseIndex>, Eigen::Aligned>\n      Tensor;\n  ConstTensor input(input_data, dims);\n  Tensor output(output_data, dims);\n\n  if (reverse) {\n    Eigen::array<bool, 3> reverse_idx = {false, true, false};\n    output =\n        input.reverse(reverse_idx).cumsum(1, exclusive).reverse(reverse_idx);\n  } else {\n    output = input.cumsum(1, exclusive);\n  }\n}\n\ntemplate <typename T>\nvoid CumSum(const T* input_data, const RuntimeShape& shape, int axis,\n            bool exclusive, bool reverse, T* output_data) {\n  const int dim = shape.DimensionsCount();\n  TFLITE_DCHECK_GE(dim, 1);\n  CumsumImpl<T>(input_data, shape, axis, exclusive, reverse, output_data);\n}\n\ninline void PReluScalarBroadcast(int size, const ArithmeticParams& params,\n                                 float alpha, const float* input_data,\n                                 float* output_data) {\n  ruy::profiler::ScopeLabel label(\"PreluScalarBroadcast/float\");\n  int i = 0;\n\n#ifdef USE_NEON\n  const float32x4_t zero_dup = vdupq_n_f32(0.0f);\n  const float32x4_t alpha_dup = vdupq_n_f32(alpha);\n  for (; i <= size - 16; i += 16) {\n    const float32x4_t input1 = vld1q_f32(input_data + i);\n    const float32x4_t input2 = vld1q_f32(input_data + i + 4);\n    const float32x4_t input3 = vld1q_f32(input_data + i + 8);\n    const float32x4_t input4 = vld1q_f32(input_data + i + 12);\n\n    const float32x4_t temp1 = vmulq_f32(input1, alpha_dup);\n    const float32x4_t temp2 = vmulq_f32(input2, alpha_dup);\n    const float32x4_t temp3 = vmulq_f32(input3, alpha_dup);\n    const float32x4_t temp4 = vmulq_f32(input4, alpha_dup);\n\n    const uint32x4_t mask1 = vcgeq_f32(input1, zero_dup);\n    const uint32x4_t mask2 = vcgeq_f32(input2, zero_dup);\n    const uint32x4_t mask3 = vcgeq_f32(input3, zero_dup);\n    const uint32x4_t mask4 = vcgeq_f32(input4, zero_dup);\n\n    const float32x4_t result1 = vbslq_f32(mask1, input1, temp1);\n    vst1q_f32(output_data + i, result1);\n    const float32x4_t result2 = vbslq_f32(mask2, input2, temp2);\n    vst1q_f32(output_data + i + 4, result2);\n    const float32x4_t result3 = vbslq_f32(mask3, input3, temp3);\n    vst1q_f32(output_data + i + 8, result3);\n    const float32x4_t result4 = vbslq_f32(mask4, input4, temp4);\n    vst1q_f32(output_data + i + 12, result4);\n  }\n\n  for (; i <= size - 4; i += 4) {\n    const float32x4_t input = vld1q_f32(input_data + i);\n    const float32x4_t temp = vmulq_f32(input, alpha_dup);\n    const uint32x4_t mask = vcgeq_f32(input, zero_dup);\n    const float32x4_t result = vbslq_f32(mask, input, temp);\n    vst1q_f32(output_data + i, result);\n  }\n#endif  // USE_NEON\n  for (; i < size; ++i) {\n    const float input = input_data[i];\n    output_data[i] = input >= 0.f ? input : input * alpha;\n  }\n}\n\ninline void PReluElementWise(int flat_size, const ArithmeticParams& params,\n                             const float* alpha_data, const float* input_data,\n                             float* output_data) {\n  ruy::profiler::ScopeLabel label(\"PreluElementWise/float\");\n\n  int i = 0;\n#ifdef USE_NEON\n  const float32x4_t zero_dup = vdupq_n_f32(0.0f);\n  for (; i <= flat_size - 16; i += 16) {\n    const float32x4_t input1 = vld1q_f32(input_data + i);\n    const float32x4_t alpha1 = vld1q_f32(alpha_data + i);\n    const float32x4_t input2 = vld1q_f32(input_data + i + 4);\n    const float32x4_t alpha2 = vld1q_f32(alpha_data + i + 4);\n    const float32x4_t input3 = vld1q_f32(input_data + i + 8);\n    const float32x4_t alpha3 = vld1q_f32(alpha_data + i + 8);\n    const float32x4_t input4 = vld1q_f32(input_data + i + 12);\n    const float32x4_t alpha4 = vld1q_f32(alpha_data + i + 12);\n\n    const float32x4_t temp1 = vmulq_f32(input1, alpha1);\n    const float32x4_t temp2 = vmulq_f32(input2, alpha2);\n    const float32x4_t temp3 = vmulq_f32(input3, alpha3);\n    const float32x4_t temp4 = vmulq_f32(input4, alpha4);\n\n    const uint32x4_t mask1 = vcgeq_f32(input1, zero_dup);\n    const uint32x4_t mask2 = vcgeq_f32(input2, zero_dup);\n    const uint32x4_t mask3 = vcgeq_f32(input3, zero_dup);\n    const uint32x4_t mask4 = vcgeq_f32(input4, zero_dup);\n\n    const float32x4_t result1 = vbslq_f32(mask1, input1, temp1);\n    vst1q_f32(output_data + i, result1);\n    const float32x4_t result2 = vbslq_f32(mask2, input2, temp2);\n    vst1q_f32(output_data + i + 4, result2);\n    const float32x4_t result3 = vbslq_f32(mask3, input3, temp3);\n    vst1q_f32(output_data + i + 8, result3);\n    const float32x4_t result4 = vbslq_f32(mask4, input4, temp4);\n    vst1q_f32(output_data + i + 12, result4);\n  }\n\n  for (; i <= flat_size - 4; i += 4) {\n    const float32x4_t input = vld1q_f32(input_data + i);\n    const float32x4_t alpha = vld1q_f32(alpha_data + i);\n\n    const float32x4_t temp = vmulq_f32(input, alpha);\n    const uint32x4_t mask = vcgeq_f32(input, zero_dup);\n    const float32x4_t result = vbslq_f32(mask, input, temp);\n    vst1q_f32(output_data + i, result);\n  }\n#endif  // USE_NEON\n  for (; i < flat_size; ++i) {\n    const float input = input_data[i];\n    const float alpha = alpha_data[i];\n    output_data[i] = input >= 0.f ? input : input * alpha;\n  }\n}\n\ninline void BroadcastPReluDispatch(\n    const ArithmeticParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& alpha_shape,\n    const float* alpha_data, const RuntimeShape& output_shape,\n    float* output_data, float (*func)(float, float)) {\n  if (params.broadcast_category == BroadcastableOpCategory::kGenericBroadcast) {\n    return reference_ops::BroadcastBinaryFunction4DSlow<float, float, float>(\n        input_shape, input_data, alpha_shape, alpha_data, output_shape,\n        output_data, func);\n  }\n\n  BinaryBroadcastFiveFold(params, input_shape, input_data, alpha_shape,\n                          alpha_data, output_shape, output_data,\n                          PReluElementWise, PReluScalarBroadcast);\n}\n\n// Returns the index with minimum value within `input_data`.\n// If there is a tie, returns the smaller index.\ntemplate <typename T>\ninline int ArgMinVector(const T* input_data, int size) {\n  T min_value = input_data[0];\n  int min_index = 0;\n  for (int i = 1; i < size; ++i) {\n    const T curr_value = input_data[i];\n    if (curr_value < min_value) {\n      min_value = curr_value;\n      min_index = i;\n    }\n  }\n  return min_index;\n}\n\n// Returns the index with maximum value within `input_data`.\n// If there is a tie, returns the smaller index.\ntemplate <typename T>\ninline int ArgMaxVector(const T* input_data, int size) {\n  T max_value = input_data[0];\n  int max_index = 0;\n  for (int i = 1; i < size; ++i) {\n    const T curr_value = input_data[i];\n    if (curr_value > max_value) {\n      max_value = curr_value;\n      max_index = i;\n    }\n  }\n  return max_index;\n}\n\ntemplate <>\ninline int ArgMinVector(const float* input_data, int size) {\n  int32_t min_index = 0;\n  float min_value = input_data[0];\n  int32_t i = 1;\n#ifdef USE_NEON\n  if (size >= 4) {\n    float32x4_t min_value_f32x4 = vld1q_f32(input_data);\n    const int32_t index_init[4] = {0, 1, 2, 3};\n    int32x4_t min_index_s32x4 = vld1q_s32(index_init);\n    int32x4_t index_s32x4 = min_index_s32x4;\n    int32x4_t inc = vdupq_n_s32(4);\n    for (i = 4; i <= size - 4; i += 4) {\n      // Increase indices by 4.\n      index_s32x4 = vaddq_s32(index_s32x4, inc);\n      float32x4_t v = vld1q_f32(&input_data[i]);\n      uint32x4_t mask = vcltq_f32(v, min_value_f32x4);\n      min_value_f32x4 = vminq_f32(min_value_f32x4, v);\n      min_index_s32x4 = vbslq_s32(mask, index_s32x4, min_index_s32x4);\n    }\n    // Find min element within float32x4_t.\n#ifdef __aarch64__\n    min_value = vminvq_f32(min_value_f32x4);\n#else\n    float32x2_t min_value_f32x2 = vpmin_f32(vget_low_f32(min_value_f32x4),\n                                            vget_high_f32(min_value_f32x4));\n    min_value_f32x2 = vpmin_f32(min_value_f32x2, min_value_f32x2);\n    min_value = vget_lane_f32(min_value_f32x2, 0);\n#endif  // __aarch64__\n    // Mask indices of non-min values with max int32_t.\n    float32x4_t fill_min_value_f32x4 = vdupq_n_f32(min_value);\n    uint32x4_t mask = vceqq_f32(min_value_f32x4, fill_min_value_f32x4);\n    int32x4_t all_set = vdupq_n_s32(std::numeric_limits<int>::max());\n    min_index_s32x4 = vbslq_s32(mask, min_index_s32x4, all_set);\n    // Find min index of min values.\n#ifdef __aarch64__\n    min_index = vminvq_s32(min_index_s32x4);\n#else\n    int32x2_t min_index_s32x2 = vpmin_s32(vget_low_s32(min_index_s32x4),\n                                          vget_high_s32(min_index_s32x4));\n    min_index_s32x2 = vpmin_s32(min_index_s32x2, min_index_s32x2);\n    min_index = vget_lane_s32(min_index_s32x2, 0);\n#endif  // __aarch64__\n  }\n#endif  // USE_NEON\n  // Leftover loop.\n  for (; i < size; ++i) {\n    const float curr_value = input_data[i];\n    if (curr_value < min_value) {\n      min_value = curr_value;\n      min_index = i;\n    }\n  }\n  return min_index;\n}\n\ntemplate <>\ninline int ArgMaxVector(const float* input_data, int size) {\n  int32_t max_index = 0;\n  float max_value = input_data[0];\n  int32_t i = 1;\n#ifdef USE_NEON\n  if (size >= 4) {\n    float32x4_t max_value_f32x4 = vld1q_f32(input_data);\n    const int32_t index_init[4] = {0, 1, 2, 3};\n    int32x4_t max_index_s32x4 = vld1q_s32(index_init);\n    int32x4_t index_s32x4 = max_index_s32x4;\n    int32x4_t inc = vdupq_n_s32(4);\n    for (i = 4; i <= size - 4; i += 4) {\n      // Increase indices by 4.\n      index_s32x4 = vaddq_s32(index_s32x4, inc);\n      float32x4_t v = vld1q_f32(&input_data[i]);\n      uint32x4_t mask = vcgtq_f32(v, max_value_f32x4);\n      max_value_f32x4 = vmaxq_f32(max_value_f32x4, v);\n      max_index_s32x4 = vbslq_s32(mask, index_s32x4, max_index_s32x4);\n    }\n    // Find max element within float32x4_t.\n#ifdef __aarch64__\n    max_value = vmaxvq_f32(max_value_f32x4);\n#else\n    float32x2_t max_value_f32x2 = vpmax_f32(vget_low_f32(max_value_f32x4),\n                                            vget_high_f32(max_value_f32x4));\n    max_value_f32x2 = vpmax_f32(max_value_f32x2, max_value_f32x2);\n    max_value = vget_lane_f32(max_value_f32x2, 0);\n#endif  // __aarch64__\n    // Mask indices of non-max values with max int32_t.\n    float32x4_t fill_max_value_f32x4 = vdupq_n_f32(max_value);\n    uint32x4_t mask = vceqq_f32(max_value_f32x4, fill_max_value_f32x4);\n    int32x4_t all_set = vdupq_n_s32(std::numeric_limits<int>::max());\n    max_index_s32x4 = vbslq_s32(mask, max_index_s32x4, all_set);\n    // Find min index of max values.\n#ifdef __aarch64__\n    max_index = vminvq_s32(max_index_s32x4);\n#else\n    int32x2_t max_index_s32x2 = vpmin_s32(vget_low_s32(max_index_s32x4),\n                                          vget_high_s32(max_index_s32x4));\n    max_index_s32x2 = vpmin_s32(max_index_s32x2, max_index_s32x2);\n    max_index = vget_lane_s32(max_index_s32x2, 0);\n#endif  // __aarch64__\n  }\n#endif  // USE_NEON\n  // Leftover loop.\n  for (; i < size; ++i) {\n    const float curr_value = input_data[i];\n    if (curr_value > max_value) {\n      max_value = curr_value;\n      max_index = i;\n    }\n  }\n  return max_index;\n}\n\ntemplate <>\ninline int ArgMaxVector(const int8_t* input_data, int size) {\n  int32_t max_index = 0;\n  int8_t max_value = input_data[0];\n  int32_t i = 0;\n#ifdef USE_NEON\n  constexpr int VECTOR_SIZE = 16;\n  if (size >= VECTOR_SIZE) {\n    int8x16_t max_value_s8x16;\n    for (; i <= size - VECTOR_SIZE; i += VECTOR_SIZE) {\n      max_value_s8x16 = vld1q_s8(input_data + i);\n      int8_t max_from_vec;\n#ifdef __aarch64__\n      max_from_vec = vmaxvq_s8(max_value_s8x16);\n#else   // 32 bit\n      int8x8_t max_val_s8x8 =\n          vpmax_s8(vget_low_s8(max_value_s8x16), vget_high_s8(max_value_s8x16));\n      max_val_s8x8 = vpmax_s8(max_val_s8x8, max_val_s8x8);\n      max_val_s8x8 = vpmax_s8(max_val_s8x8, max_val_s8x8);\n      max_val_s8x8 = vpmax_s8(max_val_s8x8, max_val_s8x8);\n      max_from_vec = vget_lane_s8(max_val_s8x8, 0);\n#endif  // __aarch64__\n      if (max_from_vec > max_value) {\n        max_value = max_from_vec;\n        max_index = i;\n      }\n    }\n  }\n  for (int start_idx = max_index; start_idx < max_index + VECTOR_SIZE;\n       start_idx++) {\n    if (input_data[start_idx] == max_value) {\n      max_index = start_idx;\n      break;\n    }\n  }\n\n#endif  // USE_NEON\n  // Leftover loop.\n  for (; i < size; ++i) {\n    const int8_t curr_value = input_data[i];\n    if (curr_value > max_value) {\n      max_value = curr_value;\n      max_index = i;\n    }\n  }\n\n  return max_index;\n}\n\ntemplate <>\ninline int ArgMaxVector(const uint8_t* input_data, int size) {\n  int32_t max_index = 0;\n  uint8_t max_value = input_data[0];\n  int32_t i = 0;\n#ifdef USE_NEON\n  constexpr int VECTOR_SIZE = 16;\n  if (size >= VECTOR_SIZE) {\n    uint8x16_t max_value_u8x16;\n    for (; i <= size - VECTOR_SIZE; i += VECTOR_SIZE) {\n      max_value_u8x16 = vld1q_u8(input_data + i);\n      uint8_t max_from_vec;\n#ifdef __aarch64__\n      max_from_vec = vmaxvq_u8(max_value_u8x16);\n#else   // 32 bit\n      uint8x8_t max_val_u8x8 =\n          vpmax_u8(vget_low_u8(max_value_u8x16), vget_high_u8(max_value_u8x16));\n      max_val_u8x8 = vpmax_u8(max_val_u8x8, max_val_u8x8);\n      max_val_u8x8 = vpmax_u8(max_val_u8x8, max_val_u8x8);\n      max_val_u8x8 = vpmax_u8(max_val_u8x8, max_val_u8x8);\n      max_from_vec = vget_lane_u8(max_val_u8x8, 0);\n#endif  // __aarch64__\n      if (max_from_vec > max_value) {\n        max_value = max_from_vec;\n        max_index = i;\n      }\n    }\n  }\n  for (int start_idx = max_index; start_idx < max_index + VECTOR_SIZE;\n       start_idx++) {\n    if (input_data[start_idx] == max_value) {\n      max_index = start_idx;\n      break;\n    }\n  }\n\n#endif  // USE_NEON\n  // Leftover loop.\n  for (; i < size; ++i) {\n    const uint8_t curr_value = input_data[i];\n    if (curr_value > max_value) {\n      max_value = curr_value;\n      max_index = i;\n    }\n  }\n\n  return max_index;\n}\n\n// Specializes ArgMinMax function with axis=dims-1.\n// In this case, ArgMinMax reduction is applied on contiguous memory.\ntemplate <typename T1, typename T2, bool is_arg_max>\ninline void ArgMinMaxLastAxis(const RuntimeShape& input_shape,\n                              const T1* input_data,\n                              const RuntimeShape& output_shape,\n                              T2* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_EQ(input_shape.Dims(0), output_shape.Dims(0));\n\n  int outer_size = input_shape.Dims(0);\n  int axis_size = input_shape.Dims(1);\n  for (int outer = 0; outer < outer_size; ++outer) {\n    if (is_arg_max) {\n      output_data[outer] = static_cast<T2>(\n          ArgMaxVector<T1>(input_data + outer * axis_size, axis_size));\n    } else {\n      output_data[outer] = static_cast<T2>(\n          ArgMinVector<T1>(input_data + outer * axis_size, axis_size));\n    }\n  }\n}\n\ntemplate <typename T1, typename T2, typename T3>\ninline void ArgMinMax(const RuntimeShape& input1_shape, const T1* input1_data,\n                      const T3* input2_data, const RuntimeShape& output_shape,\n                      T2* output_data, const bool is_arg_max) {\n  ruy::profiler::ScopeLabel label(\"ArgMinMax\");\n\n  TFLITE_DCHECK_GT(input1_shape.DimensionsCount(), 0);\n  TFLITE_DCHECK_EQ(input1_shape.DimensionsCount() - 1,\n                   output_shape.DimensionsCount());\n  int axis = input2_data[0];\n  if (axis < 0) {\n    axis += input1_shape.DimensionsCount();\n  }\n  const int axis_size = input1_shape.Dims(axis);\n\n  int outer_size = 1;\n  for (int i = 0; i < axis; ++i) {\n    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i));\n    outer_size *= input1_shape.Dims(i);\n  }\n\n  int inner_size = 1;\n  const int dims_count = input1_shape.DimensionsCount();\n  for (int i = axis + 1; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(input1_shape.Dims(i), output_shape.Dims(i - 1));\n    inner_size *= input1_shape.Dims(i);\n  }\n\n  // Call specialized function when axis=dims-1. So far, only float32 is\n  // optimized so reroute to specialized function only when T1 is float32.\n  if (inner_size == 1 &&\n      (std::is_same<T1, float>::value || std::is_same<T1, int8_t>::value ||\n       std::is_same<T1, uint8_t>::value)) {\n    if (is_arg_max) {\n      ArgMinMaxLastAxis<T1, T2, /*is_arg_max=*/true>(\n          {outer_size, axis_size}, input1_data, {outer_size}, output_data);\n    } else {\n      ArgMinMaxLastAxis<T1, T2, /*is_arg_max=*/false>(\n          {outer_size, axis_size}, input1_data, {outer_size}, output_data);\n    }\n    return;\n  }\n\n  reference_ops::ArgMinMax(input1_shape, input1_data, input2_data, output_shape,\n                           output_data, is_arg_max);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const RuntimeShape& input1_shape, const T1* input1_data,\n            const T3* input2_data, const RuntimeShape& output_shape,\n            T2* output_data) {\n  ArgMinMax(input1_shape, input1_data, input2_data, output_shape, output_data,\n            /*is_arg_max=*/true);\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\n// For backward compatibility, reference_ops has ArgMax function.\ntemplate <typename T1, typename T2, typename T3>\ninline void ArgMax(const RuntimeShape& input1_shape, const T1* input1_data,\n                   const RuntimeShape& input2_shape, const T3* input2_data,\n                   const RuntimeShape& output_shape, T2* output_data) {\n  // Drop shape of second input: not needed.\n  ArgMax(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ninline void Conv3D(const Conv3DParams& params, const RuntimeShape& input_shape,\n                   const float* input_data, const RuntimeShape& filter_shape,\n                   const float* filter_data, const RuntimeShape& bias_shape,\n                   const float* bias_data, const RuntimeShape& output_shape,\n                   float* output_data, const RuntimeShape& im2col_shape,\n                   float* im2col_data,\n                   const RuntimeShape& transposed_filter_shape,\n                   float* transposed_filter_data,\n                   CpuBackendContext* cpu_backend_context) {\n  const int stride_depth = params.stride_depth;\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  const int dilation_depth_factor = params.dilation_depth;\n  const int dilation_height_factor = params.dilation_height;\n  const int dilation_width_factor = params.dilation_width;\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n\n  ruy::profiler::ScopeLabel label(\"Conv3D\");\n\n  // NB: the float 0.0f value is represented by all zero bytes.\n  const uint8 float_zero_byte = 0x00;\n  const float* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_depth = filter_shape.Dims(0);\n  const bool need_dilated_im2col = dilation_width_factor != 1 ||\n                                   dilation_height_factor != 1 ||\n                                   dilation_depth_factor != 1;\n  const bool need_im2col = stride_depth != 1 || stride_height != 1 ||\n                           stride_width != 1 || filter_depth != 1 ||\n                           filter_height != 1 || filter_width != 1;\n\n  if (need_dilated_im2col) {\n    DilatedIm2col3D(params, filter_depth, filter_height, filter_width,\n                    float_zero_byte, input_shape, input_data, im2col_shape,\n                    im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    Im2col3D(params, filter_depth, filter_height, filter_width, float_zero_byte,\n             input_shape, input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  // Transpose the filter tensor.\n  TransposeParams transpose_params;\n  transpose_params.perm_count = 5;\n  transpose_params.perm[0] = 4;\n  transpose_params.perm[1] = 0;\n  transpose_params.perm[2] = 1;\n  transpose_params.perm[3] = 2;\n  transpose_params.perm[4] = 3;\n  Transpose<float, 5>(transpose_params, filter_shape, filter_data,\n                      transposed_filter_shape, transposed_filter_data);\n\n  const int gemm_input_dims = gemm_input_shape->DimensionsCount();\n  int m = FlatSizeSkipDim(*gemm_input_shape, gemm_input_dims - 1);\n  int n = output_shape.Dims(4);\n  int k = gemm_input_shape->Dims(gemm_input_dims - 1);\n\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = n;\n  lhs_params.cols = k;\n  cpu_backend_gemm::MatrixParams<float> rhs_params;\n  rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n  rhs_params.rows = k;\n  rhs_params.cols = m;\n  cpu_backend_gemm::MatrixParams<float> dst_params;\n  dst_params.order = cpu_backend_gemm::Order::kColMajor;\n  dst_params.rows = n;\n  dst_params.cols = m;\n  cpu_backend_gemm::GemmParams<float, float> gemm_params;\n  gemm_params.bias = bias_data;\n  gemm_params.clamp_min = output_activation_min;\n  gemm_params.clamp_max = output_activation_max;\n  cpu_backend_gemm::Gemm(lhs_params, transposed_filter_data, rhs_params,\n                         gemm_input_data, dst_params, output_data, gemm_params,\n                         cpu_backend_context);\n}\n\n// Returns in 'im_data' (assumed to be zero-initialized) image patch in storage\n// order (planes, height, width, channel), constructed from patches in\n// 'col_data', which is required to be in storage order (out_planes * out_height\n// * out_width, filter_planes, filter_height, filter_width, in_channel).\n//\n// This function is copied from tensorflow/core/kernels/conv_grad_ops_3d.cc\n// authored by Eugene Zhulenev(ezhulenev).\ntemplate <typename T>\nvoid Col2im(const T* col_data, const int channel, const int planes,\n            const int height, const int width, const int filter_p,\n            const int filter_h, const int filter_w, const int pad_pt,\n            const int pad_t, const int pad_l, const int pad_pb, const int pad_b,\n            const int pad_r, const int stride_p, const int stride_h,\n            const int stride_w, T* im_data) {\n  const int planes_col = (planes + pad_pt + pad_pb - filter_p) / stride_p + 1;\n  const int height_col = (height + pad_t + pad_b - filter_h) / stride_h + 1;\n  const int width_col = (width + pad_l + pad_r - filter_w) / stride_w + 1;\n  int p_pad = -pad_pt;\n  for (int p = 0; p < planes_col; ++p) {\n    int h_pad = -pad_t;\n    for (int h = 0; h < height_col; ++h) {\n      int w_pad = -pad_l;\n      for (int w = 0; w < width_col; ++w) {\n        T* im_patch_data =\n            im_data +\n            (p_pad * height * width + h_pad * width + w_pad) * channel;\n        for (int ip = p_pad; ip < p_pad + filter_p; ++ip) {\n          for (int ih = h_pad; ih < h_pad + filter_h; ++ih) {\n            for (int iw = w_pad; iw < w_pad + filter_w; ++iw) {\n              if (ip >= 0 && ip < planes && ih >= 0 && ih < height && iw >= 0 &&\n                  iw < width) {\n                for (int i = 0; i < channel; ++i) {\n                  im_patch_data[i] += col_data[i];\n                }\n              }\n              im_patch_data += channel;\n              col_data += channel;\n            }\n            // Jump over remaining number of channel.\n            im_patch_data += channel * (width - filter_w);\n          }\n          // Jump over remaining number of (channel * width).\n          im_patch_data += (channel * width) * (height - filter_h);\n        }\n        w_pad += stride_w;\n      }\n      h_pad += stride_h;\n    }\n    p_pad += stride_p;\n  }\n}\n\ntemplate <typename T>\nvoid BiasAdd3D(T* im_data, const T* bias_data, const RuntimeShape& input_shape,\n               float float_activation_min, float float_activation_max) {\n  if (bias_data) {\n    const int outer_size = input_shape.Dims(0) * input_shape.Dims(1) *\n                           input_shape.Dims(2) * input_shape.Dims(3);\n    const int num_channels = input_shape.Dims(4);\n    for (int n = 0; n < outer_size; ++n) {\n      for (int c = 0; c < num_channels; ++c) {\n        im_data[c] = ActivationFunctionWithMinMax(im_data[c] + bias_data[c],\n                                                  float_activation_min,\n                                                  float_activation_max);\n      }\n      im_data += num_channels;\n    }\n  } else {\n    const int flat_size = input_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      im_data[i] = ActivationFunctionWithMinMax(\n          im_data[i], float_activation_min, float_activation_max);\n    }\n  }\n}\n\ninline void Conv3DTranspose(\n    const Conv3DTransposeParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* const output_data, const RuntimeShape& col2im_shape,\n    float* col2im_data, CpuBackendContext* cpu_backend_context) {\n  ruy::profiler::ScopeLabel label(\"Conv3DTranspose/float\");\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK(col2im_data);\n\n  const int batch_size = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_channel = MatchingDim(input_shape, 4, filter_shape, 4);\n  const int output_channel = MatchingDim(output_shape, 4, filter_shape, 3);\n  const int input_spatial_size =\n      input_shape.Dims(1) * input_shape.Dims(2) * input_shape.Dims(3);\n  const int output_spatial_size =\n      output_shape.Dims(1) * output_shape.Dims(2) * output_shape.Dims(3);\n\n  const int output_spatial_dim_1 = output_shape.Dims(1);\n  const int output_spatial_dim_2 = output_shape.Dims(2);\n  const int output_spatial_dim_3 = output_shape.Dims(3);\n  const int input_offset = input_spatial_size * input_channel;\n  const int output_offset = output_spatial_size * output_channel;\n\n  const int filter_spatial_dim_1 = filter_shape.Dims(0);\n  const int filter_spatial_dim_2 = filter_shape.Dims(1);\n  const int filter_spatial_dim_3 = filter_shape.Dims(2);\n\n  const int spatial_dim_1_padding_before = params.padding_values.depth;\n  const int spatial_dim_1_padding_after =\n      params.padding_values.height + params.padding_values.depth_offset;\n  const int spatial_dim_2_padding_before = params.padding_values.height;\n  const int spatial_dim_2_padding_after =\n      params.padding_values.height + params.padding_values.height_offset;\n  const int spatial_dim_3_padding_before = params.padding_values.width;\n  const int spatial_dim_3_padding_after =\n      params.padding_values.width + params.padding_values.width_offset;\n  const int spatial_dim_1_stride = params.stride_depth;\n  const int spatial_dim_2_stride = params.stride_height;\n  const int spatial_dim_3_stride = params.stride_width;\n  const int filter_total_size = filter_spatial_dim_1 * filter_spatial_dim_2 *\n                                filter_spatial_dim_3 * output_channel;\n\n  cpu_backend_gemm::MatrixParams<float> lhs_params;\n  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;\n  lhs_params.rows = filter_total_size;\n  lhs_params.cols = input_channel;\n  float* output_data_p = output_data;\n  std::fill_n(output_data, output_offset * batch_size, 0.0f);\n  for (int i = 0; i < batch_size; ++i) {\n    cpu_backend_gemm::MatrixParams<float> rhs_params;\n    rhs_params.order = cpu_backend_gemm::Order::kColMajor;\n    rhs_params.rows = input_channel;\n    rhs_params.cols = input_spatial_size;\n    cpu_backend_gemm::MatrixParams<float> dst_params;\n    dst_params.order = cpu_backend_gemm::Order::kColMajor;\n    dst_params.rows = filter_total_size;\n    dst_params.cols = input_spatial_size;\n    cpu_backend_gemm::GemmParams<float, float> gemm_params;\n    cpu_backend_gemm::Gemm(lhs_params, filter_data, rhs_params,\n                           input_data + input_offset * i, dst_params,\n                           col2im_data, gemm_params, cpu_backend_context);\n\n    Col2im(col2im_data, output_channel, output_spatial_dim_1,\n           output_spatial_dim_2, output_spatial_dim_3, filter_spatial_dim_1,\n           filter_spatial_dim_2, filter_spatial_dim_3,\n           spatial_dim_1_padding_before, spatial_dim_2_padding_before,\n           spatial_dim_3_padding_before, spatial_dim_1_padding_after,\n           spatial_dim_2_padding_after, spatial_dim_3_padding_after,\n           spatial_dim_1_stride, spatial_dim_2_stride, spatial_dim_3_stride,\n           output_data_p);\n    output_data_p += output_offset;\n  }\n  output_data_p = output_data;\n  BiasAdd3D(output_data_p, bias_data, output_shape, params.float_activation_min,\n            params.float_activation_max);\n}\n\n}  // namespace optimized_ops\n}  // namespace tflite\n\n#if defined OPTIMIZED_OPS_H__IGNORE_DEPRECATED_DECLARATIONS\n#undef OPTIMIZED_OPS_H__IGNORE_DEPRECATED_DECLARATIONS\n#pragma GCC diagnostic pop\n#endif\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_OPTIMIZED_OPS_H_\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_POOLING_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_POOLING_H_\n\n#include <limits>\n#include \"tensorflow/lite/kernels/internal/common.h\"\n\nnamespace tflite {\nnamespace reference_integer_ops {\n\ninline bool AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const int8_t* input_data,\n                        const RuntimeShape& output_shape, int8_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int32_t acc = 0;\n          int filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              acc +=\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              filter_count++;\n            }\n          }\n          if (filter_count == 0) return false;\n          // Round to the closest integer value.\n          acc = acc > 0 ? (acc + filter_count / 2) / filter_count\n                        : (acc - filter_count / 2) / filter_count;\n          acc = std::max(acc, params.quantized_activation_min);\n          acc = std::min(acc, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<int8_t>(acc);\n        }\n      }\n    }\n  }\n  return true;\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const int8_t* input_data, const RuntimeShape& output_shape,\n                    int8_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_GE(params.quantized_activation_min,\n                   std::numeric_limits<int8_t>::min());\n  TFLITE_DCHECK_LE(params.quantized_activation_max,\n                   std::numeric_limits<int8_t>::max());\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int8_t max = std::numeric_limits<int8_t>::lowest();\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              max = std::max(\n                  max,\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);\n            }\n          }\n          max = std::max<int8_t>(max, params.quantized_activation_min);\n          max = std::min<int8_t>(max, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<int8_t>(max);\n        }\n      }\n    }\n  }\n}\n\ninline bool AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const int16_t* input_data,\n                        const RuntimeShape& output_shape,\n                        int16_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int32_t acc = 0;\n          int filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              acc +=\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              filter_count++;\n            }\n          }\n          if (filter_count == 0) return false;\n          // Round to the closest integer value.\n          acc = acc > 0 ? (acc + filter_count / 2) / filter_count\n                        : (acc - filter_count / 2) / filter_count;\n          acc = std::max(acc, params.quantized_activation_min);\n          acc = std::min(acc, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<int16_t>(acc);\n        }\n      }\n    }\n  }\n  return true;\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const int16_t* input_data, const RuntimeShape& output_shape,\n                    int16_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_GE(params.quantized_activation_min,\n                   std::numeric_limits<int16_t>::min());\n  TFLITE_DCHECK_LE(params.quantized_activation_max,\n                   std::numeric_limits<int16_t>::max());\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int16_t max = std::numeric_limits<int16_t>::lowest();\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              max = std::max(\n                  max,\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);\n            }\n          }\n          max = std::max<int16_t>(max, params.quantized_activation_min);\n          max = std::min<int16_t>(max, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<int16_t>(max);\n        }\n      }\n    }\n  }\n}\n\n}  // namespace reference_integer_ops\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_POOLING_H_\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEGACY_REFERENCE_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEGACY_REFERENCE_OPS_H_\n\n#include <stdint.h>\n#include <sys/types.h>\n\n#include \"public/gemmlowp.h\"\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/legacy_types.h\"\n#include \"tensorflow/lite/kernels/internal/reference/conv.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/reference/tanh.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\n\nnamespace reference_ops {\n\nstatic constexpr int kDepthwiseReverseShift = -1;\n\ninline void ShapeFromDims(const tflite::Dims<4>& dims, RuntimeShape* shape) {\n  shape->BuildFrom(\n      {dims.sizes[3], dims.sizes[2], dims.sizes[1], dims.sizes[0]});\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  DepthwiseConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                bias_data, DimsToShape(output_dims), output_data);\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, 1, 1, pad_width,\n                pad_height, depth_multiplier, output_activation_min,\n                output_activation_max, output_data, output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, float* output_data,\n                   const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, pad_width, pad_height,\n                depth_multiplier, output_activation_min, output_activation_max,\n                output_data, output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   float* output_data, const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n                    bias_dims, stride, stride, pad_width, pad_height,\n                    depth_multiplier, output_data, output_dims);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kDepthwiseReverseShift * output_shift;\n\n  DepthwiseConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                bias_data, DimsToShape(output_dims), output_data);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, 1, 1, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, int32 output_offset,\n                   int32 output_multiplier, int output_shift,\n                   int32 output_activation_min, int32 output_activation_max,\n                   uint8* output_data, const Dims<4>& output_dims) {\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   int32 output_offset, int32 output_multiplier,\n                   int output_shift, int32 output_activation_min,\n                   int32 output_activation_max, uint8* output_data,\n                   const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, input_offset, filter_data,\n                    filter_dims, filter_offset, bias_data, bias_dims, stride,\n                    stride, pad_width, pad_height, depth_multiplier,\n                    output_offset, output_multiplier, output_shift,\n                    output_activation_min, output_activation_max, output_data,\n                    output_dims);\n}\n\ninline void Conv(const float* input_data, const Dims<4>& input_dims,\n                 const float* filter_data, const Dims<4>& filter_dims,\n                 const float* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 float output_activation_min, float output_activation_max,\n                 float* output_data, const Dims<4>& output_dims,\n                 float* im2col_data, const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int dilation_width_factor,\n          int dilation_height_factor, int pad_width, int pad_height,\n          float* output_data, const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, dilation_width_factor,\n       dilation_height_factor, pad_width, pad_height, output_activation_min,\n       output_activation_max, output_data, output_dims, im2col_data,\n       im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, 1, 1, pad_width, pad_height,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  Conv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n           bias_dims, stride, stride, 1, 1, pad_width, pad_height, output_data,\n           output_dims, im2col_data, im2col_dims);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 int32 output_offset, int32 output_multiplier, int output_shift,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims,\n                 uint8* im2col_data, const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data, gemmlowp_context);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height, 1, 1,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const uint8* input_data, const Dims<4>& input_dims,\n          int32 input_offset, const uint8* filter_data,\n          const Dims<4>& filter_dims, int32 filter_offset,\n          const int32* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, int32 output_offset,\n          int32 output_multiplier, int output_shift,\n          int32 output_activation_min, int32 output_activation_max,\n          uint8* output_data, const Dims<4>& output_dims, uint8* im2col_data,\n          const Dims<4>& im2col_dims, gemmlowp::GemmContext* gemmlowp_context) {\n  Conv<Ac>(input_data, input_dims, input_offset, filter_data, filter_dims,\n           filter_offset, bias_data, bias_dims, stride, stride, pad_width,\n           pad_height, output_offset, output_multiplier, output_shift,\n           output_activation_min, output_activation_max, output_data,\n           output_dims, im2col_data, im2col_dims, gemmlowp_context);\n}\n\ninline void TransposeConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, float* output_data,\n                          const Dims<4>& output_dims, float* im2col_data,\n                          const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n\n  TransposeConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data,\n                /*bias_shape*/ RuntimeShape(), /*bias*/ nullptr,\n                DimsToShape(output_dims), output_data, DimsToShape(im2col_dims),\n                im2col_data);\n}\n\ninline void TransposeConv(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& output_shape,\n    float* output_data, const RuntimeShape& im2col_shape, float* im2col_data) {\n  TransposeConv(params, input_shape, input_data, filter_shape, filter_data,\n                /*bias_shape*/ RuntimeShape(), /*bias*/ nullptr, output_shape,\n                output_data, im2col_shape, im2col_data);\n}\n\ninline void FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                           const float* weights_data,\n                           const Dims<4>& weights_dims, const float* bias_data,\n                           const Dims<4>& bias_dims,\n                           float output_activation_min,\n                           float output_activation_max, float* output_data,\n                           const Dims<4>& output_dims) {\n  tflite::FullyConnectedParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(weights_dims), weights_data,\n                 DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                    const float* weights_data, const Dims<4>& weights_dims,\n                    const float* bias_data, const Dims<4>& bias_dims,\n                    float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  FullyConnected(input_data, input_dims, weights_data, weights_dims, bias_data,\n                 bias_dims, output_activation_min, output_activation_max,\n                 output_data, output_dims);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext*) {\n  FullyConnected(params, input_shape, input_data, filter_shape, filter_data,\n                 bias_shape, bias_data, output_shape, output_data);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, gemmlowp::GemmContext*) {\n  FullyConnected(params, input_shape, input_data, filter_shape, filter_data,\n                 bias_shape, bias_data, output_shape, output_data);\n}\n\ninline void FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                           int32 input_offset, const uint8* filter_data,\n                           const Dims<4>& filter_dims, int32 filter_offset,\n                           const int32* bias_data, const Dims<4>& bias_dims,\n                           int32 output_offset, int32 output_multiplier,\n                           int output_shift, int32 output_activation_min,\n                           int32 output_activation_max, uint8* output_data,\n                           const Dims<4>& output_dims,\n                           gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\ninline void FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                           int32 input_offset, const uint8* filter_data,\n                           const Dims<4>& filter_dims, int32 filter_offset,\n                           const int32* bias_data, const Dims<4>& bias_dims,\n                           int32 output_offset, int32 output_multiplier,\n                           int output_shift, int32 output_activation_min,\n                           int32 output_activation_max, int16* output_data,\n                           const Dims<4>& output_dims,\n                           gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\ninline void ShuffledFullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& weights_shape,\n    const uint8* shuffled_weights_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext*) {\n  ShuffledFullyConnected(params, input_shape, input_data, weights_shape,\n                         shuffled_weights_data, bias_shape, bias_data,\n                         output_shape, output_data,\n                         shuffled_input_workspace_data);\n}\n\ninline void ShuffledFullyConnected(\n    const uint8* input_data, const Dims<4>& input_dims,\n    const uint8* shuffled_weights_data, const Dims<4>& weights_dims,\n    const int32* bias_data, const Dims<4>& bias_dims, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    int16* output_data, const Dims<4>& output_dims,\n    uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  ShuffledFullyConnected(op_params, DimsToShape(input_dims), input_data,\n                         DimsToShape(weights_dims), shuffled_weights_data,\n                         DimsToShape(bias_dims), bias_data,\n                         DimsToShape(output_dims), output_data,\n                         shuffled_input_workspace_data, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_offset, const uint8* filter_data,\n                    const Dims<4>& filter_dims, int32 filter_offset,\n                    const int32* bias_data, const Dims<4>& bias_dims,\n                    int32 output_offset, int32 output_multiplier,\n                    int output_shift, int32 output_activation_min,\n                    int32 output_activation_max, uint8* output_data,\n                    const Dims<4>& output_dims,\n                    gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  FullyConnected(input_data, input_dims, input_offset, filter_data, filter_dims,\n                 filter_offset, bias_data, bias_dims, output_offset,\n                 output_multiplier, output_shift, output_activation_min,\n                 output_activation_max, output_data, output_dims,\n                 gemmlowp_context);\n}\n\ninline void LstmCell(const float* input_data, const Dims<4>& input_dims,\n                     const float* prev_activ_data,\n                     const Dims<4>& prev_activ_dims, const float* weights_data,\n                     const Dims<4>& weights_dims, const float* bias_data,\n                     const Dims<4>& bias_dims, const float* prev_state_data,\n                     const Dims<4>& prev_state_dims, float* output_state_data,\n                     const Dims<4>& output_state_dims, float* output_activ_data,\n                     const Dims<4>& output_activ_dims, float* concat_temp_data,\n                     const Dims<4>& concat_temp_dims, float* activ_temp_data,\n                     const Dims<4>& activ_temp_dims) {\n  tflite::LstmCellParams op_params;\n  // Float LSTM cell does not need parameters to be set: leave untouched.\n\n  LstmCell(op_params, DimsToShape(input_dims), input_data,\n           DimsToShape(prev_activ_dims), prev_activ_data,\n           DimsToShape(weights_dims), weights_data, DimsToShape(bias_dims),\n           bias_data, DimsToShape(prev_state_dims), prev_state_data,\n           DimsToShape(output_state_dims), output_state_data,\n           DimsToShape(output_activ_dims), output_activ_data,\n           DimsToShape(concat_temp_dims), concat_temp_data,\n           DimsToShape(activ_temp_dims), activ_temp_data);\n}\n\ntemplate <int StateIntegerBits>\nvoid LstmCell(const uint8* input_data_uint8, const Dims<4>& input_dims,\n              const uint8* prev_activ_data_uint8,\n              const Dims<4>& prev_activ_dims, const uint8* weights_data_uint8,\n              const Dims<4>& weights_dims, const int32* bias_data_int32,\n              const Dims<4>& bias_dims, const int16* prev_state_data_int16,\n              const Dims<4>& prev_state_dims, int16* output_state_data_int16,\n              const Dims<4>& output_state_dims, uint8* output_activ_data_uint8,\n              const Dims<4>& output_activ_dims, uint8* concat_temp_data_uint8,\n              const Dims<4>& concat_temp_dims, int16* activ_temp_data_int16,\n              const Dims<4>& activ_temp_dims, int32 weights_zero_point,\n              int32 accum_multiplier, int accum_shift,\n              gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::LstmCellParams op_params;\n  op_params.weights_zero_point = weights_zero_point;\n  op_params.accum_multiplier = accum_multiplier;\n  op_params.accum_shift = accum_shift;\n\n  LstmCell<StateIntegerBits>(\n      op_params, DimsToShape(input_dims), input_data_uint8,\n      DimsToShape(prev_activ_dims), prev_activ_data_uint8,\n      DimsToShape(weights_dims), weights_data_uint8, DimsToShape(bias_dims),\n      bias_data_int32, DimsToShape(prev_state_dims), prev_state_data_int16,\n      DimsToShape(output_state_dims), output_state_data_int16,\n      DimsToShape(output_activ_dims), output_activ_data_uint8,\n      DimsToShape(concat_temp_dims), concat_temp_data_uint8,\n      DimsToShape(activ_temp_dims), activ_temp_data_int16, gemmlowp_context);\n}\n\ntemplate <typename T>\nvoid BroadcastDiv(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastDivSlow(op_params, DimsToShape(input1_dims), input1_data,\n                   DimsToShape(input2_dims), input2_data,\n                   DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Div(const T* input1_data, const Dims<4>& input1_dims,\n                const T* input2_data, const Dims<4>& input2_dims,\n                T output_activation_min, T output_activation_max,\n                T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  Div(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac, typename Scalar>\ninline void Concatenation(int concat_dim, const Scalar* const* input_data,\n                          const Dims<4>* const* input_dims, int inputs_count,\n                          Scalar* output_data, const Dims<4>& output_dims) {\n  // For now we don't have a model with a Concatenation with fused activation.\n  TFLITE_DCHECK_EQ(Ac, FusedActivationFunctionType::kNone);\n\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::ConcatenationParams op_params;\n  op_params.axis = 3 - concat_dim;\n  op_params.inputs_count = inputs_count;\n\n  Concatenation(op_params, input_shapes_indirect.data(), input_data,\n                DimsToShape(output_dims), output_data);\n}\n\ninline void Concatenation(int concat_dim, const uint8* const* input_data,\n                          const Dims<4>* const* input_dims,\n                          const int32* input_zeropoint,\n                          const float* input_scale, int inputs_count,\n                          uint8* output_data, const Dims<4>& output_dims,\n                          const int32 output_zeropoint,\n                          const float output_scale) {\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::ConcatenationParams op_params;\n  op_params.axis = 3 - concat_dim;\n  op_params.input_zeropoint = input_zeropoint;\n  op_params.input_scale = input_scale;\n  op_params.inputs_count = inputs_count;\n  op_params.output_zeropoint = output_zeropoint;\n  op_params.output_scale = output_scale;\n\n  ConcatenationWithScaling(op_params, input_shapes_indirect.data(), input_data,\n                           DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac, typename Scalar>\nvoid DepthConcatenation(const Scalar* const* input_data,\n                        const Dims<4>* const* input_dims, int inputs_count,\n                        Scalar* output_data, const Dims<4>& output_dims) {\n  // For now we don't have a model with a Concatenation with fused activation.\n  TFLITE_DCHECK_EQ(Ac, FusedActivationFunctionType::kNone);\n\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::ConcatenationParams op_params;\n  op_params.inputs_count = inputs_count;\n\n  DepthConcatenation(op_params, input_shapes_indirect.data(), input_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename Scalar>\nvoid TensorFlowSplit(const Scalar* input_data, const Dims<4>& input_dims,\n                     int axis, int outputs_count, Scalar* const* output_data,\n                     const Dims<4>* const* output_dims) {\n  std::vector<RuntimeShape> output_shapes(outputs_count);\n  std::vector<const RuntimeShape*> output_shapes_indirect(outputs_count);\n  for (int i = 0; i < outputs_count; ++i) {\n    ShapeFromDims(*output_dims[i], &output_shapes[i]);\n    output_shapes_indirect[i] = &output_shapes[i];\n  }\n  tflite::SplitParams op_params;\n  op_params.axis = 3 - axis;\n  op_params.num_split = outputs_count;\n\n  Split(op_params, DimsToShape(input_dims), input_data,\n        output_shapes_indirect.data(), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac, typename Scalar>\nvoid TensorFlowSplit(const Scalar* input_data, const Dims<4>& input_dims,\n                     int outputs_count, Scalar* const* output_data,\n                     const Dims<4>* const* output_dims) {\n  TFLITE_DCHECK_GE(outputs_count, 1);\n  for (int i = 0; i < outputs_count; i++) {\n    /* batches = */ MatchingArraySize(*output_dims[i], 3, input_dims, 3);\n    /* height = */ MatchingArraySize(*output_dims[i], 2, input_dims, 2);\n    /* width = */ MatchingArraySize(*output_dims[i], 1, input_dims, 1);\n  }\n  // For now we don't have a model with a Split with fused activation.\n  TFLITE_DCHECK_EQ(Ac, FusedActivationFunctionType::kNone);\n\n  TensorFlowSplit(input_data, input_dims, /*axis=*/0, outputs_count,\n                  output_data, output_dims);\n}\n\ninline void Softmax(const float* input_data, const RuntimeShape& input_shape,\n                    float beta, float* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.beta = beta;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Softmax(const uint8* input_data, const RuntimeShape& input_shape,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_beta_multiplier;\n  params.input_left_shift = input_beta_left_shift;\n  params.diff_min = diff_min;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void LogSoftmax(const float* input_data, const RuntimeShape& input_shape,\n                       float* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  // No params currently used for float LogSoftmax.\n  LogSoftmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void LogSoftmax(const uint8* input_data, const RuntimeShape& input_shape,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  params.reverse_scaling_divisor = reverse_scaling_divisor;\n  params.reverse_scaling_right_shift = reverse_scaling_right_shift;\n  params.diff_min = diff_min;\n  LogSoftmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const LogisticParams& params,\n                     const RuntimeShape& input_shape, const uint8* input_data,\n                     const RuntimeShape& output_shape, uint8* output_data) {\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int input_left_shift = params.input_left_shift;\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    const uint8 input_val_u8 = input_data[i];\n    const int32 input_val_centered =\n        static_cast<int32>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered <= -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered >= input_range_radius) {\n      output_val = 255;\n    } else {\n      const int32 input_val_rescaled =\n          MultiplyByQuantizedMultiplierGreaterThanOne(\n              input_val_centered, input_multiplier, input_left_shift);\n      using FixedPoint4 = gemmlowp::FixedPoint<int32, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      // Convert from Q0.31 to Q23.8.\n      using gemmlowp::RoundingDivideByPOT;\n      int32 output_val_s32 = RoundingDivideByPOT(output_val_f0.raw(), 23);\n      if (output_val_s32 == 256) {\n        output_val_s32 = 255;\n      }\n      // Reinterpret as U0.8.\n      TFLITE_DCHECK_GE(output_val_s32, 0);\n      TFLITE_DCHECK_LE(output_val_s32, 255);\n      output_val = static_cast<uint8>(output_val_s32);\n    }\n    output_data[i] = output_val;\n  }\n}\n\ninline void Logistic(const uint8* input_data, const RuntimeShape& input_shape,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const RuntimeShape& output_shape) {\n  LogisticParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const RuntimeShape& input_shape, const int16* input_data,\n                     const RuntimeShape& output_shape, int16* output_data) {\n  LogisticParams params;\n  // No params currently needed by int16 Logistic.\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const uint8* input_data, const RuntimeShape& input_shape,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const int16* input_data, const RuntimeShape& input_shape,\n                 int input_left_shift, int16* output_data,\n                 const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Dequantize(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 zero_point, double scale, float* output_data,\n                       const Dims<4>& output_dims) {\n  tflite::DequantizationParams op_params;\n  op_params.zero_point = zero_point;\n  op_params.scale = scale;\n\n  Dequantize(op_params, DimsToShape(input_dims), input_data,\n             DimsToShape(output_dims), output_data);\n}\n\ninline void FakeQuant(const float* input_data, const Dims<4>& input_dims,\n                      float rmin, float rmax, int num_bits, float* output_data,\n                      const Dims<4>& output_dims) {\n  tflite::FakeQuantParams op_params;\n  op_params.num_bits = num_bits;\n  op_params.minmax.min = rmin;\n  op_params.minmax.max = rmax;\n\n  FakeQuant(op_params, DimsToShape(input_dims), input_data,\n            DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Gather(const T* input_data, const Dims<4>& input_dims,\n                   int input_rank, const int32* coords_data,\n                   const Dims<4>& coords_dims, T* output_data,\n                   const Dims<4>& output_dims) {\n  tflite::GatherParams op_params;\n  op_params.axis = 4 - input_rank;\n  op_params.batch_dims = 0;\n\n  Gather(op_params, DimsToShape(input_dims), input_data,\n         DimsToShape(coords_dims), coords_data, DimsToShape(output_dims),\n         output_data);\n}\n\ninline uint32 LegacyReverseBits32(uint32 n) {\n  n = ((n >> 1) & 0x55555555) | ((n & 0x55555555) << 1);\n  n = ((n >> 2) & 0x33333333) | ((n & 0x33333333) << 2);\n  n = ((n >> 4) & 0x0F0F0F0F) | ((n & 0x0F0F0F0F) << 4);\n  return (((n & 0xFF) << 24) | ((n & 0xFF00) << 8) | ((n & 0xFF0000) >> 8) |\n          ((n & 0xFF000000) >> 24));\n}\n\ninline void StridedSliceReverseIndices(tflite::StridedSliceParams* p) {\n  TFLITE_CHECK_EQ(p->start_indices_count, p->stop_indices_count);\n  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);\n\n  std::reverse(p->start_indices, p->start_indices + p->start_indices_count);\n  std::reverse(p->stop_indices, p->stop_indices + p->stop_indices_count);\n  std::reverse(p->strides, p->strides + p->strides_count);\n\n  p->begin_mask = LegacyReverseBits32(static_cast<uint32>(p->begin_mask)) >>\n                  (32 - p->start_indices_count);\n  p->ellipsis_mask =\n      LegacyReverseBits32(static_cast<uint32>(p->ellipsis_mask)) >>\n      (32 - p->start_indices_count);\n  p->end_mask = LegacyReverseBits32(static_cast<uint32>(p->end_mask)) >>\n                (32 - p->start_indices_count);\n  p->new_axis_mask =\n      LegacyReverseBits32(static_cast<uint32>(p->new_axis_mask)) >>\n      (32 - p->start_indices_count);\n  p->shrink_axis_mask =\n      LegacyReverseBits32(static_cast<uint32>(p->shrink_axis_mask)) >>\n      (32 - p->start_indices_count);\n}\n\ntemplate <typename T>\ninline void StridedSlice(const T* input_data, const Dims<4>& input_dims,\n                         int begin_mask, int end_mask, int shrink_axis_mask,\n                         const std::vector<int>& start_indices,\n                         const std::vector<int>& stop_indices,\n                         const std::vector<int>& strides, T* output_data,\n                         const Dims<4>& output_dims) {\n  TFLITE_DCHECK_EQ(start_indices.size(), 4);\n  auto op_params = strided_slice::BuildStridedSliceParams(\n      begin_mask, end_mask, shrink_axis_mask, start_indices, stop_indices,\n      strides);\n  StridedSliceReverseIndices(&op_params);\n\n  StridedSlice(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Mean(const T* input_data, const Dims<4>& input_dims,\n                 const std::vector<int>& reduction_indices, T* output_data,\n                 const Dims<4>& output_dims) {\n  tflite::MeanParams op_params;\n  op_params.axis_count = reduction_indices.size();\n  for (int i = 0; i < op_params.axis_count; ++i) {\n    op_params.axis[i] = reduction_indices[op_params.axis_count - 1 - i];\n  }\n\n  Mean(op_params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ntemplate <typename T>\nvoid Transpose(const T* input, const Dims<4>& input_dims, T* output,\n               const Dims<4>& output_dims, const int* permuted_axes) {\n  TransposeParams params;\n  params.perm_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    params.perm[i] = 3 - permuted_axes[3 - i];\n  }\n  Transpose(params, DimsToShape(input_dims), input, DimsToShape(output_dims),\n            output);\n}\n\ntemplate <typename T, ComparisonFn<T> F>\ninline void Comparison(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, const Dims<4>& input2_dims,\n                       bool* output_data, const Dims<4>& output_dims) {\n  ComparisonParams op_params;\n  // No parameters needed.\n  ComparisonImpl<T, F>(op_params, DimsToShape(input1_dims), input1_data,\n                       DimsToShape(input2_dims), input2_data,\n                       DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, ComparisonFn<int32> F>\ninline void Comparison(int left_shift, const T* input1_data,\n                       const Dims<4>& input1_dims, int32 input1_offset,\n                       int32 input1_multiplier, int input1_shift,\n                       const T* input2_data, const Dims<4>& input2_dims,\n                       int32 input2_offset, int32 input2_multiplier,\n                       int input2_shift, bool* output_data,\n                       const Dims<4>& output_dims) {\n  tflite::ComparisonParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input2_shift = kReverseShift * input2_shift;\n\n  ComparisonWithScaling<T, F>(op_params, DimsToShape(input1_dims), input1_data,\n                              DimsToShape(input2_dims), input2_data,\n                              DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, ComparisonFn<T> F>\ninline void BroadcastComparison(const T* input1_data,\n                                const Dims<4>& input1_dims,\n                                const T* input2_data,\n                                const Dims<4>& input2_dims, bool* output_data,\n                                const Dims<4>& output_dims) {\n  ComparisonParams op_params;\n  // No parameters needed.\n  BroadcastComparison4DSlowImpl<T, F>(op_params, DimsToShape(input1_dims),\n                                      input1_data, DimsToShape(input2_dims),\n                                      input2_data, DimsToShape(output_dims),\n                                      output_data);\n}\n\ntemplate <typename T, ComparisonFn<int32> F>\ninline void BroadcastComparison(int left_shift, const T* input1_data,\n                                const Dims<4>& input1_dims, int32 input1_offset,\n                                int32 input1_multiplier, int input1_shift,\n                                const T* input2_data,\n                                const Dims<4>& input2_dims, int32 input2_offset,\n                                int32 input2_multiplier, int input2_shift,\n                                bool* output_data, const Dims<4>& output_dims) {\n  ComparisonParams op_params;\n\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input2_shift = kReverseShift * input2_shift;\n\n  BroadcastComparison4DSlowWithScaling<T, F>(\n      op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\n#define TFLITE_LEGACY_COMPARISON_OP(name)                                     \\\n  template <typename T>                                                       \\\n  inline void name(const T* input1_data, const Dims<4>& input1_dims,          \\\n                   const T* input2_data, const Dims<4>& input2_dims,          \\\n                   bool* output_data, const Dims<4>& output_dims) {           \\\n    ruy::profiler::ScopeLabel label(#name);                                   \\\n    Comparison<T, name##Fn>(input1_data, input1_dims, input2_data,            \\\n                            input2_dims, output_data, output_dims);           \\\n  }                                                                           \\\n  template <typename T>                                                       \\\n  inline void name(                                                           \\\n      int left_shift, const T* input1_data, const Dims<4>& input1_dims,       \\\n      int32 input1_offset, int32 input1_multiplier, int input1_shift,         \\\n      const T* input2_data, const Dims<4>& input2_dims, int32 input2_offset,  \\\n      int32 input2_multiplier, int input2_shift, bool* output_data,           \\\n      const Dims<4>& output_dims) {                                           \\\n    ruy::profiler::ScopeLabel label(#name \"/8bit\");                           \\\n    Comparison<T, name##Fn>(left_shift, input1_data, input1_dims,             \\\n                            input1_offset, input1_multiplier, input1_shift,   \\\n                            input2_data, input2_dims, input2_offset,          \\\n                            input2_multiplier, input2_shift, output_data,     \\\n                            output_dims);                                     \\\n  }                                                                           \\\n  template <typename T>                                                       \\\n  inline void Broadcast##name(                                                \\\n      const T* input1_data, const Dims<4>& input1_dims, const T* input2_data, \\\n      const Dims<4>& input2_dims, bool* output_data,                          \\\n      const Dims<4>& output_dims) {                                           \\\n    ruy::profiler::ScopeLabel label(\"Broadcast\" #name);                       \\\n    BroadcastComparison<T, name##Fn>(input1_data, input1_dims, input2_data,   \\\n                                     input2_dims, output_data, output_dims);  \\\n  }                                                                           \\\n  template <typename T>                                                       \\\n  inline void Broadcast##name(                                                \\\n      int left_shift, const T* input1_data, const Dims<4>& input1_dims,       \\\n      int32 input1_offset, int32 input1_multiplier, int input1_shift,         \\\n      const T* input2_data, const Dims<4>& input2_dims, int32 input2_offset,  \\\n      int32 input2_multiplier, int input2_shift, bool* output_data,           \\\n      const Dims<4>& output_dims) {                                           \\\n    ruy::profiler::ScopeLabel label(\"Broadcast\" #name \"/8bit\");               \\\n    BroadcastComparison<T, name##Fn>(left_shift, input1_data, input1_dims,    \\\n                                     input1_offset, input1_multiplier,        \\\n                                     input1_shift, input2_data, input2_dims,  \\\n                                     input2_offset, input2_multiplier,        \\\n                                     input2_shift, output_data, output_dims); \\\n  }\nTFLITE_LEGACY_COMPARISON_OP(Equal);\nTFLITE_LEGACY_COMPARISON_OP(NotEqual);\nTFLITE_LEGACY_COMPARISON_OP(Greater);\nTFLITE_LEGACY_COMPARISON_OP(GreaterEqual);\nTFLITE_LEGACY_COMPARISON_OP(Less);\nTFLITE_LEGACY_COMPARISON_OP(LessEqual);\n#undef TFLITE_LEGACY_COMPARISON_OP\n\ntemplate <typename D, typename T>\ninline void Select(const D* input_condition_data,\n                   const Dims<4>& input_condition_dims, const T* input_x_data,\n                   const Dims<4>& input_x_dims, const T* input_y_data,\n                   const Dims<4>& input_y_dims, T* output_data,\n                   const Dims<4>& output_dims) {\n  Select(DimsToShape(input_condition_dims), input_condition_data,\n         DimsToShape(input_x_dims), input_x_data, DimsToShape(input_y_dims),\n         input_y_data, DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename D, typename T>\ninline void RankOneSelect(const D* input_condition_data,\n                          const Dims<4>& input_condition_dims,\n                          const T* input_x_data, const Dims<4>& input_x_dims,\n                          const T* input_y_data, const Dims<4>& input_y_dims,\n                          T* output_data, const Dims<4>& output_dims) {\n  RankOneSelect(DimsToShape(input_condition_dims), input_condition_data,\n                DimsToShape(input_x_dims), input_x_data,\n                DimsToShape(input_y_dims), input_y_data,\n                DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, typename TI>\ninline void SparseToDense(const std::vector<std::vector<TI>>& indices,\n                          const T* values, T default_value, T* output_data,\n                          const Dims<4>& output_dims, bool value_is_scalar) {\n  SparseToDense(indices, values, default_value, value_is_scalar,\n                DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename Scalar>\nvoid Pack(int dim, const Scalar* const* input_data,\n          const Dims<4>* const* input_dims, int inputs_count,\n          Scalar* output_data, const Dims<4>& output_dims) {\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::PackParams op_params;\n  op_params.axis = 3 - dim;\n  op_params.inputs_count = inputs_count;\n\n  Pack(op_params, input_shapes_indirect.data(), input_data,\n       DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename Scalar>\nvoid Unpack(int axis, const Scalar* input_data, const Dims<4>& input_dims,\n            int dimensions, int outputs_count, Scalar* const* output_datas,\n            const Dims<4>& output_dims) {\n  tflite::UnpackParams op_params;\n  op_params.axis = 3 - axis;\n  op_params.num_split = outputs_count;\n\n  Unpack(op_params, DimsToShape(input_dims), input_data,\n         DimsToShape(output_dims), output_datas);\n}\n\ntemplate <typename Scalar>\nvoid Pack(int dim, const Scalar* const* input_data,\n          const Dims<4>* const* input_dims, const int32* input_zeropoint,\n          const float* input_scale, int inputs_count, Scalar* output_data,\n          const Dims<4>& output_dims, const int32 output_zeropoint,\n          const float output_scale) {\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::PackParams op_params;\n  op_params.axis = 3 - dim;\n  op_params.input_zeropoint = input_zeropoint;\n  op_params.input_scale = input_scale;\n  op_params.inputs_count = inputs_count;\n  op_params.output_zeropoint = output_zeropoint;\n  op_params.output_scale = output_scale;\n\n  PackWithScaling(op_params, input_shapes_indirect.data(), input_data,\n                  DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const RuntimeShape& input_shape,\n                     float* output_data, const RuntimeShape& output_shape) {\n  static_assert(Ac == FusedActivationFunctionType::kNone, \"\");\n  tflite::L2NormalizationParams op_params;\n  // No params need to be set for float.\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ninline void L2Normalization(const uint8* input_data,\n                            const RuntimeShape& input_shape,\n                            int32 input_zero_point, uint8* output_data,\n                            const RuntimeShape& output_shape) {\n  tflite::L2NormalizationParams op_params;\n  op_params.input_zero_point = input_zero_point;\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  L2Normalization<Ac>(input_data, DimsToShape(input_dims), output_data,\n                      DimsToShape(output_dims));\n}\n\ninline void L2Normalization(const uint8* input_data, const Dims<4>& input_dims,\n                            int32 input_zero_point, uint8* output_data,\n                            const Dims<4>& output_dims) {\n  L2Normalization(input_data, DimsToShape(input_dims), input_zero_point,\n                  output_data, DimsToShape(output_dims));\n}\n\ninline void Relu(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Relu(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Relu1(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Relu1(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ninline void Relu6(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Relu6(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ninline void ReluX(uint8 min_value, uint8 max_value, const uint8* input_data,\n                  const RuntimeShape& input_shape, uint8* output_data,\n                  const RuntimeShape& output_shape) {\n  tflite::ActivationParams params;\n  params.quantized_activation_max = max_value;\n  params.quantized_activation_min = min_value;\n  ReluX(params, input_shape, input_data, output_shape, output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(int left_shift, const uint8* input1_data,\n                const Dims<4>& input1_dims, int32 input1_offset,\n                int32 input1_multiplier, int input1_shift,\n                const uint8* input2_data, const Dims<4>& input2_dims,\n                int32 input2_offset, int32 input2_multiplier, int input2_shift,\n                int32 output_offset, int32 output_multiplier, int output_shift,\n                int32 output_activation_min, int32 output_activation_max,\n                uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const int32* input1_data, const Dims<4>& input1_dims,\n         const int32* input2_data, const Dims<4>& input2_dims,\n         int32* output_data, const Dims<4>& output_dims) {\n  ruy::profiler::ScopeLabel label(\"Add/int32\");\n  TFLITE_DCHECK(Ac == FusedActivationFunctionType::kNone);\n\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = std::numeric_limits<int32>::min();\n  op_params.quantized_activation_max = std::numeric_limits<int32>::max();\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAdd(int left_shift, const uint8* input1_data,\n                         const Dims<4>& input1_dims, int32 input1_offset,\n                         int32 input1_multiplier, int input1_shift,\n                         const uint8* input2_data, const Dims<4>& input2_dims,\n                         int32 input2_offset, int32 input2_multiplier,\n                         int input2_shift, int32 output_offset,\n                         int32 output_multiplier, int output_shift,\n                         int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAddFivefold(\n    int y0, int y1, int y2, int y3, int y4, int left_shift,\n    const uint8* input1_data, const Dims<4>& input1_dims, int32 input1_offset,\n    int32 input1_multiplier, int input1_shift, const uint8* input2_data,\n    const Dims<4>& input2_dims, int32 input2_offset, int32 input2_multiplier,\n    int input2_shift, int32 output_offset, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  tflite::ArithmeticParams op_params;\n  op_params.broadcast_category =\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.broadcast_shape[4] = y0;\n  op_params.broadcast_shape[3] = y1;\n  op_params.broadcast_shape[2] = y2;\n  op_params.broadcast_shape[1] = y3;\n  op_params.broadcast_shape[0] = y4;\n  BroadcastAddFivefold(op_params, DimsToShape(input1_dims), input1_data,\n                       DimsToShape(input2_dims), input2_data,\n                       DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T* output_data, const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  BroadcastAdd(input1_data, input1_dims, input2_data, input2_dims,\n               output_activation_min, output_activation_max, output_data,\n               output_dims);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(const int16* input1_data, const Dims<4>& input1_dims,\n                int input1_shift, const int16* input2_data,\n                const Dims<4>& input2_dims, int input2_shift,\n                int16 output_activation_min, int16 output_activation_max,\n                int16* output_data, const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, -32768);\n    TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Sub(const float* input1_data, const Dims<4>& input1_dims,\n                const float* input2_data, const Dims<4>& input2_dims,\n                float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(FusedActivationFunctionType::kNone,\n                      &output_activation_min, &output_activation_max);\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid Sub(const T* input1_data, const Dims<4>& input1_dims, const T* input2_data,\n         const Dims<4>& input2_dims, T* output_data,\n         const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = std::numeric_limits<T>::min();\n  op_params.quantized_activation_max = std::numeric_limits<T>::max();\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int kwidth, int kheight,\n                        float output_activation_min,\n                        float output_activation_max, float* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  return AveragePool(params, DimsToShape(input_dims), input_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// Transitional version that will be moved shortly to legacy_reference_ops, as\n// part of RuntimeShape revisions.\ninline void BroadcastMul4DSlow(const uint8* input1_data,\n                               const Dims<4>& input1_dims, int32 input1_offset,\n                               const uint8* input2_data,\n                               const Dims<4>& input2_dims, int32 input2_offset,\n                               int32 output_offset, int32 output_multiplier,\n                               int output_shift, int32 output_activation_min,\n                               int32 output_activation_max, uint8* output_data,\n                               const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n  op_params.input1_offset = input1_offset;\n  op_params.input2_offset = input2_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = output_shift;\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  BroadcastMul4DSlow(\n      input1_data, input1_dims, input1_offset, input2_data, input2_dims,\n      input2_offset, output_offset, output_multiplier,\n      //\n      kReverseShift * output_shift,\n      //\n      output_activation_min, output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  BroadcastMul(input1_data, input1_dims, input1_offset, input2_data,\n               input2_dims, input2_offset, output_offset, output_multiplier,\n               output_shift, output_activation_min, output_activation_max,\n               output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nbool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int kwidth, int kheight, float* output_data,\n                 const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  return AveragePool(input_data, input_dims, stride_width, stride_height,\n                     pad_width, pad_height, kwidth, kheight,\n                     output_activation_min, output_activation_max, output_data,\n                     output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nbool AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, float* output_data,\n                 const Dims<4>& output_dims) {\n  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n                         pad_height, filter_width, filter_height, output_data,\n                         output_dims);\n}\n\ninline bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int filter_width, int filter_height,\n                        int32 output_activation_min,\n                        int32 output_activation_max, uint8* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  return AveragePool(params, DimsToShape(input_dims), input_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nbool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int filter_width, int filter_height,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  return AveragePool(input_data, input_dims, stride_width, stride_height,\n                     pad_width, pad_height, filter_width, filter_height,\n                     output_activation_min, output_activation_max, output_data,\n                     output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nbool AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims) {\n  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n                         pad_height, filter_width, filter_height,\n                         output_activation_min, output_activation_max,\n                         output_data, output_dims);\n}\n\ninline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int kwidth, int kheight,\n                    float output_activation_min, float output_activation_max,\n                    float* output_data, const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int kwidth, int kheight, float* output_data,\n             const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, kwidth, kheight, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             float* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_data, output_dims);\n}\n\ninline void MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int filter_width, int filter_height,\n                    int32 output_activation_min, int32 output_activation_max,\n                    uint8* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int filter_width, int filter_height, int32 output_activation_min,\n             int32 output_activation_max, uint8* output_data,\n             const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, filter_width, filter_height, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             int32 output_activation_min, int32 output_activation_max,\n             uint8* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\ninline void L2Pool(const float* input_data, const Dims<4>& input_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int filter_width, int filter_height,\n                   float output_activation_min, float output_activation_max,\n                   float* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  L2Pool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n         output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims,\n            int stride_width, int stride_height, int pad_width, int pad_height,\n            int filter_width, int filter_height, float* output_data,\n            const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  L2Pool(input_data, input_dims, stride_width, stride_height, pad_width,\n         pad_height, filter_width, filter_height, output_activation_min,\n         output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int filter_width, int filter_height,\n            float* output_data, const Dims<4>& output_dims) {\n  L2Pool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n             filter_width, filter_height, output_data, output_dims);\n}\n\ninline void Softmax(const float* input_data, const Dims<4>& input_dims,\n                    float beta, float* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), beta, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void Softmax(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), input_beta_multiplier,\n          input_beta_left_shift, diff_min, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const float* input_data, const Dims<4>& input_dims,\n                       float* output_data, const Dims<4>& output_dims) {\n  LogSoftmax(input_data, DimsToShape(input_dims), output_data,\n             DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const Dims<4>& output_dims) {\n  LogSoftmax(input_data, DimsToShape(input_dims), input_multiplier,\n             input_left_shift, reverse_scaling_divisor,\n             reverse_scaling_right_shift, diff_min, output_data,\n             DimsToShape(output_dims));\n}\n\ninline void Logistic(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  Logistic(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n           output_data);\n}\n\ninline void Logistic(const uint8* input_data, const Dims<4>& input_dims,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const Dims<4>& output_dims) {\n  Logistic(input_data, DimsToShape(input_dims), input_zero_point,\n           input_range_radius, input_multiplier, input_left_shift, output_data,\n           DimsToShape(output_dims));\n}\n\ninline void Logistic(const int16* input_data, const Dims<4>& input_dims,\n                     int16* output_data, const Dims<4>& output_dims) {\n  Logistic(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n           output_data);\n}\n\ninline void Tanh(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Tanh(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Tanh(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_zero_point,\n       input_range_radius, input_multiplier, input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ninline void Tanh(const int16* input_data, const Dims<4>& input_dims,\n                 int input_left_shift, int16* output_data,\n                 const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ntemplate <typename T>\ninline void DepthToSpace(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::DepthToSpaceParams op_params;\n  op_params.block_size = block_size;\n\n  DepthToSpace(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void SpaceToDepth(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::SpaceToDepthParams op_params;\n  op_params.block_size = block_size;\n\n  SpaceToDepth(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Mul(const T* input1_data, const Dims<4>& input1_dims,\n                const T* input2_data, const Dims<4>& input2_dims,\n                T output_activation_min, T output_activation_max,\n                T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Mul(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastMul(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid BroadcastMul(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T* output_data, const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int16* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  // No params in this version.\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int32 output_offset, int32 output_activation_min,\n                int32 output_activation_max, uint8* output_data,\n                const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.output_offset = output_offset;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void LocalResponseNormalization(const float* input_data,\n                                       const Dims<4>& input_dims, int range,\n                                       float bias, float alpha, float beta,\n                                       float* output_data,\n                                       const Dims<4>& output_dims) {\n  tflite::LocalResponseNormalizationParams op_params;\n  op_params.range = range;\n  op_params.bias = bias;\n  op_params.alpha = alpha;\n  op_params.beta = beta;\n\n  LocalResponseNormalization(op_params, DimsToShape(input_dims), input_data,\n                             DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename SrcT, typename DstT>\nvoid Cast(const SrcT* input_data, const Dims<4>& input_dims, DstT* output_data,\n          const Dims<4>& output_dims) {\n  Cast(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Floor(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Floor(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ntemplate <typename T>\ninline void ResizeBilinear(const T* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, T* output_data,\n                           const Dims<4>& output_dims, bool align_corners) {\n  tflite::ResizeBilinearParams op_params;\n  op_params.align_corners = align_corners;\n  op_params.half_pixel_centers = false;\n  ResizeBilinear(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(output_size_dims), output_size_data,\n                 DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ninline void ResizeBilinear(const float* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, float* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear<float>(input_data, input_dims, output_size_data,\n                        output_size_dims, output_data, output_dims,\n                        /*align_corners=*/false);\n}\n\ninline void ResizeBilinear(const uint8* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, uint8* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear<uint8>(input_data, input_dims, output_size_data,\n                        output_size_dims, output_data, output_dims,\n                        /*align_corners=*/false);\n}\n\ntemplate <typename T>\ninline void SpaceToBatchND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* paddings_data,\n                           const Dims<4>& paddings_dims, T* output_data,\n                           const Dims<4>& output_dims,\n                           const int32_t pad_value) {\n  tflite::SpaceToBatchParams op_params;\n  op_params.output_offset = pad_value;\n\n  SpaceToBatchND(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(paddings_dims), paddings_data,\n                 DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void SpaceToBatchND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* paddings_data,\n                           const Dims<4>& paddings_dims, T* output_data,\n                           const Dims<4>& output_dims) {\n  tflite::SpaceToBatchParams op_params;\n  op_params.output_offset = 0;\n\n  SpaceToBatchND(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(paddings_dims), paddings_data,\n                 DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void BatchToSpaceND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* crops_data, const Dims<4>& crops_dims,\n                           T* output_data, const Dims<4>& output_dims) {\n  BatchToSpaceND(DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(crops_dims), crops_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// Legacy signature, function covered both Pad and PadV2.\ntemplate <typename T>\ninline void PadV2(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& left_paddings,\n                  const std::vector<int>& right_paddings, T* output_data,\n                  const Dims<4>& output_dims, const T pad_value) {\n  TFLITE_DCHECK_EQ(left_paddings.size(), 4);\n  TFLITE_DCHECK_EQ(right_paddings.size(), 4);\n  tflite::PadParams op_params;\n  op_params.left_padding_count = 4;\n  op_params.right_padding_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.left_padding[i] = left_paddings[3 - i];\n    op_params.right_padding[i] = right_paddings[3 - i];\n  }\n  // SetFloatOrInt(pad_value, &op_params.pad_value);\n  const T pad_value_copy = pad_value;\n\n  Pad(op_params, DimsToShape(input_dims), input_data, &pad_value_copy,\n      DimsToShape(output_dims), output_data);\n}\n\n// Old Pad that calls legacy PadV2.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims, const int32_t pad_value) {\n  const T converted_pad_value = static_cast<T>(pad_value);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, converted_pad_value);\n}\n\n// Old Pad that only padded with 0.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims) {\n  const T pad_value = static_cast<T>(0);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, pad_value);\n}\n\ntemplate <typename T>\nvoid TensorFlowMinimum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Minimum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid TensorFlowMaximum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Maximum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, typename Op>\nvoid TensorFlowMaximumMinimum(const T* input1_data, const Dims<4>& input1_dims,\n                              const T* input2_data, const Dims<4>& input2_dims,\n                              T* output_data, const Dims<4>& output_dims,\n                              Op op) {\n  MaximumMinimumBroadcastSlow(DimsToShape(input1_dims), input1_data,\n                              DimsToShape(input2_dims), input2_data,\n                              DimsToShape(output_dims), output_data, op);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const T3* axis, const T1* input_data,\n            const tflite::Dims<4>& input_dims, T2* output_data,\n            const tflite::Dims<4>& output_dims) {\n  // Assumes the input always has 4 dimensions, and therefore,\n  // output always has three dimensions.\n  auto output_shape = RuntimeShape(\n      {output_dims.sizes[2], output_dims.sizes[1], output_dims.sizes[0]});\n  // Another way to interpret this is that output_dims.sizes[4] is always 1.\n  TFLITE_DCHECK_EQ(output_shape.FlatSize(),\n                   DimsToShape(output_dims).FlatSize());\n  // Legacy path only supported this.\n  TFLITE_DCHECK_EQ(axis[0], 3);\n  ArgMinMax(DimsToShape(input_dims), input_data, axis, output_shape,\n            output_data, std::greater<T1>());\n}\n\ntemplate <typename T1, typename T2, typename T3, typename Cmp>\nvoid ArgMinMax(const T3* axis, const T1* input_data, const Dims<4>& input_dims,\n               T2* output_data, const Dims<4>& output_dims, const Cmp& cmp) {\n  ArgMinMax(axis, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n            output_data, cmp);\n}\n\ntemplate <typename T>\ninline void Pow(const T* input1_data, const Dims<4>& input1_dims,\n                const T* input2_data, const Dims<4>& input2_dims,\n                T* output_data, const Dims<4>& output_dims) {\n  Pow(DimsToShape(input1_dims), input1_data, DimsToShape(input2_dims),\n      input2_data, DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void BroadcastPow(const T* input1_data, const Dims<4>& input1_dims,\n                         const T* input2_data, const Dims<4>& input2_dims,\n                         T* output_data, const Dims<4>& output_dims) {\n  BroadcastPow4DSlow(DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// R: Result type. T1: Input 1 type. T2: Input 2 type.\ntemplate <typename R, typename T1, typename T2>\ninline void BroadcastBinaryFunction(const T1* input1_data,\n                                    const Dims<4>& input1_dims,\n                                    const T2* input2_data,\n                                    const Dims<4>& input2_dims, R* output_data,\n                                    const Dims<4>& output_dims,\n                                    R (*func)(T1, T2)) {\n  BroadcastBinaryFunction(DimsToShape(input1_dims), input1_data,\n                          DimsToShape(input2_dims), input2_data,\n                          DimsToShape(output_dims), output_data, func);\n}\n\n// R: Result type. T1: Input 1 type. T2: Input 2 type.\ntemplate <typename R, typename T1, typename T2>\ninline void BinaryFunction(const T1* input1_data, const Dims<4>& input1_dims,\n                           const T2* input2_data, const Dims<4>& input2_dims,\n                           R* output_data, const Dims<4>& output_dims,\n                           R (*func)(T1, T2)) {\n  BinaryFunction(DimsToShape(input1_dims), input1_data,\n                 DimsToShape(input2_dims), input2_data,\n                 DimsToShape(output_dims), output_data, func);\n}\n\ntemplate <typename T>\ninline void Slice(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& begin, const std::vector<int>& size,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::SliceParams op_params;\n  op_params.begin_count = 4;\n  op_params.size_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.begin[i] = begin[3 - i];\n    op_params.size[i] = size[3 - i];\n  }\n\n  Slice(op_params, DimsToShape(input_dims), input_data,\n        DimsToShape(output_dims), output_data);\n}\n\n}  // namespace reference_ops\n}  // namespace tflite\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEGACY_REFERENCE_OPS_H_\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING_H_\n\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/cppmath.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\nnamespace reference_ops {\n\ninline bool AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const float* input_data,\n                        const RuntimeShape& output_shape, float* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          float total = 0.f;\n          float filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              total +=\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              filter_count++;\n            }\n          }\n          if (filter_count == 0) return false;\n          const float average = total / filter_count;\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              ActivationFunctionWithMinMax(average, params.float_activation_min,\n                                           params.float_activation_max);\n        }\n      }\n    }\n  }\n  return true;\n}\n\ninline bool AveragePool(const PoolParams& params,\n                        const RuntimeShape& input_shape,\n                        const uint8_t* input_data,\n                        const RuntimeShape& output_shape,\n                        uint8_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          int32_t acc = 0;\n          int filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              acc +=\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              filter_count++;\n            }\n          }\n          if (filter_count == 0) return false;\n          acc = (acc + filter_count / 2) / filter_count;\n          acc = std::max(acc, params.quantized_activation_min);\n          acc = std::min(acc, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<uint8_t>(acc);\n        }\n      }\n    }\n  }\n  return true;\n}\n\ninline void L2Pool(const PoolParams& params, const RuntimeShape& input_shape,\n                   const float* input_data, const RuntimeShape& output_shape,\n                   float* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          float sum_squares = 0.f;\n          int filter_count = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              const float val =\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)];\n              sum_squares += val * val;\n              filter_count++;\n            }\n          }\n          const float l2pool_result = std::sqrt(sum_squares / filter_count);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              ActivationFunctionWithMinMax(l2pool_result,\n                                           params.float_activation_min,\n                                           params.float_activation_max);\n        }\n      }\n    }\n  }\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const float* input_data, const RuntimeShape& output_shape,\n                    float* output_data) {\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          float max = std::numeric_limits<float>::lowest();\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              max = std::max(\n                  max,\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);\n            }\n          }\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              ActivationFunctionWithMinMax(max, params.float_activation_min,\n                                           params.float_activation_max);\n        }\n      }\n    }\n  }\n}\n\ninline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n                    const uint8_t* input_data, const RuntimeShape& output_shape,\n                    uint8_t* output_data) {\n  TFLITE_DCHECK_LE(params.quantized_activation_min,\n                   params.quantized_activation_max);\n  TFLITE_DCHECK_GE(params.quantized_activation_min, 0);\n  TFLITE_DCHECK_LE(params.quantized_activation_max, 255);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int depth = MatchingDim(input_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int stride_height = params.stride_height;\n  const int stride_width = params.stride_width;\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int out_y = 0; out_y < output_height; ++out_y) {\n      for (int out_x = 0; out_x < output_width; ++out_x) {\n        for (int channel = 0; channel < depth; ++channel) {\n          const int in_x_origin =\n              (out_x * stride_width) - params.padding_values.width;\n          const int in_y_origin =\n              (out_y * stride_height) - params.padding_values.height;\n          // Compute the boundaries of the filter region clamped so as to\n          // ensure that the filter window fits in the input array.\n          const int filter_x_start = std::max(0, -in_x_origin);\n          const int filter_x_end =\n              std::min(params.filter_width, input_width - in_x_origin);\n          const int filter_y_start = std::max(0, -in_y_origin);\n          const int filter_y_end =\n              std::min(params.filter_height, input_height - in_y_origin);\n          uint8_t max = 0;\n          for (int filter_y = filter_y_start; filter_y < filter_y_end;\n               ++filter_y) {\n            for (int filter_x = filter_x_start; filter_x < filter_x_end;\n                 ++filter_x) {\n              const int in_x = in_x_origin + filter_x;\n              const int in_y = in_y_origin + filter_y;\n              max = std::max(\n                  max,\n                  input_data[Offset(input_shape, batch, in_y, in_x, channel)]);\n            }\n          }\n          max = std::max<uint8_t>(max, params.quantized_activation_min);\n          max = std::min<uint8_t>(max, params.quantized_activation_max);\n          output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n              static_cast<uint8_t>(max);\n        }\n      }\n    }\n  }\n}\n}  // namespace reference_ops\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING_H_\n", "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/pooling.h\"\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <cstdlib>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/reference/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/padding.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace pooling {\n\n// This file has two implementation of each pooling op.\nenum KernelType {\n  kReference,\n  kGenericOptimized,\n};\n\nenum PoolType {\n  kAverage,\n  kMax,\n  kL2,\n};\n\nstruct OpData {\n  TfLitePaddingValues padding;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  // This is a builtin op, so we don't use the contents in 'buffer', if any.\n  // Instead, we allocate a new object to carry information from Prepare() to\n  // Eval().\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\ntemplate <PoolType pool_type>\nTfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  int batches = input->dims->data[0];\n  int height = input->dims->data[1];\n  int width = input->dims->data[2];\n  int channels_out = input->dims->data[3];\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n\n  // Prevent division by 0 in optimized pooling implementations\n  TF_LITE_ENSURE(context, params->stride_height > 0);\n  TF_LITE_ENSURE(context, params->stride_width > 0);\n\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width, 1, 1, height, width,\n      params->filter_height, params->filter_width, padding, &out_height,\n      &out_width);\n\n  if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n    if (pool_type == kAverage || pool_type == kMax) {\n      TFLITE_DCHECK_LE(std::abs(input->params.scale - output->params.scale),\n                       1.0e-6);\n      TFLITE_DCHECK_EQ(input->params.zero_point, output->params.zero_point);\n    }\n    if (pool_type == kL2) {\n      // We currently don't have a quantized implementation of L2Pool\n      TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);\n    }\n  }\n\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = batches;\n  output_size->data[1] = out_height;\n  output_size->data[2] = out_width;\n  output_size->data[3] = channels_out;\n  return context->ResizeTensor(context, output, output_size);\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus AverageEvalFloat(TfLiteContext* context, TfLiteNode* node,\n                              TfLitePoolParams* params, OpData* data,\n                              const TfLiteTensor* input, TfLiteTensor* output) {\n  float activation_min, activation_max;\n  CalculateActivationRange(params->activation, &activation_min,\n                           &activation_max);\n#define TF_LITE_AVERAGE_POOL(type)                                            \\\n  tflite::PoolParams op_params;                                               \\\n  op_params.stride_height = params->stride_height;                            \\\n  op_params.stride_width = params->stride_width;                              \\\n  op_params.filter_height = params->filter_height;                            \\\n  op_params.filter_width = params->filter_width;                              \\\n  op_params.padding_values.height = data->padding.height;                     \\\n  op_params.padding_values.width = data->padding.width;                       \\\n  op_params.float_activation_min = activation_min;                            \\\n  op_params.float_activation_max = activation_max;                            \\\n  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n                                            GetTensorData<float>(input),      \\\n                                            GetTensorShape(output),           \\\n                                            GetTensorData<float>(output)))\n  if (kernel_type == kReference) {\n    TF_LITE_AVERAGE_POOL(reference_ops);\n  } else {\n    TF_LITE_AVERAGE_POOL(optimized_ops);\n  }\n#undef TF_LITE_AVERAGE_POOL\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus AverageEvalQuantizedUint8(TfLiteContext* context, TfLiteNode* node,\n                                       TfLitePoolParams* params, OpData* data,\n                                       const TfLiteTensor* input,\n                                       TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                          &activation_min, &activation_max);\n#define TF_LITE_AVERAGE_POOL(type)                                            \\\n  tflite::PoolParams op_params;                                               \\\n  op_params.stride_height = params->stride_height;                            \\\n  op_params.stride_width = params->stride_width;                              \\\n  op_params.filter_height = params->filter_height;                            \\\n  op_params.filter_width = params->filter_width;                              \\\n  op_params.padding_values.height = data->padding.height;                     \\\n  op_params.padding_values.width = data->padding.width;                       \\\n  op_params.quantized_activation_min = activation_min;                        \\\n  op_params.quantized_activation_max = activation_max;                        \\\n  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n                                            GetTensorData<uint8_t>(input),    \\\n                                            GetTensorShape(output),           \\\n                                            GetTensorData<uint8_t>(output)))\n  if (kernel_type == kReference) {\n    TF_LITE_AVERAGE_POOL(reference_ops);\n  } else {\n    TF_LITE_AVERAGE_POOL(optimized_ops);\n  }\n#undef TF_LITE_AVERAGE_POOL\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus AverageEvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,\n                                      TfLitePoolParams* params, OpData* data,\n                                      const TfLiteTensor* input,\n                                      TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n\n  (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                          &activation_min, &activation_max);\n#define TF_LITE_AVERAGE_POOL(type)                                            \\\n  tflite::PoolParams op_params;                                               \\\n  op_params.stride_height = params->stride_height;                            \\\n  op_params.stride_width = params->stride_width;                              \\\n  op_params.filter_height = params->filter_height;                            \\\n  op_params.filter_width = params->filter_width;                              \\\n  op_params.padding_values.height = data->padding.height;                     \\\n  op_params.padding_values.width = data->padding.width;                       \\\n  op_params.quantized_activation_min = activation_min;                        \\\n  op_params.quantized_activation_max = activation_max;                        \\\n  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n                                            GetTensorData<int8_t>(input),     \\\n                                            GetTensorShape(output),           \\\n                                            GetTensorData<int8_t>(output)))\n  if (kernel_type == kReference) {\n    TF_LITE_AVERAGE_POOL(reference_integer_ops);\n  } else {\n    TF_LITE_AVERAGE_POOL(optimized_integer_ops);\n  }\n#undef TF_LITE_AVERAGE_POOL\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus AverageEvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,\n                                       TfLitePoolParams* params, OpData* data,\n                                       const TfLiteTensor* input,\n                                       TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  CalculateActivationRangeQuantized(context, params->activation, output,\n                                    &activation_min, &activation_max);\n#define TF_LITE_AVERAGE_POOL(type)                                            \\\n  tflite::PoolParams op_params;                                               \\\n  op_params.stride_height = params->stride_height;                            \\\n  op_params.stride_width = params->stride_width;                              \\\n  op_params.filter_height = params->filter_height;                            \\\n  op_params.filter_width = params->filter_width;                              \\\n  op_params.padding_values.height = data->padding.height;                     \\\n  op_params.padding_values.width = data->padding.width;                       \\\n  op_params.quantized_activation_min = activation_min;                        \\\n  op_params.quantized_activation_max = activation_max;                        \\\n  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n                                            GetTensorData<int16_t>(input),    \\\n                                            GetTensorShape(output),           \\\n                                            GetTensorData<int16_t>(output)))\n  TF_LITE_AVERAGE_POOL(reference_integer_ops);\n#undef TF_LITE_AVERAGE_POOL\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nvoid MaxEvalFloat(TfLiteContext* context, TfLiteNode* node,\n                  TfLitePoolParams* params, OpData* data,\n                  const TfLiteTensor* input, TfLiteTensor* output) {\n  float activation_min, activation_max;\n  CalculateActivationRange(params->activation, &activation_min,\n                           &activation_max);\n#define TF_LITE_MAX_POOL(type)                                                 \\\n  tflite::PoolParams op_params;                                                \\\n  op_params.stride_height = params->stride_height;                             \\\n  op_params.stride_width = params->stride_width;                               \\\n  op_params.filter_height = params->filter_height;                             \\\n  op_params.filter_width = params->filter_width;                               \\\n  op_params.padding_values.height = data->padding.height;                      \\\n  op_params.padding_values.width = data->padding.width;                        \\\n  op_params.float_activation_min = activation_min;                             \\\n  op_params.float_activation_max = activation_max;                             \\\n  type::MaxPool(op_params, GetTensorShape(input), GetTensorData<float>(input), \\\n                GetTensorShape(output), GetTensorData<float>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_MAX_POOL(reference_ops);\n  } else {\n    TF_LITE_MAX_POOL(optimized_ops);\n  }\n#undef TF_LITE_MAX_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid MaxEvalQuantizedUInt8(TfLiteContext* context, TfLiteNode* node,\n                           TfLitePoolParams* params, OpData* data,\n                           const TfLiteTensor* input, TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                          &activation_min, &activation_max);\n#define TF_LITE_MAX_POOL(type)                                         \\\n  tflite::PoolParams op_params;                                        \\\n  op_params.stride_height = params->stride_height;                     \\\n  op_params.stride_width = params->stride_width;                       \\\n  op_params.filter_height = params->filter_height;                     \\\n  op_params.filter_width = params->filter_width;                       \\\n  op_params.padding_values.height = data->padding.height;              \\\n  op_params.padding_values.width = data->padding.width;                \\\n  op_params.quantized_activation_min = activation_min;                 \\\n  op_params.quantized_activation_max = activation_max;                 \\\n  type::MaxPool(op_params, GetTensorShape(input),                      \\\n                GetTensorData<uint8_t>(input), GetTensorShape(output), \\\n                GetTensorData<uint8_t>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_MAX_POOL(reference_ops);\n  } else {\n    TF_LITE_MAX_POOL(optimized_ops);\n  }\n#undef TF_LITE_MAX_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid MaxEvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,\n                          TfLitePoolParams* params, OpData* data,\n                          const TfLiteTensor* input, TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                          &activation_min, &activation_max);\n#define TF_LITE_MAX_POOL(type)                                        \\\n  tflite::PoolParams op_params;                                       \\\n  op_params.stride_height = params->stride_height;                    \\\n  op_params.stride_width = params->stride_width;                      \\\n  op_params.filter_height = params->filter_height;                    \\\n  op_params.filter_width = params->filter_width;                      \\\n  op_params.padding_values.height = data->padding.height;             \\\n  op_params.padding_values.width = data->padding.width;               \\\n  op_params.quantized_activation_min = activation_min;                \\\n  op_params.quantized_activation_max = activation_max;                \\\n  type::MaxPool(op_params, GetTensorShape(input),                     \\\n                GetTensorData<int8_t>(input), GetTensorShape(output), \\\n                GetTensorData<int8_t>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_MAX_POOL(reference_integer_ops);\n  } else {\n    TF_LITE_MAX_POOL(optimized_integer_ops);\n  }\n#undef TF_LITE_MAX_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid MaxEvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,\n                           TfLitePoolParams* params, OpData* data,\n                           const TfLiteTensor* input, TfLiteTensor* output) {\n  int32_t activation_min;\n  int32_t activation_max;\n  CalculateActivationRangeQuantized(context, params->activation, output,\n                                    &activation_min, &activation_max);\n#define TF_LITE_MAX_POOL(type)                                         \\\n  tflite::PoolParams op_params;                                        \\\n  op_params.stride_height = params->stride_height;                     \\\n  op_params.stride_width = params->stride_width;                       \\\n  op_params.filter_height = params->filter_height;                     \\\n  op_params.filter_width = params->filter_width;                       \\\n  op_params.padding_values.height = data->padding.height;              \\\n  op_params.padding_values.width = data->padding.width;                \\\n  op_params.quantized_activation_min = activation_min;                 \\\n  op_params.quantized_activation_max = activation_max;                 \\\n  type::MaxPool(op_params, GetTensorShape(input),                      \\\n                GetTensorData<int16_t>(input), GetTensorShape(output), \\\n                GetTensorData<int16_t>(output))\n  TF_LITE_MAX_POOL(reference_integer_ops);\n#undef TF_LITE_MAX_POOL\n}\n\ntemplate <KernelType kernel_type>\nvoid L2EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                 TfLitePoolParams* params, OpData* data,\n                 const TfLiteTensor* input, TfLiteTensor* output) {\n  float activation_min, activation_max;\n  CalculateActivationRange(params->activation, &activation_min,\n                           &activation_max);\n#define TF_LITE_L2_POOL(type)                                                 \\\n  tflite::PoolParams op_params;                                               \\\n  op_params.stride_height = params->stride_height;                            \\\n  op_params.stride_width = params->stride_width;                              \\\n  op_params.filter_height = params->filter_height;                            \\\n  op_params.filter_width = params->filter_width;                              \\\n  op_params.padding_values.height = data->padding.height;                     \\\n  op_params.padding_values.width = data->padding.width;                       \\\n  op_params.float_activation_min = activation_min;                            \\\n  op_params.float_activation_max = activation_max;                            \\\n  type::L2Pool(op_params, GetTensorShape(input), GetTensorData<float>(input), \\\n               GetTensorShape(output), GetTensorData<float>(output))\n  if (kernel_type == kReference) {\n    TF_LITE_L2_POOL(reference_ops);\n  } else {\n    TF_LITE_L2_POOL(optimized_ops);\n  }\n#undef TF_LITE_L2_POOL\n}\n\n#undef TF_LITE_KERNEL_TYPE_DISPATCH\n\ntemplate <KernelType kernel_type>\nTfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  switch (input->type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      return AverageEvalFloat<kernel_type>(context, node, params, data, input,\n                                           output);\n    case kTfLiteUInt8:\n      return AverageEvalQuantizedUint8<kernel_type>(context, node, params, data,\n                                                    input, output);\n    case kTfLiteInt8:\n      return AverageEvalQuantizedInt8<kernel_type>(context, node, params, data,\n                                                   input, output);\n    case kTfLiteInt16:\n      return AverageEvalQuantizedInt16<kernel_type>(context, node, params, data,\n                                                    input, output);\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Type %s not currently supported.\",\n                         TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  switch (input->type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      MaxEvalFloat<kernel_type>(context, node, params, data, input, output);\n      break;\n    case kTfLiteUInt8:\n      MaxEvalQuantizedUInt8<kernel_type>(context, node, params, data, input,\n                                         output);\n      break;\n    case kTfLiteInt8:\n      MaxEvalQuantizedInt8<kernel_type>(context, node, params, data, input,\n                                        output);\n      break;\n    case kTfLiteInt16:\n      MaxEvalQuantizedInt16<kernel_type>(context, node, params, data, input,\n                                         output);\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Type %s not currently supported.\",\n                         TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus L2Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  switch (input->type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      L2EvalFloat<kernel_type>(context, node, params, data, input, output);\n      break;\n    case kTfLiteUInt8:\n    // We don't have a quantized implementation, so just fall through to the\n    // 'default' case.\n    default:\n      context->ReportError(context, \"Type %d not currently supported.\",\n                           input->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace pooling\n\nTfLiteRegistration* Register_AVERAGE_POOL_REF() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kAverage>,\n                                 pooling::AverageEval<pooling::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_MAX_POOL_REF() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kMax>,\n                                 pooling::MaxEval<pooling::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_L2_POOL_REF() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kL2>,\n                                 pooling::L2Eval<pooling::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_AVERAGE_POOL_GENERIC_OPT() {\n  static TfLiteRegistration r = {\n      pooling::Init, pooling::Free, pooling::GenericPrepare<pooling::kAverage>,\n      pooling::AverageEval<pooling::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_MAX_POOL_GENERIC_OPT() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kMax>,\n                                 pooling::MaxEval<pooling::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_L2_POOL_GENERIC_OPT() {\n  static TfLiteRegistration r = {pooling::Init, pooling::Free,\n                                 pooling::GenericPrepare<pooling::kL2>,\n                                 pooling::L2Eval<pooling::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_AVERAGE_POOL_2D() {\n  return Register_AVERAGE_POOL_GENERIC_OPT();\n}\n\nTfLiteRegistration* Register_MAX_POOL_2D() {\n  return Register_MAX_POOL_GENERIC_OPT();\n}\n\nTfLiteRegistration* Register_L2_POOL_2D() {\n  return Register_L2_POOL_GENERIC_OPT();\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "filenames": ["tensorflow/lite/kernels/internal/averagepool_quantized_test.cc", "tensorflow/lite/kernels/internal/optimized/integer_ops/pooling.h", "tensorflow/lite/kernels/internal/optimized/legacy_optimized_ops.h", "tensorflow/lite/kernels/internal/optimized/optimized_ops.h", "tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h", "tensorflow/lite/kernels/internal/reference/legacy_reference_ops.h", "tensorflow/lite/kernels/internal/reference/pooling.h", "tensorflow/lite/kernels/pooling.cc"], "buggy_code_start_loc": [43, 147, 3764, 3175, 24, 1490, 26, 120], "buggy_code_end_loc": [49, 269, 3857, 3371, 195, 1637, 133, 397], "fixing_code_start_loc": [43, 147, 3764, 3175, 24, 1490, 26, 120], "fixing_code_end_loc": [51, 272, 3861, 3379, 200, 1641, 138, 403], "type": "CWE-835", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions the strided slice implementation in TFLite has a logic bug which can allow an attacker to trigger an infinite loop. This arises from newly introduced support for [ellipsis in axis definition](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/strided_slice.cc#L103-L122). An attacker can craft a model such that `ellipsis_end_idx` is smaller than `i` (e.g., always negative). In this case, the inner loop does not increase `i` and the `continue` statement causes execution to skip over the preincrement at the end of the outer loop. We have patched the issue in GitHub commit dfa22b348b70bb89d6d6ec0ff53973bacb4f4695. TensorFlow 2.6.0 is the only affected version.", "other": {"cve": {"id": "CVE-2021-37686", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T22:15:08.967", "lastModified": "2021-08-18T20:42:29.257", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions the strided slice implementation in TFLite has a logic bug which can allow an attacker to trigger an infinite loop. This arises from newly introduced support for [ellipsis in axis definition](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/strided_slice.cc#L103-L122). An attacker can craft a model such that `ellipsis_end_idx` is smaller than `i` (e.g., always negative). In this case, the inner loop does not increase `i` and the `continue` statement causes execution to skip over the preincrement at the end of the outer loop. We have patched the issue in GitHub commit dfa22b348b70bb89d6d6ec0ff53973bacb4f4695. TensorFlow 2.6.0 is the only affected version."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico. En las versiones afectadas, la implementaci\u00f3n strided slice en TFLite presenta un fallo l\u00f3gico que puede permitir a un atacante desencadenar un bucle infinito. Esto se debe al soporte recientemente introducido para [ellipsis en la definici\u00f3n de ejes](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/strided_slice.cc#L103-L122). Un atacante puede dise\u00f1ar un modelo tal que \"ellipsis_end_idx\" sea menor que \"i\" (por ejemplo, siempre negativo). En este caso, el bucle interno no incrementa \"i\" y la sentencia \"continue\" hace que la ejecuci\u00f3n se salte el preincremento al final del bucle externo. Hemos parcheado el problema en el commit dfa22b348b70bb89d6d6ec0ff53973bacb4f4695 de GitHub. TensorFlow  versi\u00f3n 2.6.0 es la \u00fanica versi\u00f3n afectada."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-835"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.4", "matchCriteriaId": "0F83C081-51CC-415F-A8C0-0A44C75E2CD6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.3", "matchCriteriaId": "BD3F2BF8-EBA9-42BF-8F9B-D918B880B15A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D03E99A7-4E3D-427D-A156-C0713E9FB02A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "70FA6E48-6C57-40CA-809F-4E3D07CBF348"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "42187561-E491-434D-828C-F36701446634"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C66B61C8-450A-4C5E-9174-F970D6DEE778"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/dfa22b348b70bb89d6d6ec0ff53973bacb4f4695", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-mhhc-q96p-mfm9", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/dfa22b348b70bb89d6d6ec0ff53973bacb4f4695"}}