{"buggy_code": ["#ifndef _ASM_X86_PAGE_32_DEFS_H\n#define _ASM_X86_PAGE_32_DEFS_H\n\n#include <linux/const.h>\n\n/*\n * This handles the memory map.\n *\n * A __PAGE_OFFSET of 0xC0000000 means that the kernel has\n * a virtual address space of one gigabyte, which limits the\n * amount of physical memory you can use to about 950MB.\n *\n * If you want more physical memory than this then see the CONFIG_HIGHMEM4G\n * and CONFIG_HIGHMEM64G options in the kernel configuration.\n */\n#define __PAGE_OFFSET\t\t_AC(CONFIG_PAGE_OFFSET, UL)\n\n#define __START_KERNEL_map\t__PAGE_OFFSET\n\n#define THREAD_SIZE_ORDER\t1\n#define THREAD_SIZE\t\t(PAGE_SIZE << THREAD_SIZE_ORDER)\n\n#define STACKFAULT_STACK 0\n#define DOUBLEFAULT_STACK 1\n#define NMI_STACK 0\n#define DEBUG_STACK 0\n#define MCE_STACK 0\n#define N_EXCEPTION_STACKS 1\n\n#ifdef CONFIG_X86_PAE\n/* 44=32+12, the limit we can fit into an unsigned long pfn */\n#define __PHYSICAL_MASK_SHIFT\t44\n#define __VIRTUAL_MASK_SHIFT\t32\n\n#else  /* !CONFIG_X86_PAE */\n#define __PHYSICAL_MASK_SHIFT\t32\n#define __VIRTUAL_MASK_SHIFT\t32\n#endif\t/* CONFIG_X86_PAE */\n\n/*\n * Kernel image size is limited to 512 MB (see in arch/x86/kernel/head_32.S)\n */\n#define KERNEL_IMAGE_SIZE\t(512 * 1024 * 1024)\n\n#ifndef __ASSEMBLY__\n\n/*\n * This much address space is reserved for vmalloc() and iomap()\n * as well as fixmap mappings.\n */\nextern unsigned int __VMALLOC_RESERVE;\nextern int sysctl_legacy_va_layout;\n\nextern void find_low_pfn_range(void);\nextern void setup_bootmem_allocator(void);\n\n#endif\t/* !__ASSEMBLY__ */\n\n#endif /* _ASM_X86_PAGE_32_DEFS_H */\n", "#ifndef _ASM_X86_PAGE_64_DEFS_H\n#define _ASM_X86_PAGE_64_DEFS_H\n\n#define THREAD_SIZE_ORDER\t2\n#define THREAD_SIZE  (PAGE_SIZE << THREAD_SIZE_ORDER)\n#define CURRENT_MASK (~(THREAD_SIZE - 1))\n\n#define EXCEPTION_STACK_ORDER 0\n#define EXCEPTION_STKSZ (PAGE_SIZE << EXCEPTION_STACK_ORDER)\n\n#define DEBUG_STACK_ORDER (EXCEPTION_STACK_ORDER + 1)\n#define DEBUG_STKSZ (PAGE_SIZE << DEBUG_STACK_ORDER)\n\n#define IRQ_STACK_ORDER 2\n#define IRQ_STACK_SIZE (PAGE_SIZE << IRQ_STACK_ORDER)\n\n#define STACKFAULT_STACK 1\n#define DOUBLEFAULT_STACK 2\n#define NMI_STACK 3\n#define DEBUG_STACK 4\n#define MCE_STACK 5\n#define N_EXCEPTION_STACKS 5  /* hw limit: 7 */\n\n#define PUD_PAGE_SIZE\t\t(_AC(1, UL) << PUD_SHIFT)\n#define PUD_PAGE_MASK\t\t(~(PUD_PAGE_SIZE-1))\n\n/*\n * Set __PAGE_OFFSET to the most negative possible address +\n * PGDIR_SIZE*16 (pgd slot 272).  The gap is to allow a space for a\n * hypervisor to fit.  Choosing 16 slots here is arbitrary, but it's\n * what Xen requires.\n */\n#define __PAGE_OFFSET           _AC(0xffff880000000000, UL)\n\n#define __START_KERNEL_map\t_AC(0xffffffff80000000, UL)\n\n/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */\n#define __PHYSICAL_MASK_SHIFT\t46\n#define __VIRTUAL_MASK_SHIFT\t47\n\n/*\n * Kernel image size is limited to 1GiB due to the fixmap living in the\n * next 1GiB (see level2_kernel_pgt in arch/x86/kernel/head_64.S). Use\n * 512MiB by default, leaving 1.5GiB for modules once the page tables\n * are fully set up. If kernel ASLR is configured, it can extend the\n * kernel page table mapping, reducing the size of the modules area.\n */\n#define KERNEL_IMAGE_SIZE_DEFAULT      (512 * 1024 * 1024)\n#if defined(CONFIG_RANDOMIZE_BASE) && \\\n\tCONFIG_RANDOMIZE_BASE_MAX_OFFSET > KERNEL_IMAGE_SIZE_DEFAULT\n#define KERNEL_IMAGE_SIZE   CONFIG_RANDOMIZE_BASE_MAX_OFFSET\n#else\n#define KERNEL_IMAGE_SIZE      KERNEL_IMAGE_SIZE_DEFAULT\n#endif\n\n#endif /* _ASM_X86_PAGE_64_DEFS_H */\n", "#ifndef _ASM_X86_TRAPS_H\n#define _ASM_X86_TRAPS_H\n\n#include <linux/kprobes.h>\n\n#include <asm/debugreg.h>\n#include <asm/siginfo.h>\t\t\t/* TRAP_TRACE, ... */\n\n#define dotraplinkage __visible\n\nasmlinkage void divide_error(void);\nasmlinkage void debug(void);\nasmlinkage void nmi(void);\nasmlinkage void int3(void);\nasmlinkage void xen_debug(void);\nasmlinkage void xen_int3(void);\nasmlinkage void xen_stack_segment(void);\nasmlinkage void overflow(void);\nasmlinkage void bounds(void);\nasmlinkage void invalid_op(void);\nasmlinkage void device_not_available(void);\n#ifdef CONFIG_X86_64\nasmlinkage void double_fault(void);\n#endif\nasmlinkage void coprocessor_segment_overrun(void);\nasmlinkage void invalid_TSS(void);\nasmlinkage void segment_not_present(void);\nasmlinkage void stack_segment(void);\nasmlinkage void general_protection(void);\nasmlinkage void page_fault(void);\nasmlinkage void async_page_fault(void);\nasmlinkage void spurious_interrupt_bug(void);\nasmlinkage void coprocessor_error(void);\nasmlinkage void alignment_check(void);\n#ifdef CONFIG_X86_MCE\nasmlinkage void machine_check(void);\n#endif /* CONFIG_X86_MCE */\nasmlinkage void simd_coprocessor_error(void);\n\n#ifdef CONFIG_TRACING\nasmlinkage void trace_page_fault(void);\n#define trace_divide_error divide_error\n#define trace_bounds bounds\n#define trace_invalid_op invalid_op\n#define trace_device_not_available device_not_available\n#define trace_coprocessor_segment_overrun coprocessor_segment_overrun\n#define trace_invalid_TSS invalid_TSS\n#define trace_segment_not_present segment_not_present\n#define trace_general_protection general_protection\n#define trace_spurious_interrupt_bug spurious_interrupt_bug\n#define trace_coprocessor_error coprocessor_error\n#define trace_alignment_check alignment_check\n#define trace_simd_coprocessor_error simd_coprocessor_error\n#define trace_async_page_fault async_page_fault\n#endif\n\ndotraplinkage void do_divide_error(struct pt_regs *, long);\ndotraplinkage void do_debug(struct pt_regs *, long);\ndotraplinkage void do_nmi(struct pt_regs *, long);\ndotraplinkage void do_int3(struct pt_regs *, long);\ndotraplinkage void do_overflow(struct pt_regs *, long);\ndotraplinkage void do_bounds(struct pt_regs *, long);\ndotraplinkage void do_invalid_op(struct pt_regs *, long);\ndotraplinkage void do_device_not_available(struct pt_regs *, long);\ndotraplinkage void do_coprocessor_segment_overrun(struct pt_regs *, long);\ndotraplinkage void do_invalid_TSS(struct pt_regs *, long);\ndotraplinkage void do_segment_not_present(struct pt_regs *, long);\ndotraplinkage void do_stack_segment(struct pt_regs *, long);\n#ifdef CONFIG_X86_64\ndotraplinkage void do_double_fault(struct pt_regs *, long);\nasmlinkage struct pt_regs *sync_regs(struct pt_regs *);\n#endif\ndotraplinkage void do_general_protection(struct pt_regs *, long);\ndotraplinkage void do_page_fault(struct pt_regs *, unsigned long);\n#ifdef CONFIG_TRACING\ndotraplinkage void trace_do_page_fault(struct pt_regs *, unsigned long);\n#else\nstatic inline void trace_do_page_fault(struct pt_regs *regs, unsigned long error)\n{\n\tdo_page_fault(regs, error);\n}\n#endif\ndotraplinkage void do_spurious_interrupt_bug(struct pt_regs *, long);\ndotraplinkage void do_coprocessor_error(struct pt_regs *, long);\ndotraplinkage void do_alignment_check(struct pt_regs *, long);\n#ifdef CONFIG_X86_MCE\ndotraplinkage void do_machine_check(struct pt_regs *, long);\n#endif\ndotraplinkage void do_simd_coprocessor_error(struct pt_regs *, long);\n#ifdef CONFIG_X86_32\ndotraplinkage void do_iret_error(struct pt_regs *, long);\n#endif\n\nstatic inline int get_si_code(unsigned long condition)\n{\n\tif (condition & DR_STEP)\n\t\treturn TRAP_TRACE;\n\telse if (condition & (DR_TRAP0|DR_TRAP1|DR_TRAP2|DR_TRAP3))\n\t\treturn TRAP_HWBKPT;\n\telse\n\t\treturn TRAP_BRKPT;\n}\n\nextern int panic_on_unrecovered_nmi;\n\nvoid math_emulate(struct math_emu_info *);\n#ifndef CONFIG_X86_32\nasmlinkage void smp_thermal_interrupt(void);\nasmlinkage void mce_threshold_interrupt(void);\n#endif\n\n/* Interrupts/Exceptions */\nenum {\n\tX86_TRAP_DE = 0,\t/*  0, Divide-by-zero */\n\tX86_TRAP_DB,\t\t/*  1, Debug */\n\tX86_TRAP_NMI,\t\t/*  2, Non-maskable Interrupt */\n\tX86_TRAP_BP,\t\t/*  3, Breakpoint */\n\tX86_TRAP_OF,\t\t/*  4, Overflow */\n\tX86_TRAP_BR,\t\t/*  5, Bound Range Exceeded */\n\tX86_TRAP_UD,\t\t/*  6, Invalid Opcode */\n\tX86_TRAP_NM,\t\t/*  7, Device Not Available */\n\tX86_TRAP_DF,\t\t/*  8, Double Fault */\n\tX86_TRAP_OLD_MF,\t/*  9, Coprocessor Segment Overrun */\n\tX86_TRAP_TS,\t\t/* 10, Invalid TSS */\n\tX86_TRAP_NP,\t\t/* 11, Segment Not Present */\n\tX86_TRAP_SS,\t\t/* 12, Stack Segment Fault */\n\tX86_TRAP_GP,\t\t/* 13, General Protection Fault */\n\tX86_TRAP_PF,\t\t/* 14, Page Fault */\n\tX86_TRAP_SPURIOUS,\t/* 15, Spurious Interrupt */\n\tX86_TRAP_MF,\t\t/* 16, x87 Floating-Point Exception */\n\tX86_TRAP_AC,\t\t/* 17, Alignment Check */\n\tX86_TRAP_MC,\t\t/* 18, Machine Check */\n\tX86_TRAP_XF,\t\t/* 19, SIMD Floating-Point Exception */\n\tX86_TRAP_IRET = 32,\t/* 32, IRET Exception */\n};\n\n#endif /* _ASM_X86_TRAPS_H */\n", "/*\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs\n */\n#include <linux/kallsyms.h>\n#include <linux/kprobes.h>\n#include <linux/uaccess.h>\n#include <linux/hardirq.h>\n#include <linux/kdebug.h>\n#include <linux/module.h>\n#include <linux/ptrace.h>\n#include <linux/kexec.h>\n#include <linux/sysfs.h>\n#include <linux/bug.h>\n#include <linux/nmi.h>\n\n#include <asm/stacktrace.h>\n\n\n#define N_EXCEPTION_STACKS_END \\\n\t\t(N_EXCEPTION_STACKS + DEBUG_STKSZ/EXCEPTION_STKSZ - 2)\n\nstatic char x86_stack_ids[][8] = {\n\t\t[ DEBUG_STACK-1\t\t\t]\t= \"#DB\",\n\t\t[ NMI_STACK-1\t\t\t]\t= \"NMI\",\n\t\t[ DOUBLEFAULT_STACK-1\t\t]\t= \"#DF\",\n\t\t[ STACKFAULT_STACK-1\t\t]\t= \"#SS\",\n\t\t[ MCE_STACK-1\t\t\t]\t= \"#MC\",\n#if DEBUG_STKSZ > EXCEPTION_STKSZ\n\t\t[ N_EXCEPTION_STACKS ...\n\t\t  N_EXCEPTION_STACKS_END\t]\t= \"#DB[?]\"\n#endif\n};\n\nstatic unsigned long *in_exception_stack(unsigned cpu, unsigned long stack,\n\t\t\t\t\t unsigned *usedp, char **idp)\n{\n\tunsigned k;\n\n\t/*\n\t * Iterate over all exception stacks, and figure out whether\n\t * 'stack' is in one of them:\n\t */\n\tfor (k = 0; k < N_EXCEPTION_STACKS; k++) {\n\t\tunsigned long end = per_cpu(orig_ist, cpu).ist[k];\n\t\t/*\n\t\t * Is 'stack' above this exception frame's end?\n\t\t * If yes then skip to the next frame.\n\t\t */\n\t\tif (stack >= end)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Is 'stack' above this exception frame's start address?\n\t\t * If yes then we found the right frame.\n\t\t */\n\t\tif (stack >= end - EXCEPTION_STKSZ) {\n\t\t\t/*\n\t\t\t * Make sure we only iterate through an exception\n\t\t\t * stack once. If it comes up for the second time\n\t\t\t * then there's something wrong going on - just\n\t\t\t * break out and return NULL:\n\t\t\t */\n\t\t\tif (*usedp & (1U << k))\n\t\t\t\tbreak;\n\t\t\t*usedp |= 1U << k;\n\t\t\t*idp = x86_stack_ids[k];\n\t\t\treturn (unsigned long *)end;\n\t\t}\n\t\t/*\n\t\t * If this is a debug stack, and if it has a larger size than\n\t\t * the usual exception stacks, then 'stack' might still\n\t\t * be within the lower portion of the debug stack:\n\t\t */\n#if DEBUG_STKSZ > EXCEPTION_STKSZ\n\t\tif (k == DEBUG_STACK - 1 && stack >= end - DEBUG_STKSZ) {\n\t\t\tunsigned j = N_EXCEPTION_STACKS - 1;\n\n\t\t\t/*\n\t\t\t * Black magic. A large debug stack is composed of\n\t\t\t * multiple exception stack entries, which we\n\t\t\t * iterate through now. Dont look:\n\t\t\t */\n\t\t\tdo {\n\t\t\t\t++j;\n\t\t\t\tend -= EXCEPTION_STKSZ;\n\t\t\t\tx86_stack_ids[j][4] = '1' +\n\t\t\t\t\t\t(j - N_EXCEPTION_STACKS);\n\t\t\t} while (stack < end - EXCEPTION_STKSZ);\n\t\t\tif (*usedp & (1U << j))\n\t\t\t\tbreak;\n\t\t\t*usedp |= 1U << j;\n\t\t\t*idp = x86_stack_ids[j];\n\t\t\treturn (unsigned long *)end;\n\t\t}\n#endif\n\t}\n\treturn NULL;\n}\n\nstatic inline int\nin_irq_stack(unsigned long *stack, unsigned long *irq_stack,\n\t     unsigned long *irq_stack_end)\n{\n\treturn (stack >= irq_stack && stack < irq_stack_end);\n}\n\nstatic const unsigned long irq_stack_size =\n\t(IRQ_STACK_SIZE - 64) / sizeof(unsigned long);\n\nenum stack_type {\n\tSTACK_IS_UNKNOWN,\n\tSTACK_IS_NORMAL,\n\tSTACK_IS_EXCEPTION,\n\tSTACK_IS_IRQ,\n};\n\nstatic enum stack_type\nanalyze_stack(int cpu, struct task_struct *task, unsigned long *stack,\n\t      unsigned long **stack_end, unsigned long *irq_stack,\n\t      unsigned *used, char **id)\n{\n\tunsigned long addr;\n\n\taddr = ((unsigned long)stack & (~(THREAD_SIZE - 1)));\n\tif ((unsigned long)task_stack_page(task) == addr)\n\t\treturn STACK_IS_NORMAL;\n\n\t*stack_end = in_exception_stack(cpu, (unsigned long)stack,\n\t\t\t\t\tused, id);\n\tif (*stack_end)\n\t\treturn STACK_IS_EXCEPTION;\n\n\tif (!irq_stack)\n\t\treturn STACK_IS_NORMAL;\n\n\t*stack_end = irq_stack;\n\tirq_stack = irq_stack - irq_stack_size;\n\n\tif (in_irq_stack(stack, irq_stack, *stack_end))\n\t\treturn STACK_IS_IRQ;\n\n\treturn STACK_IS_UNKNOWN;\n}\n\n/*\n * x86-64 can have up to three kernel stacks:\n * process stack\n * interrupt stack\n * severe exception (double fault, nmi, stack fault, debug, mce) hardware stack\n */\n\nvoid dump_trace(struct task_struct *task, struct pt_regs *regs,\n\t\tunsigned long *stack, unsigned long bp,\n\t\tconst struct stacktrace_ops *ops, void *data)\n{\n\tconst unsigned cpu = get_cpu();\n\tstruct thread_info *tinfo;\n\tunsigned long *irq_stack = (unsigned long *)per_cpu(irq_stack_ptr, cpu);\n\tunsigned long dummy;\n\tunsigned used = 0;\n\tint graph = 0;\n\tint done = 0;\n\n\tif (!task)\n\t\ttask = current;\n\n\tif (!stack) {\n\t\tif (regs)\n\t\t\tstack = (unsigned long *)regs->sp;\n\t\telse if (task != current)\n\t\t\tstack = (unsigned long *)task->thread.sp;\n\t\telse\n\t\t\tstack = &dummy;\n\t}\n\n\tif (!bp)\n\t\tbp = stack_frame(task, regs);\n\t/*\n\t * Print function call entries in all stacks, starting at the\n\t * current stack address. If the stacks consist of nested\n\t * exceptions\n\t */\n\ttinfo = task_thread_info(task);\n\twhile (!done) {\n\t\tunsigned long *stack_end;\n\t\tenum stack_type stype;\n\t\tchar *id;\n\n\t\tstype = analyze_stack(cpu, task, stack, &stack_end,\n\t\t\t\t      irq_stack, &used, &id);\n\n\t\t/* Default finish unless specified to continue */\n\t\tdone = 1;\n\n\t\tswitch (stype) {\n\n\t\t/* Break out early if we are on the thread stack */\n\t\tcase STACK_IS_NORMAL:\n\t\t\tbreak;\n\n\t\tcase STACK_IS_EXCEPTION:\n\n\t\t\tif (ops->stack(data, id) < 0)\n\t\t\t\tbreak;\n\n\t\t\tbp = ops->walk_stack(tinfo, stack, bp, ops,\n\t\t\t\t\t     data, stack_end, &graph);\n\t\t\tops->stack(data, \"<EOE>\");\n\t\t\t/*\n\t\t\t * We link to the next stack via the\n\t\t\t * second-to-last pointer (index -2 to end) in the\n\t\t\t * exception stack:\n\t\t\t */\n\t\t\tstack = (unsigned long *) stack_end[-2];\n\t\t\tdone = 0;\n\t\t\tbreak;\n\n\t\tcase STACK_IS_IRQ:\n\n\t\t\tif (ops->stack(data, \"IRQ\") < 0)\n\t\t\t\tbreak;\n\t\t\tbp = ops->walk_stack(tinfo, stack, bp,\n\t\t\t\t     ops, data, stack_end, &graph);\n\t\t\t/*\n\t\t\t * We link to the next stack (which would be\n\t\t\t * the process stack normally) the last\n\t\t\t * pointer (index -1 to end) in the IRQ stack:\n\t\t\t */\n\t\t\tstack = (unsigned long *) (stack_end[-1]);\n\t\t\tirq_stack = NULL;\n\t\t\tops->stack(data, \"EOI\");\n\t\t\tdone = 0;\n\t\t\tbreak;\n\n\t\tcase STACK_IS_UNKNOWN:\n\t\t\tops->stack(data, \"UNK\");\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * This handles the process stack:\n\t */\n\tbp = ops->walk_stack(tinfo, stack, bp, ops, data, NULL, &graph);\n\tput_cpu();\n}\nEXPORT_SYMBOL(dump_trace);\n\nvoid\nshow_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,\n\t\t   unsigned long *sp, unsigned long bp, char *log_lvl)\n{\n\tunsigned long *irq_stack_end;\n\tunsigned long *irq_stack;\n\tunsigned long *stack;\n\tint cpu;\n\tint i;\n\n\tpreempt_disable();\n\tcpu = smp_processor_id();\n\n\tirq_stack_end\t= (unsigned long *)(per_cpu(irq_stack_ptr, cpu));\n\tirq_stack\t= (unsigned long *)(per_cpu(irq_stack_ptr, cpu) - IRQ_STACK_SIZE);\n\n\t/*\n\t * Debugging aid: \"show_stack(NULL, NULL);\" prints the\n\t * back trace for this cpu:\n\t */\n\tif (sp == NULL) {\n\t\tif (task)\n\t\t\tsp = (unsigned long *)task->thread.sp;\n\t\telse\n\t\t\tsp = (unsigned long *)&sp;\n\t}\n\n\tstack = sp;\n\tfor (i = 0; i < kstack_depth_to_print; i++) {\n\t\tif (stack >= irq_stack && stack <= irq_stack_end) {\n\t\t\tif (stack == irq_stack_end) {\n\t\t\t\tstack = (unsigned long *) (irq_stack_end[-1]);\n\t\t\t\tpr_cont(\" <EOI> \");\n\t\t\t}\n\t\t} else {\n\t\tif (((long) stack & (THREAD_SIZE-1)) == 0)\n\t\t\tbreak;\n\t\t}\n\t\tif (i && ((i % STACKSLOTS_PER_LINE) == 0))\n\t\t\tpr_cont(\"\\n\");\n\t\tpr_cont(\" %016lx\", *stack++);\n\t\ttouch_nmi_watchdog();\n\t}\n\tpreempt_enable();\n\n\tpr_cont(\"\\n\");\n\tshow_trace_log_lvl(task, regs, sp, bp, log_lvl);\n}\n\nvoid show_regs(struct pt_regs *regs)\n{\n\tint i;\n\tunsigned long sp;\n\n\tsp = regs->sp;\n\tshow_regs_print_info(KERN_DEFAULT);\n\t__show_regs(regs, 1);\n\n\t/*\n\t * When in-kernel, we also print out the stack and code at the\n\t * time of the fault..\n\t */\n\tif (!user_mode(regs)) {\n\t\tunsigned int code_prologue = code_bytes * 43 / 64;\n\t\tunsigned int code_len = code_bytes;\n\t\tunsigned char c;\n\t\tu8 *ip;\n\n\t\tprintk(KERN_DEFAULT \"Stack:\\n\");\n\t\tshow_stack_log_lvl(NULL, regs, (unsigned long *)sp,\n\t\t\t\t   0, KERN_DEFAULT);\n\n\t\tprintk(KERN_DEFAULT \"Code: \");\n\n\t\tip = (u8 *)regs->ip - code_prologue;\n\t\tif (ip < (u8 *)PAGE_OFFSET || probe_kernel_address(ip, c)) {\n\t\t\t/* try starting at IP */\n\t\t\tip = (u8 *)regs->ip;\n\t\t\tcode_len = code_len - code_prologue + 1;\n\t\t}\n\t\tfor (i = 0; i < code_len; i++, ip++) {\n\t\t\tif (ip < (u8 *)PAGE_OFFSET ||\n\t\t\t\t\tprobe_kernel_address(ip, c)) {\n\t\t\t\tpr_cont(\" Bad RIP value.\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ip == (u8 *)regs->ip)\n\t\t\t\tpr_cont(\"<%02x> \", c);\n\t\t\telse\n\t\t\t\tpr_cont(\"%02x \", c);\n\t\t}\n\t}\n\tpr_cont(\"\\n\");\n}\n\nint is_valid_bugaddr(unsigned long ip)\n{\n\tunsigned short ud2;\n\n\tif (__copy_from_user(&ud2, (const void __user *) ip, sizeof(ud2)))\n\t\treturn 0;\n\n\treturn ud2 == 0x0b0f;\n}\n", "/*\n *  linux/arch/x86_64/entry.S\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *  Copyright (C) 2000, 2001, 2002  Andi Kleen SuSE Labs\n *  Copyright (C) 2000  Pavel Machek <pavel@suse.cz>\n */\n\n/*\n * entry.S contains the system-call and fault low-level handling routines.\n *\n * Some of this is documented in Documentation/x86/entry_64.txt\n *\n * NOTE: This code handles signal-recognition, which happens every time\n * after an interrupt and after each system call.\n *\n * Normal syscalls and interrupts don't save a full stack frame, this is\n * only done for syscall tracing, signals or fork/exec et.al.\n *\n * A note on terminology:\n * - top of stack: Architecture defined interrupt frame from SS to RIP\n * at the top of the kernel process stack.\n * - partial stack frame: partially saved registers up to R11.\n * - full stack frame: Like partial stack frame, but all register saved.\n *\n * Some macro usage:\n * - CFI macros are used to generate dwarf2 unwind information for better\n * backtraces. They don't change any code.\n * - SAVE_ALL/RESTORE_ALL - Save/restore all registers\n * - SAVE_ARGS/RESTORE_ARGS - Save/restore registers that C functions modify.\n * There are unfortunately lots of special cases where some registers\n * not touched. The macro is a big mess that should be cleaned up.\n * - SAVE_REST/RESTORE_REST - Handle the registers not saved by SAVE_ARGS.\n * Gives a full stack frame.\n * - ENTRY/END Define functions in the symbol table.\n * - FIXUP_TOP_OF_STACK/RESTORE_TOP_OF_STACK - Fix up the hardware stack\n * frame that is otherwise undefined after a SYSCALL\n * - TRACE_IRQ_* - Trace hard interrupt state for lock debugging.\n * - idtentry - Define exception entry points.\n */\n\n#include <linux/linkage.h>\n#include <asm/segment.h>\n#include <asm/cache.h>\n#include <asm/errno.h>\n#include <asm/dwarf2.h>\n#include <asm/calling.h>\n#include <asm/asm-offsets.h>\n#include <asm/msr.h>\n#include <asm/unistd.h>\n#include <asm/thread_info.h>\n#include <asm/hw_irq.h>\n#include <asm/page_types.h>\n#include <asm/irqflags.h>\n#include <asm/paravirt.h>\n#include <asm/percpu.h>\n#include <asm/asm.h>\n#include <asm/context_tracking.h>\n#include <asm/smap.h>\n#include <asm/pgtable_types.h>\n#include <linux/err.h>\n\n/* Avoid __ASSEMBLER__'ifying <linux/audit.h> just for this.  */\n#include <linux/elf-em.h>\n#define AUDIT_ARCH_X86_64\t(EM_X86_64|__AUDIT_ARCH_64BIT|__AUDIT_ARCH_LE)\n#define __AUDIT_ARCH_64BIT 0x80000000\n#define __AUDIT_ARCH_LE\t   0x40000000\n\n\t.code64\n\t.section .entry.text, \"ax\"\n\n\n#ifndef CONFIG_PREEMPT\n#define retint_kernel retint_restore_args\n#endif\n\n#ifdef CONFIG_PARAVIRT\nENTRY(native_usergs_sysret64)\n\tswapgs\n\tsysretq\nENDPROC(native_usergs_sysret64)\n#endif /* CONFIG_PARAVIRT */\n\n\n.macro TRACE_IRQS_IRETQ offset=ARGOFFSET\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tbt   $9,EFLAGS-\\offset(%rsp)\t/* interrupts off? */\n\tjnc  1f\n\tTRACE_IRQS_ON\n1:\n#endif\n.endm\n\n/*\n * When dynamic function tracer is enabled it will add a breakpoint\n * to all locations that it is about to modify, sync CPUs, update\n * all the code, sync CPUs, then remove the breakpoints. In this time\n * if lockdep is enabled, it might jump back into the debug handler\n * outside the updating of the IST protection. (TRACE_IRQS_ON/OFF).\n *\n * We need to change the IDT table before calling TRACE_IRQS_ON/OFF to\n * make sure the stack pointer does not get reset back to the top\n * of the debug stack, and instead just reuses the current stack.\n */\n#if defined(CONFIG_DYNAMIC_FTRACE) && defined(CONFIG_TRACE_IRQFLAGS)\n\n.macro TRACE_IRQS_OFF_DEBUG\n\tcall debug_stack_set_zero\n\tTRACE_IRQS_OFF\n\tcall debug_stack_reset\n.endm\n\n.macro TRACE_IRQS_ON_DEBUG\n\tcall debug_stack_set_zero\n\tTRACE_IRQS_ON\n\tcall debug_stack_reset\n.endm\n\n.macro TRACE_IRQS_IRETQ_DEBUG offset=ARGOFFSET\n\tbt   $9,EFLAGS-\\offset(%rsp)\t/* interrupts off? */\n\tjnc  1f\n\tTRACE_IRQS_ON_DEBUG\n1:\n.endm\n\n#else\n# define TRACE_IRQS_OFF_DEBUG\t\tTRACE_IRQS_OFF\n# define TRACE_IRQS_ON_DEBUG\t\tTRACE_IRQS_ON\n# define TRACE_IRQS_IRETQ_DEBUG\t\tTRACE_IRQS_IRETQ\n#endif\n\n/*\n * C code is not supposed to know about undefined top of stack. Every time\n * a C function with an pt_regs argument is called from the SYSCALL based\n * fast path FIXUP_TOP_OF_STACK is needed.\n * RESTORE_TOP_OF_STACK syncs the syscall state after any possible ptregs\n * manipulation.\n */\n\n\t/* %rsp:at FRAMEEND */\n\t.macro FIXUP_TOP_OF_STACK tmp offset=0\n\tmovq PER_CPU_VAR(old_rsp),\\tmp\n\tmovq \\tmp,RSP+\\offset(%rsp)\n\tmovq $__USER_DS,SS+\\offset(%rsp)\n\tmovq $__USER_CS,CS+\\offset(%rsp)\n\tmovq $-1,RCX+\\offset(%rsp)\n\tmovq R11+\\offset(%rsp),\\tmp  /* get eflags */\n\tmovq \\tmp,EFLAGS+\\offset(%rsp)\n\t.endm\n\n\t.macro RESTORE_TOP_OF_STACK tmp offset=0\n\tmovq RSP+\\offset(%rsp),\\tmp\n\tmovq \\tmp,PER_CPU_VAR(old_rsp)\n\tmovq EFLAGS+\\offset(%rsp),\\tmp\n\tmovq \\tmp,R11+\\offset(%rsp)\n\t.endm\n\n\t.macro FAKE_STACK_FRAME child_rip\n\t/* push in order ss, rsp, eflags, cs, rip */\n\txorl %eax, %eax\n\tpushq_cfi $__KERNEL_DS /* ss */\n\t/*CFI_REL_OFFSET\tss,0*/\n\tpushq_cfi %rax /* rsp */\n\tCFI_REL_OFFSET\trsp,0\n\tpushq_cfi $(X86_EFLAGS_IF|X86_EFLAGS_FIXED) /* eflags - interrupts on */\n\t/*CFI_REL_OFFSET\trflags,0*/\n\tpushq_cfi $__KERNEL_CS /* cs */\n\t/*CFI_REL_OFFSET\tcs,0*/\n\tpushq_cfi \\child_rip /* rip */\n\tCFI_REL_OFFSET\trip,0\n\tpushq_cfi %rax /* orig rax */\n\t.endm\n\n\t.macro UNFAKE_STACK_FRAME\n\taddq $8*6, %rsp\n\tCFI_ADJUST_CFA_OFFSET\t-(6*8)\n\t.endm\n\n/*\n * initial frame state for interrupts (and exceptions without error code)\n */\n\t.macro EMPTY_FRAME start=1 offset=0\n\t.if \\start\n\tCFI_STARTPROC simple\n\tCFI_SIGNAL_FRAME\n\tCFI_DEF_CFA rsp,8+\\offset\n\t.else\n\tCFI_DEF_CFA_OFFSET 8+\\offset\n\t.endif\n\t.endm\n\n/*\n * initial frame state for interrupts (and exceptions without error code)\n */\n\t.macro INTR_FRAME start=1 offset=0\n\tEMPTY_FRAME \\start, SS+8+\\offset-RIP\n\t/*CFI_REL_OFFSET ss, SS+\\offset-RIP*/\n\tCFI_REL_OFFSET rsp, RSP+\\offset-RIP\n\t/*CFI_REL_OFFSET rflags, EFLAGS+\\offset-RIP*/\n\t/*CFI_REL_OFFSET cs, CS+\\offset-RIP*/\n\tCFI_REL_OFFSET rip, RIP+\\offset-RIP\n\t.endm\n\n/*\n * initial frame state for exceptions with error code (and interrupts\n * with vector already pushed)\n */\n\t.macro XCPT_FRAME start=1 offset=0\n\tINTR_FRAME \\start, RIP+\\offset-ORIG_RAX\n\t.endm\n\n/*\n * frame that enables calling into C.\n */\n\t.macro PARTIAL_FRAME start=1 offset=0\n\tXCPT_FRAME \\start, ORIG_RAX+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rdi, RDI+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rsi, RSI+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rdx, RDX+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rcx, RCX+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rax, RAX+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET r8, R8+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET r9, R9+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET r10, R10+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET r11, R11+\\offset-ARGOFFSET\n\t.endm\n\n/*\n * frame that enables passing a complete pt_regs to a C function.\n */\n\t.macro DEFAULT_FRAME start=1 offset=0\n\tPARTIAL_FRAME \\start, R11+\\offset-R15\n\tCFI_REL_OFFSET rbx, RBX+\\offset\n\tCFI_REL_OFFSET rbp, RBP+\\offset\n\tCFI_REL_OFFSET r12, R12+\\offset\n\tCFI_REL_OFFSET r13, R13+\\offset\n\tCFI_REL_OFFSET r14, R14+\\offset\n\tCFI_REL_OFFSET r15, R15+\\offset\n\t.endm\n\n/* save partial stack frame */\n\t.macro SAVE_ARGS_IRQ\n\tcld\n\t/* start from rbp in pt_regs and jump over */\n\tmovq_cfi rdi, (RDI-RBP)\n\tmovq_cfi rsi, (RSI-RBP)\n\tmovq_cfi rdx, (RDX-RBP)\n\tmovq_cfi rcx, (RCX-RBP)\n\tmovq_cfi rax, (RAX-RBP)\n\tmovq_cfi  r8,  (R8-RBP)\n\tmovq_cfi  r9,  (R9-RBP)\n\tmovq_cfi r10, (R10-RBP)\n\tmovq_cfi r11, (R11-RBP)\n\n\t/* Save rbp so that we can unwind from get_irq_regs() */\n\tmovq_cfi rbp, 0\n\n\t/* Save previous stack value */\n\tmovq %rsp, %rsi\n\n\tleaq -RBP(%rsp),%rdi\t/* arg1 for handler */\n\ttestl $3, CS-RBP(%rsi)\n\tje 1f\n\tSWAPGS\n\t/*\n\t * irq_count is used to check if a CPU is already on an interrupt stack\n\t * or not. While this is essentially redundant with preempt_count it is\n\t * a little cheaper to use a separate counter in the PDA (short of\n\t * moving irq_enter into assembly, which would be too much work)\n\t */\n1:\tincl PER_CPU_VAR(irq_count)\n\tcmovzq PER_CPU_VAR(irq_stack_ptr),%rsp\n\tCFI_DEF_CFA_REGISTER\trsi\n\n\t/* Store previous stack value */\n\tpushq %rsi\n\tCFI_ESCAPE\t0x0f /* DW_CFA_def_cfa_expression */, 6, \\\n\t\t\t0x77 /* DW_OP_breg7 */, 0, \\\n\t\t\t0x06 /* DW_OP_deref */, \\\n\t\t\t0x08 /* DW_OP_const1u */, SS+8-RBP, \\\n\t\t\t0x22 /* DW_OP_plus */\n\t/* We entered an interrupt context - irqs are off: */\n\tTRACE_IRQS_OFF\n\t.endm\n\nENTRY(save_paranoid)\n\tXCPT_FRAME 1 RDI+8\n\tcld\n\tmovq %rdi, RDI+8(%rsp)\n\tmovq %rsi, RSI+8(%rsp)\n\tmovq_cfi rdx, RDX+8\n\tmovq_cfi rcx, RCX+8\n\tmovq_cfi rax, RAX+8\n\tmovq %r8, R8+8(%rsp)\n\tmovq %r9, R9+8(%rsp)\n\tmovq %r10, R10+8(%rsp)\n\tmovq %r11, R11+8(%rsp)\n\tmovq_cfi rbx, RBX+8\n\tmovq %rbp, RBP+8(%rsp)\n\tmovq %r12, R12+8(%rsp)\n\tmovq %r13, R13+8(%rsp)\n\tmovq %r14, R14+8(%rsp)\n\tmovq %r15, R15+8(%rsp)\n\tmovl $1,%ebx\n\tmovl $MSR_GS_BASE,%ecx\n\trdmsr\n\ttestl %edx,%edx\n\tjs 1f\t/* negative -> in kernel */\n\tSWAPGS\n\txorl %ebx,%ebx\n1:\tret\n\tCFI_ENDPROC\nEND(save_paranoid)\n\n/*\n * A newly forked process directly context switches into this address.\n *\n * rdi: prev task we switched from\n */\nENTRY(ret_from_fork)\n\tDEFAULT_FRAME\n\n\tLOCK ; btr $TIF_FORK,TI_flags(%r8)\n\n\tpushq_cfi $0x0002\n\tpopfq_cfi\t\t\t\t# reset kernel eflags\n\n\tcall schedule_tail\t\t\t# rdi: 'prev' task parameter\n\n\tGET_THREAD_INFO(%rcx)\n\n\tRESTORE_REST\n\n\ttestl $3, CS-ARGOFFSET(%rsp)\t\t# from kernel_thread?\n\tjz   1f\n\n\ttestl $_TIF_IA32, TI_flags(%rcx)\t# 32-bit compat task needs IRET\n\tjnz  int_ret_from_sys_call\n\n\tRESTORE_TOP_OF_STACK %rdi, -ARGOFFSET\n\tjmp ret_from_sys_call\t\t\t# go to the SYSRET fastpath\n\n1:\n\tsubq $REST_SKIP, %rsp\t# leave space for volatiles\n\tCFI_ADJUST_CFA_OFFSET\tREST_SKIP\n\tmovq %rbp, %rdi\n\tcall *%rbx\n\tmovl $0, RAX(%rsp)\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(ret_from_fork)\n\n/*\n * System call entry. Up to 6 arguments in registers are supported.\n *\n * SYSCALL does not save anything on the stack and does not change the\n * stack pointer.  However, it does mask the flags register for us, so\n * CLD and CLAC are not needed.\n */\n\n/*\n * Register setup:\n * rax  system call number\n * rdi  arg0\n * rcx  return address for syscall/sysret, C arg3\n * rsi  arg1\n * rdx  arg2\n * r10  arg3 \t(--> moved to rcx for C)\n * r8   arg4\n * r9   arg5\n * r11  eflags for syscall/sysret, temporary for C\n * r12-r15,rbp,rbx saved by C code, not touched.\n *\n * Interrupts are off on entry.\n * Only called from user space.\n *\n * XXX\tif we had a free scratch register we could save the RSP into the stack frame\n *      and report it properly in ps. Unfortunately we haven't.\n *\n * When user can change the frames always force IRET. That is because\n * it deals with uncanonical addresses better. SYSRET has trouble\n * with them due to bugs in both AMD and Intel CPUs.\n */\n\nENTRY(system_call)\n\tCFI_STARTPROC\tsimple\n\tCFI_SIGNAL_FRAME\n\tCFI_DEF_CFA\trsp,KERNEL_STACK_OFFSET\n\tCFI_REGISTER\trip,rcx\n\t/*CFI_REGISTER\trflags,r11*/\n\tSWAPGS_UNSAFE_STACK\n\t/*\n\t * A hypervisor implementation might want to use a label\n\t * after the swapgs, so that it can do the swapgs\n\t * for the guest and jump here on syscall.\n\t */\nGLOBAL(system_call_after_swapgs)\n\n\tmovq\t%rsp,PER_CPU_VAR(old_rsp)\n\tmovq\tPER_CPU_VAR(kernel_stack),%rsp\n\t/*\n\t * No need to follow this irqs off/on section - it's straight\n\t * and short:\n\t */\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tSAVE_ARGS 8, 0, rax_enosys=1\n\tmovq_cfi rax,(ORIG_RAX-ARGOFFSET)\n\tmovq  %rcx,RIP-ARGOFFSET(%rsp)\n\tCFI_REL_OFFSET rip,RIP-ARGOFFSET\n\ttestl $_TIF_WORK_SYSCALL_ENTRY,TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET)\n\tjnz tracesys\nsystem_call_fastpath:\n#if __SYSCALL_MASK == ~0\n\tcmpq $__NR_syscall_max,%rax\n#else\n\tandl $__SYSCALL_MASK,%eax\n\tcmpl $__NR_syscall_max,%eax\n#endif\n\tja ret_from_sys_call  /* and return regs->ax */\n\tmovq %r10,%rcx\n\tcall *sys_call_table(,%rax,8)  # XXX:\t rip relative\n\tmovq %rax,RAX-ARGOFFSET(%rsp)\n/*\n * Syscall return path ending with SYSRET (fast path)\n * Has incomplete stack frame and undefined top of stack.\n */\nret_from_sys_call:\n\tmovl $_TIF_ALLWORK_MASK,%edi\n\t/* edi:\tflagmask */\nsysret_check:\n\tLOCKDEP_SYS_EXIT\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tmovl TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET),%edx\n\tandl %edi,%edx\n\tjnz  sysret_careful\n\tCFI_REMEMBER_STATE\n\t/*\n\t * sysretq will re-enable interrupts:\n\t */\n\tTRACE_IRQS_ON\n\tmovq RIP-ARGOFFSET(%rsp),%rcx\n\tCFI_REGISTER\trip,rcx\n\tRESTORE_ARGS 1,-ARG_SKIP,0\n\t/*CFI_REGISTER\trflags,r11*/\n\tmovq\tPER_CPU_VAR(old_rsp), %rsp\n\tUSERGS_SYSRET64\n\n\tCFI_RESTORE_STATE\n\t/* Handle reschedules */\n\t/* edx:\twork, edi: workmask */\nsysret_careful:\n\tbt $TIF_NEED_RESCHED,%edx\n\tjnc sysret_signal\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tpushq_cfi %rdi\n\tSCHEDULE_USER\n\tpopq_cfi %rdi\n\tjmp sysret_check\n\n\t/* Handle a signal */\nsysret_signal:\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n#ifdef CONFIG_AUDITSYSCALL\n\tbt $TIF_SYSCALL_AUDIT,%edx\n\tjc sysret_audit\n#endif\n\t/*\n\t * We have a signal, or exit tracing or single-step.\n\t * These all wind up with the iret return path anyway,\n\t * so just join that path right now.\n\t */\n\tFIXUP_TOP_OF_STACK %r11, -ARGOFFSET\n\tjmp int_check_syscall_exit_work\n\n#ifdef CONFIG_AUDITSYSCALL\n\t/*\n\t * Return fast path for syscall audit.  Call __audit_syscall_exit()\n\t * directly and then jump back to the fast path with TIF_SYSCALL_AUDIT\n\t * masked off.\n\t */\nsysret_audit:\n\tmovq RAX-ARGOFFSET(%rsp),%rsi\t/* second arg, syscall return value */\n\tcmpq $-MAX_ERRNO,%rsi\t/* is it < -MAX_ERRNO? */\n\tsetbe %al\t\t/* 1 if so, 0 if not */\n\tmovzbl %al,%edi\t\t/* zero-extend that into %edi */\n\tcall __audit_syscall_exit\n\tmovl $(_TIF_ALLWORK_MASK & ~_TIF_SYSCALL_AUDIT),%edi\n\tjmp sysret_check\n#endif\t/* CONFIG_AUDITSYSCALL */\n\n\t/* Do syscall tracing */\ntracesys:\n\tleaq -REST_SKIP(%rsp), %rdi\n\tmovq $AUDIT_ARCH_X86_64, %rsi\n\tcall syscall_trace_enter_phase1\n\ttest %rax, %rax\n\tjnz tracesys_phase2\t\t/* if needed, run the slow path */\n\tLOAD_ARGS 0\t\t\t/* else restore clobbered regs */\n\tjmp system_call_fastpath\t/*      and return to the fast path */\n\ntracesys_phase2:\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %rdi\n\tmovq %rsp, %rdi\n\tmovq $AUDIT_ARCH_X86_64, %rsi\n\tmovq %rax,%rdx\n\tcall syscall_trace_enter_phase2\n\n\t/*\n\t * Reload arg registers from stack in case ptrace changed them.\n\t * We don't reload %rax because syscall_trace_entry_phase2() returned\n\t * the value it wants us to use in the table lookup.\n\t */\n\tLOAD_ARGS ARGOFFSET, 1\n\tRESTORE_REST\n#if __SYSCALL_MASK == ~0\n\tcmpq $__NR_syscall_max,%rax\n#else\n\tandl $__SYSCALL_MASK,%eax\n\tcmpl $__NR_syscall_max,%eax\n#endif\n\tja   int_ret_from_sys_call\t/* RAX(%rsp) is already set */\n\tmovq %r10,%rcx\t/* fixup for C */\n\tcall *sys_call_table(,%rax,8)\n\tmovq %rax,RAX-ARGOFFSET(%rsp)\n\t/* Use IRET because user could have changed frame */\n\n/*\n * Syscall return path ending with IRET.\n * Has correct top of stack, but partial stack frame.\n */\nGLOBAL(int_ret_from_sys_call)\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tmovl $_TIF_ALLWORK_MASK,%edi\n\t/* edi:\tmask to check */\nGLOBAL(int_with_check)\n\tLOCKDEP_SYS_EXIT_IRQ\n\tGET_THREAD_INFO(%rcx)\n\tmovl TI_flags(%rcx),%edx\n\tandl %edi,%edx\n\tjnz   int_careful\n\tandl    $~TS_COMPAT,TI_status(%rcx)\n\tjmp   retint_swapgs\n\n\t/* Either reschedule or signal or syscall exit tracking needed. */\n\t/* First do a reschedule test. */\n\t/* edx:\twork, edi: workmask */\nint_careful:\n\tbt $TIF_NEED_RESCHED,%edx\n\tjnc  int_very_careful\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tpushq_cfi %rdi\n\tSCHEDULE_USER\n\tpopq_cfi %rdi\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tjmp int_with_check\n\n\t/* handle signals and tracing -- both require a full stack frame */\nint_very_careful:\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\nint_check_syscall_exit_work:\n\tSAVE_REST\n\t/* Check for syscall exit trace */\n\ttestl $_TIF_WORK_SYSCALL_EXIT,%edx\n\tjz int_signal\n\tpushq_cfi %rdi\n\tleaq 8(%rsp),%rdi\t# &ptregs -> arg1\n\tcall syscall_trace_leave\n\tpopq_cfi %rdi\n\tandl $~(_TIF_WORK_SYSCALL_EXIT|_TIF_SYSCALL_EMU),%edi\n\tjmp int_restore_rest\n\nint_signal:\n\ttestl $_TIF_DO_NOTIFY_MASK,%edx\n\tjz 1f\n\tmovq %rsp,%rdi\t\t# &ptregs -> arg1\n\txorl %esi,%esi\t\t# oldset -> arg2\n\tcall do_notify_resume\n1:\tmovl $_TIF_WORK_MASK,%edi\nint_restore_rest:\n\tRESTORE_REST\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tjmp int_with_check\n\tCFI_ENDPROC\nEND(system_call)\n\n\t.macro FORK_LIKE func\nENTRY(stub_\\func)\n\tCFI_STARTPROC\n\tpopq\t%r11\t\t\t/* save return address */\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tpushq\t%r11\t\t\t/* put it back on stack */\n\tFIXUP_TOP_OF_STACK %r11, 8\n\tDEFAULT_FRAME 0 8\t\t/* offset 8: return address */\n\tcall sys_\\func\n\tRESTORE_TOP_OF_STACK %r11, 8\n\tret $REST_SKIP\t\t/* pop extended registers */\n\tCFI_ENDPROC\nEND(stub_\\func)\n\t.endm\n\n\t.macro FIXED_FRAME label,func\nENTRY(\\label)\n\tCFI_STARTPROC\n\tPARTIAL_FRAME 0 8\t\t/* offset 8: return address */\n\tFIXUP_TOP_OF_STACK %r11, 8-ARGOFFSET\n\tcall \\func\n\tRESTORE_TOP_OF_STACK %r11, 8-ARGOFFSET\n\tret\n\tCFI_ENDPROC\nEND(\\label)\n\t.endm\n\n\tFORK_LIKE  clone\n\tFORK_LIKE  fork\n\tFORK_LIKE  vfork\n\tFIXED_FRAME stub_iopl, sys_iopl\n\nENTRY(ptregscall_common)\n\tDEFAULT_FRAME 1 8\t/* offset 8: return address */\n\tRESTORE_TOP_OF_STACK %r11, 8\n\tmovq_cfi_restore R15+8, r15\n\tmovq_cfi_restore R14+8, r14\n\tmovq_cfi_restore R13+8, r13\n\tmovq_cfi_restore R12+8, r12\n\tmovq_cfi_restore RBP+8, rbp\n\tmovq_cfi_restore RBX+8, rbx\n\tret $REST_SKIP\t\t/* pop extended registers */\n\tCFI_ENDPROC\nEND(ptregscall_common)\n\nENTRY(stub_execve)\n\tCFI_STARTPROC\n\taddq $8, %rsp\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %r11\n\tcall sys_execve\n\tmovq %rax,RAX(%rsp)\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(stub_execve)\n\n/*\n * sigreturn is special because it needs to restore all registers on return.\n * This cannot be done with SYSRET, so use the IRET return path instead.\n */\nENTRY(stub_rt_sigreturn)\n\tCFI_STARTPROC\n\taddq $8, %rsp\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %r11\n\tcall sys_rt_sigreturn\n\tmovq %rax,RAX(%rsp) # fixme, this could be done at the higher layer\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(stub_rt_sigreturn)\n\n#ifdef CONFIG_X86_X32_ABI\nENTRY(stub_x32_rt_sigreturn)\n\tCFI_STARTPROC\n\taddq $8, %rsp\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %r11\n\tcall sys32_x32_rt_sigreturn\n\tmovq %rax,RAX(%rsp) # fixme, this could be done at the higher layer\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(stub_x32_rt_sigreturn)\n\nENTRY(stub_x32_execve)\n\tCFI_STARTPROC\n\taddq $8, %rsp\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %r11\n\tcall compat_sys_execve\n\tRESTORE_TOP_OF_STACK %r11\n\tmovq %rax,RAX(%rsp)\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(stub_x32_execve)\n\n#endif\n\n/*\n * Build the entry stubs and pointer table with some assembler magic.\n * We pack 7 stubs into a single 32-byte chunk, which will fit in a\n * single cache line on all modern x86 implementations.\n */\n\t.section .init.rodata,\"a\"\nENTRY(interrupt)\n\t.section .entry.text\n\t.p2align 5\n\t.p2align CONFIG_X86_L1_CACHE_SHIFT\nENTRY(irq_entries_start)\n\tINTR_FRAME\nvector=FIRST_EXTERNAL_VECTOR\n.rept (NR_VECTORS-FIRST_EXTERNAL_VECTOR+6)/7\n\t.balign 32\n  .rept\t7\n    .if vector < NR_VECTORS\n      .if vector <> FIRST_EXTERNAL_VECTOR\n\tCFI_ADJUST_CFA_OFFSET -8\n      .endif\n1:\tpushq_cfi $(~vector+0x80)\t/* Note: always in signed byte range */\n      .if ((vector-FIRST_EXTERNAL_VECTOR)%7) <> 6\n\tjmp 2f\n      .endif\n      .previous\n\t.quad 1b\n      .section .entry.text\nvector=vector+1\n    .endif\n  .endr\n2:\tjmp common_interrupt\n.endr\n\tCFI_ENDPROC\nEND(irq_entries_start)\n\n.previous\nEND(interrupt)\n.previous\n\n/*\n * Interrupt entry/exit.\n *\n * Interrupt entry points save only callee clobbered registers in fast path.\n *\n * Entry runs with interrupts off.\n */\n\n/* 0(%rsp): ~(interrupt number) */\n\t.macro interrupt func\n\t/* reserve pt_regs for scratch regs and rbp */\n\tsubq $ORIG_RAX-RBP, %rsp\n\tCFI_ADJUST_CFA_OFFSET ORIG_RAX-RBP\n\tSAVE_ARGS_IRQ\n\tcall \\func\n\t.endm\n\n\t/*\n\t * The interrupt stubs push (~vector+0x80) onto the stack and\n\t * then jump to common_interrupt.\n\t */\n\t.p2align CONFIG_X86_L1_CACHE_SHIFT\ncommon_interrupt:\n\tXCPT_FRAME\n\tASM_CLAC\n\taddq $-0x80,(%rsp)\t\t/* Adjust vector to [-256,-1] range */\n\tinterrupt do_IRQ\n\t/* 0(%rsp): old_rsp-ARGOFFSET */\nret_from_intr:\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tdecl PER_CPU_VAR(irq_count)\n\n\t/* Restore saved previous stack */\n\tpopq %rsi\n\tCFI_DEF_CFA rsi,SS+8-RBP\t/* reg/off reset after def_cfa_expr */\n\tleaq ARGOFFSET-RBP(%rsi), %rsp\n\tCFI_DEF_CFA_REGISTER\trsp\n\tCFI_ADJUST_CFA_OFFSET\tRBP-ARGOFFSET\n\nexit_intr:\n\tGET_THREAD_INFO(%rcx)\n\ttestl $3,CS-ARGOFFSET(%rsp)\n\tje retint_kernel\n\n\t/* Interrupt came from user space */\n\t/*\n\t * Has a correct top of stack, but a partial stack frame\n\t * %rcx: thread info. Interrupts off.\n\t */\nretint_with_reschedule:\n\tmovl $_TIF_WORK_MASK,%edi\nretint_check:\n\tLOCKDEP_SYS_EXIT_IRQ\n\tmovl TI_flags(%rcx),%edx\n\tandl %edi,%edx\n\tCFI_REMEMBER_STATE\n\tjnz  retint_careful\n\nretint_swapgs:\t\t/* return to user-space */\n\t/*\n\t * The iretq could re-enable interrupts:\n\t */\n\tDISABLE_INTERRUPTS(CLBR_ANY)\n\tTRACE_IRQS_IRETQ\n\tSWAPGS\n\tjmp restore_args\n\nretint_restore_args:\t/* return to kernel space */\n\tDISABLE_INTERRUPTS(CLBR_ANY)\n\t/*\n\t * The iretq could re-enable interrupts:\n\t */\n\tTRACE_IRQS_IRETQ\nrestore_args:\n\tRESTORE_ARGS 1,8,1\n\nirq_return:\n\tINTERRUPT_RETURN\n\nENTRY(native_iret)\n\t/*\n\t * Are we returning to a stack segment from the LDT?  Note: in\n\t * 64-bit mode SS:RSP on the exception stack is always valid.\n\t */\n#ifdef CONFIG_X86_ESPFIX64\n\ttestb $4,(SS-RIP)(%rsp)\n\tjnz native_irq_return_ldt\n#endif\n\n.global native_irq_return_iret\nnative_irq_return_iret:\n\tiretq\n\t_ASM_EXTABLE(native_irq_return_iret, bad_iret)\n\n#ifdef CONFIG_X86_ESPFIX64\nnative_irq_return_ldt:\n\tpushq_cfi %rax\n\tpushq_cfi %rdi\n\tSWAPGS\n\tmovq PER_CPU_VAR(espfix_waddr),%rdi\n\tmovq %rax,(0*8)(%rdi)\t/* RAX */\n\tmovq (2*8)(%rsp),%rax\t/* RIP */\n\tmovq %rax,(1*8)(%rdi)\n\tmovq (3*8)(%rsp),%rax\t/* CS */\n\tmovq %rax,(2*8)(%rdi)\n\tmovq (4*8)(%rsp),%rax\t/* RFLAGS */\n\tmovq %rax,(3*8)(%rdi)\n\tmovq (6*8)(%rsp),%rax\t/* SS */\n\tmovq %rax,(5*8)(%rdi)\n\tmovq (5*8)(%rsp),%rax\t/* RSP */\n\tmovq %rax,(4*8)(%rdi)\n\tandl $0xffff0000,%eax\n\tpopq_cfi %rdi\n\torq PER_CPU_VAR(espfix_stack),%rax\n\tSWAPGS\n\tmovq %rax,%rsp\n\tpopq_cfi %rax\n\tjmp native_irq_return_iret\n#endif\n\n\t.section .fixup,\"ax\"\nbad_iret:\n\t/*\n\t * The iret traps when the %cs or %ss being restored is bogus.\n\t * We've lost the original trap vector and error code.\n\t * #GPF is the most likely one to get for an invalid selector.\n\t * So pretend we completed the iret and took the #GPF in user mode.\n\t *\n\t * We are now running with the kernel GS after exception recovery.\n\t * But error_entry expects us to have user GS to match the user %cs,\n\t * so swap back.\n\t */\n\tpushq $0\n\n\tSWAPGS\n\tjmp general_protection\n\n\t.previous\n\n\t/* edi: workmask, edx: work */\nretint_careful:\n\tCFI_RESTORE_STATE\n\tbt    $TIF_NEED_RESCHED,%edx\n\tjnc   retint_signal\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tpushq_cfi %rdi\n\tSCHEDULE_USER\n\tpopq_cfi %rdi\n\tGET_THREAD_INFO(%rcx)\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tjmp retint_check\n\nretint_signal:\n\ttestl $_TIF_DO_NOTIFY_MASK,%edx\n\tjz    retint_swapgs\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tSAVE_REST\n\tmovq $-1,ORIG_RAX(%rsp)\n\txorl %esi,%esi\t\t# oldset\n\tmovq %rsp,%rdi\t\t# &pt_regs\n\tcall do_notify_resume\n\tRESTORE_REST\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tGET_THREAD_INFO(%rcx)\n\tjmp retint_with_reschedule\n\n#ifdef CONFIG_PREEMPT\n\t/* Returning to kernel space. Check if we need preemption */\n\t/* rcx:\t threadinfo. interrupts off. */\nENTRY(retint_kernel)\n\tcmpl $0,PER_CPU_VAR(__preempt_count)\n\tjnz  retint_restore_args\n\tbt   $9,EFLAGS-ARGOFFSET(%rsp)\t/* interrupts off? */\n\tjnc  retint_restore_args\n\tcall preempt_schedule_irq\n\tjmp exit_intr\n#endif\n\tCFI_ENDPROC\nEND(common_interrupt)\n\n/*\n * APIC interrupts.\n */\n.macro apicinterrupt3 num sym do_sym\nENTRY(\\sym)\n\tINTR_FRAME\n\tASM_CLAC\n\tpushq_cfi $~(\\num)\n.Lcommon_\\sym:\n\tinterrupt \\do_sym\n\tjmp ret_from_intr\n\tCFI_ENDPROC\nEND(\\sym)\n.endm\n\n#ifdef CONFIG_TRACING\n#define trace(sym) trace_##sym\n#define smp_trace(sym) smp_trace_##sym\n\n.macro trace_apicinterrupt num sym\napicinterrupt3 \\num trace(\\sym) smp_trace(\\sym)\n.endm\n#else\n.macro trace_apicinterrupt num sym do_sym\n.endm\n#endif\n\n.macro apicinterrupt num sym do_sym\napicinterrupt3 \\num \\sym \\do_sym\ntrace_apicinterrupt \\num \\sym\n.endm\n\n#ifdef CONFIG_SMP\napicinterrupt3 IRQ_MOVE_CLEANUP_VECTOR \\\n\tirq_move_cleanup_interrupt smp_irq_move_cleanup_interrupt\napicinterrupt3 REBOOT_VECTOR \\\n\treboot_interrupt smp_reboot_interrupt\n#endif\n\n#ifdef CONFIG_X86_UV\napicinterrupt3 UV_BAU_MESSAGE \\\n\tuv_bau_message_intr1 uv_bau_message_interrupt\n#endif\napicinterrupt LOCAL_TIMER_VECTOR \\\n\tapic_timer_interrupt smp_apic_timer_interrupt\napicinterrupt X86_PLATFORM_IPI_VECTOR \\\n\tx86_platform_ipi smp_x86_platform_ipi\n\n#ifdef CONFIG_HAVE_KVM\napicinterrupt3 POSTED_INTR_VECTOR \\\n\tkvm_posted_intr_ipi smp_kvm_posted_intr_ipi\n#endif\n\n#ifdef CONFIG_X86_MCE_THRESHOLD\napicinterrupt THRESHOLD_APIC_VECTOR \\\n\tthreshold_interrupt smp_threshold_interrupt\n#endif\n\n#ifdef CONFIG_X86_THERMAL_VECTOR\napicinterrupt THERMAL_APIC_VECTOR \\\n\tthermal_interrupt smp_thermal_interrupt\n#endif\n\n#ifdef CONFIG_SMP\napicinterrupt CALL_FUNCTION_SINGLE_VECTOR \\\n\tcall_function_single_interrupt smp_call_function_single_interrupt\napicinterrupt CALL_FUNCTION_VECTOR \\\n\tcall_function_interrupt smp_call_function_interrupt\napicinterrupt RESCHEDULE_VECTOR \\\n\treschedule_interrupt smp_reschedule_interrupt\n#endif\n\napicinterrupt ERROR_APIC_VECTOR \\\n\terror_interrupt smp_error_interrupt\napicinterrupt SPURIOUS_APIC_VECTOR \\\n\tspurious_interrupt smp_spurious_interrupt\n\n#ifdef CONFIG_IRQ_WORK\napicinterrupt IRQ_WORK_VECTOR \\\n\tirq_work_interrupt smp_irq_work_interrupt\n#endif\n\n/*\n * Exception entry points.\n */\n#define INIT_TSS_IST(x) PER_CPU_VAR(init_tss) + (TSS_ist + ((x) - 1) * 8)\n\n.macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1\nENTRY(\\sym)\n\t/* Sanity check */\n\t.if \\shift_ist != -1 && \\paranoid == 0\n\t.error \"using shift_ist requires paranoid=1\"\n\t.endif\n\n\t.if \\has_error_code\n\tXCPT_FRAME\n\t.else\n\tINTR_FRAME\n\t.endif\n\n\tASM_CLAC\n\tPARAVIRT_ADJUST_EXCEPTION_FRAME\n\n\t.ifeq \\has_error_code\n\tpushq_cfi $-1\t\t\t/* ORIG_RAX: no syscall to restart */\n\t.endif\n\n\tsubq $ORIG_RAX-R15, %rsp\n\tCFI_ADJUST_CFA_OFFSET ORIG_RAX-R15\n\n\t.if \\paranoid\n\tcall save_paranoid\n\t.else\n\tcall error_entry\n\t.endif\n\n\tDEFAULT_FRAME 0\n\n\t.if \\paranoid\n\t.if \\shift_ist != -1\n\tTRACE_IRQS_OFF_DEBUG\t\t/* reload IDT in case of recursion */\n\t.else\n\tTRACE_IRQS_OFF\n\t.endif\n\t.endif\n\n\tmovq %rsp,%rdi\t\t\t/* pt_regs pointer */\n\n\t.if \\has_error_code\n\tmovq ORIG_RAX(%rsp),%rsi\t/* get error code */\n\tmovq $-1,ORIG_RAX(%rsp)\t\t/* no syscall to restart */\n\t.else\n\txorl %esi,%esi\t\t\t/* no error code */\n\t.endif\n\n\t.if \\shift_ist != -1\n\tsubq $EXCEPTION_STKSZ, INIT_TSS_IST(\\shift_ist)\n\t.endif\n\n\tcall \\do_sym\n\n\t.if \\shift_ist != -1\n\taddq $EXCEPTION_STKSZ, INIT_TSS_IST(\\shift_ist)\n\t.endif\n\n\t.if \\paranoid\n\tjmp paranoid_exit\t\t/* %ebx: no swapgs flag */\n\t.else\n\tjmp error_exit\t\t\t/* %ebx: no swapgs flag */\n\t.endif\n\n\tCFI_ENDPROC\nEND(\\sym)\n.endm\n\n#ifdef CONFIG_TRACING\n.macro trace_idtentry sym do_sym has_error_code:req\nidtentry trace(\\sym) trace(\\do_sym) has_error_code=\\has_error_code\nidtentry \\sym \\do_sym has_error_code=\\has_error_code\n.endm\n#else\n.macro trace_idtentry sym do_sym has_error_code:req\nidtentry \\sym \\do_sym has_error_code=\\has_error_code\n.endm\n#endif\n\nidtentry divide_error do_divide_error has_error_code=0\nidtentry overflow do_overflow has_error_code=0\nidtentry bounds do_bounds has_error_code=0\nidtentry invalid_op do_invalid_op has_error_code=0\nidtentry device_not_available do_device_not_available has_error_code=0\nidtentry double_fault do_double_fault has_error_code=1 paranoid=1\nidtentry coprocessor_segment_overrun do_coprocessor_segment_overrun has_error_code=0\nidtentry invalid_TSS do_invalid_TSS has_error_code=1\nidtentry segment_not_present do_segment_not_present has_error_code=1\nidtentry spurious_interrupt_bug do_spurious_interrupt_bug has_error_code=0\nidtentry coprocessor_error do_coprocessor_error has_error_code=0\nidtentry alignment_check do_alignment_check has_error_code=1\nidtentry simd_coprocessor_error do_simd_coprocessor_error has_error_code=0\n\n\n\t/* Reload gs selector with exception handling */\n\t/* edi:  new selector */\nENTRY(native_load_gs_index)\n\tCFI_STARTPROC\n\tpushfq_cfi\n\tDISABLE_INTERRUPTS(CLBR_ANY & ~CLBR_RDI)\n\tSWAPGS\ngs_change:\n\tmovl %edi,%gs\n2:\tmfence\t\t/* workaround */\n\tSWAPGS\n\tpopfq_cfi\n\tret\n\tCFI_ENDPROC\nEND(native_load_gs_index)\n\n\t_ASM_EXTABLE(gs_change,bad_gs)\n\t.section .fixup,\"ax\"\n\t/* running with kernelgs */\nbad_gs:\n\tSWAPGS\t\t\t/* switch back to user gs */\n\txorl %eax,%eax\n\tmovl %eax,%gs\n\tjmp  2b\n\t.previous\n\n/* Call softirq on interrupt stack. Interrupts are off. */\nENTRY(do_softirq_own_stack)\n\tCFI_STARTPROC\n\tpushq_cfi %rbp\n\tCFI_REL_OFFSET rbp,0\n\tmov  %rsp,%rbp\n\tCFI_DEF_CFA_REGISTER rbp\n\tincl PER_CPU_VAR(irq_count)\n\tcmove PER_CPU_VAR(irq_stack_ptr),%rsp\n\tpush  %rbp\t\t\t# backlink for old unwinder\n\tcall __do_softirq\n\tleaveq\n\tCFI_RESTORE\t\trbp\n\tCFI_DEF_CFA_REGISTER\trsp\n\tCFI_ADJUST_CFA_OFFSET   -8\n\tdecl PER_CPU_VAR(irq_count)\n\tret\n\tCFI_ENDPROC\nEND(do_softirq_own_stack)\n\n#ifdef CONFIG_XEN\nidtentry xen_hypervisor_callback xen_do_hypervisor_callback has_error_code=0\n\n/*\n * A note on the \"critical region\" in our callback handler.\n * We want to avoid stacking callback handlers due to events occurring\n * during handling of the last event. To do this, we keep events disabled\n * until we've done all processing. HOWEVER, we must enable events before\n * popping the stack frame (can't be done atomically) and so it would still\n * be possible to get enough handler activations to overflow the stack.\n * Although unlikely, bugs of that kind are hard to track down, so we'd\n * like to avoid the possibility.\n * So, on entry to the handler we detect whether we interrupted an\n * existing activation in its critical region -- if so, we pop the current\n * activation and restart the handler using the previous one.\n */\nENTRY(xen_do_hypervisor_callback)   # do_hypervisor_callback(struct *pt_regs)\n\tCFI_STARTPROC\n/*\n * Since we don't modify %rdi, evtchn_do_upall(struct *pt_regs) will\n * see the correct pointer to the pt_regs\n */\n\tmovq %rdi, %rsp            # we don't return, adjust the stack frame\n\tCFI_ENDPROC\n\tDEFAULT_FRAME\n11:\tincl PER_CPU_VAR(irq_count)\n\tmovq %rsp,%rbp\n\tCFI_DEF_CFA_REGISTER rbp\n\tcmovzq PER_CPU_VAR(irq_stack_ptr),%rsp\n\tpushq %rbp\t\t\t# backlink for old unwinder\n\tcall xen_evtchn_do_upcall\n\tpopq %rsp\n\tCFI_DEF_CFA_REGISTER rsp\n\tdecl PER_CPU_VAR(irq_count)\n\tjmp  error_exit\n\tCFI_ENDPROC\nEND(xen_do_hypervisor_callback)\n\n/*\n * Hypervisor uses this for application faults while it executes.\n * We get here for two reasons:\n *  1. Fault while reloading DS, ES, FS or GS\n *  2. Fault while executing IRET\n * Category 1 we do not need to fix up as Xen has already reloaded all segment\n * registers that could be reloaded and zeroed the others.\n * Category 2 we fix up by killing the current process. We cannot use the\n * normal Linux return path in this case because if we use the IRET hypercall\n * to pop the stack frame we end up in an infinite loop of failsafe callbacks.\n * We distinguish between categories by comparing each saved segment register\n * with its current contents: any discrepancy means we in category 1.\n */\nENTRY(xen_failsafe_callback)\n\tINTR_FRAME 1 (6*8)\n\t/*CFI_REL_OFFSET gs,GS*/\n\t/*CFI_REL_OFFSET fs,FS*/\n\t/*CFI_REL_OFFSET es,ES*/\n\t/*CFI_REL_OFFSET ds,DS*/\n\tCFI_REL_OFFSET r11,8\n\tCFI_REL_OFFSET rcx,0\n\tmovw %ds,%cx\n\tcmpw %cx,0x10(%rsp)\n\tCFI_REMEMBER_STATE\n\tjne 1f\n\tmovw %es,%cx\n\tcmpw %cx,0x18(%rsp)\n\tjne 1f\n\tmovw %fs,%cx\n\tcmpw %cx,0x20(%rsp)\n\tjne 1f\n\tmovw %gs,%cx\n\tcmpw %cx,0x28(%rsp)\n\tjne 1f\n\t/* All segments match their saved values => Category 2 (Bad IRET). */\n\tmovq (%rsp),%rcx\n\tCFI_RESTORE rcx\n\tmovq 8(%rsp),%r11\n\tCFI_RESTORE r11\n\taddq $0x30,%rsp\n\tCFI_ADJUST_CFA_OFFSET -0x30\n\tpushq_cfi $0\t/* RIP */\n\tpushq_cfi %r11\n\tpushq_cfi %rcx\n\tjmp general_protection\n\tCFI_RESTORE_STATE\n1:\t/* Segment mismatch => Category 1 (Bad segment). Retry the IRET. */\n\tmovq (%rsp),%rcx\n\tCFI_RESTORE rcx\n\tmovq 8(%rsp),%r11\n\tCFI_RESTORE r11\n\taddq $0x30,%rsp\n\tCFI_ADJUST_CFA_OFFSET -0x30\n\tpushq_cfi $-1 /* orig_ax = -1 => not a system call */\n\tSAVE_ALL\n\tjmp error_exit\n\tCFI_ENDPROC\nEND(xen_failsafe_callback)\n\napicinterrupt3 HYPERVISOR_CALLBACK_VECTOR \\\n\txen_hvm_callback_vector xen_evtchn_do_upcall\n\n#endif /* CONFIG_XEN */\n\n#if IS_ENABLED(CONFIG_HYPERV)\napicinterrupt3 HYPERVISOR_CALLBACK_VECTOR \\\n\thyperv_callback_vector hyperv_vector_handler\n#endif /* CONFIG_HYPERV */\n\nidtentry debug do_debug has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK\nidtentry int3 do_int3 has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK\nidtentry stack_segment do_stack_segment has_error_code=1 paranoid=1\n#ifdef CONFIG_XEN\nidtentry xen_debug do_debug has_error_code=0\nidtentry xen_int3 do_int3 has_error_code=0\nidtentry xen_stack_segment do_stack_segment has_error_code=1\n#endif\nidtentry general_protection do_general_protection has_error_code=1\ntrace_idtentry page_fault do_page_fault has_error_code=1\n#ifdef CONFIG_KVM_GUEST\nidtentry async_page_fault do_async_page_fault has_error_code=1\n#endif\n#ifdef CONFIG_X86_MCE\nidtentry machine_check has_error_code=0 paranoid=1 do_sym=*machine_check_vector(%rip)\n#endif\n\n\t/*\n\t * \"Paranoid\" exit path from exception stack.\n\t * Paranoid because this is used by NMIs and cannot take\n\t * any kernel state for granted.\n\t * We don't do kernel preemption checks here, because only\n\t * NMI should be common and it does not enable IRQs and\n\t * cannot get reschedule ticks.\n\t *\n\t * \"trace\" is 0 for the NMI handler only, because irq-tracing\n\t * is fundamentally NMI-unsafe. (we cannot change the soft and\n\t * hard flags at once, atomically)\n\t */\n\n\t/* ebx:\tno swapgs flag */\nENTRY(paranoid_exit)\n\tDEFAULT_FRAME\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF_DEBUG\n\ttestl %ebx,%ebx\t\t\t\t/* swapgs needed? */\n\tjnz paranoid_restore\n\ttestl $3,CS(%rsp)\n\tjnz   paranoid_userspace\nparanoid_swapgs:\n\tTRACE_IRQS_IRETQ 0\n\tSWAPGS_UNSAFE_STACK\n\tRESTORE_ALL 8\n\tjmp irq_return\nparanoid_restore:\n\tTRACE_IRQS_IRETQ_DEBUG 0\n\tRESTORE_ALL 8\n\tjmp irq_return\nparanoid_userspace:\n\tGET_THREAD_INFO(%rcx)\n\tmovl TI_flags(%rcx),%ebx\n\tandl $_TIF_WORK_MASK,%ebx\n\tjz paranoid_swapgs\n\tmovq %rsp,%rdi\t\t\t/* &pt_regs */\n\tcall sync_regs\n\tmovq %rax,%rsp\t\t\t/* switch stack for scheduling */\n\ttestl $_TIF_NEED_RESCHED,%ebx\n\tjnz paranoid_schedule\n\tmovl %ebx,%edx\t\t\t/* arg3: thread flags */\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\txorl %esi,%esi \t\t\t/* arg2: oldset */\n\tmovq %rsp,%rdi \t\t\t/* arg1: &pt_regs */\n\tcall do_notify_resume\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tjmp paranoid_userspace\nparanoid_schedule:\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_ANY)\n\tSCHEDULE_USER\n\tDISABLE_INTERRUPTS(CLBR_ANY)\n\tTRACE_IRQS_OFF\n\tjmp paranoid_userspace\n\tCFI_ENDPROC\nEND(paranoid_exit)\n\n/*\n * Exception entry point. This expects an error code/orig_rax on the stack.\n * returns in \"no swapgs flag\" in %ebx.\n */\nENTRY(error_entry)\n\tXCPT_FRAME\n\tCFI_ADJUST_CFA_OFFSET 15*8\n\t/* oldrax contains error code */\n\tcld\n\tmovq %rdi, RDI+8(%rsp)\n\tmovq %rsi, RSI+8(%rsp)\n\tmovq %rdx, RDX+8(%rsp)\n\tmovq %rcx, RCX+8(%rsp)\n\tmovq %rax, RAX+8(%rsp)\n\tmovq  %r8,  R8+8(%rsp)\n\tmovq  %r9,  R9+8(%rsp)\n\tmovq %r10, R10+8(%rsp)\n\tmovq %r11, R11+8(%rsp)\n\tmovq_cfi rbx, RBX+8\n\tmovq %rbp, RBP+8(%rsp)\n\tmovq %r12, R12+8(%rsp)\n\tmovq %r13, R13+8(%rsp)\n\tmovq %r14, R14+8(%rsp)\n\tmovq %r15, R15+8(%rsp)\n\txorl %ebx,%ebx\n\ttestl $3,CS+8(%rsp)\n\tje error_kernelspace\nerror_swapgs:\n\tSWAPGS\nerror_sti:\n\tTRACE_IRQS_OFF\n\tret\n\n/*\n * There are two places in the kernel that can potentially fault with\n * usergs. Handle them here. The exception handlers after iret run with\n * kernel gs again, so don't set the user space flag. B stepping K8s\n * sometimes report an truncated RIP for IRET exceptions returning to\n * compat mode. Check for these here too.\n */\nerror_kernelspace:\n\tCFI_REL_OFFSET rcx, RCX+8\n\tincl %ebx\n\tleaq native_irq_return_iret(%rip),%rcx\n\tcmpq %rcx,RIP+8(%rsp)\n\tje error_swapgs\n\tmovl %ecx,%eax\t/* zero extend */\n\tcmpq %rax,RIP+8(%rsp)\n\tje bstep_iret\n\tcmpq $gs_change,RIP+8(%rsp)\n\tje error_swapgs\n\tjmp error_sti\n\nbstep_iret:\n\t/* Fix truncated RIP */\n\tmovq %rcx,RIP+8(%rsp)\n\tjmp error_swapgs\n\tCFI_ENDPROC\nEND(error_entry)\n\n\n/* ebx:\tno swapgs flag (1: don't need swapgs, 0: need it) */\nENTRY(error_exit)\n\tDEFAULT_FRAME\n\tmovl %ebx,%eax\n\tRESTORE_REST\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tGET_THREAD_INFO(%rcx)\n\ttestl %eax,%eax\n\tjne retint_kernel\n\tLOCKDEP_SYS_EXIT_IRQ\n\tmovl TI_flags(%rcx),%edx\n\tmovl $_TIF_WORK_MASK,%edi\n\tandl %edi,%edx\n\tjnz retint_careful\n\tjmp retint_swapgs\n\tCFI_ENDPROC\nEND(error_exit)\n\n/*\n * Test if a given stack is an NMI stack or not.\n */\n\t.macro test_in_nmi reg stack nmi_ret normal_ret\n\tcmpq %\\reg, \\stack\n\tja \\normal_ret\n\tsubq $EXCEPTION_STKSZ, %\\reg\n\tcmpq %\\reg, \\stack\n\tjb \\normal_ret\n\tjmp \\nmi_ret\n\t.endm\n\n\t/* runs on exception stack */\nENTRY(nmi)\n\tINTR_FRAME\n\tPARAVIRT_ADJUST_EXCEPTION_FRAME\n\t/*\n\t * We allow breakpoints in NMIs. If a breakpoint occurs, then\n\t * the iretq it performs will take us out of NMI context.\n\t * This means that we can have nested NMIs where the next\n\t * NMI is using the top of the stack of the previous NMI. We\n\t * can't let it execute because the nested NMI will corrupt the\n\t * stack of the previous NMI. NMI handlers are not re-entrant\n\t * anyway.\n\t *\n\t * To handle this case we do the following:\n\t *  Check the a special location on the stack that contains\n\t *  a variable that is set when NMIs are executing.\n\t *  The interrupted task's stack is also checked to see if it\n\t *  is an NMI stack.\n\t *  If the variable is not set and the stack is not the NMI\n\t *  stack then:\n\t *    o Set the special variable on the stack\n\t *    o Copy the interrupt frame into a \"saved\" location on the stack\n\t *    o Copy the interrupt frame into a \"copy\" location on the stack\n\t *    o Continue processing the NMI\n\t *  If the variable is set or the previous stack is the NMI stack:\n\t *    o Modify the \"copy\" location to jump to the repeate_nmi\n\t *    o return back to the first NMI\n\t *\n\t * Now on exit of the first NMI, we first clear the stack variable\n\t * The NMI stack will tell any nested NMIs at that point that it is\n\t * nested. Then we pop the stack normally with iret, and if there was\n\t * a nested NMI that updated the copy interrupt stack frame, a\n\t * jump will be made to the repeat_nmi code that will handle the second\n\t * NMI.\n\t */\n\n\t/* Use %rdx as out temp variable throughout */\n\tpushq_cfi %rdx\n\tCFI_REL_OFFSET rdx, 0\n\n\t/*\n\t * If %cs was not the kernel segment, then the NMI triggered in user\n\t * space, which means it is definitely not nested.\n\t */\n\tcmpl $__KERNEL_CS, 16(%rsp)\n\tjne first_nmi\n\n\t/*\n\t * Check the special variable on the stack to see if NMIs are\n\t * executing.\n\t */\n\tcmpl $1, -8(%rsp)\n\tje nested_nmi\n\n\t/*\n\t * Now test if the previous stack was an NMI stack.\n\t * We need the double check. We check the NMI stack to satisfy the\n\t * race when the first NMI clears the variable before returning.\n\t * We check the variable because the first NMI could be in a\n\t * breakpoint routine using a breakpoint stack.\n\t */\n\tlea 6*8(%rsp), %rdx\n\ttest_in_nmi rdx, 4*8(%rsp), nested_nmi, first_nmi\n\tCFI_REMEMBER_STATE\n\nnested_nmi:\n\t/*\n\t * Do nothing if we interrupted the fixup in repeat_nmi.\n\t * It's about to repeat the NMI handler, so we are fine\n\t * with ignoring this one.\n\t */\n\tmovq $repeat_nmi, %rdx\n\tcmpq 8(%rsp), %rdx\n\tja 1f\n\tmovq $end_repeat_nmi, %rdx\n\tcmpq 8(%rsp), %rdx\n\tja nested_nmi_out\n\n1:\n\t/* Set up the interrupted NMIs stack to jump to repeat_nmi */\n\tleaq -1*8(%rsp), %rdx\n\tmovq %rdx, %rsp\n\tCFI_ADJUST_CFA_OFFSET 1*8\n\tleaq -10*8(%rsp), %rdx\n\tpushq_cfi $__KERNEL_DS\n\tpushq_cfi %rdx\n\tpushfq_cfi\n\tpushq_cfi $__KERNEL_CS\n\tpushq_cfi $repeat_nmi\n\n\t/* Put stack back */\n\taddq $(6*8), %rsp\n\tCFI_ADJUST_CFA_OFFSET -6*8\n\nnested_nmi_out:\n\tpopq_cfi %rdx\n\tCFI_RESTORE rdx\n\n\t/* No need to check faults here */\n\tINTERRUPT_RETURN\n\n\tCFI_RESTORE_STATE\nfirst_nmi:\n\t/*\n\t * Because nested NMIs will use the pushed location that we\n\t * stored in rdx, we must keep that space available.\n\t * Here's what our stack frame will look like:\n\t * +-------------------------+\n\t * | original SS             |\n\t * | original Return RSP     |\n\t * | original RFLAGS         |\n\t * | original CS             |\n\t * | original RIP            |\n\t * +-------------------------+\n\t * | temp storage for rdx    |\n\t * +-------------------------+\n\t * | NMI executing variable  |\n\t * +-------------------------+\n\t * | copied SS               |\n\t * | copied Return RSP       |\n\t * | copied RFLAGS           |\n\t * | copied CS               |\n\t * | copied RIP              |\n\t * +-------------------------+\n\t * | Saved SS                |\n\t * | Saved Return RSP        |\n\t * | Saved RFLAGS            |\n\t * | Saved CS                |\n\t * | Saved RIP               |\n\t * +-------------------------+\n\t * | pt_regs                 |\n\t * +-------------------------+\n\t *\n\t * The saved stack frame is used to fix up the copied stack frame\n\t * that a nested NMI may change to make the interrupted NMI iret jump\n\t * to the repeat_nmi. The original stack frame and the temp storage\n\t * is also used by nested NMIs and can not be trusted on exit.\n\t */\n\t/* Do not pop rdx, nested NMIs will corrupt that part of the stack */\n\tmovq (%rsp), %rdx\n\tCFI_RESTORE rdx\n\n\t/* Set the NMI executing variable on the stack. */\n\tpushq_cfi $1\n\n\t/*\n\t * Leave room for the \"copied\" frame\n\t */\n\tsubq $(5*8), %rsp\n\tCFI_ADJUST_CFA_OFFSET 5*8\n\n\t/* Copy the stack frame to the Saved frame */\n\t.rept 5\n\tpushq_cfi 11*8(%rsp)\n\t.endr\n\tCFI_DEF_CFA_OFFSET SS+8-RIP\n\n\t/* Everything up to here is safe from nested NMIs */\n\n\t/*\n\t * If there was a nested NMI, the first NMI's iret will return\n\t * here. But NMIs are still enabled and we can take another\n\t * nested NMI. The nested NMI checks the interrupted RIP to see\n\t * if it is between repeat_nmi and end_repeat_nmi, and if so\n\t * it will just return, as we are about to repeat an NMI anyway.\n\t * This makes it safe to copy to the stack frame that a nested\n\t * NMI will update.\n\t */\nrepeat_nmi:\n\t/*\n\t * Update the stack variable to say we are still in NMI (the update\n\t * is benign for the non-repeat case, where 1 was pushed just above\n\t * to this very stack slot).\n\t */\n\tmovq $1, 10*8(%rsp)\n\n\t/* Make another copy, this one may be modified by nested NMIs */\n\taddq $(10*8), %rsp\n\tCFI_ADJUST_CFA_OFFSET -10*8\n\t.rept 5\n\tpushq_cfi -6*8(%rsp)\n\t.endr\n\tsubq $(5*8), %rsp\n\tCFI_DEF_CFA_OFFSET SS+8-RIP\nend_repeat_nmi:\n\n\t/*\n\t * Everything below this point can be preempted by a nested\n\t * NMI if the first NMI took an exception and reset our iret stack\n\t * so that we repeat another NMI.\n\t */\n\tpushq_cfi $-1\t\t/* ORIG_RAX: no syscall to restart */\n\tsubq $ORIG_RAX-R15, %rsp\n\tCFI_ADJUST_CFA_OFFSET ORIG_RAX-R15\n\t/*\n\t * Use save_paranoid to handle SWAPGS, but no need to use paranoid_exit\n\t * as we should not be calling schedule in NMI context.\n\t * Even with normal interrupts enabled. An NMI should not be\n\t * setting NEED_RESCHED or anything that normal interrupts and\n\t * exceptions might do.\n\t */\n\tcall save_paranoid\n\tDEFAULT_FRAME 0\n\n\t/*\n\t * Save off the CR2 register. If we take a page fault in the NMI then\n\t * it could corrupt the CR2 value. If the NMI preempts a page fault\n\t * handler before it was able to read the CR2 register, and then the\n\t * NMI itself takes a page fault, the page fault that was preempted\n\t * will read the information from the NMI page fault and not the\n\t * origin fault. Save it off and restore it if it changes.\n\t * Use the r12 callee-saved register.\n\t */\n\tmovq %cr2, %r12\n\n\t/* paranoidentry do_nmi, 0; without TRACE_IRQS_OFF */\n\tmovq %rsp,%rdi\n\tmovq $-1,%rsi\n\tcall do_nmi\n\n\t/* Did the NMI take a page fault? Restore cr2 if it did */\n\tmovq %cr2, %rcx\n\tcmpq %rcx, %r12\n\tje 1f\n\tmovq %r12, %cr2\n1:\n\t\n\ttestl %ebx,%ebx\t\t\t\t/* swapgs needed? */\n\tjnz nmi_restore\nnmi_swapgs:\n\tSWAPGS_UNSAFE_STACK\nnmi_restore:\n\t/* Pop the extra iret frame at once */\n\tRESTORE_ALL 6*8\n\n\t/* Clear the NMI executing stack variable */\n\tmovq $0, 5*8(%rsp)\n\tjmp irq_return\n\tCFI_ENDPROC\nEND(nmi)\n\nENTRY(ignore_sysret)\n\tCFI_STARTPROC\n\tmov $-ENOSYS,%eax\n\tsysret\n\tCFI_ENDPROC\nEND(ignore_sysret)\n\n", "/*\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs\n *\n *  Pentium III FXSR, SSE support\n *\tGareth Hughes <gareth@valinux.com>, May 2000\n */\n\n/*\n * Handle hardware traps and faults.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/context_tracking.h>\n#include <linux/interrupt.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/kprobes.h>\n#include <linux/uaccess.h>\n#include <linux/kdebug.h>\n#include <linux/kgdb.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/ptrace.h>\n#include <linux/uprobes.h>\n#include <linux/string.h>\n#include <linux/delay.h>\n#include <linux/errno.h>\n#include <linux/kexec.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/init.h>\n#include <linux/bug.h>\n#include <linux/nmi.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/io.h>\n\n#ifdef CONFIG_EISA\n#include <linux/ioport.h>\n#include <linux/eisa.h>\n#endif\n\n#if defined(CONFIG_EDAC)\n#include <linux/edac.h>\n#endif\n\n#include <asm/kmemcheck.h>\n#include <asm/stacktrace.h>\n#include <asm/processor.h>\n#include <asm/debugreg.h>\n#include <linux/atomic.h>\n#include <asm/ftrace.h>\n#include <asm/traps.h>\n#include <asm/desc.h>\n#include <asm/i387.h>\n#include <asm/fpu-internal.h>\n#include <asm/mce.h>\n#include <asm/fixmap.h>\n#include <asm/mach_traps.h>\n#include <asm/alternative.h>\n\n#ifdef CONFIG_X86_64\n#include <asm/x86_init.h>\n#include <asm/pgalloc.h>\n#include <asm/proto.h>\n\n/* No need to be aligned, but done to keep all IDTs defined the same way. */\ngate_desc debug_idt_table[NR_VECTORS] __page_aligned_bss;\n#else\n#include <asm/processor-flags.h>\n#include <asm/setup.h>\n\nasmlinkage int system_call(void);\n#endif\n\n/* Must be page-aligned because the real IDT is used in a fixmap. */\ngate_desc idt_table[NR_VECTORS] __page_aligned_bss;\n\nDECLARE_BITMAP(used_vectors, NR_VECTORS);\nEXPORT_SYMBOL_GPL(used_vectors);\n\nstatic inline void conditional_sti(struct pt_regs *regs)\n{\n\tif (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n}\n\nstatic inline void preempt_conditional_sti(struct pt_regs *regs)\n{\n\tpreempt_count_inc();\n\tif (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n}\n\nstatic inline void conditional_cli(struct pt_regs *regs)\n{\n\tif (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_disable();\n}\n\nstatic inline void preempt_conditional_cli(struct pt_regs *regs)\n{\n\tif (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_disable();\n\tpreempt_count_dec();\n}\n\nstatic nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n#ifdef CONFIG_X86_32\n\tif (regs->flags & X86_VM_MASK) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n#endif\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n\nstatic siginfo_t *fill_trap_info(struct pt_regs *regs, int signr, int trapnr,\n\t\t\t\tsiginfo_t *info)\n{\n\tunsigned long siaddr;\n\tint sicode;\n\n\tswitch (trapnr) {\n\tdefault:\n\t\treturn SEND_SIG_PRIV;\n\n\tcase X86_TRAP_DE:\n\t\tsicode = FPE_INTDIV;\n\t\tsiaddr = uprobe_get_trap_addr(regs);\n\t\tbreak;\n\tcase X86_TRAP_UD:\n\t\tsicode = ILL_ILLOPN;\n\t\tsiaddr = uprobe_get_trap_addr(regs);\n\t\tbreak;\n\tcase X86_TRAP_AC:\n\t\tsicode = BUS_ADRALN;\n\t\tsiaddr = 0;\n\t\tbreak;\n\t}\n\n\tinfo->si_signo = signr;\n\tinfo->si_errno = 0;\n\tinfo->si_code = sicode;\n\tinfo->si_addr = (void __user *)siaddr;\n\treturn info;\n}\n\nstatic void\ndo_trap(int trapnr, int signr, char *str, struct pt_regs *regs,\n\tlong error_code, siginfo_t *info)\n{\n\tstruct task_struct *tsk = current;\n\n\n\tif (!do_trap_no_signal(tsk, trapnr, str, regs, error_code))\n\t\treturn;\n\t/*\n\t * We want error_code and trap_nr set for userspace faults and\n\t * kernelspace faults which result in die(), but not\n\t * kernelspace faults which are fixed up.  die() gives the\n\t * process no chance to handle the signal and notice the\n\t * kernel fault information, so that won't result in polluting\n\t * the information about previously queued, but not yet\n\t * delivered, faults.  See also do_general_protection below.\n\t */\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = trapnr;\n\n#ifdef CONFIG_X86_64\n\tif (show_unhandled_signals && unhandled_signal(tsk, signr) &&\n\t    printk_ratelimit()) {\n\t\tpr_info(\"%s[%d] trap %s ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, tsk->pid, str,\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n#endif\n\n\tforce_sig_info(signr, info ?: SEND_SIG_PRIV, tsk);\n}\nNOKPROBE_SYMBOL(do_trap);\n\nstatic void do_error_trap(struct pt_regs *regs, long error_code, char *str,\n\t\t\t  unsigned long trapnr, int signr)\n{\n\tenum ctx_state prev_state = exception_enter();\n\tsiginfo_t info;\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=\n\t\t\tNOTIFY_STOP) {\n\t\tconditional_sti(regs);\n\t\tdo_trap(trapnr, signr, str, regs, error_code,\n\t\t\tfill_trap_info(regs, signr, trapnr, &info));\n\t}\n\n\texception_exit(prev_state);\n}\n\n#define DO_ERROR(trapnr, signr, str, name)\t\t\t\t\\\ndotraplinkage void do_##name(struct pt_regs *regs, long error_code)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tdo_error_trap(regs, error_code, str, trapnr, signr);\t\t\\\n}\n\nDO_ERROR(X86_TRAP_DE,     SIGFPE,  \"divide error\",\t\tdivide_error)\nDO_ERROR(X86_TRAP_OF,     SIGSEGV, \"overflow\",\t\t\toverflow)\nDO_ERROR(X86_TRAP_BR,     SIGSEGV, \"bounds\",\t\t\tbounds)\nDO_ERROR(X86_TRAP_UD,     SIGILL,  \"invalid opcode\",\t\tinvalid_op)\nDO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  \"coprocessor segment overrun\",coprocessor_segment_overrun)\nDO_ERROR(X86_TRAP_TS,     SIGSEGV, \"invalid TSS\",\t\tinvalid_TSS)\nDO_ERROR(X86_TRAP_NP,     SIGBUS,  \"segment not present\",\tsegment_not_present)\n#ifdef CONFIG_X86_32\nDO_ERROR(X86_TRAP_SS,     SIGBUS,  \"stack segment\",\t\tstack_segment)\n#endif\nDO_ERROR(X86_TRAP_AC,     SIGBUS,  \"alignment check\",\t\talignment_check)\n\n#ifdef CONFIG_X86_64\n/* Runs on IST stack */\ndotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)\n{\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tif (notify_die(DIE_TRAP, \"stack segment\", regs, error_code,\n\t\t       X86_TRAP_SS, SIGBUS) != NOTIFY_STOP) {\n\t\tpreempt_conditional_sti(regs);\n\t\tdo_trap(X86_TRAP_SS, SIGBUS, \"stack segment\", regs, error_code, NULL);\n\t\tpreempt_conditional_cli(regs);\n\t}\n\texception_exit(prev_state);\n}\n\ndotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)\n{\n\tstatic const char str[] = \"double fault\";\n\tstruct task_struct *tsk = current;\n\n#ifdef CONFIG_X86_ESPFIX64\n\textern unsigned char native_irq_return_iret[];\n\n\t/*\n\t * If IRET takes a non-IST fault on the espfix64 stack, then we\n\t * end up promoting it to a doublefault.  In that case, modify\n\t * the stack to make it look like we just entered the #GP\n\t * handler from user space, similar to bad_iret.\n\t */\n\tif (((long)regs->sp >> PGDIR_SHIFT) == ESPFIX_PGD_ENTRY &&\n\t\tregs->cs == __KERNEL_CS &&\n\t\tregs->ip == (unsigned long)native_irq_return_iret)\n\t{\n\t\tstruct pt_regs *normal_regs = task_pt_regs(current);\n\n\t\t/* Fake a #GP(0) from userspace. */\n\t\tmemmove(&normal_regs->ip, (void *)regs->sp, 5*8);\n\t\tnormal_regs->orig_ax = 0;  /* Missing (lost) #GP error code */\n\t\tregs->ip = (unsigned long)general_protection;\n\t\tregs->sp = (unsigned long)&normal_regs->orig_ax;\n\t\treturn;\n\t}\n#endif\n\n\texception_enter();\n\t/* Return not checked because double check cannot be ignored */\n\tnotify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_DF;\n\n#ifdef CONFIG_DOUBLEFAULT\n\tdf_debug(regs, error_code);\n#endif\n\t/*\n\t * This is always a kernel trap and never fixable (and thus must\n\t * never return).\n\t */\n\tfor (;;)\n\t\tdie(str, regs, error_code);\n}\n#endif\n\ndotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tconditional_sti(regs);\n\n#ifdef CONFIG_X86_32\n\tif (regs->flags & X86_VM_MASK) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\tgoto exit;\n\t}\n#endif\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs))\n\t\t\tgoto exit;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\tgoto exit;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\nexit:\n\texception_exit(prev_state);\n}\nNOKPROBE_SYMBOL(do_general_protection);\n\n/* May run on IST stack. */\ndotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n{\n\tenum ctx_state prev_state;\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t/*\n\t * ftrace must be first, everything else may cause a recursive crash.\n\t * See note by declaration of modifying_ftrace_code in ftrace.c\n\t */\n\tif (unlikely(atomic_read(&modifying_ftrace_code)) &&\n\t    ftrace_int3_handler(regs))\n\t\treturn;\n#endif\n\tif (poke_int3_handler(regs))\n\t\treturn;\n\n\tprev_state = exception_enter();\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n\n#ifdef CONFIG_KPROBES\n\tif (kprobe_int3_handler(regs))\n\t\tgoto exit;\n#endif\n\n\tif (notify_die(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n\n\t/*\n\t * Let others (NMI) know that the debug stack is in use\n\t * as we may switch to the interrupt stack.\n\t */\n\tdebug_stack_usage_inc();\n\tpreempt_conditional_sti(regs);\n\tdo_trap(X86_TRAP_BP, SIGTRAP, \"int3\", regs, error_code, NULL);\n\tpreempt_conditional_cli(regs);\n\tdebug_stack_usage_dec();\nexit:\n\texception_exit(prev_state);\n}\nNOKPROBE_SYMBOL(do_int3);\n\n#ifdef CONFIG_X86_64\n/*\n * Help handler running on IST stack to switch back to user stack\n * for scheduling or signal handling. The actual stack switch is done in\n * entry.S\n */\nasmlinkage __visible struct pt_regs *sync_regs(struct pt_regs *eregs)\n{\n\tstruct pt_regs *regs = eregs;\n\t/* Did already sync */\n\tif (eregs == (struct pt_regs *)eregs->sp)\n\t\t;\n\t/* Exception from user space */\n\telse if (user_mode(eregs))\n\t\tregs = task_pt_regs(current);\n\t/*\n\t * Exception from kernel and interrupts are enabled. Move to\n\t * kernel process stack.\n\t */\n\telse if (eregs->flags & X86_EFLAGS_IF)\n\t\tregs = (struct pt_regs *)(eregs->sp -= sizeof(struct pt_regs));\n\tif (eregs != regs)\n\t\t*regs = *eregs;\n\treturn regs;\n}\nNOKPROBE_SYMBOL(sync_regs);\n#endif\n\n/*\n * Our handling of the processor debug registers is non-trivial.\n * We do not clear them on entry and exit from the kernel. Therefore\n * it is possible to get a watchpoint trap here from inside the kernel.\n * However, the code in ./ptrace.c has ensured that the user can\n * only set watchpoints on userspace addresses. Therefore the in-kernel\n * watchpoint trap can only occur in code which is reading/writing\n * from user space. Such code must not hold kernel locks (since it\n * can equally take a page fault), therefore it is safe to call\n * force_sig_info even though that claims and releases locks.\n *\n * Code in ./signal.c ensures that the debug control register\n * is restored before we deliver any signal, and therefore that\n * user code runs with the correct debug control register even though\n * we clear it here.\n *\n * Being careful here means that we don't have to be as careful in a\n * lot of more complicated places (task switching can be a bit lazy\n * about restoring all the debug state, and ptrace doesn't have to\n * find every occurrence of the TF bit that could be saved away even\n * by user code)\n *\n * May run on IST stack.\n */\ndotraplinkage void do_debug(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk = current;\n\tenum ctx_state prev_state;\n\tint user_icebp = 0;\n\tunsigned long dr6;\n\tint si_code;\n\n\tprev_state = exception_enter();\n\n\tget_debugreg(dr6, 6);\n\n\t/* Filter out all the reserved bits which are preset to 1 */\n\tdr6 &= ~DR6_RESERVED;\n\n\t/*\n\t * If dr6 has no reason to give us about the origin of this trap,\n\t * then it's very likely the result of an icebp/int01 trap.\n\t * User wants a sigtrap for that.\n\t */\n\tif (!dr6 && user_mode(regs))\n\t\tuser_icebp = 1;\n\n\t/* Catch kmemcheck conditions first of all! */\n\tif ((dr6 & DR_STEP) && kmemcheck_trap(regs))\n\t\tgoto exit;\n\n\t/* DR6 may or may not be cleared by the CPU */\n\tset_debugreg(0, 6);\n\n\t/*\n\t * The processor cleared BTF, so don't mark that we need it set.\n\t */\n\tclear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);\n\n\t/* Store the virtualized DR6 value */\n\ttsk->thread.debugreg6 = dr6;\n\n#ifdef CONFIG_KPROBES\n\tif (kprobe_debug_handler(regs))\n\t\tgoto exit;\n#endif\n\n\tif (notify_die(DIE_DEBUG, \"debug\", regs, (long)&dr6, error_code,\n\t\t\t\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n\n\t/*\n\t * Let others (NMI) know that the debug stack is in use\n\t * as we may switch to the interrupt stack.\n\t */\n\tdebug_stack_usage_inc();\n\n\t/* It's safe to allow irq's after DR6 has been saved */\n\tpreempt_conditional_sti(regs);\n\n\tif (regs->flags & X86_VM_MASK) {\n\t\thandle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,\n\t\t\t\t\tX86_TRAP_DB);\n\t\tpreempt_conditional_cli(regs);\n\t\tdebug_stack_usage_dec();\n\t\tgoto exit;\n\t}\n\n\t/*\n\t * Single-stepping through system calls: ignore any exceptions in\n\t * kernel space, but re-enable TF when returning to user mode.\n\t *\n\t * We already checked v86 mode above, so we can check for kernel mode\n\t * by just checking the CPL of CS.\n\t */\n\tif ((dr6 & DR_STEP) && !user_mode(regs)) {\n\t\ttsk->thread.debugreg6 &= ~DR_STEP;\n\t\tset_tsk_thread_flag(tsk, TIF_SINGLESTEP);\n\t\tregs->flags &= ~X86_EFLAGS_TF;\n\t}\n\tsi_code = get_si_code(tsk->thread.debugreg6);\n\tif (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)\n\t\tsend_sigtrap(tsk, regs, error_code, si_code);\n\tpreempt_conditional_cli(regs);\n\tdebug_stack_usage_dec();\n\nexit:\n\texception_exit(prev_state);\n}\nNOKPROBE_SYMBOL(do_debug);\n\n/*\n * Note that we play around with the 'TS' bit in an attempt to get\n * the correct behaviour even in the presence of the asynchronous\n * IRQ13 behaviour\n */\nstatic void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tsiginfo_t info;\n\tunsigned short err;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode_vm(regs))\n\t{\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tsave_init_fpu(task);\n\ttask->thread.trap_nr = trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo = SIGFPE;\n\tinfo.si_errno = 0;\n\tinfo.si_addr = (void __user *)uprobe_get_trap_addr(regs);\n\tif (trapnr == X86_TRAP_MF) {\n\t\tunsigned short cwd, swd;\n\t\t/*\n\t\t * (~cwd & swd) will mask out exceptions that are not set to unmasked\n\t\t * status.  0x3f is the exception bits in these regs, 0x200 is the\n\t\t * C1 reg you need in case of a stack fault, 0x040 is the stack\n\t\t * fault bit.  We should only be taking one exception at a time,\n\t\t * so if this combination doesn't produce any single exception,\n\t\t * then we have a bad program that isn't synchronizing its FPU usage\n\t\t * and it will suffer the consequences since we won't be able to\n\t\t * fully reproduce the context of the exception\n\t\t */\n\t\tcwd = get_fpu_cwd(task);\n\t\tswd = get_fpu_swd(task);\n\n\t\terr = swd & ~cwd;\n\t} else {\n\t\t/*\n\t\t * The SIMD FPU exceptions are handled a little differently, as there\n\t\t * is only a single status/control register.  Thus, to determine which\n\t\t * unmasked exception was caught we must mask the exception mask bits\n\t\t * at 0x1f80, and then use these to mask the exception bits at 0x3f.\n\t\t */\n\t\tunsigned short mxcsr = get_fpu_mxcsr(task);\n\t\terr = ~(mxcsr >> 7) & mxcsr;\n\t}\n\n\tif (err & 0x001) {\t/* Invalid op */\n\t\t/*\n\t\t * swd & 0x240 == 0x040: Stack Underflow\n\t\t * swd & 0x240 == 0x240: Stack Overflow\n\t\t * User must clear the SF bit (0x40) if set\n\t\t */\n\t\tinfo.si_code = FPE_FLTINV;\n\t} else if (err & 0x004) { /* Divide by Zero */\n\t\tinfo.si_code = FPE_FLTDIV;\n\t} else if (err & 0x008) { /* Overflow */\n\t\tinfo.si_code = FPE_FLTOVF;\n\t} else if (err & 0x012) { /* Denormal, Underflow */\n\t\tinfo.si_code = FPE_FLTUND;\n\t} else if (err & 0x020) { /* Precision */\n\t\tinfo.si_code = FPE_FLTRES;\n\t} else {\n\t\t/*\n\t\t * If we're using IRQ 13, or supposedly even some trap\n\t\t * X86_TRAP_MF implementations, it's possible\n\t\t * we get a spurious trap, which is not an error.\n\t\t */\n\t\treturn;\n\t}\n\tforce_sig_info(SIGFPE, &info, task);\n}\n\ndotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)\n{\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tmath_error(regs, error_code, X86_TRAP_MF);\n\texception_exit(prev_state);\n}\n\ndotraplinkage void\ndo_simd_coprocessor_error(struct pt_regs *regs, long error_code)\n{\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tmath_error(regs, error_code, X86_TRAP_XF);\n\texception_exit(prev_state);\n}\n\ndotraplinkage void\ndo_spurious_interrupt_bug(struct pt_regs *regs, long error_code)\n{\n\tconditional_sti(regs);\n#if 0\n\t/* No need to warn about this any longer. */\n\tpr_info(\"Ignoring P6 Local APIC Spurious Interrupt Bug...\\n\");\n#endif\n}\n\nasmlinkage __visible void __attribute__((weak)) smp_thermal_interrupt(void)\n{\n}\n\nasmlinkage __visible void __attribute__((weak)) smp_threshold_interrupt(void)\n{\n}\n\n/*\n * 'math_state_restore()' saves the current math information in the\n * old math state array, and gets the new ones from the current task\n *\n * Careful.. There are problems with IBM-designed IRQ13 behaviour.\n * Don't touch unless you *really* know how it works.\n *\n * Must be called with kernel preemption disabled (eg with local\n * local interrupts as in the case of do_device_not_available).\n */\nvoid math_state_restore(void)\n{\n\tstruct task_struct *tsk = current;\n\n\tif (!tsk_used_math(tsk)) {\n\t\tlocal_irq_enable();\n\t\t/*\n\t\t * does a slab alloc which can sleep\n\t\t */\n\t\tif (init_fpu(tsk)) {\n\t\t\t/*\n\t\t\t * ran out of memory!\n\t\t\t */\n\t\t\tdo_group_exit(SIGKILL);\n\t\t\treturn;\n\t\t}\n\t\tlocal_irq_disable();\n\t}\n\n\t__thread_fpu_begin(tsk);\n\n\t/*\n\t * Paranoid restore. send a SIGSEGV if we fail to restore the state.\n\t */\n\tif (unlikely(restore_fpu_checking(tsk))) {\n\t\tdrop_init_fpu(tsk);\n\t\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n\t\treturn;\n\t}\n\n\ttsk->thread.fpu_counter++;\n}\nEXPORT_SYMBOL_GPL(math_state_restore);\n\ndotraplinkage void\ndo_device_not_available(struct pt_regs *regs, long error_code)\n{\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tBUG_ON(use_eager_fpu());\n\n#ifdef CONFIG_MATH_EMULATION\n\tif (read_cr0() & X86_CR0_EM) {\n\t\tstruct math_emu_info info = { };\n\n\t\tconditional_sti(regs);\n\n\t\tinfo.regs = regs;\n\t\tmath_emulate(&info);\n\t\texception_exit(prev_state);\n\t\treturn;\n\t}\n#endif\n\tmath_state_restore(); /* interrupts still off */\n#ifdef CONFIG_X86_32\n\tconditional_sti(regs);\n#endif\n\texception_exit(prev_state);\n}\nNOKPROBE_SYMBOL(do_device_not_available);\n\n#ifdef CONFIG_X86_32\ndotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)\n{\n\tsiginfo_t info;\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tlocal_irq_enable();\n\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code = ILL_BADSTK;\n\tinfo.si_addr = NULL;\n\tif (notify_die(DIE_TRAP, \"iret exception\", regs, error_code,\n\t\t\tX86_TRAP_IRET, SIGILL) != NOTIFY_STOP) {\n\t\tdo_trap(X86_TRAP_IRET, SIGILL, \"iret exception\", regs, error_code,\n\t\t\t&info);\n\t}\n\texception_exit(prev_state);\n}\n#endif\n\n/* Set of traps needed for early debugging. */\nvoid __init early_trap_init(void)\n{\n\tset_intr_gate_ist(X86_TRAP_DB, &debug, DEBUG_STACK);\n\t/* int3 can be called from all */\n\tset_system_intr_gate_ist(X86_TRAP_BP, &int3, DEBUG_STACK);\n#ifdef CONFIG_X86_32\n\tset_intr_gate(X86_TRAP_PF, page_fault);\n#endif\n\tload_idt(&idt_descr);\n}\n\nvoid __init early_trap_pf_init(void)\n{\n#ifdef CONFIG_X86_64\n\tset_intr_gate(X86_TRAP_PF, page_fault);\n#endif\n}\n\nvoid __init trap_init(void)\n{\n\tint i;\n\n#ifdef CONFIG_EISA\n\tvoid __iomem *p = early_ioremap(0x0FFFD9, 4);\n\n\tif (readl(p) == 'E' + ('I'<<8) + ('S'<<16) + ('A'<<24))\n\t\tEISA_bus = 1;\n\tearly_iounmap(p, 4);\n#endif\n\n\tset_intr_gate(X86_TRAP_DE, divide_error);\n\tset_intr_gate_ist(X86_TRAP_NMI, &nmi, NMI_STACK);\n\t/* int4 can be called from all */\n\tset_system_intr_gate(X86_TRAP_OF, &overflow);\n\tset_intr_gate(X86_TRAP_BR, bounds);\n\tset_intr_gate(X86_TRAP_UD, invalid_op);\n\tset_intr_gate(X86_TRAP_NM, device_not_available);\n#ifdef CONFIG_X86_32\n\tset_task_gate(X86_TRAP_DF, GDT_ENTRY_DOUBLEFAULT_TSS);\n#else\n\tset_intr_gate_ist(X86_TRAP_DF, &double_fault, DOUBLEFAULT_STACK);\n#endif\n\tset_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);\n\tset_intr_gate(X86_TRAP_TS, invalid_TSS);\n\tset_intr_gate(X86_TRAP_NP, segment_not_present);\n\tset_intr_gate_ist(X86_TRAP_SS, &stack_segment, STACKFAULT_STACK);\n\tset_intr_gate(X86_TRAP_GP, general_protection);\n\tset_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);\n\tset_intr_gate(X86_TRAP_MF, coprocessor_error);\n\tset_intr_gate(X86_TRAP_AC, alignment_check);\n#ifdef CONFIG_X86_MCE\n\tset_intr_gate_ist(X86_TRAP_MC, &machine_check, MCE_STACK);\n#endif\n\tset_intr_gate(X86_TRAP_XF, simd_coprocessor_error);\n\n\t/* Reserve all the builtin and the syscall vector: */\n\tfor (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)\n\t\tset_bit(i, used_vectors);\n\n#ifdef CONFIG_IA32_EMULATION\n\tset_system_intr_gate(IA32_SYSCALL_VECTOR, ia32_syscall);\n\tset_bit(IA32_SYSCALL_VECTOR, used_vectors);\n#endif\n\n#ifdef CONFIG_X86_32\n\tset_system_trap_gate(SYSCALL_VECTOR, &system_call);\n\tset_bit(SYSCALL_VECTOR, used_vectors);\n#endif\n\n\t/*\n\t * Set the IDT descriptor to a fixed read-only location, so that the\n\t * \"sidt\" instruction will not leak the location of the kernel, and\n\t * to defend the IDT against arbitrary memory write vulnerabilities.\n\t * It will be reloaded in cpu_init() */\n\t__set_fixmap(FIX_RO_IDT, __pa_symbol(idt_table), PAGE_KERNEL_RO);\n\tidt_descr.address = fix_to_virt(FIX_RO_IDT);\n\n\t/*\n\t * Should be a barrier for any external CPU state:\n\t */\n\tcpu_init();\n\n\tx86_init.irqs.trap_init();\n\n#ifdef CONFIG_X86_64\n\tmemcpy(&debug_idt_table, &idt_table, IDT_ENTRIES * 16);\n\tset_nmi_gate(X86_TRAP_DB, &debug);\n\tset_nmi_gate(X86_TRAP_BP, &int3);\n#endif\n}\n"], "fixing_code": ["#ifndef _ASM_X86_PAGE_32_DEFS_H\n#define _ASM_X86_PAGE_32_DEFS_H\n\n#include <linux/const.h>\n\n/*\n * This handles the memory map.\n *\n * A __PAGE_OFFSET of 0xC0000000 means that the kernel has\n * a virtual address space of one gigabyte, which limits the\n * amount of physical memory you can use to about 950MB.\n *\n * If you want more physical memory than this then see the CONFIG_HIGHMEM4G\n * and CONFIG_HIGHMEM64G options in the kernel configuration.\n */\n#define __PAGE_OFFSET\t\t_AC(CONFIG_PAGE_OFFSET, UL)\n\n#define __START_KERNEL_map\t__PAGE_OFFSET\n\n#define THREAD_SIZE_ORDER\t1\n#define THREAD_SIZE\t\t(PAGE_SIZE << THREAD_SIZE_ORDER)\n\n#define DOUBLEFAULT_STACK 1\n#define NMI_STACK 0\n#define DEBUG_STACK 0\n#define MCE_STACK 0\n#define N_EXCEPTION_STACKS 1\n\n#ifdef CONFIG_X86_PAE\n/* 44=32+12, the limit we can fit into an unsigned long pfn */\n#define __PHYSICAL_MASK_SHIFT\t44\n#define __VIRTUAL_MASK_SHIFT\t32\n\n#else  /* !CONFIG_X86_PAE */\n#define __PHYSICAL_MASK_SHIFT\t32\n#define __VIRTUAL_MASK_SHIFT\t32\n#endif\t/* CONFIG_X86_PAE */\n\n/*\n * Kernel image size is limited to 512 MB (see in arch/x86/kernel/head_32.S)\n */\n#define KERNEL_IMAGE_SIZE\t(512 * 1024 * 1024)\n\n#ifndef __ASSEMBLY__\n\n/*\n * This much address space is reserved for vmalloc() and iomap()\n * as well as fixmap mappings.\n */\nextern unsigned int __VMALLOC_RESERVE;\nextern int sysctl_legacy_va_layout;\n\nextern void find_low_pfn_range(void);\nextern void setup_bootmem_allocator(void);\n\n#endif\t/* !__ASSEMBLY__ */\n\n#endif /* _ASM_X86_PAGE_32_DEFS_H */\n", "#ifndef _ASM_X86_PAGE_64_DEFS_H\n#define _ASM_X86_PAGE_64_DEFS_H\n\n#define THREAD_SIZE_ORDER\t2\n#define THREAD_SIZE  (PAGE_SIZE << THREAD_SIZE_ORDER)\n#define CURRENT_MASK (~(THREAD_SIZE - 1))\n\n#define EXCEPTION_STACK_ORDER 0\n#define EXCEPTION_STKSZ (PAGE_SIZE << EXCEPTION_STACK_ORDER)\n\n#define DEBUG_STACK_ORDER (EXCEPTION_STACK_ORDER + 1)\n#define DEBUG_STKSZ (PAGE_SIZE << DEBUG_STACK_ORDER)\n\n#define IRQ_STACK_ORDER 2\n#define IRQ_STACK_SIZE (PAGE_SIZE << IRQ_STACK_ORDER)\n\n#define DOUBLEFAULT_STACK 1\n#define NMI_STACK 2\n#define DEBUG_STACK 3\n#define MCE_STACK 4\n#define N_EXCEPTION_STACKS 4  /* hw limit: 7 */\n\n#define PUD_PAGE_SIZE\t\t(_AC(1, UL) << PUD_SHIFT)\n#define PUD_PAGE_MASK\t\t(~(PUD_PAGE_SIZE-1))\n\n/*\n * Set __PAGE_OFFSET to the most negative possible address +\n * PGDIR_SIZE*16 (pgd slot 272).  The gap is to allow a space for a\n * hypervisor to fit.  Choosing 16 slots here is arbitrary, but it's\n * what Xen requires.\n */\n#define __PAGE_OFFSET           _AC(0xffff880000000000, UL)\n\n#define __START_KERNEL_map\t_AC(0xffffffff80000000, UL)\n\n/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */\n#define __PHYSICAL_MASK_SHIFT\t46\n#define __VIRTUAL_MASK_SHIFT\t47\n\n/*\n * Kernel image size is limited to 1GiB due to the fixmap living in the\n * next 1GiB (see level2_kernel_pgt in arch/x86/kernel/head_64.S). Use\n * 512MiB by default, leaving 1.5GiB for modules once the page tables\n * are fully set up. If kernel ASLR is configured, it can extend the\n * kernel page table mapping, reducing the size of the modules area.\n */\n#define KERNEL_IMAGE_SIZE_DEFAULT      (512 * 1024 * 1024)\n#if defined(CONFIG_RANDOMIZE_BASE) && \\\n\tCONFIG_RANDOMIZE_BASE_MAX_OFFSET > KERNEL_IMAGE_SIZE_DEFAULT\n#define KERNEL_IMAGE_SIZE   CONFIG_RANDOMIZE_BASE_MAX_OFFSET\n#else\n#define KERNEL_IMAGE_SIZE      KERNEL_IMAGE_SIZE_DEFAULT\n#endif\n\n#endif /* _ASM_X86_PAGE_64_DEFS_H */\n", "#ifndef _ASM_X86_TRAPS_H\n#define _ASM_X86_TRAPS_H\n\n#include <linux/kprobes.h>\n\n#include <asm/debugreg.h>\n#include <asm/siginfo.h>\t\t\t/* TRAP_TRACE, ... */\n\n#define dotraplinkage __visible\n\nasmlinkage void divide_error(void);\nasmlinkage void debug(void);\nasmlinkage void nmi(void);\nasmlinkage void int3(void);\nasmlinkage void xen_debug(void);\nasmlinkage void xen_int3(void);\nasmlinkage void xen_stack_segment(void);\nasmlinkage void overflow(void);\nasmlinkage void bounds(void);\nasmlinkage void invalid_op(void);\nasmlinkage void device_not_available(void);\n#ifdef CONFIG_X86_64\nasmlinkage void double_fault(void);\n#endif\nasmlinkage void coprocessor_segment_overrun(void);\nasmlinkage void invalid_TSS(void);\nasmlinkage void segment_not_present(void);\nasmlinkage void stack_segment(void);\nasmlinkage void general_protection(void);\nasmlinkage void page_fault(void);\nasmlinkage void async_page_fault(void);\nasmlinkage void spurious_interrupt_bug(void);\nasmlinkage void coprocessor_error(void);\nasmlinkage void alignment_check(void);\n#ifdef CONFIG_X86_MCE\nasmlinkage void machine_check(void);\n#endif /* CONFIG_X86_MCE */\nasmlinkage void simd_coprocessor_error(void);\n\n#ifdef CONFIG_TRACING\nasmlinkage void trace_page_fault(void);\n#define trace_stack_segment stack_segment\n#define trace_divide_error divide_error\n#define trace_bounds bounds\n#define trace_invalid_op invalid_op\n#define trace_device_not_available device_not_available\n#define trace_coprocessor_segment_overrun coprocessor_segment_overrun\n#define trace_invalid_TSS invalid_TSS\n#define trace_segment_not_present segment_not_present\n#define trace_general_protection general_protection\n#define trace_spurious_interrupt_bug spurious_interrupt_bug\n#define trace_coprocessor_error coprocessor_error\n#define trace_alignment_check alignment_check\n#define trace_simd_coprocessor_error simd_coprocessor_error\n#define trace_async_page_fault async_page_fault\n#endif\n\ndotraplinkage void do_divide_error(struct pt_regs *, long);\ndotraplinkage void do_debug(struct pt_regs *, long);\ndotraplinkage void do_nmi(struct pt_regs *, long);\ndotraplinkage void do_int3(struct pt_regs *, long);\ndotraplinkage void do_overflow(struct pt_regs *, long);\ndotraplinkage void do_bounds(struct pt_regs *, long);\ndotraplinkage void do_invalid_op(struct pt_regs *, long);\ndotraplinkage void do_device_not_available(struct pt_regs *, long);\ndotraplinkage void do_coprocessor_segment_overrun(struct pt_regs *, long);\ndotraplinkage void do_invalid_TSS(struct pt_regs *, long);\ndotraplinkage void do_segment_not_present(struct pt_regs *, long);\ndotraplinkage void do_stack_segment(struct pt_regs *, long);\n#ifdef CONFIG_X86_64\ndotraplinkage void do_double_fault(struct pt_regs *, long);\nasmlinkage struct pt_regs *sync_regs(struct pt_regs *);\n#endif\ndotraplinkage void do_general_protection(struct pt_regs *, long);\ndotraplinkage void do_page_fault(struct pt_regs *, unsigned long);\n#ifdef CONFIG_TRACING\ndotraplinkage void trace_do_page_fault(struct pt_regs *, unsigned long);\n#else\nstatic inline void trace_do_page_fault(struct pt_regs *regs, unsigned long error)\n{\n\tdo_page_fault(regs, error);\n}\n#endif\ndotraplinkage void do_spurious_interrupt_bug(struct pt_regs *, long);\ndotraplinkage void do_coprocessor_error(struct pt_regs *, long);\ndotraplinkage void do_alignment_check(struct pt_regs *, long);\n#ifdef CONFIG_X86_MCE\ndotraplinkage void do_machine_check(struct pt_regs *, long);\n#endif\ndotraplinkage void do_simd_coprocessor_error(struct pt_regs *, long);\n#ifdef CONFIG_X86_32\ndotraplinkage void do_iret_error(struct pt_regs *, long);\n#endif\n\nstatic inline int get_si_code(unsigned long condition)\n{\n\tif (condition & DR_STEP)\n\t\treturn TRAP_TRACE;\n\telse if (condition & (DR_TRAP0|DR_TRAP1|DR_TRAP2|DR_TRAP3))\n\t\treturn TRAP_HWBKPT;\n\telse\n\t\treturn TRAP_BRKPT;\n}\n\nextern int panic_on_unrecovered_nmi;\n\nvoid math_emulate(struct math_emu_info *);\n#ifndef CONFIG_X86_32\nasmlinkage void smp_thermal_interrupt(void);\nasmlinkage void mce_threshold_interrupt(void);\n#endif\n\n/* Interrupts/Exceptions */\nenum {\n\tX86_TRAP_DE = 0,\t/*  0, Divide-by-zero */\n\tX86_TRAP_DB,\t\t/*  1, Debug */\n\tX86_TRAP_NMI,\t\t/*  2, Non-maskable Interrupt */\n\tX86_TRAP_BP,\t\t/*  3, Breakpoint */\n\tX86_TRAP_OF,\t\t/*  4, Overflow */\n\tX86_TRAP_BR,\t\t/*  5, Bound Range Exceeded */\n\tX86_TRAP_UD,\t\t/*  6, Invalid Opcode */\n\tX86_TRAP_NM,\t\t/*  7, Device Not Available */\n\tX86_TRAP_DF,\t\t/*  8, Double Fault */\n\tX86_TRAP_OLD_MF,\t/*  9, Coprocessor Segment Overrun */\n\tX86_TRAP_TS,\t\t/* 10, Invalid TSS */\n\tX86_TRAP_NP,\t\t/* 11, Segment Not Present */\n\tX86_TRAP_SS,\t\t/* 12, Stack Segment Fault */\n\tX86_TRAP_GP,\t\t/* 13, General Protection Fault */\n\tX86_TRAP_PF,\t\t/* 14, Page Fault */\n\tX86_TRAP_SPURIOUS,\t/* 15, Spurious Interrupt */\n\tX86_TRAP_MF,\t\t/* 16, x87 Floating-Point Exception */\n\tX86_TRAP_AC,\t\t/* 17, Alignment Check */\n\tX86_TRAP_MC,\t\t/* 18, Machine Check */\n\tX86_TRAP_XF,\t\t/* 19, SIMD Floating-Point Exception */\n\tX86_TRAP_IRET = 32,\t/* 32, IRET Exception */\n};\n\n#endif /* _ASM_X86_TRAPS_H */\n", "/*\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs\n */\n#include <linux/kallsyms.h>\n#include <linux/kprobes.h>\n#include <linux/uaccess.h>\n#include <linux/hardirq.h>\n#include <linux/kdebug.h>\n#include <linux/module.h>\n#include <linux/ptrace.h>\n#include <linux/kexec.h>\n#include <linux/sysfs.h>\n#include <linux/bug.h>\n#include <linux/nmi.h>\n\n#include <asm/stacktrace.h>\n\n\n#define N_EXCEPTION_STACKS_END \\\n\t\t(N_EXCEPTION_STACKS + DEBUG_STKSZ/EXCEPTION_STKSZ - 2)\n\nstatic char x86_stack_ids[][8] = {\n\t\t[ DEBUG_STACK-1\t\t\t]\t= \"#DB\",\n\t\t[ NMI_STACK-1\t\t\t]\t= \"NMI\",\n\t\t[ DOUBLEFAULT_STACK-1\t\t]\t= \"#DF\",\n\t\t[ MCE_STACK-1\t\t\t]\t= \"#MC\",\n#if DEBUG_STKSZ > EXCEPTION_STKSZ\n\t\t[ N_EXCEPTION_STACKS ...\n\t\t  N_EXCEPTION_STACKS_END\t]\t= \"#DB[?]\"\n#endif\n};\n\nstatic unsigned long *in_exception_stack(unsigned cpu, unsigned long stack,\n\t\t\t\t\t unsigned *usedp, char **idp)\n{\n\tunsigned k;\n\n\t/*\n\t * Iterate over all exception stacks, and figure out whether\n\t * 'stack' is in one of them:\n\t */\n\tfor (k = 0; k < N_EXCEPTION_STACKS; k++) {\n\t\tunsigned long end = per_cpu(orig_ist, cpu).ist[k];\n\t\t/*\n\t\t * Is 'stack' above this exception frame's end?\n\t\t * If yes then skip to the next frame.\n\t\t */\n\t\tif (stack >= end)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Is 'stack' above this exception frame's start address?\n\t\t * If yes then we found the right frame.\n\t\t */\n\t\tif (stack >= end - EXCEPTION_STKSZ) {\n\t\t\t/*\n\t\t\t * Make sure we only iterate through an exception\n\t\t\t * stack once. If it comes up for the second time\n\t\t\t * then there's something wrong going on - just\n\t\t\t * break out and return NULL:\n\t\t\t */\n\t\t\tif (*usedp & (1U << k))\n\t\t\t\tbreak;\n\t\t\t*usedp |= 1U << k;\n\t\t\t*idp = x86_stack_ids[k];\n\t\t\treturn (unsigned long *)end;\n\t\t}\n\t\t/*\n\t\t * If this is a debug stack, and if it has a larger size than\n\t\t * the usual exception stacks, then 'stack' might still\n\t\t * be within the lower portion of the debug stack:\n\t\t */\n#if DEBUG_STKSZ > EXCEPTION_STKSZ\n\t\tif (k == DEBUG_STACK - 1 && stack >= end - DEBUG_STKSZ) {\n\t\t\tunsigned j = N_EXCEPTION_STACKS - 1;\n\n\t\t\t/*\n\t\t\t * Black magic. A large debug stack is composed of\n\t\t\t * multiple exception stack entries, which we\n\t\t\t * iterate through now. Dont look:\n\t\t\t */\n\t\t\tdo {\n\t\t\t\t++j;\n\t\t\t\tend -= EXCEPTION_STKSZ;\n\t\t\t\tx86_stack_ids[j][4] = '1' +\n\t\t\t\t\t\t(j - N_EXCEPTION_STACKS);\n\t\t\t} while (stack < end - EXCEPTION_STKSZ);\n\t\t\tif (*usedp & (1U << j))\n\t\t\t\tbreak;\n\t\t\t*usedp |= 1U << j;\n\t\t\t*idp = x86_stack_ids[j];\n\t\t\treturn (unsigned long *)end;\n\t\t}\n#endif\n\t}\n\treturn NULL;\n}\n\nstatic inline int\nin_irq_stack(unsigned long *stack, unsigned long *irq_stack,\n\t     unsigned long *irq_stack_end)\n{\n\treturn (stack >= irq_stack && stack < irq_stack_end);\n}\n\nstatic const unsigned long irq_stack_size =\n\t(IRQ_STACK_SIZE - 64) / sizeof(unsigned long);\n\nenum stack_type {\n\tSTACK_IS_UNKNOWN,\n\tSTACK_IS_NORMAL,\n\tSTACK_IS_EXCEPTION,\n\tSTACK_IS_IRQ,\n};\n\nstatic enum stack_type\nanalyze_stack(int cpu, struct task_struct *task, unsigned long *stack,\n\t      unsigned long **stack_end, unsigned long *irq_stack,\n\t      unsigned *used, char **id)\n{\n\tunsigned long addr;\n\n\taddr = ((unsigned long)stack & (~(THREAD_SIZE - 1)));\n\tif ((unsigned long)task_stack_page(task) == addr)\n\t\treturn STACK_IS_NORMAL;\n\n\t*stack_end = in_exception_stack(cpu, (unsigned long)stack,\n\t\t\t\t\tused, id);\n\tif (*stack_end)\n\t\treturn STACK_IS_EXCEPTION;\n\n\tif (!irq_stack)\n\t\treturn STACK_IS_NORMAL;\n\n\t*stack_end = irq_stack;\n\tirq_stack = irq_stack - irq_stack_size;\n\n\tif (in_irq_stack(stack, irq_stack, *stack_end))\n\t\treturn STACK_IS_IRQ;\n\n\treturn STACK_IS_UNKNOWN;\n}\n\n/*\n * x86-64 can have up to three kernel stacks:\n * process stack\n * interrupt stack\n * severe exception (double fault, nmi, stack fault, debug, mce) hardware stack\n */\n\nvoid dump_trace(struct task_struct *task, struct pt_regs *regs,\n\t\tunsigned long *stack, unsigned long bp,\n\t\tconst struct stacktrace_ops *ops, void *data)\n{\n\tconst unsigned cpu = get_cpu();\n\tstruct thread_info *tinfo;\n\tunsigned long *irq_stack = (unsigned long *)per_cpu(irq_stack_ptr, cpu);\n\tunsigned long dummy;\n\tunsigned used = 0;\n\tint graph = 0;\n\tint done = 0;\n\n\tif (!task)\n\t\ttask = current;\n\n\tif (!stack) {\n\t\tif (regs)\n\t\t\tstack = (unsigned long *)regs->sp;\n\t\telse if (task != current)\n\t\t\tstack = (unsigned long *)task->thread.sp;\n\t\telse\n\t\t\tstack = &dummy;\n\t}\n\n\tif (!bp)\n\t\tbp = stack_frame(task, regs);\n\t/*\n\t * Print function call entries in all stacks, starting at the\n\t * current stack address. If the stacks consist of nested\n\t * exceptions\n\t */\n\ttinfo = task_thread_info(task);\n\twhile (!done) {\n\t\tunsigned long *stack_end;\n\t\tenum stack_type stype;\n\t\tchar *id;\n\n\t\tstype = analyze_stack(cpu, task, stack, &stack_end,\n\t\t\t\t      irq_stack, &used, &id);\n\n\t\t/* Default finish unless specified to continue */\n\t\tdone = 1;\n\n\t\tswitch (stype) {\n\n\t\t/* Break out early if we are on the thread stack */\n\t\tcase STACK_IS_NORMAL:\n\t\t\tbreak;\n\n\t\tcase STACK_IS_EXCEPTION:\n\n\t\t\tif (ops->stack(data, id) < 0)\n\t\t\t\tbreak;\n\n\t\t\tbp = ops->walk_stack(tinfo, stack, bp, ops,\n\t\t\t\t\t     data, stack_end, &graph);\n\t\t\tops->stack(data, \"<EOE>\");\n\t\t\t/*\n\t\t\t * We link to the next stack via the\n\t\t\t * second-to-last pointer (index -2 to end) in the\n\t\t\t * exception stack:\n\t\t\t */\n\t\t\tstack = (unsigned long *) stack_end[-2];\n\t\t\tdone = 0;\n\t\t\tbreak;\n\n\t\tcase STACK_IS_IRQ:\n\n\t\t\tif (ops->stack(data, \"IRQ\") < 0)\n\t\t\t\tbreak;\n\t\t\tbp = ops->walk_stack(tinfo, stack, bp,\n\t\t\t\t     ops, data, stack_end, &graph);\n\t\t\t/*\n\t\t\t * We link to the next stack (which would be\n\t\t\t * the process stack normally) the last\n\t\t\t * pointer (index -1 to end) in the IRQ stack:\n\t\t\t */\n\t\t\tstack = (unsigned long *) (stack_end[-1]);\n\t\t\tirq_stack = NULL;\n\t\t\tops->stack(data, \"EOI\");\n\t\t\tdone = 0;\n\t\t\tbreak;\n\n\t\tcase STACK_IS_UNKNOWN:\n\t\t\tops->stack(data, \"UNK\");\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * This handles the process stack:\n\t */\n\tbp = ops->walk_stack(tinfo, stack, bp, ops, data, NULL, &graph);\n\tput_cpu();\n}\nEXPORT_SYMBOL(dump_trace);\n\nvoid\nshow_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,\n\t\t   unsigned long *sp, unsigned long bp, char *log_lvl)\n{\n\tunsigned long *irq_stack_end;\n\tunsigned long *irq_stack;\n\tunsigned long *stack;\n\tint cpu;\n\tint i;\n\n\tpreempt_disable();\n\tcpu = smp_processor_id();\n\n\tirq_stack_end\t= (unsigned long *)(per_cpu(irq_stack_ptr, cpu));\n\tirq_stack\t= (unsigned long *)(per_cpu(irq_stack_ptr, cpu) - IRQ_STACK_SIZE);\n\n\t/*\n\t * Debugging aid: \"show_stack(NULL, NULL);\" prints the\n\t * back trace for this cpu:\n\t */\n\tif (sp == NULL) {\n\t\tif (task)\n\t\t\tsp = (unsigned long *)task->thread.sp;\n\t\telse\n\t\t\tsp = (unsigned long *)&sp;\n\t}\n\n\tstack = sp;\n\tfor (i = 0; i < kstack_depth_to_print; i++) {\n\t\tif (stack >= irq_stack && stack <= irq_stack_end) {\n\t\t\tif (stack == irq_stack_end) {\n\t\t\t\tstack = (unsigned long *) (irq_stack_end[-1]);\n\t\t\t\tpr_cont(\" <EOI> \");\n\t\t\t}\n\t\t} else {\n\t\tif (((long) stack & (THREAD_SIZE-1)) == 0)\n\t\t\tbreak;\n\t\t}\n\t\tif (i && ((i % STACKSLOTS_PER_LINE) == 0))\n\t\t\tpr_cont(\"\\n\");\n\t\tpr_cont(\" %016lx\", *stack++);\n\t\ttouch_nmi_watchdog();\n\t}\n\tpreempt_enable();\n\n\tpr_cont(\"\\n\");\n\tshow_trace_log_lvl(task, regs, sp, bp, log_lvl);\n}\n\nvoid show_regs(struct pt_regs *regs)\n{\n\tint i;\n\tunsigned long sp;\n\n\tsp = regs->sp;\n\tshow_regs_print_info(KERN_DEFAULT);\n\t__show_regs(regs, 1);\n\n\t/*\n\t * When in-kernel, we also print out the stack and code at the\n\t * time of the fault..\n\t */\n\tif (!user_mode(regs)) {\n\t\tunsigned int code_prologue = code_bytes * 43 / 64;\n\t\tunsigned int code_len = code_bytes;\n\t\tunsigned char c;\n\t\tu8 *ip;\n\n\t\tprintk(KERN_DEFAULT \"Stack:\\n\");\n\t\tshow_stack_log_lvl(NULL, regs, (unsigned long *)sp,\n\t\t\t\t   0, KERN_DEFAULT);\n\n\t\tprintk(KERN_DEFAULT \"Code: \");\n\n\t\tip = (u8 *)regs->ip - code_prologue;\n\t\tif (ip < (u8 *)PAGE_OFFSET || probe_kernel_address(ip, c)) {\n\t\t\t/* try starting at IP */\n\t\t\tip = (u8 *)regs->ip;\n\t\t\tcode_len = code_len - code_prologue + 1;\n\t\t}\n\t\tfor (i = 0; i < code_len; i++, ip++) {\n\t\t\tif (ip < (u8 *)PAGE_OFFSET ||\n\t\t\t\t\tprobe_kernel_address(ip, c)) {\n\t\t\t\tpr_cont(\" Bad RIP value.\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ip == (u8 *)regs->ip)\n\t\t\t\tpr_cont(\"<%02x> \", c);\n\t\t\telse\n\t\t\t\tpr_cont(\"%02x \", c);\n\t\t}\n\t}\n\tpr_cont(\"\\n\");\n}\n\nint is_valid_bugaddr(unsigned long ip)\n{\n\tunsigned short ud2;\n\n\tif (__copy_from_user(&ud2, (const void __user *) ip, sizeof(ud2)))\n\t\treturn 0;\n\n\treturn ud2 == 0x0b0f;\n}\n", "/*\n *  linux/arch/x86_64/entry.S\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *  Copyright (C) 2000, 2001, 2002  Andi Kleen SuSE Labs\n *  Copyright (C) 2000  Pavel Machek <pavel@suse.cz>\n */\n\n/*\n * entry.S contains the system-call and fault low-level handling routines.\n *\n * Some of this is documented in Documentation/x86/entry_64.txt\n *\n * NOTE: This code handles signal-recognition, which happens every time\n * after an interrupt and after each system call.\n *\n * Normal syscalls and interrupts don't save a full stack frame, this is\n * only done for syscall tracing, signals or fork/exec et.al.\n *\n * A note on terminology:\n * - top of stack: Architecture defined interrupt frame from SS to RIP\n * at the top of the kernel process stack.\n * - partial stack frame: partially saved registers up to R11.\n * - full stack frame: Like partial stack frame, but all register saved.\n *\n * Some macro usage:\n * - CFI macros are used to generate dwarf2 unwind information for better\n * backtraces. They don't change any code.\n * - SAVE_ALL/RESTORE_ALL - Save/restore all registers\n * - SAVE_ARGS/RESTORE_ARGS - Save/restore registers that C functions modify.\n * There are unfortunately lots of special cases where some registers\n * not touched. The macro is a big mess that should be cleaned up.\n * - SAVE_REST/RESTORE_REST - Handle the registers not saved by SAVE_ARGS.\n * Gives a full stack frame.\n * - ENTRY/END Define functions in the symbol table.\n * - FIXUP_TOP_OF_STACK/RESTORE_TOP_OF_STACK - Fix up the hardware stack\n * frame that is otherwise undefined after a SYSCALL\n * - TRACE_IRQ_* - Trace hard interrupt state for lock debugging.\n * - idtentry - Define exception entry points.\n */\n\n#include <linux/linkage.h>\n#include <asm/segment.h>\n#include <asm/cache.h>\n#include <asm/errno.h>\n#include <asm/dwarf2.h>\n#include <asm/calling.h>\n#include <asm/asm-offsets.h>\n#include <asm/msr.h>\n#include <asm/unistd.h>\n#include <asm/thread_info.h>\n#include <asm/hw_irq.h>\n#include <asm/page_types.h>\n#include <asm/irqflags.h>\n#include <asm/paravirt.h>\n#include <asm/percpu.h>\n#include <asm/asm.h>\n#include <asm/context_tracking.h>\n#include <asm/smap.h>\n#include <asm/pgtable_types.h>\n#include <linux/err.h>\n\n/* Avoid __ASSEMBLER__'ifying <linux/audit.h> just for this.  */\n#include <linux/elf-em.h>\n#define AUDIT_ARCH_X86_64\t(EM_X86_64|__AUDIT_ARCH_64BIT|__AUDIT_ARCH_LE)\n#define __AUDIT_ARCH_64BIT 0x80000000\n#define __AUDIT_ARCH_LE\t   0x40000000\n\n\t.code64\n\t.section .entry.text, \"ax\"\n\n\n#ifndef CONFIG_PREEMPT\n#define retint_kernel retint_restore_args\n#endif\n\n#ifdef CONFIG_PARAVIRT\nENTRY(native_usergs_sysret64)\n\tswapgs\n\tsysretq\nENDPROC(native_usergs_sysret64)\n#endif /* CONFIG_PARAVIRT */\n\n\n.macro TRACE_IRQS_IRETQ offset=ARGOFFSET\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tbt   $9,EFLAGS-\\offset(%rsp)\t/* interrupts off? */\n\tjnc  1f\n\tTRACE_IRQS_ON\n1:\n#endif\n.endm\n\n/*\n * When dynamic function tracer is enabled it will add a breakpoint\n * to all locations that it is about to modify, sync CPUs, update\n * all the code, sync CPUs, then remove the breakpoints. In this time\n * if lockdep is enabled, it might jump back into the debug handler\n * outside the updating of the IST protection. (TRACE_IRQS_ON/OFF).\n *\n * We need to change the IDT table before calling TRACE_IRQS_ON/OFF to\n * make sure the stack pointer does not get reset back to the top\n * of the debug stack, and instead just reuses the current stack.\n */\n#if defined(CONFIG_DYNAMIC_FTRACE) && defined(CONFIG_TRACE_IRQFLAGS)\n\n.macro TRACE_IRQS_OFF_DEBUG\n\tcall debug_stack_set_zero\n\tTRACE_IRQS_OFF\n\tcall debug_stack_reset\n.endm\n\n.macro TRACE_IRQS_ON_DEBUG\n\tcall debug_stack_set_zero\n\tTRACE_IRQS_ON\n\tcall debug_stack_reset\n.endm\n\n.macro TRACE_IRQS_IRETQ_DEBUG offset=ARGOFFSET\n\tbt   $9,EFLAGS-\\offset(%rsp)\t/* interrupts off? */\n\tjnc  1f\n\tTRACE_IRQS_ON_DEBUG\n1:\n.endm\n\n#else\n# define TRACE_IRQS_OFF_DEBUG\t\tTRACE_IRQS_OFF\n# define TRACE_IRQS_ON_DEBUG\t\tTRACE_IRQS_ON\n# define TRACE_IRQS_IRETQ_DEBUG\t\tTRACE_IRQS_IRETQ\n#endif\n\n/*\n * C code is not supposed to know about undefined top of stack. Every time\n * a C function with an pt_regs argument is called from the SYSCALL based\n * fast path FIXUP_TOP_OF_STACK is needed.\n * RESTORE_TOP_OF_STACK syncs the syscall state after any possible ptregs\n * manipulation.\n */\n\n\t/* %rsp:at FRAMEEND */\n\t.macro FIXUP_TOP_OF_STACK tmp offset=0\n\tmovq PER_CPU_VAR(old_rsp),\\tmp\n\tmovq \\tmp,RSP+\\offset(%rsp)\n\tmovq $__USER_DS,SS+\\offset(%rsp)\n\tmovq $__USER_CS,CS+\\offset(%rsp)\n\tmovq $-1,RCX+\\offset(%rsp)\n\tmovq R11+\\offset(%rsp),\\tmp  /* get eflags */\n\tmovq \\tmp,EFLAGS+\\offset(%rsp)\n\t.endm\n\n\t.macro RESTORE_TOP_OF_STACK tmp offset=0\n\tmovq RSP+\\offset(%rsp),\\tmp\n\tmovq \\tmp,PER_CPU_VAR(old_rsp)\n\tmovq EFLAGS+\\offset(%rsp),\\tmp\n\tmovq \\tmp,R11+\\offset(%rsp)\n\t.endm\n\n\t.macro FAKE_STACK_FRAME child_rip\n\t/* push in order ss, rsp, eflags, cs, rip */\n\txorl %eax, %eax\n\tpushq_cfi $__KERNEL_DS /* ss */\n\t/*CFI_REL_OFFSET\tss,0*/\n\tpushq_cfi %rax /* rsp */\n\tCFI_REL_OFFSET\trsp,0\n\tpushq_cfi $(X86_EFLAGS_IF|X86_EFLAGS_FIXED) /* eflags - interrupts on */\n\t/*CFI_REL_OFFSET\trflags,0*/\n\tpushq_cfi $__KERNEL_CS /* cs */\n\t/*CFI_REL_OFFSET\tcs,0*/\n\tpushq_cfi \\child_rip /* rip */\n\tCFI_REL_OFFSET\trip,0\n\tpushq_cfi %rax /* orig rax */\n\t.endm\n\n\t.macro UNFAKE_STACK_FRAME\n\taddq $8*6, %rsp\n\tCFI_ADJUST_CFA_OFFSET\t-(6*8)\n\t.endm\n\n/*\n * initial frame state for interrupts (and exceptions without error code)\n */\n\t.macro EMPTY_FRAME start=1 offset=0\n\t.if \\start\n\tCFI_STARTPROC simple\n\tCFI_SIGNAL_FRAME\n\tCFI_DEF_CFA rsp,8+\\offset\n\t.else\n\tCFI_DEF_CFA_OFFSET 8+\\offset\n\t.endif\n\t.endm\n\n/*\n * initial frame state for interrupts (and exceptions without error code)\n */\n\t.macro INTR_FRAME start=1 offset=0\n\tEMPTY_FRAME \\start, SS+8+\\offset-RIP\n\t/*CFI_REL_OFFSET ss, SS+\\offset-RIP*/\n\tCFI_REL_OFFSET rsp, RSP+\\offset-RIP\n\t/*CFI_REL_OFFSET rflags, EFLAGS+\\offset-RIP*/\n\t/*CFI_REL_OFFSET cs, CS+\\offset-RIP*/\n\tCFI_REL_OFFSET rip, RIP+\\offset-RIP\n\t.endm\n\n/*\n * initial frame state for exceptions with error code (and interrupts\n * with vector already pushed)\n */\n\t.macro XCPT_FRAME start=1 offset=0\n\tINTR_FRAME \\start, RIP+\\offset-ORIG_RAX\n\t.endm\n\n/*\n * frame that enables calling into C.\n */\n\t.macro PARTIAL_FRAME start=1 offset=0\n\tXCPT_FRAME \\start, ORIG_RAX+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rdi, RDI+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rsi, RSI+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rdx, RDX+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rcx, RCX+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET rax, RAX+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET r8, R8+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET r9, R9+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET r10, R10+\\offset-ARGOFFSET\n\tCFI_REL_OFFSET r11, R11+\\offset-ARGOFFSET\n\t.endm\n\n/*\n * frame that enables passing a complete pt_regs to a C function.\n */\n\t.macro DEFAULT_FRAME start=1 offset=0\n\tPARTIAL_FRAME \\start, R11+\\offset-R15\n\tCFI_REL_OFFSET rbx, RBX+\\offset\n\tCFI_REL_OFFSET rbp, RBP+\\offset\n\tCFI_REL_OFFSET r12, R12+\\offset\n\tCFI_REL_OFFSET r13, R13+\\offset\n\tCFI_REL_OFFSET r14, R14+\\offset\n\tCFI_REL_OFFSET r15, R15+\\offset\n\t.endm\n\n/* save partial stack frame */\n\t.macro SAVE_ARGS_IRQ\n\tcld\n\t/* start from rbp in pt_regs and jump over */\n\tmovq_cfi rdi, (RDI-RBP)\n\tmovq_cfi rsi, (RSI-RBP)\n\tmovq_cfi rdx, (RDX-RBP)\n\tmovq_cfi rcx, (RCX-RBP)\n\tmovq_cfi rax, (RAX-RBP)\n\tmovq_cfi  r8,  (R8-RBP)\n\tmovq_cfi  r9,  (R9-RBP)\n\tmovq_cfi r10, (R10-RBP)\n\tmovq_cfi r11, (R11-RBP)\n\n\t/* Save rbp so that we can unwind from get_irq_regs() */\n\tmovq_cfi rbp, 0\n\n\t/* Save previous stack value */\n\tmovq %rsp, %rsi\n\n\tleaq -RBP(%rsp),%rdi\t/* arg1 for handler */\n\ttestl $3, CS-RBP(%rsi)\n\tje 1f\n\tSWAPGS\n\t/*\n\t * irq_count is used to check if a CPU is already on an interrupt stack\n\t * or not. While this is essentially redundant with preempt_count it is\n\t * a little cheaper to use a separate counter in the PDA (short of\n\t * moving irq_enter into assembly, which would be too much work)\n\t */\n1:\tincl PER_CPU_VAR(irq_count)\n\tcmovzq PER_CPU_VAR(irq_stack_ptr),%rsp\n\tCFI_DEF_CFA_REGISTER\trsi\n\n\t/* Store previous stack value */\n\tpushq %rsi\n\tCFI_ESCAPE\t0x0f /* DW_CFA_def_cfa_expression */, 6, \\\n\t\t\t0x77 /* DW_OP_breg7 */, 0, \\\n\t\t\t0x06 /* DW_OP_deref */, \\\n\t\t\t0x08 /* DW_OP_const1u */, SS+8-RBP, \\\n\t\t\t0x22 /* DW_OP_plus */\n\t/* We entered an interrupt context - irqs are off: */\n\tTRACE_IRQS_OFF\n\t.endm\n\nENTRY(save_paranoid)\n\tXCPT_FRAME 1 RDI+8\n\tcld\n\tmovq %rdi, RDI+8(%rsp)\n\tmovq %rsi, RSI+8(%rsp)\n\tmovq_cfi rdx, RDX+8\n\tmovq_cfi rcx, RCX+8\n\tmovq_cfi rax, RAX+8\n\tmovq %r8, R8+8(%rsp)\n\tmovq %r9, R9+8(%rsp)\n\tmovq %r10, R10+8(%rsp)\n\tmovq %r11, R11+8(%rsp)\n\tmovq_cfi rbx, RBX+8\n\tmovq %rbp, RBP+8(%rsp)\n\tmovq %r12, R12+8(%rsp)\n\tmovq %r13, R13+8(%rsp)\n\tmovq %r14, R14+8(%rsp)\n\tmovq %r15, R15+8(%rsp)\n\tmovl $1,%ebx\n\tmovl $MSR_GS_BASE,%ecx\n\trdmsr\n\ttestl %edx,%edx\n\tjs 1f\t/* negative -> in kernel */\n\tSWAPGS\n\txorl %ebx,%ebx\n1:\tret\n\tCFI_ENDPROC\nEND(save_paranoid)\n\n/*\n * A newly forked process directly context switches into this address.\n *\n * rdi: prev task we switched from\n */\nENTRY(ret_from_fork)\n\tDEFAULT_FRAME\n\n\tLOCK ; btr $TIF_FORK,TI_flags(%r8)\n\n\tpushq_cfi $0x0002\n\tpopfq_cfi\t\t\t\t# reset kernel eflags\n\n\tcall schedule_tail\t\t\t# rdi: 'prev' task parameter\n\n\tGET_THREAD_INFO(%rcx)\n\n\tRESTORE_REST\n\n\ttestl $3, CS-ARGOFFSET(%rsp)\t\t# from kernel_thread?\n\tjz   1f\n\n\ttestl $_TIF_IA32, TI_flags(%rcx)\t# 32-bit compat task needs IRET\n\tjnz  int_ret_from_sys_call\n\n\tRESTORE_TOP_OF_STACK %rdi, -ARGOFFSET\n\tjmp ret_from_sys_call\t\t\t# go to the SYSRET fastpath\n\n1:\n\tsubq $REST_SKIP, %rsp\t# leave space for volatiles\n\tCFI_ADJUST_CFA_OFFSET\tREST_SKIP\n\tmovq %rbp, %rdi\n\tcall *%rbx\n\tmovl $0, RAX(%rsp)\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(ret_from_fork)\n\n/*\n * System call entry. Up to 6 arguments in registers are supported.\n *\n * SYSCALL does not save anything on the stack and does not change the\n * stack pointer.  However, it does mask the flags register for us, so\n * CLD and CLAC are not needed.\n */\n\n/*\n * Register setup:\n * rax  system call number\n * rdi  arg0\n * rcx  return address for syscall/sysret, C arg3\n * rsi  arg1\n * rdx  arg2\n * r10  arg3 \t(--> moved to rcx for C)\n * r8   arg4\n * r9   arg5\n * r11  eflags for syscall/sysret, temporary for C\n * r12-r15,rbp,rbx saved by C code, not touched.\n *\n * Interrupts are off on entry.\n * Only called from user space.\n *\n * XXX\tif we had a free scratch register we could save the RSP into the stack frame\n *      and report it properly in ps. Unfortunately we haven't.\n *\n * When user can change the frames always force IRET. That is because\n * it deals with uncanonical addresses better. SYSRET has trouble\n * with them due to bugs in both AMD and Intel CPUs.\n */\n\nENTRY(system_call)\n\tCFI_STARTPROC\tsimple\n\tCFI_SIGNAL_FRAME\n\tCFI_DEF_CFA\trsp,KERNEL_STACK_OFFSET\n\tCFI_REGISTER\trip,rcx\n\t/*CFI_REGISTER\trflags,r11*/\n\tSWAPGS_UNSAFE_STACK\n\t/*\n\t * A hypervisor implementation might want to use a label\n\t * after the swapgs, so that it can do the swapgs\n\t * for the guest and jump here on syscall.\n\t */\nGLOBAL(system_call_after_swapgs)\n\n\tmovq\t%rsp,PER_CPU_VAR(old_rsp)\n\tmovq\tPER_CPU_VAR(kernel_stack),%rsp\n\t/*\n\t * No need to follow this irqs off/on section - it's straight\n\t * and short:\n\t */\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tSAVE_ARGS 8, 0, rax_enosys=1\n\tmovq_cfi rax,(ORIG_RAX-ARGOFFSET)\n\tmovq  %rcx,RIP-ARGOFFSET(%rsp)\n\tCFI_REL_OFFSET rip,RIP-ARGOFFSET\n\ttestl $_TIF_WORK_SYSCALL_ENTRY,TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET)\n\tjnz tracesys\nsystem_call_fastpath:\n#if __SYSCALL_MASK == ~0\n\tcmpq $__NR_syscall_max,%rax\n#else\n\tandl $__SYSCALL_MASK,%eax\n\tcmpl $__NR_syscall_max,%eax\n#endif\n\tja ret_from_sys_call  /* and return regs->ax */\n\tmovq %r10,%rcx\n\tcall *sys_call_table(,%rax,8)  # XXX:\t rip relative\n\tmovq %rax,RAX-ARGOFFSET(%rsp)\n/*\n * Syscall return path ending with SYSRET (fast path)\n * Has incomplete stack frame and undefined top of stack.\n */\nret_from_sys_call:\n\tmovl $_TIF_ALLWORK_MASK,%edi\n\t/* edi:\tflagmask */\nsysret_check:\n\tLOCKDEP_SYS_EXIT\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tmovl TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET),%edx\n\tandl %edi,%edx\n\tjnz  sysret_careful\n\tCFI_REMEMBER_STATE\n\t/*\n\t * sysretq will re-enable interrupts:\n\t */\n\tTRACE_IRQS_ON\n\tmovq RIP-ARGOFFSET(%rsp),%rcx\n\tCFI_REGISTER\trip,rcx\n\tRESTORE_ARGS 1,-ARG_SKIP,0\n\t/*CFI_REGISTER\trflags,r11*/\n\tmovq\tPER_CPU_VAR(old_rsp), %rsp\n\tUSERGS_SYSRET64\n\n\tCFI_RESTORE_STATE\n\t/* Handle reschedules */\n\t/* edx:\twork, edi: workmask */\nsysret_careful:\n\tbt $TIF_NEED_RESCHED,%edx\n\tjnc sysret_signal\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tpushq_cfi %rdi\n\tSCHEDULE_USER\n\tpopq_cfi %rdi\n\tjmp sysret_check\n\n\t/* Handle a signal */\nsysret_signal:\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n#ifdef CONFIG_AUDITSYSCALL\n\tbt $TIF_SYSCALL_AUDIT,%edx\n\tjc sysret_audit\n#endif\n\t/*\n\t * We have a signal, or exit tracing or single-step.\n\t * These all wind up with the iret return path anyway,\n\t * so just join that path right now.\n\t */\n\tFIXUP_TOP_OF_STACK %r11, -ARGOFFSET\n\tjmp int_check_syscall_exit_work\n\n#ifdef CONFIG_AUDITSYSCALL\n\t/*\n\t * Return fast path for syscall audit.  Call __audit_syscall_exit()\n\t * directly and then jump back to the fast path with TIF_SYSCALL_AUDIT\n\t * masked off.\n\t */\nsysret_audit:\n\tmovq RAX-ARGOFFSET(%rsp),%rsi\t/* second arg, syscall return value */\n\tcmpq $-MAX_ERRNO,%rsi\t/* is it < -MAX_ERRNO? */\n\tsetbe %al\t\t/* 1 if so, 0 if not */\n\tmovzbl %al,%edi\t\t/* zero-extend that into %edi */\n\tcall __audit_syscall_exit\n\tmovl $(_TIF_ALLWORK_MASK & ~_TIF_SYSCALL_AUDIT),%edi\n\tjmp sysret_check\n#endif\t/* CONFIG_AUDITSYSCALL */\n\n\t/* Do syscall tracing */\ntracesys:\n\tleaq -REST_SKIP(%rsp), %rdi\n\tmovq $AUDIT_ARCH_X86_64, %rsi\n\tcall syscall_trace_enter_phase1\n\ttest %rax, %rax\n\tjnz tracesys_phase2\t\t/* if needed, run the slow path */\n\tLOAD_ARGS 0\t\t\t/* else restore clobbered regs */\n\tjmp system_call_fastpath\t/*      and return to the fast path */\n\ntracesys_phase2:\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %rdi\n\tmovq %rsp, %rdi\n\tmovq $AUDIT_ARCH_X86_64, %rsi\n\tmovq %rax,%rdx\n\tcall syscall_trace_enter_phase2\n\n\t/*\n\t * Reload arg registers from stack in case ptrace changed them.\n\t * We don't reload %rax because syscall_trace_entry_phase2() returned\n\t * the value it wants us to use in the table lookup.\n\t */\n\tLOAD_ARGS ARGOFFSET, 1\n\tRESTORE_REST\n#if __SYSCALL_MASK == ~0\n\tcmpq $__NR_syscall_max,%rax\n#else\n\tandl $__SYSCALL_MASK,%eax\n\tcmpl $__NR_syscall_max,%eax\n#endif\n\tja   int_ret_from_sys_call\t/* RAX(%rsp) is already set */\n\tmovq %r10,%rcx\t/* fixup for C */\n\tcall *sys_call_table(,%rax,8)\n\tmovq %rax,RAX-ARGOFFSET(%rsp)\n\t/* Use IRET because user could have changed frame */\n\n/*\n * Syscall return path ending with IRET.\n * Has correct top of stack, but partial stack frame.\n */\nGLOBAL(int_ret_from_sys_call)\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tmovl $_TIF_ALLWORK_MASK,%edi\n\t/* edi:\tmask to check */\nGLOBAL(int_with_check)\n\tLOCKDEP_SYS_EXIT_IRQ\n\tGET_THREAD_INFO(%rcx)\n\tmovl TI_flags(%rcx),%edx\n\tandl %edi,%edx\n\tjnz   int_careful\n\tandl    $~TS_COMPAT,TI_status(%rcx)\n\tjmp   retint_swapgs\n\n\t/* Either reschedule or signal or syscall exit tracking needed. */\n\t/* First do a reschedule test. */\n\t/* edx:\twork, edi: workmask */\nint_careful:\n\tbt $TIF_NEED_RESCHED,%edx\n\tjnc  int_very_careful\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tpushq_cfi %rdi\n\tSCHEDULE_USER\n\tpopq_cfi %rdi\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tjmp int_with_check\n\n\t/* handle signals and tracing -- both require a full stack frame */\nint_very_careful:\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\nint_check_syscall_exit_work:\n\tSAVE_REST\n\t/* Check for syscall exit trace */\n\ttestl $_TIF_WORK_SYSCALL_EXIT,%edx\n\tjz int_signal\n\tpushq_cfi %rdi\n\tleaq 8(%rsp),%rdi\t# &ptregs -> arg1\n\tcall syscall_trace_leave\n\tpopq_cfi %rdi\n\tandl $~(_TIF_WORK_SYSCALL_EXIT|_TIF_SYSCALL_EMU),%edi\n\tjmp int_restore_rest\n\nint_signal:\n\ttestl $_TIF_DO_NOTIFY_MASK,%edx\n\tjz 1f\n\tmovq %rsp,%rdi\t\t# &ptregs -> arg1\n\txorl %esi,%esi\t\t# oldset -> arg2\n\tcall do_notify_resume\n1:\tmovl $_TIF_WORK_MASK,%edi\nint_restore_rest:\n\tRESTORE_REST\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tjmp int_with_check\n\tCFI_ENDPROC\nEND(system_call)\n\n\t.macro FORK_LIKE func\nENTRY(stub_\\func)\n\tCFI_STARTPROC\n\tpopq\t%r11\t\t\t/* save return address */\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tpushq\t%r11\t\t\t/* put it back on stack */\n\tFIXUP_TOP_OF_STACK %r11, 8\n\tDEFAULT_FRAME 0 8\t\t/* offset 8: return address */\n\tcall sys_\\func\n\tRESTORE_TOP_OF_STACK %r11, 8\n\tret $REST_SKIP\t\t/* pop extended registers */\n\tCFI_ENDPROC\nEND(stub_\\func)\n\t.endm\n\n\t.macro FIXED_FRAME label,func\nENTRY(\\label)\n\tCFI_STARTPROC\n\tPARTIAL_FRAME 0 8\t\t/* offset 8: return address */\n\tFIXUP_TOP_OF_STACK %r11, 8-ARGOFFSET\n\tcall \\func\n\tRESTORE_TOP_OF_STACK %r11, 8-ARGOFFSET\n\tret\n\tCFI_ENDPROC\nEND(\\label)\n\t.endm\n\n\tFORK_LIKE  clone\n\tFORK_LIKE  fork\n\tFORK_LIKE  vfork\n\tFIXED_FRAME stub_iopl, sys_iopl\n\nENTRY(ptregscall_common)\n\tDEFAULT_FRAME 1 8\t/* offset 8: return address */\n\tRESTORE_TOP_OF_STACK %r11, 8\n\tmovq_cfi_restore R15+8, r15\n\tmovq_cfi_restore R14+8, r14\n\tmovq_cfi_restore R13+8, r13\n\tmovq_cfi_restore R12+8, r12\n\tmovq_cfi_restore RBP+8, rbp\n\tmovq_cfi_restore RBX+8, rbx\n\tret $REST_SKIP\t\t/* pop extended registers */\n\tCFI_ENDPROC\nEND(ptregscall_common)\n\nENTRY(stub_execve)\n\tCFI_STARTPROC\n\taddq $8, %rsp\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %r11\n\tcall sys_execve\n\tmovq %rax,RAX(%rsp)\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(stub_execve)\n\n/*\n * sigreturn is special because it needs to restore all registers on return.\n * This cannot be done with SYSRET, so use the IRET return path instead.\n */\nENTRY(stub_rt_sigreturn)\n\tCFI_STARTPROC\n\taddq $8, %rsp\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %r11\n\tcall sys_rt_sigreturn\n\tmovq %rax,RAX(%rsp) # fixme, this could be done at the higher layer\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(stub_rt_sigreturn)\n\n#ifdef CONFIG_X86_X32_ABI\nENTRY(stub_x32_rt_sigreturn)\n\tCFI_STARTPROC\n\taddq $8, %rsp\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %r11\n\tcall sys32_x32_rt_sigreturn\n\tmovq %rax,RAX(%rsp) # fixme, this could be done at the higher layer\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(stub_x32_rt_sigreturn)\n\nENTRY(stub_x32_execve)\n\tCFI_STARTPROC\n\taddq $8, %rsp\n\tPARTIAL_FRAME 0\n\tSAVE_REST\n\tFIXUP_TOP_OF_STACK %r11\n\tcall compat_sys_execve\n\tRESTORE_TOP_OF_STACK %r11\n\tmovq %rax,RAX(%rsp)\n\tRESTORE_REST\n\tjmp int_ret_from_sys_call\n\tCFI_ENDPROC\nEND(stub_x32_execve)\n\n#endif\n\n/*\n * Build the entry stubs and pointer table with some assembler magic.\n * We pack 7 stubs into a single 32-byte chunk, which will fit in a\n * single cache line on all modern x86 implementations.\n */\n\t.section .init.rodata,\"a\"\nENTRY(interrupt)\n\t.section .entry.text\n\t.p2align 5\n\t.p2align CONFIG_X86_L1_CACHE_SHIFT\nENTRY(irq_entries_start)\n\tINTR_FRAME\nvector=FIRST_EXTERNAL_VECTOR\n.rept (NR_VECTORS-FIRST_EXTERNAL_VECTOR+6)/7\n\t.balign 32\n  .rept\t7\n    .if vector < NR_VECTORS\n      .if vector <> FIRST_EXTERNAL_VECTOR\n\tCFI_ADJUST_CFA_OFFSET -8\n      .endif\n1:\tpushq_cfi $(~vector+0x80)\t/* Note: always in signed byte range */\n      .if ((vector-FIRST_EXTERNAL_VECTOR)%7) <> 6\n\tjmp 2f\n      .endif\n      .previous\n\t.quad 1b\n      .section .entry.text\nvector=vector+1\n    .endif\n  .endr\n2:\tjmp common_interrupt\n.endr\n\tCFI_ENDPROC\nEND(irq_entries_start)\n\n.previous\nEND(interrupt)\n.previous\n\n/*\n * Interrupt entry/exit.\n *\n * Interrupt entry points save only callee clobbered registers in fast path.\n *\n * Entry runs with interrupts off.\n */\n\n/* 0(%rsp): ~(interrupt number) */\n\t.macro interrupt func\n\t/* reserve pt_regs for scratch regs and rbp */\n\tsubq $ORIG_RAX-RBP, %rsp\n\tCFI_ADJUST_CFA_OFFSET ORIG_RAX-RBP\n\tSAVE_ARGS_IRQ\n\tcall \\func\n\t.endm\n\n\t/*\n\t * The interrupt stubs push (~vector+0x80) onto the stack and\n\t * then jump to common_interrupt.\n\t */\n\t.p2align CONFIG_X86_L1_CACHE_SHIFT\ncommon_interrupt:\n\tXCPT_FRAME\n\tASM_CLAC\n\taddq $-0x80,(%rsp)\t\t/* Adjust vector to [-256,-1] range */\n\tinterrupt do_IRQ\n\t/* 0(%rsp): old_rsp-ARGOFFSET */\nret_from_intr:\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tdecl PER_CPU_VAR(irq_count)\n\n\t/* Restore saved previous stack */\n\tpopq %rsi\n\tCFI_DEF_CFA rsi,SS+8-RBP\t/* reg/off reset after def_cfa_expr */\n\tleaq ARGOFFSET-RBP(%rsi), %rsp\n\tCFI_DEF_CFA_REGISTER\trsp\n\tCFI_ADJUST_CFA_OFFSET\tRBP-ARGOFFSET\n\nexit_intr:\n\tGET_THREAD_INFO(%rcx)\n\ttestl $3,CS-ARGOFFSET(%rsp)\n\tje retint_kernel\n\n\t/* Interrupt came from user space */\n\t/*\n\t * Has a correct top of stack, but a partial stack frame\n\t * %rcx: thread info. Interrupts off.\n\t */\nretint_with_reschedule:\n\tmovl $_TIF_WORK_MASK,%edi\nretint_check:\n\tLOCKDEP_SYS_EXIT_IRQ\n\tmovl TI_flags(%rcx),%edx\n\tandl %edi,%edx\n\tCFI_REMEMBER_STATE\n\tjnz  retint_careful\n\nretint_swapgs:\t\t/* return to user-space */\n\t/*\n\t * The iretq could re-enable interrupts:\n\t */\n\tDISABLE_INTERRUPTS(CLBR_ANY)\n\tTRACE_IRQS_IRETQ\n\tSWAPGS\n\tjmp restore_args\n\nretint_restore_args:\t/* return to kernel space */\n\tDISABLE_INTERRUPTS(CLBR_ANY)\n\t/*\n\t * The iretq could re-enable interrupts:\n\t */\n\tTRACE_IRQS_IRETQ\nrestore_args:\n\tRESTORE_ARGS 1,8,1\n\nirq_return:\n\tINTERRUPT_RETURN\n\nENTRY(native_iret)\n\t/*\n\t * Are we returning to a stack segment from the LDT?  Note: in\n\t * 64-bit mode SS:RSP on the exception stack is always valid.\n\t */\n#ifdef CONFIG_X86_ESPFIX64\n\ttestb $4,(SS-RIP)(%rsp)\n\tjnz native_irq_return_ldt\n#endif\n\n.global native_irq_return_iret\nnative_irq_return_iret:\n\tiretq\n\t_ASM_EXTABLE(native_irq_return_iret, bad_iret)\n\n#ifdef CONFIG_X86_ESPFIX64\nnative_irq_return_ldt:\n\tpushq_cfi %rax\n\tpushq_cfi %rdi\n\tSWAPGS\n\tmovq PER_CPU_VAR(espfix_waddr),%rdi\n\tmovq %rax,(0*8)(%rdi)\t/* RAX */\n\tmovq (2*8)(%rsp),%rax\t/* RIP */\n\tmovq %rax,(1*8)(%rdi)\n\tmovq (3*8)(%rsp),%rax\t/* CS */\n\tmovq %rax,(2*8)(%rdi)\n\tmovq (4*8)(%rsp),%rax\t/* RFLAGS */\n\tmovq %rax,(3*8)(%rdi)\n\tmovq (6*8)(%rsp),%rax\t/* SS */\n\tmovq %rax,(5*8)(%rdi)\n\tmovq (5*8)(%rsp),%rax\t/* RSP */\n\tmovq %rax,(4*8)(%rdi)\n\tandl $0xffff0000,%eax\n\tpopq_cfi %rdi\n\torq PER_CPU_VAR(espfix_stack),%rax\n\tSWAPGS\n\tmovq %rax,%rsp\n\tpopq_cfi %rax\n\tjmp native_irq_return_iret\n#endif\n\n\t.section .fixup,\"ax\"\nbad_iret:\n\t/*\n\t * The iret traps when the %cs or %ss being restored is bogus.\n\t * We've lost the original trap vector and error code.\n\t * #GPF is the most likely one to get for an invalid selector.\n\t * So pretend we completed the iret and took the #GPF in user mode.\n\t *\n\t * We are now running with the kernel GS after exception recovery.\n\t * But error_entry expects us to have user GS to match the user %cs,\n\t * so swap back.\n\t */\n\tpushq $0\n\n\tSWAPGS\n\tjmp general_protection\n\n\t.previous\n\n\t/* edi: workmask, edx: work */\nretint_careful:\n\tCFI_RESTORE_STATE\n\tbt    $TIF_NEED_RESCHED,%edx\n\tjnc   retint_signal\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tpushq_cfi %rdi\n\tSCHEDULE_USER\n\tpopq_cfi %rdi\n\tGET_THREAD_INFO(%rcx)\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tjmp retint_check\n\nretint_signal:\n\ttestl $_TIF_DO_NOTIFY_MASK,%edx\n\tjz    retint_swapgs\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\tSAVE_REST\n\tmovq $-1,ORIG_RAX(%rsp)\n\txorl %esi,%esi\t\t# oldset\n\tmovq %rsp,%rdi\t\t# &pt_regs\n\tcall do_notify_resume\n\tRESTORE_REST\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tGET_THREAD_INFO(%rcx)\n\tjmp retint_with_reschedule\n\n#ifdef CONFIG_PREEMPT\n\t/* Returning to kernel space. Check if we need preemption */\n\t/* rcx:\t threadinfo. interrupts off. */\nENTRY(retint_kernel)\n\tcmpl $0,PER_CPU_VAR(__preempt_count)\n\tjnz  retint_restore_args\n\tbt   $9,EFLAGS-ARGOFFSET(%rsp)\t/* interrupts off? */\n\tjnc  retint_restore_args\n\tcall preempt_schedule_irq\n\tjmp exit_intr\n#endif\n\tCFI_ENDPROC\nEND(common_interrupt)\n\n/*\n * APIC interrupts.\n */\n.macro apicinterrupt3 num sym do_sym\nENTRY(\\sym)\n\tINTR_FRAME\n\tASM_CLAC\n\tpushq_cfi $~(\\num)\n.Lcommon_\\sym:\n\tinterrupt \\do_sym\n\tjmp ret_from_intr\n\tCFI_ENDPROC\nEND(\\sym)\n.endm\n\n#ifdef CONFIG_TRACING\n#define trace(sym) trace_##sym\n#define smp_trace(sym) smp_trace_##sym\n\n.macro trace_apicinterrupt num sym\napicinterrupt3 \\num trace(\\sym) smp_trace(\\sym)\n.endm\n#else\n.macro trace_apicinterrupt num sym do_sym\n.endm\n#endif\n\n.macro apicinterrupt num sym do_sym\napicinterrupt3 \\num \\sym \\do_sym\ntrace_apicinterrupt \\num \\sym\n.endm\n\n#ifdef CONFIG_SMP\napicinterrupt3 IRQ_MOVE_CLEANUP_VECTOR \\\n\tirq_move_cleanup_interrupt smp_irq_move_cleanup_interrupt\napicinterrupt3 REBOOT_VECTOR \\\n\treboot_interrupt smp_reboot_interrupt\n#endif\n\n#ifdef CONFIG_X86_UV\napicinterrupt3 UV_BAU_MESSAGE \\\n\tuv_bau_message_intr1 uv_bau_message_interrupt\n#endif\napicinterrupt LOCAL_TIMER_VECTOR \\\n\tapic_timer_interrupt smp_apic_timer_interrupt\napicinterrupt X86_PLATFORM_IPI_VECTOR \\\n\tx86_platform_ipi smp_x86_platform_ipi\n\n#ifdef CONFIG_HAVE_KVM\napicinterrupt3 POSTED_INTR_VECTOR \\\n\tkvm_posted_intr_ipi smp_kvm_posted_intr_ipi\n#endif\n\n#ifdef CONFIG_X86_MCE_THRESHOLD\napicinterrupt THRESHOLD_APIC_VECTOR \\\n\tthreshold_interrupt smp_threshold_interrupt\n#endif\n\n#ifdef CONFIG_X86_THERMAL_VECTOR\napicinterrupt THERMAL_APIC_VECTOR \\\n\tthermal_interrupt smp_thermal_interrupt\n#endif\n\n#ifdef CONFIG_SMP\napicinterrupt CALL_FUNCTION_SINGLE_VECTOR \\\n\tcall_function_single_interrupt smp_call_function_single_interrupt\napicinterrupt CALL_FUNCTION_VECTOR \\\n\tcall_function_interrupt smp_call_function_interrupt\napicinterrupt RESCHEDULE_VECTOR \\\n\treschedule_interrupt smp_reschedule_interrupt\n#endif\n\napicinterrupt ERROR_APIC_VECTOR \\\n\terror_interrupt smp_error_interrupt\napicinterrupt SPURIOUS_APIC_VECTOR \\\n\tspurious_interrupt smp_spurious_interrupt\n\n#ifdef CONFIG_IRQ_WORK\napicinterrupt IRQ_WORK_VECTOR \\\n\tirq_work_interrupt smp_irq_work_interrupt\n#endif\n\n/*\n * Exception entry points.\n */\n#define INIT_TSS_IST(x) PER_CPU_VAR(init_tss) + (TSS_ist + ((x) - 1) * 8)\n\n.macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1\nENTRY(\\sym)\n\t/* Sanity check */\n\t.if \\shift_ist != -1 && \\paranoid == 0\n\t.error \"using shift_ist requires paranoid=1\"\n\t.endif\n\n\t.if \\has_error_code\n\tXCPT_FRAME\n\t.else\n\tINTR_FRAME\n\t.endif\n\n\tASM_CLAC\n\tPARAVIRT_ADJUST_EXCEPTION_FRAME\n\n\t.ifeq \\has_error_code\n\tpushq_cfi $-1\t\t\t/* ORIG_RAX: no syscall to restart */\n\t.endif\n\n\tsubq $ORIG_RAX-R15, %rsp\n\tCFI_ADJUST_CFA_OFFSET ORIG_RAX-R15\n\n\t.if \\paranoid\n\tcall save_paranoid\n\t.else\n\tcall error_entry\n\t.endif\n\n\tDEFAULT_FRAME 0\n\n\t.if \\paranoid\n\t.if \\shift_ist != -1\n\tTRACE_IRQS_OFF_DEBUG\t\t/* reload IDT in case of recursion */\n\t.else\n\tTRACE_IRQS_OFF\n\t.endif\n\t.endif\n\n\tmovq %rsp,%rdi\t\t\t/* pt_regs pointer */\n\n\t.if \\has_error_code\n\tmovq ORIG_RAX(%rsp),%rsi\t/* get error code */\n\tmovq $-1,ORIG_RAX(%rsp)\t\t/* no syscall to restart */\n\t.else\n\txorl %esi,%esi\t\t\t/* no error code */\n\t.endif\n\n\t.if \\shift_ist != -1\n\tsubq $EXCEPTION_STKSZ, INIT_TSS_IST(\\shift_ist)\n\t.endif\n\n\tcall \\do_sym\n\n\t.if \\shift_ist != -1\n\taddq $EXCEPTION_STKSZ, INIT_TSS_IST(\\shift_ist)\n\t.endif\n\n\t.if \\paranoid\n\tjmp paranoid_exit\t\t/* %ebx: no swapgs flag */\n\t.else\n\tjmp error_exit\t\t\t/* %ebx: no swapgs flag */\n\t.endif\n\n\tCFI_ENDPROC\nEND(\\sym)\n.endm\n\n#ifdef CONFIG_TRACING\n.macro trace_idtentry sym do_sym has_error_code:req\nidtentry trace(\\sym) trace(\\do_sym) has_error_code=\\has_error_code\nidtentry \\sym \\do_sym has_error_code=\\has_error_code\n.endm\n#else\n.macro trace_idtentry sym do_sym has_error_code:req\nidtentry \\sym \\do_sym has_error_code=\\has_error_code\n.endm\n#endif\n\nidtentry divide_error do_divide_error has_error_code=0\nidtentry overflow do_overflow has_error_code=0\nidtentry bounds do_bounds has_error_code=0\nidtentry invalid_op do_invalid_op has_error_code=0\nidtentry device_not_available do_device_not_available has_error_code=0\nidtentry double_fault do_double_fault has_error_code=1 paranoid=1\nidtentry coprocessor_segment_overrun do_coprocessor_segment_overrun has_error_code=0\nidtentry invalid_TSS do_invalid_TSS has_error_code=1\nidtentry segment_not_present do_segment_not_present has_error_code=1\nidtentry spurious_interrupt_bug do_spurious_interrupt_bug has_error_code=0\nidtentry coprocessor_error do_coprocessor_error has_error_code=0\nidtentry alignment_check do_alignment_check has_error_code=1\nidtentry simd_coprocessor_error do_simd_coprocessor_error has_error_code=0\n\n\n\t/* Reload gs selector with exception handling */\n\t/* edi:  new selector */\nENTRY(native_load_gs_index)\n\tCFI_STARTPROC\n\tpushfq_cfi\n\tDISABLE_INTERRUPTS(CLBR_ANY & ~CLBR_RDI)\n\tSWAPGS\ngs_change:\n\tmovl %edi,%gs\n2:\tmfence\t\t/* workaround */\n\tSWAPGS\n\tpopfq_cfi\n\tret\n\tCFI_ENDPROC\nEND(native_load_gs_index)\n\n\t_ASM_EXTABLE(gs_change,bad_gs)\n\t.section .fixup,\"ax\"\n\t/* running with kernelgs */\nbad_gs:\n\tSWAPGS\t\t\t/* switch back to user gs */\n\txorl %eax,%eax\n\tmovl %eax,%gs\n\tjmp  2b\n\t.previous\n\n/* Call softirq on interrupt stack. Interrupts are off. */\nENTRY(do_softirq_own_stack)\n\tCFI_STARTPROC\n\tpushq_cfi %rbp\n\tCFI_REL_OFFSET rbp,0\n\tmov  %rsp,%rbp\n\tCFI_DEF_CFA_REGISTER rbp\n\tincl PER_CPU_VAR(irq_count)\n\tcmove PER_CPU_VAR(irq_stack_ptr),%rsp\n\tpush  %rbp\t\t\t# backlink for old unwinder\n\tcall __do_softirq\n\tleaveq\n\tCFI_RESTORE\t\trbp\n\tCFI_DEF_CFA_REGISTER\trsp\n\tCFI_ADJUST_CFA_OFFSET   -8\n\tdecl PER_CPU_VAR(irq_count)\n\tret\n\tCFI_ENDPROC\nEND(do_softirq_own_stack)\n\n#ifdef CONFIG_XEN\nidtentry xen_hypervisor_callback xen_do_hypervisor_callback has_error_code=0\n\n/*\n * A note on the \"critical region\" in our callback handler.\n * We want to avoid stacking callback handlers due to events occurring\n * during handling of the last event. To do this, we keep events disabled\n * until we've done all processing. HOWEVER, we must enable events before\n * popping the stack frame (can't be done atomically) and so it would still\n * be possible to get enough handler activations to overflow the stack.\n * Although unlikely, bugs of that kind are hard to track down, so we'd\n * like to avoid the possibility.\n * So, on entry to the handler we detect whether we interrupted an\n * existing activation in its critical region -- if so, we pop the current\n * activation and restart the handler using the previous one.\n */\nENTRY(xen_do_hypervisor_callback)   # do_hypervisor_callback(struct *pt_regs)\n\tCFI_STARTPROC\n/*\n * Since we don't modify %rdi, evtchn_do_upall(struct *pt_regs) will\n * see the correct pointer to the pt_regs\n */\n\tmovq %rdi, %rsp            # we don't return, adjust the stack frame\n\tCFI_ENDPROC\n\tDEFAULT_FRAME\n11:\tincl PER_CPU_VAR(irq_count)\n\tmovq %rsp,%rbp\n\tCFI_DEF_CFA_REGISTER rbp\n\tcmovzq PER_CPU_VAR(irq_stack_ptr),%rsp\n\tpushq %rbp\t\t\t# backlink for old unwinder\n\tcall xen_evtchn_do_upcall\n\tpopq %rsp\n\tCFI_DEF_CFA_REGISTER rsp\n\tdecl PER_CPU_VAR(irq_count)\n\tjmp  error_exit\n\tCFI_ENDPROC\nEND(xen_do_hypervisor_callback)\n\n/*\n * Hypervisor uses this for application faults while it executes.\n * We get here for two reasons:\n *  1. Fault while reloading DS, ES, FS or GS\n *  2. Fault while executing IRET\n * Category 1 we do not need to fix up as Xen has already reloaded all segment\n * registers that could be reloaded and zeroed the others.\n * Category 2 we fix up by killing the current process. We cannot use the\n * normal Linux return path in this case because if we use the IRET hypercall\n * to pop the stack frame we end up in an infinite loop of failsafe callbacks.\n * We distinguish between categories by comparing each saved segment register\n * with its current contents: any discrepancy means we in category 1.\n */\nENTRY(xen_failsafe_callback)\n\tINTR_FRAME 1 (6*8)\n\t/*CFI_REL_OFFSET gs,GS*/\n\t/*CFI_REL_OFFSET fs,FS*/\n\t/*CFI_REL_OFFSET es,ES*/\n\t/*CFI_REL_OFFSET ds,DS*/\n\tCFI_REL_OFFSET r11,8\n\tCFI_REL_OFFSET rcx,0\n\tmovw %ds,%cx\n\tcmpw %cx,0x10(%rsp)\n\tCFI_REMEMBER_STATE\n\tjne 1f\n\tmovw %es,%cx\n\tcmpw %cx,0x18(%rsp)\n\tjne 1f\n\tmovw %fs,%cx\n\tcmpw %cx,0x20(%rsp)\n\tjne 1f\n\tmovw %gs,%cx\n\tcmpw %cx,0x28(%rsp)\n\tjne 1f\n\t/* All segments match their saved values => Category 2 (Bad IRET). */\n\tmovq (%rsp),%rcx\n\tCFI_RESTORE rcx\n\tmovq 8(%rsp),%r11\n\tCFI_RESTORE r11\n\taddq $0x30,%rsp\n\tCFI_ADJUST_CFA_OFFSET -0x30\n\tpushq_cfi $0\t/* RIP */\n\tpushq_cfi %r11\n\tpushq_cfi %rcx\n\tjmp general_protection\n\tCFI_RESTORE_STATE\n1:\t/* Segment mismatch => Category 1 (Bad segment). Retry the IRET. */\n\tmovq (%rsp),%rcx\n\tCFI_RESTORE rcx\n\tmovq 8(%rsp),%r11\n\tCFI_RESTORE r11\n\taddq $0x30,%rsp\n\tCFI_ADJUST_CFA_OFFSET -0x30\n\tpushq_cfi $-1 /* orig_ax = -1 => not a system call */\n\tSAVE_ALL\n\tjmp error_exit\n\tCFI_ENDPROC\nEND(xen_failsafe_callback)\n\napicinterrupt3 HYPERVISOR_CALLBACK_VECTOR \\\n\txen_hvm_callback_vector xen_evtchn_do_upcall\n\n#endif /* CONFIG_XEN */\n\n#if IS_ENABLED(CONFIG_HYPERV)\napicinterrupt3 HYPERVISOR_CALLBACK_VECTOR \\\n\thyperv_callback_vector hyperv_vector_handler\n#endif /* CONFIG_HYPERV */\n\nidtentry debug do_debug has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK\nidtentry int3 do_int3 has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK\nidtentry stack_segment do_stack_segment has_error_code=1\n#ifdef CONFIG_XEN\nidtentry xen_debug do_debug has_error_code=0\nidtentry xen_int3 do_int3 has_error_code=0\nidtentry xen_stack_segment do_stack_segment has_error_code=1\n#endif\nidtentry general_protection do_general_protection has_error_code=1\ntrace_idtentry page_fault do_page_fault has_error_code=1\n#ifdef CONFIG_KVM_GUEST\nidtentry async_page_fault do_async_page_fault has_error_code=1\n#endif\n#ifdef CONFIG_X86_MCE\nidtentry machine_check has_error_code=0 paranoid=1 do_sym=*machine_check_vector(%rip)\n#endif\n\n\t/*\n\t * \"Paranoid\" exit path from exception stack.\n\t * Paranoid because this is used by NMIs and cannot take\n\t * any kernel state for granted.\n\t * We don't do kernel preemption checks here, because only\n\t * NMI should be common and it does not enable IRQs and\n\t * cannot get reschedule ticks.\n\t *\n\t * \"trace\" is 0 for the NMI handler only, because irq-tracing\n\t * is fundamentally NMI-unsafe. (we cannot change the soft and\n\t * hard flags at once, atomically)\n\t */\n\n\t/* ebx:\tno swapgs flag */\nENTRY(paranoid_exit)\n\tDEFAULT_FRAME\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF_DEBUG\n\ttestl %ebx,%ebx\t\t\t\t/* swapgs needed? */\n\tjnz paranoid_restore\n\ttestl $3,CS(%rsp)\n\tjnz   paranoid_userspace\nparanoid_swapgs:\n\tTRACE_IRQS_IRETQ 0\n\tSWAPGS_UNSAFE_STACK\n\tRESTORE_ALL 8\n\tjmp irq_return\nparanoid_restore:\n\tTRACE_IRQS_IRETQ_DEBUG 0\n\tRESTORE_ALL 8\n\tjmp irq_return\nparanoid_userspace:\n\tGET_THREAD_INFO(%rcx)\n\tmovl TI_flags(%rcx),%ebx\n\tandl $_TIF_WORK_MASK,%ebx\n\tjz paranoid_swapgs\n\tmovq %rsp,%rdi\t\t\t/* &pt_regs */\n\tcall sync_regs\n\tmovq %rax,%rsp\t\t\t/* switch stack for scheduling */\n\ttestl $_TIF_NEED_RESCHED,%ebx\n\tjnz paranoid_schedule\n\tmovl %ebx,%edx\t\t\t/* arg3: thread flags */\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_NONE)\n\txorl %esi,%esi \t\t\t/* arg2: oldset */\n\tmovq %rsp,%rdi \t\t\t/* arg1: &pt_regs */\n\tcall do_notify_resume\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tjmp paranoid_userspace\nparanoid_schedule:\n\tTRACE_IRQS_ON\n\tENABLE_INTERRUPTS(CLBR_ANY)\n\tSCHEDULE_USER\n\tDISABLE_INTERRUPTS(CLBR_ANY)\n\tTRACE_IRQS_OFF\n\tjmp paranoid_userspace\n\tCFI_ENDPROC\nEND(paranoid_exit)\n\n/*\n * Exception entry point. This expects an error code/orig_rax on the stack.\n * returns in \"no swapgs flag\" in %ebx.\n */\nENTRY(error_entry)\n\tXCPT_FRAME\n\tCFI_ADJUST_CFA_OFFSET 15*8\n\t/* oldrax contains error code */\n\tcld\n\tmovq %rdi, RDI+8(%rsp)\n\tmovq %rsi, RSI+8(%rsp)\n\tmovq %rdx, RDX+8(%rsp)\n\tmovq %rcx, RCX+8(%rsp)\n\tmovq %rax, RAX+8(%rsp)\n\tmovq  %r8,  R8+8(%rsp)\n\tmovq  %r9,  R9+8(%rsp)\n\tmovq %r10, R10+8(%rsp)\n\tmovq %r11, R11+8(%rsp)\n\tmovq_cfi rbx, RBX+8\n\tmovq %rbp, RBP+8(%rsp)\n\tmovq %r12, R12+8(%rsp)\n\tmovq %r13, R13+8(%rsp)\n\tmovq %r14, R14+8(%rsp)\n\tmovq %r15, R15+8(%rsp)\n\txorl %ebx,%ebx\n\ttestl $3,CS+8(%rsp)\n\tje error_kernelspace\nerror_swapgs:\n\tSWAPGS\nerror_sti:\n\tTRACE_IRQS_OFF\n\tret\n\n/*\n * There are two places in the kernel that can potentially fault with\n * usergs. Handle them here. The exception handlers after iret run with\n * kernel gs again, so don't set the user space flag. B stepping K8s\n * sometimes report an truncated RIP for IRET exceptions returning to\n * compat mode. Check for these here too.\n */\nerror_kernelspace:\n\tCFI_REL_OFFSET rcx, RCX+8\n\tincl %ebx\n\tleaq native_irq_return_iret(%rip),%rcx\n\tcmpq %rcx,RIP+8(%rsp)\n\tje error_swapgs\n\tmovl %ecx,%eax\t/* zero extend */\n\tcmpq %rax,RIP+8(%rsp)\n\tje bstep_iret\n\tcmpq $gs_change,RIP+8(%rsp)\n\tje error_swapgs\n\tjmp error_sti\n\nbstep_iret:\n\t/* Fix truncated RIP */\n\tmovq %rcx,RIP+8(%rsp)\n\tjmp error_swapgs\n\tCFI_ENDPROC\nEND(error_entry)\n\n\n/* ebx:\tno swapgs flag (1: don't need swapgs, 0: need it) */\nENTRY(error_exit)\n\tDEFAULT_FRAME\n\tmovl %ebx,%eax\n\tRESTORE_REST\n\tDISABLE_INTERRUPTS(CLBR_NONE)\n\tTRACE_IRQS_OFF\n\tGET_THREAD_INFO(%rcx)\n\ttestl %eax,%eax\n\tjne retint_kernel\n\tLOCKDEP_SYS_EXIT_IRQ\n\tmovl TI_flags(%rcx),%edx\n\tmovl $_TIF_WORK_MASK,%edi\n\tandl %edi,%edx\n\tjnz retint_careful\n\tjmp retint_swapgs\n\tCFI_ENDPROC\nEND(error_exit)\n\n/*\n * Test if a given stack is an NMI stack or not.\n */\n\t.macro test_in_nmi reg stack nmi_ret normal_ret\n\tcmpq %\\reg, \\stack\n\tja \\normal_ret\n\tsubq $EXCEPTION_STKSZ, %\\reg\n\tcmpq %\\reg, \\stack\n\tjb \\normal_ret\n\tjmp \\nmi_ret\n\t.endm\n\n\t/* runs on exception stack */\nENTRY(nmi)\n\tINTR_FRAME\n\tPARAVIRT_ADJUST_EXCEPTION_FRAME\n\t/*\n\t * We allow breakpoints in NMIs. If a breakpoint occurs, then\n\t * the iretq it performs will take us out of NMI context.\n\t * This means that we can have nested NMIs where the next\n\t * NMI is using the top of the stack of the previous NMI. We\n\t * can't let it execute because the nested NMI will corrupt the\n\t * stack of the previous NMI. NMI handlers are not re-entrant\n\t * anyway.\n\t *\n\t * To handle this case we do the following:\n\t *  Check the a special location on the stack that contains\n\t *  a variable that is set when NMIs are executing.\n\t *  The interrupted task's stack is also checked to see if it\n\t *  is an NMI stack.\n\t *  If the variable is not set and the stack is not the NMI\n\t *  stack then:\n\t *    o Set the special variable on the stack\n\t *    o Copy the interrupt frame into a \"saved\" location on the stack\n\t *    o Copy the interrupt frame into a \"copy\" location on the stack\n\t *    o Continue processing the NMI\n\t *  If the variable is set or the previous stack is the NMI stack:\n\t *    o Modify the \"copy\" location to jump to the repeate_nmi\n\t *    o return back to the first NMI\n\t *\n\t * Now on exit of the first NMI, we first clear the stack variable\n\t * The NMI stack will tell any nested NMIs at that point that it is\n\t * nested. Then we pop the stack normally with iret, and if there was\n\t * a nested NMI that updated the copy interrupt stack frame, a\n\t * jump will be made to the repeat_nmi code that will handle the second\n\t * NMI.\n\t */\n\n\t/* Use %rdx as out temp variable throughout */\n\tpushq_cfi %rdx\n\tCFI_REL_OFFSET rdx, 0\n\n\t/*\n\t * If %cs was not the kernel segment, then the NMI triggered in user\n\t * space, which means it is definitely not nested.\n\t */\n\tcmpl $__KERNEL_CS, 16(%rsp)\n\tjne first_nmi\n\n\t/*\n\t * Check the special variable on the stack to see if NMIs are\n\t * executing.\n\t */\n\tcmpl $1, -8(%rsp)\n\tje nested_nmi\n\n\t/*\n\t * Now test if the previous stack was an NMI stack.\n\t * We need the double check. We check the NMI stack to satisfy the\n\t * race when the first NMI clears the variable before returning.\n\t * We check the variable because the first NMI could be in a\n\t * breakpoint routine using a breakpoint stack.\n\t */\n\tlea 6*8(%rsp), %rdx\n\ttest_in_nmi rdx, 4*8(%rsp), nested_nmi, first_nmi\n\tCFI_REMEMBER_STATE\n\nnested_nmi:\n\t/*\n\t * Do nothing if we interrupted the fixup in repeat_nmi.\n\t * It's about to repeat the NMI handler, so we are fine\n\t * with ignoring this one.\n\t */\n\tmovq $repeat_nmi, %rdx\n\tcmpq 8(%rsp), %rdx\n\tja 1f\n\tmovq $end_repeat_nmi, %rdx\n\tcmpq 8(%rsp), %rdx\n\tja nested_nmi_out\n\n1:\n\t/* Set up the interrupted NMIs stack to jump to repeat_nmi */\n\tleaq -1*8(%rsp), %rdx\n\tmovq %rdx, %rsp\n\tCFI_ADJUST_CFA_OFFSET 1*8\n\tleaq -10*8(%rsp), %rdx\n\tpushq_cfi $__KERNEL_DS\n\tpushq_cfi %rdx\n\tpushfq_cfi\n\tpushq_cfi $__KERNEL_CS\n\tpushq_cfi $repeat_nmi\n\n\t/* Put stack back */\n\taddq $(6*8), %rsp\n\tCFI_ADJUST_CFA_OFFSET -6*8\n\nnested_nmi_out:\n\tpopq_cfi %rdx\n\tCFI_RESTORE rdx\n\n\t/* No need to check faults here */\n\tINTERRUPT_RETURN\n\n\tCFI_RESTORE_STATE\nfirst_nmi:\n\t/*\n\t * Because nested NMIs will use the pushed location that we\n\t * stored in rdx, we must keep that space available.\n\t * Here's what our stack frame will look like:\n\t * +-------------------------+\n\t * | original SS             |\n\t * | original Return RSP     |\n\t * | original RFLAGS         |\n\t * | original CS             |\n\t * | original RIP            |\n\t * +-------------------------+\n\t * | temp storage for rdx    |\n\t * +-------------------------+\n\t * | NMI executing variable  |\n\t * +-------------------------+\n\t * | copied SS               |\n\t * | copied Return RSP       |\n\t * | copied RFLAGS           |\n\t * | copied CS               |\n\t * | copied RIP              |\n\t * +-------------------------+\n\t * | Saved SS                |\n\t * | Saved Return RSP        |\n\t * | Saved RFLAGS            |\n\t * | Saved CS                |\n\t * | Saved RIP               |\n\t * +-------------------------+\n\t * | pt_regs                 |\n\t * +-------------------------+\n\t *\n\t * The saved stack frame is used to fix up the copied stack frame\n\t * that a nested NMI may change to make the interrupted NMI iret jump\n\t * to the repeat_nmi. The original stack frame and the temp storage\n\t * is also used by nested NMIs and can not be trusted on exit.\n\t */\n\t/* Do not pop rdx, nested NMIs will corrupt that part of the stack */\n\tmovq (%rsp), %rdx\n\tCFI_RESTORE rdx\n\n\t/* Set the NMI executing variable on the stack. */\n\tpushq_cfi $1\n\n\t/*\n\t * Leave room for the \"copied\" frame\n\t */\n\tsubq $(5*8), %rsp\n\tCFI_ADJUST_CFA_OFFSET 5*8\n\n\t/* Copy the stack frame to the Saved frame */\n\t.rept 5\n\tpushq_cfi 11*8(%rsp)\n\t.endr\n\tCFI_DEF_CFA_OFFSET SS+8-RIP\n\n\t/* Everything up to here is safe from nested NMIs */\n\n\t/*\n\t * If there was a nested NMI, the first NMI's iret will return\n\t * here. But NMIs are still enabled and we can take another\n\t * nested NMI. The nested NMI checks the interrupted RIP to see\n\t * if it is between repeat_nmi and end_repeat_nmi, and if so\n\t * it will just return, as we are about to repeat an NMI anyway.\n\t * This makes it safe to copy to the stack frame that a nested\n\t * NMI will update.\n\t */\nrepeat_nmi:\n\t/*\n\t * Update the stack variable to say we are still in NMI (the update\n\t * is benign for the non-repeat case, where 1 was pushed just above\n\t * to this very stack slot).\n\t */\n\tmovq $1, 10*8(%rsp)\n\n\t/* Make another copy, this one may be modified by nested NMIs */\n\taddq $(10*8), %rsp\n\tCFI_ADJUST_CFA_OFFSET -10*8\n\t.rept 5\n\tpushq_cfi -6*8(%rsp)\n\t.endr\n\tsubq $(5*8), %rsp\n\tCFI_DEF_CFA_OFFSET SS+8-RIP\nend_repeat_nmi:\n\n\t/*\n\t * Everything below this point can be preempted by a nested\n\t * NMI if the first NMI took an exception and reset our iret stack\n\t * so that we repeat another NMI.\n\t */\n\tpushq_cfi $-1\t\t/* ORIG_RAX: no syscall to restart */\n\tsubq $ORIG_RAX-R15, %rsp\n\tCFI_ADJUST_CFA_OFFSET ORIG_RAX-R15\n\t/*\n\t * Use save_paranoid to handle SWAPGS, but no need to use paranoid_exit\n\t * as we should not be calling schedule in NMI context.\n\t * Even with normal interrupts enabled. An NMI should not be\n\t * setting NEED_RESCHED or anything that normal interrupts and\n\t * exceptions might do.\n\t */\n\tcall save_paranoid\n\tDEFAULT_FRAME 0\n\n\t/*\n\t * Save off the CR2 register. If we take a page fault in the NMI then\n\t * it could corrupt the CR2 value. If the NMI preempts a page fault\n\t * handler before it was able to read the CR2 register, and then the\n\t * NMI itself takes a page fault, the page fault that was preempted\n\t * will read the information from the NMI page fault and not the\n\t * origin fault. Save it off and restore it if it changes.\n\t * Use the r12 callee-saved register.\n\t */\n\tmovq %cr2, %r12\n\n\t/* paranoidentry do_nmi, 0; without TRACE_IRQS_OFF */\n\tmovq %rsp,%rdi\n\tmovq $-1,%rsi\n\tcall do_nmi\n\n\t/* Did the NMI take a page fault? Restore cr2 if it did */\n\tmovq %cr2, %rcx\n\tcmpq %rcx, %r12\n\tje 1f\n\tmovq %r12, %cr2\n1:\n\t\n\ttestl %ebx,%ebx\t\t\t\t/* swapgs needed? */\n\tjnz nmi_restore\nnmi_swapgs:\n\tSWAPGS_UNSAFE_STACK\nnmi_restore:\n\t/* Pop the extra iret frame at once */\n\tRESTORE_ALL 6*8\n\n\t/* Clear the NMI executing stack variable */\n\tmovq $0, 5*8(%rsp)\n\tjmp irq_return\n\tCFI_ENDPROC\nEND(nmi)\n\nENTRY(ignore_sysret)\n\tCFI_STARTPROC\n\tmov $-ENOSYS,%eax\n\tsysret\n\tCFI_ENDPROC\nEND(ignore_sysret)\n\n", "/*\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs\n *\n *  Pentium III FXSR, SSE support\n *\tGareth Hughes <gareth@valinux.com>, May 2000\n */\n\n/*\n * Handle hardware traps and faults.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/context_tracking.h>\n#include <linux/interrupt.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/kprobes.h>\n#include <linux/uaccess.h>\n#include <linux/kdebug.h>\n#include <linux/kgdb.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/ptrace.h>\n#include <linux/uprobes.h>\n#include <linux/string.h>\n#include <linux/delay.h>\n#include <linux/errno.h>\n#include <linux/kexec.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/init.h>\n#include <linux/bug.h>\n#include <linux/nmi.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/io.h>\n\n#ifdef CONFIG_EISA\n#include <linux/ioport.h>\n#include <linux/eisa.h>\n#endif\n\n#if defined(CONFIG_EDAC)\n#include <linux/edac.h>\n#endif\n\n#include <asm/kmemcheck.h>\n#include <asm/stacktrace.h>\n#include <asm/processor.h>\n#include <asm/debugreg.h>\n#include <linux/atomic.h>\n#include <asm/ftrace.h>\n#include <asm/traps.h>\n#include <asm/desc.h>\n#include <asm/i387.h>\n#include <asm/fpu-internal.h>\n#include <asm/mce.h>\n#include <asm/fixmap.h>\n#include <asm/mach_traps.h>\n#include <asm/alternative.h>\n\n#ifdef CONFIG_X86_64\n#include <asm/x86_init.h>\n#include <asm/pgalloc.h>\n#include <asm/proto.h>\n\n/* No need to be aligned, but done to keep all IDTs defined the same way. */\ngate_desc debug_idt_table[NR_VECTORS] __page_aligned_bss;\n#else\n#include <asm/processor-flags.h>\n#include <asm/setup.h>\n\nasmlinkage int system_call(void);\n#endif\n\n/* Must be page-aligned because the real IDT is used in a fixmap. */\ngate_desc idt_table[NR_VECTORS] __page_aligned_bss;\n\nDECLARE_BITMAP(used_vectors, NR_VECTORS);\nEXPORT_SYMBOL_GPL(used_vectors);\n\nstatic inline void conditional_sti(struct pt_regs *regs)\n{\n\tif (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n}\n\nstatic inline void preempt_conditional_sti(struct pt_regs *regs)\n{\n\tpreempt_count_inc();\n\tif (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n}\n\nstatic inline void conditional_cli(struct pt_regs *regs)\n{\n\tif (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_disable();\n}\n\nstatic inline void preempt_conditional_cli(struct pt_regs *regs)\n{\n\tif (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_disable();\n\tpreempt_count_dec();\n}\n\nstatic nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n#ifdef CONFIG_X86_32\n\tif (regs->flags & X86_VM_MASK) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n#endif\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n\nstatic siginfo_t *fill_trap_info(struct pt_regs *regs, int signr, int trapnr,\n\t\t\t\tsiginfo_t *info)\n{\n\tunsigned long siaddr;\n\tint sicode;\n\n\tswitch (trapnr) {\n\tdefault:\n\t\treturn SEND_SIG_PRIV;\n\n\tcase X86_TRAP_DE:\n\t\tsicode = FPE_INTDIV;\n\t\tsiaddr = uprobe_get_trap_addr(regs);\n\t\tbreak;\n\tcase X86_TRAP_UD:\n\t\tsicode = ILL_ILLOPN;\n\t\tsiaddr = uprobe_get_trap_addr(regs);\n\t\tbreak;\n\tcase X86_TRAP_AC:\n\t\tsicode = BUS_ADRALN;\n\t\tsiaddr = 0;\n\t\tbreak;\n\t}\n\n\tinfo->si_signo = signr;\n\tinfo->si_errno = 0;\n\tinfo->si_code = sicode;\n\tinfo->si_addr = (void __user *)siaddr;\n\treturn info;\n}\n\nstatic void\ndo_trap(int trapnr, int signr, char *str, struct pt_regs *regs,\n\tlong error_code, siginfo_t *info)\n{\n\tstruct task_struct *tsk = current;\n\n\n\tif (!do_trap_no_signal(tsk, trapnr, str, regs, error_code))\n\t\treturn;\n\t/*\n\t * We want error_code and trap_nr set for userspace faults and\n\t * kernelspace faults which result in die(), but not\n\t * kernelspace faults which are fixed up.  die() gives the\n\t * process no chance to handle the signal and notice the\n\t * kernel fault information, so that won't result in polluting\n\t * the information about previously queued, but not yet\n\t * delivered, faults.  See also do_general_protection below.\n\t */\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = trapnr;\n\n#ifdef CONFIG_X86_64\n\tif (show_unhandled_signals && unhandled_signal(tsk, signr) &&\n\t    printk_ratelimit()) {\n\t\tpr_info(\"%s[%d] trap %s ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, tsk->pid, str,\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n#endif\n\n\tforce_sig_info(signr, info ?: SEND_SIG_PRIV, tsk);\n}\nNOKPROBE_SYMBOL(do_trap);\n\nstatic void do_error_trap(struct pt_regs *regs, long error_code, char *str,\n\t\t\t  unsigned long trapnr, int signr)\n{\n\tenum ctx_state prev_state = exception_enter();\n\tsiginfo_t info;\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=\n\t\t\tNOTIFY_STOP) {\n\t\tconditional_sti(regs);\n\t\tdo_trap(trapnr, signr, str, regs, error_code,\n\t\t\tfill_trap_info(regs, signr, trapnr, &info));\n\t}\n\n\texception_exit(prev_state);\n}\n\n#define DO_ERROR(trapnr, signr, str, name)\t\t\t\t\\\ndotraplinkage void do_##name(struct pt_regs *regs, long error_code)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tdo_error_trap(regs, error_code, str, trapnr, signr);\t\t\\\n}\n\nDO_ERROR(X86_TRAP_DE,     SIGFPE,  \"divide error\",\t\tdivide_error)\nDO_ERROR(X86_TRAP_OF,     SIGSEGV, \"overflow\",\t\t\toverflow)\nDO_ERROR(X86_TRAP_BR,     SIGSEGV, \"bounds\",\t\t\tbounds)\nDO_ERROR(X86_TRAP_UD,     SIGILL,  \"invalid opcode\",\t\tinvalid_op)\nDO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  \"coprocessor segment overrun\",coprocessor_segment_overrun)\nDO_ERROR(X86_TRAP_TS,     SIGSEGV, \"invalid TSS\",\t\tinvalid_TSS)\nDO_ERROR(X86_TRAP_NP,     SIGBUS,  \"segment not present\",\tsegment_not_present)\nDO_ERROR(X86_TRAP_SS,     SIGBUS,  \"stack segment\",\t\tstack_segment)\nDO_ERROR(X86_TRAP_AC,     SIGBUS,  \"alignment check\",\t\talignment_check)\n\n#ifdef CONFIG_X86_64\n/* Runs on IST stack */\ndotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)\n{\n\tstatic const char str[] = \"double fault\";\n\tstruct task_struct *tsk = current;\n\n#ifdef CONFIG_X86_ESPFIX64\n\textern unsigned char native_irq_return_iret[];\n\n\t/*\n\t * If IRET takes a non-IST fault on the espfix64 stack, then we\n\t * end up promoting it to a doublefault.  In that case, modify\n\t * the stack to make it look like we just entered the #GP\n\t * handler from user space, similar to bad_iret.\n\t */\n\tif (((long)regs->sp >> PGDIR_SHIFT) == ESPFIX_PGD_ENTRY &&\n\t\tregs->cs == __KERNEL_CS &&\n\t\tregs->ip == (unsigned long)native_irq_return_iret)\n\t{\n\t\tstruct pt_regs *normal_regs = task_pt_regs(current);\n\n\t\t/* Fake a #GP(0) from userspace. */\n\t\tmemmove(&normal_regs->ip, (void *)regs->sp, 5*8);\n\t\tnormal_regs->orig_ax = 0;  /* Missing (lost) #GP error code */\n\t\tregs->ip = (unsigned long)general_protection;\n\t\tregs->sp = (unsigned long)&normal_regs->orig_ax;\n\t\treturn;\n\t}\n#endif\n\n\texception_enter();\n\t/* Return not checked because double check cannot be ignored */\n\tnotify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_DF;\n\n#ifdef CONFIG_DOUBLEFAULT\n\tdf_debug(regs, error_code);\n#endif\n\t/*\n\t * This is always a kernel trap and never fixable (and thus must\n\t * never return).\n\t */\n\tfor (;;)\n\t\tdie(str, regs, error_code);\n}\n#endif\n\ndotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tconditional_sti(regs);\n\n#ifdef CONFIG_X86_32\n\tif (regs->flags & X86_VM_MASK) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\tgoto exit;\n\t}\n#endif\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs))\n\t\t\tgoto exit;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\tgoto exit;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\nexit:\n\texception_exit(prev_state);\n}\nNOKPROBE_SYMBOL(do_general_protection);\n\n/* May run on IST stack. */\ndotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n{\n\tenum ctx_state prev_state;\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t/*\n\t * ftrace must be first, everything else may cause a recursive crash.\n\t * See note by declaration of modifying_ftrace_code in ftrace.c\n\t */\n\tif (unlikely(atomic_read(&modifying_ftrace_code)) &&\n\t    ftrace_int3_handler(regs))\n\t\treturn;\n#endif\n\tif (poke_int3_handler(regs))\n\t\treturn;\n\n\tprev_state = exception_enter();\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n\n#ifdef CONFIG_KPROBES\n\tif (kprobe_int3_handler(regs))\n\t\tgoto exit;\n#endif\n\n\tif (notify_die(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n\n\t/*\n\t * Let others (NMI) know that the debug stack is in use\n\t * as we may switch to the interrupt stack.\n\t */\n\tdebug_stack_usage_inc();\n\tpreempt_conditional_sti(regs);\n\tdo_trap(X86_TRAP_BP, SIGTRAP, \"int3\", regs, error_code, NULL);\n\tpreempt_conditional_cli(regs);\n\tdebug_stack_usage_dec();\nexit:\n\texception_exit(prev_state);\n}\nNOKPROBE_SYMBOL(do_int3);\n\n#ifdef CONFIG_X86_64\n/*\n * Help handler running on IST stack to switch back to user stack\n * for scheduling or signal handling. The actual stack switch is done in\n * entry.S\n */\nasmlinkage __visible struct pt_regs *sync_regs(struct pt_regs *eregs)\n{\n\tstruct pt_regs *regs = eregs;\n\t/* Did already sync */\n\tif (eregs == (struct pt_regs *)eregs->sp)\n\t\t;\n\t/* Exception from user space */\n\telse if (user_mode(eregs))\n\t\tregs = task_pt_regs(current);\n\t/*\n\t * Exception from kernel and interrupts are enabled. Move to\n\t * kernel process stack.\n\t */\n\telse if (eregs->flags & X86_EFLAGS_IF)\n\t\tregs = (struct pt_regs *)(eregs->sp -= sizeof(struct pt_regs));\n\tif (eregs != regs)\n\t\t*regs = *eregs;\n\treturn regs;\n}\nNOKPROBE_SYMBOL(sync_regs);\n#endif\n\n/*\n * Our handling of the processor debug registers is non-trivial.\n * We do not clear them on entry and exit from the kernel. Therefore\n * it is possible to get a watchpoint trap here from inside the kernel.\n * However, the code in ./ptrace.c has ensured that the user can\n * only set watchpoints on userspace addresses. Therefore the in-kernel\n * watchpoint trap can only occur in code which is reading/writing\n * from user space. Such code must not hold kernel locks (since it\n * can equally take a page fault), therefore it is safe to call\n * force_sig_info even though that claims and releases locks.\n *\n * Code in ./signal.c ensures that the debug control register\n * is restored before we deliver any signal, and therefore that\n * user code runs with the correct debug control register even though\n * we clear it here.\n *\n * Being careful here means that we don't have to be as careful in a\n * lot of more complicated places (task switching can be a bit lazy\n * about restoring all the debug state, and ptrace doesn't have to\n * find every occurrence of the TF bit that could be saved away even\n * by user code)\n *\n * May run on IST stack.\n */\ndotraplinkage void do_debug(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk = current;\n\tenum ctx_state prev_state;\n\tint user_icebp = 0;\n\tunsigned long dr6;\n\tint si_code;\n\n\tprev_state = exception_enter();\n\n\tget_debugreg(dr6, 6);\n\n\t/* Filter out all the reserved bits which are preset to 1 */\n\tdr6 &= ~DR6_RESERVED;\n\n\t/*\n\t * If dr6 has no reason to give us about the origin of this trap,\n\t * then it's very likely the result of an icebp/int01 trap.\n\t * User wants a sigtrap for that.\n\t */\n\tif (!dr6 && user_mode(regs))\n\t\tuser_icebp = 1;\n\n\t/* Catch kmemcheck conditions first of all! */\n\tif ((dr6 & DR_STEP) && kmemcheck_trap(regs))\n\t\tgoto exit;\n\n\t/* DR6 may or may not be cleared by the CPU */\n\tset_debugreg(0, 6);\n\n\t/*\n\t * The processor cleared BTF, so don't mark that we need it set.\n\t */\n\tclear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);\n\n\t/* Store the virtualized DR6 value */\n\ttsk->thread.debugreg6 = dr6;\n\n#ifdef CONFIG_KPROBES\n\tif (kprobe_debug_handler(regs))\n\t\tgoto exit;\n#endif\n\n\tif (notify_die(DIE_DEBUG, \"debug\", regs, (long)&dr6, error_code,\n\t\t\t\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n\n\t/*\n\t * Let others (NMI) know that the debug stack is in use\n\t * as we may switch to the interrupt stack.\n\t */\n\tdebug_stack_usage_inc();\n\n\t/* It's safe to allow irq's after DR6 has been saved */\n\tpreempt_conditional_sti(regs);\n\n\tif (regs->flags & X86_VM_MASK) {\n\t\thandle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,\n\t\t\t\t\tX86_TRAP_DB);\n\t\tpreempt_conditional_cli(regs);\n\t\tdebug_stack_usage_dec();\n\t\tgoto exit;\n\t}\n\n\t/*\n\t * Single-stepping through system calls: ignore any exceptions in\n\t * kernel space, but re-enable TF when returning to user mode.\n\t *\n\t * We already checked v86 mode above, so we can check for kernel mode\n\t * by just checking the CPL of CS.\n\t */\n\tif ((dr6 & DR_STEP) && !user_mode(regs)) {\n\t\ttsk->thread.debugreg6 &= ~DR_STEP;\n\t\tset_tsk_thread_flag(tsk, TIF_SINGLESTEP);\n\t\tregs->flags &= ~X86_EFLAGS_TF;\n\t}\n\tsi_code = get_si_code(tsk->thread.debugreg6);\n\tif (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)\n\t\tsend_sigtrap(tsk, regs, error_code, si_code);\n\tpreempt_conditional_cli(regs);\n\tdebug_stack_usage_dec();\n\nexit:\n\texception_exit(prev_state);\n}\nNOKPROBE_SYMBOL(do_debug);\n\n/*\n * Note that we play around with the 'TS' bit in an attempt to get\n * the correct behaviour even in the presence of the asynchronous\n * IRQ13 behaviour\n */\nstatic void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tsiginfo_t info;\n\tunsigned short err;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode_vm(regs))\n\t{\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tsave_init_fpu(task);\n\ttask->thread.trap_nr = trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo = SIGFPE;\n\tinfo.si_errno = 0;\n\tinfo.si_addr = (void __user *)uprobe_get_trap_addr(regs);\n\tif (trapnr == X86_TRAP_MF) {\n\t\tunsigned short cwd, swd;\n\t\t/*\n\t\t * (~cwd & swd) will mask out exceptions that are not set to unmasked\n\t\t * status.  0x3f is the exception bits in these regs, 0x200 is the\n\t\t * C1 reg you need in case of a stack fault, 0x040 is the stack\n\t\t * fault bit.  We should only be taking one exception at a time,\n\t\t * so if this combination doesn't produce any single exception,\n\t\t * then we have a bad program that isn't synchronizing its FPU usage\n\t\t * and it will suffer the consequences since we won't be able to\n\t\t * fully reproduce the context of the exception\n\t\t */\n\t\tcwd = get_fpu_cwd(task);\n\t\tswd = get_fpu_swd(task);\n\n\t\terr = swd & ~cwd;\n\t} else {\n\t\t/*\n\t\t * The SIMD FPU exceptions are handled a little differently, as there\n\t\t * is only a single status/control register.  Thus, to determine which\n\t\t * unmasked exception was caught we must mask the exception mask bits\n\t\t * at 0x1f80, and then use these to mask the exception bits at 0x3f.\n\t\t */\n\t\tunsigned short mxcsr = get_fpu_mxcsr(task);\n\t\terr = ~(mxcsr >> 7) & mxcsr;\n\t}\n\n\tif (err & 0x001) {\t/* Invalid op */\n\t\t/*\n\t\t * swd & 0x240 == 0x040: Stack Underflow\n\t\t * swd & 0x240 == 0x240: Stack Overflow\n\t\t * User must clear the SF bit (0x40) if set\n\t\t */\n\t\tinfo.si_code = FPE_FLTINV;\n\t} else if (err & 0x004) { /* Divide by Zero */\n\t\tinfo.si_code = FPE_FLTDIV;\n\t} else if (err & 0x008) { /* Overflow */\n\t\tinfo.si_code = FPE_FLTOVF;\n\t} else if (err & 0x012) { /* Denormal, Underflow */\n\t\tinfo.si_code = FPE_FLTUND;\n\t} else if (err & 0x020) { /* Precision */\n\t\tinfo.si_code = FPE_FLTRES;\n\t} else {\n\t\t/*\n\t\t * If we're using IRQ 13, or supposedly even some trap\n\t\t * X86_TRAP_MF implementations, it's possible\n\t\t * we get a spurious trap, which is not an error.\n\t\t */\n\t\treturn;\n\t}\n\tforce_sig_info(SIGFPE, &info, task);\n}\n\ndotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)\n{\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tmath_error(regs, error_code, X86_TRAP_MF);\n\texception_exit(prev_state);\n}\n\ndotraplinkage void\ndo_simd_coprocessor_error(struct pt_regs *regs, long error_code)\n{\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tmath_error(regs, error_code, X86_TRAP_XF);\n\texception_exit(prev_state);\n}\n\ndotraplinkage void\ndo_spurious_interrupt_bug(struct pt_regs *regs, long error_code)\n{\n\tconditional_sti(regs);\n#if 0\n\t/* No need to warn about this any longer. */\n\tpr_info(\"Ignoring P6 Local APIC Spurious Interrupt Bug...\\n\");\n#endif\n}\n\nasmlinkage __visible void __attribute__((weak)) smp_thermal_interrupt(void)\n{\n}\n\nasmlinkage __visible void __attribute__((weak)) smp_threshold_interrupt(void)\n{\n}\n\n/*\n * 'math_state_restore()' saves the current math information in the\n * old math state array, and gets the new ones from the current task\n *\n * Careful.. There are problems with IBM-designed IRQ13 behaviour.\n * Don't touch unless you *really* know how it works.\n *\n * Must be called with kernel preemption disabled (eg with local\n * local interrupts as in the case of do_device_not_available).\n */\nvoid math_state_restore(void)\n{\n\tstruct task_struct *tsk = current;\n\n\tif (!tsk_used_math(tsk)) {\n\t\tlocal_irq_enable();\n\t\t/*\n\t\t * does a slab alloc which can sleep\n\t\t */\n\t\tif (init_fpu(tsk)) {\n\t\t\t/*\n\t\t\t * ran out of memory!\n\t\t\t */\n\t\t\tdo_group_exit(SIGKILL);\n\t\t\treturn;\n\t\t}\n\t\tlocal_irq_disable();\n\t}\n\n\t__thread_fpu_begin(tsk);\n\n\t/*\n\t * Paranoid restore. send a SIGSEGV if we fail to restore the state.\n\t */\n\tif (unlikely(restore_fpu_checking(tsk))) {\n\t\tdrop_init_fpu(tsk);\n\t\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n\t\treturn;\n\t}\n\n\ttsk->thread.fpu_counter++;\n}\nEXPORT_SYMBOL_GPL(math_state_restore);\n\ndotraplinkage void\ndo_device_not_available(struct pt_regs *regs, long error_code)\n{\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tBUG_ON(use_eager_fpu());\n\n#ifdef CONFIG_MATH_EMULATION\n\tif (read_cr0() & X86_CR0_EM) {\n\t\tstruct math_emu_info info = { };\n\n\t\tconditional_sti(regs);\n\n\t\tinfo.regs = regs;\n\t\tmath_emulate(&info);\n\t\texception_exit(prev_state);\n\t\treturn;\n\t}\n#endif\n\tmath_state_restore(); /* interrupts still off */\n#ifdef CONFIG_X86_32\n\tconditional_sti(regs);\n#endif\n\texception_exit(prev_state);\n}\nNOKPROBE_SYMBOL(do_device_not_available);\n\n#ifdef CONFIG_X86_32\ndotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)\n{\n\tsiginfo_t info;\n\tenum ctx_state prev_state;\n\n\tprev_state = exception_enter();\n\tlocal_irq_enable();\n\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code = ILL_BADSTK;\n\tinfo.si_addr = NULL;\n\tif (notify_die(DIE_TRAP, \"iret exception\", regs, error_code,\n\t\t\tX86_TRAP_IRET, SIGILL) != NOTIFY_STOP) {\n\t\tdo_trap(X86_TRAP_IRET, SIGILL, \"iret exception\", regs, error_code,\n\t\t\t&info);\n\t}\n\texception_exit(prev_state);\n}\n#endif\n\n/* Set of traps needed for early debugging. */\nvoid __init early_trap_init(void)\n{\n\tset_intr_gate_ist(X86_TRAP_DB, &debug, DEBUG_STACK);\n\t/* int3 can be called from all */\n\tset_system_intr_gate_ist(X86_TRAP_BP, &int3, DEBUG_STACK);\n#ifdef CONFIG_X86_32\n\tset_intr_gate(X86_TRAP_PF, page_fault);\n#endif\n\tload_idt(&idt_descr);\n}\n\nvoid __init early_trap_pf_init(void)\n{\n#ifdef CONFIG_X86_64\n\tset_intr_gate(X86_TRAP_PF, page_fault);\n#endif\n}\n\nvoid __init trap_init(void)\n{\n\tint i;\n\n#ifdef CONFIG_EISA\n\tvoid __iomem *p = early_ioremap(0x0FFFD9, 4);\n\n\tif (readl(p) == 'E' + ('I'<<8) + ('S'<<16) + ('A'<<24))\n\t\tEISA_bus = 1;\n\tearly_iounmap(p, 4);\n#endif\n\n\tset_intr_gate(X86_TRAP_DE, divide_error);\n\tset_intr_gate_ist(X86_TRAP_NMI, &nmi, NMI_STACK);\n\t/* int4 can be called from all */\n\tset_system_intr_gate(X86_TRAP_OF, &overflow);\n\tset_intr_gate(X86_TRAP_BR, bounds);\n\tset_intr_gate(X86_TRAP_UD, invalid_op);\n\tset_intr_gate(X86_TRAP_NM, device_not_available);\n#ifdef CONFIG_X86_32\n\tset_task_gate(X86_TRAP_DF, GDT_ENTRY_DOUBLEFAULT_TSS);\n#else\n\tset_intr_gate_ist(X86_TRAP_DF, &double_fault, DOUBLEFAULT_STACK);\n#endif\n\tset_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);\n\tset_intr_gate(X86_TRAP_TS, invalid_TSS);\n\tset_intr_gate(X86_TRAP_NP, segment_not_present);\n\tset_intr_gate(X86_TRAP_SS, stack_segment);\n\tset_intr_gate(X86_TRAP_GP, general_protection);\n\tset_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);\n\tset_intr_gate(X86_TRAP_MF, coprocessor_error);\n\tset_intr_gate(X86_TRAP_AC, alignment_check);\n#ifdef CONFIG_X86_MCE\n\tset_intr_gate_ist(X86_TRAP_MC, &machine_check, MCE_STACK);\n#endif\n\tset_intr_gate(X86_TRAP_XF, simd_coprocessor_error);\n\n\t/* Reserve all the builtin and the syscall vector: */\n\tfor (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)\n\t\tset_bit(i, used_vectors);\n\n#ifdef CONFIG_IA32_EMULATION\n\tset_system_intr_gate(IA32_SYSCALL_VECTOR, ia32_syscall);\n\tset_bit(IA32_SYSCALL_VECTOR, used_vectors);\n#endif\n\n#ifdef CONFIG_X86_32\n\tset_system_trap_gate(SYSCALL_VECTOR, &system_call);\n\tset_bit(SYSCALL_VECTOR, used_vectors);\n#endif\n\n\t/*\n\t * Set the IDT descriptor to a fixed read-only location, so that the\n\t * \"sidt\" instruction will not leak the location of the kernel, and\n\t * to defend the IDT against arbitrary memory write vulnerabilities.\n\t * It will be reloaded in cpu_init() */\n\t__set_fixmap(FIX_RO_IDT, __pa_symbol(idt_table), PAGE_KERNEL_RO);\n\tidt_descr.address = fix_to_virt(FIX_RO_IDT);\n\n\t/*\n\t * Should be a barrier for any external CPU state:\n\t */\n\tcpu_init();\n\n\tx86_init.irqs.trap_init();\n\n#ifdef CONFIG_X86_64\n\tmemcpy(&debug_idt_table, &idt_table, IDT_ENTRIES * 16);\n\tset_nmi_gate(X86_TRAP_DB, &debug);\n\tset_nmi_gate(X86_TRAP_BP, &int3);\n#endif\n}\n"], "filenames": ["arch/x86/include/asm/page_32_types.h", "arch/x86/include/asm/page_64_types.h", "arch/x86/include/asm/traps.h", "arch/x86/kernel/dumpstack_64.c", "arch/x86/kernel/entry_64.S", "arch/x86/kernel/traps.c"], "buggy_code_start_loc": [23, 17, 41, 27, 1262, 236], "buggy_code_end_loc": [24, 23, 41, 28, 1263, 806], "fixing_code_start_loc": [22, 17, 42, 26, 1262, 235], "fixing_code_end_loc": [22, 22, 43, 26, 1263, 790], "type": "CWE-269", "message": "arch/x86/kernel/entry_64.S in the Linux kernel before 3.17.5 does not properly handle faults associated with the Stack Segment (SS) segment register, which allows local users to gain privileges by triggering an IRET instruction that leads to access to a GS Base address from the wrong space.", "other": {"cve": {"id": "CVE-2014-9322", "sourceIdentifier": "cve@mitre.org", "published": "2014-12-17T11:59:02.383", "lastModified": "2023-01-17T21:29:39.593", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "arch/x86/kernel/entry_64.S in the Linux kernel before 3.17.5 does not properly handle faults associated with the Stack Segment (SS) segment register, which allows local users to gain privileges by triggering an IRET instruction that leads to access to a GS Base address from the wrong space."}, {"lang": "es", "value": "arch/x86/kernel/entry_64.S en el kernel de Linux anterior a 3.17.5 no maneja correctamente los fallos asociados con el registro de segmento Stack Segment (SS), lo que permite a usuarios locales ganar privilegios mediante la provocaci\u00f3n de una instrucci\u00f3n IRET que lleva al acceso a una direcci\u00f3n de GS Base del espacio equivocado."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-269"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.2.65", "matchCriteriaId": "7F77159B-2E02-4C33-BA2A-635FA16BF9DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.3", "versionEndExcluding": "3.4.106", "matchCriteriaId": "79C4C5AF-4667-4F85-9043-D834576687A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.5", "versionEndExcluding": "3.10.62", "matchCriteriaId": "01745242-96D6-4F79-8C15-CE95C563D67A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.11", "versionEndExcluding": "3.12.35", "matchCriteriaId": "58DF8270-B8C1-4726-87FD-12A33F269252"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.14.26", "matchCriteriaId": "B80B5B61-C917-45DE-A3F9-8B1EC2B2CAA2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.15", "versionEndExcluding": "3.16.35", "matchCriteriaId": "7DC4BA70-B111-4D2E-BC78-6601CED68F08"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.17.5", "matchCriteriaId": "5BBD5646-0A01-4DCB-A47F-7B7F86FE000A"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:5.6:*:*:*:*:*:*:*", "matchCriteriaId": "903512FC-0017-4564-9B89-7E64FFB14B11"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:10.04:*:*:*:-:*:*:*", "matchCriteriaId": "01EDA41C-6B2E-49AF-B503-EB3882265C11"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:evergreen:11.4:*:*:*:*:*:*:*", "matchCriteriaId": "CCE4D64E-8C4B-4F21-A9B0-90637C85C1D0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:suse_linux_enterprise_server:10:sp4:*:*:ltss:*:*:*", "matchCriteriaId": "61853C27-E1A3-49BC-993D-6B32802F668F"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:google:android:6.0:*:*:*:*:*:*:*", "matchCriteriaId": "E70C6D8D-C9C3-4D92-8DFC-71F59E068295"}, {"vulnerable": true, "criteria": "cpe:2.3:o:google:android:6.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "691FA41B-C2CE-413F-ABB1-0B22CB322807"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=6f442be2fb22be02cafa606f1769fa1e6f894441", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00025.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-04/msg00015.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-04/msg00020.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://marc.info/?l=bugtraq&m=142722450701342&w=2", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://marc.info/?l=bugtraq&m=142722544401658&w=2", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-1998.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-2008.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-2028.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-2031.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2015-0009.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://source.android.com/security/bulletin/2016-04-02.html", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.exploit-db.com/exploits/36266", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/12/15/6", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2491-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.zerodayinitiative.com/advisories/ZDI-16-170", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1172806", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/6f442be2fb22be02cafa606f1769fa1e6f894441", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://help.joyent.com/entries/98788667-Security-Advisory-ZDI-CAN-3263-ZDI-CAN-3284-and-ZDI-CAN-3364-Vulnerabilities", "source": "cve@mitre.org", "tags": ["Permissions Required", "Third Party Advisory"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.17.5", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/6f442be2fb22be02cafa606f1769fa1e6f894441"}}