{"buggy_code": ["/*\n * Copyright (c) 2006 Chelsio, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n#include <linux/module.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/skbuff.h>\n#include <linux/timer.h>\n#include <linux/notifier.h>\n#include <linux/inetdevice.h>\n\n#include <net/neighbour.h>\n#include <net/netevent.h>\n#include <net/route.h>\n\n#include \"tcb.h\"\n#include \"cxgb3_offload.h\"\n#include \"iwch.h\"\n#include \"iwch_provider.h\"\n#include \"iwch_cm.h\"\n\nstatic char *states[] = {\n\t\"idle\",\n\t\"listen\",\n\t\"connecting\",\n\t\"mpa_wait_req\",\n\t\"mpa_req_sent\",\n\t\"mpa_req_rcvd\",\n\t\"mpa_rep_sent\",\n\t\"fpdu_mode\",\n\t\"aborting\",\n\t\"closing\",\n\t\"moribund\",\n\t\"dead\",\n\tNULL,\n};\n\nint peer2peer = 0;\nmodule_param(peer2peer, int, 0644);\nMODULE_PARM_DESC(peer2peer, \"Support peer2peer ULPs (default=0)\");\n\nstatic int ep_timeout_secs = 60;\nmodule_param(ep_timeout_secs, int, 0644);\nMODULE_PARM_DESC(ep_timeout_secs, \"CM Endpoint operation timeout \"\n\t\t\t\t   \"in seconds (default=60)\");\n\nstatic int mpa_rev = 1;\nmodule_param(mpa_rev, int, 0644);\nMODULE_PARM_DESC(mpa_rev, \"MPA Revision, 0 supports amso1100, \"\n\t\t \"1 is spec compliant. (default=1)\");\n\nstatic int markers_enabled = 0;\nmodule_param(markers_enabled, int, 0644);\nMODULE_PARM_DESC(markers_enabled, \"Enable MPA MARKERS (default(0)=disabled)\");\n\nstatic int crc_enabled = 1;\nmodule_param(crc_enabled, int, 0644);\nMODULE_PARM_DESC(crc_enabled, \"Enable MPA CRC (default(1)=enabled)\");\n\nstatic int rcv_win = 256 * 1024;\nmodule_param(rcv_win, int, 0644);\nMODULE_PARM_DESC(rcv_win, \"TCP receive window in bytes (default=256)\");\n\nstatic int snd_win = 32 * 1024;\nmodule_param(snd_win, int, 0644);\nMODULE_PARM_DESC(snd_win, \"TCP send window in bytes (default=32KB)\");\n\nstatic unsigned int nocong = 0;\nmodule_param(nocong, uint, 0644);\nMODULE_PARM_DESC(nocong, \"Turn off congestion control (default=0)\");\n\nstatic unsigned int cong_flavor = 1;\nmodule_param(cong_flavor, uint, 0644);\nMODULE_PARM_DESC(cong_flavor, \"TCP Congestion control flavor (default=1)\");\n\nstatic struct workqueue_struct *workq;\n\nstatic struct sk_buff_head rxq;\n\nstatic struct sk_buff *get_skb(struct sk_buff *skb, int len, gfp_t gfp);\nstatic void ep_timeout(unsigned long arg);\nstatic void connect_reply_upcall(struct iwch_ep *ep, int status);\n\nstatic void start_ep_timer(struct iwch_ep *ep)\n{\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tif (timer_pending(&ep->timer)) {\n\t\tPDBG(\"%s stopped / restarted timer ep %p\\n\", __func__, ep);\n\t\tdel_timer_sync(&ep->timer);\n\t} else\n\t\tget_ep(&ep->com);\n\tep->timer.expires = jiffies + ep_timeout_secs * HZ;\n\tep->timer.data = (unsigned long)ep;\n\tep->timer.function = ep_timeout;\n\tadd_timer(&ep->timer);\n}\n\nstatic void stop_ep_timer(struct iwch_ep *ep)\n{\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tif (!timer_pending(&ep->timer)) {\n\t\tWARN(1, \"%s timer stopped when its not running!  ep %p state %u\\n\",\n\t\t\t__func__, ep, ep->com.state);\n\t\treturn;\n\t}\n\tdel_timer_sync(&ep->timer);\n\tput_ep(&ep->com);\n}\n\nstatic int iwch_l2t_send(struct t3cdev *tdev, struct sk_buff *skb, struct l2t_entry *l2e)\n{\n\tint\terror = 0;\n\tstruct cxio_rdev *rdev;\n\n\trdev = (struct cxio_rdev *)tdev->ulp;\n\tif (cxio_fatal_error(rdev)) {\n\t\tkfree_skb(skb);\n\t\treturn -EIO;\n\t}\n\terror = l2t_send(tdev, skb, l2e);\n\tif (error < 0)\n\t\tkfree_skb(skb);\n\treturn error;\n}\n\nint iwch_cxgb3_ofld_send(struct t3cdev *tdev, struct sk_buff *skb)\n{\n\tint\terror = 0;\n\tstruct cxio_rdev *rdev;\n\n\trdev = (struct cxio_rdev *)tdev->ulp;\n\tif (cxio_fatal_error(rdev)) {\n\t\tkfree_skb(skb);\n\t\treturn -EIO;\n\t}\n\terror = cxgb3_ofld_send(tdev, skb);\n\tif (error < 0)\n\t\tkfree_skb(skb);\n\treturn error;\n}\n\nstatic void release_tid(struct t3cdev *tdev, u32 hwtid, struct sk_buff *skb)\n{\n\tstruct cpl_tid_release *req;\n\n\tskb = get_skb(skb, sizeof *req, GFP_KERNEL);\n\tif (!skb)\n\t\treturn;\n\treq = (struct cpl_tid_release *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_TID_RELEASE, hwtid));\n\tskb->priority = CPL_PRIORITY_SETUP;\n\tiwch_cxgb3_ofld_send(tdev, skb);\n\treturn;\n}\n\nint iwch_quiesce_tid(struct iwch_ep *ep)\n{\n\tstruct cpl_set_tcb_field *req;\n\tstruct sk_buff *skb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\n\tif (!skb)\n\t\treturn -ENOMEM;\n\treq = (struct cpl_set_tcb_field *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\treq->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, ep->hwtid));\n\treq->reply = 0;\n\treq->cpu_idx = 0;\n\treq->word = htons(W_TCB_RX_QUIESCE);\n\treq->mask = cpu_to_be64(1ULL << S_TCB_RX_QUIESCE);\n\treq->val = cpu_to_be64(1 << S_TCB_RX_QUIESCE);\n\n\tskb->priority = CPL_PRIORITY_DATA;\n\treturn iwch_cxgb3_ofld_send(ep->com.tdev, skb);\n}\n\nint iwch_resume_tid(struct iwch_ep *ep)\n{\n\tstruct cpl_set_tcb_field *req;\n\tstruct sk_buff *skb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\n\tif (!skb)\n\t\treturn -ENOMEM;\n\treq = (struct cpl_set_tcb_field *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\treq->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, ep->hwtid));\n\treq->reply = 0;\n\treq->cpu_idx = 0;\n\treq->word = htons(W_TCB_RX_QUIESCE);\n\treq->mask = cpu_to_be64(1ULL << S_TCB_RX_QUIESCE);\n\treq->val = 0;\n\n\tskb->priority = CPL_PRIORITY_DATA;\n\treturn iwch_cxgb3_ofld_send(ep->com.tdev, skb);\n}\n\nstatic void set_emss(struct iwch_ep *ep, u16 opt)\n{\n\tPDBG(\"%s ep %p opt %u\\n\", __func__, ep, opt);\n\tep->emss = T3C_DATA(ep->com.tdev)->mtus[G_TCPOPT_MSS(opt)] - 40;\n\tif (G_TCPOPT_TSTAMP(opt))\n\t\tep->emss -= 12;\n\tif (ep->emss < 128)\n\t\tep->emss = 128;\n\tPDBG(\"emss=%d\\n\", ep->emss);\n}\n\nstatic enum iwch_ep_state state_read(struct iwch_ep_common *epc)\n{\n\tunsigned long flags;\n\tenum iwch_ep_state state;\n\n\tspin_lock_irqsave(&epc->lock, flags);\n\tstate = epc->state;\n\tspin_unlock_irqrestore(&epc->lock, flags);\n\treturn state;\n}\n\nstatic void __state_set(struct iwch_ep_common *epc, enum iwch_ep_state new)\n{\n\tepc->state = new;\n}\n\nstatic void state_set(struct iwch_ep_common *epc, enum iwch_ep_state new)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&epc->lock, flags);\n\tPDBG(\"%s - %s -> %s\\n\", __func__, states[epc->state], states[new]);\n\t__state_set(epc, new);\n\tspin_unlock_irqrestore(&epc->lock, flags);\n\treturn;\n}\n\nstatic void *alloc_ep(int size, gfp_t gfp)\n{\n\tstruct iwch_ep_common *epc;\n\n\tepc = kzalloc(size, gfp);\n\tif (epc) {\n\t\tkref_init(&epc->kref);\n\t\tspin_lock_init(&epc->lock);\n\t\tinit_waitqueue_head(&epc->waitq);\n\t}\n\tPDBG(\"%s alloc ep %p\\n\", __func__, epc);\n\treturn epc;\n}\n\nvoid __free_ep(struct kref *kref)\n{\n\tstruct iwch_ep *ep;\n\tep = container_of(container_of(kref, struct iwch_ep_common, kref),\n\t\t\t  struct iwch_ep, com);\n\tPDBG(\"%s ep %p state %s\\n\", __func__, ep, states[state_read(&ep->com)]);\n\tif (test_bit(RELEASE_RESOURCES, &ep->com.flags)) {\n\t\tcxgb3_remove_tid(ep->com.tdev, (void *)ep, ep->hwtid);\n\t\tdst_release(ep->dst);\n\t\tl2t_release(ep->com.tdev, ep->l2t);\n\t}\n\tkfree(ep);\n}\n\nstatic void release_ep_resources(struct iwch_ep *ep)\n{\n\tPDBG(\"%s ep %p tid %d\\n\", __func__, ep, ep->hwtid);\n\tset_bit(RELEASE_RESOURCES, &ep->com.flags);\n\tput_ep(&ep->com);\n}\n\nstatic int status2errno(int status)\n{\n\tswitch (status) {\n\tcase CPL_ERR_NONE:\n\t\treturn 0;\n\tcase CPL_ERR_CONN_RESET:\n\t\treturn -ECONNRESET;\n\tcase CPL_ERR_ARP_MISS:\n\t\treturn -EHOSTUNREACH;\n\tcase CPL_ERR_CONN_TIMEDOUT:\n\t\treturn -ETIMEDOUT;\n\tcase CPL_ERR_TCAM_FULL:\n\t\treturn -ENOMEM;\n\tcase CPL_ERR_CONN_EXIST:\n\t\treturn -EADDRINUSE;\n\tdefault:\n\t\treturn -EIO;\n\t}\n}\n\n/*\n * Try and reuse skbs already allocated...\n */\nstatic struct sk_buff *get_skb(struct sk_buff *skb, int len, gfp_t gfp)\n{\n\tif (skb && !skb_is_nonlinear(skb) && !skb_cloned(skb)) {\n\t\tskb_trim(skb, 0);\n\t\tskb_get(skb);\n\t} else {\n\t\tskb = alloc_skb(len, gfp);\n\t}\n\treturn skb;\n}\n\nstatic struct rtable *find_route(struct t3cdev *dev, __be32 local_ip,\n\t\t\t\t __be32 peer_ip, __be16 local_port,\n\t\t\t\t __be16 peer_port, u8 tos)\n{\n\tstruct rtable *rt;\n\tstruct flowi4 fl4;\n\n\trt = ip_route_output_ports(&init_net, &fl4, NULL, peer_ip, local_ip,\n\t\t\t\t   peer_port, local_port, IPPROTO_TCP,\n\t\t\t\t   tos, 0);\n\tif (IS_ERR(rt))\n\t\treturn NULL;\n\treturn rt;\n}\n\nstatic unsigned int find_best_mtu(const struct t3c_data *d, unsigned short mtu)\n{\n\tint i = 0;\n\n\twhile (i < d->nmtus - 1 && d->mtus[i + 1] <= mtu)\n\t\t++i;\n\treturn i;\n}\n\nstatic void arp_failure_discard(struct t3cdev *dev, struct sk_buff *skb)\n{\n\tPDBG(\"%s t3cdev %p\\n\", __func__, dev);\n\tkfree_skb(skb);\n}\n\n/*\n * Handle an ARP failure for an active open.\n */\nstatic void act_open_req_arp_failure(struct t3cdev *dev, struct sk_buff *skb)\n{\n\tprintk(KERN_ERR MOD \"ARP failure duing connect\\n\");\n\tkfree_skb(skb);\n}\n\n/*\n * Handle an ARP failure for a CPL_ABORT_REQ.  Change it into a no RST variant\n * and send it along.\n */\nstatic void abort_arp_failure(struct t3cdev *dev, struct sk_buff *skb)\n{\n\tstruct cpl_abort_req *req = cplhdr(skb);\n\n\tPDBG(\"%s t3cdev %p\\n\", __func__, dev);\n\treq->cmd = CPL_ABORT_NO_RST;\n\tiwch_cxgb3_ofld_send(dev, skb);\n}\n\nstatic int send_halfclose(struct iwch_ep *ep, gfp_t gfp)\n{\n\tstruct cpl_close_con_req *req;\n\tstruct sk_buff *skb;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb = get_skb(NULL, sizeof(*req), gfp);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to alloc skb\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tskb->priority = CPL_PRIORITY_DATA;\n\tset_arp_failure_handler(skb, arp_failure_discard);\n\treq = (struct cpl_close_con_req *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_CLOSE_CON));\n\treq->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_CLOSE_CON_REQ, ep->hwtid));\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic int send_abort(struct iwch_ep *ep, struct sk_buff *skb, gfp_t gfp)\n{\n\tstruct cpl_abort_req *req;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb = get_skb(skb, sizeof(*req), gfp);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to alloc skb.\\n\",\n\t\t       __func__);\n\t\treturn -ENOMEM;\n\t}\n\tskb->priority = CPL_PRIORITY_DATA;\n\tset_arp_failure_handler(skb, abort_arp_failure);\n\treq = (struct cpl_abort_req *) skb_put(skb, sizeof(*req));\n\tmemset(req, 0, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_HOST_ABORT_CON_REQ));\n\treq->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_ABORT_REQ, ep->hwtid));\n\treq->cmd = CPL_ABORT_SEND_RST;\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic int send_connect(struct iwch_ep *ep)\n{\n\tstruct cpl_act_open_req *req;\n\tstruct sk_buff *skb;\n\tu32 opt0h, opt0l, opt2;\n\tunsigned int mtu_idx;\n\tint wscale;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\n\tskb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to alloc skb.\\n\",\n\t\t       __func__);\n\t\treturn -ENOMEM;\n\t}\n\tmtu_idx = find_best_mtu(T3C_DATA(ep->com.tdev), dst_mtu(ep->dst));\n\twscale = compute_wscale(rcv_win);\n\topt0h = V_NAGLE(0) |\n\t    V_NO_CONG(nocong) |\n\t    V_KEEP_ALIVE(1) |\n\t    F_TCAM_BYPASS |\n\t    V_WND_SCALE(wscale) |\n\t    V_MSS_IDX(mtu_idx) |\n\t    V_L2T_IDX(ep->l2t->idx) | V_TX_CHANNEL(ep->l2t->smt_idx);\n\topt0l = V_TOS((ep->tos >> 2) & M_TOS) | V_RCV_BUFSIZ(rcv_win>>10);\n\topt2 = F_RX_COALESCE_VALID | V_RX_COALESCE(0) | V_FLAVORS_VALID(1) |\n\t       V_CONG_CONTROL_FLAVOR(cong_flavor);\n\tskb->priority = CPL_PRIORITY_SETUP;\n\tset_arp_failure_handler(skb, act_open_req_arp_failure);\n\n\treq = (struct cpl_act_open_req *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_ACT_OPEN_REQ, ep->atid));\n\treq->local_port = ep->com.local_addr.sin_port;\n\treq->peer_port = ep->com.remote_addr.sin_port;\n\treq->local_ip = ep->com.local_addr.sin_addr.s_addr;\n\treq->peer_ip = ep->com.remote_addr.sin_addr.s_addr;\n\treq->opt0h = htonl(opt0h);\n\treq->opt0l = htonl(opt0l);\n\treq->params = 0;\n\treq->opt2 = htonl(opt2);\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic void send_mpa_req(struct iwch_ep *ep, struct sk_buff *skb)\n{\n\tint mpalen;\n\tstruct tx_data_wr *req;\n\tstruct mpa_message *mpa;\n\tint len;\n\n\tPDBG(\"%s ep %p pd_len %d\\n\", __func__, ep, ep->plen);\n\n\tBUG_ON(skb_cloned(skb));\n\n\tmpalen = sizeof(*mpa) + ep->plen;\n\tif (skb->data + mpalen + sizeof(*req) > skb_end_pointer(skb)) {\n\t\tkfree_skb(skb);\n\t\tskb=alloc_skb(mpalen + sizeof(*req), GFP_KERNEL);\n\t\tif (!skb) {\n\t\t\tconnect_reply_upcall(ep, -ENOMEM);\n\t\t\treturn;\n\t\t}\n\t}\n\tskb_trim(skb, 0);\n\tskb_reserve(skb, sizeof(*req));\n\tskb_put(skb, mpalen);\n\tskb->priority = CPL_PRIORITY_DATA;\n\tmpa = (struct mpa_message *) skb->data;\n\tmemset(mpa, 0, sizeof(*mpa));\n\tmemcpy(mpa->key, MPA_KEY_REQ, sizeof(mpa->key));\n\tmpa->flags = (crc_enabled ? MPA_CRC : 0) |\n\t\t     (markers_enabled ? MPA_MARKERS : 0);\n\tmpa->private_data_size = htons(ep->plen);\n\tmpa->revision = mpa_rev;\n\n\tif (ep->plen)\n\t\tmemcpy(mpa->private_data, ep->mpa_pkt + sizeof(*mpa), ep->plen);\n\n\t/*\n\t * Reference the mpa skb.  This ensures the data area\n\t * will remain in memory until the hw acks the tx.\n\t * Function tx_ack() will deref it.\n\t */\n\tskb_get(skb);\n\tset_arp_failure_handler(skb, arp_failure_discard);\n\tskb_reset_transport_header(skb);\n\tlen = skb->len;\n\treq = (struct tx_data_wr *) skb_push(skb, sizeof(*req));\n\treq->wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_TX_DATA)|F_WR_COMPL);\n\treq->wr_lo = htonl(V_WR_TID(ep->hwtid));\n\treq->len = htonl(len);\n\treq->param = htonl(V_TX_PORT(ep->l2t->smt_idx) |\n\t\t\t   V_TX_SNDBUF(snd_win>>15));\n\treq->flags = htonl(F_TX_INIT);\n\treq->sndseq = htonl(ep->snd_seq);\n\tBUG_ON(ep->mpa_skb);\n\tep->mpa_skb = skb;\n\tiwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n\tstart_ep_timer(ep);\n\tstate_set(&ep->com, MPA_REQ_SENT);\n\treturn;\n}\n\nstatic int send_mpa_reject(struct iwch_ep *ep, const void *pdata, u8 plen)\n{\n\tint mpalen;\n\tstruct tx_data_wr *req;\n\tstruct mpa_message *mpa;\n\tstruct sk_buff *skb;\n\n\tPDBG(\"%s ep %p plen %d\\n\", __func__, ep, plen);\n\n\tmpalen = sizeof(*mpa) + plen;\n\n\tskb = get_skb(NULL, mpalen + sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc skb!\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tskb_reserve(skb, sizeof(*req));\n\tmpa = (struct mpa_message *) skb_put(skb, mpalen);\n\tmemset(mpa, 0, sizeof(*mpa));\n\tmemcpy(mpa->key, MPA_KEY_REP, sizeof(mpa->key));\n\tmpa->flags = MPA_REJECT;\n\tmpa->revision = mpa_rev;\n\tmpa->private_data_size = htons(plen);\n\tif (plen)\n\t\tmemcpy(mpa->private_data, pdata, plen);\n\n\t/*\n\t * Reference the mpa skb again.  This ensures the data area\n\t * will remain in memory until the hw acks the tx.\n\t * Function tx_ack() will deref it.\n\t */\n\tskb_get(skb);\n\tskb->priority = CPL_PRIORITY_DATA;\n\tset_arp_failure_handler(skb, arp_failure_discard);\n\tskb_reset_transport_header(skb);\n\treq = (struct tx_data_wr *) skb_push(skb, sizeof(*req));\n\treq->wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_TX_DATA)|F_WR_COMPL);\n\treq->wr_lo = htonl(V_WR_TID(ep->hwtid));\n\treq->len = htonl(mpalen);\n\treq->param = htonl(V_TX_PORT(ep->l2t->smt_idx) |\n\t\t\t   V_TX_SNDBUF(snd_win>>15));\n\treq->flags = htonl(F_TX_INIT);\n\treq->sndseq = htonl(ep->snd_seq);\n\tBUG_ON(ep->mpa_skb);\n\tep->mpa_skb = skb;\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic int send_mpa_reply(struct iwch_ep *ep, const void *pdata, u8 plen)\n{\n\tint mpalen;\n\tstruct tx_data_wr *req;\n\tstruct mpa_message *mpa;\n\tint len;\n\tstruct sk_buff *skb;\n\n\tPDBG(\"%s ep %p plen %d\\n\", __func__, ep, plen);\n\n\tmpalen = sizeof(*mpa) + plen;\n\n\tskb = get_skb(NULL, mpalen + sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc skb!\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tskb->priority = CPL_PRIORITY_DATA;\n\tskb_reserve(skb, sizeof(*req));\n\tmpa = (struct mpa_message *) skb_put(skb, mpalen);\n\tmemset(mpa, 0, sizeof(*mpa));\n\tmemcpy(mpa->key, MPA_KEY_REP, sizeof(mpa->key));\n\tmpa->flags = (ep->mpa_attr.crc_enabled ? MPA_CRC : 0) |\n\t\t     (markers_enabled ? MPA_MARKERS : 0);\n\tmpa->revision = mpa_rev;\n\tmpa->private_data_size = htons(plen);\n\tif (plen)\n\t\tmemcpy(mpa->private_data, pdata, plen);\n\n\t/*\n\t * Reference the mpa skb.  This ensures the data area\n\t * will remain in memory until the hw acks the tx.\n\t * Function tx_ack() will deref it.\n\t */\n\tskb_get(skb);\n\tset_arp_failure_handler(skb, arp_failure_discard);\n\tskb_reset_transport_header(skb);\n\tlen = skb->len;\n\treq = (struct tx_data_wr *) skb_push(skb, sizeof(*req));\n\treq->wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_TX_DATA)|F_WR_COMPL);\n\treq->wr_lo = htonl(V_WR_TID(ep->hwtid));\n\treq->len = htonl(len);\n\treq->param = htonl(V_TX_PORT(ep->l2t->smt_idx) |\n\t\t\t   V_TX_SNDBUF(snd_win>>15));\n\treq->flags = htonl(F_TX_INIT);\n\treq->sndseq = htonl(ep->snd_seq);\n\tep->mpa_skb = skb;\n\tstate_set(&ep->com, MPA_REP_SENT);\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic int act_establish(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_act_establish *req = cplhdr(skb);\n\tunsigned int tid = GET_TID(req);\n\n\tPDBG(\"%s ep %p tid %d\\n\", __func__, ep, tid);\n\n\tdst_confirm(ep->dst);\n\n\t/* setup the hwtid for this connection */\n\tep->hwtid = tid;\n\tcxgb3_insert_tid(ep->com.tdev, &t3c_client, ep, tid);\n\n\tep->snd_seq = ntohl(req->snd_isn);\n\tep->rcv_seq = ntohl(req->rcv_isn);\n\n\tset_emss(ep, ntohs(req->tcp_opt));\n\n\t/* dealloc the atid */\n\tcxgb3_free_atid(ep->com.tdev, ep->atid);\n\n\t/* start MPA negotiation */\n\tsend_mpa_req(ep, skb);\n\n\treturn 0;\n}\n\nstatic void abort_connection(struct iwch_ep *ep, struct sk_buff *skb, gfp_t gfp)\n{\n\tPDBG(\"%s ep %p\\n\", __FILE__, ep);\n\tstate_set(&ep->com, ABORTING);\n\tsend_abort(ep, skb, gfp);\n}\n\nstatic void close_complete_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_CLOSE;\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"close complete delivered ep %p cm_id %p tid %d\\n\",\n\t\t     ep, ep->com.cm_id, ep->hwtid);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t\tep->com.cm_id->rem_ref(ep->com.cm_id);\n\t\tep->com.cm_id = NULL;\n\t\tep->com.qp = NULL;\n\t}\n}\n\nstatic void peer_close_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_DISCONNECT;\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"peer close delivered ep %p cm_id %p tid %d\\n\",\n\t\t     ep, ep->com.cm_id, ep->hwtid);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t}\n}\n\nstatic void peer_abort_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_CLOSE;\n\tevent.status = -ECONNRESET;\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"abort delivered ep %p cm_id %p tid %d\\n\", ep,\n\t\t     ep->com.cm_id, ep->hwtid);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t\tep->com.cm_id->rem_ref(ep->com.cm_id);\n\t\tep->com.cm_id = NULL;\n\t\tep->com.qp = NULL;\n\t}\n}\n\nstatic void connect_reply_upcall(struct iwch_ep *ep, int status)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p status %d\\n\", __func__, ep, status);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_CONNECT_REPLY;\n\tevent.status = status;\n\tmemcpy(&event.local_addr, &ep->com.local_addr,\n\t       sizeof(ep->com.local_addr));\n\tmemcpy(&event.remote_addr, &ep->com.remote_addr,\n\t       sizeof(ep->com.remote_addr));\n\n\tif ((status == 0) || (status == -ECONNREFUSED)) {\n\t\tevent.private_data_len = ep->plen;\n\t\tevent.private_data = ep->mpa_pkt + sizeof(struct mpa_message);\n\t}\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"%s ep %p tid %d status %d\\n\", __func__, ep,\n\t\t     ep->hwtid, status);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t}\n\tif (status < 0) {\n\t\tep->com.cm_id->rem_ref(ep->com.cm_id);\n\t\tep->com.cm_id = NULL;\n\t\tep->com.qp = NULL;\n\t}\n}\n\nstatic void connect_request_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p tid %d\\n\", __func__, ep, ep->hwtid);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_CONNECT_REQUEST;\n\tmemcpy(&event.local_addr, &ep->com.local_addr,\n\t       sizeof(ep->com.local_addr));\n\tmemcpy(&event.remote_addr, &ep->com.remote_addr,\n\t       sizeof(ep->com.local_addr));\n\tevent.private_data_len = ep->plen;\n\tevent.private_data = ep->mpa_pkt + sizeof(struct mpa_message);\n\tevent.provider_data = ep;\n\t/*\n\t * Until ird/ord negotiation via MPAv2 support is added, send max\n\t * supported values\n\t */\n\tevent.ird = event.ord = 8;\n\tif (state_read(&ep->parent_ep->com) != DEAD) {\n\t\tget_ep(&ep->com);\n\t\tep->parent_ep->com.cm_id->event_handler(\n\t\t\t\t\t\tep->parent_ep->com.cm_id,\n\t\t\t\t\t\t&event);\n\t}\n\tput_ep(&ep->parent_ep->com);\n\tep->parent_ep = NULL;\n}\n\nstatic void established_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_ESTABLISHED;\n\t/*\n\t * Until ird/ord negotiation via MPAv2 support is added, send max\n\t * supported values\n\t */\n\tevent.ird = event.ord = 8;\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"%s ep %p tid %d\\n\", __func__, ep, ep->hwtid);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t}\n}\n\nstatic int update_rx_credits(struct iwch_ep *ep, u32 credits)\n{\n\tstruct cpl_rx_data_ack *req;\n\tstruct sk_buff *skb;\n\n\tPDBG(\"%s ep %p credits %u\\n\", __func__, ep, credits);\n\tskb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"update_rx_credits - cannot alloc skb!\\n\");\n\t\treturn 0;\n\t}\n\n\treq = (struct cpl_rx_data_ack *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_RX_DATA_ACK, ep->hwtid));\n\treq->credit_dack = htonl(V_RX_CREDITS(credits) | V_RX_FORCE_ACK(1));\n\tskb->priority = CPL_PRIORITY_ACK;\n\tiwch_cxgb3_ofld_send(ep->com.tdev, skb);\n\treturn credits;\n}\n\nstatic void process_mpa_reply(struct iwch_ep *ep, struct sk_buff *skb)\n{\n\tstruct mpa_message *mpa;\n\tu16 plen;\n\tstruct iwch_qp_attributes attrs;\n\tenum iwch_qp_attr_mask mask;\n\tint err;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\n\t/*\n\t * Stop mpa timer.  If it expired, then the state has\n\t * changed and we bail since ep_timeout already aborted\n\t * the connection.\n\t */\n\tstop_ep_timer(ep);\n\tif (state_read(&ep->com) != MPA_REQ_SENT)\n\t\treturn;\n\n\t/*\n\t * If we get more than the supported amount of private data\n\t * then we must fail this connection.\n\t */\n\tif (ep->mpa_pkt_len + skb->len > sizeof(ep->mpa_pkt)) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t/*\n\t * copy the new data into our accumulation buffer.\n\t */\n\tskb_copy_from_linear_data(skb, &(ep->mpa_pkt[ep->mpa_pkt_len]),\n\t\t\t\t  skb->len);\n\tep->mpa_pkt_len += skb->len;\n\n\t/*\n\t * if we don't even have the mpa message, then bail.\n\t */\n\tif (ep->mpa_pkt_len < sizeof(*mpa))\n\t\treturn;\n\tmpa = (struct mpa_message *) ep->mpa_pkt;\n\n\t/* Validate MPA header. */\n\tif (mpa->revision != mpa_rev) {\n\t\terr = -EPROTO;\n\t\tgoto err;\n\t}\n\tif (memcmp(mpa->key, MPA_KEY_REP, sizeof(mpa->key))) {\n\t\terr = -EPROTO;\n\t\tgoto err;\n\t}\n\n\tplen = ntohs(mpa->private_data_size);\n\n\t/*\n\t * Fail if there's too much private data.\n\t */\n\tif (plen > MPA_MAX_PRIVATE_DATA) {\n\t\terr = -EPROTO;\n\t\tgoto err;\n\t}\n\n\t/*\n\t * If plen does not account for pkt size\n\t */\n\tif (ep->mpa_pkt_len > (sizeof(*mpa) + plen)) {\n\t\terr = -EPROTO;\n\t\tgoto err;\n\t}\n\n\tep->plen = (u8) plen;\n\n\t/*\n\t * If we don't have all the pdata yet, then bail.\n\t * We'll continue process when more data arrives.\n\t */\n\tif (ep->mpa_pkt_len < (sizeof(*mpa) + plen))\n\t\treturn;\n\n\tif (mpa->flags & MPA_REJECT) {\n\t\terr = -ECONNREFUSED;\n\t\tgoto err;\n\t}\n\n\t/*\n\t * If we get here we have accumulated the entire mpa\n\t * start reply message including private data. And\n\t * the MPA header is valid.\n\t */\n\tstate_set(&ep->com, FPDU_MODE);\n\tep->mpa_attr.initiator = 1;\n\tep->mpa_attr.crc_enabled = (mpa->flags & MPA_CRC) | crc_enabled ? 1 : 0;\n\tep->mpa_attr.recv_marker_enabled = markers_enabled;\n\tep->mpa_attr.xmit_marker_enabled = mpa->flags & MPA_MARKERS ? 1 : 0;\n\tep->mpa_attr.version = mpa_rev;\n\tPDBG(\"%s - crc_enabled=%d, recv_marker_enabled=%d, \"\n\t     \"xmit_marker_enabled=%d, version=%d\\n\", __func__,\n\t     ep->mpa_attr.crc_enabled, ep->mpa_attr.recv_marker_enabled,\n\t     ep->mpa_attr.xmit_marker_enabled, ep->mpa_attr.version);\n\n\tattrs.mpa_attr = ep->mpa_attr;\n\tattrs.max_ird = ep->ird;\n\tattrs.max_ord = ep->ord;\n\tattrs.llp_stream_handle = ep;\n\tattrs.next_state = IWCH_QP_STATE_RTS;\n\n\tmask = IWCH_QP_ATTR_NEXT_STATE |\n\t    IWCH_QP_ATTR_LLP_STREAM_HANDLE | IWCH_QP_ATTR_MPA_ATTR |\n\t    IWCH_QP_ATTR_MAX_IRD | IWCH_QP_ATTR_MAX_ORD;\n\n\t/* bind QP and TID with INIT_WR */\n\terr = iwch_modify_qp(ep->com.qp->rhp,\n\t\t\t     ep->com.qp, mask, &attrs, 1);\n\tif (err)\n\t\tgoto err;\n\n\tif (peer2peer && iwch_rqes_posted(ep->com.qp) == 0) {\n\t\tiwch_post_zb_read(ep);\n\t}\n\n\tgoto out;\nerr:\n\tabort_connection(ep, skb, GFP_KERNEL);\nout:\n\tconnect_reply_upcall(ep, err);\n\treturn;\n}\n\nstatic void process_mpa_request(struct iwch_ep *ep, struct sk_buff *skb)\n{\n\tstruct mpa_message *mpa;\n\tu16 plen;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\n\t/*\n\t * Stop mpa timer.  If it expired, then the state has\n\t * changed and we bail since ep_timeout already aborted\n\t * the connection.\n\t */\n\tstop_ep_timer(ep);\n\tif (state_read(&ep->com) != MPA_REQ_WAIT)\n\t\treturn;\n\n\t/*\n\t * If we get more than the supported amount of private data\n\t * then we must fail this connection.\n\t */\n\tif (ep->mpa_pkt_len + skb->len > sizeof(ep->mpa_pkt)) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\n\tPDBG(\"%s enter (%s line %u)\\n\", __func__, __FILE__, __LINE__);\n\n\t/*\n\t * Copy the new data into our accumulation buffer.\n\t */\n\tskb_copy_from_linear_data(skb, &(ep->mpa_pkt[ep->mpa_pkt_len]),\n\t\t\t\t  skb->len);\n\tep->mpa_pkt_len += skb->len;\n\n\t/*\n\t * If we don't even have the mpa message, then bail.\n\t * We'll continue process when more data arrives.\n\t */\n\tif (ep->mpa_pkt_len < sizeof(*mpa))\n\t\treturn;\n\tPDBG(\"%s enter (%s line %u)\\n\", __func__, __FILE__, __LINE__);\n\tmpa = (struct mpa_message *) ep->mpa_pkt;\n\n\t/*\n\t * Validate MPA Header.\n\t */\n\tif (mpa->revision != mpa_rev) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\n\tif (memcmp(mpa->key, MPA_KEY_REQ, sizeof(mpa->key))) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\n\tplen = ntohs(mpa->private_data_size);\n\n\t/*\n\t * Fail if there's too much private data.\n\t */\n\tif (plen > MPA_MAX_PRIVATE_DATA) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\n\t/*\n\t * If plen does not account for pkt size\n\t */\n\tif (ep->mpa_pkt_len > (sizeof(*mpa) + plen)) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\tep->plen = (u8) plen;\n\n\t/*\n\t * If we don't have all the pdata yet, then bail.\n\t */\n\tif (ep->mpa_pkt_len < (sizeof(*mpa) + plen))\n\t\treturn;\n\n\t/*\n\t * If we get here we have accumulated the entire mpa\n\t * start reply message including private data.\n\t */\n\tep->mpa_attr.initiator = 0;\n\tep->mpa_attr.crc_enabled = (mpa->flags & MPA_CRC) | crc_enabled ? 1 : 0;\n\tep->mpa_attr.recv_marker_enabled = markers_enabled;\n\tep->mpa_attr.xmit_marker_enabled = mpa->flags & MPA_MARKERS ? 1 : 0;\n\tep->mpa_attr.version = mpa_rev;\n\tPDBG(\"%s - crc_enabled=%d, recv_marker_enabled=%d, \"\n\t     \"xmit_marker_enabled=%d, version=%d\\n\", __func__,\n\t     ep->mpa_attr.crc_enabled, ep->mpa_attr.recv_marker_enabled,\n\t     ep->mpa_attr.xmit_marker_enabled, ep->mpa_attr.version);\n\n\tstate_set(&ep->com, MPA_REQ_RCVD);\n\n\t/* drive upcall */\n\tconnect_request_upcall(ep);\n\treturn;\n}\n\nstatic int rx_data(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_rx_data *hdr = cplhdr(skb);\n\tunsigned int dlen = ntohs(hdr->len);\n\n\tPDBG(\"%s ep %p dlen %u\\n\", __func__, ep, dlen);\n\n\tskb_pull(skb, sizeof(*hdr));\n\tskb_trim(skb, dlen);\n\n\tep->rcv_seq += dlen;\n\tBUG_ON(ep->rcv_seq != (ntohl(hdr->seq) + dlen));\n\n\tswitch (state_read(&ep->com)) {\n\tcase MPA_REQ_SENT:\n\t\tprocess_mpa_reply(ep, skb);\n\t\tbreak;\n\tcase MPA_REQ_WAIT:\n\t\tprocess_mpa_request(ep, skb);\n\t\tbreak;\n\tcase MPA_REP_SENT:\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR MOD \"%s Unexpected streaming data.\"\n\t\t       \" ep %p state %d tid %d\\n\",\n\t\t       __func__, ep, state_read(&ep->com), ep->hwtid);\n\n\t\t/*\n\t\t * The ep will timeout and inform the ULP of the failure.\n\t\t * See ep_timeout().\n\t\t */\n\t\tbreak;\n\t}\n\n\t/* update RX credits */\n\tupdate_rx_credits(ep, dlen);\n\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * Upcall from the adapter indicating data has been transmitted.\n * For us its just the single MPA request or reply.  We can now free\n * the skb holding the mpa message.\n */\nstatic int tx_ack(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_wr_ack *hdr = cplhdr(skb);\n\tunsigned int credits = ntohs(hdr->credits);\n\tunsigned long flags;\n\tint post_zb = 0;\n\n\tPDBG(\"%s ep %p credits %u\\n\", __func__, ep, credits);\n\n\tif (credits == 0) {\n\t\tPDBG(\"%s 0 credit ack  ep %p state %u\\n\",\n\t\t     __func__, ep, state_read(&ep->com));\n\t\treturn CPL_RET_BUF_DONE;\n\t}\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tBUG_ON(credits != 1);\n\tdst_confirm(ep->dst);\n\tif (!ep->mpa_skb) {\n\t\tPDBG(\"%s rdma_init wr_ack ep %p state %u\\n\",\n\t\t\t__func__, ep, ep->com.state);\n\t\tif (ep->mpa_attr.initiator) {\n\t\t\tPDBG(\"%s initiator ep %p state %u\\n\",\n\t\t\t\t__func__, ep, ep->com.state);\n\t\t\tif (peer2peer && ep->com.state == FPDU_MODE)\n\t\t\t\tpost_zb = 1;\n\t\t} else {\n\t\t\tPDBG(\"%s responder ep %p state %u\\n\",\n\t\t\t\t__func__, ep, ep->com.state);\n\t\t\tif (ep->com.state == MPA_REQ_RCVD) {\n\t\t\t\tep->com.rpl_done = 1;\n\t\t\t\twake_up(&ep->com.waitq);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tPDBG(\"%s lsm ack ep %p state %u freeing skb\\n\",\n\t\t\t__func__, ep, ep->com.state);\n\t\tkfree_skb(ep->mpa_skb);\n\t\tep->mpa_skb = NULL;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (post_zb)\n\t\tiwch_post_zb_read(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int abort_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tunsigned long flags;\n\tint release = 0;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tBUG_ON(!ep);\n\n\t/*\n\t * We get 2 abort replies from the HW.  The first one must\n\t * be ignored except for scribbling that we need one more.\n\t */\n\tif (!test_and_set_bit(ABORT_REQ_IN_PROGRESS, &ep->com.flags)) {\n\t\treturn CPL_RET_BUF_DONE;\n\t}\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tswitch (ep->com.state) {\n\tcase ABORTING:\n\t\tclose_complete_upcall(ep);\n\t\t__state_set(&ep->com, DEAD);\n\t\trelease = 1;\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"%s ep %p state %d\\n\",\n\t\t     __func__, ep, ep->com.state);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\n\tif (release)\n\t\trelease_ep_resources(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * Return whether a failed active open has allocated a TID\n */\nstatic inline int act_open_has_tid(int status)\n{\n\treturn status != CPL_ERR_TCAM_FULL && status != CPL_ERR_CONN_EXIST &&\n\t       status != CPL_ERR_ARP_MISS;\n}\n\nstatic int act_open_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_act_open_rpl *rpl = cplhdr(skb);\n\n\tPDBG(\"%s ep %p status %u errno %d\\n\", __func__, ep, rpl->status,\n\t     status2errno(rpl->status));\n\tconnect_reply_upcall(ep, status2errno(rpl->status));\n\tstate_set(&ep->com, DEAD);\n\tif (ep->com.tdev->type != T3A && act_open_has_tid(rpl->status))\n\t\trelease_tid(ep->com.tdev, GET_TID(rpl), NULL);\n\tcxgb3_free_atid(ep->com.tdev, ep->atid);\n\tdst_release(ep->dst);\n\tl2t_release(ep->com.tdev, ep->l2t);\n\tput_ep(&ep->com);\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int listen_start(struct iwch_listen_ep *ep)\n{\n\tstruct sk_buff *skb;\n\tstruct cpl_pass_open_req *req;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"t3c_listen_start failed to alloc skb!\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treq = (struct cpl_pass_open_req *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_PASS_OPEN_REQ, ep->stid));\n\treq->local_port = ep->com.local_addr.sin_port;\n\treq->local_ip = ep->com.local_addr.sin_addr.s_addr;\n\treq->peer_port = 0;\n\treq->peer_ip = 0;\n\treq->peer_netmask = 0;\n\treq->opt0h = htonl(F_DELACK | F_TCAM_BYPASS);\n\treq->opt0l = htonl(V_RCV_BUFSIZ(rcv_win>>10));\n\treq->opt1 = htonl(V_CONN_POLICY(CPL_CONN_POLICY_ASK));\n\n\tskb->priority = 1;\n\treturn iwch_cxgb3_ofld_send(ep->com.tdev, skb);\n}\n\nstatic int pass_open_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_listen_ep *ep = ctx;\n\tstruct cpl_pass_open_rpl *rpl = cplhdr(skb);\n\n\tPDBG(\"%s ep %p status %d error %d\\n\", __func__, ep,\n\t     rpl->status, status2errno(rpl->status));\n\tep->com.rpl_err = status2errno(rpl->status);\n\tep->com.rpl_done = 1;\n\twake_up(&ep->com.waitq);\n\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int listen_stop(struct iwch_listen_ep *ep)\n{\n\tstruct sk_buff *skb;\n\tstruct cpl_close_listserv_req *req;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to alloc skb\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\treq = (struct cpl_close_listserv_req *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\treq->cpu_idx = 0;\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_CLOSE_LISTSRV_REQ, ep->stid));\n\tskb->priority = 1;\n\treturn iwch_cxgb3_ofld_send(ep->com.tdev, skb);\n}\n\nstatic int close_listsrv_rpl(struct t3cdev *tdev, struct sk_buff *skb,\n\t\t\t     void *ctx)\n{\n\tstruct iwch_listen_ep *ep = ctx;\n\tstruct cpl_close_listserv_rpl *rpl = cplhdr(skb);\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tep->com.rpl_err = status2errno(rpl->status);\n\tep->com.rpl_done = 1;\n\twake_up(&ep->com.waitq);\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic void accept_cr(struct iwch_ep *ep, __be32 peer_ip, struct sk_buff *skb)\n{\n\tstruct cpl_pass_accept_rpl *rpl;\n\tunsigned int mtu_idx;\n\tu32 opt0h, opt0l, opt2;\n\tint wscale;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tBUG_ON(skb_cloned(skb));\n\tskb_trim(skb, sizeof(*rpl));\n\tskb_get(skb);\n\tmtu_idx = find_best_mtu(T3C_DATA(ep->com.tdev), dst_mtu(ep->dst));\n\twscale = compute_wscale(rcv_win);\n\topt0h = V_NAGLE(0) |\n\t    V_NO_CONG(nocong) |\n\t    V_KEEP_ALIVE(1) |\n\t    F_TCAM_BYPASS |\n\t    V_WND_SCALE(wscale) |\n\t    V_MSS_IDX(mtu_idx) |\n\t    V_L2T_IDX(ep->l2t->idx) | V_TX_CHANNEL(ep->l2t->smt_idx);\n\topt0l = V_TOS((ep->tos >> 2) & M_TOS) | V_RCV_BUFSIZ(rcv_win>>10);\n\topt2 = F_RX_COALESCE_VALID | V_RX_COALESCE(0) | V_FLAVORS_VALID(1) |\n\t       V_CONG_CONTROL_FLAVOR(cong_flavor);\n\n\trpl = cplhdr(skb);\n\trpl->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(rpl) = htonl(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL, ep->hwtid));\n\trpl->peer_ip = peer_ip;\n\trpl->opt0h = htonl(opt0h);\n\trpl->opt0l_status = htonl(opt0l | CPL_PASS_OPEN_ACCEPT);\n\trpl->opt2 = htonl(opt2);\n\trpl->rsvd = rpl->opt2;\t/* workaround for HW bug */\n\tskb->priority = CPL_PRIORITY_SETUP;\n\tiwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n\n\treturn;\n}\n\nstatic void reject_cr(struct t3cdev *tdev, u32 hwtid, __be32 peer_ip,\n\t\t      struct sk_buff *skb)\n{\n\tPDBG(\"%s t3cdev %p tid %u peer_ip %x\\n\", __func__, tdev, hwtid,\n\t     peer_ip);\n\tBUG_ON(skb_cloned(skb));\n\tskb_trim(skb, sizeof(struct cpl_tid_release));\n\tskb_get(skb);\n\n\tif (tdev->type != T3A)\n\t\trelease_tid(tdev, hwtid, skb);\n\telse {\n\t\tstruct cpl_pass_accept_rpl *rpl;\n\n\t\trpl = cplhdr(skb);\n\t\tskb->priority = CPL_PRIORITY_SETUP;\n\t\trpl->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\t\tOPCODE_TID(rpl) = htonl(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL,\n\t\t\t\t\t\t      hwtid));\n\t\trpl->peer_ip = peer_ip;\n\t\trpl->opt0h = htonl(F_TCAM_BYPASS);\n\t\trpl->opt0l_status = htonl(CPL_PASS_OPEN_REJECT);\n\t\trpl->opt2 = 0;\n\t\trpl->rsvd = rpl->opt2;\n\t\tiwch_cxgb3_ofld_send(tdev, skb);\n\t}\n}\n\nstatic int pass_accept_req(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *child_ep, *parent_ep = ctx;\n\tstruct cpl_pass_accept_req *req = cplhdr(skb);\n\tunsigned int hwtid = GET_TID(req);\n\tstruct dst_entry *dst;\n\tstruct l2t_entry *l2t;\n\tstruct rtable *rt;\n\tstruct iff_mac tim;\n\n\tPDBG(\"%s parent ep %p tid %u\\n\", __func__, parent_ep, hwtid);\n\n\tif (state_read(&parent_ep->com) != LISTEN) {\n\t\tprintk(KERN_ERR \"%s - listening ep not in LISTEN\\n\",\n\t\t       __func__);\n\t\tgoto reject;\n\t}\n\n\t/*\n\t * Find the netdev for this connection request.\n\t */\n\ttim.mac_addr = req->dst_mac;\n\ttim.vlan_tag = ntohs(req->vlan_tag);\n\tif (tdev->ctl(tdev, GET_IFF_FROM_MAC, &tim) < 0 || !tim.dev) {\n\t\tprintk(KERN_ERR \"%s bad dst mac %pM\\n\",\n\t\t\t__func__, req->dst_mac);\n\t\tgoto reject;\n\t}\n\n\t/* Find output route */\n\trt = find_route(tdev,\n\t\t\treq->local_ip,\n\t\t\treq->peer_ip,\n\t\t\treq->local_port,\n\t\t\treq->peer_port, G_PASS_OPEN_TOS(ntohl(req->tos_tid)));\n\tif (!rt) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to find dst entry!\\n\",\n\t\t       __func__);\n\t\tgoto reject;\n\t}\n\tdst = &rt->dst;\n\tl2t = t3_l2t_get(tdev, dst, NULL, &req->peer_ip);\n\tif (!l2t) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to allocate l2t entry!\\n\",\n\t\t       __func__);\n\t\tdst_release(dst);\n\t\tgoto reject;\n\t}\n\tchild_ep = alloc_ep(sizeof(*child_ep), GFP_KERNEL);\n\tif (!child_ep) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to allocate ep entry!\\n\",\n\t\t       __func__);\n\t\tl2t_release(tdev, l2t);\n\t\tdst_release(dst);\n\t\tgoto reject;\n\t}\n\tstate_set(&child_ep->com, CONNECTING);\n\tchild_ep->com.tdev = tdev;\n\tchild_ep->com.cm_id = NULL;\n\tchild_ep->com.local_addr.sin_family = PF_INET;\n\tchild_ep->com.local_addr.sin_port = req->local_port;\n\tchild_ep->com.local_addr.sin_addr.s_addr = req->local_ip;\n\tchild_ep->com.remote_addr.sin_family = PF_INET;\n\tchild_ep->com.remote_addr.sin_port = req->peer_port;\n\tchild_ep->com.remote_addr.sin_addr.s_addr = req->peer_ip;\n\tget_ep(&parent_ep->com);\n\tchild_ep->parent_ep = parent_ep;\n\tchild_ep->tos = G_PASS_OPEN_TOS(ntohl(req->tos_tid));\n\tchild_ep->l2t = l2t;\n\tchild_ep->dst = dst;\n\tchild_ep->hwtid = hwtid;\n\tinit_timer(&child_ep->timer);\n\tcxgb3_insert_tid(tdev, &t3c_client, child_ep, hwtid);\n\taccept_cr(child_ep, req->peer_ip, skb);\n\tgoto out;\nreject:\n\treject_cr(tdev, hwtid, req->peer_ip, skb);\nout:\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int pass_establish(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_pass_establish *req = cplhdr(skb);\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tep->snd_seq = ntohl(req->snd_isn);\n\tep->rcv_seq = ntohl(req->rcv_isn);\n\n\tset_emss(ep, ntohs(req->tcp_opt));\n\n\tdst_confirm(ep->dst);\n\tstate_set(&ep->com, MPA_REQ_WAIT);\n\tstart_ep_timer(ep);\n\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int peer_close(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct iwch_qp_attributes attrs;\n\tunsigned long flags;\n\tint disconnect = 1;\n\tint release = 0;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tdst_confirm(ep->dst);\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tswitch (ep->com.state) {\n\tcase MPA_REQ_WAIT:\n\t\t__state_set(&ep->com, CLOSING);\n\t\tbreak;\n\tcase MPA_REQ_SENT:\n\t\t__state_set(&ep->com, CLOSING);\n\t\tconnect_reply_upcall(ep, -ECONNRESET);\n\t\tbreak;\n\tcase MPA_REQ_RCVD:\n\n\t\t/*\n\t\t * We're gonna mark this puppy DEAD, but keep\n\t\t * the reference on it until the ULP accepts or\n\t\t * rejects the CR. Also wake up anyone waiting\n\t\t * in rdma connection migration (see iwch_accept_cr()).\n\t\t */\n\t\t__state_set(&ep->com, CLOSING);\n\t\tep->com.rpl_done = 1;\n\t\tep->com.rpl_err = -ECONNRESET;\n\t\tPDBG(\"waking up ep %p\\n\", ep);\n\t\twake_up(&ep->com.waitq);\n\t\tbreak;\n\tcase MPA_REP_SENT:\n\t\t__state_set(&ep->com, CLOSING);\n\t\tep->com.rpl_done = 1;\n\t\tep->com.rpl_err = -ECONNRESET;\n\t\tPDBG(\"waking up ep %p\\n\", ep);\n\t\twake_up(&ep->com.waitq);\n\t\tbreak;\n\tcase FPDU_MODE:\n\t\tstart_ep_timer(ep);\n\t\t__state_set(&ep->com, CLOSING);\n\t\tattrs.next_state = IWCH_QP_STATE_CLOSING;\n\t\tiwch_modify_qp(ep->com.qp->rhp, ep->com.qp,\n\t\t\t       IWCH_QP_ATTR_NEXT_STATE, &attrs, 1);\n\t\tpeer_close_upcall(ep);\n\t\tbreak;\n\tcase ABORTING:\n\t\tdisconnect = 0;\n\t\tbreak;\n\tcase CLOSING:\n\t\t__state_set(&ep->com, MORIBUND);\n\t\tdisconnect = 0;\n\t\tbreak;\n\tcase MORIBUND:\n\t\tstop_ep_timer(ep);\n\t\tif (ep->com.cm_id && ep->com.qp) {\n\t\t\tattrs.next_state = IWCH_QP_STATE_IDLE;\n\t\t\tiwch_modify_qp(ep->com.qp->rhp, ep->com.qp,\n\t\t\t\t       IWCH_QP_ATTR_NEXT_STATE, &attrs, 1);\n\t\t}\n\t\tclose_complete_upcall(ep);\n\t\t__state_set(&ep->com, DEAD);\n\t\trelease = 1;\n\t\tdisconnect = 0;\n\t\tbreak;\n\tcase DEAD:\n\t\tdisconnect = 0;\n\t\tbreak;\n\tdefault:\n\t\tBUG_ON(1);\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (disconnect)\n\t\tiwch_ep_disconnect(ep, 0, GFP_KERNEL);\n\tif (release)\n\t\trelease_ep_resources(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * Returns whether an ABORT_REQ_RSS message is a negative advice.\n */\nstatic int is_neg_adv_abort(unsigned int status)\n{\n\treturn status == CPL_ERR_RTX_NEG_ADVICE ||\n\t       status == CPL_ERR_PERSIST_NEG_ADVICE;\n}\n\nstatic int peer_abort(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct cpl_abort_req_rss *req = cplhdr(skb);\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_abort_rpl *rpl;\n\tstruct sk_buff *rpl_skb;\n\tstruct iwch_qp_attributes attrs;\n\tint ret;\n\tint release = 0;\n\tunsigned long flags;\n\n\tif (is_neg_adv_abort(req->status)) {\n\t\tPDBG(\"%s neg_adv_abort ep %p tid %d\\n\", __func__, ep,\n\t\t     ep->hwtid);\n\t\tt3_l2t_send_event(ep->com.tdev, ep->l2t);\n\t\treturn CPL_RET_BUF_DONE;\n\t}\n\n\t/*\n\t * We get 2 peer aborts from the HW.  The first one must\n\t * be ignored except for scribbling that we need one more.\n\t */\n\tif (!test_and_set_bit(PEER_ABORT_IN_PROGRESS, &ep->com.flags)) {\n\t\treturn CPL_RET_BUF_DONE;\n\t}\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tPDBG(\"%s ep %p state %u\\n\", __func__, ep, ep->com.state);\n\tswitch (ep->com.state) {\n\tcase CONNECTING:\n\t\tbreak;\n\tcase MPA_REQ_WAIT:\n\t\tstop_ep_timer(ep);\n\t\tbreak;\n\tcase MPA_REQ_SENT:\n\t\tstop_ep_timer(ep);\n\t\tconnect_reply_upcall(ep, -ECONNRESET);\n\t\tbreak;\n\tcase MPA_REP_SENT:\n\t\tep->com.rpl_done = 1;\n\t\tep->com.rpl_err = -ECONNRESET;\n\t\tPDBG(\"waking up ep %p\\n\", ep);\n\t\twake_up(&ep->com.waitq);\n\t\tbreak;\n\tcase MPA_REQ_RCVD:\n\n\t\t/*\n\t\t * We're gonna mark this puppy DEAD, but keep\n\t\t * the reference on it until the ULP accepts or\n\t\t * rejects the CR. Also wake up anyone waiting\n\t\t * in rdma connection migration (see iwch_accept_cr()).\n\t\t */\n\t\tep->com.rpl_done = 1;\n\t\tep->com.rpl_err = -ECONNRESET;\n\t\tPDBG(\"waking up ep %p\\n\", ep);\n\t\twake_up(&ep->com.waitq);\n\t\tbreak;\n\tcase MORIBUND:\n\tcase CLOSING:\n\t\tstop_ep_timer(ep);\n\t\t/*FALLTHROUGH*/\n\tcase FPDU_MODE:\n\t\tif (ep->com.cm_id && ep->com.qp) {\n\t\t\tattrs.next_state = IWCH_QP_STATE_ERROR;\n\t\t\tret = iwch_modify_qp(ep->com.qp->rhp,\n\t\t\t\t     ep->com.qp, IWCH_QP_ATTR_NEXT_STATE,\n\t\t\t\t     &attrs, 1);\n\t\t\tif (ret)\n\t\t\t\tprintk(KERN_ERR MOD\n\t\t\t\t       \"%s - qp <- error failed!\\n\",\n\t\t\t\t       __func__);\n\t\t}\n\t\tpeer_abort_upcall(ep);\n\t\tbreak;\n\tcase ABORTING:\n\t\tbreak;\n\tcase DEAD:\n\t\tPDBG(\"%s PEER_ABORT IN DEAD STATE!!!!\\n\", __func__);\n\t\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\t\treturn CPL_RET_BUF_DONE;\n\tdefault:\n\t\tBUG_ON(1);\n\t\tbreak;\n\t}\n\tdst_confirm(ep->dst);\n\tif (ep->com.state != ABORTING) {\n\t\t__state_set(&ep->com, DEAD);\n\t\trelease = 1;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\n\trpl_skb = get_skb(skb, sizeof(*rpl), GFP_KERNEL);\n\tif (!rpl_skb) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot allocate skb!\\n\",\n\t\t       __func__);\n\t\trelease = 1;\n\t\tgoto out;\n\t}\n\trpl_skb->priority = CPL_PRIORITY_DATA;\n\trpl = (struct cpl_abort_rpl *) skb_put(rpl_skb, sizeof(*rpl));\n\trpl->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_HOST_ABORT_CON_RPL));\n\trpl->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(rpl) = htonl(MK_OPCODE_TID(CPL_ABORT_RPL, ep->hwtid));\n\trpl->cmd = CPL_ABORT_NO_RST;\n\tiwch_cxgb3_ofld_send(ep->com.tdev, rpl_skb);\nout:\n\tif (release)\n\t\trelease_ep_resources(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int close_con_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct iwch_qp_attributes attrs;\n\tunsigned long flags;\n\tint release = 0;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tBUG_ON(!ep);\n\n\t/* The cm_id may be null if we failed to connect */\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tswitch (ep->com.state) {\n\tcase CLOSING:\n\t\t__state_set(&ep->com, MORIBUND);\n\t\tbreak;\n\tcase MORIBUND:\n\t\tstop_ep_timer(ep);\n\t\tif ((ep->com.cm_id) && (ep->com.qp)) {\n\t\t\tattrs.next_state = IWCH_QP_STATE_IDLE;\n\t\t\tiwch_modify_qp(ep->com.qp->rhp,\n\t\t\t\t\t     ep->com.qp,\n\t\t\t\t\t     IWCH_QP_ATTR_NEXT_STATE,\n\t\t\t\t\t     &attrs, 1);\n\t\t}\n\t\tclose_complete_upcall(ep);\n\t\t__state_set(&ep->com, DEAD);\n\t\trelease = 1;\n\t\tbreak;\n\tcase ABORTING:\n\tcase DEAD:\n\t\tbreak;\n\tdefault:\n\t\tBUG_ON(1);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (release)\n\t\trelease_ep_resources(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * T3A does 3 things when a TERM is received:\n * 1) send up a CPL_RDMA_TERMINATE message with the TERM packet\n * 2) generate an async event on the QP with the TERMINATE opcode\n * 3) post a TERMINATE opcode cqe into the associated CQ.\n *\n * For (1), we save the message in the qp for later consumer consumption.\n * For (2), we move the QP into TERMINATE, post a QP event and disconnect.\n * For (3), we toss the CQE in cxio_poll_cq().\n *\n * terminate() handles case (1)...\n */\nstatic int terminate(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\n\tif (state_read(&ep->com) != FPDU_MODE)\n\t\treturn CPL_RET_BUF_DONE;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb_pull(skb, sizeof(struct cpl_rdma_terminate));\n\tPDBG(\"%s saving %d bytes of term msg\\n\", __func__, skb->len);\n\tskb_copy_from_linear_data(skb, ep->com.qp->attr.terminate_buffer,\n\t\t\t\t  skb->len);\n\tep->com.qp->attr.terminate_msg_len = skb->len;\n\tep->com.qp->attr.is_terminate_local = 0;\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int ec_status(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct cpl_rdma_ec_status *rep = cplhdr(skb);\n\tstruct iwch_ep *ep = ctx;\n\n\tPDBG(\"%s ep %p tid %u status %d\\n\", __func__, ep, ep->hwtid,\n\t     rep->status);\n\tif (rep->status) {\n\t\tstruct iwch_qp_attributes attrs;\n\n\t\tprintk(KERN_ERR MOD \"%s BAD CLOSE - Aborting tid %u\\n\",\n\t\t       __func__, ep->hwtid);\n\t\tstop_ep_timer(ep);\n\t\tattrs.next_state = IWCH_QP_STATE_ERROR;\n\t\tiwch_modify_qp(ep->com.qp->rhp,\n\t\t\t       ep->com.qp, IWCH_QP_ATTR_NEXT_STATE,\n\t\t\t       &attrs, 1);\n\t\tabort_connection(ep, NULL, GFP_KERNEL);\n\t}\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic void ep_timeout(unsigned long arg)\n{\n\tstruct iwch_ep *ep = (struct iwch_ep *)arg;\n\tstruct iwch_qp_attributes attrs;\n\tunsigned long flags;\n\tint abort = 1;\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tPDBG(\"%s ep %p tid %u state %d\\n\", __func__, ep, ep->hwtid,\n\t     ep->com.state);\n\tswitch (ep->com.state) {\n\tcase MPA_REQ_SENT:\n\t\t__state_set(&ep->com, ABORTING);\n\t\tconnect_reply_upcall(ep, -ETIMEDOUT);\n\t\tbreak;\n\tcase MPA_REQ_WAIT:\n\t\t__state_set(&ep->com, ABORTING);\n\t\tbreak;\n\tcase CLOSING:\n\tcase MORIBUND:\n\t\tif (ep->com.cm_id && ep->com.qp) {\n\t\t\tattrs.next_state = IWCH_QP_STATE_ERROR;\n\t\t\tiwch_modify_qp(ep->com.qp->rhp,\n\t\t\t\t     ep->com.qp, IWCH_QP_ATTR_NEXT_STATE,\n\t\t\t\t     &attrs, 1);\n\t\t}\n\t\t__state_set(&ep->com, ABORTING);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"%s unexpected state ep %p state %u\\n\",\n\t\t\t__func__, ep, ep->com.state);\n\t\tabort = 0;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (abort)\n\t\tabort_connection(ep, NULL, GFP_ATOMIC);\n\tput_ep(&ep->com);\n}\n\nint iwch_reject_cr(struct iw_cm_id *cm_id, const void *pdata, u8 pdata_len)\n{\n\tint err;\n\tstruct iwch_ep *ep = to_ep(cm_id);\n\tPDBG(\"%s ep %p tid %u\\n\", __func__, ep, ep->hwtid);\n\n\tif (state_read(&ep->com) == DEAD) {\n\t\tput_ep(&ep->com);\n\t\treturn -ECONNRESET;\n\t}\n\tBUG_ON(state_read(&ep->com) != MPA_REQ_RCVD);\n\tif (mpa_rev == 0)\n\t\tabort_connection(ep, NULL, GFP_KERNEL);\n\telse {\n\t\terr = send_mpa_reject(ep, pdata, pdata_len);\n\t\terr = iwch_ep_disconnect(ep, 0, GFP_KERNEL);\n\t}\n\tput_ep(&ep->com);\n\treturn 0;\n}\n\nint iwch_accept_cr(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param)\n{\n\tint err;\n\tstruct iwch_qp_attributes attrs;\n\tenum iwch_qp_attr_mask mask;\n\tstruct iwch_ep *ep = to_ep(cm_id);\n\tstruct iwch_dev *h = to_iwch_dev(cm_id->device);\n\tstruct iwch_qp *qp = get_qhp(h, conn_param->qpn);\n\n\tPDBG(\"%s ep %p tid %u\\n\", __func__, ep, ep->hwtid);\n\tif (state_read(&ep->com) == DEAD) {\n\t\terr = -ECONNRESET;\n\t\tgoto err;\n\t}\n\n\tBUG_ON(state_read(&ep->com) != MPA_REQ_RCVD);\n\tBUG_ON(!qp);\n\n\tif ((conn_param->ord > qp->rhp->attr.max_rdma_read_qp_depth) ||\n\t    (conn_param->ird > qp->rhp->attr.max_rdma_reads_per_qp)) {\n\t\tabort_connection(ep, NULL, GFP_KERNEL);\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tcm_id->add_ref(cm_id);\n\tep->com.cm_id = cm_id;\n\tep->com.qp = qp;\n\n\tep->ird = conn_param->ird;\n\tep->ord = conn_param->ord;\n\n\tif (peer2peer && ep->ird == 0)\n\t\tep->ird = 1;\n\n\tPDBG(\"%s %d ird %d ord %d\\n\", __func__, __LINE__, ep->ird, ep->ord);\n\n\t/* bind QP to EP and move to RTS */\n\tattrs.mpa_attr = ep->mpa_attr;\n\tattrs.max_ird = ep->ird;\n\tattrs.max_ord = ep->ord;\n\tattrs.llp_stream_handle = ep;\n\tattrs.next_state = IWCH_QP_STATE_RTS;\n\n\t/* bind QP and TID with INIT_WR */\n\tmask = IWCH_QP_ATTR_NEXT_STATE |\n\t\t\t     IWCH_QP_ATTR_LLP_STREAM_HANDLE |\n\t\t\t     IWCH_QP_ATTR_MPA_ATTR |\n\t\t\t     IWCH_QP_ATTR_MAX_IRD |\n\t\t\t     IWCH_QP_ATTR_MAX_ORD;\n\n\terr = iwch_modify_qp(ep->com.qp->rhp,\n\t\t\t     ep->com.qp, mask, &attrs, 1);\n\tif (err)\n\t\tgoto err1;\n\n\t/* if needed, wait for wr_ack */\n\tif (iwch_rqes_posted(qp)) {\n\t\twait_event(ep->com.waitq, ep->com.rpl_done);\n\t\terr = ep->com.rpl_err;\n\t\tif (err)\n\t\t\tgoto err1;\n\t}\n\n\terr = send_mpa_reply(ep, conn_param->private_data,\n\t\t\t     conn_param->private_data_len);\n\tif (err)\n\t\tgoto err1;\n\n\n\tstate_set(&ep->com, FPDU_MODE);\n\testablished_upcall(ep);\n\tput_ep(&ep->com);\n\treturn 0;\nerr1:\n\tep->com.cm_id = NULL;\n\tep->com.qp = NULL;\n\tcm_id->rem_ref(cm_id);\nerr:\n\tput_ep(&ep->com);\n\treturn err;\n}\n\nstatic int is_loopback_dst(struct iw_cm_id *cm_id)\n{\n\tstruct net_device *dev;\n\tstruct sockaddr_in *raddr = (struct sockaddr_in *)&cm_id->remote_addr;\n\n\tdev = ip_dev_find(&init_net, raddr->sin_addr.s_addr);\n\tif (!dev)\n\t\treturn 0;\n\tdev_put(dev);\n\treturn 1;\n}\n\nint iwch_connect(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param)\n{\n\tstruct iwch_dev *h = to_iwch_dev(cm_id->device);\n\tstruct iwch_ep *ep;\n\tstruct rtable *rt;\n\tint err = 0;\n\tstruct sockaddr_in *laddr = (struct sockaddr_in *)&cm_id->local_addr;\n\tstruct sockaddr_in *raddr = (struct sockaddr_in *)&cm_id->remote_addr;\n\n\tif (cm_id->remote_addr.ss_family != PF_INET) {\n\t\terr = -ENOSYS;\n\t\tgoto out;\n\t}\n\n\tif (is_loopback_dst(cm_id)) {\n\t\terr = -ENOSYS;\n\t\tgoto out;\n\t}\n\n\tep = alloc_ep(sizeof(*ep), GFP_KERNEL);\n\tif (!ep) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc ep.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tinit_timer(&ep->timer);\n\tep->plen = conn_param->private_data_len;\n\tif (ep->plen)\n\t\tmemcpy(ep->mpa_pkt + sizeof(struct mpa_message),\n\t\t       conn_param->private_data, ep->plen);\n\tep->ird = conn_param->ird;\n\tep->ord = conn_param->ord;\n\n\tif (peer2peer && ep->ord == 0)\n\t\tep->ord = 1;\n\n\tep->com.tdev = h->rdev.t3cdev_p;\n\n\tcm_id->add_ref(cm_id);\n\tep->com.cm_id = cm_id;\n\tep->com.qp = get_qhp(h, conn_param->qpn);\n\tBUG_ON(!ep->com.qp);\n\tPDBG(\"%s qpn 0x%x qp %p cm_id %p\\n\", __func__, conn_param->qpn,\n\t     ep->com.qp, cm_id);\n\n\t/*\n\t * Allocate an active TID to initiate a TCP connection.\n\t */\n\tep->atid = cxgb3_alloc_atid(h->rdev.t3cdev_p, &t3c_client, ep);\n\tif (ep->atid == -1) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc atid.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto fail2;\n\t}\n\n\t/* find a route */\n\trt = find_route(h->rdev.t3cdev_p, laddr->sin_addr.s_addr,\n\t\t\traddr->sin_addr.s_addr, laddr->sin_port,\n\t\t\traddr->sin_port, IPTOS_LOWDELAY);\n\tif (!rt) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot find route.\\n\", __func__);\n\t\terr = -EHOSTUNREACH;\n\t\tgoto fail3;\n\t}\n\tep->dst = &rt->dst;\n\tep->l2t = t3_l2t_get(ep->com.tdev, ep->dst, NULL,\n\t\t\t     &raddr->sin_addr.s_addr);\n\tif (!ep->l2t) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc l2e.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto fail4;\n\t}\n\n\tstate_set(&ep->com, CONNECTING);\n\tep->tos = IPTOS_LOWDELAY;\n\tmemcpy(&ep->com.local_addr, &cm_id->local_addr,\n\t       sizeof(ep->com.local_addr));\n\tmemcpy(&ep->com.remote_addr, &cm_id->remote_addr,\n\t       sizeof(ep->com.remote_addr));\n\n\t/* send connect request to rnic */\n\terr = send_connect(ep);\n\tif (!err)\n\t\tgoto out;\n\n\tl2t_release(h->rdev.t3cdev_p, ep->l2t);\nfail4:\n\tdst_release(ep->dst);\nfail3:\n\tcxgb3_free_atid(ep->com.tdev, ep->atid);\nfail2:\n\tcm_id->rem_ref(cm_id);\n\tput_ep(&ep->com);\nout:\n\treturn err;\n}\n\nint iwch_create_listen(struct iw_cm_id *cm_id, int backlog)\n{\n\tint err = 0;\n\tstruct iwch_dev *h = to_iwch_dev(cm_id->device);\n\tstruct iwch_listen_ep *ep;\n\n\n\tmight_sleep();\n\n\tif (cm_id->local_addr.ss_family != PF_INET) {\n\t\terr = -ENOSYS;\n\t\tgoto fail1;\n\t}\n\n\tep = alloc_ep(sizeof(*ep), GFP_KERNEL);\n\tif (!ep) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc ep.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto fail1;\n\t}\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tep->com.tdev = h->rdev.t3cdev_p;\n\tcm_id->add_ref(cm_id);\n\tep->com.cm_id = cm_id;\n\tep->backlog = backlog;\n\tmemcpy(&ep->com.local_addr, &cm_id->local_addr,\n\t       sizeof(ep->com.local_addr));\n\n\t/*\n\t * Allocate a server TID.\n\t */\n\tep->stid = cxgb3_alloc_stid(h->rdev.t3cdev_p, &t3c_client, ep);\n\tif (ep->stid == -1) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc atid.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto fail2;\n\t}\n\n\tstate_set(&ep->com, LISTEN);\n\terr = listen_start(ep);\n\tif (err)\n\t\tgoto fail3;\n\n\t/* wait for pass_open_rpl */\n\twait_event(ep->com.waitq, ep->com.rpl_done);\n\terr = ep->com.rpl_err;\n\tif (!err) {\n\t\tcm_id->provider_data = ep;\n\t\tgoto out;\n\t}\nfail3:\n\tcxgb3_free_stid(ep->com.tdev, ep->stid);\nfail2:\n\tcm_id->rem_ref(cm_id);\n\tput_ep(&ep->com);\nfail1:\nout:\n\treturn err;\n}\n\nint iwch_destroy_listen(struct iw_cm_id *cm_id)\n{\n\tint err;\n\tstruct iwch_listen_ep *ep = to_listen_ep(cm_id);\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\n\tmight_sleep();\n\tstate_set(&ep->com, DEAD);\n\tep->com.rpl_done = 0;\n\tep->com.rpl_err = 0;\n\terr = listen_stop(ep);\n\tif (err)\n\t\tgoto done;\n\twait_event(ep->com.waitq, ep->com.rpl_done);\n\tcxgb3_free_stid(ep->com.tdev, ep->stid);\ndone:\n\terr = ep->com.rpl_err;\n\tcm_id->rem_ref(cm_id);\n\tput_ep(&ep->com);\n\treturn err;\n}\n\nint iwch_ep_disconnect(struct iwch_ep *ep, int abrupt, gfp_t gfp)\n{\n\tint ret=0;\n\tunsigned long flags;\n\tint close = 0;\n\tint fatal = 0;\n\tstruct t3cdev *tdev;\n\tstruct cxio_rdev *rdev;\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\n\tPDBG(\"%s ep %p state %s, abrupt %d\\n\", __func__, ep,\n\t     states[ep->com.state], abrupt);\n\n\ttdev = (struct t3cdev *)ep->com.tdev;\n\trdev = (struct cxio_rdev *)tdev->ulp;\n\tif (cxio_fatal_error(rdev)) {\n\t\tfatal = 1;\n\t\tclose_complete_upcall(ep);\n\t\tep->com.state = DEAD;\n\t}\n\tswitch (ep->com.state) {\n\tcase MPA_REQ_WAIT:\n\tcase MPA_REQ_SENT:\n\tcase MPA_REQ_RCVD:\n\tcase MPA_REP_SENT:\n\tcase FPDU_MODE:\n\t\tclose = 1;\n\t\tif (abrupt)\n\t\t\tep->com.state = ABORTING;\n\t\telse {\n\t\t\tep->com.state = CLOSING;\n\t\t\tstart_ep_timer(ep);\n\t\t}\n\t\tset_bit(CLOSE_SENT, &ep->com.flags);\n\t\tbreak;\n\tcase CLOSING:\n\t\tif (!test_and_set_bit(CLOSE_SENT, &ep->com.flags)) {\n\t\t\tclose = 1;\n\t\t\tif (abrupt) {\n\t\t\t\tstop_ep_timer(ep);\n\t\t\t\tep->com.state = ABORTING;\n\t\t\t} else\n\t\t\t\tep->com.state = MORIBUND;\n\t\t}\n\t\tbreak;\n\tcase MORIBUND:\n\tcase ABORTING:\n\tcase DEAD:\n\t\tPDBG(\"%s ignoring disconnect ep %p state %u\\n\",\n\t\t     __func__, ep, ep->com.state);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (close) {\n\t\tif (abrupt)\n\t\t\tret = send_abort(ep, NULL, gfp);\n\t\telse\n\t\t\tret = send_halfclose(ep, gfp);\n\t\tif (ret)\n\t\t\tfatal = 1;\n\t}\n\tif (fatal)\n\t\trelease_ep_resources(ep);\n\treturn ret;\n}\n\nint iwch_ep_redirect(void *ctx, struct dst_entry *old, struct dst_entry *new,\n\t\t     struct l2t_entry *l2t)\n{\n\tstruct iwch_ep *ep = ctx;\n\n\tif (ep->dst != old)\n\t\treturn 0;\n\n\tPDBG(\"%s ep %p redirect to dst %p l2t %p\\n\", __func__, ep, new,\n\t     l2t);\n\tdst_hold(new);\n\tl2t_release(ep->com.tdev, ep->l2t);\n\tep->l2t = l2t;\n\tdst_release(old);\n\tep->dst = new;\n\treturn 1;\n}\n\n/*\n * All the CM events are handled on a work queue to have a safe context.\n * These are the real handlers that are called from the work queue.\n */\nstatic const cxgb3_cpl_handler_func work_handlers[NUM_CPL_CMDS] = {\n\t[CPL_ACT_ESTABLISH]\t= act_establish,\n\t[CPL_ACT_OPEN_RPL]\t= act_open_rpl,\n\t[CPL_RX_DATA]\t\t= rx_data,\n\t[CPL_TX_DMA_ACK]\t= tx_ack,\n\t[CPL_ABORT_RPL_RSS]\t= abort_rpl,\n\t[CPL_ABORT_RPL]\t\t= abort_rpl,\n\t[CPL_PASS_OPEN_RPL]\t= pass_open_rpl,\n\t[CPL_CLOSE_LISTSRV_RPL]\t= close_listsrv_rpl,\n\t[CPL_PASS_ACCEPT_REQ]\t= pass_accept_req,\n\t[CPL_PASS_ESTABLISH]\t= pass_establish,\n\t[CPL_PEER_CLOSE]\t= peer_close,\n\t[CPL_ABORT_REQ_RSS]\t= peer_abort,\n\t[CPL_CLOSE_CON_RPL]\t= close_con_rpl,\n\t[CPL_RDMA_TERMINATE]\t= terminate,\n\t[CPL_RDMA_EC_STATUS]\t= ec_status,\n};\n\nstatic void process_work(struct work_struct *work)\n{\n\tstruct sk_buff *skb = NULL;\n\tvoid *ep;\n\tstruct t3cdev *tdev;\n\tint ret;\n\n\twhile ((skb = skb_dequeue(&rxq))) {\n\t\tep = *((void **) (skb->cb));\n\t\ttdev = *((struct t3cdev **) (skb->cb + sizeof(void *)));\n\t\tret = work_handlers[G_OPCODE(ntohl((__force __be32)skb->csum))](tdev, skb, ep);\n\t\tif (ret & CPL_RET_BUF_DONE)\n\t\t\tkfree_skb(skb);\n\n\t\t/*\n\t\t * ep was referenced in sched(), and is freed here.\n\t\t */\n\t\tput_ep((struct iwch_ep_common *)ep);\n\t}\n}\n\nstatic DECLARE_WORK(skb_work, process_work);\n\nstatic int sched(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep_common *epc = ctx;\n\n\tget_ep(epc);\n\n\t/*\n\t * Save ctx and tdev in the skb->cb area.\n\t */\n\t*((void **) skb->cb) = ctx;\n\t*((struct t3cdev **) (skb->cb + sizeof(void *))) = tdev;\n\n\t/*\n\t * Queue the skb and schedule the worker thread.\n\t */\n\tskb_queue_tail(&rxq, skb);\n\tqueue_work(workq, &skb_work);\n\treturn 0;\n}\n\nstatic int set_tcb_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct cpl_set_tcb_rpl *rpl = cplhdr(skb);\n\n\tif (rpl->status != CPL_ERR_NONE) {\n\t\tprintk(KERN_ERR MOD \"Unexpected SET_TCB_RPL status %u \"\n\t\t       \"for tid %u\\n\", rpl->status, GET_TID(rpl));\n\t}\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * All upcalls from the T3 Core go to sched() to schedule the\n * processing on a work queue.\n */\ncxgb3_cpl_handler_func t3c_handlers[NUM_CPL_CMDS] = {\n\t[CPL_ACT_ESTABLISH]\t= sched,\n\t[CPL_ACT_OPEN_RPL]\t= sched,\n\t[CPL_RX_DATA]\t\t= sched,\n\t[CPL_TX_DMA_ACK]\t= sched,\n\t[CPL_ABORT_RPL_RSS]\t= sched,\n\t[CPL_ABORT_RPL]\t\t= sched,\n\t[CPL_PASS_OPEN_RPL]\t= sched,\n\t[CPL_CLOSE_LISTSRV_RPL]\t= sched,\n\t[CPL_PASS_ACCEPT_REQ]\t= sched,\n\t[CPL_PASS_ESTABLISH]\t= sched,\n\t[CPL_PEER_CLOSE]\t= sched,\n\t[CPL_CLOSE_CON_RPL]\t= sched,\n\t[CPL_ABORT_REQ_RSS]\t= sched,\n\t[CPL_RDMA_TERMINATE]\t= sched,\n\t[CPL_RDMA_EC_STATUS]\t= sched,\n\t[CPL_SET_TCB_RPL]\t= set_tcb_rpl,\n};\n\nint __init iwch_cm_init(void)\n{\n\tskb_queue_head_init(&rxq);\n\n\tworkq = create_singlethread_workqueue(\"iw_cxgb3\");\n\tif (!workq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid __exit iwch_cm_term(void)\n{\n\tflush_workqueue(workq);\n\tdestroy_workqueue(workq);\n}\n"], "fixing_code": ["/*\n * Copyright (c) 2006 Chelsio, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n#include <linux/module.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/skbuff.h>\n#include <linux/timer.h>\n#include <linux/notifier.h>\n#include <linux/inetdevice.h>\n\n#include <net/neighbour.h>\n#include <net/netevent.h>\n#include <net/route.h>\n\n#include \"tcb.h\"\n#include \"cxgb3_offload.h\"\n#include \"iwch.h\"\n#include \"iwch_provider.h\"\n#include \"iwch_cm.h\"\n\nstatic char *states[] = {\n\t\"idle\",\n\t\"listen\",\n\t\"connecting\",\n\t\"mpa_wait_req\",\n\t\"mpa_req_sent\",\n\t\"mpa_req_rcvd\",\n\t\"mpa_rep_sent\",\n\t\"fpdu_mode\",\n\t\"aborting\",\n\t\"closing\",\n\t\"moribund\",\n\t\"dead\",\n\tNULL,\n};\n\nint peer2peer = 0;\nmodule_param(peer2peer, int, 0644);\nMODULE_PARM_DESC(peer2peer, \"Support peer2peer ULPs (default=0)\");\n\nstatic int ep_timeout_secs = 60;\nmodule_param(ep_timeout_secs, int, 0644);\nMODULE_PARM_DESC(ep_timeout_secs, \"CM Endpoint operation timeout \"\n\t\t\t\t   \"in seconds (default=60)\");\n\nstatic int mpa_rev = 1;\nmodule_param(mpa_rev, int, 0644);\nMODULE_PARM_DESC(mpa_rev, \"MPA Revision, 0 supports amso1100, \"\n\t\t \"1 is spec compliant. (default=1)\");\n\nstatic int markers_enabled = 0;\nmodule_param(markers_enabled, int, 0644);\nMODULE_PARM_DESC(markers_enabled, \"Enable MPA MARKERS (default(0)=disabled)\");\n\nstatic int crc_enabled = 1;\nmodule_param(crc_enabled, int, 0644);\nMODULE_PARM_DESC(crc_enabled, \"Enable MPA CRC (default(1)=enabled)\");\n\nstatic int rcv_win = 256 * 1024;\nmodule_param(rcv_win, int, 0644);\nMODULE_PARM_DESC(rcv_win, \"TCP receive window in bytes (default=256)\");\n\nstatic int snd_win = 32 * 1024;\nmodule_param(snd_win, int, 0644);\nMODULE_PARM_DESC(snd_win, \"TCP send window in bytes (default=32KB)\");\n\nstatic unsigned int nocong = 0;\nmodule_param(nocong, uint, 0644);\nMODULE_PARM_DESC(nocong, \"Turn off congestion control (default=0)\");\n\nstatic unsigned int cong_flavor = 1;\nmodule_param(cong_flavor, uint, 0644);\nMODULE_PARM_DESC(cong_flavor, \"TCP Congestion control flavor (default=1)\");\n\nstatic struct workqueue_struct *workq;\n\nstatic struct sk_buff_head rxq;\n\nstatic struct sk_buff *get_skb(struct sk_buff *skb, int len, gfp_t gfp);\nstatic void ep_timeout(unsigned long arg);\nstatic void connect_reply_upcall(struct iwch_ep *ep, int status);\n\nstatic void start_ep_timer(struct iwch_ep *ep)\n{\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tif (timer_pending(&ep->timer)) {\n\t\tPDBG(\"%s stopped / restarted timer ep %p\\n\", __func__, ep);\n\t\tdel_timer_sync(&ep->timer);\n\t} else\n\t\tget_ep(&ep->com);\n\tep->timer.expires = jiffies + ep_timeout_secs * HZ;\n\tep->timer.data = (unsigned long)ep;\n\tep->timer.function = ep_timeout;\n\tadd_timer(&ep->timer);\n}\n\nstatic void stop_ep_timer(struct iwch_ep *ep)\n{\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tif (!timer_pending(&ep->timer)) {\n\t\tWARN(1, \"%s timer stopped when its not running!  ep %p state %u\\n\",\n\t\t\t__func__, ep, ep->com.state);\n\t\treturn;\n\t}\n\tdel_timer_sync(&ep->timer);\n\tput_ep(&ep->com);\n}\n\nstatic int iwch_l2t_send(struct t3cdev *tdev, struct sk_buff *skb, struct l2t_entry *l2e)\n{\n\tint\terror = 0;\n\tstruct cxio_rdev *rdev;\n\n\trdev = (struct cxio_rdev *)tdev->ulp;\n\tif (cxio_fatal_error(rdev)) {\n\t\tkfree_skb(skb);\n\t\treturn -EIO;\n\t}\n\terror = l2t_send(tdev, skb, l2e);\n\tif (error < 0)\n\t\tkfree_skb(skb);\n\treturn error < 0 ? error : 0;\n}\n\nint iwch_cxgb3_ofld_send(struct t3cdev *tdev, struct sk_buff *skb)\n{\n\tint\terror = 0;\n\tstruct cxio_rdev *rdev;\n\n\trdev = (struct cxio_rdev *)tdev->ulp;\n\tif (cxio_fatal_error(rdev)) {\n\t\tkfree_skb(skb);\n\t\treturn -EIO;\n\t}\n\terror = cxgb3_ofld_send(tdev, skb);\n\tif (error < 0)\n\t\tkfree_skb(skb);\n\treturn error < 0 ? error : 0;\n}\n\nstatic void release_tid(struct t3cdev *tdev, u32 hwtid, struct sk_buff *skb)\n{\n\tstruct cpl_tid_release *req;\n\n\tskb = get_skb(skb, sizeof *req, GFP_KERNEL);\n\tif (!skb)\n\t\treturn;\n\treq = (struct cpl_tid_release *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_TID_RELEASE, hwtid));\n\tskb->priority = CPL_PRIORITY_SETUP;\n\tiwch_cxgb3_ofld_send(tdev, skb);\n\treturn;\n}\n\nint iwch_quiesce_tid(struct iwch_ep *ep)\n{\n\tstruct cpl_set_tcb_field *req;\n\tstruct sk_buff *skb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\n\tif (!skb)\n\t\treturn -ENOMEM;\n\treq = (struct cpl_set_tcb_field *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\treq->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, ep->hwtid));\n\treq->reply = 0;\n\treq->cpu_idx = 0;\n\treq->word = htons(W_TCB_RX_QUIESCE);\n\treq->mask = cpu_to_be64(1ULL << S_TCB_RX_QUIESCE);\n\treq->val = cpu_to_be64(1 << S_TCB_RX_QUIESCE);\n\n\tskb->priority = CPL_PRIORITY_DATA;\n\treturn iwch_cxgb3_ofld_send(ep->com.tdev, skb);\n}\n\nint iwch_resume_tid(struct iwch_ep *ep)\n{\n\tstruct cpl_set_tcb_field *req;\n\tstruct sk_buff *skb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\n\tif (!skb)\n\t\treturn -ENOMEM;\n\treq = (struct cpl_set_tcb_field *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\treq->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, ep->hwtid));\n\treq->reply = 0;\n\treq->cpu_idx = 0;\n\treq->word = htons(W_TCB_RX_QUIESCE);\n\treq->mask = cpu_to_be64(1ULL << S_TCB_RX_QUIESCE);\n\treq->val = 0;\n\n\tskb->priority = CPL_PRIORITY_DATA;\n\treturn iwch_cxgb3_ofld_send(ep->com.tdev, skb);\n}\n\nstatic void set_emss(struct iwch_ep *ep, u16 opt)\n{\n\tPDBG(\"%s ep %p opt %u\\n\", __func__, ep, opt);\n\tep->emss = T3C_DATA(ep->com.tdev)->mtus[G_TCPOPT_MSS(opt)] - 40;\n\tif (G_TCPOPT_TSTAMP(opt))\n\t\tep->emss -= 12;\n\tif (ep->emss < 128)\n\t\tep->emss = 128;\n\tPDBG(\"emss=%d\\n\", ep->emss);\n}\n\nstatic enum iwch_ep_state state_read(struct iwch_ep_common *epc)\n{\n\tunsigned long flags;\n\tenum iwch_ep_state state;\n\n\tspin_lock_irqsave(&epc->lock, flags);\n\tstate = epc->state;\n\tspin_unlock_irqrestore(&epc->lock, flags);\n\treturn state;\n}\n\nstatic void __state_set(struct iwch_ep_common *epc, enum iwch_ep_state new)\n{\n\tepc->state = new;\n}\n\nstatic void state_set(struct iwch_ep_common *epc, enum iwch_ep_state new)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&epc->lock, flags);\n\tPDBG(\"%s - %s -> %s\\n\", __func__, states[epc->state], states[new]);\n\t__state_set(epc, new);\n\tspin_unlock_irqrestore(&epc->lock, flags);\n\treturn;\n}\n\nstatic void *alloc_ep(int size, gfp_t gfp)\n{\n\tstruct iwch_ep_common *epc;\n\n\tepc = kzalloc(size, gfp);\n\tif (epc) {\n\t\tkref_init(&epc->kref);\n\t\tspin_lock_init(&epc->lock);\n\t\tinit_waitqueue_head(&epc->waitq);\n\t}\n\tPDBG(\"%s alloc ep %p\\n\", __func__, epc);\n\treturn epc;\n}\n\nvoid __free_ep(struct kref *kref)\n{\n\tstruct iwch_ep *ep;\n\tep = container_of(container_of(kref, struct iwch_ep_common, kref),\n\t\t\t  struct iwch_ep, com);\n\tPDBG(\"%s ep %p state %s\\n\", __func__, ep, states[state_read(&ep->com)]);\n\tif (test_bit(RELEASE_RESOURCES, &ep->com.flags)) {\n\t\tcxgb3_remove_tid(ep->com.tdev, (void *)ep, ep->hwtid);\n\t\tdst_release(ep->dst);\n\t\tl2t_release(ep->com.tdev, ep->l2t);\n\t}\n\tkfree(ep);\n}\n\nstatic void release_ep_resources(struct iwch_ep *ep)\n{\n\tPDBG(\"%s ep %p tid %d\\n\", __func__, ep, ep->hwtid);\n\tset_bit(RELEASE_RESOURCES, &ep->com.flags);\n\tput_ep(&ep->com);\n}\n\nstatic int status2errno(int status)\n{\n\tswitch (status) {\n\tcase CPL_ERR_NONE:\n\t\treturn 0;\n\tcase CPL_ERR_CONN_RESET:\n\t\treturn -ECONNRESET;\n\tcase CPL_ERR_ARP_MISS:\n\t\treturn -EHOSTUNREACH;\n\tcase CPL_ERR_CONN_TIMEDOUT:\n\t\treturn -ETIMEDOUT;\n\tcase CPL_ERR_TCAM_FULL:\n\t\treturn -ENOMEM;\n\tcase CPL_ERR_CONN_EXIST:\n\t\treturn -EADDRINUSE;\n\tdefault:\n\t\treturn -EIO;\n\t}\n}\n\n/*\n * Try and reuse skbs already allocated...\n */\nstatic struct sk_buff *get_skb(struct sk_buff *skb, int len, gfp_t gfp)\n{\n\tif (skb && !skb_is_nonlinear(skb) && !skb_cloned(skb)) {\n\t\tskb_trim(skb, 0);\n\t\tskb_get(skb);\n\t} else {\n\t\tskb = alloc_skb(len, gfp);\n\t}\n\treturn skb;\n}\n\nstatic struct rtable *find_route(struct t3cdev *dev, __be32 local_ip,\n\t\t\t\t __be32 peer_ip, __be16 local_port,\n\t\t\t\t __be16 peer_port, u8 tos)\n{\n\tstruct rtable *rt;\n\tstruct flowi4 fl4;\n\n\trt = ip_route_output_ports(&init_net, &fl4, NULL, peer_ip, local_ip,\n\t\t\t\t   peer_port, local_port, IPPROTO_TCP,\n\t\t\t\t   tos, 0);\n\tif (IS_ERR(rt))\n\t\treturn NULL;\n\treturn rt;\n}\n\nstatic unsigned int find_best_mtu(const struct t3c_data *d, unsigned short mtu)\n{\n\tint i = 0;\n\n\twhile (i < d->nmtus - 1 && d->mtus[i + 1] <= mtu)\n\t\t++i;\n\treturn i;\n}\n\nstatic void arp_failure_discard(struct t3cdev *dev, struct sk_buff *skb)\n{\n\tPDBG(\"%s t3cdev %p\\n\", __func__, dev);\n\tkfree_skb(skb);\n}\n\n/*\n * Handle an ARP failure for an active open.\n */\nstatic void act_open_req_arp_failure(struct t3cdev *dev, struct sk_buff *skb)\n{\n\tprintk(KERN_ERR MOD \"ARP failure duing connect\\n\");\n\tkfree_skb(skb);\n}\n\n/*\n * Handle an ARP failure for a CPL_ABORT_REQ.  Change it into a no RST variant\n * and send it along.\n */\nstatic void abort_arp_failure(struct t3cdev *dev, struct sk_buff *skb)\n{\n\tstruct cpl_abort_req *req = cplhdr(skb);\n\n\tPDBG(\"%s t3cdev %p\\n\", __func__, dev);\n\treq->cmd = CPL_ABORT_NO_RST;\n\tiwch_cxgb3_ofld_send(dev, skb);\n}\n\nstatic int send_halfclose(struct iwch_ep *ep, gfp_t gfp)\n{\n\tstruct cpl_close_con_req *req;\n\tstruct sk_buff *skb;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb = get_skb(NULL, sizeof(*req), gfp);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to alloc skb\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tskb->priority = CPL_PRIORITY_DATA;\n\tset_arp_failure_handler(skb, arp_failure_discard);\n\treq = (struct cpl_close_con_req *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_CLOSE_CON));\n\treq->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_CLOSE_CON_REQ, ep->hwtid));\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic int send_abort(struct iwch_ep *ep, struct sk_buff *skb, gfp_t gfp)\n{\n\tstruct cpl_abort_req *req;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb = get_skb(skb, sizeof(*req), gfp);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to alloc skb.\\n\",\n\t\t       __func__);\n\t\treturn -ENOMEM;\n\t}\n\tskb->priority = CPL_PRIORITY_DATA;\n\tset_arp_failure_handler(skb, abort_arp_failure);\n\treq = (struct cpl_abort_req *) skb_put(skb, sizeof(*req));\n\tmemset(req, 0, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_HOST_ABORT_CON_REQ));\n\treq->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_ABORT_REQ, ep->hwtid));\n\treq->cmd = CPL_ABORT_SEND_RST;\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic int send_connect(struct iwch_ep *ep)\n{\n\tstruct cpl_act_open_req *req;\n\tstruct sk_buff *skb;\n\tu32 opt0h, opt0l, opt2;\n\tunsigned int mtu_idx;\n\tint wscale;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\n\tskb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to alloc skb.\\n\",\n\t\t       __func__);\n\t\treturn -ENOMEM;\n\t}\n\tmtu_idx = find_best_mtu(T3C_DATA(ep->com.tdev), dst_mtu(ep->dst));\n\twscale = compute_wscale(rcv_win);\n\topt0h = V_NAGLE(0) |\n\t    V_NO_CONG(nocong) |\n\t    V_KEEP_ALIVE(1) |\n\t    F_TCAM_BYPASS |\n\t    V_WND_SCALE(wscale) |\n\t    V_MSS_IDX(mtu_idx) |\n\t    V_L2T_IDX(ep->l2t->idx) | V_TX_CHANNEL(ep->l2t->smt_idx);\n\topt0l = V_TOS((ep->tos >> 2) & M_TOS) | V_RCV_BUFSIZ(rcv_win>>10);\n\topt2 = F_RX_COALESCE_VALID | V_RX_COALESCE(0) | V_FLAVORS_VALID(1) |\n\t       V_CONG_CONTROL_FLAVOR(cong_flavor);\n\tskb->priority = CPL_PRIORITY_SETUP;\n\tset_arp_failure_handler(skb, act_open_req_arp_failure);\n\n\treq = (struct cpl_act_open_req *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_ACT_OPEN_REQ, ep->atid));\n\treq->local_port = ep->com.local_addr.sin_port;\n\treq->peer_port = ep->com.remote_addr.sin_port;\n\treq->local_ip = ep->com.local_addr.sin_addr.s_addr;\n\treq->peer_ip = ep->com.remote_addr.sin_addr.s_addr;\n\treq->opt0h = htonl(opt0h);\n\treq->opt0l = htonl(opt0l);\n\treq->params = 0;\n\treq->opt2 = htonl(opt2);\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic void send_mpa_req(struct iwch_ep *ep, struct sk_buff *skb)\n{\n\tint mpalen;\n\tstruct tx_data_wr *req;\n\tstruct mpa_message *mpa;\n\tint len;\n\n\tPDBG(\"%s ep %p pd_len %d\\n\", __func__, ep, ep->plen);\n\n\tBUG_ON(skb_cloned(skb));\n\n\tmpalen = sizeof(*mpa) + ep->plen;\n\tif (skb->data + mpalen + sizeof(*req) > skb_end_pointer(skb)) {\n\t\tkfree_skb(skb);\n\t\tskb=alloc_skb(mpalen + sizeof(*req), GFP_KERNEL);\n\t\tif (!skb) {\n\t\t\tconnect_reply_upcall(ep, -ENOMEM);\n\t\t\treturn;\n\t\t}\n\t}\n\tskb_trim(skb, 0);\n\tskb_reserve(skb, sizeof(*req));\n\tskb_put(skb, mpalen);\n\tskb->priority = CPL_PRIORITY_DATA;\n\tmpa = (struct mpa_message *) skb->data;\n\tmemset(mpa, 0, sizeof(*mpa));\n\tmemcpy(mpa->key, MPA_KEY_REQ, sizeof(mpa->key));\n\tmpa->flags = (crc_enabled ? MPA_CRC : 0) |\n\t\t     (markers_enabled ? MPA_MARKERS : 0);\n\tmpa->private_data_size = htons(ep->plen);\n\tmpa->revision = mpa_rev;\n\n\tif (ep->plen)\n\t\tmemcpy(mpa->private_data, ep->mpa_pkt + sizeof(*mpa), ep->plen);\n\n\t/*\n\t * Reference the mpa skb.  This ensures the data area\n\t * will remain in memory until the hw acks the tx.\n\t * Function tx_ack() will deref it.\n\t */\n\tskb_get(skb);\n\tset_arp_failure_handler(skb, arp_failure_discard);\n\tskb_reset_transport_header(skb);\n\tlen = skb->len;\n\treq = (struct tx_data_wr *) skb_push(skb, sizeof(*req));\n\treq->wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_TX_DATA)|F_WR_COMPL);\n\treq->wr_lo = htonl(V_WR_TID(ep->hwtid));\n\treq->len = htonl(len);\n\treq->param = htonl(V_TX_PORT(ep->l2t->smt_idx) |\n\t\t\t   V_TX_SNDBUF(snd_win>>15));\n\treq->flags = htonl(F_TX_INIT);\n\treq->sndseq = htonl(ep->snd_seq);\n\tBUG_ON(ep->mpa_skb);\n\tep->mpa_skb = skb;\n\tiwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n\tstart_ep_timer(ep);\n\tstate_set(&ep->com, MPA_REQ_SENT);\n\treturn;\n}\n\nstatic int send_mpa_reject(struct iwch_ep *ep, const void *pdata, u8 plen)\n{\n\tint mpalen;\n\tstruct tx_data_wr *req;\n\tstruct mpa_message *mpa;\n\tstruct sk_buff *skb;\n\n\tPDBG(\"%s ep %p plen %d\\n\", __func__, ep, plen);\n\n\tmpalen = sizeof(*mpa) + plen;\n\n\tskb = get_skb(NULL, mpalen + sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc skb!\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tskb_reserve(skb, sizeof(*req));\n\tmpa = (struct mpa_message *) skb_put(skb, mpalen);\n\tmemset(mpa, 0, sizeof(*mpa));\n\tmemcpy(mpa->key, MPA_KEY_REP, sizeof(mpa->key));\n\tmpa->flags = MPA_REJECT;\n\tmpa->revision = mpa_rev;\n\tmpa->private_data_size = htons(plen);\n\tif (plen)\n\t\tmemcpy(mpa->private_data, pdata, plen);\n\n\t/*\n\t * Reference the mpa skb again.  This ensures the data area\n\t * will remain in memory until the hw acks the tx.\n\t * Function tx_ack() will deref it.\n\t */\n\tskb_get(skb);\n\tskb->priority = CPL_PRIORITY_DATA;\n\tset_arp_failure_handler(skb, arp_failure_discard);\n\tskb_reset_transport_header(skb);\n\treq = (struct tx_data_wr *) skb_push(skb, sizeof(*req));\n\treq->wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_TX_DATA)|F_WR_COMPL);\n\treq->wr_lo = htonl(V_WR_TID(ep->hwtid));\n\treq->len = htonl(mpalen);\n\treq->param = htonl(V_TX_PORT(ep->l2t->smt_idx) |\n\t\t\t   V_TX_SNDBUF(snd_win>>15));\n\treq->flags = htonl(F_TX_INIT);\n\treq->sndseq = htonl(ep->snd_seq);\n\tBUG_ON(ep->mpa_skb);\n\tep->mpa_skb = skb;\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic int send_mpa_reply(struct iwch_ep *ep, const void *pdata, u8 plen)\n{\n\tint mpalen;\n\tstruct tx_data_wr *req;\n\tstruct mpa_message *mpa;\n\tint len;\n\tstruct sk_buff *skb;\n\n\tPDBG(\"%s ep %p plen %d\\n\", __func__, ep, plen);\n\n\tmpalen = sizeof(*mpa) + plen;\n\n\tskb = get_skb(NULL, mpalen + sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc skb!\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tskb->priority = CPL_PRIORITY_DATA;\n\tskb_reserve(skb, sizeof(*req));\n\tmpa = (struct mpa_message *) skb_put(skb, mpalen);\n\tmemset(mpa, 0, sizeof(*mpa));\n\tmemcpy(mpa->key, MPA_KEY_REP, sizeof(mpa->key));\n\tmpa->flags = (ep->mpa_attr.crc_enabled ? MPA_CRC : 0) |\n\t\t     (markers_enabled ? MPA_MARKERS : 0);\n\tmpa->revision = mpa_rev;\n\tmpa->private_data_size = htons(plen);\n\tif (plen)\n\t\tmemcpy(mpa->private_data, pdata, plen);\n\n\t/*\n\t * Reference the mpa skb.  This ensures the data area\n\t * will remain in memory until the hw acks the tx.\n\t * Function tx_ack() will deref it.\n\t */\n\tskb_get(skb);\n\tset_arp_failure_handler(skb, arp_failure_discard);\n\tskb_reset_transport_header(skb);\n\tlen = skb->len;\n\treq = (struct tx_data_wr *) skb_push(skb, sizeof(*req));\n\treq->wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_TX_DATA)|F_WR_COMPL);\n\treq->wr_lo = htonl(V_WR_TID(ep->hwtid));\n\treq->len = htonl(len);\n\treq->param = htonl(V_TX_PORT(ep->l2t->smt_idx) |\n\t\t\t   V_TX_SNDBUF(snd_win>>15));\n\treq->flags = htonl(F_TX_INIT);\n\treq->sndseq = htonl(ep->snd_seq);\n\tep->mpa_skb = skb;\n\tstate_set(&ep->com, MPA_REP_SENT);\n\treturn iwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n}\n\nstatic int act_establish(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_act_establish *req = cplhdr(skb);\n\tunsigned int tid = GET_TID(req);\n\n\tPDBG(\"%s ep %p tid %d\\n\", __func__, ep, tid);\n\n\tdst_confirm(ep->dst);\n\n\t/* setup the hwtid for this connection */\n\tep->hwtid = tid;\n\tcxgb3_insert_tid(ep->com.tdev, &t3c_client, ep, tid);\n\n\tep->snd_seq = ntohl(req->snd_isn);\n\tep->rcv_seq = ntohl(req->rcv_isn);\n\n\tset_emss(ep, ntohs(req->tcp_opt));\n\n\t/* dealloc the atid */\n\tcxgb3_free_atid(ep->com.tdev, ep->atid);\n\n\t/* start MPA negotiation */\n\tsend_mpa_req(ep, skb);\n\n\treturn 0;\n}\n\nstatic void abort_connection(struct iwch_ep *ep, struct sk_buff *skb, gfp_t gfp)\n{\n\tPDBG(\"%s ep %p\\n\", __FILE__, ep);\n\tstate_set(&ep->com, ABORTING);\n\tsend_abort(ep, skb, gfp);\n}\n\nstatic void close_complete_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_CLOSE;\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"close complete delivered ep %p cm_id %p tid %d\\n\",\n\t\t     ep, ep->com.cm_id, ep->hwtid);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t\tep->com.cm_id->rem_ref(ep->com.cm_id);\n\t\tep->com.cm_id = NULL;\n\t\tep->com.qp = NULL;\n\t}\n}\n\nstatic void peer_close_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_DISCONNECT;\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"peer close delivered ep %p cm_id %p tid %d\\n\",\n\t\t     ep, ep->com.cm_id, ep->hwtid);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t}\n}\n\nstatic void peer_abort_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_CLOSE;\n\tevent.status = -ECONNRESET;\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"abort delivered ep %p cm_id %p tid %d\\n\", ep,\n\t\t     ep->com.cm_id, ep->hwtid);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t\tep->com.cm_id->rem_ref(ep->com.cm_id);\n\t\tep->com.cm_id = NULL;\n\t\tep->com.qp = NULL;\n\t}\n}\n\nstatic void connect_reply_upcall(struct iwch_ep *ep, int status)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p status %d\\n\", __func__, ep, status);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_CONNECT_REPLY;\n\tevent.status = status;\n\tmemcpy(&event.local_addr, &ep->com.local_addr,\n\t       sizeof(ep->com.local_addr));\n\tmemcpy(&event.remote_addr, &ep->com.remote_addr,\n\t       sizeof(ep->com.remote_addr));\n\n\tif ((status == 0) || (status == -ECONNREFUSED)) {\n\t\tevent.private_data_len = ep->plen;\n\t\tevent.private_data = ep->mpa_pkt + sizeof(struct mpa_message);\n\t}\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"%s ep %p tid %d status %d\\n\", __func__, ep,\n\t\t     ep->hwtid, status);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t}\n\tif (status < 0) {\n\t\tep->com.cm_id->rem_ref(ep->com.cm_id);\n\t\tep->com.cm_id = NULL;\n\t\tep->com.qp = NULL;\n\t}\n}\n\nstatic void connect_request_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p tid %d\\n\", __func__, ep, ep->hwtid);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_CONNECT_REQUEST;\n\tmemcpy(&event.local_addr, &ep->com.local_addr,\n\t       sizeof(ep->com.local_addr));\n\tmemcpy(&event.remote_addr, &ep->com.remote_addr,\n\t       sizeof(ep->com.local_addr));\n\tevent.private_data_len = ep->plen;\n\tevent.private_data = ep->mpa_pkt + sizeof(struct mpa_message);\n\tevent.provider_data = ep;\n\t/*\n\t * Until ird/ord negotiation via MPAv2 support is added, send max\n\t * supported values\n\t */\n\tevent.ird = event.ord = 8;\n\tif (state_read(&ep->parent_ep->com) != DEAD) {\n\t\tget_ep(&ep->com);\n\t\tep->parent_ep->com.cm_id->event_handler(\n\t\t\t\t\t\tep->parent_ep->com.cm_id,\n\t\t\t\t\t\t&event);\n\t}\n\tput_ep(&ep->parent_ep->com);\n\tep->parent_ep = NULL;\n}\n\nstatic void established_upcall(struct iwch_ep *ep)\n{\n\tstruct iw_cm_event event;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tmemset(&event, 0, sizeof(event));\n\tevent.event = IW_CM_EVENT_ESTABLISHED;\n\t/*\n\t * Until ird/ord negotiation via MPAv2 support is added, send max\n\t * supported values\n\t */\n\tevent.ird = event.ord = 8;\n\tif (ep->com.cm_id) {\n\t\tPDBG(\"%s ep %p tid %d\\n\", __func__, ep, ep->hwtid);\n\t\tep->com.cm_id->event_handler(ep->com.cm_id, &event);\n\t}\n}\n\nstatic int update_rx_credits(struct iwch_ep *ep, u32 credits)\n{\n\tstruct cpl_rx_data_ack *req;\n\tstruct sk_buff *skb;\n\n\tPDBG(\"%s ep %p credits %u\\n\", __func__, ep, credits);\n\tskb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"update_rx_credits - cannot alloc skb!\\n\");\n\t\treturn 0;\n\t}\n\n\treq = (struct cpl_rx_data_ack *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_RX_DATA_ACK, ep->hwtid));\n\treq->credit_dack = htonl(V_RX_CREDITS(credits) | V_RX_FORCE_ACK(1));\n\tskb->priority = CPL_PRIORITY_ACK;\n\tiwch_cxgb3_ofld_send(ep->com.tdev, skb);\n\treturn credits;\n}\n\nstatic void process_mpa_reply(struct iwch_ep *ep, struct sk_buff *skb)\n{\n\tstruct mpa_message *mpa;\n\tu16 plen;\n\tstruct iwch_qp_attributes attrs;\n\tenum iwch_qp_attr_mask mask;\n\tint err;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\n\t/*\n\t * Stop mpa timer.  If it expired, then the state has\n\t * changed and we bail since ep_timeout already aborted\n\t * the connection.\n\t */\n\tstop_ep_timer(ep);\n\tif (state_read(&ep->com) != MPA_REQ_SENT)\n\t\treturn;\n\n\t/*\n\t * If we get more than the supported amount of private data\n\t * then we must fail this connection.\n\t */\n\tif (ep->mpa_pkt_len + skb->len > sizeof(ep->mpa_pkt)) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t/*\n\t * copy the new data into our accumulation buffer.\n\t */\n\tskb_copy_from_linear_data(skb, &(ep->mpa_pkt[ep->mpa_pkt_len]),\n\t\t\t\t  skb->len);\n\tep->mpa_pkt_len += skb->len;\n\n\t/*\n\t * if we don't even have the mpa message, then bail.\n\t */\n\tif (ep->mpa_pkt_len < sizeof(*mpa))\n\t\treturn;\n\tmpa = (struct mpa_message *) ep->mpa_pkt;\n\n\t/* Validate MPA header. */\n\tif (mpa->revision != mpa_rev) {\n\t\terr = -EPROTO;\n\t\tgoto err;\n\t}\n\tif (memcmp(mpa->key, MPA_KEY_REP, sizeof(mpa->key))) {\n\t\terr = -EPROTO;\n\t\tgoto err;\n\t}\n\n\tplen = ntohs(mpa->private_data_size);\n\n\t/*\n\t * Fail if there's too much private data.\n\t */\n\tif (plen > MPA_MAX_PRIVATE_DATA) {\n\t\terr = -EPROTO;\n\t\tgoto err;\n\t}\n\n\t/*\n\t * If plen does not account for pkt size\n\t */\n\tif (ep->mpa_pkt_len > (sizeof(*mpa) + plen)) {\n\t\terr = -EPROTO;\n\t\tgoto err;\n\t}\n\n\tep->plen = (u8) plen;\n\n\t/*\n\t * If we don't have all the pdata yet, then bail.\n\t * We'll continue process when more data arrives.\n\t */\n\tif (ep->mpa_pkt_len < (sizeof(*mpa) + plen))\n\t\treturn;\n\n\tif (mpa->flags & MPA_REJECT) {\n\t\terr = -ECONNREFUSED;\n\t\tgoto err;\n\t}\n\n\t/*\n\t * If we get here we have accumulated the entire mpa\n\t * start reply message including private data. And\n\t * the MPA header is valid.\n\t */\n\tstate_set(&ep->com, FPDU_MODE);\n\tep->mpa_attr.initiator = 1;\n\tep->mpa_attr.crc_enabled = (mpa->flags & MPA_CRC) | crc_enabled ? 1 : 0;\n\tep->mpa_attr.recv_marker_enabled = markers_enabled;\n\tep->mpa_attr.xmit_marker_enabled = mpa->flags & MPA_MARKERS ? 1 : 0;\n\tep->mpa_attr.version = mpa_rev;\n\tPDBG(\"%s - crc_enabled=%d, recv_marker_enabled=%d, \"\n\t     \"xmit_marker_enabled=%d, version=%d\\n\", __func__,\n\t     ep->mpa_attr.crc_enabled, ep->mpa_attr.recv_marker_enabled,\n\t     ep->mpa_attr.xmit_marker_enabled, ep->mpa_attr.version);\n\n\tattrs.mpa_attr = ep->mpa_attr;\n\tattrs.max_ird = ep->ird;\n\tattrs.max_ord = ep->ord;\n\tattrs.llp_stream_handle = ep;\n\tattrs.next_state = IWCH_QP_STATE_RTS;\n\n\tmask = IWCH_QP_ATTR_NEXT_STATE |\n\t    IWCH_QP_ATTR_LLP_STREAM_HANDLE | IWCH_QP_ATTR_MPA_ATTR |\n\t    IWCH_QP_ATTR_MAX_IRD | IWCH_QP_ATTR_MAX_ORD;\n\n\t/* bind QP and TID with INIT_WR */\n\terr = iwch_modify_qp(ep->com.qp->rhp,\n\t\t\t     ep->com.qp, mask, &attrs, 1);\n\tif (err)\n\t\tgoto err;\n\n\tif (peer2peer && iwch_rqes_posted(ep->com.qp) == 0) {\n\t\tiwch_post_zb_read(ep);\n\t}\n\n\tgoto out;\nerr:\n\tabort_connection(ep, skb, GFP_KERNEL);\nout:\n\tconnect_reply_upcall(ep, err);\n\treturn;\n}\n\nstatic void process_mpa_request(struct iwch_ep *ep, struct sk_buff *skb)\n{\n\tstruct mpa_message *mpa;\n\tu16 plen;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\n\t/*\n\t * Stop mpa timer.  If it expired, then the state has\n\t * changed and we bail since ep_timeout already aborted\n\t * the connection.\n\t */\n\tstop_ep_timer(ep);\n\tif (state_read(&ep->com) != MPA_REQ_WAIT)\n\t\treturn;\n\n\t/*\n\t * If we get more than the supported amount of private data\n\t * then we must fail this connection.\n\t */\n\tif (ep->mpa_pkt_len + skb->len > sizeof(ep->mpa_pkt)) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\n\tPDBG(\"%s enter (%s line %u)\\n\", __func__, __FILE__, __LINE__);\n\n\t/*\n\t * Copy the new data into our accumulation buffer.\n\t */\n\tskb_copy_from_linear_data(skb, &(ep->mpa_pkt[ep->mpa_pkt_len]),\n\t\t\t\t  skb->len);\n\tep->mpa_pkt_len += skb->len;\n\n\t/*\n\t * If we don't even have the mpa message, then bail.\n\t * We'll continue process when more data arrives.\n\t */\n\tif (ep->mpa_pkt_len < sizeof(*mpa))\n\t\treturn;\n\tPDBG(\"%s enter (%s line %u)\\n\", __func__, __FILE__, __LINE__);\n\tmpa = (struct mpa_message *) ep->mpa_pkt;\n\n\t/*\n\t * Validate MPA Header.\n\t */\n\tif (mpa->revision != mpa_rev) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\n\tif (memcmp(mpa->key, MPA_KEY_REQ, sizeof(mpa->key))) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\n\tplen = ntohs(mpa->private_data_size);\n\n\t/*\n\t * Fail if there's too much private data.\n\t */\n\tif (plen > MPA_MAX_PRIVATE_DATA) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\n\t/*\n\t * If plen does not account for pkt size\n\t */\n\tif (ep->mpa_pkt_len > (sizeof(*mpa) + plen)) {\n\t\tabort_connection(ep, skb, GFP_KERNEL);\n\t\treturn;\n\t}\n\tep->plen = (u8) plen;\n\n\t/*\n\t * If we don't have all the pdata yet, then bail.\n\t */\n\tif (ep->mpa_pkt_len < (sizeof(*mpa) + plen))\n\t\treturn;\n\n\t/*\n\t * If we get here we have accumulated the entire mpa\n\t * start reply message including private data.\n\t */\n\tep->mpa_attr.initiator = 0;\n\tep->mpa_attr.crc_enabled = (mpa->flags & MPA_CRC) | crc_enabled ? 1 : 0;\n\tep->mpa_attr.recv_marker_enabled = markers_enabled;\n\tep->mpa_attr.xmit_marker_enabled = mpa->flags & MPA_MARKERS ? 1 : 0;\n\tep->mpa_attr.version = mpa_rev;\n\tPDBG(\"%s - crc_enabled=%d, recv_marker_enabled=%d, \"\n\t     \"xmit_marker_enabled=%d, version=%d\\n\", __func__,\n\t     ep->mpa_attr.crc_enabled, ep->mpa_attr.recv_marker_enabled,\n\t     ep->mpa_attr.xmit_marker_enabled, ep->mpa_attr.version);\n\n\tstate_set(&ep->com, MPA_REQ_RCVD);\n\n\t/* drive upcall */\n\tconnect_request_upcall(ep);\n\treturn;\n}\n\nstatic int rx_data(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_rx_data *hdr = cplhdr(skb);\n\tunsigned int dlen = ntohs(hdr->len);\n\n\tPDBG(\"%s ep %p dlen %u\\n\", __func__, ep, dlen);\n\n\tskb_pull(skb, sizeof(*hdr));\n\tskb_trim(skb, dlen);\n\n\tep->rcv_seq += dlen;\n\tBUG_ON(ep->rcv_seq != (ntohl(hdr->seq) + dlen));\n\n\tswitch (state_read(&ep->com)) {\n\tcase MPA_REQ_SENT:\n\t\tprocess_mpa_reply(ep, skb);\n\t\tbreak;\n\tcase MPA_REQ_WAIT:\n\t\tprocess_mpa_request(ep, skb);\n\t\tbreak;\n\tcase MPA_REP_SENT:\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR MOD \"%s Unexpected streaming data.\"\n\t\t       \" ep %p state %d tid %d\\n\",\n\t\t       __func__, ep, state_read(&ep->com), ep->hwtid);\n\n\t\t/*\n\t\t * The ep will timeout and inform the ULP of the failure.\n\t\t * See ep_timeout().\n\t\t */\n\t\tbreak;\n\t}\n\n\t/* update RX credits */\n\tupdate_rx_credits(ep, dlen);\n\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * Upcall from the adapter indicating data has been transmitted.\n * For us its just the single MPA request or reply.  We can now free\n * the skb holding the mpa message.\n */\nstatic int tx_ack(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_wr_ack *hdr = cplhdr(skb);\n\tunsigned int credits = ntohs(hdr->credits);\n\tunsigned long flags;\n\tint post_zb = 0;\n\n\tPDBG(\"%s ep %p credits %u\\n\", __func__, ep, credits);\n\n\tif (credits == 0) {\n\t\tPDBG(\"%s 0 credit ack  ep %p state %u\\n\",\n\t\t     __func__, ep, state_read(&ep->com));\n\t\treturn CPL_RET_BUF_DONE;\n\t}\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tBUG_ON(credits != 1);\n\tdst_confirm(ep->dst);\n\tif (!ep->mpa_skb) {\n\t\tPDBG(\"%s rdma_init wr_ack ep %p state %u\\n\",\n\t\t\t__func__, ep, ep->com.state);\n\t\tif (ep->mpa_attr.initiator) {\n\t\t\tPDBG(\"%s initiator ep %p state %u\\n\",\n\t\t\t\t__func__, ep, ep->com.state);\n\t\t\tif (peer2peer && ep->com.state == FPDU_MODE)\n\t\t\t\tpost_zb = 1;\n\t\t} else {\n\t\t\tPDBG(\"%s responder ep %p state %u\\n\",\n\t\t\t\t__func__, ep, ep->com.state);\n\t\t\tif (ep->com.state == MPA_REQ_RCVD) {\n\t\t\t\tep->com.rpl_done = 1;\n\t\t\t\twake_up(&ep->com.waitq);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tPDBG(\"%s lsm ack ep %p state %u freeing skb\\n\",\n\t\t\t__func__, ep, ep->com.state);\n\t\tkfree_skb(ep->mpa_skb);\n\t\tep->mpa_skb = NULL;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (post_zb)\n\t\tiwch_post_zb_read(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int abort_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tunsigned long flags;\n\tint release = 0;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tBUG_ON(!ep);\n\n\t/*\n\t * We get 2 abort replies from the HW.  The first one must\n\t * be ignored except for scribbling that we need one more.\n\t */\n\tif (!test_and_set_bit(ABORT_REQ_IN_PROGRESS, &ep->com.flags)) {\n\t\treturn CPL_RET_BUF_DONE;\n\t}\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tswitch (ep->com.state) {\n\tcase ABORTING:\n\t\tclose_complete_upcall(ep);\n\t\t__state_set(&ep->com, DEAD);\n\t\trelease = 1;\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"%s ep %p state %d\\n\",\n\t\t     __func__, ep, ep->com.state);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\n\tif (release)\n\t\trelease_ep_resources(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * Return whether a failed active open has allocated a TID\n */\nstatic inline int act_open_has_tid(int status)\n{\n\treturn status != CPL_ERR_TCAM_FULL && status != CPL_ERR_CONN_EXIST &&\n\t       status != CPL_ERR_ARP_MISS;\n}\n\nstatic int act_open_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_act_open_rpl *rpl = cplhdr(skb);\n\n\tPDBG(\"%s ep %p status %u errno %d\\n\", __func__, ep, rpl->status,\n\t     status2errno(rpl->status));\n\tconnect_reply_upcall(ep, status2errno(rpl->status));\n\tstate_set(&ep->com, DEAD);\n\tif (ep->com.tdev->type != T3A && act_open_has_tid(rpl->status))\n\t\trelease_tid(ep->com.tdev, GET_TID(rpl), NULL);\n\tcxgb3_free_atid(ep->com.tdev, ep->atid);\n\tdst_release(ep->dst);\n\tl2t_release(ep->com.tdev, ep->l2t);\n\tput_ep(&ep->com);\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int listen_start(struct iwch_listen_ep *ep)\n{\n\tstruct sk_buff *skb;\n\tstruct cpl_pass_open_req *req;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"t3c_listen_start failed to alloc skb!\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treq = (struct cpl_pass_open_req *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_PASS_OPEN_REQ, ep->stid));\n\treq->local_port = ep->com.local_addr.sin_port;\n\treq->local_ip = ep->com.local_addr.sin_addr.s_addr;\n\treq->peer_port = 0;\n\treq->peer_ip = 0;\n\treq->peer_netmask = 0;\n\treq->opt0h = htonl(F_DELACK | F_TCAM_BYPASS);\n\treq->opt0l = htonl(V_RCV_BUFSIZ(rcv_win>>10));\n\treq->opt1 = htonl(V_CONN_POLICY(CPL_CONN_POLICY_ASK));\n\n\tskb->priority = 1;\n\treturn iwch_cxgb3_ofld_send(ep->com.tdev, skb);\n}\n\nstatic int pass_open_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_listen_ep *ep = ctx;\n\tstruct cpl_pass_open_rpl *rpl = cplhdr(skb);\n\n\tPDBG(\"%s ep %p status %d error %d\\n\", __func__, ep,\n\t     rpl->status, status2errno(rpl->status));\n\tep->com.rpl_err = status2errno(rpl->status);\n\tep->com.rpl_done = 1;\n\twake_up(&ep->com.waitq);\n\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int listen_stop(struct iwch_listen_ep *ep)\n{\n\tstruct sk_buff *skb;\n\tstruct cpl_close_listserv_req *req;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb = get_skb(NULL, sizeof(*req), GFP_KERNEL);\n\tif (!skb) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to alloc skb\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\treq = (struct cpl_close_listserv_req *) skb_put(skb, sizeof(*req));\n\treq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\treq->cpu_idx = 0;\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_CLOSE_LISTSRV_REQ, ep->stid));\n\tskb->priority = 1;\n\treturn iwch_cxgb3_ofld_send(ep->com.tdev, skb);\n}\n\nstatic int close_listsrv_rpl(struct t3cdev *tdev, struct sk_buff *skb,\n\t\t\t     void *ctx)\n{\n\tstruct iwch_listen_ep *ep = ctx;\n\tstruct cpl_close_listserv_rpl *rpl = cplhdr(skb);\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tep->com.rpl_err = status2errno(rpl->status);\n\tep->com.rpl_done = 1;\n\twake_up(&ep->com.waitq);\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic void accept_cr(struct iwch_ep *ep, __be32 peer_ip, struct sk_buff *skb)\n{\n\tstruct cpl_pass_accept_rpl *rpl;\n\tunsigned int mtu_idx;\n\tu32 opt0h, opt0l, opt2;\n\tint wscale;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tBUG_ON(skb_cloned(skb));\n\tskb_trim(skb, sizeof(*rpl));\n\tskb_get(skb);\n\tmtu_idx = find_best_mtu(T3C_DATA(ep->com.tdev), dst_mtu(ep->dst));\n\twscale = compute_wscale(rcv_win);\n\topt0h = V_NAGLE(0) |\n\t    V_NO_CONG(nocong) |\n\t    V_KEEP_ALIVE(1) |\n\t    F_TCAM_BYPASS |\n\t    V_WND_SCALE(wscale) |\n\t    V_MSS_IDX(mtu_idx) |\n\t    V_L2T_IDX(ep->l2t->idx) | V_TX_CHANNEL(ep->l2t->smt_idx);\n\topt0l = V_TOS((ep->tos >> 2) & M_TOS) | V_RCV_BUFSIZ(rcv_win>>10);\n\topt2 = F_RX_COALESCE_VALID | V_RX_COALESCE(0) | V_FLAVORS_VALID(1) |\n\t       V_CONG_CONTROL_FLAVOR(cong_flavor);\n\n\trpl = cplhdr(skb);\n\trpl->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\tOPCODE_TID(rpl) = htonl(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL, ep->hwtid));\n\trpl->peer_ip = peer_ip;\n\trpl->opt0h = htonl(opt0h);\n\trpl->opt0l_status = htonl(opt0l | CPL_PASS_OPEN_ACCEPT);\n\trpl->opt2 = htonl(opt2);\n\trpl->rsvd = rpl->opt2;\t/* workaround for HW bug */\n\tskb->priority = CPL_PRIORITY_SETUP;\n\tiwch_l2t_send(ep->com.tdev, skb, ep->l2t);\n\n\treturn;\n}\n\nstatic void reject_cr(struct t3cdev *tdev, u32 hwtid, __be32 peer_ip,\n\t\t      struct sk_buff *skb)\n{\n\tPDBG(\"%s t3cdev %p tid %u peer_ip %x\\n\", __func__, tdev, hwtid,\n\t     peer_ip);\n\tBUG_ON(skb_cloned(skb));\n\tskb_trim(skb, sizeof(struct cpl_tid_release));\n\tskb_get(skb);\n\n\tif (tdev->type != T3A)\n\t\trelease_tid(tdev, hwtid, skb);\n\telse {\n\t\tstruct cpl_pass_accept_rpl *rpl;\n\n\t\trpl = cplhdr(skb);\n\t\tskb->priority = CPL_PRIORITY_SETUP;\n\t\trpl->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\n\t\tOPCODE_TID(rpl) = htonl(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL,\n\t\t\t\t\t\t      hwtid));\n\t\trpl->peer_ip = peer_ip;\n\t\trpl->opt0h = htonl(F_TCAM_BYPASS);\n\t\trpl->opt0l_status = htonl(CPL_PASS_OPEN_REJECT);\n\t\trpl->opt2 = 0;\n\t\trpl->rsvd = rpl->opt2;\n\t\tiwch_cxgb3_ofld_send(tdev, skb);\n\t}\n}\n\nstatic int pass_accept_req(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *child_ep, *parent_ep = ctx;\n\tstruct cpl_pass_accept_req *req = cplhdr(skb);\n\tunsigned int hwtid = GET_TID(req);\n\tstruct dst_entry *dst;\n\tstruct l2t_entry *l2t;\n\tstruct rtable *rt;\n\tstruct iff_mac tim;\n\n\tPDBG(\"%s parent ep %p tid %u\\n\", __func__, parent_ep, hwtid);\n\n\tif (state_read(&parent_ep->com) != LISTEN) {\n\t\tprintk(KERN_ERR \"%s - listening ep not in LISTEN\\n\",\n\t\t       __func__);\n\t\tgoto reject;\n\t}\n\n\t/*\n\t * Find the netdev for this connection request.\n\t */\n\ttim.mac_addr = req->dst_mac;\n\ttim.vlan_tag = ntohs(req->vlan_tag);\n\tif (tdev->ctl(tdev, GET_IFF_FROM_MAC, &tim) < 0 || !tim.dev) {\n\t\tprintk(KERN_ERR \"%s bad dst mac %pM\\n\",\n\t\t\t__func__, req->dst_mac);\n\t\tgoto reject;\n\t}\n\n\t/* Find output route */\n\trt = find_route(tdev,\n\t\t\treq->local_ip,\n\t\t\treq->peer_ip,\n\t\t\treq->local_port,\n\t\t\treq->peer_port, G_PASS_OPEN_TOS(ntohl(req->tos_tid)));\n\tif (!rt) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to find dst entry!\\n\",\n\t\t       __func__);\n\t\tgoto reject;\n\t}\n\tdst = &rt->dst;\n\tl2t = t3_l2t_get(tdev, dst, NULL, &req->peer_ip);\n\tif (!l2t) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to allocate l2t entry!\\n\",\n\t\t       __func__);\n\t\tdst_release(dst);\n\t\tgoto reject;\n\t}\n\tchild_ep = alloc_ep(sizeof(*child_ep), GFP_KERNEL);\n\tif (!child_ep) {\n\t\tprintk(KERN_ERR MOD \"%s - failed to allocate ep entry!\\n\",\n\t\t       __func__);\n\t\tl2t_release(tdev, l2t);\n\t\tdst_release(dst);\n\t\tgoto reject;\n\t}\n\tstate_set(&child_ep->com, CONNECTING);\n\tchild_ep->com.tdev = tdev;\n\tchild_ep->com.cm_id = NULL;\n\tchild_ep->com.local_addr.sin_family = PF_INET;\n\tchild_ep->com.local_addr.sin_port = req->local_port;\n\tchild_ep->com.local_addr.sin_addr.s_addr = req->local_ip;\n\tchild_ep->com.remote_addr.sin_family = PF_INET;\n\tchild_ep->com.remote_addr.sin_port = req->peer_port;\n\tchild_ep->com.remote_addr.sin_addr.s_addr = req->peer_ip;\n\tget_ep(&parent_ep->com);\n\tchild_ep->parent_ep = parent_ep;\n\tchild_ep->tos = G_PASS_OPEN_TOS(ntohl(req->tos_tid));\n\tchild_ep->l2t = l2t;\n\tchild_ep->dst = dst;\n\tchild_ep->hwtid = hwtid;\n\tinit_timer(&child_ep->timer);\n\tcxgb3_insert_tid(tdev, &t3c_client, child_ep, hwtid);\n\taccept_cr(child_ep, req->peer_ip, skb);\n\tgoto out;\nreject:\n\treject_cr(tdev, hwtid, req->peer_ip, skb);\nout:\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int pass_establish(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_pass_establish *req = cplhdr(skb);\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tep->snd_seq = ntohl(req->snd_isn);\n\tep->rcv_seq = ntohl(req->rcv_isn);\n\n\tset_emss(ep, ntohs(req->tcp_opt));\n\n\tdst_confirm(ep->dst);\n\tstate_set(&ep->com, MPA_REQ_WAIT);\n\tstart_ep_timer(ep);\n\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int peer_close(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct iwch_qp_attributes attrs;\n\tunsigned long flags;\n\tint disconnect = 1;\n\tint release = 0;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tdst_confirm(ep->dst);\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tswitch (ep->com.state) {\n\tcase MPA_REQ_WAIT:\n\t\t__state_set(&ep->com, CLOSING);\n\t\tbreak;\n\tcase MPA_REQ_SENT:\n\t\t__state_set(&ep->com, CLOSING);\n\t\tconnect_reply_upcall(ep, -ECONNRESET);\n\t\tbreak;\n\tcase MPA_REQ_RCVD:\n\n\t\t/*\n\t\t * We're gonna mark this puppy DEAD, but keep\n\t\t * the reference on it until the ULP accepts or\n\t\t * rejects the CR. Also wake up anyone waiting\n\t\t * in rdma connection migration (see iwch_accept_cr()).\n\t\t */\n\t\t__state_set(&ep->com, CLOSING);\n\t\tep->com.rpl_done = 1;\n\t\tep->com.rpl_err = -ECONNRESET;\n\t\tPDBG(\"waking up ep %p\\n\", ep);\n\t\twake_up(&ep->com.waitq);\n\t\tbreak;\n\tcase MPA_REP_SENT:\n\t\t__state_set(&ep->com, CLOSING);\n\t\tep->com.rpl_done = 1;\n\t\tep->com.rpl_err = -ECONNRESET;\n\t\tPDBG(\"waking up ep %p\\n\", ep);\n\t\twake_up(&ep->com.waitq);\n\t\tbreak;\n\tcase FPDU_MODE:\n\t\tstart_ep_timer(ep);\n\t\t__state_set(&ep->com, CLOSING);\n\t\tattrs.next_state = IWCH_QP_STATE_CLOSING;\n\t\tiwch_modify_qp(ep->com.qp->rhp, ep->com.qp,\n\t\t\t       IWCH_QP_ATTR_NEXT_STATE, &attrs, 1);\n\t\tpeer_close_upcall(ep);\n\t\tbreak;\n\tcase ABORTING:\n\t\tdisconnect = 0;\n\t\tbreak;\n\tcase CLOSING:\n\t\t__state_set(&ep->com, MORIBUND);\n\t\tdisconnect = 0;\n\t\tbreak;\n\tcase MORIBUND:\n\t\tstop_ep_timer(ep);\n\t\tif (ep->com.cm_id && ep->com.qp) {\n\t\t\tattrs.next_state = IWCH_QP_STATE_IDLE;\n\t\t\tiwch_modify_qp(ep->com.qp->rhp, ep->com.qp,\n\t\t\t\t       IWCH_QP_ATTR_NEXT_STATE, &attrs, 1);\n\t\t}\n\t\tclose_complete_upcall(ep);\n\t\t__state_set(&ep->com, DEAD);\n\t\trelease = 1;\n\t\tdisconnect = 0;\n\t\tbreak;\n\tcase DEAD:\n\t\tdisconnect = 0;\n\t\tbreak;\n\tdefault:\n\t\tBUG_ON(1);\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (disconnect)\n\t\tiwch_ep_disconnect(ep, 0, GFP_KERNEL);\n\tif (release)\n\t\trelease_ep_resources(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * Returns whether an ABORT_REQ_RSS message is a negative advice.\n */\nstatic int is_neg_adv_abort(unsigned int status)\n{\n\treturn status == CPL_ERR_RTX_NEG_ADVICE ||\n\t       status == CPL_ERR_PERSIST_NEG_ADVICE;\n}\n\nstatic int peer_abort(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct cpl_abort_req_rss *req = cplhdr(skb);\n\tstruct iwch_ep *ep = ctx;\n\tstruct cpl_abort_rpl *rpl;\n\tstruct sk_buff *rpl_skb;\n\tstruct iwch_qp_attributes attrs;\n\tint ret;\n\tint release = 0;\n\tunsigned long flags;\n\n\tif (is_neg_adv_abort(req->status)) {\n\t\tPDBG(\"%s neg_adv_abort ep %p tid %d\\n\", __func__, ep,\n\t\t     ep->hwtid);\n\t\tt3_l2t_send_event(ep->com.tdev, ep->l2t);\n\t\treturn CPL_RET_BUF_DONE;\n\t}\n\n\t/*\n\t * We get 2 peer aborts from the HW.  The first one must\n\t * be ignored except for scribbling that we need one more.\n\t */\n\tif (!test_and_set_bit(PEER_ABORT_IN_PROGRESS, &ep->com.flags)) {\n\t\treturn CPL_RET_BUF_DONE;\n\t}\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tPDBG(\"%s ep %p state %u\\n\", __func__, ep, ep->com.state);\n\tswitch (ep->com.state) {\n\tcase CONNECTING:\n\t\tbreak;\n\tcase MPA_REQ_WAIT:\n\t\tstop_ep_timer(ep);\n\t\tbreak;\n\tcase MPA_REQ_SENT:\n\t\tstop_ep_timer(ep);\n\t\tconnect_reply_upcall(ep, -ECONNRESET);\n\t\tbreak;\n\tcase MPA_REP_SENT:\n\t\tep->com.rpl_done = 1;\n\t\tep->com.rpl_err = -ECONNRESET;\n\t\tPDBG(\"waking up ep %p\\n\", ep);\n\t\twake_up(&ep->com.waitq);\n\t\tbreak;\n\tcase MPA_REQ_RCVD:\n\n\t\t/*\n\t\t * We're gonna mark this puppy DEAD, but keep\n\t\t * the reference on it until the ULP accepts or\n\t\t * rejects the CR. Also wake up anyone waiting\n\t\t * in rdma connection migration (see iwch_accept_cr()).\n\t\t */\n\t\tep->com.rpl_done = 1;\n\t\tep->com.rpl_err = -ECONNRESET;\n\t\tPDBG(\"waking up ep %p\\n\", ep);\n\t\twake_up(&ep->com.waitq);\n\t\tbreak;\n\tcase MORIBUND:\n\tcase CLOSING:\n\t\tstop_ep_timer(ep);\n\t\t/*FALLTHROUGH*/\n\tcase FPDU_MODE:\n\t\tif (ep->com.cm_id && ep->com.qp) {\n\t\t\tattrs.next_state = IWCH_QP_STATE_ERROR;\n\t\t\tret = iwch_modify_qp(ep->com.qp->rhp,\n\t\t\t\t     ep->com.qp, IWCH_QP_ATTR_NEXT_STATE,\n\t\t\t\t     &attrs, 1);\n\t\t\tif (ret)\n\t\t\t\tprintk(KERN_ERR MOD\n\t\t\t\t       \"%s - qp <- error failed!\\n\",\n\t\t\t\t       __func__);\n\t\t}\n\t\tpeer_abort_upcall(ep);\n\t\tbreak;\n\tcase ABORTING:\n\t\tbreak;\n\tcase DEAD:\n\t\tPDBG(\"%s PEER_ABORT IN DEAD STATE!!!!\\n\", __func__);\n\t\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\t\treturn CPL_RET_BUF_DONE;\n\tdefault:\n\t\tBUG_ON(1);\n\t\tbreak;\n\t}\n\tdst_confirm(ep->dst);\n\tif (ep->com.state != ABORTING) {\n\t\t__state_set(&ep->com, DEAD);\n\t\trelease = 1;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\n\trpl_skb = get_skb(skb, sizeof(*rpl), GFP_KERNEL);\n\tif (!rpl_skb) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot allocate skb!\\n\",\n\t\t       __func__);\n\t\trelease = 1;\n\t\tgoto out;\n\t}\n\trpl_skb->priority = CPL_PRIORITY_DATA;\n\trpl = (struct cpl_abort_rpl *) skb_put(rpl_skb, sizeof(*rpl));\n\trpl->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_HOST_ABORT_CON_RPL));\n\trpl->wr.wr_lo = htonl(V_WR_TID(ep->hwtid));\n\tOPCODE_TID(rpl) = htonl(MK_OPCODE_TID(CPL_ABORT_RPL, ep->hwtid));\n\trpl->cmd = CPL_ABORT_NO_RST;\n\tiwch_cxgb3_ofld_send(ep->com.tdev, rpl_skb);\nout:\n\tif (release)\n\t\trelease_ep_resources(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int close_con_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\tstruct iwch_qp_attributes attrs;\n\tunsigned long flags;\n\tint release = 0;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tBUG_ON(!ep);\n\n\t/* The cm_id may be null if we failed to connect */\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tswitch (ep->com.state) {\n\tcase CLOSING:\n\t\t__state_set(&ep->com, MORIBUND);\n\t\tbreak;\n\tcase MORIBUND:\n\t\tstop_ep_timer(ep);\n\t\tif ((ep->com.cm_id) && (ep->com.qp)) {\n\t\t\tattrs.next_state = IWCH_QP_STATE_IDLE;\n\t\t\tiwch_modify_qp(ep->com.qp->rhp,\n\t\t\t\t\t     ep->com.qp,\n\t\t\t\t\t     IWCH_QP_ATTR_NEXT_STATE,\n\t\t\t\t\t     &attrs, 1);\n\t\t}\n\t\tclose_complete_upcall(ep);\n\t\t__state_set(&ep->com, DEAD);\n\t\trelease = 1;\n\t\tbreak;\n\tcase ABORTING:\n\tcase DEAD:\n\t\tbreak;\n\tdefault:\n\t\tBUG_ON(1);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (release)\n\t\trelease_ep_resources(ep);\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * T3A does 3 things when a TERM is received:\n * 1) send up a CPL_RDMA_TERMINATE message with the TERM packet\n * 2) generate an async event on the QP with the TERMINATE opcode\n * 3) post a TERMINATE opcode cqe into the associated CQ.\n *\n * For (1), we save the message in the qp for later consumer consumption.\n * For (2), we move the QP into TERMINATE, post a QP event and disconnect.\n * For (3), we toss the CQE in cxio_poll_cq().\n *\n * terminate() handles case (1)...\n */\nstatic int terminate(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep *ep = ctx;\n\n\tif (state_read(&ep->com) != FPDU_MODE)\n\t\treturn CPL_RET_BUF_DONE;\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tskb_pull(skb, sizeof(struct cpl_rdma_terminate));\n\tPDBG(\"%s saving %d bytes of term msg\\n\", __func__, skb->len);\n\tskb_copy_from_linear_data(skb, ep->com.qp->attr.terminate_buffer,\n\t\t\t\t  skb->len);\n\tep->com.qp->attr.terminate_msg_len = skb->len;\n\tep->com.qp->attr.is_terminate_local = 0;\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int ec_status(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct cpl_rdma_ec_status *rep = cplhdr(skb);\n\tstruct iwch_ep *ep = ctx;\n\n\tPDBG(\"%s ep %p tid %u status %d\\n\", __func__, ep, ep->hwtid,\n\t     rep->status);\n\tif (rep->status) {\n\t\tstruct iwch_qp_attributes attrs;\n\n\t\tprintk(KERN_ERR MOD \"%s BAD CLOSE - Aborting tid %u\\n\",\n\t\t       __func__, ep->hwtid);\n\t\tstop_ep_timer(ep);\n\t\tattrs.next_state = IWCH_QP_STATE_ERROR;\n\t\tiwch_modify_qp(ep->com.qp->rhp,\n\t\t\t       ep->com.qp, IWCH_QP_ATTR_NEXT_STATE,\n\t\t\t       &attrs, 1);\n\t\tabort_connection(ep, NULL, GFP_KERNEL);\n\t}\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic void ep_timeout(unsigned long arg)\n{\n\tstruct iwch_ep *ep = (struct iwch_ep *)arg;\n\tstruct iwch_qp_attributes attrs;\n\tunsigned long flags;\n\tint abort = 1;\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\tPDBG(\"%s ep %p tid %u state %d\\n\", __func__, ep, ep->hwtid,\n\t     ep->com.state);\n\tswitch (ep->com.state) {\n\tcase MPA_REQ_SENT:\n\t\t__state_set(&ep->com, ABORTING);\n\t\tconnect_reply_upcall(ep, -ETIMEDOUT);\n\t\tbreak;\n\tcase MPA_REQ_WAIT:\n\t\t__state_set(&ep->com, ABORTING);\n\t\tbreak;\n\tcase CLOSING:\n\tcase MORIBUND:\n\t\tif (ep->com.cm_id && ep->com.qp) {\n\t\t\tattrs.next_state = IWCH_QP_STATE_ERROR;\n\t\t\tiwch_modify_qp(ep->com.qp->rhp,\n\t\t\t\t     ep->com.qp, IWCH_QP_ATTR_NEXT_STATE,\n\t\t\t\t     &attrs, 1);\n\t\t}\n\t\t__state_set(&ep->com, ABORTING);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"%s unexpected state ep %p state %u\\n\",\n\t\t\t__func__, ep, ep->com.state);\n\t\tabort = 0;\n\t}\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (abort)\n\t\tabort_connection(ep, NULL, GFP_ATOMIC);\n\tput_ep(&ep->com);\n}\n\nint iwch_reject_cr(struct iw_cm_id *cm_id, const void *pdata, u8 pdata_len)\n{\n\tint err;\n\tstruct iwch_ep *ep = to_ep(cm_id);\n\tPDBG(\"%s ep %p tid %u\\n\", __func__, ep, ep->hwtid);\n\n\tif (state_read(&ep->com) == DEAD) {\n\t\tput_ep(&ep->com);\n\t\treturn -ECONNRESET;\n\t}\n\tBUG_ON(state_read(&ep->com) != MPA_REQ_RCVD);\n\tif (mpa_rev == 0)\n\t\tabort_connection(ep, NULL, GFP_KERNEL);\n\telse {\n\t\terr = send_mpa_reject(ep, pdata, pdata_len);\n\t\terr = iwch_ep_disconnect(ep, 0, GFP_KERNEL);\n\t}\n\tput_ep(&ep->com);\n\treturn 0;\n}\n\nint iwch_accept_cr(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param)\n{\n\tint err;\n\tstruct iwch_qp_attributes attrs;\n\tenum iwch_qp_attr_mask mask;\n\tstruct iwch_ep *ep = to_ep(cm_id);\n\tstruct iwch_dev *h = to_iwch_dev(cm_id->device);\n\tstruct iwch_qp *qp = get_qhp(h, conn_param->qpn);\n\n\tPDBG(\"%s ep %p tid %u\\n\", __func__, ep, ep->hwtid);\n\tif (state_read(&ep->com) == DEAD) {\n\t\terr = -ECONNRESET;\n\t\tgoto err;\n\t}\n\n\tBUG_ON(state_read(&ep->com) != MPA_REQ_RCVD);\n\tBUG_ON(!qp);\n\n\tif ((conn_param->ord > qp->rhp->attr.max_rdma_read_qp_depth) ||\n\t    (conn_param->ird > qp->rhp->attr.max_rdma_reads_per_qp)) {\n\t\tabort_connection(ep, NULL, GFP_KERNEL);\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tcm_id->add_ref(cm_id);\n\tep->com.cm_id = cm_id;\n\tep->com.qp = qp;\n\n\tep->ird = conn_param->ird;\n\tep->ord = conn_param->ord;\n\n\tif (peer2peer && ep->ird == 0)\n\t\tep->ird = 1;\n\n\tPDBG(\"%s %d ird %d ord %d\\n\", __func__, __LINE__, ep->ird, ep->ord);\n\n\t/* bind QP to EP and move to RTS */\n\tattrs.mpa_attr = ep->mpa_attr;\n\tattrs.max_ird = ep->ird;\n\tattrs.max_ord = ep->ord;\n\tattrs.llp_stream_handle = ep;\n\tattrs.next_state = IWCH_QP_STATE_RTS;\n\n\t/* bind QP and TID with INIT_WR */\n\tmask = IWCH_QP_ATTR_NEXT_STATE |\n\t\t\t     IWCH_QP_ATTR_LLP_STREAM_HANDLE |\n\t\t\t     IWCH_QP_ATTR_MPA_ATTR |\n\t\t\t     IWCH_QP_ATTR_MAX_IRD |\n\t\t\t     IWCH_QP_ATTR_MAX_ORD;\n\n\terr = iwch_modify_qp(ep->com.qp->rhp,\n\t\t\t     ep->com.qp, mask, &attrs, 1);\n\tif (err)\n\t\tgoto err1;\n\n\t/* if needed, wait for wr_ack */\n\tif (iwch_rqes_posted(qp)) {\n\t\twait_event(ep->com.waitq, ep->com.rpl_done);\n\t\terr = ep->com.rpl_err;\n\t\tif (err)\n\t\t\tgoto err1;\n\t}\n\n\terr = send_mpa_reply(ep, conn_param->private_data,\n\t\t\t     conn_param->private_data_len);\n\tif (err)\n\t\tgoto err1;\n\n\n\tstate_set(&ep->com, FPDU_MODE);\n\testablished_upcall(ep);\n\tput_ep(&ep->com);\n\treturn 0;\nerr1:\n\tep->com.cm_id = NULL;\n\tep->com.qp = NULL;\n\tcm_id->rem_ref(cm_id);\nerr:\n\tput_ep(&ep->com);\n\treturn err;\n}\n\nstatic int is_loopback_dst(struct iw_cm_id *cm_id)\n{\n\tstruct net_device *dev;\n\tstruct sockaddr_in *raddr = (struct sockaddr_in *)&cm_id->remote_addr;\n\n\tdev = ip_dev_find(&init_net, raddr->sin_addr.s_addr);\n\tif (!dev)\n\t\treturn 0;\n\tdev_put(dev);\n\treturn 1;\n}\n\nint iwch_connect(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param)\n{\n\tstruct iwch_dev *h = to_iwch_dev(cm_id->device);\n\tstruct iwch_ep *ep;\n\tstruct rtable *rt;\n\tint err = 0;\n\tstruct sockaddr_in *laddr = (struct sockaddr_in *)&cm_id->local_addr;\n\tstruct sockaddr_in *raddr = (struct sockaddr_in *)&cm_id->remote_addr;\n\n\tif (cm_id->remote_addr.ss_family != PF_INET) {\n\t\terr = -ENOSYS;\n\t\tgoto out;\n\t}\n\n\tif (is_loopback_dst(cm_id)) {\n\t\terr = -ENOSYS;\n\t\tgoto out;\n\t}\n\n\tep = alloc_ep(sizeof(*ep), GFP_KERNEL);\n\tif (!ep) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc ep.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tinit_timer(&ep->timer);\n\tep->plen = conn_param->private_data_len;\n\tif (ep->plen)\n\t\tmemcpy(ep->mpa_pkt + sizeof(struct mpa_message),\n\t\t       conn_param->private_data, ep->plen);\n\tep->ird = conn_param->ird;\n\tep->ord = conn_param->ord;\n\n\tif (peer2peer && ep->ord == 0)\n\t\tep->ord = 1;\n\n\tep->com.tdev = h->rdev.t3cdev_p;\n\n\tcm_id->add_ref(cm_id);\n\tep->com.cm_id = cm_id;\n\tep->com.qp = get_qhp(h, conn_param->qpn);\n\tBUG_ON(!ep->com.qp);\n\tPDBG(\"%s qpn 0x%x qp %p cm_id %p\\n\", __func__, conn_param->qpn,\n\t     ep->com.qp, cm_id);\n\n\t/*\n\t * Allocate an active TID to initiate a TCP connection.\n\t */\n\tep->atid = cxgb3_alloc_atid(h->rdev.t3cdev_p, &t3c_client, ep);\n\tif (ep->atid == -1) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc atid.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto fail2;\n\t}\n\n\t/* find a route */\n\trt = find_route(h->rdev.t3cdev_p, laddr->sin_addr.s_addr,\n\t\t\traddr->sin_addr.s_addr, laddr->sin_port,\n\t\t\traddr->sin_port, IPTOS_LOWDELAY);\n\tif (!rt) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot find route.\\n\", __func__);\n\t\terr = -EHOSTUNREACH;\n\t\tgoto fail3;\n\t}\n\tep->dst = &rt->dst;\n\tep->l2t = t3_l2t_get(ep->com.tdev, ep->dst, NULL,\n\t\t\t     &raddr->sin_addr.s_addr);\n\tif (!ep->l2t) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc l2e.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto fail4;\n\t}\n\n\tstate_set(&ep->com, CONNECTING);\n\tep->tos = IPTOS_LOWDELAY;\n\tmemcpy(&ep->com.local_addr, &cm_id->local_addr,\n\t       sizeof(ep->com.local_addr));\n\tmemcpy(&ep->com.remote_addr, &cm_id->remote_addr,\n\t       sizeof(ep->com.remote_addr));\n\n\t/* send connect request to rnic */\n\terr = send_connect(ep);\n\tif (!err)\n\t\tgoto out;\n\n\tl2t_release(h->rdev.t3cdev_p, ep->l2t);\nfail4:\n\tdst_release(ep->dst);\nfail3:\n\tcxgb3_free_atid(ep->com.tdev, ep->atid);\nfail2:\n\tcm_id->rem_ref(cm_id);\n\tput_ep(&ep->com);\nout:\n\treturn err;\n}\n\nint iwch_create_listen(struct iw_cm_id *cm_id, int backlog)\n{\n\tint err = 0;\n\tstruct iwch_dev *h = to_iwch_dev(cm_id->device);\n\tstruct iwch_listen_ep *ep;\n\n\n\tmight_sleep();\n\n\tif (cm_id->local_addr.ss_family != PF_INET) {\n\t\terr = -ENOSYS;\n\t\tgoto fail1;\n\t}\n\n\tep = alloc_ep(sizeof(*ep), GFP_KERNEL);\n\tif (!ep) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc ep.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto fail1;\n\t}\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\tep->com.tdev = h->rdev.t3cdev_p;\n\tcm_id->add_ref(cm_id);\n\tep->com.cm_id = cm_id;\n\tep->backlog = backlog;\n\tmemcpy(&ep->com.local_addr, &cm_id->local_addr,\n\t       sizeof(ep->com.local_addr));\n\n\t/*\n\t * Allocate a server TID.\n\t */\n\tep->stid = cxgb3_alloc_stid(h->rdev.t3cdev_p, &t3c_client, ep);\n\tif (ep->stid == -1) {\n\t\tprintk(KERN_ERR MOD \"%s - cannot alloc atid.\\n\", __func__);\n\t\terr = -ENOMEM;\n\t\tgoto fail2;\n\t}\n\n\tstate_set(&ep->com, LISTEN);\n\terr = listen_start(ep);\n\tif (err)\n\t\tgoto fail3;\n\n\t/* wait for pass_open_rpl */\n\twait_event(ep->com.waitq, ep->com.rpl_done);\n\terr = ep->com.rpl_err;\n\tif (!err) {\n\t\tcm_id->provider_data = ep;\n\t\tgoto out;\n\t}\nfail3:\n\tcxgb3_free_stid(ep->com.tdev, ep->stid);\nfail2:\n\tcm_id->rem_ref(cm_id);\n\tput_ep(&ep->com);\nfail1:\nout:\n\treturn err;\n}\n\nint iwch_destroy_listen(struct iw_cm_id *cm_id)\n{\n\tint err;\n\tstruct iwch_listen_ep *ep = to_listen_ep(cm_id);\n\n\tPDBG(\"%s ep %p\\n\", __func__, ep);\n\n\tmight_sleep();\n\tstate_set(&ep->com, DEAD);\n\tep->com.rpl_done = 0;\n\tep->com.rpl_err = 0;\n\terr = listen_stop(ep);\n\tif (err)\n\t\tgoto done;\n\twait_event(ep->com.waitq, ep->com.rpl_done);\n\tcxgb3_free_stid(ep->com.tdev, ep->stid);\ndone:\n\terr = ep->com.rpl_err;\n\tcm_id->rem_ref(cm_id);\n\tput_ep(&ep->com);\n\treturn err;\n}\n\nint iwch_ep_disconnect(struct iwch_ep *ep, int abrupt, gfp_t gfp)\n{\n\tint ret=0;\n\tunsigned long flags;\n\tint close = 0;\n\tint fatal = 0;\n\tstruct t3cdev *tdev;\n\tstruct cxio_rdev *rdev;\n\n\tspin_lock_irqsave(&ep->com.lock, flags);\n\n\tPDBG(\"%s ep %p state %s, abrupt %d\\n\", __func__, ep,\n\t     states[ep->com.state], abrupt);\n\n\ttdev = (struct t3cdev *)ep->com.tdev;\n\trdev = (struct cxio_rdev *)tdev->ulp;\n\tif (cxio_fatal_error(rdev)) {\n\t\tfatal = 1;\n\t\tclose_complete_upcall(ep);\n\t\tep->com.state = DEAD;\n\t}\n\tswitch (ep->com.state) {\n\tcase MPA_REQ_WAIT:\n\tcase MPA_REQ_SENT:\n\tcase MPA_REQ_RCVD:\n\tcase MPA_REP_SENT:\n\tcase FPDU_MODE:\n\t\tclose = 1;\n\t\tif (abrupt)\n\t\t\tep->com.state = ABORTING;\n\t\telse {\n\t\t\tep->com.state = CLOSING;\n\t\t\tstart_ep_timer(ep);\n\t\t}\n\t\tset_bit(CLOSE_SENT, &ep->com.flags);\n\t\tbreak;\n\tcase CLOSING:\n\t\tif (!test_and_set_bit(CLOSE_SENT, &ep->com.flags)) {\n\t\t\tclose = 1;\n\t\t\tif (abrupt) {\n\t\t\t\tstop_ep_timer(ep);\n\t\t\t\tep->com.state = ABORTING;\n\t\t\t} else\n\t\t\t\tep->com.state = MORIBUND;\n\t\t}\n\t\tbreak;\n\tcase MORIBUND:\n\tcase ABORTING:\n\tcase DEAD:\n\t\tPDBG(\"%s ignoring disconnect ep %p state %u\\n\",\n\t\t     __func__, ep, ep->com.state);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tspin_unlock_irqrestore(&ep->com.lock, flags);\n\tif (close) {\n\t\tif (abrupt)\n\t\t\tret = send_abort(ep, NULL, gfp);\n\t\telse\n\t\t\tret = send_halfclose(ep, gfp);\n\t\tif (ret)\n\t\t\tfatal = 1;\n\t}\n\tif (fatal)\n\t\trelease_ep_resources(ep);\n\treturn ret;\n}\n\nint iwch_ep_redirect(void *ctx, struct dst_entry *old, struct dst_entry *new,\n\t\t     struct l2t_entry *l2t)\n{\n\tstruct iwch_ep *ep = ctx;\n\n\tif (ep->dst != old)\n\t\treturn 0;\n\n\tPDBG(\"%s ep %p redirect to dst %p l2t %p\\n\", __func__, ep, new,\n\t     l2t);\n\tdst_hold(new);\n\tl2t_release(ep->com.tdev, ep->l2t);\n\tep->l2t = l2t;\n\tdst_release(old);\n\tep->dst = new;\n\treturn 1;\n}\n\n/*\n * All the CM events are handled on a work queue to have a safe context.\n * These are the real handlers that are called from the work queue.\n */\nstatic const cxgb3_cpl_handler_func work_handlers[NUM_CPL_CMDS] = {\n\t[CPL_ACT_ESTABLISH]\t= act_establish,\n\t[CPL_ACT_OPEN_RPL]\t= act_open_rpl,\n\t[CPL_RX_DATA]\t\t= rx_data,\n\t[CPL_TX_DMA_ACK]\t= tx_ack,\n\t[CPL_ABORT_RPL_RSS]\t= abort_rpl,\n\t[CPL_ABORT_RPL]\t\t= abort_rpl,\n\t[CPL_PASS_OPEN_RPL]\t= pass_open_rpl,\n\t[CPL_CLOSE_LISTSRV_RPL]\t= close_listsrv_rpl,\n\t[CPL_PASS_ACCEPT_REQ]\t= pass_accept_req,\n\t[CPL_PASS_ESTABLISH]\t= pass_establish,\n\t[CPL_PEER_CLOSE]\t= peer_close,\n\t[CPL_ABORT_REQ_RSS]\t= peer_abort,\n\t[CPL_CLOSE_CON_RPL]\t= close_con_rpl,\n\t[CPL_RDMA_TERMINATE]\t= terminate,\n\t[CPL_RDMA_EC_STATUS]\t= ec_status,\n};\n\nstatic void process_work(struct work_struct *work)\n{\n\tstruct sk_buff *skb = NULL;\n\tvoid *ep;\n\tstruct t3cdev *tdev;\n\tint ret;\n\n\twhile ((skb = skb_dequeue(&rxq))) {\n\t\tep = *((void **) (skb->cb));\n\t\ttdev = *((struct t3cdev **) (skb->cb + sizeof(void *)));\n\t\tret = work_handlers[G_OPCODE(ntohl((__force __be32)skb->csum))](tdev, skb, ep);\n\t\tif (ret & CPL_RET_BUF_DONE)\n\t\t\tkfree_skb(skb);\n\n\t\t/*\n\t\t * ep was referenced in sched(), and is freed here.\n\t\t */\n\t\tput_ep((struct iwch_ep_common *)ep);\n\t}\n}\n\nstatic DECLARE_WORK(skb_work, process_work);\n\nstatic int sched(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct iwch_ep_common *epc = ctx;\n\n\tget_ep(epc);\n\n\t/*\n\t * Save ctx and tdev in the skb->cb area.\n\t */\n\t*((void **) skb->cb) = ctx;\n\t*((struct t3cdev **) (skb->cb + sizeof(void *))) = tdev;\n\n\t/*\n\t * Queue the skb and schedule the worker thread.\n\t */\n\tskb_queue_tail(&rxq, skb);\n\tqueue_work(workq, &skb_work);\n\treturn 0;\n}\n\nstatic int set_tcb_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\n{\n\tstruct cpl_set_tcb_rpl *rpl = cplhdr(skb);\n\n\tif (rpl->status != CPL_ERR_NONE) {\n\t\tprintk(KERN_ERR MOD \"Unexpected SET_TCB_RPL status %u \"\n\t\t       \"for tid %u\\n\", rpl->status, GET_TID(rpl));\n\t}\n\treturn CPL_RET_BUF_DONE;\n}\n\n/*\n * All upcalls from the T3 Core go to sched() to schedule the\n * processing on a work queue.\n */\ncxgb3_cpl_handler_func t3c_handlers[NUM_CPL_CMDS] = {\n\t[CPL_ACT_ESTABLISH]\t= sched,\n\t[CPL_ACT_OPEN_RPL]\t= sched,\n\t[CPL_RX_DATA]\t\t= sched,\n\t[CPL_TX_DMA_ACK]\t= sched,\n\t[CPL_ABORT_RPL_RSS]\t= sched,\n\t[CPL_ABORT_RPL]\t\t= sched,\n\t[CPL_PASS_OPEN_RPL]\t= sched,\n\t[CPL_CLOSE_LISTSRV_RPL]\t= sched,\n\t[CPL_PASS_ACCEPT_REQ]\t= sched,\n\t[CPL_PASS_ESTABLISH]\t= sched,\n\t[CPL_PEER_CLOSE]\t= sched,\n\t[CPL_CLOSE_CON_RPL]\t= sched,\n\t[CPL_ABORT_REQ_RSS]\t= sched,\n\t[CPL_RDMA_TERMINATE]\t= sched,\n\t[CPL_RDMA_EC_STATUS]\t= sched,\n\t[CPL_SET_TCB_RPL]\t= set_tcb_rpl,\n};\n\nint __init iwch_cm_init(void)\n{\n\tskb_queue_head_init(&rxq);\n\n\tworkq = create_singlethread_workqueue(\"iw_cxgb3\");\n\tif (!workq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid __exit iwch_cm_term(void)\n{\n\tflush_workqueue(workq);\n\tdestroy_workqueue(workq);\n}\n"], "filenames": ["drivers/infiniband/hw/cxgb3/iwch_cm.c"], "buggy_code_start_loc": [152], "buggy_code_end_loc": [169], "fixing_code_start_loc": [152], "fixing_code_end_loc": [169], "type": "NVD-CWE-Other", "message": "drivers/infiniband/hw/cxgb3/iwch_cm.c in the Linux kernel before 4.5 does not properly identify error conditions, which allows remote attackers to execute arbitrary code or cause a denial of service (use-after-free) via crafted packets.", "other": {"cve": {"id": "CVE-2015-8812", "sourceIdentifier": "cve@mitre.org", "published": "2016-04-27T17:59:02.163", "lastModified": "2023-01-19T16:13:55.287", "vulnStatus": "Analyzed", "evaluatorComment": "CWE-416: Use After Free", "descriptions": [{"lang": "en", "value": "drivers/infiniband/hw/cxgb3/iwch_cm.c in the Linux kernel before 4.5 does not properly identify error conditions, which allows remote attackers to execute arbitrary code or cause a denial of service (use-after-free) via crafted packets."}, {"lang": "es", "value": "drivers/infiniband/hw/cxgb3/iwch_cm.c en el Kernel de Linux en versiones anteriores a 4.5 no identifica correctamente condiciones de error, lo que permite a atacantes remotos ejecutar c\u00f3digo arbitrario o provocar una denegaci\u00f3n de servicio (uso despu\u00e9s de liberaci\u00f3n de memoria) a trav\u00e9s de paquetes manipulados."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 10.0}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:novell:suse_linux_enterprise_real_time_extension:12:sp1:*:*:*:*:*:*", "matchCriteriaId": "5AB3CAA1-C20C-4A86-841E-EC0858164D7D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.2.78", "matchCriteriaId": "45C940E8-F2B4-4CD5-A435-3D895E7B9949"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.3", "versionEndExcluding": "3.10.99", "matchCriteriaId": "7CDC278B-3692-493D-8248-632B0BE1BA3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.11", "versionEndExcluding": "3.12.56", "matchCriteriaId": "8821755B-4B61-4C3F-9DF5-960A09A7CCC2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.14.63", "matchCriteriaId": "FED4D9EA-C42E-4811-BFA7-A62F04B44EC2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.15", "versionEndExcluding": "3.16.35", "matchCriteriaId": "7DC4BA70-B111-4D2E-BC78-6601CED68F08"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.31", "matchCriteriaId": "FEE3CC99-7C90-4991-B15A-BE015E5AE475"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.22", "matchCriteriaId": "D2D231A6-F06A-481A-8F4C-D1A7E1EC3742"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2.0", "versionEndExcluding": "4.4.4", "matchCriteriaId": "805EB381-FAD6-4338-A8A1-7FED67675C47"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:-:*:*:*", "matchCriteriaId": "CB66DB75-2B16-4EBF-9B93-CE49D8086E41"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:esm:*:*:*", "matchCriteriaId": "815D70A8-47D3-459C-A32C-9FEACA0659D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:15.10:*:*:*:*:*:*:*", "matchCriteriaId": "E88A537F-F4D0-46B9-9E37-965233C2A355"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=67f1aee6f45059fd6b0f5b0ecb2c97ad0451f6b3", "source": "cve@mitre.org", "tags": ["Vendor Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-03/msg00094.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00015.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00019.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00025.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00026.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00027.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00028.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00029.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00030.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00031.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00032.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00033.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00034.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00036.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00037.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00045.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-07/msg00005.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-08/msg00038.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2574.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2584.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.debian.org/security/2016/dsa-3503", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/02/11/1", "source": "cve@mitre.org", "tags": ["Mailing List", "Release Notes", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/83218", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.ubuntu.com/usn/USN-2946-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2946-2", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2947-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2947-2", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2947-3", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2948-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2948-2", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2949-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2967-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2967-2", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1303532", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/67f1aee6f45059fd6b0f5b0ecb2c97ad0451f6b3", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/67f1aee6f45059fd6b0f5b0ecb2c97ad0451f6b3"}}