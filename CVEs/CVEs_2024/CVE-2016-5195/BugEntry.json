{"buggy_code": ["#ifndef _LINUX_MM_H\n#define _LINUX_MM_H\n\n#include <linux/errno.h>\n\n#ifdef __KERNEL__\n\n#include <linux/mmdebug.h>\n#include <linux/gfp.h>\n#include <linux/bug.h>\n#include <linux/list.h>\n#include <linux/mmzone.h>\n#include <linux/rbtree.h>\n#include <linux/atomic.h>\n#include <linux/debug_locks.h>\n#include <linux/mm_types.h>\n#include <linux/range.h>\n#include <linux/pfn.h>\n#include <linux/percpu-refcount.h>\n#include <linux/bit_spinlock.h>\n#include <linux/shrinker.h>\n#include <linux/resource.h>\n#include <linux/page_ext.h>\n#include <linux/err.h>\n#include <linux/page_ref.h>\n\nstruct mempolicy;\nstruct anon_vma;\nstruct anon_vma_chain;\nstruct file_ra_state;\nstruct user_struct;\nstruct writeback_control;\nstruct bdi_writeback;\n\n#ifndef CONFIG_NEED_MULTIPLE_NODES\t/* Don't use mapnrs, do it properly */\nextern unsigned long max_mapnr;\n\nstatic inline void set_max_mapnr(unsigned long limit)\n{\n\tmax_mapnr = limit;\n}\n#else\nstatic inline void set_max_mapnr(unsigned long limit) { }\n#endif\n\nextern unsigned long totalram_pages;\nextern void * high_memory;\nextern int page_cluster;\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_legacy_va_layout;\n#else\n#define sysctl_legacy_va_layout 0\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\nextern const int mmap_rnd_bits_min;\nextern const int mmap_rnd_bits_max;\nextern int mmap_rnd_bits __read_mostly;\n#endif\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\nextern const int mmap_rnd_compat_bits_min;\nextern const int mmap_rnd_compat_bits_max;\nextern int mmap_rnd_compat_bits __read_mostly;\n#endif\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/processor.h>\n\n#ifndef __pa_symbol\n#define __pa_symbol(x)  __pa(RELOC_HIDE((unsigned long)(x), 0))\n#endif\n\n#ifndef page_to_virt\n#define page_to_virt(x)\t__va(PFN_PHYS(page_to_pfn(x)))\n#endif\n\n/*\n * To prevent common memory management code establishing\n * a zero page mapping on a read fault.\n * This macro should be defined within <asm/pgtable.h>.\n * s390 does this to prevent multiplexing of hardware bits\n * related to the physical page in case of virtualization.\n */\n#ifndef mm_forbids_zeropage\n#define mm_forbids_zeropage(X)\t(0)\n#endif\n\n/*\n * Default maximum number of active map areas, this limits the number of vmas\n * per mm struct. Users can overwrite this number by sysctl but there is a\n * problem.\n *\n * When a program's coredump is generated as ELF format, a section is created\n * per a vma. In ELF, the number of sections is represented in unsigned short.\n * This means the number of sections should be smaller than 65535 at coredump.\n * Because the kernel adds some informative sections to a image of program at\n * generating coredump, we need some margin. The number of extra sections is\n * 1-3 now and depends on arch. We use \"5\" as safe margin, here.\n *\n * ELF extended numbering allows more than 65535 sections, so 16-bit bound is\n * not a hard limit any more. Although some userspace tools can be surprised by\n * that.\n */\n#define MAPCOUNT_ELF_CORE_MARGIN\t(5)\n#define DEFAULT_MAX_MAP_COUNT\t(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)\n\nextern int sysctl_max_map_count;\n\nextern unsigned long sysctl_user_reserve_kbytes;\nextern unsigned long sysctl_admin_reserve_kbytes;\n\nextern int sysctl_overcommit_memory;\nextern int sysctl_overcommit_ratio;\nextern unsigned long sysctl_overcommit_kbytes;\n\nextern int overcommit_ratio_handler(struct ctl_table *, int, void __user *,\n\t\t\t\t    size_t *, loff_t *);\nextern int overcommit_kbytes_handler(struct ctl_table *, int, void __user *,\n\t\t\t\t    size_t *, loff_t *);\n\n#define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))\n\n/* to align the pointer to the (next) page boundary */\n#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)\n\n/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */\n#define PAGE_ALIGNED(addr)\tIS_ALIGNED((unsigned long)(addr), PAGE_SIZE)\n\n/*\n * Linux kernel virtual memory manager primitives.\n * The idea being to have a \"virtual\" mm in the same way\n * we have a virtual fs - giving a cleaner interface to the\n * mm details, and allowing different kinds of memory mappings\n * (from shared memory to executable loading to arbitrary\n * mmap() functions).\n */\n\nextern struct kmem_cache *vm_area_cachep;\n\n#ifndef CONFIG_MMU\nextern struct rb_root nommu_region_tree;\nextern struct rw_semaphore nommu_region_sem;\n\nextern unsigned int kobjsize(const void *objp);\n#endif\n\n/*\n * vm_flags in vm_area_struct, see mm_types.h.\n * When changing, update also include/trace/events/mmflags.h\n */\n#define VM_NONE\t\t0x00000000\n\n#define VM_READ\t\t0x00000001\t/* currently active flags */\n#define VM_WRITE\t0x00000002\n#define VM_EXEC\t\t0x00000004\n#define VM_SHARED\t0x00000008\n\n/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */\n#define VM_MAYREAD\t0x00000010\t/* limits for mprotect() etc */\n#define VM_MAYWRITE\t0x00000020\n#define VM_MAYEXEC\t0x00000040\n#define VM_MAYSHARE\t0x00000080\n\n#define VM_GROWSDOWN\t0x00000100\t/* general info on the segment */\n#define VM_UFFD_MISSING\t0x00000200\t/* missing pages tracking */\n#define VM_PFNMAP\t0x00000400\t/* Page-ranges managed without \"struct page\", just pure PFN */\n#define VM_DENYWRITE\t0x00000800\t/* ETXTBSY on write attempts.. */\n#define VM_UFFD_WP\t0x00001000\t/* wrprotect pages tracking */\n\n#define VM_LOCKED\t0x00002000\n#define VM_IO           0x00004000\t/* Memory mapped I/O or similar */\n\n\t\t\t\t\t/* Used by sys_madvise() */\n#define VM_SEQ_READ\t0x00008000\t/* App will access data sequentially */\n#define VM_RAND_READ\t0x00010000\t/* App will not benefit from clustered reads */\n\n#define VM_DONTCOPY\t0x00020000      /* Do not copy this vma on fork */\n#define VM_DONTEXPAND\t0x00040000\t/* Cannot expand with mremap() */\n#define VM_LOCKONFAULT\t0x00080000\t/* Lock the pages covered when they are faulted in */\n#define VM_ACCOUNT\t0x00100000\t/* Is a VM accounted object */\n#define VM_NORESERVE\t0x00200000\t/* should the VM suppress accounting */\n#define VM_HUGETLB\t0x00400000\t/* Huge TLB Page VM */\n#define VM_ARCH_1\t0x01000000\t/* Architecture-specific flag */\n#define VM_ARCH_2\t0x02000000\n#define VM_DONTDUMP\t0x04000000\t/* Do not include in the core dump */\n\n#ifdef CONFIG_MEM_SOFT_DIRTY\n# define VM_SOFTDIRTY\t0x08000000\t/* Not soft dirty clean area */\n#else\n# define VM_SOFTDIRTY\t0\n#endif\n\n#define VM_MIXEDMAP\t0x10000000\t/* Can contain \"struct page\" and pure PFN pages */\n#define VM_HUGEPAGE\t0x20000000\t/* MADV_HUGEPAGE marked this vma */\n#define VM_NOHUGEPAGE\t0x40000000\t/* MADV_NOHUGEPAGE marked this vma */\n#define VM_MERGEABLE\t0x80000000\t/* KSM may merge identical pages */\n\n#ifdef CONFIG_ARCH_USES_HIGH_VMA_FLAGS\n#define VM_HIGH_ARCH_BIT_0\t32\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_1\t33\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_2\t34\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_3\t35\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_0\tBIT(VM_HIGH_ARCH_BIT_0)\n#define VM_HIGH_ARCH_1\tBIT(VM_HIGH_ARCH_BIT_1)\n#define VM_HIGH_ARCH_2\tBIT(VM_HIGH_ARCH_BIT_2)\n#define VM_HIGH_ARCH_3\tBIT(VM_HIGH_ARCH_BIT_3)\n#endif /* CONFIG_ARCH_USES_HIGH_VMA_FLAGS */\n\n#if defined(CONFIG_X86)\n# define VM_PAT\t\tVM_ARCH_1\t/* PAT reserves whole VMA at once (x86) */\n#if defined (CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS)\n# define VM_PKEY_SHIFT\tVM_HIGH_ARCH_BIT_0\n# define VM_PKEY_BIT0\tVM_HIGH_ARCH_0\t/* A protection key is a 4-bit value */\n# define VM_PKEY_BIT1\tVM_HIGH_ARCH_1\n# define VM_PKEY_BIT2\tVM_HIGH_ARCH_2\n# define VM_PKEY_BIT3\tVM_HIGH_ARCH_3\n#endif\n#elif defined(CONFIG_PPC)\n# define VM_SAO\t\tVM_ARCH_1\t/* Strong Access Ordering (powerpc) */\n#elif defined(CONFIG_PARISC)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif defined(CONFIG_METAG)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif defined(CONFIG_IA64)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif !defined(CONFIG_MMU)\n# define VM_MAPPED_COPY\tVM_ARCH_1\t/* T if mapped copy of data (nommu mmap) */\n#endif\n\n#if defined(CONFIG_X86)\n/* MPX specific bounds table or bounds directory */\n# define VM_MPX\t\tVM_ARCH_2\n#endif\n\n#ifndef VM_GROWSUP\n# define VM_GROWSUP\tVM_NONE\n#endif\n\n/* Bits set in the VMA until the stack is in its final location */\n#define VM_STACK_INCOMPLETE_SETUP\t(VM_RAND_READ | VM_SEQ_READ)\n\n#ifndef VM_STACK_DEFAULT_FLAGS\t\t/* arch can override this */\n#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS\n#endif\n\n#ifdef CONFIG_STACK_GROWSUP\n#define VM_STACK\tVM_GROWSUP\n#else\n#define VM_STACK\tVM_GROWSDOWN\n#endif\n\n#define VM_STACK_FLAGS\t(VM_STACK | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)\n\n/*\n * Special vmas that are non-mergable, non-mlock()able.\n * Note: mm/huge_memory.c VM_NO_THP depends on this definition.\n */\n#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_PFNMAP | VM_MIXEDMAP)\n\n/* This mask defines which mm->def_flags a process can inherit its parent */\n#define VM_INIT_DEF_MASK\tVM_NOHUGEPAGE\n\n/* This mask is used to clear all the VMA flags used by mlock */\n#define VM_LOCKED_CLEAR_MASK\t(~(VM_LOCKED | VM_LOCKONFAULT))\n\n/*\n * mapping from the currently active vm_flags protection bits (the\n * low four bits) to a page protection mask..\n */\nextern pgprot_t protection_map[16];\n\n#define FAULT_FLAG_WRITE\t0x01\t/* Fault was a write access */\n#define FAULT_FLAG_MKWRITE\t0x02\t/* Fault was mkwrite of existing pte */\n#define FAULT_FLAG_ALLOW_RETRY\t0x04\t/* Retry fault if blocking */\n#define FAULT_FLAG_RETRY_NOWAIT\t0x08\t/* Don't drop mmap_sem and wait when retrying */\n#define FAULT_FLAG_KILLABLE\t0x10\t/* The fault task is in SIGKILL killable region */\n#define FAULT_FLAG_TRIED\t0x20\t/* Second try */\n#define FAULT_FLAG_USER\t\t0x40\t/* The fault originated in userspace */\n#define FAULT_FLAG_REMOTE\t0x80\t/* faulting for non current tsk/mm */\n#define FAULT_FLAG_INSTRUCTION  0x100\t/* The fault was during an instruction fetch */\n\n/*\n * vm_fault is filled by the the pagefault handler and passed to the vma's\n * ->fault function. The vma's ->fault is responsible for returning a bitmask\n * of VM_FAULT_xxx flags that give details about how the fault was handled.\n *\n * MM layer fills up gfp_mask for page allocations but fault handler might\n * alter it if its implementation requires a different allocation context.\n *\n * pgoff should be used in favour of virtual_address, if possible.\n */\nstruct vm_fault {\n\tunsigned int flags;\t\t/* FAULT_FLAG_xxx flags */\n\tgfp_t gfp_mask;\t\t\t/* gfp mask to be used for allocations */\n\tpgoff_t pgoff;\t\t\t/* Logical page offset based on vma */\n\tvoid __user *virtual_address;\t/* Faulting virtual address */\n\n\tstruct page *cow_page;\t\t/* Handler may choose to COW */\n\tstruct page *page;\t\t/* ->fault handlers should return a\n\t\t\t\t\t * page here, unless VM_FAULT_NOPAGE\n\t\t\t\t\t * is set (which is also implied by\n\t\t\t\t\t * VM_FAULT_ERROR).\n\t\t\t\t\t */\n\tvoid *entry;\t\t\t/* ->fault handler can alternatively\n\t\t\t\t\t * return locked DAX entry. In that\n\t\t\t\t\t * case handler should return\n\t\t\t\t\t * VM_FAULT_DAX_LOCKED and fill in\n\t\t\t\t\t * entry here.\n\t\t\t\t\t */\n};\n\n/*\n * Page fault context: passes though page fault handler instead of endless list\n * of function arguments.\n */\nstruct fault_env {\n\tstruct vm_area_struct *vma;\t/* Target VMA */\n\tunsigned long address;\t\t/* Faulting virtual address */\n\tunsigned int flags;\t\t/* FAULT_FLAG_xxx flags */\n\tpmd_t *pmd;\t\t\t/* Pointer to pmd entry matching\n\t\t\t\t\t * the 'address'\n\t\t\t\t\t */\n\tpte_t *pte;\t\t\t/* Pointer to pte entry matching\n\t\t\t\t\t * the 'address'. NULL if the page\n\t\t\t\t\t * table hasn't been allocated.\n\t\t\t\t\t */\n\tspinlock_t *ptl;\t\t/* Page table lock.\n\t\t\t\t\t * Protects pte page table if 'pte'\n\t\t\t\t\t * is not NULL, otherwise pmd.\n\t\t\t\t\t */\n\tpgtable_t prealloc_pte;\t\t/* Pre-allocated pte page table.\n\t\t\t\t\t * vm_ops->map_pages() calls\n\t\t\t\t\t * alloc_set_pte() from atomic context.\n\t\t\t\t\t * do_fault_around() pre-allocates\n\t\t\t\t\t * page table to avoid allocation from\n\t\t\t\t\t * atomic context.\n\t\t\t\t\t */\n};\n\n/*\n * These are the virtual MM functions - opening of an area, closing and\n * unmapping it (needed to keep files on disk up-to-date etc), pointer\n * to the functions called when a no-page or a wp-page exception occurs. \n */\nstruct vm_operations_struct {\n\tvoid (*open)(struct vm_area_struct * area);\n\tvoid (*close)(struct vm_area_struct * area);\n\tint (*mremap)(struct vm_area_struct * area);\n\tint (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\tint (*pmd_fault)(struct vm_area_struct *, unsigned long address,\n\t\t\t\t\t\tpmd_t *, unsigned int flags);\n\tvoid (*map_pages)(struct fault_env *fe,\n\t\t\tpgoff_t start_pgoff, pgoff_t end_pgoff);\n\n\t/* notification that a previously read-only page is about to become\n\t * writable, if an error is returned it will cause a SIGBUS */\n\tint (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n\t/* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */\n\tint (*pfn_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n\t/* called by access_process_vm when get_user_pages() fails, typically\n\t * for use by special VMAs that can switch between memory and hardware\n\t */\n\tint (*access)(struct vm_area_struct *vma, unsigned long addr,\n\t\t      void *buf, int len, int write);\n\n\t/* Called by the /proc/PID/maps code to ask the vma whether it\n\t * has a special name.  Returning non-NULL will also cause this\n\t * vma to be dumped unconditionally. */\n\tconst char *(*name)(struct vm_area_struct *vma);\n\n#ifdef CONFIG_NUMA\n\t/*\n\t * set_policy() op must add a reference to any non-NULL @new mempolicy\n\t * to hold the policy upon return.  Caller should pass NULL @new to\n\t * remove a policy and fall back to surrounding context--i.e. do not\n\t * install a MPOL_DEFAULT policy, nor the task or system default\n\t * mempolicy.\n\t */\n\tint (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);\n\n\t/*\n\t * get_policy() op must add reference [mpol_get()] to any policy at\n\t * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure\n\t * in mm/mempolicy.c will do this automatically.\n\t * get_policy() must NOT add a ref if the policy at (vma,addr) is not\n\t * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.\n\t * If no [shared/vma] mempolicy exists at the addr, get_policy() op\n\t * must return NULL--i.e., do not \"fallback\" to task or system default\n\t * policy.\n\t */\n\tstruct mempolicy *(*get_policy)(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long addr);\n#endif\n\t/*\n\t * Called by vm_normal_page() for special PTEs to find the\n\t * page for @addr.  This is useful if the default behavior\n\t * (using pte_page()) would not find the correct page.\n\t */\n\tstruct page *(*find_special_page)(struct vm_area_struct *vma,\n\t\t\t\t\t  unsigned long addr);\n};\n\nstruct mmu_gather;\nstruct inode;\n\n#define page_private(page)\t\t((page)->private)\n#define set_page_private(page, v)\t((page)->private = (v))\n\n#if !defined(__HAVE_ARCH_PTE_DEVMAP) || !defined(CONFIG_TRANSPARENT_HUGEPAGE)\nstatic inline int pmd_devmap(pmd_t pmd)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * FIXME: take this include out, include page-flags.h in\n * files which need it (119 of them)\n */\n#include <linux/page-flags.h>\n#include <linux/huge_mm.h>\n\n/*\n * Methods to modify the page usage count.\n *\n * What counts for a page usage:\n * - cache mapping   (page->mapping)\n * - private data    (page->private)\n * - page mapped in a task's page tables, each mapping\n *   is counted separately\n *\n * Also, many kernel routines increase the page count before a critical\n * routine so they can be sure the page doesn't go away from under them.\n */\n\n/*\n * Drop a ref, return true if the refcount fell to zero (the page has no users)\n */\nstatic inline int put_page_testzero(struct page *page)\n{\n\tVM_BUG_ON_PAGE(page_ref_count(page) == 0, page);\n\treturn page_ref_dec_and_test(page);\n}\n\n/*\n * Try to grab a ref unless the page has a refcount of zero, return false if\n * that is the case.\n * This can be called when MMU is off so it must not access\n * any of the virtual mappings.\n */\nstatic inline int get_page_unless_zero(struct page *page)\n{\n\treturn page_ref_add_unless(page, 1, 0);\n}\n\nextern int page_is_ram(unsigned long pfn);\n\nenum {\n\tREGION_INTERSECTS,\n\tREGION_DISJOINT,\n\tREGION_MIXED,\n};\n\nint region_intersects(resource_size_t offset, size_t size, unsigned long flags,\n\t\t      unsigned long desc);\n\n/* Support for virtually mapped pages */\nstruct page *vmalloc_to_page(const void *addr);\nunsigned long vmalloc_to_pfn(const void *addr);\n\n/*\n * Determine if an address is within the vmalloc range\n *\n * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there\n * is no special casing required.\n */\nstatic inline bool is_vmalloc_addr(const void *x)\n{\n#ifdef CONFIG_MMU\n\tunsigned long addr = (unsigned long)x;\n\n\treturn addr >= VMALLOC_START && addr < VMALLOC_END;\n#else\n\treturn false;\n#endif\n}\n#ifdef CONFIG_MMU\nextern int is_vmalloc_or_module_addr(const void *x);\n#else\nstatic inline int is_vmalloc_or_module_addr(const void *x)\n{\n\treturn 0;\n}\n#endif\n\nextern void kvfree(const void *addr);\n\nstatic inline atomic_t *compound_mapcount_ptr(struct page *page)\n{\n\treturn &page[1].compound_mapcount;\n}\n\nstatic inline int compound_mapcount(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageCompound(page), page);\n\tpage = compound_head(page);\n\treturn atomic_read(compound_mapcount_ptr(page)) + 1;\n}\n\n/*\n * The atomic page->_mapcount, starts from -1: so that transitions\n * both from it and to it can be tracked, using atomic_inc_and_test\n * and atomic_add_negative(-1).\n */\nstatic inline void page_mapcount_reset(struct page *page)\n{\n\tatomic_set(&(page)->_mapcount, -1);\n}\n\nint __page_mapcount(struct page *page);\n\nstatic inline int page_mapcount(struct page *page)\n{\n\tVM_BUG_ON_PAGE(PageSlab(page), page);\n\n\tif (unlikely(PageCompound(page)))\n\t\treturn __page_mapcount(page);\n\treturn atomic_read(&page->_mapcount) + 1;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nint total_mapcount(struct page *page);\nint page_trans_huge_mapcount(struct page *page, int *total_mapcount);\n#else\nstatic inline int total_mapcount(struct page *page)\n{\n\treturn page_mapcount(page);\n}\nstatic inline int page_trans_huge_mapcount(struct page *page,\n\t\t\t\t\t   int *total_mapcount)\n{\n\tint mapcount = page_mapcount(page);\n\tif (total_mapcount)\n\t\t*total_mapcount = mapcount;\n\treturn mapcount;\n}\n#endif\n\nstatic inline struct page *virt_to_head_page(const void *x)\n{\n\tstruct page *page = virt_to_page(x);\n\n\treturn compound_head(page);\n}\n\nvoid __put_page(struct page *page);\n\nvoid put_pages_list(struct list_head *pages);\n\nvoid split_page(struct page *page, unsigned int order);\n\n/*\n * Compound pages have a destructor function.  Provide a\n * prototype for that function and accessor functions.\n * These are _only_ valid on the head of a compound page.\n */\ntypedef void compound_page_dtor(struct page *);\n\n/* Keep the enum in sync with compound_page_dtors array in mm/page_alloc.c */\nenum compound_dtor_id {\n\tNULL_COMPOUND_DTOR,\n\tCOMPOUND_PAGE_DTOR,\n#ifdef CONFIG_HUGETLB_PAGE\n\tHUGETLB_PAGE_DTOR,\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tTRANSHUGE_PAGE_DTOR,\n#endif\n\tNR_COMPOUND_DTORS,\n};\nextern compound_page_dtor * const compound_page_dtors[];\n\nstatic inline void set_compound_page_dtor(struct page *page,\n\t\tenum compound_dtor_id compound_dtor)\n{\n\tVM_BUG_ON_PAGE(compound_dtor >= NR_COMPOUND_DTORS, page);\n\tpage[1].compound_dtor = compound_dtor;\n}\n\nstatic inline compound_page_dtor *get_compound_page_dtor(struct page *page)\n{\n\tVM_BUG_ON_PAGE(page[1].compound_dtor >= NR_COMPOUND_DTORS, page);\n\treturn compound_page_dtors[page[1].compound_dtor];\n}\n\nstatic inline unsigned int compound_order(struct page *page)\n{\n\tif (!PageHead(page))\n\t\treturn 0;\n\treturn page[1].compound_order;\n}\n\nstatic inline void set_compound_order(struct page *page, unsigned int order)\n{\n\tpage[1].compound_order = order;\n}\n\nvoid free_compound_page(struct page *page);\n\n#ifdef CONFIG_MMU\n/*\n * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when\n * servicing faults for write access.  In the normal case, do always want\n * pte_mkwrite.  But get_user_pages can cause write faults for mappings\n * that do not have writing enabled, when used by access_process_vm.\n */\nstatic inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)\n{\n\tif (likely(vma->vm_flags & VM_WRITE))\n\t\tpte = pte_mkwrite(pte);\n\treturn pte;\n}\n\nint alloc_set_pte(struct fault_env *fe, struct mem_cgroup *memcg,\n\t\tstruct page *page);\n#endif\n\n/*\n * Multiple processes may \"see\" the same page. E.g. for untouched\n * mappings of /dev/null, all processes see the same page full of\n * zeroes, and text pages of executables and shared libraries have\n * only one copy in memory, at most, normally.\n *\n * For the non-reserved pages, page_count(page) denotes a reference count.\n *   page_count() == 0 means the page is free. page->lru is then used for\n *   freelist management in the buddy allocator.\n *   page_count() > 0  means the page has been allocated.\n *\n * Pages are allocated by the slab allocator in order to provide memory\n * to kmalloc and kmem_cache_alloc. In this case, the management of the\n * page, and the fields in 'struct page' are the responsibility of mm/slab.c\n * unless a particular usage is carefully commented. (the responsibility of\n * freeing the kmalloc memory is the caller's, of course).\n *\n * A page may be used by anyone else who does a __get_free_page().\n * In this case, page_count still tracks the references, and should only\n * be used through the normal accessor functions. The top bits of page->flags\n * and page->virtual store page management information, but all other fields\n * are unused and could be used privately, carefully. The management of this\n * page is the responsibility of the one who allocated it, and those who have\n * subsequently been given references to it.\n *\n * The other pages (we may call them \"pagecache pages\") are completely\n * managed by the Linux memory manager: I/O, buffers, swapping etc.\n * The following discussion applies only to them.\n *\n * A pagecache page contains an opaque `private' member, which belongs to the\n * page's address_space. Usually, this is the address of a circular list of\n * the page's disk buffers. PG_private must be set to tell the VM to call\n * into the filesystem to release these pages.\n *\n * A page may belong to an inode's memory mapping. In this case, page->mapping\n * is the pointer to the inode, and page->index is the file offset of the page,\n * in units of PAGE_SIZE.\n *\n * If pagecache pages are not associated with an inode, they are said to be\n * anonymous pages. These may become associated with the swapcache, and in that\n * case PG_swapcache is set, and page->private is an offset into the swapcache.\n *\n * In either case (swapcache or inode backed), the pagecache itself holds one\n * reference to the page. Setting PG_private should also increment the\n * refcount. The each user mapping also has a reference to the page.\n *\n * The pagecache pages are stored in a per-mapping radix tree, which is\n * rooted at mapping->page_tree, and indexed by offset.\n * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space\n * lists, we instead now tag pages as dirty/writeback in the radix tree.\n *\n * All pagecache pages may be subject to I/O:\n * - inode pages may need to be read from disk,\n * - inode pages which have been modified and are MAP_SHARED may need\n *   to be written back to the inode on disk,\n * - anonymous pages (including MAP_PRIVATE file mappings) which have been\n *   modified may need to be swapped out to swap space and (later) to be read\n *   back into memory.\n */\n\n/*\n * The zone field is never updated after free_area_init_core()\n * sets it, so none of the operations on it need to be atomic.\n */\n\n/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */\n#define SECTIONS_PGOFF\t\t((sizeof(unsigned long)*8) - SECTIONS_WIDTH)\n#define NODES_PGOFF\t\t(SECTIONS_PGOFF - NODES_WIDTH)\n#define ZONES_PGOFF\t\t(NODES_PGOFF - ZONES_WIDTH)\n#define LAST_CPUPID_PGOFF\t(ZONES_PGOFF - LAST_CPUPID_WIDTH)\n\n/*\n * Define the bit shifts to access each section.  For non-existent\n * sections we define the shift as 0; that plus a 0 mask ensures\n * the compiler will optimise away reference to them.\n */\n#define SECTIONS_PGSHIFT\t(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))\n#define NODES_PGSHIFT\t\t(NODES_PGOFF * (NODES_WIDTH != 0))\n#define ZONES_PGSHIFT\t\t(ZONES_PGOFF * (ZONES_WIDTH != 0))\n#define LAST_CPUPID_PGSHIFT\t(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))\n\n/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */\n#ifdef NODE_NOT_IN_PAGE_FLAGS\n#define ZONEID_SHIFT\t\t(SECTIONS_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((SECTIONS_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tSECTIONS_PGOFF : ZONES_PGOFF)\n#else\n#define ZONEID_SHIFT\t\t(NODES_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((NODES_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tNODES_PGOFF : ZONES_PGOFF)\n#endif\n\n#define ZONEID_PGSHIFT\t\t(ZONEID_PGOFF * (ZONEID_SHIFT != 0))\n\n#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\n#error SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\n#endif\n\n#define ZONES_MASK\t\t((1UL << ZONES_WIDTH) - 1)\n#define NODES_MASK\t\t((1UL << NODES_WIDTH) - 1)\n#define SECTIONS_MASK\t\t((1UL << SECTIONS_WIDTH) - 1)\n#define LAST_CPUPID_MASK\t((1UL << LAST_CPUPID_SHIFT) - 1)\n#define ZONEID_MASK\t\t((1UL << ZONEID_SHIFT) - 1)\n\nstatic inline enum zone_type page_zonenum(const struct page *page)\n{\n\treturn (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nvoid get_zone_device_page(struct page *page);\nvoid put_zone_device_page(struct page *page);\nstatic inline bool is_zone_device_page(const struct page *page)\n{\n\treturn page_zonenum(page) == ZONE_DEVICE;\n}\n#else\nstatic inline void get_zone_device_page(struct page *page)\n{\n}\nstatic inline void put_zone_device_page(struct page *page)\n{\n}\nstatic inline bool is_zone_device_page(const struct page *page)\n{\n\treturn false;\n}\n#endif\n\nstatic inline void get_page(struct page *page)\n{\n\tpage = compound_head(page);\n\t/*\n\t * Getting a normal page or the head of a compound page\n\t * requires to already have an elevated page->_refcount.\n\t */\n\tVM_BUG_ON_PAGE(page_ref_count(page) <= 0, page);\n\tpage_ref_inc(page);\n\n\tif (unlikely(is_zone_device_page(page)))\n\t\tget_zone_device_page(page);\n}\n\nstatic inline void put_page(struct page *page)\n{\n\tpage = compound_head(page);\n\n\tif (put_page_testzero(page))\n\t\t__put_page(page);\n\n\tif (unlikely(is_zone_device_page(page)))\n\t\tput_zone_device_page(page);\n}\n\n#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n#define SECTION_IN_PAGE_FLAGS\n#endif\n\n/*\n * The identification function is mainly used by the buddy allocator for\n * determining if two pages could be buddies. We are not really identifying\n * the zone since we could be using the section number id if we do not have\n * node id available in page flags.\n * We only guarantee that it will return the same value for two combinable\n * pages in a zone.\n */\nstatic inline int page_zone_id(struct page *page)\n{\n\treturn (page->flags >> ZONEID_PGSHIFT) & ZONEID_MASK;\n}\n\nstatic inline int zone_to_nid(struct zone *zone)\n{\n#ifdef CONFIG_NUMA\n\treturn zone->node;\n#else\n\treturn 0;\n#endif\n}\n\n#ifdef NODE_NOT_IN_PAGE_FLAGS\nextern int page_to_nid(const struct page *page);\n#else\nstatic inline int page_to_nid(const struct page *page)\n{\n\treturn (page->flags >> NODES_PGSHIFT) & NODES_MASK;\n}\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline int cpu_pid_to_cpupid(int cpu, int pid)\n{\n\treturn ((cpu & LAST__CPU_MASK) << LAST__PID_SHIFT) | (pid & LAST__PID_MASK);\n}\n\nstatic inline int cpupid_to_pid(int cpupid)\n{\n\treturn cpupid & LAST__PID_MASK;\n}\n\nstatic inline int cpupid_to_cpu(int cpupid)\n{\n\treturn (cpupid >> LAST__PID_SHIFT) & LAST__CPU_MASK;\n}\n\nstatic inline int cpupid_to_nid(int cpupid)\n{\n\treturn cpu_to_node(cpupid_to_cpu(cpupid));\n}\n\nstatic inline bool cpupid_pid_unset(int cpupid)\n{\n\treturn cpupid_to_pid(cpupid) == (-1 & LAST__PID_MASK);\n}\n\nstatic inline bool cpupid_cpu_unset(int cpupid)\n{\n\treturn cpupid_to_cpu(cpupid) == (-1 & LAST__CPU_MASK);\n}\n\nstatic inline bool __cpupid_match_pid(pid_t task_pid, int cpupid)\n{\n\treturn (task_pid & LAST__PID_MASK) == cpupid_to_pid(cpupid);\n}\n\n#define cpupid_match_pid(task, cpupid) __cpupid_match_pid(task->pid, cpupid)\n#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\nstatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\n{\n\treturn xchg(&page->_last_cpupid, cpupid & LAST_CPUPID_MASK);\n}\n\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn page->_last_cpupid;\n}\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n\tpage->_last_cpupid = -1 & LAST_CPUPID_MASK;\n}\n#else\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn (page->flags >> LAST_CPUPID_PGSHIFT) & LAST_CPUPID_MASK;\n}\n\nextern int page_cpupid_xchg_last(struct page *page, int cpupid);\n\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n\tpage->flags |= LAST_CPUPID_MASK << LAST_CPUPID_PGSHIFT;\n}\n#endif /* LAST_CPUPID_NOT_IN_PAGE_FLAGS */\n#else /* !CONFIG_NUMA_BALANCING */\nstatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\n{\n\treturn page_to_nid(page); /* XXX */\n}\n\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn page_to_nid(page); /* XXX */\n}\n\nstatic inline int cpupid_to_nid(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpupid_to_pid(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpupid_to_cpu(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpu_pid_to_cpupid(int nid, int pid)\n{\n\treturn -1;\n}\n\nstatic inline bool cpupid_pid_unset(int cpupid)\n{\n\treturn 1;\n}\n\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n}\n\nstatic inline bool cpupid_match_pid(struct task_struct *task, int cpupid)\n{\n\treturn false;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\nstatic inline struct zone *page_zone(const struct page *page)\n{\n\treturn &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];\n}\n\nstatic inline pg_data_t *page_pgdat(const struct page *page)\n{\n\treturn NODE_DATA(page_to_nid(page));\n}\n\n#ifdef SECTION_IN_PAGE_FLAGS\nstatic inline void set_page_section(struct page *page, unsigned long section)\n{\n\tpage->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);\n\tpage->flags |= (section & SECTIONS_MASK) << SECTIONS_PGSHIFT;\n}\n\nstatic inline unsigned long page_to_section(const struct page *page)\n{\n\treturn (page->flags >> SECTIONS_PGSHIFT) & SECTIONS_MASK;\n}\n#endif\n\nstatic inline void set_page_zone(struct page *page, enum zone_type zone)\n{\n\tpage->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);\n\tpage->flags |= (zone & ZONES_MASK) << ZONES_PGSHIFT;\n}\n\nstatic inline void set_page_node(struct page *page, unsigned long node)\n{\n\tpage->flags &= ~(NODES_MASK << NODES_PGSHIFT);\n\tpage->flags |= (node & NODES_MASK) << NODES_PGSHIFT;\n}\n\nstatic inline void set_page_links(struct page *page, enum zone_type zone,\n\tunsigned long node, unsigned long pfn)\n{\n\tset_page_zone(page, zone);\n\tset_page_node(page, node);\n#ifdef SECTION_IN_PAGE_FLAGS\n\tset_page_section(page, pfn_to_section_nr(pfn));\n#endif\n}\n\n#ifdef CONFIG_MEMCG\nstatic inline struct mem_cgroup *page_memcg(struct page *page)\n{\n\treturn page->mem_cgroup;\n}\nstatic inline struct mem_cgroup *page_memcg_rcu(struct page *page)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\treturn READ_ONCE(page->mem_cgroup);\n}\n#else\nstatic inline struct mem_cgroup *page_memcg(struct page *page)\n{\n\treturn NULL;\n}\nstatic inline struct mem_cgroup *page_memcg_rcu(struct page *page)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\treturn NULL;\n}\n#endif\n\n/*\n * Some inline functions in vmstat.h depend on page_zone()\n */\n#include <linux/vmstat.h>\n\nstatic __always_inline void *lowmem_page_address(const struct page *page)\n{\n\treturn page_to_virt(page);\n}\n\n#if defined(CONFIG_HIGHMEM) && !defined(WANT_PAGE_VIRTUAL)\n#define HASHED_PAGE_VIRTUAL\n#endif\n\n#if defined(WANT_PAGE_VIRTUAL)\nstatic inline void *page_address(const struct page *page)\n{\n\treturn page->virtual;\n}\nstatic inline void set_page_address(struct page *page, void *address)\n{\n\tpage->virtual = address;\n}\n#define page_address_init()  do { } while(0)\n#endif\n\n#if defined(HASHED_PAGE_VIRTUAL)\nvoid *page_address(const struct page *page);\nvoid set_page_address(struct page *page, void *virtual);\nvoid page_address_init(void);\n#endif\n\n#if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)\n#define page_address(page) lowmem_page_address(page)\n#define set_page_address(page, address)  do { } while(0)\n#define page_address_init()  do { } while(0)\n#endif\n\nextern void *page_rmapping(struct page *page);\nextern struct anon_vma *page_anon_vma(struct page *page);\nextern struct address_space *page_mapping(struct page *page);\n\nextern struct address_space *__page_file_mapping(struct page *);\n\nstatic inline\nstruct address_space *page_file_mapping(struct page *page)\n{\n\tif (unlikely(PageSwapCache(page)))\n\t\treturn __page_file_mapping(page);\n\n\treturn page->mapping;\n}\n\nextern pgoff_t __page_file_index(struct page *page);\n\n/*\n * Return the pagecache index of the passed page.  Regular pagecache pages\n * use ->index whereas swapcache pages use swp_offset(->private)\n */\nstatic inline pgoff_t page_index(struct page *page)\n{\n\tif (unlikely(PageSwapCache(page)))\n\t\treturn __page_file_index(page);\n\treturn page->index;\n}\n\nbool page_mapped(struct page *page);\nstruct address_space *page_mapping(struct page *page);\n\n/*\n * Return true only if the page has been allocated with\n * ALLOC_NO_WATERMARKS and the low watermark was not\n * met implying that the system is under some pressure.\n */\nstatic inline bool page_is_pfmemalloc(struct page *page)\n{\n\t/*\n\t * Page index cannot be this large so this must be\n\t * a pfmemalloc page.\n\t */\n\treturn page->index == -1UL;\n}\n\n/*\n * Only to be called by the page allocator on a freshly allocated\n * page.\n */\nstatic inline void set_page_pfmemalloc(struct page *page)\n{\n\tpage->index = -1UL;\n}\n\nstatic inline void clear_page_pfmemalloc(struct page *page)\n{\n\tpage->index = 0;\n}\n\n/*\n * Different kinds of faults, as returned by handle_mm_fault().\n * Used to decide whether a process gets delivered SIGBUS or\n * just gets major/minor fault counters bumped up.\n */\n\n#define VM_FAULT_OOM\t0x0001\n#define VM_FAULT_SIGBUS\t0x0002\n#define VM_FAULT_MAJOR\t0x0004\n#define VM_FAULT_WRITE\t0x0008\t/* Special case for get_user_pages */\n#define VM_FAULT_HWPOISON 0x0010\t/* Hit poisoned small page */\n#define VM_FAULT_HWPOISON_LARGE 0x0020  /* Hit poisoned large page. Index encoded in upper bits */\n#define VM_FAULT_SIGSEGV 0x0040\n\n#define VM_FAULT_NOPAGE\t0x0100\t/* ->fault installed the pte, not return page */\n#define VM_FAULT_LOCKED\t0x0200\t/* ->fault locked the returned page */\n#define VM_FAULT_RETRY\t0x0400\t/* ->fault blocked, must retry */\n#define VM_FAULT_FALLBACK 0x0800\t/* huge page fault failed, fall back to small */\n#define VM_FAULT_DAX_LOCKED 0x1000\t/* ->fault has locked DAX entry */\n\n#define VM_FAULT_HWPOISON_LARGE_MASK 0xf000 /* encodes hpage index for large hwpoison */\n\n#define VM_FAULT_ERROR\t(VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | \\\n\t\t\t VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE | \\\n\t\t\t VM_FAULT_FALLBACK)\n\n/* Encode hstate index for a hwpoisoned large page */\n#define VM_FAULT_SET_HINDEX(x) ((x) << 12)\n#define VM_FAULT_GET_HINDEX(x) (((x) >> 12) & 0xf)\n\n/*\n * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.\n */\nextern void pagefault_out_of_memory(void);\n\n#define offset_in_page(p)\t((unsigned long)(p) & ~PAGE_MASK)\n\n/*\n * Flags passed to show_mem() and show_free_areas() to suppress output in\n * various contexts.\n */\n#define SHOW_MEM_FILTER_NODES\t\t(0x0001u)\t/* disallowed nodes */\n\nextern void show_free_areas(unsigned int flags);\nextern bool skip_free_areas_node(unsigned int flags, int nid);\n\nint shmem_zero_setup(struct vm_area_struct *);\n#ifdef CONFIG_SHMEM\nbool shmem_mapping(struct address_space *mapping);\n#else\nstatic inline bool shmem_mapping(struct address_space *mapping)\n{\n\treturn false;\n}\n#endif\n\nextern bool can_do_mlock(void);\nextern int user_shm_lock(size_t, struct user_struct *);\nextern void user_shm_unlock(size_t, struct user_struct *);\n\n/*\n * Parameter block passed down to zap_pte_range in exceptional cases.\n */\nstruct zap_details {\n\tstruct address_space *check_mapping;\t/* Check page->mapping if set */\n\tpgoff_t\tfirst_index;\t\t\t/* Lowest page->index to unmap */\n\tpgoff_t last_index;\t\t\t/* Highest page->index to unmap */\n\tbool ignore_dirty;\t\t\t/* Ignore dirty pages */\n\tbool check_swap_entries;\t\t/* Check also swap entries */\n};\n\nstruct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,\n\t\tpte_t pte);\nstruct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tpmd_t pmd);\n\nint zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size);\nvoid zap_page_range(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size, struct zap_details *);\nvoid unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,\n\t\tunsigned long start, unsigned long end);\n\n/**\n * mm_walk - callbacks for walk_page_range\n * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry\n *\t       this handler is required to be able to handle\n *\t       pmd_trans_huge() pmds.  They may simply choose to\n *\t       split_huge_page() instead of handling it explicitly.\n * @pte_entry: if set, called for each non-empty PTE (4th-level) entry\n * @pte_hole: if set, called for each hole at all levels\n * @hugetlb_entry: if set, called for each hugetlb entry\n * @test_walk: caller specific callback function to determine whether\n *             we walk over the current vma or not. Returning 0\n *             value means \"do page table walk over the current vma,\"\n *             and a negative one means \"abort current page table walk\n *             right now.\" 1 means \"skip the current vma.\"\n * @mm:        mm_struct representing the target process of page table walk\n * @vma:       vma currently walked (NULL if walking outside vmas)\n * @private:   private data for callbacks' usage\n *\n * (see the comment on walk_page_range() for more details)\n */\nstruct mm_walk {\n\tint (*pmd_entry)(pmd_t *pmd, unsigned long addr,\n\t\t\t unsigned long next, struct mm_walk *walk);\n\tint (*pte_entry)(pte_t *pte, unsigned long addr,\n\t\t\t unsigned long next, struct mm_walk *walk);\n\tint (*pte_hole)(unsigned long addr, unsigned long next,\n\t\t\tstruct mm_walk *walk);\n\tint (*hugetlb_entry)(pte_t *pte, unsigned long hmask,\n\t\t\t     unsigned long addr, unsigned long next,\n\t\t\t     struct mm_walk *walk);\n\tint (*test_walk)(unsigned long addr, unsigned long next,\n\t\t\tstruct mm_walk *walk);\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tvoid *private;\n};\n\nint walk_page_range(unsigned long addr, unsigned long end,\n\t\tstruct mm_walk *walk);\nint walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk);\nvoid free_pgd_range(struct mmu_gather *tlb, unsigned long addr,\n\t\tunsigned long end, unsigned long floor, unsigned long ceiling);\nint copy_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\tstruct vm_area_struct *vma);\nvoid unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows);\nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn);\nint follow_phys(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned int flags, unsigned long *prot, resource_size_t *phys);\nint generic_access_phys(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tvoid *buf, int len, int write);\n\nstatic inline void unmap_shared_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen)\n{\n\tunmap_mapping_range(mapping, holebegin, holelen, 0);\n}\n\nextern void truncate_pagecache(struct inode *inode, loff_t new);\nextern void truncate_setsize(struct inode *inode, loff_t newsize);\nvoid pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to);\nvoid truncate_pagecache_range(struct inode *inode, loff_t offset, loff_t end);\nint truncate_inode_page(struct address_space *mapping, struct page *page);\nint generic_error_remove_page(struct address_space *mapping, struct page *page);\nint invalidate_inode_page(struct page *page);\n\n#ifdef CONFIG_MMU\nextern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned int flags);\nextern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\t    unsigned long address, unsigned int fault_flags,\n\t\t\t    bool *unlocked);\n#else\nstatic inline int handle_mm_fault(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int flags)\n{\n\t/* should never happen if there's no MMU */\n\tBUG();\n\treturn VM_FAULT_SIGBUS;\n}\nstatic inline int fixup_user_fault(struct task_struct *tsk,\n\t\tstruct mm_struct *mm, unsigned long address,\n\t\tunsigned int fault_flags, bool *unlocked)\n{\n\t/* should never happen if there's no MMU */\n\tBUG();\n\treturn -EFAULT;\n}\n#endif\n\nextern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);\nextern int access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, int write);\n\nlong __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t      unsigned long start, unsigned long nr_pages,\n\t\t      unsigned int foll_flags, struct page **pages,\n\t\t      struct vm_area_struct **vmas, int *nonblocking);\nlong get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\t    unsigned long start, unsigned long nr_pages,\n\t\t\t    int write, int force, struct page **pages,\n\t\t\t    struct vm_area_struct **vmas);\nlong get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t\t    int write, int force, struct page **pages,\n\t\t\t    struct vm_area_struct **vmas);\nlong get_user_pages_locked(unsigned long start, unsigned long nr_pages,\n\t\t    int write, int force, struct page **pages, int *locked);\nlong __get_user_pages_unlocked(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\t       unsigned long start, unsigned long nr_pages,\n\t\t\t       int write, int force, struct page **pages,\n\t\t\t       unsigned int gup_flags);\nlong get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t    int write, int force, struct page **pages);\nint get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\tstruct page **pages);\n\n/* Container for pinned pfns / pages */\nstruct frame_vector {\n\tunsigned int nr_allocated;\t/* Number of frames we have space for */\n\tunsigned int nr_frames;\t/* Number of frames stored in ptrs array */\n\tbool got_ref;\t\t/* Did we pin pages by getting page ref? */\n\tbool is_pfns;\t\t/* Does array contain pages or pfns? */\n\tvoid *ptrs[0];\t\t/* Array of pinned pfns / pages. Use\n\t\t\t\t * pfns_vector_pages() or pfns_vector_pfns()\n\t\t\t\t * for access */\n};\n\nstruct frame_vector *frame_vector_create(unsigned int nr_frames);\nvoid frame_vector_destroy(struct frame_vector *vec);\nint get_vaddr_frames(unsigned long start, unsigned int nr_pfns,\n\t\t     bool write, bool force, struct frame_vector *vec);\nvoid put_vaddr_frames(struct frame_vector *vec);\nint frame_vector_to_pages(struct frame_vector *vec);\nvoid frame_vector_to_pfns(struct frame_vector *vec);\n\nstatic inline unsigned int frame_vector_count(struct frame_vector *vec)\n{\n\treturn vec->nr_frames;\n}\n\nstatic inline struct page **frame_vector_pages(struct frame_vector *vec)\n{\n\tif (vec->is_pfns) {\n\t\tint err = frame_vector_to_pages(vec);\n\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t}\n\treturn (struct page **)(vec->ptrs);\n}\n\nstatic inline unsigned long *frame_vector_pfns(struct frame_vector *vec)\n{\n\tif (!vec->is_pfns)\n\t\tframe_vector_to_pfns(vec);\n\treturn (unsigned long *)(vec->ptrs);\n}\n\nstruct kvec;\nint get_kernel_pages(const struct kvec *iov, int nr_pages, int write,\n\t\t\tstruct page **pages);\nint get_kernel_page(unsigned long start, int write, struct page **pages);\nstruct page *get_dump_page(unsigned long addr);\n\nextern int try_to_release_page(struct page * page, gfp_t gfp_mask);\nextern void do_invalidatepage(struct page *page, unsigned int offset,\n\t\t\t      unsigned int length);\n\nint __set_page_dirty_nobuffers(struct page *page);\nint __set_page_dirty_no_writeback(struct page *page);\nint redirty_page_for_writepage(struct writeback_control *wbc,\n\t\t\t\tstruct page *page);\nvoid account_page_dirtied(struct page *page, struct address_space *mapping);\nvoid account_page_cleaned(struct page *page, struct address_space *mapping,\n\t\t\t  struct bdi_writeback *wb);\nint set_page_dirty(struct page *page);\nint set_page_dirty_lock(struct page *page);\nvoid cancel_dirty_page(struct page *page);\nint clear_page_dirty_for_io(struct page *page);\n\nint get_cmdline(struct task_struct *task, char *buffer, int buflen);\n\n/* Is the vma a continuation of the stack vma above it? */\nstatic inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)\n{\n\treturn vma && (vma->vm_end == addr) && (vma->vm_flags & VM_GROWSDOWN);\n}\n\nstatic inline bool vma_is_anonymous(struct vm_area_struct *vma)\n{\n\treturn !vma->vm_ops;\n}\n\nstatic inline int stack_guard_page_start(struct vm_area_struct *vma,\n\t\t\t\t\t     unsigned long addr)\n{\n\treturn (vma->vm_flags & VM_GROWSDOWN) &&\n\t\t(vma->vm_start == addr) &&\n\t\t!vma_growsdown(vma->vm_prev, addr);\n}\n\n/* Is the vma a continuation of the stack vma below it? */\nstatic inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)\n{\n\treturn vma && (vma->vm_start == addr) && (vma->vm_flags & VM_GROWSUP);\n}\n\nstatic inline int stack_guard_page_end(struct vm_area_struct *vma,\n\t\t\t\t\t   unsigned long addr)\n{\n\treturn (vma->vm_flags & VM_GROWSUP) &&\n\t\t(vma->vm_end == addr) &&\n\t\t!vma_growsup(vma->vm_next, addr);\n}\n\nint vma_is_stack_for_task(struct vm_area_struct *vma, struct task_struct *t);\n\nextern unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len,\n\t\tbool need_rmap_locks);\nextern unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,\n\t\t\t      unsigned long end, pgprot_t newprot,\n\t\t\t      int dirty_accountable, int prot_numa);\nextern int mprotect_fixup(struct vm_area_struct *vma,\n\t\t\t  struct vm_area_struct **pprev, unsigned long start,\n\t\t\t  unsigned long end, unsigned long newflags);\n\n/*\n * doesn't attempt to fault and will return short.\n */\nint __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages);\n/*\n * per-process(per-mm_struct) statistics.\n */\nstatic inline unsigned long get_mm_counter(struct mm_struct *mm, int member)\n{\n\tlong val = atomic_long_read(&mm->rss_stat.count[member]);\n\n#ifdef SPLIT_RSS_COUNTING\n\t/*\n\t * counter is updated in asynchronous manner and may go to minus.\n\t * But it's never be expected number for users.\n\t */\n\tif (val < 0)\n\t\tval = 0;\n#endif\n\treturn (unsigned long)val;\n}\n\nstatic inline void add_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tatomic_long_add(value, &mm->rss_stat.count[member]);\n}\n\nstatic inline void inc_mm_counter(struct mm_struct *mm, int member)\n{\n\tatomic_long_inc(&mm->rss_stat.count[member]);\n}\n\nstatic inline void dec_mm_counter(struct mm_struct *mm, int member)\n{\n\tatomic_long_dec(&mm->rss_stat.count[member]);\n}\n\n/* Optimized variant when page is already known not to be PageAnon */\nstatic inline int mm_counter_file(struct page *page)\n{\n\tif (PageSwapBacked(page))\n\t\treturn MM_SHMEMPAGES;\n\treturn MM_FILEPAGES;\n}\n\nstatic inline int mm_counter(struct page *page)\n{\n\tif (PageAnon(page))\n\t\treturn MM_ANONPAGES;\n\treturn mm_counter_file(page);\n}\n\nstatic inline unsigned long get_mm_rss(struct mm_struct *mm)\n{\n\treturn get_mm_counter(mm, MM_FILEPAGES) +\n\t\tget_mm_counter(mm, MM_ANONPAGES) +\n\t\tget_mm_counter(mm, MM_SHMEMPAGES);\n}\n\nstatic inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_rss, get_mm_rss(mm));\n}\n\nstatic inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_vm, mm->total_vm);\n}\n\nstatic inline void update_hiwater_rss(struct mm_struct *mm)\n{\n\tunsigned long _rss = get_mm_rss(mm);\n\n\tif ((mm)->hiwater_rss < _rss)\n\t\t(mm)->hiwater_rss = _rss;\n}\n\nstatic inline void update_hiwater_vm(struct mm_struct *mm)\n{\n\tif (mm->hiwater_vm < mm->total_vm)\n\t\tmm->hiwater_vm = mm->total_vm;\n}\n\nstatic inline void reset_mm_hiwater_rss(struct mm_struct *mm)\n{\n\tmm->hiwater_rss = get_mm_rss(mm);\n}\n\nstatic inline void setmax_mm_hiwater_rss(unsigned long *maxrss,\n\t\t\t\t\t struct mm_struct *mm)\n{\n\tunsigned long hiwater_rss = get_mm_hiwater_rss(mm);\n\n\tif (*maxrss < hiwater_rss)\n\t\t*maxrss = hiwater_rss;\n}\n\n#if defined(SPLIT_RSS_COUNTING)\nvoid sync_mm_rss(struct mm_struct *mm);\n#else\nstatic inline void sync_mm_rss(struct mm_struct *mm)\n{\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTE_DEVMAP\nstatic inline int pte_devmap(pte_t pte)\n{\n\treturn 0;\n}\n#endif\n\nint vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot);\n\nextern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t       spinlock_t **ptl);\nstatic inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t    spinlock_t **ptl)\n{\n\tpte_t *ptep;\n\t__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));\n\treturn ptep;\n}\n\n#ifdef __PAGETABLE_PUD_FOLDED\nstatic inline int __pud_alloc(struct mm_struct *mm, pgd_t *pgd,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n#else\nint __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);\n#endif\n\n#if defined(__PAGETABLE_PMD_FOLDED) || !defined(CONFIG_MMU)\nstatic inline int __pmd_alloc(struct mm_struct *mm, pud_t *pud,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n\nstatic inline void mm_nr_pmds_init(struct mm_struct *mm) {}\n\nstatic inline unsigned long mm_nr_pmds(struct mm_struct *mm)\n{\n\treturn 0;\n}\n\nstatic inline void mm_inc_nr_pmds(struct mm_struct *mm) {}\nstatic inline void mm_dec_nr_pmds(struct mm_struct *mm) {}\n\n#else\nint __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);\n\nstatic inline void mm_nr_pmds_init(struct mm_struct *mm)\n{\n\tatomic_long_set(&mm->nr_pmds, 0);\n}\n\nstatic inline unsigned long mm_nr_pmds(struct mm_struct *mm)\n{\n\treturn atomic_long_read(&mm->nr_pmds);\n}\n\nstatic inline void mm_inc_nr_pmds(struct mm_struct *mm)\n{\n\tatomic_long_inc(&mm->nr_pmds);\n}\n\nstatic inline void mm_dec_nr_pmds(struct mm_struct *mm)\n{\n\tatomic_long_dec(&mm->nr_pmds);\n}\n#endif\n\nint __pte_alloc(struct mm_struct *mm, pmd_t *pmd, unsigned long address);\nint __pte_alloc_kernel(pmd_t *pmd, unsigned long address);\n\n/*\n * The following ifdef needed to get the 4level-fixup.h header to work.\n * Remove it when 4level-fixup.h has been removed.\n */\n#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)\nstatic inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)\n{\n\treturn (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?\n\t\tNULL: pud_offset(pgd, address);\n}\n\nstatic inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)\n{\n\treturn (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?\n\t\tNULL: pmd_offset(pud, address);\n}\n#endif /* CONFIG_MMU && !__ARCH_HAS_4LEVEL_HACK */\n\n#if USE_SPLIT_PTE_PTLOCKS\n#if ALLOC_SPLIT_PTLOCKS\nvoid __init ptlock_cache_init(void);\nextern bool ptlock_alloc(struct page *page);\nextern void ptlock_free(struct page *page);\n\nstatic inline spinlock_t *ptlock_ptr(struct page *page)\n{\n\treturn page->ptl;\n}\n#else /* ALLOC_SPLIT_PTLOCKS */\nstatic inline void ptlock_cache_init(void)\n{\n}\n\nstatic inline bool ptlock_alloc(struct page *page)\n{\n\treturn true;\n}\n\nstatic inline void ptlock_free(struct page *page)\n{\n}\n\nstatic inline spinlock_t *ptlock_ptr(struct page *page)\n{\n\treturn &page->ptl;\n}\n#endif /* ALLOC_SPLIT_PTLOCKS */\n\nstatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn ptlock_ptr(pmd_page(*pmd));\n}\n\nstatic inline bool ptlock_init(struct page *page)\n{\n\t/*\n\t * prep_new_page() initialize page->private (and therefore page->ptl)\n\t * with 0. Make sure nobody took it in use in between.\n\t *\n\t * It can happen if arch try to use slab for page table allocation:\n\t * slab code uses page->slab_cache, which share storage with page->ptl.\n\t */\n\tVM_BUG_ON_PAGE(*(unsigned long *)&page->ptl, page);\n\tif (!ptlock_alloc(page))\n\t\treturn false;\n\tspin_lock_init(ptlock_ptr(page));\n\treturn true;\n}\n\n/* Reset page->mapping so free_pages_check won't complain. */\nstatic inline void pte_lock_deinit(struct page *page)\n{\n\tpage->mapping = NULL;\n\tptlock_free(page);\n}\n\n#else\t/* !USE_SPLIT_PTE_PTLOCKS */\n/*\n * We use mm->page_table_lock to guard all pagetable pages of the mm.\n */\nstatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn &mm->page_table_lock;\n}\nstatic inline void ptlock_cache_init(void) {}\nstatic inline bool ptlock_init(struct page *page) { return true; }\nstatic inline void pte_lock_deinit(struct page *page) {}\n#endif /* USE_SPLIT_PTE_PTLOCKS */\n\nstatic inline void pgtable_init(void)\n{\n\tptlock_cache_init();\n\tpgtable_cache_init();\n}\n\nstatic inline bool pgtable_page_ctor(struct page *page)\n{\n\tif (!ptlock_init(page))\n\t\treturn false;\n\tinc_zone_page_state(page, NR_PAGETABLE);\n\treturn true;\n}\n\nstatic inline void pgtable_page_dtor(struct page *page)\n{\n\tpte_lock_deinit(page);\n\tdec_zone_page_state(page, NR_PAGETABLE);\n}\n\n#define pte_offset_map_lock(mm, pmd, address, ptlp)\t\\\n({\t\t\t\t\t\t\t\\\n\tspinlock_t *__ptl = pte_lockptr(mm, pmd);\t\\\n\tpte_t *__pte = pte_offset_map(pmd, address);\t\\\n\t*(ptlp) = __ptl;\t\t\t\t\\\n\tspin_lock(__ptl);\t\t\t\t\\\n\t__pte;\t\t\t\t\t\t\\\n})\n\n#define pte_unmap_unlock(pte, ptl)\tdo {\t\t\\\n\tspin_unlock(ptl);\t\t\t\t\\\n\tpte_unmap(pte);\t\t\t\t\t\\\n} while (0)\n\n#define pte_alloc(mm, pmd, address)\t\t\t\\\n\t(unlikely(pmd_none(*(pmd))) && __pte_alloc(mm, pmd, address))\n\n#define pte_alloc_map(mm, pmd, address)\t\t\t\\\n\t(pte_alloc(mm, pmd, address) ? NULL : pte_offset_map(pmd, address))\n\n#define pte_alloc_map_lock(mm, pmd, address, ptlp)\t\\\n\t(pte_alloc(mm, pmd, address) ?\t\t\t\\\n\t\t NULL : pte_offset_map_lock(mm, pmd, address, ptlp))\n\n#define pte_alloc_kernel(pmd, address)\t\t\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc_kernel(pmd, address))? \\\n\t\tNULL: pte_offset_kernel(pmd, address))\n\n#if USE_SPLIT_PMD_PTLOCKS\n\nstatic struct page *pmd_to_page(pmd_t *pmd)\n{\n\tunsigned long mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);\n\treturn virt_to_page((void *)((unsigned long) pmd & mask));\n}\n\nstatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn ptlock_ptr(pmd_to_page(pmd));\n}\n\nstatic inline bool pgtable_pmd_page_ctor(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tpage->pmd_huge_pte = NULL;\n#endif\n\treturn ptlock_init(page);\n}\n\nstatic inline void pgtable_pmd_page_dtor(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tVM_BUG_ON_PAGE(page->pmd_huge_pte, page);\n#endif\n\tptlock_free(page);\n}\n\n#define pmd_huge_pte(mm, pmd) (pmd_to_page(pmd)->pmd_huge_pte)\n\n#else\n\nstatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn &mm->page_table_lock;\n}\n\nstatic inline bool pgtable_pmd_page_ctor(struct page *page) { return true; }\nstatic inline void pgtable_pmd_page_dtor(struct page *page) {}\n\n#define pmd_huge_pte(mm, pmd) ((mm)->pmd_huge_pte)\n\n#endif\n\nstatic inline spinlock_t *pmd_lock(struct mm_struct *mm, pmd_t *pmd)\n{\n\tspinlock_t *ptl = pmd_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\treturn ptl;\n}\n\nextern void free_area_init(unsigned long * zones_size);\nextern void free_area_init_node(int nid, unsigned long * zones_size,\n\t\tunsigned long zone_start_pfn, unsigned long *zholes_size);\nextern void free_initmem(void);\n\n/*\n * Free reserved pages within range [PAGE_ALIGN(start), end & PAGE_MASK)\n * into the buddy system. The freed pages will be poisoned with pattern\n * \"poison\" if it's within range [0, UCHAR_MAX].\n * Return pages freed into the buddy system.\n */\nextern unsigned long free_reserved_area(void *start, void *end,\n\t\t\t\t\tint poison, char *s);\n\n#ifdef\tCONFIG_HIGHMEM\n/*\n * Free a highmem page into the buddy system, adjusting totalhigh_pages\n * and totalram_pages.\n */\nextern void free_highmem_page(struct page *page);\n#endif\n\nextern void adjust_managed_page_count(struct page *page, long count);\nextern void mem_init_print_info(const char *str);\n\nextern void reserve_bootmem_region(phys_addr_t start, phys_addr_t end);\n\n/* Free the reserved page into the buddy system, so it gets managed. */\nstatic inline void __free_reserved_page(struct page *page)\n{\n\tClearPageReserved(page);\n\tinit_page_count(page);\n\t__free_page(page);\n}\n\nstatic inline void free_reserved_page(struct page *page)\n{\n\t__free_reserved_page(page);\n\tadjust_managed_page_count(page, 1);\n}\n\nstatic inline void mark_page_reserved(struct page *page)\n{\n\tSetPageReserved(page);\n\tadjust_managed_page_count(page, -1);\n}\n\n/*\n * Default method to free all the __init memory into the buddy system.\n * The freed pages will be poisoned with pattern \"poison\" if it's within\n * range [0, UCHAR_MAX].\n * Return pages freed into the buddy system.\n */\nstatic inline unsigned long free_initmem_default(int poison)\n{\n\textern char __init_begin[], __init_end[];\n\n\treturn free_reserved_area(&__init_begin, &__init_end,\n\t\t\t\t  poison, \"unused kernel\");\n}\n\nstatic inline unsigned long get_num_physpages(void)\n{\n\tint nid;\n\tunsigned long phys_pages = 0;\n\n\tfor_each_online_node(nid)\n\t\tphys_pages += node_present_pages(nid);\n\n\treturn phys_pages;\n}\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n/*\n * With CONFIG_HAVE_MEMBLOCK_NODE_MAP set, an architecture may initialise its\n * zones, allocate the backing mem_map and account for memory holes in a more\n * architecture independent manner. This is a substitute for creating the\n * zone_sizes[] and zholes_size[] arrays and passing them to\n * free_area_init_node()\n *\n * An architecture is expected to register range of page frames backed by\n * physical memory with memblock_add[_node]() before calling\n * free_area_init_nodes() passing in the PFN each zone ends at. At a basic\n * usage, an architecture is expected to do something like\n *\n * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,\n * \t\t\t\t\t\t\t max_highmem_pfn};\n * for_each_valid_physical_page_range()\n * \tmemblock_add_node(base, size, nid)\n * free_area_init_nodes(max_zone_pfns);\n *\n * free_bootmem_with_active_regions() calls free_bootmem_node() for each\n * registered physical page range.  Similarly\n * sparse_memory_present_with_active_regions() calls memory_present() for\n * each range when SPARSEMEM is enabled.\n *\n * See mm/page_alloc.c for more information on each function exposed by\n * CONFIG_HAVE_MEMBLOCK_NODE_MAP.\n */\nextern void free_area_init_nodes(unsigned long *max_zone_pfn);\nunsigned long node_map_pfn_alignment(void);\nunsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern unsigned long absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern void get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn);\nextern unsigned long find_min_pfn_with_active_regions(void);\nextern void free_bootmem_with_active_regions(int nid,\n\t\t\t\t\t\tunsigned long max_low_pfn);\nextern void sparse_memory_present_with_active_regions(int nid);\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\n#if !defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) && \\\n    !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID)\nstatic inline int __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state)\n{\n\treturn 0;\n}\n#else\n/* please see mm/page_alloc.c */\nextern int __meminit early_pfn_to_nid(unsigned long pfn);\n/* there is a per-arch backend function. */\nextern int __meminit __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state);\n#endif\n\nextern void set_dma_reserve(unsigned long new_dma_reserve);\nextern void memmap_init_zone(unsigned long, int, unsigned long,\n\t\t\t\tunsigned long, enum memmap_context);\nextern void setup_per_zone_wmarks(void);\nextern int __meminit init_per_zone_wmark_min(void);\nextern void mem_init(void);\nextern void __init mmap_init(void);\nextern void show_mem(unsigned int flags);\nextern long si_mem_available(void);\nextern void si_meminfo(struct sysinfo * val);\nextern void si_meminfo_node(struct sysinfo *val, int nid);\n#ifdef __HAVE_ARCH_RESERVED_KERNEL_PAGES\nextern unsigned long arch_reserved_kernel_pages(void);\n#endif\n\nextern __printf(2, 3)\nvoid warn_alloc(gfp_t gfp_mask, const char *fmt, ...);\n\nextern void setup_per_cpu_pageset(void);\n\nextern void zone_pcp_update(struct zone *zone);\nextern void zone_pcp_reset(struct zone *zone);\n\n/* page_alloc.c */\nextern int min_free_kbytes;\nextern int watermark_scale_factor;\n\n/* nommu.c */\nextern atomic_long_t mmap_pages_allocated;\nextern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);\n\n/* interval_tree.c */\nvoid vma_interval_tree_insert(struct vm_area_struct *node,\n\t\t\t      struct rb_root *root);\nvoid vma_interval_tree_insert_after(struct vm_area_struct *node,\n\t\t\t\t    struct vm_area_struct *prev,\n\t\t\t\t    struct rb_root *root);\nvoid vma_interval_tree_remove(struct vm_area_struct *node,\n\t\t\t      struct rb_root *root);\nstruct vm_area_struct *vma_interval_tree_iter_first(struct rb_root *root,\n\t\t\t\tunsigned long start, unsigned long last);\nstruct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,\n\t\t\t\tunsigned long start, unsigned long last);\n\n#define vma_interval_tree_foreach(vma, root, start, last)\t\t\\\n\tfor (vma = vma_interval_tree_iter_first(root, start, last);\t\\\n\t     vma; vma = vma_interval_tree_iter_next(vma, start, last))\n\nvoid anon_vma_interval_tree_insert(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root *root);\nvoid anon_vma_interval_tree_remove(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root *root);\nstruct anon_vma_chain *anon_vma_interval_tree_iter_first(\n\tstruct rb_root *root, unsigned long start, unsigned long last);\nstruct anon_vma_chain *anon_vma_interval_tree_iter_next(\n\tstruct anon_vma_chain *node, unsigned long start, unsigned long last);\n#ifdef CONFIG_DEBUG_VM_RB\nvoid anon_vma_interval_tree_verify(struct anon_vma_chain *node);\n#endif\n\n#define anon_vma_interval_tree_foreach(avc, root, start, last)\t\t \\\n\tfor (avc = anon_vma_interval_tree_iter_first(root, start, last); \\\n\t     avc; avc = anon_vma_interval_tree_iter_next(avc, start, last))\n\n/* mmap.c */\nextern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);\nextern int __vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,\n\tstruct vm_area_struct *expand);\nstatic inline int vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert)\n{\n\treturn __vma_adjust(vma, start, end, pgoff, insert, NULL);\n}\nextern struct vm_area_struct *vma_merge(struct mm_struct *,\n\tstruct vm_area_struct *prev, unsigned long addr, unsigned long end,\n\tunsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,\n\tstruct mempolicy *, struct vm_userfaultfd_ctx);\nextern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);\nextern int split_vma(struct mm_struct *,\n\tstruct vm_area_struct *, unsigned long addr, int new_below);\nextern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);\nextern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,\n\tstruct rb_node **, struct rb_node *);\nextern void unlink_file_vma(struct vm_area_struct *);\nextern struct vm_area_struct *copy_vma(struct vm_area_struct **,\n\tunsigned long addr, unsigned long len, pgoff_t pgoff,\n\tbool *need_rmap_locks);\nextern void exit_mmap(struct mm_struct *);\n\nstatic inline int check_data_rlimit(unsigned long rlim,\n\t\t\t\t    unsigned long new,\n\t\t\t\t    unsigned long start,\n\t\t\t\t    unsigned long end_data,\n\t\t\t\t    unsigned long start_data)\n{\n\tif (rlim < RLIM_INFINITY) {\n\t\tif (((new - start) + (end_data - start_data)) > rlim)\n\t\t\treturn -ENOSPC;\n\t}\n\n\treturn 0;\n}\n\nextern int mm_take_all_locks(struct mm_struct *mm);\nextern void mm_drop_all_locks(struct mm_struct *mm);\n\nextern void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);\nextern struct file *get_mm_exe_file(struct mm_struct *mm);\nextern struct file *get_task_exe_file(struct task_struct *task);\n\nextern bool may_expand_vm(struct mm_struct *, vm_flags_t, unsigned long npages);\nextern void vm_stat_account(struct mm_struct *, vm_flags_t, long npages);\n\nextern bool vma_is_special_mapping(const struct vm_area_struct *vma,\n\t\t\t\t   const struct vm_special_mapping *sm);\nextern struct vm_area_struct *_install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags,\n\t\t\t\t   const struct vm_special_mapping *spec);\n/* This is an obsolete alternative to _install_special_mapping. */\nextern int install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags, struct page **pages);\n\nextern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);\n\nextern unsigned long mmap_region(struct file *file, unsigned long addr,\n\tunsigned long len, vm_flags_t vm_flags, unsigned long pgoff);\nextern unsigned long do_mmap(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot, unsigned long flags,\n\tvm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate);\nextern int do_munmap(struct mm_struct *, unsigned long, size_t);\n\nstatic inline unsigned long\ndo_mmap_pgoff(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot, unsigned long flags,\n\tunsigned long pgoff, unsigned long *populate)\n{\n\treturn do_mmap(file, addr, len, prot, flags, 0, pgoff, populate);\n}\n\n#ifdef CONFIG_MMU\nextern int __mm_populate(unsigned long addr, unsigned long len,\n\t\t\t int ignore_errors);\nstatic inline void mm_populate(unsigned long addr, unsigned long len)\n{\n\t/* Ignore errors */\n\t(void) __mm_populate(addr, len, 1);\n}\n#else\nstatic inline void mm_populate(unsigned long addr, unsigned long len) {}\n#endif\n\n/* These take the mm semaphore themselves */\nextern int __must_check vm_brk(unsigned long, unsigned long);\nextern int vm_munmap(unsigned long, size_t);\nextern unsigned long __must_check vm_mmap(struct file *, unsigned long,\n        unsigned long, unsigned long,\n        unsigned long, unsigned long);\n\nstruct vm_unmapped_area_info {\n#define VM_UNMAPPED_AREA_TOPDOWN 1\n\tunsigned long flags;\n\tunsigned long length;\n\tunsigned long low_limit;\n\tunsigned long high_limit;\n\tunsigned long align_mask;\n\tunsigned long align_offset;\n};\n\nextern unsigned long unmapped_area(struct vm_unmapped_area_info *info);\nextern unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info);\n\n/*\n * Search for an unmapped address range.\n *\n * We are looking for a range that:\n * - does not intersect with any VMA;\n * - is contained within the [low_limit, high_limit) interval;\n * - is at least the desired size.\n * - satisfies (begin_addr & align_mask) == (align_offset & align_mask)\n */\nstatic inline unsigned long\nvm_unmapped_area(struct vm_unmapped_area_info *info)\n{\n\tif (info->flags & VM_UNMAPPED_AREA_TOPDOWN)\n\t\treturn unmapped_area_topdown(info);\n\telse\n\t\treturn unmapped_area(info);\n}\n\n/* truncate.c */\nextern void truncate_inode_pages(struct address_space *, loff_t);\nextern void truncate_inode_pages_range(struct address_space *,\n\t\t\t\t       loff_t lstart, loff_t lend);\nextern void truncate_inode_pages_final(struct address_space *);\n\n/* generic vm_area_ops exported for stackable file systems */\nextern int filemap_fault(struct vm_area_struct *, struct vm_fault *);\nextern void filemap_map_pages(struct fault_env *fe,\n\t\tpgoff_t start_pgoff, pgoff_t end_pgoff);\nextern int filemap_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n/* mm/page-writeback.c */\nint write_one_page(struct page *page, int wait);\nvoid task_dirty_inc(struct task_struct *tsk);\n\n/* readahead.c */\n#define VM_MAX_READAHEAD\t128\t/* kbytes */\n#define VM_MIN_READAHEAD\t16\t/* kbytes (includes current page) */\n\nint force_page_cache_readahead(struct address_space *mapping, struct file *filp,\n\t\t\tpgoff_t offset, unsigned long nr_to_read);\n\nvoid page_cache_sync_readahead(struct address_space *mapping,\n\t\t\t       struct file_ra_state *ra,\n\t\t\t       struct file *filp,\n\t\t\t       pgoff_t offset,\n\t\t\t       unsigned long size);\n\nvoid page_cache_async_readahead(struct address_space *mapping,\n\t\t\t\tstruct file_ra_state *ra,\n\t\t\t\tstruct file *filp,\n\t\t\t\tstruct page *pg,\n\t\t\t\tpgoff_t offset,\n\t\t\t\tunsigned long size);\n\n/* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */\nextern int expand_stack(struct vm_area_struct *vma, unsigned long address);\n\n/* CONFIG_STACK_GROWSUP still needs to to grow downwards at some places */\nextern int expand_downwards(struct vm_area_struct *vma,\n\t\tunsigned long address);\n#if VM_GROWSUP\nextern int expand_upwards(struct vm_area_struct *vma, unsigned long address);\n#else\n  #define expand_upwards(vma, address) (0)\n#endif\n\n/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */\nextern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);\nextern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,\n\t\t\t\t\t     struct vm_area_struct **pprev);\n\n/* Look up the first VMA which intersects the interval start_addr..end_addr-1,\n   NULL if none.  Assume start_addr < end_addr. */\nstatic inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)\n{\n\tstruct vm_area_struct * vma = find_vma(mm,start_addr);\n\n\tif (vma && end_addr <= vma->vm_start)\n\t\tvma = NULL;\n\treturn vma;\n}\n\nstatic inline unsigned long vma_pages(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\n}\n\n/* Look up the first VMA which exactly match the interval vm_start ... vm_end */\nstatic inline struct vm_area_struct *find_exact_vma(struct mm_struct *mm,\n\t\t\t\tunsigned long vm_start, unsigned long vm_end)\n{\n\tstruct vm_area_struct *vma = find_vma(mm, vm_start);\n\n\tif (vma && (vma->vm_start != vm_start || vma->vm_end != vm_end))\n\t\tvma = NULL;\n\n\treturn vma;\n}\n\n#ifdef CONFIG_MMU\npgprot_t vm_get_page_prot(unsigned long vm_flags);\nvoid vma_set_page_prot(struct vm_area_struct *vma);\n#else\nstatic inline pgprot_t vm_get_page_prot(unsigned long vm_flags)\n{\n\treturn __pgprot(0);\n}\nstatic inline void vma_set_page_prot(struct vm_area_struct *vma)\n{\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n}\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\nunsigned long change_prot_numa(struct vm_area_struct *vma,\n\t\t\tunsigned long start, unsigned long end);\n#endif\n\nstruct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);\nint remap_pfn_range(struct vm_area_struct *, unsigned long addr,\n\t\t\tunsigned long pfn, unsigned long size, pgprot_t);\nint vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);\nint vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn);\nint vm_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn, pgprot_t pgprot);\nint vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tpfn_t pfn);\nint vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len);\n\n\nstruct page *follow_page_mask(struct vm_area_struct *vma,\n\t\t\t      unsigned long address, unsigned int foll_flags,\n\t\t\t      unsigned int *page_mask);\n\nstatic inline struct page *follow_page(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int foll_flags)\n{\n\tunsigned int unused_page_mask;\n\treturn follow_page_mask(vma, address, foll_flags, &unused_page_mask);\n}\n\n#define FOLL_WRITE\t0x01\t/* check pte is writable */\n#define FOLL_TOUCH\t0x02\t/* mark page accessed */\n#define FOLL_GET\t0x04\t/* do get_page on page */\n#define FOLL_DUMP\t0x08\t/* give error on hole if it would be zero */\n#define FOLL_FORCE\t0x10\t/* get_user_pages read/write w/o permission */\n#define FOLL_NOWAIT\t0x20\t/* if a disk transfer is needed, start the IO\n\t\t\t\t * and return without waiting upon it */\n#define FOLL_POPULATE\t0x40\t/* fault in page */\n#define FOLL_SPLIT\t0x80\t/* don't return transhuge pages, split them */\n#define FOLL_HWPOISON\t0x100\t/* check page is hwpoisoned */\n#define FOLL_NUMA\t0x200\t/* force NUMA hinting page fault */\n#define FOLL_MIGRATION\t0x400\t/* wait for page to replace migration entry */\n#define FOLL_TRIED\t0x800\t/* a retry, previous pass started an IO */\n#define FOLL_MLOCK\t0x1000\t/* lock present pages */\n#define FOLL_REMOTE\t0x2000\t/* we are working on non-current tsk/mm */\n\ntypedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,\n\t\t\tvoid *data);\nextern int apply_to_page_range(struct mm_struct *mm, unsigned long address,\n\t\t\t       unsigned long size, pte_fn_t fn, void *data);\n\n\n#ifdef CONFIG_PAGE_POISONING\nextern bool page_poisoning_enabled(void);\nextern void kernel_poison_pages(struct page *page, int numpages, int enable);\nextern bool page_is_poisoned(struct page *page);\n#else\nstatic inline bool page_poisoning_enabled(void) { return false; }\nstatic inline void kernel_poison_pages(struct page *page, int numpages,\n\t\t\t\t\tint enable) { }\nstatic inline bool page_is_poisoned(struct page *page) { return false; }\n#endif\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nextern bool _debug_pagealloc_enabled;\nextern void __kernel_map_pages(struct page *page, int numpages, int enable);\n\nstatic inline bool debug_pagealloc_enabled(void)\n{\n\treturn _debug_pagealloc_enabled;\n}\n\nstatic inline void\nkernel_map_pages(struct page *page, int numpages, int enable)\n{\n\tif (!debug_pagealloc_enabled())\n\t\treturn;\n\n\t__kernel_map_pages(page, numpages, enable);\n}\n#ifdef CONFIG_HIBERNATION\nextern bool kernel_page_present(struct page *page);\n#endif\t/* CONFIG_HIBERNATION */\n#else\t/* CONFIG_DEBUG_PAGEALLOC */\nstatic inline void\nkernel_map_pages(struct page *page, int numpages, int enable) {}\n#ifdef CONFIG_HIBERNATION\nstatic inline bool kernel_page_present(struct page *page) { return true; }\n#endif\t/* CONFIG_HIBERNATION */\nstatic inline bool debug_pagealloc_enabled(void)\n{\n\treturn false;\n}\n#endif\t/* CONFIG_DEBUG_PAGEALLOC */\n\n#ifdef __HAVE_ARCH_GATE_AREA\nextern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);\nextern int in_gate_area_no_mm(unsigned long addr);\nextern int in_gate_area(struct mm_struct *mm, unsigned long addr);\n#else\nstatic inline struct vm_area_struct *get_gate_vma(struct mm_struct *mm)\n{\n\treturn NULL;\n}\nstatic inline int in_gate_area_no_mm(unsigned long addr) { return 0; }\nstatic inline int in_gate_area(struct mm_struct *mm, unsigned long addr)\n{\n\treturn 0;\n}\n#endif\t/* __HAVE_ARCH_GATE_AREA */\n\nextern bool process_shares_mm(struct task_struct *p, struct mm_struct *mm);\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_drop_caches;\nint drop_caches_sysctl_handler(struct ctl_table *, int,\n\t\t\t\t\tvoid __user *, size_t *, loff_t *);\n#endif\n\nvoid drop_slab(void);\nvoid drop_slab_node(int nid);\n\n#ifndef CONFIG_MMU\n#define randomize_va_space 0\n#else\nextern int randomize_va_space;\n#endif\n\nconst char * arch_vma_name(struct vm_area_struct *vma);\nvoid print_vma_addr(char *prefix, unsigned long rip);\n\nvoid sparse_mem_maps_populate_node(struct page **map_map,\n\t\t\t\t   unsigned long pnum_begin,\n\t\t\t\t   unsigned long pnum_end,\n\t\t\t\t   unsigned long map_count,\n\t\t\t\t   int nodeid);\n\nstruct page *sparse_mem_map_populate(unsigned long pnum, int nid);\npgd_t *vmemmap_pgd_populate(unsigned long addr, int node);\npud_t *vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node);\npmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);\npte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node);\nvoid *vmemmap_alloc_block(unsigned long size, int node);\nstruct vmem_altmap;\nvoid *__vmemmap_alloc_block_buf(unsigned long size, int node,\n\t\tstruct vmem_altmap *altmap);\nstatic inline void *vmemmap_alloc_block_buf(unsigned long size, int node)\n{\n\treturn __vmemmap_alloc_block_buf(size, node, NULL);\n}\n\nvoid vmemmap_verify(pte_t *, int, unsigned long, unsigned long);\nint vmemmap_populate_basepages(unsigned long start, unsigned long end,\n\t\t\t       int node);\nint vmemmap_populate(unsigned long start, unsigned long end, int node);\nvoid vmemmap_populate_print_last(void);\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid vmemmap_free(unsigned long start, unsigned long end);\n#endif\nvoid register_page_bootmem_memmap(unsigned long section_nr, struct page *map,\n\t\t\t\t  unsigned long size);\n\nenum mf_flags {\n\tMF_COUNT_INCREASED = 1 << 0,\n\tMF_ACTION_REQUIRED = 1 << 1,\n\tMF_MUST_KILL = 1 << 2,\n\tMF_SOFT_OFFLINE = 1 << 3,\n};\nextern int memory_failure(unsigned long pfn, int trapno, int flags);\nextern void memory_failure_queue(unsigned long pfn, int trapno, int flags);\nextern int unpoison_memory(unsigned long pfn);\nextern int get_hwpoison_page(struct page *page);\n#define put_hwpoison_page(page)\tput_page(page)\nextern int sysctl_memory_failure_early_kill;\nextern int sysctl_memory_failure_recovery;\nextern void shake_page(struct page *p, int access);\nextern atomic_long_t num_poisoned_pages;\nextern int soft_offline_page(struct page *page, int flags);\n\n\n/*\n * Error handlers for various types of pages.\n */\nenum mf_result {\n\tMF_IGNORED,\t/* Error: cannot be handled */\n\tMF_FAILED,\t/* Error: handling failed */\n\tMF_DELAYED,\t/* Will be handled later */\n\tMF_RECOVERED,\t/* Successfully recovered */\n};\n\nenum mf_action_page_type {\n\tMF_MSG_KERNEL,\n\tMF_MSG_KERNEL_HIGH_ORDER,\n\tMF_MSG_SLAB,\n\tMF_MSG_DIFFERENT_COMPOUND,\n\tMF_MSG_POISONED_HUGE,\n\tMF_MSG_HUGE,\n\tMF_MSG_FREE_HUGE,\n\tMF_MSG_UNMAP_FAILED,\n\tMF_MSG_DIRTY_SWAPCACHE,\n\tMF_MSG_CLEAN_SWAPCACHE,\n\tMF_MSG_DIRTY_MLOCKED_LRU,\n\tMF_MSG_CLEAN_MLOCKED_LRU,\n\tMF_MSG_DIRTY_UNEVICTABLE_LRU,\n\tMF_MSG_CLEAN_UNEVICTABLE_LRU,\n\tMF_MSG_DIRTY_LRU,\n\tMF_MSG_CLEAN_LRU,\n\tMF_MSG_TRUNCATED_LRU,\n\tMF_MSG_BUDDY,\n\tMF_MSG_BUDDY_2ND,\n\tMF_MSG_UNKNOWN,\n};\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)\nextern void clear_huge_page(struct page *page,\n\t\t\t    unsigned long addr,\n\t\t\t    unsigned int pages_per_huge_page);\nextern void copy_user_huge_page(struct page *dst, struct page *src,\n\t\t\t\tunsigned long addr, struct vm_area_struct *vma,\n\t\t\t\tunsigned int pages_per_huge_page);\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */\n\nextern struct page_ext_operations debug_guardpage_ops;\nextern struct page_ext_operations page_poisoning_ops;\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nextern unsigned int _debug_guardpage_minorder;\nextern bool _debug_guardpage_enabled;\n\nstatic inline unsigned int debug_guardpage_minorder(void)\n{\n\treturn _debug_guardpage_minorder;\n}\n\nstatic inline bool debug_guardpage_enabled(void)\n{\n\treturn _debug_guardpage_enabled;\n}\n\nstatic inline bool page_is_guard(struct page *page)\n{\n\tstruct page_ext *page_ext;\n\n\tif (!debug_guardpage_enabled())\n\t\treturn false;\n\n\tpage_ext = lookup_page_ext(page);\n\tif (unlikely(!page_ext))\n\t\treturn false;\n\n\treturn test_bit(PAGE_EXT_DEBUG_GUARD, &page_ext->flags);\n}\n#else\nstatic inline unsigned int debug_guardpage_minorder(void) { return 0; }\nstatic inline bool debug_guardpage_enabled(void) { return false; }\nstatic inline bool page_is_guard(struct page *page) { return false; }\n#endif /* CONFIG_DEBUG_PAGEALLOC */\n\n#if MAX_NUMNODES > 1\nvoid __init setup_nr_node_ids(void);\n#else\nstatic inline void setup_nr_node_ids(void) {}\n#endif\n\n#endif /* __KERNEL__ */\n#endif /* _LINUX_MM_H */\n", "#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/spinlock.h>\n\n#include <linux/mm.h>\n#include <linux/memremap.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n\n#include <linux/sched.h>\n#include <linux/rwsem.h>\n#include <linux/hugetlb.h>\n\n#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/tlbflush.h>\n\n#include \"internal.h\"\n\nstatic struct page *no_page_table(struct vm_area_struct *vma,\n\t\tunsigned int flags)\n{\n\t/*\n\t * When core dumping an enormous anonymous area that nobody\n\t * has touched so far, we don't want to allocate unnecessary pages or\n\t * page tables.  Return error instead of NULL to skip handle_mm_fault,\n\t * then get_dump_page() will return NULL to leave a hole in the dump.\n\t * But we can only make this optimization where a hole would surely\n\t * be zero-filled if handle_mm_fault() actually did handle it.\n\t */\n\tif ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))\n\t\treturn ERR_PTR(-EFAULT);\n\treturn NULL;\n}\n\nstatic int follow_pfn_pte(struct vm_area_struct *vma, unsigned long address,\n\t\tpte_t *pte, unsigned int flags)\n{\n\t/* No page to get reference */\n\tif (flags & FOLL_GET)\n\t\treturn -EFAULT;\n\n\tif (flags & FOLL_TOUCH) {\n\t\tpte_t entry = *pte;\n\n\t\tif (flags & FOLL_WRITE)\n\t\t\tentry = pte_mkdirty(entry);\n\t\tentry = pte_mkyoung(entry);\n\n\t\tif (!pte_same(*pte, entry)) {\n\t\t\tset_pte_at(vma->vm_mm, address, pte, entry);\n\t\t\tupdate_mmu_cache(vma, address, pte);\n\t\t}\n\t}\n\n\t/* Proper page table entry exists, but no corresponding struct page */\n\treturn -EEXIST;\n}\n\nstatic struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap = NULL;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\tif ((flags & FOLL_WRITE) && !pte_write(pte)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\treturn NULL;\n\t}\n\n\tpage = vm_normal_page(vma, address, pte);\n\tif (!page && pte_devmap(pte) && (flags & FOLL_GET)) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET case since\n\t\t * they are only valid while holding the pgmap reference.\n\t\t */\n\t\tpgmap = get_dev_pagemap(pte_pfn(pte), NULL);\n\t\tif (pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (flags & FOLL_SPLIT && PageTransCompound(page)) {\n\t\tint ret;\n\t\tget_page(page);\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tlock_page(page);\n\t\tret = split_huge_page(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tgoto retry;\n\t}\n\n\tif (flags & FOLL_GET) {\n\t\tget_page(page);\n\n\t\t/* drop the pgmap reference now that we hold the page */\n\t\tif (pgmap) {\n\t\t\tput_dev_pagemap(pgmap);\n\t\t\tpgmap = NULL;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/* Do not mlock pte-mapped THP */\n\t\tif (PageTransCompound(page))\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * The preliminary mapping check is mainly to avoid the\n\t\t * pointless overhead of lock_page on the ZERO_PAGE\n\t\t * which might bounce very badly if there is contention.\n\t\t *\n\t\t * If the page is already locked, we don't need to\n\t\t * handle it now - vmscan will handle it later if and\n\t\t * when it attempts to reclaim the page.\n\t\t */\n\t\tif (page->mapping && trylock_page(page)) {\n\t\t\tlru_add_drain();  /* push cached pages to LRU */\n\t\t\t/*\n\t\t\t * Because we lock page here, and migration is\n\t\t\t * blocked by the pte's page reference, and we\n\t\t\t * know the page is still mapped, we don't even\n\t\t\t * need to check for file-cache page truncation.\n\t\t\t */\n\t\t\tmlock_vma_page(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}\n\n/**\n * follow_page_mask - look up a page descriptor from a user-virtual address\n * @vma: vm_area_struct mapping @address\n * @address: virtual address to look up\n * @flags: flags modifying lookup behaviour\n * @page_mask: on output, *page_mask is set according to the size of the page\n *\n * @flags can have FOLL_ flags set, defined in <linux/mm.h>\n *\n * Returns the mapped (struct page *), %NULL if no mapping exists, or\n * an error pointer if there is a mapping to something not represented\n * by a page descriptor (see also vm_normal_page()).\n */\nstruct page *follow_page_mask(struct vm_area_struct *vma,\n\t\t\t      unsigned long address, unsigned int flags,\n\t\t\t      unsigned int *page_mask)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\t*page_mask = 0;\n\n\tpage = follow_huge_addr(mm, address, flags & FOLL_WRITE);\n\tif (!IS_ERR(page)) {\n\t\tBUG_ON(flags & FOLL_GET);\n\t\treturn page;\n\t}\n\n\tpgd = pgd_offset(mm, address);\n\tif (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))\n\t\treturn no_page_table(vma, flags);\n\n\tpud = pud_offset(pgd, address);\n\tif (pud_none(*pud))\n\t\treturn no_page_table(vma, flags);\n\tif (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {\n\t\tpage = follow_huge_pud(mm, address, pud, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (unlikely(pud_bad(*pud)))\n\t\treturn no_page_table(vma, flags);\n\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none(*pmd))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {\n\t\tpage = follow_huge_pmd(mm, address, pmd, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif ((flags & FOLL_NUMA) && pmd_protnone(*pmd))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_devmap(*pmd)) {\n\t\tptl = pmd_lock(mm, pmd);\n\t\tpage = follow_devmap_pmd(vma, address, pmd, flags);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (likely(!pmd_trans_huge(*pmd)))\n\t\treturn follow_page_pte(vma, address, pmd, flags);\n\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn follow_page_pte(vma, address, pmd, flags);\n\t}\n\tif (flags & FOLL_SPLIT) {\n\t\tint ret;\n\t\tpage = pmd_page(*pmd);\n\t\tif (is_huge_zero_page(page)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tret = 0;\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tif (pmd_trans_unstable(pmd))\n\t\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tget_page(page);\n\t\t\tspin_unlock(ptl);\n\t\t\tlock_page(page);\n\t\t\tret = split_huge_page(page);\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tif (pmd_none(*pmd))\n\t\t\t\treturn no_page_table(vma, flags);\n\t\t}\n\n\t\treturn ret ? ERR_PTR(ret) :\n\t\t\tfollow_page_pte(vma, address, pmd, flags);\n\t}\n\n\tpage = follow_trans_huge_pmd(vma, address, pmd, flags);\n\tspin_unlock(ptl);\n\t*page_mask = HPAGE_PMD_NR - 1;\n\treturn page;\n}\n\nstatic int get_gate_page(struct mm_struct *mm, unsigned long address,\n\t\tunsigned int gup_flags, struct vm_area_struct **vma,\n\t\tstruct page **page)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tint ret = -EFAULT;\n\n\t/* user gate pages are read-only */\n\tif (gup_flags & FOLL_WRITE)\n\t\treturn -EFAULT;\n\tif (address > TASK_SIZE)\n\t\tpgd = pgd_offset_k(address);\n\telse\n\t\tpgd = pgd_offset_gate(mm, address);\n\tBUG_ON(pgd_none(*pgd));\n\tpud = pud_offset(pgd, address);\n\tBUG_ON(pud_none(*pud));\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none(*pmd))\n\t\treturn -EFAULT;\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tpte = pte_offset_map(pmd, address);\n\tif (pte_none(*pte))\n\t\tgoto unmap;\n\t*vma = get_gate_vma(mm);\n\tif (!page)\n\t\tgoto out;\n\t*page = vm_normal_page(*vma, address, *pte);\n\tif (!*page) {\n\t\tif ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))\n\t\t\tgoto unmap;\n\t\t*page = pte_page(*pte);\n\t}\n\tget_page(*page);\nout:\n\tret = 0;\nunmap:\n\tpte_unmap(pte);\n\treturn ret;\n}\n\n/*\n * mmap_sem must be held on entry.  If @nonblocking != NULL and\n * *@flags does not include FOLL_NOWAIT, the mmap_sem may be released.\n * If it is, *@nonblocking will be set to 0 and -EBUSY returned.\n */\nstatic int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, int *nonblocking)\n{\n\tunsigned int fault_flags = 0;\n\tint ret;\n\n\t/* mlock all present pages, but do not fault in new pages */\n\tif ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)\n\t\treturn -ENOENT;\n\t/* For mm_populate(), just skip the stack guard page. */\n\tif ((*flags & FOLL_POPULATE) &&\n\t\t\t(stack_guard_page_start(vma, address) ||\n\t\t\t stack_guard_page_end(vma, address + PAGE_SIZE)))\n\t\treturn -ENOENT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (nonblocking)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\tVM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tif (ret & VM_FAULT_ERROR) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\treturn -ENOMEM;\n\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\t\treturn *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;\n\t\tif (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\t\treturn -EFAULT;\n\t\tBUG();\n\t}\n\n\tif (tsk) {\n\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (nonblocking)\n\t\t\t*nonblocking = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t\t*flags &= ~FOLL_WRITE;\n\treturn 0;\n}\n\nstatic int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)\n{\n\tvm_flags_t vm_flags = vma->vm_flags;\n\tint write = (gup_flags & FOLL_WRITE);\n\tint foreign = (gup_flags & FOLL_REMOTE);\n\n\tif (vm_flags & (VM_IO | VM_PFNMAP))\n\t\treturn -EFAULT;\n\n\tif (write) {\n\t\tif (!(vm_flags & VM_WRITE)) {\n\t\t\tif (!(gup_flags & FOLL_FORCE))\n\t\t\t\treturn -EFAULT;\n\t\t\t/*\n\t\t\t * We used to let the write,force case do COW in a\n\t\t\t * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could\n\t\t\t * set a breakpoint in a read-only mapping of an\n\t\t\t * executable, without corrupting the file (yet only\n\t\t\t * when that file had been opened for writing!).\n\t\t\t * Anon pages in shared mappings are surprising: now\n\t\t\t * just reject it.\n\t\t\t */\n\t\t\tif (!is_cow_mapping(vm_flags))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t} else if (!(vm_flags & VM_READ)) {\n\t\tif (!(gup_flags & FOLL_FORCE))\n\t\t\treturn -EFAULT;\n\t\t/*\n\t\t * Is there actually any vma we can reach here which does not\n\t\t * have VM_MAYREAD set?\n\t\t */\n\t\tif (!(vm_flags & VM_MAYREAD))\n\t\t\treturn -EFAULT;\n\t}\n\t/*\n\t * gups are always data accesses, not instruction\n\t * fetches, so execute=false here\n\t */\n\tif (!arch_vma_access_permitted(vma, write, false, foreign))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/**\n * __get_user_pages() - pin user pages in memory\n * @tsk:\ttask_struct of target task\n * @mm:\t\tmm_struct of target mm\n * @start:\tstarting user address\n * @nr_pages:\tnumber of pages from start to pin\n * @gup_flags:\tflags modifying pin behaviour\n * @pages:\tarray that receives pointers to the pages pinned.\n *\t\tShould be at least nr_pages long. Or NULL, if caller\n *\t\tonly intends to ensure the pages are faulted in.\n * @vmas:\tarray of pointers to vmas corresponding to each page.\n *\t\tOr NULL if the caller does not require them.\n * @nonblocking: whether waiting for disk IO or mmap_sem contention\n *\n * Returns number of pages pinned. This may be fewer than the number\n * requested. If nr_pages is 0 or negative, returns 0. If no pages\n * were pinned, returns -errno. Each page returned must be released\n * with a put_page() call when it is finished with. vmas will only\n * remain valid while mmap_sem is held.\n *\n * Must be called with mmap_sem held.  It may be released.  See below.\n *\n * __get_user_pages walks a process's page tables and takes a reference to\n * each struct page that each user address corresponds to at a given\n * instant. That is, it takes the page that would be accessed if a user\n * thread accesses the given user virtual address at that instant.\n *\n * This does not guarantee that the page exists in the user mappings when\n * __get_user_pages returns, and there may even be a completely different\n * page there in some cases (eg. if mmapped pagecache has been invalidated\n * and subsequently re faulted). However it does guarantee that the page\n * won't be freed completely. And mostly callers simply care that the page\n * contains data that was valid *at some point in time*. Typically, an IO\n * or similar operation cannot guarantee anything stronger anyway because\n * locks can't be held over the syscall boundary.\n *\n * If @gup_flags & FOLL_WRITE == 0, the page must not be written to. If\n * the page is written to, set_page_dirty (or set_page_dirty_lock, as\n * appropriate) must be called after the page is finished with, and\n * before put_page is called.\n *\n * If @nonblocking != NULL, __get_user_pages will not wait for disk IO\n * or mmap_sem contention, and if waiting is needed to pin all pages,\n * *@nonblocking will be set to 0.  Further, if @gup_flags does not\n * include FOLL_NOWAIT, the mmap_sem will be released via up_read() in\n * this case.\n *\n * A caller using such a combination of @nonblocking and @gup_flags\n * must therefore hold the mmap_sem for reading only, and recognize\n * when it's been released.  Otherwise, it must be held for either\n * reading or writing and will not be released.\n *\n * In most cases, get_user_pages or get_user_pages_fast should be used\n * instead of __get_user_pages. __get_user_pages should be used only if\n * you need some special @gup_flags.\n */\nlong __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *nonblocking)\n{\n\tlong i = 0;\n\tunsigned int page_mask;\n\tstruct vm_area_struct *vma = NULL;\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & FOLL_GET));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tint ret;\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn i ? : ret;\n\t\t\t\tpage_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags))\n\t\t\t\treturn i ? : -EFAULT;\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tgup_flags);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (unlikely(fatal_signal_pending(current)))\n\t\t\treturn i ? i : -ERESTARTSYS;\n\t\tcond_resched();\n\t\tpage = follow_page_mask(vma, start, foll_flags, &page_mask);\n\t\tif (!page) {\n\t\t\tint ret;\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\tnonblocking);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\treturn i ? i : ret;\n\t\t\tcase -EBUSY:\n\t\t\t\treturn i;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\treturn i ? i : PTR_ERR(page);\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tpage_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tpage_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\n\treturn i;\n}\nEXPORT_SYMBOL(__get_user_pages);\n\nbool vma_permits_fault(struct vm_area_struct *vma, unsigned int fault_flags)\n{\n\tbool write   = !!(fault_flags & FAULT_FLAG_WRITE);\n\tbool foreign = !!(fault_flags & FAULT_FLAG_REMOTE);\n\tvm_flags_t vm_flags = write ? VM_WRITE : VM_READ;\n\n\tif (!(vm_flags & vma->vm_flags))\n\t\treturn false;\n\n\t/*\n\t * The architecture might have a hardware protection\n\t * mechanism other than read/write that can deny access.\n\t *\n\t * gup always represents data access, not instruction\n\t * fetches, so execute=false here:\n\t */\n\tif (!arch_vma_access_permitted(vma, write, false, foreign))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * fixup_user_fault() - manually resolve a user page fault\n * @tsk:\tthe task_struct to use for page fault accounting, or\n *\t\tNULL if faults are not to be recorded.\n * @mm:\t\tmm_struct of target mm\n * @address:\tuser address\n * @fault_flags:flags to pass down to handle_mm_fault()\n * @unlocked:\tdid we unlock the mmap_sem while retrying, maybe NULL if caller\n *\t\tdoes not allow retry\n *\n * This is meant to be called in the specific scenario where for locking reasons\n * we try to access user memory in atomic context (within a pagefault_disable()\n * section), this returns -EFAULT, and we want to resolve the user fault before\n * trying again.\n *\n * Typically this is meant to be used by the futex code.\n *\n * The main difference with get_user_pages() is that this function will\n * unconditionally call handle_mm_fault() which will in turn perform all the\n * necessary SW fixup of the dirty and young bits in the PTE, while\n * get_user_pages() only guarantees to update these in the struct page.\n *\n * This is important for some architectures where those bits also gate the\n * access permission to the page because they are maintained in software.  On\n * such architectures, gup() will not be enough to make a subsequent access\n * succeed.\n *\n * This function will not return with an unlocked mmap_sem. So it has not the\n * same semantics wrt the @mm->mmap_sem as does filemap_fault().\n */\nint fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,\n\t\t     unsigned long address, unsigned int fault_flags,\n\t\t     bool *unlocked)\n{\n\tstruct vm_area_struct *vma;\n\tint ret, major = 0;\n\n\tif (unlocked)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\nretry:\n\tvma = find_extend_vma(mm, address);\n\tif (!vma || address < vma->vm_start)\n\t\treturn -EFAULT;\n\n\tif (!vma_permits_fault(vma, fault_flags))\n\t\treturn -EFAULT;\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tmajor |= ret & VM_FAULT_MAJOR;\n\tif (ret & VM_FAULT_ERROR) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\treturn -ENOMEM;\n\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\t\treturn -EHWPOISON;\n\t\tif (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\t\treturn -EFAULT;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tdown_read(&mm->mmap_sem);\n\t\tif (!(fault_flags & FAULT_FLAG_TRIED)) {\n\t\t\t*unlocked = true;\n\t\t\tfault_flags &= ~FAULT_FLAG_ALLOW_RETRY;\n\t\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (tsk) {\n\t\tif (major)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(fixup_user_fault);\n\nstatic __always_inline long __get_user_pages_locked(struct task_struct *tsk,\n\t\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\t\tunsigned long start,\n\t\t\t\t\t\tunsigned long nr_pages,\n\t\t\t\t\t\tint write, int force,\n\t\t\t\t\t\tstruct page **pages,\n\t\t\t\t\t\tstruct vm_area_struct **vmas,\n\t\t\t\t\t\tint *locked, bool notify_drop,\n\t\t\t\t\t\tunsigned int flags)\n{\n\tlong ret, pages_done;\n\tbool lock_dropped;\n\n\tif (locked) {\n\t\t/* if VM_FAULT_RETRY can be returned, vmas become invalid */\n\t\tBUG_ON(vmas);\n\t\t/* check caller initialized locked */\n\t\tBUG_ON(*locked != 1);\n\t}\n\n\tif (pages)\n\t\tflags |= FOLL_GET;\n\tif (write)\n\t\tflags |= FOLL_WRITE;\n\tif (force)\n\t\tflags |= FOLL_FORCE;\n\n\tpages_done = 0;\n\tlock_dropped = false;\n\tfor (;;) {\n\t\tret = __get_user_pages(tsk, mm, start, nr_pages, flags, pages,\n\t\t\t\t       vmas, locked);\n\t\tif (!locked)\n\t\t\t/* VM_FAULT_RETRY couldn't trigger, bypass */\n\t\t\treturn ret;\n\n\t\t/* VM_FAULT_RETRY cannot return errors */\n\t\tif (!*locked) {\n\t\t\tBUG_ON(ret < 0);\n\t\t\tBUG_ON(ret >= nr_pages);\n\t\t}\n\n\t\tif (!pages)\n\t\t\t/* If it's a prefault don't insist harder */\n\t\t\treturn ret;\n\n\t\tif (ret > 0) {\n\t\t\tnr_pages -= ret;\n\t\t\tpages_done += ret;\n\t\t\tif (!nr_pages)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (*locked) {\n\t\t\t/* VM_FAULT_RETRY didn't trigger */\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\t/* VM_FAULT_RETRY triggered, so seek to the faulting offset */\n\t\tpages += ret;\n\t\tstart += ret << PAGE_SHIFT;\n\n\t\t/*\n\t\t * Repeat on the address that fired VM_FAULT_RETRY\n\t\t * without FAULT_FLAG_ALLOW_RETRY but with\n\t\t * FAULT_FLAG_TRIED.\n\t\t */\n\t\t*locked = 1;\n\t\tlock_dropped = true;\n\t\tdown_read(&mm->mmap_sem);\n\t\tret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,\n\t\t\t\t       pages, NULL, NULL);\n\t\tif (ret != 1) {\n\t\t\tBUG_ON(ret > 1);\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\tnr_pages--;\n\t\tpages_done++;\n\t\tif (!nr_pages)\n\t\t\tbreak;\n\t\tpages++;\n\t\tstart += PAGE_SIZE;\n\t}\n\tif (notify_drop && lock_dropped && *locked) {\n\t\t/*\n\t\t * We must let the caller know we temporarily dropped the lock\n\t\t * and so the critical section protected by it was lost.\n\t\t */\n\t\tup_read(&mm->mmap_sem);\n\t\t*locked = 0;\n\t}\n\treturn pages_done;\n}\n\n/*\n * We can leverage the VM_FAULT_RETRY functionality in the page fault\n * paths better by using either get_user_pages_locked() or\n * get_user_pages_unlocked().\n *\n * get_user_pages_locked() is suitable to replace the form:\n *\n *      down_read(&mm->mmap_sem);\n *      do_something()\n *      get_user_pages(tsk, mm, ..., pages, NULL);\n *      up_read(&mm->mmap_sem);\n *\n *  to:\n *\n *      int locked = 1;\n *      down_read(&mm->mmap_sem);\n *      do_something()\n *      get_user_pages_locked(tsk, mm, ..., pages, &locked);\n *      if (locked)\n *          up_read(&mm->mmap_sem);\n */\nlong get_user_pages_locked(unsigned long start, unsigned long nr_pages,\n\t\t\t   int write, int force, struct page **pages,\n\t\t\t   int *locked)\n{\n\treturn __get_user_pages_locked(current, current->mm, start, nr_pages,\n\t\t\t\t       write, force, pages, NULL, locked, true,\n\t\t\t\t       FOLL_TOUCH);\n}\nEXPORT_SYMBOL(get_user_pages_locked);\n\n/*\n * Same as get_user_pages_unlocked(...., FOLL_TOUCH) but it allows to\n * pass additional gup_flags as last parameter (like FOLL_HWPOISON).\n *\n * NOTE: here FOLL_TOUCH is not set implicitly and must be set by the\n * caller if required (just like with __get_user_pages). \"FOLL_GET\",\n * \"FOLL_WRITE\" and \"FOLL_FORCE\" are set implicitly as needed\n * according to the parameters \"pages\", \"write\", \"force\"\n * respectively.\n */\n__always_inline long __get_user_pages_unlocked(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\t\t\t       unsigned long start, unsigned long nr_pages,\n\t\t\t\t\t       int write, int force, struct page **pages,\n\t\t\t\t\t       unsigned int gup_flags)\n{\n\tlong ret;\n\tint locked = 1;\n\tdown_read(&mm->mmap_sem);\n\tret = __get_user_pages_locked(tsk, mm, start, nr_pages, write, force,\n\t\t\t\t      pages, NULL, &locked, false, gup_flags);\n\tif (locked)\n\t\tup_read(&mm->mmap_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(__get_user_pages_unlocked);\n\n/*\n * get_user_pages_unlocked() is suitable to replace the form:\n *\n *      down_read(&mm->mmap_sem);\n *      get_user_pages(tsk, mm, ..., pages, NULL);\n *      up_read(&mm->mmap_sem);\n *\n *  with:\n *\n *      get_user_pages_unlocked(tsk, mm, ..., pages);\n *\n * It is functionally equivalent to get_user_pages_fast so\n * get_user_pages_fast should be used instead, if the two parameters\n * \"tsk\" and \"mm\" are respectively equal to current and current->mm,\n * or if \"force\" shall be set to 1 (get_user_pages_fast misses the\n * \"force\" parameter).\n */\nlong get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t\t     int write, int force, struct page **pages)\n{\n\treturn __get_user_pages_unlocked(current, current->mm, start, nr_pages,\n\t\t\t\t\t write, force, pages, FOLL_TOUCH);\n}\nEXPORT_SYMBOL(get_user_pages_unlocked);\n\n/*\n * get_user_pages_remote() - pin user pages in memory\n * @tsk:\tthe task_struct to use for page fault accounting, or\n *\t\tNULL if faults are not to be recorded.\n * @mm:\t\tmm_struct of target mm\n * @start:\tstarting user address\n * @nr_pages:\tnumber of pages from start to pin\n * @write:\twhether pages will be written to by the caller\n * @force:\twhether to force access even when user mapping is currently\n *\t\tprotected (but never forces write access to shared mapping).\n * @pages:\tarray that receives pointers to the pages pinned.\n *\t\tShould be at least nr_pages long. Or NULL, if caller\n *\t\tonly intends to ensure the pages are faulted in.\n * @vmas:\tarray of pointers to vmas corresponding to each page.\n *\t\tOr NULL if the caller does not require them.\n *\n * Returns number of pages pinned. This may be fewer than the number\n * requested. If nr_pages is 0 or negative, returns 0. If no pages\n * were pinned, returns -errno. Each page returned must be released\n * with a put_page() call when it is finished with. vmas will only\n * remain valid while mmap_sem is held.\n *\n * Must be called with mmap_sem held for read or write.\n *\n * get_user_pages walks a process's page tables and takes a reference to\n * each struct page that each user address corresponds to at a given\n * instant. That is, it takes the page that would be accessed if a user\n * thread accesses the given user virtual address at that instant.\n *\n * This does not guarantee that the page exists in the user mappings when\n * get_user_pages returns, and there may even be a completely different\n * page there in some cases (eg. if mmapped pagecache has been invalidated\n * and subsequently re faulted). However it does guarantee that the page\n * won't be freed completely. And mostly callers simply care that the page\n * contains data that was valid *at some point in time*. Typically, an IO\n * or similar operation cannot guarantee anything stronger anyway because\n * locks can't be held over the syscall boundary.\n *\n * If write=0, the page must not be written to. If the page is written to,\n * set_page_dirty (or set_page_dirty_lock, as appropriate) must be called\n * after the page is finished with, and before put_page is called.\n *\n * get_user_pages is typically used for fewer-copy IO operations, to get a\n * handle on the memory by some means other than accesses via the user virtual\n * addresses. The pages may be submitted for DMA to devices or accessed via\n * their kernel linear mapping (via the kmap APIs). Care should be taken to\n * use the correct cache flushing APIs.\n *\n * See also get_user_pages_fast, for performance critical applications.\n *\n * get_user_pages should be phased out in favor of\n * get_user_pages_locked|unlocked or get_user_pages_fast. Nothing\n * should use get_user_pages because it cannot pass\n * FAULT_FLAG_ALLOW_RETRY to handle_mm_fault.\n */\nlong get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tint write, int force, struct page **pages,\n\t\tstruct vm_area_struct **vmas)\n{\n\treturn __get_user_pages_locked(tsk, mm, start, nr_pages, write, force,\n\t\t\t\t       pages, vmas, NULL, false,\n\t\t\t\t       FOLL_TOUCH | FOLL_REMOTE);\n}\nEXPORT_SYMBOL(get_user_pages_remote);\n\n/*\n * This is the same as get_user_pages_remote(), just with a\n * less-flexible calling convention where we assume that the task\n * and mm being operated on are the current task's.  We also\n * obviously don't pass FOLL_REMOTE in here.\n */\nlong get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\tint write, int force, struct page **pages,\n\t\tstruct vm_area_struct **vmas)\n{\n\treturn __get_user_pages_locked(current, current->mm, start, nr_pages,\n\t\t\t\t       write, force, pages, vmas, NULL, false,\n\t\t\t\t       FOLL_TOUCH);\n}\nEXPORT_SYMBOL(get_user_pages);\n\n/**\n * populate_vma_page_range() -  populate a range of pages in the vma.\n * @vma:   target vma\n * @start: start address\n * @end:   end address\n * @nonblocking:\n *\n * This takes care of mlocking the pages too if VM_LOCKED is set.\n *\n * return 0 on success, negative error code on error.\n *\n * vma->vm_mm->mmap_sem must be held.\n *\n * If @nonblocking is NULL, it may be held for read or write and will\n * be unperturbed.\n *\n * If @nonblocking is non-NULL, it must held for read only and may be\n * released.  If it's released, *@nonblocking will be set to 0.\n */\nlong populate_vma_page_range(struct vm_area_struct *vma,\n\t\tunsigned long start, unsigned long end, int *nonblocking)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long nr_pages = (end - start) / PAGE_SIZE;\n\tint gup_flags;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(end   & ~PAGE_MASK);\n\tVM_BUG_ON_VMA(start < vma->vm_start, vma);\n\tVM_BUG_ON_VMA(end   > vma->vm_end, vma);\n\tVM_BUG_ON_MM(!rwsem_is_locked(&mm->mmap_sem), mm);\n\n\tgup_flags = FOLL_TOUCH | FOLL_POPULATE | FOLL_MLOCK;\n\tif (vma->vm_flags & VM_LOCKONFAULT)\n\t\tgup_flags &= ~FOLL_POPULATE;\n\t/*\n\t * We want to touch writable mappings with a write fault in order\n\t * to break COW, except for shared mappings because these don't COW\n\t * and we would not want to dirty them for nothing.\n\t */\n\tif ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)\n\t\tgup_flags |= FOLL_WRITE;\n\n\t/*\n\t * We want mlock to succeed for regions that have any permissions\n\t * other than PROT_NONE.\n\t */\n\tif (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC))\n\t\tgup_flags |= FOLL_FORCE;\n\n\t/*\n\t * We made sure addr is within a VMA, so the following will\n\t * not result in a stack expansion that recurses back here.\n\t */\n\treturn __get_user_pages(current, mm, start, nr_pages, gup_flags,\n\t\t\t\tNULL, NULL, nonblocking);\n}\n\n/*\n * __mm_populate - populate and/or mlock pages within a range of address space.\n *\n * This is used to implement mlock() and the MAP_POPULATE / MAP_LOCKED mmap\n * flags. VMAs must be already marked with the desired vm_flags, and\n * mmap_sem must not be held.\n */\nint __mm_populate(unsigned long start, unsigned long len, int ignore_errors)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long end, nstart, nend;\n\tstruct vm_area_struct *vma = NULL;\n\tint locked = 0;\n\tlong ret = 0;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(len != PAGE_ALIGN(len));\n\tend = start + len;\n\n\tfor (nstart = start; nstart < end; nstart = nend) {\n\t\t/*\n\t\t * We want to fault in pages for [nstart; end) address range.\n\t\t * Find first corresponding VMA.\n\t\t */\n\t\tif (!locked) {\n\t\t\tlocked = 1;\n\t\t\tdown_read(&mm->mmap_sem);\n\t\t\tvma = find_vma(mm, nstart);\n\t\t} else if (nstart >= vma->vm_end)\n\t\t\tvma = vma->vm_next;\n\t\tif (!vma || vma->vm_start >= end)\n\t\t\tbreak;\n\t\t/*\n\t\t * Set [nstart; nend) to intersection of desired address\n\t\t * range with the first VMA. Also, skip undesirable VMA types.\n\t\t */\n\t\tnend = min(end, vma->vm_end);\n\t\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))\n\t\t\tcontinue;\n\t\tif (nstart < vma->vm_start)\n\t\t\tnstart = vma->vm_start;\n\t\t/*\n\t\t * Now fault in a range of pages. populate_vma_page_range()\n\t\t * double checks the vma flags, so that it won't mlock pages\n\t\t * if the vma was already munlocked.\n\t\t */\n\t\tret = populate_vma_page_range(vma, nstart, nend, &locked);\n\t\tif (ret < 0) {\n\t\t\tif (ignore_errors) {\n\t\t\t\tret = 0;\n\t\t\t\tcontinue;\t/* continue at next VMA */\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tnend = nstart + ret * PAGE_SIZE;\n\t\tret = 0;\n\t}\n\tif (locked)\n\t\tup_read(&mm->mmap_sem);\n\treturn ret;\t/* 0 or negative error code */\n}\n\n/**\n * get_dump_page() - pin user page in memory while writing it to core dump\n * @addr: user address\n *\n * Returns struct page pointer of user page pinned for dump,\n * to be freed afterwards by put_page().\n *\n * Returns NULL on any kind of failure - a hole must then be inserted into\n * the corefile, to preserve alignment with its headers; and also returns\n * NULL wherever the ZERO_PAGE, or an anonymous pte_none, has been found -\n * allowing a hole to be left in the corefile to save diskspace.\n *\n * Called without mmap_sem, but after all other threads have been killed.\n */\n#ifdef CONFIG_ELF_CORE\nstruct page *get_dump_page(unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\tstruct page *page;\n\n\tif (__get_user_pages(current, current->mm, addr, 1,\n\t\t\t     FOLL_FORCE | FOLL_DUMP | FOLL_GET, &page, &vma,\n\t\t\t     NULL) < 1)\n\t\treturn NULL;\n\tflush_cache_page(vma, addr, page_to_pfn(page));\n\treturn page;\n}\n#endif /* CONFIG_ELF_CORE */\n\n/*\n * Generic RCU Fast GUP\n *\n * get_user_pages_fast attempts to pin user pages by walking the page\n * tables directly and avoids taking locks. Thus the walker needs to be\n * protected from page table pages being freed from under it, and should\n * block any THP splits.\n *\n * One way to achieve this is to have the walker disable interrupts, and\n * rely on IPIs from the TLB flushing code blocking before the page table\n * pages are freed. This is unsuitable for architectures that do not need\n * to broadcast an IPI when invalidating TLBs.\n *\n * Another way to achieve this is to batch up page table containing pages\n * belonging to more than one mm_user, then rcu_sched a callback to free those\n * pages. Disabling interrupts will allow the fast_gup walker to both block\n * the rcu_sched callback, and an IPI that we broadcast for splitting THPs\n * (which is a relatively rare event). The code below adopts this strategy.\n *\n * Before activating this code, please be aware that the following assumptions\n * are currently made:\n *\n *  *) HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table is used to free\n *      pages containing page tables.\n *\n *  *) ptes can be read atomically by the architecture.\n *\n *  *) access_ok is sufficient to validate userspace address ranges.\n *\n * The last two assumptions can be relaxed by the addition of helper functions.\n *\n * This code is based heavily on the PowerPC implementation by Nick Piggin.\n */\n#ifdef CONFIG_HAVE_GENERIC_RCU_GUP\n\n#ifdef __HAVE_ARCH_PTE_SPECIAL\nstatic int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,\n\t\t\t int write, struct page **pages, int *nr)\n{\n\tpte_t *ptep, *ptem;\n\tint ret = 0;\n\n\tptem = ptep = pte_offset_map(&pmd, addr);\n\tdo {\n\t\t/*\n\t\t * In the line below we are assuming that the pte can be read\n\t\t * atomically. If this is not the case for your architecture,\n\t\t * please wrap this in a helper function!\n\t\t *\n\t\t * for an example see gup_get_pte in arch/x86/mm/gup.c\n\t\t */\n\t\tpte_t pte = READ_ONCE(*ptep);\n\t\tstruct page *head, *page;\n\n\t\t/*\n\t\t * Similar to the PMD case below, NUMA hinting must take slow\n\t\t * path using the pte_protnone check.\n\t\t */\n\t\tif (!pte_present(pte) || pte_special(pte) ||\n\t\t\tpte_protnone(pte) || (write && !pte_write(pte)))\n\t\t\tgoto pte_unmap;\n\n\t\tif (!arch_pte_access_permitted(pte, write))\n\t\t\tgoto pte_unmap;\n\n\t\tVM_BUG_ON(!pfn_valid(pte_pfn(pte)));\n\t\tpage = pte_page(pte);\n\t\thead = compound_head(page);\n\n\t\tif (!page_cache_get_speculative(head))\n\t\t\tgoto pte_unmap;\n\n\t\tif (unlikely(pte_val(pte) != pte_val(*ptep))) {\n\t\t\tput_page(head);\n\t\t\tgoto pte_unmap;\n\t\t}\n\n\t\tVM_BUG_ON_PAGE(compound_head(page) != head, page);\n\t\tpages[*nr] = page;\n\t\t(*nr)++;\n\n\t} while (ptep++, addr += PAGE_SIZE, addr != end);\n\n\tret = 1;\n\npte_unmap:\n\tpte_unmap(ptem);\n\treturn ret;\n}\n#else\n\n/*\n * If we can't determine whether or not a pte is special, then fail immediately\n * for ptes. Note, we can still pin HugeTLB and THP as these are guaranteed not\n * to be special.\n *\n * For a futex to be placed on a THP tail page, get_futex_key requires a\n * __get_user_pages_fast implementation that can pin pages. Thus it's still\n * useful to have gup_huge_pmd even if we can't operate on ptes.\n */\nstatic int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,\n\t\t\t int write, struct page **pages, int *nr)\n{\n\treturn 0;\n}\n#endif /* __HAVE_ARCH_PTE_SPECIAL */\n\nstatic int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,\n\t\tunsigned long end, int write, struct page **pages, int *nr)\n{\n\tstruct page *head, *page;\n\tint refs;\n\n\tif (write && !pmd_write(orig))\n\t\treturn 0;\n\n\trefs = 0;\n\thead = pmd_page(orig);\n\tpage = head + ((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\tdo {\n\t\tVM_BUG_ON_PAGE(compound_head(page) != head, page);\n\t\tpages[*nr] = page;\n\t\t(*nr)++;\n\t\tpage++;\n\t\trefs++;\n\t} while (addr += PAGE_SIZE, addr != end);\n\n\tif (!page_cache_add_speculative(head, refs)) {\n\t\t*nr -= refs;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {\n\t\t*nr -= refs;\n\t\twhile (refs--)\n\t\t\tput_page(head);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,\n\t\tunsigned long end, int write, struct page **pages, int *nr)\n{\n\tstruct page *head, *page;\n\tint refs;\n\n\tif (write && !pud_write(orig))\n\t\treturn 0;\n\n\trefs = 0;\n\thead = pud_page(orig);\n\tpage = head + ((addr & ~PUD_MASK) >> PAGE_SHIFT);\n\tdo {\n\t\tVM_BUG_ON_PAGE(compound_head(page) != head, page);\n\t\tpages[*nr] = page;\n\t\t(*nr)++;\n\t\tpage++;\n\t\trefs++;\n\t} while (addr += PAGE_SIZE, addr != end);\n\n\tif (!page_cache_add_speculative(head, refs)) {\n\t\t*nr -= refs;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(pud_val(orig) != pud_val(*pudp))) {\n\t\t*nr -= refs;\n\t\twhile (refs--)\n\t\t\tput_page(head);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr,\n\t\t\tunsigned long end, int write,\n\t\t\tstruct page **pages, int *nr)\n{\n\tint refs;\n\tstruct page *head, *page;\n\n\tif (write && !pgd_write(orig))\n\t\treturn 0;\n\n\trefs = 0;\n\thead = pgd_page(orig);\n\tpage = head + ((addr & ~PGDIR_MASK) >> PAGE_SHIFT);\n\tdo {\n\t\tVM_BUG_ON_PAGE(compound_head(page) != head, page);\n\t\tpages[*nr] = page;\n\t\t(*nr)++;\n\t\tpage++;\n\t\trefs++;\n\t} while (addr += PAGE_SIZE, addr != end);\n\n\tif (!page_cache_add_speculative(head, refs)) {\n\t\t*nr -= refs;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(pgd_val(orig) != pgd_val(*pgdp))) {\n\t\t*nr -= refs;\n\t\twhile (refs--)\n\t\t\tput_page(head);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,\n\t\tint write, struct page **pages, int *nr)\n{\n\tunsigned long next;\n\tpmd_t *pmdp;\n\n\tpmdp = pmd_offset(&pud, addr);\n\tdo {\n\t\tpmd_t pmd = READ_ONCE(*pmdp);\n\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (pmd_none(pmd))\n\t\t\treturn 0;\n\n\t\tif (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {\n\t\t\t/*\n\t\t\t * NUMA hinting faults need to be handled in the GUP\n\t\t\t * slowpath for accounting purposes and so that they\n\t\t\t * can be serialised against THP migration.\n\t\t\t */\n\t\t\tif (pmd_protnone(pmd))\n\t\t\t\treturn 0;\n\n\t\t\tif (!gup_huge_pmd(pmd, pmdp, addr, next, write,\n\t\t\t\tpages, nr))\n\t\t\t\treturn 0;\n\n\t\t} else if (unlikely(is_hugepd(__hugepd(pmd_val(pmd))))) {\n\t\t\t/*\n\t\t\t * architecture have different format for hugetlbfs\n\t\t\t * pmd format and THP pmd format\n\t\t\t */\n\t\t\tif (!gup_huge_pd(__hugepd(pmd_val(pmd)), addr,\n\t\t\t\t\t PMD_SHIFT, next, write, pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (!gup_pte_range(pmd, addr, next, write, pages, nr))\n\t\t\t\treturn 0;\n\t} while (pmdp++, addr = next, addr != end);\n\n\treturn 1;\n}\n\nstatic int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end,\n\t\t\t int write, struct page **pages, int *nr)\n{\n\tunsigned long next;\n\tpud_t *pudp;\n\n\tpudp = pud_offset(&pgd, addr);\n\tdo {\n\t\tpud_t pud = READ_ONCE(*pudp);\n\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_none(pud))\n\t\t\treturn 0;\n\t\tif (unlikely(pud_huge(pud))) {\n\t\t\tif (!gup_huge_pud(pud, pudp, addr, next, write,\n\t\t\t\t\t  pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (unlikely(is_hugepd(__hugepd(pud_val(pud))))) {\n\t\t\tif (!gup_huge_pd(__hugepd(pud_val(pud)), addr,\n\t\t\t\t\t PUD_SHIFT, next, write, pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (!gup_pmd_range(pud, addr, next, write, pages, nr))\n\t\t\treturn 0;\n\t} while (pudp++, addr = next, addr != end);\n\n\treturn 1;\n}\n\n/*\n * Like get_user_pages_fast() except it's IRQ-safe in that it won't fall back to\n * the regular GUP. It will only return non-negative values.\n */\nint __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long addr, len, end;\n\tunsigned long next, flags;\n\tpgd_t *pgdp;\n\tint nr = 0;\n\n\tstart &= PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,\n\t\t\t\t\tstart, len)))\n\t\treturn 0;\n\n\t/*\n\t * Disable interrupts.  We use the nested form as we can already have\n\t * interrupts disabled by get_futex_key.\n\t *\n\t * With interrupts disabled, we block page table pages from being\n\t * freed from under us. See mmu_gather_tlb in asm-generic/tlb.h\n\t * for more details.\n\t *\n\t * We do not adopt an rcu_read_lock(.) here as we also want to\n\t * block IPIs that come from THPs splitting.\n\t */\n\n\tlocal_irq_save(flags);\n\tpgdp = pgd_offset(mm, addr);\n\tdo {\n\t\tpgd_t pgd = READ_ONCE(*pgdp);\n\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none(pgd))\n\t\t\tbreak;\n\t\tif (unlikely(pgd_huge(pgd))) {\n\t\t\tif (!gup_huge_pgd(pgd, pgdp, addr, next, write,\n\t\t\t\t\t  pages, &nr))\n\t\t\t\tbreak;\n\t\t} else if (unlikely(is_hugepd(__hugepd(pgd_val(pgd))))) {\n\t\t\tif (!gup_huge_pd(__hugepd(pgd_val(pgd)), addr,\n\t\t\t\t\t PGDIR_SHIFT, next, write, pages, &nr))\n\t\t\t\tbreak;\n\t\t} else if (!gup_pud_range(pgd, addr, next, write, pages, &nr))\n\t\t\tbreak;\n\t} while (pgdp++, addr = next, addr != end);\n\tlocal_irq_restore(flags);\n\n\treturn nr;\n}\n\n/**\n * get_user_pages_fast() - pin user pages in memory\n * @start:\tstarting user address\n * @nr_pages:\tnumber of pages from start to pin\n * @write:\twhether pages will be written to\n * @pages:\tarray that receives pointers to the pages pinned.\n *\t\tShould be at least nr_pages long.\n *\n * Attempt to pin user pages in memory without taking mm->mmap_sem.\n * If not successful, it will fall back to taking the lock and\n * calling get_user_pages().\n *\n * Returns number of pages pinned. This may be fewer than the number\n * requested. If nr_pages is 0 or negative, returns 0. If no pages\n * were pinned, returns -errno.\n */\nint get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\tstruct page **pages)\n{\n\tint nr, ret;\n\n\tstart &= PAGE_MASK;\n\tnr = __get_user_pages_fast(start, nr_pages, write, pages);\n\tret = nr;\n\n\tif (nr < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr << PAGE_SHIFT;\n\t\tpages += nr;\n\n\t\tret = get_user_pages_unlocked(start, nr_pages - nr, write, 0, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr;\n\t\t\telse\n\t\t\t\tret += nr;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n#endif /* CONFIG_HAVE_GENERIC_RCU_GUP */\n"], "fixing_code": ["#ifndef _LINUX_MM_H\n#define _LINUX_MM_H\n\n#include <linux/errno.h>\n\n#ifdef __KERNEL__\n\n#include <linux/mmdebug.h>\n#include <linux/gfp.h>\n#include <linux/bug.h>\n#include <linux/list.h>\n#include <linux/mmzone.h>\n#include <linux/rbtree.h>\n#include <linux/atomic.h>\n#include <linux/debug_locks.h>\n#include <linux/mm_types.h>\n#include <linux/range.h>\n#include <linux/pfn.h>\n#include <linux/percpu-refcount.h>\n#include <linux/bit_spinlock.h>\n#include <linux/shrinker.h>\n#include <linux/resource.h>\n#include <linux/page_ext.h>\n#include <linux/err.h>\n#include <linux/page_ref.h>\n\nstruct mempolicy;\nstruct anon_vma;\nstruct anon_vma_chain;\nstruct file_ra_state;\nstruct user_struct;\nstruct writeback_control;\nstruct bdi_writeback;\n\n#ifndef CONFIG_NEED_MULTIPLE_NODES\t/* Don't use mapnrs, do it properly */\nextern unsigned long max_mapnr;\n\nstatic inline void set_max_mapnr(unsigned long limit)\n{\n\tmax_mapnr = limit;\n}\n#else\nstatic inline void set_max_mapnr(unsigned long limit) { }\n#endif\n\nextern unsigned long totalram_pages;\nextern void * high_memory;\nextern int page_cluster;\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_legacy_va_layout;\n#else\n#define sysctl_legacy_va_layout 0\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\nextern const int mmap_rnd_bits_min;\nextern const int mmap_rnd_bits_max;\nextern int mmap_rnd_bits __read_mostly;\n#endif\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\nextern const int mmap_rnd_compat_bits_min;\nextern const int mmap_rnd_compat_bits_max;\nextern int mmap_rnd_compat_bits __read_mostly;\n#endif\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/processor.h>\n\n#ifndef __pa_symbol\n#define __pa_symbol(x)  __pa(RELOC_HIDE((unsigned long)(x), 0))\n#endif\n\n#ifndef page_to_virt\n#define page_to_virt(x)\t__va(PFN_PHYS(page_to_pfn(x)))\n#endif\n\n/*\n * To prevent common memory management code establishing\n * a zero page mapping on a read fault.\n * This macro should be defined within <asm/pgtable.h>.\n * s390 does this to prevent multiplexing of hardware bits\n * related to the physical page in case of virtualization.\n */\n#ifndef mm_forbids_zeropage\n#define mm_forbids_zeropage(X)\t(0)\n#endif\n\n/*\n * Default maximum number of active map areas, this limits the number of vmas\n * per mm struct. Users can overwrite this number by sysctl but there is a\n * problem.\n *\n * When a program's coredump is generated as ELF format, a section is created\n * per a vma. In ELF, the number of sections is represented in unsigned short.\n * This means the number of sections should be smaller than 65535 at coredump.\n * Because the kernel adds some informative sections to a image of program at\n * generating coredump, we need some margin. The number of extra sections is\n * 1-3 now and depends on arch. We use \"5\" as safe margin, here.\n *\n * ELF extended numbering allows more than 65535 sections, so 16-bit bound is\n * not a hard limit any more. Although some userspace tools can be surprised by\n * that.\n */\n#define MAPCOUNT_ELF_CORE_MARGIN\t(5)\n#define DEFAULT_MAX_MAP_COUNT\t(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)\n\nextern int sysctl_max_map_count;\n\nextern unsigned long sysctl_user_reserve_kbytes;\nextern unsigned long sysctl_admin_reserve_kbytes;\n\nextern int sysctl_overcommit_memory;\nextern int sysctl_overcommit_ratio;\nextern unsigned long sysctl_overcommit_kbytes;\n\nextern int overcommit_ratio_handler(struct ctl_table *, int, void __user *,\n\t\t\t\t    size_t *, loff_t *);\nextern int overcommit_kbytes_handler(struct ctl_table *, int, void __user *,\n\t\t\t\t    size_t *, loff_t *);\n\n#define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))\n\n/* to align the pointer to the (next) page boundary */\n#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)\n\n/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */\n#define PAGE_ALIGNED(addr)\tIS_ALIGNED((unsigned long)(addr), PAGE_SIZE)\n\n/*\n * Linux kernel virtual memory manager primitives.\n * The idea being to have a \"virtual\" mm in the same way\n * we have a virtual fs - giving a cleaner interface to the\n * mm details, and allowing different kinds of memory mappings\n * (from shared memory to executable loading to arbitrary\n * mmap() functions).\n */\n\nextern struct kmem_cache *vm_area_cachep;\n\n#ifndef CONFIG_MMU\nextern struct rb_root nommu_region_tree;\nextern struct rw_semaphore nommu_region_sem;\n\nextern unsigned int kobjsize(const void *objp);\n#endif\n\n/*\n * vm_flags in vm_area_struct, see mm_types.h.\n * When changing, update also include/trace/events/mmflags.h\n */\n#define VM_NONE\t\t0x00000000\n\n#define VM_READ\t\t0x00000001\t/* currently active flags */\n#define VM_WRITE\t0x00000002\n#define VM_EXEC\t\t0x00000004\n#define VM_SHARED\t0x00000008\n\n/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */\n#define VM_MAYREAD\t0x00000010\t/* limits for mprotect() etc */\n#define VM_MAYWRITE\t0x00000020\n#define VM_MAYEXEC\t0x00000040\n#define VM_MAYSHARE\t0x00000080\n\n#define VM_GROWSDOWN\t0x00000100\t/* general info on the segment */\n#define VM_UFFD_MISSING\t0x00000200\t/* missing pages tracking */\n#define VM_PFNMAP\t0x00000400\t/* Page-ranges managed without \"struct page\", just pure PFN */\n#define VM_DENYWRITE\t0x00000800\t/* ETXTBSY on write attempts.. */\n#define VM_UFFD_WP\t0x00001000\t/* wrprotect pages tracking */\n\n#define VM_LOCKED\t0x00002000\n#define VM_IO           0x00004000\t/* Memory mapped I/O or similar */\n\n\t\t\t\t\t/* Used by sys_madvise() */\n#define VM_SEQ_READ\t0x00008000\t/* App will access data sequentially */\n#define VM_RAND_READ\t0x00010000\t/* App will not benefit from clustered reads */\n\n#define VM_DONTCOPY\t0x00020000      /* Do not copy this vma on fork */\n#define VM_DONTEXPAND\t0x00040000\t/* Cannot expand with mremap() */\n#define VM_LOCKONFAULT\t0x00080000\t/* Lock the pages covered when they are faulted in */\n#define VM_ACCOUNT\t0x00100000\t/* Is a VM accounted object */\n#define VM_NORESERVE\t0x00200000\t/* should the VM suppress accounting */\n#define VM_HUGETLB\t0x00400000\t/* Huge TLB Page VM */\n#define VM_ARCH_1\t0x01000000\t/* Architecture-specific flag */\n#define VM_ARCH_2\t0x02000000\n#define VM_DONTDUMP\t0x04000000\t/* Do not include in the core dump */\n\n#ifdef CONFIG_MEM_SOFT_DIRTY\n# define VM_SOFTDIRTY\t0x08000000\t/* Not soft dirty clean area */\n#else\n# define VM_SOFTDIRTY\t0\n#endif\n\n#define VM_MIXEDMAP\t0x10000000\t/* Can contain \"struct page\" and pure PFN pages */\n#define VM_HUGEPAGE\t0x20000000\t/* MADV_HUGEPAGE marked this vma */\n#define VM_NOHUGEPAGE\t0x40000000\t/* MADV_NOHUGEPAGE marked this vma */\n#define VM_MERGEABLE\t0x80000000\t/* KSM may merge identical pages */\n\n#ifdef CONFIG_ARCH_USES_HIGH_VMA_FLAGS\n#define VM_HIGH_ARCH_BIT_0\t32\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_1\t33\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_2\t34\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_3\t35\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_0\tBIT(VM_HIGH_ARCH_BIT_0)\n#define VM_HIGH_ARCH_1\tBIT(VM_HIGH_ARCH_BIT_1)\n#define VM_HIGH_ARCH_2\tBIT(VM_HIGH_ARCH_BIT_2)\n#define VM_HIGH_ARCH_3\tBIT(VM_HIGH_ARCH_BIT_3)\n#endif /* CONFIG_ARCH_USES_HIGH_VMA_FLAGS */\n\n#if defined(CONFIG_X86)\n# define VM_PAT\t\tVM_ARCH_1\t/* PAT reserves whole VMA at once (x86) */\n#if defined (CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS)\n# define VM_PKEY_SHIFT\tVM_HIGH_ARCH_BIT_0\n# define VM_PKEY_BIT0\tVM_HIGH_ARCH_0\t/* A protection key is a 4-bit value */\n# define VM_PKEY_BIT1\tVM_HIGH_ARCH_1\n# define VM_PKEY_BIT2\tVM_HIGH_ARCH_2\n# define VM_PKEY_BIT3\tVM_HIGH_ARCH_3\n#endif\n#elif defined(CONFIG_PPC)\n# define VM_SAO\t\tVM_ARCH_1\t/* Strong Access Ordering (powerpc) */\n#elif defined(CONFIG_PARISC)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif defined(CONFIG_METAG)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif defined(CONFIG_IA64)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif !defined(CONFIG_MMU)\n# define VM_MAPPED_COPY\tVM_ARCH_1\t/* T if mapped copy of data (nommu mmap) */\n#endif\n\n#if defined(CONFIG_X86)\n/* MPX specific bounds table or bounds directory */\n# define VM_MPX\t\tVM_ARCH_2\n#endif\n\n#ifndef VM_GROWSUP\n# define VM_GROWSUP\tVM_NONE\n#endif\n\n/* Bits set in the VMA until the stack is in its final location */\n#define VM_STACK_INCOMPLETE_SETUP\t(VM_RAND_READ | VM_SEQ_READ)\n\n#ifndef VM_STACK_DEFAULT_FLAGS\t\t/* arch can override this */\n#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS\n#endif\n\n#ifdef CONFIG_STACK_GROWSUP\n#define VM_STACK\tVM_GROWSUP\n#else\n#define VM_STACK\tVM_GROWSDOWN\n#endif\n\n#define VM_STACK_FLAGS\t(VM_STACK | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)\n\n/*\n * Special vmas that are non-mergable, non-mlock()able.\n * Note: mm/huge_memory.c VM_NO_THP depends on this definition.\n */\n#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_PFNMAP | VM_MIXEDMAP)\n\n/* This mask defines which mm->def_flags a process can inherit its parent */\n#define VM_INIT_DEF_MASK\tVM_NOHUGEPAGE\n\n/* This mask is used to clear all the VMA flags used by mlock */\n#define VM_LOCKED_CLEAR_MASK\t(~(VM_LOCKED | VM_LOCKONFAULT))\n\n/*\n * mapping from the currently active vm_flags protection bits (the\n * low four bits) to a page protection mask..\n */\nextern pgprot_t protection_map[16];\n\n#define FAULT_FLAG_WRITE\t0x01\t/* Fault was a write access */\n#define FAULT_FLAG_MKWRITE\t0x02\t/* Fault was mkwrite of existing pte */\n#define FAULT_FLAG_ALLOW_RETRY\t0x04\t/* Retry fault if blocking */\n#define FAULT_FLAG_RETRY_NOWAIT\t0x08\t/* Don't drop mmap_sem and wait when retrying */\n#define FAULT_FLAG_KILLABLE\t0x10\t/* The fault task is in SIGKILL killable region */\n#define FAULT_FLAG_TRIED\t0x20\t/* Second try */\n#define FAULT_FLAG_USER\t\t0x40\t/* The fault originated in userspace */\n#define FAULT_FLAG_REMOTE\t0x80\t/* faulting for non current tsk/mm */\n#define FAULT_FLAG_INSTRUCTION  0x100\t/* The fault was during an instruction fetch */\n\n/*\n * vm_fault is filled by the the pagefault handler and passed to the vma's\n * ->fault function. The vma's ->fault is responsible for returning a bitmask\n * of VM_FAULT_xxx flags that give details about how the fault was handled.\n *\n * MM layer fills up gfp_mask for page allocations but fault handler might\n * alter it if its implementation requires a different allocation context.\n *\n * pgoff should be used in favour of virtual_address, if possible.\n */\nstruct vm_fault {\n\tunsigned int flags;\t\t/* FAULT_FLAG_xxx flags */\n\tgfp_t gfp_mask;\t\t\t/* gfp mask to be used for allocations */\n\tpgoff_t pgoff;\t\t\t/* Logical page offset based on vma */\n\tvoid __user *virtual_address;\t/* Faulting virtual address */\n\n\tstruct page *cow_page;\t\t/* Handler may choose to COW */\n\tstruct page *page;\t\t/* ->fault handlers should return a\n\t\t\t\t\t * page here, unless VM_FAULT_NOPAGE\n\t\t\t\t\t * is set (which is also implied by\n\t\t\t\t\t * VM_FAULT_ERROR).\n\t\t\t\t\t */\n\tvoid *entry;\t\t\t/* ->fault handler can alternatively\n\t\t\t\t\t * return locked DAX entry. In that\n\t\t\t\t\t * case handler should return\n\t\t\t\t\t * VM_FAULT_DAX_LOCKED and fill in\n\t\t\t\t\t * entry here.\n\t\t\t\t\t */\n};\n\n/*\n * Page fault context: passes though page fault handler instead of endless list\n * of function arguments.\n */\nstruct fault_env {\n\tstruct vm_area_struct *vma;\t/* Target VMA */\n\tunsigned long address;\t\t/* Faulting virtual address */\n\tunsigned int flags;\t\t/* FAULT_FLAG_xxx flags */\n\tpmd_t *pmd;\t\t\t/* Pointer to pmd entry matching\n\t\t\t\t\t * the 'address'\n\t\t\t\t\t */\n\tpte_t *pte;\t\t\t/* Pointer to pte entry matching\n\t\t\t\t\t * the 'address'. NULL if the page\n\t\t\t\t\t * table hasn't been allocated.\n\t\t\t\t\t */\n\tspinlock_t *ptl;\t\t/* Page table lock.\n\t\t\t\t\t * Protects pte page table if 'pte'\n\t\t\t\t\t * is not NULL, otherwise pmd.\n\t\t\t\t\t */\n\tpgtable_t prealloc_pte;\t\t/* Pre-allocated pte page table.\n\t\t\t\t\t * vm_ops->map_pages() calls\n\t\t\t\t\t * alloc_set_pte() from atomic context.\n\t\t\t\t\t * do_fault_around() pre-allocates\n\t\t\t\t\t * page table to avoid allocation from\n\t\t\t\t\t * atomic context.\n\t\t\t\t\t */\n};\n\n/*\n * These are the virtual MM functions - opening of an area, closing and\n * unmapping it (needed to keep files on disk up-to-date etc), pointer\n * to the functions called when a no-page or a wp-page exception occurs. \n */\nstruct vm_operations_struct {\n\tvoid (*open)(struct vm_area_struct * area);\n\tvoid (*close)(struct vm_area_struct * area);\n\tint (*mremap)(struct vm_area_struct * area);\n\tint (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\tint (*pmd_fault)(struct vm_area_struct *, unsigned long address,\n\t\t\t\t\t\tpmd_t *, unsigned int flags);\n\tvoid (*map_pages)(struct fault_env *fe,\n\t\t\tpgoff_t start_pgoff, pgoff_t end_pgoff);\n\n\t/* notification that a previously read-only page is about to become\n\t * writable, if an error is returned it will cause a SIGBUS */\n\tint (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n\t/* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */\n\tint (*pfn_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n\t/* called by access_process_vm when get_user_pages() fails, typically\n\t * for use by special VMAs that can switch between memory and hardware\n\t */\n\tint (*access)(struct vm_area_struct *vma, unsigned long addr,\n\t\t      void *buf, int len, int write);\n\n\t/* Called by the /proc/PID/maps code to ask the vma whether it\n\t * has a special name.  Returning non-NULL will also cause this\n\t * vma to be dumped unconditionally. */\n\tconst char *(*name)(struct vm_area_struct *vma);\n\n#ifdef CONFIG_NUMA\n\t/*\n\t * set_policy() op must add a reference to any non-NULL @new mempolicy\n\t * to hold the policy upon return.  Caller should pass NULL @new to\n\t * remove a policy and fall back to surrounding context--i.e. do not\n\t * install a MPOL_DEFAULT policy, nor the task or system default\n\t * mempolicy.\n\t */\n\tint (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);\n\n\t/*\n\t * get_policy() op must add reference [mpol_get()] to any policy at\n\t * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure\n\t * in mm/mempolicy.c will do this automatically.\n\t * get_policy() must NOT add a ref if the policy at (vma,addr) is not\n\t * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.\n\t * If no [shared/vma] mempolicy exists at the addr, get_policy() op\n\t * must return NULL--i.e., do not \"fallback\" to task or system default\n\t * policy.\n\t */\n\tstruct mempolicy *(*get_policy)(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long addr);\n#endif\n\t/*\n\t * Called by vm_normal_page() for special PTEs to find the\n\t * page for @addr.  This is useful if the default behavior\n\t * (using pte_page()) would not find the correct page.\n\t */\n\tstruct page *(*find_special_page)(struct vm_area_struct *vma,\n\t\t\t\t\t  unsigned long addr);\n};\n\nstruct mmu_gather;\nstruct inode;\n\n#define page_private(page)\t\t((page)->private)\n#define set_page_private(page, v)\t((page)->private = (v))\n\n#if !defined(__HAVE_ARCH_PTE_DEVMAP) || !defined(CONFIG_TRANSPARENT_HUGEPAGE)\nstatic inline int pmd_devmap(pmd_t pmd)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * FIXME: take this include out, include page-flags.h in\n * files which need it (119 of them)\n */\n#include <linux/page-flags.h>\n#include <linux/huge_mm.h>\n\n/*\n * Methods to modify the page usage count.\n *\n * What counts for a page usage:\n * - cache mapping   (page->mapping)\n * - private data    (page->private)\n * - page mapped in a task's page tables, each mapping\n *   is counted separately\n *\n * Also, many kernel routines increase the page count before a critical\n * routine so they can be sure the page doesn't go away from under them.\n */\n\n/*\n * Drop a ref, return true if the refcount fell to zero (the page has no users)\n */\nstatic inline int put_page_testzero(struct page *page)\n{\n\tVM_BUG_ON_PAGE(page_ref_count(page) == 0, page);\n\treturn page_ref_dec_and_test(page);\n}\n\n/*\n * Try to grab a ref unless the page has a refcount of zero, return false if\n * that is the case.\n * This can be called when MMU is off so it must not access\n * any of the virtual mappings.\n */\nstatic inline int get_page_unless_zero(struct page *page)\n{\n\treturn page_ref_add_unless(page, 1, 0);\n}\n\nextern int page_is_ram(unsigned long pfn);\n\nenum {\n\tREGION_INTERSECTS,\n\tREGION_DISJOINT,\n\tREGION_MIXED,\n};\n\nint region_intersects(resource_size_t offset, size_t size, unsigned long flags,\n\t\t      unsigned long desc);\n\n/* Support for virtually mapped pages */\nstruct page *vmalloc_to_page(const void *addr);\nunsigned long vmalloc_to_pfn(const void *addr);\n\n/*\n * Determine if an address is within the vmalloc range\n *\n * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there\n * is no special casing required.\n */\nstatic inline bool is_vmalloc_addr(const void *x)\n{\n#ifdef CONFIG_MMU\n\tunsigned long addr = (unsigned long)x;\n\n\treturn addr >= VMALLOC_START && addr < VMALLOC_END;\n#else\n\treturn false;\n#endif\n}\n#ifdef CONFIG_MMU\nextern int is_vmalloc_or_module_addr(const void *x);\n#else\nstatic inline int is_vmalloc_or_module_addr(const void *x)\n{\n\treturn 0;\n}\n#endif\n\nextern void kvfree(const void *addr);\n\nstatic inline atomic_t *compound_mapcount_ptr(struct page *page)\n{\n\treturn &page[1].compound_mapcount;\n}\n\nstatic inline int compound_mapcount(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageCompound(page), page);\n\tpage = compound_head(page);\n\treturn atomic_read(compound_mapcount_ptr(page)) + 1;\n}\n\n/*\n * The atomic page->_mapcount, starts from -1: so that transitions\n * both from it and to it can be tracked, using atomic_inc_and_test\n * and atomic_add_negative(-1).\n */\nstatic inline void page_mapcount_reset(struct page *page)\n{\n\tatomic_set(&(page)->_mapcount, -1);\n}\n\nint __page_mapcount(struct page *page);\n\nstatic inline int page_mapcount(struct page *page)\n{\n\tVM_BUG_ON_PAGE(PageSlab(page), page);\n\n\tif (unlikely(PageCompound(page)))\n\t\treturn __page_mapcount(page);\n\treturn atomic_read(&page->_mapcount) + 1;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nint total_mapcount(struct page *page);\nint page_trans_huge_mapcount(struct page *page, int *total_mapcount);\n#else\nstatic inline int total_mapcount(struct page *page)\n{\n\treturn page_mapcount(page);\n}\nstatic inline int page_trans_huge_mapcount(struct page *page,\n\t\t\t\t\t   int *total_mapcount)\n{\n\tint mapcount = page_mapcount(page);\n\tif (total_mapcount)\n\t\t*total_mapcount = mapcount;\n\treturn mapcount;\n}\n#endif\n\nstatic inline struct page *virt_to_head_page(const void *x)\n{\n\tstruct page *page = virt_to_page(x);\n\n\treturn compound_head(page);\n}\n\nvoid __put_page(struct page *page);\n\nvoid put_pages_list(struct list_head *pages);\n\nvoid split_page(struct page *page, unsigned int order);\n\n/*\n * Compound pages have a destructor function.  Provide a\n * prototype for that function and accessor functions.\n * These are _only_ valid on the head of a compound page.\n */\ntypedef void compound_page_dtor(struct page *);\n\n/* Keep the enum in sync with compound_page_dtors array in mm/page_alloc.c */\nenum compound_dtor_id {\n\tNULL_COMPOUND_DTOR,\n\tCOMPOUND_PAGE_DTOR,\n#ifdef CONFIG_HUGETLB_PAGE\n\tHUGETLB_PAGE_DTOR,\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tTRANSHUGE_PAGE_DTOR,\n#endif\n\tNR_COMPOUND_DTORS,\n};\nextern compound_page_dtor * const compound_page_dtors[];\n\nstatic inline void set_compound_page_dtor(struct page *page,\n\t\tenum compound_dtor_id compound_dtor)\n{\n\tVM_BUG_ON_PAGE(compound_dtor >= NR_COMPOUND_DTORS, page);\n\tpage[1].compound_dtor = compound_dtor;\n}\n\nstatic inline compound_page_dtor *get_compound_page_dtor(struct page *page)\n{\n\tVM_BUG_ON_PAGE(page[1].compound_dtor >= NR_COMPOUND_DTORS, page);\n\treturn compound_page_dtors[page[1].compound_dtor];\n}\n\nstatic inline unsigned int compound_order(struct page *page)\n{\n\tif (!PageHead(page))\n\t\treturn 0;\n\treturn page[1].compound_order;\n}\n\nstatic inline void set_compound_order(struct page *page, unsigned int order)\n{\n\tpage[1].compound_order = order;\n}\n\nvoid free_compound_page(struct page *page);\n\n#ifdef CONFIG_MMU\n/*\n * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when\n * servicing faults for write access.  In the normal case, do always want\n * pte_mkwrite.  But get_user_pages can cause write faults for mappings\n * that do not have writing enabled, when used by access_process_vm.\n */\nstatic inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)\n{\n\tif (likely(vma->vm_flags & VM_WRITE))\n\t\tpte = pte_mkwrite(pte);\n\treturn pte;\n}\n\nint alloc_set_pte(struct fault_env *fe, struct mem_cgroup *memcg,\n\t\tstruct page *page);\n#endif\n\n/*\n * Multiple processes may \"see\" the same page. E.g. for untouched\n * mappings of /dev/null, all processes see the same page full of\n * zeroes, and text pages of executables and shared libraries have\n * only one copy in memory, at most, normally.\n *\n * For the non-reserved pages, page_count(page) denotes a reference count.\n *   page_count() == 0 means the page is free. page->lru is then used for\n *   freelist management in the buddy allocator.\n *   page_count() > 0  means the page has been allocated.\n *\n * Pages are allocated by the slab allocator in order to provide memory\n * to kmalloc and kmem_cache_alloc. In this case, the management of the\n * page, and the fields in 'struct page' are the responsibility of mm/slab.c\n * unless a particular usage is carefully commented. (the responsibility of\n * freeing the kmalloc memory is the caller's, of course).\n *\n * A page may be used by anyone else who does a __get_free_page().\n * In this case, page_count still tracks the references, and should only\n * be used through the normal accessor functions. The top bits of page->flags\n * and page->virtual store page management information, but all other fields\n * are unused and could be used privately, carefully. The management of this\n * page is the responsibility of the one who allocated it, and those who have\n * subsequently been given references to it.\n *\n * The other pages (we may call them \"pagecache pages\") are completely\n * managed by the Linux memory manager: I/O, buffers, swapping etc.\n * The following discussion applies only to them.\n *\n * A pagecache page contains an opaque `private' member, which belongs to the\n * page's address_space. Usually, this is the address of a circular list of\n * the page's disk buffers. PG_private must be set to tell the VM to call\n * into the filesystem to release these pages.\n *\n * A page may belong to an inode's memory mapping. In this case, page->mapping\n * is the pointer to the inode, and page->index is the file offset of the page,\n * in units of PAGE_SIZE.\n *\n * If pagecache pages are not associated with an inode, they are said to be\n * anonymous pages. These may become associated with the swapcache, and in that\n * case PG_swapcache is set, and page->private is an offset into the swapcache.\n *\n * In either case (swapcache or inode backed), the pagecache itself holds one\n * reference to the page. Setting PG_private should also increment the\n * refcount. The each user mapping also has a reference to the page.\n *\n * The pagecache pages are stored in a per-mapping radix tree, which is\n * rooted at mapping->page_tree, and indexed by offset.\n * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space\n * lists, we instead now tag pages as dirty/writeback in the radix tree.\n *\n * All pagecache pages may be subject to I/O:\n * - inode pages may need to be read from disk,\n * - inode pages which have been modified and are MAP_SHARED may need\n *   to be written back to the inode on disk,\n * - anonymous pages (including MAP_PRIVATE file mappings) which have been\n *   modified may need to be swapped out to swap space and (later) to be read\n *   back into memory.\n */\n\n/*\n * The zone field is never updated after free_area_init_core()\n * sets it, so none of the operations on it need to be atomic.\n */\n\n/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */\n#define SECTIONS_PGOFF\t\t((sizeof(unsigned long)*8) - SECTIONS_WIDTH)\n#define NODES_PGOFF\t\t(SECTIONS_PGOFF - NODES_WIDTH)\n#define ZONES_PGOFF\t\t(NODES_PGOFF - ZONES_WIDTH)\n#define LAST_CPUPID_PGOFF\t(ZONES_PGOFF - LAST_CPUPID_WIDTH)\n\n/*\n * Define the bit shifts to access each section.  For non-existent\n * sections we define the shift as 0; that plus a 0 mask ensures\n * the compiler will optimise away reference to them.\n */\n#define SECTIONS_PGSHIFT\t(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))\n#define NODES_PGSHIFT\t\t(NODES_PGOFF * (NODES_WIDTH != 0))\n#define ZONES_PGSHIFT\t\t(ZONES_PGOFF * (ZONES_WIDTH != 0))\n#define LAST_CPUPID_PGSHIFT\t(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))\n\n/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */\n#ifdef NODE_NOT_IN_PAGE_FLAGS\n#define ZONEID_SHIFT\t\t(SECTIONS_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((SECTIONS_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tSECTIONS_PGOFF : ZONES_PGOFF)\n#else\n#define ZONEID_SHIFT\t\t(NODES_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((NODES_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tNODES_PGOFF : ZONES_PGOFF)\n#endif\n\n#define ZONEID_PGSHIFT\t\t(ZONEID_PGOFF * (ZONEID_SHIFT != 0))\n\n#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\n#error SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\n#endif\n\n#define ZONES_MASK\t\t((1UL << ZONES_WIDTH) - 1)\n#define NODES_MASK\t\t((1UL << NODES_WIDTH) - 1)\n#define SECTIONS_MASK\t\t((1UL << SECTIONS_WIDTH) - 1)\n#define LAST_CPUPID_MASK\t((1UL << LAST_CPUPID_SHIFT) - 1)\n#define ZONEID_MASK\t\t((1UL << ZONEID_SHIFT) - 1)\n\nstatic inline enum zone_type page_zonenum(const struct page *page)\n{\n\treturn (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nvoid get_zone_device_page(struct page *page);\nvoid put_zone_device_page(struct page *page);\nstatic inline bool is_zone_device_page(const struct page *page)\n{\n\treturn page_zonenum(page) == ZONE_DEVICE;\n}\n#else\nstatic inline void get_zone_device_page(struct page *page)\n{\n}\nstatic inline void put_zone_device_page(struct page *page)\n{\n}\nstatic inline bool is_zone_device_page(const struct page *page)\n{\n\treturn false;\n}\n#endif\n\nstatic inline void get_page(struct page *page)\n{\n\tpage = compound_head(page);\n\t/*\n\t * Getting a normal page or the head of a compound page\n\t * requires to already have an elevated page->_refcount.\n\t */\n\tVM_BUG_ON_PAGE(page_ref_count(page) <= 0, page);\n\tpage_ref_inc(page);\n\n\tif (unlikely(is_zone_device_page(page)))\n\t\tget_zone_device_page(page);\n}\n\nstatic inline void put_page(struct page *page)\n{\n\tpage = compound_head(page);\n\n\tif (put_page_testzero(page))\n\t\t__put_page(page);\n\n\tif (unlikely(is_zone_device_page(page)))\n\t\tput_zone_device_page(page);\n}\n\n#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n#define SECTION_IN_PAGE_FLAGS\n#endif\n\n/*\n * The identification function is mainly used by the buddy allocator for\n * determining if two pages could be buddies. We are not really identifying\n * the zone since we could be using the section number id if we do not have\n * node id available in page flags.\n * We only guarantee that it will return the same value for two combinable\n * pages in a zone.\n */\nstatic inline int page_zone_id(struct page *page)\n{\n\treturn (page->flags >> ZONEID_PGSHIFT) & ZONEID_MASK;\n}\n\nstatic inline int zone_to_nid(struct zone *zone)\n{\n#ifdef CONFIG_NUMA\n\treturn zone->node;\n#else\n\treturn 0;\n#endif\n}\n\n#ifdef NODE_NOT_IN_PAGE_FLAGS\nextern int page_to_nid(const struct page *page);\n#else\nstatic inline int page_to_nid(const struct page *page)\n{\n\treturn (page->flags >> NODES_PGSHIFT) & NODES_MASK;\n}\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline int cpu_pid_to_cpupid(int cpu, int pid)\n{\n\treturn ((cpu & LAST__CPU_MASK) << LAST__PID_SHIFT) | (pid & LAST__PID_MASK);\n}\n\nstatic inline int cpupid_to_pid(int cpupid)\n{\n\treturn cpupid & LAST__PID_MASK;\n}\n\nstatic inline int cpupid_to_cpu(int cpupid)\n{\n\treturn (cpupid >> LAST__PID_SHIFT) & LAST__CPU_MASK;\n}\n\nstatic inline int cpupid_to_nid(int cpupid)\n{\n\treturn cpu_to_node(cpupid_to_cpu(cpupid));\n}\n\nstatic inline bool cpupid_pid_unset(int cpupid)\n{\n\treturn cpupid_to_pid(cpupid) == (-1 & LAST__PID_MASK);\n}\n\nstatic inline bool cpupid_cpu_unset(int cpupid)\n{\n\treturn cpupid_to_cpu(cpupid) == (-1 & LAST__CPU_MASK);\n}\n\nstatic inline bool __cpupid_match_pid(pid_t task_pid, int cpupid)\n{\n\treturn (task_pid & LAST__PID_MASK) == cpupid_to_pid(cpupid);\n}\n\n#define cpupid_match_pid(task, cpupid) __cpupid_match_pid(task->pid, cpupid)\n#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\nstatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\n{\n\treturn xchg(&page->_last_cpupid, cpupid & LAST_CPUPID_MASK);\n}\n\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn page->_last_cpupid;\n}\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n\tpage->_last_cpupid = -1 & LAST_CPUPID_MASK;\n}\n#else\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn (page->flags >> LAST_CPUPID_PGSHIFT) & LAST_CPUPID_MASK;\n}\n\nextern int page_cpupid_xchg_last(struct page *page, int cpupid);\n\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n\tpage->flags |= LAST_CPUPID_MASK << LAST_CPUPID_PGSHIFT;\n}\n#endif /* LAST_CPUPID_NOT_IN_PAGE_FLAGS */\n#else /* !CONFIG_NUMA_BALANCING */\nstatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\n{\n\treturn page_to_nid(page); /* XXX */\n}\n\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn page_to_nid(page); /* XXX */\n}\n\nstatic inline int cpupid_to_nid(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpupid_to_pid(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpupid_to_cpu(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpu_pid_to_cpupid(int nid, int pid)\n{\n\treturn -1;\n}\n\nstatic inline bool cpupid_pid_unset(int cpupid)\n{\n\treturn 1;\n}\n\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n}\n\nstatic inline bool cpupid_match_pid(struct task_struct *task, int cpupid)\n{\n\treturn false;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\nstatic inline struct zone *page_zone(const struct page *page)\n{\n\treturn &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];\n}\n\nstatic inline pg_data_t *page_pgdat(const struct page *page)\n{\n\treturn NODE_DATA(page_to_nid(page));\n}\n\n#ifdef SECTION_IN_PAGE_FLAGS\nstatic inline void set_page_section(struct page *page, unsigned long section)\n{\n\tpage->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);\n\tpage->flags |= (section & SECTIONS_MASK) << SECTIONS_PGSHIFT;\n}\n\nstatic inline unsigned long page_to_section(const struct page *page)\n{\n\treturn (page->flags >> SECTIONS_PGSHIFT) & SECTIONS_MASK;\n}\n#endif\n\nstatic inline void set_page_zone(struct page *page, enum zone_type zone)\n{\n\tpage->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);\n\tpage->flags |= (zone & ZONES_MASK) << ZONES_PGSHIFT;\n}\n\nstatic inline void set_page_node(struct page *page, unsigned long node)\n{\n\tpage->flags &= ~(NODES_MASK << NODES_PGSHIFT);\n\tpage->flags |= (node & NODES_MASK) << NODES_PGSHIFT;\n}\n\nstatic inline void set_page_links(struct page *page, enum zone_type zone,\n\tunsigned long node, unsigned long pfn)\n{\n\tset_page_zone(page, zone);\n\tset_page_node(page, node);\n#ifdef SECTION_IN_PAGE_FLAGS\n\tset_page_section(page, pfn_to_section_nr(pfn));\n#endif\n}\n\n#ifdef CONFIG_MEMCG\nstatic inline struct mem_cgroup *page_memcg(struct page *page)\n{\n\treturn page->mem_cgroup;\n}\nstatic inline struct mem_cgroup *page_memcg_rcu(struct page *page)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\treturn READ_ONCE(page->mem_cgroup);\n}\n#else\nstatic inline struct mem_cgroup *page_memcg(struct page *page)\n{\n\treturn NULL;\n}\nstatic inline struct mem_cgroup *page_memcg_rcu(struct page *page)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\treturn NULL;\n}\n#endif\n\n/*\n * Some inline functions in vmstat.h depend on page_zone()\n */\n#include <linux/vmstat.h>\n\nstatic __always_inline void *lowmem_page_address(const struct page *page)\n{\n\treturn page_to_virt(page);\n}\n\n#if defined(CONFIG_HIGHMEM) && !defined(WANT_PAGE_VIRTUAL)\n#define HASHED_PAGE_VIRTUAL\n#endif\n\n#if defined(WANT_PAGE_VIRTUAL)\nstatic inline void *page_address(const struct page *page)\n{\n\treturn page->virtual;\n}\nstatic inline void set_page_address(struct page *page, void *address)\n{\n\tpage->virtual = address;\n}\n#define page_address_init()  do { } while(0)\n#endif\n\n#if defined(HASHED_PAGE_VIRTUAL)\nvoid *page_address(const struct page *page);\nvoid set_page_address(struct page *page, void *virtual);\nvoid page_address_init(void);\n#endif\n\n#if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)\n#define page_address(page) lowmem_page_address(page)\n#define set_page_address(page, address)  do { } while(0)\n#define page_address_init()  do { } while(0)\n#endif\n\nextern void *page_rmapping(struct page *page);\nextern struct anon_vma *page_anon_vma(struct page *page);\nextern struct address_space *page_mapping(struct page *page);\n\nextern struct address_space *__page_file_mapping(struct page *);\n\nstatic inline\nstruct address_space *page_file_mapping(struct page *page)\n{\n\tif (unlikely(PageSwapCache(page)))\n\t\treturn __page_file_mapping(page);\n\n\treturn page->mapping;\n}\n\nextern pgoff_t __page_file_index(struct page *page);\n\n/*\n * Return the pagecache index of the passed page.  Regular pagecache pages\n * use ->index whereas swapcache pages use swp_offset(->private)\n */\nstatic inline pgoff_t page_index(struct page *page)\n{\n\tif (unlikely(PageSwapCache(page)))\n\t\treturn __page_file_index(page);\n\treturn page->index;\n}\n\nbool page_mapped(struct page *page);\nstruct address_space *page_mapping(struct page *page);\n\n/*\n * Return true only if the page has been allocated with\n * ALLOC_NO_WATERMARKS and the low watermark was not\n * met implying that the system is under some pressure.\n */\nstatic inline bool page_is_pfmemalloc(struct page *page)\n{\n\t/*\n\t * Page index cannot be this large so this must be\n\t * a pfmemalloc page.\n\t */\n\treturn page->index == -1UL;\n}\n\n/*\n * Only to be called by the page allocator on a freshly allocated\n * page.\n */\nstatic inline void set_page_pfmemalloc(struct page *page)\n{\n\tpage->index = -1UL;\n}\n\nstatic inline void clear_page_pfmemalloc(struct page *page)\n{\n\tpage->index = 0;\n}\n\n/*\n * Different kinds of faults, as returned by handle_mm_fault().\n * Used to decide whether a process gets delivered SIGBUS or\n * just gets major/minor fault counters bumped up.\n */\n\n#define VM_FAULT_OOM\t0x0001\n#define VM_FAULT_SIGBUS\t0x0002\n#define VM_FAULT_MAJOR\t0x0004\n#define VM_FAULT_WRITE\t0x0008\t/* Special case for get_user_pages */\n#define VM_FAULT_HWPOISON 0x0010\t/* Hit poisoned small page */\n#define VM_FAULT_HWPOISON_LARGE 0x0020  /* Hit poisoned large page. Index encoded in upper bits */\n#define VM_FAULT_SIGSEGV 0x0040\n\n#define VM_FAULT_NOPAGE\t0x0100\t/* ->fault installed the pte, not return page */\n#define VM_FAULT_LOCKED\t0x0200\t/* ->fault locked the returned page */\n#define VM_FAULT_RETRY\t0x0400\t/* ->fault blocked, must retry */\n#define VM_FAULT_FALLBACK 0x0800\t/* huge page fault failed, fall back to small */\n#define VM_FAULT_DAX_LOCKED 0x1000\t/* ->fault has locked DAX entry */\n\n#define VM_FAULT_HWPOISON_LARGE_MASK 0xf000 /* encodes hpage index for large hwpoison */\n\n#define VM_FAULT_ERROR\t(VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | \\\n\t\t\t VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE | \\\n\t\t\t VM_FAULT_FALLBACK)\n\n/* Encode hstate index for a hwpoisoned large page */\n#define VM_FAULT_SET_HINDEX(x) ((x) << 12)\n#define VM_FAULT_GET_HINDEX(x) (((x) >> 12) & 0xf)\n\n/*\n * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.\n */\nextern void pagefault_out_of_memory(void);\n\n#define offset_in_page(p)\t((unsigned long)(p) & ~PAGE_MASK)\n\n/*\n * Flags passed to show_mem() and show_free_areas() to suppress output in\n * various contexts.\n */\n#define SHOW_MEM_FILTER_NODES\t\t(0x0001u)\t/* disallowed nodes */\n\nextern void show_free_areas(unsigned int flags);\nextern bool skip_free_areas_node(unsigned int flags, int nid);\n\nint shmem_zero_setup(struct vm_area_struct *);\n#ifdef CONFIG_SHMEM\nbool shmem_mapping(struct address_space *mapping);\n#else\nstatic inline bool shmem_mapping(struct address_space *mapping)\n{\n\treturn false;\n}\n#endif\n\nextern bool can_do_mlock(void);\nextern int user_shm_lock(size_t, struct user_struct *);\nextern void user_shm_unlock(size_t, struct user_struct *);\n\n/*\n * Parameter block passed down to zap_pte_range in exceptional cases.\n */\nstruct zap_details {\n\tstruct address_space *check_mapping;\t/* Check page->mapping if set */\n\tpgoff_t\tfirst_index;\t\t\t/* Lowest page->index to unmap */\n\tpgoff_t last_index;\t\t\t/* Highest page->index to unmap */\n\tbool ignore_dirty;\t\t\t/* Ignore dirty pages */\n\tbool check_swap_entries;\t\t/* Check also swap entries */\n};\n\nstruct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,\n\t\tpte_t pte);\nstruct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tpmd_t pmd);\n\nint zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size);\nvoid zap_page_range(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size, struct zap_details *);\nvoid unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,\n\t\tunsigned long start, unsigned long end);\n\n/**\n * mm_walk - callbacks for walk_page_range\n * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry\n *\t       this handler is required to be able to handle\n *\t       pmd_trans_huge() pmds.  They may simply choose to\n *\t       split_huge_page() instead of handling it explicitly.\n * @pte_entry: if set, called for each non-empty PTE (4th-level) entry\n * @pte_hole: if set, called for each hole at all levels\n * @hugetlb_entry: if set, called for each hugetlb entry\n * @test_walk: caller specific callback function to determine whether\n *             we walk over the current vma or not. Returning 0\n *             value means \"do page table walk over the current vma,\"\n *             and a negative one means \"abort current page table walk\n *             right now.\" 1 means \"skip the current vma.\"\n * @mm:        mm_struct representing the target process of page table walk\n * @vma:       vma currently walked (NULL if walking outside vmas)\n * @private:   private data for callbacks' usage\n *\n * (see the comment on walk_page_range() for more details)\n */\nstruct mm_walk {\n\tint (*pmd_entry)(pmd_t *pmd, unsigned long addr,\n\t\t\t unsigned long next, struct mm_walk *walk);\n\tint (*pte_entry)(pte_t *pte, unsigned long addr,\n\t\t\t unsigned long next, struct mm_walk *walk);\n\tint (*pte_hole)(unsigned long addr, unsigned long next,\n\t\t\tstruct mm_walk *walk);\n\tint (*hugetlb_entry)(pte_t *pte, unsigned long hmask,\n\t\t\t     unsigned long addr, unsigned long next,\n\t\t\t     struct mm_walk *walk);\n\tint (*test_walk)(unsigned long addr, unsigned long next,\n\t\t\tstruct mm_walk *walk);\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tvoid *private;\n};\n\nint walk_page_range(unsigned long addr, unsigned long end,\n\t\tstruct mm_walk *walk);\nint walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk);\nvoid free_pgd_range(struct mmu_gather *tlb, unsigned long addr,\n\t\tunsigned long end, unsigned long floor, unsigned long ceiling);\nint copy_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\tstruct vm_area_struct *vma);\nvoid unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows);\nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn);\nint follow_phys(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned int flags, unsigned long *prot, resource_size_t *phys);\nint generic_access_phys(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tvoid *buf, int len, int write);\n\nstatic inline void unmap_shared_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen)\n{\n\tunmap_mapping_range(mapping, holebegin, holelen, 0);\n}\n\nextern void truncate_pagecache(struct inode *inode, loff_t new);\nextern void truncate_setsize(struct inode *inode, loff_t newsize);\nvoid pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to);\nvoid truncate_pagecache_range(struct inode *inode, loff_t offset, loff_t end);\nint truncate_inode_page(struct address_space *mapping, struct page *page);\nint generic_error_remove_page(struct address_space *mapping, struct page *page);\nint invalidate_inode_page(struct page *page);\n\n#ifdef CONFIG_MMU\nextern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned int flags);\nextern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\t    unsigned long address, unsigned int fault_flags,\n\t\t\t    bool *unlocked);\n#else\nstatic inline int handle_mm_fault(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int flags)\n{\n\t/* should never happen if there's no MMU */\n\tBUG();\n\treturn VM_FAULT_SIGBUS;\n}\nstatic inline int fixup_user_fault(struct task_struct *tsk,\n\t\tstruct mm_struct *mm, unsigned long address,\n\t\tunsigned int fault_flags, bool *unlocked)\n{\n\t/* should never happen if there's no MMU */\n\tBUG();\n\treturn -EFAULT;\n}\n#endif\n\nextern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);\nextern int access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, int write);\n\nlong __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t      unsigned long start, unsigned long nr_pages,\n\t\t      unsigned int foll_flags, struct page **pages,\n\t\t      struct vm_area_struct **vmas, int *nonblocking);\nlong get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\t    unsigned long start, unsigned long nr_pages,\n\t\t\t    int write, int force, struct page **pages,\n\t\t\t    struct vm_area_struct **vmas);\nlong get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t\t    int write, int force, struct page **pages,\n\t\t\t    struct vm_area_struct **vmas);\nlong get_user_pages_locked(unsigned long start, unsigned long nr_pages,\n\t\t    int write, int force, struct page **pages, int *locked);\nlong __get_user_pages_unlocked(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\t       unsigned long start, unsigned long nr_pages,\n\t\t\t       int write, int force, struct page **pages,\n\t\t\t       unsigned int gup_flags);\nlong get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t    int write, int force, struct page **pages);\nint get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\tstruct page **pages);\n\n/* Container for pinned pfns / pages */\nstruct frame_vector {\n\tunsigned int nr_allocated;\t/* Number of frames we have space for */\n\tunsigned int nr_frames;\t/* Number of frames stored in ptrs array */\n\tbool got_ref;\t\t/* Did we pin pages by getting page ref? */\n\tbool is_pfns;\t\t/* Does array contain pages or pfns? */\n\tvoid *ptrs[0];\t\t/* Array of pinned pfns / pages. Use\n\t\t\t\t * pfns_vector_pages() or pfns_vector_pfns()\n\t\t\t\t * for access */\n};\n\nstruct frame_vector *frame_vector_create(unsigned int nr_frames);\nvoid frame_vector_destroy(struct frame_vector *vec);\nint get_vaddr_frames(unsigned long start, unsigned int nr_pfns,\n\t\t     bool write, bool force, struct frame_vector *vec);\nvoid put_vaddr_frames(struct frame_vector *vec);\nint frame_vector_to_pages(struct frame_vector *vec);\nvoid frame_vector_to_pfns(struct frame_vector *vec);\n\nstatic inline unsigned int frame_vector_count(struct frame_vector *vec)\n{\n\treturn vec->nr_frames;\n}\n\nstatic inline struct page **frame_vector_pages(struct frame_vector *vec)\n{\n\tif (vec->is_pfns) {\n\t\tint err = frame_vector_to_pages(vec);\n\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t}\n\treturn (struct page **)(vec->ptrs);\n}\n\nstatic inline unsigned long *frame_vector_pfns(struct frame_vector *vec)\n{\n\tif (!vec->is_pfns)\n\t\tframe_vector_to_pfns(vec);\n\treturn (unsigned long *)(vec->ptrs);\n}\n\nstruct kvec;\nint get_kernel_pages(const struct kvec *iov, int nr_pages, int write,\n\t\t\tstruct page **pages);\nint get_kernel_page(unsigned long start, int write, struct page **pages);\nstruct page *get_dump_page(unsigned long addr);\n\nextern int try_to_release_page(struct page * page, gfp_t gfp_mask);\nextern void do_invalidatepage(struct page *page, unsigned int offset,\n\t\t\t      unsigned int length);\n\nint __set_page_dirty_nobuffers(struct page *page);\nint __set_page_dirty_no_writeback(struct page *page);\nint redirty_page_for_writepage(struct writeback_control *wbc,\n\t\t\t\tstruct page *page);\nvoid account_page_dirtied(struct page *page, struct address_space *mapping);\nvoid account_page_cleaned(struct page *page, struct address_space *mapping,\n\t\t\t  struct bdi_writeback *wb);\nint set_page_dirty(struct page *page);\nint set_page_dirty_lock(struct page *page);\nvoid cancel_dirty_page(struct page *page);\nint clear_page_dirty_for_io(struct page *page);\n\nint get_cmdline(struct task_struct *task, char *buffer, int buflen);\n\n/* Is the vma a continuation of the stack vma above it? */\nstatic inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)\n{\n\treturn vma && (vma->vm_end == addr) && (vma->vm_flags & VM_GROWSDOWN);\n}\n\nstatic inline bool vma_is_anonymous(struct vm_area_struct *vma)\n{\n\treturn !vma->vm_ops;\n}\n\nstatic inline int stack_guard_page_start(struct vm_area_struct *vma,\n\t\t\t\t\t     unsigned long addr)\n{\n\treturn (vma->vm_flags & VM_GROWSDOWN) &&\n\t\t(vma->vm_start == addr) &&\n\t\t!vma_growsdown(vma->vm_prev, addr);\n}\n\n/* Is the vma a continuation of the stack vma below it? */\nstatic inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)\n{\n\treturn vma && (vma->vm_start == addr) && (vma->vm_flags & VM_GROWSUP);\n}\n\nstatic inline int stack_guard_page_end(struct vm_area_struct *vma,\n\t\t\t\t\t   unsigned long addr)\n{\n\treturn (vma->vm_flags & VM_GROWSUP) &&\n\t\t(vma->vm_end == addr) &&\n\t\t!vma_growsup(vma->vm_next, addr);\n}\n\nint vma_is_stack_for_task(struct vm_area_struct *vma, struct task_struct *t);\n\nextern unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len,\n\t\tbool need_rmap_locks);\nextern unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,\n\t\t\t      unsigned long end, pgprot_t newprot,\n\t\t\t      int dirty_accountable, int prot_numa);\nextern int mprotect_fixup(struct vm_area_struct *vma,\n\t\t\t  struct vm_area_struct **pprev, unsigned long start,\n\t\t\t  unsigned long end, unsigned long newflags);\n\n/*\n * doesn't attempt to fault and will return short.\n */\nint __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages);\n/*\n * per-process(per-mm_struct) statistics.\n */\nstatic inline unsigned long get_mm_counter(struct mm_struct *mm, int member)\n{\n\tlong val = atomic_long_read(&mm->rss_stat.count[member]);\n\n#ifdef SPLIT_RSS_COUNTING\n\t/*\n\t * counter is updated in asynchronous manner and may go to minus.\n\t * But it's never be expected number for users.\n\t */\n\tif (val < 0)\n\t\tval = 0;\n#endif\n\treturn (unsigned long)val;\n}\n\nstatic inline void add_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tatomic_long_add(value, &mm->rss_stat.count[member]);\n}\n\nstatic inline void inc_mm_counter(struct mm_struct *mm, int member)\n{\n\tatomic_long_inc(&mm->rss_stat.count[member]);\n}\n\nstatic inline void dec_mm_counter(struct mm_struct *mm, int member)\n{\n\tatomic_long_dec(&mm->rss_stat.count[member]);\n}\n\n/* Optimized variant when page is already known not to be PageAnon */\nstatic inline int mm_counter_file(struct page *page)\n{\n\tif (PageSwapBacked(page))\n\t\treturn MM_SHMEMPAGES;\n\treturn MM_FILEPAGES;\n}\n\nstatic inline int mm_counter(struct page *page)\n{\n\tif (PageAnon(page))\n\t\treturn MM_ANONPAGES;\n\treturn mm_counter_file(page);\n}\n\nstatic inline unsigned long get_mm_rss(struct mm_struct *mm)\n{\n\treturn get_mm_counter(mm, MM_FILEPAGES) +\n\t\tget_mm_counter(mm, MM_ANONPAGES) +\n\t\tget_mm_counter(mm, MM_SHMEMPAGES);\n}\n\nstatic inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_rss, get_mm_rss(mm));\n}\n\nstatic inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_vm, mm->total_vm);\n}\n\nstatic inline void update_hiwater_rss(struct mm_struct *mm)\n{\n\tunsigned long _rss = get_mm_rss(mm);\n\n\tif ((mm)->hiwater_rss < _rss)\n\t\t(mm)->hiwater_rss = _rss;\n}\n\nstatic inline void update_hiwater_vm(struct mm_struct *mm)\n{\n\tif (mm->hiwater_vm < mm->total_vm)\n\t\tmm->hiwater_vm = mm->total_vm;\n}\n\nstatic inline void reset_mm_hiwater_rss(struct mm_struct *mm)\n{\n\tmm->hiwater_rss = get_mm_rss(mm);\n}\n\nstatic inline void setmax_mm_hiwater_rss(unsigned long *maxrss,\n\t\t\t\t\t struct mm_struct *mm)\n{\n\tunsigned long hiwater_rss = get_mm_hiwater_rss(mm);\n\n\tif (*maxrss < hiwater_rss)\n\t\t*maxrss = hiwater_rss;\n}\n\n#if defined(SPLIT_RSS_COUNTING)\nvoid sync_mm_rss(struct mm_struct *mm);\n#else\nstatic inline void sync_mm_rss(struct mm_struct *mm)\n{\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTE_DEVMAP\nstatic inline int pte_devmap(pte_t pte)\n{\n\treturn 0;\n}\n#endif\n\nint vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot);\n\nextern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t       spinlock_t **ptl);\nstatic inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t    spinlock_t **ptl)\n{\n\tpte_t *ptep;\n\t__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));\n\treturn ptep;\n}\n\n#ifdef __PAGETABLE_PUD_FOLDED\nstatic inline int __pud_alloc(struct mm_struct *mm, pgd_t *pgd,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n#else\nint __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);\n#endif\n\n#if defined(__PAGETABLE_PMD_FOLDED) || !defined(CONFIG_MMU)\nstatic inline int __pmd_alloc(struct mm_struct *mm, pud_t *pud,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n\nstatic inline void mm_nr_pmds_init(struct mm_struct *mm) {}\n\nstatic inline unsigned long mm_nr_pmds(struct mm_struct *mm)\n{\n\treturn 0;\n}\n\nstatic inline void mm_inc_nr_pmds(struct mm_struct *mm) {}\nstatic inline void mm_dec_nr_pmds(struct mm_struct *mm) {}\n\n#else\nint __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);\n\nstatic inline void mm_nr_pmds_init(struct mm_struct *mm)\n{\n\tatomic_long_set(&mm->nr_pmds, 0);\n}\n\nstatic inline unsigned long mm_nr_pmds(struct mm_struct *mm)\n{\n\treturn atomic_long_read(&mm->nr_pmds);\n}\n\nstatic inline void mm_inc_nr_pmds(struct mm_struct *mm)\n{\n\tatomic_long_inc(&mm->nr_pmds);\n}\n\nstatic inline void mm_dec_nr_pmds(struct mm_struct *mm)\n{\n\tatomic_long_dec(&mm->nr_pmds);\n}\n#endif\n\nint __pte_alloc(struct mm_struct *mm, pmd_t *pmd, unsigned long address);\nint __pte_alloc_kernel(pmd_t *pmd, unsigned long address);\n\n/*\n * The following ifdef needed to get the 4level-fixup.h header to work.\n * Remove it when 4level-fixup.h has been removed.\n */\n#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)\nstatic inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)\n{\n\treturn (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?\n\t\tNULL: pud_offset(pgd, address);\n}\n\nstatic inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)\n{\n\treturn (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?\n\t\tNULL: pmd_offset(pud, address);\n}\n#endif /* CONFIG_MMU && !__ARCH_HAS_4LEVEL_HACK */\n\n#if USE_SPLIT_PTE_PTLOCKS\n#if ALLOC_SPLIT_PTLOCKS\nvoid __init ptlock_cache_init(void);\nextern bool ptlock_alloc(struct page *page);\nextern void ptlock_free(struct page *page);\n\nstatic inline spinlock_t *ptlock_ptr(struct page *page)\n{\n\treturn page->ptl;\n}\n#else /* ALLOC_SPLIT_PTLOCKS */\nstatic inline void ptlock_cache_init(void)\n{\n}\n\nstatic inline bool ptlock_alloc(struct page *page)\n{\n\treturn true;\n}\n\nstatic inline void ptlock_free(struct page *page)\n{\n}\n\nstatic inline spinlock_t *ptlock_ptr(struct page *page)\n{\n\treturn &page->ptl;\n}\n#endif /* ALLOC_SPLIT_PTLOCKS */\n\nstatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn ptlock_ptr(pmd_page(*pmd));\n}\n\nstatic inline bool ptlock_init(struct page *page)\n{\n\t/*\n\t * prep_new_page() initialize page->private (and therefore page->ptl)\n\t * with 0. Make sure nobody took it in use in between.\n\t *\n\t * It can happen if arch try to use slab for page table allocation:\n\t * slab code uses page->slab_cache, which share storage with page->ptl.\n\t */\n\tVM_BUG_ON_PAGE(*(unsigned long *)&page->ptl, page);\n\tif (!ptlock_alloc(page))\n\t\treturn false;\n\tspin_lock_init(ptlock_ptr(page));\n\treturn true;\n}\n\n/* Reset page->mapping so free_pages_check won't complain. */\nstatic inline void pte_lock_deinit(struct page *page)\n{\n\tpage->mapping = NULL;\n\tptlock_free(page);\n}\n\n#else\t/* !USE_SPLIT_PTE_PTLOCKS */\n/*\n * We use mm->page_table_lock to guard all pagetable pages of the mm.\n */\nstatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn &mm->page_table_lock;\n}\nstatic inline void ptlock_cache_init(void) {}\nstatic inline bool ptlock_init(struct page *page) { return true; }\nstatic inline void pte_lock_deinit(struct page *page) {}\n#endif /* USE_SPLIT_PTE_PTLOCKS */\n\nstatic inline void pgtable_init(void)\n{\n\tptlock_cache_init();\n\tpgtable_cache_init();\n}\n\nstatic inline bool pgtable_page_ctor(struct page *page)\n{\n\tif (!ptlock_init(page))\n\t\treturn false;\n\tinc_zone_page_state(page, NR_PAGETABLE);\n\treturn true;\n}\n\nstatic inline void pgtable_page_dtor(struct page *page)\n{\n\tpte_lock_deinit(page);\n\tdec_zone_page_state(page, NR_PAGETABLE);\n}\n\n#define pte_offset_map_lock(mm, pmd, address, ptlp)\t\\\n({\t\t\t\t\t\t\t\\\n\tspinlock_t *__ptl = pte_lockptr(mm, pmd);\t\\\n\tpte_t *__pte = pte_offset_map(pmd, address);\t\\\n\t*(ptlp) = __ptl;\t\t\t\t\\\n\tspin_lock(__ptl);\t\t\t\t\\\n\t__pte;\t\t\t\t\t\t\\\n})\n\n#define pte_unmap_unlock(pte, ptl)\tdo {\t\t\\\n\tspin_unlock(ptl);\t\t\t\t\\\n\tpte_unmap(pte);\t\t\t\t\t\\\n} while (0)\n\n#define pte_alloc(mm, pmd, address)\t\t\t\\\n\t(unlikely(pmd_none(*(pmd))) && __pte_alloc(mm, pmd, address))\n\n#define pte_alloc_map(mm, pmd, address)\t\t\t\\\n\t(pte_alloc(mm, pmd, address) ? NULL : pte_offset_map(pmd, address))\n\n#define pte_alloc_map_lock(mm, pmd, address, ptlp)\t\\\n\t(pte_alloc(mm, pmd, address) ?\t\t\t\\\n\t\t NULL : pte_offset_map_lock(mm, pmd, address, ptlp))\n\n#define pte_alloc_kernel(pmd, address)\t\t\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc_kernel(pmd, address))? \\\n\t\tNULL: pte_offset_kernel(pmd, address))\n\n#if USE_SPLIT_PMD_PTLOCKS\n\nstatic struct page *pmd_to_page(pmd_t *pmd)\n{\n\tunsigned long mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);\n\treturn virt_to_page((void *)((unsigned long) pmd & mask));\n}\n\nstatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn ptlock_ptr(pmd_to_page(pmd));\n}\n\nstatic inline bool pgtable_pmd_page_ctor(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tpage->pmd_huge_pte = NULL;\n#endif\n\treturn ptlock_init(page);\n}\n\nstatic inline void pgtable_pmd_page_dtor(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tVM_BUG_ON_PAGE(page->pmd_huge_pte, page);\n#endif\n\tptlock_free(page);\n}\n\n#define pmd_huge_pte(mm, pmd) (pmd_to_page(pmd)->pmd_huge_pte)\n\n#else\n\nstatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn &mm->page_table_lock;\n}\n\nstatic inline bool pgtable_pmd_page_ctor(struct page *page) { return true; }\nstatic inline void pgtable_pmd_page_dtor(struct page *page) {}\n\n#define pmd_huge_pte(mm, pmd) ((mm)->pmd_huge_pte)\n\n#endif\n\nstatic inline spinlock_t *pmd_lock(struct mm_struct *mm, pmd_t *pmd)\n{\n\tspinlock_t *ptl = pmd_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\treturn ptl;\n}\n\nextern void free_area_init(unsigned long * zones_size);\nextern void free_area_init_node(int nid, unsigned long * zones_size,\n\t\tunsigned long zone_start_pfn, unsigned long *zholes_size);\nextern void free_initmem(void);\n\n/*\n * Free reserved pages within range [PAGE_ALIGN(start), end & PAGE_MASK)\n * into the buddy system. The freed pages will be poisoned with pattern\n * \"poison\" if it's within range [0, UCHAR_MAX].\n * Return pages freed into the buddy system.\n */\nextern unsigned long free_reserved_area(void *start, void *end,\n\t\t\t\t\tint poison, char *s);\n\n#ifdef\tCONFIG_HIGHMEM\n/*\n * Free a highmem page into the buddy system, adjusting totalhigh_pages\n * and totalram_pages.\n */\nextern void free_highmem_page(struct page *page);\n#endif\n\nextern void adjust_managed_page_count(struct page *page, long count);\nextern void mem_init_print_info(const char *str);\n\nextern void reserve_bootmem_region(phys_addr_t start, phys_addr_t end);\n\n/* Free the reserved page into the buddy system, so it gets managed. */\nstatic inline void __free_reserved_page(struct page *page)\n{\n\tClearPageReserved(page);\n\tinit_page_count(page);\n\t__free_page(page);\n}\n\nstatic inline void free_reserved_page(struct page *page)\n{\n\t__free_reserved_page(page);\n\tadjust_managed_page_count(page, 1);\n}\n\nstatic inline void mark_page_reserved(struct page *page)\n{\n\tSetPageReserved(page);\n\tadjust_managed_page_count(page, -1);\n}\n\n/*\n * Default method to free all the __init memory into the buddy system.\n * The freed pages will be poisoned with pattern \"poison\" if it's within\n * range [0, UCHAR_MAX].\n * Return pages freed into the buddy system.\n */\nstatic inline unsigned long free_initmem_default(int poison)\n{\n\textern char __init_begin[], __init_end[];\n\n\treturn free_reserved_area(&__init_begin, &__init_end,\n\t\t\t\t  poison, \"unused kernel\");\n}\n\nstatic inline unsigned long get_num_physpages(void)\n{\n\tint nid;\n\tunsigned long phys_pages = 0;\n\n\tfor_each_online_node(nid)\n\t\tphys_pages += node_present_pages(nid);\n\n\treturn phys_pages;\n}\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n/*\n * With CONFIG_HAVE_MEMBLOCK_NODE_MAP set, an architecture may initialise its\n * zones, allocate the backing mem_map and account for memory holes in a more\n * architecture independent manner. This is a substitute for creating the\n * zone_sizes[] and zholes_size[] arrays and passing them to\n * free_area_init_node()\n *\n * An architecture is expected to register range of page frames backed by\n * physical memory with memblock_add[_node]() before calling\n * free_area_init_nodes() passing in the PFN each zone ends at. At a basic\n * usage, an architecture is expected to do something like\n *\n * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,\n * \t\t\t\t\t\t\t max_highmem_pfn};\n * for_each_valid_physical_page_range()\n * \tmemblock_add_node(base, size, nid)\n * free_area_init_nodes(max_zone_pfns);\n *\n * free_bootmem_with_active_regions() calls free_bootmem_node() for each\n * registered physical page range.  Similarly\n * sparse_memory_present_with_active_regions() calls memory_present() for\n * each range when SPARSEMEM is enabled.\n *\n * See mm/page_alloc.c for more information on each function exposed by\n * CONFIG_HAVE_MEMBLOCK_NODE_MAP.\n */\nextern void free_area_init_nodes(unsigned long *max_zone_pfn);\nunsigned long node_map_pfn_alignment(void);\nunsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern unsigned long absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern void get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn);\nextern unsigned long find_min_pfn_with_active_regions(void);\nextern void free_bootmem_with_active_regions(int nid,\n\t\t\t\t\t\tunsigned long max_low_pfn);\nextern void sparse_memory_present_with_active_regions(int nid);\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\n#if !defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) && \\\n    !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID)\nstatic inline int __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state)\n{\n\treturn 0;\n}\n#else\n/* please see mm/page_alloc.c */\nextern int __meminit early_pfn_to_nid(unsigned long pfn);\n/* there is a per-arch backend function. */\nextern int __meminit __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state);\n#endif\n\nextern void set_dma_reserve(unsigned long new_dma_reserve);\nextern void memmap_init_zone(unsigned long, int, unsigned long,\n\t\t\t\tunsigned long, enum memmap_context);\nextern void setup_per_zone_wmarks(void);\nextern int __meminit init_per_zone_wmark_min(void);\nextern void mem_init(void);\nextern void __init mmap_init(void);\nextern void show_mem(unsigned int flags);\nextern long si_mem_available(void);\nextern void si_meminfo(struct sysinfo * val);\nextern void si_meminfo_node(struct sysinfo *val, int nid);\n#ifdef __HAVE_ARCH_RESERVED_KERNEL_PAGES\nextern unsigned long arch_reserved_kernel_pages(void);\n#endif\n\nextern __printf(2, 3)\nvoid warn_alloc(gfp_t gfp_mask, const char *fmt, ...);\n\nextern void setup_per_cpu_pageset(void);\n\nextern void zone_pcp_update(struct zone *zone);\nextern void zone_pcp_reset(struct zone *zone);\n\n/* page_alloc.c */\nextern int min_free_kbytes;\nextern int watermark_scale_factor;\n\n/* nommu.c */\nextern atomic_long_t mmap_pages_allocated;\nextern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);\n\n/* interval_tree.c */\nvoid vma_interval_tree_insert(struct vm_area_struct *node,\n\t\t\t      struct rb_root *root);\nvoid vma_interval_tree_insert_after(struct vm_area_struct *node,\n\t\t\t\t    struct vm_area_struct *prev,\n\t\t\t\t    struct rb_root *root);\nvoid vma_interval_tree_remove(struct vm_area_struct *node,\n\t\t\t      struct rb_root *root);\nstruct vm_area_struct *vma_interval_tree_iter_first(struct rb_root *root,\n\t\t\t\tunsigned long start, unsigned long last);\nstruct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,\n\t\t\t\tunsigned long start, unsigned long last);\n\n#define vma_interval_tree_foreach(vma, root, start, last)\t\t\\\n\tfor (vma = vma_interval_tree_iter_first(root, start, last);\t\\\n\t     vma; vma = vma_interval_tree_iter_next(vma, start, last))\n\nvoid anon_vma_interval_tree_insert(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root *root);\nvoid anon_vma_interval_tree_remove(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root *root);\nstruct anon_vma_chain *anon_vma_interval_tree_iter_first(\n\tstruct rb_root *root, unsigned long start, unsigned long last);\nstruct anon_vma_chain *anon_vma_interval_tree_iter_next(\n\tstruct anon_vma_chain *node, unsigned long start, unsigned long last);\n#ifdef CONFIG_DEBUG_VM_RB\nvoid anon_vma_interval_tree_verify(struct anon_vma_chain *node);\n#endif\n\n#define anon_vma_interval_tree_foreach(avc, root, start, last)\t\t \\\n\tfor (avc = anon_vma_interval_tree_iter_first(root, start, last); \\\n\t     avc; avc = anon_vma_interval_tree_iter_next(avc, start, last))\n\n/* mmap.c */\nextern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);\nextern int __vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,\n\tstruct vm_area_struct *expand);\nstatic inline int vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert)\n{\n\treturn __vma_adjust(vma, start, end, pgoff, insert, NULL);\n}\nextern struct vm_area_struct *vma_merge(struct mm_struct *,\n\tstruct vm_area_struct *prev, unsigned long addr, unsigned long end,\n\tunsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,\n\tstruct mempolicy *, struct vm_userfaultfd_ctx);\nextern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);\nextern int split_vma(struct mm_struct *,\n\tstruct vm_area_struct *, unsigned long addr, int new_below);\nextern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);\nextern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,\n\tstruct rb_node **, struct rb_node *);\nextern void unlink_file_vma(struct vm_area_struct *);\nextern struct vm_area_struct *copy_vma(struct vm_area_struct **,\n\tunsigned long addr, unsigned long len, pgoff_t pgoff,\n\tbool *need_rmap_locks);\nextern void exit_mmap(struct mm_struct *);\n\nstatic inline int check_data_rlimit(unsigned long rlim,\n\t\t\t\t    unsigned long new,\n\t\t\t\t    unsigned long start,\n\t\t\t\t    unsigned long end_data,\n\t\t\t\t    unsigned long start_data)\n{\n\tif (rlim < RLIM_INFINITY) {\n\t\tif (((new - start) + (end_data - start_data)) > rlim)\n\t\t\treturn -ENOSPC;\n\t}\n\n\treturn 0;\n}\n\nextern int mm_take_all_locks(struct mm_struct *mm);\nextern void mm_drop_all_locks(struct mm_struct *mm);\n\nextern void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);\nextern struct file *get_mm_exe_file(struct mm_struct *mm);\nextern struct file *get_task_exe_file(struct task_struct *task);\n\nextern bool may_expand_vm(struct mm_struct *, vm_flags_t, unsigned long npages);\nextern void vm_stat_account(struct mm_struct *, vm_flags_t, long npages);\n\nextern bool vma_is_special_mapping(const struct vm_area_struct *vma,\n\t\t\t\t   const struct vm_special_mapping *sm);\nextern struct vm_area_struct *_install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags,\n\t\t\t\t   const struct vm_special_mapping *spec);\n/* This is an obsolete alternative to _install_special_mapping. */\nextern int install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags, struct page **pages);\n\nextern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);\n\nextern unsigned long mmap_region(struct file *file, unsigned long addr,\n\tunsigned long len, vm_flags_t vm_flags, unsigned long pgoff);\nextern unsigned long do_mmap(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot, unsigned long flags,\n\tvm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate);\nextern int do_munmap(struct mm_struct *, unsigned long, size_t);\n\nstatic inline unsigned long\ndo_mmap_pgoff(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot, unsigned long flags,\n\tunsigned long pgoff, unsigned long *populate)\n{\n\treturn do_mmap(file, addr, len, prot, flags, 0, pgoff, populate);\n}\n\n#ifdef CONFIG_MMU\nextern int __mm_populate(unsigned long addr, unsigned long len,\n\t\t\t int ignore_errors);\nstatic inline void mm_populate(unsigned long addr, unsigned long len)\n{\n\t/* Ignore errors */\n\t(void) __mm_populate(addr, len, 1);\n}\n#else\nstatic inline void mm_populate(unsigned long addr, unsigned long len) {}\n#endif\n\n/* These take the mm semaphore themselves */\nextern int __must_check vm_brk(unsigned long, unsigned long);\nextern int vm_munmap(unsigned long, size_t);\nextern unsigned long __must_check vm_mmap(struct file *, unsigned long,\n        unsigned long, unsigned long,\n        unsigned long, unsigned long);\n\nstruct vm_unmapped_area_info {\n#define VM_UNMAPPED_AREA_TOPDOWN 1\n\tunsigned long flags;\n\tunsigned long length;\n\tunsigned long low_limit;\n\tunsigned long high_limit;\n\tunsigned long align_mask;\n\tunsigned long align_offset;\n};\n\nextern unsigned long unmapped_area(struct vm_unmapped_area_info *info);\nextern unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info);\n\n/*\n * Search for an unmapped address range.\n *\n * We are looking for a range that:\n * - does not intersect with any VMA;\n * - is contained within the [low_limit, high_limit) interval;\n * - is at least the desired size.\n * - satisfies (begin_addr & align_mask) == (align_offset & align_mask)\n */\nstatic inline unsigned long\nvm_unmapped_area(struct vm_unmapped_area_info *info)\n{\n\tif (info->flags & VM_UNMAPPED_AREA_TOPDOWN)\n\t\treturn unmapped_area_topdown(info);\n\telse\n\t\treturn unmapped_area(info);\n}\n\n/* truncate.c */\nextern void truncate_inode_pages(struct address_space *, loff_t);\nextern void truncate_inode_pages_range(struct address_space *,\n\t\t\t\t       loff_t lstart, loff_t lend);\nextern void truncate_inode_pages_final(struct address_space *);\n\n/* generic vm_area_ops exported for stackable file systems */\nextern int filemap_fault(struct vm_area_struct *, struct vm_fault *);\nextern void filemap_map_pages(struct fault_env *fe,\n\t\tpgoff_t start_pgoff, pgoff_t end_pgoff);\nextern int filemap_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n/* mm/page-writeback.c */\nint write_one_page(struct page *page, int wait);\nvoid task_dirty_inc(struct task_struct *tsk);\n\n/* readahead.c */\n#define VM_MAX_READAHEAD\t128\t/* kbytes */\n#define VM_MIN_READAHEAD\t16\t/* kbytes (includes current page) */\n\nint force_page_cache_readahead(struct address_space *mapping, struct file *filp,\n\t\t\tpgoff_t offset, unsigned long nr_to_read);\n\nvoid page_cache_sync_readahead(struct address_space *mapping,\n\t\t\t       struct file_ra_state *ra,\n\t\t\t       struct file *filp,\n\t\t\t       pgoff_t offset,\n\t\t\t       unsigned long size);\n\nvoid page_cache_async_readahead(struct address_space *mapping,\n\t\t\t\tstruct file_ra_state *ra,\n\t\t\t\tstruct file *filp,\n\t\t\t\tstruct page *pg,\n\t\t\t\tpgoff_t offset,\n\t\t\t\tunsigned long size);\n\n/* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */\nextern int expand_stack(struct vm_area_struct *vma, unsigned long address);\n\n/* CONFIG_STACK_GROWSUP still needs to to grow downwards at some places */\nextern int expand_downwards(struct vm_area_struct *vma,\n\t\tunsigned long address);\n#if VM_GROWSUP\nextern int expand_upwards(struct vm_area_struct *vma, unsigned long address);\n#else\n  #define expand_upwards(vma, address) (0)\n#endif\n\n/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */\nextern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);\nextern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,\n\t\t\t\t\t     struct vm_area_struct **pprev);\n\n/* Look up the first VMA which intersects the interval start_addr..end_addr-1,\n   NULL if none.  Assume start_addr < end_addr. */\nstatic inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)\n{\n\tstruct vm_area_struct * vma = find_vma(mm,start_addr);\n\n\tif (vma && end_addr <= vma->vm_start)\n\t\tvma = NULL;\n\treturn vma;\n}\n\nstatic inline unsigned long vma_pages(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\n}\n\n/* Look up the first VMA which exactly match the interval vm_start ... vm_end */\nstatic inline struct vm_area_struct *find_exact_vma(struct mm_struct *mm,\n\t\t\t\tunsigned long vm_start, unsigned long vm_end)\n{\n\tstruct vm_area_struct *vma = find_vma(mm, vm_start);\n\n\tif (vma && (vma->vm_start != vm_start || vma->vm_end != vm_end))\n\t\tvma = NULL;\n\n\treturn vma;\n}\n\n#ifdef CONFIG_MMU\npgprot_t vm_get_page_prot(unsigned long vm_flags);\nvoid vma_set_page_prot(struct vm_area_struct *vma);\n#else\nstatic inline pgprot_t vm_get_page_prot(unsigned long vm_flags)\n{\n\treturn __pgprot(0);\n}\nstatic inline void vma_set_page_prot(struct vm_area_struct *vma)\n{\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n}\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\nunsigned long change_prot_numa(struct vm_area_struct *vma,\n\t\t\tunsigned long start, unsigned long end);\n#endif\n\nstruct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);\nint remap_pfn_range(struct vm_area_struct *, unsigned long addr,\n\t\t\tunsigned long pfn, unsigned long size, pgprot_t);\nint vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);\nint vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn);\nint vm_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn, pgprot_t pgprot);\nint vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tpfn_t pfn);\nint vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len);\n\n\nstruct page *follow_page_mask(struct vm_area_struct *vma,\n\t\t\t      unsigned long address, unsigned int foll_flags,\n\t\t\t      unsigned int *page_mask);\n\nstatic inline struct page *follow_page(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int foll_flags)\n{\n\tunsigned int unused_page_mask;\n\treturn follow_page_mask(vma, address, foll_flags, &unused_page_mask);\n}\n\n#define FOLL_WRITE\t0x01\t/* check pte is writable */\n#define FOLL_TOUCH\t0x02\t/* mark page accessed */\n#define FOLL_GET\t0x04\t/* do get_page on page */\n#define FOLL_DUMP\t0x08\t/* give error on hole if it would be zero */\n#define FOLL_FORCE\t0x10\t/* get_user_pages read/write w/o permission */\n#define FOLL_NOWAIT\t0x20\t/* if a disk transfer is needed, start the IO\n\t\t\t\t * and return without waiting upon it */\n#define FOLL_POPULATE\t0x40\t/* fault in page */\n#define FOLL_SPLIT\t0x80\t/* don't return transhuge pages, split them */\n#define FOLL_HWPOISON\t0x100\t/* check page is hwpoisoned */\n#define FOLL_NUMA\t0x200\t/* force NUMA hinting page fault */\n#define FOLL_MIGRATION\t0x400\t/* wait for page to replace migration entry */\n#define FOLL_TRIED\t0x800\t/* a retry, previous pass started an IO */\n#define FOLL_MLOCK\t0x1000\t/* lock present pages */\n#define FOLL_REMOTE\t0x2000\t/* we are working on non-current tsk/mm */\n#define FOLL_COW\t0x4000\t/* internal GUP flag */\n\ntypedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,\n\t\t\tvoid *data);\nextern int apply_to_page_range(struct mm_struct *mm, unsigned long address,\n\t\t\t       unsigned long size, pte_fn_t fn, void *data);\n\n\n#ifdef CONFIG_PAGE_POISONING\nextern bool page_poisoning_enabled(void);\nextern void kernel_poison_pages(struct page *page, int numpages, int enable);\nextern bool page_is_poisoned(struct page *page);\n#else\nstatic inline bool page_poisoning_enabled(void) { return false; }\nstatic inline void kernel_poison_pages(struct page *page, int numpages,\n\t\t\t\t\tint enable) { }\nstatic inline bool page_is_poisoned(struct page *page) { return false; }\n#endif\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nextern bool _debug_pagealloc_enabled;\nextern void __kernel_map_pages(struct page *page, int numpages, int enable);\n\nstatic inline bool debug_pagealloc_enabled(void)\n{\n\treturn _debug_pagealloc_enabled;\n}\n\nstatic inline void\nkernel_map_pages(struct page *page, int numpages, int enable)\n{\n\tif (!debug_pagealloc_enabled())\n\t\treturn;\n\n\t__kernel_map_pages(page, numpages, enable);\n}\n#ifdef CONFIG_HIBERNATION\nextern bool kernel_page_present(struct page *page);\n#endif\t/* CONFIG_HIBERNATION */\n#else\t/* CONFIG_DEBUG_PAGEALLOC */\nstatic inline void\nkernel_map_pages(struct page *page, int numpages, int enable) {}\n#ifdef CONFIG_HIBERNATION\nstatic inline bool kernel_page_present(struct page *page) { return true; }\n#endif\t/* CONFIG_HIBERNATION */\nstatic inline bool debug_pagealloc_enabled(void)\n{\n\treturn false;\n}\n#endif\t/* CONFIG_DEBUG_PAGEALLOC */\n\n#ifdef __HAVE_ARCH_GATE_AREA\nextern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);\nextern int in_gate_area_no_mm(unsigned long addr);\nextern int in_gate_area(struct mm_struct *mm, unsigned long addr);\n#else\nstatic inline struct vm_area_struct *get_gate_vma(struct mm_struct *mm)\n{\n\treturn NULL;\n}\nstatic inline int in_gate_area_no_mm(unsigned long addr) { return 0; }\nstatic inline int in_gate_area(struct mm_struct *mm, unsigned long addr)\n{\n\treturn 0;\n}\n#endif\t/* __HAVE_ARCH_GATE_AREA */\n\nextern bool process_shares_mm(struct task_struct *p, struct mm_struct *mm);\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_drop_caches;\nint drop_caches_sysctl_handler(struct ctl_table *, int,\n\t\t\t\t\tvoid __user *, size_t *, loff_t *);\n#endif\n\nvoid drop_slab(void);\nvoid drop_slab_node(int nid);\n\n#ifndef CONFIG_MMU\n#define randomize_va_space 0\n#else\nextern int randomize_va_space;\n#endif\n\nconst char * arch_vma_name(struct vm_area_struct *vma);\nvoid print_vma_addr(char *prefix, unsigned long rip);\n\nvoid sparse_mem_maps_populate_node(struct page **map_map,\n\t\t\t\t   unsigned long pnum_begin,\n\t\t\t\t   unsigned long pnum_end,\n\t\t\t\t   unsigned long map_count,\n\t\t\t\t   int nodeid);\n\nstruct page *sparse_mem_map_populate(unsigned long pnum, int nid);\npgd_t *vmemmap_pgd_populate(unsigned long addr, int node);\npud_t *vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node);\npmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);\npte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node);\nvoid *vmemmap_alloc_block(unsigned long size, int node);\nstruct vmem_altmap;\nvoid *__vmemmap_alloc_block_buf(unsigned long size, int node,\n\t\tstruct vmem_altmap *altmap);\nstatic inline void *vmemmap_alloc_block_buf(unsigned long size, int node)\n{\n\treturn __vmemmap_alloc_block_buf(size, node, NULL);\n}\n\nvoid vmemmap_verify(pte_t *, int, unsigned long, unsigned long);\nint vmemmap_populate_basepages(unsigned long start, unsigned long end,\n\t\t\t       int node);\nint vmemmap_populate(unsigned long start, unsigned long end, int node);\nvoid vmemmap_populate_print_last(void);\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid vmemmap_free(unsigned long start, unsigned long end);\n#endif\nvoid register_page_bootmem_memmap(unsigned long section_nr, struct page *map,\n\t\t\t\t  unsigned long size);\n\nenum mf_flags {\n\tMF_COUNT_INCREASED = 1 << 0,\n\tMF_ACTION_REQUIRED = 1 << 1,\n\tMF_MUST_KILL = 1 << 2,\n\tMF_SOFT_OFFLINE = 1 << 3,\n};\nextern int memory_failure(unsigned long pfn, int trapno, int flags);\nextern void memory_failure_queue(unsigned long pfn, int trapno, int flags);\nextern int unpoison_memory(unsigned long pfn);\nextern int get_hwpoison_page(struct page *page);\n#define put_hwpoison_page(page)\tput_page(page)\nextern int sysctl_memory_failure_early_kill;\nextern int sysctl_memory_failure_recovery;\nextern void shake_page(struct page *p, int access);\nextern atomic_long_t num_poisoned_pages;\nextern int soft_offline_page(struct page *page, int flags);\n\n\n/*\n * Error handlers for various types of pages.\n */\nenum mf_result {\n\tMF_IGNORED,\t/* Error: cannot be handled */\n\tMF_FAILED,\t/* Error: handling failed */\n\tMF_DELAYED,\t/* Will be handled later */\n\tMF_RECOVERED,\t/* Successfully recovered */\n};\n\nenum mf_action_page_type {\n\tMF_MSG_KERNEL,\n\tMF_MSG_KERNEL_HIGH_ORDER,\n\tMF_MSG_SLAB,\n\tMF_MSG_DIFFERENT_COMPOUND,\n\tMF_MSG_POISONED_HUGE,\n\tMF_MSG_HUGE,\n\tMF_MSG_FREE_HUGE,\n\tMF_MSG_UNMAP_FAILED,\n\tMF_MSG_DIRTY_SWAPCACHE,\n\tMF_MSG_CLEAN_SWAPCACHE,\n\tMF_MSG_DIRTY_MLOCKED_LRU,\n\tMF_MSG_CLEAN_MLOCKED_LRU,\n\tMF_MSG_DIRTY_UNEVICTABLE_LRU,\n\tMF_MSG_CLEAN_UNEVICTABLE_LRU,\n\tMF_MSG_DIRTY_LRU,\n\tMF_MSG_CLEAN_LRU,\n\tMF_MSG_TRUNCATED_LRU,\n\tMF_MSG_BUDDY,\n\tMF_MSG_BUDDY_2ND,\n\tMF_MSG_UNKNOWN,\n};\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)\nextern void clear_huge_page(struct page *page,\n\t\t\t    unsigned long addr,\n\t\t\t    unsigned int pages_per_huge_page);\nextern void copy_user_huge_page(struct page *dst, struct page *src,\n\t\t\t\tunsigned long addr, struct vm_area_struct *vma,\n\t\t\t\tunsigned int pages_per_huge_page);\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */\n\nextern struct page_ext_operations debug_guardpage_ops;\nextern struct page_ext_operations page_poisoning_ops;\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nextern unsigned int _debug_guardpage_minorder;\nextern bool _debug_guardpage_enabled;\n\nstatic inline unsigned int debug_guardpage_minorder(void)\n{\n\treturn _debug_guardpage_minorder;\n}\n\nstatic inline bool debug_guardpage_enabled(void)\n{\n\treturn _debug_guardpage_enabled;\n}\n\nstatic inline bool page_is_guard(struct page *page)\n{\n\tstruct page_ext *page_ext;\n\n\tif (!debug_guardpage_enabled())\n\t\treturn false;\n\n\tpage_ext = lookup_page_ext(page);\n\tif (unlikely(!page_ext))\n\t\treturn false;\n\n\treturn test_bit(PAGE_EXT_DEBUG_GUARD, &page_ext->flags);\n}\n#else\nstatic inline unsigned int debug_guardpage_minorder(void) { return 0; }\nstatic inline bool debug_guardpage_enabled(void) { return false; }\nstatic inline bool page_is_guard(struct page *page) { return false; }\n#endif /* CONFIG_DEBUG_PAGEALLOC */\n\n#if MAX_NUMNODES > 1\nvoid __init setup_nr_node_ids(void);\n#else\nstatic inline void setup_nr_node_ids(void) {}\n#endif\n\n#endif /* __KERNEL__ */\n#endif /* _LINUX_MM_H */\n", "#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/spinlock.h>\n\n#include <linux/mm.h>\n#include <linux/memremap.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n\n#include <linux/sched.h>\n#include <linux/rwsem.h>\n#include <linux/hugetlb.h>\n\n#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/tlbflush.h>\n\n#include \"internal.h\"\n\nstatic struct page *no_page_table(struct vm_area_struct *vma,\n\t\tunsigned int flags)\n{\n\t/*\n\t * When core dumping an enormous anonymous area that nobody\n\t * has touched so far, we don't want to allocate unnecessary pages or\n\t * page tables.  Return error instead of NULL to skip handle_mm_fault,\n\t * then get_dump_page() will return NULL to leave a hole in the dump.\n\t * But we can only make this optimization where a hole would surely\n\t * be zero-filled if handle_mm_fault() actually did handle it.\n\t */\n\tif ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))\n\t\treturn ERR_PTR(-EFAULT);\n\treturn NULL;\n}\n\nstatic int follow_pfn_pte(struct vm_area_struct *vma, unsigned long address,\n\t\tpte_t *pte, unsigned int flags)\n{\n\t/* No page to get reference */\n\tif (flags & FOLL_GET)\n\t\treturn -EFAULT;\n\n\tif (flags & FOLL_TOUCH) {\n\t\tpte_t entry = *pte;\n\n\t\tif (flags & FOLL_WRITE)\n\t\t\tentry = pte_mkdirty(entry);\n\t\tentry = pte_mkyoung(entry);\n\n\t\tif (!pte_same(*pte, entry)) {\n\t\t\tset_pte_at(vma->vm_mm, address, pte, entry);\n\t\t\tupdate_mmu_cache(vma, address, pte);\n\t\t}\n\t}\n\n\t/* Proper page table entry exists, but no corresponding struct page */\n\treturn -EEXIST;\n}\n\n/*\n * FOLL_FORCE can write to even unwritable pte's, but only\n * after we've gone through a COW cycle and they are dirty.\n */\nstatic inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n{\n\treturn pte_write(pte) ||\n\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n}\n\nstatic struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap = NULL;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\tif ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\treturn NULL;\n\t}\n\n\tpage = vm_normal_page(vma, address, pte);\n\tif (!page && pte_devmap(pte) && (flags & FOLL_GET)) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET case since\n\t\t * they are only valid while holding the pgmap reference.\n\t\t */\n\t\tpgmap = get_dev_pagemap(pte_pfn(pte), NULL);\n\t\tif (pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (flags & FOLL_SPLIT && PageTransCompound(page)) {\n\t\tint ret;\n\t\tget_page(page);\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tlock_page(page);\n\t\tret = split_huge_page(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tgoto retry;\n\t}\n\n\tif (flags & FOLL_GET) {\n\t\tget_page(page);\n\n\t\t/* drop the pgmap reference now that we hold the page */\n\t\tif (pgmap) {\n\t\t\tput_dev_pagemap(pgmap);\n\t\t\tpgmap = NULL;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/* Do not mlock pte-mapped THP */\n\t\tif (PageTransCompound(page))\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * The preliminary mapping check is mainly to avoid the\n\t\t * pointless overhead of lock_page on the ZERO_PAGE\n\t\t * which might bounce very badly if there is contention.\n\t\t *\n\t\t * If the page is already locked, we don't need to\n\t\t * handle it now - vmscan will handle it later if and\n\t\t * when it attempts to reclaim the page.\n\t\t */\n\t\tif (page->mapping && trylock_page(page)) {\n\t\t\tlru_add_drain();  /* push cached pages to LRU */\n\t\t\t/*\n\t\t\t * Because we lock page here, and migration is\n\t\t\t * blocked by the pte's page reference, and we\n\t\t\t * know the page is still mapped, we don't even\n\t\t\t * need to check for file-cache page truncation.\n\t\t\t */\n\t\t\tmlock_vma_page(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}\n\n/**\n * follow_page_mask - look up a page descriptor from a user-virtual address\n * @vma: vm_area_struct mapping @address\n * @address: virtual address to look up\n * @flags: flags modifying lookup behaviour\n * @page_mask: on output, *page_mask is set according to the size of the page\n *\n * @flags can have FOLL_ flags set, defined in <linux/mm.h>\n *\n * Returns the mapped (struct page *), %NULL if no mapping exists, or\n * an error pointer if there is a mapping to something not represented\n * by a page descriptor (see also vm_normal_page()).\n */\nstruct page *follow_page_mask(struct vm_area_struct *vma,\n\t\t\t      unsigned long address, unsigned int flags,\n\t\t\t      unsigned int *page_mask)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\t*page_mask = 0;\n\n\tpage = follow_huge_addr(mm, address, flags & FOLL_WRITE);\n\tif (!IS_ERR(page)) {\n\t\tBUG_ON(flags & FOLL_GET);\n\t\treturn page;\n\t}\n\n\tpgd = pgd_offset(mm, address);\n\tif (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))\n\t\treturn no_page_table(vma, flags);\n\n\tpud = pud_offset(pgd, address);\n\tif (pud_none(*pud))\n\t\treturn no_page_table(vma, flags);\n\tif (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {\n\t\tpage = follow_huge_pud(mm, address, pud, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (unlikely(pud_bad(*pud)))\n\t\treturn no_page_table(vma, flags);\n\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none(*pmd))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {\n\t\tpage = follow_huge_pmd(mm, address, pmd, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif ((flags & FOLL_NUMA) && pmd_protnone(*pmd))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_devmap(*pmd)) {\n\t\tptl = pmd_lock(mm, pmd);\n\t\tpage = follow_devmap_pmd(vma, address, pmd, flags);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (likely(!pmd_trans_huge(*pmd)))\n\t\treturn follow_page_pte(vma, address, pmd, flags);\n\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn follow_page_pte(vma, address, pmd, flags);\n\t}\n\tif (flags & FOLL_SPLIT) {\n\t\tint ret;\n\t\tpage = pmd_page(*pmd);\n\t\tif (is_huge_zero_page(page)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tret = 0;\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tif (pmd_trans_unstable(pmd))\n\t\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tget_page(page);\n\t\t\tspin_unlock(ptl);\n\t\t\tlock_page(page);\n\t\t\tret = split_huge_page(page);\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tif (pmd_none(*pmd))\n\t\t\t\treturn no_page_table(vma, flags);\n\t\t}\n\n\t\treturn ret ? ERR_PTR(ret) :\n\t\t\tfollow_page_pte(vma, address, pmd, flags);\n\t}\n\n\tpage = follow_trans_huge_pmd(vma, address, pmd, flags);\n\tspin_unlock(ptl);\n\t*page_mask = HPAGE_PMD_NR - 1;\n\treturn page;\n}\n\nstatic int get_gate_page(struct mm_struct *mm, unsigned long address,\n\t\tunsigned int gup_flags, struct vm_area_struct **vma,\n\t\tstruct page **page)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tint ret = -EFAULT;\n\n\t/* user gate pages are read-only */\n\tif (gup_flags & FOLL_WRITE)\n\t\treturn -EFAULT;\n\tif (address > TASK_SIZE)\n\t\tpgd = pgd_offset_k(address);\n\telse\n\t\tpgd = pgd_offset_gate(mm, address);\n\tBUG_ON(pgd_none(*pgd));\n\tpud = pud_offset(pgd, address);\n\tBUG_ON(pud_none(*pud));\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none(*pmd))\n\t\treturn -EFAULT;\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tpte = pte_offset_map(pmd, address);\n\tif (pte_none(*pte))\n\t\tgoto unmap;\n\t*vma = get_gate_vma(mm);\n\tif (!page)\n\t\tgoto out;\n\t*page = vm_normal_page(*vma, address, *pte);\n\tif (!*page) {\n\t\tif ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))\n\t\t\tgoto unmap;\n\t\t*page = pte_page(*pte);\n\t}\n\tget_page(*page);\nout:\n\tret = 0;\nunmap:\n\tpte_unmap(pte);\n\treturn ret;\n}\n\n/*\n * mmap_sem must be held on entry.  If @nonblocking != NULL and\n * *@flags does not include FOLL_NOWAIT, the mmap_sem may be released.\n * If it is, *@nonblocking will be set to 0 and -EBUSY returned.\n */\nstatic int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, int *nonblocking)\n{\n\tunsigned int fault_flags = 0;\n\tint ret;\n\n\t/* mlock all present pages, but do not fault in new pages */\n\tif ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)\n\t\treturn -ENOENT;\n\t/* For mm_populate(), just skip the stack guard page. */\n\tif ((*flags & FOLL_POPULATE) &&\n\t\t\t(stack_guard_page_start(vma, address) ||\n\t\t\t stack_guard_page_end(vma, address + PAGE_SIZE)))\n\t\treturn -ENOENT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (nonblocking)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\tVM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tif (ret & VM_FAULT_ERROR) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\treturn -ENOMEM;\n\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\t\treturn *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;\n\t\tif (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\t\treturn -EFAULT;\n\t\tBUG();\n\t}\n\n\tif (tsk) {\n\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (nonblocking)\n\t\t\t*nonblocking = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t        *flags |= FOLL_COW;\n\treturn 0;\n}\n\nstatic int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)\n{\n\tvm_flags_t vm_flags = vma->vm_flags;\n\tint write = (gup_flags & FOLL_WRITE);\n\tint foreign = (gup_flags & FOLL_REMOTE);\n\n\tif (vm_flags & (VM_IO | VM_PFNMAP))\n\t\treturn -EFAULT;\n\n\tif (write) {\n\t\tif (!(vm_flags & VM_WRITE)) {\n\t\t\tif (!(gup_flags & FOLL_FORCE))\n\t\t\t\treturn -EFAULT;\n\t\t\t/*\n\t\t\t * We used to let the write,force case do COW in a\n\t\t\t * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could\n\t\t\t * set a breakpoint in a read-only mapping of an\n\t\t\t * executable, without corrupting the file (yet only\n\t\t\t * when that file had been opened for writing!).\n\t\t\t * Anon pages in shared mappings are surprising: now\n\t\t\t * just reject it.\n\t\t\t */\n\t\t\tif (!is_cow_mapping(vm_flags))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t} else if (!(vm_flags & VM_READ)) {\n\t\tif (!(gup_flags & FOLL_FORCE))\n\t\t\treturn -EFAULT;\n\t\t/*\n\t\t * Is there actually any vma we can reach here which does not\n\t\t * have VM_MAYREAD set?\n\t\t */\n\t\tif (!(vm_flags & VM_MAYREAD))\n\t\t\treturn -EFAULT;\n\t}\n\t/*\n\t * gups are always data accesses, not instruction\n\t * fetches, so execute=false here\n\t */\n\tif (!arch_vma_access_permitted(vma, write, false, foreign))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/**\n * __get_user_pages() - pin user pages in memory\n * @tsk:\ttask_struct of target task\n * @mm:\t\tmm_struct of target mm\n * @start:\tstarting user address\n * @nr_pages:\tnumber of pages from start to pin\n * @gup_flags:\tflags modifying pin behaviour\n * @pages:\tarray that receives pointers to the pages pinned.\n *\t\tShould be at least nr_pages long. Or NULL, if caller\n *\t\tonly intends to ensure the pages are faulted in.\n * @vmas:\tarray of pointers to vmas corresponding to each page.\n *\t\tOr NULL if the caller does not require them.\n * @nonblocking: whether waiting for disk IO or mmap_sem contention\n *\n * Returns number of pages pinned. This may be fewer than the number\n * requested. If nr_pages is 0 or negative, returns 0. If no pages\n * were pinned, returns -errno. Each page returned must be released\n * with a put_page() call when it is finished with. vmas will only\n * remain valid while mmap_sem is held.\n *\n * Must be called with mmap_sem held.  It may be released.  See below.\n *\n * __get_user_pages walks a process's page tables and takes a reference to\n * each struct page that each user address corresponds to at a given\n * instant. That is, it takes the page that would be accessed if a user\n * thread accesses the given user virtual address at that instant.\n *\n * This does not guarantee that the page exists in the user mappings when\n * __get_user_pages returns, and there may even be a completely different\n * page there in some cases (eg. if mmapped pagecache has been invalidated\n * and subsequently re faulted). However it does guarantee that the page\n * won't be freed completely. And mostly callers simply care that the page\n * contains data that was valid *at some point in time*. Typically, an IO\n * or similar operation cannot guarantee anything stronger anyway because\n * locks can't be held over the syscall boundary.\n *\n * If @gup_flags & FOLL_WRITE == 0, the page must not be written to. If\n * the page is written to, set_page_dirty (or set_page_dirty_lock, as\n * appropriate) must be called after the page is finished with, and\n * before put_page is called.\n *\n * If @nonblocking != NULL, __get_user_pages will not wait for disk IO\n * or mmap_sem contention, and if waiting is needed to pin all pages,\n * *@nonblocking will be set to 0.  Further, if @gup_flags does not\n * include FOLL_NOWAIT, the mmap_sem will be released via up_read() in\n * this case.\n *\n * A caller using such a combination of @nonblocking and @gup_flags\n * must therefore hold the mmap_sem for reading only, and recognize\n * when it's been released.  Otherwise, it must be held for either\n * reading or writing and will not be released.\n *\n * In most cases, get_user_pages or get_user_pages_fast should be used\n * instead of __get_user_pages. __get_user_pages should be used only if\n * you need some special @gup_flags.\n */\nlong __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *nonblocking)\n{\n\tlong i = 0;\n\tunsigned int page_mask;\n\tstruct vm_area_struct *vma = NULL;\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & FOLL_GET));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tint ret;\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn i ? : ret;\n\t\t\t\tpage_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags))\n\t\t\t\treturn i ? : -EFAULT;\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tgup_flags);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (unlikely(fatal_signal_pending(current)))\n\t\t\treturn i ? i : -ERESTARTSYS;\n\t\tcond_resched();\n\t\tpage = follow_page_mask(vma, start, foll_flags, &page_mask);\n\t\tif (!page) {\n\t\t\tint ret;\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\tnonblocking);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\treturn i ? i : ret;\n\t\t\tcase -EBUSY:\n\t\t\t\treturn i;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\treturn i ? i : PTR_ERR(page);\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tpage_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tpage_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\n\treturn i;\n}\nEXPORT_SYMBOL(__get_user_pages);\n\nbool vma_permits_fault(struct vm_area_struct *vma, unsigned int fault_flags)\n{\n\tbool write   = !!(fault_flags & FAULT_FLAG_WRITE);\n\tbool foreign = !!(fault_flags & FAULT_FLAG_REMOTE);\n\tvm_flags_t vm_flags = write ? VM_WRITE : VM_READ;\n\n\tif (!(vm_flags & vma->vm_flags))\n\t\treturn false;\n\n\t/*\n\t * The architecture might have a hardware protection\n\t * mechanism other than read/write that can deny access.\n\t *\n\t * gup always represents data access, not instruction\n\t * fetches, so execute=false here:\n\t */\n\tif (!arch_vma_access_permitted(vma, write, false, foreign))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * fixup_user_fault() - manually resolve a user page fault\n * @tsk:\tthe task_struct to use for page fault accounting, or\n *\t\tNULL if faults are not to be recorded.\n * @mm:\t\tmm_struct of target mm\n * @address:\tuser address\n * @fault_flags:flags to pass down to handle_mm_fault()\n * @unlocked:\tdid we unlock the mmap_sem while retrying, maybe NULL if caller\n *\t\tdoes not allow retry\n *\n * This is meant to be called in the specific scenario where for locking reasons\n * we try to access user memory in atomic context (within a pagefault_disable()\n * section), this returns -EFAULT, and we want to resolve the user fault before\n * trying again.\n *\n * Typically this is meant to be used by the futex code.\n *\n * The main difference with get_user_pages() is that this function will\n * unconditionally call handle_mm_fault() which will in turn perform all the\n * necessary SW fixup of the dirty and young bits in the PTE, while\n * get_user_pages() only guarantees to update these in the struct page.\n *\n * This is important for some architectures where those bits also gate the\n * access permission to the page because they are maintained in software.  On\n * such architectures, gup() will not be enough to make a subsequent access\n * succeed.\n *\n * This function will not return with an unlocked mmap_sem. So it has not the\n * same semantics wrt the @mm->mmap_sem as does filemap_fault().\n */\nint fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,\n\t\t     unsigned long address, unsigned int fault_flags,\n\t\t     bool *unlocked)\n{\n\tstruct vm_area_struct *vma;\n\tint ret, major = 0;\n\n\tif (unlocked)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\nretry:\n\tvma = find_extend_vma(mm, address);\n\tif (!vma || address < vma->vm_start)\n\t\treturn -EFAULT;\n\n\tif (!vma_permits_fault(vma, fault_flags))\n\t\treturn -EFAULT;\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tmajor |= ret & VM_FAULT_MAJOR;\n\tif (ret & VM_FAULT_ERROR) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\treturn -ENOMEM;\n\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\t\treturn -EHWPOISON;\n\t\tif (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\t\treturn -EFAULT;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tdown_read(&mm->mmap_sem);\n\t\tif (!(fault_flags & FAULT_FLAG_TRIED)) {\n\t\t\t*unlocked = true;\n\t\t\tfault_flags &= ~FAULT_FLAG_ALLOW_RETRY;\n\t\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (tsk) {\n\t\tif (major)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(fixup_user_fault);\n\nstatic __always_inline long __get_user_pages_locked(struct task_struct *tsk,\n\t\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\t\tunsigned long start,\n\t\t\t\t\t\tunsigned long nr_pages,\n\t\t\t\t\t\tint write, int force,\n\t\t\t\t\t\tstruct page **pages,\n\t\t\t\t\t\tstruct vm_area_struct **vmas,\n\t\t\t\t\t\tint *locked, bool notify_drop,\n\t\t\t\t\t\tunsigned int flags)\n{\n\tlong ret, pages_done;\n\tbool lock_dropped;\n\n\tif (locked) {\n\t\t/* if VM_FAULT_RETRY can be returned, vmas become invalid */\n\t\tBUG_ON(vmas);\n\t\t/* check caller initialized locked */\n\t\tBUG_ON(*locked != 1);\n\t}\n\n\tif (pages)\n\t\tflags |= FOLL_GET;\n\tif (write)\n\t\tflags |= FOLL_WRITE;\n\tif (force)\n\t\tflags |= FOLL_FORCE;\n\n\tpages_done = 0;\n\tlock_dropped = false;\n\tfor (;;) {\n\t\tret = __get_user_pages(tsk, mm, start, nr_pages, flags, pages,\n\t\t\t\t       vmas, locked);\n\t\tif (!locked)\n\t\t\t/* VM_FAULT_RETRY couldn't trigger, bypass */\n\t\t\treturn ret;\n\n\t\t/* VM_FAULT_RETRY cannot return errors */\n\t\tif (!*locked) {\n\t\t\tBUG_ON(ret < 0);\n\t\t\tBUG_ON(ret >= nr_pages);\n\t\t}\n\n\t\tif (!pages)\n\t\t\t/* If it's a prefault don't insist harder */\n\t\t\treturn ret;\n\n\t\tif (ret > 0) {\n\t\t\tnr_pages -= ret;\n\t\t\tpages_done += ret;\n\t\t\tif (!nr_pages)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (*locked) {\n\t\t\t/* VM_FAULT_RETRY didn't trigger */\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\t/* VM_FAULT_RETRY triggered, so seek to the faulting offset */\n\t\tpages += ret;\n\t\tstart += ret << PAGE_SHIFT;\n\n\t\t/*\n\t\t * Repeat on the address that fired VM_FAULT_RETRY\n\t\t * without FAULT_FLAG_ALLOW_RETRY but with\n\t\t * FAULT_FLAG_TRIED.\n\t\t */\n\t\t*locked = 1;\n\t\tlock_dropped = true;\n\t\tdown_read(&mm->mmap_sem);\n\t\tret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,\n\t\t\t\t       pages, NULL, NULL);\n\t\tif (ret != 1) {\n\t\t\tBUG_ON(ret > 1);\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\tnr_pages--;\n\t\tpages_done++;\n\t\tif (!nr_pages)\n\t\t\tbreak;\n\t\tpages++;\n\t\tstart += PAGE_SIZE;\n\t}\n\tif (notify_drop && lock_dropped && *locked) {\n\t\t/*\n\t\t * We must let the caller know we temporarily dropped the lock\n\t\t * and so the critical section protected by it was lost.\n\t\t */\n\t\tup_read(&mm->mmap_sem);\n\t\t*locked = 0;\n\t}\n\treturn pages_done;\n}\n\n/*\n * We can leverage the VM_FAULT_RETRY functionality in the page fault\n * paths better by using either get_user_pages_locked() or\n * get_user_pages_unlocked().\n *\n * get_user_pages_locked() is suitable to replace the form:\n *\n *      down_read(&mm->mmap_sem);\n *      do_something()\n *      get_user_pages(tsk, mm, ..., pages, NULL);\n *      up_read(&mm->mmap_sem);\n *\n *  to:\n *\n *      int locked = 1;\n *      down_read(&mm->mmap_sem);\n *      do_something()\n *      get_user_pages_locked(tsk, mm, ..., pages, &locked);\n *      if (locked)\n *          up_read(&mm->mmap_sem);\n */\nlong get_user_pages_locked(unsigned long start, unsigned long nr_pages,\n\t\t\t   int write, int force, struct page **pages,\n\t\t\t   int *locked)\n{\n\treturn __get_user_pages_locked(current, current->mm, start, nr_pages,\n\t\t\t\t       write, force, pages, NULL, locked, true,\n\t\t\t\t       FOLL_TOUCH);\n}\nEXPORT_SYMBOL(get_user_pages_locked);\n\n/*\n * Same as get_user_pages_unlocked(...., FOLL_TOUCH) but it allows to\n * pass additional gup_flags as last parameter (like FOLL_HWPOISON).\n *\n * NOTE: here FOLL_TOUCH is not set implicitly and must be set by the\n * caller if required (just like with __get_user_pages). \"FOLL_GET\",\n * \"FOLL_WRITE\" and \"FOLL_FORCE\" are set implicitly as needed\n * according to the parameters \"pages\", \"write\", \"force\"\n * respectively.\n */\n__always_inline long __get_user_pages_unlocked(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\t\t\t       unsigned long start, unsigned long nr_pages,\n\t\t\t\t\t       int write, int force, struct page **pages,\n\t\t\t\t\t       unsigned int gup_flags)\n{\n\tlong ret;\n\tint locked = 1;\n\tdown_read(&mm->mmap_sem);\n\tret = __get_user_pages_locked(tsk, mm, start, nr_pages, write, force,\n\t\t\t\t      pages, NULL, &locked, false, gup_flags);\n\tif (locked)\n\t\tup_read(&mm->mmap_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(__get_user_pages_unlocked);\n\n/*\n * get_user_pages_unlocked() is suitable to replace the form:\n *\n *      down_read(&mm->mmap_sem);\n *      get_user_pages(tsk, mm, ..., pages, NULL);\n *      up_read(&mm->mmap_sem);\n *\n *  with:\n *\n *      get_user_pages_unlocked(tsk, mm, ..., pages);\n *\n * It is functionally equivalent to get_user_pages_fast so\n * get_user_pages_fast should be used instead, if the two parameters\n * \"tsk\" and \"mm\" are respectively equal to current and current->mm,\n * or if \"force\" shall be set to 1 (get_user_pages_fast misses the\n * \"force\" parameter).\n */\nlong get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t\t     int write, int force, struct page **pages)\n{\n\treturn __get_user_pages_unlocked(current, current->mm, start, nr_pages,\n\t\t\t\t\t write, force, pages, FOLL_TOUCH);\n}\nEXPORT_SYMBOL(get_user_pages_unlocked);\n\n/*\n * get_user_pages_remote() - pin user pages in memory\n * @tsk:\tthe task_struct to use for page fault accounting, or\n *\t\tNULL if faults are not to be recorded.\n * @mm:\t\tmm_struct of target mm\n * @start:\tstarting user address\n * @nr_pages:\tnumber of pages from start to pin\n * @write:\twhether pages will be written to by the caller\n * @force:\twhether to force access even when user mapping is currently\n *\t\tprotected (but never forces write access to shared mapping).\n * @pages:\tarray that receives pointers to the pages pinned.\n *\t\tShould be at least nr_pages long. Or NULL, if caller\n *\t\tonly intends to ensure the pages are faulted in.\n * @vmas:\tarray of pointers to vmas corresponding to each page.\n *\t\tOr NULL if the caller does not require them.\n *\n * Returns number of pages pinned. This may be fewer than the number\n * requested. If nr_pages is 0 or negative, returns 0. If no pages\n * were pinned, returns -errno. Each page returned must be released\n * with a put_page() call when it is finished with. vmas will only\n * remain valid while mmap_sem is held.\n *\n * Must be called with mmap_sem held for read or write.\n *\n * get_user_pages walks a process's page tables and takes a reference to\n * each struct page that each user address corresponds to at a given\n * instant. That is, it takes the page that would be accessed if a user\n * thread accesses the given user virtual address at that instant.\n *\n * This does not guarantee that the page exists in the user mappings when\n * get_user_pages returns, and there may even be a completely different\n * page there in some cases (eg. if mmapped pagecache has been invalidated\n * and subsequently re faulted). However it does guarantee that the page\n * won't be freed completely. And mostly callers simply care that the page\n * contains data that was valid *at some point in time*. Typically, an IO\n * or similar operation cannot guarantee anything stronger anyway because\n * locks can't be held over the syscall boundary.\n *\n * If write=0, the page must not be written to. If the page is written to,\n * set_page_dirty (or set_page_dirty_lock, as appropriate) must be called\n * after the page is finished with, and before put_page is called.\n *\n * get_user_pages is typically used for fewer-copy IO operations, to get a\n * handle on the memory by some means other than accesses via the user virtual\n * addresses. The pages may be submitted for DMA to devices or accessed via\n * their kernel linear mapping (via the kmap APIs). Care should be taken to\n * use the correct cache flushing APIs.\n *\n * See also get_user_pages_fast, for performance critical applications.\n *\n * get_user_pages should be phased out in favor of\n * get_user_pages_locked|unlocked or get_user_pages_fast. Nothing\n * should use get_user_pages because it cannot pass\n * FAULT_FLAG_ALLOW_RETRY to handle_mm_fault.\n */\nlong get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tint write, int force, struct page **pages,\n\t\tstruct vm_area_struct **vmas)\n{\n\treturn __get_user_pages_locked(tsk, mm, start, nr_pages, write, force,\n\t\t\t\t       pages, vmas, NULL, false,\n\t\t\t\t       FOLL_TOUCH | FOLL_REMOTE);\n}\nEXPORT_SYMBOL(get_user_pages_remote);\n\n/*\n * This is the same as get_user_pages_remote(), just with a\n * less-flexible calling convention where we assume that the task\n * and mm being operated on are the current task's.  We also\n * obviously don't pass FOLL_REMOTE in here.\n */\nlong get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\tint write, int force, struct page **pages,\n\t\tstruct vm_area_struct **vmas)\n{\n\treturn __get_user_pages_locked(current, current->mm, start, nr_pages,\n\t\t\t\t       write, force, pages, vmas, NULL, false,\n\t\t\t\t       FOLL_TOUCH);\n}\nEXPORT_SYMBOL(get_user_pages);\n\n/**\n * populate_vma_page_range() -  populate a range of pages in the vma.\n * @vma:   target vma\n * @start: start address\n * @end:   end address\n * @nonblocking:\n *\n * This takes care of mlocking the pages too if VM_LOCKED is set.\n *\n * return 0 on success, negative error code on error.\n *\n * vma->vm_mm->mmap_sem must be held.\n *\n * If @nonblocking is NULL, it may be held for read or write and will\n * be unperturbed.\n *\n * If @nonblocking is non-NULL, it must held for read only and may be\n * released.  If it's released, *@nonblocking will be set to 0.\n */\nlong populate_vma_page_range(struct vm_area_struct *vma,\n\t\tunsigned long start, unsigned long end, int *nonblocking)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long nr_pages = (end - start) / PAGE_SIZE;\n\tint gup_flags;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(end   & ~PAGE_MASK);\n\tVM_BUG_ON_VMA(start < vma->vm_start, vma);\n\tVM_BUG_ON_VMA(end   > vma->vm_end, vma);\n\tVM_BUG_ON_MM(!rwsem_is_locked(&mm->mmap_sem), mm);\n\n\tgup_flags = FOLL_TOUCH | FOLL_POPULATE | FOLL_MLOCK;\n\tif (vma->vm_flags & VM_LOCKONFAULT)\n\t\tgup_flags &= ~FOLL_POPULATE;\n\t/*\n\t * We want to touch writable mappings with a write fault in order\n\t * to break COW, except for shared mappings because these don't COW\n\t * and we would not want to dirty them for nothing.\n\t */\n\tif ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)\n\t\tgup_flags |= FOLL_WRITE;\n\n\t/*\n\t * We want mlock to succeed for regions that have any permissions\n\t * other than PROT_NONE.\n\t */\n\tif (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC))\n\t\tgup_flags |= FOLL_FORCE;\n\n\t/*\n\t * We made sure addr is within a VMA, so the following will\n\t * not result in a stack expansion that recurses back here.\n\t */\n\treturn __get_user_pages(current, mm, start, nr_pages, gup_flags,\n\t\t\t\tNULL, NULL, nonblocking);\n}\n\n/*\n * __mm_populate - populate and/or mlock pages within a range of address space.\n *\n * This is used to implement mlock() and the MAP_POPULATE / MAP_LOCKED mmap\n * flags. VMAs must be already marked with the desired vm_flags, and\n * mmap_sem must not be held.\n */\nint __mm_populate(unsigned long start, unsigned long len, int ignore_errors)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long end, nstart, nend;\n\tstruct vm_area_struct *vma = NULL;\n\tint locked = 0;\n\tlong ret = 0;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(len != PAGE_ALIGN(len));\n\tend = start + len;\n\n\tfor (nstart = start; nstart < end; nstart = nend) {\n\t\t/*\n\t\t * We want to fault in pages for [nstart; end) address range.\n\t\t * Find first corresponding VMA.\n\t\t */\n\t\tif (!locked) {\n\t\t\tlocked = 1;\n\t\t\tdown_read(&mm->mmap_sem);\n\t\t\tvma = find_vma(mm, nstart);\n\t\t} else if (nstart >= vma->vm_end)\n\t\t\tvma = vma->vm_next;\n\t\tif (!vma || vma->vm_start >= end)\n\t\t\tbreak;\n\t\t/*\n\t\t * Set [nstart; nend) to intersection of desired address\n\t\t * range with the first VMA. Also, skip undesirable VMA types.\n\t\t */\n\t\tnend = min(end, vma->vm_end);\n\t\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))\n\t\t\tcontinue;\n\t\tif (nstart < vma->vm_start)\n\t\t\tnstart = vma->vm_start;\n\t\t/*\n\t\t * Now fault in a range of pages. populate_vma_page_range()\n\t\t * double checks the vma flags, so that it won't mlock pages\n\t\t * if the vma was already munlocked.\n\t\t */\n\t\tret = populate_vma_page_range(vma, nstart, nend, &locked);\n\t\tif (ret < 0) {\n\t\t\tif (ignore_errors) {\n\t\t\t\tret = 0;\n\t\t\t\tcontinue;\t/* continue at next VMA */\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tnend = nstart + ret * PAGE_SIZE;\n\t\tret = 0;\n\t}\n\tif (locked)\n\t\tup_read(&mm->mmap_sem);\n\treturn ret;\t/* 0 or negative error code */\n}\n\n/**\n * get_dump_page() - pin user page in memory while writing it to core dump\n * @addr: user address\n *\n * Returns struct page pointer of user page pinned for dump,\n * to be freed afterwards by put_page().\n *\n * Returns NULL on any kind of failure - a hole must then be inserted into\n * the corefile, to preserve alignment with its headers; and also returns\n * NULL wherever the ZERO_PAGE, or an anonymous pte_none, has been found -\n * allowing a hole to be left in the corefile to save diskspace.\n *\n * Called without mmap_sem, but after all other threads have been killed.\n */\n#ifdef CONFIG_ELF_CORE\nstruct page *get_dump_page(unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\tstruct page *page;\n\n\tif (__get_user_pages(current, current->mm, addr, 1,\n\t\t\t     FOLL_FORCE | FOLL_DUMP | FOLL_GET, &page, &vma,\n\t\t\t     NULL) < 1)\n\t\treturn NULL;\n\tflush_cache_page(vma, addr, page_to_pfn(page));\n\treturn page;\n}\n#endif /* CONFIG_ELF_CORE */\n\n/*\n * Generic RCU Fast GUP\n *\n * get_user_pages_fast attempts to pin user pages by walking the page\n * tables directly and avoids taking locks. Thus the walker needs to be\n * protected from page table pages being freed from under it, and should\n * block any THP splits.\n *\n * One way to achieve this is to have the walker disable interrupts, and\n * rely on IPIs from the TLB flushing code blocking before the page table\n * pages are freed. This is unsuitable for architectures that do not need\n * to broadcast an IPI when invalidating TLBs.\n *\n * Another way to achieve this is to batch up page table containing pages\n * belonging to more than one mm_user, then rcu_sched a callback to free those\n * pages. Disabling interrupts will allow the fast_gup walker to both block\n * the rcu_sched callback, and an IPI that we broadcast for splitting THPs\n * (which is a relatively rare event). The code below adopts this strategy.\n *\n * Before activating this code, please be aware that the following assumptions\n * are currently made:\n *\n *  *) HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table is used to free\n *      pages containing page tables.\n *\n *  *) ptes can be read atomically by the architecture.\n *\n *  *) access_ok is sufficient to validate userspace address ranges.\n *\n * The last two assumptions can be relaxed by the addition of helper functions.\n *\n * This code is based heavily on the PowerPC implementation by Nick Piggin.\n */\n#ifdef CONFIG_HAVE_GENERIC_RCU_GUP\n\n#ifdef __HAVE_ARCH_PTE_SPECIAL\nstatic int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,\n\t\t\t int write, struct page **pages, int *nr)\n{\n\tpte_t *ptep, *ptem;\n\tint ret = 0;\n\n\tptem = ptep = pte_offset_map(&pmd, addr);\n\tdo {\n\t\t/*\n\t\t * In the line below we are assuming that the pte can be read\n\t\t * atomically. If this is not the case for your architecture,\n\t\t * please wrap this in a helper function!\n\t\t *\n\t\t * for an example see gup_get_pte in arch/x86/mm/gup.c\n\t\t */\n\t\tpte_t pte = READ_ONCE(*ptep);\n\t\tstruct page *head, *page;\n\n\t\t/*\n\t\t * Similar to the PMD case below, NUMA hinting must take slow\n\t\t * path using the pte_protnone check.\n\t\t */\n\t\tif (!pte_present(pte) || pte_special(pte) ||\n\t\t\tpte_protnone(pte) || (write && !pte_write(pte)))\n\t\t\tgoto pte_unmap;\n\n\t\tif (!arch_pte_access_permitted(pte, write))\n\t\t\tgoto pte_unmap;\n\n\t\tVM_BUG_ON(!pfn_valid(pte_pfn(pte)));\n\t\tpage = pte_page(pte);\n\t\thead = compound_head(page);\n\n\t\tif (!page_cache_get_speculative(head))\n\t\t\tgoto pte_unmap;\n\n\t\tif (unlikely(pte_val(pte) != pte_val(*ptep))) {\n\t\t\tput_page(head);\n\t\t\tgoto pte_unmap;\n\t\t}\n\n\t\tVM_BUG_ON_PAGE(compound_head(page) != head, page);\n\t\tpages[*nr] = page;\n\t\t(*nr)++;\n\n\t} while (ptep++, addr += PAGE_SIZE, addr != end);\n\n\tret = 1;\n\npte_unmap:\n\tpte_unmap(ptem);\n\treturn ret;\n}\n#else\n\n/*\n * If we can't determine whether or not a pte is special, then fail immediately\n * for ptes. Note, we can still pin HugeTLB and THP as these are guaranteed not\n * to be special.\n *\n * For a futex to be placed on a THP tail page, get_futex_key requires a\n * __get_user_pages_fast implementation that can pin pages. Thus it's still\n * useful to have gup_huge_pmd even if we can't operate on ptes.\n */\nstatic int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,\n\t\t\t int write, struct page **pages, int *nr)\n{\n\treturn 0;\n}\n#endif /* __HAVE_ARCH_PTE_SPECIAL */\n\nstatic int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,\n\t\tunsigned long end, int write, struct page **pages, int *nr)\n{\n\tstruct page *head, *page;\n\tint refs;\n\n\tif (write && !pmd_write(orig))\n\t\treturn 0;\n\n\trefs = 0;\n\thead = pmd_page(orig);\n\tpage = head + ((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\tdo {\n\t\tVM_BUG_ON_PAGE(compound_head(page) != head, page);\n\t\tpages[*nr] = page;\n\t\t(*nr)++;\n\t\tpage++;\n\t\trefs++;\n\t} while (addr += PAGE_SIZE, addr != end);\n\n\tif (!page_cache_add_speculative(head, refs)) {\n\t\t*nr -= refs;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {\n\t\t*nr -= refs;\n\t\twhile (refs--)\n\t\t\tput_page(head);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,\n\t\tunsigned long end, int write, struct page **pages, int *nr)\n{\n\tstruct page *head, *page;\n\tint refs;\n\n\tif (write && !pud_write(orig))\n\t\treturn 0;\n\n\trefs = 0;\n\thead = pud_page(orig);\n\tpage = head + ((addr & ~PUD_MASK) >> PAGE_SHIFT);\n\tdo {\n\t\tVM_BUG_ON_PAGE(compound_head(page) != head, page);\n\t\tpages[*nr] = page;\n\t\t(*nr)++;\n\t\tpage++;\n\t\trefs++;\n\t} while (addr += PAGE_SIZE, addr != end);\n\n\tif (!page_cache_add_speculative(head, refs)) {\n\t\t*nr -= refs;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(pud_val(orig) != pud_val(*pudp))) {\n\t\t*nr -= refs;\n\t\twhile (refs--)\n\t\t\tput_page(head);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr,\n\t\t\tunsigned long end, int write,\n\t\t\tstruct page **pages, int *nr)\n{\n\tint refs;\n\tstruct page *head, *page;\n\n\tif (write && !pgd_write(orig))\n\t\treturn 0;\n\n\trefs = 0;\n\thead = pgd_page(orig);\n\tpage = head + ((addr & ~PGDIR_MASK) >> PAGE_SHIFT);\n\tdo {\n\t\tVM_BUG_ON_PAGE(compound_head(page) != head, page);\n\t\tpages[*nr] = page;\n\t\t(*nr)++;\n\t\tpage++;\n\t\trefs++;\n\t} while (addr += PAGE_SIZE, addr != end);\n\n\tif (!page_cache_add_speculative(head, refs)) {\n\t\t*nr -= refs;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(pgd_val(orig) != pgd_val(*pgdp))) {\n\t\t*nr -= refs;\n\t\twhile (refs--)\n\t\t\tput_page(head);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,\n\t\tint write, struct page **pages, int *nr)\n{\n\tunsigned long next;\n\tpmd_t *pmdp;\n\n\tpmdp = pmd_offset(&pud, addr);\n\tdo {\n\t\tpmd_t pmd = READ_ONCE(*pmdp);\n\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (pmd_none(pmd))\n\t\t\treturn 0;\n\n\t\tif (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {\n\t\t\t/*\n\t\t\t * NUMA hinting faults need to be handled in the GUP\n\t\t\t * slowpath for accounting purposes and so that they\n\t\t\t * can be serialised against THP migration.\n\t\t\t */\n\t\t\tif (pmd_protnone(pmd))\n\t\t\t\treturn 0;\n\n\t\t\tif (!gup_huge_pmd(pmd, pmdp, addr, next, write,\n\t\t\t\tpages, nr))\n\t\t\t\treturn 0;\n\n\t\t} else if (unlikely(is_hugepd(__hugepd(pmd_val(pmd))))) {\n\t\t\t/*\n\t\t\t * architecture have different format for hugetlbfs\n\t\t\t * pmd format and THP pmd format\n\t\t\t */\n\t\t\tif (!gup_huge_pd(__hugepd(pmd_val(pmd)), addr,\n\t\t\t\t\t PMD_SHIFT, next, write, pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (!gup_pte_range(pmd, addr, next, write, pages, nr))\n\t\t\t\treturn 0;\n\t} while (pmdp++, addr = next, addr != end);\n\n\treturn 1;\n}\n\nstatic int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end,\n\t\t\t int write, struct page **pages, int *nr)\n{\n\tunsigned long next;\n\tpud_t *pudp;\n\n\tpudp = pud_offset(&pgd, addr);\n\tdo {\n\t\tpud_t pud = READ_ONCE(*pudp);\n\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_none(pud))\n\t\t\treturn 0;\n\t\tif (unlikely(pud_huge(pud))) {\n\t\t\tif (!gup_huge_pud(pud, pudp, addr, next, write,\n\t\t\t\t\t  pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (unlikely(is_hugepd(__hugepd(pud_val(pud))))) {\n\t\t\tif (!gup_huge_pd(__hugepd(pud_val(pud)), addr,\n\t\t\t\t\t PUD_SHIFT, next, write, pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (!gup_pmd_range(pud, addr, next, write, pages, nr))\n\t\t\treturn 0;\n\t} while (pudp++, addr = next, addr != end);\n\n\treturn 1;\n}\n\n/*\n * Like get_user_pages_fast() except it's IRQ-safe in that it won't fall back to\n * the regular GUP. It will only return non-negative values.\n */\nint __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long addr, len, end;\n\tunsigned long next, flags;\n\tpgd_t *pgdp;\n\tint nr = 0;\n\n\tstart &= PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,\n\t\t\t\t\tstart, len)))\n\t\treturn 0;\n\n\t/*\n\t * Disable interrupts.  We use the nested form as we can already have\n\t * interrupts disabled by get_futex_key.\n\t *\n\t * With interrupts disabled, we block page table pages from being\n\t * freed from under us. See mmu_gather_tlb in asm-generic/tlb.h\n\t * for more details.\n\t *\n\t * We do not adopt an rcu_read_lock(.) here as we also want to\n\t * block IPIs that come from THPs splitting.\n\t */\n\n\tlocal_irq_save(flags);\n\tpgdp = pgd_offset(mm, addr);\n\tdo {\n\t\tpgd_t pgd = READ_ONCE(*pgdp);\n\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none(pgd))\n\t\t\tbreak;\n\t\tif (unlikely(pgd_huge(pgd))) {\n\t\t\tif (!gup_huge_pgd(pgd, pgdp, addr, next, write,\n\t\t\t\t\t  pages, &nr))\n\t\t\t\tbreak;\n\t\t} else if (unlikely(is_hugepd(__hugepd(pgd_val(pgd))))) {\n\t\t\tif (!gup_huge_pd(__hugepd(pgd_val(pgd)), addr,\n\t\t\t\t\t PGDIR_SHIFT, next, write, pages, &nr))\n\t\t\t\tbreak;\n\t\t} else if (!gup_pud_range(pgd, addr, next, write, pages, &nr))\n\t\t\tbreak;\n\t} while (pgdp++, addr = next, addr != end);\n\tlocal_irq_restore(flags);\n\n\treturn nr;\n}\n\n/**\n * get_user_pages_fast() - pin user pages in memory\n * @start:\tstarting user address\n * @nr_pages:\tnumber of pages from start to pin\n * @write:\twhether pages will be written to\n * @pages:\tarray that receives pointers to the pages pinned.\n *\t\tShould be at least nr_pages long.\n *\n * Attempt to pin user pages in memory without taking mm->mmap_sem.\n * If not successful, it will fall back to taking the lock and\n * calling get_user_pages().\n *\n * Returns number of pages pinned. This may be fewer than the number\n * requested. If nr_pages is 0 or negative, returns 0. If no pages\n * were pinned, returns -errno.\n */\nint get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\tstruct page **pages)\n{\n\tint nr, ret;\n\n\tstart &= PAGE_MASK;\n\tnr = __get_user_pages_fast(start, nr_pages, write, pages);\n\tret = nr;\n\n\tif (nr < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr << PAGE_SHIFT;\n\t\tpages += nr;\n\n\t\tret = get_user_pages_unlocked(start, nr_pages - nr, write, 0, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr;\n\t\t\telse\n\t\t\t\tret += nr;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n#endif /* CONFIG_HAVE_GENERIC_RCU_GUP */\n"], "filenames": ["include/linux/mm.h", "mm/gup.c"], "buggy_code_start_loc": [2234, 60], "buggy_code_end_loc": [2234, 416], "fixing_code_start_loc": [2235, 61], "fixing_code_end_loc": [2236, 426], "type": "CWE-362", "message": "Race condition in mm/gup.c in the Linux kernel 2.x through 4.x before 4.8.3 allows local users to gain privileges by leveraging incorrect handling of a copy-on-write (COW) feature to write to a read-only memory mapping, as exploited in the wild in October 2016, aka \"Dirty COW.\"", "other": {"cve": {"id": "CVE-2016-5195", "sourceIdentifier": "chrome-cve-admin@google.com", "published": "2016-11-10T21:59:00.197", "lastModified": "2023-01-17T21:00:56.457", "vulnStatus": "Analyzed", "cisaExploitAdd": "2022-03-03", "cisaActionDue": "2022-03-24", "cisaRequiredAction": "Apply updates per vendor instructions.", "cisaVulnerabilityName": "Linux Kernel Race Condition Vulnerability", "descriptions": [{"lang": "en", "value": "Race condition in mm/gup.c in the Linux kernel 2.x through 4.x before 4.8.3 allows local users to gain privileges by leveraging incorrect handling of a copy-on-write (COW) feature to write to a read-only memory mapping, as exploited in the wild in October 2016, aka \"Dirty COW.\""}, {"lang": "es", "value": "La condici\u00f3n de carrera en mm / gup.c en el kernel de Linux 2.x a 4.x antes de 4.8.3 permite a los usuarios locales obtener privilegios aprovechando el manejo incorrecto de una funci\u00f3n copy-on-write (COW) para escribir en un read- only la cartograf\u00eda de la memoria, como explotados en la naturaleza en octubre de 2016, vulnerabilidad tambi\u00e9n conocida como \"Dirty COW\"."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:-:*:*:*", "matchCriteriaId": "CB66DB75-2B16-4EBF-9B93-CE49D8086E41"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:esm:*:*:*", "matchCriteriaId": "815D70A8-47D3-459C-A32C-9FEACA0659D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:esm:*:*:*", "matchCriteriaId": "7A5301BF-1402-4BE0-A0F8-69FBE79BC6D6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.10:*:*:*:*:*:*:*", "matchCriteriaId": "1AFB20FA-CB00-4729-AB3A-816454C6D096"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.22", "versionEndExcluding": "3.2.83", "matchCriteriaId": "6C039170-F1A6-48B9-8A16-AEBFD9924804"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.3", "versionEndExcluding": "3.4.113", "matchCriteriaId": "9A93F019-B0C0-4723-869E-C715F15E11C9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.5", "versionEndExcluding": "3.10.104", "matchCriteriaId": "B3B44636-A1EC-47C9-BE92-BC761CBB1B7B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.11", "versionEndExcluding": "3.12.66", "matchCriteriaId": "1E7C6515-C636-45C4-9766-BA26B89F1424"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.16.38", "matchCriteriaId": "8B1131A4-6EEF-4A1F-B706-1A61A471D632"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.44", "matchCriteriaId": "EBC11DAF-1AA0-4B60-A20C-6276BDBF3BC4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.35", "matchCriteriaId": "98821D4F-193B-44AB-8AA9-6F767F25F5E8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.26", "matchCriteriaId": "905253FB-85D4-4961-8C57-5A1B36741C18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.7.9", "matchCriteriaId": "72401FBF-CEB9-47FD-BAC0-EFC49B634BAA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.8", "versionEndExcluding": "4.8.3", "matchCriteriaId": "0F5B9915-B0CF-4BDA-A889-14834175FDE0"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:5:*:*:*:*:*:*:*", "matchCriteriaId": "AA9B3CC0-DF1C-4A86-B2A3-A9D428A5A6E6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:6.0:*:*:*:*:*:*:*", "matchCriteriaId": "2F6AB192-9D7D-4A9A-8995-E53A9DE9EAFC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_aus:6.2:*:*:*:*:*:*:*", "matchCriteriaId": "D68FB2BB-D103-4CA6-A51E-83DB349DDDE5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_aus:6.4:*:*:*:*:*:*:*", "matchCriteriaId": "512237D6-2B4B-4057-8F7C-F11639304028"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_aus:6.5:*:*:*:*:*:*:*", "matchCriteriaId": "79191794-6151-46E9-AAFD-3EC0C05B03B1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:6.6:*:*:*:*:*:*:*", "matchCriteriaId": "319EC0C6-94C5-494A-9C5D-DC5124DFC8E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:6.7:*:*:*:*:*:*:*", "matchCriteriaId": "967EC28A-607F-48F4-AD64-5E3041C768F0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:7.1:*:*:*:*:*:*:*", "matchCriteriaId": "A67A7B7A-998D-4B8C-8831-6E58406565FE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_long_life:5.6:*:*:*:*:*:*:*", "matchCriteriaId": "84A82ED6-976A-43F1-8820-F5DCB9DDABD9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_long_life:5.9:*:*:*:*:*:*:*", "matchCriteriaId": "5DBE05B8-17F9-4CC7-9579-1C1D57FEFD9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_tus:6.5:*:*:*:*:*:*:*", "matchCriteriaId": "7F4DE47C-0A23-4BCE-BCA1-425F7C1450E5"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "16F59A04-14CF-49E2-9973-645477EA09DA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}], "references": [{"url": "http://fortiguard.com/advisory/FG-IR-16-063", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=19be0eaffa3ac7d8eb6784ad9bdbc7d67ed8e619", "source": "chrome-cve-admin@google.com", "tags": ["Issue Tracking", "Patch", "Vendor Advisory"]}, {"url": "http://kb.juniper.net/InfoCenter/index?page=content&id=JSA10770", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://kb.juniper.net/InfoCenter/index?page=content&id=JSA10774", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://kb.juniper.net/InfoCenter/index?page=content&id=JSA10807", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00034.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00035.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00036.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00038.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00039.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00040.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00045.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00048.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00049.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00050.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00051.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00052.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00053.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00054.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00055.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00056.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00057.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00058.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00063.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00064.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00065.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00066.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00067.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-10/msg00072.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00033.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00100.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2020-04/msg00041.html", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://packetstormsecurity.com/files/139277/Kernel-Live-Patch-Security-Notice-LSN-0012-1.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://packetstormsecurity.com/files/139286/DirtyCow-Linux-Kernel-Race-Condition.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://packetstormsecurity.com/files/139287/DirtyCow-Local-Root-Proof-Of-Concept.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://packetstormsecurity.com/files/139922/Linux-Kernel-Dirty-COW-PTRACE_POKEDATA-Privilege-Escalation.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://packetstormsecurity.com/files/139923/Linux-Kernel-Dirty-COW-PTRACE_POKEDATA-Privilege-Escalation.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://packetstormsecurity.com/files/142151/Kernel-Live-Patch-Security-Notice-LSN-0021-1.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2098.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2105.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2106.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2107.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2110.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2118.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2120.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2124.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2126.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2127.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2128.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2132.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2133.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-20161026-linux", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.debian.org/security/2016/dsa-3696", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.huawei.com/en/psirt/security-advisories/huawei-sa-20161207-01-dirtycow-en", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.8.3", "source": "chrome-cve-admin@google.com", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/10/21/1", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/10/26/7", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/10/27/13", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/10/30/1", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/11/03/7", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2022/03/07/1", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2022/08/08/1", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2022/08/08/2", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2022/08/08/7", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2022/08/08/8", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2022/08/09/4", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2022/08/15/1", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.oracle.com/technetwork/security-advisory/cpujul2018-4258247.html", "source": "chrome-cve-admin@google.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/archive/1/539611/100/0/threaded", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/archive/1/540252/100/0/threaded", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/archive/1/540344/100/0/threaded", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/archive/1/540736/100/0/threaded", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/archive/1/archive/1/539611/100/0/threaded", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/archive/1/archive/1/540252/100/0/threaded", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/archive/1/archive/1/540344/100/0/threaded", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/archive/1/archive/1/540736/100/0/threaded", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/bid/93793", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1037078", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.ubuntu.com/usn/USN-3104-1", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-3104-2", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-3105-1", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-3105-2", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-3106-1", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-3106-2", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-3106-3", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-3106-4", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-3107-1", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-3107-2", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:0372", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/security/cve/cve-2016-5195", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/security/vulnerabilities/2706661", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://bto.bluecoat.com/security-advisory/sa134", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1384344", "source": "chrome-cve-admin@google.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://bugzilla.suse.com/show_bug.cgi?id=1004418", "source": "chrome-cve-admin@google.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://dirtycow.ninja", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/dirtycow/dirtycow.github.io/wiki/PoCs", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/dirtycow/dirtycow.github.io/wiki/VulnerabilityDetails", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/19be0eaffa3ac7d8eb6784ad9bdbc7d67ed8e619", "source": "chrome-cve-admin@google.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://h20566.www2.hpe.com/hpsc/doc/public/display?docLocale=en_US&docId=emr_na-c05352241", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://h20566.www2.hpe.com/hpsc/doc/public/display?docLocale=en_US&docId=emr_na-hpesbgn03707en_us", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://h20566.www2.hpe.com/hpsc/doc/public/display?docLocale=en_US&docId=emr_na-hpesbgn03722en_us", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://h20566.www2.hpe.com/hpsc/doc/public/display?docLocale=en_US&docId=emr_na-hpesbgn03742en_us", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://h20566.www2.hpe.com/hpsc/doc/public/display?docLocale=en_US&docId=emr_na-hpesbgn03761en_us", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://h20566.www2.hpe.com/portal/site/hpsc/public/kb/docDisplay?docId=emr_na-c05341463", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://h20566.www2.hpe.com/portal/site/hpsc/public/kb/docDisplay?docId=emr_na-c05347541", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://h20566.www2.hpe.com/portal/site/hpsc/public/kb/docDisplay?docId=emr_na-c05352241", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://help.ecostruxureit.com/display/public/UADCO8x/StruxureWare+Data+Center+Operation+Software+Vulnerability+Fixes", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://kc.mcafee.com/corporate/index?page=content&id=SB10176", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://kc.mcafee.com/corporate/index?page=content&id=SB10177", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://kc.mcafee.com/corporate/index?page=content&id=SB10222", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/E7M62SRP6CZLJ4ZXCRZKV4WPLQBSR7DT/", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/NWMDLBWMGZKFHMRJ7QUQVCERP5QHDB6W/", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/W3APRVDVPDBXLH4DC5UKZVCR742MJIM3/", "source": "chrome-cve-admin@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://people.canonical.com/~ubuntu-security/cve/2016/CVE-2016-5195.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://security-tracker.debian.org/tracker/CVE-2016-5195", "source": "chrome-cve-admin@google.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20161025-0001/", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://security.paloaltonetworks.com/CVE-2016-5195", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://source.android.com/security/bulletin/2016-11-01.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://source.android.com/security/bulletin/2016-12-01.html", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-20181107-vcsd", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.arista.com/en/support/advisories-notices/security-advisories/1753-security-advisory-0026", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.exploit-db.com/exploits/40611/", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://www.exploit-db.com/exploits/40616/", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://www.exploit-db.com/exploits/40839/", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://www.exploit-db.com/exploits/40847/", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://www.kb.cert.org/vuls/id/243144", "source": "chrome-cve-admin@google.com", "tags": ["Third Party Advisory", "US Government Resource"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/19be0eaffa3ac7d8eb6784ad9bdbc7d67ed8e619"}}