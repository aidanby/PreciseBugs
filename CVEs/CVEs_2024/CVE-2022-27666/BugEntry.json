{"buggy_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _NET_ESP_H\n#define _NET_ESP_H\n\n#include <linux/skbuff.h>\n\nstruct ip_esp_hdr;\n\nstatic inline struct ip_esp_hdr *ip_esp_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ip_esp_hdr *)skb_transport_header(skb);\n}\n\nstatic inline void esp_output_fill_trailer(u8 *tail, int tfclen, int plen, __u8 proto)\n{\n\t/* Fill padding... */\n\tif (tfclen) {\n\t\tmemset(tail, 0, tfclen);\n\t\ttail += tfclen;\n\t}\n\tdo {\n\t\tint i;\n\t\tfor (i = 0; i < plen - 2; i++)\n\t\t\ttail[i] = i + 1;\n\t} while (0);\n\ttail[plen - 2] = plen - 2;\n\ttail[plen - 1] = proto;\n}\n\nstruct esp_info {\n\tstruct\tip_esp_hdr *esph;\n\t__be64\tseqno;\n\tint\ttfclen;\n\tint\ttailen;\n\tint\tplen;\n\tint\tclen;\n\tint \tlen;\n\tint \tnfrags;\n\t__u8\tproto;\n\tbool\tinplace;\n};\n\nint esp_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp);\nint esp_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp);\nint esp_input_done2(struct sk_buff *skb, int err);\nint esp6_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp);\nint esp6_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp);\nint esp6_input_done2(struct sk_buff *skb, int err);\n#endif\n", "// SPDX-License-Identifier: GPL-2.0-only\n#define pr_fmt(fmt) \"IPsec: \" fmt\n\n#include <crypto/aead.h>\n#include <crypto/authenc.h>\n#include <linux/err.h>\n#include <linux/module.h>\n#include <net/ip.h>\n#include <net/xfrm.h>\n#include <net/esp.h>\n#include <linux/scatterlist.h>\n#include <linux/kernel.h>\n#include <linux/pfkeyv2.h>\n#include <linux/rtnetlink.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/in6.h>\n#include <net/icmp.h>\n#include <net/protocol.h>\n#include <net/udp.h>\n#include <net/tcp.h>\n#include <net/espintcp.h>\n\n#include <linux/highmem.h>\n\nstruct esp_skb_cb {\n\tstruct xfrm_skb_cb xfrm;\n\tvoid *tmp;\n};\n\nstruct esp_output_extra {\n\t__be32 seqhi;\n\tu32 esphoff;\n};\n\n#define ESP_SKB_CB(__skb) ((struct esp_skb_cb *)&((__skb)->cb[0]))\n\n/*\n * Allocate an AEAD request structure with extra space for SG and IV.\n *\n * For alignment considerations the IV is placed at the front, followed\n * by the request and finally the SG list.\n *\n * TODO: Use spare space in skb for this where possible.\n */\nstatic void *esp_alloc_tmp(struct crypto_aead *aead, int nfrags, int extralen)\n{\n\tunsigned int len;\n\n\tlen = extralen;\n\n\tlen += crypto_aead_ivsize(aead);\n\n\tif (len) {\n\t\tlen += crypto_aead_alignmask(aead) &\n\t\t       ~(crypto_tfm_ctx_alignment() - 1);\n\t\tlen = ALIGN(len, crypto_tfm_ctx_alignment());\n\t}\n\n\tlen += sizeof(struct aead_request) + crypto_aead_reqsize(aead);\n\tlen = ALIGN(len, __alignof__(struct scatterlist));\n\n\tlen += sizeof(struct scatterlist) * nfrags;\n\n\treturn kmalloc(len, GFP_ATOMIC);\n}\n\nstatic inline void *esp_tmp_extra(void *tmp)\n{\n\treturn PTR_ALIGN(tmp, __alignof__(struct esp_output_extra));\n}\n\nstatic inline u8 *esp_tmp_iv(struct crypto_aead *aead, void *tmp, int extralen)\n{\n\treturn crypto_aead_ivsize(aead) ?\n\t       PTR_ALIGN((u8 *)tmp + extralen,\n\t\t\t crypto_aead_alignmask(aead) + 1) : tmp + extralen;\n}\n\nstatic inline struct aead_request *esp_tmp_req(struct crypto_aead *aead, u8 *iv)\n{\n\tstruct aead_request *req;\n\n\treq = (void *)PTR_ALIGN(iv + crypto_aead_ivsize(aead),\n\t\t\t\tcrypto_tfm_ctx_alignment());\n\taead_request_set_tfm(req, aead);\n\treturn req;\n}\n\nstatic inline struct scatterlist *esp_req_sg(struct crypto_aead *aead,\n\t\t\t\t\t     struct aead_request *req)\n{\n\treturn (void *)ALIGN((unsigned long)(req + 1) +\n\t\t\t     crypto_aead_reqsize(aead),\n\t\t\t     __alignof__(struct scatterlist));\n}\n\nstatic void esp_ssg_unref(struct xfrm_state *x, void *tmp)\n{\n\tstruct crypto_aead *aead = x->data;\n\tint extralen = 0;\n\tu8 *iv;\n\tstruct aead_request *req;\n\tstruct scatterlist *sg;\n\n\tif (x->props.flags & XFRM_STATE_ESN)\n\t\textralen += sizeof(struct esp_output_extra);\n\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\n\t/* Unref skb_frag_pages in the src scatterlist if necessary.\n\t * Skip the first sg which comes from skb->data.\n\t */\n\tif (req->src != req->dst)\n\t\tfor (sg = sg_next(req->src); sg; sg = sg_next(sg))\n\t\t\tput_page(sg_page(sg));\n}\n\n#ifdef CONFIG_INET_ESPINTCP\nstruct esp_tcp_sk {\n\tstruct sock *sk;\n\tstruct rcu_head rcu;\n};\n\nstatic void esp_free_tcp_sk(struct rcu_head *head)\n{\n\tstruct esp_tcp_sk *esk = container_of(head, struct esp_tcp_sk, rcu);\n\n\tsock_put(esk->sk);\n\tkfree(esk);\n}\n\nstatic struct sock *esp_find_tcp_sk(struct xfrm_state *x)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct esp_tcp_sk *esk;\n\t__be16 sport, dport;\n\tstruct sock *nsk;\n\tstruct sock *sk;\n\n\tsk = rcu_dereference(x->encap_sk);\n\tif (sk && sk->sk_state == TCP_ESTABLISHED)\n\t\treturn sk;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (sk && sk == nsk) {\n\t\tesk = kmalloc(sizeof(*esk), GFP_ATOMIC);\n\t\tif (!esk) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tRCU_INIT_POINTER(x->encap_sk, NULL);\n\t\tesk->sk = sk;\n\t\tcall_rcu(&esk->rcu, esp_free_tcp_sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\tsk = inet_lookup_established(xs_net(x), &tcp_hashinfo, x->id.daddr.a4,\n\t\t\t\t     dport, x->props.saddr.a4, sport, 0);\n\tif (!sk)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (!tcp_is_ulp_esp(sk)) {\n\t\tsock_put(sk);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tspin_lock_bh(&x->lock);\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (encap->encap_sport != sport ||\n\t    encap->encap_dport != dport) {\n\t\tsock_put(sk);\n\t\tsk = nsk ?: ERR_PTR(-EREMCHG);\n\t} else if (sk == nsk) {\n\t\tsock_put(sk);\n\t} else {\n\t\trcu_assign_pointer(x->encap_sk, sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\treturn sk;\n}\n\nstatic int esp_output_tcp_finish(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tint err;\n\n\trcu_read_lock();\n\n\tsk = esp_find_tcp_sk(x);\n\terr = PTR_ERR_OR_ZERO(sk);\n\tif (err)\n\t\tgoto out;\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\terr = espintcp_queue_out(sk, skb);\n\telse\n\t\terr = espintcp_push_skb(sk, skb);\n\tbh_unlock_sock(sk);\n\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic int esp_output_tcp_encap_cb(struct net *net, struct sock *sk,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct xfrm_state *x = dst->xfrm;\n\n\treturn esp_output_tcp_finish(x, skb);\n}\n\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint err;\n\n\tlocal_bh_disable();\n\terr = xfrm_trans_queue_net(xs_net(x), skb, esp_output_tcp_encap_cb);\n\tlocal_bh_enable();\n\n\t/* EINPROGRESS just happens to do the right thing.  It\n\t * actually means that the skb has been consumed and\n\t * isn't coming back.\n\t */\n\treturn err ?: -EINPROGRESS;\n}\n#else\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n\n\treturn -EOPNOTSUPP;\n}\n#endif\n\nstatic void esp_output_done(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tvoid *tmp;\n\tstruct xfrm_state *x;\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tstruct sec_path *sp = skb_sec_path(skb);\n\n\t\tx = sp->xvec[sp->len - 1];\n\t} else {\n\t\tx = skb_dst(skb)->xfrm;\n\t}\n\n\ttmp = ESP_SKB_CB(skb)->tmp;\n\tesp_ssg_unref(x, tmp);\n\tkfree(tmp);\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tif (err) {\n\t\t\tXFRM_INC_STATS(xs_net(x), LINUX_MIB_XFRMOUTSTATEPROTOERROR);\n\t\t\tkfree_skb(skb);\n\t\t\treturn;\n\t\t}\n\n\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\tsecpath_reset(skb);\n\t\txfrm_dev_resume(skb);\n\t} else {\n\t\tif (!err &&\n\t\t    x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\t\tesp_output_tail_tcp(x, skb);\n\t\telse\n\t\t\txfrm_output_resume(skb->sk, skb, err);\n\t}\n}\n\n/* Move ESP header back into place. */\nstatic void esp_restore_header(struct sk_buff *skb, unsigned int offset)\n{\n\tstruct ip_esp_hdr *esph = (void *)(skb->data + offset);\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\t__be32 *seqhi = esp_tmp_extra(tmp);\n\n\tesph->seq_no = esph->spi;\n\tesph->spi = *seqhi;\n}\n\nstatic void esp_output_restore_header(struct sk_buff *skb)\n{\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\tstruct esp_output_extra *extra = esp_tmp_extra(tmp);\n\n\tesp_restore_header(skb, skb_transport_offset(skb) + extra->esphoff -\n\t\t\t\tsizeof(__be32));\n}\n\nstatic struct ip_esp_hdr *esp_output_set_extra(struct sk_buff *skb,\n\t\t\t\t\t       struct xfrm_state *x,\n\t\t\t\t\t       struct ip_esp_hdr *esph,\n\t\t\t\t\t       struct esp_output_extra *extra)\n{\n\t/* For ESN we move the header forward by 4 bytes to\n\t * accommodate the high bits.  We will move it back after\n\t * encryption.\n\t */\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\t__u32 seqhi;\n\t\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\n\t\tif (xo)\n\t\t\tseqhi = xo->seq.hi;\n\t\telse\n\t\t\tseqhi = XFRM_SKB_CB(skb)->seq.output.hi;\n\n\t\textra->esphoff = (unsigned char *)esph -\n\t\t\t\t skb_transport_header(skb);\n\t\tesph = (struct ip_esp_hdr *)((unsigned char *)esph - 4);\n\t\textra->seqhi = esph->spi;\n\t\tesph->seq_no = htonl(seqhi);\n\t}\n\n\tesph->spi = x->id.spi;\n\n\treturn esph;\n}\n\nstatic void esp_output_done_esn(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\tesp_output_restore_header(skb);\n\tesp_output_done(base, err);\n}\n\nstatic struct ip_esp_hdr *esp_output_udp_encap(struct sk_buff *skb,\n\t\t\t\t\t       int encap_type,\n\t\t\t\t\t       struct esp_info *esp,\n\t\t\t\t\t       __be16 sport,\n\t\t\t\t\t       __be16 dport)\n{\n\tstruct udphdr *uh;\n\t__be32 *udpdata32;\n\tunsigned int len;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len + sizeof(struct iphdr) > IP_MAX_MTU)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tuh = (struct udphdr *)esp->esph;\n\tuh->source = sport;\n\tuh->dest = dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\t*skb_mac_header(skb) = IPPROTO_UDP;\n\n\tif (encap_type == UDP_ENCAP_ESPINUDP_NON_IKE) {\n\t\tudpdata32 = (__be32 *)(uh + 1);\n\t\tudpdata32[0] = udpdata32[1] = 0;\n\t\treturn (struct ip_esp_hdr *)(udpdata32 + 2);\n\t}\n\n\treturn (struct ip_esp_hdr *)(uh + 1);\n}\n\n#ifdef CONFIG_INET_ESPINTCP\nstatic struct ip_esp_hdr *esp_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    struct esp_info *esp)\n{\n\t__be16 *lenp = (void *)esp->esph;\n\tstruct ip_esp_hdr *esph;\n\tunsigned int len;\n\tstruct sock *sk;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len > IP_MAX_MTU)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\trcu_read_lock();\n\tsk = esp_find_tcp_sk(x);\n\trcu_read_unlock();\n\n\tif (IS_ERR(sk))\n\t\treturn ERR_CAST(sk);\n\n\t*lenp = htons(len);\n\tesph = (struct ip_esp_hdr *)(lenp + 1);\n\n\treturn esph;\n}\n#else\nstatic struct ip_esp_hdr *esp_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    struct esp_info *esp)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n#endif\n\nstatic int esp_output_encap(struct xfrm_state *x, struct sk_buff *skb,\n\t\t\t    struct esp_info *esp)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct ip_esp_hdr *esph;\n\t__be16 sport, dport;\n\tint encap_type;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tencap_type = encap->encap_type;\n\tspin_unlock_bh(&x->lock);\n\n\tswitch (encap_type) {\n\tdefault:\n\tcase UDP_ENCAP_ESPINUDP:\n\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\tesph = esp_output_udp_encap(skb, encap_type, esp, sport, dport);\n\t\tbreak;\n\tcase TCP_ENCAP_ESPINTCP:\n\t\tesph = esp_output_tcp_encap(x, skb, esp);\n\t\tbreak;\n\t}\n\n\tif (IS_ERR(esph))\n\t\treturn PTR_ERR(esph);\n\n\tesp->esph = esph;\n\n\treturn 0;\n}\n\nint esp_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *tail;\n\tint nfrags;\n\tint esph_offset;\n\tstruct page *page;\n\tstruct sk_buff *trailer;\n\tint tailen = esp->tailen;\n\n\t/* this is non-NULL only with TCP/UDP Encapsulation */\n\tif (x->encap) {\n\t\tint err = esp_output_encap(x, skb, esp);\n\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tif (!skb_cloned(skb)) {\n\t\tif (tailen <= skb_tailroom(skb)) {\n\t\t\tnfrags = 1;\n\t\t\ttrailer = skb;\n\t\t\ttail = skb_tail_pointer(trailer);\n\n\t\t\tgoto skip_cow;\n\t\t} else if ((skb_shinfo(skb)->nr_frags < MAX_SKB_FRAGS)\n\t\t\t   && !skb_has_frag_list(skb)) {\n\t\t\tint allocsize;\n\t\t\tstruct sock *sk = skb->sk;\n\t\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\t\tesp->inplace = false;\n\n\t\t\tallocsize = ALIGN(tailen, L1_CACHE_BYTES);\n\n\t\t\tspin_lock_bh(&x->lock);\n\n\t\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\t\tspin_unlock_bh(&x->lock);\n\t\t\t\tgoto cow;\n\t\t\t}\n\n\t\t\tpage = pfrag->page;\n\t\t\tget_page(page);\n\n\t\t\ttail = page_address(page) + pfrag->offset;\n\n\t\t\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\n\t\t\t__skb_fill_page_desc(skb, nfrags, page, pfrag->offset,\n\t\t\t\t\t     tailen);\n\t\t\tskb_shinfo(skb)->nr_frags = ++nfrags;\n\n\t\t\tpfrag->offset = pfrag->offset + allocsize;\n\n\t\t\tspin_unlock_bh(&x->lock);\n\n\t\t\tnfrags++;\n\n\t\t\tskb->len += tailen;\n\t\t\tskb->data_len += tailen;\n\t\t\tskb->truesize += tailen;\n\t\t\tif (sk && sk_fullsock(sk))\n\t\t\t\trefcount_add(tailen, &sk->sk_wmem_alloc);\n\n\t\t\tgoto out;\n\t\t}\n\t}\n\ncow:\n\tesph_offset = (unsigned char *)esp->esph - skb_transport_header(skb);\n\n\tnfrags = skb_cow_data(skb, tailen, &trailer);\n\tif (nfrags < 0)\n\t\tgoto out;\n\ttail = skb_tail_pointer(trailer);\n\tesp->esph = (struct ip_esp_hdr *)(skb_transport_header(skb) + esph_offset);\n\nskip_cow:\n\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\tpskb_put(skb, trailer, tailen);\n\nout:\n\treturn nfrags;\n}\nEXPORT_SYMBOL_GPL(esp_output_head);\n\nint esp_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *iv;\n\tint alen;\n\tvoid *tmp;\n\tint ivlen;\n\tint assoclen;\n\tint extralen;\n\tstruct page *page;\n\tstruct ip_esp_hdr *esph;\n\tstruct crypto_aead *aead;\n\tstruct aead_request *req;\n\tstruct scatterlist *sg, *dsg;\n\tstruct esp_output_extra *extra;\n\tint err = -ENOMEM;\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\textralen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\textralen += sizeof(*extra);\n\t\tassoclen += sizeof(__be32);\n\t}\n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\tivlen = crypto_aead_ivsize(aead);\n\n\ttmp = esp_alloc_tmp(aead, esp->nfrags + 2, extralen);\n\tif (!tmp)\n\t\tgoto error;\n\n\textra = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tif (esp->inplace)\n\t\tdsg = sg;\n\telse\n\t\tdsg = &sg[esp->nfrags];\n\n\tesph = esp_output_set_extra(skb, x, esp->esph, extra);\n\tesp->esph = esph;\n\n\tsg_init_table(sg, esp->nfrags);\n\terr = skb_to_sgvec(skb, sg,\n\t\t           (unsigned char *)esph - skb->data,\n\t\t           assoclen + ivlen + esp->clen + alen);\n\tif (unlikely(err < 0))\n\t\tgoto error_free;\n\n\tif (!esp->inplace) {\n\t\tint allocsize;\n\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\tallocsize = ALIGN(skb->data_len, L1_CACHE_BYTES);\n\n\t\tspin_lock_bh(&x->lock);\n\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\tgoto error_free;\n\t\t}\n\n\t\tskb_shinfo(skb)->nr_frags = 1;\n\n\t\tpage = pfrag->page;\n\t\tget_page(page);\n\t\t/* replace page frags in skb with new page */\n\t\t__skb_fill_page_desc(skb, 0, page, pfrag->offset, skb->data_len);\n\t\tpfrag->offset = pfrag->offset + allocsize;\n\t\tspin_unlock_bh(&x->lock);\n\n\t\tsg_init_table(dsg, skb_shinfo(skb)->nr_frags + 1);\n\t\terr = skb_to_sgvec(skb, dsg,\n\t\t\t           (unsigned char *)esph - skb->data,\n\t\t\t           assoclen + ivlen + esp->clen + alen);\n\t\tif (unlikely(err < 0))\n\t\t\tgoto error_free;\n\t}\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_output_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_output_done, skb);\n\n\taead_request_set_crypt(req, sg, dsg, ivlen + esp->clen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\tmemset(iv, 0, ivlen);\n\tmemcpy(iv + ivlen - min(ivlen, 8), (u8 *)&esp->seqno + 8 - min(ivlen, 8),\n\t       min(ivlen, 8));\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\terr = crypto_aead_encrypt(req);\n\n\tswitch (err) {\n\tcase -EINPROGRESS:\n\t\tgoto error;\n\n\tcase -ENOSPC:\n\t\terr = NET_XMIT_DROP;\n\t\tbreak;\n\n\tcase 0:\n\t\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\t\tesp_output_restore_header(skb);\n\t}\n\n\tif (sg != dsg)\n\t\tesp_ssg_unref(x, tmp);\n\n\tif (!err && x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\terr = esp_output_tail_tcp(x, skb);\n\nerror_free:\n\tkfree(tmp);\nerror:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp_output_tail);\n\nstatic int esp_output(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint alen;\n\tint blksize;\n\tstruct ip_esp_hdr *esph;\n\tstruct crypto_aead *aead;\n\tstruct esp_info esp;\n\n\tesp.inplace = true;\n\n\tesp.proto = *skb_mac_header(skb);\n\t*skb_mac_header(skb) = IPPROTO_ESP;\n\n\t/* skb is pure payload to encrypt */\n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\n\tesp.tfclen = 0;\n\tif (x->tfcpad) {\n\t\tstruct xfrm_dst *dst = (struct xfrm_dst *)skb_dst(skb);\n\t\tu32 padto;\n\n\t\tpadto = min(x->tfcpad, xfrm_state_mtu(x, dst->child_mtu_cached));\n\t\tif (skb->len < padto)\n\t\t\tesp.tfclen = padto - skb->len;\n\t}\n\tblksize = ALIGN(crypto_aead_blocksize(aead), 4);\n\tesp.clen = ALIGN(skb->len + 2 + esp.tfclen, blksize);\n\tesp.plen = esp.clen - skb->len - esp.tfclen;\n\tesp.tailen = esp.tfclen + esp.plen + alen;\n\n\tesp.esph = ip_esp_hdr(skb);\n\n\tesp.nfrags = esp_output_head(x, skb, &esp);\n\tif (esp.nfrags < 0)\n\t\treturn esp.nfrags;\n\n\tesph = esp.esph;\n\tesph->spi = x->id.spi;\n\n\tesph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);\n\tesp.seqno = cpu_to_be64(XFRM_SKB_CB(skb)->seq.output.low +\n\t\t\t\t ((u64)XFRM_SKB_CB(skb)->seq.output.hi << 32));\n\n\tskb_push(skb, -skb_network_offset(skb));\n\n\treturn esp_output_tail(x, skb, &esp);\n}\n\nstatic inline int esp_remove_trailer(struct sk_buff *skb)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint alen, hlen, elen;\n\tint padlen, trimlen;\n\t__wsum csumdiff;\n\tu8 nexthdr[2];\n\tint ret;\n\n\talen = crypto_aead_authsize(aead);\n\thlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\telen = skb->len - hlen;\n\n\tif (xo && (xo->flags & XFRM_ESP_NO_TRAILER)) {\n\t\tret = xo->proto;\n\t\tgoto out;\n\t}\n\n\tif (skb_copy_bits(skb, skb->len - alen - 2, nexthdr, 2))\n\t\tBUG();\n\n\tret = -EINVAL;\n\tpadlen = nexthdr[0];\n\tif (padlen + 2 + alen >= elen) {\n\t\tnet_dbg_ratelimited(\"ipsec esp packet is garbage padlen=%d, elen=%d\\n\",\n\t\t\t\t    padlen + 2, elen - alen);\n\t\tgoto out;\n\t}\n\n\ttrimlen = alen + padlen + 2;\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tcsumdiff = skb_checksum(skb, skb->len - trimlen, trimlen, 0);\n\t\tskb->csum = csum_block_sub(skb->csum, csumdiff,\n\t\t\t\t\t   skb->len - trimlen);\n\t}\n\tpskb_trim(skb, skb->len - trimlen);\n\n\tret = nexthdr[1];\n\nout:\n\treturn ret;\n}\n\nint esp_input_done2(struct sk_buff *skb, int err)\n{\n\tconst struct iphdr *iph;\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint hlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\tint ihl;\n\n\tif (!xo || !(xo->flags & CRYPTO_DONE))\n\t\tkfree(ESP_SKB_CB(skb)->tmp);\n\n\tif (unlikely(err))\n\t\tgoto out;\n\n\terr = esp_remove_trailer(skb);\n\tif (unlikely(err < 0))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tihl = iph->ihl * 4;\n\n\tif (x->encap) {\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\t\tstruct tcphdr *th = (void *)(skb_network_header(skb) + ihl);\n\t\tstruct udphdr *uh = (void *)(skb_network_header(skb) + ihl);\n\t\t__be16 source;\n\n\t\tswitch (x->encap->encap_type) {\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\tsource = th->source;\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tsource = uh->source;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * 1) if the NAT-T peer's IP or port changed then\n\t\t *    advertize the change to the keying daemon.\n\t\t *    This is an inbound SA, so just compare\n\t\t *    SRC ports.\n\t\t */\n\t\tif (iph->saddr != x->props.saddr.a4 ||\n\t\t    source != encap->encap_sport) {\n\t\t\txfrm_address_t ipaddr;\n\n\t\t\tipaddr.a4 = iph->saddr;\n\t\t\tkm_new_mapping(x, &ipaddr, source);\n\n\t\t\t/* XXX: perhaps add an extra\n\t\t\t * policy check here, to see\n\t\t\t * if we should allow or\n\t\t\t * reject a packet from a\n\t\t\t * different source\n\t\t\t * address/port.\n\t\t\t */\n\t\t}\n\n\t\t/*\n\t\t * 2) ignore UDP/TCP checksums in case\n\t\t *    of NAT-T in Transport Mode, or\n\t\t *    perform other post-processing fixes\n\t\t *    as per draft-ietf-ipsec-udp-encaps-06,\n\t\t *    section 3.1.2\n\t\t */\n\t\tif (x->props.mode == XFRM_MODE_TRANSPORT)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\n\tskb_pull_rcsum(skb, hlen);\n\tif (x->props.mode == XFRM_MODE_TUNNEL)\n\t\tskb_reset_transport_header(skb);\n\telse\n\t\tskb_set_transport_header(skb, -ihl);\n\n\t/* RFC4303: Drop dummy packets without any error */\n\tif (err == IPPROTO_NONE)\n\t\terr = -EINVAL;\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp_input_done2);\n\nstatic void esp_input_done(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\txfrm_input_resume(skb, esp_input_done2(skb, err));\n}\n\nstatic void esp_input_restore_header(struct sk_buff *skb)\n{\n\tesp_restore_header(skb, 0);\n\t__skb_pull(skb, 4);\n}\n\nstatic void esp_input_set_header(struct sk_buff *skb, __be32 *seqhi)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct ip_esp_hdr *esph;\n\n\t/* For ESN we move the header forward by 4 bytes to\n\t * accommodate the high bits.  We will move it back after\n\t * decryption.\n\t */\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tesph = skb_push(skb, 4);\n\t\t*seqhi = esph->spi;\n\t\tesph->spi = esph->seq_no;\n\t\tesph->seq_no = XFRM_SKB_CB(skb)->seq.input.hi;\n\t}\n}\n\nstatic void esp_input_done_esn(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\tesp_input_restore_header(skb);\n\tesp_input_done(base, err);\n}\n\n/*\n * Note: detecting truncated vs. non-truncated authentication data is very\n * expensive, so we only support truncated data, which is the recommended\n * and common case.\n */\nstatic int esp_input(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct crypto_aead *aead = x->data;\n\tstruct aead_request *req;\n\tstruct sk_buff *trailer;\n\tint ivlen = crypto_aead_ivsize(aead);\n\tint elen = skb->len - sizeof(struct ip_esp_hdr) - ivlen;\n\tint nfrags;\n\tint assoclen;\n\tint seqhilen;\n\t__be32 *seqhi;\n\tvoid *tmp;\n\tu8 *iv;\n\tstruct scatterlist *sg;\n\tint err = -EINVAL;\n\n\tif (!pskb_may_pull(skb, sizeof(struct ip_esp_hdr) + ivlen))\n\t\tgoto out;\n\n\tif (elen <= 0)\n\t\tgoto out;\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\tseqhilen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\tseqhilen += sizeof(__be32);\n\t\tassoclen += seqhilen;\n\t}\n\n\tif (!skb_cloned(skb)) {\n\t\tif (!skb_is_nonlinear(skb)) {\n\t\t\tnfrags = 1;\n\n\t\t\tgoto skip_cow;\n\t\t} else if (!skb_has_frag_list(skb)) {\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\t\t\tnfrags++;\n\n\t\t\tgoto skip_cow;\n\t\t}\n\t}\n\n\terr = skb_cow_data(skb, 0, &trailer);\n\tif (err < 0)\n\t\tgoto out;\n\n\tnfrags = err;\n\nskip_cow:\n\terr = -ENOMEM;\n\ttmp = esp_alloc_tmp(aead, nfrags, seqhilen);\n\tif (!tmp)\n\t\tgoto out;\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\tseqhi = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, seqhilen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tesp_input_set_header(skb, seqhi);\n\n\tsg_init_table(sg, nfrags);\n\terr = skb_to_sgvec(skb, sg, 0, skb->len);\n\tif (unlikely(err < 0)) {\n\t\tkfree(tmp);\n\t\tgoto out;\n\t}\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_input_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_input_done, skb);\n\n\taead_request_set_crypt(req, sg, sg, elen + ivlen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\terr = crypto_aead_decrypt(req);\n\tif (err == -EINPROGRESS)\n\t\tgoto out;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\tesp_input_restore_header(skb);\n\n\terr = esp_input_done2(skb, err);\n\nout:\n\treturn err;\n}\n\nstatic int esp4_err(struct sk_buff *skb, u32 info)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\tstruct ip_esp_hdr *esph = (struct ip_esp_hdr *)(skb->data+(iph->ihl<<2));\n\tstruct xfrm_state *x;\n\n\tswitch (icmp_hdr(skb)->type) {\n\tcase ICMP_DEST_UNREACH:\n\t\tif (icmp_hdr(skb)->code != ICMP_FRAG_NEEDED)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase ICMP_REDIRECT:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tx = xfrm_state_lookup(net, skb->mark, (const xfrm_address_t *)&iph->daddr,\n\t\t\t      esph->spi, IPPROTO_ESP, AF_INET);\n\tif (!x)\n\t\treturn 0;\n\n\tif (icmp_hdr(skb)->type == ICMP_DEST_UNREACH)\n\t\tipv4_update_pmtu(skb, net, info, 0, IPPROTO_ESP);\n\telse\n\t\tipv4_redirect(skb, net, 0, IPPROTO_ESP);\n\txfrm_state_put(x);\n\n\treturn 0;\n}\n\nstatic void esp_destroy(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead = x->data;\n\n\tif (!aead)\n\t\treturn;\n\n\tcrypto_free_aead(aead);\n}\n\nstatic int esp_init_aead(struct xfrm_state *x)\n{\n\tchar aead_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_aead *aead;\n\tint err;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(aead_name, CRYPTO_MAX_ALG_NAME, \"%s(%s)\",\n\t\t     x->geniv, x->aead->alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto error;\n\n\taead = crypto_alloc_aead(aead_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead))\n\t\tgoto error;\n\n\tx->data = aead;\n\n\terr = crypto_aead_setkey(aead, x->aead->alg_key,\n\t\t\t\t (x->aead->alg_key_len + 7) / 8);\n\tif (err)\n\t\tgoto error;\n\n\terr = crypto_aead_setauthsize(aead, x->aead->alg_icv_len / 8);\n\tif (err)\n\t\tgoto error;\n\nerror:\n\treturn err;\n}\n\nstatic int esp_init_authenc(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead;\n\tstruct crypto_authenc_key_param *param;\n\tstruct rtattr *rta;\n\tchar *key;\n\tchar *p;\n\tchar authenc_name[CRYPTO_MAX_ALG_NAME];\n\tunsigned int keylen;\n\tint err;\n\n\terr = -EINVAL;\n\tif (!x->ealg)\n\t\tgoto error;\n\n\terr = -ENAMETOOLONG;\n\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthencesn(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME)\n\t\t\tgoto error;\n\t} else {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthenc(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME)\n\t\t\tgoto error;\n\t}\n\n\taead = crypto_alloc_aead(authenc_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead))\n\t\tgoto error;\n\n\tx->data = aead;\n\n\tkeylen = (x->aalg ? (x->aalg->alg_key_len + 7) / 8 : 0) +\n\t\t (x->ealg->alg_key_len + 7) / 8 + RTA_SPACE(sizeof(*param));\n\terr = -ENOMEM;\n\tkey = kmalloc(keylen, GFP_KERNEL);\n\tif (!key)\n\t\tgoto error;\n\n\tp = key;\n\trta = (void *)p;\n\trta->rta_type = CRYPTO_AUTHENC_KEYA_PARAM;\n\trta->rta_len = RTA_LENGTH(sizeof(*param));\n\tparam = RTA_DATA(rta);\n\tp += RTA_SPACE(sizeof(*param));\n\n\tif (x->aalg) {\n\t\tstruct xfrm_algo_desc *aalg_desc;\n\n\t\tmemcpy(p, x->aalg->alg_key, (x->aalg->alg_key_len + 7) / 8);\n\t\tp += (x->aalg->alg_key_len + 7) / 8;\n\n\t\taalg_desc = xfrm_aalg_get_byname(x->aalg->alg_name, 0);\n\t\tBUG_ON(!aalg_desc);\n\n\t\terr = -EINVAL;\n\t\tif (aalg_desc->uinfo.auth.icv_fullbits / 8 !=\n\t\t    crypto_aead_authsize(aead)) {\n\t\t\tpr_info(\"ESP: %s digestsize %u != %hu\\n\",\n\t\t\t\tx->aalg->alg_name,\n\t\t\t\tcrypto_aead_authsize(aead),\n\t\t\t\taalg_desc->uinfo.auth.icv_fullbits / 8);\n\t\t\tgoto free_key;\n\t\t}\n\n\t\terr = crypto_aead_setauthsize(\n\t\t\taead, x->aalg->alg_trunc_len / 8);\n\t\tif (err)\n\t\t\tgoto free_key;\n\t}\n\n\tparam->enckeylen = cpu_to_be32((x->ealg->alg_key_len + 7) / 8);\n\tmemcpy(p, x->ealg->alg_key, (x->ealg->alg_key_len + 7) / 8);\n\n\terr = crypto_aead_setkey(aead, key, keylen);\n\nfree_key:\n\tkfree(key);\n\nerror:\n\treturn err;\n}\n\nstatic int esp_init_state(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead;\n\tu32 align;\n\tint err;\n\n\tx->data = NULL;\n\n\tif (x->aead)\n\t\terr = esp_init_aead(x);\n\telse\n\t\terr = esp_init_authenc(x);\n\n\tif (err)\n\t\tgoto error;\n\n\taead = x->data;\n\n\tx->props.header_len = sizeof(struct ip_esp_hdr) +\n\t\t\t      crypto_aead_ivsize(aead);\n\tif (x->props.mode == XFRM_MODE_TUNNEL)\n\t\tx->props.header_len += sizeof(struct iphdr);\n\telse if (x->props.mode == XFRM_MODE_BEET && x->sel.family != AF_INET6)\n\t\tx->props.header_len += IPV4_BEET_PHMAXLEN;\n\tif (x->encap) {\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\n\t\tswitch (encap->encap_type) {\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto error;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\t\tx->props.header_len += sizeof(struct udphdr);\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tx->props.header_len += sizeof(struct udphdr) + 2 * sizeof(u32);\n\t\t\tbreak;\n#ifdef CONFIG_INET_ESPINTCP\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\t/* only the length field, TCP encap is done by\n\t\t\t * the socket\n\t\t\t */\n\t\t\tx->props.header_len += 2;\n\t\t\tbreak;\n#endif\n\t\t}\n\t}\n\n\talign = ALIGN(crypto_aead_blocksize(aead), 4);\n\tx->props.trailer_len = align + 1 + crypto_aead_authsize(aead);\n\nerror:\n\treturn err;\n}\n\nstatic int esp4_rcv_cb(struct sk_buff *skb, int err)\n{\n\treturn 0;\n}\n\nstatic const struct xfrm_type esp_type =\n{\n\t.owner\t\t= THIS_MODULE,\n\t.proto\t     \t= IPPROTO_ESP,\n\t.flags\t\t= XFRM_TYPE_REPLAY_PROT,\n\t.init_state\t= esp_init_state,\n\t.destructor\t= esp_destroy,\n\t.input\t\t= esp_input,\n\t.output\t\t= esp_output,\n};\n\nstatic struct xfrm4_protocol esp4_protocol = {\n\t.handler\t=\txfrm4_rcv,\n\t.input_handler\t=\txfrm_input,\n\t.cb_handler\t=\tesp4_rcv_cb,\n\t.err_handler\t=\tesp4_err,\n\t.priority\t=\t0,\n};\n\nstatic int __init esp4_init(void)\n{\n\tif (xfrm_register_type(&esp_type, AF_INET) < 0) {\n\t\tpr_info(\"%s: can't add xfrm type\\n\", __func__);\n\t\treturn -EAGAIN;\n\t}\n\tif (xfrm4_protocol_register(&esp4_protocol, IPPROTO_ESP) < 0) {\n\t\tpr_info(\"%s: can't add protocol\\n\", __func__);\n\t\txfrm_unregister_type(&esp_type, AF_INET);\n\t\treturn -EAGAIN;\n\t}\n\treturn 0;\n}\n\nstatic void __exit esp4_fini(void)\n{\n\tif (xfrm4_protocol_deregister(&esp4_protocol, IPPROTO_ESP) < 0)\n\t\tpr_info(\"%s: can't remove protocol\\n\", __func__);\n\txfrm_unregister_type(&esp_type, AF_INET);\n}\n\nmodule_init(esp4_init);\nmodule_exit(esp4_fini);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_XFRM_TYPE(AF_INET, XFRM_PROTO_ESP);\n", "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Copyright (C)2002 USAGI/WIDE Project\n *\n * Authors\n *\n *\tMitsuru KANDA @USAGI       : IPv6 Support\n *\tKazunori MIYAZAWA @USAGI   :\n *\tKunihiro Ishiguro <kunihiro@ipinfusion.com>\n *\n *\tThis file is derived from net/ipv4/esp.c\n */\n\n#define pr_fmt(fmt) \"IPv6: \" fmt\n\n#include <crypto/aead.h>\n#include <crypto/authenc.h>\n#include <linux/err.h>\n#include <linux/module.h>\n#include <net/ip.h>\n#include <net/xfrm.h>\n#include <net/esp.h>\n#include <linux/scatterlist.h>\n#include <linux/kernel.h>\n#include <linux/pfkeyv2.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <net/ip6_checksum.h>\n#include <net/ip6_route.h>\n#include <net/icmp.h>\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/udp.h>\n#include <linux/icmpv6.h>\n#include <net/tcp.h>\n#include <net/espintcp.h>\n#include <net/inet6_hashtables.h>\n\n#include <linux/highmem.h>\n\nstruct esp_skb_cb {\n\tstruct xfrm_skb_cb xfrm;\n\tvoid *tmp;\n};\n\nstruct esp_output_extra {\n\t__be32 seqhi;\n\tu32 esphoff;\n};\n\n#define ESP_SKB_CB(__skb) ((struct esp_skb_cb *)&((__skb)->cb[0]))\n\n/*\n * Allocate an AEAD request structure with extra space for SG and IV.\n *\n * For alignment considerations the upper 32 bits of the sequence number are\n * placed at the front, if present. Followed by the IV, the request and finally\n * the SG list.\n *\n * TODO: Use spare space in skb for this where possible.\n */\nstatic void *esp_alloc_tmp(struct crypto_aead *aead, int nfrags, int seqihlen)\n{\n\tunsigned int len;\n\n\tlen = seqihlen;\n\n\tlen += crypto_aead_ivsize(aead);\n\n\tif (len) {\n\t\tlen += crypto_aead_alignmask(aead) &\n\t\t       ~(crypto_tfm_ctx_alignment() - 1);\n\t\tlen = ALIGN(len, crypto_tfm_ctx_alignment());\n\t}\n\n\tlen += sizeof(struct aead_request) + crypto_aead_reqsize(aead);\n\tlen = ALIGN(len, __alignof__(struct scatterlist));\n\n\tlen += sizeof(struct scatterlist) * nfrags;\n\n\treturn kmalloc(len, GFP_ATOMIC);\n}\n\nstatic inline void *esp_tmp_extra(void *tmp)\n{\n\treturn PTR_ALIGN(tmp, __alignof__(struct esp_output_extra));\n}\n\nstatic inline u8 *esp_tmp_iv(struct crypto_aead *aead, void *tmp, int seqhilen)\n{\n\treturn crypto_aead_ivsize(aead) ?\n\t       PTR_ALIGN((u8 *)tmp + seqhilen,\n\t\t\t crypto_aead_alignmask(aead) + 1) : tmp + seqhilen;\n}\n\nstatic inline struct aead_request *esp_tmp_req(struct crypto_aead *aead, u8 *iv)\n{\n\tstruct aead_request *req;\n\n\treq = (void *)PTR_ALIGN(iv + crypto_aead_ivsize(aead),\n\t\t\t\tcrypto_tfm_ctx_alignment());\n\taead_request_set_tfm(req, aead);\n\treturn req;\n}\n\nstatic inline struct scatterlist *esp_req_sg(struct crypto_aead *aead,\n\t\t\t\t\t     struct aead_request *req)\n{\n\treturn (void *)ALIGN((unsigned long)(req + 1) +\n\t\t\t     crypto_aead_reqsize(aead),\n\t\t\t     __alignof__(struct scatterlist));\n}\n\nstatic void esp_ssg_unref(struct xfrm_state *x, void *tmp)\n{\n\tstruct crypto_aead *aead = x->data;\n\tint extralen = 0;\n\tu8 *iv;\n\tstruct aead_request *req;\n\tstruct scatterlist *sg;\n\n\tif (x->props.flags & XFRM_STATE_ESN)\n\t\textralen += sizeof(struct esp_output_extra);\n\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\n\t/* Unref skb_frag_pages in the src scatterlist if necessary.\n\t * Skip the first sg which comes from skb->data.\n\t */\n\tif (req->src != req->dst)\n\t\tfor (sg = sg_next(req->src); sg; sg = sg_next(sg))\n\t\t\tput_page(sg_page(sg));\n}\n\n#ifdef CONFIG_INET6_ESPINTCP\nstruct esp_tcp_sk {\n\tstruct sock *sk;\n\tstruct rcu_head rcu;\n};\n\nstatic void esp_free_tcp_sk(struct rcu_head *head)\n{\n\tstruct esp_tcp_sk *esk = container_of(head, struct esp_tcp_sk, rcu);\n\n\tsock_put(esk->sk);\n\tkfree(esk);\n}\n\nstatic struct sock *esp6_find_tcp_sk(struct xfrm_state *x)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct esp_tcp_sk *esk;\n\t__be16 sport, dport;\n\tstruct sock *nsk;\n\tstruct sock *sk;\n\n\tsk = rcu_dereference(x->encap_sk);\n\tif (sk && sk->sk_state == TCP_ESTABLISHED)\n\t\treturn sk;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (sk && sk == nsk) {\n\t\tesk = kmalloc(sizeof(*esk), GFP_ATOMIC);\n\t\tif (!esk) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tRCU_INIT_POINTER(x->encap_sk, NULL);\n\t\tesk->sk = sk;\n\t\tcall_rcu(&esk->rcu, esp_free_tcp_sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\tsk = __inet6_lookup_established(xs_net(x), &tcp_hashinfo, &x->id.daddr.in6,\n\t\t\t\t\tdport, &x->props.saddr.in6, ntohs(sport), 0, 0);\n\tif (!sk)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (!tcp_is_ulp_esp(sk)) {\n\t\tsock_put(sk);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tspin_lock_bh(&x->lock);\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (encap->encap_sport != sport ||\n\t    encap->encap_dport != dport) {\n\t\tsock_put(sk);\n\t\tsk = nsk ?: ERR_PTR(-EREMCHG);\n\t} else if (sk == nsk) {\n\t\tsock_put(sk);\n\t} else {\n\t\trcu_assign_pointer(x->encap_sk, sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\treturn sk;\n}\n\nstatic int esp_output_tcp_finish(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tint err;\n\n\trcu_read_lock();\n\n\tsk = esp6_find_tcp_sk(x);\n\terr = PTR_ERR_OR_ZERO(sk);\n\tif (err)\n\t\tgoto out;\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\terr = espintcp_queue_out(sk, skb);\n\telse\n\t\terr = espintcp_push_skb(sk, skb);\n\tbh_unlock_sock(sk);\n\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic int esp_output_tcp_encap_cb(struct net *net, struct sock *sk,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct xfrm_state *x = dst->xfrm;\n\n\treturn esp_output_tcp_finish(x, skb);\n}\n\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint err;\n\n\tlocal_bh_disable();\n\terr = xfrm_trans_queue_net(xs_net(x), skb, esp_output_tcp_encap_cb);\n\tlocal_bh_enable();\n\n\t/* EINPROGRESS just happens to do the right thing.  It\n\t * actually means that the skb has been consumed and\n\t * isn't coming back.\n\t */\n\treturn err ?: -EINPROGRESS;\n}\n#else\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n\n\treturn -EOPNOTSUPP;\n}\n#endif\n\nstatic void esp_output_encap_csum(struct sk_buff *skb)\n{\n\t/* UDP encap with IPv6 requires a valid checksum */\n\tif (*skb_mac_header(skb) == IPPROTO_UDP) {\n\t\tstruct udphdr *uh = udp_hdr(skb);\n\t\tstruct ipv6hdr *ip6h = ipv6_hdr(skb);\n\t\tint len = ntohs(uh->len);\n\t\tunsigned int offset = skb_transport_offset(skb);\n\t\t__wsum csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\t\tuh->check = csum_ipv6_magic(&ip6h->saddr, &ip6h->daddr,\n\t\t\t\t\t    len, IPPROTO_UDP, csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\nstatic void esp_output_done(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tvoid *tmp;\n\tstruct xfrm_state *x;\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tstruct sec_path *sp = skb_sec_path(skb);\n\n\t\tx = sp->xvec[sp->len - 1];\n\t} else {\n\t\tx = skb_dst(skb)->xfrm;\n\t}\n\n\ttmp = ESP_SKB_CB(skb)->tmp;\n\tesp_ssg_unref(x, tmp);\n\tkfree(tmp);\n\n\tesp_output_encap_csum(skb);\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tif (err) {\n\t\t\tXFRM_INC_STATS(xs_net(x), LINUX_MIB_XFRMOUTSTATEPROTOERROR);\n\t\t\tkfree_skb(skb);\n\t\t\treturn;\n\t\t}\n\n\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\tsecpath_reset(skb);\n\t\txfrm_dev_resume(skb);\n\t} else {\n\t\tif (!err &&\n\t\t    x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\t\tesp_output_tail_tcp(x, skb);\n\t\telse\n\t\t\txfrm_output_resume(skb->sk, skb, err);\n\t}\n}\n\n/* Move ESP header back into place. */\nstatic void esp_restore_header(struct sk_buff *skb, unsigned int offset)\n{\n\tstruct ip_esp_hdr *esph = (void *)(skb->data + offset);\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\t__be32 *seqhi = esp_tmp_extra(tmp);\n\n\tesph->seq_no = esph->spi;\n\tesph->spi = *seqhi;\n}\n\nstatic void esp_output_restore_header(struct sk_buff *skb)\n{\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\tstruct esp_output_extra *extra = esp_tmp_extra(tmp);\n\n\tesp_restore_header(skb, skb_transport_offset(skb) + extra->esphoff -\n\t\t\t\tsizeof(__be32));\n}\n\nstatic struct ip_esp_hdr *esp_output_set_esn(struct sk_buff *skb,\n\t\t\t\t\t     struct xfrm_state *x,\n\t\t\t\t\t     struct ip_esp_hdr *esph,\n\t\t\t\t\t     struct esp_output_extra *extra)\n{\n\t/* For ESN we move the header forward by 4 bytes to\n\t * accomodate the high bits.  We will move it back after\n\t * encryption.\n\t */\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\t__u32 seqhi;\n\t\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\n\t\tif (xo)\n\t\t\tseqhi = xo->seq.hi;\n\t\telse\n\t\t\tseqhi = XFRM_SKB_CB(skb)->seq.output.hi;\n\n\t\textra->esphoff = (unsigned char *)esph -\n\t\t\t\t skb_transport_header(skb);\n\t\tesph = (struct ip_esp_hdr *)((unsigned char *)esph - 4);\n\t\textra->seqhi = esph->spi;\n\t\tesph->seq_no = htonl(seqhi);\n\t}\n\n\tesph->spi = x->id.spi;\n\n\treturn esph;\n}\n\nstatic void esp_output_done_esn(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\tesp_output_restore_header(skb);\n\tesp_output_done(base, err);\n}\n\nstatic struct ip_esp_hdr *esp6_output_udp_encap(struct sk_buff *skb,\n\t\t\t\t\t       int encap_type,\n\t\t\t\t\t       struct esp_info *esp,\n\t\t\t\t\t       __be16 sport,\n\t\t\t\t\t       __be16 dport)\n{\n\tstruct udphdr *uh;\n\t__be32 *udpdata32;\n\tunsigned int len;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len > U16_MAX)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tuh = (struct udphdr *)esp->esph;\n\tuh->source = sport;\n\tuh->dest = dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\t*skb_mac_header(skb) = IPPROTO_UDP;\n\n\tif (encap_type == UDP_ENCAP_ESPINUDP_NON_IKE) {\n\t\tudpdata32 = (__be32 *)(uh + 1);\n\t\tudpdata32[0] = udpdata32[1] = 0;\n\t\treturn (struct ip_esp_hdr *)(udpdata32 + 2);\n\t}\n\n\treturn (struct ip_esp_hdr *)(uh + 1);\n}\n\n#ifdef CONFIG_INET6_ESPINTCP\nstatic struct ip_esp_hdr *esp6_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\tstruct sk_buff *skb,\n\t\t\t\t\t\tstruct esp_info *esp)\n{\n\t__be16 *lenp = (void *)esp->esph;\n\tstruct ip_esp_hdr *esph;\n\tunsigned int len;\n\tstruct sock *sk;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len > IP_MAX_MTU)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\trcu_read_lock();\n\tsk = esp6_find_tcp_sk(x);\n\trcu_read_unlock();\n\n\tif (IS_ERR(sk))\n\t\treturn ERR_CAST(sk);\n\n\t*lenp = htons(len);\n\tesph = (struct ip_esp_hdr *)(lenp + 1);\n\n\treturn esph;\n}\n#else\nstatic struct ip_esp_hdr *esp6_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\tstruct sk_buff *skb,\n\t\t\t\t\t\tstruct esp_info *esp)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n#endif\n\nstatic int esp6_output_encap(struct xfrm_state *x, struct sk_buff *skb,\n\t\t\t    struct esp_info *esp)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct ip_esp_hdr *esph;\n\t__be16 sport, dport;\n\tint encap_type;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tencap_type = encap->encap_type;\n\tspin_unlock_bh(&x->lock);\n\n\tswitch (encap_type) {\n\tdefault:\n\tcase UDP_ENCAP_ESPINUDP:\n\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\tesph = esp6_output_udp_encap(skb, encap_type, esp, sport, dport);\n\t\tbreak;\n\tcase TCP_ENCAP_ESPINTCP:\n\t\tesph = esp6_output_tcp_encap(x, skb, esp);\n\t\tbreak;\n\t}\n\n\tif (IS_ERR(esph))\n\t\treturn PTR_ERR(esph);\n\n\tesp->esph = esph;\n\n\treturn 0;\n}\n\nint esp6_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *tail;\n\tint nfrags;\n\tint esph_offset;\n\tstruct page *page;\n\tstruct sk_buff *trailer;\n\tint tailen = esp->tailen;\n\n\tif (x->encap) {\n\t\tint err = esp6_output_encap(x, skb, esp);\n\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tif (!skb_cloned(skb)) {\n\t\tif (tailen <= skb_tailroom(skb)) {\n\t\t\tnfrags = 1;\n\t\t\ttrailer = skb;\n\t\t\ttail = skb_tail_pointer(trailer);\n\n\t\t\tgoto skip_cow;\n\t\t} else if ((skb_shinfo(skb)->nr_frags < MAX_SKB_FRAGS)\n\t\t\t   && !skb_has_frag_list(skb)) {\n\t\t\tint allocsize;\n\t\t\tstruct sock *sk = skb->sk;\n\t\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\t\tesp->inplace = false;\n\n\t\t\tallocsize = ALIGN(tailen, L1_CACHE_BYTES);\n\n\t\t\tspin_lock_bh(&x->lock);\n\n\t\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\t\tspin_unlock_bh(&x->lock);\n\t\t\t\tgoto cow;\n\t\t\t}\n\n\t\t\tpage = pfrag->page;\n\t\t\tget_page(page);\n\n\t\t\ttail = page_address(page) + pfrag->offset;\n\n\t\t\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\n\t\t\t__skb_fill_page_desc(skb, nfrags, page, pfrag->offset,\n\t\t\t\t\t     tailen);\n\t\t\tskb_shinfo(skb)->nr_frags = ++nfrags;\n\n\t\t\tpfrag->offset = pfrag->offset + allocsize;\n\n\t\t\tspin_unlock_bh(&x->lock);\n\n\t\t\tnfrags++;\n\n\t\t\tskb->len += tailen;\n\t\t\tskb->data_len += tailen;\n\t\t\tskb->truesize += tailen;\n\t\t\tif (sk && sk_fullsock(sk))\n\t\t\t\trefcount_add(tailen, &sk->sk_wmem_alloc);\n\n\t\t\tgoto out;\n\t\t}\n\t}\n\ncow:\n\tesph_offset = (unsigned char *)esp->esph - skb_transport_header(skb);\n\n\tnfrags = skb_cow_data(skb, tailen, &trailer);\n\tif (nfrags < 0)\n\t\tgoto out;\n\ttail = skb_tail_pointer(trailer);\n\tesp->esph = (struct ip_esp_hdr *)(skb_transport_header(skb) + esph_offset);\n\nskip_cow:\n\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\tpskb_put(skb, trailer, tailen);\n\nout:\n\treturn nfrags;\n}\nEXPORT_SYMBOL_GPL(esp6_output_head);\n\nint esp6_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *iv;\n\tint alen;\n\tvoid *tmp;\n\tint ivlen;\n\tint assoclen;\n\tint extralen;\n\tstruct page *page;\n\tstruct ip_esp_hdr *esph;\n\tstruct aead_request *req;\n\tstruct crypto_aead *aead;\n\tstruct scatterlist *sg, *dsg;\n\tstruct esp_output_extra *extra;\n\tint err = -ENOMEM;\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\textralen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\textralen += sizeof(*extra);\n\t\tassoclen += sizeof(__be32);\n\t}\n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\tivlen = crypto_aead_ivsize(aead);\n\n\ttmp = esp_alloc_tmp(aead, esp->nfrags + 2, extralen);\n\tif (!tmp)\n\t\tgoto error;\n\n\textra = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tif (esp->inplace)\n\t\tdsg = sg;\n\telse\n\t\tdsg = &sg[esp->nfrags];\n\n\tesph = esp_output_set_esn(skb, x, esp->esph, extra);\n\tesp->esph = esph;\n\n\tsg_init_table(sg, esp->nfrags);\n\terr = skb_to_sgvec(skb, sg,\n\t\t           (unsigned char *)esph - skb->data,\n\t\t           assoclen + ivlen + esp->clen + alen);\n\tif (unlikely(err < 0))\n\t\tgoto error_free;\n\n\tif (!esp->inplace) {\n\t\tint allocsize;\n\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\tallocsize = ALIGN(skb->data_len, L1_CACHE_BYTES);\n\n\t\tspin_lock_bh(&x->lock);\n\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\tgoto error_free;\n\t\t}\n\n\t\tskb_shinfo(skb)->nr_frags = 1;\n\n\t\tpage = pfrag->page;\n\t\tget_page(page);\n\t\t/* replace page frags in skb with new page */\n\t\t__skb_fill_page_desc(skb, 0, page, pfrag->offset, skb->data_len);\n\t\tpfrag->offset = pfrag->offset + allocsize;\n\t\tspin_unlock_bh(&x->lock);\n\n\t\tsg_init_table(dsg, skb_shinfo(skb)->nr_frags + 1);\n\t\terr = skb_to_sgvec(skb, dsg,\n\t\t\t           (unsigned char *)esph - skb->data,\n\t\t\t           assoclen + ivlen + esp->clen + alen);\n\t\tif (unlikely(err < 0))\n\t\t\tgoto error_free;\n\t}\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_output_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_output_done, skb);\n\n\taead_request_set_crypt(req, sg, dsg, ivlen + esp->clen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\tmemset(iv, 0, ivlen);\n\tmemcpy(iv + ivlen - min(ivlen, 8), (u8 *)&esp->seqno + 8 - min(ivlen, 8),\n\t       min(ivlen, 8));\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\terr = crypto_aead_encrypt(req);\n\n\tswitch (err) {\n\tcase -EINPROGRESS:\n\t\tgoto error;\n\n\tcase -ENOSPC:\n\t\terr = NET_XMIT_DROP;\n\t\tbreak;\n\n\tcase 0:\n\t\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\t\tesp_output_restore_header(skb);\n\t\tesp_output_encap_csum(skb);\n\t}\n\n\tif (sg != dsg)\n\t\tesp_ssg_unref(x, tmp);\n\n\tif (!err && x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\terr = esp_output_tail_tcp(x, skb);\n\nerror_free:\n\tkfree(tmp);\nerror:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp6_output_tail);\n\nstatic int esp6_output(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint alen;\n\tint blksize;\n\tstruct ip_esp_hdr *esph;\n\tstruct crypto_aead *aead;\n\tstruct esp_info esp;\n\n\tesp.inplace = true;\n\n\tesp.proto = *skb_mac_header(skb);\n\t*skb_mac_header(skb) = IPPROTO_ESP;\n\n\t/* skb is pure payload to encrypt */\n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\n\tesp.tfclen = 0;\n\tif (x->tfcpad) {\n\t\tstruct xfrm_dst *dst = (struct xfrm_dst *)skb_dst(skb);\n\t\tu32 padto;\n\n\t\tpadto = min(x->tfcpad, xfrm_state_mtu(x, dst->child_mtu_cached));\n\t\tif (skb->len < padto)\n\t\t\tesp.tfclen = padto - skb->len;\n\t}\n\tblksize = ALIGN(crypto_aead_blocksize(aead), 4);\n\tesp.clen = ALIGN(skb->len + 2 + esp.tfclen, blksize);\n\tesp.plen = esp.clen - skb->len - esp.tfclen;\n\tesp.tailen = esp.tfclen + esp.plen + alen;\n\n\tesp.esph = ip_esp_hdr(skb);\n\n\tesp.nfrags = esp6_output_head(x, skb, &esp);\n\tif (esp.nfrags < 0)\n\t\treturn esp.nfrags;\n\n\tesph = esp.esph;\n\tesph->spi = x->id.spi;\n\n\tesph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);\n\tesp.seqno = cpu_to_be64(XFRM_SKB_CB(skb)->seq.output.low +\n\t\t\t    ((u64)XFRM_SKB_CB(skb)->seq.output.hi << 32));\n\n\tskb_push(skb, -skb_network_offset(skb));\n\n\treturn esp6_output_tail(x, skb, &esp);\n}\n\nstatic inline int esp_remove_trailer(struct sk_buff *skb)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint alen, hlen, elen;\n\tint padlen, trimlen;\n\t__wsum csumdiff;\n\tu8 nexthdr[2];\n\tint ret;\n\n\talen = crypto_aead_authsize(aead);\n\thlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\telen = skb->len - hlen;\n\n\tif (xo && (xo->flags & XFRM_ESP_NO_TRAILER)) {\n\t\tret = xo->proto;\n\t\tgoto out;\n\t}\n\n\tret = skb_copy_bits(skb, skb->len - alen - 2, nexthdr, 2);\n\tBUG_ON(ret);\n\n\tret = -EINVAL;\n\tpadlen = nexthdr[0];\n\tif (padlen + 2 + alen >= elen) {\n\t\tnet_dbg_ratelimited(\"ipsec esp packet is garbage padlen=%d, elen=%d\\n\",\n\t\t\t\t    padlen + 2, elen - alen);\n\t\tgoto out;\n\t}\n\n\ttrimlen = alen + padlen + 2;\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tcsumdiff = skb_checksum(skb, skb->len - trimlen, trimlen, 0);\n\t\tskb->csum = csum_block_sub(skb->csum, csumdiff,\n\t\t\t\t\t   skb->len - trimlen);\n\t}\n\tpskb_trim(skb, skb->len - trimlen);\n\n\tret = nexthdr[1];\n\nout:\n\treturn ret;\n}\n\nint esp6_input_done2(struct sk_buff *skb, int err)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint hlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\tint hdr_len = skb_network_header_len(skb);\n\n\tif (!xo || !(xo->flags & CRYPTO_DONE))\n\t\tkfree(ESP_SKB_CB(skb)->tmp);\n\n\tif (unlikely(err))\n\t\tgoto out;\n\n\terr = esp_remove_trailer(skb);\n\tif (unlikely(err < 0))\n\t\tgoto out;\n\n\tif (x->encap) {\n\t\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\t\tint offset = skb_network_offset(skb) + sizeof(*ip6h);\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\t\tu8 nexthdr = ip6h->nexthdr;\n\t\t__be16 frag_off, source;\n\t\tstruct udphdr *uh;\n\t\tstruct tcphdr *th;\n\n\t\toffset = ipv6_skip_exthdr(skb, offset, &nexthdr, &frag_off);\n\n\t\tif (offset < 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tuh = (void *)(skb->data + offset);\n\t\tth = (void *)(skb->data + offset);\n\t\thdr_len += offset;\n\n\t\tswitch (x->encap->encap_type) {\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\tsource = th->source;\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tsource = uh->source;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * 1) if the NAT-T peer's IP or port changed then\n\t\t *    advertize the change to the keying daemon.\n\t\t *    This is an inbound SA, so just compare\n\t\t *    SRC ports.\n\t\t */\n\t\tif (!ipv6_addr_equal(&ip6h->saddr, &x->props.saddr.in6) ||\n\t\t    source != encap->encap_sport) {\n\t\t\txfrm_address_t ipaddr;\n\n\t\t\tmemcpy(&ipaddr.a6, &ip6h->saddr.s6_addr, sizeof(ipaddr.a6));\n\t\t\tkm_new_mapping(x, &ipaddr, source);\n\n\t\t\t/* XXX: perhaps add an extra\n\t\t\t * policy check here, to see\n\t\t\t * if we should allow or\n\t\t\t * reject a packet from a\n\t\t\t * different source\n\t\t\t * address/port.\n\t\t\t */\n\t\t}\n\n\t\t/*\n\t\t * 2) ignore UDP/TCP checksums in case\n\t\t *    of NAT-T in Transport Mode, or\n\t\t *    perform other post-processing fixes\n\t\t *    as per draft-ietf-ipsec-udp-encaps-06,\n\t\t *    section 3.1.2\n\t\t */\n\t\tif (x->props.mode == XFRM_MODE_TRANSPORT)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\n\tskb_postpull_rcsum(skb, skb_network_header(skb),\n\t\t\t   skb_network_header_len(skb));\n\tskb_pull_rcsum(skb, hlen);\n\tif (x->props.mode == XFRM_MODE_TUNNEL)\n\t\tskb_reset_transport_header(skb);\n\telse\n\t\tskb_set_transport_header(skb, -hdr_len);\n\n\t/* RFC4303: Drop dummy packets without any error */\n\tif (err == IPPROTO_NONE)\n\t\terr = -EINVAL;\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp6_input_done2);\n\nstatic void esp_input_done(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\txfrm_input_resume(skb, esp6_input_done2(skb, err));\n}\n\nstatic void esp_input_restore_header(struct sk_buff *skb)\n{\n\tesp_restore_header(skb, 0);\n\t__skb_pull(skb, 4);\n}\n\nstatic void esp_input_set_header(struct sk_buff *skb, __be32 *seqhi)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\n\t/* For ESN we move the header forward by 4 bytes to\n\t * accomodate the high bits.  We will move it back after\n\t * decryption.\n\t */\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tstruct ip_esp_hdr *esph = skb_push(skb, 4);\n\n\t\t*seqhi = esph->spi;\n\t\tesph->spi = esph->seq_no;\n\t\tesph->seq_no = XFRM_SKB_CB(skb)->seq.input.hi;\n\t}\n}\n\nstatic void esp_input_done_esn(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\tesp_input_restore_header(skb);\n\tesp_input_done(base, err);\n}\n\nstatic int esp6_input(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct crypto_aead *aead = x->data;\n\tstruct aead_request *req;\n\tstruct sk_buff *trailer;\n\tint ivlen = crypto_aead_ivsize(aead);\n\tint elen = skb->len - sizeof(struct ip_esp_hdr) - ivlen;\n\tint nfrags;\n\tint assoclen;\n\tint seqhilen;\n\tint ret = 0;\n\tvoid *tmp;\n\t__be32 *seqhi;\n\tu8 *iv;\n\tstruct scatterlist *sg;\n\n\tif (!pskb_may_pull(skb, sizeof(struct ip_esp_hdr) + ivlen)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (elen <= 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\tseqhilen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\tseqhilen += sizeof(__be32);\n\t\tassoclen += seqhilen;\n\t}\n\n\tif (!skb_cloned(skb)) {\n\t\tif (!skb_is_nonlinear(skb)) {\n\t\t\tnfrags = 1;\n\n\t\t\tgoto skip_cow;\n\t\t} else if (!skb_has_frag_list(skb)) {\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\t\t\tnfrags++;\n\n\t\t\tgoto skip_cow;\n\t\t}\n\t}\n\n\tnfrags = skb_cow_data(skb, 0, &trailer);\n\tif (nfrags < 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\nskip_cow:\n\tret = -ENOMEM;\n\ttmp = esp_alloc_tmp(aead, nfrags, seqhilen);\n\tif (!tmp)\n\t\tgoto out;\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\tseqhi = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, seqhilen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tesp_input_set_header(skb, seqhi);\n\n\tsg_init_table(sg, nfrags);\n\tret = skb_to_sgvec(skb, sg, 0, skb->len);\n\tif (unlikely(ret < 0)) {\n\t\tkfree(tmp);\n\t\tgoto out;\n\t}\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_input_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_input_done, skb);\n\n\taead_request_set_crypt(req, sg, sg, elen + ivlen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\tret = crypto_aead_decrypt(req);\n\tif (ret == -EINPROGRESS)\n\t\tgoto out;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\tesp_input_restore_header(skb);\n\n\tret = esp6_input_done2(skb, ret);\n\nout:\n\treturn ret;\n}\n\nstatic int esp6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t    u8 type, u8 code, int offset, __be32 info)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct ipv6hdr *iph = (const struct ipv6hdr *)skb->data;\n\tstruct ip_esp_hdr *esph = (struct ip_esp_hdr *)(skb->data + offset);\n\tstruct xfrm_state *x;\n\n\tif (type != ICMPV6_PKT_TOOBIG &&\n\t    type != NDISC_REDIRECT)\n\t\treturn 0;\n\n\tx = xfrm_state_lookup(net, skb->mark, (const xfrm_address_t *)&iph->daddr,\n\t\t\t      esph->spi, IPPROTO_ESP, AF_INET6);\n\tif (!x)\n\t\treturn 0;\n\n\tif (type == NDISC_REDIRECT)\n\t\tip6_redirect(skb, net, skb->dev->ifindex, 0,\n\t\t\t     sock_net_uid(net, NULL));\n\telse\n\t\tip6_update_pmtu(skb, net, info, 0, 0, sock_net_uid(net, NULL));\n\txfrm_state_put(x);\n\n\treturn 0;\n}\n\nstatic void esp6_destroy(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead = x->data;\n\n\tif (!aead)\n\t\treturn;\n\n\tcrypto_free_aead(aead);\n}\n\nstatic int esp_init_aead(struct xfrm_state *x)\n{\n\tchar aead_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_aead *aead;\n\tint err;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(aead_name, CRYPTO_MAX_ALG_NAME, \"%s(%s)\",\n\t\t     x->geniv, x->aead->alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto error;\n\n\taead = crypto_alloc_aead(aead_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead))\n\t\tgoto error;\n\n\tx->data = aead;\n\n\terr = crypto_aead_setkey(aead, x->aead->alg_key,\n\t\t\t\t (x->aead->alg_key_len + 7) / 8);\n\tif (err)\n\t\tgoto error;\n\n\terr = crypto_aead_setauthsize(aead, x->aead->alg_icv_len / 8);\n\tif (err)\n\t\tgoto error;\n\nerror:\n\treturn err;\n}\n\nstatic int esp_init_authenc(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead;\n\tstruct crypto_authenc_key_param *param;\n\tstruct rtattr *rta;\n\tchar *key;\n\tchar *p;\n\tchar authenc_name[CRYPTO_MAX_ALG_NAME];\n\tunsigned int keylen;\n\tint err;\n\n\terr = -EINVAL;\n\tif (!x->ealg)\n\t\tgoto error;\n\n\terr = -ENAMETOOLONG;\n\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthencesn(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME)\n\t\t\tgoto error;\n\t} else {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthenc(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME)\n\t\t\tgoto error;\n\t}\n\n\taead = crypto_alloc_aead(authenc_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead))\n\t\tgoto error;\n\n\tx->data = aead;\n\n\tkeylen = (x->aalg ? (x->aalg->alg_key_len + 7) / 8 : 0) +\n\t\t (x->ealg->alg_key_len + 7) / 8 + RTA_SPACE(sizeof(*param));\n\terr = -ENOMEM;\n\tkey = kmalloc(keylen, GFP_KERNEL);\n\tif (!key)\n\t\tgoto error;\n\n\tp = key;\n\trta = (void *)p;\n\trta->rta_type = CRYPTO_AUTHENC_KEYA_PARAM;\n\trta->rta_len = RTA_LENGTH(sizeof(*param));\n\tparam = RTA_DATA(rta);\n\tp += RTA_SPACE(sizeof(*param));\n\n\tif (x->aalg) {\n\t\tstruct xfrm_algo_desc *aalg_desc;\n\n\t\tmemcpy(p, x->aalg->alg_key, (x->aalg->alg_key_len + 7) / 8);\n\t\tp += (x->aalg->alg_key_len + 7) / 8;\n\n\t\taalg_desc = xfrm_aalg_get_byname(x->aalg->alg_name, 0);\n\t\tBUG_ON(!aalg_desc);\n\n\t\terr = -EINVAL;\n\t\tif (aalg_desc->uinfo.auth.icv_fullbits / 8 !=\n\t\t    crypto_aead_authsize(aead)) {\n\t\t\tpr_info(\"ESP: %s digestsize %u != %u\\n\",\n\t\t\t\tx->aalg->alg_name,\n\t\t\t\tcrypto_aead_authsize(aead),\n\t\t\t\taalg_desc->uinfo.auth.icv_fullbits / 8);\n\t\t\tgoto free_key;\n\t\t}\n\n\t\terr = crypto_aead_setauthsize(\n\t\t\taead, x->aalg->alg_trunc_len / 8);\n\t\tif (err)\n\t\t\tgoto free_key;\n\t}\n\n\tparam->enckeylen = cpu_to_be32((x->ealg->alg_key_len + 7) / 8);\n\tmemcpy(p, x->ealg->alg_key, (x->ealg->alg_key_len + 7) / 8);\n\n\terr = crypto_aead_setkey(aead, key, keylen);\n\nfree_key:\n\tkfree(key);\n\nerror:\n\treturn err;\n}\n\nstatic int esp6_init_state(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead;\n\tu32 align;\n\tint err;\n\n\tx->data = NULL;\n\n\tif (x->aead)\n\t\terr = esp_init_aead(x);\n\telse\n\t\terr = esp_init_authenc(x);\n\n\tif (err)\n\t\tgoto error;\n\n\taead = x->data;\n\n\tx->props.header_len = sizeof(struct ip_esp_hdr) +\n\t\t\t      crypto_aead_ivsize(aead);\n\tswitch (x->props.mode) {\n\tcase XFRM_MODE_BEET:\n\t\tif (x->sel.family != AF_INET6)\n\t\t\tx->props.header_len += IPV4_BEET_PHMAXLEN +\n\t\t\t\t\t       (sizeof(struct ipv6hdr) - sizeof(struct iphdr));\n\t\tbreak;\n\tdefault:\n\tcase XFRM_MODE_TRANSPORT:\n\t\tbreak;\n\tcase XFRM_MODE_TUNNEL:\n\t\tx->props.header_len += sizeof(struct ipv6hdr);\n\t\tbreak;\n\t}\n\n\tif (x->encap) {\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\n\t\tswitch (encap->encap_type) {\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto error;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\t\tx->props.header_len += sizeof(struct udphdr);\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tx->props.header_len += sizeof(struct udphdr) + 2 * sizeof(u32);\n\t\t\tbreak;\n#ifdef CONFIG_INET6_ESPINTCP\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\t/* only the length field, TCP encap is done by\n\t\t\t * the socket\n\t\t\t */\n\t\t\tx->props.header_len += 2;\n\t\t\tbreak;\n#endif\n\t\t}\n\t}\n\n\talign = ALIGN(crypto_aead_blocksize(aead), 4);\n\tx->props.trailer_len = align + 1 + crypto_aead_authsize(aead);\n\nerror:\n\treturn err;\n}\n\nstatic int esp6_rcv_cb(struct sk_buff *skb, int err)\n{\n\treturn 0;\n}\n\nstatic const struct xfrm_type esp6_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.proto\t\t= IPPROTO_ESP,\n\t.flags\t\t= XFRM_TYPE_REPLAY_PROT,\n\t.init_state\t= esp6_init_state,\n\t.destructor\t= esp6_destroy,\n\t.input\t\t= esp6_input,\n\t.output\t\t= esp6_output,\n};\n\nstatic struct xfrm6_protocol esp6_protocol = {\n\t.handler\t=\txfrm6_rcv,\n\t.input_handler\t=\txfrm_input,\n\t.cb_handler\t=\tesp6_rcv_cb,\n\t.err_handler\t=\tesp6_err,\n\t.priority\t=\t0,\n};\n\nstatic int __init esp6_init(void)\n{\n\tif (xfrm_register_type(&esp6_type, AF_INET6) < 0) {\n\t\tpr_info(\"%s: can't add xfrm type\\n\", __func__);\n\t\treturn -EAGAIN;\n\t}\n\tif (xfrm6_protocol_register(&esp6_protocol, IPPROTO_ESP) < 0) {\n\t\tpr_info(\"%s: can't add protocol\\n\", __func__);\n\t\txfrm_unregister_type(&esp6_type, AF_INET6);\n\t\treturn -EAGAIN;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit esp6_fini(void)\n{\n\tif (xfrm6_protocol_deregister(&esp6_protocol, IPPROTO_ESP) < 0)\n\t\tpr_info(\"%s: can't remove protocol\\n\", __func__);\n\txfrm_unregister_type(&esp6_type, AF_INET6);\n}\n\nmodule_init(esp6_init);\nmodule_exit(esp6_fini);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_XFRM_TYPE(AF_INET6, XFRM_PROTO_ESP);\n"], "fixing_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _NET_ESP_H\n#define _NET_ESP_H\n\n#include <linux/skbuff.h>\n\n#define ESP_SKB_FRAG_MAXSIZE (PAGE_SIZE << SKB_FRAG_PAGE_ORDER)\n\nstruct ip_esp_hdr;\n\nstatic inline struct ip_esp_hdr *ip_esp_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ip_esp_hdr *)skb_transport_header(skb);\n}\n\nstatic inline void esp_output_fill_trailer(u8 *tail, int tfclen, int plen, __u8 proto)\n{\n\t/* Fill padding... */\n\tif (tfclen) {\n\t\tmemset(tail, 0, tfclen);\n\t\ttail += tfclen;\n\t}\n\tdo {\n\t\tint i;\n\t\tfor (i = 0; i < plen - 2; i++)\n\t\t\ttail[i] = i + 1;\n\t} while (0);\n\ttail[plen - 2] = plen - 2;\n\ttail[plen - 1] = proto;\n}\n\nstruct esp_info {\n\tstruct\tip_esp_hdr *esph;\n\t__be64\tseqno;\n\tint\ttfclen;\n\tint\ttailen;\n\tint\tplen;\n\tint\tclen;\n\tint \tlen;\n\tint \tnfrags;\n\t__u8\tproto;\n\tbool\tinplace;\n};\n\nint esp_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp);\nint esp_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp);\nint esp_input_done2(struct sk_buff *skb, int err);\nint esp6_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp);\nint esp6_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp);\nint esp6_input_done2(struct sk_buff *skb, int err);\n#endif\n", "// SPDX-License-Identifier: GPL-2.0-only\n#define pr_fmt(fmt) \"IPsec: \" fmt\n\n#include <crypto/aead.h>\n#include <crypto/authenc.h>\n#include <linux/err.h>\n#include <linux/module.h>\n#include <net/ip.h>\n#include <net/xfrm.h>\n#include <net/esp.h>\n#include <linux/scatterlist.h>\n#include <linux/kernel.h>\n#include <linux/pfkeyv2.h>\n#include <linux/rtnetlink.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/in6.h>\n#include <net/icmp.h>\n#include <net/protocol.h>\n#include <net/udp.h>\n#include <net/tcp.h>\n#include <net/espintcp.h>\n\n#include <linux/highmem.h>\n\nstruct esp_skb_cb {\n\tstruct xfrm_skb_cb xfrm;\n\tvoid *tmp;\n};\n\nstruct esp_output_extra {\n\t__be32 seqhi;\n\tu32 esphoff;\n};\n\n#define ESP_SKB_CB(__skb) ((struct esp_skb_cb *)&((__skb)->cb[0]))\n\n/*\n * Allocate an AEAD request structure with extra space for SG and IV.\n *\n * For alignment considerations the IV is placed at the front, followed\n * by the request and finally the SG list.\n *\n * TODO: Use spare space in skb for this where possible.\n */\nstatic void *esp_alloc_tmp(struct crypto_aead *aead, int nfrags, int extralen)\n{\n\tunsigned int len;\n\n\tlen = extralen;\n\n\tlen += crypto_aead_ivsize(aead);\n\n\tif (len) {\n\t\tlen += crypto_aead_alignmask(aead) &\n\t\t       ~(crypto_tfm_ctx_alignment() - 1);\n\t\tlen = ALIGN(len, crypto_tfm_ctx_alignment());\n\t}\n\n\tlen += sizeof(struct aead_request) + crypto_aead_reqsize(aead);\n\tlen = ALIGN(len, __alignof__(struct scatterlist));\n\n\tlen += sizeof(struct scatterlist) * nfrags;\n\n\treturn kmalloc(len, GFP_ATOMIC);\n}\n\nstatic inline void *esp_tmp_extra(void *tmp)\n{\n\treturn PTR_ALIGN(tmp, __alignof__(struct esp_output_extra));\n}\n\nstatic inline u8 *esp_tmp_iv(struct crypto_aead *aead, void *tmp, int extralen)\n{\n\treturn crypto_aead_ivsize(aead) ?\n\t       PTR_ALIGN((u8 *)tmp + extralen,\n\t\t\t crypto_aead_alignmask(aead) + 1) : tmp + extralen;\n}\n\nstatic inline struct aead_request *esp_tmp_req(struct crypto_aead *aead, u8 *iv)\n{\n\tstruct aead_request *req;\n\n\treq = (void *)PTR_ALIGN(iv + crypto_aead_ivsize(aead),\n\t\t\t\tcrypto_tfm_ctx_alignment());\n\taead_request_set_tfm(req, aead);\n\treturn req;\n}\n\nstatic inline struct scatterlist *esp_req_sg(struct crypto_aead *aead,\n\t\t\t\t\t     struct aead_request *req)\n{\n\treturn (void *)ALIGN((unsigned long)(req + 1) +\n\t\t\t     crypto_aead_reqsize(aead),\n\t\t\t     __alignof__(struct scatterlist));\n}\n\nstatic void esp_ssg_unref(struct xfrm_state *x, void *tmp)\n{\n\tstruct crypto_aead *aead = x->data;\n\tint extralen = 0;\n\tu8 *iv;\n\tstruct aead_request *req;\n\tstruct scatterlist *sg;\n\n\tif (x->props.flags & XFRM_STATE_ESN)\n\t\textralen += sizeof(struct esp_output_extra);\n\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\n\t/* Unref skb_frag_pages in the src scatterlist if necessary.\n\t * Skip the first sg which comes from skb->data.\n\t */\n\tif (req->src != req->dst)\n\t\tfor (sg = sg_next(req->src); sg; sg = sg_next(sg))\n\t\t\tput_page(sg_page(sg));\n}\n\n#ifdef CONFIG_INET_ESPINTCP\nstruct esp_tcp_sk {\n\tstruct sock *sk;\n\tstruct rcu_head rcu;\n};\n\nstatic void esp_free_tcp_sk(struct rcu_head *head)\n{\n\tstruct esp_tcp_sk *esk = container_of(head, struct esp_tcp_sk, rcu);\n\n\tsock_put(esk->sk);\n\tkfree(esk);\n}\n\nstatic struct sock *esp_find_tcp_sk(struct xfrm_state *x)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct esp_tcp_sk *esk;\n\t__be16 sport, dport;\n\tstruct sock *nsk;\n\tstruct sock *sk;\n\n\tsk = rcu_dereference(x->encap_sk);\n\tif (sk && sk->sk_state == TCP_ESTABLISHED)\n\t\treturn sk;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (sk && sk == nsk) {\n\t\tesk = kmalloc(sizeof(*esk), GFP_ATOMIC);\n\t\tif (!esk) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tRCU_INIT_POINTER(x->encap_sk, NULL);\n\t\tesk->sk = sk;\n\t\tcall_rcu(&esk->rcu, esp_free_tcp_sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\tsk = inet_lookup_established(xs_net(x), &tcp_hashinfo, x->id.daddr.a4,\n\t\t\t\t     dport, x->props.saddr.a4, sport, 0);\n\tif (!sk)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (!tcp_is_ulp_esp(sk)) {\n\t\tsock_put(sk);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tspin_lock_bh(&x->lock);\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (encap->encap_sport != sport ||\n\t    encap->encap_dport != dport) {\n\t\tsock_put(sk);\n\t\tsk = nsk ?: ERR_PTR(-EREMCHG);\n\t} else if (sk == nsk) {\n\t\tsock_put(sk);\n\t} else {\n\t\trcu_assign_pointer(x->encap_sk, sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\treturn sk;\n}\n\nstatic int esp_output_tcp_finish(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tint err;\n\n\trcu_read_lock();\n\n\tsk = esp_find_tcp_sk(x);\n\terr = PTR_ERR_OR_ZERO(sk);\n\tif (err)\n\t\tgoto out;\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\terr = espintcp_queue_out(sk, skb);\n\telse\n\t\terr = espintcp_push_skb(sk, skb);\n\tbh_unlock_sock(sk);\n\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic int esp_output_tcp_encap_cb(struct net *net, struct sock *sk,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct xfrm_state *x = dst->xfrm;\n\n\treturn esp_output_tcp_finish(x, skb);\n}\n\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint err;\n\n\tlocal_bh_disable();\n\terr = xfrm_trans_queue_net(xs_net(x), skb, esp_output_tcp_encap_cb);\n\tlocal_bh_enable();\n\n\t/* EINPROGRESS just happens to do the right thing.  It\n\t * actually means that the skb has been consumed and\n\t * isn't coming back.\n\t */\n\treturn err ?: -EINPROGRESS;\n}\n#else\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n\n\treturn -EOPNOTSUPP;\n}\n#endif\n\nstatic void esp_output_done(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tvoid *tmp;\n\tstruct xfrm_state *x;\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tstruct sec_path *sp = skb_sec_path(skb);\n\n\t\tx = sp->xvec[sp->len - 1];\n\t} else {\n\t\tx = skb_dst(skb)->xfrm;\n\t}\n\n\ttmp = ESP_SKB_CB(skb)->tmp;\n\tesp_ssg_unref(x, tmp);\n\tkfree(tmp);\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tif (err) {\n\t\t\tXFRM_INC_STATS(xs_net(x), LINUX_MIB_XFRMOUTSTATEPROTOERROR);\n\t\t\tkfree_skb(skb);\n\t\t\treturn;\n\t\t}\n\n\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\tsecpath_reset(skb);\n\t\txfrm_dev_resume(skb);\n\t} else {\n\t\tif (!err &&\n\t\t    x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\t\tesp_output_tail_tcp(x, skb);\n\t\telse\n\t\t\txfrm_output_resume(skb->sk, skb, err);\n\t}\n}\n\n/* Move ESP header back into place. */\nstatic void esp_restore_header(struct sk_buff *skb, unsigned int offset)\n{\n\tstruct ip_esp_hdr *esph = (void *)(skb->data + offset);\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\t__be32 *seqhi = esp_tmp_extra(tmp);\n\n\tesph->seq_no = esph->spi;\n\tesph->spi = *seqhi;\n}\n\nstatic void esp_output_restore_header(struct sk_buff *skb)\n{\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\tstruct esp_output_extra *extra = esp_tmp_extra(tmp);\n\n\tesp_restore_header(skb, skb_transport_offset(skb) + extra->esphoff -\n\t\t\t\tsizeof(__be32));\n}\n\nstatic struct ip_esp_hdr *esp_output_set_extra(struct sk_buff *skb,\n\t\t\t\t\t       struct xfrm_state *x,\n\t\t\t\t\t       struct ip_esp_hdr *esph,\n\t\t\t\t\t       struct esp_output_extra *extra)\n{\n\t/* For ESN we move the header forward by 4 bytes to\n\t * accommodate the high bits.  We will move it back after\n\t * encryption.\n\t */\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\t__u32 seqhi;\n\t\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\n\t\tif (xo)\n\t\t\tseqhi = xo->seq.hi;\n\t\telse\n\t\t\tseqhi = XFRM_SKB_CB(skb)->seq.output.hi;\n\n\t\textra->esphoff = (unsigned char *)esph -\n\t\t\t\t skb_transport_header(skb);\n\t\tesph = (struct ip_esp_hdr *)((unsigned char *)esph - 4);\n\t\textra->seqhi = esph->spi;\n\t\tesph->seq_no = htonl(seqhi);\n\t}\n\n\tesph->spi = x->id.spi;\n\n\treturn esph;\n}\n\nstatic void esp_output_done_esn(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\tesp_output_restore_header(skb);\n\tesp_output_done(base, err);\n}\n\nstatic struct ip_esp_hdr *esp_output_udp_encap(struct sk_buff *skb,\n\t\t\t\t\t       int encap_type,\n\t\t\t\t\t       struct esp_info *esp,\n\t\t\t\t\t       __be16 sport,\n\t\t\t\t\t       __be16 dport)\n{\n\tstruct udphdr *uh;\n\t__be32 *udpdata32;\n\tunsigned int len;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len + sizeof(struct iphdr) > IP_MAX_MTU)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tuh = (struct udphdr *)esp->esph;\n\tuh->source = sport;\n\tuh->dest = dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\t*skb_mac_header(skb) = IPPROTO_UDP;\n\n\tif (encap_type == UDP_ENCAP_ESPINUDP_NON_IKE) {\n\t\tudpdata32 = (__be32 *)(uh + 1);\n\t\tudpdata32[0] = udpdata32[1] = 0;\n\t\treturn (struct ip_esp_hdr *)(udpdata32 + 2);\n\t}\n\n\treturn (struct ip_esp_hdr *)(uh + 1);\n}\n\n#ifdef CONFIG_INET_ESPINTCP\nstatic struct ip_esp_hdr *esp_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    struct esp_info *esp)\n{\n\t__be16 *lenp = (void *)esp->esph;\n\tstruct ip_esp_hdr *esph;\n\tunsigned int len;\n\tstruct sock *sk;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len > IP_MAX_MTU)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\trcu_read_lock();\n\tsk = esp_find_tcp_sk(x);\n\trcu_read_unlock();\n\n\tif (IS_ERR(sk))\n\t\treturn ERR_CAST(sk);\n\n\t*lenp = htons(len);\n\tesph = (struct ip_esp_hdr *)(lenp + 1);\n\n\treturn esph;\n}\n#else\nstatic struct ip_esp_hdr *esp_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    struct esp_info *esp)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n#endif\n\nstatic int esp_output_encap(struct xfrm_state *x, struct sk_buff *skb,\n\t\t\t    struct esp_info *esp)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct ip_esp_hdr *esph;\n\t__be16 sport, dport;\n\tint encap_type;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tencap_type = encap->encap_type;\n\tspin_unlock_bh(&x->lock);\n\n\tswitch (encap_type) {\n\tdefault:\n\tcase UDP_ENCAP_ESPINUDP:\n\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\tesph = esp_output_udp_encap(skb, encap_type, esp, sport, dport);\n\t\tbreak;\n\tcase TCP_ENCAP_ESPINTCP:\n\t\tesph = esp_output_tcp_encap(x, skb, esp);\n\t\tbreak;\n\t}\n\n\tif (IS_ERR(esph))\n\t\treturn PTR_ERR(esph);\n\n\tesp->esph = esph;\n\n\treturn 0;\n}\n\nint esp_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *tail;\n\tint nfrags;\n\tint esph_offset;\n\tstruct page *page;\n\tstruct sk_buff *trailer;\n\tint tailen = esp->tailen;\n\tunsigned int allocsz;\n\n\t/* this is non-NULL only with TCP/UDP Encapsulation */\n\tif (x->encap) {\n\t\tint err = esp_output_encap(x, skb, esp);\n\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tallocsz = ALIGN(skb->data_len + tailen, L1_CACHE_BYTES);\n\tif (allocsz > ESP_SKB_FRAG_MAXSIZE)\n\t\tgoto cow;\n\n\tif (!skb_cloned(skb)) {\n\t\tif (tailen <= skb_tailroom(skb)) {\n\t\t\tnfrags = 1;\n\t\t\ttrailer = skb;\n\t\t\ttail = skb_tail_pointer(trailer);\n\n\t\t\tgoto skip_cow;\n\t\t} else if ((skb_shinfo(skb)->nr_frags < MAX_SKB_FRAGS)\n\t\t\t   && !skb_has_frag_list(skb)) {\n\t\t\tint allocsize;\n\t\t\tstruct sock *sk = skb->sk;\n\t\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\t\tesp->inplace = false;\n\n\t\t\tallocsize = ALIGN(tailen, L1_CACHE_BYTES);\n\n\t\t\tspin_lock_bh(&x->lock);\n\n\t\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\t\tspin_unlock_bh(&x->lock);\n\t\t\t\tgoto cow;\n\t\t\t}\n\n\t\t\tpage = pfrag->page;\n\t\t\tget_page(page);\n\n\t\t\ttail = page_address(page) + pfrag->offset;\n\n\t\t\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\n\t\t\t__skb_fill_page_desc(skb, nfrags, page, pfrag->offset,\n\t\t\t\t\t     tailen);\n\t\t\tskb_shinfo(skb)->nr_frags = ++nfrags;\n\n\t\t\tpfrag->offset = pfrag->offset + allocsize;\n\n\t\t\tspin_unlock_bh(&x->lock);\n\n\t\t\tnfrags++;\n\n\t\t\tskb->len += tailen;\n\t\t\tskb->data_len += tailen;\n\t\t\tskb->truesize += tailen;\n\t\t\tif (sk && sk_fullsock(sk))\n\t\t\t\trefcount_add(tailen, &sk->sk_wmem_alloc);\n\n\t\t\tgoto out;\n\t\t}\n\t}\n\ncow:\n\tesph_offset = (unsigned char *)esp->esph - skb_transport_header(skb);\n\n\tnfrags = skb_cow_data(skb, tailen, &trailer);\n\tif (nfrags < 0)\n\t\tgoto out;\n\ttail = skb_tail_pointer(trailer);\n\tesp->esph = (struct ip_esp_hdr *)(skb_transport_header(skb) + esph_offset);\n\nskip_cow:\n\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\tpskb_put(skb, trailer, tailen);\n\nout:\n\treturn nfrags;\n}\nEXPORT_SYMBOL_GPL(esp_output_head);\n\nint esp_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *iv;\n\tint alen;\n\tvoid *tmp;\n\tint ivlen;\n\tint assoclen;\n\tint extralen;\n\tstruct page *page;\n\tstruct ip_esp_hdr *esph;\n\tstruct crypto_aead *aead;\n\tstruct aead_request *req;\n\tstruct scatterlist *sg, *dsg;\n\tstruct esp_output_extra *extra;\n\tint err = -ENOMEM;\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\textralen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\textralen += sizeof(*extra);\n\t\tassoclen += sizeof(__be32);\n\t}\n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\tivlen = crypto_aead_ivsize(aead);\n\n\ttmp = esp_alloc_tmp(aead, esp->nfrags + 2, extralen);\n\tif (!tmp)\n\t\tgoto error;\n\n\textra = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tif (esp->inplace)\n\t\tdsg = sg;\n\telse\n\t\tdsg = &sg[esp->nfrags];\n\n\tesph = esp_output_set_extra(skb, x, esp->esph, extra);\n\tesp->esph = esph;\n\n\tsg_init_table(sg, esp->nfrags);\n\terr = skb_to_sgvec(skb, sg,\n\t\t           (unsigned char *)esph - skb->data,\n\t\t           assoclen + ivlen + esp->clen + alen);\n\tif (unlikely(err < 0))\n\t\tgoto error_free;\n\n\tif (!esp->inplace) {\n\t\tint allocsize;\n\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\tallocsize = ALIGN(skb->data_len, L1_CACHE_BYTES);\n\n\t\tspin_lock_bh(&x->lock);\n\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\tgoto error_free;\n\t\t}\n\n\t\tskb_shinfo(skb)->nr_frags = 1;\n\n\t\tpage = pfrag->page;\n\t\tget_page(page);\n\t\t/* replace page frags in skb with new page */\n\t\t__skb_fill_page_desc(skb, 0, page, pfrag->offset, skb->data_len);\n\t\tpfrag->offset = pfrag->offset + allocsize;\n\t\tspin_unlock_bh(&x->lock);\n\n\t\tsg_init_table(dsg, skb_shinfo(skb)->nr_frags + 1);\n\t\terr = skb_to_sgvec(skb, dsg,\n\t\t\t           (unsigned char *)esph - skb->data,\n\t\t\t           assoclen + ivlen + esp->clen + alen);\n\t\tif (unlikely(err < 0))\n\t\t\tgoto error_free;\n\t}\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_output_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_output_done, skb);\n\n\taead_request_set_crypt(req, sg, dsg, ivlen + esp->clen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\tmemset(iv, 0, ivlen);\n\tmemcpy(iv + ivlen - min(ivlen, 8), (u8 *)&esp->seqno + 8 - min(ivlen, 8),\n\t       min(ivlen, 8));\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\terr = crypto_aead_encrypt(req);\n\n\tswitch (err) {\n\tcase -EINPROGRESS:\n\t\tgoto error;\n\n\tcase -ENOSPC:\n\t\terr = NET_XMIT_DROP;\n\t\tbreak;\n\n\tcase 0:\n\t\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\t\tesp_output_restore_header(skb);\n\t}\n\n\tif (sg != dsg)\n\t\tesp_ssg_unref(x, tmp);\n\n\tif (!err && x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\terr = esp_output_tail_tcp(x, skb);\n\nerror_free:\n\tkfree(tmp);\nerror:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp_output_tail);\n\nstatic int esp_output(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint alen;\n\tint blksize;\n\tstruct ip_esp_hdr *esph;\n\tstruct crypto_aead *aead;\n\tstruct esp_info esp;\n\n\tesp.inplace = true;\n\n\tesp.proto = *skb_mac_header(skb);\n\t*skb_mac_header(skb) = IPPROTO_ESP;\n\n\t/* skb is pure payload to encrypt */\n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\n\tesp.tfclen = 0;\n\tif (x->tfcpad) {\n\t\tstruct xfrm_dst *dst = (struct xfrm_dst *)skb_dst(skb);\n\t\tu32 padto;\n\n\t\tpadto = min(x->tfcpad, xfrm_state_mtu(x, dst->child_mtu_cached));\n\t\tif (skb->len < padto)\n\t\t\tesp.tfclen = padto - skb->len;\n\t}\n\tblksize = ALIGN(crypto_aead_blocksize(aead), 4);\n\tesp.clen = ALIGN(skb->len + 2 + esp.tfclen, blksize);\n\tesp.plen = esp.clen - skb->len - esp.tfclen;\n\tesp.tailen = esp.tfclen + esp.plen + alen;\n\n\tesp.esph = ip_esp_hdr(skb);\n\n\tesp.nfrags = esp_output_head(x, skb, &esp);\n\tif (esp.nfrags < 0)\n\t\treturn esp.nfrags;\n\n\tesph = esp.esph;\n\tesph->spi = x->id.spi;\n\n\tesph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);\n\tesp.seqno = cpu_to_be64(XFRM_SKB_CB(skb)->seq.output.low +\n\t\t\t\t ((u64)XFRM_SKB_CB(skb)->seq.output.hi << 32));\n\n\tskb_push(skb, -skb_network_offset(skb));\n\n\treturn esp_output_tail(x, skb, &esp);\n}\n\nstatic inline int esp_remove_trailer(struct sk_buff *skb)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint alen, hlen, elen;\n\tint padlen, trimlen;\n\t__wsum csumdiff;\n\tu8 nexthdr[2];\n\tint ret;\n\n\talen = crypto_aead_authsize(aead);\n\thlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\telen = skb->len - hlen;\n\n\tif (xo && (xo->flags & XFRM_ESP_NO_TRAILER)) {\n\t\tret = xo->proto;\n\t\tgoto out;\n\t}\n\n\tif (skb_copy_bits(skb, skb->len - alen - 2, nexthdr, 2))\n\t\tBUG();\n\n\tret = -EINVAL;\n\tpadlen = nexthdr[0];\n\tif (padlen + 2 + alen >= elen) {\n\t\tnet_dbg_ratelimited(\"ipsec esp packet is garbage padlen=%d, elen=%d\\n\",\n\t\t\t\t    padlen + 2, elen - alen);\n\t\tgoto out;\n\t}\n\n\ttrimlen = alen + padlen + 2;\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tcsumdiff = skb_checksum(skb, skb->len - trimlen, trimlen, 0);\n\t\tskb->csum = csum_block_sub(skb->csum, csumdiff,\n\t\t\t\t\t   skb->len - trimlen);\n\t}\n\tpskb_trim(skb, skb->len - trimlen);\n\n\tret = nexthdr[1];\n\nout:\n\treturn ret;\n}\n\nint esp_input_done2(struct sk_buff *skb, int err)\n{\n\tconst struct iphdr *iph;\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint hlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\tint ihl;\n\n\tif (!xo || !(xo->flags & CRYPTO_DONE))\n\t\tkfree(ESP_SKB_CB(skb)->tmp);\n\n\tif (unlikely(err))\n\t\tgoto out;\n\n\terr = esp_remove_trailer(skb);\n\tif (unlikely(err < 0))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tihl = iph->ihl * 4;\n\n\tif (x->encap) {\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\t\tstruct tcphdr *th = (void *)(skb_network_header(skb) + ihl);\n\t\tstruct udphdr *uh = (void *)(skb_network_header(skb) + ihl);\n\t\t__be16 source;\n\n\t\tswitch (x->encap->encap_type) {\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\tsource = th->source;\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tsource = uh->source;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * 1) if the NAT-T peer's IP or port changed then\n\t\t *    advertize the change to the keying daemon.\n\t\t *    This is an inbound SA, so just compare\n\t\t *    SRC ports.\n\t\t */\n\t\tif (iph->saddr != x->props.saddr.a4 ||\n\t\t    source != encap->encap_sport) {\n\t\t\txfrm_address_t ipaddr;\n\n\t\t\tipaddr.a4 = iph->saddr;\n\t\t\tkm_new_mapping(x, &ipaddr, source);\n\n\t\t\t/* XXX: perhaps add an extra\n\t\t\t * policy check here, to see\n\t\t\t * if we should allow or\n\t\t\t * reject a packet from a\n\t\t\t * different source\n\t\t\t * address/port.\n\t\t\t */\n\t\t}\n\n\t\t/*\n\t\t * 2) ignore UDP/TCP checksums in case\n\t\t *    of NAT-T in Transport Mode, or\n\t\t *    perform other post-processing fixes\n\t\t *    as per draft-ietf-ipsec-udp-encaps-06,\n\t\t *    section 3.1.2\n\t\t */\n\t\tif (x->props.mode == XFRM_MODE_TRANSPORT)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\n\tskb_pull_rcsum(skb, hlen);\n\tif (x->props.mode == XFRM_MODE_TUNNEL)\n\t\tskb_reset_transport_header(skb);\n\telse\n\t\tskb_set_transport_header(skb, -ihl);\n\n\t/* RFC4303: Drop dummy packets without any error */\n\tif (err == IPPROTO_NONE)\n\t\terr = -EINVAL;\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp_input_done2);\n\nstatic void esp_input_done(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\txfrm_input_resume(skb, esp_input_done2(skb, err));\n}\n\nstatic void esp_input_restore_header(struct sk_buff *skb)\n{\n\tesp_restore_header(skb, 0);\n\t__skb_pull(skb, 4);\n}\n\nstatic void esp_input_set_header(struct sk_buff *skb, __be32 *seqhi)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct ip_esp_hdr *esph;\n\n\t/* For ESN we move the header forward by 4 bytes to\n\t * accommodate the high bits.  We will move it back after\n\t * decryption.\n\t */\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tesph = skb_push(skb, 4);\n\t\t*seqhi = esph->spi;\n\t\tesph->spi = esph->seq_no;\n\t\tesph->seq_no = XFRM_SKB_CB(skb)->seq.input.hi;\n\t}\n}\n\nstatic void esp_input_done_esn(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\tesp_input_restore_header(skb);\n\tesp_input_done(base, err);\n}\n\n/*\n * Note: detecting truncated vs. non-truncated authentication data is very\n * expensive, so we only support truncated data, which is the recommended\n * and common case.\n */\nstatic int esp_input(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct crypto_aead *aead = x->data;\n\tstruct aead_request *req;\n\tstruct sk_buff *trailer;\n\tint ivlen = crypto_aead_ivsize(aead);\n\tint elen = skb->len - sizeof(struct ip_esp_hdr) - ivlen;\n\tint nfrags;\n\tint assoclen;\n\tint seqhilen;\n\t__be32 *seqhi;\n\tvoid *tmp;\n\tu8 *iv;\n\tstruct scatterlist *sg;\n\tint err = -EINVAL;\n\n\tif (!pskb_may_pull(skb, sizeof(struct ip_esp_hdr) + ivlen))\n\t\tgoto out;\n\n\tif (elen <= 0)\n\t\tgoto out;\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\tseqhilen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\tseqhilen += sizeof(__be32);\n\t\tassoclen += seqhilen;\n\t}\n\n\tif (!skb_cloned(skb)) {\n\t\tif (!skb_is_nonlinear(skb)) {\n\t\t\tnfrags = 1;\n\n\t\t\tgoto skip_cow;\n\t\t} else if (!skb_has_frag_list(skb)) {\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\t\t\tnfrags++;\n\n\t\t\tgoto skip_cow;\n\t\t}\n\t}\n\n\terr = skb_cow_data(skb, 0, &trailer);\n\tif (err < 0)\n\t\tgoto out;\n\n\tnfrags = err;\n\nskip_cow:\n\terr = -ENOMEM;\n\ttmp = esp_alloc_tmp(aead, nfrags, seqhilen);\n\tif (!tmp)\n\t\tgoto out;\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\tseqhi = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, seqhilen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tesp_input_set_header(skb, seqhi);\n\n\tsg_init_table(sg, nfrags);\n\terr = skb_to_sgvec(skb, sg, 0, skb->len);\n\tif (unlikely(err < 0)) {\n\t\tkfree(tmp);\n\t\tgoto out;\n\t}\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_input_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_input_done, skb);\n\n\taead_request_set_crypt(req, sg, sg, elen + ivlen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\terr = crypto_aead_decrypt(req);\n\tif (err == -EINPROGRESS)\n\t\tgoto out;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\tesp_input_restore_header(skb);\n\n\terr = esp_input_done2(skb, err);\n\nout:\n\treturn err;\n}\n\nstatic int esp4_err(struct sk_buff *skb, u32 info)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\tstruct ip_esp_hdr *esph = (struct ip_esp_hdr *)(skb->data+(iph->ihl<<2));\n\tstruct xfrm_state *x;\n\n\tswitch (icmp_hdr(skb)->type) {\n\tcase ICMP_DEST_UNREACH:\n\t\tif (icmp_hdr(skb)->code != ICMP_FRAG_NEEDED)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase ICMP_REDIRECT:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tx = xfrm_state_lookup(net, skb->mark, (const xfrm_address_t *)&iph->daddr,\n\t\t\t      esph->spi, IPPROTO_ESP, AF_INET);\n\tif (!x)\n\t\treturn 0;\n\n\tif (icmp_hdr(skb)->type == ICMP_DEST_UNREACH)\n\t\tipv4_update_pmtu(skb, net, info, 0, IPPROTO_ESP);\n\telse\n\t\tipv4_redirect(skb, net, 0, IPPROTO_ESP);\n\txfrm_state_put(x);\n\n\treturn 0;\n}\n\nstatic void esp_destroy(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead = x->data;\n\n\tif (!aead)\n\t\treturn;\n\n\tcrypto_free_aead(aead);\n}\n\nstatic int esp_init_aead(struct xfrm_state *x)\n{\n\tchar aead_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_aead *aead;\n\tint err;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(aead_name, CRYPTO_MAX_ALG_NAME, \"%s(%s)\",\n\t\t     x->geniv, x->aead->alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto error;\n\n\taead = crypto_alloc_aead(aead_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead))\n\t\tgoto error;\n\n\tx->data = aead;\n\n\terr = crypto_aead_setkey(aead, x->aead->alg_key,\n\t\t\t\t (x->aead->alg_key_len + 7) / 8);\n\tif (err)\n\t\tgoto error;\n\n\terr = crypto_aead_setauthsize(aead, x->aead->alg_icv_len / 8);\n\tif (err)\n\t\tgoto error;\n\nerror:\n\treturn err;\n}\n\nstatic int esp_init_authenc(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead;\n\tstruct crypto_authenc_key_param *param;\n\tstruct rtattr *rta;\n\tchar *key;\n\tchar *p;\n\tchar authenc_name[CRYPTO_MAX_ALG_NAME];\n\tunsigned int keylen;\n\tint err;\n\n\terr = -EINVAL;\n\tif (!x->ealg)\n\t\tgoto error;\n\n\terr = -ENAMETOOLONG;\n\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthencesn(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME)\n\t\t\tgoto error;\n\t} else {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthenc(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME)\n\t\t\tgoto error;\n\t}\n\n\taead = crypto_alloc_aead(authenc_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead))\n\t\tgoto error;\n\n\tx->data = aead;\n\n\tkeylen = (x->aalg ? (x->aalg->alg_key_len + 7) / 8 : 0) +\n\t\t (x->ealg->alg_key_len + 7) / 8 + RTA_SPACE(sizeof(*param));\n\terr = -ENOMEM;\n\tkey = kmalloc(keylen, GFP_KERNEL);\n\tif (!key)\n\t\tgoto error;\n\n\tp = key;\n\trta = (void *)p;\n\trta->rta_type = CRYPTO_AUTHENC_KEYA_PARAM;\n\trta->rta_len = RTA_LENGTH(sizeof(*param));\n\tparam = RTA_DATA(rta);\n\tp += RTA_SPACE(sizeof(*param));\n\n\tif (x->aalg) {\n\t\tstruct xfrm_algo_desc *aalg_desc;\n\n\t\tmemcpy(p, x->aalg->alg_key, (x->aalg->alg_key_len + 7) / 8);\n\t\tp += (x->aalg->alg_key_len + 7) / 8;\n\n\t\taalg_desc = xfrm_aalg_get_byname(x->aalg->alg_name, 0);\n\t\tBUG_ON(!aalg_desc);\n\n\t\terr = -EINVAL;\n\t\tif (aalg_desc->uinfo.auth.icv_fullbits / 8 !=\n\t\t    crypto_aead_authsize(aead)) {\n\t\t\tpr_info(\"ESP: %s digestsize %u != %hu\\n\",\n\t\t\t\tx->aalg->alg_name,\n\t\t\t\tcrypto_aead_authsize(aead),\n\t\t\t\taalg_desc->uinfo.auth.icv_fullbits / 8);\n\t\t\tgoto free_key;\n\t\t}\n\n\t\terr = crypto_aead_setauthsize(\n\t\t\taead, x->aalg->alg_trunc_len / 8);\n\t\tif (err)\n\t\t\tgoto free_key;\n\t}\n\n\tparam->enckeylen = cpu_to_be32((x->ealg->alg_key_len + 7) / 8);\n\tmemcpy(p, x->ealg->alg_key, (x->ealg->alg_key_len + 7) / 8);\n\n\terr = crypto_aead_setkey(aead, key, keylen);\n\nfree_key:\n\tkfree(key);\n\nerror:\n\treturn err;\n}\n\nstatic int esp_init_state(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead;\n\tu32 align;\n\tint err;\n\n\tx->data = NULL;\n\n\tif (x->aead)\n\t\terr = esp_init_aead(x);\n\telse\n\t\terr = esp_init_authenc(x);\n\n\tif (err)\n\t\tgoto error;\n\n\taead = x->data;\n\n\tx->props.header_len = sizeof(struct ip_esp_hdr) +\n\t\t\t      crypto_aead_ivsize(aead);\n\tif (x->props.mode == XFRM_MODE_TUNNEL)\n\t\tx->props.header_len += sizeof(struct iphdr);\n\telse if (x->props.mode == XFRM_MODE_BEET && x->sel.family != AF_INET6)\n\t\tx->props.header_len += IPV4_BEET_PHMAXLEN;\n\tif (x->encap) {\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\n\t\tswitch (encap->encap_type) {\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto error;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\t\tx->props.header_len += sizeof(struct udphdr);\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tx->props.header_len += sizeof(struct udphdr) + 2 * sizeof(u32);\n\t\t\tbreak;\n#ifdef CONFIG_INET_ESPINTCP\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\t/* only the length field, TCP encap is done by\n\t\t\t * the socket\n\t\t\t */\n\t\t\tx->props.header_len += 2;\n\t\t\tbreak;\n#endif\n\t\t}\n\t}\n\n\talign = ALIGN(crypto_aead_blocksize(aead), 4);\n\tx->props.trailer_len = align + 1 + crypto_aead_authsize(aead);\n\nerror:\n\treturn err;\n}\n\nstatic int esp4_rcv_cb(struct sk_buff *skb, int err)\n{\n\treturn 0;\n}\n\nstatic const struct xfrm_type esp_type =\n{\n\t.owner\t\t= THIS_MODULE,\n\t.proto\t     \t= IPPROTO_ESP,\n\t.flags\t\t= XFRM_TYPE_REPLAY_PROT,\n\t.init_state\t= esp_init_state,\n\t.destructor\t= esp_destroy,\n\t.input\t\t= esp_input,\n\t.output\t\t= esp_output,\n};\n\nstatic struct xfrm4_protocol esp4_protocol = {\n\t.handler\t=\txfrm4_rcv,\n\t.input_handler\t=\txfrm_input,\n\t.cb_handler\t=\tesp4_rcv_cb,\n\t.err_handler\t=\tesp4_err,\n\t.priority\t=\t0,\n};\n\nstatic int __init esp4_init(void)\n{\n\tif (xfrm_register_type(&esp_type, AF_INET) < 0) {\n\t\tpr_info(\"%s: can't add xfrm type\\n\", __func__);\n\t\treturn -EAGAIN;\n\t}\n\tif (xfrm4_protocol_register(&esp4_protocol, IPPROTO_ESP) < 0) {\n\t\tpr_info(\"%s: can't add protocol\\n\", __func__);\n\t\txfrm_unregister_type(&esp_type, AF_INET);\n\t\treturn -EAGAIN;\n\t}\n\treturn 0;\n}\n\nstatic void __exit esp4_fini(void)\n{\n\tif (xfrm4_protocol_deregister(&esp4_protocol, IPPROTO_ESP) < 0)\n\t\tpr_info(\"%s: can't remove protocol\\n\", __func__);\n\txfrm_unregister_type(&esp_type, AF_INET);\n}\n\nmodule_init(esp4_init);\nmodule_exit(esp4_fini);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_XFRM_TYPE(AF_INET, XFRM_PROTO_ESP);\n", "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Copyright (C)2002 USAGI/WIDE Project\n *\n * Authors\n *\n *\tMitsuru KANDA @USAGI       : IPv6 Support\n *\tKazunori MIYAZAWA @USAGI   :\n *\tKunihiro Ishiguro <kunihiro@ipinfusion.com>\n *\n *\tThis file is derived from net/ipv4/esp.c\n */\n\n#define pr_fmt(fmt) \"IPv6: \" fmt\n\n#include <crypto/aead.h>\n#include <crypto/authenc.h>\n#include <linux/err.h>\n#include <linux/module.h>\n#include <net/ip.h>\n#include <net/xfrm.h>\n#include <net/esp.h>\n#include <linux/scatterlist.h>\n#include <linux/kernel.h>\n#include <linux/pfkeyv2.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <net/ip6_checksum.h>\n#include <net/ip6_route.h>\n#include <net/icmp.h>\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/udp.h>\n#include <linux/icmpv6.h>\n#include <net/tcp.h>\n#include <net/espintcp.h>\n#include <net/inet6_hashtables.h>\n\n#include <linux/highmem.h>\n\nstruct esp_skb_cb {\n\tstruct xfrm_skb_cb xfrm;\n\tvoid *tmp;\n};\n\nstruct esp_output_extra {\n\t__be32 seqhi;\n\tu32 esphoff;\n};\n\n#define ESP_SKB_CB(__skb) ((struct esp_skb_cb *)&((__skb)->cb[0]))\n\n/*\n * Allocate an AEAD request structure with extra space for SG and IV.\n *\n * For alignment considerations the upper 32 bits of the sequence number are\n * placed at the front, if present. Followed by the IV, the request and finally\n * the SG list.\n *\n * TODO: Use spare space in skb for this where possible.\n */\nstatic void *esp_alloc_tmp(struct crypto_aead *aead, int nfrags, int seqihlen)\n{\n\tunsigned int len;\n\n\tlen = seqihlen;\n\n\tlen += crypto_aead_ivsize(aead);\n\n\tif (len) {\n\t\tlen += crypto_aead_alignmask(aead) &\n\t\t       ~(crypto_tfm_ctx_alignment() - 1);\n\t\tlen = ALIGN(len, crypto_tfm_ctx_alignment());\n\t}\n\n\tlen += sizeof(struct aead_request) + crypto_aead_reqsize(aead);\n\tlen = ALIGN(len, __alignof__(struct scatterlist));\n\n\tlen += sizeof(struct scatterlist) * nfrags;\n\n\treturn kmalloc(len, GFP_ATOMIC);\n}\n\nstatic inline void *esp_tmp_extra(void *tmp)\n{\n\treturn PTR_ALIGN(tmp, __alignof__(struct esp_output_extra));\n}\n\nstatic inline u8 *esp_tmp_iv(struct crypto_aead *aead, void *tmp, int seqhilen)\n{\n\treturn crypto_aead_ivsize(aead) ?\n\t       PTR_ALIGN((u8 *)tmp + seqhilen,\n\t\t\t crypto_aead_alignmask(aead) + 1) : tmp + seqhilen;\n}\n\nstatic inline struct aead_request *esp_tmp_req(struct crypto_aead *aead, u8 *iv)\n{\n\tstruct aead_request *req;\n\n\treq = (void *)PTR_ALIGN(iv + crypto_aead_ivsize(aead),\n\t\t\t\tcrypto_tfm_ctx_alignment());\n\taead_request_set_tfm(req, aead);\n\treturn req;\n}\n\nstatic inline struct scatterlist *esp_req_sg(struct crypto_aead *aead,\n\t\t\t\t\t     struct aead_request *req)\n{\n\treturn (void *)ALIGN((unsigned long)(req + 1) +\n\t\t\t     crypto_aead_reqsize(aead),\n\t\t\t     __alignof__(struct scatterlist));\n}\n\nstatic void esp_ssg_unref(struct xfrm_state *x, void *tmp)\n{\n\tstruct crypto_aead *aead = x->data;\n\tint extralen = 0;\n\tu8 *iv;\n\tstruct aead_request *req;\n\tstruct scatterlist *sg;\n\n\tif (x->props.flags & XFRM_STATE_ESN)\n\t\textralen += sizeof(struct esp_output_extra);\n\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\n\t/* Unref skb_frag_pages in the src scatterlist if necessary.\n\t * Skip the first sg which comes from skb->data.\n\t */\n\tif (req->src != req->dst)\n\t\tfor (sg = sg_next(req->src); sg; sg = sg_next(sg))\n\t\t\tput_page(sg_page(sg));\n}\n\n#ifdef CONFIG_INET6_ESPINTCP\nstruct esp_tcp_sk {\n\tstruct sock *sk;\n\tstruct rcu_head rcu;\n};\n\nstatic void esp_free_tcp_sk(struct rcu_head *head)\n{\n\tstruct esp_tcp_sk *esk = container_of(head, struct esp_tcp_sk, rcu);\n\n\tsock_put(esk->sk);\n\tkfree(esk);\n}\n\nstatic struct sock *esp6_find_tcp_sk(struct xfrm_state *x)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct esp_tcp_sk *esk;\n\t__be16 sport, dport;\n\tstruct sock *nsk;\n\tstruct sock *sk;\n\n\tsk = rcu_dereference(x->encap_sk);\n\tif (sk && sk->sk_state == TCP_ESTABLISHED)\n\t\treturn sk;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (sk && sk == nsk) {\n\t\tesk = kmalloc(sizeof(*esk), GFP_ATOMIC);\n\t\tif (!esk) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tRCU_INIT_POINTER(x->encap_sk, NULL);\n\t\tesk->sk = sk;\n\t\tcall_rcu(&esk->rcu, esp_free_tcp_sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\tsk = __inet6_lookup_established(xs_net(x), &tcp_hashinfo, &x->id.daddr.in6,\n\t\t\t\t\tdport, &x->props.saddr.in6, ntohs(sport), 0, 0);\n\tif (!sk)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (!tcp_is_ulp_esp(sk)) {\n\t\tsock_put(sk);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tspin_lock_bh(&x->lock);\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (encap->encap_sport != sport ||\n\t    encap->encap_dport != dport) {\n\t\tsock_put(sk);\n\t\tsk = nsk ?: ERR_PTR(-EREMCHG);\n\t} else if (sk == nsk) {\n\t\tsock_put(sk);\n\t} else {\n\t\trcu_assign_pointer(x->encap_sk, sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\treturn sk;\n}\n\nstatic int esp_output_tcp_finish(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tint err;\n\n\trcu_read_lock();\n\n\tsk = esp6_find_tcp_sk(x);\n\terr = PTR_ERR_OR_ZERO(sk);\n\tif (err)\n\t\tgoto out;\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\terr = espintcp_queue_out(sk, skb);\n\telse\n\t\terr = espintcp_push_skb(sk, skb);\n\tbh_unlock_sock(sk);\n\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic int esp_output_tcp_encap_cb(struct net *net, struct sock *sk,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct xfrm_state *x = dst->xfrm;\n\n\treturn esp_output_tcp_finish(x, skb);\n}\n\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint err;\n\n\tlocal_bh_disable();\n\terr = xfrm_trans_queue_net(xs_net(x), skb, esp_output_tcp_encap_cb);\n\tlocal_bh_enable();\n\n\t/* EINPROGRESS just happens to do the right thing.  It\n\t * actually means that the skb has been consumed and\n\t * isn't coming back.\n\t */\n\treturn err ?: -EINPROGRESS;\n}\n#else\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n\n\treturn -EOPNOTSUPP;\n}\n#endif\n\nstatic void esp_output_encap_csum(struct sk_buff *skb)\n{\n\t/* UDP encap with IPv6 requires a valid checksum */\n\tif (*skb_mac_header(skb) == IPPROTO_UDP) {\n\t\tstruct udphdr *uh = udp_hdr(skb);\n\t\tstruct ipv6hdr *ip6h = ipv6_hdr(skb);\n\t\tint len = ntohs(uh->len);\n\t\tunsigned int offset = skb_transport_offset(skb);\n\t\t__wsum csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\t\tuh->check = csum_ipv6_magic(&ip6h->saddr, &ip6h->daddr,\n\t\t\t\t\t    len, IPPROTO_UDP, csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\nstatic void esp_output_done(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tvoid *tmp;\n\tstruct xfrm_state *x;\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tstruct sec_path *sp = skb_sec_path(skb);\n\n\t\tx = sp->xvec[sp->len - 1];\n\t} else {\n\t\tx = skb_dst(skb)->xfrm;\n\t}\n\n\ttmp = ESP_SKB_CB(skb)->tmp;\n\tesp_ssg_unref(x, tmp);\n\tkfree(tmp);\n\n\tesp_output_encap_csum(skb);\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tif (err) {\n\t\t\tXFRM_INC_STATS(xs_net(x), LINUX_MIB_XFRMOUTSTATEPROTOERROR);\n\t\t\tkfree_skb(skb);\n\t\t\treturn;\n\t\t}\n\n\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\tsecpath_reset(skb);\n\t\txfrm_dev_resume(skb);\n\t} else {\n\t\tif (!err &&\n\t\t    x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\t\tesp_output_tail_tcp(x, skb);\n\t\telse\n\t\t\txfrm_output_resume(skb->sk, skb, err);\n\t}\n}\n\n/* Move ESP header back into place. */\nstatic void esp_restore_header(struct sk_buff *skb, unsigned int offset)\n{\n\tstruct ip_esp_hdr *esph = (void *)(skb->data + offset);\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\t__be32 *seqhi = esp_tmp_extra(tmp);\n\n\tesph->seq_no = esph->spi;\n\tesph->spi = *seqhi;\n}\n\nstatic void esp_output_restore_header(struct sk_buff *skb)\n{\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\tstruct esp_output_extra *extra = esp_tmp_extra(tmp);\n\n\tesp_restore_header(skb, skb_transport_offset(skb) + extra->esphoff -\n\t\t\t\tsizeof(__be32));\n}\n\nstatic struct ip_esp_hdr *esp_output_set_esn(struct sk_buff *skb,\n\t\t\t\t\t     struct xfrm_state *x,\n\t\t\t\t\t     struct ip_esp_hdr *esph,\n\t\t\t\t\t     struct esp_output_extra *extra)\n{\n\t/* For ESN we move the header forward by 4 bytes to\n\t * accomodate the high bits.  We will move it back after\n\t * encryption.\n\t */\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\t__u32 seqhi;\n\t\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\n\t\tif (xo)\n\t\t\tseqhi = xo->seq.hi;\n\t\telse\n\t\t\tseqhi = XFRM_SKB_CB(skb)->seq.output.hi;\n\n\t\textra->esphoff = (unsigned char *)esph -\n\t\t\t\t skb_transport_header(skb);\n\t\tesph = (struct ip_esp_hdr *)((unsigned char *)esph - 4);\n\t\textra->seqhi = esph->spi;\n\t\tesph->seq_no = htonl(seqhi);\n\t}\n\n\tesph->spi = x->id.spi;\n\n\treturn esph;\n}\n\nstatic void esp_output_done_esn(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\tesp_output_restore_header(skb);\n\tesp_output_done(base, err);\n}\n\nstatic struct ip_esp_hdr *esp6_output_udp_encap(struct sk_buff *skb,\n\t\t\t\t\t       int encap_type,\n\t\t\t\t\t       struct esp_info *esp,\n\t\t\t\t\t       __be16 sport,\n\t\t\t\t\t       __be16 dport)\n{\n\tstruct udphdr *uh;\n\t__be32 *udpdata32;\n\tunsigned int len;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len > U16_MAX)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tuh = (struct udphdr *)esp->esph;\n\tuh->source = sport;\n\tuh->dest = dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\t*skb_mac_header(skb) = IPPROTO_UDP;\n\n\tif (encap_type == UDP_ENCAP_ESPINUDP_NON_IKE) {\n\t\tudpdata32 = (__be32 *)(uh + 1);\n\t\tudpdata32[0] = udpdata32[1] = 0;\n\t\treturn (struct ip_esp_hdr *)(udpdata32 + 2);\n\t}\n\n\treturn (struct ip_esp_hdr *)(uh + 1);\n}\n\n#ifdef CONFIG_INET6_ESPINTCP\nstatic struct ip_esp_hdr *esp6_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\tstruct sk_buff *skb,\n\t\t\t\t\t\tstruct esp_info *esp)\n{\n\t__be16 *lenp = (void *)esp->esph;\n\tstruct ip_esp_hdr *esph;\n\tunsigned int len;\n\tstruct sock *sk;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len > IP_MAX_MTU)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\trcu_read_lock();\n\tsk = esp6_find_tcp_sk(x);\n\trcu_read_unlock();\n\n\tif (IS_ERR(sk))\n\t\treturn ERR_CAST(sk);\n\n\t*lenp = htons(len);\n\tesph = (struct ip_esp_hdr *)(lenp + 1);\n\n\treturn esph;\n}\n#else\nstatic struct ip_esp_hdr *esp6_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\tstruct sk_buff *skb,\n\t\t\t\t\t\tstruct esp_info *esp)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n#endif\n\nstatic int esp6_output_encap(struct xfrm_state *x, struct sk_buff *skb,\n\t\t\t    struct esp_info *esp)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct ip_esp_hdr *esph;\n\t__be16 sport, dport;\n\tint encap_type;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tencap_type = encap->encap_type;\n\tspin_unlock_bh(&x->lock);\n\n\tswitch (encap_type) {\n\tdefault:\n\tcase UDP_ENCAP_ESPINUDP:\n\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\tesph = esp6_output_udp_encap(skb, encap_type, esp, sport, dport);\n\t\tbreak;\n\tcase TCP_ENCAP_ESPINTCP:\n\t\tesph = esp6_output_tcp_encap(x, skb, esp);\n\t\tbreak;\n\t}\n\n\tif (IS_ERR(esph))\n\t\treturn PTR_ERR(esph);\n\n\tesp->esph = esph;\n\n\treturn 0;\n}\n\nint esp6_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *tail;\n\tint nfrags;\n\tint esph_offset;\n\tstruct page *page;\n\tstruct sk_buff *trailer;\n\tint tailen = esp->tailen;\n\tunsigned int allocsz;\n\n\tif (x->encap) {\n\t\tint err = esp6_output_encap(x, skb, esp);\n\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tallocsz = ALIGN(skb->data_len + tailen, L1_CACHE_BYTES);\n\tif (allocsz > ESP_SKB_FRAG_MAXSIZE)\n\t\tgoto cow;\n\n\tif (!skb_cloned(skb)) {\n\t\tif (tailen <= skb_tailroom(skb)) {\n\t\t\tnfrags = 1;\n\t\t\ttrailer = skb;\n\t\t\ttail = skb_tail_pointer(trailer);\n\n\t\t\tgoto skip_cow;\n\t\t} else if ((skb_shinfo(skb)->nr_frags < MAX_SKB_FRAGS)\n\t\t\t   && !skb_has_frag_list(skb)) {\n\t\t\tint allocsize;\n\t\t\tstruct sock *sk = skb->sk;\n\t\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\t\tesp->inplace = false;\n\n\t\t\tallocsize = ALIGN(tailen, L1_CACHE_BYTES);\n\n\t\t\tspin_lock_bh(&x->lock);\n\n\t\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\t\tspin_unlock_bh(&x->lock);\n\t\t\t\tgoto cow;\n\t\t\t}\n\n\t\t\tpage = pfrag->page;\n\t\t\tget_page(page);\n\n\t\t\ttail = page_address(page) + pfrag->offset;\n\n\t\t\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\n\t\t\t__skb_fill_page_desc(skb, nfrags, page, pfrag->offset,\n\t\t\t\t\t     tailen);\n\t\t\tskb_shinfo(skb)->nr_frags = ++nfrags;\n\n\t\t\tpfrag->offset = pfrag->offset + allocsize;\n\n\t\t\tspin_unlock_bh(&x->lock);\n\n\t\t\tnfrags++;\n\n\t\t\tskb->len += tailen;\n\t\t\tskb->data_len += tailen;\n\t\t\tskb->truesize += tailen;\n\t\t\tif (sk && sk_fullsock(sk))\n\t\t\t\trefcount_add(tailen, &sk->sk_wmem_alloc);\n\n\t\t\tgoto out;\n\t\t}\n\t}\n\ncow:\n\tesph_offset = (unsigned char *)esp->esph - skb_transport_header(skb);\n\n\tnfrags = skb_cow_data(skb, tailen, &trailer);\n\tif (nfrags < 0)\n\t\tgoto out;\n\ttail = skb_tail_pointer(trailer);\n\tesp->esph = (struct ip_esp_hdr *)(skb_transport_header(skb) + esph_offset);\n\nskip_cow:\n\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\tpskb_put(skb, trailer, tailen);\n\nout:\n\treturn nfrags;\n}\nEXPORT_SYMBOL_GPL(esp6_output_head);\n\nint esp6_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *iv;\n\tint alen;\n\tvoid *tmp;\n\tint ivlen;\n\tint assoclen;\n\tint extralen;\n\tstruct page *page;\n\tstruct ip_esp_hdr *esph;\n\tstruct aead_request *req;\n\tstruct crypto_aead *aead;\n\tstruct scatterlist *sg, *dsg;\n\tstruct esp_output_extra *extra;\n\tint err = -ENOMEM;\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\textralen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\textralen += sizeof(*extra);\n\t\tassoclen += sizeof(__be32);\n\t}\n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\tivlen = crypto_aead_ivsize(aead);\n\n\ttmp = esp_alloc_tmp(aead, esp->nfrags + 2, extralen);\n\tif (!tmp)\n\t\tgoto error;\n\n\textra = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tif (esp->inplace)\n\t\tdsg = sg;\n\telse\n\t\tdsg = &sg[esp->nfrags];\n\n\tesph = esp_output_set_esn(skb, x, esp->esph, extra);\n\tesp->esph = esph;\n\n\tsg_init_table(sg, esp->nfrags);\n\terr = skb_to_sgvec(skb, sg,\n\t\t           (unsigned char *)esph - skb->data,\n\t\t           assoclen + ivlen + esp->clen + alen);\n\tif (unlikely(err < 0))\n\t\tgoto error_free;\n\n\tif (!esp->inplace) {\n\t\tint allocsize;\n\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\tallocsize = ALIGN(skb->data_len, L1_CACHE_BYTES);\n\n\t\tspin_lock_bh(&x->lock);\n\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\tgoto error_free;\n\t\t}\n\n\t\tskb_shinfo(skb)->nr_frags = 1;\n\n\t\tpage = pfrag->page;\n\t\tget_page(page);\n\t\t/* replace page frags in skb with new page */\n\t\t__skb_fill_page_desc(skb, 0, page, pfrag->offset, skb->data_len);\n\t\tpfrag->offset = pfrag->offset + allocsize;\n\t\tspin_unlock_bh(&x->lock);\n\n\t\tsg_init_table(dsg, skb_shinfo(skb)->nr_frags + 1);\n\t\terr = skb_to_sgvec(skb, dsg,\n\t\t\t           (unsigned char *)esph - skb->data,\n\t\t\t           assoclen + ivlen + esp->clen + alen);\n\t\tif (unlikely(err < 0))\n\t\t\tgoto error_free;\n\t}\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_output_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_output_done, skb);\n\n\taead_request_set_crypt(req, sg, dsg, ivlen + esp->clen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\tmemset(iv, 0, ivlen);\n\tmemcpy(iv + ivlen - min(ivlen, 8), (u8 *)&esp->seqno + 8 - min(ivlen, 8),\n\t       min(ivlen, 8));\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\terr = crypto_aead_encrypt(req);\n\n\tswitch (err) {\n\tcase -EINPROGRESS:\n\t\tgoto error;\n\n\tcase -ENOSPC:\n\t\terr = NET_XMIT_DROP;\n\t\tbreak;\n\n\tcase 0:\n\t\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\t\tesp_output_restore_header(skb);\n\t\tesp_output_encap_csum(skb);\n\t}\n\n\tif (sg != dsg)\n\t\tesp_ssg_unref(x, tmp);\n\n\tif (!err && x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\terr = esp_output_tail_tcp(x, skb);\n\nerror_free:\n\tkfree(tmp);\nerror:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp6_output_tail);\n\nstatic int esp6_output(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint alen;\n\tint blksize;\n\tstruct ip_esp_hdr *esph;\n\tstruct crypto_aead *aead;\n\tstruct esp_info esp;\n\n\tesp.inplace = true;\n\n\tesp.proto = *skb_mac_header(skb);\n\t*skb_mac_header(skb) = IPPROTO_ESP;\n\n\t/* skb is pure payload to encrypt */\n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\n\tesp.tfclen = 0;\n\tif (x->tfcpad) {\n\t\tstruct xfrm_dst *dst = (struct xfrm_dst *)skb_dst(skb);\n\t\tu32 padto;\n\n\t\tpadto = min(x->tfcpad, xfrm_state_mtu(x, dst->child_mtu_cached));\n\t\tif (skb->len < padto)\n\t\t\tesp.tfclen = padto - skb->len;\n\t}\n\tblksize = ALIGN(crypto_aead_blocksize(aead), 4);\n\tesp.clen = ALIGN(skb->len + 2 + esp.tfclen, blksize);\n\tesp.plen = esp.clen - skb->len - esp.tfclen;\n\tesp.tailen = esp.tfclen + esp.plen + alen;\n\n\tesp.esph = ip_esp_hdr(skb);\n\n\tesp.nfrags = esp6_output_head(x, skb, &esp);\n\tif (esp.nfrags < 0)\n\t\treturn esp.nfrags;\n\n\tesph = esp.esph;\n\tesph->spi = x->id.spi;\n\n\tesph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);\n\tesp.seqno = cpu_to_be64(XFRM_SKB_CB(skb)->seq.output.low +\n\t\t\t    ((u64)XFRM_SKB_CB(skb)->seq.output.hi << 32));\n\n\tskb_push(skb, -skb_network_offset(skb));\n\n\treturn esp6_output_tail(x, skb, &esp);\n}\n\nstatic inline int esp_remove_trailer(struct sk_buff *skb)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint alen, hlen, elen;\n\tint padlen, trimlen;\n\t__wsum csumdiff;\n\tu8 nexthdr[2];\n\tint ret;\n\n\talen = crypto_aead_authsize(aead);\n\thlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\telen = skb->len - hlen;\n\n\tif (xo && (xo->flags & XFRM_ESP_NO_TRAILER)) {\n\t\tret = xo->proto;\n\t\tgoto out;\n\t}\n\n\tret = skb_copy_bits(skb, skb->len - alen - 2, nexthdr, 2);\n\tBUG_ON(ret);\n\n\tret = -EINVAL;\n\tpadlen = nexthdr[0];\n\tif (padlen + 2 + alen >= elen) {\n\t\tnet_dbg_ratelimited(\"ipsec esp packet is garbage padlen=%d, elen=%d\\n\",\n\t\t\t\t    padlen + 2, elen - alen);\n\t\tgoto out;\n\t}\n\n\ttrimlen = alen + padlen + 2;\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tcsumdiff = skb_checksum(skb, skb->len - trimlen, trimlen, 0);\n\t\tskb->csum = csum_block_sub(skb->csum, csumdiff,\n\t\t\t\t\t   skb->len - trimlen);\n\t}\n\tpskb_trim(skb, skb->len - trimlen);\n\n\tret = nexthdr[1];\n\nout:\n\treturn ret;\n}\n\nint esp6_input_done2(struct sk_buff *skb, int err)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint hlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\tint hdr_len = skb_network_header_len(skb);\n\n\tif (!xo || !(xo->flags & CRYPTO_DONE))\n\t\tkfree(ESP_SKB_CB(skb)->tmp);\n\n\tif (unlikely(err))\n\t\tgoto out;\n\n\terr = esp_remove_trailer(skb);\n\tif (unlikely(err < 0))\n\t\tgoto out;\n\n\tif (x->encap) {\n\t\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\t\tint offset = skb_network_offset(skb) + sizeof(*ip6h);\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\t\tu8 nexthdr = ip6h->nexthdr;\n\t\t__be16 frag_off, source;\n\t\tstruct udphdr *uh;\n\t\tstruct tcphdr *th;\n\n\t\toffset = ipv6_skip_exthdr(skb, offset, &nexthdr, &frag_off);\n\n\t\tif (offset < 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tuh = (void *)(skb->data + offset);\n\t\tth = (void *)(skb->data + offset);\n\t\thdr_len += offset;\n\n\t\tswitch (x->encap->encap_type) {\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\tsource = th->source;\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tsource = uh->source;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * 1) if the NAT-T peer's IP or port changed then\n\t\t *    advertize the change to the keying daemon.\n\t\t *    This is an inbound SA, so just compare\n\t\t *    SRC ports.\n\t\t */\n\t\tif (!ipv6_addr_equal(&ip6h->saddr, &x->props.saddr.in6) ||\n\t\t    source != encap->encap_sport) {\n\t\t\txfrm_address_t ipaddr;\n\n\t\t\tmemcpy(&ipaddr.a6, &ip6h->saddr.s6_addr, sizeof(ipaddr.a6));\n\t\t\tkm_new_mapping(x, &ipaddr, source);\n\n\t\t\t/* XXX: perhaps add an extra\n\t\t\t * policy check here, to see\n\t\t\t * if we should allow or\n\t\t\t * reject a packet from a\n\t\t\t * different source\n\t\t\t * address/port.\n\t\t\t */\n\t\t}\n\n\t\t/*\n\t\t * 2) ignore UDP/TCP checksums in case\n\t\t *    of NAT-T in Transport Mode, or\n\t\t *    perform other post-processing fixes\n\t\t *    as per draft-ietf-ipsec-udp-encaps-06,\n\t\t *    section 3.1.2\n\t\t */\n\t\tif (x->props.mode == XFRM_MODE_TRANSPORT)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\n\tskb_postpull_rcsum(skb, skb_network_header(skb),\n\t\t\t   skb_network_header_len(skb));\n\tskb_pull_rcsum(skb, hlen);\n\tif (x->props.mode == XFRM_MODE_TUNNEL)\n\t\tskb_reset_transport_header(skb);\n\telse\n\t\tskb_set_transport_header(skb, -hdr_len);\n\n\t/* RFC4303: Drop dummy packets without any error */\n\tif (err == IPPROTO_NONE)\n\t\terr = -EINVAL;\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp6_input_done2);\n\nstatic void esp_input_done(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\txfrm_input_resume(skb, esp6_input_done2(skb, err));\n}\n\nstatic void esp_input_restore_header(struct sk_buff *skb)\n{\n\tesp_restore_header(skb, 0);\n\t__skb_pull(skb, 4);\n}\n\nstatic void esp_input_set_header(struct sk_buff *skb, __be32 *seqhi)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\n\t/* For ESN we move the header forward by 4 bytes to\n\t * accomodate the high bits.  We will move it back after\n\t * decryption.\n\t */\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tstruct ip_esp_hdr *esph = skb_push(skb, 4);\n\n\t\t*seqhi = esph->spi;\n\t\tesph->spi = esph->seq_no;\n\t\tesph->seq_no = XFRM_SKB_CB(skb)->seq.input.hi;\n\t}\n}\n\nstatic void esp_input_done_esn(struct crypto_async_request *base, int err)\n{\n\tstruct sk_buff *skb = base->data;\n\n\tesp_input_restore_header(skb);\n\tesp_input_done(base, err);\n}\n\nstatic int esp6_input(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct crypto_aead *aead = x->data;\n\tstruct aead_request *req;\n\tstruct sk_buff *trailer;\n\tint ivlen = crypto_aead_ivsize(aead);\n\tint elen = skb->len - sizeof(struct ip_esp_hdr) - ivlen;\n\tint nfrags;\n\tint assoclen;\n\tint seqhilen;\n\tint ret = 0;\n\tvoid *tmp;\n\t__be32 *seqhi;\n\tu8 *iv;\n\tstruct scatterlist *sg;\n\n\tif (!pskb_may_pull(skb, sizeof(struct ip_esp_hdr) + ivlen)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (elen <= 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\tseqhilen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\tseqhilen += sizeof(__be32);\n\t\tassoclen += seqhilen;\n\t}\n\n\tif (!skb_cloned(skb)) {\n\t\tif (!skb_is_nonlinear(skb)) {\n\t\t\tnfrags = 1;\n\n\t\t\tgoto skip_cow;\n\t\t} else if (!skb_has_frag_list(skb)) {\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\t\t\tnfrags++;\n\n\t\t\tgoto skip_cow;\n\t\t}\n\t}\n\n\tnfrags = skb_cow_data(skb, 0, &trailer);\n\tif (nfrags < 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\nskip_cow:\n\tret = -ENOMEM;\n\ttmp = esp_alloc_tmp(aead, nfrags, seqhilen);\n\tif (!tmp)\n\t\tgoto out;\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\tseqhi = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, seqhilen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tesp_input_set_header(skb, seqhi);\n\n\tsg_init_table(sg, nfrags);\n\tret = skb_to_sgvec(skb, sg, 0, skb->len);\n\tif (unlikely(ret < 0)) {\n\t\tkfree(tmp);\n\t\tgoto out;\n\t}\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_input_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_input_done, skb);\n\n\taead_request_set_crypt(req, sg, sg, elen + ivlen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\tret = crypto_aead_decrypt(req);\n\tif (ret == -EINPROGRESS)\n\t\tgoto out;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\tesp_input_restore_header(skb);\n\n\tret = esp6_input_done2(skb, ret);\n\nout:\n\treturn ret;\n}\n\nstatic int esp6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t    u8 type, u8 code, int offset, __be32 info)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct ipv6hdr *iph = (const struct ipv6hdr *)skb->data;\n\tstruct ip_esp_hdr *esph = (struct ip_esp_hdr *)(skb->data + offset);\n\tstruct xfrm_state *x;\n\n\tif (type != ICMPV6_PKT_TOOBIG &&\n\t    type != NDISC_REDIRECT)\n\t\treturn 0;\n\n\tx = xfrm_state_lookup(net, skb->mark, (const xfrm_address_t *)&iph->daddr,\n\t\t\t      esph->spi, IPPROTO_ESP, AF_INET6);\n\tif (!x)\n\t\treturn 0;\n\n\tif (type == NDISC_REDIRECT)\n\t\tip6_redirect(skb, net, skb->dev->ifindex, 0,\n\t\t\t     sock_net_uid(net, NULL));\n\telse\n\t\tip6_update_pmtu(skb, net, info, 0, 0, sock_net_uid(net, NULL));\n\txfrm_state_put(x);\n\n\treturn 0;\n}\n\nstatic void esp6_destroy(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead = x->data;\n\n\tif (!aead)\n\t\treturn;\n\n\tcrypto_free_aead(aead);\n}\n\nstatic int esp_init_aead(struct xfrm_state *x)\n{\n\tchar aead_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_aead *aead;\n\tint err;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(aead_name, CRYPTO_MAX_ALG_NAME, \"%s(%s)\",\n\t\t     x->geniv, x->aead->alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto error;\n\n\taead = crypto_alloc_aead(aead_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead))\n\t\tgoto error;\n\n\tx->data = aead;\n\n\terr = crypto_aead_setkey(aead, x->aead->alg_key,\n\t\t\t\t (x->aead->alg_key_len + 7) / 8);\n\tif (err)\n\t\tgoto error;\n\n\terr = crypto_aead_setauthsize(aead, x->aead->alg_icv_len / 8);\n\tif (err)\n\t\tgoto error;\n\nerror:\n\treturn err;\n}\n\nstatic int esp_init_authenc(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead;\n\tstruct crypto_authenc_key_param *param;\n\tstruct rtattr *rta;\n\tchar *key;\n\tchar *p;\n\tchar authenc_name[CRYPTO_MAX_ALG_NAME];\n\tunsigned int keylen;\n\tint err;\n\n\terr = -EINVAL;\n\tif (!x->ealg)\n\t\tgoto error;\n\n\terr = -ENAMETOOLONG;\n\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthencesn(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME)\n\t\t\tgoto error;\n\t} else {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthenc(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME)\n\t\t\tgoto error;\n\t}\n\n\taead = crypto_alloc_aead(authenc_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead))\n\t\tgoto error;\n\n\tx->data = aead;\n\n\tkeylen = (x->aalg ? (x->aalg->alg_key_len + 7) / 8 : 0) +\n\t\t (x->ealg->alg_key_len + 7) / 8 + RTA_SPACE(sizeof(*param));\n\terr = -ENOMEM;\n\tkey = kmalloc(keylen, GFP_KERNEL);\n\tif (!key)\n\t\tgoto error;\n\n\tp = key;\n\trta = (void *)p;\n\trta->rta_type = CRYPTO_AUTHENC_KEYA_PARAM;\n\trta->rta_len = RTA_LENGTH(sizeof(*param));\n\tparam = RTA_DATA(rta);\n\tp += RTA_SPACE(sizeof(*param));\n\n\tif (x->aalg) {\n\t\tstruct xfrm_algo_desc *aalg_desc;\n\n\t\tmemcpy(p, x->aalg->alg_key, (x->aalg->alg_key_len + 7) / 8);\n\t\tp += (x->aalg->alg_key_len + 7) / 8;\n\n\t\taalg_desc = xfrm_aalg_get_byname(x->aalg->alg_name, 0);\n\t\tBUG_ON(!aalg_desc);\n\n\t\terr = -EINVAL;\n\t\tif (aalg_desc->uinfo.auth.icv_fullbits / 8 !=\n\t\t    crypto_aead_authsize(aead)) {\n\t\t\tpr_info(\"ESP: %s digestsize %u != %u\\n\",\n\t\t\t\tx->aalg->alg_name,\n\t\t\t\tcrypto_aead_authsize(aead),\n\t\t\t\taalg_desc->uinfo.auth.icv_fullbits / 8);\n\t\t\tgoto free_key;\n\t\t}\n\n\t\terr = crypto_aead_setauthsize(\n\t\t\taead, x->aalg->alg_trunc_len / 8);\n\t\tif (err)\n\t\t\tgoto free_key;\n\t}\n\n\tparam->enckeylen = cpu_to_be32((x->ealg->alg_key_len + 7) / 8);\n\tmemcpy(p, x->ealg->alg_key, (x->ealg->alg_key_len + 7) / 8);\n\n\terr = crypto_aead_setkey(aead, key, keylen);\n\nfree_key:\n\tkfree(key);\n\nerror:\n\treturn err;\n}\n\nstatic int esp6_init_state(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead;\n\tu32 align;\n\tint err;\n\n\tx->data = NULL;\n\n\tif (x->aead)\n\t\terr = esp_init_aead(x);\n\telse\n\t\terr = esp_init_authenc(x);\n\n\tif (err)\n\t\tgoto error;\n\n\taead = x->data;\n\n\tx->props.header_len = sizeof(struct ip_esp_hdr) +\n\t\t\t      crypto_aead_ivsize(aead);\n\tswitch (x->props.mode) {\n\tcase XFRM_MODE_BEET:\n\t\tif (x->sel.family != AF_INET6)\n\t\t\tx->props.header_len += IPV4_BEET_PHMAXLEN +\n\t\t\t\t\t       (sizeof(struct ipv6hdr) - sizeof(struct iphdr));\n\t\tbreak;\n\tdefault:\n\tcase XFRM_MODE_TRANSPORT:\n\t\tbreak;\n\tcase XFRM_MODE_TUNNEL:\n\t\tx->props.header_len += sizeof(struct ipv6hdr);\n\t\tbreak;\n\t}\n\n\tif (x->encap) {\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\n\t\tswitch (encap->encap_type) {\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto error;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\t\tx->props.header_len += sizeof(struct udphdr);\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tx->props.header_len += sizeof(struct udphdr) + 2 * sizeof(u32);\n\t\t\tbreak;\n#ifdef CONFIG_INET6_ESPINTCP\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\t/* only the length field, TCP encap is done by\n\t\t\t * the socket\n\t\t\t */\n\t\t\tx->props.header_len += 2;\n\t\t\tbreak;\n#endif\n\t\t}\n\t}\n\n\talign = ALIGN(crypto_aead_blocksize(aead), 4);\n\tx->props.trailer_len = align + 1 + crypto_aead_authsize(aead);\n\nerror:\n\treturn err;\n}\n\nstatic int esp6_rcv_cb(struct sk_buff *skb, int err)\n{\n\treturn 0;\n}\n\nstatic const struct xfrm_type esp6_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.proto\t\t= IPPROTO_ESP,\n\t.flags\t\t= XFRM_TYPE_REPLAY_PROT,\n\t.init_state\t= esp6_init_state,\n\t.destructor\t= esp6_destroy,\n\t.input\t\t= esp6_input,\n\t.output\t\t= esp6_output,\n};\n\nstatic struct xfrm6_protocol esp6_protocol = {\n\t.handler\t=\txfrm6_rcv,\n\t.input_handler\t=\txfrm_input,\n\t.cb_handler\t=\tesp6_rcv_cb,\n\t.err_handler\t=\tesp6_err,\n\t.priority\t=\t0,\n};\n\nstatic int __init esp6_init(void)\n{\n\tif (xfrm_register_type(&esp6_type, AF_INET6) < 0) {\n\t\tpr_info(\"%s: can't add xfrm type\\n\", __func__);\n\t\treturn -EAGAIN;\n\t}\n\tif (xfrm6_protocol_register(&esp6_protocol, IPPROTO_ESP) < 0) {\n\t\tpr_info(\"%s: can't add protocol\\n\", __func__);\n\t\txfrm_unregister_type(&esp6_type, AF_INET6);\n\t\treturn -EAGAIN;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit esp6_fini(void)\n{\n\tif (xfrm6_protocol_deregister(&esp6_protocol, IPPROTO_ESP) < 0)\n\t\tpr_info(\"%s: can't remove protocol\\n\", __func__);\n\txfrm_unregister_type(&esp6_type, AF_INET6);\n}\n\nmodule_init(esp6_init);\nmodule_exit(esp6_fini);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_XFRM_TYPE(AF_INET6, XFRM_PROTO_ESP);\n"], "filenames": ["include/net/esp.h", "net/ipv4/esp4.c", "net/ipv6/esp6.c"], "buggy_code_start_loc": [5, 448, 484], "buggy_code_end_loc": [5, 456, 491], "fixing_code_start_loc": [6, 449, 485], "fixing_code_end_loc": [8, 462, 497], "type": "CWE-787", "message": "A heap buffer overflow flaw was found in IPsec ESP transformation code in net/ipv4/esp4.c and net/ipv6/esp6.c. This flaw allows a local attacker with a normal user privilege to overwrite kernel heap objects and may cause a local privilege escalation threat.", "other": {"cve": {"id": "CVE-2022-27666", "sourceIdentifier": "cve@mitre.org", "published": "2022-03-23T06:15:06.717", "lastModified": "2023-02-01T14:32:53.840", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A heap buffer overflow flaw was found in IPsec ESP transformation code in net/ipv4/esp4.c and net/ipv6/esp6.c. This flaw allows a local attacker with a normal user privilege to overwrite kernel heap objects and may cause a local privilege escalation threat."}, {"lang": "es", "value": "Se ha encontrado un fallo de desbordamiento del b\u00fafer de la pila en el c\u00f3digo de transformaci\u00f3n de IPsec ESP en net/ipv4/esp4.c y net/ipv6/esp6.c. Este fallo permite a un atacante local con un privilegio de usuario normal sobrescribir los objetos de la pila del n\u00facleo y puede causar una amenaza de escalada de privilegios local"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.17", "matchCriteriaId": "A37A8EE9-3F14-4C7A-A882-DA8A6AD1897C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.17:-:*:*:*:*:*:*", "matchCriteriaId": "A59F7FD3-F505-48BD-8875-F07A33F42F6C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.17:rc1:*:*:*:*:*:*", "matchCriteriaId": "7BD5F8D9-54FA-4CB0-B4F0-CB0471FDDB2D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.17:rc2:*:*:*:*:*:*", "matchCriteriaId": "E6E34B23-78B4-4516-9BD8-61B33F4AC49A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.17:rc3:*:*:*:*:*:*", "matchCriteriaId": "C030FA3D-03F4-4FB9-9DBF-D08E5CAC51AA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.17:rc4:*:*:*:*:*:*", "matchCriteriaId": "B2D2677C-5389-4AE9-869D-0F881E80D923"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.17:rc5:*:*:*:*:*:*", "matchCriteriaId": "EFA3917C-C322-4D92-912D-ECE45B2E7416"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.17:rc6:*:*:*:*:*:*", "matchCriteriaId": "BED18363-5ABC-4639-8BBA-68E771E5BB3F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.17:rc7:*:*:*:*:*:*", "matchCriteriaId": "7F635F96-FA0A-4769-ADE8-232B3AC9116D"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:34:*:*:*:*:*:*:*", "matchCriteriaId": "A930E247-0B43-43CB-98FF-6CE7B8189835"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:35:*:*:*:*:*:*:*", "matchCriteriaId": "80E516C0-98A4-4ADE-B69F-66A772E2BAAA"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:virtualization:4.0:*:*:*:*:*:*:*", "matchCriteriaId": "6BBD7A51-0590-4DDF-8249-5AFA8D645CB6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "6770B6C3-732E-4E22-BF1C-2D2FD610061C"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300s:-:*:*:*:*:*:*:*", "matchCriteriaId": "9F9C8C20-42EB-4AB5-BD97-212DEB070C43"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "7FFF7106-ED78-49BA-9EC5-B889E3685D53"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500s:-:*:*:*:*:*:*:*", "matchCriteriaId": "E63D8B0F-006E-4801-BF9D-1C001BBFB4F9"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "56409CEC-5A1E-4450-AA42-641E459CC2AF"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700s:-:*:*:*:*:*:*:*", "matchCriteriaId": "B06F4839-D16A-4A61-9BB5-55B13F41E47F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "108A2215-50FB-4074-94CF-C130FA14566D"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300e:-:*:*:*:*:*:*:*", "matchCriteriaId": "7AFC73CE-ABB9-42D3-9A71-3F5BC5381E0E"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "32F0B6C0-F930-480D-962B-3F4EFDCC13C7"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500e:-:*:*:*:*:*:*:*", "matchCriteriaId": "803BC414-B250-4E3A-A478-A3881340D6B8"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "0FEB3337-BFDE-462A-908B-176F92053CEC"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700e:-:*:*:*:*:*:*:*", "matchCriteriaId": "736AEAE9-782B-4F71-9893-DED53367E102"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "D0B4AD8A-F172-4558-AEC6-FF424BA2D912"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410s:-:*:*:*:*:*:*:*", "matchCriteriaId": "8497A4C9-8474-4A62-8331-3FE862ED4098"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410c_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "234DEFE0-5CE5-4B0A-96B8-5D227CB8ED31"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410c:-:*:*:*:*:*:*:*", "matchCriteriaId": "CDDF61B7-EC5C-467C-B710-B89F502CD04F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:11.0:*:*:*:*:*:*:*", "matchCriteriaId": "FA6FEEC2-9F11-4643-8827-749718254FED"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2061633", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/ebe48d368e97d007bfeb76fcb065d6cfc4c96645", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20220429-0001/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2022/dsa-5127", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2022/dsa-5173", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/ebe48d368e97d007bfeb76fcb065d6cfc4c96645"}}