{"buggy_code": ["/*\n * fs/f2fs/super.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/statfs.h>\n#include <linux/buffer_head.h>\n#include <linux/backing-dev.h>\n#include <linux/kthread.h>\n#include <linux/parser.h>\n#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/random.h>\n#include <linux/exportfs.h>\n#include <linux/blkdev.h>\n#include <linux/f2fs_fs.h>\n#include <linux/sysfs.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include \"xattr.h\"\n#include \"gc.h\"\n#include \"trace.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/f2fs.h>\n\nstatic struct proc_dir_entry *f2fs_proc_root;\nstatic struct kmem_cache *f2fs_inode_cachep;\nstatic struct kset *f2fs_kset;\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\nchar *fault_name[FAULT_MAX] = {\n\t[FAULT_KMALLOC]\t\t= \"kmalloc\",\n\t[FAULT_PAGE_ALLOC]\t= \"page alloc\",\n\t[FAULT_ALLOC_NID]\t= \"alloc nid\",\n\t[FAULT_ORPHAN]\t\t= \"orphan\",\n\t[FAULT_BLOCK]\t\t= \"no more block\",\n\t[FAULT_DIR_DEPTH]\t= \"too big dir depth\",\n\t[FAULT_EVICT_INODE]\t= \"evict_inode fail\",\n\t[FAULT_TRUNCATE]\t= \"truncate fail\",\n\t[FAULT_IO]\t\t= \"IO error\",\n\t[FAULT_CHECKPOINT]\t= \"checkpoint error\",\n};\n\nstatic void f2fs_build_fault_attr(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tunsigned int rate)\n{\n\tstruct f2fs_fault_info *ffi = &sbi->fault_info;\n\n\tif (rate) {\n\t\tatomic_set(&ffi->inject_ops, 0);\n\t\tffi->inject_rate = rate;\n\t\tffi->inject_type = (1 << FAULT_MAX) - 1;\n\t} else {\n\t\tmemset(ffi, 0, sizeof(struct f2fs_fault_info));\n\t}\n}\n#endif\n\n/* f2fs-wide shrinker description */\nstatic struct shrinker f2fs_shrinker_info = {\n\t.scan_objects = f2fs_shrink_scan,\n\t.count_objects = f2fs_shrink_count,\n\t.seeks = DEFAULT_SEEKS,\n};\n\nenum {\n\tOpt_gc_background,\n\tOpt_disable_roll_forward,\n\tOpt_norecovery,\n\tOpt_discard,\n\tOpt_nodiscard,\n\tOpt_noheap,\n\tOpt_heap,\n\tOpt_user_xattr,\n\tOpt_nouser_xattr,\n\tOpt_acl,\n\tOpt_noacl,\n\tOpt_active_logs,\n\tOpt_disable_ext_identify,\n\tOpt_inline_xattr,\n\tOpt_noinline_xattr,\n\tOpt_inline_data,\n\tOpt_inline_dentry,\n\tOpt_noinline_dentry,\n\tOpt_flush_merge,\n\tOpt_noflush_merge,\n\tOpt_nobarrier,\n\tOpt_fastboot,\n\tOpt_extent_cache,\n\tOpt_noextent_cache,\n\tOpt_noinline_data,\n\tOpt_data_flush,\n\tOpt_mode,\n\tOpt_io_size_bits,\n\tOpt_fault_injection,\n\tOpt_lazytime,\n\tOpt_nolazytime,\n\tOpt_err,\n};\n\nstatic match_table_t f2fs_tokens = {\n\t{Opt_gc_background, \"background_gc=%s\"},\n\t{Opt_disable_roll_forward, \"disable_roll_forward\"},\n\t{Opt_norecovery, \"norecovery\"},\n\t{Opt_discard, \"discard\"},\n\t{Opt_nodiscard, \"nodiscard\"},\n\t{Opt_noheap, \"no_heap\"},\n\t{Opt_heap, \"heap\"},\n\t{Opt_user_xattr, \"user_xattr\"},\n\t{Opt_nouser_xattr, \"nouser_xattr\"},\n\t{Opt_acl, \"acl\"},\n\t{Opt_noacl, \"noacl\"},\n\t{Opt_active_logs, \"active_logs=%u\"},\n\t{Opt_disable_ext_identify, \"disable_ext_identify\"},\n\t{Opt_inline_xattr, \"inline_xattr\"},\n\t{Opt_noinline_xattr, \"noinline_xattr\"},\n\t{Opt_inline_data, \"inline_data\"},\n\t{Opt_inline_dentry, \"inline_dentry\"},\n\t{Opt_noinline_dentry, \"noinline_dentry\"},\n\t{Opt_flush_merge, \"flush_merge\"},\n\t{Opt_noflush_merge, \"noflush_merge\"},\n\t{Opt_nobarrier, \"nobarrier\"},\n\t{Opt_fastboot, \"fastboot\"},\n\t{Opt_extent_cache, \"extent_cache\"},\n\t{Opt_noextent_cache, \"noextent_cache\"},\n\t{Opt_noinline_data, \"noinline_data\"},\n\t{Opt_data_flush, \"data_flush\"},\n\t{Opt_mode, \"mode=%s\"},\n\t{Opt_io_size_bits, \"io_bits=%u\"},\n\t{Opt_fault_injection, \"fault_injection=%u\"},\n\t{Opt_lazytime, \"lazytime\"},\n\t{Opt_nolazytime, \"nolazytime\"},\n\t{Opt_err, NULL},\n};\n\n/* Sysfs support for f2fs */\nenum {\n\tGC_THREAD,\t/* struct f2fs_gc_thread */\n\tSM_INFO,\t/* struct f2fs_sm_info */\n\tDCC_INFO,\t/* struct discard_cmd_control */\n\tNM_INFO,\t/* struct f2fs_nm_info */\n\tF2FS_SBI,\t/* struct f2fs_sb_info */\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tFAULT_INFO_RATE,\t/* struct f2fs_fault_info */\n\tFAULT_INFO_TYPE,\t/* struct f2fs_fault_info */\n#endif\n};\n\nstruct f2fs_attr {\n\tstruct attribute attr;\n\tssize_t (*show)(struct f2fs_attr *, struct f2fs_sb_info *, char *);\n\tssize_t (*store)(struct f2fs_attr *, struct f2fs_sb_info *,\n\t\t\t const char *, size_t);\n\tint struct_type;\n\tint offset;\n};\n\nstatic unsigned char *__struct_ptr(struct f2fs_sb_info *sbi, int struct_type)\n{\n\tif (struct_type == GC_THREAD)\n\t\treturn (unsigned char *)sbi->gc_thread;\n\telse if (struct_type == SM_INFO)\n\t\treturn (unsigned char *)SM_I(sbi);\n\telse if (struct_type == DCC_INFO)\n\t\treturn (unsigned char *)SM_I(sbi)->dcc_info;\n\telse if (struct_type == NM_INFO)\n\t\treturn (unsigned char *)NM_I(sbi);\n\telse if (struct_type == F2FS_SBI)\n\t\treturn (unsigned char *)sbi;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\telse if (struct_type == FAULT_INFO_RATE ||\n\t\t\t\t\tstruct_type == FAULT_INFO_TYPE)\n\t\treturn (unsigned char *)&sbi->fault_info;\n#endif\n\treturn NULL;\n}\n\nstatic ssize_t lifetime_write_kbytes_show(struct f2fs_attr *a,\n\t\tstruct f2fs_sb_info *sbi, char *buf)\n{\n\tstruct super_block *sb = sbi->sb;\n\n\tif (!sb->s_bdev->bd_part)\n\t\treturn snprintf(buf, PAGE_SIZE, \"0\\n\");\n\n\treturn snprintf(buf, PAGE_SIZE, \"%llu\\n\",\n\t\t(unsigned long long)(sbi->kbytes_written +\n\t\t\tBD_PART_WRITTEN(sbi)));\n}\n\nstatic ssize_t f2fs_sbi_show(struct f2fs_attr *a,\n\t\t\tstruct f2fs_sb_info *sbi, char *buf)\n{\n\tunsigned char *ptr = NULL;\n\tunsigned int *ui;\n\n\tptr = __struct_ptr(sbi, a->struct_type);\n\tif (!ptr)\n\t\treturn -EINVAL;\n\n\tui = (unsigned int *)(ptr + a->offset);\n\n\treturn snprintf(buf, PAGE_SIZE, \"%u\\n\", *ui);\n}\n\nstatic ssize_t f2fs_sbi_store(struct f2fs_attr *a,\n\t\t\tstruct f2fs_sb_info *sbi,\n\t\t\tconst char *buf, size_t count)\n{\n\tunsigned char *ptr;\n\tunsigned long t;\n\tunsigned int *ui;\n\tssize_t ret;\n\n\tptr = __struct_ptr(sbi, a->struct_type);\n\tif (!ptr)\n\t\treturn -EINVAL;\n\n\tui = (unsigned int *)(ptr + a->offset);\n\n\tret = kstrtoul(skip_spaces(buf), 0, &t);\n\tif (ret < 0)\n\t\treturn ret;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (a->struct_type == FAULT_INFO_TYPE && t >= (1 << FAULT_MAX))\n\t\treturn -EINVAL;\n#endif\n\t*ui = t;\n\treturn count;\n}\n\nstatic ssize_t f2fs_attr_show(struct kobject *kobj,\n\t\t\t\tstruct attribute *attr, char *buf)\n{\n\tstruct f2fs_sb_info *sbi = container_of(kobj, struct f2fs_sb_info,\n\t\t\t\t\t\t\t\ts_kobj);\n\tstruct f2fs_attr *a = container_of(attr, struct f2fs_attr, attr);\n\n\treturn a->show ? a->show(a, sbi, buf) : 0;\n}\n\nstatic ssize_t f2fs_attr_store(struct kobject *kobj, struct attribute *attr,\n\t\t\t\t\t\tconst char *buf, size_t len)\n{\n\tstruct f2fs_sb_info *sbi = container_of(kobj, struct f2fs_sb_info,\n\t\t\t\t\t\t\t\t\ts_kobj);\n\tstruct f2fs_attr *a = container_of(attr, struct f2fs_attr, attr);\n\n\treturn a->store ? a->store(a, sbi, buf, len) : 0;\n}\n\nstatic void f2fs_sb_release(struct kobject *kobj)\n{\n\tstruct f2fs_sb_info *sbi = container_of(kobj, struct f2fs_sb_info,\n\t\t\t\t\t\t\t\ts_kobj);\n\tcomplete(&sbi->s_kobj_unregister);\n}\n\n#define F2FS_ATTR_OFFSET(_struct_type, _name, _mode, _show, _store, _offset) \\\nstatic struct f2fs_attr f2fs_attr_##_name = {\t\t\t\\\n\t.attr = {.name = __stringify(_name), .mode = _mode },\t\\\n\t.show\t= _show,\t\t\t\t\t\\\n\t.store\t= _store,\t\t\t\t\t\\\n\t.struct_type = _struct_type,\t\t\t\t\\\n\t.offset = _offset\t\t\t\t\t\\\n}\n\n#define F2FS_RW_ATTR(struct_type, struct_name, name, elname)\t\\\n\tF2FS_ATTR_OFFSET(struct_type, name, 0644,\t\t\\\n\t\tf2fs_sbi_show, f2fs_sbi_store,\t\t\t\\\n\t\toffsetof(struct struct_name, elname))\n\n#define F2FS_GENERAL_RO_ATTR(name) \\\nstatic struct f2fs_attr f2fs_attr_##name = __ATTR(name, 0444, name##_show, NULL)\n\nF2FS_RW_ATTR(GC_THREAD, f2fs_gc_kthread, gc_min_sleep_time, min_sleep_time);\nF2FS_RW_ATTR(GC_THREAD, f2fs_gc_kthread, gc_max_sleep_time, max_sleep_time);\nF2FS_RW_ATTR(GC_THREAD, f2fs_gc_kthread, gc_no_gc_sleep_time, no_gc_sleep_time);\nF2FS_RW_ATTR(GC_THREAD, f2fs_gc_kthread, gc_idle, gc_idle);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, reclaim_segments, rec_prefree_segments);\nF2FS_RW_ATTR(DCC_INFO, discard_cmd_control, max_small_discards, max_discards);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, batched_trim_sections, trim_sections);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, ipu_policy, ipu_policy);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, min_ipu_util, min_ipu_util);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, min_fsync_blocks, min_fsync_blocks);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, min_hot_blocks, min_hot_blocks);\nF2FS_RW_ATTR(NM_INFO, f2fs_nm_info, ram_thresh, ram_thresh);\nF2FS_RW_ATTR(NM_INFO, f2fs_nm_info, ra_nid_pages, ra_nid_pages);\nF2FS_RW_ATTR(NM_INFO, f2fs_nm_info, dirty_nats_ratio, dirty_nats_ratio);\nF2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, max_victim_search, max_victim_search);\nF2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, dir_level, dir_level);\nF2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, cp_interval, interval_time[CP_TIME]);\nF2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, idle_interval, interval_time[REQ_TIME]);\n#ifdef CONFIG_F2FS_FAULT_INJECTION\nF2FS_RW_ATTR(FAULT_INFO_RATE, f2fs_fault_info, inject_rate, inject_rate);\nF2FS_RW_ATTR(FAULT_INFO_TYPE, f2fs_fault_info, inject_type, inject_type);\n#endif\nF2FS_GENERAL_RO_ATTR(lifetime_write_kbytes);\n\n#define ATTR_LIST(name) (&f2fs_attr_##name.attr)\nstatic struct attribute *f2fs_attrs[] = {\n\tATTR_LIST(gc_min_sleep_time),\n\tATTR_LIST(gc_max_sleep_time),\n\tATTR_LIST(gc_no_gc_sleep_time),\n\tATTR_LIST(gc_idle),\n\tATTR_LIST(reclaim_segments),\n\tATTR_LIST(max_small_discards),\n\tATTR_LIST(batched_trim_sections),\n\tATTR_LIST(ipu_policy),\n\tATTR_LIST(min_ipu_util),\n\tATTR_LIST(min_fsync_blocks),\n\tATTR_LIST(min_hot_blocks),\n\tATTR_LIST(max_victim_search),\n\tATTR_LIST(dir_level),\n\tATTR_LIST(ram_thresh),\n\tATTR_LIST(ra_nid_pages),\n\tATTR_LIST(dirty_nats_ratio),\n\tATTR_LIST(cp_interval),\n\tATTR_LIST(idle_interval),\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tATTR_LIST(inject_rate),\n\tATTR_LIST(inject_type),\n#endif\n\tATTR_LIST(lifetime_write_kbytes),\n\tNULL,\n};\n\nstatic const struct sysfs_ops f2fs_attr_ops = {\n\t.show\t= f2fs_attr_show,\n\t.store\t= f2fs_attr_store,\n};\n\nstatic struct kobj_type f2fs_ktype = {\n\t.default_attrs\t= f2fs_attrs,\n\t.sysfs_ops\t= &f2fs_attr_ops,\n\t.release\t= f2fs_sb_release,\n};\n\nvoid f2fs_msg(struct super_block *sb, const char *level, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(\"%sF2FS-fs (%s): %pV\\n\", level, sb->s_id, &vaf);\n\tva_end(args);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct f2fs_inode_info *fi = (struct f2fs_inode_info *) foo;\n\n\tinode_init_once(&fi->vfs_inode);\n}\n\nstatic int parse_options(struct super_block *sb, char *options)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct request_queue *q;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tchar *p, *name;\n\tint arg = 0;\n\n\tif (!options)\n\t\treturn 0;\n\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tint token;\n\t\tif (!*p)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Initialize args struct so we know whether arg was\n\t\t * found; some options take optional arguments.\n\t\t */\n\t\targs[0].to = args[0].from = NULL;\n\t\ttoken = match_token(p, f2fs_tokens, args);\n\n\t\tswitch (token) {\n\t\tcase Opt_gc_background:\n\t\t\tname = match_strdup(&args[0]);\n\n\t\t\tif (!name)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (strlen(name) == 2 && !strncmp(name, \"on\", 2)) {\n\t\t\t\tset_opt(sbi, BG_GC);\n\t\t\t\tclear_opt(sbi, FORCE_FG_GC);\n\t\t\t} else if (strlen(name) == 3 && !strncmp(name, \"off\", 3)) {\n\t\t\t\tclear_opt(sbi, BG_GC);\n\t\t\t\tclear_opt(sbi, FORCE_FG_GC);\n\t\t\t} else if (strlen(name) == 4 && !strncmp(name, \"sync\", 4)) {\n\t\t\t\tset_opt(sbi, BG_GC);\n\t\t\t\tset_opt(sbi, FORCE_FG_GC);\n\t\t\t} else {\n\t\t\t\tkfree(name);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tkfree(name);\n\t\t\tbreak;\n\t\tcase Opt_disable_roll_forward:\n\t\t\tset_opt(sbi, DISABLE_ROLL_FORWARD);\n\t\t\tbreak;\n\t\tcase Opt_norecovery:\n\t\t\t/* this option mounts f2fs with ro */\n\t\t\tset_opt(sbi, DISABLE_ROLL_FORWARD);\n\t\t\tif (!f2fs_readonly(sb))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase Opt_discard:\n\t\t\tq = bdev_get_queue(sb->s_bdev);\n\t\t\tif (blk_queue_discard(q)) {\n\t\t\t\tset_opt(sbi, DISCARD);\n\t\t\t} else if (!f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"mounting with \\\"discard\\\" option, but \"\n\t\t\t\t\t\"the device does not support discard\");\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Opt_nodiscard:\n\t\t\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"discard is required for zoned block devices\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tclear_opt(sbi, DISCARD);\n\t\t\tbreak;\n\t\tcase Opt_noheap:\n\t\t\tset_opt(sbi, NOHEAP);\n\t\t\tbreak;\n\t\tcase Opt_heap:\n\t\t\tclear_opt(sbi, NOHEAP);\n\t\t\tbreak;\n#ifdef CONFIG_F2FS_FS_XATTR\n\t\tcase Opt_user_xattr:\n\t\t\tset_opt(sbi, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tclear_opt(sbi, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_inline_xattr:\n\t\t\tset_opt(sbi, INLINE_XATTR);\n\t\t\tbreak;\n\t\tcase Opt_noinline_xattr:\n\t\t\tclear_opt(sbi, INLINE_XATTR);\n\t\t\tbreak;\n#else\n\t\tcase Opt_user_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"user_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"nouser_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_inline_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"inline_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noinline_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"noinline_xattr options not supported\");\n\t\t\tbreak;\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\t\tcase Opt_acl:\n\t\t\tset_opt(sbi, POSIX_ACL);\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tclear_opt(sbi, POSIX_ACL);\n\t\t\tbreak;\n#else\n\t\tcase Opt_acl:\n\t\t\tf2fs_msg(sb, KERN_INFO, \"acl options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tf2fs_msg(sb, KERN_INFO, \"noacl options not supported\");\n\t\t\tbreak;\n#endif\n\t\tcase Opt_active_logs:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (arg != 2 && arg != 4 && arg != NR_CURSEG_TYPE)\n\t\t\t\treturn -EINVAL;\n\t\t\tsbi->active_logs = arg;\n\t\t\tbreak;\n\t\tcase Opt_disable_ext_identify:\n\t\t\tset_opt(sbi, DISABLE_EXT_IDENTIFY);\n\t\t\tbreak;\n\t\tcase Opt_inline_data:\n\t\t\tset_opt(sbi, INLINE_DATA);\n\t\t\tbreak;\n\t\tcase Opt_inline_dentry:\n\t\t\tset_opt(sbi, INLINE_DENTRY);\n\t\t\tbreak;\n\t\tcase Opt_noinline_dentry:\n\t\t\tclear_opt(sbi, INLINE_DENTRY);\n\t\t\tbreak;\n\t\tcase Opt_flush_merge:\n\t\t\tset_opt(sbi, FLUSH_MERGE);\n\t\t\tbreak;\n\t\tcase Opt_noflush_merge:\n\t\t\tclear_opt(sbi, FLUSH_MERGE);\n\t\t\tbreak;\n\t\tcase Opt_nobarrier:\n\t\t\tset_opt(sbi, NOBARRIER);\n\t\t\tbreak;\n\t\tcase Opt_fastboot:\n\t\t\tset_opt(sbi, FASTBOOT);\n\t\t\tbreak;\n\t\tcase Opt_extent_cache:\n\t\t\tset_opt(sbi, EXTENT_CACHE);\n\t\t\tbreak;\n\t\tcase Opt_noextent_cache:\n\t\t\tclear_opt(sbi, EXTENT_CACHE);\n\t\t\tbreak;\n\t\tcase Opt_noinline_data:\n\t\t\tclear_opt(sbi, INLINE_DATA);\n\t\t\tbreak;\n\t\tcase Opt_data_flush:\n\t\t\tset_opt(sbi, DATA_FLUSH);\n\t\t\tbreak;\n\t\tcase Opt_mode:\n\t\t\tname = match_strdup(&args[0]);\n\n\t\t\tif (!name)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (strlen(name) == 8 &&\n\t\t\t\t\t!strncmp(name, \"adaptive\", 8)) {\n\t\t\t\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\t \"adaptive mode is not allowed with \"\n\t\t\t\t\t\t \"zoned block device feature\");\n\t\t\t\t\tkfree(name);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tset_opt_mode(sbi, F2FS_MOUNT_ADAPTIVE);\n\t\t\t} else if (strlen(name) == 3 &&\n\t\t\t\t\t!strncmp(name, \"lfs\", 3)) {\n\t\t\t\tset_opt_mode(sbi, F2FS_MOUNT_LFS);\n\t\t\t} else {\n\t\t\t\tkfree(name);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tkfree(name);\n\t\t\tbreak;\n\t\tcase Opt_io_size_bits:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (arg > __ilog2_u32(BIO_MAX_PAGES)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"Not support %d, larger than %d\",\n\t\t\t\t\t1 << arg, BIO_MAX_PAGES);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tsbi->write_io_size_bits = arg;\n\t\t\tbreak;\n\t\tcase Opt_fault_injection:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\t\t\tf2fs_build_fault_attr(sbi, arg);\n\t\t\tset_opt(sbi, FAULT_INJECTION);\n#else\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"FAULT_INJECTION was not selected\");\n#endif\n\t\t\tbreak;\n\t\tcase Opt_lazytime:\n\t\t\tsb->s_flags |= MS_LAZYTIME;\n\t\t\tbreak;\n\t\tcase Opt_nolazytime:\n\t\t\tsb->s_flags &= ~MS_LAZYTIME;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Unrecognized mount option \\\"%s\\\" or missing value\",\n\t\t\t\tp);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (F2FS_IO_SIZE_BITS(sbi) && !test_opt(sbi, LFS)) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Should set mode=lfs with %uKB-sized IO\",\n\t\t\t\tF2FS_IO_SIZE_KB(sbi));\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic struct inode *f2fs_alloc_inode(struct super_block *sb)\n{\n\tstruct f2fs_inode_info *fi;\n\n\tfi = kmem_cache_alloc(f2fs_inode_cachep, GFP_F2FS_ZERO);\n\tif (!fi)\n\t\treturn NULL;\n\n\tinit_once((void *) fi);\n\n\t/* Initialize f2fs-specific inode info */\n\tfi->vfs_inode.i_version = 1;\n\tatomic_set(&fi->dirty_pages, 0);\n\tfi->i_current_depth = 1;\n\tfi->i_advise = 0;\n\tinit_rwsem(&fi->i_sem);\n\tINIT_LIST_HEAD(&fi->dirty_list);\n\tINIT_LIST_HEAD(&fi->gdirty_list);\n\tINIT_LIST_HEAD(&fi->inmem_pages);\n\tmutex_init(&fi->inmem_lock);\n\tinit_rwsem(&fi->dio_rwsem[READ]);\n\tinit_rwsem(&fi->dio_rwsem[WRITE]);\n\n\t/* Will be used by directory only */\n\tfi->i_dir_level = F2FS_SB(sb)->dir_level;\n\treturn &fi->vfs_inode;\n}\n\nstatic int f2fs_drop_inode(struct inode *inode)\n{\n\tint ret;\n\t/*\n\t * This is to avoid a deadlock condition like below.\n\t * writeback_single_inode(inode)\n\t *  - f2fs_write_data_page\n\t *    - f2fs_gc -> iput -> evict\n\t *       - inode_wait_for_writeback(inode)\n\t */\n\tif ((!inode_unhashed(inode) && inode->i_state & I_SYNC)) {\n\t\tif (!inode->i_nlink && !is_bad_inode(inode)) {\n\t\t\t/* to avoid evict_inode call simultaneously */\n\t\t\tatomic_inc(&inode->i_count);\n\t\t\tspin_unlock(&inode->i_lock);\n\n\t\t\t/* some remained atomic pages should discarded */\n\t\t\tif (f2fs_is_atomic_file(inode))\n\t\t\t\tdrop_inmem_pages(inode);\n\n\t\t\t/* should remain fi->extent_tree for writepage */\n\t\t\tf2fs_destroy_extent_node(inode);\n\n\t\t\tsb_start_intwrite(inode->i_sb);\n\t\t\tf2fs_i_size_write(inode, 0);\n\n\t\t\tif (F2FS_HAS_BLOCKS(inode))\n\t\t\t\tf2fs_truncate(inode);\n\n\t\t\tsb_end_intwrite(inode->i_sb);\n\n\t\t\tfscrypt_put_encryption_info(inode, NULL);\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tatomic_dec(&inode->i_count);\n\t\t}\n\t\ttrace_f2fs_drop_inode(inode, 0);\n\t\treturn 0;\n\t}\n\tret = generic_drop_inode(inode);\n\ttrace_f2fs_drop_inode(inode, ret);\n\treturn ret;\n}\n\nint f2fs_inode_dirtied(struct inode *inode, bool sync)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint ret = 0;\n\n\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\tif (is_inode_flag_set(inode, FI_DIRTY_INODE)) {\n\t\tret = 1;\n\t} else {\n\t\tset_inode_flag(inode, FI_DIRTY_INODE);\n\t\tstat_inc_dirty_inode(sbi, DIRTY_META);\n\t}\n\tif (sync && list_empty(&F2FS_I(inode)->gdirty_list)) {\n\t\tlist_add_tail(&F2FS_I(inode)->gdirty_list,\n\t\t\t\t&sbi->inode_list[DIRTY_META]);\n\t\tinc_page_count(sbi, F2FS_DIRTY_IMETA);\n\t}\n\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\treturn ret;\n}\n\nvoid f2fs_inode_synced(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\tif (!is_inode_flag_set(inode, FI_DIRTY_INODE)) {\n\t\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\t\treturn;\n\t}\n\tif (!list_empty(&F2FS_I(inode)->gdirty_list)) {\n\t\tlist_del_init(&F2FS_I(inode)->gdirty_list);\n\t\tdec_page_count(sbi, F2FS_DIRTY_IMETA);\n\t}\n\tclear_inode_flag(inode, FI_DIRTY_INODE);\n\tclear_inode_flag(inode, FI_AUTO_RECOVER);\n\tstat_dec_dirty_inode(F2FS_I_SB(inode), DIRTY_META);\n\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n}\n\n/*\n * f2fs_dirty_inode() is called from __mark_inode_dirty()\n *\n * We should call set_dirty_inode to write the dirty inode through write_inode.\n */\nstatic void f2fs_dirty_inode(struct inode *inode, int flags)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino == F2FS_NODE_INO(sbi) ||\n\t\t\tinode->i_ino == F2FS_META_INO(sbi))\n\t\treturn;\n\n\tif (flags == I_DIRTY_TIME)\n\t\treturn;\n\n\tif (is_inode_flag_set(inode, FI_AUTO_RECOVER))\n\t\tclear_inode_flag(inode, FI_AUTO_RECOVER);\n\n\tf2fs_inode_dirtied(inode, false);\n}\n\nstatic void f2fs_i_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(f2fs_inode_cachep, F2FS_I(inode));\n}\n\nstatic void f2fs_destroy_inode(struct inode *inode)\n{\n\tcall_rcu(&inode->i_rcu, f2fs_i_callback);\n}\n\nstatic void destroy_percpu_info(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_destroy(&sbi->alloc_valid_block_count);\n\tpercpu_counter_destroy(&sbi->total_valid_inode_count);\n}\n\nstatic void destroy_device_list(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\tblkdev_put(FDEV(i).bdev, FMODE_EXCL);\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\tkfree(FDEV(i).blkz_type);\n#endif\n\t}\n\tkfree(sbi->devs);\n}\n\nstatic void f2fs_put_super(struct super_block *sb)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\n\tif (sbi->s_proc) {\n\t\tremove_proc_entry(\"segment_info\", sbi->s_proc);\n\t\tremove_proc_entry(\"segment_bits\", sbi->s_proc);\n\t\tremove_proc_entry(sb->s_id, f2fs_proc_root);\n\t}\n\tkobject_del(&sbi->s_kobj);\n\n\tstop_gc_thread(sbi);\n\n\t/* prevent remaining shrinker jobs */\n\tmutex_lock(&sbi->umount_mutex);\n\n\t/*\n\t * We don't need to do checkpoint when superblock is clean.\n\t * But, the previous checkpoint was not done by umount, it needs to do\n\t * clean checkpoint again.\n\t */\n\tif (is_sbi_flag_set(sbi, SBI_IS_DIRTY) ||\n\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* be sure to wait for any on-going discard commands */\n\tf2fs_wait_discard_bios(sbi);\n\n\t/* write_checkpoint can update stat informaion */\n\tf2fs_destroy_stats(sbi);\n\n\t/*\n\t * normally superblock is clean, so we need to release this.\n\t * In addition, EIO will skip do checkpoint, we need this as well.\n\t */\n\trelease_ino_entry(sbi, true);\n\n\tf2fs_leave_shrinker(sbi);\n\tmutex_unlock(&sbi->umount_mutex);\n\n\t/* our cp_error case, we can wait for any writeback page */\n\tf2fs_flush_merged_bios(sbi);\n\n\tiput(sbi->node_inode);\n\tiput(sbi->meta_inode);\n\n\t/* destroy f2fs internal modules */\n\tdestroy_node_manager(sbi);\n\tdestroy_segment_manager(sbi);\n\n\tkfree(sbi->ckpt);\n\tkobject_put(&sbi->s_kobj);\n\twait_for_completion(&sbi->s_kobj_unregister);\n\n\tsb->s_fs_info = NULL;\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi->raw_super);\n\n\tdestroy_device_list(sbi);\n\tmempool_destroy(sbi->write_io_dummy);\n\tdestroy_percpu_info(sbi);\n\tkfree(sbi);\n}\n\nint f2fs_sync_fs(struct super_block *sb, int sync)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tint err = 0;\n\n\ttrace_f2fs_sync_fs(sb, sync);\n\n\tif (sync) {\n\t\tstruct cp_control cpc;\n\n\t\tcpc.reason = __get_cp_reason(sbi);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t}\n\tf2fs_trace_ios(NULL, 1);\n\n\treturn err;\n}\n\nstatic int f2fs_freeze(struct super_block *sb)\n{\n\tif (f2fs_readonly(sb))\n\t\treturn 0;\n\n\t/* IO error happened before */\n\tif (unlikely(f2fs_cp_error(F2FS_SB(sb))))\n\t\treturn -EIO;\n\n\t/* must be clean, since sync_filesystem() was already called */\n\tif (is_sbi_flag_set(F2FS_SB(sb), SBI_IS_DIRTY))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int f2fs_unfreeze(struct super_block *sb)\n{\n\treturn 0;\n}\n\nstatic int f2fs_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct super_block *sb = dentry->d_sb;\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tu64 id = huge_encode_dev(sb->s_bdev->bd_dev);\n\tblock_t total_count, user_block_count, start_count, ovp_count;\n\n\ttotal_count = le64_to_cpu(sbi->raw_super->block_count);\n\tuser_block_count = sbi->user_block_count;\n\tstart_count = le32_to_cpu(sbi->raw_super->segment0_blkaddr);\n\tovp_count = SM_I(sbi)->ovp_segments << sbi->log_blocks_per_seg;\n\tbuf->f_type = F2FS_SUPER_MAGIC;\n\tbuf->f_bsize = sbi->blocksize;\n\n\tbuf->f_blocks = total_count - start_count;\n\tbuf->f_bfree = user_block_count - valid_user_blocks(sbi) + ovp_count;\n\tbuf->f_bavail = user_block_count - valid_user_blocks(sbi);\n\n\tbuf->f_files = sbi->total_node_count - F2FS_RESERVED_NODE_NUM;\n\tbuf->f_ffree = min(buf->f_files - valid_node_count(sbi),\n\t\t\t\t\t\t\tbuf->f_bavail);\n\n\tbuf->f_namelen = F2FS_NAME_LEN;\n\tbuf->f_fsid.val[0] = (u32)id;\n\tbuf->f_fsid.val[1] = (u32)(id >> 32);\n\n\treturn 0;\n}\n\nstatic int f2fs_show_options(struct seq_file *seq, struct dentry *root)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(root->d_sb);\n\n\tif (!f2fs_readonly(sbi->sb) && test_opt(sbi, BG_GC)) {\n\t\tif (test_opt(sbi, FORCE_FG_GC))\n\t\t\tseq_printf(seq, \",background_gc=%s\", \"sync\");\n\t\telse\n\t\t\tseq_printf(seq, \",background_gc=%s\", \"on\");\n\t} else {\n\t\tseq_printf(seq, \",background_gc=%s\", \"off\");\n\t}\n\tif (test_opt(sbi, DISABLE_ROLL_FORWARD))\n\t\tseq_puts(seq, \",disable_roll_forward\");\n\tif (test_opt(sbi, DISCARD))\n\t\tseq_puts(seq, \",discard\");\n\tif (test_opt(sbi, NOHEAP))\n\t\tseq_puts(seq, \",no_heap\");\n\telse\n\t\tseq_puts(seq, \",heap\");\n#ifdef CONFIG_F2FS_FS_XATTR\n\tif (test_opt(sbi, XATTR_USER))\n\t\tseq_puts(seq, \",user_xattr\");\n\telse\n\t\tseq_puts(seq, \",nouser_xattr\");\n\tif (test_opt(sbi, INLINE_XATTR))\n\t\tseq_puts(seq, \",inline_xattr\");\n\telse\n\t\tseq_puts(seq, \",noinline_xattr\");\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\tif (test_opt(sbi, POSIX_ACL))\n\t\tseq_puts(seq, \",acl\");\n\telse\n\t\tseq_puts(seq, \",noacl\");\n#endif\n\tif (test_opt(sbi, DISABLE_EXT_IDENTIFY))\n\t\tseq_puts(seq, \",disable_ext_identify\");\n\tif (test_opt(sbi, INLINE_DATA))\n\t\tseq_puts(seq, \",inline_data\");\n\telse\n\t\tseq_puts(seq, \",noinline_data\");\n\tif (test_opt(sbi, INLINE_DENTRY))\n\t\tseq_puts(seq, \",inline_dentry\");\n\telse\n\t\tseq_puts(seq, \",noinline_dentry\");\n\tif (!f2fs_readonly(sbi->sb) && test_opt(sbi, FLUSH_MERGE))\n\t\tseq_puts(seq, \",flush_merge\");\n\tif (test_opt(sbi, NOBARRIER))\n\t\tseq_puts(seq, \",nobarrier\");\n\tif (test_opt(sbi, FASTBOOT))\n\t\tseq_puts(seq, \",fastboot\");\n\tif (test_opt(sbi, EXTENT_CACHE))\n\t\tseq_puts(seq, \",extent_cache\");\n\telse\n\t\tseq_puts(seq, \",noextent_cache\");\n\tif (test_opt(sbi, DATA_FLUSH))\n\t\tseq_puts(seq, \",data_flush\");\n\n\tseq_puts(seq, \",mode=\");\n\tif (test_opt(sbi, ADAPTIVE))\n\t\tseq_puts(seq, \"adaptive\");\n\telse if (test_opt(sbi, LFS))\n\t\tseq_puts(seq, \"lfs\");\n\tseq_printf(seq, \",active_logs=%u\", sbi->active_logs);\n\tif (F2FS_IO_SIZE_BITS(sbi))\n\t\tseq_printf(seq, \",io_size=%uKB\", F2FS_IO_SIZE_KB(sbi));\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (test_opt(sbi, FAULT_INJECTION))\n\t\tseq_puts(seq, \",fault_injection\");\n#endif\n\n\treturn 0;\n}\n\nstatic int segment_info_seq_show(struct seq_file *seq, void *offset)\n{\n\tstruct super_block *sb = seq->private;\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tunsigned int total_segs =\n\t\t\tle32_to_cpu(sbi->raw_super->segment_count_main);\n\tint i;\n\n\tseq_puts(seq, \"format: segment_type|valid_blocks\\n\"\n\t\t\"segment_type(0:HD, 1:WD, 2:CD, 3:HN, 4:WN, 5:CN)\\n\");\n\n\tfor (i = 0; i < total_segs; i++) {\n\t\tstruct seg_entry *se = get_seg_entry(sbi, i);\n\n\t\tif ((i % 10) == 0)\n\t\t\tseq_printf(seq, \"%-10d\", i);\n\t\tseq_printf(seq, \"%d|%-3u\", se->type,\n\t\t\t\t\tget_valid_blocks(sbi, i, false));\n\t\tif ((i % 10) == 9 || i == (total_segs - 1))\n\t\t\tseq_putc(seq, '\\n');\n\t\telse\n\t\t\tseq_putc(seq, ' ');\n\t}\n\n\treturn 0;\n}\n\nstatic int segment_bits_seq_show(struct seq_file *seq, void *offset)\n{\n\tstruct super_block *sb = seq->private;\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tunsigned int total_segs =\n\t\t\tle32_to_cpu(sbi->raw_super->segment_count_main);\n\tint i, j;\n\n\tseq_puts(seq, \"format: segment_type|valid_blocks|bitmaps\\n\"\n\t\t\"segment_type(0:HD, 1:WD, 2:CD, 3:HN, 4:WN, 5:CN)\\n\");\n\n\tfor (i = 0; i < total_segs; i++) {\n\t\tstruct seg_entry *se = get_seg_entry(sbi, i);\n\n\t\tseq_printf(seq, \"%-10d\", i);\n\t\tseq_printf(seq, \"%d|%-3u|\", se->type,\n\t\t\t\t\tget_valid_blocks(sbi, i, false));\n\t\tfor (j = 0; j < SIT_VBLOCK_MAP_SIZE; j++)\n\t\t\tseq_printf(seq, \" %.2x\", se->cur_valid_map[j]);\n\t\tseq_putc(seq, '\\n');\n\t}\n\treturn 0;\n}\n\n#define F2FS_PROC_FILE_DEF(_name)\t\t\t\t\t\\\nstatic int _name##_open_fs(struct inode *inode, struct file *file)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn single_open(file, _name##_seq_show, PDE_DATA(inode));\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic const struct file_operations f2fs_seq_##_name##_fops = {\t\t\\\n\t.open = _name##_open_fs,\t\t\t\t\t\\\n\t.read = seq_read,\t\t\t\t\t\t\\\n\t.llseek = seq_lseek,\t\t\t\t\t\t\\\n\t.release = single_release,\t\t\t\t\t\\\n};\n\nF2FS_PROC_FILE_DEF(segment_info);\nF2FS_PROC_FILE_DEF(segment_bits);\n\nstatic void default_options(struct f2fs_sb_info *sbi)\n{\n\t/* init some FS parameters */\n\tsbi->active_logs = NR_CURSEG_TYPE;\n\n\tset_opt(sbi, BG_GC);\n\tset_opt(sbi, INLINE_XATTR);\n\tset_opt(sbi, INLINE_DATA);\n\tset_opt(sbi, INLINE_DENTRY);\n\tset_opt(sbi, EXTENT_CACHE);\n\tset_opt(sbi, NOHEAP);\n\tsbi->sb->s_flags |= MS_LAZYTIME;\n\tset_opt(sbi, FLUSH_MERGE);\n\tif (f2fs_sb_mounted_blkzoned(sbi->sb)) {\n\t\tset_opt_mode(sbi, F2FS_MOUNT_LFS);\n\t\tset_opt(sbi, DISCARD);\n\t} else {\n\t\tset_opt_mode(sbi, F2FS_MOUNT_ADAPTIVE);\n\t}\n\n#ifdef CONFIG_F2FS_FS_XATTR\n\tset_opt(sbi, XATTR_USER);\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\tset_opt(sbi, POSIX_ACL);\n#endif\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tf2fs_build_fault_attr(sbi, 0);\n#endif\n}\n\nstatic int f2fs_remount(struct super_block *sb, int *flags, char *data)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct f2fs_mount_info org_mount_opt;\n\tint err, active_logs;\n\tbool need_restart_gc = false;\n\tbool need_stop_gc = false;\n\tbool no_extent_cache = !test_opt(sbi, EXTENT_CACHE);\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct f2fs_fault_info ffi = sbi->fault_info;\n#endif\n\n\t/*\n\t * Save the old mount options in case we\n\t * need to restore them.\n\t */\n\torg_mount_opt = sbi->mount_opt;\n\tactive_logs = sbi->active_logs;\n\n\t/* recover superblocks we couldn't write due to previous RO mount */\n\tif (!(*flags & MS_RDONLY) && is_sbi_flag_set(sbi, SBI_NEED_SB_WRITE)) {\n\t\terr = f2fs_commit_super(sbi, false);\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Try to recover all the superblocks, ret: %d\", err);\n\t\tif (!err)\n\t\t\tclear_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t}\n\n\tsbi->mount_opt.opt = 0;\n\tdefault_options(sbi);\n\n\t/* parse mount options */\n\terr = parse_options(sb, data);\n\tif (err)\n\t\tgoto restore_opts;\n\n\t/*\n\t * Previous and new state of filesystem is RO,\n\t * so skip checking GC and FLUSH_MERGE conditions.\n\t */\n\tif (f2fs_readonly(sb) && (*flags & MS_RDONLY))\n\t\tgoto skip;\n\n\t/* disallow enable/disable extent_cache dynamically */\n\tif (no_extent_cache == !!test_opt(sbi, EXTENT_CACHE)) {\n\t\terr = -EINVAL;\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\t\"switch extent_cache option is not allowed\");\n\t\tgoto restore_opts;\n\t}\n\n\t/*\n\t * We stop the GC thread if FS is mounted as RO\n\t * or if background_gc = off is passed in mount\n\t * option. Also sync the filesystem.\n\t */\n\tif ((*flags & MS_RDONLY) || !test_opt(sbi, BG_GC)) {\n\t\tif (sbi->gc_thread) {\n\t\t\tstop_gc_thread(sbi);\n\t\t\tneed_restart_gc = true;\n\t\t}\n\t} else if (!sbi->gc_thread) {\n\t\terr = start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto restore_opts;\n\t\tneed_stop_gc = true;\n\t}\n\n\tif (*flags & MS_RDONLY) {\n\t\twriteback_inodes_sb(sb, WB_REASON_SYNC);\n\t\tsync_inodes_sb(sb);\n\n\t\tset_sbi_flag(sbi, SBI_IS_DIRTY);\n\t\tset_sbi_flag(sbi, SBI_IS_CLOSE);\n\t\tf2fs_sync_fs(sb, 1);\n\t\tclear_sbi_flag(sbi, SBI_IS_CLOSE);\n\t}\n\n\t/*\n\t * We stop issue flush thread if FS is mounted as RO\n\t * or if flush_merge is not passed in mount option.\n\t */\n\tif ((*flags & MS_RDONLY) || !test_opt(sbi, FLUSH_MERGE)) {\n\t\tclear_opt(sbi, FLUSH_MERGE);\n\t\tdestroy_flush_cmd_control(sbi, false);\n\t} else {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\tgoto restore_gc;\n\t}\nskip:\n\t/* Update the POSIXACL Flag */\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? MS_POSIXACL : 0);\n\n\treturn 0;\nrestore_gc:\n\tif (need_restart_gc) {\n\t\tif (start_gc_thread(sbi))\n\t\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\t\"background gc thread has stopped\");\n\t} else if (need_stop_gc) {\n\t\tstop_gc_thread(sbi);\n\t}\nrestore_opts:\n\tsbi->mount_opt = org_mount_opt;\n\tsbi->active_logs = active_logs;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tsbi->fault_info = ffi;\n#endif\n\treturn err;\n}\n\nstatic struct super_operations f2fs_sops = {\n\t.alloc_inode\t= f2fs_alloc_inode,\n\t.drop_inode\t= f2fs_drop_inode,\n\t.destroy_inode\t= f2fs_destroy_inode,\n\t.write_inode\t= f2fs_write_inode,\n\t.dirty_inode\t= f2fs_dirty_inode,\n\t.show_options\t= f2fs_show_options,\n\t.evict_inode\t= f2fs_evict_inode,\n\t.put_super\t= f2fs_put_super,\n\t.sync_fs\t= f2fs_sync_fs,\n\t.freeze_fs\t= f2fs_freeze,\n\t.unfreeze_fs\t= f2fs_unfreeze,\n\t.statfs\t\t= f2fs_statfs,\n\t.remount_fs\t= f2fs_remount,\n};\n\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\nstatic int f2fs_get_context(struct inode *inode, void *ctx, size_t len)\n{\n\treturn f2fs_getxattr(inode, F2FS_XATTR_INDEX_ENCRYPTION,\n\t\t\t\tF2FS_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\tctx, len, NULL);\n}\n\nstatic int f2fs_set_context(struct inode *inode, const void *ctx, size_t len,\n\t\t\t\t\t\t\tvoid *fs_data)\n{\n\treturn f2fs_setxattr(inode, F2FS_XATTR_INDEX_ENCRYPTION,\n\t\t\t\tF2FS_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\tctx, len, fs_data, XATTR_CREATE);\n}\n\nstatic unsigned f2fs_max_namelen(struct inode *inode)\n{\n\treturn S_ISLNK(inode->i_mode) ?\n\t\t\tinode->i_sb->s_blocksize : F2FS_NAME_LEN;\n}\n\nstatic const struct fscrypt_operations f2fs_cryptops = {\n\t.key_prefix\t= \"f2fs:\",\n\t.get_context\t= f2fs_get_context,\n\t.set_context\t= f2fs_set_context,\n\t.is_encrypted\t= f2fs_encrypted_inode,\n\t.empty_dir\t= f2fs_empty_dir,\n\t.max_namelen\t= f2fs_max_namelen,\n};\n#else\nstatic const struct fscrypt_operations f2fs_cryptops = {\n\t.is_encrypted\t= f2fs_encrypted_inode,\n};\n#endif\n\nstatic struct inode *f2fs_nfs_get_inode(struct super_block *sb,\n\t\tu64 ino, u32 generation)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct inode *inode;\n\n\tif (check_nid_range(sbi, ino))\n\t\treturn ERR_PTR(-ESTALE);\n\n\t/*\n\t * f2fs_iget isn't quite right if the inode is currently unallocated!\n\t * However f2fs_iget currently does appropriate checks to handle stale\n\t * inodes so everything is OK.\n\t */\n\tinode = f2fs_iget(sb, ino);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (unlikely(generation && inode->i_generation != generation)) {\n\t\t/* we didn't find the right inode.. */\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ESTALE);\n\t}\n\treturn inode;\n}\n\nstatic struct dentry *f2fs_fh_to_dentry(struct super_block *sb, struct fid *fid,\n\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_dentry(sb, fid, fh_len, fh_type,\n\t\t\t\t    f2fs_nfs_get_inode);\n}\n\nstatic struct dentry *f2fs_fh_to_parent(struct super_block *sb, struct fid *fid,\n\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_parent(sb, fid, fh_len, fh_type,\n\t\t\t\t    f2fs_nfs_get_inode);\n}\n\nstatic const struct export_operations f2fs_export_ops = {\n\t.fh_to_dentry = f2fs_fh_to_dentry,\n\t.fh_to_parent = f2fs_fh_to_parent,\n\t.get_parent = f2fs_get_parent,\n};\n\nstatic loff_t max_file_blocks(void)\n{\n\tloff_t result = (DEF_ADDRS_PER_INODE - F2FS_INLINE_XATTR_ADDRS);\n\tloff_t leaf_count = ADDRS_PER_BLOCK;\n\n\t/* two direct node blocks */\n\tresult += (leaf_count * 2);\n\n\t/* two indirect node blocks */\n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += (leaf_count * 2);\n\n\t/* one double indirect node block */\n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += leaf_count;\n\n\treturn result;\n}\n\nstatic int __f2fs_commit_super(struct buffer_head *bh,\n\t\t\tstruct f2fs_super_block *super)\n{\n\tlock_buffer(bh);\n\tif (super)\n\t\tmemcpy(bh->b_data + F2FS_SUPER_OFFSET, super, sizeof(*super));\n\tset_buffer_uptodate(bh);\n\tset_buffer_dirty(bh);\n\tunlock_buffer(bh);\n\n\t/* it's rare case, we can do fua all the time */\n\treturn __sync_dirty_buffer(bh, REQ_PREFLUSH | REQ_FUA);\n}\n\nstatic inline bool sanity_check_area_boundary(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct buffer_head *bh)\n{\n\tstruct f2fs_super_block *raw_super = (struct f2fs_super_block *)\n\t\t\t\t\t(bh->b_data + F2FS_SUPER_OFFSET);\n\tstruct super_block *sb = sbi->sb;\n\tu32 segment0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tu32 cp_blkaddr = le32_to_cpu(raw_super->cp_blkaddr);\n\tu32 sit_blkaddr = le32_to_cpu(raw_super->sit_blkaddr);\n\tu32 nat_blkaddr = le32_to_cpu(raw_super->nat_blkaddr);\n\tu32 ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tu32 main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tu32 segment_count_ckpt = le32_to_cpu(raw_super->segment_count_ckpt);\n\tu32 segment_count_sit = le32_to_cpu(raw_super->segment_count_sit);\n\tu32 segment_count_nat = le32_to_cpu(raw_super->segment_count_nat);\n\tu32 segment_count_ssa = le32_to_cpu(raw_super->segment_count_ssa);\n\tu32 segment_count_main = le32_to_cpu(raw_super->segment_count_main);\n\tu32 segment_count = le32_to_cpu(raw_super->segment_count);\n\tu32 log_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tu64 main_end_blkaddr = main_blkaddr +\n\t\t\t\t(segment_count_main << log_blocks_per_seg);\n\tu64 seg_end_blkaddr = segment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg);\n\n\tif (segment0_blkaddr != cp_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Mismatch start address, segment0(%u) cp_blkaddr(%u)\",\n\t\t\tsegment0_blkaddr, cp_blkaddr);\n\t\treturn true;\n\t}\n\n\tif (cp_blkaddr + (segment_count_ckpt << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tsit_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong CP boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tcp_blkaddr, sit_blkaddr,\n\t\t\tsegment_count_ckpt << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (sit_blkaddr + (segment_count_sit << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tnat_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong SIT boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tsit_blkaddr, nat_blkaddr,\n\t\t\tsegment_count_sit << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (nat_blkaddr + (segment_count_nat << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tssa_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong NAT boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tnat_blkaddr, ssa_blkaddr,\n\t\t\tsegment_count_nat << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (ssa_blkaddr + (segment_count_ssa << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tmain_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong SSA boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tssa_blkaddr, main_blkaddr,\n\t\t\tsegment_count_ssa << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (main_end_blkaddr > seg_end_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong MAIN_AREA boundary, start(%u) end(%u) block(%u)\",\n\t\t\tmain_blkaddr,\n\t\t\tsegment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg),\n\t\t\tsegment_count_main << log_blocks_per_seg);\n\t\treturn true;\n\t} else if (main_end_blkaddr < seg_end_blkaddr) {\n\t\tint err = 0;\n\t\tchar *res;\n\n\t\t/* fix in-memory information all the time */\n\t\traw_super->segment_count = cpu_to_le32((main_end_blkaddr -\n\t\t\t\tsegment0_blkaddr) >> log_blocks_per_seg);\n\n\t\tif (f2fs_readonly(sb) || bdev_read_only(sb->s_bdev)) {\n\t\t\tset_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t\t\tres = \"internally\";\n\t\t} else {\n\t\t\terr = __f2fs_commit_super(bh, NULL);\n\t\t\tres = err ? \"failed\" : \"done\";\n\t\t}\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Fix alignment : %s, start(%u) end(%u) block(%u)\",\n\t\t\tres, main_blkaddr,\n\t\t\tsegment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg),\n\t\t\tsegment_count_main << log_blocks_per_seg);\n\t\tif (err)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int sanity_check_raw_super(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct buffer_head *bh)\n{\n\tstruct f2fs_super_block *raw_super = (struct f2fs_super_block *)\n\t\t\t\t\t(bh->b_data + F2FS_SUPER_OFFSET);\n\tstruct super_block *sb = sbi->sb;\n\tunsigned int blocksize;\n\n\tif (F2FS_SUPER_MAGIC != le32_to_cpu(raw_super->magic)) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Magic Mismatch, valid(0x%x) - read(0x%x)\",\n\t\t\tF2FS_SUPER_MAGIC, le32_to_cpu(raw_super->magic));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB page cache size */\n\tif (F2FS_BLKSIZE != PAGE_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid page_cache_size (%lu), supports only 4KB\\n\",\n\t\t\tPAGE_SIZE);\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB block size */\n\tblocksize = 1 << le32_to_cpu(raw_super->log_blocksize);\n\tif (blocksize != F2FS_BLKSIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid blocksize (%u), supports only 4KB\\n\",\n\t\t\tblocksize);\n\t\treturn 1;\n\t}\n\n\t/* check log blocks per segment */\n\tif (le32_to_cpu(raw_super->log_blocks_per_seg) != 9) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log blocks per segment (%u)\\n\",\n\t\t\tle32_to_cpu(raw_super->log_blocks_per_seg));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support 512/1024/2048/4096 bytes sector size */\n\tif (le32_to_cpu(raw_super->log_sectorsize) >\n\t\t\t\tF2FS_MAX_LOG_SECTOR_SIZE ||\n\t\tle32_to_cpu(raw_super->log_sectorsize) <\n\t\t\t\tF2FS_MIN_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO, \"Invalid log sectorsize (%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\tif (le32_to_cpu(raw_super->log_sectors_per_block) +\n\t\tle32_to_cpu(raw_super->log_sectorsize) !=\n\t\t\tF2FS_MAX_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log sectors per block(%u) log sectorsize(%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectors_per_block),\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\n\t/* check reserved ino info */\n\tif (le32_to_cpu(raw_super->node_ino) != 1 ||\n\t\tle32_to_cpu(raw_super->meta_ino) != 2 ||\n\t\tle32_to_cpu(raw_super->root_ino) != 3) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid Fs Meta Ino: node(%u) meta(%u) root(%u)\",\n\t\t\tle32_to_cpu(raw_super->node_ino),\n\t\t\tle32_to_cpu(raw_super->meta_ino),\n\t\t\tle32_to_cpu(raw_super->root_ino));\n\t\treturn 1;\n\t}\n\n\t/* check CP/SIT/NAT/SSA/MAIN_AREA area boundary */\n\tif (sanity_check_area_boundary(sbi, bh))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nint sanity_check_ckpt(struct f2fs_sb_info *sbi)\n{\n\tunsigned int total, fsmeta;\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tunsigned int ovp_segments, reserved_segments;\n\n\ttotal = le32_to_cpu(raw_super->segment_count);\n\tfsmeta = le32_to_cpu(raw_super->segment_count_ckpt);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_sit);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_nat);\n\tfsmeta += le32_to_cpu(ckpt->rsvd_segment_count);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_ssa);\n\n\tif (unlikely(fsmeta >= total))\n\t\treturn 1;\n\n\tovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\treserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\n\tif (unlikely(fsmeta < F2FS_MIN_SEGMENTS ||\n\t\t\tovp_segments == 0 || reserved_segments == 0)) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong layout: check mkfs.f2fs version\");\n\t\treturn 1;\n\t}\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR, \"A bug case: need to run fsck\");\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void init_sb_info(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = sbi->raw_super;\n\tint i;\n\n\tsbi->log_sectors_per_block =\n\t\tle32_to_cpu(raw_super->log_sectors_per_block);\n\tsbi->log_blocksize = le32_to_cpu(raw_super->log_blocksize);\n\tsbi->blocksize = 1 << sbi->log_blocksize;\n\tsbi->log_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tsbi->blocks_per_seg = 1 << sbi->log_blocks_per_seg;\n\tsbi->segs_per_sec = le32_to_cpu(raw_super->segs_per_sec);\n\tsbi->secs_per_zone = le32_to_cpu(raw_super->secs_per_zone);\n\tsbi->total_sections = le32_to_cpu(raw_super->section_count);\n\tsbi->total_node_count =\n\t\t(le32_to_cpu(raw_super->segment_count_nat) / 2)\n\t\t\t* sbi->blocks_per_seg * NAT_ENTRY_PER_BLOCK;\n\tsbi->root_ino_num = le32_to_cpu(raw_super->root_ino);\n\tsbi->node_ino_num = le32_to_cpu(raw_super->node_ino);\n\tsbi->meta_ino_num = le32_to_cpu(raw_super->meta_ino);\n\tsbi->cur_victim_sec = NULL_SECNO;\n\tsbi->max_victim_search = DEF_MAX_VICTIM_SEARCH;\n\n\tsbi->dir_level = DEF_DIR_LEVEL;\n\tsbi->interval_time[CP_TIME] = DEF_CP_INTERVAL;\n\tsbi->interval_time[REQ_TIME] = DEF_IDLE_INTERVAL;\n\tclear_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\tfor (i = 0; i < NR_COUNT_TYPE; i++)\n\t\tatomic_set(&sbi->nr_pages[i], 0);\n\n\tatomic_set(&sbi->wb_sync_req, 0);\n\n\tINIT_LIST_HEAD(&sbi->s_list);\n\tmutex_init(&sbi->umount_mutex);\n\tmutex_init(&sbi->wio_mutex[NODE]);\n\tmutex_init(&sbi->wio_mutex[DATA]);\n\tspin_lock_init(&sbi->cp_lock);\n}\n\nstatic int init_percpu_info(struct f2fs_sb_info *sbi)\n{\n\tint err;\n\n\terr = percpu_counter_init(&sbi->alloc_valid_block_count, 0, GFP_KERNEL);\n\tif (err)\n\t\treturn err;\n\n\treturn percpu_counter_init(&sbi->total_valid_inode_count, 0,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int init_blkz_info(struct f2fs_sb_info *sbi, int devi)\n{\n\tstruct block_device *bdev = FDEV(devi).bdev;\n\tsector_t nr_sectors = bdev->bd_part->nr_sects;\n\tsector_t sector = 0;\n\tstruct blk_zone *zones;\n\tunsigned int i, nr_zones;\n\tunsigned int n = 0;\n\tint err = -EIO;\n\n\tif (!f2fs_sb_mounted_blkzoned(sbi->sb))\n\t\treturn 0;\n\n\tif (sbi->blocks_per_blkz && sbi->blocks_per_blkz !=\n\t\t\t\tSECTOR_TO_BLOCK(bdev_zone_sectors(bdev)))\n\t\treturn -EINVAL;\n\tsbi->blocks_per_blkz = SECTOR_TO_BLOCK(bdev_zone_sectors(bdev));\n\tif (sbi->log_blocks_per_blkz && sbi->log_blocks_per_blkz !=\n\t\t\t\t__ilog2_u32(sbi->blocks_per_blkz))\n\t\treturn -EINVAL;\n\tsbi->log_blocks_per_blkz = __ilog2_u32(sbi->blocks_per_blkz);\n\tFDEV(devi).nr_blkz = SECTOR_TO_BLOCK(nr_sectors) >>\n\t\t\t\t\tsbi->log_blocks_per_blkz;\n\tif (nr_sectors & (bdev_zone_sectors(bdev) - 1))\n\t\tFDEV(devi).nr_blkz++;\n\n\tFDEV(devi).blkz_type = kmalloc(FDEV(devi).nr_blkz, GFP_KERNEL);\n\tif (!FDEV(devi).blkz_type)\n\t\treturn -ENOMEM;\n\n#define F2FS_REPORT_NR_ZONES   4096\n\n\tzones = kcalloc(F2FS_REPORT_NR_ZONES, sizeof(struct blk_zone),\n\t\t\tGFP_KERNEL);\n\tif (!zones)\n\t\treturn -ENOMEM;\n\n\t/* Get block zones type */\n\twhile (zones && sector < nr_sectors) {\n\n\t\tnr_zones = F2FS_REPORT_NR_ZONES;\n\t\terr = blkdev_report_zones(bdev, sector,\n\t\t\t\t\t  zones, &nr_zones,\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!nr_zones) {\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 0; i < nr_zones; i++) {\n\t\t\tFDEV(devi).blkz_type[n] = zones[i].type;\n\t\t\tsector += zones[i].len;\n\t\t\tn++;\n\t\t}\n\t}\n\n\tkfree(zones);\n\n\treturn err;\n}\n#endif\n\n/*\n * Read f2fs raw super block.\n * Because we have two copies of super block, so read both of them\n * to get the first valid one. If any one of them is broken, we pass\n * them recovery flag back to the caller.\n */\nstatic int read_raw_super_block(struct f2fs_sb_info *sbi,\n\t\t\tstruct f2fs_super_block **raw_super,\n\t\t\tint *valid_super_block, int *recovery)\n{\n\tstruct super_block *sb = sbi->sb;\n\tint block;\n\tstruct buffer_head *bh;\n\tstruct f2fs_super_block *super;\n\tint err = 0;\n\n\tsuper = kzalloc(sizeof(struct f2fs_super_block), GFP_KERNEL);\n\tif (!super)\n\t\treturn -ENOMEM;\n\n\tfor (block = 0; block < 2; block++) {\n\t\tbh = sb_bread(sb, block);\n\t\tif (!bh) {\n\t\t\tf2fs_msg(sb, KERN_ERR, \"Unable to read %dth superblock\",\n\t\t\t\tblock + 1);\n\t\t\terr = -EIO;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* sanity checking of raw super */\n\t\tif (sanity_check_raw_super(sbi, bh)) {\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Can't find valid F2FS filesystem in %dth superblock\",\n\t\t\t\tblock + 1);\n\t\t\terr = -EINVAL;\n\t\t\tbrelse(bh);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!*raw_super) {\n\t\t\tmemcpy(super, bh->b_data + F2FS_SUPER_OFFSET,\n\t\t\t\t\t\t\tsizeof(*super));\n\t\t\t*valid_super_block = block;\n\t\t\t*raw_super = super;\n\t\t}\n\t\tbrelse(bh);\n\t}\n\n\t/* Fail to read any one of the superblocks*/\n\tif (err < 0)\n\t\t*recovery = 1;\n\n\t/* No valid superblock */\n\tif (!*raw_super)\n\t\tkfree(super);\n\telse\n\t\terr = 0;\n\n\treturn err;\n}\n\nint f2fs_commit_super(struct f2fs_sb_info *sbi, bool recover)\n{\n\tstruct buffer_head *bh;\n\tint err;\n\n\tif ((recover && f2fs_readonly(sbi->sb)) ||\n\t\t\t\tbdev_read_only(sbi->sb->s_bdev)) {\n\t\tset_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t\treturn -EROFS;\n\t}\n\n\t/* write back-up superblock first */\n\tbh = sb_getblk(sbi->sb, sbi->valid_super_block ? 0: 1);\n\tif (!bh)\n\t\treturn -EIO;\n\terr = __f2fs_commit_super(bh, F2FS_RAW_SUPER(sbi));\n\tbrelse(bh);\n\n\t/* if we are in recovery path, skip writing valid superblock */\n\tif (recover || err)\n\t\treturn err;\n\n\t/* write current valid superblock */\n\tbh = sb_getblk(sbi->sb, sbi->valid_super_block);\n\tif (!bh)\n\t\treturn -EIO;\n\terr = __f2fs_commit_super(bh, F2FS_RAW_SUPER(sbi));\n\tbrelse(bh);\n\treturn err;\n}\n\nstatic int f2fs_scan_devices(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tunsigned int max_devices = MAX_DEVICES;\n\tint i;\n\n\t/* Initialize single device information */\n\tif (!RDEV(0).path[0]) {\n\t\tif (!bdev_is_zoned(sbi->sb->s_bdev))\n\t\t\treturn 0;\n\t\tmax_devices = 1;\n\t}\n\n\t/*\n\t * Initialize multiple devices information, or single\n\t * zoned block device information.\n\t */\n\tsbi->devs = kcalloc(max_devices, sizeof(struct f2fs_dev_info),\n\t\t\t\tGFP_KERNEL);\n\tif (!sbi->devs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < max_devices; i++) {\n\n\t\tif (i > 0 && !RDEV(i).path[0])\n\t\t\tbreak;\n\n\t\tif (max_devices == 1) {\n\t\t\t/* Single zoned block device mount */\n\t\t\tFDEV(0).bdev =\n\t\t\t\tblkdev_get_by_dev(sbi->sb->s_bdev->bd_dev,\n\t\t\t\t\tsbi->sb->s_mode, sbi->sb->s_type);\n\t\t} else {\n\t\t\t/* Multi-device mount */\n\t\t\tmemcpy(FDEV(i).path, RDEV(i).path, MAX_PATH_LEN);\n\t\t\tFDEV(i).total_segments =\n\t\t\t\tle32_to_cpu(RDEV(i).total_segments);\n\t\t\tif (i == 0) {\n\t\t\t\tFDEV(i).start_blk = 0;\n\t\t\t\tFDEV(i).end_blk = FDEV(i).start_blk +\n\t\t\t\t    (FDEV(i).total_segments <<\n\t\t\t\t    sbi->log_blocks_per_seg) - 1 +\n\t\t\t\t    le32_to_cpu(raw_super->segment0_blkaddr);\n\t\t\t} else {\n\t\t\t\tFDEV(i).start_blk = FDEV(i - 1).end_blk + 1;\n\t\t\t\tFDEV(i).end_blk = FDEV(i).start_blk +\n\t\t\t\t\t(FDEV(i).total_segments <<\n\t\t\t\t\tsbi->log_blocks_per_seg) - 1;\n\t\t\t}\n\t\t\tFDEV(i).bdev = blkdev_get_by_path(FDEV(i).path,\n\t\t\t\t\tsbi->sb->s_mode, sbi->sb->s_type);\n\t\t}\n\t\tif (IS_ERR(FDEV(i).bdev))\n\t\t\treturn PTR_ERR(FDEV(i).bdev);\n\n\t\t/* to release errored devices */\n\t\tsbi->s_ndevs = i + 1;\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\tif (bdev_zoned_model(FDEV(i).bdev) == BLK_ZONED_HM &&\n\t\t\t\t!f2fs_sb_mounted_blkzoned(sbi->sb)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\"Zoned block device feature not enabled\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (bdev_zoned_model(FDEV(i).bdev) != BLK_ZONED_NONE) {\n\t\t\tif (init_blkz_info(sbi, i)) {\n\t\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\t\"Failed to initialize F2FS blkzone information\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (max_devices == 1)\n\t\t\t\tbreak;\n\t\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\t\"Mount Device [%2d]: %20s, %8u, %8x - %8x (zone: %s)\",\n\t\t\t\ti, FDEV(i).path,\n\t\t\t\tFDEV(i).total_segments,\n\t\t\t\tFDEV(i).start_blk, FDEV(i).end_blk,\n\t\t\t\tbdev_zoned_model(FDEV(i).bdev) == BLK_ZONED_HA ?\n\t\t\t\t\"Host-aware\" : \"Host-managed\");\n\t\t\tcontinue;\n\t\t}\n#endif\n\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"Mount Device [%2d]: %20s, %8u, %8x - %8x\",\n\t\t\t\ti, FDEV(i).path,\n\t\t\t\tFDEV(i).total_segments,\n\t\t\t\tFDEV(i).start_blk, FDEV(i).end_blk);\n\t}\n\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"IO Block Size: %8d KB\", F2FS_IO_SIZE_KB(sbi));\n\treturn 0;\n}\n\nstatic int f2fs_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct f2fs_sb_info *sbi;\n\tstruct f2fs_super_block *raw_super;\n\tstruct inode *root;\n\tint err;\n\tbool retry = true, need_fsck = false;\n\tchar *options = NULL;\n\tint recovery, i, valid_super_block;\n\tstruct curseg_info *seg_i;\n\ntry_onemore:\n\terr = -EINVAL;\n\traw_super = NULL;\n\tvalid_super_block = -1;\n\trecovery = 0;\n\n\t/* allocate memory for f2fs-specific super block info */\n\tsbi = kzalloc(sizeof(struct f2fs_sb_info), GFP_KERNEL);\n\tif (!sbi)\n\t\treturn -ENOMEM;\n\n\tsbi->sb = sb;\n\n\t/* Load the checksum driver */\n\tsbi->s_chksum_driver = crypto_alloc_shash(\"crc32\", 0, 0);\n\tif (IS_ERR(sbi->s_chksum_driver)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Cannot load crc32 driver.\");\n\t\terr = PTR_ERR(sbi->s_chksum_driver);\n\t\tsbi->s_chksum_driver = NULL;\n\t\tgoto free_sbi;\n\t}\n\n\t/* set a block size */\n\tif (unlikely(!sb_set_blocksize(sb, F2FS_BLKSIZE))) {\n\t\tf2fs_msg(sb, KERN_ERR, \"unable to set blocksize\");\n\t\tgoto free_sbi;\n\t}\n\n\terr = read_raw_super_block(sbi, &raw_super, &valid_super_block,\n\t\t\t\t\t\t\t\t&recovery);\n\tif (err)\n\t\tgoto free_sbi;\n\n\tsb->s_fs_info = sbi;\n\tsbi->raw_super = raw_super;\n\n\t/*\n\t * The BLKZONED feature indicates that the drive was formatted with\n\t * zone alignment optimization. This is optional for host-aware\n\t * devices, but mandatory for host-managed zoned block devices.\n\t */\n#ifndef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t \"Zoned block device support is not enabled\\n\");\n\t\tgoto free_sb_buf;\n\t}\n#endif\n\tdefault_options(sbi);\n\t/* parse mount options */\n\toptions = kstrdup((const char *)data, GFP_KERNEL);\n\tif (data && !options) {\n\t\terr = -ENOMEM;\n\t\tgoto free_sb_buf;\n\t}\n\n\terr = parse_options(sb, options);\n\tif (err)\n\t\tgoto free_options;\n\n\tsbi->max_file_blocks = max_file_blocks();\n\tsb->s_maxbytes = sbi->max_file_blocks <<\n\t\t\t\tle32_to_cpu(raw_super->log_blocksize);\n\tsb->s_max_links = F2FS_LINK_MAX;\n\tget_random_bytes(&sbi->s_next_generation, sizeof(u32));\n\n\tsb->s_op = &f2fs_sops;\n\tsb->s_cop = &f2fs_cryptops;\n\tsb->s_xattr = f2fs_xattr_handlers;\n\tsb->s_export_op = &f2fs_export_ops;\n\tsb->s_magic = F2FS_SUPER_MAGIC;\n\tsb->s_time_gran = 1;\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? MS_POSIXACL : 0);\n\tmemcpy(sb->s_uuid, raw_super->uuid, sizeof(raw_super->uuid));\n\n\t/* init f2fs-specific super block info */\n\tsbi->valid_super_block = valid_super_block;\n\tmutex_init(&sbi->gc_mutex);\n\tmutex_init(&sbi->cp_mutex);\n\tinit_rwsem(&sbi->node_write);\n\tinit_rwsem(&sbi->node_change);\n\n\t/* disallow all the data/node/meta page writes */\n\tset_sbi_flag(sbi, SBI_POR_DOING);\n\tspin_lock_init(&sbi->stat_lock);\n\n\tinit_rwsem(&sbi->read_io.io_rwsem);\n\tsbi->read_io.sbi = sbi;\n\tsbi->read_io.bio = NULL;\n\tfor (i = 0; i < NR_PAGE_TYPE; i++) {\n\t\tinit_rwsem(&sbi->write_io[i].io_rwsem);\n\t\tsbi->write_io[i].sbi = sbi;\n\t\tsbi->write_io[i].bio = NULL;\n\t}\n\n\tinit_rwsem(&sbi->cp_rwsem);\n\tinit_waitqueue_head(&sbi->cp_wait);\n\tinit_sb_info(sbi);\n\n\terr = init_percpu_info(sbi);\n\tif (err)\n\t\tgoto free_options;\n\n\tif (F2FS_IO_SIZE(sbi) > 1) {\n\t\tsbi->write_io_dummy =\n\t\t\tmempool_create_page_pool(2 * (F2FS_IO_SIZE(sbi) - 1), 0);\n\t\tif (!sbi->write_io_dummy)\n\t\t\tgoto free_options;\n\t}\n\n\t/* get an inode for meta space */\n\tsbi->meta_inode = f2fs_iget(sb, F2FS_META_INO(sbi));\n\tif (IS_ERR(sbi->meta_inode)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read F2FS meta data inode\");\n\t\terr = PTR_ERR(sbi->meta_inode);\n\t\tgoto free_io_dummy;\n\t}\n\n\terr = get_valid_checkpoint(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to get valid F2FS checkpoint\");\n\t\tgoto free_meta_inode;\n\t}\n\n\t/* Initialize device list */\n\terr = f2fs_scan_devices(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to find devices\");\n\t\tgoto free_devices;\n\t}\n\n\tsbi->total_valid_node_count =\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_node_count);\n\tpercpu_counter_set(&sbi->total_valid_inode_count,\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_inode_count));\n\tsbi->user_block_count = le64_to_cpu(sbi->ckpt->user_block_count);\n\tsbi->total_valid_block_count =\n\t\t\t\tle64_to_cpu(sbi->ckpt->valid_block_count);\n\tsbi->last_valid_block_count = sbi->total_valid_block_count;\n\n\tfor (i = 0; i < NR_INODE_TYPE; i++) {\n\t\tINIT_LIST_HEAD(&sbi->inode_list[i]);\n\t\tspin_lock_init(&sbi->inode_lock[i]);\n\t}\n\n\tinit_extent_cache_info(sbi);\n\n\tinit_ino_entry_info(sbi);\n\n\t/* setup f2fs internal modules */\n\terr = build_segment_manager(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Failed to initialize F2FS segment manager\");\n\t\tgoto free_sm;\n\t}\n\terr = build_node_manager(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Failed to initialize F2FS node manager\");\n\t\tgoto free_nm;\n\t}\n\n\t/* For write statistics */\n\tif (sb->s_bdev->bd_part)\n\t\tsbi->sectors_written_start =\n\t\t\t(u64)part_stat_read(sb->s_bdev->bd_part, sectors[1]);\n\n\t/* Read accumulated write IO statistics if exists */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_NODE);\n\tif (__exist_node_summaries(sbi))\n\t\tsbi->kbytes_written =\n\t\t\tle64_to_cpu(seg_i->journal->info.kbytes_written);\n\n\tbuild_gc_manager(sbi);\n\n\t/* get an inode for node space */\n\tsbi->node_inode = f2fs_iget(sb, F2FS_NODE_INO(sbi));\n\tif (IS_ERR(sbi->node_inode)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read node inode\");\n\t\terr = PTR_ERR(sbi->node_inode);\n\t\tgoto free_nm;\n\t}\n\n\tf2fs_join_shrinker(sbi);\n\n\terr = f2fs_build_stats(sbi);\n\tif (err)\n\t\tgoto free_nm;\n\n\t/* if there are nt orphan nodes free them */\n\terr = recover_orphan_inodes(sbi);\n\tif (err)\n\t\tgoto free_node_inode;\n\n\t/* read root inode and dentry */\n\troot = f2fs_iget(sb, F2FS_ROOT_INO(sbi));\n\tif (IS_ERR(root)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read root inode\");\n\t\terr = PTR_ERR(root);\n\t\tgoto free_node_inode;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) {\n\t\tiput(root);\n\t\terr = -EINVAL;\n\t\tgoto free_node_inode;\n\t}\n\n\tsb->s_root = d_make_root(root); /* allocate root dentry */\n\tif (!sb->s_root) {\n\t\terr = -ENOMEM;\n\t\tgoto free_root_inode;\n\t}\n\n\tif (f2fs_proc_root)\n\t\tsbi->s_proc = proc_mkdir(sb->s_id, f2fs_proc_root);\n\n\tif (sbi->s_proc) {\n\t\tproc_create_data(\"segment_info\", S_IRUGO, sbi->s_proc,\n\t\t\t\t &f2fs_seq_segment_info_fops, sb);\n\t\tproc_create_data(\"segment_bits\", S_IRUGO, sbi->s_proc,\n\t\t\t\t &f2fs_seq_segment_bits_fops, sb);\n\t}\n\n\tsbi->s_kobj.kset = f2fs_kset;\n\tinit_completion(&sbi->s_kobj_unregister);\n\terr = kobject_init_and_add(&sbi->s_kobj, &f2fs_ktype, NULL,\n\t\t\t\t\t\t\t\"%s\", sb->s_id);\n\tif (err)\n\t\tgoto free_proc;\n\n\t/* recover fsynced data */\n\tif (!test_opt(sbi, DISABLE_ROLL_FORWARD)) {\n\t\t/*\n\t\t * mount should be failed, when device has readonly mode, and\n\t\t * previous checkpoint was not done by clean system shutdown.\n\t\t */\n\t\tif (bdev_read_only(sb->s_bdev) &&\n\t\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\t\terr = -EROFS;\n\t\t\tgoto free_kobj;\n\t\t}\n\n\t\tif (need_fsck)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\t\tif (!retry)\n\t\t\tgoto skip_recovery;\n\n\t\terr = recover_fsync_data(sbi, false);\n\t\tif (err < 0) {\n\t\t\tneed_fsck = true;\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Cannot recover all fsync data errno=%d\", err);\n\t\t\tgoto free_kobj;\n\t\t}\n\t} else {\n\t\terr = recover_fsync_data(sbi, true);\n\n\t\tif (!f2fs_readonly(sb) && err > 0) {\n\t\t\terr = -EINVAL;\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Need to recover fsync data\");\n\t\t\tgoto free_kobj;\n\t\t}\n\t}\nskip_recovery:\n\t/* recover_fsync_data() cleared this already */\n\tclear_sbi_flag(sbi, SBI_POR_DOING);\n\n\t/*\n\t * If filesystem is not mounted as read-only then\n\t * do start the gc_thread.\n\t */\n\tif (test_opt(sbi, BG_GC) && !f2fs_readonly(sb)) {\n\t\t/* After POR, we can run background GC thread.*/\n\t\terr = start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto free_kobj;\n\t}\n\tkfree(options);\n\n\t/* recover broken superblock */\n\tif (recovery) {\n\t\terr = f2fs_commit_super(sbi, true);\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Try to recover %dth superblock, ret: %d\",\n\t\t\tsbi->valid_super_block ? 1 : 2, err);\n\t}\n\n\tf2fs_msg(sbi->sb, KERN_NOTICE, \"Mounted with checkpoint version = %llx\",\n\t\t\t\tcur_cp_version(F2FS_CKPT(sbi)));\n\tf2fs_update_time(sbi, CP_TIME);\n\tf2fs_update_time(sbi, REQ_TIME);\n\treturn 0;\n\nfree_kobj:\n\tf2fs_sync_inode_meta(sbi);\n\tkobject_del(&sbi->s_kobj);\n\tkobject_put(&sbi->s_kobj);\n\twait_for_completion(&sbi->s_kobj_unregister);\nfree_proc:\n\tif (sbi->s_proc) {\n\t\tremove_proc_entry(\"segment_info\", sbi->s_proc);\n\t\tremove_proc_entry(\"segment_bits\", sbi->s_proc);\n\t\tremove_proc_entry(sb->s_id, f2fs_proc_root);\n\t}\nfree_root_inode:\n\tdput(sb->s_root);\n\tsb->s_root = NULL;\nfree_node_inode:\n\ttruncate_inode_pages_final(NODE_MAPPING(sbi));\n\tmutex_lock(&sbi->umount_mutex);\n\trelease_ino_entry(sbi, true);\n\tf2fs_leave_shrinker(sbi);\n\t/*\n\t * Some dirty meta pages can be produced by recover_orphan_inodes()\n\t * failed by EIO. Then, iput(node_inode) can trigger balance_fs_bg()\n\t * followed by write_checkpoint() through f2fs_write_node_pages(), which\n\t * falls into an infinite loop in sync_meta_pages().\n\t */\n\ttruncate_inode_pages_final(META_MAPPING(sbi));\n\tiput(sbi->node_inode);\n\tmutex_unlock(&sbi->umount_mutex);\n\tf2fs_destroy_stats(sbi);\nfree_nm:\n\tdestroy_node_manager(sbi);\nfree_sm:\n\tdestroy_segment_manager(sbi);\nfree_devices:\n\tdestroy_device_list(sbi);\n\tkfree(sbi->ckpt);\nfree_meta_inode:\n\tmake_bad_inode(sbi->meta_inode);\n\tiput(sbi->meta_inode);\nfree_io_dummy:\n\tmempool_destroy(sbi->write_io_dummy);\nfree_options:\n\tdestroy_percpu_info(sbi);\n\tkfree(options);\nfree_sb_buf:\n\tkfree(raw_super);\nfree_sbi:\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi);\n\n\t/* give only one another chance */\n\tif (retry) {\n\t\tretry = false;\n\t\tshrink_dcache_sb(sb);\n\t\tgoto try_onemore;\n\t}\n\treturn err;\n}\n\nstatic struct dentry *f2fs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *data)\n{\n\treturn mount_bdev(fs_type, flags, dev_name, data, f2fs_fill_super);\n}\n\nstatic void kill_f2fs_super(struct super_block *sb)\n{\n\tif (sb->s_root)\n\t\tset_sbi_flag(F2FS_SB(sb), SBI_IS_CLOSE);\n\tkill_block_super(sb);\n}\n\nstatic struct file_system_type f2fs_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"f2fs\",\n\t.mount\t\t= f2fs_mount,\n\t.kill_sb\t= kill_f2fs_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"f2fs\");\n\nstatic int __init init_inodecache(void)\n{\n\tf2fs_inode_cachep = kmem_cache_create(\"f2fs_inode_cache\",\n\t\t\tsizeof(struct f2fs_inode_info), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT|SLAB_ACCOUNT, NULL);\n\tif (!f2fs_inode_cachep)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void destroy_inodecache(void)\n{\n\t/*\n\t * Make sure all delayed rcu free inodes are flushed before we\n\t * destroy cache.\n\t */\n\trcu_barrier();\n\tkmem_cache_destroy(f2fs_inode_cachep);\n}\n\nstatic int __init init_f2fs_fs(void)\n{\n\tint err;\n\n\tf2fs_build_trace_ios();\n\n\terr = init_inodecache();\n\tif (err)\n\t\tgoto fail;\n\terr = create_node_manager_caches();\n\tif (err)\n\t\tgoto free_inodecache;\n\terr = create_segment_manager_caches();\n\tif (err)\n\t\tgoto free_node_manager_caches;\n\terr = create_checkpoint_caches();\n\tif (err)\n\t\tgoto free_segment_manager_caches;\n\terr = create_extent_cache();\n\tif (err)\n\t\tgoto free_checkpoint_caches;\n\tf2fs_kset = kset_create_and_add(\"f2fs\", NULL, fs_kobj);\n\tif (!f2fs_kset) {\n\t\terr = -ENOMEM;\n\t\tgoto free_extent_cache;\n\t}\n\terr = register_shrinker(&f2fs_shrinker_info);\n\tif (err)\n\t\tgoto free_kset;\n\n\terr = register_filesystem(&f2fs_fs_type);\n\tif (err)\n\t\tgoto free_shrinker;\n\terr = f2fs_create_root_stats();\n\tif (err)\n\t\tgoto free_filesystem;\n\tf2fs_proc_root = proc_mkdir(\"fs/f2fs\", NULL);\n\treturn 0;\n\nfree_filesystem:\n\tunregister_filesystem(&f2fs_fs_type);\nfree_shrinker:\n\tunregister_shrinker(&f2fs_shrinker_info);\nfree_kset:\n\tkset_unregister(f2fs_kset);\nfree_extent_cache:\n\tdestroy_extent_cache();\nfree_checkpoint_caches:\n\tdestroy_checkpoint_caches();\nfree_segment_manager_caches:\n\tdestroy_segment_manager_caches();\nfree_node_manager_caches:\n\tdestroy_node_manager_caches();\nfree_inodecache:\n\tdestroy_inodecache();\nfail:\n\treturn err;\n}\n\nstatic void __exit exit_f2fs_fs(void)\n{\n\tremove_proc_entry(\"fs/f2fs\", NULL);\n\tf2fs_destroy_root_stats();\n\tunregister_filesystem(&f2fs_fs_type);\n\tunregister_shrinker(&f2fs_shrinker_info);\n\tkset_unregister(f2fs_kset);\n\tdestroy_extent_cache();\n\tdestroy_checkpoint_caches();\n\tdestroy_segment_manager_caches();\n\tdestroy_node_manager_caches();\n\tdestroy_inodecache();\n\tf2fs_destroy_trace_ios();\n}\n\nmodule_init(init_f2fs_fs)\nmodule_exit(exit_f2fs_fs)\n\nMODULE_AUTHOR(\"Samsung Electronics's Praesto Team\");\nMODULE_DESCRIPTION(\"Flash Friendly File System\");\nMODULE_LICENSE(\"GPL\");\n\n", "/**\n * include/linux/f2fs_fs.h\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#ifndef _LINUX_F2FS_FS_H\n#define _LINUX_F2FS_FS_H\n\n#include <linux/pagemap.h>\n#include <linux/types.h>\n\n#define F2FS_SUPER_OFFSET\t\t1024\t/* byte-size offset */\n#define F2FS_MIN_LOG_SECTOR_SIZE\t9\t/* 9 bits for 512 bytes */\n#define F2FS_MAX_LOG_SECTOR_SIZE\t12\t/* 12 bits for 4096 bytes */\n#define F2FS_LOG_SECTORS_PER_BLOCK\t3\t/* log number for sector/blk */\n#define F2FS_BLKSIZE\t\t\t4096\t/* support only 4KB block */\n#define F2FS_BLKSIZE_BITS\t\t12\t/* bits for F2FS_BLKSIZE */\n#define F2FS_MAX_EXTENSION\t\t64\t/* # of extension entries */\n#define F2FS_BLK_ALIGN(x)\t(((x) + F2FS_BLKSIZE - 1) >> F2FS_BLKSIZE_BITS)\n\n#define NULL_ADDR\t\t((block_t)0)\t/* used as block_t addresses */\n#define NEW_ADDR\t\t((block_t)-1)\t/* used as block_t addresses */\n\n#define F2FS_BYTES_TO_BLK(bytes)\t((bytes) >> F2FS_BLKSIZE_BITS)\n#define F2FS_BLK_TO_BYTES(blk)\t\t((blk) << F2FS_BLKSIZE_BITS)\n\n/* 0, 1(node nid), 2(meta nid) are reserved node id */\n#define F2FS_RESERVED_NODE_NUM\t\t3\n\n#define F2FS_ROOT_INO(sbi)\t((sbi)->root_ino_num)\n#define F2FS_NODE_INO(sbi)\t((sbi)->node_ino_num)\n#define F2FS_META_INO(sbi)\t((sbi)->meta_ino_num)\n\n#define F2FS_IO_SIZE(sbi)\t(1 << (sbi)->write_io_size_bits) /* Blocks */\n#define F2FS_IO_SIZE_KB(sbi)\t(1 << ((sbi)->write_io_size_bits + 2)) /* KB */\n#define F2FS_IO_SIZE_BYTES(sbi)\t(1 << ((sbi)->write_io_size_bits + 12)) /* B */\n#define F2FS_IO_SIZE_BITS(sbi)\t((sbi)->write_io_size_bits) /* power of 2 */\n#define F2FS_IO_SIZE_MASK(sbi)\t(F2FS_IO_SIZE(sbi) - 1)\n\n/* This flag is used by node and meta inodes, and by recovery */\n#define GFP_F2FS_ZERO\t\t(GFP_NOFS | __GFP_ZERO)\n#define GFP_F2FS_HIGH_ZERO\t(GFP_NOFS | __GFP_ZERO | __GFP_HIGHMEM)\n\n/*\n * For further optimization on multi-head logs, on-disk layout supports maximum\n * 16 logs by default. The number, 16, is expected to cover all the cases\n * enoughly. The implementaion currently uses no more than 6 logs.\n * Half the logs are used for nodes, and the other half are used for data.\n */\n#define MAX_ACTIVE_LOGS\t16\n#define MAX_ACTIVE_NODE_LOGS\t8\n#define MAX_ACTIVE_DATA_LOGS\t8\n\n#define VERSION_LEN\t256\n#define MAX_VOLUME_NAME\t\t512\n#define MAX_PATH_LEN\t\t64\n#define MAX_DEVICES\t\t8\n\n/*\n * For superblock\n */\nstruct f2fs_device {\n\t__u8 path[MAX_PATH_LEN];\n\t__le32 total_segments;\n} __packed;\n\nstruct f2fs_super_block {\n\t__le32 magic;\t\t\t/* Magic Number */\n\t__le16 major_ver;\t\t/* Major Version */\n\t__le16 minor_ver;\t\t/* Minor Version */\n\t__le32 log_sectorsize;\t\t/* log2 sector size in bytes */\n\t__le32 log_sectors_per_block;\t/* log2 # of sectors per block */\n\t__le32 log_blocksize;\t\t/* log2 block size in bytes */\n\t__le32 log_blocks_per_seg;\t/* log2 # of blocks per segment */\n\t__le32 segs_per_sec;\t\t/* # of segments per section */\n\t__le32 secs_per_zone;\t\t/* # of sections per zone */\n\t__le32 checksum_offset;\t\t/* checksum offset inside super block */\n\t__le64 block_count;\t\t/* total # of user blocks */\n\t__le32 section_count;\t\t/* total # of sections */\n\t__le32 segment_count;\t\t/* total # of segments */\n\t__le32 segment_count_ckpt;\t/* # of segments for checkpoint */\n\t__le32 segment_count_sit;\t/* # of segments for SIT */\n\t__le32 segment_count_nat;\t/* # of segments for NAT */\n\t__le32 segment_count_ssa;\t/* # of segments for SSA */\n\t__le32 segment_count_main;\t/* # of segments for main area */\n\t__le32 segment0_blkaddr;\t/* start block address of segment 0 */\n\t__le32 cp_blkaddr;\t\t/* start block address of checkpoint */\n\t__le32 sit_blkaddr;\t\t/* start block address of SIT */\n\t__le32 nat_blkaddr;\t\t/* start block address of NAT */\n\t__le32 ssa_blkaddr;\t\t/* start block address of SSA */\n\t__le32 main_blkaddr;\t\t/* start block address of main area */\n\t__le32 root_ino;\t\t/* root inode number */\n\t__le32 node_ino;\t\t/* node inode number */\n\t__le32 meta_ino;\t\t/* meta inode number */\n\t__u8 uuid[16];\t\t\t/* 128-bit uuid for volume */\n\t__le16 volume_name[MAX_VOLUME_NAME];\t/* volume name */\n\t__le32 extension_count;\t\t/* # of extensions below */\n\t__u8 extension_list[F2FS_MAX_EXTENSION][8];\t/* extension array */\n\t__le32 cp_payload;\n\t__u8 version[VERSION_LEN];\t/* the kernel version */\n\t__u8 init_version[VERSION_LEN];\t/* the initial kernel version */\n\t__le32 feature;\t\t\t/* defined features */\n\t__u8 encryption_level;\t\t/* versioning level for encryption */\n\t__u8 encrypt_pw_salt[16];\t/* Salt used for string2key algorithm */\n\tstruct f2fs_device devs[MAX_DEVICES];\t/* device list */\n\t__u8 reserved[327];\t\t/* valid reserved region */\n} __packed;\n\n/*\n * For checkpoint\n */\n#define CP_NAT_BITS_FLAG\t0x00000080\n#define CP_CRC_RECOVERY_FLAG\t0x00000040\n#define CP_FASTBOOT_FLAG\t0x00000020\n#define CP_FSCK_FLAG\t\t0x00000010\n#define CP_ERROR_FLAG\t\t0x00000008\n#define CP_COMPACT_SUM_FLAG\t0x00000004\n#define CP_ORPHAN_PRESENT_FLAG\t0x00000002\n#define CP_UMOUNT_FLAG\t\t0x00000001\n\n#define F2FS_CP_PACKS\t\t2\t/* # of checkpoint packs */\n\nstruct f2fs_checkpoint {\n\t__le64 checkpoint_ver;\t\t/* checkpoint block version number */\n\t__le64 user_block_count;\t/* # of user blocks */\n\t__le64 valid_block_count;\t/* # of valid blocks in main area */\n\t__le32 rsvd_segment_count;\t/* # of reserved segments for gc */\n\t__le32 overprov_segment_count;\t/* # of overprovision segments */\n\t__le32 free_segment_count;\t/* # of free segments in main area */\n\n\t/* information of current node segments */\n\t__le32 cur_node_segno[MAX_ACTIVE_NODE_LOGS];\n\t__le16 cur_node_blkoff[MAX_ACTIVE_NODE_LOGS];\n\t/* information of current data segments */\n\t__le32 cur_data_segno[MAX_ACTIVE_DATA_LOGS];\n\t__le16 cur_data_blkoff[MAX_ACTIVE_DATA_LOGS];\n\t__le32 ckpt_flags;\t\t/* Flags : umount and journal_present */\n\t__le32 cp_pack_total_block_count;\t/* total # of one cp pack */\n\t__le32 cp_pack_start_sum;\t/* start block number of data summary */\n\t__le32 valid_node_count;\t/* Total number of valid nodes */\n\t__le32 valid_inode_count;\t/* Total number of valid inodes */\n\t__le32 next_free_nid;\t\t/* Next free node number */\n\t__le32 sit_ver_bitmap_bytesize;\t/* Default value 64 */\n\t__le32 nat_ver_bitmap_bytesize; /* Default value 256 */\n\t__le32 checksum_offset;\t\t/* checksum offset inside cp block */\n\t__le64 elapsed_time;\t\t/* mounted time */\n\t/* allocation type of current segment */\n\tunsigned char alloc_type[MAX_ACTIVE_LOGS];\n\n\t/* SIT and NAT version bitmap */\n\tunsigned char sit_nat_version_bitmap[1];\n} __packed;\n\n/*\n * For orphan inode management\n */\n#define F2FS_ORPHANS_PER_BLOCK\t1020\n\n#define GET_ORPHAN_BLOCKS(n)\t(((n) + F2FS_ORPHANS_PER_BLOCK - 1) / \\\n\t\t\t\t\tF2FS_ORPHANS_PER_BLOCK)\n\nstruct f2fs_orphan_block {\n\t__le32 ino[F2FS_ORPHANS_PER_BLOCK];\t/* inode numbers */\n\t__le32 reserved;\t/* reserved */\n\t__le16 blk_addr;\t/* block index in current CP */\n\t__le16 blk_count;\t/* Number of orphan inode blocks in CP */\n\t__le32 entry_count;\t/* Total number of orphan nodes in current CP */\n\t__le32 check_sum;\t/* CRC32 for orphan inode block */\n} __packed;\n\n/*\n * For NODE structure\n */\nstruct f2fs_extent {\n\t__le32 fofs;\t\t/* start file offset of the extent */\n\t__le32 blk;\t\t/* start block address of the extent */\n\t__le32 len;\t\t/* lengh of the extent */\n} __packed;\n\n#define F2FS_NAME_LEN\t\t255\n#define F2FS_INLINE_XATTR_ADDRS\t50\t/* 200 bytes for inline xattrs */\n#define DEF_ADDRS_PER_INODE\t923\t/* Address Pointers in an Inode */\n#define DEF_NIDS_PER_INODE\t5\t/* Node IDs in an Inode */\n#define ADDRS_PER_INODE(inode)\taddrs_per_inode(inode)\n#define ADDRS_PER_BLOCK\t\t1018\t/* Address Pointers in a Direct Block */\n#define NIDS_PER_BLOCK\t\t1018\t/* Node IDs in an Indirect Block */\n\n#define ADDRS_PER_PAGE(page, inode)\t\\\n\t(IS_INODE(page) ? ADDRS_PER_INODE(inode) : ADDRS_PER_BLOCK)\n\n#define\tNODE_DIR1_BLOCK\t\t(DEF_ADDRS_PER_INODE + 1)\n#define\tNODE_DIR2_BLOCK\t\t(DEF_ADDRS_PER_INODE + 2)\n#define\tNODE_IND1_BLOCK\t\t(DEF_ADDRS_PER_INODE + 3)\n#define\tNODE_IND2_BLOCK\t\t(DEF_ADDRS_PER_INODE + 4)\n#define\tNODE_DIND_BLOCK\t\t(DEF_ADDRS_PER_INODE + 5)\n\n#define F2FS_INLINE_XATTR\t0x01\t/* file inline xattr flag */\n#define F2FS_INLINE_DATA\t0x02\t/* file inline data flag */\n#define F2FS_INLINE_DENTRY\t0x04\t/* file inline dentry flag */\n#define F2FS_DATA_EXIST\t\t0x08\t/* file inline data exist flag */\n#define F2FS_INLINE_DOTS\t0x10\t/* file having implicit dot dentries */\n\n#define MAX_INLINE_DATA\t\t(sizeof(__le32) * (DEF_ADDRS_PER_INODE - \\\n\t\t\t\t\t\tF2FS_INLINE_XATTR_ADDRS - 1))\n\nstruct f2fs_inode {\n\t__le16 i_mode;\t\t\t/* file mode */\n\t__u8 i_advise;\t\t\t/* file hints */\n\t__u8 i_inline;\t\t\t/* file inline flags */\n\t__le32 i_uid;\t\t\t/* user ID */\n\t__le32 i_gid;\t\t\t/* group ID */\n\t__le32 i_links;\t\t\t/* links count */\n\t__le64 i_size;\t\t\t/* file size in bytes */\n\t__le64 i_blocks;\t\t/* file size in blocks */\n\t__le64 i_atime;\t\t\t/* access time */\n\t__le64 i_ctime;\t\t\t/* change time */\n\t__le64 i_mtime;\t\t\t/* modification time */\n\t__le32 i_atime_nsec;\t\t/* access time in nano scale */\n\t__le32 i_ctime_nsec;\t\t/* change time in nano scale */\n\t__le32 i_mtime_nsec;\t\t/* modification time in nano scale */\n\t__le32 i_generation;\t\t/* file version (for NFS) */\n\t__le32 i_current_depth;\t\t/* only for directory depth */\n\t__le32 i_xattr_nid;\t\t/* nid to save xattr */\n\t__le32 i_flags;\t\t\t/* file attributes */\n\t__le32 i_pino;\t\t\t/* parent inode number */\n\t__le32 i_namelen;\t\t/* file name length */\n\t__u8 i_name[F2FS_NAME_LEN];\t/* file name for SPOR */\n\t__u8 i_dir_level;\t\t/* dentry_level for large dir */\n\n\tstruct f2fs_extent i_ext;\t/* caching a largest extent */\n\n\t__le32 i_addr[DEF_ADDRS_PER_INODE];\t/* Pointers to data blocks */\n\n\t__le32 i_nid[DEF_NIDS_PER_INODE];\t/* direct(2), indirect(2),\n\t\t\t\t\t\tdouble_indirect(1) node id */\n} __packed;\n\nstruct direct_node {\n\t__le32 addr[ADDRS_PER_BLOCK];\t/* array of data block address */\n} __packed;\n\nstruct indirect_node {\n\t__le32 nid[NIDS_PER_BLOCK];\t/* array of data block address */\n} __packed;\n\nenum {\n\tCOLD_BIT_SHIFT = 0,\n\tFSYNC_BIT_SHIFT,\n\tDENT_BIT_SHIFT,\n\tOFFSET_BIT_SHIFT\n};\n\n#define OFFSET_BIT_MASK\t\t(0x07)\t/* (0x01 << OFFSET_BIT_SHIFT) - 1 */\n\nstruct node_footer {\n\t__le32 nid;\t\t/* node id */\n\t__le32 ino;\t\t/* inode nunmber */\n\t__le32 flag;\t\t/* include cold/fsync/dentry marks and offset */\n\t__le64 cp_ver;\t\t/* checkpoint version */\n\t__le32 next_blkaddr;\t/* next node page block address */\n} __packed;\n\nstruct f2fs_node {\n\t/* can be one of three types: inode, direct, and indirect types */\n\tunion {\n\t\tstruct f2fs_inode i;\n\t\tstruct direct_node dn;\n\t\tstruct indirect_node in;\n\t};\n\tstruct node_footer footer;\n} __packed;\n\n/*\n * For NAT entries\n */\n#define NAT_ENTRY_PER_BLOCK (PAGE_SIZE / sizeof(struct f2fs_nat_entry))\n#define NAT_ENTRY_BITMAP_SIZE\t((NAT_ENTRY_PER_BLOCK + 7) / 8)\n\nstruct f2fs_nat_entry {\n\t__u8 version;\t\t/* latest version of cached nat entry */\n\t__le32 ino;\t\t/* inode number */\n\t__le32 block_addr;\t/* block address */\n} __packed;\n\nstruct f2fs_nat_block {\n\tstruct f2fs_nat_entry entries[NAT_ENTRY_PER_BLOCK];\n} __packed;\n\n/*\n * For SIT entries\n *\n * Each segment is 2MB in size by default so that a bitmap for validity of\n * there-in blocks should occupy 64 bytes, 512 bits.\n * Not allow to change this.\n */\n#define SIT_VBLOCK_MAP_SIZE 64\n#define SIT_ENTRY_PER_BLOCK (PAGE_SIZE / sizeof(struct f2fs_sit_entry))\n\n/*\n * Note that f2fs_sit_entry->vblocks has the following bit-field information.\n * [15:10] : allocation type such as CURSEG_XXXX_TYPE\n * [9:0] : valid block count\n */\n#define SIT_VBLOCKS_SHIFT\t10\n#define SIT_VBLOCKS_MASK\t((1 << SIT_VBLOCKS_SHIFT) - 1)\n#define GET_SIT_VBLOCKS(raw_sit)\t\t\t\t\\\n\t(le16_to_cpu((raw_sit)->vblocks) & SIT_VBLOCKS_MASK)\n#define GET_SIT_TYPE(raw_sit)\t\t\t\t\t\\\n\t((le16_to_cpu((raw_sit)->vblocks) & ~SIT_VBLOCKS_MASK)\t\\\n\t >> SIT_VBLOCKS_SHIFT)\n\nstruct f2fs_sit_entry {\n\t__le16 vblocks;\t\t\t\t/* reference above */\n\t__u8 valid_map[SIT_VBLOCK_MAP_SIZE];\t/* bitmap for valid blocks */\n\t__le64 mtime;\t\t\t\t/* segment age for cleaning */\n} __packed;\n\nstruct f2fs_sit_block {\n\tstruct f2fs_sit_entry entries[SIT_ENTRY_PER_BLOCK];\n} __packed;\n\n/*\n * For segment summary\n *\n * One summary block contains exactly 512 summary entries, which represents\n * exactly 2MB segment by default. Not allow to change the basic units.\n *\n * NOTE: For initializing fields, you must use set_summary\n *\n * - If data page, nid represents dnode's nid\n * - If node page, nid represents the node page's nid.\n *\n * The ofs_in_node is used by only data page. It represents offset\n * from node's page's beginning to get a data block address.\n * ex) data_blkaddr = (block_t)(nodepage_start_address + ofs_in_node)\n */\n#define ENTRIES_IN_SUM\t\t512\n#define\tSUMMARY_SIZE\t\t(7)\t/* sizeof(struct summary) */\n#define\tSUM_FOOTER_SIZE\t\t(5)\t/* sizeof(struct summary_footer) */\n#define SUM_ENTRY_SIZE\t\t(SUMMARY_SIZE * ENTRIES_IN_SUM)\n\n/* a summary entry for a 4KB-sized block in a segment */\nstruct f2fs_summary {\n\t__le32 nid;\t\t/* parent node id */\n\tunion {\n\t\t__u8 reserved[3];\n\t\tstruct {\n\t\t\t__u8 version;\t\t/* node version number */\n\t\t\t__le16 ofs_in_node;\t/* block index in parent node */\n\t\t} __packed;\n\t};\n} __packed;\n\n/* summary block type, node or data, is stored to the summary_footer */\n#define SUM_TYPE_NODE\t\t(1)\n#define SUM_TYPE_DATA\t\t(0)\n\nstruct summary_footer {\n\tunsigned char entry_type;\t/* SUM_TYPE_XXX */\n\t__le32 check_sum;\t\t/* summary checksum */\n} __packed;\n\n#define SUM_JOURNAL_SIZE\t(F2FS_BLKSIZE - SUM_FOOTER_SIZE -\\\n\t\t\t\tSUM_ENTRY_SIZE)\n#define NAT_JOURNAL_ENTRIES\t((SUM_JOURNAL_SIZE - 2) /\\\n\t\t\t\tsizeof(struct nat_journal_entry))\n#define NAT_JOURNAL_RESERVED\t((SUM_JOURNAL_SIZE - 2) %\\\n\t\t\t\tsizeof(struct nat_journal_entry))\n#define SIT_JOURNAL_ENTRIES\t((SUM_JOURNAL_SIZE - 2) /\\\n\t\t\t\tsizeof(struct sit_journal_entry))\n#define SIT_JOURNAL_RESERVED\t((SUM_JOURNAL_SIZE - 2) %\\\n\t\t\t\tsizeof(struct sit_journal_entry))\n\n/* Reserved area should make size of f2fs_extra_info equals to\n * that of nat_journal and sit_journal.\n */\n#define EXTRA_INFO_RESERVED\t(SUM_JOURNAL_SIZE - 2 - 8)\n\n/*\n * frequently updated NAT/SIT entries can be stored in the spare area in\n * summary blocks\n */\nenum {\n\tNAT_JOURNAL = 0,\n\tSIT_JOURNAL\n};\n\nstruct nat_journal_entry {\n\t__le32 nid;\n\tstruct f2fs_nat_entry ne;\n} __packed;\n\nstruct nat_journal {\n\tstruct nat_journal_entry entries[NAT_JOURNAL_ENTRIES];\n\t__u8 reserved[NAT_JOURNAL_RESERVED];\n} __packed;\n\nstruct sit_journal_entry {\n\t__le32 segno;\n\tstruct f2fs_sit_entry se;\n} __packed;\n\nstruct sit_journal {\n\tstruct sit_journal_entry entries[SIT_JOURNAL_ENTRIES];\n\t__u8 reserved[SIT_JOURNAL_RESERVED];\n} __packed;\n\nstruct f2fs_extra_info {\n\t__le64 kbytes_written;\n\t__u8 reserved[EXTRA_INFO_RESERVED];\n} __packed;\n\nstruct f2fs_journal {\n\tunion {\n\t\t__le16 n_nats;\n\t\t__le16 n_sits;\n\t};\n\t/* spare area is used by NAT or SIT journals or extra info */\n\tunion {\n\t\tstruct nat_journal nat_j;\n\t\tstruct sit_journal sit_j;\n\t\tstruct f2fs_extra_info info;\n\t};\n} __packed;\n\n/* 4KB-sized summary block structure */\nstruct f2fs_summary_block {\n\tstruct f2fs_summary entries[ENTRIES_IN_SUM];\n\tstruct f2fs_journal journal;\n\tstruct summary_footer footer;\n} __packed;\n\n/*\n * For directory operations\n */\n#define F2FS_DOT_HASH\t\t0\n#define F2FS_DDOT_HASH\t\tF2FS_DOT_HASH\n#define F2FS_MAX_HASH\t\t(~((0x3ULL) << 62))\n#define F2FS_HASH_COL_BIT\t((0x1ULL) << 63)\n\ntypedef __le32\tf2fs_hash_t;\n\n/* One directory entry slot covers 8bytes-long file name */\n#define F2FS_SLOT_LEN\t\t8\n#define F2FS_SLOT_LEN_BITS\t3\n\n#define GET_DENTRY_SLOTS(x) (((x) + F2FS_SLOT_LEN - 1) >> F2FS_SLOT_LEN_BITS)\n\n/* MAX level for dir lookup */\n#define MAX_DIR_HASH_DEPTH\t63\n\n/* MAX buckets in one level of dir */\n#define MAX_DIR_BUCKETS\t\t(1 << ((MAX_DIR_HASH_DEPTH / 2) - 1))\n\n/*\n * space utilization of regular dentry and inline dentry\n *\t\tregular dentry\t\t\tinline dentry\n * bitmap\t1 * 27 = 27\t\t\t1 * 23 = 23\n * reserved\t1 * 3 = 3\t\t\t1 * 7 = 7\n * dentry\t11 * 214 = 2354\t\t\t11 * 182 = 2002\n * filename\t8 * 214 = 1712\t\t\t8 * 182 = 1456\n * total\t4096\t\t\t\t3488\n *\n * Note: there are more reserved space in inline dentry than in regular\n * dentry, when converting inline dentry we should handle this carefully.\n */\n#define NR_DENTRY_IN_BLOCK\t214\t/* the number of dentry in a block */\n#define SIZE_OF_DIR_ENTRY\t11\t/* by byte */\n#define SIZE_OF_DENTRY_BITMAP\t((NR_DENTRY_IN_BLOCK + BITS_PER_BYTE - 1) / \\\n\t\t\t\t\tBITS_PER_BYTE)\n#define SIZE_OF_RESERVED\t(PAGE_SIZE - ((SIZE_OF_DIR_ENTRY + \\\n\t\t\t\tF2FS_SLOT_LEN) * \\\n\t\t\t\tNR_DENTRY_IN_BLOCK + SIZE_OF_DENTRY_BITMAP))\n\n/* One directory entry slot representing F2FS_SLOT_LEN-sized file name */\nstruct f2fs_dir_entry {\n\t__le32 hash_code;\t/* hash code of file name */\n\t__le32 ino;\t\t/* inode number */\n\t__le16 name_len;\t/* lengh of file name */\n\t__u8 file_type;\t\t/* file type */\n} __packed;\n\n/* 4KB-sized directory entry block */\nstruct f2fs_dentry_block {\n\t/* validity bitmap for directory entries in each block */\n\t__u8 dentry_bitmap[SIZE_OF_DENTRY_BITMAP];\n\t__u8 reserved[SIZE_OF_RESERVED];\n\tstruct f2fs_dir_entry dentry[NR_DENTRY_IN_BLOCK];\n\t__u8 filename[NR_DENTRY_IN_BLOCK][F2FS_SLOT_LEN];\n} __packed;\n\n/* for inline dir */\n#define NR_INLINE_DENTRY\t(MAX_INLINE_DATA * BITS_PER_BYTE / \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tBITS_PER_BYTE + 1))\n#define INLINE_DENTRY_BITMAP_SIZE\t((NR_INLINE_DENTRY + \\\n\t\t\t\t\tBITS_PER_BYTE - 1) / BITS_PER_BYTE)\n#define INLINE_RESERVED_SIZE\t(MAX_INLINE_DATA - \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tNR_INLINE_DENTRY + INLINE_DENTRY_BITMAP_SIZE))\n\n/* inline directory entry structure */\nstruct f2fs_inline_dentry {\n\t__u8 dentry_bitmap[INLINE_DENTRY_BITMAP_SIZE];\n\t__u8 reserved[INLINE_RESERVED_SIZE];\n\tstruct f2fs_dir_entry dentry[NR_INLINE_DENTRY];\n\t__u8 filename[NR_INLINE_DENTRY][F2FS_SLOT_LEN];\n} __packed;\n\n/* file types used in inode_info->flags */\nenum {\n\tF2FS_FT_UNKNOWN,\n\tF2FS_FT_REG_FILE,\n\tF2FS_FT_DIR,\n\tF2FS_FT_CHRDEV,\n\tF2FS_FT_BLKDEV,\n\tF2FS_FT_FIFO,\n\tF2FS_FT_SOCK,\n\tF2FS_FT_SYMLINK,\n\tF2FS_FT_MAX\n};\n\n#define S_SHIFT 12\n\n#endif  /* _LINUX_F2FS_FS_H */\n"], "fixing_code": ["/*\n * fs/f2fs/super.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/statfs.h>\n#include <linux/buffer_head.h>\n#include <linux/backing-dev.h>\n#include <linux/kthread.h>\n#include <linux/parser.h>\n#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/random.h>\n#include <linux/exportfs.h>\n#include <linux/blkdev.h>\n#include <linux/f2fs_fs.h>\n#include <linux/sysfs.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include \"xattr.h\"\n#include \"gc.h\"\n#include \"trace.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/f2fs.h>\n\nstatic struct proc_dir_entry *f2fs_proc_root;\nstatic struct kmem_cache *f2fs_inode_cachep;\nstatic struct kset *f2fs_kset;\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\nchar *fault_name[FAULT_MAX] = {\n\t[FAULT_KMALLOC]\t\t= \"kmalloc\",\n\t[FAULT_PAGE_ALLOC]\t= \"page alloc\",\n\t[FAULT_ALLOC_NID]\t= \"alloc nid\",\n\t[FAULT_ORPHAN]\t\t= \"orphan\",\n\t[FAULT_BLOCK]\t\t= \"no more block\",\n\t[FAULT_DIR_DEPTH]\t= \"too big dir depth\",\n\t[FAULT_EVICT_INODE]\t= \"evict_inode fail\",\n\t[FAULT_TRUNCATE]\t= \"truncate fail\",\n\t[FAULT_IO]\t\t= \"IO error\",\n\t[FAULT_CHECKPOINT]\t= \"checkpoint error\",\n};\n\nstatic void f2fs_build_fault_attr(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tunsigned int rate)\n{\n\tstruct f2fs_fault_info *ffi = &sbi->fault_info;\n\n\tif (rate) {\n\t\tatomic_set(&ffi->inject_ops, 0);\n\t\tffi->inject_rate = rate;\n\t\tffi->inject_type = (1 << FAULT_MAX) - 1;\n\t} else {\n\t\tmemset(ffi, 0, sizeof(struct f2fs_fault_info));\n\t}\n}\n#endif\n\n/* f2fs-wide shrinker description */\nstatic struct shrinker f2fs_shrinker_info = {\n\t.scan_objects = f2fs_shrink_scan,\n\t.count_objects = f2fs_shrink_count,\n\t.seeks = DEFAULT_SEEKS,\n};\n\nenum {\n\tOpt_gc_background,\n\tOpt_disable_roll_forward,\n\tOpt_norecovery,\n\tOpt_discard,\n\tOpt_nodiscard,\n\tOpt_noheap,\n\tOpt_heap,\n\tOpt_user_xattr,\n\tOpt_nouser_xattr,\n\tOpt_acl,\n\tOpt_noacl,\n\tOpt_active_logs,\n\tOpt_disable_ext_identify,\n\tOpt_inline_xattr,\n\tOpt_noinline_xattr,\n\tOpt_inline_data,\n\tOpt_inline_dentry,\n\tOpt_noinline_dentry,\n\tOpt_flush_merge,\n\tOpt_noflush_merge,\n\tOpt_nobarrier,\n\tOpt_fastboot,\n\tOpt_extent_cache,\n\tOpt_noextent_cache,\n\tOpt_noinline_data,\n\tOpt_data_flush,\n\tOpt_mode,\n\tOpt_io_size_bits,\n\tOpt_fault_injection,\n\tOpt_lazytime,\n\tOpt_nolazytime,\n\tOpt_err,\n};\n\nstatic match_table_t f2fs_tokens = {\n\t{Opt_gc_background, \"background_gc=%s\"},\n\t{Opt_disable_roll_forward, \"disable_roll_forward\"},\n\t{Opt_norecovery, \"norecovery\"},\n\t{Opt_discard, \"discard\"},\n\t{Opt_nodiscard, \"nodiscard\"},\n\t{Opt_noheap, \"no_heap\"},\n\t{Opt_heap, \"heap\"},\n\t{Opt_user_xattr, \"user_xattr\"},\n\t{Opt_nouser_xattr, \"nouser_xattr\"},\n\t{Opt_acl, \"acl\"},\n\t{Opt_noacl, \"noacl\"},\n\t{Opt_active_logs, \"active_logs=%u\"},\n\t{Opt_disable_ext_identify, \"disable_ext_identify\"},\n\t{Opt_inline_xattr, \"inline_xattr\"},\n\t{Opt_noinline_xattr, \"noinline_xattr\"},\n\t{Opt_inline_data, \"inline_data\"},\n\t{Opt_inline_dentry, \"inline_dentry\"},\n\t{Opt_noinline_dentry, \"noinline_dentry\"},\n\t{Opt_flush_merge, \"flush_merge\"},\n\t{Opt_noflush_merge, \"noflush_merge\"},\n\t{Opt_nobarrier, \"nobarrier\"},\n\t{Opt_fastboot, \"fastboot\"},\n\t{Opt_extent_cache, \"extent_cache\"},\n\t{Opt_noextent_cache, \"noextent_cache\"},\n\t{Opt_noinline_data, \"noinline_data\"},\n\t{Opt_data_flush, \"data_flush\"},\n\t{Opt_mode, \"mode=%s\"},\n\t{Opt_io_size_bits, \"io_bits=%u\"},\n\t{Opt_fault_injection, \"fault_injection=%u\"},\n\t{Opt_lazytime, \"lazytime\"},\n\t{Opt_nolazytime, \"nolazytime\"},\n\t{Opt_err, NULL},\n};\n\n/* Sysfs support for f2fs */\nenum {\n\tGC_THREAD,\t/* struct f2fs_gc_thread */\n\tSM_INFO,\t/* struct f2fs_sm_info */\n\tDCC_INFO,\t/* struct discard_cmd_control */\n\tNM_INFO,\t/* struct f2fs_nm_info */\n\tF2FS_SBI,\t/* struct f2fs_sb_info */\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tFAULT_INFO_RATE,\t/* struct f2fs_fault_info */\n\tFAULT_INFO_TYPE,\t/* struct f2fs_fault_info */\n#endif\n};\n\nstruct f2fs_attr {\n\tstruct attribute attr;\n\tssize_t (*show)(struct f2fs_attr *, struct f2fs_sb_info *, char *);\n\tssize_t (*store)(struct f2fs_attr *, struct f2fs_sb_info *,\n\t\t\t const char *, size_t);\n\tint struct_type;\n\tint offset;\n};\n\nstatic unsigned char *__struct_ptr(struct f2fs_sb_info *sbi, int struct_type)\n{\n\tif (struct_type == GC_THREAD)\n\t\treturn (unsigned char *)sbi->gc_thread;\n\telse if (struct_type == SM_INFO)\n\t\treturn (unsigned char *)SM_I(sbi);\n\telse if (struct_type == DCC_INFO)\n\t\treturn (unsigned char *)SM_I(sbi)->dcc_info;\n\telse if (struct_type == NM_INFO)\n\t\treturn (unsigned char *)NM_I(sbi);\n\telse if (struct_type == F2FS_SBI)\n\t\treturn (unsigned char *)sbi;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\telse if (struct_type == FAULT_INFO_RATE ||\n\t\t\t\t\tstruct_type == FAULT_INFO_TYPE)\n\t\treturn (unsigned char *)&sbi->fault_info;\n#endif\n\treturn NULL;\n}\n\nstatic ssize_t lifetime_write_kbytes_show(struct f2fs_attr *a,\n\t\tstruct f2fs_sb_info *sbi, char *buf)\n{\n\tstruct super_block *sb = sbi->sb;\n\n\tif (!sb->s_bdev->bd_part)\n\t\treturn snprintf(buf, PAGE_SIZE, \"0\\n\");\n\n\treturn snprintf(buf, PAGE_SIZE, \"%llu\\n\",\n\t\t(unsigned long long)(sbi->kbytes_written +\n\t\t\tBD_PART_WRITTEN(sbi)));\n}\n\nstatic ssize_t f2fs_sbi_show(struct f2fs_attr *a,\n\t\t\tstruct f2fs_sb_info *sbi, char *buf)\n{\n\tunsigned char *ptr = NULL;\n\tunsigned int *ui;\n\n\tptr = __struct_ptr(sbi, a->struct_type);\n\tif (!ptr)\n\t\treturn -EINVAL;\n\n\tui = (unsigned int *)(ptr + a->offset);\n\n\treturn snprintf(buf, PAGE_SIZE, \"%u\\n\", *ui);\n}\n\nstatic ssize_t f2fs_sbi_store(struct f2fs_attr *a,\n\t\t\tstruct f2fs_sb_info *sbi,\n\t\t\tconst char *buf, size_t count)\n{\n\tunsigned char *ptr;\n\tunsigned long t;\n\tunsigned int *ui;\n\tssize_t ret;\n\n\tptr = __struct_ptr(sbi, a->struct_type);\n\tif (!ptr)\n\t\treturn -EINVAL;\n\n\tui = (unsigned int *)(ptr + a->offset);\n\n\tret = kstrtoul(skip_spaces(buf), 0, &t);\n\tif (ret < 0)\n\t\treturn ret;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (a->struct_type == FAULT_INFO_TYPE && t >= (1 << FAULT_MAX))\n\t\treturn -EINVAL;\n#endif\n\t*ui = t;\n\treturn count;\n}\n\nstatic ssize_t f2fs_attr_show(struct kobject *kobj,\n\t\t\t\tstruct attribute *attr, char *buf)\n{\n\tstruct f2fs_sb_info *sbi = container_of(kobj, struct f2fs_sb_info,\n\t\t\t\t\t\t\t\ts_kobj);\n\tstruct f2fs_attr *a = container_of(attr, struct f2fs_attr, attr);\n\n\treturn a->show ? a->show(a, sbi, buf) : 0;\n}\n\nstatic ssize_t f2fs_attr_store(struct kobject *kobj, struct attribute *attr,\n\t\t\t\t\t\tconst char *buf, size_t len)\n{\n\tstruct f2fs_sb_info *sbi = container_of(kobj, struct f2fs_sb_info,\n\t\t\t\t\t\t\t\t\ts_kobj);\n\tstruct f2fs_attr *a = container_of(attr, struct f2fs_attr, attr);\n\n\treturn a->store ? a->store(a, sbi, buf, len) : 0;\n}\n\nstatic void f2fs_sb_release(struct kobject *kobj)\n{\n\tstruct f2fs_sb_info *sbi = container_of(kobj, struct f2fs_sb_info,\n\t\t\t\t\t\t\t\ts_kobj);\n\tcomplete(&sbi->s_kobj_unregister);\n}\n\n#define F2FS_ATTR_OFFSET(_struct_type, _name, _mode, _show, _store, _offset) \\\nstatic struct f2fs_attr f2fs_attr_##_name = {\t\t\t\\\n\t.attr = {.name = __stringify(_name), .mode = _mode },\t\\\n\t.show\t= _show,\t\t\t\t\t\\\n\t.store\t= _store,\t\t\t\t\t\\\n\t.struct_type = _struct_type,\t\t\t\t\\\n\t.offset = _offset\t\t\t\t\t\\\n}\n\n#define F2FS_RW_ATTR(struct_type, struct_name, name, elname)\t\\\n\tF2FS_ATTR_OFFSET(struct_type, name, 0644,\t\t\\\n\t\tf2fs_sbi_show, f2fs_sbi_store,\t\t\t\\\n\t\toffsetof(struct struct_name, elname))\n\n#define F2FS_GENERAL_RO_ATTR(name) \\\nstatic struct f2fs_attr f2fs_attr_##name = __ATTR(name, 0444, name##_show, NULL)\n\nF2FS_RW_ATTR(GC_THREAD, f2fs_gc_kthread, gc_min_sleep_time, min_sleep_time);\nF2FS_RW_ATTR(GC_THREAD, f2fs_gc_kthread, gc_max_sleep_time, max_sleep_time);\nF2FS_RW_ATTR(GC_THREAD, f2fs_gc_kthread, gc_no_gc_sleep_time, no_gc_sleep_time);\nF2FS_RW_ATTR(GC_THREAD, f2fs_gc_kthread, gc_idle, gc_idle);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, reclaim_segments, rec_prefree_segments);\nF2FS_RW_ATTR(DCC_INFO, discard_cmd_control, max_small_discards, max_discards);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, batched_trim_sections, trim_sections);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, ipu_policy, ipu_policy);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, min_ipu_util, min_ipu_util);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, min_fsync_blocks, min_fsync_blocks);\nF2FS_RW_ATTR(SM_INFO, f2fs_sm_info, min_hot_blocks, min_hot_blocks);\nF2FS_RW_ATTR(NM_INFO, f2fs_nm_info, ram_thresh, ram_thresh);\nF2FS_RW_ATTR(NM_INFO, f2fs_nm_info, ra_nid_pages, ra_nid_pages);\nF2FS_RW_ATTR(NM_INFO, f2fs_nm_info, dirty_nats_ratio, dirty_nats_ratio);\nF2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, max_victim_search, max_victim_search);\nF2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, dir_level, dir_level);\nF2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, cp_interval, interval_time[CP_TIME]);\nF2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, idle_interval, interval_time[REQ_TIME]);\n#ifdef CONFIG_F2FS_FAULT_INJECTION\nF2FS_RW_ATTR(FAULT_INFO_RATE, f2fs_fault_info, inject_rate, inject_rate);\nF2FS_RW_ATTR(FAULT_INFO_TYPE, f2fs_fault_info, inject_type, inject_type);\n#endif\nF2FS_GENERAL_RO_ATTR(lifetime_write_kbytes);\n\n#define ATTR_LIST(name) (&f2fs_attr_##name.attr)\nstatic struct attribute *f2fs_attrs[] = {\n\tATTR_LIST(gc_min_sleep_time),\n\tATTR_LIST(gc_max_sleep_time),\n\tATTR_LIST(gc_no_gc_sleep_time),\n\tATTR_LIST(gc_idle),\n\tATTR_LIST(reclaim_segments),\n\tATTR_LIST(max_small_discards),\n\tATTR_LIST(batched_trim_sections),\n\tATTR_LIST(ipu_policy),\n\tATTR_LIST(min_ipu_util),\n\tATTR_LIST(min_fsync_blocks),\n\tATTR_LIST(min_hot_blocks),\n\tATTR_LIST(max_victim_search),\n\tATTR_LIST(dir_level),\n\tATTR_LIST(ram_thresh),\n\tATTR_LIST(ra_nid_pages),\n\tATTR_LIST(dirty_nats_ratio),\n\tATTR_LIST(cp_interval),\n\tATTR_LIST(idle_interval),\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tATTR_LIST(inject_rate),\n\tATTR_LIST(inject_type),\n#endif\n\tATTR_LIST(lifetime_write_kbytes),\n\tNULL,\n};\n\nstatic const struct sysfs_ops f2fs_attr_ops = {\n\t.show\t= f2fs_attr_show,\n\t.store\t= f2fs_attr_store,\n};\n\nstatic struct kobj_type f2fs_ktype = {\n\t.default_attrs\t= f2fs_attrs,\n\t.sysfs_ops\t= &f2fs_attr_ops,\n\t.release\t= f2fs_sb_release,\n};\n\nvoid f2fs_msg(struct super_block *sb, const char *level, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(\"%sF2FS-fs (%s): %pV\\n\", level, sb->s_id, &vaf);\n\tva_end(args);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct f2fs_inode_info *fi = (struct f2fs_inode_info *) foo;\n\n\tinode_init_once(&fi->vfs_inode);\n}\n\nstatic int parse_options(struct super_block *sb, char *options)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct request_queue *q;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tchar *p, *name;\n\tint arg = 0;\n\n\tif (!options)\n\t\treturn 0;\n\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tint token;\n\t\tif (!*p)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Initialize args struct so we know whether arg was\n\t\t * found; some options take optional arguments.\n\t\t */\n\t\targs[0].to = args[0].from = NULL;\n\t\ttoken = match_token(p, f2fs_tokens, args);\n\n\t\tswitch (token) {\n\t\tcase Opt_gc_background:\n\t\t\tname = match_strdup(&args[0]);\n\n\t\t\tif (!name)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (strlen(name) == 2 && !strncmp(name, \"on\", 2)) {\n\t\t\t\tset_opt(sbi, BG_GC);\n\t\t\t\tclear_opt(sbi, FORCE_FG_GC);\n\t\t\t} else if (strlen(name) == 3 && !strncmp(name, \"off\", 3)) {\n\t\t\t\tclear_opt(sbi, BG_GC);\n\t\t\t\tclear_opt(sbi, FORCE_FG_GC);\n\t\t\t} else if (strlen(name) == 4 && !strncmp(name, \"sync\", 4)) {\n\t\t\t\tset_opt(sbi, BG_GC);\n\t\t\t\tset_opt(sbi, FORCE_FG_GC);\n\t\t\t} else {\n\t\t\t\tkfree(name);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tkfree(name);\n\t\t\tbreak;\n\t\tcase Opt_disable_roll_forward:\n\t\t\tset_opt(sbi, DISABLE_ROLL_FORWARD);\n\t\t\tbreak;\n\t\tcase Opt_norecovery:\n\t\t\t/* this option mounts f2fs with ro */\n\t\t\tset_opt(sbi, DISABLE_ROLL_FORWARD);\n\t\t\tif (!f2fs_readonly(sb))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase Opt_discard:\n\t\t\tq = bdev_get_queue(sb->s_bdev);\n\t\t\tif (blk_queue_discard(q)) {\n\t\t\t\tset_opt(sbi, DISCARD);\n\t\t\t} else if (!f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"mounting with \\\"discard\\\" option, but \"\n\t\t\t\t\t\"the device does not support discard\");\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Opt_nodiscard:\n\t\t\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"discard is required for zoned block devices\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tclear_opt(sbi, DISCARD);\n\t\t\tbreak;\n\t\tcase Opt_noheap:\n\t\t\tset_opt(sbi, NOHEAP);\n\t\t\tbreak;\n\t\tcase Opt_heap:\n\t\t\tclear_opt(sbi, NOHEAP);\n\t\t\tbreak;\n#ifdef CONFIG_F2FS_FS_XATTR\n\t\tcase Opt_user_xattr:\n\t\t\tset_opt(sbi, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tclear_opt(sbi, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_inline_xattr:\n\t\t\tset_opt(sbi, INLINE_XATTR);\n\t\t\tbreak;\n\t\tcase Opt_noinline_xattr:\n\t\t\tclear_opt(sbi, INLINE_XATTR);\n\t\t\tbreak;\n#else\n\t\tcase Opt_user_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"user_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"nouser_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_inline_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"inline_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noinline_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"noinline_xattr options not supported\");\n\t\t\tbreak;\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\t\tcase Opt_acl:\n\t\t\tset_opt(sbi, POSIX_ACL);\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tclear_opt(sbi, POSIX_ACL);\n\t\t\tbreak;\n#else\n\t\tcase Opt_acl:\n\t\t\tf2fs_msg(sb, KERN_INFO, \"acl options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tf2fs_msg(sb, KERN_INFO, \"noacl options not supported\");\n\t\t\tbreak;\n#endif\n\t\tcase Opt_active_logs:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (arg != 2 && arg != 4 && arg != NR_CURSEG_TYPE)\n\t\t\t\treturn -EINVAL;\n\t\t\tsbi->active_logs = arg;\n\t\t\tbreak;\n\t\tcase Opt_disable_ext_identify:\n\t\t\tset_opt(sbi, DISABLE_EXT_IDENTIFY);\n\t\t\tbreak;\n\t\tcase Opt_inline_data:\n\t\t\tset_opt(sbi, INLINE_DATA);\n\t\t\tbreak;\n\t\tcase Opt_inline_dentry:\n\t\t\tset_opt(sbi, INLINE_DENTRY);\n\t\t\tbreak;\n\t\tcase Opt_noinline_dentry:\n\t\t\tclear_opt(sbi, INLINE_DENTRY);\n\t\t\tbreak;\n\t\tcase Opt_flush_merge:\n\t\t\tset_opt(sbi, FLUSH_MERGE);\n\t\t\tbreak;\n\t\tcase Opt_noflush_merge:\n\t\t\tclear_opt(sbi, FLUSH_MERGE);\n\t\t\tbreak;\n\t\tcase Opt_nobarrier:\n\t\t\tset_opt(sbi, NOBARRIER);\n\t\t\tbreak;\n\t\tcase Opt_fastboot:\n\t\t\tset_opt(sbi, FASTBOOT);\n\t\t\tbreak;\n\t\tcase Opt_extent_cache:\n\t\t\tset_opt(sbi, EXTENT_CACHE);\n\t\t\tbreak;\n\t\tcase Opt_noextent_cache:\n\t\t\tclear_opt(sbi, EXTENT_CACHE);\n\t\t\tbreak;\n\t\tcase Opt_noinline_data:\n\t\t\tclear_opt(sbi, INLINE_DATA);\n\t\t\tbreak;\n\t\tcase Opt_data_flush:\n\t\t\tset_opt(sbi, DATA_FLUSH);\n\t\t\tbreak;\n\t\tcase Opt_mode:\n\t\t\tname = match_strdup(&args[0]);\n\n\t\t\tif (!name)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (strlen(name) == 8 &&\n\t\t\t\t\t!strncmp(name, \"adaptive\", 8)) {\n\t\t\t\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\t \"adaptive mode is not allowed with \"\n\t\t\t\t\t\t \"zoned block device feature\");\n\t\t\t\t\tkfree(name);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tset_opt_mode(sbi, F2FS_MOUNT_ADAPTIVE);\n\t\t\t} else if (strlen(name) == 3 &&\n\t\t\t\t\t!strncmp(name, \"lfs\", 3)) {\n\t\t\t\tset_opt_mode(sbi, F2FS_MOUNT_LFS);\n\t\t\t} else {\n\t\t\t\tkfree(name);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tkfree(name);\n\t\t\tbreak;\n\t\tcase Opt_io_size_bits:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (arg > __ilog2_u32(BIO_MAX_PAGES)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"Not support %d, larger than %d\",\n\t\t\t\t\t1 << arg, BIO_MAX_PAGES);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tsbi->write_io_size_bits = arg;\n\t\t\tbreak;\n\t\tcase Opt_fault_injection:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\t\t\tf2fs_build_fault_attr(sbi, arg);\n\t\t\tset_opt(sbi, FAULT_INJECTION);\n#else\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"FAULT_INJECTION was not selected\");\n#endif\n\t\t\tbreak;\n\t\tcase Opt_lazytime:\n\t\t\tsb->s_flags |= MS_LAZYTIME;\n\t\t\tbreak;\n\t\tcase Opt_nolazytime:\n\t\t\tsb->s_flags &= ~MS_LAZYTIME;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Unrecognized mount option \\\"%s\\\" or missing value\",\n\t\t\t\tp);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (F2FS_IO_SIZE_BITS(sbi) && !test_opt(sbi, LFS)) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Should set mode=lfs with %uKB-sized IO\",\n\t\t\t\tF2FS_IO_SIZE_KB(sbi));\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic struct inode *f2fs_alloc_inode(struct super_block *sb)\n{\n\tstruct f2fs_inode_info *fi;\n\n\tfi = kmem_cache_alloc(f2fs_inode_cachep, GFP_F2FS_ZERO);\n\tif (!fi)\n\t\treturn NULL;\n\n\tinit_once((void *) fi);\n\n\t/* Initialize f2fs-specific inode info */\n\tfi->vfs_inode.i_version = 1;\n\tatomic_set(&fi->dirty_pages, 0);\n\tfi->i_current_depth = 1;\n\tfi->i_advise = 0;\n\tinit_rwsem(&fi->i_sem);\n\tINIT_LIST_HEAD(&fi->dirty_list);\n\tINIT_LIST_HEAD(&fi->gdirty_list);\n\tINIT_LIST_HEAD(&fi->inmem_pages);\n\tmutex_init(&fi->inmem_lock);\n\tinit_rwsem(&fi->dio_rwsem[READ]);\n\tinit_rwsem(&fi->dio_rwsem[WRITE]);\n\n\t/* Will be used by directory only */\n\tfi->i_dir_level = F2FS_SB(sb)->dir_level;\n\treturn &fi->vfs_inode;\n}\n\nstatic int f2fs_drop_inode(struct inode *inode)\n{\n\tint ret;\n\t/*\n\t * This is to avoid a deadlock condition like below.\n\t * writeback_single_inode(inode)\n\t *  - f2fs_write_data_page\n\t *    - f2fs_gc -> iput -> evict\n\t *       - inode_wait_for_writeback(inode)\n\t */\n\tif ((!inode_unhashed(inode) && inode->i_state & I_SYNC)) {\n\t\tif (!inode->i_nlink && !is_bad_inode(inode)) {\n\t\t\t/* to avoid evict_inode call simultaneously */\n\t\t\tatomic_inc(&inode->i_count);\n\t\t\tspin_unlock(&inode->i_lock);\n\n\t\t\t/* some remained atomic pages should discarded */\n\t\t\tif (f2fs_is_atomic_file(inode))\n\t\t\t\tdrop_inmem_pages(inode);\n\n\t\t\t/* should remain fi->extent_tree for writepage */\n\t\t\tf2fs_destroy_extent_node(inode);\n\n\t\t\tsb_start_intwrite(inode->i_sb);\n\t\t\tf2fs_i_size_write(inode, 0);\n\n\t\t\tif (F2FS_HAS_BLOCKS(inode))\n\t\t\t\tf2fs_truncate(inode);\n\n\t\t\tsb_end_intwrite(inode->i_sb);\n\n\t\t\tfscrypt_put_encryption_info(inode, NULL);\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tatomic_dec(&inode->i_count);\n\t\t}\n\t\ttrace_f2fs_drop_inode(inode, 0);\n\t\treturn 0;\n\t}\n\tret = generic_drop_inode(inode);\n\ttrace_f2fs_drop_inode(inode, ret);\n\treturn ret;\n}\n\nint f2fs_inode_dirtied(struct inode *inode, bool sync)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint ret = 0;\n\n\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\tif (is_inode_flag_set(inode, FI_DIRTY_INODE)) {\n\t\tret = 1;\n\t} else {\n\t\tset_inode_flag(inode, FI_DIRTY_INODE);\n\t\tstat_inc_dirty_inode(sbi, DIRTY_META);\n\t}\n\tif (sync && list_empty(&F2FS_I(inode)->gdirty_list)) {\n\t\tlist_add_tail(&F2FS_I(inode)->gdirty_list,\n\t\t\t\t&sbi->inode_list[DIRTY_META]);\n\t\tinc_page_count(sbi, F2FS_DIRTY_IMETA);\n\t}\n\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\treturn ret;\n}\n\nvoid f2fs_inode_synced(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\tif (!is_inode_flag_set(inode, FI_DIRTY_INODE)) {\n\t\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\t\treturn;\n\t}\n\tif (!list_empty(&F2FS_I(inode)->gdirty_list)) {\n\t\tlist_del_init(&F2FS_I(inode)->gdirty_list);\n\t\tdec_page_count(sbi, F2FS_DIRTY_IMETA);\n\t}\n\tclear_inode_flag(inode, FI_DIRTY_INODE);\n\tclear_inode_flag(inode, FI_AUTO_RECOVER);\n\tstat_dec_dirty_inode(F2FS_I_SB(inode), DIRTY_META);\n\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n}\n\n/*\n * f2fs_dirty_inode() is called from __mark_inode_dirty()\n *\n * We should call set_dirty_inode to write the dirty inode through write_inode.\n */\nstatic void f2fs_dirty_inode(struct inode *inode, int flags)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino == F2FS_NODE_INO(sbi) ||\n\t\t\tinode->i_ino == F2FS_META_INO(sbi))\n\t\treturn;\n\n\tif (flags == I_DIRTY_TIME)\n\t\treturn;\n\n\tif (is_inode_flag_set(inode, FI_AUTO_RECOVER))\n\t\tclear_inode_flag(inode, FI_AUTO_RECOVER);\n\n\tf2fs_inode_dirtied(inode, false);\n}\n\nstatic void f2fs_i_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(f2fs_inode_cachep, F2FS_I(inode));\n}\n\nstatic void f2fs_destroy_inode(struct inode *inode)\n{\n\tcall_rcu(&inode->i_rcu, f2fs_i_callback);\n}\n\nstatic void destroy_percpu_info(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_destroy(&sbi->alloc_valid_block_count);\n\tpercpu_counter_destroy(&sbi->total_valid_inode_count);\n}\n\nstatic void destroy_device_list(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\tblkdev_put(FDEV(i).bdev, FMODE_EXCL);\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\tkfree(FDEV(i).blkz_type);\n#endif\n\t}\n\tkfree(sbi->devs);\n}\n\nstatic void f2fs_put_super(struct super_block *sb)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\n\tif (sbi->s_proc) {\n\t\tremove_proc_entry(\"segment_info\", sbi->s_proc);\n\t\tremove_proc_entry(\"segment_bits\", sbi->s_proc);\n\t\tremove_proc_entry(sb->s_id, f2fs_proc_root);\n\t}\n\tkobject_del(&sbi->s_kobj);\n\n\tstop_gc_thread(sbi);\n\n\t/* prevent remaining shrinker jobs */\n\tmutex_lock(&sbi->umount_mutex);\n\n\t/*\n\t * We don't need to do checkpoint when superblock is clean.\n\t * But, the previous checkpoint was not done by umount, it needs to do\n\t * clean checkpoint again.\n\t */\n\tif (is_sbi_flag_set(sbi, SBI_IS_DIRTY) ||\n\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* be sure to wait for any on-going discard commands */\n\tf2fs_wait_discard_bios(sbi);\n\n\t/* write_checkpoint can update stat informaion */\n\tf2fs_destroy_stats(sbi);\n\n\t/*\n\t * normally superblock is clean, so we need to release this.\n\t * In addition, EIO will skip do checkpoint, we need this as well.\n\t */\n\trelease_ino_entry(sbi, true);\n\n\tf2fs_leave_shrinker(sbi);\n\tmutex_unlock(&sbi->umount_mutex);\n\n\t/* our cp_error case, we can wait for any writeback page */\n\tf2fs_flush_merged_bios(sbi);\n\n\tiput(sbi->node_inode);\n\tiput(sbi->meta_inode);\n\n\t/* destroy f2fs internal modules */\n\tdestroy_node_manager(sbi);\n\tdestroy_segment_manager(sbi);\n\n\tkfree(sbi->ckpt);\n\tkobject_put(&sbi->s_kobj);\n\twait_for_completion(&sbi->s_kobj_unregister);\n\n\tsb->s_fs_info = NULL;\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi->raw_super);\n\n\tdestroy_device_list(sbi);\n\tmempool_destroy(sbi->write_io_dummy);\n\tdestroy_percpu_info(sbi);\n\tkfree(sbi);\n}\n\nint f2fs_sync_fs(struct super_block *sb, int sync)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tint err = 0;\n\n\ttrace_f2fs_sync_fs(sb, sync);\n\n\tif (sync) {\n\t\tstruct cp_control cpc;\n\n\t\tcpc.reason = __get_cp_reason(sbi);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t}\n\tf2fs_trace_ios(NULL, 1);\n\n\treturn err;\n}\n\nstatic int f2fs_freeze(struct super_block *sb)\n{\n\tif (f2fs_readonly(sb))\n\t\treturn 0;\n\n\t/* IO error happened before */\n\tif (unlikely(f2fs_cp_error(F2FS_SB(sb))))\n\t\treturn -EIO;\n\n\t/* must be clean, since sync_filesystem() was already called */\n\tif (is_sbi_flag_set(F2FS_SB(sb), SBI_IS_DIRTY))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int f2fs_unfreeze(struct super_block *sb)\n{\n\treturn 0;\n}\n\nstatic int f2fs_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct super_block *sb = dentry->d_sb;\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tu64 id = huge_encode_dev(sb->s_bdev->bd_dev);\n\tblock_t total_count, user_block_count, start_count, ovp_count;\n\n\ttotal_count = le64_to_cpu(sbi->raw_super->block_count);\n\tuser_block_count = sbi->user_block_count;\n\tstart_count = le32_to_cpu(sbi->raw_super->segment0_blkaddr);\n\tovp_count = SM_I(sbi)->ovp_segments << sbi->log_blocks_per_seg;\n\tbuf->f_type = F2FS_SUPER_MAGIC;\n\tbuf->f_bsize = sbi->blocksize;\n\n\tbuf->f_blocks = total_count - start_count;\n\tbuf->f_bfree = user_block_count - valid_user_blocks(sbi) + ovp_count;\n\tbuf->f_bavail = user_block_count - valid_user_blocks(sbi);\n\n\tbuf->f_files = sbi->total_node_count - F2FS_RESERVED_NODE_NUM;\n\tbuf->f_ffree = min(buf->f_files - valid_node_count(sbi),\n\t\t\t\t\t\t\tbuf->f_bavail);\n\n\tbuf->f_namelen = F2FS_NAME_LEN;\n\tbuf->f_fsid.val[0] = (u32)id;\n\tbuf->f_fsid.val[1] = (u32)(id >> 32);\n\n\treturn 0;\n}\n\nstatic int f2fs_show_options(struct seq_file *seq, struct dentry *root)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(root->d_sb);\n\n\tif (!f2fs_readonly(sbi->sb) && test_opt(sbi, BG_GC)) {\n\t\tif (test_opt(sbi, FORCE_FG_GC))\n\t\t\tseq_printf(seq, \",background_gc=%s\", \"sync\");\n\t\telse\n\t\t\tseq_printf(seq, \",background_gc=%s\", \"on\");\n\t} else {\n\t\tseq_printf(seq, \",background_gc=%s\", \"off\");\n\t}\n\tif (test_opt(sbi, DISABLE_ROLL_FORWARD))\n\t\tseq_puts(seq, \",disable_roll_forward\");\n\tif (test_opt(sbi, DISCARD))\n\t\tseq_puts(seq, \",discard\");\n\tif (test_opt(sbi, NOHEAP))\n\t\tseq_puts(seq, \",no_heap\");\n\telse\n\t\tseq_puts(seq, \",heap\");\n#ifdef CONFIG_F2FS_FS_XATTR\n\tif (test_opt(sbi, XATTR_USER))\n\t\tseq_puts(seq, \",user_xattr\");\n\telse\n\t\tseq_puts(seq, \",nouser_xattr\");\n\tif (test_opt(sbi, INLINE_XATTR))\n\t\tseq_puts(seq, \",inline_xattr\");\n\telse\n\t\tseq_puts(seq, \",noinline_xattr\");\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\tif (test_opt(sbi, POSIX_ACL))\n\t\tseq_puts(seq, \",acl\");\n\telse\n\t\tseq_puts(seq, \",noacl\");\n#endif\n\tif (test_opt(sbi, DISABLE_EXT_IDENTIFY))\n\t\tseq_puts(seq, \",disable_ext_identify\");\n\tif (test_opt(sbi, INLINE_DATA))\n\t\tseq_puts(seq, \",inline_data\");\n\telse\n\t\tseq_puts(seq, \",noinline_data\");\n\tif (test_opt(sbi, INLINE_DENTRY))\n\t\tseq_puts(seq, \",inline_dentry\");\n\telse\n\t\tseq_puts(seq, \",noinline_dentry\");\n\tif (!f2fs_readonly(sbi->sb) && test_opt(sbi, FLUSH_MERGE))\n\t\tseq_puts(seq, \",flush_merge\");\n\tif (test_opt(sbi, NOBARRIER))\n\t\tseq_puts(seq, \",nobarrier\");\n\tif (test_opt(sbi, FASTBOOT))\n\t\tseq_puts(seq, \",fastboot\");\n\tif (test_opt(sbi, EXTENT_CACHE))\n\t\tseq_puts(seq, \",extent_cache\");\n\telse\n\t\tseq_puts(seq, \",noextent_cache\");\n\tif (test_opt(sbi, DATA_FLUSH))\n\t\tseq_puts(seq, \",data_flush\");\n\n\tseq_puts(seq, \",mode=\");\n\tif (test_opt(sbi, ADAPTIVE))\n\t\tseq_puts(seq, \"adaptive\");\n\telse if (test_opt(sbi, LFS))\n\t\tseq_puts(seq, \"lfs\");\n\tseq_printf(seq, \",active_logs=%u\", sbi->active_logs);\n\tif (F2FS_IO_SIZE_BITS(sbi))\n\t\tseq_printf(seq, \",io_size=%uKB\", F2FS_IO_SIZE_KB(sbi));\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (test_opt(sbi, FAULT_INJECTION))\n\t\tseq_puts(seq, \",fault_injection\");\n#endif\n\n\treturn 0;\n}\n\nstatic int segment_info_seq_show(struct seq_file *seq, void *offset)\n{\n\tstruct super_block *sb = seq->private;\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tunsigned int total_segs =\n\t\t\tle32_to_cpu(sbi->raw_super->segment_count_main);\n\tint i;\n\n\tseq_puts(seq, \"format: segment_type|valid_blocks\\n\"\n\t\t\"segment_type(0:HD, 1:WD, 2:CD, 3:HN, 4:WN, 5:CN)\\n\");\n\n\tfor (i = 0; i < total_segs; i++) {\n\t\tstruct seg_entry *se = get_seg_entry(sbi, i);\n\n\t\tif ((i % 10) == 0)\n\t\t\tseq_printf(seq, \"%-10d\", i);\n\t\tseq_printf(seq, \"%d|%-3u\", se->type,\n\t\t\t\t\tget_valid_blocks(sbi, i, false));\n\t\tif ((i % 10) == 9 || i == (total_segs - 1))\n\t\t\tseq_putc(seq, '\\n');\n\t\telse\n\t\t\tseq_putc(seq, ' ');\n\t}\n\n\treturn 0;\n}\n\nstatic int segment_bits_seq_show(struct seq_file *seq, void *offset)\n{\n\tstruct super_block *sb = seq->private;\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tunsigned int total_segs =\n\t\t\tle32_to_cpu(sbi->raw_super->segment_count_main);\n\tint i, j;\n\n\tseq_puts(seq, \"format: segment_type|valid_blocks|bitmaps\\n\"\n\t\t\"segment_type(0:HD, 1:WD, 2:CD, 3:HN, 4:WN, 5:CN)\\n\");\n\n\tfor (i = 0; i < total_segs; i++) {\n\t\tstruct seg_entry *se = get_seg_entry(sbi, i);\n\n\t\tseq_printf(seq, \"%-10d\", i);\n\t\tseq_printf(seq, \"%d|%-3u|\", se->type,\n\t\t\t\t\tget_valid_blocks(sbi, i, false));\n\t\tfor (j = 0; j < SIT_VBLOCK_MAP_SIZE; j++)\n\t\t\tseq_printf(seq, \" %.2x\", se->cur_valid_map[j]);\n\t\tseq_putc(seq, '\\n');\n\t}\n\treturn 0;\n}\n\n#define F2FS_PROC_FILE_DEF(_name)\t\t\t\t\t\\\nstatic int _name##_open_fs(struct inode *inode, struct file *file)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn single_open(file, _name##_seq_show, PDE_DATA(inode));\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic const struct file_operations f2fs_seq_##_name##_fops = {\t\t\\\n\t.open = _name##_open_fs,\t\t\t\t\t\\\n\t.read = seq_read,\t\t\t\t\t\t\\\n\t.llseek = seq_lseek,\t\t\t\t\t\t\\\n\t.release = single_release,\t\t\t\t\t\\\n};\n\nF2FS_PROC_FILE_DEF(segment_info);\nF2FS_PROC_FILE_DEF(segment_bits);\n\nstatic void default_options(struct f2fs_sb_info *sbi)\n{\n\t/* init some FS parameters */\n\tsbi->active_logs = NR_CURSEG_TYPE;\n\n\tset_opt(sbi, BG_GC);\n\tset_opt(sbi, INLINE_XATTR);\n\tset_opt(sbi, INLINE_DATA);\n\tset_opt(sbi, INLINE_DENTRY);\n\tset_opt(sbi, EXTENT_CACHE);\n\tset_opt(sbi, NOHEAP);\n\tsbi->sb->s_flags |= MS_LAZYTIME;\n\tset_opt(sbi, FLUSH_MERGE);\n\tif (f2fs_sb_mounted_blkzoned(sbi->sb)) {\n\t\tset_opt_mode(sbi, F2FS_MOUNT_LFS);\n\t\tset_opt(sbi, DISCARD);\n\t} else {\n\t\tset_opt_mode(sbi, F2FS_MOUNT_ADAPTIVE);\n\t}\n\n#ifdef CONFIG_F2FS_FS_XATTR\n\tset_opt(sbi, XATTR_USER);\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\tset_opt(sbi, POSIX_ACL);\n#endif\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tf2fs_build_fault_attr(sbi, 0);\n#endif\n}\n\nstatic int f2fs_remount(struct super_block *sb, int *flags, char *data)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct f2fs_mount_info org_mount_opt;\n\tint err, active_logs;\n\tbool need_restart_gc = false;\n\tbool need_stop_gc = false;\n\tbool no_extent_cache = !test_opt(sbi, EXTENT_CACHE);\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct f2fs_fault_info ffi = sbi->fault_info;\n#endif\n\n\t/*\n\t * Save the old mount options in case we\n\t * need to restore them.\n\t */\n\torg_mount_opt = sbi->mount_opt;\n\tactive_logs = sbi->active_logs;\n\n\t/* recover superblocks we couldn't write due to previous RO mount */\n\tif (!(*flags & MS_RDONLY) && is_sbi_flag_set(sbi, SBI_NEED_SB_WRITE)) {\n\t\terr = f2fs_commit_super(sbi, false);\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Try to recover all the superblocks, ret: %d\", err);\n\t\tif (!err)\n\t\t\tclear_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t}\n\n\tsbi->mount_opt.opt = 0;\n\tdefault_options(sbi);\n\n\t/* parse mount options */\n\terr = parse_options(sb, data);\n\tif (err)\n\t\tgoto restore_opts;\n\n\t/*\n\t * Previous and new state of filesystem is RO,\n\t * so skip checking GC and FLUSH_MERGE conditions.\n\t */\n\tif (f2fs_readonly(sb) && (*flags & MS_RDONLY))\n\t\tgoto skip;\n\n\t/* disallow enable/disable extent_cache dynamically */\n\tif (no_extent_cache == !!test_opt(sbi, EXTENT_CACHE)) {\n\t\terr = -EINVAL;\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\t\"switch extent_cache option is not allowed\");\n\t\tgoto restore_opts;\n\t}\n\n\t/*\n\t * We stop the GC thread if FS is mounted as RO\n\t * or if background_gc = off is passed in mount\n\t * option. Also sync the filesystem.\n\t */\n\tif ((*flags & MS_RDONLY) || !test_opt(sbi, BG_GC)) {\n\t\tif (sbi->gc_thread) {\n\t\t\tstop_gc_thread(sbi);\n\t\t\tneed_restart_gc = true;\n\t\t}\n\t} else if (!sbi->gc_thread) {\n\t\terr = start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto restore_opts;\n\t\tneed_stop_gc = true;\n\t}\n\n\tif (*flags & MS_RDONLY) {\n\t\twriteback_inodes_sb(sb, WB_REASON_SYNC);\n\t\tsync_inodes_sb(sb);\n\n\t\tset_sbi_flag(sbi, SBI_IS_DIRTY);\n\t\tset_sbi_flag(sbi, SBI_IS_CLOSE);\n\t\tf2fs_sync_fs(sb, 1);\n\t\tclear_sbi_flag(sbi, SBI_IS_CLOSE);\n\t}\n\n\t/*\n\t * We stop issue flush thread if FS is mounted as RO\n\t * or if flush_merge is not passed in mount option.\n\t */\n\tif ((*flags & MS_RDONLY) || !test_opt(sbi, FLUSH_MERGE)) {\n\t\tclear_opt(sbi, FLUSH_MERGE);\n\t\tdestroy_flush_cmd_control(sbi, false);\n\t} else {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\tgoto restore_gc;\n\t}\nskip:\n\t/* Update the POSIXACL Flag */\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? MS_POSIXACL : 0);\n\n\treturn 0;\nrestore_gc:\n\tif (need_restart_gc) {\n\t\tif (start_gc_thread(sbi))\n\t\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\t\"background gc thread has stopped\");\n\t} else if (need_stop_gc) {\n\t\tstop_gc_thread(sbi);\n\t}\nrestore_opts:\n\tsbi->mount_opt = org_mount_opt;\n\tsbi->active_logs = active_logs;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tsbi->fault_info = ffi;\n#endif\n\treturn err;\n}\n\nstatic struct super_operations f2fs_sops = {\n\t.alloc_inode\t= f2fs_alloc_inode,\n\t.drop_inode\t= f2fs_drop_inode,\n\t.destroy_inode\t= f2fs_destroy_inode,\n\t.write_inode\t= f2fs_write_inode,\n\t.dirty_inode\t= f2fs_dirty_inode,\n\t.show_options\t= f2fs_show_options,\n\t.evict_inode\t= f2fs_evict_inode,\n\t.put_super\t= f2fs_put_super,\n\t.sync_fs\t= f2fs_sync_fs,\n\t.freeze_fs\t= f2fs_freeze,\n\t.unfreeze_fs\t= f2fs_unfreeze,\n\t.statfs\t\t= f2fs_statfs,\n\t.remount_fs\t= f2fs_remount,\n};\n\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\nstatic int f2fs_get_context(struct inode *inode, void *ctx, size_t len)\n{\n\treturn f2fs_getxattr(inode, F2FS_XATTR_INDEX_ENCRYPTION,\n\t\t\t\tF2FS_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\tctx, len, NULL);\n}\n\nstatic int f2fs_set_context(struct inode *inode, const void *ctx, size_t len,\n\t\t\t\t\t\t\tvoid *fs_data)\n{\n\treturn f2fs_setxattr(inode, F2FS_XATTR_INDEX_ENCRYPTION,\n\t\t\t\tF2FS_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\tctx, len, fs_data, XATTR_CREATE);\n}\n\nstatic unsigned f2fs_max_namelen(struct inode *inode)\n{\n\treturn S_ISLNK(inode->i_mode) ?\n\t\t\tinode->i_sb->s_blocksize : F2FS_NAME_LEN;\n}\n\nstatic const struct fscrypt_operations f2fs_cryptops = {\n\t.key_prefix\t= \"f2fs:\",\n\t.get_context\t= f2fs_get_context,\n\t.set_context\t= f2fs_set_context,\n\t.is_encrypted\t= f2fs_encrypted_inode,\n\t.empty_dir\t= f2fs_empty_dir,\n\t.max_namelen\t= f2fs_max_namelen,\n};\n#else\nstatic const struct fscrypt_operations f2fs_cryptops = {\n\t.is_encrypted\t= f2fs_encrypted_inode,\n};\n#endif\n\nstatic struct inode *f2fs_nfs_get_inode(struct super_block *sb,\n\t\tu64 ino, u32 generation)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct inode *inode;\n\n\tif (check_nid_range(sbi, ino))\n\t\treturn ERR_PTR(-ESTALE);\n\n\t/*\n\t * f2fs_iget isn't quite right if the inode is currently unallocated!\n\t * However f2fs_iget currently does appropriate checks to handle stale\n\t * inodes so everything is OK.\n\t */\n\tinode = f2fs_iget(sb, ino);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (unlikely(generation && inode->i_generation != generation)) {\n\t\t/* we didn't find the right inode.. */\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ESTALE);\n\t}\n\treturn inode;\n}\n\nstatic struct dentry *f2fs_fh_to_dentry(struct super_block *sb, struct fid *fid,\n\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_dentry(sb, fid, fh_len, fh_type,\n\t\t\t\t    f2fs_nfs_get_inode);\n}\n\nstatic struct dentry *f2fs_fh_to_parent(struct super_block *sb, struct fid *fid,\n\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_parent(sb, fid, fh_len, fh_type,\n\t\t\t\t    f2fs_nfs_get_inode);\n}\n\nstatic const struct export_operations f2fs_export_ops = {\n\t.fh_to_dentry = f2fs_fh_to_dentry,\n\t.fh_to_parent = f2fs_fh_to_parent,\n\t.get_parent = f2fs_get_parent,\n};\n\nstatic loff_t max_file_blocks(void)\n{\n\tloff_t result = (DEF_ADDRS_PER_INODE - F2FS_INLINE_XATTR_ADDRS);\n\tloff_t leaf_count = ADDRS_PER_BLOCK;\n\n\t/* two direct node blocks */\n\tresult += (leaf_count * 2);\n\n\t/* two indirect node blocks */\n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += (leaf_count * 2);\n\n\t/* one double indirect node block */\n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += leaf_count;\n\n\treturn result;\n}\n\nstatic int __f2fs_commit_super(struct buffer_head *bh,\n\t\t\tstruct f2fs_super_block *super)\n{\n\tlock_buffer(bh);\n\tif (super)\n\t\tmemcpy(bh->b_data + F2FS_SUPER_OFFSET, super, sizeof(*super));\n\tset_buffer_uptodate(bh);\n\tset_buffer_dirty(bh);\n\tunlock_buffer(bh);\n\n\t/* it's rare case, we can do fua all the time */\n\treturn __sync_dirty_buffer(bh, REQ_PREFLUSH | REQ_FUA);\n}\n\nstatic inline bool sanity_check_area_boundary(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct buffer_head *bh)\n{\n\tstruct f2fs_super_block *raw_super = (struct f2fs_super_block *)\n\t\t\t\t\t(bh->b_data + F2FS_SUPER_OFFSET);\n\tstruct super_block *sb = sbi->sb;\n\tu32 segment0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tu32 cp_blkaddr = le32_to_cpu(raw_super->cp_blkaddr);\n\tu32 sit_blkaddr = le32_to_cpu(raw_super->sit_blkaddr);\n\tu32 nat_blkaddr = le32_to_cpu(raw_super->nat_blkaddr);\n\tu32 ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tu32 main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tu32 segment_count_ckpt = le32_to_cpu(raw_super->segment_count_ckpt);\n\tu32 segment_count_sit = le32_to_cpu(raw_super->segment_count_sit);\n\tu32 segment_count_nat = le32_to_cpu(raw_super->segment_count_nat);\n\tu32 segment_count_ssa = le32_to_cpu(raw_super->segment_count_ssa);\n\tu32 segment_count_main = le32_to_cpu(raw_super->segment_count_main);\n\tu32 segment_count = le32_to_cpu(raw_super->segment_count);\n\tu32 log_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tu64 main_end_blkaddr = main_blkaddr +\n\t\t\t\t(segment_count_main << log_blocks_per_seg);\n\tu64 seg_end_blkaddr = segment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg);\n\n\tif (segment0_blkaddr != cp_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Mismatch start address, segment0(%u) cp_blkaddr(%u)\",\n\t\t\tsegment0_blkaddr, cp_blkaddr);\n\t\treturn true;\n\t}\n\n\tif (cp_blkaddr + (segment_count_ckpt << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tsit_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong CP boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tcp_blkaddr, sit_blkaddr,\n\t\t\tsegment_count_ckpt << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (sit_blkaddr + (segment_count_sit << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tnat_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong SIT boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tsit_blkaddr, nat_blkaddr,\n\t\t\tsegment_count_sit << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (nat_blkaddr + (segment_count_nat << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tssa_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong NAT boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tnat_blkaddr, ssa_blkaddr,\n\t\t\tsegment_count_nat << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (ssa_blkaddr + (segment_count_ssa << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tmain_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong SSA boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tssa_blkaddr, main_blkaddr,\n\t\t\tsegment_count_ssa << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (main_end_blkaddr > seg_end_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong MAIN_AREA boundary, start(%u) end(%u) block(%u)\",\n\t\t\tmain_blkaddr,\n\t\t\tsegment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg),\n\t\t\tsegment_count_main << log_blocks_per_seg);\n\t\treturn true;\n\t} else if (main_end_blkaddr < seg_end_blkaddr) {\n\t\tint err = 0;\n\t\tchar *res;\n\n\t\t/* fix in-memory information all the time */\n\t\traw_super->segment_count = cpu_to_le32((main_end_blkaddr -\n\t\t\t\tsegment0_blkaddr) >> log_blocks_per_seg);\n\n\t\tif (f2fs_readonly(sb) || bdev_read_only(sb->s_bdev)) {\n\t\t\tset_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t\t\tres = \"internally\";\n\t\t} else {\n\t\t\terr = __f2fs_commit_super(bh, NULL);\n\t\t\tres = err ? \"failed\" : \"done\";\n\t\t}\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Fix alignment : %s, start(%u) end(%u) block(%u)\",\n\t\t\tres, main_blkaddr,\n\t\t\tsegment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg),\n\t\t\tsegment_count_main << log_blocks_per_seg);\n\t\tif (err)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int sanity_check_raw_super(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct buffer_head *bh)\n{\n\tstruct f2fs_super_block *raw_super = (struct f2fs_super_block *)\n\t\t\t\t\t(bh->b_data + F2FS_SUPER_OFFSET);\n\tstruct super_block *sb = sbi->sb;\n\tunsigned int blocksize;\n\n\tif (F2FS_SUPER_MAGIC != le32_to_cpu(raw_super->magic)) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Magic Mismatch, valid(0x%x) - read(0x%x)\",\n\t\t\tF2FS_SUPER_MAGIC, le32_to_cpu(raw_super->magic));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB page cache size */\n\tif (F2FS_BLKSIZE != PAGE_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid page_cache_size (%lu), supports only 4KB\\n\",\n\t\t\tPAGE_SIZE);\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB block size */\n\tblocksize = 1 << le32_to_cpu(raw_super->log_blocksize);\n\tif (blocksize != F2FS_BLKSIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid blocksize (%u), supports only 4KB\\n\",\n\t\t\tblocksize);\n\t\treturn 1;\n\t}\n\n\t/* check log blocks per segment */\n\tif (le32_to_cpu(raw_super->log_blocks_per_seg) != 9) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log blocks per segment (%u)\\n\",\n\t\t\tle32_to_cpu(raw_super->log_blocks_per_seg));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support 512/1024/2048/4096 bytes sector size */\n\tif (le32_to_cpu(raw_super->log_sectorsize) >\n\t\t\t\tF2FS_MAX_LOG_SECTOR_SIZE ||\n\t\tle32_to_cpu(raw_super->log_sectorsize) <\n\t\t\t\tF2FS_MIN_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO, \"Invalid log sectorsize (%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\tif (le32_to_cpu(raw_super->log_sectors_per_block) +\n\t\tle32_to_cpu(raw_super->log_sectorsize) !=\n\t\t\tF2FS_MAX_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log sectors per block(%u) log sectorsize(%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectors_per_block),\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\n\t/* check reserved ino info */\n\tif (le32_to_cpu(raw_super->node_ino) != 1 ||\n\t\tle32_to_cpu(raw_super->meta_ino) != 2 ||\n\t\tle32_to_cpu(raw_super->root_ino) != 3) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid Fs Meta Ino: node(%u) meta(%u) root(%u)\",\n\t\t\tle32_to_cpu(raw_super->node_ino),\n\t\t\tle32_to_cpu(raw_super->meta_ino),\n\t\t\tle32_to_cpu(raw_super->root_ino));\n\t\treturn 1;\n\t}\n\n\tif (le32_to_cpu(raw_super->segment_count) > F2FS_MAX_SEGMENT) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid segment count (%u)\",\n\t\t\tle32_to_cpu(raw_super->segment_count));\n\t\treturn 1;\n\t}\n\n\t/* check CP/SIT/NAT/SSA/MAIN_AREA area boundary */\n\tif (sanity_check_area_boundary(sbi, bh))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nint sanity_check_ckpt(struct f2fs_sb_info *sbi)\n{\n\tunsigned int total, fsmeta;\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tunsigned int ovp_segments, reserved_segments;\n\n\ttotal = le32_to_cpu(raw_super->segment_count);\n\tfsmeta = le32_to_cpu(raw_super->segment_count_ckpt);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_sit);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_nat);\n\tfsmeta += le32_to_cpu(ckpt->rsvd_segment_count);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_ssa);\n\n\tif (unlikely(fsmeta >= total))\n\t\treturn 1;\n\n\tovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\treserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\n\tif (unlikely(fsmeta < F2FS_MIN_SEGMENTS ||\n\t\t\tovp_segments == 0 || reserved_segments == 0)) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong layout: check mkfs.f2fs version\");\n\t\treturn 1;\n\t}\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR, \"A bug case: need to run fsck\");\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void init_sb_info(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = sbi->raw_super;\n\tint i;\n\n\tsbi->log_sectors_per_block =\n\t\tle32_to_cpu(raw_super->log_sectors_per_block);\n\tsbi->log_blocksize = le32_to_cpu(raw_super->log_blocksize);\n\tsbi->blocksize = 1 << sbi->log_blocksize;\n\tsbi->log_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tsbi->blocks_per_seg = 1 << sbi->log_blocks_per_seg;\n\tsbi->segs_per_sec = le32_to_cpu(raw_super->segs_per_sec);\n\tsbi->secs_per_zone = le32_to_cpu(raw_super->secs_per_zone);\n\tsbi->total_sections = le32_to_cpu(raw_super->section_count);\n\tsbi->total_node_count =\n\t\t(le32_to_cpu(raw_super->segment_count_nat) / 2)\n\t\t\t* sbi->blocks_per_seg * NAT_ENTRY_PER_BLOCK;\n\tsbi->root_ino_num = le32_to_cpu(raw_super->root_ino);\n\tsbi->node_ino_num = le32_to_cpu(raw_super->node_ino);\n\tsbi->meta_ino_num = le32_to_cpu(raw_super->meta_ino);\n\tsbi->cur_victim_sec = NULL_SECNO;\n\tsbi->max_victim_search = DEF_MAX_VICTIM_SEARCH;\n\n\tsbi->dir_level = DEF_DIR_LEVEL;\n\tsbi->interval_time[CP_TIME] = DEF_CP_INTERVAL;\n\tsbi->interval_time[REQ_TIME] = DEF_IDLE_INTERVAL;\n\tclear_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\tfor (i = 0; i < NR_COUNT_TYPE; i++)\n\t\tatomic_set(&sbi->nr_pages[i], 0);\n\n\tatomic_set(&sbi->wb_sync_req, 0);\n\n\tINIT_LIST_HEAD(&sbi->s_list);\n\tmutex_init(&sbi->umount_mutex);\n\tmutex_init(&sbi->wio_mutex[NODE]);\n\tmutex_init(&sbi->wio_mutex[DATA]);\n\tspin_lock_init(&sbi->cp_lock);\n}\n\nstatic int init_percpu_info(struct f2fs_sb_info *sbi)\n{\n\tint err;\n\n\terr = percpu_counter_init(&sbi->alloc_valid_block_count, 0, GFP_KERNEL);\n\tif (err)\n\t\treturn err;\n\n\treturn percpu_counter_init(&sbi->total_valid_inode_count, 0,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int init_blkz_info(struct f2fs_sb_info *sbi, int devi)\n{\n\tstruct block_device *bdev = FDEV(devi).bdev;\n\tsector_t nr_sectors = bdev->bd_part->nr_sects;\n\tsector_t sector = 0;\n\tstruct blk_zone *zones;\n\tunsigned int i, nr_zones;\n\tunsigned int n = 0;\n\tint err = -EIO;\n\n\tif (!f2fs_sb_mounted_blkzoned(sbi->sb))\n\t\treturn 0;\n\n\tif (sbi->blocks_per_blkz && sbi->blocks_per_blkz !=\n\t\t\t\tSECTOR_TO_BLOCK(bdev_zone_sectors(bdev)))\n\t\treturn -EINVAL;\n\tsbi->blocks_per_blkz = SECTOR_TO_BLOCK(bdev_zone_sectors(bdev));\n\tif (sbi->log_blocks_per_blkz && sbi->log_blocks_per_blkz !=\n\t\t\t\t__ilog2_u32(sbi->blocks_per_blkz))\n\t\treturn -EINVAL;\n\tsbi->log_blocks_per_blkz = __ilog2_u32(sbi->blocks_per_blkz);\n\tFDEV(devi).nr_blkz = SECTOR_TO_BLOCK(nr_sectors) >>\n\t\t\t\t\tsbi->log_blocks_per_blkz;\n\tif (nr_sectors & (bdev_zone_sectors(bdev) - 1))\n\t\tFDEV(devi).nr_blkz++;\n\n\tFDEV(devi).blkz_type = kmalloc(FDEV(devi).nr_blkz, GFP_KERNEL);\n\tif (!FDEV(devi).blkz_type)\n\t\treturn -ENOMEM;\n\n#define F2FS_REPORT_NR_ZONES   4096\n\n\tzones = kcalloc(F2FS_REPORT_NR_ZONES, sizeof(struct blk_zone),\n\t\t\tGFP_KERNEL);\n\tif (!zones)\n\t\treturn -ENOMEM;\n\n\t/* Get block zones type */\n\twhile (zones && sector < nr_sectors) {\n\n\t\tnr_zones = F2FS_REPORT_NR_ZONES;\n\t\terr = blkdev_report_zones(bdev, sector,\n\t\t\t\t\t  zones, &nr_zones,\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!nr_zones) {\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 0; i < nr_zones; i++) {\n\t\t\tFDEV(devi).blkz_type[n] = zones[i].type;\n\t\t\tsector += zones[i].len;\n\t\t\tn++;\n\t\t}\n\t}\n\n\tkfree(zones);\n\n\treturn err;\n}\n#endif\n\n/*\n * Read f2fs raw super block.\n * Because we have two copies of super block, so read both of them\n * to get the first valid one. If any one of them is broken, we pass\n * them recovery flag back to the caller.\n */\nstatic int read_raw_super_block(struct f2fs_sb_info *sbi,\n\t\t\tstruct f2fs_super_block **raw_super,\n\t\t\tint *valid_super_block, int *recovery)\n{\n\tstruct super_block *sb = sbi->sb;\n\tint block;\n\tstruct buffer_head *bh;\n\tstruct f2fs_super_block *super;\n\tint err = 0;\n\n\tsuper = kzalloc(sizeof(struct f2fs_super_block), GFP_KERNEL);\n\tif (!super)\n\t\treturn -ENOMEM;\n\n\tfor (block = 0; block < 2; block++) {\n\t\tbh = sb_bread(sb, block);\n\t\tif (!bh) {\n\t\t\tf2fs_msg(sb, KERN_ERR, \"Unable to read %dth superblock\",\n\t\t\t\tblock + 1);\n\t\t\terr = -EIO;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* sanity checking of raw super */\n\t\tif (sanity_check_raw_super(sbi, bh)) {\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Can't find valid F2FS filesystem in %dth superblock\",\n\t\t\t\tblock + 1);\n\t\t\terr = -EINVAL;\n\t\t\tbrelse(bh);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!*raw_super) {\n\t\t\tmemcpy(super, bh->b_data + F2FS_SUPER_OFFSET,\n\t\t\t\t\t\t\tsizeof(*super));\n\t\t\t*valid_super_block = block;\n\t\t\t*raw_super = super;\n\t\t}\n\t\tbrelse(bh);\n\t}\n\n\t/* Fail to read any one of the superblocks*/\n\tif (err < 0)\n\t\t*recovery = 1;\n\n\t/* No valid superblock */\n\tif (!*raw_super)\n\t\tkfree(super);\n\telse\n\t\terr = 0;\n\n\treturn err;\n}\n\nint f2fs_commit_super(struct f2fs_sb_info *sbi, bool recover)\n{\n\tstruct buffer_head *bh;\n\tint err;\n\n\tif ((recover && f2fs_readonly(sbi->sb)) ||\n\t\t\t\tbdev_read_only(sbi->sb->s_bdev)) {\n\t\tset_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t\treturn -EROFS;\n\t}\n\n\t/* write back-up superblock first */\n\tbh = sb_getblk(sbi->sb, sbi->valid_super_block ? 0: 1);\n\tif (!bh)\n\t\treturn -EIO;\n\terr = __f2fs_commit_super(bh, F2FS_RAW_SUPER(sbi));\n\tbrelse(bh);\n\n\t/* if we are in recovery path, skip writing valid superblock */\n\tif (recover || err)\n\t\treturn err;\n\n\t/* write current valid superblock */\n\tbh = sb_getblk(sbi->sb, sbi->valid_super_block);\n\tif (!bh)\n\t\treturn -EIO;\n\terr = __f2fs_commit_super(bh, F2FS_RAW_SUPER(sbi));\n\tbrelse(bh);\n\treturn err;\n}\n\nstatic int f2fs_scan_devices(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tunsigned int max_devices = MAX_DEVICES;\n\tint i;\n\n\t/* Initialize single device information */\n\tif (!RDEV(0).path[0]) {\n\t\tif (!bdev_is_zoned(sbi->sb->s_bdev))\n\t\t\treturn 0;\n\t\tmax_devices = 1;\n\t}\n\n\t/*\n\t * Initialize multiple devices information, or single\n\t * zoned block device information.\n\t */\n\tsbi->devs = kcalloc(max_devices, sizeof(struct f2fs_dev_info),\n\t\t\t\tGFP_KERNEL);\n\tif (!sbi->devs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < max_devices; i++) {\n\n\t\tif (i > 0 && !RDEV(i).path[0])\n\t\t\tbreak;\n\n\t\tif (max_devices == 1) {\n\t\t\t/* Single zoned block device mount */\n\t\t\tFDEV(0).bdev =\n\t\t\t\tblkdev_get_by_dev(sbi->sb->s_bdev->bd_dev,\n\t\t\t\t\tsbi->sb->s_mode, sbi->sb->s_type);\n\t\t} else {\n\t\t\t/* Multi-device mount */\n\t\t\tmemcpy(FDEV(i).path, RDEV(i).path, MAX_PATH_LEN);\n\t\t\tFDEV(i).total_segments =\n\t\t\t\tle32_to_cpu(RDEV(i).total_segments);\n\t\t\tif (i == 0) {\n\t\t\t\tFDEV(i).start_blk = 0;\n\t\t\t\tFDEV(i).end_blk = FDEV(i).start_blk +\n\t\t\t\t    (FDEV(i).total_segments <<\n\t\t\t\t    sbi->log_blocks_per_seg) - 1 +\n\t\t\t\t    le32_to_cpu(raw_super->segment0_blkaddr);\n\t\t\t} else {\n\t\t\t\tFDEV(i).start_blk = FDEV(i - 1).end_blk + 1;\n\t\t\t\tFDEV(i).end_blk = FDEV(i).start_blk +\n\t\t\t\t\t(FDEV(i).total_segments <<\n\t\t\t\t\tsbi->log_blocks_per_seg) - 1;\n\t\t\t}\n\t\t\tFDEV(i).bdev = blkdev_get_by_path(FDEV(i).path,\n\t\t\t\t\tsbi->sb->s_mode, sbi->sb->s_type);\n\t\t}\n\t\tif (IS_ERR(FDEV(i).bdev))\n\t\t\treturn PTR_ERR(FDEV(i).bdev);\n\n\t\t/* to release errored devices */\n\t\tsbi->s_ndevs = i + 1;\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\tif (bdev_zoned_model(FDEV(i).bdev) == BLK_ZONED_HM &&\n\t\t\t\t!f2fs_sb_mounted_blkzoned(sbi->sb)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\"Zoned block device feature not enabled\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (bdev_zoned_model(FDEV(i).bdev) != BLK_ZONED_NONE) {\n\t\t\tif (init_blkz_info(sbi, i)) {\n\t\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\t\"Failed to initialize F2FS blkzone information\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (max_devices == 1)\n\t\t\t\tbreak;\n\t\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\t\"Mount Device [%2d]: %20s, %8u, %8x - %8x (zone: %s)\",\n\t\t\t\ti, FDEV(i).path,\n\t\t\t\tFDEV(i).total_segments,\n\t\t\t\tFDEV(i).start_blk, FDEV(i).end_blk,\n\t\t\t\tbdev_zoned_model(FDEV(i).bdev) == BLK_ZONED_HA ?\n\t\t\t\t\"Host-aware\" : \"Host-managed\");\n\t\t\tcontinue;\n\t\t}\n#endif\n\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"Mount Device [%2d]: %20s, %8u, %8x - %8x\",\n\t\t\t\ti, FDEV(i).path,\n\t\t\t\tFDEV(i).total_segments,\n\t\t\t\tFDEV(i).start_blk, FDEV(i).end_blk);\n\t}\n\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"IO Block Size: %8d KB\", F2FS_IO_SIZE_KB(sbi));\n\treturn 0;\n}\n\nstatic int f2fs_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct f2fs_sb_info *sbi;\n\tstruct f2fs_super_block *raw_super;\n\tstruct inode *root;\n\tint err;\n\tbool retry = true, need_fsck = false;\n\tchar *options = NULL;\n\tint recovery, i, valid_super_block;\n\tstruct curseg_info *seg_i;\n\ntry_onemore:\n\terr = -EINVAL;\n\traw_super = NULL;\n\tvalid_super_block = -1;\n\trecovery = 0;\n\n\t/* allocate memory for f2fs-specific super block info */\n\tsbi = kzalloc(sizeof(struct f2fs_sb_info), GFP_KERNEL);\n\tif (!sbi)\n\t\treturn -ENOMEM;\n\n\tsbi->sb = sb;\n\n\t/* Load the checksum driver */\n\tsbi->s_chksum_driver = crypto_alloc_shash(\"crc32\", 0, 0);\n\tif (IS_ERR(sbi->s_chksum_driver)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Cannot load crc32 driver.\");\n\t\terr = PTR_ERR(sbi->s_chksum_driver);\n\t\tsbi->s_chksum_driver = NULL;\n\t\tgoto free_sbi;\n\t}\n\n\t/* set a block size */\n\tif (unlikely(!sb_set_blocksize(sb, F2FS_BLKSIZE))) {\n\t\tf2fs_msg(sb, KERN_ERR, \"unable to set blocksize\");\n\t\tgoto free_sbi;\n\t}\n\n\terr = read_raw_super_block(sbi, &raw_super, &valid_super_block,\n\t\t\t\t\t\t\t\t&recovery);\n\tif (err)\n\t\tgoto free_sbi;\n\n\tsb->s_fs_info = sbi;\n\tsbi->raw_super = raw_super;\n\n\t/*\n\t * The BLKZONED feature indicates that the drive was formatted with\n\t * zone alignment optimization. This is optional for host-aware\n\t * devices, but mandatory for host-managed zoned block devices.\n\t */\n#ifndef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t \"Zoned block device support is not enabled\\n\");\n\t\tgoto free_sb_buf;\n\t}\n#endif\n\tdefault_options(sbi);\n\t/* parse mount options */\n\toptions = kstrdup((const char *)data, GFP_KERNEL);\n\tif (data && !options) {\n\t\terr = -ENOMEM;\n\t\tgoto free_sb_buf;\n\t}\n\n\terr = parse_options(sb, options);\n\tif (err)\n\t\tgoto free_options;\n\n\tsbi->max_file_blocks = max_file_blocks();\n\tsb->s_maxbytes = sbi->max_file_blocks <<\n\t\t\t\tle32_to_cpu(raw_super->log_blocksize);\n\tsb->s_max_links = F2FS_LINK_MAX;\n\tget_random_bytes(&sbi->s_next_generation, sizeof(u32));\n\n\tsb->s_op = &f2fs_sops;\n\tsb->s_cop = &f2fs_cryptops;\n\tsb->s_xattr = f2fs_xattr_handlers;\n\tsb->s_export_op = &f2fs_export_ops;\n\tsb->s_magic = F2FS_SUPER_MAGIC;\n\tsb->s_time_gran = 1;\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? MS_POSIXACL : 0);\n\tmemcpy(sb->s_uuid, raw_super->uuid, sizeof(raw_super->uuid));\n\n\t/* init f2fs-specific super block info */\n\tsbi->valid_super_block = valid_super_block;\n\tmutex_init(&sbi->gc_mutex);\n\tmutex_init(&sbi->cp_mutex);\n\tinit_rwsem(&sbi->node_write);\n\tinit_rwsem(&sbi->node_change);\n\n\t/* disallow all the data/node/meta page writes */\n\tset_sbi_flag(sbi, SBI_POR_DOING);\n\tspin_lock_init(&sbi->stat_lock);\n\n\tinit_rwsem(&sbi->read_io.io_rwsem);\n\tsbi->read_io.sbi = sbi;\n\tsbi->read_io.bio = NULL;\n\tfor (i = 0; i < NR_PAGE_TYPE; i++) {\n\t\tinit_rwsem(&sbi->write_io[i].io_rwsem);\n\t\tsbi->write_io[i].sbi = sbi;\n\t\tsbi->write_io[i].bio = NULL;\n\t}\n\n\tinit_rwsem(&sbi->cp_rwsem);\n\tinit_waitqueue_head(&sbi->cp_wait);\n\tinit_sb_info(sbi);\n\n\terr = init_percpu_info(sbi);\n\tif (err)\n\t\tgoto free_options;\n\n\tif (F2FS_IO_SIZE(sbi) > 1) {\n\t\tsbi->write_io_dummy =\n\t\t\tmempool_create_page_pool(2 * (F2FS_IO_SIZE(sbi) - 1), 0);\n\t\tif (!sbi->write_io_dummy)\n\t\t\tgoto free_options;\n\t}\n\n\t/* get an inode for meta space */\n\tsbi->meta_inode = f2fs_iget(sb, F2FS_META_INO(sbi));\n\tif (IS_ERR(sbi->meta_inode)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read F2FS meta data inode\");\n\t\terr = PTR_ERR(sbi->meta_inode);\n\t\tgoto free_io_dummy;\n\t}\n\n\terr = get_valid_checkpoint(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to get valid F2FS checkpoint\");\n\t\tgoto free_meta_inode;\n\t}\n\n\t/* Initialize device list */\n\terr = f2fs_scan_devices(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to find devices\");\n\t\tgoto free_devices;\n\t}\n\n\tsbi->total_valid_node_count =\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_node_count);\n\tpercpu_counter_set(&sbi->total_valid_inode_count,\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_inode_count));\n\tsbi->user_block_count = le64_to_cpu(sbi->ckpt->user_block_count);\n\tsbi->total_valid_block_count =\n\t\t\t\tle64_to_cpu(sbi->ckpt->valid_block_count);\n\tsbi->last_valid_block_count = sbi->total_valid_block_count;\n\n\tfor (i = 0; i < NR_INODE_TYPE; i++) {\n\t\tINIT_LIST_HEAD(&sbi->inode_list[i]);\n\t\tspin_lock_init(&sbi->inode_lock[i]);\n\t}\n\n\tinit_extent_cache_info(sbi);\n\n\tinit_ino_entry_info(sbi);\n\n\t/* setup f2fs internal modules */\n\terr = build_segment_manager(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Failed to initialize F2FS segment manager\");\n\t\tgoto free_sm;\n\t}\n\terr = build_node_manager(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Failed to initialize F2FS node manager\");\n\t\tgoto free_nm;\n\t}\n\n\t/* For write statistics */\n\tif (sb->s_bdev->bd_part)\n\t\tsbi->sectors_written_start =\n\t\t\t(u64)part_stat_read(sb->s_bdev->bd_part, sectors[1]);\n\n\t/* Read accumulated write IO statistics if exists */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_NODE);\n\tif (__exist_node_summaries(sbi))\n\t\tsbi->kbytes_written =\n\t\t\tle64_to_cpu(seg_i->journal->info.kbytes_written);\n\n\tbuild_gc_manager(sbi);\n\n\t/* get an inode for node space */\n\tsbi->node_inode = f2fs_iget(sb, F2FS_NODE_INO(sbi));\n\tif (IS_ERR(sbi->node_inode)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read node inode\");\n\t\terr = PTR_ERR(sbi->node_inode);\n\t\tgoto free_nm;\n\t}\n\n\tf2fs_join_shrinker(sbi);\n\n\terr = f2fs_build_stats(sbi);\n\tif (err)\n\t\tgoto free_nm;\n\n\t/* if there are nt orphan nodes free them */\n\terr = recover_orphan_inodes(sbi);\n\tif (err)\n\t\tgoto free_node_inode;\n\n\t/* read root inode and dentry */\n\troot = f2fs_iget(sb, F2FS_ROOT_INO(sbi));\n\tif (IS_ERR(root)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read root inode\");\n\t\terr = PTR_ERR(root);\n\t\tgoto free_node_inode;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) {\n\t\tiput(root);\n\t\terr = -EINVAL;\n\t\tgoto free_node_inode;\n\t}\n\n\tsb->s_root = d_make_root(root); /* allocate root dentry */\n\tif (!sb->s_root) {\n\t\terr = -ENOMEM;\n\t\tgoto free_root_inode;\n\t}\n\n\tif (f2fs_proc_root)\n\t\tsbi->s_proc = proc_mkdir(sb->s_id, f2fs_proc_root);\n\n\tif (sbi->s_proc) {\n\t\tproc_create_data(\"segment_info\", S_IRUGO, sbi->s_proc,\n\t\t\t\t &f2fs_seq_segment_info_fops, sb);\n\t\tproc_create_data(\"segment_bits\", S_IRUGO, sbi->s_proc,\n\t\t\t\t &f2fs_seq_segment_bits_fops, sb);\n\t}\n\n\tsbi->s_kobj.kset = f2fs_kset;\n\tinit_completion(&sbi->s_kobj_unregister);\n\terr = kobject_init_and_add(&sbi->s_kobj, &f2fs_ktype, NULL,\n\t\t\t\t\t\t\t\"%s\", sb->s_id);\n\tif (err)\n\t\tgoto free_proc;\n\n\t/* recover fsynced data */\n\tif (!test_opt(sbi, DISABLE_ROLL_FORWARD)) {\n\t\t/*\n\t\t * mount should be failed, when device has readonly mode, and\n\t\t * previous checkpoint was not done by clean system shutdown.\n\t\t */\n\t\tif (bdev_read_only(sb->s_bdev) &&\n\t\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\t\terr = -EROFS;\n\t\t\tgoto free_kobj;\n\t\t}\n\n\t\tif (need_fsck)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\t\tif (!retry)\n\t\t\tgoto skip_recovery;\n\n\t\terr = recover_fsync_data(sbi, false);\n\t\tif (err < 0) {\n\t\t\tneed_fsck = true;\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Cannot recover all fsync data errno=%d\", err);\n\t\t\tgoto free_kobj;\n\t\t}\n\t} else {\n\t\terr = recover_fsync_data(sbi, true);\n\n\t\tif (!f2fs_readonly(sb) && err > 0) {\n\t\t\terr = -EINVAL;\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Need to recover fsync data\");\n\t\t\tgoto free_kobj;\n\t\t}\n\t}\nskip_recovery:\n\t/* recover_fsync_data() cleared this already */\n\tclear_sbi_flag(sbi, SBI_POR_DOING);\n\n\t/*\n\t * If filesystem is not mounted as read-only then\n\t * do start the gc_thread.\n\t */\n\tif (test_opt(sbi, BG_GC) && !f2fs_readonly(sb)) {\n\t\t/* After POR, we can run background GC thread.*/\n\t\terr = start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto free_kobj;\n\t}\n\tkfree(options);\n\n\t/* recover broken superblock */\n\tif (recovery) {\n\t\terr = f2fs_commit_super(sbi, true);\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Try to recover %dth superblock, ret: %d\",\n\t\t\tsbi->valid_super_block ? 1 : 2, err);\n\t}\n\n\tf2fs_msg(sbi->sb, KERN_NOTICE, \"Mounted with checkpoint version = %llx\",\n\t\t\t\tcur_cp_version(F2FS_CKPT(sbi)));\n\tf2fs_update_time(sbi, CP_TIME);\n\tf2fs_update_time(sbi, REQ_TIME);\n\treturn 0;\n\nfree_kobj:\n\tf2fs_sync_inode_meta(sbi);\n\tkobject_del(&sbi->s_kobj);\n\tkobject_put(&sbi->s_kobj);\n\twait_for_completion(&sbi->s_kobj_unregister);\nfree_proc:\n\tif (sbi->s_proc) {\n\t\tremove_proc_entry(\"segment_info\", sbi->s_proc);\n\t\tremove_proc_entry(\"segment_bits\", sbi->s_proc);\n\t\tremove_proc_entry(sb->s_id, f2fs_proc_root);\n\t}\nfree_root_inode:\n\tdput(sb->s_root);\n\tsb->s_root = NULL;\nfree_node_inode:\n\ttruncate_inode_pages_final(NODE_MAPPING(sbi));\n\tmutex_lock(&sbi->umount_mutex);\n\trelease_ino_entry(sbi, true);\n\tf2fs_leave_shrinker(sbi);\n\t/*\n\t * Some dirty meta pages can be produced by recover_orphan_inodes()\n\t * failed by EIO. Then, iput(node_inode) can trigger balance_fs_bg()\n\t * followed by write_checkpoint() through f2fs_write_node_pages(), which\n\t * falls into an infinite loop in sync_meta_pages().\n\t */\n\ttruncate_inode_pages_final(META_MAPPING(sbi));\n\tiput(sbi->node_inode);\n\tmutex_unlock(&sbi->umount_mutex);\n\tf2fs_destroy_stats(sbi);\nfree_nm:\n\tdestroy_node_manager(sbi);\nfree_sm:\n\tdestroy_segment_manager(sbi);\nfree_devices:\n\tdestroy_device_list(sbi);\n\tkfree(sbi->ckpt);\nfree_meta_inode:\n\tmake_bad_inode(sbi->meta_inode);\n\tiput(sbi->meta_inode);\nfree_io_dummy:\n\tmempool_destroy(sbi->write_io_dummy);\nfree_options:\n\tdestroy_percpu_info(sbi);\n\tkfree(options);\nfree_sb_buf:\n\tkfree(raw_super);\nfree_sbi:\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi);\n\n\t/* give only one another chance */\n\tif (retry) {\n\t\tretry = false;\n\t\tshrink_dcache_sb(sb);\n\t\tgoto try_onemore;\n\t}\n\treturn err;\n}\n\nstatic struct dentry *f2fs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *data)\n{\n\treturn mount_bdev(fs_type, flags, dev_name, data, f2fs_fill_super);\n}\n\nstatic void kill_f2fs_super(struct super_block *sb)\n{\n\tif (sb->s_root)\n\t\tset_sbi_flag(F2FS_SB(sb), SBI_IS_CLOSE);\n\tkill_block_super(sb);\n}\n\nstatic struct file_system_type f2fs_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"f2fs\",\n\t.mount\t\t= f2fs_mount,\n\t.kill_sb\t= kill_f2fs_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"f2fs\");\n\nstatic int __init init_inodecache(void)\n{\n\tf2fs_inode_cachep = kmem_cache_create(\"f2fs_inode_cache\",\n\t\t\tsizeof(struct f2fs_inode_info), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT|SLAB_ACCOUNT, NULL);\n\tif (!f2fs_inode_cachep)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void destroy_inodecache(void)\n{\n\t/*\n\t * Make sure all delayed rcu free inodes are flushed before we\n\t * destroy cache.\n\t */\n\trcu_barrier();\n\tkmem_cache_destroy(f2fs_inode_cachep);\n}\n\nstatic int __init init_f2fs_fs(void)\n{\n\tint err;\n\n\tf2fs_build_trace_ios();\n\n\terr = init_inodecache();\n\tif (err)\n\t\tgoto fail;\n\terr = create_node_manager_caches();\n\tif (err)\n\t\tgoto free_inodecache;\n\terr = create_segment_manager_caches();\n\tif (err)\n\t\tgoto free_node_manager_caches;\n\terr = create_checkpoint_caches();\n\tif (err)\n\t\tgoto free_segment_manager_caches;\n\terr = create_extent_cache();\n\tif (err)\n\t\tgoto free_checkpoint_caches;\n\tf2fs_kset = kset_create_and_add(\"f2fs\", NULL, fs_kobj);\n\tif (!f2fs_kset) {\n\t\terr = -ENOMEM;\n\t\tgoto free_extent_cache;\n\t}\n\terr = register_shrinker(&f2fs_shrinker_info);\n\tif (err)\n\t\tgoto free_kset;\n\n\terr = register_filesystem(&f2fs_fs_type);\n\tif (err)\n\t\tgoto free_shrinker;\n\terr = f2fs_create_root_stats();\n\tif (err)\n\t\tgoto free_filesystem;\n\tf2fs_proc_root = proc_mkdir(\"fs/f2fs\", NULL);\n\treturn 0;\n\nfree_filesystem:\n\tunregister_filesystem(&f2fs_fs_type);\nfree_shrinker:\n\tunregister_shrinker(&f2fs_shrinker_info);\nfree_kset:\n\tkset_unregister(f2fs_kset);\nfree_extent_cache:\n\tdestroy_extent_cache();\nfree_checkpoint_caches:\n\tdestroy_checkpoint_caches();\nfree_segment_manager_caches:\n\tdestroy_segment_manager_caches();\nfree_node_manager_caches:\n\tdestroy_node_manager_caches();\nfree_inodecache:\n\tdestroy_inodecache();\nfail:\n\treturn err;\n}\n\nstatic void __exit exit_f2fs_fs(void)\n{\n\tremove_proc_entry(\"fs/f2fs\", NULL);\n\tf2fs_destroy_root_stats();\n\tunregister_filesystem(&f2fs_fs_type);\n\tunregister_shrinker(&f2fs_shrinker_info);\n\tkset_unregister(f2fs_kset);\n\tdestroy_extent_cache();\n\tdestroy_checkpoint_caches();\n\tdestroy_segment_manager_caches();\n\tdestroy_node_manager_caches();\n\tdestroy_inodecache();\n\tf2fs_destroy_trace_ios();\n}\n\nmodule_init(init_f2fs_fs)\nmodule_exit(exit_f2fs_fs)\n\nMODULE_AUTHOR(\"Samsung Electronics's Praesto Team\");\nMODULE_DESCRIPTION(\"Flash Friendly File System\");\nMODULE_LICENSE(\"GPL\");\n\n", "/**\n * include/linux/f2fs_fs.h\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#ifndef _LINUX_F2FS_FS_H\n#define _LINUX_F2FS_FS_H\n\n#include <linux/pagemap.h>\n#include <linux/types.h>\n\n#define F2FS_SUPER_OFFSET\t\t1024\t/* byte-size offset */\n#define F2FS_MIN_LOG_SECTOR_SIZE\t9\t/* 9 bits for 512 bytes */\n#define F2FS_MAX_LOG_SECTOR_SIZE\t12\t/* 12 bits for 4096 bytes */\n#define F2FS_LOG_SECTORS_PER_BLOCK\t3\t/* log number for sector/blk */\n#define F2FS_BLKSIZE\t\t\t4096\t/* support only 4KB block */\n#define F2FS_BLKSIZE_BITS\t\t12\t/* bits for F2FS_BLKSIZE */\n#define F2FS_MAX_EXTENSION\t\t64\t/* # of extension entries */\n#define F2FS_BLK_ALIGN(x)\t(((x) + F2FS_BLKSIZE - 1) >> F2FS_BLKSIZE_BITS)\n\n#define NULL_ADDR\t\t((block_t)0)\t/* used as block_t addresses */\n#define NEW_ADDR\t\t((block_t)-1)\t/* used as block_t addresses */\n\n#define F2FS_BYTES_TO_BLK(bytes)\t((bytes) >> F2FS_BLKSIZE_BITS)\n#define F2FS_BLK_TO_BYTES(blk)\t\t((blk) << F2FS_BLKSIZE_BITS)\n\n/* 0, 1(node nid), 2(meta nid) are reserved node id */\n#define F2FS_RESERVED_NODE_NUM\t\t3\n\n#define F2FS_ROOT_INO(sbi)\t((sbi)->root_ino_num)\n#define F2FS_NODE_INO(sbi)\t((sbi)->node_ino_num)\n#define F2FS_META_INO(sbi)\t((sbi)->meta_ino_num)\n\n#define F2FS_IO_SIZE(sbi)\t(1 << (sbi)->write_io_size_bits) /* Blocks */\n#define F2FS_IO_SIZE_KB(sbi)\t(1 << ((sbi)->write_io_size_bits + 2)) /* KB */\n#define F2FS_IO_SIZE_BYTES(sbi)\t(1 << ((sbi)->write_io_size_bits + 12)) /* B */\n#define F2FS_IO_SIZE_BITS(sbi)\t((sbi)->write_io_size_bits) /* power of 2 */\n#define F2FS_IO_SIZE_MASK(sbi)\t(F2FS_IO_SIZE(sbi) - 1)\n\n/* This flag is used by node and meta inodes, and by recovery */\n#define GFP_F2FS_ZERO\t\t(GFP_NOFS | __GFP_ZERO)\n#define GFP_F2FS_HIGH_ZERO\t(GFP_NOFS | __GFP_ZERO | __GFP_HIGHMEM)\n\n/*\n * For further optimization on multi-head logs, on-disk layout supports maximum\n * 16 logs by default. The number, 16, is expected to cover all the cases\n * enoughly. The implementaion currently uses no more than 6 logs.\n * Half the logs are used for nodes, and the other half are used for data.\n */\n#define MAX_ACTIVE_LOGS\t16\n#define MAX_ACTIVE_NODE_LOGS\t8\n#define MAX_ACTIVE_DATA_LOGS\t8\n\n#define VERSION_LEN\t256\n#define MAX_VOLUME_NAME\t\t512\n#define MAX_PATH_LEN\t\t64\n#define MAX_DEVICES\t\t8\n\n/*\n * For superblock\n */\nstruct f2fs_device {\n\t__u8 path[MAX_PATH_LEN];\n\t__le32 total_segments;\n} __packed;\n\nstruct f2fs_super_block {\n\t__le32 magic;\t\t\t/* Magic Number */\n\t__le16 major_ver;\t\t/* Major Version */\n\t__le16 minor_ver;\t\t/* Minor Version */\n\t__le32 log_sectorsize;\t\t/* log2 sector size in bytes */\n\t__le32 log_sectors_per_block;\t/* log2 # of sectors per block */\n\t__le32 log_blocksize;\t\t/* log2 block size in bytes */\n\t__le32 log_blocks_per_seg;\t/* log2 # of blocks per segment */\n\t__le32 segs_per_sec;\t\t/* # of segments per section */\n\t__le32 secs_per_zone;\t\t/* # of sections per zone */\n\t__le32 checksum_offset;\t\t/* checksum offset inside super block */\n\t__le64 block_count;\t\t/* total # of user blocks */\n\t__le32 section_count;\t\t/* total # of sections */\n\t__le32 segment_count;\t\t/* total # of segments */\n\t__le32 segment_count_ckpt;\t/* # of segments for checkpoint */\n\t__le32 segment_count_sit;\t/* # of segments for SIT */\n\t__le32 segment_count_nat;\t/* # of segments for NAT */\n\t__le32 segment_count_ssa;\t/* # of segments for SSA */\n\t__le32 segment_count_main;\t/* # of segments for main area */\n\t__le32 segment0_blkaddr;\t/* start block address of segment 0 */\n\t__le32 cp_blkaddr;\t\t/* start block address of checkpoint */\n\t__le32 sit_blkaddr;\t\t/* start block address of SIT */\n\t__le32 nat_blkaddr;\t\t/* start block address of NAT */\n\t__le32 ssa_blkaddr;\t\t/* start block address of SSA */\n\t__le32 main_blkaddr;\t\t/* start block address of main area */\n\t__le32 root_ino;\t\t/* root inode number */\n\t__le32 node_ino;\t\t/* node inode number */\n\t__le32 meta_ino;\t\t/* meta inode number */\n\t__u8 uuid[16];\t\t\t/* 128-bit uuid for volume */\n\t__le16 volume_name[MAX_VOLUME_NAME];\t/* volume name */\n\t__le32 extension_count;\t\t/* # of extensions below */\n\t__u8 extension_list[F2FS_MAX_EXTENSION][8];\t/* extension array */\n\t__le32 cp_payload;\n\t__u8 version[VERSION_LEN];\t/* the kernel version */\n\t__u8 init_version[VERSION_LEN];\t/* the initial kernel version */\n\t__le32 feature;\t\t\t/* defined features */\n\t__u8 encryption_level;\t\t/* versioning level for encryption */\n\t__u8 encrypt_pw_salt[16];\t/* Salt used for string2key algorithm */\n\tstruct f2fs_device devs[MAX_DEVICES];\t/* device list */\n\t__u8 reserved[327];\t\t/* valid reserved region */\n} __packed;\n\n/*\n * For checkpoint\n */\n#define CP_NAT_BITS_FLAG\t0x00000080\n#define CP_CRC_RECOVERY_FLAG\t0x00000040\n#define CP_FASTBOOT_FLAG\t0x00000020\n#define CP_FSCK_FLAG\t\t0x00000010\n#define CP_ERROR_FLAG\t\t0x00000008\n#define CP_COMPACT_SUM_FLAG\t0x00000004\n#define CP_ORPHAN_PRESENT_FLAG\t0x00000002\n#define CP_UMOUNT_FLAG\t\t0x00000001\n\n#define F2FS_CP_PACKS\t\t2\t/* # of checkpoint packs */\n\nstruct f2fs_checkpoint {\n\t__le64 checkpoint_ver;\t\t/* checkpoint block version number */\n\t__le64 user_block_count;\t/* # of user blocks */\n\t__le64 valid_block_count;\t/* # of valid blocks in main area */\n\t__le32 rsvd_segment_count;\t/* # of reserved segments for gc */\n\t__le32 overprov_segment_count;\t/* # of overprovision segments */\n\t__le32 free_segment_count;\t/* # of free segments in main area */\n\n\t/* information of current node segments */\n\t__le32 cur_node_segno[MAX_ACTIVE_NODE_LOGS];\n\t__le16 cur_node_blkoff[MAX_ACTIVE_NODE_LOGS];\n\t/* information of current data segments */\n\t__le32 cur_data_segno[MAX_ACTIVE_DATA_LOGS];\n\t__le16 cur_data_blkoff[MAX_ACTIVE_DATA_LOGS];\n\t__le32 ckpt_flags;\t\t/* Flags : umount and journal_present */\n\t__le32 cp_pack_total_block_count;\t/* total # of one cp pack */\n\t__le32 cp_pack_start_sum;\t/* start block number of data summary */\n\t__le32 valid_node_count;\t/* Total number of valid nodes */\n\t__le32 valid_inode_count;\t/* Total number of valid inodes */\n\t__le32 next_free_nid;\t\t/* Next free node number */\n\t__le32 sit_ver_bitmap_bytesize;\t/* Default value 64 */\n\t__le32 nat_ver_bitmap_bytesize; /* Default value 256 */\n\t__le32 checksum_offset;\t\t/* checksum offset inside cp block */\n\t__le64 elapsed_time;\t\t/* mounted time */\n\t/* allocation type of current segment */\n\tunsigned char alloc_type[MAX_ACTIVE_LOGS];\n\n\t/* SIT and NAT version bitmap */\n\tunsigned char sit_nat_version_bitmap[1];\n} __packed;\n\n/*\n * For orphan inode management\n */\n#define F2FS_ORPHANS_PER_BLOCK\t1020\n\n#define GET_ORPHAN_BLOCKS(n)\t(((n) + F2FS_ORPHANS_PER_BLOCK - 1) / \\\n\t\t\t\t\tF2FS_ORPHANS_PER_BLOCK)\n\nstruct f2fs_orphan_block {\n\t__le32 ino[F2FS_ORPHANS_PER_BLOCK];\t/* inode numbers */\n\t__le32 reserved;\t/* reserved */\n\t__le16 blk_addr;\t/* block index in current CP */\n\t__le16 blk_count;\t/* Number of orphan inode blocks in CP */\n\t__le32 entry_count;\t/* Total number of orphan nodes in current CP */\n\t__le32 check_sum;\t/* CRC32 for orphan inode block */\n} __packed;\n\n/*\n * For NODE structure\n */\nstruct f2fs_extent {\n\t__le32 fofs;\t\t/* start file offset of the extent */\n\t__le32 blk;\t\t/* start block address of the extent */\n\t__le32 len;\t\t/* lengh of the extent */\n} __packed;\n\n#define F2FS_NAME_LEN\t\t255\n#define F2FS_INLINE_XATTR_ADDRS\t50\t/* 200 bytes for inline xattrs */\n#define DEF_ADDRS_PER_INODE\t923\t/* Address Pointers in an Inode */\n#define DEF_NIDS_PER_INODE\t5\t/* Node IDs in an Inode */\n#define ADDRS_PER_INODE(inode)\taddrs_per_inode(inode)\n#define ADDRS_PER_BLOCK\t\t1018\t/* Address Pointers in a Direct Block */\n#define NIDS_PER_BLOCK\t\t1018\t/* Node IDs in an Indirect Block */\n\n#define ADDRS_PER_PAGE(page, inode)\t\\\n\t(IS_INODE(page) ? ADDRS_PER_INODE(inode) : ADDRS_PER_BLOCK)\n\n#define\tNODE_DIR1_BLOCK\t\t(DEF_ADDRS_PER_INODE + 1)\n#define\tNODE_DIR2_BLOCK\t\t(DEF_ADDRS_PER_INODE + 2)\n#define\tNODE_IND1_BLOCK\t\t(DEF_ADDRS_PER_INODE + 3)\n#define\tNODE_IND2_BLOCK\t\t(DEF_ADDRS_PER_INODE + 4)\n#define\tNODE_DIND_BLOCK\t\t(DEF_ADDRS_PER_INODE + 5)\n\n#define F2FS_INLINE_XATTR\t0x01\t/* file inline xattr flag */\n#define F2FS_INLINE_DATA\t0x02\t/* file inline data flag */\n#define F2FS_INLINE_DENTRY\t0x04\t/* file inline dentry flag */\n#define F2FS_DATA_EXIST\t\t0x08\t/* file inline data exist flag */\n#define F2FS_INLINE_DOTS\t0x10\t/* file having implicit dot dentries */\n\n#define MAX_INLINE_DATA\t\t(sizeof(__le32) * (DEF_ADDRS_PER_INODE - \\\n\t\t\t\t\t\tF2FS_INLINE_XATTR_ADDRS - 1))\n\nstruct f2fs_inode {\n\t__le16 i_mode;\t\t\t/* file mode */\n\t__u8 i_advise;\t\t\t/* file hints */\n\t__u8 i_inline;\t\t\t/* file inline flags */\n\t__le32 i_uid;\t\t\t/* user ID */\n\t__le32 i_gid;\t\t\t/* group ID */\n\t__le32 i_links;\t\t\t/* links count */\n\t__le64 i_size;\t\t\t/* file size in bytes */\n\t__le64 i_blocks;\t\t/* file size in blocks */\n\t__le64 i_atime;\t\t\t/* access time */\n\t__le64 i_ctime;\t\t\t/* change time */\n\t__le64 i_mtime;\t\t\t/* modification time */\n\t__le32 i_atime_nsec;\t\t/* access time in nano scale */\n\t__le32 i_ctime_nsec;\t\t/* change time in nano scale */\n\t__le32 i_mtime_nsec;\t\t/* modification time in nano scale */\n\t__le32 i_generation;\t\t/* file version (for NFS) */\n\t__le32 i_current_depth;\t\t/* only for directory depth */\n\t__le32 i_xattr_nid;\t\t/* nid to save xattr */\n\t__le32 i_flags;\t\t\t/* file attributes */\n\t__le32 i_pino;\t\t\t/* parent inode number */\n\t__le32 i_namelen;\t\t/* file name length */\n\t__u8 i_name[F2FS_NAME_LEN];\t/* file name for SPOR */\n\t__u8 i_dir_level;\t\t/* dentry_level for large dir */\n\n\tstruct f2fs_extent i_ext;\t/* caching a largest extent */\n\n\t__le32 i_addr[DEF_ADDRS_PER_INODE];\t/* Pointers to data blocks */\n\n\t__le32 i_nid[DEF_NIDS_PER_INODE];\t/* direct(2), indirect(2),\n\t\t\t\t\t\tdouble_indirect(1) node id */\n} __packed;\n\nstruct direct_node {\n\t__le32 addr[ADDRS_PER_BLOCK];\t/* array of data block address */\n} __packed;\n\nstruct indirect_node {\n\t__le32 nid[NIDS_PER_BLOCK];\t/* array of data block address */\n} __packed;\n\nenum {\n\tCOLD_BIT_SHIFT = 0,\n\tFSYNC_BIT_SHIFT,\n\tDENT_BIT_SHIFT,\n\tOFFSET_BIT_SHIFT\n};\n\n#define OFFSET_BIT_MASK\t\t(0x07)\t/* (0x01 << OFFSET_BIT_SHIFT) - 1 */\n\nstruct node_footer {\n\t__le32 nid;\t\t/* node id */\n\t__le32 ino;\t\t/* inode nunmber */\n\t__le32 flag;\t\t/* include cold/fsync/dentry marks and offset */\n\t__le64 cp_ver;\t\t/* checkpoint version */\n\t__le32 next_blkaddr;\t/* next node page block address */\n} __packed;\n\nstruct f2fs_node {\n\t/* can be one of three types: inode, direct, and indirect types */\n\tunion {\n\t\tstruct f2fs_inode i;\n\t\tstruct direct_node dn;\n\t\tstruct indirect_node in;\n\t};\n\tstruct node_footer footer;\n} __packed;\n\n/*\n * For NAT entries\n */\n#define NAT_ENTRY_PER_BLOCK (PAGE_SIZE / sizeof(struct f2fs_nat_entry))\n#define NAT_ENTRY_BITMAP_SIZE\t((NAT_ENTRY_PER_BLOCK + 7) / 8)\n\nstruct f2fs_nat_entry {\n\t__u8 version;\t\t/* latest version of cached nat entry */\n\t__le32 ino;\t\t/* inode number */\n\t__le32 block_addr;\t/* block address */\n} __packed;\n\nstruct f2fs_nat_block {\n\tstruct f2fs_nat_entry entries[NAT_ENTRY_PER_BLOCK];\n} __packed;\n\n/*\n * For SIT entries\n *\n * Each segment is 2MB in size by default so that a bitmap for validity of\n * there-in blocks should occupy 64 bytes, 512 bits.\n * Not allow to change this.\n */\n#define SIT_VBLOCK_MAP_SIZE 64\n#define SIT_ENTRY_PER_BLOCK (PAGE_SIZE / sizeof(struct f2fs_sit_entry))\n\n/*\n * F2FS uses 4 bytes to represent block address. As a result, supported size of\n * disk is 16 TB and it equals to 16 * 1024 * 1024 / 2 segments.\n */\n#define F2FS_MAX_SEGMENT       ((16 * 1024 * 1024) / 2)\n\n/*\n * Note that f2fs_sit_entry->vblocks has the following bit-field information.\n * [15:10] : allocation type such as CURSEG_XXXX_TYPE\n * [9:0] : valid block count\n */\n#define SIT_VBLOCKS_SHIFT\t10\n#define SIT_VBLOCKS_MASK\t((1 << SIT_VBLOCKS_SHIFT) - 1)\n#define GET_SIT_VBLOCKS(raw_sit)\t\t\t\t\\\n\t(le16_to_cpu((raw_sit)->vblocks) & SIT_VBLOCKS_MASK)\n#define GET_SIT_TYPE(raw_sit)\t\t\t\t\t\\\n\t((le16_to_cpu((raw_sit)->vblocks) & ~SIT_VBLOCKS_MASK)\t\\\n\t >> SIT_VBLOCKS_SHIFT)\n\nstruct f2fs_sit_entry {\n\t__le16 vblocks;\t\t\t\t/* reference above */\n\t__u8 valid_map[SIT_VBLOCK_MAP_SIZE];\t/* bitmap for valid blocks */\n\t__le64 mtime;\t\t\t\t/* segment age for cleaning */\n} __packed;\n\nstruct f2fs_sit_block {\n\tstruct f2fs_sit_entry entries[SIT_ENTRY_PER_BLOCK];\n} __packed;\n\n/*\n * For segment summary\n *\n * One summary block contains exactly 512 summary entries, which represents\n * exactly 2MB segment by default. Not allow to change the basic units.\n *\n * NOTE: For initializing fields, you must use set_summary\n *\n * - If data page, nid represents dnode's nid\n * - If node page, nid represents the node page's nid.\n *\n * The ofs_in_node is used by only data page. It represents offset\n * from node's page's beginning to get a data block address.\n * ex) data_blkaddr = (block_t)(nodepage_start_address + ofs_in_node)\n */\n#define ENTRIES_IN_SUM\t\t512\n#define\tSUMMARY_SIZE\t\t(7)\t/* sizeof(struct summary) */\n#define\tSUM_FOOTER_SIZE\t\t(5)\t/* sizeof(struct summary_footer) */\n#define SUM_ENTRY_SIZE\t\t(SUMMARY_SIZE * ENTRIES_IN_SUM)\n\n/* a summary entry for a 4KB-sized block in a segment */\nstruct f2fs_summary {\n\t__le32 nid;\t\t/* parent node id */\n\tunion {\n\t\t__u8 reserved[3];\n\t\tstruct {\n\t\t\t__u8 version;\t\t/* node version number */\n\t\t\t__le16 ofs_in_node;\t/* block index in parent node */\n\t\t} __packed;\n\t};\n} __packed;\n\n/* summary block type, node or data, is stored to the summary_footer */\n#define SUM_TYPE_NODE\t\t(1)\n#define SUM_TYPE_DATA\t\t(0)\n\nstruct summary_footer {\n\tunsigned char entry_type;\t/* SUM_TYPE_XXX */\n\t__le32 check_sum;\t\t/* summary checksum */\n} __packed;\n\n#define SUM_JOURNAL_SIZE\t(F2FS_BLKSIZE - SUM_FOOTER_SIZE -\\\n\t\t\t\tSUM_ENTRY_SIZE)\n#define NAT_JOURNAL_ENTRIES\t((SUM_JOURNAL_SIZE - 2) /\\\n\t\t\t\tsizeof(struct nat_journal_entry))\n#define NAT_JOURNAL_RESERVED\t((SUM_JOURNAL_SIZE - 2) %\\\n\t\t\t\tsizeof(struct nat_journal_entry))\n#define SIT_JOURNAL_ENTRIES\t((SUM_JOURNAL_SIZE - 2) /\\\n\t\t\t\tsizeof(struct sit_journal_entry))\n#define SIT_JOURNAL_RESERVED\t((SUM_JOURNAL_SIZE - 2) %\\\n\t\t\t\tsizeof(struct sit_journal_entry))\n\n/* Reserved area should make size of f2fs_extra_info equals to\n * that of nat_journal and sit_journal.\n */\n#define EXTRA_INFO_RESERVED\t(SUM_JOURNAL_SIZE - 2 - 8)\n\n/*\n * frequently updated NAT/SIT entries can be stored in the spare area in\n * summary blocks\n */\nenum {\n\tNAT_JOURNAL = 0,\n\tSIT_JOURNAL\n};\n\nstruct nat_journal_entry {\n\t__le32 nid;\n\tstruct f2fs_nat_entry ne;\n} __packed;\n\nstruct nat_journal {\n\tstruct nat_journal_entry entries[NAT_JOURNAL_ENTRIES];\n\t__u8 reserved[NAT_JOURNAL_RESERVED];\n} __packed;\n\nstruct sit_journal_entry {\n\t__le32 segno;\n\tstruct f2fs_sit_entry se;\n} __packed;\n\nstruct sit_journal {\n\tstruct sit_journal_entry entries[SIT_JOURNAL_ENTRIES];\n\t__u8 reserved[SIT_JOURNAL_RESERVED];\n} __packed;\n\nstruct f2fs_extra_info {\n\t__le64 kbytes_written;\n\t__u8 reserved[EXTRA_INFO_RESERVED];\n} __packed;\n\nstruct f2fs_journal {\n\tunion {\n\t\t__le16 n_nats;\n\t\t__le16 n_sits;\n\t};\n\t/* spare area is used by NAT or SIT journals or extra info */\n\tunion {\n\t\tstruct nat_journal nat_j;\n\t\tstruct sit_journal sit_j;\n\t\tstruct f2fs_extra_info info;\n\t};\n} __packed;\n\n/* 4KB-sized summary block structure */\nstruct f2fs_summary_block {\n\tstruct f2fs_summary entries[ENTRIES_IN_SUM];\n\tstruct f2fs_journal journal;\n\tstruct summary_footer footer;\n} __packed;\n\n/*\n * For directory operations\n */\n#define F2FS_DOT_HASH\t\t0\n#define F2FS_DDOT_HASH\t\tF2FS_DOT_HASH\n#define F2FS_MAX_HASH\t\t(~((0x3ULL) << 62))\n#define F2FS_HASH_COL_BIT\t((0x1ULL) << 63)\n\ntypedef __le32\tf2fs_hash_t;\n\n/* One directory entry slot covers 8bytes-long file name */\n#define F2FS_SLOT_LEN\t\t8\n#define F2FS_SLOT_LEN_BITS\t3\n\n#define GET_DENTRY_SLOTS(x) (((x) + F2FS_SLOT_LEN - 1) >> F2FS_SLOT_LEN_BITS)\n\n/* MAX level for dir lookup */\n#define MAX_DIR_HASH_DEPTH\t63\n\n/* MAX buckets in one level of dir */\n#define MAX_DIR_BUCKETS\t\t(1 << ((MAX_DIR_HASH_DEPTH / 2) - 1))\n\n/*\n * space utilization of regular dentry and inline dentry\n *\t\tregular dentry\t\t\tinline dentry\n * bitmap\t1 * 27 = 27\t\t\t1 * 23 = 23\n * reserved\t1 * 3 = 3\t\t\t1 * 7 = 7\n * dentry\t11 * 214 = 2354\t\t\t11 * 182 = 2002\n * filename\t8 * 214 = 1712\t\t\t8 * 182 = 1456\n * total\t4096\t\t\t\t3488\n *\n * Note: there are more reserved space in inline dentry than in regular\n * dentry, when converting inline dentry we should handle this carefully.\n */\n#define NR_DENTRY_IN_BLOCK\t214\t/* the number of dentry in a block */\n#define SIZE_OF_DIR_ENTRY\t11\t/* by byte */\n#define SIZE_OF_DENTRY_BITMAP\t((NR_DENTRY_IN_BLOCK + BITS_PER_BYTE - 1) / \\\n\t\t\t\t\tBITS_PER_BYTE)\n#define SIZE_OF_RESERVED\t(PAGE_SIZE - ((SIZE_OF_DIR_ENTRY + \\\n\t\t\t\tF2FS_SLOT_LEN) * \\\n\t\t\t\tNR_DENTRY_IN_BLOCK + SIZE_OF_DENTRY_BITMAP))\n\n/* One directory entry slot representing F2FS_SLOT_LEN-sized file name */\nstruct f2fs_dir_entry {\n\t__le32 hash_code;\t/* hash code of file name */\n\t__le32 ino;\t\t/* inode number */\n\t__le16 name_len;\t/* lengh of file name */\n\t__u8 file_type;\t\t/* file type */\n} __packed;\n\n/* 4KB-sized directory entry block */\nstruct f2fs_dentry_block {\n\t/* validity bitmap for directory entries in each block */\n\t__u8 dentry_bitmap[SIZE_OF_DENTRY_BITMAP];\n\t__u8 reserved[SIZE_OF_RESERVED];\n\tstruct f2fs_dir_entry dentry[NR_DENTRY_IN_BLOCK];\n\t__u8 filename[NR_DENTRY_IN_BLOCK][F2FS_SLOT_LEN];\n} __packed;\n\n/* for inline dir */\n#define NR_INLINE_DENTRY\t(MAX_INLINE_DATA * BITS_PER_BYTE / \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tBITS_PER_BYTE + 1))\n#define INLINE_DENTRY_BITMAP_SIZE\t((NR_INLINE_DENTRY + \\\n\t\t\t\t\tBITS_PER_BYTE - 1) / BITS_PER_BYTE)\n#define INLINE_RESERVED_SIZE\t(MAX_INLINE_DATA - \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tNR_INLINE_DENTRY + INLINE_DENTRY_BITMAP_SIZE))\n\n/* inline directory entry structure */\nstruct f2fs_inline_dentry {\n\t__u8 dentry_bitmap[INLINE_DENTRY_BITMAP_SIZE];\n\t__u8 reserved[INLINE_RESERVED_SIZE];\n\tstruct f2fs_dir_entry dentry[NR_INLINE_DENTRY];\n\t__u8 filename[NR_INLINE_DENTRY][F2FS_SLOT_LEN];\n} __packed;\n\n/* file types used in inode_info->flags */\nenum {\n\tF2FS_FT_UNKNOWN,\n\tF2FS_FT_REG_FILE,\n\tF2FS_FT_DIR,\n\tF2FS_FT_CHRDEV,\n\tF2FS_FT_BLKDEV,\n\tF2FS_FT_FIFO,\n\tF2FS_FT_SOCK,\n\tF2FS_FT_SYMLINK,\n\tF2FS_FT_MAX\n};\n\n#define S_SHIFT 12\n\n#endif  /* _LINUX_F2FS_FS_H */\n"], "filenames": ["fs/f2fs/super.c", "include/linux/f2fs_fs.h"], "buggy_code_start_loc": [1496, 304], "buggy_code_end_loc": [1496, 304], "fixing_code_start_loc": [1497, 305], "fixing_code_end_loc": [1504, 311], "type": "NVD-CWE-noinfo", "message": "The sanity_check_raw_super function in fs/f2fs/super.c in the Linux kernel before 4.11.1 does not validate the segment count, which allows local users to gain privileges via unspecified vectors.", "other": {"cve": {"id": "CVE-2017-10662", "sourceIdentifier": "cve@mitre.org", "published": "2017-08-19T18:29:00.320", "lastModified": "2023-01-17T21:02:22.613", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The sanity_check_raw_super function in fs/f2fs/super.c in the Linux kernel before 4.11.1 does not validate the segment count, which allows local users to gain privileges via unspecified vectors."}, {"lang": "es", "value": "La funci\u00f3n sanity_check_raw_super en fs/f2fs/super.c en el kernel Linux en versiones anteriores a la 4.11.1 no valida el conteo de segmentos, lo que permite que usuarios locales obtengan privilegios mediante vectores sin especificar."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.18.53", "matchCriteriaId": "782F0A68-1913-44D3-9276-E9D60E14746F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.41", "matchCriteriaId": "9019BEC9-FE77-4506-A019-B8B4D8BCEBAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.68", "matchCriteriaId": "8E5474E5-2A5B-4534-A1AE-46BE3C5884D6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.9.28", "matchCriteriaId": "08EE535F-28D2-470D-A06D-93027A507803"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.10.16", "matchCriteriaId": "8120FC70-5A9E-433F-AEE0-85DED2CE0B9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.11", "versionEndExcluding": "4.11.1", "matchCriteriaId": "493C208D-F4C9-4EE9-84FC-A33FEF355F46"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b9dd46188edc2f0d1f37328637860bb65a771124", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.11.1", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/100215", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1481146", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/b9dd46188edc2f0d1f37328637860bb65a771124", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://source.android.com/security/bulletin/2017-08-01", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/b9dd46188edc2f0d1f37328637860bb65a771124"}}