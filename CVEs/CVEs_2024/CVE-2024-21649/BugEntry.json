{"buggy_code": [".. _algo-code_structure:\n\nAlgorithm code structure\n========================\n\n.. note::\n\n  These guidelines are Python specific.\n\nHere we provide some more information on algorithm code is organized.\nMost of these structures are generated automatically when you create a\n:ref:`personalized algorithm starting point <algo-dev-create-algorithm>`. We detail\nthem here so that you understand why the algorithm code is structured as it is,\nand so that you know how to modify it if necessary.\n\nDefining functions\n------------------\n\nThe functions that will be available to the user have to be defined in the\n``__init__.py`` file at the base of your algorithm module. Other than that,\nyou have complete freedom in which functions you implement.\n\nVantage6 algorithms commonly have an orchestator or aggregator part and a\nremote part. The orchestrator part is responsible for combining the partial\nresults of the remote parts. The remote part is usually executed at each of the\nnodes included in the analysis. While this structure is common for vantage6\nalgorithms, it is not required.\n\nIf you do follow this structure however, we recommend the following file\nstructure:\n\n.. code:: bash\n\n   my_algorithm/\n   \u251c\u2500\u2500 __init__.py\n   \u251c\u2500\u2500 central.py\n   \u2514\u2500\u2500 partial.py\n\nwhere ``__init__.py`` contains the following:\n\n.. code:: python\n\n   from .central import my_central_function\n   from .partial import my_partial_function\n\nand where ``central.py`` and ``partial.py`` obviously contain the implementation\nof those functions.\n\n.. _implementing-decorators:\n\nImplementing the algorithm functions\n------------------------------------\n\nLet's say you are implementing a function called ``my_function``:\n\n.. code:: python\n\n   def my_function(column_name: str):\n       pass\n\nYou have complete freedom as to what arguments you define in your function;\n``column_name`` is just an example. Note that these arguments\nhave to be provided by the user when the algorithm is called. This is explained\n:ref:`here <pyclient-create-task>` for the Python client.\n\nOften, you will want to use the data that is available at the node. This data\ncan be provided to your algorithm function in the following way:\n\n.. code:: python\n\n    import pandas as pd\n    from vantage6.algorithm.tools.decorators import data\n\n    @data(2)\n    def my_function(df1: pd.DataFrame, df2: pd.DataFrame, column_name: str):\n        pass\n\nThe ``@data(2)`` decorator indicates that the first two arguments of the\nfunction are dataframes that should be provided by the vantage6 infrastructure.\nIn this case, the user would have to specify two databases when calling the\nalgorithm. Note that depending on the type of the database used, the user may\nalso have to specify additional parameters such as a SQL query or the name of a\nworksheet in an Excel file.\n\nNote that it is also possible to just specify ``@data()`` without an argument -\nin that case, a single dataframe is added to the arguments.\n\nFor some data sources it's not trivial to construct a dataframe from the data.\nOne of these data sources is the OHDSI OMOP CDM database. For this data source,\nthe ``@database_connection`` is available:\n\n.. code:: python\n\n    from rpy2.robjects import RS4\n    from vantage6.algorithm.tools.decorators import (\n        database_connection, OHDSIMetaData\n    )\n\n    @database_connection(types=[\"OMOP\"], include_metadata=True)\n    def my_function(connection: RS4, metadata: OHDSIMetaData,\n                    <other_arguments>):\n        pass\n\nThis decorator provides the algorithm with a database connection that can be\nused to interact with the database. For instance, you can use this connection\nto execute functions from\n`python-ohdsi <https://python-ohdsi.readthedocs.io/>`_ package. The\n``include_metadata`` argument indicates whether the metadata of the database\nshould also be provided. It is possible to connect to multiple databases at\nonce, but you can also specify a single database by using the ``types``\nargument.\n\n.. code:: python\n\n    from rpy2.robjects import RS4\n    from vantage6.algorithm.tools.decorators import database_connection\n\n    @database_connection(types=[\"OMOP\", \"OMOP\"], include_metadata=False)\n    def my_function(connection1: RS4, connection2: Connection,\n                    <other_arguments>):\n        pass\n\n.. note::\n\n    The ``@database_connection`` decorator is current only available for\n    OMOP CDM databases. The connection object ``RS4`` is an R object, mapped\n    to Python using the `rpy2 <https://rpy2.github.io/>`_, package. This\n    object can be passed directly on to the functions from\n    `python-ohdsi <https://python-ohdsi.readthedocs.io/>`.\n\nAnother useful decorator is the ``@algorithm_client`` decorator:\n\n.. code:: python\n\n    import pandas as pd\n    from vantage6.client.algorithm_client import AlgorithmClient\n    from vantage6.algorithm.tools.decorators import algorithm_client, data\n\n    @data()\n    @algorithm_client\n    def my_function(client: AlgorithmClient, df1: pd.DataFrame, column_name: str):\n        pass\n\nThis decorator provides the algorithm with a client that can be used to interact\nwith the vantage6 central server. For instance, you can use this client in\nthe central part of an algorithm to create a subtasks for each node with\n``client.task.create()``. A full list of all commands that are available\ncan be found in the :ref:`algorithm client documentation <algo-client-api-ref>`.\n\n.. warning::\n\n    The decorators ``@data`` and ``@algorithm_client`` each have one reserved\n    keyword: ``mock_data`` for the ``@data`` decorator and ``mock_client`` for\n    the ``@algorithm_client`` decorator. These keywords should not be used as\n    argument names in your algorithm functions.\n\n    The reserved keywords are used by the\n    :ref:`MockAlgorithmClient <mock-test-algo-dev>` to mock the data and the\n    algorithm client. This is useful for testing your algorithm locally.\n\n\nAlgorithm wrappers\n------------------\n\nThe vantage6 :ref:`wrappers <wrapper-concepts>` are used to simplify the\ninteraction between the algorithm and the node. The wrappers are responsible\nfor reading the input data from the data source and supplying it to the algorithm.\nThey also take care of writing the results back to the data source.\n\nAs algorithm developer, you do not have to worry about the wrappers. The only\nthing you have to make sure is that the following line is present at the end of\nyour ``Dockerfile``:\n\n.. code:: docker\n\n    CMD python -c \"from vantage6.algorithm.tools.wrap import wrap_algorithm; wrap_algorithm('${PKG_NAME}')\"\n\nwhere ``${PKG_NAME}`` is the name of your algorithm package. The ``wrap_algorithm``\nfunction will wrap your algorithm.\n\nFor R, the command is slightly different:\n\n.. code:: r\n\n   CMD Rscript -e \"vtg::docker.wrapper('$PKG_NAME')\"\n\nAlso, note that when using R, this only works for CSV files.\n\n.. _vpn-in-algo-dev:\n\nVPN\n---\n\nWithin vantage6, it is possible to communicate with algorithm instances running\non different nodes via the :ref:`VPN network feature <vpn-feature>`. Each of\nthe algorithm instances has their own IP address and port within the VPN\nnetwork. In your algorithm code, you can use the ``AlgorithmClient`` to obtain\nthe IP address and port of other algorithm instances. For example:\n\n.. code:: python\n\n    from vantage6.client import AlgorithmClient\n\n    def my_function(client: AlgorithmClient, ...):\n        # Get the IP address and port of the algorithm instance with id 1\n        child_addresses = client.get_child_addresses()\n        # returns something like:\n        # [\n        #     {\n        #       'port': 1234,\n        #       'ip': 11.22.33.44,\n        #       'label': 'some_label',\n        #       'organization_id': 22,\n        #       'task_id': 333,\n        #       'parent_id': 332,\n        #     }, ...\n        # ]\n\n        # Do something with the IP address and port\n\nThe function ``get_child_addresses()`` gets the VPN addresses of all child\ntasks of the current task. Similarly, the function ``get_parent_address()``\nis available to get the VPN address of the parent task. Finally, there is\na client function ``get_addresses()`` that returns the VPN addresses of all\nalgorithm instances that are part of the same task.\n\nVPN communication is only possible if the docker container exposes ports to\nthe VPN network. In the algorithm boilerplate, one port is exposed by default.\nIf you need to expose more ports (e.g. for sending different information to\ndifferent parts of your algorithm), you can do so by adding lines to the\nDockerfile:\n\n.. code:: bash\n\n   # port 8888 is used by the algorithm for communication purposes\n   EXPOSE 8888\n   LABEL p8888 = \"some-label\"\n\n   # port 8889 is used by the algorithm for data-exchange\n   EXPOSE 8889\n   LABEL p8889 = \"some-other-label\"\n\nThe ``EXPOSE`` command exposes the port to the VPN network. The ``LABEL``\ncommand adds a label to the port. This label returned with the clients'\n``get_addresses()`` function suite. You may specify as many ports as you need.\nNote that you *must* specify the label with ``p`` as prefix followed by the\nport number. The vantage6 infrastructure relies on this naming convention.\n\n\nDockerfile structure\n--------------------\n\nOnce the algorithm code is written, the algorithm needs to be packaged and made\navailable for retrieval by the nodes. The algorithm is packaged in a Docker\nimage. A Docker image is created from a Dockerfile, which acts as a blue-print.\n\nThe Dockerfile is already present in the boilerplate code. Usually, the only\nline that you need to update is the ``PKG_NAME`` variable to the name of your\nalgorithm package.\n\n", ".. _algo-dev-guide:\n\nAlgorithm development step-by-step guide\n========================================\n\nThis page offers a step-by-step guide to develop a vantage6 algorithm.\nWe refer to the `algorithm concepts <algo-concepts>`_ section\nregularly. In that section, we explain the fundamentals of algorithm containers\nin more detail than in this guide.\n\nAlso, note that this guide is mainly aimed at developers who want to develop\ntheir algorithm in Python, although we will try to clearly indicate where\nthis differs from algorithms written in other languages.\n\n.. _algo-dev-create-algorithm:\n\nStarting point\n--------------\n\nWhen starting to develop a new vantage6 algorithm in Python, the easiest way to\nstart is:\n\n.. code::\n\n   v6 algorithm create\n\nRunning this command will prompt you to answering some questions, which will\nresult in a personalized starting point or 'boilerplate' for your algorithm.\nAfter doing so, you will have a new folder with the name of your algorithm,\nboilerplate code and a checklist in the README.md file that you can follow to\ncomplete your algorithm.\n\n.. note::\n   There is also a `boilerplate for R <https://github.com/IKNL/vtg.tpl>`_,\n   but this is not flexible and it is not updated as frequently as the Python\n   boilerplate.\n\nSetting up your environment\n---------------------------\n\nIt is good practice to set up a virtual environment for your algorithm\npackage.\n\n.. code:: bash\n\n   # This code is just a suggestion - there are many ways of doing this.\n\n   # go to the algorithm directory\n   cd /path/to/algorithm\n\n   # create a Python environment. Be sure to replace <my-algorithm-env> with\n   # the name of your environment.\n   conda create -n <my-algorithm-env> python=3.10\n   conda activate <my-algorithm-env>\n\n   # install the algorithm dependencies\n   pip install -r requirements.txt\n\nAlso, it is always good to use a version control system such as ``git`` to\nkeep track of your changes. An initial commit of the boilerplate code could be:\n\n.. code:: bash\n\n   cd /path/to/algorithm\n   git init\n   git add .\n   git commit -m \"Initial commit\"\n\nNote that having your code in a git repository is necessary if you want to\n:ref:`update your algorithm <algo-dev-update-algo>`.\n\nImplementing your algorithm\n---------------------------\n\nYour personalized starting point should make clear to you which functions you need to\nimplement - there are `TODO` comments in the code that indicate where you need\nto add your own code.\n\nYou may wonder why the boilerplate code is structured the way it is. This\nis explained in the :ref:`code structure section <algo-code_structure>`.\n\n.. _algo-env-vars:\n\nEnvironment variables\n---------------------\n\nThe algorithms have access to several environment variables. You can also\nspecify additional environment variables via the ``algorithm_env`` option\nin the node configuration files (see the\n:ref:`example node configuration file <node-configure-structure>`).\n\nEnvironment variables provided by the vantage6 infrastructure are used\nto locate certain files or to add local configuration settings into the\ncontainer. These are usually used in the Python wrapper and you don't normally\nneed them in your functions. However, you can access them in your functions\nas follows:\n\n.. code:: python\n\n   import os\n\n   def my_function():\n       input_file = os.environ[\"INPUT_FILE\"]\n       token_file = os.environ[\"DEFAULT_DATABASE_URI\"]\n\n       # do something with the input file and database URI\n\nThe environment variables that you specify in the node configuration file\ncan be used in the exact same manner. You can view all environment variables\nthat are available to your algorithm by ``print(os.environ)``.\n\nReturning results\n-----------------\n\nReturning the results of you algorithm is rather straightforward. At the end\nof your algorithm function, you can simply return the results as a dictionary:\n\n.. code:: python\n\n    def my_function(column_name: str):\n        return {\n            \"result\": 42\n        }\n\nThese results will be returned to the user after the algorithm has finished.\n\n.. warning::\n\n    The results that you return should be JSON serializable. This means that\n    you cannot, for example, return a ``pandas.DataFrame`` or a\n    ``numpy.ndarray`` (such objects may not be readable to a non-Python using\n    recipient or may even be insecure to send over the internet). They should\n    be converted to a JSON-serializable format first.\n\nExample functions\n-----------------\n\nJust an example of how you can implement your algorithm:\n\nCentral function\n~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n  from vantage6.algorithm.tools.decorators import algorithm_client\n  from vantage6.client.algorithm_client import AlgorithmClient\n\n   @algorithm_client\n   def main(client: AlgorithmClient, *args, **kwargs):\n      # Run partial function.\n      task = client.task.create(\n         {\n            \"method\": \"my_algorithm\",\n            \"args\": args,\n            \"kwargs\": kwargs\n         },\n         organization_ids=[1, 2]\n      )\n\n       # wait for the federated part to complete\n       # and return\n       results = wait_and_collect(task)\n\n       return results\n\nPartial function\n~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n   import pandas as pd\n   from vantage6.algorithm.tools.decorators import data\n\n   @data(1)\n   def my_partial_function(data: pd.DataFrame, column_name: str):\n       # do something with the data\n       data[column_name] = data[column_name] + 1\n\n       # return the results\n       return {\n           \"result\": sum(data[colum_name].to_list())\n       }\n\n.. _mock-test-algo-dev:\n\nTesting your algorithm\n----------------------\n\nIt can be helpful to test your algorithm outside of Docker using the\n``MockAlgorithmClient``. This may save\ntime as it does not require you to set up a test infrastructure with a vantage6\nserver and nodes, and allows you to test your algorithm without building a\nDocker image every time. The algorithm boilerplate code comes with a test file that\nyou can use to test your algorithm using the ``MockAlgorithmClient`` - you can\nof course extend that to add more or different tests.\n\nThe :ref:`MockAlgorithmClient <mock-client-api-ref>` has the same interface as\nthe ``AlgorithmClient``, so it should be easy to switch between the two. An\nexample of how you can use the ``MockAlgorithmClient`` to test your algorithm\nis included in the boilerplate code.\n\nWriting documentation\n---------------------\n\nIt is important that you add documentation of your algorithm so that users\nknow how to use it. In principle, you may choose any format of documentation,\nand you may choose to host it anywhere you like. However, in our experience it\nworks well to keep your documentation close to your code. We recommend using the\n``readthedocs`` platform to host your documentation. Alternatively, you could\nuse a ``README`` file in the root of your algorithm directory - if the\ndocumentation is not too extensive, this may be sufficient.\n\n.. note::\n\n    We intend to provide a template for the documentation of algorithms in the\n    future. This template will be based on the ``readthedocs`` platform.\n\nPackage & distribute\n--------------------\n\nThe algorithm boilerplate comes with a ``Dockerfile`` that is a blueprint for\ncreating a Docker image of your algorithm. This Docker image is the package\nthat you will distribute to the nodes.\n\nIf you go to the folder containing your algorithm, you will also find the\nDockerfile there, immediately at the top directory. You can then build the\nproject as follows:\n\n.. code:: bash\n\n   docker build -t repo/image:tag .\n\nThe ``-t`` indicated the name of your image. This name is also used as\nreference where the image is located on the internet. Once the Docker image is\ncreated it needs to be uploaded to a registry so that nodes can retrieve it,\nwhich you can do by pushing the image:\n\n.. code:: bash\n\n   docker push repo/image:tag\n\nHere are a few examples of how to build and upload your image:\n\n.. code:: bash\n\n    # Build and upload to Docker Hub. Replace <my-user-name> with your Docker\n    # Hub username and make sure you are logged in with ``docker login``.\n    docker build -t my-user-name/algorithm-example:latest .\n    docker push my-user-name/algorithm-example:latest\n\n    # Build and upload to private registry. Here you don't need to provide\n    # a username but you should write out the full image URL. Also, again you\n    # need to be logged in with ``docker login``.\n    docker build -t harbor2.vantage6.ai/PROJECT/algorithm-example:latest .\n    docker push harbor2.vantage6.ai/PROJECT/algorithm-example:latest\n\nNow that your algorithm has been uploaded it is available for nodes to retrieve\nwhen they need it.\n\nCalling your algorithm from vantage6\n------------------------------------\n\nIf you want to test your algorithm in the context of vantage6, you should\nset up a vantage6 infrastructure. You should create a server and at least one\nnode (depending on your algorithm you may need more). Follow the instructions\nin the :ref:`server-admin-guide` and :ref:`node-admin-guide` to set up your\ninfrastructure. If you are running them on the same machine, take care to\nprovide the node with the proper address of the server as detailed\n:ref:`here <use-server-local>`.\n\nOnce your infrastructure is set up, you can create a task for your algorithm.\nYou can do this either via the :ref:`UI <ui>` or via the\n:ref:`Python client <pyclient-create-task>`.\n\n.. todo Add example with ``v6 dev``\n\n.. _algo-dev-update-algo:\n\nUpdating your algorithm\n-----------------------\n\nAt some point, there may be changes in the vantage6 infrastructure that require\nyou to update your algorithm. Such changes are made available via\nthe ``v6 algorithm update`` command. This command will update your algorithm\nto the latest version of the vantage6 infrastructure.\n\nYou can also use the ``v6 algorithm update`` command to update your algorithm\nif you want to modify your answers to the questionnaire. In that case, you\nshould be sure to commit the changes in ``git`` before running the command.\n", "import os\nimport json\nimport jwt\n\nfrom pathlib import Path\nfrom functools import wraps\nfrom dataclasses import dataclass\n\nimport pandas as pd\n\nfrom vantage6.algorithm.client import AlgorithmClient\nfrom vantage6.algorithm.tools.mock_client import MockAlgorithmClient\nfrom vantage6.algorithm.tools.util import info, error, warn\nfrom vantage6.algorithm.tools.wrappers import load_data\nfrom vantage6.algorithm.tools.preprocessing import preprocess_data\n\nOHDSI_AVAILABLE = True\ntry:\n    from ohdsi.database_connector import connect as connect_to_omop\nexcept ImportError:\n    OHDSI_AVAILABLE = False\n\n\n@dataclass\nclass RunMetaData:\n    \"\"\"Dataclass containing metadata of the run.\"\"\"\n    task_id: int | None\n    node_id: int | None\n    collaboration_id: int | None\n    organization_id: int | None\n    temporary_directory: Path | None\n    output_file: Path | None\n    input_file: Path | None\n    token_file: Path | None\n\n\n@dataclass\nclass OHDSIMetaData:\n    \"\"\"Dataclass containing metadata of the OMOP database.\"\"\"\n    database: str | None\n    cdm_schema: str | None\n    results_schema: str | None\n    incremental_folder: Path | None\n    cohort_statistics_folder: Path | None\n    export_folder: Path | None\n\n\ndef _algorithm_client() -> callable:\n    \"\"\"\n    Decorator that adds an algorithm client object to a function\n\n    By adding @algorithm_client to a function, the ``algorithm_client``\n    argument will be added to the front of the argument list. This client can\n    be used to communicate with the server.\n\n    There is one reserved argument `mock_client` in the function to be\n    decorated. If this argument is provided, the decorator will add this\n    MockAlgorithmClient to the front of the argument list instead of the\n    regular AlgorithmClient.\n\n    Parameters\n    ----------\n    func : callable\n        Function to decorate\n\n    Returns\n    -------\n    callable\n        Decorated function\n\n    Examples\n    --------\n    >>> @algorithm_client\n    >>> def my_algorithm(algorithm_client: AlgorithmClient, <other arguments>):\n    >>>     pass\n    \"\"\"\n    def protection_decorator(func: callable, *args, **kwargs) -> callable:\n        @wraps(func)\n        def decorator(*args, mock_client: MockAlgorithmClient = None,\n                      **kwargs) -> callable:\n            \"\"\"\n            Wrap the function with the client object\n\n            Parameters\n            ----------\n            mock_client : MockAlgorithmClient\n                Mock client to use instead of the regular client\n            \"\"\"\n            if mock_client is not None:\n                return func(mock_client, *args, **kwargs)\n            # read server address from the environment\n            host = os.environ[\"HOST\"]\n            port = os.environ[\"PORT\"]\n            api_path = os.environ[\"API_PATH\"]\n\n            # read token from the environment\n            token_file = os.environ[\"TOKEN_FILE\"]\n            info(\"Reading token\")\n            with open(token_file) as fp:\n                token = fp.read().strip()\n\n            client = AlgorithmClient(token=token, host=host, port=port,\n                                     path=api_path)\n            return func(client, *args, **kwargs)\n        # set attribute that this function is wrapped in an algorithm client\n        decorator.wrapped_in_algorithm_client_decorator = True\n        return decorator\n    return protection_decorator\n\n\n# alias for algorithm_client so that algorithm developers can do\n# @algorithm_client instead of @algorithm_client()\nalgorithm_client = _algorithm_client()\n\n\ndef data(number_of_databases: int = 1) -> callable:\n    \"\"\"\n    Decorator that adds algorithm data to a function\n\n    By adding `@data()` to a function, one or several pandas dataframes will be\n    added to the front of the argument list. This data will be read from the\n    databases that the user who creates the task provides.\n\n    Note that the user should provide exactly as many databases as the\n    decorated function requires when they create the task.\n\n    There is one reserved argument `mock_data` in the function to be\n    decorated. If this argument is provided, the decorator will add this\n    mocked data to the front of the argument list, instead of reading in the\n    data from the databases.\n\n    Parameters\n    ----------\n    number_of_databases: int\n        Number of data sources to load. These will be loaded in order by which\n        the user provided them. Default is 1.\n\n    Returns\n    -------\n    callable\n        Decorated function\n\n    Examples\n    --------\n    >>> @data(number_of_databases=2)\n    >>> def my_algorithm(first_df: pd.DataFrame, second_df: pd.DataFrame,\n    >>>                  <other arguments>):\n    >>>     pass\n    \"\"\"\n    def protection_decorator(func: callable, *args, **kwargs) -> callable:\n        @wraps(func)\n        def decorator(*args, mock_data: list[pd.DataFrame] = None,\n                      **kwargs) -> callable:\n            \"\"\"\n            Wrap the function with the data\n\n            Parameters\n            ----------\n            mock_data : list[pd.DataFrame]\n                Mock data to use instead of the regular data\n            \"\"\"\n            if mock_data is not None:\n                return func(*mock_data, *args, **kwargs)\n\n            # read the labels that the user requested\n            labels = _get_user_database_labels()\n\n            # check if user provided enough databases\n            if len(labels) < number_of_databases:\n                error(f\"Algorithm requires {number_of_databases} databases \"\n                      f\"but only {len(labels)} were provided. \"\n                      \"Exiting...\")\n                exit(1)\n            elif len(labels) > number_of_databases:\n                warn(f\"Algorithm requires only {number_of_databases} databases\"\n                     f\", but {len(labels)} were provided. Using the \"\n                     f\"first {number_of_databases} databases.\")\n\n            for i in range(number_of_databases):\n                label = labels[i]\n                # read the data from the database\n                info(\"Reading data from database\")\n                data_ = _get_data_from_label(label)\n\n                # do any data preprocessing here\n                info(f\"Applying preprocessing for database '{label}'\")\n                env_prepro = os.environ.get(f\"{label.upper()}_PREPROCESSING\")\n                if env_prepro is not None:\n                    preprocess = json.loads(env_prepro)\n                    data_ = preprocess_data(data_, preprocess)\n\n                # add the data to the arguments\n                args = (data_, *args)\n\n            return func(*args, **kwargs)\n        # set attribute that this function is wrapped in a data decorator\n        decorator.wrapped_in_data_decorator = True\n        return decorator\n    return protection_decorator\n\n\ndef database_connection(types: list[str], include_metadata: bool = True) \\\n        -> callable:\n    \"\"\"\n    Decorator that adds a database connection to a function\n\n    By adding @database_connection to a function, a database connection will\n    be added to the front of the argument list. This connection can be used to\n    communicate with the database.\n\n    Parameters\n    ----------\n    types : list[str]\n        List of types of databases to connect to. Currently only \"OMOP\" is\n        supported.\n    include_metadata : bool\n        Whether to include metadata in the function arguments. This metadata\n        contains the database name, CDM schema, and results schema. Default is\n        True.\n\n    Example\n    -------\n    For a single OMOP data source:\n    >>> @database_connection(types=[\"OMOP\"])\n    >>> def my_algorithm(connection: Connection, meta: OHDSIMetaData,\n    >>>                  <other arguments>):\n    >>>     pass\n\n    In case you have multiple OMOP data sources:\n    >>> @database_connection(types=[\"OMOP\", \"OMOP\"])\n    >>> def my_algorithm(connection1: Connection, meta1: OHDSIMetaData,\n    >>>                  connection2: Connection, meta2: OHDSIMetaData,\n    >>>                  <other arguments>):\n    >>>     pass\n\n    In the case you do not want to include the metadata:\n    >>> @database_connection(types=[\"OMOP\"], include_metadata=False)\n    >>> def my_algorithm(connection: Connection, <other arguments>):\n    >>>     pass\n    \"\"\"\n    def connection_decorator(func: callable, *args, **kwargs) -> callable:\n        @wraps(func)\n        def decorator(*args, **kwargs) -> callable:\n            \"\"\"\n            Wrap the function with the database connection\n            \"\"\"\n            labels = _get_user_database_labels()\n            if len(labels) < len(types):\n                error(f\"User provided {len(labels)} databases, but algorithm \"\n                      f\"requires {len(types)} database connections. Exiting.\")\n                exit(1)\n            if len(labels) > len(types):\n                warn(f\"User provided {len(labels)} databases, but algorithm \"\n                     f\"requires {len(types)} database connections. Using the \"\n                     f\"first {len(types)} databases.\")\n\n            db_args = []\n            # Note: zip will stop at the shortest iterable, so this is exactly\n            # what we want in the len(labels) > len(types) case.\n            for type_, label in zip(types, labels):\n                match type_.upper():\n                    case \"OMOP\":\n                        info(\"Creating OMOP database connection\")\n                        connection = _create_omop_database_connection(label)\n                        db_args.append(connection)\n                        if include_metadata:\n                            meta = get_ohdsi_metadata(label)\n                            db_args.append(meta)\n                    # case \"FHIR\":\n                    #     pass\n\n            return func(*db_args, *args, **kwargs)\n\n        return decorator\n    return connection_decorator\n\n\ndef metadata(func: callable) -> callable:\n    @wraps(func)\n    def decorator(*args, **kwargs) -> callable:\n        \"\"\"\n        Decorator the function with metadata from the run.\n\n        Decorator that adds metadata from the run to the function. This\n        includes the task id, node id, collaboration id, organization id,\n        temporary directory, output file, input file, and token file.\n\n        Example\n        -------\n        >>> @metadata\n        >>> def my_algorithm(metadata: RunMetaData, <other arguments>):\n        >>>     pass\n        \"\"\"\n        token_file = os.environ[\"TOKEN_FILE\"]\n        info(\"Reading token\")\n        with open(token_file) as fp:\n            token = fp.read().strip()\n\n        info(\"Extracting payload from token\")\n        payload = _extract_token_payload(token)\n\n        metadata = RunMetaData(\n            task_id=payload[\"task_id\"],\n            node_id=payload[\"node_id\"],\n            collaboration_id=payload[\"collaboration_id\"],\n            organization_id=payload[\"organization_id\"],\n            temporary_directory=Path(os.environ[\"TEMPORARY_FOLDER\"]),\n            output_file=Path(os.environ[\"OUTPUT_FILE\"]),\n            input_file=Path(os.environ[\"INPUT_FILE\"]),\n            token_file=Path(os.environ[\"TOKEN_FILE\"])\n        )\n        return func(metadata, *args, **kwargs)\n    return decorator\n\n\ndef get_ohdsi_metadata(label: str) -> OHDSIMetaData:\n    \"\"\"\n    Retrieve the OHDSI metadata from the environment variables.\n\n    The following environment variables are expected to be set in the\n    node configuration in the `env` key of the `database` section:\n\n    - `CDM_DATABASE`\n    - `CDM_SCHEMA`\n    - `RESULTS_SCHEMA`\n\n    In case these are not set, the algorithm execution is terminated.\n\n    Example\n    -------\n    >>> get_ohdsi_metadata(\"my_database\")\n    \"\"\"\n    # check that all node environment variables are set\n    expected_env_vars = [\"CDM_DATABASE\", \"CDM_SCHEMA\", \"RESULTS_SCHEMA\"]\n    label_ = label.upper()\n    for var in expected_env_vars:\n        _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')\n\n    tmp = Path(os.environ[\"TEMPORARY_FOLDER\"])\n    metadata = OHDSIMetaData(\n        database=os.environ[f\"{label_}_DB_PARAM_CDM_DATABASE\"],\n        cdm_schema=os.environ[f\"{label_}_DB_PARAM_CDM_SCHEMA\"],\n        results_schema=os.environ[f\"{label_}_DB_PARAM_RESULTS_SCHEMA\"],\n        incremental_folder=tmp / \"incremental\",\n        cohort_statistics_folder=tmp / \"cohort_statistics\",\n        export_folder=tmp / \"export\"\n    )\n    return metadata\n\n\ndef _create_omop_database_connection(label: str) -> callable:\n    \"\"\"\n    Create a connection to an OMOP database.\n\n    It expects that the following environment variables are set:\n    - DB_PARAM_DBMS: type of database to connect to\n    - DB_PARAM_USER: username to connect to the database\n    - DB_PARAM_PASSWORD: password to connect to the database\n\n    These should be provided in the vantage6 node configuration file in the\n    `database` section without the `DB_PARAM_` prefix. For example:\n\n    ```yaml\n    ...\n    databases:\n      - label: my_database\n        type: OMOP\n        uri: jdbc:postgresql://host.docker.internal:5454/postgres\n        env:\n            DBMS: \"postgresql\"\n            USER: \"my_user\"\n            PASSWORD: \"my_password\"\n    ...\n    ```\n\n    Parameters\n    ----------\n    label : str\n        Label of the database to connect to\n\n    Returns\n    -------\n    callable\n        OHDSI Database Connection object\n    \"\"\"\n\n    # check that the OHDSI package is available in this container\n    if not OHDSI_AVAILABLE:\n        error(\"OHDSI/DatabaseConnector is not available.\")\n        error(\"Did you use 'algorithm-ohdsi-base' image to build this \"\n              \"algorithm?\")\n        exit(1)\n\n    # environment vars are always uppercase\n    label_ = label.upper()\n\n    # check that the required environment variables are set\n    for var in (\"DBMS\", \"USER\", \"PASSWORD\"):\n        _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')\n\n    info(\"Reading OHDSI environment variables\")\n    dbms = os.environ[f\"{label_}_DB_PARAM_DBMS\"]\n    uri = os.environ[f\"{label_}_DATABASE_URI\"]\n    user = os.environ[f\"{label_}_DB_PARAM_USER\"]\n    password = os.environ[f\"{label_}_DB_PARAM_PASSWORD\"]\n    info(f' - dbms: {dbms}')\n    info(f' - uri: {uri}')\n    info(f' - user: {user}')\n\n    info(\"Creating OHDSI database connection\")\n    return connect_to_omop(dbms=dbms, connection_string=uri, password=password,\n                           user=user)\n\n\ndef _check_environment_var_exists_or_exit(var: str):\n    \"\"\"\n    Check if the environment variable 'var' exists or print and exit.\n\n    Parameters\n    ----------\n    var : str\n        Environment variable name to check\n    \"\"\"\n    if var not in os.environ:\n        error(f\"Environment variable '{var}' is not set. Exiting...\")\n        exit(1)\n\n\ndef _get_data_from_label(label: str) -> pd.DataFrame:\n    \"\"\"\n    Load data from a database based on the label\n\n    Parameters\n    ----------\n    label : str\n        Label of the database to load\n\n    Returns\n    -------\n    pd.DataFrame\n        Data from the database\n    \"\"\"\n    # Load the input data from the input file - this may e.g. include the\n    database_uri = os.environ[f\"{label.upper()}_DATABASE_URI\"]\n    info(f\"Using '{database_uri}' with label '{label}' as database\")\n\n    # Get the database type from the environment variable, this variable is\n    # set by the vantage6 node based on its configuration file.\n    database_type = os.environ.get(\n        f\"{label.upper()}_DATABASE_TYPE\", \"csv\").lower()\n\n    # Load the data based on the database type. Try to provide environment\n    # variables that should be available for some data types.\n    return load_data(\n        database_uri,\n        database_type,\n        query=os.environ.get(f\"{label.upper()}_QUERY\"),\n        sheet_name=os.environ.get(f\"{label.upper()}_SHEET_NAME\")\n    )\n\n\ndef _get_user_database_labels() -> list[str]:\n    \"\"\"\n    Get the database labels from the environment\n\n    Returns\n    -------\n    list[str]\n        List of database labels\n    \"\"\"\n    # read the labels that the user requested, which is a comma\n    # separated list of labels.\n    labels = os.environ[\"USER_REQUESTED_DATABASE_LABELS\"]\n    return labels.split(',')\n\n\ndef _extract_token_payload(token: str) -> dict:\n    \"\"\"\n    Extract the payload from the token.\n\n    Parameters\n    ----------\n    token: str\n        The token as a string.\n\n    Returns\n    -------\n    dict\n        The payload as a dictionary. It contains the keys: `client_type`,\n        `node_id`, `organization_id`, `collaboration_id`, `task_id`, `image`,\n        and `databases`.\n    \"\"\"\n    jwt_payload = jwt.decode(token, options={\"verify_signature\": False})\n    return jwt_payload['sub']\n", "import sys\n\n\ndef info(msg: str) -> None:\n    \"\"\"\n    Print an info message to stdout.\n\n    Parameters\n    ----------\n    msg : str\n        Message to be printed\n    \"\"\"\n    sys.stdout.write(f\"info > {msg}\\n\")\n\n\ndef warn(msg: str) -> None:\n    \"\"\"\n    Print a warning message to stdout.\n\n    Parameters\n    ----------\n    msg : str\n        Warning message to be printed\n    \"\"\"\n    sys.stdout.write(f\"warn > {msg}\\n\")\n\n\ndef error(msg: str) -> None:\n    \"\"\"\n    Print an error message to stdout.\n\n    Parameters\n    ----------\n    msg : str\n        Error message to be printed\n    \"\"\"\n    sys.stdout.write(f\"error > {msg}\\n\")\n", "import os\nimport importlib\nimport traceback\n\nfrom typing import Any\n\nfrom vantage6.common.client import deserialization\nfrom vantage6.common import serialization\nfrom vantage6.algorithm.tools.util import info, error\nfrom vantage6.algorithm.tools.exceptions import DeserializationException\n\n\ndef wrap_algorithm(module: str, log_traceback: bool = True) -> None:\n    \"\"\"\n    Wrap an algorithm module to provide input and output handling for the\n    vantage6 infrastructure.\n\n    Data is received in the form of files, whose location should be\n    specified in the following environment variables:\n\n    - ``INPUT_FILE``: input arguments for the algorithm. This file should be\n      encoded in JSON format.\n    - ``OUTPUT_FILE``: location where the results of the algorithm should\n      be stored\n    - ``TOKEN_FILE``: access token for the vantage6 server REST api\n    - ``USER_REQUESTED_DATABASE_LABELS``: comma-separated list of database\n      labels that the user requested\n    - ``<DB_LABEL>_DATABASE_URI``: uri of the each of the databases that\n      the user requested, where ``<DB_LABEL>`` is the label of the\n      database given in ``USER_REQUESTED_DATABASE_LABELS``.\n\n    The wrapper expects the input file to be a json file. Any other file\n    format will result in an error.\n\n    Parameters\n    ----------\n    module : str\n        Python module name of the algorithm to wrap.\n    log_traceback: bool\n        Whether to print the full error message from algorithms or not, by\n        default False. Algorithm developers should set this to False if\n        the error messages may contain sensitive information. By default True.\n    \"\"\"\n    info(f\"wrapper for {module}\")\n\n    # read input from the mounted input file.\n    input_file = os.environ[\"INPUT_FILE\"]\n    info(f\"Reading input file {input_file}\")\n    input_data = load_input(input_file)\n\n    # make the actual call to the method/function\n    info(\"Dispatching ...\")\n    output = _run_algorithm_method(input_data, module, log_traceback)\n\n    # write output from the method to mounted output file. Which will be\n    # transferred back to the server by the node-instance.\n    output_file = os.environ[\"OUTPUT_FILE\"]\n    info(f\"Writing output to {output_file}\")\n\n    _write_output(output, output_file)\n\n\ndef _run_algorithm_method(input_data: dict, module: str,\n                          log_traceback: bool = True) -> Any:\n    \"\"\"\n    Load the algorithm module and call the correct method to run an algorithm.\n\n    Parameters\n    ----------\n    input_data : dict\n        The input data that is passed to the algorithm. This should at least\n        contain the key 'method' which is the name of the method that should be\n        called. Other keys depend on the algorithm.\n    module : str\n        The module that contains the algorithm.\n    log_traceback: bool, optional\n        Whether to print the full error message from algorithms or not, by\n        default False. Algorithm developers should set this to False if\n        the error messages may contain sensitive information. By default True.\n\n    Returns\n    -------\n    Any\n        The result of the algorithm.\n    \"\"\"\n    # import algorithm module\n    try:\n        lib = importlib.import_module(module)\n        info(f\"Module '{module}' imported!\")\n    except ModuleNotFoundError:\n        error(f\"Module '{module}' can not be imported! Exiting...\")\n        exit(1)\n\n    # get algorithm method and attempt to load it\n    method_name = input_data[\"method\"]\n    try:\n        method = getattr(lib, method_name)\n    except AttributeError:\n        error(f\"Method '{method_name}' not found!\\n\")\n        exit(1)\n\n    # get the args and kwargs input for this function.\n    args = input_data.get(\"args\", [])\n    kwargs = input_data.get(\"kwargs\", {})\n\n    # try to run the method\n    try:\n        result = method(*args, **kwargs)\n    except Exception as e:\n        error(f\"Error encountered while calling {method_name}: {e}\")\n        if log_traceback:\n            error(traceback.print_exc())\n        exit(1)\n\n    return result\n\n\ndef load_input(input_file: str) -> Any:\n    \"\"\"\n    Load the input from the input file.\n\n    Parameters\n    ----------\n    input_file : str\n        File containing the input\n\n    Returns\n    -------\n    input_data : Any\n        Input data for the algorithm\n\n    Raises\n    ------\n    DeserializationException\n        Failed to deserialize input data\n    \"\"\"\n    with open(input_file, \"rb\") as fp:\n        try:\n            input_data = deserialization.deserialize(fp)\n        except DeserializationException:\n            raise DeserializationException('Could not deserialize input')\n    return input_data\n\n\ndef _write_output(output: Any, output_file: str) -> None:\n    \"\"\"\n    Write output to output file using JSON serialization.\n\n    Parameters\n    ----------\n    output : Any\n        Output of the algorithm\n    output_file : str\n        Path to the output file\n    \"\"\"\n    with open(output_file, 'wb') as fp:\n        serialized = serialization.serialize(output)\n        fp.write(serialized)\n", "from pathlib import Path\n\n#\n#   PACKAGE GLOBALS\n#\nSTRING_ENCODING = \"utf-8\"\n\nAPPNAME = \"vantage6\"\n\nMAIN_VERSION_NAME = \"cotopaxi\"\n\nDEFAULT_DOCKER_REGISTRY = \"harbor2.vantage6.ai\"\n\nDEFAULT_NODE_IMAGE = f\"infrastructure/node:{MAIN_VERSION_NAME}\"\n\nDEFAULT_NODE_IMAGE_WO_TAG = \"infrastructure/node\"\n\nDEFAULT_SERVER_IMAGE = f\"infrastructure/server:{MAIN_VERSION_NAME}\"\n\nDEFAULT_UI_IMAGE = f\"infrastructure/ui:{MAIN_VERSION_NAME}\"\n\n#\n#   COMMON GLOBALS\n#\nPACKAGE_FOLDER = Path(__file__).parent.parent.parent\n\nVPN_CONFIG_FILE = 'vpn-config.ovpn.conf'\n\nDATABASE_TYPES = [\"csv\", \"parquet\", \"sql\", \"sparql\", \"omop\", \"excel\", \"other\"]\n\nPING_INTERVAL_SECONDS = 60\n\n# start trying to refresh the JWT token of the node 10 minutes before it\n# expires.\nNODE_CLIENT_REFRESH_BEFORE_EXPIRES_SECONDS = 600\n\n# The basics image can be used (mainly by the UI) to collect column names\nBASIC_PROCESSING_IMAGE = 'harbor2.vantage6.ai/algorithms/basics'\n", "import json\n\n\n# TODO BvB 2023-02-03: I feel this function could be given a better name. And\n# it might not have to be in a separate file.\ndef serialize(data: any) -> bytes:\n    \"\"\"\n    Serialize data using the specified format\n\n    Parameters\n    ----------\n    data: any\n        The data to be serialized\n\n    Returns\n    -------\n    bytes\n        A JSON-serialized and then encoded bytes object representing the data\n    \"\"\"\n    return json.dumps(data).encode()\n", "\"\"\"\nDocker manager\n\nThe docker manager is responsible for communicating with the docker-daemon and\nis a wrapper around the docker module. It has methods\nfor creating docker networks, docker volumes, start containers and retrieve\nresults from finished containers.\n\"\"\"\nimport os\nimport time\nimport logging\nimport docker\nimport re\nimport shutil\n\nfrom typing import NamedTuple\nfrom pathlib import Path\n\nfrom vantage6.common import logger_name\nfrom vantage6.common import get_database_config\nfrom vantage6.common.docker.addons import get_container, running_in_docker\nfrom vantage6.common.globals import APPNAME, BASIC_PROCESSING_IMAGE\nfrom vantage6.common.task_status import TaskStatus, has_task_failed\nfrom vantage6.common.docker.network_manager import NetworkManager\nfrom vantage6.algorithm.tools.wrappers import get_column_names\nfrom vantage6.cli.context import NodeContext\nfrom vantage6.node.context import DockerNodeContext\nfrom vantage6.node.docker.docker_base import DockerBaseManager\nfrom vantage6.node.docker.vpn_manager import VPNManager\nfrom vantage6.node.docker.task_manager import DockerTaskManager\nfrom vantage6.node.docker.squid import Squid\nfrom vantage6.common.client.node_client import NodeClient\nfrom vantage6.node.docker.exceptions import (\n    UnknownAlgorithmStartFail,\n    PermanentAlgorithmStartFail,\n    AlgorithmContainerNotFound\n)\n\nlog = logging.getLogger(logger_name(__name__))\n\n\nclass Result(NamedTuple):\n    \"\"\"\n    Data class to store the result of the docker image.\n\n    Attributes\n    ----------\n    run_id: int\n        ID of the current algorithm run\n    logs: str\n        Logs attached to current algorithm run\n    data: str\n        Output data of the algorithm\n    status_code: int\n        Status code of the algorithm run\n    \"\"\"\n    run_id: int\n    task_id: int\n    logs: str\n    data: str\n    status: str\n    parent_id: int | None\n\n\nclass ToBeKilled(NamedTuple):\n    \"\"\" Data class to store which tasks should be killed \"\"\"\n    task_id: int\n    run_id: int\n    organization_id: int\n\n\nclass KilledRun(NamedTuple):\n    \"\"\" Data class to store which algorithms have been killed \"\"\"\n    run_id: int\n    task_id: int\n    parent_id: int\n\n\nclass DockerManager(DockerBaseManager):\n    \"\"\"\n    Wrapper for the docker-py module.\n\n    This class manages tasks related to Docker, such as logging in to\n    docker registries, managing input/output files, logs etc. Results\n    can be retrieved through `get_result()` which returns the first available\n    algorithm result.\n    \"\"\"\n    log = logging.getLogger(logger_name(__name__))\n\n    def __init__(self, ctx: DockerNodeContext | NodeContext,\n                 isolated_network_mgr: NetworkManager,\n                 vpn_manager: VPNManager, tasks_dir: Path, client: NodeClient,\n                 proxy: Squid | None = None) -> None:\n        \"\"\" Initialization of DockerManager creates docker connection and\n            sets some default values.\n\n            Parameters\n            ----------\n            ctx: DockerNodeContext | NodeContext\n                Context object from which some settings are obtained\n            isolated_network_mgr: NetworkManager\n                Manager for the isolated network\n            vpn_manager: VPNManager\n                VPN Manager object\n            tasks_dir: Path\n                Directory in which this task's data are stored\n            client: NodeClient\n                Client object to communicate with the server\n            proxy: Squid | None\n                Squid proxy object\n        \"\"\"\n        self.log.debug(\"Initializing DockerManager\")\n        super().__init__(isolated_network_mgr)\n\n        self.data_volume_name = ctx.docker_volume_name\n        config = ctx.config\n        self.algorithm_env = config.get('algorithm_env', {})\n        self.vpn_manager = vpn_manager\n        self.client = client\n        self.__tasks_dir = tasks_dir\n        self.alpine_image = config.get('alpine')\n        self.proxy = proxy\n\n        # keep track of the running containers\n        self.active_tasks: list[DockerTaskManager] = []\n\n        # keep track of the containers that have failed to start\n        self.failed_tasks: list[DockerTaskManager] = []\n\n        # before a task is executed it gets exposed to these policies\n        self._policies = config.get(\"policies\", {})\n\n        # node name is used to identify algorithm containers belonging\n        # to this node. This is required as multiple nodes may run at\n        # a single machine sharing the docker daemon while using a\n        # different server. Using a different server means that there\n        # could be duplicate result_id's running at the node at the same\n        # time.\n        self.node_name = ctx.name\n\n        # name of the container that is running the node\n        self.node_container_name = ctx.docker_container_name\n\n        # login to the registries\n        docker_registries = ctx.config.get(\"docker_registries\", [])\n        self.login_to_registries(docker_registries)\n\n        # set database uri and whether or not it is a file\n        self._set_database(ctx.databases)\n\n        # keep track of linked docker services\n        self.linked_services: list[str] = []\n\n        # set algorithm device requests\n        self.algorithm_device_requests = []\n        if 'algorithm_device_requests' in config:\n            self._set_algorithm_device_requests(\n                config['algorithm_device_requests']\n            )\n\n    def _set_database(self, databases: dict | list) -> None:\n        \"\"\"\n        Set database location and whether or not it is a file\n\n        Parameters\n        ----------\n        databases: dict | list\n            databases as specified in the config file\n        \"\"\"\n        db_labels = [db['label'] for db in databases]\n\n        # If we're running in a docker container, database_uri would point\n        # to a path on the *host* (since it's been read from the config\n        # file). That's no good here. Therefore, we expect the CLI to set\n        # the environment variables for us. This has the added bonus that we\n        # can override the URI from the command line as well.\n        self.databases = {}\n        for label in db_labels:\n            label_upper = label.upper()\n            db_config = get_database_config(databases, label)\n            if running_in_docker():\n                uri = os.environ[f'{label_upper}_DATABASE_URI']\n            else:\n                uri = db_config['uri']\n\n            if running_in_docker():\n                db_is_file = Path(f'/mnt/{uri}').exists()\n                if db_is_file:\n                    uri = f'/mnt/{uri}'\n            else:\n                db_is_file = Path(uri).exists()\n\n            if db_is_file:\n                # We'll copy the file to the folder `data` in our task_dir.\n                self.log.info(f'Copying {uri} to {self.__tasks_dir}')\n                shutil.copy(uri, self.__tasks_dir)\n                uri = self.__tasks_dir / os.path.basename(uri)\n\n            self.databases[label] = {'uri': uri, 'is_file': db_is_file,\n                                     'type': db_config['type'],\n                                     'env': db_config.get('env', {})}\n        self.log.debug(f\"Databases: {self.databases}\")\n\n    def _set_algorithm_device_requests(self, device_requests_config: dict) \\\n            -> None:\n        \"\"\"\n        Configure device access for the algorithm container.\n\n        Parameters\n        ----------\n        device_requests_config: dict\n           A dictionary containing configuration options for device access.\n           Supported keys:\n           - 'gpu': A boolean value indicating whether GPU access is required.\n        \"\"\"\n        device_requests = []\n        if device_requests_config.get('gpu', False):\n            device = docker.types.DeviceRequest(count=-1,\n                                                capabilities=[['gpu']])\n            device_requests.append(device)\n        self.algorithm_device_requests = device_requests\n\n    def create_volume(self, volume_name: str) -> None:\n        \"\"\"\n        Create a temporary volume for a single run.\n\n        A single run can consist of multiple algorithm containers. It is\n        important to note that all algorithm containers having the same job_id\n        have access to this container.\n\n        Parameters\n        ----------\n        volume_name: str\n            Name of the volume to be created\n        \"\"\"\n        try:\n            self.docker.volumes.get(volume_name)\n            self.log.debug(f\"Volume {volume_name} already exists.\")\n\n        except docker.errors.NotFound:\n            self.log.debug(f\"Creating volume {volume_name}\")\n            self.docker.volumes.create(volume_name)\n\n    def is_docker_image_allowed(\n        self, docker_image_name: str, task_info: dict\n    ) -> bool:\n        \"\"\"\n        Checks the docker image name.\n\n        Against a list of regular expressions as defined in the configuration\n        file. If no expressions are defined, all docker images are accepted.\n\n        Parameters\n        ----------\n        docker_image_name: str\n            uri to the docker image\n        task_info: dict\n            Dictionary with information about the task\n\n        Returns\n        -------\n        bool\n            Whether docker image is allowed or not\n        \"\"\"\n        # check if algorithm matches any of the regex cases\n        allow_basics = self._policies.get('allow_basics_algorithm', True)\n        allowed_algorithms = self._policies.get('allowed_algorithms')\n        if docker_image_name.startswith(BASIC_PROCESSING_IMAGE):\n            if not allow_basics:\n                self.log.warn(\"A task was sent with a basics algorithm that \"\n                              \"this node does not allow to run.\")\n                return False\n            # else: basics are allowed, so we don't need to check the regex\n        elif allowed_algorithms:\n            if isinstance(allowed_algorithms, str):\n                allowed_algorithms = [allowed_algorithms]\n            found = False\n            for regex_expr in allowed_algorithms:\n                expr_ = re.compile(regex_expr)\n                if expr_.match(docker_image_name):\n                    found = True\n\n            if not found:\n                self.log.warn(\"A task was sent with a docker image that this\"\n                              \" node does not allow to run.\")\n                return False\n\n        # check if user or their organization is allowed\n        allowed_users = self._policies.get('allowed_users', [])\n        allowed_orgs = self._policies.get('allowed_organizations', [])\n        if allowed_users or allowed_orgs:\n            is_allowed = self.client.check_user_allowed_to_send_task(\n                allowed_users, allowed_orgs, task_info['init_org']['id'],\n                task_info['init_user']['id']\n            )\n            if not is_allowed:\n                self.log.warn(\"A task was sent by a user or organization that \"\n                              \"this node does not allow to start tasks.\")\n                return False\n\n        # if no limits are declared, log warning\n        if not self._policies:\n            self.log.warn(\"All docker images are allowed on this Node!\")\n\n        return True\n\n    def is_running(self, run_id: int) -> bool:\n        \"\"\"\n        Check if a container is already running for <run_id>.\n\n        Parameters\n        ----------\n        run_id: int\n            run_id of the algorithm container to be found\n\n        Returns\n        -------\n        bool\n            Whether or not algorithm container is running already\n        \"\"\"\n        running_containers = self.docker.containers.list(filters={\n            \"label\": [\n                f\"{APPNAME}-type=algorithm\",\n                f\"node={self.node_name}\",\n                f\"run_id={run_id}\"\n            ]\n        })\n        return bool(running_containers)\n\n    def cleanup_tasks(self) -> list[KilledRun]:\n        \"\"\"\n        Stop all active tasks\n\n        Returns\n        -------\n        list[KilledRun]:\n            List of information on tasks that have been killed\n        \"\"\"\n        run_ids_killed = []\n        if self.active_tasks:\n            self.log.debug(f'Killing {len(self.active_tasks)} active task(s)')\n        while self.active_tasks:\n            task = self.active_tasks.pop()\n            task.cleanup()\n            run_ids_killed.append(KilledRun(\n                run_id=task.run_id,\n                task_id=task.task_id,\n                parent_id=task.parent_id\n            ))\n        return run_ids_killed\n\n    def cleanup(self) -> None:\n        \"\"\"\n        Stop all active tasks and delete the isolated network\n\n        Note: the temporary docker volumes are kept as they may still be used\n        by a parent container\n        \"\"\"\n        # note: the function `cleanup_tasks` returns a list of tasks that were\n        # killed, but we don't register them as killed so they will be run\n        # again when the node is restarted\n        self.cleanup_tasks()\n        for service in self.linked_services:\n            self.isolated_network_mgr.disconnect(service)\n\n        # remove the node container from the network, it runs this code.. so\n        # it does not make sense to delete it just yet\n        self.isolated_network_mgr.disconnect(self.node_container_name)\n\n        # remove the connected containers and the network\n        self.isolated_network_mgr.delete(kill_containers=True)\n\n    def run(self, run_id: int, task_info: dict, image: str,\n            docker_input: bytes, tmp_vol_name: str, token: str,\n            databases_to_use: list[str]\n            ) -> tuple[TaskStatus, list[dict] | None]:\n        \"\"\"\n        Checks if docker task is running. If not, creates DockerTaskManager to\n        run the task\n\n        Parameters\n        ----------\n        run_id: int\n            Server run identifier\n        task_info: dict\n            Dictionary with task information\n        image: str\n            Docker image name\n        docker_input: bytes\n            Input that can be read by docker container\n        tmp_vol_name: str\n            Name of temporary docker volume assigned to the algorithm\n        token: str\n            Bearer token that the container can use\n        databases_to_use: list[str]\n            Labels of the databases to use\n\n        Returns\n        -------\n        TaskStatus, list[dict] | None\n            Returns a tuple with the status of the task and a description of\n            each port on the VPN client that forwards traffic to the algorithm\n            container (``None`` if VPN is not set up).\n        \"\"\"\n        # Verify that an allowed image is used\n        if not self.is_docker_image_allowed(image, task_info):\n            msg = f\"Docker image {image} is not allowed on this Node!\"\n            self.log.critical(msg)\n            return TaskStatus.NOT_ALLOWED,  None\n\n        # Check that this task is not already running\n        if self.is_running(run_id):\n            self.log.warn(\"Task is already being executed, discarding task\")\n            self.log.debug(f\"run_id={run_id} is discarded\")\n            return TaskStatus.ACTIVE, None\n\n        task = DockerTaskManager(\n            image=image,\n            run_id=run_id,\n            task_info=task_info,\n            vpn_manager=self.vpn_manager,\n            node_name=self.node_name,\n            tasks_dir=self.__tasks_dir,\n            isolated_network_mgr=self.isolated_network_mgr,\n            databases=self.databases,\n            docker_volume_name=self.data_volume_name,\n            alpine_image=self.alpine_image,\n            proxy=self.proxy,\n            device_requests=self.algorithm_device_requests\n        )\n\n        # attempt to kick of the task. If it fails do to unknown reasons we try\n        # again. If it fails permanently we add it to the failed tasks to be\n        # handled by the speaking worker of the node\n        attempts = 1\n        while not (task.status == TaskStatus.ACTIVE) and attempts < 3:\n            try:\n                vpn_ports = task.run(\n                    docker_input=docker_input, tmp_vol_name=tmp_vol_name,\n                    token=token, algorithm_env=self.algorithm_env,\n                    databases_to_use=databases_to_use\n                )\n\n            except UnknownAlgorithmStartFail:\n                self.log.exception(f'Failed to start run {run_id} for an '\n                                   'unknown reason. Retrying...')\n                time.sleep(1)  # add some time before retrying the next attempt\n\n            except PermanentAlgorithmStartFail:\n                break\n\n            attempts += 1\n\n        # keep track of the active container\n        if has_task_failed(task.status):\n            self.failed_tasks.append(task)\n            return task.status, None\n        else:\n            self.active_tasks.append(task)\n            return task.status, vpn_ports\n\n    def get_result(self) -> Result:\n        \"\"\"\n        Returns the oldest (FIFO) finished docker container.\n\n        This is a blocking method until a finished container shows up. Once the\n        container is obtained and the results are read, the container is\n        removed from the docker environment.\n\n        Returns\n        -------\n        Result\n            result of the docker image\n        \"\"\"\n\n        # get finished results and get the first one, if no result is available\n        # this is blocking\n        finished_tasks = []\n        while (not finished_tasks) and (not self.failed_tasks):\n            for task in self.active_tasks:\n\n                try:\n                    if task.is_finished():\n                        finished_tasks.append(task)\n                        self.active_tasks.remove(task)\n                        break\n                except AlgorithmContainerNotFound:\n                    self.log.exception(f'Failed to find container for '\n                                       f'result {task.result_id}')\n                    self.failed_tasks.append(task)\n                    self.active_tasks.remove(task)\n                    break\n\n            # sleep for a second before checking again\n            time.sleep(1)\n\n        if finished_tasks:\n            # at least one task is finished\n\n            finished_task = finished_tasks.pop()\n            self.log.debug(f\"Run id={finished_task.run_id} is finished\")\n\n            # Check exit status and report\n            logs = finished_task.report_status()\n\n            # Cleanup containers\n            finished_task.cleanup()\n\n            # Retrieve results from file\n            results = finished_task.get_results()\n\n            # remove the VPN ports of this run from the database\n            self.client.request(\n                'port', params={'run_id': finished_task.run_id},\n                method=\"DELETE\"\n            )\n        else:\n            # at least one task failed to start\n            finished_task = self.failed_tasks.pop()\n            logs = 'Container failed'\n            results = b''\n\n        return Result(\n            run_id=finished_task.run_id,\n            task_id=finished_task.task_id,\n            logs=logs,\n            data=results,\n            status=finished_task.status,\n            parent_id=finished_task.parent_id,\n        )\n\n    def login_to_registries(self, registries: list = []) -> None:\n        \"\"\"\n        Login to the docker registries\n\n        Parameters\n        ----------\n        registries: list\n            list of registries to login to\n        \"\"\"\n        for registry in registries:\n            try:\n                self.docker.login(\n                    username=registry.get(\"username\"),\n                    password=registry.get(\"password\"),\n                    registry=registry.get(\"registry\")\n                )\n                self.log.info(f\"Logged in to {registry.get('registry')}\")\n            except docker.errors.APIError as e:\n                self.log.warn(f\"Could not login to {registry.get('registry')}\")\n                self.log.debug(e)\n\n    def link_container_to_network(self, container_name: str,\n                                  config_alias: str) -> None:\n        \"\"\"\n        Link a docker container to the isolated docker network\n\n        Parameters\n        ----------\n        container_name: str\n            Name of the docker container to be linked to the network\n        config_alias: str\n            Alias of the docker container defined in the config file\n        \"\"\"\n        container = get_container(\n            docker_client=self.docker, name=container_name\n        )\n        if not container:\n            self.log.error(f\"Could not link docker container {container_name} \"\n                           \"that was specified in the configuration file to \"\n                           \"the isolated docker network.\")\n            self.log.error(\"Container not found!\")\n            return\n        self.isolated_network_mgr.connect(\n            container_name=container_name,\n            aliases=[config_alias]\n        )\n        self.linked_services.append(container_name)\n\n    def kill_selected_tasks(\n        self, org_id: int, kill_list: list[ToBeKilled] = None\n    ) -> list[KilledRun]:\n        \"\"\"\n        Kill tasks specified by a kill list, if they are currently running on\n        this node\n\n        Parameters\n        ----------\n        org_id: int\n            The organization id of this node\n        kill_list: list[ToBeKilled]\n            A list of info about tasks that should be killed.\n\n        Returns\n        -------\n        list[KilledRun]\n            List with information on killed tasks\n        \"\"\"\n        killed_list = []\n        for container_to_kill in kill_list:\n            if container_to_kill['organization_id'] != org_id:\n                continue  # this run is on another node\n            # find the task\n            task = next((\n                t for t in self.active_tasks\n                if t.run_id == container_to_kill['run_id']\n            ), None)\n            if task:\n                self.log.info(\n                    f\"Killing containers for run_id={task.run_id}\")\n                self.active_tasks.remove(task)\n                task.cleanup()\n                killed_list.append(KilledRun(\n                    run_id=task.run_id,\n                    task_id=task.task_id,\n                    parent_id=task.parent_id,\n                ))\n            else:\n                self.log.warn(\n                    \"Received instruction to kill run_id=\"\n                    f\"{container_to_kill['run_id']}, but it was not \"\n                    \"found running on this node.\")\n        return killed_list\n\n    def kill_tasks(self, org_id: int,\n                   kill_list: list[ToBeKilled] = None) -> list[KilledRun]:\n        \"\"\"\n        Kill tasks currently running on this node.\n\n        Parameters\n        ----------\n        org_id: int\n            The organization id of this node\n        kill_list: list[ToBeKilled] (optional)\n            A list of info on tasks that should be killed. If the list\n            is not specified, all running algorithm containers will be killed.\n\n        Returns\n        -------\n        list[KilledRun]\n            List of dictionaries with information on killed tasks\n        \"\"\"\n        if kill_list:\n            killed_runs = self.kill_selected_tasks(org_id=org_id,\n                                                   kill_list=kill_list)\n        else:\n            # received instruction to kill all tasks on this node\n            self.log.warn(\n                \"Received instruction from server to kill all algorithms \"\n                \"running on this node. Executing that now...\")\n            killed_runs = self.cleanup_tasks()\n            if len(killed_runs):\n                self.log.warn(\n                    \"Killed the following run ids as instructed via socket:\"\n                    f\" {', '.join([str(r.run_id) for r in killed_runs])}\"\n                )\n            else:\n                self.log.warn(\n                    \"Instructed to kill tasks but none were running\"\n                )\n        return killed_runs\n\n    def get_column_names(self, label: str, type_: str) -> list[str]:\n        \"\"\"\n        Get column names from a node database\n\n        Parameters\n        ----------\n        label: str\n            Label of the database\n        type_: str\n            Type of the database\n\n        Returns\n        -------\n        list[str]\n            List of column names\n        \"\"\"\n        db = self.databases.get(label)\n        if not db:\n            self.log.error(\"Database with label %s not found\", label)\n            return []\n        if not db['is_file']:\n            self.log.error(\"Database with label %s is not a file. Cannot\"\n                           \" determine columns without query\", label)\n            return []\n        if db['type'] == 'excel':\n            self.log.error(\"Cannot determine columns for excel database \"\n                           \" without a worksheet\")\n            return []\n        if type_ not in ('csv', 'sparql'):\n            self.log.error(\"Cannot determine columns for database of type %s.\"\n                           \"Only csv and sparql are supported\", type_)\n            return []\n        return get_column_names(db['uri'], type_)\n", "# TODO the task folder is also created by this class. This folder needs\n# to be cleaned at some point.\nimport logging\nimport os\nimport docker.errors\nimport json\n\nfrom pathlib import Path\n\nfrom vantage6.common.globals import APPNAME\nfrom vantage6.common.docker.addons import (\n    remove_container_if_exists, remove_container, pull_if_newer,\n    running_in_docker\n)\nfrom vantage6.common.docker.network_manager import NetworkManager\nfrom vantage6.common.task_status import TaskStatus\nfrom vantage6.node.util import get_parent_id\nfrom vantage6.node.globals import ALPINE_IMAGE\nfrom vantage6.node.docker.vpn_manager import VPNManager\nfrom vantage6.node.docker.squid import Squid\nfrom vantage6.node.docker.docker_base import DockerBaseManager\nfrom vantage6.node.docker.exceptions import (\n    UnknownAlgorithmStartFail,\n    PermanentAlgorithmStartFail,\n    AlgorithmContainerNotFound\n)\n\n\nclass DockerTaskManager(DockerBaseManager):\n    \"\"\"\n    Manager for running a vantage6 algorithm container within docker.\n\n    Ensures that the environment is properly set up (docker volumes,\n    directories, environment variables, etc). Then runs the algorithm as a\n    docker container. Finally, it monitors the container state and can return\n    it's results when the algorithm finished.\n    \"\"\"\n\n    def __init__(self, image: str, vpn_manager: VPNManager, node_name: str,\n                 run_id: int, task_info: dict, tasks_dir: Path,\n                 isolated_network_mgr: NetworkManager,\n                 databases: dict, docker_volume_name: str,\n                 alpine_image: str | None = None, proxy: Squid | None = None,\n                 device_requests: list | None = None):\n        \"\"\"\n        Initialization creates DockerTaskManager instance\n\n        Parameters\n        ----------\n        image: str\n            Name of docker image to be run\n        vpn_manager: VPNManager\n            VPN manager required to set up traffic forwarding via VPN\n        node_name: str\n            Name of the node, to track running algorithms\n        run_id: int\n            Algorithm run identifier\n        task_info: dict\n            Dictionary with info about the task\n        tasks_dir: Path\n            Directory in which this task's data are stored\n        isolated_network_mgr: NetworkManager\n            Manager of isolated network to which algorithm needs to connect\n        databases: dict\n            List of databases\n        docker_volume_name: str\n            Name of the docker volume\n        alpine_image: str | None\n            Name of alternative Alpine image to be used\n        device_requests: list | None\n            List of DeviceRequest objects to be passed to the algorithm\n            container\n        \"\"\"\n        self.task_id = task_info['id']\n        self.log = logging.getLogger(f\"task ({self.task_id})\")\n\n        super().__init__(isolated_network_mgr)\n        self.image = image\n        self.__vpn_manager = vpn_manager\n        self.run_id = run_id\n        self.task_id = task_info['id']\n        self.parent_id = get_parent_id(task_info)\n        self.__tasks_dir = tasks_dir\n        self.databases = databases\n        self.data_volume_name = docker_volume_name\n        self.node_name = node_name\n        self.alpine_image = ALPINE_IMAGE if alpine_image is None \\\n            else alpine_image\n        self.proxy = proxy\n\n        self.container = None\n        self.status_code = None\n        self.docker_input = None\n\n        self.labels = {\n            f\"{APPNAME}-type\": \"algorithm\",\n            \"node\": node_name,\n            \"run_id\": str(run_id)\n        }\n        self.helper_labels = self.labels.copy()\n        self.helper_labels[f\"{APPNAME}-type\"] = \"algorithm-helper\"\n\n        # FIXME: these values should be retrieved from DockerNodeContext\n        #   in some way.\n        self.tmp_folder = \"/mnt/tmp\"\n        self.data_folder = \"/mnt/data\"\n\n        # keep track of the task status\n        self.status: TaskStatus = TaskStatus.INITIALIZING\n\n        # set device requests\n        self.device_requests = []\n        if device_requests:\n            self.device_requests = device_requests\n\n    def is_finished(self) -> bool:\n        \"\"\"\n        Checks if algorithm container is finished\n\n        Returns\n        -------\n        bool:\n            True if algorithm container is finished\n        \"\"\"\n        try:\n            self.container.reload()\n        except docker.errors.NotFound:\n            self.log.error(\"Container not found\")\n            self.log.debug(f\"- task id: {self.task_id}\")\n            self.log.debug(f\"- result id: {self.task_id}\")\n            self.status = TaskStatus.UNKNOWN_ERROR\n            raise AlgorithmContainerNotFound\n\n        return self.container.status == 'exited'\n\n    def report_status(self) -> str:\n        \"\"\"\n        Checks if algorithm has exited successfully. If not, it prints an\n        error message\n\n        Returns\n        -------\n        logs: str\n            Log messages of the algorithm container\n        \"\"\"\n        logs = self.container.logs().decode('utf8')\n\n        # report if the container has a different status than 0\n        self.status_code = self.container.attrs[\"State\"][\"ExitCode\"]\n        if self.status_code:\n            self.log.error(f\"Received non-zero exitcode: {self.status_code}\")\n            self.log.error(f\"  Container id: {self.container.id}\")\n            self.log.info(logs)\n            self.status = TaskStatus.CRASHED\n        else:\n            self.status = TaskStatus.COMPLETED\n        return logs\n\n    def get_results(self) -> bytes:\n        \"\"\"\n        Read results output file of the algorithm container\n\n        Returns\n        -------\n        bytes:\n            Results of the algorithm container\n        \"\"\"\n        with open(self.output_file, \"rb\") as fp:\n            results = fp.read()\n        return results\n\n    def pull(self):\n        \"\"\" Pull the latest docker image. \"\"\"\n        try:\n            self.log.info(f\"Retrieving latest image: '{self.image}'\")\n            pull_if_newer(self.docker, self.image, self.log)\n\n        except docker.errors.APIError as e:\n            self.log.debug('Failed to pull image: could not find image')\n            self.log.exception(e)\n            self.status = TaskStatus.NO_DOCKER_IMAGE\n            raise PermanentAlgorithmStartFail\n        except Exception as e:\n            self.log.debug('Failed to pull image')\n            self.log.exception(e)\n            self.status = TaskStatus.FAILED\n            raise PermanentAlgorithmStartFail\n\n    def run(\n        self, docker_input: bytes, tmp_vol_name: str, token: str,\n        algorithm_env: dict, databases_to_use: list[str]\n    ) -> list[dict] | None:\n        \"\"\"\n        Runs the docker-image in detached mode.\n\n        It will will attach all mounts (input, output and datafile) to the\n        docker image. And will supply some environment variables.\n\n        Parameters\n        ----------\n        docker_input: bytes\n            Input that can be read by docker container\n        tmp_vol_name: str\n            Name of temporary docker volume assigned to the algorithm\n        token: str\n            Bearer token that the container can use\n        algorithm_env: dict\n            Dictionary with additional environment variables to set\n        databases_to_use: list[str]\n            List of labels of databases to use in the task\n\n        Returns\n        -------\n        list[dict] | None\n            Description of each port on the VPN client that forwards traffic to\n            the algo container. None if VPN is not set up.\n        \"\"\"\n        # generate task folders\n        self._make_task_folders()\n\n        # prepare volumes\n        self.docker_input = docker_input\n        self.volumes = self._prepare_volumes(tmp_vol_name, token)\n        self.log.debug(f\"volumes: {self.volumes}\")\n\n        # setup environment variables\n        self.environment_variables = \\\n            self._setup_environment_vars(algorithm_env=algorithm_env,\n                                         databases_to_use=databases_to_use)\n\n        # run the algorithm as docker container\n        vpn_ports = self._run_algorithm()\n        return vpn_ports\n\n    def cleanup(self) -> None:\n        \"\"\"Cleanup the containers generated for this task\"\"\"\n        remove_container(self.helper_container, kill=True)\n        remove_container(self.container, kill=True)\n\n    def _run_algorithm(self) -> list[dict] | None:\n        \"\"\"\n        Run the algorithm container\n\n        Start up a helper container to complete VPN setup, pull the latest\n        image and then run the algorithm\n\n        Returns\n        -------\n        list[dict] or None\n            Description of each port on the VPN client that forwards traffic to\n            the algo container. None if VPN is inactive\n        \"\"\"\n        vpn_ports = None\n        container_name = f'{APPNAME}-{self.node_name}-run-{self.run_id}'\n        helper_container_name = container_name + '-helper'\n\n        # Try to pull the latest image\n        self.pull()\n\n        # remove algorithm containers if they were already running\n        self.log.debug(\"Check if algorithm container is already running\")\n        remove_container_if_exists(\n            docker_client=self.docker, name=container_name\n        )\n        remove_container_if_exists(\n            docker_client=self.docker, name=helper_container_name\n        )\n\n        if self.__vpn_manager:\n            # if VPN is active, network exceptions must be configured\n            # First, start a container that runs indefinitely. The algorithm\n            # container will run in the same network and network exceptions\n            # will therefore also affect the algorithm.\n            self.log.debug(\"Start helper container to setup VPN network\")\n            self.helper_container = self.docker.containers.run(\n                command='sleep infinity',\n                image=self.alpine_image,\n                labels=self.helper_labels,\n                network=self.isolated_network_mgr.network_name,\n                name=helper_container_name,\n                detach=True\n            )\n            # setup forwarding of traffic via VPN client to and from the\n            # algorithm container:\n            self.log.debug(\"Setup port forwarder\")\n            vpn_ports = self.__vpn_manager.forward_vpn_traffic(\n                helper_container=self.helper_container,\n                algo_image_name=self.image\n            )\n\n        # try reading docker input\n        # FIXME BvB 2023-02-03: why do we read docker input here? It is never\n        # really used below. Should it?\n        deserialized_input = None\n        if self.docker_input:\n            self.log.debug(\"Deserialize input\")\n            try:\n                deserialized_input = json.loads(self.docker_input)\n            except Exception:\n                pass\n\n        # attempt to run the image\n        try:\n            if deserialized_input:\n                self.log.info(f\"Run docker image {self.image} with input \"\n                              f\"{self._printable_input(deserialized_input)}\")\n            else:\n                self.log.info(f\"Run docker image {self.image}\")\n            self.container = self.docker.containers.run(\n                self.image,\n                detach=True,\n                environment=self.environment_variables,\n                network='container:' + self.helper_container.id,\n                volumes=self.volumes,\n                name=container_name,\n                labels=self.labels,\n                device_requests=self.device_requests\n            )\n\n        except Exception as e:\n            self.status = TaskStatus.START_FAILED\n            raise UnknownAlgorithmStartFail(e)\n\n        self.status = TaskStatus.ACTIVE\n        return vpn_ports\n\n    @staticmethod\n    def _printable_input(input_: str | dict) -> str:\n        \"\"\"\n        Return a version of the input with limited number of characters\n\n        Parameters\n        ----------\n        input: str | dict\n            Deserialized input of a task\n\n        Returns\n        -------\n        str\n            Input with limited number of characters, to be printed to logs\n        \"\"\"\n        if isinstance(input_, dict):\n            input_ = str(input_)\n        if len(input_) > 550:\n            return f'{input_[:500]}... ({len(input_)-500} characters omitted)'\n        return input_\n\n    def _make_task_folders(self) -> None:\n        \"\"\" Generate task folders \"\"\"\n        # FIXME: We should have a separate mount/volume for this. At the\n        #   moment this is a potential leak as containers might access input,\n        #   output and token from other containers.\n        #\n        #   This was not possible yet as mounting volumes from containers\n        #   is terrible when working from windows (as you have to convert\n        #   from windows to unix several times...).\n\n        # If we're running in docker __tasks_dir will point to a location on\n        # the data volume.\n        # Alternatively, if we're not running in docker it should point to the\n        # folder on the host that can act like a data volume. In both cases,\n        # we can just copy the required files to it\n        self.task_folder_name = f\"task-{self.run_id:09d}\"\n        self.task_folder_path = \\\n            os.path.join(self.__tasks_dir, self.task_folder_name)\n        os.makedirs(self.task_folder_path, exist_ok=True)\n        self.output_file = os.path.join(self.task_folder_path, \"output\")\n\n    def _prepare_volumes(self, tmp_vol_name: str, token: str) -> dict:\n        \"\"\"\n        Generate docker volumes required to run the algorithm\n\n        Parameters\n        ----------\n        tmp_vol_name: str\n            Name of temporary docker volume assigned to the algorithm\n        token: str\n            Bearer token that the container can use\n\n        Returns\n        -------\n        dict:\n            Volumes to support running the algorithm\n        \"\"\"\n        if isinstance(self.docker_input, str):\n            self.docker_input = self.docker_input.encode('utf8')\n\n        # Create I/O files & token for the algorithm container\n        self.log.debug(\"prepare IO files in docker volume\")\n        io_files = [\n            ('input', self.docker_input),\n            ('output', b''),\n            ('token', token.encode(\"ascii\")),\n        ]\n\n        for (filename, data) in io_files:\n            filepath = os.path.join(self.task_folder_path, filename)\n\n            with open(filepath, 'wb') as fp:\n                fp.write(data)\n\n        volumes = {\n            tmp_vol_name: {\"bind\": self.tmp_folder, \"mode\": \"rw\"},\n        }\n\n        if running_in_docker():\n            volumes[self.data_volume_name] = \\\n                {\"bind\": self.data_folder, \"mode\": \"rw\"}\n        else:\n            volumes[self.__tasks_dir] = \\\n                {\"bind\": self.data_folder, \"mode\": \"rw\"}\n        return volumes\n\n    def _setup_environment_vars(self, algorithm_env: dict,\n                                databases_to_use: list[str]) -> dict:\n        \"\"\"\"\n        Set environment variables required to run the algorithm\n\n        Parameters\n        ----------\n        algorithm_env: dict\n            Dictionary with additional environment variables to set\n        databases_to_use: list[str]\n            Labels of the databases to use\n\n        Returns\n        -------\n        dict:\n            Environment variables required to run algorithm\n        \"\"\"\n        try:\n            proxy_host = os.environ['PROXY_SERVER_HOST']\n\n        except Exception:\n            self.log.warn(\"PROXY_SERVER_HOST not set, using \"\n                          \"host.docker.internal\")\n            self.log.debug(os.environ)\n            proxy_host = 'host.docker.internal'\n\n        # define environment variables for the docker-container, the\n        # host, port and api_path are from the local proxy server to\n        # facilitate indirect communication with the central server\n        # FIXME: we should only prepend data_folder if database_uri is a\n        #   filename\n        environment_variables = {\n            \"INPUT_FILE\": f\"{self.data_folder}/{self.task_folder_name}/input\",\n            \"OUTPUT_FILE\":\n                f\"{self.data_folder}/{self.task_folder_name}/output\",\n            \"TOKEN_FILE\": f\"{self.data_folder}/{self.task_folder_name}/token\",\n            \"TEMPORARY_FOLDER\": self.tmp_folder,\n            \"HOST\": f\"http://{proxy_host}\",\n            \"PORT\": os.environ.get(\"PROXY_SERVER_PORT\", 8080),\n            \"API_PATH\": \"\",\n        }\n\n        # Add squid proxy environment variables\n        if self.proxy:\n            # applications/libraries in the algorithm container need to adhere\n            # to the proxy settings. Because we are not sure which application\n            # is used for the request we both set HTTP_PROXY and http_proxy and\n            # HTTPS_PROXY and https_proxy for the secure connection.\n            environment_variables[\"HTTP_PROXY\"] = self.proxy.address\n            environment_variables[\"http_proxy\"] = self.proxy.address\n            environment_variables[\"HTTPS_PROXY\"] = self.proxy.address\n            environment_variables[\"https_proxy\"] = self.proxy.address\n\n            no_proxy = []\n            if self.__vpn_manager:\n                # Computing all ips in the vpn network is not feasible as the\n                # no_proxy environment variable will be too long for the\n                # container to start. So we only add the net + mask. For some\n                # applications and libraries this is format is ignored.\n                no_proxy.append(self.__vpn_manager.subnet)\n            no_proxy.append(\"localhost\")\n            no_proxy.append(proxy_host)\n\n            # Add the NO_PROXY and no_proxy environment variable.\n            environment_variables[\"NO_PROXY\"] = ', '.join(no_proxy)\n            environment_variables[\"no_proxy\"] = ', '.join(no_proxy)\n\n        for database in databases_to_use:\n            if database['label'] not in self.databases:\n                # In this case the algorithm might crash if it tries to access\n                # the DATABASE_LABEL environment variable\n                self.log.warning(\"A user specified a database that does not \"\n                                 \"exist. Available databases are: \"\n                                 f\"{self.databases.keys()}. This is likely to \"\n                                 \"result in an algorithm crash.\")\n                self.log.debug(f\"User specified database: {database}\")\n            # define env vars for the preprocessing and extra parameters such\n            # as query and sheet_name\n            extra_params = json.loads(database.get(\"parameters\")) \\\n                if database.get(\"parameters\") else {}\n            for optional_key in ['query', 'sheet_name', 'preprocessing']:\n                if optional_key in extra_params:\n                    env_var_value = extra_params[optional_key] \\\n                        if optional_key != 'preprocessing' \\\n                        else json.dumps(extra_params[optional_key])\n                    environment_variables[f\"{database['label'].upper()}_\"\n                                          f\"{optional_key.upper()}\"] = \\\n                        env_var_value\n\n        environment_variables[\"USER_REQUESTED_DATABASE_LABELS\"] = \\\n            \",\".join([db['label'] for db in databases_to_use])\n\n        # Only prepend the data_folder is it is a file-based database\n        # This allows algorithms to access multiple data sources at the\n        # same time\n        db_labels = []\n        for label in self.databases:\n            db = self.databases[label]\n\n            uri_var_name = f'{label.upper()}_DATABASE_URI'\n            environment_variables[uri_var_name] = \\\n                f\"{self.data_folder}/{os.path.basename(db['uri'])}\" \\\n                if db['is_file'] else db['uri']\n\n            type_var_name = f'{label.upper()}_DATABASE_TYPE'\n            environment_variables[type_var_name] = db['type']\n\n            # Add optional database parameter settings, these can be used by\n            # the algorithm (wrapper). Note that all env keys are prefixed\n            # with DB_PARAM_ to avoid collisions with other environment\n            # variables.\n            if 'env' in db:\n                for key in db['env']:\n                    env_key = f'{label.upper()}_DB_PARAM_{key.upper()}'\n                    environment_variables[env_key] = db['env'][key]\n\n            db_labels.append(label)\n        environment_variables['DB_LABELS'] = ','.join(db_labels)\n\n        self.log.debug(f\"environment: {environment_variables}\")\n\n        # Load additional environment variables\n        if algorithm_env:\n            environment_variables = \\\n                {**environment_variables, **algorithm_env}\n            self.log.info('Custom environment variables are loaded!')\n            self.log.debug(f\"custom environment: {algorithm_env}\")\n        return environment_variables\n", "from pathlib import Path\nfrom vantage6.common.globals import APPNAME\n\n#\n#   NODE SETTINGS\n#\nDEFAULT_NODE_SYSTEM_FOLDERS = False\n\n#\n#   INSTALLATION SETTINGS\n#\nPACKAGE_FOLDER = Path(__file__).parent.parent.parent\n\nNODE_PROXY_SERVER_HOSTNAME = \"proxyserver\"\n\nDATA_FOLDER = PACKAGE_FOLDER / APPNAME / \"_data\"\n\n# with open(Path(PACKAGE_FOLDER) / APPNAME / \"node\" / \"VERSION\") as f:\n#     VERSION = f.read()\n\n\n# constants for retrying node login\nSLEEP_BTWN_NODE_LOGIN_TRIES = 10  # retry every 10s\nTIME_LIMIT_RETRY_CONNECT_NODE = 60 * 60 * 24 * 7  # i.e. 1 week\n\n# constant for waiting for the initial websocket connection\nTIME_LIMIT_INITIAL_CONNECTION_WEBSOCKET = 60\n\n#\n#    VPN CONFIGURATION RELATED CONSTANTS\n#\n# TODO move part of these constants elsewhere?! Or make context?\nVPN_CLIENT_IMAGE = 'harbor2.vantage6.ai/infrastructure/vpn-client'\nNETWORK_CONFIG_IMAGE = 'harbor2.vantage6.ai/infrastructure/vpn-configurator'\nALPINE_IMAGE = 'harbor2.vantage6.ai/infrastructure/alpine'\nMAX_CHECK_VPN_ATTEMPTS = 60   # max attempts to obtain VPN IP (1 second apart)\nFREE_PORT_RANGE = range(49152, 65535)\nDEFAULT_ALGO_VPN_PORT = '8888'  # default VPN port for algorithm container\n\n#\n#   SSH TUNNEL RELATED CONSTANTS\n#\nSSH_TUNNEL_IMAGE = \"harbor2.vantage6.ai/infrastructure/ssh-tunnel\"\n\n#\n#   SQUID RELATED CONSTANTS\n#\nSQUID_IMAGE = \"harbor2.vantage6.ai/infrastructure/squid\"\n", "import sys\nfrom pathlib import Path\nfrom threading import Thread\nimport time\nimport os.path\n\nimport click\nimport questionary as q\nimport docker\n\nfrom colorama import Fore, Style\n\nfrom vantage6.common import (\n    warning, error, info, debug,\n    get_database_config\n)\nfrom vantage6.common.globals import (\n    APPNAME,\n    DEFAULT_DOCKER_REGISTRY,\n    DEFAULT_NODE_IMAGE,\n    DEFAULT_NODE_IMAGE_WO_TAG,\n)\nfrom vantage6.common.docker.addons import (\n  pull_if_newer,\n  remove_container_if_exists,\n  check_docker_running\n)\n\nfrom vantage6.cli.context import NodeContext\nfrom vantage6.cli.globals import (\n    DEFAULT_NODE_SYSTEM_FOLDERS as N_FOL\n)\nfrom vantage6.cli.configuration_wizard import (\n    configuration_wizard,\n    select_configuration_questionaire,\n)\nfrom vantage6.cli.utils import check_config_name_allowed\nfrom vantage6.cli import __version__\nfrom vantage6.cli.node.common import print_log_worker, create_client\n\n\n@click.command()\n@click.option(\"-n\", \"--name\", default=None, help=\"Configuration name\")\n@click.option(\"-c\", \"--config\", default=None,\n              help='Absolute path to configuration-file; overrides NAME')\n@click.option('--system', 'system_folders', flag_value=True,\n              help=\"Search for the configuration in the system folders\")\n@click.option('--user', 'system_folders', flag_value=False, default=N_FOL,\n              help=\"Search for the configuration in the user folders. This is \"\n                   \"the default\")\n@click.option('-i', '--image', default=None, help=\"Node Docker image to use\")\n@click.option('--keep/--auto-remove', default=False,\n              help=\"Keep node container after finishing. Useful for debugging\")\n@click.option('--force-db-mount', is_flag=True,\n              help=\"Always mount node databases; skip the check if they are \"\n                   \"existing files.\")\n@click.option('--attach/--detach', default=False,\n              help=\"Show node logs on the current console after starting the \"\n                   \"node\")\n@click.option('--mount-src', default='',\n              help=\"Override vantage6 source code in container with the source\"\n                   \" code in this path\")\ndef cli_node_start(name: str, config: str, system_folders: bool, image: str,\n                   keep: bool, mount_src: str, attach: bool,\n                   force_db_mount: bool) -> None:\n    \"\"\"\n    Start the node.\n    \"\"\"\n    check_docker_running()\n    info(\"Starting node...\")\n    info(\"Finding Docker daemon\")\n    docker_client = docker.from_env()\n    NodeContext.LOGGING_ENABLED = False\n    if config:\n        name = Path(config).stem\n        ctx = NodeContext(name, system_folders, config)\n\n    else:\n        # in case no name is supplied, ask the user to select one\n        if not name:\n            name = select_configuration_questionaire(\"node\", system_folders)\n\n        # check that config exists, if not a questionaire will be invoked\n        if not NodeContext.config_exists(name, system_folders):\n            warning(f\"Configuration {Fore.RED}{name}{Style.RESET_ALL} does not\"\n                    \" exist.\")\n\n            if q.confirm(\"Create this configuration now?\").ask():\n                configuration_wizard(\"node\", name, system_folders)\n\n            else:\n                error(\"Config file couldn't be loaded\")\n                sys.exit(0)\n\n        ctx = NodeContext(name, system_folders)\n\n    # check if config name is allowed docker name, else exit\n    check_config_name_allowed(ctx.name)\n\n    # check that this node is not already running\n    running_nodes = docker_client.containers.list(\n        filters={\"label\": f\"{APPNAME}-type=node\"}\n    )\n\n    suffix = \"system\" if system_folders else \"user\"\n    for node in running_nodes:\n        if node.name == f\"{APPNAME}-{name}-{suffix}\":\n            error(f\"Node {Fore.RED}{name}{Style.RESET_ALL} is already running\")\n            exit(1)\n\n    # make sure the (host)-task and -log dir exists\n    info(\"Checking that data and log dirs exist\")\n    ctx.data_dir.mkdir(parents=True, exist_ok=True)\n    ctx.log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Determine image-name. First we check if the option --image has been used.\n    # Then we check if the image has been specified in the config file, and\n    # finally we use the default settings from the package.\n    if not image:\n        custom_images: dict = ctx.config.get('images')\n        if custom_images:\n            image = custom_images.get(\"node\")\n        else:\n            # if no custom image is specified, find the server version and use\n            # the latest images from that minor version\n            client = create_client(ctx)\n            major_minor = None\n            try:\n                # try to get server version, skip if can't get a connection\n                version = client.util.get_server_version(\n                    attempts_on_timeout=3\n                )['version']\n                major_minor = '.'.join(version.split('.')[:2])\n                image = (f\"{DEFAULT_DOCKER_REGISTRY}/\"\n                         f\"{DEFAULT_NODE_IMAGE_WO_TAG}\"\n                         f\":{major_minor}\")\n            except Exception:\n                warning(\"Could not determine server version. Using default \"\n                        \"node image\")\n\n            if major_minor and not __version__.startswith(major_minor):\n                warning(\n                    \"Version mismatch between CLI and server/node. CLI is \"\n                    f\"running on version {__version__}, while node and server \"\n                    f\"are on version {major_minor}. This might cause \"\n                    f\"unexpected issues; changing to {major_minor}.<latest> \"\n                    \"is recommended.\"\n                )\n\n        # fail safe, in case no custom image is specified and we can't get the\n        # server version\n        if not image:\n            image = f\"{DEFAULT_DOCKER_REGISTRY}/{DEFAULT_NODE_IMAGE}\"\n\n    info(f\"Pulling latest node image '{image}'\")\n    try:\n        # docker_client.images.pull(image)\n        pull_if_newer(docker.from_env(), image)\n\n    except Exception as e:\n        warning(' ... Getting latest node image failed:')\n        warning(f\"     {e}\")\n    else:\n        info(\" ... success!\")\n\n    info(\"Creating Docker data volume\")\n\n    data_volume = docker_client.volumes.create(ctx.docker_volume_name)\n    vpn_volume = docker_client.volumes.create(ctx.docker_vpn_volume_name)\n    ssh_volume = docker_client.volumes.create(ctx.docker_ssh_volume_name)\n    squid_volume = docker_client.volumes.create(ctx.docker_squid_volume_name)\n\n    info(\"Creating file & folder mounts\")\n    # FIXME: should obtain mount points from DockerNodeContext\n    mounts = [\n        # (target, source)\n        (f\"/mnt/log/{name}\", str(ctx.log_dir)),\n        (\"/mnt/data\", data_volume.name),\n        (\"/mnt/vpn\", vpn_volume.name),\n        (\"/mnt/ssh\", ssh_volume.name),\n        (\"/mnt/squid\", squid_volume.name),\n        (\"/mnt/config\", str(ctx.config_dir)),\n        (\"/var/run/docker.sock\", \"/var/run/docker.sock\"),\n    ]\n\n    if mount_src:\n        # If mount_src is a relative path, docker will consider it a volume.\n        mount_src = os.path.abspath(mount_src)\n        mounts.append(('/vantage6', mount_src))\n\n    # FIXME: Code duplication: Node.__init__() (vantage6/node/__init__.py)\n    #   uses a lot of the same logic. Suggest moving this to\n    #   ctx.get_private_key()\n    filename = ctx.config.get(\"encryption\", {}).get(\"private_key\")\n    # filename may be set to an empty string\n    if not filename:\n        filename = 'private_key.pem'\n\n    # Location may be overridden by the environment\n    filename = os.environ.get('PRIVATE_KEY', filename)\n\n    # If ctx.get_data_file() receives an absolute path, it is returned as-is\n    fullpath = Path(ctx.get_data_file(filename))\n    if fullpath:\n        if Path(fullpath).exists():\n            mounts.append((\"/mnt/private_key.pem\", str(fullpath)))\n        else:\n            warning(f\"private key file provided {fullpath}, \"\n                    \"but does not exists\")\n\n    # Mount private keys for ssh tunnels\n    ssh_tunnels = ctx.config.get(\"ssh-tunnels\", [])\n    for ssh_tunnel in ssh_tunnels:\n        hostname = ssh_tunnel.get(\"hostname\")\n        key_path = ssh_tunnel.get(\"ssh\", {}).get(\"identity\", {}).get(\"key\")\n        if not key_path:\n            error(f\"SSH tunnel identity {Fore.RED}{hostname}{Style.RESET_ALL} \"\n                  \"key not provided. Continuing to start without this tunnel.\")\n        key_path = Path(key_path)\n        if not key_path.exists():\n            error(f\"SSH tunnel identity {Fore.RED}{hostname}{Style.RESET_ALL} \"\n                  \"key does not exist. Continuing to start without this \"\n                  \"tunnel.\")\n\n        info(f\"  Mounting private key for {hostname} at {key_path}\")\n\n        # we remove the .tmp in the container, this is because the file is\n        # mounted in a volume mount point. Somehow the file is than empty in\n        # the volume but not for the node instance. By removing the .tmp we\n        # make sure that the file is not empty in the volume.\n        mounts.append((f\"/mnt/ssh/{hostname}.pem.tmp\", str(key_path)))\n\n    env = {\n        \"DATA_VOLUME_NAME\": data_volume.name,\n        \"VPN_VOLUME_NAME\": vpn_volume.name,\n        \"PRIVATE_KEY\": \"/mnt/private_key.pem\"\n    }\n\n    # only mount the DB if it is a file\n    info(\"Setting up databases\")\n    db_labels = [db['label'] for db in ctx.databases]\n    for label in db_labels:\n\n        db_config = get_database_config(ctx.databases, label)\n        uri = db_config['uri']\n        db_type = db_config['type']\n\n        info(f\"  Processing {Fore.GREEN}{db_type}{Style.RESET_ALL} database \"\n             f\"{Fore.GREEN}{label}:{uri}{Style.RESET_ALL}\")\n        label_capitals = label.upper()\n\n        try:\n            file_based = Path(uri).exists()\n        except Exception:\n            # If the database uri cannot be parsed, it is definitely not a\n            # file. In case of http servers or sql servers, checking the path\n            # of the the uri will lead to an OS-dependent error, which is why\n            # we catch all exceptions here.\n            file_based = False\n\n        if not file_based and not force_db_mount:\n            debug('  - non file-based database added')\n            env[f'{label_capitals}_DATABASE_URI'] = uri\n        else:\n            debug('  - file-based database added')\n            suffix = Path(uri).suffix\n            env[f'{label_capitals}_DATABASE_URI'] = f'{label}{suffix}'\n            mounts.append((f'/mnt/{label}{suffix}', str(uri)))\n\n    system_folders_option = \"--system\" if system_folders else \"--user\"\n    cmd = f'vnode-local start -c /mnt/config/{name}.yaml -n {name} '\\\n          f' --dockerized {system_folders_option}'\n\n    info(\"Running Docker container\")\n    volumes = []\n    for mount in mounts:\n        volumes.append(f'{mount[1]}:{mount[0]}')\n\n    remove_container_if_exists(\n        docker_client=docker_client, name=ctx.docker_container_name\n    )\n\n    container = docker_client.containers.run(\n        image,\n        command=cmd,\n        volumes=volumes,\n        detach=True,\n        labels={\n            f\"{APPNAME}-type\": \"node\",\n            \"system\": str(system_folders),\n            \"name\": ctx.config_file_name\n        },\n        environment=env,\n        name=ctx.docker_container_name,\n        auto_remove=not keep,\n        tty=True\n    )\n\n    info(f\"Success! container id = {container}\")\n\n    if attach:\n        logs = container.attach(stream=True, logs=True)\n        Thread(target=print_log_worker, args=(logs,), daemon=True).start()\n        while True:\n            try:\n                time.sleep(1)\n            except KeyboardInterrupt:\n                info(\"Closing log file. Keyboard Interrupt.\")\n                info(\"Note that your node is still running! Shut it down with \"\n                     f\"'{Fore.RED}v6 node stop{Style.RESET_ALL}'\")\n                exit(0)\n"], "fixing_code": [".. _algo-code_structure:\n\nAlgorithm code structure\n========================\n\n.. note::\n\n  These guidelines are Python specific.\n\nHere we provide some more information on algorithm code is organized.\nMost of these structures are generated automatically when you create a\n:ref:`personalized algorithm starting point <algo-dev-create-algorithm>`. We detail\nthem here so that you understand why the algorithm code is structured as it is,\nand so that you know how to modify it if necessary.\n\nDefining functions\n------------------\n\nThe functions that will be available to the user have to be defined in the\n``__init__.py`` file at the base of your algorithm module. Other than that,\nyou have complete freedom in which functions you implement.\n\nVantage6 algorithms commonly have an orchestator or aggregator part and a\nremote part. The orchestrator part is responsible for combining the partial\nresults of the remote parts. The remote part is usually executed at each of the\nnodes included in the analysis. While this structure is common for vantage6\nalgorithms, it is not required.\n\nIf you do follow this structure however, we recommend the following file\nstructure:\n\n.. code:: bash\n\n   my_algorithm/\n   \u251c\u2500\u2500 __init__.py\n   \u251c\u2500\u2500 central.py\n   \u2514\u2500\u2500 partial.py\n\nwhere ``__init__.py`` contains the following:\n\n.. code:: python\n\n   from .central import my_central_function\n   from .partial import my_partial_function\n\nand where ``central.py`` and ``partial.py`` obviously contain the implementation\nof those functions.\n\n.. _implementing-decorators:\n\nImplementing the algorithm functions\n------------------------------------\n\nLet's say you are implementing a function called ``my_function``:\n\n.. code:: python\n\n   def my_function(column_name: str):\n       pass\n\nYou have complete freedom as to what arguments you define in your function;\n``column_name`` is just an example. Note that these arguments\nhave to be provided by the user when the algorithm is called. This is explained\n:ref:`here <pyclient-create-task>` for the Python client.\n\nOften, you will want to use the data that is available at the node. This data\ncan be provided to your algorithm function in the following way:\n\n.. code:: python\n\n    import pandas as pd\n    from vantage6.algorithm.tools.decorators import data\n\n    @data(2)\n    def my_function(df1: pd.DataFrame, df2: pd.DataFrame, column_name: str):\n        pass\n\nThe ``@data(2)`` decorator indicates that the first two arguments of the\nfunction are dataframes that should be provided by the vantage6 infrastructure.\nIn this case, the user would have to specify two databases when calling the\nalgorithm. Note that depending on the type of the database used, the user may\nalso have to specify additional parameters such as a SQL query or the name of a\nworksheet in an Excel file.\n\nNote that it is also possible to just specify ``@data()`` without an argument -\nin that case, a single dataframe is added to the arguments.\n\nFor some data sources it's not trivial to construct a dataframe from the data.\nOne of these data sources is the OHDSI OMOP CDM database. For this data source,\nthe ``@database_connection`` is available:\n\n.. code:: python\n\n    from rpy2.robjects import RS4\n    from vantage6.algorithm.tools.decorators import (\n        database_connection, OHDSIMetaData\n    )\n\n    @database_connection(types=[\"OMOP\"], include_metadata=True)\n    def my_function(connection: RS4, metadata: OHDSIMetaData,\n                    <other_arguments>):\n        pass\n\nThis decorator provides the algorithm with a database connection that can be\nused to interact with the database. For instance, you can use this connection\nto execute functions from\n`python-ohdsi <https://python-ohdsi.readthedocs.io/>`_ package. The\n``include_metadata`` argument indicates whether the metadata of the database\nshould also be provided. It is possible to connect to multiple databases at\nonce, but you can also specify a single database by using the ``types``\nargument.\n\n.. code:: python\n\n    from rpy2.robjects import RS4\n    from vantage6.algorithm.tools.decorators import database_connection\n\n    @database_connection(types=[\"OMOP\", \"OMOP\"], include_metadata=False)\n    def my_function(connection1: RS4, connection2: Connection,\n                    <other_arguments>):\n        pass\n\n.. note::\n\n    The ``@database_connection`` decorator is current only available for\n    OMOP CDM databases. The connection object ``RS4`` is an R object, mapped\n    to Python using the `rpy2 <https://rpy2.github.io/>`_, package. This\n    object can be passed directly on to the functions from\n    `python-ohdsi <https://python-ohdsi.readthedocs.io/>`.\n\nAnother useful decorator is the ``@algorithm_client`` decorator:\n\n.. code:: python\n\n    import pandas as pd\n    from vantage6.client.algorithm_client import AlgorithmClient\n    from vantage6.algorithm.tools.decorators import algorithm_client, data\n\n    @data()\n    @algorithm_client\n    def my_function(client: AlgorithmClient, df1: pd.DataFrame, column_name: str):\n        pass\n\nThis decorator provides the algorithm with a client that can be used to interact\nwith the vantage6 central server. For instance, you can use this client in\nthe central part of an algorithm to create a subtasks for each node with\n``client.task.create()``. A full list of all commands that are available\ncan be found in the :ref:`algorithm client documentation <algo-client-api-ref>`.\n\n.. warning::\n\n    The decorators ``@data`` and ``@algorithm_client`` each have one reserved\n    keyword: ``mock_data`` for the ``@data`` decorator and ``mock_client`` for\n    the ``@algorithm_client`` decorator. These keywords should not be used as\n    argument names in your algorithm functions.\n\n    The reserved keywords are used by the\n    :ref:`MockAlgorithmClient <mock-test-algo-dev>` to mock the data and the\n    algorithm client. This is useful for testing your algorithm locally.\n\n\nAlgorithm wrappers\n------------------\n\nThe vantage6 :ref:`wrappers <wrapper-concepts>` are used to simplify the\ninteraction between the algorithm and the node. The wrappers are responsible\nfor reading the input data from the data source and supplying it to the algorithm.\nThey also take care of writing the results back to the data source.\n\nAs algorithm developer, you do not have to worry about the wrappers. The main\npoint you have to make sure is that the following line is present at the end of\nyour ``Dockerfile``:\n\n.. code:: docker\n\n    CMD python -c \"from vantage6.algorithm.tools.wrap import wrap_algorithm; wrap_algorithm()\"\n\nThe ``wrap_algorithm`` function will wrap your algorithm to ensure that the\nvantage6 algorithm tools are available to it. Note that the ``wrap_algorithm``\nfunction will also read the ``PKG_NAME`` environment variable from the\n``Dockerfile`` so make sure that this variable is set correctly.\n\nFor R, the command is slightly different:\n\n.. code:: r\n\n   CMD Rscript -e \"vtg::docker.wrapper('$PKG_NAME')\"\n\nAlso, note that when using R, this only works for CSV files.\n\n.. _vpn-in-algo-dev:\n\nVPN\n---\n\nWithin vantage6, it is possible to communicate with algorithm instances running\non different nodes via the :ref:`VPN network feature <vpn-feature>`. Each of\nthe algorithm instances has their own IP address and port within the VPN\nnetwork. In your algorithm code, you can use the ``AlgorithmClient`` to obtain\nthe IP address and port of other algorithm instances. For example:\n\n.. code:: python\n\n    from vantage6.client import AlgorithmClient\n\n    def my_function(client: AlgorithmClient, ...):\n        # Get the IP address and port of the algorithm instance with id 1\n        child_addresses = client.get_child_addresses()\n        # returns something like:\n        # [\n        #     {\n        #       'port': 1234,\n        #       'ip': 11.22.33.44,\n        #       'label': 'some_label',\n        #       'organization_id': 22,\n        #       'task_id': 333,\n        #       'parent_id': 332,\n        #     }, ...\n        # ]\n\n        # Do something with the IP address and port\n\nThe function ``get_child_addresses()`` gets the VPN addresses of all child\ntasks of the current task. Similarly, the function ``get_parent_address()``\nis available to get the VPN address of the parent task. Finally, there is\na client function ``get_addresses()`` that returns the VPN addresses of all\nalgorithm instances that are part of the same task.\n\nVPN communication is only possible if the docker container exposes ports to\nthe VPN network. In the algorithm boilerplate, one port is exposed by default.\nIf you need to expose more ports (e.g. for sending different information to\ndifferent parts of your algorithm), you can do so by adding lines to the\nDockerfile:\n\n.. code:: bash\n\n   # port 8888 is used by the algorithm for communication purposes\n   EXPOSE 8888\n   LABEL p8888 = \"some-label\"\n\n   # port 8889 is used by the algorithm for data-exchange\n   EXPOSE 8889\n   LABEL p8889 = \"some-other-label\"\n\nThe ``EXPOSE`` command exposes the port to the VPN network. The ``LABEL``\ncommand adds a label to the port. This label returned with the clients'\n``get_addresses()`` function suite. You may specify as many ports as you need.\nNote that you *must* specify the label with ``p`` as prefix followed by the\nport number. The vantage6 infrastructure relies on this naming convention.\n\n\nDockerfile structure\n--------------------\n\nOnce the algorithm code is written, the algorithm needs to be packaged and made\navailable for retrieval by the nodes. The algorithm is packaged in a Docker\nimage. A Docker image is created from a Dockerfile, which acts as a blue-print.\n\nThe Dockerfile is already present in the boilerplate code. Usually, the only\nline that you need to update is the ``PKG_NAME`` variable to the name of your\nalgorithm package.\n\n", ".. _algo-dev-guide:\n\nAlgorithm development step-by-step guide\n========================================\n\nThis page offers a step-by-step guide to develop a vantage6 algorithm.\nWe refer to the `algorithm concepts <algo-concepts>`_ section\nregularly. In that section, we explain the fundamentals of algorithm containers\nin more detail than in this guide.\n\nAlso, note that this guide is mainly aimed at developers who want to develop\ntheir algorithm in Python, although we will try to clearly indicate where\nthis differs from algorithms written in other languages.\n\n.. _algo-dev-create-algorithm:\n\nStarting point\n--------------\n\nWhen starting to develop a new vantage6 algorithm in Python, the easiest way to\nstart is:\n\n.. code::\n\n   v6 algorithm create\n\nRunning this command will prompt you to answering some questions, which will\nresult in a personalized starting point or 'boilerplate' for your algorithm.\nAfter doing so, you will have a new folder with the name of your algorithm,\nboilerplate code and a checklist in the README.md file that you can follow to\ncomplete your algorithm.\n\n.. note::\n   There is also a `boilerplate for R <https://github.com/IKNL/vtg.tpl>`_,\n   but this is not flexible and it is not updated as frequently as the Python\n   boilerplate.\n\nSetting up your environment\n---------------------------\n\nIt is good practice to set up a virtual environment for your algorithm\npackage.\n\n.. code:: bash\n\n   # This code is just a suggestion - there are many ways of doing this.\n\n   # go to the algorithm directory\n   cd /path/to/algorithm\n\n   # create a Python environment. Be sure to replace <my-algorithm-env> with\n   # the name of your environment.\n   conda create -n <my-algorithm-env> python=3.10\n   conda activate <my-algorithm-env>\n\n   # install the algorithm dependencies\n   pip install -r requirements.txt\n\nAlso, it is always good to use a version control system such as ``git`` to\nkeep track of your changes. An initial commit of the boilerplate code could be:\n\n.. code:: bash\n\n   cd /path/to/algorithm\n   git init\n   git add .\n   git commit -m \"Initial commit\"\n\nNote that having your code in a git repository is necessary if you want to\n:ref:`update your algorithm <algo-dev-update-algo>`.\n\nImplementing your algorithm\n---------------------------\n\nYour personalized starting point should make clear to you which functions you need to\nimplement - there are `TODO` comments in the code that indicate where you need\nto add your own code.\n\nYou may wonder why the boilerplate code is structured the way it is. This\nis explained in the :ref:`code structure section <algo-code_structure>`.\n\n.. _algo-env-vars:\n\nEnvironment variables\n---------------------\n\nThe algorithms have access to several environment variables. You can also\nspecify additional environment variables via the ``algorithm_env`` option\nin the node configuration files (see the\n:ref:`example node configuration file <node-configure-structure>`).\n\nEnvironment variables provided by the vantage6 infrastructure are used\nto locate certain files or to add local configuration settings into the\ncontainer. These are usually used in the Python wrapper and you don't normally\nneed them in your functions. However, you can access them in your functions\nas follows:\n\n.. code:: python\n\n   from vantage6.algorithm.tools.util import get_env_var\n\n   def my_function():\n       input_file = get_env_var(\"INPUT_FILE\")\n       token_file = get_env_var(\"DEFAULT_DATABASE_URI\")\n\n       # do something with the input file and database URI\n\n.. note::\n\n   The ``get_env_var`` function is used here rather than the standard\n   ``os.environ`` dictionary because the environment variables are encoded\n   for security purposes. The ``get_env_var`` function will decode the\n   environment variable for you.\n\nThe environment variables that you specify in the node configuration file\ncan be used in the exact same manner. You can view all environment variables\nthat are available to your algorithm by ``print(os.environ)``.\n\nReturning results\n-----------------\n\nReturning the results of you algorithm is rather straightforward. At the end\nof your algorithm function, you can simply return the results as a dictionary:\n\n.. code:: python\n\n    def my_function(column_name: str):\n        return {\n            \"result\": 42\n        }\n\nThese results will be returned to the user after the algorithm has finished.\n\n.. warning::\n\n    The results that you return should be JSON serializable. This means that\n    you cannot, for example, return a ``pandas.DataFrame`` or a\n    ``numpy.ndarray`` (such objects may not be readable to a non-Python using\n    recipient or may even be insecure to send over the internet). They should\n    be converted to a JSON-serializable format first.\n\nExample functions\n-----------------\n\nJust an example of how you can implement your algorithm:\n\nCentral function\n~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n  from vantage6.algorithm.tools.decorators import algorithm_client\n  from vantage6.client.algorithm_client import AlgorithmClient\n\n   @algorithm_client\n   def main(client: AlgorithmClient, *args, **kwargs):\n      # Run partial function.\n      task = client.task.create(\n         {\n            \"method\": \"my_algorithm\",\n            \"args\": args,\n            \"kwargs\": kwargs\n         },\n         organization_ids=[1, 2]\n      )\n\n       # wait for the federated part to complete\n       # and return\n       results = wait_and_collect(task)\n\n       return results\n\nPartial function\n~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n   import pandas as pd\n   from vantage6.algorithm.tools.decorators import data\n\n   @data(1)\n   def my_partial_function(data: pd.DataFrame, column_name: str):\n       # do something with the data\n       data[column_name] = data[column_name] + 1\n\n       # return the results\n       return {\n           \"result\": sum(data[colum_name].to_list())\n       }\n\n.. _mock-test-algo-dev:\n\nTesting your algorithm\n----------------------\n\nIt can be helpful to test your algorithm outside of Docker using the\n``MockAlgorithmClient``. This may save\ntime as it does not require you to set up a test infrastructure with a vantage6\nserver and nodes, and allows you to test your algorithm without building a\nDocker image every time. The algorithm boilerplate code comes with a test file that\nyou can use to test your algorithm using the ``MockAlgorithmClient`` - you can\nof course extend that to add more or different tests.\n\nThe :ref:`MockAlgorithmClient <mock-client-api-ref>` has the same interface as\nthe ``AlgorithmClient``, so it should be easy to switch between the two. An\nexample of how you can use the ``MockAlgorithmClient`` to test your algorithm\nis included in the boilerplate code.\n\nWriting documentation\n---------------------\n\nIt is important that you add documentation of your algorithm so that users\nknow how to use it. In principle, you may choose any format of documentation,\nand you may choose to host it anywhere you like. However, in our experience it\nworks well to keep your documentation close to your code. We recommend using the\n``readthedocs`` platform to host your documentation. Alternatively, you could\nuse a ``README`` file in the root of your algorithm directory - if the\ndocumentation is not too extensive, this may be sufficient.\n\n.. note::\n\n    We intend to provide a template for the documentation of algorithms in the\n    future. This template will be based on the ``readthedocs`` platform.\n\nPackage & distribute\n--------------------\n\nThe algorithm boilerplate comes with a ``Dockerfile`` that is a blueprint for\ncreating a Docker image of your algorithm. This Docker image is the package\nthat you will distribute to the nodes.\n\nIf you go to the folder containing your algorithm, you will also find the\nDockerfile there, immediately at the top directory. You can then build the\nproject as follows:\n\n.. code:: bash\n\n   docker build -t repo/image:tag .\n\nThe ``-t`` indicated the name of your image. This name is also used as\nreference where the image is located on the internet. Once the Docker image is\ncreated it needs to be uploaded to a registry so that nodes can retrieve it,\nwhich you can do by pushing the image:\n\n.. code:: bash\n\n   docker push repo/image:tag\n\nHere are a few examples of how to build and upload your image:\n\n.. code:: bash\n\n    # Build and upload to Docker Hub. Replace <my-user-name> with your Docker\n    # Hub username and make sure you are logged in with ``docker login``.\n    docker build -t my-user-name/algorithm-example:latest .\n    docker push my-user-name/algorithm-example:latest\n\n    # Build and upload to private registry. Here you don't need to provide\n    # a username but you should write out the full image URL. Also, again you\n    # need to be logged in with ``docker login``.\n    docker build -t harbor2.vantage6.ai/PROJECT/algorithm-example:latest .\n    docker push harbor2.vantage6.ai/PROJECT/algorithm-example:latest\n\nNow that your algorithm has been uploaded it is available for nodes to retrieve\nwhen they need it.\n\nCalling your algorithm from vantage6\n------------------------------------\n\nIf you want to test your algorithm in the context of vantage6, you should\nset up a vantage6 infrastructure. You should create a server and at least one\nnode (depending on your algorithm you may need more). Follow the instructions\nin the :ref:`server-admin-guide` and :ref:`node-admin-guide` to set up your\ninfrastructure. If you are running them on the same machine, take care to\nprovide the node with the proper address of the server as detailed\n:ref:`here <use-server-local>`.\n\nOnce your infrastructure is set up, you can create a task for your algorithm.\nYou can do this either via the :ref:`UI <ui>` or via the\n:ref:`Python client <pyclient-create-task>`.\n\n.. todo Add example with ``v6 dev``\n\n.. _algo-dev-update-algo:\n\nUpdating your algorithm\n-----------------------\n\nAt some point, there may be changes in the vantage6 infrastructure that require\nyou to update your algorithm. Such changes are made available via\nthe ``v6 algorithm update`` command. This command will update your algorithm\nto the latest version of the vantage6 infrastructure.\n\nYou can also use the ``v6 algorithm update`` command to update your algorithm\nif you want to modify your answers to the questionnaire. In that case, you\nshould be sure to commit the changes in ``git`` before running the command.\n", "import os\nimport json\nimport jwt\n\nfrom pathlib import Path\nfrom functools import wraps\nfrom dataclasses import dataclass\n\nimport pandas as pd\n\nfrom vantage6.algorithm.client import AlgorithmClient\nfrom vantage6.algorithm.tools.mock_client import MockAlgorithmClient\nfrom vantage6.algorithm.tools.util import info, error, warn, get_env_var\nfrom vantage6.algorithm.tools.wrappers import load_data\nfrom vantage6.algorithm.tools.preprocessing import preprocess_data\n\nOHDSI_AVAILABLE = True\ntry:\n    from ohdsi.database_connector import connect as connect_to_omop\nexcept ImportError:\n    OHDSI_AVAILABLE = False\n\n\n@dataclass\nclass RunMetaData:\n    \"\"\"Dataclass containing metadata of the run.\"\"\"\n    task_id: int | None\n    node_id: int | None\n    collaboration_id: int | None\n    organization_id: int | None\n    temporary_directory: Path | None\n    output_file: Path | None\n    input_file: Path | None\n    token_file: Path | None\n\n\n@dataclass\nclass OHDSIMetaData:\n    \"\"\"Dataclass containing metadata of the OMOP database.\"\"\"\n    database: str | None\n    cdm_schema: str | None\n    results_schema: str | None\n    incremental_folder: Path | None\n    cohort_statistics_folder: Path | None\n    export_folder: Path | None\n\n\ndef _algorithm_client() -> callable:\n    \"\"\"\n    Decorator that adds an algorithm client object to a function\n\n    By adding @algorithm_client to a function, the ``algorithm_client``\n    argument will be added to the front of the argument list. This client can\n    be used to communicate with the server.\n\n    There is one reserved argument `mock_client` in the function to be\n    decorated. If this argument is provided, the decorator will add this\n    MockAlgorithmClient to the front of the argument list instead of the\n    regular AlgorithmClient.\n\n    Parameters\n    ----------\n    func : callable\n        Function to decorate\n\n    Returns\n    -------\n    callable\n        Decorated function\n\n    Examples\n    --------\n    >>> @algorithm_client\n    >>> def my_algorithm(algorithm_client: AlgorithmClient, <other arguments>):\n    >>>     pass\n    \"\"\"\n    def protection_decorator(func: callable, *args, **kwargs) -> callable:\n        @wraps(func)\n        def decorator(*args, mock_client: MockAlgorithmClient = None,\n                      **kwargs) -> callable:\n            \"\"\"\n            Wrap the function with the client object\n\n            Parameters\n            ----------\n            mock_client : MockAlgorithmClient\n                Mock client to use instead of the regular client\n            \"\"\"\n            if mock_client is not None:\n                return func(mock_client, *args, **kwargs)\n            # read server address from the environment\n            host = get_env_var(\"HOST\")\n            port = get_env_var(\"PORT\")\n            api_path = get_env_var(\"API_PATH\")\n\n            # read token from the environment\n            token_file = get_env_var(\"TOKEN_FILE\")\n            info(\"Reading token\")\n            with open(token_file) as fp:\n                token = fp.read().strip()\n\n            client = AlgorithmClient(token=token, host=host, port=port,\n                                     path=api_path)\n            return func(client, *args, **kwargs)\n        # set attribute that this function is wrapped in an algorithm client\n        decorator.wrapped_in_algorithm_client_decorator = True\n        return decorator\n    return protection_decorator\n\n\n# alias for algorithm_client so that algorithm developers can do\n# @algorithm_client instead of @algorithm_client()\nalgorithm_client = _algorithm_client()\n\n\ndef data(number_of_databases: int = 1) -> callable:\n    \"\"\"\n    Decorator that adds algorithm data to a function\n\n    By adding `@data()` to a function, one or several pandas dataframes will be\n    added to the front of the argument list. This data will be read from the\n    databases that the user who creates the task provides.\n\n    Note that the user should provide exactly as many databases as the\n    decorated function requires when they create the task.\n\n    There is one reserved argument `mock_data` in the function to be\n    decorated. If this argument is provided, the decorator will add this\n    mocked data to the front of the argument list, instead of reading in the\n    data from the databases.\n\n    Parameters\n    ----------\n    number_of_databases: int\n        Number of data sources to load. These will be loaded in order by which\n        the user provided them. Default is 1.\n\n    Returns\n    -------\n    callable\n        Decorated function\n\n    Examples\n    --------\n    >>> @data(number_of_databases=2)\n    >>> def my_algorithm(first_df: pd.DataFrame, second_df: pd.DataFrame,\n    >>>                  <other arguments>):\n    >>>     pass\n    \"\"\"\n    def protection_decorator(func: callable, *args, **kwargs) -> callable:\n        @wraps(func)\n        def decorator(*args, mock_data: list[pd.DataFrame] = None,\n                      **kwargs) -> callable:\n            \"\"\"\n            Wrap the function with the data\n\n            Parameters\n            ----------\n            mock_data : list[pd.DataFrame]\n                Mock data to use instead of the regular data\n            \"\"\"\n            if mock_data is not None:\n                return func(*mock_data, *args, **kwargs)\n\n            # read the labels that the user requested\n            labels = _get_user_database_labels()\n\n            # check if user provided enough databases\n            if len(labels) < number_of_databases:\n                error(f\"Algorithm requires {number_of_databases} databases \"\n                      f\"but only {len(labels)} were provided. \"\n                      \"Exiting...\")\n                exit(1)\n            elif len(labels) > number_of_databases:\n                warn(f\"Algorithm requires only {number_of_databases} databases\"\n                     f\", but {len(labels)} were provided. Using the \"\n                     f\"first {number_of_databases} databases.\")\n\n            for i in range(number_of_databases):\n                label = labels[i]\n                # read the data from the database\n                info(\"Reading data from database\")\n                data_ = _get_data_from_label(label)\n\n                # do any data preprocessing here\n                info(f\"Applying preprocessing for database '{label}'\")\n                env_prepro = get_env_var(f\"{label.upper()}_PREPROCESSING\")\n                if env_prepro is not None:\n                    preprocess = json.loads(env_prepro)\n                    data_ = preprocess_data(data_, preprocess)\n\n                # add the data to the arguments\n                args = (data_, *args)\n\n            return func(*args, **kwargs)\n        # set attribute that this function is wrapped in a data decorator\n        decorator.wrapped_in_data_decorator = True\n        return decorator\n    return protection_decorator\n\n\ndef database_connection(types: list[str], include_metadata: bool = True) \\\n        -> callable:\n    \"\"\"\n    Decorator that adds a database connection to a function\n\n    By adding @database_connection to a function, a database connection will\n    be added to the front of the argument list. This connection can be used to\n    communicate with the database.\n\n    Parameters\n    ----------\n    types : list[str]\n        List of types of databases to connect to. Currently only \"OMOP\" is\n        supported.\n    include_metadata : bool\n        Whether to include metadata in the function arguments. This metadata\n        contains the database name, CDM schema, and results schema. Default is\n        True.\n\n    Example\n    -------\n    For a single OMOP data source:\n    >>> @database_connection(types=[\"OMOP\"])\n    >>> def my_algorithm(connection: Connection, meta: OHDSIMetaData,\n    >>>                  <other arguments>):\n    >>>     pass\n\n    In case you have multiple OMOP data sources:\n    >>> @database_connection(types=[\"OMOP\", \"OMOP\"])\n    >>> def my_algorithm(connection1: Connection, meta1: OHDSIMetaData,\n    >>>                  connection2: Connection, meta2: OHDSIMetaData,\n    >>>                  <other arguments>):\n    >>>     pass\n\n    In the case you do not want to include the metadata:\n    >>> @database_connection(types=[\"OMOP\"], include_metadata=False)\n    >>> def my_algorithm(connection: Connection, <other arguments>):\n    >>>     pass\n    \"\"\"\n    def connection_decorator(func: callable, *args, **kwargs) -> callable:\n        @wraps(func)\n        def decorator(*args, **kwargs) -> callable:\n            \"\"\"\n            Wrap the function with the database connection\n            \"\"\"\n            labels = _get_user_database_labels()\n            if len(labels) < len(types):\n                error(f\"User provided {len(labels)} databases, but algorithm \"\n                      f\"requires {len(types)} database connections. Exiting.\")\n                exit(1)\n            if len(labels) > len(types):\n                warn(f\"User provided {len(labels)} databases, but algorithm \"\n                     f\"requires {len(types)} database connections. Using the \"\n                     f\"first {len(types)} databases.\")\n\n            db_args = []\n            # Note: zip will stop at the shortest iterable, so this is exactly\n            # what we want in the len(labels) > len(types) case.\n            for type_, label in zip(types, labels):\n                match type_.upper():\n                    case \"OMOP\":\n                        info(\"Creating OMOP database connection\")\n                        connection = _create_omop_database_connection(label)\n                        db_args.append(connection)\n                        if include_metadata:\n                            meta = get_ohdsi_metadata(label)\n                            db_args.append(meta)\n                    # case \"FHIR\":\n                    #     pass\n\n            return func(*db_args, *args, **kwargs)\n\n        return decorator\n    return connection_decorator\n\n\ndef metadata(func: callable) -> callable:\n    @wraps(func)\n    def decorator(*args, **kwargs) -> callable:\n        \"\"\"\n        Decorator the function with metadata from the run.\n\n        Decorator that adds metadata from the run to the function. This\n        includes the task id, node id, collaboration id, organization id,\n        temporary directory, output file, input file, and token file.\n\n        Example\n        -------\n        >>> @metadata\n        >>> def my_algorithm(metadata: RunMetaData, <other arguments>):\n        >>>     pass\n        \"\"\"\n        token_file = get_env_var(\"TOKEN_FILE\")\n        info(\"Reading token\")\n        with open(token_file) as fp:\n            token = fp.read().strip()\n\n        info(\"Extracting payload from token\")\n        payload = _extract_token_payload(token)\n\n        metadata = RunMetaData(\n            task_id=payload[\"task_id\"],\n            node_id=payload[\"node_id\"],\n            collaboration_id=payload[\"collaboration_id\"],\n            organization_id=payload[\"organization_id\"],\n            temporary_directory=Path(get_env_var(\"TEMPORARY_FOLDER\")),\n            output_file=Path(get_env_var(\"OUTPUT_FILE\")),\n            input_file=Path(get_env_var(\"INPUT_FILE\")),\n            token_file=Path(get_env_var(\"TOKEN_FILE\"))\n        )\n        return func(metadata, *args, **kwargs)\n    return decorator\n\n\ndef get_ohdsi_metadata(label: str) -> OHDSIMetaData:\n    \"\"\"\n    Retrieve the OHDSI metadata from the environment variables.\n\n    The following environment variables are expected to be set in the\n    node configuration in the `env` key of the `database` section:\n\n    - `CDM_DATABASE`\n    - `CDM_SCHEMA`\n    - `RESULTS_SCHEMA`\n\n    In case these are not set, the algorithm execution is terminated.\n\n    Example\n    -------\n    >>> get_ohdsi_metadata(\"my_database\")\n    \"\"\"\n    # check that all node environment variables are set\n    expected_env_vars = [\"CDM_DATABASE\", \"CDM_SCHEMA\", \"RESULTS_SCHEMA\"]\n    label_ = label.upper()\n    for var in expected_env_vars:\n        _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')\n\n    tmp = Path(get_env_var(\"TEMPORARY_FOLDER\"))\n    metadata = OHDSIMetaData(\n        database=get_env_var(f\"{label_}_DB_PARAM_CDM_DATABASE\"),\n        cdm_schema=get_env_var(f\"{label_}_DB_PARAM_CDM_SCHEMA\"),\n        results_schema=get_env_var(f\"{label_}_DB_PARAM_RESULTS_SCHEMA\"),\n        incremental_folder=tmp / \"incremental\",\n        cohort_statistics_folder=tmp / \"cohort_statistics\",\n        export_folder=tmp / \"export\"\n    )\n    return metadata\n\n\ndef _create_omop_database_connection(label: str) -> callable:\n    \"\"\"\n    Create a connection to an OMOP database.\n\n    It expects that the following environment variables are set:\n    - DB_PARAM_DBMS: type of database to connect to\n    - DB_PARAM_USER: username to connect to the database\n    - DB_PARAM_PASSWORD: password to connect to the database\n\n    These should be provided in the vantage6 node configuration file in the\n    `database` section without the `DB_PARAM_` prefix. For example:\n\n    ```yaml\n    ...\n    databases:\n      - label: my_database\n        type: OMOP\n        uri: jdbc:postgresql://host.docker.internal:5454/postgres\n        env:\n            DBMS: \"postgresql\"\n            USER: \"my_user\"\n            PASSWORD: \"my_password\"\n    ...\n    ```\n\n    Parameters\n    ----------\n    label : str\n        Label of the database to connect to\n\n    Returns\n    -------\n    callable\n        OHDSI Database Connection object\n    \"\"\"\n\n    # check that the OHDSI package is available in this container\n    if not OHDSI_AVAILABLE:\n        error(\"OHDSI/DatabaseConnector is not available.\")\n        error(\"Did you use 'algorithm-ohdsi-base' image to build this \"\n              \"algorithm?\")\n        exit(1)\n\n    # environment vars are always uppercase\n    label_ = label.upper()\n\n    # check that the required environment variables are set\n    for var in (\"DBMS\", \"USER\", \"PASSWORD\"):\n        _check_environment_var_exists_or_exit(f'{label_}_DB_PARAM_{var}')\n\n    info(\"Reading OHDSI environment variables\")\n    dbms = get_env_var(f\"{label_}_DB_PARAM_DBMS\")\n    uri = get_env_var(f\"{label_}_DATABASE_URI\")\n    user = get_env_var(f\"{label_}_DB_PARAM_USER\")\n    password = get_env_var(f\"{label_}_DB_PARAM_PASSWORD\")\n    info(f' - dbms: {dbms}')\n    info(f' - uri: {uri}')\n    info(f' - user: {user}')\n\n    info(\"Creating OHDSI database connection\")\n    return connect_to_omop(dbms=dbms, connection_string=uri, password=password,\n                           user=user)\n\n\ndef _check_environment_var_exists_or_exit(var: str):\n    \"\"\"\n    Check if the environment variable 'var' exists or print and exit.\n\n    Parameters\n    ----------\n    var : str\n        Environment variable name to check\n    \"\"\"\n    if var not in os.environ:\n        error(f\"Environment variable '{var}' is not set. Exiting...\")\n        exit(1)\n\n\ndef _get_data_from_label(label: str) -> pd.DataFrame:\n    \"\"\"\n    Load data from a database based on the label\n\n    Parameters\n    ----------\n    label : str\n        Label of the database to load\n\n    Returns\n    -------\n    pd.DataFrame\n        Data from the database\n    \"\"\"\n    # Load the input data from the input file - this may e.g. include the\n    database_uri = get_env_var(f\"{label.upper()}_DATABASE_URI\")\n    info(f\"Using '{database_uri}' with label '{label}' as database\")\n\n    # Get the database type from the environment variable, this variable is\n    # set by the vantage6 node based on its configuration file.\n    database_type = get_env_var(\n        f\"{label.upper()}_DATABASE_TYPE\", \"csv\").lower()\n\n    # Load the data based on the database type. Try to provide environment\n    # variables that should be available for some data types.\n    return load_data(\n        database_uri,\n        database_type,\n        query=get_env_var(f\"{label.upper()}_QUERY\"),\n        sheet_name=get_env_var(f\"{label.upper()}_SHEET_NAME\")\n    )\n\n\ndef _get_user_database_labels() -> list[str]:\n    \"\"\"\n    Get the database labels from the environment\n\n    Returns\n    -------\n    list[str]\n        List of database labels\n    \"\"\"\n    # read the labels that the user requested, which is a comma\n    # separated list of labels.\n    labels = get_env_var(\"USER_REQUESTED_DATABASE_LABELS\")\n    return labels.split(',')\n\n\ndef _extract_token_payload(token: str) -> dict:\n    \"\"\"\n    Extract the payload from the token.\n\n    Parameters\n    ----------\n    token: str\n        The token as a string.\n\n    Returns\n    -------\n    dict\n        The payload as a dictionary. It contains the keys: `client_type`,\n        `node_id`, `organization_id`, `collaboration_id`, `task_id`, `image`,\n        and `databases`.\n    \"\"\"\n    jwt_payload = jwt.decode(token, options={\"verify_signature\": False})\n    return jwt_payload['sub']\n", "import sys\nimport os\nimport base64\nfrom vantage6.common.globals import STRING_ENCODING, ENV_VAR_EQUALS_REPLACEMENT\n\n\ndef info(msg: str) -> None:\n    \"\"\"\n    Print an info message to stdout.\n\n    Parameters\n    ----------\n    msg : str\n        Message to be printed\n    \"\"\"\n    sys.stdout.write(f\"info > {msg}\\n\")\n\n\ndef warn(msg: str) -> None:\n    \"\"\"\n    Print a warning message to stdout.\n\n    Parameters\n    ----------\n    msg : str\n        Warning message to be printed\n    \"\"\"\n    sys.stdout.write(f\"warn > {msg}\\n\")\n\n\ndef error(msg: str) -> None:\n    \"\"\"\n    Print an error message to stdout.\n\n    Parameters\n    ----------\n    msg : str\n        Error message to be printed\n    \"\"\"\n    sys.stdout.write(f\"error > {msg}\\n\")\n\n\ndef get_env_var(var_name: str, default: str | None = None) -> str:\n    \"\"\"\n    Get the value of an environment variable. Environment variables are encoded\n    by the node so they need to be decoded here.\n\n    Note that this decoding follows the reverse of the encoding in the node:\n    first replace '=' back and then decode the base32 string.\n\n    Parameters\n    ----------\n    var_name : str\n        Name of the environment variable\n\n    Returns\n    -------\n    var_value : str | None\n        Value of the environment variable, or default value if not found\n    \"\"\"\n    try:\n        encoded_env_var_value = \\\n            os.environ[var_name].replace(\n                ENV_VAR_EQUALS_REPLACEMENT, \"=\"\n            ).encode(STRING_ENCODING)\n        return base64.b32decode(encoded_env_var_value).decode(STRING_ENCODING)\n    except KeyError:\n        return default\n", "import os\nimport importlib\nimport traceback\n\nfrom typing import Any\n\nfrom vantage6.common.client import deserialization\nfrom vantage6.common import serialization\nfrom vantage6.algorithm.tools.util import info, error, get_env_var\nfrom vantage6.algorithm.tools.exceptions import DeserializationException\n\n\ndef wrap_algorithm(log_traceback: bool = True) -> None:\n    \"\"\"\n    Wrap an algorithm module to provide input and output handling for the\n    vantage6 infrastructure.\n\n    Data is received in the form of files, whose location should be\n    specified in the following environment variables:\n\n    - ``INPUT_FILE``: input arguments for the algorithm. This file should be\n      encoded in JSON format.\n    - ``OUTPUT_FILE``: location where the results of the algorithm should\n      be stored\n    - ``TOKEN_FILE``: access token for the vantage6 server REST api\n    - ``USER_REQUESTED_DATABASE_LABELS``: comma-separated list of database\n      labels that the user requested\n    - ``<DB_LABEL>_DATABASE_URI``: uri of the each of the databases that\n      the user requested, where ``<DB_LABEL>`` is the label of the\n      database given in ``USER_REQUESTED_DATABASE_LABELS``.\n\n    The wrapper expects the input file to be a json file. Any other file\n    format will result in an error.\n\n    Parameters\n    ----------\n    module : str\n        Python module name of the algorithm to wrap.\n    log_traceback: bool\n        Whether to print the full error message from algorithms or not, by\n        default False. Algorithm developers should set this to False if\n        the error messages may contain sensitive information. By default True.\n    \"\"\"\n    # get the module name from the environment variable. Note that this env var\n    # is set in the Dockerfile and is therefore not encoded.\n    module = os.environ.get(\"PKG_NAME\")\n    info(f\"wrapper for {module}\")\n\n    # read input from the mounted input file.\n    input_file = get_env_var(\"INPUT_FILE\")\n    info(f\"Reading input file {input_file}\")\n    input_data = load_input(input_file)\n\n    # make the actual call to the method/function\n    info(\"Dispatching ...\")\n    output = _run_algorithm_method(input_data, module, log_traceback)\n\n    # write output from the method to mounted output file. Which will be\n    # transferred back to the server by the node-instance.\n    output_file = get_env_var(\"OUTPUT_FILE\")\n    info(f\"Writing output to {output_file}\")\n\n    _write_output(output, output_file)\n\n\ndef _run_algorithm_method(input_data: dict, module: str,\n                          log_traceback: bool = True) -> Any:\n    \"\"\"\n    Load the algorithm module and call the correct method to run an algorithm.\n\n    Parameters\n    ----------\n    input_data : dict\n        The input data that is passed to the algorithm. This should at least\n        contain the key 'method' which is the name of the method that should be\n        called. Other keys depend on the algorithm.\n    module : str\n        The module that contains the algorithm.\n    log_traceback: bool, optional\n        Whether to print the full error message from algorithms or not, by\n        default False. Algorithm developers should set this to False if\n        the error messages may contain sensitive information. By default True.\n\n    Returns\n    -------\n    Any\n        The result of the algorithm.\n    \"\"\"\n    # import algorithm module\n    try:\n        lib = importlib.import_module(module)\n        info(f\"Module '{module}' imported!\")\n    except ModuleNotFoundError:\n        error(f\"Module '{module}' can not be imported! Exiting...\")\n        exit(1)\n\n    # get algorithm method and attempt to load it\n    method_name = input_data[\"method\"]\n    try:\n        method = getattr(lib, method_name)\n    except AttributeError:\n        error(f\"Method '{method_name}' not found!\\n\")\n        exit(1)\n\n    # get the args and kwargs input for this function.\n    args = input_data.get(\"args\", [])\n    kwargs = input_data.get(\"kwargs\", {})\n\n    # try to run the method\n    try:\n        result = method(*args, **kwargs)\n    except Exception as e:\n        error(f\"Error encountered while calling {method_name}: {e}\")\n        if log_traceback:\n            error(traceback.print_exc())\n        exit(1)\n\n    return result\n\n\ndef load_input(input_file: str) -> Any:\n    \"\"\"\n    Load the input from the input file.\n\n    Parameters\n    ----------\n    input_file : str\n        File containing the input\n\n    Returns\n    -------\n    input_data : Any\n        Input data for the algorithm\n\n    Raises\n    ------\n    DeserializationException\n        Failed to deserialize input data\n    \"\"\"\n    with open(input_file, \"rb\") as fp:\n        try:\n            input_data = deserialization.deserialize(fp)\n        except DeserializationException:\n            raise DeserializationException('Could not deserialize input')\n    return input_data\n\n\ndef _write_output(output: Any, output_file: str) -> None:\n    \"\"\"\n    Write output to output file using JSON serialization.\n\n    Parameters\n    ----------\n    output : Any\n        Output of the algorithm\n    output_file : str\n        Path to the output file\n    \"\"\"\n    with open(output_file, 'wb') as fp:\n        serialized = serialization.serialize(output)\n        fp.write(serialized)\n", "from pathlib import Path\n\n#\n#   PACKAGE GLOBALS\n#\nSTRING_ENCODING = \"utf-8\"\n\nAPPNAME = \"vantage6\"\n\nMAIN_VERSION_NAME = \"cotopaxi\"\n\nDEFAULT_DOCKER_REGISTRY = \"harbor2.vantage6.ai\"\n\nDEFAULT_NODE_IMAGE = f\"infrastructure/node:{MAIN_VERSION_NAME}\"\n\nDEFAULT_NODE_IMAGE_WO_TAG = \"infrastructure/node\"\n\nDEFAULT_SERVER_IMAGE = f\"infrastructure/server:{MAIN_VERSION_NAME}\"\n\nDEFAULT_UI_IMAGE = f\"infrastructure/ui:{MAIN_VERSION_NAME}\"\n\n#\n#   COMMON GLOBALS\n#\nPACKAGE_FOLDER = Path(__file__).parent.parent.parent\n\nVPN_CONFIG_FILE = 'vpn-config.ovpn.conf'\n\nDATABASE_TYPES = [\"csv\", \"parquet\", \"sql\", \"sparql\", \"omop\", \"excel\", \"other\"]\n\nPING_INTERVAL_SECONDS = 60\n\n# start trying to refresh the JWT token of the node 10 minutes before it\n# expires.\nNODE_CLIENT_REFRESH_BEFORE_EXPIRES_SECONDS = 600\n\n# The basics image can be used (mainly by the UI) to collect column names\nBASIC_PROCESSING_IMAGE = 'harbor2.vantage6.ai/algorithms/basics'\n\n# Character to replace '=' with in encoded environment variables\nENV_VAR_EQUALS_REPLACEMENT = \"!\"\n", "import json\nfrom vantage6.common.globals import STRING_ENCODING\n\n\n# TODO BvB 2023-02-03: I feel this function could be given a better name. And\n# it might not have to be in a separate file.\ndef serialize(data: any) -> bytes:\n    \"\"\"\n    Serialize data using the specified format\n\n    Parameters\n    ----------\n    data: any\n        The data to be serialized\n\n    Returns\n    -------\n    bytes\n        A JSON-serialized and then encoded bytes object representing the data\n    \"\"\"\n    return json.dumps(data).encode(STRING_ENCODING)\n", "\"\"\"\nDocker manager\n\nThe docker manager is responsible for communicating with the docker-daemon and\nis a wrapper around the docker module. It has methods\nfor creating docker networks, docker volumes, start containers and retrieve\nresults from finished containers.\n\"\"\"\nimport os\nimport time\nimport logging\nimport docker\nimport re\nimport shutil\n\nfrom typing import NamedTuple\nfrom pathlib import Path\n\nfrom vantage6.common import logger_name\nfrom vantage6.common import get_database_config\nfrom vantage6.common.docker.addons import get_container, running_in_docker\nfrom vantage6.common.globals import APPNAME, BASIC_PROCESSING_IMAGE\nfrom vantage6.common.task_status import TaskStatus, has_task_failed\nfrom vantage6.common.docker.network_manager import NetworkManager\nfrom vantage6.algorithm.tools.wrappers import get_column_names\nfrom vantage6.cli.context import NodeContext\nfrom vantage6.node.context import DockerNodeContext\nfrom vantage6.node.docker.docker_base import DockerBaseManager\nfrom vantage6.node.docker.vpn_manager import VPNManager\nfrom vantage6.node.docker.task_manager import DockerTaskManager\nfrom vantage6.node.docker.squid import Squid\nfrom vantage6.common.client.node_client import NodeClient\nfrom vantage6.node.docker.exceptions import (\n    UnknownAlgorithmStartFail,\n    PermanentAlgorithmStartFail,\n    AlgorithmContainerNotFound\n)\n\nlog = logging.getLogger(logger_name(__name__))\n\n\nclass Result(NamedTuple):\n    \"\"\"\n    Data class to store the result of the docker image.\n\n    Attributes\n    ----------\n    run_id: int\n        ID of the current algorithm run\n    logs: str\n        Logs attached to current algorithm run\n    data: str\n        Output data of the algorithm\n    status_code: int\n        Status code of the algorithm run\n    \"\"\"\n    run_id: int\n    task_id: int\n    logs: str\n    data: str\n    status: str\n    parent_id: int | None\n\n\nclass ToBeKilled(NamedTuple):\n    \"\"\" Data class to store which tasks should be killed \"\"\"\n    task_id: int\n    run_id: int\n    organization_id: int\n\n\nclass KilledRun(NamedTuple):\n    \"\"\" Data class to store which algorithms have been killed \"\"\"\n    run_id: int\n    task_id: int\n    parent_id: int\n\n\nclass DockerManager(DockerBaseManager):\n    \"\"\"\n    Wrapper for the docker-py module.\n\n    This class manages tasks related to Docker, such as logging in to\n    docker registries, managing input/output files, logs etc. Results\n    can be retrieved through `get_result()` which returns the first available\n    algorithm result.\n    \"\"\"\n    log = logging.getLogger(logger_name(__name__))\n\n    def __init__(self, ctx: DockerNodeContext | NodeContext,\n                 isolated_network_mgr: NetworkManager,\n                 vpn_manager: VPNManager, tasks_dir: Path, client: NodeClient,\n                 proxy: Squid | None = None) -> None:\n        \"\"\" Initialization of DockerManager creates docker connection and\n            sets some default values.\n\n            Parameters\n            ----------\n            ctx: DockerNodeContext | NodeContext\n                Context object from which some settings are obtained\n            isolated_network_mgr: NetworkManager\n                Manager for the isolated network\n            vpn_manager: VPNManager\n                VPN Manager object\n            tasks_dir: Path\n                Directory in which this task's data are stored\n            client: NodeClient\n                Client object to communicate with the server\n            proxy: Squid | None\n                Squid proxy object\n        \"\"\"\n        self.log.debug(\"Initializing DockerManager\")\n        super().__init__(isolated_network_mgr)\n\n        self.data_volume_name = ctx.docker_volume_name\n        config = ctx.config\n        self.algorithm_env = config.get('algorithm_env', {})\n        self.vpn_manager = vpn_manager\n        self.client = client\n        self.__tasks_dir = tasks_dir\n        self.alpine_image = config.get('alpine')\n        self.proxy = proxy\n\n        # keep track of the running containers\n        self.active_tasks: list[DockerTaskManager] = []\n\n        # keep track of the containers that have failed to start\n        self.failed_tasks: list[DockerTaskManager] = []\n\n        # before a task is executed it gets exposed to these policies\n        self._policies = config.get(\"policies\", {})\n\n        # node name is used to identify algorithm containers belonging\n        # to this node. This is required as multiple nodes may run at\n        # a single machine sharing the docker daemon while using a\n        # different server. Using a different server means that there\n        # could be duplicate result_id's running at the node at the same\n        # time.\n        self.node_name = ctx.name\n\n        # name of the container that is running the node\n        self.node_container_name = ctx.docker_container_name\n\n        # login to the registries\n        docker_registries = ctx.config.get(\"docker_registries\", [])\n        self.login_to_registries(docker_registries)\n\n        # set database uri and whether or not it is a file\n        self._set_database(ctx.databases)\n\n        # keep track of linked docker services\n        self.linked_services: list[str] = []\n\n        # set algorithm device requests\n        self.algorithm_device_requests = []\n        if 'algorithm_device_requests' in config:\n            self._set_algorithm_device_requests(\n                config['algorithm_device_requests']\n            )\n\n    def _set_database(self, databases: dict | list) -> None:\n        \"\"\"\n        Set database location and whether or not it is a file\n\n        Parameters\n        ----------\n        databases: dict | list\n            databases as specified in the config file\n        \"\"\"\n        db_labels = [db['label'] for db in databases]\n\n        # If we're running in a docker container, database_uri would point\n        # to a path on the *host* (since it's been read from the config\n        # file). That's no good here. Therefore, we expect the CLI to set\n        # the environment variables for us. This has the added bonus that we\n        # can override the URI from the command line as well.\n        self.databases = {}\n        for label in db_labels:\n            label_upper = label.upper()\n            db_config = get_database_config(databases, label)\n            if running_in_docker():\n                uri = os.environ[f'{label_upper}_DATABASE_URI']\n            else:\n                uri = db_config['uri']\n\n            if running_in_docker():\n                db_is_file = Path(f'/mnt/{uri}').exists()\n                if db_is_file:\n                    uri = f'/mnt/{uri}'\n            else:\n                db_is_file = Path(uri).exists()\n\n            if db_is_file:\n                # We'll copy the file to the folder `data` in our task_dir.\n                self.log.info(f'Copying {uri} to {self.__tasks_dir}')\n                shutil.copy(uri, self.__tasks_dir)\n                uri = self.__tasks_dir / os.path.basename(uri)\n\n            self.databases[label] = {'uri': uri, 'is_file': db_is_file,\n                                     'type': db_config['type'],\n                                     'env': db_config.get('env', {})}\n        self.log.debug(f\"Databases: {self.databases}\")\n\n    def _set_algorithm_device_requests(self, device_requests_config: dict) \\\n            -> None:\n        \"\"\"\n        Configure device access for the algorithm container.\n\n        Parameters\n        ----------\n        device_requests_config: dict\n           A dictionary containing configuration options for device access.\n           Supported keys:\n           - 'gpu': A boolean value indicating whether GPU access is required.\n        \"\"\"\n        device_requests = []\n        if device_requests_config.get('gpu', False):\n            device = docker.types.DeviceRequest(count=-1,\n                                                capabilities=[['gpu']])\n            device_requests.append(device)\n        self.algorithm_device_requests = device_requests\n\n    def create_volume(self, volume_name: str) -> None:\n        \"\"\"\n        Create a temporary volume for a single run.\n\n        A single run can consist of multiple algorithm containers. It is\n        important to note that all algorithm containers having the same job_id\n        have access to this container.\n\n        Parameters\n        ----------\n        volume_name: str\n            Name of the volume to be created\n        \"\"\"\n        try:\n            self.docker.volumes.get(volume_name)\n            self.log.debug(f\"Volume {volume_name} already exists.\")\n\n        except docker.errors.NotFound:\n            self.log.debug(f\"Creating volume {volume_name}\")\n            self.docker.volumes.create(volume_name)\n\n    def is_docker_image_allowed(\n        self, docker_image_name: str, task_info: dict\n    ) -> bool:\n        \"\"\"\n        Checks the docker image name.\n\n        Against a list of regular expressions as defined in the configuration\n        file. If no expressions are defined, all docker images are accepted.\n\n        Parameters\n        ----------\n        docker_image_name: str\n            uri to the docker image\n        task_info: dict\n            Dictionary with information about the task\n\n        Returns\n        -------\n        bool\n            Whether docker image is allowed or not\n        \"\"\"\n        # check if algorithm matches any of the regex cases\n        allow_basics = self._policies.get('allow_basics_algorithm', True)\n        allowed_algorithms = self._policies.get('allowed_algorithms')\n        if docker_image_name.startswith(BASIC_PROCESSING_IMAGE):\n            if not allow_basics:\n                self.log.warn(\"A task was sent with a basics algorithm that \"\n                              \"this node does not allow to run.\")\n                return False\n            # else: basics are allowed, so we don't need to check the regex\n        elif allowed_algorithms:\n            if isinstance(allowed_algorithms, str):\n                allowed_algorithms = [allowed_algorithms]\n            found = False\n            for regex_expr in allowed_algorithms:\n                expr_ = re.compile(regex_expr)\n                if expr_.match(docker_image_name):\n                    found = True\n\n            if not found:\n                self.log.warn(\"A task was sent with a docker image that this\"\n                              \" node does not allow to run.\")\n                return False\n\n        # check if user or their organization is allowed\n        allowed_users = self._policies.get('allowed_users', [])\n        allowed_orgs = self._policies.get('allowed_organizations', [])\n        if allowed_users or allowed_orgs:\n            is_allowed = self.client.check_user_allowed_to_send_task(\n                allowed_users, allowed_orgs, task_info['init_org']['id'],\n                task_info['init_user']['id']\n            )\n            if not is_allowed:\n                self.log.warn(\"A task was sent by a user or organization that \"\n                              \"this node does not allow to start tasks.\")\n                return False\n\n        # if no limits are declared, log warning\n        if not self._policies:\n            self.log.warn(\"All docker images are allowed on this Node!\")\n\n        return True\n\n    def is_running(self, run_id: int) -> bool:\n        \"\"\"\n        Check if a container is already running for <run_id>.\n\n        Parameters\n        ----------\n        run_id: int\n            run_id of the algorithm container to be found\n\n        Returns\n        -------\n        bool\n            Whether or not algorithm container is running already\n        \"\"\"\n        running_containers = self.docker.containers.list(filters={\n            \"label\": [\n                f\"{APPNAME}-type=algorithm\",\n                f\"node={self.node_name}\",\n                f\"run_id={run_id}\"\n            ]\n        })\n        return bool(running_containers)\n\n    def cleanup_tasks(self) -> list[KilledRun]:\n        \"\"\"\n        Stop all active tasks\n\n        Returns\n        -------\n        list[KilledRun]:\n            List of information on tasks that have been killed\n        \"\"\"\n        run_ids_killed = []\n        if self.active_tasks:\n            self.log.debug(f'Killing {len(self.active_tasks)} active task(s)')\n        while self.active_tasks:\n            task = self.active_tasks.pop()\n            task.cleanup()\n            run_ids_killed.append(KilledRun(\n                run_id=task.run_id,\n                task_id=task.task_id,\n                parent_id=task.parent_id\n            ))\n        return run_ids_killed\n\n    def cleanup(self) -> None:\n        \"\"\"\n        Stop all active tasks and delete the isolated network\n\n        Note: the temporary docker volumes are kept as they may still be used\n        by a parent container\n        \"\"\"\n        # note: the function `cleanup_tasks` returns a list of tasks that were\n        # killed, but we don't register them as killed so they will be run\n        # again when the node is restarted\n        self.cleanup_tasks()\n        for service in self.linked_services:\n            self.isolated_network_mgr.disconnect(service)\n\n        # remove the node container from the network, it runs this code.. so\n        # it does not make sense to delete it just yet\n        self.isolated_network_mgr.disconnect(self.node_container_name)\n\n        # remove the connected containers and the network\n        self.isolated_network_mgr.delete(kill_containers=True)\n\n    def run(self, run_id: int, task_info: dict, image: str,\n            docker_input: bytes, tmp_vol_name: str, token: str,\n            databases_to_use: list[str]\n            ) -> tuple[TaskStatus, list[dict] | None]:\n        \"\"\"\n        Checks if docker task is running. If not, creates DockerTaskManager to\n        run the task\n\n        Parameters\n        ----------\n        run_id: int\n            Server run identifier\n        task_info: dict\n            Dictionary with task information\n        image: str\n            Docker image name\n        docker_input: bytes\n            Input that can be read by docker container\n        tmp_vol_name: str\n            Name of temporary docker volume assigned to the algorithm\n        token: str\n            Bearer token that the container can use\n        databases_to_use: list[str]\n            Labels of the databases to use\n\n        Returns\n        -------\n        TaskStatus, list[dict] | None\n            Returns a tuple with the status of the task and a description of\n            each port on the VPN client that forwards traffic to the algorithm\n            container (``None`` if VPN is not set up).\n        \"\"\"\n        # Verify that an allowed image is used\n        if not self.is_docker_image_allowed(image, task_info):\n            msg = f\"Docker image {image} is not allowed on this Node!\"\n            self.log.critical(msg)\n            return TaskStatus.NOT_ALLOWED,  None\n\n        # Check that this task is not already running\n        if self.is_running(run_id):\n            self.log.warn(\"Task is already being executed, discarding task\")\n            self.log.debug(f\"run_id={run_id} is discarded\")\n            return TaskStatus.ACTIVE, None\n\n        task = DockerTaskManager(\n            image=image,\n            run_id=run_id,\n            task_info=task_info,\n            vpn_manager=self.vpn_manager,\n            node_name=self.node_name,\n            tasks_dir=self.__tasks_dir,\n            isolated_network_mgr=self.isolated_network_mgr,\n            databases=self.databases,\n            docker_volume_name=self.data_volume_name,\n            alpine_image=self.alpine_image,\n            proxy=self.proxy,\n            device_requests=self.algorithm_device_requests\n        )\n\n        # attempt to kick of the task. If it fails do to unknown reasons we try\n        # again. If it fails permanently we add it to the failed tasks to be\n        # handled by the speaking worker of the node\n        attempts = 1\n        while not (task.status == TaskStatus.ACTIVE) and attempts < 3:\n            try:\n                vpn_ports = task.run(\n                    docker_input=docker_input, tmp_vol_name=tmp_vol_name,\n                    token=token, algorithm_env=self.algorithm_env,\n                    databases_to_use=databases_to_use\n                )\n\n            except UnknownAlgorithmStartFail:\n                self.log.exception(f'Failed to start run {run_id} for an '\n                                   'unknown reason. Retrying...')\n                time.sleep(1)  # add some time before retrying the next attempt\n\n            except PermanentAlgorithmStartFail:\n                break\n\n            attempts += 1\n\n        # keep track of the active container\n        if has_task_failed(task.status):\n            self.failed_tasks.append(task)\n            return task.status, None\n        else:\n            self.active_tasks.append(task)\n            return task.status, vpn_ports\n\n    def get_result(self) -> Result:\n        \"\"\"\n        Returns the oldest (FIFO) finished docker container.\n\n        This is a blocking method until a finished container shows up. Once the\n        container is obtained and the results are read, the container is\n        removed from the docker environment.\n\n        Returns\n        -------\n        Result\n            result of the docker image\n        \"\"\"\n\n        # get finished results and get the first one, if no result is available\n        # this is blocking\n        finished_tasks = []\n        while (not finished_tasks) and (not self.failed_tasks):\n            for task in self.active_tasks:\n\n                try:\n                    if task.is_finished():\n                        finished_tasks.append(task)\n                        self.active_tasks.remove(task)\n                        break\n                except AlgorithmContainerNotFound:\n                    self.log.exception('Failed to find container for '\n                                       'algorithm with run_id %s', task.run_id)\n                    self.failed_tasks.append(task)\n                    self.active_tasks.remove(task)\n                    break\n\n            # sleep for a second before checking again\n            time.sleep(1)\n\n        if finished_tasks:\n            # at least one task is finished\n\n            finished_task = finished_tasks.pop()\n            self.log.debug(f\"Run id={finished_task.run_id} is finished\")\n\n            # Check exit status and report\n            logs = finished_task.report_status()\n\n            # Cleanup containers\n            finished_task.cleanup()\n\n            # Retrieve results from file\n            results = finished_task.get_results()\n\n            # remove the VPN ports of this run from the database\n            self.client.request(\n                'port', params={'run_id': finished_task.run_id},\n                method=\"DELETE\"\n            )\n        else:\n            # at least one task failed to start\n            finished_task = self.failed_tasks.pop()\n            logs = 'Container failed. Check node logs for details'\n            results = b''\n\n        return Result(\n            run_id=finished_task.run_id,\n            task_id=finished_task.task_id,\n            logs=logs,\n            data=results,\n            status=finished_task.status,\n            parent_id=finished_task.parent_id,\n        )\n\n    def login_to_registries(self, registries: list = []) -> None:\n        \"\"\"\n        Login to the docker registries\n\n        Parameters\n        ----------\n        registries: list\n            list of registries to login to\n        \"\"\"\n        for registry in registries:\n            try:\n                self.docker.login(\n                    username=registry.get(\"username\"),\n                    password=registry.get(\"password\"),\n                    registry=registry.get(\"registry\")\n                )\n                self.log.info(f\"Logged in to {registry.get('registry')}\")\n            except docker.errors.APIError as e:\n                self.log.warn(f\"Could not login to {registry.get('registry')}\")\n                self.log.debug(e)\n\n    def link_container_to_network(self, container_name: str,\n                                  config_alias: str) -> None:\n        \"\"\"\n        Link a docker container to the isolated docker network\n\n        Parameters\n        ----------\n        container_name: str\n            Name of the docker container to be linked to the network\n        config_alias: str\n            Alias of the docker container defined in the config file\n        \"\"\"\n        container = get_container(\n            docker_client=self.docker, name=container_name\n        )\n        if not container:\n            self.log.error(f\"Could not link docker container {container_name} \"\n                           \"that was specified in the configuration file to \"\n                           \"the isolated docker network.\")\n            self.log.error(\"Container not found!\")\n            return\n        self.isolated_network_mgr.connect(\n            container_name=container_name,\n            aliases=[config_alias]\n        )\n        self.linked_services.append(container_name)\n\n    def kill_selected_tasks(\n        self, org_id: int, kill_list: list[ToBeKilled] = None\n    ) -> list[KilledRun]:\n        \"\"\"\n        Kill tasks specified by a kill list, if they are currently running on\n        this node\n\n        Parameters\n        ----------\n        org_id: int\n            The organization id of this node\n        kill_list: list[ToBeKilled]\n            A list of info about tasks that should be killed.\n\n        Returns\n        -------\n        list[KilledRun]\n            List with information on killed tasks\n        \"\"\"\n        killed_list = []\n        for container_to_kill in kill_list:\n            if container_to_kill['organization_id'] != org_id:\n                continue  # this run is on another node\n            # find the task\n            task = next((\n                t for t in self.active_tasks\n                if t.run_id == container_to_kill['run_id']\n            ), None)\n            if task:\n                self.log.info(\n                    f\"Killing containers for run_id={task.run_id}\")\n                self.active_tasks.remove(task)\n                task.cleanup()\n                killed_list.append(KilledRun(\n                    run_id=task.run_id,\n                    task_id=task.task_id,\n                    parent_id=task.parent_id,\n                ))\n            else:\n                self.log.warn(\n                    \"Received instruction to kill run_id=\"\n                    f\"{container_to_kill['run_id']}, but it was not \"\n                    \"found running on this node.\")\n        return killed_list\n\n    def kill_tasks(self, org_id: int,\n                   kill_list: list[ToBeKilled] = None) -> list[KilledRun]:\n        \"\"\"\n        Kill tasks currently running on this node.\n\n        Parameters\n        ----------\n        org_id: int\n            The organization id of this node\n        kill_list: list[ToBeKilled] (optional)\n            A list of info on tasks that should be killed. If the list\n            is not specified, all running algorithm containers will be killed.\n\n        Returns\n        -------\n        list[KilledRun]\n            List of dictionaries with information on killed tasks\n        \"\"\"\n        if kill_list:\n            killed_runs = self.kill_selected_tasks(org_id=org_id,\n                                                   kill_list=kill_list)\n        else:\n            # received instruction to kill all tasks on this node\n            self.log.warn(\n                \"Received instruction from server to kill all algorithms \"\n                \"running on this node. Executing that now...\")\n            killed_runs = self.cleanup_tasks()\n            if len(killed_runs):\n                self.log.warn(\n                    \"Killed the following run ids as instructed via socket:\"\n                    f\" {', '.join([str(r.run_id) for r in killed_runs])}\"\n                )\n            else:\n                self.log.warn(\n                    \"Instructed to kill tasks but none were running\"\n                )\n        return killed_runs\n\n    def get_column_names(self, label: str, type_: str) -> list[str]:\n        \"\"\"\n        Get column names from a node database\n\n        Parameters\n        ----------\n        label: str\n            Label of the database\n        type_: str\n            Type of the database\n\n        Returns\n        -------\n        list[str]\n            List of column names\n        \"\"\"\n        db = self.databases.get(label)\n        if not db:\n            self.log.error(\"Database with label %s not found\", label)\n            return []\n        if not db['is_file']:\n            self.log.error(\"Database with label %s is not a file. Cannot\"\n                           \" determine columns without query\", label)\n            return []\n        if db['type'] == 'excel':\n            self.log.error(\"Cannot determine columns for excel database \"\n                           \" without a worksheet\")\n            return []\n        if type_ not in ('csv', 'sparql'):\n            self.log.error(\"Cannot determine columns for database of type %s.\"\n                           \"Only csv and sparql are supported\", type_)\n            return []\n        return get_column_names(db['uri'], type_)\n", "# TODO the task folder is also created by this class. This folder needs\n# to be cleaned at some point.\nimport logging\nimport os\nimport docker.errors\nimport json\nimport base64\n\nfrom pathlib import Path\n\nfrom vantage6.common.globals import (\n    APPNAME, ENV_VAR_EQUALS_REPLACEMENT, STRING_ENCODING\n)\nfrom vantage6.common.docker.addons import (\n    remove_container_if_exists, remove_container, pull_if_newer,\n    running_in_docker\n)\nfrom vantage6.common.docker.network_manager import NetworkManager\nfrom vantage6.common.task_status import TaskStatus\nfrom vantage6.node.util import get_parent_id\nfrom vantage6.node.globals import ALPINE_IMAGE, ENV_VARS_NOT_SETTABLE_BY_NODE\nfrom vantage6.node.docker.vpn_manager import VPNManager\nfrom vantage6.node.docker.squid import Squid\nfrom vantage6.node.docker.docker_base import DockerBaseManager\nfrom vantage6.node.docker.exceptions import (\n    UnknownAlgorithmStartFail,\n    PermanentAlgorithmStartFail,\n    AlgorithmContainerNotFound\n)\n\n\nclass DockerTaskManager(DockerBaseManager):\n    \"\"\"\n    Manager for running a vantage6 algorithm container within docker.\n\n    Ensures that the environment is properly set up (docker volumes,\n    directories, environment variables, etc). Then runs the algorithm as a\n    docker container. Finally, it monitors the container state and can return\n    it's results when the algorithm finished.\n    \"\"\"\n\n    def __init__(self, image: str, vpn_manager: VPNManager, node_name: str,\n                 run_id: int, task_info: dict, tasks_dir: Path,\n                 isolated_network_mgr: NetworkManager,\n                 databases: dict, docker_volume_name: str,\n                 alpine_image: str | None = None, proxy: Squid | None = None,\n                 device_requests: list | None = None):\n        \"\"\"\n        Initialization creates DockerTaskManager instance\n\n        Parameters\n        ----------\n        image: str\n            Name of docker image to be run\n        vpn_manager: VPNManager\n            VPN manager required to set up traffic forwarding via VPN\n        node_name: str\n            Name of the node, to track running algorithms\n        run_id: int\n            Algorithm run identifier\n        task_info: dict\n            Dictionary with info about the task\n        tasks_dir: Path\n            Directory in which this task's data are stored\n        isolated_network_mgr: NetworkManager\n            Manager of isolated network to which algorithm needs to connect\n        databases: dict\n            List of databases\n        docker_volume_name: str\n            Name of the docker volume\n        alpine_image: str | None\n            Name of alternative Alpine image to be used\n        device_requests: list | None\n            List of DeviceRequest objects to be passed to the algorithm\n            container\n        \"\"\"\n        self.task_id = task_info['id']\n        self.log = logging.getLogger(f\"task ({self.task_id})\")\n\n        super().__init__(isolated_network_mgr)\n        self.image = image\n        self.__vpn_manager = vpn_manager\n        self.run_id = run_id\n        self.task_id = task_info['id']\n        self.parent_id = get_parent_id(task_info)\n        self.__tasks_dir = tasks_dir\n        self.databases = databases\n        self.data_volume_name = docker_volume_name\n        self.node_name = node_name\n        self.alpine_image = ALPINE_IMAGE if alpine_image is None \\\n            else alpine_image\n        self.proxy = proxy\n\n        self.container = None\n        self.status_code = None\n        self.docker_input = None\n\n        self.labels = {\n            f\"{APPNAME}-type\": \"algorithm\",\n            \"node\": node_name,\n            \"run_id\": str(run_id)\n        }\n        self.helper_labels = self.labels.copy()\n        self.helper_labels[f\"{APPNAME}-type\"] = \"algorithm-helper\"\n\n        # FIXME: these values should be retrieved from DockerNodeContext\n        #   in some way.\n        self.tmp_folder = \"/mnt/tmp\"\n        self.data_folder = \"/mnt/data\"\n\n        # keep track of the task status\n        self.status: TaskStatus = TaskStatus.INITIALIZING\n\n        # set device requests\n        self.device_requests = []\n        if device_requests:\n            self.device_requests = device_requests\n\n    def is_finished(self) -> bool:\n        \"\"\"\n        Checks if algorithm container is finished\n\n        Returns\n        -------\n        bool:\n            True if algorithm container is finished\n        \"\"\"\n        try:\n            self.container.reload()\n        except (docker.errors.NotFound, AttributeError):\n            self.log.error(\"Container not found\")\n            self.log.debug(f\"- task id: {self.task_id}\")\n            self.log.debug(f\"- result id: {self.task_id}\")\n            self.status = TaskStatus.UNKNOWN_ERROR\n            raise AlgorithmContainerNotFound\n\n        return self.container.status == 'exited'\n\n    def report_status(self) -> str:\n        \"\"\"\n        Checks if algorithm has exited successfully. If not, it prints an\n        error message\n\n        Returns\n        -------\n        logs: str\n            Log messages of the algorithm container\n        \"\"\"\n        logs = self.container.logs().decode('utf8')\n\n        # report if the container has a different status than 0\n        self.status_code = self.container.attrs[\"State\"][\"ExitCode\"]\n        if self.status_code:\n            self.log.error(f\"Received non-zero exitcode: {self.status_code}\")\n            self.log.error(f\"  Container id: {self.container.id}\")\n            self.log.info(logs)\n            self.status = TaskStatus.CRASHED\n        else:\n            self.status = TaskStatus.COMPLETED\n        return logs\n\n    def get_results(self) -> bytes:\n        \"\"\"\n        Read results output file of the algorithm container\n\n        Returns\n        -------\n        bytes:\n            Results of the algorithm container\n        \"\"\"\n        with open(self.output_file, \"rb\") as fp:\n            results = fp.read()\n        return results\n\n    def pull(self):\n        \"\"\" Pull the latest docker image. \"\"\"\n        try:\n            self.log.info(f\"Retrieving latest image: '{self.image}'\")\n            pull_if_newer(self.docker, self.image, self.log)\n\n        except docker.errors.APIError as e:\n            self.log.debug('Failed to pull image: could not find image')\n            self.log.exception(e)\n            self.status = TaskStatus.NO_DOCKER_IMAGE\n            raise PermanentAlgorithmStartFail\n        except Exception as e:\n            self.log.debug('Failed to pull image')\n            self.log.exception(e)\n            self.status = TaskStatus.FAILED\n            raise PermanentAlgorithmStartFail\n\n    def run(\n        self, docker_input: bytes, tmp_vol_name: str, token: str,\n        algorithm_env: dict, databases_to_use: list[str]\n    ) -> list[dict] | None:\n        \"\"\"\n        Runs the docker-image in detached mode.\n\n        It will will attach all mounts (input, output and datafile) to the\n        docker image. And will supply some environment variables.\n\n        Parameters\n        ----------\n        docker_input: bytes\n            Input that can be read by docker container\n        tmp_vol_name: str\n            Name of temporary docker volume assigned to the algorithm\n        token: str\n            Bearer token that the container can use\n        algorithm_env: dict\n            Dictionary with additional environment variables to set\n        databases_to_use: list[str]\n            List of labels of databases to use in the task\n\n        Returns\n        -------\n        list[dict] | None\n            Description of each port on the VPN client that forwards traffic to\n            the algo container. None if VPN is not set up.\n        \"\"\"\n        # generate task folders\n        self._make_task_folders()\n\n        # prepare volumes\n        self.docker_input = docker_input\n        self.volumes = self._prepare_volumes(tmp_vol_name, token)\n        self.log.debug(f\"volumes: {self.volumes}\")\n\n        # setup environment variables\n        self.environment_variables = \\\n            self._setup_environment_vars(algorithm_env=algorithm_env,\n                                         databases_to_use=databases_to_use)\n\n        # run the algorithm as docker container\n        vpn_ports = self._run_algorithm()\n        return vpn_ports\n\n    def cleanup(self) -> None:\n        \"\"\"Cleanup the containers generated for this task\"\"\"\n        remove_container(self.helper_container, kill=True)\n        remove_container(self.container, kill=True)\n\n    def _run_algorithm(self) -> list[dict] | None:\n        \"\"\"\n        Run the algorithm container\n\n        Start up a helper container to complete VPN setup, pull the latest\n        image and then run the algorithm\n\n        Returns\n        -------\n        list[dict] or None\n            Description of each port on the VPN client that forwards traffic to\n            the algo container. None if VPN is inactive\n        \"\"\"\n        vpn_ports = None\n        container_name = f'{APPNAME}-{self.node_name}-run-{self.run_id}'\n        helper_container_name = container_name + '-helper'\n\n        # Try to pull the latest image\n        self.pull()\n\n        # remove algorithm containers if they were already running\n        self.log.debug(\"Check if algorithm container is already running\")\n        remove_container_if_exists(\n            docker_client=self.docker, name=container_name\n        )\n        remove_container_if_exists(\n            docker_client=self.docker, name=helper_container_name\n        )\n\n        if self.__vpn_manager:\n            # if VPN is active, network exceptions must be configured\n            # First, start a container that runs indefinitely. The algorithm\n            # container will run in the same network and network exceptions\n            # will therefore also affect the algorithm.\n            self.log.debug(\"Start helper container to setup VPN network\")\n            self.helper_container = self.docker.containers.run(\n                command='sleep infinity',\n                image=self.alpine_image,\n                labels=self.helper_labels,\n                network=self.isolated_network_mgr.network_name,\n                name=helper_container_name,\n                detach=True\n            )\n            # setup forwarding of traffic via VPN client to and from the\n            # algorithm container:\n            self.log.debug(\"Setup port forwarder\")\n            vpn_ports = self.__vpn_manager.forward_vpn_traffic(\n                helper_container=self.helper_container,\n                algo_image_name=self.image\n            )\n\n        # try reading docker input\n        # FIXME BvB 2023-02-03: why do we read docker input here? It is never\n        # really used below. Should it?\n        deserialized_input = None\n        if self.docker_input:\n            self.log.debug(\"Deserialize input\")\n            try:\n                deserialized_input = json.loads(self.docker_input)\n            except Exception:\n                pass\n\n        # attempt to run the image\n        try:\n            if deserialized_input:\n                self.log.info(f\"Run docker image {self.image} with input \"\n                              f\"{self._printable_input(deserialized_input)}\")\n            else:\n                self.log.info(f\"Run docker image {self.image}\")\n            self.container = self.docker.containers.run(\n                self.image,\n                detach=True,\n                environment=self.environment_variables,\n                network='container:' + self.helper_container.id,\n                volumes=self.volumes,\n                name=container_name,\n                labels=self.labels,\n                device_requests=self.device_requests\n            )\n\n        except Exception as e:\n            self.status = TaskStatus.START_FAILED\n            raise UnknownAlgorithmStartFail(e)\n\n        self.status = TaskStatus.ACTIVE\n        return vpn_ports\n\n    @staticmethod\n    def _printable_input(input_: str | dict) -> str:\n        \"\"\"\n        Return a version of the input with limited number of characters\n\n        Parameters\n        ----------\n        input: str | dict\n            Deserialized input of a task\n\n        Returns\n        -------\n        str\n            Input with limited number of characters, to be printed to logs\n        \"\"\"\n        if isinstance(input_, dict):\n            input_ = str(input_)\n        if len(input_) > 550:\n            return f'{input_[:500]}... ({len(input_)-500} characters omitted)'\n        return input_\n\n    def _make_task_folders(self) -> None:\n        \"\"\" Generate task folders \"\"\"\n        # FIXME: We should have a separate mount/volume for this. At the\n        #   moment this is a potential leak as containers might access input,\n        #   output and token from other containers.\n        #\n        #   This was not possible yet as mounting volumes from containers\n        #   is terrible when working from windows (as you have to convert\n        #   from windows to unix several times...).\n\n        # If we're running in docker __tasks_dir will point to a location on\n        # the data volume.\n        # Alternatively, if we're not running in docker it should point to the\n        # folder on the host that can act like a data volume. In both cases,\n        # we can just copy the required files to it\n        self.task_folder_name = f\"task-{self.run_id:09d}\"\n        self.task_folder_path = \\\n            os.path.join(self.__tasks_dir, self.task_folder_name)\n        os.makedirs(self.task_folder_path, exist_ok=True)\n        self.output_file = os.path.join(self.task_folder_path, \"output\")\n\n    def _prepare_volumes(self, tmp_vol_name: str, token: str) -> dict:\n        \"\"\"\n        Generate docker volumes required to run the algorithm\n\n        Parameters\n        ----------\n        tmp_vol_name: str\n            Name of temporary docker volume assigned to the algorithm\n        token: str\n            Bearer token that the container can use\n\n        Returns\n        -------\n        dict:\n            Volumes to support running the algorithm\n        \"\"\"\n        if isinstance(self.docker_input, str):\n            self.docker_input = self.docker_input.encode('utf8')\n\n        # Create I/O files & token for the algorithm container\n        self.log.debug(\"prepare IO files in docker volume\")\n        io_files = [\n            ('input', self.docker_input),\n            ('output', b''),\n            ('token', token.encode(\"ascii\")),\n        ]\n\n        for (filename, data) in io_files:\n            filepath = os.path.join(self.task_folder_path, filename)\n\n            with open(filepath, 'wb') as fp:\n                fp.write(data)\n\n        volumes = {\n            tmp_vol_name: {\"bind\": self.tmp_folder, \"mode\": \"rw\"},\n        }\n\n        if running_in_docker():\n            volumes[self.data_volume_name] = \\\n                {\"bind\": self.data_folder, \"mode\": \"rw\"}\n        else:\n            volumes[self.__tasks_dir] = \\\n                {\"bind\": self.data_folder, \"mode\": \"rw\"}\n        return volumes\n\n    def _setup_environment_vars(self, algorithm_env: dict,\n                                databases_to_use: list[str]) -> dict:\n        \"\"\"\"\n        Set environment variables required to run the algorithm\n\n        Parameters\n        ----------\n        algorithm_env: dict\n            Dictionary with additional environment variables to set\n        databases_to_use: list[str]\n            Labels of the databases to use\n\n        Returns\n        -------\n        dict:\n            Environment variables required to run algorithm\n        \"\"\"\n        try:\n            proxy_host = os.environ['PROXY_SERVER_HOST']\n\n        except Exception:\n            self.log.warn(\"PROXY_SERVER_HOST not set, using \"\n                          \"host.docker.internal\")\n            self.log.debug(os.environ)\n            proxy_host = 'host.docker.internal'\n\n        # define environment variables for the docker-container, the\n        # host, port and api_path are from the local proxy server to\n        # facilitate indirect communication with the central server\n        # FIXME: we should only prepend data_folder if database_uri is a\n        #   filename\n        environment_variables = {\n            \"INPUT_FILE\": f\"{self.data_folder}/{self.task_folder_name}/input\",\n            \"OUTPUT_FILE\":\n                f\"{self.data_folder}/{self.task_folder_name}/output\",\n            \"TOKEN_FILE\": f\"{self.data_folder}/{self.task_folder_name}/token\",\n            \"TEMPORARY_FOLDER\": self.tmp_folder,\n            \"HOST\": f\"http://{proxy_host}\",\n            \"PORT\": os.environ.get(\"PROXY_SERVER_PORT\", 8080),\n            \"API_PATH\": \"\",\n        }\n\n        # Add squid proxy environment variables\n        if self.proxy:\n            # applications/libraries in the algorithm container need to adhere\n            # to the proxy settings. Because we are not sure which application\n            # is used for the request we both set HTTP_PROXY and http_proxy and\n            # HTTPS_PROXY and https_proxy for the secure connection.\n            environment_variables[\"HTTP_PROXY\"] = self.proxy.address\n            environment_variables[\"http_proxy\"] = self.proxy.address\n            environment_variables[\"HTTPS_PROXY\"] = self.proxy.address\n            environment_variables[\"https_proxy\"] = self.proxy.address\n\n            no_proxy = []\n            if self.__vpn_manager:\n                # Computing all ips in the vpn network is not feasible as the\n                # no_proxy environment variable will be too long for the\n                # container to start. So we only add the net + mask. For some\n                # applications and libraries this is format is ignored.\n                no_proxy.append(self.__vpn_manager.subnet)\n            no_proxy.append(\"localhost\")\n            no_proxy.append(proxy_host)\n\n            # Add the NO_PROXY and no_proxy environment variable.\n            environment_variables[\"NO_PROXY\"] = ', '.join(no_proxy)\n            environment_variables[\"no_proxy\"] = ', '.join(no_proxy)\n\n        for database in databases_to_use:\n            if database['label'] not in self.databases:\n                # In this case the algorithm might crash if it tries to access\n                # the DATABASE_LABEL environment variable\n                self.log.warning(\"A user specified a database that does not \"\n                                 \"exist. Available databases are: \"\n                                 f\"{self.databases.keys()}. This is likely to \"\n                                 \"result in an algorithm crash.\")\n                self.log.debug(f\"User specified database: {database}\")\n            # define env vars for the preprocessing and extra parameters such\n            # as query and sheet_name\n            extra_params = json.loads(database.get(\"parameters\")) \\\n                if database.get(\"parameters\") else {}\n            for optional_key in ['query', 'sheet_name', 'preprocessing']:\n                if optional_key in extra_params:\n                    env_var_value = extra_params[optional_key] \\\n                        if optional_key != 'preprocessing' \\\n                        else json.dumps(extra_params[optional_key])\n                    environment_variables[f\"{database['label'].upper()}_\"\n                                          f\"{optional_key.upper()}\"] = \\\n                        env_var_value\n\n        environment_variables[\"USER_REQUESTED_DATABASE_LABELS\"] = \\\n            \",\".join([db['label'] for db in databases_to_use])\n\n        # Only prepend the data_folder is it is a file-based database\n        # This allows algorithms to access multiple data sources at the\n        # same time\n        db_labels = []\n        for label in self.databases:\n            db = self.databases[label]\n\n            uri_var_name = f'{label.upper()}_DATABASE_URI'\n            environment_variables[uri_var_name] = \\\n                f\"{self.data_folder}/{os.path.basename(db['uri'])}\" \\\n                if db['is_file'] else db['uri']\n\n            type_var_name = f'{label.upper()}_DATABASE_TYPE'\n            environment_variables[type_var_name] = db['type']\n\n            # Add optional database parameter settings, these can be used by\n            # the algorithm (wrapper). Note that all env keys are prefixed\n            # with DB_PARAM_ to avoid collisions with other environment\n            # variables.\n            if 'env' in db:\n                for key in db['env']:\n                    env_key = f'{label.upper()}_DB_PARAM_{key.upper()}'\n                    environment_variables[env_key] = db['env'][key]\n\n            db_labels.append(label)\n        environment_variables['DB_LABELS'] = ','.join(db_labels)\n\n\n        # Load additional environment variables\n        if algorithm_env:\n            environment_variables = \\\n                {**environment_variables, **algorithm_env}\n            self.log.info('Custom environment variables are loaded!')\n            self.log.debug(f\"custom environment: {algorithm_env}\")\n\n        # validate whether environment variables don't contain any illegal\n        # characters\n        self._validate_environment_variables(environment_variables)\n\n        # print the environment before encoding it so that the user can see\n        # what is passed to the container\n        self.log.debug(f\"environment: {environment_variables}\")\n\n        # encode environment variables to prevent special characters from being\n        # possibly code injection\n        environment_variables = self._encode_environment_variables(\n            environment_variables)\n\n        return environment_variables\n\n    def _validate_environment_variables(self,\n                                        environment_variables: dict) -> None:\n        \"\"\"\n        Check whether environment variables don't contain any illegal\n        characters\n\n        Parameters\n        ----------\n        environment_variables: dict\n            Environment variables required to run algorithm\n\n        Raises\n        ------\n        PermanentAlgorithmStartFail\n            If environment variables contain illegal characters\n        \"\"\"\n        msg = None\n        for key in environment_variables:\n            if not key.isidentifier():\n                msg = (\n                    f\"Environment variable '{key}' is invalid: environment \"\n                    \" variable names should only contain number, letters and \"\n                    \" underscores, and start with a letter.\"\n                )\n            elif key in ENV_VARS_NOT_SETTABLE_BY_NODE:\n                msg = (\n                    f\"Environment variable '{key}' cannot be set: this \"\n                    \"variable is set in the algorithm Dockerfile and cannot \"\n                    \"be overwritten.\"\n                )\n            if msg:\n                self.status = TaskStatus.FAILED\n                self.log.error(msg)\n                raise PermanentAlgorithmStartFail(msg)\n\n    def _encode_environment_variables(self, environment_variables: dict) \\\n            -> dict:\n        \"\"\"\n        Encode environment variable values to ensure that special characters\n        are not interpretable as code while transferring them to the algorithm\n        container.\n\n        Parameters\n        ----------\n        environment_variables: dict\n            Environment variables required to run algorithm\n\n        Returns\n        -------\n        dict:\n            Environment variables with encoded values\n        \"\"\"\n        def _encode(string: str) -> str:\n            \"\"\" Encode env var value\n\n            We first encode to bytes, then to b32 and then decode to a string.\n            Finally, '=' is replaced by less sensitve characters to prevent\n            issues with interpreting the encoded string in the env var value.\n\n            Parameters\n            ----------\n            string: str\n                String to be encoded\n\n            Returns\n            -------\n            str:\n                Encoded string\n\n            Examples\n            --------\n            >>> _encode(\"abc\")\n            'MFRGG!!!'\n            \"\"\"\n            return base64.b32encode(\n                string.encode(STRING_ENCODING)\n            ).decode(STRING_ENCODING).replace('=', ENV_VAR_EQUALS_REPLACEMENT)\n\n        self.log.debug(\"Encoding environment variables\")\n\n        encoded_environment_variables = {}\n        for key, val in environment_variables.items():\n            encoded_environment_variables[key] = _encode(val)\n        return encoded_environment_variables\n", "from pathlib import Path\nfrom vantage6.common.globals import APPNAME\n\n#\n#   NODE SETTINGS\n#\nDEFAULT_NODE_SYSTEM_FOLDERS = False\n\n#\n#   INSTALLATION SETTINGS\n#\nPACKAGE_FOLDER = Path(__file__).parent.parent.parent\n\nNODE_PROXY_SERVER_HOSTNAME = \"proxyserver\"\n\nDATA_FOLDER = PACKAGE_FOLDER / APPNAME / \"_data\"\n\n# with open(Path(PACKAGE_FOLDER) / APPNAME / \"node\" / \"VERSION\") as f:\n#     VERSION = f.read()\n\n\n# constants for retrying node login\nSLEEP_BTWN_NODE_LOGIN_TRIES = 10  # retry every 10s\nTIME_LIMIT_RETRY_CONNECT_NODE = 60 * 60 * 24 * 7  # i.e. 1 week\n\n# constant for waiting for the initial websocket connection\nTIME_LIMIT_INITIAL_CONNECTION_WEBSOCKET = 60\n\n#\n#    VPN CONFIGURATION RELATED CONSTANTS\n#\n# TODO move part of these constants elsewhere?! Or make context?\nVPN_CLIENT_IMAGE = 'harbor2.vantage6.ai/infrastructure/vpn-client'\nNETWORK_CONFIG_IMAGE = 'harbor2.vantage6.ai/infrastructure/vpn-configurator'\nALPINE_IMAGE = 'harbor2.vantage6.ai/infrastructure/alpine'\nMAX_CHECK_VPN_ATTEMPTS = 60   # max attempts to obtain VPN IP (1 second apart)\nFREE_PORT_RANGE = range(49152, 65535)\nDEFAULT_ALGO_VPN_PORT = '8888'  # default VPN port for algorithm container\n\n#\n#   SSH TUNNEL RELATED CONSTANTS\n#\nSSH_TUNNEL_IMAGE = \"harbor2.vantage6.ai/infrastructure/ssh-tunnel\"\n\n#\n#   SQUID RELATED CONSTANTS\n#\nSQUID_IMAGE = \"harbor2.vantage6.ai/infrastructure/squid\"\n\n# Environment variables that should be set in the Dockerfile and that may not\n# be overwritten by the user.\nENV_VARS_NOT_SETTABLE_BY_NODE = [\"PKG_NAME\"]\n", "import sys\nfrom pathlib import Path\nfrom threading import Thread\nimport time\nimport os.path\n\nimport click\nimport questionary as q\nimport docker\n\nfrom colorama import Fore, Style\n\nfrom vantage6.common import warning, error, info, debug, get_database_config\nfrom vantage6.common.globals import (\n    APPNAME,\n    DEFAULT_DOCKER_REGISTRY,\n    DEFAULT_NODE_IMAGE,\n    DEFAULT_NODE_IMAGE_WO_TAG,\n)\nfrom vantage6.common.docker.addons import (\n  pull_if_newer,\n  remove_container_if_exists,\n  check_docker_running\n)\n\nfrom vantage6.cli.context import NodeContext\nfrom vantage6.cli.globals import (\n    DEFAULT_NODE_SYSTEM_FOLDERS as N_FOL\n)\nfrom vantage6.cli.configuration_wizard import (\n    configuration_wizard,\n    select_configuration_questionaire,\n)\nfrom vantage6.cli.utils import check_config_name_allowed\nfrom vantage6.cli import __version__\nfrom vantage6.cli.node.common import print_log_worker, create_client\n\n\n@click.command()\n@click.option(\"-n\", \"--name\", default=None, help=\"Configuration name\")\n@click.option(\"-c\", \"--config\", default=None,\n              help='Absolute path to configuration-file; overrides NAME')\n@click.option('--system', 'system_folders', flag_value=True,\n              help=\"Search for the configuration in the system folders\")\n@click.option('--user', 'system_folders', flag_value=False, default=N_FOL,\n              help=\"Search for the configuration in the user folders. This is \"\n                   \"the default\")\n@click.option('-i', '--image', default=None, help=\"Node Docker image to use\")\n@click.option('--keep/--auto-remove', default=False,\n              help=\"Keep node container after finishing. Useful for debugging\")\n@click.option('--force-db-mount', is_flag=True,\n              help=\"Always mount node databases; skip the check if they are \"\n                   \"existing files.\")\n@click.option('--attach/--detach', default=False,\n              help=\"Show node logs on the current console after starting the \"\n                   \"node\")\n@click.option('--mount-src', default='',\n              help=\"Override vantage6 source code in container with the source\"\n                   \" code in this path\")\ndef cli_node_start(name: str, config: str, system_folders: bool, image: str,\n                   keep: bool, mount_src: str, attach: bool,\n                   force_db_mount: bool) -> None:\n    \"\"\"\n    Start the node.\n    \"\"\"\n    check_docker_running()\n    info(\"Starting node...\")\n    info(\"Finding Docker daemon\")\n    docker_client = docker.from_env()\n    NodeContext.LOGGING_ENABLED = False\n    if config:\n        name = Path(config).stem\n        ctx = NodeContext(name, system_folders, config)\n\n    else:\n        # in case no name is supplied, ask the user to select one\n        if not name:\n            name = select_configuration_questionaire(\"node\", system_folders)\n\n        # check that config exists, if not a questionaire will be invoked\n        if not NodeContext.config_exists(name, system_folders):\n            warning(f\"Configuration {Fore.RED}{name}{Style.RESET_ALL} does not\"\n                    \" exist.\")\n\n            if q.confirm(\"Create this configuration now?\").ask():\n                configuration_wizard(\"node\", name, system_folders)\n\n            else:\n                error(\"Config file couldn't be loaded\")\n                sys.exit(0)\n\n        ctx = NodeContext(name, system_folders)\n\n    # check if config name is allowed docker name, else exit\n    check_config_name_allowed(ctx.name)\n\n    # check that this node is not already running\n    running_nodes = docker_client.containers.list(\n        filters={\"label\": f\"{APPNAME}-type=node\"}\n    )\n\n    suffix = \"system\" if system_folders else \"user\"\n    for node in running_nodes:\n        if node.name == f\"{APPNAME}-{name}-{suffix}\":\n            error(f\"Node {Fore.RED}{name}{Style.RESET_ALL} is already running\")\n            exit(1)\n\n    # make sure the (host)-task and -log dir exists\n    info(\"Checking that data and log dirs exist\")\n    ctx.data_dir.mkdir(parents=True, exist_ok=True)\n    ctx.log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Determine image-name. First we check if the option --image has been used.\n    # Then we check if the image has been specified in the config file, and\n    # finally we use the default settings from the package.\n    if not image:\n        custom_images: dict = ctx.config.get('images')\n        if custom_images:\n            image = custom_images.get(\"node\")\n        else:\n            # if no custom image is specified, find the server version and use\n            # the latest images from that minor version\n            client = create_client(ctx)\n            major_minor = None\n            try:\n                # try to get server version, skip if can't get a connection\n                version = client.util.get_server_version(\n                    attempts_on_timeout=3\n                )['version']\n                major_minor = '.'.join(version.split('.')[:2])\n                image = (f\"{DEFAULT_DOCKER_REGISTRY}/\"\n                         f\"{DEFAULT_NODE_IMAGE_WO_TAG}\"\n                         f\":{major_minor}\")\n            except Exception:\n                warning(\"Could not determine server version. Using default \"\n                        \"node image\")\n\n            if major_minor and not __version__.startswith(major_minor):\n                warning(\n                    \"Version mismatch between CLI and server/node. CLI is \"\n                    f\"running on version {__version__}, while node and server \"\n                    f\"are on version {major_minor}. This might cause \"\n                    f\"unexpected issues; changing to {major_minor}.<latest> \"\n                    \"is recommended.\"\n                )\n\n        # fail safe, in case no custom image is specified and we can't get the\n        # server version\n        if not image:\n            image = f\"{DEFAULT_DOCKER_REGISTRY}/{DEFAULT_NODE_IMAGE}\"\n\n    info(f\"Pulling latest node image '{image}'\")\n    try:\n        # docker_client.images.pull(image)\n        pull_if_newer(docker.from_env(), image)\n\n    except Exception as e:\n        warning(' ... Getting latest node image failed:')\n        warning(f\"     {e}\")\n    else:\n        info(\" ... success!\")\n\n    info(\"Creating Docker data volume\")\n\n    data_volume = docker_client.volumes.create(ctx.docker_volume_name)\n    vpn_volume = docker_client.volumes.create(ctx.docker_vpn_volume_name)\n    ssh_volume = docker_client.volumes.create(ctx.docker_ssh_volume_name)\n    squid_volume = docker_client.volumes.create(ctx.docker_squid_volume_name)\n\n    info(\"Creating file & folder mounts\")\n    # FIXME: should obtain mount points from DockerNodeContext\n    mounts = [\n        # (target, source)\n        (f\"/mnt/log/{name}\", str(ctx.log_dir)),\n        (\"/mnt/data\", data_volume.name),\n        (\"/mnt/vpn\", vpn_volume.name),\n        (\"/mnt/ssh\", ssh_volume.name),\n        (\"/mnt/squid\", squid_volume.name),\n        (\"/mnt/config\", str(ctx.config_dir)),\n        (\"/var/run/docker.sock\", \"/var/run/docker.sock\"),\n    ]\n\n    if mount_src:\n        # If mount_src is a relative path, docker will consider it a volume.\n        mount_src = os.path.abspath(mount_src)\n        mounts.append(('/vantage6', mount_src))\n\n    # FIXME: Code duplication: Node.__init__() (vantage6/node/__init__.py)\n    #   uses a lot of the same logic. Suggest moving this to\n    #   ctx.get_private_key()\n    filename = ctx.config.get(\"encryption\", {}).get(\"private_key\")\n    # filename may be set to an empty string\n    if not filename:\n        filename = 'private_key.pem'\n\n    # Location may be overridden by the environment\n    filename = os.environ.get('PRIVATE_KEY', filename)\n\n    # If ctx.get_data_file() receives an absolute path, it is returned as-is\n    fullpath = Path(ctx.get_data_file(filename))\n    if fullpath:\n        if Path(fullpath).exists():\n            mounts.append((\"/mnt/private_key.pem\", str(fullpath)))\n        else:\n            warning(f\"private key file provided {fullpath}, \"\n                    \"but does not exists\")\n\n    # Mount private keys for ssh tunnels\n    ssh_tunnels = ctx.config.get(\"ssh-tunnels\", [])\n    for ssh_tunnel in ssh_tunnels:\n        hostname = ssh_tunnel.get(\"hostname\")\n        key_path = ssh_tunnel.get(\"ssh\", {}).get(\"identity\", {}).get(\"key\")\n        if not key_path:\n            error(f\"SSH tunnel identity {Fore.RED}{hostname}{Style.RESET_ALL} \"\n                  \"key not provided. Continuing to start without this tunnel.\")\n        key_path = Path(key_path)\n        if not key_path.exists():\n            error(f\"SSH tunnel identity {Fore.RED}{hostname}{Style.RESET_ALL} \"\n                  \"key does not exist. Continuing to start without this \"\n                  \"tunnel.\")\n\n        info(f\"  Mounting private key for {hostname} at {key_path}\")\n\n        # we remove the .tmp in the container, this is because the file is\n        # mounted in a volume mount point. Somehow the file is than empty in\n        # the volume but not for the node instance. By removing the .tmp we\n        # make sure that the file is not empty in the volume.\n        mounts.append((f\"/mnt/ssh/{hostname}.pem.tmp\", str(key_path)))\n\n    env = {\n        \"DATA_VOLUME_NAME\": data_volume.name,\n        \"VPN_VOLUME_NAME\": vpn_volume.name,\n        \"PRIVATE_KEY\": \"/mnt/private_key.pem\"\n    }\n\n    # only mount the DB if it is a file\n    info(\"Setting up databases\")\n    db_labels = [db['label'] for db in ctx.databases]\n    for label in db_labels:\n\n        # check that label contains only valid characters\n        if not label.isidentifier():\n            error(f\"Database label {Fore.RED}{label}{Style.RESET_ALL} contains\"\n                  \" invalid characters. Only letters, numbers, and underscores\"\n                  \" are allowed, and it cannot start with a number.\")\n            exit(1)\n\n        db_config = get_database_config(ctx.databases, label)\n        uri = db_config['uri']\n        db_type = db_config['type']\n\n        info(f\"  Processing {Fore.GREEN}{db_type}{Style.RESET_ALL} database \"\n             f\"{Fore.GREEN}{label}:{uri}{Style.RESET_ALL}\")\n        label_capitals = label.upper()\n\n        try:\n            file_based = Path(uri).exists()\n        except Exception:\n            # If the database uri cannot be parsed, it is definitely not a\n            # file. In case of http servers or sql servers, checking the path\n            # of the the uri will lead to an OS-dependent error, which is why\n            # we catch all exceptions here.\n            file_based = False\n\n        if not file_based and not force_db_mount:\n            debug('  - non file-based database added')\n            env[f'{label_capitals}_DATABASE_URI'] = uri\n        else:\n            debug('  - file-based database added')\n            suffix = Path(uri).suffix\n            env[f'{label_capitals}_DATABASE_URI'] = f'{label}{suffix}'\n            mounts.append((f'/mnt/{label}{suffix}', str(uri)))\n\n    system_folders_option = \"--system\" if system_folders else \"--user\"\n    cmd = f'vnode-local start -c /mnt/config/{name}.yaml -n {name} '\\\n          f' --dockerized {system_folders_option}'\n\n    info(\"Running Docker container\")\n    volumes = []\n    for mount in mounts:\n        volumes.append(f'{mount[1]}:{mount[0]}')\n\n    remove_container_if_exists(\n        docker_client=docker_client, name=ctx.docker_container_name\n    )\n\n    container = docker_client.containers.run(\n        image,\n        command=cmd,\n        volumes=volumes,\n        detach=True,\n        labels={\n            f\"{APPNAME}-type\": \"node\",\n            \"system\": str(system_folders),\n            \"name\": ctx.config_file_name\n        },\n        environment=env,\n        name=ctx.docker_container_name,\n        auto_remove=not keep,\n        tty=True\n    )\n\n    info(f\"Success! container id = {container}\")\n\n    if attach:\n        logs = container.attach(stream=True, logs=True)\n        Thread(target=print_log_worker, args=(logs,), daemon=True).start()\n        while True:\n            try:\n                time.sleep(1)\n            except KeyboardInterrupt:\n                info(\"Closing log file. Keyboard Interrupt.\")\n                info(\"Note that your node is still running! Shut it down with \"\n                     f\"'{Fore.RED}v6 node stop{Style.RESET_ALL}'\")\n                exit(0)\n"], "filenames": ["docs/algorithms/code_details.rst", "docs/algorithms/develop.rst", "vantage6-algorithm-tools/vantage6/algorithm/tools/decorators.py", "vantage6-algorithm-tools/vantage6/algorithm/tools/util.py", "vantage6-algorithm-tools/vantage6/algorithm/tools/wrap.py", "vantage6-common/vantage6/common/globals.py", "vantage6-common/vantage6/common/serialization.py", "vantage6-node/vantage6/node/docker/docker_manager.py", "vantage6-node/vantage6/node/docker/task_manager.py", "vantage6-node/vantage6/node/globals.py", "vantage6/vantage6/cli/node/start.py"], "buggy_code_start_loc": [170, 100, 13, 1, 9, 38, 1, 488, 6, 48, 13], "buggy_code_end_loc": [180, 106, 474, 37, 58, 38, 21, 521, 541, 48, 243], "fixing_code_start_loc": [170, 100, 13, 2, 9, 39, 2, 488, 7, 49, 13], "fixing_code_end_loc": [182, 114, 474, 69, 61, 42, 22, 521, 643, 53, 248], "type": "CWE-94", "message": "The vantage6 technology enables to manage and deploy privacy enhancing technologies like Federated Learning (FL) and Multi-Party Computation (MPC). Prior to 4.2.0, authenticated users could inject code into algorithm environment variables, resulting in remote code execution.  This vulnerability is patched in 4.2.0.", "other": {"cve": {"id": "CVE-2024-21649", "sourceIdentifier": "security-advisories@github.com", "published": "2024-01-30T16:15:47.653", "lastModified": "2024-02-08T16:43:53.780", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The vantage6 technology enables to manage and deploy privacy enhancing technologies like Federated Learning (FL) and Multi-Party Computation (MPC). Prior to 4.2.0, authenticated users could inject code into algorithm environment variables, resulting in remote code execution.  This vulnerability is patched in 4.2.0."}, {"lang": "es", "value": "La tecnolog\u00eda vantage6 permite gestionar e implementar tecnolog\u00edas que mejoran la privacidad, como el Federated Learning (FL) y la Multi-Party Computation (MPC). Antes de 4.2.0, los usuarios autenticados pod\u00edan inyectar c\u00f3digo en variables de entorno de algoritmos, lo que daba como resultado la ejecuci\u00f3n remota de c\u00f3digo. Esta vulnerabilidad est\u00e1 parcheada en 4.2.0."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-94"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-94"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:vantage6:vantage6:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.2.0", "matchCriteriaId": "A9E3A3A7-C004-4E76-B2A3-46F0F1C68AD4"}]}]}], "references": [{"url": "https://github.com/vantage6/vantage6/commit/eac19db737145d3ca987adf037a454fae0790ddd", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/vantage6/vantage6/security/advisories/GHSA-w9h2-px87-74vx", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/vantage6/vantage6/commit/eac19db737145d3ca987adf037a454fae0790ddd"}}