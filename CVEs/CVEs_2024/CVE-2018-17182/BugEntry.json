{"buggy_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MM_TYPES_H\n#define _LINUX_MM_TYPES_H\n\n#include <linux/mm_types_task.h>\n\n#include <linux/auxvec.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/rbtree.h>\n#include <linux/rwsem.h>\n#include <linux/completion.h>\n#include <linux/cpumask.h>\n#include <linux/uprobes.h>\n#include <linux/page-flags-layout.h>\n#include <linux/workqueue.h>\n\n#include <asm/mmu.h>\n\n#ifndef AT_VECTOR_SIZE_ARCH\n#define AT_VECTOR_SIZE_ARCH 0\n#endif\n#define AT_VECTOR_SIZE (2*(AT_VECTOR_SIZE_ARCH + AT_VECTOR_SIZE_BASE + 1))\n\ntypedef int vm_fault_t;\n\nstruct address_space;\nstruct mem_cgroup;\nstruct hmm;\n\n/*\n * Each physical page in the system has a struct page associated with\n * it to keep track of whatever it is we are using the page for at the\n * moment. Note that we have no way to track which tasks are using\n * a page, though if it is a pagecache page, rmap structures can tell us\n * who is mapping it.\n *\n * If you allocate the page using alloc_pages(), you can use some of the\n * space in struct page for your own purposes.  The five words in the main\n * union are available, except for bit 0 of the first word which must be\n * kept clear.  Many users use this word to store a pointer to an object\n * which is guaranteed to be aligned.  If you use the same storage as\n * page->mapping, you must restore it to NULL before freeing the page.\n *\n * If your page will not be mapped to userspace, you can also use the four\n * bytes in the mapcount union, but you must call page_mapcount_reset()\n * before freeing it.\n *\n * If you want to use the refcount field, it must be used in such a way\n * that other CPUs temporarily incrementing and then decrementing the\n * refcount does not cause problems.  On receiving the page from\n * alloc_pages(), the refcount will be positive.\n *\n * If you allocate pages of order > 0, you can use some of the fields\n * in each subpage, but you may need to restore some of their values\n * afterwards.\n *\n * SLUB uses cmpxchg_double() to atomically update its freelist and\n * counters.  That requires that freelist & counters be adjacent and\n * double-word aligned.  We align all struct pages to double-word\n * boundaries, and ensure that 'freelist' is aligned within the\n * struct.\n */\n#ifdef CONFIG_HAVE_ALIGNED_STRUCT_PAGE\n#define _struct_page_alignment\t__aligned(2 * sizeof(unsigned long))\n#else\n#define _struct_page_alignment\n#endif\n\nstruct page {\n\tunsigned long flags;\t\t/* Atomic flags, some possibly\n\t\t\t\t\t * updated asynchronously */\n\t/*\n\t * Five words (20/40 bytes) are available in this union.\n\t * WARNING: bit 0 of the first word is used for PageTail(). That\n\t * means the other users of this union MUST NOT use the bit to\n\t * avoid collision and false-positive PageTail().\n\t */\n\tunion {\n\t\tstruct {\t/* Page cache and anonymous pages */\n\t\t\t/**\n\t\t\t * @lru: Pageout list, eg. active_list protected by\n\t\t\t * zone_lru_lock.  Sometimes used as a generic list\n\t\t\t * by the page owner.\n\t\t\t */\n\t\t\tstruct list_head lru;\n\t\t\t/* See page-flags.h for PAGE_MAPPING_FLAGS */\n\t\t\tstruct address_space *mapping;\n\t\t\tpgoff_t index;\t\t/* Our offset within mapping. */\n\t\t\t/**\n\t\t\t * @private: Mapping-private opaque data.\n\t\t\t * Usually used for buffer_heads if PagePrivate.\n\t\t\t * Used for swp_entry_t if PageSwapCache.\n\t\t\t * Indicates order in the buddy system if PageBuddy.\n\t\t\t */\n\t\t\tunsigned long private;\n\t\t};\n\t\tstruct {\t/* slab, slob and slub */\n\t\t\tunion {\n\t\t\t\tstruct list_head slab_list;\t/* uses lru */\n\t\t\t\tstruct {\t/* Partial pages */\n\t\t\t\t\tstruct page *next;\n#ifdef CONFIG_64BIT\n\t\t\t\t\tint pages;\t/* Nr of pages left */\n\t\t\t\t\tint pobjects;\t/* Approximate count */\n#else\n\t\t\t\t\tshort int pages;\n\t\t\t\t\tshort int pobjects;\n#endif\n\t\t\t\t};\n\t\t\t};\n\t\t\tstruct kmem_cache *slab_cache; /* not slob */\n\t\t\t/* Double-word boundary */\n\t\t\tvoid *freelist;\t\t/* first free object */\n\t\t\tunion {\n\t\t\t\tvoid *s_mem;\t/* slab: first object */\n\t\t\t\tunsigned long counters;\t\t/* SLUB */\n\t\t\t\tstruct {\t\t\t/* SLUB */\n\t\t\t\t\tunsigned inuse:16;\n\t\t\t\t\tunsigned objects:15;\n\t\t\t\t\tunsigned frozen:1;\n\t\t\t\t};\n\t\t\t};\n\t\t};\n\t\tstruct {\t/* Tail pages of compound page */\n\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n\n\t\t\t/* First tail page only */\n\t\t\tunsigned char compound_dtor;\n\t\t\tunsigned char compound_order;\n\t\t\tatomic_t compound_mapcount;\n\t\t};\n\t\tstruct {\t/* Second tail page of compound page */\n\t\t\tunsigned long _compound_pad_1;\t/* compound_head */\n\t\t\tunsigned long _compound_pad_2;\n\t\t\tstruct list_head deferred_list;\n\t\t};\n\t\tstruct {\t/* Page table pages */\n\t\t\tunsigned long _pt_pad_1;\t/* compound_head */\n\t\t\tpgtable_t pmd_huge_pte; /* protected by page->ptl */\n\t\t\tunsigned long _pt_pad_2;\t/* mapping */\n\t\t\tunion {\n\t\t\t\tstruct mm_struct *pt_mm; /* x86 pgds only */\n\t\t\t\tatomic_t pt_frag_refcount; /* powerpc */\n\t\t\t};\n#if ALLOC_SPLIT_PTLOCKS\n\t\t\tspinlock_t *ptl;\n#else\n\t\t\tspinlock_t ptl;\n#endif\n\t\t};\n\t\tstruct {\t/* ZONE_DEVICE pages */\n\t\t\t/** @pgmap: Points to the hosting device page map. */\n\t\t\tstruct dev_pagemap *pgmap;\n\t\t\tunsigned long hmm_data;\n\t\t\tunsigned long _zd_pad_1;\t/* uses mapping */\n\t\t};\n\n\t\t/** @rcu_head: You can use this to free a page by RCU. */\n\t\tstruct rcu_head rcu_head;\n\t};\n\n\tunion {\t\t/* This union is 4 bytes in size. */\n\t\t/*\n\t\t * If the page can be mapped to userspace, encodes the number\n\t\t * of times this page is referenced by a page table.\n\t\t */\n\t\tatomic_t _mapcount;\n\n\t\t/*\n\t\t * If the page is neither PageSlab nor mappable to userspace,\n\t\t * the value stored here may help determine what this page\n\t\t * is used for.  See page-flags.h for a list of page types\n\t\t * which are currently stored here.\n\t\t */\n\t\tunsigned int page_type;\n\n\t\tunsigned int active;\t\t/* SLAB */\n\t\tint units;\t\t\t/* SLOB */\n\t};\n\n\t/* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */\n\tatomic_t _refcount;\n\n#ifdef CONFIG_MEMCG\n\tstruct mem_cgroup *mem_cgroup;\n#endif\n\n\t/*\n\t * On machines where all RAM is mapped into kernel address space,\n\t * we can simply calculate the virtual address. On machines with\n\t * highmem some memory is mapped into kernel virtual memory\n\t * dynamically, so we need a place to store that address.\n\t * Note that this field could be 16 bits on x86 ... ;)\n\t *\n\t * Architectures with slow multiplication can define\n\t * WANT_PAGE_VIRTUAL in asm/page.h\n\t */\n#if defined(WANT_PAGE_VIRTUAL)\n\tvoid *virtual;\t\t\t/* Kernel virtual address (NULL if\n\t\t\t\t\t   not kmapped, ie. highmem) */\n#endif /* WANT_PAGE_VIRTUAL */\n\n#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\n\tint _last_cpupid;\n#endif\n} _struct_page_alignment;\n\n#define PAGE_FRAG_CACHE_MAX_SIZE\t__ALIGN_MASK(32768, ~PAGE_MASK)\n#define PAGE_FRAG_CACHE_MAX_ORDER\tget_order(PAGE_FRAG_CACHE_MAX_SIZE)\n\nstruct page_frag_cache {\n\tvoid * va;\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\t__u16 offset;\n\t__u16 size;\n#else\n\t__u32 offset;\n#endif\n\t/* we maintain a pagecount bias, so that we dont dirty cache line\n\t * containing page->_refcount every time we allocate a fragment.\n\t */\n\tunsigned int\t\tpagecnt_bias;\n\tbool pfmemalloc;\n};\n\ntypedef unsigned long vm_flags_t;\n\n/*\n * A region containing a mapping of a non-memory backed file under NOMMU\n * conditions.  These are held in a global tree and are pinned by the VMAs that\n * map parts of them.\n */\nstruct vm_region {\n\tstruct rb_node\tvm_rb;\t\t/* link in global region tree */\n\tvm_flags_t\tvm_flags;\t/* VMA vm_flags */\n\tunsigned long\tvm_start;\t/* start address of region */\n\tunsigned long\tvm_end;\t\t/* region initialised to here */\n\tunsigned long\tvm_top;\t\t/* region allocated to here */\n\tunsigned long\tvm_pgoff;\t/* the offset in vm_file corresponding to vm_start */\n\tstruct file\t*vm_file;\t/* the backing file or NULL */\n\n\tint\t\tvm_usage;\t/* region usage count (access under nommu_region_sem) */\n\tbool\t\tvm_icache_flushed : 1; /* true if the icache has been flushed for\n\t\t\t\t\t\t* this region */\n};\n\n#ifdef CONFIG_USERFAULTFD\n#define NULL_VM_UFFD_CTX ((struct vm_userfaultfd_ctx) { NULL, })\nstruct vm_userfaultfd_ctx {\n\tstruct userfaultfd_ctx *ctx;\n};\n#else /* CONFIG_USERFAULTFD */\n#define NULL_VM_UFFD_CTX ((struct vm_userfaultfd_ctx) {})\nstruct vm_userfaultfd_ctx {};\n#endif /* CONFIG_USERFAULTFD */\n\n/*\n * This struct defines a memory VMM memory area. There is one of these\n * per VM-area/task.  A VM area is any part of the process virtual memory\n * space that has a special rule for the page-fault handlers (ie a shared\n * library, the executable area etc).\n */\nstruct vm_area_struct {\n\t/* The first cache line has the info for VMA tree walking. */\n\n\tunsigned long vm_start;\t\t/* Our start address within vm_mm. */\n\tunsigned long vm_end;\t\t/* The first byte after our end address\n\t\t\t\t\t   within vm_mm. */\n\n\t/* linked list of VM areas per task, sorted by address */\n\tstruct vm_area_struct *vm_next, *vm_prev;\n\n\tstruct rb_node vm_rb;\n\n\t/*\n\t * Largest free memory gap in bytes to the left of this VMA.\n\t * Either between this VMA and vma->vm_prev, or between one of the\n\t * VMAs below us in the VMA rbtree and its ->vm_prev. This helps\n\t * get_unmapped_area find a free area of the right size.\n\t */\n\tunsigned long rb_subtree_gap;\n\n\t/* Second cache line starts here. */\n\n\tstruct mm_struct *vm_mm;\t/* The address space we belong to. */\n\tpgprot_t vm_page_prot;\t\t/* Access permissions of this VMA. */\n\tunsigned long vm_flags;\t\t/* Flags, see mm.h. */\n\n\t/*\n\t * For areas with an address space and backing store,\n\t * linkage into the address_space->i_mmap interval tree.\n\t */\n\tstruct {\n\t\tstruct rb_node rb;\n\t\tunsigned long rb_subtree_last;\n\t} shared;\n\n\t/*\n\t * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma\n\t * list, after a COW of one of the file pages.\tA MAP_SHARED vma\n\t * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack\n\t * or brk vma (with NULL file) can only be in an anon_vma list.\n\t */\n\tstruct list_head anon_vma_chain; /* Serialized by mmap_sem &\n\t\t\t\t\t  * page_table_lock */\n\tstruct anon_vma *anon_vma;\t/* Serialized by page_table_lock */\n\n\t/* Function pointers to deal with this struct. */\n\tconst struct vm_operations_struct *vm_ops;\n\n\t/* Information about our backing store: */\n\tunsigned long vm_pgoff;\t\t/* Offset (within vm_file) in PAGE_SIZE\n\t\t\t\t\t   units */\n\tstruct file * vm_file;\t\t/* File we map to (can be NULL). */\n\tvoid * vm_private_data;\t\t/* was vm_pte (shared mem) */\n\n\tatomic_long_t swap_readahead_info;\n#ifndef CONFIG_MMU\n\tstruct vm_region *vm_region;\t/* NOMMU mapping region */\n#endif\n#ifdef CONFIG_NUMA\n\tstruct mempolicy *vm_policy;\t/* NUMA policy for the VMA */\n#endif\n\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx;\n} __randomize_layout;\n\nstruct core_thread {\n\tstruct task_struct *task;\n\tstruct core_thread *next;\n};\n\nstruct core_state {\n\tatomic_t nr_threads;\n\tstruct core_thread dumper;\n\tstruct completion startup;\n};\n\nstruct kioctx_table;\nstruct mm_struct {\n\tstruct {\n\t\tstruct vm_area_struct *mmap;\t\t/* list of VMAs */\n\t\tstruct rb_root mm_rb;\n\t\tu32 vmacache_seqnum;                   /* per-thread vmacache */\n#ifdef CONFIG_MMU\n\t\tunsigned long (*get_unmapped_area) (struct file *filp,\n\t\t\t\tunsigned long addr, unsigned long len,\n\t\t\t\tunsigned long pgoff, unsigned long flags);\n#endif\n\t\tunsigned long mmap_base;\t/* base of mmap area */\n\t\tunsigned long mmap_legacy_base;\t/* base of mmap area in bottom-up allocations */\n#ifdef CONFIG_HAVE_ARCH_COMPAT_MMAP_BASES\n\t\t/* Base adresses for compatible mmap() */\n\t\tunsigned long mmap_compat_base;\n\t\tunsigned long mmap_compat_legacy_base;\n#endif\n\t\tunsigned long task_size;\t/* size of task vm space */\n\t\tunsigned long highest_vm_end;\t/* highest vma end address */\n\t\tpgd_t * pgd;\n\n\t\t/**\n\t\t * @mm_users: The number of users including userspace.\n\t\t *\n\t\t * Use mmget()/mmget_not_zero()/mmput() to modify. When this\n\t\t * drops to 0 (i.e. when the task exits and there are no other\n\t\t * temporary reference holders), we also release a reference on\n\t\t * @mm_count (which may then free the &struct mm_struct if\n\t\t * @mm_count also drops to 0).\n\t\t */\n\t\tatomic_t mm_users;\n\n\t\t/**\n\t\t * @mm_count: The number of references to &struct mm_struct\n\t\t * (@mm_users count as 1).\n\t\t *\n\t\t * Use mmgrab()/mmdrop() to modify. When this drops to 0, the\n\t\t * &struct mm_struct is freed.\n\t\t */\n\t\tatomic_t mm_count;\n\n#ifdef CONFIG_MMU\n\t\tatomic_long_t pgtables_bytes;\t/* PTE page table pages */\n#endif\n\t\tint map_count;\t\t\t/* number of VMAs */\n\n\t\tspinlock_t page_table_lock; /* Protects page tables and some\n\t\t\t\t\t     * counters\n\t\t\t\t\t     */\n\t\tstruct rw_semaphore mmap_sem;\n\n\t\tstruct list_head mmlist; /* List of maybe swapped mm's.\tThese\n\t\t\t\t\t  * are globally strung together off\n\t\t\t\t\t  * init_mm.mmlist, and are protected\n\t\t\t\t\t  * by mmlist_lock\n\t\t\t\t\t  */\n\n\n\t\tunsigned long hiwater_rss; /* High-watermark of RSS usage */\n\t\tunsigned long hiwater_vm;  /* High-water virtual memory usage */\n\n\t\tunsigned long total_vm;\t   /* Total pages mapped */\n\t\tunsigned long locked_vm;   /* Pages that have PG_mlocked set */\n\t\tunsigned long pinned_vm;   /* Refcount permanently increased */\n\t\tunsigned long data_vm;\t   /* VM_WRITE & ~VM_SHARED & ~VM_STACK */\n\t\tunsigned long exec_vm;\t   /* VM_EXEC & ~VM_WRITE & ~VM_STACK */\n\t\tunsigned long stack_vm;\t   /* VM_STACK */\n\t\tunsigned long def_flags;\n\n\t\tspinlock_t arg_lock; /* protect the below fields */\n\t\tunsigned long start_code, end_code, start_data, end_data;\n\t\tunsigned long start_brk, brk, start_stack;\n\t\tunsigned long arg_start, arg_end, env_start, env_end;\n\n\t\tunsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */\n\n\t\t/*\n\t\t * Special counters, in some configurations protected by the\n\t\t * page_table_lock, in other configurations by being atomic.\n\t\t */\n\t\tstruct mm_rss_stat rss_stat;\n\n\t\tstruct linux_binfmt *binfmt;\n\n\t\t/* Architecture-specific MM context */\n\t\tmm_context_t context;\n\n\t\tunsigned long flags; /* Must use atomic bitops to access */\n\n\t\tstruct core_state *core_state; /* coredumping support */\n#ifdef CONFIG_MEMBARRIER\n\t\tatomic_t membarrier_state;\n#endif\n#ifdef CONFIG_AIO\n\t\tspinlock_t\t\t\tioctx_lock;\n\t\tstruct kioctx_table __rcu\t*ioctx_table;\n#endif\n#ifdef CONFIG_MEMCG\n\t\t/*\n\t\t * \"owner\" points to a task that is regarded as the canonical\n\t\t * user/owner of this mm. All of the following must be true in\n\t\t * order for it to be changed:\n\t\t *\n\t\t * current == mm->owner\n\t\t * current->mm != mm\n\t\t * new_owner->mm == mm\n\t\t * new_owner->alloc_lock is held\n\t\t */\n\t\tstruct task_struct __rcu *owner;\n#endif\n\t\tstruct user_namespace *user_ns;\n\n\t\t/* store ref to file /proc/<pid>/exe symlink points to */\n\t\tstruct file __rcu *exe_file;\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tstruct mmu_notifier_mm *mmu_notifier_mm;\n#endif\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS\n\t\tpgtable_t pmd_huge_pte; /* protected by page_table_lock */\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t/*\n\t\t * numa_next_scan is the next time that the PTEs will be marked\n\t\t * pte_numa. NUMA hinting faults will gather statistics and\n\t\t * migrate pages to new nodes if necessary.\n\t\t */\n\t\tunsigned long numa_next_scan;\n\n\t\t/* Restart point for scanning and setting pte_numa */\n\t\tunsigned long numa_scan_offset;\n\n\t\t/* numa_scan_seq prevents two threads setting pte_numa */\n\t\tint numa_scan_seq;\n#endif\n\t\t/*\n\t\t * An operation with batched TLB flushing is going on. Anything\n\t\t * that can move process memory needs to flush the TLB when\n\t\t * moving a PROT_NONE or PROT_NUMA mapped page.\n\t\t */\n\t\tatomic_t tlb_flush_pending;\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\n\t\t/* See flush_tlb_batched_pending() */\n\t\tbool tlb_flush_batched;\n#endif\n\t\tstruct uprobes_state uprobes_state;\n#ifdef CONFIG_HUGETLB_PAGE\n\t\tatomic_long_t hugetlb_usage;\n#endif\n\t\tstruct work_struct async_put_work;\n\n#if IS_ENABLED(CONFIG_HMM)\n\t\t/* HMM needs to track a few things per mm */\n\t\tstruct hmm *hmm;\n#endif\n\t} __randomize_layout;\n\n\t/*\n\t * The mm_cpumask needs to be at the end of mm_struct, because it\n\t * is dynamically sized based on nr_cpu_ids.\n\t */\n\tunsigned long cpu_bitmap[];\n};\n\nextern struct mm_struct init_mm;\n\n/* Pointer magic because the dynamic array size confuses some compilers. */\nstatic inline void mm_init_cpumask(struct mm_struct *mm)\n{\n\tunsigned long cpu_bitmap = (unsigned long)mm;\n\n\tcpu_bitmap += offsetof(struct mm_struct, cpu_bitmap);\n\tcpumask_clear((struct cpumask *)cpu_bitmap);\n}\n\n/* Future-safe accessor for struct mm_struct's cpu_vm_mask. */\nstatic inline cpumask_t *mm_cpumask(struct mm_struct *mm)\n{\n\treturn (struct cpumask *)&mm->cpu_bitmap;\n}\n\nstruct mmu_gather;\nextern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,\n\t\t\t\tunsigned long start, unsigned long end);\nextern void tlb_finish_mmu(struct mmu_gather *tlb,\n\t\t\t\tunsigned long start, unsigned long end);\n\nstatic inline void init_tlb_flush_pending(struct mm_struct *mm)\n{\n\tatomic_set(&mm->tlb_flush_pending, 0);\n}\n\nstatic inline void inc_tlb_flush_pending(struct mm_struct *mm)\n{\n\tatomic_inc(&mm->tlb_flush_pending);\n\t/*\n\t * The only time this value is relevant is when there are indeed pages\n\t * to flush. And we'll only flush pages after changing them, which\n\t * requires the PTL.\n\t *\n\t * So the ordering here is:\n\t *\n\t *\tatomic_inc(&mm->tlb_flush_pending);\n\t *\tspin_lock(&ptl);\n\t *\t...\n\t *\tset_pte_at();\n\t *\tspin_unlock(&ptl);\n\t *\n\t *\t\t\t\tspin_lock(&ptl)\n\t *\t\t\t\tmm_tlb_flush_pending();\n\t *\t\t\t\t....\n\t *\t\t\t\tspin_unlock(&ptl);\n\t *\n\t *\tflush_tlb_range();\n\t *\tatomic_dec(&mm->tlb_flush_pending);\n\t *\n\t * Where the increment if constrained by the PTL unlock, it thus\n\t * ensures that the increment is visible if the PTE modification is\n\t * visible. After all, if there is no PTE modification, nobody cares\n\t * about TLB flushes either.\n\t *\n\t * This very much relies on users (mm_tlb_flush_pending() and\n\t * mm_tlb_flush_nested()) only caring about _specific_ PTEs (and\n\t * therefore specific PTLs), because with SPLIT_PTE_PTLOCKS and RCpc\n\t * locks (PPC) the unlock of one doesn't order against the lock of\n\t * another PTL.\n\t *\n\t * The decrement is ordered by the flush_tlb_range(), such that\n\t * mm_tlb_flush_pending() will not return false unless all flushes have\n\t * completed.\n\t */\n}\n\nstatic inline void dec_tlb_flush_pending(struct mm_struct *mm)\n{\n\t/*\n\t * See inc_tlb_flush_pending().\n\t *\n\t * This cannot be smp_mb__before_atomic() because smp_mb() simply does\n\t * not order against TLB invalidate completion, which is what we need.\n\t *\n\t * Therefore we must rely on tlb_flush_*() to guarantee order.\n\t */\n\tatomic_dec(&mm->tlb_flush_pending);\n}\n\nstatic inline bool mm_tlb_flush_pending(struct mm_struct *mm)\n{\n\t/*\n\t * Must be called after having acquired the PTL; orders against that\n\t * PTLs release and therefore ensures that if we observe the modified\n\t * PTE we must also observe the increment from inc_tlb_flush_pending().\n\t *\n\t * That is, it only guarantees to return true if there is a flush\n\t * pending for _this_ PTL.\n\t */\n\treturn atomic_read(&mm->tlb_flush_pending);\n}\n\nstatic inline bool mm_tlb_flush_nested(struct mm_struct *mm)\n{\n\t/*\n\t * Similar to mm_tlb_flush_pending(), we must have acquired the PTL\n\t * for which there is a TLB flush pending in order to guarantee\n\t * we've seen both that PTE modification and the increment.\n\t *\n\t * (no requirement on actually still holding the PTL, that is irrelevant)\n\t */\n\treturn atomic_read(&mm->tlb_flush_pending) > 1;\n}\n\nstruct vm_fault;\n\nstruct vm_special_mapping {\n\tconst char *name;\t/* The name, e.g. \"[vdso]\". */\n\n\t/*\n\t * If .fault is not provided, this points to a\n\t * NULL-terminated array of pages that back the special mapping.\n\t *\n\t * This must not be NULL unless .fault is provided.\n\t */\n\tstruct page **pages;\n\n\t/*\n\t * If non-NULL, then this is called to resolve page faults\n\t * on the special mapping.  If used, .pages is not checked.\n\t */\n\tvm_fault_t (*fault)(const struct vm_special_mapping *sm,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tstruct vm_fault *vmf);\n\n\tint (*mremap)(const struct vm_special_mapping *sm,\n\t\t     struct vm_area_struct *new_vma);\n};\n\nenum tlb_flush_reason {\n\tTLB_FLUSH_ON_TASK_SWITCH,\n\tTLB_REMOTE_SHOOTDOWN,\n\tTLB_LOCAL_SHOOTDOWN,\n\tTLB_LOCAL_MM_SHOOTDOWN,\n\tTLB_REMOTE_SEND_IPI,\n\tNR_TLB_FLUSH_REASONS,\n};\n\n /*\n  * A swap entry has to fit into a \"unsigned long\", as the entry is hidden\n  * in the \"index\" field of the swapper address space.\n  */\ntypedef struct {\n\tunsigned long val;\n} swp_entry_t;\n\n#endif /* _LINUX_MM_TYPES_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MM_TYPES_TASK_H\n#define _LINUX_MM_TYPES_TASK_H\n\n/*\n * Here are the definitions of the MM data types that are embedded in 'struct task_struct'.\n *\n * (These are defined separately to decouple sched.h from mm_types.h as much as possible.)\n */\n\n#include <linux/types.h>\n#include <linux/threads.h>\n#include <linux/atomic.h>\n#include <linux/cpumask.h>\n\n#include <asm/page.h>\n\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\n#include <asm/tlbbatch.h>\n#endif\n\n#define USE_SPLIT_PTE_PTLOCKS\t(NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS)\n#define USE_SPLIT_PMD_PTLOCKS\t(USE_SPLIT_PTE_PTLOCKS && \\\n\t\tIS_ENABLED(CONFIG_ARCH_ENABLE_SPLIT_PMD_PTLOCK))\n#define ALLOC_SPLIT_PTLOCKS\t(SPINLOCK_SIZE > BITS_PER_LONG/8)\n\n/*\n * The per task VMA cache array:\n */\n#define VMACACHE_BITS 2\n#define VMACACHE_SIZE (1U << VMACACHE_BITS)\n#define VMACACHE_MASK (VMACACHE_SIZE - 1)\n\nstruct vmacache {\n\tu32 seqnum;\n\tstruct vm_area_struct *vmas[VMACACHE_SIZE];\n};\n\nenum {\n\tMM_FILEPAGES,\t/* Resident file mapping pages */\n\tMM_ANONPAGES,\t/* Resident anonymous pages */\n\tMM_SWAPENTS,\t/* Anonymous swap entries */\n\tMM_SHMEMPAGES,\t/* Resident shared memory pages */\n\tNR_MM_COUNTERS\n};\n\n#if USE_SPLIT_PTE_PTLOCKS && defined(CONFIG_MMU)\n#define SPLIT_RSS_COUNTING\n/* per-thread cached information, */\nstruct task_rss_stat {\n\tint events;\t/* for synchronization threshold */\n\tint count[NR_MM_COUNTERS];\n};\n#endif /* USE_SPLIT_PTE_PTLOCKS */\n\nstruct mm_rss_stat {\n\tatomic_long_t count[NR_MM_COUNTERS];\n};\n\nstruct page_frag {\n\tstruct page *page;\n#if (BITS_PER_LONG > 32) || (PAGE_SIZE >= 65536)\n\t__u32 offset;\n\t__u32 size;\n#else\n\t__u16 offset;\n\t__u16 size;\n#endif\n};\n\n/* Track pages that require TLB flushes */\nstruct tlbflush_unmap_batch {\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\n\t/*\n\t * The arch code makes the following promise: generic code can modify a\n\t * PTE, then call arch_tlbbatch_add_mm() (which internally provides all\n\t * needed barriers), then call arch_tlbbatch_flush(), and the entries\n\t * will be flushed on all CPUs by the time that arch_tlbbatch_flush()\n\t * returns.\n\t */\n\tstruct arch_tlbflush_unmap_batch arch;\n\n\t/* True if a flush is needed. */\n\tbool flush_required;\n\n\t/*\n\t * If true then the PTE was dirty when unmapped. The entry must be\n\t * flushed before IO is initiated or a stale TLB entry potentially\n\t * allows an update without redirtying the page.\n\t */\n\tbool writable;\n#endif\n};\n\n#endif /* _LINUX_MM_TYPES_TASK_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef VM_EVENT_ITEM_H_INCLUDED\n#define VM_EVENT_ITEM_H_INCLUDED\n\n#ifdef CONFIG_ZONE_DMA\n#define DMA_ZONE(xx) xx##_DMA,\n#else\n#define DMA_ZONE(xx)\n#endif\n\n#ifdef CONFIG_ZONE_DMA32\n#define DMA32_ZONE(xx) xx##_DMA32,\n#else\n#define DMA32_ZONE(xx)\n#endif\n\n#ifdef CONFIG_HIGHMEM\n#define HIGHMEM_ZONE(xx) xx##_HIGH,\n#else\n#define HIGHMEM_ZONE(xx)\n#endif\n\n#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL, HIGHMEM_ZONE(xx) xx##_MOVABLE\n\nenum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n\t\tFOR_ALL_ZONES(PGALLOC),\n\t\tFOR_ALL_ZONES(ALLOCSTALL),\n\t\tFOR_ALL_ZONES(PGSCAN_SKIP),\n\t\tPGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,\n\t\tPGFAULT, PGMAJFAULT,\n\t\tPGLAZYFREED,\n\t\tPGREFILL,\n\t\tPGSTEAL_KSWAPD,\n\t\tPGSTEAL_DIRECT,\n\t\tPGSCAN_KSWAPD,\n\t\tPGSCAN_DIRECT,\n\t\tPGSCAN_DIRECT_THROTTLE,\n#ifdef CONFIG_NUMA\n\t\tPGSCAN_ZONE_RECLAIM_FAILED,\n#endif\n\t\tPGINODESTEAL, SLABS_SCANNED, KSWAPD_INODESTEAL,\n\t\tKSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,\n\t\tPAGEOUTRUN, PGROTATED,\n\t\tDROP_PAGECACHE, DROP_SLAB,\n\t\tOOM_KILL,\n#ifdef CONFIG_NUMA_BALANCING\n\t\tNUMA_PTE_UPDATES,\n\t\tNUMA_HUGE_PTE_UPDATES,\n\t\tNUMA_HINT_FAULTS,\n\t\tNUMA_HINT_FAULTS_LOCAL,\n\t\tNUMA_PAGE_MIGRATE,\n#endif\n#ifdef CONFIG_MIGRATION\n\t\tPGMIGRATE_SUCCESS, PGMIGRATE_FAIL,\n#endif\n#ifdef CONFIG_COMPACTION\n\t\tCOMPACTMIGRATE_SCANNED, COMPACTFREE_SCANNED,\n\t\tCOMPACTISOLATED,\n\t\tCOMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,\n\t\tKCOMPACTD_WAKE,\n\t\tKCOMPACTD_MIGRATE_SCANNED, KCOMPACTD_FREE_SCANNED,\n#endif\n#ifdef CONFIG_HUGETLB_PAGE\n\t\tHTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,\n#endif\n\t\tUNEVICTABLE_PGCULLED,\t/* culled to noreclaim list */\n\t\tUNEVICTABLE_PGSCANNED,\t/* scanned for reclaimability */\n\t\tUNEVICTABLE_PGRESCUED,\t/* rescued from noreclaim list */\n\t\tUNEVICTABLE_PGMLOCKED,\n\t\tUNEVICTABLE_PGMUNLOCKED,\n\t\tUNEVICTABLE_PGCLEARED,\t/* on COW, page truncate */\n\t\tUNEVICTABLE_PGSTRANDED,\t/* unable to isolate on unlock */\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\tTHP_FAULT_ALLOC,\n\t\tTHP_FAULT_FALLBACK,\n\t\tTHP_COLLAPSE_ALLOC,\n\t\tTHP_COLLAPSE_ALLOC_FAILED,\n\t\tTHP_FILE_ALLOC,\n\t\tTHP_FILE_MAPPED,\n\t\tTHP_SPLIT_PAGE,\n\t\tTHP_SPLIT_PAGE_FAILED,\n\t\tTHP_DEFERRED_SPLIT_PAGE,\n\t\tTHP_SPLIT_PMD,\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\n\t\tTHP_SPLIT_PUD,\n#endif\n\t\tTHP_ZERO_PAGE_ALLOC,\n\t\tTHP_ZERO_PAGE_ALLOC_FAILED,\n\t\tTHP_SWPOUT,\n\t\tTHP_SWPOUT_FALLBACK,\n#endif\n#ifdef CONFIG_MEMORY_BALLOON\n\t\tBALLOON_INFLATE,\n\t\tBALLOON_DEFLATE,\n#ifdef CONFIG_BALLOON_COMPACTION\n\t\tBALLOON_MIGRATE,\n#endif\n#endif\n#ifdef CONFIG_DEBUG_TLBFLUSH\n\t\tNR_TLB_REMOTE_FLUSH,\t/* cpu tried to flush others' tlbs */\n\t\tNR_TLB_REMOTE_FLUSH_RECEIVED,/* cpu received ipi for flush */\n\t\tNR_TLB_LOCAL_FLUSH_ALL,\n\t\tNR_TLB_LOCAL_FLUSH_ONE,\n#endif /* CONFIG_DEBUG_TLBFLUSH */\n#ifdef CONFIG_DEBUG_VM_VMACACHE\n\t\tVMACACHE_FIND_CALLS,\n\t\tVMACACHE_FIND_HITS,\n\t\tVMACACHE_FULL_FLUSHES,\n#endif\n#ifdef CONFIG_SWAP\n\t\tSWAP_RA,\n\t\tSWAP_RA_HIT,\n#endif\n\t\tNR_VM_EVENT_ITEMS\n};\n\n#ifndef CONFIG_TRANSPARENT_HUGEPAGE\n#define THP_FILE_ALLOC ({ BUILD_BUG(); 0; })\n#define THP_FILE_MAPPED ({ BUILD_BUG(); 0; })\n#endif\n\n#endif\t\t/* VM_EVENT_ITEM_H_INCLUDED */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_VMACACHE_H\n#define __LINUX_VMACACHE_H\n\n#include <linux/sched.h>\n#include <linux/mm.h>\n\nstatic inline void vmacache_flush(struct task_struct *tsk)\n{\n\tmemset(tsk->vmacache.vmas, 0, sizeof(tsk->vmacache.vmas));\n}\n\nextern void vmacache_flush_all(struct mm_struct *mm);\nextern void vmacache_update(unsigned long addr, struct vm_area_struct *newvma);\nextern struct vm_area_struct *vmacache_find(struct mm_struct *mm,\n\t\t\t\t\t\t    unsigned long addr);\n\n#ifndef CONFIG_MMU\nextern struct vm_area_struct *vmacache_find_exact(struct mm_struct *mm,\n\t\t\t\t\t\t  unsigned long start,\n\t\t\t\t\t\t  unsigned long end);\n#endif\n\nstatic inline void vmacache_invalidate(struct mm_struct *mm)\n{\n\tmm->vmacache_seqnum++;\n\n\t/* deal with overflows */\n\tif (unlikely(mm->vmacache_seqnum == 0))\n\t\tvmacache_flush_all(mm);\n}\n\n#endif /* __LINUX_VMACACHE_H */\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * mm/debug.c\n *\n * mm/ specific debug routines.\n *\n */\n\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/trace_events.h>\n#include <linux/memcontrol.h>\n#include <trace/events/mmflags.h>\n#include <linux/migrate.h>\n#include <linux/page_owner.h>\n\n#include \"internal.h\"\n\nchar *migrate_reason_names[MR_TYPES] = {\n\t\"compaction\",\n\t\"memory_failure\",\n\t\"memory_hotplug\",\n\t\"syscall_or_cpuset\",\n\t\"mempolicy_mbind\",\n\t\"numa_misplaced\",\n\t\"cma\",\n};\n\nconst struct trace_print_flags pageflag_names[] = {\n\t__def_pageflag_names,\n\t{0, NULL}\n};\n\nconst struct trace_print_flags gfpflag_names[] = {\n\t__def_gfpflag_names,\n\t{0, NULL}\n};\n\nconst struct trace_print_flags vmaflag_names[] = {\n\t__def_vmaflag_names,\n\t{0, NULL}\n};\n\nvoid __dump_page(struct page *page, const char *reason)\n{\n\tbool page_poisoned = PagePoisoned(page);\n\tint mapcount;\n\n\t/*\n\t * If struct page is poisoned don't access Page*() functions as that\n\t * leads to recursive loop. Page*() check for poisoned pages, and calls\n\t * dump_page() when detected.\n\t */\n\tif (page_poisoned) {\n\t\tpr_emerg(\"page:%px is uninitialized and poisoned\", page);\n\t\tgoto hex_only;\n\t}\n\n\t/*\n\t * Avoid VM_BUG_ON() in page_mapcount().\n\t * page->_mapcount space in struct page is used by sl[aou]b pages to\n\t * encode own info.\n\t */\n\tmapcount = PageSlab(page) ? 0 : page_mapcount(page);\n\n\tpr_emerg(\"page:%px count:%d mapcount:%d mapping:%px index:%#lx\",\n\t\t  page, page_ref_count(page), mapcount,\n\t\t  page->mapping, page_to_pgoff(page));\n\tif (PageCompound(page))\n\t\tpr_cont(\" compound_mapcount: %d\", compound_mapcount(page));\n\tpr_cont(\"\\n\");\n\tBUILD_BUG_ON(ARRAY_SIZE(pageflag_names) != __NR_PAGEFLAGS + 1);\n\n\tpr_emerg(\"flags: %#lx(%pGp)\\n\", page->flags, &page->flags);\n\nhex_only:\n\tprint_hex_dump(KERN_ALERT, \"raw: \", DUMP_PREFIX_NONE, 32,\n\t\t\tsizeof(unsigned long), page,\n\t\t\tsizeof(struct page), false);\n\n\tif (reason)\n\t\tpr_alert(\"page dumped because: %s\\n\", reason);\n\n#ifdef CONFIG_MEMCG\n\tif (!page_poisoned && page->mem_cgroup)\n\t\tpr_alert(\"page->mem_cgroup:%px\\n\", page->mem_cgroup);\n#endif\n}\n\nvoid dump_page(struct page *page, const char *reason)\n{\n\t__dump_page(page, reason);\n\tdump_page_owner(page);\n}\nEXPORT_SYMBOL(dump_page);\n\n#ifdef CONFIG_DEBUG_VM\n\nvoid dump_vma(const struct vm_area_struct *vma)\n{\n\tpr_emerg(\"vma %px start %px end %px\\n\"\n\t\t\"next %px prev %px mm %px\\n\"\n\t\t\"prot %lx anon_vma %px vm_ops %px\\n\"\n\t\t\"pgoff %lx file %px private_data %px\\n\"\n\t\t\"flags: %#lx(%pGv)\\n\",\n\t\tvma, (void *)vma->vm_start, (void *)vma->vm_end, vma->vm_next,\n\t\tvma->vm_prev, vma->vm_mm,\n\t\t(unsigned long)pgprot_val(vma->vm_page_prot),\n\t\tvma->anon_vma, vma->vm_ops, vma->vm_pgoff,\n\t\tvma->vm_file, vma->vm_private_data,\n\t\tvma->vm_flags, &vma->vm_flags);\n}\nEXPORT_SYMBOL(dump_vma);\n\nvoid dump_mm(const struct mm_struct *mm)\n{\n\tpr_emerg(\"mm %px mmap %px seqnum %d task_size %lu\\n\"\n#ifdef CONFIG_MMU\n\t\t\"get_unmapped_area %px\\n\"\n#endif\n\t\t\"mmap_base %lu mmap_legacy_base %lu highest_vm_end %lu\\n\"\n\t\t\"pgd %px mm_users %d mm_count %d pgtables_bytes %lu map_count %d\\n\"\n\t\t\"hiwater_rss %lx hiwater_vm %lx total_vm %lx locked_vm %lx\\n\"\n\t\t\"pinned_vm %lx data_vm %lx exec_vm %lx stack_vm %lx\\n\"\n\t\t\"start_code %lx end_code %lx start_data %lx end_data %lx\\n\"\n\t\t\"start_brk %lx brk %lx start_stack %lx\\n\"\n\t\t\"arg_start %lx arg_end %lx env_start %lx env_end %lx\\n\"\n\t\t\"binfmt %px flags %lx core_state %px\\n\"\n#ifdef CONFIG_AIO\n\t\t\"ioctx_table %px\\n\"\n#endif\n#ifdef CONFIG_MEMCG\n\t\t\"owner %px \"\n#endif\n\t\t\"exe_file %px\\n\"\n#ifdef CONFIG_MMU_NOTIFIER\n\t\t\"mmu_notifier_mm %px\\n\"\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t\"numa_next_scan %lu numa_scan_offset %lu numa_scan_seq %d\\n\"\n#endif\n\t\t\"tlb_flush_pending %d\\n\"\n\t\t\"def_flags: %#lx(%pGv)\\n\",\n\n\t\tmm, mm->mmap, mm->vmacache_seqnum, mm->task_size,\n#ifdef CONFIG_MMU\n\t\tmm->get_unmapped_area,\n#endif\n\t\tmm->mmap_base, mm->mmap_legacy_base, mm->highest_vm_end,\n\t\tmm->pgd, atomic_read(&mm->mm_users),\n\t\tatomic_read(&mm->mm_count),\n\t\tmm_pgtables_bytes(mm),\n\t\tmm->map_count,\n\t\tmm->hiwater_rss, mm->hiwater_vm, mm->total_vm, mm->locked_vm,\n\t\tmm->pinned_vm, mm->data_vm, mm->exec_vm, mm->stack_vm,\n\t\tmm->start_code, mm->end_code, mm->start_data, mm->end_data,\n\t\tmm->start_brk, mm->brk, mm->start_stack,\n\t\tmm->arg_start, mm->arg_end, mm->env_start, mm->env_end,\n\t\tmm->binfmt, mm->flags, mm->core_state,\n#ifdef CONFIG_AIO\n\t\tmm->ioctx_table,\n#endif\n#ifdef CONFIG_MEMCG\n\t\tmm->owner,\n#endif\n\t\tmm->exe_file,\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tmm->mmu_notifier_mm,\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\tmm->numa_next_scan, mm->numa_scan_offset, mm->numa_scan_seq,\n#endif\n\t\tatomic_read(&mm->tlb_flush_pending),\n\t\tmm->def_flags, &mm->def_flags\n\t);\n}\n\n#endif\t\t/* CONFIG_DEBUG_VM */\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2014 Davidlohr Bueso.\n */\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n#include <linux/mm.h>\n#include <linux/vmacache.h>\n#include <asm/pgtable.h>\n\n/*\n * Hash based on the pmd of addr if configured with MMU, which provides a good\n * hit rate for workloads with spatial locality.  Otherwise, use pages.\n */\n#ifdef CONFIG_MMU\n#define VMACACHE_SHIFT\tPMD_SHIFT\n#else\n#define VMACACHE_SHIFT\tPAGE_SHIFT\n#endif\n#define VMACACHE_HASH(addr) ((addr >> VMACACHE_SHIFT) & VMACACHE_MASK)\n\n/*\n * Flush vma caches for threads that share a given mm.\n *\n * The operation is safe because the caller holds the mmap_sem\n * exclusively and other threads accessing the vma cache will\n * have mmap_sem held at least for read, so no extra locking\n * is required to maintain the vma cache.\n */\nvoid vmacache_flush_all(struct mm_struct *mm)\n{\n\tstruct task_struct *g, *p;\n\n\tcount_vm_vmacache_event(VMACACHE_FULL_FLUSHES);\n\n\t/*\n\t * Single threaded tasks need not iterate the entire\n\t * list of process. We can avoid the flushing as well\n\t * since the mm's seqnum was increased and don't have\n\t * to worry about other threads' seqnum. Current's\n\t * flush will occur upon the next lookup.\n\t */\n\tif (atomic_read(&mm->mm_users) == 1)\n\t\treturn;\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, p) {\n\t\t/*\n\t\t * Only flush the vmacache pointers as the\n\t\t * mm seqnum is already set and curr's will\n\t\t * be set upon invalidation when the next\n\t\t * lookup is done.\n\t\t */\n\t\tif (mm == p->mm)\n\t\t\tvmacache_flush(p);\n\t}\n\trcu_read_unlock();\n}\n\n/*\n * This task may be accessing a foreign mm via (for example)\n * get_user_pages()->find_vma().  The vmacache is task-local and this\n * task's vmacache pertains to a different mm (ie, its own).  There is\n * nothing we can do here.\n *\n * Also handle the case where a kernel thread has adopted this mm via use_mm().\n * That kernel thread's vmacache is not applicable to this mm.\n */\nstatic inline bool vmacache_valid_mm(struct mm_struct *mm)\n{\n\treturn current->mm == mm && !(current->flags & PF_KTHREAD);\n}\n\nvoid vmacache_update(unsigned long addr, struct vm_area_struct *newvma)\n{\n\tif (vmacache_valid_mm(newvma->vm_mm))\n\t\tcurrent->vmacache.vmas[VMACACHE_HASH(addr)] = newvma;\n}\n\nstatic bool vmacache_valid(struct mm_struct *mm)\n{\n\tstruct task_struct *curr;\n\n\tif (!vmacache_valid_mm(mm))\n\t\treturn false;\n\n\tcurr = current;\n\tif (mm->vmacache_seqnum != curr->vmacache.seqnum) {\n\t\t/*\n\t\t * First attempt will always be invalid, initialize\n\t\t * the new cache for this task here.\n\t\t */\n\t\tcurr->vmacache.seqnum = mm->vmacache_seqnum;\n\t\tvmacache_flush(curr);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstruct vm_area_struct *vmacache_find(struct mm_struct *mm, unsigned long addr)\n{\n\tint idx = VMACACHE_HASH(addr);\n\tint i;\n\n\tcount_vm_vmacache_event(VMACACHE_FIND_CALLS);\n\n\tif (!vmacache_valid(mm))\n\t\treturn NULL;\n\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\tstruct vm_area_struct *vma = current->vmacache.vmas[idx];\n\n\t\tif (vma) {\n#ifdef CONFIG_DEBUG_VM_VMACACHE\n\t\t\tif (WARN_ON_ONCE(vma->vm_mm != mm))\n\t\t\t\tbreak;\n#endif\n\t\t\tif (vma->vm_start <= addr && vma->vm_end > addr) {\n\t\t\t\tcount_vm_vmacache_event(VMACACHE_FIND_HITS);\n\t\t\t\treturn vma;\n\t\t\t}\n\t\t}\n\t\tif (++idx == VMACACHE_SIZE)\n\t\t\tidx = 0;\n\t}\n\n\treturn NULL;\n}\n\n#ifndef CONFIG_MMU\nstruct vm_area_struct *vmacache_find_exact(struct mm_struct *mm,\n\t\t\t\t\t   unsigned long start,\n\t\t\t\t\t   unsigned long end)\n{\n\tint idx = VMACACHE_HASH(start);\n\tint i;\n\n\tcount_vm_vmacache_event(VMACACHE_FIND_CALLS);\n\n\tif (!vmacache_valid(mm))\n\t\treturn NULL;\n\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\tstruct vm_area_struct *vma = current->vmacache.vmas[idx];\n\n\t\tif (vma && vma->vm_start == start && vma->vm_end == end) {\n\t\t\tcount_vm_vmacache_event(VMACACHE_FIND_HITS);\n\t\t\treturn vma;\n\t\t}\n\t\tif (++idx == VMACACHE_SIZE)\n\t\t\tidx = 0;\n\t}\n\n\treturn NULL;\n}\n#endif\n"], "fixing_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MM_TYPES_H\n#define _LINUX_MM_TYPES_H\n\n#include <linux/mm_types_task.h>\n\n#include <linux/auxvec.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/rbtree.h>\n#include <linux/rwsem.h>\n#include <linux/completion.h>\n#include <linux/cpumask.h>\n#include <linux/uprobes.h>\n#include <linux/page-flags-layout.h>\n#include <linux/workqueue.h>\n\n#include <asm/mmu.h>\n\n#ifndef AT_VECTOR_SIZE_ARCH\n#define AT_VECTOR_SIZE_ARCH 0\n#endif\n#define AT_VECTOR_SIZE (2*(AT_VECTOR_SIZE_ARCH + AT_VECTOR_SIZE_BASE + 1))\n\ntypedef int vm_fault_t;\n\nstruct address_space;\nstruct mem_cgroup;\nstruct hmm;\n\n/*\n * Each physical page in the system has a struct page associated with\n * it to keep track of whatever it is we are using the page for at the\n * moment. Note that we have no way to track which tasks are using\n * a page, though if it is a pagecache page, rmap structures can tell us\n * who is mapping it.\n *\n * If you allocate the page using alloc_pages(), you can use some of the\n * space in struct page for your own purposes.  The five words in the main\n * union are available, except for bit 0 of the first word which must be\n * kept clear.  Many users use this word to store a pointer to an object\n * which is guaranteed to be aligned.  If you use the same storage as\n * page->mapping, you must restore it to NULL before freeing the page.\n *\n * If your page will not be mapped to userspace, you can also use the four\n * bytes in the mapcount union, but you must call page_mapcount_reset()\n * before freeing it.\n *\n * If you want to use the refcount field, it must be used in such a way\n * that other CPUs temporarily incrementing and then decrementing the\n * refcount does not cause problems.  On receiving the page from\n * alloc_pages(), the refcount will be positive.\n *\n * If you allocate pages of order > 0, you can use some of the fields\n * in each subpage, but you may need to restore some of their values\n * afterwards.\n *\n * SLUB uses cmpxchg_double() to atomically update its freelist and\n * counters.  That requires that freelist & counters be adjacent and\n * double-word aligned.  We align all struct pages to double-word\n * boundaries, and ensure that 'freelist' is aligned within the\n * struct.\n */\n#ifdef CONFIG_HAVE_ALIGNED_STRUCT_PAGE\n#define _struct_page_alignment\t__aligned(2 * sizeof(unsigned long))\n#else\n#define _struct_page_alignment\n#endif\n\nstruct page {\n\tunsigned long flags;\t\t/* Atomic flags, some possibly\n\t\t\t\t\t * updated asynchronously */\n\t/*\n\t * Five words (20/40 bytes) are available in this union.\n\t * WARNING: bit 0 of the first word is used for PageTail(). That\n\t * means the other users of this union MUST NOT use the bit to\n\t * avoid collision and false-positive PageTail().\n\t */\n\tunion {\n\t\tstruct {\t/* Page cache and anonymous pages */\n\t\t\t/**\n\t\t\t * @lru: Pageout list, eg. active_list protected by\n\t\t\t * zone_lru_lock.  Sometimes used as a generic list\n\t\t\t * by the page owner.\n\t\t\t */\n\t\t\tstruct list_head lru;\n\t\t\t/* See page-flags.h for PAGE_MAPPING_FLAGS */\n\t\t\tstruct address_space *mapping;\n\t\t\tpgoff_t index;\t\t/* Our offset within mapping. */\n\t\t\t/**\n\t\t\t * @private: Mapping-private opaque data.\n\t\t\t * Usually used for buffer_heads if PagePrivate.\n\t\t\t * Used for swp_entry_t if PageSwapCache.\n\t\t\t * Indicates order in the buddy system if PageBuddy.\n\t\t\t */\n\t\t\tunsigned long private;\n\t\t};\n\t\tstruct {\t/* slab, slob and slub */\n\t\t\tunion {\n\t\t\t\tstruct list_head slab_list;\t/* uses lru */\n\t\t\t\tstruct {\t/* Partial pages */\n\t\t\t\t\tstruct page *next;\n#ifdef CONFIG_64BIT\n\t\t\t\t\tint pages;\t/* Nr of pages left */\n\t\t\t\t\tint pobjects;\t/* Approximate count */\n#else\n\t\t\t\t\tshort int pages;\n\t\t\t\t\tshort int pobjects;\n#endif\n\t\t\t\t};\n\t\t\t};\n\t\t\tstruct kmem_cache *slab_cache; /* not slob */\n\t\t\t/* Double-word boundary */\n\t\t\tvoid *freelist;\t\t/* first free object */\n\t\t\tunion {\n\t\t\t\tvoid *s_mem;\t/* slab: first object */\n\t\t\t\tunsigned long counters;\t\t/* SLUB */\n\t\t\t\tstruct {\t\t\t/* SLUB */\n\t\t\t\t\tunsigned inuse:16;\n\t\t\t\t\tunsigned objects:15;\n\t\t\t\t\tunsigned frozen:1;\n\t\t\t\t};\n\t\t\t};\n\t\t};\n\t\tstruct {\t/* Tail pages of compound page */\n\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n\n\t\t\t/* First tail page only */\n\t\t\tunsigned char compound_dtor;\n\t\t\tunsigned char compound_order;\n\t\t\tatomic_t compound_mapcount;\n\t\t};\n\t\tstruct {\t/* Second tail page of compound page */\n\t\t\tunsigned long _compound_pad_1;\t/* compound_head */\n\t\t\tunsigned long _compound_pad_2;\n\t\t\tstruct list_head deferred_list;\n\t\t};\n\t\tstruct {\t/* Page table pages */\n\t\t\tunsigned long _pt_pad_1;\t/* compound_head */\n\t\t\tpgtable_t pmd_huge_pte; /* protected by page->ptl */\n\t\t\tunsigned long _pt_pad_2;\t/* mapping */\n\t\t\tunion {\n\t\t\t\tstruct mm_struct *pt_mm; /* x86 pgds only */\n\t\t\t\tatomic_t pt_frag_refcount; /* powerpc */\n\t\t\t};\n#if ALLOC_SPLIT_PTLOCKS\n\t\t\tspinlock_t *ptl;\n#else\n\t\t\tspinlock_t ptl;\n#endif\n\t\t};\n\t\tstruct {\t/* ZONE_DEVICE pages */\n\t\t\t/** @pgmap: Points to the hosting device page map. */\n\t\t\tstruct dev_pagemap *pgmap;\n\t\t\tunsigned long hmm_data;\n\t\t\tunsigned long _zd_pad_1;\t/* uses mapping */\n\t\t};\n\n\t\t/** @rcu_head: You can use this to free a page by RCU. */\n\t\tstruct rcu_head rcu_head;\n\t};\n\n\tunion {\t\t/* This union is 4 bytes in size. */\n\t\t/*\n\t\t * If the page can be mapped to userspace, encodes the number\n\t\t * of times this page is referenced by a page table.\n\t\t */\n\t\tatomic_t _mapcount;\n\n\t\t/*\n\t\t * If the page is neither PageSlab nor mappable to userspace,\n\t\t * the value stored here may help determine what this page\n\t\t * is used for.  See page-flags.h for a list of page types\n\t\t * which are currently stored here.\n\t\t */\n\t\tunsigned int page_type;\n\n\t\tunsigned int active;\t\t/* SLAB */\n\t\tint units;\t\t\t/* SLOB */\n\t};\n\n\t/* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */\n\tatomic_t _refcount;\n\n#ifdef CONFIG_MEMCG\n\tstruct mem_cgroup *mem_cgroup;\n#endif\n\n\t/*\n\t * On machines where all RAM is mapped into kernel address space,\n\t * we can simply calculate the virtual address. On machines with\n\t * highmem some memory is mapped into kernel virtual memory\n\t * dynamically, so we need a place to store that address.\n\t * Note that this field could be 16 bits on x86 ... ;)\n\t *\n\t * Architectures with slow multiplication can define\n\t * WANT_PAGE_VIRTUAL in asm/page.h\n\t */\n#if defined(WANT_PAGE_VIRTUAL)\n\tvoid *virtual;\t\t\t/* Kernel virtual address (NULL if\n\t\t\t\t\t   not kmapped, ie. highmem) */\n#endif /* WANT_PAGE_VIRTUAL */\n\n#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\n\tint _last_cpupid;\n#endif\n} _struct_page_alignment;\n\n#define PAGE_FRAG_CACHE_MAX_SIZE\t__ALIGN_MASK(32768, ~PAGE_MASK)\n#define PAGE_FRAG_CACHE_MAX_ORDER\tget_order(PAGE_FRAG_CACHE_MAX_SIZE)\n\nstruct page_frag_cache {\n\tvoid * va;\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\t__u16 offset;\n\t__u16 size;\n#else\n\t__u32 offset;\n#endif\n\t/* we maintain a pagecount bias, so that we dont dirty cache line\n\t * containing page->_refcount every time we allocate a fragment.\n\t */\n\tunsigned int\t\tpagecnt_bias;\n\tbool pfmemalloc;\n};\n\ntypedef unsigned long vm_flags_t;\n\n/*\n * A region containing a mapping of a non-memory backed file under NOMMU\n * conditions.  These are held in a global tree and are pinned by the VMAs that\n * map parts of them.\n */\nstruct vm_region {\n\tstruct rb_node\tvm_rb;\t\t/* link in global region tree */\n\tvm_flags_t\tvm_flags;\t/* VMA vm_flags */\n\tunsigned long\tvm_start;\t/* start address of region */\n\tunsigned long\tvm_end;\t\t/* region initialised to here */\n\tunsigned long\tvm_top;\t\t/* region allocated to here */\n\tunsigned long\tvm_pgoff;\t/* the offset in vm_file corresponding to vm_start */\n\tstruct file\t*vm_file;\t/* the backing file or NULL */\n\n\tint\t\tvm_usage;\t/* region usage count (access under nommu_region_sem) */\n\tbool\t\tvm_icache_flushed : 1; /* true if the icache has been flushed for\n\t\t\t\t\t\t* this region */\n};\n\n#ifdef CONFIG_USERFAULTFD\n#define NULL_VM_UFFD_CTX ((struct vm_userfaultfd_ctx) { NULL, })\nstruct vm_userfaultfd_ctx {\n\tstruct userfaultfd_ctx *ctx;\n};\n#else /* CONFIG_USERFAULTFD */\n#define NULL_VM_UFFD_CTX ((struct vm_userfaultfd_ctx) {})\nstruct vm_userfaultfd_ctx {};\n#endif /* CONFIG_USERFAULTFD */\n\n/*\n * This struct defines a memory VMM memory area. There is one of these\n * per VM-area/task.  A VM area is any part of the process virtual memory\n * space that has a special rule for the page-fault handlers (ie a shared\n * library, the executable area etc).\n */\nstruct vm_area_struct {\n\t/* The first cache line has the info for VMA tree walking. */\n\n\tunsigned long vm_start;\t\t/* Our start address within vm_mm. */\n\tunsigned long vm_end;\t\t/* The first byte after our end address\n\t\t\t\t\t   within vm_mm. */\n\n\t/* linked list of VM areas per task, sorted by address */\n\tstruct vm_area_struct *vm_next, *vm_prev;\n\n\tstruct rb_node vm_rb;\n\n\t/*\n\t * Largest free memory gap in bytes to the left of this VMA.\n\t * Either between this VMA and vma->vm_prev, or between one of the\n\t * VMAs below us in the VMA rbtree and its ->vm_prev. This helps\n\t * get_unmapped_area find a free area of the right size.\n\t */\n\tunsigned long rb_subtree_gap;\n\n\t/* Second cache line starts here. */\n\n\tstruct mm_struct *vm_mm;\t/* The address space we belong to. */\n\tpgprot_t vm_page_prot;\t\t/* Access permissions of this VMA. */\n\tunsigned long vm_flags;\t\t/* Flags, see mm.h. */\n\n\t/*\n\t * For areas with an address space and backing store,\n\t * linkage into the address_space->i_mmap interval tree.\n\t */\n\tstruct {\n\t\tstruct rb_node rb;\n\t\tunsigned long rb_subtree_last;\n\t} shared;\n\n\t/*\n\t * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma\n\t * list, after a COW of one of the file pages.\tA MAP_SHARED vma\n\t * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack\n\t * or brk vma (with NULL file) can only be in an anon_vma list.\n\t */\n\tstruct list_head anon_vma_chain; /* Serialized by mmap_sem &\n\t\t\t\t\t  * page_table_lock */\n\tstruct anon_vma *anon_vma;\t/* Serialized by page_table_lock */\n\n\t/* Function pointers to deal with this struct. */\n\tconst struct vm_operations_struct *vm_ops;\n\n\t/* Information about our backing store: */\n\tunsigned long vm_pgoff;\t\t/* Offset (within vm_file) in PAGE_SIZE\n\t\t\t\t\t   units */\n\tstruct file * vm_file;\t\t/* File we map to (can be NULL). */\n\tvoid * vm_private_data;\t\t/* was vm_pte (shared mem) */\n\n\tatomic_long_t swap_readahead_info;\n#ifndef CONFIG_MMU\n\tstruct vm_region *vm_region;\t/* NOMMU mapping region */\n#endif\n#ifdef CONFIG_NUMA\n\tstruct mempolicy *vm_policy;\t/* NUMA policy for the VMA */\n#endif\n\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx;\n} __randomize_layout;\n\nstruct core_thread {\n\tstruct task_struct *task;\n\tstruct core_thread *next;\n};\n\nstruct core_state {\n\tatomic_t nr_threads;\n\tstruct core_thread dumper;\n\tstruct completion startup;\n};\n\nstruct kioctx_table;\nstruct mm_struct {\n\tstruct {\n\t\tstruct vm_area_struct *mmap;\t\t/* list of VMAs */\n\t\tstruct rb_root mm_rb;\n\t\tu64 vmacache_seqnum;                   /* per-thread vmacache */\n#ifdef CONFIG_MMU\n\t\tunsigned long (*get_unmapped_area) (struct file *filp,\n\t\t\t\tunsigned long addr, unsigned long len,\n\t\t\t\tunsigned long pgoff, unsigned long flags);\n#endif\n\t\tunsigned long mmap_base;\t/* base of mmap area */\n\t\tunsigned long mmap_legacy_base;\t/* base of mmap area in bottom-up allocations */\n#ifdef CONFIG_HAVE_ARCH_COMPAT_MMAP_BASES\n\t\t/* Base adresses for compatible mmap() */\n\t\tunsigned long mmap_compat_base;\n\t\tunsigned long mmap_compat_legacy_base;\n#endif\n\t\tunsigned long task_size;\t/* size of task vm space */\n\t\tunsigned long highest_vm_end;\t/* highest vma end address */\n\t\tpgd_t * pgd;\n\n\t\t/**\n\t\t * @mm_users: The number of users including userspace.\n\t\t *\n\t\t * Use mmget()/mmget_not_zero()/mmput() to modify. When this\n\t\t * drops to 0 (i.e. when the task exits and there are no other\n\t\t * temporary reference holders), we also release a reference on\n\t\t * @mm_count (which may then free the &struct mm_struct if\n\t\t * @mm_count also drops to 0).\n\t\t */\n\t\tatomic_t mm_users;\n\n\t\t/**\n\t\t * @mm_count: The number of references to &struct mm_struct\n\t\t * (@mm_users count as 1).\n\t\t *\n\t\t * Use mmgrab()/mmdrop() to modify. When this drops to 0, the\n\t\t * &struct mm_struct is freed.\n\t\t */\n\t\tatomic_t mm_count;\n\n#ifdef CONFIG_MMU\n\t\tatomic_long_t pgtables_bytes;\t/* PTE page table pages */\n#endif\n\t\tint map_count;\t\t\t/* number of VMAs */\n\n\t\tspinlock_t page_table_lock; /* Protects page tables and some\n\t\t\t\t\t     * counters\n\t\t\t\t\t     */\n\t\tstruct rw_semaphore mmap_sem;\n\n\t\tstruct list_head mmlist; /* List of maybe swapped mm's.\tThese\n\t\t\t\t\t  * are globally strung together off\n\t\t\t\t\t  * init_mm.mmlist, and are protected\n\t\t\t\t\t  * by mmlist_lock\n\t\t\t\t\t  */\n\n\n\t\tunsigned long hiwater_rss; /* High-watermark of RSS usage */\n\t\tunsigned long hiwater_vm;  /* High-water virtual memory usage */\n\n\t\tunsigned long total_vm;\t   /* Total pages mapped */\n\t\tunsigned long locked_vm;   /* Pages that have PG_mlocked set */\n\t\tunsigned long pinned_vm;   /* Refcount permanently increased */\n\t\tunsigned long data_vm;\t   /* VM_WRITE & ~VM_SHARED & ~VM_STACK */\n\t\tunsigned long exec_vm;\t   /* VM_EXEC & ~VM_WRITE & ~VM_STACK */\n\t\tunsigned long stack_vm;\t   /* VM_STACK */\n\t\tunsigned long def_flags;\n\n\t\tspinlock_t arg_lock; /* protect the below fields */\n\t\tunsigned long start_code, end_code, start_data, end_data;\n\t\tunsigned long start_brk, brk, start_stack;\n\t\tunsigned long arg_start, arg_end, env_start, env_end;\n\n\t\tunsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */\n\n\t\t/*\n\t\t * Special counters, in some configurations protected by the\n\t\t * page_table_lock, in other configurations by being atomic.\n\t\t */\n\t\tstruct mm_rss_stat rss_stat;\n\n\t\tstruct linux_binfmt *binfmt;\n\n\t\t/* Architecture-specific MM context */\n\t\tmm_context_t context;\n\n\t\tunsigned long flags; /* Must use atomic bitops to access */\n\n\t\tstruct core_state *core_state; /* coredumping support */\n#ifdef CONFIG_MEMBARRIER\n\t\tatomic_t membarrier_state;\n#endif\n#ifdef CONFIG_AIO\n\t\tspinlock_t\t\t\tioctx_lock;\n\t\tstruct kioctx_table __rcu\t*ioctx_table;\n#endif\n#ifdef CONFIG_MEMCG\n\t\t/*\n\t\t * \"owner\" points to a task that is regarded as the canonical\n\t\t * user/owner of this mm. All of the following must be true in\n\t\t * order for it to be changed:\n\t\t *\n\t\t * current == mm->owner\n\t\t * current->mm != mm\n\t\t * new_owner->mm == mm\n\t\t * new_owner->alloc_lock is held\n\t\t */\n\t\tstruct task_struct __rcu *owner;\n#endif\n\t\tstruct user_namespace *user_ns;\n\n\t\t/* store ref to file /proc/<pid>/exe symlink points to */\n\t\tstruct file __rcu *exe_file;\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tstruct mmu_notifier_mm *mmu_notifier_mm;\n#endif\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS\n\t\tpgtable_t pmd_huge_pte; /* protected by page_table_lock */\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t/*\n\t\t * numa_next_scan is the next time that the PTEs will be marked\n\t\t * pte_numa. NUMA hinting faults will gather statistics and\n\t\t * migrate pages to new nodes if necessary.\n\t\t */\n\t\tunsigned long numa_next_scan;\n\n\t\t/* Restart point for scanning and setting pte_numa */\n\t\tunsigned long numa_scan_offset;\n\n\t\t/* numa_scan_seq prevents two threads setting pte_numa */\n\t\tint numa_scan_seq;\n#endif\n\t\t/*\n\t\t * An operation with batched TLB flushing is going on. Anything\n\t\t * that can move process memory needs to flush the TLB when\n\t\t * moving a PROT_NONE or PROT_NUMA mapped page.\n\t\t */\n\t\tatomic_t tlb_flush_pending;\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\n\t\t/* See flush_tlb_batched_pending() */\n\t\tbool tlb_flush_batched;\n#endif\n\t\tstruct uprobes_state uprobes_state;\n#ifdef CONFIG_HUGETLB_PAGE\n\t\tatomic_long_t hugetlb_usage;\n#endif\n\t\tstruct work_struct async_put_work;\n\n#if IS_ENABLED(CONFIG_HMM)\n\t\t/* HMM needs to track a few things per mm */\n\t\tstruct hmm *hmm;\n#endif\n\t} __randomize_layout;\n\n\t/*\n\t * The mm_cpumask needs to be at the end of mm_struct, because it\n\t * is dynamically sized based on nr_cpu_ids.\n\t */\n\tunsigned long cpu_bitmap[];\n};\n\nextern struct mm_struct init_mm;\n\n/* Pointer magic because the dynamic array size confuses some compilers. */\nstatic inline void mm_init_cpumask(struct mm_struct *mm)\n{\n\tunsigned long cpu_bitmap = (unsigned long)mm;\n\n\tcpu_bitmap += offsetof(struct mm_struct, cpu_bitmap);\n\tcpumask_clear((struct cpumask *)cpu_bitmap);\n}\n\n/* Future-safe accessor for struct mm_struct's cpu_vm_mask. */\nstatic inline cpumask_t *mm_cpumask(struct mm_struct *mm)\n{\n\treturn (struct cpumask *)&mm->cpu_bitmap;\n}\n\nstruct mmu_gather;\nextern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,\n\t\t\t\tunsigned long start, unsigned long end);\nextern void tlb_finish_mmu(struct mmu_gather *tlb,\n\t\t\t\tunsigned long start, unsigned long end);\n\nstatic inline void init_tlb_flush_pending(struct mm_struct *mm)\n{\n\tatomic_set(&mm->tlb_flush_pending, 0);\n}\n\nstatic inline void inc_tlb_flush_pending(struct mm_struct *mm)\n{\n\tatomic_inc(&mm->tlb_flush_pending);\n\t/*\n\t * The only time this value is relevant is when there are indeed pages\n\t * to flush. And we'll only flush pages after changing them, which\n\t * requires the PTL.\n\t *\n\t * So the ordering here is:\n\t *\n\t *\tatomic_inc(&mm->tlb_flush_pending);\n\t *\tspin_lock(&ptl);\n\t *\t...\n\t *\tset_pte_at();\n\t *\tspin_unlock(&ptl);\n\t *\n\t *\t\t\t\tspin_lock(&ptl)\n\t *\t\t\t\tmm_tlb_flush_pending();\n\t *\t\t\t\t....\n\t *\t\t\t\tspin_unlock(&ptl);\n\t *\n\t *\tflush_tlb_range();\n\t *\tatomic_dec(&mm->tlb_flush_pending);\n\t *\n\t * Where the increment if constrained by the PTL unlock, it thus\n\t * ensures that the increment is visible if the PTE modification is\n\t * visible. After all, if there is no PTE modification, nobody cares\n\t * about TLB flushes either.\n\t *\n\t * This very much relies on users (mm_tlb_flush_pending() and\n\t * mm_tlb_flush_nested()) only caring about _specific_ PTEs (and\n\t * therefore specific PTLs), because with SPLIT_PTE_PTLOCKS and RCpc\n\t * locks (PPC) the unlock of one doesn't order against the lock of\n\t * another PTL.\n\t *\n\t * The decrement is ordered by the flush_tlb_range(), such that\n\t * mm_tlb_flush_pending() will not return false unless all flushes have\n\t * completed.\n\t */\n}\n\nstatic inline void dec_tlb_flush_pending(struct mm_struct *mm)\n{\n\t/*\n\t * See inc_tlb_flush_pending().\n\t *\n\t * This cannot be smp_mb__before_atomic() because smp_mb() simply does\n\t * not order against TLB invalidate completion, which is what we need.\n\t *\n\t * Therefore we must rely on tlb_flush_*() to guarantee order.\n\t */\n\tatomic_dec(&mm->tlb_flush_pending);\n}\n\nstatic inline bool mm_tlb_flush_pending(struct mm_struct *mm)\n{\n\t/*\n\t * Must be called after having acquired the PTL; orders against that\n\t * PTLs release and therefore ensures that if we observe the modified\n\t * PTE we must also observe the increment from inc_tlb_flush_pending().\n\t *\n\t * That is, it only guarantees to return true if there is a flush\n\t * pending for _this_ PTL.\n\t */\n\treturn atomic_read(&mm->tlb_flush_pending);\n}\n\nstatic inline bool mm_tlb_flush_nested(struct mm_struct *mm)\n{\n\t/*\n\t * Similar to mm_tlb_flush_pending(), we must have acquired the PTL\n\t * for which there is a TLB flush pending in order to guarantee\n\t * we've seen both that PTE modification and the increment.\n\t *\n\t * (no requirement on actually still holding the PTL, that is irrelevant)\n\t */\n\treturn atomic_read(&mm->tlb_flush_pending) > 1;\n}\n\nstruct vm_fault;\n\nstruct vm_special_mapping {\n\tconst char *name;\t/* The name, e.g. \"[vdso]\". */\n\n\t/*\n\t * If .fault is not provided, this points to a\n\t * NULL-terminated array of pages that back the special mapping.\n\t *\n\t * This must not be NULL unless .fault is provided.\n\t */\n\tstruct page **pages;\n\n\t/*\n\t * If non-NULL, then this is called to resolve page faults\n\t * on the special mapping.  If used, .pages is not checked.\n\t */\n\tvm_fault_t (*fault)(const struct vm_special_mapping *sm,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tstruct vm_fault *vmf);\n\n\tint (*mremap)(const struct vm_special_mapping *sm,\n\t\t     struct vm_area_struct *new_vma);\n};\n\nenum tlb_flush_reason {\n\tTLB_FLUSH_ON_TASK_SWITCH,\n\tTLB_REMOTE_SHOOTDOWN,\n\tTLB_LOCAL_SHOOTDOWN,\n\tTLB_LOCAL_MM_SHOOTDOWN,\n\tTLB_REMOTE_SEND_IPI,\n\tNR_TLB_FLUSH_REASONS,\n};\n\n /*\n  * A swap entry has to fit into a \"unsigned long\", as the entry is hidden\n  * in the \"index\" field of the swapper address space.\n  */\ntypedef struct {\n\tunsigned long val;\n} swp_entry_t;\n\n#endif /* _LINUX_MM_TYPES_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MM_TYPES_TASK_H\n#define _LINUX_MM_TYPES_TASK_H\n\n/*\n * Here are the definitions of the MM data types that are embedded in 'struct task_struct'.\n *\n * (These are defined separately to decouple sched.h from mm_types.h as much as possible.)\n */\n\n#include <linux/types.h>\n#include <linux/threads.h>\n#include <linux/atomic.h>\n#include <linux/cpumask.h>\n\n#include <asm/page.h>\n\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\n#include <asm/tlbbatch.h>\n#endif\n\n#define USE_SPLIT_PTE_PTLOCKS\t(NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS)\n#define USE_SPLIT_PMD_PTLOCKS\t(USE_SPLIT_PTE_PTLOCKS && \\\n\t\tIS_ENABLED(CONFIG_ARCH_ENABLE_SPLIT_PMD_PTLOCK))\n#define ALLOC_SPLIT_PTLOCKS\t(SPINLOCK_SIZE > BITS_PER_LONG/8)\n\n/*\n * The per task VMA cache array:\n */\n#define VMACACHE_BITS 2\n#define VMACACHE_SIZE (1U << VMACACHE_BITS)\n#define VMACACHE_MASK (VMACACHE_SIZE - 1)\n\nstruct vmacache {\n\tu64 seqnum;\n\tstruct vm_area_struct *vmas[VMACACHE_SIZE];\n};\n\nenum {\n\tMM_FILEPAGES,\t/* Resident file mapping pages */\n\tMM_ANONPAGES,\t/* Resident anonymous pages */\n\tMM_SWAPENTS,\t/* Anonymous swap entries */\n\tMM_SHMEMPAGES,\t/* Resident shared memory pages */\n\tNR_MM_COUNTERS\n};\n\n#if USE_SPLIT_PTE_PTLOCKS && defined(CONFIG_MMU)\n#define SPLIT_RSS_COUNTING\n/* per-thread cached information, */\nstruct task_rss_stat {\n\tint events;\t/* for synchronization threshold */\n\tint count[NR_MM_COUNTERS];\n};\n#endif /* USE_SPLIT_PTE_PTLOCKS */\n\nstruct mm_rss_stat {\n\tatomic_long_t count[NR_MM_COUNTERS];\n};\n\nstruct page_frag {\n\tstruct page *page;\n#if (BITS_PER_LONG > 32) || (PAGE_SIZE >= 65536)\n\t__u32 offset;\n\t__u32 size;\n#else\n\t__u16 offset;\n\t__u16 size;\n#endif\n};\n\n/* Track pages that require TLB flushes */\nstruct tlbflush_unmap_batch {\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\n\t/*\n\t * The arch code makes the following promise: generic code can modify a\n\t * PTE, then call arch_tlbbatch_add_mm() (which internally provides all\n\t * needed barriers), then call arch_tlbbatch_flush(), and the entries\n\t * will be flushed on all CPUs by the time that arch_tlbbatch_flush()\n\t * returns.\n\t */\n\tstruct arch_tlbflush_unmap_batch arch;\n\n\t/* True if a flush is needed. */\n\tbool flush_required;\n\n\t/*\n\t * If true then the PTE was dirty when unmapped. The entry must be\n\t * flushed before IO is initiated or a stale TLB entry potentially\n\t * allows an update without redirtying the page.\n\t */\n\tbool writable;\n#endif\n};\n\n#endif /* _LINUX_MM_TYPES_TASK_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef VM_EVENT_ITEM_H_INCLUDED\n#define VM_EVENT_ITEM_H_INCLUDED\n\n#ifdef CONFIG_ZONE_DMA\n#define DMA_ZONE(xx) xx##_DMA,\n#else\n#define DMA_ZONE(xx)\n#endif\n\n#ifdef CONFIG_ZONE_DMA32\n#define DMA32_ZONE(xx) xx##_DMA32,\n#else\n#define DMA32_ZONE(xx)\n#endif\n\n#ifdef CONFIG_HIGHMEM\n#define HIGHMEM_ZONE(xx) xx##_HIGH,\n#else\n#define HIGHMEM_ZONE(xx)\n#endif\n\n#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL, HIGHMEM_ZONE(xx) xx##_MOVABLE\n\nenum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n\t\tFOR_ALL_ZONES(PGALLOC),\n\t\tFOR_ALL_ZONES(ALLOCSTALL),\n\t\tFOR_ALL_ZONES(PGSCAN_SKIP),\n\t\tPGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,\n\t\tPGFAULT, PGMAJFAULT,\n\t\tPGLAZYFREED,\n\t\tPGREFILL,\n\t\tPGSTEAL_KSWAPD,\n\t\tPGSTEAL_DIRECT,\n\t\tPGSCAN_KSWAPD,\n\t\tPGSCAN_DIRECT,\n\t\tPGSCAN_DIRECT_THROTTLE,\n#ifdef CONFIG_NUMA\n\t\tPGSCAN_ZONE_RECLAIM_FAILED,\n#endif\n\t\tPGINODESTEAL, SLABS_SCANNED, KSWAPD_INODESTEAL,\n\t\tKSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,\n\t\tPAGEOUTRUN, PGROTATED,\n\t\tDROP_PAGECACHE, DROP_SLAB,\n\t\tOOM_KILL,\n#ifdef CONFIG_NUMA_BALANCING\n\t\tNUMA_PTE_UPDATES,\n\t\tNUMA_HUGE_PTE_UPDATES,\n\t\tNUMA_HINT_FAULTS,\n\t\tNUMA_HINT_FAULTS_LOCAL,\n\t\tNUMA_PAGE_MIGRATE,\n#endif\n#ifdef CONFIG_MIGRATION\n\t\tPGMIGRATE_SUCCESS, PGMIGRATE_FAIL,\n#endif\n#ifdef CONFIG_COMPACTION\n\t\tCOMPACTMIGRATE_SCANNED, COMPACTFREE_SCANNED,\n\t\tCOMPACTISOLATED,\n\t\tCOMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,\n\t\tKCOMPACTD_WAKE,\n\t\tKCOMPACTD_MIGRATE_SCANNED, KCOMPACTD_FREE_SCANNED,\n#endif\n#ifdef CONFIG_HUGETLB_PAGE\n\t\tHTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,\n#endif\n\t\tUNEVICTABLE_PGCULLED,\t/* culled to noreclaim list */\n\t\tUNEVICTABLE_PGSCANNED,\t/* scanned for reclaimability */\n\t\tUNEVICTABLE_PGRESCUED,\t/* rescued from noreclaim list */\n\t\tUNEVICTABLE_PGMLOCKED,\n\t\tUNEVICTABLE_PGMUNLOCKED,\n\t\tUNEVICTABLE_PGCLEARED,\t/* on COW, page truncate */\n\t\tUNEVICTABLE_PGSTRANDED,\t/* unable to isolate on unlock */\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\tTHP_FAULT_ALLOC,\n\t\tTHP_FAULT_FALLBACK,\n\t\tTHP_COLLAPSE_ALLOC,\n\t\tTHP_COLLAPSE_ALLOC_FAILED,\n\t\tTHP_FILE_ALLOC,\n\t\tTHP_FILE_MAPPED,\n\t\tTHP_SPLIT_PAGE,\n\t\tTHP_SPLIT_PAGE_FAILED,\n\t\tTHP_DEFERRED_SPLIT_PAGE,\n\t\tTHP_SPLIT_PMD,\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\n\t\tTHP_SPLIT_PUD,\n#endif\n\t\tTHP_ZERO_PAGE_ALLOC,\n\t\tTHP_ZERO_PAGE_ALLOC_FAILED,\n\t\tTHP_SWPOUT,\n\t\tTHP_SWPOUT_FALLBACK,\n#endif\n#ifdef CONFIG_MEMORY_BALLOON\n\t\tBALLOON_INFLATE,\n\t\tBALLOON_DEFLATE,\n#ifdef CONFIG_BALLOON_COMPACTION\n\t\tBALLOON_MIGRATE,\n#endif\n#endif\n#ifdef CONFIG_DEBUG_TLBFLUSH\n\t\tNR_TLB_REMOTE_FLUSH,\t/* cpu tried to flush others' tlbs */\n\t\tNR_TLB_REMOTE_FLUSH_RECEIVED,/* cpu received ipi for flush */\n\t\tNR_TLB_LOCAL_FLUSH_ALL,\n\t\tNR_TLB_LOCAL_FLUSH_ONE,\n#endif /* CONFIG_DEBUG_TLBFLUSH */\n#ifdef CONFIG_DEBUG_VM_VMACACHE\n\t\tVMACACHE_FIND_CALLS,\n\t\tVMACACHE_FIND_HITS,\n#endif\n#ifdef CONFIG_SWAP\n\t\tSWAP_RA,\n\t\tSWAP_RA_HIT,\n#endif\n\t\tNR_VM_EVENT_ITEMS\n};\n\n#ifndef CONFIG_TRANSPARENT_HUGEPAGE\n#define THP_FILE_ALLOC ({ BUILD_BUG(); 0; })\n#define THP_FILE_MAPPED ({ BUILD_BUG(); 0; })\n#endif\n\n#endif\t\t/* VM_EVENT_ITEM_H_INCLUDED */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_VMACACHE_H\n#define __LINUX_VMACACHE_H\n\n#include <linux/sched.h>\n#include <linux/mm.h>\n\nstatic inline void vmacache_flush(struct task_struct *tsk)\n{\n\tmemset(tsk->vmacache.vmas, 0, sizeof(tsk->vmacache.vmas));\n}\n\nextern void vmacache_update(unsigned long addr, struct vm_area_struct *newvma);\nextern struct vm_area_struct *vmacache_find(struct mm_struct *mm,\n\t\t\t\t\t\t    unsigned long addr);\n\n#ifndef CONFIG_MMU\nextern struct vm_area_struct *vmacache_find_exact(struct mm_struct *mm,\n\t\t\t\t\t\t  unsigned long start,\n\t\t\t\t\t\t  unsigned long end);\n#endif\n\nstatic inline void vmacache_invalidate(struct mm_struct *mm)\n{\n\tmm->vmacache_seqnum++;\n}\n\n#endif /* __LINUX_VMACACHE_H */\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * mm/debug.c\n *\n * mm/ specific debug routines.\n *\n */\n\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/trace_events.h>\n#include <linux/memcontrol.h>\n#include <trace/events/mmflags.h>\n#include <linux/migrate.h>\n#include <linux/page_owner.h>\n\n#include \"internal.h\"\n\nchar *migrate_reason_names[MR_TYPES] = {\n\t\"compaction\",\n\t\"memory_failure\",\n\t\"memory_hotplug\",\n\t\"syscall_or_cpuset\",\n\t\"mempolicy_mbind\",\n\t\"numa_misplaced\",\n\t\"cma\",\n};\n\nconst struct trace_print_flags pageflag_names[] = {\n\t__def_pageflag_names,\n\t{0, NULL}\n};\n\nconst struct trace_print_flags gfpflag_names[] = {\n\t__def_gfpflag_names,\n\t{0, NULL}\n};\n\nconst struct trace_print_flags vmaflag_names[] = {\n\t__def_vmaflag_names,\n\t{0, NULL}\n};\n\nvoid __dump_page(struct page *page, const char *reason)\n{\n\tbool page_poisoned = PagePoisoned(page);\n\tint mapcount;\n\n\t/*\n\t * If struct page is poisoned don't access Page*() functions as that\n\t * leads to recursive loop. Page*() check for poisoned pages, and calls\n\t * dump_page() when detected.\n\t */\n\tif (page_poisoned) {\n\t\tpr_emerg(\"page:%px is uninitialized and poisoned\", page);\n\t\tgoto hex_only;\n\t}\n\n\t/*\n\t * Avoid VM_BUG_ON() in page_mapcount().\n\t * page->_mapcount space in struct page is used by sl[aou]b pages to\n\t * encode own info.\n\t */\n\tmapcount = PageSlab(page) ? 0 : page_mapcount(page);\n\n\tpr_emerg(\"page:%px count:%d mapcount:%d mapping:%px index:%#lx\",\n\t\t  page, page_ref_count(page), mapcount,\n\t\t  page->mapping, page_to_pgoff(page));\n\tif (PageCompound(page))\n\t\tpr_cont(\" compound_mapcount: %d\", compound_mapcount(page));\n\tpr_cont(\"\\n\");\n\tBUILD_BUG_ON(ARRAY_SIZE(pageflag_names) != __NR_PAGEFLAGS + 1);\n\n\tpr_emerg(\"flags: %#lx(%pGp)\\n\", page->flags, &page->flags);\n\nhex_only:\n\tprint_hex_dump(KERN_ALERT, \"raw: \", DUMP_PREFIX_NONE, 32,\n\t\t\tsizeof(unsigned long), page,\n\t\t\tsizeof(struct page), false);\n\n\tif (reason)\n\t\tpr_alert(\"page dumped because: %s\\n\", reason);\n\n#ifdef CONFIG_MEMCG\n\tif (!page_poisoned && page->mem_cgroup)\n\t\tpr_alert(\"page->mem_cgroup:%px\\n\", page->mem_cgroup);\n#endif\n}\n\nvoid dump_page(struct page *page, const char *reason)\n{\n\t__dump_page(page, reason);\n\tdump_page_owner(page);\n}\nEXPORT_SYMBOL(dump_page);\n\n#ifdef CONFIG_DEBUG_VM\n\nvoid dump_vma(const struct vm_area_struct *vma)\n{\n\tpr_emerg(\"vma %px start %px end %px\\n\"\n\t\t\"next %px prev %px mm %px\\n\"\n\t\t\"prot %lx anon_vma %px vm_ops %px\\n\"\n\t\t\"pgoff %lx file %px private_data %px\\n\"\n\t\t\"flags: %#lx(%pGv)\\n\",\n\t\tvma, (void *)vma->vm_start, (void *)vma->vm_end, vma->vm_next,\n\t\tvma->vm_prev, vma->vm_mm,\n\t\t(unsigned long)pgprot_val(vma->vm_page_prot),\n\t\tvma->anon_vma, vma->vm_ops, vma->vm_pgoff,\n\t\tvma->vm_file, vma->vm_private_data,\n\t\tvma->vm_flags, &vma->vm_flags);\n}\nEXPORT_SYMBOL(dump_vma);\n\nvoid dump_mm(const struct mm_struct *mm)\n{\n\tpr_emerg(\"mm %px mmap %px seqnum %llu task_size %lu\\n\"\n#ifdef CONFIG_MMU\n\t\t\"get_unmapped_area %px\\n\"\n#endif\n\t\t\"mmap_base %lu mmap_legacy_base %lu highest_vm_end %lu\\n\"\n\t\t\"pgd %px mm_users %d mm_count %d pgtables_bytes %lu map_count %d\\n\"\n\t\t\"hiwater_rss %lx hiwater_vm %lx total_vm %lx locked_vm %lx\\n\"\n\t\t\"pinned_vm %lx data_vm %lx exec_vm %lx stack_vm %lx\\n\"\n\t\t\"start_code %lx end_code %lx start_data %lx end_data %lx\\n\"\n\t\t\"start_brk %lx brk %lx start_stack %lx\\n\"\n\t\t\"arg_start %lx arg_end %lx env_start %lx env_end %lx\\n\"\n\t\t\"binfmt %px flags %lx core_state %px\\n\"\n#ifdef CONFIG_AIO\n\t\t\"ioctx_table %px\\n\"\n#endif\n#ifdef CONFIG_MEMCG\n\t\t\"owner %px \"\n#endif\n\t\t\"exe_file %px\\n\"\n#ifdef CONFIG_MMU_NOTIFIER\n\t\t\"mmu_notifier_mm %px\\n\"\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t\"numa_next_scan %lu numa_scan_offset %lu numa_scan_seq %d\\n\"\n#endif\n\t\t\"tlb_flush_pending %d\\n\"\n\t\t\"def_flags: %#lx(%pGv)\\n\",\n\n\t\tmm, mm->mmap, (long long) mm->vmacache_seqnum, mm->task_size,\n#ifdef CONFIG_MMU\n\t\tmm->get_unmapped_area,\n#endif\n\t\tmm->mmap_base, mm->mmap_legacy_base, mm->highest_vm_end,\n\t\tmm->pgd, atomic_read(&mm->mm_users),\n\t\tatomic_read(&mm->mm_count),\n\t\tmm_pgtables_bytes(mm),\n\t\tmm->map_count,\n\t\tmm->hiwater_rss, mm->hiwater_vm, mm->total_vm, mm->locked_vm,\n\t\tmm->pinned_vm, mm->data_vm, mm->exec_vm, mm->stack_vm,\n\t\tmm->start_code, mm->end_code, mm->start_data, mm->end_data,\n\t\tmm->start_brk, mm->brk, mm->start_stack,\n\t\tmm->arg_start, mm->arg_end, mm->env_start, mm->env_end,\n\t\tmm->binfmt, mm->flags, mm->core_state,\n#ifdef CONFIG_AIO\n\t\tmm->ioctx_table,\n#endif\n#ifdef CONFIG_MEMCG\n\t\tmm->owner,\n#endif\n\t\tmm->exe_file,\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tmm->mmu_notifier_mm,\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\tmm->numa_next_scan, mm->numa_scan_offset, mm->numa_scan_seq,\n#endif\n\t\tatomic_read(&mm->tlb_flush_pending),\n\t\tmm->def_flags, &mm->def_flags\n\t);\n}\n\n#endif\t\t/* CONFIG_DEBUG_VM */\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2014 Davidlohr Bueso.\n */\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n#include <linux/mm.h>\n#include <linux/vmacache.h>\n#include <asm/pgtable.h>\n\n/*\n * Hash based on the pmd of addr if configured with MMU, which provides a good\n * hit rate for workloads with spatial locality.  Otherwise, use pages.\n */\n#ifdef CONFIG_MMU\n#define VMACACHE_SHIFT\tPMD_SHIFT\n#else\n#define VMACACHE_SHIFT\tPAGE_SHIFT\n#endif\n#define VMACACHE_HASH(addr) ((addr >> VMACACHE_SHIFT) & VMACACHE_MASK)\n\n/*\n * This task may be accessing a foreign mm via (for example)\n * get_user_pages()->find_vma().  The vmacache is task-local and this\n * task's vmacache pertains to a different mm (ie, its own).  There is\n * nothing we can do here.\n *\n * Also handle the case where a kernel thread has adopted this mm via use_mm().\n * That kernel thread's vmacache is not applicable to this mm.\n */\nstatic inline bool vmacache_valid_mm(struct mm_struct *mm)\n{\n\treturn current->mm == mm && !(current->flags & PF_KTHREAD);\n}\n\nvoid vmacache_update(unsigned long addr, struct vm_area_struct *newvma)\n{\n\tif (vmacache_valid_mm(newvma->vm_mm))\n\t\tcurrent->vmacache.vmas[VMACACHE_HASH(addr)] = newvma;\n}\n\nstatic bool vmacache_valid(struct mm_struct *mm)\n{\n\tstruct task_struct *curr;\n\n\tif (!vmacache_valid_mm(mm))\n\t\treturn false;\n\n\tcurr = current;\n\tif (mm->vmacache_seqnum != curr->vmacache.seqnum) {\n\t\t/*\n\t\t * First attempt will always be invalid, initialize\n\t\t * the new cache for this task here.\n\t\t */\n\t\tcurr->vmacache.seqnum = mm->vmacache_seqnum;\n\t\tvmacache_flush(curr);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstruct vm_area_struct *vmacache_find(struct mm_struct *mm, unsigned long addr)\n{\n\tint idx = VMACACHE_HASH(addr);\n\tint i;\n\n\tcount_vm_vmacache_event(VMACACHE_FIND_CALLS);\n\n\tif (!vmacache_valid(mm))\n\t\treturn NULL;\n\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\tstruct vm_area_struct *vma = current->vmacache.vmas[idx];\n\n\t\tif (vma) {\n#ifdef CONFIG_DEBUG_VM_VMACACHE\n\t\t\tif (WARN_ON_ONCE(vma->vm_mm != mm))\n\t\t\t\tbreak;\n#endif\n\t\t\tif (vma->vm_start <= addr && vma->vm_end > addr) {\n\t\t\t\tcount_vm_vmacache_event(VMACACHE_FIND_HITS);\n\t\t\t\treturn vma;\n\t\t\t}\n\t\t}\n\t\tif (++idx == VMACACHE_SIZE)\n\t\t\tidx = 0;\n\t}\n\n\treturn NULL;\n}\n\n#ifndef CONFIG_MMU\nstruct vm_area_struct *vmacache_find_exact(struct mm_struct *mm,\n\t\t\t\t\t   unsigned long start,\n\t\t\t\t\t   unsigned long end)\n{\n\tint idx = VMACACHE_HASH(start);\n\tint i;\n\n\tcount_vm_vmacache_event(VMACACHE_FIND_CALLS);\n\n\tif (!vmacache_valid(mm))\n\t\treturn NULL;\n\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\tstruct vm_area_struct *vma = current->vmacache.vmas[idx];\n\n\t\tif (vma && vma->vm_start == start && vma->vm_end == end) {\n\t\t\tcount_vm_vmacache_event(VMACACHE_FIND_HITS);\n\t\t\treturn vma;\n\t\t}\n\t\tif (++idx == VMACACHE_SIZE)\n\t\t\tidx = 0;\n\t}\n\n\treturn NULL;\n}\n#endif\n"], "filenames": ["include/linux/mm_types.h", "include/linux/mm_types_task.h", "include/linux/vm_event_item.h", "include/linux/vmacache.h", "mm/debug.c", "mm/vmacache.c"], "buggy_code_start_loc": [344, 35, 108, 13, 117, 21], "buggy_code_end_loc": [345, 36, 109, 31, 146, 59], "fixing_code_start_loc": [344, 35, 107, 12, 117, 20], "fixing_code_end_loc": [345, 36, 107, 25, 146, 20], "type": "CWE-416", "message": "An issue was discovered in the Linux kernel through 4.18.8. The vmacache_flush_all function in mm/vmacache.c mishandles sequence number overflows. An attacker can trigger a use-after-free (and possibly gain privileges) via certain thread creation, map, unmap, invalidation, and dereference operations.", "other": {"cve": {"id": "CVE-2018-17182", "sourceIdentifier": "cve@mitre.org", "published": "2018-09-19T09:29:00.620", "lastModified": "2023-02-24T18:33:26.187", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux kernel through 4.18.8. The vmacache_flush_all function in mm/vmacache.c mishandles sequence number overflows. An attacker can trigger a use-after-free (and possibly gain privileges) via certain thread creation, map, unmap, invalidation, and dereference operations."}, {"lang": "es", "value": "Se ha descubierto un problema en el kernel de Linux hasta la versi\u00f3n 4.18.8. La funci\u00f3n vmacache_flush_all en mm/vmacache.c manipula incorrectamente los desbordamientos de n\u00fameros de secuencias. Un atacante puede desencadenar un uso de memoria previamente liberada (y posiblemente la obtenci\u00f3n de privilegios) mediante determinadas operaciones thread creation, map, unmap, invalidation y dereference."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.16", "versionEndExcluding": "3.16.58", "matchCriteriaId": "ACCD57BD-57A5-4BEA-A884-2A21E7B92EBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.123", "matchCriteriaId": "D8AB84BB-69BD-43DB-9CE3-0381B1FC862C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.4.157", "matchCriteriaId": "5DE38E54-E821-4DB3-86DA-E6DCDD4361E0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.9.128", "matchCriteriaId": "139B519B-2681-4096-A1CE-2CF58E0CE274"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.14.71", "matchCriteriaId": "E8C1C6C9-9654-4BB7-9908-EFC7BA9040E8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.15", "versionEndExcluding": "4.18.9", "matchCriteriaId": "8F765B15-BD3A-4C48-984C-05084867A943"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:active_iq_performance_analytics_services:-:*:*:*:*:*:*:*", "matchCriteriaId": "83077160-BB98-408B-81F0-8EF9E566BF28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:element_software:-:*:*:*:*:*:*:*", "matchCriteriaId": "85DF4B3F-4BBC-42B7-B729-096934523D63"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=7a9cdebdcc17e426fb5287e4a82db1dfe86339b2", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/105417", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/bid/106503", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1041748", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3656", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/7a9cdebdcc17e426fb5287e4a82db1dfe86339b2", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2018/10/msg00003.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20190204-0001/", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3776-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3776-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3777-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3777-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3777-3/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4308", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.exploit-db.com/exploits/45497/", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "https://www.openwall.com/lists/oss-security/2018/09/18/4", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/7a9cdebdcc17e426fb5287e4a82db1dfe86339b2"}}