{"buggy_code": ["/*\n * linux/ipc/msg.c\n * Copyright (C) 1992 Krishna Balasubramanian\n *\n * Removed all the remaining kerneld mess\n * Catch the -EFAULT stuff properly\n * Use GFP_KERNEL for messages as in 1.2\n * Fixed up the unchecked user space derefs\n * Copyright (C) 1998 Alan Cox & Andi Kleen\n *\n * /proc/sysvipc/msg support (c) 1999 Dragos Acostachioaie <dragos@iname.com>\n *\n * mostly rewritten, threaded and wake-one semantics added\n * MSGMAX limit removed, sysctl's added\n * (c) 1999 Manfred Spraul <manfred@colorfullife.com>\n *\n * support for audit of ipc object properties and permission changes\n * Dustin Kirkland <dustin.kirkland@us.ibm.com>\n *\n * namespaces support\n * OpenVZ, SWsoft Inc.\n * Pavel Emelianov <xemul@openvz.org>\n */\n\n#include <linux/capability.h>\n#include <linux/msg.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/proc_fs.h>\n#include <linux/list.h>\n#include <linux/security.h>\n#include <linux/sched.h>\n#include <linux/syscalls.h>\n#include <linux/audit.h>\n#include <linux/seq_file.h>\n#include <linux/rwsem.h>\n#include <linux/nsproxy.h>\n#include <linux/ipc_namespace.h>\n\n#include <asm/current.h>\n#include <asm/uaccess.h>\n#include \"util.h\"\n\n/*\n * one msg_receiver structure for each sleeping receiver:\n */\nstruct msg_receiver {\n\tstruct list_head\tr_list;\n\tstruct task_struct\t*r_tsk;\n\n\tint\t\t\tr_mode;\n\tlong\t\t\tr_msgtype;\n\tlong\t\t\tr_maxsize;\n\n\tstruct msg_msg\t\t*volatile r_msg;\n};\n\n/* one msg_sender for each sleeping sender */\nstruct msg_sender {\n\tstruct list_head\tlist;\n\tstruct task_struct\t*tsk;\n};\n\n#define SEARCH_ANY\t\t1\n#define SEARCH_EQUAL\t\t2\n#define SEARCH_NOTEQUAL\t\t3\n#define SEARCH_LESSEQUAL\t4\n#define SEARCH_NUMBER\t\t5\n\n#define msg_ids(ns)\t((ns)->ids[IPC_MSG_IDS])\n\n#define msg_unlock(msq)\t\tipc_unlock(&(msq)->q_perm)\n\nstatic void freeque(struct ipc_namespace *, struct kern_ipc_perm *);\nstatic int newque(struct ipc_namespace *, struct ipc_params *);\n#ifdef CONFIG_PROC_FS\nstatic int sysvipc_msg_proc_show(struct seq_file *s, void *it);\n#endif\n\n/*\n * Scale msgmni with the available lowmem size: the memory dedicated to msg\n * queues should occupy at most 1/MSG_MEM_SCALE of lowmem.\n * Also take into account the number of nsproxies created so far.\n * This should be done staying within the (MSGMNI , IPCMNI/nr_ipc_ns) range.\n */\nvoid recompute_msgmni(struct ipc_namespace *ns)\n{\n\tstruct sysinfo i;\n\tunsigned long allowed;\n\tint nb_ns;\n\n\tsi_meminfo(&i);\n\tallowed = (((i.totalram - i.totalhigh) / MSG_MEM_SCALE) * i.mem_unit)\n\t\t/ MSGMNB;\n\tnb_ns = atomic_read(&nr_ipc_ns);\n\tallowed /= nb_ns;\n\n\tif (allowed < MSGMNI) {\n\t\tns->msg_ctlmni = MSGMNI;\n\t\treturn;\n\t}\n\n\tif (allowed > IPCMNI / nb_ns) {\n\t\tns->msg_ctlmni = IPCMNI / nb_ns;\n\t\treturn;\n\t}\n\n\tns->msg_ctlmni = allowed;\n}\n\nvoid msg_init_ns(struct ipc_namespace *ns)\n{\n\tns->msg_ctlmax = MSGMAX;\n\tns->msg_ctlmnb = MSGMNB;\n\n\trecompute_msgmni(ns);\n\n\tatomic_set(&ns->msg_bytes, 0);\n\tatomic_set(&ns->msg_hdrs, 0);\n\tipc_init_ids(&ns->ids[IPC_MSG_IDS]);\n}\n\n#ifdef CONFIG_IPC_NS\nvoid msg_exit_ns(struct ipc_namespace *ns)\n{\n\tfree_ipcs(ns, &msg_ids(ns), freeque);\n\tidr_destroy(&ns->ids[IPC_MSG_IDS].ipcs_idr);\n}\n#endif\n\nvoid __init msg_init(void)\n{\n\tmsg_init_ns(&init_ipc_ns);\n\n\tprintk(KERN_INFO \"msgmni has been set to %d\\n\",\n\t\tinit_ipc_ns.msg_ctlmni);\n\n\tipc_init_proc_interface(\"sysvipc/msg\",\n\t\t\t\t\"       key      msqid perms      cbytes       qnum lspid lrpid   uid   gid  cuid  cgid      stime      rtime      ctime\\n\",\n\t\t\t\tIPC_MSG_IDS, sysvipc_msg_proc_show);\n}\n\n/*\n * msg_lock_(check_) routines are called in the paths where the rw_mutex\n * is not held.\n */\nstatic inline struct msg_queue *msg_lock(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_lock(&msg_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn (struct msg_queue *)ipcp;\n\n\treturn container_of(ipcp, struct msg_queue, q_perm);\n}\n\nstatic inline struct msg_queue *msg_lock_check(struct ipc_namespace *ns,\n\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_lock_check(&msg_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn (struct msg_queue *)ipcp;\n\n\treturn container_of(ipcp, struct msg_queue, q_perm);\n}\n\nstatic inline void msg_rmid(struct ipc_namespace *ns, struct msg_queue *s)\n{\n\tipc_rmid(&msg_ids(ns), &s->q_perm);\n}\n\n/**\n * newque - Create a new msg queue\n * @ns: namespace\n * @params: ptr to the structure that contains the key and msgflg\n *\n * Called with msg_ids.rw_mutex held (writer)\n */\nstatic int newque(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tstruct msg_queue *msq;\n\tint id, retval;\n\tkey_t key = params->key;\n\tint msgflg = params->flg;\n\n\tmsq = ipc_rcu_alloc(sizeof(*msq));\n\tif (!msq)\n\t\treturn -ENOMEM;\n\n\tmsq->q_perm.mode = msgflg & S_IRWXUGO;\n\tmsq->q_perm.key = key;\n\n\tmsq->q_perm.security = NULL;\n\tretval = security_msg_queue_alloc(msq);\n\tif (retval) {\n\t\tipc_rcu_putref(msq);\n\t\treturn retval;\n\t}\n\n\t/*\n\t * ipc_addid() locks msq\n\t */\n\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n\tif (id < 0) {\n\t\tsecurity_msg_queue_free(msq);\n\t\tipc_rcu_putref(msq);\n\t\treturn id;\n\t}\n\n\tmsq->q_stime = msq->q_rtime = 0;\n\tmsq->q_ctime = get_seconds();\n\tmsq->q_cbytes = msq->q_qnum = 0;\n\tmsq->q_qbytes = ns->msg_ctlmnb;\n\tmsq->q_lspid = msq->q_lrpid = 0;\n\tINIT_LIST_HEAD(&msq->q_messages);\n\tINIT_LIST_HEAD(&msq->q_receivers);\n\tINIT_LIST_HEAD(&msq->q_senders);\n\n\tmsg_unlock(msq);\n\n\treturn msq->q_perm.id;\n}\n\nstatic inline void ss_add(struct msg_queue *msq, struct msg_sender *mss)\n{\n\tmss->tsk = current;\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tlist_add_tail(&mss->list, &msq->q_senders);\n}\n\nstatic inline void ss_del(struct msg_sender *mss)\n{\n\tif (mss->list.next != NULL)\n\t\tlist_del(&mss->list);\n}\n\nstatic void ss_wakeup(struct list_head *h, int kill)\n{\n\tstruct list_head *tmp;\n\n\ttmp = h->next;\n\twhile (tmp != h) {\n\t\tstruct msg_sender *mss;\n\n\t\tmss = list_entry(tmp, struct msg_sender, list);\n\t\ttmp = tmp->next;\n\t\tif (kill)\n\t\t\tmss->list.next = NULL;\n\t\twake_up_process(mss->tsk);\n\t}\n}\n\nstatic void expunge_all(struct msg_queue *msq, int res)\n{\n\tstruct list_head *tmp;\n\n\ttmp = msq->q_receivers.next;\n\twhile (tmp != &msq->q_receivers) {\n\t\tstruct msg_receiver *msr;\n\n\t\tmsr = list_entry(tmp, struct msg_receiver, r_list);\n\t\ttmp = tmp->next;\n\t\tmsr->r_msg = NULL;\n\t\twake_up_process(msr->r_tsk);\n\t\tsmp_mb();\n\t\tmsr->r_msg = ERR_PTR(res);\n\t}\n}\n\n/*\n * freeque() wakes up waiters on the sender and receiver waiting queue,\n * removes the message queue from message queue ID IDR, and cleans up all the\n * messages associated with this queue.\n *\n * msg_ids.rw_mutex (writer) and the spinlock for this message queue are held\n * before freeque() is called. msg_ids.rw_mutex remains locked on exit.\n */\nstatic void freeque(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n{\n\tstruct list_head *tmp;\n\tstruct msg_queue *msq = container_of(ipcp, struct msg_queue, q_perm);\n\n\texpunge_all(msq, -EIDRM);\n\tss_wakeup(&msq->q_senders, 1);\n\tmsg_rmid(ns, msq);\n\tmsg_unlock(msq);\n\n\ttmp = msq->q_messages.next;\n\twhile (tmp != &msq->q_messages) {\n\t\tstruct msg_msg *msg = list_entry(tmp, struct msg_msg, m_list);\n\n\t\ttmp = tmp->next;\n\t\tatomic_dec(&ns->msg_hdrs);\n\t\tfree_msg(msg);\n\t}\n\tatomic_sub(msq->q_cbytes, &ns->msg_bytes);\n\tsecurity_msg_queue_free(msq);\n\tipc_rcu_putref(msq);\n}\n\n/*\n * Called with msg_ids.rw_mutex and ipcp locked.\n */\nstatic inline int msg_security(struct kern_ipc_perm *ipcp, int msgflg)\n{\n\tstruct msg_queue *msq = container_of(ipcp, struct msg_queue, q_perm);\n\n\treturn security_msg_queue_associate(msq, msgflg);\n}\n\nSYSCALL_DEFINE2(msgget, key_t, key, int, msgflg)\n{\n\tstruct ipc_namespace *ns;\n\tstruct ipc_ops msg_ops;\n\tstruct ipc_params msg_params;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tmsg_ops.getnew = newque;\n\tmsg_ops.associate = msg_security;\n\tmsg_ops.more_checks = NULL;\n\n\tmsg_params.key = key;\n\tmsg_params.flg = msgflg;\n\n\treturn ipcget(ns, &msg_ids(ns), &msg_ops, &msg_params);\n}\n\nstatic inline unsigned long\ncopy_msqid_to_user(void __user *buf, struct msqid64_ds *in, int version)\n{\n\tswitch(version) {\n\tcase IPC_64:\n\t\treturn copy_to_user(buf, in, sizeof(*in));\n\tcase IPC_OLD:\n\t{\n\t\tstruct msqid_ds out;\n\n\t\tmemset(&out, 0, sizeof(out));\n\n\t\tipc64_perm_to_ipc_perm(&in->msg_perm, &out.msg_perm);\n\n\t\tout.msg_stime\t\t= in->msg_stime;\n\t\tout.msg_rtime\t\t= in->msg_rtime;\n\t\tout.msg_ctime\t\t= in->msg_ctime;\n\n\t\tif (in->msg_cbytes > USHRT_MAX)\n\t\t\tout.msg_cbytes\t= USHRT_MAX;\n\t\telse\n\t\t\tout.msg_cbytes\t= in->msg_cbytes;\n\t\tout.msg_lcbytes\t\t= in->msg_cbytes;\n\n\t\tif (in->msg_qnum > USHRT_MAX)\n\t\t\tout.msg_qnum\t= USHRT_MAX;\n\t\telse\n\t\t\tout.msg_qnum\t= in->msg_qnum;\n\n\t\tif (in->msg_qbytes > USHRT_MAX)\n\t\t\tout.msg_qbytes\t= USHRT_MAX;\n\t\telse\n\t\t\tout.msg_qbytes\t= in->msg_qbytes;\n\t\tout.msg_lqbytes\t\t= in->msg_qbytes;\n\n\t\tout.msg_lspid\t\t= in->msg_lspid;\n\t\tout.msg_lrpid\t\t= in->msg_lrpid;\n\n\t\treturn copy_to_user(buf, &out, sizeof(out));\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic inline unsigned long\ncopy_msqid_from_user(struct msqid64_ds *out, void __user *buf, int version)\n{\n\tswitch(version) {\n\tcase IPC_64:\n\t\tif (copy_from_user(out, buf, sizeof(*out)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase IPC_OLD:\n\t{\n\t\tstruct msqid_ds tbuf_old;\n\n\t\tif (copy_from_user(&tbuf_old, buf, sizeof(tbuf_old)))\n\t\t\treturn -EFAULT;\n\n\t\tout->msg_perm.uid      \t= tbuf_old.msg_perm.uid;\n\t\tout->msg_perm.gid      \t= tbuf_old.msg_perm.gid;\n\t\tout->msg_perm.mode     \t= tbuf_old.msg_perm.mode;\n\n\t\tif (tbuf_old.msg_qbytes == 0)\n\t\t\tout->msg_qbytes\t= tbuf_old.msg_lqbytes;\n\t\telse\n\t\t\tout->msg_qbytes\t= tbuf_old.msg_qbytes;\n\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n/*\n * This function handles some msgctl commands which require the rw_mutex\n * to be held in write mode.\n * NOTE: no locks must be held, the rw_mutex is taken inside this function.\n */\nstatic int msgctl_down(struct ipc_namespace *ns, int msqid, int cmd,\n\t\t       struct msqid_ds __user *buf, int version)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct msqid64_ds uninitialized_var(msqid64);\n\tstruct msg_queue *msq;\n\tint err;\n\n\tif (cmd == IPC_SET) {\n\t\tif (copy_msqid_from_user(&msqid64, buf, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down(ns, &msg_ids(ns), msqid, cmd,\n\t\t\t       &msqid64.msg_perm, msqid64.msg_qbytes);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tmsq = container_of(ipcp, struct msg_queue, q_perm);\n\n\terr = security_msg_queue_msgctl(msq, cmd);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tswitch (cmd) {\n\tcase IPC_RMID:\n\t\tfreeque(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tif (msqid64.msg_qbytes > ns->msg_ctlmnb &&\n\t\t    !capable(CAP_SYS_RESOURCE)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = ipc_update_perm(&msqid64.msg_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\n\t\tmsq->q_qbytes = msqid64.msg_qbytes;\n\n\t\tmsq->q_ctime = get_seconds();\n\t\t/* sleeping receivers might be excluded by\n\t\t * stricter permissions.\n\t\t */\n\t\texpunge_all(msq, -EAGAIN);\n\t\t/* sleeping senders might be able to send\n\t\t * due to a larger queue size.\n\t\t */\n\t\tss_wakeup(&msq->q_senders, 0);\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t}\nout_unlock:\n\tmsg_unlock(msq);\nout_up:\n\tup_write(&msg_ids(ns).rw_mutex);\n\treturn err;\n}\n\nSYSCALL_DEFINE3(msgctl, int, msqid, int, cmd, struct msqid_ds __user *, buf)\n{\n\tstruct msg_queue *msq;\n\tint err, version;\n\tstruct ipc_namespace *ns;\n\n\tif (msqid < 0 || cmd < 0)\n\t\treturn -EINVAL;\n\n\tversion = ipc_parse_version(&cmd);\n\tns = current->nsproxy->ipc_ns;\n\n\tswitch (cmd) {\n\tcase IPC_INFO:\n\tcase MSG_INFO:\n\t{\n\t\tstruct msginfo msginfo;\n\t\tint max_id;\n\n\t\tif (!buf)\n\t\t\treturn -EFAULT;\n\t\t/*\n\t\t * We must not return kernel stack data.\n\t\t * due to padding, it's not enough\n\t\t * to set all member fields.\n\t\t */\n\t\terr = security_msg_queue_msgctl(NULL, cmd);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tmemset(&msginfo, 0, sizeof(msginfo));\n\t\tmsginfo.msgmni = ns->msg_ctlmni;\n\t\tmsginfo.msgmax = ns->msg_ctlmax;\n\t\tmsginfo.msgmnb = ns->msg_ctlmnb;\n\t\tmsginfo.msgssz = MSGSSZ;\n\t\tmsginfo.msgseg = MSGSEG;\n\t\tdown_read(&msg_ids(ns).rw_mutex);\n\t\tif (cmd == MSG_INFO) {\n\t\t\tmsginfo.msgpool = msg_ids(ns).in_use;\n\t\t\tmsginfo.msgmap = atomic_read(&ns->msg_hdrs);\n\t\t\tmsginfo.msgtql = atomic_read(&ns->msg_bytes);\n\t\t} else {\n\t\t\tmsginfo.msgmap = MSGMAP;\n\t\t\tmsginfo.msgpool = MSGPOOL;\n\t\t\tmsginfo.msgtql = MSGTQL;\n\t\t}\n\t\tmax_id = ipc_get_maxid(&msg_ids(ns));\n\t\tup_read(&msg_ids(ns).rw_mutex);\n\t\tif (copy_to_user(buf, &msginfo, sizeof(struct msginfo)))\n\t\t\treturn -EFAULT;\n\t\treturn (max_id < 0) ? 0 : max_id;\n\t}\n\tcase MSG_STAT:\t/* msqid is an index rather than a msg queue id */\n\tcase IPC_STAT:\n\t{\n\t\tstruct msqid64_ds tbuf;\n\t\tint success_return;\n\n\t\tif (!buf)\n\t\t\treturn -EFAULT;\n\n\t\tif (cmd == MSG_STAT) {\n\t\t\tmsq = msg_lock(ns, msqid);\n\t\t\tif (IS_ERR(msq))\n\t\t\t\treturn PTR_ERR(msq);\n\t\t\tsuccess_return = msq->q_perm.id;\n\t\t} else {\n\t\t\tmsq = msg_lock_check(ns, msqid);\n\t\t\tif (IS_ERR(msq))\n\t\t\t\treturn PTR_ERR(msq);\n\t\t\tsuccess_return = 0;\n\t\t}\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IRUGO))\n\t\t\tgoto out_unlock;\n\n\t\terr = security_msg_queue_msgctl(msq, cmd);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\n\t\tmemset(&tbuf, 0, sizeof(tbuf));\n\n\t\tkernel_to_ipc64_perm(&msq->q_perm, &tbuf.msg_perm);\n\t\ttbuf.msg_stime  = msq->q_stime;\n\t\ttbuf.msg_rtime  = msq->q_rtime;\n\t\ttbuf.msg_ctime  = msq->q_ctime;\n\t\ttbuf.msg_cbytes = msq->q_cbytes;\n\t\ttbuf.msg_qnum   = msq->q_qnum;\n\t\ttbuf.msg_qbytes = msq->q_qbytes;\n\t\ttbuf.msg_lspid  = msq->q_lspid;\n\t\ttbuf.msg_lrpid  = msq->q_lrpid;\n\t\tmsg_unlock(msq);\n\t\tif (copy_msqid_to_user(buf, &tbuf, version))\n\t\t\treturn -EFAULT;\n\t\treturn success_return;\n\t}\n\tcase IPC_SET:\n\tcase IPC_RMID:\n\t\terr = msgctl_down(ns, msqid, cmd, buf, version);\n\t\treturn err;\n\tdefault:\n\t\treturn  -EINVAL;\n\t}\n\nout_unlock:\n\tmsg_unlock(msq);\n\treturn err;\n}\n\nstatic int testmsg(struct msg_msg *msg, long type, int mode)\n{\n\tswitch(mode)\n\t{\n\t\tcase SEARCH_ANY:\n\t\tcase SEARCH_NUMBER:\n\t\t\treturn 1;\n\t\tcase SEARCH_LESSEQUAL:\n\t\t\tif (msg->m_type <=type)\n\t\t\t\treturn 1;\n\t\t\tbreak;\n\t\tcase SEARCH_EQUAL:\n\t\t\tif (msg->m_type == type)\n\t\t\t\treturn 1;\n\t\t\tbreak;\n\t\tcase SEARCH_NOTEQUAL:\n\t\t\tif (msg->m_type != type)\n\t\t\t\treturn 1;\n\t\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic inline int pipelined_send(struct msg_queue *msq, struct msg_msg *msg)\n{\n\tstruct list_head *tmp;\n\n\ttmp = msq->q_receivers.next;\n\twhile (tmp != &msq->q_receivers) {\n\t\tstruct msg_receiver *msr;\n\n\t\tmsr = list_entry(tmp, struct msg_receiver, r_list);\n\t\ttmp = tmp->next;\n\t\tif (testmsg(msg, msr->r_msgtype, msr->r_mode) &&\n\t\t    !security_msg_queue_msgrcv(msq, msg, msr->r_tsk,\n\t\t\t\t\t       msr->r_msgtype, msr->r_mode)) {\n\n\t\t\tlist_del(&msr->r_list);\n\t\t\tif (msr->r_maxsize < msg->m_ts) {\n\t\t\t\tmsr->r_msg = NULL;\n\t\t\t\twake_up_process(msr->r_tsk);\n\t\t\t\tsmp_mb();\n\t\t\t\tmsr->r_msg = ERR_PTR(-E2BIG);\n\t\t\t} else {\n\t\t\t\tmsr->r_msg = NULL;\n\t\t\t\tmsq->q_lrpid = task_pid_vnr(msr->r_tsk);\n\t\t\t\tmsq->q_rtime = get_seconds();\n\t\t\t\twake_up_process(msr->r_tsk);\n\t\t\t\tsmp_mb();\n\t\t\t\tmsr->r_msg = msg;\n\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nlong do_msgsnd(int msqid, long mtype, void __user *mtext,\n\t\tsize_t msgsz, int msgflg)\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint err;\n\tstruct ipc_namespace *ns;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msgsz > ns->msg_ctlmax || (long) msgsz < 0 || msqid < 0)\n\t\treturn -EINVAL;\n\tif (mtype < 1)\n\t\treturn -EINVAL;\n\n\tmsg = load_msg(mtext, msgsz);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\n\tmsg->m_type = mtype;\n\tmsg->m_ts = msgsz;\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\terr = PTR_ERR(msq);\n\t\tgoto out_free;\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_sender s;\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IWUGO))\n\t\t\tgoto out_unlock_free;\n\n\t\terr = security_msg_queue_msgsnd(msq, msg, msgflg);\n\t\tif (err)\n\t\t\tgoto out_unlock_free;\n\n\t\tif (msgsz + msq->q_cbytes <= msq->q_qbytes &&\n\t\t\t\t1 + msq->q_qnum <= msq->q_qbytes) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* queue full, wait: */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_add(msq, &s);\n\t\tipc_rcu_getref(msq);\n\t\tmsg_unlock(msq);\n\t\tschedule();\n\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\tipc_rcu_putref(msq);\n\t\tif (msq->q_perm.deleted) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_del(&s);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = -ERESTARTNOHAND;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t}\n\n\tmsq->q_lspid = task_tgid_vnr(current);\n\tmsq->q_stime = get_seconds();\n\n\tif (!pipelined_send(msq, msg)) {\n\t\t/* no one is waiting for this message, enqueue it */\n\t\tlist_add_tail(&msg->m_list, &msq->q_messages);\n\t\tmsq->q_cbytes += msgsz;\n\t\tmsq->q_qnum++;\n\t\tatomic_add(msgsz, &ns->msg_bytes);\n\t\tatomic_inc(&ns->msg_hdrs);\n\t}\n\n\terr = 0;\n\tmsg = NULL;\n\nout_unlock_free:\n\tmsg_unlock(msq);\nout_free:\n\tif (msg != NULL)\n\t\tfree_msg(msg);\n\treturn err;\n}\n\nSYSCALL_DEFINE4(msgsnd, int, msqid, struct msgbuf __user *, msgp, size_t, msgsz,\n\t\tint, msgflg)\n{\n\tlong mtype;\n\n\tif (get_user(mtype, &msgp->mtype))\n\t\treturn -EFAULT;\n\treturn do_msgsnd(msqid, mtype, msgp->mtext, msgsz, msgflg);\n}\n\nstatic inline int convert_mode(long *msgtyp, int msgflg)\n{\n\tif (msgflg & MSG_COPY)\n\t\treturn SEARCH_NUMBER;\n\t/*\n\t *  find message of correct type.\n\t *  msgtyp = 0 => get first.\n\t *  msgtyp > 0 => get first message of matching type.\n\t *  msgtyp < 0 => get message with least type must be < abs(msgtype).\n\t */\n\tif (*msgtyp == 0)\n\t\treturn SEARCH_ANY;\n\tif (*msgtyp < 0) {\n\t\t*msgtyp = -*msgtyp;\n\t\treturn SEARCH_LESSEQUAL;\n\t}\n\tif (msgflg & MSG_EXCEPT)\n\t\treturn SEARCH_NOTEQUAL;\n\treturn SEARCH_EQUAL;\n}\n\nstatic long do_msg_fill(void __user *dest, struct msg_msg *msg, size_t bufsz)\n{\n\tstruct msgbuf __user *msgp = dest;\n\tsize_t msgsz;\n\n\tif (put_user(msg->m_type, &msgp->mtype))\n\t\treturn -EFAULT;\n\n\tmsgsz = (bufsz > msg->m_ts) ? msg->m_ts : bufsz;\n\tif (store_msg(msgp->mtext, msg, msgsz))\n\t\treturn -EFAULT;\n\treturn msgsz;\n}\n\n#ifdef CONFIG_CHECKPOINT_RESTORE\n/*\n * This function creates new kernel message structure, large enough to store\n * bufsz message bytes.\n */\nstatic inline struct msg_msg *prepare_copy(void __user *buf, size_t bufsz)\n{\n\tstruct msg_msg *copy;\n\n\t/*\n\t * Create dummy message to copy real message to.\n\t */\n\tcopy = load_msg(buf, bufsz);\n\tif (!IS_ERR(copy))\n\t\tcopy->m_ts = bufsz;\n\treturn copy;\n}\n\nstatic inline void free_copy(struct msg_msg *copy)\n{\n\tif (copy)\n\t\tfree_msg(copy);\n}\n#else\nstatic inline struct msg_msg *prepare_copy(void __user *buf, size_t bufsz)\n{\n\treturn ERR_PTR(-ENOSYS);\n}\n\nstatic inline void free_copy(struct msg_msg *copy)\n{\n}\n#endif\n\nstatic struct msg_msg *find_msg(struct msg_queue *msq, long *msgtyp, int mode)\n{\n\tstruct msg_msg *msg;\n\tlong count = 0;\n\n\tlist_for_each_entry(msg, &msq->q_messages, m_list) {\n\t\tif (testmsg(msg, *msgtyp, mode) &&\n\t\t    !security_msg_queue_msgrcv(msq, msg, current,\n\t\t\t\t\t       *msgtyp, mode)) {\n\t\t\tif (mode == SEARCH_LESSEQUAL && msg->m_type != 1) {\n\t\t\t\t*msgtyp = msg->m_type - 1;\n\t\t\t} else if (mode == SEARCH_NUMBER) {\n\t\t\t\tif (*msgtyp == count)\n\t\t\t\t\treturn msg;\n\t\t\t} else\n\t\t\t\treturn msg;\n\t\t\tcount++;\n\t\t}\n\t}\n\n\treturn ERR_PTR(-EAGAIN);\n}\n\n\nlong do_msgrcv(int msqid, void __user *buf, size_t bufsz, long msgtyp,\n\t       int msgflg,\n\t       long (*msg_handler)(void __user *, struct msg_msg *, size_t))\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint mode;\n\tstruct ipc_namespace *ns;\n\tstruct msg_msg *copy = NULL;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msqid < 0 || (long) bufsz < 0)\n\t\treturn -EINVAL;\n\tif (msgflg & MSG_COPY) {\n\t\tcopy = prepare_copy(buf, min_t(size_t, bufsz, ns->msg_ctlmax));\n\t\tif (IS_ERR(copy))\n\t\t\treturn PTR_ERR(copy);\n\t}\n\tmode = convert_mode(&msgtyp, msgflg);\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\tfree_copy(copy);\n\t\treturn PTR_ERR(msq);\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_receiver msr_d;\n\n\t\tmsg = ERR_PTR(-EACCES);\n\t\tif (ipcperms(ns, &msq->q_perm, S_IRUGO))\n\t\t\tgoto out_unlock;\n\n\t\tmsg = find_msg(msq, &msgtyp, mode);\n\n\t\tif (!IS_ERR(msg)) {\n\t\t\t/*\n\t\t\t * Found a suitable message.\n\t\t\t * Unlink it from the queue.\n\t\t\t */\n\t\t\tif ((bufsz < msg->m_ts) && !(msgflg & MSG_NOERROR)) {\n\t\t\t\tmsg = ERR_PTR(-E2BIG);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\t/*\n\t\t\t * If we are copying, then do not unlink message and do\n\t\t\t * not update queue parameters.\n\t\t\t */\n\t\t\tif (msgflg & MSG_COPY) {\n\t\t\t\tmsg = copy_msg(msg, copy);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tlist_del(&msg->m_list);\n\t\t\tmsq->q_qnum--;\n\t\t\tmsq->q_rtime = get_seconds();\n\t\t\tmsq->q_lrpid = task_tgid_vnr(current);\n\t\t\tmsq->q_cbytes -= msg->m_ts;\n\t\t\tatomic_sub(msg->m_ts, &ns->msg_bytes);\n\t\t\tatomic_dec(&ns->msg_hdrs);\n\t\t\tss_wakeup(&msq->q_senders, 0);\n\t\t\tmsg_unlock(msq);\n\t\t\tbreak;\n\t\t}\n\t\t/* No message waiting. Wait for a message */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\tmsg = ERR_PTR(-ENOMSG);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tlist_add_tail(&msr_d.r_list, &msq->q_receivers);\n\t\tmsr_d.r_tsk = current;\n\t\tmsr_d.r_msgtype = msgtyp;\n\t\tmsr_d.r_mode = mode;\n\t\tif (msgflg & MSG_NOERROR)\n\t\t\tmsr_d.r_maxsize = INT_MAX;\n\t\telse\n\t\t\tmsr_d.r_maxsize = bufsz;\n\t\tmsr_d.r_msg = ERR_PTR(-EAGAIN);\n\t\tcurrent->state = TASK_INTERRUPTIBLE;\n\t\tmsg_unlock(msq);\n\n\t\tschedule();\n\n\t\t/* Lockless receive, part 1:\n\t\t * Disable preemption.  We don't hold a reference to the queue\n\t\t * and getting a reference would defeat the idea of a lockless\n\t\t * operation, thus the code relies on rcu to guarantee the\n\t\t * existence of msq:\n\t\t * Prior to destruction, expunge_all(-EIRDM) changes r_msg.\n\t\t * Thus if r_msg is -EAGAIN, then the queue not yet destroyed.\n\t\t * rcu_read_lock() prevents preemption between reading r_msg\n\t\t * and the spin_lock() inside ipc_lock_by_ptr().\n\t\t */\n\t\trcu_read_lock();\n\n\t\t/* Lockless receive, part 2:\n\t\t * Wait until pipelined_send or expunge_all are outside of\n\t\t * wake_up_process(). There is a race with exit(), see\n\t\t * ipc/mqueue.c for the details.\n\t\t */\n\t\tmsg = (struct msg_msg*)msr_d.r_msg;\n\t\twhile (msg == NULL) {\n\t\t\tcpu_relax();\n\t\t\tmsg = (struct msg_msg *)msr_d.r_msg;\n\t\t}\n\n\t\t/* Lockless receive, part 3:\n\t\t * If there is a message or an error then accept it without\n\t\t * locking.\n\t\t */\n\t\tif (msg != ERR_PTR(-EAGAIN)) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Lockless receive, part 3:\n\t\t * Acquire the queue spinlock.\n\t\t */\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\trcu_read_unlock();\n\n\t\t/* Lockless receive, part 4:\n\t\t * Repeat test after acquiring the spinlock.\n\t\t */\n\t\tmsg = (struct msg_msg*)msr_d.r_msg;\n\t\tif (msg != ERR_PTR(-EAGAIN))\n\t\t\tgoto out_unlock;\n\n\t\tlist_del(&msr_d.r_list);\n\t\tif (signal_pending(current)) {\n\t\t\tmsg = ERR_PTR(-ERESTARTNOHAND);\nout_unlock:\n\t\t\tmsg_unlock(msq);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (IS_ERR(msg)) {\n\t\tfree_copy(copy);\n\t\treturn PTR_ERR(msg);\n\t}\n\n\tbufsz = msg_handler(buf, msg, bufsz);\n\tfree_msg(msg);\n\n\treturn bufsz;\n}\n\nSYSCALL_DEFINE5(msgrcv, int, msqid, struct msgbuf __user *, msgp, size_t, msgsz,\n\t\tlong, msgtyp, int, msgflg)\n{\n\treturn do_msgrcv(msqid, msgp, msgsz, msgtyp, msgflg, do_msg_fill);\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int sysvipc_msg_proc_show(struct seq_file *s, void *it)\n{\n\tstruct user_namespace *user_ns = seq_user_ns(s);\n\tstruct msg_queue *msq = it;\n\n\treturn seq_printf(s,\n\t\t\t\"%10d %10d  %4o  %10lu %10lu %5u %5u %5u %5u %5u %5u %10lu %10lu %10lu\\n\",\n\t\t\tmsq->q_perm.key,\n\t\t\tmsq->q_perm.id,\n\t\t\tmsq->q_perm.mode,\n\t\t\tmsq->q_cbytes,\n\t\t\tmsq->q_qnum,\n\t\t\tmsq->q_lspid,\n\t\t\tmsq->q_lrpid,\n\t\t\tfrom_kuid_munged(user_ns, msq->q_perm.uid),\n\t\t\tfrom_kgid_munged(user_ns, msq->q_perm.gid),\n\t\t\tfrom_kuid_munged(user_ns, msq->q_perm.cuid),\n\t\t\tfrom_kgid_munged(user_ns, msq->q_perm.cgid),\n\t\t\tmsq->q_stime,\n\t\t\tmsq->q_rtime,\n\t\t\tmsq->q_ctime);\n}\n#endif\n", "/*\n * linux/ipc/sem.c\n * Copyright (C) 1992 Krishna Balasubramanian\n * Copyright (C) 1995 Eric Schenk, Bruno Haible\n *\n * /proc/sysvipc/sem support (c) 1999 Dragos Acostachioaie <dragos@iname.com>\n *\n * SMP-threaded, sysctl's added\n * (c) 1999 Manfred Spraul <manfred@colorfullife.com>\n * Enforced range limit on SEM_UNDO\n * (c) 2001 Red Hat Inc\n * Lockless wakeup\n * (c) 2003 Manfred Spraul <manfred@colorfullife.com>\n * Further wakeup optimizations, documentation\n * (c) 2010 Manfred Spraul <manfred@colorfullife.com>\n *\n * support for audit of ipc object properties and permission changes\n * Dustin Kirkland <dustin.kirkland@us.ibm.com>\n *\n * namespaces support\n * OpenVZ, SWsoft Inc.\n * Pavel Emelianov <xemul@openvz.org>\n *\n * Implementation notes: (May 2010)\n * This file implements System V semaphores.\n *\n * User space visible behavior:\n * - FIFO ordering for semop() operations (just FIFO, not starvation\n *   protection)\n * - multiple semaphore operations that alter the same semaphore in\n *   one semop() are handled.\n * - sem_ctime (time of last semctl()) is updated in the IPC_SET, SETVAL and\n *   SETALL calls.\n * - two Linux specific semctl() commands: SEM_STAT, SEM_INFO.\n * - undo adjustments at process exit are limited to 0..SEMVMX.\n * - namespace are supported.\n * - SEMMSL, SEMMNS, SEMOPM and SEMMNI can be configured at runtine by writing\n *   to /proc/sys/kernel/sem.\n * - statistics about the usage are reported in /proc/sysvipc/sem.\n *\n * Internals:\n * - scalability:\n *   - all global variables are read-mostly.\n *   - semop() calls and semctl(RMID) are synchronized by RCU.\n *   - most operations do write operations (actually: spin_lock calls) to\n *     the per-semaphore array structure.\n *   Thus: Perfect SMP scaling between independent semaphore arrays.\n *         If multiple semaphores in one array are used, then cache line\n *         trashing on the semaphore array spinlock will limit the scaling.\n * - semncnt and semzcnt are calculated on demand in count_semncnt() and\n *   count_semzcnt()\n * - the task that performs a successful semop() scans the list of all\n *   sleeping tasks and completes any pending operations that can be fulfilled.\n *   Semaphores are actively given to waiting tasks (necessary for FIFO).\n *   (see update_queue())\n * - To improve the scalability, the actual wake-up calls are performed after\n *   dropping all locks. (see wake_up_sem_queue_prepare(),\n *   wake_up_sem_queue_do())\n * - All work is done by the waker, the woken up task does not have to do\n *   anything - not even acquiring a lock or dropping a refcount.\n * - A woken up task may not even touch the semaphore array anymore, it may\n *   have been destroyed already by a semctl(RMID).\n * - The synchronizations between wake-ups due to a timeout/signal and a\n *   wake-up due to a completed semaphore operation is achieved by using an\n *   intermediate state (IN_WAKEUP).\n * - UNDO values are stored in an array (one per process and per\n *   semaphore array, lazily allocated). For backwards compatibility, multiple\n *   modes for the UNDO variables are supported (per process, per thread)\n *   (see copy_semundo, CLONE_SYSVSEM)\n * - There are two lists of the pending operations: a per-array list\n *   and per-semaphore list (stored in the array). This allows to achieve FIFO\n *   ordering without always scanning all pending operations.\n *   The worst-case behavior is nevertheless O(N^2) for N wakeups.\n */\n\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/proc_fs.h>\n#include <linux/time.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/audit.h>\n#include <linux/capability.h>\n#include <linux/seq_file.h>\n#include <linux/rwsem.h>\n#include <linux/nsproxy.h>\n#include <linux/ipc_namespace.h>\n\n#include <asm/uaccess.h>\n#include \"util.h\"\n\n/* One semaphore structure for each semaphore in the system. */\nstruct sem {\n\tint\tsemval;\t\t/* current value */\n\tint\tsempid;\t\t/* pid of last operation */\n\tstruct list_head sem_pending; /* pending single-sop operations */\n};\n\n/* One queue for each sleeping process in the system. */\nstruct sem_queue {\n\tstruct list_head\tlist;\t /* queue of pending operations */\n\tstruct task_struct\t*sleeper; /* this process */\n\tstruct sem_undo\t\t*undo;\t /* undo structure */\n\tint\t\t\tpid;\t /* process id of requesting process */\n\tint\t\t\tstatus;\t /* completion status of operation */\n\tstruct sembuf\t\t*sops;\t /* array of pending operations */\n\tint\t\t\tnsops;\t /* number of operations */\n\tint\t\t\talter;\t /* does *sops alter the array? */\n};\n\n/* Each task has a list of undo requests. They are executed automatically\n * when the process exits.\n */\nstruct sem_undo {\n\tstruct list_head\tlist_proc;\t/* per-process list: *\n\t\t\t\t\t\t * all undos from one process\n\t\t\t\t\t\t * rcu protected */\n\tstruct rcu_head\t\trcu;\t\t/* rcu struct for sem_undo */\n\tstruct sem_undo_list\t*ulp;\t\t/* back ptr to sem_undo_list */\n\tstruct list_head\tlist_id;\t/* per semaphore array list:\n\t\t\t\t\t\t * all undos for one array */\n\tint\t\t\tsemid;\t\t/* semaphore set identifier */\n\tshort\t\t\t*semadj;\t/* array of adjustments */\n\t\t\t\t\t\t/* one per semaphore */\n};\n\n/* sem_undo_list controls shared access to the list of sem_undo structures\n * that may be shared among all a CLONE_SYSVSEM task group.\n */\nstruct sem_undo_list {\n\tatomic_t\t\trefcnt;\n\tspinlock_t\t\tlock;\n\tstruct list_head\tlist_proc;\n};\n\n\n#define sem_ids(ns)\t((ns)->ids[IPC_SEM_IDS])\n\n#define sem_unlock(sma)\t\tipc_unlock(&(sma)->sem_perm)\n#define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n\nstatic int newary(struct ipc_namespace *, struct ipc_params *);\nstatic void freeary(struct ipc_namespace *, struct kern_ipc_perm *);\n#ifdef CONFIG_PROC_FS\nstatic int sysvipc_sem_proc_show(struct seq_file *s, void *it);\n#endif\n\n#define SEMMSL_FAST\t256 /* 512 bytes on stack */\n#define SEMOPM_FAST\t64  /* ~ 372 bytes on stack */\n\n/*\n * linked list protection:\n *\tsem_undo.id_next,\n *\tsem_array.sem_pending{,last},\n *\tsem_array.sem_undo: sem_lock() for read/write\n *\tsem_undo.proc_next: only \"current\" is allowed to read/write that field.\n *\t\n */\n\n#define sc_semmsl\tsem_ctls[0]\n#define sc_semmns\tsem_ctls[1]\n#define sc_semopm\tsem_ctls[2]\n#define sc_semmni\tsem_ctls[3]\n\nvoid sem_init_ns(struct ipc_namespace *ns)\n{\n\tns->sc_semmsl = SEMMSL;\n\tns->sc_semmns = SEMMNS;\n\tns->sc_semopm = SEMOPM;\n\tns->sc_semmni = SEMMNI;\n\tns->used_sems = 0;\n\tipc_init_ids(&ns->ids[IPC_SEM_IDS]);\n}\n\n#ifdef CONFIG_IPC_NS\nvoid sem_exit_ns(struct ipc_namespace *ns)\n{\n\tfree_ipcs(ns, &sem_ids(ns), freeary);\n\tidr_destroy(&ns->ids[IPC_SEM_IDS].ipcs_idr);\n}\n#endif\n\nvoid __init sem_init (void)\n{\n\tsem_init_ns(&init_ipc_ns);\n\tipc_init_proc_interface(\"sysvipc/sem\",\n\t\t\t\t\"       key      semid perms      nsems   uid   gid  cuid  cgid      otime      ctime\\n\",\n\t\t\t\tIPC_SEM_IDS, sysvipc_sem_proc_show);\n}\n\n/*\n * sem_lock_(check_) routines are called in the paths where the rw_mutex\n * is not held.\n */\nstatic inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct sem_array *sma;\n\n\trcu_read_lock();\n\tipcp = ipc_obtain_object(&sem_ids(ns), id);\n\tif (IS_ERR(ipcp)) {\n\t\tsma = ERR_CAST(ipcp);\n\t\tgoto err;\n\t}\n\n\tspin_lock(&ipcp->lock);\n\n\t/* ipc_rmid() may have already freed the ID while sem_lock\n\t * was spinning: verify that the structure is still valid\n\t */\n\tif (!ipcp->deleted)\n\t\treturn container_of(ipcp, struct sem_array, sem_perm);\n\n\tspin_unlock(&ipcp->lock);\n\tsma = ERR_PTR(-EINVAL);\nerr:\n\trcu_read_unlock();\n\treturn sma;\n}\n\nstatic inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}\n\nstatic inline struct sem_array *sem_lock_check(struct ipc_namespace *ns,\n\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_lock_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}\n\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}\n\nstatic inline void sem_lock_and_putref(struct sem_array *sma)\n{\n\tipc_lock_by_ptr(&sma->sem_perm);\n\tipc_rcu_putref(sma);\n}\n\nstatic inline void sem_getref_and_unlock(struct sem_array *sma)\n{\n\tipc_rcu_getref(sma);\n\tipc_unlock(&(sma)->sem_perm);\n}\n\nstatic inline void sem_putref(struct sem_array *sma)\n{\n\tipc_lock_by_ptr(&sma->sem_perm);\n\tipc_rcu_putref(sma);\n\tipc_unlock(&(sma)->sem_perm);\n}\n\n/*\n * Call inside the rcu read section.\n */\nstatic inline void sem_getref(struct sem_array *sma)\n{\n\tspin_lock(&(sma)->sem_perm.lock);\n\tipc_rcu_getref(sma);\n\tipc_unlock(&(sma)->sem_perm);\n}\n\nstatic inline void sem_rmid(struct ipc_namespace *ns, struct sem_array *s)\n{\n\tipc_rmid(&sem_ids(ns), &s->sem_perm);\n}\n\n/*\n * Lockless wakeup algorithm:\n * Without the check/retry algorithm a lockless wakeup is possible:\n * - queue.status is initialized to -EINTR before blocking.\n * - wakeup is performed by\n *\t* unlinking the queue entry from sma->sem_pending\n *\t* setting queue.status to IN_WAKEUP\n *\t  This is the notification for the blocked thread that a\n *\t  result value is imminent.\n *\t* call wake_up_process\n *\t* set queue.status to the final value.\n * - the previously blocked thread checks queue.status:\n *   \t* if it's IN_WAKEUP, then it must wait until the value changes\n *   \t* if it's not -EINTR, then the operation was completed by\n *   \t  update_queue. semtimedop can return queue.status without\n *   \t  performing any operation on the sem array.\n *   \t* otherwise it must acquire the spinlock and check what's up.\n *\n * The two-stage algorithm is necessary to protect against the following\n * races:\n * - if queue.status is set after wake_up_process, then the woken up idle\n *   thread could race forward and try (and fail) to acquire sma->lock\n *   before update_queue had a chance to set queue.status\n * - if queue.status is written before wake_up_process and if the\n *   blocked process is woken up by a signal between writing\n *   queue.status and the wake_up_process, then the woken up\n *   process could return from semtimedop and die by calling\n *   sys_exit before wake_up_process is called. Then wake_up_process\n *   will oops, because the task structure is already invalid.\n *   (yes, this happened on s390 with sysv msg).\n *\n */\n#define IN_WAKEUP\t1\n\n/**\n * newary - Create a new semaphore set\n * @ns: namespace\n * @params: ptr to the structure that contains key, semflg and nsems\n *\n * Called with sem_ids.rw_mutex held (as a writer)\n */\n\nstatic int newary(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tint id;\n\tint retval;\n\tstruct sem_array *sma;\n\tint size;\n\tkey_t key = params->key;\n\tint nsems = params->u.nsems;\n\tint semflg = params->flg;\n\tint i;\n\n\tif (!nsems)\n\t\treturn -EINVAL;\n\tif (ns->used_sems + nsems > ns->sc_semmns)\n\t\treturn -ENOSPC;\n\n\tsize = sizeof (*sma) + nsems * sizeof (struct sem);\n\tsma = ipc_rcu_alloc(size);\n\tif (!sma) {\n\t\treturn -ENOMEM;\n\t}\n\tmemset (sma, 0, size);\n\n\tsma->sem_perm.mode = (semflg & S_IRWXUGO);\n\tsma->sem_perm.key = key;\n\n\tsma->sem_perm.security = NULL;\n\tretval = security_sem_alloc(sma);\n\tif (retval) {\n\t\tipc_rcu_putref(sma);\n\t\treturn retval;\n\t}\n\n\tid = ipc_addid(&sem_ids(ns), &sma->sem_perm, ns->sc_semmni);\n\tif (id < 0) {\n\t\tsecurity_sem_free(sma);\n\t\tipc_rcu_putref(sma);\n\t\treturn id;\n\t}\n\tns->used_sems += nsems;\n\n\tsma->sem_base = (struct sem *) &sma[1];\n\n\tfor (i = 0; i < nsems; i++)\n\t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n\n\tsma->complex_count = 0;\n\tINIT_LIST_HEAD(&sma->sem_pending);\n\tINIT_LIST_HEAD(&sma->list_id);\n\tsma->sem_nsems = nsems;\n\tsma->sem_ctime = get_seconds();\n\tsem_unlock(sma);\n\n\treturn sma->sem_perm.id;\n}\n\n\n/*\n * Called with sem_ids.rw_mutex and ipcp locked.\n */\nstatic inline int sem_security(struct kern_ipc_perm *ipcp, int semflg)\n{\n\tstruct sem_array *sma;\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\treturn security_sem_associate(sma, semflg);\n}\n\n/*\n * Called with sem_ids.rw_mutex and ipcp locked.\n */\nstatic inline int sem_more_checks(struct kern_ipc_perm *ipcp,\n\t\t\t\tstruct ipc_params *params)\n{\n\tstruct sem_array *sma;\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\tif (params->u.nsems > sma->sem_nsems)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nSYSCALL_DEFINE3(semget, key_t, key, int, nsems, int, semflg)\n{\n\tstruct ipc_namespace *ns;\n\tstruct ipc_ops sem_ops;\n\tstruct ipc_params sem_params;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsems < 0 || nsems > ns->sc_semmsl)\n\t\treturn -EINVAL;\n\n\tsem_ops.getnew = newary;\n\tsem_ops.associate = sem_security;\n\tsem_ops.more_checks = sem_more_checks;\n\n\tsem_params.key = key;\n\tsem_params.flg = semflg;\n\tsem_params.u.nsems = nsems;\n\n\treturn ipcget(ns, &sem_ids(ns), &sem_ops, &sem_params);\n}\n\n/*\n * Determine whether a sequence of semaphore operations would succeed\n * all at once. Return 0 if yes, 1 if need to sleep, else return error code.\n */\n\nstatic int try_atomic_semop (struct sem_array * sma, struct sembuf * sops,\n\t\t\t     int nsops, struct sem_undo *un, int pid)\n{\n\tint result, sem_op;\n\tstruct sembuf *sop;\n\tstruct sem * curr;\n\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tcurr = sma->sem_base + sop->sem_num;\n\t\tsem_op = sop->sem_op;\n\t\tresult = curr->semval;\n  \n\t\tif (!sem_op && result)\n\t\t\tgoto would_block;\n\n\t\tresult += sem_op;\n\t\tif (result < 0)\n\t\t\tgoto would_block;\n\t\tif (result > SEMVMX)\n\t\t\tgoto out_of_range;\n\t\tif (sop->sem_flg & SEM_UNDO) {\n\t\t\tint undo = un->semadj[sop->sem_num] - sem_op;\n\t\t\t/*\n\t \t\t *\tExceeding the undo range is an error.\n\t\t\t */\n\t\t\tif (undo < (-SEMAEM - 1) || undo > SEMAEM)\n\t\t\t\tgoto out_of_range;\n\t\t}\n\t\tcurr->semval = result;\n\t}\n\n\tsop--;\n\twhile (sop >= sops) {\n\t\tsma->sem_base[sop->sem_num].sempid = pid;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tun->semadj[sop->sem_num] -= sop->sem_op;\n\t\tsop--;\n\t}\n\t\n\treturn 0;\n\nout_of_range:\n\tresult = -ERANGE;\n\tgoto undo;\n\nwould_block:\n\tif (sop->sem_flg & IPC_NOWAIT)\n\t\tresult = -EAGAIN;\n\telse\n\t\tresult = 1;\n\nundo:\n\tsop--;\n\twhile (sop >= sops) {\n\t\tsma->sem_base[sop->sem_num].semval -= sop->sem_op;\n\t\tsop--;\n\t}\n\n\treturn result;\n}\n\n/** wake_up_sem_queue_prepare(q, error): Prepare wake-up\n * @q: queue entry that must be signaled\n * @error: Error value for the signal\n *\n * Prepare the wake-up of the queue entry q.\n */\nstatic void wake_up_sem_queue_prepare(struct list_head *pt,\n\t\t\t\tstruct sem_queue *q, int error)\n{\n\tif (list_empty(pt)) {\n\t\t/*\n\t\t * Hold preempt off so that we don't get preempted and have the\n\t\t * wakee busy-wait until we're scheduled back on.\n\t\t */\n\t\tpreempt_disable();\n\t}\n\tq->status = IN_WAKEUP;\n\tq->pid = error;\n\n\tlist_add_tail(&q->list, pt);\n}\n\n/**\n * wake_up_sem_queue_do(pt) - do the actual wake-up\n * @pt: list of tasks to be woken up\n *\n * Do the actual wake-up.\n * The function is called without any locks held, thus the semaphore array\n * could be destroyed already and the tasks can disappear as soon as the\n * status is set to the actual return code.\n */\nstatic void wake_up_sem_queue_do(struct list_head *pt)\n{\n\tstruct sem_queue *q, *t;\n\tint did_something;\n\n\tdid_something = !list_empty(pt);\n\tlist_for_each_entry_safe(q, t, pt, list) {\n\t\twake_up_process(q->sleeper);\n\t\t/* q can disappear immediately after writing q->status. */\n\t\tsmp_wmb();\n\t\tq->status = q->pid;\n\t}\n\tif (did_something)\n\t\tpreempt_enable();\n}\n\nstatic void unlink_queue(struct sem_array *sma, struct sem_queue *q)\n{\n\tlist_del(&q->list);\n\tif (q->nsops > 1)\n\t\tsma->complex_count--;\n}\n\n/** check_restart(sma, q)\n * @sma: semaphore array\n * @q: the operation that just completed\n *\n * update_queue is O(N^2) when it restarts scanning the whole queue of\n * waiting operations. Therefore this function checks if the restart is\n * really necessary. It is called after a previously waiting operation\n * was completed.\n */\nstatic int check_restart(struct sem_array *sma, struct sem_queue *q)\n{\n\tstruct sem *curr;\n\tstruct sem_queue *h;\n\n\t/* if the operation didn't modify the array, then no restart */\n\tif (q->alter == 0)\n\t\treturn 0;\n\n\t/* pending complex operations are too difficult to analyse */\n\tif (sma->complex_count)\n\t\treturn 1;\n\n\t/* we were a sleeping complex operation. Too difficult */\n\tif (q->nsops > 1)\n\t\treturn 1;\n\n\tcurr = sma->sem_base + q->sops[0].sem_num;\n\n\t/* No-one waits on this queue */\n\tif (list_empty(&curr->sem_pending))\n\t\treturn 0;\n\n\t/* the new semaphore value */\n\tif (curr->semval) {\n\t\t/* It is impossible that someone waits for the new value:\n\t\t * - q is a previously sleeping simple operation that\n\t\t *   altered the array. It must be a decrement, because\n\t\t *   simple increments never sleep.\n\t\t * - The value is not 0, thus wait-for-zero won't proceed.\n\t\t * - If there are older (higher priority) decrements\n\t\t *   in the queue, then they have observed the original\n\t\t *   semval value and couldn't proceed. The operation\n\t\t *   decremented to value - thus they won't proceed either.\n\t\t */\n\t\tBUG_ON(q->sops[0].sem_op >= 0);\n\t\treturn 0;\n\t}\n\t/*\n\t * semval is 0. Check if there are wait-for-zero semops.\n\t * They must be the first entries in the per-semaphore queue\n\t */\n\th = list_first_entry(&curr->sem_pending, struct sem_queue, list);\n\tBUG_ON(h->nsops != 1);\n\tBUG_ON(h->sops[0].sem_num != q->sops[0].sem_num);\n\n\t/* Yes, there is a wait-for-zero semop. Restart */\n\tif (h->sops[0].sem_op == 0)\n\t\treturn 1;\n\n\t/* Again - no-one is waiting for the new value. */\n\treturn 0;\n}\n\n\n/**\n * update_queue(sma, semnum): Look for tasks that can be completed.\n * @sma: semaphore array.\n * @semnum: semaphore that was modified.\n * @pt: list head for the tasks that must be woken up.\n *\n * update_queue must be called after a semaphore in a semaphore array\n * was modified. If multiple semaphores were modified, update_queue must\n * be called with semnum = -1, as well as with the number of each modified\n * semaphore.\n * The tasks that must be woken up are added to @pt. The return code\n * is stored in q->pid.\n * The function return 1 if at least one semop was completed successfully.\n */\nstatic int update_queue(struct sem_array *sma, int semnum, struct list_head *pt)\n{\n\tstruct sem_queue *q;\n\tstruct list_head *walk;\n\tstruct list_head *pending_list;\n\tint semop_completed = 0;\n\n\tif (semnum == -1)\n\t\tpending_list = &sma->sem_pending;\n\telse\n\t\tpending_list = &sma->sem_base[semnum].sem_pending;\n\nagain:\n\twalk = pending_list->next;\n\twhile (walk != pending_list) {\n\t\tint error, restart;\n\n\t\tq = container_of(walk, struct sem_queue, list);\n\t\twalk = walk->next;\n\n\t\t/* If we are scanning the single sop, per-semaphore list of\n\t\t * one semaphore and that semaphore is 0, then it is not\n\t\t * necessary to scan the \"alter\" entries: simple increments\n\t\t * that affect only one entry succeed immediately and cannot\n\t\t * be in the  per semaphore pending queue, and decrements\n\t\t * cannot be successful if the value is already 0.\n\t\t */\n\t\tif (semnum != -1 && sma->sem_base[semnum].semval == 0 &&\n\t\t\t\tq->alter)\n\t\t\tbreak;\n\n\t\terror = try_atomic_semop(sma, q->sops, q->nsops,\n\t\t\t\t\t q->undo, q->pid);\n\n\t\t/* Does q->sleeper still need to sleep? */\n\t\tif (error > 0)\n\t\t\tcontinue;\n\n\t\tunlink_queue(sma, q);\n\n\t\tif (error) {\n\t\t\trestart = 0;\n\t\t} else {\n\t\t\tsemop_completed = 1;\n\t\t\trestart = check_restart(sma, q);\n\t\t}\n\n\t\twake_up_sem_queue_prepare(pt, q, error);\n\t\tif (restart)\n\t\t\tgoto again;\n\t}\n\treturn semop_completed;\n}\n\n/**\n * do_smart_update(sma, sops, nsops, otime, pt) - optimized update_queue\n * @sma: semaphore array\n * @sops: operations that were performed\n * @nsops: number of operations\n * @otime: force setting otime\n * @pt: list head of the tasks that must be woken up.\n *\n * do_smart_update() does the required called to update_queue, based on the\n * actual changes that were performed on the semaphore array.\n * Note that the function does not do the actual wake-up: the caller is\n * responsible for calling wake_up_sem_queue_do(@pt).\n * It is safe to perform this call after dropping all locks.\n */\nstatic void do_smart_update(struct sem_array *sma, struct sembuf *sops, int nsops,\n\t\t\tint otime, struct list_head *pt)\n{\n\tint i;\n\n\tif (sma->complex_count || sops == NULL) {\n\t\tif (update_queue(sma, -1, pt))\n\t\t\totime = 1;\n\t}\n\n\tif (!sops) {\n\t\t/* No semops; something special is going on. */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tif (update_queue(sma, i, pt))\n\t\t\t\totime = 1;\n\t\t}\n\t\tgoto done;\n\t}\n\n\t/* Check the semaphores that were modified. */\n\tfor (i = 0; i < nsops; i++) {\n\t\tif (sops[i].sem_op > 0 ||\n\t\t\t(sops[i].sem_op < 0 &&\n\t\t\t\tsma->sem_base[sops[i].sem_num].semval == 0))\n\t\t\tif (update_queue(sma, sops[i].sem_num, pt))\n\t\t\t\totime = 1;\n\t}\ndone:\n\tif (otime)\n\t\tsma->sem_otime = get_seconds();\n}\n\n\n/* The following counts are associated to each semaphore:\n *   semncnt        number of tasks waiting on semval being nonzero\n *   semzcnt        number of tasks waiting on semval being zero\n * This model assumes that a task waits on exactly one semaphore.\n * Since semaphore operations are to be performed atomically, tasks actually\n * wait on a whole sequence of semaphores simultaneously.\n * The counts we return here are a rough approximation, but still\n * warrant that semncnt+semzcnt>0 if the task is on the pending queue.\n */\nstatic int count_semncnt (struct sem_array * sma, ushort semnum)\n{\n\tint semncnt;\n\tstruct sem_queue * q;\n\n\tsemncnt = 0;\n\tlist_for_each_entry(q, &sma->sem_pending, list) {\n\t\tstruct sembuf * sops = q->sops;\n\t\tint nsops = q->nsops;\n\t\tint i;\n\t\tfor (i = 0; i < nsops; i++)\n\t\t\tif (sops[i].sem_num == semnum\n\t\t\t    && (sops[i].sem_op < 0)\n\t\t\t    && !(sops[i].sem_flg & IPC_NOWAIT))\n\t\t\t\tsemncnt++;\n\t}\n\treturn semncnt;\n}\n\nstatic int count_semzcnt (struct sem_array * sma, ushort semnum)\n{\n\tint semzcnt;\n\tstruct sem_queue * q;\n\n\tsemzcnt = 0;\n\tlist_for_each_entry(q, &sma->sem_pending, list) {\n\t\tstruct sembuf * sops = q->sops;\n\t\tint nsops = q->nsops;\n\t\tint i;\n\t\tfor (i = 0; i < nsops; i++)\n\t\t\tif (sops[i].sem_num == semnum\n\t\t\t    && (sops[i].sem_op == 0)\n\t\t\t    && !(sops[i].sem_flg & IPC_NOWAIT))\n\t\t\t\tsemzcnt++;\n\t}\n\treturn semzcnt;\n}\n\n/* Free a semaphore set. freeary() is called with sem_ids.rw_mutex locked\n * as a writer and the spinlock for this semaphore set hold. sem_ids.rw_mutex\n * remains locked on exit.\n */\nstatic void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n{\n\tstruct sem_undo *un, *tu;\n\tstruct sem_queue *q, *tq;\n\tstruct sem_array *sma = container_of(ipcp, struct sem_array, sem_perm);\n\tstruct list_head tasks;\n\tint i;\n\n\t/* Free the existing undo structures for this semaphore set.  */\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry_safe(un, tu, &sma->list_id, list_id) {\n\t\tlist_del(&un->list_id);\n\t\tspin_lock(&un->ulp->lock);\n\t\tun->semid = -1;\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&un->ulp->lock);\n\t\tkfree_rcu(un, rcu);\n\t}\n\n\t/* Wake up all pending processes and let them fail with EIDRM. */\n\tINIT_LIST_HEAD(&tasks);\n\tlist_for_each_entry_safe(q, tq, &sma->sem_pending, list) {\n\t\tunlink_queue(sma, q);\n\t\twake_up_sem_queue_prepare(&tasks, q, -EIDRM);\n\t}\n\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\tstruct sem *sem = sma->sem_base + i;\n\t\tlist_for_each_entry_safe(q, tq, &sem->sem_pending, list) {\n\t\t\tunlink_queue(sma, q);\n\t\t\twake_up_sem_queue_prepare(&tasks, q, -EIDRM);\n\t\t}\n\t}\n\n\t/* Remove the semaphore set from the IDR */\n\tsem_rmid(ns, sma);\n\tsem_unlock(sma);\n\n\twake_up_sem_queue_do(&tasks);\n\tns->used_sems -= sma->sem_nsems;\n\tsecurity_sem_free(sma);\n\tipc_rcu_putref(sma);\n}\n\nstatic unsigned long copy_semid_to_user(void __user *buf, struct semid64_ds *in, int version)\n{\n\tswitch(version) {\n\tcase IPC_64:\n\t\treturn copy_to_user(buf, in, sizeof(*in));\n\tcase IPC_OLD:\n\t    {\n\t\tstruct semid_ds out;\n\n\t\tmemset(&out, 0, sizeof(out));\n\n\t\tipc64_perm_to_ipc_perm(&in->sem_perm, &out.sem_perm);\n\n\t\tout.sem_otime\t= in->sem_otime;\n\t\tout.sem_ctime\t= in->sem_ctime;\n\t\tout.sem_nsems\t= in->sem_nsems;\n\n\t\treturn copy_to_user(buf, &out, sizeof(out));\n\t    }\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int semctl_nolock(struct ipc_namespace *ns, int semid,\n\t\t\t int cmd, int version, void __user *p)\n{\n\tint err;\n\tstruct sem_array *sma;\n\n\tswitch(cmd) {\n\tcase IPC_INFO:\n\tcase SEM_INFO:\n\t{\n\t\tstruct seminfo seminfo;\n\t\tint max_id;\n\n\t\terr = security_sem_semctl(NULL, cmd);\n\t\tif (err)\n\t\t\treturn err;\n\t\t\n\t\tmemset(&seminfo,0,sizeof(seminfo));\n\t\tseminfo.semmni = ns->sc_semmni;\n\t\tseminfo.semmns = ns->sc_semmns;\n\t\tseminfo.semmsl = ns->sc_semmsl;\n\t\tseminfo.semopm = ns->sc_semopm;\n\t\tseminfo.semvmx = SEMVMX;\n\t\tseminfo.semmnu = SEMMNU;\n\t\tseminfo.semmap = SEMMAP;\n\t\tseminfo.semume = SEMUME;\n\t\tdown_read(&sem_ids(ns).rw_mutex);\n\t\tif (cmd == SEM_INFO) {\n\t\t\tseminfo.semusz = sem_ids(ns).in_use;\n\t\t\tseminfo.semaem = ns->used_sems;\n\t\t} else {\n\t\t\tseminfo.semusz = SEMUSZ;\n\t\t\tseminfo.semaem = SEMAEM;\n\t\t}\n\t\tmax_id = ipc_get_maxid(&sem_ids(ns));\n\t\tup_read(&sem_ids(ns).rw_mutex);\n\t\tif (copy_to_user(p, &seminfo, sizeof(struct seminfo))) \n\t\t\treturn -EFAULT;\n\t\treturn (max_id < 0) ? 0: max_id;\n\t}\n\tcase IPC_STAT:\n\tcase SEM_STAT:\n\t{\n\t\tstruct semid64_ds tbuf;\n\t\tint id = 0;\n\n\t\tmemset(&tbuf, 0, sizeof(tbuf));\n\n\t\tif (cmd == SEM_STAT) {\n\t\t\trcu_read_lock();\n\t\t\tsma = sem_obtain_object(ns, semid);\n\t\t\tif (IS_ERR(sma)) {\n\t\t\t\terr = PTR_ERR(sma);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tid = sma->sem_perm.id;\n\t\t} else {\n\t\t\trcu_read_lock();\n\t\t\tsma = sem_obtain_object_check(ns, semid);\n\t\t\tif (IS_ERR(sma)) {\n\t\t\t\terr = PTR_ERR(sma);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &sma->sem_perm, S_IRUGO))\n\t\t\tgoto out_unlock;\n\n\t\terr = security_sem_semctl(sma, cmd);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\n\t\tkernel_to_ipc64_perm(&sma->sem_perm, &tbuf.sem_perm);\n\t\ttbuf.sem_otime  = sma->sem_otime;\n\t\ttbuf.sem_ctime  = sma->sem_ctime;\n\t\ttbuf.sem_nsems  = sma->sem_nsems;\n\t\trcu_read_unlock();\n\t\tif (copy_semid_to_user(p, &tbuf, version))\n\t\t\treturn -EFAULT;\n\t\treturn id;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tint nsems;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tsma = sem_lock_check(ns, semid);\n\tif (IS_ERR(sma))\n\t\treturn PTR_ERR(sma);\n\n\tINIT_LIST_HEAD(&tasks);\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO))\n\t\tgoto out_unlock;\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\tif(semnum < 0 || semnum >= nsems)\n\t\tgoto out_unlock;\n\n\tcurr = &sma->sem_base[semnum];\n\n\terr = -ERANGE;\n\tif (val > SEMVMX || val < 0)\n\t\tgoto out_unlock;\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\terr = 0;\nout_unlock:\n\tsem_unlock(sma);\n\twake_up_sem_queue_do(&tasks);\n\treturn err;\n}\n\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\n\t\tspin_lock(&sma->sem_perm.lock);\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tipc_rcu_getref(sma);\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tspin_lock(&sma->sem_perm.lock);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}\n\nstatic inline unsigned long\ncopy_semid_from_user(struct semid64_ds *out, void __user *buf, int version)\n{\n\tswitch(version) {\n\tcase IPC_64:\n\t\tif (copy_from_user(out, buf, sizeof(*out)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase IPC_OLD:\n\t    {\n\t\tstruct semid_ds tbuf_old;\n\n\t\tif(copy_from_user(&tbuf_old, buf, sizeof(tbuf_old)))\n\t\t\treturn -EFAULT;\n\n\t\tout->sem_perm.uid\t= tbuf_old.sem_perm.uid;\n\t\tout->sem_perm.gid\t= tbuf_old.sem_perm.gid;\n\t\tout->sem_perm.mode\t= tbuf_old.sem_perm.mode;\n\n\t\treturn 0;\n\t    }\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n/*\n * This function handles some semctl commands which require the rw_mutex\n * to be held in write mode.\n * NOTE: no locks must be held, the rw_mutex is taken inside this function.\n */\nstatic int semctl_down(struct ipc_namespace *ns, int semid,\n\t\t       int cmd, int version, void __user *p)\n{\n\tstruct sem_array *sma;\n\tint err;\n\tstruct semid64_ds semid64;\n\tstruct kern_ipc_perm *ipcp;\n\n\tif(cmd == IPC_SET) {\n\t\tif (copy_semid_from_user(&semid64, p, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down_nolock(ns, &sem_ids(ns), semid, cmd,\n\t\t\t\t      &semid64.sem_perm, 0);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_unlock;\n\t}\n\n\tswitch(cmd){\n\tcase IPC_RMID:\n\t\tipc_lock_object(&sma->sem_perm);\n\t\tfreeary(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tipc_lock_object(&sma->sem_perm);\n\t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t\tsma->sem_ctime = get_seconds();\n\t\tbreak;\n\tdefault:\n\t\trcu_read_unlock();\n\t\terr = -EINVAL;\n\t\tgoto out_up;\n\t}\n\nout_unlock:\n\tsem_unlock(sma);\nout_up:\n\tup_write(&sem_ids(ns).rw_mutex);\n\treturn err;\n}\n\nSYSCALL_DEFINE4(semctl, int, semid, int, semnum, int, cmd, unsigned long, arg)\n{\n\tint version;\n\tstruct ipc_namespace *ns;\n\tvoid __user *p = (void __user *)arg;\n\n\tif (semid < 0)\n\t\treturn -EINVAL;\n\n\tversion = ipc_parse_version(&cmd);\n\tns = current->nsproxy->ipc_ns;\n\n\tswitch(cmd) {\n\tcase IPC_INFO:\n\tcase SEM_INFO:\n\tcase IPC_STAT:\n\tcase SEM_STAT:\n\t\treturn semctl_nolock(ns, semid, cmd, version, p);\n\tcase GETALL:\n\tcase GETVAL:\n\tcase GETPID:\n\tcase GETNCNT:\n\tcase GETZCNT:\n\tcase SETALL:\n\t\treturn semctl_main(ns, semid, semnum, cmd, p);\n\tcase SETVAL:\n\t\treturn semctl_setval(ns, semid, semnum, arg);\n\tcase IPC_RMID:\n\tcase IPC_SET:\n\t\treturn semctl_down(ns, semid, cmd, version, p);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n/* If the task doesn't already have a undo_list, then allocate one\n * here.  We guarantee there is only one thread using this undo list,\n * and current is THE ONE\n *\n * If this allocation and assignment succeeds, but later\n * portions of this code fail, there is no need to free the sem_undo_list.\n * Just let it stay associated with the task, and it'll be freed later\n * at exit time.\n *\n * This can block, so callers must hold no locks.\n */\nstatic inline int get_undo_list(struct sem_undo_list **undo_listp)\n{\n\tstruct sem_undo_list *undo_list;\n\n\tundo_list = current->sysvsem.undo_list;\n\tif (!undo_list) {\n\t\tundo_list = kzalloc(sizeof(*undo_list), GFP_KERNEL);\n\t\tif (undo_list == NULL)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_init(&undo_list->lock);\n\t\tatomic_set(&undo_list->refcnt, 1);\n\t\tINIT_LIST_HEAD(&undo_list->list_proc);\n\n\t\tcurrent->sysvsem.undo_list = undo_list;\n\t}\n\t*undo_listp = undo_list;\n\treturn 0;\n}\n\nstatic struct sem_undo *__lookup_undo(struct sem_undo_list *ulp, int semid)\n{\n\tstruct sem_undo *un;\n\n\tlist_for_each_entry_rcu(un, &ulp->list_proc, list_proc) {\n\t\tif (un->semid == semid)\n\t\t\treturn un;\n\t}\n\treturn NULL;\n}\n\nstatic struct sem_undo *lookup_undo(struct sem_undo_list *ulp, int semid)\n{\n\tstruct sem_undo *un;\n\n  \tassert_spin_locked(&ulp->lock);\n\n\tun = __lookup_undo(ulp, semid);\n\tif (un) {\n\t\tlist_del_rcu(&un->list_proc);\n\t\tlist_add_rcu(&un->list_proc, &ulp->list_proc);\n\t}\n\treturn un;\n}\n\n/**\n * find_alloc_undo - Lookup (and if not present create) undo array\n * @ns: namespace\n * @semid: semaphore array id\n *\n * The function looks up (and if not present creates) the undo structure.\n * The size of the undo structure depends on the size of the semaphore\n * array, thus the alloc path is not that straightforward.\n * Lifetime-rules: sem_undo is rcu-protected, on success, the function\n * performs a rcu_read_lock().\n */\nstatic struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems;\n\tint error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tipc_rcu_getref(sma);\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma);\nout:\n\treturn un;\n}\n\n\n/**\n * get_queue_result - Retrieve the result code from sem_queue\n * @q: Pointer to queue structure\n *\n * Retrieve the return code from the pending queue. If IN_WAKEUP is found in\n * q->status, then we must loop until the value is replaced with the final\n * value: This may happen if a task is woken up by an unrelated event (e.g.\n * signal) and in parallel the task is woken up by another task because it got\n * the requested semaphores.\n *\n * The function can be called with or without holding the semaphore spinlock.\n */\nstatic int get_queue_result(struct sem_queue *q)\n{\n\tint error;\n\n\terror = q->status;\n\twhile (unlikely(error == IN_WAKEUP)) {\n\t\tcpu_relax();\n\t\terror = q->status;\n\t}\n\n\treturn error;\n}\n\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tif (undos) {\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else\n\t\tun = NULL;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\tif (un)\n\t\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tipc_lock_object(&sma->sem_perm);\n\tif (un) {\n\t\tif (un->semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto out_unlock_free;\n\t\t} else {\n\t\t\t/*\n\t\t\t * rcu lock can be released, \"un\" cannot disappear:\n\t\t\t * - sem_lock is acquired, thus IPC_RMID is\n\t\t\t *   impossible.\n\t\t\t * - exit_sem is impossible, it always operates on\n\t\t\t *   current (or a dead task).\n\t\t\t */\n\n\t\t\trcu_read_unlock();\n\t\t}\n\t}\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid;\n\t\tint i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\t\trcu_read_unlock();\n\n\t\tif (semid == -1)\n\t\t\tbreak;\n\n\t\tsma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);\n\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma))\n\t\t\tcontinue;\n\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}\n\t\t}\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tINIT_LIST_HEAD(&tasks);\n\t\tdo_smart_update(sma, NULL, 0, 1, &tasks);\n\t\tsem_unlock(sma);\n\t\twake_up_sem_queue_do(&tasks);\n\n\t\tkfree_rcu(un, rcu);\n\t}\n\tkfree(ulp);\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int sysvipc_sem_proc_show(struct seq_file *s, void *it)\n{\n\tstruct user_namespace *user_ns = seq_user_ns(s);\n\tstruct sem_array *sma = it;\n\n\treturn seq_printf(s,\n\t\t\t  \"%10d %10d  %4o %10u %5u %5u %5u %5u %10lu %10lu\\n\",\n\t\t\t  sma->sem_perm.key,\n\t\t\t  sma->sem_perm.id,\n\t\t\t  sma->sem_perm.mode,\n\t\t\t  sma->sem_nsems,\n\t\t\t  from_kuid_munged(user_ns, sma->sem_perm.uid),\n\t\t\t  from_kgid_munged(user_ns, sma->sem_perm.gid),\n\t\t\t  from_kuid_munged(user_ns, sma->sem_perm.cuid),\n\t\t\t  from_kgid_munged(user_ns, sma->sem_perm.cgid),\n\t\t\t  sma->sem_otime,\n\t\t\t  sma->sem_ctime);\n}\n#endif\n", "/*\n * linux/ipc/util.c\n * Copyright (C) 1992 Krishna Balasubramanian\n *\n * Sep 1997 - Call suser() last after \"normal\" permission checks so we\n *            get BSD style process accounting right.\n *            Occurs in several places in the IPC code.\n *            Chris Evans, <chris@ferret.lmh.ox.ac.uk>\n * Nov 1999 - ipc helper functions, unified SMP locking\n *\t      Manfred Spraul <manfred@colorfullife.com>\n * Oct 2002 - One lock per IPC id. RCU ipc_free for lock-free grow_ary().\n *            Mingming Cao <cmm@us.ibm.com>\n * Mar 2006 - support for audit of ipc object properties\n *            Dustin Kirkland <dustin.kirkland@us.ibm.com>\n * Jun 2006 - namespaces ssupport\n *            OpenVZ, SWsoft Inc.\n *            Pavel Emelianov <xemul@openvz.org>\n */\n\n#include <linux/mm.h>\n#include <linux/shm.h>\n#include <linux/init.h>\n#include <linux/msg.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/notifier.h>\n#include <linux/capability.h>\n#include <linux/highuid.h>\n#include <linux/security.h>\n#include <linux/rcupdate.h>\n#include <linux/workqueue.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/audit.h>\n#include <linux/nsproxy.h>\n#include <linux/rwsem.h>\n#include <linux/memory.h>\n#include <linux/ipc_namespace.h>\n\n#include <asm/unistd.h>\n\n#include \"util.h\"\n\nstruct ipc_proc_iface {\n\tconst char *path;\n\tconst char *header;\n\tint ids;\n\tint (*show)(struct seq_file *, void *);\n};\n\nstatic void ipc_memory_notifier(struct work_struct *work)\n{\n\tipcns_notify(IPCNS_MEMCHANGED);\n}\n\nstatic int ipc_memory_callback(struct notifier_block *self,\n\t\t\t\tunsigned long action, void *arg)\n{\n\tstatic DECLARE_WORK(ipc_memory_wq, ipc_memory_notifier);\n\n\tswitch (action) {\n\tcase MEM_ONLINE:    /* memory successfully brought online */\n\tcase MEM_OFFLINE:   /* or offline: it's time to recompute msgmni */\n\t\t/*\n\t\t * This is done by invoking the ipcns notifier chain with the\n\t\t * IPC_MEMCHANGED event.\n\t\t * In order not to keep the lock on the hotplug memory chain\n\t\t * for too long, queue a work item that will, when waken up,\n\t\t * activate the ipcns notification chain.\n\t\t * No need to keep several ipc work items on the queue.\n\t\t */\n\t\tif (!work_pending(&ipc_memory_wq))\n\t\t\tschedule_work(&ipc_memory_wq);\n\t\tbreak;\n\tcase MEM_GOING_ONLINE:\n\tcase MEM_GOING_OFFLINE:\n\tcase MEM_CANCEL_ONLINE:\n\tcase MEM_CANCEL_OFFLINE:\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block ipc_memory_nb = {\n\t.notifier_call = ipc_memory_callback,\n\t.priority = IPC_CALLBACK_PRI,\n};\n\n/**\n *\tipc_init\t-\tinitialise IPC subsystem\n *\n *\tThe various system5 IPC resources (semaphores, messages and shared\n *\tmemory) are initialised\n *\tA callback routine is registered into the memory hotplug notifier\n *\tchain: since msgmni scales to lowmem this callback routine will be\n *\tcalled upon successful memory add / remove to recompute msmgni.\n */\n \nstatic int __init ipc_init(void)\n{\n\tsem_init();\n\tmsg_init();\n\tshm_init();\n\tregister_hotmemory_notifier(&ipc_memory_nb);\n\tregister_ipcns_notifier(&init_ipc_ns);\n\treturn 0;\n}\n__initcall(ipc_init);\n\n/**\n *\tipc_init_ids\t\t-\tinitialise IPC identifiers\n *\t@ids: Identifier set\n *\n *\tSet up the sequence range to use for the ipc identifier range (limited\n *\tbelow IPCMNI) then initialise the ids idr.\n */\n \nvoid ipc_init_ids(struct ipc_ids *ids)\n{\n\tinit_rwsem(&ids->rw_mutex);\n\n\tids->in_use = 0;\n\tids->seq = 0;\n\tids->next_id = -1;\n\t{\n\t\tint seq_limit = INT_MAX/SEQ_MULTIPLIER;\n\t\tif (seq_limit > USHRT_MAX)\n\t\t\tids->seq_max = USHRT_MAX;\n\t\t else\n\t\t \tids->seq_max = seq_limit;\n\t}\n\n\tidr_init(&ids->ipcs_idr);\n}\n\n#ifdef CONFIG_PROC_FS\nstatic const struct file_operations sysvipc_proc_fops;\n/**\n *\tipc_init_proc_interface\t-  Create a proc interface for sysipc types using a seq_file interface.\n *\t@path: Path in procfs\n *\t@header: Banner to be printed at the beginning of the file.\n *\t@ids: ipc id table to iterate.\n *\t@show: show routine.\n */\nvoid __init ipc_init_proc_interface(const char *path, const char *header,\n\t\tint ids, int (*show)(struct seq_file *, void *))\n{\n\tstruct proc_dir_entry *pde;\n\tstruct ipc_proc_iface *iface;\n\n\tiface = kmalloc(sizeof(*iface), GFP_KERNEL);\n\tif (!iface)\n\t\treturn;\n\tiface->path\t= path;\n\tiface->header\t= header;\n\tiface->ids\t= ids;\n\tiface->show\t= show;\n\n\tpde = proc_create_data(path,\n\t\t\t       S_IRUGO,        /* world readable */\n\t\t\t       NULL,           /* parent dir */\n\t\t\t       &sysvipc_proc_fops,\n\t\t\t       iface);\n\tif (!pde) {\n\t\tkfree(iface);\n\t}\n}\n#endif\n\n/**\n *\tipc_findkey\t-\tfind a key in an ipc identifier set\t\n *\t@ids: Identifier set\n *\t@key: The key to find\n *\t\n *\tRequires ipc_ids.rw_mutex locked.\n *\tReturns the LOCKED pointer to the ipc structure if found or NULL\n *\tif not.\n *\tIf key is found ipc points to the owning ipc structure\n */\n \nstatic struct kern_ipc_perm *ipc_findkey(struct ipc_ids *ids, key_t key)\n{\n\tstruct kern_ipc_perm *ipc;\n\tint next_id;\n\tint total;\n\n\tfor (total = 0, next_id = 0; total < ids->in_use; next_id++) {\n\t\tipc = idr_find(&ids->ipcs_idr, next_id);\n\n\t\tif (ipc == NULL)\n\t\t\tcontinue;\n\n\t\tif (ipc->key != key) {\n\t\t\ttotal++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tipc_lock_by_ptr(ipc);\n\t\treturn ipc;\n\t}\n\n\treturn NULL;\n}\n\n/**\n *\tipc_get_maxid \t-\tget the last assigned id\n *\t@ids: IPC identifier set\n *\n *\tCalled with ipc_ids.rw_mutex held.\n */\n\nint ipc_get_maxid(struct ipc_ids *ids)\n{\n\tstruct kern_ipc_perm *ipc;\n\tint max_id = -1;\n\tint total, id;\n\n\tif (ids->in_use == 0)\n\t\treturn -1;\n\n\tif (ids->in_use == IPCMNI)\n\t\treturn IPCMNI - 1;\n\n\t/* Look for the last assigned id */\n\ttotal = 0;\n\tfor (id = 0; id < IPCMNI && total < ids->in_use; id++) {\n\t\tipc = idr_find(&ids->ipcs_idr, id);\n\t\tif (ipc != NULL) {\n\t\t\tmax_id = id;\n\t\t\ttotal++;\n\t\t}\n\t}\n\treturn max_id;\n}\n\n/**\n *\tipc_addid \t-\tadd an IPC identifier\n *\t@ids: IPC identifier set\n *\t@new: new IPC permission set\n *\t@size: limit for the number of used ids\n *\n *\tAdd an entry 'new' to the IPC ids idr. The permissions object is\n *\tinitialised and the first free entry is set up and the id assigned\n *\tis returned. The 'new' entry is returned in a locked state on success.\n *\tOn failure the entry is not locked and a negative err-code is returned.\n *\n *\tCalled with ipc_ids.rw_mutex held as a writer.\n */\n \nint ipc_addid(struct ipc_ids* ids, struct kern_ipc_perm* new, int size)\n{\n\tkuid_t euid;\n\tkgid_t egid;\n\tint id;\n\tint next_id = ids->next_id;\n\n\tif (size > IPCMNI)\n\t\tsize = IPCMNI;\n\n\tif (ids->in_use >= size)\n\t\treturn -ENOSPC;\n\n\tidr_preload(GFP_KERNEL);\n\n\tspin_lock_init(&new->lock);\n\tnew->deleted = 0;\n\trcu_read_lock();\n\tspin_lock(&new->lock);\n\n\tid = idr_alloc(&ids->ipcs_idr, new,\n\t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n\t\t       GFP_NOWAIT);\n\tidr_preload_end();\n\tif (id < 0) {\n\t\tspin_unlock(&new->lock);\n\t\trcu_read_unlock();\n\t\treturn id;\n\t}\n\n\tids->in_use++;\n\n\tcurrent_euid_egid(&euid, &egid);\n\tnew->cuid = new->uid = euid;\n\tnew->gid = new->cgid = egid;\n\n\tif (next_id < 0) {\n\t\tnew->seq = ids->seq++;\n\t\tif (ids->seq > ids->seq_max)\n\t\t\tids->seq = 0;\n\t} else {\n\t\tnew->seq = ipcid_to_seqx(next_id);\n\t\tids->next_id = -1;\n\t}\n\n\tnew->id = ipc_buildid(id, new->seq);\n\treturn id;\n}\n\n/**\n *\tipcget_new\t-\tcreate a new ipc object\n *\t@ns: namespace\n *\t@ids: IPC identifer set\n *\t@ops: the actual creation routine to call\n *\t@params: its parameters\n *\n *\tThis routine is called by sys_msgget, sys_semget() and sys_shmget()\n *\twhen the key is IPC_PRIVATE.\n */\nstatic int ipcget_new(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\tstruct ipc_ops *ops, struct ipc_params *params)\n{\n\tint err;\n\n\tdown_write(&ids->rw_mutex);\n\terr = ops->getnew(ns, params);\n\tup_write(&ids->rw_mutex);\n\treturn err;\n}\n\n/**\n *\tipc_check_perms\t-\tcheck security and permissions for an IPC\n *\t@ns: IPC namespace\n *\t@ipcp: ipc permission set\n *\t@ops: the actual security routine to call\n *\t@params: its parameters\n *\n *\tThis routine is called by sys_msgget(), sys_semget() and sys_shmget()\n *      when the key is not IPC_PRIVATE and that key already exists in the\n *      ids IDR.\n *\n *\tOn success, the IPC id is returned.\n *\n *\tIt is called with ipc_ids.rw_mutex and ipcp->lock held.\n */\nstatic int ipc_check_perms(struct ipc_namespace *ns,\n\t\t\t   struct kern_ipc_perm *ipcp,\n\t\t\t   struct ipc_ops *ops,\n\t\t\t   struct ipc_params *params)\n{\n\tint err;\n\n\tif (ipcperms(ns, ipcp, params->flg))\n\t\terr = -EACCES;\n\telse {\n\t\terr = ops->associate(ipcp, params->flg);\n\t\tif (!err)\n\t\t\terr = ipcp->id;\n\t}\n\n\treturn err;\n}\n\n/**\n *\tipcget_public\t-\tget an ipc object or create a new one\n *\t@ns: namespace\n *\t@ids: IPC identifer set\n *\t@ops: the actual creation routine to call\n *\t@params: its parameters\n *\n *\tThis routine is called by sys_msgget, sys_semget() and sys_shmget()\n *\twhen the key is not IPC_PRIVATE.\n *\tIt adds a new entry if the key is not found and does some permission\n *      / security checkings if the key is found.\n *\n *\tOn success, the ipc id is returned.\n */\nstatic int ipcget_public(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\tstruct ipc_ops *ops, struct ipc_params *params)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tint flg = params->flg;\n\tint err;\n\n\t/*\n\t * Take the lock as a writer since we are potentially going to add\n\t * a new entry + read locks are not \"upgradable\"\n\t */\n\tdown_write(&ids->rw_mutex);\n\tipcp = ipc_findkey(ids, params->key);\n\tif (ipcp == NULL) {\n\t\t/* key not used */\n\t\tif (!(flg & IPC_CREAT))\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\terr = ops->getnew(ns, params);\n\t} else {\n\t\t/* ipc object has been locked by ipc_findkey() */\n\n\t\tif (flg & IPC_CREAT && flg & IPC_EXCL)\n\t\t\terr = -EEXIST;\n\t\telse {\n\t\t\terr = 0;\n\t\t\tif (ops->more_checks)\n\t\t\t\terr = ops->more_checks(ipcp, params);\n\t\t\tif (!err)\n\t\t\t\t/*\n\t\t\t\t * ipc_check_perms returns the IPC id on\n\t\t\t\t * success\n\t\t\t\t */\n\t\t\t\terr = ipc_check_perms(ns, ipcp, ops, params);\n\t\t}\n\t\tipc_unlock(ipcp);\n\t}\n\tup_write(&ids->rw_mutex);\n\n\treturn err;\n}\n\n\n/**\n *\tipc_rmid\t-\tremove an IPC identifier\n *\t@ids: IPC identifier set\n *\t@ipcp: ipc perm structure containing the identifier to remove\n *\n *\tipc_ids.rw_mutex (as a writer) and the spinlock for this ID are held\n *\tbefore this function is called, and remain locked on the exit.\n */\n \nvoid ipc_rmid(struct ipc_ids *ids, struct kern_ipc_perm *ipcp)\n{\n\tint lid = ipcid_to_idx(ipcp->id);\n\n\tidr_remove(&ids->ipcs_idr, lid);\n\n\tids->in_use--;\n\n\tipcp->deleted = 1;\n\n\treturn;\n}\n\n/**\n *\tipc_alloc\t-\tallocate ipc space\n *\t@size: size desired\n *\n *\tAllocate memory from the appropriate pools and return a pointer to it.\n *\tNULL is returned if the allocation fails\n */\n \nvoid* ipc_alloc(int size)\n{\n\tvoid* out;\n\tif(size > PAGE_SIZE)\n\t\tout = vmalloc(size);\n\telse\n\t\tout = kmalloc(size, GFP_KERNEL);\n\treturn out;\n}\n\n/**\n *\tipc_free        -       free ipc space\n *\t@ptr: pointer returned by ipc_alloc\n *\t@size: size of block\n *\n *\tFree a block created with ipc_alloc(). The caller must know the size\n *\tused in the allocation call.\n */\n\nvoid ipc_free(void* ptr, int size)\n{\n\tif(size > PAGE_SIZE)\n\t\tvfree(ptr);\n\telse\n\t\tkfree(ptr);\n}\n\n/*\n * rcu allocations:\n * There are three headers that are prepended to the actual allocation:\n * - during use: ipc_rcu_hdr.\n * - during the rcu grace period: ipc_rcu_grace.\n * - [only if vmalloc]: ipc_rcu_sched.\n * Their lifetime doesn't overlap, thus the headers share the same memory.\n * Unlike a normal union, they are right-aligned, thus some container_of\n * forward/backward casting is necessary:\n */\nstruct ipc_rcu_hdr\n{\n\tint refcount;\n\tint is_vmalloc;\n\tvoid *data[0];\n};\n\n\nstruct ipc_rcu_grace\n{\n\tstruct rcu_head rcu;\n\t/* \"void *\" makes sure alignment of following data is sane. */\n\tvoid *data[0];\n};\n\nstruct ipc_rcu_sched\n{\n\tstruct work_struct work;\n\t/* \"void *\" makes sure alignment of following data is sane. */\n\tvoid *data[0];\n};\n\n#define HDRLEN_KMALLOC\t\t(sizeof(struct ipc_rcu_grace) > sizeof(struct ipc_rcu_hdr) ? \\\n\t\t\t\t\tsizeof(struct ipc_rcu_grace) : sizeof(struct ipc_rcu_hdr))\n#define HDRLEN_VMALLOC\t\t(sizeof(struct ipc_rcu_sched) > HDRLEN_KMALLOC ? \\\n\t\t\t\t\tsizeof(struct ipc_rcu_sched) : HDRLEN_KMALLOC)\n\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n *\tipc_rcu_alloc\t-\tallocate ipc and rcu space \n *\t@size: size desired\n *\n *\tAllocate memory for the rcu header structure +  the object.\n *\tReturns the pointer to the object.\n *\tNULL is returned if the allocation fails. \n */\n \nvoid* ipc_rcu_alloc(int size)\n{\n\tvoid* out;\n\t/* \n\t * We prepend the allocation with the rcu struct, and\n\t * workqueue if necessary (for vmalloc). \n\t */\n\tif (rcu_use_vmalloc(size)) {\n\t\tout = vmalloc(HDRLEN_VMALLOC + size);\n\t\tif (out) {\n\t\t\tout += HDRLEN_VMALLOC;\n\t\t\tcontainer_of(out, struct ipc_rcu_hdr, data)->is_vmalloc = 1;\n\t\t\tcontainer_of(out, struct ipc_rcu_hdr, data)->refcount = 1;\n\t\t}\n\t} else {\n\t\tout = kmalloc(HDRLEN_KMALLOC + size, GFP_KERNEL);\n\t\tif (out) {\n\t\t\tout += HDRLEN_KMALLOC;\n\t\t\tcontainer_of(out, struct ipc_rcu_hdr, data)->is_vmalloc = 0;\n\t\t\tcontainer_of(out, struct ipc_rcu_hdr, data)->refcount = 1;\n\t\t}\n\t}\n\n\treturn out;\n}\n\nvoid ipc_rcu_getref(void *ptr)\n{\n\tcontainer_of(ptr, struct ipc_rcu_hdr, data)->refcount++;\n}\n\nstatic void ipc_do_vfree(struct work_struct *work)\n{\n\tvfree(container_of(work, struct ipc_rcu_sched, work));\n}\n\n/**\n * ipc_schedule_free - free ipc + rcu space\n * @head: RCU callback structure for queued work\n * \n * Since RCU callback function is called in bh,\n * we need to defer the vfree to schedule_work().\n */\nstatic void ipc_schedule_free(struct rcu_head *head)\n{\n\tstruct ipc_rcu_grace *grace;\n\tstruct ipc_rcu_sched *sched;\n\n\tgrace = container_of(head, struct ipc_rcu_grace, rcu);\n\tsched = container_of(&(grace->data[0]), struct ipc_rcu_sched,\n\t\t\t\tdata[0]);\n\n\tINIT_WORK(&sched->work, ipc_do_vfree);\n\tschedule_work(&sched->work);\n}\n\nvoid ipc_rcu_putref(void *ptr)\n{\n\tif (--container_of(ptr, struct ipc_rcu_hdr, data)->refcount > 0)\n\t\treturn;\n\n\tif (container_of(ptr, struct ipc_rcu_hdr, data)->is_vmalloc) {\n\t\tcall_rcu(&container_of(ptr, struct ipc_rcu_grace, data)->rcu,\n\t\t\t\tipc_schedule_free);\n\t} else {\n\t\tkfree_rcu(container_of(ptr, struct ipc_rcu_grace, data), rcu);\n\t}\n}\n\n/**\n *\tipcperms\t-\tcheck IPC permissions\n *\t@ns: IPC namespace\n *\t@ipcp: IPC permission set\n *\t@flag: desired permission set.\n *\n *\tCheck user, group, other permissions for access\n *\tto ipc resources. return 0 if allowed\n *\n * \t@flag will most probably be 0 or S_...UGO from <linux/stat.h>\n */\n \nint ipcperms(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp, short flag)\n{\n\tkuid_t euid = current_euid();\n\tint requested_mode, granted_mode;\n\n\taudit_ipc_obj(ipcp);\n\trequested_mode = (flag >> 6) | (flag >> 3) | flag;\n\tgranted_mode = ipcp->mode;\n\tif (uid_eq(euid, ipcp->cuid) ||\n\t    uid_eq(euid, ipcp->uid))\n\t\tgranted_mode >>= 6;\n\telse if (in_group_p(ipcp->cgid) || in_group_p(ipcp->gid))\n\t\tgranted_mode >>= 3;\n\t/* is there some bit set in requested_mode but not in granted_mode? */\n\tif ((requested_mode & ~granted_mode & 0007) && \n\t    !ns_capable(ns->user_ns, CAP_IPC_OWNER))\n\t\treturn -1;\n\n\treturn security_ipc_permission(ipcp, flag);\n}\n\n/*\n * Functions to convert between the kern_ipc_perm structure and the\n * old/new ipc_perm structures\n */\n\n/**\n *\tkernel_to_ipc64_perm\t-\tconvert kernel ipc permissions to user\n *\t@in: kernel permissions\n *\t@out: new style IPC permissions\n *\n *\tTurn the kernel object @in into a set of permissions descriptions\n *\tfor returning to userspace (@out).\n */\n \n\nvoid kernel_to_ipc64_perm (struct kern_ipc_perm *in, struct ipc64_perm *out)\n{\n\tout->key\t= in->key;\n\tout->uid\t= from_kuid_munged(current_user_ns(), in->uid);\n\tout->gid\t= from_kgid_munged(current_user_ns(), in->gid);\n\tout->cuid\t= from_kuid_munged(current_user_ns(), in->cuid);\n\tout->cgid\t= from_kgid_munged(current_user_ns(), in->cgid);\n\tout->mode\t= in->mode;\n\tout->seq\t= in->seq;\n}\n\n/**\n *\tipc64_perm_to_ipc_perm\t-\tconvert new ipc permissions to old\n *\t@in: new style IPC permissions\n *\t@out: old style IPC permissions\n *\n *\tTurn the new style permissions object @in into a compatibility\n *\tobject and store it into the @out pointer.\n */\n \nvoid ipc64_perm_to_ipc_perm (struct ipc64_perm *in, struct ipc_perm *out)\n{\n\tout->key\t= in->key;\n\tSET_UID(out->uid, in->uid);\n\tSET_GID(out->gid, in->gid);\n\tSET_UID(out->cuid, in->cuid);\n\tSET_GID(out->cgid, in->cgid);\n\tout->mode\t= in->mode;\n\tout->seq\t= in->seq;\n}\n\n/**\n * ipc_obtain_object\n * @ids: ipc identifier set\n * @id: ipc id to look for\n *\n * Look for an id in the ipc ids idr and return associated ipc object.\n *\n * Call inside the RCU critical section.\n * The ipc object is *not* locked on exit.\n */\nstruct kern_ipc_perm *ipc_obtain_object(struct ipc_ids *ids, int id)\n{\n\tstruct kern_ipc_perm *out;\n\tint lid = ipcid_to_idx(id);\n\n\tout = idr_find(&ids->ipcs_idr, lid);\n\tif (!out)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn out;\n}\n\n/**\n * ipc_lock - Lock an ipc structure without rw_mutex held\n * @ids: IPC identifier set\n * @id: ipc id to look for\n *\n * Look for an id in the ipc ids idr and lock the associated ipc object.\n *\n * The ipc object is locked on successful exit.\n */\nstruct kern_ipc_perm *ipc_lock(struct ipc_ids *ids, int id)\n{\n\tstruct kern_ipc_perm *out;\n\n\trcu_read_lock();\n\tout = ipc_obtain_object(ids, id);\n\tif (IS_ERR(out))\n\t\tgoto err1;\n\n\tspin_lock(&out->lock);\n\n\t/* ipc_rmid() may have already freed the ID while ipc_lock\n\t * was spinning: here verify that the structure is still valid\n\t */\n\tif (!out->deleted)\n\t\treturn out;\n\n\tspin_unlock(&out->lock);\n\tout = ERR_PTR(-EINVAL);\nerr1:\n\trcu_read_unlock();\n\treturn out;\n}\n\n/**\n * ipc_obtain_object_check\n * @ids: ipc identifier set\n * @id: ipc id to look for\n *\n * Similar to ipc_obtain_object() but also checks\n * the ipc object reference counter.\n *\n * Call inside the RCU critical section.\n * The ipc object is *not* locked on exit.\n */\nstruct kern_ipc_perm *ipc_obtain_object_check(struct ipc_ids *ids, int id)\n{\n\tstruct kern_ipc_perm *out = ipc_obtain_object(ids, id);\n\n\tif (IS_ERR(out))\n\t\tgoto out;\n\n\tif (ipc_checkid(out, id))\n\t\treturn ERR_PTR(-EIDRM);\nout:\n\treturn out;\n}\n\nstruct kern_ipc_perm *ipc_lock_check(struct ipc_ids *ids, int id)\n{\n\tstruct kern_ipc_perm *out;\n\n\tout = ipc_lock(ids, id);\n\tif (IS_ERR(out))\n\t\treturn out;\n\n\tif (ipc_checkid(out, id)) {\n\t\tipc_unlock(out);\n\t\treturn ERR_PTR(-EIDRM);\n\t}\n\n\treturn out;\n}\n\n/**\n * ipcget - Common sys_*get() code\n * @ns : namsepace\n * @ids : IPC identifier set\n * @ops : operations to be called on ipc object creation, permission checks\n *        and further checks\n * @params : the parameters needed by the previous operations.\n *\n * Common routine called by sys_msgget(), sys_semget() and sys_shmget().\n */\nint ipcget(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\t\tstruct ipc_ops *ops, struct ipc_params *params)\n{\n\tif (params->key == IPC_PRIVATE)\n\t\treturn ipcget_new(ns, ids, ops, params);\n\telse\n\t\treturn ipcget_public(ns, ids, ops, params);\n}\n\n/**\n * ipc_update_perm - update the permissions of an IPC.\n * @in:  the permission given as input.\n * @out: the permission of the ipc to set.\n */\nint ipc_update_perm(struct ipc64_perm *in, struct kern_ipc_perm *out)\n{\n\tkuid_t uid = make_kuid(current_user_ns(), in->uid);\n\tkgid_t gid = make_kgid(current_user_ns(), in->gid);\n\tif (!uid_valid(uid) || !gid_valid(gid))\n\t\treturn -EINVAL;\n\n\tout->uid = uid;\n\tout->gid = gid;\n\tout->mode = (out->mode & ~S_IRWXUGO)\n\t\t| (in->mode & S_IRWXUGO);\n\n\treturn 0;\n}\n\n/**\n * ipcctl_pre_down - retrieve an ipc and check permissions for some IPC_XXX cmd\n * @ns:  the ipc namespace\n * @ids:  the table of ids where to look for the ipc\n * @id:   the id of the ipc to retrieve\n * @cmd:  the cmd to check\n * @perm: the permission to set\n * @extra_perm: one extra permission parameter used by msq\n *\n * This function does some common audit and permissions check for some IPC_XXX\n * cmd and is called from semctl_down, shmctl_down and msgctl_down.\n * It must be called without any lock held and\n *  - retrieves the ipc with the given id in the given table.\n *  - performs some audit and permission check, depending on the given cmd\n *  - returns the ipc with both ipc and rw_mutex locks held in case of success\n *    or an err-code without any lock held otherwise.\n */\nstruct kern_ipc_perm *ipcctl_pre_down(struct ipc_namespace *ns,\n\t\t\t\t      struct ipc_ids *ids, int id, int cmd,\n\t\t\t\t      struct ipc64_perm *perm, int extra_perm)\n{\n\tstruct kern_ipc_perm *ipcp;\n\n\tipcp = ipcctl_pre_down_nolock(ns, ids, id, cmd, perm, extra_perm);\n\tif (IS_ERR(ipcp))\n\t\tgoto out;\n\n\tspin_lock(&ipcp->lock);\nout:\n\treturn ipcp;\n}\n\nstruct kern_ipc_perm *ipcctl_pre_down_nolock(struct ipc_namespace *ns,\n\t\t\t\t\t     struct ipc_ids *ids, int id, int cmd,\n\t\t\t\t\t     struct ipc64_perm *perm, int extra_perm)\n{\n\tkuid_t euid;\n\tint err = -EPERM;\n\tstruct kern_ipc_perm *ipcp;\n\n\tdown_write(&ids->rw_mutex);\n\trcu_read_lock();\n\n\tipcp = ipc_obtain_object_check(ids, id);\n\tif (IS_ERR(ipcp)) {\n\t\terr = PTR_ERR(ipcp);\n\t\tgoto out_up;\n\t}\n\n\taudit_ipc_obj(ipcp);\n\tif (cmd == IPC_SET)\n\t\taudit_ipc_set_perm(extra_perm, perm->uid,\n\t\t\t\t   perm->gid, perm->mode);\n\n\teuid = current_euid();\n\tif (uid_eq(euid, ipcp->cuid) || uid_eq(euid, ipcp->uid)  ||\n\t    ns_capable(ns->user_ns, CAP_SYS_ADMIN))\n\t\treturn ipcp;\n\nout_up:\n\t/*\n\t * Unsuccessful lookup, unlock and return\n\t * the corresponding error.\n\t */\n\trcu_read_unlock();\n\tup_write(&ids->rw_mutex);\n\n\treturn ERR_PTR(err);\n}\n\n#ifdef CONFIG_ARCH_WANT_IPC_PARSE_VERSION\n\n\n/**\n *\tipc_parse_version\t-\tIPC call version\n *\t@cmd: pointer to command\n *\n *\tReturn IPC_64 for new style IPC and IPC_OLD for old style IPC. \n *\tThe @cmd value is turned from an encoding command and version into\n *\tjust the command code.\n */\n \nint ipc_parse_version (int *cmd)\n{\n\tif (*cmd & IPC_64) {\n\t\t*cmd ^= IPC_64;\n\t\treturn IPC_64;\n\t} else {\n\t\treturn IPC_OLD;\n\t}\n}\n\n#endif /* CONFIG_ARCH_WANT_IPC_PARSE_VERSION */\n\n#ifdef CONFIG_PROC_FS\nstruct ipc_proc_iter {\n\tstruct ipc_namespace *ns;\n\tstruct ipc_proc_iface *iface;\n};\n\n/*\n * This routine locks the ipc structure found at least at position pos.\n */\nstatic struct kern_ipc_perm *sysvipc_find_ipc(struct ipc_ids *ids, loff_t pos,\n\t\t\t\t\t      loff_t *new_pos)\n{\n\tstruct kern_ipc_perm *ipc;\n\tint total, id;\n\n\ttotal = 0;\n\tfor (id = 0; id < pos && total < ids->in_use; id++) {\n\t\tipc = idr_find(&ids->ipcs_idr, id);\n\t\tif (ipc != NULL)\n\t\t\ttotal++;\n\t}\n\n\tif (total >= ids->in_use)\n\t\treturn NULL;\n\n\tfor ( ; pos < IPCMNI; pos++) {\n\t\tipc = idr_find(&ids->ipcs_idr, pos);\n\t\tif (ipc != NULL) {\n\t\t\t*new_pos = pos + 1;\n\t\t\tipc_lock_by_ptr(ipc);\n\t\t\treturn ipc;\n\t\t}\n\t}\n\n\t/* Out of range - return NULL to terminate iteration */\n\treturn NULL;\n}\n\nstatic void *sysvipc_proc_next(struct seq_file *s, void *it, loff_t *pos)\n{\n\tstruct ipc_proc_iter *iter = s->private;\n\tstruct ipc_proc_iface *iface = iter->iface;\n\tstruct kern_ipc_perm *ipc = it;\n\n\t/* If we had an ipc id locked before, unlock it */\n\tif (ipc && ipc != SEQ_START_TOKEN)\n\t\tipc_unlock(ipc);\n\n\treturn sysvipc_find_ipc(&iter->ns->ids[iface->ids], *pos, pos);\n}\n\n/*\n * File positions: pos 0 -> header, pos n -> ipc id = n - 1.\n * SeqFile iterator: iterator value locked ipc pointer or SEQ_TOKEN_START.\n */\nstatic void *sysvipc_proc_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct ipc_proc_iter *iter = s->private;\n\tstruct ipc_proc_iface *iface = iter->iface;\n\tstruct ipc_ids *ids;\n\n\tids = &iter->ns->ids[iface->ids];\n\n\t/*\n\t * Take the lock - this will be released by the corresponding\n\t * call to stop().\n\t */\n\tdown_read(&ids->rw_mutex);\n\n\t/* pos < 0 is invalid */\n\tif (*pos < 0)\n\t\treturn NULL;\n\n\t/* pos == 0 means header */\n\tif (*pos == 0)\n\t\treturn SEQ_START_TOKEN;\n\n\t/* Find the (pos-1)th ipc */\n\treturn sysvipc_find_ipc(ids, *pos - 1, pos);\n}\n\nstatic void sysvipc_proc_stop(struct seq_file *s, void *it)\n{\n\tstruct kern_ipc_perm *ipc = it;\n\tstruct ipc_proc_iter *iter = s->private;\n\tstruct ipc_proc_iface *iface = iter->iface;\n\tstruct ipc_ids *ids;\n\n\t/* If we had a locked structure, release it */\n\tif (ipc && ipc != SEQ_START_TOKEN)\n\t\tipc_unlock(ipc);\n\n\tids = &iter->ns->ids[iface->ids];\n\t/* Release the lock we took in start() */\n\tup_read(&ids->rw_mutex);\n}\n\nstatic int sysvipc_proc_show(struct seq_file *s, void *it)\n{\n\tstruct ipc_proc_iter *iter = s->private;\n\tstruct ipc_proc_iface *iface = iter->iface;\n\n\tif (it == SEQ_START_TOKEN)\n\t\treturn seq_puts(s, iface->header);\n\n\treturn iface->show(s, it);\n}\n\nstatic const struct seq_operations sysvipc_proc_seqops = {\n\t.start = sysvipc_proc_start,\n\t.stop  = sysvipc_proc_stop,\n\t.next  = sysvipc_proc_next,\n\t.show  = sysvipc_proc_show,\n};\n\nstatic int sysvipc_proc_open(struct inode *inode, struct file *file)\n{\n\tint ret;\n\tstruct seq_file *seq;\n\tstruct ipc_proc_iter *iter;\n\n\tret = -ENOMEM;\n\titer = kmalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter)\n\t\tgoto out;\n\n\tret = seq_open(file, &sysvipc_proc_seqops);\n\tif (ret)\n\t\tgoto out_kfree;\n\n\tseq = file->private_data;\n\tseq->private = iter;\n\n\titer->iface = PDE(inode)->data;\n\titer->ns    = get_ipc_ns(current->nsproxy->ipc_ns);\nout:\n\treturn ret;\nout_kfree:\n\tkfree(iter);\n\tgoto out;\n}\n\nstatic int sysvipc_proc_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct ipc_proc_iter *iter = seq->private;\n\tput_ipc_ns(iter->ns);\n\treturn seq_release_private(inode, file);\n}\n\nstatic const struct file_operations sysvipc_proc_fops = {\n\t.open    = sysvipc_proc_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = sysvipc_proc_release,\n};\n#endif /* CONFIG_PROC_FS */\n", "/*\n * linux/ipc/util.h\n * Copyright (C) 1999 Christoph Rohland\n *\n * ipc helper functions (c) 1999 Manfred Spraul <manfred@colorfullife.com>\n * namespaces support.      2006 OpenVZ, SWsoft Inc.\n *                               Pavel Emelianov <xemul@openvz.org>\n */\n\n#ifndef _IPC_UTIL_H\n#define _IPC_UTIL_H\n\n#include <linux/unistd.h>\n#include <linux/err.h>\n\n#define SEQ_MULTIPLIER\t(IPCMNI)\n\nvoid sem_init (void);\nvoid msg_init (void);\nvoid shm_init (void);\n\nstruct ipc_namespace;\n\n#ifdef CONFIG_POSIX_MQUEUE\nextern void mq_clear_sbinfo(struct ipc_namespace *ns);\nextern void mq_put_mnt(struct ipc_namespace *ns);\n#else\nstatic inline void mq_clear_sbinfo(struct ipc_namespace *ns) { }\nstatic inline void mq_put_mnt(struct ipc_namespace *ns) { }\n#endif\n\n#ifdef CONFIG_SYSVIPC\nvoid sem_init_ns(struct ipc_namespace *ns);\nvoid msg_init_ns(struct ipc_namespace *ns);\nvoid shm_init_ns(struct ipc_namespace *ns);\n\nvoid sem_exit_ns(struct ipc_namespace *ns);\nvoid msg_exit_ns(struct ipc_namespace *ns);\nvoid shm_exit_ns(struct ipc_namespace *ns);\n#else\nstatic inline void sem_init_ns(struct ipc_namespace *ns) { }\nstatic inline void msg_init_ns(struct ipc_namespace *ns) { }\nstatic inline void shm_init_ns(struct ipc_namespace *ns) { }\n\nstatic inline void sem_exit_ns(struct ipc_namespace *ns) { }\nstatic inline void msg_exit_ns(struct ipc_namespace *ns) { }\nstatic inline void shm_exit_ns(struct ipc_namespace *ns) { }\n#endif\n\n/*\n * Structure that holds the parameters needed by the ipc operations\n * (see after)\n */\nstruct ipc_params {\n\tkey_t key;\n\tint flg;\n\tunion {\n\t\tsize_t size;\t/* for shared memories */\n\t\tint nsems;\t/* for semaphores */\n\t} u;\t\t\t/* holds the getnew() specific param */\n};\n\n/*\n * Structure that holds some ipc operations. This structure is used to unify\n * the calls to sys_msgget(), sys_semget(), sys_shmget()\n *      . routine to call to create a new ipc object. Can be one of newque,\n *        newary, newseg\n *      . routine to call to check permissions for a new ipc object.\n *        Can be one of security_msg_associate, security_sem_associate,\n *        security_shm_associate\n *      . routine to call for an extra check if needed\n */\nstruct ipc_ops {\n\tint (*getnew) (struct ipc_namespace *, struct ipc_params *);\n\tint (*associate) (struct kern_ipc_perm *, int);\n\tint (*more_checks) (struct kern_ipc_perm *, struct ipc_params *);\n};\n\nstruct seq_file;\nstruct ipc_ids;\n\nvoid ipc_init_ids(struct ipc_ids *);\n#ifdef CONFIG_PROC_FS\nvoid __init ipc_init_proc_interface(const char *path, const char *header,\n\t\tint ids, int (*show)(struct seq_file *, void *));\n#else\n#define ipc_init_proc_interface(path, header, ids, show) do {} while (0)\n#endif\n\n#define IPC_SEM_IDS\t0\n#define IPC_MSG_IDS\t1\n#define IPC_SHM_IDS\t2\n\n#define ipcid_to_idx(id) ((id) % SEQ_MULTIPLIER)\n#define ipcid_to_seqx(id) ((id) / SEQ_MULTIPLIER)\n\n/* must be called with ids->rw_mutex acquired for writing */\nint ipc_addid(struct ipc_ids *, struct kern_ipc_perm *, int);\n\n/* must be called with ids->rw_mutex acquired for reading */\nint ipc_get_maxid(struct ipc_ids *);\n\n/* must be called with both locks acquired. */\nvoid ipc_rmid(struct ipc_ids *, struct kern_ipc_perm *);\n\n/* must be called with ipcp locked */\nint ipcperms(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp, short flg);\n\n/* for rare, potentially huge allocations.\n * both function can sleep\n */\nvoid* ipc_alloc(int size);\nvoid ipc_free(void* ptr, int size);\n\n/*\n * For allocation that need to be freed by RCU.\n * Objects are reference counted, they start with reference count 1.\n * getref increases the refcount, the putref call that reduces the recount\n * to 0 schedules the rcu destruction. Caller must guarantee locking.\n */\nvoid* ipc_rcu_alloc(int size);\nvoid ipc_rcu_getref(void *ptr);\nvoid ipc_rcu_putref(void *ptr);\n\nstruct kern_ipc_perm *ipc_lock(struct ipc_ids *, int);\nstruct kern_ipc_perm *ipc_obtain_object(struct ipc_ids *ids, int id);\n\nvoid kernel_to_ipc64_perm(struct kern_ipc_perm *in, struct ipc64_perm *out);\nvoid ipc64_perm_to_ipc_perm(struct ipc64_perm *in, struct ipc_perm *out);\nint ipc_update_perm(struct ipc64_perm *in, struct kern_ipc_perm *out);\nstruct kern_ipc_perm *ipcctl_pre_down_nolock(struct ipc_namespace *ns,\n\t\t\t\t\t     struct ipc_ids *ids, int id, int cmd,\n\t\t\t\t\t     struct ipc64_perm *perm, int extra_perm);\nstruct kern_ipc_perm *ipcctl_pre_down(struct ipc_namespace *ns,\n\t\t\t\t      struct ipc_ids *ids, int id, int cmd,\n\t\t\t\t      struct ipc64_perm *perm, int extra_perm);\n\n#ifndef CONFIG_ARCH_WANT_IPC_PARSE_VERSION\n  /* On IA-64, we always use the \"64-bit version\" of the IPC structures.  */ \n# define ipc_parse_version(cmd)\tIPC_64\n#else\nint ipc_parse_version (int *cmd);\n#endif\n\nextern void free_msg(struct msg_msg *msg);\nextern struct msg_msg *load_msg(const void __user *src, int len);\nextern struct msg_msg *copy_msg(struct msg_msg *src, struct msg_msg *dst);\nextern int store_msg(void __user *dest, struct msg_msg *msg, int len);\n\nextern void recompute_msgmni(struct ipc_namespace *);\n\nstatic inline int ipc_buildid(int id, int seq)\n{\n\treturn SEQ_MULTIPLIER * seq + id;\n}\n\nstatic inline int ipc_checkid(struct kern_ipc_perm *ipcp, int uid)\n{\n\treturn uid / SEQ_MULTIPLIER != ipcp->seq;\n}\n\nstatic inline void ipc_lock_by_ptr(struct kern_ipc_perm *perm)\n{\n\trcu_read_lock();\n\tspin_lock(&perm->lock);\n}\n\nstatic inline void ipc_unlock(struct kern_ipc_perm *perm)\n{\n\tspin_unlock(&perm->lock);\n\trcu_read_unlock();\n}\n\nstatic inline void ipc_lock_object(struct kern_ipc_perm *perm)\n{\n\tspin_lock(&perm->lock);\n}\n\nstruct kern_ipc_perm *ipc_lock_check(struct ipc_ids *ids, int id);\nstruct kern_ipc_perm *ipc_obtain_object_check(struct ipc_ids *ids, int id);\nint ipcget(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\t\tstruct ipc_ops *ops, struct ipc_params *params);\nvoid free_ipcs(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\tvoid (*free)(struct ipc_namespace *, struct kern_ipc_perm *));\n#endif\n"], "fixing_code": ["/*\n * linux/ipc/msg.c\n * Copyright (C) 1992 Krishna Balasubramanian\n *\n * Removed all the remaining kerneld mess\n * Catch the -EFAULT stuff properly\n * Use GFP_KERNEL for messages as in 1.2\n * Fixed up the unchecked user space derefs\n * Copyright (C) 1998 Alan Cox & Andi Kleen\n *\n * /proc/sysvipc/msg support (c) 1999 Dragos Acostachioaie <dragos@iname.com>\n *\n * mostly rewritten, threaded and wake-one semantics added\n * MSGMAX limit removed, sysctl's added\n * (c) 1999 Manfred Spraul <manfred@colorfullife.com>\n *\n * support for audit of ipc object properties and permission changes\n * Dustin Kirkland <dustin.kirkland@us.ibm.com>\n *\n * namespaces support\n * OpenVZ, SWsoft Inc.\n * Pavel Emelianov <xemul@openvz.org>\n */\n\n#include <linux/capability.h>\n#include <linux/msg.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/proc_fs.h>\n#include <linux/list.h>\n#include <linux/security.h>\n#include <linux/sched.h>\n#include <linux/syscalls.h>\n#include <linux/audit.h>\n#include <linux/seq_file.h>\n#include <linux/rwsem.h>\n#include <linux/nsproxy.h>\n#include <linux/ipc_namespace.h>\n\n#include <asm/current.h>\n#include <asm/uaccess.h>\n#include \"util.h\"\n\n/*\n * one msg_receiver structure for each sleeping receiver:\n */\nstruct msg_receiver {\n\tstruct list_head\tr_list;\n\tstruct task_struct\t*r_tsk;\n\n\tint\t\t\tr_mode;\n\tlong\t\t\tr_msgtype;\n\tlong\t\t\tr_maxsize;\n\n\tstruct msg_msg\t\t*volatile r_msg;\n};\n\n/* one msg_sender for each sleeping sender */\nstruct msg_sender {\n\tstruct list_head\tlist;\n\tstruct task_struct\t*tsk;\n};\n\n#define SEARCH_ANY\t\t1\n#define SEARCH_EQUAL\t\t2\n#define SEARCH_NOTEQUAL\t\t3\n#define SEARCH_LESSEQUAL\t4\n#define SEARCH_NUMBER\t\t5\n\n#define msg_ids(ns)\t((ns)->ids[IPC_MSG_IDS])\n\n#define msg_unlock(msq)\t\tipc_unlock(&(msq)->q_perm)\n\nstatic void freeque(struct ipc_namespace *, struct kern_ipc_perm *);\nstatic int newque(struct ipc_namespace *, struct ipc_params *);\n#ifdef CONFIG_PROC_FS\nstatic int sysvipc_msg_proc_show(struct seq_file *s, void *it);\n#endif\n\n/*\n * Scale msgmni with the available lowmem size: the memory dedicated to msg\n * queues should occupy at most 1/MSG_MEM_SCALE of lowmem.\n * Also take into account the number of nsproxies created so far.\n * This should be done staying within the (MSGMNI , IPCMNI/nr_ipc_ns) range.\n */\nvoid recompute_msgmni(struct ipc_namespace *ns)\n{\n\tstruct sysinfo i;\n\tunsigned long allowed;\n\tint nb_ns;\n\n\tsi_meminfo(&i);\n\tallowed = (((i.totalram - i.totalhigh) / MSG_MEM_SCALE) * i.mem_unit)\n\t\t/ MSGMNB;\n\tnb_ns = atomic_read(&nr_ipc_ns);\n\tallowed /= nb_ns;\n\n\tif (allowed < MSGMNI) {\n\t\tns->msg_ctlmni = MSGMNI;\n\t\treturn;\n\t}\n\n\tif (allowed > IPCMNI / nb_ns) {\n\t\tns->msg_ctlmni = IPCMNI / nb_ns;\n\t\treturn;\n\t}\n\n\tns->msg_ctlmni = allowed;\n}\n\nvoid msg_init_ns(struct ipc_namespace *ns)\n{\n\tns->msg_ctlmax = MSGMAX;\n\tns->msg_ctlmnb = MSGMNB;\n\n\trecompute_msgmni(ns);\n\n\tatomic_set(&ns->msg_bytes, 0);\n\tatomic_set(&ns->msg_hdrs, 0);\n\tipc_init_ids(&ns->ids[IPC_MSG_IDS]);\n}\n\n#ifdef CONFIG_IPC_NS\nvoid msg_exit_ns(struct ipc_namespace *ns)\n{\n\tfree_ipcs(ns, &msg_ids(ns), freeque);\n\tidr_destroy(&ns->ids[IPC_MSG_IDS].ipcs_idr);\n}\n#endif\n\nvoid __init msg_init(void)\n{\n\tmsg_init_ns(&init_ipc_ns);\n\n\tprintk(KERN_INFO \"msgmni has been set to %d\\n\",\n\t\tinit_ipc_ns.msg_ctlmni);\n\n\tipc_init_proc_interface(\"sysvipc/msg\",\n\t\t\t\t\"       key      msqid perms      cbytes       qnum lspid lrpid   uid   gid  cuid  cgid      stime      rtime      ctime\\n\",\n\t\t\t\tIPC_MSG_IDS, sysvipc_msg_proc_show);\n}\n\n/*\n * msg_lock_(check_) routines are called in the paths where the rw_mutex\n * is not held.\n */\nstatic inline struct msg_queue *msg_lock(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_lock(&msg_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn (struct msg_queue *)ipcp;\n\n\treturn container_of(ipcp, struct msg_queue, q_perm);\n}\n\nstatic inline struct msg_queue *msg_lock_check(struct ipc_namespace *ns,\n\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_lock_check(&msg_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn (struct msg_queue *)ipcp;\n\n\treturn container_of(ipcp, struct msg_queue, q_perm);\n}\n\nstatic inline void msg_rmid(struct ipc_namespace *ns, struct msg_queue *s)\n{\n\tipc_rmid(&msg_ids(ns), &s->q_perm);\n}\n\n/**\n * newque - Create a new msg queue\n * @ns: namespace\n * @params: ptr to the structure that contains the key and msgflg\n *\n * Called with msg_ids.rw_mutex held (writer)\n */\nstatic int newque(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tstruct msg_queue *msq;\n\tint id, retval;\n\tkey_t key = params->key;\n\tint msgflg = params->flg;\n\n\tmsq = ipc_rcu_alloc(sizeof(*msq));\n\tif (!msq)\n\t\treturn -ENOMEM;\n\n\tmsq->q_perm.mode = msgflg & S_IRWXUGO;\n\tmsq->q_perm.key = key;\n\n\tmsq->q_perm.security = NULL;\n\tretval = security_msg_queue_alloc(msq);\n\tif (retval) {\n\t\tipc_rcu_putref(msq);\n\t\treturn retval;\n\t}\n\n\t/*\n\t * ipc_addid() locks msq\n\t */\n\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n\tif (id < 0) {\n\t\tsecurity_msg_queue_free(msq);\n\t\tipc_rcu_putref(msq);\n\t\treturn id;\n\t}\n\n\tmsq->q_stime = msq->q_rtime = 0;\n\tmsq->q_ctime = get_seconds();\n\tmsq->q_cbytes = msq->q_qnum = 0;\n\tmsq->q_qbytes = ns->msg_ctlmnb;\n\tmsq->q_lspid = msq->q_lrpid = 0;\n\tINIT_LIST_HEAD(&msq->q_messages);\n\tINIT_LIST_HEAD(&msq->q_receivers);\n\tINIT_LIST_HEAD(&msq->q_senders);\n\n\tmsg_unlock(msq);\n\n\treturn msq->q_perm.id;\n}\n\nstatic inline void ss_add(struct msg_queue *msq, struct msg_sender *mss)\n{\n\tmss->tsk = current;\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tlist_add_tail(&mss->list, &msq->q_senders);\n}\n\nstatic inline void ss_del(struct msg_sender *mss)\n{\n\tif (mss->list.next != NULL)\n\t\tlist_del(&mss->list);\n}\n\nstatic void ss_wakeup(struct list_head *h, int kill)\n{\n\tstruct list_head *tmp;\n\n\ttmp = h->next;\n\twhile (tmp != h) {\n\t\tstruct msg_sender *mss;\n\n\t\tmss = list_entry(tmp, struct msg_sender, list);\n\t\ttmp = tmp->next;\n\t\tif (kill)\n\t\t\tmss->list.next = NULL;\n\t\twake_up_process(mss->tsk);\n\t}\n}\n\nstatic void expunge_all(struct msg_queue *msq, int res)\n{\n\tstruct list_head *tmp;\n\n\ttmp = msq->q_receivers.next;\n\twhile (tmp != &msq->q_receivers) {\n\t\tstruct msg_receiver *msr;\n\n\t\tmsr = list_entry(tmp, struct msg_receiver, r_list);\n\t\ttmp = tmp->next;\n\t\tmsr->r_msg = NULL;\n\t\twake_up_process(msr->r_tsk);\n\t\tsmp_mb();\n\t\tmsr->r_msg = ERR_PTR(res);\n\t}\n}\n\n/*\n * freeque() wakes up waiters on the sender and receiver waiting queue,\n * removes the message queue from message queue ID IDR, and cleans up all the\n * messages associated with this queue.\n *\n * msg_ids.rw_mutex (writer) and the spinlock for this message queue are held\n * before freeque() is called. msg_ids.rw_mutex remains locked on exit.\n */\nstatic void freeque(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n{\n\tstruct list_head *tmp;\n\tstruct msg_queue *msq = container_of(ipcp, struct msg_queue, q_perm);\n\n\texpunge_all(msq, -EIDRM);\n\tss_wakeup(&msq->q_senders, 1);\n\tmsg_rmid(ns, msq);\n\tmsg_unlock(msq);\n\n\ttmp = msq->q_messages.next;\n\twhile (tmp != &msq->q_messages) {\n\t\tstruct msg_msg *msg = list_entry(tmp, struct msg_msg, m_list);\n\n\t\ttmp = tmp->next;\n\t\tatomic_dec(&ns->msg_hdrs);\n\t\tfree_msg(msg);\n\t}\n\tatomic_sub(msq->q_cbytes, &ns->msg_bytes);\n\tsecurity_msg_queue_free(msq);\n\tipc_rcu_putref(msq);\n}\n\n/*\n * Called with msg_ids.rw_mutex and ipcp locked.\n */\nstatic inline int msg_security(struct kern_ipc_perm *ipcp, int msgflg)\n{\n\tstruct msg_queue *msq = container_of(ipcp, struct msg_queue, q_perm);\n\n\treturn security_msg_queue_associate(msq, msgflg);\n}\n\nSYSCALL_DEFINE2(msgget, key_t, key, int, msgflg)\n{\n\tstruct ipc_namespace *ns;\n\tstruct ipc_ops msg_ops;\n\tstruct ipc_params msg_params;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tmsg_ops.getnew = newque;\n\tmsg_ops.associate = msg_security;\n\tmsg_ops.more_checks = NULL;\n\n\tmsg_params.key = key;\n\tmsg_params.flg = msgflg;\n\n\treturn ipcget(ns, &msg_ids(ns), &msg_ops, &msg_params);\n}\n\nstatic inline unsigned long\ncopy_msqid_to_user(void __user *buf, struct msqid64_ds *in, int version)\n{\n\tswitch(version) {\n\tcase IPC_64:\n\t\treturn copy_to_user(buf, in, sizeof(*in));\n\tcase IPC_OLD:\n\t{\n\t\tstruct msqid_ds out;\n\n\t\tmemset(&out, 0, sizeof(out));\n\n\t\tipc64_perm_to_ipc_perm(&in->msg_perm, &out.msg_perm);\n\n\t\tout.msg_stime\t\t= in->msg_stime;\n\t\tout.msg_rtime\t\t= in->msg_rtime;\n\t\tout.msg_ctime\t\t= in->msg_ctime;\n\n\t\tif (in->msg_cbytes > USHRT_MAX)\n\t\t\tout.msg_cbytes\t= USHRT_MAX;\n\t\telse\n\t\t\tout.msg_cbytes\t= in->msg_cbytes;\n\t\tout.msg_lcbytes\t\t= in->msg_cbytes;\n\n\t\tif (in->msg_qnum > USHRT_MAX)\n\t\t\tout.msg_qnum\t= USHRT_MAX;\n\t\telse\n\t\t\tout.msg_qnum\t= in->msg_qnum;\n\n\t\tif (in->msg_qbytes > USHRT_MAX)\n\t\t\tout.msg_qbytes\t= USHRT_MAX;\n\t\telse\n\t\t\tout.msg_qbytes\t= in->msg_qbytes;\n\t\tout.msg_lqbytes\t\t= in->msg_qbytes;\n\n\t\tout.msg_lspid\t\t= in->msg_lspid;\n\t\tout.msg_lrpid\t\t= in->msg_lrpid;\n\n\t\treturn copy_to_user(buf, &out, sizeof(out));\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic inline unsigned long\ncopy_msqid_from_user(struct msqid64_ds *out, void __user *buf, int version)\n{\n\tswitch(version) {\n\tcase IPC_64:\n\t\tif (copy_from_user(out, buf, sizeof(*out)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase IPC_OLD:\n\t{\n\t\tstruct msqid_ds tbuf_old;\n\n\t\tif (copy_from_user(&tbuf_old, buf, sizeof(tbuf_old)))\n\t\t\treturn -EFAULT;\n\n\t\tout->msg_perm.uid      \t= tbuf_old.msg_perm.uid;\n\t\tout->msg_perm.gid      \t= tbuf_old.msg_perm.gid;\n\t\tout->msg_perm.mode     \t= tbuf_old.msg_perm.mode;\n\n\t\tif (tbuf_old.msg_qbytes == 0)\n\t\t\tout->msg_qbytes\t= tbuf_old.msg_lqbytes;\n\t\telse\n\t\t\tout->msg_qbytes\t= tbuf_old.msg_qbytes;\n\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n/*\n * This function handles some msgctl commands which require the rw_mutex\n * to be held in write mode.\n * NOTE: no locks must be held, the rw_mutex is taken inside this function.\n */\nstatic int msgctl_down(struct ipc_namespace *ns, int msqid, int cmd,\n\t\t       struct msqid_ds __user *buf, int version)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct msqid64_ds uninitialized_var(msqid64);\n\tstruct msg_queue *msq;\n\tint err;\n\n\tif (cmd == IPC_SET) {\n\t\tif (copy_msqid_from_user(&msqid64, buf, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down(ns, &msg_ids(ns), msqid, cmd,\n\t\t\t       &msqid64.msg_perm, msqid64.msg_qbytes);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tmsq = container_of(ipcp, struct msg_queue, q_perm);\n\n\terr = security_msg_queue_msgctl(msq, cmd);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tswitch (cmd) {\n\tcase IPC_RMID:\n\t\tfreeque(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tif (msqid64.msg_qbytes > ns->msg_ctlmnb &&\n\t\t    !capable(CAP_SYS_RESOURCE)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = ipc_update_perm(&msqid64.msg_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\n\t\tmsq->q_qbytes = msqid64.msg_qbytes;\n\n\t\tmsq->q_ctime = get_seconds();\n\t\t/* sleeping receivers might be excluded by\n\t\t * stricter permissions.\n\t\t */\n\t\texpunge_all(msq, -EAGAIN);\n\t\t/* sleeping senders might be able to send\n\t\t * due to a larger queue size.\n\t\t */\n\t\tss_wakeup(&msq->q_senders, 0);\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t}\nout_unlock:\n\tmsg_unlock(msq);\nout_up:\n\tup_write(&msg_ids(ns).rw_mutex);\n\treturn err;\n}\n\nSYSCALL_DEFINE3(msgctl, int, msqid, int, cmd, struct msqid_ds __user *, buf)\n{\n\tstruct msg_queue *msq;\n\tint err, version;\n\tstruct ipc_namespace *ns;\n\n\tif (msqid < 0 || cmd < 0)\n\t\treturn -EINVAL;\n\n\tversion = ipc_parse_version(&cmd);\n\tns = current->nsproxy->ipc_ns;\n\n\tswitch (cmd) {\n\tcase IPC_INFO:\n\tcase MSG_INFO:\n\t{\n\t\tstruct msginfo msginfo;\n\t\tint max_id;\n\n\t\tif (!buf)\n\t\t\treturn -EFAULT;\n\t\t/*\n\t\t * We must not return kernel stack data.\n\t\t * due to padding, it's not enough\n\t\t * to set all member fields.\n\t\t */\n\t\terr = security_msg_queue_msgctl(NULL, cmd);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tmemset(&msginfo, 0, sizeof(msginfo));\n\t\tmsginfo.msgmni = ns->msg_ctlmni;\n\t\tmsginfo.msgmax = ns->msg_ctlmax;\n\t\tmsginfo.msgmnb = ns->msg_ctlmnb;\n\t\tmsginfo.msgssz = MSGSSZ;\n\t\tmsginfo.msgseg = MSGSEG;\n\t\tdown_read(&msg_ids(ns).rw_mutex);\n\t\tif (cmd == MSG_INFO) {\n\t\t\tmsginfo.msgpool = msg_ids(ns).in_use;\n\t\t\tmsginfo.msgmap = atomic_read(&ns->msg_hdrs);\n\t\t\tmsginfo.msgtql = atomic_read(&ns->msg_bytes);\n\t\t} else {\n\t\t\tmsginfo.msgmap = MSGMAP;\n\t\t\tmsginfo.msgpool = MSGPOOL;\n\t\t\tmsginfo.msgtql = MSGTQL;\n\t\t}\n\t\tmax_id = ipc_get_maxid(&msg_ids(ns));\n\t\tup_read(&msg_ids(ns).rw_mutex);\n\t\tif (copy_to_user(buf, &msginfo, sizeof(struct msginfo)))\n\t\t\treturn -EFAULT;\n\t\treturn (max_id < 0) ? 0 : max_id;\n\t}\n\tcase MSG_STAT:\t/* msqid is an index rather than a msg queue id */\n\tcase IPC_STAT:\n\t{\n\t\tstruct msqid64_ds tbuf;\n\t\tint success_return;\n\n\t\tif (!buf)\n\t\t\treturn -EFAULT;\n\n\t\tif (cmd == MSG_STAT) {\n\t\t\tmsq = msg_lock(ns, msqid);\n\t\t\tif (IS_ERR(msq))\n\t\t\t\treturn PTR_ERR(msq);\n\t\t\tsuccess_return = msq->q_perm.id;\n\t\t} else {\n\t\t\tmsq = msg_lock_check(ns, msqid);\n\t\t\tif (IS_ERR(msq))\n\t\t\t\treturn PTR_ERR(msq);\n\t\t\tsuccess_return = 0;\n\t\t}\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IRUGO))\n\t\t\tgoto out_unlock;\n\n\t\terr = security_msg_queue_msgctl(msq, cmd);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\n\t\tmemset(&tbuf, 0, sizeof(tbuf));\n\n\t\tkernel_to_ipc64_perm(&msq->q_perm, &tbuf.msg_perm);\n\t\ttbuf.msg_stime  = msq->q_stime;\n\t\ttbuf.msg_rtime  = msq->q_rtime;\n\t\ttbuf.msg_ctime  = msq->q_ctime;\n\t\ttbuf.msg_cbytes = msq->q_cbytes;\n\t\ttbuf.msg_qnum   = msq->q_qnum;\n\t\ttbuf.msg_qbytes = msq->q_qbytes;\n\t\ttbuf.msg_lspid  = msq->q_lspid;\n\t\ttbuf.msg_lrpid  = msq->q_lrpid;\n\t\tmsg_unlock(msq);\n\t\tif (copy_msqid_to_user(buf, &tbuf, version))\n\t\t\treturn -EFAULT;\n\t\treturn success_return;\n\t}\n\tcase IPC_SET:\n\tcase IPC_RMID:\n\t\terr = msgctl_down(ns, msqid, cmd, buf, version);\n\t\treturn err;\n\tdefault:\n\t\treturn  -EINVAL;\n\t}\n\nout_unlock:\n\tmsg_unlock(msq);\n\treturn err;\n}\n\nstatic int testmsg(struct msg_msg *msg, long type, int mode)\n{\n\tswitch(mode)\n\t{\n\t\tcase SEARCH_ANY:\n\t\tcase SEARCH_NUMBER:\n\t\t\treturn 1;\n\t\tcase SEARCH_LESSEQUAL:\n\t\t\tif (msg->m_type <=type)\n\t\t\t\treturn 1;\n\t\t\tbreak;\n\t\tcase SEARCH_EQUAL:\n\t\t\tif (msg->m_type == type)\n\t\t\t\treturn 1;\n\t\t\tbreak;\n\t\tcase SEARCH_NOTEQUAL:\n\t\t\tif (msg->m_type != type)\n\t\t\t\treturn 1;\n\t\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic inline int pipelined_send(struct msg_queue *msq, struct msg_msg *msg)\n{\n\tstruct list_head *tmp;\n\n\ttmp = msq->q_receivers.next;\n\twhile (tmp != &msq->q_receivers) {\n\t\tstruct msg_receiver *msr;\n\n\t\tmsr = list_entry(tmp, struct msg_receiver, r_list);\n\t\ttmp = tmp->next;\n\t\tif (testmsg(msg, msr->r_msgtype, msr->r_mode) &&\n\t\t    !security_msg_queue_msgrcv(msq, msg, msr->r_tsk,\n\t\t\t\t\t       msr->r_msgtype, msr->r_mode)) {\n\n\t\t\tlist_del(&msr->r_list);\n\t\t\tif (msr->r_maxsize < msg->m_ts) {\n\t\t\t\tmsr->r_msg = NULL;\n\t\t\t\twake_up_process(msr->r_tsk);\n\t\t\t\tsmp_mb();\n\t\t\t\tmsr->r_msg = ERR_PTR(-E2BIG);\n\t\t\t} else {\n\t\t\t\tmsr->r_msg = NULL;\n\t\t\t\tmsq->q_lrpid = task_pid_vnr(msr->r_tsk);\n\t\t\t\tmsq->q_rtime = get_seconds();\n\t\t\t\twake_up_process(msr->r_tsk);\n\t\t\t\tsmp_mb();\n\t\t\t\tmsr->r_msg = msg;\n\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nlong do_msgsnd(int msqid, long mtype, void __user *mtext,\n\t\tsize_t msgsz, int msgflg)\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint err;\n\tstruct ipc_namespace *ns;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msgsz > ns->msg_ctlmax || (long) msgsz < 0 || msqid < 0)\n\t\treturn -EINVAL;\n\tif (mtype < 1)\n\t\treturn -EINVAL;\n\n\tmsg = load_msg(mtext, msgsz);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\n\tmsg->m_type = mtype;\n\tmsg->m_ts = msgsz;\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\terr = PTR_ERR(msq);\n\t\tgoto out_free;\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_sender s;\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &msq->q_perm, S_IWUGO))\n\t\t\tgoto out_unlock_free;\n\n\t\terr = security_msg_queue_msgsnd(msq, msg, msgflg);\n\t\tif (err)\n\t\t\tgoto out_unlock_free;\n\n\t\tif (msgsz + msq->q_cbytes <= msq->q_qbytes &&\n\t\t\t\t1 + msq->q_qnum <= msq->q_qbytes) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* queue full, wait: */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_add(msq, &s);\n\n\t\tif (!ipc_rcu_getref(msq)) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\n\t\tmsg_unlock(msq);\n\t\tschedule();\n\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\tipc_rcu_putref(msq);\n\t\tif (msq->q_perm.deleted) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t\tss_del(&s);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = -ERESTARTNOHAND;\n\t\t\tgoto out_unlock_free;\n\t\t}\n\t}\n\n\tmsq->q_lspid = task_tgid_vnr(current);\n\tmsq->q_stime = get_seconds();\n\n\tif (!pipelined_send(msq, msg)) {\n\t\t/* no one is waiting for this message, enqueue it */\n\t\tlist_add_tail(&msg->m_list, &msq->q_messages);\n\t\tmsq->q_cbytes += msgsz;\n\t\tmsq->q_qnum++;\n\t\tatomic_add(msgsz, &ns->msg_bytes);\n\t\tatomic_inc(&ns->msg_hdrs);\n\t}\n\n\terr = 0;\n\tmsg = NULL;\n\nout_unlock_free:\n\tmsg_unlock(msq);\nout_free:\n\tif (msg != NULL)\n\t\tfree_msg(msg);\n\treturn err;\n}\n\nSYSCALL_DEFINE4(msgsnd, int, msqid, struct msgbuf __user *, msgp, size_t, msgsz,\n\t\tint, msgflg)\n{\n\tlong mtype;\n\n\tif (get_user(mtype, &msgp->mtype))\n\t\treturn -EFAULT;\n\treturn do_msgsnd(msqid, mtype, msgp->mtext, msgsz, msgflg);\n}\n\nstatic inline int convert_mode(long *msgtyp, int msgflg)\n{\n\tif (msgflg & MSG_COPY)\n\t\treturn SEARCH_NUMBER;\n\t/*\n\t *  find message of correct type.\n\t *  msgtyp = 0 => get first.\n\t *  msgtyp > 0 => get first message of matching type.\n\t *  msgtyp < 0 => get message with least type must be < abs(msgtype).\n\t */\n\tif (*msgtyp == 0)\n\t\treturn SEARCH_ANY;\n\tif (*msgtyp < 0) {\n\t\t*msgtyp = -*msgtyp;\n\t\treturn SEARCH_LESSEQUAL;\n\t}\n\tif (msgflg & MSG_EXCEPT)\n\t\treturn SEARCH_NOTEQUAL;\n\treturn SEARCH_EQUAL;\n}\n\nstatic long do_msg_fill(void __user *dest, struct msg_msg *msg, size_t bufsz)\n{\n\tstruct msgbuf __user *msgp = dest;\n\tsize_t msgsz;\n\n\tif (put_user(msg->m_type, &msgp->mtype))\n\t\treturn -EFAULT;\n\n\tmsgsz = (bufsz > msg->m_ts) ? msg->m_ts : bufsz;\n\tif (store_msg(msgp->mtext, msg, msgsz))\n\t\treturn -EFAULT;\n\treturn msgsz;\n}\n\n#ifdef CONFIG_CHECKPOINT_RESTORE\n/*\n * This function creates new kernel message structure, large enough to store\n * bufsz message bytes.\n */\nstatic inline struct msg_msg *prepare_copy(void __user *buf, size_t bufsz)\n{\n\tstruct msg_msg *copy;\n\n\t/*\n\t * Create dummy message to copy real message to.\n\t */\n\tcopy = load_msg(buf, bufsz);\n\tif (!IS_ERR(copy))\n\t\tcopy->m_ts = bufsz;\n\treturn copy;\n}\n\nstatic inline void free_copy(struct msg_msg *copy)\n{\n\tif (copy)\n\t\tfree_msg(copy);\n}\n#else\nstatic inline struct msg_msg *prepare_copy(void __user *buf, size_t bufsz)\n{\n\treturn ERR_PTR(-ENOSYS);\n}\n\nstatic inline void free_copy(struct msg_msg *copy)\n{\n}\n#endif\n\nstatic struct msg_msg *find_msg(struct msg_queue *msq, long *msgtyp, int mode)\n{\n\tstruct msg_msg *msg;\n\tlong count = 0;\n\n\tlist_for_each_entry(msg, &msq->q_messages, m_list) {\n\t\tif (testmsg(msg, *msgtyp, mode) &&\n\t\t    !security_msg_queue_msgrcv(msq, msg, current,\n\t\t\t\t\t       *msgtyp, mode)) {\n\t\t\tif (mode == SEARCH_LESSEQUAL && msg->m_type != 1) {\n\t\t\t\t*msgtyp = msg->m_type - 1;\n\t\t\t} else if (mode == SEARCH_NUMBER) {\n\t\t\t\tif (*msgtyp == count)\n\t\t\t\t\treturn msg;\n\t\t\t} else\n\t\t\t\treturn msg;\n\t\t\tcount++;\n\t\t}\n\t}\n\n\treturn ERR_PTR(-EAGAIN);\n}\n\n\nlong do_msgrcv(int msqid, void __user *buf, size_t bufsz, long msgtyp,\n\t       int msgflg,\n\t       long (*msg_handler)(void __user *, struct msg_msg *, size_t))\n{\n\tstruct msg_queue *msq;\n\tstruct msg_msg *msg;\n\tint mode;\n\tstruct ipc_namespace *ns;\n\tstruct msg_msg *copy = NULL;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (msqid < 0 || (long) bufsz < 0)\n\t\treturn -EINVAL;\n\tif (msgflg & MSG_COPY) {\n\t\tcopy = prepare_copy(buf, min_t(size_t, bufsz, ns->msg_ctlmax));\n\t\tif (IS_ERR(copy))\n\t\t\treturn PTR_ERR(copy);\n\t}\n\tmode = convert_mode(&msgtyp, msgflg);\n\n\tmsq = msg_lock_check(ns, msqid);\n\tif (IS_ERR(msq)) {\n\t\tfree_copy(copy);\n\t\treturn PTR_ERR(msq);\n\t}\n\n\tfor (;;) {\n\t\tstruct msg_receiver msr_d;\n\n\t\tmsg = ERR_PTR(-EACCES);\n\t\tif (ipcperms(ns, &msq->q_perm, S_IRUGO))\n\t\t\tgoto out_unlock;\n\n\t\tmsg = find_msg(msq, &msgtyp, mode);\n\n\t\tif (!IS_ERR(msg)) {\n\t\t\t/*\n\t\t\t * Found a suitable message.\n\t\t\t * Unlink it from the queue.\n\t\t\t */\n\t\t\tif ((bufsz < msg->m_ts) && !(msgflg & MSG_NOERROR)) {\n\t\t\t\tmsg = ERR_PTR(-E2BIG);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\t/*\n\t\t\t * If we are copying, then do not unlink message and do\n\t\t\t * not update queue parameters.\n\t\t\t */\n\t\t\tif (msgflg & MSG_COPY) {\n\t\t\t\tmsg = copy_msg(msg, copy);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tlist_del(&msg->m_list);\n\t\t\tmsq->q_qnum--;\n\t\t\tmsq->q_rtime = get_seconds();\n\t\t\tmsq->q_lrpid = task_tgid_vnr(current);\n\t\t\tmsq->q_cbytes -= msg->m_ts;\n\t\t\tatomic_sub(msg->m_ts, &ns->msg_bytes);\n\t\t\tatomic_dec(&ns->msg_hdrs);\n\t\t\tss_wakeup(&msq->q_senders, 0);\n\t\t\tmsg_unlock(msq);\n\t\t\tbreak;\n\t\t}\n\t\t/* No message waiting. Wait for a message */\n\t\tif (msgflg & IPC_NOWAIT) {\n\t\t\tmsg = ERR_PTR(-ENOMSG);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tlist_add_tail(&msr_d.r_list, &msq->q_receivers);\n\t\tmsr_d.r_tsk = current;\n\t\tmsr_d.r_msgtype = msgtyp;\n\t\tmsr_d.r_mode = mode;\n\t\tif (msgflg & MSG_NOERROR)\n\t\t\tmsr_d.r_maxsize = INT_MAX;\n\t\telse\n\t\t\tmsr_d.r_maxsize = bufsz;\n\t\tmsr_d.r_msg = ERR_PTR(-EAGAIN);\n\t\tcurrent->state = TASK_INTERRUPTIBLE;\n\t\tmsg_unlock(msq);\n\n\t\tschedule();\n\n\t\t/* Lockless receive, part 1:\n\t\t * Disable preemption.  We don't hold a reference to the queue\n\t\t * and getting a reference would defeat the idea of a lockless\n\t\t * operation, thus the code relies on rcu to guarantee the\n\t\t * existence of msq:\n\t\t * Prior to destruction, expunge_all(-EIRDM) changes r_msg.\n\t\t * Thus if r_msg is -EAGAIN, then the queue not yet destroyed.\n\t\t * rcu_read_lock() prevents preemption between reading r_msg\n\t\t * and the spin_lock() inside ipc_lock_by_ptr().\n\t\t */\n\t\trcu_read_lock();\n\n\t\t/* Lockless receive, part 2:\n\t\t * Wait until pipelined_send or expunge_all are outside of\n\t\t * wake_up_process(). There is a race with exit(), see\n\t\t * ipc/mqueue.c for the details.\n\t\t */\n\t\tmsg = (struct msg_msg*)msr_d.r_msg;\n\t\twhile (msg == NULL) {\n\t\t\tcpu_relax();\n\t\t\tmsg = (struct msg_msg *)msr_d.r_msg;\n\t\t}\n\n\t\t/* Lockless receive, part 3:\n\t\t * If there is a message or an error then accept it without\n\t\t * locking.\n\t\t */\n\t\tif (msg != ERR_PTR(-EAGAIN)) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Lockless receive, part 3:\n\t\t * Acquire the queue spinlock.\n\t\t */\n\t\tipc_lock_by_ptr(&msq->q_perm);\n\t\trcu_read_unlock();\n\n\t\t/* Lockless receive, part 4:\n\t\t * Repeat test after acquiring the spinlock.\n\t\t */\n\t\tmsg = (struct msg_msg*)msr_d.r_msg;\n\t\tif (msg != ERR_PTR(-EAGAIN))\n\t\t\tgoto out_unlock;\n\n\t\tlist_del(&msr_d.r_list);\n\t\tif (signal_pending(current)) {\n\t\t\tmsg = ERR_PTR(-ERESTARTNOHAND);\nout_unlock:\n\t\t\tmsg_unlock(msq);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (IS_ERR(msg)) {\n\t\tfree_copy(copy);\n\t\treturn PTR_ERR(msg);\n\t}\n\n\tbufsz = msg_handler(buf, msg, bufsz);\n\tfree_msg(msg);\n\n\treturn bufsz;\n}\n\nSYSCALL_DEFINE5(msgrcv, int, msqid, struct msgbuf __user *, msgp, size_t, msgsz,\n\t\tlong, msgtyp, int, msgflg)\n{\n\treturn do_msgrcv(msqid, msgp, msgsz, msgtyp, msgflg, do_msg_fill);\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int sysvipc_msg_proc_show(struct seq_file *s, void *it)\n{\n\tstruct user_namespace *user_ns = seq_user_ns(s);\n\tstruct msg_queue *msq = it;\n\n\treturn seq_printf(s,\n\t\t\t\"%10d %10d  %4o  %10lu %10lu %5u %5u %5u %5u %5u %5u %10lu %10lu %10lu\\n\",\n\t\t\tmsq->q_perm.key,\n\t\t\tmsq->q_perm.id,\n\t\t\tmsq->q_perm.mode,\n\t\t\tmsq->q_cbytes,\n\t\t\tmsq->q_qnum,\n\t\t\tmsq->q_lspid,\n\t\t\tmsq->q_lrpid,\n\t\t\tfrom_kuid_munged(user_ns, msq->q_perm.uid),\n\t\t\tfrom_kgid_munged(user_ns, msq->q_perm.gid),\n\t\t\tfrom_kuid_munged(user_ns, msq->q_perm.cuid),\n\t\t\tfrom_kgid_munged(user_ns, msq->q_perm.cgid),\n\t\t\tmsq->q_stime,\n\t\t\tmsq->q_rtime,\n\t\t\tmsq->q_ctime);\n}\n#endif\n", "/*\n * linux/ipc/sem.c\n * Copyright (C) 1992 Krishna Balasubramanian\n * Copyright (C) 1995 Eric Schenk, Bruno Haible\n *\n * /proc/sysvipc/sem support (c) 1999 Dragos Acostachioaie <dragos@iname.com>\n *\n * SMP-threaded, sysctl's added\n * (c) 1999 Manfred Spraul <manfred@colorfullife.com>\n * Enforced range limit on SEM_UNDO\n * (c) 2001 Red Hat Inc\n * Lockless wakeup\n * (c) 2003 Manfred Spraul <manfred@colorfullife.com>\n * Further wakeup optimizations, documentation\n * (c) 2010 Manfred Spraul <manfred@colorfullife.com>\n *\n * support for audit of ipc object properties and permission changes\n * Dustin Kirkland <dustin.kirkland@us.ibm.com>\n *\n * namespaces support\n * OpenVZ, SWsoft Inc.\n * Pavel Emelianov <xemul@openvz.org>\n *\n * Implementation notes: (May 2010)\n * This file implements System V semaphores.\n *\n * User space visible behavior:\n * - FIFO ordering for semop() operations (just FIFO, not starvation\n *   protection)\n * - multiple semaphore operations that alter the same semaphore in\n *   one semop() are handled.\n * - sem_ctime (time of last semctl()) is updated in the IPC_SET, SETVAL and\n *   SETALL calls.\n * - two Linux specific semctl() commands: SEM_STAT, SEM_INFO.\n * - undo adjustments at process exit are limited to 0..SEMVMX.\n * - namespace are supported.\n * - SEMMSL, SEMMNS, SEMOPM and SEMMNI can be configured at runtine by writing\n *   to /proc/sys/kernel/sem.\n * - statistics about the usage are reported in /proc/sysvipc/sem.\n *\n * Internals:\n * - scalability:\n *   - all global variables are read-mostly.\n *   - semop() calls and semctl(RMID) are synchronized by RCU.\n *   - most operations do write operations (actually: spin_lock calls) to\n *     the per-semaphore array structure.\n *   Thus: Perfect SMP scaling between independent semaphore arrays.\n *         If multiple semaphores in one array are used, then cache line\n *         trashing on the semaphore array spinlock will limit the scaling.\n * - semncnt and semzcnt are calculated on demand in count_semncnt() and\n *   count_semzcnt()\n * - the task that performs a successful semop() scans the list of all\n *   sleeping tasks and completes any pending operations that can be fulfilled.\n *   Semaphores are actively given to waiting tasks (necessary for FIFO).\n *   (see update_queue())\n * - To improve the scalability, the actual wake-up calls are performed after\n *   dropping all locks. (see wake_up_sem_queue_prepare(),\n *   wake_up_sem_queue_do())\n * - All work is done by the waker, the woken up task does not have to do\n *   anything - not even acquiring a lock or dropping a refcount.\n * - A woken up task may not even touch the semaphore array anymore, it may\n *   have been destroyed already by a semctl(RMID).\n * - The synchronizations between wake-ups due to a timeout/signal and a\n *   wake-up due to a completed semaphore operation is achieved by using an\n *   intermediate state (IN_WAKEUP).\n * - UNDO values are stored in an array (one per process and per\n *   semaphore array, lazily allocated). For backwards compatibility, multiple\n *   modes for the UNDO variables are supported (per process, per thread)\n *   (see copy_semundo, CLONE_SYSVSEM)\n * - There are two lists of the pending operations: a per-array list\n *   and per-semaphore list (stored in the array). This allows to achieve FIFO\n *   ordering without always scanning all pending operations.\n *   The worst-case behavior is nevertheless O(N^2) for N wakeups.\n */\n\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/proc_fs.h>\n#include <linux/time.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/audit.h>\n#include <linux/capability.h>\n#include <linux/seq_file.h>\n#include <linux/rwsem.h>\n#include <linux/nsproxy.h>\n#include <linux/ipc_namespace.h>\n\n#include <asm/uaccess.h>\n#include \"util.h\"\n\n/* One semaphore structure for each semaphore in the system. */\nstruct sem {\n\tint\tsemval;\t\t/* current value */\n\tint\tsempid;\t\t/* pid of last operation */\n\tspinlock_t\tlock;\t/* spinlock for fine-grained semtimedop */\n\tstruct list_head sem_pending; /* pending single-sop operations */\n};\n\n/* One queue for each sleeping process in the system. */\nstruct sem_queue {\n\tstruct list_head\tlist;\t /* queue of pending operations */\n\tstruct task_struct\t*sleeper; /* this process */\n\tstruct sem_undo\t\t*undo;\t /* undo structure */\n\tint\t\t\tpid;\t /* process id of requesting process */\n\tint\t\t\tstatus;\t /* completion status of operation */\n\tstruct sembuf\t\t*sops;\t /* array of pending operations */\n\tint\t\t\tnsops;\t /* number of operations */\n\tint\t\t\talter;\t /* does *sops alter the array? */\n};\n\n/* Each task has a list of undo requests. They are executed automatically\n * when the process exits.\n */\nstruct sem_undo {\n\tstruct list_head\tlist_proc;\t/* per-process list: *\n\t\t\t\t\t\t * all undos from one process\n\t\t\t\t\t\t * rcu protected */\n\tstruct rcu_head\t\trcu;\t\t/* rcu struct for sem_undo */\n\tstruct sem_undo_list\t*ulp;\t\t/* back ptr to sem_undo_list */\n\tstruct list_head\tlist_id;\t/* per semaphore array list:\n\t\t\t\t\t\t * all undos for one array */\n\tint\t\t\tsemid;\t\t/* semaphore set identifier */\n\tshort\t\t\t*semadj;\t/* array of adjustments */\n\t\t\t\t\t\t/* one per semaphore */\n};\n\n/* sem_undo_list controls shared access to the list of sem_undo structures\n * that may be shared among all a CLONE_SYSVSEM task group.\n */\nstruct sem_undo_list {\n\tatomic_t\t\trefcnt;\n\tspinlock_t\t\tlock;\n\tstruct list_head\tlist_proc;\n};\n\n\n#define sem_ids(ns)\t((ns)->ids[IPC_SEM_IDS])\n\n#define sem_checkid(sma, semid)\tipc_checkid(&sma->sem_perm, semid)\n\nstatic int newary(struct ipc_namespace *, struct ipc_params *);\nstatic void freeary(struct ipc_namespace *, struct kern_ipc_perm *);\n#ifdef CONFIG_PROC_FS\nstatic int sysvipc_sem_proc_show(struct seq_file *s, void *it);\n#endif\n\n#define SEMMSL_FAST\t256 /* 512 bytes on stack */\n#define SEMOPM_FAST\t64  /* ~ 372 bytes on stack */\n\n/*\n * linked list protection:\n *\tsem_undo.id_next,\n *\tsem_array.sem_pending{,last},\n *\tsem_array.sem_undo: sem_lock() for read/write\n *\tsem_undo.proc_next: only \"current\" is allowed to read/write that field.\n *\t\n */\n\n#define sc_semmsl\tsem_ctls[0]\n#define sc_semmns\tsem_ctls[1]\n#define sc_semopm\tsem_ctls[2]\n#define sc_semmni\tsem_ctls[3]\n\nvoid sem_init_ns(struct ipc_namespace *ns)\n{\n\tns->sc_semmsl = SEMMSL;\n\tns->sc_semmns = SEMMNS;\n\tns->sc_semopm = SEMOPM;\n\tns->sc_semmni = SEMMNI;\n\tns->used_sems = 0;\n\tipc_init_ids(&ns->ids[IPC_SEM_IDS]);\n}\n\n#ifdef CONFIG_IPC_NS\nvoid sem_exit_ns(struct ipc_namespace *ns)\n{\n\tfree_ipcs(ns, &sem_ids(ns), freeary);\n\tidr_destroy(&ns->ids[IPC_SEM_IDS].ipcs_idr);\n}\n#endif\n\nvoid __init sem_init (void)\n{\n\tsem_init_ns(&init_ipc_ns);\n\tipc_init_proc_interface(\"sysvipc/sem\",\n\t\t\t\t\"       key      semid perms      nsems   uid   gid  cuid  cgid      otime      ctime\\n\",\n\t\t\t\tIPC_SEM_IDS, sysvipc_sem_proc_show);\n}\n\n/*\n * If the request contains only one semaphore operation, and there are\n * no complex transactions pending, lock only the semaphore involved.\n * Otherwise, lock the entire semaphore array, since we either have\n * multiple semaphores in our own semops, or we need to look at\n * semaphores from other pending complex operations.\n *\n * Carefully guard against sma->complex_count changing between zero\n * and non-zero while we are spinning for the lock. The value of\n * sma->complex_count cannot change while we are holding the lock,\n * so sem_unlock should be fine.\n *\n * The global lock path checks that all the local locks have been released,\n * checking each local lock once. This means that the local lock paths\n * cannot start their critical sections while the global lock is held.\n */\nstatic inline int sem_lock(struct sem_array *sma, struct sembuf *sops,\n\t\t\t      int nsops)\n{\n\tint locknum;\n again:\n\tif (nsops == 1 && !sma->complex_count) {\n\t\tstruct sem *sem = sma->sem_base + sops->sem_num;\n\n\t\t/* Lock just the semaphore we are interested in. */\n\t\tspin_lock(&sem->lock);\n\n\t\t/*\n\t\t * If sma->complex_count was set while we were spinning,\n\t\t * we may need to look at things we did not lock here.\n\t\t */\n\t\tif (unlikely(sma->complex_count)) {\n\t\t\tspin_unlock(&sem->lock);\n\t\t\tgoto lock_array;\n\t\t}\n\n\t\t/*\n\t\t * Another process is holding the global lock on the\n\t\t * sem_array; we cannot enter our critical section,\n\t\t * but have to wait for the global lock to be released.\n\t\t */\n\t\tif (unlikely(spin_is_locked(&sma->sem_perm.lock))) {\n\t\t\tspin_unlock(&sem->lock);\n\t\t\tspin_unlock_wait(&sma->sem_perm.lock);\n\t\t\tgoto again;\n\t\t}\n\n\t\tlocknum = sops->sem_num;\n\t} else {\n\t\tint i;\n\t\t/*\n\t\t * Lock the semaphore array, and wait for all of the\n\t\t * individual semaphore locks to go away.  The code\n\t\t * above ensures no new single-lock holders will enter\n\t\t * their critical section while the array lock is held.\n\t\t */\n lock_array:\n\t\tspin_lock(&sma->sem_perm.lock);\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem *sem = sma->sem_base + i;\n\t\t\tspin_unlock_wait(&sem->lock);\n\t\t}\n\t\tlocknum = -1;\n\t}\n\treturn locknum;\n}\n\nstatic inline void sem_unlock(struct sem_array *sma, int locknum)\n{\n\tif (locknum == -1) {\n\t\tspin_unlock(&sma->sem_perm.lock);\n\t} else {\n\t\tstruct sem *sem = sma->sem_base + locknum;\n\t\tspin_unlock(&sem->lock);\n\t}\n\trcu_read_unlock();\n}\n\n/*\n * sem_lock_(check_) routines are called in the paths where the rw_mutex\n * is not held.\n */\nstatic inline struct sem_array *sem_obtain_lock(struct ipc_namespace *ns,\n\t\t\tint id, struct sembuf *sops, int nsops, int *locknum)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tstruct sem_array *sma;\n\n\trcu_read_lock();\n\tipcp = ipc_obtain_object(&sem_ids(ns), id);\n\tif (IS_ERR(ipcp)) {\n\t\tsma = ERR_CAST(ipcp);\n\t\tgoto err;\n\t}\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\t*locknum = sem_lock(sma, sops, nsops);\n\n\t/* ipc_rmid() may have already freed the ID while sem_lock\n\t * was spinning: verify that the structure is still valid\n\t */\n\tif (!ipcp->deleted)\n\t\treturn container_of(ipcp, struct sem_array, sem_perm);\n\n\tsem_unlock(sma, *locknum);\n\tsma = ERR_PTR(-EINVAL);\nerr:\n\trcu_read_unlock();\n\treturn sma;\n}\n\nstatic inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}\n\nstatic inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns,\n\t\t\t\t\t\t\tint id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_check(&sem_ids(ns), id);\n\n\tif (IS_ERR(ipcp))\n\t\treturn ERR_CAST(ipcp);\n\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}\n\nstatic inline void sem_lock_and_putref(struct sem_array *sma)\n{\n\trcu_read_lock();\n\tsem_lock(sma, NULL, -1);\n\tipc_rcu_putref(sma);\n}\n\nstatic inline void sem_getref_and_unlock(struct sem_array *sma)\n{\n\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n\tsem_unlock(sma, -1);\n}\n\nstatic inline void sem_putref(struct sem_array *sma)\n{\n\tsem_lock_and_putref(sma);\n\tsem_unlock(sma, -1);\n}\n\n/*\n * Call inside the rcu read section.\n */\nstatic inline void sem_getref(struct sem_array *sma)\n{\n\tsem_lock(sma, NULL, -1);\n\tWARN_ON_ONCE(!ipc_rcu_getref(sma));\n\tsem_unlock(sma, -1);\n}\n\nstatic inline void sem_rmid(struct ipc_namespace *ns, struct sem_array *s)\n{\n\tipc_rmid(&sem_ids(ns), &s->sem_perm);\n}\n\n/*\n * Lockless wakeup algorithm:\n * Without the check/retry algorithm a lockless wakeup is possible:\n * - queue.status is initialized to -EINTR before blocking.\n * - wakeup is performed by\n *\t* unlinking the queue entry from sma->sem_pending\n *\t* setting queue.status to IN_WAKEUP\n *\t  This is the notification for the blocked thread that a\n *\t  result value is imminent.\n *\t* call wake_up_process\n *\t* set queue.status to the final value.\n * - the previously blocked thread checks queue.status:\n *   \t* if it's IN_WAKEUP, then it must wait until the value changes\n *   \t* if it's not -EINTR, then the operation was completed by\n *   \t  update_queue. semtimedop can return queue.status without\n *   \t  performing any operation on the sem array.\n *   \t* otherwise it must acquire the spinlock and check what's up.\n *\n * The two-stage algorithm is necessary to protect against the following\n * races:\n * - if queue.status is set after wake_up_process, then the woken up idle\n *   thread could race forward and try (and fail) to acquire sma->lock\n *   before update_queue had a chance to set queue.status\n * - if queue.status is written before wake_up_process and if the\n *   blocked process is woken up by a signal between writing\n *   queue.status and the wake_up_process, then the woken up\n *   process could return from semtimedop and die by calling\n *   sys_exit before wake_up_process is called. Then wake_up_process\n *   will oops, because the task structure is already invalid.\n *   (yes, this happened on s390 with sysv msg).\n *\n */\n#define IN_WAKEUP\t1\n\n/**\n * newary - Create a new semaphore set\n * @ns: namespace\n * @params: ptr to the structure that contains key, semflg and nsems\n *\n * Called with sem_ids.rw_mutex held (as a writer)\n */\n\nstatic int newary(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tint id;\n\tint retval;\n\tstruct sem_array *sma;\n\tint size;\n\tkey_t key = params->key;\n\tint nsems = params->u.nsems;\n\tint semflg = params->flg;\n\tint i;\n\n\tif (!nsems)\n\t\treturn -EINVAL;\n\tif (ns->used_sems + nsems > ns->sc_semmns)\n\t\treturn -ENOSPC;\n\n\tsize = sizeof (*sma) + nsems * sizeof (struct sem);\n\tsma = ipc_rcu_alloc(size);\n\tif (!sma) {\n\t\treturn -ENOMEM;\n\t}\n\tmemset (sma, 0, size);\n\n\tsma->sem_perm.mode = (semflg & S_IRWXUGO);\n\tsma->sem_perm.key = key;\n\n\tsma->sem_perm.security = NULL;\n\tretval = security_sem_alloc(sma);\n\tif (retval) {\n\t\tipc_rcu_putref(sma);\n\t\treturn retval;\n\t}\n\n\tid = ipc_addid(&sem_ids(ns), &sma->sem_perm, ns->sc_semmni);\n\tif (id < 0) {\n\t\tsecurity_sem_free(sma);\n\t\tipc_rcu_putref(sma);\n\t\treturn id;\n\t}\n\tns->used_sems += nsems;\n\n\tsma->sem_base = (struct sem *) &sma[1];\n\n\tfor (i = 0; i < nsems; i++) {\n\t\tINIT_LIST_HEAD(&sma->sem_base[i].sem_pending);\n\t\tspin_lock_init(&sma->sem_base[i].lock);\n\t}\n\n\tsma->complex_count = 0;\n\tINIT_LIST_HEAD(&sma->sem_pending);\n\tINIT_LIST_HEAD(&sma->list_id);\n\tsma->sem_nsems = nsems;\n\tsma->sem_ctime = get_seconds();\n\tsem_unlock(sma, -1);\n\n\treturn sma->sem_perm.id;\n}\n\n\n/*\n * Called with sem_ids.rw_mutex and ipcp locked.\n */\nstatic inline int sem_security(struct kern_ipc_perm *ipcp, int semflg)\n{\n\tstruct sem_array *sma;\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\treturn security_sem_associate(sma, semflg);\n}\n\n/*\n * Called with sem_ids.rw_mutex and ipcp locked.\n */\nstatic inline int sem_more_checks(struct kern_ipc_perm *ipcp,\n\t\t\t\tstruct ipc_params *params)\n{\n\tstruct sem_array *sma;\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\tif (params->u.nsems > sma->sem_nsems)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nSYSCALL_DEFINE3(semget, key_t, key, int, nsems, int, semflg)\n{\n\tstruct ipc_namespace *ns;\n\tstruct ipc_ops sem_ops;\n\tstruct ipc_params sem_params;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsems < 0 || nsems > ns->sc_semmsl)\n\t\treturn -EINVAL;\n\n\tsem_ops.getnew = newary;\n\tsem_ops.associate = sem_security;\n\tsem_ops.more_checks = sem_more_checks;\n\n\tsem_params.key = key;\n\tsem_params.flg = semflg;\n\tsem_params.u.nsems = nsems;\n\n\treturn ipcget(ns, &sem_ids(ns), &sem_ops, &sem_params);\n}\n\n/*\n * Determine whether a sequence of semaphore operations would succeed\n * all at once. Return 0 if yes, 1 if need to sleep, else return error code.\n */\n\nstatic int try_atomic_semop (struct sem_array * sma, struct sembuf * sops,\n\t\t\t     int nsops, struct sem_undo *un, int pid)\n{\n\tint result, sem_op;\n\tstruct sembuf *sop;\n\tstruct sem * curr;\n\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tcurr = sma->sem_base + sop->sem_num;\n\t\tsem_op = sop->sem_op;\n\t\tresult = curr->semval;\n  \n\t\tif (!sem_op && result)\n\t\t\tgoto would_block;\n\n\t\tresult += sem_op;\n\t\tif (result < 0)\n\t\t\tgoto would_block;\n\t\tif (result > SEMVMX)\n\t\t\tgoto out_of_range;\n\t\tif (sop->sem_flg & SEM_UNDO) {\n\t\t\tint undo = un->semadj[sop->sem_num] - sem_op;\n\t\t\t/*\n\t \t\t *\tExceeding the undo range is an error.\n\t\t\t */\n\t\t\tif (undo < (-SEMAEM - 1) || undo > SEMAEM)\n\t\t\t\tgoto out_of_range;\n\t\t}\n\t\tcurr->semval = result;\n\t}\n\n\tsop--;\n\twhile (sop >= sops) {\n\t\tsma->sem_base[sop->sem_num].sempid = pid;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tun->semadj[sop->sem_num] -= sop->sem_op;\n\t\tsop--;\n\t}\n\t\n\treturn 0;\n\nout_of_range:\n\tresult = -ERANGE;\n\tgoto undo;\n\nwould_block:\n\tif (sop->sem_flg & IPC_NOWAIT)\n\t\tresult = -EAGAIN;\n\telse\n\t\tresult = 1;\n\nundo:\n\tsop--;\n\twhile (sop >= sops) {\n\t\tsma->sem_base[sop->sem_num].semval -= sop->sem_op;\n\t\tsop--;\n\t}\n\n\treturn result;\n}\n\n/** wake_up_sem_queue_prepare(q, error): Prepare wake-up\n * @q: queue entry that must be signaled\n * @error: Error value for the signal\n *\n * Prepare the wake-up of the queue entry q.\n */\nstatic void wake_up_sem_queue_prepare(struct list_head *pt,\n\t\t\t\tstruct sem_queue *q, int error)\n{\n\tif (list_empty(pt)) {\n\t\t/*\n\t\t * Hold preempt off so that we don't get preempted and have the\n\t\t * wakee busy-wait until we're scheduled back on.\n\t\t */\n\t\tpreempt_disable();\n\t}\n\tq->status = IN_WAKEUP;\n\tq->pid = error;\n\n\tlist_add_tail(&q->list, pt);\n}\n\n/**\n * wake_up_sem_queue_do(pt) - do the actual wake-up\n * @pt: list of tasks to be woken up\n *\n * Do the actual wake-up.\n * The function is called without any locks held, thus the semaphore array\n * could be destroyed already and the tasks can disappear as soon as the\n * status is set to the actual return code.\n */\nstatic void wake_up_sem_queue_do(struct list_head *pt)\n{\n\tstruct sem_queue *q, *t;\n\tint did_something;\n\n\tdid_something = !list_empty(pt);\n\tlist_for_each_entry_safe(q, t, pt, list) {\n\t\twake_up_process(q->sleeper);\n\t\t/* q can disappear immediately after writing q->status. */\n\t\tsmp_wmb();\n\t\tq->status = q->pid;\n\t}\n\tif (did_something)\n\t\tpreempt_enable();\n}\n\nstatic void unlink_queue(struct sem_array *sma, struct sem_queue *q)\n{\n\tlist_del(&q->list);\n\tif (q->nsops > 1)\n\t\tsma->complex_count--;\n}\n\n/** check_restart(sma, q)\n * @sma: semaphore array\n * @q: the operation that just completed\n *\n * update_queue is O(N^2) when it restarts scanning the whole queue of\n * waiting operations. Therefore this function checks if the restart is\n * really necessary. It is called after a previously waiting operation\n * was completed.\n */\nstatic int check_restart(struct sem_array *sma, struct sem_queue *q)\n{\n\tstruct sem *curr;\n\tstruct sem_queue *h;\n\n\t/* if the operation didn't modify the array, then no restart */\n\tif (q->alter == 0)\n\t\treturn 0;\n\n\t/* pending complex operations are too difficult to analyse */\n\tif (sma->complex_count)\n\t\treturn 1;\n\n\t/* we were a sleeping complex operation. Too difficult */\n\tif (q->nsops > 1)\n\t\treturn 1;\n\n\tcurr = sma->sem_base + q->sops[0].sem_num;\n\n\t/* No-one waits on this queue */\n\tif (list_empty(&curr->sem_pending))\n\t\treturn 0;\n\n\t/* the new semaphore value */\n\tif (curr->semval) {\n\t\t/* It is impossible that someone waits for the new value:\n\t\t * - q is a previously sleeping simple operation that\n\t\t *   altered the array. It must be a decrement, because\n\t\t *   simple increments never sleep.\n\t\t * - The value is not 0, thus wait-for-zero won't proceed.\n\t\t * - If there are older (higher priority) decrements\n\t\t *   in the queue, then they have observed the original\n\t\t *   semval value and couldn't proceed. The operation\n\t\t *   decremented to value - thus they won't proceed either.\n\t\t */\n\t\tBUG_ON(q->sops[0].sem_op >= 0);\n\t\treturn 0;\n\t}\n\t/*\n\t * semval is 0. Check if there are wait-for-zero semops.\n\t * They must be the first entries in the per-semaphore queue\n\t */\n\th = list_first_entry(&curr->sem_pending, struct sem_queue, list);\n\tBUG_ON(h->nsops != 1);\n\tBUG_ON(h->sops[0].sem_num != q->sops[0].sem_num);\n\n\t/* Yes, there is a wait-for-zero semop. Restart */\n\tif (h->sops[0].sem_op == 0)\n\t\treturn 1;\n\n\t/* Again - no-one is waiting for the new value. */\n\treturn 0;\n}\n\n\n/**\n * update_queue(sma, semnum): Look for tasks that can be completed.\n * @sma: semaphore array.\n * @semnum: semaphore that was modified.\n * @pt: list head for the tasks that must be woken up.\n *\n * update_queue must be called after a semaphore in a semaphore array\n * was modified. If multiple semaphores were modified, update_queue must\n * be called with semnum = -1, as well as with the number of each modified\n * semaphore.\n * The tasks that must be woken up are added to @pt. The return code\n * is stored in q->pid.\n * The function return 1 if at least one semop was completed successfully.\n */\nstatic int update_queue(struct sem_array *sma, int semnum, struct list_head *pt)\n{\n\tstruct sem_queue *q;\n\tstruct list_head *walk;\n\tstruct list_head *pending_list;\n\tint semop_completed = 0;\n\n\tif (semnum == -1)\n\t\tpending_list = &sma->sem_pending;\n\telse\n\t\tpending_list = &sma->sem_base[semnum].sem_pending;\n\nagain:\n\twalk = pending_list->next;\n\twhile (walk != pending_list) {\n\t\tint error, restart;\n\n\t\tq = container_of(walk, struct sem_queue, list);\n\t\twalk = walk->next;\n\n\t\t/* If we are scanning the single sop, per-semaphore list of\n\t\t * one semaphore and that semaphore is 0, then it is not\n\t\t * necessary to scan the \"alter\" entries: simple increments\n\t\t * that affect only one entry succeed immediately and cannot\n\t\t * be in the  per semaphore pending queue, and decrements\n\t\t * cannot be successful if the value is already 0.\n\t\t */\n\t\tif (semnum != -1 && sma->sem_base[semnum].semval == 0 &&\n\t\t\t\tq->alter)\n\t\t\tbreak;\n\n\t\terror = try_atomic_semop(sma, q->sops, q->nsops,\n\t\t\t\t\t q->undo, q->pid);\n\n\t\t/* Does q->sleeper still need to sleep? */\n\t\tif (error > 0)\n\t\t\tcontinue;\n\n\t\tunlink_queue(sma, q);\n\n\t\tif (error) {\n\t\t\trestart = 0;\n\t\t} else {\n\t\t\tsemop_completed = 1;\n\t\t\trestart = check_restart(sma, q);\n\t\t}\n\n\t\twake_up_sem_queue_prepare(pt, q, error);\n\t\tif (restart)\n\t\t\tgoto again;\n\t}\n\treturn semop_completed;\n}\n\n/**\n * do_smart_update(sma, sops, nsops, otime, pt) - optimized update_queue\n * @sma: semaphore array\n * @sops: operations that were performed\n * @nsops: number of operations\n * @otime: force setting otime\n * @pt: list head of the tasks that must be woken up.\n *\n * do_smart_update() does the required called to update_queue, based on the\n * actual changes that were performed on the semaphore array.\n * Note that the function does not do the actual wake-up: the caller is\n * responsible for calling wake_up_sem_queue_do(@pt).\n * It is safe to perform this call after dropping all locks.\n */\nstatic void do_smart_update(struct sem_array *sma, struct sembuf *sops, int nsops,\n\t\t\tint otime, struct list_head *pt)\n{\n\tint i;\n\n\tif (sma->complex_count || sops == NULL) {\n\t\tif (update_queue(sma, -1, pt))\n\t\t\totime = 1;\n\t}\n\n\tif (!sops) {\n\t\t/* No semops; something special is going on. */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tif (update_queue(sma, i, pt))\n\t\t\t\totime = 1;\n\t\t}\n\t\tgoto done;\n\t}\n\n\t/* Check the semaphores that were modified. */\n\tfor (i = 0; i < nsops; i++) {\n\t\tif (sops[i].sem_op > 0 ||\n\t\t\t(sops[i].sem_op < 0 &&\n\t\t\t\tsma->sem_base[sops[i].sem_num].semval == 0))\n\t\t\tif (update_queue(sma, sops[i].sem_num, pt))\n\t\t\t\totime = 1;\n\t}\ndone:\n\tif (otime)\n\t\tsma->sem_otime = get_seconds();\n}\n\n\n/* The following counts are associated to each semaphore:\n *   semncnt        number of tasks waiting on semval being nonzero\n *   semzcnt        number of tasks waiting on semval being zero\n * This model assumes that a task waits on exactly one semaphore.\n * Since semaphore operations are to be performed atomically, tasks actually\n * wait on a whole sequence of semaphores simultaneously.\n * The counts we return here are a rough approximation, but still\n * warrant that semncnt+semzcnt>0 if the task is on the pending queue.\n */\nstatic int count_semncnt (struct sem_array * sma, ushort semnum)\n{\n\tint semncnt;\n\tstruct sem_queue * q;\n\n\tsemncnt = 0;\n\tlist_for_each_entry(q, &sma->sem_pending, list) {\n\t\tstruct sembuf * sops = q->sops;\n\t\tint nsops = q->nsops;\n\t\tint i;\n\t\tfor (i = 0; i < nsops; i++)\n\t\t\tif (sops[i].sem_num == semnum\n\t\t\t    && (sops[i].sem_op < 0)\n\t\t\t    && !(sops[i].sem_flg & IPC_NOWAIT))\n\t\t\t\tsemncnt++;\n\t}\n\treturn semncnt;\n}\n\nstatic int count_semzcnt (struct sem_array * sma, ushort semnum)\n{\n\tint semzcnt;\n\tstruct sem_queue * q;\n\n\tsemzcnt = 0;\n\tlist_for_each_entry(q, &sma->sem_pending, list) {\n\t\tstruct sembuf * sops = q->sops;\n\t\tint nsops = q->nsops;\n\t\tint i;\n\t\tfor (i = 0; i < nsops; i++)\n\t\t\tif (sops[i].sem_num == semnum\n\t\t\t    && (sops[i].sem_op == 0)\n\t\t\t    && !(sops[i].sem_flg & IPC_NOWAIT))\n\t\t\t\tsemzcnt++;\n\t}\n\treturn semzcnt;\n}\n\n/* Free a semaphore set. freeary() is called with sem_ids.rw_mutex locked\n * as a writer and the spinlock for this semaphore set hold. sem_ids.rw_mutex\n * remains locked on exit.\n */\nstatic void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)\n{\n\tstruct sem_undo *un, *tu;\n\tstruct sem_queue *q, *tq;\n\tstruct sem_array *sma = container_of(ipcp, struct sem_array, sem_perm);\n\tstruct list_head tasks;\n\tint i;\n\n\t/* Free the existing undo structures for this semaphore set.  */\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry_safe(un, tu, &sma->list_id, list_id) {\n\t\tlist_del(&un->list_id);\n\t\tspin_lock(&un->ulp->lock);\n\t\tun->semid = -1;\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&un->ulp->lock);\n\t\tkfree_rcu(un, rcu);\n\t}\n\n\t/* Wake up all pending processes and let them fail with EIDRM. */\n\tINIT_LIST_HEAD(&tasks);\n\tlist_for_each_entry_safe(q, tq, &sma->sem_pending, list) {\n\t\tunlink_queue(sma, q);\n\t\twake_up_sem_queue_prepare(&tasks, q, -EIDRM);\n\t}\n\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\tstruct sem *sem = sma->sem_base + i;\n\t\tlist_for_each_entry_safe(q, tq, &sem->sem_pending, list) {\n\t\t\tunlink_queue(sma, q);\n\t\t\twake_up_sem_queue_prepare(&tasks, q, -EIDRM);\n\t\t}\n\t}\n\n\t/* Remove the semaphore set from the IDR */\n\tsem_rmid(ns, sma);\n\tsem_unlock(sma, -1);\n\n\twake_up_sem_queue_do(&tasks);\n\tns->used_sems -= sma->sem_nsems;\n\tsecurity_sem_free(sma);\n\tipc_rcu_putref(sma);\n}\n\nstatic unsigned long copy_semid_to_user(void __user *buf, struct semid64_ds *in, int version)\n{\n\tswitch(version) {\n\tcase IPC_64:\n\t\treturn copy_to_user(buf, in, sizeof(*in));\n\tcase IPC_OLD:\n\t    {\n\t\tstruct semid_ds out;\n\n\t\tmemset(&out, 0, sizeof(out));\n\n\t\tipc64_perm_to_ipc_perm(&in->sem_perm, &out.sem_perm);\n\n\t\tout.sem_otime\t= in->sem_otime;\n\t\tout.sem_ctime\t= in->sem_ctime;\n\t\tout.sem_nsems\t= in->sem_nsems;\n\n\t\treturn copy_to_user(buf, &out, sizeof(out));\n\t    }\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int semctl_nolock(struct ipc_namespace *ns, int semid,\n\t\t\t int cmd, int version, void __user *p)\n{\n\tint err;\n\tstruct sem_array *sma;\n\n\tswitch(cmd) {\n\tcase IPC_INFO:\n\tcase SEM_INFO:\n\t{\n\t\tstruct seminfo seminfo;\n\t\tint max_id;\n\n\t\terr = security_sem_semctl(NULL, cmd);\n\t\tif (err)\n\t\t\treturn err;\n\t\t\n\t\tmemset(&seminfo,0,sizeof(seminfo));\n\t\tseminfo.semmni = ns->sc_semmni;\n\t\tseminfo.semmns = ns->sc_semmns;\n\t\tseminfo.semmsl = ns->sc_semmsl;\n\t\tseminfo.semopm = ns->sc_semopm;\n\t\tseminfo.semvmx = SEMVMX;\n\t\tseminfo.semmnu = SEMMNU;\n\t\tseminfo.semmap = SEMMAP;\n\t\tseminfo.semume = SEMUME;\n\t\tdown_read(&sem_ids(ns).rw_mutex);\n\t\tif (cmd == SEM_INFO) {\n\t\t\tseminfo.semusz = sem_ids(ns).in_use;\n\t\t\tseminfo.semaem = ns->used_sems;\n\t\t} else {\n\t\t\tseminfo.semusz = SEMUSZ;\n\t\t\tseminfo.semaem = SEMAEM;\n\t\t}\n\t\tmax_id = ipc_get_maxid(&sem_ids(ns));\n\t\tup_read(&sem_ids(ns).rw_mutex);\n\t\tif (copy_to_user(p, &seminfo, sizeof(struct seminfo))) \n\t\t\treturn -EFAULT;\n\t\treturn (max_id < 0) ? 0: max_id;\n\t}\n\tcase IPC_STAT:\n\tcase SEM_STAT:\n\t{\n\t\tstruct semid64_ds tbuf;\n\t\tint id = 0;\n\n\t\tmemset(&tbuf, 0, sizeof(tbuf));\n\n\t\tif (cmd == SEM_STAT) {\n\t\t\trcu_read_lock();\n\t\t\tsma = sem_obtain_object(ns, semid);\n\t\t\tif (IS_ERR(sma)) {\n\t\t\t\terr = PTR_ERR(sma);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tid = sma->sem_perm.id;\n\t\t} else {\n\t\t\trcu_read_lock();\n\t\t\tsma = sem_obtain_object_check(ns, semid);\n\t\t\tif (IS_ERR(sma)) {\n\t\t\t\terr = PTR_ERR(sma);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\n\t\terr = -EACCES;\n\t\tif (ipcperms(ns, &sma->sem_perm, S_IRUGO))\n\t\t\tgoto out_unlock;\n\n\t\terr = security_sem_semctl(sma, cmd);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\n\t\tkernel_to_ipc64_perm(&sma->sem_perm, &tbuf.sem_perm);\n\t\ttbuf.sem_otime  = sma->sem_otime;\n\t\ttbuf.sem_ctime  = sma->sem_ctime;\n\t\ttbuf.sem_nsems  = sma->sem_nsems;\n\t\trcu_read_unlock();\n\t\tif (copy_semid_to_user(p, &tbuf, version))\n\t\t\treturn -EFAULT;\n\t\treturn id;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum,\n\t\tunsigned long arg)\n{\n\tstruct sem_undo *un;\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err;\n\tstruct list_head tasks;\n\tint val;\n#if defined(CONFIG_64BIT) && defined(__BIG_ENDIAN)\n\t/* big-endian 64bit */\n\tval = arg >> 32;\n#else\n\t/* 32bit or little-endian 64bit */\n\tval = arg;\n#endif\n\n\tif (val > SEMVMX || val < 0)\n\t\treturn -ERANGE;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tif (semnum < 0 || semnum >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (ipcperms(ns, &sma->sem_perm, S_IWUGO)) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\terr = security_sem_semctl(sma, SETVAL);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\treturn -EACCES;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\n\tcurr = &sma->sem_base[semnum];\n\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_for_each_entry(un, &sma->list_id, list_id)\n\t\tun->semadj[semnum] = 0;\n\n\tcurr->semval = val;\n\tcurr->sempid = task_tgid_vnr(current);\n\tsma->sem_ctime = get_seconds();\n\t/* maybe some queued-up processes were waiting for this */\n\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\tsem_unlock(sma, -1);\n\twake_up_sem_queue_do(&tasks);\n\treturn 0;\n}\n\nstatic int semctl_main(struct ipc_namespace *ns, int semid, int semnum,\n\t\tint cmd, void __user *p)\n{\n\tstruct sem_array *sma;\n\tstruct sem* curr;\n\tint err, nsems;\n\tushort fast_sem_io[SEMMSL_FAST];\n\tushort* sem_io = fast_sem_io;\n\tstruct list_head tasks;\n\n\tINIT_LIST_HEAD(&tasks);\n\n\trcu_read_lock();\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn PTR_ERR(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm,\n\t\t\tcmd == SETALL ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terr = -EACCES;\n\tswitch (cmd) {\n\tcase GETALL:\n\t{\n\t\tushort __user *array = p;\n\t\tint i;\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_getref(sma);\n\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tsem_lock_and_putref(sma);\n\t\t\tif (sma->sem_perm.deleted) {\n\t\t\t\tsem_unlock(sma, -1);\n\t\t\t\terr = -EIDRM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t} else\n\t\t\tsem_lock(sma, NULL, -1);\n\n\t\tfor (i = 0; i < sma->sem_nsems; i++)\n\t\t\tsem_io[i] = sma->sem_base[i].semval;\n\t\tsem_unlock(sma, -1);\n\t\terr = 0;\n\t\tif(copy_to_user(array, sem_io, nsems*sizeof(ushort)))\n\t\t\terr = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tcase SETALL:\n\t{\n\t\tint i;\n\t\tstruct sem_undo *un;\n\n\t\tif (!ipc_rcu_getref(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIDRM;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif(nsems > SEMMSL_FAST) {\n\t\t\tsem_io = ipc_alloc(sizeof(ushort)*nsems);\n\t\t\tif(sem_io == NULL) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (copy_from_user (sem_io, p, nsems*sizeof(ushort))) {\n\t\t\tsem_putref(sma);\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++) {\n\t\t\tif (sem_io[i] > SEMVMX) {\n\t\t\t\tsem_putref(sma);\n\t\t\t\terr = -ERANGE;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t\tsem_lock_and_putref(sma);\n\t\tif (sma->sem_perm.deleted) {\n\t\t\tsem_unlock(sma, -1);\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tfor (i = 0; i < nsems; i++)\n\t\t\tsma->sem_base[i].semval = sem_io[i];\n\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_for_each_entry(un, &sma->list_id, list_id) {\n\t\t\tfor (i = 0; i < nsems; i++)\n\t\t\t\tun->semadj[i] = 0;\n\t\t}\n\t\tsma->sem_ctime = get_seconds();\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tdo_smart_update(sma, NULL, 0, 0, &tasks);\n\t\terr = 0;\n\t\tgoto out_unlock;\n\t}\n\t/* GETVAL, GETPID, GETNCTN, GETZCNT: fall-through */\n\t}\n\terr = -EINVAL;\n\tif (semnum < 0 || semnum >= nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\tsem_lock(sma, NULL, -1);\n\tcurr = &sma->sem_base[semnum];\n\n\tswitch (cmd) {\n\tcase GETVAL:\n\t\terr = curr->semval;\n\t\tgoto out_unlock;\n\tcase GETPID:\n\t\terr = curr->sempid;\n\t\tgoto out_unlock;\n\tcase GETNCNT:\n\t\terr = count_semncnt(sma,semnum);\n\t\tgoto out_unlock;\n\tcase GETZCNT:\n\t\terr = count_semzcnt(sma,semnum);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sem_io != fast_sem_io)\n\t\tipc_free(sem_io, sizeof(ushort)*nsems);\n\treturn err;\n}\n\nstatic inline unsigned long\ncopy_semid_from_user(struct semid64_ds *out, void __user *buf, int version)\n{\n\tswitch(version) {\n\tcase IPC_64:\n\t\tif (copy_from_user(out, buf, sizeof(*out)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase IPC_OLD:\n\t    {\n\t\tstruct semid_ds tbuf_old;\n\n\t\tif(copy_from_user(&tbuf_old, buf, sizeof(tbuf_old)))\n\t\t\treturn -EFAULT;\n\n\t\tout->sem_perm.uid\t= tbuf_old.sem_perm.uid;\n\t\tout->sem_perm.gid\t= tbuf_old.sem_perm.gid;\n\t\tout->sem_perm.mode\t= tbuf_old.sem_perm.mode;\n\n\t\treturn 0;\n\t    }\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n/*\n * This function handles some semctl commands which require the rw_mutex\n * to be held in write mode.\n * NOTE: no locks must be held, the rw_mutex is taken inside this function.\n */\nstatic int semctl_down(struct ipc_namespace *ns, int semid,\n\t\t       int cmd, int version, void __user *p)\n{\n\tstruct sem_array *sma;\n\tint err;\n\tstruct semid64_ds semid64;\n\tstruct kern_ipc_perm *ipcp;\n\n\tif(cmd == IPC_SET) {\n\t\tif (copy_semid_from_user(&semid64, p, version))\n\t\t\treturn -EFAULT;\n\t}\n\n\tipcp = ipcctl_pre_down_nolock(ns, &sem_ids(ns), semid, cmd,\n\t\t\t\t      &semid64.sem_perm, 0);\n\tif (IS_ERR(ipcp))\n\t\treturn PTR_ERR(ipcp);\n\n\tsma = container_of(ipcp, struct sem_array, sem_perm);\n\n\terr = security_sem_semctl(sma, cmd);\n\tif (err) {\n\t\trcu_read_unlock();\n\t\tgoto out_unlock;\n\t}\n\n\tswitch(cmd){\n\tcase IPC_RMID:\n\t\tsem_lock(sma, NULL, -1);\n\t\tfreeary(ns, ipcp);\n\t\tgoto out_up;\n\tcase IPC_SET:\n\t\tsem_lock(sma, NULL, -1);\n\t\terr = ipc_update_perm(&semid64.sem_perm, ipcp);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t\tsma->sem_ctime = get_seconds();\n\t\tbreak;\n\tdefault:\n\t\trcu_read_unlock();\n\t\terr = -EINVAL;\n\t\tgoto out_up;\n\t}\n\nout_unlock:\n\tsem_unlock(sma, -1);\nout_up:\n\tup_write(&sem_ids(ns).rw_mutex);\n\treturn err;\n}\n\nSYSCALL_DEFINE4(semctl, int, semid, int, semnum, int, cmd, unsigned long, arg)\n{\n\tint version;\n\tstruct ipc_namespace *ns;\n\tvoid __user *p = (void __user *)arg;\n\n\tif (semid < 0)\n\t\treturn -EINVAL;\n\n\tversion = ipc_parse_version(&cmd);\n\tns = current->nsproxy->ipc_ns;\n\n\tswitch(cmd) {\n\tcase IPC_INFO:\n\tcase SEM_INFO:\n\tcase IPC_STAT:\n\tcase SEM_STAT:\n\t\treturn semctl_nolock(ns, semid, cmd, version, p);\n\tcase GETALL:\n\tcase GETVAL:\n\tcase GETPID:\n\tcase GETNCNT:\n\tcase GETZCNT:\n\tcase SETALL:\n\t\treturn semctl_main(ns, semid, semnum, cmd, p);\n\tcase SETVAL:\n\t\treturn semctl_setval(ns, semid, semnum, arg);\n\tcase IPC_RMID:\n\tcase IPC_SET:\n\t\treturn semctl_down(ns, semid, cmd, version, p);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n/* If the task doesn't already have a undo_list, then allocate one\n * here.  We guarantee there is only one thread using this undo list,\n * and current is THE ONE\n *\n * If this allocation and assignment succeeds, but later\n * portions of this code fail, there is no need to free the sem_undo_list.\n * Just let it stay associated with the task, and it'll be freed later\n * at exit time.\n *\n * This can block, so callers must hold no locks.\n */\nstatic inline int get_undo_list(struct sem_undo_list **undo_listp)\n{\n\tstruct sem_undo_list *undo_list;\n\n\tundo_list = current->sysvsem.undo_list;\n\tif (!undo_list) {\n\t\tundo_list = kzalloc(sizeof(*undo_list), GFP_KERNEL);\n\t\tif (undo_list == NULL)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_init(&undo_list->lock);\n\t\tatomic_set(&undo_list->refcnt, 1);\n\t\tINIT_LIST_HEAD(&undo_list->list_proc);\n\n\t\tcurrent->sysvsem.undo_list = undo_list;\n\t}\n\t*undo_listp = undo_list;\n\treturn 0;\n}\n\nstatic struct sem_undo *__lookup_undo(struct sem_undo_list *ulp, int semid)\n{\n\tstruct sem_undo *un;\n\n\tlist_for_each_entry_rcu(un, &ulp->list_proc, list_proc) {\n\t\tif (un->semid == semid)\n\t\t\treturn un;\n\t}\n\treturn NULL;\n}\n\nstatic struct sem_undo *lookup_undo(struct sem_undo_list *ulp, int semid)\n{\n\tstruct sem_undo *un;\n\n  \tassert_spin_locked(&ulp->lock);\n\n\tun = __lookup_undo(ulp, semid);\n\tif (un) {\n\t\tlist_del_rcu(&un->list_proc);\n\t\tlist_add_rcu(&un->list_proc, &ulp->list_proc);\n\t}\n\treturn un;\n}\n\n/**\n * find_alloc_undo - Lookup (and if not present create) undo array\n * @ns: namespace\n * @semid: semaphore array id\n *\n * The function looks up (and if not present creates) the undo structure.\n * The size of the undo structure depends on the size of the semaphore\n * array, thus the alloc path is not that straightforward.\n * Lifetime-rules: sem_undo is rcu-protected, on success, the function\n * performs a rcu_read_lock().\n */\nstatic struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)\n{\n\tstruct sem_array *sma;\n\tstruct sem_undo_list *ulp;\n\tstruct sem_undo *un, *new;\n\tint nsems, error;\n\n\terror = get_undo_list(&ulp);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\trcu_read_lock();\n\tspin_lock(&ulp->lock);\n\tun = lookup_undo(ulp, semid);\n\tspin_unlock(&ulp->lock);\n\tif (likely(un!=NULL))\n\t\tgoto out;\n\n\t/* no undo structure around - allocate one. */\n\t/* step 1: figure out the size of the semaphore array */\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\treturn ERR_CAST(sma);\n\t}\n\n\tnsems = sma->sem_nsems;\n\tif (!ipc_rcu_getref(sma)) {\n\t\trcu_read_unlock();\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\t/* step 2: allocate new undo structure */\n\tnew = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);\n\tif (!new) {\n\t\tsem_putref(sma);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* step 3: Acquire the lock on semaphore array */\n\tsem_lock_and_putref(sma);\n\tif (sma->sem_perm.deleted) {\n\t\tsem_unlock(sma, -1);\n\t\tkfree(new);\n\t\tun = ERR_PTR(-EIDRM);\n\t\tgoto out;\n\t}\n\tspin_lock(&ulp->lock);\n\n\t/*\n\t * step 4: check for races: did someone else allocate the undo struct?\n\t */\n\tun = lookup_undo(ulp, semid);\n\tif (un) {\n\t\tkfree(new);\n\t\tgoto success;\n\t}\n\t/* step 5: initialize & link new undo structure */\n\tnew->semadj = (short *) &new[1];\n\tnew->ulp = ulp;\n\tnew->semid = semid;\n\tassert_spin_locked(&ulp->lock);\n\tlist_add_rcu(&new->list_proc, &ulp->list_proc);\n\tassert_spin_locked(&sma->sem_perm.lock);\n\tlist_add(&new->list_id, &sma->list_id);\n\tun = new;\n\nsuccess:\n\tspin_unlock(&ulp->lock);\n\trcu_read_lock();\n\tsem_unlock(sma, -1);\nout:\n\treturn un;\n}\n\n\n/**\n * get_queue_result - Retrieve the result code from sem_queue\n * @q: Pointer to queue structure\n *\n * Retrieve the return code from the pending queue. If IN_WAKEUP is found in\n * q->status, then we must loop until the value is replaced with the final\n * value: This may happen if a task is woken up by an unrelated event (e.g.\n * signal) and in parallel the task is woken up by another task because it got\n * the requested semaphores.\n *\n * The function can be called with or without holding the semaphore spinlock.\n */\nstatic int get_queue_result(struct sem_queue *q)\n{\n\tint error;\n\n\terror = q->status;\n\twhile (unlikely(error == IN_WAKEUP)) {\n\t\tcpu_relax();\n\t\terror = q->status;\n\t}\n\n\treturn error;\n}\n\n\nSYSCALL_DEFINE4(semtimedop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops, const struct timespec __user *, timeout)\n{\n\tint error = -EINVAL;\n\tstruct sem_array *sma;\n\tstruct sembuf fast_sops[SEMOPM_FAST];\n\tstruct sembuf* sops = fast_sops, *sop;\n\tstruct sem_undo *un;\n\tint undos = 0, alter = 0, max, locknum;\n\tstruct sem_queue queue;\n\tunsigned long jiffies_left = 0;\n\tstruct ipc_namespace *ns;\n\tstruct list_head tasks;\n\n\tns = current->nsproxy->ipc_ns;\n\n\tif (nsops < 1 || semid < 0)\n\t\treturn -EINVAL;\n\tif (nsops > ns->sc_semopm)\n\t\treturn -E2BIG;\n\tif(nsops > SEMOPM_FAST) {\n\t\tsops = kmalloc(sizeof(*sops)*nsops,GFP_KERNEL);\n\t\tif(sops==NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user (sops, tsops, nsops * sizeof(*tsops))) {\n\t\terror=-EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (timeout) {\n\t\tstruct timespec _timeout;\n\t\tif (copy_from_user(&_timeout, timeout, sizeof(*timeout))) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (_timeout.tv_sec < 0 || _timeout.tv_nsec < 0 ||\n\t\t\t_timeout.tv_nsec >= 1000000000L) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tjiffies_left = timespec_to_jiffies(&_timeout);\n\t}\n\tmax = 0;\n\tfor (sop = sops; sop < sops + nsops; sop++) {\n\t\tif (sop->sem_num >= max)\n\t\t\tmax = sop->sem_num;\n\t\tif (sop->sem_flg & SEM_UNDO)\n\t\t\tundos = 1;\n\t\tif (sop->sem_op != 0)\n\t\t\talter = 1;\n\t}\n\n\tINIT_LIST_HEAD(&tasks);\n\n\tif (undos) {\n\t\t/* On success, find_alloc_undo takes the rcu_read_lock */\n\t\tun = find_alloc_undo(ns, semid);\n\t\tif (IS_ERR(un)) {\n\t\t\terror = PTR_ERR(un);\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\tun = NULL;\n\t\trcu_read_lock();\n\t}\n\n\tsma = sem_obtain_object_check(ns, semid);\n\tif (IS_ERR(sma)) {\n\t\trcu_read_unlock();\n\t\terror = PTR_ERR(sma);\n\t\tgoto out_free;\n\t}\n\n\terror = -EFBIG;\n\tif (max >= sma->sem_nsems) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = -EACCES;\n\tif (ipcperms(ns, &sma->sem_perm, alter ? S_IWUGO : S_IRUGO)) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\terror = security_sem_semop(sma, sops, nsops, alter);\n\tif (error) {\n\t\trcu_read_unlock();\n\t\tgoto out_wakeup;\n\t}\n\n\t/*\n\t * semid identifiers are not unique - find_alloc_undo may have\n\t * allocated an undo structure, it was invalidated by an RMID\n\t * and now a new array with received the same id. Check and fail.\n\t * This case can be detected checking un->semid. The existence of\n\t * \"un\" itself is guaranteed by rcu.\n\t */\n\terror = -EIDRM;\n\tlocknum = sem_lock(sma, sops, nsops);\n\tif (un && un->semid == -1)\n\t\tgoto out_unlock_free;\n\n\terror = try_atomic_semop (sma, sops, nsops, un, task_tgid_vnr(current));\n\tif (error <= 0) {\n\t\tif (alter && error == 0)\n\t\t\tdo_smart_update(sma, sops, nsops, 1, &tasks);\n\n\t\tgoto out_unlock_free;\n\t}\n\n\t/* We need to sleep on this operation, so we put the current\n\t * task into the pending queue and go to sleep.\n\t */\n\t\t\n\tqueue.sops = sops;\n\tqueue.nsops = nsops;\n\tqueue.undo = un;\n\tqueue.pid = task_tgid_vnr(current);\n\tqueue.alter = alter;\n\n\tif (nsops == 1) {\n\t\tstruct sem *curr;\n\t\tcurr = &sma->sem_base[sops->sem_num];\n\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &curr->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &curr->sem_pending);\n\t} else {\n\t\tif (alter)\n\t\t\tlist_add_tail(&queue.list, &sma->sem_pending);\n\t\telse\n\t\t\tlist_add(&queue.list, &sma->sem_pending);\n\t\tsma->complex_count++;\n\t}\n\n\tqueue.status = -EINTR;\n\tqueue.sleeper = current;\n\nsleep_again:\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tsem_unlock(sma, locknum);\n\n\tif (timeout)\n\t\tjiffies_left = schedule_timeout(jiffies_left);\n\telse\n\t\tschedule();\n\n\terror = get_queue_result(&queue);\n\n\tif (error != -EINTR) {\n\t\t/* fast path: update_queue already obtained all requested\n\t\t * resources.\n\t\t * Perform a smp_mb(): User space could assume that semop()\n\t\t * is a memory barrier: Without the mb(), the cpu could\n\t\t * speculatively read in user space stale data that was\n\t\t * overwritten by the previous owner of the semaphore.\n\t\t */\n\t\tsmp_mb();\n\n\t\tgoto out_free;\n\t}\n\n\tsma = sem_obtain_lock(ns, semid, sops, nsops, &locknum);\n\n\t/*\n\t * Wait until it's guaranteed that no wakeup_sem_queue_do() is ongoing.\n\t */\n\terror = get_queue_result(&queue);\n\n\t/*\n\t * Array removed? If yes, leave without sem_unlock().\n\t */\n\tif (IS_ERR(sma)) {\n\t\tgoto out_free;\n\t}\n\n\n\t/*\n\t * If queue.status != -EINTR we are woken up by another process.\n\t * Leave without unlink_queue(), but with sem_unlock().\n\t */\n\n\tif (error != -EINTR) {\n\t\tgoto out_unlock_free;\n\t}\n\n\t/*\n\t * If an interrupt occurred we have to clean up the queue\n\t */\n\tif (timeout && jiffies_left == 0)\n\t\terror = -EAGAIN;\n\n\t/*\n\t * If the wakeup was spurious, just retry\n\t */\n\tif (error == -EINTR && !signal_pending(current))\n\t\tgoto sleep_again;\n\n\tunlink_queue(sma, &queue);\n\nout_unlock_free:\n\tsem_unlock(sma, locknum);\nout_wakeup:\n\twake_up_sem_queue_do(&tasks);\nout_free:\n\tif(sops != fast_sops)\n\t\tkfree(sops);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops,\n\t\tunsigned, nsops)\n{\n\treturn sys_semtimedop(semid, tsops, nsops, NULL);\n}\n\n/* If CLONE_SYSVSEM is set, establish sharing of SEM_UNDO state between\n * parent and child tasks.\n */\n\nint copy_semundo(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sem_undo_list *undo_list;\n\tint error;\n\n\tif (clone_flags & CLONE_SYSVSEM) {\n\t\terror = get_undo_list(&undo_list);\n\t\tif (error)\n\t\t\treturn error;\n\t\tatomic_inc(&undo_list->refcnt);\n\t\ttsk->sysvsem.undo_list = undo_list;\n\t} else \n\t\ttsk->sysvsem.undo_list = NULL;\n\n\treturn 0;\n}\n\n/*\n * add semadj values to semaphores, free undo structures.\n * undo structures are not freed when semaphore arrays are destroyed\n * so some of them may be out of date.\n * IMPLEMENTATION NOTE: There is some confusion over whether the\n * set of adjustments that needs to be done should be done in an atomic\n * manner or not. That is, if we are attempting to decrement the semval\n * should we queue up and wait until we can do so legally?\n * The original implementation attempted to do this (queue and wait).\n * The current implementation does not do so. The POSIX standard\n * and SVID should be consulted to determine what behavior is mandated.\n */\nvoid exit_sem(struct task_struct *tsk)\n{\n\tstruct sem_undo_list *ulp;\n\n\tulp = tsk->sysvsem.undo_list;\n\tif (!ulp)\n\t\treturn;\n\ttsk->sysvsem.undo_list = NULL;\n\n\tif (!atomic_dec_and_test(&ulp->refcnt))\n\t\treturn;\n\n\tfor (;;) {\n\t\tstruct sem_array *sma;\n\t\tstruct sem_undo *un;\n\t\tstruct list_head tasks;\n\t\tint semid, i;\n\n\t\trcu_read_lock();\n\t\tun = list_entry_rcu(ulp->list_proc.next,\n\t\t\t\t    struct sem_undo, list_proc);\n\t\tif (&un->list_proc == &ulp->list_proc)\n\t\t\tsemid = -1;\n\t\t else\n\t\t\tsemid = un->semid;\n\n\t\tif (semid == -1) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\tsma = sem_obtain_object_check(tsk->nsproxy->ipc_ns, un->semid);\n\t\t/* exit_sem raced with IPC_RMID, nothing to do */\n\t\tif (IS_ERR(sma)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tsem_lock(sma, NULL, -1);\n\t\tun = __lookup_undo(ulp, semid);\n\t\tif (un == NULL) {\n\t\t\t/* exit_sem raced with IPC_RMID+semget() that created\n\t\t\t * exactly the same semid. Nothing to do.\n\t\t\t */\n\t\t\tsem_unlock(sma, -1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* remove un from the linked lists */\n\t\tassert_spin_locked(&sma->sem_perm.lock);\n\t\tlist_del(&un->list_id);\n\n\t\tspin_lock(&ulp->lock);\n\t\tlist_del_rcu(&un->list_proc);\n\t\tspin_unlock(&ulp->lock);\n\n\t\t/* perform adjustments registered in un */\n\t\tfor (i = 0; i < sma->sem_nsems; i++) {\n\t\t\tstruct sem * semaphore = &sma->sem_base[i];\n\t\t\tif (un->semadj[i]) {\n\t\t\t\tsemaphore->semval += un->semadj[i];\n\t\t\t\t/*\n\t\t\t\t * Range checks of the new semaphore value,\n\t\t\t\t * not defined by sus:\n\t\t\t\t * - Some unices ignore the undo entirely\n\t\t\t\t *   (e.g. HP UX 11i 11.22, Tru64 V5.1)\n\t\t\t\t * - some cap the value (e.g. FreeBSD caps\n\t\t\t\t *   at 0, but doesn't enforce SEMVMX)\n\t\t\t\t *\n\t\t\t\t * Linux caps the semaphore value, both at 0\n\t\t\t\t * and at SEMVMX.\n\t\t\t\t *\n\t\t\t\t * \tManfred <manfred@colorfullife.com>\n\t\t\t\t */\n\t\t\t\tif (semaphore->semval < 0)\n\t\t\t\t\tsemaphore->semval = 0;\n\t\t\t\tif (semaphore->semval > SEMVMX)\n\t\t\t\t\tsemaphore->semval = SEMVMX;\n\t\t\t\tsemaphore->sempid = task_tgid_vnr(current);\n\t\t\t}\n\t\t}\n\t\t/* maybe some queued-up processes were waiting for this */\n\t\tINIT_LIST_HEAD(&tasks);\n\t\tdo_smart_update(sma, NULL, 0, 1, &tasks);\n\t\tsem_unlock(sma, -1);\n\t\twake_up_sem_queue_do(&tasks);\n\n\t\tkfree_rcu(un, rcu);\n\t}\n\tkfree(ulp);\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int sysvipc_sem_proc_show(struct seq_file *s, void *it)\n{\n\tstruct user_namespace *user_ns = seq_user_ns(s);\n\tstruct sem_array *sma = it;\n\n\treturn seq_printf(s,\n\t\t\t  \"%10d %10d  %4o %10u %5u %5u %5u %5u %10lu %10lu\\n\",\n\t\t\t  sma->sem_perm.key,\n\t\t\t  sma->sem_perm.id,\n\t\t\t  sma->sem_perm.mode,\n\t\t\t  sma->sem_nsems,\n\t\t\t  from_kuid_munged(user_ns, sma->sem_perm.uid),\n\t\t\t  from_kgid_munged(user_ns, sma->sem_perm.gid),\n\t\t\t  from_kuid_munged(user_ns, sma->sem_perm.cuid),\n\t\t\t  from_kgid_munged(user_ns, sma->sem_perm.cgid),\n\t\t\t  sma->sem_otime,\n\t\t\t  sma->sem_ctime);\n}\n#endif\n", "/*\n * linux/ipc/util.c\n * Copyright (C) 1992 Krishna Balasubramanian\n *\n * Sep 1997 - Call suser() last after \"normal\" permission checks so we\n *            get BSD style process accounting right.\n *            Occurs in several places in the IPC code.\n *            Chris Evans, <chris@ferret.lmh.ox.ac.uk>\n * Nov 1999 - ipc helper functions, unified SMP locking\n *\t      Manfred Spraul <manfred@colorfullife.com>\n * Oct 2002 - One lock per IPC id. RCU ipc_free for lock-free grow_ary().\n *            Mingming Cao <cmm@us.ibm.com>\n * Mar 2006 - support for audit of ipc object properties\n *            Dustin Kirkland <dustin.kirkland@us.ibm.com>\n * Jun 2006 - namespaces ssupport\n *            OpenVZ, SWsoft Inc.\n *            Pavel Emelianov <xemul@openvz.org>\n */\n\n#include <linux/mm.h>\n#include <linux/shm.h>\n#include <linux/init.h>\n#include <linux/msg.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/notifier.h>\n#include <linux/capability.h>\n#include <linux/highuid.h>\n#include <linux/security.h>\n#include <linux/rcupdate.h>\n#include <linux/workqueue.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/audit.h>\n#include <linux/nsproxy.h>\n#include <linux/rwsem.h>\n#include <linux/memory.h>\n#include <linux/ipc_namespace.h>\n\n#include <asm/unistd.h>\n\n#include \"util.h\"\n\nstruct ipc_proc_iface {\n\tconst char *path;\n\tconst char *header;\n\tint ids;\n\tint (*show)(struct seq_file *, void *);\n};\n\nstatic void ipc_memory_notifier(struct work_struct *work)\n{\n\tipcns_notify(IPCNS_MEMCHANGED);\n}\n\nstatic int ipc_memory_callback(struct notifier_block *self,\n\t\t\t\tunsigned long action, void *arg)\n{\n\tstatic DECLARE_WORK(ipc_memory_wq, ipc_memory_notifier);\n\n\tswitch (action) {\n\tcase MEM_ONLINE:    /* memory successfully brought online */\n\tcase MEM_OFFLINE:   /* or offline: it's time to recompute msgmni */\n\t\t/*\n\t\t * This is done by invoking the ipcns notifier chain with the\n\t\t * IPC_MEMCHANGED event.\n\t\t * In order not to keep the lock on the hotplug memory chain\n\t\t * for too long, queue a work item that will, when waken up,\n\t\t * activate the ipcns notification chain.\n\t\t * No need to keep several ipc work items on the queue.\n\t\t */\n\t\tif (!work_pending(&ipc_memory_wq))\n\t\t\tschedule_work(&ipc_memory_wq);\n\t\tbreak;\n\tcase MEM_GOING_ONLINE:\n\tcase MEM_GOING_OFFLINE:\n\tcase MEM_CANCEL_ONLINE:\n\tcase MEM_CANCEL_OFFLINE:\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block ipc_memory_nb = {\n\t.notifier_call = ipc_memory_callback,\n\t.priority = IPC_CALLBACK_PRI,\n};\n\n/**\n *\tipc_init\t-\tinitialise IPC subsystem\n *\n *\tThe various system5 IPC resources (semaphores, messages and shared\n *\tmemory) are initialised\n *\tA callback routine is registered into the memory hotplug notifier\n *\tchain: since msgmni scales to lowmem this callback routine will be\n *\tcalled upon successful memory add / remove to recompute msmgni.\n */\n \nstatic int __init ipc_init(void)\n{\n\tsem_init();\n\tmsg_init();\n\tshm_init();\n\tregister_hotmemory_notifier(&ipc_memory_nb);\n\tregister_ipcns_notifier(&init_ipc_ns);\n\treturn 0;\n}\n__initcall(ipc_init);\n\n/**\n *\tipc_init_ids\t\t-\tinitialise IPC identifiers\n *\t@ids: Identifier set\n *\n *\tSet up the sequence range to use for the ipc identifier range (limited\n *\tbelow IPCMNI) then initialise the ids idr.\n */\n \nvoid ipc_init_ids(struct ipc_ids *ids)\n{\n\tinit_rwsem(&ids->rw_mutex);\n\n\tids->in_use = 0;\n\tids->seq = 0;\n\tids->next_id = -1;\n\t{\n\t\tint seq_limit = INT_MAX/SEQ_MULTIPLIER;\n\t\tif (seq_limit > USHRT_MAX)\n\t\t\tids->seq_max = USHRT_MAX;\n\t\t else\n\t\t \tids->seq_max = seq_limit;\n\t}\n\n\tidr_init(&ids->ipcs_idr);\n}\n\n#ifdef CONFIG_PROC_FS\nstatic const struct file_operations sysvipc_proc_fops;\n/**\n *\tipc_init_proc_interface\t-  Create a proc interface for sysipc types using a seq_file interface.\n *\t@path: Path in procfs\n *\t@header: Banner to be printed at the beginning of the file.\n *\t@ids: ipc id table to iterate.\n *\t@show: show routine.\n */\nvoid __init ipc_init_proc_interface(const char *path, const char *header,\n\t\tint ids, int (*show)(struct seq_file *, void *))\n{\n\tstruct proc_dir_entry *pde;\n\tstruct ipc_proc_iface *iface;\n\n\tiface = kmalloc(sizeof(*iface), GFP_KERNEL);\n\tif (!iface)\n\t\treturn;\n\tiface->path\t= path;\n\tiface->header\t= header;\n\tiface->ids\t= ids;\n\tiface->show\t= show;\n\n\tpde = proc_create_data(path,\n\t\t\t       S_IRUGO,        /* world readable */\n\t\t\t       NULL,           /* parent dir */\n\t\t\t       &sysvipc_proc_fops,\n\t\t\t       iface);\n\tif (!pde) {\n\t\tkfree(iface);\n\t}\n}\n#endif\n\n/**\n *\tipc_findkey\t-\tfind a key in an ipc identifier set\t\n *\t@ids: Identifier set\n *\t@key: The key to find\n *\t\n *\tRequires ipc_ids.rw_mutex locked.\n *\tReturns the LOCKED pointer to the ipc structure if found or NULL\n *\tif not.\n *\tIf key is found ipc points to the owning ipc structure\n */\n \nstatic struct kern_ipc_perm *ipc_findkey(struct ipc_ids *ids, key_t key)\n{\n\tstruct kern_ipc_perm *ipc;\n\tint next_id;\n\tint total;\n\n\tfor (total = 0, next_id = 0; total < ids->in_use; next_id++) {\n\t\tipc = idr_find(&ids->ipcs_idr, next_id);\n\n\t\tif (ipc == NULL)\n\t\t\tcontinue;\n\n\t\tif (ipc->key != key) {\n\t\t\ttotal++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tipc_lock_by_ptr(ipc);\n\t\treturn ipc;\n\t}\n\n\treturn NULL;\n}\n\n/**\n *\tipc_get_maxid \t-\tget the last assigned id\n *\t@ids: IPC identifier set\n *\n *\tCalled with ipc_ids.rw_mutex held.\n */\n\nint ipc_get_maxid(struct ipc_ids *ids)\n{\n\tstruct kern_ipc_perm *ipc;\n\tint max_id = -1;\n\tint total, id;\n\n\tif (ids->in_use == 0)\n\t\treturn -1;\n\n\tif (ids->in_use == IPCMNI)\n\t\treturn IPCMNI - 1;\n\n\t/* Look for the last assigned id */\n\ttotal = 0;\n\tfor (id = 0; id < IPCMNI && total < ids->in_use; id++) {\n\t\tipc = idr_find(&ids->ipcs_idr, id);\n\t\tif (ipc != NULL) {\n\t\t\tmax_id = id;\n\t\t\ttotal++;\n\t\t}\n\t}\n\treturn max_id;\n}\n\n/**\n *\tipc_addid \t-\tadd an IPC identifier\n *\t@ids: IPC identifier set\n *\t@new: new IPC permission set\n *\t@size: limit for the number of used ids\n *\n *\tAdd an entry 'new' to the IPC ids idr. The permissions object is\n *\tinitialised and the first free entry is set up and the id assigned\n *\tis returned. The 'new' entry is returned in a locked state on success.\n *\tOn failure the entry is not locked and a negative err-code is returned.\n *\n *\tCalled with ipc_ids.rw_mutex held as a writer.\n */\n \nint ipc_addid(struct ipc_ids* ids, struct kern_ipc_perm* new, int size)\n{\n\tkuid_t euid;\n\tkgid_t egid;\n\tint id;\n\tint next_id = ids->next_id;\n\n\tif (size > IPCMNI)\n\t\tsize = IPCMNI;\n\n\tif (ids->in_use >= size)\n\t\treturn -ENOSPC;\n\n\tidr_preload(GFP_KERNEL);\n\n\tspin_lock_init(&new->lock);\n\tnew->deleted = 0;\n\trcu_read_lock();\n\tspin_lock(&new->lock);\n\n\tid = idr_alloc(&ids->ipcs_idr, new,\n\t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n\t\t       GFP_NOWAIT);\n\tidr_preload_end();\n\tif (id < 0) {\n\t\tspin_unlock(&new->lock);\n\t\trcu_read_unlock();\n\t\treturn id;\n\t}\n\n\tids->in_use++;\n\n\tcurrent_euid_egid(&euid, &egid);\n\tnew->cuid = new->uid = euid;\n\tnew->gid = new->cgid = egid;\n\n\tif (next_id < 0) {\n\t\tnew->seq = ids->seq++;\n\t\tif (ids->seq > ids->seq_max)\n\t\t\tids->seq = 0;\n\t} else {\n\t\tnew->seq = ipcid_to_seqx(next_id);\n\t\tids->next_id = -1;\n\t}\n\n\tnew->id = ipc_buildid(id, new->seq);\n\treturn id;\n}\n\n/**\n *\tipcget_new\t-\tcreate a new ipc object\n *\t@ns: namespace\n *\t@ids: IPC identifer set\n *\t@ops: the actual creation routine to call\n *\t@params: its parameters\n *\n *\tThis routine is called by sys_msgget, sys_semget() and sys_shmget()\n *\twhen the key is IPC_PRIVATE.\n */\nstatic int ipcget_new(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\tstruct ipc_ops *ops, struct ipc_params *params)\n{\n\tint err;\n\n\tdown_write(&ids->rw_mutex);\n\terr = ops->getnew(ns, params);\n\tup_write(&ids->rw_mutex);\n\treturn err;\n}\n\n/**\n *\tipc_check_perms\t-\tcheck security and permissions for an IPC\n *\t@ns: IPC namespace\n *\t@ipcp: ipc permission set\n *\t@ops: the actual security routine to call\n *\t@params: its parameters\n *\n *\tThis routine is called by sys_msgget(), sys_semget() and sys_shmget()\n *      when the key is not IPC_PRIVATE and that key already exists in the\n *      ids IDR.\n *\n *\tOn success, the IPC id is returned.\n *\n *\tIt is called with ipc_ids.rw_mutex and ipcp->lock held.\n */\nstatic int ipc_check_perms(struct ipc_namespace *ns,\n\t\t\t   struct kern_ipc_perm *ipcp,\n\t\t\t   struct ipc_ops *ops,\n\t\t\t   struct ipc_params *params)\n{\n\tint err;\n\n\tif (ipcperms(ns, ipcp, params->flg))\n\t\terr = -EACCES;\n\telse {\n\t\terr = ops->associate(ipcp, params->flg);\n\t\tif (!err)\n\t\t\terr = ipcp->id;\n\t}\n\n\treturn err;\n}\n\n/**\n *\tipcget_public\t-\tget an ipc object or create a new one\n *\t@ns: namespace\n *\t@ids: IPC identifer set\n *\t@ops: the actual creation routine to call\n *\t@params: its parameters\n *\n *\tThis routine is called by sys_msgget, sys_semget() and sys_shmget()\n *\twhen the key is not IPC_PRIVATE.\n *\tIt adds a new entry if the key is not found and does some permission\n *      / security checkings if the key is found.\n *\n *\tOn success, the ipc id is returned.\n */\nstatic int ipcget_public(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\tstruct ipc_ops *ops, struct ipc_params *params)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tint flg = params->flg;\n\tint err;\n\n\t/*\n\t * Take the lock as a writer since we are potentially going to add\n\t * a new entry + read locks are not \"upgradable\"\n\t */\n\tdown_write(&ids->rw_mutex);\n\tipcp = ipc_findkey(ids, params->key);\n\tif (ipcp == NULL) {\n\t\t/* key not used */\n\t\tif (!(flg & IPC_CREAT))\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\terr = ops->getnew(ns, params);\n\t} else {\n\t\t/* ipc object has been locked by ipc_findkey() */\n\n\t\tif (flg & IPC_CREAT && flg & IPC_EXCL)\n\t\t\terr = -EEXIST;\n\t\telse {\n\t\t\terr = 0;\n\t\t\tif (ops->more_checks)\n\t\t\t\terr = ops->more_checks(ipcp, params);\n\t\t\tif (!err)\n\t\t\t\t/*\n\t\t\t\t * ipc_check_perms returns the IPC id on\n\t\t\t\t * success\n\t\t\t\t */\n\t\t\t\terr = ipc_check_perms(ns, ipcp, ops, params);\n\t\t}\n\t\tipc_unlock(ipcp);\n\t}\n\tup_write(&ids->rw_mutex);\n\n\treturn err;\n}\n\n\n/**\n *\tipc_rmid\t-\tremove an IPC identifier\n *\t@ids: IPC identifier set\n *\t@ipcp: ipc perm structure containing the identifier to remove\n *\n *\tipc_ids.rw_mutex (as a writer) and the spinlock for this ID are held\n *\tbefore this function is called, and remain locked on the exit.\n */\n \nvoid ipc_rmid(struct ipc_ids *ids, struct kern_ipc_perm *ipcp)\n{\n\tint lid = ipcid_to_idx(ipcp->id);\n\n\tidr_remove(&ids->ipcs_idr, lid);\n\n\tids->in_use--;\n\n\tipcp->deleted = 1;\n\n\treturn;\n}\n\n/**\n *\tipc_alloc\t-\tallocate ipc space\n *\t@size: size desired\n *\n *\tAllocate memory from the appropriate pools and return a pointer to it.\n *\tNULL is returned if the allocation fails\n */\n \nvoid *ipc_alloc(int size)\n{\n\tvoid *out;\n\tif(size > PAGE_SIZE)\n\t\tout = vmalloc(size);\n\telse\n\t\tout = kmalloc(size, GFP_KERNEL);\n\treturn out;\n}\n\n/**\n *\tipc_free        -       free ipc space\n *\t@ptr: pointer returned by ipc_alloc\n *\t@size: size of block\n *\n *\tFree a block created with ipc_alloc(). The caller must know the size\n *\tused in the allocation call.\n */\n\nvoid ipc_free(void* ptr, int size)\n{\n\tif(size > PAGE_SIZE)\n\t\tvfree(ptr);\n\telse\n\t\tkfree(ptr);\n}\n\n/*\n * rcu allocations:\n * There are three headers that are prepended to the actual allocation:\n * - during use: ipc_rcu_hdr.\n * - during the rcu grace period: ipc_rcu_grace.\n * - [only if vmalloc]: ipc_rcu_sched.\n * Their lifetime doesn't overlap, thus the headers share the same memory.\n * Unlike a normal union, they are right-aligned, thus some container_of\n * forward/backward casting is necessary:\n */\nstruct ipc_rcu_hdr\n{\n\tatomic_t refcount;\n\tint is_vmalloc;\n\tvoid *data[0];\n};\n\n\nstruct ipc_rcu_grace\n{\n\tstruct rcu_head rcu;\n\t/* \"void *\" makes sure alignment of following data is sane. */\n\tvoid *data[0];\n};\n\nstruct ipc_rcu_sched\n{\n\tstruct work_struct work;\n\t/* \"void *\" makes sure alignment of following data is sane. */\n\tvoid *data[0];\n};\n\n#define HDRLEN_KMALLOC\t\t(sizeof(struct ipc_rcu_grace) > sizeof(struct ipc_rcu_hdr) ? \\\n\t\t\t\t\tsizeof(struct ipc_rcu_grace) : sizeof(struct ipc_rcu_hdr))\n#define HDRLEN_VMALLOC\t\t(sizeof(struct ipc_rcu_sched) > HDRLEN_KMALLOC ? \\\n\t\t\t\t\tsizeof(struct ipc_rcu_sched) : HDRLEN_KMALLOC)\n\nstatic inline int rcu_use_vmalloc(int size)\n{\n\t/* Too big for a single page? */\n\tif (HDRLEN_KMALLOC + size > PAGE_SIZE)\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n *\tipc_rcu_alloc\t-\tallocate ipc and rcu space \n *\t@size: size desired\n *\n *\tAllocate memory for the rcu header structure +  the object.\n *\tReturns the pointer to the object or NULL upon failure.\n */\nvoid *ipc_rcu_alloc(int size)\n{\n\tvoid *out;\n\n\t/*\n\t * We prepend the allocation with the rcu struct, and\n\t * workqueue if necessary (for vmalloc).\n\t */\n\tif (rcu_use_vmalloc(size)) {\n\t\tout = vmalloc(HDRLEN_VMALLOC + size);\n\t\tif (!out)\n\t\t\tgoto done;\n\n\t\tout += HDRLEN_VMALLOC;\n\t\tcontainer_of(out, struct ipc_rcu_hdr, data)->is_vmalloc = 1;\n\t} else {\n\t\tout = kmalloc(HDRLEN_KMALLOC + size, GFP_KERNEL);\n\t\tif (!out)\n\t\t\tgoto done;\n\n\t\tout += HDRLEN_KMALLOC;\n\t\tcontainer_of(out, struct ipc_rcu_hdr, data)->is_vmalloc = 0;\n\t}\n\n\t/* set reference counter no matter what kind of allocation was done */\n\tatomic_set(&container_of(out, struct ipc_rcu_hdr, data)->refcount, 1);\ndone:\n\treturn out;\n}\n\nint ipc_rcu_getref(void *ptr)\n{\n\treturn atomic_inc_not_zero(&container_of(ptr, struct ipc_rcu_hdr, data)->refcount);\n}\n\nstatic void ipc_do_vfree(struct work_struct *work)\n{\n\tvfree(container_of(work, struct ipc_rcu_sched, work));\n}\n\n/**\n * ipc_schedule_free - free ipc + rcu space\n * @head: RCU callback structure for queued work\n * \n * Since RCU callback function is called in bh,\n * we need to defer the vfree to schedule_work().\n */\nstatic void ipc_schedule_free(struct rcu_head *head)\n{\n\tstruct ipc_rcu_grace *grace;\n\tstruct ipc_rcu_sched *sched;\n\n\tgrace = container_of(head, struct ipc_rcu_grace, rcu);\n\tsched = container_of(&(grace->data[0]), struct ipc_rcu_sched,\n\t\t\t\tdata[0]);\n\n\tINIT_WORK(&sched->work, ipc_do_vfree);\n\tschedule_work(&sched->work);\n}\n\nvoid ipc_rcu_putref(void *ptr)\n{\n\tif (!atomic_dec_and_test(&container_of(ptr, struct ipc_rcu_hdr, data)->refcount))\n\t\treturn;\n\n\tif (container_of(ptr, struct ipc_rcu_hdr, data)->is_vmalloc) {\n\t\tcall_rcu(&container_of(ptr, struct ipc_rcu_grace, data)->rcu,\n\t\t\t\tipc_schedule_free);\n\t} else {\n\t\tkfree_rcu(container_of(ptr, struct ipc_rcu_grace, data), rcu);\n\t}\n}\n\n/**\n *\tipcperms\t-\tcheck IPC permissions\n *\t@ns: IPC namespace\n *\t@ipcp: IPC permission set\n *\t@flag: desired permission set.\n *\n *\tCheck user, group, other permissions for access\n *\tto ipc resources. return 0 if allowed\n *\n * \t@flag will most probably be 0 or S_...UGO from <linux/stat.h>\n */\n \nint ipcperms(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp, short flag)\n{\n\tkuid_t euid = current_euid();\n\tint requested_mode, granted_mode;\n\n\taudit_ipc_obj(ipcp);\n\trequested_mode = (flag >> 6) | (flag >> 3) | flag;\n\tgranted_mode = ipcp->mode;\n\tif (uid_eq(euid, ipcp->cuid) ||\n\t    uid_eq(euid, ipcp->uid))\n\t\tgranted_mode >>= 6;\n\telse if (in_group_p(ipcp->cgid) || in_group_p(ipcp->gid))\n\t\tgranted_mode >>= 3;\n\t/* is there some bit set in requested_mode but not in granted_mode? */\n\tif ((requested_mode & ~granted_mode & 0007) && \n\t    !ns_capable(ns->user_ns, CAP_IPC_OWNER))\n\t\treturn -1;\n\n\treturn security_ipc_permission(ipcp, flag);\n}\n\n/*\n * Functions to convert between the kern_ipc_perm structure and the\n * old/new ipc_perm structures\n */\n\n/**\n *\tkernel_to_ipc64_perm\t-\tconvert kernel ipc permissions to user\n *\t@in: kernel permissions\n *\t@out: new style IPC permissions\n *\n *\tTurn the kernel object @in into a set of permissions descriptions\n *\tfor returning to userspace (@out).\n */\n \n\nvoid kernel_to_ipc64_perm (struct kern_ipc_perm *in, struct ipc64_perm *out)\n{\n\tout->key\t= in->key;\n\tout->uid\t= from_kuid_munged(current_user_ns(), in->uid);\n\tout->gid\t= from_kgid_munged(current_user_ns(), in->gid);\n\tout->cuid\t= from_kuid_munged(current_user_ns(), in->cuid);\n\tout->cgid\t= from_kgid_munged(current_user_ns(), in->cgid);\n\tout->mode\t= in->mode;\n\tout->seq\t= in->seq;\n}\n\n/**\n *\tipc64_perm_to_ipc_perm\t-\tconvert new ipc permissions to old\n *\t@in: new style IPC permissions\n *\t@out: old style IPC permissions\n *\n *\tTurn the new style permissions object @in into a compatibility\n *\tobject and store it into the @out pointer.\n */\n \nvoid ipc64_perm_to_ipc_perm (struct ipc64_perm *in, struct ipc_perm *out)\n{\n\tout->key\t= in->key;\n\tSET_UID(out->uid, in->uid);\n\tSET_GID(out->gid, in->gid);\n\tSET_UID(out->cuid, in->cuid);\n\tSET_GID(out->cgid, in->cgid);\n\tout->mode\t= in->mode;\n\tout->seq\t= in->seq;\n}\n\n/**\n * ipc_obtain_object\n * @ids: ipc identifier set\n * @id: ipc id to look for\n *\n * Look for an id in the ipc ids idr and return associated ipc object.\n *\n * Call inside the RCU critical section.\n * The ipc object is *not* locked on exit.\n */\nstruct kern_ipc_perm *ipc_obtain_object(struct ipc_ids *ids, int id)\n{\n\tstruct kern_ipc_perm *out;\n\tint lid = ipcid_to_idx(id);\n\n\tout = idr_find(&ids->ipcs_idr, lid);\n\tif (!out)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn out;\n}\n\n/**\n * ipc_lock - Lock an ipc structure without rw_mutex held\n * @ids: IPC identifier set\n * @id: ipc id to look for\n *\n * Look for an id in the ipc ids idr and lock the associated ipc object.\n *\n * The ipc object is locked on successful exit.\n */\nstruct kern_ipc_perm *ipc_lock(struct ipc_ids *ids, int id)\n{\n\tstruct kern_ipc_perm *out;\n\n\trcu_read_lock();\n\tout = ipc_obtain_object(ids, id);\n\tif (IS_ERR(out))\n\t\tgoto err1;\n\n\tspin_lock(&out->lock);\n\n\t/* ipc_rmid() may have already freed the ID while ipc_lock\n\t * was spinning: here verify that the structure is still valid\n\t */\n\tif (!out->deleted)\n\t\treturn out;\n\n\tspin_unlock(&out->lock);\n\tout = ERR_PTR(-EINVAL);\nerr1:\n\trcu_read_unlock();\n\treturn out;\n}\n\n/**\n * ipc_obtain_object_check\n * @ids: ipc identifier set\n * @id: ipc id to look for\n *\n * Similar to ipc_obtain_object() but also checks\n * the ipc object reference counter.\n *\n * Call inside the RCU critical section.\n * The ipc object is *not* locked on exit.\n */\nstruct kern_ipc_perm *ipc_obtain_object_check(struct ipc_ids *ids, int id)\n{\n\tstruct kern_ipc_perm *out = ipc_obtain_object(ids, id);\n\n\tif (IS_ERR(out))\n\t\tgoto out;\n\n\tif (ipc_checkid(out, id))\n\t\treturn ERR_PTR(-EIDRM);\nout:\n\treturn out;\n}\n\nstruct kern_ipc_perm *ipc_lock_check(struct ipc_ids *ids, int id)\n{\n\tstruct kern_ipc_perm *out;\n\n\tout = ipc_lock(ids, id);\n\tif (IS_ERR(out))\n\t\treturn out;\n\n\tif (ipc_checkid(out, id)) {\n\t\tipc_unlock(out);\n\t\treturn ERR_PTR(-EIDRM);\n\t}\n\n\treturn out;\n}\n\n/**\n * ipcget - Common sys_*get() code\n * @ns : namsepace\n * @ids : IPC identifier set\n * @ops : operations to be called on ipc object creation, permission checks\n *        and further checks\n * @params : the parameters needed by the previous operations.\n *\n * Common routine called by sys_msgget(), sys_semget() and sys_shmget().\n */\nint ipcget(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\t\tstruct ipc_ops *ops, struct ipc_params *params)\n{\n\tif (params->key == IPC_PRIVATE)\n\t\treturn ipcget_new(ns, ids, ops, params);\n\telse\n\t\treturn ipcget_public(ns, ids, ops, params);\n}\n\n/**\n * ipc_update_perm - update the permissions of an IPC.\n * @in:  the permission given as input.\n * @out: the permission of the ipc to set.\n */\nint ipc_update_perm(struct ipc64_perm *in, struct kern_ipc_perm *out)\n{\n\tkuid_t uid = make_kuid(current_user_ns(), in->uid);\n\tkgid_t gid = make_kgid(current_user_ns(), in->gid);\n\tif (!uid_valid(uid) || !gid_valid(gid))\n\t\treturn -EINVAL;\n\n\tout->uid = uid;\n\tout->gid = gid;\n\tout->mode = (out->mode & ~S_IRWXUGO)\n\t\t| (in->mode & S_IRWXUGO);\n\n\treturn 0;\n}\n\n/**\n * ipcctl_pre_down - retrieve an ipc and check permissions for some IPC_XXX cmd\n * @ns:  the ipc namespace\n * @ids:  the table of ids where to look for the ipc\n * @id:   the id of the ipc to retrieve\n * @cmd:  the cmd to check\n * @perm: the permission to set\n * @extra_perm: one extra permission parameter used by msq\n *\n * This function does some common audit and permissions check for some IPC_XXX\n * cmd and is called from semctl_down, shmctl_down and msgctl_down.\n * It must be called without any lock held and\n *  - retrieves the ipc with the given id in the given table.\n *  - performs some audit and permission check, depending on the given cmd\n *  - returns the ipc with both ipc and rw_mutex locks held in case of success\n *    or an err-code without any lock held otherwise.\n */\nstruct kern_ipc_perm *ipcctl_pre_down(struct ipc_namespace *ns,\n\t\t\t\t      struct ipc_ids *ids, int id, int cmd,\n\t\t\t\t      struct ipc64_perm *perm, int extra_perm)\n{\n\tstruct kern_ipc_perm *ipcp;\n\n\tipcp = ipcctl_pre_down_nolock(ns, ids, id, cmd, perm, extra_perm);\n\tif (IS_ERR(ipcp))\n\t\tgoto out;\n\n\tspin_lock(&ipcp->lock);\nout:\n\treturn ipcp;\n}\n\nstruct kern_ipc_perm *ipcctl_pre_down_nolock(struct ipc_namespace *ns,\n\t\t\t\t\t     struct ipc_ids *ids, int id, int cmd,\n\t\t\t\t\t     struct ipc64_perm *perm, int extra_perm)\n{\n\tkuid_t euid;\n\tint err = -EPERM;\n\tstruct kern_ipc_perm *ipcp;\n\n\tdown_write(&ids->rw_mutex);\n\trcu_read_lock();\n\n\tipcp = ipc_obtain_object_check(ids, id);\n\tif (IS_ERR(ipcp)) {\n\t\terr = PTR_ERR(ipcp);\n\t\tgoto out_up;\n\t}\n\n\taudit_ipc_obj(ipcp);\n\tif (cmd == IPC_SET)\n\t\taudit_ipc_set_perm(extra_perm, perm->uid,\n\t\t\t\t   perm->gid, perm->mode);\n\n\teuid = current_euid();\n\tif (uid_eq(euid, ipcp->cuid) || uid_eq(euid, ipcp->uid)  ||\n\t    ns_capable(ns->user_ns, CAP_SYS_ADMIN))\n\t\treturn ipcp;\n\nout_up:\n\t/*\n\t * Unsuccessful lookup, unlock and return\n\t * the corresponding error.\n\t */\n\trcu_read_unlock();\n\tup_write(&ids->rw_mutex);\n\n\treturn ERR_PTR(err);\n}\n\n#ifdef CONFIG_ARCH_WANT_IPC_PARSE_VERSION\n\n\n/**\n *\tipc_parse_version\t-\tIPC call version\n *\t@cmd: pointer to command\n *\n *\tReturn IPC_64 for new style IPC and IPC_OLD for old style IPC. \n *\tThe @cmd value is turned from an encoding command and version into\n *\tjust the command code.\n */\n \nint ipc_parse_version (int *cmd)\n{\n\tif (*cmd & IPC_64) {\n\t\t*cmd ^= IPC_64;\n\t\treturn IPC_64;\n\t} else {\n\t\treturn IPC_OLD;\n\t}\n}\n\n#endif /* CONFIG_ARCH_WANT_IPC_PARSE_VERSION */\n\n#ifdef CONFIG_PROC_FS\nstruct ipc_proc_iter {\n\tstruct ipc_namespace *ns;\n\tstruct ipc_proc_iface *iface;\n};\n\n/*\n * This routine locks the ipc structure found at least at position pos.\n */\nstatic struct kern_ipc_perm *sysvipc_find_ipc(struct ipc_ids *ids, loff_t pos,\n\t\t\t\t\t      loff_t *new_pos)\n{\n\tstruct kern_ipc_perm *ipc;\n\tint total, id;\n\n\ttotal = 0;\n\tfor (id = 0; id < pos && total < ids->in_use; id++) {\n\t\tipc = idr_find(&ids->ipcs_idr, id);\n\t\tif (ipc != NULL)\n\t\t\ttotal++;\n\t}\n\n\tif (total >= ids->in_use)\n\t\treturn NULL;\n\n\tfor ( ; pos < IPCMNI; pos++) {\n\t\tipc = idr_find(&ids->ipcs_idr, pos);\n\t\tif (ipc != NULL) {\n\t\t\t*new_pos = pos + 1;\n\t\t\tipc_lock_by_ptr(ipc);\n\t\t\treturn ipc;\n\t\t}\n\t}\n\n\t/* Out of range - return NULL to terminate iteration */\n\treturn NULL;\n}\n\nstatic void *sysvipc_proc_next(struct seq_file *s, void *it, loff_t *pos)\n{\n\tstruct ipc_proc_iter *iter = s->private;\n\tstruct ipc_proc_iface *iface = iter->iface;\n\tstruct kern_ipc_perm *ipc = it;\n\n\t/* If we had an ipc id locked before, unlock it */\n\tif (ipc && ipc != SEQ_START_TOKEN)\n\t\tipc_unlock(ipc);\n\n\treturn sysvipc_find_ipc(&iter->ns->ids[iface->ids], *pos, pos);\n}\n\n/*\n * File positions: pos 0 -> header, pos n -> ipc id = n - 1.\n * SeqFile iterator: iterator value locked ipc pointer or SEQ_TOKEN_START.\n */\nstatic void *sysvipc_proc_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct ipc_proc_iter *iter = s->private;\n\tstruct ipc_proc_iface *iface = iter->iface;\n\tstruct ipc_ids *ids;\n\n\tids = &iter->ns->ids[iface->ids];\n\n\t/*\n\t * Take the lock - this will be released by the corresponding\n\t * call to stop().\n\t */\n\tdown_read(&ids->rw_mutex);\n\n\t/* pos < 0 is invalid */\n\tif (*pos < 0)\n\t\treturn NULL;\n\n\t/* pos == 0 means header */\n\tif (*pos == 0)\n\t\treturn SEQ_START_TOKEN;\n\n\t/* Find the (pos-1)th ipc */\n\treturn sysvipc_find_ipc(ids, *pos - 1, pos);\n}\n\nstatic void sysvipc_proc_stop(struct seq_file *s, void *it)\n{\n\tstruct kern_ipc_perm *ipc = it;\n\tstruct ipc_proc_iter *iter = s->private;\n\tstruct ipc_proc_iface *iface = iter->iface;\n\tstruct ipc_ids *ids;\n\n\t/* If we had a locked structure, release it */\n\tif (ipc && ipc != SEQ_START_TOKEN)\n\t\tipc_unlock(ipc);\n\n\tids = &iter->ns->ids[iface->ids];\n\t/* Release the lock we took in start() */\n\tup_read(&ids->rw_mutex);\n}\n\nstatic int sysvipc_proc_show(struct seq_file *s, void *it)\n{\n\tstruct ipc_proc_iter *iter = s->private;\n\tstruct ipc_proc_iface *iface = iter->iface;\n\n\tif (it == SEQ_START_TOKEN)\n\t\treturn seq_puts(s, iface->header);\n\n\treturn iface->show(s, it);\n}\n\nstatic const struct seq_operations sysvipc_proc_seqops = {\n\t.start = sysvipc_proc_start,\n\t.stop  = sysvipc_proc_stop,\n\t.next  = sysvipc_proc_next,\n\t.show  = sysvipc_proc_show,\n};\n\nstatic int sysvipc_proc_open(struct inode *inode, struct file *file)\n{\n\tint ret;\n\tstruct seq_file *seq;\n\tstruct ipc_proc_iter *iter;\n\n\tret = -ENOMEM;\n\titer = kmalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter)\n\t\tgoto out;\n\n\tret = seq_open(file, &sysvipc_proc_seqops);\n\tif (ret)\n\t\tgoto out_kfree;\n\n\tseq = file->private_data;\n\tseq->private = iter;\n\n\titer->iface = PDE(inode)->data;\n\titer->ns    = get_ipc_ns(current->nsproxy->ipc_ns);\nout:\n\treturn ret;\nout_kfree:\n\tkfree(iter);\n\tgoto out;\n}\n\nstatic int sysvipc_proc_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct ipc_proc_iter *iter = seq->private;\n\tput_ipc_ns(iter->ns);\n\treturn seq_release_private(inode, file);\n}\n\nstatic const struct file_operations sysvipc_proc_fops = {\n\t.open    = sysvipc_proc_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = sysvipc_proc_release,\n};\n#endif /* CONFIG_PROC_FS */\n", "/*\n * linux/ipc/util.h\n * Copyright (C) 1999 Christoph Rohland\n *\n * ipc helper functions (c) 1999 Manfred Spraul <manfred@colorfullife.com>\n * namespaces support.      2006 OpenVZ, SWsoft Inc.\n *                               Pavel Emelianov <xemul@openvz.org>\n */\n\n#ifndef _IPC_UTIL_H\n#define _IPC_UTIL_H\n\n#include <linux/unistd.h>\n#include <linux/err.h>\n\n#define SEQ_MULTIPLIER\t(IPCMNI)\n\nvoid sem_init (void);\nvoid msg_init (void);\nvoid shm_init (void);\n\nstruct ipc_namespace;\n\n#ifdef CONFIG_POSIX_MQUEUE\nextern void mq_clear_sbinfo(struct ipc_namespace *ns);\nextern void mq_put_mnt(struct ipc_namespace *ns);\n#else\nstatic inline void mq_clear_sbinfo(struct ipc_namespace *ns) { }\nstatic inline void mq_put_mnt(struct ipc_namespace *ns) { }\n#endif\n\n#ifdef CONFIG_SYSVIPC\nvoid sem_init_ns(struct ipc_namespace *ns);\nvoid msg_init_ns(struct ipc_namespace *ns);\nvoid shm_init_ns(struct ipc_namespace *ns);\n\nvoid sem_exit_ns(struct ipc_namespace *ns);\nvoid msg_exit_ns(struct ipc_namespace *ns);\nvoid shm_exit_ns(struct ipc_namespace *ns);\n#else\nstatic inline void sem_init_ns(struct ipc_namespace *ns) { }\nstatic inline void msg_init_ns(struct ipc_namespace *ns) { }\nstatic inline void shm_init_ns(struct ipc_namespace *ns) { }\n\nstatic inline void sem_exit_ns(struct ipc_namespace *ns) { }\nstatic inline void msg_exit_ns(struct ipc_namespace *ns) { }\nstatic inline void shm_exit_ns(struct ipc_namespace *ns) { }\n#endif\n\n/*\n * Structure that holds the parameters needed by the ipc operations\n * (see after)\n */\nstruct ipc_params {\n\tkey_t key;\n\tint flg;\n\tunion {\n\t\tsize_t size;\t/* for shared memories */\n\t\tint nsems;\t/* for semaphores */\n\t} u;\t\t\t/* holds the getnew() specific param */\n};\n\n/*\n * Structure that holds some ipc operations. This structure is used to unify\n * the calls to sys_msgget(), sys_semget(), sys_shmget()\n *      . routine to call to create a new ipc object. Can be one of newque,\n *        newary, newseg\n *      . routine to call to check permissions for a new ipc object.\n *        Can be one of security_msg_associate, security_sem_associate,\n *        security_shm_associate\n *      . routine to call for an extra check if needed\n */\nstruct ipc_ops {\n\tint (*getnew) (struct ipc_namespace *, struct ipc_params *);\n\tint (*associate) (struct kern_ipc_perm *, int);\n\tint (*more_checks) (struct kern_ipc_perm *, struct ipc_params *);\n};\n\nstruct seq_file;\nstruct ipc_ids;\n\nvoid ipc_init_ids(struct ipc_ids *);\n#ifdef CONFIG_PROC_FS\nvoid __init ipc_init_proc_interface(const char *path, const char *header,\n\t\tint ids, int (*show)(struct seq_file *, void *));\n#else\n#define ipc_init_proc_interface(path, header, ids, show) do {} while (0)\n#endif\n\n#define IPC_SEM_IDS\t0\n#define IPC_MSG_IDS\t1\n#define IPC_SHM_IDS\t2\n\n#define ipcid_to_idx(id) ((id) % SEQ_MULTIPLIER)\n#define ipcid_to_seqx(id) ((id) / SEQ_MULTIPLIER)\n\n/* must be called with ids->rw_mutex acquired for writing */\nint ipc_addid(struct ipc_ids *, struct kern_ipc_perm *, int);\n\n/* must be called with ids->rw_mutex acquired for reading */\nint ipc_get_maxid(struct ipc_ids *);\n\n/* must be called with both locks acquired. */\nvoid ipc_rmid(struct ipc_ids *, struct kern_ipc_perm *);\n\n/* must be called with ipcp locked */\nint ipcperms(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp, short flg);\n\n/* for rare, potentially huge allocations.\n * both function can sleep\n */\nvoid* ipc_alloc(int size);\nvoid ipc_free(void* ptr, int size);\n\n/*\n * For allocation that need to be freed by RCU.\n * Objects are reference counted, they start with reference count 1.\n * getref increases the refcount, the putref call that reduces the recount\n * to 0 schedules the rcu destruction. Caller must guarantee locking.\n */\nvoid* ipc_rcu_alloc(int size);\nint ipc_rcu_getref(void *ptr);\nvoid ipc_rcu_putref(void *ptr);\n\nstruct kern_ipc_perm *ipc_lock(struct ipc_ids *, int);\nstruct kern_ipc_perm *ipc_obtain_object(struct ipc_ids *ids, int id);\n\nvoid kernel_to_ipc64_perm(struct kern_ipc_perm *in, struct ipc64_perm *out);\nvoid ipc64_perm_to_ipc_perm(struct ipc64_perm *in, struct ipc_perm *out);\nint ipc_update_perm(struct ipc64_perm *in, struct kern_ipc_perm *out);\nstruct kern_ipc_perm *ipcctl_pre_down_nolock(struct ipc_namespace *ns,\n\t\t\t\t\t     struct ipc_ids *ids, int id, int cmd,\n\t\t\t\t\t     struct ipc64_perm *perm, int extra_perm);\nstruct kern_ipc_perm *ipcctl_pre_down(struct ipc_namespace *ns,\n\t\t\t\t      struct ipc_ids *ids, int id, int cmd,\n\t\t\t\t      struct ipc64_perm *perm, int extra_perm);\n\n#ifndef CONFIG_ARCH_WANT_IPC_PARSE_VERSION\n  /* On IA-64, we always use the \"64-bit version\" of the IPC structures.  */ \n# define ipc_parse_version(cmd)\tIPC_64\n#else\nint ipc_parse_version (int *cmd);\n#endif\n\nextern void free_msg(struct msg_msg *msg);\nextern struct msg_msg *load_msg(const void __user *src, int len);\nextern struct msg_msg *copy_msg(struct msg_msg *src, struct msg_msg *dst);\nextern int store_msg(void __user *dest, struct msg_msg *msg, int len);\n\nextern void recompute_msgmni(struct ipc_namespace *);\n\nstatic inline int ipc_buildid(int id, int seq)\n{\n\treturn SEQ_MULTIPLIER * seq + id;\n}\n\nstatic inline int ipc_checkid(struct kern_ipc_perm *ipcp, int uid)\n{\n\treturn uid / SEQ_MULTIPLIER != ipcp->seq;\n}\n\nstatic inline void ipc_lock_by_ptr(struct kern_ipc_perm *perm)\n{\n\trcu_read_lock();\n\tspin_lock(&perm->lock);\n}\n\nstatic inline void ipc_unlock(struct kern_ipc_perm *perm)\n{\n\tspin_unlock(&perm->lock);\n\trcu_read_unlock();\n}\n\nstatic inline void ipc_lock_object(struct kern_ipc_perm *perm)\n{\n\tspin_lock(&perm->lock);\n}\n\nstruct kern_ipc_perm *ipc_lock_check(struct ipc_ids *ids, int id);\nstruct kern_ipc_perm *ipc_obtain_object_check(struct ipc_ids *ids, int id);\nint ipcget(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\t\tstruct ipc_ops *ops, struct ipc_params *params);\nvoid free_ipcs(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\tvoid (*free)(struct ipc_namespace *, struct kern_ipc_perm *));\n#endif\n"], "filenames": ["ipc/msg.c", "ipc/sem.c", "ipc/util.c", "ipc/util.h"], "buggy_code_start_loc": [690, 96, 442, 122], "buggy_code_end_loc": [691, 1786, 582, 123], "fixing_code_start_loc": [690, 97, 442, 122], "fixing_code_end_loc": [696, 1857, 584, 123], "type": "CWE-189", "message": "The ipc_rcu_putref function in ipc/util.c in the Linux kernel before 3.10 does not properly manage a reference count, which allows local users to cause a denial of service (memory consumption or system crash) via a crafted application.", "other": {"cve": {"id": "CVE-2013-4483", "sourceIdentifier": "secalert@redhat.com", "published": "2013-11-04T15:55:05.537", "lastModified": "2023-02-13T04:47:01.560", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The ipc_rcu_putref function in ipc/util.c in the Linux kernel before 3.10 does not properly manage a reference count, which allows local users to cause a denial of service (memory consumption or system crash) via a crafted application."}, {"lang": "es", "value": "La funci\u00f3n ipc_rcu_putref en ipc / util.c del kernel de Linux antes de 3.10 no gestiona adecuadamente una cuenta de referencia, que permite a usuarios locales provocar una denegaci\u00f3n de servicio (consumo de memoria o la ca\u00edda del sistema) a trav\u00e9s de una aplicaci\u00f3n manipulada."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-189"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.9.11", "matchCriteriaId": "B4B89243-CD7E-4171-B88E-ECB50D6DFCF9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.35:*:*:*:*:*:*:*", "matchCriteriaId": "F72412E3-8DA9-4CC9-A426-B534202ADBA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.36:*:*:*:*:*:*:*", "matchCriteriaId": "FCAA9D7A-3C3E-4C0B-9D38-EA80E68C2E46"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.37:*:*:*:*:*:*:*", "matchCriteriaId": "4A9E3AE5-3FCF-4CBB-A30B-082BCFBFB0CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.38:*:*:*:*:*:*:*", "matchCriteriaId": "CF715657-4C3A-4392-B85D-1BBF4DE45D89"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.39:*:*:*:*:*:*:*", "matchCriteriaId": "4B63C618-AC3D-4EF7-AFDF-27B9BF482B78"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.40:*:*:*:*:*:*:*", "matchCriteriaId": "C33DA5A9-5E40-4365-9602-82FB4DCD15B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.41:*:*:*:*:*:*:*", "matchCriteriaId": "EFAFDB74-40BD-46FA-89AC-617EB2C7160B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.42:*:*:*:*:*:*:*", "matchCriteriaId": "CF5F17DA-30A7-40CF-BD7C-CEDF06D64617"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.43:*:*:*:*:*:*:*", "matchCriteriaId": "71A276F5-BD9D-4C1B-90DF-9B0C15B6F7DF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.44:*:*:*:*:*:*:*", "matchCriteriaId": "F8F6EBEC-3C29-444B-BB85-6EF239B59EC1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.45:*:*:*:*:*:*:*", "matchCriteriaId": "FDB91302-FD18-44CF-A8A8-B31483328539"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.46:*:*:*:*:*:*:*", "matchCriteriaId": "9B81DC2B-46FA-4640-AD6C-2A404D94BA0B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.47:*:*:*:*:*:*:*", "matchCriteriaId": "BA6A1663-BC4C-4FC9-B5EB-A52EDED17B26"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.48:*:*:*:*:*:*:*", "matchCriteriaId": "69C33D6C-6B9F-49F4-B505-E7B589CDEC50"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.49:*:*:*:*:*:*:*", "matchCriteriaId": "C464796B-2F31-4159-A132-82A0C74137B7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.50:*:*:*:*:*:*:*", "matchCriteriaId": "1D6C6E46-FE29-4D2D-A0EC-43DA5112BCC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.51:*:*:*:*:*:*:*", "matchCriteriaId": "1A370E91-73A1-4D62-8E7B-696B920203F8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.52:*:*:*:*:*:*:*", "matchCriteriaId": "340197CD-9645-4B7E-B976-F3F5A7D4C5BE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.53:*:*:*:*:*:*:*", "matchCriteriaId": "96030636-0C4A-4A10-B768-525D6A0E18CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.54:*:*:*:*:*:*:*", "matchCriteriaId": "A42D8419-914F-4AD6-B0E9-C1290D514FF1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.55:*:*:*:*:*:*:*", "matchCriteriaId": "F4E2C88B-42EA-4F4F-B1F6-A9332EC6888B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.56:*:*:*:*:*:*:*", "matchCriteriaId": "2449D13B-3314-4182-832F-03F6B11AA31F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.57:*:*:*:*:*:*:*", "matchCriteriaId": "9A35B66C-F050-4462-A58E-FEE061B5582E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.58:*:*:*:*:*:*:*", "matchCriteriaId": "1B551164-0167-49BB-A3AE-4034BDA3DCB4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.59:*:*:*:*:*:*:*", "matchCriteriaId": "7244278E-49B6-4405-A14C-F3540C8F5AF8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.60:*:*:*:*:*:*:*", "matchCriteriaId": "B4C3E4B8-7274-4ABB-B7CE-6A39C183CE18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.61:*:*:*:*:*:*:*", "matchCriteriaId": "6501EDB9-4847-47F8-90EE-B295626E4CDC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.62:*:*:*:*:*:*:*", "matchCriteriaId": "2D676D48-7521-45E2-8563-6B966FF86A35"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.63:*:*:*:*:*:*:*", "matchCriteriaId": "3B69FA17-0AB9-4986-A5A7-2A4C1DD24222"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.64:*:*:*:*:*:*:*", "matchCriteriaId": "7BC35593-96C7-41F0-B738-1568F8129121"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.65:*:*:*:*:*:*:*", "matchCriteriaId": "38D23794-0E7C-4FA5-A7A8-CF940E3FA962"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.66:*:*:*:*:*:*:*", "matchCriteriaId": "008E1E7D-4C20-4560-9288-EF532ADB0029"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.67:*:*:*:*:*:*:*", "matchCriteriaId": "3B3A7044-A92E-47A9-A7BD-35E5B575F5FD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.68:*:*:*:*:*:*:*", "matchCriteriaId": "783E2980-B6AB-489E-B157-B6A2E10A32CA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc2:*:*:*:*:*:*", "matchCriteriaId": "99372D07-C06A-41FA-9843-6D57F99AB5AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc3:*:*:*:*:*:*", "matchCriteriaId": "2B9DC110-D260-4DB4-B8B0-EF1D160ADA07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc4:*:*:*:*:*:*", "matchCriteriaId": "6192FE84-4D53-40D4-AF61-78CE7136141A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc5:*:*:*:*:*:*", "matchCriteriaId": "42FEF3CF-1302-45EB-89CC-3786FE4BAC1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc6:*:*:*:*:*:*", "matchCriteriaId": "AE6A6B58-2C89-4DE4-BA57-78100818095C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc7:*:*:*:*:*:*", "matchCriteriaId": "1D467F87-2F13-4D26-9A93-E0BA526FEA24"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "FE348F7B-02DE-47D5-8011-F83DA9426021"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "E91594EA-F0A3-41B3-A9C6-F7864FC2F229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "9E1ECCDB-0208-48F6-B44F-16CC0ECE3503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "FBA8B5DE-372E-47E0-A0F6-BE286D509CC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "9A1CA083-2CF8-45AE-9E15-1AA3A8352E3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "19D69A49-5290-4C5F-8157-719AD58D253D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "290BD969-42E7-47B0-B21B-06DE4865432C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "23A9E29E-DE78-4C73-9FBD-C2410F5FC8B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "018434C9-E75F-45CB-A169-DAB4B1D864D7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "DC0AC68F-EC58-4C4F-8CBC-A59ECC00CCDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "C123C844-F6D7-471E-A62E-F756042FB1CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.12:*:*:*:*:*:*:*", "matchCriteriaId": "A11C38BB-7FA2-49B0-AAC9-83DB387A06DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.13:*:*:*:*:*:*:*", "matchCriteriaId": "61F3733C-E5F6-4855-B471-DF3FB823613B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.14:*:*:*:*:*:*:*", "matchCriteriaId": "1DDCA75F-9A06-4457-9A45-38A38E7F7086"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.15:*:*:*:*:*:*:*", "matchCriteriaId": "7AEA837E-7864-4003-8DB7-111ED710A7E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.16:*:*:*:*:*:*:*", "matchCriteriaId": "B6FE471F-2D1F-4A1D-A197-7E46B75787E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.17:*:*:*:*:*:*:*", "matchCriteriaId": "FDA9E6AB-58DC-4EC5-A25C-11F9D0B38BF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.18:*:*:*:*:*:*:*", "matchCriteriaId": "DC6B8DB3-B05B-41A2-B091-342D66AAE8F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.19:*:*:*:*:*:*:*", "matchCriteriaId": "958F0FF8-33EF-4A71-A0BD-572C85211DBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.20:*:*:*:*:*:*:*", "matchCriteriaId": "FBA39F48-B02F-4C48-B304-DA9CCA055244"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.21:*:*:*:*:*:*:*", "matchCriteriaId": "1FF841F3-48A7-41D7-9C45-A8170435A5EB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.22:*:*:*:*:*:*:*", "matchCriteriaId": "EF506916-A6DC-4B1E-90E5-959492AF55F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.23:*:*:*:*:*:*:*", "matchCriteriaId": "B3CDAD1F-2C6A-48C0-8FAB-C2659373FA25"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.24:*:*:*:*:*:*:*", "matchCriteriaId": "4FFE4B22-C96A-43D0-B993-F51EDD9C5E0E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.25:*:*:*:*:*:*:*", "matchCriteriaId": "F571CC8B-B212-4553-B463-1DB01D616E8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.26:*:*:*:*:*:*:*", "matchCriteriaId": "84E3E151-D437-48ED-A529-731EEFF88567"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.27:*:*:*:*:*:*:*", "matchCriteriaId": "E9E3EA3C-CCA5-4433-86E0-3D02C4757A0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.28:*:*:*:*:*:*:*", "matchCriteriaId": "F7AC4F7D-9FA6-4CF1-B2E9-70BF7D4D177C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.29:*:*:*:*:*:*:*", "matchCriteriaId": "3CE3A80D-9648-43CC-8F99-D741ED6552BF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.30:*:*:*:*:*:*:*", "matchCriteriaId": "C8A98C03-A465-41B4-A551-A26FEC7FFD94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "AFB76697-1C2F-48C0-9B14-517EC053D4B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc1:*:*:*:*:*:*", "matchCriteriaId": "BED88DFD-1DC5-4505-A441-44ECDEF0252D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc2:*:*:*:*:*:*", "matchCriteriaId": "DBFD2ACD-728A-4082-BB6A-A1EF6E58E47D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc3:*:*:*:*:*:*", "matchCriteriaId": "C31B0E51-F62D-4053-B04F-FC4D5BC373D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc4:*:*:*:*:*:*", "matchCriteriaId": "A914303E-1CB6-4AAD-9F5F-DE5433C4E814"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc5:*:*:*:*:*:*", "matchCriteriaId": "203BBA69-90B2-4C5E-8023-C14180742421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc6:*:*:*:*:*:*", "matchCriteriaId": "0DBFAB53-B889-4028-AC0E-7E165B152A18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc7:*:*:*:*:*:*", "matchCriteriaId": "FE409AEC-F677-4DEF-8EB7-2C35809043CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.1:*:*:*:*:*:*:*", "matchCriteriaId": "578EC12B-402F-4AD4-B8F8-C9B2CAB06891"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.2:*:*:*:*:*:*:*", "matchCriteriaId": "877002ED-8097-4BB4-BB88-6FC6306C38B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.3:*:*:*:*:*:*:*", "matchCriteriaId": "76294CE3-D72C-41D5-9E0F-B693D0042699"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.4:*:*:*:*:*:*:*", "matchCriteriaId": "916E97D4-1FAB-42F5-826B-653B1C0909A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.5:*:*:*:*:*:*:*", "matchCriteriaId": "33FD2217-C5D0-48C1-AD74-3527127FEF9C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.6:*:*:*:*:*:*:*", "matchCriteriaId": "2E92971F-B629-4E0A-9A50-8B235F9704B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.7:*:*:*:*:*:*:*", "matchCriteriaId": "EDD3A069-3829-4EE2-9D5A-29459F29D4C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.8:*:*:*:*:*:*:*", "matchCriteriaId": "A4A0964C-CEB2-41D7-A69C-1599B05B6171"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "0F960FA6-F904-4A4E-B483-44C70090E9A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:*:*", "matchCriteriaId": "261C1B41-C9E0-414F-8368-51C0C0B8AD38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:*:*", "matchCriteriaId": "5CCA261D-2B97-492F-89A0-5F209A804350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:*:*", "matchCriteriaId": "1B1C0C68-9194-473F-BE5E-EC7F184899FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:*:*", "matchCriteriaId": "D7A6AC9E-BEA6-44B0-B3B3-F0F94E32424A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:*:*", "matchCriteriaId": "16038328-9399-4B85-B777-BA4757D02C9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:*:*", "matchCriteriaId": "16CA2757-FA8D-43D9-96E8-D3C0EB6E1DEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:*:*", "matchCriteriaId": "E8CB5481-5EAE-401E-BD7E-D3095CCA9E94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.1:*:*:*:*:*:*:*", "matchCriteriaId": "A0F36FAC-141D-476D-84C5-A558C199F904"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.2:*:*:*:*:*:*:*", "matchCriteriaId": "51D64824-25F6-4761-BD6A-29038A143744"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.3:*:*:*:*:*:*:*", "matchCriteriaId": "E284C8A1-740F-454D-A774-99CD3A21B594"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.4:*:*:*:*:*:*:*", "matchCriteriaId": "C70D72AE-0CBF-4324-9935-57E28EC6279C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.5:*:*:*:*:*:*:*", "matchCriteriaId": "F674B06B-7E86-4E41-9126-8152D0DDABAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.6:*:*:*:*:*:*:*", "matchCriteriaId": "7039B3EC-8B22-413E-B582-B4BEC6181241"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.7:*:*:*:*:*:*:*", "matchCriteriaId": "35CF1DD2-80B9-4476-8963-5C3EF52B33F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.8:*:*:*:*:*:*:*", "matchCriteriaId": "BFB0B05B-A5CE-4B9C-AE7F-83062868D35B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.9:*:*:*:*:*:*:*", "matchCriteriaId": "D166A66E-7454-47EC-BB56-861A9AFEAFE1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.10:*:*:*:*:*:*:*", "matchCriteriaId": "7DA94F50-2A62-4300-BF4D-A342AAE35629"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.11:*:*:*:*:*:*:*", "matchCriteriaId": "252D937B-50DC-444F-AE73-5FCF6203DF27"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.12:*:*:*:*:*:*:*", "matchCriteriaId": "F6D8EE51-02C1-47BC-A92C-0A8ABEFD28FF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.13:*:*:*:*:*:*:*", "matchCriteriaId": "7F20A5D7-3B38-4911-861A-04C8310D5916"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.14:*:*:*:*:*:*:*", "matchCriteriaId": "D472DE3A-71D8-4F40-9DDE-85929A2B047D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.15:*:*:*:*:*:*:*", "matchCriteriaId": "B2AED943-65A8-4FDB-BBD0-CCEF8682A48C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.16:*:*:*:*:*:*:*", "matchCriteriaId": "D4640185-F3D8-4575-A71D-4C889A93DE2C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.17:*:*:*:*:*:*:*", "matchCriteriaId": "144CCF7C-025E-4879-B2E7-ABB8E4390BE5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.18:*:*:*:*:*:*:*", "matchCriteriaId": "B6FAA052-0B2B-40CE-8C98-919B8D08A5ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.19:*:*:*:*:*:*:*", "matchCriteriaId": "4B5A53DE-9C83-4A6B-96F3-23C03BF445D9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.20:*:*:*:*:*:*:*", "matchCriteriaId": "063EB879-CB05-4E33-AA90-9E43516839B5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.21:*:*:*:*:*:*:*", "matchCriteriaId": "2D25764F-4B02-4C65-954E-8C7D6632DE00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.22:*:*:*:*:*:*:*", "matchCriteriaId": "F31F5BF3-CD0A-465C-857F-273841BCD28A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.23:*:*:*:*:*:*:*", "matchCriteriaId": "FF302C8A-079B-42B9-B455-CD9083BFA067"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.24:*:*:*:*:*:*:*", "matchCriteriaId": "744999C0-33D3-4363-B3DB-E0D02CDD3918"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.25:*:*:*:*:*:*:*", "matchCriteriaId": "C2E77A76-2A60-45D8-9337-867BC22C5110"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.26:*:*:*:*:*:*:*", "matchCriteriaId": "C9F4AAE7-C870-46B7-B559-2949737BE777"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.27:*:*:*:*:*:*:*", "matchCriteriaId": "20FA2824-20B0-48B8-BB0A-4904C1D3E8AA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.28:*:*:*:*:*:*:*", "matchCriteriaId": "9F9B347E-61AC-419F-9701-B862BBFA46F2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.29:*:*:*:*:*:*:*", "matchCriteriaId": "989F351C-8B7C-4C1B-AFA2-AE9431576368"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.30:*:*:*:*:*:*:*", "matchCriteriaId": "8D22172A-9FA7-42E0-8451-165D8E47A573"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.31:*:*:*:*:*:*:*", "matchCriteriaId": "CE31624C-94F9-45D8-9B4A-D0028F10602F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.32:*:*:*:*:*:*:*", "matchCriteriaId": "70967A83-28F6-4568-9ADA-6EF232E5BBC2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.1:*:*:*:*:*:*:*", "matchCriteriaId": "962B0C45-AB29-4383-AC16-C6E8245D0FF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.2:*:*:*:*:*:*:*", "matchCriteriaId": "A0EE126B-74B2-4F79-BFE1-3DC169F3F9B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.3:*:*:*:*:*:*:*", "matchCriteriaId": "392075E0-A9C7-4B4A-90F9-7F1ADFF5EFA7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.4:*:*:*:*:*:*:*", "matchCriteriaId": "ECC66968-06F0-4874-A95A-A292C36E45C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.5:*:*:*:*:*:*:*", "matchCriteriaId": "5FE986E6-1068-4E1B-8EAB-DF1EAF32B4E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.6:*:*:*:*:*:*:*", "matchCriteriaId": "543E8536-1A8E-4E76-B89F-1B1F9F26FAB8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.7:*:*:*:*:*:*:*", "matchCriteriaId": "EC2B45E3-31E1-4B46-85FA-3A84E75B8F84"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6:*:*:*:*:*:*:*", "matchCriteriaId": "DDB8CC75-D3EE-417C-A83D-CB6D666FE595"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.1:*:*:*:*:*:*:*", "matchCriteriaId": "09A072F1-7BEE-4236-ACBB-55DB8FEF4A03"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.2:*:*:*:*:*:*:*", "matchCriteriaId": "E19D5A58-17D6-4502-A57A-70B2F84817A4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.3:*:*:*:*:*:*:*", "matchCriteriaId": "D58BA035-1204-4DFA-98A1-12111FB6222E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.4:*:*:*:*:*:*:*", "matchCriteriaId": "A17F2E87-8EB8-476A-B5B5-9AE5CF53D9FE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.5:*:*:*:*:*:*:*", "matchCriteriaId": "A8CCC101-5852-4299-9B67-EA1B149D58C0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.6:*:*:*:*:*:*:*", "matchCriteriaId": "B8074D32-C252-4AD3-A579-1C5EDDD7014B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.7:*:*:*:*:*:*:*", "matchCriteriaId": "962AA802-8179-4606-AAC0-9363BAEABC9F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.8:*:*:*:*:*:*:*", "matchCriteriaId": "1286C858-D5A2-45F3-86D1-E50FE53FB23C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.9:*:*:*:*:*:*:*", "matchCriteriaId": "5AC4A13E-F560-4D01-98A3-E2A2B82EB25B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.10:*:*:*:*:*:*:*", "matchCriteriaId": "942C462A-5398-4BB9-A792-598682E1FEF2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.11:*:*:*:*:*:*:*", "matchCriteriaId": "B852F7E0-0282-483D-BB4D-18CB7A4F1392"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7:*:*:*:*:*:*:*", "matchCriteriaId": "53ED9A31-99CC-41C8-8B72-5B2A9B49AA6C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.1:*:*:*:*:*:*:*", "matchCriteriaId": "EFD646BC-62F7-47CF-B0BE-768F701F7D9A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.2:*:*:*:*:*:*:*", "matchCriteriaId": "F43D418E-87C1-4C83-9FF1-4F45B4F452DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.3:*:*:*:*:*:*:*", "matchCriteriaId": "680D0E00-F29A-487C-8770-8E7EAC672B7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.4:*:*:*:*:*:*:*", "matchCriteriaId": "2DCA96A4-A836-4E94-A39C-3AD3EA1D9611"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.5:*:*:*:*:*:*:*", "matchCriteriaId": "753C05E3-B603-4E36-B9BA-FAEDCBF62A7D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.6:*:*:*:*:*:*:*", "matchCriteriaId": "E385C2E0-B9F1-4564-8E6D-56FD9E762405"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.7:*:*:*:*:*:*:*", "matchCriteriaId": "041335D4-05E1-4004-9381-28AAD5994B47"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.8:*:*:*:*:*:*:*", "matchCriteriaId": "370F2AE5-3DBC-46B9-AC70-F052C9229C00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.9:*:*:*:*:*:*:*", "matchCriteriaId": "7A971BE3-259D-4494-BBC5-12793D92DB57"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.10:*:*:*:*:*:*:*", "matchCriteriaId": "8E4719A6-FDEA-4714-A830-E23A52AE90BC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.0:*:*:*:*:*:*:*", "matchCriteriaId": "1A6E41FB-38CE-49F2-B796-9A5AA648E73F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.1:*:*:*:*:*:*:*", "matchCriteriaId": "93523FE1-5993-46CB-9299-7C8C1A04E873"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.2:*:*:*:*:*:*:*", "matchCriteriaId": "27ADC356-6BE9-43A3-9E0B-393DC4B1559A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.3:*:*:*:*:*:*:*", "matchCriteriaId": "4F543D23-1774-4D14-A7D1-AD49EDEA94DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.4:*:*:*:*:*:*:*", "matchCriteriaId": "FC323F58-CA00-4C3C-BA4D-CC2C0A6E5F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.5:*:*:*:*:*:*:*", "matchCriteriaId": "FEA0B2E3-668D-40ED-9D3D-709EB6449F8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.6:*:*:*:*:*:*:*", "matchCriteriaId": "3431B258-4EC8-4E7F-87BB-4D934880601E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.7:*:*:*:*:*:*:*", "matchCriteriaId": "1B09FA1E-8B28-4F2A-BA7E-8E1C40365970"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.8:*:*:*:*:*:*:*", "matchCriteriaId": "91917120-9D68-41C0-8B5D-85C256BC6200"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.9:*:*:*:*:*:*:*", "matchCriteriaId": "AAD268A0-096C-4C31-BEC5-D47F5149D462"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.10:*:*:*:*:*:*:*", "matchCriteriaId": "32BD2427-C47F-4660-A1D9-448E500EF5B9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.11:*:*:*:*:*:*:*", "matchCriteriaId": "02048CE5-81C7-4DFB-BC40-CE4C86B7E022"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.12:*:*:*:*:*:*:*", "matchCriteriaId": "934D2B37-0575-4A75-B00B-0028316D6DF0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.13:*:*:*:*:*:*:*", "matchCriteriaId": "06754C21-995C-4850-A4DC-F21826C0F8C5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc1:*:*:*:*:*:*", "matchCriteriaId": "42633FF9-FB0C-4095-B4A1-8D623A98683B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc2:*:*:*:*:*:*", "matchCriteriaId": "08C04619-89A2-4B15-82A2-48BCC662C1F1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc3:*:*:*:*:*:*", "matchCriteriaId": "5B039196-7159-476C-876A-C61242CC41DA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc4:*:*:*:*:*:*", "matchCriteriaId": "3A9E0457-53C9-44DD-ACFB-31EE1D1E060E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc5:*:*:*:*:*:*", "matchCriteriaId": "BEE406E7-87BA-44BA-BF61-673E6CC44A2F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc6:*:*:*:*:*:*", "matchCriteriaId": "29FBA173-658F-45DC-8205-934CACD67166"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc7:*:*:*:*:*:*", "matchCriteriaId": "139700F0-BA32-40CF-B9DF-C9C450384FDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.0:*:*:*:*:*:*:*", "matchCriteriaId": "E578085C-3968-4543-BEBA-EE3C3CB4FA02"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.1:*:*:*:*:*:*:*", "matchCriteriaId": "4DCFA441-68FB-4559-A245-FF0B79DE43CA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.2:*:*:*:*:*:*:*", "matchCriteriaId": "8C2508D8-6571-4B81-A0D7-E494CCD039CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.3:*:*:*:*:*:*:*", "matchCriteriaId": "8B516926-5E86-4C0A-85F3-F64E1FCDA249"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.4:*:*:*:*:*:*:*", "matchCriteriaId": "069D774D-79BE-479F-BF4E-F021AD808114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.5:*:*:*:*:*:*:*", "matchCriteriaId": "D15B27A9-46E0-4DDF-A00C-29F8F1F18D73"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.6:*:*:*:*:*:*:*", "matchCriteriaId": "A381BB4A-28B4-4672-87EE-91B3DDD6C71A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.7:*:*:*:*:*:*:*", "matchCriteriaId": "922F80CF-937D-4FA2-AFF2-6E47FFE9E1E9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.8:*:*:*:*:*:*:*", "matchCriteriaId": "A548ADF4-9E3B-407C-A5ED-05150EB3A185"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.9:*:*:*:*:*:*:*", "matchCriteriaId": "9C623230-4497-41B9-9BD2-7A6CFDD77983"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.10:*:*:*:*:*:*:*", "matchCriteriaId": "C72FA8A6-60A6-4486-A245-7BEF8B2A2711"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=6062a8dc0517bce23e3c2f7d2fea5e22411269a3", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-updates/2014-02/msg00045.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-0285.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2015-0284.html", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2013/10/30/4", "source": "secalert@redhat.com", "tags": ["Patch"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1024854", "source": "secalert@redhat.com", "tags": ["Patch"]}, {"url": "https://github.com/torvalds/linux/commit/6062a8dc0517bce23e3c2f7d2fea5e22411269a3", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v3.x/patch-3.10.bz2", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/6062a8dc0517bce23e3c2f7d2fea5e22411269a3"}}