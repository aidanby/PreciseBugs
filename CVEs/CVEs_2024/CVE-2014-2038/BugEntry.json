{"buggy_code": ["/*\n * linux/fs/nfs/write.c\n *\n * Write file data over NFS.\n *\n * Copyright (C) 1996, 1997, Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/writeback.h>\n#include <linux/swap.h>\n#include <linux/migrate.h>\n\n#include <linux/sunrpc/clnt.h>\n#include <linux/nfs_fs.h>\n#include <linux/nfs_mount.h>\n#include <linux/nfs_page.h>\n#include <linux/backing-dev.h>\n#include <linux/export.h>\n\n#include <asm/uaccess.h>\n\n#include \"delegation.h\"\n#include \"internal.h\"\n#include \"iostat.h\"\n#include \"nfs4_fs.h\"\n#include \"fscache.h\"\n#include \"pnfs.h\"\n\n#include \"nfstrace.h\"\n\n#define NFSDBG_FACILITY\t\tNFSDBG_PAGECACHE\n\n#define MIN_POOL_WRITE\t\t(32)\n#define MIN_POOL_COMMIT\t\t(4)\n\n/*\n * Local function declarations\n */\nstatic void nfs_redirty_request(struct nfs_page *req);\nstatic const struct rpc_call_ops nfs_write_common_ops;\nstatic const struct rpc_call_ops nfs_commit_ops;\nstatic const struct nfs_pgio_completion_ops nfs_async_write_completion_ops;\nstatic const struct nfs_commit_completion_ops nfs_commit_completion_ops;\n\nstatic struct kmem_cache *nfs_wdata_cachep;\nstatic mempool_t *nfs_wdata_mempool;\nstatic struct kmem_cache *nfs_cdata_cachep;\nstatic mempool_t *nfs_commit_mempool;\n\nstruct nfs_commit_data *nfs_commitdata_alloc(void)\n{\n\tstruct nfs_commit_data *p = mempool_alloc(nfs_commit_mempool, GFP_NOIO);\n\n\tif (p) {\n\t\tmemset(p, 0, sizeof(*p));\n\t\tINIT_LIST_HEAD(&p->pages);\n\t}\n\treturn p;\n}\nEXPORT_SYMBOL_GPL(nfs_commitdata_alloc);\n\nvoid nfs_commit_free(struct nfs_commit_data *p)\n{\n\tmempool_free(p, nfs_commit_mempool);\n}\nEXPORT_SYMBOL_GPL(nfs_commit_free);\n\nstruct nfs_write_header *nfs_writehdr_alloc(void)\n{\n\tstruct nfs_write_header *p = mempool_alloc(nfs_wdata_mempool, GFP_NOIO);\n\n\tif (p) {\n\t\tstruct nfs_pgio_header *hdr = &p->header;\n\n\t\tmemset(p, 0, sizeof(*p));\n\t\tINIT_LIST_HEAD(&hdr->pages);\n\t\tINIT_LIST_HEAD(&hdr->rpc_list);\n\t\tspin_lock_init(&hdr->lock);\n\t\tatomic_set(&hdr->refcnt, 0);\n\t\thdr->verf = &p->verf;\n\t}\n\treturn p;\n}\nEXPORT_SYMBOL_GPL(nfs_writehdr_alloc);\n\nstatic struct nfs_write_data *nfs_writedata_alloc(struct nfs_pgio_header *hdr,\n\t\t\t\t\t\t  unsigned int pagecount)\n{\n\tstruct nfs_write_data *data, *prealloc;\n\n\tprealloc = &container_of(hdr, struct nfs_write_header, header)->rpc_data;\n\tif (prealloc->header == NULL)\n\t\tdata = prealloc;\n\telse\n\t\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\tgoto out;\n\n\tif (nfs_pgarray_set(&data->pages, pagecount)) {\n\t\tdata->header = hdr;\n\t\tatomic_inc(&hdr->refcnt);\n\t} else {\n\t\tif (data != prealloc)\n\t\t\tkfree(data);\n\t\tdata = NULL;\n\t}\nout:\n\treturn data;\n}\n\nvoid nfs_writehdr_free(struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_write_header *whdr = container_of(hdr, struct nfs_write_header, header);\n\tmempool_free(whdr, nfs_wdata_mempool);\n}\nEXPORT_SYMBOL_GPL(nfs_writehdr_free);\n\nvoid nfs_writedata_release(struct nfs_write_data *wdata)\n{\n\tstruct nfs_pgio_header *hdr = wdata->header;\n\tstruct nfs_write_header *write_header = container_of(hdr, struct nfs_write_header, header);\n\n\tput_nfs_open_context(wdata->args.context);\n\tif (wdata->pages.pagevec != wdata->pages.page_array)\n\t\tkfree(wdata->pages.pagevec);\n\tif (wdata == &write_header->rpc_data) {\n\t\twdata->header = NULL;\n\t\twdata = NULL;\n\t}\n\tif (atomic_dec_and_test(&hdr->refcnt))\n\t\thdr->completion_ops->completion(hdr);\n\t/* Note: we only free the rpc_task after callbacks are done.\n\t * See the comment in rpc_free_task() for why\n\t */\n\tkfree(wdata);\n}\nEXPORT_SYMBOL_GPL(nfs_writedata_release);\n\nstatic void nfs_context_set_write_error(struct nfs_open_context *ctx, int error)\n{\n\tctx->error = error;\n\tsmp_wmb();\n\tset_bit(NFS_CONTEXT_ERROR_WRITE, &ctx->flags);\n}\n\nstatic struct nfs_page *\nnfs_page_find_request_locked(struct nfs_inode *nfsi, struct page *page)\n{\n\tstruct nfs_page *req = NULL;\n\n\tif (PagePrivate(page))\n\t\treq = (struct nfs_page *)page_private(page);\n\telse if (unlikely(PageSwapCache(page))) {\n\t\tstruct nfs_page *freq, *t;\n\n\t\t/* Linearly search the commit list for the correct req */\n\t\tlist_for_each_entry_safe(freq, t, &nfsi->commit_info.list, wb_list) {\n\t\t\tif (freq->wb_page == page) {\n\t\t\t\treq = freq;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (req)\n\t\tkref_get(&req->wb_kref);\n\n\treturn req;\n}\n\nstatic struct nfs_page *nfs_page_find_request(struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tstruct nfs_page *req = NULL;\n\n\tspin_lock(&inode->i_lock);\n\treq = nfs_page_find_request_locked(NFS_I(inode), page);\n\tspin_unlock(&inode->i_lock);\n\treturn req;\n}\n\n/* Adjust the file length if we're writing beyond the end */\nstatic void nfs_grow_file(struct page *page, unsigned int offset, unsigned int count)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tloff_t end, i_size;\n\tpgoff_t end_index;\n\n\tspin_lock(&inode->i_lock);\n\ti_size = i_size_read(inode);\n\tend_index = (i_size - 1) >> PAGE_CACHE_SHIFT;\n\tif (i_size > 0 && page_file_index(page) < end_index)\n\t\tgoto out;\n\tend = page_file_offset(page) + ((loff_t)offset+count);\n\tif (i_size >= end)\n\t\tgoto out;\n\ti_size_write(inode, end);\n\tnfs_inc_stats(inode, NFSIOS_EXTENDWRITE);\nout:\n\tspin_unlock(&inode->i_lock);\n}\n\n/* A writeback failed: mark the page as bad, and invalidate the page cache */\nstatic void nfs_set_pageerror(struct page *page)\n{\n\tnfs_zap_mapping(page_file_mapping(page)->host, page_file_mapping(page));\n}\n\n/* We can set the PG_uptodate flag if we see that a write request\n * covers the full page.\n */\nstatic void nfs_mark_uptodate(struct page *page, unsigned int base, unsigned int count)\n{\n\tif (PageUptodate(page))\n\t\treturn;\n\tif (base != 0)\n\t\treturn;\n\tif (count != nfs_page_length(page))\n\t\treturn;\n\tSetPageUptodate(page);\n}\n\nstatic int wb_priority(struct writeback_control *wbc)\n{\n\tif (wbc->for_reclaim)\n\t\treturn FLUSH_HIGHPRI | FLUSH_STABLE;\n\tif (wbc->for_kupdate || wbc->for_background)\n\t\treturn FLUSH_LOWPRI | FLUSH_COND_STABLE;\n\treturn FLUSH_COND_STABLE;\n}\n\n/*\n * NFS congestion control\n */\n\nint nfs_congestion_kb;\n\n#define NFS_CONGESTION_ON_THRESH \t(nfs_congestion_kb >> (PAGE_SHIFT-10))\n#define NFS_CONGESTION_OFF_THRESH\t\\\n\t(NFS_CONGESTION_ON_THRESH - (NFS_CONGESTION_ON_THRESH >> 2))\n\nstatic void nfs_set_page_writeback(struct page *page)\n{\n\tstruct nfs_server *nfss = NFS_SERVER(page_file_mapping(page)->host);\n\tint ret = test_set_page_writeback(page);\n\n\tWARN_ON_ONCE(ret != 0);\n\n\tif (atomic_long_inc_return(&nfss->writeback) >\n\t\t\tNFS_CONGESTION_ON_THRESH) {\n\t\tset_bdi_congested(&nfss->backing_dev_info,\n\t\t\t\t\tBLK_RW_ASYNC);\n\t}\n}\n\nstatic void nfs_end_page_writeback(struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tstruct nfs_server *nfss = NFS_SERVER(inode);\n\n\tend_page_writeback(page);\n\tif (atomic_long_dec_return(&nfss->writeback) < NFS_CONGESTION_OFF_THRESH)\n\t\tclear_bdi_congested(&nfss->backing_dev_info, BLK_RW_ASYNC);\n}\n\nstatic struct nfs_page *nfs_find_and_lock_request(struct page *page, bool nonblock)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tstruct nfs_page *req;\n\tint ret;\n\n\tspin_lock(&inode->i_lock);\n\tfor (;;) {\n\t\treq = nfs_page_find_request_locked(NFS_I(inode), page);\n\t\tif (req == NULL)\n\t\t\tbreak;\n\t\tif (nfs_lock_request(req))\n\t\t\tbreak;\n\t\t/* Note: If we hold the page lock, as is the case in nfs_writepage,\n\t\t *\t then the call to nfs_lock_request() will always\n\t\t *\t succeed provided that someone hasn't already marked the\n\t\t *\t request as dirty (in which case we don't care).\n\t\t */\n\t\tspin_unlock(&inode->i_lock);\n\t\tif (!nonblock)\n\t\t\tret = nfs_wait_on_request(req);\n\t\telse\n\t\t\tret = -EAGAIN;\n\t\tnfs_release_request(req);\n\t\tif (ret != 0)\n\t\t\treturn ERR_PTR(ret);\n\t\tspin_lock(&inode->i_lock);\n\t}\n\tspin_unlock(&inode->i_lock);\n\treturn req;\n}\n\n/*\n * Find an associated nfs write request, and prepare to flush it out\n * May return an error if the user signalled nfs_wait_on_request().\n */\nstatic int nfs_page_async_flush(struct nfs_pageio_descriptor *pgio,\n\t\t\t\tstruct page *page, bool nonblock)\n{\n\tstruct nfs_page *req;\n\tint ret = 0;\n\n\treq = nfs_find_and_lock_request(page, nonblock);\n\tif (!req)\n\t\tgoto out;\n\tret = PTR_ERR(req);\n\tif (IS_ERR(req))\n\t\tgoto out;\n\n\tnfs_set_page_writeback(page);\n\tWARN_ON_ONCE(test_bit(PG_CLEAN, &req->wb_flags));\n\n\tret = 0;\n\tif (!nfs_pageio_add_request(pgio, req)) {\n\t\tnfs_redirty_request(req);\n\t\tret = pgio->pg_error;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int nfs_do_writepage(struct page *page, struct writeback_control *wbc, struct nfs_pageio_descriptor *pgio)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tint ret;\n\n\tnfs_inc_stats(inode, NFSIOS_VFSWRITEPAGE);\n\tnfs_add_stats(inode, NFSIOS_WRITEPAGES, 1);\n\n\tnfs_pageio_cond_complete(pgio, page_file_index(page));\n\tret = nfs_page_async_flush(pgio, page, wbc->sync_mode == WB_SYNC_NONE);\n\tif (ret == -EAGAIN) {\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\n/*\n * Write an mmapped page to the server.\n */\nstatic int nfs_writepage_locked(struct page *page, struct writeback_control *wbc)\n{\n\tstruct nfs_pageio_descriptor pgio;\n\tint err;\n\n\tNFS_PROTO(page_file_mapping(page)->host)->write_pageio_init(&pgio,\n\t\t\t\t\t\t\t  page->mapping->host,\n\t\t\t\t\t\t\t  wb_priority(wbc),\n\t\t\t\t\t\t\t  &nfs_async_write_completion_ops);\n\terr = nfs_do_writepage(page, wbc, &pgio);\n\tnfs_pageio_complete(&pgio);\n\tif (err < 0)\n\t\treturn err;\n\tif (pgio.pg_error < 0)\n\t\treturn pgio.pg_error;\n\treturn 0;\n}\n\nint nfs_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tint ret;\n\n\tret = nfs_writepage_locked(page, wbc);\n\tunlock_page(page);\n\treturn ret;\n}\n\nstatic int nfs_writepages_callback(struct page *page, struct writeback_control *wbc, void *data)\n{\n\tint ret;\n\n\tret = nfs_do_writepage(page, wbc, data);\n\tunlock_page(page);\n\treturn ret;\n}\n\nint nfs_writepages(struct address_space *mapping, struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\tunsigned long *bitlock = &NFS_I(inode)->flags;\n\tstruct nfs_pageio_descriptor pgio;\n\tint err;\n\n\t/* Stop dirtying of new pages while we sync */\n\terr = wait_on_bit_lock(bitlock, NFS_INO_FLUSHING,\n\t\t\tnfs_wait_bit_killable, TASK_KILLABLE);\n\tif (err)\n\t\tgoto out_err;\n\n\tnfs_inc_stats(inode, NFSIOS_VFSWRITEPAGES);\n\n\tNFS_PROTO(inode)->write_pageio_init(&pgio, inode, wb_priority(wbc), &nfs_async_write_completion_ops);\n\terr = write_cache_pages(mapping, wbc, nfs_writepages_callback, &pgio);\n\tnfs_pageio_complete(&pgio);\n\n\tclear_bit_unlock(NFS_INO_FLUSHING, bitlock);\n\tsmp_mb__after_clear_bit();\n\twake_up_bit(bitlock, NFS_INO_FLUSHING);\n\n\tif (err < 0)\n\t\tgoto out_err;\n\terr = pgio.pg_error;\n\tif (err < 0)\n\t\tgoto out_err;\n\treturn 0;\nout_err:\n\treturn err;\n}\n\n/*\n * Insert a write request into an inode\n */\nstatic void nfs_inode_add_request(struct inode *inode, struct nfs_page *req)\n{\n\tstruct nfs_inode *nfsi = NFS_I(inode);\n\n\t/* Lock the request! */\n\tnfs_lock_request(req);\n\n\tspin_lock(&inode->i_lock);\n\tif (!nfsi->npages && NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))\n\t\tinode->i_version++;\n\t/*\n\t * Swap-space should not get truncated. Hence no need to plug the race\n\t * with invalidate/truncate.\n\t */\n\tif (likely(!PageSwapCache(req->wb_page))) {\n\t\tset_bit(PG_MAPPED, &req->wb_flags);\n\t\tSetPagePrivate(req->wb_page);\n\t\tset_page_private(req->wb_page, (unsigned long)req);\n\t}\n\tnfsi->npages++;\n\tkref_get(&req->wb_kref);\n\tspin_unlock(&inode->i_lock);\n}\n\n/*\n * Remove a write request from an inode\n */\nstatic void nfs_inode_remove_request(struct nfs_page *req)\n{\n\tstruct inode *inode = req->wb_context->dentry->d_inode;\n\tstruct nfs_inode *nfsi = NFS_I(inode);\n\n\tspin_lock(&inode->i_lock);\n\tif (likely(!PageSwapCache(req->wb_page))) {\n\t\tset_page_private(req->wb_page, 0);\n\t\tClearPagePrivate(req->wb_page);\n\t\tclear_bit(PG_MAPPED, &req->wb_flags);\n\t}\n\tnfsi->npages--;\n\tspin_unlock(&inode->i_lock);\n\tnfs_release_request(req);\n}\n\nstatic void\nnfs_mark_request_dirty(struct nfs_page *req)\n{\n\t__set_page_dirty_nobuffers(req->wb_page);\n}\n\n#if IS_ENABLED(CONFIG_NFS_V3) || IS_ENABLED(CONFIG_NFS_V4)\n/**\n * nfs_request_add_commit_list - add request to a commit list\n * @req: pointer to a struct nfs_page\n * @dst: commit list head\n * @cinfo: holds list lock and accounting info\n *\n * This sets the PG_CLEAN bit, updates the cinfo count of\n * number of outstanding requests requiring a commit as well as\n * the MM page stats.\n *\n * The caller must _not_ hold the cinfo->lock, but must be\n * holding the nfs_page lock.\n */\nvoid\nnfs_request_add_commit_list(struct nfs_page *req, struct list_head *dst,\n\t\t\t    struct nfs_commit_info *cinfo)\n{\n\tset_bit(PG_CLEAN, &(req)->wb_flags);\n\tspin_lock(cinfo->lock);\n\tnfs_list_add_request(req, dst);\n\tcinfo->mds->ncommit++;\n\tspin_unlock(cinfo->lock);\n\tif (!cinfo->dreq) {\n\t\tinc_zone_page_state(req->wb_page, NR_UNSTABLE_NFS);\n\t\tinc_bdi_stat(page_file_mapping(req->wb_page)->backing_dev_info,\n\t\t\t     BDI_RECLAIMABLE);\n\t\t__mark_inode_dirty(req->wb_context->dentry->d_inode,\n\t\t\t\t   I_DIRTY_DATASYNC);\n\t}\n}\nEXPORT_SYMBOL_GPL(nfs_request_add_commit_list);\n\n/**\n * nfs_request_remove_commit_list - Remove request from a commit list\n * @req: pointer to a nfs_page\n * @cinfo: holds list lock and accounting info\n *\n * This clears the PG_CLEAN bit, and updates the cinfo's count of\n * number of outstanding requests requiring a commit\n * It does not update the MM page stats.\n *\n * The caller _must_ hold the cinfo->lock and the nfs_page lock.\n */\nvoid\nnfs_request_remove_commit_list(struct nfs_page *req,\n\t\t\t       struct nfs_commit_info *cinfo)\n{\n\tif (!test_and_clear_bit(PG_CLEAN, &(req)->wb_flags))\n\t\treturn;\n\tnfs_list_remove_request(req);\n\tcinfo->mds->ncommit--;\n}\nEXPORT_SYMBOL_GPL(nfs_request_remove_commit_list);\n\nstatic void nfs_init_cinfo_from_inode(struct nfs_commit_info *cinfo,\n\t\t\t\t      struct inode *inode)\n{\n\tcinfo->lock = &inode->i_lock;\n\tcinfo->mds = &NFS_I(inode)->commit_info;\n\tcinfo->ds = pnfs_get_ds_info(inode);\n\tcinfo->dreq = NULL;\n\tcinfo->completion_ops = &nfs_commit_completion_ops;\n}\n\nvoid nfs_init_cinfo(struct nfs_commit_info *cinfo,\n\t\t    struct inode *inode,\n\t\t    struct nfs_direct_req *dreq)\n{\n\tif (dreq)\n\t\tnfs_init_cinfo_from_dreq(cinfo, dreq);\n\telse\n\t\tnfs_init_cinfo_from_inode(cinfo, inode);\n}\nEXPORT_SYMBOL_GPL(nfs_init_cinfo);\n\n/*\n * Add a request to the inode's commit list.\n */\nvoid\nnfs_mark_request_commit(struct nfs_page *req, struct pnfs_layout_segment *lseg,\n\t\t\tstruct nfs_commit_info *cinfo)\n{\n\tif (pnfs_mark_request_commit(req, lseg, cinfo))\n\t\treturn;\n\tnfs_request_add_commit_list(req, &cinfo->mds->list, cinfo);\n}\n\nstatic void\nnfs_clear_page_commit(struct page *page)\n{\n\tdec_zone_page_state(page, NR_UNSTABLE_NFS);\n\tdec_bdi_stat(page_file_mapping(page)->backing_dev_info, BDI_RECLAIMABLE);\n}\n\nstatic void\nnfs_clear_request_commit(struct nfs_page *req)\n{\n\tif (test_bit(PG_CLEAN, &req->wb_flags)) {\n\t\tstruct inode *inode = req->wb_context->dentry->d_inode;\n\t\tstruct nfs_commit_info cinfo;\n\n\t\tnfs_init_cinfo_from_inode(&cinfo, inode);\n\t\tif (!pnfs_clear_request_commit(req, &cinfo)) {\n\t\t\tspin_lock(cinfo.lock);\n\t\t\tnfs_request_remove_commit_list(req, &cinfo);\n\t\t\tspin_unlock(cinfo.lock);\n\t\t}\n\t\tnfs_clear_page_commit(req->wb_page);\n\t}\n}\n\nstatic inline\nint nfs_write_need_commit(struct nfs_write_data *data)\n{\n\tif (data->verf.committed == NFS_DATA_SYNC)\n\t\treturn data->header->lseg == NULL;\n\treturn data->verf.committed != NFS_FILE_SYNC;\n}\n\n#else\nstatic void nfs_init_cinfo_from_inode(struct nfs_commit_info *cinfo,\n\t\t\t\t      struct inode *inode)\n{\n}\n\nvoid nfs_init_cinfo(struct nfs_commit_info *cinfo,\n\t\t    struct inode *inode,\n\t\t    struct nfs_direct_req *dreq)\n{\n}\n\nvoid\nnfs_mark_request_commit(struct nfs_page *req, struct pnfs_layout_segment *lseg,\n\t\t\tstruct nfs_commit_info *cinfo)\n{\n}\n\nstatic void\nnfs_clear_request_commit(struct nfs_page *req)\n{\n}\n\nstatic inline\nint nfs_write_need_commit(struct nfs_write_data *data)\n{\n\treturn 0;\n}\n\n#endif\n\nstatic void nfs_write_completion(struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_commit_info cinfo;\n\tunsigned long bytes = 0;\n\n\tif (test_bit(NFS_IOHDR_REDO, &hdr->flags))\n\t\tgoto out;\n\tnfs_init_cinfo_from_inode(&cinfo, hdr->inode);\n\twhile (!list_empty(&hdr->pages)) {\n\t\tstruct nfs_page *req = nfs_list_entry(hdr->pages.next);\n\n\t\tbytes += req->wb_bytes;\n\t\tnfs_list_remove_request(req);\n\t\tif (test_bit(NFS_IOHDR_ERROR, &hdr->flags) &&\n\t\t    (hdr->good_bytes < bytes)) {\n\t\t\tnfs_set_pageerror(req->wb_page);\n\t\t\tnfs_context_set_write_error(req->wb_context, hdr->error);\n\t\t\tgoto remove_req;\n\t\t}\n\t\tif (test_bit(NFS_IOHDR_NEED_RESCHED, &hdr->flags)) {\n\t\t\tnfs_mark_request_dirty(req);\n\t\t\tgoto next;\n\t\t}\n\t\tif (test_bit(NFS_IOHDR_NEED_COMMIT, &hdr->flags)) {\n\t\t\tmemcpy(&req->wb_verf, &hdr->verf->verifier, sizeof(req->wb_verf));\n\t\t\tnfs_mark_request_commit(req, hdr->lseg, &cinfo);\n\t\t\tgoto next;\n\t\t}\nremove_req:\n\t\tnfs_inode_remove_request(req);\nnext:\n\t\tnfs_unlock_request(req);\n\t\tnfs_end_page_writeback(req->wb_page);\n\t\tnfs_release_request(req);\n\t}\nout:\n\thdr->release(hdr);\n}\n\n#if  IS_ENABLED(CONFIG_NFS_V3) || IS_ENABLED(CONFIG_NFS_V4)\nstatic unsigned long\nnfs_reqs_to_commit(struct nfs_commit_info *cinfo)\n{\n\treturn cinfo->mds->ncommit;\n}\n\n/* cinfo->lock held by caller */\nint\nnfs_scan_commit_list(struct list_head *src, struct list_head *dst,\n\t\t     struct nfs_commit_info *cinfo, int max)\n{\n\tstruct nfs_page *req, *tmp;\n\tint ret = 0;\n\n\tlist_for_each_entry_safe(req, tmp, src, wb_list) {\n\t\tif (!nfs_lock_request(req))\n\t\t\tcontinue;\n\t\tkref_get(&req->wb_kref);\n\t\tif (cond_resched_lock(cinfo->lock))\n\t\t\tlist_safe_reset_next(req, tmp, wb_list);\n\t\tnfs_request_remove_commit_list(req, cinfo);\n\t\tnfs_list_add_request(req, dst);\n\t\tret++;\n\t\tif ((ret == max) && !cinfo->dreq)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/*\n * nfs_scan_commit - Scan an inode for commit requests\n * @inode: NFS inode to scan\n * @dst: mds destination list\n * @cinfo: mds and ds lists of reqs ready to commit\n *\n * Moves requests from the inode's 'commit' request list.\n * The requests are *not* checked to ensure that they form a contiguous set.\n */\nint\nnfs_scan_commit(struct inode *inode, struct list_head *dst,\n\t\tstruct nfs_commit_info *cinfo)\n{\n\tint ret = 0;\n\n\tspin_lock(cinfo->lock);\n\tif (cinfo->mds->ncommit > 0) {\n\t\tconst int max = INT_MAX;\n\n\t\tret = nfs_scan_commit_list(&cinfo->mds->list, dst,\n\t\t\t\t\t   cinfo, max);\n\t\tret += pnfs_scan_commit_lists(inode, cinfo, max - ret);\n\t}\n\tspin_unlock(cinfo->lock);\n\treturn ret;\n}\n\n#else\nstatic unsigned long nfs_reqs_to_commit(struct nfs_commit_info *cinfo)\n{\n\treturn 0;\n}\n\nint nfs_scan_commit(struct inode *inode, struct list_head *dst,\n\t\t    struct nfs_commit_info *cinfo)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Search for an existing write request, and attempt to update\n * it to reflect a new dirty region on a given page.\n *\n * If the attempt fails, then the existing request is flushed out\n * to disk.\n */\nstatic struct nfs_page *nfs_try_to_update_request(struct inode *inode,\n\t\tstruct page *page,\n\t\tunsigned int offset,\n\t\tunsigned int bytes)\n{\n\tstruct nfs_page *req;\n\tunsigned int rqend;\n\tunsigned int end;\n\tint error;\n\n\tif (!PagePrivate(page))\n\t\treturn NULL;\n\n\tend = offset + bytes;\n\tspin_lock(&inode->i_lock);\n\n\tfor (;;) {\n\t\treq = nfs_page_find_request_locked(NFS_I(inode), page);\n\t\tif (req == NULL)\n\t\t\tgoto out_unlock;\n\n\t\trqend = req->wb_offset + req->wb_bytes;\n\t\t/*\n\t\t * Tell the caller to flush out the request if\n\t\t * the offsets are non-contiguous.\n\t\t * Note: nfs_flush_incompatible() will already\n\t\t * have flushed out requests having wrong owners.\n\t\t */\n\t\tif (offset > rqend\n\t\t    || end < req->wb_offset)\n\t\t\tgoto out_flushme;\n\n\t\tif (nfs_lock_request(req))\n\t\t\tbreak;\n\n\t\t/* The request is locked, so wait and then retry */\n\t\tspin_unlock(&inode->i_lock);\n\t\terror = nfs_wait_on_request(req);\n\t\tnfs_release_request(req);\n\t\tif (error != 0)\n\t\t\tgoto out_err;\n\t\tspin_lock(&inode->i_lock);\n\t}\n\n\t/* Okay, the request matches. Update the region */\n\tif (offset < req->wb_offset) {\n\t\treq->wb_offset = offset;\n\t\treq->wb_pgbase = offset;\n\t}\n\tif (end > rqend)\n\t\treq->wb_bytes = end - req->wb_offset;\n\telse\n\t\treq->wb_bytes = rqend - req->wb_offset;\nout_unlock:\n\tspin_unlock(&inode->i_lock);\n\tif (req)\n\t\tnfs_clear_request_commit(req);\n\treturn req;\nout_flushme:\n\tspin_unlock(&inode->i_lock);\n\tnfs_release_request(req);\n\terror = nfs_wb_page(inode, page);\nout_err:\n\treturn ERR_PTR(error);\n}\n\n/*\n * Try to update an existing write request, or create one if there is none.\n *\n * Note: Should always be called with the Page Lock held to prevent races\n * if we have to add a new request. Also assumes that the caller has\n * already called nfs_flush_incompatible() if necessary.\n */\nstatic struct nfs_page * nfs_setup_write_request(struct nfs_open_context* ctx,\n\t\tstruct page *page, unsigned int offset, unsigned int bytes)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tstruct nfs_page\t*req;\n\n\treq = nfs_try_to_update_request(inode, page, offset, bytes);\n\tif (req != NULL)\n\t\tgoto out;\n\treq = nfs_create_request(ctx, inode, page, offset, bytes);\n\tif (IS_ERR(req))\n\t\tgoto out;\n\tnfs_inode_add_request(inode, req);\nout:\n\treturn req;\n}\n\nstatic int nfs_writepage_setup(struct nfs_open_context *ctx, struct page *page,\n\t\tunsigned int offset, unsigned int count)\n{\n\tstruct nfs_page\t*req;\n\n\treq = nfs_setup_write_request(ctx, page, offset, count);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\t/* Update file length */\n\tnfs_grow_file(page, offset, count);\n\tnfs_mark_uptodate(page, req->wb_pgbase, req->wb_bytes);\n\tnfs_mark_request_dirty(req);\n\tnfs_unlock_and_release_request(req);\n\treturn 0;\n}\n\nint nfs_flush_incompatible(struct file *file, struct page *page)\n{\n\tstruct nfs_open_context *ctx = nfs_file_open_context(file);\n\tstruct nfs_lock_context *l_ctx;\n\tstruct nfs_page\t*req;\n\tint do_flush, status;\n\t/*\n\t * Look for a request corresponding to this page. If there\n\t * is one, and it belongs to another file, we flush it out\n\t * before we try to copy anything into the page. Do this\n\t * due to the lack of an ACCESS-type call in NFSv2.\n\t * Also do the same if we find a request from an existing\n\t * dropped page.\n\t */\n\tdo {\n\t\treq = nfs_page_find_request(page);\n\t\tif (req == NULL)\n\t\t\treturn 0;\n\t\tl_ctx = req->wb_lock_context;\n\t\tdo_flush = req->wb_page != page || req->wb_context != ctx;\n\t\tif (l_ctx && ctx->dentry->d_inode->i_flock != NULL) {\n\t\t\tdo_flush |= l_ctx->lockowner.l_owner != current->files\n\t\t\t\t|| l_ctx->lockowner.l_pid != current->tgid;\n\t\t}\n\t\tnfs_release_request(req);\n\t\tif (!do_flush)\n\t\t\treturn 0;\n\t\tstatus = nfs_wb_page(page_file_mapping(page)->host, page);\n\t} while (status == 0);\n\treturn status;\n}\n\n/*\n * Avoid buffered writes when a open context credential's key would\n * expire soon.\n *\n * Returns -EACCES if the key will expire within RPC_KEY_EXPIRE_FAIL.\n *\n * Return 0 and set a credential flag which triggers the inode to flush\n * and performs  NFS_FILE_SYNC writes if the key will expired within\n * RPC_KEY_EXPIRE_TIMEO.\n */\nint\nnfs_key_timeout_notify(struct file *filp, struct inode *inode)\n{\n\tstruct nfs_open_context *ctx = nfs_file_open_context(filp);\n\tstruct rpc_auth *auth = NFS_SERVER(inode)->client->cl_auth;\n\n\treturn rpcauth_key_timeout_notify(auth, ctx->cred);\n}\n\n/*\n * Test if the open context credential key is marked to expire soon.\n */\nbool nfs_ctx_key_to_expire(struct nfs_open_context *ctx)\n{\n\treturn rpcauth_cred_key_to_expire(ctx->cred);\n}\n\n/*\n * If the page cache is marked as unsafe or invalid, then we can't rely on\n * the PageUptodate() flag. In this case, we will need to turn off\n * write optimisations that depend on the page contents being correct.\n */\nstatic bool nfs_write_pageuptodate(struct page *page, struct inode *inode)\n{\n\tif (nfs_have_delegated_attributes(inode))\n\t\tgoto out;\n\tif (NFS_I(inode)->cache_validity & (NFS_INO_INVALID_DATA|NFS_INO_REVAL_PAGECACHE))\n\t\treturn false;\nout:\n\treturn PageUptodate(page) != 0;\n}\n\n/* If we know the page is up to date, and we're not using byte range locks (or\n * if we have the whole file locked for writing), it may be more efficient to\n * extend the write to cover the entire page in order to avoid fragmentation\n * inefficiencies.\n *\n * If the file is opened for synchronous writes or if we have a write delegation\n * from the server then we can just skip the rest of the checks.\n */\nstatic int nfs_can_extend_write(struct file *file, struct page *page, struct inode *inode)\n{\n\tif (file->f_flags & O_DSYNC)\n\t\treturn 0;\n\tif (NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))\n\t\treturn 1;\n\tif (nfs_write_pageuptodate(page, inode) && (inode->i_flock == NULL ||\n\t\t\t(inode->i_flock->fl_start == 0 &&\n\t\t\tinode->i_flock->fl_end == OFFSET_MAX &&\n\t\t\tinode->i_flock->fl_type != F_RDLCK)))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * Update and possibly write a cached page of an NFS file.\n *\n * XXX: Keep an eye on generic_file_read to make sure it doesn't do bad\n * things with a page scheduled for an RPC call (e.g. invalidate it).\n */\nint nfs_updatepage(struct file *file, struct page *page,\n\t\tunsigned int offset, unsigned int count)\n{\n\tstruct nfs_open_context *ctx = nfs_file_open_context(file);\n\tstruct inode\t*inode = page_file_mapping(page)->host;\n\tint\t\tstatus = 0;\n\n\tnfs_inc_stats(inode, NFSIOS_VFSUPDATEPAGE);\n\n\tdprintk(\"NFS:       nfs_updatepage(%pD2 %d@%lld)\\n\",\n\t\tfile, count, (long long)(page_file_offset(page) + offset));\n\n\tif (nfs_can_extend_write(file, page, inode)) {\n\t\tcount = max(count + offset, nfs_page_length(page));\n\t\toffset = 0;\n\t}\n\n\tstatus = nfs_writepage_setup(ctx, page, offset, count);\n\tif (status < 0)\n\t\tnfs_set_pageerror(page);\n\telse\n\t\t__set_page_dirty_nobuffers(page);\n\n\tdprintk(\"NFS:       nfs_updatepage returns %d (isize %lld)\\n\",\n\t\t\tstatus, (long long)i_size_read(inode));\n\treturn status;\n}\n\nstatic int flush_task_priority(int how)\n{\n\tswitch (how & (FLUSH_HIGHPRI|FLUSH_LOWPRI)) {\n\t\tcase FLUSH_HIGHPRI:\n\t\t\treturn RPC_PRIORITY_HIGH;\n\t\tcase FLUSH_LOWPRI:\n\t\t\treturn RPC_PRIORITY_LOW;\n\t}\n\treturn RPC_PRIORITY_NORMAL;\n}\n\nint nfs_initiate_write(struct rpc_clnt *clnt,\n\t\t       struct nfs_write_data *data,\n\t\t       const struct rpc_call_ops *call_ops,\n\t\t       int how, int flags)\n{\n\tstruct inode *inode = data->header->inode;\n\tint priority = flush_task_priority(how);\n\tstruct rpc_task *task;\n\tstruct rpc_message msg = {\n\t\t.rpc_argp = &data->args,\n\t\t.rpc_resp = &data->res,\n\t\t.rpc_cred = data->header->cred,\n\t};\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_client = clnt,\n\t\t.task = &data->task,\n\t\t.rpc_message = &msg,\n\t\t.callback_ops = call_ops,\n\t\t.callback_data = data,\n\t\t.workqueue = nfsiod_workqueue,\n\t\t.flags = RPC_TASK_ASYNC | flags,\n\t\t.priority = priority,\n\t};\n\tint ret = 0;\n\n\t/* Set up the initial task struct.  */\n\tNFS_PROTO(inode)->write_setup(data, &msg);\n\n\tdprintk(\"NFS: %5u initiated write call \"\n\t\t\"(req %s/%llu, %u bytes @ offset %llu)\\n\",\n\t\tdata->task.tk_pid,\n\t\tinode->i_sb->s_id,\n\t\t(unsigned long long)NFS_FILEID(inode),\n\t\tdata->args.count,\n\t\t(unsigned long long)data->args.offset);\n\n\tnfs4_state_protect_write(NFS_SERVER(inode)->nfs_client,\n\t\t\t\t &task_setup_data.rpc_client, &msg, data);\n\n\ttask = rpc_run_task(&task_setup_data);\n\tif (IS_ERR(task)) {\n\t\tret = PTR_ERR(task);\n\t\tgoto out;\n\t}\n\tif (how & FLUSH_SYNC) {\n\t\tret = rpc_wait_for_completion_task(task);\n\t\tif (ret == 0)\n\t\t\tret = task->tk_status;\n\t}\n\trpc_put_task(task);\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nfs_initiate_write);\n\n/*\n * Set up the argument/result storage required for the RPC call.\n */\nstatic void nfs_write_rpcsetup(struct nfs_write_data *data,\n\t\tunsigned int count, unsigned int offset,\n\t\tint how, struct nfs_commit_info *cinfo)\n{\n\tstruct nfs_page *req = data->header->req;\n\n\t/* Set up the RPC argument and reply structs\n\t * NB: take care not to mess about with data->commit et al. */\n\n\tdata->args.fh     = NFS_FH(data->header->inode);\n\tdata->args.offset = req_offset(req) + offset;\n\t/* pnfs_set_layoutcommit needs this */\n\tdata->mds_offset = data->args.offset;\n\tdata->args.pgbase = req->wb_pgbase + offset;\n\tdata->args.pages  = data->pages.pagevec;\n\tdata->args.count  = count;\n\tdata->args.context = get_nfs_open_context(req->wb_context);\n\tdata->args.lock_context = req->wb_lock_context;\n\tdata->args.stable  = NFS_UNSTABLE;\n\tswitch (how & (FLUSH_STABLE | FLUSH_COND_STABLE)) {\n\tcase 0:\n\t\tbreak;\n\tcase FLUSH_COND_STABLE:\n\t\tif (nfs_reqs_to_commit(cinfo))\n\t\t\tbreak;\n\tdefault:\n\t\tdata->args.stable = NFS_FILE_SYNC;\n\t}\n\n\tdata->res.fattr   = &data->fattr;\n\tdata->res.count   = count;\n\tdata->res.verf    = &data->verf;\n\tnfs_fattr_init(&data->fattr);\n}\n\nstatic int nfs_do_write(struct nfs_write_data *data,\n\t\tconst struct rpc_call_ops *call_ops,\n\t\tint how)\n{\n\tstruct inode *inode = data->header->inode;\n\n\treturn nfs_initiate_write(NFS_CLIENT(inode), data, call_ops, how, 0);\n}\n\nstatic int nfs_do_multiple_writes(struct list_head *head,\n\t\tconst struct rpc_call_ops *call_ops,\n\t\tint how)\n{\n\tstruct nfs_write_data *data;\n\tint ret = 0;\n\n\twhile (!list_empty(head)) {\n\t\tint ret2;\n\n\t\tdata = list_first_entry(head, struct nfs_write_data, list);\n\t\tlist_del_init(&data->list);\n\t\t\n\t\tret2 = nfs_do_write(data, call_ops, how);\n\t\t if (ret == 0)\n\t\t\t ret = ret2;\n\t}\n\treturn ret;\n}\n\n/* If a nfs_flush_* function fails, it should remove reqs from @head and\n * call this on each, which will prepare them to be retried on next\n * writeback using standard nfs.\n */\nstatic void nfs_redirty_request(struct nfs_page *req)\n{\n\tnfs_mark_request_dirty(req);\n\tnfs_unlock_request(req);\n\tnfs_end_page_writeback(req->wb_page);\n\tnfs_release_request(req);\n}\n\nstatic void nfs_async_write_error(struct list_head *head)\n{\n\tstruct nfs_page\t*req;\n\n\twhile (!list_empty(head)) {\n\t\treq = nfs_list_entry(head->next);\n\t\tnfs_list_remove_request(req);\n\t\tnfs_redirty_request(req);\n\t}\n}\n\nstatic const struct nfs_pgio_completion_ops nfs_async_write_completion_ops = {\n\t.error_cleanup = nfs_async_write_error,\n\t.completion = nfs_write_completion,\n};\n\nstatic void nfs_flush_error(struct nfs_pageio_descriptor *desc,\n\t\tstruct nfs_pgio_header *hdr)\n{\n\tset_bit(NFS_IOHDR_REDO, &hdr->flags);\n\twhile (!list_empty(&hdr->rpc_list)) {\n\t\tstruct nfs_write_data *data = list_first_entry(&hdr->rpc_list,\n\t\t\t\tstruct nfs_write_data, list);\n\t\tlist_del(&data->list);\n\t\tnfs_writedata_release(data);\n\t}\n\tdesc->pg_completion_ops->error_cleanup(&desc->pg_list);\n}\n\n/*\n * Generate multiple small requests to write out a single\n * contiguous dirty area on one page.\n */\nstatic int nfs_flush_multi(struct nfs_pageio_descriptor *desc,\n\t\t\t   struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_page *req = hdr->req;\n\tstruct page *page = req->wb_page;\n\tstruct nfs_write_data *data;\n\tsize_t wsize = desc->pg_bsize, nbytes;\n\tunsigned int offset;\n\tint requests = 0;\n\tstruct nfs_commit_info cinfo;\n\n\tnfs_init_cinfo(&cinfo, desc->pg_inode, desc->pg_dreq);\n\n\tif ((desc->pg_ioflags & FLUSH_COND_STABLE) &&\n\t    (desc->pg_moreio || nfs_reqs_to_commit(&cinfo) ||\n\t     desc->pg_count > wsize))\n\t\tdesc->pg_ioflags &= ~FLUSH_COND_STABLE;\n\n\n\toffset = 0;\n\tnbytes = desc->pg_count;\n\tdo {\n\t\tsize_t len = min(nbytes, wsize);\n\n\t\tdata = nfs_writedata_alloc(hdr, 1);\n\t\tif (!data) {\n\t\t\tnfs_flush_error(desc, hdr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tdata->pages.pagevec[0] = page;\n\t\tnfs_write_rpcsetup(data, len, offset, desc->pg_ioflags, &cinfo);\n\t\tlist_add(&data->list, &hdr->rpc_list);\n\t\trequests++;\n\t\tnbytes -= len;\n\t\toffset += len;\n\t} while (nbytes != 0);\n\tnfs_list_remove_request(req);\n\tnfs_list_add_request(req, &hdr->pages);\n\tdesc->pg_rpc_callops = &nfs_write_common_ops;\n\treturn 0;\n}\n\n/*\n * Create an RPC task for the given write request and kick it.\n * The page must have been locked by the caller.\n *\n * It may happen that the page we're passed is not marked dirty.\n * This is the case if nfs_updatepage detects a conflicting request\n * that has been written but not committed.\n */\nstatic int nfs_flush_one(struct nfs_pageio_descriptor *desc,\n\t\t\t struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_page\t\t*req;\n\tstruct page\t\t**pages;\n\tstruct nfs_write_data\t*data;\n\tstruct list_head *head = &desc->pg_list;\n\tstruct nfs_commit_info cinfo;\n\n\tdata = nfs_writedata_alloc(hdr, nfs_page_array_len(desc->pg_base,\n\t\t\t\t\t\t\t   desc->pg_count));\n\tif (!data) {\n\t\tnfs_flush_error(desc, hdr);\n\t\treturn -ENOMEM;\n\t}\n\n\tnfs_init_cinfo(&cinfo, desc->pg_inode, desc->pg_dreq);\n\tpages = data->pages.pagevec;\n\twhile (!list_empty(head)) {\n\t\treq = nfs_list_entry(head->next);\n\t\tnfs_list_remove_request(req);\n\t\tnfs_list_add_request(req, &hdr->pages);\n\t\t*pages++ = req->wb_page;\n\t}\n\n\tif ((desc->pg_ioflags & FLUSH_COND_STABLE) &&\n\t    (desc->pg_moreio || nfs_reqs_to_commit(&cinfo)))\n\t\tdesc->pg_ioflags &= ~FLUSH_COND_STABLE;\n\n\t/* Set up the argument struct */\n\tnfs_write_rpcsetup(data, desc->pg_count, 0, desc->pg_ioflags, &cinfo);\n\tlist_add(&data->list, &hdr->rpc_list);\n\tdesc->pg_rpc_callops = &nfs_write_common_ops;\n\treturn 0;\n}\n\nint nfs_generic_flush(struct nfs_pageio_descriptor *desc,\n\t\t      struct nfs_pgio_header *hdr)\n{\n\tif (desc->pg_bsize < PAGE_CACHE_SIZE)\n\t\treturn nfs_flush_multi(desc, hdr);\n\treturn nfs_flush_one(desc, hdr);\n}\nEXPORT_SYMBOL_GPL(nfs_generic_flush);\n\nstatic int nfs_generic_pg_writepages(struct nfs_pageio_descriptor *desc)\n{\n\tstruct nfs_write_header *whdr;\n\tstruct nfs_pgio_header *hdr;\n\tint ret;\n\n\twhdr = nfs_writehdr_alloc();\n\tif (!whdr) {\n\t\tdesc->pg_completion_ops->error_cleanup(&desc->pg_list);\n\t\treturn -ENOMEM;\n\t}\n\thdr = &whdr->header;\n\tnfs_pgheader_init(desc, hdr, nfs_writehdr_free);\n\tatomic_inc(&hdr->refcnt);\n\tret = nfs_generic_flush(desc, hdr);\n\tif (ret == 0)\n\t\tret = nfs_do_multiple_writes(&hdr->rpc_list,\n\t\t\t\t\t     desc->pg_rpc_callops,\n\t\t\t\t\t     desc->pg_ioflags);\n\tif (atomic_dec_and_test(&hdr->refcnt))\n\t\thdr->completion_ops->completion(hdr);\n\treturn ret;\n}\n\nstatic const struct nfs_pageio_ops nfs_pageio_write_ops = {\n\t.pg_test = nfs_generic_pg_test,\n\t.pg_doio = nfs_generic_pg_writepages,\n};\n\nvoid nfs_pageio_init_write(struct nfs_pageio_descriptor *pgio,\n\t\t\t       struct inode *inode, int ioflags,\n\t\t\t       const struct nfs_pgio_completion_ops *compl_ops)\n{\n\tnfs_pageio_init(pgio, inode, &nfs_pageio_write_ops, compl_ops,\n\t\t\t\tNFS_SERVER(inode)->wsize, ioflags);\n}\nEXPORT_SYMBOL_GPL(nfs_pageio_init_write);\n\nvoid nfs_pageio_reset_write_mds(struct nfs_pageio_descriptor *pgio)\n{\n\tpgio->pg_ops = &nfs_pageio_write_ops;\n\tpgio->pg_bsize = NFS_SERVER(pgio->pg_inode)->wsize;\n}\nEXPORT_SYMBOL_GPL(nfs_pageio_reset_write_mds);\n\n\nvoid nfs_write_prepare(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_write_data *data = calldata;\n\tint err;\n\terr = NFS_PROTO(data->header->inode)->write_rpc_prepare(task, data);\n\tif (err)\n\t\trpc_exit(task, err);\n}\n\nvoid nfs_commit_prepare(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_commit_data *data = calldata;\n\n\tNFS_PROTO(data->inode)->commit_rpc_prepare(task, data);\n}\n\n/*\n * Handle a write reply that flushes a whole page.\n *\n * FIXME: There is an inherent race with invalidate_inode_pages and\n *\t  writebacks since the page->count is kept > 1 for as long\n *\t  as the page has a write request pending.\n */\nstatic void nfs_writeback_done_common(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_write_data\t*data = calldata;\n\n\tnfs_writeback_done(task, data);\n}\n\nstatic void nfs_writeback_release_common(void *calldata)\n{\n\tstruct nfs_write_data\t*data = calldata;\n\tstruct nfs_pgio_header *hdr = data->header;\n\tint status = data->task.tk_status;\n\n\tif ((status >= 0) && nfs_write_need_commit(data)) {\n\t\tspin_lock(&hdr->lock);\n\t\tif (test_bit(NFS_IOHDR_NEED_RESCHED, &hdr->flags))\n\t\t\t; /* Do nothing */\n\t\telse if (!test_and_set_bit(NFS_IOHDR_NEED_COMMIT, &hdr->flags))\n\t\t\tmemcpy(hdr->verf, &data->verf, sizeof(*hdr->verf));\n\t\telse if (memcmp(hdr->verf, &data->verf, sizeof(*hdr->verf)))\n\t\t\tset_bit(NFS_IOHDR_NEED_RESCHED, &hdr->flags);\n\t\tspin_unlock(&hdr->lock);\n\t}\n\tnfs_writedata_release(data);\n}\n\nstatic const struct rpc_call_ops nfs_write_common_ops = {\n\t.rpc_call_prepare = nfs_write_prepare,\n\t.rpc_call_done = nfs_writeback_done_common,\n\t.rpc_release = nfs_writeback_release_common,\n};\n\n\n/*\n * This function is called when the WRITE call is complete.\n */\nvoid nfs_writeback_done(struct rpc_task *task, struct nfs_write_data *data)\n{\n\tstruct nfs_writeargs\t*argp = &data->args;\n\tstruct nfs_writeres\t*resp = &data->res;\n\tstruct inode\t\t*inode = data->header->inode;\n\tint status;\n\n\tdprintk(\"NFS: %5u nfs_writeback_done (status %d)\\n\",\n\t\ttask->tk_pid, task->tk_status);\n\n\t/*\n\t * ->write_done will attempt to use post-op attributes to detect\n\t * conflicting writes by other clients.  A strict interpretation\n\t * of close-to-open would allow us to continue caching even if\n\t * another writer had changed the file, but some applications\n\t * depend on tighter cache coherency when writing.\n\t */\n\tstatus = NFS_PROTO(inode)->write_done(task, data);\n\tif (status != 0)\n\t\treturn;\n\tnfs_add_stats(inode, NFSIOS_SERVERWRITTENBYTES, resp->count);\n\n#if IS_ENABLED(CONFIG_NFS_V3) || IS_ENABLED(CONFIG_NFS_V4)\n\tif (resp->verf->committed < argp->stable && task->tk_status >= 0) {\n\t\t/* We tried a write call, but the server did not\n\t\t * commit data to stable storage even though we\n\t\t * requested it.\n\t\t * Note: There is a known bug in Tru64 < 5.0 in which\n\t\t *\t the server reports NFS_DATA_SYNC, but performs\n\t\t *\t NFS_FILE_SYNC. We therefore implement this checking\n\t\t *\t as a dprintk() in order to avoid filling syslog.\n\t\t */\n\t\tstatic unsigned long    complain;\n\n\t\t/* Note this will print the MDS for a DS write */\n\t\tif (time_before(complain, jiffies)) {\n\t\t\tdprintk(\"NFS:       faulty NFS server %s:\"\n\t\t\t\t\" (committed = %d) != (stable = %d)\\n\",\n\t\t\t\tNFS_SERVER(inode)->nfs_client->cl_hostname,\n\t\t\t\tresp->verf->committed, argp->stable);\n\t\t\tcomplain = jiffies + 300 * HZ;\n\t\t}\n\t}\n#endif\n\tif (task->tk_status < 0)\n\t\tnfs_set_pgio_error(data->header, task->tk_status, argp->offset);\n\telse if (resp->count < argp->count) {\n\t\tstatic unsigned long    complain;\n\n\t\t/* This a short write! */\n\t\tnfs_inc_stats(inode, NFSIOS_SHORTWRITE);\n\n\t\t/* Has the server at least made some progress? */\n\t\tif (resp->count == 0) {\n\t\t\tif (time_before(complain, jiffies)) {\n\t\t\t\tprintk(KERN_WARNING\n\t\t\t\t       \"NFS: Server wrote zero bytes, expected %u.\\n\",\n\t\t\t\t       argp->count);\n\t\t\t\tcomplain = jiffies + 300 * HZ;\n\t\t\t}\n\t\t\tnfs_set_pgio_error(data->header, -EIO, argp->offset);\n\t\t\ttask->tk_status = -EIO;\n\t\t\treturn;\n\t\t}\n\t\t/* Was this an NFSv2 write or an NFSv3 stable write? */\n\t\tif (resp->verf->committed != NFS_UNSTABLE) {\n\t\t\t/* Resend from where the server left off */\n\t\t\tdata->mds_offset += resp->count;\n\t\t\targp->offset += resp->count;\n\t\t\targp->pgbase += resp->count;\n\t\t\targp->count -= resp->count;\n\t\t} else {\n\t\t\t/* Resend as a stable write in order to avoid\n\t\t\t * headaches in the case of a server crash.\n\t\t\t */\n\t\t\targp->stable = NFS_FILE_SYNC;\n\t\t}\n\t\trpc_restart_call_prepare(task);\n\t}\n}\n\n\n#if IS_ENABLED(CONFIG_NFS_V3) || IS_ENABLED(CONFIG_NFS_V4)\nstatic int nfs_commit_set_lock(struct nfs_inode *nfsi, int may_wait)\n{\n\tint ret;\n\n\tif (!test_and_set_bit(NFS_INO_COMMIT, &nfsi->flags))\n\t\treturn 1;\n\tif (!may_wait)\n\t\treturn 0;\n\tret = out_of_line_wait_on_bit_lock(&nfsi->flags,\n\t\t\t\tNFS_INO_COMMIT,\n\t\t\t\tnfs_wait_bit_killable,\n\t\t\t\tTASK_KILLABLE);\n\treturn (ret < 0) ? ret : 1;\n}\n\nstatic void nfs_commit_clear_lock(struct nfs_inode *nfsi)\n{\n\tclear_bit(NFS_INO_COMMIT, &nfsi->flags);\n\tsmp_mb__after_clear_bit();\n\twake_up_bit(&nfsi->flags, NFS_INO_COMMIT);\n}\n\nvoid nfs_commitdata_release(struct nfs_commit_data *data)\n{\n\tput_nfs_open_context(data->context);\n\tnfs_commit_free(data);\n}\nEXPORT_SYMBOL_GPL(nfs_commitdata_release);\n\nint nfs_initiate_commit(struct rpc_clnt *clnt, struct nfs_commit_data *data,\n\t\t\tconst struct rpc_call_ops *call_ops,\n\t\t\tint how, int flags)\n{\n\tstruct rpc_task *task;\n\tint priority = flush_task_priority(how);\n\tstruct rpc_message msg = {\n\t\t.rpc_argp = &data->args,\n\t\t.rpc_resp = &data->res,\n\t\t.rpc_cred = data->cred,\n\t};\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.task = &data->task,\n\t\t.rpc_client = clnt,\n\t\t.rpc_message = &msg,\n\t\t.callback_ops = call_ops,\n\t\t.callback_data = data,\n\t\t.workqueue = nfsiod_workqueue,\n\t\t.flags = RPC_TASK_ASYNC | flags,\n\t\t.priority = priority,\n\t};\n\t/* Set up the initial task struct.  */\n\tNFS_PROTO(data->inode)->commit_setup(data, &msg);\n\n\tdprintk(\"NFS: %5u initiated commit call\\n\", data->task.tk_pid);\n\n\tnfs4_state_protect(NFS_SERVER(data->inode)->nfs_client,\n\t\tNFS_SP4_MACH_CRED_COMMIT, &task_setup_data.rpc_client, &msg);\n\n\ttask = rpc_run_task(&task_setup_data);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\tif (how & FLUSH_SYNC)\n\t\trpc_wait_for_completion_task(task);\n\trpc_put_task(task);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nfs_initiate_commit);\n\n/*\n * Set up the argument/result storage required for the RPC call.\n */\nvoid nfs_init_commit(struct nfs_commit_data *data,\n\t\t     struct list_head *head,\n\t\t     struct pnfs_layout_segment *lseg,\n\t\t     struct nfs_commit_info *cinfo)\n{\n\tstruct nfs_page *first = nfs_list_entry(head->next);\n\tstruct inode *inode = first->wb_context->dentry->d_inode;\n\n\t/* Set up the RPC argument and reply structs\n\t * NB: take care not to mess about with data->commit et al. */\n\n\tlist_splice_init(head, &data->pages);\n\n\tdata->inode\t  = inode;\n\tdata->cred\t  = first->wb_context->cred;\n\tdata->lseg\t  = lseg; /* reference transferred */\n\tdata->mds_ops     = &nfs_commit_ops;\n\tdata->completion_ops = cinfo->completion_ops;\n\tdata->dreq\t  = cinfo->dreq;\n\n\tdata->args.fh     = NFS_FH(data->inode);\n\t/* Note: we always request a commit of the entire inode */\n\tdata->args.offset = 0;\n\tdata->args.count  = 0;\n\tdata->context     = get_nfs_open_context(first->wb_context);\n\tdata->res.fattr   = &data->fattr;\n\tdata->res.verf    = &data->verf;\n\tnfs_fattr_init(&data->fattr);\n}\nEXPORT_SYMBOL_GPL(nfs_init_commit);\n\nvoid nfs_retry_commit(struct list_head *page_list,\n\t\t      struct pnfs_layout_segment *lseg,\n\t\t      struct nfs_commit_info *cinfo)\n{\n\tstruct nfs_page *req;\n\n\twhile (!list_empty(page_list)) {\n\t\treq = nfs_list_entry(page_list->next);\n\t\tnfs_list_remove_request(req);\n\t\tnfs_mark_request_commit(req, lseg, cinfo);\n\t\tif (!cinfo->dreq) {\n\t\t\tdec_zone_page_state(req->wb_page, NR_UNSTABLE_NFS);\n\t\t\tdec_bdi_stat(page_file_mapping(req->wb_page)->backing_dev_info,\n\t\t\t\t     BDI_RECLAIMABLE);\n\t\t}\n\t\tnfs_unlock_and_release_request(req);\n\t}\n}\nEXPORT_SYMBOL_GPL(nfs_retry_commit);\n\n/*\n * Commit dirty pages\n */\nstatic int\nnfs_commit_list(struct inode *inode, struct list_head *head, int how,\n\t\tstruct nfs_commit_info *cinfo)\n{\n\tstruct nfs_commit_data\t*data;\n\n\tdata = nfs_commitdata_alloc();\n\n\tif (!data)\n\t\tgoto out_bad;\n\n\t/* Set up the argument struct */\n\tnfs_init_commit(data, head, NULL, cinfo);\n\tatomic_inc(&cinfo->mds->rpcs_out);\n\treturn nfs_initiate_commit(NFS_CLIENT(inode), data, data->mds_ops,\n\t\t\t\t   how, 0);\n out_bad:\n\tnfs_retry_commit(head, NULL, cinfo);\n\tcinfo->completion_ops->error_cleanup(NFS_I(inode));\n\treturn -ENOMEM;\n}\n\n/*\n * COMMIT call returned\n */\nstatic void nfs_commit_done(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_commit_data\t*data = calldata;\n\n        dprintk(\"NFS: %5u nfs_commit_done (status %d)\\n\",\n                                task->tk_pid, task->tk_status);\n\n\t/* Call the NFS version-specific code */\n\tNFS_PROTO(data->inode)->commit_done(task, data);\n}\n\nstatic void nfs_commit_release_pages(struct nfs_commit_data *data)\n{\n\tstruct nfs_page\t*req;\n\tint status = data->task.tk_status;\n\tstruct nfs_commit_info cinfo;\n\n\twhile (!list_empty(&data->pages)) {\n\t\treq = nfs_list_entry(data->pages.next);\n\t\tnfs_list_remove_request(req);\n\t\tnfs_clear_page_commit(req->wb_page);\n\n\t\tdprintk(\"NFS:       commit (%s/%llu %d@%lld)\",\n\t\t\treq->wb_context->dentry->d_sb->s_id,\n\t\t\t(unsigned long long)NFS_FILEID(req->wb_context->dentry->d_inode),\n\t\t\treq->wb_bytes,\n\t\t\t(long long)req_offset(req));\n\t\tif (status < 0) {\n\t\t\tnfs_context_set_write_error(req->wb_context, status);\n\t\t\tnfs_inode_remove_request(req);\n\t\t\tdprintk(\", error = %d\\n\", status);\n\t\t\tgoto next;\n\t\t}\n\n\t\t/* Okay, COMMIT succeeded, apparently. Check the verifier\n\t\t * returned by the server against all stored verfs. */\n\t\tif (!memcmp(&req->wb_verf, &data->verf.verifier, sizeof(req->wb_verf))) {\n\t\t\t/* We have a match */\n\t\t\tnfs_inode_remove_request(req);\n\t\t\tdprintk(\" OK\\n\");\n\t\t\tgoto next;\n\t\t}\n\t\t/* We have a mismatch. Write the page again */\n\t\tdprintk(\" mismatch\\n\");\n\t\tnfs_mark_request_dirty(req);\n\t\tset_bit(NFS_CONTEXT_RESEND_WRITES, &req->wb_context->flags);\n\tnext:\n\t\tnfs_unlock_and_release_request(req);\n\t}\n\tnfs_init_cinfo(&cinfo, data->inode, data->dreq);\n\tif (atomic_dec_and_test(&cinfo.mds->rpcs_out))\n\t\tnfs_commit_clear_lock(NFS_I(data->inode));\n}\n\nstatic void nfs_commit_release(void *calldata)\n{\n\tstruct nfs_commit_data *data = calldata;\n\n\tdata->completion_ops->completion(data);\n\tnfs_commitdata_release(calldata);\n}\n\nstatic const struct rpc_call_ops nfs_commit_ops = {\n\t.rpc_call_prepare = nfs_commit_prepare,\n\t.rpc_call_done = nfs_commit_done,\n\t.rpc_release = nfs_commit_release,\n};\n\nstatic const struct nfs_commit_completion_ops nfs_commit_completion_ops = {\n\t.completion = nfs_commit_release_pages,\n\t.error_cleanup = nfs_commit_clear_lock,\n};\n\nint nfs_generic_commit_list(struct inode *inode, struct list_head *head,\n\t\t\t    int how, struct nfs_commit_info *cinfo)\n{\n\tint status;\n\n\tstatus = pnfs_commit_list(inode, head, how, cinfo);\n\tif (status == PNFS_NOT_ATTEMPTED)\n\t\tstatus = nfs_commit_list(inode, head, how, cinfo);\n\treturn status;\n}\n\nint nfs_commit_inode(struct inode *inode, int how)\n{\n\tLIST_HEAD(head);\n\tstruct nfs_commit_info cinfo;\n\tint may_wait = how & FLUSH_SYNC;\n\tint res;\n\n\tres = nfs_commit_set_lock(NFS_I(inode), may_wait);\n\tif (res <= 0)\n\t\tgoto out_mark_dirty;\n\tnfs_init_cinfo_from_inode(&cinfo, inode);\n\tres = nfs_scan_commit(inode, &head, &cinfo);\n\tif (res) {\n\t\tint error;\n\n\t\terror = nfs_generic_commit_list(inode, &head, how, &cinfo);\n\t\tif (error < 0)\n\t\t\treturn error;\n\t\tif (!may_wait)\n\t\t\tgoto out_mark_dirty;\n\t\terror = wait_on_bit(&NFS_I(inode)->flags,\n\t\t\t\tNFS_INO_COMMIT,\n\t\t\t\tnfs_wait_bit_killable,\n\t\t\t\tTASK_KILLABLE);\n\t\tif (error < 0)\n\t\t\treturn error;\n\t} else\n\t\tnfs_commit_clear_lock(NFS_I(inode));\n\treturn res;\n\t/* Note: If we exit without ensuring that the commit is complete,\n\t * we must mark the inode as dirty. Otherwise, future calls to\n\t * sync_inode() with the WB_SYNC_ALL flag set will fail to ensure\n\t * that the data is on the disk.\n\t */\nout_mark_dirty:\n\t__mark_inode_dirty(inode, I_DIRTY_DATASYNC);\n\treturn res;\n}\n\nstatic int nfs_commit_unstable_pages(struct inode *inode, struct writeback_control *wbc)\n{\n\tstruct nfs_inode *nfsi = NFS_I(inode);\n\tint flags = FLUSH_SYNC;\n\tint ret = 0;\n\n\t/* no commits means nothing needs to be done */\n\tif (!nfsi->commit_info.ncommit)\n\t\treturn ret;\n\n\tif (wbc->sync_mode == WB_SYNC_NONE) {\n\t\t/* Don't commit yet if this is a non-blocking flush and there\n\t\t * are a lot of outstanding writes for this mapping.\n\t\t */\n\t\tif (nfsi->commit_info.ncommit <= (nfsi->npages >> 1))\n\t\t\tgoto out_mark_dirty;\n\n\t\t/* don't wait for the COMMIT response */\n\t\tflags = 0;\n\t}\n\n\tret = nfs_commit_inode(inode, flags);\n\tif (ret >= 0) {\n\t\tif (wbc->sync_mode == WB_SYNC_NONE) {\n\t\t\tif (ret < wbc->nr_to_write)\n\t\t\t\twbc->nr_to_write -= ret;\n\t\t\telse\n\t\t\t\twbc->nr_to_write = 0;\n\t\t}\n\t\treturn 0;\n\t}\nout_mark_dirty:\n\t__mark_inode_dirty(inode, I_DIRTY_DATASYNC);\n\treturn ret;\n}\n#else\nstatic int nfs_commit_unstable_pages(struct inode *inode, struct writeback_control *wbc)\n{\n\treturn 0;\n}\n#endif\n\nint nfs_write_inode(struct inode *inode, struct writeback_control *wbc)\n{\n\treturn nfs_commit_unstable_pages(inode, wbc);\n}\nEXPORT_SYMBOL_GPL(nfs_write_inode);\n\n/*\n * flush the inode to disk.\n */\nint nfs_wb_all(struct inode *inode)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_ALL,\n\t\t.nr_to_write = LONG_MAX,\n\t\t.range_start = 0,\n\t\t.range_end = LLONG_MAX,\n\t};\n\tint ret;\n\n\ttrace_nfs_writeback_inode_enter(inode);\n\n\tret = sync_inode(inode, &wbc);\n\n\ttrace_nfs_writeback_inode_exit(inode, ret);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nfs_wb_all);\n\nint nfs_wb_page_cancel(struct inode *inode, struct page *page)\n{\n\tstruct nfs_page *req;\n\tint ret = 0;\n\n\tfor (;;) {\n\t\twait_on_page_writeback(page);\n\t\treq = nfs_page_find_request(page);\n\t\tif (req == NULL)\n\t\t\tbreak;\n\t\tif (nfs_lock_request(req)) {\n\t\t\tnfs_clear_request_commit(req);\n\t\t\tnfs_inode_remove_request(req);\n\t\t\t/*\n\t\t\t * In case nfs_inode_remove_request has marked the\n\t\t\t * page as being dirty\n\t\t\t */\n\t\t\tcancel_dirty_page(page, PAGE_CACHE_SIZE);\n\t\t\tnfs_unlock_and_release_request(req);\n\t\t\tbreak;\n\t\t}\n\t\tret = nfs_wait_on_request(req);\n\t\tnfs_release_request(req);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/*\n * Write back all requests on one page - we do this before reading it.\n */\nint nfs_wb_page(struct inode *inode, struct page *page)\n{\n\tloff_t range_start = page_file_offset(page);\n\tloff_t range_end = range_start + (loff_t)(PAGE_CACHE_SIZE - 1);\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_ALL,\n\t\t.nr_to_write = 0,\n\t\t.range_start = range_start,\n\t\t.range_end = range_end,\n\t};\n\tint ret;\n\n\ttrace_nfs_writeback_page_enter(inode);\n\n\tfor (;;) {\n\t\twait_on_page_writeback(page);\n\t\tif (clear_page_dirty_for_io(page)) {\n\t\t\tret = nfs_writepage_locked(page, &wbc);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out_error;\n\t\t\tcontinue;\n\t\t}\n\t\tret = 0;\n\t\tif (!PagePrivate(page))\n\t\t\tbreak;\n\t\tret = nfs_commit_inode(inode, FLUSH_SYNC);\n\t\tif (ret < 0)\n\t\t\tgoto out_error;\n\t}\nout_error:\n\ttrace_nfs_writeback_page_exit(inode, ret);\n\treturn ret;\n}\n\n#ifdef CONFIG_MIGRATION\nint nfs_migrate_page(struct address_space *mapping, struct page *newpage,\n\t\tstruct page *page, enum migrate_mode mode)\n{\n\t/*\n\t * If PagePrivate is set, then the page is currently associated with\n\t * an in-progress read or write request. Don't try to migrate it.\n\t *\n\t * FIXME: we could do this in principle, but we'll need a way to ensure\n\t *        that we can safely release the inode reference while holding\n\t *        the page lock.\n\t */\n\tif (PagePrivate(page))\n\t\treturn -EBUSY;\n\n\tif (!nfs_fscache_release_page(page, GFP_KERNEL))\n\t\treturn -EBUSY;\n\n\treturn migrate_page(mapping, newpage, page, mode);\n}\n#endif\n\nint __init nfs_init_writepagecache(void)\n{\n\tnfs_wdata_cachep = kmem_cache_create(\"nfs_write_data\",\n\t\t\t\t\t     sizeof(struct nfs_write_header),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (nfs_wdata_cachep == NULL)\n\t\treturn -ENOMEM;\n\n\tnfs_wdata_mempool = mempool_create_slab_pool(MIN_POOL_WRITE,\n\t\t\t\t\t\t     nfs_wdata_cachep);\n\tif (nfs_wdata_mempool == NULL)\n\t\tgoto out_destroy_write_cache;\n\n\tnfs_cdata_cachep = kmem_cache_create(\"nfs_commit_data\",\n\t\t\t\t\t     sizeof(struct nfs_commit_data),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (nfs_cdata_cachep == NULL)\n\t\tgoto out_destroy_write_mempool;\n\n\tnfs_commit_mempool = mempool_create_slab_pool(MIN_POOL_COMMIT,\n\t\t\t\t\t\t      nfs_cdata_cachep);\n\tif (nfs_commit_mempool == NULL)\n\t\tgoto out_destroy_commit_cache;\n\n\t/*\n\t * NFS congestion size, scale with available memory.\n\t *\n\t *  64MB:    8192k\n\t * 128MB:   11585k\n\t * 256MB:   16384k\n\t * 512MB:   23170k\n\t *   1GB:   32768k\n\t *   2GB:   46340k\n\t *   4GB:   65536k\n\t *   8GB:   92681k\n\t *  16GB:  131072k\n\t *\n\t * This allows larger machines to have larger/more transfers.\n\t * Limit the default to 256M\n\t */\n\tnfs_congestion_kb = (16*int_sqrt(totalram_pages)) << (PAGE_SHIFT-10);\n\tif (nfs_congestion_kb > 256*1024)\n\t\tnfs_congestion_kb = 256*1024;\n\n\treturn 0;\n\nout_destroy_commit_cache:\n\tkmem_cache_destroy(nfs_cdata_cachep);\nout_destroy_write_mempool:\n\tmempool_destroy(nfs_wdata_mempool);\nout_destroy_write_cache:\n\tkmem_cache_destroy(nfs_wdata_cachep);\n\treturn -ENOMEM;\n}\n\nvoid nfs_destroy_writepagecache(void)\n{\n\tmempool_destroy(nfs_commit_mempool);\n\tkmem_cache_destroy(nfs_cdata_cachep);\n\tmempool_destroy(nfs_wdata_mempool);\n\tkmem_cache_destroy(nfs_wdata_cachep);\n}\n\n"], "fixing_code": ["/*\n * linux/fs/nfs/write.c\n *\n * Write file data over NFS.\n *\n * Copyright (C) 1996, 1997, Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/writeback.h>\n#include <linux/swap.h>\n#include <linux/migrate.h>\n\n#include <linux/sunrpc/clnt.h>\n#include <linux/nfs_fs.h>\n#include <linux/nfs_mount.h>\n#include <linux/nfs_page.h>\n#include <linux/backing-dev.h>\n#include <linux/export.h>\n\n#include <asm/uaccess.h>\n\n#include \"delegation.h\"\n#include \"internal.h\"\n#include \"iostat.h\"\n#include \"nfs4_fs.h\"\n#include \"fscache.h\"\n#include \"pnfs.h\"\n\n#include \"nfstrace.h\"\n\n#define NFSDBG_FACILITY\t\tNFSDBG_PAGECACHE\n\n#define MIN_POOL_WRITE\t\t(32)\n#define MIN_POOL_COMMIT\t\t(4)\n\n/*\n * Local function declarations\n */\nstatic void nfs_redirty_request(struct nfs_page *req);\nstatic const struct rpc_call_ops nfs_write_common_ops;\nstatic const struct rpc_call_ops nfs_commit_ops;\nstatic const struct nfs_pgio_completion_ops nfs_async_write_completion_ops;\nstatic const struct nfs_commit_completion_ops nfs_commit_completion_ops;\n\nstatic struct kmem_cache *nfs_wdata_cachep;\nstatic mempool_t *nfs_wdata_mempool;\nstatic struct kmem_cache *nfs_cdata_cachep;\nstatic mempool_t *nfs_commit_mempool;\n\nstruct nfs_commit_data *nfs_commitdata_alloc(void)\n{\n\tstruct nfs_commit_data *p = mempool_alloc(nfs_commit_mempool, GFP_NOIO);\n\n\tif (p) {\n\t\tmemset(p, 0, sizeof(*p));\n\t\tINIT_LIST_HEAD(&p->pages);\n\t}\n\treturn p;\n}\nEXPORT_SYMBOL_GPL(nfs_commitdata_alloc);\n\nvoid nfs_commit_free(struct nfs_commit_data *p)\n{\n\tmempool_free(p, nfs_commit_mempool);\n}\nEXPORT_SYMBOL_GPL(nfs_commit_free);\n\nstruct nfs_write_header *nfs_writehdr_alloc(void)\n{\n\tstruct nfs_write_header *p = mempool_alloc(nfs_wdata_mempool, GFP_NOIO);\n\n\tif (p) {\n\t\tstruct nfs_pgio_header *hdr = &p->header;\n\n\t\tmemset(p, 0, sizeof(*p));\n\t\tINIT_LIST_HEAD(&hdr->pages);\n\t\tINIT_LIST_HEAD(&hdr->rpc_list);\n\t\tspin_lock_init(&hdr->lock);\n\t\tatomic_set(&hdr->refcnt, 0);\n\t\thdr->verf = &p->verf;\n\t}\n\treturn p;\n}\nEXPORT_SYMBOL_GPL(nfs_writehdr_alloc);\n\nstatic struct nfs_write_data *nfs_writedata_alloc(struct nfs_pgio_header *hdr,\n\t\t\t\t\t\t  unsigned int pagecount)\n{\n\tstruct nfs_write_data *data, *prealloc;\n\n\tprealloc = &container_of(hdr, struct nfs_write_header, header)->rpc_data;\n\tif (prealloc->header == NULL)\n\t\tdata = prealloc;\n\telse\n\t\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\tgoto out;\n\n\tif (nfs_pgarray_set(&data->pages, pagecount)) {\n\t\tdata->header = hdr;\n\t\tatomic_inc(&hdr->refcnt);\n\t} else {\n\t\tif (data != prealloc)\n\t\t\tkfree(data);\n\t\tdata = NULL;\n\t}\nout:\n\treturn data;\n}\n\nvoid nfs_writehdr_free(struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_write_header *whdr = container_of(hdr, struct nfs_write_header, header);\n\tmempool_free(whdr, nfs_wdata_mempool);\n}\nEXPORT_SYMBOL_GPL(nfs_writehdr_free);\n\nvoid nfs_writedata_release(struct nfs_write_data *wdata)\n{\n\tstruct nfs_pgio_header *hdr = wdata->header;\n\tstruct nfs_write_header *write_header = container_of(hdr, struct nfs_write_header, header);\n\n\tput_nfs_open_context(wdata->args.context);\n\tif (wdata->pages.pagevec != wdata->pages.page_array)\n\t\tkfree(wdata->pages.pagevec);\n\tif (wdata == &write_header->rpc_data) {\n\t\twdata->header = NULL;\n\t\twdata = NULL;\n\t}\n\tif (atomic_dec_and_test(&hdr->refcnt))\n\t\thdr->completion_ops->completion(hdr);\n\t/* Note: we only free the rpc_task after callbacks are done.\n\t * See the comment in rpc_free_task() for why\n\t */\n\tkfree(wdata);\n}\nEXPORT_SYMBOL_GPL(nfs_writedata_release);\n\nstatic void nfs_context_set_write_error(struct nfs_open_context *ctx, int error)\n{\n\tctx->error = error;\n\tsmp_wmb();\n\tset_bit(NFS_CONTEXT_ERROR_WRITE, &ctx->flags);\n}\n\nstatic struct nfs_page *\nnfs_page_find_request_locked(struct nfs_inode *nfsi, struct page *page)\n{\n\tstruct nfs_page *req = NULL;\n\n\tif (PagePrivate(page))\n\t\treq = (struct nfs_page *)page_private(page);\n\telse if (unlikely(PageSwapCache(page))) {\n\t\tstruct nfs_page *freq, *t;\n\n\t\t/* Linearly search the commit list for the correct req */\n\t\tlist_for_each_entry_safe(freq, t, &nfsi->commit_info.list, wb_list) {\n\t\t\tif (freq->wb_page == page) {\n\t\t\t\treq = freq;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (req)\n\t\tkref_get(&req->wb_kref);\n\n\treturn req;\n}\n\nstatic struct nfs_page *nfs_page_find_request(struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tstruct nfs_page *req = NULL;\n\n\tspin_lock(&inode->i_lock);\n\treq = nfs_page_find_request_locked(NFS_I(inode), page);\n\tspin_unlock(&inode->i_lock);\n\treturn req;\n}\n\n/* Adjust the file length if we're writing beyond the end */\nstatic void nfs_grow_file(struct page *page, unsigned int offset, unsigned int count)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tloff_t end, i_size;\n\tpgoff_t end_index;\n\n\tspin_lock(&inode->i_lock);\n\ti_size = i_size_read(inode);\n\tend_index = (i_size - 1) >> PAGE_CACHE_SHIFT;\n\tif (i_size > 0 && page_file_index(page) < end_index)\n\t\tgoto out;\n\tend = page_file_offset(page) + ((loff_t)offset+count);\n\tif (i_size >= end)\n\t\tgoto out;\n\ti_size_write(inode, end);\n\tnfs_inc_stats(inode, NFSIOS_EXTENDWRITE);\nout:\n\tspin_unlock(&inode->i_lock);\n}\n\n/* A writeback failed: mark the page as bad, and invalidate the page cache */\nstatic void nfs_set_pageerror(struct page *page)\n{\n\tnfs_zap_mapping(page_file_mapping(page)->host, page_file_mapping(page));\n}\n\n/* We can set the PG_uptodate flag if we see that a write request\n * covers the full page.\n */\nstatic void nfs_mark_uptodate(struct page *page, unsigned int base, unsigned int count)\n{\n\tif (PageUptodate(page))\n\t\treturn;\n\tif (base != 0)\n\t\treturn;\n\tif (count != nfs_page_length(page))\n\t\treturn;\n\tSetPageUptodate(page);\n}\n\nstatic int wb_priority(struct writeback_control *wbc)\n{\n\tif (wbc->for_reclaim)\n\t\treturn FLUSH_HIGHPRI | FLUSH_STABLE;\n\tif (wbc->for_kupdate || wbc->for_background)\n\t\treturn FLUSH_LOWPRI | FLUSH_COND_STABLE;\n\treturn FLUSH_COND_STABLE;\n}\n\n/*\n * NFS congestion control\n */\n\nint nfs_congestion_kb;\n\n#define NFS_CONGESTION_ON_THRESH \t(nfs_congestion_kb >> (PAGE_SHIFT-10))\n#define NFS_CONGESTION_OFF_THRESH\t\\\n\t(NFS_CONGESTION_ON_THRESH - (NFS_CONGESTION_ON_THRESH >> 2))\n\nstatic void nfs_set_page_writeback(struct page *page)\n{\n\tstruct nfs_server *nfss = NFS_SERVER(page_file_mapping(page)->host);\n\tint ret = test_set_page_writeback(page);\n\n\tWARN_ON_ONCE(ret != 0);\n\n\tif (atomic_long_inc_return(&nfss->writeback) >\n\t\t\tNFS_CONGESTION_ON_THRESH) {\n\t\tset_bdi_congested(&nfss->backing_dev_info,\n\t\t\t\t\tBLK_RW_ASYNC);\n\t}\n}\n\nstatic void nfs_end_page_writeback(struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tstruct nfs_server *nfss = NFS_SERVER(inode);\n\n\tend_page_writeback(page);\n\tif (atomic_long_dec_return(&nfss->writeback) < NFS_CONGESTION_OFF_THRESH)\n\t\tclear_bdi_congested(&nfss->backing_dev_info, BLK_RW_ASYNC);\n}\n\nstatic struct nfs_page *nfs_find_and_lock_request(struct page *page, bool nonblock)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tstruct nfs_page *req;\n\tint ret;\n\n\tspin_lock(&inode->i_lock);\n\tfor (;;) {\n\t\treq = nfs_page_find_request_locked(NFS_I(inode), page);\n\t\tif (req == NULL)\n\t\t\tbreak;\n\t\tif (nfs_lock_request(req))\n\t\t\tbreak;\n\t\t/* Note: If we hold the page lock, as is the case in nfs_writepage,\n\t\t *\t then the call to nfs_lock_request() will always\n\t\t *\t succeed provided that someone hasn't already marked the\n\t\t *\t request as dirty (in which case we don't care).\n\t\t */\n\t\tspin_unlock(&inode->i_lock);\n\t\tif (!nonblock)\n\t\t\tret = nfs_wait_on_request(req);\n\t\telse\n\t\t\tret = -EAGAIN;\n\t\tnfs_release_request(req);\n\t\tif (ret != 0)\n\t\t\treturn ERR_PTR(ret);\n\t\tspin_lock(&inode->i_lock);\n\t}\n\tspin_unlock(&inode->i_lock);\n\treturn req;\n}\n\n/*\n * Find an associated nfs write request, and prepare to flush it out\n * May return an error if the user signalled nfs_wait_on_request().\n */\nstatic int nfs_page_async_flush(struct nfs_pageio_descriptor *pgio,\n\t\t\t\tstruct page *page, bool nonblock)\n{\n\tstruct nfs_page *req;\n\tint ret = 0;\n\n\treq = nfs_find_and_lock_request(page, nonblock);\n\tif (!req)\n\t\tgoto out;\n\tret = PTR_ERR(req);\n\tif (IS_ERR(req))\n\t\tgoto out;\n\n\tnfs_set_page_writeback(page);\n\tWARN_ON_ONCE(test_bit(PG_CLEAN, &req->wb_flags));\n\n\tret = 0;\n\tif (!nfs_pageio_add_request(pgio, req)) {\n\t\tnfs_redirty_request(req);\n\t\tret = pgio->pg_error;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int nfs_do_writepage(struct page *page, struct writeback_control *wbc, struct nfs_pageio_descriptor *pgio)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tint ret;\n\n\tnfs_inc_stats(inode, NFSIOS_VFSWRITEPAGE);\n\tnfs_add_stats(inode, NFSIOS_WRITEPAGES, 1);\n\n\tnfs_pageio_cond_complete(pgio, page_file_index(page));\n\tret = nfs_page_async_flush(pgio, page, wbc->sync_mode == WB_SYNC_NONE);\n\tif (ret == -EAGAIN) {\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\n/*\n * Write an mmapped page to the server.\n */\nstatic int nfs_writepage_locked(struct page *page, struct writeback_control *wbc)\n{\n\tstruct nfs_pageio_descriptor pgio;\n\tint err;\n\n\tNFS_PROTO(page_file_mapping(page)->host)->write_pageio_init(&pgio,\n\t\t\t\t\t\t\t  page->mapping->host,\n\t\t\t\t\t\t\t  wb_priority(wbc),\n\t\t\t\t\t\t\t  &nfs_async_write_completion_ops);\n\terr = nfs_do_writepage(page, wbc, &pgio);\n\tnfs_pageio_complete(&pgio);\n\tif (err < 0)\n\t\treturn err;\n\tif (pgio.pg_error < 0)\n\t\treturn pgio.pg_error;\n\treturn 0;\n}\n\nint nfs_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tint ret;\n\n\tret = nfs_writepage_locked(page, wbc);\n\tunlock_page(page);\n\treturn ret;\n}\n\nstatic int nfs_writepages_callback(struct page *page, struct writeback_control *wbc, void *data)\n{\n\tint ret;\n\n\tret = nfs_do_writepage(page, wbc, data);\n\tunlock_page(page);\n\treturn ret;\n}\n\nint nfs_writepages(struct address_space *mapping, struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\tunsigned long *bitlock = &NFS_I(inode)->flags;\n\tstruct nfs_pageio_descriptor pgio;\n\tint err;\n\n\t/* Stop dirtying of new pages while we sync */\n\terr = wait_on_bit_lock(bitlock, NFS_INO_FLUSHING,\n\t\t\tnfs_wait_bit_killable, TASK_KILLABLE);\n\tif (err)\n\t\tgoto out_err;\n\n\tnfs_inc_stats(inode, NFSIOS_VFSWRITEPAGES);\n\n\tNFS_PROTO(inode)->write_pageio_init(&pgio, inode, wb_priority(wbc), &nfs_async_write_completion_ops);\n\terr = write_cache_pages(mapping, wbc, nfs_writepages_callback, &pgio);\n\tnfs_pageio_complete(&pgio);\n\n\tclear_bit_unlock(NFS_INO_FLUSHING, bitlock);\n\tsmp_mb__after_clear_bit();\n\twake_up_bit(bitlock, NFS_INO_FLUSHING);\n\n\tif (err < 0)\n\t\tgoto out_err;\n\terr = pgio.pg_error;\n\tif (err < 0)\n\t\tgoto out_err;\n\treturn 0;\nout_err:\n\treturn err;\n}\n\n/*\n * Insert a write request into an inode\n */\nstatic void nfs_inode_add_request(struct inode *inode, struct nfs_page *req)\n{\n\tstruct nfs_inode *nfsi = NFS_I(inode);\n\n\t/* Lock the request! */\n\tnfs_lock_request(req);\n\n\tspin_lock(&inode->i_lock);\n\tif (!nfsi->npages && NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))\n\t\tinode->i_version++;\n\t/*\n\t * Swap-space should not get truncated. Hence no need to plug the race\n\t * with invalidate/truncate.\n\t */\n\tif (likely(!PageSwapCache(req->wb_page))) {\n\t\tset_bit(PG_MAPPED, &req->wb_flags);\n\t\tSetPagePrivate(req->wb_page);\n\t\tset_page_private(req->wb_page, (unsigned long)req);\n\t}\n\tnfsi->npages++;\n\tkref_get(&req->wb_kref);\n\tspin_unlock(&inode->i_lock);\n}\n\n/*\n * Remove a write request from an inode\n */\nstatic void nfs_inode_remove_request(struct nfs_page *req)\n{\n\tstruct inode *inode = req->wb_context->dentry->d_inode;\n\tstruct nfs_inode *nfsi = NFS_I(inode);\n\n\tspin_lock(&inode->i_lock);\n\tif (likely(!PageSwapCache(req->wb_page))) {\n\t\tset_page_private(req->wb_page, 0);\n\t\tClearPagePrivate(req->wb_page);\n\t\tclear_bit(PG_MAPPED, &req->wb_flags);\n\t}\n\tnfsi->npages--;\n\tspin_unlock(&inode->i_lock);\n\tnfs_release_request(req);\n}\n\nstatic void\nnfs_mark_request_dirty(struct nfs_page *req)\n{\n\t__set_page_dirty_nobuffers(req->wb_page);\n}\n\n#if IS_ENABLED(CONFIG_NFS_V3) || IS_ENABLED(CONFIG_NFS_V4)\n/**\n * nfs_request_add_commit_list - add request to a commit list\n * @req: pointer to a struct nfs_page\n * @dst: commit list head\n * @cinfo: holds list lock and accounting info\n *\n * This sets the PG_CLEAN bit, updates the cinfo count of\n * number of outstanding requests requiring a commit as well as\n * the MM page stats.\n *\n * The caller must _not_ hold the cinfo->lock, but must be\n * holding the nfs_page lock.\n */\nvoid\nnfs_request_add_commit_list(struct nfs_page *req, struct list_head *dst,\n\t\t\t    struct nfs_commit_info *cinfo)\n{\n\tset_bit(PG_CLEAN, &(req)->wb_flags);\n\tspin_lock(cinfo->lock);\n\tnfs_list_add_request(req, dst);\n\tcinfo->mds->ncommit++;\n\tspin_unlock(cinfo->lock);\n\tif (!cinfo->dreq) {\n\t\tinc_zone_page_state(req->wb_page, NR_UNSTABLE_NFS);\n\t\tinc_bdi_stat(page_file_mapping(req->wb_page)->backing_dev_info,\n\t\t\t     BDI_RECLAIMABLE);\n\t\t__mark_inode_dirty(req->wb_context->dentry->d_inode,\n\t\t\t\t   I_DIRTY_DATASYNC);\n\t}\n}\nEXPORT_SYMBOL_GPL(nfs_request_add_commit_list);\n\n/**\n * nfs_request_remove_commit_list - Remove request from a commit list\n * @req: pointer to a nfs_page\n * @cinfo: holds list lock and accounting info\n *\n * This clears the PG_CLEAN bit, and updates the cinfo's count of\n * number of outstanding requests requiring a commit\n * It does not update the MM page stats.\n *\n * The caller _must_ hold the cinfo->lock and the nfs_page lock.\n */\nvoid\nnfs_request_remove_commit_list(struct nfs_page *req,\n\t\t\t       struct nfs_commit_info *cinfo)\n{\n\tif (!test_and_clear_bit(PG_CLEAN, &(req)->wb_flags))\n\t\treturn;\n\tnfs_list_remove_request(req);\n\tcinfo->mds->ncommit--;\n}\nEXPORT_SYMBOL_GPL(nfs_request_remove_commit_list);\n\nstatic void nfs_init_cinfo_from_inode(struct nfs_commit_info *cinfo,\n\t\t\t\t      struct inode *inode)\n{\n\tcinfo->lock = &inode->i_lock;\n\tcinfo->mds = &NFS_I(inode)->commit_info;\n\tcinfo->ds = pnfs_get_ds_info(inode);\n\tcinfo->dreq = NULL;\n\tcinfo->completion_ops = &nfs_commit_completion_ops;\n}\n\nvoid nfs_init_cinfo(struct nfs_commit_info *cinfo,\n\t\t    struct inode *inode,\n\t\t    struct nfs_direct_req *dreq)\n{\n\tif (dreq)\n\t\tnfs_init_cinfo_from_dreq(cinfo, dreq);\n\telse\n\t\tnfs_init_cinfo_from_inode(cinfo, inode);\n}\nEXPORT_SYMBOL_GPL(nfs_init_cinfo);\n\n/*\n * Add a request to the inode's commit list.\n */\nvoid\nnfs_mark_request_commit(struct nfs_page *req, struct pnfs_layout_segment *lseg,\n\t\t\tstruct nfs_commit_info *cinfo)\n{\n\tif (pnfs_mark_request_commit(req, lseg, cinfo))\n\t\treturn;\n\tnfs_request_add_commit_list(req, &cinfo->mds->list, cinfo);\n}\n\nstatic void\nnfs_clear_page_commit(struct page *page)\n{\n\tdec_zone_page_state(page, NR_UNSTABLE_NFS);\n\tdec_bdi_stat(page_file_mapping(page)->backing_dev_info, BDI_RECLAIMABLE);\n}\n\nstatic void\nnfs_clear_request_commit(struct nfs_page *req)\n{\n\tif (test_bit(PG_CLEAN, &req->wb_flags)) {\n\t\tstruct inode *inode = req->wb_context->dentry->d_inode;\n\t\tstruct nfs_commit_info cinfo;\n\n\t\tnfs_init_cinfo_from_inode(&cinfo, inode);\n\t\tif (!pnfs_clear_request_commit(req, &cinfo)) {\n\t\t\tspin_lock(cinfo.lock);\n\t\t\tnfs_request_remove_commit_list(req, &cinfo);\n\t\t\tspin_unlock(cinfo.lock);\n\t\t}\n\t\tnfs_clear_page_commit(req->wb_page);\n\t}\n}\n\nstatic inline\nint nfs_write_need_commit(struct nfs_write_data *data)\n{\n\tif (data->verf.committed == NFS_DATA_SYNC)\n\t\treturn data->header->lseg == NULL;\n\treturn data->verf.committed != NFS_FILE_SYNC;\n}\n\n#else\nstatic void nfs_init_cinfo_from_inode(struct nfs_commit_info *cinfo,\n\t\t\t\t      struct inode *inode)\n{\n}\n\nvoid nfs_init_cinfo(struct nfs_commit_info *cinfo,\n\t\t    struct inode *inode,\n\t\t    struct nfs_direct_req *dreq)\n{\n}\n\nvoid\nnfs_mark_request_commit(struct nfs_page *req, struct pnfs_layout_segment *lseg,\n\t\t\tstruct nfs_commit_info *cinfo)\n{\n}\n\nstatic void\nnfs_clear_request_commit(struct nfs_page *req)\n{\n}\n\nstatic inline\nint nfs_write_need_commit(struct nfs_write_data *data)\n{\n\treturn 0;\n}\n\n#endif\n\nstatic void nfs_write_completion(struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_commit_info cinfo;\n\tunsigned long bytes = 0;\n\n\tif (test_bit(NFS_IOHDR_REDO, &hdr->flags))\n\t\tgoto out;\n\tnfs_init_cinfo_from_inode(&cinfo, hdr->inode);\n\twhile (!list_empty(&hdr->pages)) {\n\t\tstruct nfs_page *req = nfs_list_entry(hdr->pages.next);\n\n\t\tbytes += req->wb_bytes;\n\t\tnfs_list_remove_request(req);\n\t\tif (test_bit(NFS_IOHDR_ERROR, &hdr->flags) &&\n\t\t    (hdr->good_bytes < bytes)) {\n\t\t\tnfs_set_pageerror(req->wb_page);\n\t\t\tnfs_context_set_write_error(req->wb_context, hdr->error);\n\t\t\tgoto remove_req;\n\t\t}\n\t\tif (test_bit(NFS_IOHDR_NEED_RESCHED, &hdr->flags)) {\n\t\t\tnfs_mark_request_dirty(req);\n\t\t\tgoto next;\n\t\t}\n\t\tif (test_bit(NFS_IOHDR_NEED_COMMIT, &hdr->flags)) {\n\t\t\tmemcpy(&req->wb_verf, &hdr->verf->verifier, sizeof(req->wb_verf));\n\t\t\tnfs_mark_request_commit(req, hdr->lseg, &cinfo);\n\t\t\tgoto next;\n\t\t}\nremove_req:\n\t\tnfs_inode_remove_request(req);\nnext:\n\t\tnfs_unlock_request(req);\n\t\tnfs_end_page_writeback(req->wb_page);\n\t\tnfs_release_request(req);\n\t}\nout:\n\thdr->release(hdr);\n}\n\n#if  IS_ENABLED(CONFIG_NFS_V3) || IS_ENABLED(CONFIG_NFS_V4)\nstatic unsigned long\nnfs_reqs_to_commit(struct nfs_commit_info *cinfo)\n{\n\treturn cinfo->mds->ncommit;\n}\n\n/* cinfo->lock held by caller */\nint\nnfs_scan_commit_list(struct list_head *src, struct list_head *dst,\n\t\t     struct nfs_commit_info *cinfo, int max)\n{\n\tstruct nfs_page *req, *tmp;\n\tint ret = 0;\n\n\tlist_for_each_entry_safe(req, tmp, src, wb_list) {\n\t\tif (!nfs_lock_request(req))\n\t\t\tcontinue;\n\t\tkref_get(&req->wb_kref);\n\t\tif (cond_resched_lock(cinfo->lock))\n\t\t\tlist_safe_reset_next(req, tmp, wb_list);\n\t\tnfs_request_remove_commit_list(req, cinfo);\n\t\tnfs_list_add_request(req, dst);\n\t\tret++;\n\t\tif ((ret == max) && !cinfo->dreq)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/*\n * nfs_scan_commit - Scan an inode for commit requests\n * @inode: NFS inode to scan\n * @dst: mds destination list\n * @cinfo: mds and ds lists of reqs ready to commit\n *\n * Moves requests from the inode's 'commit' request list.\n * The requests are *not* checked to ensure that they form a contiguous set.\n */\nint\nnfs_scan_commit(struct inode *inode, struct list_head *dst,\n\t\tstruct nfs_commit_info *cinfo)\n{\n\tint ret = 0;\n\n\tspin_lock(cinfo->lock);\n\tif (cinfo->mds->ncommit > 0) {\n\t\tconst int max = INT_MAX;\n\n\t\tret = nfs_scan_commit_list(&cinfo->mds->list, dst,\n\t\t\t\t\t   cinfo, max);\n\t\tret += pnfs_scan_commit_lists(inode, cinfo, max - ret);\n\t}\n\tspin_unlock(cinfo->lock);\n\treturn ret;\n}\n\n#else\nstatic unsigned long nfs_reqs_to_commit(struct nfs_commit_info *cinfo)\n{\n\treturn 0;\n}\n\nint nfs_scan_commit(struct inode *inode, struct list_head *dst,\n\t\t    struct nfs_commit_info *cinfo)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Search for an existing write request, and attempt to update\n * it to reflect a new dirty region on a given page.\n *\n * If the attempt fails, then the existing request is flushed out\n * to disk.\n */\nstatic struct nfs_page *nfs_try_to_update_request(struct inode *inode,\n\t\tstruct page *page,\n\t\tunsigned int offset,\n\t\tunsigned int bytes)\n{\n\tstruct nfs_page *req;\n\tunsigned int rqend;\n\tunsigned int end;\n\tint error;\n\n\tif (!PagePrivate(page))\n\t\treturn NULL;\n\n\tend = offset + bytes;\n\tspin_lock(&inode->i_lock);\n\n\tfor (;;) {\n\t\treq = nfs_page_find_request_locked(NFS_I(inode), page);\n\t\tif (req == NULL)\n\t\t\tgoto out_unlock;\n\n\t\trqend = req->wb_offset + req->wb_bytes;\n\t\t/*\n\t\t * Tell the caller to flush out the request if\n\t\t * the offsets are non-contiguous.\n\t\t * Note: nfs_flush_incompatible() will already\n\t\t * have flushed out requests having wrong owners.\n\t\t */\n\t\tif (offset > rqend\n\t\t    || end < req->wb_offset)\n\t\t\tgoto out_flushme;\n\n\t\tif (nfs_lock_request(req))\n\t\t\tbreak;\n\n\t\t/* The request is locked, so wait and then retry */\n\t\tspin_unlock(&inode->i_lock);\n\t\terror = nfs_wait_on_request(req);\n\t\tnfs_release_request(req);\n\t\tif (error != 0)\n\t\t\tgoto out_err;\n\t\tspin_lock(&inode->i_lock);\n\t}\n\n\t/* Okay, the request matches. Update the region */\n\tif (offset < req->wb_offset) {\n\t\treq->wb_offset = offset;\n\t\treq->wb_pgbase = offset;\n\t}\n\tif (end > rqend)\n\t\treq->wb_bytes = end - req->wb_offset;\n\telse\n\t\treq->wb_bytes = rqend - req->wb_offset;\nout_unlock:\n\tspin_unlock(&inode->i_lock);\n\tif (req)\n\t\tnfs_clear_request_commit(req);\n\treturn req;\nout_flushme:\n\tspin_unlock(&inode->i_lock);\n\tnfs_release_request(req);\n\terror = nfs_wb_page(inode, page);\nout_err:\n\treturn ERR_PTR(error);\n}\n\n/*\n * Try to update an existing write request, or create one if there is none.\n *\n * Note: Should always be called with the Page Lock held to prevent races\n * if we have to add a new request. Also assumes that the caller has\n * already called nfs_flush_incompatible() if necessary.\n */\nstatic struct nfs_page * nfs_setup_write_request(struct nfs_open_context* ctx,\n\t\tstruct page *page, unsigned int offset, unsigned int bytes)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tstruct nfs_page\t*req;\n\n\treq = nfs_try_to_update_request(inode, page, offset, bytes);\n\tif (req != NULL)\n\t\tgoto out;\n\treq = nfs_create_request(ctx, inode, page, offset, bytes);\n\tif (IS_ERR(req))\n\t\tgoto out;\n\tnfs_inode_add_request(inode, req);\nout:\n\treturn req;\n}\n\nstatic int nfs_writepage_setup(struct nfs_open_context *ctx, struct page *page,\n\t\tunsigned int offset, unsigned int count)\n{\n\tstruct nfs_page\t*req;\n\n\treq = nfs_setup_write_request(ctx, page, offset, count);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\t/* Update file length */\n\tnfs_grow_file(page, offset, count);\n\tnfs_mark_uptodate(page, req->wb_pgbase, req->wb_bytes);\n\tnfs_mark_request_dirty(req);\n\tnfs_unlock_and_release_request(req);\n\treturn 0;\n}\n\nint nfs_flush_incompatible(struct file *file, struct page *page)\n{\n\tstruct nfs_open_context *ctx = nfs_file_open_context(file);\n\tstruct nfs_lock_context *l_ctx;\n\tstruct nfs_page\t*req;\n\tint do_flush, status;\n\t/*\n\t * Look for a request corresponding to this page. If there\n\t * is one, and it belongs to another file, we flush it out\n\t * before we try to copy anything into the page. Do this\n\t * due to the lack of an ACCESS-type call in NFSv2.\n\t * Also do the same if we find a request from an existing\n\t * dropped page.\n\t */\n\tdo {\n\t\treq = nfs_page_find_request(page);\n\t\tif (req == NULL)\n\t\t\treturn 0;\n\t\tl_ctx = req->wb_lock_context;\n\t\tdo_flush = req->wb_page != page || req->wb_context != ctx;\n\t\tif (l_ctx && ctx->dentry->d_inode->i_flock != NULL) {\n\t\t\tdo_flush |= l_ctx->lockowner.l_owner != current->files\n\t\t\t\t|| l_ctx->lockowner.l_pid != current->tgid;\n\t\t}\n\t\tnfs_release_request(req);\n\t\tif (!do_flush)\n\t\t\treturn 0;\n\t\tstatus = nfs_wb_page(page_file_mapping(page)->host, page);\n\t} while (status == 0);\n\treturn status;\n}\n\n/*\n * Avoid buffered writes when a open context credential's key would\n * expire soon.\n *\n * Returns -EACCES if the key will expire within RPC_KEY_EXPIRE_FAIL.\n *\n * Return 0 and set a credential flag which triggers the inode to flush\n * and performs  NFS_FILE_SYNC writes if the key will expired within\n * RPC_KEY_EXPIRE_TIMEO.\n */\nint\nnfs_key_timeout_notify(struct file *filp, struct inode *inode)\n{\n\tstruct nfs_open_context *ctx = nfs_file_open_context(filp);\n\tstruct rpc_auth *auth = NFS_SERVER(inode)->client->cl_auth;\n\n\treturn rpcauth_key_timeout_notify(auth, ctx->cred);\n}\n\n/*\n * Test if the open context credential key is marked to expire soon.\n */\nbool nfs_ctx_key_to_expire(struct nfs_open_context *ctx)\n{\n\treturn rpcauth_cred_key_to_expire(ctx->cred);\n}\n\n/*\n * If the page cache is marked as unsafe or invalid, then we can't rely on\n * the PageUptodate() flag. In this case, we will need to turn off\n * write optimisations that depend on the page contents being correct.\n */\nstatic bool nfs_write_pageuptodate(struct page *page, struct inode *inode)\n{\n\tif (nfs_have_delegated_attributes(inode))\n\t\tgoto out;\n\tif (NFS_I(inode)->cache_validity & (NFS_INO_INVALID_DATA|NFS_INO_REVAL_PAGECACHE))\n\t\treturn false;\nout:\n\treturn PageUptodate(page) != 0;\n}\n\n/* If we know the page is up to date, and we're not using byte range locks (or\n * if we have the whole file locked for writing), it may be more efficient to\n * extend the write to cover the entire page in order to avoid fragmentation\n * inefficiencies.\n *\n * If the file is opened for synchronous writes then we can just skip the rest\n * of the checks.\n */\nstatic int nfs_can_extend_write(struct file *file, struct page *page, struct inode *inode)\n{\n\tif (file->f_flags & O_DSYNC)\n\t\treturn 0;\n\tif (!nfs_write_pageuptodate(page, inode))\n\t\treturn 0;\n\tif (NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))\n\t\treturn 1;\n\tif (inode->i_flock == NULL || (inode->i_flock->fl_start == 0 &&\n\t\t\tinode->i_flock->fl_end == OFFSET_MAX &&\n\t\t\tinode->i_flock->fl_type != F_RDLCK))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * Update and possibly write a cached page of an NFS file.\n *\n * XXX: Keep an eye on generic_file_read to make sure it doesn't do bad\n * things with a page scheduled for an RPC call (e.g. invalidate it).\n */\nint nfs_updatepage(struct file *file, struct page *page,\n\t\tunsigned int offset, unsigned int count)\n{\n\tstruct nfs_open_context *ctx = nfs_file_open_context(file);\n\tstruct inode\t*inode = page_file_mapping(page)->host;\n\tint\t\tstatus = 0;\n\n\tnfs_inc_stats(inode, NFSIOS_VFSUPDATEPAGE);\n\n\tdprintk(\"NFS:       nfs_updatepage(%pD2 %d@%lld)\\n\",\n\t\tfile, count, (long long)(page_file_offset(page) + offset));\n\n\tif (nfs_can_extend_write(file, page, inode)) {\n\t\tcount = max(count + offset, nfs_page_length(page));\n\t\toffset = 0;\n\t}\n\n\tstatus = nfs_writepage_setup(ctx, page, offset, count);\n\tif (status < 0)\n\t\tnfs_set_pageerror(page);\n\telse\n\t\t__set_page_dirty_nobuffers(page);\n\n\tdprintk(\"NFS:       nfs_updatepage returns %d (isize %lld)\\n\",\n\t\t\tstatus, (long long)i_size_read(inode));\n\treturn status;\n}\n\nstatic int flush_task_priority(int how)\n{\n\tswitch (how & (FLUSH_HIGHPRI|FLUSH_LOWPRI)) {\n\t\tcase FLUSH_HIGHPRI:\n\t\t\treturn RPC_PRIORITY_HIGH;\n\t\tcase FLUSH_LOWPRI:\n\t\t\treturn RPC_PRIORITY_LOW;\n\t}\n\treturn RPC_PRIORITY_NORMAL;\n}\n\nint nfs_initiate_write(struct rpc_clnt *clnt,\n\t\t       struct nfs_write_data *data,\n\t\t       const struct rpc_call_ops *call_ops,\n\t\t       int how, int flags)\n{\n\tstruct inode *inode = data->header->inode;\n\tint priority = flush_task_priority(how);\n\tstruct rpc_task *task;\n\tstruct rpc_message msg = {\n\t\t.rpc_argp = &data->args,\n\t\t.rpc_resp = &data->res,\n\t\t.rpc_cred = data->header->cred,\n\t};\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_client = clnt,\n\t\t.task = &data->task,\n\t\t.rpc_message = &msg,\n\t\t.callback_ops = call_ops,\n\t\t.callback_data = data,\n\t\t.workqueue = nfsiod_workqueue,\n\t\t.flags = RPC_TASK_ASYNC | flags,\n\t\t.priority = priority,\n\t};\n\tint ret = 0;\n\n\t/* Set up the initial task struct.  */\n\tNFS_PROTO(inode)->write_setup(data, &msg);\n\n\tdprintk(\"NFS: %5u initiated write call \"\n\t\t\"(req %s/%llu, %u bytes @ offset %llu)\\n\",\n\t\tdata->task.tk_pid,\n\t\tinode->i_sb->s_id,\n\t\t(unsigned long long)NFS_FILEID(inode),\n\t\tdata->args.count,\n\t\t(unsigned long long)data->args.offset);\n\n\tnfs4_state_protect_write(NFS_SERVER(inode)->nfs_client,\n\t\t\t\t &task_setup_data.rpc_client, &msg, data);\n\n\ttask = rpc_run_task(&task_setup_data);\n\tif (IS_ERR(task)) {\n\t\tret = PTR_ERR(task);\n\t\tgoto out;\n\t}\n\tif (how & FLUSH_SYNC) {\n\t\tret = rpc_wait_for_completion_task(task);\n\t\tif (ret == 0)\n\t\t\tret = task->tk_status;\n\t}\n\trpc_put_task(task);\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nfs_initiate_write);\n\n/*\n * Set up the argument/result storage required for the RPC call.\n */\nstatic void nfs_write_rpcsetup(struct nfs_write_data *data,\n\t\tunsigned int count, unsigned int offset,\n\t\tint how, struct nfs_commit_info *cinfo)\n{\n\tstruct nfs_page *req = data->header->req;\n\n\t/* Set up the RPC argument and reply structs\n\t * NB: take care not to mess about with data->commit et al. */\n\n\tdata->args.fh     = NFS_FH(data->header->inode);\n\tdata->args.offset = req_offset(req) + offset;\n\t/* pnfs_set_layoutcommit needs this */\n\tdata->mds_offset = data->args.offset;\n\tdata->args.pgbase = req->wb_pgbase + offset;\n\tdata->args.pages  = data->pages.pagevec;\n\tdata->args.count  = count;\n\tdata->args.context = get_nfs_open_context(req->wb_context);\n\tdata->args.lock_context = req->wb_lock_context;\n\tdata->args.stable  = NFS_UNSTABLE;\n\tswitch (how & (FLUSH_STABLE | FLUSH_COND_STABLE)) {\n\tcase 0:\n\t\tbreak;\n\tcase FLUSH_COND_STABLE:\n\t\tif (nfs_reqs_to_commit(cinfo))\n\t\t\tbreak;\n\tdefault:\n\t\tdata->args.stable = NFS_FILE_SYNC;\n\t}\n\n\tdata->res.fattr   = &data->fattr;\n\tdata->res.count   = count;\n\tdata->res.verf    = &data->verf;\n\tnfs_fattr_init(&data->fattr);\n}\n\nstatic int nfs_do_write(struct nfs_write_data *data,\n\t\tconst struct rpc_call_ops *call_ops,\n\t\tint how)\n{\n\tstruct inode *inode = data->header->inode;\n\n\treturn nfs_initiate_write(NFS_CLIENT(inode), data, call_ops, how, 0);\n}\n\nstatic int nfs_do_multiple_writes(struct list_head *head,\n\t\tconst struct rpc_call_ops *call_ops,\n\t\tint how)\n{\n\tstruct nfs_write_data *data;\n\tint ret = 0;\n\n\twhile (!list_empty(head)) {\n\t\tint ret2;\n\n\t\tdata = list_first_entry(head, struct nfs_write_data, list);\n\t\tlist_del_init(&data->list);\n\t\t\n\t\tret2 = nfs_do_write(data, call_ops, how);\n\t\t if (ret == 0)\n\t\t\t ret = ret2;\n\t}\n\treturn ret;\n}\n\n/* If a nfs_flush_* function fails, it should remove reqs from @head and\n * call this on each, which will prepare them to be retried on next\n * writeback using standard nfs.\n */\nstatic void nfs_redirty_request(struct nfs_page *req)\n{\n\tnfs_mark_request_dirty(req);\n\tnfs_unlock_request(req);\n\tnfs_end_page_writeback(req->wb_page);\n\tnfs_release_request(req);\n}\n\nstatic void nfs_async_write_error(struct list_head *head)\n{\n\tstruct nfs_page\t*req;\n\n\twhile (!list_empty(head)) {\n\t\treq = nfs_list_entry(head->next);\n\t\tnfs_list_remove_request(req);\n\t\tnfs_redirty_request(req);\n\t}\n}\n\nstatic const struct nfs_pgio_completion_ops nfs_async_write_completion_ops = {\n\t.error_cleanup = nfs_async_write_error,\n\t.completion = nfs_write_completion,\n};\n\nstatic void nfs_flush_error(struct nfs_pageio_descriptor *desc,\n\t\tstruct nfs_pgio_header *hdr)\n{\n\tset_bit(NFS_IOHDR_REDO, &hdr->flags);\n\twhile (!list_empty(&hdr->rpc_list)) {\n\t\tstruct nfs_write_data *data = list_first_entry(&hdr->rpc_list,\n\t\t\t\tstruct nfs_write_data, list);\n\t\tlist_del(&data->list);\n\t\tnfs_writedata_release(data);\n\t}\n\tdesc->pg_completion_ops->error_cleanup(&desc->pg_list);\n}\n\n/*\n * Generate multiple small requests to write out a single\n * contiguous dirty area on one page.\n */\nstatic int nfs_flush_multi(struct nfs_pageio_descriptor *desc,\n\t\t\t   struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_page *req = hdr->req;\n\tstruct page *page = req->wb_page;\n\tstruct nfs_write_data *data;\n\tsize_t wsize = desc->pg_bsize, nbytes;\n\tunsigned int offset;\n\tint requests = 0;\n\tstruct nfs_commit_info cinfo;\n\n\tnfs_init_cinfo(&cinfo, desc->pg_inode, desc->pg_dreq);\n\n\tif ((desc->pg_ioflags & FLUSH_COND_STABLE) &&\n\t    (desc->pg_moreio || nfs_reqs_to_commit(&cinfo) ||\n\t     desc->pg_count > wsize))\n\t\tdesc->pg_ioflags &= ~FLUSH_COND_STABLE;\n\n\n\toffset = 0;\n\tnbytes = desc->pg_count;\n\tdo {\n\t\tsize_t len = min(nbytes, wsize);\n\n\t\tdata = nfs_writedata_alloc(hdr, 1);\n\t\tif (!data) {\n\t\t\tnfs_flush_error(desc, hdr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tdata->pages.pagevec[0] = page;\n\t\tnfs_write_rpcsetup(data, len, offset, desc->pg_ioflags, &cinfo);\n\t\tlist_add(&data->list, &hdr->rpc_list);\n\t\trequests++;\n\t\tnbytes -= len;\n\t\toffset += len;\n\t} while (nbytes != 0);\n\tnfs_list_remove_request(req);\n\tnfs_list_add_request(req, &hdr->pages);\n\tdesc->pg_rpc_callops = &nfs_write_common_ops;\n\treturn 0;\n}\n\n/*\n * Create an RPC task for the given write request and kick it.\n * The page must have been locked by the caller.\n *\n * It may happen that the page we're passed is not marked dirty.\n * This is the case if nfs_updatepage detects a conflicting request\n * that has been written but not committed.\n */\nstatic int nfs_flush_one(struct nfs_pageio_descriptor *desc,\n\t\t\t struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_page\t\t*req;\n\tstruct page\t\t**pages;\n\tstruct nfs_write_data\t*data;\n\tstruct list_head *head = &desc->pg_list;\n\tstruct nfs_commit_info cinfo;\n\n\tdata = nfs_writedata_alloc(hdr, nfs_page_array_len(desc->pg_base,\n\t\t\t\t\t\t\t   desc->pg_count));\n\tif (!data) {\n\t\tnfs_flush_error(desc, hdr);\n\t\treturn -ENOMEM;\n\t}\n\n\tnfs_init_cinfo(&cinfo, desc->pg_inode, desc->pg_dreq);\n\tpages = data->pages.pagevec;\n\twhile (!list_empty(head)) {\n\t\treq = nfs_list_entry(head->next);\n\t\tnfs_list_remove_request(req);\n\t\tnfs_list_add_request(req, &hdr->pages);\n\t\t*pages++ = req->wb_page;\n\t}\n\n\tif ((desc->pg_ioflags & FLUSH_COND_STABLE) &&\n\t    (desc->pg_moreio || nfs_reqs_to_commit(&cinfo)))\n\t\tdesc->pg_ioflags &= ~FLUSH_COND_STABLE;\n\n\t/* Set up the argument struct */\n\tnfs_write_rpcsetup(data, desc->pg_count, 0, desc->pg_ioflags, &cinfo);\n\tlist_add(&data->list, &hdr->rpc_list);\n\tdesc->pg_rpc_callops = &nfs_write_common_ops;\n\treturn 0;\n}\n\nint nfs_generic_flush(struct nfs_pageio_descriptor *desc,\n\t\t      struct nfs_pgio_header *hdr)\n{\n\tif (desc->pg_bsize < PAGE_CACHE_SIZE)\n\t\treturn nfs_flush_multi(desc, hdr);\n\treturn nfs_flush_one(desc, hdr);\n}\nEXPORT_SYMBOL_GPL(nfs_generic_flush);\n\nstatic int nfs_generic_pg_writepages(struct nfs_pageio_descriptor *desc)\n{\n\tstruct nfs_write_header *whdr;\n\tstruct nfs_pgio_header *hdr;\n\tint ret;\n\n\twhdr = nfs_writehdr_alloc();\n\tif (!whdr) {\n\t\tdesc->pg_completion_ops->error_cleanup(&desc->pg_list);\n\t\treturn -ENOMEM;\n\t}\n\thdr = &whdr->header;\n\tnfs_pgheader_init(desc, hdr, nfs_writehdr_free);\n\tatomic_inc(&hdr->refcnt);\n\tret = nfs_generic_flush(desc, hdr);\n\tif (ret == 0)\n\t\tret = nfs_do_multiple_writes(&hdr->rpc_list,\n\t\t\t\t\t     desc->pg_rpc_callops,\n\t\t\t\t\t     desc->pg_ioflags);\n\tif (atomic_dec_and_test(&hdr->refcnt))\n\t\thdr->completion_ops->completion(hdr);\n\treturn ret;\n}\n\nstatic const struct nfs_pageio_ops nfs_pageio_write_ops = {\n\t.pg_test = nfs_generic_pg_test,\n\t.pg_doio = nfs_generic_pg_writepages,\n};\n\nvoid nfs_pageio_init_write(struct nfs_pageio_descriptor *pgio,\n\t\t\t       struct inode *inode, int ioflags,\n\t\t\t       const struct nfs_pgio_completion_ops *compl_ops)\n{\n\tnfs_pageio_init(pgio, inode, &nfs_pageio_write_ops, compl_ops,\n\t\t\t\tNFS_SERVER(inode)->wsize, ioflags);\n}\nEXPORT_SYMBOL_GPL(nfs_pageio_init_write);\n\nvoid nfs_pageio_reset_write_mds(struct nfs_pageio_descriptor *pgio)\n{\n\tpgio->pg_ops = &nfs_pageio_write_ops;\n\tpgio->pg_bsize = NFS_SERVER(pgio->pg_inode)->wsize;\n}\nEXPORT_SYMBOL_GPL(nfs_pageio_reset_write_mds);\n\n\nvoid nfs_write_prepare(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_write_data *data = calldata;\n\tint err;\n\terr = NFS_PROTO(data->header->inode)->write_rpc_prepare(task, data);\n\tif (err)\n\t\trpc_exit(task, err);\n}\n\nvoid nfs_commit_prepare(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_commit_data *data = calldata;\n\n\tNFS_PROTO(data->inode)->commit_rpc_prepare(task, data);\n}\n\n/*\n * Handle a write reply that flushes a whole page.\n *\n * FIXME: There is an inherent race with invalidate_inode_pages and\n *\t  writebacks since the page->count is kept > 1 for as long\n *\t  as the page has a write request pending.\n */\nstatic void nfs_writeback_done_common(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_write_data\t*data = calldata;\n\n\tnfs_writeback_done(task, data);\n}\n\nstatic void nfs_writeback_release_common(void *calldata)\n{\n\tstruct nfs_write_data\t*data = calldata;\n\tstruct nfs_pgio_header *hdr = data->header;\n\tint status = data->task.tk_status;\n\n\tif ((status >= 0) && nfs_write_need_commit(data)) {\n\t\tspin_lock(&hdr->lock);\n\t\tif (test_bit(NFS_IOHDR_NEED_RESCHED, &hdr->flags))\n\t\t\t; /* Do nothing */\n\t\telse if (!test_and_set_bit(NFS_IOHDR_NEED_COMMIT, &hdr->flags))\n\t\t\tmemcpy(hdr->verf, &data->verf, sizeof(*hdr->verf));\n\t\telse if (memcmp(hdr->verf, &data->verf, sizeof(*hdr->verf)))\n\t\t\tset_bit(NFS_IOHDR_NEED_RESCHED, &hdr->flags);\n\t\tspin_unlock(&hdr->lock);\n\t}\n\tnfs_writedata_release(data);\n}\n\nstatic const struct rpc_call_ops nfs_write_common_ops = {\n\t.rpc_call_prepare = nfs_write_prepare,\n\t.rpc_call_done = nfs_writeback_done_common,\n\t.rpc_release = nfs_writeback_release_common,\n};\n\n\n/*\n * This function is called when the WRITE call is complete.\n */\nvoid nfs_writeback_done(struct rpc_task *task, struct nfs_write_data *data)\n{\n\tstruct nfs_writeargs\t*argp = &data->args;\n\tstruct nfs_writeres\t*resp = &data->res;\n\tstruct inode\t\t*inode = data->header->inode;\n\tint status;\n\n\tdprintk(\"NFS: %5u nfs_writeback_done (status %d)\\n\",\n\t\ttask->tk_pid, task->tk_status);\n\n\t/*\n\t * ->write_done will attempt to use post-op attributes to detect\n\t * conflicting writes by other clients.  A strict interpretation\n\t * of close-to-open would allow us to continue caching even if\n\t * another writer had changed the file, but some applications\n\t * depend on tighter cache coherency when writing.\n\t */\n\tstatus = NFS_PROTO(inode)->write_done(task, data);\n\tif (status != 0)\n\t\treturn;\n\tnfs_add_stats(inode, NFSIOS_SERVERWRITTENBYTES, resp->count);\n\n#if IS_ENABLED(CONFIG_NFS_V3) || IS_ENABLED(CONFIG_NFS_V4)\n\tif (resp->verf->committed < argp->stable && task->tk_status >= 0) {\n\t\t/* We tried a write call, but the server did not\n\t\t * commit data to stable storage even though we\n\t\t * requested it.\n\t\t * Note: There is a known bug in Tru64 < 5.0 in which\n\t\t *\t the server reports NFS_DATA_SYNC, but performs\n\t\t *\t NFS_FILE_SYNC. We therefore implement this checking\n\t\t *\t as a dprintk() in order to avoid filling syslog.\n\t\t */\n\t\tstatic unsigned long    complain;\n\n\t\t/* Note this will print the MDS for a DS write */\n\t\tif (time_before(complain, jiffies)) {\n\t\t\tdprintk(\"NFS:       faulty NFS server %s:\"\n\t\t\t\t\" (committed = %d) != (stable = %d)\\n\",\n\t\t\t\tNFS_SERVER(inode)->nfs_client->cl_hostname,\n\t\t\t\tresp->verf->committed, argp->stable);\n\t\t\tcomplain = jiffies + 300 * HZ;\n\t\t}\n\t}\n#endif\n\tif (task->tk_status < 0)\n\t\tnfs_set_pgio_error(data->header, task->tk_status, argp->offset);\n\telse if (resp->count < argp->count) {\n\t\tstatic unsigned long    complain;\n\n\t\t/* This a short write! */\n\t\tnfs_inc_stats(inode, NFSIOS_SHORTWRITE);\n\n\t\t/* Has the server at least made some progress? */\n\t\tif (resp->count == 0) {\n\t\t\tif (time_before(complain, jiffies)) {\n\t\t\t\tprintk(KERN_WARNING\n\t\t\t\t       \"NFS: Server wrote zero bytes, expected %u.\\n\",\n\t\t\t\t       argp->count);\n\t\t\t\tcomplain = jiffies + 300 * HZ;\n\t\t\t}\n\t\t\tnfs_set_pgio_error(data->header, -EIO, argp->offset);\n\t\t\ttask->tk_status = -EIO;\n\t\t\treturn;\n\t\t}\n\t\t/* Was this an NFSv2 write or an NFSv3 stable write? */\n\t\tif (resp->verf->committed != NFS_UNSTABLE) {\n\t\t\t/* Resend from where the server left off */\n\t\t\tdata->mds_offset += resp->count;\n\t\t\targp->offset += resp->count;\n\t\t\targp->pgbase += resp->count;\n\t\t\targp->count -= resp->count;\n\t\t} else {\n\t\t\t/* Resend as a stable write in order to avoid\n\t\t\t * headaches in the case of a server crash.\n\t\t\t */\n\t\t\targp->stable = NFS_FILE_SYNC;\n\t\t}\n\t\trpc_restart_call_prepare(task);\n\t}\n}\n\n\n#if IS_ENABLED(CONFIG_NFS_V3) || IS_ENABLED(CONFIG_NFS_V4)\nstatic int nfs_commit_set_lock(struct nfs_inode *nfsi, int may_wait)\n{\n\tint ret;\n\n\tif (!test_and_set_bit(NFS_INO_COMMIT, &nfsi->flags))\n\t\treturn 1;\n\tif (!may_wait)\n\t\treturn 0;\n\tret = out_of_line_wait_on_bit_lock(&nfsi->flags,\n\t\t\t\tNFS_INO_COMMIT,\n\t\t\t\tnfs_wait_bit_killable,\n\t\t\t\tTASK_KILLABLE);\n\treturn (ret < 0) ? ret : 1;\n}\n\nstatic void nfs_commit_clear_lock(struct nfs_inode *nfsi)\n{\n\tclear_bit(NFS_INO_COMMIT, &nfsi->flags);\n\tsmp_mb__after_clear_bit();\n\twake_up_bit(&nfsi->flags, NFS_INO_COMMIT);\n}\n\nvoid nfs_commitdata_release(struct nfs_commit_data *data)\n{\n\tput_nfs_open_context(data->context);\n\tnfs_commit_free(data);\n}\nEXPORT_SYMBOL_GPL(nfs_commitdata_release);\n\nint nfs_initiate_commit(struct rpc_clnt *clnt, struct nfs_commit_data *data,\n\t\t\tconst struct rpc_call_ops *call_ops,\n\t\t\tint how, int flags)\n{\n\tstruct rpc_task *task;\n\tint priority = flush_task_priority(how);\n\tstruct rpc_message msg = {\n\t\t.rpc_argp = &data->args,\n\t\t.rpc_resp = &data->res,\n\t\t.rpc_cred = data->cred,\n\t};\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.task = &data->task,\n\t\t.rpc_client = clnt,\n\t\t.rpc_message = &msg,\n\t\t.callback_ops = call_ops,\n\t\t.callback_data = data,\n\t\t.workqueue = nfsiod_workqueue,\n\t\t.flags = RPC_TASK_ASYNC | flags,\n\t\t.priority = priority,\n\t};\n\t/* Set up the initial task struct.  */\n\tNFS_PROTO(data->inode)->commit_setup(data, &msg);\n\n\tdprintk(\"NFS: %5u initiated commit call\\n\", data->task.tk_pid);\n\n\tnfs4_state_protect(NFS_SERVER(data->inode)->nfs_client,\n\t\tNFS_SP4_MACH_CRED_COMMIT, &task_setup_data.rpc_client, &msg);\n\n\ttask = rpc_run_task(&task_setup_data);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\tif (how & FLUSH_SYNC)\n\t\trpc_wait_for_completion_task(task);\n\trpc_put_task(task);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nfs_initiate_commit);\n\n/*\n * Set up the argument/result storage required for the RPC call.\n */\nvoid nfs_init_commit(struct nfs_commit_data *data,\n\t\t     struct list_head *head,\n\t\t     struct pnfs_layout_segment *lseg,\n\t\t     struct nfs_commit_info *cinfo)\n{\n\tstruct nfs_page *first = nfs_list_entry(head->next);\n\tstruct inode *inode = first->wb_context->dentry->d_inode;\n\n\t/* Set up the RPC argument and reply structs\n\t * NB: take care not to mess about with data->commit et al. */\n\n\tlist_splice_init(head, &data->pages);\n\n\tdata->inode\t  = inode;\n\tdata->cred\t  = first->wb_context->cred;\n\tdata->lseg\t  = lseg; /* reference transferred */\n\tdata->mds_ops     = &nfs_commit_ops;\n\tdata->completion_ops = cinfo->completion_ops;\n\tdata->dreq\t  = cinfo->dreq;\n\n\tdata->args.fh     = NFS_FH(data->inode);\n\t/* Note: we always request a commit of the entire inode */\n\tdata->args.offset = 0;\n\tdata->args.count  = 0;\n\tdata->context     = get_nfs_open_context(first->wb_context);\n\tdata->res.fattr   = &data->fattr;\n\tdata->res.verf    = &data->verf;\n\tnfs_fattr_init(&data->fattr);\n}\nEXPORT_SYMBOL_GPL(nfs_init_commit);\n\nvoid nfs_retry_commit(struct list_head *page_list,\n\t\t      struct pnfs_layout_segment *lseg,\n\t\t      struct nfs_commit_info *cinfo)\n{\n\tstruct nfs_page *req;\n\n\twhile (!list_empty(page_list)) {\n\t\treq = nfs_list_entry(page_list->next);\n\t\tnfs_list_remove_request(req);\n\t\tnfs_mark_request_commit(req, lseg, cinfo);\n\t\tif (!cinfo->dreq) {\n\t\t\tdec_zone_page_state(req->wb_page, NR_UNSTABLE_NFS);\n\t\t\tdec_bdi_stat(page_file_mapping(req->wb_page)->backing_dev_info,\n\t\t\t\t     BDI_RECLAIMABLE);\n\t\t}\n\t\tnfs_unlock_and_release_request(req);\n\t}\n}\nEXPORT_SYMBOL_GPL(nfs_retry_commit);\n\n/*\n * Commit dirty pages\n */\nstatic int\nnfs_commit_list(struct inode *inode, struct list_head *head, int how,\n\t\tstruct nfs_commit_info *cinfo)\n{\n\tstruct nfs_commit_data\t*data;\n\n\tdata = nfs_commitdata_alloc();\n\n\tif (!data)\n\t\tgoto out_bad;\n\n\t/* Set up the argument struct */\n\tnfs_init_commit(data, head, NULL, cinfo);\n\tatomic_inc(&cinfo->mds->rpcs_out);\n\treturn nfs_initiate_commit(NFS_CLIENT(inode), data, data->mds_ops,\n\t\t\t\t   how, 0);\n out_bad:\n\tnfs_retry_commit(head, NULL, cinfo);\n\tcinfo->completion_ops->error_cleanup(NFS_I(inode));\n\treturn -ENOMEM;\n}\n\n/*\n * COMMIT call returned\n */\nstatic void nfs_commit_done(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_commit_data\t*data = calldata;\n\n        dprintk(\"NFS: %5u nfs_commit_done (status %d)\\n\",\n                                task->tk_pid, task->tk_status);\n\n\t/* Call the NFS version-specific code */\n\tNFS_PROTO(data->inode)->commit_done(task, data);\n}\n\nstatic void nfs_commit_release_pages(struct nfs_commit_data *data)\n{\n\tstruct nfs_page\t*req;\n\tint status = data->task.tk_status;\n\tstruct nfs_commit_info cinfo;\n\n\twhile (!list_empty(&data->pages)) {\n\t\treq = nfs_list_entry(data->pages.next);\n\t\tnfs_list_remove_request(req);\n\t\tnfs_clear_page_commit(req->wb_page);\n\n\t\tdprintk(\"NFS:       commit (%s/%llu %d@%lld)\",\n\t\t\treq->wb_context->dentry->d_sb->s_id,\n\t\t\t(unsigned long long)NFS_FILEID(req->wb_context->dentry->d_inode),\n\t\t\treq->wb_bytes,\n\t\t\t(long long)req_offset(req));\n\t\tif (status < 0) {\n\t\t\tnfs_context_set_write_error(req->wb_context, status);\n\t\t\tnfs_inode_remove_request(req);\n\t\t\tdprintk(\", error = %d\\n\", status);\n\t\t\tgoto next;\n\t\t}\n\n\t\t/* Okay, COMMIT succeeded, apparently. Check the verifier\n\t\t * returned by the server against all stored verfs. */\n\t\tif (!memcmp(&req->wb_verf, &data->verf.verifier, sizeof(req->wb_verf))) {\n\t\t\t/* We have a match */\n\t\t\tnfs_inode_remove_request(req);\n\t\t\tdprintk(\" OK\\n\");\n\t\t\tgoto next;\n\t\t}\n\t\t/* We have a mismatch. Write the page again */\n\t\tdprintk(\" mismatch\\n\");\n\t\tnfs_mark_request_dirty(req);\n\t\tset_bit(NFS_CONTEXT_RESEND_WRITES, &req->wb_context->flags);\n\tnext:\n\t\tnfs_unlock_and_release_request(req);\n\t}\n\tnfs_init_cinfo(&cinfo, data->inode, data->dreq);\n\tif (atomic_dec_and_test(&cinfo.mds->rpcs_out))\n\t\tnfs_commit_clear_lock(NFS_I(data->inode));\n}\n\nstatic void nfs_commit_release(void *calldata)\n{\n\tstruct nfs_commit_data *data = calldata;\n\n\tdata->completion_ops->completion(data);\n\tnfs_commitdata_release(calldata);\n}\n\nstatic const struct rpc_call_ops nfs_commit_ops = {\n\t.rpc_call_prepare = nfs_commit_prepare,\n\t.rpc_call_done = nfs_commit_done,\n\t.rpc_release = nfs_commit_release,\n};\n\nstatic const struct nfs_commit_completion_ops nfs_commit_completion_ops = {\n\t.completion = nfs_commit_release_pages,\n\t.error_cleanup = nfs_commit_clear_lock,\n};\n\nint nfs_generic_commit_list(struct inode *inode, struct list_head *head,\n\t\t\t    int how, struct nfs_commit_info *cinfo)\n{\n\tint status;\n\n\tstatus = pnfs_commit_list(inode, head, how, cinfo);\n\tif (status == PNFS_NOT_ATTEMPTED)\n\t\tstatus = nfs_commit_list(inode, head, how, cinfo);\n\treturn status;\n}\n\nint nfs_commit_inode(struct inode *inode, int how)\n{\n\tLIST_HEAD(head);\n\tstruct nfs_commit_info cinfo;\n\tint may_wait = how & FLUSH_SYNC;\n\tint res;\n\n\tres = nfs_commit_set_lock(NFS_I(inode), may_wait);\n\tif (res <= 0)\n\t\tgoto out_mark_dirty;\n\tnfs_init_cinfo_from_inode(&cinfo, inode);\n\tres = nfs_scan_commit(inode, &head, &cinfo);\n\tif (res) {\n\t\tint error;\n\n\t\terror = nfs_generic_commit_list(inode, &head, how, &cinfo);\n\t\tif (error < 0)\n\t\t\treturn error;\n\t\tif (!may_wait)\n\t\t\tgoto out_mark_dirty;\n\t\terror = wait_on_bit(&NFS_I(inode)->flags,\n\t\t\t\tNFS_INO_COMMIT,\n\t\t\t\tnfs_wait_bit_killable,\n\t\t\t\tTASK_KILLABLE);\n\t\tif (error < 0)\n\t\t\treturn error;\n\t} else\n\t\tnfs_commit_clear_lock(NFS_I(inode));\n\treturn res;\n\t/* Note: If we exit without ensuring that the commit is complete,\n\t * we must mark the inode as dirty. Otherwise, future calls to\n\t * sync_inode() with the WB_SYNC_ALL flag set will fail to ensure\n\t * that the data is on the disk.\n\t */\nout_mark_dirty:\n\t__mark_inode_dirty(inode, I_DIRTY_DATASYNC);\n\treturn res;\n}\n\nstatic int nfs_commit_unstable_pages(struct inode *inode, struct writeback_control *wbc)\n{\n\tstruct nfs_inode *nfsi = NFS_I(inode);\n\tint flags = FLUSH_SYNC;\n\tint ret = 0;\n\n\t/* no commits means nothing needs to be done */\n\tif (!nfsi->commit_info.ncommit)\n\t\treturn ret;\n\n\tif (wbc->sync_mode == WB_SYNC_NONE) {\n\t\t/* Don't commit yet if this is a non-blocking flush and there\n\t\t * are a lot of outstanding writes for this mapping.\n\t\t */\n\t\tif (nfsi->commit_info.ncommit <= (nfsi->npages >> 1))\n\t\t\tgoto out_mark_dirty;\n\n\t\t/* don't wait for the COMMIT response */\n\t\tflags = 0;\n\t}\n\n\tret = nfs_commit_inode(inode, flags);\n\tif (ret >= 0) {\n\t\tif (wbc->sync_mode == WB_SYNC_NONE) {\n\t\t\tif (ret < wbc->nr_to_write)\n\t\t\t\twbc->nr_to_write -= ret;\n\t\t\telse\n\t\t\t\twbc->nr_to_write = 0;\n\t\t}\n\t\treturn 0;\n\t}\nout_mark_dirty:\n\t__mark_inode_dirty(inode, I_DIRTY_DATASYNC);\n\treturn ret;\n}\n#else\nstatic int nfs_commit_unstable_pages(struct inode *inode, struct writeback_control *wbc)\n{\n\treturn 0;\n}\n#endif\n\nint nfs_write_inode(struct inode *inode, struct writeback_control *wbc)\n{\n\treturn nfs_commit_unstable_pages(inode, wbc);\n}\nEXPORT_SYMBOL_GPL(nfs_write_inode);\n\n/*\n * flush the inode to disk.\n */\nint nfs_wb_all(struct inode *inode)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_ALL,\n\t\t.nr_to_write = LONG_MAX,\n\t\t.range_start = 0,\n\t\t.range_end = LLONG_MAX,\n\t};\n\tint ret;\n\n\ttrace_nfs_writeback_inode_enter(inode);\n\n\tret = sync_inode(inode, &wbc);\n\n\ttrace_nfs_writeback_inode_exit(inode, ret);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nfs_wb_all);\n\nint nfs_wb_page_cancel(struct inode *inode, struct page *page)\n{\n\tstruct nfs_page *req;\n\tint ret = 0;\n\n\tfor (;;) {\n\t\twait_on_page_writeback(page);\n\t\treq = nfs_page_find_request(page);\n\t\tif (req == NULL)\n\t\t\tbreak;\n\t\tif (nfs_lock_request(req)) {\n\t\t\tnfs_clear_request_commit(req);\n\t\t\tnfs_inode_remove_request(req);\n\t\t\t/*\n\t\t\t * In case nfs_inode_remove_request has marked the\n\t\t\t * page as being dirty\n\t\t\t */\n\t\t\tcancel_dirty_page(page, PAGE_CACHE_SIZE);\n\t\t\tnfs_unlock_and_release_request(req);\n\t\t\tbreak;\n\t\t}\n\t\tret = nfs_wait_on_request(req);\n\t\tnfs_release_request(req);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/*\n * Write back all requests on one page - we do this before reading it.\n */\nint nfs_wb_page(struct inode *inode, struct page *page)\n{\n\tloff_t range_start = page_file_offset(page);\n\tloff_t range_end = range_start + (loff_t)(PAGE_CACHE_SIZE - 1);\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_ALL,\n\t\t.nr_to_write = 0,\n\t\t.range_start = range_start,\n\t\t.range_end = range_end,\n\t};\n\tint ret;\n\n\ttrace_nfs_writeback_page_enter(inode);\n\n\tfor (;;) {\n\t\twait_on_page_writeback(page);\n\t\tif (clear_page_dirty_for_io(page)) {\n\t\t\tret = nfs_writepage_locked(page, &wbc);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out_error;\n\t\t\tcontinue;\n\t\t}\n\t\tret = 0;\n\t\tif (!PagePrivate(page))\n\t\t\tbreak;\n\t\tret = nfs_commit_inode(inode, FLUSH_SYNC);\n\t\tif (ret < 0)\n\t\t\tgoto out_error;\n\t}\nout_error:\n\ttrace_nfs_writeback_page_exit(inode, ret);\n\treturn ret;\n}\n\n#ifdef CONFIG_MIGRATION\nint nfs_migrate_page(struct address_space *mapping, struct page *newpage,\n\t\tstruct page *page, enum migrate_mode mode)\n{\n\t/*\n\t * If PagePrivate is set, then the page is currently associated with\n\t * an in-progress read or write request. Don't try to migrate it.\n\t *\n\t * FIXME: we could do this in principle, but we'll need a way to ensure\n\t *        that we can safely release the inode reference while holding\n\t *        the page lock.\n\t */\n\tif (PagePrivate(page))\n\t\treturn -EBUSY;\n\n\tif (!nfs_fscache_release_page(page, GFP_KERNEL))\n\t\treturn -EBUSY;\n\n\treturn migrate_page(mapping, newpage, page, mode);\n}\n#endif\n\nint __init nfs_init_writepagecache(void)\n{\n\tnfs_wdata_cachep = kmem_cache_create(\"nfs_write_data\",\n\t\t\t\t\t     sizeof(struct nfs_write_header),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (nfs_wdata_cachep == NULL)\n\t\treturn -ENOMEM;\n\n\tnfs_wdata_mempool = mempool_create_slab_pool(MIN_POOL_WRITE,\n\t\t\t\t\t\t     nfs_wdata_cachep);\n\tif (nfs_wdata_mempool == NULL)\n\t\tgoto out_destroy_write_cache;\n\n\tnfs_cdata_cachep = kmem_cache_create(\"nfs_commit_data\",\n\t\t\t\t\t     sizeof(struct nfs_commit_data),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (nfs_cdata_cachep == NULL)\n\t\tgoto out_destroy_write_mempool;\n\n\tnfs_commit_mempool = mempool_create_slab_pool(MIN_POOL_COMMIT,\n\t\t\t\t\t\t      nfs_cdata_cachep);\n\tif (nfs_commit_mempool == NULL)\n\t\tgoto out_destroy_commit_cache;\n\n\t/*\n\t * NFS congestion size, scale with available memory.\n\t *\n\t *  64MB:    8192k\n\t * 128MB:   11585k\n\t * 256MB:   16384k\n\t * 512MB:   23170k\n\t *   1GB:   32768k\n\t *   2GB:   46340k\n\t *   4GB:   65536k\n\t *   8GB:   92681k\n\t *  16GB:  131072k\n\t *\n\t * This allows larger machines to have larger/more transfers.\n\t * Limit the default to 256M\n\t */\n\tnfs_congestion_kb = (16*int_sqrt(totalram_pages)) << (PAGE_SHIFT-10);\n\tif (nfs_congestion_kb > 256*1024)\n\t\tnfs_congestion_kb = 256*1024;\n\n\treturn 0;\n\nout_destroy_commit_cache:\n\tkmem_cache_destroy(nfs_cdata_cachep);\nout_destroy_write_mempool:\n\tmempool_destroy(nfs_wdata_mempool);\nout_destroy_write_cache:\n\tkmem_cache_destroy(nfs_wdata_cachep);\n\treturn -ENOMEM;\n}\n\nvoid nfs_destroy_writepagecache(void)\n{\n\tmempool_destroy(nfs_commit_mempool);\n\tkmem_cache_destroy(nfs_cdata_cachep);\n\tmempool_destroy(nfs_wdata_mempool);\n\tkmem_cache_destroy(nfs_wdata_cachep);\n}\n\n"], "filenames": ["fs/nfs/write.c"], "buggy_code_start_loc": [925], "buggy_code_end_loc": [938], "fixing_code_start_loc": [925], "fixing_code_end_loc": [939], "type": "CWE-200", "message": "The nfs_can_extend_write function in fs/nfs/write.c in the Linux kernel before 3.13.3 relies on a write delegation to extend a write operation without a certain up-to-date verification, which allows local users to obtain sensitive information from kernel memory in opportunistic circumstances by writing to a file in an NFS filesystem and then reading the same file.", "other": {"cve": {"id": "CVE-2014-2038", "sourceIdentifier": "secalert@redhat.com", "published": "2014-02-28T06:18:54.617", "lastModified": "2023-02-13T00:38:37.960", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The nfs_can_extend_write function in fs/nfs/write.c in the Linux kernel before 3.13.3 relies on a write delegation to extend a write operation without a certain up-to-date verification, which allows local users to obtain sensitive information from kernel memory in opportunistic circumstances by writing to a file in an NFS filesystem and then reading the same file."}, {"lang": "es", "value": "La funci\u00f3n nfs_can_extend_write en fs/nfs/write.c en el kernel de Linux anterior a 3.13.3 se basa en una delegaci\u00f3n de escritura para extender una operaci\u00f3n de escritura sin cierta verificaci\u00f3n actualizada, lo que permite a usuarios locales obtener informaci\u00f3n sensible de la memoria del kernel en circunstancias oportunistas mediante la escritura a un archivo en un sistema de archivos NFS y luego leyendo el mismo archivo."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.13.3", "matchCriteriaId": "F2068F34-DC08-4C43-A718-95BA6B8DD6B8"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:esm:*:*:*", "matchCriteriaId": "8D305F7A-D159-4716-AB26-5E38BB5CD991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:13.10:*:*:*:*:*:*:*", "matchCriteriaId": "7F61F047-129C-41A6-8A27-FFCBB8563E91"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=263b4509ec4d47e0da3e753f85a39ea12d1eff24", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.13.3", "source": "secalert@redhat.com", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/02/20/16", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2137-1", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2140-1", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1066939", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/263b4509ec4d47e0da3e753f85a39ea12d1eff24", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/263b4509ec4d47e0da3e753f85a39ea12d1eff24"}}