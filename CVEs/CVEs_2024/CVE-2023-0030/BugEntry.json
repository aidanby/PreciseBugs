{"buggy_code": ["/*\n * Copyright 2017 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n#include \"uvmm.h\"\n#include \"umem.h\"\n#include \"ummu.h\"\n\n#include <core/client.h>\n#include <core/memory.h>\n\n#include <nvif/if000c.h>\n#include <nvif/unpack.h>\n\nstatic const struct nvkm_object_func nvkm_uvmm;\nstruct nvkm_vmm *\nnvkm_uvmm_search(struct nvkm_client *client, u64 handle)\n{\n\tstruct nvkm_object *object;\n\n\tobject = nvkm_object_search(client, handle, &nvkm_uvmm);\n\tif (IS_ERR(object))\n\t\treturn (void *)object;\n\n\treturn nvkm_uvmm(object)->vmm;\n}\n\nstatic int\nnvkm_uvmm_mthd_unmap(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_unmap_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tint ret = -ENOSYS;\n\tu64 addr;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\taddr = args->v0.addr;\n\t} else\n\t\treturn ret;\n\n\tmutex_lock(&vmm->mutex);\n\tvma = nvkm_vmm_node_search(vmm, addr);\n\tif (ret = -ENOENT, !vma || vma->addr != addr) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx: %016llx\",\n\t\t\t  addr, vma ? vma->addr : ~0ULL);\n\t\tgoto done;\n\t}\n\n\tif (ret = -ENOENT, (!vma->user && !client->super) || vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d %d %d\", addr,\n\t\t\t  vma->user, !client->super, vma->busy);\n\t\tgoto done;\n\t}\n\n\tif (ret = -EINVAL, !vma->memory) {\n\t\tVMM_DEBUG(vmm, \"unmapped\");\n\t\tgoto done;\n\t}\n\n\tnvkm_vmm_unmap_locked(vmm, vma);\n\tret = 0;\ndone:\n\tmutex_unlock(&vmm->mutex);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_map(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_map_v0 v0;\n\t} *args = argv;\n\tu64 addr, size, handle, offset;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tstruct nvkm_memory *memory;\n\tint ret = -ENOSYS;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, true))) {\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t\thandle = args->v0.memory;\n\t\toffset = args->v0.offset;\n\t} else\n\t\treturn ret;\n\n\tmemory = nvkm_umem_search(client, handle);\n\tif (IS_ERR(memory)) {\n\t\tVMM_DEBUG(vmm, \"memory %016llx %ld\\n\", handle, PTR_ERR(memory));\n\t\treturn PTR_ERR(memory);\n\t}\n\n\tmutex_lock(&vmm->mutex);\n\tif (ret = -ENOENT, !(vma = nvkm_vmm_node_search(vmm, addr))) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx\", addr);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -ENOENT, (!vma->user && !client->super) || vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d %d %d\", addr,\n\t\t\t  vma->user, !client->super, vma->busy);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -EINVAL, vma->addr != addr || vma->size != size) {\n\t\tif (addr + size > vma->addr + vma->size || vma->memory ||\n\t\t    (vma->refd == NVKM_VMA_PAGE_NONE && !vma->mapref)) {\n\t\t\tVMM_DEBUG(vmm, \"split %d %d %d \"\n\t\t\t\t       \"%016llx %016llx %016llx %016llx\",\n\t\t\t\t  !!vma->memory, vma->refd, vma->mapref,\n\t\t\t\t  addr, size, vma->addr, (u64)vma->size);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tif (vma->addr != addr) {\n\t\t\tconst u64 tail = vma->size + vma->addr - addr;\n\t\t\tif (ret = -ENOMEM, !(vma = nvkm_vma_tail(vma, tail)))\n\t\t\t\tgoto fail;\n\t\t\tvma->part = true;\n\t\t\tnvkm_vmm_node_insert(vmm, vma);\n\t\t}\n\n\t\tif (vma->size != size) {\n\t\t\tconst u64 tail = vma->size - size;\n\t\t\tstruct nvkm_vma *tmp;\n\t\t\tif (ret = -ENOMEM, !(tmp = nvkm_vma_tail(vma, tail))) {\n\t\t\t\tnvkm_vmm_unmap_region(vmm, vma);\n\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\ttmp->part = true;\n\t\t\tnvkm_vmm_node_insert(vmm, tmp);\n\t\t}\n\t}\n\tvma->busy = true;\n\tmutex_unlock(&vmm->mutex);\n\n\tret = nvkm_memory_map(memory, offset, vmm, vma, argv, argc);\n\tif (ret == 0) {\n\t\t/* Successful map will clear vma->busy. */\n\t\tnvkm_memory_unref(&memory);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&vmm->mutex);\n\tvma->busy = false;\n\tnvkm_vmm_unmap_region(vmm, vma);\nfail:\n\tmutex_unlock(&vmm->mutex);\n\tnvkm_memory_unref(&memory);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_put(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_put_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tint ret = -ENOSYS;\n\tu64 addr;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\taddr = args->v0.addr;\n\t} else\n\t\treturn ret;\n\n\tmutex_lock(&vmm->mutex);\n\tvma = nvkm_vmm_node_search(vmm, args->v0.addr);\n\tif (ret = -ENOENT, !vma || vma->addr != addr || vma->part) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx: %016llx %d\", addr,\n\t\t\t  vma ? vma->addr : ~0ULL, vma ? vma->part : 0);\n\t\tgoto done;\n\t}\n\n\tif (ret = -ENOENT, (!vma->user && !client->super) || vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d %d %d\", addr,\n\t\t\t  vma->user, !client->super, vma->busy);\n\t\tgoto done;\n\t}\n\n\tnvkm_vmm_put_locked(vmm, vma);\n\tret = 0;\ndone:\n\tmutex_unlock(&vmm->mutex);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_get(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_get_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tint ret = -ENOSYS;\n\tbool getref, mapref, sparse;\n\tu8 page, align;\n\tu64 size;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\tgetref = args->v0.type == NVIF_VMM_GET_V0_PTES;\n\t\tmapref = args->v0.type == NVIF_VMM_GET_V0_ADDR;\n\t\tsparse = args->v0.sparse;\n\t\tpage = args->v0.page;\n\t\talign = args->v0.align;\n\t\tsize = args->v0.size;\n\t} else\n\t\treturn ret;\n\n\tmutex_lock(&vmm->mutex);\n\tret = nvkm_vmm_get_locked(vmm, getref, mapref, sparse,\n\t\t\t\t  page, align, size, &vma);\n\tmutex_unlock(&vmm->mutex);\n\tif (ret)\n\t\treturn ret;\n\n\targs->v0.addr = vma->addr;\n\tvma->user = !client->super;\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_page(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tunion {\n\t\tstruct nvif_vmm_page_v0 v0;\n\t} *args = argv;\n\tconst struct nvkm_vmm_page *page;\n\tint ret = -ENOSYS;\n\tu8 type, index, nr;\n\n\tpage = uvmm->vmm->func->page;\n\tfor (nr = 0; page[nr].shift; nr++);\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\tif ((index = args->v0.index) >= nr)\n\t\t\treturn -EINVAL;\n\t\ttype = page[index].type;\n\t\targs->v0.shift = page[index].shift;\n\t\targs->v0.sparse = !!(type & NVKM_VMM_PAGE_SPARSE);\n\t\targs->v0.vram = !!(type & NVKM_VMM_PAGE_VRAM);\n\t\targs->v0.host = !!(type & NVKM_VMM_PAGE_HOST);\n\t\targs->v0.comp = !!(type & NVKM_VMM_PAGE_COMP);\n\t} else\n\t\treturn -ENOSYS;\n\n\treturn 0;\n}\n\nstatic int\nnvkm_uvmm_mthd(struct nvkm_object *object, u32 mthd, void *argv, u32 argc)\n{\n\tstruct nvkm_uvmm *uvmm = nvkm_uvmm(object);\n\tswitch (mthd) {\n\tcase NVIF_VMM_V0_PAGE  : return nvkm_uvmm_mthd_page  (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_GET   : return nvkm_uvmm_mthd_get   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_PUT   : return nvkm_uvmm_mthd_put   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_MAP   : return nvkm_uvmm_mthd_map   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_UNMAP : return nvkm_uvmm_mthd_unmap (uvmm, argv, argc);\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\nstatic void *\nnvkm_uvmm_dtor(struct nvkm_object *object)\n{\n\tstruct nvkm_uvmm *uvmm = nvkm_uvmm(object);\n\tnvkm_vmm_unref(&uvmm->vmm);\n\treturn uvmm;\n}\n\nstatic const struct nvkm_object_func\nnvkm_uvmm = {\n\t.dtor = nvkm_uvmm_dtor,\n\t.mthd = nvkm_uvmm_mthd,\n};\n\nint\nnvkm_uvmm_new(const struct nvkm_oclass *oclass, void *argv, u32 argc,\n\t      struct nvkm_object **pobject)\n{\n\tstruct nvkm_mmu *mmu = nvkm_ummu(oclass->parent)->mmu;\n\tconst bool more = oclass->base.maxver >= 0;\n\tunion {\n\t\tstruct nvif_vmm_v0 v0;\n\t} *args = argv;\n\tconst struct nvkm_vmm_page *page;\n\tstruct nvkm_uvmm *uvmm;\n\tint ret = -ENOSYS;\n\tu64 addr, size;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, more))) {\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t} else\n\t\treturn ret;\n\n\tif (!(uvmm = kzalloc(sizeof(*uvmm), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\tnvkm_object_ctor(&nvkm_uvmm, oclass, &uvmm->object);\n\t*pobject = &uvmm->object;\n\n\tif (!mmu->vmm) {\n\t\tret = mmu->func->vmm.ctor(mmu, addr, size, argv, argc,\n\t\t\t\t\t  NULL, \"user\", &uvmm->vmm);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tuvmm->vmm->debug = max(uvmm->vmm->debug, oclass->client->debug);\n\t} else {\n\t\tif (size)\n\t\t\treturn -EINVAL;\n\n\t\tuvmm->vmm = nvkm_vmm_ref(mmu->vmm);\n\t}\n\n\tpage = uvmm->vmm->func->page;\n\targs->v0.page_nr = 0;\n\twhile (page && (page++)->shift)\n\t\targs->v0.page_nr++;\n\targs->v0.addr = uvmm->vmm->start;\n\targs->v0.size = uvmm->vmm->limit;\n\treturn 0;\n}\n", "/*\n * Copyright 2017 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n#define NVKM_VMM_LEVELS_MAX 5\n#include \"vmm.h\"\n\n#include <subdev/fb.h>\n\nstatic void\nnvkm_vmm_pt_del(struct nvkm_vmm_pt **ppgt)\n{\n\tstruct nvkm_vmm_pt *pgt = *ppgt;\n\tif (pgt) {\n\t\tkvfree(pgt->pde);\n\t\tkfree(pgt);\n\t\t*ppgt = NULL;\n\t}\n}\n\n\nstatic struct nvkm_vmm_pt *\nnvkm_vmm_pt_new(const struct nvkm_vmm_desc *desc, bool sparse,\n\t\tconst struct nvkm_vmm_page *page)\n{\n\tconst u32 pten = 1 << desc->bits;\n\tstruct nvkm_vmm_pt *pgt;\n\tu32 lpte = 0;\n\n\tif (desc->type > PGT) {\n\t\tif (desc->type == SPT) {\n\t\t\tconst struct nvkm_vmm_desc *pair = page[-1].desc;\n\t\t\tlpte = pten >> (desc->bits - pair->bits);\n\t\t} else {\n\t\t\tlpte = pten;\n\t\t}\n\t}\n\n\tif (!(pgt = kzalloc(sizeof(*pgt) + lpte, GFP_KERNEL)))\n\t\treturn NULL;\n\tpgt->page = page ? page->shift : 0;\n\tpgt->sparse = sparse;\n\n\tif (desc->type == PGD) {\n\t\tpgt->pde = kvcalloc(pten, sizeof(*pgt->pde), GFP_KERNEL);\n\t\tif (!pgt->pde) {\n\t\t\tkfree(pgt);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn pgt;\n}\n\nstruct nvkm_vmm_iter {\n\tconst struct nvkm_vmm_page *page;\n\tconst struct nvkm_vmm_desc *desc;\n\tstruct nvkm_vmm *vmm;\n\tu64 cnt;\n\tu16 max, lvl;\n\tu32 pte[NVKM_VMM_LEVELS_MAX];\n\tstruct nvkm_vmm_pt *pt[NVKM_VMM_LEVELS_MAX];\n\tint flush;\n};\n\n#ifdef CONFIG_NOUVEAU_DEBUG_MMU\nstatic const char *\nnvkm_vmm_desc_type(const struct nvkm_vmm_desc *desc)\n{\n\tswitch (desc->type) {\n\tcase PGD: return \"PGD\";\n\tcase PGT: return \"PGT\";\n\tcase SPT: return \"SPT\";\n\tcase LPT: return \"LPT\";\n\tdefault:\n\t\treturn \"UNKNOWN\";\n\t}\n}\n\nstatic void\nnvkm_vmm_trace(struct nvkm_vmm_iter *it, char *buf)\n{\n\tint lvl;\n\tfor (lvl = it->max; lvl >= 0; lvl--) {\n\t\tif (lvl >= it->lvl)\n\t\t\tbuf += sprintf(buf,  \"%05x:\", it->pte[lvl]);\n\t\telse\n\t\t\tbuf += sprintf(buf, \"xxxxx:\");\n\t}\n}\n\n#define TRA(i,f,a...) do {                                                     \\\n\tchar _buf[NVKM_VMM_LEVELS_MAX * 7];                                    \\\n\tstruct nvkm_vmm_iter *_it = (i);                                       \\\n\tnvkm_vmm_trace(_it, _buf);                                             \\\n\tVMM_TRACE(_it->vmm, \"%s \"f, _buf, ##a);                                \\\n} while(0)\n#else\n#define TRA(i,f,a...)\n#endif\n\nstatic inline void\nnvkm_vmm_flush_mark(struct nvkm_vmm_iter *it)\n{\n\tit->flush = min(it->flush, it->max - it->lvl);\n}\n\nstatic inline void\nnvkm_vmm_flush(struct nvkm_vmm_iter *it)\n{\n\tif (it->flush != NVKM_VMM_LEVELS_MAX) {\n\t\tif (it->vmm->func->flush) {\n\t\t\tTRA(it, \"flush: %d\", it->flush);\n\t\t\tit->vmm->func->flush(it->vmm, it->flush);\n\t\t}\n\t\tit->flush = NVKM_VMM_LEVELS_MAX;\n\t}\n}\n\nstatic void\nnvkm_vmm_unref_pdes(struct nvkm_vmm_iter *it)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc[it->lvl].type == SPT;\n\tstruct nvkm_vmm_pt *pgd = it->pt[it->lvl + 1];\n\tstruct nvkm_vmm_pt *pgt = it->pt[it->lvl];\n\tstruct nvkm_mmu_pt *pt = pgt->pt[type];\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tu32 pdei = it->pte[it->lvl + 1];\n\n\t/* Recurse up the tree, unreferencing/destroying unneeded PDs. */\n\tit->lvl++;\n\tif (--pgd->refs[0]) {\n\t\tconst struct nvkm_vmm_desc_func *func = desc[it->lvl].func;\n\t\t/* PD has other valid PDEs, so we need a proper update. */\n\t\tTRA(it, \"PDE unmap %s\", nvkm_vmm_desc_type(&desc[it->lvl - 1]));\n\t\tpgt->pt[type] = NULL;\n\t\tif (!pgt->refs[!type]) {\n\t\t\t/* PDE no longer required. */\n\t\t\tif (pgd->pt[0]) {\n\t\t\t\tif (pgt->sparse) {\n\t\t\t\t\tfunc->sparse(vmm, pgd->pt[0], pdei, 1);\n\t\t\t\t\tpgd->pde[pdei] = NVKM_VMM_PDE_SPARSE;\n\t\t\t\t} else {\n\t\t\t\t\tfunc->unmap(vmm, pgd->pt[0], pdei, 1);\n\t\t\t\t\tpgd->pde[pdei] = NULL;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* Special handling for Tesla-class GPUs,\n\t\t\t\t * where there's no central PD, but each\n\t\t\t\t * instance has its own embedded PD.\n\t\t\t\t */\n\t\t\t\tfunc->pde(vmm, pgd, pdei);\n\t\t\t\tpgd->pde[pdei] = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\t/* PDE was pointing at dual-PTs and we're removing\n\t\t\t * one of them, leaving the other in place.\n\t\t\t */\n\t\t\tfunc->pde(vmm, pgd, pdei);\n\t\t}\n\n\t\t/* GPU may have cached the PTs, flush before freeing. */\n\t\tnvkm_vmm_flush_mark(it);\n\t\tnvkm_vmm_flush(it);\n\t} else {\n\t\t/* PD has no valid PDEs left, so we can just destroy it. */\n\t\tnvkm_vmm_unref_pdes(it);\n\t}\n\n\t/* Destroy PD/PT. */\n\tTRA(it, \"PDE free %s\", nvkm_vmm_desc_type(&desc[it->lvl - 1]));\n\tnvkm_mmu_ptc_put(vmm->mmu, vmm->bootstrapped, &pt);\n\tif (!pgt->refs[!type])\n\t\tnvkm_vmm_pt_del(&pgt);\n\tit->lvl--;\n}\n\nstatic void\nnvkm_vmm_unref_sptes(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgt,\n\t\t     const struct nvkm_vmm_desc *desc, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *pair = it->page[-1].desc;\n\tconst u32 sptb = desc->bits - pair->bits;\n\tconst u32 sptn = 1 << sptb;\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tu32 spti = ptei & (sptn - 1), lpti, pteb;\n\n\t/* Determine how many SPTEs are being touched under each LPTE,\n\t * and drop reference counts.\n\t */\n\tfor (lpti = ptei >> sptb; ptes; spti = 0, lpti++) {\n\t\tconst u32 pten = min(sptn - spti, ptes);\n\t\tpgt->pte[lpti] -= pten;\n\t\tptes -= pten;\n\t}\n\n\t/* We're done here if there's no corresponding LPT. */\n\tif (!pgt->refs[0])\n\t\treturn;\n\n\tfor (ptei = pteb = ptei >> sptb; ptei < lpti; pteb = ptei) {\n\t\t/* Skip over any LPTEs that still have valid SPTEs. */\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_SPTES) {\n\t\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\t\tif (!(pgt->pte[ptei] & NVKM_VMM_PTE_SPTES))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* As there's no more non-UNMAPPED SPTEs left in the range\n\t\t * covered by a number of LPTEs, the LPTEs once again take\n\t\t * control over their address range.\n\t\t *\n\t\t * Determine how many LPTEs need to transition state.\n\t\t */\n\t\tpgt->pte[ptei] &= ~NVKM_VMM_PTE_VALID;\n\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\tif (pgt->pte[ptei] & NVKM_VMM_PTE_SPTES)\n\t\t\t\tbreak;\n\t\t\tpgt->pte[ptei] &= ~NVKM_VMM_PTE_VALID;\n\t\t}\n\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_SPARSE) {\n\t\t\tTRA(it, \"LPTE %05x: U -> S %d PTEs\", pteb, ptes);\n\t\t\tpair->func->sparse(vmm, pgt->pt[0], pteb, ptes);\n\t\t} else\n\t\tif (pair->func->invalid) {\n\t\t\t/* If the MMU supports it, restore the LPTE to the\n\t\t\t * INVALID state to tell the MMU there is no point\n\t\t\t * trying to fetch the corresponding SPTEs.\n\t\t\t */\n\t\t\tTRA(it, \"LPTE %05x: U -> I %d PTEs\", pteb, ptes);\n\t\t\tpair->func->invalid(vmm, pgt->pt[0], pteb, ptes);\n\t\t}\n\t}\n}\n\nstatic bool\nnvkm_vmm_unref_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc->type == SPT;\n\tstruct nvkm_vmm_pt *pgt = it->pt[0];\n\n\t/* Drop PTE references. */\n\tpgt->refs[type] -= ptes;\n\n\t/* Dual-PTs need special handling, unless PDE becoming invalid. */\n\tif (desc->type == SPT && (pgt->refs[0] || pgt->refs[1]))\n\t\tnvkm_vmm_unref_sptes(it, pgt, desc, ptei, ptes);\n\n\t/* PT no longer neeed?  Destroy it. */\n\tif (!pgt->refs[type]) {\n\t\tit->lvl++;\n\t\tTRA(it, \"%s empty\", nvkm_vmm_desc_type(desc));\n\t\tit->lvl--;\n\t\tnvkm_vmm_unref_pdes(it);\n\t\treturn false; /* PTE writes for unmap() not necessary. */\n\t}\n\n\treturn true;\n}\n\nstatic void\nnvkm_vmm_ref_sptes(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgt,\n\t\t   const struct nvkm_vmm_desc *desc, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *pair = it->page[-1].desc;\n\tconst u32 sptb = desc->bits - pair->bits;\n\tconst u32 sptn = 1 << sptb;\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tu32 spti = ptei & (sptn - 1), lpti, pteb;\n\n\t/* Determine how many SPTEs are being touched under each LPTE,\n\t * and increase reference counts.\n\t */\n\tfor (lpti = ptei >> sptb; ptes; spti = 0, lpti++) {\n\t\tconst u32 pten = min(sptn - spti, ptes);\n\t\tpgt->pte[lpti] += pten;\n\t\tptes -= pten;\n\t}\n\n\t/* We're done here if there's no corresponding LPT. */\n\tif (!pgt->refs[0])\n\t\treturn;\n\n\tfor (ptei = pteb = ptei >> sptb; ptei < lpti; pteb = ptei) {\n\t\t/* Skip over any LPTEs that already have valid SPTEs. */\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_VALID) {\n\t\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\t\tif (!(pgt->pte[ptei] & NVKM_VMM_PTE_VALID))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* As there are now non-UNMAPPED SPTEs in the range covered\n\t\t * by a number of LPTEs, we need to transfer control of the\n\t\t * address range to the SPTEs.\n\t\t *\n\t\t * Determine how many LPTEs need to transition state.\n\t\t */\n\t\tpgt->pte[ptei] |= NVKM_VMM_PTE_VALID;\n\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\tif (pgt->pte[ptei] & NVKM_VMM_PTE_VALID)\n\t\t\t\tbreak;\n\t\t\tpgt->pte[ptei] |= NVKM_VMM_PTE_VALID;\n\t\t}\n\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_SPARSE) {\n\t\t\tconst u32 spti = pteb * sptn;\n\t\t\tconst u32 sptc = ptes * sptn;\n\t\t\t/* The entire LPTE is marked as sparse, we need\n\t\t\t * to make sure that the SPTEs are too.\n\t\t\t */\n\t\t\tTRA(it, \"SPTE %05x: U -> S %d PTEs\", spti, sptc);\n\t\t\tdesc->func->sparse(vmm, pgt->pt[1], spti, sptc);\n\t\t\t/* Sparse LPTEs prevent SPTEs from being accessed. */\n\t\t\tTRA(it, \"LPTE %05x: S -> U %d PTEs\", pteb, ptes);\n\t\t\tpair->func->unmap(vmm, pgt->pt[0], pteb, ptes);\n\t\t} else\n\t\tif (pair->func->invalid) {\n\t\t\t/* MMU supports blocking SPTEs by marking an LPTE\n\t\t\t * as INVALID.  We need to reverse that here.\n\t\t\t */\n\t\t\tTRA(it, \"LPTE %05x: I -> U %d PTEs\", pteb, ptes);\n\t\t\tpair->func->unmap(vmm, pgt->pt[0], pteb, ptes);\n\t\t}\n\t}\n}\n\nstatic bool\nnvkm_vmm_ref_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc->type == SPT;\n\tstruct nvkm_vmm_pt *pgt = it->pt[0];\n\n\t/* Take PTE references. */\n\tpgt->refs[type] += ptes;\n\n\t/* Dual-PTs need special handling. */\n\tif (desc->type == SPT)\n\t\tnvkm_vmm_ref_sptes(it, pgt, desc, ptei, ptes);\n\n\treturn true;\n}\n\nstatic void\nnvkm_vmm_sparse_ptes(const struct nvkm_vmm_desc *desc,\n\t\t     struct nvkm_vmm_pt *pgt, u32 ptei, u32 ptes)\n{\n\tif (desc->type == PGD) {\n\t\twhile (ptes--)\n\t\t\tpgt->pde[ptei++] = NVKM_VMM_PDE_SPARSE;\n\t} else\n\tif (desc->type == LPT) {\n\t\tmemset(&pgt->pte[ptei], NVKM_VMM_PTE_SPARSE, ptes);\n\t}\n}\n\nstatic bool\nnvkm_vmm_sparse_unref_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tstruct nvkm_vmm_pt *pt = it->pt[0];\n\tif (it->desc->type == PGD)\n\t\tmemset(&pt->pde[ptei], 0x00, sizeof(pt->pde[0]) * ptes);\n\telse\n\tif (it->desc->type == LPT)\n\t\tmemset(&pt->pte[ptei], 0x00, sizeof(pt->pte[0]) * ptes);\n\treturn nvkm_vmm_unref_ptes(it, ptei, ptes);\n}\n\nstatic bool\nnvkm_vmm_sparse_ref_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tnvkm_vmm_sparse_ptes(it->desc, it->pt[0], ptei, ptes);\n\treturn nvkm_vmm_ref_ptes(it, ptei, ptes);\n}\n\nstatic bool\nnvkm_vmm_ref_hwpt(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgd, u32 pdei)\n{\n\tconst struct nvkm_vmm_desc *desc = &it->desc[it->lvl - 1];\n\tconst int type = desc->type == SPT;\n\tstruct nvkm_vmm_pt *pgt = pgd->pde[pdei];\n\tconst bool zero = !pgt->sparse && !desc->func->invalid;\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tstruct nvkm_mmu *mmu = vmm->mmu;\n\tstruct nvkm_mmu_pt *pt;\n\tu32 pten = 1 << desc->bits;\n\tu32 pteb, ptei, ptes;\n\tu32 size = desc->size * pten;\n\n\tpgd->refs[0]++;\n\n\tpgt->pt[type] = nvkm_mmu_ptc_get(mmu, size, desc->align, zero);\n\tif (!pgt->pt[type]) {\n\t\tit->lvl--;\n\t\tnvkm_vmm_unref_pdes(it);\n\t\treturn false;\n\t}\n\n\tif (zero)\n\t\tgoto done;\n\n\tpt = pgt->pt[type];\n\n\tif (desc->type == LPT && pgt->refs[1]) {\n\t\t/* SPT already exists covering the same range as this LPT,\n\t\t * which means we need to be careful that any LPTEs which\n\t\t * overlap valid SPTEs are unmapped as opposed to invalid\n\t\t * or sparse, which would prevent the MMU from looking at\n\t\t * the SPTEs on some GPUs.\n\t\t */\n\t\tfor (ptei = pteb = 0; ptei < pten; pteb = ptei) {\n\t\t\tbool spte = pgt->pte[ptei] & NVKM_VMM_PTE_SPTES;\n\t\t\tfor (ptes = 1, ptei++; ptei < pten; ptes++, ptei++) {\n\t\t\t\tbool next = pgt->pte[ptei] & NVKM_VMM_PTE_SPTES;\n\t\t\t\tif (spte != next)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!spte) {\n\t\t\t\tif (pgt->sparse)\n\t\t\t\t\tdesc->func->sparse(vmm, pt, pteb, ptes);\n\t\t\t\telse\n\t\t\t\t\tdesc->func->invalid(vmm, pt, pteb, ptes);\n\t\t\t\tmemset(&pgt->pte[pteb], 0x00, ptes);\n\t\t\t} else {\n\t\t\t\tdesc->func->unmap(vmm, pt, pteb, ptes);\n\t\t\t\twhile (ptes--)\n\t\t\t\t\tpgt->pte[pteb++] |= NVKM_VMM_PTE_VALID;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (pgt->sparse) {\n\t\t\tnvkm_vmm_sparse_ptes(desc, pgt, 0, pten);\n\t\t\tdesc->func->sparse(vmm, pt, 0, pten);\n\t\t} else {\n\t\t\tdesc->func->invalid(vmm, pt, 0, pten);\n\t\t}\n\t}\n\ndone:\n\tTRA(it, \"PDE write %s\", nvkm_vmm_desc_type(desc));\n\tit->desc[it->lvl].func->pde(it->vmm, pgd, pdei);\n\tnvkm_vmm_flush_mark(it);\n\treturn true;\n}\n\nstatic bool\nnvkm_vmm_ref_swpt(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgd, u32 pdei)\n{\n\tconst struct nvkm_vmm_desc *desc = &it->desc[it->lvl - 1];\n\tstruct nvkm_vmm_pt *pgt = pgd->pde[pdei];\n\n\tpgt = nvkm_vmm_pt_new(desc, NVKM_VMM_PDE_SPARSED(pgt), it->page);\n\tif (!pgt) {\n\t\tif (!pgd->refs[0])\n\t\t\tnvkm_vmm_unref_pdes(it);\n\t\treturn false;\n\t}\n\n\tpgd->pde[pdei] = pgt;\n\treturn true;\n}\n\nstatic inline u64\nnvkm_vmm_iter(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t      u64 addr, u64 size, const char *name, bool ref,\n\t      bool (*REF_PTES)(struct nvkm_vmm_iter *, u32, u32),\n\t      nvkm_vmm_pte_func MAP_PTES, struct nvkm_vmm_map *map,\n\t      nvkm_vmm_pxe_func CLR_PTES)\n{\n\tconst struct nvkm_vmm_desc *desc = page->desc;\n\tstruct nvkm_vmm_iter it;\n\tu64 bits = addr >> page->shift;\n\n\tit.page = page;\n\tit.desc = desc;\n\tit.vmm = vmm;\n\tit.cnt = size >> page->shift;\n\tit.flush = NVKM_VMM_LEVELS_MAX;\n\n\t/* Deconstruct address into PTE indices for each mapping level. */\n\tfor (it.lvl = 0; desc[it.lvl].bits; it.lvl++) {\n\t\tit.pte[it.lvl] = bits & ((1 << desc[it.lvl].bits) - 1);\n\t\tbits >>= desc[it.lvl].bits;\n\t}\n\tit.max = --it.lvl;\n\tit.pt[it.max] = vmm->pd;\n\n\tit.lvl = 0;\n\tTRA(&it, \"%s: %016llx %016llx %d %lld PTEs\", name,\n\t         addr, size, page->shift, it.cnt);\n\tit.lvl = it.max;\n\n\t/* Depth-first traversal of page tables. */\n\twhile (it.cnt) {\n\t\tstruct nvkm_vmm_pt *pgt = it.pt[it.lvl];\n\t\tconst int type = desc->type == SPT;\n\t\tconst u32 pten = 1 << desc->bits;\n\t\tconst u32 ptei = it.pte[0];\n\t\tconst u32 ptes = min_t(u64, it.cnt, pten - ptei);\n\n\t\t/* Walk down the tree, finding page tables for each level. */\n\t\tfor (; it.lvl; it.lvl--) {\n\t\t\tconst u32 pdei = it.pte[it.lvl];\n\t\t\tstruct nvkm_vmm_pt *pgd = pgt;\n\n\t\t\t/* Software PT. */\n\t\t\tif (ref && NVKM_VMM_PDE_INVALID(pgd->pde[pdei])) {\n\t\t\t\tif (!nvkm_vmm_ref_swpt(&it, pgd, pdei))\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\tit.pt[it.lvl - 1] = pgt = pgd->pde[pdei];\n\n\t\t\t/* Hardware PT.\n\t\t\t *\n\t\t\t * This is a separate step from above due to GF100 and\n\t\t\t * newer having dual page tables at some levels, which\n\t\t\t * are refcounted independently.\n\t\t\t */\n\t\t\tif (ref && !pgt->refs[desc[it.lvl - 1].type == SPT]) {\n\t\t\t\tif (!nvkm_vmm_ref_hwpt(&it, pgd, pdei))\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\t\t}\n\n\t\t/* Handle PTE updates. */\n\t\tif (!REF_PTES || REF_PTES(&it, ptei, ptes)) {\n\t\t\tstruct nvkm_mmu_pt *pt = pgt->pt[type];\n\t\t\tif (MAP_PTES || CLR_PTES) {\n\t\t\t\tif (MAP_PTES)\n\t\t\t\t\tMAP_PTES(vmm, pt, ptei, ptes, map);\n\t\t\t\telse\n\t\t\t\t\tCLR_PTES(vmm, pt, ptei, ptes);\n\t\t\t\tnvkm_vmm_flush_mark(&it);\n\t\t\t}\n\t\t}\n\n\t\t/* Walk back up the tree to the next position. */\n\t\tit.pte[it.lvl] += ptes;\n\t\tit.cnt -= ptes;\n\t\tif (it.cnt) {\n\t\t\twhile (it.pte[it.lvl] == (1 << desc[it.lvl].bits)) {\n\t\t\t\tit.pte[it.lvl++] = 0;\n\t\t\t\tit.pte[it.lvl]++;\n\t\t\t}\n\t\t}\n\t};\n\n\tnvkm_vmm_flush(&it);\n\treturn ~0ULL;\n\nfail:\n\t/* Reconstruct the failure address so the caller is able to\n\t * reverse any partially completed operations.\n\t */\n\taddr = it.pte[it.max--];\n\tdo {\n\t\taddr  = addr << desc[it.max].bits;\n\t\taddr |= it.pte[it.max];\n\t} while (it.max--);\n\n\treturn addr << page->shift;\n}\n\nstatic void\nnvkm_vmm_ptes_sparse_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\t u64 addr, u64 size)\n{\n\tnvkm_vmm_iter(vmm, page, addr, size, \"sparse unref\", false,\n\t\t      nvkm_vmm_sparse_unref_ptes, NULL, NULL,\n\t\t      page->desc->func->invalid ?\n\t\t      page->desc->func->invalid : page->desc->func->unmap);\n}\n\nstatic int\nnvkm_vmm_ptes_sparse_get(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\t u64 addr, u64 size)\n{\n\tif ((page->type & NVKM_VMM_PAGE_SPARSE)) {\n\t\tu64 fail = nvkm_vmm_iter(vmm, page, addr, size, \"sparse ref\",\n\t\t\t\t\t true, nvkm_vmm_sparse_ref_ptes, NULL,\n\t\t\t\t\t NULL, page->desc->func->sparse);\n\t\tif (fail != ~0ULL) {\n\t\t\tif ((size = fail - addr))\n\t\t\t\tnvkm_vmm_ptes_sparse_put(vmm, page, addr, size);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int\nnvkm_vmm_ptes_sparse(struct nvkm_vmm *vmm, u64 addr, u64 size, bool ref)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tint m = 0, i;\n\tu64 start = addr;\n\tu64 block;\n\n\twhile (size) {\n\t\t/* Limit maximum page size based on remaining size. */\n\t\twhile (size < (1ULL << page[m].shift))\n\t\t\tm++;\n\t\ti = m;\n\n\t\t/* Find largest page size suitable for alignment. */\n\t\twhile (!IS_ALIGNED(addr, 1ULL << page[i].shift))\n\t\t\ti++;\n\n\t\t/* Determine number of PTEs at this page size. */\n\t\tif (i != m) {\n\t\t\t/* Limited to alignment boundary of next page size. */\n\t\t\tu64 next = 1ULL << page[i - 1].shift;\n\t\t\tu64 part = ALIGN(addr, next) - addr;\n\t\t\tif (size - part >= next)\n\t\t\t\tblock = (part >> page[i].shift) << page[i].shift;\n\t\t\telse\n\t\t\t\tblock = (size >> page[i].shift) << page[i].shift;\n\t\t} else {\n\t\t\tblock = (size >> page[i].shift) << page[i].shift;\n\t\t}\n\n\t\t/* Perform operation. */\n\t\tif (ref) {\n\t\t\tint ret = nvkm_vmm_ptes_sparse_get(vmm, &page[i], addr, block);\n\t\t\tif (ret) {\n\t\t\t\tif ((size = addr - start))\n\t\t\t\t\tnvkm_vmm_ptes_sparse(vmm, start, size, false);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t} else {\n\t\t\tnvkm_vmm_ptes_sparse_put(vmm, &page[i], addr, block);\n\t\t}\n\n\t\tsize -= block;\n\t\taddr += block;\n\t}\n\n\treturn 0;\n}\n\nstatic void\nnvkm_vmm_ptes_unmap_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\tu64 addr, u64 size, bool sparse)\n{\n\tconst struct nvkm_vmm_desc_func *func = page->desc->func;\n\tnvkm_vmm_iter(vmm, page, addr, size, \"unmap + unref\",\n\t\t      false, nvkm_vmm_unref_ptes, NULL, NULL,\n\t\t      sparse ? func->sparse : func->invalid ? func->invalid :\n\t\t\t\t\t\t\t      func->unmap);\n}\n\nstatic int\nnvkm_vmm_ptes_get_map(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t      u64 addr, u64 size, struct nvkm_vmm_map *map,\n\t\t      nvkm_vmm_pte_func func)\n{\n\tu64 fail = nvkm_vmm_iter(vmm, page, addr, size, \"ref + map\", true,\n\t\t\t\t nvkm_vmm_ref_ptes, func, map, NULL);\n\tif (fail != ~0ULL) {\n\t\tif ((size = fail - addr))\n\t\t\tnvkm_vmm_ptes_unmap_put(vmm, page, addr, size, false);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void\nnvkm_vmm_ptes_unmap(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t    u64 addr, u64 size, bool sparse)\n{\n\tconst struct nvkm_vmm_desc_func *func = page->desc->func;\n\tnvkm_vmm_iter(vmm, page, addr, size, \"unmap\", false, NULL, NULL, NULL,\n\t\t      sparse ? func->sparse : func->invalid ? func->invalid :\n\t\t\t\t\t\t\t      func->unmap);\n}\n\nstatic void\nnvkm_vmm_ptes_map(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t  u64 addr, u64 size, struct nvkm_vmm_map *map,\n\t\t  nvkm_vmm_pte_func func)\n{\n\tnvkm_vmm_iter(vmm, page, addr, size, \"map\", false,\n\t\t      NULL, func, map, NULL);\n}\n\nstatic void\nnvkm_vmm_ptes_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t  u64 addr, u64 size)\n{\n\tnvkm_vmm_iter(vmm, page, addr, size, \"unref\", false,\n\t\t      nvkm_vmm_unref_ptes, NULL, NULL, NULL);\n}\n\nstatic int\nnvkm_vmm_ptes_get(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t  u64 addr, u64 size)\n{\n\tu64 fail = nvkm_vmm_iter(vmm, page, addr, size, \"ref\", true,\n\t\t\t\t nvkm_vmm_ref_ptes, NULL, NULL, NULL);\n\tif (fail != ~0ULL) {\n\t\tif (fail != addr)\n\t\t\tnvkm_vmm_ptes_put(vmm, page, addr, fail - addr);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic inline struct nvkm_vma *\nnvkm_vma_new(u64 addr, u64 size)\n{\n\tstruct nvkm_vma *vma = kzalloc(sizeof(*vma), GFP_KERNEL);\n\tif (vma) {\n\t\tvma->addr = addr;\n\t\tvma->size = size;\n\t\tvma->page = NVKM_VMA_PAGE_NONE;\n\t\tvma->refd = NVKM_VMA_PAGE_NONE;\n\t}\n\treturn vma;\n}\n\nstruct nvkm_vma *\nnvkm_vma_tail(struct nvkm_vma *vma, u64 tail)\n{\n\tstruct nvkm_vma *new;\n\n\tBUG_ON(vma->size == tail);\n\n\tif (!(new = nvkm_vma_new(vma->addr + (vma->size - tail), tail)))\n\t\treturn NULL;\n\tvma->size -= tail;\n\n\tnew->mapref = vma->mapref;\n\tnew->sparse = vma->sparse;\n\tnew->page = vma->page;\n\tnew->refd = vma->refd;\n\tnew->used = vma->used;\n\tnew->part = vma->part;\n\tnew->user = vma->user;\n\tnew->busy = vma->busy;\n\tlist_add(&new->head, &vma->head);\n\treturn new;\n}\n\nstatic void\nnvkm_vmm_free_insert(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct rb_node **ptr = &vmm->free.rb_node;\n\tstruct rb_node *parent = NULL;\n\n\twhile (*ptr) {\n\t\tstruct nvkm_vma *this = rb_entry(*ptr, typeof(*this), tree);\n\t\tparent = *ptr;\n\t\tif (vma->size < this->size)\n\t\t\tptr = &parent->rb_left;\n\t\telse\n\t\tif (vma->size > this->size)\n\t\t\tptr = &parent->rb_right;\n\t\telse\n\t\tif (vma->addr < this->addr)\n\t\t\tptr = &parent->rb_left;\n\t\telse\n\t\tif (vma->addr > this->addr)\n\t\t\tptr = &parent->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->tree, parent, ptr);\n\trb_insert_color(&vma->tree, &vmm->free);\n}\n\nvoid\nnvkm_vmm_node_insert(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct rb_node **ptr = &vmm->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\n\twhile (*ptr) {\n\t\tstruct nvkm_vma *this = rb_entry(*ptr, typeof(*this), tree);\n\t\tparent = *ptr;\n\t\tif (vma->addr < this->addr)\n\t\t\tptr = &parent->rb_left;\n\t\telse\n\t\tif (vma->addr > this->addr)\n\t\t\tptr = &parent->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->tree, parent, ptr);\n\trb_insert_color(&vma->tree, &vmm->root);\n}\n\nstruct nvkm_vma *\nnvkm_vmm_node_search(struct nvkm_vmm *vmm, u64 addr)\n{\n\tstruct rb_node *node = vmm->root.rb_node;\n\twhile (node) {\n\t\tstruct nvkm_vma *vma = rb_entry(node, typeof(*vma), tree);\n\t\tif (addr < vma->addr)\n\t\t\tnode = node->rb_left;\n\t\telse\n\t\tif (addr >= vma->addr + vma->size)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn vma;\n\t}\n\treturn NULL;\n}\n\nstatic void\nnvkm_vmm_dtor(struct nvkm_vmm *vmm)\n{\n\tstruct nvkm_vma *vma;\n\tstruct rb_node *node;\n\n\twhile ((node = rb_first(&vmm->root))) {\n\t\tstruct nvkm_vma *vma = rb_entry(node, typeof(*vma), tree);\n\t\tnvkm_vmm_put(vmm, &vma);\n\t}\n\n\tif (vmm->bootstrapped) {\n\t\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\t\tconst u64 limit = vmm->limit - vmm->start;\n\n\t\twhile (page[1].shift)\n\t\t\tpage++;\n\n\t\tnvkm_mmu_ptc_dump(vmm->mmu);\n\t\tnvkm_vmm_ptes_put(vmm, page, vmm->start, limit);\n\t}\n\n\tvma = list_first_entry(&vmm->list, typeof(*vma), head);\n\tlist_del(&vma->head);\n\tkfree(vma);\n\tWARN_ON(!list_empty(&vmm->list));\n\n\tif (vmm->nullp) {\n\t\tdma_free_coherent(vmm->mmu->subdev.device->dev, 16 * 1024,\n\t\t\t\t  vmm->nullp, vmm->null);\n\t}\n\n\tif (vmm->pd) {\n\t\tnvkm_mmu_ptc_put(vmm->mmu, true, &vmm->pd->pt[0]);\n\t\tnvkm_vmm_pt_del(&vmm->pd);\n\t}\n}\n\nint\nnvkm_vmm_ctor(const struct nvkm_vmm_func *func, struct nvkm_mmu *mmu,\n\t      u32 pd_header, u64 addr, u64 size, struct lock_class_key *key,\n\t      const char *name, struct nvkm_vmm *vmm)\n{\n\tstatic struct lock_class_key _key;\n\tconst struct nvkm_vmm_page *page = func->page;\n\tconst struct nvkm_vmm_desc *desc;\n\tstruct nvkm_vma *vma;\n\tint levels, bits = 0;\n\n\tvmm->func = func;\n\tvmm->mmu = mmu;\n\tvmm->name = name;\n\tvmm->debug = mmu->subdev.debug;\n\tkref_init(&vmm->kref);\n\n\t__mutex_init(&vmm->mutex, \"&vmm->mutex\", key ? key : &_key);\n\n\t/* Locate the smallest page size supported by the backend, it will\n\t * have the the deepest nesting of page tables.\n\t */\n\twhile (page[1].shift)\n\t\tpage++;\n\n\t/* Locate the structure that describes the layout of the top-level\n\t * page table, and determine the number of valid bits in a virtual\n\t * address.\n\t */\n\tfor (levels = 0, desc = page->desc; desc->bits; desc++, levels++)\n\t\tbits += desc->bits;\n\tbits += page->shift;\n\tdesc--;\n\n\tif (WARN_ON(levels > NVKM_VMM_LEVELS_MAX))\n\t\treturn -EINVAL;\n\n\tvmm->start = addr;\n\tvmm->limit = size ? (addr + size) : (1ULL << bits);\n\tif (vmm->start > vmm->limit || vmm->limit > (1ULL << bits))\n\t\treturn -EINVAL;\n\n\t/* Allocate top-level page table. */\n\tvmm->pd = nvkm_vmm_pt_new(desc, false, NULL);\n\tif (!vmm->pd)\n\t\treturn -ENOMEM;\n\tvmm->pd->refs[0] = 1;\n\tINIT_LIST_HEAD(&vmm->join);\n\n\t/* ... and the GPU storage for it, except on Tesla-class GPUs that\n\t * have the PD embedded in the instance structure.\n\t */\n\tif (desc->size) {\n\t\tconst u32 size = pd_header + desc->size * (1 << desc->bits);\n\t\tvmm->pd->pt[0] = nvkm_mmu_ptc_get(mmu, size, desc->align, true);\n\t\tif (!vmm->pd->pt[0])\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Initialise address-space MM. */\n\tINIT_LIST_HEAD(&vmm->list);\n\tvmm->free = RB_ROOT;\n\tvmm->root = RB_ROOT;\n\n\tif (!(vma = nvkm_vma_new(vmm->start, vmm->limit - vmm->start)))\n\t\treturn -ENOMEM;\n\n\tnvkm_vmm_free_insert(vmm, vma);\n\tlist_add(&vma->head, &vmm->list);\n\treturn 0;\n}\n\nint\nnvkm_vmm_new_(const struct nvkm_vmm_func *func, struct nvkm_mmu *mmu,\n\t      u32 hdr, u64 addr, u64 size, struct lock_class_key *key,\n\t      const char *name, struct nvkm_vmm **pvmm)\n{\n\tif (!(*pvmm = kzalloc(sizeof(**pvmm), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\treturn nvkm_vmm_ctor(func, mmu, hdr, addr, size, key, name, *pvmm);\n}\n\n#define node(root, dir) ((root)->head.dir == &vmm->list) ? NULL :              \\\n\tlist_entry((root)->head.dir, struct nvkm_vma, head)\n\nvoid\nnvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *next;\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\n\tif (vma->part) {\n\t\tstruct nvkm_vma *prev = node(vma, prev);\n\t\tif (!prev->memory) {\n\t\t\tprev->size += vma->size;\n\t\t\trb_erase(&vma->tree, &vmm->root);\n\t\t\tlist_del(&vma->head);\n\t\t\tkfree(vma);\n\t\t\tvma = prev;\n\t\t}\n\t}\n\n\tnext = node(vma, next);\n\tif (next && next->part) {\n\t\tif (!next->memory) {\n\t\t\tvma->size += next->size;\n\t\t\trb_erase(&next->tree, &vmm->root);\n\t\t\tlist_del(&next->head);\n\t\t\tkfree(next);\n\t\t}\n\t}\n}\n\nvoid\nnvkm_vmm_unmap_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[vma->refd];\n\n\tif (vma->mapref) {\n\t\tnvkm_vmm_ptes_unmap_put(vmm, page, vma->addr, vma->size, vma->sparse);\n\t\tvma->refd = NVKM_VMA_PAGE_NONE;\n\t} else {\n\t\tnvkm_vmm_ptes_unmap(vmm, page, vma->addr, vma->size, vma->sparse);\n\t}\n\n\tnvkm_vmm_unmap_region(vmm, vma);\n}\n\nvoid\nnvkm_vmm_unmap(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tif (vma->memory) {\n\t\tmutex_lock(&vmm->mutex);\n\t\tnvkm_vmm_unmap_locked(vmm, vma);\n\t\tmutex_unlock(&vmm->mutex);\n\t}\n}\n\nstatic int\nnvkm_vmm_map_valid(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t   void *argv, u32 argc, struct nvkm_vmm_map *map)\n{\n\tswitch (nvkm_memory_target(map->memory)) {\n\tcase NVKM_MEM_TARGET_VRAM:\n\t\tif (!(map->page->type & NVKM_VMM_PAGE_VRAM)) {\n\t\t\tVMM_DEBUG(vmm, \"%d !VRAM\", map->page->shift);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase NVKM_MEM_TARGET_HOST:\n\tcase NVKM_MEM_TARGET_NCOH:\n\t\tif (!(map->page->type & NVKM_VMM_PAGE_HOST)) {\n\t\t\tVMM_DEBUG(vmm, \"%d !HOST\", map->page->shift);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -ENOSYS;\n\t}\n\n\tif (!IS_ALIGNED(     vma->addr, 1ULL << map->page->shift) ||\n\t    !IS_ALIGNED((u64)vma->size, 1ULL << map->page->shift) ||\n\t    !IS_ALIGNED(   map->offset, 1ULL << map->page->shift) ||\n\t    nvkm_memory_page(map->memory) < map->page->shift) {\n\t\tVMM_DEBUG(vmm, \"alignment %016llx %016llx %016llx %d %d\",\n\t\t    vma->addr, (u64)vma->size, map->offset, map->page->shift,\n\t\t    nvkm_memory_page(map->memory));\n\t\treturn -EINVAL;\n\t}\n\n\treturn vmm->func->valid(vmm, argv, argc, map);\n}\n\nstatic int\nnvkm_vmm_map_choose(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t    void *argv, u32 argc, struct nvkm_vmm_map *map)\n{\n\tfor (map->page = vmm->func->page; map->page->shift; map->page++) {\n\t\tVMM_DEBUG(vmm, \"trying %d\", map->page->shift);\n\t\tif (!nvkm_vmm_map_valid(vmm, vma, argv, argc, map))\n\t\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int\nnvkm_vmm_map_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t    void *argv, u32 argc, struct nvkm_vmm_map *map)\n{\n\tnvkm_vmm_pte_func func;\n\tint ret;\n\n\t/* Make sure we won't overrun the end of the memory object. */\n\tif (unlikely(nvkm_memory_size(map->memory) < map->offset + vma->size)) {\n\t\tVMM_DEBUG(vmm, \"overrun %016llx %016llx %016llx\",\n\t\t\t  nvkm_memory_size(map->memory),\n\t\t\t  map->offset, (u64)vma->size);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Check remaining arguments for validity. */\n\tif (vma->page == NVKM_VMA_PAGE_NONE &&\n\t    vma->refd == NVKM_VMA_PAGE_NONE) {\n\t\t/* Find the largest page size we can perform the mapping at. */\n\t\tconst u32 debug = vmm->debug;\n\t\tvmm->debug = 0;\n\t\tret = nvkm_vmm_map_choose(vmm, vma, argv, argc, map);\n\t\tvmm->debug = debug;\n\t\tif (ret) {\n\t\t\tVMM_DEBUG(vmm, \"invalid at any page size\");\n\t\t\tnvkm_vmm_map_choose(vmm, vma, argv, argc, map);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\t/* Page size of the VMA is already pre-determined. */\n\t\tif (vma->refd != NVKM_VMA_PAGE_NONE)\n\t\t\tmap->page = &vmm->func->page[vma->refd];\n\t\telse\n\t\t\tmap->page = &vmm->func->page[vma->page];\n\n\t\tret = nvkm_vmm_map_valid(vmm, vma, argv, argc, map);\n\t\tif (ret) {\n\t\t\tVMM_DEBUG(vmm, \"invalid %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* Deal with the 'offset' argument, and fetch the backend function. */\n\tmap->off = map->offset;\n\tif (map->mem) {\n\t\tfor (; map->off; map->mem = map->mem->next) {\n\t\t\tu64 size = (u64)map->mem->length << NVKM_RAM_MM_SHIFT;\n\t\t\tif (size > map->off)\n\t\t\t\tbreak;\n\t\t\tmap->off -= size;\n\t\t}\n\t\tfunc = map->page->desc->func->mem;\n\t} else\n\tif (map->sgl) {\n\t\tfor (; map->off; map->sgl = sg_next(map->sgl)) {\n\t\t\tu64 size = sg_dma_len(map->sgl);\n\t\t\tif (size > map->off)\n\t\t\t\tbreak;\n\t\t\tmap->off -= size;\n\t\t}\n\t\tfunc = map->page->desc->func->sgl;\n\t} else {\n\t\tmap->dma += map->offset >> PAGE_SHIFT;\n\t\tmap->off  = map->offset & PAGE_MASK;\n\t\tfunc = map->page->desc->func->dma;\n\t}\n\n\t/* Perform the map. */\n\tif (vma->refd == NVKM_VMA_PAGE_NONE) {\n\t\tret = nvkm_vmm_ptes_get_map(vmm, map->page, vma->addr, vma->size, map, func);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvma->refd = map->page - vmm->func->page;\n\t} else {\n\t\tnvkm_vmm_ptes_map(vmm, map->page, vma->addr, vma->size, map, func);\n\t}\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\tvma->memory = nvkm_memory_ref(map->memory);\n\tvma->tags = map->tags;\n\treturn 0;\n}\n\nint\nnvkm_vmm_map(struct nvkm_vmm *vmm, struct nvkm_vma *vma, void *argv, u32 argc,\n\t     struct nvkm_vmm_map *map)\n{\n\tint ret;\n\tmutex_lock(&vmm->mutex);\n\tret = nvkm_vmm_map_locked(vmm, vma, argv, argc, map);\n\tvma->busy = false;\n\tmutex_unlock(&vmm->mutex);\n\treturn ret;\n}\n\nstatic void\nnvkm_vmm_put_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *prev, *next;\n\n\tif ((prev = node(vma, prev)) && !prev->used) {\n\t\trb_erase(&prev->tree, &vmm->free);\n\t\tlist_del(&prev->head);\n\t\tvma->addr  = prev->addr;\n\t\tvma->size += prev->size;\n\t\tkfree(prev);\n\t}\n\n\tif ((next = node(vma, next)) && !next->used) {\n\t\trb_erase(&next->tree, &vmm->free);\n\t\tlist_del(&next->head);\n\t\tvma->size += next->size;\n\t\tkfree(next);\n\t}\n\n\tnvkm_vmm_free_insert(vmm, vma);\n}\n\nvoid\nnvkm_vmm_put_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tstruct nvkm_vma *next = vma;\n\n\tBUG_ON(vma->part);\n\n\tif (vma->mapref || !vma->sparse) {\n\t\tdo {\n\t\t\tconst bool map = next->memory != NULL;\n\t\t\tconst u8  refd = next->refd;\n\t\t\tconst u64 addr = next->addr;\n\t\t\tu64 size = next->size;\n\n\t\t\t/* Merge regions that are in the same state. */\n\t\t\twhile ((next = node(next, next)) && next->part &&\n\t\t\t       (next->memory != NULL) == map &&\n\t\t\t       (next->refd == refd))\n\t\t\t\tsize += next->size;\n\n\t\t\tif (map) {\n\t\t\t\t/* Region(s) are mapped, merge the unmap\n\t\t\t\t * and dereference into a single walk of\n\t\t\t\t * the page tree.\n\t\t\t\t */\n\t\t\t\tnvkm_vmm_ptes_unmap_put(vmm, &page[refd], addr,\n\t\t\t\t\t\t\tsize, vma->sparse);\n\t\t\t} else\n\t\t\tif (refd != NVKM_VMA_PAGE_NONE) {\n\t\t\t\t/* Drop allocation-time PTE references. */\n\t\t\t\tnvkm_vmm_ptes_put(vmm, &page[refd], addr, size);\n\t\t\t}\n\t\t} while (next && next->part);\n\t}\n\n\t/* Merge any mapped regions that were split from the initial\n\t * address-space allocation back into the allocated VMA, and\n\t * release memory/compression resources.\n\t */\n\tnext = vma;\n\tdo {\n\t\tif (next->memory)\n\t\t\tnvkm_vmm_unmap_region(vmm, next);\n\t} while ((next = node(vma, next)) && next->part);\n\n\tif (vma->sparse && !vma->mapref) {\n\t\t/* Sparse region that was allocated with a fixed page size,\n\t\t * meaning all relevant PTEs were referenced once when the\n\t\t * region was allocated, and remained that way, regardless\n\t\t * of whether memory was mapped into it afterwards.\n\t\t *\n\t\t * The process of unmapping, unsparsing, and dereferencing\n\t\t * PTEs can be done in a single page tree walk.\n\t\t */\n\t\tnvkm_vmm_ptes_sparse_put(vmm, &page[vma->refd], vma->addr, vma->size);\n\t} else\n\tif (vma->sparse) {\n\t\t/* Sparse region that wasn't allocated with a fixed page size,\n\t\t * PTE references were taken both at allocation time (to make\n\t\t * the GPU see the region as sparse), and when mapping memory\n\t\t * into the region.\n\t\t *\n\t\t * The latter was handled above, and the remaining references\n\t\t * are dealt with here.\n\t\t */\n\t\tnvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, false);\n\t}\n\n\t/* Remove VMA from the list of allocated nodes. */\n\trb_erase(&vma->tree, &vmm->root);\n\n\t/* Merge VMA back into the free list. */\n\tvma->page = NVKM_VMA_PAGE_NONE;\n\tvma->refd = NVKM_VMA_PAGE_NONE;\n\tvma->used = false;\n\tvma->user = false;\n\tnvkm_vmm_put_region(vmm, vma);\n}\n\nvoid\nnvkm_vmm_put(struct nvkm_vmm *vmm, struct nvkm_vma **pvma)\n{\n\tstruct nvkm_vma *vma = *pvma;\n\tif (vma) {\n\t\tmutex_lock(&vmm->mutex);\n\t\tnvkm_vmm_put_locked(vmm, vma);\n\t\tmutex_unlock(&vmm->mutex);\n\t\t*pvma = NULL;\n\t}\n}\n\nint\nnvkm_vmm_get_locked(struct nvkm_vmm *vmm, bool getref, bool mapref, bool sparse,\n\t\t    u8 shift, u8 align, u64 size, struct nvkm_vma **pvma)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[NVKM_VMA_PAGE_NONE];\n\tstruct rb_node *node = NULL, *temp;\n\tstruct nvkm_vma *vma = NULL, *tmp;\n\tu64 addr, tail;\n\tint ret;\n\n\tVMM_TRACE(vmm, \"getref %d mapref %d sparse %d \"\n\t\t       \"shift: %d align: %d size: %016llx\",\n\t\t  getref, mapref, sparse, shift, align, size);\n\n\t/* Zero-sized, or lazily-allocated sparse VMAs, make no sense. */\n\tif (unlikely(!size || (!getref && !mapref && sparse))) {\n\t\tVMM_DEBUG(vmm, \"args %016llx %d %d %d\",\n\t\t\t  size, getref, mapref, sparse);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Tesla-class GPUs can only select page size per-PDE, which means\n\t * we're required to know the mapping granularity up-front to find\n\t * a suitable region of address-space.\n\t *\n\t * The same goes if we're requesting up-front allocation of PTES.\n\t */\n\tif (unlikely((getref || vmm->func->page_block) && !shift)) {\n\t\tVMM_DEBUG(vmm, \"page size required: %d %016llx\",\n\t\t\t  getref, vmm->func->page_block);\n\t\treturn -EINVAL;\n\t}\n\n\t/* If a specific page size was requested, determine its index and\n\t * make sure the requested size is a multiple of the page size.\n\t */\n\tif (shift) {\n\t\tfor (page = vmm->func->page; page->shift; page++) {\n\t\t\tif (shift == page->shift)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!page->shift || !IS_ALIGNED(size, 1ULL << page->shift)) {\n\t\t\tVMM_DEBUG(vmm, \"page %d %016llx\", shift, size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\talign = max_t(u8, align, shift);\n\t} else {\n\t\talign = max_t(u8, align, 12);\n\t}\n\n\t/* Locate smallest block that can possibly satisfy the allocation. */\n\ttemp = vmm->free.rb_node;\n\twhile (temp) {\n\t\tstruct nvkm_vma *this = rb_entry(temp, typeof(*this), tree);\n\t\tif (this->size < size) {\n\t\t\ttemp = temp->rb_right;\n\t\t} else {\n\t\t\tnode = temp;\n\t\t\ttemp = temp->rb_left;\n\t\t}\n\t}\n\n\tif (unlikely(!node))\n\t\treturn -ENOSPC;\n\n\t/* Take into account alignment restrictions, trying larger blocks\n\t * in turn until we find a suitable free block.\n\t */\n\tdo {\n\t\tstruct nvkm_vma *this = rb_entry(node, typeof(*this), tree);\n\t\tstruct nvkm_vma *prev = node(this, prev);\n\t\tstruct nvkm_vma *next = node(this, next);\n\t\tconst int p = page - vmm->func->page;\n\n\t\taddr = this->addr;\n\t\tif (vmm->func->page_block && prev && prev->page != p)\n\t\t\taddr = ALIGN(addr, vmm->func->page_block);\n\t\taddr = ALIGN(addr, 1ULL << align);\n\n\t\ttail = this->addr + this->size;\n\t\tif (vmm->func->page_block && next && next->page != p)\n\t\t\ttail = ALIGN_DOWN(tail, vmm->func->page_block);\n\n\t\tif (addr <= tail && tail - addr >= size) {\n\t\t\trb_erase(&this->tree, &vmm->free);\n\t\t\tvma = this;\n\t\t\tbreak;\n\t\t}\n\t} while ((node = rb_next(node)));\n\n\tif (unlikely(!vma))\n\t\treturn -ENOSPC;\n\n\t/* If the VMA we found isn't already exactly the requested size,\n\t * it needs to be split, and the remaining free blocks returned.\n\t */\n\tif (addr != vma->addr) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size + vma->addr - addr))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, vma);\n\t\tvma = tmp;\n\t}\n\n\tif (size != vma->size) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size - size))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, tmp);\n\t}\n\n\t/* Pre-allocate page tables and/or setup sparse mappings. */\n\tif (sparse && getref)\n\t\tret = nvkm_vmm_ptes_sparse_get(vmm, page, vma->addr, vma->size);\n\telse if (sparse)\n\t\tret = nvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, true);\n\telse if (getref)\n\t\tret = nvkm_vmm_ptes_get(vmm, page, vma->addr, vma->size);\n\telse\n\t\tret = 0;\n\tif (ret) {\n\t\tnvkm_vmm_put_region(vmm, vma);\n\t\treturn ret;\n\t}\n\n\tvma->mapref = mapref && !getref;\n\tvma->sparse = sparse;\n\tvma->page = page - vmm->func->page;\n\tvma->refd = getref ? vma->page : NVKM_VMA_PAGE_NONE;\n\tvma->used = true;\n\tnvkm_vmm_node_insert(vmm, vma);\n\t*pvma = vma;\n\treturn 0;\n}\n\nint\nnvkm_vmm_get(struct nvkm_vmm *vmm, u8 page, u64 size, struct nvkm_vma **pvma)\n{\n\tint ret;\n\tmutex_lock(&vmm->mutex);\n\tret = nvkm_vmm_get_locked(vmm, false, true, false, page, 0, size, pvma);\n\tmutex_unlock(&vmm->mutex);\n\treturn ret;\n}\n\nvoid\nnvkm_vmm_part(struct nvkm_vmm *vmm, struct nvkm_memory *inst)\n{\n\tif (inst && vmm->func->part) {\n\t\tmutex_lock(&vmm->mutex);\n\t\tvmm->func->part(vmm, inst);\n\t\tmutex_unlock(&vmm->mutex);\n\t}\n}\n\nint\nnvkm_vmm_join(struct nvkm_vmm *vmm, struct nvkm_memory *inst)\n{\n\tint ret = 0;\n\tif (vmm->func->join) {\n\t\tmutex_lock(&vmm->mutex);\n\t\tret = vmm->func->join(vmm, inst);\n\t\tmutex_unlock(&vmm->mutex);\n\t}\n\treturn ret;\n}\n\nstatic bool\nnvkm_vmm_boot_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc->type == SPT;\n\tnvkm_memory_boot(it->pt[0]->pt[type]->memory, it->vmm);\n\treturn false;\n}\n\nint\nnvkm_vmm_boot(struct nvkm_vmm *vmm)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tconst u64 limit = vmm->limit - vmm->start;\n\tint ret;\n\n\twhile (page[1].shift)\n\t\tpage++;\n\n\tret = nvkm_vmm_ptes_get(vmm, page, vmm->start, limit);\n\tif (ret)\n\t\treturn ret;\n\n\tnvkm_vmm_iter(vmm, page, vmm->start, limit, \"bootstrap\", false,\n\t\t      nvkm_vmm_boot_ptes, NULL, NULL, NULL);\n\tvmm->bootstrapped = true;\n\treturn 0;\n}\n\nstatic void\nnvkm_vmm_del(struct kref *kref)\n{\n\tstruct nvkm_vmm *vmm = container_of(kref, typeof(*vmm), kref);\n\tnvkm_vmm_dtor(vmm);\n\tkfree(vmm);\n}\n\nvoid\nnvkm_vmm_unref(struct nvkm_vmm **pvmm)\n{\n\tstruct nvkm_vmm *vmm = *pvmm;\n\tif (vmm) {\n\t\tkref_put(&vmm->kref, nvkm_vmm_del);\n\t\t*pvmm = NULL;\n\t}\n}\n\nstruct nvkm_vmm *\nnvkm_vmm_ref(struct nvkm_vmm *vmm)\n{\n\tif (vmm)\n\t\tkref_get(&vmm->kref);\n\treturn vmm;\n}\n\nint\nnvkm_vmm_new(struct nvkm_device *device, u64 addr, u64 size, void *argv,\n\t     u32 argc, struct lock_class_key *key, const char *name,\n\t     struct nvkm_vmm **pvmm)\n{\n\tstruct nvkm_mmu *mmu = device->mmu;\n\tstruct nvkm_vmm *vmm = NULL;\n\tint ret;\n\tret = mmu->func->vmm.ctor(mmu, addr, size, argv, argc, key, name, &vmm);\n\tif (ret)\n\t\tnvkm_vmm_unref(&vmm);\n\t*pvmm = vmm;\n\treturn ret;\n}\n", "#ifndef __NVKM_VMM_H__\n#define __NVKM_VMM_H__\n#include \"priv.h\"\n#include <core/memory.h>\nenum nvkm_memory_target;\n\nstruct nvkm_vmm_pt {\n\t/* Some GPUs have a mapping level with a dual page tables to\n\t * support large and small pages in the same address-range.\n\t *\n\t * We track the state of both page tables in one place, which\n\t * is why there's multiple PT pointers/refcounts here.\n\t */\n\tstruct nvkm_mmu_pt *pt[2];\n\tu32 refs[2];\n\n\t/* Page size handled by this PT.\n\t *\n\t * Tesla backend needs to know this when writinge PDEs,\n\t * otherwise unnecessary.\n\t */\n\tu8 page;\n\n\t/* Entire page table sparse.\n\t *\n\t * Used to propagate sparseness to child page tables.\n\t */\n\tbool sparse:1;\n\n\t/* Tracking for page directories.\n\t *\n\t * The array is indexed by PDE, and will either point to the\n\t * child page table, or indicate the PDE is marked as sparse.\n\t **/\n#define NVKM_VMM_PDE_INVALID(pde) IS_ERR_OR_NULL(pde)\n#define NVKM_VMM_PDE_SPARSED(pde) IS_ERR(pde)\n#define NVKM_VMM_PDE_SPARSE       ERR_PTR(-EBUSY)\n\tstruct nvkm_vmm_pt **pde;\n\n\t/* Tracking for dual page tables.\n\t *\n\t * There's one entry for each LPTE, keeping track of whether\n\t * there are valid SPTEs in the same address-range.\n\t *\n\t * This information is used to manage LPTE state transitions.\n\t */\n#define NVKM_VMM_PTE_SPARSE 0x80\n#define NVKM_VMM_PTE_VALID  0x40\n#define NVKM_VMM_PTE_SPTES  0x3f\n\tu8 pte[];\n};\n\ntypedef void (*nvkm_vmm_pxe_func)(struct nvkm_vmm *,\n\t\t\t\t  struct nvkm_mmu_pt *, u32 ptei, u32 ptes);\ntypedef void (*nvkm_vmm_pde_func)(struct nvkm_vmm *,\n\t\t\t\t  struct nvkm_vmm_pt *, u32 pdei);\ntypedef void (*nvkm_vmm_pte_func)(struct nvkm_vmm *, struct nvkm_mmu_pt *,\n\t\t\t\t  u32 ptei, u32 ptes, struct nvkm_vmm_map *);\n\nstruct nvkm_vmm_desc_func {\n\tnvkm_vmm_pxe_func invalid;\n\tnvkm_vmm_pxe_func unmap;\n\tnvkm_vmm_pxe_func sparse;\n\n\tnvkm_vmm_pde_func pde;\n\n\tnvkm_vmm_pte_func mem;\n\tnvkm_vmm_pte_func dma;\n\tnvkm_vmm_pte_func sgl;\n};\n\nextern const struct nvkm_vmm_desc_func gf100_vmm_pgd;\nvoid gf100_vmm_pgd_pde(struct nvkm_vmm *, struct nvkm_vmm_pt *, u32);\nextern const struct nvkm_vmm_desc_func gf100_vmm_pgt;\nvoid gf100_vmm_pgt_unmap(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32);\nvoid gf100_vmm_pgt_mem(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32,\n\t\t       struct nvkm_vmm_map *);\nvoid gf100_vmm_pgt_dma(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32,\n\t\t       struct nvkm_vmm_map *);\nvoid gf100_vmm_pgt_sgl(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32,\n\t\t       struct nvkm_vmm_map *);\n\nvoid gk104_vmm_lpt_invalid(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32);\n\nstruct nvkm_vmm_desc {\n\tenum {\n\t\tPGD,\n\t\tPGT,\n\t\tSPT,\n\t\tLPT,\n\t} type;\n\tu8 bits;\t/* VMA bits covered by PT. */\n\tu8 size;\t/* Bytes-per-PTE. */\n\tu32 align;\t/* PT address alignment. */\n\tconst struct nvkm_vmm_desc_func *func;\n};\n\nextern const struct nvkm_vmm_desc nv50_vmm_desc_12[];\nextern const struct nvkm_vmm_desc nv50_vmm_desc_16[];\n\nextern const struct nvkm_vmm_desc gk104_vmm_desc_16_12[];\nextern const struct nvkm_vmm_desc gk104_vmm_desc_16_16[];\nextern const struct nvkm_vmm_desc gk104_vmm_desc_17_12[];\nextern const struct nvkm_vmm_desc gk104_vmm_desc_17_17[];\n\nextern const struct nvkm_vmm_desc gm200_vmm_desc_16_12[];\nextern const struct nvkm_vmm_desc gm200_vmm_desc_16_16[];\nextern const struct nvkm_vmm_desc gm200_vmm_desc_17_12[];\nextern const struct nvkm_vmm_desc gm200_vmm_desc_17_17[];\n\nextern const struct nvkm_vmm_desc gp100_vmm_desc_12[];\nextern const struct nvkm_vmm_desc gp100_vmm_desc_16[];\n\nstruct nvkm_vmm_page {\n\tu8 shift;\n\tconst struct nvkm_vmm_desc *desc;\n#define NVKM_VMM_PAGE_SPARSE                                               0x01\n#define NVKM_VMM_PAGE_VRAM                                                 0x02\n#define NVKM_VMM_PAGE_HOST                                                 0x04\n#define NVKM_VMM_PAGE_COMP                                                 0x08\n#define NVKM_VMM_PAGE_Sxxx                                (NVKM_VMM_PAGE_SPARSE)\n#define NVKM_VMM_PAGE_xVxx                                  (NVKM_VMM_PAGE_VRAM)\n#define NVKM_VMM_PAGE_SVxx             (NVKM_VMM_PAGE_Sxxx | NVKM_VMM_PAGE_VRAM)\n#define NVKM_VMM_PAGE_xxHx                                  (NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_SxHx             (NVKM_VMM_PAGE_Sxxx | NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_xVHx             (NVKM_VMM_PAGE_xVxx | NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_SVHx             (NVKM_VMM_PAGE_SVxx | NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_xVxC             (NVKM_VMM_PAGE_xVxx | NVKM_VMM_PAGE_COMP)\n#define NVKM_VMM_PAGE_SVxC             (NVKM_VMM_PAGE_SVxx | NVKM_VMM_PAGE_COMP)\n#define NVKM_VMM_PAGE_xxHC             (NVKM_VMM_PAGE_xxHx | NVKM_VMM_PAGE_COMP)\n#define NVKM_VMM_PAGE_SxHC             (NVKM_VMM_PAGE_SxHx | NVKM_VMM_PAGE_COMP)\n\tu8 type;\n};\n\nstruct nvkm_vmm_func {\n\tint (*join)(struct nvkm_vmm *, struct nvkm_memory *inst);\n\tvoid (*part)(struct nvkm_vmm *, struct nvkm_memory *inst);\n\n\tint (*aper)(enum nvkm_memory_target);\n\tint (*valid)(struct nvkm_vmm *, void *argv, u32 argc,\n\t\t     struct nvkm_vmm_map *);\n\tvoid (*flush)(struct nvkm_vmm *, int depth);\n\n\tu64 page_block;\n\tconst struct nvkm_vmm_page page[];\n};\n\nstruct nvkm_vmm_join {\n\tstruct nvkm_memory *inst;\n\tstruct list_head head;\n};\n\nint nvkm_vmm_new_(const struct nvkm_vmm_func *, struct nvkm_mmu *,\n\t\t  u32 pd_header, u64 addr, u64 size, struct lock_class_key *,\n\t\t  const char *name, struct nvkm_vmm **);\nint nvkm_vmm_ctor(const struct nvkm_vmm_func *, struct nvkm_mmu *,\n\t\t  u32 pd_header, u64 addr, u64 size, struct lock_class_key *,\n\t\t  const char *name, struct nvkm_vmm *);\nstruct nvkm_vma *nvkm_vmm_node_search(struct nvkm_vmm *, u64 addr);\nint nvkm_vmm_get_locked(struct nvkm_vmm *, bool getref, bool mapref,\n\t\t\tbool sparse, u8 page, u8 align, u64 size,\n\t\t\tstruct nvkm_vma **pvma);\nvoid nvkm_vmm_put_locked(struct nvkm_vmm *, struct nvkm_vma *);\nvoid nvkm_vmm_unmap_locked(struct nvkm_vmm *, struct nvkm_vma *);\nvoid nvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma);\n\nstruct nvkm_vma *nvkm_vma_tail(struct nvkm_vma *, u64 tail);\nvoid nvkm_vmm_node_insert(struct nvkm_vmm *, struct nvkm_vma *);\n\nint nv04_vmm_new_(const struct nvkm_vmm_func *, struct nvkm_mmu *, u32,\n\t\t  u64, u64, void *, u32, struct lock_class_key *,\n\t\t  const char *, struct nvkm_vmm **);\nint nv04_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\n\nint nv50_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\nvoid nv50_vmm_part(struct nvkm_vmm *, struct nvkm_memory *);\nint nv50_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\nvoid nv50_vmm_flush(struct nvkm_vmm *, int);\n\nint gf100_vmm_new_(const struct nvkm_vmm_func *, const struct nvkm_vmm_func *,\n\t\t   struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t   struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gf100_vmm_join_(struct nvkm_vmm *, struct nvkm_memory *, u64 base);\nint gf100_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\nvoid gf100_vmm_part(struct nvkm_vmm *, struct nvkm_memory *);\nint gf100_vmm_aper(enum nvkm_memory_target);\nint gf100_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\nvoid gf100_vmm_flush_(struct nvkm_vmm *, int);\nvoid gf100_vmm_flush(struct nvkm_vmm *, int);\n\nint gk20a_vmm_aper(enum nvkm_memory_target);\n\nint gm200_vmm_new_(const struct nvkm_vmm_func *, const struct nvkm_vmm_func *,\n\t\t   struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t   struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gm200_vmm_join_(struct nvkm_vmm *, struct nvkm_memory *, u64 base);\nint gm200_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\n\nint gp100_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\nint gp100_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\nvoid gp100_vmm_flush(struct nvkm_vmm *, int);\n\nint nv04_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint nv41_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint nv44_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint nv50_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint mcp77_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint g84_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\tstruct lock_class_key *, const char *, struct nvkm_vmm **);\nint gf100_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gk104_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gk20a_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gm200_vmm_new_fixed(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t\tstruct lock_class_key *, const char *,\n\t\t\tstruct nvkm_vmm **);\nint gm200_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gm20b_vmm_new_fixed(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t\tstruct lock_class_key *, const char *,\n\t\t\tstruct nvkm_vmm **);\nint gm20b_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gp100_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gp10b_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gv100_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\n\n#define VMM_PRINT(l,v,p,f,a...) do {                                           \\\n\tstruct nvkm_vmm *_vmm = (v);                                           \\\n\tif (CONFIG_NOUVEAU_DEBUG >= (l) && _vmm->debug >= (l)) {               \\\n\t\tnvkm_printk_(&_vmm->mmu->subdev, 0, p, \"%s: \"f\"\\n\",            \\\n\t\t\t     _vmm->name, ##a);                                 \\\n\t}                                                                      \\\n} while(0)\n#define VMM_DEBUG(v,f,a...) VMM_PRINT(NV_DBG_DEBUG, (v), info, f, ##a)\n#define VMM_TRACE(v,f,a...) VMM_PRINT(NV_DBG_TRACE, (v), info, f, ##a)\n#define VMM_SPAM(v,f,a...)  VMM_PRINT(NV_DBG_SPAM , (v),  dbg, f, ##a)\n\n#define VMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,BASE,SIZE,NEXT) do {            \\\n\tnvkm_kmap((PT)->memory);                                               \\\n\twhile (PTEN) {                                                         \\\n\t\tu64 _ptes = ((SIZE) - MAP->off) >> MAP->page->shift;           \\\n\t\tu64 _addr = ((BASE) + MAP->off);                               \\\n                                                                               \\\n\t\tif (_ptes > PTEN) {                                            \\\n\t\t\tMAP->off += PTEN << MAP->page->shift;                  \\\n\t\t\t_ptes = PTEN;                                          \\\n\t\t} else {                                                       \\\n\t\t\tMAP->off = 0;                                          \\\n\t\t\tNEXT;                                                  \\\n\t\t}                                                              \\\n                                                                               \\\n\t\tVMM_SPAM(VMM, \"ITER %08x %08x PTE(s)\", PTEI, (u32)_ptes);      \\\n                                                                               \\\n\t\tFILL(VMM, PT, PTEI, _ptes, MAP, _addr);                        \\\n\t\tPTEI += _ptes;                                                 \\\n\t\tPTEN -= _ptes;                                                 \\\n\t};                                                                     \\\n\tnvkm_done((PT)->memory);                                               \\\n} while(0)\n\n#define VMM_MAP_ITER_MEM(VMM,PT,PTEI,PTEN,MAP,FILL)                            \\\n\tVMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,                                \\\n\t\t     ((u64)MAP->mem->offset << NVKM_RAM_MM_SHIFT),             \\\n\t\t     ((u64)MAP->mem->length << NVKM_RAM_MM_SHIFT),             \\\n\t\t     (MAP->mem = MAP->mem->next))\n#define VMM_MAP_ITER_DMA(VMM,PT,PTEI,PTEN,MAP,FILL)                            \\\n\tVMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,                                \\\n\t\t     *MAP->dma, PAGE_SIZE, MAP->dma++)\n#define VMM_MAP_ITER_SGL(VMM,PT,PTEI,PTEN,MAP,FILL)                            \\\n\tVMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,                                \\\n\t\t     sg_dma_address(MAP->sgl), sg_dma_len(MAP->sgl),           \\\n\t\t     (MAP->sgl = sg_next(MAP->sgl)))\n\n#define VMM_FO(m,o,d,c,b) nvkm_fo##b((m)->memory, (o), (d), (c))\n#define VMM_WO(m,o,d,c,b) nvkm_wo##b((m)->memory, (o), (d))\n#define VMM_XO(m,v,o,d,c,b,fn,f,a...) do {                                     \\\n\tconst u32 _pteo = (o); u##b _data = (d);                               \\\n\tVMM_SPAM((v), \"   %010llx \"f, (m)->addr + _pteo, _data, ##a);          \\\n\tVMM_##fn((m), (m)->base + _pteo, _data, (c), b);                       \\\n} while(0)\n\n#define VMM_WO032(m,v,o,d) VMM_XO((m),(v),(o),(d),  1, 32, WO, \"%08x\")\n#define VMM_FO032(m,v,o,d,c)                                                   \\\n\tVMM_XO((m),(v),(o),(d),(c), 32, FO, \"%08x %08x\", (c))\n\n#define VMM_WO064(m,v,o,d) VMM_XO((m),(v),(o),(d),  1, 64, WO, \"%016llx\")\n#define VMM_FO064(m,v,o,d,c)                                                   \\\n\tVMM_XO((m),(v),(o),(d),(c), 64, FO, \"%016llx %08x\", (c))\n\n#define VMM_XO128(m,v,o,lo,hi,c,f,a...) do {                                   \\\n\tu32 _pteo = (o), _ptes = (c);                                          \\\n\tconst u64 _addr = (m)->addr + _pteo;                                   \\\n\tVMM_SPAM((v), \"   %010llx %016llx%016llx\"f, _addr, (hi), (lo), ##a);   \\\n\twhile (_ptes--) {                                                      \\\n\t\tnvkm_wo64((m)->memory, (m)->base + _pteo + 0, (lo));           \\\n\t\tnvkm_wo64((m)->memory, (m)->base + _pteo + 8, (hi));           \\\n\t\t_pteo += 0x10;                                                 \\\n\t}                                                                      \\\n} while(0)\n\n#define VMM_WO128(m,v,o,lo,hi) VMM_XO128((m),(v),(o),(lo),(hi), 1, \"\")\n#define VMM_FO128(m,v,o,lo,hi,c) do {                                          \\\n\tnvkm_kmap((m)->memory);                                                \\\n\tVMM_XO128((m),(v),(o),(lo),(hi),(c), \" %08x\", (c));                    \\\n\tnvkm_done((m)->memory);                                                \\\n} while(0)\n#endif\n"], "fixing_code": ["/*\n * Copyright 2017 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n#include \"uvmm.h\"\n#include \"umem.h\"\n#include \"ummu.h\"\n\n#include <core/client.h>\n#include <core/memory.h>\n\n#include <nvif/if000c.h>\n#include <nvif/unpack.h>\n\nstatic const struct nvkm_object_func nvkm_uvmm;\nstruct nvkm_vmm *\nnvkm_uvmm_search(struct nvkm_client *client, u64 handle)\n{\n\tstruct nvkm_object *object;\n\n\tobject = nvkm_object_search(client, handle, &nvkm_uvmm);\n\tif (IS_ERR(object))\n\t\treturn (void *)object;\n\n\treturn nvkm_uvmm(object)->vmm;\n}\n\nstatic int\nnvkm_uvmm_mthd_unmap(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_unmap_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tint ret = -ENOSYS;\n\tu64 addr;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\taddr = args->v0.addr;\n\t} else\n\t\treturn ret;\n\n\tmutex_lock(&vmm->mutex);\n\tvma = nvkm_vmm_node_search(vmm, addr);\n\tif (ret = -ENOENT, !vma || vma->addr != addr) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx: %016llx\",\n\t\t\t  addr, vma ? vma->addr : ~0ULL);\n\t\tgoto done;\n\t}\n\n\tif (ret = -ENOENT, (!vma->user && !client->super) || vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d %d %d\", addr,\n\t\t\t  vma->user, !client->super, vma->busy);\n\t\tgoto done;\n\t}\n\n\tif (ret = -EINVAL, !vma->memory) {\n\t\tVMM_DEBUG(vmm, \"unmapped\");\n\t\tgoto done;\n\t}\n\n\tnvkm_vmm_unmap_locked(vmm, vma);\n\tret = 0;\ndone:\n\tmutex_unlock(&vmm->mutex);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_map(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_map_v0 v0;\n\t} *args = argv;\n\tu64 addr, size, handle, offset;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tstruct nvkm_memory *memory;\n\tint ret = -ENOSYS;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, true))) {\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t\thandle = args->v0.memory;\n\t\toffset = args->v0.offset;\n\t} else\n\t\treturn ret;\n\n\tmemory = nvkm_umem_search(client, handle);\n\tif (IS_ERR(memory)) {\n\t\tVMM_DEBUG(vmm, \"memory %016llx %ld\\n\", handle, PTR_ERR(memory));\n\t\treturn PTR_ERR(memory);\n\t}\n\n\tmutex_lock(&vmm->mutex);\n\tif (ret = -ENOENT, !(vma = nvkm_vmm_node_search(vmm, addr))) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx\", addr);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -ENOENT, (!vma->user && !client->super) || vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d %d %d\", addr,\n\t\t\t  vma->user, !client->super, vma->busy);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -EINVAL, vma->addr != addr || vma->size != size) {\n\t\tif (addr + size > vma->addr + vma->size || vma->memory ||\n\t\t    (vma->refd == NVKM_VMA_PAGE_NONE && !vma->mapref)) {\n\t\t\tVMM_DEBUG(vmm, \"split %d %d %d \"\n\t\t\t\t       \"%016llx %016llx %016llx %016llx\",\n\t\t\t\t  !!vma->memory, vma->refd, vma->mapref,\n\t\t\t\t  addr, size, vma->addr, (u64)vma->size);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tvma = nvkm_vmm_node_split(vmm, vma, addr, size);\n\t\tif (!vma) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tvma->busy = true;\n\tmutex_unlock(&vmm->mutex);\n\n\tret = nvkm_memory_map(memory, offset, vmm, vma, argv, argc);\n\tif (ret == 0) {\n\t\t/* Successful map will clear vma->busy. */\n\t\tnvkm_memory_unref(&memory);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&vmm->mutex);\n\tvma->busy = false;\n\tnvkm_vmm_unmap_region(vmm, vma);\nfail:\n\tmutex_unlock(&vmm->mutex);\n\tnvkm_memory_unref(&memory);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_put(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_put_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tint ret = -ENOSYS;\n\tu64 addr;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\taddr = args->v0.addr;\n\t} else\n\t\treturn ret;\n\n\tmutex_lock(&vmm->mutex);\n\tvma = nvkm_vmm_node_search(vmm, args->v0.addr);\n\tif (ret = -ENOENT, !vma || vma->addr != addr || vma->part) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx: %016llx %d\", addr,\n\t\t\t  vma ? vma->addr : ~0ULL, vma ? vma->part : 0);\n\t\tgoto done;\n\t}\n\n\tif (ret = -ENOENT, (!vma->user && !client->super) || vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d %d %d\", addr,\n\t\t\t  vma->user, !client->super, vma->busy);\n\t\tgoto done;\n\t}\n\n\tnvkm_vmm_put_locked(vmm, vma);\n\tret = 0;\ndone:\n\tmutex_unlock(&vmm->mutex);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_get(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_get_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tint ret = -ENOSYS;\n\tbool getref, mapref, sparse;\n\tu8 page, align;\n\tu64 size;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\tgetref = args->v0.type == NVIF_VMM_GET_V0_PTES;\n\t\tmapref = args->v0.type == NVIF_VMM_GET_V0_ADDR;\n\t\tsparse = args->v0.sparse;\n\t\tpage = args->v0.page;\n\t\talign = args->v0.align;\n\t\tsize = args->v0.size;\n\t} else\n\t\treturn ret;\n\n\tmutex_lock(&vmm->mutex);\n\tret = nvkm_vmm_get_locked(vmm, getref, mapref, sparse,\n\t\t\t\t  page, align, size, &vma);\n\tmutex_unlock(&vmm->mutex);\n\tif (ret)\n\t\treturn ret;\n\n\targs->v0.addr = vma->addr;\n\tvma->user = !client->super;\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_page(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tunion {\n\t\tstruct nvif_vmm_page_v0 v0;\n\t} *args = argv;\n\tconst struct nvkm_vmm_page *page;\n\tint ret = -ENOSYS;\n\tu8 type, index, nr;\n\n\tpage = uvmm->vmm->func->page;\n\tfor (nr = 0; page[nr].shift; nr++);\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\tif ((index = args->v0.index) >= nr)\n\t\t\treturn -EINVAL;\n\t\ttype = page[index].type;\n\t\targs->v0.shift = page[index].shift;\n\t\targs->v0.sparse = !!(type & NVKM_VMM_PAGE_SPARSE);\n\t\targs->v0.vram = !!(type & NVKM_VMM_PAGE_VRAM);\n\t\targs->v0.host = !!(type & NVKM_VMM_PAGE_HOST);\n\t\targs->v0.comp = !!(type & NVKM_VMM_PAGE_COMP);\n\t} else\n\t\treturn -ENOSYS;\n\n\treturn 0;\n}\n\nstatic int\nnvkm_uvmm_mthd(struct nvkm_object *object, u32 mthd, void *argv, u32 argc)\n{\n\tstruct nvkm_uvmm *uvmm = nvkm_uvmm(object);\n\tswitch (mthd) {\n\tcase NVIF_VMM_V0_PAGE  : return nvkm_uvmm_mthd_page  (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_GET   : return nvkm_uvmm_mthd_get   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_PUT   : return nvkm_uvmm_mthd_put   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_MAP   : return nvkm_uvmm_mthd_map   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_UNMAP : return nvkm_uvmm_mthd_unmap (uvmm, argv, argc);\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\nstatic void *\nnvkm_uvmm_dtor(struct nvkm_object *object)\n{\n\tstruct nvkm_uvmm *uvmm = nvkm_uvmm(object);\n\tnvkm_vmm_unref(&uvmm->vmm);\n\treturn uvmm;\n}\n\nstatic const struct nvkm_object_func\nnvkm_uvmm = {\n\t.dtor = nvkm_uvmm_dtor,\n\t.mthd = nvkm_uvmm_mthd,\n};\n\nint\nnvkm_uvmm_new(const struct nvkm_oclass *oclass, void *argv, u32 argc,\n\t      struct nvkm_object **pobject)\n{\n\tstruct nvkm_mmu *mmu = nvkm_ummu(oclass->parent)->mmu;\n\tconst bool more = oclass->base.maxver >= 0;\n\tunion {\n\t\tstruct nvif_vmm_v0 v0;\n\t} *args = argv;\n\tconst struct nvkm_vmm_page *page;\n\tstruct nvkm_uvmm *uvmm;\n\tint ret = -ENOSYS;\n\tu64 addr, size;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, more))) {\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t} else\n\t\treturn ret;\n\n\tif (!(uvmm = kzalloc(sizeof(*uvmm), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\tnvkm_object_ctor(&nvkm_uvmm, oclass, &uvmm->object);\n\t*pobject = &uvmm->object;\n\n\tif (!mmu->vmm) {\n\t\tret = mmu->func->vmm.ctor(mmu, addr, size, argv, argc,\n\t\t\t\t\t  NULL, \"user\", &uvmm->vmm);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tuvmm->vmm->debug = max(uvmm->vmm->debug, oclass->client->debug);\n\t} else {\n\t\tif (size)\n\t\t\treturn -EINVAL;\n\n\t\tuvmm->vmm = nvkm_vmm_ref(mmu->vmm);\n\t}\n\n\tpage = uvmm->vmm->func->page;\n\targs->v0.page_nr = 0;\n\twhile (page && (page++)->shift)\n\t\targs->v0.page_nr++;\n\targs->v0.addr = uvmm->vmm->start;\n\targs->v0.size = uvmm->vmm->limit;\n\treturn 0;\n}\n", "/*\n * Copyright 2017 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n#define NVKM_VMM_LEVELS_MAX 5\n#include \"vmm.h\"\n\n#include <subdev/fb.h>\n\nstatic void\nnvkm_vmm_pt_del(struct nvkm_vmm_pt **ppgt)\n{\n\tstruct nvkm_vmm_pt *pgt = *ppgt;\n\tif (pgt) {\n\t\tkvfree(pgt->pde);\n\t\tkfree(pgt);\n\t\t*ppgt = NULL;\n\t}\n}\n\n\nstatic struct nvkm_vmm_pt *\nnvkm_vmm_pt_new(const struct nvkm_vmm_desc *desc, bool sparse,\n\t\tconst struct nvkm_vmm_page *page)\n{\n\tconst u32 pten = 1 << desc->bits;\n\tstruct nvkm_vmm_pt *pgt;\n\tu32 lpte = 0;\n\n\tif (desc->type > PGT) {\n\t\tif (desc->type == SPT) {\n\t\t\tconst struct nvkm_vmm_desc *pair = page[-1].desc;\n\t\t\tlpte = pten >> (desc->bits - pair->bits);\n\t\t} else {\n\t\t\tlpte = pten;\n\t\t}\n\t}\n\n\tif (!(pgt = kzalloc(sizeof(*pgt) + lpte, GFP_KERNEL)))\n\t\treturn NULL;\n\tpgt->page = page ? page->shift : 0;\n\tpgt->sparse = sparse;\n\n\tif (desc->type == PGD) {\n\t\tpgt->pde = kvcalloc(pten, sizeof(*pgt->pde), GFP_KERNEL);\n\t\tif (!pgt->pde) {\n\t\t\tkfree(pgt);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn pgt;\n}\n\nstruct nvkm_vmm_iter {\n\tconst struct nvkm_vmm_page *page;\n\tconst struct nvkm_vmm_desc *desc;\n\tstruct nvkm_vmm *vmm;\n\tu64 cnt;\n\tu16 max, lvl;\n\tu32 pte[NVKM_VMM_LEVELS_MAX];\n\tstruct nvkm_vmm_pt *pt[NVKM_VMM_LEVELS_MAX];\n\tint flush;\n};\n\n#ifdef CONFIG_NOUVEAU_DEBUG_MMU\nstatic const char *\nnvkm_vmm_desc_type(const struct nvkm_vmm_desc *desc)\n{\n\tswitch (desc->type) {\n\tcase PGD: return \"PGD\";\n\tcase PGT: return \"PGT\";\n\tcase SPT: return \"SPT\";\n\tcase LPT: return \"LPT\";\n\tdefault:\n\t\treturn \"UNKNOWN\";\n\t}\n}\n\nstatic void\nnvkm_vmm_trace(struct nvkm_vmm_iter *it, char *buf)\n{\n\tint lvl;\n\tfor (lvl = it->max; lvl >= 0; lvl--) {\n\t\tif (lvl >= it->lvl)\n\t\t\tbuf += sprintf(buf,  \"%05x:\", it->pte[lvl]);\n\t\telse\n\t\t\tbuf += sprintf(buf, \"xxxxx:\");\n\t}\n}\n\n#define TRA(i,f,a...) do {                                                     \\\n\tchar _buf[NVKM_VMM_LEVELS_MAX * 7];                                    \\\n\tstruct nvkm_vmm_iter *_it = (i);                                       \\\n\tnvkm_vmm_trace(_it, _buf);                                             \\\n\tVMM_TRACE(_it->vmm, \"%s \"f, _buf, ##a);                                \\\n} while(0)\n#else\n#define TRA(i,f,a...)\n#endif\n\nstatic inline void\nnvkm_vmm_flush_mark(struct nvkm_vmm_iter *it)\n{\n\tit->flush = min(it->flush, it->max - it->lvl);\n}\n\nstatic inline void\nnvkm_vmm_flush(struct nvkm_vmm_iter *it)\n{\n\tif (it->flush != NVKM_VMM_LEVELS_MAX) {\n\t\tif (it->vmm->func->flush) {\n\t\t\tTRA(it, \"flush: %d\", it->flush);\n\t\t\tit->vmm->func->flush(it->vmm, it->flush);\n\t\t}\n\t\tit->flush = NVKM_VMM_LEVELS_MAX;\n\t}\n}\n\nstatic void\nnvkm_vmm_unref_pdes(struct nvkm_vmm_iter *it)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc[it->lvl].type == SPT;\n\tstruct nvkm_vmm_pt *pgd = it->pt[it->lvl + 1];\n\tstruct nvkm_vmm_pt *pgt = it->pt[it->lvl];\n\tstruct nvkm_mmu_pt *pt = pgt->pt[type];\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tu32 pdei = it->pte[it->lvl + 1];\n\n\t/* Recurse up the tree, unreferencing/destroying unneeded PDs. */\n\tit->lvl++;\n\tif (--pgd->refs[0]) {\n\t\tconst struct nvkm_vmm_desc_func *func = desc[it->lvl].func;\n\t\t/* PD has other valid PDEs, so we need a proper update. */\n\t\tTRA(it, \"PDE unmap %s\", nvkm_vmm_desc_type(&desc[it->lvl - 1]));\n\t\tpgt->pt[type] = NULL;\n\t\tif (!pgt->refs[!type]) {\n\t\t\t/* PDE no longer required. */\n\t\t\tif (pgd->pt[0]) {\n\t\t\t\tif (pgt->sparse) {\n\t\t\t\t\tfunc->sparse(vmm, pgd->pt[0], pdei, 1);\n\t\t\t\t\tpgd->pde[pdei] = NVKM_VMM_PDE_SPARSE;\n\t\t\t\t} else {\n\t\t\t\t\tfunc->unmap(vmm, pgd->pt[0], pdei, 1);\n\t\t\t\t\tpgd->pde[pdei] = NULL;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* Special handling for Tesla-class GPUs,\n\t\t\t\t * where there's no central PD, but each\n\t\t\t\t * instance has its own embedded PD.\n\t\t\t\t */\n\t\t\t\tfunc->pde(vmm, pgd, pdei);\n\t\t\t\tpgd->pde[pdei] = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\t/* PDE was pointing at dual-PTs and we're removing\n\t\t\t * one of them, leaving the other in place.\n\t\t\t */\n\t\t\tfunc->pde(vmm, pgd, pdei);\n\t\t}\n\n\t\t/* GPU may have cached the PTs, flush before freeing. */\n\t\tnvkm_vmm_flush_mark(it);\n\t\tnvkm_vmm_flush(it);\n\t} else {\n\t\t/* PD has no valid PDEs left, so we can just destroy it. */\n\t\tnvkm_vmm_unref_pdes(it);\n\t}\n\n\t/* Destroy PD/PT. */\n\tTRA(it, \"PDE free %s\", nvkm_vmm_desc_type(&desc[it->lvl - 1]));\n\tnvkm_mmu_ptc_put(vmm->mmu, vmm->bootstrapped, &pt);\n\tif (!pgt->refs[!type])\n\t\tnvkm_vmm_pt_del(&pgt);\n\tit->lvl--;\n}\n\nstatic void\nnvkm_vmm_unref_sptes(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgt,\n\t\t     const struct nvkm_vmm_desc *desc, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *pair = it->page[-1].desc;\n\tconst u32 sptb = desc->bits - pair->bits;\n\tconst u32 sptn = 1 << sptb;\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tu32 spti = ptei & (sptn - 1), lpti, pteb;\n\n\t/* Determine how many SPTEs are being touched under each LPTE,\n\t * and drop reference counts.\n\t */\n\tfor (lpti = ptei >> sptb; ptes; spti = 0, lpti++) {\n\t\tconst u32 pten = min(sptn - spti, ptes);\n\t\tpgt->pte[lpti] -= pten;\n\t\tptes -= pten;\n\t}\n\n\t/* We're done here if there's no corresponding LPT. */\n\tif (!pgt->refs[0])\n\t\treturn;\n\n\tfor (ptei = pteb = ptei >> sptb; ptei < lpti; pteb = ptei) {\n\t\t/* Skip over any LPTEs that still have valid SPTEs. */\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_SPTES) {\n\t\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\t\tif (!(pgt->pte[ptei] & NVKM_VMM_PTE_SPTES))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* As there's no more non-UNMAPPED SPTEs left in the range\n\t\t * covered by a number of LPTEs, the LPTEs once again take\n\t\t * control over their address range.\n\t\t *\n\t\t * Determine how many LPTEs need to transition state.\n\t\t */\n\t\tpgt->pte[ptei] &= ~NVKM_VMM_PTE_VALID;\n\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\tif (pgt->pte[ptei] & NVKM_VMM_PTE_SPTES)\n\t\t\t\tbreak;\n\t\t\tpgt->pte[ptei] &= ~NVKM_VMM_PTE_VALID;\n\t\t}\n\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_SPARSE) {\n\t\t\tTRA(it, \"LPTE %05x: U -> S %d PTEs\", pteb, ptes);\n\t\t\tpair->func->sparse(vmm, pgt->pt[0], pteb, ptes);\n\t\t} else\n\t\tif (pair->func->invalid) {\n\t\t\t/* If the MMU supports it, restore the LPTE to the\n\t\t\t * INVALID state to tell the MMU there is no point\n\t\t\t * trying to fetch the corresponding SPTEs.\n\t\t\t */\n\t\t\tTRA(it, \"LPTE %05x: U -> I %d PTEs\", pteb, ptes);\n\t\t\tpair->func->invalid(vmm, pgt->pt[0], pteb, ptes);\n\t\t}\n\t}\n}\n\nstatic bool\nnvkm_vmm_unref_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc->type == SPT;\n\tstruct nvkm_vmm_pt *pgt = it->pt[0];\n\n\t/* Drop PTE references. */\n\tpgt->refs[type] -= ptes;\n\n\t/* Dual-PTs need special handling, unless PDE becoming invalid. */\n\tif (desc->type == SPT && (pgt->refs[0] || pgt->refs[1]))\n\t\tnvkm_vmm_unref_sptes(it, pgt, desc, ptei, ptes);\n\n\t/* PT no longer neeed?  Destroy it. */\n\tif (!pgt->refs[type]) {\n\t\tit->lvl++;\n\t\tTRA(it, \"%s empty\", nvkm_vmm_desc_type(desc));\n\t\tit->lvl--;\n\t\tnvkm_vmm_unref_pdes(it);\n\t\treturn false; /* PTE writes for unmap() not necessary. */\n\t}\n\n\treturn true;\n}\n\nstatic void\nnvkm_vmm_ref_sptes(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgt,\n\t\t   const struct nvkm_vmm_desc *desc, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *pair = it->page[-1].desc;\n\tconst u32 sptb = desc->bits - pair->bits;\n\tconst u32 sptn = 1 << sptb;\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tu32 spti = ptei & (sptn - 1), lpti, pteb;\n\n\t/* Determine how many SPTEs are being touched under each LPTE,\n\t * and increase reference counts.\n\t */\n\tfor (lpti = ptei >> sptb; ptes; spti = 0, lpti++) {\n\t\tconst u32 pten = min(sptn - spti, ptes);\n\t\tpgt->pte[lpti] += pten;\n\t\tptes -= pten;\n\t}\n\n\t/* We're done here if there's no corresponding LPT. */\n\tif (!pgt->refs[0])\n\t\treturn;\n\n\tfor (ptei = pteb = ptei >> sptb; ptei < lpti; pteb = ptei) {\n\t\t/* Skip over any LPTEs that already have valid SPTEs. */\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_VALID) {\n\t\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\t\tif (!(pgt->pte[ptei] & NVKM_VMM_PTE_VALID))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* As there are now non-UNMAPPED SPTEs in the range covered\n\t\t * by a number of LPTEs, we need to transfer control of the\n\t\t * address range to the SPTEs.\n\t\t *\n\t\t * Determine how many LPTEs need to transition state.\n\t\t */\n\t\tpgt->pte[ptei] |= NVKM_VMM_PTE_VALID;\n\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\tif (pgt->pte[ptei] & NVKM_VMM_PTE_VALID)\n\t\t\t\tbreak;\n\t\t\tpgt->pte[ptei] |= NVKM_VMM_PTE_VALID;\n\t\t}\n\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_SPARSE) {\n\t\t\tconst u32 spti = pteb * sptn;\n\t\t\tconst u32 sptc = ptes * sptn;\n\t\t\t/* The entire LPTE is marked as sparse, we need\n\t\t\t * to make sure that the SPTEs are too.\n\t\t\t */\n\t\t\tTRA(it, \"SPTE %05x: U -> S %d PTEs\", spti, sptc);\n\t\t\tdesc->func->sparse(vmm, pgt->pt[1], spti, sptc);\n\t\t\t/* Sparse LPTEs prevent SPTEs from being accessed. */\n\t\t\tTRA(it, \"LPTE %05x: S -> U %d PTEs\", pteb, ptes);\n\t\t\tpair->func->unmap(vmm, pgt->pt[0], pteb, ptes);\n\t\t} else\n\t\tif (pair->func->invalid) {\n\t\t\t/* MMU supports blocking SPTEs by marking an LPTE\n\t\t\t * as INVALID.  We need to reverse that here.\n\t\t\t */\n\t\t\tTRA(it, \"LPTE %05x: I -> U %d PTEs\", pteb, ptes);\n\t\t\tpair->func->unmap(vmm, pgt->pt[0], pteb, ptes);\n\t\t}\n\t}\n}\n\nstatic bool\nnvkm_vmm_ref_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc->type == SPT;\n\tstruct nvkm_vmm_pt *pgt = it->pt[0];\n\n\t/* Take PTE references. */\n\tpgt->refs[type] += ptes;\n\n\t/* Dual-PTs need special handling. */\n\tif (desc->type == SPT)\n\t\tnvkm_vmm_ref_sptes(it, pgt, desc, ptei, ptes);\n\n\treturn true;\n}\n\nstatic void\nnvkm_vmm_sparse_ptes(const struct nvkm_vmm_desc *desc,\n\t\t     struct nvkm_vmm_pt *pgt, u32 ptei, u32 ptes)\n{\n\tif (desc->type == PGD) {\n\t\twhile (ptes--)\n\t\t\tpgt->pde[ptei++] = NVKM_VMM_PDE_SPARSE;\n\t} else\n\tif (desc->type == LPT) {\n\t\tmemset(&pgt->pte[ptei], NVKM_VMM_PTE_SPARSE, ptes);\n\t}\n}\n\nstatic bool\nnvkm_vmm_sparse_unref_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tstruct nvkm_vmm_pt *pt = it->pt[0];\n\tif (it->desc->type == PGD)\n\t\tmemset(&pt->pde[ptei], 0x00, sizeof(pt->pde[0]) * ptes);\n\telse\n\tif (it->desc->type == LPT)\n\t\tmemset(&pt->pte[ptei], 0x00, sizeof(pt->pte[0]) * ptes);\n\treturn nvkm_vmm_unref_ptes(it, ptei, ptes);\n}\n\nstatic bool\nnvkm_vmm_sparse_ref_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tnvkm_vmm_sparse_ptes(it->desc, it->pt[0], ptei, ptes);\n\treturn nvkm_vmm_ref_ptes(it, ptei, ptes);\n}\n\nstatic bool\nnvkm_vmm_ref_hwpt(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgd, u32 pdei)\n{\n\tconst struct nvkm_vmm_desc *desc = &it->desc[it->lvl - 1];\n\tconst int type = desc->type == SPT;\n\tstruct nvkm_vmm_pt *pgt = pgd->pde[pdei];\n\tconst bool zero = !pgt->sparse && !desc->func->invalid;\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tstruct nvkm_mmu *mmu = vmm->mmu;\n\tstruct nvkm_mmu_pt *pt;\n\tu32 pten = 1 << desc->bits;\n\tu32 pteb, ptei, ptes;\n\tu32 size = desc->size * pten;\n\n\tpgd->refs[0]++;\n\n\tpgt->pt[type] = nvkm_mmu_ptc_get(mmu, size, desc->align, zero);\n\tif (!pgt->pt[type]) {\n\t\tit->lvl--;\n\t\tnvkm_vmm_unref_pdes(it);\n\t\treturn false;\n\t}\n\n\tif (zero)\n\t\tgoto done;\n\n\tpt = pgt->pt[type];\n\n\tif (desc->type == LPT && pgt->refs[1]) {\n\t\t/* SPT already exists covering the same range as this LPT,\n\t\t * which means we need to be careful that any LPTEs which\n\t\t * overlap valid SPTEs are unmapped as opposed to invalid\n\t\t * or sparse, which would prevent the MMU from looking at\n\t\t * the SPTEs on some GPUs.\n\t\t */\n\t\tfor (ptei = pteb = 0; ptei < pten; pteb = ptei) {\n\t\t\tbool spte = pgt->pte[ptei] & NVKM_VMM_PTE_SPTES;\n\t\t\tfor (ptes = 1, ptei++; ptei < pten; ptes++, ptei++) {\n\t\t\t\tbool next = pgt->pte[ptei] & NVKM_VMM_PTE_SPTES;\n\t\t\t\tif (spte != next)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!spte) {\n\t\t\t\tif (pgt->sparse)\n\t\t\t\t\tdesc->func->sparse(vmm, pt, pteb, ptes);\n\t\t\t\telse\n\t\t\t\t\tdesc->func->invalid(vmm, pt, pteb, ptes);\n\t\t\t\tmemset(&pgt->pte[pteb], 0x00, ptes);\n\t\t\t} else {\n\t\t\t\tdesc->func->unmap(vmm, pt, pteb, ptes);\n\t\t\t\twhile (ptes--)\n\t\t\t\t\tpgt->pte[pteb++] |= NVKM_VMM_PTE_VALID;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (pgt->sparse) {\n\t\t\tnvkm_vmm_sparse_ptes(desc, pgt, 0, pten);\n\t\t\tdesc->func->sparse(vmm, pt, 0, pten);\n\t\t} else {\n\t\t\tdesc->func->invalid(vmm, pt, 0, pten);\n\t\t}\n\t}\n\ndone:\n\tTRA(it, \"PDE write %s\", nvkm_vmm_desc_type(desc));\n\tit->desc[it->lvl].func->pde(it->vmm, pgd, pdei);\n\tnvkm_vmm_flush_mark(it);\n\treturn true;\n}\n\nstatic bool\nnvkm_vmm_ref_swpt(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgd, u32 pdei)\n{\n\tconst struct nvkm_vmm_desc *desc = &it->desc[it->lvl - 1];\n\tstruct nvkm_vmm_pt *pgt = pgd->pde[pdei];\n\n\tpgt = nvkm_vmm_pt_new(desc, NVKM_VMM_PDE_SPARSED(pgt), it->page);\n\tif (!pgt) {\n\t\tif (!pgd->refs[0])\n\t\t\tnvkm_vmm_unref_pdes(it);\n\t\treturn false;\n\t}\n\n\tpgd->pde[pdei] = pgt;\n\treturn true;\n}\n\nstatic inline u64\nnvkm_vmm_iter(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t      u64 addr, u64 size, const char *name, bool ref,\n\t      bool (*REF_PTES)(struct nvkm_vmm_iter *, u32, u32),\n\t      nvkm_vmm_pte_func MAP_PTES, struct nvkm_vmm_map *map,\n\t      nvkm_vmm_pxe_func CLR_PTES)\n{\n\tconst struct nvkm_vmm_desc *desc = page->desc;\n\tstruct nvkm_vmm_iter it;\n\tu64 bits = addr >> page->shift;\n\n\tit.page = page;\n\tit.desc = desc;\n\tit.vmm = vmm;\n\tit.cnt = size >> page->shift;\n\tit.flush = NVKM_VMM_LEVELS_MAX;\n\n\t/* Deconstruct address into PTE indices for each mapping level. */\n\tfor (it.lvl = 0; desc[it.lvl].bits; it.lvl++) {\n\t\tit.pte[it.lvl] = bits & ((1 << desc[it.lvl].bits) - 1);\n\t\tbits >>= desc[it.lvl].bits;\n\t}\n\tit.max = --it.lvl;\n\tit.pt[it.max] = vmm->pd;\n\n\tit.lvl = 0;\n\tTRA(&it, \"%s: %016llx %016llx %d %lld PTEs\", name,\n\t         addr, size, page->shift, it.cnt);\n\tit.lvl = it.max;\n\n\t/* Depth-first traversal of page tables. */\n\twhile (it.cnt) {\n\t\tstruct nvkm_vmm_pt *pgt = it.pt[it.lvl];\n\t\tconst int type = desc->type == SPT;\n\t\tconst u32 pten = 1 << desc->bits;\n\t\tconst u32 ptei = it.pte[0];\n\t\tconst u32 ptes = min_t(u64, it.cnt, pten - ptei);\n\n\t\t/* Walk down the tree, finding page tables for each level. */\n\t\tfor (; it.lvl; it.lvl--) {\n\t\t\tconst u32 pdei = it.pte[it.lvl];\n\t\t\tstruct nvkm_vmm_pt *pgd = pgt;\n\n\t\t\t/* Software PT. */\n\t\t\tif (ref && NVKM_VMM_PDE_INVALID(pgd->pde[pdei])) {\n\t\t\t\tif (!nvkm_vmm_ref_swpt(&it, pgd, pdei))\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\tit.pt[it.lvl - 1] = pgt = pgd->pde[pdei];\n\n\t\t\t/* Hardware PT.\n\t\t\t *\n\t\t\t * This is a separate step from above due to GF100 and\n\t\t\t * newer having dual page tables at some levels, which\n\t\t\t * are refcounted independently.\n\t\t\t */\n\t\t\tif (ref && !pgt->refs[desc[it.lvl - 1].type == SPT]) {\n\t\t\t\tif (!nvkm_vmm_ref_hwpt(&it, pgd, pdei))\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\t\t}\n\n\t\t/* Handle PTE updates. */\n\t\tif (!REF_PTES || REF_PTES(&it, ptei, ptes)) {\n\t\t\tstruct nvkm_mmu_pt *pt = pgt->pt[type];\n\t\t\tif (MAP_PTES || CLR_PTES) {\n\t\t\t\tif (MAP_PTES)\n\t\t\t\t\tMAP_PTES(vmm, pt, ptei, ptes, map);\n\t\t\t\telse\n\t\t\t\t\tCLR_PTES(vmm, pt, ptei, ptes);\n\t\t\t\tnvkm_vmm_flush_mark(&it);\n\t\t\t}\n\t\t}\n\n\t\t/* Walk back up the tree to the next position. */\n\t\tit.pte[it.lvl] += ptes;\n\t\tit.cnt -= ptes;\n\t\tif (it.cnt) {\n\t\t\twhile (it.pte[it.lvl] == (1 << desc[it.lvl].bits)) {\n\t\t\t\tit.pte[it.lvl++] = 0;\n\t\t\t\tit.pte[it.lvl]++;\n\t\t\t}\n\t\t}\n\t};\n\n\tnvkm_vmm_flush(&it);\n\treturn ~0ULL;\n\nfail:\n\t/* Reconstruct the failure address so the caller is able to\n\t * reverse any partially completed operations.\n\t */\n\taddr = it.pte[it.max--];\n\tdo {\n\t\taddr  = addr << desc[it.max].bits;\n\t\taddr |= it.pte[it.max];\n\t} while (it.max--);\n\n\treturn addr << page->shift;\n}\n\nstatic void\nnvkm_vmm_ptes_sparse_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\t u64 addr, u64 size)\n{\n\tnvkm_vmm_iter(vmm, page, addr, size, \"sparse unref\", false,\n\t\t      nvkm_vmm_sparse_unref_ptes, NULL, NULL,\n\t\t      page->desc->func->invalid ?\n\t\t      page->desc->func->invalid : page->desc->func->unmap);\n}\n\nstatic int\nnvkm_vmm_ptes_sparse_get(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\t u64 addr, u64 size)\n{\n\tif ((page->type & NVKM_VMM_PAGE_SPARSE)) {\n\t\tu64 fail = nvkm_vmm_iter(vmm, page, addr, size, \"sparse ref\",\n\t\t\t\t\t true, nvkm_vmm_sparse_ref_ptes, NULL,\n\t\t\t\t\t NULL, page->desc->func->sparse);\n\t\tif (fail != ~0ULL) {\n\t\t\tif ((size = fail - addr))\n\t\t\t\tnvkm_vmm_ptes_sparse_put(vmm, page, addr, size);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int\nnvkm_vmm_ptes_sparse(struct nvkm_vmm *vmm, u64 addr, u64 size, bool ref)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tint m = 0, i;\n\tu64 start = addr;\n\tu64 block;\n\n\twhile (size) {\n\t\t/* Limit maximum page size based on remaining size. */\n\t\twhile (size < (1ULL << page[m].shift))\n\t\t\tm++;\n\t\ti = m;\n\n\t\t/* Find largest page size suitable for alignment. */\n\t\twhile (!IS_ALIGNED(addr, 1ULL << page[i].shift))\n\t\t\ti++;\n\n\t\t/* Determine number of PTEs at this page size. */\n\t\tif (i != m) {\n\t\t\t/* Limited to alignment boundary of next page size. */\n\t\t\tu64 next = 1ULL << page[i - 1].shift;\n\t\t\tu64 part = ALIGN(addr, next) - addr;\n\t\t\tif (size - part >= next)\n\t\t\t\tblock = (part >> page[i].shift) << page[i].shift;\n\t\t\telse\n\t\t\t\tblock = (size >> page[i].shift) << page[i].shift;\n\t\t} else {\n\t\t\tblock = (size >> page[i].shift) << page[i].shift;\n\t\t}\n\n\t\t/* Perform operation. */\n\t\tif (ref) {\n\t\t\tint ret = nvkm_vmm_ptes_sparse_get(vmm, &page[i], addr, block);\n\t\t\tif (ret) {\n\t\t\t\tif ((size = addr - start))\n\t\t\t\t\tnvkm_vmm_ptes_sparse(vmm, start, size, false);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t} else {\n\t\t\tnvkm_vmm_ptes_sparse_put(vmm, &page[i], addr, block);\n\t\t}\n\n\t\tsize -= block;\n\t\taddr += block;\n\t}\n\n\treturn 0;\n}\n\nstatic void\nnvkm_vmm_ptes_unmap_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\tu64 addr, u64 size, bool sparse)\n{\n\tconst struct nvkm_vmm_desc_func *func = page->desc->func;\n\tnvkm_vmm_iter(vmm, page, addr, size, \"unmap + unref\",\n\t\t      false, nvkm_vmm_unref_ptes, NULL, NULL,\n\t\t      sparse ? func->sparse : func->invalid ? func->invalid :\n\t\t\t\t\t\t\t      func->unmap);\n}\n\nstatic int\nnvkm_vmm_ptes_get_map(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t      u64 addr, u64 size, struct nvkm_vmm_map *map,\n\t\t      nvkm_vmm_pte_func func)\n{\n\tu64 fail = nvkm_vmm_iter(vmm, page, addr, size, \"ref + map\", true,\n\t\t\t\t nvkm_vmm_ref_ptes, func, map, NULL);\n\tif (fail != ~0ULL) {\n\t\tif ((size = fail - addr))\n\t\t\tnvkm_vmm_ptes_unmap_put(vmm, page, addr, size, false);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void\nnvkm_vmm_ptes_unmap(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t    u64 addr, u64 size, bool sparse)\n{\n\tconst struct nvkm_vmm_desc_func *func = page->desc->func;\n\tnvkm_vmm_iter(vmm, page, addr, size, \"unmap\", false, NULL, NULL, NULL,\n\t\t      sparse ? func->sparse : func->invalid ? func->invalid :\n\t\t\t\t\t\t\t      func->unmap);\n}\n\nstatic void\nnvkm_vmm_ptes_map(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t  u64 addr, u64 size, struct nvkm_vmm_map *map,\n\t\t  nvkm_vmm_pte_func func)\n{\n\tnvkm_vmm_iter(vmm, page, addr, size, \"map\", false,\n\t\t      NULL, func, map, NULL);\n}\n\nstatic void\nnvkm_vmm_ptes_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t  u64 addr, u64 size)\n{\n\tnvkm_vmm_iter(vmm, page, addr, size, \"unref\", false,\n\t\t      nvkm_vmm_unref_ptes, NULL, NULL, NULL);\n}\n\nstatic int\nnvkm_vmm_ptes_get(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t  u64 addr, u64 size)\n{\n\tu64 fail = nvkm_vmm_iter(vmm, page, addr, size, \"ref\", true,\n\t\t\t\t nvkm_vmm_ref_ptes, NULL, NULL, NULL);\n\tif (fail != ~0ULL) {\n\t\tif (fail != addr)\n\t\t\tnvkm_vmm_ptes_put(vmm, page, addr, fail - addr);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic inline struct nvkm_vma *\nnvkm_vma_new(u64 addr, u64 size)\n{\n\tstruct nvkm_vma *vma = kzalloc(sizeof(*vma), GFP_KERNEL);\n\tif (vma) {\n\t\tvma->addr = addr;\n\t\tvma->size = size;\n\t\tvma->page = NVKM_VMA_PAGE_NONE;\n\t\tvma->refd = NVKM_VMA_PAGE_NONE;\n\t}\n\treturn vma;\n}\n\nstruct nvkm_vma *\nnvkm_vma_tail(struct nvkm_vma *vma, u64 tail)\n{\n\tstruct nvkm_vma *new;\n\n\tBUG_ON(vma->size == tail);\n\n\tif (!(new = nvkm_vma_new(vma->addr + (vma->size - tail), tail)))\n\t\treturn NULL;\n\tvma->size -= tail;\n\n\tnew->mapref = vma->mapref;\n\tnew->sparse = vma->sparse;\n\tnew->page = vma->page;\n\tnew->refd = vma->refd;\n\tnew->used = vma->used;\n\tnew->part = vma->part;\n\tnew->user = vma->user;\n\tnew->busy = vma->busy;\n\tlist_add(&new->head, &vma->head);\n\treturn new;\n}\n\nstatic inline void\nnvkm_vmm_free_remove(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\trb_erase(&vma->tree, &vmm->free);\n}\n\nstatic inline void\nnvkm_vmm_free_delete(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tnvkm_vmm_free_remove(vmm, vma);\n\tlist_del(&vma->head);\n\tkfree(vma);\n}\n\nstatic void\nnvkm_vmm_free_insert(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct rb_node **ptr = &vmm->free.rb_node;\n\tstruct rb_node *parent = NULL;\n\n\twhile (*ptr) {\n\t\tstruct nvkm_vma *this = rb_entry(*ptr, typeof(*this), tree);\n\t\tparent = *ptr;\n\t\tif (vma->size < this->size)\n\t\t\tptr = &parent->rb_left;\n\t\telse\n\t\tif (vma->size > this->size)\n\t\t\tptr = &parent->rb_right;\n\t\telse\n\t\tif (vma->addr < this->addr)\n\t\t\tptr = &parent->rb_left;\n\t\telse\n\t\tif (vma->addr > this->addr)\n\t\t\tptr = &parent->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->tree, parent, ptr);\n\trb_insert_color(&vma->tree, &vmm->free);\n}\n\nstatic inline void\nnvkm_vmm_node_remove(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\trb_erase(&vma->tree, &vmm->root);\n}\n\nstatic inline void\nnvkm_vmm_node_delete(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tnvkm_vmm_node_remove(vmm, vma);\n\tlist_del(&vma->head);\n\tkfree(vma);\n}\n\nstatic void\nnvkm_vmm_node_insert(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct rb_node **ptr = &vmm->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\n\twhile (*ptr) {\n\t\tstruct nvkm_vma *this = rb_entry(*ptr, typeof(*this), tree);\n\t\tparent = *ptr;\n\t\tif (vma->addr < this->addr)\n\t\t\tptr = &parent->rb_left;\n\t\telse\n\t\tif (vma->addr > this->addr)\n\t\t\tptr = &parent->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->tree, parent, ptr);\n\trb_insert_color(&vma->tree, &vmm->root);\n}\n\nstruct nvkm_vma *\nnvkm_vmm_node_search(struct nvkm_vmm *vmm, u64 addr)\n{\n\tstruct rb_node *node = vmm->root.rb_node;\n\twhile (node) {\n\t\tstruct nvkm_vma *vma = rb_entry(node, typeof(*vma), tree);\n\t\tif (addr < vma->addr)\n\t\t\tnode = node->rb_left;\n\t\telse\n\t\tif (addr >= vma->addr + vma->size)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn vma;\n\t}\n\treturn NULL;\n}\n\n#define node(root, dir) (((root)->head.dir == &vmm->list) ? NULL :             \\\n\tlist_entry((root)->head.dir, struct nvkm_vma, head))\n\nstatic struct nvkm_vma *\nnvkm_vmm_node_merge(struct nvkm_vmm *vmm, struct nvkm_vma *prev,\n\t\t    struct nvkm_vma *vma, struct nvkm_vma *next, u64 size)\n{\n\tif (next) {\n\t\tif (vma->size == size) {\n\t\t\tvma->size += next->size;\n\t\t\tnvkm_vmm_node_delete(vmm, next);\n\t\t\tif (prev) {\n\t\t\t\tprev->size += vma->size;\n\t\t\t\tnvkm_vmm_node_delete(vmm, vma);\n\t\t\t\treturn prev;\n\t\t\t}\n\t\t\treturn vma;\n\t\t}\n\t\tBUG_ON(prev);\n\n\t\tnvkm_vmm_node_remove(vmm, next);\n\t\tvma->size -= size;\n\t\tnext->addr -= size;\n\t\tnext->size += size;\n\t\tnvkm_vmm_node_insert(vmm, next);\n\t\treturn next;\n\t}\n\n\tif (prev) {\n\t\tif (vma->size != size) {\n\t\t\tnvkm_vmm_node_remove(vmm, vma);\n\t\t\tprev->size += size;\n\t\t\tvma->addr += size;\n\t\t\tvma->size -= size;\n\t\t\tnvkm_vmm_node_insert(vmm, vma);\n\t\t} else {\n\t\t\tprev->size += vma->size;\n\t\t\tnvkm_vmm_node_delete(vmm, vma);\n\t\t}\n\t\treturn prev;\n\t}\n\n\treturn vma;\n}\n\nstruct nvkm_vma *\nnvkm_vmm_node_split(struct nvkm_vmm *vmm,\n\t\t    struct nvkm_vma *vma, u64 addr, u64 size)\n{\n\tstruct nvkm_vma *prev = NULL;\n\n\tif (vma->addr != addr) {\n\t\tprev = vma;\n\t\tif (!(vma = nvkm_vma_tail(vma, vma->size + vma->addr - addr)))\n\t\t\treturn NULL;\n\t\tvma->part = true;\n\t\tnvkm_vmm_node_insert(vmm, vma);\n\t}\n\n\tif (vma->size != size) {\n\t\tstruct nvkm_vma *tmp;\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size - size))) {\n\t\t\tnvkm_vmm_node_merge(vmm, prev, vma, NULL, vma->size);\n\t\t\treturn NULL;\n\t\t}\n\t\ttmp->part = true;\n\t\tnvkm_vmm_node_insert(vmm, tmp);\n\t}\n\n\treturn vma;\n}\n\nstatic void\nnvkm_vmm_dtor(struct nvkm_vmm *vmm)\n{\n\tstruct nvkm_vma *vma;\n\tstruct rb_node *node;\n\n\twhile ((node = rb_first(&vmm->root))) {\n\t\tstruct nvkm_vma *vma = rb_entry(node, typeof(*vma), tree);\n\t\tnvkm_vmm_put(vmm, &vma);\n\t}\n\n\tif (vmm->bootstrapped) {\n\t\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\t\tconst u64 limit = vmm->limit - vmm->start;\n\n\t\twhile (page[1].shift)\n\t\t\tpage++;\n\n\t\tnvkm_mmu_ptc_dump(vmm->mmu);\n\t\tnvkm_vmm_ptes_put(vmm, page, vmm->start, limit);\n\t}\n\n\tvma = list_first_entry(&vmm->list, typeof(*vma), head);\n\tlist_del(&vma->head);\n\tkfree(vma);\n\tWARN_ON(!list_empty(&vmm->list));\n\n\tif (vmm->nullp) {\n\t\tdma_free_coherent(vmm->mmu->subdev.device->dev, 16 * 1024,\n\t\t\t\t  vmm->nullp, vmm->null);\n\t}\n\n\tif (vmm->pd) {\n\t\tnvkm_mmu_ptc_put(vmm->mmu, true, &vmm->pd->pt[0]);\n\t\tnvkm_vmm_pt_del(&vmm->pd);\n\t}\n}\n\nint\nnvkm_vmm_ctor(const struct nvkm_vmm_func *func, struct nvkm_mmu *mmu,\n\t      u32 pd_header, u64 addr, u64 size, struct lock_class_key *key,\n\t      const char *name, struct nvkm_vmm *vmm)\n{\n\tstatic struct lock_class_key _key;\n\tconst struct nvkm_vmm_page *page = func->page;\n\tconst struct nvkm_vmm_desc *desc;\n\tstruct nvkm_vma *vma;\n\tint levels, bits = 0;\n\n\tvmm->func = func;\n\tvmm->mmu = mmu;\n\tvmm->name = name;\n\tvmm->debug = mmu->subdev.debug;\n\tkref_init(&vmm->kref);\n\n\t__mutex_init(&vmm->mutex, \"&vmm->mutex\", key ? key : &_key);\n\n\t/* Locate the smallest page size supported by the backend, it will\n\t * have the the deepest nesting of page tables.\n\t */\n\twhile (page[1].shift)\n\t\tpage++;\n\n\t/* Locate the structure that describes the layout of the top-level\n\t * page table, and determine the number of valid bits in a virtual\n\t * address.\n\t */\n\tfor (levels = 0, desc = page->desc; desc->bits; desc++, levels++)\n\t\tbits += desc->bits;\n\tbits += page->shift;\n\tdesc--;\n\n\tif (WARN_ON(levels > NVKM_VMM_LEVELS_MAX))\n\t\treturn -EINVAL;\n\n\tvmm->start = addr;\n\tvmm->limit = size ? (addr + size) : (1ULL << bits);\n\tif (vmm->start > vmm->limit || vmm->limit > (1ULL << bits))\n\t\treturn -EINVAL;\n\n\t/* Allocate top-level page table. */\n\tvmm->pd = nvkm_vmm_pt_new(desc, false, NULL);\n\tif (!vmm->pd)\n\t\treturn -ENOMEM;\n\tvmm->pd->refs[0] = 1;\n\tINIT_LIST_HEAD(&vmm->join);\n\n\t/* ... and the GPU storage for it, except on Tesla-class GPUs that\n\t * have the PD embedded in the instance structure.\n\t */\n\tif (desc->size) {\n\t\tconst u32 size = pd_header + desc->size * (1 << desc->bits);\n\t\tvmm->pd->pt[0] = nvkm_mmu_ptc_get(mmu, size, desc->align, true);\n\t\tif (!vmm->pd->pt[0])\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Initialise address-space MM. */\n\tINIT_LIST_HEAD(&vmm->list);\n\tvmm->free = RB_ROOT;\n\tvmm->root = RB_ROOT;\n\n\tif (!(vma = nvkm_vma_new(vmm->start, vmm->limit - vmm->start)))\n\t\treturn -ENOMEM;\n\n\tnvkm_vmm_free_insert(vmm, vma);\n\tlist_add(&vma->head, &vmm->list);\n\treturn 0;\n}\n\nint\nnvkm_vmm_new_(const struct nvkm_vmm_func *func, struct nvkm_mmu *mmu,\n\t      u32 hdr, u64 addr, u64 size, struct lock_class_key *key,\n\t      const char *name, struct nvkm_vmm **pvmm)\n{\n\tif (!(*pvmm = kzalloc(sizeof(**pvmm), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\treturn nvkm_vmm_ctor(func, mmu, hdr, addr, size, key, name, *pvmm);\n}\n\nvoid\nnvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *next = node(vma, next);\n\tstruct nvkm_vma *prev = NULL;\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\n\tif (!vma->part || ((prev = node(vma, prev)), prev->memory))\n\t\tprev = NULL;\n\tif (!next->part || next->memory)\n\t\tnext = NULL;\n\tnvkm_vmm_node_merge(vmm, prev, vma, next, vma->size);\n}\n\nvoid\nnvkm_vmm_unmap_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[vma->refd];\n\n\tif (vma->mapref) {\n\t\tnvkm_vmm_ptes_unmap_put(vmm, page, vma->addr, vma->size, vma->sparse);\n\t\tvma->refd = NVKM_VMA_PAGE_NONE;\n\t} else {\n\t\tnvkm_vmm_ptes_unmap(vmm, page, vma->addr, vma->size, vma->sparse);\n\t}\n\n\tnvkm_vmm_unmap_region(vmm, vma);\n}\n\nvoid\nnvkm_vmm_unmap(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tif (vma->memory) {\n\t\tmutex_lock(&vmm->mutex);\n\t\tnvkm_vmm_unmap_locked(vmm, vma);\n\t\tmutex_unlock(&vmm->mutex);\n\t}\n}\n\nstatic int\nnvkm_vmm_map_valid(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t   void *argv, u32 argc, struct nvkm_vmm_map *map)\n{\n\tswitch (nvkm_memory_target(map->memory)) {\n\tcase NVKM_MEM_TARGET_VRAM:\n\t\tif (!(map->page->type & NVKM_VMM_PAGE_VRAM)) {\n\t\t\tVMM_DEBUG(vmm, \"%d !VRAM\", map->page->shift);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase NVKM_MEM_TARGET_HOST:\n\tcase NVKM_MEM_TARGET_NCOH:\n\t\tif (!(map->page->type & NVKM_VMM_PAGE_HOST)) {\n\t\t\tVMM_DEBUG(vmm, \"%d !HOST\", map->page->shift);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -ENOSYS;\n\t}\n\n\tif (!IS_ALIGNED(     vma->addr, 1ULL << map->page->shift) ||\n\t    !IS_ALIGNED((u64)vma->size, 1ULL << map->page->shift) ||\n\t    !IS_ALIGNED(   map->offset, 1ULL << map->page->shift) ||\n\t    nvkm_memory_page(map->memory) < map->page->shift) {\n\t\tVMM_DEBUG(vmm, \"alignment %016llx %016llx %016llx %d %d\",\n\t\t    vma->addr, (u64)vma->size, map->offset, map->page->shift,\n\t\t    nvkm_memory_page(map->memory));\n\t\treturn -EINVAL;\n\t}\n\n\treturn vmm->func->valid(vmm, argv, argc, map);\n}\n\nstatic int\nnvkm_vmm_map_choose(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t    void *argv, u32 argc, struct nvkm_vmm_map *map)\n{\n\tfor (map->page = vmm->func->page; map->page->shift; map->page++) {\n\t\tVMM_DEBUG(vmm, \"trying %d\", map->page->shift);\n\t\tif (!nvkm_vmm_map_valid(vmm, vma, argv, argc, map))\n\t\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int\nnvkm_vmm_map_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t    void *argv, u32 argc, struct nvkm_vmm_map *map)\n{\n\tnvkm_vmm_pte_func func;\n\tint ret;\n\n\t/* Make sure we won't overrun the end of the memory object. */\n\tif (unlikely(nvkm_memory_size(map->memory) < map->offset + vma->size)) {\n\t\tVMM_DEBUG(vmm, \"overrun %016llx %016llx %016llx\",\n\t\t\t  nvkm_memory_size(map->memory),\n\t\t\t  map->offset, (u64)vma->size);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Check remaining arguments for validity. */\n\tif (vma->page == NVKM_VMA_PAGE_NONE &&\n\t    vma->refd == NVKM_VMA_PAGE_NONE) {\n\t\t/* Find the largest page size we can perform the mapping at. */\n\t\tconst u32 debug = vmm->debug;\n\t\tvmm->debug = 0;\n\t\tret = nvkm_vmm_map_choose(vmm, vma, argv, argc, map);\n\t\tvmm->debug = debug;\n\t\tif (ret) {\n\t\t\tVMM_DEBUG(vmm, \"invalid at any page size\");\n\t\t\tnvkm_vmm_map_choose(vmm, vma, argv, argc, map);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\t/* Page size of the VMA is already pre-determined. */\n\t\tif (vma->refd != NVKM_VMA_PAGE_NONE)\n\t\t\tmap->page = &vmm->func->page[vma->refd];\n\t\telse\n\t\t\tmap->page = &vmm->func->page[vma->page];\n\n\t\tret = nvkm_vmm_map_valid(vmm, vma, argv, argc, map);\n\t\tif (ret) {\n\t\t\tVMM_DEBUG(vmm, \"invalid %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* Deal with the 'offset' argument, and fetch the backend function. */\n\tmap->off = map->offset;\n\tif (map->mem) {\n\t\tfor (; map->off; map->mem = map->mem->next) {\n\t\t\tu64 size = (u64)map->mem->length << NVKM_RAM_MM_SHIFT;\n\t\t\tif (size > map->off)\n\t\t\t\tbreak;\n\t\t\tmap->off -= size;\n\t\t}\n\t\tfunc = map->page->desc->func->mem;\n\t} else\n\tif (map->sgl) {\n\t\tfor (; map->off; map->sgl = sg_next(map->sgl)) {\n\t\t\tu64 size = sg_dma_len(map->sgl);\n\t\t\tif (size > map->off)\n\t\t\t\tbreak;\n\t\t\tmap->off -= size;\n\t\t}\n\t\tfunc = map->page->desc->func->sgl;\n\t} else {\n\t\tmap->dma += map->offset >> PAGE_SHIFT;\n\t\tmap->off  = map->offset & PAGE_MASK;\n\t\tfunc = map->page->desc->func->dma;\n\t}\n\n\t/* Perform the map. */\n\tif (vma->refd == NVKM_VMA_PAGE_NONE) {\n\t\tret = nvkm_vmm_ptes_get_map(vmm, map->page, vma->addr, vma->size, map, func);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvma->refd = map->page - vmm->func->page;\n\t} else {\n\t\tnvkm_vmm_ptes_map(vmm, map->page, vma->addr, vma->size, map, func);\n\t}\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\tvma->memory = nvkm_memory_ref(map->memory);\n\tvma->tags = map->tags;\n\treturn 0;\n}\n\nint\nnvkm_vmm_map(struct nvkm_vmm *vmm, struct nvkm_vma *vma, void *argv, u32 argc,\n\t     struct nvkm_vmm_map *map)\n{\n\tint ret;\n\tmutex_lock(&vmm->mutex);\n\tret = nvkm_vmm_map_locked(vmm, vma, argv, argc, map);\n\tvma->busy = false;\n\tmutex_unlock(&vmm->mutex);\n\treturn ret;\n}\n\nstatic void\nnvkm_vmm_put_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *prev, *next;\n\n\tif ((prev = node(vma, prev)) && !prev->used) {\n\t\tvma->addr  = prev->addr;\n\t\tvma->size += prev->size;\n\t\tnvkm_vmm_free_delete(vmm, prev);\n\t}\n\n\tif ((next = node(vma, next)) && !next->used) {\n\t\tvma->size += next->size;\n\t\tnvkm_vmm_free_delete(vmm, next);\n\t}\n\n\tnvkm_vmm_free_insert(vmm, vma);\n}\n\nvoid\nnvkm_vmm_put_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tstruct nvkm_vma *next = vma;\n\n\tBUG_ON(vma->part);\n\n\tif (vma->mapref || !vma->sparse) {\n\t\tdo {\n\t\t\tconst bool map = next->memory != NULL;\n\t\t\tconst u8  refd = next->refd;\n\t\t\tconst u64 addr = next->addr;\n\t\t\tu64 size = next->size;\n\n\t\t\t/* Merge regions that are in the same state. */\n\t\t\twhile ((next = node(next, next)) && next->part &&\n\t\t\t       (next->memory != NULL) == map &&\n\t\t\t       (next->refd == refd))\n\t\t\t\tsize += next->size;\n\n\t\t\tif (map) {\n\t\t\t\t/* Region(s) are mapped, merge the unmap\n\t\t\t\t * and dereference into a single walk of\n\t\t\t\t * the page tree.\n\t\t\t\t */\n\t\t\t\tnvkm_vmm_ptes_unmap_put(vmm, &page[refd], addr,\n\t\t\t\t\t\t\tsize, vma->sparse);\n\t\t\t} else\n\t\t\tif (refd != NVKM_VMA_PAGE_NONE) {\n\t\t\t\t/* Drop allocation-time PTE references. */\n\t\t\t\tnvkm_vmm_ptes_put(vmm, &page[refd], addr, size);\n\t\t\t}\n\t\t} while (next && next->part);\n\t}\n\n\t/* Merge any mapped regions that were split from the initial\n\t * address-space allocation back into the allocated VMA, and\n\t * release memory/compression resources.\n\t */\n\tnext = vma;\n\tdo {\n\t\tif (next->memory)\n\t\t\tnvkm_vmm_unmap_region(vmm, next);\n\t} while ((next = node(vma, next)) && next->part);\n\n\tif (vma->sparse && !vma->mapref) {\n\t\t/* Sparse region that was allocated with a fixed page size,\n\t\t * meaning all relevant PTEs were referenced once when the\n\t\t * region was allocated, and remained that way, regardless\n\t\t * of whether memory was mapped into it afterwards.\n\t\t *\n\t\t * The process of unmapping, unsparsing, and dereferencing\n\t\t * PTEs can be done in a single page tree walk.\n\t\t */\n\t\tnvkm_vmm_ptes_sparse_put(vmm, &page[vma->refd], vma->addr, vma->size);\n\t} else\n\tif (vma->sparse) {\n\t\t/* Sparse region that wasn't allocated with a fixed page size,\n\t\t * PTE references were taken both at allocation time (to make\n\t\t * the GPU see the region as sparse), and when mapping memory\n\t\t * into the region.\n\t\t *\n\t\t * The latter was handled above, and the remaining references\n\t\t * are dealt with here.\n\t\t */\n\t\tnvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, false);\n\t}\n\n\t/* Remove VMA from the list of allocated nodes. */\n\tnvkm_vmm_node_remove(vmm, vma);\n\n\t/* Merge VMA back into the free list. */\n\tvma->page = NVKM_VMA_PAGE_NONE;\n\tvma->refd = NVKM_VMA_PAGE_NONE;\n\tvma->used = false;\n\tvma->user = false;\n\tnvkm_vmm_put_region(vmm, vma);\n}\n\nvoid\nnvkm_vmm_put(struct nvkm_vmm *vmm, struct nvkm_vma **pvma)\n{\n\tstruct nvkm_vma *vma = *pvma;\n\tif (vma) {\n\t\tmutex_lock(&vmm->mutex);\n\t\tnvkm_vmm_put_locked(vmm, vma);\n\t\tmutex_unlock(&vmm->mutex);\n\t\t*pvma = NULL;\n\t}\n}\n\nint\nnvkm_vmm_get_locked(struct nvkm_vmm *vmm, bool getref, bool mapref, bool sparse,\n\t\t    u8 shift, u8 align, u64 size, struct nvkm_vma **pvma)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[NVKM_VMA_PAGE_NONE];\n\tstruct rb_node *node = NULL, *temp;\n\tstruct nvkm_vma *vma = NULL, *tmp;\n\tu64 addr, tail;\n\tint ret;\n\n\tVMM_TRACE(vmm, \"getref %d mapref %d sparse %d \"\n\t\t       \"shift: %d align: %d size: %016llx\",\n\t\t  getref, mapref, sparse, shift, align, size);\n\n\t/* Zero-sized, or lazily-allocated sparse VMAs, make no sense. */\n\tif (unlikely(!size || (!getref && !mapref && sparse))) {\n\t\tVMM_DEBUG(vmm, \"args %016llx %d %d %d\",\n\t\t\t  size, getref, mapref, sparse);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Tesla-class GPUs can only select page size per-PDE, which means\n\t * we're required to know the mapping granularity up-front to find\n\t * a suitable region of address-space.\n\t *\n\t * The same goes if we're requesting up-front allocation of PTES.\n\t */\n\tif (unlikely((getref || vmm->func->page_block) && !shift)) {\n\t\tVMM_DEBUG(vmm, \"page size required: %d %016llx\",\n\t\t\t  getref, vmm->func->page_block);\n\t\treturn -EINVAL;\n\t}\n\n\t/* If a specific page size was requested, determine its index and\n\t * make sure the requested size is a multiple of the page size.\n\t */\n\tif (shift) {\n\t\tfor (page = vmm->func->page; page->shift; page++) {\n\t\t\tif (shift == page->shift)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!page->shift || !IS_ALIGNED(size, 1ULL << page->shift)) {\n\t\t\tVMM_DEBUG(vmm, \"page %d %016llx\", shift, size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\talign = max_t(u8, align, shift);\n\t} else {\n\t\talign = max_t(u8, align, 12);\n\t}\n\n\t/* Locate smallest block that can possibly satisfy the allocation. */\n\ttemp = vmm->free.rb_node;\n\twhile (temp) {\n\t\tstruct nvkm_vma *this = rb_entry(temp, typeof(*this), tree);\n\t\tif (this->size < size) {\n\t\t\ttemp = temp->rb_right;\n\t\t} else {\n\t\t\tnode = temp;\n\t\t\ttemp = temp->rb_left;\n\t\t}\n\t}\n\n\tif (unlikely(!node))\n\t\treturn -ENOSPC;\n\n\t/* Take into account alignment restrictions, trying larger blocks\n\t * in turn until we find a suitable free block.\n\t */\n\tdo {\n\t\tstruct nvkm_vma *this = rb_entry(node, typeof(*this), tree);\n\t\tstruct nvkm_vma *prev = node(this, prev);\n\t\tstruct nvkm_vma *next = node(this, next);\n\t\tconst int p = page - vmm->func->page;\n\n\t\taddr = this->addr;\n\t\tif (vmm->func->page_block && prev && prev->page != p)\n\t\t\taddr = ALIGN(addr, vmm->func->page_block);\n\t\taddr = ALIGN(addr, 1ULL << align);\n\n\t\ttail = this->addr + this->size;\n\t\tif (vmm->func->page_block && next && next->page != p)\n\t\t\ttail = ALIGN_DOWN(tail, vmm->func->page_block);\n\n\t\tif (addr <= tail && tail - addr >= size) {\n\t\t\tnvkm_vmm_free_remove(vmm, this);\n\t\t\tvma = this;\n\t\t\tbreak;\n\t\t}\n\t} while ((node = rb_next(node)));\n\n\tif (unlikely(!vma))\n\t\treturn -ENOSPC;\n\n\t/* If the VMA we found isn't already exactly the requested size,\n\t * it needs to be split, and the remaining free blocks returned.\n\t */\n\tif (addr != vma->addr) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size + vma->addr - addr))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, vma);\n\t\tvma = tmp;\n\t}\n\n\tif (size != vma->size) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size - size))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, tmp);\n\t}\n\n\t/* Pre-allocate page tables and/or setup sparse mappings. */\n\tif (sparse && getref)\n\t\tret = nvkm_vmm_ptes_sparse_get(vmm, page, vma->addr, vma->size);\n\telse if (sparse)\n\t\tret = nvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, true);\n\telse if (getref)\n\t\tret = nvkm_vmm_ptes_get(vmm, page, vma->addr, vma->size);\n\telse\n\t\tret = 0;\n\tif (ret) {\n\t\tnvkm_vmm_put_region(vmm, vma);\n\t\treturn ret;\n\t}\n\n\tvma->mapref = mapref && !getref;\n\tvma->sparse = sparse;\n\tvma->page = page - vmm->func->page;\n\tvma->refd = getref ? vma->page : NVKM_VMA_PAGE_NONE;\n\tvma->used = true;\n\tnvkm_vmm_node_insert(vmm, vma);\n\t*pvma = vma;\n\treturn 0;\n}\n\nint\nnvkm_vmm_get(struct nvkm_vmm *vmm, u8 page, u64 size, struct nvkm_vma **pvma)\n{\n\tint ret;\n\tmutex_lock(&vmm->mutex);\n\tret = nvkm_vmm_get_locked(vmm, false, true, false, page, 0, size, pvma);\n\tmutex_unlock(&vmm->mutex);\n\treturn ret;\n}\n\nvoid\nnvkm_vmm_part(struct nvkm_vmm *vmm, struct nvkm_memory *inst)\n{\n\tif (inst && vmm->func->part) {\n\t\tmutex_lock(&vmm->mutex);\n\t\tvmm->func->part(vmm, inst);\n\t\tmutex_unlock(&vmm->mutex);\n\t}\n}\n\nint\nnvkm_vmm_join(struct nvkm_vmm *vmm, struct nvkm_memory *inst)\n{\n\tint ret = 0;\n\tif (vmm->func->join) {\n\t\tmutex_lock(&vmm->mutex);\n\t\tret = vmm->func->join(vmm, inst);\n\t\tmutex_unlock(&vmm->mutex);\n\t}\n\treturn ret;\n}\n\nstatic bool\nnvkm_vmm_boot_ptes(struct nvkm_vmm_iter *it, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc->type == SPT;\n\tnvkm_memory_boot(it->pt[0]->pt[type]->memory, it->vmm);\n\treturn false;\n}\n\nint\nnvkm_vmm_boot(struct nvkm_vmm *vmm)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tconst u64 limit = vmm->limit - vmm->start;\n\tint ret;\n\n\twhile (page[1].shift)\n\t\tpage++;\n\n\tret = nvkm_vmm_ptes_get(vmm, page, vmm->start, limit);\n\tif (ret)\n\t\treturn ret;\n\n\tnvkm_vmm_iter(vmm, page, vmm->start, limit, \"bootstrap\", false,\n\t\t      nvkm_vmm_boot_ptes, NULL, NULL, NULL);\n\tvmm->bootstrapped = true;\n\treturn 0;\n}\n\nstatic void\nnvkm_vmm_del(struct kref *kref)\n{\n\tstruct nvkm_vmm *vmm = container_of(kref, typeof(*vmm), kref);\n\tnvkm_vmm_dtor(vmm);\n\tkfree(vmm);\n}\n\nvoid\nnvkm_vmm_unref(struct nvkm_vmm **pvmm)\n{\n\tstruct nvkm_vmm *vmm = *pvmm;\n\tif (vmm) {\n\t\tkref_put(&vmm->kref, nvkm_vmm_del);\n\t\t*pvmm = NULL;\n\t}\n}\n\nstruct nvkm_vmm *\nnvkm_vmm_ref(struct nvkm_vmm *vmm)\n{\n\tif (vmm)\n\t\tkref_get(&vmm->kref);\n\treturn vmm;\n}\n\nint\nnvkm_vmm_new(struct nvkm_device *device, u64 addr, u64 size, void *argv,\n\t     u32 argc, struct lock_class_key *key, const char *name,\n\t     struct nvkm_vmm **pvmm)\n{\n\tstruct nvkm_mmu *mmu = device->mmu;\n\tstruct nvkm_vmm *vmm = NULL;\n\tint ret;\n\tret = mmu->func->vmm.ctor(mmu, addr, size, argv, argc, key, name, &vmm);\n\tif (ret)\n\t\tnvkm_vmm_unref(&vmm);\n\t*pvmm = vmm;\n\treturn ret;\n}\n", "#ifndef __NVKM_VMM_H__\n#define __NVKM_VMM_H__\n#include \"priv.h\"\n#include <core/memory.h>\nenum nvkm_memory_target;\n\nstruct nvkm_vmm_pt {\n\t/* Some GPUs have a mapping level with a dual page tables to\n\t * support large and small pages in the same address-range.\n\t *\n\t * We track the state of both page tables in one place, which\n\t * is why there's multiple PT pointers/refcounts here.\n\t */\n\tstruct nvkm_mmu_pt *pt[2];\n\tu32 refs[2];\n\n\t/* Page size handled by this PT.\n\t *\n\t * Tesla backend needs to know this when writinge PDEs,\n\t * otherwise unnecessary.\n\t */\n\tu8 page;\n\n\t/* Entire page table sparse.\n\t *\n\t * Used to propagate sparseness to child page tables.\n\t */\n\tbool sparse:1;\n\n\t/* Tracking for page directories.\n\t *\n\t * The array is indexed by PDE, and will either point to the\n\t * child page table, or indicate the PDE is marked as sparse.\n\t **/\n#define NVKM_VMM_PDE_INVALID(pde) IS_ERR_OR_NULL(pde)\n#define NVKM_VMM_PDE_SPARSED(pde) IS_ERR(pde)\n#define NVKM_VMM_PDE_SPARSE       ERR_PTR(-EBUSY)\n\tstruct nvkm_vmm_pt **pde;\n\n\t/* Tracking for dual page tables.\n\t *\n\t * There's one entry for each LPTE, keeping track of whether\n\t * there are valid SPTEs in the same address-range.\n\t *\n\t * This information is used to manage LPTE state transitions.\n\t */\n#define NVKM_VMM_PTE_SPARSE 0x80\n#define NVKM_VMM_PTE_VALID  0x40\n#define NVKM_VMM_PTE_SPTES  0x3f\n\tu8 pte[];\n};\n\ntypedef void (*nvkm_vmm_pxe_func)(struct nvkm_vmm *,\n\t\t\t\t  struct nvkm_mmu_pt *, u32 ptei, u32 ptes);\ntypedef void (*nvkm_vmm_pde_func)(struct nvkm_vmm *,\n\t\t\t\t  struct nvkm_vmm_pt *, u32 pdei);\ntypedef void (*nvkm_vmm_pte_func)(struct nvkm_vmm *, struct nvkm_mmu_pt *,\n\t\t\t\t  u32 ptei, u32 ptes, struct nvkm_vmm_map *);\n\nstruct nvkm_vmm_desc_func {\n\tnvkm_vmm_pxe_func invalid;\n\tnvkm_vmm_pxe_func unmap;\n\tnvkm_vmm_pxe_func sparse;\n\n\tnvkm_vmm_pde_func pde;\n\n\tnvkm_vmm_pte_func mem;\n\tnvkm_vmm_pte_func dma;\n\tnvkm_vmm_pte_func sgl;\n};\n\nextern const struct nvkm_vmm_desc_func gf100_vmm_pgd;\nvoid gf100_vmm_pgd_pde(struct nvkm_vmm *, struct nvkm_vmm_pt *, u32);\nextern const struct nvkm_vmm_desc_func gf100_vmm_pgt;\nvoid gf100_vmm_pgt_unmap(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32);\nvoid gf100_vmm_pgt_mem(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32,\n\t\t       struct nvkm_vmm_map *);\nvoid gf100_vmm_pgt_dma(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32,\n\t\t       struct nvkm_vmm_map *);\nvoid gf100_vmm_pgt_sgl(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32,\n\t\t       struct nvkm_vmm_map *);\n\nvoid gk104_vmm_lpt_invalid(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32);\n\nstruct nvkm_vmm_desc {\n\tenum {\n\t\tPGD,\n\t\tPGT,\n\t\tSPT,\n\t\tLPT,\n\t} type;\n\tu8 bits;\t/* VMA bits covered by PT. */\n\tu8 size;\t/* Bytes-per-PTE. */\n\tu32 align;\t/* PT address alignment. */\n\tconst struct nvkm_vmm_desc_func *func;\n};\n\nextern const struct nvkm_vmm_desc nv50_vmm_desc_12[];\nextern const struct nvkm_vmm_desc nv50_vmm_desc_16[];\n\nextern const struct nvkm_vmm_desc gk104_vmm_desc_16_12[];\nextern const struct nvkm_vmm_desc gk104_vmm_desc_16_16[];\nextern const struct nvkm_vmm_desc gk104_vmm_desc_17_12[];\nextern const struct nvkm_vmm_desc gk104_vmm_desc_17_17[];\n\nextern const struct nvkm_vmm_desc gm200_vmm_desc_16_12[];\nextern const struct nvkm_vmm_desc gm200_vmm_desc_16_16[];\nextern const struct nvkm_vmm_desc gm200_vmm_desc_17_12[];\nextern const struct nvkm_vmm_desc gm200_vmm_desc_17_17[];\n\nextern const struct nvkm_vmm_desc gp100_vmm_desc_12[];\nextern const struct nvkm_vmm_desc gp100_vmm_desc_16[];\n\nstruct nvkm_vmm_page {\n\tu8 shift;\n\tconst struct nvkm_vmm_desc *desc;\n#define NVKM_VMM_PAGE_SPARSE                                               0x01\n#define NVKM_VMM_PAGE_VRAM                                                 0x02\n#define NVKM_VMM_PAGE_HOST                                                 0x04\n#define NVKM_VMM_PAGE_COMP                                                 0x08\n#define NVKM_VMM_PAGE_Sxxx                                (NVKM_VMM_PAGE_SPARSE)\n#define NVKM_VMM_PAGE_xVxx                                  (NVKM_VMM_PAGE_VRAM)\n#define NVKM_VMM_PAGE_SVxx             (NVKM_VMM_PAGE_Sxxx | NVKM_VMM_PAGE_VRAM)\n#define NVKM_VMM_PAGE_xxHx                                  (NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_SxHx             (NVKM_VMM_PAGE_Sxxx | NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_xVHx             (NVKM_VMM_PAGE_xVxx | NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_SVHx             (NVKM_VMM_PAGE_SVxx | NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_xVxC             (NVKM_VMM_PAGE_xVxx | NVKM_VMM_PAGE_COMP)\n#define NVKM_VMM_PAGE_SVxC             (NVKM_VMM_PAGE_SVxx | NVKM_VMM_PAGE_COMP)\n#define NVKM_VMM_PAGE_xxHC             (NVKM_VMM_PAGE_xxHx | NVKM_VMM_PAGE_COMP)\n#define NVKM_VMM_PAGE_SxHC             (NVKM_VMM_PAGE_SxHx | NVKM_VMM_PAGE_COMP)\n\tu8 type;\n};\n\nstruct nvkm_vmm_func {\n\tint (*join)(struct nvkm_vmm *, struct nvkm_memory *inst);\n\tvoid (*part)(struct nvkm_vmm *, struct nvkm_memory *inst);\n\n\tint (*aper)(enum nvkm_memory_target);\n\tint (*valid)(struct nvkm_vmm *, void *argv, u32 argc,\n\t\t     struct nvkm_vmm_map *);\n\tvoid (*flush)(struct nvkm_vmm *, int depth);\n\n\tu64 page_block;\n\tconst struct nvkm_vmm_page page[];\n};\n\nstruct nvkm_vmm_join {\n\tstruct nvkm_memory *inst;\n\tstruct list_head head;\n};\n\nint nvkm_vmm_new_(const struct nvkm_vmm_func *, struct nvkm_mmu *,\n\t\t  u32 pd_header, u64 addr, u64 size, struct lock_class_key *,\n\t\t  const char *name, struct nvkm_vmm **);\nint nvkm_vmm_ctor(const struct nvkm_vmm_func *, struct nvkm_mmu *,\n\t\t  u32 pd_header, u64 addr, u64 size, struct lock_class_key *,\n\t\t  const char *name, struct nvkm_vmm *);\nstruct nvkm_vma *nvkm_vmm_node_search(struct nvkm_vmm *, u64 addr);\nstruct nvkm_vma *nvkm_vmm_node_split(struct nvkm_vmm *, struct nvkm_vma *,\n\t\t\t\t     u64 addr, u64 size);\nint nvkm_vmm_get_locked(struct nvkm_vmm *, bool getref, bool mapref,\n\t\t\tbool sparse, u8 page, u8 align, u64 size,\n\t\t\tstruct nvkm_vma **pvma);\nvoid nvkm_vmm_put_locked(struct nvkm_vmm *, struct nvkm_vma *);\nvoid nvkm_vmm_unmap_locked(struct nvkm_vmm *, struct nvkm_vma *);\nvoid nvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma);\n\nstruct nvkm_vma *nvkm_vma_tail(struct nvkm_vma *, u64 tail);\n\nint nv04_vmm_new_(const struct nvkm_vmm_func *, struct nvkm_mmu *, u32,\n\t\t  u64, u64, void *, u32, struct lock_class_key *,\n\t\t  const char *, struct nvkm_vmm **);\nint nv04_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\n\nint nv50_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\nvoid nv50_vmm_part(struct nvkm_vmm *, struct nvkm_memory *);\nint nv50_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\nvoid nv50_vmm_flush(struct nvkm_vmm *, int);\n\nint gf100_vmm_new_(const struct nvkm_vmm_func *, const struct nvkm_vmm_func *,\n\t\t   struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t   struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gf100_vmm_join_(struct nvkm_vmm *, struct nvkm_memory *, u64 base);\nint gf100_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\nvoid gf100_vmm_part(struct nvkm_vmm *, struct nvkm_memory *);\nint gf100_vmm_aper(enum nvkm_memory_target);\nint gf100_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\nvoid gf100_vmm_flush_(struct nvkm_vmm *, int);\nvoid gf100_vmm_flush(struct nvkm_vmm *, int);\n\nint gk20a_vmm_aper(enum nvkm_memory_target);\n\nint gm200_vmm_new_(const struct nvkm_vmm_func *, const struct nvkm_vmm_func *,\n\t\t   struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t   struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gm200_vmm_join_(struct nvkm_vmm *, struct nvkm_memory *, u64 base);\nint gm200_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\n\nint gp100_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\nint gp100_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\nvoid gp100_vmm_flush(struct nvkm_vmm *, int);\n\nint nv04_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint nv41_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint nv44_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint nv50_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint mcp77_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint g84_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\tstruct lock_class_key *, const char *, struct nvkm_vmm **);\nint gf100_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gk104_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gk20a_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gm200_vmm_new_fixed(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t\tstruct lock_class_key *, const char *,\n\t\t\tstruct nvkm_vmm **);\nint gm200_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gm20b_vmm_new_fixed(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t\tstruct lock_class_key *, const char *,\n\t\t\tstruct nvkm_vmm **);\nint gm20b_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gp100_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gp10b_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gv100_vmm_new(struct nvkm_mmu *, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\n\n#define VMM_PRINT(l,v,p,f,a...) do {                                           \\\n\tstruct nvkm_vmm *_vmm = (v);                                           \\\n\tif (CONFIG_NOUVEAU_DEBUG >= (l) && _vmm->debug >= (l)) {               \\\n\t\tnvkm_printk_(&_vmm->mmu->subdev, 0, p, \"%s: \"f\"\\n\",            \\\n\t\t\t     _vmm->name, ##a);                                 \\\n\t}                                                                      \\\n} while(0)\n#define VMM_DEBUG(v,f,a...) VMM_PRINT(NV_DBG_DEBUG, (v), info, f, ##a)\n#define VMM_TRACE(v,f,a...) VMM_PRINT(NV_DBG_TRACE, (v), info, f, ##a)\n#define VMM_SPAM(v,f,a...)  VMM_PRINT(NV_DBG_SPAM , (v),  dbg, f, ##a)\n\n#define VMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,BASE,SIZE,NEXT) do {            \\\n\tnvkm_kmap((PT)->memory);                                               \\\n\twhile (PTEN) {                                                         \\\n\t\tu64 _ptes = ((SIZE) - MAP->off) >> MAP->page->shift;           \\\n\t\tu64 _addr = ((BASE) + MAP->off);                               \\\n                                                                               \\\n\t\tif (_ptes > PTEN) {                                            \\\n\t\t\tMAP->off += PTEN << MAP->page->shift;                  \\\n\t\t\t_ptes = PTEN;                                          \\\n\t\t} else {                                                       \\\n\t\t\tMAP->off = 0;                                          \\\n\t\t\tNEXT;                                                  \\\n\t\t}                                                              \\\n                                                                               \\\n\t\tVMM_SPAM(VMM, \"ITER %08x %08x PTE(s)\", PTEI, (u32)_ptes);      \\\n                                                                               \\\n\t\tFILL(VMM, PT, PTEI, _ptes, MAP, _addr);                        \\\n\t\tPTEI += _ptes;                                                 \\\n\t\tPTEN -= _ptes;                                                 \\\n\t};                                                                     \\\n\tnvkm_done((PT)->memory);                                               \\\n} while(0)\n\n#define VMM_MAP_ITER_MEM(VMM,PT,PTEI,PTEN,MAP,FILL)                            \\\n\tVMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,                                \\\n\t\t     ((u64)MAP->mem->offset << NVKM_RAM_MM_SHIFT),             \\\n\t\t     ((u64)MAP->mem->length << NVKM_RAM_MM_SHIFT),             \\\n\t\t     (MAP->mem = MAP->mem->next))\n#define VMM_MAP_ITER_DMA(VMM,PT,PTEI,PTEN,MAP,FILL)                            \\\n\tVMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,                                \\\n\t\t     *MAP->dma, PAGE_SIZE, MAP->dma++)\n#define VMM_MAP_ITER_SGL(VMM,PT,PTEI,PTEN,MAP,FILL)                            \\\n\tVMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,                                \\\n\t\t     sg_dma_address(MAP->sgl), sg_dma_len(MAP->sgl),           \\\n\t\t     (MAP->sgl = sg_next(MAP->sgl)))\n\n#define VMM_FO(m,o,d,c,b) nvkm_fo##b((m)->memory, (o), (d), (c))\n#define VMM_WO(m,o,d,c,b) nvkm_wo##b((m)->memory, (o), (d))\n#define VMM_XO(m,v,o,d,c,b,fn,f,a...) do {                                     \\\n\tconst u32 _pteo = (o); u##b _data = (d);                               \\\n\tVMM_SPAM((v), \"   %010llx \"f, (m)->addr + _pteo, _data, ##a);          \\\n\tVMM_##fn((m), (m)->base + _pteo, _data, (c), b);                       \\\n} while(0)\n\n#define VMM_WO032(m,v,o,d) VMM_XO((m),(v),(o),(d),  1, 32, WO, \"%08x\")\n#define VMM_FO032(m,v,o,d,c)                                                   \\\n\tVMM_XO((m),(v),(o),(d),(c), 32, FO, \"%08x %08x\", (c))\n\n#define VMM_WO064(m,v,o,d) VMM_XO((m),(v),(o),(d),  1, 64, WO, \"%016llx\")\n#define VMM_FO064(m,v,o,d,c)                                                   \\\n\tVMM_XO((m),(v),(o),(d),(c), 64, FO, \"%016llx %08x\", (c))\n\n#define VMM_XO128(m,v,o,lo,hi,c,f,a...) do {                                   \\\n\tu32 _pteo = (o), _ptes = (c);                                          \\\n\tconst u64 _addr = (m)->addr + _pteo;                                   \\\n\tVMM_SPAM((v), \"   %010llx %016llx%016llx\"f, _addr, (hi), (lo), ##a);   \\\n\twhile (_ptes--) {                                                      \\\n\t\tnvkm_wo64((m)->memory, (m)->base + _pteo + 0, (lo));           \\\n\t\tnvkm_wo64((m)->memory, (m)->base + _pteo + 8, (hi));           \\\n\t\t_pteo += 0x10;                                                 \\\n\t}                                                                      \\\n} while(0)\n\n#define VMM_WO128(m,v,o,lo,hi) VMM_XO128((m),(v),(o),(lo),(hi), 1, \"\")\n#define VMM_FO128(m,v,o,lo,hi,c) do {                                          \\\n\tnvkm_kmap((m)->memory);                                                \\\n\tVMM_XO128((m),(v),(o),(lo),(hi),(c), \" %08x\", (c));                    \\\n\tnvkm_done((m)->memory);                                                \\\n} while(0)\n#endif\n"], "filenames": ["drivers/gpu/drm/nouveau/nvkm/subdev/mmu/uvmm.c", "drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c", "drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.h"], "buggy_code_start_loc": [137, 769, 159], "buggy_code_end_loc": [154, 1361, 169], "fixing_code_start_loc": [137, 770, 160], "fixing_code_end_loc": [141, 1440, 169], "type": "CWE-416", "message": "A use-after-free flaw was found in the Linux kernel\u2019s nouveau driver in how a user triggers a memory overflow that causes the nvkm_vma_tail function to fail. This flaw allows a local user to crash or potentially escalate their privileges on the system.", "other": {"cve": {"id": "CVE-2023-0030", "sourceIdentifier": "secalert@redhat.com", "published": "2023-03-08T23:15:10.963", "lastModified": "2023-04-13T17:15:09.433", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "A use-after-free flaw was found in the Linux kernel\u2019s nouveau driver in how a user triggers a memory overflow that causes the nvkm_vma_tail function to fail. This flaw allows a local user to crash or potentially escalate their privileges on the system."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.0", "matchCriteriaId": "D1C581ED-7982-4D97-89D0-9DD8E7B66D0D"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2157270", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/729eba3355674f2d9524629b73683ba1d1cd3f10", "source": "secalert@redhat.com", "tags": ["Broken Link", "Patch"]}, {"url": "https://security.netapp.com/advisory/ntap-20230413-0010/", "source": "secalert@redhat.com"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/729eba3355674f2d9524629b73683ba1d1cd3f10"}}