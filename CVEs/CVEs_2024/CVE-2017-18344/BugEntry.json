{"buggy_code": ["/*\n * linux/kernel/posix-timers.c\n *\n *\n * 2002-10-15  Posix Clocks & timers\n *                           by George Anzinger george@mvista.com\n *\n *\t\t\t     Copyright (C) 2002 2003 by MontaVista Software.\n *\n * 2004-06-01  Fix CLOCK_REALTIME clock/timer TIMER_ABSTIME bug.\n *\t\t\t     Copyright (C) 2004 Boris Hu\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or (at\n * your option) any later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n * General Public License for more details.\n\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n *\n * MontaVista Software | 1237 East Arques Avenue | Sunnyvale | CA 94085 | USA\n */\n\n/* These are all the functions necessary to implement\n * POSIX clocks & timers\n */\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/time.h>\n#include <linux/mutex.h>\n#include <linux/sched/task.h>\n\n#include <linux/uaccess.h>\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/compiler.h>\n#include <linux/hash.h>\n#include <linux/posix-clock.h>\n#include <linux/posix-timers.h>\n#include <linux/syscalls.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n#include <linux/export.h>\n#include <linux/hashtable.h>\n#include <linux/compat.h>\n\n#include \"timekeeping.h\"\n#include \"posix-timers.h\"\n\n/*\n * Management arrays for POSIX timers. Timers are now kept in static hash table\n * with 512 entries.\n * Timer ids are allocated by local routine, which selects proper hash head by\n * key, constructed from current->signal address and per signal struct counter.\n * This keeps timer ids unique per process, but now they can intersect between\n * processes.\n */\n\n/*\n * Lets keep our timers in a slab cache :-)\n */\nstatic struct kmem_cache *posix_timers_cache;\n\nstatic DEFINE_HASHTABLE(posix_timers_hashtable, 9);\nstatic DEFINE_SPINLOCK(hash_lock);\n\nstatic const struct k_clock * const posix_clocks[];\nstatic const struct k_clock *clockid_to_kclock(const clockid_t id);\nstatic const struct k_clock clock_realtime, clock_monotonic;\n\n/*\n * we assume that the new SIGEV_THREAD_ID shares no bits with the other\n * SIGEV values.  Here we put out an error if this assumption fails.\n */\n#if SIGEV_THREAD_ID != (SIGEV_THREAD_ID & \\\n                       ~(SIGEV_SIGNAL | SIGEV_NONE | SIGEV_THREAD))\n#error \"SIGEV_THREAD_ID must not share bit with other SIGEV values!\"\n#endif\n\n/*\n * parisc wants ENOTSUP instead of EOPNOTSUPP\n */\n#ifndef ENOTSUP\n# define ENANOSLEEP_NOTSUP EOPNOTSUPP\n#else\n# define ENANOSLEEP_NOTSUP ENOTSUP\n#endif\n\n/*\n * The timer ID is turned into a timer address by idr_find().\n * Verifying a valid ID consists of:\n *\n * a) checking that idr_find() returns other than -1.\n * b) checking that the timer id matches the one in the timer itself.\n * c) that the timer owner is in the callers thread group.\n */\n\n/*\n * CLOCKs: The POSIX standard calls for a couple of clocks and allows us\n *\t    to implement others.  This structure defines the various\n *\t    clocks.\n *\n * RESOLUTION: Clock resolution is used to round up timer and interval\n *\t    times, NOT to report clock times, which are reported with as\n *\t    much resolution as the system can muster.  In some cases this\n *\t    resolution may depend on the underlying clock hardware and\n *\t    may not be quantifiable until run time, and only then is the\n *\t    necessary code is written.\tThe standard says we should say\n *\t    something about this issue in the documentation...\n *\n * FUNCTIONS: The CLOCKs structure defines possible functions to\n *\t    handle various clock functions.\n *\n *\t    The standard POSIX timer management code assumes the\n *\t    following: 1.) The k_itimer struct (sched.h) is used for\n *\t    the timer.  2.) The list, it_lock, it_clock, it_id and\n *\t    it_pid fields are not modified by timer code.\n *\n * Permissions: It is assumed that the clock_settime() function defined\n *\t    for each clock will take care of permission checks.\t Some\n *\t    clocks may be set able by any user (i.e. local process\n *\t    clocks) others not.\t Currently the only set able clock we\n *\t    have is CLOCK_REALTIME and its high res counter part, both of\n *\t    which we beg off on and pass to do_sys_settimeofday().\n */\nstatic struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags);\n\n#define lock_timer(tid, flags)\t\t\t\t\t\t   \\\n({\tstruct k_itimer *__timr;\t\t\t\t\t   \\\n\t__cond_lock(&__timr->it_lock, __timr = __lock_timer(tid, flags));  \\\n\t__timr;\t\t\t\t\t\t\t\t   \\\n})\n\nstatic int hash(struct signal_struct *sig, unsigned int nr)\n{\n\treturn hash_32(hash32_ptr(sig) ^ nr, HASH_BITS(posix_timers_hashtable));\n}\n\nstatic struct k_itimer *__posix_timers_find(struct hlist_head *head,\n\t\t\t\t\t    struct signal_struct *sig,\n\t\t\t\t\t    timer_t id)\n{\n\tstruct k_itimer *timer;\n\n\thlist_for_each_entry_rcu(timer, head, t_hash) {\n\t\tif ((timer->it_signal == sig) && (timer->it_id == id))\n\t\t\treturn timer;\n\t}\n\treturn NULL;\n}\n\nstatic struct k_itimer *posix_timer_by_id(timer_t id)\n{\n\tstruct signal_struct *sig = current->signal;\n\tstruct hlist_head *head = &posix_timers_hashtable[hash(sig, id)];\n\n\treturn __posix_timers_find(head, sig, id);\n}\n\nstatic int posix_timer_add(struct k_itimer *timer)\n{\n\tstruct signal_struct *sig = current->signal;\n\tint first_free_id = sig->posix_timer_id;\n\tstruct hlist_head *head;\n\tint ret = -ENOENT;\n\n\tdo {\n\t\tspin_lock(&hash_lock);\n\t\thead = &posix_timers_hashtable[hash(sig, sig->posix_timer_id)];\n\t\tif (!__posix_timers_find(head, sig, sig->posix_timer_id)) {\n\t\t\thlist_add_head_rcu(&timer->t_hash, head);\n\t\t\tret = sig->posix_timer_id;\n\t\t}\n\t\tif (++sig->posix_timer_id < 0)\n\t\t\tsig->posix_timer_id = 0;\n\t\tif ((sig->posix_timer_id == first_free_id) && (ret == -ENOENT))\n\t\t\t/* Loop over all possible ids completed */\n\t\t\tret = -EAGAIN;\n\t\tspin_unlock(&hash_lock);\n\t} while (ret == -ENOENT);\n\treturn ret;\n}\n\nstatic inline void unlock_timer(struct k_itimer *timr, unsigned long flags)\n{\n\tspin_unlock_irqrestore(&timr->it_lock, flags);\n}\n\n/* Get clock_realtime */\nstatic int posix_clock_realtime_get(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_real_ts64(tp);\n\treturn 0;\n}\n\n/* Set clock_realtime */\nstatic int posix_clock_realtime_set(const clockid_t which_clock,\n\t\t\t\t    const struct timespec64 *tp)\n{\n\treturn do_sys_settimeofday64(tp, NULL);\n}\n\nstatic int posix_clock_realtime_adj(const clockid_t which_clock,\n\t\t\t\t    struct timex *t)\n{\n\treturn do_adjtimex(t);\n}\n\n/*\n * Get monotonic time for posix timers\n */\nstatic int posix_ktime_get_ts(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_ts64(tp);\n\treturn 0;\n}\n\n/*\n * Get monotonic-raw time for posix timers\n */\nstatic int posix_get_monotonic_raw(clockid_t which_clock, struct timespec64 *tp)\n{\n\tgetrawmonotonic64(tp);\n\treturn 0;\n}\n\n\nstatic int posix_get_realtime_coarse(clockid_t which_clock, struct timespec64 *tp)\n{\n\t*tp = current_kernel_time64();\n\treturn 0;\n}\n\nstatic int posix_get_monotonic_coarse(clockid_t which_clock,\n\t\t\t\t\t\tstruct timespec64 *tp)\n{\n\t*tp = get_monotonic_coarse64();\n\treturn 0;\n}\n\nstatic int posix_get_coarse_res(const clockid_t which_clock, struct timespec64 *tp)\n{\n\t*tp = ktime_to_timespec64(KTIME_LOW_RES);\n\treturn 0;\n}\n\nstatic int posix_get_boottime(const clockid_t which_clock, struct timespec64 *tp)\n{\n\tget_monotonic_boottime64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_tai(clockid_t which_clock, struct timespec64 *tp)\n{\n\ttimekeeping_clocktai64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_hrtimer_res(clockid_t which_clock, struct timespec64 *tp)\n{\n\ttp->tv_sec = 0;\n\ttp->tv_nsec = hrtimer_resolution;\n\treturn 0;\n}\n\n/*\n * Initialize everything, well, just everything in Posix clocks/timers ;)\n */\nstatic __init int init_posix_timers(void)\n{\n\tposix_timers_cache = kmem_cache_create(\"posix_timers_cache\",\n\t\t\t\t\tsizeof (struct k_itimer), 0, SLAB_PANIC,\n\t\t\t\t\tNULL);\n\treturn 0;\n}\n__initcall(init_posix_timers);\n\nstatic void common_hrtimer_rearm(struct k_itimer *timr)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\tif (!timr->it_interval)\n\t\treturn;\n\n\ttimr->it_overrun += (unsigned int) hrtimer_forward(timer,\n\t\t\t\t\t\ttimer->base->get_time(),\n\t\t\t\t\t\ttimr->it_interval);\n\thrtimer_restart(timer);\n}\n\n/*\n * This function is exported for use by the signal deliver code.  It is\n * called just prior to the info block being released and passes that\n * block to us.  It's function is to update the overrun entry AND to\n * restart the timer.  It should only be called if the timer is to be\n * restarted (i.e. we have flagged this in the sys_private entry of the\n * info block).\n *\n * To protect against the timer going away while the interrupt is queued,\n * we require that the it_requeue_pending flag be set.\n */\nvoid posixtimer_rearm(struct siginfo *info)\n{\n\tstruct k_itimer *timr;\n\tunsigned long flags;\n\n\ttimr = lock_timer(info->si_tid, &flags);\n\tif (!timr)\n\t\treturn;\n\n\tif (timr->it_requeue_pending == info->si_sys_private) {\n\t\ttimr->kclock->timer_rearm(timr);\n\n\t\ttimr->it_active = 1;\n\t\ttimr->it_overrun_last = timr->it_overrun;\n\t\ttimr->it_overrun = -1;\n\t\t++timr->it_requeue_pending;\n\n\t\tinfo->si_overrun += timr->it_overrun_last;\n\t}\n\n\tunlock_timer(timr, flags);\n}\n\nint posix_timer_event(struct k_itimer *timr, int si_private)\n{\n\tstruct task_struct *task;\n\tint shared, ret = -1;\n\t/*\n\t * FIXME: if ->sigq is queued we can race with\n\t * dequeue_signal()->posixtimer_rearm().\n\t *\n\t * If dequeue_signal() sees the \"right\" value of\n\t * si_sys_private it calls posixtimer_rearm().\n\t * We re-queue ->sigq and drop ->it_lock().\n\t * posixtimer_rearm() locks the timer\n\t * and re-schedules it while ->sigq is pending.\n\t * Not really bad, but not that we want.\n\t */\n\ttimr->sigq->info.si_sys_private = si_private;\n\n\trcu_read_lock();\n\ttask = pid_task(timr->it_pid, PIDTYPE_PID);\n\tif (task) {\n\t\tshared = !(timr->it_sigev_notify & SIGEV_THREAD_ID);\n\t\tret = send_sigqueue(timr->sigq, task, shared);\n\t}\n\trcu_read_unlock();\n\t/* If we failed to send the signal the timer stops. */\n\treturn ret > 0;\n}\n\n/*\n * This function gets called when a POSIX.1b interval timer expires.  It\n * is used as a callback from the kernel internal timer.  The\n * run_timer_list code ALWAYS calls with interrupts on.\n\n * This code is for CLOCK_REALTIME* and CLOCK_MONOTONIC* timers.\n */\nstatic enum hrtimer_restart posix_timer_fn(struct hrtimer *timer)\n{\n\tstruct k_itimer *timr;\n\tunsigned long flags;\n\tint si_private = 0;\n\tenum hrtimer_restart ret = HRTIMER_NORESTART;\n\n\ttimr = container_of(timer, struct k_itimer, it.real.timer);\n\tspin_lock_irqsave(&timr->it_lock, flags);\n\n\ttimr->it_active = 0;\n\tif (timr->it_interval != 0)\n\t\tsi_private = ++timr->it_requeue_pending;\n\n\tif (posix_timer_event(timr, si_private)) {\n\t\t/*\n\t\t * signal was not sent because of sig_ignor\n\t\t * we will not get a call back to restart it AND\n\t\t * it should be restarted.\n\t\t */\n\t\tif (timr->it_interval != 0) {\n\t\t\tktime_t now = hrtimer_cb_get_time(timer);\n\n\t\t\t/*\n\t\t\t * FIXME: What we really want, is to stop this\n\t\t\t * timer completely and restart it in case the\n\t\t\t * SIG_IGN is removed. This is a non trivial\n\t\t\t * change which involves sighand locking\n\t\t\t * (sigh !), which we don't want to do late in\n\t\t\t * the release cycle.\n\t\t\t *\n\t\t\t * For now we just let timers with an interval\n\t\t\t * less than a jiffie expire every jiffie to\n\t\t\t * avoid softirq starvation in case of SIG_IGN\n\t\t\t * and a very small interval, which would put\n\t\t\t * the timer right back on the softirq pending\n\t\t\t * list. By moving now ahead of time we trick\n\t\t\t * hrtimer_forward() to expire the timer\n\t\t\t * later, while we still maintain the overrun\n\t\t\t * accuracy, but have some inconsistency in\n\t\t\t * the timer_gettime() case. This is at least\n\t\t\t * better than a starved softirq. A more\n\t\t\t * complex fix which solves also another related\n\t\t\t * inconsistency is already in the pipeline.\n\t\t\t */\n#ifdef CONFIG_HIGH_RES_TIMERS\n\t\t\t{\n\t\t\t\tktime_t kj = NSEC_PER_SEC / HZ;\n\n\t\t\t\tif (timr->it_interval < kj)\n\t\t\t\t\tnow = ktime_add(now, kj);\n\t\t\t}\n#endif\n\t\t\ttimr->it_overrun += (unsigned int)\n\t\t\t\thrtimer_forward(timer, now,\n\t\t\t\t\t\ttimr->it_interval);\n\t\t\tret = HRTIMER_RESTART;\n\t\t\t++timr->it_requeue_pending;\n\t\t\ttimr->it_active = 1;\n\t\t}\n\t}\n\n\tunlock_timer(timr, flags);\n\treturn ret;\n}\n\nstatic struct pid *good_sigevent(sigevent_t * event)\n{\n\tstruct task_struct *rtn = current->group_leader;\n\n\tif ((event->sigev_notify & SIGEV_THREAD_ID ) &&\n\t\t(!(rtn = find_task_by_vpid(event->sigev_notify_thread_id)) ||\n\t\t !same_thread_group(rtn, current) ||\n\t\t (event->sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_SIGNAL))\n\t\treturn NULL;\n\n\tif (((event->sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_NONE) &&\n\t    ((event->sigev_signo <= 0) || (event->sigev_signo > SIGRTMAX)))\n\t\treturn NULL;\n\n\treturn task_pid(rtn);\n}\n\nstatic struct k_itimer * alloc_posix_timer(void)\n{\n\tstruct k_itimer *tmr;\n\ttmr = kmem_cache_zalloc(posix_timers_cache, GFP_KERNEL);\n\tif (!tmr)\n\t\treturn tmr;\n\tif (unlikely(!(tmr->sigq = sigqueue_alloc()))) {\n\t\tkmem_cache_free(posix_timers_cache, tmr);\n\t\treturn NULL;\n\t}\n\tmemset(&tmr->sigq->info, 0, sizeof(siginfo_t));\n\treturn tmr;\n}\n\nstatic void k_itimer_rcu_free(struct rcu_head *head)\n{\n\tstruct k_itimer *tmr = container_of(head, struct k_itimer, it.rcu);\n\n\tkmem_cache_free(posix_timers_cache, tmr);\n}\n\n#define IT_ID_SET\t1\n#define IT_ID_NOT_SET\t0\nstatic void release_posix_timer(struct k_itimer *tmr, int it_id_set)\n{\n\tif (it_id_set) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&hash_lock, flags);\n\t\thlist_del_rcu(&tmr->t_hash);\n\t\tspin_unlock_irqrestore(&hash_lock, flags);\n\t}\n\tput_pid(tmr->it_pid);\n\tsigqueue_free(tmr->sigq);\n\tcall_rcu(&tmr->it.rcu, k_itimer_rcu_free);\n}\n\nstatic int common_timer_create(struct k_itimer *new_timer)\n{\n\thrtimer_init(&new_timer->it.real.timer, new_timer->it_clock, 0);\n\treturn 0;\n}\n\n/* Create a POSIX.1b interval timer. */\nstatic int do_timer_create(clockid_t which_clock, struct sigevent *event,\n\t\t\t   timer_t __user *created_timer_id)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct k_itimer *new_timer;\n\tint error, new_timer_id;\n\tint it_id_set = IT_ID_NOT_SET;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->timer_create)\n\t\treturn -EOPNOTSUPP;\n\n\tnew_timer = alloc_posix_timer();\n\tif (unlikely(!new_timer))\n\t\treturn -EAGAIN;\n\n\tspin_lock_init(&new_timer->it_lock);\n\tnew_timer_id = posix_timer_add(new_timer);\n\tif (new_timer_id < 0) {\n\t\terror = new_timer_id;\n\t\tgoto out;\n\t}\n\n\tit_id_set = IT_ID_SET;\n\tnew_timer->it_id = (timer_t) new_timer_id;\n\tnew_timer->it_clock = which_clock;\n\tnew_timer->kclock = kc;\n\tnew_timer->it_overrun = -1;\n\n\tif (event) {\n\t\trcu_read_lock();\n\t\tnew_timer->it_pid = get_pid(good_sigevent(event));\n\t\trcu_read_unlock();\n\t\tif (!new_timer->it_pid) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnew_timer->it_sigev_notify     = event->sigev_notify;\n\t\tnew_timer->sigq->info.si_signo = event->sigev_signo;\n\t\tnew_timer->sigq->info.si_value = event->sigev_value;\n\t} else {\n\t\tnew_timer->it_sigev_notify     = SIGEV_SIGNAL;\n\t\tnew_timer->sigq->info.si_signo = SIGALRM;\n\t\tmemset(&new_timer->sigq->info.si_value, 0, sizeof(sigval_t));\n\t\tnew_timer->sigq->info.si_value.sival_int = new_timer->it_id;\n\t\tnew_timer->it_pid = get_pid(task_tgid(current));\n\t}\n\n\tnew_timer->sigq->info.si_tid   = new_timer->it_id;\n\tnew_timer->sigq->info.si_code  = SI_TIMER;\n\n\tif (copy_to_user(created_timer_id,\n\t\t\t &new_timer_id, sizeof (new_timer_id))) {\n\t\terror = -EFAULT;\n\t\tgoto out;\n\t}\n\n\terror = kc->timer_create(new_timer);\n\tif (error)\n\t\tgoto out;\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tnew_timer->it_signal = current->signal;\n\tlist_add(&new_timer->list, &current->signal->posix_timers);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\treturn 0;\n\t/*\n\t * In the case of the timer belonging to another task, after\n\t * the task is unlocked, the timer is owned by the other task\n\t * and may cease to exist at any time.  Don't use or modify\n\t * new_timer after the unlock call.\n\t */\nout:\n\trelease_posix_timer(new_timer, it_id_set);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(timer_create, const clockid_t, which_clock,\n\t\tstruct sigevent __user *, timer_event_spec,\n\t\ttimer_t __user *, created_timer_id)\n{\n\tif (timer_event_spec) {\n\t\tsigevent_t event;\n\n\t\tif (copy_from_user(&event, timer_event_spec, sizeof (event)))\n\t\t\treturn -EFAULT;\n\t\treturn do_timer_create(which_clock, &event, created_timer_id);\n\t}\n\treturn do_timer_create(which_clock, NULL, created_timer_id);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(timer_create, clockid_t, which_clock,\n\t\t       struct compat_sigevent __user *, timer_event_spec,\n\t\t       timer_t __user *, created_timer_id)\n{\n\tif (timer_event_spec) {\n\t\tsigevent_t event;\n\n\t\tif (get_compat_sigevent(&event, timer_event_spec))\n\t\t\treturn -EFAULT;\n\t\treturn do_timer_create(which_clock, &event, created_timer_id);\n\t}\n\treturn do_timer_create(which_clock, NULL, created_timer_id);\n}\n#endif\n\n/*\n * Locking issues: We need to protect the result of the id look up until\n * we get the timer locked down so it is not deleted under us.  The\n * removal is done under the idr spinlock so we use that here to bridge\n * the find to the timer lock.  To avoid a dead lock, the timer id MUST\n * be release with out holding the timer lock.\n */\nstatic struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags)\n{\n\tstruct k_itimer *timr;\n\n\t/*\n\t * timer_t could be any type >= int and we want to make sure any\n\t * @timer_id outside positive int range fails lookup.\n\t */\n\tif ((unsigned long long)timer_id > INT_MAX)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttimr = posix_timer_by_id(timer_id);\n\tif (timr) {\n\t\tspin_lock_irqsave(&timr->it_lock, *flags);\n\t\tif (timr->it_signal == current->signal) {\n\t\t\trcu_read_unlock();\n\t\t\treturn timr;\n\t\t}\n\t\tspin_unlock_irqrestore(&timr->it_lock, *flags);\n\t}\n\trcu_read_unlock();\n\n\treturn NULL;\n}\n\nstatic ktime_t common_hrtimer_remaining(struct k_itimer *timr, ktime_t now)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\treturn __hrtimer_expires_remaining_adjusted(timer, now);\n}\n\nstatic int common_hrtimer_forward(struct k_itimer *timr, ktime_t now)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\treturn (int)hrtimer_forward(timer, now, timr->it_interval);\n}\n\n/*\n * Get the time remaining on a POSIX.1b interval timer.  This function\n * is ALWAYS called with spin_lock_irq on the timer, thus it must not\n * mess with irq.\n *\n * We have a couple of messes to clean up here.  First there is the case\n * of a timer that has a requeue pending.  These timers should appear to\n * be in the timer list with an expiry as if we were to requeue them\n * now.\n *\n * The second issue is the SIGEV_NONE timer which may be active but is\n * not really ever put in the timer list (to save system resources).\n * This timer may be expired, and if so, we will do it here.  Otherwise\n * it is the same as a requeue pending timer WRT to what we should\n * report.\n */\nvoid common_timer_get(struct k_itimer *timr, struct itimerspec64 *cur_setting)\n{\n\tconst struct k_clock *kc = timr->kclock;\n\tktime_t now, remaining, iv;\n\tstruct timespec64 ts64;\n\tbool sig_none;\n\n\tsig_none = (timr->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE;\n\tiv = timr->it_interval;\n\n\t/* interval timer ? */\n\tif (iv) {\n\t\tcur_setting->it_interval = ktime_to_timespec64(iv);\n\t} else if (!timr->it_active) {\n\t\t/*\n\t\t * SIGEV_NONE oneshot timers are never queued. Check them\n\t\t * below.\n\t\t */\n\t\tif (!sig_none)\n\t\t\treturn;\n\t}\n\n\t/*\n\t * The timespec64 based conversion is suboptimal, but it's not\n\t * worth to implement yet another callback.\n\t */\n\tkc->clock_get(timr->it_clock, &ts64);\n\tnow = timespec64_to_ktime(ts64);\n\n\t/*\n\t * When a requeue is pending or this is a SIGEV_NONE timer move the\n\t * expiry time forward by intervals, so expiry is > now.\n\t */\n\tif (iv && (timr->it_requeue_pending & REQUEUE_PENDING || sig_none))\n\t\ttimr->it_overrun += kc->timer_forward(timr, now);\n\n\tremaining = kc->timer_remaining(timr, now);\n\t/* Return 0 only, when the timer is expired and not pending */\n\tif (remaining <= 0) {\n\t\t/*\n\t\t * A single shot SIGEV_NONE timer must return 0, when\n\t\t * it is expired !\n\t\t */\n\t\tif (!sig_none)\n\t\t\tcur_setting->it_value.tv_nsec = 1;\n\t} else {\n\t\tcur_setting->it_value = ktime_to_timespec64(remaining);\n\t}\n}\n\n/* Get the time remaining on a POSIX.1b interval timer. */\nstatic int do_timer_gettime(timer_t timer_id,  struct itimerspec64 *setting)\n{\n\tstruct k_itimer *timr;\n\tconst struct k_clock *kc;\n\tunsigned long flags;\n\tint ret = 0;\n\n\ttimr = lock_timer(timer_id, &flags);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\tmemset(setting, 0, sizeof(*setting));\n\tkc = timr->kclock;\n\tif (WARN_ON_ONCE(!kc || !kc->timer_get))\n\t\tret = -EINVAL;\n\telse\n\t\tkc->timer_get(timr, setting);\n\n\tunlock_timer(timr, flags);\n\treturn ret;\n}\n\n/* Get the time remaining on a POSIX.1b interval timer. */\nSYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,\n\t\tstruct itimerspec __user *, setting)\n{\n\tstruct itimerspec64 cur_setting;\n\n\tint ret = do_timer_gettime(timer_id, &cur_setting);\n\tif (!ret) {\n\t\tif (put_itimerspec64(&cur_setting, setting))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,\n\t\t       struct compat_itimerspec __user *, setting)\n{\n\tstruct itimerspec64 cur_setting;\n\n\tint ret = do_timer_gettime(timer_id, &cur_setting);\n\tif (!ret) {\n\t\tif (put_compat_itimerspec64(&cur_setting, setting))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n#endif\n\n/*\n * Get the number of overruns of a POSIX.1b interval timer.  This is to\n * be the overrun of the timer last delivered.  At the same time we are\n * accumulating overruns on the next timer.  The overrun is frozen when\n * the signal is delivered, either at the notify time (if the info block\n * is not queued) or at the actual delivery time (as we are informed by\n * the call back to posixtimer_rearm().  So all we need to do is\n * to pick up the frozen overrun.\n */\nSYSCALL_DEFINE1(timer_getoverrun, timer_t, timer_id)\n{\n\tstruct k_itimer *timr;\n\tint overrun;\n\tunsigned long flags;\n\n\ttimr = lock_timer(timer_id, &flags);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\toverrun = timr->it_overrun_last;\n\tunlock_timer(timr, flags);\n\n\treturn overrun;\n}\n\nstatic void common_hrtimer_arm(struct k_itimer *timr, ktime_t expires,\n\t\t\t       bool absolute, bool sigev_none)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\tenum hrtimer_mode mode;\n\n\tmode = absolute ? HRTIMER_MODE_ABS : HRTIMER_MODE_REL;\n\t/*\n\t * Posix magic: Relative CLOCK_REALTIME timers are not affected by\n\t * clock modifications, so they become CLOCK_MONOTONIC based under the\n\t * hood. See hrtimer_init(). Update timr->kclock, so the generic\n\t * functions which use timr->kclock->clock_get() work.\n\t *\n\t * Note: it_clock stays unmodified, because the next timer_set() might\n\t * use ABSTIME, so it needs to switch back.\n\t */\n\tif (timr->it_clock == CLOCK_REALTIME)\n\t\ttimr->kclock = absolute ? &clock_realtime : &clock_monotonic;\n\n\thrtimer_init(&timr->it.real.timer, timr->it_clock, mode);\n\ttimr->it.real.timer.function = posix_timer_fn;\n\n\tif (!absolute)\n\t\texpires = ktime_add_safe(expires, timer->base->get_time());\n\thrtimer_set_expires(timer, expires);\n\n\tif (!sigev_none)\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n}\n\nstatic int common_hrtimer_try_to_cancel(struct k_itimer *timr)\n{\n\treturn hrtimer_try_to_cancel(&timr->it.real.timer);\n}\n\n/* Set a POSIX.1b interval timer. */\nint common_timer_set(struct k_itimer *timr, int flags,\n\t\t     struct itimerspec64 *new_setting,\n\t\t     struct itimerspec64 *old_setting)\n{\n\tconst struct k_clock *kc = timr->kclock;\n\tbool sigev_none;\n\tktime_t expires;\n\n\tif (old_setting)\n\t\tcommon_timer_get(timr, old_setting);\n\n\t/* Prevent rearming by clearing the interval */\n\ttimr->it_interval = 0;\n\t/*\n\t * Careful here. On SMP systems the timer expiry function could be\n\t * active and spinning on timr->it_lock.\n\t */\n\tif (kc->timer_try_to_cancel(timr) < 0)\n\t\treturn TIMER_RETRY;\n\n\ttimr->it_active = 0;\n\ttimr->it_requeue_pending = (timr->it_requeue_pending + 2) &\n\t\t~REQUEUE_PENDING;\n\ttimr->it_overrun_last = 0;\n\n\t/* Switch off the timer when it_value is zero */\n\tif (!new_setting->it_value.tv_sec && !new_setting->it_value.tv_nsec)\n\t\treturn 0;\n\n\ttimr->it_interval = timespec64_to_ktime(new_setting->it_interval);\n\texpires = timespec64_to_ktime(new_setting->it_value);\n\tsigev_none = (timr->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE;\n\n\tkc->timer_arm(timr, expires, flags & TIMER_ABSTIME, sigev_none);\n\ttimr->it_active = !sigev_none;\n\treturn 0;\n}\n\nstatic int do_timer_settime(timer_t timer_id, int flags,\n\t\t\t    struct itimerspec64 *new_spec64,\n\t\t\t    struct itimerspec64 *old_spec64)\n{\n\tconst struct k_clock *kc;\n\tstruct k_itimer *timr;\n\tunsigned long flag;\n\tint error = 0;\n\n\tif (!timespec64_valid(&new_spec64->it_interval) ||\n\t    !timespec64_valid(&new_spec64->it_value))\n\t\treturn -EINVAL;\n\n\tif (old_spec64)\n\t\tmemset(old_spec64, 0, sizeof(*old_spec64));\nretry:\n\ttimr = lock_timer(timer_id, &flag);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\tkc = timr->kclock;\n\tif (WARN_ON_ONCE(!kc || !kc->timer_set))\n\t\terror = -EINVAL;\n\telse\n\t\terror = kc->timer_set(timr, flags, new_spec64, old_spec64);\n\n\tunlock_timer(timr, flag);\n\tif (error == TIMER_RETRY) {\n\t\told_spec64 = NULL;\t// We already got the old time...\n\t\tgoto retry;\n\t}\n\n\treturn error;\n}\n\n/* Set a POSIX.1b interval timer */\nSYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,\n\t\tconst struct itimerspec __user *, new_setting,\n\t\tstruct itimerspec __user *, old_setting)\n{\n\tstruct itimerspec64 new_spec, old_spec;\n\tstruct itimerspec64 *rtn = old_setting ? &old_spec : NULL;\n\tint error = 0;\n\n\tif (!new_setting)\n\t\treturn -EINVAL;\n\n\tif (get_itimerspec64(&new_spec, new_setting))\n\t\treturn -EFAULT;\n\n\terror = do_timer_settime(timer_id, flags, &new_spec, rtn);\n\tif (!error && old_setting) {\n\t\tif (put_itimerspec64(&old_spec, old_setting))\n\t\t\terror = -EFAULT;\n\t}\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,\n\t\t       struct compat_itimerspec __user *, new,\n\t\t       struct compat_itimerspec __user *, old)\n{\n\tstruct itimerspec64 new_spec, old_spec;\n\tstruct itimerspec64 *rtn = old ? &old_spec : NULL;\n\tint error = 0;\n\n\tif (!new)\n\t\treturn -EINVAL;\n\tif (get_compat_itimerspec64(&new_spec, new))\n\t\treturn -EFAULT;\n\n\terror = do_timer_settime(timer_id, flags, &new_spec, rtn);\n\tif (!error && old) {\n\t\tif (put_compat_itimerspec64(&old_spec, old))\n\t\t\terror = -EFAULT;\n\t}\n\treturn error;\n}\n#endif\n\nint common_timer_del(struct k_itimer *timer)\n{\n\tconst struct k_clock *kc = timer->kclock;\n\n\ttimer->it_interval = 0;\n\tif (kc->timer_try_to_cancel(timer) < 0)\n\t\treturn TIMER_RETRY;\n\ttimer->it_active = 0;\n\treturn 0;\n}\n\nstatic inline int timer_delete_hook(struct k_itimer *timer)\n{\n\tconst struct k_clock *kc = timer->kclock;\n\n\tif (WARN_ON_ONCE(!kc || !kc->timer_del))\n\t\treturn -EINVAL;\n\treturn kc->timer_del(timer);\n}\n\n/* Delete a POSIX.1b interval timer. */\nSYSCALL_DEFINE1(timer_delete, timer_t, timer_id)\n{\n\tstruct k_itimer *timer;\n\tunsigned long flags;\n\nretry_delete:\n\ttimer = lock_timer(timer_id, &flags);\n\tif (!timer)\n\t\treturn -EINVAL;\n\n\tif (timer_delete_hook(timer) == TIMER_RETRY) {\n\t\tunlock_timer(timer, flags);\n\t\tgoto retry_delete;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\tlist_del(&timer->list);\n\tspin_unlock(&current->sighand->siglock);\n\t/*\n\t * This keeps any tasks waiting on the spin lock from thinking\n\t * they got something (see the lock code above).\n\t */\n\ttimer->it_signal = NULL;\n\n\tunlock_timer(timer, flags);\n\trelease_posix_timer(timer, IT_ID_SET);\n\treturn 0;\n}\n\n/*\n * return timer owned by the process, used by exit_itimers\n */\nstatic void itimer_delete(struct k_itimer *timer)\n{\n\tunsigned long flags;\n\nretry_delete:\n\tspin_lock_irqsave(&timer->it_lock, flags);\n\n\tif (timer_delete_hook(timer) == TIMER_RETRY) {\n\t\tunlock_timer(timer, flags);\n\t\tgoto retry_delete;\n\t}\n\tlist_del(&timer->list);\n\t/*\n\t * This keeps any tasks waiting on the spin lock from thinking\n\t * they got something (see the lock code above).\n\t */\n\ttimer->it_signal = NULL;\n\n\tunlock_timer(timer, flags);\n\trelease_posix_timer(timer, IT_ID_SET);\n}\n\n/*\n * This is called by do_exit or de_thread, only when there are no more\n * references to the shared signal_struct.\n */\nvoid exit_itimers(struct signal_struct *sig)\n{\n\tstruct k_itimer *tmr;\n\n\twhile (!list_empty(&sig->posix_timers)) {\n\t\ttmr = list_entry(sig->posix_timers.next, struct k_itimer, list);\n\t\titimer_delete(tmr);\n\t}\n}\n\nSYSCALL_DEFINE2(clock_settime, const clockid_t, which_clock,\n\t\tconst struct timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 new_tp;\n\n\tif (!kc || !kc->clock_set)\n\t\treturn -EINVAL;\n\n\tif (get_timespec64(&new_tp, tp))\n\t\treturn -EFAULT;\n\n\treturn kc->clock_set(which_clock, &new_tp);\n}\n\nSYSCALL_DEFINE2(clock_gettime, const clockid_t, which_clock,\n\t\tstruct timespec __user *,tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 kernel_tp;\n\tint error;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terror = kc->clock_get(which_clock, &kernel_tp);\n\n\tif (!error && put_timespec64(&kernel_tp, tp))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\nSYSCALL_DEFINE2(clock_adjtime, const clockid_t, which_clock,\n\t\tstruct timex __user *, utx)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timex ktx;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->clock_adj)\n\t\treturn -EOPNOTSUPP;\n\n\tif (copy_from_user(&ktx, utx, sizeof(ktx)))\n\t\treturn -EFAULT;\n\n\terr = kc->clock_adj(which_clock, &ktx);\n\n\tif (err >= 0 && copy_to_user(utx, &ktx, sizeof(ktx)))\n\t\treturn -EFAULT;\n\n\treturn err;\n}\n\nSYSCALL_DEFINE2(clock_getres, const clockid_t, which_clock,\n\t\tstruct timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 rtn_tp;\n\tint error;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terror = kc->clock_getres(which_clock, &rtn_tp);\n\n\tif (!error && tp && put_timespec64(&rtn_tp, tp))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT\n\nCOMPAT_SYSCALL_DEFINE2(clock_settime, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\n\tif (!kc || !kc->clock_set)\n\t\treturn -EINVAL;\n\n\tif (compat_get_timespec64(&ts, tp))\n\t\treturn -EFAULT;\n\n\treturn kc->clock_set(which_clock, &ts);\n}\n\nCOMPAT_SYSCALL_DEFINE2(clock_gettime, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terr = kc->clock_get(which_clock, &ts);\n\n\tif (!err && compat_put_timespec64(&ts, tp))\n\t\terr = -EFAULT;\n\n\treturn err;\n}\n\nCOMPAT_SYSCALL_DEFINE2(clock_adjtime, clockid_t, which_clock,\n\t\t       struct compat_timex __user *, utp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timex ktx;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->clock_adj)\n\t\treturn -EOPNOTSUPP;\n\n\terr = compat_get_timex(&ktx, utp);\n\tif (err)\n\t\treturn err;\n\n\terr = kc->clock_adj(which_clock, &ktx);\n\n\tif (err >= 0)\n\t\terr = compat_put_timex(utp, &ktx);\n\n\treturn err;\n}\n\nCOMPAT_SYSCALL_DEFINE2(clock_getres, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terr = kc->clock_getres(which_clock, &ts);\n\tif (!err && tp && compat_put_timespec64(&ts, tp))\n\t\treturn -EFAULT;\n\n\treturn err;\n}\n\n#endif\n\n/*\n * nanosleep for monotonic and realtime clocks\n */\nstatic int common_nsleep(const clockid_t which_clock, int flags,\n\t\t\t const struct timespec64 *rqtp)\n{\n\treturn hrtimer_nanosleep(rqtp, flags & TIMER_ABSTIME ?\n\t\t\t\t HRTIMER_MODE_ABS : HRTIMER_MODE_REL,\n\t\t\t\t which_clock);\n}\n\nSYSCALL_DEFINE4(clock_nanosleep, const clockid_t, which_clock, int, flags,\n\t\tconst struct timespec __user *, rqtp,\n\t\tstruct timespec __user *, rmtp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 t;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->nsleep)\n\t\treturn -ENANOSLEEP_NOTSUP;\n\n\tif (get_timespec64(&t, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&t))\n\t\treturn -EINVAL;\n\tif (flags & TIMER_ABSTIME)\n\t\trmtp = NULL;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_NATIVE : TT_NONE;\n\tcurrent->restart_block.nanosleep.rmtp = rmtp;\n\n\treturn kc->nsleep(which_clock, flags, &t);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(clock_nanosleep, clockid_t, which_clock, int, flags,\n\t\t       struct compat_timespec __user *, rqtp,\n\t\t       struct compat_timespec __user *, rmtp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 t;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->nsleep)\n\t\treturn -ENANOSLEEP_NOTSUP;\n\n\tif (compat_get_timespec64(&t, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&t))\n\t\treturn -EINVAL;\n\tif (flags & TIMER_ABSTIME)\n\t\trmtp = NULL;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_COMPAT : TT_NONE;\n\tcurrent->restart_block.nanosleep.compat_rmtp = rmtp;\n\n\treturn kc->nsleep(which_clock, flags, &t);\n}\n#endif\n\nstatic const struct k_clock clock_realtime = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_clock_realtime_get,\n\t.clock_set\t\t= posix_clock_realtime_set,\n\t.clock_adj\t\t= posix_clock_realtime_adj,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_monotonic = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_ktime_get_ts,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_monotonic_raw = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_monotonic_raw,\n};\n\nstatic const struct k_clock clock_realtime_coarse = {\n\t.clock_getres\t\t= posix_get_coarse_res,\n\t.clock_get\t\t= posix_get_realtime_coarse,\n};\n\nstatic const struct k_clock clock_monotonic_coarse = {\n\t.clock_getres\t\t= posix_get_coarse_res,\n\t.clock_get\t\t= posix_get_monotonic_coarse,\n};\n\nstatic const struct k_clock clock_tai = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_tai,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_boottime = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_boottime,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock * const posix_clocks[] = {\n\t[CLOCK_REALTIME]\t\t= &clock_realtime,\n\t[CLOCK_MONOTONIC]\t\t= &clock_monotonic,\n\t[CLOCK_PROCESS_CPUTIME_ID]\t= &clock_process,\n\t[CLOCK_THREAD_CPUTIME_ID]\t= &clock_thread,\n\t[CLOCK_MONOTONIC_RAW]\t\t= &clock_monotonic_raw,\n\t[CLOCK_REALTIME_COARSE]\t\t= &clock_realtime_coarse,\n\t[CLOCK_MONOTONIC_COARSE]\t= &clock_monotonic_coarse,\n\t[CLOCK_BOOTTIME]\t\t= &clock_boottime,\n\t[CLOCK_REALTIME_ALARM]\t\t= &alarm_clock,\n\t[CLOCK_BOOTTIME_ALARM]\t\t= &alarm_clock,\n\t[CLOCK_TAI]\t\t\t= &clock_tai,\n};\n\nstatic const struct k_clock *clockid_to_kclock(const clockid_t id)\n{\n\tif (id < 0)\n\t\treturn (id & CLOCKFD_MASK) == CLOCKFD ?\n\t\t\t&clock_posix_dynamic : &clock_posix_cpu;\n\n\tif (id >= ARRAY_SIZE(posix_clocks) || !posix_clocks[id])\n\t\treturn NULL;\n\treturn posix_clocks[id];\n}\n"], "fixing_code": ["/*\n * linux/kernel/posix-timers.c\n *\n *\n * 2002-10-15  Posix Clocks & timers\n *                           by George Anzinger george@mvista.com\n *\n *\t\t\t     Copyright (C) 2002 2003 by MontaVista Software.\n *\n * 2004-06-01  Fix CLOCK_REALTIME clock/timer TIMER_ABSTIME bug.\n *\t\t\t     Copyright (C) 2004 Boris Hu\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or (at\n * your option) any later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n * General Public License for more details.\n\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n *\n * MontaVista Software | 1237 East Arques Avenue | Sunnyvale | CA 94085 | USA\n */\n\n/* These are all the functions necessary to implement\n * POSIX clocks & timers\n */\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/time.h>\n#include <linux/mutex.h>\n#include <linux/sched/task.h>\n\n#include <linux/uaccess.h>\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/compiler.h>\n#include <linux/hash.h>\n#include <linux/posix-clock.h>\n#include <linux/posix-timers.h>\n#include <linux/syscalls.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n#include <linux/export.h>\n#include <linux/hashtable.h>\n#include <linux/compat.h>\n\n#include \"timekeeping.h\"\n#include \"posix-timers.h\"\n\n/*\n * Management arrays for POSIX timers. Timers are now kept in static hash table\n * with 512 entries.\n * Timer ids are allocated by local routine, which selects proper hash head by\n * key, constructed from current->signal address and per signal struct counter.\n * This keeps timer ids unique per process, but now they can intersect between\n * processes.\n */\n\n/*\n * Lets keep our timers in a slab cache :-)\n */\nstatic struct kmem_cache *posix_timers_cache;\n\nstatic DEFINE_HASHTABLE(posix_timers_hashtable, 9);\nstatic DEFINE_SPINLOCK(hash_lock);\n\nstatic const struct k_clock * const posix_clocks[];\nstatic const struct k_clock *clockid_to_kclock(const clockid_t id);\nstatic const struct k_clock clock_realtime, clock_monotonic;\n\n/*\n * we assume that the new SIGEV_THREAD_ID shares no bits with the other\n * SIGEV values.  Here we put out an error if this assumption fails.\n */\n#if SIGEV_THREAD_ID != (SIGEV_THREAD_ID & \\\n                       ~(SIGEV_SIGNAL | SIGEV_NONE | SIGEV_THREAD))\n#error \"SIGEV_THREAD_ID must not share bit with other SIGEV values!\"\n#endif\n\n/*\n * parisc wants ENOTSUP instead of EOPNOTSUPP\n */\n#ifndef ENOTSUP\n# define ENANOSLEEP_NOTSUP EOPNOTSUPP\n#else\n# define ENANOSLEEP_NOTSUP ENOTSUP\n#endif\n\n/*\n * The timer ID is turned into a timer address by idr_find().\n * Verifying a valid ID consists of:\n *\n * a) checking that idr_find() returns other than -1.\n * b) checking that the timer id matches the one in the timer itself.\n * c) that the timer owner is in the callers thread group.\n */\n\n/*\n * CLOCKs: The POSIX standard calls for a couple of clocks and allows us\n *\t    to implement others.  This structure defines the various\n *\t    clocks.\n *\n * RESOLUTION: Clock resolution is used to round up timer and interval\n *\t    times, NOT to report clock times, which are reported with as\n *\t    much resolution as the system can muster.  In some cases this\n *\t    resolution may depend on the underlying clock hardware and\n *\t    may not be quantifiable until run time, and only then is the\n *\t    necessary code is written.\tThe standard says we should say\n *\t    something about this issue in the documentation...\n *\n * FUNCTIONS: The CLOCKs structure defines possible functions to\n *\t    handle various clock functions.\n *\n *\t    The standard POSIX timer management code assumes the\n *\t    following: 1.) The k_itimer struct (sched.h) is used for\n *\t    the timer.  2.) The list, it_lock, it_clock, it_id and\n *\t    it_pid fields are not modified by timer code.\n *\n * Permissions: It is assumed that the clock_settime() function defined\n *\t    for each clock will take care of permission checks.\t Some\n *\t    clocks may be set able by any user (i.e. local process\n *\t    clocks) others not.\t Currently the only set able clock we\n *\t    have is CLOCK_REALTIME and its high res counter part, both of\n *\t    which we beg off on and pass to do_sys_settimeofday().\n */\nstatic struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags);\n\n#define lock_timer(tid, flags)\t\t\t\t\t\t   \\\n({\tstruct k_itimer *__timr;\t\t\t\t\t   \\\n\t__cond_lock(&__timr->it_lock, __timr = __lock_timer(tid, flags));  \\\n\t__timr;\t\t\t\t\t\t\t\t   \\\n})\n\nstatic int hash(struct signal_struct *sig, unsigned int nr)\n{\n\treturn hash_32(hash32_ptr(sig) ^ nr, HASH_BITS(posix_timers_hashtable));\n}\n\nstatic struct k_itimer *__posix_timers_find(struct hlist_head *head,\n\t\t\t\t\t    struct signal_struct *sig,\n\t\t\t\t\t    timer_t id)\n{\n\tstruct k_itimer *timer;\n\n\thlist_for_each_entry_rcu(timer, head, t_hash) {\n\t\tif ((timer->it_signal == sig) && (timer->it_id == id))\n\t\t\treturn timer;\n\t}\n\treturn NULL;\n}\n\nstatic struct k_itimer *posix_timer_by_id(timer_t id)\n{\n\tstruct signal_struct *sig = current->signal;\n\tstruct hlist_head *head = &posix_timers_hashtable[hash(sig, id)];\n\n\treturn __posix_timers_find(head, sig, id);\n}\n\nstatic int posix_timer_add(struct k_itimer *timer)\n{\n\tstruct signal_struct *sig = current->signal;\n\tint first_free_id = sig->posix_timer_id;\n\tstruct hlist_head *head;\n\tint ret = -ENOENT;\n\n\tdo {\n\t\tspin_lock(&hash_lock);\n\t\thead = &posix_timers_hashtable[hash(sig, sig->posix_timer_id)];\n\t\tif (!__posix_timers_find(head, sig, sig->posix_timer_id)) {\n\t\t\thlist_add_head_rcu(&timer->t_hash, head);\n\t\t\tret = sig->posix_timer_id;\n\t\t}\n\t\tif (++sig->posix_timer_id < 0)\n\t\t\tsig->posix_timer_id = 0;\n\t\tif ((sig->posix_timer_id == first_free_id) && (ret == -ENOENT))\n\t\t\t/* Loop over all possible ids completed */\n\t\t\tret = -EAGAIN;\n\t\tspin_unlock(&hash_lock);\n\t} while (ret == -ENOENT);\n\treturn ret;\n}\n\nstatic inline void unlock_timer(struct k_itimer *timr, unsigned long flags)\n{\n\tspin_unlock_irqrestore(&timr->it_lock, flags);\n}\n\n/* Get clock_realtime */\nstatic int posix_clock_realtime_get(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_real_ts64(tp);\n\treturn 0;\n}\n\n/* Set clock_realtime */\nstatic int posix_clock_realtime_set(const clockid_t which_clock,\n\t\t\t\t    const struct timespec64 *tp)\n{\n\treturn do_sys_settimeofday64(tp, NULL);\n}\n\nstatic int posix_clock_realtime_adj(const clockid_t which_clock,\n\t\t\t\t    struct timex *t)\n{\n\treturn do_adjtimex(t);\n}\n\n/*\n * Get monotonic time for posix timers\n */\nstatic int posix_ktime_get_ts(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_ts64(tp);\n\treturn 0;\n}\n\n/*\n * Get monotonic-raw time for posix timers\n */\nstatic int posix_get_monotonic_raw(clockid_t which_clock, struct timespec64 *tp)\n{\n\tgetrawmonotonic64(tp);\n\treturn 0;\n}\n\n\nstatic int posix_get_realtime_coarse(clockid_t which_clock, struct timespec64 *tp)\n{\n\t*tp = current_kernel_time64();\n\treturn 0;\n}\n\nstatic int posix_get_monotonic_coarse(clockid_t which_clock,\n\t\t\t\t\t\tstruct timespec64 *tp)\n{\n\t*tp = get_monotonic_coarse64();\n\treturn 0;\n}\n\nstatic int posix_get_coarse_res(const clockid_t which_clock, struct timespec64 *tp)\n{\n\t*tp = ktime_to_timespec64(KTIME_LOW_RES);\n\treturn 0;\n}\n\nstatic int posix_get_boottime(const clockid_t which_clock, struct timespec64 *tp)\n{\n\tget_monotonic_boottime64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_tai(clockid_t which_clock, struct timespec64 *tp)\n{\n\ttimekeeping_clocktai64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_hrtimer_res(clockid_t which_clock, struct timespec64 *tp)\n{\n\ttp->tv_sec = 0;\n\ttp->tv_nsec = hrtimer_resolution;\n\treturn 0;\n}\n\n/*\n * Initialize everything, well, just everything in Posix clocks/timers ;)\n */\nstatic __init int init_posix_timers(void)\n{\n\tposix_timers_cache = kmem_cache_create(\"posix_timers_cache\",\n\t\t\t\t\tsizeof (struct k_itimer), 0, SLAB_PANIC,\n\t\t\t\t\tNULL);\n\treturn 0;\n}\n__initcall(init_posix_timers);\n\nstatic void common_hrtimer_rearm(struct k_itimer *timr)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\tif (!timr->it_interval)\n\t\treturn;\n\n\ttimr->it_overrun += (unsigned int) hrtimer_forward(timer,\n\t\t\t\t\t\ttimer->base->get_time(),\n\t\t\t\t\t\ttimr->it_interval);\n\thrtimer_restart(timer);\n}\n\n/*\n * This function is exported for use by the signal deliver code.  It is\n * called just prior to the info block being released and passes that\n * block to us.  It's function is to update the overrun entry AND to\n * restart the timer.  It should only be called if the timer is to be\n * restarted (i.e. we have flagged this in the sys_private entry of the\n * info block).\n *\n * To protect against the timer going away while the interrupt is queued,\n * we require that the it_requeue_pending flag be set.\n */\nvoid posixtimer_rearm(struct siginfo *info)\n{\n\tstruct k_itimer *timr;\n\tunsigned long flags;\n\n\ttimr = lock_timer(info->si_tid, &flags);\n\tif (!timr)\n\t\treturn;\n\n\tif (timr->it_requeue_pending == info->si_sys_private) {\n\t\ttimr->kclock->timer_rearm(timr);\n\n\t\ttimr->it_active = 1;\n\t\ttimr->it_overrun_last = timr->it_overrun;\n\t\ttimr->it_overrun = -1;\n\t\t++timr->it_requeue_pending;\n\n\t\tinfo->si_overrun += timr->it_overrun_last;\n\t}\n\n\tunlock_timer(timr, flags);\n}\n\nint posix_timer_event(struct k_itimer *timr, int si_private)\n{\n\tstruct task_struct *task;\n\tint shared, ret = -1;\n\t/*\n\t * FIXME: if ->sigq is queued we can race with\n\t * dequeue_signal()->posixtimer_rearm().\n\t *\n\t * If dequeue_signal() sees the \"right\" value of\n\t * si_sys_private it calls posixtimer_rearm().\n\t * We re-queue ->sigq and drop ->it_lock().\n\t * posixtimer_rearm() locks the timer\n\t * and re-schedules it while ->sigq is pending.\n\t * Not really bad, but not that we want.\n\t */\n\ttimr->sigq->info.si_sys_private = si_private;\n\n\trcu_read_lock();\n\ttask = pid_task(timr->it_pid, PIDTYPE_PID);\n\tif (task) {\n\t\tshared = !(timr->it_sigev_notify & SIGEV_THREAD_ID);\n\t\tret = send_sigqueue(timr->sigq, task, shared);\n\t}\n\trcu_read_unlock();\n\t/* If we failed to send the signal the timer stops. */\n\treturn ret > 0;\n}\n\n/*\n * This function gets called when a POSIX.1b interval timer expires.  It\n * is used as a callback from the kernel internal timer.  The\n * run_timer_list code ALWAYS calls with interrupts on.\n\n * This code is for CLOCK_REALTIME* and CLOCK_MONOTONIC* timers.\n */\nstatic enum hrtimer_restart posix_timer_fn(struct hrtimer *timer)\n{\n\tstruct k_itimer *timr;\n\tunsigned long flags;\n\tint si_private = 0;\n\tenum hrtimer_restart ret = HRTIMER_NORESTART;\n\n\ttimr = container_of(timer, struct k_itimer, it.real.timer);\n\tspin_lock_irqsave(&timr->it_lock, flags);\n\n\ttimr->it_active = 0;\n\tif (timr->it_interval != 0)\n\t\tsi_private = ++timr->it_requeue_pending;\n\n\tif (posix_timer_event(timr, si_private)) {\n\t\t/*\n\t\t * signal was not sent because of sig_ignor\n\t\t * we will not get a call back to restart it AND\n\t\t * it should be restarted.\n\t\t */\n\t\tif (timr->it_interval != 0) {\n\t\t\tktime_t now = hrtimer_cb_get_time(timer);\n\n\t\t\t/*\n\t\t\t * FIXME: What we really want, is to stop this\n\t\t\t * timer completely and restart it in case the\n\t\t\t * SIG_IGN is removed. This is a non trivial\n\t\t\t * change which involves sighand locking\n\t\t\t * (sigh !), which we don't want to do late in\n\t\t\t * the release cycle.\n\t\t\t *\n\t\t\t * For now we just let timers with an interval\n\t\t\t * less than a jiffie expire every jiffie to\n\t\t\t * avoid softirq starvation in case of SIG_IGN\n\t\t\t * and a very small interval, which would put\n\t\t\t * the timer right back on the softirq pending\n\t\t\t * list. By moving now ahead of time we trick\n\t\t\t * hrtimer_forward() to expire the timer\n\t\t\t * later, while we still maintain the overrun\n\t\t\t * accuracy, but have some inconsistency in\n\t\t\t * the timer_gettime() case. This is at least\n\t\t\t * better than a starved softirq. A more\n\t\t\t * complex fix which solves also another related\n\t\t\t * inconsistency is already in the pipeline.\n\t\t\t */\n#ifdef CONFIG_HIGH_RES_TIMERS\n\t\t\t{\n\t\t\t\tktime_t kj = NSEC_PER_SEC / HZ;\n\n\t\t\t\tif (timr->it_interval < kj)\n\t\t\t\t\tnow = ktime_add(now, kj);\n\t\t\t}\n#endif\n\t\t\ttimr->it_overrun += (unsigned int)\n\t\t\t\thrtimer_forward(timer, now,\n\t\t\t\t\t\ttimr->it_interval);\n\t\t\tret = HRTIMER_RESTART;\n\t\t\t++timr->it_requeue_pending;\n\t\t\ttimr->it_active = 1;\n\t\t}\n\t}\n\n\tunlock_timer(timr, flags);\n\treturn ret;\n}\n\nstatic struct pid *good_sigevent(sigevent_t * event)\n{\n\tstruct task_struct *rtn = current->group_leader;\n\n\tswitch (event->sigev_notify) {\n\tcase SIGEV_SIGNAL | SIGEV_THREAD_ID:\n\t\trtn = find_task_by_vpid(event->sigev_notify_thread_id);\n\t\tif (!rtn || !same_thread_group(rtn, current))\n\t\t\treturn NULL;\n\t\t/* FALLTHRU */\n\tcase SIGEV_SIGNAL:\n\tcase SIGEV_THREAD:\n\t\tif (event->sigev_signo <= 0 || event->sigev_signo > SIGRTMAX)\n\t\t\treturn NULL;\n\t\t/* FALLTHRU */\n\tcase SIGEV_NONE:\n\t\treturn task_pid(rtn);\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct k_itimer * alloc_posix_timer(void)\n{\n\tstruct k_itimer *tmr;\n\ttmr = kmem_cache_zalloc(posix_timers_cache, GFP_KERNEL);\n\tif (!tmr)\n\t\treturn tmr;\n\tif (unlikely(!(tmr->sigq = sigqueue_alloc()))) {\n\t\tkmem_cache_free(posix_timers_cache, tmr);\n\t\treturn NULL;\n\t}\n\tmemset(&tmr->sigq->info, 0, sizeof(siginfo_t));\n\treturn tmr;\n}\n\nstatic void k_itimer_rcu_free(struct rcu_head *head)\n{\n\tstruct k_itimer *tmr = container_of(head, struct k_itimer, it.rcu);\n\n\tkmem_cache_free(posix_timers_cache, tmr);\n}\n\n#define IT_ID_SET\t1\n#define IT_ID_NOT_SET\t0\nstatic void release_posix_timer(struct k_itimer *tmr, int it_id_set)\n{\n\tif (it_id_set) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&hash_lock, flags);\n\t\thlist_del_rcu(&tmr->t_hash);\n\t\tspin_unlock_irqrestore(&hash_lock, flags);\n\t}\n\tput_pid(tmr->it_pid);\n\tsigqueue_free(tmr->sigq);\n\tcall_rcu(&tmr->it.rcu, k_itimer_rcu_free);\n}\n\nstatic int common_timer_create(struct k_itimer *new_timer)\n{\n\thrtimer_init(&new_timer->it.real.timer, new_timer->it_clock, 0);\n\treturn 0;\n}\n\n/* Create a POSIX.1b interval timer. */\nstatic int do_timer_create(clockid_t which_clock, struct sigevent *event,\n\t\t\t   timer_t __user *created_timer_id)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct k_itimer *new_timer;\n\tint error, new_timer_id;\n\tint it_id_set = IT_ID_NOT_SET;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->timer_create)\n\t\treturn -EOPNOTSUPP;\n\n\tnew_timer = alloc_posix_timer();\n\tif (unlikely(!new_timer))\n\t\treturn -EAGAIN;\n\n\tspin_lock_init(&new_timer->it_lock);\n\tnew_timer_id = posix_timer_add(new_timer);\n\tif (new_timer_id < 0) {\n\t\terror = new_timer_id;\n\t\tgoto out;\n\t}\n\n\tit_id_set = IT_ID_SET;\n\tnew_timer->it_id = (timer_t) new_timer_id;\n\tnew_timer->it_clock = which_clock;\n\tnew_timer->kclock = kc;\n\tnew_timer->it_overrun = -1;\n\n\tif (event) {\n\t\trcu_read_lock();\n\t\tnew_timer->it_pid = get_pid(good_sigevent(event));\n\t\trcu_read_unlock();\n\t\tif (!new_timer->it_pid) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnew_timer->it_sigev_notify     = event->sigev_notify;\n\t\tnew_timer->sigq->info.si_signo = event->sigev_signo;\n\t\tnew_timer->sigq->info.si_value = event->sigev_value;\n\t} else {\n\t\tnew_timer->it_sigev_notify     = SIGEV_SIGNAL;\n\t\tnew_timer->sigq->info.si_signo = SIGALRM;\n\t\tmemset(&new_timer->sigq->info.si_value, 0, sizeof(sigval_t));\n\t\tnew_timer->sigq->info.si_value.sival_int = new_timer->it_id;\n\t\tnew_timer->it_pid = get_pid(task_tgid(current));\n\t}\n\n\tnew_timer->sigq->info.si_tid   = new_timer->it_id;\n\tnew_timer->sigq->info.si_code  = SI_TIMER;\n\n\tif (copy_to_user(created_timer_id,\n\t\t\t &new_timer_id, sizeof (new_timer_id))) {\n\t\terror = -EFAULT;\n\t\tgoto out;\n\t}\n\n\terror = kc->timer_create(new_timer);\n\tif (error)\n\t\tgoto out;\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tnew_timer->it_signal = current->signal;\n\tlist_add(&new_timer->list, &current->signal->posix_timers);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\treturn 0;\n\t/*\n\t * In the case of the timer belonging to another task, after\n\t * the task is unlocked, the timer is owned by the other task\n\t * and may cease to exist at any time.  Don't use or modify\n\t * new_timer after the unlock call.\n\t */\nout:\n\trelease_posix_timer(new_timer, it_id_set);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(timer_create, const clockid_t, which_clock,\n\t\tstruct sigevent __user *, timer_event_spec,\n\t\ttimer_t __user *, created_timer_id)\n{\n\tif (timer_event_spec) {\n\t\tsigevent_t event;\n\n\t\tif (copy_from_user(&event, timer_event_spec, sizeof (event)))\n\t\t\treturn -EFAULT;\n\t\treturn do_timer_create(which_clock, &event, created_timer_id);\n\t}\n\treturn do_timer_create(which_clock, NULL, created_timer_id);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(timer_create, clockid_t, which_clock,\n\t\t       struct compat_sigevent __user *, timer_event_spec,\n\t\t       timer_t __user *, created_timer_id)\n{\n\tif (timer_event_spec) {\n\t\tsigevent_t event;\n\n\t\tif (get_compat_sigevent(&event, timer_event_spec))\n\t\t\treturn -EFAULT;\n\t\treturn do_timer_create(which_clock, &event, created_timer_id);\n\t}\n\treturn do_timer_create(which_clock, NULL, created_timer_id);\n}\n#endif\n\n/*\n * Locking issues: We need to protect the result of the id look up until\n * we get the timer locked down so it is not deleted under us.  The\n * removal is done under the idr spinlock so we use that here to bridge\n * the find to the timer lock.  To avoid a dead lock, the timer id MUST\n * be release with out holding the timer lock.\n */\nstatic struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags)\n{\n\tstruct k_itimer *timr;\n\n\t/*\n\t * timer_t could be any type >= int and we want to make sure any\n\t * @timer_id outside positive int range fails lookup.\n\t */\n\tif ((unsigned long long)timer_id > INT_MAX)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttimr = posix_timer_by_id(timer_id);\n\tif (timr) {\n\t\tspin_lock_irqsave(&timr->it_lock, *flags);\n\t\tif (timr->it_signal == current->signal) {\n\t\t\trcu_read_unlock();\n\t\t\treturn timr;\n\t\t}\n\t\tspin_unlock_irqrestore(&timr->it_lock, *flags);\n\t}\n\trcu_read_unlock();\n\n\treturn NULL;\n}\n\nstatic ktime_t common_hrtimer_remaining(struct k_itimer *timr, ktime_t now)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\treturn __hrtimer_expires_remaining_adjusted(timer, now);\n}\n\nstatic int common_hrtimer_forward(struct k_itimer *timr, ktime_t now)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\treturn (int)hrtimer_forward(timer, now, timr->it_interval);\n}\n\n/*\n * Get the time remaining on a POSIX.1b interval timer.  This function\n * is ALWAYS called with spin_lock_irq on the timer, thus it must not\n * mess with irq.\n *\n * We have a couple of messes to clean up here.  First there is the case\n * of a timer that has a requeue pending.  These timers should appear to\n * be in the timer list with an expiry as if we were to requeue them\n * now.\n *\n * The second issue is the SIGEV_NONE timer which may be active but is\n * not really ever put in the timer list (to save system resources).\n * This timer may be expired, and if so, we will do it here.  Otherwise\n * it is the same as a requeue pending timer WRT to what we should\n * report.\n */\nvoid common_timer_get(struct k_itimer *timr, struct itimerspec64 *cur_setting)\n{\n\tconst struct k_clock *kc = timr->kclock;\n\tktime_t now, remaining, iv;\n\tstruct timespec64 ts64;\n\tbool sig_none;\n\n\tsig_none = timr->it_sigev_notify == SIGEV_NONE;\n\tiv = timr->it_interval;\n\n\t/* interval timer ? */\n\tif (iv) {\n\t\tcur_setting->it_interval = ktime_to_timespec64(iv);\n\t} else if (!timr->it_active) {\n\t\t/*\n\t\t * SIGEV_NONE oneshot timers are never queued. Check them\n\t\t * below.\n\t\t */\n\t\tif (!sig_none)\n\t\t\treturn;\n\t}\n\n\t/*\n\t * The timespec64 based conversion is suboptimal, but it's not\n\t * worth to implement yet another callback.\n\t */\n\tkc->clock_get(timr->it_clock, &ts64);\n\tnow = timespec64_to_ktime(ts64);\n\n\t/*\n\t * When a requeue is pending or this is a SIGEV_NONE timer move the\n\t * expiry time forward by intervals, so expiry is > now.\n\t */\n\tif (iv && (timr->it_requeue_pending & REQUEUE_PENDING || sig_none))\n\t\ttimr->it_overrun += kc->timer_forward(timr, now);\n\n\tremaining = kc->timer_remaining(timr, now);\n\t/* Return 0 only, when the timer is expired and not pending */\n\tif (remaining <= 0) {\n\t\t/*\n\t\t * A single shot SIGEV_NONE timer must return 0, when\n\t\t * it is expired !\n\t\t */\n\t\tif (!sig_none)\n\t\t\tcur_setting->it_value.tv_nsec = 1;\n\t} else {\n\t\tcur_setting->it_value = ktime_to_timespec64(remaining);\n\t}\n}\n\n/* Get the time remaining on a POSIX.1b interval timer. */\nstatic int do_timer_gettime(timer_t timer_id,  struct itimerspec64 *setting)\n{\n\tstruct k_itimer *timr;\n\tconst struct k_clock *kc;\n\tunsigned long flags;\n\tint ret = 0;\n\n\ttimr = lock_timer(timer_id, &flags);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\tmemset(setting, 0, sizeof(*setting));\n\tkc = timr->kclock;\n\tif (WARN_ON_ONCE(!kc || !kc->timer_get))\n\t\tret = -EINVAL;\n\telse\n\t\tkc->timer_get(timr, setting);\n\n\tunlock_timer(timr, flags);\n\treturn ret;\n}\n\n/* Get the time remaining on a POSIX.1b interval timer. */\nSYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,\n\t\tstruct itimerspec __user *, setting)\n{\n\tstruct itimerspec64 cur_setting;\n\n\tint ret = do_timer_gettime(timer_id, &cur_setting);\n\tif (!ret) {\n\t\tif (put_itimerspec64(&cur_setting, setting))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,\n\t\t       struct compat_itimerspec __user *, setting)\n{\n\tstruct itimerspec64 cur_setting;\n\n\tint ret = do_timer_gettime(timer_id, &cur_setting);\n\tif (!ret) {\n\t\tif (put_compat_itimerspec64(&cur_setting, setting))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n#endif\n\n/*\n * Get the number of overruns of a POSIX.1b interval timer.  This is to\n * be the overrun of the timer last delivered.  At the same time we are\n * accumulating overruns on the next timer.  The overrun is frozen when\n * the signal is delivered, either at the notify time (if the info block\n * is not queued) or at the actual delivery time (as we are informed by\n * the call back to posixtimer_rearm().  So all we need to do is\n * to pick up the frozen overrun.\n */\nSYSCALL_DEFINE1(timer_getoverrun, timer_t, timer_id)\n{\n\tstruct k_itimer *timr;\n\tint overrun;\n\tunsigned long flags;\n\n\ttimr = lock_timer(timer_id, &flags);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\toverrun = timr->it_overrun_last;\n\tunlock_timer(timr, flags);\n\n\treturn overrun;\n}\n\nstatic void common_hrtimer_arm(struct k_itimer *timr, ktime_t expires,\n\t\t\t       bool absolute, bool sigev_none)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\tenum hrtimer_mode mode;\n\n\tmode = absolute ? HRTIMER_MODE_ABS : HRTIMER_MODE_REL;\n\t/*\n\t * Posix magic: Relative CLOCK_REALTIME timers are not affected by\n\t * clock modifications, so they become CLOCK_MONOTONIC based under the\n\t * hood. See hrtimer_init(). Update timr->kclock, so the generic\n\t * functions which use timr->kclock->clock_get() work.\n\t *\n\t * Note: it_clock stays unmodified, because the next timer_set() might\n\t * use ABSTIME, so it needs to switch back.\n\t */\n\tif (timr->it_clock == CLOCK_REALTIME)\n\t\ttimr->kclock = absolute ? &clock_realtime : &clock_monotonic;\n\n\thrtimer_init(&timr->it.real.timer, timr->it_clock, mode);\n\ttimr->it.real.timer.function = posix_timer_fn;\n\n\tif (!absolute)\n\t\texpires = ktime_add_safe(expires, timer->base->get_time());\n\thrtimer_set_expires(timer, expires);\n\n\tif (!sigev_none)\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n}\n\nstatic int common_hrtimer_try_to_cancel(struct k_itimer *timr)\n{\n\treturn hrtimer_try_to_cancel(&timr->it.real.timer);\n}\n\n/* Set a POSIX.1b interval timer. */\nint common_timer_set(struct k_itimer *timr, int flags,\n\t\t     struct itimerspec64 *new_setting,\n\t\t     struct itimerspec64 *old_setting)\n{\n\tconst struct k_clock *kc = timr->kclock;\n\tbool sigev_none;\n\tktime_t expires;\n\n\tif (old_setting)\n\t\tcommon_timer_get(timr, old_setting);\n\n\t/* Prevent rearming by clearing the interval */\n\ttimr->it_interval = 0;\n\t/*\n\t * Careful here. On SMP systems the timer expiry function could be\n\t * active and spinning on timr->it_lock.\n\t */\n\tif (kc->timer_try_to_cancel(timr) < 0)\n\t\treturn TIMER_RETRY;\n\n\ttimr->it_active = 0;\n\ttimr->it_requeue_pending = (timr->it_requeue_pending + 2) &\n\t\t~REQUEUE_PENDING;\n\ttimr->it_overrun_last = 0;\n\n\t/* Switch off the timer when it_value is zero */\n\tif (!new_setting->it_value.tv_sec && !new_setting->it_value.tv_nsec)\n\t\treturn 0;\n\n\ttimr->it_interval = timespec64_to_ktime(new_setting->it_interval);\n\texpires = timespec64_to_ktime(new_setting->it_value);\n\tsigev_none = timr->it_sigev_notify == SIGEV_NONE;\n\n\tkc->timer_arm(timr, expires, flags & TIMER_ABSTIME, sigev_none);\n\ttimr->it_active = !sigev_none;\n\treturn 0;\n}\n\nstatic int do_timer_settime(timer_t timer_id, int flags,\n\t\t\t    struct itimerspec64 *new_spec64,\n\t\t\t    struct itimerspec64 *old_spec64)\n{\n\tconst struct k_clock *kc;\n\tstruct k_itimer *timr;\n\tunsigned long flag;\n\tint error = 0;\n\n\tif (!timespec64_valid(&new_spec64->it_interval) ||\n\t    !timespec64_valid(&new_spec64->it_value))\n\t\treturn -EINVAL;\n\n\tif (old_spec64)\n\t\tmemset(old_spec64, 0, sizeof(*old_spec64));\nretry:\n\ttimr = lock_timer(timer_id, &flag);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\tkc = timr->kclock;\n\tif (WARN_ON_ONCE(!kc || !kc->timer_set))\n\t\terror = -EINVAL;\n\telse\n\t\terror = kc->timer_set(timr, flags, new_spec64, old_spec64);\n\n\tunlock_timer(timr, flag);\n\tif (error == TIMER_RETRY) {\n\t\told_spec64 = NULL;\t// We already got the old time...\n\t\tgoto retry;\n\t}\n\n\treturn error;\n}\n\n/* Set a POSIX.1b interval timer */\nSYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,\n\t\tconst struct itimerspec __user *, new_setting,\n\t\tstruct itimerspec __user *, old_setting)\n{\n\tstruct itimerspec64 new_spec, old_spec;\n\tstruct itimerspec64 *rtn = old_setting ? &old_spec : NULL;\n\tint error = 0;\n\n\tif (!new_setting)\n\t\treturn -EINVAL;\n\n\tif (get_itimerspec64(&new_spec, new_setting))\n\t\treturn -EFAULT;\n\n\terror = do_timer_settime(timer_id, flags, &new_spec, rtn);\n\tif (!error && old_setting) {\n\t\tif (put_itimerspec64(&old_spec, old_setting))\n\t\t\terror = -EFAULT;\n\t}\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,\n\t\t       struct compat_itimerspec __user *, new,\n\t\t       struct compat_itimerspec __user *, old)\n{\n\tstruct itimerspec64 new_spec, old_spec;\n\tstruct itimerspec64 *rtn = old ? &old_spec : NULL;\n\tint error = 0;\n\n\tif (!new)\n\t\treturn -EINVAL;\n\tif (get_compat_itimerspec64(&new_spec, new))\n\t\treturn -EFAULT;\n\n\terror = do_timer_settime(timer_id, flags, &new_spec, rtn);\n\tif (!error && old) {\n\t\tif (put_compat_itimerspec64(&old_spec, old))\n\t\t\terror = -EFAULT;\n\t}\n\treturn error;\n}\n#endif\n\nint common_timer_del(struct k_itimer *timer)\n{\n\tconst struct k_clock *kc = timer->kclock;\n\n\ttimer->it_interval = 0;\n\tif (kc->timer_try_to_cancel(timer) < 0)\n\t\treturn TIMER_RETRY;\n\ttimer->it_active = 0;\n\treturn 0;\n}\n\nstatic inline int timer_delete_hook(struct k_itimer *timer)\n{\n\tconst struct k_clock *kc = timer->kclock;\n\n\tif (WARN_ON_ONCE(!kc || !kc->timer_del))\n\t\treturn -EINVAL;\n\treturn kc->timer_del(timer);\n}\n\n/* Delete a POSIX.1b interval timer. */\nSYSCALL_DEFINE1(timer_delete, timer_t, timer_id)\n{\n\tstruct k_itimer *timer;\n\tunsigned long flags;\n\nretry_delete:\n\ttimer = lock_timer(timer_id, &flags);\n\tif (!timer)\n\t\treturn -EINVAL;\n\n\tif (timer_delete_hook(timer) == TIMER_RETRY) {\n\t\tunlock_timer(timer, flags);\n\t\tgoto retry_delete;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\tlist_del(&timer->list);\n\tspin_unlock(&current->sighand->siglock);\n\t/*\n\t * This keeps any tasks waiting on the spin lock from thinking\n\t * they got something (see the lock code above).\n\t */\n\ttimer->it_signal = NULL;\n\n\tunlock_timer(timer, flags);\n\trelease_posix_timer(timer, IT_ID_SET);\n\treturn 0;\n}\n\n/*\n * return timer owned by the process, used by exit_itimers\n */\nstatic void itimer_delete(struct k_itimer *timer)\n{\n\tunsigned long flags;\n\nretry_delete:\n\tspin_lock_irqsave(&timer->it_lock, flags);\n\n\tif (timer_delete_hook(timer) == TIMER_RETRY) {\n\t\tunlock_timer(timer, flags);\n\t\tgoto retry_delete;\n\t}\n\tlist_del(&timer->list);\n\t/*\n\t * This keeps any tasks waiting on the spin lock from thinking\n\t * they got something (see the lock code above).\n\t */\n\ttimer->it_signal = NULL;\n\n\tunlock_timer(timer, flags);\n\trelease_posix_timer(timer, IT_ID_SET);\n}\n\n/*\n * This is called by do_exit or de_thread, only when there are no more\n * references to the shared signal_struct.\n */\nvoid exit_itimers(struct signal_struct *sig)\n{\n\tstruct k_itimer *tmr;\n\n\twhile (!list_empty(&sig->posix_timers)) {\n\t\ttmr = list_entry(sig->posix_timers.next, struct k_itimer, list);\n\t\titimer_delete(tmr);\n\t}\n}\n\nSYSCALL_DEFINE2(clock_settime, const clockid_t, which_clock,\n\t\tconst struct timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 new_tp;\n\n\tif (!kc || !kc->clock_set)\n\t\treturn -EINVAL;\n\n\tif (get_timespec64(&new_tp, tp))\n\t\treturn -EFAULT;\n\n\treturn kc->clock_set(which_clock, &new_tp);\n}\n\nSYSCALL_DEFINE2(clock_gettime, const clockid_t, which_clock,\n\t\tstruct timespec __user *,tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 kernel_tp;\n\tint error;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terror = kc->clock_get(which_clock, &kernel_tp);\n\n\tif (!error && put_timespec64(&kernel_tp, tp))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\nSYSCALL_DEFINE2(clock_adjtime, const clockid_t, which_clock,\n\t\tstruct timex __user *, utx)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timex ktx;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->clock_adj)\n\t\treturn -EOPNOTSUPP;\n\n\tif (copy_from_user(&ktx, utx, sizeof(ktx)))\n\t\treturn -EFAULT;\n\n\terr = kc->clock_adj(which_clock, &ktx);\n\n\tif (err >= 0 && copy_to_user(utx, &ktx, sizeof(ktx)))\n\t\treturn -EFAULT;\n\n\treturn err;\n}\n\nSYSCALL_DEFINE2(clock_getres, const clockid_t, which_clock,\n\t\tstruct timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 rtn_tp;\n\tint error;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terror = kc->clock_getres(which_clock, &rtn_tp);\n\n\tif (!error && tp && put_timespec64(&rtn_tp, tp))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT\n\nCOMPAT_SYSCALL_DEFINE2(clock_settime, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\n\tif (!kc || !kc->clock_set)\n\t\treturn -EINVAL;\n\n\tif (compat_get_timespec64(&ts, tp))\n\t\treturn -EFAULT;\n\n\treturn kc->clock_set(which_clock, &ts);\n}\n\nCOMPAT_SYSCALL_DEFINE2(clock_gettime, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terr = kc->clock_get(which_clock, &ts);\n\n\tif (!err && compat_put_timespec64(&ts, tp))\n\t\terr = -EFAULT;\n\n\treturn err;\n}\n\nCOMPAT_SYSCALL_DEFINE2(clock_adjtime, clockid_t, which_clock,\n\t\t       struct compat_timex __user *, utp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timex ktx;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->clock_adj)\n\t\treturn -EOPNOTSUPP;\n\n\terr = compat_get_timex(&ktx, utp);\n\tif (err)\n\t\treturn err;\n\n\terr = kc->clock_adj(which_clock, &ktx);\n\n\tif (err >= 0)\n\t\terr = compat_put_timex(utp, &ktx);\n\n\treturn err;\n}\n\nCOMPAT_SYSCALL_DEFINE2(clock_getres, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terr = kc->clock_getres(which_clock, &ts);\n\tif (!err && tp && compat_put_timespec64(&ts, tp))\n\t\treturn -EFAULT;\n\n\treturn err;\n}\n\n#endif\n\n/*\n * nanosleep for monotonic and realtime clocks\n */\nstatic int common_nsleep(const clockid_t which_clock, int flags,\n\t\t\t const struct timespec64 *rqtp)\n{\n\treturn hrtimer_nanosleep(rqtp, flags & TIMER_ABSTIME ?\n\t\t\t\t HRTIMER_MODE_ABS : HRTIMER_MODE_REL,\n\t\t\t\t which_clock);\n}\n\nSYSCALL_DEFINE4(clock_nanosleep, const clockid_t, which_clock, int, flags,\n\t\tconst struct timespec __user *, rqtp,\n\t\tstruct timespec __user *, rmtp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 t;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->nsleep)\n\t\treturn -ENANOSLEEP_NOTSUP;\n\n\tif (get_timespec64(&t, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&t))\n\t\treturn -EINVAL;\n\tif (flags & TIMER_ABSTIME)\n\t\trmtp = NULL;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_NATIVE : TT_NONE;\n\tcurrent->restart_block.nanosleep.rmtp = rmtp;\n\n\treturn kc->nsleep(which_clock, flags, &t);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(clock_nanosleep, clockid_t, which_clock, int, flags,\n\t\t       struct compat_timespec __user *, rqtp,\n\t\t       struct compat_timespec __user *, rmtp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 t;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->nsleep)\n\t\treturn -ENANOSLEEP_NOTSUP;\n\n\tif (compat_get_timespec64(&t, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&t))\n\t\treturn -EINVAL;\n\tif (flags & TIMER_ABSTIME)\n\t\trmtp = NULL;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_COMPAT : TT_NONE;\n\tcurrent->restart_block.nanosleep.compat_rmtp = rmtp;\n\n\treturn kc->nsleep(which_clock, flags, &t);\n}\n#endif\n\nstatic const struct k_clock clock_realtime = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_clock_realtime_get,\n\t.clock_set\t\t= posix_clock_realtime_set,\n\t.clock_adj\t\t= posix_clock_realtime_adj,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_monotonic = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_ktime_get_ts,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_monotonic_raw = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_monotonic_raw,\n};\n\nstatic const struct k_clock clock_realtime_coarse = {\n\t.clock_getres\t\t= posix_get_coarse_res,\n\t.clock_get\t\t= posix_get_realtime_coarse,\n};\n\nstatic const struct k_clock clock_monotonic_coarse = {\n\t.clock_getres\t\t= posix_get_coarse_res,\n\t.clock_get\t\t= posix_get_monotonic_coarse,\n};\n\nstatic const struct k_clock clock_tai = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_tai,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_boottime = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_boottime,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock * const posix_clocks[] = {\n\t[CLOCK_REALTIME]\t\t= &clock_realtime,\n\t[CLOCK_MONOTONIC]\t\t= &clock_monotonic,\n\t[CLOCK_PROCESS_CPUTIME_ID]\t= &clock_process,\n\t[CLOCK_THREAD_CPUTIME_ID]\t= &clock_thread,\n\t[CLOCK_MONOTONIC_RAW]\t\t= &clock_monotonic_raw,\n\t[CLOCK_REALTIME_COARSE]\t\t= &clock_realtime_coarse,\n\t[CLOCK_MONOTONIC_COARSE]\t= &clock_monotonic_coarse,\n\t[CLOCK_BOOTTIME]\t\t= &clock_boottime,\n\t[CLOCK_REALTIME_ALARM]\t\t= &alarm_clock,\n\t[CLOCK_BOOTTIME_ALARM]\t\t= &alarm_clock,\n\t[CLOCK_TAI]\t\t\t= &clock_tai,\n};\n\nstatic const struct k_clock *clockid_to_kclock(const clockid_t id)\n{\n\tif (id < 0)\n\t\treturn (id & CLOCKFD_MASK) == CLOCKFD ?\n\t\t\t&clock_posix_dynamic : &clock_posix_cpu;\n\n\tif (id >= ARRAY_SIZE(posix_clocks) || !posix_clocks[id])\n\t\treturn NULL;\n\treturn posix_clocks[id];\n}\n"], "filenames": ["kernel/time/posix-timers.c"], "buggy_code_start_loc": [437], "buggy_code_end_loc": [860], "fixing_code_start_loc": [437], "fixing_code_end_loc": [865], "type": "CWE-125", "message": "The timer_create syscall implementation in kernel/time/posix-timers.c in the Linux kernel before 4.14.8 doesn't properly validate the sigevent->sigev_notify field, which leads to out-of-bounds access in the show_timer function (called when /proc/$PID/timers is read). This allows userspace applications to read arbitrary kernel memory (on a kernel built with CONFIG_POSIX_TIMERS and CONFIG_CHECKPOINT_RESTORE).", "other": {"cve": {"id": "CVE-2017-18344", "sourceIdentifier": "cve@mitre.org", "published": "2018-07-26T19:29:00.427", "lastModified": "2020-10-15T13:28:10.487", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The timer_create syscall implementation in kernel/time/posix-timers.c in the Linux kernel before 4.14.8 doesn't properly validate the sigevent->sigev_notify field, which leads to out-of-bounds access in the show_timer function (called when /proc/$PID/timers is read). This allows userspace applications to read arbitrary kernel memory (on a kernel built with CONFIG_POSIX_TIMERS and CONFIG_CHECKPOINT_RESTORE)."}, {"lang": "es", "value": "La implementaci\u00f3n de llamada del sistema timer_create en kernel/time/posix-timers.c en el kernel de Linux en versiones anteriores a la 4.14.8 no valida correctamente el campo sigevent->sigev_notify, conduciendo a un acceso fuera de l\u00edmites en la funci\u00f3n show_timer (que se llama cuando se lee /proc/$PID/timers). Esto permite que las aplicaciones del espacio del usuario lean memoria del kernel arbitraria (en un kernel construido con CONFIG_POSIX_TIMERS y CONFIG_CHECKPOINT_RESTORE)."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.14.8", "matchCriteriaId": "41108D9D-6660-4D49-A5EE-169D8290F391"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:esm:*:*:*", "matchCriteriaId": "8D305F7A-D159-4716-AB26-5E38BB5CD991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:mrg_realtime:2.0:*:*:*:*:*:*:*", "matchCriteriaId": "AFB0FFE3-4BE1-4024-BCC6-1B87074DE2E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_desktop:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "33C068A4-3780-4EAB-A937-6082DF847564"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "51EF4996-72F4-4FA4-814F-F5991E7A8318"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:7.2:*:*:*:*:*:*:*", "matchCriteriaId": "1C8D871B-AEA1-4407-AEE3-47EC782250FF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:7.3:*:*:*:*:*:*:*", "matchCriteriaId": "98381E61-F082-4302-B51F-5648884F998B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:7.4:*:*:*:*:*:*:*", "matchCriteriaId": "D99A687E-EAE6-417E-A88E-D0082BC194CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_eus:7.3:*:*:*:*:*:*:*", "matchCriteriaId": "A8442C20-41F9-47FD-9A12-E724D3A31FD7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_eus:7.5:*:*:*:*:*:*:*", "matchCriteriaId": "A4E9DD8A-A68B-4A69-8B01-BFF92A2020A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.2:*:*:*:*:*:*:*", "matchCriteriaId": "6755B6AD-0422-467B-8115-34A60B1D1A40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.3:*:*:*:*:*:*:*", "matchCriteriaId": "24C0F4E1-C52C-41E0-9F14-F83ADD5CC7ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.4:*:*:*:*:*:*:*", "matchCriteriaId": "D5F7E11E-FB34-4467-8919-2B6BEAABF665"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_workstation:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "825ECE2D-E232-46E0-A047-074B34DB1E97"}]}]}], "references": [{"url": "http://www.securityfocus.com/bid/104909", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1041414", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2948", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3083", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3096", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3459", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3540", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3586", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3590", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3591", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.14.8", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/cef31d9af908243421258f1df35a4a644604efbe", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3742-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3742-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.exploit-db.com/exploits/45175/", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/cef31d9af908243421258f1df35a4a644604efbe"}}