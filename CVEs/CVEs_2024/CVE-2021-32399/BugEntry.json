{"buggy_code": ["/*\n   BlueZ - Bluetooth protocol stack for Linux\n\n   Copyright (C) 2014 Intel Corporation\n\n   This program is free software; you can redistribute it and/or modify\n   it under the terms of the GNU General Public License version 2 as\n   published by the Free Software Foundation;\n\n   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.\n   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY\n   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES\n   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,\n   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS\n   SOFTWARE IS DISCLAIMED.\n*/\n\n#include <linux/sched/signal.h>\n\n#include <net/bluetooth/bluetooth.h>\n#include <net/bluetooth/hci_core.h>\n#include <net/bluetooth/mgmt.h>\n\n#include \"smp.h\"\n#include \"hci_request.h\"\n#include \"msft.h\"\n\n#define HCI_REQ_DONE\t  0\n#define HCI_REQ_PEND\t  1\n#define HCI_REQ_CANCELED  2\n\nvoid hci_req_init(struct hci_request *req, struct hci_dev *hdev)\n{\n\tskb_queue_head_init(&req->cmd_q);\n\treq->hdev = hdev;\n\treq->err = 0;\n}\n\nvoid hci_req_purge(struct hci_request *req)\n{\n\tskb_queue_purge(&req->cmd_q);\n}\n\nbool hci_req_status_pend(struct hci_dev *hdev)\n{\n\treturn hdev->req_status == HCI_REQ_PEND;\n}\n\nstatic int req_run(struct hci_request *req, hci_req_complete_t complete,\n\t\t   hci_req_complete_skb_t complete_skb)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct sk_buff *skb;\n\tunsigned long flags;\n\n\tbt_dev_dbg(hdev, \"length %u\", skb_queue_len(&req->cmd_q));\n\n\t/* If an error occurred during request building, remove all HCI\n\t * commands queued on the HCI request queue.\n\t */\n\tif (req->err) {\n\t\tskb_queue_purge(&req->cmd_q);\n\t\treturn req->err;\n\t}\n\n\t/* Do not allow empty requests */\n\tif (skb_queue_empty(&req->cmd_q))\n\t\treturn -ENODATA;\n\n\tskb = skb_peek_tail(&req->cmd_q);\n\tif (complete) {\n\t\tbt_cb(skb)->hci.req_complete = complete;\n\t} else if (complete_skb) {\n\t\tbt_cb(skb)->hci.req_complete_skb = complete_skb;\n\t\tbt_cb(skb)->hci.req_flags |= HCI_REQ_SKB;\n\t}\n\n\tspin_lock_irqsave(&hdev->cmd_q.lock, flags);\n\tskb_queue_splice_tail(&req->cmd_q, &hdev->cmd_q);\n\tspin_unlock_irqrestore(&hdev->cmd_q.lock, flags);\n\n\tqueue_work(hdev->workqueue, &hdev->cmd_work);\n\n\treturn 0;\n}\n\nint hci_req_run(struct hci_request *req, hci_req_complete_t complete)\n{\n\treturn req_run(req, complete, NULL);\n}\n\nint hci_req_run_skb(struct hci_request *req, hci_req_complete_skb_t complete)\n{\n\treturn req_run(req, NULL, complete);\n}\n\nstatic void hci_req_sync_complete(struct hci_dev *hdev, u8 result, u16 opcode,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tbt_dev_dbg(hdev, \"result 0x%2.2x\", result);\n\n\tif (hdev->req_status == HCI_REQ_PEND) {\n\t\thdev->req_result = result;\n\t\thdev->req_status = HCI_REQ_DONE;\n\t\tif (skb)\n\t\t\thdev->req_skb = skb_get(skb);\n\t\twake_up_interruptible(&hdev->req_wait_q);\n\t}\n}\n\nvoid hci_req_sync_cancel(struct hci_dev *hdev, int err)\n{\n\tbt_dev_dbg(hdev, \"err 0x%2.2x\", err);\n\n\tif (hdev->req_status == HCI_REQ_PEND) {\n\t\thdev->req_result = err;\n\t\thdev->req_status = HCI_REQ_CANCELED;\n\t\twake_up_interruptible(&hdev->req_wait_q);\n\t}\n}\n\nstruct sk_buff *__hci_cmd_sync_ev(struct hci_dev *hdev, u16 opcode, u32 plen,\n\t\t\t\t  const void *param, u8 event, u32 timeout)\n{\n\tstruct hci_request req;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\thci_req_init(&req, hdev);\n\n\thci_req_add_ev(&req, opcode, plen, param, event);\n\n\thdev->req_status = HCI_REQ_PEND;\n\n\terr = hci_req_run_skb(&req, hci_req_sync_complete);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\terr = wait_event_interruptible_timeout(hdev->req_wait_q,\n\t\t\thdev->req_status != HCI_REQ_PEND, timeout);\n\n\tif (err == -ERESTARTSYS)\n\t\treturn ERR_PTR(-EINTR);\n\n\tswitch (hdev->req_status) {\n\tcase HCI_REQ_DONE:\n\t\terr = -bt_to_errno(hdev->req_result);\n\t\tbreak;\n\n\tcase HCI_REQ_CANCELED:\n\t\terr = -hdev->req_result;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ETIMEDOUT;\n\t\tbreak;\n\t}\n\n\thdev->req_status = hdev->req_result = 0;\n\tskb = hdev->req_skb;\n\thdev->req_skb = NULL;\n\n\tbt_dev_dbg(hdev, \"end: err %d\", err);\n\n\tif (err < 0) {\n\t\tkfree_skb(skb);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tif (!skb)\n\t\treturn ERR_PTR(-ENODATA);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(__hci_cmd_sync_ev);\n\nstruct sk_buff *__hci_cmd_sync(struct hci_dev *hdev, u16 opcode, u32 plen,\n\t\t\t       const void *param, u32 timeout)\n{\n\treturn __hci_cmd_sync_ev(hdev, opcode, plen, param, 0, timeout);\n}\nEXPORT_SYMBOL(__hci_cmd_sync);\n\n/* Execute request and wait for completion. */\nint __hci_req_sync(struct hci_dev *hdev, int (*func)(struct hci_request *req,\n\t\t\t\t\t\t     unsigned long opt),\n\t\t   unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tstruct hci_request req;\n\tint err = 0;\n\n\tbt_dev_dbg(hdev, \"start\");\n\n\thci_req_init(&req, hdev);\n\n\thdev->req_status = HCI_REQ_PEND;\n\n\terr = func(&req, opt);\n\tif (err) {\n\t\tif (hci_status)\n\t\t\t*hci_status = HCI_ERROR_UNSPECIFIED;\n\t\treturn err;\n\t}\n\n\terr = hci_req_run_skb(&req, hci_req_sync_complete);\n\tif (err < 0) {\n\t\thdev->req_status = 0;\n\n\t\t/* ENODATA means the HCI request command queue is empty.\n\t\t * This can happen when a request with conditionals doesn't\n\t\t * trigger any commands to be sent. This is normal behavior\n\t\t * and should not trigger an error return.\n\t\t */\n\t\tif (err == -ENODATA) {\n\t\t\tif (hci_status)\n\t\t\t\t*hci_status = 0;\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (hci_status)\n\t\t\t*hci_status = HCI_ERROR_UNSPECIFIED;\n\n\t\treturn err;\n\t}\n\n\terr = wait_event_interruptible_timeout(hdev->req_wait_q,\n\t\t\thdev->req_status != HCI_REQ_PEND, timeout);\n\n\tif (err == -ERESTARTSYS)\n\t\treturn -EINTR;\n\n\tswitch (hdev->req_status) {\n\tcase HCI_REQ_DONE:\n\t\terr = -bt_to_errno(hdev->req_result);\n\t\tif (hci_status)\n\t\t\t*hci_status = hdev->req_result;\n\t\tbreak;\n\n\tcase HCI_REQ_CANCELED:\n\t\terr = -hdev->req_result;\n\t\tif (hci_status)\n\t\t\t*hci_status = HCI_ERROR_UNSPECIFIED;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ETIMEDOUT;\n\t\tif (hci_status)\n\t\t\t*hci_status = HCI_ERROR_UNSPECIFIED;\n\t\tbreak;\n\t}\n\n\tkfree_skb(hdev->req_skb);\n\thdev->req_skb = NULL;\n\thdev->req_status = hdev->req_result = 0;\n\n\tbt_dev_dbg(hdev, \"end: err %d\", err);\n\n\treturn err;\n}\n\nint hci_req_sync(struct hci_dev *hdev, int (*req)(struct hci_request *req,\n\t\t\t\t\t\t  unsigned long opt),\n\t\t unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tint ret;\n\n\tif (!test_bit(HCI_UP, &hdev->flags))\n\t\treturn -ENETDOWN;\n\n\t/* Serialize all requests */\n\thci_req_sync_lock(hdev);\n\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n\thci_req_sync_unlock(hdev);\n\n\treturn ret;\n}\n\nstruct sk_buff *hci_prepare_cmd(struct hci_dev *hdev, u16 opcode, u32 plen,\n\t\t\t\tconst void *param)\n{\n\tint len = HCI_COMMAND_HDR_SIZE + plen;\n\tstruct hci_command_hdr *hdr;\n\tstruct sk_buff *skb;\n\n\tskb = bt_skb_alloc(len, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\thdr = skb_put(skb, HCI_COMMAND_HDR_SIZE);\n\thdr->opcode = cpu_to_le16(opcode);\n\thdr->plen   = plen;\n\n\tif (plen)\n\t\tskb_put_data(skb, param, plen);\n\n\tbt_dev_dbg(hdev, \"skb len %d\", skb->len);\n\n\thci_skb_pkt_type(skb) = HCI_COMMAND_PKT;\n\thci_skb_opcode(skb) = opcode;\n\n\treturn skb;\n}\n\n/* Queue a command to an asynchronous HCI request */\nvoid hci_req_add_ev(struct hci_request *req, u16 opcode, u32 plen,\n\t\t    const void *param, u8 event)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct sk_buff *skb;\n\n\tbt_dev_dbg(hdev, \"opcode 0x%4.4x plen %d\", opcode, plen);\n\n\t/* If an error occurred during request building, there is no point in\n\t * queueing the HCI command. We can simply return.\n\t */\n\tif (req->err)\n\t\treturn;\n\n\tskb = hci_prepare_cmd(hdev, opcode, plen, param);\n\tif (!skb) {\n\t\tbt_dev_err(hdev, \"no memory for command (opcode 0x%4.4x)\",\n\t\t\t   opcode);\n\t\treq->err = -ENOMEM;\n\t\treturn;\n\t}\n\n\tif (skb_queue_empty(&req->cmd_q))\n\t\tbt_cb(skb)->hci.req_flags |= HCI_REQ_START;\n\n\tbt_cb(skb)->hci.req_event = event;\n\n\tskb_queue_tail(&req->cmd_q, skb);\n}\n\nvoid hci_req_add(struct hci_request *req, u16 opcode, u32 plen,\n\t\t const void *param)\n{\n\thci_req_add_ev(req, opcode, plen, param, 0);\n}\n\nvoid __hci_req_write_fast_connectable(struct hci_request *req, bool enable)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_write_page_scan_activity acp;\n\tu8 type;\n\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\treturn;\n\n\tif (hdev->hci_ver < BLUETOOTH_VER_1_2)\n\t\treturn;\n\n\tif (enable) {\n\t\ttype = PAGE_SCAN_TYPE_INTERLACED;\n\n\t\t/* 160 msec page scan interval */\n\t\tacp.interval = cpu_to_le16(0x0100);\n\t} else {\n\t\ttype = hdev->def_page_scan_type;\n\t\tacp.interval = cpu_to_le16(hdev->def_page_scan_int);\n\t}\n\n\tacp.window = cpu_to_le16(hdev->def_page_scan_window);\n\n\tif (__cpu_to_le16(hdev->page_scan_interval) != acp.interval ||\n\t    __cpu_to_le16(hdev->page_scan_window) != acp.window)\n\t\thci_req_add(req, HCI_OP_WRITE_PAGE_SCAN_ACTIVITY,\n\t\t\t    sizeof(acp), &acp);\n\n\tif (hdev->page_scan_type != type)\n\t\thci_req_add(req, HCI_OP_WRITE_PAGE_SCAN_TYPE, 1, &type);\n}\n\nstatic void start_interleave_scan(struct hci_dev *hdev)\n{\n\thdev->interleave_scan_state = INTERLEAVE_SCAN_NO_FILTER;\n\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t   &hdev->interleave_scan, 0);\n}\n\nstatic bool is_interleave_scanning(struct hci_dev *hdev)\n{\n\treturn hdev->interleave_scan_state != INTERLEAVE_SCAN_NONE;\n}\n\nstatic void cancel_interleave_scan(struct hci_dev *hdev)\n{\n\tbt_dev_dbg(hdev, \"cancelling interleave scan\");\n\n\tcancel_delayed_work_sync(&hdev->interleave_scan);\n\n\thdev->interleave_scan_state = INTERLEAVE_SCAN_NONE;\n}\n\n/* Return true if interleave_scan wasn't started until exiting this function,\n * otherwise, return false\n */\nstatic bool __hci_update_interleaved_scan(struct hci_dev *hdev)\n{\n\t/* Do interleaved scan only if all of the following are true:\n\t * - There is at least one ADV monitor\n\t * - At least one pending LE connection or one device to be scanned for\n\t * - Monitor offloading is not supported\n\t * If so, we should alternate between allowlist scan and one without\n\t * any filters to save power.\n\t */\n\tbool use_interleaving = hci_is_adv_monitoring(hdev) &&\n\t\t\t\t!(list_empty(&hdev->pend_le_conns) &&\n\t\t\t\t  list_empty(&hdev->pend_le_reports)) &&\n\t\t\t\thci_get_adv_monitor_offload_ext(hdev) ==\n\t\t\t\t    HCI_ADV_MONITOR_EXT_NONE;\n\tbool is_interleaving = is_interleave_scanning(hdev);\n\n\tif (use_interleaving && !is_interleaving) {\n\t\tstart_interleave_scan(hdev);\n\t\tbt_dev_dbg(hdev, \"starting interleave scan\");\n\t\treturn true;\n\t}\n\n\tif (!use_interleaving && is_interleaving)\n\t\tcancel_interleave_scan(hdev);\n\n\treturn false;\n}\n\n/* This function controls the background scanning based on hdev->pend_le_conns\n * list. If there are pending LE connection we start the background scanning,\n * otherwise we stop it.\n *\n * This function requires the caller holds hdev->lock.\n */\nstatic void __hci_update_background_scan(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\tif (!test_bit(HCI_UP, &hdev->flags) ||\n\t    test_bit(HCI_INIT, &hdev->flags) ||\n\t    hci_dev_test_flag(hdev, HCI_SETUP) ||\n\t    hci_dev_test_flag(hdev, HCI_CONFIG) ||\n\t    hci_dev_test_flag(hdev, HCI_AUTO_OFF) ||\n\t    hci_dev_test_flag(hdev, HCI_UNREGISTER))\n\t\treturn;\n\n\t/* No point in doing scanning if LE support hasn't been enabled */\n\tif (!hci_dev_test_flag(hdev, HCI_LE_ENABLED))\n\t\treturn;\n\n\t/* If discovery is active don't interfere with it */\n\tif (hdev->discovery.state != DISCOVERY_STOPPED)\n\t\treturn;\n\n\t/* Reset RSSI and UUID filters when starting background scanning\n\t * since these filters are meant for service discovery only.\n\t *\n\t * The Start Discovery and Start Service Discovery operations\n\t * ensure to set proper values for RSSI threshold and UUID\n\t * filter list. So it is safe to just reset them here.\n\t */\n\thci_discovery_filter_clear(hdev);\n\n\tbt_dev_dbg(hdev, \"ADV monitoring is %s\",\n\t\t   hci_is_adv_monitoring(hdev) ? \"on\" : \"off\");\n\n\tif (list_empty(&hdev->pend_le_conns) &&\n\t    list_empty(&hdev->pend_le_reports) &&\n\t    !hci_is_adv_monitoring(hdev)) {\n\t\t/* If there is no pending LE connections or devices\n\t\t * to be scanned for or no ADV monitors, we should stop the\n\t\t * background scanning.\n\t\t */\n\n\t\t/* If controller is not scanning we are done. */\n\t\tif (!hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\t\treturn;\n\n\t\thci_req_add_le_scan_disable(req, false);\n\n\t\tbt_dev_dbg(hdev, \"stopping background scanning\");\n\t} else {\n\t\t/* If there is at least one pending LE connection, we should\n\t\t * keep the background scan running.\n\t\t */\n\n\t\t/* If controller is connecting, we should not start scanning\n\t\t * since some controllers are not able to scan and connect at\n\t\t * the same time.\n\t\t */\n\t\tif (hci_lookup_le_connect(hdev))\n\t\t\treturn;\n\n\t\t/* If controller is currently scanning, we stop it to ensure we\n\t\t * don't miss any advertising (due to duplicates filter).\n\t\t */\n\t\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\t\thci_req_add_le_scan_disable(req, false);\n\n\t\thci_req_add_le_passive_scan(req);\n\t\tbt_dev_dbg(hdev, \"starting background scanning\");\n\t}\n}\n\nvoid __hci_req_update_name(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_write_local_name cp;\n\n\tmemcpy(cp.name, hdev->dev_name, sizeof(cp.name));\n\n\thci_req_add(req, HCI_OP_WRITE_LOCAL_NAME, sizeof(cp), &cp);\n}\n\n#define PNP_INFO_SVCLASS_ID\t\t0x1200\n\nstatic u8 *create_uuid16_list(struct hci_dev *hdev, u8 *data, ptrdiff_t len)\n{\n\tu8 *ptr = data, *uuids_start = NULL;\n\tstruct bt_uuid *uuid;\n\n\tif (len < 4)\n\t\treturn ptr;\n\n\tlist_for_each_entry(uuid, &hdev->uuids, list) {\n\t\tu16 uuid16;\n\n\t\tif (uuid->size != 16)\n\t\t\tcontinue;\n\n\t\tuuid16 = get_unaligned_le16(&uuid->uuid[12]);\n\t\tif (uuid16 < 0x1100)\n\t\t\tcontinue;\n\n\t\tif (uuid16 == PNP_INFO_SVCLASS_ID)\n\t\t\tcontinue;\n\n\t\tif (!uuids_start) {\n\t\t\tuuids_start = ptr;\n\t\t\tuuids_start[0] = 1;\n\t\t\tuuids_start[1] = EIR_UUID16_ALL;\n\t\t\tptr += 2;\n\t\t}\n\n\t\t/* Stop if not enough space to put next UUID */\n\t\tif ((ptr - data) + sizeof(u16) > len) {\n\t\t\tuuids_start[1] = EIR_UUID16_SOME;\n\t\t\tbreak;\n\t\t}\n\n\t\t*ptr++ = (uuid16 & 0x00ff);\n\t\t*ptr++ = (uuid16 & 0xff00) >> 8;\n\t\tuuids_start[0] += sizeof(uuid16);\n\t}\n\n\treturn ptr;\n}\n\nstatic u8 *create_uuid32_list(struct hci_dev *hdev, u8 *data, ptrdiff_t len)\n{\n\tu8 *ptr = data, *uuids_start = NULL;\n\tstruct bt_uuid *uuid;\n\n\tif (len < 6)\n\t\treturn ptr;\n\n\tlist_for_each_entry(uuid, &hdev->uuids, list) {\n\t\tif (uuid->size != 32)\n\t\t\tcontinue;\n\n\t\tif (!uuids_start) {\n\t\t\tuuids_start = ptr;\n\t\t\tuuids_start[0] = 1;\n\t\t\tuuids_start[1] = EIR_UUID32_ALL;\n\t\t\tptr += 2;\n\t\t}\n\n\t\t/* Stop if not enough space to put next UUID */\n\t\tif ((ptr - data) + sizeof(u32) > len) {\n\t\t\tuuids_start[1] = EIR_UUID32_SOME;\n\t\t\tbreak;\n\t\t}\n\n\t\tmemcpy(ptr, &uuid->uuid[12], sizeof(u32));\n\t\tptr += sizeof(u32);\n\t\tuuids_start[0] += sizeof(u32);\n\t}\n\n\treturn ptr;\n}\n\nstatic u8 *create_uuid128_list(struct hci_dev *hdev, u8 *data, ptrdiff_t len)\n{\n\tu8 *ptr = data, *uuids_start = NULL;\n\tstruct bt_uuid *uuid;\n\n\tif (len < 18)\n\t\treturn ptr;\n\n\tlist_for_each_entry(uuid, &hdev->uuids, list) {\n\t\tif (uuid->size != 128)\n\t\t\tcontinue;\n\n\t\tif (!uuids_start) {\n\t\t\tuuids_start = ptr;\n\t\t\tuuids_start[0] = 1;\n\t\t\tuuids_start[1] = EIR_UUID128_ALL;\n\t\t\tptr += 2;\n\t\t}\n\n\t\t/* Stop if not enough space to put next UUID */\n\t\tif ((ptr - data) + 16 > len) {\n\t\t\tuuids_start[1] = EIR_UUID128_SOME;\n\t\t\tbreak;\n\t\t}\n\n\t\tmemcpy(ptr, uuid->uuid, 16);\n\t\tptr += 16;\n\t\tuuids_start[0] += 16;\n\t}\n\n\treturn ptr;\n}\n\nstatic void create_eir(struct hci_dev *hdev, u8 *data)\n{\n\tu8 *ptr = data;\n\tsize_t name_len;\n\n\tname_len = strlen(hdev->dev_name);\n\n\tif (name_len > 0) {\n\t\t/* EIR Data type */\n\t\tif (name_len > 48) {\n\t\t\tname_len = 48;\n\t\t\tptr[1] = EIR_NAME_SHORT;\n\t\t} else\n\t\t\tptr[1] = EIR_NAME_COMPLETE;\n\n\t\t/* EIR Data length */\n\t\tptr[0] = name_len + 1;\n\n\t\tmemcpy(ptr + 2, hdev->dev_name, name_len);\n\n\t\tptr += (name_len + 2);\n\t}\n\n\tif (hdev->inq_tx_power != HCI_TX_POWER_INVALID) {\n\t\tptr[0] = 2;\n\t\tptr[1] = EIR_TX_POWER;\n\t\tptr[2] = (u8) hdev->inq_tx_power;\n\n\t\tptr += 3;\n\t}\n\n\tif (hdev->devid_source > 0) {\n\t\tptr[0] = 9;\n\t\tptr[1] = EIR_DEVICE_ID;\n\n\t\tput_unaligned_le16(hdev->devid_source, ptr + 2);\n\t\tput_unaligned_le16(hdev->devid_vendor, ptr + 4);\n\t\tput_unaligned_le16(hdev->devid_product, ptr + 6);\n\t\tput_unaligned_le16(hdev->devid_version, ptr + 8);\n\n\t\tptr += 10;\n\t}\n\n\tptr = create_uuid16_list(hdev, ptr, HCI_MAX_EIR_LENGTH - (ptr - data));\n\tptr = create_uuid32_list(hdev, ptr, HCI_MAX_EIR_LENGTH - (ptr - data));\n\tptr = create_uuid128_list(hdev, ptr, HCI_MAX_EIR_LENGTH - (ptr - data));\n}\n\nvoid __hci_req_update_eir(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_write_eir cp;\n\n\tif (!hdev_is_powered(hdev))\n\t\treturn;\n\n\tif (!lmp_ext_inq_capable(hdev))\n\t\treturn;\n\n\tif (!hci_dev_test_flag(hdev, HCI_SSP_ENABLED))\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_SERVICE_CACHE))\n\t\treturn;\n\n\tmemset(&cp, 0, sizeof(cp));\n\n\tcreate_eir(hdev, cp.data);\n\n\tif (memcmp(cp.data, hdev->eir, sizeof(cp.data)) == 0)\n\t\treturn;\n\n\tmemcpy(hdev->eir, cp.data, sizeof(cp.data));\n\n\thci_req_add(req, HCI_OP_WRITE_EIR, sizeof(cp), &cp);\n}\n\nvoid hci_req_add_le_scan_disable(struct hci_request *req, bool rpa_le_conn)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\tif (hdev->scanning_paused) {\n\t\tbt_dev_dbg(hdev, \"Scanning is paused for suspend\");\n\t\treturn;\n\t}\n\n\tif (hdev->suspended)\n\t\tset_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks);\n\n\tif (use_ext_scan(hdev)) {\n\t\tstruct hci_cp_le_set_ext_scan_enable cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\t\tcp.enable = LE_SCAN_DISABLE;\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_ENABLE, sizeof(cp),\n\t\t\t    &cp);\n\t} else {\n\t\tstruct hci_cp_le_set_scan_enable cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\t\tcp.enable = LE_SCAN_DISABLE;\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_ENABLE, sizeof(cp), &cp);\n\t}\n\n\t/* Disable address resolution */\n\tif (use_ll_privacy(hdev) &&\n\t    hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY) &&\n\t    hci_dev_test_flag(hdev, HCI_LL_RPA_RESOLUTION) && !rpa_le_conn) {\n\t\t__u8 enable = 0x00;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_ADDR_RESOLV_ENABLE, 1, &enable);\n\t}\n}\n\nstatic void del_from_white_list(struct hci_request *req, bdaddr_t *bdaddr,\n\t\t\t\tu8 bdaddr_type)\n{\n\tstruct hci_cp_le_del_from_white_list cp;\n\n\tcp.bdaddr_type = bdaddr_type;\n\tbacpy(&cp.bdaddr, bdaddr);\n\n\tbt_dev_dbg(req->hdev, \"Remove %pMR (0x%x) from whitelist\", &cp.bdaddr,\n\t\t   cp.bdaddr_type);\n\thci_req_add(req, HCI_OP_LE_DEL_FROM_WHITE_LIST, sizeof(cp), &cp);\n\n\tif (use_ll_privacy(req->hdev) &&\n\t    hci_dev_test_flag(req->hdev, HCI_ENABLE_LL_PRIVACY)) {\n\t\tstruct smp_irk *irk;\n\n\t\tirk = hci_find_irk_by_addr(req->hdev, bdaddr, bdaddr_type);\n\t\tif (irk) {\n\t\t\tstruct hci_cp_le_del_from_resolv_list cp;\n\n\t\t\tcp.bdaddr_type = bdaddr_type;\n\t\t\tbacpy(&cp.bdaddr, bdaddr);\n\n\t\t\thci_req_add(req, HCI_OP_LE_DEL_FROM_RESOLV_LIST,\n\t\t\t\t    sizeof(cp), &cp);\n\t\t}\n\t}\n}\n\n/* Adds connection to white list if needed. On error, returns -1. */\nstatic int add_to_white_list(struct hci_request *req,\n\t\t\t     struct hci_conn_params *params, u8 *num_entries,\n\t\t\t     bool allow_rpa)\n{\n\tstruct hci_cp_le_add_to_white_list cp;\n\tstruct hci_dev *hdev = req->hdev;\n\n\t/* Already in white list */\n\tif (hci_bdaddr_list_lookup(&hdev->le_white_list, &params->addr,\n\t\t\t\t   params->addr_type))\n\t\treturn 0;\n\n\t/* Select filter policy to accept all advertising */\n\tif (*num_entries >= hdev->le_white_list_size)\n\t\treturn -1;\n\n\t/* White list can not be used with RPAs */\n\tif (!allow_rpa &&\n\t    !hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY) &&\n\t    hci_find_irk_by_addr(hdev, &params->addr, params->addr_type)) {\n\t\treturn -1;\n\t}\n\n\t/* During suspend, only wakeable devices can be in whitelist */\n\tif (hdev->suspended && !hci_conn_test_flag(HCI_CONN_FLAG_REMOTE_WAKEUP,\n\t\t\t\t\t\t   params->current_flags))\n\t\treturn 0;\n\n\t*num_entries += 1;\n\tcp.bdaddr_type = params->addr_type;\n\tbacpy(&cp.bdaddr, &params->addr);\n\n\tbt_dev_dbg(hdev, \"Add %pMR (0x%x) to whitelist\", &cp.bdaddr,\n\t\t   cp.bdaddr_type);\n\thci_req_add(req, HCI_OP_LE_ADD_TO_WHITE_LIST, sizeof(cp), &cp);\n\n\tif (use_ll_privacy(hdev) &&\n\t    hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY)) {\n\t\tstruct smp_irk *irk;\n\n\t\tirk = hci_find_irk_by_addr(hdev, &params->addr,\n\t\t\t\t\t   params->addr_type);\n\t\tif (irk) {\n\t\t\tstruct hci_cp_le_add_to_resolv_list cp;\n\n\t\t\tcp.bdaddr_type = params->addr_type;\n\t\t\tbacpy(&cp.bdaddr, &params->addr);\n\t\t\tmemcpy(cp.peer_irk, irk->val, 16);\n\n\t\t\tif (hci_dev_test_flag(hdev, HCI_PRIVACY))\n\t\t\t\tmemcpy(cp.local_irk, hdev->irk, 16);\n\t\t\telse\n\t\t\t\tmemset(cp.local_irk, 0, 16);\n\n\t\t\thci_req_add(req, HCI_OP_LE_ADD_TO_RESOLV_LIST,\n\t\t\t\t    sizeof(cp), &cp);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic u8 update_white_list(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_conn_params *params;\n\tstruct bdaddr_list *b;\n\tu8 num_entries = 0;\n\tbool pend_conn, pend_report;\n\t/* We allow whitelisting even with RPAs in suspend. In the worst case,\n\t * we won't be able to wake from devices that use the privacy1.2\n\t * features. Additionally, once we support privacy1.2 and IRK\n\t * offloading, we can update this to also check for those conditions.\n\t */\n\tbool allow_rpa = hdev->suspended;\n\n\t/* Go through the current white list programmed into the\n\t * controller one by one and check if that address is still\n\t * in the list of pending connections or list of devices to\n\t * report. If not present in either list, then queue the\n\t * command to remove it from the controller.\n\t */\n\tlist_for_each_entry(b, &hdev->le_white_list, list) {\n\t\tpend_conn = hci_pend_le_action_lookup(&hdev->pend_le_conns,\n\t\t\t\t\t\t      &b->bdaddr,\n\t\t\t\t\t\t      b->bdaddr_type);\n\t\tpend_report = hci_pend_le_action_lookup(&hdev->pend_le_reports,\n\t\t\t\t\t\t\t&b->bdaddr,\n\t\t\t\t\t\t\tb->bdaddr_type);\n\n\t\t/* If the device is not likely to connect or report,\n\t\t * remove it from the whitelist.\n\t\t */\n\t\tif (!pend_conn && !pend_report) {\n\t\t\tdel_from_white_list(req, &b->bdaddr, b->bdaddr_type);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* White list can not be used with RPAs */\n\t\tif (!allow_rpa &&\n\t\t    !hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY) &&\n\t\t    hci_find_irk_by_addr(hdev, &b->bdaddr, b->bdaddr_type)) {\n\t\t\treturn 0x00;\n\t\t}\n\n\t\tnum_entries++;\n\t}\n\n\t/* Since all no longer valid white list entries have been\n\t * removed, walk through the list of pending connections\n\t * and ensure that any new device gets programmed into\n\t * the controller.\n\t *\n\t * If the list of the devices is larger than the list of\n\t * available white list entries in the controller, then\n\t * just abort and return filer policy value to not use the\n\t * white list.\n\t */\n\tlist_for_each_entry(params, &hdev->pend_le_conns, action) {\n\t\tif (add_to_white_list(req, params, &num_entries, allow_rpa))\n\t\t\treturn 0x00;\n\t}\n\n\t/* After adding all new pending connections, walk through\n\t * the list of pending reports and also add these to the\n\t * white list if there is still space. Abort if space runs out.\n\t */\n\tlist_for_each_entry(params, &hdev->pend_le_reports, action) {\n\t\tif (add_to_white_list(req, params, &num_entries, allow_rpa))\n\t\t\treturn 0x00;\n\t}\n\n\t/* Use the allowlist unless the following conditions are all true:\n\t * - We are not currently suspending\n\t * - There are 1 or more ADV monitors registered and it's not offloaded\n\t * - Interleaved scanning is not currently using the allowlist\n\t */\n\tif (!idr_is_empty(&hdev->adv_monitors_idr) && !hdev->suspended &&\n\t    hci_get_adv_monitor_offload_ext(hdev) == HCI_ADV_MONITOR_EXT_NONE &&\n\t    hdev->interleave_scan_state != INTERLEAVE_SCAN_ALLOWLIST)\n\t\treturn 0x00;\n\n\t/* Select filter policy to use white list */\n\treturn 0x01;\n}\n\nstatic bool scan_use_rpa(struct hci_dev *hdev)\n{\n\treturn hci_dev_test_flag(hdev, HCI_PRIVACY);\n}\n\nstatic void hci_req_start_scan(struct hci_request *req, u8 type, u16 interval,\n\t\t\t       u16 window, u8 own_addr_type, u8 filter_policy,\n\t\t\t       bool addr_resolv)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\tif (hdev->scanning_paused) {\n\t\tbt_dev_dbg(hdev, \"Scanning is paused for suspend\");\n\t\treturn;\n\t}\n\n\tif (use_ll_privacy(hdev) &&\n\t    hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY) &&\n\t    addr_resolv) {\n\t\tu8 enable = 0x01;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_ADDR_RESOLV_ENABLE, 1, &enable);\n\t}\n\n\t/* Use ext scanning if set ext scan param and ext scan enable is\n\t * supported\n\t */\n\tif (use_ext_scan(hdev)) {\n\t\tstruct hci_cp_le_set_ext_scan_params *ext_param_cp;\n\t\tstruct hci_cp_le_set_ext_scan_enable ext_enable_cp;\n\t\tstruct hci_cp_le_scan_phy_params *phy_params;\n\t\tu8 data[sizeof(*ext_param_cp) + sizeof(*phy_params) * 2];\n\t\tu32 plen;\n\n\t\text_param_cp = (void *)data;\n\t\tphy_params = (void *)ext_param_cp->data;\n\n\t\tmemset(ext_param_cp, 0, sizeof(*ext_param_cp));\n\t\text_param_cp->own_addr_type = own_addr_type;\n\t\text_param_cp->filter_policy = filter_policy;\n\n\t\tplen = sizeof(*ext_param_cp);\n\n\t\tif (scan_1m(hdev) || scan_2m(hdev)) {\n\t\t\text_param_cp->scanning_phys |= LE_SCAN_PHY_1M;\n\n\t\t\tmemset(phy_params, 0, sizeof(*phy_params));\n\t\t\tphy_params->type = type;\n\t\t\tphy_params->interval = cpu_to_le16(interval);\n\t\t\tphy_params->window = cpu_to_le16(window);\n\n\t\t\tplen += sizeof(*phy_params);\n\t\t\tphy_params++;\n\t\t}\n\n\t\tif (scan_coded(hdev)) {\n\t\t\text_param_cp->scanning_phys |= LE_SCAN_PHY_CODED;\n\n\t\t\tmemset(phy_params, 0, sizeof(*phy_params));\n\t\t\tphy_params->type = type;\n\t\t\tphy_params->interval = cpu_to_le16(interval);\n\t\t\tphy_params->window = cpu_to_le16(window);\n\n\t\t\tplen += sizeof(*phy_params);\n\t\t\tphy_params++;\n\t\t}\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_PARAMS,\n\t\t\t    plen, ext_param_cp);\n\n\t\tmemset(&ext_enable_cp, 0, sizeof(ext_enable_cp));\n\t\text_enable_cp.enable = LE_SCAN_ENABLE;\n\t\text_enable_cp.filter_dup = LE_SCAN_FILTER_DUP_ENABLE;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_ENABLE,\n\t\t\t    sizeof(ext_enable_cp), &ext_enable_cp);\n\t} else {\n\t\tstruct hci_cp_le_set_scan_param param_cp;\n\t\tstruct hci_cp_le_set_scan_enable enable_cp;\n\n\t\tmemset(&param_cp, 0, sizeof(param_cp));\n\t\tparam_cp.type = type;\n\t\tparam_cp.interval = cpu_to_le16(interval);\n\t\tparam_cp.window = cpu_to_le16(window);\n\t\tparam_cp.own_address_type = own_addr_type;\n\t\tparam_cp.filter_policy = filter_policy;\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_PARAM, sizeof(param_cp),\n\t\t\t    &param_cp);\n\n\t\tmemset(&enable_cp, 0, sizeof(enable_cp));\n\t\tenable_cp.enable = LE_SCAN_ENABLE;\n\t\tenable_cp.filter_dup = LE_SCAN_FILTER_DUP_ENABLE;\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_ENABLE, sizeof(enable_cp),\n\t\t\t    &enable_cp);\n\t}\n}\n\n/* Returns true if an le connection is in the scanning state */\nstatic inline bool hci_is_le_conn_scanning(struct hci_dev *hdev)\n{\n\tstruct hci_conn_hash *h = &hdev->conn_hash;\n\tstruct hci_conn  *c;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(c, &h->list, list) {\n\t\tif (c->type == LE_LINK && c->state == BT_CONNECT &&\n\t\t    test_bit(HCI_CONN_SCANNING, &c->flags)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\n\treturn false;\n}\n\n/* Ensure to call hci_req_add_le_scan_disable() first to disable the\n * controller based address resolution to be able to reconfigure\n * resolving list.\n */\nvoid hci_req_add_le_passive_scan(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 own_addr_type;\n\tu8 filter_policy;\n\tu16 window, interval;\n\t/* Background scanning should run with address resolution */\n\tbool addr_resolv = true;\n\n\tif (hdev->scanning_paused) {\n\t\tbt_dev_dbg(hdev, \"Scanning is paused for suspend\");\n\t\treturn;\n\t}\n\n\t/* Set require_privacy to false since no SCAN_REQ are send\n\t * during passive scanning. Not using an non-resolvable address\n\t * here is important so that peer devices using direct\n\t * advertising with our address will be correctly reported\n\t * by the controller.\n\t */\n\tif (hci_update_random_address(req, false, scan_use_rpa(hdev),\n\t\t\t\t      &own_addr_type))\n\t\treturn;\n\n\tif (hdev->enable_advmon_interleave_scan &&\n\t    __hci_update_interleaved_scan(hdev))\n\t\treturn;\n\n\tbt_dev_dbg(hdev, \"interleave state %d\", hdev->interleave_scan_state);\n\t/* Adding or removing entries from the white list must\n\t * happen before enabling scanning. The controller does\n\t * not allow white list modification while scanning.\n\t */\n\tfilter_policy = update_white_list(req);\n\n\t/* When the controller is using random resolvable addresses and\n\t * with that having LE privacy enabled, then controllers with\n\t * Extended Scanner Filter Policies support can now enable support\n\t * for handling directed advertising.\n\t *\n\t * So instead of using filter polices 0x00 (no whitelist)\n\t * and 0x01 (whitelist enabled) use the new filter policies\n\t * 0x02 (no whitelist) and 0x03 (whitelist enabled).\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_PRIVACY) &&\n\t    (hdev->le_features[0] & HCI_LE_EXT_SCAN_POLICY))\n\t\tfilter_policy |= 0x02;\n\n\tif (hdev->suspended) {\n\t\twindow = hdev->le_scan_window_suspend;\n\t\tinterval = hdev->le_scan_int_suspend;\n\n\t\tset_bit(SUSPEND_SCAN_ENABLE, hdev->suspend_tasks);\n\t} else if (hci_is_le_conn_scanning(hdev)) {\n\t\twindow = hdev->le_scan_window_connect;\n\t\tinterval = hdev->le_scan_int_connect;\n\t} else if (hci_is_adv_monitoring(hdev)) {\n\t\twindow = hdev->le_scan_window_adv_monitor;\n\t\tinterval = hdev->le_scan_int_adv_monitor;\n\t} else {\n\t\twindow = hdev->le_scan_window;\n\t\tinterval = hdev->le_scan_interval;\n\t}\n\n\tbt_dev_dbg(hdev, \"LE passive scan with whitelist = %d\", filter_policy);\n\thci_req_start_scan(req, LE_SCAN_PASSIVE, interval, window,\n\t\t\t   own_addr_type, filter_policy, addr_resolv);\n}\n\nstatic bool adv_instance_is_scannable(struct hci_dev *hdev, u8 instance)\n{\n\tstruct adv_info *adv_instance;\n\n\t/* Instance 0x00 always set local name */\n\tif (instance == 0x00)\n\t\treturn true;\n\n\tadv_instance = hci_find_adv_instance(hdev, instance);\n\tif (!adv_instance)\n\t\treturn false;\n\n\tif (adv_instance->flags & MGMT_ADV_FLAG_APPEARANCE ||\n\t    adv_instance->flags & MGMT_ADV_FLAG_LOCAL_NAME)\n\t\treturn true;\n\n\treturn adv_instance->scan_rsp_len ? true : false;\n}\n\nstatic void hci_req_clear_event_filter(struct hci_request *req)\n{\n\tstruct hci_cp_set_event_filter f;\n\n\tmemset(&f, 0, sizeof(f));\n\tf.flt_type = HCI_FLT_CLEAR_ALL;\n\thci_req_add(req, HCI_OP_SET_EVENT_FLT, 1, &f);\n\n\t/* Update page scan state (since we may have modified it when setting\n\t * the event filter).\n\t */\n\t__hci_req_update_scan(req);\n}\n\nstatic void hci_req_set_event_filter(struct hci_request *req)\n{\n\tstruct bdaddr_list_with_flags *b;\n\tstruct hci_cp_set_event_filter f;\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 scan = SCAN_DISABLED;\n\n\t/* Always clear event filter when starting */\n\thci_req_clear_event_filter(req);\n\n\tlist_for_each_entry(b, &hdev->whitelist, list) {\n\t\tif (!hci_conn_test_flag(HCI_CONN_FLAG_REMOTE_WAKEUP,\n\t\t\t\t\tb->current_flags))\n\t\t\tcontinue;\n\n\t\tmemset(&f, 0, sizeof(f));\n\t\tbacpy(&f.addr_conn_flt.bdaddr, &b->bdaddr);\n\t\tf.flt_type = HCI_FLT_CONN_SETUP;\n\t\tf.cond_type = HCI_CONN_SETUP_ALLOW_BDADDR;\n\t\tf.addr_conn_flt.auto_accept = HCI_CONN_SETUP_AUTO_ON;\n\n\t\tbt_dev_dbg(hdev, \"Adding event filters for %pMR\", &b->bdaddr);\n\t\thci_req_add(req, HCI_OP_SET_EVENT_FLT, sizeof(f), &f);\n\t\tscan = SCAN_PAGE;\n\t}\n\n\tif (scan)\n\t\tset_bit(SUSPEND_SCAN_ENABLE, hdev->suspend_tasks);\n\telse\n\t\tset_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks);\n\n\thci_req_add(req, HCI_OP_WRITE_SCAN_ENABLE, 1, &scan);\n}\n\nstatic void cancel_adv_timeout(struct hci_dev *hdev)\n{\n\tif (hdev->adv_instance_timeout) {\n\t\thdev->adv_instance_timeout = 0;\n\t\tcancel_delayed_work(&hdev->adv_instance_expire);\n\t}\n}\n\n/* This function requires the caller holds hdev->lock */\nvoid __hci_req_pause_adv_instances(struct hci_request *req)\n{\n\tbt_dev_dbg(req->hdev, \"Pausing advertising instances\");\n\n\t/* Call to disable any advertisements active on the controller.\n\t * This will succeed even if no advertisements are configured.\n\t */\n\t__hci_req_disable_advertising(req);\n\n\t/* If we are using software rotation, pause the loop */\n\tif (!ext_adv_capable(req->hdev))\n\t\tcancel_adv_timeout(req->hdev);\n}\n\n/* This function requires the caller holds hdev->lock */\nstatic void __hci_req_resume_adv_instances(struct hci_request *req)\n{\n\tstruct adv_info *adv;\n\n\tbt_dev_dbg(req->hdev, \"Resuming advertising instances\");\n\n\tif (ext_adv_capable(req->hdev)) {\n\t\t/* Call for each tracked instance to be re-enabled */\n\t\tlist_for_each_entry(adv, &req->hdev->adv_instances, list) {\n\t\t\t__hci_req_enable_ext_advertising(req,\n\t\t\t\t\t\t\t adv->instance);\n\t\t}\n\n\t} else {\n\t\t/* Schedule for most recent instance to be restarted and begin\n\t\t * the software rotation loop\n\t\t */\n\t\t__hci_req_schedule_adv_instance(req,\n\t\t\t\t\t\treq->hdev->cur_adv_instance,\n\t\t\t\t\t\ttrue);\n\t}\n}\n\n/* This function requires the caller holds hdev->lock */\nint hci_req_resume_adv_instances(struct hci_dev *hdev)\n{\n\tstruct hci_request req;\n\n\thci_req_init(&req, hdev);\n\t__hci_req_resume_adv_instances(&req);\n\n\treturn hci_req_run(&req, NULL);\n}\n\nstatic void suspend_req_complete(struct hci_dev *hdev, u8 status, u16 opcode)\n{\n\tbt_dev_dbg(hdev, \"Request complete opcode=0x%x, status=0x%x\", opcode,\n\t\t   status);\n\tif (test_bit(SUSPEND_SCAN_ENABLE, hdev->suspend_tasks) ||\n\t    test_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks)) {\n\t\tclear_bit(SUSPEND_SCAN_ENABLE, hdev->suspend_tasks);\n\t\tclear_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks);\n\t\twake_up(&hdev->suspend_wait_q);\n\t}\n\n\tif (test_bit(SUSPEND_SET_ADV_FILTER, hdev->suspend_tasks)) {\n\t\tclear_bit(SUSPEND_SET_ADV_FILTER, hdev->suspend_tasks);\n\t\twake_up(&hdev->suspend_wait_q);\n\t}\n}\n\nstatic void hci_req_add_set_adv_filter_enable(struct hci_request *req,\n\t\t\t\t\t      bool enable)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\tswitch (hci_get_adv_monitor_offload_ext(hdev)) {\n\tcase HCI_ADV_MONITOR_EXT_MSFT:\n\t\tmsft_req_add_set_filter_enable(req, enable);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\t/* No need to block when enabling since it's on resume path */\n\tif (hdev->suspended && !enable)\n\t\tset_bit(SUSPEND_SET_ADV_FILTER, hdev->suspend_tasks);\n}\n\n/* Call with hci_dev_lock */\nvoid hci_req_prepare_suspend(struct hci_dev *hdev, enum suspended_state next)\n{\n\tint old_state;\n\tstruct hci_conn *conn;\n\tstruct hci_request req;\n\tu8 page_scan;\n\tint disconnect_counter;\n\n\tif (next == hdev->suspend_state) {\n\t\tbt_dev_dbg(hdev, \"Same state before and after: %d\", next);\n\t\tgoto done;\n\t}\n\n\thdev->suspend_state = next;\n\thci_req_init(&req, hdev);\n\n\tif (next == BT_SUSPEND_DISCONNECT) {\n\t\t/* Mark device as suspended */\n\t\thdev->suspended = true;\n\n\t\t/* Pause discovery if not already stopped */\n\t\told_state = hdev->discovery.state;\n\t\tif (old_state != DISCOVERY_STOPPED) {\n\t\t\tset_bit(SUSPEND_PAUSE_DISCOVERY, hdev->suspend_tasks);\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_STOPPING);\n\t\t\tqueue_work(hdev->req_workqueue, &hdev->discov_update);\n\t\t}\n\n\t\thdev->discovery_paused = true;\n\t\thdev->discovery_old_state = old_state;\n\n\t\t/* Stop directed advertising */\n\t\told_state = hci_dev_test_flag(hdev, HCI_ADVERTISING);\n\t\tif (old_state) {\n\t\t\tset_bit(SUSPEND_PAUSE_ADVERTISING, hdev->suspend_tasks);\n\t\t\tcancel_delayed_work(&hdev->discov_off);\n\t\t\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t\t\t   &hdev->discov_off, 0);\n\t\t}\n\n\t\t/* Pause other advertisements */\n\t\tif (hdev->adv_instance_cnt)\n\t\t\t__hci_req_pause_adv_instances(&req);\n\n\t\thdev->advertising_paused = true;\n\t\thdev->advertising_old_state = old_state;\n\t\t/* Disable page scan */\n\t\tpage_scan = SCAN_DISABLED;\n\t\thci_req_add(&req, HCI_OP_WRITE_SCAN_ENABLE, 1, &page_scan);\n\n\t\t/* Disable LE passive scan if enabled */\n\t\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN)) {\n\t\t\tcancel_interleave_scan(hdev);\n\t\t\thci_req_add_le_scan_disable(&req, false);\n\t\t}\n\n\t\t/* Disable advertisement filters */\n\t\thci_req_add_set_adv_filter_enable(&req, false);\n\n\t\t/* Mark task needing completion */\n\t\tset_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks);\n\n\t\t/* Prevent disconnects from causing scanning to be re-enabled */\n\t\thdev->scanning_paused = true;\n\n\t\t/* Run commands before disconnecting */\n\t\thci_req_run(&req, suspend_req_complete);\n\n\t\tdisconnect_counter = 0;\n\t\t/* Soft disconnect everything (power off) */\n\t\tlist_for_each_entry(conn, &hdev->conn_hash.list, list) {\n\t\t\thci_disconnect(conn, HCI_ERROR_REMOTE_POWER_OFF);\n\t\t\tdisconnect_counter++;\n\t\t}\n\n\t\tif (disconnect_counter > 0) {\n\t\t\tbt_dev_dbg(hdev,\n\t\t\t\t   \"Had %d disconnects. Will wait on them\",\n\t\t\t\t   disconnect_counter);\n\t\t\tset_bit(SUSPEND_DISCONNECTING, hdev->suspend_tasks);\n\t\t}\n\t} else if (next == BT_SUSPEND_CONFIGURE_WAKE) {\n\t\t/* Unpause to take care of updating scanning params */\n\t\thdev->scanning_paused = false;\n\t\t/* Enable event filter for paired devices */\n\t\thci_req_set_event_filter(&req);\n\t\t/* Enable passive scan at lower duty cycle */\n\t\t__hci_update_background_scan(&req);\n\t\t/* Pause scan changes again. */\n\t\thdev->scanning_paused = true;\n\t\thci_req_run(&req, suspend_req_complete);\n\t} else {\n\t\thdev->suspended = false;\n\t\thdev->scanning_paused = false;\n\n\t\thci_req_clear_event_filter(&req);\n\t\t/* Reset passive/background scanning to normal */\n\t\t__hci_update_background_scan(&req);\n\t\t/* Enable all of the advertisement filters */\n\t\thci_req_add_set_adv_filter_enable(&req, true);\n\n\t\t/* Unpause directed advertising */\n\t\thdev->advertising_paused = false;\n\t\tif (hdev->advertising_old_state) {\n\t\t\tset_bit(SUSPEND_UNPAUSE_ADVERTISING,\n\t\t\t\thdev->suspend_tasks);\n\t\t\thci_dev_set_flag(hdev, HCI_ADVERTISING);\n\t\t\tqueue_work(hdev->req_workqueue,\n\t\t\t\t   &hdev->discoverable_update);\n\t\t\thdev->advertising_old_state = 0;\n\t\t}\n\n\t\t/* Resume other advertisements */\n\t\tif (hdev->adv_instance_cnt)\n\t\t\t__hci_req_resume_adv_instances(&req);\n\n\t\t/* Unpause discovery */\n\t\thdev->discovery_paused = false;\n\t\tif (hdev->discovery_old_state != DISCOVERY_STOPPED &&\n\t\t    hdev->discovery_old_state != DISCOVERY_STOPPING) {\n\t\t\tset_bit(SUSPEND_UNPAUSE_DISCOVERY, hdev->suspend_tasks);\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_STARTING);\n\t\t\tqueue_work(hdev->req_workqueue, &hdev->discov_update);\n\t\t}\n\n\t\thci_req_run(&req, suspend_req_complete);\n\t}\n\n\thdev->suspend_state = next;\n\ndone:\n\tclear_bit(SUSPEND_PREPARE_NOTIFIER, hdev->suspend_tasks);\n\twake_up(&hdev->suspend_wait_q);\n}\n\nstatic bool adv_cur_instance_is_scannable(struct hci_dev *hdev)\n{\n\treturn adv_instance_is_scannable(hdev, hdev->cur_adv_instance);\n}\n\nvoid __hci_req_disable_advertising(struct hci_request *req)\n{\n\tif (ext_adv_capable(req->hdev)) {\n\t\t__hci_req_disable_ext_adv_instance(req, 0x00);\n\n\t} else {\n\t\tu8 enable = 0x00;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_ADV_ENABLE, sizeof(enable), &enable);\n\t}\n}\n\nstatic u32 get_adv_instance_flags(struct hci_dev *hdev, u8 instance)\n{\n\tu32 flags;\n\tstruct adv_info *adv_instance;\n\n\tif (instance == 0x00) {\n\t\t/* Instance 0 always manages the \"Tx Power\" and \"Flags\"\n\t\t * fields\n\t\t */\n\t\tflags = MGMT_ADV_FLAG_TX_POWER | MGMT_ADV_FLAG_MANAGED_FLAGS;\n\n\t\t/* For instance 0, the HCI_ADVERTISING_CONNECTABLE setting\n\t\t * corresponds to the \"connectable\" instance flag.\n\t\t */\n\t\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING_CONNECTABLE))\n\t\t\tflags |= MGMT_ADV_FLAG_CONNECTABLE;\n\n\t\tif (hci_dev_test_flag(hdev, HCI_LIMITED_DISCOVERABLE))\n\t\t\tflags |= MGMT_ADV_FLAG_LIMITED_DISCOV;\n\t\telse if (hci_dev_test_flag(hdev, HCI_DISCOVERABLE))\n\t\t\tflags |= MGMT_ADV_FLAG_DISCOV;\n\n\t\treturn flags;\n\t}\n\n\tadv_instance = hci_find_adv_instance(hdev, instance);\n\n\t/* Return 0 when we got an invalid instance identifier. */\n\tif (!adv_instance)\n\t\treturn 0;\n\n\treturn adv_instance->flags;\n}\n\nstatic bool adv_use_rpa(struct hci_dev *hdev, uint32_t flags)\n{\n\t/* If privacy is not enabled don't use RPA */\n\tif (!hci_dev_test_flag(hdev, HCI_PRIVACY))\n\t\treturn false;\n\n\t/* If basic privacy mode is enabled use RPA */\n\tif (!hci_dev_test_flag(hdev, HCI_LIMITED_PRIVACY))\n\t\treturn true;\n\n\t/* If limited privacy mode is enabled don't use RPA if we're\n\t * both discoverable and bondable.\n\t */\n\tif ((flags & MGMT_ADV_FLAG_DISCOV) &&\n\t    hci_dev_test_flag(hdev, HCI_BONDABLE))\n\t\treturn false;\n\n\t/* We're neither bondable nor discoverable in the limited\n\t * privacy mode, therefore use RPA.\n\t */\n\treturn true;\n}\n\nstatic bool is_advertising_allowed(struct hci_dev *hdev, bool connectable)\n{\n\t/* If there is no connection we are OK to advertise. */\n\tif (hci_conn_num(hdev, LE_LINK) == 0)\n\t\treturn true;\n\n\t/* Check le_states if there is any connection in slave role. */\n\tif (hdev->conn_hash.le_num_slave > 0) {\n\t\t/* Slave connection state and non connectable mode bit 20. */\n\t\tif (!connectable && !(hdev->le_states[2] & 0x10))\n\t\t\treturn false;\n\n\t\t/* Slave connection state and connectable mode bit 38\n\t\t * and scannable bit 21.\n\t\t */\n\t\tif (connectable && (!(hdev->le_states[4] & 0x40) ||\n\t\t\t\t    !(hdev->le_states[2] & 0x20)))\n\t\t\treturn false;\n\t}\n\n\t/* Check le_states if there is any connection in master role. */\n\tif (hci_conn_num(hdev, LE_LINK) != hdev->conn_hash.le_num_slave) {\n\t\t/* Master connection state and non connectable mode bit 18. */\n\t\tif (!connectable && !(hdev->le_states[2] & 0x02))\n\t\t\treturn false;\n\n\t\t/* Master connection state and connectable mode bit 35 and\n\t\t * scannable 19.\n\t\t */\n\t\tif (connectable && (!(hdev->le_states[4] & 0x08) ||\n\t\t\t\t    !(hdev->le_states[2] & 0x08)))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nvoid __hci_req_enable_advertising(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct adv_info *adv_instance;\n\tstruct hci_cp_le_set_adv_param cp;\n\tu8 own_addr_type, enable = 0x01;\n\tbool connectable;\n\tu16 adv_min_interval, adv_max_interval;\n\tu32 flags;\n\n\tflags = get_adv_instance_flags(hdev, hdev->cur_adv_instance);\n\tadv_instance = hci_find_adv_instance(hdev, hdev->cur_adv_instance);\n\n\t/* If the \"connectable\" instance flag was not set, then choose between\n\t * ADV_IND and ADV_NONCONN_IND based on the global connectable setting.\n\t */\n\tconnectable = (flags & MGMT_ADV_FLAG_CONNECTABLE) ||\n\t\t      mgmt_get_connectable(hdev);\n\n\tif (!is_advertising_allowed(hdev, connectable))\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_LE_ADV))\n\t\t__hci_req_disable_advertising(req);\n\n\t/* Clear the HCI_LE_ADV bit temporarily so that the\n\t * hci_update_random_address knows that it's safe to go ahead\n\t * and write a new random address. The flag will be set back on\n\t * as soon as the SET_ADV_ENABLE HCI command completes.\n\t */\n\thci_dev_clear_flag(hdev, HCI_LE_ADV);\n\n\t/* Set require_privacy to true only when non-connectable\n\t * advertising is used. In that case it is fine to use a\n\t * non-resolvable private address.\n\t */\n\tif (hci_update_random_address(req, !connectable,\n\t\t\t\t      adv_use_rpa(hdev, flags),\n\t\t\t\t      &own_addr_type) < 0)\n\t\treturn;\n\n\tmemset(&cp, 0, sizeof(cp));\n\n\tif (adv_instance) {\n\t\tadv_min_interval = adv_instance->min_interval;\n\t\tadv_max_interval = adv_instance->max_interval;\n\t} else {\n\t\tadv_min_interval = hdev->le_adv_min_interval;\n\t\tadv_max_interval = hdev->le_adv_max_interval;\n\t}\n\n\tif (connectable) {\n\t\tcp.type = LE_ADV_IND;\n\t} else {\n\t\tif (adv_cur_instance_is_scannable(hdev))\n\t\t\tcp.type = LE_ADV_SCAN_IND;\n\t\telse\n\t\t\tcp.type = LE_ADV_NONCONN_IND;\n\n\t\tif (!hci_dev_test_flag(hdev, HCI_DISCOVERABLE) ||\n\t\t    hci_dev_test_flag(hdev, HCI_LIMITED_DISCOVERABLE)) {\n\t\t\tadv_min_interval = DISCOV_LE_FAST_ADV_INT_MIN;\n\t\t\tadv_max_interval = DISCOV_LE_FAST_ADV_INT_MAX;\n\t\t}\n\t}\n\n\tcp.min_interval = cpu_to_le16(adv_min_interval);\n\tcp.max_interval = cpu_to_le16(adv_max_interval);\n\tcp.own_address_type = own_addr_type;\n\tcp.channel_map = hdev->le_adv_channel_map;\n\n\thci_req_add(req, HCI_OP_LE_SET_ADV_PARAM, sizeof(cp), &cp);\n\n\thci_req_add(req, HCI_OP_LE_SET_ADV_ENABLE, sizeof(enable), &enable);\n}\n\nu8 append_local_name(struct hci_dev *hdev, u8 *ptr, u8 ad_len)\n{\n\tsize_t short_len;\n\tsize_t complete_len;\n\n\t/* no space left for name (+ NULL + type + len) */\n\tif ((HCI_MAX_AD_LENGTH - ad_len) < HCI_MAX_SHORT_NAME_LENGTH + 3)\n\t\treturn ad_len;\n\n\t/* use complete name if present and fits */\n\tcomplete_len = strlen(hdev->dev_name);\n\tif (complete_len && complete_len <= HCI_MAX_SHORT_NAME_LENGTH)\n\t\treturn eir_append_data(ptr, ad_len, EIR_NAME_COMPLETE,\n\t\t\t\t       hdev->dev_name, complete_len + 1);\n\n\t/* use short name if present */\n\tshort_len = strlen(hdev->short_name);\n\tif (short_len)\n\t\treturn eir_append_data(ptr, ad_len, EIR_NAME_SHORT,\n\t\t\t\t       hdev->short_name, short_len + 1);\n\n\t/* use shortened full name if present, we already know that name\n\t * is longer then HCI_MAX_SHORT_NAME_LENGTH\n\t */\n\tif (complete_len) {\n\t\tu8 name[HCI_MAX_SHORT_NAME_LENGTH + 1];\n\n\t\tmemcpy(name, hdev->dev_name, HCI_MAX_SHORT_NAME_LENGTH);\n\t\tname[HCI_MAX_SHORT_NAME_LENGTH] = '\\0';\n\n\t\treturn eir_append_data(ptr, ad_len, EIR_NAME_SHORT, name,\n\t\t\t\t       sizeof(name));\n\t}\n\n\treturn ad_len;\n}\n\nstatic u8 append_appearance(struct hci_dev *hdev, u8 *ptr, u8 ad_len)\n{\n\treturn eir_append_le16(ptr, ad_len, EIR_APPEARANCE, hdev->appearance);\n}\n\nstatic u8 create_default_scan_rsp_data(struct hci_dev *hdev, u8 *ptr)\n{\n\tu8 scan_rsp_len = 0;\n\n\tif (hdev->appearance) {\n\t\tscan_rsp_len = append_appearance(hdev, ptr, scan_rsp_len);\n\t}\n\n\treturn append_local_name(hdev, ptr, scan_rsp_len);\n}\n\nstatic u8 create_instance_scan_rsp_data(struct hci_dev *hdev, u8 instance,\n\t\t\t\t\tu8 *ptr)\n{\n\tstruct adv_info *adv_instance;\n\tu32 instance_flags;\n\tu8 scan_rsp_len = 0;\n\n\tadv_instance = hci_find_adv_instance(hdev, instance);\n\tif (!adv_instance)\n\t\treturn 0;\n\n\tinstance_flags = adv_instance->flags;\n\n\tif ((instance_flags & MGMT_ADV_FLAG_APPEARANCE) && hdev->appearance) {\n\t\tscan_rsp_len = append_appearance(hdev, ptr, scan_rsp_len);\n\t}\n\n\tmemcpy(&ptr[scan_rsp_len], adv_instance->scan_rsp_data,\n\t       adv_instance->scan_rsp_len);\n\n\tscan_rsp_len += adv_instance->scan_rsp_len;\n\n\tif (instance_flags & MGMT_ADV_FLAG_LOCAL_NAME)\n\t\tscan_rsp_len = append_local_name(hdev, ptr, scan_rsp_len);\n\n\treturn scan_rsp_len;\n}\n\nvoid __hci_req_update_scan_rsp_data(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 len;\n\n\tif (!hci_dev_test_flag(hdev, HCI_LE_ENABLED))\n\t\treturn;\n\n\tif (ext_adv_capable(hdev)) {\n\t\tstruct hci_cp_le_set_ext_scan_rsp_data cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tif (instance)\n\t\t\tlen = create_instance_scan_rsp_data(hdev, instance,\n\t\t\t\t\t\t\t    cp.data);\n\t\telse\n\t\t\tlen = create_default_scan_rsp_data(hdev, cp.data);\n\n\t\tif (hdev->scan_rsp_data_len == len &&\n\t\t    !memcmp(cp.data, hdev->scan_rsp_data, len))\n\t\t\treturn;\n\n\t\tmemcpy(hdev->scan_rsp_data, cp.data, sizeof(cp.data));\n\t\thdev->scan_rsp_data_len = len;\n\n\t\tcp.handle = instance;\n\t\tcp.length = len;\n\t\tcp.operation = LE_SET_ADV_DATA_OP_COMPLETE;\n\t\tcp.frag_pref = LE_SET_ADV_DATA_NO_FRAG;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_RSP_DATA, sizeof(cp),\n\t\t\t    &cp);\n\t} else {\n\t\tstruct hci_cp_le_set_scan_rsp_data cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tif (instance)\n\t\t\tlen = create_instance_scan_rsp_data(hdev, instance,\n\t\t\t\t\t\t\t    cp.data);\n\t\telse\n\t\t\tlen = create_default_scan_rsp_data(hdev, cp.data);\n\n\t\tif (hdev->scan_rsp_data_len == len &&\n\t\t    !memcmp(cp.data, hdev->scan_rsp_data, len))\n\t\t\treturn;\n\n\t\tmemcpy(hdev->scan_rsp_data, cp.data, sizeof(cp.data));\n\t\thdev->scan_rsp_data_len = len;\n\n\t\tcp.length = len;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_RSP_DATA, sizeof(cp), &cp);\n\t}\n}\n\nstatic u8 create_instance_adv_data(struct hci_dev *hdev, u8 instance, u8 *ptr)\n{\n\tstruct adv_info *adv_instance = NULL;\n\tu8 ad_len = 0, flags = 0;\n\tu32 instance_flags;\n\n\t/* Return 0 when the current instance identifier is invalid. */\n\tif (instance) {\n\t\tadv_instance = hci_find_adv_instance(hdev, instance);\n\t\tif (!adv_instance)\n\t\t\treturn 0;\n\t}\n\n\tinstance_flags = get_adv_instance_flags(hdev, instance);\n\n\t/* If instance already has the flags set skip adding it once\n\t * again.\n\t */\n\tif (adv_instance && eir_get_data(adv_instance->adv_data,\n\t\t\t\t\t adv_instance->adv_data_len, EIR_FLAGS,\n\t\t\t\t\t NULL))\n\t\tgoto skip_flags;\n\n\t/* The Add Advertising command allows userspace to set both the general\n\t * and limited discoverable flags.\n\t */\n\tif (instance_flags & MGMT_ADV_FLAG_DISCOV)\n\t\tflags |= LE_AD_GENERAL;\n\n\tif (instance_flags & MGMT_ADV_FLAG_LIMITED_DISCOV)\n\t\tflags |= LE_AD_LIMITED;\n\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\tflags |= LE_AD_NO_BREDR;\n\n\tif (flags || (instance_flags & MGMT_ADV_FLAG_MANAGED_FLAGS)) {\n\t\t/* If a discovery flag wasn't provided, simply use the global\n\t\t * settings.\n\t\t */\n\t\tif (!flags)\n\t\t\tflags |= mgmt_get_adv_discov_flags(hdev);\n\n\t\t/* If flags would still be empty, then there is no need to\n\t\t * include the \"Flags\" AD field\".\n\t\t */\n\t\tif (flags) {\n\t\t\tptr[0] = 0x02;\n\t\t\tptr[1] = EIR_FLAGS;\n\t\t\tptr[2] = flags;\n\n\t\t\tad_len += 3;\n\t\t\tptr += 3;\n\t\t}\n\t}\n\nskip_flags:\n\tif (adv_instance) {\n\t\tmemcpy(ptr, adv_instance->adv_data,\n\t\t       adv_instance->adv_data_len);\n\t\tad_len += adv_instance->adv_data_len;\n\t\tptr += adv_instance->adv_data_len;\n\t}\n\n\tif (instance_flags & MGMT_ADV_FLAG_TX_POWER) {\n\t\ts8 adv_tx_power;\n\n\t\tif (ext_adv_capable(hdev)) {\n\t\t\tif (adv_instance)\n\t\t\t\tadv_tx_power = adv_instance->tx_power;\n\t\t\telse\n\t\t\t\tadv_tx_power = hdev->adv_tx_power;\n\t\t} else {\n\t\t\tadv_tx_power = hdev->adv_tx_power;\n\t\t}\n\n\t\t/* Provide Tx Power only if we can provide a valid value for it */\n\t\tif (adv_tx_power != HCI_TX_POWER_INVALID) {\n\t\t\tptr[0] = 0x02;\n\t\t\tptr[1] = EIR_TX_POWER;\n\t\t\tptr[2] = (u8)adv_tx_power;\n\n\t\t\tad_len += 3;\n\t\t\tptr += 3;\n\t\t}\n\t}\n\n\treturn ad_len;\n}\n\nvoid __hci_req_update_adv_data(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 len;\n\n\tif (!hci_dev_test_flag(hdev, HCI_LE_ENABLED))\n\t\treturn;\n\n\tif (ext_adv_capable(hdev)) {\n\t\tstruct hci_cp_le_set_ext_adv_data cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tlen = create_instance_adv_data(hdev, instance, cp.data);\n\n\t\t/* There's nothing to do if the data hasn't changed */\n\t\tif (hdev->adv_data_len == len &&\n\t\t    memcmp(cp.data, hdev->adv_data, len) == 0)\n\t\t\treturn;\n\n\t\tmemcpy(hdev->adv_data, cp.data, sizeof(cp.data));\n\t\thdev->adv_data_len = len;\n\n\t\tcp.length = len;\n\t\tcp.handle = instance;\n\t\tcp.operation = LE_SET_ADV_DATA_OP_COMPLETE;\n\t\tcp.frag_pref = LE_SET_ADV_DATA_NO_FRAG;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_ADV_DATA, sizeof(cp), &cp);\n\t} else {\n\t\tstruct hci_cp_le_set_adv_data cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tlen = create_instance_adv_data(hdev, instance, cp.data);\n\n\t\t/* There's nothing to do if the data hasn't changed */\n\t\tif (hdev->adv_data_len == len &&\n\t\t    memcmp(cp.data, hdev->adv_data, len) == 0)\n\t\t\treturn;\n\n\t\tmemcpy(hdev->adv_data, cp.data, sizeof(cp.data));\n\t\thdev->adv_data_len = len;\n\n\t\tcp.length = len;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_ADV_DATA, sizeof(cp), &cp);\n\t}\n}\n\nint hci_req_update_adv_data(struct hci_dev *hdev, u8 instance)\n{\n\tstruct hci_request req;\n\n\thci_req_init(&req, hdev);\n\t__hci_req_update_adv_data(&req, instance);\n\n\treturn hci_req_run(&req, NULL);\n}\n\nstatic void enable_addr_resolution_complete(struct hci_dev *hdev, u8 status,\n\t\t\t\t\t    u16 opcode)\n{\n\tBT_DBG(\"%s status %u\", hdev->name, status);\n}\n\nvoid hci_req_disable_address_resolution(struct hci_dev *hdev)\n{\n\tstruct hci_request req;\n\t__u8 enable = 0x00;\n\n\tif (!use_ll_privacy(hdev) &&\n\t    !hci_dev_test_flag(hdev, HCI_LL_RPA_RESOLUTION))\n\t\treturn;\n\n\thci_req_init(&req, hdev);\n\n\thci_req_add(&req, HCI_OP_LE_SET_ADDR_RESOLV_ENABLE, 1, &enable);\n\n\thci_req_run(&req, enable_addr_resolution_complete);\n}\n\nstatic void adv_enable_complete(struct hci_dev *hdev, u8 status, u16 opcode)\n{\n\tbt_dev_dbg(hdev, \"status %u\", status);\n}\n\nvoid hci_req_reenable_advertising(struct hci_dev *hdev)\n{\n\tstruct hci_request req;\n\n\tif (!hci_dev_test_flag(hdev, HCI_ADVERTISING) &&\n\t    list_empty(&hdev->adv_instances))\n\t\treturn;\n\n\thci_req_init(&req, hdev);\n\n\tif (hdev->cur_adv_instance) {\n\t\t__hci_req_schedule_adv_instance(&req, hdev->cur_adv_instance,\n\t\t\t\t\t\ttrue);\n\t} else {\n\t\tif (ext_adv_capable(hdev)) {\n\t\t\t__hci_req_start_ext_adv(&req, 0x00);\n\t\t} else {\n\t\t\t__hci_req_update_adv_data(&req, 0x00);\n\t\t\t__hci_req_update_scan_rsp_data(&req, 0x00);\n\t\t\t__hci_req_enable_advertising(&req);\n\t\t}\n\t}\n\n\thci_req_run(&req, adv_enable_complete);\n}\n\nstatic void adv_timeout_expire(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    adv_instance_expire.work);\n\n\tstruct hci_request req;\n\tu8 instance;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\thci_dev_lock(hdev);\n\n\thdev->adv_instance_timeout = 0;\n\n\tinstance = hdev->cur_adv_instance;\n\tif (instance == 0x00)\n\t\tgoto unlock;\n\n\thci_req_init(&req, hdev);\n\n\thci_req_clear_adv_instance(hdev, NULL, &req, instance, false);\n\n\tif (list_empty(&hdev->adv_instances))\n\t\t__hci_req_disable_advertising(&req);\n\n\thci_req_run(&req, NULL);\n\nunlock:\n\thci_dev_unlock(hdev);\n}\n\nstatic int hci_req_add_le_interleaved_scan(struct hci_request *req,\n\t\t\t\t\t   unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tint ret = 0;\n\n\thci_dev_lock(hdev);\n\n\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\thci_req_add_le_scan_disable(req, false);\n\thci_req_add_le_passive_scan(req);\n\n\tswitch (hdev->interleave_scan_state) {\n\tcase INTERLEAVE_SCAN_ALLOWLIST:\n\t\tbt_dev_dbg(hdev, \"next state: allowlist\");\n\t\thdev->interleave_scan_state = INTERLEAVE_SCAN_NO_FILTER;\n\t\tbreak;\n\tcase INTERLEAVE_SCAN_NO_FILTER:\n\t\tbt_dev_dbg(hdev, \"next state: no filter\");\n\t\thdev->interleave_scan_state = INTERLEAVE_SCAN_ALLOWLIST;\n\t\tbreak;\n\tcase INTERLEAVE_SCAN_NONE:\n\t\tBT_ERR(\"unexpected error\");\n\t\tret = -1;\n\t}\n\n\thci_dev_unlock(hdev);\n\n\treturn ret;\n}\n\nstatic void interleave_scan_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    interleave_scan.work);\n\tu8 status;\n\tunsigned long timeout;\n\n\tif (hdev->interleave_scan_state == INTERLEAVE_SCAN_ALLOWLIST) {\n\t\ttimeout = msecs_to_jiffies(hdev->advmon_allowlist_duration);\n\t} else if (hdev->interleave_scan_state == INTERLEAVE_SCAN_NO_FILTER) {\n\t\ttimeout = msecs_to_jiffies(hdev->advmon_no_filter_duration);\n\t} else {\n\t\tbt_dev_err(hdev, \"unexpected error\");\n\t\treturn;\n\t}\n\n\thci_req_sync(hdev, hci_req_add_le_interleaved_scan, 0,\n\t\t     HCI_CMD_TIMEOUT, &status);\n\n\t/* Don't continue interleaving if it was canceled */\n\tif (is_interleave_scanning(hdev))\n\t\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t\t   &hdev->interleave_scan, timeout);\n}\n\nint hci_get_random_address(struct hci_dev *hdev, bool require_privacy,\n\t\t\t   bool use_rpa, struct adv_info *adv_instance,\n\t\t\t   u8 *own_addr_type, bdaddr_t *rand_addr)\n{\n\tint err;\n\n\tbacpy(rand_addr, BDADDR_ANY);\n\n\t/* If privacy is enabled use a resolvable private address. If\n\t * current RPA has expired then generate a new one.\n\t */\n\tif (use_rpa) {\n\t\tint to;\n\n\t\t/* If Controller supports LL Privacy use own address type is\n\t\t * 0x03\n\t\t */\n\t\tif (use_ll_privacy(hdev))\n\t\t\t*own_addr_type = ADDR_LE_DEV_RANDOM_RESOLVED;\n\t\telse\n\t\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\n\t\tif (adv_instance) {\n\t\t\tif (!adv_instance->rpa_expired &&\n\t\t\t    !bacmp(&adv_instance->random_addr, &hdev->rpa))\n\t\t\t\treturn 0;\n\n\t\t\tadv_instance->rpa_expired = false;\n\t\t} else {\n\t\t\tif (!hci_dev_test_and_clear_flag(hdev, HCI_RPA_EXPIRED) &&\n\t\t\t    !bacmp(&hdev->random_addr, &hdev->rpa))\n\t\t\t\treturn 0;\n\t\t}\n\n\t\terr = smp_generate_rpa(hdev, hdev->irk, &hdev->rpa);\n\t\tif (err < 0) {\n\t\t\tbt_dev_err(hdev, \"failed to generate new RPA\");\n\t\t\treturn err;\n\t\t}\n\n\t\tbacpy(rand_addr, &hdev->rpa);\n\n\t\tto = msecs_to_jiffies(hdev->rpa_timeout * 1000);\n\t\tif (adv_instance)\n\t\t\tqueue_delayed_work(hdev->workqueue,\n\t\t\t\t\t   &adv_instance->rpa_expired_cb, to);\n\t\telse\n\t\t\tqueue_delayed_work(hdev->workqueue,\n\t\t\t\t\t   &hdev->rpa_expired, to);\n\n\t\treturn 0;\n\t}\n\n\t/* In case of required privacy without resolvable private address,\n\t * use an non-resolvable private address. This is useful for\n\t * non-connectable advertising.\n\t */\n\tif (require_privacy) {\n\t\tbdaddr_t nrpa;\n\n\t\twhile (true) {\n\t\t\t/* The non-resolvable private address is generated\n\t\t\t * from random six bytes with the two most significant\n\t\t\t * bits cleared.\n\t\t\t */\n\t\t\tget_random_bytes(&nrpa, 6);\n\t\t\tnrpa.b[5] &= 0x3f;\n\n\t\t\t/* The non-resolvable private address shall not be\n\t\t\t * equal to the public address.\n\t\t\t */\n\t\t\tif (bacmp(&hdev->bdaddr, &nrpa))\n\t\t\t\tbreak;\n\t\t}\n\n\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\t\tbacpy(rand_addr, &nrpa);\n\n\t\treturn 0;\n\t}\n\n\t/* No privacy so use a public address. */\n\t*own_addr_type = ADDR_LE_DEV_PUBLIC;\n\n\treturn 0;\n}\n\nvoid __hci_req_clear_ext_adv_sets(struct hci_request *req)\n{\n\thci_req_add(req, HCI_OP_LE_CLEAR_ADV_SETS, 0, NULL);\n}\n\nint __hci_req_setup_ext_adv_instance(struct hci_request *req, u8 instance)\n{\n\tstruct hci_cp_le_set_ext_adv_params cp;\n\tstruct hci_dev *hdev = req->hdev;\n\tbool connectable;\n\tu32 flags;\n\tbdaddr_t random_addr;\n\tu8 own_addr_type;\n\tint err;\n\tstruct adv_info *adv_instance;\n\tbool secondary_adv;\n\n\tif (instance > 0) {\n\t\tadv_instance = hci_find_adv_instance(hdev, instance);\n\t\tif (!adv_instance)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tadv_instance = NULL;\n\t}\n\n\tflags = get_adv_instance_flags(hdev, instance);\n\n\t/* If the \"connectable\" instance flag was not set, then choose between\n\t * ADV_IND and ADV_NONCONN_IND based on the global connectable setting.\n\t */\n\tconnectable = (flags & MGMT_ADV_FLAG_CONNECTABLE) ||\n\t\t      mgmt_get_connectable(hdev);\n\n\tif (!is_advertising_allowed(hdev, connectable))\n\t\treturn -EPERM;\n\n\t/* Set require_privacy to true only when non-connectable\n\t * advertising is used. In that case it is fine to use a\n\t * non-resolvable private address.\n\t */\n\terr = hci_get_random_address(hdev, !connectable,\n\t\t\t\t     adv_use_rpa(hdev, flags), adv_instance,\n\t\t\t\t     &own_addr_type, &random_addr);\n\tif (err < 0)\n\t\treturn err;\n\n\tmemset(&cp, 0, sizeof(cp));\n\n\tif (adv_instance) {\n\t\thci_cpu_to_le24(adv_instance->min_interval, cp.min_interval);\n\t\thci_cpu_to_le24(adv_instance->max_interval, cp.max_interval);\n\t\tcp.tx_power = adv_instance->tx_power;\n\t} else {\n\t\thci_cpu_to_le24(hdev->le_adv_min_interval, cp.min_interval);\n\t\thci_cpu_to_le24(hdev->le_adv_max_interval, cp.max_interval);\n\t\tcp.tx_power = HCI_ADV_TX_POWER_NO_PREFERENCE;\n\t}\n\n\tsecondary_adv = (flags & MGMT_ADV_FLAG_SEC_MASK);\n\n\tif (connectable) {\n\t\tif (secondary_adv)\n\t\t\tcp.evt_properties = cpu_to_le16(LE_EXT_ADV_CONN_IND);\n\t\telse\n\t\t\tcp.evt_properties = cpu_to_le16(LE_LEGACY_ADV_IND);\n\t} else if (adv_instance_is_scannable(hdev, instance)) {\n\t\tif (secondary_adv)\n\t\t\tcp.evt_properties = cpu_to_le16(LE_EXT_ADV_SCAN_IND);\n\t\telse\n\t\t\tcp.evt_properties = cpu_to_le16(LE_LEGACY_ADV_SCAN_IND);\n\t} else {\n\t\tif (secondary_adv)\n\t\t\tcp.evt_properties = cpu_to_le16(LE_EXT_ADV_NON_CONN_IND);\n\t\telse\n\t\t\tcp.evt_properties = cpu_to_le16(LE_LEGACY_NONCONN_IND);\n\t}\n\n\tcp.own_addr_type = own_addr_type;\n\tcp.channel_map = hdev->le_adv_channel_map;\n\tcp.handle = instance;\n\n\tif (flags & MGMT_ADV_FLAG_SEC_2M) {\n\t\tcp.primary_phy = HCI_ADV_PHY_1M;\n\t\tcp.secondary_phy = HCI_ADV_PHY_2M;\n\t} else if (flags & MGMT_ADV_FLAG_SEC_CODED) {\n\t\tcp.primary_phy = HCI_ADV_PHY_CODED;\n\t\tcp.secondary_phy = HCI_ADV_PHY_CODED;\n\t} else {\n\t\t/* In all other cases use 1M */\n\t\tcp.primary_phy = HCI_ADV_PHY_1M;\n\t\tcp.secondary_phy = HCI_ADV_PHY_1M;\n\t}\n\n\thci_req_add(req, HCI_OP_LE_SET_EXT_ADV_PARAMS, sizeof(cp), &cp);\n\n\tif (own_addr_type == ADDR_LE_DEV_RANDOM &&\n\t    bacmp(&random_addr, BDADDR_ANY)) {\n\t\tstruct hci_cp_le_set_adv_set_rand_addr cp;\n\n\t\t/* Check if random address need to be updated */\n\t\tif (adv_instance) {\n\t\t\tif (!bacmp(&random_addr, &adv_instance->random_addr))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!bacmp(&random_addr, &hdev->random_addr))\n\t\t\t\treturn 0;\n\t\t}\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tcp.handle = instance;\n\t\tbacpy(&cp.bdaddr, &random_addr);\n\n\t\thci_req_add(req,\n\t\t\t    HCI_OP_LE_SET_ADV_SET_RAND_ADDR,\n\t\t\t    sizeof(cp), &cp);\n\t}\n\n\treturn 0;\n}\n\nint __hci_req_enable_ext_advertising(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_le_set_ext_adv_enable *cp;\n\tstruct hci_cp_ext_adv_set *adv_set;\n\tu8 data[sizeof(*cp) + sizeof(*adv_set) * 1];\n\tstruct adv_info *adv_instance;\n\n\tif (instance > 0) {\n\t\tadv_instance = hci_find_adv_instance(hdev, instance);\n\t\tif (!adv_instance)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tadv_instance = NULL;\n\t}\n\n\tcp = (void *) data;\n\tadv_set = (void *) cp->data;\n\n\tmemset(cp, 0, sizeof(*cp));\n\n\tcp->enable = 0x01;\n\tcp->num_of_sets = 0x01;\n\n\tmemset(adv_set, 0, sizeof(*adv_set));\n\n\tadv_set->handle = instance;\n\n\t/* Set duration per instance since controller is responsible for\n\t * scheduling it.\n\t */\n\tif (adv_instance && adv_instance->duration) {\n\t\tu16 duration = adv_instance->timeout * MSEC_PER_SEC;\n\n\t\t/* Time = N * 10 ms */\n\t\tadv_set->duration = cpu_to_le16(duration / 10);\n\t}\n\n\thci_req_add(req, HCI_OP_LE_SET_EXT_ADV_ENABLE,\n\t\t    sizeof(*cp) + sizeof(*adv_set) * cp->num_of_sets,\n\t\t    data);\n\n\treturn 0;\n}\n\nint __hci_req_disable_ext_adv_instance(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_le_set_ext_adv_enable *cp;\n\tstruct hci_cp_ext_adv_set *adv_set;\n\tu8 data[sizeof(*cp) + sizeof(*adv_set) * 1];\n\tu8 req_size;\n\n\t/* If request specifies an instance that doesn't exist, fail */\n\tif (instance > 0 && !hci_find_adv_instance(hdev, instance))\n\t\treturn -EINVAL;\n\n\tmemset(data, 0, sizeof(data));\n\n\tcp = (void *)data;\n\tadv_set = (void *)cp->data;\n\n\t/* Instance 0x00 indicates all advertising instances will be disabled */\n\tcp->num_of_sets = !!instance;\n\tcp->enable = 0x00;\n\n\tadv_set->handle = instance;\n\n\treq_size = sizeof(*cp) + sizeof(*adv_set) * cp->num_of_sets;\n\thci_req_add(req, HCI_OP_LE_SET_EXT_ADV_ENABLE, req_size, data);\n\n\treturn 0;\n}\n\nint __hci_req_remove_ext_adv_instance(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\t/* If request specifies an instance that doesn't exist, fail */\n\tif (instance > 0 && !hci_find_adv_instance(hdev, instance))\n\t\treturn -EINVAL;\n\n\thci_req_add(req, HCI_OP_LE_REMOVE_ADV_SET, sizeof(instance), &instance);\n\n\treturn 0;\n}\n\nint __hci_req_start_ext_adv(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct adv_info *adv_instance = hci_find_adv_instance(hdev, instance);\n\tint err;\n\n\t/* If instance isn't pending, the chip knows about it, and it's safe to\n\t * disable\n\t */\n\tif (adv_instance && !adv_instance->pending)\n\t\t__hci_req_disable_ext_adv_instance(req, instance);\n\n\terr = __hci_req_setup_ext_adv_instance(req, instance);\n\tif (err < 0)\n\t\treturn err;\n\n\t__hci_req_update_scan_rsp_data(req, instance);\n\t__hci_req_enable_ext_advertising(req, instance);\n\n\treturn 0;\n}\n\nint __hci_req_schedule_adv_instance(struct hci_request *req, u8 instance,\n\t\t\t\t    bool force)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct adv_info *adv_instance = NULL;\n\tu16 timeout;\n\n\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING) ||\n\t    list_empty(&hdev->adv_instances))\n\t\treturn -EPERM;\n\n\tif (hdev->adv_instance_timeout)\n\t\treturn -EBUSY;\n\n\tadv_instance = hci_find_adv_instance(hdev, instance);\n\tif (!adv_instance)\n\t\treturn -ENOENT;\n\n\t/* A zero timeout means unlimited advertising. As long as there is\n\t * only one instance, duration should be ignored. We still set a timeout\n\t * in case further instances are being added later on.\n\t *\n\t * If the remaining lifetime of the instance is more than the duration\n\t * then the timeout corresponds to the duration, otherwise it will be\n\t * reduced to the remaining instance lifetime.\n\t */\n\tif (adv_instance->timeout == 0 ||\n\t    adv_instance->duration <= adv_instance->remaining_time)\n\t\ttimeout = adv_instance->duration;\n\telse\n\t\ttimeout = adv_instance->remaining_time;\n\n\t/* The remaining time is being reduced unless the instance is being\n\t * advertised without time limit.\n\t */\n\tif (adv_instance->timeout)\n\t\tadv_instance->remaining_time =\n\t\t\t\tadv_instance->remaining_time - timeout;\n\n\t/* Only use work for scheduling instances with legacy advertising */\n\tif (!ext_adv_capable(hdev)) {\n\t\thdev->adv_instance_timeout = timeout;\n\t\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t   &hdev->adv_instance_expire,\n\t\t\t   msecs_to_jiffies(timeout * 1000));\n\t}\n\n\t/* If we're just re-scheduling the same instance again then do not\n\t * execute any HCI commands. This happens when a single instance is\n\t * being advertised.\n\t */\n\tif (!force && hdev->cur_adv_instance == instance &&\n\t    hci_dev_test_flag(hdev, HCI_LE_ADV))\n\t\treturn 0;\n\n\thdev->cur_adv_instance = instance;\n\tif (ext_adv_capable(hdev)) {\n\t\t__hci_req_start_ext_adv(req, instance);\n\t} else {\n\t\t__hci_req_update_adv_data(req, instance);\n\t\t__hci_req_update_scan_rsp_data(req, instance);\n\t\t__hci_req_enable_advertising(req);\n\t}\n\n\treturn 0;\n}\n\n/* For a single instance:\n * - force == true: The instance will be removed even when its remaining\n *   lifetime is not zero.\n * - force == false: the instance will be deactivated but kept stored unless\n *   the remaining lifetime is zero.\n *\n * For instance == 0x00:\n * - force == true: All instances will be removed regardless of their timeout\n *   setting.\n * - force == false: Only instances that have a timeout will be removed.\n */\nvoid hci_req_clear_adv_instance(struct hci_dev *hdev, struct sock *sk,\n\t\t\t\tstruct hci_request *req, u8 instance,\n\t\t\t\tbool force)\n{\n\tstruct adv_info *adv_instance, *n, *next_instance = NULL;\n\tint err;\n\tu8 rem_inst;\n\n\t/* Cancel any timeout concerning the removed instance(s). */\n\tif (!instance || hdev->cur_adv_instance == instance)\n\t\tcancel_adv_timeout(hdev);\n\n\t/* Get the next instance to advertise BEFORE we remove\n\t * the current one. This can be the same instance again\n\t * if there is only one instance.\n\t */\n\tif (instance && hdev->cur_adv_instance == instance)\n\t\tnext_instance = hci_get_next_instance(hdev, instance);\n\n\tif (instance == 0x00) {\n\t\tlist_for_each_entry_safe(adv_instance, n, &hdev->adv_instances,\n\t\t\t\t\t list) {\n\t\t\tif (!(force || adv_instance->timeout))\n\t\t\t\tcontinue;\n\n\t\t\trem_inst = adv_instance->instance;\n\t\t\terr = hci_remove_adv_instance(hdev, rem_inst);\n\t\t\tif (!err)\n\t\t\t\tmgmt_advertising_removed(sk, hdev, rem_inst);\n\t\t}\n\t} else {\n\t\tadv_instance = hci_find_adv_instance(hdev, instance);\n\n\t\tif (force || (adv_instance && adv_instance->timeout &&\n\t\t\t      !adv_instance->remaining_time)) {\n\t\t\t/* Don't advertise a removed instance. */\n\t\t\tif (next_instance &&\n\t\t\t    next_instance->instance == instance)\n\t\t\t\tnext_instance = NULL;\n\n\t\t\terr = hci_remove_adv_instance(hdev, instance);\n\t\t\tif (!err)\n\t\t\t\tmgmt_advertising_removed(sk, hdev, instance);\n\t\t}\n\t}\n\n\tif (!req || !hdev_is_powered(hdev) ||\n\t    hci_dev_test_flag(hdev, HCI_ADVERTISING))\n\t\treturn;\n\n\tif (next_instance && !ext_adv_capable(hdev))\n\t\t__hci_req_schedule_adv_instance(req, next_instance->instance,\n\t\t\t\t\t\tfalse);\n}\n\nstatic void set_random_addr(struct hci_request *req, bdaddr_t *rpa)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\t/* If we're advertising or initiating an LE connection we can't\n\t * go ahead and change the random address at this time. This is\n\t * because the eventual initiator address used for the\n\t * subsequently created connection will be undefined (some\n\t * controllers use the new address and others the one we had\n\t * when the operation started).\n\t *\n\t * In this kind of scenario skip the update and let the random\n\t * address be updated at the next cycle.\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_LE_ADV) ||\n\t    hci_lookup_le_connect(hdev)) {\n\t\tbt_dev_dbg(hdev, \"Deferring random address update\");\n\t\thci_dev_set_flag(hdev, HCI_RPA_EXPIRED);\n\t\treturn;\n\t}\n\n\thci_req_add(req, HCI_OP_LE_SET_RANDOM_ADDR, 6, rpa);\n}\n\nint hci_update_random_address(struct hci_request *req, bool require_privacy,\n\t\t\t      bool use_rpa, u8 *own_addr_type)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tint err;\n\n\t/* If privacy is enabled use a resolvable private address. If\n\t * current RPA has expired or there is something else than\n\t * the current RPA in use, then generate a new one.\n\t */\n\tif (use_rpa) {\n\t\tint to;\n\n\t\t/* If Controller supports LL Privacy use own address type is\n\t\t * 0x03\n\t\t */\n\t\tif (use_ll_privacy(hdev))\n\t\t\t*own_addr_type = ADDR_LE_DEV_RANDOM_RESOLVED;\n\t\telse\n\t\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\n\t\tif (!hci_dev_test_and_clear_flag(hdev, HCI_RPA_EXPIRED) &&\n\t\t    !bacmp(&hdev->random_addr, &hdev->rpa))\n\t\t\treturn 0;\n\n\t\terr = smp_generate_rpa(hdev, hdev->irk, &hdev->rpa);\n\t\tif (err < 0) {\n\t\t\tbt_dev_err(hdev, \"failed to generate new RPA\");\n\t\t\treturn err;\n\t\t}\n\n\t\tset_random_addr(req, &hdev->rpa);\n\n\t\tto = msecs_to_jiffies(hdev->rpa_timeout * 1000);\n\t\tqueue_delayed_work(hdev->workqueue, &hdev->rpa_expired, to);\n\n\t\treturn 0;\n\t}\n\n\t/* In case of required privacy without resolvable private address,\n\t * use an non-resolvable private address. This is useful for active\n\t * scanning and non-connectable advertising.\n\t */\n\tif (require_privacy) {\n\t\tbdaddr_t nrpa;\n\n\t\twhile (true) {\n\t\t\t/* The non-resolvable private address is generated\n\t\t\t * from random six bytes with the two most significant\n\t\t\t * bits cleared.\n\t\t\t */\n\t\t\tget_random_bytes(&nrpa, 6);\n\t\t\tnrpa.b[5] &= 0x3f;\n\n\t\t\t/* The non-resolvable private address shall not be\n\t\t\t * equal to the public address.\n\t\t\t */\n\t\t\tif (bacmp(&hdev->bdaddr, &nrpa))\n\t\t\t\tbreak;\n\t\t}\n\n\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\t\tset_random_addr(req, &nrpa);\n\t\treturn 0;\n\t}\n\n\t/* If forcing static address is in use or there is no public\n\t * address use the static address as random address (but skip\n\t * the HCI command if the current random address is already the\n\t * static one.\n\t *\n\t * In case BR/EDR has been disabled on a dual-mode controller\n\t * and a static address has been configured, then use that\n\t * address instead of the public BR/EDR address.\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_FORCE_STATIC_ADDR) ||\n\t    !bacmp(&hdev->bdaddr, BDADDR_ANY) ||\n\t    (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED) &&\n\t     bacmp(&hdev->static_addr, BDADDR_ANY))) {\n\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\t\tif (bacmp(&hdev->static_addr, &hdev->random_addr))\n\t\t\thci_req_add(req, HCI_OP_LE_SET_RANDOM_ADDR, 6,\n\t\t\t\t    &hdev->static_addr);\n\t\treturn 0;\n\t}\n\n\t/* Neither privacy nor static address is being used so use a\n\t * public address.\n\t */\n\t*own_addr_type = ADDR_LE_DEV_PUBLIC;\n\n\treturn 0;\n}\n\nstatic bool disconnected_whitelist_entries(struct hci_dev *hdev)\n{\n\tstruct bdaddr_list *b;\n\n\tlist_for_each_entry(b, &hdev->whitelist, list) {\n\t\tstruct hci_conn *conn;\n\n\t\tconn = hci_conn_hash_lookup_ba(hdev, ACL_LINK, &b->bdaddr);\n\t\tif (!conn)\n\t\t\treturn true;\n\n\t\tif (conn->state != BT_CONNECTED && conn->state != BT_CONFIG)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nvoid __hci_req_update_scan(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 scan;\n\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\treturn;\n\n\tif (!hdev_is_powered(hdev))\n\t\treturn;\n\n\tif (mgmt_powering_down(hdev))\n\t\treturn;\n\n\tif (hdev->scanning_paused)\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_CONNECTABLE) ||\n\t    disconnected_whitelist_entries(hdev))\n\t\tscan = SCAN_PAGE;\n\telse\n\t\tscan = SCAN_DISABLED;\n\n\tif (hci_dev_test_flag(hdev, HCI_DISCOVERABLE))\n\t\tscan |= SCAN_INQUIRY;\n\n\tif (test_bit(HCI_PSCAN, &hdev->flags) == !!(scan & SCAN_PAGE) &&\n\t    test_bit(HCI_ISCAN, &hdev->flags) == !!(scan & SCAN_INQUIRY))\n\t\treturn;\n\n\thci_req_add(req, HCI_OP_WRITE_SCAN_ENABLE, 1, &scan);\n}\n\nstatic int update_scan(struct hci_request *req, unsigned long opt)\n{\n\thci_dev_lock(req->hdev);\n\t__hci_req_update_scan(req);\n\thci_dev_unlock(req->hdev);\n\treturn 0;\n}\n\nstatic void scan_update_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev, scan_update);\n\n\thci_req_sync(hdev, update_scan, 0, HCI_CMD_TIMEOUT, NULL);\n}\n\nstatic int connectable_update(struct hci_request *req, unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\thci_dev_lock(hdev);\n\n\t__hci_req_update_scan(req);\n\n\t/* If BR/EDR is not enabled and we disable advertising as a\n\t * by-product of disabling connectable, we need to update the\n\t * advertising flags.\n\t */\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\t__hci_req_update_adv_data(req, hdev->cur_adv_instance);\n\n\t/* Update the advertising parameters if necessary */\n\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING) ||\n\t    !list_empty(&hdev->adv_instances)) {\n\t\tif (ext_adv_capable(hdev))\n\t\t\t__hci_req_start_ext_adv(req, hdev->cur_adv_instance);\n\t\telse\n\t\t\t__hci_req_enable_advertising(req);\n\t}\n\n\t__hci_update_background_scan(req);\n\n\thci_dev_unlock(hdev);\n\n\treturn 0;\n}\n\nstatic void connectable_update_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    connectable_update);\n\tu8 status;\n\n\thci_req_sync(hdev, connectable_update, 0, HCI_CMD_TIMEOUT, &status);\n\tmgmt_set_connectable_complete(hdev, status);\n}\n\nstatic u8 get_service_classes(struct hci_dev *hdev)\n{\n\tstruct bt_uuid *uuid;\n\tu8 val = 0;\n\n\tlist_for_each_entry(uuid, &hdev->uuids, list)\n\t\tval |= uuid->svc_hint;\n\n\treturn val;\n}\n\nvoid __hci_req_update_class(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 cod[3];\n\n\tbt_dev_dbg(hdev, \"\");\n\n\tif (!hdev_is_powered(hdev))\n\t\treturn;\n\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_SERVICE_CACHE))\n\t\treturn;\n\n\tcod[0] = hdev->minor_class;\n\tcod[1] = hdev->major_class;\n\tcod[2] = get_service_classes(hdev);\n\n\tif (hci_dev_test_flag(hdev, HCI_LIMITED_DISCOVERABLE))\n\t\tcod[1] |= 0x20;\n\n\tif (memcmp(cod, hdev->dev_class, 3) == 0)\n\t\treturn;\n\n\thci_req_add(req, HCI_OP_WRITE_CLASS_OF_DEV, sizeof(cod), cod);\n}\n\nstatic void write_iac(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_write_current_iac_lap cp;\n\n\tif (!hci_dev_test_flag(hdev, HCI_DISCOVERABLE))\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_LIMITED_DISCOVERABLE)) {\n\t\t/* Limited discoverable mode */\n\t\tcp.num_iac = min_t(u8, hdev->num_iac, 2);\n\t\tcp.iac_lap[0] = 0x00;\t/* LIAC */\n\t\tcp.iac_lap[1] = 0x8b;\n\t\tcp.iac_lap[2] = 0x9e;\n\t\tcp.iac_lap[3] = 0x33;\t/* GIAC */\n\t\tcp.iac_lap[4] = 0x8b;\n\t\tcp.iac_lap[5] = 0x9e;\n\t} else {\n\t\t/* General discoverable mode */\n\t\tcp.num_iac = 1;\n\t\tcp.iac_lap[0] = 0x33;\t/* GIAC */\n\t\tcp.iac_lap[1] = 0x8b;\n\t\tcp.iac_lap[2] = 0x9e;\n\t}\n\n\thci_req_add(req, HCI_OP_WRITE_CURRENT_IAC_LAP,\n\t\t    (cp.num_iac * 3) + 1, &cp);\n}\n\nstatic int discoverable_update(struct hci_request *req, unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\thci_dev_lock(hdev);\n\n\tif (hci_dev_test_flag(hdev, HCI_BREDR_ENABLED)) {\n\t\twrite_iac(req);\n\t\t__hci_req_update_scan(req);\n\t\t__hci_req_update_class(req);\n\t}\n\n\t/* Advertising instances don't use the global discoverable setting, so\n\t * only update AD if advertising was enabled using Set Advertising.\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING)) {\n\t\t__hci_req_update_adv_data(req, 0x00);\n\n\t\t/* Discoverable mode affects the local advertising\n\t\t * address in limited privacy mode.\n\t\t */\n\t\tif (hci_dev_test_flag(hdev, HCI_LIMITED_PRIVACY)) {\n\t\t\tif (ext_adv_capable(hdev))\n\t\t\t\t__hci_req_start_ext_adv(req, 0x00);\n\t\t\telse\n\t\t\t\t__hci_req_enable_advertising(req);\n\t\t}\n\t}\n\n\thci_dev_unlock(hdev);\n\n\treturn 0;\n}\n\nstatic void discoverable_update_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    discoverable_update);\n\tu8 status;\n\n\thci_req_sync(hdev, discoverable_update, 0, HCI_CMD_TIMEOUT, &status);\n\tmgmt_set_discoverable_complete(hdev, status);\n}\n\nvoid __hci_abort_conn(struct hci_request *req, struct hci_conn *conn,\n\t\t      u8 reason)\n{\n\tswitch (conn->state) {\n\tcase BT_CONNECTED:\n\tcase BT_CONFIG:\n\t\tif (conn->type == AMP_LINK) {\n\t\t\tstruct hci_cp_disconn_phy_link cp;\n\n\t\t\tcp.phy_handle = HCI_PHY_HANDLE(conn->handle);\n\t\t\tcp.reason = reason;\n\t\t\thci_req_add(req, HCI_OP_DISCONN_PHY_LINK, sizeof(cp),\n\t\t\t\t    &cp);\n\t\t} else {\n\t\t\tstruct hci_cp_disconnect dc;\n\n\t\t\tdc.handle = cpu_to_le16(conn->handle);\n\t\t\tdc.reason = reason;\n\t\t\thci_req_add(req, HCI_OP_DISCONNECT, sizeof(dc), &dc);\n\t\t}\n\n\t\tconn->state = BT_DISCONN;\n\n\t\tbreak;\n\tcase BT_CONNECT:\n\t\tif (conn->type == LE_LINK) {\n\t\t\tif (test_bit(HCI_CONN_SCANNING, &conn->flags))\n\t\t\t\tbreak;\n\t\t\thci_req_add(req, HCI_OP_LE_CREATE_CONN_CANCEL,\n\t\t\t\t    0, NULL);\n\t\t} else if (conn->type == ACL_LINK) {\n\t\t\tif (req->hdev->hci_ver < BLUETOOTH_VER_1_2)\n\t\t\t\tbreak;\n\t\t\thci_req_add(req, HCI_OP_CREATE_CONN_CANCEL,\n\t\t\t\t    6, &conn->dst);\n\t\t}\n\t\tbreak;\n\tcase BT_CONNECT2:\n\t\tif (conn->type == ACL_LINK) {\n\t\t\tstruct hci_cp_reject_conn_req rej;\n\n\t\t\tbacpy(&rej.bdaddr, &conn->dst);\n\t\t\trej.reason = reason;\n\n\t\t\thci_req_add(req, HCI_OP_REJECT_CONN_REQ,\n\t\t\t\t    sizeof(rej), &rej);\n\t\t} else if (conn->type == SCO_LINK || conn->type == ESCO_LINK) {\n\t\t\tstruct hci_cp_reject_sync_conn_req rej;\n\n\t\t\tbacpy(&rej.bdaddr, &conn->dst);\n\n\t\t\t/* SCO rejection has its own limited set of\n\t\t\t * allowed error values (0x0D-0x0F) which isn't\n\t\t\t * compatible with most values passed to this\n\t\t\t * function. To be safe hard-code one of the\n\t\t\t * values that's suitable for SCO.\n\t\t\t */\n\t\t\trej.reason = HCI_ERROR_REJ_LIMITED_RESOURCES;\n\n\t\t\thci_req_add(req, HCI_OP_REJECT_SYNC_CONN_REQ,\n\t\t\t\t    sizeof(rej), &rej);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tconn->state = BT_CLOSED;\n\t\tbreak;\n\t}\n}\n\nstatic void abort_conn_complete(struct hci_dev *hdev, u8 status, u16 opcode)\n{\n\tif (status)\n\t\tbt_dev_dbg(hdev, \"Failed to abort connection: status 0x%2.2x\", status);\n}\n\nint hci_abort_conn(struct hci_conn *conn, u8 reason)\n{\n\tstruct hci_request req;\n\tint err;\n\n\thci_req_init(&req, conn->hdev);\n\n\t__hci_abort_conn(&req, conn, reason);\n\n\terr = hci_req_run(&req, abort_conn_complete);\n\tif (err && err != -ENODATA) {\n\t\tbt_dev_err(conn->hdev, \"failed to run HCI request: err %d\", err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int update_bg_scan(struct hci_request *req, unsigned long opt)\n{\n\thci_dev_lock(req->hdev);\n\t__hci_update_background_scan(req);\n\thci_dev_unlock(req->hdev);\n\treturn 0;\n}\n\nstatic void bg_scan_update(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    bg_scan_update);\n\tstruct hci_conn *conn;\n\tu8 status;\n\tint err;\n\n\terr = hci_req_sync(hdev, update_bg_scan, 0, HCI_CMD_TIMEOUT, &status);\n\tif (!err)\n\t\treturn;\n\n\thci_dev_lock(hdev);\n\n\tconn = hci_conn_hash_lookup_state(hdev, LE_LINK, BT_CONNECT);\n\tif (conn)\n\t\thci_le_conn_failed(conn, status);\n\n\thci_dev_unlock(hdev);\n}\n\nstatic int le_scan_disable(struct hci_request *req, unsigned long opt)\n{\n\thci_req_add_le_scan_disable(req, false);\n\treturn 0;\n}\n\nstatic int bredr_inquiry(struct hci_request *req, unsigned long opt)\n{\n\tu8 length = opt;\n\tconst u8 giac[3] = { 0x33, 0x8b, 0x9e };\n\tconst u8 liac[3] = { 0x00, 0x8b, 0x9e };\n\tstruct hci_cp_inquiry cp;\n\n\tbt_dev_dbg(req->hdev, \"\");\n\n\thci_dev_lock(req->hdev);\n\thci_inquiry_cache_flush(req->hdev);\n\thci_dev_unlock(req->hdev);\n\n\tmemset(&cp, 0, sizeof(cp));\n\n\tif (req->hdev->discovery.limited)\n\t\tmemcpy(&cp.lap, liac, sizeof(cp.lap));\n\telse\n\t\tmemcpy(&cp.lap, giac, sizeof(cp.lap));\n\n\tcp.length = length;\n\n\thci_req_add(req, HCI_OP_INQUIRY, sizeof(cp), &cp);\n\n\treturn 0;\n}\n\nstatic void le_scan_disable_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    le_scan_disable.work);\n\tu8 status;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\tif (!hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\treturn;\n\n\tcancel_delayed_work(&hdev->le_scan_restart);\n\n\thci_req_sync(hdev, le_scan_disable, 0, HCI_CMD_TIMEOUT, &status);\n\tif (status) {\n\t\tbt_dev_err(hdev, \"failed to disable LE scan: status 0x%02x\",\n\t\t\t   status);\n\t\treturn;\n\t}\n\n\thdev->discovery.scan_start = 0;\n\n\t/* If we were running LE only scan, change discovery state. If\n\t * we were running both LE and BR/EDR inquiry simultaneously,\n\t * and BR/EDR inquiry is already finished, stop discovery,\n\t * otherwise BR/EDR inquiry will stop discovery when finished.\n\t * If we will resolve remote device name, do not change\n\t * discovery state.\n\t */\n\n\tif (hdev->discovery.type == DISCOV_TYPE_LE)\n\t\tgoto discov_stopped;\n\n\tif (hdev->discovery.type != DISCOV_TYPE_INTERLEAVED)\n\t\treturn;\n\n\tif (test_bit(HCI_QUIRK_SIMULTANEOUS_DISCOVERY, &hdev->quirks)) {\n\t\tif (!test_bit(HCI_INQUIRY, &hdev->flags) &&\n\t\t    hdev->discovery.state != DISCOVERY_RESOLVING)\n\t\t\tgoto discov_stopped;\n\n\t\treturn;\n\t}\n\n\thci_req_sync(hdev, bredr_inquiry, DISCOV_INTERLEAVED_INQUIRY_LEN,\n\t\t     HCI_CMD_TIMEOUT, &status);\n\tif (status) {\n\t\tbt_dev_err(hdev, \"inquiry failed: status 0x%02x\", status);\n\t\tgoto discov_stopped;\n\t}\n\n\treturn;\n\ndiscov_stopped:\n\thci_dev_lock(hdev);\n\thci_discovery_set_state(hdev, DISCOVERY_STOPPED);\n\thci_dev_unlock(hdev);\n}\n\nstatic int le_scan_restart(struct hci_request *req, unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\t/* If controller is not scanning we are done. */\n\tif (!hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\treturn 0;\n\n\tif (hdev->scanning_paused) {\n\t\tbt_dev_dbg(hdev, \"Scanning is paused for suspend\");\n\t\treturn 0;\n\t}\n\n\thci_req_add_le_scan_disable(req, false);\n\n\tif (use_ext_scan(hdev)) {\n\t\tstruct hci_cp_le_set_ext_scan_enable ext_enable_cp;\n\n\t\tmemset(&ext_enable_cp, 0, sizeof(ext_enable_cp));\n\t\text_enable_cp.enable = LE_SCAN_ENABLE;\n\t\text_enable_cp.filter_dup = LE_SCAN_FILTER_DUP_ENABLE;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_ENABLE,\n\t\t\t    sizeof(ext_enable_cp), &ext_enable_cp);\n\t} else {\n\t\tstruct hci_cp_le_set_scan_enable cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\t\tcp.enable = LE_SCAN_ENABLE;\n\t\tcp.filter_dup = LE_SCAN_FILTER_DUP_ENABLE;\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_ENABLE, sizeof(cp), &cp);\n\t}\n\n\treturn 0;\n}\n\nstatic void le_scan_restart_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    le_scan_restart.work);\n\tunsigned long timeout, duration, scan_start, now;\n\tu8 status;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\thci_req_sync(hdev, le_scan_restart, 0, HCI_CMD_TIMEOUT, &status);\n\tif (status) {\n\t\tbt_dev_err(hdev, \"failed to restart LE scan: status %d\",\n\t\t\t   status);\n\t\treturn;\n\t}\n\n\thci_dev_lock(hdev);\n\n\tif (!test_bit(HCI_QUIRK_STRICT_DUPLICATE_FILTER, &hdev->quirks) ||\n\t    !hdev->discovery.scan_start)\n\t\tgoto unlock;\n\n\t/* When the scan was started, hdev->le_scan_disable has been queued\n\t * after duration from scan_start. During scan restart this job\n\t * has been canceled, and we need to queue it again after proper\n\t * timeout, to make sure that scan does not run indefinitely.\n\t */\n\tduration = hdev->discovery.scan_duration;\n\tscan_start = hdev->discovery.scan_start;\n\tnow = jiffies;\n\tif (now - scan_start <= duration) {\n\t\tint elapsed;\n\n\t\tif (now >= scan_start)\n\t\t\telapsed = now - scan_start;\n\t\telse\n\t\t\telapsed = ULONG_MAX - scan_start + now;\n\n\t\ttimeout = duration - elapsed;\n\t} else {\n\t\ttimeout = 0;\n\t}\n\n\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t   &hdev->le_scan_disable, timeout);\n\nunlock:\n\thci_dev_unlock(hdev);\n}\n\nstatic int active_scan(struct hci_request *req, unsigned long opt)\n{\n\tuint16_t interval = opt;\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 own_addr_type;\n\t/* White list is not used for discovery */\n\tu8 filter_policy = 0x00;\n\t/* Discovery doesn't require controller address resolution */\n\tbool addr_resolv = false;\n\tint err;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\t/* If controller is scanning, it means the background scanning is\n\t * running. Thus, we should temporarily stop it in order to set the\n\t * discovery scanning parameters.\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN)) {\n\t\thci_req_add_le_scan_disable(req, false);\n\t\tcancel_interleave_scan(hdev);\n\t}\n\n\t/* All active scans will be done with either a resolvable private\n\t * address (when privacy feature has been enabled) or non-resolvable\n\t * private address.\n\t */\n\terr = hci_update_random_address(req, true, scan_use_rpa(hdev),\n\t\t\t\t\t&own_addr_type);\n\tif (err < 0)\n\t\town_addr_type = ADDR_LE_DEV_PUBLIC;\n\n\thci_req_start_scan(req, LE_SCAN_ACTIVE, interval,\n\t\t\t   hdev->le_scan_window_discovery, own_addr_type,\n\t\t\t   filter_policy, addr_resolv);\n\treturn 0;\n}\n\nstatic int interleaved_discov(struct hci_request *req, unsigned long opt)\n{\n\tint err;\n\n\tbt_dev_dbg(req->hdev, \"\");\n\n\terr = active_scan(req, opt);\n\tif (err)\n\t\treturn err;\n\n\treturn bredr_inquiry(req, DISCOV_BREDR_INQUIRY_LEN);\n}\n\nstatic void start_discovery(struct hci_dev *hdev, u8 *status)\n{\n\tunsigned long timeout;\n\n\tbt_dev_dbg(hdev, \"type %u\", hdev->discovery.type);\n\n\tswitch (hdev->discovery.type) {\n\tcase DISCOV_TYPE_BREDR:\n\t\tif (!hci_dev_test_flag(hdev, HCI_INQUIRY))\n\t\t\thci_req_sync(hdev, bredr_inquiry,\n\t\t\t\t     DISCOV_BREDR_INQUIRY_LEN, HCI_CMD_TIMEOUT,\n\t\t\t\t     status);\n\t\treturn;\n\tcase DISCOV_TYPE_INTERLEAVED:\n\t\t/* When running simultaneous discovery, the LE scanning time\n\t\t * should occupy the whole discovery time sine BR/EDR inquiry\n\t\t * and LE scanning are scheduled by the controller.\n\t\t *\n\t\t * For interleaving discovery in comparison, BR/EDR inquiry\n\t\t * and LE scanning are done sequentially with separate\n\t\t * timeouts.\n\t\t */\n\t\tif (test_bit(HCI_QUIRK_SIMULTANEOUS_DISCOVERY,\n\t\t\t     &hdev->quirks)) {\n\t\t\ttimeout = msecs_to_jiffies(DISCOV_LE_TIMEOUT);\n\t\t\t/* During simultaneous discovery, we double LE scan\n\t\t\t * interval. We must leave some time for the controller\n\t\t\t * to do BR/EDR inquiry.\n\t\t\t */\n\t\t\thci_req_sync(hdev, interleaved_discov,\n\t\t\t\t     hdev->le_scan_int_discovery * 2, HCI_CMD_TIMEOUT,\n\t\t\t\t     status);\n\t\t\tbreak;\n\t\t}\n\n\t\ttimeout = msecs_to_jiffies(hdev->discov_interleaved_timeout);\n\t\thci_req_sync(hdev, active_scan, hdev->le_scan_int_discovery,\n\t\t\t     HCI_CMD_TIMEOUT, status);\n\t\tbreak;\n\tcase DISCOV_TYPE_LE:\n\t\ttimeout = msecs_to_jiffies(DISCOV_LE_TIMEOUT);\n\t\thci_req_sync(hdev, active_scan, hdev->le_scan_int_discovery,\n\t\t\t     HCI_CMD_TIMEOUT, status);\n\t\tbreak;\n\tdefault:\n\t\t*status = HCI_ERROR_UNSPECIFIED;\n\t\treturn;\n\t}\n\n\tif (*status)\n\t\treturn;\n\n\tbt_dev_dbg(hdev, \"timeout %u ms\", jiffies_to_msecs(timeout));\n\n\t/* When service discovery is used and the controller has a\n\t * strict duplicate filter, it is important to remember the\n\t * start and duration of the scan. This is required for\n\t * restarting scanning during the discovery phase.\n\t */\n\tif (test_bit(HCI_QUIRK_STRICT_DUPLICATE_FILTER, &hdev->quirks) &&\n\t\t     hdev->discovery.result_filtering) {\n\t\thdev->discovery.scan_start = jiffies;\n\t\thdev->discovery.scan_duration = timeout;\n\t}\n\n\tqueue_delayed_work(hdev->req_workqueue, &hdev->le_scan_disable,\n\t\t\t   timeout);\n}\n\nbool hci_req_stop_discovery(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct discovery_state *d = &hdev->discovery;\n\tstruct hci_cp_remote_name_req_cancel cp;\n\tstruct inquiry_entry *e;\n\tbool ret = false;\n\n\tbt_dev_dbg(hdev, \"state %u\", hdev->discovery.state);\n\n\tif (d->state == DISCOVERY_FINDING || d->state == DISCOVERY_STOPPING) {\n\t\tif (test_bit(HCI_INQUIRY, &hdev->flags))\n\t\t\thci_req_add(req, HCI_OP_INQUIRY_CANCEL, 0, NULL);\n\n\t\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN)) {\n\t\t\tcancel_delayed_work(&hdev->le_scan_disable);\n\t\t\thci_req_add_le_scan_disable(req, false);\n\t\t}\n\n\t\tret = true;\n\t} else {\n\t\t/* Passive scanning */\n\t\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN)) {\n\t\t\thci_req_add_le_scan_disable(req, false);\n\t\t\tret = true;\n\t\t}\n\t}\n\n\t/* No further actions needed for LE-only discovery */\n\tif (d->type == DISCOV_TYPE_LE)\n\t\treturn ret;\n\n\tif (d->state == DISCOVERY_RESOLVING || d->state == DISCOVERY_STOPPING) {\n\t\te = hci_inquiry_cache_lookup_resolve(hdev, BDADDR_ANY,\n\t\t\t\t\t\t     NAME_PENDING);\n\t\tif (!e)\n\t\t\treturn ret;\n\n\t\tbacpy(&cp.bdaddr, &e->data.bdaddr);\n\t\thci_req_add(req, HCI_OP_REMOTE_NAME_REQ_CANCEL, sizeof(cp),\n\t\t\t    &cp);\n\t\tret = true;\n\t}\n\n\treturn ret;\n}\n\nstatic int stop_discovery(struct hci_request *req, unsigned long opt)\n{\n\thci_dev_lock(req->hdev);\n\thci_req_stop_discovery(req);\n\thci_dev_unlock(req->hdev);\n\n\treturn 0;\n}\n\nstatic void discov_update(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    discov_update);\n\tu8 status = 0;\n\n\tswitch (hdev->discovery.state) {\n\tcase DISCOVERY_STARTING:\n\t\tstart_discovery(hdev, &status);\n\t\tmgmt_start_discovery_complete(hdev, status);\n\t\tif (status)\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_STOPPED);\n\t\telse\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_FINDING);\n\t\tbreak;\n\tcase DISCOVERY_STOPPING:\n\t\thci_req_sync(hdev, stop_discovery, 0, HCI_CMD_TIMEOUT, &status);\n\t\tmgmt_stop_discovery_complete(hdev, status);\n\t\tif (!status)\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_STOPPED);\n\t\tbreak;\n\tcase DISCOVERY_STOPPED:\n\tdefault:\n\t\treturn;\n\t}\n}\n\nstatic void discov_off(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    discov_off.work);\n\n\tbt_dev_dbg(hdev, \"\");\n\n\thci_dev_lock(hdev);\n\n\t/* When discoverable timeout triggers, then just make sure\n\t * the limited discoverable flag is cleared. Even in the case\n\t * of a timeout triggered from general discoverable, it is\n\t * safe to unconditionally clear the flag.\n\t */\n\thci_dev_clear_flag(hdev, HCI_LIMITED_DISCOVERABLE);\n\thci_dev_clear_flag(hdev, HCI_DISCOVERABLE);\n\thdev->discov_timeout = 0;\n\n\thci_dev_unlock(hdev);\n\n\thci_req_sync(hdev, discoverable_update, 0, HCI_CMD_TIMEOUT, NULL);\n\tmgmt_new_settings(hdev);\n}\n\nstatic int powered_update_hci(struct hci_request *req, unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 link_sec;\n\n\thci_dev_lock(hdev);\n\n\tif (hci_dev_test_flag(hdev, HCI_SSP_ENABLED) &&\n\t    !lmp_host_ssp_capable(hdev)) {\n\t\tu8 mode = 0x01;\n\n\t\thci_req_add(req, HCI_OP_WRITE_SSP_MODE, sizeof(mode), &mode);\n\n\t\tif (bredr_sc_enabled(hdev) && !lmp_host_sc_capable(hdev)) {\n\t\t\tu8 support = 0x01;\n\n\t\t\thci_req_add(req, HCI_OP_WRITE_SC_SUPPORT,\n\t\t\t\t    sizeof(support), &support);\n\t\t}\n\t}\n\n\tif (hci_dev_test_flag(hdev, HCI_LE_ENABLED) &&\n\t    lmp_bredr_capable(hdev)) {\n\t\tstruct hci_cp_write_le_host_supported cp;\n\n\t\tcp.le = 0x01;\n\t\tcp.simul = 0x00;\n\n\t\t/* Check first if we already have the right\n\t\t * host state (host features set)\n\t\t */\n\t\tif (cp.le != lmp_host_le_capable(hdev) ||\n\t\t    cp.simul != lmp_host_le_br_capable(hdev))\n\t\t\thci_req_add(req, HCI_OP_WRITE_LE_HOST_SUPPORTED,\n\t\t\t\t    sizeof(cp), &cp);\n\t}\n\n\tif (hci_dev_test_flag(hdev, HCI_LE_ENABLED)) {\n\t\t/* Make sure the controller has a good default for\n\t\t * advertising data. This also applies to the case\n\t\t * where BR/EDR was toggled during the AUTO_OFF phase.\n\t\t */\n\t\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING) ||\n\t\t    list_empty(&hdev->adv_instances)) {\n\t\t\tint err;\n\n\t\t\tif (ext_adv_capable(hdev)) {\n\t\t\t\terr = __hci_req_setup_ext_adv_instance(req,\n\t\t\t\t\t\t\t\t       0x00);\n\t\t\t\tif (!err)\n\t\t\t\t\t__hci_req_update_scan_rsp_data(req,\n\t\t\t\t\t\t\t\t       0x00);\n\t\t\t} else {\n\t\t\t\terr = 0;\n\t\t\t\t__hci_req_update_adv_data(req, 0x00);\n\t\t\t\t__hci_req_update_scan_rsp_data(req, 0x00);\n\t\t\t}\n\n\t\t\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING)) {\n\t\t\t\tif (!ext_adv_capable(hdev))\n\t\t\t\t\t__hci_req_enable_advertising(req);\n\t\t\t\telse if (!err)\n\t\t\t\t\t__hci_req_enable_ext_advertising(req,\n\t\t\t\t\t\t\t\t\t 0x00);\n\t\t\t}\n\t\t} else if (!list_empty(&hdev->adv_instances)) {\n\t\t\tstruct adv_info *adv_instance;\n\n\t\t\tadv_instance = list_first_entry(&hdev->adv_instances,\n\t\t\t\t\t\t\tstruct adv_info, list);\n\t\t\t__hci_req_schedule_adv_instance(req,\n\t\t\t\t\t\t\tadv_instance->instance,\n\t\t\t\t\t\t\ttrue);\n\t\t}\n\t}\n\n\tlink_sec = hci_dev_test_flag(hdev, HCI_LINK_SECURITY);\n\tif (link_sec != test_bit(HCI_AUTH, &hdev->flags))\n\t\thci_req_add(req, HCI_OP_WRITE_AUTH_ENABLE,\n\t\t\t    sizeof(link_sec), &link_sec);\n\n\tif (lmp_bredr_capable(hdev)) {\n\t\tif (hci_dev_test_flag(hdev, HCI_FAST_CONNECTABLE))\n\t\t\t__hci_req_write_fast_connectable(req, true);\n\t\telse\n\t\t\t__hci_req_write_fast_connectable(req, false);\n\t\t__hci_req_update_scan(req);\n\t\t__hci_req_update_class(req);\n\t\t__hci_req_update_name(req);\n\t\t__hci_req_update_eir(req);\n\t}\n\n\thci_dev_unlock(hdev);\n\treturn 0;\n}\n\nint __hci_req_hci_power_on(struct hci_dev *hdev)\n{\n\t/* Register the available SMP channels (BR/EDR and LE) only when\n\t * successfully powering on the controller. This late\n\t * registration is required so that LE SMP can clearly decide if\n\t * the public address or static address is used.\n\t */\n\tsmp_register(hdev);\n\n\treturn __hci_req_sync(hdev, powered_update_hci, 0, HCI_CMD_TIMEOUT,\n\t\t\t      NULL);\n}\n\nvoid hci_request_setup(struct hci_dev *hdev)\n{\n\tINIT_WORK(&hdev->discov_update, discov_update);\n\tINIT_WORK(&hdev->bg_scan_update, bg_scan_update);\n\tINIT_WORK(&hdev->scan_update, scan_update_work);\n\tINIT_WORK(&hdev->connectable_update, connectable_update_work);\n\tINIT_WORK(&hdev->discoverable_update, discoverable_update_work);\n\tINIT_DELAYED_WORK(&hdev->discov_off, discov_off);\n\tINIT_DELAYED_WORK(&hdev->le_scan_disable, le_scan_disable_work);\n\tINIT_DELAYED_WORK(&hdev->le_scan_restart, le_scan_restart_work);\n\tINIT_DELAYED_WORK(&hdev->adv_instance_expire, adv_timeout_expire);\n\tINIT_DELAYED_WORK(&hdev->interleave_scan, interleave_scan_work);\n}\n\nvoid hci_request_cancel_all(struct hci_dev *hdev)\n{\n\thci_req_sync_cancel(hdev, ENODEV);\n\n\tcancel_work_sync(&hdev->discov_update);\n\tcancel_work_sync(&hdev->bg_scan_update);\n\tcancel_work_sync(&hdev->scan_update);\n\tcancel_work_sync(&hdev->connectable_update);\n\tcancel_work_sync(&hdev->discoverable_update);\n\tcancel_delayed_work_sync(&hdev->discov_off);\n\tcancel_delayed_work_sync(&hdev->le_scan_disable);\n\tcancel_delayed_work_sync(&hdev->le_scan_restart);\n\n\tif (hdev->adv_instance_timeout) {\n\t\tcancel_delayed_work_sync(&hdev->adv_instance_expire);\n\t\thdev->adv_instance_timeout = 0;\n\t}\n\n\tcancel_interleave_scan(hdev);\n}\n"], "fixing_code": ["/*\n   BlueZ - Bluetooth protocol stack for Linux\n\n   Copyright (C) 2014 Intel Corporation\n\n   This program is free software; you can redistribute it and/or modify\n   it under the terms of the GNU General Public License version 2 as\n   published by the Free Software Foundation;\n\n   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS.\n   IN NO EVENT SHALL THE COPYRIGHT HOLDER(S) AND AUTHOR(S) BE LIABLE FOR ANY\n   CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES\n   WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n   ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n   OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n   ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PATENTS,\n   COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS, RELATING TO USE OF THIS\n   SOFTWARE IS DISCLAIMED.\n*/\n\n#include <linux/sched/signal.h>\n\n#include <net/bluetooth/bluetooth.h>\n#include <net/bluetooth/hci_core.h>\n#include <net/bluetooth/mgmt.h>\n\n#include \"smp.h\"\n#include \"hci_request.h\"\n#include \"msft.h\"\n\n#define HCI_REQ_DONE\t  0\n#define HCI_REQ_PEND\t  1\n#define HCI_REQ_CANCELED  2\n\nvoid hci_req_init(struct hci_request *req, struct hci_dev *hdev)\n{\n\tskb_queue_head_init(&req->cmd_q);\n\treq->hdev = hdev;\n\treq->err = 0;\n}\n\nvoid hci_req_purge(struct hci_request *req)\n{\n\tskb_queue_purge(&req->cmd_q);\n}\n\nbool hci_req_status_pend(struct hci_dev *hdev)\n{\n\treturn hdev->req_status == HCI_REQ_PEND;\n}\n\nstatic int req_run(struct hci_request *req, hci_req_complete_t complete,\n\t\t   hci_req_complete_skb_t complete_skb)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct sk_buff *skb;\n\tunsigned long flags;\n\n\tbt_dev_dbg(hdev, \"length %u\", skb_queue_len(&req->cmd_q));\n\n\t/* If an error occurred during request building, remove all HCI\n\t * commands queued on the HCI request queue.\n\t */\n\tif (req->err) {\n\t\tskb_queue_purge(&req->cmd_q);\n\t\treturn req->err;\n\t}\n\n\t/* Do not allow empty requests */\n\tif (skb_queue_empty(&req->cmd_q))\n\t\treturn -ENODATA;\n\n\tskb = skb_peek_tail(&req->cmd_q);\n\tif (complete) {\n\t\tbt_cb(skb)->hci.req_complete = complete;\n\t} else if (complete_skb) {\n\t\tbt_cb(skb)->hci.req_complete_skb = complete_skb;\n\t\tbt_cb(skb)->hci.req_flags |= HCI_REQ_SKB;\n\t}\n\n\tspin_lock_irqsave(&hdev->cmd_q.lock, flags);\n\tskb_queue_splice_tail(&req->cmd_q, &hdev->cmd_q);\n\tspin_unlock_irqrestore(&hdev->cmd_q.lock, flags);\n\n\tqueue_work(hdev->workqueue, &hdev->cmd_work);\n\n\treturn 0;\n}\n\nint hci_req_run(struct hci_request *req, hci_req_complete_t complete)\n{\n\treturn req_run(req, complete, NULL);\n}\n\nint hci_req_run_skb(struct hci_request *req, hci_req_complete_skb_t complete)\n{\n\treturn req_run(req, NULL, complete);\n}\n\nstatic void hci_req_sync_complete(struct hci_dev *hdev, u8 result, u16 opcode,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tbt_dev_dbg(hdev, \"result 0x%2.2x\", result);\n\n\tif (hdev->req_status == HCI_REQ_PEND) {\n\t\thdev->req_result = result;\n\t\thdev->req_status = HCI_REQ_DONE;\n\t\tif (skb)\n\t\t\thdev->req_skb = skb_get(skb);\n\t\twake_up_interruptible(&hdev->req_wait_q);\n\t}\n}\n\nvoid hci_req_sync_cancel(struct hci_dev *hdev, int err)\n{\n\tbt_dev_dbg(hdev, \"err 0x%2.2x\", err);\n\n\tif (hdev->req_status == HCI_REQ_PEND) {\n\t\thdev->req_result = err;\n\t\thdev->req_status = HCI_REQ_CANCELED;\n\t\twake_up_interruptible(&hdev->req_wait_q);\n\t}\n}\n\nstruct sk_buff *__hci_cmd_sync_ev(struct hci_dev *hdev, u16 opcode, u32 plen,\n\t\t\t\t  const void *param, u8 event, u32 timeout)\n{\n\tstruct hci_request req;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\thci_req_init(&req, hdev);\n\n\thci_req_add_ev(&req, opcode, plen, param, event);\n\n\thdev->req_status = HCI_REQ_PEND;\n\n\terr = hci_req_run_skb(&req, hci_req_sync_complete);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\terr = wait_event_interruptible_timeout(hdev->req_wait_q,\n\t\t\thdev->req_status != HCI_REQ_PEND, timeout);\n\n\tif (err == -ERESTARTSYS)\n\t\treturn ERR_PTR(-EINTR);\n\n\tswitch (hdev->req_status) {\n\tcase HCI_REQ_DONE:\n\t\terr = -bt_to_errno(hdev->req_result);\n\t\tbreak;\n\n\tcase HCI_REQ_CANCELED:\n\t\terr = -hdev->req_result;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ETIMEDOUT;\n\t\tbreak;\n\t}\n\n\thdev->req_status = hdev->req_result = 0;\n\tskb = hdev->req_skb;\n\thdev->req_skb = NULL;\n\n\tbt_dev_dbg(hdev, \"end: err %d\", err);\n\n\tif (err < 0) {\n\t\tkfree_skb(skb);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tif (!skb)\n\t\treturn ERR_PTR(-ENODATA);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(__hci_cmd_sync_ev);\n\nstruct sk_buff *__hci_cmd_sync(struct hci_dev *hdev, u16 opcode, u32 plen,\n\t\t\t       const void *param, u32 timeout)\n{\n\treturn __hci_cmd_sync_ev(hdev, opcode, plen, param, 0, timeout);\n}\nEXPORT_SYMBOL(__hci_cmd_sync);\n\n/* Execute request and wait for completion. */\nint __hci_req_sync(struct hci_dev *hdev, int (*func)(struct hci_request *req,\n\t\t\t\t\t\t     unsigned long opt),\n\t\t   unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tstruct hci_request req;\n\tint err = 0;\n\n\tbt_dev_dbg(hdev, \"start\");\n\n\thci_req_init(&req, hdev);\n\n\thdev->req_status = HCI_REQ_PEND;\n\n\terr = func(&req, opt);\n\tif (err) {\n\t\tif (hci_status)\n\t\t\t*hci_status = HCI_ERROR_UNSPECIFIED;\n\t\treturn err;\n\t}\n\n\terr = hci_req_run_skb(&req, hci_req_sync_complete);\n\tif (err < 0) {\n\t\thdev->req_status = 0;\n\n\t\t/* ENODATA means the HCI request command queue is empty.\n\t\t * This can happen when a request with conditionals doesn't\n\t\t * trigger any commands to be sent. This is normal behavior\n\t\t * and should not trigger an error return.\n\t\t */\n\t\tif (err == -ENODATA) {\n\t\t\tif (hci_status)\n\t\t\t\t*hci_status = 0;\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (hci_status)\n\t\t\t*hci_status = HCI_ERROR_UNSPECIFIED;\n\n\t\treturn err;\n\t}\n\n\terr = wait_event_interruptible_timeout(hdev->req_wait_q,\n\t\t\thdev->req_status != HCI_REQ_PEND, timeout);\n\n\tif (err == -ERESTARTSYS)\n\t\treturn -EINTR;\n\n\tswitch (hdev->req_status) {\n\tcase HCI_REQ_DONE:\n\t\terr = -bt_to_errno(hdev->req_result);\n\t\tif (hci_status)\n\t\t\t*hci_status = hdev->req_result;\n\t\tbreak;\n\n\tcase HCI_REQ_CANCELED:\n\t\terr = -hdev->req_result;\n\t\tif (hci_status)\n\t\t\t*hci_status = HCI_ERROR_UNSPECIFIED;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ETIMEDOUT;\n\t\tif (hci_status)\n\t\t\t*hci_status = HCI_ERROR_UNSPECIFIED;\n\t\tbreak;\n\t}\n\n\tkfree_skb(hdev->req_skb);\n\thdev->req_skb = NULL;\n\thdev->req_status = hdev->req_result = 0;\n\n\tbt_dev_dbg(hdev, \"end: err %d\", err);\n\n\treturn err;\n}\n\nint hci_req_sync(struct hci_dev *hdev, int (*req)(struct hci_request *req,\n\t\t\t\t\t\t  unsigned long opt),\n\t\t unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tint ret;\n\n\t/* Serialize all requests */\n\thci_req_sync_lock(hdev);\n\t/* check the state after obtaing the lock to protect the HCI_UP\n\t * against any races from hci_dev_do_close when the controller\n\t * gets removed.\n\t */\n\tif (test_bit(HCI_UP, &hdev->flags))\n\t\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n\telse\n\t\tret = -ENETDOWN;\n\thci_req_sync_unlock(hdev);\n\n\treturn ret;\n}\n\nstruct sk_buff *hci_prepare_cmd(struct hci_dev *hdev, u16 opcode, u32 plen,\n\t\t\t\tconst void *param)\n{\n\tint len = HCI_COMMAND_HDR_SIZE + plen;\n\tstruct hci_command_hdr *hdr;\n\tstruct sk_buff *skb;\n\n\tskb = bt_skb_alloc(len, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\thdr = skb_put(skb, HCI_COMMAND_HDR_SIZE);\n\thdr->opcode = cpu_to_le16(opcode);\n\thdr->plen   = plen;\n\n\tif (plen)\n\t\tskb_put_data(skb, param, plen);\n\n\tbt_dev_dbg(hdev, \"skb len %d\", skb->len);\n\n\thci_skb_pkt_type(skb) = HCI_COMMAND_PKT;\n\thci_skb_opcode(skb) = opcode;\n\n\treturn skb;\n}\n\n/* Queue a command to an asynchronous HCI request */\nvoid hci_req_add_ev(struct hci_request *req, u16 opcode, u32 plen,\n\t\t    const void *param, u8 event)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct sk_buff *skb;\n\n\tbt_dev_dbg(hdev, \"opcode 0x%4.4x plen %d\", opcode, plen);\n\n\t/* If an error occurred during request building, there is no point in\n\t * queueing the HCI command. We can simply return.\n\t */\n\tif (req->err)\n\t\treturn;\n\n\tskb = hci_prepare_cmd(hdev, opcode, plen, param);\n\tif (!skb) {\n\t\tbt_dev_err(hdev, \"no memory for command (opcode 0x%4.4x)\",\n\t\t\t   opcode);\n\t\treq->err = -ENOMEM;\n\t\treturn;\n\t}\n\n\tif (skb_queue_empty(&req->cmd_q))\n\t\tbt_cb(skb)->hci.req_flags |= HCI_REQ_START;\n\n\tbt_cb(skb)->hci.req_event = event;\n\n\tskb_queue_tail(&req->cmd_q, skb);\n}\n\nvoid hci_req_add(struct hci_request *req, u16 opcode, u32 plen,\n\t\t const void *param)\n{\n\thci_req_add_ev(req, opcode, plen, param, 0);\n}\n\nvoid __hci_req_write_fast_connectable(struct hci_request *req, bool enable)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_write_page_scan_activity acp;\n\tu8 type;\n\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\treturn;\n\n\tif (hdev->hci_ver < BLUETOOTH_VER_1_2)\n\t\treturn;\n\n\tif (enable) {\n\t\ttype = PAGE_SCAN_TYPE_INTERLACED;\n\n\t\t/* 160 msec page scan interval */\n\t\tacp.interval = cpu_to_le16(0x0100);\n\t} else {\n\t\ttype = hdev->def_page_scan_type;\n\t\tacp.interval = cpu_to_le16(hdev->def_page_scan_int);\n\t}\n\n\tacp.window = cpu_to_le16(hdev->def_page_scan_window);\n\n\tif (__cpu_to_le16(hdev->page_scan_interval) != acp.interval ||\n\t    __cpu_to_le16(hdev->page_scan_window) != acp.window)\n\t\thci_req_add(req, HCI_OP_WRITE_PAGE_SCAN_ACTIVITY,\n\t\t\t    sizeof(acp), &acp);\n\n\tif (hdev->page_scan_type != type)\n\t\thci_req_add(req, HCI_OP_WRITE_PAGE_SCAN_TYPE, 1, &type);\n}\n\nstatic void start_interleave_scan(struct hci_dev *hdev)\n{\n\thdev->interleave_scan_state = INTERLEAVE_SCAN_NO_FILTER;\n\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t   &hdev->interleave_scan, 0);\n}\n\nstatic bool is_interleave_scanning(struct hci_dev *hdev)\n{\n\treturn hdev->interleave_scan_state != INTERLEAVE_SCAN_NONE;\n}\n\nstatic void cancel_interleave_scan(struct hci_dev *hdev)\n{\n\tbt_dev_dbg(hdev, \"cancelling interleave scan\");\n\n\tcancel_delayed_work_sync(&hdev->interleave_scan);\n\n\thdev->interleave_scan_state = INTERLEAVE_SCAN_NONE;\n}\n\n/* Return true if interleave_scan wasn't started until exiting this function,\n * otherwise, return false\n */\nstatic bool __hci_update_interleaved_scan(struct hci_dev *hdev)\n{\n\t/* Do interleaved scan only if all of the following are true:\n\t * - There is at least one ADV monitor\n\t * - At least one pending LE connection or one device to be scanned for\n\t * - Monitor offloading is not supported\n\t * If so, we should alternate between allowlist scan and one without\n\t * any filters to save power.\n\t */\n\tbool use_interleaving = hci_is_adv_monitoring(hdev) &&\n\t\t\t\t!(list_empty(&hdev->pend_le_conns) &&\n\t\t\t\t  list_empty(&hdev->pend_le_reports)) &&\n\t\t\t\thci_get_adv_monitor_offload_ext(hdev) ==\n\t\t\t\t    HCI_ADV_MONITOR_EXT_NONE;\n\tbool is_interleaving = is_interleave_scanning(hdev);\n\n\tif (use_interleaving && !is_interleaving) {\n\t\tstart_interleave_scan(hdev);\n\t\tbt_dev_dbg(hdev, \"starting interleave scan\");\n\t\treturn true;\n\t}\n\n\tif (!use_interleaving && is_interleaving)\n\t\tcancel_interleave_scan(hdev);\n\n\treturn false;\n}\n\n/* This function controls the background scanning based on hdev->pend_le_conns\n * list. If there are pending LE connection we start the background scanning,\n * otherwise we stop it.\n *\n * This function requires the caller holds hdev->lock.\n */\nstatic void __hci_update_background_scan(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\tif (!test_bit(HCI_UP, &hdev->flags) ||\n\t    test_bit(HCI_INIT, &hdev->flags) ||\n\t    hci_dev_test_flag(hdev, HCI_SETUP) ||\n\t    hci_dev_test_flag(hdev, HCI_CONFIG) ||\n\t    hci_dev_test_flag(hdev, HCI_AUTO_OFF) ||\n\t    hci_dev_test_flag(hdev, HCI_UNREGISTER))\n\t\treturn;\n\n\t/* No point in doing scanning if LE support hasn't been enabled */\n\tif (!hci_dev_test_flag(hdev, HCI_LE_ENABLED))\n\t\treturn;\n\n\t/* If discovery is active don't interfere with it */\n\tif (hdev->discovery.state != DISCOVERY_STOPPED)\n\t\treturn;\n\n\t/* Reset RSSI and UUID filters when starting background scanning\n\t * since these filters are meant for service discovery only.\n\t *\n\t * The Start Discovery and Start Service Discovery operations\n\t * ensure to set proper values for RSSI threshold and UUID\n\t * filter list. So it is safe to just reset them here.\n\t */\n\thci_discovery_filter_clear(hdev);\n\n\tbt_dev_dbg(hdev, \"ADV monitoring is %s\",\n\t\t   hci_is_adv_monitoring(hdev) ? \"on\" : \"off\");\n\n\tif (list_empty(&hdev->pend_le_conns) &&\n\t    list_empty(&hdev->pend_le_reports) &&\n\t    !hci_is_adv_monitoring(hdev)) {\n\t\t/* If there is no pending LE connections or devices\n\t\t * to be scanned for or no ADV monitors, we should stop the\n\t\t * background scanning.\n\t\t */\n\n\t\t/* If controller is not scanning we are done. */\n\t\tif (!hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\t\treturn;\n\n\t\thci_req_add_le_scan_disable(req, false);\n\n\t\tbt_dev_dbg(hdev, \"stopping background scanning\");\n\t} else {\n\t\t/* If there is at least one pending LE connection, we should\n\t\t * keep the background scan running.\n\t\t */\n\n\t\t/* If controller is connecting, we should not start scanning\n\t\t * since some controllers are not able to scan and connect at\n\t\t * the same time.\n\t\t */\n\t\tif (hci_lookup_le_connect(hdev))\n\t\t\treturn;\n\n\t\t/* If controller is currently scanning, we stop it to ensure we\n\t\t * don't miss any advertising (due to duplicates filter).\n\t\t */\n\t\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\t\thci_req_add_le_scan_disable(req, false);\n\n\t\thci_req_add_le_passive_scan(req);\n\t\tbt_dev_dbg(hdev, \"starting background scanning\");\n\t}\n}\n\nvoid __hci_req_update_name(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_write_local_name cp;\n\n\tmemcpy(cp.name, hdev->dev_name, sizeof(cp.name));\n\n\thci_req_add(req, HCI_OP_WRITE_LOCAL_NAME, sizeof(cp), &cp);\n}\n\n#define PNP_INFO_SVCLASS_ID\t\t0x1200\n\nstatic u8 *create_uuid16_list(struct hci_dev *hdev, u8 *data, ptrdiff_t len)\n{\n\tu8 *ptr = data, *uuids_start = NULL;\n\tstruct bt_uuid *uuid;\n\n\tif (len < 4)\n\t\treturn ptr;\n\n\tlist_for_each_entry(uuid, &hdev->uuids, list) {\n\t\tu16 uuid16;\n\n\t\tif (uuid->size != 16)\n\t\t\tcontinue;\n\n\t\tuuid16 = get_unaligned_le16(&uuid->uuid[12]);\n\t\tif (uuid16 < 0x1100)\n\t\t\tcontinue;\n\n\t\tif (uuid16 == PNP_INFO_SVCLASS_ID)\n\t\t\tcontinue;\n\n\t\tif (!uuids_start) {\n\t\t\tuuids_start = ptr;\n\t\t\tuuids_start[0] = 1;\n\t\t\tuuids_start[1] = EIR_UUID16_ALL;\n\t\t\tptr += 2;\n\t\t}\n\n\t\t/* Stop if not enough space to put next UUID */\n\t\tif ((ptr - data) + sizeof(u16) > len) {\n\t\t\tuuids_start[1] = EIR_UUID16_SOME;\n\t\t\tbreak;\n\t\t}\n\n\t\t*ptr++ = (uuid16 & 0x00ff);\n\t\t*ptr++ = (uuid16 & 0xff00) >> 8;\n\t\tuuids_start[0] += sizeof(uuid16);\n\t}\n\n\treturn ptr;\n}\n\nstatic u8 *create_uuid32_list(struct hci_dev *hdev, u8 *data, ptrdiff_t len)\n{\n\tu8 *ptr = data, *uuids_start = NULL;\n\tstruct bt_uuid *uuid;\n\n\tif (len < 6)\n\t\treturn ptr;\n\n\tlist_for_each_entry(uuid, &hdev->uuids, list) {\n\t\tif (uuid->size != 32)\n\t\t\tcontinue;\n\n\t\tif (!uuids_start) {\n\t\t\tuuids_start = ptr;\n\t\t\tuuids_start[0] = 1;\n\t\t\tuuids_start[1] = EIR_UUID32_ALL;\n\t\t\tptr += 2;\n\t\t}\n\n\t\t/* Stop if not enough space to put next UUID */\n\t\tif ((ptr - data) + sizeof(u32) > len) {\n\t\t\tuuids_start[1] = EIR_UUID32_SOME;\n\t\t\tbreak;\n\t\t}\n\n\t\tmemcpy(ptr, &uuid->uuid[12], sizeof(u32));\n\t\tptr += sizeof(u32);\n\t\tuuids_start[0] += sizeof(u32);\n\t}\n\n\treturn ptr;\n}\n\nstatic u8 *create_uuid128_list(struct hci_dev *hdev, u8 *data, ptrdiff_t len)\n{\n\tu8 *ptr = data, *uuids_start = NULL;\n\tstruct bt_uuid *uuid;\n\n\tif (len < 18)\n\t\treturn ptr;\n\n\tlist_for_each_entry(uuid, &hdev->uuids, list) {\n\t\tif (uuid->size != 128)\n\t\t\tcontinue;\n\n\t\tif (!uuids_start) {\n\t\t\tuuids_start = ptr;\n\t\t\tuuids_start[0] = 1;\n\t\t\tuuids_start[1] = EIR_UUID128_ALL;\n\t\t\tptr += 2;\n\t\t}\n\n\t\t/* Stop if not enough space to put next UUID */\n\t\tif ((ptr - data) + 16 > len) {\n\t\t\tuuids_start[1] = EIR_UUID128_SOME;\n\t\t\tbreak;\n\t\t}\n\n\t\tmemcpy(ptr, uuid->uuid, 16);\n\t\tptr += 16;\n\t\tuuids_start[0] += 16;\n\t}\n\n\treturn ptr;\n}\n\nstatic void create_eir(struct hci_dev *hdev, u8 *data)\n{\n\tu8 *ptr = data;\n\tsize_t name_len;\n\n\tname_len = strlen(hdev->dev_name);\n\n\tif (name_len > 0) {\n\t\t/* EIR Data type */\n\t\tif (name_len > 48) {\n\t\t\tname_len = 48;\n\t\t\tptr[1] = EIR_NAME_SHORT;\n\t\t} else\n\t\t\tptr[1] = EIR_NAME_COMPLETE;\n\n\t\t/* EIR Data length */\n\t\tptr[0] = name_len + 1;\n\n\t\tmemcpy(ptr + 2, hdev->dev_name, name_len);\n\n\t\tptr += (name_len + 2);\n\t}\n\n\tif (hdev->inq_tx_power != HCI_TX_POWER_INVALID) {\n\t\tptr[0] = 2;\n\t\tptr[1] = EIR_TX_POWER;\n\t\tptr[2] = (u8) hdev->inq_tx_power;\n\n\t\tptr += 3;\n\t}\n\n\tif (hdev->devid_source > 0) {\n\t\tptr[0] = 9;\n\t\tptr[1] = EIR_DEVICE_ID;\n\n\t\tput_unaligned_le16(hdev->devid_source, ptr + 2);\n\t\tput_unaligned_le16(hdev->devid_vendor, ptr + 4);\n\t\tput_unaligned_le16(hdev->devid_product, ptr + 6);\n\t\tput_unaligned_le16(hdev->devid_version, ptr + 8);\n\n\t\tptr += 10;\n\t}\n\n\tptr = create_uuid16_list(hdev, ptr, HCI_MAX_EIR_LENGTH - (ptr - data));\n\tptr = create_uuid32_list(hdev, ptr, HCI_MAX_EIR_LENGTH - (ptr - data));\n\tptr = create_uuid128_list(hdev, ptr, HCI_MAX_EIR_LENGTH - (ptr - data));\n}\n\nvoid __hci_req_update_eir(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_write_eir cp;\n\n\tif (!hdev_is_powered(hdev))\n\t\treturn;\n\n\tif (!lmp_ext_inq_capable(hdev))\n\t\treturn;\n\n\tif (!hci_dev_test_flag(hdev, HCI_SSP_ENABLED))\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_SERVICE_CACHE))\n\t\treturn;\n\n\tmemset(&cp, 0, sizeof(cp));\n\n\tcreate_eir(hdev, cp.data);\n\n\tif (memcmp(cp.data, hdev->eir, sizeof(cp.data)) == 0)\n\t\treturn;\n\n\tmemcpy(hdev->eir, cp.data, sizeof(cp.data));\n\n\thci_req_add(req, HCI_OP_WRITE_EIR, sizeof(cp), &cp);\n}\n\nvoid hci_req_add_le_scan_disable(struct hci_request *req, bool rpa_le_conn)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\tif (hdev->scanning_paused) {\n\t\tbt_dev_dbg(hdev, \"Scanning is paused for suspend\");\n\t\treturn;\n\t}\n\n\tif (hdev->suspended)\n\t\tset_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks);\n\n\tif (use_ext_scan(hdev)) {\n\t\tstruct hci_cp_le_set_ext_scan_enable cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\t\tcp.enable = LE_SCAN_DISABLE;\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_ENABLE, sizeof(cp),\n\t\t\t    &cp);\n\t} else {\n\t\tstruct hci_cp_le_set_scan_enable cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\t\tcp.enable = LE_SCAN_DISABLE;\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_ENABLE, sizeof(cp), &cp);\n\t}\n\n\t/* Disable address resolution */\n\tif (use_ll_privacy(hdev) &&\n\t    hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY) &&\n\t    hci_dev_test_flag(hdev, HCI_LL_RPA_RESOLUTION) && !rpa_le_conn) {\n\t\t__u8 enable = 0x00;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_ADDR_RESOLV_ENABLE, 1, &enable);\n\t}\n}\n\nstatic void del_from_white_list(struct hci_request *req, bdaddr_t *bdaddr,\n\t\t\t\tu8 bdaddr_type)\n{\n\tstruct hci_cp_le_del_from_white_list cp;\n\n\tcp.bdaddr_type = bdaddr_type;\n\tbacpy(&cp.bdaddr, bdaddr);\n\n\tbt_dev_dbg(req->hdev, \"Remove %pMR (0x%x) from whitelist\", &cp.bdaddr,\n\t\t   cp.bdaddr_type);\n\thci_req_add(req, HCI_OP_LE_DEL_FROM_WHITE_LIST, sizeof(cp), &cp);\n\n\tif (use_ll_privacy(req->hdev) &&\n\t    hci_dev_test_flag(req->hdev, HCI_ENABLE_LL_PRIVACY)) {\n\t\tstruct smp_irk *irk;\n\n\t\tirk = hci_find_irk_by_addr(req->hdev, bdaddr, bdaddr_type);\n\t\tif (irk) {\n\t\t\tstruct hci_cp_le_del_from_resolv_list cp;\n\n\t\t\tcp.bdaddr_type = bdaddr_type;\n\t\t\tbacpy(&cp.bdaddr, bdaddr);\n\n\t\t\thci_req_add(req, HCI_OP_LE_DEL_FROM_RESOLV_LIST,\n\t\t\t\t    sizeof(cp), &cp);\n\t\t}\n\t}\n}\n\n/* Adds connection to white list if needed. On error, returns -1. */\nstatic int add_to_white_list(struct hci_request *req,\n\t\t\t     struct hci_conn_params *params, u8 *num_entries,\n\t\t\t     bool allow_rpa)\n{\n\tstruct hci_cp_le_add_to_white_list cp;\n\tstruct hci_dev *hdev = req->hdev;\n\n\t/* Already in white list */\n\tif (hci_bdaddr_list_lookup(&hdev->le_white_list, &params->addr,\n\t\t\t\t   params->addr_type))\n\t\treturn 0;\n\n\t/* Select filter policy to accept all advertising */\n\tif (*num_entries >= hdev->le_white_list_size)\n\t\treturn -1;\n\n\t/* White list can not be used with RPAs */\n\tif (!allow_rpa &&\n\t    !hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY) &&\n\t    hci_find_irk_by_addr(hdev, &params->addr, params->addr_type)) {\n\t\treturn -1;\n\t}\n\n\t/* During suspend, only wakeable devices can be in whitelist */\n\tif (hdev->suspended && !hci_conn_test_flag(HCI_CONN_FLAG_REMOTE_WAKEUP,\n\t\t\t\t\t\t   params->current_flags))\n\t\treturn 0;\n\n\t*num_entries += 1;\n\tcp.bdaddr_type = params->addr_type;\n\tbacpy(&cp.bdaddr, &params->addr);\n\n\tbt_dev_dbg(hdev, \"Add %pMR (0x%x) to whitelist\", &cp.bdaddr,\n\t\t   cp.bdaddr_type);\n\thci_req_add(req, HCI_OP_LE_ADD_TO_WHITE_LIST, sizeof(cp), &cp);\n\n\tif (use_ll_privacy(hdev) &&\n\t    hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY)) {\n\t\tstruct smp_irk *irk;\n\n\t\tirk = hci_find_irk_by_addr(hdev, &params->addr,\n\t\t\t\t\t   params->addr_type);\n\t\tif (irk) {\n\t\t\tstruct hci_cp_le_add_to_resolv_list cp;\n\n\t\t\tcp.bdaddr_type = params->addr_type;\n\t\t\tbacpy(&cp.bdaddr, &params->addr);\n\t\t\tmemcpy(cp.peer_irk, irk->val, 16);\n\n\t\t\tif (hci_dev_test_flag(hdev, HCI_PRIVACY))\n\t\t\t\tmemcpy(cp.local_irk, hdev->irk, 16);\n\t\t\telse\n\t\t\t\tmemset(cp.local_irk, 0, 16);\n\n\t\t\thci_req_add(req, HCI_OP_LE_ADD_TO_RESOLV_LIST,\n\t\t\t\t    sizeof(cp), &cp);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic u8 update_white_list(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_conn_params *params;\n\tstruct bdaddr_list *b;\n\tu8 num_entries = 0;\n\tbool pend_conn, pend_report;\n\t/* We allow whitelisting even with RPAs in suspend. In the worst case,\n\t * we won't be able to wake from devices that use the privacy1.2\n\t * features. Additionally, once we support privacy1.2 and IRK\n\t * offloading, we can update this to also check for those conditions.\n\t */\n\tbool allow_rpa = hdev->suspended;\n\n\t/* Go through the current white list programmed into the\n\t * controller one by one and check if that address is still\n\t * in the list of pending connections or list of devices to\n\t * report. If not present in either list, then queue the\n\t * command to remove it from the controller.\n\t */\n\tlist_for_each_entry(b, &hdev->le_white_list, list) {\n\t\tpend_conn = hci_pend_le_action_lookup(&hdev->pend_le_conns,\n\t\t\t\t\t\t      &b->bdaddr,\n\t\t\t\t\t\t      b->bdaddr_type);\n\t\tpend_report = hci_pend_le_action_lookup(&hdev->pend_le_reports,\n\t\t\t\t\t\t\t&b->bdaddr,\n\t\t\t\t\t\t\tb->bdaddr_type);\n\n\t\t/* If the device is not likely to connect or report,\n\t\t * remove it from the whitelist.\n\t\t */\n\t\tif (!pend_conn && !pend_report) {\n\t\t\tdel_from_white_list(req, &b->bdaddr, b->bdaddr_type);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* White list can not be used with RPAs */\n\t\tif (!allow_rpa &&\n\t\t    !hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY) &&\n\t\t    hci_find_irk_by_addr(hdev, &b->bdaddr, b->bdaddr_type)) {\n\t\t\treturn 0x00;\n\t\t}\n\n\t\tnum_entries++;\n\t}\n\n\t/* Since all no longer valid white list entries have been\n\t * removed, walk through the list of pending connections\n\t * and ensure that any new device gets programmed into\n\t * the controller.\n\t *\n\t * If the list of the devices is larger than the list of\n\t * available white list entries in the controller, then\n\t * just abort and return filer policy value to not use the\n\t * white list.\n\t */\n\tlist_for_each_entry(params, &hdev->pend_le_conns, action) {\n\t\tif (add_to_white_list(req, params, &num_entries, allow_rpa))\n\t\t\treturn 0x00;\n\t}\n\n\t/* After adding all new pending connections, walk through\n\t * the list of pending reports and also add these to the\n\t * white list if there is still space. Abort if space runs out.\n\t */\n\tlist_for_each_entry(params, &hdev->pend_le_reports, action) {\n\t\tif (add_to_white_list(req, params, &num_entries, allow_rpa))\n\t\t\treturn 0x00;\n\t}\n\n\t/* Use the allowlist unless the following conditions are all true:\n\t * - We are not currently suspending\n\t * - There are 1 or more ADV monitors registered and it's not offloaded\n\t * - Interleaved scanning is not currently using the allowlist\n\t */\n\tif (!idr_is_empty(&hdev->adv_monitors_idr) && !hdev->suspended &&\n\t    hci_get_adv_monitor_offload_ext(hdev) == HCI_ADV_MONITOR_EXT_NONE &&\n\t    hdev->interleave_scan_state != INTERLEAVE_SCAN_ALLOWLIST)\n\t\treturn 0x00;\n\n\t/* Select filter policy to use white list */\n\treturn 0x01;\n}\n\nstatic bool scan_use_rpa(struct hci_dev *hdev)\n{\n\treturn hci_dev_test_flag(hdev, HCI_PRIVACY);\n}\n\nstatic void hci_req_start_scan(struct hci_request *req, u8 type, u16 interval,\n\t\t\t       u16 window, u8 own_addr_type, u8 filter_policy,\n\t\t\t       bool addr_resolv)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\tif (hdev->scanning_paused) {\n\t\tbt_dev_dbg(hdev, \"Scanning is paused for suspend\");\n\t\treturn;\n\t}\n\n\tif (use_ll_privacy(hdev) &&\n\t    hci_dev_test_flag(hdev, HCI_ENABLE_LL_PRIVACY) &&\n\t    addr_resolv) {\n\t\tu8 enable = 0x01;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_ADDR_RESOLV_ENABLE, 1, &enable);\n\t}\n\n\t/* Use ext scanning if set ext scan param and ext scan enable is\n\t * supported\n\t */\n\tif (use_ext_scan(hdev)) {\n\t\tstruct hci_cp_le_set_ext_scan_params *ext_param_cp;\n\t\tstruct hci_cp_le_set_ext_scan_enable ext_enable_cp;\n\t\tstruct hci_cp_le_scan_phy_params *phy_params;\n\t\tu8 data[sizeof(*ext_param_cp) + sizeof(*phy_params) * 2];\n\t\tu32 plen;\n\n\t\text_param_cp = (void *)data;\n\t\tphy_params = (void *)ext_param_cp->data;\n\n\t\tmemset(ext_param_cp, 0, sizeof(*ext_param_cp));\n\t\text_param_cp->own_addr_type = own_addr_type;\n\t\text_param_cp->filter_policy = filter_policy;\n\n\t\tplen = sizeof(*ext_param_cp);\n\n\t\tif (scan_1m(hdev) || scan_2m(hdev)) {\n\t\t\text_param_cp->scanning_phys |= LE_SCAN_PHY_1M;\n\n\t\t\tmemset(phy_params, 0, sizeof(*phy_params));\n\t\t\tphy_params->type = type;\n\t\t\tphy_params->interval = cpu_to_le16(interval);\n\t\t\tphy_params->window = cpu_to_le16(window);\n\n\t\t\tplen += sizeof(*phy_params);\n\t\t\tphy_params++;\n\t\t}\n\n\t\tif (scan_coded(hdev)) {\n\t\t\text_param_cp->scanning_phys |= LE_SCAN_PHY_CODED;\n\n\t\t\tmemset(phy_params, 0, sizeof(*phy_params));\n\t\t\tphy_params->type = type;\n\t\t\tphy_params->interval = cpu_to_le16(interval);\n\t\t\tphy_params->window = cpu_to_le16(window);\n\n\t\t\tplen += sizeof(*phy_params);\n\t\t\tphy_params++;\n\t\t}\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_PARAMS,\n\t\t\t    plen, ext_param_cp);\n\n\t\tmemset(&ext_enable_cp, 0, sizeof(ext_enable_cp));\n\t\text_enable_cp.enable = LE_SCAN_ENABLE;\n\t\text_enable_cp.filter_dup = LE_SCAN_FILTER_DUP_ENABLE;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_ENABLE,\n\t\t\t    sizeof(ext_enable_cp), &ext_enable_cp);\n\t} else {\n\t\tstruct hci_cp_le_set_scan_param param_cp;\n\t\tstruct hci_cp_le_set_scan_enable enable_cp;\n\n\t\tmemset(&param_cp, 0, sizeof(param_cp));\n\t\tparam_cp.type = type;\n\t\tparam_cp.interval = cpu_to_le16(interval);\n\t\tparam_cp.window = cpu_to_le16(window);\n\t\tparam_cp.own_address_type = own_addr_type;\n\t\tparam_cp.filter_policy = filter_policy;\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_PARAM, sizeof(param_cp),\n\t\t\t    &param_cp);\n\n\t\tmemset(&enable_cp, 0, sizeof(enable_cp));\n\t\tenable_cp.enable = LE_SCAN_ENABLE;\n\t\tenable_cp.filter_dup = LE_SCAN_FILTER_DUP_ENABLE;\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_ENABLE, sizeof(enable_cp),\n\t\t\t    &enable_cp);\n\t}\n}\n\n/* Returns true if an le connection is in the scanning state */\nstatic inline bool hci_is_le_conn_scanning(struct hci_dev *hdev)\n{\n\tstruct hci_conn_hash *h = &hdev->conn_hash;\n\tstruct hci_conn  *c;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(c, &h->list, list) {\n\t\tif (c->type == LE_LINK && c->state == BT_CONNECT &&\n\t\t    test_bit(HCI_CONN_SCANNING, &c->flags)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\n\treturn false;\n}\n\n/* Ensure to call hci_req_add_le_scan_disable() first to disable the\n * controller based address resolution to be able to reconfigure\n * resolving list.\n */\nvoid hci_req_add_le_passive_scan(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 own_addr_type;\n\tu8 filter_policy;\n\tu16 window, interval;\n\t/* Background scanning should run with address resolution */\n\tbool addr_resolv = true;\n\n\tif (hdev->scanning_paused) {\n\t\tbt_dev_dbg(hdev, \"Scanning is paused for suspend\");\n\t\treturn;\n\t}\n\n\t/* Set require_privacy to false since no SCAN_REQ are send\n\t * during passive scanning. Not using an non-resolvable address\n\t * here is important so that peer devices using direct\n\t * advertising with our address will be correctly reported\n\t * by the controller.\n\t */\n\tif (hci_update_random_address(req, false, scan_use_rpa(hdev),\n\t\t\t\t      &own_addr_type))\n\t\treturn;\n\n\tif (hdev->enable_advmon_interleave_scan &&\n\t    __hci_update_interleaved_scan(hdev))\n\t\treturn;\n\n\tbt_dev_dbg(hdev, \"interleave state %d\", hdev->interleave_scan_state);\n\t/* Adding or removing entries from the white list must\n\t * happen before enabling scanning. The controller does\n\t * not allow white list modification while scanning.\n\t */\n\tfilter_policy = update_white_list(req);\n\n\t/* When the controller is using random resolvable addresses and\n\t * with that having LE privacy enabled, then controllers with\n\t * Extended Scanner Filter Policies support can now enable support\n\t * for handling directed advertising.\n\t *\n\t * So instead of using filter polices 0x00 (no whitelist)\n\t * and 0x01 (whitelist enabled) use the new filter policies\n\t * 0x02 (no whitelist) and 0x03 (whitelist enabled).\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_PRIVACY) &&\n\t    (hdev->le_features[0] & HCI_LE_EXT_SCAN_POLICY))\n\t\tfilter_policy |= 0x02;\n\n\tif (hdev->suspended) {\n\t\twindow = hdev->le_scan_window_suspend;\n\t\tinterval = hdev->le_scan_int_suspend;\n\n\t\tset_bit(SUSPEND_SCAN_ENABLE, hdev->suspend_tasks);\n\t} else if (hci_is_le_conn_scanning(hdev)) {\n\t\twindow = hdev->le_scan_window_connect;\n\t\tinterval = hdev->le_scan_int_connect;\n\t} else if (hci_is_adv_monitoring(hdev)) {\n\t\twindow = hdev->le_scan_window_adv_monitor;\n\t\tinterval = hdev->le_scan_int_adv_monitor;\n\t} else {\n\t\twindow = hdev->le_scan_window;\n\t\tinterval = hdev->le_scan_interval;\n\t}\n\n\tbt_dev_dbg(hdev, \"LE passive scan with whitelist = %d\", filter_policy);\n\thci_req_start_scan(req, LE_SCAN_PASSIVE, interval, window,\n\t\t\t   own_addr_type, filter_policy, addr_resolv);\n}\n\nstatic bool adv_instance_is_scannable(struct hci_dev *hdev, u8 instance)\n{\n\tstruct adv_info *adv_instance;\n\n\t/* Instance 0x00 always set local name */\n\tif (instance == 0x00)\n\t\treturn true;\n\n\tadv_instance = hci_find_adv_instance(hdev, instance);\n\tif (!adv_instance)\n\t\treturn false;\n\n\tif (adv_instance->flags & MGMT_ADV_FLAG_APPEARANCE ||\n\t    adv_instance->flags & MGMT_ADV_FLAG_LOCAL_NAME)\n\t\treturn true;\n\n\treturn adv_instance->scan_rsp_len ? true : false;\n}\n\nstatic void hci_req_clear_event_filter(struct hci_request *req)\n{\n\tstruct hci_cp_set_event_filter f;\n\n\tmemset(&f, 0, sizeof(f));\n\tf.flt_type = HCI_FLT_CLEAR_ALL;\n\thci_req_add(req, HCI_OP_SET_EVENT_FLT, 1, &f);\n\n\t/* Update page scan state (since we may have modified it when setting\n\t * the event filter).\n\t */\n\t__hci_req_update_scan(req);\n}\n\nstatic void hci_req_set_event_filter(struct hci_request *req)\n{\n\tstruct bdaddr_list_with_flags *b;\n\tstruct hci_cp_set_event_filter f;\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 scan = SCAN_DISABLED;\n\n\t/* Always clear event filter when starting */\n\thci_req_clear_event_filter(req);\n\n\tlist_for_each_entry(b, &hdev->whitelist, list) {\n\t\tif (!hci_conn_test_flag(HCI_CONN_FLAG_REMOTE_WAKEUP,\n\t\t\t\t\tb->current_flags))\n\t\t\tcontinue;\n\n\t\tmemset(&f, 0, sizeof(f));\n\t\tbacpy(&f.addr_conn_flt.bdaddr, &b->bdaddr);\n\t\tf.flt_type = HCI_FLT_CONN_SETUP;\n\t\tf.cond_type = HCI_CONN_SETUP_ALLOW_BDADDR;\n\t\tf.addr_conn_flt.auto_accept = HCI_CONN_SETUP_AUTO_ON;\n\n\t\tbt_dev_dbg(hdev, \"Adding event filters for %pMR\", &b->bdaddr);\n\t\thci_req_add(req, HCI_OP_SET_EVENT_FLT, sizeof(f), &f);\n\t\tscan = SCAN_PAGE;\n\t}\n\n\tif (scan)\n\t\tset_bit(SUSPEND_SCAN_ENABLE, hdev->suspend_tasks);\n\telse\n\t\tset_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks);\n\n\thci_req_add(req, HCI_OP_WRITE_SCAN_ENABLE, 1, &scan);\n}\n\nstatic void cancel_adv_timeout(struct hci_dev *hdev)\n{\n\tif (hdev->adv_instance_timeout) {\n\t\thdev->adv_instance_timeout = 0;\n\t\tcancel_delayed_work(&hdev->adv_instance_expire);\n\t}\n}\n\n/* This function requires the caller holds hdev->lock */\nvoid __hci_req_pause_adv_instances(struct hci_request *req)\n{\n\tbt_dev_dbg(req->hdev, \"Pausing advertising instances\");\n\n\t/* Call to disable any advertisements active on the controller.\n\t * This will succeed even if no advertisements are configured.\n\t */\n\t__hci_req_disable_advertising(req);\n\n\t/* If we are using software rotation, pause the loop */\n\tif (!ext_adv_capable(req->hdev))\n\t\tcancel_adv_timeout(req->hdev);\n}\n\n/* This function requires the caller holds hdev->lock */\nstatic void __hci_req_resume_adv_instances(struct hci_request *req)\n{\n\tstruct adv_info *adv;\n\n\tbt_dev_dbg(req->hdev, \"Resuming advertising instances\");\n\n\tif (ext_adv_capable(req->hdev)) {\n\t\t/* Call for each tracked instance to be re-enabled */\n\t\tlist_for_each_entry(adv, &req->hdev->adv_instances, list) {\n\t\t\t__hci_req_enable_ext_advertising(req,\n\t\t\t\t\t\t\t adv->instance);\n\t\t}\n\n\t} else {\n\t\t/* Schedule for most recent instance to be restarted and begin\n\t\t * the software rotation loop\n\t\t */\n\t\t__hci_req_schedule_adv_instance(req,\n\t\t\t\t\t\treq->hdev->cur_adv_instance,\n\t\t\t\t\t\ttrue);\n\t}\n}\n\n/* This function requires the caller holds hdev->lock */\nint hci_req_resume_adv_instances(struct hci_dev *hdev)\n{\n\tstruct hci_request req;\n\n\thci_req_init(&req, hdev);\n\t__hci_req_resume_adv_instances(&req);\n\n\treturn hci_req_run(&req, NULL);\n}\n\nstatic void suspend_req_complete(struct hci_dev *hdev, u8 status, u16 opcode)\n{\n\tbt_dev_dbg(hdev, \"Request complete opcode=0x%x, status=0x%x\", opcode,\n\t\t   status);\n\tif (test_bit(SUSPEND_SCAN_ENABLE, hdev->suspend_tasks) ||\n\t    test_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks)) {\n\t\tclear_bit(SUSPEND_SCAN_ENABLE, hdev->suspend_tasks);\n\t\tclear_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks);\n\t\twake_up(&hdev->suspend_wait_q);\n\t}\n\n\tif (test_bit(SUSPEND_SET_ADV_FILTER, hdev->suspend_tasks)) {\n\t\tclear_bit(SUSPEND_SET_ADV_FILTER, hdev->suspend_tasks);\n\t\twake_up(&hdev->suspend_wait_q);\n\t}\n}\n\nstatic void hci_req_add_set_adv_filter_enable(struct hci_request *req,\n\t\t\t\t\t      bool enable)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\tswitch (hci_get_adv_monitor_offload_ext(hdev)) {\n\tcase HCI_ADV_MONITOR_EXT_MSFT:\n\t\tmsft_req_add_set_filter_enable(req, enable);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\t/* No need to block when enabling since it's on resume path */\n\tif (hdev->suspended && !enable)\n\t\tset_bit(SUSPEND_SET_ADV_FILTER, hdev->suspend_tasks);\n}\n\n/* Call with hci_dev_lock */\nvoid hci_req_prepare_suspend(struct hci_dev *hdev, enum suspended_state next)\n{\n\tint old_state;\n\tstruct hci_conn *conn;\n\tstruct hci_request req;\n\tu8 page_scan;\n\tint disconnect_counter;\n\n\tif (next == hdev->suspend_state) {\n\t\tbt_dev_dbg(hdev, \"Same state before and after: %d\", next);\n\t\tgoto done;\n\t}\n\n\thdev->suspend_state = next;\n\thci_req_init(&req, hdev);\n\n\tif (next == BT_SUSPEND_DISCONNECT) {\n\t\t/* Mark device as suspended */\n\t\thdev->suspended = true;\n\n\t\t/* Pause discovery if not already stopped */\n\t\told_state = hdev->discovery.state;\n\t\tif (old_state != DISCOVERY_STOPPED) {\n\t\t\tset_bit(SUSPEND_PAUSE_DISCOVERY, hdev->suspend_tasks);\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_STOPPING);\n\t\t\tqueue_work(hdev->req_workqueue, &hdev->discov_update);\n\t\t}\n\n\t\thdev->discovery_paused = true;\n\t\thdev->discovery_old_state = old_state;\n\n\t\t/* Stop directed advertising */\n\t\told_state = hci_dev_test_flag(hdev, HCI_ADVERTISING);\n\t\tif (old_state) {\n\t\t\tset_bit(SUSPEND_PAUSE_ADVERTISING, hdev->suspend_tasks);\n\t\t\tcancel_delayed_work(&hdev->discov_off);\n\t\t\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t\t\t   &hdev->discov_off, 0);\n\t\t}\n\n\t\t/* Pause other advertisements */\n\t\tif (hdev->adv_instance_cnt)\n\t\t\t__hci_req_pause_adv_instances(&req);\n\n\t\thdev->advertising_paused = true;\n\t\thdev->advertising_old_state = old_state;\n\t\t/* Disable page scan */\n\t\tpage_scan = SCAN_DISABLED;\n\t\thci_req_add(&req, HCI_OP_WRITE_SCAN_ENABLE, 1, &page_scan);\n\n\t\t/* Disable LE passive scan if enabled */\n\t\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN)) {\n\t\t\tcancel_interleave_scan(hdev);\n\t\t\thci_req_add_le_scan_disable(&req, false);\n\t\t}\n\n\t\t/* Disable advertisement filters */\n\t\thci_req_add_set_adv_filter_enable(&req, false);\n\n\t\t/* Mark task needing completion */\n\t\tset_bit(SUSPEND_SCAN_DISABLE, hdev->suspend_tasks);\n\n\t\t/* Prevent disconnects from causing scanning to be re-enabled */\n\t\thdev->scanning_paused = true;\n\n\t\t/* Run commands before disconnecting */\n\t\thci_req_run(&req, suspend_req_complete);\n\n\t\tdisconnect_counter = 0;\n\t\t/* Soft disconnect everything (power off) */\n\t\tlist_for_each_entry(conn, &hdev->conn_hash.list, list) {\n\t\t\thci_disconnect(conn, HCI_ERROR_REMOTE_POWER_OFF);\n\t\t\tdisconnect_counter++;\n\t\t}\n\n\t\tif (disconnect_counter > 0) {\n\t\t\tbt_dev_dbg(hdev,\n\t\t\t\t   \"Had %d disconnects. Will wait on them\",\n\t\t\t\t   disconnect_counter);\n\t\t\tset_bit(SUSPEND_DISCONNECTING, hdev->suspend_tasks);\n\t\t}\n\t} else if (next == BT_SUSPEND_CONFIGURE_WAKE) {\n\t\t/* Unpause to take care of updating scanning params */\n\t\thdev->scanning_paused = false;\n\t\t/* Enable event filter for paired devices */\n\t\thci_req_set_event_filter(&req);\n\t\t/* Enable passive scan at lower duty cycle */\n\t\t__hci_update_background_scan(&req);\n\t\t/* Pause scan changes again. */\n\t\thdev->scanning_paused = true;\n\t\thci_req_run(&req, suspend_req_complete);\n\t} else {\n\t\thdev->suspended = false;\n\t\thdev->scanning_paused = false;\n\n\t\thci_req_clear_event_filter(&req);\n\t\t/* Reset passive/background scanning to normal */\n\t\t__hci_update_background_scan(&req);\n\t\t/* Enable all of the advertisement filters */\n\t\thci_req_add_set_adv_filter_enable(&req, true);\n\n\t\t/* Unpause directed advertising */\n\t\thdev->advertising_paused = false;\n\t\tif (hdev->advertising_old_state) {\n\t\t\tset_bit(SUSPEND_UNPAUSE_ADVERTISING,\n\t\t\t\thdev->suspend_tasks);\n\t\t\thci_dev_set_flag(hdev, HCI_ADVERTISING);\n\t\t\tqueue_work(hdev->req_workqueue,\n\t\t\t\t   &hdev->discoverable_update);\n\t\t\thdev->advertising_old_state = 0;\n\t\t}\n\n\t\t/* Resume other advertisements */\n\t\tif (hdev->adv_instance_cnt)\n\t\t\t__hci_req_resume_adv_instances(&req);\n\n\t\t/* Unpause discovery */\n\t\thdev->discovery_paused = false;\n\t\tif (hdev->discovery_old_state != DISCOVERY_STOPPED &&\n\t\t    hdev->discovery_old_state != DISCOVERY_STOPPING) {\n\t\t\tset_bit(SUSPEND_UNPAUSE_DISCOVERY, hdev->suspend_tasks);\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_STARTING);\n\t\t\tqueue_work(hdev->req_workqueue, &hdev->discov_update);\n\t\t}\n\n\t\thci_req_run(&req, suspend_req_complete);\n\t}\n\n\thdev->suspend_state = next;\n\ndone:\n\tclear_bit(SUSPEND_PREPARE_NOTIFIER, hdev->suspend_tasks);\n\twake_up(&hdev->suspend_wait_q);\n}\n\nstatic bool adv_cur_instance_is_scannable(struct hci_dev *hdev)\n{\n\treturn adv_instance_is_scannable(hdev, hdev->cur_adv_instance);\n}\n\nvoid __hci_req_disable_advertising(struct hci_request *req)\n{\n\tif (ext_adv_capable(req->hdev)) {\n\t\t__hci_req_disable_ext_adv_instance(req, 0x00);\n\n\t} else {\n\t\tu8 enable = 0x00;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_ADV_ENABLE, sizeof(enable), &enable);\n\t}\n}\n\nstatic u32 get_adv_instance_flags(struct hci_dev *hdev, u8 instance)\n{\n\tu32 flags;\n\tstruct adv_info *adv_instance;\n\n\tif (instance == 0x00) {\n\t\t/* Instance 0 always manages the \"Tx Power\" and \"Flags\"\n\t\t * fields\n\t\t */\n\t\tflags = MGMT_ADV_FLAG_TX_POWER | MGMT_ADV_FLAG_MANAGED_FLAGS;\n\n\t\t/* For instance 0, the HCI_ADVERTISING_CONNECTABLE setting\n\t\t * corresponds to the \"connectable\" instance flag.\n\t\t */\n\t\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING_CONNECTABLE))\n\t\t\tflags |= MGMT_ADV_FLAG_CONNECTABLE;\n\n\t\tif (hci_dev_test_flag(hdev, HCI_LIMITED_DISCOVERABLE))\n\t\t\tflags |= MGMT_ADV_FLAG_LIMITED_DISCOV;\n\t\telse if (hci_dev_test_flag(hdev, HCI_DISCOVERABLE))\n\t\t\tflags |= MGMT_ADV_FLAG_DISCOV;\n\n\t\treturn flags;\n\t}\n\n\tadv_instance = hci_find_adv_instance(hdev, instance);\n\n\t/* Return 0 when we got an invalid instance identifier. */\n\tif (!adv_instance)\n\t\treturn 0;\n\n\treturn adv_instance->flags;\n}\n\nstatic bool adv_use_rpa(struct hci_dev *hdev, uint32_t flags)\n{\n\t/* If privacy is not enabled don't use RPA */\n\tif (!hci_dev_test_flag(hdev, HCI_PRIVACY))\n\t\treturn false;\n\n\t/* If basic privacy mode is enabled use RPA */\n\tif (!hci_dev_test_flag(hdev, HCI_LIMITED_PRIVACY))\n\t\treturn true;\n\n\t/* If limited privacy mode is enabled don't use RPA if we're\n\t * both discoverable and bondable.\n\t */\n\tif ((flags & MGMT_ADV_FLAG_DISCOV) &&\n\t    hci_dev_test_flag(hdev, HCI_BONDABLE))\n\t\treturn false;\n\n\t/* We're neither bondable nor discoverable in the limited\n\t * privacy mode, therefore use RPA.\n\t */\n\treturn true;\n}\n\nstatic bool is_advertising_allowed(struct hci_dev *hdev, bool connectable)\n{\n\t/* If there is no connection we are OK to advertise. */\n\tif (hci_conn_num(hdev, LE_LINK) == 0)\n\t\treturn true;\n\n\t/* Check le_states if there is any connection in slave role. */\n\tif (hdev->conn_hash.le_num_slave > 0) {\n\t\t/* Slave connection state and non connectable mode bit 20. */\n\t\tif (!connectable && !(hdev->le_states[2] & 0x10))\n\t\t\treturn false;\n\n\t\t/* Slave connection state and connectable mode bit 38\n\t\t * and scannable bit 21.\n\t\t */\n\t\tif (connectable && (!(hdev->le_states[4] & 0x40) ||\n\t\t\t\t    !(hdev->le_states[2] & 0x20)))\n\t\t\treturn false;\n\t}\n\n\t/* Check le_states if there is any connection in master role. */\n\tif (hci_conn_num(hdev, LE_LINK) != hdev->conn_hash.le_num_slave) {\n\t\t/* Master connection state and non connectable mode bit 18. */\n\t\tif (!connectable && !(hdev->le_states[2] & 0x02))\n\t\t\treturn false;\n\n\t\t/* Master connection state and connectable mode bit 35 and\n\t\t * scannable 19.\n\t\t */\n\t\tif (connectable && (!(hdev->le_states[4] & 0x08) ||\n\t\t\t\t    !(hdev->le_states[2] & 0x08)))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nvoid __hci_req_enable_advertising(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct adv_info *adv_instance;\n\tstruct hci_cp_le_set_adv_param cp;\n\tu8 own_addr_type, enable = 0x01;\n\tbool connectable;\n\tu16 adv_min_interval, adv_max_interval;\n\tu32 flags;\n\n\tflags = get_adv_instance_flags(hdev, hdev->cur_adv_instance);\n\tadv_instance = hci_find_adv_instance(hdev, hdev->cur_adv_instance);\n\n\t/* If the \"connectable\" instance flag was not set, then choose between\n\t * ADV_IND and ADV_NONCONN_IND based on the global connectable setting.\n\t */\n\tconnectable = (flags & MGMT_ADV_FLAG_CONNECTABLE) ||\n\t\t      mgmt_get_connectable(hdev);\n\n\tif (!is_advertising_allowed(hdev, connectable))\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_LE_ADV))\n\t\t__hci_req_disable_advertising(req);\n\n\t/* Clear the HCI_LE_ADV bit temporarily so that the\n\t * hci_update_random_address knows that it's safe to go ahead\n\t * and write a new random address. The flag will be set back on\n\t * as soon as the SET_ADV_ENABLE HCI command completes.\n\t */\n\thci_dev_clear_flag(hdev, HCI_LE_ADV);\n\n\t/* Set require_privacy to true only when non-connectable\n\t * advertising is used. In that case it is fine to use a\n\t * non-resolvable private address.\n\t */\n\tif (hci_update_random_address(req, !connectable,\n\t\t\t\t      adv_use_rpa(hdev, flags),\n\t\t\t\t      &own_addr_type) < 0)\n\t\treturn;\n\n\tmemset(&cp, 0, sizeof(cp));\n\n\tif (adv_instance) {\n\t\tadv_min_interval = adv_instance->min_interval;\n\t\tadv_max_interval = adv_instance->max_interval;\n\t} else {\n\t\tadv_min_interval = hdev->le_adv_min_interval;\n\t\tadv_max_interval = hdev->le_adv_max_interval;\n\t}\n\n\tif (connectable) {\n\t\tcp.type = LE_ADV_IND;\n\t} else {\n\t\tif (adv_cur_instance_is_scannable(hdev))\n\t\t\tcp.type = LE_ADV_SCAN_IND;\n\t\telse\n\t\t\tcp.type = LE_ADV_NONCONN_IND;\n\n\t\tif (!hci_dev_test_flag(hdev, HCI_DISCOVERABLE) ||\n\t\t    hci_dev_test_flag(hdev, HCI_LIMITED_DISCOVERABLE)) {\n\t\t\tadv_min_interval = DISCOV_LE_FAST_ADV_INT_MIN;\n\t\t\tadv_max_interval = DISCOV_LE_FAST_ADV_INT_MAX;\n\t\t}\n\t}\n\n\tcp.min_interval = cpu_to_le16(adv_min_interval);\n\tcp.max_interval = cpu_to_le16(adv_max_interval);\n\tcp.own_address_type = own_addr_type;\n\tcp.channel_map = hdev->le_adv_channel_map;\n\n\thci_req_add(req, HCI_OP_LE_SET_ADV_PARAM, sizeof(cp), &cp);\n\n\thci_req_add(req, HCI_OP_LE_SET_ADV_ENABLE, sizeof(enable), &enable);\n}\n\nu8 append_local_name(struct hci_dev *hdev, u8 *ptr, u8 ad_len)\n{\n\tsize_t short_len;\n\tsize_t complete_len;\n\n\t/* no space left for name (+ NULL + type + len) */\n\tif ((HCI_MAX_AD_LENGTH - ad_len) < HCI_MAX_SHORT_NAME_LENGTH + 3)\n\t\treturn ad_len;\n\n\t/* use complete name if present and fits */\n\tcomplete_len = strlen(hdev->dev_name);\n\tif (complete_len && complete_len <= HCI_MAX_SHORT_NAME_LENGTH)\n\t\treturn eir_append_data(ptr, ad_len, EIR_NAME_COMPLETE,\n\t\t\t\t       hdev->dev_name, complete_len + 1);\n\n\t/* use short name if present */\n\tshort_len = strlen(hdev->short_name);\n\tif (short_len)\n\t\treturn eir_append_data(ptr, ad_len, EIR_NAME_SHORT,\n\t\t\t\t       hdev->short_name, short_len + 1);\n\n\t/* use shortened full name if present, we already know that name\n\t * is longer then HCI_MAX_SHORT_NAME_LENGTH\n\t */\n\tif (complete_len) {\n\t\tu8 name[HCI_MAX_SHORT_NAME_LENGTH + 1];\n\n\t\tmemcpy(name, hdev->dev_name, HCI_MAX_SHORT_NAME_LENGTH);\n\t\tname[HCI_MAX_SHORT_NAME_LENGTH] = '\\0';\n\n\t\treturn eir_append_data(ptr, ad_len, EIR_NAME_SHORT, name,\n\t\t\t\t       sizeof(name));\n\t}\n\n\treturn ad_len;\n}\n\nstatic u8 append_appearance(struct hci_dev *hdev, u8 *ptr, u8 ad_len)\n{\n\treturn eir_append_le16(ptr, ad_len, EIR_APPEARANCE, hdev->appearance);\n}\n\nstatic u8 create_default_scan_rsp_data(struct hci_dev *hdev, u8 *ptr)\n{\n\tu8 scan_rsp_len = 0;\n\n\tif (hdev->appearance) {\n\t\tscan_rsp_len = append_appearance(hdev, ptr, scan_rsp_len);\n\t}\n\n\treturn append_local_name(hdev, ptr, scan_rsp_len);\n}\n\nstatic u8 create_instance_scan_rsp_data(struct hci_dev *hdev, u8 instance,\n\t\t\t\t\tu8 *ptr)\n{\n\tstruct adv_info *adv_instance;\n\tu32 instance_flags;\n\tu8 scan_rsp_len = 0;\n\n\tadv_instance = hci_find_adv_instance(hdev, instance);\n\tif (!adv_instance)\n\t\treturn 0;\n\n\tinstance_flags = adv_instance->flags;\n\n\tif ((instance_flags & MGMT_ADV_FLAG_APPEARANCE) && hdev->appearance) {\n\t\tscan_rsp_len = append_appearance(hdev, ptr, scan_rsp_len);\n\t}\n\n\tmemcpy(&ptr[scan_rsp_len], adv_instance->scan_rsp_data,\n\t       adv_instance->scan_rsp_len);\n\n\tscan_rsp_len += adv_instance->scan_rsp_len;\n\n\tif (instance_flags & MGMT_ADV_FLAG_LOCAL_NAME)\n\t\tscan_rsp_len = append_local_name(hdev, ptr, scan_rsp_len);\n\n\treturn scan_rsp_len;\n}\n\nvoid __hci_req_update_scan_rsp_data(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 len;\n\n\tif (!hci_dev_test_flag(hdev, HCI_LE_ENABLED))\n\t\treturn;\n\n\tif (ext_adv_capable(hdev)) {\n\t\tstruct hci_cp_le_set_ext_scan_rsp_data cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tif (instance)\n\t\t\tlen = create_instance_scan_rsp_data(hdev, instance,\n\t\t\t\t\t\t\t    cp.data);\n\t\telse\n\t\t\tlen = create_default_scan_rsp_data(hdev, cp.data);\n\n\t\tif (hdev->scan_rsp_data_len == len &&\n\t\t    !memcmp(cp.data, hdev->scan_rsp_data, len))\n\t\t\treturn;\n\n\t\tmemcpy(hdev->scan_rsp_data, cp.data, sizeof(cp.data));\n\t\thdev->scan_rsp_data_len = len;\n\n\t\tcp.handle = instance;\n\t\tcp.length = len;\n\t\tcp.operation = LE_SET_ADV_DATA_OP_COMPLETE;\n\t\tcp.frag_pref = LE_SET_ADV_DATA_NO_FRAG;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_RSP_DATA, sizeof(cp),\n\t\t\t    &cp);\n\t} else {\n\t\tstruct hci_cp_le_set_scan_rsp_data cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tif (instance)\n\t\t\tlen = create_instance_scan_rsp_data(hdev, instance,\n\t\t\t\t\t\t\t    cp.data);\n\t\telse\n\t\t\tlen = create_default_scan_rsp_data(hdev, cp.data);\n\n\t\tif (hdev->scan_rsp_data_len == len &&\n\t\t    !memcmp(cp.data, hdev->scan_rsp_data, len))\n\t\t\treturn;\n\n\t\tmemcpy(hdev->scan_rsp_data, cp.data, sizeof(cp.data));\n\t\thdev->scan_rsp_data_len = len;\n\n\t\tcp.length = len;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_RSP_DATA, sizeof(cp), &cp);\n\t}\n}\n\nstatic u8 create_instance_adv_data(struct hci_dev *hdev, u8 instance, u8 *ptr)\n{\n\tstruct adv_info *adv_instance = NULL;\n\tu8 ad_len = 0, flags = 0;\n\tu32 instance_flags;\n\n\t/* Return 0 when the current instance identifier is invalid. */\n\tif (instance) {\n\t\tadv_instance = hci_find_adv_instance(hdev, instance);\n\t\tif (!adv_instance)\n\t\t\treturn 0;\n\t}\n\n\tinstance_flags = get_adv_instance_flags(hdev, instance);\n\n\t/* If instance already has the flags set skip adding it once\n\t * again.\n\t */\n\tif (adv_instance && eir_get_data(adv_instance->adv_data,\n\t\t\t\t\t adv_instance->adv_data_len, EIR_FLAGS,\n\t\t\t\t\t NULL))\n\t\tgoto skip_flags;\n\n\t/* The Add Advertising command allows userspace to set both the general\n\t * and limited discoverable flags.\n\t */\n\tif (instance_flags & MGMT_ADV_FLAG_DISCOV)\n\t\tflags |= LE_AD_GENERAL;\n\n\tif (instance_flags & MGMT_ADV_FLAG_LIMITED_DISCOV)\n\t\tflags |= LE_AD_LIMITED;\n\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\tflags |= LE_AD_NO_BREDR;\n\n\tif (flags || (instance_flags & MGMT_ADV_FLAG_MANAGED_FLAGS)) {\n\t\t/* If a discovery flag wasn't provided, simply use the global\n\t\t * settings.\n\t\t */\n\t\tif (!flags)\n\t\t\tflags |= mgmt_get_adv_discov_flags(hdev);\n\n\t\t/* If flags would still be empty, then there is no need to\n\t\t * include the \"Flags\" AD field\".\n\t\t */\n\t\tif (flags) {\n\t\t\tptr[0] = 0x02;\n\t\t\tptr[1] = EIR_FLAGS;\n\t\t\tptr[2] = flags;\n\n\t\t\tad_len += 3;\n\t\t\tptr += 3;\n\t\t}\n\t}\n\nskip_flags:\n\tif (adv_instance) {\n\t\tmemcpy(ptr, adv_instance->adv_data,\n\t\t       adv_instance->adv_data_len);\n\t\tad_len += adv_instance->adv_data_len;\n\t\tptr += adv_instance->adv_data_len;\n\t}\n\n\tif (instance_flags & MGMT_ADV_FLAG_TX_POWER) {\n\t\ts8 adv_tx_power;\n\n\t\tif (ext_adv_capable(hdev)) {\n\t\t\tif (adv_instance)\n\t\t\t\tadv_tx_power = adv_instance->tx_power;\n\t\t\telse\n\t\t\t\tadv_tx_power = hdev->adv_tx_power;\n\t\t} else {\n\t\t\tadv_tx_power = hdev->adv_tx_power;\n\t\t}\n\n\t\t/* Provide Tx Power only if we can provide a valid value for it */\n\t\tif (adv_tx_power != HCI_TX_POWER_INVALID) {\n\t\t\tptr[0] = 0x02;\n\t\t\tptr[1] = EIR_TX_POWER;\n\t\t\tptr[2] = (u8)adv_tx_power;\n\n\t\t\tad_len += 3;\n\t\t\tptr += 3;\n\t\t}\n\t}\n\n\treturn ad_len;\n}\n\nvoid __hci_req_update_adv_data(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 len;\n\n\tif (!hci_dev_test_flag(hdev, HCI_LE_ENABLED))\n\t\treturn;\n\n\tif (ext_adv_capable(hdev)) {\n\t\tstruct hci_cp_le_set_ext_adv_data cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tlen = create_instance_adv_data(hdev, instance, cp.data);\n\n\t\t/* There's nothing to do if the data hasn't changed */\n\t\tif (hdev->adv_data_len == len &&\n\t\t    memcmp(cp.data, hdev->adv_data, len) == 0)\n\t\t\treturn;\n\n\t\tmemcpy(hdev->adv_data, cp.data, sizeof(cp.data));\n\t\thdev->adv_data_len = len;\n\n\t\tcp.length = len;\n\t\tcp.handle = instance;\n\t\tcp.operation = LE_SET_ADV_DATA_OP_COMPLETE;\n\t\tcp.frag_pref = LE_SET_ADV_DATA_NO_FRAG;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_ADV_DATA, sizeof(cp), &cp);\n\t} else {\n\t\tstruct hci_cp_le_set_adv_data cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tlen = create_instance_adv_data(hdev, instance, cp.data);\n\n\t\t/* There's nothing to do if the data hasn't changed */\n\t\tif (hdev->adv_data_len == len &&\n\t\t    memcmp(cp.data, hdev->adv_data, len) == 0)\n\t\t\treturn;\n\n\t\tmemcpy(hdev->adv_data, cp.data, sizeof(cp.data));\n\t\thdev->adv_data_len = len;\n\n\t\tcp.length = len;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_ADV_DATA, sizeof(cp), &cp);\n\t}\n}\n\nint hci_req_update_adv_data(struct hci_dev *hdev, u8 instance)\n{\n\tstruct hci_request req;\n\n\thci_req_init(&req, hdev);\n\t__hci_req_update_adv_data(&req, instance);\n\n\treturn hci_req_run(&req, NULL);\n}\n\nstatic void enable_addr_resolution_complete(struct hci_dev *hdev, u8 status,\n\t\t\t\t\t    u16 opcode)\n{\n\tBT_DBG(\"%s status %u\", hdev->name, status);\n}\n\nvoid hci_req_disable_address_resolution(struct hci_dev *hdev)\n{\n\tstruct hci_request req;\n\t__u8 enable = 0x00;\n\n\tif (!use_ll_privacy(hdev) &&\n\t    !hci_dev_test_flag(hdev, HCI_LL_RPA_RESOLUTION))\n\t\treturn;\n\n\thci_req_init(&req, hdev);\n\n\thci_req_add(&req, HCI_OP_LE_SET_ADDR_RESOLV_ENABLE, 1, &enable);\n\n\thci_req_run(&req, enable_addr_resolution_complete);\n}\n\nstatic void adv_enable_complete(struct hci_dev *hdev, u8 status, u16 opcode)\n{\n\tbt_dev_dbg(hdev, \"status %u\", status);\n}\n\nvoid hci_req_reenable_advertising(struct hci_dev *hdev)\n{\n\tstruct hci_request req;\n\n\tif (!hci_dev_test_flag(hdev, HCI_ADVERTISING) &&\n\t    list_empty(&hdev->adv_instances))\n\t\treturn;\n\n\thci_req_init(&req, hdev);\n\n\tif (hdev->cur_adv_instance) {\n\t\t__hci_req_schedule_adv_instance(&req, hdev->cur_adv_instance,\n\t\t\t\t\t\ttrue);\n\t} else {\n\t\tif (ext_adv_capable(hdev)) {\n\t\t\t__hci_req_start_ext_adv(&req, 0x00);\n\t\t} else {\n\t\t\t__hci_req_update_adv_data(&req, 0x00);\n\t\t\t__hci_req_update_scan_rsp_data(&req, 0x00);\n\t\t\t__hci_req_enable_advertising(&req);\n\t\t}\n\t}\n\n\thci_req_run(&req, adv_enable_complete);\n}\n\nstatic void adv_timeout_expire(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    adv_instance_expire.work);\n\n\tstruct hci_request req;\n\tu8 instance;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\thci_dev_lock(hdev);\n\n\thdev->adv_instance_timeout = 0;\n\n\tinstance = hdev->cur_adv_instance;\n\tif (instance == 0x00)\n\t\tgoto unlock;\n\n\thci_req_init(&req, hdev);\n\n\thci_req_clear_adv_instance(hdev, NULL, &req, instance, false);\n\n\tif (list_empty(&hdev->adv_instances))\n\t\t__hci_req_disable_advertising(&req);\n\n\thci_req_run(&req, NULL);\n\nunlock:\n\thci_dev_unlock(hdev);\n}\n\nstatic int hci_req_add_le_interleaved_scan(struct hci_request *req,\n\t\t\t\t\t   unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tint ret = 0;\n\n\thci_dev_lock(hdev);\n\n\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\thci_req_add_le_scan_disable(req, false);\n\thci_req_add_le_passive_scan(req);\n\n\tswitch (hdev->interleave_scan_state) {\n\tcase INTERLEAVE_SCAN_ALLOWLIST:\n\t\tbt_dev_dbg(hdev, \"next state: allowlist\");\n\t\thdev->interleave_scan_state = INTERLEAVE_SCAN_NO_FILTER;\n\t\tbreak;\n\tcase INTERLEAVE_SCAN_NO_FILTER:\n\t\tbt_dev_dbg(hdev, \"next state: no filter\");\n\t\thdev->interleave_scan_state = INTERLEAVE_SCAN_ALLOWLIST;\n\t\tbreak;\n\tcase INTERLEAVE_SCAN_NONE:\n\t\tBT_ERR(\"unexpected error\");\n\t\tret = -1;\n\t}\n\n\thci_dev_unlock(hdev);\n\n\treturn ret;\n}\n\nstatic void interleave_scan_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    interleave_scan.work);\n\tu8 status;\n\tunsigned long timeout;\n\n\tif (hdev->interleave_scan_state == INTERLEAVE_SCAN_ALLOWLIST) {\n\t\ttimeout = msecs_to_jiffies(hdev->advmon_allowlist_duration);\n\t} else if (hdev->interleave_scan_state == INTERLEAVE_SCAN_NO_FILTER) {\n\t\ttimeout = msecs_to_jiffies(hdev->advmon_no_filter_duration);\n\t} else {\n\t\tbt_dev_err(hdev, \"unexpected error\");\n\t\treturn;\n\t}\n\n\thci_req_sync(hdev, hci_req_add_le_interleaved_scan, 0,\n\t\t     HCI_CMD_TIMEOUT, &status);\n\n\t/* Don't continue interleaving if it was canceled */\n\tif (is_interleave_scanning(hdev))\n\t\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t\t   &hdev->interleave_scan, timeout);\n}\n\nint hci_get_random_address(struct hci_dev *hdev, bool require_privacy,\n\t\t\t   bool use_rpa, struct adv_info *adv_instance,\n\t\t\t   u8 *own_addr_type, bdaddr_t *rand_addr)\n{\n\tint err;\n\n\tbacpy(rand_addr, BDADDR_ANY);\n\n\t/* If privacy is enabled use a resolvable private address. If\n\t * current RPA has expired then generate a new one.\n\t */\n\tif (use_rpa) {\n\t\tint to;\n\n\t\t/* If Controller supports LL Privacy use own address type is\n\t\t * 0x03\n\t\t */\n\t\tif (use_ll_privacy(hdev))\n\t\t\t*own_addr_type = ADDR_LE_DEV_RANDOM_RESOLVED;\n\t\telse\n\t\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\n\t\tif (adv_instance) {\n\t\t\tif (!adv_instance->rpa_expired &&\n\t\t\t    !bacmp(&adv_instance->random_addr, &hdev->rpa))\n\t\t\t\treturn 0;\n\n\t\t\tadv_instance->rpa_expired = false;\n\t\t} else {\n\t\t\tif (!hci_dev_test_and_clear_flag(hdev, HCI_RPA_EXPIRED) &&\n\t\t\t    !bacmp(&hdev->random_addr, &hdev->rpa))\n\t\t\t\treturn 0;\n\t\t}\n\n\t\terr = smp_generate_rpa(hdev, hdev->irk, &hdev->rpa);\n\t\tif (err < 0) {\n\t\t\tbt_dev_err(hdev, \"failed to generate new RPA\");\n\t\t\treturn err;\n\t\t}\n\n\t\tbacpy(rand_addr, &hdev->rpa);\n\n\t\tto = msecs_to_jiffies(hdev->rpa_timeout * 1000);\n\t\tif (adv_instance)\n\t\t\tqueue_delayed_work(hdev->workqueue,\n\t\t\t\t\t   &adv_instance->rpa_expired_cb, to);\n\t\telse\n\t\t\tqueue_delayed_work(hdev->workqueue,\n\t\t\t\t\t   &hdev->rpa_expired, to);\n\n\t\treturn 0;\n\t}\n\n\t/* In case of required privacy without resolvable private address,\n\t * use an non-resolvable private address. This is useful for\n\t * non-connectable advertising.\n\t */\n\tif (require_privacy) {\n\t\tbdaddr_t nrpa;\n\n\t\twhile (true) {\n\t\t\t/* The non-resolvable private address is generated\n\t\t\t * from random six bytes with the two most significant\n\t\t\t * bits cleared.\n\t\t\t */\n\t\t\tget_random_bytes(&nrpa, 6);\n\t\t\tnrpa.b[5] &= 0x3f;\n\n\t\t\t/* The non-resolvable private address shall not be\n\t\t\t * equal to the public address.\n\t\t\t */\n\t\t\tif (bacmp(&hdev->bdaddr, &nrpa))\n\t\t\t\tbreak;\n\t\t}\n\n\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\t\tbacpy(rand_addr, &nrpa);\n\n\t\treturn 0;\n\t}\n\n\t/* No privacy so use a public address. */\n\t*own_addr_type = ADDR_LE_DEV_PUBLIC;\n\n\treturn 0;\n}\n\nvoid __hci_req_clear_ext_adv_sets(struct hci_request *req)\n{\n\thci_req_add(req, HCI_OP_LE_CLEAR_ADV_SETS, 0, NULL);\n}\n\nint __hci_req_setup_ext_adv_instance(struct hci_request *req, u8 instance)\n{\n\tstruct hci_cp_le_set_ext_adv_params cp;\n\tstruct hci_dev *hdev = req->hdev;\n\tbool connectable;\n\tu32 flags;\n\tbdaddr_t random_addr;\n\tu8 own_addr_type;\n\tint err;\n\tstruct adv_info *adv_instance;\n\tbool secondary_adv;\n\n\tif (instance > 0) {\n\t\tadv_instance = hci_find_adv_instance(hdev, instance);\n\t\tif (!adv_instance)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tadv_instance = NULL;\n\t}\n\n\tflags = get_adv_instance_flags(hdev, instance);\n\n\t/* If the \"connectable\" instance flag was not set, then choose between\n\t * ADV_IND and ADV_NONCONN_IND based on the global connectable setting.\n\t */\n\tconnectable = (flags & MGMT_ADV_FLAG_CONNECTABLE) ||\n\t\t      mgmt_get_connectable(hdev);\n\n\tif (!is_advertising_allowed(hdev, connectable))\n\t\treturn -EPERM;\n\n\t/* Set require_privacy to true only when non-connectable\n\t * advertising is used. In that case it is fine to use a\n\t * non-resolvable private address.\n\t */\n\terr = hci_get_random_address(hdev, !connectable,\n\t\t\t\t     adv_use_rpa(hdev, flags), adv_instance,\n\t\t\t\t     &own_addr_type, &random_addr);\n\tif (err < 0)\n\t\treturn err;\n\n\tmemset(&cp, 0, sizeof(cp));\n\n\tif (adv_instance) {\n\t\thci_cpu_to_le24(adv_instance->min_interval, cp.min_interval);\n\t\thci_cpu_to_le24(adv_instance->max_interval, cp.max_interval);\n\t\tcp.tx_power = adv_instance->tx_power;\n\t} else {\n\t\thci_cpu_to_le24(hdev->le_adv_min_interval, cp.min_interval);\n\t\thci_cpu_to_le24(hdev->le_adv_max_interval, cp.max_interval);\n\t\tcp.tx_power = HCI_ADV_TX_POWER_NO_PREFERENCE;\n\t}\n\n\tsecondary_adv = (flags & MGMT_ADV_FLAG_SEC_MASK);\n\n\tif (connectable) {\n\t\tif (secondary_adv)\n\t\t\tcp.evt_properties = cpu_to_le16(LE_EXT_ADV_CONN_IND);\n\t\telse\n\t\t\tcp.evt_properties = cpu_to_le16(LE_LEGACY_ADV_IND);\n\t} else if (adv_instance_is_scannable(hdev, instance)) {\n\t\tif (secondary_adv)\n\t\t\tcp.evt_properties = cpu_to_le16(LE_EXT_ADV_SCAN_IND);\n\t\telse\n\t\t\tcp.evt_properties = cpu_to_le16(LE_LEGACY_ADV_SCAN_IND);\n\t} else {\n\t\tif (secondary_adv)\n\t\t\tcp.evt_properties = cpu_to_le16(LE_EXT_ADV_NON_CONN_IND);\n\t\telse\n\t\t\tcp.evt_properties = cpu_to_le16(LE_LEGACY_NONCONN_IND);\n\t}\n\n\tcp.own_addr_type = own_addr_type;\n\tcp.channel_map = hdev->le_adv_channel_map;\n\tcp.handle = instance;\n\n\tif (flags & MGMT_ADV_FLAG_SEC_2M) {\n\t\tcp.primary_phy = HCI_ADV_PHY_1M;\n\t\tcp.secondary_phy = HCI_ADV_PHY_2M;\n\t} else if (flags & MGMT_ADV_FLAG_SEC_CODED) {\n\t\tcp.primary_phy = HCI_ADV_PHY_CODED;\n\t\tcp.secondary_phy = HCI_ADV_PHY_CODED;\n\t} else {\n\t\t/* In all other cases use 1M */\n\t\tcp.primary_phy = HCI_ADV_PHY_1M;\n\t\tcp.secondary_phy = HCI_ADV_PHY_1M;\n\t}\n\n\thci_req_add(req, HCI_OP_LE_SET_EXT_ADV_PARAMS, sizeof(cp), &cp);\n\n\tif (own_addr_type == ADDR_LE_DEV_RANDOM &&\n\t    bacmp(&random_addr, BDADDR_ANY)) {\n\t\tstruct hci_cp_le_set_adv_set_rand_addr cp;\n\n\t\t/* Check if random address need to be updated */\n\t\tif (adv_instance) {\n\t\t\tif (!bacmp(&random_addr, &adv_instance->random_addr))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!bacmp(&random_addr, &hdev->random_addr))\n\t\t\t\treturn 0;\n\t\t}\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\n\t\tcp.handle = instance;\n\t\tbacpy(&cp.bdaddr, &random_addr);\n\n\t\thci_req_add(req,\n\t\t\t    HCI_OP_LE_SET_ADV_SET_RAND_ADDR,\n\t\t\t    sizeof(cp), &cp);\n\t}\n\n\treturn 0;\n}\n\nint __hci_req_enable_ext_advertising(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_le_set_ext_adv_enable *cp;\n\tstruct hci_cp_ext_adv_set *adv_set;\n\tu8 data[sizeof(*cp) + sizeof(*adv_set) * 1];\n\tstruct adv_info *adv_instance;\n\n\tif (instance > 0) {\n\t\tadv_instance = hci_find_adv_instance(hdev, instance);\n\t\tif (!adv_instance)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tadv_instance = NULL;\n\t}\n\n\tcp = (void *) data;\n\tadv_set = (void *) cp->data;\n\n\tmemset(cp, 0, sizeof(*cp));\n\n\tcp->enable = 0x01;\n\tcp->num_of_sets = 0x01;\n\n\tmemset(adv_set, 0, sizeof(*adv_set));\n\n\tadv_set->handle = instance;\n\n\t/* Set duration per instance since controller is responsible for\n\t * scheduling it.\n\t */\n\tif (adv_instance && adv_instance->duration) {\n\t\tu16 duration = adv_instance->timeout * MSEC_PER_SEC;\n\n\t\t/* Time = N * 10 ms */\n\t\tadv_set->duration = cpu_to_le16(duration / 10);\n\t}\n\n\thci_req_add(req, HCI_OP_LE_SET_EXT_ADV_ENABLE,\n\t\t    sizeof(*cp) + sizeof(*adv_set) * cp->num_of_sets,\n\t\t    data);\n\n\treturn 0;\n}\n\nint __hci_req_disable_ext_adv_instance(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_le_set_ext_adv_enable *cp;\n\tstruct hci_cp_ext_adv_set *adv_set;\n\tu8 data[sizeof(*cp) + sizeof(*adv_set) * 1];\n\tu8 req_size;\n\n\t/* If request specifies an instance that doesn't exist, fail */\n\tif (instance > 0 && !hci_find_adv_instance(hdev, instance))\n\t\treturn -EINVAL;\n\n\tmemset(data, 0, sizeof(data));\n\n\tcp = (void *)data;\n\tadv_set = (void *)cp->data;\n\n\t/* Instance 0x00 indicates all advertising instances will be disabled */\n\tcp->num_of_sets = !!instance;\n\tcp->enable = 0x00;\n\n\tadv_set->handle = instance;\n\n\treq_size = sizeof(*cp) + sizeof(*adv_set) * cp->num_of_sets;\n\thci_req_add(req, HCI_OP_LE_SET_EXT_ADV_ENABLE, req_size, data);\n\n\treturn 0;\n}\n\nint __hci_req_remove_ext_adv_instance(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\t/* If request specifies an instance that doesn't exist, fail */\n\tif (instance > 0 && !hci_find_adv_instance(hdev, instance))\n\t\treturn -EINVAL;\n\n\thci_req_add(req, HCI_OP_LE_REMOVE_ADV_SET, sizeof(instance), &instance);\n\n\treturn 0;\n}\n\nint __hci_req_start_ext_adv(struct hci_request *req, u8 instance)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct adv_info *adv_instance = hci_find_adv_instance(hdev, instance);\n\tint err;\n\n\t/* If instance isn't pending, the chip knows about it, and it's safe to\n\t * disable\n\t */\n\tif (adv_instance && !adv_instance->pending)\n\t\t__hci_req_disable_ext_adv_instance(req, instance);\n\n\terr = __hci_req_setup_ext_adv_instance(req, instance);\n\tif (err < 0)\n\t\treturn err;\n\n\t__hci_req_update_scan_rsp_data(req, instance);\n\t__hci_req_enable_ext_advertising(req, instance);\n\n\treturn 0;\n}\n\nint __hci_req_schedule_adv_instance(struct hci_request *req, u8 instance,\n\t\t\t\t    bool force)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct adv_info *adv_instance = NULL;\n\tu16 timeout;\n\n\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING) ||\n\t    list_empty(&hdev->adv_instances))\n\t\treturn -EPERM;\n\n\tif (hdev->adv_instance_timeout)\n\t\treturn -EBUSY;\n\n\tadv_instance = hci_find_adv_instance(hdev, instance);\n\tif (!adv_instance)\n\t\treturn -ENOENT;\n\n\t/* A zero timeout means unlimited advertising. As long as there is\n\t * only one instance, duration should be ignored. We still set a timeout\n\t * in case further instances are being added later on.\n\t *\n\t * If the remaining lifetime of the instance is more than the duration\n\t * then the timeout corresponds to the duration, otherwise it will be\n\t * reduced to the remaining instance lifetime.\n\t */\n\tif (adv_instance->timeout == 0 ||\n\t    adv_instance->duration <= adv_instance->remaining_time)\n\t\ttimeout = adv_instance->duration;\n\telse\n\t\ttimeout = adv_instance->remaining_time;\n\n\t/* The remaining time is being reduced unless the instance is being\n\t * advertised without time limit.\n\t */\n\tif (adv_instance->timeout)\n\t\tadv_instance->remaining_time =\n\t\t\t\tadv_instance->remaining_time - timeout;\n\n\t/* Only use work for scheduling instances with legacy advertising */\n\tif (!ext_adv_capable(hdev)) {\n\t\thdev->adv_instance_timeout = timeout;\n\t\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t   &hdev->adv_instance_expire,\n\t\t\t   msecs_to_jiffies(timeout * 1000));\n\t}\n\n\t/* If we're just re-scheduling the same instance again then do not\n\t * execute any HCI commands. This happens when a single instance is\n\t * being advertised.\n\t */\n\tif (!force && hdev->cur_adv_instance == instance &&\n\t    hci_dev_test_flag(hdev, HCI_LE_ADV))\n\t\treturn 0;\n\n\thdev->cur_adv_instance = instance;\n\tif (ext_adv_capable(hdev)) {\n\t\t__hci_req_start_ext_adv(req, instance);\n\t} else {\n\t\t__hci_req_update_adv_data(req, instance);\n\t\t__hci_req_update_scan_rsp_data(req, instance);\n\t\t__hci_req_enable_advertising(req);\n\t}\n\n\treturn 0;\n}\n\n/* For a single instance:\n * - force == true: The instance will be removed even when its remaining\n *   lifetime is not zero.\n * - force == false: the instance will be deactivated but kept stored unless\n *   the remaining lifetime is zero.\n *\n * For instance == 0x00:\n * - force == true: All instances will be removed regardless of their timeout\n *   setting.\n * - force == false: Only instances that have a timeout will be removed.\n */\nvoid hci_req_clear_adv_instance(struct hci_dev *hdev, struct sock *sk,\n\t\t\t\tstruct hci_request *req, u8 instance,\n\t\t\t\tbool force)\n{\n\tstruct adv_info *adv_instance, *n, *next_instance = NULL;\n\tint err;\n\tu8 rem_inst;\n\n\t/* Cancel any timeout concerning the removed instance(s). */\n\tif (!instance || hdev->cur_adv_instance == instance)\n\t\tcancel_adv_timeout(hdev);\n\n\t/* Get the next instance to advertise BEFORE we remove\n\t * the current one. This can be the same instance again\n\t * if there is only one instance.\n\t */\n\tif (instance && hdev->cur_adv_instance == instance)\n\t\tnext_instance = hci_get_next_instance(hdev, instance);\n\n\tif (instance == 0x00) {\n\t\tlist_for_each_entry_safe(adv_instance, n, &hdev->adv_instances,\n\t\t\t\t\t list) {\n\t\t\tif (!(force || adv_instance->timeout))\n\t\t\t\tcontinue;\n\n\t\t\trem_inst = adv_instance->instance;\n\t\t\terr = hci_remove_adv_instance(hdev, rem_inst);\n\t\t\tif (!err)\n\t\t\t\tmgmt_advertising_removed(sk, hdev, rem_inst);\n\t\t}\n\t} else {\n\t\tadv_instance = hci_find_adv_instance(hdev, instance);\n\n\t\tif (force || (adv_instance && adv_instance->timeout &&\n\t\t\t      !adv_instance->remaining_time)) {\n\t\t\t/* Don't advertise a removed instance. */\n\t\t\tif (next_instance &&\n\t\t\t    next_instance->instance == instance)\n\t\t\t\tnext_instance = NULL;\n\n\t\t\terr = hci_remove_adv_instance(hdev, instance);\n\t\t\tif (!err)\n\t\t\t\tmgmt_advertising_removed(sk, hdev, instance);\n\t\t}\n\t}\n\n\tif (!req || !hdev_is_powered(hdev) ||\n\t    hci_dev_test_flag(hdev, HCI_ADVERTISING))\n\t\treturn;\n\n\tif (next_instance && !ext_adv_capable(hdev))\n\t\t__hci_req_schedule_adv_instance(req, next_instance->instance,\n\t\t\t\t\t\tfalse);\n}\n\nstatic void set_random_addr(struct hci_request *req, bdaddr_t *rpa)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\t/* If we're advertising or initiating an LE connection we can't\n\t * go ahead and change the random address at this time. This is\n\t * because the eventual initiator address used for the\n\t * subsequently created connection will be undefined (some\n\t * controllers use the new address and others the one we had\n\t * when the operation started).\n\t *\n\t * In this kind of scenario skip the update and let the random\n\t * address be updated at the next cycle.\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_LE_ADV) ||\n\t    hci_lookup_le_connect(hdev)) {\n\t\tbt_dev_dbg(hdev, \"Deferring random address update\");\n\t\thci_dev_set_flag(hdev, HCI_RPA_EXPIRED);\n\t\treturn;\n\t}\n\n\thci_req_add(req, HCI_OP_LE_SET_RANDOM_ADDR, 6, rpa);\n}\n\nint hci_update_random_address(struct hci_request *req, bool require_privacy,\n\t\t\t      bool use_rpa, u8 *own_addr_type)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tint err;\n\n\t/* If privacy is enabled use a resolvable private address. If\n\t * current RPA has expired or there is something else than\n\t * the current RPA in use, then generate a new one.\n\t */\n\tif (use_rpa) {\n\t\tint to;\n\n\t\t/* If Controller supports LL Privacy use own address type is\n\t\t * 0x03\n\t\t */\n\t\tif (use_ll_privacy(hdev))\n\t\t\t*own_addr_type = ADDR_LE_DEV_RANDOM_RESOLVED;\n\t\telse\n\t\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\n\t\tif (!hci_dev_test_and_clear_flag(hdev, HCI_RPA_EXPIRED) &&\n\t\t    !bacmp(&hdev->random_addr, &hdev->rpa))\n\t\t\treturn 0;\n\n\t\terr = smp_generate_rpa(hdev, hdev->irk, &hdev->rpa);\n\t\tif (err < 0) {\n\t\t\tbt_dev_err(hdev, \"failed to generate new RPA\");\n\t\t\treturn err;\n\t\t}\n\n\t\tset_random_addr(req, &hdev->rpa);\n\n\t\tto = msecs_to_jiffies(hdev->rpa_timeout * 1000);\n\t\tqueue_delayed_work(hdev->workqueue, &hdev->rpa_expired, to);\n\n\t\treturn 0;\n\t}\n\n\t/* In case of required privacy without resolvable private address,\n\t * use an non-resolvable private address. This is useful for active\n\t * scanning and non-connectable advertising.\n\t */\n\tif (require_privacy) {\n\t\tbdaddr_t nrpa;\n\n\t\twhile (true) {\n\t\t\t/* The non-resolvable private address is generated\n\t\t\t * from random six bytes with the two most significant\n\t\t\t * bits cleared.\n\t\t\t */\n\t\t\tget_random_bytes(&nrpa, 6);\n\t\t\tnrpa.b[5] &= 0x3f;\n\n\t\t\t/* The non-resolvable private address shall not be\n\t\t\t * equal to the public address.\n\t\t\t */\n\t\t\tif (bacmp(&hdev->bdaddr, &nrpa))\n\t\t\t\tbreak;\n\t\t}\n\n\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\t\tset_random_addr(req, &nrpa);\n\t\treturn 0;\n\t}\n\n\t/* If forcing static address is in use or there is no public\n\t * address use the static address as random address (but skip\n\t * the HCI command if the current random address is already the\n\t * static one.\n\t *\n\t * In case BR/EDR has been disabled on a dual-mode controller\n\t * and a static address has been configured, then use that\n\t * address instead of the public BR/EDR address.\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_FORCE_STATIC_ADDR) ||\n\t    !bacmp(&hdev->bdaddr, BDADDR_ANY) ||\n\t    (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED) &&\n\t     bacmp(&hdev->static_addr, BDADDR_ANY))) {\n\t\t*own_addr_type = ADDR_LE_DEV_RANDOM;\n\t\tif (bacmp(&hdev->static_addr, &hdev->random_addr))\n\t\t\thci_req_add(req, HCI_OP_LE_SET_RANDOM_ADDR, 6,\n\t\t\t\t    &hdev->static_addr);\n\t\treturn 0;\n\t}\n\n\t/* Neither privacy nor static address is being used so use a\n\t * public address.\n\t */\n\t*own_addr_type = ADDR_LE_DEV_PUBLIC;\n\n\treturn 0;\n}\n\nstatic bool disconnected_whitelist_entries(struct hci_dev *hdev)\n{\n\tstruct bdaddr_list *b;\n\n\tlist_for_each_entry(b, &hdev->whitelist, list) {\n\t\tstruct hci_conn *conn;\n\n\t\tconn = hci_conn_hash_lookup_ba(hdev, ACL_LINK, &b->bdaddr);\n\t\tif (!conn)\n\t\t\treturn true;\n\n\t\tif (conn->state != BT_CONNECTED && conn->state != BT_CONFIG)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nvoid __hci_req_update_scan(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 scan;\n\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\treturn;\n\n\tif (!hdev_is_powered(hdev))\n\t\treturn;\n\n\tif (mgmt_powering_down(hdev))\n\t\treturn;\n\n\tif (hdev->scanning_paused)\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_CONNECTABLE) ||\n\t    disconnected_whitelist_entries(hdev))\n\t\tscan = SCAN_PAGE;\n\telse\n\t\tscan = SCAN_DISABLED;\n\n\tif (hci_dev_test_flag(hdev, HCI_DISCOVERABLE))\n\t\tscan |= SCAN_INQUIRY;\n\n\tif (test_bit(HCI_PSCAN, &hdev->flags) == !!(scan & SCAN_PAGE) &&\n\t    test_bit(HCI_ISCAN, &hdev->flags) == !!(scan & SCAN_INQUIRY))\n\t\treturn;\n\n\thci_req_add(req, HCI_OP_WRITE_SCAN_ENABLE, 1, &scan);\n}\n\nstatic int update_scan(struct hci_request *req, unsigned long opt)\n{\n\thci_dev_lock(req->hdev);\n\t__hci_req_update_scan(req);\n\thci_dev_unlock(req->hdev);\n\treturn 0;\n}\n\nstatic void scan_update_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev, scan_update);\n\n\thci_req_sync(hdev, update_scan, 0, HCI_CMD_TIMEOUT, NULL);\n}\n\nstatic int connectable_update(struct hci_request *req, unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\thci_dev_lock(hdev);\n\n\t__hci_req_update_scan(req);\n\n\t/* If BR/EDR is not enabled and we disable advertising as a\n\t * by-product of disabling connectable, we need to update the\n\t * advertising flags.\n\t */\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\t__hci_req_update_adv_data(req, hdev->cur_adv_instance);\n\n\t/* Update the advertising parameters if necessary */\n\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING) ||\n\t    !list_empty(&hdev->adv_instances)) {\n\t\tif (ext_adv_capable(hdev))\n\t\t\t__hci_req_start_ext_adv(req, hdev->cur_adv_instance);\n\t\telse\n\t\t\t__hci_req_enable_advertising(req);\n\t}\n\n\t__hci_update_background_scan(req);\n\n\thci_dev_unlock(hdev);\n\n\treturn 0;\n}\n\nstatic void connectable_update_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    connectable_update);\n\tu8 status;\n\n\thci_req_sync(hdev, connectable_update, 0, HCI_CMD_TIMEOUT, &status);\n\tmgmt_set_connectable_complete(hdev, status);\n}\n\nstatic u8 get_service_classes(struct hci_dev *hdev)\n{\n\tstruct bt_uuid *uuid;\n\tu8 val = 0;\n\n\tlist_for_each_entry(uuid, &hdev->uuids, list)\n\t\tval |= uuid->svc_hint;\n\n\treturn val;\n}\n\nvoid __hci_req_update_class(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 cod[3];\n\n\tbt_dev_dbg(hdev, \"\");\n\n\tif (!hdev_is_powered(hdev))\n\t\treturn;\n\n\tif (!hci_dev_test_flag(hdev, HCI_BREDR_ENABLED))\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_SERVICE_CACHE))\n\t\treturn;\n\n\tcod[0] = hdev->minor_class;\n\tcod[1] = hdev->major_class;\n\tcod[2] = get_service_classes(hdev);\n\n\tif (hci_dev_test_flag(hdev, HCI_LIMITED_DISCOVERABLE))\n\t\tcod[1] |= 0x20;\n\n\tif (memcmp(cod, hdev->dev_class, 3) == 0)\n\t\treturn;\n\n\thci_req_add(req, HCI_OP_WRITE_CLASS_OF_DEV, sizeof(cod), cod);\n}\n\nstatic void write_iac(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct hci_cp_write_current_iac_lap cp;\n\n\tif (!hci_dev_test_flag(hdev, HCI_DISCOVERABLE))\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_LIMITED_DISCOVERABLE)) {\n\t\t/* Limited discoverable mode */\n\t\tcp.num_iac = min_t(u8, hdev->num_iac, 2);\n\t\tcp.iac_lap[0] = 0x00;\t/* LIAC */\n\t\tcp.iac_lap[1] = 0x8b;\n\t\tcp.iac_lap[2] = 0x9e;\n\t\tcp.iac_lap[3] = 0x33;\t/* GIAC */\n\t\tcp.iac_lap[4] = 0x8b;\n\t\tcp.iac_lap[5] = 0x9e;\n\t} else {\n\t\t/* General discoverable mode */\n\t\tcp.num_iac = 1;\n\t\tcp.iac_lap[0] = 0x33;\t/* GIAC */\n\t\tcp.iac_lap[1] = 0x8b;\n\t\tcp.iac_lap[2] = 0x9e;\n\t}\n\n\thci_req_add(req, HCI_OP_WRITE_CURRENT_IAC_LAP,\n\t\t    (cp.num_iac * 3) + 1, &cp);\n}\n\nstatic int discoverable_update(struct hci_request *req, unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\thci_dev_lock(hdev);\n\n\tif (hci_dev_test_flag(hdev, HCI_BREDR_ENABLED)) {\n\t\twrite_iac(req);\n\t\t__hci_req_update_scan(req);\n\t\t__hci_req_update_class(req);\n\t}\n\n\t/* Advertising instances don't use the global discoverable setting, so\n\t * only update AD if advertising was enabled using Set Advertising.\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING)) {\n\t\t__hci_req_update_adv_data(req, 0x00);\n\n\t\t/* Discoverable mode affects the local advertising\n\t\t * address in limited privacy mode.\n\t\t */\n\t\tif (hci_dev_test_flag(hdev, HCI_LIMITED_PRIVACY)) {\n\t\t\tif (ext_adv_capable(hdev))\n\t\t\t\t__hci_req_start_ext_adv(req, 0x00);\n\t\t\telse\n\t\t\t\t__hci_req_enable_advertising(req);\n\t\t}\n\t}\n\n\thci_dev_unlock(hdev);\n\n\treturn 0;\n}\n\nstatic void discoverable_update_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    discoverable_update);\n\tu8 status;\n\n\thci_req_sync(hdev, discoverable_update, 0, HCI_CMD_TIMEOUT, &status);\n\tmgmt_set_discoverable_complete(hdev, status);\n}\n\nvoid __hci_abort_conn(struct hci_request *req, struct hci_conn *conn,\n\t\t      u8 reason)\n{\n\tswitch (conn->state) {\n\tcase BT_CONNECTED:\n\tcase BT_CONFIG:\n\t\tif (conn->type == AMP_LINK) {\n\t\t\tstruct hci_cp_disconn_phy_link cp;\n\n\t\t\tcp.phy_handle = HCI_PHY_HANDLE(conn->handle);\n\t\t\tcp.reason = reason;\n\t\t\thci_req_add(req, HCI_OP_DISCONN_PHY_LINK, sizeof(cp),\n\t\t\t\t    &cp);\n\t\t} else {\n\t\t\tstruct hci_cp_disconnect dc;\n\n\t\t\tdc.handle = cpu_to_le16(conn->handle);\n\t\t\tdc.reason = reason;\n\t\t\thci_req_add(req, HCI_OP_DISCONNECT, sizeof(dc), &dc);\n\t\t}\n\n\t\tconn->state = BT_DISCONN;\n\n\t\tbreak;\n\tcase BT_CONNECT:\n\t\tif (conn->type == LE_LINK) {\n\t\t\tif (test_bit(HCI_CONN_SCANNING, &conn->flags))\n\t\t\t\tbreak;\n\t\t\thci_req_add(req, HCI_OP_LE_CREATE_CONN_CANCEL,\n\t\t\t\t    0, NULL);\n\t\t} else if (conn->type == ACL_LINK) {\n\t\t\tif (req->hdev->hci_ver < BLUETOOTH_VER_1_2)\n\t\t\t\tbreak;\n\t\t\thci_req_add(req, HCI_OP_CREATE_CONN_CANCEL,\n\t\t\t\t    6, &conn->dst);\n\t\t}\n\t\tbreak;\n\tcase BT_CONNECT2:\n\t\tif (conn->type == ACL_LINK) {\n\t\t\tstruct hci_cp_reject_conn_req rej;\n\n\t\t\tbacpy(&rej.bdaddr, &conn->dst);\n\t\t\trej.reason = reason;\n\n\t\t\thci_req_add(req, HCI_OP_REJECT_CONN_REQ,\n\t\t\t\t    sizeof(rej), &rej);\n\t\t} else if (conn->type == SCO_LINK || conn->type == ESCO_LINK) {\n\t\t\tstruct hci_cp_reject_sync_conn_req rej;\n\n\t\t\tbacpy(&rej.bdaddr, &conn->dst);\n\n\t\t\t/* SCO rejection has its own limited set of\n\t\t\t * allowed error values (0x0D-0x0F) which isn't\n\t\t\t * compatible with most values passed to this\n\t\t\t * function. To be safe hard-code one of the\n\t\t\t * values that's suitable for SCO.\n\t\t\t */\n\t\t\trej.reason = HCI_ERROR_REJ_LIMITED_RESOURCES;\n\n\t\t\thci_req_add(req, HCI_OP_REJECT_SYNC_CONN_REQ,\n\t\t\t\t    sizeof(rej), &rej);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tconn->state = BT_CLOSED;\n\t\tbreak;\n\t}\n}\n\nstatic void abort_conn_complete(struct hci_dev *hdev, u8 status, u16 opcode)\n{\n\tif (status)\n\t\tbt_dev_dbg(hdev, \"Failed to abort connection: status 0x%2.2x\", status);\n}\n\nint hci_abort_conn(struct hci_conn *conn, u8 reason)\n{\n\tstruct hci_request req;\n\tint err;\n\n\thci_req_init(&req, conn->hdev);\n\n\t__hci_abort_conn(&req, conn, reason);\n\n\terr = hci_req_run(&req, abort_conn_complete);\n\tif (err && err != -ENODATA) {\n\t\tbt_dev_err(conn->hdev, \"failed to run HCI request: err %d\", err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int update_bg_scan(struct hci_request *req, unsigned long opt)\n{\n\thci_dev_lock(req->hdev);\n\t__hci_update_background_scan(req);\n\thci_dev_unlock(req->hdev);\n\treturn 0;\n}\n\nstatic void bg_scan_update(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    bg_scan_update);\n\tstruct hci_conn *conn;\n\tu8 status;\n\tint err;\n\n\terr = hci_req_sync(hdev, update_bg_scan, 0, HCI_CMD_TIMEOUT, &status);\n\tif (!err)\n\t\treturn;\n\n\thci_dev_lock(hdev);\n\n\tconn = hci_conn_hash_lookup_state(hdev, LE_LINK, BT_CONNECT);\n\tif (conn)\n\t\thci_le_conn_failed(conn, status);\n\n\thci_dev_unlock(hdev);\n}\n\nstatic int le_scan_disable(struct hci_request *req, unsigned long opt)\n{\n\thci_req_add_le_scan_disable(req, false);\n\treturn 0;\n}\n\nstatic int bredr_inquiry(struct hci_request *req, unsigned long opt)\n{\n\tu8 length = opt;\n\tconst u8 giac[3] = { 0x33, 0x8b, 0x9e };\n\tconst u8 liac[3] = { 0x00, 0x8b, 0x9e };\n\tstruct hci_cp_inquiry cp;\n\n\tbt_dev_dbg(req->hdev, \"\");\n\n\thci_dev_lock(req->hdev);\n\thci_inquiry_cache_flush(req->hdev);\n\thci_dev_unlock(req->hdev);\n\n\tmemset(&cp, 0, sizeof(cp));\n\n\tif (req->hdev->discovery.limited)\n\t\tmemcpy(&cp.lap, liac, sizeof(cp.lap));\n\telse\n\t\tmemcpy(&cp.lap, giac, sizeof(cp.lap));\n\n\tcp.length = length;\n\n\thci_req_add(req, HCI_OP_INQUIRY, sizeof(cp), &cp);\n\n\treturn 0;\n}\n\nstatic void le_scan_disable_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    le_scan_disable.work);\n\tu8 status;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\tif (!hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\treturn;\n\n\tcancel_delayed_work(&hdev->le_scan_restart);\n\n\thci_req_sync(hdev, le_scan_disable, 0, HCI_CMD_TIMEOUT, &status);\n\tif (status) {\n\t\tbt_dev_err(hdev, \"failed to disable LE scan: status 0x%02x\",\n\t\t\t   status);\n\t\treturn;\n\t}\n\n\thdev->discovery.scan_start = 0;\n\n\t/* If we were running LE only scan, change discovery state. If\n\t * we were running both LE and BR/EDR inquiry simultaneously,\n\t * and BR/EDR inquiry is already finished, stop discovery,\n\t * otherwise BR/EDR inquiry will stop discovery when finished.\n\t * If we will resolve remote device name, do not change\n\t * discovery state.\n\t */\n\n\tif (hdev->discovery.type == DISCOV_TYPE_LE)\n\t\tgoto discov_stopped;\n\n\tif (hdev->discovery.type != DISCOV_TYPE_INTERLEAVED)\n\t\treturn;\n\n\tif (test_bit(HCI_QUIRK_SIMULTANEOUS_DISCOVERY, &hdev->quirks)) {\n\t\tif (!test_bit(HCI_INQUIRY, &hdev->flags) &&\n\t\t    hdev->discovery.state != DISCOVERY_RESOLVING)\n\t\t\tgoto discov_stopped;\n\n\t\treturn;\n\t}\n\n\thci_req_sync(hdev, bredr_inquiry, DISCOV_INTERLEAVED_INQUIRY_LEN,\n\t\t     HCI_CMD_TIMEOUT, &status);\n\tif (status) {\n\t\tbt_dev_err(hdev, \"inquiry failed: status 0x%02x\", status);\n\t\tgoto discov_stopped;\n\t}\n\n\treturn;\n\ndiscov_stopped:\n\thci_dev_lock(hdev);\n\thci_discovery_set_state(hdev, DISCOVERY_STOPPED);\n\thci_dev_unlock(hdev);\n}\n\nstatic int le_scan_restart(struct hci_request *req, unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\n\t/* If controller is not scanning we are done. */\n\tif (!hci_dev_test_flag(hdev, HCI_LE_SCAN))\n\t\treturn 0;\n\n\tif (hdev->scanning_paused) {\n\t\tbt_dev_dbg(hdev, \"Scanning is paused for suspend\");\n\t\treturn 0;\n\t}\n\n\thci_req_add_le_scan_disable(req, false);\n\n\tif (use_ext_scan(hdev)) {\n\t\tstruct hci_cp_le_set_ext_scan_enable ext_enable_cp;\n\n\t\tmemset(&ext_enable_cp, 0, sizeof(ext_enable_cp));\n\t\text_enable_cp.enable = LE_SCAN_ENABLE;\n\t\text_enable_cp.filter_dup = LE_SCAN_FILTER_DUP_ENABLE;\n\n\t\thci_req_add(req, HCI_OP_LE_SET_EXT_SCAN_ENABLE,\n\t\t\t    sizeof(ext_enable_cp), &ext_enable_cp);\n\t} else {\n\t\tstruct hci_cp_le_set_scan_enable cp;\n\n\t\tmemset(&cp, 0, sizeof(cp));\n\t\tcp.enable = LE_SCAN_ENABLE;\n\t\tcp.filter_dup = LE_SCAN_FILTER_DUP_ENABLE;\n\t\thci_req_add(req, HCI_OP_LE_SET_SCAN_ENABLE, sizeof(cp), &cp);\n\t}\n\n\treturn 0;\n}\n\nstatic void le_scan_restart_work(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    le_scan_restart.work);\n\tunsigned long timeout, duration, scan_start, now;\n\tu8 status;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\thci_req_sync(hdev, le_scan_restart, 0, HCI_CMD_TIMEOUT, &status);\n\tif (status) {\n\t\tbt_dev_err(hdev, \"failed to restart LE scan: status %d\",\n\t\t\t   status);\n\t\treturn;\n\t}\n\n\thci_dev_lock(hdev);\n\n\tif (!test_bit(HCI_QUIRK_STRICT_DUPLICATE_FILTER, &hdev->quirks) ||\n\t    !hdev->discovery.scan_start)\n\t\tgoto unlock;\n\n\t/* When the scan was started, hdev->le_scan_disable has been queued\n\t * after duration from scan_start. During scan restart this job\n\t * has been canceled, and we need to queue it again after proper\n\t * timeout, to make sure that scan does not run indefinitely.\n\t */\n\tduration = hdev->discovery.scan_duration;\n\tscan_start = hdev->discovery.scan_start;\n\tnow = jiffies;\n\tif (now - scan_start <= duration) {\n\t\tint elapsed;\n\n\t\tif (now >= scan_start)\n\t\t\telapsed = now - scan_start;\n\t\telse\n\t\t\telapsed = ULONG_MAX - scan_start + now;\n\n\t\ttimeout = duration - elapsed;\n\t} else {\n\t\ttimeout = 0;\n\t}\n\n\tqueue_delayed_work(hdev->req_workqueue,\n\t\t\t   &hdev->le_scan_disable, timeout);\n\nunlock:\n\thci_dev_unlock(hdev);\n}\n\nstatic int active_scan(struct hci_request *req, unsigned long opt)\n{\n\tuint16_t interval = opt;\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 own_addr_type;\n\t/* White list is not used for discovery */\n\tu8 filter_policy = 0x00;\n\t/* Discovery doesn't require controller address resolution */\n\tbool addr_resolv = false;\n\tint err;\n\n\tbt_dev_dbg(hdev, \"\");\n\n\t/* If controller is scanning, it means the background scanning is\n\t * running. Thus, we should temporarily stop it in order to set the\n\t * discovery scanning parameters.\n\t */\n\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN)) {\n\t\thci_req_add_le_scan_disable(req, false);\n\t\tcancel_interleave_scan(hdev);\n\t}\n\n\t/* All active scans will be done with either a resolvable private\n\t * address (when privacy feature has been enabled) or non-resolvable\n\t * private address.\n\t */\n\terr = hci_update_random_address(req, true, scan_use_rpa(hdev),\n\t\t\t\t\t&own_addr_type);\n\tif (err < 0)\n\t\town_addr_type = ADDR_LE_DEV_PUBLIC;\n\n\thci_req_start_scan(req, LE_SCAN_ACTIVE, interval,\n\t\t\t   hdev->le_scan_window_discovery, own_addr_type,\n\t\t\t   filter_policy, addr_resolv);\n\treturn 0;\n}\n\nstatic int interleaved_discov(struct hci_request *req, unsigned long opt)\n{\n\tint err;\n\n\tbt_dev_dbg(req->hdev, \"\");\n\n\terr = active_scan(req, opt);\n\tif (err)\n\t\treturn err;\n\n\treturn bredr_inquiry(req, DISCOV_BREDR_INQUIRY_LEN);\n}\n\nstatic void start_discovery(struct hci_dev *hdev, u8 *status)\n{\n\tunsigned long timeout;\n\n\tbt_dev_dbg(hdev, \"type %u\", hdev->discovery.type);\n\n\tswitch (hdev->discovery.type) {\n\tcase DISCOV_TYPE_BREDR:\n\t\tif (!hci_dev_test_flag(hdev, HCI_INQUIRY))\n\t\t\thci_req_sync(hdev, bredr_inquiry,\n\t\t\t\t     DISCOV_BREDR_INQUIRY_LEN, HCI_CMD_TIMEOUT,\n\t\t\t\t     status);\n\t\treturn;\n\tcase DISCOV_TYPE_INTERLEAVED:\n\t\t/* When running simultaneous discovery, the LE scanning time\n\t\t * should occupy the whole discovery time sine BR/EDR inquiry\n\t\t * and LE scanning are scheduled by the controller.\n\t\t *\n\t\t * For interleaving discovery in comparison, BR/EDR inquiry\n\t\t * and LE scanning are done sequentially with separate\n\t\t * timeouts.\n\t\t */\n\t\tif (test_bit(HCI_QUIRK_SIMULTANEOUS_DISCOVERY,\n\t\t\t     &hdev->quirks)) {\n\t\t\ttimeout = msecs_to_jiffies(DISCOV_LE_TIMEOUT);\n\t\t\t/* During simultaneous discovery, we double LE scan\n\t\t\t * interval. We must leave some time for the controller\n\t\t\t * to do BR/EDR inquiry.\n\t\t\t */\n\t\t\thci_req_sync(hdev, interleaved_discov,\n\t\t\t\t     hdev->le_scan_int_discovery * 2, HCI_CMD_TIMEOUT,\n\t\t\t\t     status);\n\t\t\tbreak;\n\t\t}\n\n\t\ttimeout = msecs_to_jiffies(hdev->discov_interleaved_timeout);\n\t\thci_req_sync(hdev, active_scan, hdev->le_scan_int_discovery,\n\t\t\t     HCI_CMD_TIMEOUT, status);\n\t\tbreak;\n\tcase DISCOV_TYPE_LE:\n\t\ttimeout = msecs_to_jiffies(DISCOV_LE_TIMEOUT);\n\t\thci_req_sync(hdev, active_scan, hdev->le_scan_int_discovery,\n\t\t\t     HCI_CMD_TIMEOUT, status);\n\t\tbreak;\n\tdefault:\n\t\t*status = HCI_ERROR_UNSPECIFIED;\n\t\treturn;\n\t}\n\n\tif (*status)\n\t\treturn;\n\n\tbt_dev_dbg(hdev, \"timeout %u ms\", jiffies_to_msecs(timeout));\n\n\t/* When service discovery is used and the controller has a\n\t * strict duplicate filter, it is important to remember the\n\t * start and duration of the scan. This is required for\n\t * restarting scanning during the discovery phase.\n\t */\n\tif (test_bit(HCI_QUIRK_STRICT_DUPLICATE_FILTER, &hdev->quirks) &&\n\t\t     hdev->discovery.result_filtering) {\n\t\thdev->discovery.scan_start = jiffies;\n\t\thdev->discovery.scan_duration = timeout;\n\t}\n\n\tqueue_delayed_work(hdev->req_workqueue, &hdev->le_scan_disable,\n\t\t\t   timeout);\n}\n\nbool hci_req_stop_discovery(struct hci_request *req)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tstruct discovery_state *d = &hdev->discovery;\n\tstruct hci_cp_remote_name_req_cancel cp;\n\tstruct inquiry_entry *e;\n\tbool ret = false;\n\n\tbt_dev_dbg(hdev, \"state %u\", hdev->discovery.state);\n\n\tif (d->state == DISCOVERY_FINDING || d->state == DISCOVERY_STOPPING) {\n\t\tif (test_bit(HCI_INQUIRY, &hdev->flags))\n\t\t\thci_req_add(req, HCI_OP_INQUIRY_CANCEL, 0, NULL);\n\n\t\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN)) {\n\t\t\tcancel_delayed_work(&hdev->le_scan_disable);\n\t\t\thci_req_add_le_scan_disable(req, false);\n\t\t}\n\n\t\tret = true;\n\t} else {\n\t\t/* Passive scanning */\n\t\tif (hci_dev_test_flag(hdev, HCI_LE_SCAN)) {\n\t\t\thci_req_add_le_scan_disable(req, false);\n\t\t\tret = true;\n\t\t}\n\t}\n\n\t/* No further actions needed for LE-only discovery */\n\tif (d->type == DISCOV_TYPE_LE)\n\t\treturn ret;\n\n\tif (d->state == DISCOVERY_RESOLVING || d->state == DISCOVERY_STOPPING) {\n\t\te = hci_inquiry_cache_lookup_resolve(hdev, BDADDR_ANY,\n\t\t\t\t\t\t     NAME_PENDING);\n\t\tif (!e)\n\t\t\treturn ret;\n\n\t\tbacpy(&cp.bdaddr, &e->data.bdaddr);\n\t\thci_req_add(req, HCI_OP_REMOTE_NAME_REQ_CANCEL, sizeof(cp),\n\t\t\t    &cp);\n\t\tret = true;\n\t}\n\n\treturn ret;\n}\n\nstatic int stop_discovery(struct hci_request *req, unsigned long opt)\n{\n\thci_dev_lock(req->hdev);\n\thci_req_stop_discovery(req);\n\thci_dev_unlock(req->hdev);\n\n\treturn 0;\n}\n\nstatic void discov_update(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    discov_update);\n\tu8 status = 0;\n\n\tswitch (hdev->discovery.state) {\n\tcase DISCOVERY_STARTING:\n\t\tstart_discovery(hdev, &status);\n\t\tmgmt_start_discovery_complete(hdev, status);\n\t\tif (status)\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_STOPPED);\n\t\telse\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_FINDING);\n\t\tbreak;\n\tcase DISCOVERY_STOPPING:\n\t\thci_req_sync(hdev, stop_discovery, 0, HCI_CMD_TIMEOUT, &status);\n\t\tmgmt_stop_discovery_complete(hdev, status);\n\t\tif (!status)\n\t\t\thci_discovery_set_state(hdev, DISCOVERY_STOPPED);\n\t\tbreak;\n\tcase DISCOVERY_STOPPED:\n\tdefault:\n\t\treturn;\n\t}\n}\n\nstatic void discov_off(struct work_struct *work)\n{\n\tstruct hci_dev *hdev = container_of(work, struct hci_dev,\n\t\t\t\t\t    discov_off.work);\n\n\tbt_dev_dbg(hdev, \"\");\n\n\thci_dev_lock(hdev);\n\n\t/* When discoverable timeout triggers, then just make sure\n\t * the limited discoverable flag is cleared. Even in the case\n\t * of a timeout triggered from general discoverable, it is\n\t * safe to unconditionally clear the flag.\n\t */\n\thci_dev_clear_flag(hdev, HCI_LIMITED_DISCOVERABLE);\n\thci_dev_clear_flag(hdev, HCI_DISCOVERABLE);\n\thdev->discov_timeout = 0;\n\n\thci_dev_unlock(hdev);\n\n\thci_req_sync(hdev, discoverable_update, 0, HCI_CMD_TIMEOUT, NULL);\n\tmgmt_new_settings(hdev);\n}\n\nstatic int powered_update_hci(struct hci_request *req, unsigned long opt)\n{\n\tstruct hci_dev *hdev = req->hdev;\n\tu8 link_sec;\n\n\thci_dev_lock(hdev);\n\n\tif (hci_dev_test_flag(hdev, HCI_SSP_ENABLED) &&\n\t    !lmp_host_ssp_capable(hdev)) {\n\t\tu8 mode = 0x01;\n\n\t\thci_req_add(req, HCI_OP_WRITE_SSP_MODE, sizeof(mode), &mode);\n\n\t\tif (bredr_sc_enabled(hdev) && !lmp_host_sc_capable(hdev)) {\n\t\t\tu8 support = 0x01;\n\n\t\t\thci_req_add(req, HCI_OP_WRITE_SC_SUPPORT,\n\t\t\t\t    sizeof(support), &support);\n\t\t}\n\t}\n\n\tif (hci_dev_test_flag(hdev, HCI_LE_ENABLED) &&\n\t    lmp_bredr_capable(hdev)) {\n\t\tstruct hci_cp_write_le_host_supported cp;\n\n\t\tcp.le = 0x01;\n\t\tcp.simul = 0x00;\n\n\t\t/* Check first if we already have the right\n\t\t * host state (host features set)\n\t\t */\n\t\tif (cp.le != lmp_host_le_capable(hdev) ||\n\t\t    cp.simul != lmp_host_le_br_capable(hdev))\n\t\t\thci_req_add(req, HCI_OP_WRITE_LE_HOST_SUPPORTED,\n\t\t\t\t    sizeof(cp), &cp);\n\t}\n\n\tif (hci_dev_test_flag(hdev, HCI_LE_ENABLED)) {\n\t\t/* Make sure the controller has a good default for\n\t\t * advertising data. This also applies to the case\n\t\t * where BR/EDR was toggled during the AUTO_OFF phase.\n\t\t */\n\t\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING) ||\n\t\t    list_empty(&hdev->adv_instances)) {\n\t\t\tint err;\n\n\t\t\tif (ext_adv_capable(hdev)) {\n\t\t\t\terr = __hci_req_setup_ext_adv_instance(req,\n\t\t\t\t\t\t\t\t       0x00);\n\t\t\t\tif (!err)\n\t\t\t\t\t__hci_req_update_scan_rsp_data(req,\n\t\t\t\t\t\t\t\t       0x00);\n\t\t\t} else {\n\t\t\t\terr = 0;\n\t\t\t\t__hci_req_update_adv_data(req, 0x00);\n\t\t\t\t__hci_req_update_scan_rsp_data(req, 0x00);\n\t\t\t}\n\n\t\t\tif (hci_dev_test_flag(hdev, HCI_ADVERTISING)) {\n\t\t\t\tif (!ext_adv_capable(hdev))\n\t\t\t\t\t__hci_req_enable_advertising(req);\n\t\t\t\telse if (!err)\n\t\t\t\t\t__hci_req_enable_ext_advertising(req,\n\t\t\t\t\t\t\t\t\t 0x00);\n\t\t\t}\n\t\t} else if (!list_empty(&hdev->adv_instances)) {\n\t\t\tstruct adv_info *adv_instance;\n\n\t\t\tadv_instance = list_first_entry(&hdev->adv_instances,\n\t\t\t\t\t\t\tstruct adv_info, list);\n\t\t\t__hci_req_schedule_adv_instance(req,\n\t\t\t\t\t\t\tadv_instance->instance,\n\t\t\t\t\t\t\ttrue);\n\t\t}\n\t}\n\n\tlink_sec = hci_dev_test_flag(hdev, HCI_LINK_SECURITY);\n\tif (link_sec != test_bit(HCI_AUTH, &hdev->flags))\n\t\thci_req_add(req, HCI_OP_WRITE_AUTH_ENABLE,\n\t\t\t    sizeof(link_sec), &link_sec);\n\n\tif (lmp_bredr_capable(hdev)) {\n\t\tif (hci_dev_test_flag(hdev, HCI_FAST_CONNECTABLE))\n\t\t\t__hci_req_write_fast_connectable(req, true);\n\t\telse\n\t\t\t__hci_req_write_fast_connectable(req, false);\n\t\t__hci_req_update_scan(req);\n\t\t__hci_req_update_class(req);\n\t\t__hci_req_update_name(req);\n\t\t__hci_req_update_eir(req);\n\t}\n\n\thci_dev_unlock(hdev);\n\treturn 0;\n}\n\nint __hci_req_hci_power_on(struct hci_dev *hdev)\n{\n\t/* Register the available SMP channels (BR/EDR and LE) only when\n\t * successfully powering on the controller. This late\n\t * registration is required so that LE SMP can clearly decide if\n\t * the public address or static address is used.\n\t */\n\tsmp_register(hdev);\n\n\treturn __hci_req_sync(hdev, powered_update_hci, 0, HCI_CMD_TIMEOUT,\n\t\t\t      NULL);\n}\n\nvoid hci_request_setup(struct hci_dev *hdev)\n{\n\tINIT_WORK(&hdev->discov_update, discov_update);\n\tINIT_WORK(&hdev->bg_scan_update, bg_scan_update);\n\tINIT_WORK(&hdev->scan_update, scan_update_work);\n\tINIT_WORK(&hdev->connectable_update, connectable_update_work);\n\tINIT_WORK(&hdev->discoverable_update, discoverable_update_work);\n\tINIT_DELAYED_WORK(&hdev->discov_off, discov_off);\n\tINIT_DELAYED_WORK(&hdev->le_scan_disable, le_scan_disable_work);\n\tINIT_DELAYED_WORK(&hdev->le_scan_restart, le_scan_restart_work);\n\tINIT_DELAYED_WORK(&hdev->adv_instance_expire, adv_timeout_expire);\n\tINIT_DELAYED_WORK(&hdev->interleave_scan, interleave_scan_work);\n}\n\nvoid hci_request_cancel_all(struct hci_dev *hdev)\n{\n\thci_req_sync_cancel(hdev, ENODEV);\n\n\tcancel_work_sync(&hdev->discov_update);\n\tcancel_work_sync(&hdev->bg_scan_update);\n\tcancel_work_sync(&hdev->scan_update);\n\tcancel_work_sync(&hdev->connectable_update);\n\tcancel_work_sync(&hdev->discoverable_update);\n\tcancel_delayed_work_sync(&hdev->discov_off);\n\tcancel_delayed_work_sync(&hdev->le_scan_disable);\n\tcancel_delayed_work_sync(&hdev->le_scan_restart);\n\n\tif (hdev->adv_instance_timeout) {\n\t\tcancel_delayed_work_sync(&hdev->adv_instance_expire);\n\t\thdev->adv_instance_timeout = 0;\n\t}\n\n\tcancel_interleave_scan(hdev);\n}\n"], "filenames": ["net/bluetooth/hci_request.c"], "buggy_code_start_loc": [275], "buggy_code_end_loc": [281], "fixing_code_start_loc": [274], "fixing_code_end_loc": [285], "type": "CWE-362", "message": "net/bluetooth/hci_request.c in the Linux kernel through 5.12.2 has a race condition for removal of the HCI controller.", "other": {"cve": {"id": "CVE-2021-32399", "sourceIdentifier": "cve@mitre.org", "published": "2021-05-10T22:15:06.053", "lastModified": "2022-05-13T20:53:50.767", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "net/bluetooth/hci_request.c in the Linux kernel through 5.12.2 has a race condition for removal of the HCI controller."}, {"lang": "es", "value": "El archivo net/bluetooth/hci_request.c en el kernel de Linux versiones hasta 5.12.2, presenta una condici\u00f3n de carrera para la eliminaci\u00f3n del controlador HCI"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "5.12.2", "matchCriteriaId": "E122193F-2C56-4FF3-BD30-DA6BC408C64B"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:cloud_backup:-:*:*:*:*:*:*:*", "matchCriteriaId": "5C2089EE-5D7F-47EC-8EA5-0F69790564C4"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:solidfire_baseboard_management_controller_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "FB9B8171-F6CA-427D-81E0-6536D3BBFA8D"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:solidfire_baseboard_management_controller:-:*:*:*:*:*:*:*", "matchCriteriaId": "090AA6F4-4404-4E26-82AB-C3A22636F276"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "6770B6C3-732E-4E22-BF1C-2D2FD610061C"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300s:-:*:*:*:*:*:*:*", "matchCriteriaId": "9F9C8C20-42EB-4AB5-BD97-212DEB070C43"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "7FFF7106-ED78-49BA-9EC5-B889E3685D53"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500s:-:*:*:*:*:*:*:*", "matchCriteriaId": "E63D8B0F-006E-4801-BF9D-1C001BBFB4F9"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "56409CEC-5A1E-4450-AA42-641E459CC2AF"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700s:-:*:*:*:*:*:*:*", "matchCriteriaId": "B06F4839-D16A-4A61-9BB5-55B13F41E47F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "108A2215-50FB-4074-94CF-C130FA14566D"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300e:-:*:*:*:*:*:*:*", "matchCriteriaId": "7AFC73CE-ABB9-42D3-9A71-3F5BC5381E0E"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "32F0B6C0-F930-480D-962B-3F4EFDCC13C7"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500e:-:*:*:*:*:*:*:*", "matchCriteriaId": "803BC414-B250-4E3A-A478-A3881340D6B8"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "0FEB3337-BFDE-462A-908B-176F92053CEC"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700e:-:*:*:*:*:*:*:*", "matchCriteriaId": "736AEAE9-782B-4F71-9893-DED53367E102"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "D0B4AD8A-F172-4558-AEC6-FF424BA2D912"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410s:-:*:*:*:*:*:*:*", "matchCriteriaId": "8497A4C9-8474-4A62-8331-3FE862ED4098"}]}]}], "references": [{"url": "http://www.openwall.com/lists/oss-security/2021/05/11/2", "source": "cve@mitre.org", "tags": ["Exploit", "Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=e2cb6b891ad2b8caa9131e3be70f45243df82a80", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/e2cb6b891ad2b8caa9131e3be70f45243df82a80", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2021/06/msg00019.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2021/06/msg00020.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20210622-0006/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/e2cb6b891ad2b8caa9131e3be70f45243df82a80"}}