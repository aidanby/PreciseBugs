{"buggy_code": ["# Handle redirects to S3\nlocation ~ ^/internal/s3/(?<s3_hostname>[^/]+)/(?<s3_path>.*) {\n    internal;\n    include /etc/nginx/zulip-include/headers;\n    add_header Content-Security-Policy \"default-src 'none'; style-src 'self' 'unsafe-inline'; img-src 'self'; object-src 'self'; plugin-types application/pdf;\";\n\n    # The components of this path are originally double-URI-escaped\n    # (see zerver/view/upload.py).  \"location\" matches are on\n    # unescaped values, which fills $s3_path with a properly\n    # single-escaped path to pass to the upstream server.\n    # (see associated commit message for more details)\n    set $download_url https://$s3_hostname/$s3_path;\n    proxy_set_header Host $s3_hostname;\n\n    # Ensure that we only get _one_ of these headers: the one that\n    # Django added, not the one from S3.\n    proxy_hide_header Content-Disposition;\n    proxy_hide_header Cache-Control;\n    proxy_hide_header Expires;\n    proxy_hide_header Set-Cookie;\n    # We are _leaving_ S3 to provide Content-Type and Accept-Ranges\n    # headers, which are the two remaining headers which nginx would\n    # also pass through from the first response.  Django explicitly\n    # unsets the former, and does not set the latter.\n\n    # nginx does its own DNS resolution, which is necessary here to\n    # resolve the IP of the S3 server.  Point it at the local caching\n    # systemd resolved service.  The validity duration is set to match\n    # S3's DNS validity.\n    resolver 127.0.0.53 valid=300s;\n    resolver_timeout 10s;\n\n    proxy_pass $download_url$is_args$args;\n    proxy_cache uploads;\n    # If the S3 response doesn't contain Cache-Control headers (which\n    # we don't expect it to) then we assume they are valid for a very\n    # long time.  The size of the cache is controlled by\n    # `s3_disk_cache_size` and read frequency, set via\n    # `s3_cache_inactive_time`.\n    proxy_cache_valid 200 1y;\n    # Don't include query parameters in the cache key, since those\n    # include a time-based auth token\n    proxy_cache_key $download_url;\n}\n\n# Internal file-serving\nlocation /internal/local/uploads {\n    internal;\n    include /etc/nginx/zulip-include/headers;\n    add_header Content-Security-Policy \"default-src 'none'; style-src 'self' 'unsafe-inline'; img-src 'self'; object-src 'self'; plugin-types application/pdf;\";\n    include /etc/nginx/zulip-include/uploads.types;\n    alias /home/zulip/uploads/files;\n}\n\nlocation /internal/local/user_avatars {\n    internal;\n    include /etc/nginx/zulip-include/headers;\n    add_header Content-Security-Policy \"default-src 'none' img-src 'self'\";\n    include /etc/nginx/zulip-include/uploads.types;\n    alias /home/zulip/uploads/avatars;\n}\n", "# This cache is only used if S3 file storage is configured.\nproxy_cache_path /srv/zulip-uploaded-files-cache\n    levels=1:2\n    keys_zone=uploads:<%= @s3_memory_cache_size %>\n    inactive=<%= @s3_cache_inactive_time %>\n    max_size=<%= @s3_disk_cache_size %>;\n", "import logging\nimport os\nimport secrets\nimport urllib\nfrom mimetypes import guess_type\nfrom typing import IO, Any, Callable, Optional\n\nimport boto3\nimport botocore\nfrom boto3.session import Session\nfrom botocore.client import Config\nfrom django.conf import settings\nfrom mypy_boto3_s3.client import S3Client\nfrom mypy_boto3_s3.service_resource import Bucket, Object\n\nfrom zerver.lib.avatar_hash import user_avatar_path\nfrom zerver.lib.upload.base import (\n    INLINE_MIME_TYPES,\n    MEDIUM_AVATAR_SIZE,\n    ZulipUploadBackend,\n    create_attachment,\n    resize_avatar,\n    resize_emoji,\n    resize_logo,\n    sanitize_name,\n)\nfrom zerver.models import Realm, RealmEmoji, UserProfile\n\n# Duration that the signed upload URLs that we redirect to when\n# accessing uploaded files are available for clients to fetch before\n# they expire.\nSIGNED_UPLOAD_URL_DURATION = 60\n\n# Performance note:\n#\n# For writing files to S3, the file could either be stored in RAM\n# (if it is less than 2.5MiB or so) or an actual temporary file on disk.\n#\n# Because we set FILE_UPLOAD_MAX_MEMORY_SIZE to 0, only the latter case\n# should occur in practice.\n#\n# This is great, because passing the pseudofile object that Django gives\n# you to boto would be a pain.\n\n# To come up with a s3 key we randomly generate a \"directory\". The\n# \"file name\" is the original filename provided by the user run\n# through a sanitization function.\n\n\n# https://github.com/boto/botocore/issues/2644 means that the IMDS\n# request _always_ pulls from the environment.  Monkey-patch the\n# `should_bypass_proxies` function if we need to skip them, based\n# on S3_SKIP_PROXY.\nif settings.S3_SKIP_PROXY is True:  # nocoverage\n    botocore.utils.should_bypass_proxies = lambda url: True\n\n\ndef get_bucket(bucket_name: str, session: Optional[Session] = None) -> Bucket:\n    if session is None:\n        session = Session(settings.S3_KEY, settings.S3_SECRET_KEY)\n    bucket = session.resource(\n        \"s3\", region_name=settings.S3_REGION, endpoint_url=settings.S3_ENDPOINT_URL\n    ).Bucket(bucket_name)\n    return bucket\n\n\ndef upload_image_to_s3(\n    bucket: Bucket,\n    file_name: str,\n    content_type: Optional[str],\n    user_profile: UserProfile,\n    contents: bytes,\n) -> None:\n    key = bucket.Object(file_name)\n    metadata = {\n        \"user_profile_id\": str(user_profile.id),\n        \"realm_id\": str(user_profile.realm_id),\n    }\n\n    content_disposition = \"\"\n    if content_type is None:\n        content_type = \"\"\n    if content_type not in INLINE_MIME_TYPES:\n        content_disposition = \"attachment\"\n\n    key.put(\n        Body=contents,\n        Metadata=metadata,\n        ContentType=content_type,\n        ContentDisposition=content_disposition,\n    )\n\n\ndef get_signed_upload_url(path: str) -> str:\n    client = boto3.client(\n        \"s3\",\n        aws_access_key_id=settings.S3_KEY,\n        aws_secret_access_key=settings.S3_SECRET_KEY,\n        region_name=settings.S3_REGION,\n        endpoint_url=settings.S3_ENDPOINT_URL,\n    )\n\n    return client.generate_presigned_url(\n        ClientMethod=\"get_object\",\n        Params={\n            \"Bucket\": settings.S3_AUTH_UPLOADS_BUCKET,\n            \"Key\": path,\n        },\n        ExpiresIn=SIGNED_UPLOAD_URL_DURATION,\n        HttpMethod=\"GET\",\n    )\n\n\nclass S3UploadBackend(ZulipUploadBackend):\n    def __init__(self) -> None:\n        self.session = Session(settings.S3_KEY, settings.S3_SECRET_KEY)\n        self.avatar_bucket = get_bucket(settings.S3_AVATAR_BUCKET, self.session)\n        self.uploads_bucket = get_bucket(settings.S3_AUTH_UPLOADS_BUCKET, self.session)\n\n        self._boto_client: Optional[S3Client] = None\n        self.public_upload_url_base = self.construct_public_upload_url_base()\n\n    def construct_public_upload_url_base(self) -> str:\n        # Return the pattern for public URL for a key in the S3 Avatar bucket.\n        # For Amazon S3 itself, this will return the following:\n        #     f\"https://{self.avatar_bucket.name}.{network_location}/{key}\"\n        #\n        # However, we need this function to properly handle S3 style\n        # file upload backends that Zulip supports, which can have a\n        # different URL format. Configuring no signature and providing\n        # no access key makes `generate_presigned_url` just return the\n        # normal public URL for a key.\n        #\n        # It unfortunately takes 2ms per query to call\n        # generate_presigned_url, even with our cached boto\n        # client. Since we need to potentially compute hundreds of\n        # avatar URLs in single `GET /messages` request, we instead\n        # back-compute the URL pattern here.\n\n        DUMMY_KEY = \"dummy_key_ignored\"\n        foo_url = self.get_boto_client().generate_presigned_url(\n            ClientMethod=\"get_object\",\n            Params={\n                \"Bucket\": self.avatar_bucket.name,\n                \"Key\": DUMMY_KEY,\n            },\n            ExpiresIn=0,\n        )\n        split_url = urllib.parse.urlsplit(foo_url)\n        assert split_url.path.endswith(f\"/{DUMMY_KEY}\")\n\n        return urllib.parse.urlunsplit(\n            (split_url.scheme, split_url.netloc, split_url.path[: -len(DUMMY_KEY)], \"\", \"\")\n        )\n\n    def get_public_upload_url(\n        self,\n        key: str,\n    ) -> str:\n        assert not key.startswith(\"/\")\n        return urllib.parse.urljoin(self.public_upload_url_base, key)\n\n    def get_boto_client(self) -> S3Client:\n        \"\"\"\n        Creating the client takes a long time so we need to cache it.\n        \"\"\"\n        if self._boto_client is None:\n            config = Config(signature_version=botocore.UNSIGNED)\n            self._boto_client = self.session.client(\n                \"s3\",\n                region_name=settings.S3_REGION,\n                endpoint_url=settings.S3_ENDPOINT_URL,\n                config=config,\n            )\n        return self._boto_client\n\n    def delete_file_from_s3(self, path_id: str, bucket: Bucket) -> bool:\n        key = bucket.Object(path_id)\n\n        try:\n            key.load()\n        except botocore.exceptions.ClientError:\n            file_name = path_id.split(\"/\")[-1]\n            logging.warning(\n                \"%s does not exist. Its entry in the database will be removed.\", file_name\n            )\n            return False\n        key.delete()\n        return True\n\n    def get_public_upload_root_url(self) -> str:\n        return self.public_upload_url_base\n\n    def generate_message_upload_path(self, realm_id: str, uploaded_file_name: str) -> str:\n        return \"/\".join(\n            [\n                realm_id,\n                secrets.token_urlsafe(18),\n                sanitize_name(uploaded_file_name),\n            ]\n        )\n\n    def upload_message_file(\n        self,\n        uploaded_file_name: str,\n        uploaded_file_size: int,\n        content_type: Optional[str],\n        file_data: bytes,\n        user_profile: UserProfile,\n        target_realm: Optional[Realm] = None,\n    ) -> str:\n        if target_realm is None:\n            target_realm = user_profile.realm\n        s3_file_name = self.generate_message_upload_path(str(target_realm.id), uploaded_file_name)\n        url = f\"/user_uploads/{s3_file_name}\"\n\n        upload_image_to_s3(\n            self.uploads_bucket,\n            s3_file_name,\n            content_type,\n            user_profile,\n            file_data,\n        )\n\n        create_attachment(\n            uploaded_file_name, s3_file_name, user_profile, target_realm, uploaded_file_size\n        )\n        return url\n\n    def delete_message_image(self, path_id: str) -> bool:\n        return self.delete_file_from_s3(path_id, self.uploads_bucket)\n\n    def write_avatar_images(\n        self,\n        s3_file_name: str,\n        target_user_profile: UserProfile,\n        image_data: bytes,\n        content_type: Optional[str],\n    ) -> None:\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".original\",\n            content_type,\n            target_user_profile,\n            image_data,\n        )\n\n        # custom 500px wide version\n        resized_medium = resize_avatar(image_data, MEDIUM_AVATAR_SIZE)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \"-medium.png\",\n            \"image/png\",\n            target_user_profile,\n            resized_medium,\n        )\n\n        resized_data = resize_avatar(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name,\n            \"image/png\",\n            target_user_profile,\n            resized_data,\n        )\n        # See avatar_url in avatar.py for URL.  (That code also handles the case\n        # that users use gravatar.)\n\n    def upload_avatar_image(\n        self,\n        user_file: IO[bytes],\n        acting_user_profile: UserProfile,\n        target_user_profile: UserProfile,\n        content_type: Optional[str] = None,\n    ) -> None:\n        if content_type is None:\n            content_type = guess_type(user_file.name)[0]\n        s3_file_name = user_avatar_path(target_user_profile)\n\n        image_data = user_file.read()\n        self.write_avatar_images(s3_file_name, target_user_profile, image_data, content_type)\n\n    def delete_avatar_image(self, user: UserProfile) -> None:\n        path_id = user_avatar_path(user)\n\n        self.delete_file_from_s3(path_id + \".original\", self.avatar_bucket)\n        self.delete_file_from_s3(path_id + \"-medium.png\", self.avatar_bucket)\n        self.delete_file_from_s3(path_id, self.avatar_bucket)\n\n    def get_avatar_key(self, file_name: str) -> Object:\n        key = self.avatar_bucket.Object(file_name)\n        return key\n\n    def copy_avatar(self, source_profile: UserProfile, target_profile: UserProfile) -> None:\n        s3_source_file_name = user_avatar_path(source_profile)\n        s3_target_file_name = user_avatar_path(target_profile)\n\n        key = self.get_avatar_key(s3_source_file_name + \".original\")\n        image_data = key.get()[\"Body\"].read()\n        content_type = key.content_type\n\n        self.write_avatar_images(s3_target_file_name, target_profile, image_data, content_type)\n\n    def get_avatar_url(self, hash_key: str, medium: bool = False) -> str:\n        medium_suffix = \"-medium.png\" if medium else \"\"\n        return self.get_public_upload_url(f\"{hash_key}{medium_suffix}\")\n\n    def get_export_tarball_url(self, realm: Realm, export_path: str) -> str:\n        # export_path has a leading /\n        return self.get_public_upload_url(export_path[1:])\n\n    def upload_realm_icon_image(self, icon_file: IO[bytes], user_profile: UserProfile) -> None:\n        content_type = guess_type(icon_file.name)[0]\n        s3_file_name = os.path.join(self.realm_avatar_and_logo_path(user_profile.realm), \"icon\")\n\n        image_data = icon_file.read()\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".original\",\n            content_type,\n            user_profile,\n            image_data,\n        )\n\n        resized_data = resize_avatar(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".png\",\n            \"image/png\",\n            user_profile,\n            resized_data,\n        )\n        # See avatar_url in avatar.py for URL.  (That code also handles the case\n        # that users use gravatar.)\n\n    def get_realm_icon_url(self, realm_id: int, version: int) -> str:\n        public_url = self.get_public_upload_url(f\"{realm_id}/realm/icon.png\")\n        return public_url + f\"?version={version}\"\n\n    def upload_realm_logo_image(\n        self, logo_file: IO[bytes], user_profile: UserProfile, night: bool\n    ) -> None:\n        content_type = guess_type(logo_file.name)[0]\n        if night:\n            basename = \"night_logo\"\n        else:\n            basename = \"logo\"\n        s3_file_name = os.path.join(self.realm_avatar_and_logo_path(user_profile.realm), basename)\n\n        image_data = logo_file.read()\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".original\",\n            content_type,\n            user_profile,\n            image_data,\n        )\n\n        resized_data = resize_logo(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".png\",\n            \"image/png\",\n            user_profile,\n            resized_data,\n        )\n        # See avatar_url in avatar.py for URL.  (That code also handles the case\n        # that users use gravatar.)\n\n    def get_realm_logo_url(self, realm_id: int, version: int, night: bool) -> str:\n        if not night:\n            file_name = \"logo.png\"\n        else:\n            file_name = \"night_logo.png\"\n        public_url = self.get_public_upload_url(f\"{realm_id}/realm/{file_name}\")\n        return public_url + f\"?version={version}\"\n\n    def ensure_avatar_image(self, user_profile: UserProfile, is_medium: bool = False) -> None:\n        # BUG: The else case should be user_avatar_path(user_profile) + \".png\".\n        # See #12852 for details on this bug and how to migrate it.\n        file_extension = \"-medium.png\" if is_medium else \"\"\n        file_path = user_avatar_path(user_profile)\n        s3_file_name = file_path\n\n        key = self.avatar_bucket.Object(file_path + \".original\")\n        image_data = key.get()[\"Body\"].read()\n\n        if is_medium:\n            resized_avatar = resize_avatar(image_data, MEDIUM_AVATAR_SIZE)\n        else:\n            resized_avatar = resize_avatar(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + file_extension,\n            \"image/png\",\n            user_profile,\n            resized_avatar,\n        )\n\n    def upload_emoji_image(\n        self, emoji_file: IO[bytes], emoji_file_name: str, user_profile: UserProfile\n    ) -> bool:\n        content_type = guess_type(emoji_file_name)[0]\n        emoji_path = RealmEmoji.PATH_ID_TEMPLATE.format(\n            realm_id=user_profile.realm_id,\n            emoji_file_name=emoji_file_name,\n        )\n\n        image_data = emoji_file.read()\n        upload_image_to_s3(\n            self.avatar_bucket,\n            \".\".join((emoji_path, \"original\")),\n            content_type,\n            user_profile,\n            image_data,\n        )\n\n        resized_image_data, is_animated, still_image_data = resize_emoji(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            emoji_path,\n            content_type,\n            user_profile,\n            resized_image_data,\n        )\n        if is_animated:\n            still_path = RealmEmoji.STILL_PATH_ID_TEMPLATE.format(\n                realm_id=user_profile.realm_id,\n                emoji_filename_without_extension=os.path.splitext(emoji_file_name)[0],\n            )\n            assert still_image_data is not None\n            upload_image_to_s3(\n                self.avatar_bucket,\n                still_path,\n                \"image/png\",\n                user_profile,\n                still_image_data,\n            )\n\n        return is_animated\n\n    def get_emoji_url(self, emoji_file_name: str, realm_id: int, still: bool = False) -> str:\n        if still:\n            emoji_path = RealmEmoji.STILL_PATH_ID_TEMPLATE.format(\n                realm_id=realm_id,\n                emoji_filename_without_extension=os.path.splitext(emoji_file_name)[0],\n            )\n            return self.get_public_upload_url(emoji_path)\n        else:\n            emoji_path = RealmEmoji.PATH_ID_TEMPLATE.format(\n                realm_id=realm_id, emoji_file_name=emoji_file_name\n            )\n            return self.get_public_upload_url(emoji_path)\n\n    def upload_export_tarball(\n        self,\n        realm: Optional[Realm],\n        tarball_path: str,\n        percent_callback: Optional[Callable[[Any], None]] = None,\n    ) -> str:\n        # We use the avatar bucket, because it's world-readable.\n        key = self.avatar_bucket.Object(\n            os.path.join(\"exports\", secrets.token_hex(16), os.path.basename(tarball_path))\n        )\n\n        if percent_callback is None:\n            key.upload_file(Filename=tarball_path)\n        else:\n            key.upload_file(Filename=tarball_path, Callback=percent_callback)\n\n        public_url = self.get_public_upload_url(key.key)\n        return public_url\n\n    def delete_export_tarball(self, export_path: str) -> Optional[str]:\n        assert export_path.startswith(\"/\")\n        path_id = export_path[1:]\n        if self.delete_file_from_s3(path_id, self.avatar_bucket):\n            return export_path\n        return None\n", "import base64\nimport binascii\nimport os\nfrom datetime import timedelta\nfrom mimetypes import guess_type\nfrom typing import Optional, Union\nfrom urllib.parse import quote, urlparse\n\nfrom django.conf import settings\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.core.files.uploadedfile import UploadedFile\nfrom django.core.signing import BadSignature, TimestampSigner\nfrom django.http import (\n    FileResponse,\n    HttpRequest,\n    HttpResponse,\n    HttpResponseBase,\n    HttpResponseForbidden,\n    HttpResponseNotFound,\n)\nfrom django.shortcuts import redirect\nfrom django.urls import reverse\nfrom django.utils.cache import patch_cache_control\nfrom django.utils.translation import gettext as _\n\nfrom zerver.context_processors import get_valid_realm_from_request\nfrom zerver.lib.exceptions import JsonableError\nfrom zerver.lib.response import json_success\nfrom zerver.lib.upload import (\n    check_upload_within_quota,\n    get_public_upload_root_url,\n    upload_message_image_from_request,\n)\nfrom zerver.lib.upload.base import INLINE_MIME_TYPES\nfrom zerver.lib.upload.local import assert_is_local_storage_path\nfrom zerver.lib.upload.s3 import get_signed_upload_url\nfrom zerver.models import UserProfile, validate_attachment_request\n\n\ndef patch_disposition_header(response: HttpResponse, url: str, is_attachment: bool) -> None:\n    \"\"\"\n    This replicates django.utils.http.content_disposition_header's\n    algorithm, which is introduced in Django 4.2.\n\n    \"\"\"\n    # TODO: Replace this with django.utils.http.content_disposition_header when we upgrade in Django 4.2\n    disposition = \"attachment\" if is_attachment else \"inline\"\n\n    # Trim to only the filename part of the URL\n    filename = os.path.basename(urlparse(url).path)\n\n    # Content-Disposition is defined in RFC 6266:\n    # https://datatracker.ietf.org/doc/html/rfc6266\n    #\n    # For the 'filename' attribute of it, see RFC 8187:\n    # https://datatracker.ietf.org/doc/html/rfc8187\n    try:\n        # If the filename is pure-ASCII (determined by trying to\n        # encode it as such), then we escape slashes and quotes, and\n        # provide a filename=\"...\"\n        filename.encode(\"ascii\")\n        file_expr = 'filename=\"{}\"'.format(filename.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', r\"\\\"\"))\n    except UnicodeEncodeError:\n        # If it contains non-ASCII characters, we URI-escape it and\n        # provide a filename*=encoding'language'value\n        file_expr = f\"filename*=utf-8''{quote(filename)}\"\n\n    response.headers[\"Content-Disposition\"] = f\"{disposition}; {file_expr}\"\n\n\ndef internal_nginx_redirect(internal_path: str) -> HttpResponse:\n    # The following headers from this initial response are\n    # _preserved_, if present, and sent unmodified to the client;\n    # all other headers are overridden by the redirected URL:\n    #  - Content-Type\n    #  - Content-Disposition\n    #  - Accept-Ranges\n    #  - Set-Cookie\n    #  - Cache-Control\n    #  - Expires\n    # As such, we unset the Content-type header to allow nginx to set\n    # it from the static file; the caller can set Content-Disposition\n    # and Cache-Control on this response as they desire, and the\n    # client will see those values.\n    response = HttpResponse()\n    response[\"X-Accel-Redirect\"] = internal_path\n    del response[\"Content-Type\"]\n    return response\n\n\ndef serve_s3(request: HttpRequest, path_id: str, download: bool = False) -> HttpResponse:\n    url = get_signed_upload_url(path_id)\n    assert url.startswith(\"https://\")\n\n    if settings.DEVELOPMENT:\n        # In development, we do not have the nginx server to offload\n        # the response to; serve a redirect to the short-lived S3 URL.\n        # This means the content cannot be cached by the browser, but\n        # this is acceptable in development.\n        return redirect(url)\n\n    # We over-escape the path, to work around it being impossible to\n    # get the _unescaped_ new internal request URI in nginx.\n    parsed_url = urlparse(url)\n    assert parsed_url.hostname is not None\n    assert parsed_url.path is not None\n    assert parsed_url.query is not None\n    escaped_path_parts = parsed_url.hostname + quote(parsed_url.path) + \"?\" + parsed_url.query\n    response = internal_nginx_redirect(\"/internal/s3/\" + escaped_path_parts)\n    patch_disposition_header(response, path_id, download)\n    patch_cache_control(response, private=True, immutable=True)\n    return response\n\n\ndef serve_local(request: HttpRequest, path_id: str, download: bool = False) -> HttpResponseBase:\n    assert settings.LOCAL_FILES_DIR is not None\n    local_path = os.path.join(settings.LOCAL_FILES_DIR, path_id)\n    assert_is_local_storage_path(\"files\", local_path)\n    if not os.path.isfile(local_path):\n        return HttpResponseNotFound(\"<p>File not found</p>\")\n\n    if settings.DEVELOPMENT:\n        # In development, we do not have the nginx server to offload\n        # the response to; serve it directly ourselves.\n        # FileResponse handles setting Content-Disposition, etc.\n        response: HttpResponseBase = FileResponse(\n            open(local_path, \"rb\"), as_attachment=download  # noqa: SIM115\n        )\n        patch_cache_control(response, private=True, immutable=True)\n        return response\n\n    response = internal_nginx_redirect(quote(f\"/internal/local/uploads/{path_id}\"))\n    patch_disposition_header(response, local_path, download)\n    patch_cache_control(response, private=True, immutable=True)\n    return response\n\n\ndef serve_file_download_backend(\n    request: HttpRequest, user_profile: UserProfile, realm_id_str: str, filename: str\n) -> HttpResponseBase:\n    return serve_file(request, user_profile, realm_id_str, filename, url_only=False, download=True)\n\n\ndef serve_file_backend(\n    request: HttpRequest,\n    maybe_user_profile: Union[UserProfile, AnonymousUser],\n    realm_id_str: str,\n    filename: str,\n) -> HttpResponseBase:\n    return serve_file(request, maybe_user_profile, realm_id_str, filename, url_only=False)\n\n\ndef serve_file_url_backend(\n    request: HttpRequest, user_profile: UserProfile, realm_id_str: str, filename: str\n) -> HttpResponseBase:\n    \"\"\"\n    We should return a signed, short-lived URL\n    that the client can use for native mobile download, rather than serving a redirect.\n    \"\"\"\n\n    return serve_file(request, user_profile, realm_id_str, filename, url_only=True)\n\n\ndef serve_file(\n    request: HttpRequest,\n    maybe_user_profile: Union[UserProfile, AnonymousUser],\n    realm_id_str: str,\n    filename: str,\n    url_only: bool = False,\n    download: bool = False,\n) -> HttpResponseBase:\n    path_id = f\"{realm_id_str}/{filename}\"\n    realm = get_valid_realm_from_request(request)\n    is_authorized = validate_attachment_request(maybe_user_profile, path_id, realm)\n\n    if is_authorized is None:\n        return HttpResponseNotFound(_(\"<p>File not found.</p>\"))\n    if not is_authorized:\n        return HttpResponseForbidden(_(\"<p>You are not authorized to view this file.</p>\"))\n    if url_only:\n        url = generate_unauthed_file_access_url(path_id)\n        return json_success(request, data=dict(url=url))\n\n    mimetype, encoding = guess_type(path_id)\n    download = download or mimetype not in INLINE_MIME_TYPES\n\n    if settings.LOCAL_UPLOADS_DIR is not None:\n        return serve_local(request, path_id, download=download)\n    else:\n        return serve_s3(request, path_id, download=download)\n\n\nUSER_UPLOADS_ACCESS_TOKEN_SALT = \"user_uploads_\"\n\n\ndef generate_unauthed_file_access_url(path_id: str) -> str:\n    signed_data = TimestampSigner(salt=USER_UPLOADS_ACCESS_TOKEN_SALT).sign(path_id)\n    token = base64.b16encode(signed_data.encode()).decode()\n\n    filename = path_id.split(\"/\")[-1]\n    return reverse(\"file_unauthed_from_token\", args=[token, filename])\n\n\ndef get_file_path_id_from_token(token: str) -> Optional[str]:\n    signer = TimestampSigner(salt=USER_UPLOADS_ACCESS_TOKEN_SALT)\n    try:\n        signed_data = base64.b16decode(token).decode()\n        path_id = signer.unsign(signed_data, max_age=timedelta(seconds=60))\n    except (BadSignature, binascii.Error):\n        return None\n\n    return path_id\n\n\ndef serve_file_unauthed_from_token(\n    request: HttpRequest, token: str, filename: str\n) -> HttpResponseBase:\n    path_id = get_file_path_id_from_token(token)\n    if path_id is None:\n        raise JsonableError(_(\"Invalid token\"))\n    if path_id.split(\"/\")[-1] != filename:\n        raise JsonableError(_(\"Invalid filename\"))\n\n    mimetype, encoding = guess_type(path_id)\n    download = mimetype not in INLINE_MIME_TYPES\n\n    if settings.LOCAL_UPLOADS_DIR is not None:\n        return serve_local(request, path_id, download=download)\n    else:\n        return serve_s3(request, path_id, download=download)\n\n\ndef serve_local_avatar_unauthed(request: HttpRequest, path: str) -> HttpResponseBase:\n    \"\"\"Serves avatar images off disk, via nginx (or directly in dev), with no auth.\n\n    This is done unauthed because these need to be accessed from HTML\n    emails, where the client does not have any auth.  We rely on the\n    URL being generated using the AVATAR_SALT secret.\n\n    \"\"\"\n    if settings.LOCAL_AVATARS_DIR is None:\n        # We do not expect clients to hit this URL when using the S3\n        # backend; however, there is no reason to not serve the\n        # redirect to S3 where the content lives.\n        return redirect(\n            get_public_upload_root_url() + path + \"?\" + request.GET.urlencode(), permanent=True\n        )\n\n    local_path = os.path.join(settings.LOCAL_AVATARS_DIR, path)\n    assert_is_local_storage_path(\"avatars\", local_path)\n    if not os.path.isfile(local_path):\n        return HttpResponseNotFound(\"<p>File not found</p>\")\n\n    if settings.DEVELOPMENT:\n        response: HttpResponseBase = FileResponse(open(local_path, \"rb\"))  # noqa: SIM115\n    else:\n        response = internal_nginx_redirect(quote(f\"/internal/local/user_avatars/{path}\"))\n\n    # We do _not_ mark the contents as immutable for caching purposes,\n    # since the path for avatar images is hashed only by their user-id\n    # and a salt, and as such are reused when a user's avatar is\n    # updated.\n    return response\n\n\ndef upload_file_backend(request: HttpRequest, user_profile: UserProfile) -> HttpResponse:\n    if len(request.FILES) == 0:\n        raise JsonableError(_(\"You must specify a file to upload\"))\n    if len(request.FILES) != 1:\n        raise JsonableError(_(\"You may only upload one file at a time\"))\n\n    user_file = list(request.FILES.values())[0]\n    assert isinstance(user_file, UploadedFile)\n    file_size = user_file.size\n    assert file_size is not None\n    if settings.MAX_FILE_UPLOAD_SIZE * 1024 * 1024 < file_size:\n        raise JsonableError(\n            _(\"Uploaded file is larger than the allowed limit of {} MiB\").format(\n                settings.MAX_FILE_UPLOAD_SIZE,\n            )\n        )\n    check_upload_within_quota(user_profile.realm, file_size)\n\n    uri = upload_message_image_from_request(user_file, user_profile, file_size)\n    return json_success(request, data={\"uri\": uri})\n"], "fixing_code": ["# Handle redirects to S3\nlocation ~ ^/internal/s3/(?<s3_hostname>[^/]+)/(?<s3_path>.*) {\n    internal;\n    include /etc/nginx/zulip-include/headers;\n    add_header Content-Security-Policy \"default-src 'none'; style-src 'self' 'unsafe-inline'; img-src 'self'; object-src 'self'; plugin-types application/pdf;\";\n\n    # The components of this path are originally double-URI-escaped\n    # (see zerver/view/upload.py).  \"location\" matches are on\n    # unescaped values, which fills $s3_path with a properly\n    # single-escaped path to pass to the upstream server.\n    # (see associated commit message for more details)\n    set $download_url https://$s3_hostname/$s3_path;\n    proxy_set_header Host $s3_hostname;\n\n    # Ensure that we only get _one_ of these headers: the one that\n    # Django added, not the one from S3.\n    proxy_hide_header Cache-Control;\n    proxy_hide_header Expires;\n    proxy_hide_header Set-Cookie;\n    # We are _leaving_ S3 to provide Content-Type,\n    # Content-Disposition, and Accept-Ranges headers, which are the\n    # three remaining headers which nginx would also pass through from\n    # the first response.  Django explicitly unsets the first, and\n    # does not set the latter two.\n\n    # nginx does its own DNS resolution, which is necessary here to\n    # resolve the IP of the S3 server.  Point it at the local caching\n    # systemd resolved service.  The validity duration is set to match\n    # S3's DNS validity.\n    resolver 127.0.0.53 valid=300s;\n    resolver_timeout 10s;\n\n    proxy_pass $download_url$is_args$args;\n    proxy_cache uploads;\n    # If the S3 response doesn't contain Cache-Control headers (which\n    # we don't expect it to) then we assume they are valid for a very\n    # long time.  The size of the cache is controlled by\n    # `s3_disk_cache_size` and read frequency, set via\n    # `s3_cache_inactive_time`.\n    proxy_cache_valid 200 1y;\n\n    # We only include the requested content-disposition in the cache\n    # key, so that we cache \"Content-Disposition: attachment\"\n    # separately from the inline version.\n    proxy_cache_key $download_url$s3_disposition_cache_key;\n}\n\n# Internal file-serving\nlocation /internal/local/uploads {\n    internal;\n    include /etc/nginx/zulip-include/headers;\n    add_header Content-Security-Policy \"default-src 'none'; style-src 'self' 'unsafe-inline'; img-src 'self'; object-src 'self'; plugin-types application/pdf;\";\n    include /etc/nginx/zulip-include/uploads.types;\n    alias /home/zulip/uploads/files;\n}\n\nlocation /internal/local/user_avatars {\n    internal;\n    include /etc/nginx/zulip-include/headers;\n    add_header Content-Security-Policy \"default-src 'none' img-src 'self'\";\n    include /etc/nginx/zulip-include/uploads.types;\n    alias /home/zulip/uploads/avatars;\n}\n", "# This cache is only used if S3 file storage is configured.\nproxy_cache_path /srv/zulip-uploaded-files-cache\n    levels=1:2\n    keys_zone=uploads:<%= @s3_memory_cache_size %>\n    inactive=<%= @s3_cache_inactive_time %>\n    max_size=<%= @s3_disk_cache_size %>;\n\n# This is used when proxying requests to S3; we wish to know if the\n# proxied request is asking to override the Content-Disposition in its\n# response, so we can adjust our cache key.  Unfortunately, $arg_foo\n# style variables pre-parsed from query parameters don't work with\n# query parameters with dashes, so we parse it out by hand.  Despite\n# needing to be declared at the 'http' level,. nginx applies maps like\n# this lazily, so this only affects internal S3 proxied requests.\n\nmap $args $s3_disposition_cache_key {\n    default \"\";\n    \"~(^|&)(?<param>response-content-disposition=[^&]+)\" \"?$param\";\n}\n", "import logging\nimport os\nimport secrets\nimport urllib\nfrom mimetypes import guess_type\nfrom typing import IO, Any, Callable, Optional\n\nimport boto3\nimport botocore\nfrom boto3.session import Session\nfrom botocore.client import Config\nfrom django.conf import settings\nfrom mypy_boto3_s3.client import S3Client\nfrom mypy_boto3_s3.service_resource import Bucket, Object\n\nfrom zerver.lib.avatar_hash import user_avatar_path\nfrom zerver.lib.upload.base import (\n    INLINE_MIME_TYPES,\n    MEDIUM_AVATAR_SIZE,\n    ZulipUploadBackend,\n    create_attachment,\n    resize_avatar,\n    resize_emoji,\n    resize_logo,\n    sanitize_name,\n)\nfrom zerver.models import Realm, RealmEmoji, UserProfile\n\n# Duration that the signed upload URLs that we redirect to when\n# accessing uploaded files are available for clients to fetch before\n# they expire.\nSIGNED_UPLOAD_URL_DURATION = 60\n\n# Performance note:\n#\n# For writing files to S3, the file could either be stored in RAM\n# (if it is less than 2.5MiB or so) or an actual temporary file on disk.\n#\n# Because we set FILE_UPLOAD_MAX_MEMORY_SIZE to 0, only the latter case\n# should occur in practice.\n#\n# This is great, because passing the pseudofile object that Django gives\n# you to boto would be a pain.\n\n# To come up with a s3 key we randomly generate a \"directory\". The\n# \"file name\" is the original filename provided by the user run\n# through a sanitization function.\n\n\n# https://github.com/boto/botocore/issues/2644 means that the IMDS\n# request _always_ pulls from the environment.  Monkey-patch the\n# `should_bypass_proxies` function if we need to skip them, based\n# on S3_SKIP_PROXY.\nif settings.S3_SKIP_PROXY is True:  # nocoverage\n    botocore.utils.should_bypass_proxies = lambda url: True\n\n\ndef get_bucket(bucket_name: str, session: Optional[Session] = None) -> Bucket:\n    if session is None:\n        session = Session(settings.S3_KEY, settings.S3_SECRET_KEY)\n    bucket = session.resource(\n        \"s3\", region_name=settings.S3_REGION, endpoint_url=settings.S3_ENDPOINT_URL\n    ).Bucket(bucket_name)\n    return bucket\n\n\ndef upload_image_to_s3(\n    bucket: Bucket,\n    file_name: str,\n    content_type: Optional[str],\n    user_profile: UserProfile,\n    contents: bytes,\n) -> None:\n    key = bucket.Object(file_name)\n    metadata = {\n        \"user_profile_id\": str(user_profile.id),\n        \"realm_id\": str(user_profile.realm_id),\n    }\n\n    content_disposition = \"\"\n    if content_type is None:\n        content_type = \"\"\n    if content_type not in INLINE_MIME_TYPES:\n        content_disposition = \"attachment\"\n\n    key.put(\n        Body=contents,\n        Metadata=metadata,\n        ContentType=content_type,\n        ContentDisposition=content_disposition,\n    )\n\n\ndef get_signed_upload_url(path: str, force_download: bool = False) -> str:\n    client = boto3.client(\n        \"s3\",\n        aws_access_key_id=settings.S3_KEY,\n        aws_secret_access_key=settings.S3_SECRET_KEY,\n        region_name=settings.S3_REGION,\n        endpoint_url=settings.S3_ENDPOINT_URL,\n    )\n    params = {\n        \"Bucket\": settings.S3_AUTH_UPLOADS_BUCKET,\n        \"Key\": path,\n    }\n    if force_download:\n        params[\"ResponseContentDisposition\"] = \"attachment\"\n\n    return client.generate_presigned_url(\n        ClientMethod=\"get_object\",\n        Params=params,\n        ExpiresIn=SIGNED_UPLOAD_URL_DURATION,\n        HttpMethod=\"GET\",\n    )\n\n\nclass S3UploadBackend(ZulipUploadBackend):\n    def __init__(self) -> None:\n        self.session = Session(settings.S3_KEY, settings.S3_SECRET_KEY)\n        self.avatar_bucket = get_bucket(settings.S3_AVATAR_BUCKET, self.session)\n        self.uploads_bucket = get_bucket(settings.S3_AUTH_UPLOADS_BUCKET, self.session)\n\n        self._boto_client: Optional[S3Client] = None\n        self.public_upload_url_base = self.construct_public_upload_url_base()\n\n    def construct_public_upload_url_base(self) -> str:\n        # Return the pattern for public URL for a key in the S3 Avatar bucket.\n        # For Amazon S3 itself, this will return the following:\n        #     f\"https://{self.avatar_bucket.name}.{network_location}/{key}\"\n        #\n        # However, we need this function to properly handle S3 style\n        # file upload backends that Zulip supports, which can have a\n        # different URL format. Configuring no signature and providing\n        # no access key makes `generate_presigned_url` just return the\n        # normal public URL for a key.\n        #\n        # It unfortunately takes 2ms per query to call\n        # generate_presigned_url, even with our cached boto\n        # client. Since we need to potentially compute hundreds of\n        # avatar URLs in single `GET /messages` request, we instead\n        # back-compute the URL pattern here.\n\n        DUMMY_KEY = \"dummy_key_ignored\"\n        foo_url = self.get_boto_client().generate_presigned_url(\n            ClientMethod=\"get_object\",\n            Params={\n                \"Bucket\": self.avatar_bucket.name,\n                \"Key\": DUMMY_KEY,\n            },\n            ExpiresIn=0,\n        )\n        split_url = urllib.parse.urlsplit(foo_url)\n        assert split_url.path.endswith(f\"/{DUMMY_KEY}\")\n\n        return urllib.parse.urlunsplit(\n            (split_url.scheme, split_url.netloc, split_url.path[: -len(DUMMY_KEY)], \"\", \"\")\n        )\n\n    def get_public_upload_url(\n        self,\n        key: str,\n    ) -> str:\n        assert not key.startswith(\"/\")\n        return urllib.parse.urljoin(self.public_upload_url_base, key)\n\n    def get_boto_client(self) -> S3Client:\n        \"\"\"\n        Creating the client takes a long time so we need to cache it.\n        \"\"\"\n        if self._boto_client is None:\n            config = Config(signature_version=botocore.UNSIGNED)\n            self._boto_client = self.session.client(\n                \"s3\",\n                region_name=settings.S3_REGION,\n                endpoint_url=settings.S3_ENDPOINT_URL,\n                config=config,\n            )\n        return self._boto_client\n\n    def delete_file_from_s3(self, path_id: str, bucket: Bucket) -> bool:\n        key = bucket.Object(path_id)\n\n        try:\n            key.load()\n        except botocore.exceptions.ClientError:\n            file_name = path_id.split(\"/\")[-1]\n            logging.warning(\n                \"%s does not exist. Its entry in the database will be removed.\", file_name\n            )\n            return False\n        key.delete()\n        return True\n\n    def get_public_upload_root_url(self) -> str:\n        return self.public_upload_url_base\n\n    def generate_message_upload_path(self, realm_id: str, uploaded_file_name: str) -> str:\n        return \"/\".join(\n            [\n                realm_id,\n                secrets.token_urlsafe(18),\n                sanitize_name(uploaded_file_name),\n            ]\n        )\n\n    def upload_message_file(\n        self,\n        uploaded_file_name: str,\n        uploaded_file_size: int,\n        content_type: Optional[str],\n        file_data: bytes,\n        user_profile: UserProfile,\n        target_realm: Optional[Realm] = None,\n    ) -> str:\n        if target_realm is None:\n            target_realm = user_profile.realm\n        s3_file_name = self.generate_message_upload_path(str(target_realm.id), uploaded_file_name)\n        url = f\"/user_uploads/{s3_file_name}\"\n\n        upload_image_to_s3(\n            self.uploads_bucket,\n            s3_file_name,\n            content_type,\n            user_profile,\n            file_data,\n        )\n\n        create_attachment(\n            uploaded_file_name, s3_file_name, user_profile, target_realm, uploaded_file_size\n        )\n        return url\n\n    def delete_message_image(self, path_id: str) -> bool:\n        return self.delete_file_from_s3(path_id, self.uploads_bucket)\n\n    def write_avatar_images(\n        self,\n        s3_file_name: str,\n        target_user_profile: UserProfile,\n        image_data: bytes,\n        content_type: Optional[str],\n    ) -> None:\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".original\",\n            content_type,\n            target_user_profile,\n            image_data,\n        )\n\n        # custom 500px wide version\n        resized_medium = resize_avatar(image_data, MEDIUM_AVATAR_SIZE)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \"-medium.png\",\n            \"image/png\",\n            target_user_profile,\n            resized_medium,\n        )\n\n        resized_data = resize_avatar(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name,\n            \"image/png\",\n            target_user_profile,\n            resized_data,\n        )\n        # See avatar_url in avatar.py for URL.  (That code also handles the case\n        # that users use gravatar.)\n\n    def upload_avatar_image(\n        self,\n        user_file: IO[bytes],\n        acting_user_profile: UserProfile,\n        target_user_profile: UserProfile,\n        content_type: Optional[str] = None,\n    ) -> None:\n        if content_type is None:\n            content_type = guess_type(user_file.name)[0]\n        s3_file_name = user_avatar_path(target_user_profile)\n\n        image_data = user_file.read()\n        self.write_avatar_images(s3_file_name, target_user_profile, image_data, content_type)\n\n    def delete_avatar_image(self, user: UserProfile) -> None:\n        path_id = user_avatar_path(user)\n\n        self.delete_file_from_s3(path_id + \".original\", self.avatar_bucket)\n        self.delete_file_from_s3(path_id + \"-medium.png\", self.avatar_bucket)\n        self.delete_file_from_s3(path_id, self.avatar_bucket)\n\n    def get_avatar_key(self, file_name: str) -> Object:\n        key = self.avatar_bucket.Object(file_name)\n        return key\n\n    def copy_avatar(self, source_profile: UserProfile, target_profile: UserProfile) -> None:\n        s3_source_file_name = user_avatar_path(source_profile)\n        s3_target_file_name = user_avatar_path(target_profile)\n\n        key = self.get_avatar_key(s3_source_file_name + \".original\")\n        image_data = key.get()[\"Body\"].read()\n        content_type = key.content_type\n\n        self.write_avatar_images(s3_target_file_name, target_profile, image_data, content_type)\n\n    def get_avatar_url(self, hash_key: str, medium: bool = False) -> str:\n        medium_suffix = \"-medium.png\" if medium else \"\"\n        return self.get_public_upload_url(f\"{hash_key}{medium_suffix}\")\n\n    def get_export_tarball_url(self, realm: Realm, export_path: str) -> str:\n        # export_path has a leading /\n        return self.get_public_upload_url(export_path[1:])\n\n    def upload_realm_icon_image(self, icon_file: IO[bytes], user_profile: UserProfile) -> None:\n        content_type = guess_type(icon_file.name)[0]\n        s3_file_name = os.path.join(self.realm_avatar_and_logo_path(user_profile.realm), \"icon\")\n\n        image_data = icon_file.read()\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".original\",\n            content_type,\n            user_profile,\n            image_data,\n        )\n\n        resized_data = resize_avatar(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".png\",\n            \"image/png\",\n            user_profile,\n            resized_data,\n        )\n        # See avatar_url in avatar.py for URL.  (That code also handles the case\n        # that users use gravatar.)\n\n    def get_realm_icon_url(self, realm_id: int, version: int) -> str:\n        public_url = self.get_public_upload_url(f\"{realm_id}/realm/icon.png\")\n        return public_url + f\"?version={version}\"\n\n    def upload_realm_logo_image(\n        self, logo_file: IO[bytes], user_profile: UserProfile, night: bool\n    ) -> None:\n        content_type = guess_type(logo_file.name)[0]\n        if night:\n            basename = \"night_logo\"\n        else:\n            basename = \"logo\"\n        s3_file_name = os.path.join(self.realm_avatar_and_logo_path(user_profile.realm), basename)\n\n        image_data = logo_file.read()\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".original\",\n            content_type,\n            user_profile,\n            image_data,\n        )\n\n        resized_data = resize_logo(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + \".png\",\n            \"image/png\",\n            user_profile,\n            resized_data,\n        )\n        # See avatar_url in avatar.py for URL.  (That code also handles the case\n        # that users use gravatar.)\n\n    def get_realm_logo_url(self, realm_id: int, version: int, night: bool) -> str:\n        if not night:\n            file_name = \"logo.png\"\n        else:\n            file_name = \"night_logo.png\"\n        public_url = self.get_public_upload_url(f\"{realm_id}/realm/{file_name}\")\n        return public_url + f\"?version={version}\"\n\n    def ensure_avatar_image(self, user_profile: UserProfile, is_medium: bool = False) -> None:\n        # BUG: The else case should be user_avatar_path(user_profile) + \".png\".\n        # See #12852 for details on this bug and how to migrate it.\n        file_extension = \"-medium.png\" if is_medium else \"\"\n        file_path = user_avatar_path(user_profile)\n        s3_file_name = file_path\n\n        key = self.avatar_bucket.Object(file_path + \".original\")\n        image_data = key.get()[\"Body\"].read()\n\n        if is_medium:\n            resized_avatar = resize_avatar(image_data, MEDIUM_AVATAR_SIZE)\n        else:\n            resized_avatar = resize_avatar(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            s3_file_name + file_extension,\n            \"image/png\",\n            user_profile,\n            resized_avatar,\n        )\n\n    def upload_emoji_image(\n        self, emoji_file: IO[bytes], emoji_file_name: str, user_profile: UserProfile\n    ) -> bool:\n        content_type = guess_type(emoji_file_name)[0]\n        emoji_path = RealmEmoji.PATH_ID_TEMPLATE.format(\n            realm_id=user_profile.realm_id,\n            emoji_file_name=emoji_file_name,\n        )\n\n        image_data = emoji_file.read()\n        upload_image_to_s3(\n            self.avatar_bucket,\n            \".\".join((emoji_path, \"original\")),\n            content_type,\n            user_profile,\n            image_data,\n        )\n\n        resized_image_data, is_animated, still_image_data = resize_emoji(image_data)\n        upload_image_to_s3(\n            self.avatar_bucket,\n            emoji_path,\n            content_type,\n            user_profile,\n            resized_image_data,\n        )\n        if is_animated:\n            still_path = RealmEmoji.STILL_PATH_ID_TEMPLATE.format(\n                realm_id=user_profile.realm_id,\n                emoji_filename_without_extension=os.path.splitext(emoji_file_name)[0],\n            )\n            assert still_image_data is not None\n            upload_image_to_s3(\n                self.avatar_bucket,\n                still_path,\n                \"image/png\",\n                user_profile,\n                still_image_data,\n            )\n\n        return is_animated\n\n    def get_emoji_url(self, emoji_file_name: str, realm_id: int, still: bool = False) -> str:\n        if still:\n            emoji_path = RealmEmoji.STILL_PATH_ID_TEMPLATE.format(\n                realm_id=realm_id,\n                emoji_filename_without_extension=os.path.splitext(emoji_file_name)[0],\n            )\n            return self.get_public_upload_url(emoji_path)\n        else:\n            emoji_path = RealmEmoji.PATH_ID_TEMPLATE.format(\n                realm_id=realm_id, emoji_file_name=emoji_file_name\n            )\n            return self.get_public_upload_url(emoji_path)\n\n    def upload_export_tarball(\n        self,\n        realm: Optional[Realm],\n        tarball_path: str,\n        percent_callback: Optional[Callable[[Any], None]] = None,\n    ) -> str:\n        # We use the avatar bucket, because it's world-readable.\n        key = self.avatar_bucket.Object(\n            os.path.join(\"exports\", secrets.token_hex(16), os.path.basename(tarball_path))\n        )\n\n        if percent_callback is None:\n            key.upload_file(Filename=tarball_path)\n        else:\n            key.upload_file(Filename=tarball_path, Callback=percent_callback)\n\n        public_url = self.get_public_upload_url(key.key)\n        return public_url\n\n    def delete_export_tarball(self, export_path: str) -> Optional[str]:\n        assert export_path.startswith(\"/\")\n        path_id = export_path[1:]\n        if self.delete_file_from_s3(path_id, self.avatar_bucket):\n            return export_path\n        return None\n", "import base64\nimport binascii\nimport os\nfrom datetime import timedelta\nfrom mimetypes import guess_type\nfrom typing import Optional, Union\nfrom urllib.parse import quote, urlparse\n\nfrom django.conf import settings\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.core.files.uploadedfile import UploadedFile\nfrom django.core.signing import BadSignature, TimestampSigner\nfrom django.http import (\n    FileResponse,\n    HttpRequest,\n    HttpResponse,\n    HttpResponseBase,\n    HttpResponseForbidden,\n    HttpResponseNotFound,\n)\nfrom django.shortcuts import redirect\nfrom django.urls import reverse\nfrom django.utils.cache import patch_cache_control\nfrom django.utils.translation import gettext as _\n\nfrom zerver.context_processors import get_valid_realm_from_request\nfrom zerver.lib.exceptions import JsonableError\nfrom zerver.lib.response import json_success\nfrom zerver.lib.upload import (\n    check_upload_within_quota,\n    get_public_upload_root_url,\n    upload_message_image_from_request,\n)\nfrom zerver.lib.upload.base import INLINE_MIME_TYPES\nfrom zerver.lib.upload.local import assert_is_local_storage_path\nfrom zerver.lib.upload.s3 import get_signed_upload_url\nfrom zerver.models import UserProfile, validate_attachment_request\n\n\ndef patch_disposition_header(response: HttpResponse, url: str, is_attachment: bool) -> None:\n    \"\"\"\n    This replicates django.utils.http.content_disposition_header's\n    algorithm, which is introduced in Django 4.2.\n\n    \"\"\"\n    # TODO: Replace this with django.utils.http.content_disposition_header when we upgrade in Django 4.2\n    disposition = \"attachment\" if is_attachment else \"inline\"\n\n    # Trim to only the filename part of the URL\n    filename = os.path.basename(urlparse(url).path)\n\n    # Content-Disposition is defined in RFC 6266:\n    # https://datatracker.ietf.org/doc/html/rfc6266\n    #\n    # For the 'filename' attribute of it, see RFC 8187:\n    # https://datatracker.ietf.org/doc/html/rfc8187\n    try:\n        # If the filename is pure-ASCII (determined by trying to\n        # encode it as such), then we escape slashes and quotes, and\n        # provide a filename=\"...\"\n        filename.encode(\"ascii\")\n        file_expr = 'filename=\"{}\"'.format(filename.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', r\"\\\"\"))\n    except UnicodeEncodeError:\n        # If it contains non-ASCII characters, we URI-escape it and\n        # provide a filename*=encoding'language'value\n        file_expr = f\"filename*=utf-8''{quote(filename)}\"\n\n    response.headers[\"Content-Disposition\"] = f\"{disposition}; {file_expr}\"\n\n\ndef internal_nginx_redirect(internal_path: str) -> HttpResponse:\n    # The following headers from this initial response are\n    # _preserved_, if present, and sent unmodified to the client;\n    # all other headers are overridden by the redirected URL:\n    #  - Content-Type\n    #  - Content-Disposition\n    #  - Accept-Ranges\n    #  - Set-Cookie\n    #  - Cache-Control\n    #  - Expires\n    # As such, we unset the Content-type header to allow nginx to set\n    # it from the static file; the caller can set Content-Disposition\n    # and Cache-Control on this response as they desire, and the\n    # client will see those values.\n    response = HttpResponse()\n    response[\"X-Accel-Redirect\"] = internal_path\n    del response[\"Content-Type\"]\n    return response\n\n\ndef serve_s3(request: HttpRequest, path_id: str, force_download: bool = False) -> HttpResponse:\n    url = get_signed_upload_url(path_id, force_download=force_download)\n    assert url.startswith(\"https://\")\n\n    if settings.DEVELOPMENT:\n        # In development, we do not have the nginx server to offload\n        # the response to; serve a redirect to the short-lived S3 URL.\n        # This means the content cannot be cached by the browser, but\n        # this is acceptable in development.\n        return redirect(url)\n\n    # We over-escape the path, to work around it being impossible to\n    # get the _unescaped_ new internal request URI in nginx.\n    parsed_url = urlparse(url)\n    assert parsed_url.hostname is not None\n    assert parsed_url.path is not None\n    assert parsed_url.query is not None\n    escaped_path_parts = parsed_url.hostname + quote(parsed_url.path) + \"?\" + parsed_url.query\n    response = internal_nginx_redirect(\"/internal/s3/\" + escaped_path_parts)\n\n    # It is important that S3 generate both the Content-Type and\n    # Content-Disposition headers; when the file was uploaded, we\n    # stored the browser-provided value for the former, and set\n    # Content-Disposition according to if that was safe.  As such,\n    # only S3 knows if a given attachment is safe to inline; we only\n    # override Content-Disposition to \"attachment\", and do so by\n    # telling S3 that is what we want in the signed URL.\n    patch_cache_control(response, private=True, immutable=True)\n    return response\n\n\ndef serve_local(\n    request: HttpRequest, path_id: str, force_download: bool = False\n) -> HttpResponseBase:\n    assert settings.LOCAL_FILES_DIR is not None\n    local_path = os.path.join(settings.LOCAL_FILES_DIR, path_id)\n    assert_is_local_storage_path(\"files\", local_path)\n    if not os.path.isfile(local_path):\n        return HttpResponseNotFound(\"<p>File not found</p>\")\n\n    mimetype, encoding = guess_type(path_id)\n    download = force_download or mimetype not in INLINE_MIME_TYPES\n\n    if settings.DEVELOPMENT:\n        # In development, we do not have the nginx server to offload\n        # the response to; serve it directly ourselves.\n        # FileResponse handles setting Content-Disposition, etc.\n        response: HttpResponseBase = FileResponse(\n            open(local_path, \"rb\"), as_attachment=download  # noqa: SIM115\n        )\n        patch_cache_control(response, private=True, immutable=True)\n        return response\n\n    response = internal_nginx_redirect(quote(f\"/internal/local/uploads/{path_id}\"))\n    patch_disposition_header(response, local_path, download)\n    patch_cache_control(response, private=True, immutable=True)\n    return response\n\n\ndef serve_file_download_backend(\n    request: HttpRequest, user_profile: UserProfile, realm_id_str: str, filename: str\n) -> HttpResponseBase:\n    return serve_file(\n        request, user_profile, realm_id_str, filename, url_only=False, force_download=True\n    )\n\n\ndef serve_file_backend(\n    request: HttpRequest,\n    maybe_user_profile: Union[UserProfile, AnonymousUser],\n    realm_id_str: str,\n    filename: str,\n) -> HttpResponseBase:\n    return serve_file(request, maybe_user_profile, realm_id_str, filename, url_only=False)\n\n\ndef serve_file_url_backend(\n    request: HttpRequest, user_profile: UserProfile, realm_id_str: str, filename: str\n) -> HttpResponseBase:\n    \"\"\"\n    We should return a signed, short-lived URL\n    that the client can use for native mobile download, rather than serving a redirect.\n    \"\"\"\n\n    return serve_file(request, user_profile, realm_id_str, filename, url_only=True)\n\n\ndef serve_file(\n    request: HttpRequest,\n    maybe_user_profile: Union[UserProfile, AnonymousUser],\n    realm_id_str: str,\n    filename: str,\n    url_only: bool = False,\n    force_download: bool = False,\n) -> HttpResponseBase:\n    path_id = f\"{realm_id_str}/{filename}\"\n    realm = get_valid_realm_from_request(request)\n    is_authorized = validate_attachment_request(maybe_user_profile, path_id, realm)\n\n    if is_authorized is None:\n        return HttpResponseNotFound(_(\"<p>File not found.</p>\"))\n    if not is_authorized:\n        return HttpResponseForbidden(_(\"<p>You are not authorized to view this file.</p>\"))\n    if url_only:\n        url = generate_unauthed_file_access_url(path_id)\n        return json_success(request, data=dict(url=url))\n\n    if settings.LOCAL_UPLOADS_DIR is not None:\n        return serve_local(request, path_id, force_download=force_download)\n    else:\n        return serve_s3(request, path_id, force_download=force_download)\n\n\nUSER_UPLOADS_ACCESS_TOKEN_SALT = \"user_uploads_\"\n\n\ndef generate_unauthed_file_access_url(path_id: str) -> str:\n    signed_data = TimestampSigner(salt=USER_UPLOADS_ACCESS_TOKEN_SALT).sign(path_id)\n    token = base64.b16encode(signed_data.encode()).decode()\n\n    filename = path_id.split(\"/\")[-1]\n    return reverse(\"file_unauthed_from_token\", args=[token, filename])\n\n\ndef get_file_path_id_from_token(token: str) -> Optional[str]:\n    signer = TimestampSigner(salt=USER_UPLOADS_ACCESS_TOKEN_SALT)\n    try:\n        signed_data = base64.b16decode(token).decode()\n        path_id = signer.unsign(signed_data, max_age=timedelta(seconds=60))\n    except (BadSignature, binascii.Error):\n        return None\n\n    return path_id\n\n\ndef serve_file_unauthed_from_token(\n    request: HttpRequest, token: str, filename: str\n) -> HttpResponseBase:\n    path_id = get_file_path_id_from_token(token)\n    if path_id is None:\n        raise JsonableError(_(\"Invalid token\"))\n    if path_id.split(\"/\")[-1] != filename:\n        raise JsonableError(_(\"Invalid filename\"))\n\n    if settings.LOCAL_UPLOADS_DIR is not None:\n        return serve_local(request, path_id)\n    else:\n        return serve_s3(request, path_id)\n\n\ndef serve_local_avatar_unauthed(request: HttpRequest, path: str) -> HttpResponseBase:\n    \"\"\"Serves avatar images off disk, via nginx (or directly in dev), with no auth.\n\n    This is done unauthed because these need to be accessed from HTML\n    emails, where the client does not have any auth.  We rely on the\n    URL being generated using the AVATAR_SALT secret.\n\n    \"\"\"\n    if settings.LOCAL_AVATARS_DIR is None:\n        # We do not expect clients to hit this URL when using the S3\n        # backend; however, there is no reason to not serve the\n        # redirect to S3 where the content lives.\n        return redirect(\n            get_public_upload_root_url() + path + \"?\" + request.GET.urlencode(), permanent=True\n        )\n\n    local_path = os.path.join(settings.LOCAL_AVATARS_DIR, path)\n    assert_is_local_storage_path(\"avatars\", local_path)\n    if not os.path.isfile(local_path):\n        return HttpResponseNotFound(\"<p>File not found</p>\")\n\n    if settings.DEVELOPMENT:\n        response: HttpResponseBase = FileResponse(open(local_path, \"rb\"))  # noqa: SIM115\n    else:\n        response = internal_nginx_redirect(quote(f\"/internal/local/user_avatars/{path}\"))\n\n    # We do _not_ mark the contents as immutable for caching purposes,\n    # since the path for avatar images is hashed only by their user-id\n    # and a salt, and as such are reused when a user's avatar is\n    # updated.\n    return response\n\n\ndef upload_file_backend(request: HttpRequest, user_profile: UserProfile) -> HttpResponse:\n    if len(request.FILES) == 0:\n        raise JsonableError(_(\"You must specify a file to upload\"))\n    if len(request.FILES) != 1:\n        raise JsonableError(_(\"You may only upload one file at a time\"))\n\n    user_file = list(request.FILES.values())[0]\n    assert isinstance(user_file, UploadedFile)\n    file_size = user_file.size\n    assert file_size is not None\n    if settings.MAX_FILE_UPLOAD_SIZE * 1024 * 1024 < file_size:\n        raise JsonableError(\n            _(\"Uploaded file is larger than the allowed limit of {} MiB\").format(\n                settings.MAX_FILE_UPLOAD_SIZE,\n            )\n        )\n    check_upload_within_quota(user_profile.realm, file_size)\n\n    uri = upload_message_image_from_request(user_file, user_profile, file_size)\n    return json_success(request, data={\"uri\": uri})\n"], "filenames": ["puppet/zulip/files/nginx/zulip-include-frontend/uploads-internal.conf", "puppet/zulip/templates/nginx/s3-cache.template.erb", "zerver/lib/upload/s3.py", "zerver/views/upload.py"], "buggy_code_start_loc": [17, 6, 94, 91], "buggy_code_end_loc": [44, 6, 109, 231], "fixing_code_start_loc": [16, 7, 94, 91], "fixing_code_end_loc": [46, 20, 112, 239], "type": "CWE-436", "message": "Zulip is an open-source team collaboration tool. In versions of zulip prior to commit `2f6c5a8` but after commit `04cf68b` users could upload files with arbitrary `Content-Type` which would be served from the Zulip hostname with `Content-Disposition: inline` and no `Content-Security-Policy` header, allowing them to trick other users into executing arbitrary Javascript in the context of the Zulip application. Among other things, this enables session theft. Only deployments which use the S3 storage (not the local-disk storage) are affected, and only deployments which deployed commit 04cf68b45ebb5c03247a0d6453e35ffc175d55da, which has only been in `main`, not any numbered release. Users affected should upgrade from main again to deploy this fix. Switching from S3 storage to the local-disk storage would nominally mitigate this, but is likely more involved than upgrading to the latest `main` which addresses the issue.", "other": {"cve": {"id": "CVE-2023-22735", "sourceIdentifier": "security-advisories@github.com", "published": "2023-02-07T19:15:09.303", "lastModified": "2023-02-16T15:51:30.133", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Zulip is an open-source team collaboration tool. In versions of zulip prior to commit `2f6c5a8` but after commit `04cf68b` users could upload files with arbitrary `Content-Type` which would be served from the Zulip hostname with `Content-Disposition: inline` and no `Content-Security-Policy` header, allowing them to trick other users into executing arbitrary Javascript in the context of the Zulip application. Among other things, this enables session theft. Only deployments which use the S3 storage (not the local-disk storage) are affected, and only deployments which deployed commit 04cf68b45ebb5c03247a0d6453e35ffc175d55da, which has only been in `main`, not any numbered release. Users affected should upgrade from main again to deploy this fix. Switching from S3 storage to the local-disk storage would nominally mitigate this, but is likely more involved than upgrading to the latest `main` which addresses the issue."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:R/S:U/C:L/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 4.6, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.1, "impactScore": 2.5}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:L/UI:R/S:C/C:L/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 4.4, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.3, "impactScore": 2.7}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-436"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:zulip:zulip_server:2023-01-09:*:*:*:*:*:*:*", "matchCriteriaId": "31D6A69A-4A00-46FE-B1E4-43C140BF1C24"}]}]}], "references": [{"url": "https://github.com/zulip/zulip/commit/04cf68b45ebb5c03247a0d6453e35ffc175d55da", "source": "security-advisories@github.com", "tags": ["Product"]}, {"url": "https://github.com/zulip/zulip/commit/2f6c5a883e106aa82a570d3d1f243993284b70f3", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/zulip/zulip/security/advisories/GHSA-wm83-3764-5wqh", "source": "security-advisories@github.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://zulip.readthedocs.io/en/latest/production/upgrade-or-modify.html#upgrading-from-a-git-repository", "source": "security-advisories@github.com", "tags": ["Not Applicable"]}]}, "github_commit_url": "https://github.com/zulip/zulip/commit/2f6c5a883e106aa82a570d3d1f243993284b70f3"}}