{"buggy_code": ["/*M///////////////////////////////////////////////////////////////////////////////////////\n//\n//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.\n//\n//  By downloading, copying, installing or using the software you agree to this license.\n//  If you do not agree to this license, do not download, install,\n//  copy or use the software.\n//\n//\n//                           License Agreement\n//                For Open Source Computer Vision Library\n//\n// Copyright (C) 2000-2008, Intel Corporation, all rights reserved.\n// Copyright (C) 2009, Willow Garage Inc., all rights reserved.\n// Third party copyrights are property of their respective owners.\n//\n// Redistribution and use in source and binary forms, with or without modification,\n// are permitted provided that the following conditions are met:\n//\n//   * Redistribution's of source code must retain the above copyright notice,\n//     this list of conditions and the following disclaimer.\n//\n//   * Redistribution's in binary form must reproduce the above copyright notice,\n//     this list of conditions and the following disclaimer in the documentation\n//     and/or other materials provided with the distribution.\n//\n//   * The name of the copyright holders may not be used to endorse or promote products\n//     derived from this software without specific prior written permission.\n//\n// This software is provided by the copyright holders and contributors \"as is\" and\n// any express or implied warranties, including, but not limited to, the implied\n// warranties of merchantability and fitness for a particular purpose are disclaimed.\n// In no event shall the Intel Corporation or contributors be liable for any direct,\n// indirect, incidental, special, exemplary, or consequential damages\n// (including, but not limited to, procurement of substitute goods or services;\n// loss of use, data, or profits; or business interruption) however caused\n// and on any theory of liability, whether in contract, strict liability,\n// or tort (including negligence or otherwise) arising in any way out of\n// the use of this software, even if advised of the possibility of such damage.\n//\n//M*/\n\n#include \"precomp.hpp\"\n#include \"opencv2/core/hal/intrin.hpp\"\n#include \"opencl_kernels_video.hpp\"\n\nusing namespace std;\n#define EPS 0.001F\n#define INF 1E+10F\n\nnamespace cv\n{\n\nclass DISOpticalFlowImpl CV_FINAL : public DISOpticalFlow\n{\n  public:\n    DISOpticalFlowImpl();\n\n    void calc(InputArray I0, InputArray I1, InputOutputArray flow) CV_OVERRIDE;\n    void collectGarbage() CV_OVERRIDE;\n\n  protected: //!< algorithm parameters\n    int finest_scale, coarsest_scale;\n    int patch_size;\n    int patch_stride;\n    int grad_descent_iter;\n    int variational_refinement_iter;\n    float variational_refinement_alpha;\n    float variational_refinement_gamma;\n    float variational_refinement_delta;\n    bool use_mean_normalization;\n    bool use_spatial_propagation;\n\n  protected: //!< some auxiliary variables\n    int border_size;\n    int w, h;   //!< flow buffer width and height on the current scale\n    int ws, hs; //!< sparse flow buffer width and height on the current scale\n\n  public:\n    int getFinestScale() const CV_OVERRIDE { return finest_scale; }\n    void setFinestScale(int val) CV_OVERRIDE { finest_scale = val; }\n    int getPatchSize() const CV_OVERRIDE { return patch_size; }\n    void setPatchSize(int val) CV_OVERRIDE { patch_size = val; }\n    int getPatchStride() const CV_OVERRIDE { return patch_stride; }\n    void setPatchStride(int val) CV_OVERRIDE { patch_stride = val; }\n    int getGradientDescentIterations() const CV_OVERRIDE { return grad_descent_iter; }\n    void setGradientDescentIterations(int val) CV_OVERRIDE { grad_descent_iter = val; }\n    int getVariationalRefinementIterations() const CV_OVERRIDE { return variational_refinement_iter; }\n    void setVariationalRefinementIterations(int val) CV_OVERRIDE { variational_refinement_iter = val; }\n    float getVariationalRefinementAlpha() const CV_OVERRIDE { return variational_refinement_alpha; }\n    void setVariationalRefinementAlpha(float val) CV_OVERRIDE { variational_refinement_alpha = val; }\n    float getVariationalRefinementDelta() const CV_OVERRIDE { return variational_refinement_delta; }\n    void setVariationalRefinementDelta(float val) CV_OVERRIDE { variational_refinement_delta = val; }\n    float getVariationalRefinementGamma() const CV_OVERRIDE { return variational_refinement_gamma; }\n    void setVariationalRefinementGamma(float val) CV_OVERRIDE { variational_refinement_gamma = val; }\n\n    bool getUseMeanNormalization() const CV_OVERRIDE { return use_mean_normalization; }\n    void setUseMeanNormalization(bool val) CV_OVERRIDE { use_mean_normalization = val; }\n    bool getUseSpatialPropagation() const CV_OVERRIDE { return use_spatial_propagation; }\n    void setUseSpatialPropagation(bool val) CV_OVERRIDE { use_spatial_propagation = val; }\n\n  protected:                      //!< internal buffers\n    vector<Mat_<uchar> > I0s;     //!< Gaussian pyramid for the current frame\n    vector<Mat_<uchar> > I1s;     //!< Gaussian pyramid for the next frame\n    vector<Mat_<uchar> > I1s_ext; //!< I1s with borders\n\n    vector<Mat_<short> > I0xs; //!< Gaussian pyramid for the x gradient of the current frame\n    vector<Mat_<short> > I0ys; //!< Gaussian pyramid for the y gradient of the current frame\n\n    vector<Mat_<float> > Ux; //!< x component of the flow vectors\n    vector<Mat_<float> > Uy; //!< y component of the flow vectors\n\n    vector<Mat_<float> > initial_Ux; //!< x component of the initial flow field, if one was passed as an input\n    vector<Mat_<float> > initial_Uy; //!< y component of the initial flow field, if one was passed as an input\n\n    Mat_<Vec2f> U; //!< a buffer for the merged flow\n\n    Mat_<float> Sx; //!< intermediate sparse flow representation (x component)\n    Mat_<float> Sy; //!< intermediate sparse flow representation (y component)\n\n    /* Structure tensor components: */\n    Mat_<float> I0xx_buf; //!< sum of squares of x gradient values\n    Mat_<float> I0yy_buf; //!< sum of squares of y gradient values\n    Mat_<float> I0xy_buf; //!< sum of x and y gradient products\n\n    /* Extra buffers that are useful if patch mean-normalization is used: */\n    Mat_<float> I0x_buf; //!< sum of x gradient values\n    Mat_<float> I0y_buf; //!< sum of y gradient values\n\n    /* Auxiliary buffers used in structure tensor computation: */\n    Mat_<float> I0xx_buf_aux;\n    Mat_<float> I0yy_buf_aux;\n    Mat_<float> I0xy_buf_aux;\n    Mat_<float> I0x_buf_aux;\n    Mat_<float> I0y_buf_aux;\n\n    vector<Ptr<VariationalRefinement> > variational_refinement_processors;\n\n  private: //!< private methods and parallel sections\n    void prepareBuffers(Mat &I0, Mat &I1, Mat &flow, bool use_flow);\n    void precomputeStructureTensor(Mat &dst_I0xx, Mat &dst_I0yy, Mat &dst_I0xy, Mat &dst_I0x, Mat &dst_I0y, Mat &I0x,\n                                   Mat &I0y);\n\n    struct PatchInverseSearch_ParBody : public ParallelLoopBody\n    {\n        DISOpticalFlowImpl *dis;\n        int nstripes, stripe_sz;\n        int hs;\n        Mat *Sx, *Sy, *Ux, *Uy, *I0, *I1, *I0x, *I0y;\n        int num_iter, pyr_level;\n\n        PatchInverseSearch_ParBody(DISOpticalFlowImpl &_dis, int _nstripes, int _hs, Mat &dst_Sx, Mat &dst_Sy,\n                                   Mat &src_Ux, Mat &src_Uy, Mat &_I0, Mat &_I1, Mat &_I0x, Mat &_I0y, int _num_iter,\n                                   int _pyr_level);\n        void operator()(const Range &range) const CV_OVERRIDE;\n    };\n\n    struct Densification_ParBody : public ParallelLoopBody\n    {\n        DISOpticalFlowImpl *dis;\n        int nstripes, stripe_sz;\n        int h;\n        Mat *Ux, *Uy, *Sx, *Sy, *I0, *I1;\n\n        Densification_ParBody(DISOpticalFlowImpl &_dis, int _nstripes, int _h, Mat &dst_Ux, Mat &dst_Uy, Mat &src_Sx,\n                              Mat &src_Sy, Mat &_I0, Mat &_I1);\n        void operator()(const Range &range) const CV_OVERRIDE;\n    };\n\n#ifdef HAVE_OPENCL\n    vector<UMat> u_I0s;     //!< Gaussian pyramid for the current frame\n    vector<UMat> u_I1s;     //!< Gaussian pyramid for the next frame\n    vector<UMat> u_I1s_ext; //!< I1s with borders\n\n    vector<UMat> u_I0xs; //!< Gaussian pyramid for the x gradient of the current frame\n    vector<UMat> u_I0ys; //!< Gaussian pyramid for the y gradient of the current frame\n\n    vector<UMat> u_Ux; //!< x component of the flow vectors\n    vector<UMat> u_Uy; //!< y component of the flow vectors\n\n    vector<UMat> u_initial_Ux; //!< x component of the initial flow field, if one was passed as an input\n    vector<UMat> u_initial_Uy; //!< y component of the initial flow field, if one was passed as an input\n\n    UMat u_U; //!< a buffer for the merged flow\n\n    UMat u_Sx; //!< intermediate sparse flow representation (x component)\n    UMat u_Sy; //!< intermediate sparse flow representation (y component)\n\n    /* Structure tensor components: */\n    UMat u_I0xx_buf; //!< sum of squares of x gradient values\n    UMat u_I0yy_buf; //!< sum of squares of y gradient values\n    UMat u_I0xy_buf; //!< sum of x and y gradient products\n\n    /* Extra buffers that are useful if patch mean-normalization is used: */\n    UMat u_I0x_buf; //!< sum of x gradient values\n    UMat u_I0y_buf; //!< sum of y gradient values\n\n    /* Auxiliary buffers used in structure tensor computation: */\n    UMat u_I0xx_buf_aux;\n    UMat u_I0yy_buf_aux;\n    UMat u_I0xy_buf_aux;\n    UMat u_I0x_buf_aux;\n    UMat u_I0y_buf_aux;\n\n    bool ocl_precomputeStructureTensor(UMat &dst_I0xx, UMat &dst_I0yy, UMat &dst_I0xy,\n                                       UMat &dst_I0x, UMat &dst_I0y, UMat &I0x, UMat &I0y);\n    void ocl_prepareBuffers(UMat &I0, UMat &I1, UMat &flow, bool use_flow);\n    bool ocl_calc(InputArray I0, InputArray I1, InputOutputArray flow);\n    bool ocl_Densification(UMat &dst_Ux, UMat &dst_Uy, UMat &src_Sx, UMat &src_Sy, UMat &_I0, UMat &_I1);\n    bool ocl_PatchInverseSearch(UMat &src_Ux, UMat &src_Uy,\n                                UMat &I0, UMat &I1, UMat &I0x, UMat &I0y, int num_iter, int pyr_level);\n#endif\n};\n\nDISOpticalFlowImpl::DISOpticalFlowImpl()\n{\n    finest_scale = 2;\n    patch_size = 8;\n    patch_stride = 4;\n    grad_descent_iter = 16;\n    variational_refinement_iter = 5;\n    variational_refinement_alpha = 20.f;\n    variational_refinement_gamma = 10.f;\n    variational_refinement_delta = 5.f;\n\n    border_size = 16;\n    use_mean_normalization = true;\n    use_spatial_propagation = true;\n    coarsest_scale = 10;\n\n    /* Use separate variational refinement instances for different scales to avoid repeated memory allocation: */\n    int max_possible_scales = 10;\n    ws = hs = w = h = 0;\n    for (int i = 0; i < max_possible_scales; i++)\n        variational_refinement_processors.push_back(VariationalRefinement::create());\n}\n\nvoid DISOpticalFlowImpl::prepareBuffers(Mat &I0, Mat &I1, Mat &flow, bool use_flow)\n{\n    I0s.resize(coarsest_scale + 1);\n    I1s.resize(coarsest_scale + 1);\n    I1s_ext.resize(coarsest_scale + 1);\n    I0xs.resize(coarsest_scale + 1);\n    I0ys.resize(coarsest_scale + 1);\n    Ux.resize(coarsest_scale + 1);\n    Uy.resize(coarsest_scale + 1);\n\n    Mat flow_uv[2];\n    if (use_flow)\n    {\n        split(flow, flow_uv);\n        initial_Ux.resize(coarsest_scale + 1);\n        initial_Uy.resize(coarsest_scale + 1);\n    }\n\n    int fraction = 1;\n    int cur_rows = 0, cur_cols = 0;\n\n    for (int i = 0; i <= coarsest_scale; i++)\n    {\n        /* Avoid initializing the pyramid levels above the finest scale, as they won't be used anyway */\n        if (i == finest_scale)\n        {\n            cur_rows = I0.rows / fraction;\n            cur_cols = I0.cols / fraction;\n            I0s[i].create(cur_rows, cur_cols);\n            resize(I0, I0s[i], I0s[i].size(), 0.0, 0.0, INTER_AREA);\n            I1s[i].create(cur_rows, cur_cols);\n            resize(I1, I1s[i], I1s[i].size(), 0.0, 0.0, INTER_AREA);\n\n            /* These buffers are reused in each scale so we initialize them once on the finest scale: */\n            Sx.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            Sy.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0xx_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0yy_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0xy_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0x_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0y_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n\n            I0xx_buf_aux.create(cur_rows, cur_cols / patch_stride);\n            I0yy_buf_aux.create(cur_rows, cur_cols / patch_stride);\n            I0xy_buf_aux.create(cur_rows, cur_cols / patch_stride);\n            I0x_buf_aux.create(cur_rows, cur_cols / patch_stride);\n            I0y_buf_aux.create(cur_rows, cur_cols / patch_stride);\n\n            U.create(cur_rows, cur_cols);\n        }\n        else if (i > finest_scale)\n        {\n            cur_rows = I0s[i - 1].rows / 2;\n            cur_cols = I0s[i - 1].cols / 2;\n            I0s[i].create(cur_rows, cur_cols);\n            resize(I0s[i - 1], I0s[i], I0s[i].size(), 0.0, 0.0, INTER_AREA);\n            I1s[i].create(cur_rows, cur_cols);\n            resize(I1s[i - 1], I1s[i], I1s[i].size(), 0.0, 0.0, INTER_AREA);\n        }\n\n        if (i >= finest_scale)\n        {\n            I1s_ext[i].create(cur_rows + 2 * border_size, cur_cols + 2 * border_size);\n            copyMakeBorder(I1s[i], I1s_ext[i], border_size, border_size, border_size, border_size, BORDER_REPLICATE);\n            I0xs[i].create(cur_rows, cur_cols);\n            I0ys[i].create(cur_rows, cur_cols);\n            spatialGradient(I0s[i], I0xs[i], I0ys[i]);\n            Ux[i].create(cur_rows, cur_cols);\n            Uy[i].create(cur_rows, cur_cols);\n            variational_refinement_processors[i]->setAlpha(variational_refinement_alpha);\n            variational_refinement_processors[i]->setDelta(variational_refinement_delta);\n            variational_refinement_processors[i]->setGamma(variational_refinement_gamma);\n            variational_refinement_processors[i]->setSorIterations(5);\n            variational_refinement_processors[i]->setFixedPointIterations(variational_refinement_iter);\n\n            if (use_flow)\n            {\n                resize(flow_uv[0], initial_Ux[i], Size(cur_cols, cur_rows));\n                initial_Ux[i] /= fraction;\n                resize(flow_uv[1], initial_Uy[i], Size(cur_cols, cur_rows));\n                initial_Uy[i] /= fraction;\n            }\n        }\n\n        fraction *= 2;\n    }\n}\n\n/* This function computes the structure tensor elements (local sums of I0x^2, I0x*I0y and I0y^2).\n * A simple box filter is not used instead because we need to compute these sums on a sparse grid\n * and store them densely in the output buffers.\n */\nvoid DISOpticalFlowImpl::precomputeStructureTensor(Mat &dst_I0xx, Mat &dst_I0yy, Mat &dst_I0xy, Mat &dst_I0x,\n                                                   Mat &dst_I0y, Mat &I0x, Mat &I0y)\n{\n    float *I0xx_ptr = dst_I0xx.ptr<float>();\n    float *I0yy_ptr = dst_I0yy.ptr<float>();\n    float *I0xy_ptr = dst_I0xy.ptr<float>();\n    float *I0x_ptr = dst_I0x.ptr<float>();\n    float *I0y_ptr = dst_I0y.ptr<float>();\n\n    float *I0xx_aux_ptr = I0xx_buf_aux.ptr<float>();\n    float *I0yy_aux_ptr = I0yy_buf_aux.ptr<float>();\n    float *I0xy_aux_ptr = I0xy_buf_aux.ptr<float>();\n    float *I0x_aux_ptr = I0x_buf_aux.ptr<float>();\n    float *I0y_aux_ptr = I0y_buf_aux.ptr<float>();\n\n    /* Separable box filter: horizontal pass */\n    for (int i = 0; i < h; i++)\n    {\n        float sum_xx = 0.0f, sum_yy = 0.0f, sum_xy = 0.0f, sum_x = 0.0f, sum_y = 0.0f;\n        short *x_row = I0x.ptr<short>(i);\n        short *y_row = I0y.ptr<short>(i);\n        for (int j = 0; j < patch_size; j++)\n        {\n            sum_xx += x_row[j] * x_row[j];\n            sum_yy += y_row[j] * y_row[j];\n            sum_xy += x_row[j] * y_row[j];\n            sum_x += x_row[j];\n            sum_y += y_row[j];\n        }\n        I0xx_aux_ptr[i * ws] = sum_xx;\n        I0yy_aux_ptr[i * ws] = sum_yy;\n        I0xy_aux_ptr[i * ws] = sum_xy;\n        I0x_aux_ptr[i * ws] = sum_x;\n        I0y_aux_ptr[i * ws] = sum_y;\n        int js = 1;\n        for (int j = patch_size; j < w; j++)\n        {\n            sum_xx += (x_row[j] * x_row[j] - x_row[j - patch_size] * x_row[j - patch_size]);\n            sum_yy += (y_row[j] * y_row[j] - y_row[j - patch_size] * y_row[j - patch_size]);\n            sum_xy += (x_row[j] * y_row[j] - x_row[j - patch_size] * y_row[j - patch_size]);\n            sum_x += (x_row[j] - x_row[j - patch_size]);\n            sum_y += (y_row[j] - y_row[j - patch_size]);\n            if ((j - patch_size + 1) % patch_stride == 0)\n            {\n                I0xx_aux_ptr[i * ws + js] = sum_xx;\n                I0yy_aux_ptr[i * ws + js] = sum_yy;\n                I0xy_aux_ptr[i * ws + js] = sum_xy;\n                I0x_aux_ptr[i * ws + js] = sum_x;\n                I0y_aux_ptr[i * ws + js] = sum_y;\n                js++;\n            }\n        }\n    }\n\n    AutoBuffer<float> sum_xx(ws), sum_yy(ws), sum_xy(ws), sum_x(ws), sum_y(ws);\n    for (int j = 0; j < ws; j++)\n    {\n        sum_xx[j] = 0.0f;\n        sum_yy[j] = 0.0f;\n        sum_xy[j] = 0.0f;\n        sum_x[j] = 0.0f;\n        sum_y[j] = 0.0f;\n    }\n\n    /* Separable box filter: vertical pass */\n    for (int i = 0; i < patch_size; i++)\n        for (int j = 0; j < ws; j++)\n        {\n            sum_xx[j] += I0xx_aux_ptr[i * ws + j];\n            sum_yy[j] += I0yy_aux_ptr[i * ws + j];\n            sum_xy[j] += I0xy_aux_ptr[i * ws + j];\n            sum_x[j] += I0x_aux_ptr[i * ws + j];\n            sum_y[j] += I0y_aux_ptr[i * ws + j];\n        }\n    for (int j = 0; j < ws; j++)\n    {\n        I0xx_ptr[j] = sum_xx[j];\n        I0yy_ptr[j] = sum_yy[j];\n        I0xy_ptr[j] = sum_xy[j];\n        I0x_ptr[j] = sum_x[j];\n        I0y_ptr[j] = sum_y[j];\n    }\n    int is = 1;\n    for (int i = patch_size; i < h; i++)\n    {\n        for (int j = 0; j < ws; j++)\n        {\n            sum_xx[j] += (I0xx_aux_ptr[i * ws + j] - I0xx_aux_ptr[(i - patch_size) * ws + j]);\n            sum_yy[j] += (I0yy_aux_ptr[i * ws + j] - I0yy_aux_ptr[(i - patch_size) * ws + j]);\n            sum_xy[j] += (I0xy_aux_ptr[i * ws + j] - I0xy_aux_ptr[(i - patch_size) * ws + j]);\n            sum_x[j] += (I0x_aux_ptr[i * ws + j] - I0x_aux_ptr[(i - patch_size) * ws + j]);\n            sum_y[j] += (I0y_aux_ptr[i * ws + j] - I0y_aux_ptr[(i - patch_size) * ws + j]);\n        }\n        if ((i - patch_size + 1) % patch_stride == 0)\n        {\n            for (int j = 0; j < ws; j++)\n            {\n                I0xx_ptr[is * ws + j] = sum_xx[j];\n                I0yy_ptr[is * ws + j] = sum_yy[j];\n                I0xy_ptr[is * ws + j] = sum_xy[j];\n                I0x_ptr[is * ws + j] = sum_x[j];\n                I0y_ptr[is * ws + j] = sum_y[j];\n            }\n            is++;\n        }\n    }\n}\n\nDISOpticalFlowImpl::PatchInverseSearch_ParBody::PatchInverseSearch_ParBody(DISOpticalFlowImpl &_dis, int _nstripes,\n                                                                           int _hs, Mat &dst_Sx, Mat &dst_Sy,\n                                                                           Mat &src_Ux, Mat &src_Uy, Mat &_I0, Mat &_I1,\n                                                                           Mat &_I0x, Mat &_I0y, int _num_iter,\n                                                                           int _pyr_level)\n    : dis(&_dis), nstripes(_nstripes), hs(_hs), Sx(&dst_Sx), Sy(&dst_Sy), Ux(&src_Ux), Uy(&src_Uy), I0(&_I0), I1(&_I1),\n      I0x(&_I0x), I0y(&_I0y), num_iter(_num_iter), pyr_level(_pyr_level)\n{\n    stripe_sz = (int)ceil(hs / (double)nstripes);\n}\n\n/////////////////////////////////////////////* Patch processing functions */////////////////////////////////////////////\n\n/* Some auxiliary macros */\n#define HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION                                                                         \\\n    v_float32x4 w00v = v_setall_f32(w00);                                                                              \\\n    v_float32x4 w01v = v_setall_f32(w01);                                                                              \\\n    v_float32x4 w10v = v_setall_f32(w10);                                                                              \\\n    v_float32x4 w11v = v_setall_f32(w11);                                                                              \\\n                                                                                                                       \\\n    v_uint8x16 I0_row_16, I1_row_16, I1_row_shifted_16, I1_row_next_16, I1_row_next_shifted_16;                        \\\n    v_uint16x8 I0_row_8, I1_row_8, I1_row_shifted_8, I1_row_next_8, I1_row_next_shifted_8, tmp;                        \\\n    v_uint32x4 I0_row_4_left, I1_row_4_left, I1_row_shifted_4_left, I1_row_next_4_left, I1_row_next_shifted_4_left;    \\\n    v_uint32x4 I0_row_4_right, I1_row_4_right, I1_row_shifted_4_right, I1_row_next_4_right,                            \\\n      I1_row_next_shifted_4_right;                                                                                     \\\n    v_float32x4 I_diff_left, I_diff_right;                                                                             \\\n                                                                                                                       \\\n    /* Preload and expand the first row of I1: */                                                                      \\\n    I1_row_16 = v_load(I1_ptr);                                                                                        \\\n    I1_row_shifted_16 = v_extract<1>(I1_row_16, I1_row_16);                                                            \\\n    v_expand(I1_row_16, I1_row_8, tmp);                                                                                \\\n    v_expand(I1_row_shifted_16, I1_row_shifted_8, tmp);                                                                \\\n    v_expand(I1_row_8, I1_row_4_left, I1_row_4_right);                                                                 \\\n    v_expand(I1_row_shifted_8, I1_row_shifted_4_left, I1_row_shifted_4_right);                                         \\\n    I1_ptr += I1_stride;\n\n#define HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION                                                                      \\\n    /* Load the next row of I1: */                                                                                     \\\n    I1_row_next_16 = v_load(I1_ptr);                                                                                   \\\n    /* Circular shift left by 1 element: */                                                                            \\\n    I1_row_next_shifted_16 = v_extract<1>(I1_row_next_16, I1_row_next_16);                                             \\\n    /* Expand to 8 ushorts (we only need the first 8 values): */                                                       \\\n    v_expand(I1_row_next_16, I1_row_next_8, tmp);                                                                      \\\n    v_expand(I1_row_next_shifted_16, I1_row_next_shifted_8, tmp);                                                      \\\n    /* Separate the left and right halves: */                                                                          \\\n    v_expand(I1_row_next_8, I1_row_next_4_left, I1_row_next_4_right);                                                  \\\n    v_expand(I1_row_next_shifted_8, I1_row_next_shifted_4_left, I1_row_next_shifted_4_right);                          \\\n                                                                                                                       \\\n    /* Load current row of I0: */                                                                                      \\\n    I0_row_16 = v_load(I0_ptr);                                                                                        \\\n    v_expand(I0_row_16, I0_row_8, tmp);                                                                                \\\n    v_expand(I0_row_8, I0_row_4_left, I0_row_4_right);                                                                 \\\n                                                                                                                       \\\n    /* Compute diffs between I0 and bilinearly interpolated I1: */                                                     \\\n    I_diff_left = w00v * v_cvt_f32(v_reinterpret_as_s32(I1_row_4_left)) +                                              \\\n                  w01v * v_cvt_f32(v_reinterpret_as_s32(I1_row_shifted_4_left)) +                                      \\\n                  w10v * v_cvt_f32(v_reinterpret_as_s32(I1_row_next_4_left)) +                                         \\\n                  w11v * v_cvt_f32(v_reinterpret_as_s32(I1_row_next_shifted_4_left)) -                                 \\\n                  v_cvt_f32(v_reinterpret_as_s32(I0_row_4_left));                                                      \\\n    I_diff_right = w00v * v_cvt_f32(v_reinterpret_as_s32(I1_row_4_right)) +                                            \\\n                   w01v * v_cvt_f32(v_reinterpret_as_s32(I1_row_shifted_4_right)) +                                    \\\n                   w10v * v_cvt_f32(v_reinterpret_as_s32(I1_row_next_4_right)) +                                       \\\n                   w11v * v_cvt_f32(v_reinterpret_as_s32(I1_row_next_shifted_4_right)) -                               \\\n                   v_cvt_f32(v_reinterpret_as_s32(I0_row_4_right));\n\n#define HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW                                                                     \\\n    I0_ptr += I0_stride;                                                                                               \\\n    I1_ptr += I1_stride;                                                                                               \\\n                                                                                                                       \\\n    I1_row_4_left = I1_row_next_4_left;                                                                                \\\n    I1_row_4_right = I1_row_next_4_right;                                                                              \\\n    I1_row_shifted_4_left = I1_row_next_shifted_4_left;                                                                \\\n    I1_row_shifted_4_right = I1_row_next_shifted_4_right;\n\n/* This function essentially performs one iteration of gradient descent when finding the most similar patch in I1 for a\n * given one in I0. It assumes that I0_ptr and I1_ptr already point to the corresponding patches and w00, w01, w10, w11\n * are precomputed bilinear interpolation weights. It returns the SSD (sum of squared differences) between these patches\n * and computes the values (dst_dUx, dst_dUy) that are used in the flow vector update. HAL acceleration is implemented\n * only for the default patch size (8x8). Everything is processed in floats as using fixed-point approximations harms\n * the quality significantly.\n */\ninline float processPatch(float &dst_dUx, float &dst_dUy, uchar *I0_ptr, uchar *I1_ptr, short *I0x_ptr, short *I0y_ptr,\n                          int I0_stride, int I1_stride, float w00, float w01, float w10, float w11, int patch_sz)\n{\n    float SSD = 0.0f;\n#if CV_SIMD128\n    if (patch_sz == 8)\n    {\n        /* Variables to accumulate the sums */\n        v_float32x4 Ux_vec = v_setall_f32(0);\n        v_float32x4 Uy_vec = v_setall_f32(0);\n        v_float32x4 SSD_vec = v_setall_f32(0);\n\n        v_int16x8 I0x_row, I0y_row;\n        v_int32x4 I0x_row_4_left, I0x_row_4_right, I0y_row_4_left, I0y_row_4_right;\n\n        HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION;\n        for (int row = 0; row < 8; row++)\n        {\n            HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION;\n            I0x_row = v_load(I0x_ptr);\n            v_expand(I0x_row, I0x_row_4_left, I0x_row_4_right);\n            I0y_row = v_load(I0y_ptr);\n            v_expand(I0y_row, I0y_row_4_left, I0y_row_4_right);\n\n            /* Update the sums: */\n            Ux_vec += I_diff_left * v_cvt_f32(I0x_row_4_left) + I_diff_right * v_cvt_f32(I0x_row_4_right);\n            Uy_vec += I_diff_left * v_cvt_f32(I0y_row_4_left) + I_diff_right * v_cvt_f32(I0y_row_4_right);\n            SSD_vec += I_diff_left * I_diff_left + I_diff_right * I_diff_right;\n\n            I0x_ptr += I0_stride;\n            I0y_ptr += I0_stride;\n            HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW;\n        }\n\n        /* Final reduce operations: */\n        dst_dUx = v_reduce_sum(Ux_vec);\n        dst_dUy = v_reduce_sum(Uy_vec);\n        SSD = v_reduce_sum(SSD_vec);\n    }\n    else\n    {\n#endif\n        dst_dUx = 0.0f;\n        dst_dUy = 0.0f;\n        float diff;\n        for (int i = 0; i < patch_sz; i++)\n            for (int j = 0; j < patch_sz; j++)\n            {\n                diff = w00 * I1_ptr[i * I1_stride + j] + w01 * I1_ptr[i * I1_stride + j + 1] +\n                       w10 * I1_ptr[(i + 1) * I1_stride + j] + w11 * I1_ptr[(i + 1) * I1_stride + j + 1] -\n                       I0_ptr[i * I0_stride + j];\n\n                SSD += diff * diff;\n                dst_dUx += diff * I0x_ptr[i * I0_stride + j];\n                dst_dUy += diff * I0y_ptr[i * I0_stride + j];\n            }\n#if CV_SIMD128\n    }\n#endif\n    return SSD;\n}\n\n/* Same as processPatch, but with patch mean normalization, which improves robustness under changing\n * lighting conditions\n */\ninline float processPatchMeanNorm(float &dst_dUx, float &dst_dUy, uchar *I0_ptr, uchar *I1_ptr, short *I0x_ptr,\n                                  short *I0y_ptr, int I0_stride, int I1_stride, float w00, float w01, float w10,\n                                  float w11, int patch_sz, float x_grad_sum, float y_grad_sum)\n{\n    float sum_diff = 0.0, sum_diff_sq = 0.0;\n    float sum_I0x_mul = 0.0, sum_I0y_mul = 0.0;\n    float n = (float)patch_sz * patch_sz;\n\n#if CV_SIMD128\n    if (patch_sz == 8)\n    {\n        /* Variables to accumulate the sums */\n        v_float32x4 sum_I0x_mul_vec = v_setall_f32(0);\n        v_float32x4 sum_I0y_mul_vec = v_setall_f32(0);\n        v_float32x4 sum_diff_vec = v_setall_f32(0);\n        v_float32x4 sum_diff_sq_vec = v_setall_f32(0);\n\n        v_int16x8 I0x_row, I0y_row;\n        v_int32x4 I0x_row_4_left, I0x_row_4_right, I0y_row_4_left, I0y_row_4_right;\n\n        HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION;\n        for (int row = 0; row < 8; row++)\n        {\n            HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION;\n            I0x_row = v_load(I0x_ptr);\n            v_expand(I0x_row, I0x_row_4_left, I0x_row_4_right);\n            I0y_row = v_load(I0y_ptr);\n            v_expand(I0y_row, I0y_row_4_left, I0y_row_4_right);\n\n            /* Update the sums: */\n            sum_I0x_mul_vec += I_diff_left * v_cvt_f32(I0x_row_4_left) + I_diff_right * v_cvt_f32(I0x_row_4_right);\n            sum_I0y_mul_vec += I_diff_left * v_cvt_f32(I0y_row_4_left) + I_diff_right * v_cvt_f32(I0y_row_4_right);\n            sum_diff_sq_vec += I_diff_left * I_diff_left + I_diff_right * I_diff_right;\n            sum_diff_vec += I_diff_left + I_diff_right;\n\n            I0x_ptr += I0_stride;\n            I0y_ptr += I0_stride;\n            HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW;\n        }\n\n        /* Final reduce operations: */\n        sum_I0x_mul = v_reduce_sum(sum_I0x_mul_vec);\n        sum_I0y_mul = v_reduce_sum(sum_I0y_mul_vec);\n        sum_diff = v_reduce_sum(sum_diff_vec);\n        sum_diff_sq = v_reduce_sum(sum_diff_sq_vec);\n    }\n    else\n    {\n#endif\n        float diff;\n        for (int i = 0; i < patch_sz; i++)\n            for (int j = 0; j < patch_sz; j++)\n            {\n                diff = w00 * I1_ptr[i * I1_stride + j] + w01 * I1_ptr[i * I1_stride + j + 1] +\n                       w10 * I1_ptr[(i + 1) * I1_stride + j] + w11 * I1_ptr[(i + 1) * I1_stride + j + 1] -\n                       I0_ptr[i * I0_stride + j];\n\n                sum_diff += diff;\n                sum_diff_sq += diff * diff;\n\n                sum_I0x_mul += diff * I0x_ptr[i * I0_stride + j];\n                sum_I0y_mul += diff * I0y_ptr[i * I0_stride + j];\n            }\n#if CV_SIMD128\n    }\n#endif\n    dst_dUx = sum_I0x_mul - sum_diff * x_grad_sum / n;\n    dst_dUy = sum_I0y_mul - sum_diff * y_grad_sum / n;\n    return sum_diff_sq - sum_diff * sum_diff / n;\n}\n\n/* Similar to processPatch, but compute only the sum of squared differences (SSD) between the patches */\ninline float computeSSD(uchar *I0_ptr, uchar *I1_ptr, int I0_stride, int I1_stride, float w00, float w01, float w10,\n                        float w11, int patch_sz)\n{\n    float SSD = 0.0f;\n#if CV_SIMD128\n    if (patch_sz == 8)\n    {\n        v_float32x4 SSD_vec = v_setall_f32(0);\n        HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION;\n        for (int row = 0; row < 8; row++)\n        {\n            HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION;\n            SSD_vec += I_diff_left * I_diff_left + I_diff_right * I_diff_right;\n            HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW;\n        }\n        SSD = v_reduce_sum(SSD_vec);\n    }\n    else\n    {\n#endif\n        float diff;\n        for (int i = 0; i < patch_sz; i++)\n            for (int j = 0; j < patch_sz; j++)\n            {\n                diff = w00 * I1_ptr[i * I1_stride + j] + w01 * I1_ptr[i * I1_stride + j + 1] +\n                       w10 * I1_ptr[(i + 1) * I1_stride + j] + w11 * I1_ptr[(i + 1) * I1_stride + j + 1] -\n                       I0_ptr[i * I0_stride + j];\n                SSD += diff * diff;\n            }\n#if CV_SIMD128\n    }\n#endif\n    return SSD;\n}\n\n/* Same as computeSSD, but with patch mean normalization */\ninline float computeSSDMeanNorm(uchar *I0_ptr, uchar *I1_ptr, int I0_stride, int I1_stride, float w00, float w01,\n                                float w10, float w11, int patch_sz)\n{\n    float sum_diff = 0.0f, sum_diff_sq = 0.0f;\n    float n = (float)patch_sz * patch_sz;\n#if CV_SIMD128\n    if (patch_sz == 8)\n    {\n        v_float32x4 sum_diff_vec = v_setall_f32(0);\n        v_float32x4 sum_diff_sq_vec = v_setall_f32(0);\n        HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION;\n        for (int row = 0; row < 8; row++)\n        {\n            HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION;\n            sum_diff_sq_vec += I_diff_left * I_diff_left + I_diff_right * I_diff_right;\n            sum_diff_vec += I_diff_left + I_diff_right;\n            HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW;\n        }\n        sum_diff = v_reduce_sum(sum_diff_vec);\n        sum_diff_sq = v_reduce_sum(sum_diff_sq_vec);\n    }\n    else\n    {\n#endif\n        float diff;\n        for (int i = 0; i < patch_sz; i++)\n            for (int j = 0; j < patch_sz; j++)\n            {\n                diff = w00 * I1_ptr[i * I1_stride + j] + w01 * I1_ptr[i * I1_stride + j + 1] +\n                       w10 * I1_ptr[(i + 1) * I1_stride + j] + w11 * I1_ptr[(i + 1) * I1_stride + j + 1] -\n                       I0_ptr[i * I0_stride + j];\n\n                sum_diff += diff;\n                sum_diff_sq += diff * diff;\n            }\n#if CV_SIMD128\n    }\n#endif\n    return sum_diff_sq - sum_diff * sum_diff / n;\n}\n\n#undef HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION\n#undef HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION\n#undef HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\nvoid DISOpticalFlowImpl::PatchInverseSearch_ParBody::operator()(const Range &range) const\n{\n    // force separate processing of stripes if we are using spatial propagation:\n    if (dis->use_spatial_propagation && range.end > range.start + 1)\n    {\n        for (int n = range.start; n < range.end; n++)\n            (*this)(Range(n, n + 1));\n        return;\n    }\n    int psz = dis->patch_size;\n    int psz2 = psz / 2;\n    int w_ext = dis->w + 2 * dis->border_size; //!< width of I1_ext\n    int bsz = dis->border_size;\n\n    /* Input dense flow */\n    float *Ux_ptr = Ux->ptr<float>();\n    float *Uy_ptr = Uy->ptr<float>();\n\n    /* Output sparse flow */\n    float *Sx_ptr = Sx->ptr<float>();\n    float *Sy_ptr = Sy->ptr<float>();\n\n    uchar *I0_ptr = I0->ptr<uchar>();\n    uchar *I1_ptr = I1->ptr<uchar>();\n    short *I0x_ptr = I0x->ptr<short>();\n    short *I0y_ptr = I0y->ptr<short>();\n\n    /* Precomputed structure tensor */\n    float *xx_ptr = dis->I0xx_buf.ptr<float>();\n    float *yy_ptr = dis->I0yy_buf.ptr<float>();\n    float *xy_ptr = dis->I0xy_buf.ptr<float>();\n    /* And extra buffers for mean-normalization: */\n    float *x_ptr = dis->I0x_buf.ptr<float>();\n    float *y_ptr = dis->I0y_buf.ptr<float>();\n\n    bool use_temporal_candidates = false;\n    float *initial_Ux_ptr = NULL, *initial_Uy_ptr = NULL;\n    if (!dis->initial_Ux.empty())\n    {\n        initial_Ux_ptr = dis->initial_Ux[pyr_level].ptr<float>();\n        initial_Uy_ptr = dis->initial_Uy[pyr_level].ptr<float>();\n        use_temporal_candidates = true;\n    }\n\n    int i, j, dir;\n    int start_is, end_is, start_js, end_js;\n    int start_i, start_j;\n    float i_lower_limit = bsz - psz + 1.0f;\n    float i_upper_limit = bsz + dis->h - 1.0f;\n    float j_lower_limit = bsz - psz + 1.0f;\n    float j_upper_limit = bsz + dis->w - 1.0f;\n    float dUx, dUy, i_I1, j_I1, w00, w01, w10, w11, dx, dy;\n\n#define INIT_BILINEAR_WEIGHTS(Ux, Uy)                                                                                  \\\n    i_I1 = min(max(i + Uy + bsz, i_lower_limit), i_upper_limit);                                                       \\\n    j_I1 = min(max(j + Ux + bsz, j_lower_limit), j_upper_limit);                                                       \\\n                                                                                                                       \\\n    w11 = (i_I1 - floor(i_I1)) * (j_I1 - floor(j_I1));                                                                 \\\n    w10 = (i_I1 - floor(i_I1)) * (floor(j_I1) + 1 - j_I1);                                                             \\\n    w01 = (floor(i_I1) + 1 - i_I1) * (j_I1 - floor(j_I1));                                                             \\\n    w00 = (floor(i_I1) + 1 - i_I1) * (floor(j_I1) + 1 - j_I1);\n\n#define COMPUTE_SSD(dst, Ux, Uy)                                                                                       \\\n    INIT_BILINEAR_WEIGHTS(Ux, Uy);                                                                                     \\\n    if (dis->use_mean_normalization)                                                                                   \\\n        dst = computeSSDMeanNorm(I0_ptr + i * dis->w + j, I1_ptr + (int)i_I1 * w_ext + (int)j_I1, dis->w, w_ext, w00,  \\\n                                 w01, w10, w11, psz);                                                                  \\\n    else                                                                                                               \\\n        dst = computeSSD(I0_ptr + i * dis->w + j, I1_ptr + (int)i_I1 * w_ext + (int)j_I1, dis->w, w_ext, w00, w01,     \\\n                         w10, w11, psz);\n\n    int num_inner_iter = (int)floor(dis->grad_descent_iter / (float)num_iter);\n    for (int iter = 0; iter < num_iter; iter++)\n    {\n        if (iter % 2 == 0)\n        {\n            dir = 1;\n            start_is = min(range.start * stripe_sz, hs);\n            end_is = min(range.end * stripe_sz, hs);\n            start_js = 0;\n            end_js = dis->ws;\n            start_i = start_is * dis->patch_stride;\n            start_j = 0;\n        }\n        else\n        {\n            dir = -1;\n            start_is = min(range.end * stripe_sz, hs) - 1;\n            end_is = min(range.start * stripe_sz, hs) - 1;\n            start_js = dis->ws - 1;\n            end_js = -1;\n            start_i = start_is * dis->patch_stride;\n            start_j = (dis->ws - 1) * dis->patch_stride;\n        }\n\n        i = start_i;\n        for (int is = start_is; dir * is < dir * end_is; is += dir)\n        {\n            j = start_j;\n            for (int js = start_js; dir * js < dir * end_js; js += dir)\n            {\n                if (iter == 0)\n                {\n                    /* Using result form the previous pyramid level as the very first approximation: */\n                    Sx_ptr[is * dis->ws + js] = Ux_ptr[(i + psz2) * dis->w + j + psz2];\n                    Sy_ptr[is * dis->ws + js] = Uy_ptr[(i + psz2) * dis->w + j + psz2];\n                }\n\n                float min_SSD = INF, cur_SSD;\n                if (use_temporal_candidates || dis->use_spatial_propagation)\n                {\n                    COMPUTE_SSD(min_SSD, Sx_ptr[is * dis->ws + js], Sy_ptr[is * dis->ws + js]);\n                }\n\n                if (use_temporal_candidates)\n                {\n                    /* Try temporal candidates (vectors from the initial flow field that was passed to the function) */\n                    COMPUTE_SSD(cur_SSD, initial_Ux_ptr[(i + psz2) * dis->w + j + psz2],\n                                initial_Uy_ptr[(i + psz2) * dis->w + j + psz2]);\n                    if (cur_SSD < min_SSD)\n                    {\n                        min_SSD = cur_SSD;\n                        Sx_ptr[is * dis->ws + js] = initial_Ux_ptr[(i + psz2) * dis->w + j + psz2];\n                        Sy_ptr[is * dis->ws + js] = initial_Uy_ptr[(i + psz2) * dis->w + j + psz2];\n                    }\n                }\n\n                if (dis->use_spatial_propagation)\n                {\n                    /* Try spatial candidates: */\n                    if (dir * js > dir * start_js)\n                    {\n                        COMPUTE_SSD(cur_SSD, Sx_ptr[is * dis->ws + js - dir], Sy_ptr[is * dis->ws + js - dir]);\n                        if (cur_SSD < min_SSD)\n                        {\n                            min_SSD = cur_SSD;\n                            Sx_ptr[is * dis->ws + js] = Sx_ptr[is * dis->ws + js - dir];\n                            Sy_ptr[is * dis->ws + js] = Sy_ptr[is * dis->ws + js - dir];\n                        }\n                    }\n                    /* Flow vectors won't actually propagate across different stripes, which is the reason for keeping\n                     * the number of stripes constant. It works well enough in practice and doesn't introduce any\n                     * visible seams.\n                     */\n                    if (dir * is > dir * start_is)\n                    {\n                        COMPUTE_SSD(cur_SSD, Sx_ptr[(is - dir) * dis->ws + js], Sy_ptr[(is - dir) * dis->ws + js]);\n                        if (cur_SSD < min_SSD)\n                        {\n                            min_SSD = cur_SSD;\n                            Sx_ptr[is * dis->ws + js] = Sx_ptr[(is - dir) * dis->ws + js];\n                            Sy_ptr[is * dis->ws + js] = Sy_ptr[(is - dir) * dis->ws + js];\n                        }\n                    }\n                }\n\n                /* Use the best candidate as a starting point for the gradient descent: */\n                float cur_Ux = Sx_ptr[is * dis->ws + js];\n                float cur_Uy = Sy_ptr[is * dis->ws + js];\n\n                /* Computing the inverse of the structure tensor: */\n                float detH = xx_ptr[is * dis->ws + js] * yy_ptr[is * dis->ws + js] -\n                             xy_ptr[is * dis->ws + js] * xy_ptr[is * dis->ws + js];\n                if (abs(detH) < EPS)\n                    detH = EPS;\n                float invH11 = yy_ptr[is * dis->ws + js] / detH;\n                float invH12 = -xy_ptr[is * dis->ws + js] / detH;\n                float invH22 = xx_ptr[is * dis->ws + js] / detH;\n                float prev_SSD = INF, SSD;\n                float x_grad_sum = x_ptr[is * dis->ws + js];\n                float y_grad_sum = y_ptr[is * dis->ws + js];\n\n                for (int t = 0; t < num_inner_iter; t++)\n                {\n                    INIT_BILINEAR_WEIGHTS(cur_Ux, cur_Uy);\n                    if (dis->use_mean_normalization)\n                        SSD = processPatchMeanNorm(dUx, dUy, I0_ptr + i * dis->w + j,\n                                                   I1_ptr + (int)i_I1 * w_ext + (int)j_I1, I0x_ptr + i * dis->w + j,\n                                                   I0y_ptr + i * dis->w + j, dis->w, w_ext, w00, w01, w10, w11, psz,\n                                                   x_grad_sum, y_grad_sum);\n                    else\n                        SSD = processPatch(dUx, dUy, I0_ptr + i * dis->w + j, I1_ptr + (int)i_I1 * w_ext + (int)j_I1,\n                                           I0x_ptr + i * dis->w + j, I0y_ptr + i * dis->w + j, dis->w, w_ext, w00, w01,\n                                           w10, w11, psz);\n\n                    dx = invH11 * dUx + invH12 * dUy;\n                    dy = invH12 * dUx + invH22 * dUy;\n                    cur_Ux -= dx;\n                    cur_Uy -= dy;\n\n                    /* Break when patch distance stops decreasing */\n                    if (SSD >= prev_SSD)\n                        break;\n                    prev_SSD = SSD;\n                }\n\n                /* If gradient descent converged to a flow vector that is very far from the initial approximation\n                 * (more than patch size) then we don't use it. Noticeably improves the robustness.\n                 */\n                if (norm(Vec2f(cur_Ux - Sx_ptr[is * dis->ws + js], cur_Uy - Sy_ptr[is * dis->ws + js])) <= psz)\n                {\n                    Sx_ptr[is * dis->ws + js] = cur_Ux;\n                    Sy_ptr[is * dis->ws + js] = cur_Uy;\n                }\n                j += dir * dis->patch_stride;\n            }\n            i += dir * dis->patch_stride;\n        }\n    }\n#undef INIT_BILINEAR_WEIGHTS\n#undef COMPUTE_SSD\n}\n\nDISOpticalFlowImpl::Densification_ParBody::Densification_ParBody(DISOpticalFlowImpl &_dis, int _nstripes, int _h,\n                                                                 Mat &dst_Ux, Mat &dst_Uy, Mat &src_Sx, Mat &src_Sy,\n                                                                 Mat &_I0, Mat &_I1)\n    : dis(&_dis), nstripes(_nstripes), h(_h), Ux(&dst_Ux), Uy(&dst_Uy), Sx(&src_Sx), Sy(&src_Sy), I0(&_I0), I1(&_I1)\n{\n    stripe_sz = (int)ceil(h / (double)nstripes);\n}\n\n/* This function transforms a sparse optical flow field obtained by PatchInverseSearch (which computes flow values\n * on a sparse grid defined by patch_stride) into a dense optical flow field by weighted averaging of values from the\n * overlapping patches.\n */\nvoid DISOpticalFlowImpl::Densification_ParBody::operator()(const Range &range) const\n{\n    int start_i = min(range.start * stripe_sz, h);\n    int end_i = min(range.end * stripe_sz, h);\n\n    /* Input sparse flow */\n    float *Sx_ptr = Sx->ptr<float>();\n    float *Sy_ptr = Sy->ptr<float>();\n\n    /* Output dense flow */\n    float *Ux_ptr = Ux->ptr<float>();\n    float *Uy_ptr = Uy->ptr<float>();\n\n    uchar *I0_ptr = I0->ptr<uchar>();\n    uchar *I1_ptr = I1->ptr<uchar>();\n\n    int psz = dis->patch_size;\n    int pstr = dis->patch_stride;\n    int i_l, i_u;\n    int j_l, j_u;\n    float i_m, j_m, diff;\n\n    /* These values define the set of sparse grid locations that contain patches overlapping with the current dense flow\n     * location */\n    int start_is, end_is;\n    int start_js, end_js;\n\n/* Some helper macros for updating this set of sparse grid locations */\n#define UPDATE_SPARSE_I_COORDINATES                                                                                    \\\n    if (i % pstr == 0 && i + psz <= h)                                                                                 \\\n        end_is++;                                                                                                      \\\n    if (i - psz >= 0 && (i - psz) % pstr == 0 && start_is < end_is)                                                    \\\n        start_is++;\n\n#define UPDATE_SPARSE_J_COORDINATES                                                                                    \\\n    if (j % pstr == 0 && j + psz <= dis->w)                                                                            \\\n        end_js++;                                                                                                      \\\n    if (j - psz >= 0 && (j - psz) % pstr == 0 && start_js < end_js)                                                    \\\n        start_js++;\n\n    start_is = 0;\n    end_is = -1;\n    for (int i = 0; i < start_i; i++)\n    {\n        UPDATE_SPARSE_I_COORDINATES;\n    }\n    for (int i = start_i; i < end_i; i++)\n    {\n        UPDATE_SPARSE_I_COORDINATES;\n        start_js = 0;\n        end_js = -1;\n        for (int j = 0; j < dis->w; j++)\n        {\n            UPDATE_SPARSE_J_COORDINATES;\n            float coef, sum_coef = 0.0f;\n            float sum_Ux = 0.0f;\n            float sum_Uy = 0.0f;\n\n            /* Iterate through all the patches that overlap the current location (i,j) */\n            for (int is = start_is; is <= end_is; is++)\n                for (int js = start_js; js <= end_js; js++)\n                {\n                    j_m = min(max(j + Sx_ptr[is * dis->ws + js], 0.0f), dis->w - 1.0f - EPS);\n                    i_m = min(max(i + Sy_ptr[is * dis->ws + js], 0.0f), dis->h - 1.0f - EPS);\n                    j_l = (int)j_m;\n                    j_u = j_l + 1;\n                    i_l = (int)i_m;\n                    i_u = i_l + 1;\n                    diff = (j_m - j_l) * (i_m - i_l) * I1_ptr[i_u * dis->w + j_u] +\n                           (j_u - j_m) * (i_m - i_l) * I1_ptr[i_u * dis->w + j_l] +\n                           (j_m - j_l) * (i_u - i_m) * I1_ptr[i_l * dis->w + j_u] +\n                           (j_u - j_m) * (i_u - i_m) * I1_ptr[i_l * dis->w + j_l] - I0_ptr[i * dis->w + j];\n                    coef = 1 / max(1.0f, abs(diff));\n                    sum_Ux += coef * Sx_ptr[is * dis->ws + js];\n                    sum_Uy += coef * Sy_ptr[is * dis->ws + js];\n                    sum_coef += coef;\n                }\n            CV_DbgAssert(sum_coef != 0);\n            Ux_ptr[i * dis->w + j] = sum_Ux / sum_coef;\n            Uy_ptr[i * dis->w + j] = sum_Uy / sum_coef;\n        }\n    }\n#undef UPDATE_SPARSE_I_COORDINATES\n#undef UPDATE_SPARSE_J_COORDINATES\n}\n\n#ifdef HAVE_OPENCL\nbool DISOpticalFlowImpl::ocl_PatchInverseSearch(UMat &src_Ux, UMat &src_Uy,\n                                                UMat &I0, UMat &I1, UMat &I0x, UMat &I0y, int num_iter, int pyr_level)\n{\n    size_t globalSize[] = {(size_t)ws, (size_t)hs};\n    size_t localSize[]  = {16, 16};\n    int idx;\n    int num_inner_iter = (int)floor(grad_descent_iter / (float)num_iter);\n\n    String subgroups_build_options;\n    if (ocl::Device::getDefault().isExtensionSupported(\"cl_khr_subgroups\"))\n        subgroups_build_options = \"-DCV_USE_SUBGROUPS=1\";\n\n\n    for (int iter = 0; iter < num_iter; iter++)\n    {\n        if (iter == 0)\n        {\n            ocl::Kernel k1(\"dis_patch_inverse_search_fwd_1\", ocl::video::dis_flow_oclsrc, subgroups_build_options);\n            size_t global_sz[] = {(size_t)hs * 8};\n            size_t local_sz[]  = {8};\n            idx = 0;\n\n            idx = k1.set(idx, ocl::KernelArg::PtrReadOnly(src_Ux));\n            idx = k1.set(idx, ocl::KernelArg::PtrReadOnly(src_Uy));\n            idx = k1.set(idx, ocl::KernelArg::PtrReadOnly(I0));\n            idx = k1.set(idx, ocl::KernelArg::PtrReadOnly(I1));\n            idx = k1.set(idx, (int)border_size);\n            idx = k1.set(idx, (int)patch_size);\n            idx = k1.set(idx, (int)patch_stride);\n            idx = k1.set(idx, (int)w);\n            idx = k1.set(idx, (int)h);\n            idx = k1.set(idx, (int)ws);\n            idx = k1.set(idx, (int)hs);\n            idx = k1.set(idx, (int)pyr_level);\n            idx = k1.set(idx, ocl::KernelArg::PtrWriteOnly(u_Sx));\n            idx = k1.set(idx, ocl::KernelArg::PtrWriteOnly(u_Sy));\n            if (!k1.run(1, global_sz, local_sz, false))\n                return false;\n\n            ocl::Kernel k2(\"dis_patch_inverse_search_fwd_2\", ocl::video::dis_flow_oclsrc);\n            idx = 0;\n\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(src_Ux));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(src_Uy));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(I0));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(I1));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(I0x));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(I0y));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0xx_buf));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0yy_buf));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0xy_buf));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0x_buf));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0y_buf));\n            idx = k2.set(idx, (int)border_size);\n            idx = k2.set(idx, (int)patch_size);\n            idx = k2.set(idx, (int)patch_stride);\n            idx = k2.set(idx, (int)w);\n            idx = k2.set(idx, (int)h);\n            idx = k2.set(idx, (int)ws);\n            idx = k2.set(idx, (int)hs);\n            idx = k2.set(idx, (int)num_inner_iter);\n            idx = k2.set(idx, (int)pyr_level);\n            idx = k2.set(idx, ocl::KernelArg::PtrReadWrite(u_Sx));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadWrite(u_Sy));\n            if (!k2.run(2, globalSize, localSize, false))\n                return false;\n        }\n        else\n        {\n            ocl::Kernel k3(\"dis_patch_inverse_search_bwd_1\", ocl::video::dis_flow_oclsrc, subgroups_build_options);\n            size_t global_sz[] = {(size_t)hs * 8};\n            size_t local_sz[]  = {8};\n            idx = 0;\n\n            idx = k3.set(idx, ocl::KernelArg::PtrReadOnly(I0));\n            idx = k3.set(idx, ocl::KernelArg::PtrReadOnly(I1));\n            idx = k3.set(idx, (int)border_size);\n            idx = k3.set(idx, (int)patch_size);\n            idx = k3.set(idx, (int)patch_stride);\n            idx = k3.set(idx, (int)w);\n            idx = k3.set(idx, (int)h);\n            idx = k3.set(idx, (int)ws);\n            idx = k3.set(idx, (int)hs);\n            idx = k3.set(idx, (int)pyr_level);\n            idx = k3.set(idx, ocl::KernelArg::PtrReadWrite(u_Sx));\n            idx = k3.set(idx, ocl::KernelArg::PtrReadWrite(u_Sy));\n            if (!k3.run(1, global_sz, local_sz, false))\n                return false;\n\n            ocl::Kernel k4(\"dis_patch_inverse_search_bwd_2\", ocl::video::dis_flow_oclsrc);\n            idx = 0;\n\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(I0));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(I1));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(I0x));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(I0y));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0xx_buf));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0yy_buf));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0xy_buf));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0x_buf));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0y_buf));\n            idx = k4.set(idx, (int)border_size);\n            idx = k4.set(idx, (int)patch_size);\n            idx = k4.set(idx, (int)patch_stride);\n            idx = k4.set(idx, (int)w);\n            idx = k4.set(idx, (int)h);\n            idx = k4.set(idx, (int)ws);\n            idx = k4.set(idx, (int)hs);\n            idx = k4.set(idx, (int)num_inner_iter);\n            idx = k4.set(idx, ocl::KernelArg::PtrReadWrite(u_Sx));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadWrite(u_Sy));\n            if (!k4.run(2, globalSize, localSize, false))\n                return false;\n        }\n    }\n    return true;\n}\n\nbool DISOpticalFlowImpl::ocl_Densification(UMat &dst_Ux, UMat &dst_Uy, UMat &src_Sx, UMat &src_Sy, UMat &_I0, UMat &_I1)\n{\n    size_t globalSize[] = {(size_t)w, (size_t)h};\n    size_t localSize[]  = {16, 16};\n\n    ocl::Kernel kernel(\"dis_densification\", ocl::video::dis_flow_oclsrc);\n    kernel.args(ocl::KernelArg::PtrReadOnly(src_Sx),\n                ocl::KernelArg::PtrReadOnly(src_Sy),\n                ocl::KernelArg::PtrReadOnly(_I0),\n                ocl::KernelArg::PtrReadOnly(_I1),\n                (int)patch_size, (int)patch_stride,\n                (int)w, (int)h, (int)ws,\n                ocl::KernelArg::PtrWriteOnly(dst_Ux),\n                ocl::KernelArg::PtrWriteOnly(dst_Uy));\n    return kernel.run(2, globalSize, localSize, false);\n}\n\nvoid DISOpticalFlowImpl::ocl_prepareBuffers(UMat &I0, UMat &I1, UMat &flow, bool use_flow)\n{\n    u_I0s.resize(coarsest_scale + 1);\n    u_I1s.resize(coarsest_scale + 1);\n    u_I1s_ext.resize(coarsest_scale + 1);\n    u_I0xs.resize(coarsest_scale + 1);\n    u_I0ys.resize(coarsest_scale + 1);\n    u_Ux.resize(coarsest_scale + 1);\n    u_Uy.resize(coarsest_scale + 1);\n\n    vector<UMat> flow_uv(2);\n    if (use_flow)\n    {\n        split(flow, flow_uv);\n        u_initial_Ux.resize(coarsest_scale + 1);\n        u_initial_Uy.resize(coarsest_scale + 1);\n    }\n\n    int fraction = 1;\n    int cur_rows = 0, cur_cols = 0;\n\n    for (int i = 0; i <= coarsest_scale; i++)\n    {\n        /* Avoid initializing the pyramid levels above the finest scale, as they won't be used anyway */\n        if (i == finest_scale)\n        {\n            cur_rows = I0.rows / fraction;\n            cur_cols = I0.cols / fraction;\n            u_I0s[i].create(cur_rows, cur_cols, CV_8UC1);\n            resize(I0, u_I0s[i], u_I0s[i].size(), 0.0, 0.0, INTER_AREA);\n            u_I1s[i].create(cur_rows, cur_cols, CV_8UC1);\n            resize(I1, u_I1s[i], u_I1s[i].size(), 0.0, 0.0, INTER_AREA);\n\n            /* These buffers are reused in each scale so we initialize them once on the finest scale: */\n            u_Sx.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_Sy.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0xx_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0yy_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0xy_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0x_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0y_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n\n            u_I0xx_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n            u_I0yy_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n            u_I0xy_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n            u_I0x_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n            u_I0y_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n\n            u_U.create(cur_rows, cur_cols, CV_32FC2);\n        }\n        else if (i > finest_scale)\n        {\n            cur_rows = u_I0s[i - 1].rows / 2;\n            cur_cols = u_I0s[i - 1].cols / 2;\n            u_I0s[i].create(cur_rows, cur_cols, CV_8UC1);\n            resize(u_I0s[i - 1], u_I0s[i], u_I0s[i].size(), 0.0, 0.0, INTER_AREA);\n            u_I1s[i].create(cur_rows, cur_cols, CV_8UC1);\n            resize(u_I1s[i - 1], u_I1s[i], u_I1s[i].size(), 0.0, 0.0, INTER_AREA);\n        }\n\n        if (i >= finest_scale)\n        {\n            u_I1s_ext[i].create(cur_rows + 2 * border_size, cur_cols + 2 * border_size, CV_8UC1);\n            copyMakeBorder(u_I1s[i], u_I1s_ext[i], border_size, border_size, border_size, border_size, BORDER_REPLICATE);\n            u_I0xs[i].create(cur_rows, cur_cols, CV_16SC1);\n            u_I0ys[i].create(cur_rows, cur_cols, CV_16SC1);\n            spatialGradient(u_I0s[i], u_I0xs[i], u_I0ys[i]);\n            u_Ux[i].create(cur_rows, cur_cols, CV_32FC1);\n            u_Uy[i].create(cur_rows, cur_cols, CV_32FC1);\n            variational_refinement_processors[i]->setAlpha(variational_refinement_alpha);\n            variational_refinement_processors[i]->setDelta(variational_refinement_delta);\n            variational_refinement_processors[i]->setGamma(variational_refinement_gamma);\n            variational_refinement_processors[i]->setSorIterations(5);\n            variational_refinement_processors[i]->setFixedPointIterations(variational_refinement_iter);\n\n            if (use_flow)\n            {\n                resize(flow_uv[0], u_initial_Ux[i], Size(cur_cols, cur_rows));\n                divide(u_initial_Ux[i], static_cast<float>(fraction), u_initial_Ux[i]);\n                resize(flow_uv[1], u_initial_Uy[i], Size(cur_cols, cur_rows));\n                divide(u_initial_Uy[i], static_cast<float>(fraction), u_initial_Uy[i]);\n            }\n        }\n\n        fraction *= 2;\n    }\n}\n\nbool DISOpticalFlowImpl::ocl_precomputeStructureTensor(UMat &dst_I0xx, UMat &dst_I0yy, UMat &dst_I0xy,\n                                                       UMat &dst_I0x, UMat &dst_I0y, UMat &I0x, UMat &I0y)\n{\n    size_t globalSizeX[] = {(size_t)h};\n    size_t localSizeX[]  = {16};\n\n    ocl::Kernel kernelX(\"dis_precomputeStructureTensor_hor\", ocl::video::dis_flow_oclsrc);\n    kernelX.args(ocl::KernelArg::PtrReadOnly(I0x),\n                 ocl::KernelArg::PtrReadOnly(I0y),\n                 (int)patch_size, (int)patch_stride,\n                 (int)w, (int)h, (int)ws,\n                 ocl::KernelArg::PtrWriteOnly(u_I0xx_buf_aux),\n                 ocl::KernelArg::PtrWriteOnly(u_I0yy_buf_aux),\n                 ocl::KernelArg::PtrWriteOnly(u_I0xy_buf_aux),\n                 ocl::KernelArg::PtrWriteOnly(u_I0x_buf_aux),\n                 ocl::KernelArg::PtrWriteOnly(u_I0y_buf_aux));\n    if (!kernelX.run(1, globalSizeX, localSizeX, false))\n        return false;\n\n    size_t globalSizeY[] = {(size_t)ws};\n    size_t localSizeY[]  = {16};\n\n    ocl::Kernel kernelY(\"dis_precomputeStructureTensor_ver\", ocl::video::dis_flow_oclsrc);\n    kernelY.args(ocl::KernelArg::PtrReadOnly(u_I0xx_buf_aux),\n                 ocl::KernelArg::PtrReadOnly(u_I0yy_buf_aux),\n                 ocl::KernelArg::PtrReadOnly(u_I0xy_buf_aux),\n                 ocl::KernelArg::PtrReadOnly(u_I0x_buf_aux),\n                 ocl::KernelArg::PtrReadOnly(u_I0y_buf_aux),\n                 (int)patch_size, (int)patch_stride,\n                 (int)w, (int)h, (int)ws,\n                 ocl::KernelArg::PtrWriteOnly(dst_I0xx),\n                 ocl::KernelArg::PtrWriteOnly(dst_I0yy),\n                 ocl::KernelArg::PtrWriteOnly(dst_I0xy),\n                 ocl::KernelArg::PtrWriteOnly(dst_I0x),\n                 ocl::KernelArg::PtrWriteOnly(dst_I0y));\n    return kernelY.run(1, globalSizeY, localSizeY, false);\n}\n\nbool DISOpticalFlowImpl::ocl_calc(InputArray I0, InputArray I1, InputOutputArray flow)\n{\n    UMat I0Mat = I0.getUMat();\n    UMat I1Mat = I1.getUMat();\n    bool use_input_flow = false;\n    if (flow.sameSize(I0) && flow.depth() == CV_32F && flow.channels() == 2)\n        use_input_flow = true;\n    else\n        flow.create(I1Mat.size(), CV_32FC2);\n    UMat &u_flowMat = flow.getUMatRef();\n    coarsest_scale = min((int)(log(max(I0Mat.cols, I0Mat.rows) / (4.0 * patch_size)) / log(2.0) + 0.5), /* Original code serach for maximal movement of width/4 */\n                         (int)(log(min(I0Mat.cols, I0Mat.rows) / patch_size) / log(2.0)));              /* Deepest pyramid level greater or equal than patch*/\n\n    ocl_prepareBuffers(I0Mat, I1Mat, u_flowMat, use_input_flow);\n    u_Ux[coarsest_scale].setTo(0.0f);\n    u_Uy[coarsest_scale].setTo(0.0f);\n\n    for (int i = coarsest_scale; i >= finest_scale; i--)\n    {\n        w = u_I0s[i].cols;\n        h = u_I0s[i].rows;\n        ws = 1 + (w - patch_size) / patch_stride;\n        hs = 1 + (h - patch_size) / patch_stride;\n\n        if (!ocl_precomputeStructureTensor(u_I0xx_buf, u_I0yy_buf, u_I0xy_buf,\n                                           u_I0x_buf, u_I0y_buf, u_I0xs[i], u_I0ys[i]))\n            return false;\n\n        if (!ocl_PatchInverseSearch(u_Ux[i], u_Uy[i], u_I0s[i], u_I1s_ext[i], u_I0xs[i], u_I0ys[i], 2, i))\n            return false;\n\n        if (!ocl_Densification(u_Ux[i], u_Uy[i], u_Sx, u_Sy, u_I0s[i], u_I1s[i]))\n            return false;\n\n        if (variational_refinement_iter > 0)\n            variational_refinement_processors[i]->calcUV(u_I0s[i], u_I1s[i],\n                                                         u_Ux[i].getMat(ACCESS_WRITE), u_Uy[i].getMat(ACCESS_WRITE));\n\n        if (i > finest_scale)\n        {\n            resize(u_Ux[i], u_Ux[i - 1], u_Ux[i - 1].size());\n            resize(u_Uy[i], u_Uy[i - 1], u_Uy[i - 1].size());\n            multiply(u_Ux[i - 1], 2, u_Ux[i - 1]);\n            multiply(u_Uy[i - 1], 2, u_Uy[i - 1]);\n        }\n    }\n    vector<UMat> uxy(2);\n    uxy[0] = u_Ux[finest_scale];\n    uxy[1] = u_Uy[finest_scale];\n    merge(uxy, u_U);\n    resize(u_U, u_flowMat, u_flowMat.size());\n    multiply(u_flowMat, 1 << finest_scale, u_flowMat);\n\n    return true;\n}\n#endif\n\nvoid DISOpticalFlowImpl::calc(InputArray I0, InputArray I1, InputOutputArray flow)\n{\n    CV_Assert(!I0.empty() && I0.depth() == CV_8U && I0.channels() == 1);\n    CV_Assert(!I1.empty() && I1.depth() == CV_8U && I1.channels() == 1);\n    CV_Assert(I0.sameSize(I1));\n    CV_Assert(I0.isContinuous());\n    CV_Assert(I1.isContinuous());\n\n    CV_OCL_RUN(flow.isUMat() &&\n               (patch_size == 8) && (use_spatial_propagation == true),\n               ocl_calc(I0, I1, flow));\n\n    Mat I0Mat = I0.getMat();\n    Mat I1Mat = I1.getMat();\n    bool use_input_flow = false;\n    if (flow.sameSize(I0) && flow.depth() == CV_32F && flow.channels() == 2)\n        use_input_flow = true;\n    else\n        flow.create(I1Mat.size(), CV_32FC2);\n    Mat flowMat = flow.getMat();\n    coarsest_scale = min((int)(log(max(I0Mat.cols, I0Mat.rows) / (4.0 * patch_size)) / log(2.0) + 0.5), /* Original code serach for maximal movement of width/4 */\n                         (int)(log(min(I0Mat.cols, I0Mat.rows) / patch_size) / log(2.0)));              /* Deepest pyramid level greater or equal than patch*/\n    int num_stripes = getNumThreads();\n\n    prepareBuffers(I0Mat, I1Mat, flowMat, use_input_flow);\n    Ux[coarsest_scale].setTo(0.0f);\n    Uy[coarsest_scale].setTo(0.0f);\n\n    for (int i = coarsest_scale; i >= finest_scale; i--)\n    {\n        w = I0s[i].cols;\n        h = I0s[i].rows;\n        ws = 1 + (w - patch_size) / patch_stride;\n        hs = 1 + (h - patch_size) / patch_stride;\n\n        precomputeStructureTensor(I0xx_buf, I0yy_buf, I0xy_buf, I0x_buf, I0y_buf, I0xs[i], I0ys[i]);\n        if (use_spatial_propagation)\n        {\n            /* Use a fixed number of stripes regardless the number of threads to make inverse search\n             * with spatial propagation reproducible\n             */\n            parallel_for_(Range(0, 8), PatchInverseSearch_ParBody(*this, 8, hs, Sx, Sy, Ux[i], Uy[i], I0s[i],\n                                                                  I1s_ext[i], I0xs[i], I0ys[i], 2, i));\n        }\n        else\n        {\n            parallel_for_(Range(0, num_stripes),\n                          PatchInverseSearch_ParBody(*this, num_stripes, hs, Sx, Sy, Ux[i], Uy[i], I0s[i], I1s_ext[i],\n                                                     I0xs[i], I0ys[i], 1, i));\n        }\n\n        parallel_for_(Range(0, num_stripes),\n                      Densification_ParBody(*this, num_stripes, I0s[i].rows, Ux[i], Uy[i], Sx, Sy, I0s[i], I1s[i]));\n        if (variational_refinement_iter > 0)\n            variational_refinement_processors[i]->calcUV(I0s[i], I1s[i], Ux[i], Uy[i]);\n\n        if (i > finest_scale)\n        {\n            resize(Ux[i], Ux[i - 1], Ux[i - 1].size());\n            resize(Uy[i], Uy[i - 1], Uy[i - 1].size());\n            Ux[i - 1] *= 2;\n            Uy[i - 1] *= 2;\n        }\n    }\n    Mat uxy[] = {Ux[finest_scale], Uy[finest_scale]};\n    merge(uxy, 2, U);\n    resize(U, flowMat, flowMat.size());\n    flowMat *= 1 << finest_scale;\n}\n\nvoid DISOpticalFlowImpl::collectGarbage()\n{\n    I0s.clear();\n    I1s.clear();\n    I1s_ext.clear();\n    I0xs.clear();\n    I0ys.clear();\n    Ux.clear();\n    Uy.clear();\n    U.release();\n    Sx.release();\n    Sy.release();\n    I0xx_buf.release();\n    I0yy_buf.release();\n    I0xy_buf.release();\n    I0xx_buf_aux.release();\n    I0yy_buf_aux.release();\n    I0xy_buf_aux.release();\n\n#ifdef HAVE_OPENCL\n    u_I0s.clear();\n    u_I1s.clear();\n    u_I1s_ext.clear();\n    u_I0xs.clear();\n    u_I0ys.clear();\n    u_Ux.clear();\n    u_Uy.clear();\n    u_U.release();\n    u_Sx.release();\n    u_Sy.release();\n    u_I0xx_buf.release();\n    u_I0yy_buf.release();\n    u_I0xy_buf.release();\n    u_I0xx_buf_aux.release();\n    u_I0yy_buf_aux.release();\n    u_I0xy_buf_aux.release();\n#endif\n\n    for (int i = finest_scale; i <= coarsest_scale; i++)\n        variational_refinement_processors[i]->collectGarbage();\n    variational_refinement_processors.clear();\n}\n\nPtr<DISOpticalFlow> DISOpticalFlow::create(int preset)\n{\n    Ptr<DISOpticalFlow> dis = makePtr<DISOpticalFlowImpl>();\n    dis->setPatchSize(8);\n    if (preset == DISOpticalFlow::PRESET_ULTRAFAST)\n    {\n        dis->setFinestScale(2);\n        dis->setPatchStride(4);\n        dis->setGradientDescentIterations(12);\n        dis->setVariationalRefinementIterations(0);\n    }\n    else if (preset == DISOpticalFlow::PRESET_FAST)\n    {\n        dis->setFinestScale(2);\n        dis->setPatchStride(4);\n        dis->setGradientDescentIterations(16);\n        dis->setVariationalRefinementIterations(5);\n    }\n    else if (preset == DISOpticalFlow::PRESET_MEDIUM)\n    {\n        dis->setFinestScale(1);\n        dis->setPatchStride(3);\n        dis->setGradientDescentIterations(25);\n        dis->setVariationalRefinementIterations(5);\n    }\n\n    return dis;\n}\n}\n", "/*M///////////////////////////////////////////////////////////////////////////////////////\n//\n//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.\n//\n//  By downloading, copying, installing or using the software you agree to this license.\n//  If you do not agree to this license, do not download, install,\n//  copy or use the software.\n//\n//\n//                        Intel License Agreement\n//                For Open Source Computer Vision Library\n//\n// Copyright (C) 2000, Intel Corporation, all rights reserved.\n// Third party copyrights are property of their respective owners.\n//\n// Redistribution and use in source and binary forms, with or without modification,\n// are permitted provided that the following conditions are met:\n//\n//   * Redistribution's of source code must retain the above copyright notice,\n//     this list of conditions and the following disclaimer.\n//\n//   * Redistribution's in binary form must reproduce the above copyright notice,\n//     this list of conditions and the following disclaimer in the documentation\n//     and/or other materials provided with the distribution.\n//\n//   * The name of Intel Corporation may not be used to endorse or promote products\n//     derived from this software without specific prior written permission.\n//\n// This software is provided by the copyright holders and contributors \"as is\" and\n// any express or implied warranties, including, but not limited to, the implied\n// warranties of merchantability and fitness for a particular purpose are disclaimed.\n// In no event shall the Intel Corporation or contributors be liable for any direct,\n// indirect, incidental, special, exemplary, or consequential damages\n// (including, but not limited to, procurement of substitute goods or services;\n// loss of use, data, or profits; or business interruption) however caused\n// and on any theory of liability, whether in contract, strict liability,\n// or tort (including negligence or otherwise) arising in any way out of\n// the use of this software, even if advised of the possibility of such damage.\n//\n//M*/\n\n#include \"test_precomp.hpp\"\n\nnamespace opencv_test { namespace {\n\nstatic string getDataDir() { return TS::ptr()->get_data_path(); }\n\nstatic string getRubberWhaleFrame1() { return getDataDir() + \"optflow/RubberWhale1.png\"; }\n\nstatic string getRubberWhaleFrame2() { return getDataDir() + \"optflow/RubberWhale2.png\"; }\n\nstatic string getRubberWhaleGroundTruth() { return getDataDir() + \"optflow/RubberWhale.flo\"; }\n\nstatic bool isFlowCorrect(float u) { return !cvIsNaN(u) && (fabs(u) < 1e9); }\n\nstatic float calcRMSE(Mat flow1, Mat flow2)\n{\n    float sum = 0;\n    int counter = 0;\n    const int rows = flow1.rows;\n    const int cols = flow1.cols;\n\n    for (int y = 0; y < rows; ++y)\n    {\n        for (int x = 0; x < cols; ++x)\n        {\n            Vec2f flow1_at_point = flow1.at<Vec2f>(y, x);\n            Vec2f flow2_at_point = flow2.at<Vec2f>(y, x);\n\n            float u1 = flow1_at_point[0];\n            float v1 = flow1_at_point[1];\n            float u2 = flow2_at_point[0];\n            float v2 = flow2_at_point[1];\n\n            if (isFlowCorrect(u1) && isFlowCorrect(u2) && isFlowCorrect(v1) && isFlowCorrect(v2))\n            {\n                sum += (u1 - u2) * (u1 - u2) + (v1 - v2) * (v1 - v2);\n                counter++;\n            }\n        }\n    }\n    return (float)sqrt(sum / (1e-9 + counter));\n}\n\nbool readRubberWhale(Mat &dst_frame_1, Mat &dst_frame_2, Mat &dst_GT)\n{\n    const string frame1_path = getRubberWhaleFrame1();\n    const string frame2_path = getRubberWhaleFrame2();\n    const string gt_flow_path = getRubberWhaleGroundTruth();\n\n    dst_frame_1 = imread(frame1_path);\n    dst_frame_2 = imread(frame2_path);\n    dst_GT = readOpticalFlow(gt_flow_path);\n\n    if (dst_frame_1.empty() || dst_frame_2.empty() || dst_GT.empty())\n        return false;\n    else\n        return true;\n}\n\nTEST(DenseOpticalFlow_DIS, ReferenceAccuracy)\n{\n    Mat frame1, frame2, GT;\n    ASSERT_TRUE(readRubberWhale(frame1, frame2, GT));\n    int presets[] = {DISOpticalFlow::PRESET_ULTRAFAST, DISOpticalFlow::PRESET_FAST, DISOpticalFlow::PRESET_MEDIUM};\n    float target_RMSE[] = {0.86f, 0.74f, 0.49f};\n    cvtColor(frame1, frame1, COLOR_BGR2GRAY);\n    cvtColor(frame2, frame2, COLOR_BGR2GRAY);\n\n    Ptr<DenseOpticalFlow> algo;\n\n    // iterate over presets:\n    for (int i = 0; i < 3; i++)\n    {\n        Mat flow;\n        algo = DISOpticalFlow::create(presets[i]);\n        algo->calc(frame1, frame2, flow);\n        ASSERT_EQ(GT.rows, flow.rows);\n        ASSERT_EQ(GT.cols, flow.cols);\n        EXPECT_LE(calcRMSE(GT, flow), target_RMSE[i]);\n    }\n}\n\nTEST(DenseOpticalFlow_VariationalRefinement, ReferenceAccuracy)\n{\n    Mat frame1, frame2, GT;\n    ASSERT_TRUE(readRubberWhale(frame1, frame2, GT));\n    float target_RMSE = 0.86f;\n    cvtColor(frame1, frame1, COLOR_BGR2GRAY);\n    cvtColor(frame2, frame2, COLOR_BGR2GRAY);\n\n    Ptr<VariationalRefinement> var_ref;\n    var_ref = VariationalRefinement::create();\n    var_ref->setAlpha(20.0f);\n    var_ref->setDelta(5.0f);\n    var_ref->setGamma(10.0f);\n    var_ref->setSorIterations(25);\n    var_ref->setFixedPointIterations(25);\n    Mat flow(frame1.size(), CV_32FC2);\n    flow.setTo(0.0f);\n    var_ref->calc(frame1, frame2, flow);\n    ASSERT_EQ(GT.rows, flow.rows);\n    ASSERT_EQ(GT.cols, flow.cols);\n    EXPECT_LE(calcRMSE(GT, flow), target_RMSE);\n}\n\n}} // namespace\n"], "fixing_code": ["/*M///////////////////////////////////////////////////////////////////////////////////////\n//\n//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.\n//\n//  By downloading, copying, installing or using the software you agree to this license.\n//  If you do not agree to this license, do not download, install,\n//  copy or use the software.\n//\n//\n//                           License Agreement\n//                For Open Source Computer Vision Library\n//\n// Copyright (C) 2000-2008, Intel Corporation, all rights reserved.\n// Copyright (C) 2009, Willow Garage Inc., all rights reserved.\n// Third party copyrights are property of their respective owners.\n//\n// Redistribution and use in source and binary forms, with or without modification,\n// are permitted provided that the following conditions are met:\n//\n//   * Redistribution's of source code must retain the above copyright notice,\n//     this list of conditions and the following disclaimer.\n//\n//   * Redistribution's in binary form must reproduce the above copyright notice,\n//     this list of conditions and the following disclaimer in the documentation\n//     and/or other materials provided with the distribution.\n//\n//   * The name of the copyright holders may not be used to endorse or promote products\n//     derived from this software without specific prior written permission.\n//\n// This software is provided by the copyright holders and contributors \"as is\" and\n// any express or implied warranties, including, but not limited to, the implied\n// warranties of merchantability and fitness for a particular purpose are disclaimed.\n// In no event shall the Intel Corporation or contributors be liable for any direct,\n// indirect, incidental, special, exemplary, or consequential damages\n// (including, but not limited to, procurement of substitute goods or services;\n// loss of use, data, or profits; or business interruption) however caused\n// and on any theory of liability, whether in contract, strict liability,\n// or tort (including negligence or otherwise) arising in any way out of\n// the use of this software, even if advised of the possibility of such damage.\n//\n//M*/\n\n#include \"precomp.hpp\"\n#include \"opencv2/core/hal/intrin.hpp\"\n#include \"opencl_kernels_video.hpp\"\n\nusing namespace std;\n#define EPS 0.001F\n#define INF 1E+10F\n\nnamespace cv\n{\n\nclass DISOpticalFlowImpl CV_FINAL : public DISOpticalFlow\n{\n  public:\n    DISOpticalFlowImpl();\n\n    void calc(InputArray I0, InputArray I1, InputOutputArray flow) CV_OVERRIDE;\n    void collectGarbage() CV_OVERRIDE;\n\n  protected: //!< algorithm parameters\n    int finest_scale, coarsest_scale;\n    int patch_size;\n    int patch_stride;\n    int grad_descent_iter;\n    int variational_refinement_iter;\n    float variational_refinement_alpha;\n    float variational_refinement_gamma;\n    float variational_refinement_delta;\n    bool use_mean_normalization;\n    bool use_spatial_propagation;\n\n  protected: //!< some auxiliary variables\n    int border_size;\n    int w, h;   //!< flow buffer width and height on the current scale\n    int ws, hs; //!< sparse flow buffer width and height on the current scale\n\n  public:\n    int getFinestScale() const CV_OVERRIDE { return finest_scale; }\n    void setFinestScale(int val) CV_OVERRIDE { finest_scale = val; }\n    int getPatchSize() const CV_OVERRIDE { return patch_size; }\n    void setPatchSize(int val) CV_OVERRIDE { patch_size = val; }\n    int getPatchStride() const CV_OVERRIDE { return patch_stride; }\n    void setPatchStride(int val) CV_OVERRIDE { patch_stride = val; }\n    int getGradientDescentIterations() const CV_OVERRIDE { return grad_descent_iter; }\n    void setGradientDescentIterations(int val) CV_OVERRIDE { grad_descent_iter = val; }\n    int getVariationalRefinementIterations() const CV_OVERRIDE { return variational_refinement_iter; }\n    void setVariationalRefinementIterations(int val) CV_OVERRIDE { variational_refinement_iter = val; }\n    float getVariationalRefinementAlpha() const CV_OVERRIDE { return variational_refinement_alpha; }\n    void setVariationalRefinementAlpha(float val) CV_OVERRIDE { variational_refinement_alpha = val; }\n    float getVariationalRefinementDelta() const CV_OVERRIDE { return variational_refinement_delta; }\n    void setVariationalRefinementDelta(float val) CV_OVERRIDE { variational_refinement_delta = val; }\n    float getVariationalRefinementGamma() const CV_OVERRIDE { return variational_refinement_gamma; }\n    void setVariationalRefinementGamma(float val) CV_OVERRIDE { variational_refinement_gamma = val; }\n\n    bool getUseMeanNormalization() const CV_OVERRIDE { return use_mean_normalization; }\n    void setUseMeanNormalization(bool val) CV_OVERRIDE { use_mean_normalization = val; }\n    bool getUseSpatialPropagation() const CV_OVERRIDE { return use_spatial_propagation; }\n    void setUseSpatialPropagation(bool val) CV_OVERRIDE { use_spatial_propagation = val; }\n\n  protected:                      //!< internal buffers\n    vector<Mat_<uchar> > I0s;     //!< Gaussian pyramid for the current frame\n    vector<Mat_<uchar> > I1s;     //!< Gaussian pyramid for the next frame\n    vector<Mat_<uchar> > I1s_ext; //!< I1s with borders\n\n    vector<Mat_<short> > I0xs; //!< Gaussian pyramid for the x gradient of the current frame\n    vector<Mat_<short> > I0ys; //!< Gaussian pyramid for the y gradient of the current frame\n\n    vector<Mat_<float> > Ux; //!< x component of the flow vectors\n    vector<Mat_<float> > Uy; //!< y component of the flow vectors\n\n    vector<Mat_<float> > initial_Ux; //!< x component of the initial flow field, if one was passed as an input\n    vector<Mat_<float> > initial_Uy; //!< y component of the initial flow field, if one was passed as an input\n\n    Mat_<Vec2f> U; //!< a buffer for the merged flow\n\n    Mat_<float> Sx; //!< intermediate sparse flow representation (x component)\n    Mat_<float> Sy; //!< intermediate sparse flow representation (y component)\n\n    /* Structure tensor components: */\n    Mat_<float> I0xx_buf; //!< sum of squares of x gradient values\n    Mat_<float> I0yy_buf; //!< sum of squares of y gradient values\n    Mat_<float> I0xy_buf; //!< sum of x and y gradient products\n\n    /* Extra buffers that are useful if patch mean-normalization is used: */\n    Mat_<float> I0x_buf; //!< sum of x gradient values\n    Mat_<float> I0y_buf; //!< sum of y gradient values\n\n    /* Auxiliary buffers used in structure tensor computation: */\n    Mat_<float> I0xx_buf_aux;\n    Mat_<float> I0yy_buf_aux;\n    Mat_<float> I0xy_buf_aux;\n    Mat_<float> I0x_buf_aux;\n    Mat_<float> I0y_buf_aux;\n\n    vector<Ptr<VariationalRefinement> > variational_refinement_processors;\n\n  private: //!< private methods and parallel sections\n    void prepareBuffers(Mat &I0, Mat &I1, Mat &flow, bool use_flow);\n    void precomputeStructureTensor(Mat &dst_I0xx, Mat &dst_I0yy, Mat &dst_I0xy, Mat &dst_I0x, Mat &dst_I0y, Mat &I0x,\n                                   Mat &I0y);\n    int autoSelectCoarsestScale(int img_width);\n    void autoSelectPatchSizeAndScales(int img_width);\n\n    struct PatchInverseSearch_ParBody : public ParallelLoopBody\n    {\n        DISOpticalFlowImpl *dis;\n        int nstripes, stripe_sz;\n        int hs;\n        Mat *Sx, *Sy, *Ux, *Uy, *I0, *I1, *I0x, *I0y;\n        int num_iter, pyr_level;\n\n        PatchInverseSearch_ParBody(DISOpticalFlowImpl &_dis, int _nstripes, int _hs, Mat &dst_Sx, Mat &dst_Sy,\n                                   Mat &src_Ux, Mat &src_Uy, Mat &_I0, Mat &_I1, Mat &_I0x, Mat &_I0y, int _num_iter,\n                                   int _pyr_level);\n        void operator()(const Range &range) const CV_OVERRIDE;\n    };\n\n    struct Densification_ParBody : public ParallelLoopBody\n    {\n        DISOpticalFlowImpl *dis;\n        int nstripes, stripe_sz;\n        int h;\n        Mat *Ux, *Uy, *Sx, *Sy, *I0, *I1;\n\n        Densification_ParBody(DISOpticalFlowImpl &_dis, int _nstripes, int _h, Mat &dst_Ux, Mat &dst_Uy, Mat &src_Sx,\n                              Mat &src_Sy, Mat &_I0, Mat &_I1);\n        void operator()(const Range &range) const CV_OVERRIDE;\n    };\n\n#ifdef HAVE_OPENCL\n    vector<UMat> u_I0s;     //!< Gaussian pyramid for the current frame\n    vector<UMat> u_I1s;     //!< Gaussian pyramid for the next frame\n    vector<UMat> u_I1s_ext; //!< I1s with borders\n\n    vector<UMat> u_I0xs; //!< Gaussian pyramid for the x gradient of the current frame\n    vector<UMat> u_I0ys; //!< Gaussian pyramid for the y gradient of the current frame\n\n    vector<UMat> u_Ux; //!< x component of the flow vectors\n    vector<UMat> u_Uy; //!< y component of the flow vectors\n\n    vector<UMat> u_initial_Ux; //!< x component of the initial flow field, if one was passed as an input\n    vector<UMat> u_initial_Uy; //!< y component of the initial flow field, if one was passed as an input\n\n    UMat u_U; //!< a buffer for the merged flow\n\n    UMat u_Sx; //!< intermediate sparse flow representation (x component)\n    UMat u_Sy; //!< intermediate sparse flow representation (y component)\n\n    /* Structure tensor components: */\n    UMat u_I0xx_buf; //!< sum of squares of x gradient values\n    UMat u_I0yy_buf; //!< sum of squares of y gradient values\n    UMat u_I0xy_buf; //!< sum of x and y gradient products\n\n    /* Extra buffers that are useful if patch mean-normalization is used: */\n    UMat u_I0x_buf; //!< sum of x gradient values\n    UMat u_I0y_buf; //!< sum of y gradient values\n\n    /* Auxiliary buffers used in structure tensor computation: */\n    UMat u_I0xx_buf_aux;\n    UMat u_I0yy_buf_aux;\n    UMat u_I0xy_buf_aux;\n    UMat u_I0x_buf_aux;\n    UMat u_I0y_buf_aux;\n\n    bool ocl_precomputeStructureTensor(UMat &dst_I0xx, UMat &dst_I0yy, UMat &dst_I0xy,\n                                       UMat &dst_I0x, UMat &dst_I0y, UMat &I0x, UMat &I0y);\n    void ocl_prepareBuffers(UMat &I0, UMat &I1, UMat &flow, bool use_flow);\n    bool ocl_calc(InputArray I0, InputArray I1, InputOutputArray flow);\n    bool ocl_Densification(UMat &dst_Ux, UMat &dst_Uy, UMat &src_Sx, UMat &src_Sy, UMat &_I0, UMat &_I1);\n    bool ocl_PatchInverseSearch(UMat &src_Ux, UMat &src_Uy,\n                                UMat &I0, UMat &I1, UMat &I0x, UMat &I0y, int num_iter, int pyr_level);\n#endif\n};\n\nDISOpticalFlowImpl::DISOpticalFlowImpl()\n{\n    finest_scale = 2;\n    patch_size = 8;\n    patch_stride = 4;\n    grad_descent_iter = 16;\n    variational_refinement_iter = 5;\n    variational_refinement_alpha = 20.f;\n    variational_refinement_gamma = 10.f;\n    variational_refinement_delta = 5.f;\n\n    border_size = 16;\n    use_mean_normalization = true;\n    use_spatial_propagation = true;\n    coarsest_scale = 10;\n\n    /* Use separate variational refinement instances for different scales to avoid repeated memory allocation: */\n    int max_possible_scales = 10;\n    ws = hs = w = h = 0;\n    for (int i = 0; i < max_possible_scales; i++)\n        variational_refinement_processors.push_back(VariationalRefinement::create());\n}\n\nvoid DISOpticalFlowImpl::prepareBuffers(Mat &I0, Mat &I1, Mat &flow, bool use_flow)\n{\n    I0s.resize(coarsest_scale + 1);\n    I1s.resize(coarsest_scale + 1);\n    I1s_ext.resize(coarsest_scale + 1);\n    I0xs.resize(coarsest_scale + 1);\n    I0ys.resize(coarsest_scale + 1);\n    Ux.resize(coarsest_scale + 1);\n    Uy.resize(coarsest_scale + 1);\n\n    Mat flow_uv[2];\n    if (use_flow)\n    {\n        split(flow, flow_uv);\n        initial_Ux.resize(coarsest_scale + 1);\n        initial_Uy.resize(coarsest_scale + 1);\n    }\n\n    int fraction = 1;\n    int cur_rows = 0, cur_cols = 0;\n\n    for (int i = 0; i <= coarsest_scale; i++)\n    {\n        /* Avoid initializing the pyramid levels above the finest scale, as they won't be used anyway */\n        if (i == finest_scale)\n        {\n            cur_rows = I0.rows / fraction;\n            cur_cols = I0.cols / fraction;\n            I0s[i].create(cur_rows, cur_cols);\n            resize(I0, I0s[i], I0s[i].size(), 0.0, 0.0, INTER_AREA);\n            I1s[i].create(cur_rows, cur_cols);\n            resize(I1, I1s[i], I1s[i].size(), 0.0, 0.0, INTER_AREA);\n\n            /* These buffers are reused in each scale so we initialize them once on the finest scale: */\n            Sx.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            Sy.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0xx_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0yy_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0xy_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0x_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n            I0y_buf.create(cur_rows / patch_stride, cur_cols / patch_stride);\n\n            I0xx_buf_aux.create(cur_rows, cur_cols / patch_stride);\n            I0yy_buf_aux.create(cur_rows, cur_cols / patch_stride);\n            I0xy_buf_aux.create(cur_rows, cur_cols / patch_stride);\n            I0x_buf_aux.create(cur_rows, cur_cols / patch_stride);\n            I0y_buf_aux.create(cur_rows, cur_cols / patch_stride);\n\n            U.create(cur_rows, cur_cols);\n        }\n        else if (i > finest_scale)\n        {\n            cur_rows = I0s[i - 1].rows / 2;\n            cur_cols = I0s[i - 1].cols / 2;\n            I0s[i].create(cur_rows, cur_cols);\n            resize(I0s[i - 1], I0s[i], I0s[i].size(), 0.0, 0.0, INTER_AREA);\n            I1s[i].create(cur_rows, cur_cols);\n            resize(I1s[i - 1], I1s[i], I1s[i].size(), 0.0, 0.0, INTER_AREA);\n        }\n\n        if (i >= finest_scale)\n        {\n            I1s_ext[i].create(cur_rows + 2 * border_size, cur_cols + 2 * border_size);\n            copyMakeBorder(I1s[i], I1s_ext[i], border_size, border_size, border_size, border_size, BORDER_REPLICATE);\n            I0xs[i].create(cur_rows, cur_cols);\n            I0ys[i].create(cur_rows, cur_cols);\n            spatialGradient(I0s[i], I0xs[i], I0ys[i]);\n            Ux[i].create(cur_rows, cur_cols);\n            Uy[i].create(cur_rows, cur_cols);\n            variational_refinement_processors[i]->setAlpha(variational_refinement_alpha);\n            variational_refinement_processors[i]->setDelta(variational_refinement_delta);\n            variational_refinement_processors[i]->setGamma(variational_refinement_gamma);\n            variational_refinement_processors[i]->setSorIterations(5);\n            variational_refinement_processors[i]->setFixedPointIterations(variational_refinement_iter);\n\n            if (use_flow)\n            {\n                resize(flow_uv[0], initial_Ux[i], Size(cur_cols, cur_rows));\n                initial_Ux[i] /= fraction;\n                resize(flow_uv[1], initial_Uy[i], Size(cur_cols, cur_rows));\n                initial_Uy[i] /= fraction;\n            }\n        }\n\n        fraction *= 2;\n    }\n}\n\n/* This function computes the structure tensor elements (local sums of I0x^2, I0x*I0y and I0y^2).\n * A simple box filter is not used instead because we need to compute these sums on a sparse grid\n * and store them densely in the output buffers.\n */\nvoid DISOpticalFlowImpl::precomputeStructureTensor(Mat &dst_I0xx, Mat &dst_I0yy, Mat &dst_I0xy, Mat &dst_I0x,\n                                                   Mat &dst_I0y, Mat &I0x, Mat &I0y)\n{\n    float *I0xx_ptr = dst_I0xx.ptr<float>();\n    float *I0yy_ptr = dst_I0yy.ptr<float>();\n    float *I0xy_ptr = dst_I0xy.ptr<float>();\n    float *I0x_ptr = dst_I0x.ptr<float>();\n    float *I0y_ptr = dst_I0y.ptr<float>();\n\n    float *I0xx_aux_ptr = I0xx_buf_aux.ptr<float>();\n    float *I0yy_aux_ptr = I0yy_buf_aux.ptr<float>();\n    float *I0xy_aux_ptr = I0xy_buf_aux.ptr<float>();\n    float *I0x_aux_ptr = I0x_buf_aux.ptr<float>();\n    float *I0y_aux_ptr = I0y_buf_aux.ptr<float>();\n\n    /* Separable box filter: horizontal pass */\n    for (int i = 0; i < h; i++)\n    {\n        float sum_xx = 0.0f, sum_yy = 0.0f, sum_xy = 0.0f, sum_x = 0.0f, sum_y = 0.0f;\n        short *x_row = I0x.ptr<short>(i);\n        short *y_row = I0y.ptr<short>(i);\n        for (int j = 0; j < patch_size; j++)\n        {\n            sum_xx += x_row[j] * x_row[j];\n            sum_yy += y_row[j] * y_row[j];\n            sum_xy += x_row[j] * y_row[j];\n            sum_x += x_row[j];\n            sum_y += y_row[j];\n        }\n        I0xx_aux_ptr[i * ws] = sum_xx;\n        I0yy_aux_ptr[i * ws] = sum_yy;\n        I0xy_aux_ptr[i * ws] = sum_xy;\n        I0x_aux_ptr[i * ws] = sum_x;\n        I0y_aux_ptr[i * ws] = sum_y;\n        int js = 1;\n        for (int j = patch_size; j < w; j++)\n        {\n            sum_xx += (x_row[j] * x_row[j] - x_row[j - patch_size] * x_row[j - patch_size]);\n            sum_yy += (y_row[j] * y_row[j] - y_row[j - patch_size] * y_row[j - patch_size]);\n            sum_xy += (x_row[j] * y_row[j] - x_row[j - patch_size] * y_row[j - patch_size]);\n            sum_x += (x_row[j] - x_row[j - patch_size]);\n            sum_y += (y_row[j] - y_row[j - patch_size]);\n            if ((j - patch_size + 1) % patch_stride == 0)\n            {\n                I0xx_aux_ptr[i * ws + js] = sum_xx;\n                I0yy_aux_ptr[i * ws + js] = sum_yy;\n                I0xy_aux_ptr[i * ws + js] = sum_xy;\n                I0x_aux_ptr[i * ws + js] = sum_x;\n                I0y_aux_ptr[i * ws + js] = sum_y;\n                js++;\n            }\n        }\n    }\n\n    AutoBuffer<float> sum_xx(ws), sum_yy(ws), sum_xy(ws), sum_x(ws), sum_y(ws);\n    for (int j = 0; j < ws; j++)\n    {\n        sum_xx[j] = 0.0f;\n        sum_yy[j] = 0.0f;\n        sum_xy[j] = 0.0f;\n        sum_x[j] = 0.0f;\n        sum_y[j] = 0.0f;\n    }\n\n    /* Separable box filter: vertical pass */\n    for (int i = 0; i < patch_size; i++)\n        for (int j = 0; j < ws; j++)\n        {\n            sum_xx[j] += I0xx_aux_ptr[i * ws + j];\n            sum_yy[j] += I0yy_aux_ptr[i * ws + j];\n            sum_xy[j] += I0xy_aux_ptr[i * ws + j];\n            sum_x[j] += I0x_aux_ptr[i * ws + j];\n            sum_y[j] += I0y_aux_ptr[i * ws + j];\n        }\n    for (int j = 0; j < ws; j++)\n    {\n        I0xx_ptr[j] = sum_xx[j];\n        I0yy_ptr[j] = sum_yy[j];\n        I0xy_ptr[j] = sum_xy[j];\n        I0x_ptr[j] = sum_x[j];\n        I0y_ptr[j] = sum_y[j];\n    }\n    int is = 1;\n    for (int i = patch_size; i < h; i++)\n    {\n        for (int j = 0; j < ws; j++)\n        {\n            sum_xx[j] += (I0xx_aux_ptr[i * ws + j] - I0xx_aux_ptr[(i - patch_size) * ws + j]);\n            sum_yy[j] += (I0yy_aux_ptr[i * ws + j] - I0yy_aux_ptr[(i - patch_size) * ws + j]);\n            sum_xy[j] += (I0xy_aux_ptr[i * ws + j] - I0xy_aux_ptr[(i - patch_size) * ws + j]);\n            sum_x[j] += (I0x_aux_ptr[i * ws + j] - I0x_aux_ptr[(i - patch_size) * ws + j]);\n            sum_y[j] += (I0y_aux_ptr[i * ws + j] - I0y_aux_ptr[(i - patch_size) * ws + j]);\n        }\n        if ((i - patch_size + 1) % patch_stride == 0)\n        {\n            for (int j = 0; j < ws; j++)\n            {\n                I0xx_ptr[is * ws + j] = sum_xx[j];\n                I0yy_ptr[is * ws + j] = sum_yy[j];\n                I0xy_ptr[is * ws + j] = sum_xy[j];\n                I0x_ptr[is * ws + j] = sum_x[j];\n                I0y_ptr[is * ws + j] = sum_y[j];\n            }\n            is++;\n        }\n    }\n}\n\nint DISOpticalFlowImpl::autoSelectCoarsestScale(int img_width)\n{\n    const int fratio = 5;\n    return std::max(0, (int)std::floor(log2((2.0f*(float)img_width) / ((float)fratio * (float)patch_size))));\n}\n\nvoid DISOpticalFlowImpl::autoSelectPatchSizeAndScales(int img_width)\n{\n    switch (finest_scale)\n    {\n    case 1:\n        patch_size = 8;\n        coarsest_scale = autoSelectCoarsestScale(img_width);\n        finest_scale = std::max(coarsest_scale-2, 0);\n        break;\n\n    case 3:\n        patch_size = 12;\n        coarsest_scale = autoSelectCoarsestScale(img_width);\n        finest_scale = std::max(coarsest_scale-4, 0);\n        break;\n\n    case 4:\n        patch_size = 12;\n        coarsest_scale = autoSelectCoarsestScale(img_width);\n        finest_scale = std::max(coarsest_scale-5, 0);\n        break;\n\n    // default case, fall-through.\n    case 2:\n    default:\n        patch_size = 8;\n        coarsest_scale = autoSelectCoarsestScale(img_width);\n        finest_scale = std::max(coarsest_scale-2, 0);\n        break;\n    }\n}\n\nDISOpticalFlowImpl::PatchInverseSearch_ParBody::PatchInverseSearch_ParBody(DISOpticalFlowImpl &_dis, int _nstripes,\n                                                                           int _hs, Mat &dst_Sx, Mat &dst_Sy,\n                                                                           Mat &src_Ux, Mat &src_Uy, Mat &_I0, Mat &_I1,\n                                                                           Mat &_I0x, Mat &_I0y, int _num_iter,\n                                                                           int _pyr_level)\n    : dis(&_dis), nstripes(_nstripes), hs(_hs), Sx(&dst_Sx), Sy(&dst_Sy), Ux(&src_Ux), Uy(&src_Uy), I0(&_I0), I1(&_I1),\n      I0x(&_I0x), I0y(&_I0y), num_iter(_num_iter), pyr_level(_pyr_level)\n{\n    stripe_sz = (int)ceil(hs / (double)nstripes);\n}\n\n/////////////////////////////////////////////* Patch processing functions */////////////////////////////////////////////\n\n/* Some auxiliary macros */\n#define HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION                                                                         \\\n    v_float32x4 w00v = v_setall_f32(w00);                                                                              \\\n    v_float32x4 w01v = v_setall_f32(w01);                                                                              \\\n    v_float32x4 w10v = v_setall_f32(w10);                                                                              \\\n    v_float32x4 w11v = v_setall_f32(w11);                                                                              \\\n                                                                                                                       \\\n    v_uint8x16 I0_row_16, I1_row_16, I1_row_shifted_16, I1_row_next_16, I1_row_next_shifted_16;                        \\\n    v_uint16x8 I0_row_8, I1_row_8, I1_row_shifted_8, I1_row_next_8, I1_row_next_shifted_8, tmp;                        \\\n    v_uint32x4 I0_row_4_left, I1_row_4_left, I1_row_shifted_4_left, I1_row_next_4_left, I1_row_next_shifted_4_left;    \\\n    v_uint32x4 I0_row_4_right, I1_row_4_right, I1_row_shifted_4_right, I1_row_next_4_right,                            \\\n      I1_row_next_shifted_4_right;                                                                                     \\\n    v_float32x4 I_diff_left, I_diff_right;                                                                             \\\n                                                                                                                       \\\n    /* Preload and expand the first row of I1: */                                                                      \\\n    I1_row_16 = v_load(I1_ptr);                                                                                        \\\n    I1_row_shifted_16 = v_extract<1>(I1_row_16, I1_row_16);                                                            \\\n    v_expand(I1_row_16, I1_row_8, tmp);                                                                                \\\n    v_expand(I1_row_shifted_16, I1_row_shifted_8, tmp);                                                                \\\n    v_expand(I1_row_8, I1_row_4_left, I1_row_4_right);                                                                 \\\n    v_expand(I1_row_shifted_8, I1_row_shifted_4_left, I1_row_shifted_4_right);                                         \\\n    I1_ptr += I1_stride;\n\n#define HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION                                                                      \\\n    /* Load the next row of I1: */                                                                                     \\\n    I1_row_next_16 = v_load(I1_ptr);                                                                                   \\\n    /* Circular shift left by 1 element: */                                                                            \\\n    I1_row_next_shifted_16 = v_extract<1>(I1_row_next_16, I1_row_next_16);                                             \\\n    /* Expand to 8 ushorts (we only need the first 8 values): */                                                       \\\n    v_expand(I1_row_next_16, I1_row_next_8, tmp);                                                                      \\\n    v_expand(I1_row_next_shifted_16, I1_row_next_shifted_8, tmp);                                                      \\\n    /* Separate the left and right halves: */                                                                          \\\n    v_expand(I1_row_next_8, I1_row_next_4_left, I1_row_next_4_right);                                                  \\\n    v_expand(I1_row_next_shifted_8, I1_row_next_shifted_4_left, I1_row_next_shifted_4_right);                          \\\n                                                                                                                       \\\n    /* Load current row of I0: */                                                                                      \\\n    I0_row_16 = v_load(I0_ptr);                                                                                        \\\n    v_expand(I0_row_16, I0_row_8, tmp);                                                                                \\\n    v_expand(I0_row_8, I0_row_4_left, I0_row_4_right);                                                                 \\\n                                                                                                                       \\\n    /* Compute diffs between I0 and bilinearly interpolated I1: */                                                     \\\n    I_diff_left = w00v * v_cvt_f32(v_reinterpret_as_s32(I1_row_4_left)) +                                              \\\n                  w01v * v_cvt_f32(v_reinterpret_as_s32(I1_row_shifted_4_left)) +                                      \\\n                  w10v * v_cvt_f32(v_reinterpret_as_s32(I1_row_next_4_left)) +                                         \\\n                  w11v * v_cvt_f32(v_reinterpret_as_s32(I1_row_next_shifted_4_left)) -                                 \\\n                  v_cvt_f32(v_reinterpret_as_s32(I0_row_4_left));                                                      \\\n    I_diff_right = w00v * v_cvt_f32(v_reinterpret_as_s32(I1_row_4_right)) +                                            \\\n                   w01v * v_cvt_f32(v_reinterpret_as_s32(I1_row_shifted_4_right)) +                                    \\\n                   w10v * v_cvt_f32(v_reinterpret_as_s32(I1_row_next_4_right)) +                                       \\\n                   w11v * v_cvt_f32(v_reinterpret_as_s32(I1_row_next_shifted_4_right)) -                               \\\n                   v_cvt_f32(v_reinterpret_as_s32(I0_row_4_right));\n\n#define HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW                                                                     \\\n    I0_ptr += I0_stride;                                                                                               \\\n    I1_ptr += I1_stride;                                                                                               \\\n                                                                                                                       \\\n    I1_row_4_left = I1_row_next_4_left;                                                                                \\\n    I1_row_4_right = I1_row_next_4_right;                                                                              \\\n    I1_row_shifted_4_left = I1_row_next_shifted_4_left;                                                                \\\n    I1_row_shifted_4_right = I1_row_next_shifted_4_right;\n\n/* This function essentially performs one iteration of gradient descent when finding the most similar patch in I1 for a\n * given one in I0. It assumes that I0_ptr and I1_ptr already point to the corresponding patches and w00, w01, w10, w11\n * are precomputed bilinear interpolation weights. It returns the SSD (sum of squared differences) between these patches\n * and computes the values (dst_dUx, dst_dUy) that are used in the flow vector update. HAL acceleration is implemented\n * only for the default patch size (8x8). Everything is processed in floats as using fixed-point approximations harms\n * the quality significantly.\n */\ninline float processPatch(float &dst_dUx, float &dst_dUy, uchar *I0_ptr, uchar *I1_ptr, short *I0x_ptr, short *I0y_ptr,\n                          int I0_stride, int I1_stride, float w00, float w01, float w10, float w11, int patch_sz)\n{\n    float SSD = 0.0f;\n#if CV_SIMD128\n    if (patch_sz == 8)\n    {\n        /* Variables to accumulate the sums */\n        v_float32x4 Ux_vec = v_setall_f32(0);\n        v_float32x4 Uy_vec = v_setall_f32(0);\n        v_float32x4 SSD_vec = v_setall_f32(0);\n\n        v_int16x8 I0x_row, I0y_row;\n        v_int32x4 I0x_row_4_left, I0x_row_4_right, I0y_row_4_left, I0y_row_4_right;\n\n        HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION;\n        for (int row = 0; row < 8; row++)\n        {\n            HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION;\n            I0x_row = v_load(I0x_ptr);\n            v_expand(I0x_row, I0x_row_4_left, I0x_row_4_right);\n            I0y_row = v_load(I0y_ptr);\n            v_expand(I0y_row, I0y_row_4_left, I0y_row_4_right);\n\n            /* Update the sums: */\n            Ux_vec += I_diff_left * v_cvt_f32(I0x_row_4_left) + I_diff_right * v_cvt_f32(I0x_row_4_right);\n            Uy_vec += I_diff_left * v_cvt_f32(I0y_row_4_left) + I_diff_right * v_cvt_f32(I0y_row_4_right);\n            SSD_vec += I_diff_left * I_diff_left + I_diff_right * I_diff_right;\n\n            I0x_ptr += I0_stride;\n            I0y_ptr += I0_stride;\n            HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW;\n        }\n\n        /* Final reduce operations: */\n        dst_dUx = v_reduce_sum(Ux_vec);\n        dst_dUy = v_reduce_sum(Uy_vec);\n        SSD = v_reduce_sum(SSD_vec);\n    }\n    else\n    {\n#endif\n        dst_dUx = 0.0f;\n        dst_dUy = 0.0f;\n        float diff;\n        for (int i = 0; i < patch_sz; i++)\n            for (int j = 0; j < patch_sz; j++)\n            {\n                diff = w00 * I1_ptr[i * I1_stride + j] + w01 * I1_ptr[i * I1_stride + j + 1] +\n                       w10 * I1_ptr[(i + 1) * I1_stride + j] + w11 * I1_ptr[(i + 1) * I1_stride + j + 1] -\n                       I0_ptr[i * I0_stride + j];\n\n                SSD += diff * diff;\n                dst_dUx += diff * I0x_ptr[i * I0_stride + j];\n                dst_dUy += diff * I0y_ptr[i * I0_stride + j];\n            }\n#if CV_SIMD128\n    }\n#endif\n    return SSD;\n}\n\n/* Same as processPatch, but with patch mean normalization, which improves robustness under changing\n * lighting conditions\n */\ninline float processPatchMeanNorm(float &dst_dUx, float &dst_dUy, uchar *I0_ptr, uchar *I1_ptr, short *I0x_ptr,\n                                  short *I0y_ptr, int I0_stride, int I1_stride, float w00, float w01, float w10,\n                                  float w11, int patch_sz, float x_grad_sum, float y_grad_sum)\n{\n    float sum_diff = 0.0, sum_diff_sq = 0.0;\n    float sum_I0x_mul = 0.0, sum_I0y_mul = 0.0;\n    float n = (float)patch_sz * patch_sz;\n\n#if CV_SIMD128\n    if (patch_sz == 8)\n    {\n        /* Variables to accumulate the sums */\n        v_float32x4 sum_I0x_mul_vec = v_setall_f32(0);\n        v_float32x4 sum_I0y_mul_vec = v_setall_f32(0);\n        v_float32x4 sum_diff_vec = v_setall_f32(0);\n        v_float32x4 sum_diff_sq_vec = v_setall_f32(0);\n\n        v_int16x8 I0x_row, I0y_row;\n        v_int32x4 I0x_row_4_left, I0x_row_4_right, I0y_row_4_left, I0y_row_4_right;\n\n        HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION;\n        for (int row = 0; row < 8; row++)\n        {\n            HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION;\n            I0x_row = v_load(I0x_ptr);\n            v_expand(I0x_row, I0x_row_4_left, I0x_row_4_right);\n            I0y_row = v_load(I0y_ptr);\n            v_expand(I0y_row, I0y_row_4_left, I0y_row_4_right);\n\n            /* Update the sums: */\n            sum_I0x_mul_vec += I_diff_left * v_cvt_f32(I0x_row_4_left) + I_diff_right * v_cvt_f32(I0x_row_4_right);\n            sum_I0y_mul_vec += I_diff_left * v_cvt_f32(I0y_row_4_left) + I_diff_right * v_cvt_f32(I0y_row_4_right);\n            sum_diff_sq_vec += I_diff_left * I_diff_left + I_diff_right * I_diff_right;\n            sum_diff_vec += I_diff_left + I_diff_right;\n\n            I0x_ptr += I0_stride;\n            I0y_ptr += I0_stride;\n            HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW;\n        }\n\n        /* Final reduce operations: */\n        sum_I0x_mul = v_reduce_sum(sum_I0x_mul_vec);\n        sum_I0y_mul = v_reduce_sum(sum_I0y_mul_vec);\n        sum_diff = v_reduce_sum(sum_diff_vec);\n        sum_diff_sq = v_reduce_sum(sum_diff_sq_vec);\n    }\n    else\n    {\n#endif\n        float diff;\n        for (int i = 0; i < patch_sz; i++)\n            for (int j = 0; j < patch_sz; j++)\n            {\n                diff = w00 * I1_ptr[i * I1_stride + j] + w01 * I1_ptr[i * I1_stride + j + 1] +\n                       w10 * I1_ptr[(i + 1) * I1_stride + j] + w11 * I1_ptr[(i + 1) * I1_stride + j + 1] -\n                       I0_ptr[i * I0_stride + j];\n\n                sum_diff += diff;\n                sum_diff_sq += diff * diff;\n\n                sum_I0x_mul += diff * I0x_ptr[i * I0_stride + j];\n                sum_I0y_mul += diff * I0y_ptr[i * I0_stride + j];\n            }\n#if CV_SIMD128\n    }\n#endif\n    dst_dUx = sum_I0x_mul - sum_diff * x_grad_sum / n;\n    dst_dUy = sum_I0y_mul - sum_diff * y_grad_sum / n;\n    return sum_diff_sq - sum_diff * sum_diff / n;\n}\n\n/* Similar to processPatch, but compute only the sum of squared differences (SSD) between the patches */\ninline float computeSSD(uchar *I0_ptr, uchar *I1_ptr, int I0_stride, int I1_stride, float w00, float w01, float w10,\n                        float w11, int patch_sz)\n{\n    float SSD = 0.0f;\n#if CV_SIMD128\n    if (patch_sz == 8)\n    {\n        v_float32x4 SSD_vec = v_setall_f32(0);\n        HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION;\n        for (int row = 0; row < 8; row++)\n        {\n            HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION;\n            SSD_vec += I_diff_left * I_diff_left + I_diff_right * I_diff_right;\n            HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW;\n        }\n        SSD = v_reduce_sum(SSD_vec);\n    }\n    else\n    {\n#endif\n        float diff;\n        for (int i = 0; i < patch_sz; i++)\n            for (int j = 0; j < patch_sz; j++)\n            {\n                diff = w00 * I1_ptr[i * I1_stride + j] + w01 * I1_ptr[i * I1_stride + j + 1] +\n                       w10 * I1_ptr[(i + 1) * I1_stride + j] + w11 * I1_ptr[(i + 1) * I1_stride + j + 1] -\n                       I0_ptr[i * I0_stride + j];\n                SSD += diff * diff;\n            }\n#if CV_SIMD128\n    }\n#endif\n    return SSD;\n}\n\n/* Same as computeSSD, but with patch mean normalization */\ninline float computeSSDMeanNorm(uchar *I0_ptr, uchar *I1_ptr, int I0_stride, int I1_stride, float w00, float w01,\n                                float w10, float w11, int patch_sz)\n{\n    float sum_diff = 0.0f, sum_diff_sq = 0.0f;\n    float n = (float)patch_sz * patch_sz;\n#if CV_SIMD128\n    if (patch_sz == 8)\n    {\n        v_float32x4 sum_diff_vec = v_setall_f32(0);\n        v_float32x4 sum_diff_sq_vec = v_setall_f32(0);\n        HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION;\n        for (int row = 0; row < 8; row++)\n        {\n            HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION;\n            sum_diff_sq_vec += I_diff_left * I_diff_left + I_diff_right * I_diff_right;\n            sum_diff_vec += I_diff_left + I_diff_right;\n            HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW;\n        }\n        sum_diff = v_reduce_sum(sum_diff_vec);\n        sum_diff_sq = v_reduce_sum(sum_diff_sq_vec);\n    }\n    else\n    {\n#endif\n        float diff;\n        for (int i = 0; i < patch_sz; i++)\n            for (int j = 0; j < patch_sz; j++)\n            {\n                diff = w00 * I1_ptr[i * I1_stride + j] + w01 * I1_ptr[i * I1_stride + j + 1] +\n                       w10 * I1_ptr[(i + 1) * I1_stride + j] + w11 * I1_ptr[(i + 1) * I1_stride + j + 1] -\n                       I0_ptr[i * I0_stride + j];\n\n                sum_diff += diff;\n                sum_diff_sq += diff * diff;\n            }\n#if CV_SIMD128\n    }\n#endif\n    return sum_diff_sq - sum_diff * sum_diff / n;\n}\n\n#undef HAL_INIT_BILINEAR_8x8_PATCH_EXTRACTION\n#undef HAL_PROCESS_BILINEAR_8x8_PATCH_EXTRACTION\n#undef HAL_BILINEAR_8x8_PATCH_EXTRACTION_NEXT_ROW\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\nvoid DISOpticalFlowImpl::PatchInverseSearch_ParBody::operator()(const Range &range) const\n{\n    // force separate processing of stripes if we are using spatial propagation:\n    if (dis->use_spatial_propagation && range.end > range.start + 1)\n    {\n        for (int n = range.start; n < range.end; n++)\n            (*this)(Range(n, n + 1));\n        return;\n    }\n    int psz = dis->patch_size;\n    int psz2 = psz / 2;\n    int w_ext = dis->w + 2 * dis->border_size; //!< width of I1_ext\n    int bsz = dis->border_size;\n\n    /* Input dense flow */\n    float *Ux_ptr = Ux->ptr<float>();\n    float *Uy_ptr = Uy->ptr<float>();\n\n    /* Output sparse flow */\n    float *Sx_ptr = Sx->ptr<float>();\n    float *Sy_ptr = Sy->ptr<float>();\n\n    uchar *I0_ptr = I0->ptr<uchar>();\n    uchar *I1_ptr = I1->ptr<uchar>();\n    short *I0x_ptr = I0x->ptr<short>();\n    short *I0y_ptr = I0y->ptr<short>();\n\n    /* Precomputed structure tensor */\n    float *xx_ptr = dis->I0xx_buf.ptr<float>();\n    float *yy_ptr = dis->I0yy_buf.ptr<float>();\n    float *xy_ptr = dis->I0xy_buf.ptr<float>();\n    /* And extra buffers for mean-normalization: */\n    float *x_ptr = dis->I0x_buf.ptr<float>();\n    float *y_ptr = dis->I0y_buf.ptr<float>();\n\n    bool use_temporal_candidates = false;\n    float *initial_Ux_ptr = NULL, *initial_Uy_ptr = NULL;\n    if (!dis->initial_Ux.empty())\n    {\n        initial_Ux_ptr = dis->initial_Ux[pyr_level].ptr<float>();\n        initial_Uy_ptr = dis->initial_Uy[pyr_level].ptr<float>();\n        use_temporal_candidates = true;\n    }\n\n    int i, j, dir;\n    int start_is, end_is, start_js, end_js;\n    int start_i, start_j;\n    float i_lower_limit = bsz - psz + 1.0f;\n    float i_upper_limit = bsz + dis->h - 1.0f;\n    float j_lower_limit = bsz - psz + 1.0f;\n    float j_upper_limit = bsz + dis->w - 1.0f;\n    float dUx, dUy, i_I1, j_I1, w00, w01, w10, w11, dx, dy;\n\n#define INIT_BILINEAR_WEIGHTS(Ux, Uy)                                                                                  \\\n    i_I1 = min(max(i + Uy + bsz, i_lower_limit), i_upper_limit);                                                       \\\n    j_I1 = min(max(j + Ux + bsz, j_lower_limit), j_upper_limit);                                                       \\\n                                                                                                                       \\\n    w11 = (i_I1 - floor(i_I1)) * (j_I1 - floor(j_I1));                                                                 \\\n    w10 = (i_I1 - floor(i_I1)) * (floor(j_I1) + 1 - j_I1);                                                             \\\n    w01 = (floor(i_I1) + 1 - i_I1) * (j_I1 - floor(j_I1));                                                             \\\n    w00 = (floor(i_I1) + 1 - i_I1) * (floor(j_I1) + 1 - j_I1);\n\n#define COMPUTE_SSD(dst, Ux, Uy)                                                                                       \\\n    INIT_BILINEAR_WEIGHTS(Ux, Uy);                                                                                     \\\n    if (dis->use_mean_normalization)                                                                                   \\\n        dst = computeSSDMeanNorm(I0_ptr + i * dis->w + j, I1_ptr + (int)i_I1 * w_ext + (int)j_I1, dis->w, w_ext, w00,  \\\n                                 w01, w10, w11, psz);                                                                  \\\n    else                                                                                                               \\\n        dst = computeSSD(I0_ptr + i * dis->w + j, I1_ptr + (int)i_I1 * w_ext + (int)j_I1, dis->w, w_ext, w00, w01,     \\\n                         w10, w11, psz);\n\n    int num_inner_iter = (int)floor(dis->grad_descent_iter / (float)num_iter);\n    for (int iter = 0; iter < num_iter; iter++)\n    {\n        if (iter % 2 == 0)\n        {\n            dir = 1;\n            start_is = min(range.start * stripe_sz, hs);\n            end_is = min(range.end * stripe_sz, hs);\n            start_js = 0;\n            end_js = dis->ws;\n            start_i = start_is * dis->patch_stride;\n            start_j = 0;\n        }\n        else\n        {\n            dir = -1;\n            start_is = min(range.end * stripe_sz, hs) - 1;\n            end_is = min(range.start * stripe_sz, hs) - 1;\n            start_js = dis->ws - 1;\n            end_js = -1;\n            start_i = start_is * dis->patch_stride;\n            start_j = (dis->ws - 1) * dis->patch_stride;\n        }\n\n        i = start_i;\n        for (int is = start_is; dir * is < dir * end_is; is += dir)\n        {\n            j = start_j;\n            for (int js = start_js; dir * js < dir * end_js; js += dir)\n            {\n                if (iter == 0)\n                {\n                    /* Using result form the previous pyramid level as the very first approximation: */\n                    Sx_ptr[is * dis->ws + js] = Ux_ptr[(i + psz2) * dis->w + j + psz2];\n                    Sy_ptr[is * dis->ws + js] = Uy_ptr[(i + psz2) * dis->w + j + psz2];\n                }\n\n                float min_SSD = INF, cur_SSD;\n                if (use_temporal_candidates || dis->use_spatial_propagation)\n                {\n                    COMPUTE_SSD(min_SSD, Sx_ptr[is * dis->ws + js], Sy_ptr[is * dis->ws + js]);\n                }\n\n                if (use_temporal_candidates)\n                {\n                    /* Try temporal candidates (vectors from the initial flow field that was passed to the function) */\n                    COMPUTE_SSD(cur_SSD, initial_Ux_ptr[(i + psz2) * dis->w + j + psz2],\n                                initial_Uy_ptr[(i + psz2) * dis->w + j + psz2]);\n                    if (cur_SSD < min_SSD)\n                    {\n                        min_SSD = cur_SSD;\n                        Sx_ptr[is * dis->ws + js] = initial_Ux_ptr[(i + psz2) * dis->w + j + psz2];\n                        Sy_ptr[is * dis->ws + js] = initial_Uy_ptr[(i + psz2) * dis->w + j + psz2];\n                    }\n                }\n\n                if (dis->use_spatial_propagation)\n                {\n                    /* Try spatial candidates: */\n                    if (dir * js > dir * start_js)\n                    {\n                        COMPUTE_SSD(cur_SSD, Sx_ptr[is * dis->ws + js - dir], Sy_ptr[is * dis->ws + js - dir]);\n                        if (cur_SSD < min_SSD)\n                        {\n                            min_SSD = cur_SSD;\n                            Sx_ptr[is * dis->ws + js] = Sx_ptr[is * dis->ws + js - dir];\n                            Sy_ptr[is * dis->ws + js] = Sy_ptr[is * dis->ws + js - dir];\n                        }\n                    }\n                    /* Flow vectors won't actually propagate across different stripes, which is the reason for keeping\n                     * the number of stripes constant. It works well enough in practice and doesn't introduce any\n                     * visible seams.\n                     */\n                    if (dir * is > dir * start_is)\n                    {\n                        COMPUTE_SSD(cur_SSD, Sx_ptr[(is - dir) * dis->ws + js], Sy_ptr[(is - dir) * dis->ws + js]);\n                        if (cur_SSD < min_SSD)\n                        {\n                            min_SSD = cur_SSD;\n                            Sx_ptr[is * dis->ws + js] = Sx_ptr[(is - dir) * dis->ws + js];\n                            Sy_ptr[is * dis->ws + js] = Sy_ptr[(is - dir) * dis->ws + js];\n                        }\n                    }\n                }\n\n                /* Use the best candidate as a starting point for the gradient descent: */\n                float cur_Ux = Sx_ptr[is * dis->ws + js];\n                float cur_Uy = Sy_ptr[is * dis->ws + js];\n\n                /* Computing the inverse of the structure tensor: */\n                float detH = xx_ptr[is * dis->ws + js] * yy_ptr[is * dis->ws + js] -\n                             xy_ptr[is * dis->ws + js] * xy_ptr[is * dis->ws + js];\n                if (abs(detH) < EPS)\n                    detH = EPS;\n                float invH11 = yy_ptr[is * dis->ws + js] / detH;\n                float invH12 = -xy_ptr[is * dis->ws + js] / detH;\n                float invH22 = xx_ptr[is * dis->ws + js] / detH;\n                float prev_SSD = INF, SSD;\n                float x_grad_sum = x_ptr[is * dis->ws + js];\n                float y_grad_sum = y_ptr[is * dis->ws + js];\n\n                for (int t = 0; t < num_inner_iter; t++)\n                {\n                    INIT_BILINEAR_WEIGHTS(cur_Ux, cur_Uy);\n                    if (dis->use_mean_normalization)\n                        SSD = processPatchMeanNorm(dUx, dUy, I0_ptr + i * dis->w + j,\n                                                   I1_ptr + (int)i_I1 * w_ext + (int)j_I1, I0x_ptr + i * dis->w + j,\n                                                   I0y_ptr + i * dis->w + j, dis->w, w_ext, w00, w01, w10, w11, psz,\n                                                   x_grad_sum, y_grad_sum);\n                    else\n                        SSD = processPatch(dUx, dUy, I0_ptr + i * dis->w + j, I1_ptr + (int)i_I1 * w_ext + (int)j_I1,\n                                           I0x_ptr + i * dis->w + j, I0y_ptr + i * dis->w + j, dis->w, w_ext, w00, w01,\n                                           w10, w11, psz);\n\n                    dx = invH11 * dUx + invH12 * dUy;\n                    dy = invH12 * dUx + invH22 * dUy;\n                    cur_Ux -= dx;\n                    cur_Uy -= dy;\n\n                    /* Break when patch distance stops decreasing */\n                    if (SSD >= prev_SSD)\n                        break;\n                    prev_SSD = SSD;\n                }\n\n                /* If gradient descent converged to a flow vector that is very far from the initial approximation\n                 * (more than patch size) then we don't use it. Noticeably improves the robustness.\n                 */\n                if (norm(Vec2f(cur_Ux - Sx_ptr[is * dis->ws + js], cur_Uy - Sy_ptr[is * dis->ws + js])) <= psz)\n                {\n                    Sx_ptr[is * dis->ws + js] = cur_Ux;\n                    Sy_ptr[is * dis->ws + js] = cur_Uy;\n                }\n                j += dir * dis->patch_stride;\n            }\n            i += dir * dis->patch_stride;\n        }\n    }\n#undef INIT_BILINEAR_WEIGHTS\n#undef COMPUTE_SSD\n}\n\nDISOpticalFlowImpl::Densification_ParBody::Densification_ParBody(DISOpticalFlowImpl &_dis, int _nstripes, int _h,\n                                                                 Mat &dst_Ux, Mat &dst_Uy, Mat &src_Sx, Mat &src_Sy,\n                                                                 Mat &_I0, Mat &_I1)\n    : dis(&_dis), nstripes(_nstripes), h(_h), Ux(&dst_Ux), Uy(&dst_Uy), Sx(&src_Sx), Sy(&src_Sy), I0(&_I0), I1(&_I1)\n{\n    stripe_sz = (int)ceil(h / (double)nstripes);\n}\n\n/* This function transforms a sparse optical flow field obtained by PatchInverseSearch (which computes flow values\n * on a sparse grid defined by patch_stride) into a dense optical flow field by weighted averaging of values from the\n * overlapping patches.\n */\nvoid DISOpticalFlowImpl::Densification_ParBody::operator()(const Range &range) const\n{\n    int start_i = min(range.start * stripe_sz, h);\n    int end_i = min(range.end * stripe_sz, h);\n\n    /* Input sparse flow */\n    float *Sx_ptr = Sx->ptr<float>();\n    float *Sy_ptr = Sy->ptr<float>();\n\n    /* Output dense flow */\n    float *Ux_ptr = Ux->ptr<float>();\n    float *Uy_ptr = Uy->ptr<float>();\n\n    uchar *I0_ptr = I0->ptr<uchar>();\n    uchar *I1_ptr = I1->ptr<uchar>();\n\n    int psz = dis->patch_size;\n    int pstr = dis->patch_stride;\n    int i_l, i_u;\n    int j_l, j_u;\n    float i_m, j_m, diff;\n\n    /* These values define the set of sparse grid locations that contain patches overlapping with the current dense flow\n     * location */\n    int start_is, end_is;\n    int start_js, end_js;\n\n/* Some helper macros for updating this set of sparse grid locations */\n#define UPDATE_SPARSE_I_COORDINATES                                                                                    \\\n    if (i % pstr == 0 && i + psz <= h)                                                                                 \\\n        end_is++;                                                                                                      \\\n    if (i - psz >= 0 && (i - psz) % pstr == 0 && start_is < end_is)                                                    \\\n        start_is++;\n\n#define UPDATE_SPARSE_J_COORDINATES                                                                                    \\\n    if (j % pstr == 0 && j + psz <= dis->w)                                                                            \\\n        end_js++;                                                                                                      \\\n    if (j - psz >= 0 && (j - psz) % pstr == 0 && start_js < end_js)                                                    \\\n        start_js++;\n\n    start_is = 0;\n    end_is = -1;\n    for (int i = 0; i < start_i; i++)\n    {\n        UPDATE_SPARSE_I_COORDINATES;\n    }\n    for (int i = start_i; i < end_i; i++)\n    {\n        UPDATE_SPARSE_I_COORDINATES;\n        start_js = 0;\n        end_js = -1;\n        for (int j = 0; j < dis->w; j++)\n        {\n            UPDATE_SPARSE_J_COORDINATES;\n            float coef, sum_coef = 0.0f;\n            float sum_Ux = 0.0f;\n            float sum_Uy = 0.0f;\n\n            /* Iterate through all the patches that overlap the current location (i,j) */\n            for (int is = start_is; is <= end_is; is++)\n                for (int js = start_js; js <= end_js; js++)\n                {\n                    j_m = min(max(j + Sx_ptr[is * dis->ws + js], 0.0f), dis->w - 1.0f - EPS);\n                    i_m = min(max(i + Sy_ptr[is * dis->ws + js], 0.0f), dis->h - 1.0f - EPS);\n                    j_l = (int)j_m;\n                    j_u = j_l + 1;\n                    i_l = (int)i_m;\n                    i_u = i_l + 1;\n                    diff = (j_m - j_l) * (i_m - i_l) * I1_ptr[i_u * dis->w + j_u] +\n                           (j_u - j_m) * (i_m - i_l) * I1_ptr[i_u * dis->w + j_l] +\n                           (j_m - j_l) * (i_u - i_m) * I1_ptr[i_l * dis->w + j_u] +\n                           (j_u - j_m) * (i_u - i_m) * I1_ptr[i_l * dis->w + j_l] - I0_ptr[i * dis->w + j];\n                    coef = 1 / max(1.0f, abs(diff));\n                    sum_Ux += coef * Sx_ptr[is * dis->ws + js];\n                    sum_Uy += coef * Sy_ptr[is * dis->ws + js];\n                    sum_coef += coef;\n                }\n            CV_DbgAssert(sum_coef != 0);\n            Ux_ptr[i * dis->w + j] = sum_Ux / sum_coef;\n            Uy_ptr[i * dis->w + j] = sum_Uy / sum_coef;\n        }\n    }\n#undef UPDATE_SPARSE_I_COORDINATES\n#undef UPDATE_SPARSE_J_COORDINATES\n}\n\n#ifdef HAVE_OPENCL\nbool DISOpticalFlowImpl::ocl_PatchInverseSearch(UMat &src_Ux, UMat &src_Uy,\n                                                UMat &I0, UMat &I1, UMat &I0x, UMat &I0y, int num_iter, int pyr_level)\n{\n    size_t globalSize[] = {(size_t)ws, (size_t)hs};\n    size_t localSize[]  = {16, 16};\n    int idx;\n    int num_inner_iter = (int)floor(grad_descent_iter / (float)num_iter);\n\n    String subgroups_build_options;\n    if (ocl::Device::getDefault().isExtensionSupported(\"cl_khr_subgroups\"))\n        subgroups_build_options = \"-DCV_USE_SUBGROUPS=1\";\n\n\n    for (int iter = 0; iter < num_iter; iter++)\n    {\n        if (iter == 0)\n        {\n            ocl::Kernel k1(\"dis_patch_inverse_search_fwd_1\", ocl::video::dis_flow_oclsrc, subgroups_build_options);\n            size_t global_sz[] = {(size_t)hs * 8};\n            size_t local_sz[]  = {8};\n            idx = 0;\n\n            idx = k1.set(idx, ocl::KernelArg::PtrReadOnly(src_Ux));\n            idx = k1.set(idx, ocl::KernelArg::PtrReadOnly(src_Uy));\n            idx = k1.set(idx, ocl::KernelArg::PtrReadOnly(I0));\n            idx = k1.set(idx, ocl::KernelArg::PtrReadOnly(I1));\n            idx = k1.set(idx, (int)border_size);\n            idx = k1.set(idx, (int)patch_size);\n            idx = k1.set(idx, (int)patch_stride);\n            idx = k1.set(idx, (int)w);\n            idx = k1.set(idx, (int)h);\n            idx = k1.set(idx, (int)ws);\n            idx = k1.set(idx, (int)hs);\n            idx = k1.set(idx, (int)pyr_level);\n            idx = k1.set(idx, ocl::KernelArg::PtrWriteOnly(u_Sx));\n            idx = k1.set(idx, ocl::KernelArg::PtrWriteOnly(u_Sy));\n            if (!k1.run(1, global_sz, local_sz, false))\n                return false;\n\n            ocl::Kernel k2(\"dis_patch_inverse_search_fwd_2\", ocl::video::dis_flow_oclsrc);\n            idx = 0;\n\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(src_Ux));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(src_Uy));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(I0));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(I1));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(I0x));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(I0y));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0xx_buf));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0yy_buf));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0xy_buf));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0x_buf));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadOnly(u_I0y_buf));\n            idx = k2.set(idx, (int)border_size);\n            idx = k2.set(idx, (int)patch_size);\n            idx = k2.set(idx, (int)patch_stride);\n            idx = k2.set(idx, (int)w);\n            idx = k2.set(idx, (int)h);\n            idx = k2.set(idx, (int)ws);\n            idx = k2.set(idx, (int)hs);\n            idx = k2.set(idx, (int)num_inner_iter);\n            idx = k2.set(idx, (int)pyr_level);\n            idx = k2.set(idx, ocl::KernelArg::PtrReadWrite(u_Sx));\n            idx = k2.set(idx, ocl::KernelArg::PtrReadWrite(u_Sy));\n            if (!k2.run(2, globalSize, localSize, false))\n                return false;\n        }\n        else\n        {\n            ocl::Kernel k3(\"dis_patch_inverse_search_bwd_1\", ocl::video::dis_flow_oclsrc, subgroups_build_options);\n            size_t global_sz[] = {(size_t)hs * 8};\n            size_t local_sz[]  = {8};\n            idx = 0;\n\n            idx = k3.set(idx, ocl::KernelArg::PtrReadOnly(I0));\n            idx = k3.set(idx, ocl::KernelArg::PtrReadOnly(I1));\n            idx = k3.set(idx, (int)border_size);\n            idx = k3.set(idx, (int)patch_size);\n            idx = k3.set(idx, (int)patch_stride);\n            idx = k3.set(idx, (int)w);\n            idx = k3.set(idx, (int)h);\n            idx = k3.set(idx, (int)ws);\n            idx = k3.set(idx, (int)hs);\n            idx = k3.set(idx, (int)pyr_level);\n            idx = k3.set(idx, ocl::KernelArg::PtrReadWrite(u_Sx));\n            idx = k3.set(idx, ocl::KernelArg::PtrReadWrite(u_Sy));\n            if (!k3.run(1, global_sz, local_sz, false))\n                return false;\n\n            ocl::Kernel k4(\"dis_patch_inverse_search_bwd_2\", ocl::video::dis_flow_oclsrc);\n            idx = 0;\n\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(I0));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(I1));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(I0x));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(I0y));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0xx_buf));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0yy_buf));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0xy_buf));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0x_buf));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadOnly(u_I0y_buf));\n            idx = k4.set(idx, (int)border_size);\n            idx = k4.set(idx, (int)patch_size);\n            idx = k4.set(idx, (int)patch_stride);\n            idx = k4.set(idx, (int)w);\n            idx = k4.set(idx, (int)h);\n            idx = k4.set(idx, (int)ws);\n            idx = k4.set(idx, (int)hs);\n            idx = k4.set(idx, (int)num_inner_iter);\n            idx = k4.set(idx, ocl::KernelArg::PtrReadWrite(u_Sx));\n            idx = k4.set(idx, ocl::KernelArg::PtrReadWrite(u_Sy));\n            if (!k4.run(2, globalSize, localSize, false))\n                return false;\n        }\n    }\n    return true;\n}\n\nbool DISOpticalFlowImpl::ocl_Densification(UMat &dst_Ux, UMat &dst_Uy, UMat &src_Sx, UMat &src_Sy, UMat &_I0, UMat &_I1)\n{\n    size_t globalSize[] = {(size_t)w, (size_t)h};\n    size_t localSize[]  = {16, 16};\n\n    ocl::Kernel kernel(\"dis_densification\", ocl::video::dis_flow_oclsrc);\n    kernel.args(ocl::KernelArg::PtrReadOnly(src_Sx),\n                ocl::KernelArg::PtrReadOnly(src_Sy),\n                ocl::KernelArg::PtrReadOnly(_I0),\n                ocl::KernelArg::PtrReadOnly(_I1),\n                (int)patch_size, (int)patch_stride,\n                (int)w, (int)h, (int)ws,\n                ocl::KernelArg::PtrWriteOnly(dst_Ux),\n                ocl::KernelArg::PtrWriteOnly(dst_Uy));\n    return kernel.run(2, globalSize, localSize, false);\n}\n\nvoid DISOpticalFlowImpl::ocl_prepareBuffers(UMat &I0, UMat &I1, UMat &flow, bool use_flow)\n{\n    u_I0s.resize(coarsest_scale + 1);\n    u_I1s.resize(coarsest_scale + 1);\n    u_I1s_ext.resize(coarsest_scale + 1);\n    u_I0xs.resize(coarsest_scale + 1);\n    u_I0ys.resize(coarsest_scale + 1);\n    u_Ux.resize(coarsest_scale + 1);\n    u_Uy.resize(coarsest_scale + 1);\n\n    vector<UMat> flow_uv(2);\n    if (use_flow)\n    {\n        split(flow, flow_uv);\n        u_initial_Ux.resize(coarsest_scale + 1);\n        u_initial_Uy.resize(coarsest_scale + 1);\n    }\n\n    int fraction = 1;\n    int cur_rows = 0, cur_cols = 0;\n\n    for (int i = 0; i <= coarsest_scale; i++)\n    {\n        /* Avoid initializing the pyramid levels above the finest scale, as they won't be used anyway */\n        if (i == finest_scale)\n        {\n            cur_rows = I0.rows / fraction;\n            cur_cols = I0.cols / fraction;\n            u_I0s[i].create(cur_rows, cur_cols, CV_8UC1);\n            resize(I0, u_I0s[i], u_I0s[i].size(), 0.0, 0.0, INTER_AREA);\n            u_I1s[i].create(cur_rows, cur_cols, CV_8UC1);\n            resize(I1, u_I1s[i], u_I1s[i].size(), 0.0, 0.0, INTER_AREA);\n\n            /* These buffers are reused in each scale so we initialize them once on the finest scale: */\n            u_Sx.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_Sy.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0xx_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0yy_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0xy_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0x_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n            u_I0y_buf.create(cur_rows / patch_stride, cur_cols / patch_stride, CV_32FC1);\n\n            u_I0xx_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n            u_I0yy_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n            u_I0xy_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n            u_I0x_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n            u_I0y_buf_aux.create(cur_rows, cur_cols / patch_stride, CV_32FC1);\n\n            u_U.create(cur_rows, cur_cols, CV_32FC2);\n        }\n        else if (i > finest_scale)\n        {\n            cur_rows = u_I0s[i - 1].rows / 2;\n            cur_cols = u_I0s[i - 1].cols / 2;\n            u_I0s[i].create(cur_rows, cur_cols, CV_8UC1);\n            resize(u_I0s[i - 1], u_I0s[i], u_I0s[i].size(), 0.0, 0.0, INTER_AREA);\n            u_I1s[i].create(cur_rows, cur_cols, CV_8UC1);\n            resize(u_I1s[i - 1], u_I1s[i], u_I1s[i].size(), 0.0, 0.0, INTER_AREA);\n        }\n\n        if (i >= finest_scale)\n        {\n            u_I1s_ext[i].create(cur_rows + 2 * border_size, cur_cols + 2 * border_size, CV_8UC1);\n            copyMakeBorder(u_I1s[i], u_I1s_ext[i], border_size, border_size, border_size, border_size, BORDER_REPLICATE);\n            u_I0xs[i].create(cur_rows, cur_cols, CV_16SC1);\n            u_I0ys[i].create(cur_rows, cur_cols, CV_16SC1);\n            spatialGradient(u_I0s[i], u_I0xs[i], u_I0ys[i]);\n            u_Ux[i].create(cur_rows, cur_cols, CV_32FC1);\n            u_Uy[i].create(cur_rows, cur_cols, CV_32FC1);\n            variational_refinement_processors[i]->setAlpha(variational_refinement_alpha);\n            variational_refinement_processors[i]->setDelta(variational_refinement_delta);\n            variational_refinement_processors[i]->setGamma(variational_refinement_gamma);\n            variational_refinement_processors[i]->setSorIterations(5);\n            variational_refinement_processors[i]->setFixedPointIterations(variational_refinement_iter);\n\n            if (use_flow)\n            {\n                resize(flow_uv[0], u_initial_Ux[i], Size(cur_cols, cur_rows));\n                divide(u_initial_Ux[i], static_cast<float>(fraction), u_initial_Ux[i]);\n                resize(flow_uv[1], u_initial_Uy[i], Size(cur_cols, cur_rows));\n                divide(u_initial_Uy[i], static_cast<float>(fraction), u_initial_Uy[i]);\n            }\n        }\n\n        fraction *= 2;\n    }\n}\n\nbool DISOpticalFlowImpl::ocl_precomputeStructureTensor(UMat &dst_I0xx, UMat &dst_I0yy, UMat &dst_I0xy,\n                                                       UMat &dst_I0x, UMat &dst_I0y, UMat &I0x, UMat &I0y)\n{\n    size_t globalSizeX[] = {(size_t)h};\n    size_t localSizeX[]  = {16};\n\n    ocl::Kernel kernelX(\"dis_precomputeStructureTensor_hor\", ocl::video::dis_flow_oclsrc);\n    kernelX.args(ocl::KernelArg::PtrReadOnly(I0x),\n                 ocl::KernelArg::PtrReadOnly(I0y),\n                 (int)patch_size, (int)patch_stride,\n                 (int)w, (int)h, (int)ws,\n                 ocl::KernelArg::PtrWriteOnly(u_I0xx_buf_aux),\n                 ocl::KernelArg::PtrWriteOnly(u_I0yy_buf_aux),\n                 ocl::KernelArg::PtrWriteOnly(u_I0xy_buf_aux),\n                 ocl::KernelArg::PtrWriteOnly(u_I0x_buf_aux),\n                 ocl::KernelArg::PtrWriteOnly(u_I0y_buf_aux));\n    if (!kernelX.run(1, globalSizeX, localSizeX, false))\n        return false;\n\n    size_t globalSizeY[] = {(size_t)ws};\n    size_t localSizeY[]  = {16};\n\n    ocl::Kernel kernelY(\"dis_precomputeStructureTensor_ver\", ocl::video::dis_flow_oclsrc);\n    kernelY.args(ocl::KernelArg::PtrReadOnly(u_I0xx_buf_aux),\n                 ocl::KernelArg::PtrReadOnly(u_I0yy_buf_aux),\n                 ocl::KernelArg::PtrReadOnly(u_I0xy_buf_aux),\n                 ocl::KernelArg::PtrReadOnly(u_I0x_buf_aux),\n                 ocl::KernelArg::PtrReadOnly(u_I0y_buf_aux),\n                 (int)patch_size, (int)patch_stride,\n                 (int)w, (int)h, (int)ws,\n                 ocl::KernelArg::PtrWriteOnly(dst_I0xx),\n                 ocl::KernelArg::PtrWriteOnly(dst_I0yy),\n                 ocl::KernelArg::PtrWriteOnly(dst_I0xy),\n                 ocl::KernelArg::PtrWriteOnly(dst_I0x),\n                 ocl::KernelArg::PtrWriteOnly(dst_I0y));\n    return kernelY.run(1, globalSizeY, localSizeY, false);\n}\n\nbool DISOpticalFlowImpl::ocl_calc(InputArray I0, InputArray I1, InputOutputArray flow)\n{\n    UMat I0Mat = I0.getUMat();\n    UMat I1Mat = I1.getUMat();\n    bool use_input_flow = false;\n    if (flow.sameSize(I0) && flow.depth() == CV_32F && flow.channels() == 2)\n        use_input_flow = true;\n    else\n        flow.create(I1Mat.size(), CV_32FC2);\n    UMat &u_flowMat = flow.getUMatRef();\n    coarsest_scale = min((int)(log(max(I0Mat.cols, I0Mat.rows) / (4.0 * patch_size)) / log(2.0) + 0.5), /* Original code search for maximal movement of width/4 */\n                         (int)(log(min(I0Mat.cols, I0Mat.rows) / patch_size) / log(2.0)));              /* Deepest pyramid level greater or equal than patch*/\n\n    if (coarsest_scale<0)\n        CV_Error(cv::Error::StsBadSize, \"The input image must have either width or height >= 12\");\n\n    if (coarsest_scale<finest_scale)\n    {\n        // choose the finest level based on coarsest level.\n        // Refs: https://github.com/tikroeger/OF_DIS/blob/2c9f2a674f3128d3a41c10e41cc9f3a35bb1b523/run_dense.cpp#L239\n        int original_img_width = I0.size().width;\n        autoSelectPatchSizeAndScales(original_img_width);\n    }\n\n    ocl_prepareBuffers(I0Mat, I1Mat, u_flowMat, use_input_flow);\n    u_Ux[coarsest_scale].setTo(0.0f);\n    u_Uy[coarsest_scale].setTo(0.0f);\n\n    for (int i = coarsest_scale; i >= finest_scale; i--)\n    {\n        w = u_I0s[i].cols;\n        h = u_I0s[i].rows;\n        ws = 1 + (w - patch_size) / patch_stride;\n        hs = 1 + (h - patch_size) / patch_stride;\n\n        if (!ocl_precomputeStructureTensor(u_I0xx_buf, u_I0yy_buf, u_I0xy_buf,\n                                           u_I0x_buf, u_I0y_buf, u_I0xs[i], u_I0ys[i]))\n            return false;\n\n        if (!ocl_PatchInverseSearch(u_Ux[i], u_Uy[i], u_I0s[i], u_I1s_ext[i], u_I0xs[i], u_I0ys[i], 2, i))\n            return false;\n\n        if (!ocl_Densification(u_Ux[i], u_Uy[i], u_Sx, u_Sy, u_I0s[i], u_I1s[i]))\n            return false;\n\n        if (variational_refinement_iter > 0)\n            variational_refinement_processors[i]->calcUV(u_I0s[i], u_I1s[i],\n                                                         u_Ux[i].getMat(ACCESS_WRITE), u_Uy[i].getMat(ACCESS_WRITE));\n\n        if (i > finest_scale)\n        {\n            resize(u_Ux[i], u_Ux[i - 1], u_Ux[i - 1].size());\n            resize(u_Uy[i], u_Uy[i - 1], u_Uy[i - 1].size());\n            multiply(u_Ux[i - 1], 2, u_Ux[i - 1]);\n            multiply(u_Uy[i - 1], 2, u_Uy[i - 1]);\n        }\n    }\n    vector<UMat> uxy(2);\n    uxy[0] = u_Ux[finest_scale];\n    uxy[1] = u_Uy[finest_scale];\n    merge(uxy, u_U);\n    resize(u_U, u_flowMat, u_flowMat.size());\n    multiply(u_flowMat, 1 << finest_scale, u_flowMat);\n\n    return true;\n}\n#endif\n\nvoid DISOpticalFlowImpl::calc(InputArray I0, InputArray I1, InputOutputArray flow)\n{\n    CV_Assert(!I0.empty() && I0.depth() == CV_8U && I0.channels() == 1);\n    CV_Assert(!I1.empty() && I1.depth() == CV_8U && I1.channels() == 1);\n    CV_Assert(I0.sameSize(I1));\n    CV_Assert(I0.isContinuous());\n    CV_Assert(I1.isContinuous());\n\n    CV_OCL_RUN(flow.isUMat() &&\n               (patch_size == 8) && (use_spatial_propagation == true),\n               ocl_calc(I0, I1, flow));\n\n    Mat I0Mat = I0.getMat();\n    Mat I1Mat = I1.getMat();\n    bool use_input_flow = false;\n    if (flow.sameSize(I0) && flow.depth() == CV_32F && flow.channels() == 2)\n        use_input_flow = true;\n    else\n        flow.create(I1Mat.size(), CV_32FC2);\n    Mat flowMat = flow.getMat();\n    coarsest_scale = min((int)(log(max(I0Mat.cols, I0Mat.rows) / (4.0 * patch_size)) / log(2.0) + 0.5), /* Original code search for maximal movement of width/4 */\n                         (int)(log(min(I0Mat.cols, I0Mat.rows) / patch_size) / log(2.0)));              /* Deepest pyramid level greater or equal than patch*/\n\n    if (coarsest_scale<0)\n        CV_Error(cv::Error::StsBadSize, \"The input image must have either width or height >= 12\");\n\n    if (coarsest_scale<finest_scale)\n    {\n        // choose the finest level based on coarsest level.\n        // Refs: https://github.com/tikroeger/OF_DIS/blob/2c9f2a674f3128d3a41c10e41cc9f3a35bb1b523/run_dense.cpp#L239\n        int original_img_width = I0.size().width;\n        autoSelectPatchSizeAndScales(original_img_width);\n    }\n\n    int num_stripes = getNumThreads();\n\n    prepareBuffers(I0Mat, I1Mat, flowMat, use_input_flow);\n    Ux[coarsest_scale].setTo(0.0f);\n    Uy[coarsest_scale].setTo(0.0f);\n\n    for (int i = coarsest_scale; i >= finest_scale; i--)\n    {\n        w = I0s[i].cols;\n        h = I0s[i].rows;\n        ws = 1 + (w - patch_size) / patch_stride;\n        hs = 1 + (h - patch_size) / patch_stride;\n\n        precomputeStructureTensor(I0xx_buf, I0yy_buf, I0xy_buf, I0x_buf, I0y_buf, I0xs[i], I0ys[i]);\n        if (use_spatial_propagation)\n        {\n            /* Use a fixed number of stripes regardless the number of threads to make inverse search\n             * with spatial propagation reproducible\n             */\n            parallel_for_(Range(0, 8), PatchInverseSearch_ParBody(*this, 8, hs, Sx, Sy, Ux[i], Uy[i], I0s[i],\n                                                                  I1s_ext[i], I0xs[i], I0ys[i], 2, i));\n        }\n        else\n        {\n            parallel_for_(Range(0, num_stripes),\n                          PatchInverseSearch_ParBody(*this, num_stripes, hs, Sx, Sy, Ux[i], Uy[i], I0s[i], I1s_ext[i],\n                                                     I0xs[i], I0ys[i], 1, i));\n        }\n\n        parallel_for_(Range(0, num_stripes),\n                      Densification_ParBody(*this, num_stripes, I0s[i].rows, Ux[i], Uy[i], Sx, Sy, I0s[i], I1s[i]));\n        if (variational_refinement_iter > 0)\n            variational_refinement_processors[i]->calcUV(I0s[i], I1s[i], Ux[i], Uy[i]);\n\n        if (i > finest_scale)\n        {\n            resize(Ux[i], Ux[i - 1], Ux[i - 1].size());\n            resize(Uy[i], Uy[i - 1], Uy[i - 1].size());\n            Ux[i - 1] *= 2;\n            Uy[i - 1] *= 2;\n        }\n    }\n    Mat uxy[] = {Ux[finest_scale], Uy[finest_scale]};\n    merge(uxy, 2, U);\n    resize(U, flowMat, flowMat.size());\n    flowMat *= 1 << finest_scale;\n}\n\nvoid DISOpticalFlowImpl::collectGarbage()\n{\n    I0s.clear();\n    I1s.clear();\n    I1s_ext.clear();\n    I0xs.clear();\n    I0ys.clear();\n    Ux.clear();\n    Uy.clear();\n    U.release();\n    Sx.release();\n    Sy.release();\n    I0xx_buf.release();\n    I0yy_buf.release();\n    I0xy_buf.release();\n    I0xx_buf_aux.release();\n    I0yy_buf_aux.release();\n    I0xy_buf_aux.release();\n\n#ifdef HAVE_OPENCL\n    u_I0s.clear();\n    u_I1s.clear();\n    u_I1s_ext.clear();\n    u_I0xs.clear();\n    u_I0ys.clear();\n    u_Ux.clear();\n    u_Uy.clear();\n    u_U.release();\n    u_Sx.release();\n    u_Sy.release();\n    u_I0xx_buf.release();\n    u_I0yy_buf.release();\n    u_I0xy_buf.release();\n    u_I0xx_buf_aux.release();\n    u_I0yy_buf_aux.release();\n    u_I0xy_buf_aux.release();\n#endif\n\n    for (int i = finest_scale; i <= coarsest_scale; i++)\n        variational_refinement_processors[i]->collectGarbage();\n    variational_refinement_processors.clear();\n}\n\nPtr<DISOpticalFlow> DISOpticalFlow::create(int preset)\n{\n    Ptr<DISOpticalFlow> dis = makePtr<DISOpticalFlowImpl>();\n    dis->setPatchSize(8);\n    if (preset == DISOpticalFlow::PRESET_ULTRAFAST)\n    {\n        dis->setFinestScale(2);\n        dis->setPatchStride(4);\n        dis->setGradientDescentIterations(12);\n        dis->setVariationalRefinementIterations(0);\n    }\n    else if (preset == DISOpticalFlow::PRESET_FAST)\n    {\n        dis->setFinestScale(2);\n        dis->setPatchStride(4);\n        dis->setGradientDescentIterations(16);\n        dis->setVariationalRefinementIterations(5);\n    }\n    else if (preset == DISOpticalFlow::PRESET_MEDIUM)\n    {\n        dis->setFinestScale(1);\n        dis->setPatchStride(3);\n        dis->setGradientDescentIterations(25);\n        dis->setVariationalRefinementIterations(5);\n    }\n\n    return dis;\n}\n}\n", "/*M///////////////////////////////////////////////////////////////////////////////////////\n//\n//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.\n//\n//  By downloading, copying, installing or using the software you agree to this license.\n//  If you do not agree to this license, do not download, install,\n//  copy or use the software.\n//\n//\n//                        Intel License Agreement\n//                For Open Source Computer Vision Library\n//\n// Copyright (C) 2000, Intel Corporation, all rights reserved.\n// Third party copyrights are property of their respective owners.\n//\n// Redistribution and use in source and binary forms, with or without modification,\n// are permitted provided that the following conditions are met:\n//\n//   * Redistribution's of source code must retain the above copyright notice,\n//     this list of conditions and the following disclaimer.\n//\n//   * Redistribution's in binary form must reproduce the above copyright notice,\n//     this list of conditions and the following disclaimer in the documentation\n//     and/or other materials provided with the distribution.\n//\n//   * The name of Intel Corporation may not be used to endorse or promote products\n//     derived from this software without specific prior written permission.\n//\n// This software is provided by the copyright holders and contributors \"as is\" and\n// any express or implied warranties, including, but not limited to, the implied\n// warranties of merchantability and fitness for a particular purpose are disclaimed.\n// In no event shall the Intel Corporation or contributors be liable for any direct,\n// indirect, incidental, special, exemplary, or consequential damages\n// (including, but not limited to, procurement of substitute goods or services;\n// loss of use, data, or profits; or business interruption) however caused\n// and on any theory of liability, whether in contract, strict liability,\n// or tort (including negligence or otherwise) arising in any way out of\n// the use of this software, even if advised of the possibility of such damage.\n//\n//M*/\n\n#include \"test_precomp.hpp\"\n\nnamespace opencv_test { namespace {\n\nstatic string getDataDir() { return TS::ptr()->get_data_path(); }\n\nstatic string getRubberWhaleFrame1() { return getDataDir() + \"optflow/RubberWhale1.png\"; }\n\nstatic string getRubberWhaleFrame2() { return getDataDir() + \"optflow/RubberWhale2.png\"; }\n\nstatic string getRubberWhaleGroundTruth() { return getDataDir() + \"optflow/RubberWhale.flo\"; }\n\nstatic bool isFlowCorrect(float u) { return !cvIsNaN(u) && (fabs(u) < 1e9); }\n\nstatic float calcRMSE(Mat flow1, Mat flow2)\n{\n    float sum = 0;\n    int counter = 0;\n    const int rows = flow1.rows;\n    const int cols = flow1.cols;\n\n    for (int y = 0; y < rows; ++y)\n    {\n        for (int x = 0; x < cols; ++x)\n        {\n            Vec2f flow1_at_point = flow1.at<Vec2f>(y, x);\n            Vec2f flow2_at_point = flow2.at<Vec2f>(y, x);\n\n            float u1 = flow1_at_point[0];\n            float v1 = flow1_at_point[1];\n            float u2 = flow2_at_point[0];\n            float v2 = flow2_at_point[1];\n\n            if (isFlowCorrect(u1) && isFlowCorrect(u2) && isFlowCorrect(v1) && isFlowCorrect(v2))\n            {\n                sum += (u1 - u2) * (u1 - u2) + (v1 - v2) * (v1 - v2);\n                counter++;\n            }\n        }\n    }\n    return (float)sqrt(sum / (1e-9 + counter));\n}\n\nbool readRubberWhale(Mat &dst_frame_1, Mat &dst_frame_2, Mat &dst_GT)\n{\n    const string frame1_path = getRubberWhaleFrame1();\n    const string frame2_path = getRubberWhaleFrame2();\n    const string gt_flow_path = getRubberWhaleGroundTruth();\n\n    dst_frame_1 = imread(frame1_path);\n    dst_frame_2 = imread(frame2_path);\n    dst_GT = readOpticalFlow(gt_flow_path);\n\n    if (dst_frame_1.empty() || dst_frame_2.empty() || dst_GT.empty())\n        return false;\n    else\n        return true;\n}\n\nTEST(DenseOpticalFlow_DIS, ReferenceAccuracy)\n{\n    Mat frame1, frame2, GT;\n    ASSERT_TRUE(readRubberWhale(frame1, frame2, GT));\n    int presets[] = {DISOpticalFlow::PRESET_ULTRAFAST, DISOpticalFlow::PRESET_FAST, DISOpticalFlow::PRESET_MEDIUM};\n    float target_RMSE[] = {0.86f, 0.74f, 0.49f};\n    cvtColor(frame1, frame1, COLOR_BGR2GRAY);\n    cvtColor(frame2, frame2, COLOR_BGR2GRAY);\n\n    Ptr<DenseOpticalFlow> algo;\n\n    // iterate over presets:\n    for (int i = 0; i < 3; i++)\n    {\n        Mat flow;\n        algo = DISOpticalFlow::create(presets[i]);\n        algo->calc(frame1, frame2, flow);\n        ASSERT_EQ(GT.rows, flow.rows);\n        ASSERT_EQ(GT.cols, flow.cols);\n        EXPECT_LE(calcRMSE(GT, flow), target_RMSE[i]);\n    }\n}\n\nTEST(DenseOpticalFlow_DIS, InvalidImgSize_CoarsestLevelLessThanZero)\n{\n    cv::Ptr<cv::DISOpticalFlow> of = cv::DISOpticalFlow::create();\n    const int mat_size = 10;\n\n    cv::Mat x(mat_size, mat_size, CV_8UC1, 42);\n    cv::Mat y(mat_size, mat_size, CV_8UC1, 42);\n    cv::Mat flow;\n\n    ASSERT_THROW(of->calc(x, y, flow), cv::Exception);\n}\n\n// make sure that autoSelectPatchSizeAndScales() works properly.\nTEST(DenseOpticalFlow_DIS, InvalidImgSize_CoarsestLevelLessThanFinestLevel)\n{\n    cv::Ptr<cv::DISOpticalFlow> of = cv::DISOpticalFlow::create();\n    const int mat_size = 80;\n\n    cv::Mat x(mat_size, mat_size, CV_8UC1, 42);\n    cv::Mat y(mat_size, mat_size, CV_8UC1, 42);\n    cv::Mat flow;\n\n    of->calc(x, y, flow);\n\n    ASSERT_EQ(flow.rows, mat_size);\n    ASSERT_EQ(flow.cols, mat_size);\n}\n\nTEST(DenseOpticalFlow_VariationalRefinement, ReferenceAccuracy)\n{\n    Mat frame1, frame2, GT;\n    ASSERT_TRUE(readRubberWhale(frame1, frame2, GT));\n    float target_RMSE = 0.86f;\n    cvtColor(frame1, frame1, COLOR_BGR2GRAY);\n    cvtColor(frame2, frame2, COLOR_BGR2GRAY);\n\n    Ptr<VariationalRefinement> var_ref;\n    var_ref = VariationalRefinement::create();\n    var_ref->setAlpha(20.0f);\n    var_ref->setDelta(5.0f);\n    var_ref->setGamma(10.0f);\n    var_ref->setSorIterations(25);\n    var_ref->setFixedPointIterations(25);\n    Mat flow(frame1.size(), CV_32FC2);\n    flow.setTo(0.0f);\n    var_ref->calc(frame1, frame2, flow);\n    ASSERT_EQ(GT.rows, flow.rows);\n    ASSERT_EQ(GT.cols, flow.cols);\n    EXPECT_LE(calcRMSE(GT, flow), target_RMSE);\n}\n\n}} // namespace\n"], "filenames": ["modules/video/src/dis_flow.cpp", "modules/video/test/test_OF_accuracy.cpp"], "buggy_code_start_loc": [142, 123], "buggy_code_end_loc": [1389, 123], "fixing_code_start_loc": [143, 124], "fixing_code_end_loc": [1453, 152], "type": "CWE-125", "message": "An out-of-bounds read was discovered in OpenCV before 4.1.1. Specifically, variable coarsest_scale is assumed to be greater than or equal to finest_scale within the calc()/ocl_calc() functions in dis_flow.cpp. However, this is not true when dealing with small images, leading to an out-of-bounds read of the heap-allocated arrays Ux and Uy.", "other": {"cve": {"id": "CVE-2019-19624", "sourceIdentifier": "cve@mitre.org", "published": "2019-12-06T15:15:10.330", "lastModified": "2019-12-17T20:48:25.560", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An out-of-bounds read was discovered in OpenCV before 4.1.1. Specifically, variable coarsest_scale is assumed to be greater than or equal to finest_scale within the calc()/ocl_calc() functions in dis_flow.cpp. However, this is not true when dealing with small images, leading to an out-of-bounds read of the heap-allocated arrays Ux and Uy."}, {"lang": "es", "value": "Se detect\u00f3 una lectura fuera de l\u00edmites en OpenCV versiones anteriores a 4.1.1. Espec\u00edficamente, una variable coarsest_scale es asumida para ser mayor o igual que finest_scale dentro de las funciones calc() y ocl_calc() en el archivo dis_flow.cpp. Sin embargo, esto no es cierto cuando se trata de im\u00e1genes peque\u00f1as, conllevando a una lectura fuera de l\u00edmites de las matrices Ux y Uy asignadas de la pila."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 2.5}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 6.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:opencv:opencv:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.1.1", "matchCriteriaId": "B00DAA9A-1738-491E-9220-BFFB8580CAB4"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}]}]}], "references": [{"url": "https://access.redhat.com/security/cve/cve-2019-19624", "source": "nvd@nist.gov", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/opencv/opencv/commit/d1615ba11a93062b1429fce9f0f638d1572d3418", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/opencv/opencv/issues/14554", "source": "cve@mitre.org", "tags": ["Exploit", "Issue Tracking", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/opencv/opencv/commit/d1615ba11a93062b1429fce9f0f638d1572d3418"}}