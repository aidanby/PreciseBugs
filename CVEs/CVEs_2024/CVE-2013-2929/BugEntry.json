{"buggy_code": ["#ifndef _ASM_IA64_PROCESSOR_H\n#define _ASM_IA64_PROCESSOR_H\n\n/*\n * Copyright (C) 1998-2004 Hewlett-Packard Co\n *\tDavid Mosberger-Tang <davidm@hpl.hp.com>\n *\tStephane Eranian <eranian@hpl.hp.com>\n * Copyright (C) 1999 Asit Mallick <asit.k.mallick@intel.com>\n * Copyright (C) 1999 Don Dugger <don.dugger@intel.com>\n *\n * 11/24/98\tS.Eranian\tadded ia64_set_iva()\n * 12/03/99\tD. Mosberger\timplement thread_saved_pc() via kernel unwind API\n * 06/16/00\tA. Mallick\tadded csd/ssd/tssd for ia32 support\n */\n\n\n#include <asm/intrinsics.h>\n#include <asm/kregs.h>\n#include <asm/ptrace.h>\n#include <asm/ustack.h>\n\n#define __ARCH_WANT_UNLOCKED_CTXSW\n#define ARCH_HAS_PREFETCH_SWITCH_STACK\n\n#define IA64_NUM_PHYS_STACK_REG\t96\n#define IA64_NUM_DBG_REGS\t8\n\n#define DEFAULT_MAP_BASE\t__IA64_UL_CONST(0x2000000000000000)\n#define DEFAULT_TASK_SIZE\t__IA64_UL_CONST(0xa000000000000000)\n\n/*\n * TASK_SIZE really is a mis-named.  It really is the maximum user\n * space address (plus one).  On IA-64, there are five regions of 2TB\n * each (assuming 8KB page size), for a total of 8TB of user virtual\n * address space.\n */\n#define TASK_SIZE       \tDEFAULT_TASK_SIZE\n\n/*\n * This decides where the kernel will search for a free chunk of vm\n * space during mmap's.\n */\n#define TASK_UNMAPPED_BASE\t(current->thread.map_base)\n\n#define IA64_THREAD_FPH_VALID\t(__IA64_UL(1) << 0)\t/* floating-point high state valid? */\n#define IA64_THREAD_DBG_VALID\t(__IA64_UL(1) << 1)\t/* debug registers valid? */\n#define IA64_THREAD_PM_VALID\t(__IA64_UL(1) << 2)\t/* performance registers valid? */\n#define IA64_THREAD_UAC_NOPRINT\t(__IA64_UL(1) << 3)\t/* don't log unaligned accesses */\n#define IA64_THREAD_UAC_SIGBUS\t(__IA64_UL(1) << 4)\t/* generate SIGBUS on unaligned acc. */\n#define IA64_THREAD_MIGRATION\t(__IA64_UL(1) << 5)\t/* require migration\n\t\t\t\t\t\t\t   sync at ctx sw */\n#define IA64_THREAD_FPEMU_NOPRINT (__IA64_UL(1) << 6)\t/* don't log any fpswa faults */\n#define IA64_THREAD_FPEMU_SIGFPE  (__IA64_UL(1) << 7)\t/* send a SIGFPE for fpswa faults */\n\n#define IA64_THREAD_UAC_SHIFT\t3\n#define IA64_THREAD_UAC_MASK\t(IA64_THREAD_UAC_NOPRINT | IA64_THREAD_UAC_SIGBUS)\n#define IA64_THREAD_FPEMU_SHIFT\t6\n#define IA64_THREAD_FPEMU_MASK\t(IA64_THREAD_FPEMU_NOPRINT | IA64_THREAD_FPEMU_SIGFPE)\n\n\n/*\n * This shift should be large enough to be able to represent 1000000000/itc_freq with good\n * accuracy while being small enough to fit 10*1000000000<<IA64_NSEC_PER_CYC_SHIFT in 64 bits\n * (this will give enough slack to represent 10 seconds worth of time as a scaled number).\n */\n#define IA64_NSEC_PER_CYC_SHIFT\t30\n\n#ifndef __ASSEMBLY__\n\n#include <linux/cache.h>\n#include <linux/compiler.h>\n#include <linux/threads.h>\n#include <linux/types.h>\n\n#include <asm/fpu.h>\n#include <asm/page.h>\n#include <asm/percpu.h>\n#include <asm/rse.h>\n#include <asm/unwind.h>\n#include <linux/atomic.h>\n#ifdef CONFIG_NUMA\n#include <asm/nodedata.h>\n#endif\n\n/* like above but expressed as bitfields for more efficient access: */\nstruct ia64_psr {\n\t__u64 reserved0 : 1;\n\t__u64 be : 1;\n\t__u64 up : 1;\n\t__u64 ac : 1;\n\t__u64 mfl : 1;\n\t__u64 mfh : 1;\n\t__u64 reserved1 : 7;\n\t__u64 ic : 1;\n\t__u64 i : 1;\n\t__u64 pk : 1;\n\t__u64 reserved2 : 1;\n\t__u64 dt : 1;\n\t__u64 dfl : 1;\n\t__u64 dfh : 1;\n\t__u64 sp : 1;\n\t__u64 pp : 1;\n\t__u64 di : 1;\n\t__u64 si : 1;\n\t__u64 db : 1;\n\t__u64 lp : 1;\n\t__u64 tb : 1;\n\t__u64 rt : 1;\n\t__u64 reserved3 : 4;\n\t__u64 cpl : 2;\n\t__u64 is : 1;\n\t__u64 mc : 1;\n\t__u64 it : 1;\n\t__u64 id : 1;\n\t__u64 da : 1;\n\t__u64 dd : 1;\n\t__u64 ss : 1;\n\t__u64 ri : 2;\n\t__u64 ed : 1;\n\t__u64 bn : 1;\n\t__u64 reserved4 : 19;\n};\n\nunion ia64_isr {\n\t__u64  val;\n\tstruct {\n\t\t__u64 code : 16;\n\t\t__u64 vector : 8;\n\t\t__u64 reserved1 : 8;\n\t\t__u64 x : 1;\n\t\t__u64 w : 1;\n\t\t__u64 r : 1;\n\t\t__u64 na : 1;\n\t\t__u64 sp : 1;\n\t\t__u64 rs : 1;\n\t\t__u64 ir : 1;\n\t\t__u64 ni : 1;\n\t\t__u64 so : 1;\n\t\t__u64 ei : 2;\n\t\t__u64 ed : 1;\n\t\t__u64 reserved2 : 20;\n\t};\n};\n\nunion ia64_lid {\n\t__u64 val;\n\tstruct {\n\t\t__u64  rv  : 16;\n\t\t__u64  eid : 8;\n\t\t__u64  id  : 8;\n\t\t__u64  ig  : 32;\n\t};\n};\n\nunion ia64_tpr {\n\t__u64 val;\n\tstruct {\n\t\t__u64 ig0 : 4;\n\t\t__u64 mic : 4;\n\t\t__u64 rsv : 8;\n\t\t__u64 mmi : 1;\n\t\t__u64 ig1 : 47;\n\t};\n};\n\nunion ia64_itir {\n\t__u64 val;\n\tstruct {\n\t\t__u64 rv3  :  2; /* 0-1 */\n\t\t__u64 ps   :  6; /* 2-7 */\n\t\t__u64 key  : 24; /* 8-31 */\n\t\t__u64 rv4  : 32; /* 32-63 */\n\t};\n};\n\nunion  ia64_rr {\n\t__u64 val;\n\tstruct {\n\t\t__u64  ve\t:  1;  /* enable hw walker */\n\t\t__u64  reserved0:  1;  /* reserved */\n\t\t__u64  ps\t:  6;  /* log page size */\n\t\t__u64  rid\t: 24;  /* region id */\n\t\t__u64  reserved1: 32;  /* reserved */\n\t};\n};\n\n/*\n * CPU type, hardware bug flags, and per-CPU state.  Frequently used\n * state comes earlier:\n */\nstruct cpuinfo_ia64 {\n\tunsigned int softirq_pending;\n\tunsigned long itm_delta;\t/* # of clock cycles between clock ticks */\n\tunsigned long itm_next;\t\t/* interval timer mask value to use for next clock tick */\n\tunsigned long nsec_per_cyc;\t/* (1000000000<<IA64_NSEC_PER_CYC_SHIFT)/itc_freq */\n\tunsigned long unimpl_va_mask;\t/* mask of unimplemented virtual address bits (from PAL) */\n\tunsigned long unimpl_pa_mask;\t/* mask of unimplemented physical address bits (from PAL) */\n\tunsigned long itc_freq;\t\t/* frequency of ITC counter */\n\tunsigned long proc_freq;\t/* frequency of processor */\n\tunsigned long cyc_per_usec;\t/* itc_freq/1000000 */\n\tunsigned long ptce_base;\n\tunsigned int ptce_count[2];\n\tunsigned int ptce_stride[2];\n\tstruct task_struct *ksoftirqd;\t/* kernel softirq daemon for this CPU */\n\n#ifdef CONFIG_SMP\n\tunsigned long loops_per_jiffy;\n\tint cpu;\n\tunsigned int socket_id;\t/* physical processor socket id */\n\tunsigned short core_id;\t/* core id */\n\tunsigned short thread_id; /* thread id */\n\tunsigned short num_log;\t/* Total number of logical processors on\n\t\t\t\t * this socket that were successfully booted */\n\tunsigned char cores_per_socket;\t/* Cores per processor socket */\n\tunsigned char threads_per_core;\t/* Threads per core */\n#endif\n\n\t/* CPUID-derived information: */\n\tunsigned long ppn;\n\tunsigned long features;\n\tunsigned char number;\n\tunsigned char revision;\n\tunsigned char model;\n\tunsigned char family;\n\tunsigned char archrev;\n\tchar vendor[16];\n\tchar *model_name;\n\n#ifdef CONFIG_NUMA\n\tstruct ia64_node_data *node_data;\n#endif\n};\n\nDECLARE_PER_CPU(struct cpuinfo_ia64, ia64_cpu_info);\n\n/*\n * The \"local\" data variable.  It refers to the per-CPU data of the currently executing\n * CPU, much like \"current\" points to the per-task data of the currently executing task.\n * Do not use the address of local_cpu_data, since it will be different from\n * cpu_data(smp_processor_id())!\n */\n#define local_cpu_data\t\t(&__ia64_per_cpu_var(ia64_cpu_info))\n#define cpu_data(cpu)\t\t(&per_cpu(ia64_cpu_info, cpu))\n\nextern void print_cpu_info (struct cpuinfo_ia64 *);\n\ntypedef struct {\n\tunsigned long seg;\n} mm_segment_t;\n\n#define SET_UNALIGN_CTL(task,value)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\t(task)->thread.flags = (((task)->thread.flags & ~IA64_THREAD_UAC_MASK)\t\t\t\\\n\t\t\t\t| (((value) << IA64_THREAD_UAC_SHIFT) & IA64_THREAD_UAC_MASK));\t\\\n\t0;\t\t\t\t\t\t\t\t\t\t\t\\\n})\n#define GET_UNALIGN_CTL(task,addr)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\tput_user(((task)->thread.flags & IA64_THREAD_UAC_MASK) >> IA64_THREAD_UAC_SHIFT,\t\\\n\t\t (int __user *) (addr));\t\t\t\t\t\t\t\\\n})\n\n#define SET_FPEMU_CTL(task,value)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\t(task)->thread.flags = (((task)->thread.flags & ~IA64_THREAD_FPEMU_MASK)\t\t\\\n\t\t\t  | (((value) << IA64_THREAD_FPEMU_SHIFT) & IA64_THREAD_FPEMU_MASK));\t\\\n\t0;\t\t\t\t\t\t\t\t\t\t\t\\\n})\n#define GET_FPEMU_CTL(task,addr)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\tput_user(((task)->thread.flags & IA64_THREAD_FPEMU_MASK) >> IA64_THREAD_FPEMU_SHIFT,\t\\\n\t\t (int __user *) (addr));\t\t\t\t\t\t\t\\\n})\n\nstruct thread_struct {\n\t__u32 flags;\t\t\t/* various thread flags (see IA64_THREAD_*) */\n\t/* writing on_ustack is performance-critical, so it's worth spending 8 bits on it... */\n\t__u8 on_ustack;\t\t\t/* executing on user-stacks? */\n\t__u8 pad[3];\n\t__u64 ksp;\t\t\t/* kernel stack pointer */\n\t__u64 map_base;\t\t\t/* base address for get_unmapped_area() */\n\t__u64 rbs_bot;\t\t\t/* the base address for the RBS */\n\tint last_fph_cpu;\t\t/* CPU that may hold the contents of f32-f127 */\n\n#ifdef CONFIG_PERFMON\n\tvoid *pfm_context;\t\t     /* pointer to detailed PMU context */\n\tunsigned long pfm_needs_checking;    /* when >0, pending perfmon work on kernel exit */\n# define INIT_THREAD_PM\t\t.pfm_context =\t\tNULL,     \\\n\t\t\t\t.pfm_needs_checking =\t0UL,\n#else\n# define INIT_THREAD_PM\n#endif\n\tunsigned long dbr[IA64_NUM_DBG_REGS];\n\tunsigned long ibr[IA64_NUM_DBG_REGS];\n\tstruct ia64_fpreg fph[96];\t/* saved/loaded on demand */\n};\n\n#define INIT_THREAD {\t\t\t\t\t\t\\\n\t.flags =\t0,\t\t\t\t\t\\\n\t.on_ustack =\t0,\t\t\t\t\t\\\n\t.ksp =\t\t0,\t\t\t\t\t\\\n\t.map_base =\tDEFAULT_MAP_BASE,\t\t\t\\\n\t.rbs_bot =\tSTACK_TOP - DEFAULT_USER_STACK_SIZE,\t\\\n\t.last_fph_cpu =  -1,\t\t\t\t\t\\\n\tINIT_THREAD_PM\t\t\t\t\t\t\\\n\t.dbr =\t\t{0, },\t\t\t\t\t\\\n\t.ibr =\t\t{0, },\t\t\t\t\t\\\n\t.fph =\t\t{{{{0}}}, }\t\t\t\t\\\n}\n\n#define start_thread(regs,new_ip,new_sp) do {\t\t\t\t\t\t\t\\\n\tregs->cr_ipsr = ((regs->cr_ipsr | (IA64_PSR_BITS_TO_SET | IA64_PSR_CPL))\t\t\\\n\t\t\t & ~(IA64_PSR_BITS_TO_CLEAR | IA64_PSR_RI | IA64_PSR_IS));\t\t\\\n\tregs->cr_iip = new_ip;\t\t\t\t\t\t\t\t\t\\\n\tregs->ar_rsc = 0xf;\t\t/* eager mode, privilege level 3 */\t\t\t\\\n\tregs->ar_rnat = 0;\t\t\t\t\t\t\t\t\t\\\n\tregs->ar_bspstore = current->thread.rbs_bot;\t\t\t\t\t\t\\\n\tregs->ar_fpsr = FPSR_DEFAULT;\t\t\t\t\t\t\t\t\\\n\tregs->loadrs = 0;\t\t\t\t\t\t\t\t\t\\\n\tregs->r8 = get_dumpable(current->mm);\t/* set \"don't zap registers\" flag */\t\t\\\n\tregs->r12 = new_sp - 16;\t/* allocate 16 byte scratch area */\t\t\t\\\n\tif (unlikely(!get_dumpable(current->mm))) {\t\t\t\t\t\t\t\\\n\t\t/*\t\t\t\t\t\t\t\t\t\t\\\n\t\t * Zap scratch regs to avoid leaking bits between processes with different\t\\\n\t\t * uid/privileges.\t\t\t\t\t\t\t\t\\\n\t\t */\t\t\t\t\t\t\t\t\t\t\\\n\t\tregs->ar_pfs = 0; regs->b0 = 0; regs->pr = 0;\t\t\t\t\t\\\n\t\tregs->r1 = 0; regs->r9  = 0; regs->r11 = 0; regs->r13 = 0; regs->r15 = 0;\t\\\n\t}\t\t\t\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/* Forward declarations, a strange C thing... */\nstruct mm_struct;\nstruct task_struct;\n\n/*\n * Free all resources held by a thread. This is called after the\n * parent of DEAD_TASK has collected the exit status of the task via\n * wait().\n */\n#define release_thread(dead_task)\n\n/* Get wait channel for task P.  */\nextern unsigned long get_wchan (struct task_struct *p);\n\n/* Return instruction pointer of blocked task TSK.  */\n#define KSTK_EIP(tsk)\t\t\t\t\t\\\n  ({\t\t\t\t\t\t\t\\\n\tstruct pt_regs *_regs = task_pt_regs(tsk);\t\\\n\t_regs->cr_iip + ia64_psr(_regs)->ri;\t\t\\\n  })\n\n/* Return stack pointer of blocked task TSK.  */\n#define KSTK_ESP(tsk)  ((tsk)->thread.ksp)\n\nextern void ia64_getreg_unknown_kr (void);\nextern void ia64_setreg_unknown_kr (void);\n\n#define ia64_get_kr(regnum)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tunsigned long r = 0;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tswitch (regnum) {\t\t\t\t\t\\\n\t    case 0: r = ia64_getreg(_IA64_REG_AR_KR0); break;\t\\\n\t    case 1: r = ia64_getreg(_IA64_REG_AR_KR1); break;\t\\\n\t    case 2: r = ia64_getreg(_IA64_REG_AR_KR2); break;\t\\\n\t    case 3: r = ia64_getreg(_IA64_REG_AR_KR3); break;\t\\\n\t    case 4: r = ia64_getreg(_IA64_REG_AR_KR4); break;\t\\\n\t    case 5: r = ia64_getreg(_IA64_REG_AR_KR5); break;\t\\\n\t    case 6: r = ia64_getreg(_IA64_REG_AR_KR6); break;\t\\\n\t    case 7: r = ia64_getreg(_IA64_REG_AR_KR7); break;\t\\\n\t    default: ia64_getreg_unknown_kr(); break;\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tr;\t\t\t\t\t\t\t\\\n})\n\n#define ia64_set_kr(regnum, r) \t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tswitch (regnum) {\t\t\t\t\t\\\n\t    case 0: ia64_setreg(_IA64_REG_AR_KR0, r); break;\t\\\n\t    case 1: ia64_setreg(_IA64_REG_AR_KR1, r); break;\t\\\n\t    case 2: ia64_setreg(_IA64_REG_AR_KR2, r); break;\t\\\n\t    case 3: ia64_setreg(_IA64_REG_AR_KR3, r); break;\t\\\n\t    case 4: ia64_setreg(_IA64_REG_AR_KR4, r); break;\t\\\n\t    case 5: ia64_setreg(_IA64_REG_AR_KR5, r); break;\t\\\n\t    case 6: ia64_setreg(_IA64_REG_AR_KR6, r); break;\t\\\n\t    case 7: ia64_setreg(_IA64_REG_AR_KR7, r); break;\t\\\n\t    default: ia64_setreg_unknown_kr(); break;\t\t\\\n\t}\t\t\t\t\t\t\t\\\n})\n\n/*\n * The following three macros can't be inline functions because we don't have struct\n * task_struct at this point.\n */\n\n/*\n * Return TRUE if task T owns the fph partition of the CPU we're running on.\n * Must be called from code that has preemption disabled.\n */\n#define ia64_is_local_fpu_owner(t)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\tstruct task_struct *__ia64_islfo_task = (t);\t\t\t\t\t\t\\\n\t(__ia64_islfo_task->thread.last_fph_cpu == smp_processor_id()\t\t\t\t\\\n\t && __ia64_islfo_task == (struct task_struct *) ia64_get_kr(IA64_KR_FPU_OWNER));\t\\\n})\n\n/*\n * Mark task T as owning the fph partition of the CPU we're running on.\n * Must be called from code that has preemption disabled.\n */\n#define ia64_set_local_fpu_owner(t) do {\t\t\t\t\t\t\\\n\tstruct task_struct *__ia64_slfo_task = (t);\t\t\t\t\t\\\n\t__ia64_slfo_task->thread.last_fph_cpu = smp_processor_id();\t\t\t\\\n\tia64_set_kr(IA64_KR_FPU_OWNER, (unsigned long) __ia64_slfo_task);\t\t\\\n} while (0)\n\n/* Mark the fph partition of task T as being invalid on all CPUs.  */\n#define ia64_drop_fpu(t)\t((t)->thread.last_fph_cpu = -1)\n\nextern void __ia64_init_fpu (void);\nextern void __ia64_save_fpu (struct ia64_fpreg *fph);\nextern void __ia64_load_fpu (struct ia64_fpreg *fph);\nextern void ia64_save_debug_regs (unsigned long *save_area);\nextern void ia64_load_debug_regs (unsigned long *save_area);\n\n#define ia64_fph_enable()\tdo { ia64_rsm(IA64_PSR_DFH); ia64_srlz_d(); } while (0)\n#define ia64_fph_disable()\tdo { ia64_ssm(IA64_PSR_DFH); ia64_srlz_d(); } while (0)\n\n/* load fp 0.0 into fph */\nstatic inline void\nia64_init_fpu (void) {\n\tia64_fph_enable();\n\t__ia64_init_fpu();\n\tia64_fph_disable();\n}\n\n/* save f32-f127 at FPH */\nstatic inline void\nia64_save_fpu (struct ia64_fpreg *fph) {\n\tia64_fph_enable();\n\t__ia64_save_fpu(fph);\n\tia64_fph_disable();\n}\n\n/* load f32-f127 from FPH */\nstatic inline void\nia64_load_fpu (struct ia64_fpreg *fph) {\n\tia64_fph_enable();\n\t__ia64_load_fpu(fph);\n\tia64_fph_disable();\n}\n\nstatic inline __u64\nia64_clear_ic (void)\n{\n\t__u64 psr;\n\tpsr = ia64_getreg(_IA64_REG_PSR);\n\tia64_stop();\n\tia64_rsm(IA64_PSR_I | IA64_PSR_IC);\n\tia64_srlz_i();\n\treturn psr;\n}\n\n/*\n * Restore the psr.\n */\nstatic inline void\nia64_set_psr (__u64 psr)\n{\n\tia64_stop();\n\tia64_setreg(_IA64_REG_PSR_L, psr);\n\tia64_srlz_i();\n}\n\n/*\n * Insert a translation into an instruction and/or data translation\n * register.\n */\nstatic inline void\nia64_itr (__u64 target_mask, __u64 tr_num,\n\t  __u64 vmaddr, __u64 pte,\n\t  __u64 log_page_size)\n{\n\tia64_setreg(_IA64_REG_CR_ITIR, (log_page_size << 2));\n\tia64_setreg(_IA64_REG_CR_IFA, vmaddr);\n\tia64_stop();\n\tif (target_mask & 0x1)\n\t\tia64_itri(tr_num, pte);\n\tif (target_mask & 0x2)\n\t\tia64_itrd(tr_num, pte);\n}\n\n/*\n * Insert a translation into the instruction and/or data translation\n * cache.\n */\nstatic inline void\nia64_itc (__u64 target_mask, __u64 vmaddr, __u64 pte,\n\t  __u64 log_page_size)\n{\n\tia64_setreg(_IA64_REG_CR_ITIR, (log_page_size << 2));\n\tia64_setreg(_IA64_REG_CR_IFA, vmaddr);\n\tia64_stop();\n\t/* as per EAS2.6, itc must be the last instruction in an instruction group */\n\tif (target_mask & 0x1)\n\t\tia64_itci(pte);\n\tif (target_mask & 0x2)\n\t\tia64_itcd(pte);\n}\n\n/*\n * Purge a range of addresses from instruction and/or data translation\n * register(s).\n */\nstatic inline void\nia64_ptr (__u64 target_mask, __u64 vmaddr, __u64 log_size)\n{\n\tif (target_mask & 0x1)\n\t\tia64_ptri(vmaddr, (log_size << 2));\n\tif (target_mask & 0x2)\n\t\tia64_ptrd(vmaddr, (log_size << 2));\n}\n\n/* Set the interrupt vector address.  The address must be suitably aligned (32KB).  */\nstatic inline void\nia64_set_iva (void *ivt_addr)\n{\n\tia64_setreg(_IA64_REG_CR_IVA, (__u64) ivt_addr);\n\tia64_srlz_i();\n}\n\n/* Set the page table address and control bits.  */\nstatic inline void\nia64_set_pta (__u64 pta)\n{\n\t/* Note: srlz.i implies srlz.d */\n\tia64_setreg(_IA64_REG_CR_PTA, pta);\n\tia64_srlz_i();\n}\n\nstatic inline void\nia64_eoi (void)\n{\n\tia64_setreg(_IA64_REG_CR_EOI, 0);\n\tia64_srlz_d();\n}\n\n#define cpu_relax()\tia64_hint(ia64_hint_pause)\n\nstatic inline int\nia64_get_irr(unsigned int vector)\n{\n\tunsigned int reg = vector / 64;\n\tunsigned int bit = vector % 64;\n\tu64 irr;\n\n\tswitch (reg) {\n\tcase 0: irr = ia64_getreg(_IA64_REG_CR_IRR0); break;\n\tcase 1: irr = ia64_getreg(_IA64_REG_CR_IRR1); break;\n\tcase 2: irr = ia64_getreg(_IA64_REG_CR_IRR2); break;\n\tcase 3: irr = ia64_getreg(_IA64_REG_CR_IRR3); break;\n\t}\n\n\treturn test_bit(bit, &irr);\n}\n\nstatic inline void\nia64_set_lrr0 (unsigned long val)\n{\n\tia64_setreg(_IA64_REG_CR_LRR0, val);\n\tia64_srlz_d();\n}\n\nstatic inline void\nia64_set_lrr1 (unsigned long val)\n{\n\tia64_setreg(_IA64_REG_CR_LRR1, val);\n\tia64_srlz_d();\n}\n\n\n/*\n * Given the address to which a spill occurred, return the unat bit\n * number that corresponds to this address.\n */\nstatic inline __u64\nia64_unat_pos (void *spill_addr)\n{\n\treturn ((__u64) spill_addr >> 3) & 0x3f;\n}\n\n/*\n * Set the NaT bit of an integer register which was spilled at address\n * SPILL_ADDR.  UNAT is the mask to be updated.\n */\nstatic inline void\nia64_set_unat (__u64 *unat, void *spill_addr, unsigned long nat)\n{\n\t__u64 bit = ia64_unat_pos(spill_addr);\n\t__u64 mask = 1UL << bit;\n\n\t*unat = (*unat & ~mask) | (nat << bit);\n}\n\n/*\n * Return saved PC of a blocked thread.\n * Note that the only way T can block is through a call to schedule() -> switch_to().\n */\nstatic inline unsigned long\nthread_saved_pc (struct task_struct *t)\n{\n\tstruct unw_frame_info info;\n\tunsigned long ip;\n\n\tunw_init_from_blocked_task(&info, t);\n\tif (unw_unwind(&info) < 0)\n\t\treturn 0;\n\tunw_get_ip(&info, &ip);\n\treturn ip;\n}\n\n/*\n * Get the current instruction/program counter value.\n */\n#define current_text_addr() \\\n\t({ void *_pc; _pc = (void *)ia64_getreg(_IA64_REG_IP); _pc; })\n\nstatic inline __u64\nia64_get_ivr (void)\n{\n\t__u64 r;\n\tia64_srlz_d();\n\tr = ia64_getreg(_IA64_REG_CR_IVR);\n\tia64_srlz_d();\n\treturn r;\n}\n\nstatic inline void\nia64_set_dbr (__u64 regnum, __u64 value)\n{\n\t__ia64_set_dbr(regnum, value);\n#ifdef CONFIG_ITANIUM\n\tia64_srlz_d();\n#endif\n}\n\nstatic inline __u64\nia64_get_dbr (__u64 regnum)\n{\n\t__u64 retval;\n\n\tretval = __ia64_get_dbr(regnum);\n#ifdef CONFIG_ITANIUM\n\tia64_srlz_d();\n#endif\n\treturn retval;\n}\n\nstatic inline __u64\nia64_rotr (__u64 w, __u64 n)\n{\n\treturn (w >> n) | (w << (64 - n));\n}\n\n#define ia64_rotl(w,n)\tia64_rotr((w), (64) - (n))\n\n/*\n * Take a mapped kernel address and return the equivalent address\n * in the region 7 identity mapped virtual area.\n */\nstatic inline void *\nia64_imva (void *addr)\n{\n\tvoid *result;\n\tresult = (void *) ia64_tpa(addr);\n\treturn __va(result);\n}\n\n#define ARCH_HAS_PREFETCH\n#define ARCH_HAS_PREFETCHW\n#define ARCH_HAS_SPINLOCK_PREFETCH\n#define PREFETCH_STRIDE\t\t\tL1_CACHE_BYTES\n\nstatic inline void\nprefetch (const void *x)\n{\n\t ia64_lfetch(ia64_lfhint_none, x);\n}\n\nstatic inline void\nprefetchw (const void *x)\n{\n\tia64_lfetch_excl(ia64_lfhint_none, x);\n}\n\n#define spin_lock_prefetch(x)\tprefetchw(x)\n\nextern unsigned long boot_option_idle_override;\n\nenum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_FORCE_MWAIT,\n\t\t\t IDLE_NOMWAIT, IDLE_POLL};\n\nvoid default_idle(void);\n\n#define ia64_platform_is(x) (strcmp(x, ia64_platform_name) == 0)\n\n#endif /* !__ASSEMBLY__ */\n\n#endif /* _ASM_IA64_PROCESSOR_H */\n", "/*\n *  linux/fs/exec.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n/*\n * #!-checking implemented by tytso.\n */\n/*\n * Demand-loading implemented 01.12.91 - no need to read anything but\n * the header into memory. The inode of the executable is put into\n * \"current->executable\", and page faults do the actual loading. Clean.\n *\n * Once more I can proudly say that linux stood up to being changed: it\n * was less than 2 hours work to get demand-loading completely implemented.\n *\n * Demand loading changed July 1993 by Eric Youngdale.   Use mmap instead,\n * current->executable is only used by the procfs.  This allows a dispatch\n * table to check for several different types  of binary formats.  We keep\n * trying until we recognize the file or we run out of supported binary\n * formats. \n */\n\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/mm.h>\n#include <linux/stat.h>\n#include <linux/fcntl.h>\n#include <linux/swap.h>\n#include <linux/string.h>\n#include <linux/init.h>\n#include <linux/pagemap.h>\n#include <linux/perf_event.h>\n#include <linux/highmem.h>\n#include <linux/spinlock.h>\n#include <linux/key.h>\n#include <linux/personality.h>\n#include <linux/binfmts.h>\n#include <linux/utsname.h>\n#include <linux/pid_namespace.h>\n#include <linux/module.h>\n#include <linux/namei.h>\n#include <linux/mount.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/tsacct_kern.h>\n#include <linux/cn_proc.h>\n#include <linux/audit.h>\n#include <linux/tracehook.h>\n#include <linux/kmod.h>\n#include <linux/fsnotify.h>\n#include <linux/fs_struct.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/oom.h>\n#include <linux/compat.h>\n\n#include <asm/uaccess.h>\n#include <asm/mmu_context.h>\n#include <asm/tlb.h>\n\n#include <trace/events/task.h>\n#include \"internal.h\"\n#include \"coredump.h\"\n\n#include <trace/events/sched.h>\n\nint suid_dumpable = 0;\n\nstatic LIST_HEAD(formats);\nstatic DEFINE_RWLOCK(binfmt_lock);\n\nvoid __register_binfmt(struct linux_binfmt * fmt, int insert)\n{\n\tBUG_ON(!fmt);\n\tif (WARN_ON(!fmt->load_binary))\n\t\treturn;\n\twrite_lock(&binfmt_lock);\n\tinsert ? list_add(&fmt->lh, &formats) :\n\t\t list_add_tail(&fmt->lh, &formats);\n\twrite_unlock(&binfmt_lock);\n}\n\nEXPORT_SYMBOL(__register_binfmt);\n\nvoid unregister_binfmt(struct linux_binfmt * fmt)\n{\n\twrite_lock(&binfmt_lock);\n\tlist_del(&fmt->lh);\n\twrite_unlock(&binfmt_lock);\n}\n\nEXPORT_SYMBOL(unregister_binfmt);\n\nstatic inline void put_binfmt(struct linux_binfmt * fmt)\n{\n\tmodule_put(fmt->module);\n}\n\n/*\n * Note that a shared library must be both readable and executable due to\n * security reasons.\n *\n * Also note that we take the address to load from from the file itself.\n */\nSYSCALL_DEFINE1(uselib, const char __user *, library)\n{\n\tstruct file *file;\n\tstruct filename *tmp = getname(library);\n\tint error = PTR_ERR(tmp);\n\tstatic const struct open_flags uselib_flags = {\n\t\t.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,\n\t\t.acc_mode = MAY_READ | MAY_EXEC | MAY_OPEN,\n\t\t.intent = LOOKUP_OPEN,\n\t\t.lookup_flags = LOOKUP_FOLLOW,\n\t};\n\n\tif (IS_ERR(tmp))\n\t\tgoto out;\n\n\tfile = do_filp_open(AT_FDCWD, tmp, &uselib_flags);\n\tputname(tmp);\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out;\n\n\terror = -EINVAL;\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\tgoto exit;\n\n\terror = -EACCES;\n\tif (file->f_path.mnt->mnt_flags & MNT_NOEXEC)\n\t\tgoto exit;\n\n\tfsnotify_open(file);\n\n\terror = -ENOEXEC;\n\tif(file->f_op) {\n\t\tstruct linux_binfmt * fmt;\n\n\t\tread_lock(&binfmt_lock);\n\t\tlist_for_each_entry(fmt, &formats, lh) {\n\t\t\tif (!fmt->load_shlib)\n\t\t\t\tcontinue;\n\t\t\tif (!try_module_get(fmt->module))\n\t\t\t\tcontinue;\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\terror = fmt->load_shlib(file);\n\t\t\tread_lock(&binfmt_lock);\n\t\t\tput_binfmt(fmt);\n\t\t\tif (error != -ENOEXEC)\n\t\t\t\tbreak;\n\t\t}\n\t\tread_unlock(&binfmt_lock);\n\t}\nexit:\n\tfput(file);\nout:\n  \treturn error;\n}\n\n#ifdef CONFIG_MMU\n/*\n * The nascent bprm->mm is not visible until exec_mmap() but it can\n * use a lot of memory, account these pages in current->mm temporary\n * for oom_badness()->get_mm_rss(). Once exec succeeds or fails, we\n * change the counter back via acct_arg_size(0).\n */\nstatic void acct_arg_size(struct linux_binprm *bprm, unsigned long pages)\n{\n\tstruct mm_struct *mm = current->mm;\n\tlong diff = (long)(pages - bprm->vma_pages);\n\n\tif (!mm || !diff)\n\t\treturn;\n\n\tbprm->vma_pages = pages;\n\tadd_mm_counter(mm, MM_ANONPAGES, diff);\n}\n\nstatic struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\tint ret;\n\n#ifdef CONFIG_STACK_GROWSUP\n\tif (write) {\n\t\tret = expand_downwards(bprm->vma, pos);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\t}\n#endif\n\tret = get_user_pages(current, bprm->mm, pos,\n\t\t\t1, write, 1, &page, NULL);\n\tif (ret <= 0)\n\t\treturn NULL;\n\n\tif (write) {\n\t\tunsigned long size = bprm->vma->vm_end - bprm->vma->vm_start;\n\t\tstruct rlimit *rlim;\n\n\t\tacct_arg_size(bprm, size / PAGE_SIZE);\n\n\t\t/*\n\t\t * We've historically supported up to 32 pages (ARG_MAX)\n\t\t * of argument strings even with small stacks\n\t\t */\n\t\tif (size <= ARG_MAX)\n\t\t\treturn page;\n\n\t\t/*\n\t\t * Limit to 1/4-th the stack size for the argv+env strings.\n\t\t * This ensures that:\n\t\t *  - the remaining binfmt code will not run out of stack space,\n\t\t *  - the program will have a reasonable amount of stack left\n\t\t *    to work from.\n\t\t */\n\t\trlim = current->signal->rlim;\n\t\tif (size > ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur) / 4) {\n\t\t\tput_page(page);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn page;\n}\n\nstatic void put_arg_page(struct page *page)\n{\n\tput_page(page);\n}\n\nstatic void free_arg_page(struct linux_binprm *bprm, int i)\n{\n}\n\nstatic void free_arg_pages(struct linux_binprm *bprm)\n{\n}\n\nstatic void flush_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tstruct page *page)\n{\n\tflush_cache_page(bprm->vma, pos, page_to_pfn(page));\n}\n\nstatic int __bprm_mm_init(struct linux_binprm *bprm)\n{\n\tint err;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mm_struct *mm = bprm->mm;\n\n\tbprm->vma = vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);\n\tif (!vma)\n\t\treturn -ENOMEM;\n\n\tdown_write(&mm->mmap_sem);\n\tvma->vm_mm = mm;\n\n\t/*\n\t * Place the stack at the largest stack address the architecture\n\t * supports. Later, we'll move this to an appropriate place. We don't\n\t * use STACK_TOP because that can depend on attributes which aren't\n\t * configured yet.\n\t */\n\tBUILD_BUG_ON(VM_STACK_FLAGS & VM_STACK_INCOMPLETE_SETUP);\n\tvma->vm_end = STACK_TOP_MAX;\n\tvma->vm_start = vma->vm_end - PAGE_SIZE;\n\tvma->vm_flags = VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP;\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n\tINIT_LIST_HEAD(&vma->anon_vma_chain);\n\n\terr = insert_vm_struct(mm, vma);\n\tif (err)\n\t\tgoto err;\n\n\tmm->stack_vm = mm->total_vm = 1;\n\tup_write(&mm->mmap_sem);\n\tbprm->p = vma->vm_end - sizeof(void *);\n\treturn 0;\nerr:\n\tup_write(&mm->mmap_sem);\n\tbprm->vma = NULL;\n\tkmem_cache_free(vm_area_cachep, vma);\n\treturn err;\n}\n\nstatic bool valid_arg_len(struct linux_binprm *bprm, long len)\n{\n\treturn len <= MAX_ARG_STRLEN;\n}\n\n#else\n\nstatic inline void acct_arg_size(struct linux_binprm *bprm, unsigned long pages)\n{\n}\n\nstatic struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\n\tpage = bprm->page[pos / PAGE_SIZE];\n\tif (!page && write) {\n\t\tpage = alloc_page(GFP_HIGHUSER|__GFP_ZERO);\n\t\tif (!page)\n\t\t\treturn NULL;\n\t\tbprm->page[pos / PAGE_SIZE] = page;\n\t}\n\n\treturn page;\n}\n\nstatic void put_arg_page(struct page *page)\n{\n}\n\nstatic void free_arg_page(struct linux_binprm *bprm, int i)\n{\n\tif (bprm->page[i]) {\n\t\t__free_page(bprm->page[i]);\n\t\tbprm->page[i] = NULL;\n\t}\n}\n\nstatic void free_arg_pages(struct linux_binprm *bprm)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_ARG_PAGES; i++)\n\t\tfree_arg_page(bprm, i);\n}\n\nstatic void flush_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tstruct page *page)\n{\n}\n\nstatic int __bprm_mm_init(struct linux_binprm *bprm)\n{\n\tbprm->p = PAGE_SIZE * MAX_ARG_PAGES - sizeof(void *);\n\treturn 0;\n}\n\nstatic bool valid_arg_len(struct linux_binprm *bprm, long len)\n{\n\treturn len <= bprm->p;\n}\n\n#endif /* CONFIG_MMU */\n\n/*\n * Create a new mm_struct and populate it with a temporary stack\n * vm_area_struct.  We don't have enough context at this point to set the stack\n * flags, permissions, and offset, so we use temporary values.  We'll update\n * them later in setup_arg_pages().\n */\nstatic int bprm_mm_init(struct linux_binprm *bprm)\n{\n\tint err;\n\tstruct mm_struct *mm = NULL;\n\n\tbprm->mm = mm = mm_alloc();\n\terr = -ENOMEM;\n\tif (!mm)\n\t\tgoto err;\n\n\terr = init_new_context(current, mm);\n\tif (err)\n\t\tgoto err;\n\n\terr = __bprm_mm_init(bprm);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tif (mm) {\n\t\tbprm->mm = NULL;\n\t\tmmdrop(mm);\n\t}\n\n\treturn err;\n}\n\nstruct user_arg_ptr {\n#ifdef CONFIG_COMPAT\n\tbool is_compat;\n#endif\n\tunion {\n\t\tconst char __user *const __user *native;\n#ifdef CONFIG_COMPAT\n\t\tconst compat_uptr_t __user *compat;\n#endif\n\t} ptr;\n};\n\nstatic const char __user *get_user_arg_ptr(struct user_arg_ptr argv, int nr)\n{\n\tconst char __user *native;\n\n#ifdef CONFIG_COMPAT\n\tif (unlikely(argv.is_compat)) {\n\t\tcompat_uptr_t compat;\n\n\t\tif (get_user(compat, argv.ptr.compat + nr))\n\t\t\treturn ERR_PTR(-EFAULT);\n\n\t\treturn compat_ptr(compat);\n\t}\n#endif\n\n\tif (get_user(native, argv.ptr.native + nr))\n\t\treturn ERR_PTR(-EFAULT);\n\n\treturn native;\n}\n\n/*\n * count() counts the number of strings in array ARGV.\n */\nstatic int count(struct user_arg_ptr argv, int max)\n{\n\tint i = 0;\n\n\tif (argv.ptr.native != NULL) {\n\t\tfor (;;) {\n\t\t\tconst char __user *p = get_user_arg_ptr(argv, i);\n\n\t\t\tif (!p)\n\t\t\t\tbreak;\n\n\t\t\tif (IS_ERR(p))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tif (i >= max)\n\t\t\t\treturn -E2BIG;\n\t\t\t++i;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\treturn -ERESTARTNOHAND;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\treturn i;\n}\n\n/*\n * 'copy_strings()' copies argument/environment strings from the old\n * processes's memory to the new process's stack.  The call to get_user_pages()\n * ensures the destination page is created and not swapped out.\n */\nstatic int copy_strings(int argc, struct user_arg_ptr argv,\n\t\t\tstruct linux_binprm *bprm)\n{\n\tstruct page *kmapped_page = NULL;\n\tchar *kaddr = NULL;\n\tunsigned long kpos = 0;\n\tint ret;\n\n\twhile (argc-- > 0) {\n\t\tconst char __user *str;\n\t\tint len;\n\t\tunsigned long pos;\n\n\t\tret = -EFAULT;\n\t\tstr = get_user_arg_ptr(argv, argc);\n\t\tif (IS_ERR(str))\n\t\t\tgoto out;\n\n\t\tlen = strnlen_user(str, MAX_ARG_STRLEN);\n\t\tif (!len)\n\t\t\tgoto out;\n\n\t\tret = -E2BIG;\n\t\tif (!valid_arg_len(bprm, len))\n\t\t\tgoto out;\n\n\t\t/* We're going to work our way backwords. */\n\t\tpos = bprm->p;\n\t\tstr += len;\n\t\tbprm->p -= len;\n\n\t\twhile (len > 0) {\n\t\t\tint offset, bytes_to_copy;\n\n\t\t\tif (fatal_signal_pending(current)) {\n\t\t\t\tret = -ERESTARTNOHAND;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcond_resched();\n\n\t\t\toffset = pos % PAGE_SIZE;\n\t\t\tif (offset == 0)\n\t\t\t\toffset = PAGE_SIZE;\n\n\t\t\tbytes_to_copy = offset;\n\t\t\tif (bytes_to_copy > len)\n\t\t\t\tbytes_to_copy = len;\n\n\t\t\toffset -= bytes_to_copy;\n\t\t\tpos -= bytes_to_copy;\n\t\t\tstr -= bytes_to_copy;\n\t\t\tlen -= bytes_to_copy;\n\n\t\t\tif (!kmapped_page || kpos != (pos & PAGE_MASK)) {\n\t\t\t\tstruct page *page;\n\n\t\t\t\tpage = get_arg_page(bprm, pos, 1);\n\t\t\t\tif (!page) {\n\t\t\t\t\tret = -E2BIG;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tif (kmapped_page) {\n\t\t\t\t\tflush_kernel_dcache_page(kmapped_page);\n\t\t\t\t\tkunmap(kmapped_page);\n\t\t\t\t\tput_arg_page(kmapped_page);\n\t\t\t\t}\n\t\t\t\tkmapped_page = page;\n\t\t\t\tkaddr = kmap(kmapped_page);\n\t\t\t\tkpos = pos & PAGE_MASK;\n\t\t\t\tflush_arg_page(bprm, kpos, kmapped_page);\n\t\t\t}\n\t\t\tif (copy_from_user(kaddr+offset, str, bytes_to_copy)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tret = 0;\nout:\n\tif (kmapped_page) {\n\t\tflush_kernel_dcache_page(kmapped_page);\n\t\tkunmap(kmapped_page);\n\t\tput_arg_page(kmapped_page);\n\t}\n\treturn ret;\n}\n\n/*\n * Like copy_strings, but get argv and its values from kernel memory.\n */\nint copy_strings_kernel(int argc, const char *const *__argv,\n\t\t\tstruct linux_binprm *bprm)\n{\n\tint r;\n\tmm_segment_t oldfs = get_fs();\n\tstruct user_arg_ptr argv = {\n\t\t.ptr.native = (const char __user *const  __user *)__argv,\n\t};\n\n\tset_fs(KERNEL_DS);\n\tr = copy_strings(argc, argv, bprm);\n\tset_fs(oldfs);\n\n\treturn r;\n}\nEXPORT_SYMBOL(copy_strings_kernel);\n\n#ifdef CONFIG_MMU\n\n/*\n * During bprm_mm_init(), we create a temporary stack at STACK_TOP_MAX.  Once\n * the binfmt code determines where the new stack should reside, we shift it to\n * its final location.  The process proceeds as follows:\n *\n * 1) Use shift to calculate the new vma endpoints.\n * 2) Extend vma to cover both the old and new ranges.  This ensures the\n *    arguments passed to subsequent functions are consistent.\n * 3) Move vma's page tables to the new range.\n * 4) Free up any cleared pgd range.\n * 5) Shrink the vma to cover only the new range.\n */\nstatic int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long old_start = vma->vm_start;\n\tunsigned long old_end = vma->vm_end;\n\tunsigned long length = old_end - old_start;\n\tunsigned long new_start = old_start - shift;\n\tunsigned long new_end = old_end - shift;\n\tstruct mmu_gather tlb;\n\n\tBUG_ON(new_start > new_end);\n\n\t/*\n\t * ensure there are no vmas between where we want to go\n\t * and where we are\n\t */\n\tif (vma != find_vma(mm, new_start))\n\t\treturn -EFAULT;\n\n\t/*\n\t * cover the whole range: [new_start, old_end)\n\t */\n\tif (vma_adjust(vma, new_start, old_end, vma->vm_pgoff, NULL))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * move the page tables downwards, on failure we rely on\n\t * process cleanup to remove whatever mess we made.\n\t */\n\tif (length != move_page_tables(vma, old_start,\n\t\t\t\t       vma, new_start, length, false))\n\t\treturn -ENOMEM;\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm, old_start, old_end);\n\tif (new_end > old_start) {\n\t\t/*\n\t\t * when the old and new regions overlap clear from new_end.\n\t\t */\n\t\tfree_pgd_range(&tlb, new_end, old_end, new_end,\n\t\t\tvma->vm_next ? vma->vm_next->vm_start : USER_PGTABLES_CEILING);\n\t} else {\n\t\t/*\n\t\t * otherwise, clean from old_start; this is done to not touch\n\t\t * the address space in [new_end, old_start) some architectures\n\t\t * have constraints on va-space that make this illegal (IA64) -\n\t\t * for the others its just a little faster.\n\t\t */\n\t\tfree_pgd_range(&tlb, old_start, old_end, new_end,\n\t\t\tvma->vm_next ? vma->vm_next->vm_start : USER_PGTABLES_CEILING);\n\t}\n\ttlb_finish_mmu(&tlb, old_start, old_end);\n\n\t/*\n\t * Shrink the vma to just the new range.  Always succeeds.\n\t */\n\tvma_adjust(vma, new_start, new_end, vma->vm_pgoff, NULL);\n\n\treturn 0;\n}\n\n/*\n * Finalizes the stack vm_area_struct. The flags and permissions are updated,\n * the stack is optionally relocated, and some extra space is added.\n */\nint setup_arg_pages(struct linux_binprm *bprm,\n\t\t    unsigned long stack_top,\n\t\t    int executable_stack)\n{\n\tunsigned long ret;\n\tunsigned long stack_shift;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = bprm->vma;\n\tstruct vm_area_struct *prev = NULL;\n\tunsigned long vm_flags;\n\tunsigned long stack_base;\n\tunsigned long stack_size;\n\tunsigned long stack_expand;\n\tunsigned long rlim_stack;\n\n#ifdef CONFIG_STACK_GROWSUP\n\t/* Limit stack size to 1GB */\n\tstack_base = rlimit_max(RLIMIT_STACK);\n\tif (stack_base > (1 << 30))\n\t\tstack_base = 1 << 30;\n\n\t/* Make sure we didn't let the argument array grow too large. */\n\tif (vma->vm_end - vma->vm_start > stack_base)\n\t\treturn -ENOMEM;\n\n\tstack_base = PAGE_ALIGN(stack_top - stack_base);\n\n\tstack_shift = vma->vm_start - stack_base;\n\tmm->arg_start = bprm->p - stack_shift;\n\tbprm->p = vma->vm_end - stack_shift;\n#else\n\tstack_top = arch_align_stack(stack_top);\n\tstack_top = PAGE_ALIGN(stack_top);\n\n\tif (unlikely(stack_top < mmap_min_addr) ||\n\t    unlikely(vma->vm_end - vma->vm_start >= stack_top - mmap_min_addr))\n\t\treturn -ENOMEM;\n\n\tstack_shift = vma->vm_end - stack_top;\n\n\tbprm->p -= stack_shift;\n\tmm->arg_start = bprm->p;\n#endif\n\n\tif (bprm->loader)\n\t\tbprm->loader -= stack_shift;\n\tbprm->exec -= stack_shift;\n\n\tdown_write(&mm->mmap_sem);\n\tvm_flags = VM_STACK_FLAGS;\n\n\t/*\n\t * Adjust stack execute permissions; explicitly enable for\n\t * EXSTACK_ENABLE_X, disable for EXSTACK_DISABLE_X and leave alone\n\t * (arch default) otherwise.\n\t */\n\tif (unlikely(executable_stack == EXSTACK_ENABLE_X))\n\t\tvm_flags |= VM_EXEC;\n\telse if (executable_stack == EXSTACK_DISABLE_X)\n\t\tvm_flags &= ~VM_EXEC;\n\tvm_flags |= mm->def_flags;\n\tvm_flags |= VM_STACK_INCOMPLETE_SETUP;\n\n\tret = mprotect_fixup(vma, &prev, vma->vm_start, vma->vm_end,\n\t\t\tvm_flags);\n\tif (ret)\n\t\tgoto out_unlock;\n\tBUG_ON(prev != vma);\n\n\t/* Move stack pages down in memory. */\n\tif (stack_shift) {\n\t\tret = shift_arg_pages(vma, stack_shift);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* mprotect_fixup is overkill to remove the temporary stack flags */\n\tvma->vm_flags &= ~VM_STACK_INCOMPLETE_SETUP;\n\n\tstack_expand = 131072UL; /* randomly 32*4k (or 2*64k) pages */\n\tstack_size = vma->vm_end - vma->vm_start;\n\t/*\n\t * Align this down to a page boundary as expand_stack\n\t * will align it up.\n\t */\n\trlim_stack = rlimit(RLIMIT_STACK) & PAGE_MASK;\n#ifdef CONFIG_STACK_GROWSUP\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_start + rlim_stack;\n\telse\n\t\tstack_base = vma->vm_end + stack_expand;\n#else\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_end - rlim_stack;\n\telse\n\t\tstack_base = vma->vm_start - stack_expand;\n#endif\n\tcurrent->mm->start_stack = bprm->p;\n\tret = expand_stack(vma, stack_base);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(setup_arg_pages);\n\n#endif /* CONFIG_MMU */\n\nstruct file *open_exec(const char *name)\n{\n\tstruct file *file;\n\tint err;\n\tstruct filename tmp = { .name = name };\n\tstatic const struct open_flags open_exec_flags = {\n\t\t.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,\n\t\t.acc_mode = MAY_EXEC | MAY_OPEN,\n\t\t.intent = LOOKUP_OPEN,\n\t\t.lookup_flags = LOOKUP_FOLLOW,\n\t};\n\n\tfile = do_filp_open(AT_FDCWD, &tmp, &open_exec_flags);\n\tif (IS_ERR(file))\n\t\tgoto out;\n\n\terr = -EACCES;\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\tgoto exit;\n\n\tif (file->f_path.mnt->mnt_flags & MNT_NOEXEC)\n\t\tgoto exit;\n\n\tfsnotify_open(file);\n\n\terr = deny_write_access(file);\n\tif (err)\n\t\tgoto exit;\n\nout:\n\treturn file;\n\nexit:\n\tfput(file);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL(open_exec);\n\nint kernel_read(struct file *file, loff_t offset,\n\t\tchar *addr, unsigned long count)\n{\n\tmm_segment_t old_fs;\n\tloff_t pos = offset;\n\tint result;\n\n\told_fs = get_fs();\n\tset_fs(get_ds());\n\t/* The cast to a user pointer is valid due to the set_fs() */\n\tresult = vfs_read(file, (void __user *)addr, count, &pos);\n\tset_fs(old_fs);\n\treturn result;\n}\n\nEXPORT_SYMBOL(kernel_read);\n\nssize_t read_code(struct file *file, unsigned long addr, loff_t pos, size_t len)\n{\n\tssize_t res = file->f_op->read(file, (void __user *)addr, len, &pos);\n\tif (res > 0)\n\t\tflush_icache_range(addr, addr + len);\n\treturn res;\n}\nEXPORT_SYMBOL(read_code);\n\nstatic int exec_mmap(struct mm_struct *mm)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct * old_mm, *active_mm;\n\n\t/* Notify parent that we're no longer interested in the old VM */\n\ttsk = current;\n\told_mm = current->mm;\n\tmm_release(tsk, old_mm);\n\n\tif (old_mm) {\n\t\tsync_mm_rss(old_mm);\n\t\t/*\n\t\t * Make sure that if there is a core dump in progress\n\t\t * for the old mm, we get out and die instead of going\n\t\t * through with the exec.  We must hold mmap_sem around\n\t\t * checking core_state and changing tsk->mm.\n\t\t */\n\t\tdown_read(&old_mm->mmap_sem);\n\t\tif (unlikely(old_mm->core_state)) {\n\t\t\tup_read(&old_mm->mmap_sem);\n\t\t\treturn -EINTR;\n\t\t}\n\t}\n\ttask_lock(tsk);\n\tactive_mm = tsk->active_mm;\n\ttsk->mm = mm;\n\ttsk->active_mm = mm;\n\tactivate_mm(active_mm, mm);\n\ttask_unlock(tsk);\n\tarch_pick_mmap_layout(mm);\n\tif (old_mm) {\n\t\tup_read(&old_mm->mmap_sem);\n\t\tBUG_ON(active_mm != old_mm);\n\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, old_mm);\n\t\tmm_update_next_owner(old_mm);\n\t\tmmput(old_mm);\n\t\treturn 0;\n\t}\n\tmmdrop(active_mm);\n\treturn 0;\n}\n\n/*\n * This function makes sure the current process has its own signal table,\n * so that flush_signal_handlers can later reset the handlers without\n * disturbing other processes.  (Other processes might share the signal\n * table via the CLONE_SIGHAND option to clone().)\n */\nstatic int de_thread(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tstruct sighand_struct *oldsighand = tsk->sighand;\n\tspinlock_t *lock = &oldsighand->siglock;\n\n\tif (thread_group_empty(tsk))\n\t\tgoto no_thread_group;\n\n\t/*\n\t * Kill all other threads in the thread group.\n\t */\n\tspin_lock_irq(lock);\n\tif (signal_group_exit(sig)) {\n\t\t/*\n\t\t * Another group action in progress, just\n\t\t * return so that the signal is processed.\n\t\t */\n\t\tspin_unlock_irq(lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tsig->group_exit_task = tsk;\n\tsig->notify_count = zap_other_threads(tsk);\n\tif (!thread_group_leader(tsk))\n\t\tsig->notify_count--;\n\n\twhile (sig->notify_count) {\n\t\t__set_current_state(TASK_KILLABLE);\n\t\tspin_unlock_irq(lock);\n\t\tschedule();\n\t\tif (unlikely(__fatal_signal_pending(tsk)))\n\t\t\tgoto killed;\n\t\tspin_lock_irq(lock);\n\t}\n\tspin_unlock_irq(lock);\n\n\t/*\n\t * At this point all other threads have exited, all we have to\n\t * do is to wait for the thread group leader to become inactive,\n\t * and to assume its PID:\n\t */\n\tif (!thread_group_leader(tsk)) {\n\t\tstruct task_struct *leader = tsk->group_leader;\n\n\t\tsig->notify_count = -1;\t/* for exit_notify() */\n\t\tfor (;;) {\n\t\t\tthreadgroup_change_begin(tsk);\n\t\t\twrite_lock_irq(&tasklist_lock);\n\t\t\tif (likely(leader->exit_state))\n\t\t\t\tbreak;\n\t\t\t__set_current_state(TASK_KILLABLE);\n\t\t\twrite_unlock_irq(&tasklist_lock);\n\t\t\tthreadgroup_change_end(tsk);\n\t\t\tschedule();\n\t\t\tif (unlikely(__fatal_signal_pending(tsk)))\n\t\t\t\tgoto killed;\n\t\t}\n\n\t\t/*\n\t\t * The only record we have of the real-time age of a\n\t\t * process, regardless of execs it's done, is start_time.\n\t\t * All the past CPU time is accumulated in signal_struct\n\t\t * from sister threads now dead.  But in this non-leader\n\t\t * exec, nothing survives from the original leader thread,\n\t\t * whose birth marks the true age of this process now.\n\t\t * When we take on its identity by switching to its PID, we\n\t\t * also take its birthdate (always earlier than our own).\n\t\t */\n\t\ttsk->start_time = leader->start_time;\n\t\ttsk->real_start_time = leader->real_start_time;\n\n\t\tBUG_ON(!same_thread_group(leader, tsk));\n\t\tBUG_ON(has_group_leader_pid(tsk));\n\t\t/*\n\t\t * An exec() starts a new thread group with the\n\t\t * TGID of the previous thread group. Rehash the\n\t\t * two threads with a switched PID, and release\n\t\t * the former thread group leader:\n\t\t */\n\n\t\t/* Become a process group leader with the old leader's pid.\n\t\t * The old leader becomes a thread of the this thread group.\n\t\t * Note: The old leader also uses this pid until release_task\n\t\t *       is called.  Odd but simple and correct.\n\t\t */\n\t\ttsk->pid = leader->pid;\n\t\tchange_pid(tsk, PIDTYPE_PID, task_pid(leader));\n\t\ttransfer_pid(leader, tsk, PIDTYPE_PGID);\n\t\ttransfer_pid(leader, tsk, PIDTYPE_SID);\n\n\t\tlist_replace_rcu(&leader->tasks, &tsk->tasks);\n\t\tlist_replace_init(&leader->sibling, &tsk->sibling);\n\n\t\ttsk->group_leader = tsk;\n\t\tleader->group_leader = tsk;\n\n\t\ttsk->exit_signal = SIGCHLD;\n\t\tleader->exit_signal = -1;\n\n\t\tBUG_ON(leader->exit_state != EXIT_ZOMBIE);\n\t\tleader->exit_state = EXIT_DEAD;\n\n\t\t/*\n\t\t * We are going to release_task()->ptrace_unlink() silently,\n\t\t * the tracer can sleep in do_wait(). EXIT_DEAD guarantees\n\t\t * the tracer wont't block again waiting for this thread.\n\t\t */\n\t\tif (unlikely(leader->ptrace))\n\t\t\t__wake_up_parent(leader, leader->parent);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tthreadgroup_change_end(tsk);\n\n\t\trelease_task(leader);\n\t}\n\n\tsig->group_exit_task = NULL;\n\tsig->notify_count = 0;\n\nno_thread_group:\n\t/* we have changed execution domain */\n\ttsk->exit_signal = SIGCHLD;\n\n\texit_itimers(sig);\n\tflush_itimer_signals();\n\n\tif (atomic_read(&oldsighand->count) != 1) {\n\t\tstruct sighand_struct *newsighand;\n\t\t/*\n\t\t * This ->sighand is shared with the CLONE_SIGHAND\n\t\t * but not CLONE_THREAD task, switch to the new one.\n\t\t */\n\t\tnewsighand = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);\n\t\tif (!newsighand)\n\t\t\treturn -ENOMEM;\n\n\t\tatomic_set(&newsighand->count, 1);\n\t\tmemcpy(newsighand->action, oldsighand->action,\n\t\t       sizeof(newsighand->action));\n\n\t\twrite_lock_irq(&tasklist_lock);\n\t\tspin_lock(&oldsighand->siglock);\n\t\trcu_assign_pointer(tsk->sighand, newsighand);\n\t\tspin_unlock(&oldsighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\n\t\t__cleanup_sighand(oldsighand);\n\t}\n\n\tBUG_ON(!thread_group_leader(tsk));\n\treturn 0;\n\nkilled:\n\t/* protects against exit_notify() and __exit_signal() */\n\tread_lock(&tasklist_lock);\n\tsig->group_exit_task = NULL;\n\tsig->notify_count = 0;\n\tread_unlock(&tasklist_lock);\n\treturn -EAGAIN;\n}\n\nchar *get_task_comm(char *buf, struct task_struct *tsk)\n{\n\t/* buf must be at least sizeof(tsk->comm) in size */\n\ttask_lock(tsk);\n\tstrncpy(buf, tsk->comm, sizeof(tsk->comm));\n\ttask_unlock(tsk);\n\treturn buf;\n}\nEXPORT_SYMBOL_GPL(get_task_comm);\n\n/*\n * These functions flushes out all traces of the currently running executable\n * so that a new one can be started\n */\n\nvoid set_task_comm(struct task_struct *tsk, char *buf)\n{\n\ttask_lock(tsk);\n\ttrace_task_rename(tsk, buf);\n\tstrlcpy(tsk->comm, buf, sizeof(tsk->comm));\n\ttask_unlock(tsk);\n\tperf_event_comm(tsk);\n}\n\nstatic void filename_to_taskname(char *tcomm, const char *fn, unsigned int len)\n{\n\tint i, ch;\n\n\t/* Copies the binary name from after last slash */\n\tfor (i = 0; (ch = *(fn++)) != '\\0';) {\n\t\tif (ch == '/')\n\t\t\ti = 0; /* overwrite what we wrote */\n\t\telse\n\t\t\tif (i < len - 1)\n\t\t\t\ttcomm[i++] = ch;\n\t}\n\ttcomm[i] = '\\0';\n}\n\nint flush_old_exec(struct linux_binprm * bprm)\n{\n\tint retval;\n\n\t/*\n\t * Make sure we have a private signal table and that\n\t * we are unassociated from the previous thread group.\n\t */\n\tretval = de_thread(current);\n\tif (retval)\n\t\tgoto out;\n\n\tset_mm_exe_file(bprm->mm, bprm->file);\n\n\tfilename_to_taskname(bprm->tcomm, bprm->filename, sizeof(bprm->tcomm));\n\t/*\n\t * Release all of the old mmap stuff\n\t */\n\tacct_arg_size(bprm, 0);\n\tretval = exec_mmap(bprm->mm);\n\tif (retval)\n\t\tgoto out;\n\n\tbprm->mm = NULL;\t\t/* We're using it now */\n\n\tset_fs(USER_DS);\n\tcurrent->flags &=\n\t\t~(PF_RANDOMIZE | PF_FORKNOEXEC | PF_KTHREAD | PF_NOFREEZE);\n\tflush_thread();\n\tcurrent->personality &= ~bprm->per_clear;\n\n\treturn 0;\n\nout:\n\treturn retval;\n}\nEXPORT_SYMBOL(flush_old_exec);\n\nvoid would_dump(struct linux_binprm *bprm, struct file *file)\n{\n\tif (inode_permission(file_inode(file), MAY_READ) < 0)\n\t\tbprm->interp_flags |= BINPRM_FLAGS_ENFORCE_NONDUMP;\n}\nEXPORT_SYMBOL(would_dump);\n\nvoid setup_new_exec(struct linux_binprm * bprm)\n{\n\tarch_pick_mmap_layout(current->mm);\n\n\t/* This is the point of no return */\n\tcurrent->sas_ss_sp = current->sas_ss_size = 0;\n\n\tif (uid_eq(current_euid(), current_uid()) && gid_eq(current_egid(), current_gid()))\n\t\tset_dumpable(current->mm, SUID_DUMP_USER);\n\telse\n\t\tset_dumpable(current->mm, suid_dumpable);\n\n\tset_task_comm(current, bprm->tcomm);\n\n\t/* Set the new mm task size. We have to do that late because it may\n\t * depend on TIF_32BIT which is only updated in flush_thread() on\n\t * some architectures like powerpc\n\t */\n\tcurrent->mm->task_size = TASK_SIZE;\n\n\t/* install the new credentials */\n\tif (!uid_eq(bprm->cred->uid, current_euid()) ||\n\t    !gid_eq(bprm->cred->gid, current_egid())) {\n\t\tcurrent->pdeath_signal = 0;\n\t} else {\n\t\twould_dump(bprm, bprm->file);\n\t\tif (bprm->interp_flags & BINPRM_FLAGS_ENFORCE_NONDUMP)\n\t\t\tset_dumpable(current->mm, suid_dumpable);\n\t}\n\n\t/* An exec changes our domain. We are no longer part of the thread\n\t   group */\n\n\tcurrent->self_exec_id++;\n\t\t\t\n\tflush_signal_handlers(current, 0);\n\tdo_close_on_exec(current->files);\n}\nEXPORT_SYMBOL(setup_new_exec);\n\n/*\n * Prepare credentials and lock ->cred_guard_mutex.\n * install_exec_creds() commits the new creds and drops the lock.\n * Or, if exec fails before, free_bprm() should release ->cred and\n * and unlock.\n */\nint prepare_bprm_creds(struct linux_binprm *bprm)\n{\n\tif (mutex_lock_interruptible(&current->signal->cred_guard_mutex))\n\t\treturn -ERESTARTNOINTR;\n\n\tbprm->cred = prepare_exec_creds();\n\tif (likely(bprm->cred))\n\t\treturn 0;\n\n\tmutex_unlock(&current->signal->cred_guard_mutex);\n\treturn -ENOMEM;\n}\n\nvoid free_bprm(struct linux_binprm *bprm)\n{\n\tfree_arg_pages(bprm);\n\tif (bprm->cred) {\n\t\tmutex_unlock(&current->signal->cred_guard_mutex);\n\t\tabort_creds(bprm->cred);\n\t}\n\t/* If a binfmt changed the interp, free it. */\n\tif (bprm->interp != bprm->filename)\n\t\tkfree(bprm->interp);\n\tkfree(bprm);\n}\n\nint bprm_change_interp(char *interp, struct linux_binprm *bprm)\n{\n\t/* If a binfmt changed the interp, free it first. */\n\tif (bprm->interp != bprm->filename)\n\t\tkfree(bprm->interp);\n\tbprm->interp = kstrdup(interp, GFP_KERNEL);\n\tif (!bprm->interp)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nEXPORT_SYMBOL(bprm_change_interp);\n\n/*\n * install the new credentials for this executable\n */\nvoid install_exec_creds(struct linux_binprm *bprm)\n{\n\tsecurity_bprm_committing_creds(bprm);\n\n\tcommit_creds(bprm->cred);\n\tbprm->cred = NULL;\n\n\t/*\n\t * Disable monitoring for regular users\n\t * when executing setuid binaries. Must\n\t * wait until new credentials are committed\n\t * by commit_creds() above\n\t */\n\tif (get_dumpable(current->mm) != SUID_DUMP_USER)\n\t\tperf_event_exit_task(current);\n\t/*\n\t * cred_guard_mutex must be held at least to this point to prevent\n\t * ptrace_attach() from altering our determination of the task's\n\t * credentials; any time after this it may be unlocked.\n\t */\n\tsecurity_bprm_committed_creds(bprm);\n\tmutex_unlock(&current->signal->cred_guard_mutex);\n}\nEXPORT_SYMBOL(install_exec_creds);\n\n/*\n * determine how safe it is to execute the proposed program\n * - the caller must hold ->cred_guard_mutex to protect against\n *   PTRACE_ATTACH\n */\nstatic int check_unsafe_exec(struct linux_binprm *bprm)\n{\n\tstruct task_struct *p = current, *t;\n\tunsigned n_fs;\n\tint res = 0;\n\n\tif (p->ptrace) {\n\t\tif (p->ptrace & PT_PTRACE_CAP)\n\t\t\tbprm->unsafe |= LSM_UNSAFE_PTRACE_CAP;\n\t\telse\n\t\t\tbprm->unsafe |= LSM_UNSAFE_PTRACE;\n\t}\n\n\t/*\n\t * This isn't strictly necessary, but it makes it harder for LSMs to\n\t * mess up.\n\t */\n\tif (current->no_new_privs)\n\t\tbprm->unsafe |= LSM_UNSAFE_NO_NEW_PRIVS;\n\n\tn_fs = 1;\n\tspin_lock(&p->fs->lock);\n\trcu_read_lock();\n\tfor (t = next_thread(p); t != p; t = next_thread(t)) {\n\t\tif (t->fs == p->fs)\n\t\t\tn_fs++;\n\t}\n\trcu_read_unlock();\n\n\tif (p->fs->users > n_fs) {\n\t\tbprm->unsafe |= LSM_UNSAFE_SHARE;\n\t} else {\n\t\tres = -EAGAIN;\n\t\tif (!p->fs->in_exec) {\n\t\t\tp->fs->in_exec = 1;\n\t\t\tres = 1;\n\t\t}\n\t}\n\tspin_unlock(&p->fs->lock);\n\n\treturn res;\n}\n\n/* \n * Fill the binprm structure from the inode. \n * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes\n *\n * This may be called multiple times for binary chains (scripts for example).\n */\nint prepare_binprm(struct linux_binprm *bprm)\n{\n\tumode_t mode;\n\tstruct inode * inode = file_inode(bprm->file);\n\tint retval;\n\n\tmode = inode->i_mode;\n\tif (bprm->file->f_op == NULL)\n\t\treturn -EACCES;\n\n\t/* clear any previous set[ug]id data from a previous binary */\n\tbprm->cred->euid = current_euid();\n\tbprm->cred->egid = current_egid();\n\n\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&\n\t    !current->no_new_privs &&\n\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&\n\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {\n\t\t/* Set-uid? */\n\t\tif (mode & S_ISUID) {\n\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\t\tbprm->cred->euid = inode->i_uid;\n\t\t}\n\n\t\t/* Set-gid? */\n\t\t/*\n\t\t * If setgid is set but no group execute bit then this\n\t\t * is a candidate for mandatory locking, not a setgid\n\t\t * executable.\n\t\t */\n\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\t\tbprm->cred->egid = inode->i_gid;\n\t\t}\n\t}\n\n\t/* fill in binprm security blob */\n\tretval = security_bprm_set_creds(bprm);\n\tif (retval)\n\t\treturn retval;\n\tbprm->cred_prepared = 1;\n\n\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\treturn kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);\n}\n\nEXPORT_SYMBOL(prepare_binprm);\n\n/*\n * Arguments are '\\0' separated strings found at the location bprm->p\n * points to; chop off the first by relocating brpm->p to right after\n * the first '\\0' encountered.\n */\nint remove_arg_zero(struct linux_binprm *bprm)\n{\n\tint ret = 0;\n\tunsigned long offset;\n\tchar *kaddr;\n\tstruct page *page;\n\n\tif (!bprm->argc)\n\t\treturn 0;\n\n\tdo {\n\t\toffset = bprm->p & ~PAGE_MASK;\n\t\tpage = get_arg_page(bprm, bprm->p, 0);\n\t\tif (!page) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tkaddr = kmap_atomic(page);\n\n\t\tfor (; offset < PAGE_SIZE && kaddr[offset];\n\t\t\t\toffset++, bprm->p++)\n\t\t\t;\n\n\t\tkunmap_atomic(kaddr);\n\t\tput_arg_page(page);\n\n\t\tif (offset == PAGE_SIZE)\n\t\t\tfree_arg_page(bprm, (bprm->p >> PAGE_SHIFT) - 1);\n\t} while (offset == PAGE_SIZE);\n\n\tbprm->p++;\n\tbprm->argc--;\n\tret = 0;\n\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(remove_arg_zero);\n\n#define printable(c) (((c)=='\\t') || ((c)=='\\n') || (0x20<=(c) && (c)<=0x7e))\n/*\n * cycle the list of binary formats handler, until one recognizes the image\n */\nint search_binary_handler(struct linux_binprm *bprm)\n{\n\tbool need_retry = IS_ENABLED(CONFIG_MODULES);\n\tstruct linux_binfmt *fmt;\n\tint retval;\n\n\t/* This allows 4 levels of binfmt rewrites before failing hard. */\n\tif (bprm->recursion_depth > 5)\n\t\treturn -ELOOP;\n\n\tretval = security_bprm_check(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = audit_bprm(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = -ENOENT;\n retry:\n\tread_lock(&binfmt_lock);\n\tlist_for_each_entry(fmt, &formats, lh) {\n\t\tif (!try_module_get(fmt->module))\n\t\t\tcontinue;\n\t\tread_unlock(&binfmt_lock);\n\t\tbprm->recursion_depth++;\n\t\tretval = fmt->load_binary(bprm);\n\t\tbprm->recursion_depth--;\n\t\tif (retval >= 0 || retval != -ENOEXEC ||\n\t\t    bprm->mm == NULL || bprm->file == NULL) {\n\t\t\tput_binfmt(fmt);\n\t\t\treturn retval;\n\t\t}\n\t\tread_lock(&binfmt_lock);\n\t\tput_binfmt(fmt);\n\t}\n\tread_unlock(&binfmt_lock);\n\n\tif (need_retry && retval == -ENOEXEC) {\n\t\tif (printable(bprm->buf[0]) && printable(bprm->buf[1]) &&\n\t\t    printable(bprm->buf[2]) && printable(bprm->buf[3]))\n\t\t\treturn retval;\n\t\tif (request_module(\"binfmt-%04x\", *(ushort *)(bprm->buf + 2)) < 0)\n\t\t\treturn retval;\n\t\tneed_retry = false;\n\t\tgoto retry;\n\t}\n\n\treturn retval;\n}\nEXPORT_SYMBOL(search_binary_handler);\n\nstatic int exec_binprm(struct linux_binprm *bprm)\n{\n\tpid_t old_pid, old_vpid;\n\tint ret;\n\n\t/* Need to fetch pid before load_binary changes it */\n\told_pid = current->pid;\n\trcu_read_lock();\n\told_vpid = task_pid_nr_ns(current, task_active_pid_ns(current->parent));\n\trcu_read_unlock();\n\n\tret = search_binary_handler(bprm);\n\tif (ret >= 0) {\n\t\ttrace_sched_process_exec(current, old_pid, bprm);\n\t\tptrace_event(PTRACE_EVENT_EXEC, old_vpid);\n\t\tcurrent->did_exec = 1;\n\t\tproc_exec_connector(current);\n\n\t\tif (bprm->file) {\n\t\t\tallow_write_access(bprm->file);\n\t\t\tfput(bprm->file);\n\t\t\tbprm->file = NULL; /* to catch use-after-free */\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n/*\n * sys_execve() executes a new program.\n */\nstatic int do_execve_common(const char *filename,\n\t\t\t\tstruct user_arg_ptr argv,\n\t\t\t\tstruct user_arg_ptr envp)\n{\n\tstruct linux_binprm *bprm;\n\tstruct file *file;\n\tstruct files_struct *displaced;\n\tbool clear_in_exec;\n\tint retval;\n\n\t/*\n\t * We move the actual failure in case of RLIMIT_NPROC excess from\n\t * set*uid() to execve() because too many poorly written programs\n\t * don't check setuid() return code.  Here we additionally recheck\n\t * whether NPROC limit is still exceeded.\n\t */\n\tif ((current->flags & PF_NPROC_EXCEEDED) &&\n\t    atomic_read(&current_user()->processes) > rlimit(RLIMIT_NPROC)) {\n\t\tretval = -EAGAIN;\n\t\tgoto out_ret;\n\t}\n\n\t/* We're below the limit (still or again), so we don't want to make\n\t * further execve() calls fail. */\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\tgoto out_ret;\n\n\tretval = -ENOMEM;\n\tbprm = kzalloc(sizeof(*bprm), GFP_KERNEL);\n\tif (!bprm)\n\t\tgoto out_files;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_free;\n\n\tretval = check_unsafe_exec(bprm);\n\tif (retval < 0)\n\t\tgoto out_free;\n\tclear_in_exec = retval;\n\tcurrent->in_execve = 1;\n\n\tfile = open_exec(filename);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\tbprm->filename = filename;\n\tbprm->interp = filename;\n\n\tretval = bprm_mm_init(bprm);\n\tif (retval)\n\t\tgoto out_file;\n\n\tbprm->argc = count(argv, MAX_ARG_STRINGS);\n\tif ((retval = bprm->argc) < 0)\n\t\tgoto out;\n\n\tbprm->envc = count(envp, MAX_ARG_STRINGS);\n\tif ((retval = bprm->envc) < 0)\n\t\tgoto out;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tbprm->exec = bprm->p;\n\tretval = copy_strings(bprm->envc, envp, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings(bprm->argc, argv, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = exec_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\tacct_update_integrals(current);\n\ttask_numa_free(current);\n\tfree_bprm(bprm);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\tif (bprm->mm) {\n\t\tacct_arg_size(bprm, 0);\n\t\tmmput(bprm->mm);\n\t}\n\nout_file:\n\tif (bprm->file) {\n\t\tallow_write_access(bprm->file);\n\t\tfput(bprm->file);\n\t}\n\nout_unmark:\n\tif (clear_in_exec)\n\t\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_free:\n\tfree_bprm(bprm);\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\nout_ret:\n\treturn retval;\n}\n\nint do_execve(const char *filename,\n\tconst char __user *const __user *__argv,\n\tconst char __user *const __user *__envp)\n{\n\tstruct user_arg_ptr argv = { .ptr.native = __argv };\n\tstruct user_arg_ptr envp = { .ptr.native = __envp };\n\treturn do_execve_common(filename, argv, envp);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_do_execve(const char *filename,\n\tconst compat_uptr_t __user *__argv,\n\tconst compat_uptr_t __user *__envp)\n{\n\tstruct user_arg_ptr argv = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __argv,\n\t};\n\tstruct user_arg_ptr envp = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __envp,\n\t};\n\treturn do_execve_common(filename, argv, envp);\n}\n#endif\n\nvoid set_binfmt(struct linux_binfmt *new)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tif (mm->binfmt)\n\t\tmodule_put(mm->binfmt->module);\n\n\tmm->binfmt = new;\n\tif (new)\n\t\t__module_get(new->module);\n}\n\nEXPORT_SYMBOL(set_binfmt);\n\n/*\n * set_dumpable converts traditional three-value dumpable to two flags and\n * stores them into mm->flags.  It modifies lower two bits of mm->flags, but\n * these bits are not changed atomically.  So get_dumpable can observe the\n * intermediate state.  To avoid doing unexpected behavior, get get_dumpable\n * return either old dumpable or new one by paying attention to the order of\n * modifying the bits.\n *\n * dumpable |   mm->flags (binary)\n * old  new | initial interim  final\n * ---------+-----------------------\n *  0    1  |   00      01      01\n *  0    2  |   00      10(*)   11\n *  1    0  |   01      00      00\n *  1    2  |   01      11      11\n *  2    0  |   11      10(*)   00\n *  2    1  |   11      11      01\n *\n * (*) get_dumpable regards interim value of 10 as 11.\n */\nvoid set_dumpable(struct mm_struct *mm, int value)\n{\n\tswitch (value) {\n\tcase SUID_DUMP_DISABLE:\n\t\tclear_bit(MMF_DUMPABLE, &mm->flags);\n\t\tsmp_wmb();\n\t\tclear_bit(MMF_DUMP_SECURELY, &mm->flags);\n\t\tbreak;\n\tcase SUID_DUMP_USER:\n\t\tset_bit(MMF_DUMPABLE, &mm->flags);\n\t\tsmp_wmb();\n\t\tclear_bit(MMF_DUMP_SECURELY, &mm->flags);\n\t\tbreak;\n\tcase SUID_DUMP_ROOT:\n\t\tset_bit(MMF_DUMP_SECURELY, &mm->flags);\n\t\tsmp_wmb();\n\t\tset_bit(MMF_DUMPABLE, &mm->flags);\n\t\tbreak;\n\t}\n}\n\nint __get_dumpable(unsigned long mm_flags)\n{\n\tint ret;\n\n\tret = mm_flags & MMF_DUMPABLE_MASK;\n\treturn (ret > SUID_DUMP_USER) ? SUID_DUMP_ROOT : ret;\n}\n\nint get_dumpable(struct mm_struct *mm)\n{\n\treturn __get_dumpable(mm->flags);\n}\n\nSYSCALL_DEFINE3(execve,\n\t\tconst char __user *, filename,\n\t\tconst char __user *const __user *, argv,\n\t\tconst char __user *const __user *, envp)\n{\n\tstruct filename *path = getname(filename);\n\tint error = PTR_ERR(path);\n\tif (!IS_ERR(path)) {\n\t\terror = do_execve(path->name, argv, envp);\n\t\tputname(path);\n\t}\n\treturn error;\n}\n#ifdef CONFIG_COMPAT\nasmlinkage long compat_sys_execve(const char __user * filename,\n\tconst compat_uptr_t __user * argv,\n\tconst compat_uptr_t __user * envp)\n{\n\tstruct filename *path = getname(filename);\n\tint error = PTR_ERR(path);\n\tif (!IS_ERR(path)) {\n\t\terror = compat_do_execve(path->name, argv, envp);\n\t\tputname(path);\n\t}\n\treturn error;\n}\n#endif\n", "#ifndef _LINUX_BINFMTS_H\n#define _LINUX_BINFMTS_H\n\n#include <linux/sched.h>\n#include <linux/unistd.h>\n#include <asm/exec.h>\n#include <uapi/linux/binfmts.h>\n\n#define CORENAME_MAX_SIZE 128\n\n/*\n * This structure is used to hold the arguments that are used when loading binaries.\n */\nstruct linux_binprm {\n\tchar buf[BINPRM_BUF_SIZE];\n#ifdef CONFIG_MMU\n\tstruct vm_area_struct *vma;\n\tunsigned long vma_pages;\n#else\n# define MAX_ARG_PAGES\t32\n\tstruct page *page[MAX_ARG_PAGES];\n#endif\n\tstruct mm_struct *mm;\n\tunsigned long p; /* current top of mem */\n\tunsigned int\n\t\tcred_prepared:1,/* true if creds already prepared (multiple\n\t\t\t\t * preps happen for interpreters) */\n\t\tcap_effective:1;/* true if has elevated effective capabilities,\n\t\t\t\t * false if not; except for init which inherits\n\t\t\t\t * its parent's caps anyway */\n#ifdef __alpha__\n\tunsigned int taso:1;\n#endif\n\tunsigned int recursion_depth; /* only for search_binary_handler() */\n\tstruct file * file;\n\tstruct cred *cred;\t/* new credentials */\n\tint unsafe;\t\t/* how unsafe this exec is (mask of LSM_UNSAFE_*) */\n\tunsigned int per_clear;\t/* bits to clear in current->personality */\n\tint argc, envc;\n\tconst char * filename;\t/* Name of binary as seen by procps */\n\tconst char * interp;\t/* Name of the binary really executed. Most\n\t\t\t\t   of the time same as filename, but could be\n\t\t\t\t   different for binfmt_{misc,script} */\n\tunsigned interp_flags;\n\tunsigned interp_data;\n\tunsigned long loader, exec;\n\tchar tcomm[TASK_COMM_LEN];\n};\n\n#define BINPRM_FLAGS_ENFORCE_NONDUMP_BIT 0\n#define BINPRM_FLAGS_ENFORCE_NONDUMP (1 << BINPRM_FLAGS_ENFORCE_NONDUMP_BIT)\n\n/* fd of the binary should be passed to the interpreter */\n#define BINPRM_FLAGS_EXECFD_BIT 1\n#define BINPRM_FLAGS_EXECFD (1 << BINPRM_FLAGS_EXECFD_BIT)\n\n/* Function parameter for binfmt->coredump */\nstruct coredump_params {\n\tsiginfo_t *siginfo;\n\tstruct pt_regs *regs;\n\tstruct file *file;\n\tunsigned long limit;\n\tunsigned long mm_flags;\n};\n\n/*\n * This structure defines the functions that are used to load the binary formats that\n * linux accepts.\n */\nstruct linux_binfmt {\n\tstruct list_head lh;\n\tstruct module *module;\n\tint (*load_binary)(struct linux_binprm *);\n\tint (*load_shlib)(struct file *);\n\tint (*core_dump)(struct coredump_params *cprm);\n\tunsigned long min_coredump;\t/* minimal dump size */\n};\n\nextern void __register_binfmt(struct linux_binfmt *fmt, int insert);\n\n/* Registration of default binfmt handlers */\nstatic inline void register_binfmt(struct linux_binfmt *fmt)\n{\n\t__register_binfmt(fmt, 0);\n}\n/* Same as above, but adds a new binfmt at the top of the list */\nstatic inline void insert_binfmt(struct linux_binfmt *fmt)\n{\n\t__register_binfmt(fmt, 1);\n}\n\nextern void unregister_binfmt(struct linux_binfmt *);\n\nextern int prepare_binprm(struct linux_binprm *);\nextern int __must_check remove_arg_zero(struct linux_binprm *);\nextern int search_binary_handler(struct linux_binprm *);\nextern int flush_old_exec(struct linux_binprm * bprm);\nextern void setup_new_exec(struct linux_binprm * bprm);\nextern void would_dump(struct linux_binprm *, struct file *);\n\nextern int suid_dumpable;\n#define SUID_DUMP_DISABLE\t0\t/* No setuid dumping */\n#define SUID_DUMP_USER\t\t1\t/* Dump as user of process */\n#define SUID_DUMP_ROOT\t\t2\t/* Dump as root */\n\n/* Stack area protections */\n#define EXSTACK_DEFAULT   0\t/* Whatever the arch defaults to */\n#define EXSTACK_DISABLE_X 1\t/* Disable executable stacks */\n#define EXSTACK_ENABLE_X  2\t/* Enable executable stacks */\n\nextern int setup_arg_pages(struct linux_binprm * bprm,\n\t\t\t   unsigned long stack_top,\n\t\t\t   int executable_stack);\nextern int bprm_change_interp(char *interp, struct linux_binprm *bprm);\nextern int copy_strings_kernel(int argc, const char *const *argv,\n\t\t\t       struct linux_binprm *bprm);\nextern int prepare_bprm_creds(struct linux_binprm *bprm);\nextern void install_exec_creds(struct linux_binprm *bprm);\nextern void set_binfmt(struct linux_binfmt *new);\nextern void free_bprm(struct linux_binprm *);\nextern ssize_t read_code(struct file *, unsigned long, loff_t, size_t);\n\n#endif /* _LINUX_BINFMTS_H */\n", "#ifndef _LINUX_SCHED_H\n#define _LINUX_SCHED_H\n\n#include <uapi/linux/sched.h>\n\n\nstruct sched_param {\n\tint sched_priority;\n};\n\n#include <asm/param.h>\t/* for HZ */\n\n#include <linux/capability.h>\n#include <linux/threads.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/timex.h>\n#include <linux/jiffies.h>\n#include <linux/rbtree.h>\n#include <linux/thread_info.h>\n#include <linux/cpumask.h>\n#include <linux/errno.h>\n#include <linux/nodemask.h>\n#include <linux/mm_types.h>\n#include <linux/preempt.h>\n\n#include <asm/page.h>\n#include <asm/ptrace.h>\n#include <asm/cputime.h>\n\n#include <linux/smp.h>\n#include <linux/sem.h>\n#include <linux/signal.h>\n#include <linux/compiler.h>\n#include <linux/completion.h>\n#include <linux/pid.h>\n#include <linux/percpu.h>\n#include <linux/topology.h>\n#include <linux/proportions.h>\n#include <linux/seccomp.h>\n#include <linux/rcupdate.h>\n#include <linux/rculist.h>\n#include <linux/rtmutex.h>\n\n#include <linux/time.h>\n#include <linux/param.h>\n#include <linux/resource.h>\n#include <linux/timer.h>\n#include <linux/hrtimer.h>\n#include <linux/task_io_accounting.h>\n#include <linux/latencytop.h>\n#include <linux/cred.h>\n#include <linux/llist.h>\n#include <linux/uidgid.h>\n#include <linux/gfp.h>\n\n#include <asm/processor.h>\n\nstruct exec_domain;\nstruct futex_pi_state;\nstruct robust_list_head;\nstruct bio_list;\nstruct fs_struct;\nstruct perf_event_context;\nstruct blk_plug;\n\n/*\n * List of flags we want to share for kernel threads,\n * if only because they are not used by them anyway.\n */\n#define CLONE_KERNEL\t(CLONE_FS | CLONE_FILES | CLONE_SIGHAND)\n\n/*\n * These are the constant used to fake the fixed-point load-average\n * counting. Some notes:\n *  - 11 bit fractions expand to 22 bits by the multiplies: this gives\n *    a load-average precision of 10 bits integer + 11 bits fractional\n *  - if you want to count load-averages more often, you need more\n *    precision, or rounding will get you. With 2-second counting freq,\n *    the EXP_n values would be 1981, 2034 and 2043 if still using only\n *    11 bit fractions.\n */\nextern unsigned long avenrun[];\t\t/* Load averages */\nextern void get_avenrun(unsigned long *loads, unsigned long offset, int shift);\n\n#define FSHIFT\t\t11\t\t/* nr of bits of precision */\n#define FIXED_1\t\t(1<<FSHIFT)\t/* 1.0 as fixed-point */\n#define LOAD_FREQ\t(5*HZ+1)\t/* 5 sec intervals */\n#define EXP_1\t\t1884\t\t/* 1/exp(5sec/1min) as fixed-point */\n#define EXP_5\t\t2014\t\t/* 1/exp(5sec/5min) */\n#define EXP_15\t\t2037\t\t/* 1/exp(5sec/15min) */\n\n#define CALC_LOAD(load,exp,n) \\\n\tload *= exp; \\\n\tload += n*(FIXED_1-exp); \\\n\tload >>= FSHIFT;\n\nextern unsigned long total_forks;\nextern int nr_threads;\nDECLARE_PER_CPU(unsigned long, process_counts);\nextern int nr_processes(void);\nextern unsigned long nr_running(void);\nextern unsigned long nr_iowait(void);\nextern unsigned long nr_iowait_cpu(int cpu);\nextern unsigned long this_cpu_load(void);\n\n\nextern void calc_global_load(unsigned long ticks);\nextern void update_cpu_load_nohz(void);\n\nextern unsigned long get_parent_ip(unsigned long addr);\n\nextern void dump_cpu_task(int cpu);\n\nstruct seq_file;\nstruct cfs_rq;\nstruct task_group;\n#ifdef CONFIG_SCHED_DEBUG\nextern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);\nextern void proc_sched_set_task(struct task_struct *p);\nextern void\nprint_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);\n#endif\n\n/*\n * Task state bitmask. NOTE! These bits are also\n * encoded in fs/proc/array.c: get_task_state().\n *\n * We have two separate sets of flags: task->state\n * is about runnability, while task->exit_state are\n * about the task exiting. Confusing, but this way\n * modifying one set can't modify the other one by\n * mistake.\n */\n#define TASK_RUNNING\t\t0\n#define TASK_INTERRUPTIBLE\t1\n#define TASK_UNINTERRUPTIBLE\t2\n#define __TASK_STOPPED\t\t4\n#define __TASK_TRACED\t\t8\n/* in tsk->exit_state */\n#define EXIT_ZOMBIE\t\t16\n#define EXIT_DEAD\t\t32\n/* in tsk->state again */\n#define TASK_DEAD\t\t64\n#define TASK_WAKEKILL\t\t128\n#define TASK_WAKING\t\t256\n#define TASK_PARKED\t\t512\n#define TASK_STATE_MAX\t\t1024\n\n#define TASK_STATE_TO_CHAR_STR \"RSDTtZXxKWP\"\n\nextern char ___assert_task_state[1 - 2*!!(\n\t\tsizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];\n\n/* Convenience macros for the sake of set_task_state */\n#define TASK_KILLABLE\t\t(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)\n#define TASK_STOPPED\t\t(TASK_WAKEKILL | __TASK_STOPPED)\n#define TASK_TRACED\t\t(TASK_WAKEKILL | __TASK_TRACED)\n\n/* Convenience macros for the sake of wake_up */\n#define TASK_NORMAL\t\t(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)\n#define TASK_ALL\t\t(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)\n\n/* get_task_state() */\n#define TASK_REPORT\t\t(TASK_RUNNING | TASK_INTERRUPTIBLE | \\\n\t\t\t\t TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \\\n\t\t\t\t __TASK_TRACED)\n\n#define task_is_traced(task)\t((task->state & __TASK_TRACED) != 0)\n#define task_is_stopped(task)\t((task->state & __TASK_STOPPED) != 0)\n#define task_is_dead(task)\t((task)->exit_state != 0)\n#define task_is_stopped_or_traced(task)\t\\\n\t\t\t((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)\n#define task_contributes_to_load(task)\t\\\n\t\t\t\t((task->state & TASK_UNINTERRUPTIBLE) != 0 && \\\n\t\t\t\t (task->flags & PF_FROZEN) == 0)\n\n#define __set_task_state(tsk, state_value)\t\t\\\n\tdo { (tsk)->state = (state_value); } while (0)\n#define set_task_state(tsk, state_value)\t\t\\\n\tset_mb((tsk)->state, (state_value))\n\n/*\n * set_current_state() includes a barrier so that the write of current->state\n * is correctly serialised wrt the caller's subsequent test of whether to\n * actually sleep:\n *\n *\tset_current_state(TASK_UNINTERRUPTIBLE);\n *\tif (do_i_need_to_sleep())\n *\t\tschedule();\n *\n * If the caller does not need such serialisation then use __set_current_state()\n */\n#define __set_current_state(state_value)\t\t\t\\\n\tdo { current->state = (state_value); } while (0)\n#define set_current_state(state_value)\t\t\\\n\tset_mb(current->state, (state_value))\n\n/* Task command name length */\n#define TASK_COMM_LEN 16\n\n#include <linux/spinlock.h>\n\n/*\n * This serializes \"schedule()\" and also protects\n * the run-queue from deletions/modifications (but\n * _adding_ to the beginning of the run-queue has\n * a separate lock).\n */\nextern rwlock_t tasklist_lock;\nextern spinlock_t mmlist_lock;\n\nstruct task_struct;\n\n#ifdef CONFIG_PROVE_RCU\nextern int lockdep_tasklist_lock_is_held(void);\n#endif /* #ifdef CONFIG_PROVE_RCU */\n\nextern void sched_init(void);\nextern void sched_init_smp(void);\nextern asmlinkage void schedule_tail(struct task_struct *prev);\nextern void init_idle(struct task_struct *idle, int cpu);\nextern void init_idle_bootup_task(struct task_struct *idle);\n\nextern int runqueue_is_locked(int cpu);\n\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\nextern void nohz_balance_enter_idle(int cpu);\nextern void set_cpu_sd_state_idle(void);\nextern int get_nohz_timer_target(void);\n#else\nstatic inline void nohz_balance_enter_idle(int cpu) { }\nstatic inline void set_cpu_sd_state_idle(void) { }\n#endif\n\n/*\n * Only dump TASK_* tasks. (0 for all tasks)\n */\nextern void show_state_filter(unsigned long state_filter);\n\nstatic inline void show_state(void)\n{\n\tshow_state_filter(0);\n}\n\nextern void show_regs(struct pt_regs *);\n\n/*\n * TASK is a pointer to the task whose backtrace we want to see (or NULL for current\n * task), SP is the stack pointer of the first frame that should be shown in the back\n * trace (or NULL if the entire call-chain of the task should be shown).\n */\nextern void show_stack(struct task_struct *task, unsigned long *sp);\n\nvoid io_schedule(void);\nlong io_schedule_timeout(long timeout);\n\nextern void cpu_init (void);\nextern void trap_init(void);\nextern void update_process_times(int user);\nextern void scheduler_tick(void);\n\nextern void sched_show_task(struct task_struct *p);\n\n#ifdef CONFIG_LOCKUP_DETECTOR\nextern void touch_softlockup_watchdog(void);\nextern void touch_softlockup_watchdog_sync(void);\nextern void touch_all_softlockup_watchdogs(void);\nextern int proc_dowatchdog_thresh(struct ctl_table *table, int write,\n\t\t\t\t  void __user *buffer,\n\t\t\t\t  size_t *lenp, loff_t *ppos);\nextern unsigned int  softlockup_panic;\nvoid lockup_detector_init(void);\n#else\nstatic inline void touch_softlockup_watchdog(void)\n{\n}\nstatic inline void touch_softlockup_watchdog_sync(void)\n{\n}\nstatic inline void touch_all_softlockup_watchdogs(void)\n{\n}\nstatic inline void lockup_detector_init(void)\n{\n}\n#endif\n\n/* Attach to any functions which should be ignored in wchan output. */\n#define __sched\t\t__attribute__((__section__(\".sched.text\")))\n\n/* Linker adds these: start and end of __sched functions */\nextern char __sched_text_start[], __sched_text_end[];\n\n/* Is this address in the __sched functions? */\nextern int in_sched_functions(unsigned long addr);\n\n#define\tMAX_SCHEDULE_TIMEOUT\tLONG_MAX\nextern signed long schedule_timeout(signed long timeout);\nextern signed long schedule_timeout_interruptible(signed long timeout);\nextern signed long schedule_timeout_killable(signed long timeout);\nextern signed long schedule_timeout_uninterruptible(signed long timeout);\nasmlinkage void schedule(void);\nextern void schedule_preempt_disabled(void);\n\nstruct nsproxy;\nstruct user_namespace;\n\n#ifdef CONFIG_MMU\nextern void arch_pick_mmap_layout(struct mm_struct *mm);\nextern unsigned long\narch_get_unmapped_area(struct file *, unsigned long, unsigned long,\n\t\t       unsigned long, unsigned long);\nextern unsigned long\narch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t  unsigned long flags);\n#else\nstatic inline void arch_pick_mmap_layout(struct mm_struct *mm) {}\n#endif\n\n\nextern void set_dumpable(struct mm_struct *mm, int value);\nextern int get_dumpable(struct mm_struct *mm);\n\n/* mm flags */\n/* dumpable bits */\n#define MMF_DUMPABLE      0  /* core dump is permitted */\n#define MMF_DUMP_SECURELY 1  /* core file is readable only by root */\n\n#define MMF_DUMPABLE_BITS 2\n#define MMF_DUMPABLE_MASK ((1 << MMF_DUMPABLE_BITS) - 1)\n\n/* coredump filter bits */\n#define MMF_DUMP_ANON_PRIVATE\t2\n#define MMF_DUMP_ANON_SHARED\t3\n#define MMF_DUMP_MAPPED_PRIVATE\t4\n#define MMF_DUMP_MAPPED_SHARED\t5\n#define MMF_DUMP_ELF_HEADERS\t6\n#define MMF_DUMP_HUGETLB_PRIVATE 7\n#define MMF_DUMP_HUGETLB_SHARED  8\n\n#define MMF_DUMP_FILTER_SHIFT\tMMF_DUMPABLE_BITS\n#define MMF_DUMP_FILTER_BITS\t7\n#define MMF_DUMP_FILTER_MASK \\\n\t(((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)\n#define MMF_DUMP_FILTER_DEFAULT \\\n\t((1 << MMF_DUMP_ANON_PRIVATE) |\t(1 << MMF_DUMP_ANON_SHARED) |\\\n\t (1 << MMF_DUMP_HUGETLB_PRIVATE) | MMF_DUMP_MASK_DEFAULT_ELF)\n\n#ifdef CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS\n# define MMF_DUMP_MASK_DEFAULT_ELF\t(1 << MMF_DUMP_ELF_HEADERS)\n#else\n# define MMF_DUMP_MASK_DEFAULT_ELF\t0\n#endif\n\t\t\t\t\t/* leave room for more dump flags */\n#define MMF_VM_MERGEABLE\t16\t/* KSM may merge identical pages */\n#define MMF_VM_HUGEPAGE\t\t17\t/* set when VM_HUGEPAGE is set on vma */\n#define MMF_EXE_FILE_CHANGED\t18\t/* see prctl_set_mm_exe_file() */\n\n#define MMF_HAS_UPROBES\t\t19\t/* has uprobes */\n#define MMF_RECALC_UPROBES\t20\t/* MMF_HAS_UPROBES can be wrong */\n\n#define MMF_INIT_MASK\t\t(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)\n\nstruct sighand_struct {\n\tatomic_t\t\tcount;\n\tstruct k_sigaction\taction[_NSIG];\n\tspinlock_t\t\tsiglock;\n\twait_queue_head_t\tsignalfd_wqh;\n};\n\nstruct pacct_struct {\n\tint\t\t\tac_flag;\n\tlong\t\t\tac_exitcode;\n\tunsigned long\t\tac_mem;\n\tcputime_t\t\tac_utime, ac_stime;\n\tunsigned long\t\tac_minflt, ac_majflt;\n};\n\nstruct cpu_itimer {\n\tcputime_t expires;\n\tcputime_t incr;\n\tu32 error;\n\tu32 incr_error;\n};\n\n/**\n * struct cputime - snaphsot of system and user cputime\n * @utime: time spent in user mode\n * @stime: time spent in system mode\n *\n * Gathers a generic snapshot of user and system time.\n */\nstruct cputime {\n\tcputime_t utime;\n\tcputime_t stime;\n};\n\n/**\n * struct task_cputime - collected CPU time counts\n * @utime:\t\ttime spent in user mode, in &cputime_t units\n * @stime:\t\ttime spent in kernel mode, in &cputime_t units\n * @sum_exec_runtime:\ttotal time spent on the CPU, in nanoseconds\n *\n * This is an extension of struct cputime that includes the total runtime\n * spent by the task from the scheduler point of view.\n *\n * As a result, this structure groups together three kinds of CPU time\n * that are tracked for threads and thread groups.  Most things considering\n * CPU time want to group these counts together and treat all three\n * of them in parallel.\n */\nstruct task_cputime {\n\tcputime_t utime;\n\tcputime_t stime;\n\tunsigned long long sum_exec_runtime;\n};\n/* Alternate field names when used to cache expirations. */\n#define prof_exp\tstime\n#define virt_exp\tutime\n#define sched_exp\tsum_exec_runtime\n\n#define INIT_CPUTIME\t\\\n\t(struct task_cputime) {\t\t\t\t\t\\\n\t\t.utime = 0,\t\t\t\t\t\\\n\t\t.stime = 0,\t\t\t\t\t\\\n\t\t.sum_exec_runtime = 0,\t\t\t\t\\\n\t}\n\n#define PREEMPT_ENABLED\t\t(PREEMPT_NEED_RESCHED)\n\n#ifdef CONFIG_PREEMPT_COUNT\n#define PREEMPT_DISABLED\t(1 + PREEMPT_ENABLED)\n#else\n#define PREEMPT_DISABLED\tPREEMPT_ENABLED\n#endif\n\n/*\n * Disable preemption until the scheduler is running.\n * Reset by start_kernel()->sched_init()->init_idle().\n *\n * We include PREEMPT_ACTIVE to avoid cond_resched() from working\n * before the scheduler is active -- see should_resched().\n */\n#define INIT_PREEMPT_COUNT\t(PREEMPT_DISABLED + PREEMPT_ACTIVE)\n\n/**\n * struct thread_group_cputimer - thread group interval timer counts\n * @cputime:\t\tthread group interval timers.\n * @running:\t\tnon-zero when there are timers running and\n * \t\t\t@cputime receives updates.\n * @lock:\t\tlock for fields in this struct.\n *\n * This structure contains the version of task_cputime, above, that is\n * used for thread group CPU timer calculations.\n */\nstruct thread_group_cputimer {\n\tstruct task_cputime cputime;\n\tint running;\n\traw_spinlock_t lock;\n};\n\n#include <linux/rwsem.h>\nstruct autogroup;\n\n/*\n * NOTE! \"signal_struct\" does not have its own\n * locking, because a shared signal_struct always\n * implies a shared sighand_struct, so locking\n * sighand_struct is always a proper superset of\n * the locking of signal_struct.\n */\nstruct signal_struct {\n\tatomic_t\t\tsigcnt;\n\tatomic_t\t\tlive;\n\tint\t\t\tnr_threads;\n\n\twait_queue_head_t\twait_chldexit;\t/* for wait4() */\n\n\t/* current thread group signal load-balancing target: */\n\tstruct task_struct\t*curr_target;\n\n\t/* shared signal handling: */\n\tstruct sigpending\tshared_pending;\n\n\t/* thread group exit support */\n\tint\t\t\tgroup_exit_code;\n\t/* overloaded:\n\t * - notify group_exit_task when ->count is equal to notify_count\n\t * - everyone except group_exit_task is stopped during signal delivery\n\t *   of fatal signals, group_exit_task processes the signal.\n\t */\n\tint\t\t\tnotify_count;\n\tstruct task_struct\t*group_exit_task;\n\n\t/* thread group stop support, overloads group_exit_code too */\n\tint\t\t\tgroup_stop_count;\n\tunsigned int\t\tflags; /* see SIGNAL_* flags below */\n\n\t/*\n\t * PR_SET_CHILD_SUBREAPER marks a process, like a service\n\t * manager, to re-parent orphan (double-forking) child processes\n\t * to this process instead of 'init'. The service manager is\n\t * able to receive SIGCHLD signals and is able to investigate\n\t * the process until it calls wait(). All children of this\n\t * process will inherit a flag if they should look for a\n\t * child_subreaper process at exit.\n\t */\n\tunsigned int\t\tis_child_subreaper:1;\n\tunsigned int\t\thas_child_subreaper:1;\n\n\t/* POSIX.1b Interval Timers */\n\tint\t\t\tposix_timer_id;\n\tstruct list_head\tposix_timers;\n\n\t/* ITIMER_REAL timer for the process */\n\tstruct hrtimer real_timer;\n\tstruct pid *leader_pid;\n\tktime_t it_real_incr;\n\n\t/*\n\t * ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use\n\t * CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these\n\t * values are defined to 0 and 1 respectively\n\t */\n\tstruct cpu_itimer it[2];\n\n\t/*\n\t * Thread group totals for process CPU timers.\n\t * See thread_group_cputimer(), et al, for details.\n\t */\n\tstruct thread_group_cputimer cputimer;\n\n\t/* Earliest-expiration cache. */\n\tstruct task_cputime cputime_expires;\n\n\tstruct list_head cpu_timers[3];\n\n\tstruct pid *tty_old_pgrp;\n\n\t/* boolean value for session group leader */\n\tint leader;\n\n\tstruct tty_struct *tty; /* NULL if no tty */\n\n#ifdef CONFIG_SCHED_AUTOGROUP\n\tstruct autogroup *autogroup;\n#endif\n\t/*\n\t * Cumulative resource counters for dead threads in the group,\n\t * and for reaped dead child processes forked by this group.\n\t * Live threads maintain their own counters and add to these\n\t * in __exit_signal, except for the group leader.\n\t */\n\tcputime_t utime, stime, cutime, cstime;\n\tcputime_t gtime;\n\tcputime_t cgtime;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\n\tstruct cputime prev_cputime;\n#endif\n\tunsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;\n\tunsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;\n\tunsigned long inblock, oublock, cinblock, coublock;\n\tunsigned long maxrss, cmaxrss;\n\tstruct task_io_accounting ioac;\n\n\t/*\n\t * Cumulative ns of schedule CPU time fo dead threads in the\n\t * group, not including a zombie group leader, (This only differs\n\t * from jiffies_to_ns(utime + stime) if sched_clock uses something\n\t * other than jiffies.)\n\t */\n\tunsigned long long sum_sched_runtime;\n\n\t/*\n\t * We don't bother to synchronize most readers of this at all,\n\t * because there is no reader checking a limit that actually needs\n\t * to get both rlim_cur and rlim_max atomically, and either one\n\t * alone is a single word that can safely be read normally.\n\t * getrlimit/setrlimit use task_lock(current->group_leader) to\n\t * protect this instead of the siglock, because they really\n\t * have no need to disable irqs.\n\t */\n\tstruct rlimit rlim[RLIM_NLIMITS];\n\n#ifdef CONFIG_BSD_PROCESS_ACCT\n\tstruct pacct_struct pacct;\t/* per-process accounting information */\n#endif\n#ifdef CONFIG_TASKSTATS\n\tstruct taskstats *stats;\n#endif\n#ifdef CONFIG_AUDIT\n\tunsigned audit_tty;\n\tunsigned audit_tty_log_passwd;\n\tstruct tty_audit_buf *tty_audit_buf;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/*\n\t * group_rwsem prevents new tasks from entering the threadgroup and\n\t * member tasks from exiting,a more specifically, setting of\n\t * PF_EXITING.  fork and exit paths are protected with this rwsem\n\t * using threadgroup_change_begin/end().  Users which require\n\t * threadgroup to remain stable should use threadgroup_[un]lock()\n\t * which also takes care of exec path.  Currently, cgroup is the\n\t * only user.\n\t */\n\tstruct rw_semaphore group_rwsem;\n#endif\n\n\toom_flags_t oom_flags;\n\tshort oom_score_adj;\t\t/* OOM kill score adjustment */\n\tshort oom_score_adj_min;\t/* OOM kill score adjustment min value.\n\t\t\t\t\t * Only settable by CAP_SYS_RESOURCE. */\n\n\tstruct mutex cred_guard_mutex;\t/* guard against foreign influences on\n\t\t\t\t\t * credential calculations\n\t\t\t\t\t * (notably. ptrace) */\n};\n\n/*\n * Bits in flags field of signal_struct.\n */\n#define SIGNAL_STOP_STOPPED\t0x00000001 /* job control stop in effect */\n#define SIGNAL_STOP_CONTINUED\t0x00000002 /* SIGCONT since WCONTINUED reap */\n#define SIGNAL_GROUP_EXIT\t0x00000004 /* group exit in progress */\n#define SIGNAL_GROUP_COREDUMP\t0x00000008 /* coredump in progress */\n/*\n * Pending notifications to parent.\n */\n#define SIGNAL_CLD_STOPPED\t0x00000010\n#define SIGNAL_CLD_CONTINUED\t0x00000020\n#define SIGNAL_CLD_MASK\t\t(SIGNAL_CLD_STOPPED|SIGNAL_CLD_CONTINUED)\n\n#define SIGNAL_UNKILLABLE\t0x00000040 /* for init: ignore fatal signals */\n\n/* If true, all threads except ->group_exit_task have pending SIGKILL */\nstatic inline int signal_group_exit(const struct signal_struct *sig)\n{\n\treturn\t(sig->flags & SIGNAL_GROUP_EXIT) ||\n\t\t(sig->group_exit_task != NULL);\n}\n\n/*\n * Some day this will be a full-fledged user tracking system..\n */\nstruct user_struct {\n\tatomic_t __count;\t/* reference count */\n\tatomic_t processes;\t/* How many processes does this user have? */\n\tatomic_t files;\t\t/* How many open files does this user have? */\n\tatomic_t sigpending;\t/* How many pending signals does this user have? */\n#ifdef CONFIG_INOTIFY_USER\n\tatomic_t inotify_watches; /* How many inotify watches does this user have? */\n\tatomic_t inotify_devs;\t/* How many inotify devs does this user have opened? */\n#endif\n#ifdef CONFIG_FANOTIFY\n\tatomic_t fanotify_listeners;\n#endif\n#ifdef CONFIG_EPOLL\n\tatomic_long_t epoll_watches; /* The number of file descriptors currently watched */\n#endif\n#ifdef CONFIG_POSIX_MQUEUE\n\t/* protected by mq_lock\t*/\n\tunsigned long mq_bytes;\t/* How many bytes can be allocated to mqueue? */\n#endif\n\tunsigned long locked_shm; /* How many pages of mlocked shm ? */\n\n#ifdef CONFIG_KEYS\n\tstruct key *uid_keyring;\t/* UID specific keyring */\n\tstruct key *session_keyring;\t/* UID's default session keyring */\n#endif\n\n\t/* Hash table maintenance information */\n\tstruct hlist_node uidhash_node;\n\tkuid_t uid;\n\n#ifdef CONFIG_PERF_EVENTS\n\tatomic_long_t locked_vm;\n#endif\n};\n\nextern int uids_sysfs_init(void);\n\nextern struct user_struct *find_user(kuid_t);\n\nextern struct user_struct root_user;\n#define INIT_USER (&root_user)\n\n\nstruct backing_dev_info;\nstruct reclaim_state;\n\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\nstruct sched_info {\n\t/* cumulative counters */\n\tunsigned long pcount;\t      /* # of times run on this cpu */\n\tunsigned long long run_delay; /* time spent waiting on a runqueue */\n\n\t/* timestamps */\n\tunsigned long long last_arrival,/* when we last ran on a cpu */\n\t\t\t   last_queued;\t/* when we were last queued to run */\n};\n#endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */\n\n#ifdef CONFIG_TASK_DELAY_ACCT\nstruct task_delay_info {\n\tspinlock_t\tlock;\n\tunsigned int\tflags;\t/* Private per-task flags */\n\n\t/* For each stat XXX, add following, aligned appropriately\n\t *\n\t * struct timespec XXX_start, XXX_end;\n\t * u64 XXX_delay;\n\t * u32 XXX_count;\n\t *\n\t * Atomicity of updates to XXX_delay, XXX_count protected by\n\t * single lock above (split into XXX_lock if contention is an issue).\n\t */\n\n\t/*\n\t * XXX_count is incremented on every XXX operation, the delay\n\t * associated with the operation is added to XXX_delay.\n\t * XXX_delay contains the accumulated delay time in nanoseconds.\n\t */\n\tstruct timespec blkio_start, blkio_end;\t/* Shared by blkio, swapin */\n\tu64 blkio_delay;\t/* wait for sync block io completion */\n\tu64 swapin_delay;\t/* wait for swapin block io completion */\n\tu32 blkio_count;\t/* total count of the number of sync block */\n\t\t\t\t/* io operations performed */\n\tu32 swapin_count;\t/* total count of the number of swapin block */\n\t\t\t\t/* io operations performed */\n\n\tstruct timespec freepages_start, freepages_end;\n\tu64 freepages_delay;\t/* wait for memory reclaim */\n\tu32 freepages_count;\t/* total count of memory reclaim */\n};\n#endif\t/* CONFIG_TASK_DELAY_ACCT */\n\nstatic inline int sched_info_on(void)\n{\n#ifdef CONFIG_SCHEDSTATS\n\treturn 1;\n#elif defined(CONFIG_TASK_DELAY_ACCT)\n\textern int delayacct_on;\n\treturn delayacct_on;\n#else\n\treturn 0;\n#endif\n}\n\nenum cpu_idle_type {\n\tCPU_IDLE,\n\tCPU_NOT_IDLE,\n\tCPU_NEWLY_IDLE,\n\tCPU_MAX_IDLE_TYPES\n};\n\n/*\n * Increase resolution of cpu_power calculations\n */\n#define SCHED_POWER_SHIFT\t10\n#define SCHED_POWER_SCALE\t(1L << SCHED_POWER_SHIFT)\n\n/*\n * sched-domains (multiprocessor balancing) declarations:\n */\n#ifdef CONFIG_SMP\n#define SD_LOAD_BALANCE\t\t0x0001\t/* Do load balancing on this domain. */\n#define SD_BALANCE_NEWIDLE\t0x0002\t/* Balance when about to become idle */\n#define SD_BALANCE_EXEC\t\t0x0004\t/* Balance on exec */\n#define SD_BALANCE_FORK\t\t0x0008\t/* Balance on fork, clone */\n#define SD_BALANCE_WAKE\t\t0x0010  /* Balance on wakeup */\n#define SD_WAKE_AFFINE\t\t0x0020\t/* Wake task to waking CPU */\n#define SD_SHARE_CPUPOWER\t0x0080\t/* Domain members share cpu power */\n#define SD_SHARE_PKG_RESOURCES\t0x0200\t/* Domain members share cpu pkg resources */\n#define SD_SERIALIZE\t\t0x0400\t/* Only a single load balancing instance */\n#define SD_ASYM_PACKING\t\t0x0800  /* Place busy groups earlier in the domain */\n#define SD_PREFER_SIBLING\t0x1000\t/* Prefer to place tasks in a sibling domain */\n#define SD_OVERLAP\t\t0x2000\t/* sched_domains of this level overlap */\n#define SD_NUMA\t\t\t0x4000\t/* cross-node balancing */\n\nextern int __weak arch_sd_sibiling_asym_packing(void);\n\nstruct sched_domain_attr {\n\tint relax_domain_level;\n};\n\n#define SD_ATTR_INIT\t(struct sched_domain_attr) {\t\\\n\t.relax_domain_level = -1,\t\t\t\\\n}\n\nextern int sched_domain_level_max;\n\nstruct sched_group;\n\nstruct sched_domain {\n\t/* These fields must be setup */\n\tstruct sched_domain *parent;\t/* top domain must be null terminated */\n\tstruct sched_domain *child;\t/* bottom domain must be null terminated */\n\tstruct sched_group *groups;\t/* the balancing groups of the domain */\n\tunsigned long min_interval;\t/* Minimum balance interval ms */\n\tunsigned long max_interval;\t/* Maximum balance interval ms */\n\tunsigned int busy_factor;\t/* less balancing by factor if busy */\n\tunsigned int imbalance_pct;\t/* No balance until over watermark */\n\tunsigned int cache_nice_tries;\t/* Leave cache hot tasks for # tries */\n\tunsigned int busy_idx;\n\tunsigned int idle_idx;\n\tunsigned int newidle_idx;\n\tunsigned int wake_idx;\n\tunsigned int forkexec_idx;\n\tunsigned int smt_gain;\n\n\tint nohz_idle;\t\t\t/* NOHZ IDLE status */\n\tint flags;\t\t\t/* See SD_* */\n\tint level;\n\n\t/* Runtime fields. */\n\tunsigned long last_balance;\t/* init to jiffies. units in jiffies */\n\tunsigned int balance_interval;\t/* initialise to 1. units in ms. */\n\tunsigned int nr_balance_failed; /* initialise to 0 */\n\n\tu64 last_update;\n\n\t/* idle_balance() stats */\n\tu64 max_newidle_lb_cost;\n\tunsigned long next_decay_max_lb_cost;\n\n#ifdef CONFIG_SCHEDSTATS\n\t/* load_balance() stats */\n\tunsigned int lb_count[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_failed[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_balanced[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_imbalance[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_gained[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_hot_gained[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_nobusyg[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_nobusyq[CPU_MAX_IDLE_TYPES];\n\n\t/* Active load balancing */\n\tunsigned int alb_count;\n\tunsigned int alb_failed;\n\tunsigned int alb_pushed;\n\n\t/* SD_BALANCE_EXEC stats */\n\tunsigned int sbe_count;\n\tunsigned int sbe_balanced;\n\tunsigned int sbe_pushed;\n\n\t/* SD_BALANCE_FORK stats */\n\tunsigned int sbf_count;\n\tunsigned int sbf_balanced;\n\tunsigned int sbf_pushed;\n\n\t/* try_to_wake_up() stats */\n\tunsigned int ttwu_wake_remote;\n\tunsigned int ttwu_move_affine;\n\tunsigned int ttwu_move_balance;\n#endif\n#ifdef CONFIG_SCHED_DEBUG\n\tchar *name;\n#endif\n\tunion {\n\t\tvoid *private;\t\t/* used during construction */\n\t\tstruct rcu_head rcu;\t/* used during destruction */\n\t};\n\n\tunsigned int span_weight;\n\t/*\n\t * Span of all CPUs in this domain.\n\t *\n\t * NOTE: this field is variable length. (Allocated dynamically\n\t * by attaching extra space to the end of the structure,\n\t * depending on how many CPUs the kernel has booted up with)\n\t */\n\tunsigned long span[0];\n};\n\nstatic inline struct cpumask *sched_domain_span(struct sched_domain *sd)\n{\n\treturn to_cpumask(sd->span);\n}\n\nextern void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t\t    struct sched_domain_attr *dattr_new);\n\n/* Allocate an array of sched domains, for partition_sched_domains(). */\ncpumask_var_t *alloc_sched_domains(unsigned int ndoms);\nvoid free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);\n\nbool cpus_share_cache(int this_cpu, int that_cpu);\n\n#else /* CONFIG_SMP */\n\nstruct sched_domain_attr;\n\nstatic inline void\npartition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\tstruct sched_domain_attr *dattr_new)\n{\n}\n\nstatic inline bool cpus_share_cache(int this_cpu, int that_cpu)\n{\n\treturn true;\n}\n\n#endif\t/* !CONFIG_SMP */\n\n\nstruct io_context;\t\t\t/* See blkdev.h */\n\n\n#ifdef ARCH_HAS_PREFETCH_SWITCH_STACK\nextern void prefetch_stack(struct task_struct *t);\n#else\nstatic inline void prefetch_stack(struct task_struct *t) { }\n#endif\n\nstruct audit_context;\t\t/* See audit.c */\nstruct mempolicy;\nstruct pipe_inode_info;\nstruct uts_namespace;\n\nstruct load_weight {\n\tunsigned long weight, inv_weight;\n};\n\nstruct sched_avg {\n\t/*\n\t * These sums represent an infinite geometric series and so are bound\n\t * above by 1024/(1-y).  Thus we only need a u32 to store them for all\n\t * choices of y < 1-2^(-32)*1024.\n\t */\n\tu32 runnable_avg_sum, runnable_avg_period;\n\tu64 last_runnable_update;\n\ts64 decay_count;\n\tunsigned long load_avg_contrib;\n};\n\n#ifdef CONFIG_SCHEDSTATS\nstruct sched_statistics {\n\tu64\t\t\twait_start;\n\tu64\t\t\twait_max;\n\tu64\t\t\twait_count;\n\tu64\t\t\twait_sum;\n\tu64\t\t\tiowait_count;\n\tu64\t\t\tiowait_sum;\n\n\tu64\t\t\tsleep_start;\n\tu64\t\t\tsleep_max;\n\ts64\t\t\tsum_sleep_runtime;\n\n\tu64\t\t\tblock_start;\n\tu64\t\t\tblock_max;\n\tu64\t\t\texec_max;\n\tu64\t\t\tslice_max;\n\n\tu64\t\t\tnr_migrations_cold;\n\tu64\t\t\tnr_failed_migrations_affine;\n\tu64\t\t\tnr_failed_migrations_running;\n\tu64\t\t\tnr_failed_migrations_hot;\n\tu64\t\t\tnr_forced_migrations;\n\n\tu64\t\t\tnr_wakeups;\n\tu64\t\t\tnr_wakeups_sync;\n\tu64\t\t\tnr_wakeups_migrate;\n\tu64\t\t\tnr_wakeups_local;\n\tu64\t\t\tnr_wakeups_remote;\n\tu64\t\t\tnr_wakeups_affine;\n\tu64\t\t\tnr_wakeups_affine_attempts;\n\tu64\t\t\tnr_wakeups_passive;\n\tu64\t\t\tnr_wakeups_idle;\n};\n#endif\n\nstruct sched_entity {\n\tstruct load_weight\tload;\t\t/* for load-balancing */\n\tstruct rb_node\t\trun_node;\n\tstruct list_head\tgroup_node;\n\tunsigned int\t\ton_rq;\n\n\tu64\t\t\texec_start;\n\tu64\t\t\tsum_exec_runtime;\n\tu64\t\t\tvruntime;\n\tu64\t\t\tprev_sum_exec_runtime;\n\n\tu64\t\t\tnr_migrations;\n\n#ifdef CONFIG_SCHEDSTATS\n\tstruct sched_statistics statistics;\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tstruct sched_entity\t*parent;\n\t/* rq on which this entity is (to be) queued: */\n\tstruct cfs_rq\t\t*cfs_rq;\n\t/* rq \"owned\" by this entity/group: */\n\tstruct cfs_rq\t\t*my_q;\n#endif\n\n#ifdef CONFIG_SMP\n\t/* Per-entity load-tracking */\n\tstruct sched_avg\tavg;\n#endif\n};\n\nstruct sched_rt_entity {\n\tstruct list_head run_list;\n\tunsigned long timeout;\n\tunsigned long watchdog_stamp;\n\tunsigned int time_slice;\n\n\tstruct sched_rt_entity *back;\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity\t*parent;\n\t/* rq on which this entity is (to be) queued: */\n\tstruct rt_rq\t\t*rt_rq;\n\t/* rq \"owned\" by this entity/group: */\n\tstruct rt_rq\t\t*my_q;\n#endif\n};\n\n\nstruct rcu_node;\n\nenum perf_event_task_context {\n\tperf_invalid_context = -1,\n\tperf_hw_context = 0,\n\tperf_sw_context,\n\tperf_nr_task_contexts,\n};\n\nstruct task_struct {\n\tvolatile long state;\t/* -1 unrunnable, 0 runnable, >0 stopped */\n\tvoid *stack;\n\tatomic_t usage;\n\tunsigned int flags;\t/* per process flags, defined below */\n\tunsigned int ptrace;\n\n#ifdef CONFIG_SMP\n\tstruct llist_node wake_entry;\n\tint on_cpu;\n\tstruct task_struct *last_wakee;\n\tunsigned long wakee_flips;\n\tunsigned long wakee_flip_decay_ts;\n\n\tint wake_cpu;\n#endif\n\tint on_rq;\n\n\tint prio, static_prio, normal_prio;\n\tunsigned int rt_priority;\n\tconst struct sched_class *sched_class;\n\tstruct sched_entity se;\n\tstruct sched_rt_entity rt;\n#ifdef CONFIG_CGROUP_SCHED\n\tstruct task_group *sched_task_group;\n#endif\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\t/* list of struct preempt_notifier: */\n\tstruct hlist_head preempt_notifiers;\n#endif\n\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tunsigned int btrace_seq;\n#endif\n\n\tunsigned int policy;\n\tint nr_cpus_allowed;\n\tcpumask_t cpus_allowed;\n\n#ifdef CONFIG_PREEMPT_RCU\n\tint rcu_read_lock_nesting;\n\tchar rcu_read_unlock_special;\n\tstruct list_head rcu_node_entry;\n#endif /* #ifdef CONFIG_PREEMPT_RCU */\n#ifdef CONFIG_TREE_PREEMPT_RCU\n\tstruct rcu_node *rcu_blocked_node;\n#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */\n#ifdef CONFIG_RCU_BOOST\n\tstruct rt_mutex *rcu_boost_mutex;\n#endif /* #ifdef CONFIG_RCU_BOOST */\n\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\n\tstruct sched_info sched_info;\n#endif\n\n\tstruct list_head tasks;\n#ifdef CONFIG_SMP\n\tstruct plist_node pushable_tasks;\n#endif\n\n\tstruct mm_struct *mm, *active_mm;\n#ifdef CONFIG_COMPAT_BRK\n\tunsigned brk_randomized:1;\n#endif\n#if defined(SPLIT_RSS_COUNTING)\n\tstruct task_rss_stat\trss_stat;\n#endif\n/* task state */\n\tint exit_state;\n\tint exit_code, exit_signal;\n\tint pdeath_signal;  /*  The signal sent when the parent dies  */\n\tunsigned int jobctl;\t/* JOBCTL_*, siglock protected */\n\n\t/* Used for emulating ABI behavior of previous Linux versions */\n\tunsigned int personality;\n\n\tunsigned did_exec:1;\n\tunsigned in_execve:1;\t/* Tell the LSMs that the process is doing an\n\t\t\t\t * execve */\n\tunsigned in_iowait:1;\n\n\t/* task may not gain privileges */\n\tunsigned no_new_privs:1;\n\n\t/* Revert to default priority/policy when forking */\n\tunsigned sched_reset_on_fork:1;\n\tunsigned sched_contributes_to_load:1;\n\n\tpid_t pid;\n\tpid_t tgid;\n\n#ifdef CONFIG_CC_STACKPROTECTOR\n\t/* Canary value for the -fstack-protector gcc feature */\n\tunsigned long stack_canary;\n#endif\n\t/*\n\t * pointers to (original) parent process, youngest child, younger sibling,\n\t * older sibling, respectively.  (p->father can be replaced with\n\t * p->real_parent->pid)\n\t */\n\tstruct task_struct __rcu *real_parent; /* real parent process */\n\tstruct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */\n\t/*\n\t * children/sibling forms the list of my natural children\n\t */\n\tstruct list_head children;\t/* list of my children */\n\tstruct list_head sibling;\t/* linkage in my parent's children list */\n\tstruct task_struct *group_leader;\t/* threadgroup leader */\n\n\t/*\n\t * ptraced is the list of tasks this task is using ptrace on.\n\t * This includes both natural children and PTRACE_ATTACH targets.\n\t * p->ptrace_entry is p's link on the p->parent->ptraced list.\n\t */\n\tstruct list_head ptraced;\n\tstruct list_head ptrace_entry;\n\n\t/* PID/PID hash table linkage. */\n\tstruct pid_link pids[PIDTYPE_MAX];\n\tstruct list_head thread_group;\n\n\tstruct completion *vfork_done;\t\t/* for vfork() */\n\tint __user *set_child_tid;\t\t/* CLONE_CHILD_SETTID */\n\tint __user *clear_child_tid;\t\t/* CLONE_CHILD_CLEARTID */\n\n\tcputime_t utime, stime, utimescaled, stimescaled;\n\tcputime_t gtime;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\n\tstruct cputime prev_cputime;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_t vtime_seqlock;\n\tunsigned long long vtime_snap;\n\tenum {\n\t\tVTIME_SLEEPING = 0,\n\t\tVTIME_USER,\n\t\tVTIME_SYS,\n\t} vtime_snap_whence;\n#endif\n\tunsigned long nvcsw, nivcsw; /* context switch counts */\n\tstruct timespec start_time; \t\t/* monotonic time */\n\tstruct timespec real_start_time;\t/* boot based time */\n/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */\n\tunsigned long min_flt, maj_flt;\n\n\tstruct task_cputime cputime_expires;\n\tstruct list_head cpu_timers[3];\n\n/* process credentials */\n\tconst struct cred __rcu *real_cred; /* objective and real subjective task\n\t\t\t\t\t * credentials (COW) */\n\tconst struct cred __rcu *cred;\t/* effective (overridable) subjective task\n\t\t\t\t\t * credentials (COW) */\n\tchar comm[TASK_COMM_LEN]; /* executable name excluding path\n\t\t\t\t     - access with [gs]et_task_comm (which lock\n\t\t\t\t       it with task_lock())\n\t\t\t\t     - initialized normally by setup_new_exec */\n/* file system info */\n\tint link_count, total_link_count;\n#ifdef CONFIG_SYSVIPC\n/* ipc stuff */\n\tstruct sysv_sem sysvsem;\n#endif\n#ifdef CONFIG_DETECT_HUNG_TASK\n/* hung task detection */\n\tunsigned long last_switch_count;\n#endif\n/* CPU-specific state of this task */\n\tstruct thread_struct thread;\n/* filesystem information */\n\tstruct fs_struct *fs;\n/* open file information */\n\tstruct files_struct *files;\n/* namespaces */\n\tstruct nsproxy *nsproxy;\n/* signal handlers */\n\tstruct signal_struct *signal;\n\tstruct sighand_struct *sighand;\n\n\tsigset_t blocked, real_blocked;\n\tsigset_t saved_sigmask;\t/* restored if set_restore_sigmask() was used */\n\tstruct sigpending pending;\n\n\tunsigned long sas_ss_sp;\n\tsize_t sas_ss_size;\n\tint (*notifier)(void *priv);\n\tvoid *notifier_data;\n\tsigset_t *notifier_mask;\n\tstruct callback_head *task_works;\n\n\tstruct audit_context *audit_context;\n#ifdef CONFIG_AUDITSYSCALL\n\tkuid_t loginuid;\n\tunsigned int sessionid;\n#endif\n\tstruct seccomp seccomp;\n\n/* Thread group tracking */\n   \tu32 parent_exec_id;\n   \tu32 self_exec_id;\n/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,\n * mempolicy */\n\tspinlock_t alloc_lock;\n\n\t/* Protection of the PI data structures: */\n\traw_spinlock_t pi_lock;\n\n#ifdef CONFIG_RT_MUTEXES\n\t/* PI waiters blocked on a rt_mutex held by this task */\n\tstruct plist_head pi_waiters;\n\t/* Deadlock detection and priority inheritance handling */\n\tstruct rt_mutex_waiter *pi_blocked_on;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\t/* mutex deadlock detection */\n\tstruct mutex_waiter *blocked_on;\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tunsigned int irq_events;\n\tunsigned long hardirq_enable_ip;\n\tunsigned long hardirq_disable_ip;\n\tunsigned int hardirq_enable_event;\n\tunsigned int hardirq_disable_event;\n\tint hardirqs_enabled;\n\tint hardirq_context;\n\tunsigned long softirq_disable_ip;\n\tunsigned long softirq_enable_ip;\n\tunsigned int softirq_disable_event;\n\tunsigned int softirq_enable_event;\n\tint softirqs_enabled;\n\tint softirq_context;\n#endif\n#ifdef CONFIG_LOCKDEP\n# define MAX_LOCK_DEPTH 48UL\n\tu64 curr_chain_key;\n\tint lockdep_depth;\n\tunsigned int lockdep_recursion;\n\tstruct held_lock held_locks[MAX_LOCK_DEPTH];\n\tgfp_t lockdep_reclaim_gfp;\n#endif\n\n/* journalling filesystem info */\n\tvoid *journal_info;\n\n/* stacked block device info */\n\tstruct bio_list *bio_list;\n\n#ifdef CONFIG_BLOCK\n/* stack plugging */\n\tstruct blk_plug *plug;\n#endif\n\n/* VM state */\n\tstruct reclaim_state *reclaim_state;\n\n\tstruct backing_dev_info *backing_dev_info;\n\n\tstruct io_context *io_context;\n\n\tunsigned long ptrace_message;\n\tsiginfo_t *last_siginfo; /* For ptrace use.  */\n\tstruct task_io_accounting ioac;\n#if defined(CONFIG_TASK_XACCT)\n\tu64 acct_rss_mem1;\t/* accumulated rss usage */\n\tu64 acct_vm_mem1;\t/* accumulated virtual memory usage */\n\tcputime_t acct_timexpd;\t/* stime + utime since last update */\n#endif\n#ifdef CONFIG_CPUSETS\n\tnodemask_t mems_allowed;\t/* Protected by alloc_lock */\n\tseqcount_t mems_allowed_seq;\t/* Seqence no to catch updates */\n\tint cpuset_mem_spread_rotor;\n\tint cpuset_slab_spread_rotor;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/* Control Group info protected by css_set_lock */\n\tstruct css_set __rcu *cgroups;\n\t/* cg_list protected by css_set_lock and tsk->alloc_lock */\n\tstruct list_head cg_list;\n#endif\n#ifdef CONFIG_FUTEX\n\tstruct robust_list_head __user *robust_list;\n#ifdef CONFIG_COMPAT\n\tstruct compat_robust_list_head __user *compat_robust_list;\n#endif\n\tstruct list_head pi_state_list;\n\tstruct futex_pi_state *pi_state_cache;\n#endif\n#ifdef CONFIG_PERF_EVENTS\n\tstruct perf_event_context *perf_event_ctxp[perf_nr_task_contexts];\n\tstruct mutex perf_event_mutex;\n\tstruct list_head perf_event_list;\n#endif\n#ifdef CONFIG_NUMA\n\tstruct mempolicy *mempolicy;\t/* Protected by alloc_lock */\n\tshort il_next;\n\tshort pref_node_fork;\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\tint numa_scan_seq;\n\tunsigned int numa_scan_period;\n\tunsigned int numa_scan_period_max;\n\tint numa_preferred_nid;\n\tint numa_migrate_deferred;\n\tunsigned long numa_migrate_retry;\n\tu64 node_stamp;\t\t\t/* migration stamp  */\n\tstruct callback_head numa_work;\n\n\tstruct list_head numa_entry;\n\tstruct numa_group *numa_group;\n\n\t/*\n\t * Exponential decaying average of faults on a per-node basis.\n\t * Scheduling placement decisions are made based on the these counts.\n\t * The values remain static for the duration of a PTE scan\n\t */\n\tunsigned long *numa_faults;\n\tunsigned long total_numa_faults;\n\n\t/*\n\t * numa_faults_buffer records faults per node during the current\n\t * scan window. When the scan completes, the counts in numa_faults\n\t * decay and these values are copied.\n\t */\n\tunsigned long *numa_faults_buffer;\n\n\t/*\n\t * numa_faults_locality tracks if faults recorded during the last\n\t * scan window were remote/local. The task scan period is adapted\n\t * based on the locality of the faults with different weights\n\t * depending on whether they were shared or private faults\n\t */\n\tunsigned long numa_faults_locality[2];\n\n\tunsigned long numa_pages_migrated;\n#endif /* CONFIG_NUMA_BALANCING */\n\n\tstruct rcu_head rcu;\n\n\t/*\n\t * cache last used pipe for splice\n\t */\n\tstruct pipe_inode_info *splice_pipe;\n\n\tstruct page_frag task_frag;\n\n#ifdef\tCONFIG_TASK_DELAY_ACCT\n\tstruct task_delay_info *delays;\n#endif\n#ifdef CONFIG_FAULT_INJECTION\n\tint make_it_fail;\n#endif\n\t/*\n\t * when (nr_dirtied >= nr_dirtied_pause), it's time to call\n\t * balance_dirty_pages() for some dirty throttling pause\n\t */\n\tint nr_dirtied;\n\tint nr_dirtied_pause;\n\tunsigned long dirty_paused_when; /* start of a write-and-pause period */\n\n#ifdef CONFIG_LATENCYTOP\n\tint latency_record_count;\n\tstruct latency_record latency_record[LT_SAVECOUNT];\n#endif\n\t/*\n\t * time slack values; these are used to round up poll() and\n\t * select() etc timeout values. These are in nanoseconds.\n\t */\n\tunsigned long timer_slack_ns;\n\tunsigned long default_timer_slack_ns;\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t/* Index of current stored address in ret_stack */\n\tint curr_ret_stack;\n\t/* Stack of return addresses for return function tracing */\n\tstruct ftrace_ret_stack\t*ret_stack;\n\t/* time stamp for last schedule */\n\tunsigned long long ftrace_timestamp;\n\t/*\n\t * Number of functions that haven't been traced\n\t * because of depth overrun.\n\t */\n\tatomic_t trace_overrun;\n\t/* Pause for the tracing */\n\tatomic_t tracing_graph_pause;\n#endif\n#ifdef CONFIG_TRACING\n\t/* state flags for use by tracers */\n\tunsigned long trace;\n\t/* bitmask and counter of trace recursion */\n\tunsigned long trace_recursion;\n#endif /* CONFIG_TRACING */\n#ifdef CONFIG_MEMCG /* memcg uses this to do batch job */\n\tstruct memcg_batch_info {\n\t\tint do_batch;\t/* incremented when batch uncharge started */\n\t\tstruct mem_cgroup *memcg; /* target memcg of uncharge */\n\t\tunsigned long nr_pages;\t/* uncharged usage */\n\t\tunsigned long memsw_nr_pages; /* uncharged mem+swap usage */\n\t} memcg_batch;\n\tunsigned int memcg_kmem_skip_account;\n\tstruct memcg_oom_info {\n\t\tstruct mem_cgroup *memcg;\n\t\tgfp_t gfp_mask;\n\t\tint order;\n\t\tunsigned int may_oom:1;\n\t} memcg_oom;\n#endif\n#ifdef CONFIG_UPROBES\n\tstruct uprobe_task *utask;\n#endif\n#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)\n\tunsigned int\tsequential_io;\n\tunsigned int\tsequential_io_avg;\n#endif\n};\n\n/* Future-safe accessor for struct task_struct's cpus_allowed. */\n#define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)\n\n#define TNF_MIGRATED\t0x01\n#define TNF_NO_GROUP\t0x02\n#define TNF_SHARED\t0x04\n#define TNF_FAULT_LOCAL\t0x08\n\n#ifdef CONFIG_NUMA_BALANCING\nextern void task_numa_fault(int last_node, int node, int pages, int flags);\nextern pid_t task_numa_group_id(struct task_struct *p);\nextern void set_numabalancing_state(bool enabled);\nextern void task_numa_free(struct task_struct *p);\n\nextern unsigned int sysctl_numa_balancing_migrate_deferred;\n#else\nstatic inline void task_numa_fault(int last_node, int node, int pages,\n\t\t\t\t   int flags)\n{\n}\nstatic inline pid_t task_numa_group_id(struct task_struct *p)\n{\n\treturn 0;\n}\nstatic inline void set_numabalancing_state(bool enabled)\n{\n}\nstatic inline void task_numa_free(struct task_struct *p)\n{\n}\n#endif\n\nstatic inline struct pid *task_pid(struct task_struct *task)\n{\n\treturn task->pids[PIDTYPE_PID].pid;\n}\n\nstatic inline struct pid *task_tgid(struct task_struct *task)\n{\n\treturn task->group_leader->pids[PIDTYPE_PID].pid;\n}\n\n/*\n * Without tasklist or rcu lock it is not safe to dereference\n * the result of task_pgrp/task_session even if task == current,\n * we can race with another thread doing sys_setsid/sys_setpgid.\n */\nstatic inline struct pid *task_pgrp(struct task_struct *task)\n{\n\treturn task->group_leader->pids[PIDTYPE_PGID].pid;\n}\n\nstatic inline struct pid *task_session(struct task_struct *task)\n{\n\treturn task->group_leader->pids[PIDTYPE_SID].pid;\n}\n\nstruct pid_namespace;\n\n/*\n * the helpers to get the task's different pids as they are seen\n * from various namespaces\n *\n * task_xid_nr()     : global id, i.e. the id seen from the init namespace;\n * task_xid_vnr()    : virtual id, i.e. the id seen from the pid namespace of\n *                     current.\n * task_xid_nr_ns()  : id seen from the ns specified;\n *\n * set_task_vxid()   : assigns a virtual id to a task;\n *\n * see also pid_nr() etc in include/linux/pid.h\n */\npid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,\n\t\t\tstruct pid_namespace *ns);\n\nstatic inline pid_t task_pid_nr(struct task_struct *tsk)\n{\n\treturn tsk->pid;\n}\n\nstatic inline pid_t task_pid_nr_ns(struct task_struct *tsk,\n\t\t\t\t\tstruct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);\n}\n\nstatic inline pid_t task_pid_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);\n}\n\n\nstatic inline pid_t task_tgid_nr(struct task_struct *tsk)\n{\n\treturn tsk->tgid;\n}\n\npid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);\n\nstatic inline pid_t task_tgid_vnr(struct task_struct *tsk)\n{\n\treturn pid_vnr(task_tgid(tsk));\n}\n\n\nstatic inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,\n\t\t\t\t\tstruct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);\n}\n\nstatic inline pid_t task_pgrp_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, NULL);\n}\n\n\nstatic inline pid_t task_session_nr_ns(struct task_struct *tsk,\n\t\t\t\t\tstruct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);\n}\n\nstatic inline pid_t task_session_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);\n}\n\n/* obsolete, do not use */\nstatic inline pid_t task_pgrp_nr(struct task_struct *tsk)\n{\n\treturn task_pgrp_nr_ns(tsk, &init_pid_ns);\n}\n\n/**\n * pid_alive - check that a task structure is not stale\n * @p: Task structure to be checked.\n *\n * Test if a process is not yet dead (at most zombie state)\n * If pid_alive fails, then pointers within the task structure\n * can be stale and must not be dereferenced.\n *\n * Return: 1 if the process is alive. 0 otherwise.\n */\nstatic inline int pid_alive(struct task_struct *p)\n{\n\treturn p->pids[PIDTYPE_PID].pid != NULL;\n}\n\n/**\n * is_global_init - check if a task structure is init\n * @tsk: Task structure to be checked.\n *\n * Check if a task structure is the first user space task the kernel created.\n *\n * Return: 1 if the task structure is init. 0 otherwise.\n */\nstatic inline int is_global_init(struct task_struct *tsk)\n{\n\treturn tsk->pid == 1;\n}\n\nextern struct pid *cad_pid;\n\nextern void free_task(struct task_struct *tsk);\n#define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)\n\nextern void __put_task_struct(struct task_struct *t);\n\nstatic inline void put_task_struct(struct task_struct *t)\n{\n\tif (atomic_dec_and_test(&t->usage))\n\t\t__put_task_struct(t);\n}\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\nextern void task_cputime(struct task_struct *t,\n\t\t\t cputime_t *utime, cputime_t *stime);\nextern void task_cputime_scaled(struct task_struct *t,\n\t\t\t\tcputime_t *utimescaled, cputime_t *stimescaled);\nextern cputime_t task_gtime(struct task_struct *t);\n#else\nstatic inline void task_cputime(struct task_struct *t,\n\t\t\t\tcputime_t *utime, cputime_t *stime)\n{\n\tif (utime)\n\t\t*utime = t->utime;\n\tif (stime)\n\t\t*stime = t->stime;\n}\n\nstatic inline void task_cputime_scaled(struct task_struct *t,\n\t\t\t\t       cputime_t *utimescaled,\n\t\t\t\t       cputime_t *stimescaled)\n{\n\tif (utimescaled)\n\t\t*utimescaled = t->utimescaled;\n\tif (stimescaled)\n\t\t*stimescaled = t->stimescaled;\n}\n\nstatic inline cputime_t task_gtime(struct task_struct *t)\n{\n\treturn t->gtime;\n}\n#endif\nextern void task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);\nextern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);\n\n/*\n * Per process flags\n */\n#define PF_EXITING\t0x00000004\t/* getting shut down */\n#define PF_EXITPIDONE\t0x00000008\t/* pi exit done on shut down */\n#define PF_VCPU\t\t0x00000010\t/* I'm a virtual CPU */\n#define PF_WQ_WORKER\t0x00000020\t/* I'm a workqueue worker */\n#define PF_FORKNOEXEC\t0x00000040\t/* forked but didn't exec */\n#define PF_MCE_PROCESS  0x00000080      /* process policy on mce errors */\n#define PF_SUPERPRIV\t0x00000100\t/* used super-user privileges */\n#define PF_DUMPCORE\t0x00000200\t/* dumped core */\n#define PF_SIGNALED\t0x00000400\t/* killed by a signal */\n#define PF_MEMALLOC\t0x00000800\t/* Allocating memory */\n#define PF_NPROC_EXCEEDED 0x00001000\t/* set_user noticed that RLIMIT_NPROC was exceeded */\n#define PF_USED_MATH\t0x00002000\t/* if unset the fpu must be initialized before use */\n#define PF_USED_ASYNC\t0x00004000\t/* used async_schedule*(), used by module init */\n#define PF_NOFREEZE\t0x00008000\t/* this thread should not be frozen */\n#define PF_FROZEN\t0x00010000\t/* frozen for system suspend */\n#define PF_FSTRANS\t0x00020000\t/* inside a filesystem transaction */\n#define PF_KSWAPD\t0x00040000\t/* I am kswapd */\n#define PF_MEMALLOC_NOIO 0x00080000\t/* Allocating memory without IO involved */\n#define PF_LESS_THROTTLE 0x00100000\t/* Throttle me less: I clean memory */\n#define PF_KTHREAD\t0x00200000\t/* I am a kernel thread */\n#define PF_RANDOMIZE\t0x00400000\t/* randomize virtual address space */\n#define PF_SWAPWRITE\t0x00800000\t/* Allowed to write to swap */\n#define PF_SPREAD_PAGE\t0x01000000\t/* Spread page cache over cpuset */\n#define PF_SPREAD_SLAB\t0x02000000\t/* Spread some slab caches over cpuset */\n#define PF_NO_SETAFFINITY 0x04000000\t/* Userland is not allowed to meddle with cpus_allowed */\n#define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */\n#define PF_MEMPOLICY\t0x10000000\t/* Non-default NUMA mempolicy */\n#define PF_MUTEX_TESTER\t0x20000000\t/* Thread belongs to the rt mutex tester */\n#define PF_FREEZER_SKIP\t0x40000000\t/* Freezer should not count it as freezable */\n#define PF_SUSPEND_TASK 0x80000000      /* this thread called freeze_processes and should not be frozen */\n\n/*\n * Only the _current_ task can read/write to tsk->flags, but other\n * tasks can access tsk->flags in readonly mode for example\n * with tsk_used_math (like during threaded core dumping).\n * There is however an exception to this rule during ptrace\n * or during fork: the ptracer task is allowed to write to the\n * child->flags of its traced child (same goes for fork, the parent\n * can write to the child->flags), because we're guaranteed the\n * child is not running and in turn not changing child->flags\n * at the same time the parent does it.\n */\n#define clear_stopped_child_used_math(child) do { (child)->flags &= ~PF_USED_MATH; } while (0)\n#define set_stopped_child_used_math(child) do { (child)->flags |= PF_USED_MATH; } while (0)\n#define clear_used_math() clear_stopped_child_used_math(current)\n#define set_used_math() set_stopped_child_used_math(current)\n#define conditional_stopped_child_used_math(condition, child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)\n#define conditional_used_math(condition) \\\n\tconditional_stopped_child_used_math(condition, current)\n#define copy_to_stopped_child_used_math(child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)\n/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */\n#define tsk_used_math(p) ((p)->flags & PF_USED_MATH)\n#define used_math() tsk_used_math(current)\n\n/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags */\nstatic inline gfp_t memalloc_noio_flags(gfp_t flags)\n{\n\tif (unlikely(current->flags & PF_MEMALLOC_NOIO))\n\t\tflags &= ~__GFP_IO;\n\treturn flags;\n}\n\nstatic inline unsigned int memalloc_noio_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOIO;\n\tcurrent->flags |= PF_MEMALLOC_NOIO;\n\treturn flags;\n}\n\nstatic inline void memalloc_noio_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;\n}\n\n/*\n * task->jobctl flags\n */\n#define JOBCTL_STOP_SIGMASK\t0xffff\t/* signr of the last group stop */\n\n#define JOBCTL_STOP_DEQUEUED_BIT 16\t/* stop signal dequeued */\n#define JOBCTL_STOP_PENDING_BIT\t17\t/* task should stop for group stop */\n#define JOBCTL_STOP_CONSUME_BIT\t18\t/* consume group stop count */\n#define JOBCTL_TRAP_STOP_BIT\t19\t/* trap for STOP */\n#define JOBCTL_TRAP_NOTIFY_BIT\t20\t/* trap for NOTIFY */\n#define JOBCTL_TRAPPING_BIT\t21\t/* switching to TRACED */\n#define JOBCTL_LISTENING_BIT\t22\t/* ptracer is listening for events */\n\n#define JOBCTL_STOP_DEQUEUED\t(1 << JOBCTL_STOP_DEQUEUED_BIT)\n#define JOBCTL_STOP_PENDING\t(1 << JOBCTL_STOP_PENDING_BIT)\n#define JOBCTL_STOP_CONSUME\t(1 << JOBCTL_STOP_CONSUME_BIT)\n#define JOBCTL_TRAP_STOP\t(1 << JOBCTL_TRAP_STOP_BIT)\n#define JOBCTL_TRAP_NOTIFY\t(1 << JOBCTL_TRAP_NOTIFY_BIT)\n#define JOBCTL_TRAPPING\t\t(1 << JOBCTL_TRAPPING_BIT)\n#define JOBCTL_LISTENING\t(1 << JOBCTL_LISTENING_BIT)\n\n#define JOBCTL_TRAP_MASK\t(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)\n#define JOBCTL_PENDING_MASK\t(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)\n\nextern bool task_set_jobctl_pending(struct task_struct *task,\n\t\t\t\t    unsigned int mask);\nextern void task_clear_jobctl_trapping(struct task_struct *task);\nextern void task_clear_jobctl_pending(struct task_struct *task,\n\t\t\t\t      unsigned int mask);\n\n#ifdef CONFIG_PREEMPT_RCU\n\n#define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */\n#define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */\n\nstatic inline void rcu_copy_process(struct task_struct *p)\n{\n\tp->rcu_read_lock_nesting = 0;\n\tp->rcu_read_unlock_special = 0;\n#ifdef CONFIG_TREE_PREEMPT_RCU\n\tp->rcu_blocked_node = NULL;\n#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */\n#ifdef CONFIG_RCU_BOOST\n\tp->rcu_boost_mutex = NULL;\n#endif /* #ifdef CONFIG_RCU_BOOST */\n\tINIT_LIST_HEAD(&p->rcu_node_entry);\n}\n\n#else\n\nstatic inline void rcu_copy_process(struct task_struct *p)\n{\n}\n\n#endif\n\nstatic inline void tsk_restore_flags(struct task_struct *task,\n\t\t\t\tunsigned long orig_flags, unsigned long flags)\n{\n\ttask->flags &= ~flags;\n\ttask->flags |= orig_flags & flags;\n}\n\n#ifdef CONFIG_SMP\nextern void do_set_cpus_allowed(struct task_struct *p,\n\t\t\t       const struct cpumask *new_mask);\n\nextern int set_cpus_allowed_ptr(struct task_struct *p,\n\t\t\t\tconst struct cpumask *new_mask);\n#else\nstatic inline void do_set_cpus_allowed(struct task_struct *p,\n\t\t\t\t      const struct cpumask *new_mask)\n{\n}\nstatic inline int set_cpus_allowed_ptr(struct task_struct *p,\n\t\t\t\t       const struct cpumask *new_mask)\n{\n\tif (!cpumask_test_cpu(0, new_mask))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_NO_HZ_COMMON\nvoid calc_load_enter_idle(void);\nvoid calc_load_exit_idle(void);\n#else\nstatic inline void calc_load_enter_idle(void) { }\nstatic inline void calc_load_exit_idle(void) { }\n#endif /* CONFIG_NO_HZ_COMMON */\n\n#ifndef CONFIG_CPUMASK_OFFSTACK\nstatic inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)\n{\n\treturn set_cpus_allowed_ptr(p, &new_mask);\n}\n#endif\n\n/*\n * Do not use outside of architecture code which knows its limitations.\n *\n * sched_clock() has no promise of monotonicity or bounded drift between\n * CPUs, use (which you should not) requires disabling IRQs.\n *\n * Please use one of the three interfaces below.\n */\nextern unsigned long long notrace sched_clock(void);\n/*\n * See the comment in kernel/sched/clock.c\n */\nextern u64 cpu_clock(int cpu);\nextern u64 local_clock(void);\nextern u64 sched_clock_cpu(int cpu);\n\n\nextern void sched_clock_init(void);\n\n#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\nstatic inline void sched_clock_tick(void)\n{\n}\n\nstatic inline void sched_clock_idle_sleep_event(void)\n{\n}\n\nstatic inline void sched_clock_idle_wakeup_event(u64 delta_ns)\n{\n}\n#else\n/*\n * Architectures can set this to 1 if they have specified\n * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,\n * but then during bootup it turns out that sched_clock()\n * is reliable after all:\n */\nextern int sched_clock_stable;\n\nextern void sched_clock_tick(void);\nextern void sched_clock_idle_sleep_event(void);\nextern void sched_clock_idle_wakeup_event(u64 delta_ns);\n#endif\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n/*\n * An i/f to runtime opt-in for irq time accounting based off of sched_clock.\n * The reason for this explicit opt-in is not to have perf penalty with\n * slow sched_clocks.\n */\nextern void enable_sched_clock_irqtime(void);\nextern void disable_sched_clock_irqtime(void);\n#else\nstatic inline void enable_sched_clock_irqtime(void) {}\nstatic inline void disable_sched_clock_irqtime(void) {}\n#endif\n\nextern unsigned long long\ntask_sched_runtime(struct task_struct *task);\n\n/* sched_exec is called by processes performing an exec */\n#ifdef CONFIG_SMP\nextern void sched_exec(void);\n#else\n#define sched_exec()   {}\n#endif\n\nextern void sched_clock_idle_sleep_event(void);\nextern void sched_clock_idle_wakeup_event(u64 delta_ns);\n\n#ifdef CONFIG_HOTPLUG_CPU\nextern void idle_task_exit(void);\n#else\nstatic inline void idle_task_exit(void) {}\n#endif\n\n#if defined(CONFIG_NO_HZ_COMMON) && defined(CONFIG_SMP)\nextern void wake_up_nohz_cpu(int cpu);\n#else\nstatic inline void wake_up_nohz_cpu(int cpu) { }\n#endif\n\n#ifdef CONFIG_NO_HZ_FULL\nextern bool sched_can_stop_tick(void);\nextern u64 scheduler_tick_max_deferment(void);\n#else\nstatic inline bool sched_can_stop_tick(void) { return false; }\n#endif\n\n#ifdef CONFIG_SCHED_AUTOGROUP\nextern void sched_autogroup_create_attach(struct task_struct *p);\nextern void sched_autogroup_detach(struct task_struct *p);\nextern void sched_autogroup_fork(struct signal_struct *sig);\nextern void sched_autogroup_exit(struct signal_struct *sig);\n#ifdef CONFIG_PROC_FS\nextern void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m);\nextern int proc_sched_autogroup_set_nice(struct task_struct *p, int nice);\n#endif\n#else\nstatic inline void sched_autogroup_create_attach(struct task_struct *p) { }\nstatic inline void sched_autogroup_detach(struct task_struct *p) { }\nstatic inline void sched_autogroup_fork(struct signal_struct *sig) { }\nstatic inline void sched_autogroup_exit(struct signal_struct *sig) { }\n#endif\n\nextern bool yield_to(struct task_struct *p, bool preempt);\nextern void set_user_nice(struct task_struct *p, long nice);\nextern int task_prio(const struct task_struct *p);\nextern int task_nice(const struct task_struct *p);\nextern int can_nice(const struct task_struct *p, const int nice);\nextern int task_curr(const struct task_struct *p);\nextern int idle_cpu(int cpu);\nextern int sched_setscheduler(struct task_struct *, int,\n\t\t\t      const struct sched_param *);\nextern int sched_setscheduler_nocheck(struct task_struct *, int,\n\t\t\t\t      const struct sched_param *);\nextern struct task_struct *idle_task(int cpu);\n/**\n * is_idle_task - is the specified task an idle task?\n * @p: the task in question.\n *\n * Return: 1 if @p is an idle task. 0 otherwise.\n */\nstatic inline bool is_idle_task(const struct task_struct *p)\n{\n\treturn p->pid == 0;\n}\nextern struct task_struct *curr_task(int cpu);\nextern void set_curr_task(int cpu, struct task_struct *p);\n\nvoid yield(void);\n\n/*\n * The default (Linux) execution domain.\n */\nextern struct exec_domain\tdefault_exec_domain;\n\nunion thread_union {\n\tstruct thread_info thread_info;\n\tunsigned long stack[THREAD_SIZE/sizeof(long)];\n};\n\n#ifndef __HAVE_ARCH_KSTACK_END\nstatic inline int kstack_end(void *addr)\n{\n\t/* Reliable end of stack detection:\n\t * Some APM bios versions misalign the stack\n\t */\n\treturn !(((unsigned long)addr+sizeof(void*)-1) & (THREAD_SIZE-sizeof(void*)));\n}\n#endif\n\nextern union thread_union init_thread_union;\nextern struct task_struct init_task;\n\nextern struct   mm_struct init_mm;\n\nextern struct pid_namespace init_pid_ns;\n\n/*\n * find a task by one of its numerical ids\n *\n * find_task_by_pid_ns():\n *      finds a task by its pid in the specified namespace\n * find_task_by_vpid():\n *      finds a task by its virtual pid\n *\n * see also find_vpid() etc in include/linux/pid.h\n */\n\nextern struct task_struct *find_task_by_vpid(pid_t nr);\nextern struct task_struct *find_task_by_pid_ns(pid_t nr,\n\t\tstruct pid_namespace *ns);\n\n/* per-UID process charging. */\nextern struct user_struct * alloc_uid(kuid_t);\nstatic inline struct user_struct *get_uid(struct user_struct *u)\n{\n\tatomic_inc(&u->__count);\n\treturn u;\n}\nextern void free_uid(struct user_struct *);\n\n#include <asm/current.h>\n\nextern void xtime_update(unsigned long ticks);\n\nextern int wake_up_state(struct task_struct *tsk, unsigned int state);\nextern int wake_up_process(struct task_struct *tsk);\nextern void wake_up_new_task(struct task_struct *tsk);\n#ifdef CONFIG_SMP\n extern void kick_process(struct task_struct *tsk);\n#else\n static inline void kick_process(struct task_struct *tsk) { }\n#endif\nextern void sched_fork(unsigned long clone_flags, struct task_struct *p);\nextern void sched_dead(struct task_struct *p);\n\nextern void proc_caches_init(void);\nextern void flush_signals(struct task_struct *);\nextern void __flush_signals(struct task_struct *);\nextern void ignore_signals(struct task_struct *);\nextern void flush_signal_handlers(struct task_struct *, int force_default);\nextern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);\n\nstatic inline int dequeue_signal_lock(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&tsk->sighand->siglock, flags);\n\tret = dequeue_signal(tsk, mask, info);\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, flags);\n\n\treturn ret;\n}\n\nextern void block_all_signals(int (*notifier)(void *priv), void *priv,\n\t\t\t      sigset_t *mask);\nextern void unblock_all_signals(void);\nextern void release_task(struct task_struct * p);\nextern int send_sig_info(int, struct siginfo *, struct task_struct *);\nextern int force_sigsegv(int, struct task_struct *);\nextern int force_sig_info(int, struct siginfo *, struct task_struct *);\nextern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);\nextern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);\nextern int kill_pid_info_as_cred(int, struct siginfo *, struct pid *,\n\t\t\t\tconst struct cred *, u32);\nextern int kill_pgrp(struct pid *pid, int sig, int priv);\nextern int kill_pid(struct pid *pid, int sig, int priv);\nextern int kill_proc_info(int, struct siginfo *, pid_t);\nextern __must_check bool do_notify_parent(struct task_struct *, int);\nextern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);\nextern void force_sig(int, struct task_struct *);\nextern int send_sig(int, struct task_struct *, int);\nextern int zap_other_threads(struct task_struct *p);\nextern struct sigqueue *sigqueue_alloc(void);\nextern void sigqueue_free(struct sigqueue *);\nextern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);\nextern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);\n\nstatic inline void restore_saved_sigmask(void)\n{\n\tif (test_and_clear_restore_sigmask())\n\t\t__set_current_blocked(&current->saved_sigmask);\n}\n\nstatic inline sigset_t *sigmask_to_save(void)\n{\n\tsigset_t *res = &current->blocked;\n\tif (unlikely(test_restore_sigmask()))\n\t\tres = &current->saved_sigmask;\n\treturn res;\n}\n\nstatic inline int kill_cad_pid(int sig, int priv)\n{\n\treturn kill_pid(cad_pid, sig, priv);\n}\n\n/* These can be the second arg to send_sig_info/send_group_sig_info.  */\n#define SEND_SIG_NOINFO ((struct siginfo *) 0)\n#define SEND_SIG_PRIV\t((struct siginfo *) 1)\n#define SEND_SIG_FORCED\t((struct siginfo *) 2)\n\n/*\n * True if we are on the alternate signal stack.\n */\nstatic inline int on_sig_stack(unsigned long sp)\n{\n#ifdef CONFIG_STACK_GROWSUP\n\treturn sp >= current->sas_ss_sp &&\n\t\tsp - current->sas_ss_sp < current->sas_ss_size;\n#else\n\treturn sp > current->sas_ss_sp &&\n\t\tsp - current->sas_ss_sp <= current->sas_ss_size;\n#endif\n}\n\nstatic inline int sas_ss_flags(unsigned long sp)\n{\n\treturn (current->sas_ss_size == 0 ? SS_DISABLE\n\t\t: on_sig_stack(sp) ? SS_ONSTACK : 0);\n}\n\nstatic inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)\n{\n\tif (unlikely((ksig->ka.sa.sa_flags & SA_ONSTACK)) && ! sas_ss_flags(sp))\n#ifdef CONFIG_STACK_GROWSUP\n\t\treturn current->sas_ss_sp;\n#else\n\t\treturn current->sas_ss_sp + current->sas_ss_size;\n#endif\n\treturn sp;\n}\n\n/*\n * Routines for handling mm_structs\n */\nextern struct mm_struct * mm_alloc(void);\n\n/* mmdrop drops the mm and the page tables */\nextern void __mmdrop(struct mm_struct *);\nstatic inline void mmdrop(struct mm_struct * mm)\n{\n\tif (unlikely(atomic_dec_and_test(&mm->mm_count)))\n\t\t__mmdrop(mm);\n}\n\n/* mmput gets rid of the mappings and all user-space */\nextern void mmput(struct mm_struct *);\n/* Grab a reference to a task's mm, if it is not already going away */\nextern struct mm_struct *get_task_mm(struct task_struct *task);\n/*\n * Grab a reference to a task's mm, if it is not already going away\n * and ptrace_may_access with the mode parameter passed to it\n * succeeds.\n */\nextern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);\n/* Remove the current tasks stale references to the old mm_struct */\nextern void mm_release(struct task_struct *, struct mm_struct *);\n/* Allocate a new mm structure and copy contents from tsk->mm */\nextern struct mm_struct *dup_mm(struct task_struct *tsk);\n\nextern int copy_thread(unsigned long, unsigned long, unsigned long,\n\t\t\tstruct task_struct *);\nextern void flush_thread(void);\nextern void exit_thread(void);\n\nextern void exit_files(struct task_struct *);\nextern void __cleanup_sighand(struct sighand_struct *);\n\nextern void exit_itimers(struct signal_struct *);\nextern void flush_itimer_signals(void);\n\nextern void do_group_exit(int);\n\nextern int allow_signal(int);\nextern int disallow_signal(int);\n\nextern int do_execve(const char *,\n\t\t     const char __user * const __user *,\n\t\t     const char __user * const __user *);\nextern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);\nstruct task_struct *fork_idle(int);\nextern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);\n\nextern void set_task_comm(struct task_struct *tsk, char *from);\nextern char *get_task_comm(char *to, struct task_struct *tsk);\n\n#ifdef CONFIG_SMP\nvoid scheduler_ipi(void);\nextern unsigned long wait_task_inactive(struct task_struct *, long match_state);\n#else\nstatic inline void scheduler_ipi(void) { }\nstatic inline unsigned long wait_task_inactive(struct task_struct *p,\n\t\t\t\t\t       long match_state)\n{\n\treturn 1;\n}\n#endif\n\n#define next_task(p) \\\n\tlist_entry_rcu((p)->tasks.next, struct task_struct, tasks)\n\n#define for_each_process(p) \\\n\tfor (p = &init_task ; (p = next_task(p)) != &init_task ; )\n\nextern bool current_is_single_threaded(void);\n\n/*\n * Careful: do_each_thread/while_each_thread is a double loop so\n *          'break' will not work as expected - use goto instead.\n */\n#define do_each_thread(g, t) \\\n\tfor (g = t = &init_task ; (g = t = next_task(g)) != &init_task ; ) do\n\n#define while_each_thread(g, t) \\\n\twhile ((t = next_thread(t)) != g)\n\nstatic inline int get_nr_threads(struct task_struct *tsk)\n{\n\treturn tsk->signal->nr_threads;\n}\n\nstatic inline bool thread_group_leader(struct task_struct *p)\n{\n\treturn p->exit_signal >= 0;\n}\n\n/* Do to the insanities of de_thread it is possible for a process\n * to have the pid of the thread group leader without actually being\n * the thread group leader.  For iteration through the pids in proc\n * all we care about is that we have a task with the appropriate\n * pid, we don't actually care if we have the right task.\n */\nstatic inline bool has_group_leader_pid(struct task_struct *p)\n{\n\treturn task_pid(p) == p->signal->leader_pid;\n}\n\nstatic inline\nbool same_thread_group(struct task_struct *p1, struct task_struct *p2)\n{\n\treturn p1->signal == p2->signal;\n}\n\nstatic inline struct task_struct *next_thread(const struct task_struct *p)\n{\n\treturn list_entry_rcu(p->thread_group.next,\n\t\t\t      struct task_struct, thread_group);\n}\n\nstatic inline int thread_group_empty(struct task_struct *p)\n{\n\treturn list_empty(&p->thread_group);\n}\n\n#define delay_group_leader(p) \\\n\t\t(thread_group_leader(p) && !thread_group_empty(p))\n\n/*\n * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring\n * subscriptions and synchronises with wait4().  Also used in procfs.  Also\n * pins the final release of task.io_context.  Also protects ->cpuset and\n * ->cgroup.subsys[]. And ->vfork_done.\n *\n * Nests both inside and outside of read_lock(&tasklist_lock).\n * It must not be nested with write_lock_irq(&tasklist_lock),\n * neither inside nor outside.\n */\nstatic inline void task_lock(struct task_struct *p)\n{\n\tspin_lock(&p->alloc_lock);\n}\n\nstatic inline void task_unlock(struct task_struct *p)\n{\n\tspin_unlock(&p->alloc_lock);\n}\n\nextern struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t\t\tunsigned long *flags);\n\nstatic inline struct sighand_struct *lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t\t       unsigned long *flags)\n{\n\tstruct sighand_struct *ret;\n\n\tret = __lock_task_sighand(tsk, flags);\n\t(void)__cond_lock(&tsk->sighand->siglock, ret);\n\treturn ret;\n}\n\nstatic inline void unlock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t\tunsigned long *flags)\n{\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, *flags);\n}\n\n#ifdef CONFIG_CGROUPS\nstatic inline void threadgroup_change_begin(struct task_struct *tsk)\n{\n\tdown_read(&tsk->signal->group_rwsem);\n}\nstatic inline void threadgroup_change_end(struct task_struct *tsk)\n{\n\tup_read(&tsk->signal->group_rwsem);\n}\n\n/**\n * threadgroup_lock - lock threadgroup\n * @tsk: member task of the threadgroup to lock\n *\n * Lock the threadgroup @tsk belongs to.  No new task is allowed to enter\n * and member tasks aren't allowed to exit (as indicated by PF_EXITING) or\n * change ->group_leader/pid.  This is useful for cases where the threadgroup\n * needs to stay stable across blockable operations.\n *\n * fork and exit paths explicitly call threadgroup_change_{begin|end}() for\n * synchronization.  While held, no new task will be added to threadgroup\n * and no existing live task will have its PF_EXITING set.\n *\n * de_thread() does threadgroup_change_{begin|end}() when a non-leader\n * sub-thread becomes a new leader.\n */\nstatic inline void threadgroup_lock(struct task_struct *tsk)\n{\n\tdown_write(&tsk->signal->group_rwsem);\n}\n\n/**\n * threadgroup_unlock - unlock threadgroup\n * @tsk: member task of the threadgroup to unlock\n *\n * Reverse threadgroup_lock().\n */\nstatic inline void threadgroup_unlock(struct task_struct *tsk)\n{\n\tup_write(&tsk->signal->group_rwsem);\n}\n#else\nstatic inline void threadgroup_change_begin(struct task_struct *tsk) {}\nstatic inline void threadgroup_change_end(struct task_struct *tsk) {}\nstatic inline void threadgroup_lock(struct task_struct *tsk) {}\nstatic inline void threadgroup_unlock(struct task_struct *tsk) {}\n#endif\n\n#ifndef __HAVE_THREAD_FUNCTIONS\n\n#define task_thread_info(task)\t((struct thread_info *)(task)->stack)\n#define task_stack_page(task)\t((task)->stack)\n\nstatic inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)\n{\n\t*task_thread_info(p) = *task_thread_info(org);\n\ttask_thread_info(p)->task = p;\n}\n\nstatic inline unsigned long *end_of_stack(struct task_struct *p)\n{\n\treturn (unsigned long *)(task_thread_info(p) + 1);\n}\n\n#endif\n\nstatic inline int object_is_on_stack(void *obj)\n{\n\tvoid *stack = task_stack_page(current);\n\n\treturn (obj >= stack) && (obj < (stack + THREAD_SIZE));\n}\n\nextern void thread_info_cache_init(void);\n\n#ifdef CONFIG_DEBUG_STACK_USAGE\nstatic inline unsigned long stack_not_used(struct task_struct *p)\n{\n\tunsigned long *n = end_of_stack(p);\n\n\tdo { \t/* Skip over canary */\n\t\tn++;\n\t} while (!*n);\n\n\treturn (unsigned long)n - (unsigned long)end_of_stack(p);\n}\n#endif\n\n/* set thread flags in other task's structures\n * - see asm/thread_info.h for TIF_xxxx flags available\n */\nstatic inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tset_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tclear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_set_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void set_tsk_need_resched(struct task_struct *tsk)\n{\n\tset_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline void clear_tsk_need_resched(struct task_struct *tsk)\n{\n\tclear_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline int test_tsk_need_resched(struct task_struct *tsk)\n{\n\treturn unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));\n}\n\nstatic inline int restart_syscall(void)\n{\n\tset_tsk_thread_flag(current, TIF_SIGPENDING);\n\treturn -ERESTARTNOINTR;\n}\n\nstatic inline int signal_pending(struct task_struct *p)\n{\n\treturn unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));\n}\n\nstatic inline int __fatal_signal_pending(struct task_struct *p)\n{\n\treturn unlikely(sigismember(&p->pending.signal, SIGKILL));\n}\n\nstatic inline int fatal_signal_pending(struct task_struct *p)\n{\n\treturn signal_pending(p) && __fatal_signal_pending(p);\n}\n\nstatic inline int signal_pending_state(long state, struct task_struct *p)\n{\n\tif (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))\n\t\treturn 0;\n\tif (!signal_pending(p))\n\t\treturn 0;\n\n\treturn (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);\n}\n\n/*\n * cond_resched() and cond_resched_lock(): latency reduction via\n * explicit rescheduling in places that are safe. The return\n * value indicates whether a reschedule was done in fact.\n * cond_resched_lock() will drop the spinlock before scheduling,\n * cond_resched_softirq() will enable bhs before scheduling.\n */\nextern int _cond_resched(void);\n\n#define cond_resched() ({\t\t\t\\\n\t__might_sleep(__FILE__, __LINE__, 0);\t\\\n\t_cond_resched();\t\t\t\\\n})\n\nextern int __cond_resched_lock(spinlock_t *lock);\n\n#ifdef CONFIG_PREEMPT_COUNT\n#define PREEMPT_LOCK_OFFSET\tPREEMPT_OFFSET\n#else\n#define PREEMPT_LOCK_OFFSET\t0\n#endif\n\n#define cond_resched_lock(lock) ({\t\t\t\t\\\n\t__might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);\t\\\n\t__cond_resched_lock(lock);\t\t\t\t\\\n})\n\nextern int __cond_resched_softirq(void);\n\n#define cond_resched_softirq() ({\t\t\t\t\t\\\n\t__might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);\t\\\n\t__cond_resched_softirq();\t\t\t\t\t\\\n})\n\nstatic inline void cond_resched_rcu(void)\n{\n#if defined(CONFIG_DEBUG_ATOMIC_SLEEP) || !defined(CONFIG_PREEMPT_RCU)\n\trcu_read_unlock();\n\tcond_resched();\n\trcu_read_lock();\n#endif\n}\n\n/*\n * Does a critical section need to be broken due to another\n * task waiting?: (technically does not depend on CONFIG_PREEMPT,\n * but a general need for low latency)\n */\nstatic inline int spin_needbreak(spinlock_t *lock)\n{\n#ifdef CONFIG_PREEMPT\n\treturn spin_is_contended(lock);\n#else\n\treturn 0;\n#endif\n}\n\n/*\n * Idle thread specific functions to determine the need_resched\n * polling state. We have two versions, one based on TS_POLLING in\n * thread_info.status and one based on TIF_POLLING_NRFLAG in\n * thread_info.flags\n */\n#ifdef TS_POLLING\nstatic inline int tsk_is_polling(struct task_struct *p)\n{\n\treturn task_thread_info(p)->status & TS_POLLING;\n}\nstatic inline void __current_set_polling(void)\n{\n\tcurrent_thread_info()->status |= TS_POLLING;\n}\n\nstatic inline bool __must_check current_set_polling_and_test(void)\n{\n\t__current_set_polling();\n\n\t/*\n\t * Polling state must be visible before we test NEED_RESCHED,\n\t * paired by resched_task()\n\t */\n\tsmp_mb();\n\n\treturn unlikely(tif_need_resched());\n}\n\nstatic inline void __current_clr_polling(void)\n{\n\tcurrent_thread_info()->status &= ~TS_POLLING;\n}\n\nstatic inline bool __must_check current_clr_polling_and_test(void)\n{\n\t__current_clr_polling();\n\n\t/*\n\t * Polling state must be visible before we test NEED_RESCHED,\n\t * paired by resched_task()\n\t */\n\tsmp_mb();\n\n\treturn unlikely(tif_need_resched());\n}\n#elif defined(TIF_POLLING_NRFLAG)\nstatic inline int tsk_is_polling(struct task_struct *p)\n{\n\treturn test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);\n}\n\nstatic inline void __current_set_polling(void)\n{\n\tset_thread_flag(TIF_POLLING_NRFLAG);\n}\n\nstatic inline bool __must_check current_set_polling_and_test(void)\n{\n\t__current_set_polling();\n\n\t/*\n\t * Polling state must be visible before we test NEED_RESCHED,\n\t * paired by resched_task()\n\t *\n\t * XXX: assumes set/clear bit are identical barrier wise.\n\t */\n\tsmp_mb__after_clear_bit();\n\n\treturn unlikely(tif_need_resched());\n}\n\nstatic inline void __current_clr_polling(void)\n{\n\tclear_thread_flag(TIF_POLLING_NRFLAG);\n}\n\nstatic inline bool __must_check current_clr_polling_and_test(void)\n{\n\t__current_clr_polling();\n\n\t/*\n\t * Polling state must be visible before we test NEED_RESCHED,\n\t * paired by resched_task()\n\t */\n\tsmp_mb__after_clear_bit();\n\n\treturn unlikely(tif_need_resched());\n}\n\n#else\nstatic inline int tsk_is_polling(struct task_struct *p) { return 0; }\nstatic inline void __current_set_polling(void) { }\nstatic inline void __current_clr_polling(void) { }\n\nstatic inline bool __must_check current_set_polling_and_test(void)\n{\n\treturn unlikely(tif_need_resched());\n}\nstatic inline bool __must_check current_clr_polling_and_test(void)\n{\n\treturn unlikely(tif_need_resched());\n}\n#endif\n\nstatic __always_inline bool need_resched(void)\n{\n\treturn unlikely(tif_need_resched());\n}\n\n/*\n * Thread group CPU time accounting.\n */\nvoid thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);\nvoid thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);\n\nstatic inline void thread_group_cputime_init(struct signal_struct *sig)\n{\n\traw_spin_lock_init(&sig->cputimer.lock);\n}\n\n/*\n * Reevaluate whether the task has signals pending delivery.\n * Wake the task if so.\n * This is required every time the blocked sigset_t changes.\n * callers must hold sighand->siglock.\n */\nextern void recalc_sigpending_and_wake(struct task_struct *t);\nextern void recalc_sigpending(void);\n\nextern void signal_wake_up_state(struct task_struct *t, unsigned int state);\n\nstatic inline void signal_wake_up(struct task_struct *t, bool resume)\n{\n\tsignal_wake_up_state(t, resume ? TASK_WAKEKILL : 0);\n}\nstatic inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)\n{\n\tsignal_wake_up_state(t, resume ? __TASK_TRACED : 0);\n}\n\n/*\n * Wrappers for p->thread_info->cpu access. No-op on UP.\n */\n#ifdef CONFIG_SMP\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n\treturn task_thread_info(p)->cpu;\n}\n\nstatic inline int task_node(const struct task_struct *p)\n{\n\treturn cpu_to_node(task_cpu(p));\n}\n\nextern void set_task_cpu(struct task_struct *p, unsigned int cpu);\n\n#else\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n\treturn 0;\n}\n\nstatic inline void set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n}\n\n#endif /* CONFIG_SMP */\n\nextern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);\nextern long sched_getaffinity(pid_t pid, struct cpumask *mask);\n\n#ifdef CONFIG_CGROUP_SCHED\nextern struct task_group root_task_group;\n#endif /* CONFIG_CGROUP_SCHED */\n\nextern int task_can_switch_user(struct user_struct *up,\n\t\t\t\t\tstruct task_struct *tsk);\n\n#ifdef CONFIG_TASK_XACCT\nstatic inline void add_rchar(struct task_struct *tsk, ssize_t amt)\n{\n\ttsk->ioac.rchar += amt;\n}\n\nstatic inline void add_wchar(struct task_struct *tsk, ssize_t amt)\n{\n\ttsk->ioac.wchar += amt;\n}\n\nstatic inline void inc_syscr(struct task_struct *tsk)\n{\n\ttsk->ioac.syscr++;\n}\n\nstatic inline void inc_syscw(struct task_struct *tsk)\n{\n\ttsk->ioac.syscw++;\n}\n#else\nstatic inline void add_rchar(struct task_struct *tsk, ssize_t amt)\n{\n}\n\nstatic inline void add_wchar(struct task_struct *tsk, ssize_t amt)\n{\n}\n\nstatic inline void inc_syscr(struct task_struct *tsk)\n{\n}\n\nstatic inline void inc_syscw(struct task_struct *tsk)\n{\n}\n#endif\n\n#ifndef TASK_SIZE_OF\n#define TASK_SIZE_OF(tsk)\tTASK_SIZE\n#endif\n\n#ifdef CONFIG_MM_OWNER\nextern void mm_update_next_owner(struct mm_struct *mm);\nextern void mm_init_owner(struct mm_struct *mm, struct task_struct *p);\n#else\nstatic inline void mm_update_next_owner(struct mm_struct *mm)\n{\n}\n\nstatic inline void mm_init_owner(struct mm_struct *mm, struct task_struct *p)\n{\n}\n#endif /* CONFIG_MM_OWNER */\n\nstatic inline unsigned long task_rlimit(const struct task_struct *tsk,\n\t\tunsigned int limit)\n{\n\treturn ACCESS_ONCE(tsk->signal->rlim[limit].rlim_cur);\n}\n\nstatic inline unsigned long task_rlimit_max(const struct task_struct *tsk,\n\t\tunsigned int limit)\n{\n\treturn ACCESS_ONCE(tsk->signal->rlim[limit].rlim_max);\n}\n\nstatic inline unsigned long rlimit(unsigned int limit)\n{\n\treturn task_rlimit(current, limit);\n}\n\nstatic inline unsigned long rlimit_max(unsigned int limit)\n{\n\treturn task_rlimit_max(current, limit);\n}\n\n#endif\n", "/*\n * linux/kernel/ptrace.c\n *\n * (C) Copyright 1999 Linus Torvalds\n *\n * Common interfaces for \"ptrace()\" which we do not want\n * to continually duplicate across every architecture.\n */\n\n#include <linux/capability.h>\n#include <linux/export.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/ptrace.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/uio.h>\n#include <linux/audit.h>\n#include <linux/pid_namespace.h>\n#include <linux/syscalls.h>\n#include <linux/uaccess.h>\n#include <linux/regset.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/cn_proc.h>\n#include <linux/compat.h>\n\n\nstatic int ptrace_trapping_sleep_fn(void *flags)\n{\n\tschedule();\n\treturn 0;\n}\n\n/*\n * ptrace a task: make the debugger its new parent and\n * move it to the ptrace list.\n *\n * Must be called with the tasklist lock write-held.\n */\nvoid __ptrace_link(struct task_struct *child, struct task_struct *new_parent)\n{\n\tBUG_ON(!list_empty(&child->ptrace_entry));\n\tlist_add(&child->ptrace_entry, &new_parent->ptraced);\n\tchild->parent = new_parent;\n}\n\n/**\n * __ptrace_unlink - unlink ptracee and restore its execution state\n * @child: ptracee to be unlinked\n *\n * Remove @child from the ptrace list, move it back to the original parent,\n * and restore the execution state so that it conforms to the group stop\n * state.\n *\n * Unlinking can happen via two paths - explicit PTRACE_DETACH or ptracer\n * exiting.  For PTRACE_DETACH, unless the ptracee has been killed between\n * ptrace_check_attach() and here, it's guaranteed to be in TASK_TRACED.\n * If the ptracer is exiting, the ptracee can be in any state.\n *\n * After detach, the ptracee should be in a state which conforms to the\n * group stop.  If the group is stopped or in the process of stopping, the\n * ptracee should be put into TASK_STOPPED; otherwise, it should be woken\n * up from TASK_TRACED.\n *\n * If the ptracee is in TASK_TRACED and needs to be moved to TASK_STOPPED,\n * it goes through TRACED -> RUNNING -> STOPPED transition which is similar\n * to but in the opposite direction of what happens while attaching to a\n * stopped task.  However, in this direction, the intermediate RUNNING\n * state is not hidden even from the current ptracer and if it immediately\n * re-attaches and performs a WNOHANG wait(2), it may fail.\n *\n * CONTEXT:\n * write_lock_irq(tasklist_lock)\n */\nvoid __ptrace_unlink(struct task_struct *child)\n{\n\tBUG_ON(!child->ptrace);\n\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\n\tspin_lock(&child->sighand->siglock);\n\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tptrace_signal_wake_up(child, true);\n\n\tspin_unlock(&child->sighand->siglock);\n}\n\n/* Ensure that nothing can wake it up, even SIGKILL */\nstatic bool ptrace_freeze_traced(struct task_struct *task)\n{\n\tbool ret = false;\n\n\t/* Lockless, nobody but us can set this flag */\n\tif (task->jobctl & JOBCTL_LISTENING)\n\t\treturn ret;\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (task_is_traced(task) && !__fatal_signal_pending(task)) {\n\t\ttask->state = __TASK_TRACED;\n\t\tret = true;\n\t}\n\tspin_unlock_irq(&task->sighand->siglock);\n\n\treturn ret;\n}\n\nstatic void ptrace_unfreeze_traced(struct task_struct *task)\n{\n\tif (task->state != __TASK_TRACED)\n\t\treturn;\n\n\tWARN_ON(!task->ptrace || task->parent != current);\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (__fatal_signal_pending(task))\n\t\twake_up_state(task, __TASK_TRACED);\n\telse\n\t\ttask->state = TASK_TRACED;\n\tspin_unlock_irq(&task->sighand->siglock);\n}\n\n/**\n * ptrace_check_attach - check whether ptracee is ready for ptrace operation\n * @child: ptracee to check for\n * @ignore_state: don't check whether @child is currently %TASK_TRACED\n *\n * Check whether @child is being ptraced by %current and ready for further\n * ptrace operations.  If @ignore_state is %false, @child also should be in\n * %TASK_TRACED state and on return the child is guaranteed to be traced\n * and not executing.  If @ignore_state is %true, @child can be in any\n * state.\n *\n * CONTEXT:\n * Grabs and releases tasklist_lock and @child->sighand->siglock.\n *\n * RETURNS:\n * 0 on success, -ESRCH if %child is not ready.\n */\nstatic int ptrace_check_attach(struct task_struct *child, bool ignore_state)\n{\n\tint ret = -ESRCH;\n\n\t/*\n\t * We take the read lock around doing both checks to close a\n\t * possible race where someone else was tracing our child and\n\t * detached between these two checks.  After this locked check,\n\t * we are sure that this is our traced child and that can only\n\t * be changed by us so it's not changing right after this.\n\t */\n\tread_lock(&tasklist_lock);\n\tif (child->ptrace && child->parent == current) {\n\t\tWARN_ON(child->state == __TASK_TRACED);\n\t\t/*\n\t\t * child->sighand can't be NULL, release_task()\n\t\t * does ptrace_unlink() before __exit_signal().\n\t\t */\n\t\tif (ignore_state || ptrace_freeze_traced(child))\n\t\t\tret = 0;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tif (!ret && !ignore_state) {\n\t\tif (!wait_task_inactive(child, __TASK_TRACED)) {\n\t\t\t/*\n\t\t\t * This can only happen if may_ptrace_stop() fails and\n\t\t\t * ptrace_stop() changes ->state back to TASK_RUNNING,\n\t\t\t * so we should not worry about leaking __TASK_TRACED.\n\t\t\t */\n\t\t\tWARN_ON(child->state == __TASK_TRACED);\n\t\t\tret = -ESRCH;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int ptrace_has_cap(struct user_namespace *ns, unsigned int mode)\n{\n\tif (mode & PTRACE_MODE_NOAUDIT)\n\t\treturn has_ns_capability_noaudit(current, ns, CAP_SYS_PTRACE);\n\telse\n\t\treturn has_ns_capability(current, ns, CAP_SYS_PTRACE);\n}\n\n/* Returns 0 on success, -errno on denial. */\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\tint dumpable = 0;\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\ttcred = __task_cred(task);\n\tif (uid_eq(cred->uid, tcred->euid) &&\n\t    uid_eq(cred->uid, tcred->suid) &&\n\t    uid_eq(cred->uid, tcred->uid)  &&\n\t    gid_eq(cred->gid, tcred->egid) &&\n\t    gid_eq(cred->gid, tcred->sgid) &&\n\t    gid_eq(cred->gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tsmp_rmb();\n\tif (task->mm)\n\t\tdumpable = get_dumpable(task->mm);\n\trcu_read_lock();\n\tif (!dumpable && !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn security_ptrace_access_check(task, mode);\n}\n\nbool ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tint err;\n\ttask_lock(task);\n\terr = __ptrace_may_access(task, mode);\n\ttask_unlock(task);\n\treturn !err;\n}\n\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT,\n\t\t\t    ptrace_trapping_sleep_fn, TASK_UNINTERRUPTIBLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n\n/**\n * ptrace_traceme  --  helper for PTRACE_TRACEME\n *\n * Performs checks and sets PT_PTRACED.\n * Should be used by all ptrace implementations for PTRACE_TRACEME.\n */\nstatic int ptrace_traceme(void)\n{\n\tint ret = -EPERM;\n\n\twrite_lock_irq(&tasklist_lock);\n\t/* Are we already being traced? */\n\tif (!current->ptrace) {\n\t\tret = security_ptrace_traceme(current->parent);\n\t\t/*\n\t\t * Check PF_EXITING to ensure ->real_parent has not passed\n\t\t * exit_ptrace(). Otherwise we don't report the error but\n\t\t * pretend ->real_parent untraces us right after return.\n\t\t */\n\t\tif (!ret && !(current->real_parent->flags & PF_EXITING)) {\n\t\t\tcurrent->ptrace = PT_PTRACED;\n\t\t\t__ptrace_link(current, current->real_parent);\n\t\t}\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * Called with irqs disabled, returns true if childs should reap themselves.\n */\nstatic int ignoring_children(struct sighand_struct *sigh)\n{\n\tint ret;\n\tspin_lock(&sigh->siglock);\n\tret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||\n\t      (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);\n\tspin_unlock(&sigh->siglock);\n\treturn ret;\n}\n\n/*\n * Called with tasklist_lock held for writing.\n * Unlink a traced task, and clean it up if it was a traced zombie.\n * Return true if it needs to be reaped with release_task().\n * (We can't call release_task() here because we already hold tasklist_lock.)\n *\n * If it's a zombie, our attachedness prevented normal parent notification\n * or self-reaping.  Do notification now if it would have happened earlier.\n * If it should reap itself, return true.\n *\n * If it's our own child, there is no notification to do. But if our normal\n * children self-reap, then this child was prevented by ptrace and we must\n * reap it now, in that case we must also wake up sub-threads sleeping in\n * do_wait().\n */\nstatic bool __ptrace_detach(struct task_struct *tracer, struct task_struct *p)\n{\n\tbool dead;\n\n\t__ptrace_unlink(p);\n\n\tif (p->exit_state != EXIT_ZOMBIE)\n\t\treturn false;\n\n\tdead = !thread_group_leader(p);\n\n\tif (!dead && thread_group_empty(p)) {\n\t\tif (!same_thread_group(p->real_parent, tracer))\n\t\t\tdead = do_notify_parent(p, p->exit_signal);\n\t\telse if (ignoring_children(tracer->sighand)) {\n\t\t\t__wake_up_parent(p, tracer);\n\t\t\tdead = true;\n\t\t}\n\t}\n\t/* Mark it as in the process of being reaped. */\n\tif (dead)\n\t\tp->exit_state = EXIT_DEAD;\n\treturn dead;\n}\n\nstatic int ptrace_detach(struct task_struct *child, unsigned int data)\n{\n\tbool dead = false;\n\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\t/* Architecture-specific hardware disable .. */\n\tptrace_disable(child);\n\tclear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\n\twrite_lock_irq(&tasklist_lock);\n\t/*\n\t * This child can be already killed. Make sure de_thread() or\n\t * our sub-thread doing do_wait() didn't do release_task() yet.\n\t */\n\tif (child->ptrace) {\n\t\tchild->exit_code = data;\n\t\tdead = __ptrace_detach(current, child);\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_ptrace_connector(child, PTRACE_DETACH);\n\tif (unlikely(dead))\n\t\trelease_task(child);\n\n\treturn 0;\n}\n\n/*\n * Detach all tasks we were using ptrace on. Called with tasklist held\n * for writing, and returns with it held too. But note it can release\n * and reacquire the lock.\n */\nvoid exit_ptrace(struct task_struct *tracer)\n\t__releases(&tasklist_lock)\n\t__acquires(&tasklist_lock)\n{\n\tstruct task_struct *p, *n;\n\tLIST_HEAD(ptrace_dead);\n\n\tif (likely(list_empty(&tracer->ptraced)))\n\t\treturn;\n\n\tlist_for_each_entry_safe(p, n, &tracer->ptraced, ptrace_entry) {\n\t\tif (unlikely(p->ptrace & PT_EXITKILL))\n\t\t\tsend_sig_info(SIGKILL, SEND_SIG_FORCED, p);\n\n\t\tif (__ptrace_detach(tracer, p))\n\t\t\tlist_add(&p->ptrace_entry, &ptrace_dead);\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\tBUG_ON(!list_empty(&tracer->ptraced));\n\n\tlist_for_each_entry_safe(p, n, &ptrace_dead, ptrace_entry) {\n\t\tlist_del_init(&p->ptrace_entry);\n\t\trelease_task(p);\n\t}\n\n\twrite_lock_irq(&tasklist_lock);\n}\n\nint ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tretval = access_process_vm(tsk, src, buf, this_len, 0);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tif (copy_to_user(dst, buf, retval))\n\t\t\treturn -EFAULT;\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nint ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tif (copy_from_user(buf, src, this_len))\n\t\t\treturn -EFAULT;\n\t\tretval = access_process_vm(tsk, dst, buf, this_len, 1);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nstatic int ptrace_setoptions(struct task_struct *child, unsigned long data)\n{\n\tunsigned flags;\n\n\tif (data & ~(unsigned long)PTRACE_O_MASK)\n\t\treturn -EINVAL;\n\n\t/* Avoid intermediate state when all opts are cleared */\n\tflags = child->ptrace;\n\tflags &= ~(PTRACE_O_MASK << PT_OPT_FLAG_SHIFT);\n\tflags |= (data << PT_OPT_FLAG_SHIFT);\n\tchild->ptrace = flags;\n\n\treturn 0;\n}\n\nstatic int ptrace_getsiginfo(struct task_struct *child, siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\t*info = *child->last_siginfo;\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_setsiginfo(struct task_struct *child, const siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\t*child->last_siginfo = *info;\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_peek_siginfo(struct task_struct *child,\n\t\t\t\tunsigned long addr,\n\t\t\t\tunsigned long data)\n{\n\tstruct ptrace_peeksiginfo_args arg;\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint ret, i;\n\n\tret = copy_from_user(&arg, (void __user *) addr,\n\t\t\t\tsizeof(struct ptrace_peeksiginfo_args));\n\tif (ret)\n\t\treturn -EFAULT;\n\n\tif (arg.flags & ~PTRACE_PEEKSIGINFO_SHARED)\n\t\treturn -EINVAL; /* unknown flags */\n\n\tif (arg.nr < 0)\n\t\treturn -EINVAL;\n\n\tif (arg.flags & PTRACE_PEEKSIGINFO_SHARED)\n\t\tpending = &child->signal->shared_pending;\n\telse\n\t\tpending = &child->pending;\n\n\tfor (i = 0; i < arg.nr; ) {\n\t\tsiginfo_t info;\n\t\ts32 off = arg.off + i;\n\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tlist_for_each_entry(q, &pending->list, list) {\n\t\t\tif (!off--) {\n\t\t\t\tcopy_siginfo(&info, &q->info);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\t\tif (off >= 0) /* beyond the end of the list */\n\t\t\tbreak;\n\n#ifdef CONFIG_COMPAT\n\t\tif (unlikely(is_compat_task())) {\n\t\t\tcompat_siginfo_t __user *uinfo = compat_ptr(data);\n\n\t\t\tif (copy_siginfo_to_user32(uinfo, &info) ||\n\t\t\t    __put_user(info.si_code, &uinfo->si_code)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t} else\n#endif\n\t\t{\n\t\t\tsiginfo_t __user *uinfo = (siginfo_t __user *) data;\n\n\t\t\tif (copy_siginfo_to_user(uinfo, &info) ||\n\t\t\t    __put_user(info.si_code, &uinfo->si_code)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tdata += sizeof(siginfo_t);\n\t\ti++;\n\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (i > 0)\n\t\treturn i;\n\n\treturn ret;\n}\n\n#ifdef PTRACE_SINGLESTEP\n#define is_singlestep(request)\t\t((request) == PTRACE_SINGLESTEP)\n#else\n#define is_singlestep(request)\t\t0\n#endif\n\n#ifdef PTRACE_SINGLEBLOCK\n#define is_singleblock(request)\t\t((request) == PTRACE_SINGLEBLOCK)\n#else\n#define is_singleblock(request)\t\t0\n#endif\n\n#ifdef PTRACE_SYSEMU\n#define is_sysemu_singlestep(request)\t((request) == PTRACE_SYSEMU_SINGLESTEP)\n#else\n#define is_sysemu_singlestep(request)\t0\n#endif\n\nstatic int ptrace_resume(struct task_struct *child, long request,\n\t\t\t unsigned long data)\n{\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\tif (request == PTRACE_SYSCALL)\n\t\tset_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\telse\n\t\tclear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\n#ifdef TIF_SYSCALL_EMU\n\tif (request == PTRACE_SYSEMU || request == PTRACE_SYSEMU_SINGLESTEP)\n\t\tset_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n\telse\n\t\tclear_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n#endif\n\n\tif (is_singleblock(request)) {\n\t\tif (unlikely(!arch_has_block_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_block_step(child);\n\t} else if (is_singlestep(request) || is_sysemu_singlestep(request)) {\n\t\tif (unlikely(!arch_has_single_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_single_step(child);\n\t} else {\n\t\tuser_disable_single_step(child);\n\t}\n\n\tchild->exit_code = data;\n\twake_up_state(child, __TASK_TRACED);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\nstatic const struct user_regset *\nfind_regset(const struct user_regset_view *view, unsigned int type)\n{\n\tconst struct user_regset *regset;\n\tint n;\n\n\tfor (n = 0; n < view->n; ++n) {\n\t\tregset = view->regsets + n;\n\t\tif (regset->core_note_type == type)\n\t\t\treturn regset;\n\t}\n\n\treturn NULL;\n}\n\nstatic int ptrace_regset(struct task_struct *task, int req, unsigned int type,\n\t\t\t struct iovec *kiov)\n{\n\tconst struct user_regset_view *view = task_user_regset_view(task);\n\tconst struct user_regset *regset = find_regset(view, type);\n\tint regset_no;\n\n\tif (!regset || (kiov->iov_len % regset->size) != 0)\n\t\treturn -EINVAL;\n\n\tregset_no = regset - view->regsets;\n\tkiov->iov_len = min(kiov->iov_len,\n\t\t\t    (__kernel_size_t) (regset->n * regset->size));\n\n\tif (req == PTRACE_GETREGSET)\n\t\treturn copy_regset_to_user(task, view, regset_no, 0,\n\t\t\t\t\t   kiov->iov_len, kiov->iov_base);\n\telse\n\t\treturn copy_regset_from_user(task, view, regset_no, 0,\n\t\t\t\t\t     kiov->iov_len, kiov->iov_base);\n}\n\n/*\n * This is declared in linux/regset.h and defined in machine-dependent\n * code.  We put the export here, near the primary machine-neutral use,\n * to ensure no machine forgets it.\n */\nEXPORT_SYMBOL_GPL(task_user_regset_view);\n#endif\n\nint ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tsiginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_PEEKSIGINFO:\n\t\tret = ptrace_peek_siginfo(child, addr, data);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tif (copy_from_user(&siginfo, datavp, sizeof siginfo))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGMASK:\n\t\tif (addr != sizeof(sigset_t)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_to_user(datavp, &child->blocked, sizeof(sigset_t)))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\tbreak;\n\n\tcase PTRACE_SETSIGMASK: {\n\t\tsigset_t new_set;\n\n\t\tif (addr != sizeof(sigset_t)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_from_user(&new_set, datavp, sizeof(sigset_t))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\t/*\n\t\t * Every thread does recalc_sigpending() after resume, so\n\t\t * retarget_shared_pending() and recalc_sigpending() are not\n\t\t * called here.\n\t\t */\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tchild->blocked = new_set;\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tptrace_signal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET: {\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic struct task_struct *ptrace_get_task_struct(pid_t pid)\n{\n\tstruct task_struct *child;\n\n\trcu_read_lock();\n\tchild = find_task_by_vpid(pid);\n\tif (child)\n\t\tget_task_struct(child);\n\trcu_read_unlock();\n\n\tif (!child)\n\t\treturn ERR_PTR(-ESRCH);\n\treturn child;\n}\n\n#ifndef arch_ptrace_attach\n#define arch_ptrace_attach(child)\tdo { } while (0)\n#endif\n\nSYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,\n\t\tunsigned long, data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(current);\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (ret < 0)\n\t\tgoto out_put_task_struct;\n\n\tret = arch_ptrace(child, request, addr, data);\n\tif (ret || request != PTRACE_DETACH)\n\t\tptrace_unfreeze_traced(child);\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n\nint generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tunsigned long tmp;\n\tint copied;\n\n\tcopied = access_process_vm(tsk, addr, &tmp, sizeof(tmp), 0);\n\tif (copied != sizeof(tmp))\n\t\treturn -EIO;\n\treturn put_user(tmp, (unsigned long __user *)data);\n}\n\nint generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tint copied;\n\n\tcopied = access_process_vm(tsk, addr, &data, sizeof(data), 1);\n\treturn (copied == sizeof(data)) ? 0 : -EIO;\n}\n\n#if defined CONFIG_COMPAT\n#include <linux/compat.h>\n\nint compat_ptrace_request(struct task_struct *child, compat_long_t request,\n\t\t\t  compat_ulong_t addr, compat_ulong_t data)\n{\n\tcompat_ulong_t __user *datap = compat_ptr(data);\n\tcompat_ulong_t word;\n\tsiginfo_t siginfo;\n\tint ret;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\tret = access_process_vm(child, addr, &word, sizeof(word), 0);\n\t\tif (ret != sizeof(word))\n\t\t\tret = -EIO;\n\t\telse\n\t\t\tret = put_user(word, datap);\n\t\tbreak;\n\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\tret = access_process_vm(child, addr, &data, sizeof(data), 1);\n\t\tret = (ret != sizeof(data) ? -EIO : 0);\n\t\tbreak;\n\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user((compat_ulong_t) child->ptrace_message, datap);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user32(\n\t\t\t\t(struct compat_siginfo __user *) datap,\n\t\t\t\t&siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tmemset(&siginfo, 0, sizeof siginfo);\n\t\tif (copy_siginfo_from_user32(\n\t\t\t    &siginfo, (struct compat_siginfo __user *) datap))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct compat_iovec __user *uiov =\n\t\t\t(struct compat_iovec __user *) datap;\n\t\tcompat_uptr_t ptr;\n\t\tcompat_size_t len;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(ptr, &uiov->iov_base) ||\n\t\t    __get_user(len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tkiov.iov_base = compat_ptr(ptr);\n\t\tkiov.iov_len = len;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t}\n\n\treturn ret;\n}\n\nasmlinkage long compat_sys_ptrace(compat_long_t request, compat_long_t pid,\n\t\t\t\t  compat_long_t addr, compat_long_t data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (!ret) {\n\t\tret = compat_arch_ptrace(child, request, addr, data);\n\t\tif (ret || request != PTRACE_DETACH)\n\t\t\tptrace_unfreeze_traced(child);\n\t}\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n#endif\t/* CONFIG_COMPAT */\n"], "fixing_code": ["#ifndef _ASM_IA64_PROCESSOR_H\n#define _ASM_IA64_PROCESSOR_H\n\n/*\n * Copyright (C) 1998-2004 Hewlett-Packard Co\n *\tDavid Mosberger-Tang <davidm@hpl.hp.com>\n *\tStephane Eranian <eranian@hpl.hp.com>\n * Copyright (C) 1999 Asit Mallick <asit.k.mallick@intel.com>\n * Copyright (C) 1999 Don Dugger <don.dugger@intel.com>\n *\n * 11/24/98\tS.Eranian\tadded ia64_set_iva()\n * 12/03/99\tD. Mosberger\timplement thread_saved_pc() via kernel unwind API\n * 06/16/00\tA. Mallick\tadded csd/ssd/tssd for ia32 support\n */\n\n\n#include <asm/intrinsics.h>\n#include <asm/kregs.h>\n#include <asm/ptrace.h>\n#include <asm/ustack.h>\n\n#define __ARCH_WANT_UNLOCKED_CTXSW\n#define ARCH_HAS_PREFETCH_SWITCH_STACK\n\n#define IA64_NUM_PHYS_STACK_REG\t96\n#define IA64_NUM_DBG_REGS\t8\n\n#define DEFAULT_MAP_BASE\t__IA64_UL_CONST(0x2000000000000000)\n#define DEFAULT_TASK_SIZE\t__IA64_UL_CONST(0xa000000000000000)\n\n/*\n * TASK_SIZE really is a mis-named.  It really is the maximum user\n * space address (plus one).  On IA-64, there are five regions of 2TB\n * each (assuming 8KB page size), for a total of 8TB of user virtual\n * address space.\n */\n#define TASK_SIZE       \tDEFAULT_TASK_SIZE\n\n/*\n * This decides where the kernel will search for a free chunk of vm\n * space during mmap's.\n */\n#define TASK_UNMAPPED_BASE\t(current->thread.map_base)\n\n#define IA64_THREAD_FPH_VALID\t(__IA64_UL(1) << 0)\t/* floating-point high state valid? */\n#define IA64_THREAD_DBG_VALID\t(__IA64_UL(1) << 1)\t/* debug registers valid? */\n#define IA64_THREAD_PM_VALID\t(__IA64_UL(1) << 2)\t/* performance registers valid? */\n#define IA64_THREAD_UAC_NOPRINT\t(__IA64_UL(1) << 3)\t/* don't log unaligned accesses */\n#define IA64_THREAD_UAC_SIGBUS\t(__IA64_UL(1) << 4)\t/* generate SIGBUS on unaligned acc. */\n#define IA64_THREAD_MIGRATION\t(__IA64_UL(1) << 5)\t/* require migration\n\t\t\t\t\t\t\t   sync at ctx sw */\n#define IA64_THREAD_FPEMU_NOPRINT (__IA64_UL(1) << 6)\t/* don't log any fpswa faults */\n#define IA64_THREAD_FPEMU_SIGFPE  (__IA64_UL(1) << 7)\t/* send a SIGFPE for fpswa faults */\n\n#define IA64_THREAD_UAC_SHIFT\t3\n#define IA64_THREAD_UAC_MASK\t(IA64_THREAD_UAC_NOPRINT | IA64_THREAD_UAC_SIGBUS)\n#define IA64_THREAD_FPEMU_SHIFT\t6\n#define IA64_THREAD_FPEMU_MASK\t(IA64_THREAD_FPEMU_NOPRINT | IA64_THREAD_FPEMU_SIGFPE)\n\n\n/*\n * This shift should be large enough to be able to represent 1000000000/itc_freq with good\n * accuracy while being small enough to fit 10*1000000000<<IA64_NSEC_PER_CYC_SHIFT in 64 bits\n * (this will give enough slack to represent 10 seconds worth of time as a scaled number).\n */\n#define IA64_NSEC_PER_CYC_SHIFT\t30\n\n#ifndef __ASSEMBLY__\n\n#include <linux/cache.h>\n#include <linux/compiler.h>\n#include <linux/threads.h>\n#include <linux/types.h>\n\n#include <asm/fpu.h>\n#include <asm/page.h>\n#include <asm/percpu.h>\n#include <asm/rse.h>\n#include <asm/unwind.h>\n#include <linux/atomic.h>\n#ifdef CONFIG_NUMA\n#include <asm/nodedata.h>\n#endif\n\n/* like above but expressed as bitfields for more efficient access: */\nstruct ia64_psr {\n\t__u64 reserved0 : 1;\n\t__u64 be : 1;\n\t__u64 up : 1;\n\t__u64 ac : 1;\n\t__u64 mfl : 1;\n\t__u64 mfh : 1;\n\t__u64 reserved1 : 7;\n\t__u64 ic : 1;\n\t__u64 i : 1;\n\t__u64 pk : 1;\n\t__u64 reserved2 : 1;\n\t__u64 dt : 1;\n\t__u64 dfl : 1;\n\t__u64 dfh : 1;\n\t__u64 sp : 1;\n\t__u64 pp : 1;\n\t__u64 di : 1;\n\t__u64 si : 1;\n\t__u64 db : 1;\n\t__u64 lp : 1;\n\t__u64 tb : 1;\n\t__u64 rt : 1;\n\t__u64 reserved3 : 4;\n\t__u64 cpl : 2;\n\t__u64 is : 1;\n\t__u64 mc : 1;\n\t__u64 it : 1;\n\t__u64 id : 1;\n\t__u64 da : 1;\n\t__u64 dd : 1;\n\t__u64 ss : 1;\n\t__u64 ri : 2;\n\t__u64 ed : 1;\n\t__u64 bn : 1;\n\t__u64 reserved4 : 19;\n};\n\nunion ia64_isr {\n\t__u64  val;\n\tstruct {\n\t\t__u64 code : 16;\n\t\t__u64 vector : 8;\n\t\t__u64 reserved1 : 8;\n\t\t__u64 x : 1;\n\t\t__u64 w : 1;\n\t\t__u64 r : 1;\n\t\t__u64 na : 1;\n\t\t__u64 sp : 1;\n\t\t__u64 rs : 1;\n\t\t__u64 ir : 1;\n\t\t__u64 ni : 1;\n\t\t__u64 so : 1;\n\t\t__u64 ei : 2;\n\t\t__u64 ed : 1;\n\t\t__u64 reserved2 : 20;\n\t};\n};\n\nunion ia64_lid {\n\t__u64 val;\n\tstruct {\n\t\t__u64  rv  : 16;\n\t\t__u64  eid : 8;\n\t\t__u64  id  : 8;\n\t\t__u64  ig  : 32;\n\t};\n};\n\nunion ia64_tpr {\n\t__u64 val;\n\tstruct {\n\t\t__u64 ig0 : 4;\n\t\t__u64 mic : 4;\n\t\t__u64 rsv : 8;\n\t\t__u64 mmi : 1;\n\t\t__u64 ig1 : 47;\n\t};\n};\n\nunion ia64_itir {\n\t__u64 val;\n\tstruct {\n\t\t__u64 rv3  :  2; /* 0-1 */\n\t\t__u64 ps   :  6; /* 2-7 */\n\t\t__u64 key  : 24; /* 8-31 */\n\t\t__u64 rv4  : 32; /* 32-63 */\n\t};\n};\n\nunion  ia64_rr {\n\t__u64 val;\n\tstruct {\n\t\t__u64  ve\t:  1;  /* enable hw walker */\n\t\t__u64  reserved0:  1;  /* reserved */\n\t\t__u64  ps\t:  6;  /* log page size */\n\t\t__u64  rid\t: 24;  /* region id */\n\t\t__u64  reserved1: 32;  /* reserved */\n\t};\n};\n\n/*\n * CPU type, hardware bug flags, and per-CPU state.  Frequently used\n * state comes earlier:\n */\nstruct cpuinfo_ia64 {\n\tunsigned int softirq_pending;\n\tunsigned long itm_delta;\t/* # of clock cycles between clock ticks */\n\tunsigned long itm_next;\t\t/* interval timer mask value to use for next clock tick */\n\tunsigned long nsec_per_cyc;\t/* (1000000000<<IA64_NSEC_PER_CYC_SHIFT)/itc_freq */\n\tunsigned long unimpl_va_mask;\t/* mask of unimplemented virtual address bits (from PAL) */\n\tunsigned long unimpl_pa_mask;\t/* mask of unimplemented physical address bits (from PAL) */\n\tunsigned long itc_freq;\t\t/* frequency of ITC counter */\n\tunsigned long proc_freq;\t/* frequency of processor */\n\tunsigned long cyc_per_usec;\t/* itc_freq/1000000 */\n\tunsigned long ptce_base;\n\tunsigned int ptce_count[2];\n\tunsigned int ptce_stride[2];\n\tstruct task_struct *ksoftirqd;\t/* kernel softirq daemon for this CPU */\n\n#ifdef CONFIG_SMP\n\tunsigned long loops_per_jiffy;\n\tint cpu;\n\tunsigned int socket_id;\t/* physical processor socket id */\n\tunsigned short core_id;\t/* core id */\n\tunsigned short thread_id; /* thread id */\n\tunsigned short num_log;\t/* Total number of logical processors on\n\t\t\t\t * this socket that were successfully booted */\n\tunsigned char cores_per_socket;\t/* Cores per processor socket */\n\tunsigned char threads_per_core;\t/* Threads per core */\n#endif\n\n\t/* CPUID-derived information: */\n\tunsigned long ppn;\n\tunsigned long features;\n\tunsigned char number;\n\tunsigned char revision;\n\tunsigned char model;\n\tunsigned char family;\n\tunsigned char archrev;\n\tchar vendor[16];\n\tchar *model_name;\n\n#ifdef CONFIG_NUMA\n\tstruct ia64_node_data *node_data;\n#endif\n};\n\nDECLARE_PER_CPU(struct cpuinfo_ia64, ia64_cpu_info);\n\n/*\n * The \"local\" data variable.  It refers to the per-CPU data of the currently executing\n * CPU, much like \"current\" points to the per-task data of the currently executing task.\n * Do not use the address of local_cpu_data, since it will be different from\n * cpu_data(smp_processor_id())!\n */\n#define local_cpu_data\t\t(&__ia64_per_cpu_var(ia64_cpu_info))\n#define cpu_data(cpu)\t\t(&per_cpu(ia64_cpu_info, cpu))\n\nextern void print_cpu_info (struct cpuinfo_ia64 *);\n\ntypedef struct {\n\tunsigned long seg;\n} mm_segment_t;\n\n#define SET_UNALIGN_CTL(task,value)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\t(task)->thread.flags = (((task)->thread.flags & ~IA64_THREAD_UAC_MASK)\t\t\t\\\n\t\t\t\t| (((value) << IA64_THREAD_UAC_SHIFT) & IA64_THREAD_UAC_MASK));\t\\\n\t0;\t\t\t\t\t\t\t\t\t\t\t\\\n})\n#define GET_UNALIGN_CTL(task,addr)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\tput_user(((task)->thread.flags & IA64_THREAD_UAC_MASK) >> IA64_THREAD_UAC_SHIFT,\t\\\n\t\t (int __user *) (addr));\t\t\t\t\t\t\t\\\n})\n\n#define SET_FPEMU_CTL(task,value)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\t(task)->thread.flags = (((task)->thread.flags & ~IA64_THREAD_FPEMU_MASK)\t\t\\\n\t\t\t  | (((value) << IA64_THREAD_FPEMU_SHIFT) & IA64_THREAD_FPEMU_MASK));\t\\\n\t0;\t\t\t\t\t\t\t\t\t\t\t\\\n})\n#define GET_FPEMU_CTL(task,addr)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\tput_user(((task)->thread.flags & IA64_THREAD_FPEMU_MASK) >> IA64_THREAD_FPEMU_SHIFT,\t\\\n\t\t (int __user *) (addr));\t\t\t\t\t\t\t\\\n})\n\nstruct thread_struct {\n\t__u32 flags;\t\t\t/* various thread flags (see IA64_THREAD_*) */\n\t/* writing on_ustack is performance-critical, so it's worth spending 8 bits on it... */\n\t__u8 on_ustack;\t\t\t/* executing on user-stacks? */\n\t__u8 pad[3];\n\t__u64 ksp;\t\t\t/* kernel stack pointer */\n\t__u64 map_base;\t\t\t/* base address for get_unmapped_area() */\n\t__u64 rbs_bot;\t\t\t/* the base address for the RBS */\n\tint last_fph_cpu;\t\t/* CPU that may hold the contents of f32-f127 */\n\n#ifdef CONFIG_PERFMON\n\tvoid *pfm_context;\t\t     /* pointer to detailed PMU context */\n\tunsigned long pfm_needs_checking;    /* when >0, pending perfmon work on kernel exit */\n# define INIT_THREAD_PM\t\t.pfm_context =\t\tNULL,     \\\n\t\t\t\t.pfm_needs_checking =\t0UL,\n#else\n# define INIT_THREAD_PM\n#endif\n\tunsigned long dbr[IA64_NUM_DBG_REGS];\n\tunsigned long ibr[IA64_NUM_DBG_REGS];\n\tstruct ia64_fpreg fph[96];\t/* saved/loaded on demand */\n};\n\n#define INIT_THREAD {\t\t\t\t\t\t\\\n\t.flags =\t0,\t\t\t\t\t\\\n\t.on_ustack =\t0,\t\t\t\t\t\\\n\t.ksp =\t\t0,\t\t\t\t\t\\\n\t.map_base =\tDEFAULT_MAP_BASE,\t\t\t\\\n\t.rbs_bot =\tSTACK_TOP - DEFAULT_USER_STACK_SIZE,\t\\\n\t.last_fph_cpu =  -1,\t\t\t\t\t\\\n\tINIT_THREAD_PM\t\t\t\t\t\t\\\n\t.dbr =\t\t{0, },\t\t\t\t\t\\\n\t.ibr =\t\t{0, },\t\t\t\t\t\\\n\t.fph =\t\t{{{{0}}}, }\t\t\t\t\\\n}\n\n#define start_thread(regs,new_ip,new_sp) do {\t\t\t\t\t\t\t\\\n\tregs->cr_ipsr = ((regs->cr_ipsr | (IA64_PSR_BITS_TO_SET | IA64_PSR_CPL))\t\t\\\n\t\t\t & ~(IA64_PSR_BITS_TO_CLEAR | IA64_PSR_RI | IA64_PSR_IS));\t\t\\\n\tregs->cr_iip = new_ip;\t\t\t\t\t\t\t\t\t\\\n\tregs->ar_rsc = 0xf;\t\t/* eager mode, privilege level 3 */\t\t\t\\\n\tregs->ar_rnat = 0;\t\t\t\t\t\t\t\t\t\\\n\tregs->ar_bspstore = current->thread.rbs_bot;\t\t\t\t\t\t\\\n\tregs->ar_fpsr = FPSR_DEFAULT;\t\t\t\t\t\t\t\t\\\n\tregs->loadrs = 0;\t\t\t\t\t\t\t\t\t\\\n\tregs->r8 = get_dumpable(current->mm);\t/* set \"don't zap registers\" flag */\t\t\\\n\tregs->r12 = new_sp - 16;\t/* allocate 16 byte scratch area */\t\t\t\\\n\tif (unlikely(get_dumpable(current->mm) != SUID_DUMP_USER)) {\t\\\n\t\t/*\t\t\t\t\t\t\t\t\t\t\\\n\t\t * Zap scratch regs to avoid leaking bits between processes with different\t\\\n\t\t * uid/privileges.\t\t\t\t\t\t\t\t\\\n\t\t */\t\t\t\t\t\t\t\t\t\t\\\n\t\tregs->ar_pfs = 0; regs->b0 = 0; regs->pr = 0;\t\t\t\t\t\\\n\t\tregs->r1 = 0; regs->r9  = 0; regs->r11 = 0; regs->r13 = 0; regs->r15 = 0;\t\\\n\t}\t\t\t\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/* Forward declarations, a strange C thing... */\nstruct mm_struct;\nstruct task_struct;\n\n/*\n * Free all resources held by a thread. This is called after the\n * parent of DEAD_TASK has collected the exit status of the task via\n * wait().\n */\n#define release_thread(dead_task)\n\n/* Get wait channel for task P.  */\nextern unsigned long get_wchan (struct task_struct *p);\n\n/* Return instruction pointer of blocked task TSK.  */\n#define KSTK_EIP(tsk)\t\t\t\t\t\\\n  ({\t\t\t\t\t\t\t\\\n\tstruct pt_regs *_regs = task_pt_regs(tsk);\t\\\n\t_regs->cr_iip + ia64_psr(_regs)->ri;\t\t\\\n  })\n\n/* Return stack pointer of blocked task TSK.  */\n#define KSTK_ESP(tsk)  ((tsk)->thread.ksp)\n\nextern void ia64_getreg_unknown_kr (void);\nextern void ia64_setreg_unknown_kr (void);\n\n#define ia64_get_kr(regnum)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tunsigned long r = 0;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tswitch (regnum) {\t\t\t\t\t\\\n\t    case 0: r = ia64_getreg(_IA64_REG_AR_KR0); break;\t\\\n\t    case 1: r = ia64_getreg(_IA64_REG_AR_KR1); break;\t\\\n\t    case 2: r = ia64_getreg(_IA64_REG_AR_KR2); break;\t\\\n\t    case 3: r = ia64_getreg(_IA64_REG_AR_KR3); break;\t\\\n\t    case 4: r = ia64_getreg(_IA64_REG_AR_KR4); break;\t\\\n\t    case 5: r = ia64_getreg(_IA64_REG_AR_KR5); break;\t\\\n\t    case 6: r = ia64_getreg(_IA64_REG_AR_KR6); break;\t\\\n\t    case 7: r = ia64_getreg(_IA64_REG_AR_KR7); break;\t\\\n\t    default: ia64_getreg_unknown_kr(); break;\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tr;\t\t\t\t\t\t\t\\\n})\n\n#define ia64_set_kr(regnum, r) \t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tswitch (regnum) {\t\t\t\t\t\\\n\t    case 0: ia64_setreg(_IA64_REG_AR_KR0, r); break;\t\\\n\t    case 1: ia64_setreg(_IA64_REG_AR_KR1, r); break;\t\\\n\t    case 2: ia64_setreg(_IA64_REG_AR_KR2, r); break;\t\\\n\t    case 3: ia64_setreg(_IA64_REG_AR_KR3, r); break;\t\\\n\t    case 4: ia64_setreg(_IA64_REG_AR_KR4, r); break;\t\\\n\t    case 5: ia64_setreg(_IA64_REG_AR_KR5, r); break;\t\\\n\t    case 6: ia64_setreg(_IA64_REG_AR_KR6, r); break;\t\\\n\t    case 7: ia64_setreg(_IA64_REG_AR_KR7, r); break;\t\\\n\t    default: ia64_setreg_unknown_kr(); break;\t\t\\\n\t}\t\t\t\t\t\t\t\\\n})\n\n/*\n * The following three macros can't be inline functions because we don't have struct\n * task_struct at this point.\n */\n\n/*\n * Return TRUE if task T owns the fph partition of the CPU we're running on.\n * Must be called from code that has preemption disabled.\n */\n#define ia64_is_local_fpu_owner(t)\t\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\tstruct task_struct *__ia64_islfo_task = (t);\t\t\t\t\t\t\\\n\t(__ia64_islfo_task->thread.last_fph_cpu == smp_processor_id()\t\t\t\t\\\n\t && __ia64_islfo_task == (struct task_struct *) ia64_get_kr(IA64_KR_FPU_OWNER));\t\\\n})\n\n/*\n * Mark task T as owning the fph partition of the CPU we're running on.\n * Must be called from code that has preemption disabled.\n */\n#define ia64_set_local_fpu_owner(t) do {\t\t\t\t\t\t\\\n\tstruct task_struct *__ia64_slfo_task = (t);\t\t\t\t\t\\\n\t__ia64_slfo_task->thread.last_fph_cpu = smp_processor_id();\t\t\t\\\n\tia64_set_kr(IA64_KR_FPU_OWNER, (unsigned long) __ia64_slfo_task);\t\t\\\n} while (0)\n\n/* Mark the fph partition of task T as being invalid on all CPUs.  */\n#define ia64_drop_fpu(t)\t((t)->thread.last_fph_cpu = -1)\n\nextern void __ia64_init_fpu (void);\nextern void __ia64_save_fpu (struct ia64_fpreg *fph);\nextern void __ia64_load_fpu (struct ia64_fpreg *fph);\nextern void ia64_save_debug_regs (unsigned long *save_area);\nextern void ia64_load_debug_regs (unsigned long *save_area);\n\n#define ia64_fph_enable()\tdo { ia64_rsm(IA64_PSR_DFH); ia64_srlz_d(); } while (0)\n#define ia64_fph_disable()\tdo { ia64_ssm(IA64_PSR_DFH); ia64_srlz_d(); } while (0)\n\n/* load fp 0.0 into fph */\nstatic inline void\nia64_init_fpu (void) {\n\tia64_fph_enable();\n\t__ia64_init_fpu();\n\tia64_fph_disable();\n}\n\n/* save f32-f127 at FPH */\nstatic inline void\nia64_save_fpu (struct ia64_fpreg *fph) {\n\tia64_fph_enable();\n\t__ia64_save_fpu(fph);\n\tia64_fph_disable();\n}\n\n/* load f32-f127 from FPH */\nstatic inline void\nia64_load_fpu (struct ia64_fpreg *fph) {\n\tia64_fph_enable();\n\t__ia64_load_fpu(fph);\n\tia64_fph_disable();\n}\n\nstatic inline __u64\nia64_clear_ic (void)\n{\n\t__u64 psr;\n\tpsr = ia64_getreg(_IA64_REG_PSR);\n\tia64_stop();\n\tia64_rsm(IA64_PSR_I | IA64_PSR_IC);\n\tia64_srlz_i();\n\treturn psr;\n}\n\n/*\n * Restore the psr.\n */\nstatic inline void\nia64_set_psr (__u64 psr)\n{\n\tia64_stop();\n\tia64_setreg(_IA64_REG_PSR_L, psr);\n\tia64_srlz_i();\n}\n\n/*\n * Insert a translation into an instruction and/or data translation\n * register.\n */\nstatic inline void\nia64_itr (__u64 target_mask, __u64 tr_num,\n\t  __u64 vmaddr, __u64 pte,\n\t  __u64 log_page_size)\n{\n\tia64_setreg(_IA64_REG_CR_ITIR, (log_page_size << 2));\n\tia64_setreg(_IA64_REG_CR_IFA, vmaddr);\n\tia64_stop();\n\tif (target_mask & 0x1)\n\t\tia64_itri(tr_num, pte);\n\tif (target_mask & 0x2)\n\t\tia64_itrd(tr_num, pte);\n}\n\n/*\n * Insert a translation into the instruction and/or data translation\n * cache.\n */\nstatic inline void\nia64_itc (__u64 target_mask, __u64 vmaddr, __u64 pte,\n\t  __u64 log_page_size)\n{\n\tia64_setreg(_IA64_REG_CR_ITIR, (log_page_size << 2));\n\tia64_setreg(_IA64_REG_CR_IFA, vmaddr);\n\tia64_stop();\n\t/* as per EAS2.6, itc must be the last instruction in an instruction group */\n\tif (target_mask & 0x1)\n\t\tia64_itci(pte);\n\tif (target_mask & 0x2)\n\t\tia64_itcd(pte);\n}\n\n/*\n * Purge a range of addresses from instruction and/or data translation\n * register(s).\n */\nstatic inline void\nia64_ptr (__u64 target_mask, __u64 vmaddr, __u64 log_size)\n{\n\tif (target_mask & 0x1)\n\t\tia64_ptri(vmaddr, (log_size << 2));\n\tif (target_mask & 0x2)\n\t\tia64_ptrd(vmaddr, (log_size << 2));\n}\n\n/* Set the interrupt vector address.  The address must be suitably aligned (32KB).  */\nstatic inline void\nia64_set_iva (void *ivt_addr)\n{\n\tia64_setreg(_IA64_REG_CR_IVA, (__u64) ivt_addr);\n\tia64_srlz_i();\n}\n\n/* Set the page table address and control bits.  */\nstatic inline void\nia64_set_pta (__u64 pta)\n{\n\t/* Note: srlz.i implies srlz.d */\n\tia64_setreg(_IA64_REG_CR_PTA, pta);\n\tia64_srlz_i();\n}\n\nstatic inline void\nia64_eoi (void)\n{\n\tia64_setreg(_IA64_REG_CR_EOI, 0);\n\tia64_srlz_d();\n}\n\n#define cpu_relax()\tia64_hint(ia64_hint_pause)\n\nstatic inline int\nia64_get_irr(unsigned int vector)\n{\n\tunsigned int reg = vector / 64;\n\tunsigned int bit = vector % 64;\n\tu64 irr;\n\n\tswitch (reg) {\n\tcase 0: irr = ia64_getreg(_IA64_REG_CR_IRR0); break;\n\tcase 1: irr = ia64_getreg(_IA64_REG_CR_IRR1); break;\n\tcase 2: irr = ia64_getreg(_IA64_REG_CR_IRR2); break;\n\tcase 3: irr = ia64_getreg(_IA64_REG_CR_IRR3); break;\n\t}\n\n\treturn test_bit(bit, &irr);\n}\n\nstatic inline void\nia64_set_lrr0 (unsigned long val)\n{\n\tia64_setreg(_IA64_REG_CR_LRR0, val);\n\tia64_srlz_d();\n}\n\nstatic inline void\nia64_set_lrr1 (unsigned long val)\n{\n\tia64_setreg(_IA64_REG_CR_LRR1, val);\n\tia64_srlz_d();\n}\n\n\n/*\n * Given the address to which a spill occurred, return the unat bit\n * number that corresponds to this address.\n */\nstatic inline __u64\nia64_unat_pos (void *spill_addr)\n{\n\treturn ((__u64) spill_addr >> 3) & 0x3f;\n}\n\n/*\n * Set the NaT bit of an integer register which was spilled at address\n * SPILL_ADDR.  UNAT is the mask to be updated.\n */\nstatic inline void\nia64_set_unat (__u64 *unat, void *spill_addr, unsigned long nat)\n{\n\t__u64 bit = ia64_unat_pos(spill_addr);\n\t__u64 mask = 1UL << bit;\n\n\t*unat = (*unat & ~mask) | (nat << bit);\n}\n\n/*\n * Return saved PC of a blocked thread.\n * Note that the only way T can block is through a call to schedule() -> switch_to().\n */\nstatic inline unsigned long\nthread_saved_pc (struct task_struct *t)\n{\n\tstruct unw_frame_info info;\n\tunsigned long ip;\n\n\tunw_init_from_blocked_task(&info, t);\n\tif (unw_unwind(&info) < 0)\n\t\treturn 0;\n\tunw_get_ip(&info, &ip);\n\treturn ip;\n}\n\n/*\n * Get the current instruction/program counter value.\n */\n#define current_text_addr() \\\n\t({ void *_pc; _pc = (void *)ia64_getreg(_IA64_REG_IP); _pc; })\n\nstatic inline __u64\nia64_get_ivr (void)\n{\n\t__u64 r;\n\tia64_srlz_d();\n\tr = ia64_getreg(_IA64_REG_CR_IVR);\n\tia64_srlz_d();\n\treturn r;\n}\n\nstatic inline void\nia64_set_dbr (__u64 regnum, __u64 value)\n{\n\t__ia64_set_dbr(regnum, value);\n#ifdef CONFIG_ITANIUM\n\tia64_srlz_d();\n#endif\n}\n\nstatic inline __u64\nia64_get_dbr (__u64 regnum)\n{\n\t__u64 retval;\n\n\tretval = __ia64_get_dbr(regnum);\n#ifdef CONFIG_ITANIUM\n\tia64_srlz_d();\n#endif\n\treturn retval;\n}\n\nstatic inline __u64\nia64_rotr (__u64 w, __u64 n)\n{\n\treturn (w >> n) | (w << (64 - n));\n}\n\n#define ia64_rotl(w,n)\tia64_rotr((w), (64) - (n))\n\n/*\n * Take a mapped kernel address and return the equivalent address\n * in the region 7 identity mapped virtual area.\n */\nstatic inline void *\nia64_imva (void *addr)\n{\n\tvoid *result;\n\tresult = (void *) ia64_tpa(addr);\n\treturn __va(result);\n}\n\n#define ARCH_HAS_PREFETCH\n#define ARCH_HAS_PREFETCHW\n#define ARCH_HAS_SPINLOCK_PREFETCH\n#define PREFETCH_STRIDE\t\t\tL1_CACHE_BYTES\n\nstatic inline void\nprefetch (const void *x)\n{\n\t ia64_lfetch(ia64_lfhint_none, x);\n}\n\nstatic inline void\nprefetchw (const void *x)\n{\n\tia64_lfetch_excl(ia64_lfhint_none, x);\n}\n\n#define spin_lock_prefetch(x)\tprefetchw(x)\n\nextern unsigned long boot_option_idle_override;\n\nenum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_FORCE_MWAIT,\n\t\t\t IDLE_NOMWAIT, IDLE_POLL};\n\nvoid default_idle(void);\n\n#define ia64_platform_is(x) (strcmp(x, ia64_platform_name) == 0)\n\n#endif /* !__ASSEMBLY__ */\n\n#endif /* _ASM_IA64_PROCESSOR_H */\n", "/*\n *  linux/fs/exec.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n/*\n * #!-checking implemented by tytso.\n */\n/*\n * Demand-loading implemented 01.12.91 - no need to read anything but\n * the header into memory. The inode of the executable is put into\n * \"current->executable\", and page faults do the actual loading. Clean.\n *\n * Once more I can proudly say that linux stood up to being changed: it\n * was less than 2 hours work to get demand-loading completely implemented.\n *\n * Demand loading changed July 1993 by Eric Youngdale.   Use mmap instead,\n * current->executable is only used by the procfs.  This allows a dispatch\n * table to check for several different types  of binary formats.  We keep\n * trying until we recognize the file or we run out of supported binary\n * formats. \n */\n\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/mm.h>\n#include <linux/stat.h>\n#include <linux/fcntl.h>\n#include <linux/swap.h>\n#include <linux/string.h>\n#include <linux/init.h>\n#include <linux/pagemap.h>\n#include <linux/perf_event.h>\n#include <linux/highmem.h>\n#include <linux/spinlock.h>\n#include <linux/key.h>\n#include <linux/personality.h>\n#include <linux/binfmts.h>\n#include <linux/utsname.h>\n#include <linux/pid_namespace.h>\n#include <linux/module.h>\n#include <linux/namei.h>\n#include <linux/mount.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/tsacct_kern.h>\n#include <linux/cn_proc.h>\n#include <linux/audit.h>\n#include <linux/tracehook.h>\n#include <linux/kmod.h>\n#include <linux/fsnotify.h>\n#include <linux/fs_struct.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/oom.h>\n#include <linux/compat.h>\n\n#include <asm/uaccess.h>\n#include <asm/mmu_context.h>\n#include <asm/tlb.h>\n\n#include <trace/events/task.h>\n#include \"internal.h\"\n#include \"coredump.h\"\n\n#include <trace/events/sched.h>\n\nint suid_dumpable = 0;\n\nstatic LIST_HEAD(formats);\nstatic DEFINE_RWLOCK(binfmt_lock);\n\nvoid __register_binfmt(struct linux_binfmt * fmt, int insert)\n{\n\tBUG_ON(!fmt);\n\tif (WARN_ON(!fmt->load_binary))\n\t\treturn;\n\twrite_lock(&binfmt_lock);\n\tinsert ? list_add(&fmt->lh, &formats) :\n\t\t list_add_tail(&fmt->lh, &formats);\n\twrite_unlock(&binfmt_lock);\n}\n\nEXPORT_SYMBOL(__register_binfmt);\n\nvoid unregister_binfmt(struct linux_binfmt * fmt)\n{\n\twrite_lock(&binfmt_lock);\n\tlist_del(&fmt->lh);\n\twrite_unlock(&binfmt_lock);\n}\n\nEXPORT_SYMBOL(unregister_binfmt);\n\nstatic inline void put_binfmt(struct linux_binfmt * fmt)\n{\n\tmodule_put(fmt->module);\n}\n\n/*\n * Note that a shared library must be both readable and executable due to\n * security reasons.\n *\n * Also note that we take the address to load from from the file itself.\n */\nSYSCALL_DEFINE1(uselib, const char __user *, library)\n{\n\tstruct file *file;\n\tstruct filename *tmp = getname(library);\n\tint error = PTR_ERR(tmp);\n\tstatic const struct open_flags uselib_flags = {\n\t\t.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,\n\t\t.acc_mode = MAY_READ | MAY_EXEC | MAY_OPEN,\n\t\t.intent = LOOKUP_OPEN,\n\t\t.lookup_flags = LOOKUP_FOLLOW,\n\t};\n\n\tif (IS_ERR(tmp))\n\t\tgoto out;\n\n\tfile = do_filp_open(AT_FDCWD, tmp, &uselib_flags);\n\tputname(tmp);\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out;\n\n\terror = -EINVAL;\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\tgoto exit;\n\n\terror = -EACCES;\n\tif (file->f_path.mnt->mnt_flags & MNT_NOEXEC)\n\t\tgoto exit;\n\n\tfsnotify_open(file);\n\n\terror = -ENOEXEC;\n\tif(file->f_op) {\n\t\tstruct linux_binfmt * fmt;\n\n\t\tread_lock(&binfmt_lock);\n\t\tlist_for_each_entry(fmt, &formats, lh) {\n\t\t\tif (!fmt->load_shlib)\n\t\t\t\tcontinue;\n\t\t\tif (!try_module_get(fmt->module))\n\t\t\t\tcontinue;\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\terror = fmt->load_shlib(file);\n\t\t\tread_lock(&binfmt_lock);\n\t\t\tput_binfmt(fmt);\n\t\t\tif (error != -ENOEXEC)\n\t\t\t\tbreak;\n\t\t}\n\t\tread_unlock(&binfmt_lock);\n\t}\nexit:\n\tfput(file);\nout:\n  \treturn error;\n}\n\n#ifdef CONFIG_MMU\n/*\n * The nascent bprm->mm is not visible until exec_mmap() but it can\n * use a lot of memory, account these pages in current->mm temporary\n * for oom_badness()->get_mm_rss(). Once exec succeeds or fails, we\n * change the counter back via acct_arg_size(0).\n */\nstatic void acct_arg_size(struct linux_binprm *bprm, unsigned long pages)\n{\n\tstruct mm_struct *mm = current->mm;\n\tlong diff = (long)(pages - bprm->vma_pages);\n\n\tif (!mm || !diff)\n\t\treturn;\n\n\tbprm->vma_pages = pages;\n\tadd_mm_counter(mm, MM_ANONPAGES, diff);\n}\n\nstatic struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\tint ret;\n\n#ifdef CONFIG_STACK_GROWSUP\n\tif (write) {\n\t\tret = expand_downwards(bprm->vma, pos);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\t}\n#endif\n\tret = get_user_pages(current, bprm->mm, pos,\n\t\t\t1, write, 1, &page, NULL);\n\tif (ret <= 0)\n\t\treturn NULL;\n\n\tif (write) {\n\t\tunsigned long size = bprm->vma->vm_end - bprm->vma->vm_start;\n\t\tstruct rlimit *rlim;\n\n\t\tacct_arg_size(bprm, size / PAGE_SIZE);\n\n\t\t/*\n\t\t * We've historically supported up to 32 pages (ARG_MAX)\n\t\t * of argument strings even with small stacks\n\t\t */\n\t\tif (size <= ARG_MAX)\n\t\t\treturn page;\n\n\t\t/*\n\t\t * Limit to 1/4-th the stack size for the argv+env strings.\n\t\t * This ensures that:\n\t\t *  - the remaining binfmt code will not run out of stack space,\n\t\t *  - the program will have a reasonable amount of stack left\n\t\t *    to work from.\n\t\t */\n\t\trlim = current->signal->rlim;\n\t\tif (size > ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur) / 4) {\n\t\t\tput_page(page);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn page;\n}\n\nstatic void put_arg_page(struct page *page)\n{\n\tput_page(page);\n}\n\nstatic void free_arg_page(struct linux_binprm *bprm, int i)\n{\n}\n\nstatic void free_arg_pages(struct linux_binprm *bprm)\n{\n}\n\nstatic void flush_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tstruct page *page)\n{\n\tflush_cache_page(bprm->vma, pos, page_to_pfn(page));\n}\n\nstatic int __bprm_mm_init(struct linux_binprm *bprm)\n{\n\tint err;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mm_struct *mm = bprm->mm;\n\n\tbprm->vma = vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);\n\tif (!vma)\n\t\treturn -ENOMEM;\n\n\tdown_write(&mm->mmap_sem);\n\tvma->vm_mm = mm;\n\n\t/*\n\t * Place the stack at the largest stack address the architecture\n\t * supports. Later, we'll move this to an appropriate place. We don't\n\t * use STACK_TOP because that can depend on attributes which aren't\n\t * configured yet.\n\t */\n\tBUILD_BUG_ON(VM_STACK_FLAGS & VM_STACK_INCOMPLETE_SETUP);\n\tvma->vm_end = STACK_TOP_MAX;\n\tvma->vm_start = vma->vm_end - PAGE_SIZE;\n\tvma->vm_flags = VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP;\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n\tINIT_LIST_HEAD(&vma->anon_vma_chain);\n\n\terr = insert_vm_struct(mm, vma);\n\tif (err)\n\t\tgoto err;\n\n\tmm->stack_vm = mm->total_vm = 1;\n\tup_write(&mm->mmap_sem);\n\tbprm->p = vma->vm_end - sizeof(void *);\n\treturn 0;\nerr:\n\tup_write(&mm->mmap_sem);\n\tbprm->vma = NULL;\n\tkmem_cache_free(vm_area_cachep, vma);\n\treturn err;\n}\n\nstatic bool valid_arg_len(struct linux_binprm *bprm, long len)\n{\n\treturn len <= MAX_ARG_STRLEN;\n}\n\n#else\n\nstatic inline void acct_arg_size(struct linux_binprm *bprm, unsigned long pages)\n{\n}\n\nstatic struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\n\tpage = bprm->page[pos / PAGE_SIZE];\n\tif (!page && write) {\n\t\tpage = alloc_page(GFP_HIGHUSER|__GFP_ZERO);\n\t\tif (!page)\n\t\t\treturn NULL;\n\t\tbprm->page[pos / PAGE_SIZE] = page;\n\t}\n\n\treturn page;\n}\n\nstatic void put_arg_page(struct page *page)\n{\n}\n\nstatic void free_arg_page(struct linux_binprm *bprm, int i)\n{\n\tif (bprm->page[i]) {\n\t\t__free_page(bprm->page[i]);\n\t\tbprm->page[i] = NULL;\n\t}\n}\n\nstatic void free_arg_pages(struct linux_binprm *bprm)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_ARG_PAGES; i++)\n\t\tfree_arg_page(bprm, i);\n}\n\nstatic void flush_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tstruct page *page)\n{\n}\n\nstatic int __bprm_mm_init(struct linux_binprm *bprm)\n{\n\tbprm->p = PAGE_SIZE * MAX_ARG_PAGES - sizeof(void *);\n\treturn 0;\n}\n\nstatic bool valid_arg_len(struct linux_binprm *bprm, long len)\n{\n\treturn len <= bprm->p;\n}\n\n#endif /* CONFIG_MMU */\n\n/*\n * Create a new mm_struct and populate it with a temporary stack\n * vm_area_struct.  We don't have enough context at this point to set the stack\n * flags, permissions, and offset, so we use temporary values.  We'll update\n * them later in setup_arg_pages().\n */\nstatic int bprm_mm_init(struct linux_binprm *bprm)\n{\n\tint err;\n\tstruct mm_struct *mm = NULL;\n\n\tbprm->mm = mm = mm_alloc();\n\terr = -ENOMEM;\n\tif (!mm)\n\t\tgoto err;\n\n\terr = init_new_context(current, mm);\n\tif (err)\n\t\tgoto err;\n\n\terr = __bprm_mm_init(bprm);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tif (mm) {\n\t\tbprm->mm = NULL;\n\t\tmmdrop(mm);\n\t}\n\n\treturn err;\n}\n\nstruct user_arg_ptr {\n#ifdef CONFIG_COMPAT\n\tbool is_compat;\n#endif\n\tunion {\n\t\tconst char __user *const __user *native;\n#ifdef CONFIG_COMPAT\n\t\tconst compat_uptr_t __user *compat;\n#endif\n\t} ptr;\n};\n\nstatic const char __user *get_user_arg_ptr(struct user_arg_ptr argv, int nr)\n{\n\tconst char __user *native;\n\n#ifdef CONFIG_COMPAT\n\tif (unlikely(argv.is_compat)) {\n\t\tcompat_uptr_t compat;\n\n\t\tif (get_user(compat, argv.ptr.compat + nr))\n\t\t\treturn ERR_PTR(-EFAULT);\n\n\t\treturn compat_ptr(compat);\n\t}\n#endif\n\n\tif (get_user(native, argv.ptr.native + nr))\n\t\treturn ERR_PTR(-EFAULT);\n\n\treturn native;\n}\n\n/*\n * count() counts the number of strings in array ARGV.\n */\nstatic int count(struct user_arg_ptr argv, int max)\n{\n\tint i = 0;\n\n\tif (argv.ptr.native != NULL) {\n\t\tfor (;;) {\n\t\t\tconst char __user *p = get_user_arg_ptr(argv, i);\n\n\t\t\tif (!p)\n\t\t\t\tbreak;\n\n\t\t\tif (IS_ERR(p))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tif (i >= max)\n\t\t\t\treturn -E2BIG;\n\t\t\t++i;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\treturn -ERESTARTNOHAND;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\treturn i;\n}\n\n/*\n * 'copy_strings()' copies argument/environment strings from the old\n * processes's memory to the new process's stack.  The call to get_user_pages()\n * ensures the destination page is created and not swapped out.\n */\nstatic int copy_strings(int argc, struct user_arg_ptr argv,\n\t\t\tstruct linux_binprm *bprm)\n{\n\tstruct page *kmapped_page = NULL;\n\tchar *kaddr = NULL;\n\tunsigned long kpos = 0;\n\tint ret;\n\n\twhile (argc-- > 0) {\n\t\tconst char __user *str;\n\t\tint len;\n\t\tunsigned long pos;\n\n\t\tret = -EFAULT;\n\t\tstr = get_user_arg_ptr(argv, argc);\n\t\tif (IS_ERR(str))\n\t\t\tgoto out;\n\n\t\tlen = strnlen_user(str, MAX_ARG_STRLEN);\n\t\tif (!len)\n\t\t\tgoto out;\n\n\t\tret = -E2BIG;\n\t\tif (!valid_arg_len(bprm, len))\n\t\t\tgoto out;\n\n\t\t/* We're going to work our way backwords. */\n\t\tpos = bprm->p;\n\t\tstr += len;\n\t\tbprm->p -= len;\n\n\t\twhile (len > 0) {\n\t\t\tint offset, bytes_to_copy;\n\n\t\t\tif (fatal_signal_pending(current)) {\n\t\t\t\tret = -ERESTARTNOHAND;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcond_resched();\n\n\t\t\toffset = pos % PAGE_SIZE;\n\t\t\tif (offset == 0)\n\t\t\t\toffset = PAGE_SIZE;\n\n\t\t\tbytes_to_copy = offset;\n\t\t\tif (bytes_to_copy > len)\n\t\t\t\tbytes_to_copy = len;\n\n\t\t\toffset -= bytes_to_copy;\n\t\t\tpos -= bytes_to_copy;\n\t\t\tstr -= bytes_to_copy;\n\t\t\tlen -= bytes_to_copy;\n\n\t\t\tif (!kmapped_page || kpos != (pos & PAGE_MASK)) {\n\t\t\t\tstruct page *page;\n\n\t\t\t\tpage = get_arg_page(bprm, pos, 1);\n\t\t\t\tif (!page) {\n\t\t\t\t\tret = -E2BIG;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tif (kmapped_page) {\n\t\t\t\t\tflush_kernel_dcache_page(kmapped_page);\n\t\t\t\t\tkunmap(kmapped_page);\n\t\t\t\t\tput_arg_page(kmapped_page);\n\t\t\t\t}\n\t\t\t\tkmapped_page = page;\n\t\t\t\tkaddr = kmap(kmapped_page);\n\t\t\t\tkpos = pos & PAGE_MASK;\n\t\t\t\tflush_arg_page(bprm, kpos, kmapped_page);\n\t\t\t}\n\t\t\tif (copy_from_user(kaddr+offset, str, bytes_to_copy)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tret = 0;\nout:\n\tif (kmapped_page) {\n\t\tflush_kernel_dcache_page(kmapped_page);\n\t\tkunmap(kmapped_page);\n\t\tput_arg_page(kmapped_page);\n\t}\n\treturn ret;\n}\n\n/*\n * Like copy_strings, but get argv and its values from kernel memory.\n */\nint copy_strings_kernel(int argc, const char *const *__argv,\n\t\t\tstruct linux_binprm *bprm)\n{\n\tint r;\n\tmm_segment_t oldfs = get_fs();\n\tstruct user_arg_ptr argv = {\n\t\t.ptr.native = (const char __user *const  __user *)__argv,\n\t};\n\n\tset_fs(KERNEL_DS);\n\tr = copy_strings(argc, argv, bprm);\n\tset_fs(oldfs);\n\n\treturn r;\n}\nEXPORT_SYMBOL(copy_strings_kernel);\n\n#ifdef CONFIG_MMU\n\n/*\n * During bprm_mm_init(), we create a temporary stack at STACK_TOP_MAX.  Once\n * the binfmt code determines where the new stack should reside, we shift it to\n * its final location.  The process proceeds as follows:\n *\n * 1) Use shift to calculate the new vma endpoints.\n * 2) Extend vma to cover both the old and new ranges.  This ensures the\n *    arguments passed to subsequent functions are consistent.\n * 3) Move vma's page tables to the new range.\n * 4) Free up any cleared pgd range.\n * 5) Shrink the vma to cover only the new range.\n */\nstatic int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long old_start = vma->vm_start;\n\tunsigned long old_end = vma->vm_end;\n\tunsigned long length = old_end - old_start;\n\tunsigned long new_start = old_start - shift;\n\tunsigned long new_end = old_end - shift;\n\tstruct mmu_gather tlb;\n\n\tBUG_ON(new_start > new_end);\n\n\t/*\n\t * ensure there are no vmas between where we want to go\n\t * and where we are\n\t */\n\tif (vma != find_vma(mm, new_start))\n\t\treturn -EFAULT;\n\n\t/*\n\t * cover the whole range: [new_start, old_end)\n\t */\n\tif (vma_adjust(vma, new_start, old_end, vma->vm_pgoff, NULL))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * move the page tables downwards, on failure we rely on\n\t * process cleanup to remove whatever mess we made.\n\t */\n\tif (length != move_page_tables(vma, old_start,\n\t\t\t\t       vma, new_start, length, false))\n\t\treturn -ENOMEM;\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm, old_start, old_end);\n\tif (new_end > old_start) {\n\t\t/*\n\t\t * when the old and new regions overlap clear from new_end.\n\t\t */\n\t\tfree_pgd_range(&tlb, new_end, old_end, new_end,\n\t\t\tvma->vm_next ? vma->vm_next->vm_start : USER_PGTABLES_CEILING);\n\t} else {\n\t\t/*\n\t\t * otherwise, clean from old_start; this is done to not touch\n\t\t * the address space in [new_end, old_start) some architectures\n\t\t * have constraints on va-space that make this illegal (IA64) -\n\t\t * for the others its just a little faster.\n\t\t */\n\t\tfree_pgd_range(&tlb, old_start, old_end, new_end,\n\t\t\tvma->vm_next ? vma->vm_next->vm_start : USER_PGTABLES_CEILING);\n\t}\n\ttlb_finish_mmu(&tlb, old_start, old_end);\n\n\t/*\n\t * Shrink the vma to just the new range.  Always succeeds.\n\t */\n\tvma_adjust(vma, new_start, new_end, vma->vm_pgoff, NULL);\n\n\treturn 0;\n}\n\n/*\n * Finalizes the stack vm_area_struct. The flags and permissions are updated,\n * the stack is optionally relocated, and some extra space is added.\n */\nint setup_arg_pages(struct linux_binprm *bprm,\n\t\t    unsigned long stack_top,\n\t\t    int executable_stack)\n{\n\tunsigned long ret;\n\tunsigned long stack_shift;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = bprm->vma;\n\tstruct vm_area_struct *prev = NULL;\n\tunsigned long vm_flags;\n\tunsigned long stack_base;\n\tunsigned long stack_size;\n\tunsigned long stack_expand;\n\tunsigned long rlim_stack;\n\n#ifdef CONFIG_STACK_GROWSUP\n\t/* Limit stack size to 1GB */\n\tstack_base = rlimit_max(RLIMIT_STACK);\n\tif (stack_base > (1 << 30))\n\t\tstack_base = 1 << 30;\n\n\t/* Make sure we didn't let the argument array grow too large. */\n\tif (vma->vm_end - vma->vm_start > stack_base)\n\t\treturn -ENOMEM;\n\n\tstack_base = PAGE_ALIGN(stack_top - stack_base);\n\n\tstack_shift = vma->vm_start - stack_base;\n\tmm->arg_start = bprm->p - stack_shift;\n\tbprm->p = vma->vm_end - stack_shift;\n#else\n\tstack_top = arch_align_stack(stack_top);\n\tstack_top = PAGE_ALIGN(stack_top);\n\n\tif (unlikely(stack_top < mmap_min_addr) ||\n\t    unlikely(vma->vm_end - vma->vm_start >= stack_top - mmap_min_addr))\n\t\treturn -ENOMEM;\n\n\tstack_shift = vma->vm_end - stack_top;\n\n\tbprm->p -= stack_shift;\n\tmm->arg_start = bprm->p;\n#endif\n\n\tif (bprm->loader)\n\t\tbprm->loader -= stack_shift;\n\tbprm->exec -= stack_shift;\n\n\tdown_write(&mm->mmap_sem);\n\tvm_flags = VM_STACK_FLAGS;\n\n\t/*\n\t * Adjust stack execute permissions; explicitly enable for\n\t * EXSTACK_ENABLE_X, disable for EXSTACK_DISABLE_X and leave alone\n\t * (arch default) otherwise.\n\t */\n\tif (unlikely(executable_stack == EXSTACK_ENABLE_X))\n\t\tvm_flags |= VM_EXEC;\n\telse if (executable_stack == EXSTACK_DISABLE_X)\n\t\tvm_flags &= ~VM_EXEC;\n\tvm_flags |= mm->def_flags;\n\tvm_flags |= VM_STACK_INCOMPLETE_SETUP;\n\n\tret = mprotect_fixup(vma, &prev, vma->vm_start, vma->vm_end,\n\t\t\tvm_flags);\n\tif (ret)\n\t\tgoto out_unlock;\n\tBUG_ON(prev != vma);\n\n\t/* Move stack pages down in memory. */\n\tif (stack_shift) {\n\t\tret = shift_arg_pages(vma, stack_shift);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* mprotect_fixup is overkill to remove the temporary stack flags */\n\tvma->vm_flags &= ~VM_STACK_INCOMPLETE_SETUP;\n\n\tstack_expand = 131072UL; /* randomly 32*4k (or 2*64k) pages */\n\tstack_size = vma->vm_end - vma->vm_start;\n\t/*\n\t * Align this down to a page boundary as expand_stack\n\t * will align it up.\n\t */\n\trlim_stack = rlimit(RLIMIT_STACK) & PAGE_MASK;\n#ifdef CONFIG_STACK_GROWSUP\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_start + rlim_stack;\n\telse\n\t\tstack_base = vma->vm_end + stack_expand;\n#else\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_end - rlim_stack;\n\telse\n\t\tstack_base = vma->vm_start - stack_expand;\n#endif\n\tcurrent->mm->start_stack = bprm->p;\n\tret = expand_stack(vma, stack_base);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(setup_arg_pages);\n\n#endif /* CONFIG_MMU */\n\nstruct file *open_exec(const char *name)\n{\n\tstruct file *file;\n\tint err;\n\tstruct filename tmp = { .name = name };\n\tstatic const struct open_flags open_exec_flags = {\n\t\t.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,\n\t\t.acc_mode = MAY_EXEC | MAY_OPEN,\n\t\t.intent = LOOKUP_OPEN,\n\t\t.lookup_flags = LOOKUP_FOLLOW,\n\t};\n\n\tfile = do_filp_open(AT_FDCWD, &tmp, &open_exec_flags);\n\tif (IS_ERR(file))\n\t\tgoto out;\n\n\terr = -EACCES;\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\tgoto exit;\n\n\tif (file->f_path.mnt->mnt_flags & MNT_NOEXEC)\n\t\tgoto exit;\n\n\tfsnotify_open(file);\n\n\terr = deny_write_access(file);\n\tif (err)\n\t\tgoto exit;\n\nout:\n\treturn file;\n\nexit:\n\tfput(file);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL(open_exec);\n\nint kernel_read(struct file *file, loff_t offset,\n\t\tchar *addr, unsigned long count)\n{\n\tmm_segment_t old_fs;\n\tloff_t pos = offset;\n\tint result;\n\n\told_fs = get_fs();\n\tset_fs(get_ds());\n\t/* The cast to a user pointer is valid due to the set_fs() */\n\tresult = vfs_read(file, (void __user *)addr, count, &pos);\n\tset_fs(old_fs);\n\treturn result;\n}\n\nEXPORT_SYMBOL(kernel_read);\n\nssize_t read_code(struct file *file, unsigned long addr, loff_t pos, size_t len)\n{\n\tssize_t res = file->f_op->read(file, (void __user *)addr, len, &pos);\n\tif (res > 0)\n\t\tflush_icache_range(addr, addr + len);\n\treturn res;\n}\nEXPORT_SYMBOL(read_code);\n\nstatic int exec_mmap(struct mm_struct *mm)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct * old_mm, *active_mm;\n\n\t/* Notify parent that we're no longer interested in the old VM */\n\ttsk = current;\n\told_mm = current->mm;\n\tmm_release(tsk, old_mm);\n\n\tif (old_mm) {\n\t\tsync_mm_rss(old_mm);\n\t\t/*\n\t\t * Make sure that if there is a core dump in progress\n\t\t * for the old mm, we get out and die instead of going\n\t\t * through with the exec.  We must hold mmap_sem around\n\t\t * checking core_state and changing tsk->mm.\n\t\t */\n\t\tdown_read(&old_mm->mmap_sem);\n\t\tif (unlikely(old_mm->core_state)) {\n\t\t\tup_read(&old_mm->mmap_sem);\n\t\t\treturn -EINTR;\n\t\t}\n\t}\n\ttask_lock(tsk);\n\tactive_mm = tsk->active_mm;\n\ttsk->mm = mm;\n\ttsk->active_mm = mm;\n\tactivate_mm(active_mm, mm);\n\ttask_unlock(tsk);\n\tarch_pick_mmap_layout(mm);\n\tif (old_mm) {\n\t\tup_read(&old_mm->mmap_sem);\n\t\tBUG_ON(active_mm != old_mm);\n\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, old_mm);\n\t\tmm_update_next_owner(old_mm);\n\t\tmmput(old_mm);\n\t\treturn 0;\n\t}\n\tmmdrop(active_mm);\n\treturn 0;\n}\n\n/*\n * This function makes sure the current process has its own signal table,\n * so that flush_signal_handlers can later reset the handlers without\n * disturbing other processes.  (Other processes might share the signal\n * table via the CLONE_SIGHAND option to clone().)\n */\nstatic int de_thread(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tstruct sighand_struct *oldsighand = tsk->sighand;\n\tspinlock_t *lock = &oldsighand->siglock;\n\n\tif (thread_group_empty(tsk))\n\t\tgoto no_thread_group;\n\n\t/*\n\t * Kill all other threads in the thread group.\n\t */\n\tspin_lock_irq(lock);\n\tif (signal_group_exit(sig)) {\n\t\t/*\n\t\t * Another group action in progress, just\n\t\t * return so that the signal is processed.\n\t\t */\n\t\tspin_unlock_irq(lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tsig->group_exit_task = tsk;\n\tsig->notify_count = zap_other_threads(tsk);\n\tif (!thread_group_leader(tsk))\n\t\tsig->notify_count--;\n\n\twhile (sig->notify_count) {\n\t\t__set_current_state(TASK_KILLABLE);\n\t\tspin_unlock_irq(lock);\n\t\tschedule();\n\t\tif (unlikely(__fatal_signal_pending(tsk)))\n\t\t\tgoto killed;\n\t\tspin_lock_irq(lock);\n\t}\n\tspin_unlock_irq(lock);\n\n\t/*\n\t * At this point all other threads have exited, all we have to\n\t * do is to wait for the thread group leader to become inactive,\n\t * and to assume its PID:\n\t */\n\tif (!thread_group_leader(tsk)) {\n\t\tstruct task_struct *leader = tsk->group_leader;\n\n\t\tsig->notify_count = -1;\t/* for exit_notify() */\n\t\tfor (;;) {\n\t\t\tthreadgroup_change_begin(tsk);\n\t\t\twrite_lock_irq(&tasklist_lock);\n\t\t\tif (likely(leader->exit_state))\n\t\t\t\tbreak;\n\t\t\t__set_current_state(TASK_KILLABLE);\n\t\t\twrite_unlock_irq(&tasklist_lock);\n\t\t\tthreadgroup_change_end(tsk);\n\t\t\tschedule();\n\t\t\tif (unlikely(__fatal_signal_pending(tsk)))\n\t\t\t\tgoto killed;\n\t\t}\n\n\t\t/*\n\t\t * The only record we have of the real-time age of a\n\t\t * process, regardless of execs it's done, is start_time.\n\t\t * All the past CPU time is accumulated in signal_struct\n\t\t * from sister threads now dead.  But in this non-leader\n\t\t * exec, nothing survives from the original leader thread,\n\t\t * whose birth marks the true age of this process now.\n\t\t * When we take on its identity by switching to its PID, we\n\t\t * also take its birthdate (always earlier than our own).\n\t\t */\n\t\ttsk->start_time = leader->start_time;\n\t\ttsk->real_start_time = leader->real_start_time;\n\n\t\tBUG_ON(!same_thread_group(leader, tsk));\n\t\tBUG_ON(has_group_leader_pid(tsk));\n\t\t/*\n\t\t * An exec() starts a new thread group with the\n\t\t * TGID of the previous thread group. Rehash the\n\t\t * two threads with a switched PID, and release\n\t\t * the former thread group leader:\n\t\t */\n\n\t\t/* Become a process group leader with the old leader's pid.\n\t\t * The old leader becomes a thread of the this thread group.\n\t\t * Note: The old leader also uses this pid until release_task\n\t\t *       is called.  Odd but simple and correct.\n\t\t */\n\t\ttsk->pid = leader->pid;\n\t\tchange_pid(tsk, PIDTYPE_PID, task_pid(leader));\n\t\ttransfer_pid(leader, tsk, PIDTYPE_PGID);\n\t\ttransfer_pid(leader, tsk, PIDTYPE_SID);\n\n\t\tlist_replace_rcu(&leader->tasks, &tsk->tasks);\n\t\tlist_replace_init(&leader->sibling, &tsk->sibling);\n\n\t\ttsk->group_leader = tsk;\n\t\tleader->group_leader = tsk;\n\n\t\ttsk->exit_signal = SIGCHLD;\n\t\tleader->exit_signal = -1;\n\n\t\tBUG_ON(leader->exit_state != EXIT_ZOMBIE);\n\t\tleader->exit_state = EXIT_DEAD;\n\n\t\t/*\n\t\t * We are going to release_task()->ptrace_unlink() silently,\n\t\t * the tracer can sleep in do_wait(). EXIT_DEAD guarantees\n\t\t * the tracer wont't block again waiting for this thread.\n\t\t */\n\t\tif (unlikely(leader->ptrace))\n\t\t\t__wake_up_parent(leader, leader->parent);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tthreadgroup_change_end(tsk);\n\n\t\trelease_task(leader);\n\t}\n\n\tsig->group_exit_task = NULL;\n\tsig->notify_count = 0;\n\nno_thread_group:\n\t/* we have changed execution domain */\n\ttsk->exit_signal = SIGCHLD;\n\n\texit_itimers(sig);\n\tflush_itimer_signals();\n\n\tif (atomic_read(&oldsighand->count) != 1) {\n\t\tstruct sighand_struct *newsighand;\n\t\t/*\n\t\t * This ->sighand is shared with the CLONE_SIGHAND\n\t\t * but not CLONE_THREAD task, switch to the new one.\n\t\t */\n\t\tnewsighand = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);\n\t\tif (!newsighand)\n\t\t\treturn -ENOMEM;\n\n\t\tatomic_set(&newsighand->count, 1);\n\t\tmemcpy(newsighand->action, oldsighand->action,\n\t\t       sizeof(newsighand->action));\n\n\t\twrite_lock_irq(&tasklist_lock);\n\t\tspin_lock(&oldsighand->siglock);\n\t\trcu_assign_pointer(tsk->sighand, newsighand);\n\t\tspin_unlock(&oldsighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\n\t\t__cleanup_sighand(oldsighand);\n\t}\n\n\tBUG_ON(!thread_group_leader(tsk));\n\treturn 0;\n\nkilled:\n\t/* protects against exit_notify() and __exit_signal() */\n\tread_lock(&tasklist_lock);\n\tsig->group_exit_task = NULL;\n\tsig->notify_count = 0;\n\tread_unlock(&tasklist_lock);\n\treturn -EAGAIN;\n}\n\nchar *get_task_comm(char *buf, struct task_struct *tsk)\n{\n\t/* buf must be at least sizeof(tsk->comm) in size */\n\ttask_lock(tsk);\n\tstrncpy(buf, tsk->comm, sizeof(tsk->comm));\n\ttask_unlock(tsk);\n\treturn buf;\n}\nEXPORT_SYMBOL_GPL(get_task_comm);\n\n/*\n * These functions flushes out all traces of the currently running executable\n * so that a new one can be started\n */\n\nvoid set_task_comm(struct task_struct *tsk, char *buf)\n{\n\ttask_lock(tsk);\n\ttrace_task_rename(tsk, buf);\n\tstrlcpy(tsk->comm, buf, sizeof(tsk->comm));\n\ttask_unlock(tsk);\n\tperf_event_comm(tsk);\n}\n\nstatic void filename_to_taskname(char *tcomm, const char *fn, unsigned int len)\n{\n\tint i, ch;\n\n\t/* Copies the binary name from after last slash */\n\tfor (i = 0; (ch = *(fn++)) != '\\0';) {\n\t\tif (ch == '/')\n\t\t\ti = 0; /* overwrite what we wrote */\n\t\telse\n\t\t\tif (i < len - 1)\n\t\t\t\ttcomm[i++] = ch;\n\t}\n\ttcomm[i] = '\\0';\n}\n\nint flush_old_exec(struct linux_binprm * bprm)\n{\n\tint retval;\n\n\t/*\n\t * Make sure we have a private signal table and that\n\t * we are unassociated from the previous thread group.\n\t */\n\tretval = de_thread(current);\n\tif (retval)\n\t\tgoto out;\n\n\tset_mm_exe_file(bprm->mm, bprm->file);\n\n\tfilename_to_taskname(bprm->tcomm, bprm->filename, sizeof(bprm->tcomm));\n\t/*\n\t * Release all of the old mmap stuff\n\t */\n\tacct_arg_size(bprm, 0);\n\tretval = exec_mmap(bprm->mm);\n\tif (retval)\n\t\tgoto out;\n\n\tbprm->mm = NULL;\t\t/* We're using it now */\n\n\tset_fs(USER_DS);\n\tcurrent->flags &=\n\t\t~(PF_RANDOMIZE | PF_FORKNOEXEC | PF_KTHREAD | PF_NOFREEZE);\n\tflush_thread();\n\tcurrent->personality &= ~bprm->per_clear;\n\n\treturn 0;\n\nout:\n\treturn retval;\n}\nEXPORT_SYMBOL(flush_old_exec);\n\nvoid would_dump(struct linux_binprm *bprm, struct file *file)\n{\n\tif (inode_permission(file_inode(file), MAY_READ) < 0)\n\t\tbprm->interp_flags |= BINPRM_FLAGS_ENFORCE_NONDUMP;\n}\nEXPORT_SYMBOL(would_dump);\n\nvoid setup_new_exec(struct linux_binprm * bprm)\n{\n\tarch_pick_mmap_layout(current->mm);\n\n\t/* This is the point of no return */\n\tcurrent->sas_ss_sp = current->sas_ss_size = 0;\n\n\tif (uid_eq(current_euid(), current_uid()) && gid_eq(current_egid(), current_gid()))\n\t\tset_dumpable(current->mm, SUID_DUMP_USER);\n\telse\n\t\tset_dumpable(current->mm, suid_dumpable);\n\n\tset_task_comm(current, bprm->tcomm);\n\n\t/* Set the new mm task size. We have to do that late because it may\n\t * depend on TIF_32BIT which is only updated in flush_thread() on\n\t * some architectures like powerpc\n\t */\n\tcurrent->mm->task_size = TASK_SIZE;\n\n\t/* install the new credentials */\n\tif (!uid_eq(bprm->cred->uid, current_euid()) ||\n\t    !gid_eq(bprm->cred->gid, current_egid())) {\n\t\tcurrent->pdeath_signal = 0;\n\t} else {\n\t\twould_dump(bprm, bprm->file);\n\t\tif (bprm->interp_flags & BINPRM_FLAGS_ENFORCE_NONDUMP)\n\t\t\tset_dumpable(current->mm, suid_dumpable);\n\t}\n\n\t/* An exec changes our domain. We are no longer part of the thread\n\t   group */\n\n\tcurrent->self_exec_id++;\n\t\t\t\n\tflush_signal_handlers(current, 0);\n\tdo_close_on_exec(current->files);\n}\nEXPORT_SYMBOL(setup_new_exec);\n\n/*\n * Prepare credentials and lock ->cred_guard_mutex.\n * install_exec_creds() commits the new creds and drops the lock.\n * Or, if exec fails before, free_bprm() should release ->cred and\n * and unlock.\n */\nint prepare_bprm_creds(struct linux_binprm *bprm)\n{\n\tif (mutex_lock_interruptible(&current->signal->cred_guard_mutex))\n\t\treturn -ERESTARTNOINTR;\n\n\tbprm->cred = prepare_exec_creds();\n\tif (likely(bprm->cred))\n\t\treturn 0;\n\n\tmutex_unlock(&current->signal->cred_guard_mutex);\n\treturn -ENOMEM;\n}\n\nvoid free_bprm(struct linux_binprm *bprm)\n{\n\tfree_arg_pages(bprm);\n\tif (bprm->cred) {\n\t\tmutex_unlock(&current->signal->cred_guard_mutex);\n\t\tabort_creds(bprm->cred);\n\t}\n\t/* If a binfmt changed the interp, free it. */\n\tif (bprm->interp != bprm->filename)\n\t\tkfree(bprm->interp);\n\tkfree(bprm);\n}\n\nint bprm_change_interp(char *interp, struct linux_binprm *bprm)\n{\n\t/* If a binfmt changed the interp, free it first. */\n\tif (bprm->interp != bprm->filename)\n\t\tkfree(bprm->interp);\n\tbprm->interp = kstrdup(interp, GFP_KERNEL);\n\tif (!bprm->interp)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nEXPORT_SYMBOL(bprm_change_interp);\n\n/*\n * install the new credentials for this executable\n */\nvoid install_exec_creds(struct linux_binprm *bprm)\n{\n\tsecurity_bprm_committing_creds(bprm);\n\n\tcommit_creds(bprm->cred);\n\tbprm->cred = NULL;\n\n\t/*\n\t * Disable monitoring for regular users\n\t * when executing setuid binaries. Must\n\t * wait until new credentials are committed\n\t * by commit_creds() above\n\t */\n\tif (get_dumpable(current->mm) != SUID_DUMP_USER)\n\t\tperf_event_exit_task(current);\n\t/*\n\t * cred_guard_mutex must be held at least to this point to prevent\n\t * ptrace_attach() from altering our determination of the task's\n\t * credentials; any time after this it may be unlocked.\n\t */\n\tsecurity_bprm_committed_creds(bprm);\n\tmutex_unlock(&current->signal->cred_guard_mutex);\n}\nEXPORT_SYMBOL(install_exec_creds);\n\n/*\n * determine how safe it is to execute the proposed program\n * - the caller must hold ->cred_guard_mutex to protect against\n *   PTRACE_ATTACH\n */\nstatic int check_unsafe_exec(struct linux_binprm *bprm)\n{\n\tstruct task_struct *p = current, *t;\n\tunsigned n_fs;\n\tint res = 0;\n\n\tif (p->ptrace) {\n\t\tif (p->ptrace & PT_PTRACE_CAP)\n\t\t\tbprm->unsafe |= LSM_UNSAFE_PTRACE_CAP;\n\t\telse\n\t\t\tbprm->unsafe |= LSM_UNSAFE_PTRACE;\n\t}\n\n\t/*\n\t * This isn't strictly necessary, but it makes it harder for LSMs to\n\t * mess up.\n\t */\n\tif (current->no_new_privs)\n\t\tbprm->unsafe |= LSM_UNSAFE_NO_NEW_PRIVS;\n\n\tn_fs = 1;\n\tspin_lock(&p->fs->lock);\n\trcu_read_lock();\n\tfor (t = next_thread(p); t != p; t = next_thread(t)) {\n\t\tif (t->fs == p->fs)\n\t\t\tn_fs++;\n\t}\n\trcu_read_unlock();\n\n\tif (p->fs->users > n_fs) {\n\t\tbprm->unsafe |= LSM_UNSAFE_SHARE;\n\t} else {\n\t\tres = -EAGAIN;\n\t\tif (!p->fs->in_exec) {\n\t\t\tp->fs->in_exec = 1;\n\t\t\tres = 1;\n\t\t}\n\t}\n\tspin_unlock(&p->fs->lock);\n\n\treturn res;\n}\n\n/* \n * Fill the binprm structure from the inode. \n * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes\n *\n * This may be called multiple times for binary chains (scripts for example).\n */\nint prepare_binprm(struct linux_binprm *bprm)\n{\n\tumode_t mode;\n\tstruct inode * inode = file_inode(bprm->file);\n\tint retval;\n\n\tmode = inode->i_mode;\n\tif (bprm->file->f_op == NULL)\n\t\treturn -EACCES;\n\n\t/* clear any previous set[ug]id data from a previous binary */\n\tbprm->cred->euid = current_euid();\n\tbprm->cred->egid = current_egid();\n\n\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&\n\t    !current->no_new_privs &&\n\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&\n\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {\n\t\t/* Set-uid? */\n\t\tif (mode & S_ISUID) {\n\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\t\tbprm->cred->euid = inode->i_uid;\n\t\t}\n\n\t\t/* Set-gid? */\n\t\t/*\n\t\t * If setgid is set but no group execute bit then this\n\t\t * is a candidate for mandatory locking, not a setgid\n\t\t * executable.\n\t\t */\n\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\t\tbprm->cred->egid = inode->i_gid;\n\t\t}\n\t}\n\n\t/* fill in binprm security blob */\n\tretval = security_bprm_set_creds(bprm);\n\tif (retval)\n\t\treturn retval;\n\tbprm->cred_prepared = 1;\n\n\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\treturn kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);\n}\n\nEXPORT_SYMBOL(prepare_binprm);\n\n/*\n * Arguments are '\\0' separated strings found at the location bprm->p\n * points to; chop off the first by relocating brpm->p to right after\n * the first '\\0' encountered.\n */\nint remove_arg_zero(struct linux_binprm *bprm)\n{\n\tint ret = 0;\n\tunsigned long offset;\n\tchar *kaddr;\n\tstruct page *page;\n\n\tif (!bprm->argc)\n\t\treturn 0;\n\n\tdo {\n\t\toffset = bprm->p & ~PAGE_MASK;\n\t\tpage = get_arg_page(bprm, bprm->p, 0);\n\t\tif (!page) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tkaddr = kmap_atomic(page);\n\n\t\tfor (; offset < PAGE_SIZE && kaddr[offset];\n\t\t\t\toffset++, bprm->p++)\n\t\t\t;\n\n\t\tkunmap_atomic(kaddr);\n\t\tput_arg_page(page);\n\n\t\tif (offset == PAGE_SIZE)\n\t\t\tfree_arg_page(bprm, (bprm->p >> PAGE_SHIFT) - 1);\n\t} while (offset == PAGE_SIZE);\n\n\tbprm->p++;\n\tbprm->argc--;\n\tret = 0;\n\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(remove_arg_zero);\n\n#define printable(c) (((c)=='\\t') || ((c)=='\\n') || (0x20<=(c) && (c)<=0x7e))\n/*\n * cycle the list of binary formats handler, until one recognizes the image\n */\nint search_binary_handler(struct linux_binprm *bprm)\n{\n\tbool need_retry = IS_ENABLED(CONFIG_MODULES);\n\tstruct linux_binfmt *fmt;\n\tint retval;\n\n\t/* This allows 4 levels of binfmt rewrites before failing hard. */\n\tif (bprm->recursion_depth > 5)\n\t\treturn -ELOOP;\n\n\tretval = security_bprm_check(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = audit_bprm(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = -ENOENT;\n retry:\n\tread_lock(&binfmt_lock);\n\tlist_for_each_entry(fmt, &formats, lh) {\n\t\tif (!try_module_get(fmt->module))\n\t\t\tcontinue;\n\t\tread_unlock(&binfmt_lock);\n\t\tbprm->recursion_depth++;\n\t\tretval = fmt->load_binary(bprm);\n\t\tbprm->recursion_depth--;\n\t\tif (retval >= 0 || retval != -ENOEXEC ||\n\t\t    bprm->mm == NULL || bprm->file == NULL) {\n\t\t\tput_binfmt(fmt);\n\t\t\treturn retval;\n\t\t}\n\t\tread_lock(&binfmt_lock);\n\t\tput_binfmt(fmt);\n\t}\n\tread_unlock(&binfmt_lock);\n\n\tif (need_retry && retval == -ENOEXEC) {\n\t\tif (printable(bprm->buf[0]) && printable(bprm->buf[1]) &&\n\t\t    printable(bprm->buf[2]) && printable(bprm->buf[3]))\n\t\t\treturn retval;\n\t\tif (request_module(\"binfmt-%04x\", *(ushort *)(bprm->buf + 2)) < 0)\n\t\t\treturn retval;\n\t\tneed_retry = false;\n\t\tgoto retry;\n\t}\n\n\treturn retval;\n}\nEXPORT_SYMBOL(search_binary_handler);\n\nstatic int exec_binprm(struct linux_binprm *bprm)\n{\n\tpid_t old_pid, old_vpid;\n\tint ret;\n\n\t/* Need to fetch pid before load_binary changes it */\n\told_pid = current->pid;\n\trcu_read_lock();\n\told_vpid = task_pid_nr_ns(current, task_active_pid_ns(current->parent));\n\trcu_read_unlock();\n\n\tret = search_binary_handler(bprm);\n\tif (ret >= 0) {\n\t\ttrace_sched_process_exec(current, old_pid, bprm);\n\t\tptrace_event(PTRACE_EVENT_EXEC, old_vpid);\n\t\tcurrent->did_exec = 1;\n\t\tproc_exec_connector(current);\n\n\t\tif (bprm->file) {\n\t\t\tallow_write_access(bprm->file);\n\t\t\tfput(bprm->file);\n\t\t\tbprm->file = NULL; /* to catch use-after-free */\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n/*\n * sys_execve() executes a new program.\n */\nstatic int do_execve_common(const char *filename,\n\t\t\t\tstruct user_arg_ptr argv,\n\t\t\t\tstruct user_arg_ptr envp)\n{\n\tstruct linux_binprm *bprm;\n\tstruct file *file;\n\tstruct files_struct *displaced;\n\tbool clear_in_exec;\n\tint retval;\n\n\t/*\n\t * We move the actual failure in case of RLIMIT_NPROC excess from\n\t * set*uid() to execve() because too many poorly written programs\n\t * don't check setuid() return code.  Here we additionally recheck\n\t * whether NPROC limit is still exceeded.\n\t */\n\tif ((current->flags & PF_NPROC_EXCEEDED) &&\n\t    atomic_read(&current_user()->processes) > rlimit(RLIMIT_NPROC)) {\n\t\tretval = -EAGAIN;\n\t\tgoto out_ret;\n\t}\n\n\t/* We're below the limit (still or again), so we don't want to make\n\t * further execve() calls fail. */\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\tgoto out_ret;\n\n\tretval = -ENOMEM;\n\tbprm = kzalloc(sizeof(*bprm), GFP_KERNEL);\n\tif (!bprm)\n\t\tgoto out_files;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_free;\n\n\tretval = check_unsafe_exec(bprm);\n\tif (retval < 0)\n\t\tgoto out_free;\n\tclear_in_exec = retval;\n\tcurrent->in_execve = 1;\n\n\tfile = open_exec(filename);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\tbprm->filename = filename;\n\tbprm->interp = filename;\n\n\tretval = bprm_mm_init(bprm);\n\tif (retval)\n\t\tgoto out_file;\n\n\tbprm->argc = count(argv, MAX_ARG_STRINGS);\n\tif ((retval = bprm->argc) < 0)\n\t\tgoto out;\n\n\tbprm->envc = count(envp, MAX_ARG_STRINGS);\n\tif ((retval = bprm->envc) < 0)\n\t\tgoto out;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tbprm->exec = bprm->p;\n\tretval = copy_strings(bprm->envc, envp, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings(bprm->argc, argv, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = exec_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\tacct_update_integrals(current);\n\ttask_numa_free(current);\n\tfree_bprm(bprm);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\tif (bprm->mm) {\n\t\tacct_arg_size(bprm, 0);\n\t\tmmput(bprm->mm);\n\t}\n\nout_file:\n\tif (bprm->file) {\n\t\tallow_write_access(bprm->file);\n\t\tfput(bprm->file);\n\t}\n\nout_unmark:\n\tif (clear_in_exec)\n\t\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_free:\n\tfree_bprm(bprm);\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\nout_ret:\n\treturn retval;\n}\n\nint do_execve(const char *filename,\n\tconst char __user *const __user *__argv,\n\tconst char __user *const __user *__envp)\n{\n\tstruct user_arg_ptr argv = { .ptr.native = __argv };\n\tstruct user_arg_ptr envp = { .ptr.native = __envp };\n\treturn do_execve_common(filename, argv, envp);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_do_execve(const char *filename,\n\tconst compat_uptr_t __user *__argv,\n\tconst compat_uptr_t __user *__envp)\n{\n\tstruct user_arg_ptr argv = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __argv,\n\t};\n\tstruct user_arg_ptr envp = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __envp,\n\t};\n\treturn do_execve_common(filename, argv, envp);\n}\n#endif\n\nvoid set_binfmt(struct linux_binfmt *new)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tif (mm->binfmt)\n\t\tmodule_put(mm->binfmt->module);\n\n\tmm->binfmt = new;\n\tif (new)\n\t\t__module_get(new->module);\n}\n\nEXPORT_SYMBOL(set_binfmt);\n\n/*\n * set_dumpable converts traditional three-value dumpable to two flags and\n * stores them into mm->flags.  It modifies lower two bits of mm->flags, but\n * these bits are not changed atomically.  So get_dumpable can observe the\n * intermediate state.  To avoid doing unexpected behavior, get get_dumpable\n * return either old dumpable or new one by paying attention to the order of\n * modifying the bits.\n *\n * dumpable |   mm->flags (binary)\n * old  new | initial interim  final\n * ---------+-----------------------\n *  0    1  |   00      01      01\n *  0    2  |   00      10(*)   11\n *  1    0  |   01      00      00\n *  1    2  |   01      11      11\n *  2    0  |   11      10(*)   00\n *  2    1  |   11      11      01\n *\n * (*) get_dumpable regards interim value of 10 as 11.\n */\nvoid set_dumpable(struct mm_struct *mm, int value)\n{\n\tswitch (value) {\n\tcase SUID_DUMP_DISABLE:\n\t\tclear_bit(MMF_DUMPABLE, &mm->flags);\n\t\tsmp_wmb();\n\t\tclear_bit(MMF_DUMP_SECURELY, &mm->flags);\n\t\tbreak;\n\tcase SUID_DUMP_USER:\n\t\tset_bit(MMF_DUMPABLE, &mm->flags);\n\t\tsmp_wmb();\n\t\tclear_bit(MMF_DUMP_SECURELY, &mm->flags);\n\t\tbreak;\n\tcase SUID_DUMP_ROOT:\n\t\tset_bit(MMF_DUMP_SECURELY, &mm->flags);\n\t\tsmp_wmb();\n\t\tset_bit(MMF_DUMPABLE, &mm->flags);\n\t\tbreak;\n\t}\n}\n\nint __get_dumpable(unsigned long mm_flags)\n{\n\tint ret;\n\n\tret = mm_flags & MMF_DUMPABLE_MASK;\n\treturn (ret > SUID_DUMP_USER) ? SUID_DUMP_ROOT : ret;\n}\n\n/*\n * This returns the actual value of the suid_dumpable flag. For things\n * that are using this for checking for privilege transitions, it must\n * test against SUID_DUMP_USER rather than treating it as a boolean\n * value.\n */\nint get_dumpable(struct mm_struct *mm)\n{\n\treturn __get_dumpable(mm->flags);\n}\n\nSYSCALL_DEFINE3(execve,\n\t\tconst char __user *, filename,\n\t\tconst char __user *const __user *, argv,\n\t\tconst char __user *const __user *, envp)\n{\n\tstruct filename *path = getname(filename);\n\tint error = PTR_ERR(path);\n\tif (!IS_ERR(path)) {\n\t\terror = do_execve(path->name, argv, envp);\n\t\tputname(path);\n\t}\n\treturn error;\n}\n#ifdef CONFIG_COMPAT\nasmlinkage long compat_sys_execve(const char __user * filename,\n\tconst compat_uptr_t __user * argv,\n\tconst compat_uptr_t __user * envp)\n{\n\tstruct filename *path = getname(filename);\n\tint error = PTR_ERR(path);\n\tif (!IS_ERR(path)) {\n\t\terror = compat_do_execve(path->name, argv, envp);\n\t\tputname(path);\n\t}\n\treturn error;\n}\n#endif\n", "#ifndef _LINUX_BINFMTS_H\n#define _LINUX_BINFMTS_H\n\n#include <linux/sched.h>\n#include <linux/unistd.h>\n#include <asm/exec.h>\n#include <uapi/linux/binfmts.h>\n\n#define CORENAME_MAX_SIZE 128\n\n/*\n * This structure is used to hold the arguments that are used when loading binaries.\n */\nstruct linux_binprm {\n\tchar buf[BINPRM_BUF_SIZE];\n#ifdef CONFIG_MMU\n\tstruct vm_area_struct *vma;\n\tunsigned long vma_pages;\n#else\n# define MAX_ARG_PAGES\t32\n\tstruct page *page[MAX_ARG_PAGES];\n#endif\n\tstruct mm_struct *mm;\n\tunsigned long p; /* current top of mem */\n\tunsigned int\n\t\tcred_prepared:1,/* true if creds already prepared (multiple\n\t\t\t\t * preps happen for interpreters) */\n\t\tcap_effective:1;/* true if has elevated effective capabilities,\n\t\t\t\t * false if not; except for init which inherits\n\t\t\t\t * its parent's caps anyway */\n#ifdef __alpha__\n\tunsigned int taso:1;\n#endif\n\tunsigned int recursion_depth; /* only for search_binary_handler() */\n\tstruct file * file;\n\tstruct cred *cred;\t/* new credentials */\n\tint unsafe;\t\t/* how unsafe this exec is (mask of LSM_UNSAFE_*) */\n\tunsigned int per_clear;\t/* bits to clear in current->personality */\n\tint argc, envc;\n\tconst char * filename;\t/* Name of binary as seen by procps */\n\tconst char * interp;\t/* Name of the binary really executed. Most\n\t\t\t\t   of the time same as filename, but could be\n\t\t\t\t   different for binfmt_{misc,script} */\n\tunsigned interp_flags;\n\tunsigned interp_data;\n\tunsigned long loader, exec;\n\tchar tcomm[TASK_COMM_LEN];\n};\n\n#define BINPRM_FLAGS_ENFORCE_NONDUMP_BIT 0\n#define BINPRM_FLAGS_ENFORCE_NONDUMP (1 << BINPRM_FLAGS_ENFORCE_NONDUMP_BIT)\n\n/* fd of the binary should be passed to the interpreter */\n#define BINPRM_FLAGS_EXECFD_BIT 1\n#define BINPRM_FLAGS_EXECFD (1 << BINPRM_FLAGS_EXECFD_BIT)\n\n/* Function parameter for binfmt->coredump */\nstruct coredump_params {\n\tsiginfo_t *siginfo;\n\tstruct pt_regs *regs;\n\tstruct file *file;\n\tunsigned long limit;\n\tunsigned long mm_flags;\n};\n\n/*\n * This structure defines the functions that are used to load the binary formats that\n * linux accepts.\n */\nstruct linux_binfmt {\n\tstruct list_head lh;\n\tstruct module *module;\n\tint (*load_binary)(struct linux_binprm *);\n\tint (*load_shlib)(struct file *);\n\tint (*core_dump)(struct coredump_params *cprm);\n\tunsigned long min_coredump;\t/* minimal dump size */\n};\n\nextern void __register_binfmt(struct linux_binfmt *fmt, int insert);\n\n/* Registration of default binfmt handlers */\nstatic inline void register_binfmt(struct linux_binfmt *fmt)\n{\n\t__register_binfmt(fmt, 0);\n}\n/* Same as above, but adds a new binfmt at the top of the list */\nstatic inline void insert_binfmt(struct linux_binfmt *fmt)\n{\n\t__register_binfmt(fmt, 1);\n}\n\nextern void unregister_binfmt(struct linux_binfmt *);\n\nextern int prepare_binprm(struct linux_binprm *);\nextern int __must_check remove_arg_zero(struct linux_binprm *);\nextern int search_binary_handler(struct linux_binprm *);\nextern int flush_old_exec(struct linux_binprm * bprm);\nextern void setup_new_exec(struct linux_binprm * bprm);\nextern void would_dump(struct linux_binprm *, struct file *);\n\nextern int suid_dumpable;\n\n/* Stack area protections */\n#define EXSTACK_DEFAULT   0\t/* Whatever the arch defaults to */\n#define EXSTACK_DISABLE_X 1\t/* Disable executable stacks */\n#define EXSTACK_ENABLE_X  2\t/* Enable executable stacks */\n\nextern int setup_arg_pages(struct linux_binprm * bprm,\n\t\t\t   unsigned long stack_top,\n\t\t\t   int executable_stack);\nextern int bprm_change_interp(char *interp, struct linux_binprm *bprm);\nextern int copy_strings_kernel(int argc, const char *const *argv,\n\t\t\t       struct linux_binprm *bprm);\nextern int prepare_bprm_creds(struct linux_binprm *bprm);\nextern void install_exec_creds(struct linux_binprm *bprm);\nextern void set_binfmt(struct linux_binfmt *new);\nextern void free_bprm(struct linux_binprm *);\nextern ssize_t read_code(struct file *, unsigned long, loff_t, size_t);\n\n#endif /* _LINUX_BINFMTS_H */\n", "#ifndef _LINUX_SCHED_H\n#define _LINUX_SCHED_H\n\n#include <uapi/linux/sched.h>\n\n\nstruct sched_param {\n\tint sched_priority;\n};\n\n#include <asm/param.h>\t/* for HZ */\n\n#include <linux/capability.h>\n#include <linux/threads.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/timex.h>\n#include <linux/jiffies.h>\n#include <linux/rbtree.h>\n#include <linux/thread_info.h>\n#include <linux/cpumask.h>\n#include <linux/errno.h>\n#include <linux/nodemask.h>\n#include <linux/mm_types.h>\n#include <linux/preempt.h>\n\n#include <asm/page.h>\n#include <asm/ptrace.h>\n#include <asm/cputime.h>\n\n#include <linux/smp.h>\n#include <linux/sem.h>\n#include <linux/signal.h>\n#include <linux/compiler.h>\n#include <linux/completion.h>\n#include <linux/pid.h>\n#include <linux/percpu.h>\n#include <linux/topology.h>\n#include <linux/proportions.h>\n#include <linux/seccomp.h>\n#include <linux/rcupdate.h>\n#include <linux/rculist.h>\n#include <linux/rtmutex.h>\n\n#include <linux/time.h>\n#include <linux/param.h>\n#include <linux/resource.h>\n#include <linux/timer.h>\n#include <linux/hrtimer.h>\n#include <linux/task_io_accounting.h>\n#include <linux/latencytop.h>\n#include <linux/cred.h>\n#include <linux/llist.h>\n#include <linux/uidgid.h>\n#include <linux/gfp.h>\n\n#include <asm/processor.h>\n\nstruct exec_domain;\nstruct futex_pi_state;\nstruct robust_list_head;\nstruct bio_list;\nstruct fs_struct;\nstruct perf_event_context;\nstruct blk_plug;\n\n/*\n * List of flags we want to share for kernel threads,\n * if only because they are not used by them anyway.\n */\n#define CLONE_KERNEL\t(CLONE_FS | CLONE_FILES | CLONE_SIGHAND)\n\n/*\n * These are the constant used to fake the fixed-point load-average\n * counting. Some notes:\n *  - 11 bit fractions expand to 22 bits by the multiplies: this gives\n *    a load-average precision of 10 bits integer + 11 bits fractional\n *  - if you want to count load-averages more often, you need more\n *    precision, or rounding will get you. With 2-second counting freq,\n *    the EXP_n values would be 1981, 2034 and 2043 if still using only\n *    11 bit fractions.\n */\nextern unsigned long avenrun[];\t\t/* Load averages */\nextern void get_avenrun(unsigned long *loads, unsigned long offset, int shift);\n\n#define FSHIFT\t\t11\t\t/* nr of bits of precision */\n#define FIXED_1\t\t(1<<FSHIFT)\t/* 1.0 as fixed-point */\n#define LOAD_FREQ\t(5*HZ+1)\t/* 5 sec intervals */\n#define EXP_1\t\t1884\t\t/* 1/exp(5sec/1min) as fixed-point */\n#define EXP_5\t\t2014\t\t/* 1/exp(5sec/5min) */\n#define EXP_15\t\t2037\t\t/* 1/exp(5sec/15min) */\n\n#define CALC_LOAD(load,exp,n) \\\n\tload *= exp; \\\n\tload += n*(FIXED_1-exp); \\\n\tload >>= FSHIFT;\n\nextern unsigned long total_forks;\nextern int nr_threads;\nDECLARE_PER_CPU(unsigned long, process_counts);\nextern int nr_processes(void);\nextern unsigned long nr_running(void);\nextern unsigned long nr_iowait(void);\nextern unsigned long nr_iowait_cpu(int cpu);\nextern unsigned long this_cpu_load(void);\n\n\nextern void calc_global_load(unsigned long ticks);\nextern void update_cpu_load_nohz(void);\n\nextern unsigned long get_parent_ip(unsigned long addr);\n\nextern void dump_cpu_task(int cpu);\n\nstruct seq_file;\nstruct cfs_rq;\nstruct task_group;\n#ifdef CONFIG_SCHED_DEBUG\nextern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);\nextern void proc_sched_set_task(struct task_struct *p);\nextern void\nprint_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);\n#endif\n\n/*\n * Task state bitmask. NOTE! These bits are also\n * encoded in fs/proc/array.c: get_task_state().\n *\n * We have two separate sets of flags: task->state\n * is about runnability, while task->exit_state are\n * about the task exiting. Confusing, but this way\n * modifying one set can't modify the other one by\n * mistake.\n */\n#define TASK_RUNNING\t\t0\n#define TASK_INTERRUPTIBLE\t1\n#define TASK_UNINTERRUPTIBLE\t2\n#define __TASK_STOPPED\t\t4\n#define __TASK_TRACED\t\t8\n/* in tsk->exit_state */\n#define EXIT_ZOMBIE\t\t16\n#define EXIT_DEAD\t\t32\n/* in tsk->state again */\n#define TASK_DEAD\t\t64\n#define TASK_WAKEKILL\t\t128\n#define TASK_WAKING\t\t256\n#define TASK_PARKED\t\t512\n#define TASK_STATE_MAX\t\t1024\n\n#define TASK_STATE_TO_CHAR_STR \"RSDTtZXxKWP\"\n\nextern char ___assert_task_state[1 - 2*!!(\n\t\tsizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];\n\n/* Convenience macros for the sake of set_task_state */\n#define TASK_KILLABLE\t\t(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)\n#define TASK_STOPPED\t\t(TASK_WAKEKILL | __TASK_STOPPED)\n#define TASK_TRACED\t\t(TASK_WAKEKILL | __TASK_TRACED)\n\n/* Convenience macros for the sake of wake_up */\n#define TASK_NORMAL\t\t(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)\n#define TASK_ALL\t\t(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)\n\n/* get_task_state() */\n#define TASK_REPORT\t\t(TASK_RUNNING | TASK_INTERRUPTIBLE | \\\n\t\t\t\t TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \\\n\t\t\t\t __TASK_TRACED)\n\n#define task_is_traced(task)\t((task->state & __TASK_TRACED) != 0)\n#define task_is_stopped(task)\t((task->state & __TASK_STOPPED) != 0)\n#define task_is_dead(task)\t((task)->exit_state != 0)\n#define task_is_stopped_or_traced(task)\t\\\n\t\t\t((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)\n#define task_contributes_to_load(task)\t\\\n\t\t\t\t((task->state & TASK_UNINTERRUPTIBLE) != 0 && \\\n\t\t\t\t (task->flags & PF_FROZEN) == 0)\n\n#define __set_task_state(tsk, state_value)\t\t\\\n\tdo { (tsk)->state = (state_value); } while (0)\n#define set_task_state(tsk, state_value)\t\t\\\n\tset_mb((tsk)->state, (state_value))\n\n/*\n * set_current_state() includes a barrier so that the write of current->state\n * is correctly serialised wrt the caller's subsequent test of whether to\n * actually sleep:\n *\n *\tset_current_state(TASK_UNINTERRUPTIBLE);\n *\tif (do_i_need_to_sleep())\n *\t\tschedule();\n *\n * If the caller does not need such serialisation then use __set_current_state()\n */\n#define __set_current_state(state_value)\t\t\t\\\n\tdo { current->state = (state_value); } while (0)\n#define set_current_state(state_value)\t\t\\\n\tset_mb(current->state, (state_value))\n\n/* Task command name length */\n#define TASK_COMM_LEN 16\n\n#include <linux/spinlock.h>\n\n/*\n * This serializes \"schedule()\" and also protects\n * the run-queue from deletions/modifications (but\n * _adding_ to the beginning of the run-queue has\n * a separate lock).\n */\nextern rwlock_t tasklist_lock;\nextern spinlock_t mmlist_lock;\n\nstruct task_struct;\n\n#ifdef CONFIG_PROVE_RCU\nextern int lockdep_tasklist_lock_is_held(void);\n#endif /* #ifdef CONFIG_PROVE_RCU */\n\nextern void sched_init(void);\nextern void sched_init_smp(void);\nextern asmlinkage void schedule_tail(struct task_struct *prev);\nextern void init_idle(struct task_struct *idle, int cpu);\nextern void init_idle_bootup_task(struct task_struct *idle);\n\nextern int runqueue_is_locked(int cpu);\n\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\nextern void nohz_balance_enter_idle(int cpu);\nextern void set_cpu_sd_state_idle(void);\nextern int get_nohz_timer_target(void);\n#else\nstatic inline void nohz_balance_enter_idle(int cpu) { }\nstatic inline void set_cpu_sd_state_idle(void) { }\n#endif\n\n/*\n * Only dump TASK_* tasks. (0 for all tasks)\n */\nextern void show_state_filter(unsigned long state_filter);\n\nstatic inline void show_state(void)\n{\n\tshow_state_filter(0);\n}\n\nextern void show_regs(struct pt_regs *);\n\n/*\n * TASK is a pointer to the task whose backtrace we want to see (or NULL for current\n * task), SP is the stack pointer of the first frame that should be shown in the back\n * trace (or NULL if the entire call-chain of the task should be shown).\n */\nextern void show_stack(struct task_struct *task, unsigned long *sp);\n\nvoid io_schedule(void);\nlong io_schedule_timeout(long timeout);\n\nextern void cpu_init (void);\nextern void trap_init(void);\nextern void update_process_times(int user);\nextern void scheduler_tick(void);\n\nextern void sched_show_task(struct task_struct *p);\n\n#ifdef CONFIG_LOCKUP_DETECTOR\nextern void touch_softlockup_watchdog(void);\nextern void touch_softlockup_watchdog_sync(void);\nextern void touch_all_softlockup_watchdogs(void);\nextern int proc_dowatchdog_thresh(struct ctl_table *table, int write,\n\t\t\t\t  void __user *buffer,\n\t\t\t\t  size_t *lenp, loff_t *ppos);\nextern unsigned int  softlockup_panic;\nvoid lockup_detector_init(void);\n#else\nstatic inline void touch_softlockup_watchdog(void)\n{\n}\nstatic inline void touch_softlockup_watchdog_sync(void)\n{\n}\nstatic inline void touch_all_softlockup_watchdogs(void)\n{\n}\nstatic inline void lockup_detector_init(void)\n{\n}\n#endif\n\n/* Attach to any functions which should be ignored in wchan output. */\n#define __sched\t\t__attribute__((__section__(\".sched.text\")))\n\n/* Linker adds these: start and end of __sched functions */\nextern char __sched_text_start[], __sched_text_end[];\n\n/* Is this address in the __sched functions? */\nextern int in_sched_functions(unsigned long addr);\n\n#define\tMAX_SCHEDULE_TIMEOUT\tLONG_MAX\nextern signed long schedule_timeout(signed long timeout);\nextern signed long schedule_timeout_interruptible(signed long timeout);\nextern signed long schedule_timeout_killable(signed long timeout);\nextern signed long schedule_timeout_uninterruptible(signed long timeout);\nasmlinkage void schedule(void);\nextern void schedule_preempt_disabled(void);\n\nstruct nsproxy;\nstruct user_namespace;\n\n#ifdef CONFIG_MMU\nextern void arch_pick_mmap_layout(struct mm_struct *mm);\nextern unsigned long\narch_get_unmapped_area(struct file *, unsigned long, unsigned long,\n\t\t       unsigned long, unsigned long);\nextern unsigned long\narch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t  unsigned long flags);\n#else\nstatic inline void arch_pick_mmap_layout(struct mm_struct *mm) {}\n#endif\n\n\nextern void set_dumpable(struct mm_struct *mm, int value);\nextern int get_dumpable(struct mm_struct *mm);\n\n#define SUID_DUMP_DISABLE\t0\t/* No setuid dumping */\n#define SUID_DUMP_USER\t\t1\t/* Dump as user of process */\n#define SUID_DUMP_ROOT\t\t2\t/* Dump as root */\n\n/* mm flags */\n/* dumpable bits */\n#define MMF_DUMPABLE      0  /* core dump is permitted */\n#define MMF_DUMP_SECURELY 1  /* core file is readable only by root */\n\n#define MMF_DUMPABLE_BITS 2\n#define MMF_DUMPABLE_MASK ((1 << MMF_DUMPABLE_BITS) - 1)\n\n/* coredump filter bits */\n#define MMF_DUMP_ANON_PRIVATE\t2\n#define MMF_DUMP_ANON_SHARED\t3\n#define MMF_DUMP_MAPPED_PRIVATE\t4\n#define MMF_DUMP_MAPPED_SHARED\t5\n#define MMF_DUMP_ELF_HEADERS\t6\n#define MMF_DUMP_HUGETLB_PRIVATE 7\n#define MMF_DUMP_HUGETLB_SHARED  8\n\n#define MMF_DUMP_FILTER_SHIFT\tMMF_DUMPABLE_BITS\n#define MMF_DUMP_FILTER_BITS\t7\n#define MMF_DUMP_FILTER_MASK \\\n\t(((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)\n#define MMF_DUMP_FILTER_DEFAULT \\\n\t((1 << MMF_DUMP_ANON_PRIVATE) |\t(1 << MMF_DUMP_ANON_SHARED) |\\\n\t (1 << MMF_DUMP_HUGETLB_PRIVATE) | MMF_DUMP_MASK_DEFAULT_ELF)\n\n#ifdef CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS\n# define MMF_DUMP_MASK_DEFAULT_ELF\t(1 << MMF_DUMP_ELF_HEADERS)\n#else\n# define MMF_DUMP_MASK_DEFAULT_ELF\t0\n#endif\n\t\t\t\t\t/* leave room for more dump flags */\n#define MMF_VM_MERGEABLE\t16\t/* KSM may merge identical pages */\n#define MMF_VM_HUGEPAGE\t\t17\t/* set when VM_HUGEPAGE is set on vma */\n#define MMF_EXE_FILE_CHANGED\t18\t/* see prctl_set_mm_exe_file() */\n\n#define MMF_HAS_UPROBES\t\t19\t/* has uprobes */\n#define MMF_RECALC_UPROBES\t20\t/* MMF_HAS_UPROBES can be wrong */\n\n#define MMF_INIT_MASK\t\t(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)\n\nstruct sighand_struct {\n\tatomic_t\t\tcount;\n\tstruct k_sigaction\taction[_NSIG];\n\tspinlock_t\t\tsiglock;\n\twait_queue_head_t\tsignalfd_wqh;\n};\n\nstruct pacct_struct {\n\tint\t\t\tac_flag;\n\tlong\t\t\tac_exitcode;\n\tunsigned long\t\tac_mem;\n\tcputime_t\t\tac_utime, ac_stime;\n\tunsigned long\t\tac_minflt, ac_majflt;\n};\n\nstruct cpu_itimer {\n\tcputime_t expires;\n\tcputime_t incr;\n\tu32 error;\n\tu32 incr_error;\n};\n\n/**\n * struct cputime - snaphsot of system and user cputime\n * @utime: time spent in user mode\n * @stime: time spent in system mode\n *\n * Gathers a generic snapshot of user and system time.\n */\nstruct cputime {\n\tcputime_t utime;\n\tcputime_t stime;\n};\n\n/**\n * struct task_cputime - collected CPU time counts\n * @utime:\t\ttime spent in user mode, in &cputime_t units\n * @stime:\t\ttime spent in kernel mode, in &cputime_t units\n * @sum_exec_runtime:\ttotal time spent on the CPU, in nanoseconds\n *\n * This is an extension of struct cputime that includes the total runtime\n * spent by the task from the scheduler point of view.\n *\n * As a result, this structure groups together three kinds of CPU time\n * that are tracked for threads and thread groups.  Most things considering\n * CPU time want to group these counts together and treat all three\n * of them in parallel.\n */\nstruct task_cputime {\n\tcputime_t utime;\n\tcputime_t stime;\n\tunsigned long long sum_exec_runtime;\n};\n/* Alternate field names when used to cache expirations. */\n#define prof_exp\tstime\n#define virt_exp\tutime\n#define sched_exp\tsum_exec_runtime\n\n#define INIT_CPUTIME\t\\\n\t(struct task_cputime) {\t\t\t\t\t\\\n\t\t.utime = 0,\t\t\t\t\t\\\n\t\t.stime = 0,\t\t\t\t\t\\\n\t\t.sum_exec_runtime = 0,\t\t\t\t\\\n\t}\n\n#define PREEMPT_ENABLED\t\t(PREEMPT_NEED_RESCHED)\n\n#ifdef CONFIG_PREEMPT_COUNT\n#define PREEMPT_DISABLED\t(1 + PREEMPT_ENABLED)\n#else\n#define PREEMPT_DISABLED\tPREEMPT_ENABLED\n#endif\n\n/*\n * Disable preemption until the scheduler is running.\n * Reset by start_kernel()->sched_init()->init_idle().\n *\n * We include PREEMPT_ACTIVE to avoid cond_resched() from working\n * before the scheduler is active -- see should_resched().\n */\n#define INIT_PREEMPT_COUNT\t(PREEMPT_DISABLED + PREEMPT_ACTIVE)\n\n/**\n * struct thread_group_cputimer - thread group interval timer counts\n * @cputime:\t\tthread group interval timers.\n * @running:\t\tnon-zero when there are timers running and\n * \t\t\t@cputime receives updates.\n * @lock:\t\tlock for fields in this struct.\n *\n * This structure contains the version of task_cputime, above, that is\n * used for thread group CPU timer calculations.\n */\nstruct thread_group_cputimer {\n\tstruct task_cputime cputime;\n\tint running;\n\traw_spinlock_t lock;\n};\n\n#include <linux/rwsem.h>\nstruct autogroup;\n\n/*\n * NOTE! \"signal_struct\" does not have its own\n * locking, because a shared signal_struct always\n * implies a shared sighand_struct, so locking\n * sighand_struct is always a proper superset of\n * the locking of signal_struct.\n */\nstruct signal_struct {\n\tatomic_t\t\tsigcnt;\n\tatomic_t\t\tlive;\n\tint\t\t\tnr_threads;\n\n\twait_queue_head_t\twait_chldexit;\t/* for wait4() */\n\n\t/* current thread group signal load-balancing target: */\n\tstruct task_struct\t*curr_target;\n\n\t/* shared signal handling: */\n\tstruct sigpending\tshared_pending;\n\n\t/* thread group exit support */\n\tint\t\t\tgroup_exit_code;\n\t/* overloaded:\n\t * - notify group_exit_task when ->count is equal to notify_count\n\t * - everyone except group_exit_task is stopped during signal delivery\n\t *   of fatal signals, group_exit_task processes the signal.\n\t */\n\tint\t\t\tnotify_count;\n\tstruct task_struct\t*group_exit_task;\n\n\t/* thread group stop support, overloads group_exit_code too */\n\tint\t\t\tgroup_stop_count;\n\tunsigned int\t\tflags; /* see SIGNAL_* flags below */\n\n\t/*\n\t * PR_SET_CHILD_SUBREAPER marks a process, like a service\n\t * manager, to re-parent orphan (double-forking) child processes\n\t * to this process instead of 'init'. The service manager is\n\t * able to receive SIGCHLD signals and is able to investigate\n\t * the process until it calls wait(). All children of this\n\t * process will inherit a flag if they should look for a\n\t * child_subreaper process at exit.\n\t */\n\tunsigned int\t\tis_child_subreaper:1;\n\tunsigned int\t\thas_child_subreaper:1;\n\n\t/* POSIX.1b Interval Timers */\n\tint\t\t\tposix_timer_id;\n\tstruct list_head\tposix_timers;\n\n\t/* ITIMER_REAL timer for the process */\n\tstruct hrtimer real_timer;\n\tstruct pid *leader_pid;\n\tktime_t it_real_incr;\n\n\t/*\n\t * ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use\n\t * CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these\n\t * values are defined to 0 and 1 respectively\n\t */\n\tstruct cpu_itimer it[2];\n\n\t/*\n\t * Thread group totals for process CPU timers.\n\t * See thread_group_cputimer(), et al, for details.\n\t */\n\tstruct thread_group_cputimer cputimer;\n\n\t/* Earliest-expiration cache. */\n\tstruct task_cputime cputime_expires;\n\n\tstruct list_head cpu_timers[3];\n\n\tstruct pid *tty_old_pgrp;\n\n\t/* boolean value for session group leader */\n\tint leader;\n\n\tstruct tty_struct *tty; /* NULL if no tty */\n\n#ifdef CONFIG_SCHED_AUTOGROUP\n\tstruct autogroup *autogroup;\n#endif\n\t/*\n\t * Cumulative resource counters for dead threads in the group,\n\t * and for reaped dead child processes forked by this group.\n\t * Live threads maintain their own counters and add to these\n\t * in __exit_signal, except for the group leader.\n\t */\n\tcputime_t utime, stime, cutime, cstime;\n\tcputime_t gtime;\n\tcputime_t cgtime;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\n\tstruct cputime prev_cputime;\n#endif\n\tunsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;\n\tunsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;\n\tunsigned long inblock, oublock, cinblock, coublock;\n\tunsigned long maxrss, cmaxrss;\n\tstruct task_io_accounting ioac;\n\n\t/*\n\t * Cumulative ns of schedule CPU time fo dead threads in the\n\t * group, not including a zombie group leader, (This only differs\n\t * from jiffies_to_ns(utime + stime) if sched_clock uses something\n\t * other than jiffies.)\n\t */\n\tunsigned long long sum_sched_runtime;\n\n\t/*\n\t * We don't bother to synchronize most readers of this at all,\n\t * because there is no reader checking a limit that actually needs\n\t * to get both rlim_cur and rlim_max atomically, and either one\n\t * alone is a single word that can safely be read normally.\n\t * getrlimit/setrlimit use task_lock(current->group_leader) to\n\t * protect this instead of the siglock, because they really\n\t * have no need to disable irqs.\n\t */\n\tstruct rlimit rlim[RLIM_NLIMITS];\n\n#ifdef CONFIG_BSD_PROCESS_ACCT\n\tstruct pacct_struct pacct;\t/* per-process accounting information */\n#endif\n#ifdef CONFIG_TASKSTATS\n\tstruct taskstats *stats;\n#endif\n#ifdef CONFIG_AUDIT\n\tunsigned audit_tty;\n\tunsigned audit_tty_log_passwd;\n\tstruct tty_audit_buf *tty_audit_buf;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/*\n\t * group_rwsem prevents new tasks from entering the threadgroup and\n\t * member tasks from exiting,a more specifically, setting of\n\t * PF_EXITING.  fork and exit paths are protected with this rwsem\n\t * using threadgroup_change_begin/end().  Users which require\n\t * threadgroup to remain stable should use threadgroup_[un]lock()\n\t * which also takes care of exec path.  Currently, cgroup is the\n\t * only user.\n\t */\n\tstruct rw_semaphore group_rwsem;\n#endif\n\n\toom_flags_t oom_flags;\n\tshort oom_score_adj;\t\t/* OOM kill score adjustment */\n\tshort oom_score_adj_min;\t/* OOM kill score adjustment min value.\n\t\t\t\t\t * Only settable by CAP_SYS_RESOURCE. */\n\n\tstruct mutex cred_guard_mutex;\t/* guard against foreign influences on\n\t\t\t\t\t * credential calculations\n\t\t\t\t\t * (notably. ptrace) */\n};\n\n/*\n * Bits in flags field of signal_struct.\n */\n#define SIGNAL_STOP_STOPPED\t0x00000001 /* job control stop in effect */\n#define SIGNAL_STOP_CONTINUED\t0x00000002 /* SIGCONT since WCONTINUED reap */\n#define SIGNAL_GROUP_EXIT\t0x00000004 /* group exit in progress */\n#define SIGNAL_GROUP_COREDUMP\t0x00000008 /* coredump in progress */\n/*\n * Pending notifications to parent.\n */\n#define SIGNAL_CLD_STOPPED\t0x00000010\n#define SIGNAL_CLD_CONTINUED\t0x00000020\n#define SIGNAL_CLD_MASK\t\t(SIGNAL_CLD_STOPPED|SIGNAL_CLD_CONTINUED)\n\n#define SIGNAL_UNKILLABLE\t0x00000040 /* for init: ignore fatal signals */\n\n/* If true, all threads except ->group_exit_task have pending SIGKILL */\nstatic inline int signal_group_exit(const struct signal_struct *sig)\n{\n\treturn\t(sig->flags & SIGNAL_GROUP_EXIT) ||\n\t\t(sig->group_exit_task != NULL);\n}\n\n/*\n * Some day this will be a full-fledged user tracking system..\n */\nstruct user_struct {\n\tatomic_t __count;\t/* reference count */\n\tatomic_t processes;\t/* How many processes does this user have? */\n\tatomic_t files;\t\t/* How many open files does this user have? */\n\tatomic_t sigpending;\t/* How many pending signals does this user have? */\n#ifdef CONFIG_INOTIFY_USER\n\tatomic_t inotify_watches; /* How many inotify watches does this user have? */\n\tatomic_t inotify_devs;\t/* How many inotify devs does this user have opened? */\n#endif\n#ifdef CONFIG_FANOTIFY\n\tatomic_t fanotify_listeners;\n#endif\n#ifdef CONFIG_EPOLL\n\tatomic_long_t epoll_watches; /* The number of file descriptors currently watched */\n#endif\n#ifdef CONFIG_POSIX_MQUEUE\n\t/* protected by mq_lock\t*/\n\tunsigned long mq_bytes;\t/* How many bytes can be allocated to mqueue? */\n#endif\n\tunsigned long locked_shm; /* How many pages of mlocked shm ? */\n\n#ifdef CONFIG_KEYS\n\tstruct key *uid_keyring;\t/* UID specific keyring */\n\tstruct key *session_keyring;\t/* UID's default session keyring */\n#endif\n\n\t/* Hash table maintenance information */\n\tstruct hlist_node uidhash_node;\n\tkuid_t uid;\n\n#ifdef CONFIG_PERF_EVENTS\n\tatomic_long_t locked_vm;\n#endif\n};\n\nextern int uids_sysfs_init(void);\n\nextern struct user_struct *find_user(kuid_t);\n\nextern struct user_struct root_user;\n#define INIT_USER (&root_user)\n\n\nstruct backing_dev_info;\nstruct reclaim_state;\n\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\nstruct sched_info {\n\t/* cumulative counters */\n\tunsigned long pcount;\t      /* # of times run on this cpu */\n\tunsigned long long run_delay; /* time spent waiting on a runqueue */\n\n\t/* timestamps */\n\tunsigned long long last_arrival,/* when we last ran on a cpu */\n\t\t\t   last_queued;\t/* when we were last queued to run */\n};\n#endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */\n\n#ifdef CONFIG_TASK_DELAY_ACCT\nstruct task_delay_info {\n\tspinlock_t\tlock;\n\tunsigned int\tflags;\t/* Private per-task flags */\n\n\t/* For each stat XXX, add following, aligned appropriately\n\t *\n\t * struct timespec XXX_start, XXX_end;\n\t * u64 XXX_delay;\n\t * u32 XXX_count;\n\t *\n\t * Atomicity of updates to XXX_delay, XXX_count protected by\n\t * single lock above (split into XXX_lock if contention is an issue).\n\t */\n\n\t/*\n\t * XXX_count is incremented on every XXX operation, the delay\n\t * associated with the operation is added to XXX_delay.\n\t * XXX_delay contains the accumulated delay time in nanoseconds.\n\t */\n\tstruct timespec blkio_start, blkio_end;\t/* Shared by blkio, swapin */\n\tu64 blkio_delay;\t/* wait for sync block io completion */\n\tu64 swapin_delay;\t/* wait for swapin block io completion */\n\tu32 blkio_count;\t/* total count of the number of sync block */\n\t\t\t\t/* io operations performed */\n\tu32 swapin_count;\t/* total count of the number of swapin block */\n\t\t\t\t/* io operations performed */\n\n\tstruct timespec freepages_start, freepages_end;\n\tu64 freepages_delay;\t/* wait for memory reclaim */\n\tu32 freepages_count;\t/* total count of memory reclaim */\n};\n#endif\t/* CONFIG_TASK_DELAY_ACCT */\n\nstatic inline int sched_info_on(void)\n{\n#ifdef CONFIG_SCHEDSTATS\n\treturn 1;\n#elif defined(CONFIG_TASK_DELAY_ACCT)\n\textern int delayacct_on;\n\treturn delayacct_on;\n#else\n\treturn 0;\n#endif\n}\n\nenum cpu_idle_type {\n\tCPU_IDLE,\n\tCPU_NOT_IDLE,\n\tCPU_NEWLY_IDLE,\n\tCPU_MAX_IDLE_TYPES\n};\n\n/*\n * Increase resolution of cpu_power calculations\n */\n#define SCHED_POWER_SHIFT\t10\n#define SCHED_POWER_SCALE\t(1L << SCHED_POWER_SHIFT)\n\n/*\n * sched-domains (multiprocessor balancing) declarations:\n */\n#ifdef CONFIG_SMP\n#define SD_LOAD_BALANCE\t\t0x0001\t/* Do load balancing on this domain. */\n#define SD_BALANCE_NEWIDLE\t0x0002\t/* Balance when about to become idle */\n#define SD_BALANCE_EXEC\t\t0x0004\t/* Balance on exec */\n#define SD_BALANCE_FORK\t\t0x0008\t/* Balance on fork, clone */\n#define SD_BALANCE_WAKE\t\t0x0010  /* Balance on wakeup */\n#define SD_WAKE_AFFINE\t\t0x0020\t/* Wake task to waking CPU */\n#define SD_SHARE_CPUPOWER\t0x0080\t/* Domain members share cpu power */\n#define SD_SHARE_PKG_RESOURCES\t0x0200\t/* Domain members share cpu pkg resources */\n#define SD_SERIALIZE\t\t0x0400\t/* Only a single load balancing instance */\n#define SD_ASYM_PACKING\t\t0x0800  /* Place busy groups earlier in the domain */\n#define SD_PREFER_SIBLING\t0x1000\t/* Prefer to place tasks in a sibling domain */\n#define SD_OVERLAP\t\t0x2000\t/* sched_domains of this level overlap */\n#define SD_NUMA\t\t\t0x4000\t/* cross-node balancing */\n\nextern int __weak arch_sd_sibiling_asym_packing(void);\n\nstruct sched_domain_attr {\n\tint relax_domain_level;\n};\n\n#define SD_ATTR_INIT\t(struct sched_domain_attr) {\t\\\n\t.relax_domain_level = -1,\t\t\t\\\n}\n\nextern int sched_domain_level_max;\n\nstruct sched_group;\n\nstruct sched_domain {\n\t/* These fields must be setup */\n\tstruct sched_domain *parent;\t/* top domain must be null terminated */\n\tstruct sched_domain *child;\t/* bottom domain must be null terminated */\n\tstruct sched_group *groups;\t/* the balancing groups of the domain */\n\tunsigned long min_interval;\t/* Minimum balance interval ms */\n\tunsigned long max_interval;\t/* Maximum balance interval ms */\n\tunsigned int busy_factor;\t/* less balancing by factor if busy */\n\tunsigned int imbalance_pct;\t/* No balance until over watermark */\n\tunsigned int cache_nice_tries;\t/* Leave cache hot tasks for # tries */\n\tunsigned int busy_idx;\n\tunsigned int idle_idx;\n\tunsigned int newidle_idx;\n\tunsigned int wake_idx;\n\tunsigned int forkexec_idx;\n\tunsigned int smt_gain;\n\n\tint nohz_idle;\t\t\t/* NOHZ IDLE status */\n\tint flags;\t\t\t/* See SD_* */\n\tint level;\n\n\t/* Runtime fields. */\n\tunsigned long last_balance;\t/* init to jiffies. units in jiffies */\n\tunsigned int balance_interval;\t/* initialise to 1. units in ms. */\n\tunsigned int nr_balance_failed; /* initialise to 0 */\n\n\tu64 last_update;\n\n\t/* idle_balance() stats */\n\tu64 max_newidle_lb_cost;\n\tunsigned long next_decay_max_lb_cost;\n\n#ifdef CONFIG_SCHEDSTATS\n\t/* load_balance() stats */\n\tunsigned int lb_count[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_failed[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_balanced[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_imbalance[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_gained[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_hot_gained[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_nobusyg[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_nobusyq[CPU_MAX_IDLE_TYPES];\n\n\t/* Active load balancing */\n\tunsigned int alb_count;\n\tunsigned int alb_failed;\n\tunsigned int alb_pushed;\n\n\t/* SD_BALANCE_EXEC stats */\n\tunsigned int sbe_count;\n\tunsigned int sbe_balanced;\n\tunsigned int sbe_pushed;\n\n\t/* SD_BALANCE_FORK stats */\n\tunsigned int sbf_count;\n\tunsigned int sbf_balanced;\n\tunsigned int sbf_pushed;\n\n\t/* try_to_wake_up() stats */\n\tunsigned int ttwu_wake_remote;\n\tunsigned int ttwu_move_affine;\n\tunsigned int ttwu_move_balance;\n#endif\n#ifdef CONFIG_SCHED_DEBUG\n\tchar *name;\n#endif\n\tunion {\n\t\tvoid *private;\t\t/* used during construction */\n\t\tstruct rcu_head rcu;\t/* used during destruction */\n\t};\n\n\tunsigned int span_weight;\n\t/*\n\t * Span of all CPUs in this domain.\n\t *\n\t * NOTE: this field is variable length. (Allocated dynamically\n\t * by attaching extra space to the end of the structure,\n\t * depending on how many CPUs the kernel has booted up with)\n\t */\n\tunsigned long span[0];\n};\n\nstatic inline struct cpumask *sched_domain_span(struct sched_domain *sd)\n{\n\treturn to_cpumask(sd->span);\n}\n\nextern void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t\t    struct sched_domain_attr *dattr_new);\n\n/* Allocate an array of sched domains, for partition_sched_domains(). */\ncpumask_var_t *alloc_sched_domains(unsigned int ndoms);\nvoid free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);\n\nbool cpus_share_cache(int this_cpu, int that_cpu);\n\n#else /* CONFIG_SMP */\n\nstruct sched_domain_attr;\n\nstatic inline void\npartition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\tstruct sched_domain_attr *dattr_new)\n{\n}\n\nstatic inline bool cpus_share_cache(int this_cpu, int that_cpu)\n{\n\treturn true;\n}\n\n#endif\t/* !CONFIG_SMP */\n\n\nstruct io_context;\t\t\t/* See blkdev.h */\n\n\n#ifdef ARCH_HAS_PREFETCH_SWITCH_STACK\nextern void prefetch_stack(struct task_struct *t);\n#else\nstatic inline void prefetch_stack(struct task_struct *t) { }\n#endif\n\nstruct audit_context;\t\t/* See audit.c */\nstruct mempolicy;\nstruct pipe_inode_info;\nstruct uts_namespace;\n\nstruct load_weight {\n\tunsigned long weight, inv_weight;\n};\n\nstruct sched_avg {\n\t/*\n\t * These sums represent an infinite geometric series and so are bound\n\t * above by 1024/(1-y).  Thus we only need a u32 to store them for all\n\t * choices of y < 1-2^(-32)*1024.\n\t */\n\tu32 runnable_avg_sum, runnable_avg_period;\n\tu64 last_runnable_update;\n\ts64 decay_count;\n\tunsigned long load_avg_contrib;\n};\n\n#ifdef CONFIG_SCHEDSTATS\nstruct sched_statistics {\n\tu64\t\t\twait_start;\n\tu64\t\t\twait_max;\n\tu64\t\t\twait_count;\n\tu64\t\t\twait_sum;\n\tu64\t\t\tiowait_count;\n\tu64\t\t\tiowait_sum;\n\n\tu64\t\t\tsleep_start;\n\tu64\t\t\tsleep_max;\n\ts64\t\t\tsum_sleep_runtime;\n\n\tu64\t\t\tblock_start;\n\tu64\t\t\tblock_max;\n\tu64\t\t\texec_max;\n\tu64\t\t\tslice_max;\n\n\tu64\t\t\tnr_migrations_cold;\n\tu64\t\t\tnr_failed_migrations_affine;\n\tu64\t\t\tnr_failed_migrations_running;\n\tu64\t\t\tnr_failed_migrations_hot;\n\tu64\t\t\tnr_forced_migrations;\n\n\tu64\t\t\tnr_wakeups;\n\tu64\t\t\tnr_wakeups_sync;\n\tu64\t\t\tnr_wakeups_migrate;\n\tu64\t\t\tnr_wakeups_local;\n\tu64\t\t\tnr_wakeups_remote;\n\tu64\t\t\tnr_wakeups_affine;\n\tu64\t\t\tnr_wakeups_affine_attempts;\n\tu64\t\t\tnr_wakeups_passive;\n\tu64\t\t\tnr_wakeups_idle;\n};\n#endif\n\nstruct sched_entity {\n\tstruct load_weight\tload;\t\t/* for load-balancing */\n\tstruct rb_node\t\trun_node;\n\tstruct list_head\tgroup_node;\n\tunsigned int\t\ton_rq;\n\n\tu64\t\t\texec_start;\n\tu64\t\t\tsum_exec_runtime;\n\tu64\t\t\tvruntime;\n\tu64\t\t\tprev_sum_exec_runtime;\n\n\tu64\t\t\tnr_migrations;\n\n#ifdef CONFIG_SCHEDSTATS\n\tstruct sched_statistics statistics;\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tstruct sched_entity\t*parent;\n\t/* rq on which this entity is (to be) queued: */\n\tstruct cfs_rq\t\t*cfs_rq;\n\t/* rq \"owned\" by this entity/group: */\n\tstruct cfs_rq\t\t*my_q;\n#endif\n\n#ifdef CONFIG_SMP\n\t/* Per-entity load-tracking */\n\tstruct sched_avg\tavg;\n#endif\n};\n\nstruct sched_rt_entity {\n\tstruct list_head run_list;\n\tunsigned long timeout;\n\tunsigned long watchdog_stamp;\n\tunsigned int time_slice;\n\n\tstruct sched_rt_entity *back;\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity\t*parent;\n\t/* rq on which this entity is (to be) queued: */\n\tstruct rt_rq\t\t*rt_rq;\n\t/* rq \"owned\" by this entity/group: */\n\tstruct rt_rq\t\t*my_q;\n#endif\n};\n\n\nstruct rcu_node;\n\nenum perf_event_task_context {\n\tperf_invalid_context = -1,\n\tperf_hw_context = 0,\n\tperf_sw_context,\n\tperf_nr_task_contexts,\n};\n\nstruct task_struct {\n\tvolatile long state;\t/* -1 unrunnable, 0 runnable, >0 stopped */\n\tvoid *stack;\n\tatomic_t usage;\n\tunsigned int flags;\t/* per process flags, defined below */\n\tunsigned int ptrace;\n\n#ifdef CONFIG_SMP\n\tstruct llist_node wake_entry;\n\tint on_cpu;\n\tstruct task_struct *last_wakee;\n\tunsigned long wakee_flips;\n\tunsigned long wakee_flip_decay_ts;\n\n\tint wake_cpu;\n#endif\n\tint on_rq;\n\n\tint prio, static_prio, normal_prio;\n\tunsigned int rt_priority;\n\tconst struct sched_class *sched_class;\n\tstruct sched_entity se;\n\tstruct sched_rt_entity rt;\n#ifdef CONFIG_CGROUP_SCHED\n\tstruct task_group *sched_task_group;\n#endif\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\t/* list of struct preempt_notifier: */\n\tstruct hlist_head preempt_notifiers;\n#endif\n\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tunsigned int btrace_seq;\n#endif\n\n\tunsigned int policy;\n\tint nr_cpus_allowed;\n\tcpumask_t cpus_allowed;\n\n#ifdef CONFIG_PREEMPT_RCU\n\tint rcu_read_lock_nesting;\n\tchar rcu_read_unlock_special;\n\tstruct list_head rcu_node_entry;\n#endif /* #ifdef CONFIG_PREEMPT_RCU */\n#ifdef CONFIG_TREE_PREEMPT_RCU\n\tstruct rcu_node *rcu_blocked_node;\n#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */\n#ifdef CONFIG_RCU_BOOST\n\tstruct rt_mutex *rcu_boost_mutex;\n#endif /* #ifdef CONFIG_RCU_BOOST */\n\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\n\tstruct sched_info sched_info;\n#endif\n\n\tstruct list_head tasks;\n#ifdef CONFIG_SMP\n\tstruct plist_node pushable_tasks;\n#endif\n\n\tstruct mm_struct *mm, *active_mm;\n#ifdef CONFIG_COMPAT_BRK\n\tunsigned brk_randomized:1;\n#endif\n#if defined(SPLIT_RSS_COUNTING)\n\tstruct task_rss_stat\trss_stat;\n#endif\n/* task state */\n\tint exit_state;\n\tint exit_code, exit_signal;\n\tint pdeath_signal;  /*  The signal sent when the parent dies  */\n\tunsigned int jobctl;\t/* JOBCTL_*, siglock protected */\n\n\t/* Used for emulating ABI behavior of previous Linux versions */\n\tunsigned int personality;\n\n\tunsigned did_exec:1;\n\tunsigned in_execve:1;\t/* Tell the LSMs that the process is doing an\n\t\t\t\t * execve */\n\tunsigned in_iowait:1;\n\n\t/* task may not gain privileges */\n\tunsigned no_new_privs:1;\n\n\t/* Revert to default priority/policy when forking */\n\tunsigned sched_reset_on_fork:1;\n\tunsigned sched_contributes_to_load:1;\n\n\tpid_t pid;\n\tpid_t tgid;\n\n#ifdef CONFIG_CC_STACKPROTECTOR\n\t/* Canary value for the -fstack-protector gcc feature */\n\tunsigned long stack_canary;\n#endif\n\t/*\n\t * pointers to (original) parent process, youngest child, younger sibling,\n\t * older sibling, respectively.  (p->father can be replaced with\n\t * p->real_parent->pid)\n\t */\n\tstruct task_struct __rcu *real_parent; /* real parent process */\n\tstruct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */\n\t/*\n\t * children/sibling forms the list of my natural children\n\t */\n\tstruct list_head children;\t/* list of my children */\n\tstruct list_head sibling;\t/* linkage in my parent's children list */\n\tstruct task_struct *group_leader;\t/* threadgroup leader */\n\n\t/*\n\t * ptraced is the list of tasks this task is using ptrace on.\n\t * This includes both natural children and PTRACE_ATTACH targets.\n\t * p->ptrace_entry is p's link on the p->parent->ptraced list.\n\t */\n\tstruct list_head ptraced;\n\tstruct list_head ptrace_entry;\n\n\t/* PID/PID hash table linkage. */\n\tstruct pid_link pids[PIDTYPE_MAX];\n\tstruct list_head thread_group;\n\n\tstruct completion *vfork_done;\t\t/* for vfork() */\n\tint __user *set_child_tid;\t\t/* CLONE_CHILD_SETTID */\n\tint __user *clear_child_tid;\t\t/* CLONE_CHILD_CLEARTID */\n\n\tcputime_t utime, stime, utimescaled, stimescaled;\n\tcputime_t gtime;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\n\tstruct cputime prev_cputime;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_t vtime_seqlock;\n\tunsigned long long vtime_snap;\n\tenum {\n\t\tVTIME_SLEEPING = 0,\n\t\tVTIME_USER,\n\t\tVTIME_SYS,\n\t} vtime_snap_whence;\n#endif\n\tunsigned long nvcsw, nivcsw; /* context switch counts */\n\tstruct timespec start_time; \t\t/* monotonic time */\n\tstruct timespec real_start_time;\t/* boot based time */\n/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */\n\tunsigned long min_flt, maj_flt;\n\n\tstruct task_cputime cputime_expires;\n\tstruct list_head cpu_timers[3];\n\n/* process credentials */\n\tconst struct cred __rcu *real_cred; /* objective and real subjective task\n\t\t\t\t\t * credentials (COW) */\n\tconst struct cred __rcu *cred;\t/* effective (overridable) subjective task\n\t\t\t\t\t * credentials (COW) */\n\tchar comm[TASK_COMM_LEN]; /* executable name excluding path\n\t\t\t\t     - access with [gs]et_task_comm (which lock\n\t\t\t\t       it with task_lock())\n\t\t\t\t     - initialized normally by setup_new_exec */\n/* file system info */\n\tint link_count, total_link_count;\n#ifdef CONFIG_SYSVIPC\n/* ipc stuff */\n\tstruct sysv_sem sysvsem;\n#endif\n#ifdef CONFIG_DETECT_HUNG_TASK\n/* hung task detection */\n\tunsigned long last_switch_count;\n#endif\n/* CPU-specific state of this task */\n\tstruct thread_struct thread;\n/* filesystem information */\n\tstruct fs_struct *fs;\n/* open file information */\n\tstruct files_struct *files;\n/* namespaces */\n\tstruct nsproxy *nsproxy;\n/* signal handlers */\n\tstruct signal_struct *signal;\n\tstruct sighand_struct *sighand;\n\n\tsigset_t blocked, real_blocked;\n\tsigset_t saved_sigmask;\t/* restored if set_restore_sigmask() was used */\n\tstruct sigpending pending;\n\n\tunsigned long sas_ss_sp;\n\tsize_t sas_ss_size;\n\tint (*notifier)(void *priv);\n\tvoid *notifier_data;\n\tsigset_t *notifier_mask;\n\tstruct callback_head *task_works;\n\n\tstruct audit_context *audit_context;\n#ifdef CONFIG_AUDITSYSCALL\n\tkuid_t loginuid;\n\tunsigned int sessionid;\n#endif\n\tstruct seccomp seccomp;\n\n/* Thread group tracking */\n   \tu32 parent_exec_id;\n   \tu32 self_exec_id;\n/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,\n * mempolicy */\n\tspinlock_t alloc_lock;\n\n\t/* Protection of the PI data structures: */\n\traw_spinlock_t pi_lock;\n\n#ifdef CONFIG_RT_MUTEXES\n\t/* PI waiters blocked on a rt_mutex held by this task */\n\tstruct plist_head pi_waiters;\n\t/* Deadlock detection and priority inheritance handling */\n\tstruct rt_mutex_waiter *pi_blocked_on;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\t/* mutex deadlock detection */\n\tstruct mutex_waiter *blocked_on;\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tunsigned int irq_events;\n\tunsigned long hardirq_enable_ip;\n\tunsigned long hardirq_disable_ip;\n\tunsigned int hardirq_enable_event;\n\tunsigned int hardirq_disable_event;\n\tint hardirqs_enabled;\n\tint hardirq_context;\n\tunsigned long softirq_disable_ip;\n\tunsigned long softirq_enable_ip;\n\tunsigned int softirq_disable_event;\n\tunsigned int softirq_enable_event;\n\tint softirqs_enabled;\n\tint softirq_context;\n#endif\n#ifdef CONFIG_LOCKDEP\n# define MAX_LOCK_DEPTH 48UL\n\tu64 curr_chain_key;\n\tint lockdep_depth;\n\tunsigned int lockdep_recursion;\n\tstruct held_lock held_locks[MAX_LOCK_DEPTH];\n\tgfp_t lockdep_reclaim_gfp;\n#endif\n\n/* journalling filesystem info */\n\tvoid *journal_info;\n\n/* stacked block device info */\n\tstruct bio_list *bio_list;\n\n#ifdef CONFIG_BLOCK\n/* stack plugging */\n\tstruct blk_plug *plug;\n#endif\n\n/* VM state */\n\tstruct reclaim_state *reclaim_state;\n\n\tstruct backing_dev_info *backing_dev_info;\n\n\tstruct io_context *io_context;\n\n\tunsigned long ptrace_message;\n\tsiginfo_t *last_siginfo; /* For ptrace use.  */\n\tstruct task_io_accounting ioac;\n#if defined(CONFIG_TASK_XACCT)\n\tu64 acct_rss_mem1;\t/* accumulated rss usage */\n\tu64 acct_vm_mem1;\t/* accumulated virtual memory usage */\n\tcputime_t acct_timexpd;\t/* stime + utime since last update */\n#endif\n#ifdef CONFIG_CPUSETS\n\tnodemask_t mems_allowed;\t/* Protected by alloc_lock */\n\tseqcount_t mems_allowed_seq;\t/* Seqence no to catch updates */\n\tint cpuset_mem_spread_rotor;\n\tint cpuset_slab_spread_rotor;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/* Control Group info protected by css_set_lock */\n\tstruct css_set __rcu *cgroups;\n\t/* cg_list protected by css_set_lock and tsk->alloc_lock */\n\tstruct list_head cg_list;\n#endif\n#ifdef CONFIG_FUTEX\n\tstruct robust_list_head __user *robust_list;\n#ifdef CONFIG_COMPAT\n\tstruct compat_robust_list_head __user *compat_robust_list;\n#endif\n\tstruct list_head pi_state_list;\n\tstruct futex_pi_state *pi_state_cache;\n#endif\n#ifdef CONFIG_PERF_EVENTS\n\tstruct perf_event_context *perf_event_ctxp[perf_nr_task_contexts];\n\tstruct mutex perf_event_mutex;\n\tstruct list_head perf_event_list;\n#endif\n#ifdef CONFIG_NUMA\n\tstruct mempolicy *mempolicy;\t/* Protected by alloc_lock */\n\tshort il_next;\n\tshort pref_node_fork;\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\tint numa_scan_seq;\n\tunsigned int numa_scan_period;\n\tunsigned int numa_scan_period_max;\n\tint numa_preferred_nid;\n\tint numa_migrate_deferred;\n\tunsigned long numa_migrate_retry;\n\tu64 node_stamp;\t\t\t/* migration stamp  */\n\tstruct callback_head numa_work;\n\n\tstruct list_head numa_entry;\n\tstruct numa_group *numa_group;\n\n\t/*\n\t * Exponential decaying average of faults on a per-node basis.\n\t * Scheduling placement decisions are made based on the these counts.\n\t * The values remain static for the duration of a PTE scan\n\t */\n\tunsigned long *numa_faults;\n\tunsigned long total_numa_faults;\n\n\t/*\n\t * numa_faults_buffer records faults per node during the current\n\t * scan window. When the scan completes, the counts in numa_faults\n\t * decay and these values are copied.\n\t */\n\tunsigned long *numa_faults_buffer;\n\n\t/*\n\t * numa_faults_locality tracks if faults recorded during the last\n\t * scan window were remote/local. The task scan period is adapted\n\t * based on the locality of the faults with different weights\n\t * depending on whether they were shared or private faults\n\t */\n\tunsigned long numa_faults_locality[2];\n\n\tunsigned long numa_pages_migrated;\n#endif /* CONFIG_NUMA_BALANCING */\n\n\tstruct rcu_head rcu;\n\n\t/*\n\t * cache last used pipe for splice\n\t */\n\tstruct pipe_inode_info *splice_pipe;\n\n\tstruct page_frag task_frag;\n\n#ifdef\tCONFIG_TASK_DELAY_ACCT\n\tstruct task_delay_info *delays;\n#endif\n#ifdef CONFIG_FAULT_INJECTION\n\tint make_it_fail;\n#endif\n\t/*\n\t * when (nr_dirtied >= nr_dirtied_pause), it's time to call\n\t * balance_dirty_pages() for some dirty throttling pause\n\t */\n\tint nr_dirtied;\n\tint nr_dirtied_pause;\n\tunsigned long dirty_paused_when; /* start of a write-and-pause period */\n\n#ifdef CONFIG_LATENCYTOP\n\tint latency_record_count;\n\tstruct latency_record latency_record[LT_SAVECOUNT];\n#endif\n\t/*\n\t * time slack values; these are used to round up poll() and\n\t * select() etc timeout values. These are in nanoseconds.\n\t */\n\tunsigned long timer_slack_ns;\n\tunsigned long default_timer_slack_ns;\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t/* Index of current stored address in ret_stack */\n\tint curr_ret_stack;\n\t/* Stack of return addresses for return function tracing */\n\tstruct ftrace_ret_stack\t*ret_stack;\n\t/* time stamp for last schedule */\n\tunsigned long long ftrace_timestamp;\n\t/*\n\t * Number of functions that haven't been traced\n\t * because of depth overrun.\n\t */\n\tatomic_t trace_overrun;\n\t/* Pause for the tracing */\n\tatomic_t tracing_graph_pause;\n#endif\n#ifdef CONFIG_TRACING\n\t/* state flags for use by tracers */\n\tunsigned long trace;\n\t/* bitmask and counter of trace recursion */\n\tunsigned long trace_recursion;\n#endif /* CONFIG_TRACING */\n#ifdef CONFIG_MEMCG /* memcg uses this to do batch job */\n\tstruct memcg_batch_info {\n\t\tint do_batch;\t/* incremented when batch uncharge started */\n\t\tstruct mem_cgroup *memcg; /* target memcg of uncharge */\n\t\tunsigned long nr_pages;\t/* uncharged usage */\n\t\tunsigned long memsw_nr_pages; /* uncharged mem+swap usage */\n\t} memcg_batch;\n\tunsigned int memcg_kmem_skip_account;\n\tstruct memcg_oom_info {\n\t\tstruct mem_cgroup *memcg;\n\t\tgfp_t gfp_mask;\n\t\tint order;\n\t\tunsigned int may_oom:1;\n\t} memcg_oom;\n#endif\n#ifdef CONFIG_UPROBES\n\tstruct uprobe_task *utask;\n#endif\n#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)\n\tunsigned int\tsequential_io;\n\tunsigned int\tsequential_io_avg;\n#endif\n};\n\n/* Future-safe accessor for struct task_struct's cpus_allowed. */\n#define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)\n\n#define TNF_MIGRATED\t0x01\n#define TNF_NO_GROUP\t0x02\n#define TNF_SHARED\t0x04\n#define TNF_FAULT_LOCAL\t0x08\n\n#ifdef CONFIG_NUMA_BALANCING\nextern void task_numa_fault(int last_node, int node, int pages, int flags);\nextern pid_t task_numa_group_id(struct task_struct *p);\nextern void set_numabalancing_state(bool enabled);\nextern void task_numa_free(struct task_struct *p);\n\nextern unsigned int sysctl_numa_balancing_migrate_deferred;\n#else\nstatic inline void task_numa_fault(int last_node, int node, int pages,\n\t\t\t\t   int flags)\n{\n}\nstatic inline pid_t task_numa_group_id(struct task_struct *p)\n{\n\treturn 0;\n}\nstatic inline void set_numabalancing_state(bool enabled)\n{\n}\nstatic inline void task_numa_free(struct task_struct *p)\n{\n}\n#endif\n\nstatic inline struct pid *task_pid(struct task_struct *task)\n{\n\treturn task->pids[PIDTYPE_PID].pid;\n}\n\nstatic inline struct pid *task_tgid(struct task_struct *task)\n{\n\treturn task->group_leader->pids[PIDTYPE_PID].pid;\n}\n\n/*\n * Without tasklist or rcu lock it is not safe to dereference\n * the result of task_pgrp/task_session even if task == current,\n * we can race with another thread doing sys_setsid/sys_setpgid.\n */\nstatic inline struct pid *task_pgrp(struct task_struct *task)\n{\n\treturn task->group_leader->pids[PIDTYPE_PGID].pid;\n}\n\nstatic inline struct pid *task_session(struct task_struct *task)\n{\n\treturn task->group_leader->pids[PIDTYPE_SID].pid;\n}\n\nstruct pid_namespace;\n\n/*\n * the helpers to get the task's different pids as they are seen\n * from various namespaces\n *\n * task_xid_nr()     : global id, i.e. the id seen from the init namespace;\n * task_xid_vnr()    : virtual id, i.e. the id seen from the pid namespace of\n *                     current.\n * task_xid_nr_ns()  : id seen from the ns specified;\n *\n * set_task_vxid()   : assigns a virtual id to a task;\n *\n * see also pid_nr() etc in include/linux/pid.h\n */\npid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,\n\t\t\tstruct pid_namespace *ns);\n\nstatic inline pid_t task_pid_nr(struct task_struct *tsk)\n{\n\treturn tsk->pid;\n}\n\nstatic inline pid_t task_pid_nr_ns(struct task_struct *tsk,\n\t\t\t\t\tstruct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);\n}\n\nstatic inline pid_t task_pid_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);\n}\n\n\nstatic inline pid_t task_tgid_nr(struct task_struct *tsk)\n{\n\treturn tsk->tgid;\n}\n\npid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);\n\nstatic inline pid_t task_tgid_vnr(struct task_struct *tsk)\n{\n\treturn pid_vnr(task_tgid(tsk));\n}\n\n\nstatic inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,\n\t\t\t\t\tstruct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);\n}\n\nstatic inline pid_t task_pgrp_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, NULL);\n}\n\n\nstatic inline pid_t task_session_nr_ns(struct task_struct *tsk,\n\t\t\t\t\tstruct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);\n}\n\nstatic inline pid_t task_session_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);\n}\n\n/* obsolete, do not use */\nstatic inline pid_t task_pgrp_nr(struct task_struct *tsk)\n{\n\treturn task_pgrp_nr_ns(tsk, &init_pid_ns);\n}\n\n/**\n * pid_alive - check that a task structure is not stale\n * @p: Task structure to be checked.\n *\n * Test if a process is not yet dead (at most zombie state)\n * If pid_alive fails, then pointers within the task structure\n * can be stale and must not be dereferenced.\n *\n * Return: 1 if the process is alive. 0 otherwise.\n */\nstatic inline int pid_alive(struct task_struct *p)\n{\n\treturn p->pids[PIDTYPE_PID].pid != NULL;\n}\n\n/**\n * is_global_init - check if a task structure is init\n * @tsk: Task structure to be checked.\n *\n * Check if a task structure is the first user space task the kernel created.\n *\n * Return: 1 if the task structure is init. 0 otherwise.\n */\nstatic inline int is_global_init(struct task_struct *tsk)\n{\n\treturn tsk->pid == 1;\n}\n\nextern struct pid *cad_pid;\n\nextern void free_task(struct task_struct *tsk);\n#define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)\n\nextern void __put_task_struct(struct task_struct *t);\n\nstatic inline void put_task_struct(struct task_struct *t)\n{\n\tif (atomic_dec_and_test(&t->usage))\n\t\t__put_task_struct(t);\n}\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\nextern void task_cputime(struct task_struct *t,\n\t\t\t cputime_t *utime, cputime_t *stime);\nextern void task_cputime_scaled(struct task_struct *t,\n\t\t\t\tcputime_t *utimescaled, cputime_t *stimescaled);\nextern cputime_t task_gtime(struct task_struct *t);\n#else\nstatic inline void task_cputime(struct task_struct *t,\n\t\t\t\tcputime_t *utime, cputime_t *stime)\n{\n\tif (utime)\n\t\t*utime = t->utime;\n\tif (stime)\n\t\t*stime = t->stime;\n}\n\nstatic inline void task_cputime_scaled(struct task_struct *t,\n\t\t\t\t       cputime_t *utimescaled,\n\t\t\t\t       cputime_t *stimescaled)\n{\n\tif (utimescaled)\n\t\t*utimescaled = t->utimescaled;\n\tif (stimescaled)\n\t\t*stimescaled = t->stimescaled;\n}\n\nstatic inline cputime_t task_gtime(struct task_struct *t)\n{\n\treturn t->gtime;\n}\n#endif\nextern void task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);\nextern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);\n\n/*\n * Per process flags\n */\n#define PF_EXITING\t0x00000004\t/* getting shut down */\n#define PF_EXITPIDONE\t0x00000008\t/* pi exit done on shut down */\n#define PF_VCPU\t\t0x00000010\t/* I'm a virtual CPU */\n#define PF_WQ_WORKER\t0x00000020\t/* I'm a workqueue worker */\n#define PF_FORKNOEXEC\t0x00000040\t/* forked but didn't exec */\n#define PF_MCE_PROCESS  0x00000080      /* process policy on mce errors */\n#define PF_SUPERPRIV\t0x00000100\t/* used super-user privileges */\n#define PF_DUMPCORE\t0x00000200\t/* dumped core */\n#define PF_SIGNALED\t0x00000400\t/* killed by a signal */\n#define PF_MEMALLOC\t0x00000800\t/* Allocating memory */\n#define PF_NPROC_EXCEEDED 0x00001000\t/* set_user noticed that RLIMIT_NPROC was exceeded */\n#define PF_USED_MATH\t0x00002000\t/* if unset the fpu must be initialized before use */\n#define PF_USED_ASYNC\t0x00004000\t/* used async_schedule*(), used by module init */\n#define PF_NOFREEZE\t0x00008000\t/* this thread should not be frozen */\n#define PF_FROZEN\t0x00010000\t/* frozen for system suspend */\n#define PF_FSTRANS\t0x00020000\t/* inside a filesystem transaction */\n#define PF_KSWAPD\t0x00040000\t/* I am kswapd */\n#define PF_MEMALLOC_NOIO 0x00080000\t/* Allocating memory without IO involved */\n#define PF_LESS_THROTTLE 0x00100000\t/* Throttle me less: I clean memory */\n#define PF_KTHREAD\t0x00200000\t/* I am a kernel thread */\n#define PF_RANDOMIZE\t0x00400000\t/* randomize virtual address space */\n#define PF_SWAPWRITE\t0x00800000\t/* Allowed to write to swap */\n#define PF_SPREAD_PAGE\t0x01000000\t/* Spread page cache over cpuset */\n#define PF_SPREAD_SLAB\t0x02000000\t/* Spread some slab caches over cpuset */\n#define PF_NO_SETAFFINITY 0x04000000\t/* Userland is not allowed to meddle with cpus_allowed */\n#define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */\n#define PF_MEMPOLICY\t0x10000000\t/* Non-default NUMA mempolicy */\n#define PF_MUTEX_TESTER\t0x20000000\t/* Thread belongs to the rt mutex tester */\n#define PF_FREEZER_SKIP\t0x40000000\t/* Freezer should not count it as freezable */\n#define PF_SUSPEND_TASK 0x80000000      /* this thread called freeze_processes and should not be frozen */\n\n/*\n * Only the _current_ task can read/write to tsk->flags, but other\n * tasks can access tsk->flags in readonly mode for example\n * with tsk_used_math (like during threaded core dumping).\n * There is however an exception to this rule during ptrace\n * or during fork: the ptracer task is allowed to write to the\n * child->flags of its traced child (same goes for fork, the parent\n * can write to the child->flags), because we're guaranteed the\n * child is not running and in turn not changing child->flags\n * at the same time the parent does it.\n */\n#define clear_stopped_child_used_math(child) do { (child)->flags &= ~PF_USED_MATH; } while (0)\n#define set_stopped_child_used_math(child) do { (child)->flags |= PF_USED_MATH; } while (0)\n#define clear_used_math() clear_stopped_child_used_math(current)\n#define set_used_math() set_stopped_child_used_math(current)\n#define conditional_stopped_child_used_math(condition, child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)\n#define conditional_used_math(condition) \\\n\tconditional_stopped_child_used_math(condition, current)\n#define copy_to_stopped_child_used_math(child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)\n/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */\n#define tsk_used_math(p) ((p)->flags & PF_USED_MATH)\n#define used_math() tsk_used_math(current)\n\n/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags */\nstatic inline gfp_t memalloc_noio_flags(gfp_t flags)\n{\n\tif (unlikely(current->flags & PF_MEMALLOC_NOIO))\n\t\tflags &= ~__GFP_IO;\n\treturn flags;\n}\n\nstatic inline unsigned int memalloc_noio_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOIO;\n\tcurrent->flags |= PF_MEMALLOC_NOIO;\n\treturn flags;\n}\n\nstatic inline void memalloc_noio_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;\n}\n\n/*\n * task->jobctl flags\n */\n#define JOBCTL_STOP_SIGMASK\t0xffff\t/* signr of the last group stop */\n\n#define JOBCTL_STOP_DEQUEUED_BIT 16\t/* stop signal dequeued */\n#define JOBCTL_STOP_PENDING_BIT\t17\t/* task should stop for group stop */\n#define JOBCTL_STOP_CONSUME_BIT\t18\t/* consume group stop count */\n#define JOBCTL_TRAP_STOP_BIT\t19\t/* trap for STOP */\n#define JOBCTL_TRAP_NOTIFY_BIT\t20\t/* trap for NOTIFY */\n#define JOBCTL_TRAPPING_BIT\t21\t/* switching to TRACED */\n#define JOBCTL_LISTENING_BIT\t22\t/* ptracer is listening for events */\n\n#define JOBCTL_STOP_DEQUEUED\t(1 << JOBCTL_STOP_DEQUEUED_BIT)\n#define JOBCTL_STOP_PENDING\t(1 << JOBCTL_STOP_PENDING_BIT)\n#define JOBCTL_STOP_CONSUME\t(1 << JOBCTL_STOP_CONSUME_BIT)\n#define JOBCTL_TRAP_STOP\t(1 << JOBCTL_TRAP_STOP_BIT)\n#define JOBCTL_TRAP_NOTIFY\t(1 << JOBCTL_TRAP_NOTIFY_BIT)\n#define JOBCTL_TRAPPING\t\t(1 << JOBCTL_TRAPPING_BIT)\n#define JOBCTL_LISTENING\t(1 << JOBCTL_LISTENING_BIT)\n\n#define JOBCTL_TRAP_MASK\t(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)\n#define JOBCTL_PENDING_MASK\t(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)\n\nextern bool task_set_jobctl_pending(struct task_struct *task,\n\t\t\t\t    unsigned int mask);\nextern void task_clear_jobctl_trapping(struct task_struct *task);\nextern void task_clear_jobctl_pending(struct task_struct *task,\n\t\t\t\t      unsigned int mask);\n\n#ifdef CONFIG_PREEMPT_RCU\n\n#define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */\n#define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */\n\nstatic inline void rcu_copy_process(struct task_struct *p)\n{\n\tp->rcu_read_lock_nesting = 0;\n\tp->rcu_read_unlock_special = 0;\n#ifdef CONFIG_TREE_PREEMPT_RCU\n\tp->rcu_blocked_node = NULL;\n#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */\n#ifdef CONFIG_RCU_BOOST\n\tp->rcu_boost_mutex = NULL;\n#endif /* #ifdef CONFIG_RCU_BOOST */\n\tINIT_LIST_HEAD(&p->rcu_node_entry);\n}\n\n#else\n\nstatic inline void rcu_copy_process(struct task_struct *p)\n{\n}\n\n#endif\n\nstatic inline void tsk_restore_flags(struct task_struct *task,\n\t\t\t\tunsigned long orig_flags, unsigned long flags)\n{\n\ttask->flags &= ~flags;\n\ttask->flags |= orig_flags & flags;\n}\n\n#ifdef CONFIG_SMP\nextern void do_set_cpus_allowed(struct task_struct *p,\n\t\t\t       const struct cpumask *new_mask);\n\nextern int set_cpus_allowed_ptr(struct task_struct *p,\n\t\t\t\tconst struct cpumask *new_mask);\n#else\nstatic inline void do_set_cpus_allowed(struct task_struct *p,\n\t\t\t\t      const struct cpumask *new_mask)\n{\n}\nstatic inline int set_cpus_allowed_ptr(struct task_struct *p,\n\t\t\t\t       const struct cpumask *new_mask)\n{\n\tif (!cpumask_test_cpu(0, new_mask))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_NO_HZ_COMMON\nvoid calc_load_enter_idle(void);\nvoid calc_load_exit_idle(void);\n#else\nstatic inline void calc_load_enter_idle(void) { }\nstatic inline void calc_load_exit_idle(void) { }\n#endif /* CONFIG_NO_HZ_COMMON */\n\n#ifndef CONFIG_CPUMASK_OFFSTACK\nstatic inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)\n{\n\treturn set_cpus_allowed_ptr(p, &new_mask);\n}\n#endif\n\n/*\n * Do not use outside of architecture code which knows its limitations.\n *\n * sched_clock() has no promise of monotonicity or bounded drift between\n * CPUs, use (which you should not) requires disabling IRQs.\n *\n * Please use one of the three interfaces below.\n */\nextern unsigned long long notrace sched_clock(void);\n/*\n * See the comment in kernel/sched/clock.c\n */\nextern u64 cpu_clock(int cpu);\nextern u64 local_clock(void);\nextern u64 sched_clock_cpu(int cpu);\n\n\nextern void sched_clock_init(void);\n\n#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\nstatic inline void sched_clock_tick(void)\n{\n}\n\nstatic inline void sched_clock_idle_sleep_event(void)\n{\n}\n\nstatic inline void sched_clock_idle_wakeup_event(u64 delta_ns)\n{\n}\n#else\n/*\n * Architectures can set this to 1 if they have specified\n * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,\n * but then during bootup it turns out that sched_clock()\n * is reliable after all:\n */\nextern int sched_clock_stable;\n\nextern void sched_clock_tick(void);\nextern void sched_clock_idle_sleep_event(void);\nextern void sched_clock_idle_wakeup_event(u64 delta_ns);\n#endif\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n/*\n * An i/f to runtime opt-in for irq time accounting based off of sched_clock.\n * The reason for this explicit opt-in is not to have perf penalty with\n * slow sched_clocks.\n */\nextern void enable_sched_clock_irqtime(void);\nextern void disable_sched_clock_irqtime(void);\n#else\nstatic inline void enable_sched_clock_irqtime(void) {}\nstatic inline void disable_sched_clock_irqtime(void) {}\n#endif\n\nextern unsigned long long\ntask_sched_runtime(struct task_struct *task);\n\n/* sched_exec is called by processes performing an exec */\n#ifdef CONFIG_SMP\nextern void sched_exec(void);\n#else\n#define sched_exec()   {}\n#endif\n\nextern void sched_clock_idle_sleep_event(void);\nextern void sched_clock_idle_wakeup_event(u64 delta_ns);\n\n#ifdef CONFIG_HOTPLUG_CPU\nextern void idle_task_exit(void);\n#else\nstatic inline void idle_task_exit(void) {}\n#endif\n\n#if defined(CONFIG_NO_HZ_COMMON) && defined(CONFIG_SMP)\nextern void wake_up_nohz_cpu(int cpu);\n#else\nstatic inline void wake_up_nohz_cpu(int cpu) { }\n#endif\n\n#ifdef CONFIG_NO_HZ_FULL\nextern bool sched_can_stop_tick(void);\nextern u64 scheduler_tick_max_deferment(void);\n#else\nstatic inline bool sched_can_stop_tick(void) { return false; }\n#endif\n\n#ifdef CONFIG_SCHED_AUTOGROUP\nextern void sched_autogroup_create_attach(struct task_struct *p);\nextern void sched_autogroup_detach(struct task_struct *p);\nextern void sched_autogroup_fork(struct signal_struct *sig);\nextern void sched_autogroup_exit(struct signal_struct *sig);\n#ifdef CONFIG_PROC_FS\nextern void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m);\nextern int proc_sched_autogroup_set_nice(struct task_struct *p, int nice);\n#endif\n#else\nstatic inline void sched_autogroup_create_attach(struct task_struct *p) { }\nstatic inline void sched_autogroup_detach(struct task_struct *p) { }\nstatic inline void sched_autogroup_fork(struct signal_struct *sig) { }\nstatic inline void sched_autogroup_exit(struct signal_struct *sig) { }\n#endif\n\nextern bool yield_to(struct task_struct *p, bool preempt);\nextern void set_user_nice(struct task_struct *p, long nice);\nextern int task_prio(const struct task_struct *p);\nextern int task_nice(const struct task_struct *p);\nextern int can_nice(const struct task_struct *p, const int nice);\nextern int task_curr(const struct task_struct *p);\nextern int idle_cpu(int cpu);\nextern int sched_setscheduler(struct task_struct *, int,\n\t\t\t      const struct sched_param *);\nextern int sched_setscheduler_nocheck(struct task_struct *, int,\n\t\t\t\t      const struct sched_param *);\nextern struct task_struct *idle_task(int cpu);\n/**\n * is_idle_task - is the specified task an idle task?\n * @p: the task in question.\n *\n * Return: 1 if @p is an idle task. 0 otherwise.\n */\nstatic inline bool is_idle_task(const struct task_struct *p)\n{\n\treturn p->pid == 0;\n}\nextern struct task_struct *curr_task(int cpu);\nextern void set_curr_task(int cpu, struct task_struct *p);\n\nvoid yield(void);\n\n/*\n * The default (Linux) execution domain.\n */\nextern struct exec_domain\tdefault_exec_domain;\n\nunion thread_union {\n\tstruct thread_info thread_info;\n\tunsigned long stack[THREAD_SIZE/sizeof(long)];\n};\n\n#ifndef __HAVE_ARCH_KSTACK_END\nstatic inline int kstack_end(void *addr)\n{\n\t/* Reliable end of stack detection:\n\t * Some APM bios versions misalign the stack\n\t */\n\treturn !(((unsigned long)addr+sizeof(void*)-1) & (THREAD_SIZE-sizeof(void*)));\n}\n#endif\n\nextern union thread_union init_thread_union;\nextern struct task_struct init_task;\n\nextern struct   mm_struct init_mm;\n\nextern struct pid_namespace init_pid_ns;\n\n/*\n * find a task by one of its numerical ids\n *\n * find_task_by_pid_ns():\n *      finds a task by its pid in the specified namespace\n * find_task_by_vpid():\n *      finds a task by its virtual pid\n *\n * see also find_vpid() etc in include/linux/pid.h\n */\n\nextern struct task_struct *find_task_by_vpid(pid_t nr);\nextern struct task_struct *find_task_by_pid_ns(pid_t nr,\n\t\tstruct pid_namespace *ns);\n\n/* per-UID process charging. */\nextern struct user_struct * alloc_uid(kuid_t);\nstatic inline struct user_struct *get_uid(struct user_struct *u)\n{\n\tatomic_inc(&u->__count);\n\treturn u;\n}\nextern void free_uid(struct user_struct *);\n\n#include <asm/current.h>\n\nextern void xtime_update(unsigned long ticks);\n\nextern int wake_up_state(struct task_struct *tsk, unsigned int state);\nextern int wake_up_process(struct task_struct *tsk);\nextern void wake_up_new_task(struct task_struct *tsk);\n#ifdef CONFIG_SMP\n extern void kick_process(struct task_struct *tsk);\n#else\n static inline void kick_process(struct task_struct *tsk) { }\n#endif\nextern void sched_fork(unsigned long clone_flags, struct task_struct *p);\nextern void sched_dead(struct task_struct *p);\n\nextern void proc_caches_init(void);\nextern void flush_signals(struct task_struct *);\nextern void __flush_signals(struct task_struct *);\nextern void ignore_signals(struct task_struct *);\nextern void flush_signal_handlers(struct task_struct *, int force_default);\nextern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);\n\nstatic inline int dequeue_signal_lock(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&tsk->sighand->siglock, flags);\n\tret = dequeue_signal(tsk, mask, info);\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, flags);\n\n\treturn ret;\n}\n\nextern void block_all_signals(int (*notifier)(void *priv), void *priv,\n\t\t\t      sigset_t *mask);\nextern void unblock_all_signals(void);\nextern void release_task(struct task_struct * p);\nextern int send_sig_info(int, struct siginfo *, struct task_struct *);\nextern int force_sigsegv(int, struct task_struct *);\nextern int force_sig_info(int, struct siginfo *, struct task_struct *);\nextern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);\nextern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);\nextern int kill_pid_info_as_cred(int, struct siginfo *, struct pid *,\n\t\t\t\tconst struct cred *, u32);\nextern int kill_pgrp(struct pid *pid, int sig, int priv);\nextern int kill_pid(struct pid *pid, int sig, int priv);\nextern int kill_proc_info(int, struct siginfo *, pid_t);\nextern __must_check bool do_notify_parent(struct task_struct *, int);\nextern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);\nextern void force_sig(int, struct task_struct *);\nextern int send_sig(int, struct task_struct *, int);\nextern int zap_other_threads(struct task_struct *p);\nextern struct sigqueue *sigqueue_alloc(void);\nextern void sigqueue_free(struct sigqueue *);\nextern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);\nextern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);\n\nstatic inline void restore_saved_sigmask(void)\n{\n\tif (test_and_clear_restore_sigmask())\n\t\t__set_current_blocked(&current->saved_sigmask);\n}\n\nstatic inline sigset_t *sigmask_to_save(void)\n{\n\tsigset_t *res = &current->blocked;\n\tif (unlikely(test_restore_sigmask()))\n\t\tres = &current->saved_sigmask;\n\treturn res;\n}\n\nstatic inline int kill_cad_pid(int sig, int priv)\n{\n\treturn kill_pid(cad_pid, sig, priv);\n}\n\n/* These can be the second arg to send_sig_info/send_group_sig_info.  */\n#define SEND_SIG_NOINFO ((struct siginfo *) 0)\n#define SEND_SIG_PRIV\t((struct siginfo *) 1)\n#define SEND_SIG_FORCED\t((struct siginfo *) 2)\n\n/*\n * True if we are on the alternate signal stack.\n */\nstatic inline int on_sig_stack(unsigned long sp)\n{\n#ifdef CONFIG_STACK_GROWSUP\n\treturn sp >= current->sas_ss_sp &&\n\t\tsp - current->sas_ss_sp < current->sas_ss_size;\n#else\n\treturn sp > current->sas_ss_sp &&\n\t\tsp - current->sas_ss_sp <= current->sas_ss_size;\n#endif\n}\n\nstatic inline int sas_ss_flags(unsigned long sp)\n{\n\treturn (current->sas_ss_size == 0 ? SS_DISABLE\n\t\t: on_sig_stack(sp) ? SS_ONSTACK : 0);\n}\n\nstatic inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)\n{\n\tif (unlikely((ksig->ka.sa.sa_flags & SA_ONSTACK)) && ! sas_ss_flags(sp))\n#ifdef CONFIG_STACK_GROWSUP\n\t\treturn current->sas_ss_sp;\n#else\n\t\treturn current->sas_ss_sp + current->sas_ss_size;\n#endif\n\treturn sp;\n}\n\n/*\n * Routines for handling mm_structs\n */\nextern struct mm_struct * mm_alloc(void);\n\n/* mmdrop drops the mm and the page tables */\nextern void __mmdrop(struct mm_struct *);\nstatic inline void mmdrop(struct mm_struct * mm)\n{\n\tif (unlikely(atomic_dec_and_test(&mm->mm_count)))\n\t\t__mmdrop(mm);\n}\n\n/* mmput gets rid of the mappings and all user-space */\nextern void mmput(struct mm_struct *);\n/* Grab a reference to a task's mm, if it is not already going away */\nextern struct mm_struct *get_task_mm(struct task_struct *task);\n/*\n * Grab a reference to a task's mm, if it is not already going away\n * and ptrace_may_access with the mode parameter passed to it\n * succeeds.\n */\nextern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);\n/* Remove the current tasks stale references to the old mm_struct */\nextern void mm_release(struct task_struct *, struct mm_struct *);\n/* Allocate a new mm structure and copy contents from tsk->mm */\nextern struct mm_struct *dup_mm(struct task_struct *tsk);\n\nextern int copy_thread(unsigned long, unsigned long, unsigned long,\n\t\t\tstruct task_struct *);\nextern void flush_thread(void);\nextern void exit_thread(void);\n\nextern void exit_files(struct task_struct *);\nextern void __cleanup_sighand(struct sighand_struct *);\n\nextern void exit_itimers(struct signal_struct *);\nextern void flush_itimer_signals(void);\n\nextern void do_group_exit(int);\n\nextern int allow_signal(int);\nextern int disallow_signal(int);\n\nextern int do_execve(const char *,\n\t\t     const char __user * const __user *,\n\t\t     const char __user * const __user *);\nextern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);\nstruct task_struct *fork_idle(int);\nextern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);\n\nextern void set_task_comm(struct task_struct *tsk, char *from);\nextern char *get_task_comm(char *to, struct task_struct *tsk);\n\n#ifdef CONFIG_SMP\nvoid scheduler_ipi(void);\nextern unsigned long wait_task_inactive(struct task_struct *, long match_state);\n#else\nstatic inline void scheduler_ipi(void) { }\nstatic inline unsigned long wait_task_inactive(struct task_struct *p,\n\t\t\t\t\t       long match_state)\n{\n\treturn 1;\n}\n#endif\n\n#define next_task(p) \\\n\tlist_entry_rcu((p)->tasks.next, struct task_struct, tasks)\n\n#define for_each_process(p) \\\n\tfor (p = &init_task ; (p = next_task(p)) != &init_task ; )\n\nextern bool current_is_single_threaded(void);\n\n/*\n * Careful: do_each_thread/while_each_thread is a double loop so\n *          'break' will not work as expected - use goto instead.\n */\n#define do_each_thread(g, t) \\\n\tfor (g = t = &init_task ; (g = t = next_task(g)) != &init_task ; ) do\n\n#define while_each_thread(g, t) \\\n\twhile ((t = next_thread(t)) != g)\n\nstatic inline int get_nr_threads(struct task_struct *tsk)\n{\n\treturn tsk->signal->nr_threads;\n}\n\nstatic inline bool thread_group_leader(struct task_struct *p)\n{\n\treturn p->exit_signal >= 0;\n}\n\n/* Do to the insanities of de_thread it is possible for a process\n * to have the pid of the thread group leader without actually being\n * the thread group leader.  For iteration through the pids in proc\n * all we care about is that we have a task with the appropriate\n * pid, we don't actually care if we have the right task.\n */\nstatic inline bool has_group_leader_pid(struct task_struct *p)\n{\n\treturn task_pid(p) == p->signal->leader_pid;\n}\n\nstatic inline\nbool same_thread_group(struct task_struct *p1, struct task_struct *p2)\n{\n\treturn p1->signal == p2->signal;\n}\n\nstatic inline struct task_struct *next_thread(const struct task_struct *p)\n{\n\treturn list_entry_rcu(p->thread_group.next,\n\t\t\t      struct task_struct, thread_group);\n}\n\nstatic inline int thread_group_empty(struct task_struct *p)\n{\n\treturn list_empty(&p->thread_group);\n}\n\n#define delay_group_leader(p) \\\n\t\t(thread_group_leader(p) && !thread_group_empty(p))\n\n/*\n * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring\n * subscriptions and synchronises with wait4().  Also used in procfs.  Also\n * pins the final release of task.io_context.  Also protects ->cpuset and\n * ->cgroup.subsys[]. And ->vfork_done.\n *\n * Nests both inside and outside of read_lock(&tasklist_lock).\n * It must not be nested with write_lock_irq(&tasklist_lock),\n * neither inside nor outside.\n */\nstatic inline void task_lock(struct task_struct *p)\n{\n\tspin_lock(&p->alloc_lock);\n}\n\nstatic inline void task_unlock(struct task_struct *p)\n{\n\tspin_unlock(&p->alloc_lock);\n}\n\nextern struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t\t\tunsigned long *flags);\n\nstatic inline struct sighand_struct *lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t\t       unsigned long *flags)\n{\n\tstruct sighand_struct *ret;\n\n\tret = __lock_task_sighand(tsk, flags);\n\t(void)__cond_lock(&tsk->sighand->siglock, ret);\n\treturn ret;\n}\n\nstatic inline void unlock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t\tunsigned long *flags)\n{\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, *flags);\n}\n\n#ifdef CONFIG_CGROUPS\nstatic inline void threadgroup_change_begin(struct task_struct *tsk)\n{\n\tdown_read(&tsk->signal->group_rwsem);\n}\nstatic inline void threadgroup_change_end(struct task_struct *tsk)\n{\n\tup_read(&tsk->signal->group_rwsem);\n}\n\n/**\n * threadgroup_lock - lock threadgroup\n * @tsk: member task of the threadgroup to lock\n *\n * Lock the threadgroup @tsk belongs to.  No new task is allowed to enter\n * and member tasks aren't allowed to exit (as indicated by PF_EXITING) or\n * change ->group_leader/pid.  This is useful for cases where the threadgroup\n * needs to stay stable across blockable operations.\n *\n * fork and exit paths explicitly call threadgroup_change_{begin|end}() for\n * synchronization.  While held, no new task will be added to threadgroup\n * and no existing live task will have its PF_EXITING set.\n *\n * de_thread() does threadgroup_change_{begin|end}() when a non-leader\n * sub-thread becomes a new leader.\n */\nstatic inline void threadgroup_lock(struct task_struct *tsk)\n{\n\tdown_write(&tsk->signal->group_rwsem);\n}\n\n/**\n * threadgroup_unlock - unlock threadgroup\n * @tsk: member task of the threadgroup to unlock\n *\n * Reverse threadgroup_lock().\n */\nstatic inline void threadgroup_unlock(struct task_struct *tsk)\n{\n\tup_write(&tsk->signal->group_rwsem);\n}\n#else\nstatic inline void threadgroup_change_begin(struct task_struct *tsk) {}\nstatic inline void threadgroup_change_end(struct task_struct *tsk) {}\nstatic inline void threadgroup_lock(struct task_struct *tsk) {}\nstatic inline void threadgroup_unlock(struct task_struct *tsk) {}\n#endif\n\n#ifndef __HAVE_THREAD_FUNCTIONS\n\n#define task_thread_info(task)\t((struct thread_info *)(task)->stack)\n#define task_stack_page(task)\t((task)->stack)\n\nstatic inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)\n{\n\t*task_thread_info(p) = *task_thread_info(org);\n\ttask_thread_info(p)->task = p;\n}\n\nstatic inline unsigned long *end_of_stack(struct task_struct *p)\n{\n\treturn (unsigned long *)(task_thread_info(p) + 1);\n}\n\n#endif\n\nstatic inline int object_is_on_stack(void *obj)\n{\n\tvoid *stack = task_stack_page(current);\n\n\treturn (obj >= stack) && (obj < (stack + THREAD_SIZE));\n}\n\nextern void thread_info_cache_init(void);\n\n#ifdef CONFIG_DEBUG_STACK_USAGE\nstatic inline unsigned long stack_not_used(struct task_struct *p)\n{\n\tunsigned long *n = end_of_stack(p);\n\n\tdo { \t/* Skip over canary */\n\t\tn++;\n\t} while (!*n);\n\n\treturn (unsigned long)n - (unsigned long)end_of_stack(p);\n}\n#endif\n\n/* set thread flags in other task's structures\n * - see asm/thread_info.h for TIF_xxxx flags available\n */\nstatic inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tset_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tclear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_set_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void set_tsk_need_resched(struct task_struct *tsk)\n{\n\tset_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline void clear_tsk_need_resched(struct task_struct *tsk)\n{\n\tclear_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline int test_tsk_need_resched(struct task_struct *tsk)\n{\n\treturn unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));\n}\n\nstatic inline int restart_syscall(void)\n{\n\tset_tsk_thread_flag(current, TIF_SIGPENDING);\n\treturn -ERESTARTNOINTR;\n}\n\nstatic inline int signal_pending(struct task_struct *p)\n{\n\treturn unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));\n}\n\nstatic inline int __fatal_signal_pending(struct task_struct *p)\n{\n\treturn unlikely(sigismember(&p->pending.signal, SIGKILL));\n}\n\nstatic inline int fatal_signal_pending(struct task_struct *p)\n{\n\treturn signal_pending(p) && __fatal_signal_pending(p);\n}\n\nstatic inline int signal_pending_state(long state, struct task_struct *p)\n{\n\tif (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))\n\t\treturn 0;\n\tif (!signal_pending(p))\n\t\treturn 0;\n\n\treturn (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);\n}\n\n/*\n * cond_resched() and cond_resched_lock(): latency reduction via\n * explicit rescheduling in places that are safe. The return\n * value indicates whether a reschedule was done in fact.\n * cond_resched_lock() will drop the spinlock before scheduling,\n * cond_resched_softirq() will enable bhs before scheduling.\n */\nextern int _cond_resched(void);\n\n#define cond_resched() ({\t\t\t\\\n\t__might_sleep(__FILE__, __LINE__, 0);\t\\\n\t_cond_resched();\t\t\t\\\n})\n\nextern int __cond_resched_lock(spinlock_t *lock);\n\n#ifdef CONFIG_PREEMPT_COUNT\n#define PREEMPT_LOCK_OFFSET\tPREEMPT_OFFSET\n#else\n#define PREEMPT_LOCK_OFFSET\t0\n#endif\n\n#define cond_resched_lock(lock) ({\t\t\t\t\\\n\t__might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);\t\\\n\t__cond_resched_lock(lock);\t\t\t\t\\\n})\n\nextern int __cond_resched_softirq(void);\n\n#define cond_resched_softirq() ({\t\t\t\t\t\\\n\t__might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);\t\\\n\t__cond_resched_softirq();\t\t\t\t\t\\\n})\n\nstatic inline void cond_resched_rcu(void)\n{\n#if defined(CONFIG_DEBUG_ATOMIC_SLEEP) || !defined(CONFIG_PREEMPT_RCU)\n\trcu_read_unlock();\n\tcond_resched();\n\trcu_read_lock();\n#endif\n}\n\n/*\n * Does a critical section need to be broken due to another\n * task waiting?: (technically does not depend on CONFIG_PREEMPT,\n * but a general need for low latency)\n */\nstatic inline int spin_needbreak(spinlock_t *lock)\n{\n#ifdef CONFIG_PREEMPT\n\treturn spin_is_contended(lock);\n#else\n\treturn 0;\n#endif\n}\n\n/*\n * Idle thread specific functions to determine the need_resched\n * polling state. We have two versions, one based on TS_POLLING in\n * thread_info.status and one based on TIF_POLLING_NRFLAG in\n * thread_info.flags\n */\n#ifdef TS_POLLING\nstatic inline int tsk_is_polling(struct task_struct *p)\n{\n\treturn task_thread_info(p)->status & TS_POLLING;\n}\nstatic inline void __current_set_polling(void)\n{\n\tcurrent_thread_info()->status |= TS_POLLING;\n}\n\nstatic inline bool __must_check current_set_polling_and_test(void)\n{\n\t__current_set_polling();\n\n\t/*\n\t * Polling state must be visible before we test NEED_RESCHED,\n\t * paired by resched_task()\n\t */\n\tsmp_mb();\n\n\treturn unlikely(tif_need_resched());\n}\n\nstatic inline void __current_clr_polling(void)\n{\n\tcurrent_thread_info()->status &= ~TS_POLLING;\n}\n\nstatic inline bool __must_check current_clr_polling_and_test(void)\n{\n\t__current_clr_polling();\n\n\t/*\n\t * Polling state must be visible before we test NEED_RESCHED,\n\t * paired by resched_task()\n\t */\n\tsmp_mb();\n\n\treturn unlikely(tif_need_resched());\n}\n#elif defined(TIF_POLLING_NRFLAG)\nstatic inline int tsk_is_polling(struct task_struct *p)\n{\n\treturn test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);\n}\n\nstatic inline void __current_set_polling(void)\n{\n\tset_thread_flag(TIF_POLLING_NRFLAG);\n}\n\nstatic inline bool __must_check current_set_polling_and_test(void)\n{\n\t__current_set_polling();\n\n\t/*\n\t * Polling state must be visible before we test NEED_RESCHED,\n\t * paired by resched_task()\n\t *\n\t * XXX: assumes set/clear bit are identical barrier wise.\n\t */\n\tsmp_mb__after_clear_bit();\n\n\treturn unlikely(tif_need_resched());\n}\n\nstatic inline void __current_clr_polling(void)\n{\n\tclear_thread_flag(TIF_POLLING_NRFLAG);\n}\n\nstatic inline bool __must_check current_clr_polling_and_test(void)\n{\n\t__current_clr_polling();\n\n\t/*\n\t * Polling state must be visible before we test NEED_RESCHED,\n\t * paired by resched_task()\n\t */\n\tsmp_mb__after_clear_bit();\n\n\treturn unlikely(tif_need_resched());\n}\n\n#else\nstatic inline int tsk_is_polling(struct task_struct *p) { return 0; }\nstatic inline void __current_set_polling(void) { }\nstatic inline void __current_clr_polling(void) { }\n\nstatic inline bool __must_check current_set_polling_and_test(void)\n{\n\treturn unlikely(tif_need_resched());\n}\nstatic inline bool __must_check current_clr_polling_and_test(void)\n{\n\treturn unlikely(tif_need_resched());\n}\n#endif\n\nstatic __always_inline bool need_resched(void)\n{\n\treturn unlikely(tif_need_resched());\n}\n\n/*\n * Thread group CPU time accounting.\n */\nvoid thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);\nvoid thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);\n\nstatic inline void thread_group_cputime_init(struct signal_struct *sig)\n{\n\traw_spin_lock_init(&sig->cputimer.lock);\n}\n\n/*\n * Reevaluate whether the task has signals pending delivery.\n * Wake the task if so.\n * This is required every time the blocked sigset_t changes.\n * callers must hold sighand->siglock.\n */\nextern void recalc_sigpending_and_wake(struct task_struct *t);\nextern void recalc_sigpending(void);\n\nextern void signal_wake_up_state(struct task_struct *t, unsigned int state);\n\nstatic inline void signal_wake_up(struct task_struct *t, bool resume)\n{\n\tsignal_wake_up_state(t, resume ? TASK_WAKEKILL : 0);\n}\nstatic inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)\n{\n\tsignal_wake_up_state(t, resume ? __TASK_TRACED : 0);\n}\n\n/*\n * Wrappers for p->thread_info->cpu access. No-op on UP.\n */\n#ifdef CONFIG_SMP\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n\treturn task_thread_info(p)->cpu;\n}\n\nstatic inline int task_node(const struct task_struct *p)\n{\n\treturn cpu_to_node(task_cpu(p));\n}\n\nextern void set_task_cpu(struct task_struct *p, unsigned int cpu);\n\n#else\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n\treturn 0;\n}\n\nstatic inline void set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n}\n\n#endif /* CONFIG_SMP */\n\nextern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);\nextern long sched_getaffinity(pid_t pid, struct cpumask *mask);\n\n#ifdef CONFIG_CGROUP_SCHED\nextern struct task_group root_task_group;\n#endif /* CONFIG_CGROUP_SCHED */\n\nextern int task_can_switch_user(struct user_struct *up,\n\t\t\t\t\tstruct task_struct *tsk);\n\n#ifdef CONFIG_TASK_XACCT\nstatic inline void add_rchar(struct task_struct *tsk, ssize_t amt)\n{\n\ttsk->ioac.rchar += amt;\n}\n\nstatic inline void add_wchar(struct task_struct *tsk, ssize_t amt)\n{\n\ttsk->ioac.wchar += amt;\n}\n\nstatic inline void inc_syscr(struct task_struct *tsk)\n{\n\ttsk->ioac.syscr++;\n}\n\nstatic inline void inc_syscw(struct task_struct *tsk)\n{\n\ttsk->ioac.syscw++;\n}\n#else\nstatic inline void add_rchar(struct task_struct *tsk, ssize_t amt)\n{\n}\n\nstatic inline void add_wchar(struct task_struct *tsk, ssize_t amt)\n{\n}\n\nstatic inline void inc_syscr(struct task_struct *tsk)\n{\n}\n\nstatic inline void inc_syscw(struct task_struct *tsk)\n{\n}\n#endif\n\n#ifndef TASK_SIZE_OF\n#define TASK_SIZE_OF(tsk)\tTASK_SIZE\n#endif\n\n#ifdef CONFIG_MM_OWNER\nextern void mm_update_next_owner(struct mm_struct *mm);\nextern void mm_init_owner(struct mm_struct *mm, struct task_struct *p);\n#else\nstatic inline void mm_update_next_owner(struct mm_struct *mm)\n{\n}\n\nstatic inline void mm_init_owner(struct mm_struct *mm, struct task_struct *p)\n{\n}\n#endif /* CONFIG_MM_OWNER */\n\nstatic inline unsigned long task_rlimit(const struct task_struct *tsk,\n\t\tunsigned int limit)\n{\n\treturn ACCESS_ONCE(tsk->signal->rlim[limit].rlim_cur);\n}\n\nstatic inline unsigned long task_rlimit_max(const struct task_struct *tsk,\n\t\tunsigned int limit)\n{\n\treturn ACCESS_ONCE(tsk->signal->rlim[limit].rlim_max);\n}\n\nstatic inline unsigned long rlimit(unsigned int limit)\n{\n\treturn task_rlimit(current, limit);\n}\n\nstatic inline unsigned long rlimit_max(unsigned int limit)\n{\n\treturn task_rlimit_max(current, limit);\n}\n\n#endif\n", "/*\n * linux/kernel/ptrace.c\n *\n * (C) Copyright 1999 Linus Torvalds\n *\n * Common interfaces for \"ptrace()\" which we do not want\n * to continually duplicate across every architecture.\n */\n\n#include <linux/capability.h>\n#include <linux/export.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/ptrace.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/uio.h>\n#include <linux/audit.h>\n#include <linux/pid_namespace.h>\n#include <linux/syscalls.h>\n#include <linux/uaccess.h>\n#include <linux/regset.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/cn_proc.h>\n#include <linux/compat.h>\n\n\nstatic int ptrace_trapping_sleep_fn(void *flags)\n{\n\tschedule();\n\treturn 0;\n}\n\n/*\n * ptrace a task: make the debugger its new parent and\n * move it to the ptrace list.\n *\n * Must be called with the tasklist lock write-held.\n */\nvoid __ptrace_link(struct task_struct *child, struct task_struct *new_parent)\n{\n\tBUG_ON(!list_empty(&child->ptrace_entry));\n\tlist_add(&child->ptrace_entry, &new_parent->ptraced);\n\tchild->parent = new_parent;\n}\n\n/**\n * __ptrace_unlink - unlink ptracee and restore its execution state\n * @child: ptracee to be unlinked\n *\n * Remove @child from the ptrace list, move it back to the original parent,\n * and restore the execution state so that it conforms to the group stop\n * state.\n *\n * Unlinking can happen via two paths - explicit PTRACE_DETACH or ptracer\n * exiting.  For PTRACE_DETACH, unless the ptracee has been killed between\n * ptrace_check_attach() and here, it's guaranteed to be in TASK_TRACED.\n * If the ptracer is exiting, the ptracee can be in any state.\n *\n * After detach, the ptracee should be in a state which conforms to the\n * group stop.  If the group is stopped or in the process of stopping, the\n * ptracee should be put into TASK_STOPPED; otherwise, it should be woken\n * up from TASK_TRACED.\n *\n * If the ptracee is in TASK_TRACED and needs to be moved to TASK_STOPPED,\n * it goes through TRACED -> RUNNING -> STOPPED transition which is similar\n * to but in the opposite direction of what happens while attaching to a\n * stopped task.  However, in this direction, the intermediate RUNNING\n * state is not hidden even from the current ptracer and if it immediately\n * re-attaches and performs a WNOHANG wait(2), it may fail.\n *\n * CONTEXT:\n * write_lock_irq(tasklist_lock)\n */\nvoid __ptrace_unlink(struct task_struct *child)\n{\n\tBUG_ON(!child->ptrace);\n\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\n\tspin_lock(&child->sighand->siglock);\n\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tptrace_signal_wake_up(child, true);\n\n\tspin_unlock(&child->sighand->siglock);\n}\n\n/* Ensure that nothing can wake it up, even SIGKILL */\nstatic bool ptrace_freeze_traced(struct task_struct *task)\n{\n\tbool ret = false;\n\n\t/* Lockless, nobody but us can set this flag */\n\tif (task->jobctl & JOBCTL_LISTENING)\n\t\treturn ret;\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (task_is_traced(task) && !__fatal_signal_pending(task)) {\n\t\ttask->state = __TASK_TRACED;\n\t\tret = true;\n\t}\n\tspin_unlock_irq(&task->sighand->siglock);\n\n\treturn ret;\n}\n\nstatic void ptrace_unfreeze_traced(struct task_struct *task)\n{\n\tif (task->state != __TASK_TRACED)\n\t\treturn;\n\n\tWARN_ON(!task->ptrace || task->parent != current);\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (__fatal_signal_pending(task))\n\t\twake_up_state(task, __TASK_TRACED);\n\telse\n\t\ttask->state = TASK_TRACED;\n\tspin_unlock_irq(&task->sighand->siglock);\n}\n\n/**\n * ptrace_check_attach - check whether ptracee is ready for ptrace operation\n * @child: ptracee to check for\n * @ignore_state: don't check whether @child is currently %TASK_TRACED\n *\n * Check whether @child is being ptraced by %current and ready for further\n * ptrace operations.  If @ignore_state is %false, @child also should be in\n * %TASK_TRACED state and on return the child is guaranteed to be traced\n * and not executing.  If @ignore_state is %true, @child can be in any\n * state.\n *\n * CONTEXT:\n * Grabs and releases tasklist_lock and @child->sighand->siglock.\n *\n * RETURNS:\n * 0 on success, -ESRCH if %child is not ready.\n */\nstatic int ptrace_check_attach(struct task_struct *child, bool ignore_state)\n{\n\tint ret = -ESRCH;\n\n\t/*\n\t * We take the read lock around doing both checks to close a\n\t * possible race where someone else was tracing our child and\n\t * detached between these two checks.  After this locked check,\n\t * we are sure that this is our traced child and that can only\n\t * be changed by us so it's not changing right after this.\n\t */\n\tread_lock(&tasklist_lock);\n\tif (child->ptrace && child->parent == current) {\n\t\tWARN_ON(child->state == __TASK_TRACED);\n\t\t/*\n\t\t * child->sighand can't be NULL, release_task()\n\t\t * does ptrace_unlink() before __exit_signal().\n\t\t */\n\t\tif (ignore_state || ptrace_freeze_traced(child))\n\t\t\tret = 0;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tif (!ret && !ignore_state) {\n\t\tif (!wait_task_inactive(child, __TASK_TRACED)) {\n\t\t\t/*\n\t\t\t * This can only happen if may_ptrace_stop() fails and\n\t\t\t * ptrace_stop() changes ->state back to TASK_RUNNING,\n\t\t\t * so we should not worry about leaking __TASK_TRACED.\n\t\t\t */\n\t\t\tWARN_ON(child->state == __TASK_TRACED);\n\t\t\tret = -ESRCH;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int ptrace_has_cap(struct user_namespace *ns, unsigned int mode)\n{\n\tif (mode & PTRACE_MODE_NOAUDIT)\n\t\treturn has_ns_capability_noaudit(current, ns, CAP_SYS_PTRACE);\n\telse\n\t\treturn has_ns_capability(current, ns, CAP_SYS_PTRACE);\n}\n\n/* Returns 0 on success, -errno on denial. */\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\tint dumpable = 0;\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\ttcred = __task_cred(task);\n\tif (uid_eq(cred->uid, tcred->euid) &&\n\t    uid_eq(cred->uid, tcred->suid) &&\n\t    uid_eq(cred->uid, tcred->uid)  &&\n\t    gid_eq(cred->gid, tcred->egid) &&\n\t    gid_eq(cred->gid, tcred->sgid) &&\n\t    gid_eq(cred->gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tsmp_rmb();\n\tif (task->mm)\n\t\tdumpable = get_dumpable(task->mm);\n\trcu_read_lock();\n\tif (dumpable != SUID_DUMP_USER &&\n\t    !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn security_ptrace_access_check(task, mode);\n}\n\nbool ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tint err;\n\ttask_lock(task);\n\terr = __ptrace_may_access(task, mode);\n\ttask_unlock(task);\n\treturn !err;\n}\n\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT,\n\t\t\t    ptrace_trapping_sleep_fn, TASK_UNINTERRUPTIBLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n\n/**\n * ptrace_traceme  --  helper for PTRACE_TRACEME\n *\n * Performs checks and sets PT_PTRACED.\n * Should be used by all ptrace implementations for PTRACE_TRACEME.\n */\nstatic int ptrace_traceme(void)\n{\n\tint ret = -EPERM;\n\n\twrite_lock_irq(&tasklist_lock);\n\t/* Are we already being traced? */\n\tif (!current->ptrace) {\n\t\tret = security_ptrace_traceme(current->parent);\n\t\t/*\n\t\t * Check PF_EXITING to ensure ->real_parent has not passed\n\t\t * exit_ptrace(). Otherwise we don't report the error but\n\t\t * pretend ->real_parent untraces us right after return.\n\t\t */\n\t\tif (!ret && !(current->real_parent->flags & PF_EXITING)) {\n\t\t\tcurrent->ptrace = PT_PTRACED;\n\t\t\t__ptrace_link(current, current->real_parent);\n\t\t}\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * Called with irqs disabled, returns true if childs should reap themselves.\n */\nstatic int ignoring_children(struct sighand_struct *sigh)\n{\n\tint ret;\n\tspin_lock(&sigh->siglock);\n\tret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||\n\t      (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);\n\tspin_unlock(&sigh->siglock);\n\treturn ret;\n}\n\n/*\n * Called with tasklist_lock held for writing.\n * Unlink a traced task, and clean it up if it was a traced zombie.\n * Return true if it needs to be reaped with release_task().\n * (We can't call release_task() here because we already hold tasklist_lock.)\n *\n * If it's a zombie, our attachedness prevented normal parent notification\n * or self-reaping.  Do notification now if it would have happened earlier.\n * If it should reap itself, return true.\n *\n * If it's our own child, there is no notification to do. But if our normal\n * children self-reap, then this child was prevented by ptrace and we must\n * reap it now, in that case we must also wake up sub-threads sleeping in\n * do_wait().\n */\nstatic bool __ptrace_detach(struct task_struct *tracer, struct task_struct *p)\n{\n\tbool dead;\n\n\t__ptrace_unlink(p);\n\n\tif (p->exit_state != EXIT_ZOMBIE)\n\t\treturn false;\n\n\tdead = !thread_group_leader(p);\n\n\tif (!dead && thread_group_empty(p)) {\n\t\tif (!same_thread_group(p->real_parent, tracer))\n\t\t\tdead = do_notify_parent(p, p->exit_signal);\n\t\telse if (ignoring_children(tracer->sighand)) {\n\t\t\t__wake_up_parent(p, tracer);\n\t\t\tdead = true;\n\t\t}\n\t}\n\t/* Mark it as in the process of being reaped. */\n\tif (dead)\n\t\tp->exit_state = EXIT_DEAD;\n\treturn dead;\n}\n\nstatic int ptrace_detach(struct task_struct *child, unsigned int data)\n{\n\tbool dead = false;\n\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\t/* Architecture-specific hardware disable .. */\n\tptrace_disable(child);\n\tclear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\n\twrite_lock_irq(&tasklist_lock);\n\t/*\n\t * This child can be already killed. Make sure de_thread() or\n\t * our sub-thread doing do_wait() didn't do release_task() yet.\n\t */\n\tif (child->ptrace) {\n\t\tchild->exit_code = data;\n\t\tdead = __ptrace_detach(current, child);\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_ptrace_connector(child, PTRACE_DETACH);\n\tif (unlikely(dead))\n\t\trelease_task(child);\n\n\treturn 0;\n}\n\n/*\n * Detach all tasks we were using ptrace on. Called with tasklist held\n * for writing, and returns with it held too. But note it can release\n * and reacquire the lock.\n */\nvoid exit_ptrace(struct task_struct *tracer)\n\t__releases(&tasklist_lock)\n\t__acquires(&tasklist_lock)\n{\n\tstruct task_struct *p, *n;\n\tLIST_HEAD(ptrace_dead);\n\n\tif (likely(list_empty(&tracer->ptraced)))\n\t\treturn;\n\n\tlist_for_each_entry_safe(p, n, &tracer->ptraced, ptrace_entry) {\n\t\tif (unlikely(p->ptrace & PT_EXITKILL))\n\t\t\tsend_sig_info(SIGKILL, SEND_SIG_FORCED, p);\n\n\t\tif (__ptrace_detach(tracer, p))\n\t\t\tlist_add(&p->ptrace_entry, &ptrace_dead);\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\tBUG_ON(!list_empty(&tracer->ptraced));\n\n\tlist_for_each_entry_safe(p, n, &ptrace_dead, ptrace_entry) {\n\t\tlist_del_init(&p->ptrace_entry);\n\t\trelease_task(p);\n\t}\n\n\twrite_lock_irq(&tasklist_lock);\n}\n\nint ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tretval = access_process_vm(tsk, src, buf, this_len, 0);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tif (copy_to_user(dst, buf, retval))\n\t\t\treturn -EFAULT;\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nint ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tif (copy_from_user(buf, src, this_len))\n\t\t\treturn -EFAULT;\n\t\tretval = access_process_vm(tsk, dst, buf, this_len, 1);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nstatic int ptrace_setoptions(struct task_struct *child, unsigned long data)\n{\n\tunsigned flags;\n\n\tif (data & ~(unsigned long)PTRACE_O_MASK)\n\t\treturn -EINVAL;\n\n\t/* Avoid intermediate state when all opts are cleared */\n\tflags = child->ptrace;\n\tflags &= ~(PTRACE_O_MASK << PT_OPT_FLAG_SHIFT);\n\tflags |= (data << PT_OPT_FLAG_SHIFT);\n\tchild->ptrace = flags;\n\n\treturn 0;\n}\n\nstatic int ptrace_getsiginfo(struct task_struct *child, siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\t*info = *child->last_siginfo;\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_setsiginfo(struct task_struct *child, const siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\t*child->last_siginfo = *info;\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_peek_siginfo(struct task_struct *child,\n\t\t\t\tunsigned long addr,\n\t\t\t\tunsigned long data)\n{\n\tstruct ptrace_peeksiginfo_args arg;\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint ret, i;\n\n\tret = copy_from_user(&arg, (void __user *) addr,\n\t\t\t\tsizeof(struct ptrace_peeksiginfo_args));\n\tif (ret)\n\t\treturn -EFAULT;\n\n\tif (arg.flags & ~PTRACE_PEEKSIGINFO_SHARED)\n\t\treturn -EINVAL; /* unknown flags */\n\n\tif (arg.nr < 0)\n\t\treturn -EINVAL;\n\n\tif (arg.flags & PTRACE_PEEKSIGINFO_SHARED)\n\t\tpending = &child->signal->shared_pending;\n\telse\n\t\tpending = &child->pending;\n\n\tfor (i = 0; i < arg.nr; ) {\n\t\tsiginfo_t info;\n\t\ts32 off = arg.off + i;\n\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tlist_for_each_entry(q, &pending->list, list) {\n\t\t\tif (!off--) {\n\t\t\t\tcopy_siginfo(&info, &q->info);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\t\tif (off >= 0) /* beyond the end of the list */\n\t\t\tbreak;\n\n#ifdef CONFIG_COMPAT\n\t\tif (unlikely(is_compat_task())) {\n\t\t\tcompat_siginfo_t __user *uinfo = compat_ptr(data);\n\n\t\t\tif (copy_siginfo_to_user32(uinfo, &info) ||\n\t\t\t    __put_user(info.si_code, &uinfo->si_code)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t} else\n#endif\n\t\t{\n\t\t\tsiginfo_t __user *uinfo = (siginfo_t __user *) data;\n\n\t\t\tif (copy_siginfo_to_user(uinfo, &info) ||\n\t\t\t    __put_user(info.si_code, &uinfo->si_code)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tdata += sizeof(siginfo_t);\n\t\ti++;\n\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (i > 0)\n\t\treturn i;\n\n\treturn ret;\n}\n\n#ifdef PTRACE_SINGLESTEP\n#define is_singlestep(request)\t\t((request) == PTRACE_SINGLESTEP)\n#else\n#define is_singlestep(request)\t\t0\n#endif\n\n#ifdef PTRACE_SINGLEBLOCK\n#define is_singleblock(request)\t\t((request) == PTRACE_SINGLEBLOCK)\n#else\n#define is_singleblock(request)\t\t0\n#endif\n\n#ifdef PTRACE_SYSEMU\n#define is_sysemu_singlestep(request)\t((request) == PTRACE_SYSEMU_SINGLESTEP)\n#else\n#define is_sysemu_singlestep(request)\t0\n#endif\n\nstatic int ptrace_resume(struct task_struct *child, long request,\n\t\t\t unsigned long data)\n{\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\tif (request == PTRACE_SYSCALL)\n\t\tset_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\telse\n\t\tclear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\n#ifdef TIF_SYSCALL_EMU\n\tif (request == PTRACE_SYSEMU || request == PTRACE_SYSEMU_SINGLESTEP)\n\t\tset_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n\telse\n\t\tclear_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n#endif\n\n\tif (is_singleblock(request)) {\n\t\tif (unlikely(!arch_has_block_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_block_step(child);\n\t} else if (is_singlestep(request) || is_sysemu_singlestep(request)) {\n\t\tif (unlikely(!arch_has_single_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_single_step(child);\n\t} else {\n\t\tuser_disable_single_step(child);\n\t}\n\n\tchild->exit_code = data;\n\twake_up_state(child, __TASK_TRACED);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\nstatic const struct user_regset *\nfind_regset(const struct user_regset_view *view, unsigned int type)\n{\n\tconst struct user_regset *regset;\n\tint n;\n\n\tfor (n = 0; n < view->n; ++n) {\n\t\tregset = view->regsets + n;\n\t\tif (regset->core_note_type == type)\n\t\t\treturn regset;\n\t}\n\n\treturn NULL;\n}\n\nstatic int ptrace_regset(struct task_struct *task, int req, unsigned int type,\n\t\t\t struct iovec *kiov)\n{\n\tconst struct user_regset_view *view = task_user_regset_view(task);\n\tconst struct user_regset *regset = find_regset(view, type);\n\tint regset_no;\n\n\tif (!regset || (kiov->iov_len % regset->size) != 0)\n\t\treturn -EINVAL;\n\n\tregset_no = regset - view->regsets;\n\tkiov->iov_len = min(kiov->iov_len,\n\t\t\t    (__kernel_size_t) (regset->n * regset->size));\n\n\tif (req == PTRACE_GETREGSET)\n\t\treturn copy_regset_to_user(task, view, regset_no, 0,\n\t\t\t\t\t   kiov->iov_len, kiov->iov_base);\n\telse\n\t\treturn copy_regset_from_user(task, view, regset_no, 0,\n\t\t\t\t\t     kiov->iov_len, kiov->iov_base);\n}\n\n/*\n * This is declared in linux/regset.h and defined in machine-dependent\n * code.  We put the export here, near the primary machine-neutral use,\n * to ensure no machine forgets it.\n */\nEXPORT_SYMBOL_GPL(task_user_regset_view);\n#endif\n\nint ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tsiginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_PEEKSIGINFO:\n\t\tret = ptrace_peek_siginfo(child, addr, data);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tif (copy_from_user(&siginfo, datavp, sizeof siginfo))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGMASK:\n\t\tif (addr != sizeof(sigset_t)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_to_user(datavp, &child->blocked, sizeof(sigset_t)))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\tbreak;\n\n\tcase PTRACE_SETSIGMASK: {\n\t\tsigset_t new_set;\n\n\t\tif (addr != sizeof(sigset_t)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_from_user(&new_set, datavp, sizeof(sigset_t))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\t/*\n\t\t * Every thread does recalc_sigpending() after resume, so\n\t\t * retarget_shared_pending() and recalc_sigpending() are not\n\t\t * called here.\n\t\t */\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tchild->blocked = new_set;\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tptrace_signal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET: {\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic struct task_struct *ptrace_get_task_struct(pid_t pid)\n{\n\tstruct task_struct *child;\n\n\trcu_read_lock();\n\tchild = find_task_by_vpid(pid);\n\tif (child)\n\t\tget_task_struct(child);\n\trcu_read_unlock();\n\n\tif (!child)\n\t\treturn ERR_PTR(-ESRCH);\n\treturn child;\n}\n\n#ifndef arch_ptrace_attach\n#define arch_ptrace_attach(child)\tdo { } while (0)\n#endif\n\nSYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,\n\t\tunsigned long, data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(current);\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (ret < 0)\n\t\tgoto out_put_task_struct;\n\n\tret = arch_ptrace(child, request, addr, data);\n\tif (ret || request != PTRACE_DETACH)\n\t\tptrace_unfreeze_traced(child);\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n\nint generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tunsigned long tmp;\n\tint copied;\n\n\tcopied = access_process_vm(tsk, addr, &tmp, sizeof(tmp), 0);\n\tif (copied != sizeof(tmp))\n\t\treturn -EIO;\n\treturn put_user(tmp, (unsigned long __user *)data);\n}\n\nint generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tint copied;\n\n\tcopied = access_process_vm(tsk, addr, &data, sizeof(data), 1);\n\treturn (copied == sizeof(data)) ? 0 : -EIO;\n}\n\n#if defined CONFIG_COMPAT\n#include <linux/compat.h>\n\nint compat_ptrace_request(struct task_struct *child, compat_long_t request,\n\t\t\t  compat_ulong_t addr, compat_ulong_t data)\n{\n\tcompat_ulong_t __user *datap = compat_ptr(data);\n\tcompat_ulong_t word;\n\tsiginfo_t siginfo;\n\tint ret;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\tret = access_process_vm(child, addr, &word, sizeof(word), 0);\n\t\tif (ret != sizeof(word))\n\t\t\tret = -EIO;\n\t\telse\n\t\t\tret = put_user(word, datap);\n\t\tbreak;\n\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\tret = access_process_vm(child, addr, &data, sizeof(data), 1);\n\t\tret = (ret != sizeof(data) ? -EIO : 0);\n\t\tbreak;\n\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user((compat_ulong_t) child->ptrace_message, datap);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user32(\n\t\t\t\t(struct compat_siginfo __user *) datap,\n\t\t\t\t&siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tmemset(&siginfo, 0, sizeof siginfo);\n\t\tif (copy_siginfo_from_user32(\n\t\t\t    &siginfo, (struct compat_siginfo __user *) datap))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct compat_iovec __user *uiov =\n\t\t\t(struct compat_iovec __user *) datap;\n\t\tcompat_uptr_t ptr;\n\t\tcompat_size_t len;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(ptr, &uiov->iov_base) ||\n\t\t    __get_user(len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tkiov.iov_base = compat_ptr(ptr);\n\t\tkiov.iov_len = len;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t}\n\n\treturn ret;\n}\n\nasmlinkage long compat_sys_ptrace(compat_long_t request, compat_long_t pid,\n\t\t\t\t  compat_long_t addr, compat_long_t data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (!ret) {\n\t\tret = compat_arch_ptrace(child, request, addr, data);\n\t\tif (ret || request != PTRACE_DETACH)\n\t\t\tptrace_unfreeze_traced(child);\n\t}\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n#endif\t/* CONFIG_COMPAT */\n"], "filenames": ["arch/ia64/include/asm/processor.h", "fs/exec.c", "include/linux/binfmts.h", "include/linux/sched.h", "kernel/ptrace.c"], "buggy_code_start_loc": [322, 1671, 102, 324, 260], "buggy_code_end_loc": [323, 1671, 105, 324, 261], "fixing_code_start_loc": [322, 1672, 101, 325, 260], "fixing_code_end_loc": [323, 1678, 101, 329, 262], "type": "CWE-264", "message": "The Linux kernel before 3.12.2 does not properly use the get_dumpable function, which allows local users to bypass intended ptrace restrictions or obtain sensitive information from IA64 scratch registers via a crafted application, related to kernel/ptrace.c and arch/ia64/include/asm/processor.h.", "other": {"cve": {"id": "CVE-2013-2929", "sourceIdentifier": "cve-coordination@google.com", "published": "2013-12-09T18:55:09.280", "lastModified": "2018-04-28T01:29:00.407", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Linux kernel before 3.12.2 does not properly use the get_dumpable function, which allows local users to bypass intended ptrace restrictions or obtain sensitive information from IA64 scratch registers via a crafted application, related to kernel/ptrace.c and arch/ia64/include/asm/processor.h."}, {"lang": "es", "value": "El kernel Linux anterior a 3.12.2 no utiliza apropiadamente la funci\u00f3n get_dumpable, lo que permite a usuarios locales sortear restricciones ptrace u obtener informaci\u00f3n sensible de los registros IA64 a trav\u00e9s de una aplicaci\u00f3n manipulada, relacionado con kernel/ptrace.c y arch/ia64/include/asm/processor.h"}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:P/A:N", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 3.3}, "baseSeverity": "LOW", "exploitabilityScore": 3.4, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-264"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.12.1", "matchCriteriaId": "3E5AFED7-B198-4A43-B496-CD6B399748A6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.35:*:*:*:*:*:*:*", "matchCriteriaId": "F72412E3-8DA9-4CC9-A426-B534202ADBA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.36:*:*:*:*:*:*:*", "matchCriteriaId": "FCAA9D7A-3C3E-4C0B-9D38-EA80E68C2E46"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.37:*:*:*:*:*:*:*", "matchCriteriaId": "4A9E3AE5-3FCF-4CBB-A30B-082BCFBFB0CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.38:*:*:*:*:*:*:*", "matchCriteriaId": "CF715657-4C3A-4392-B85D-1BBF4DE45D89"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.39:*:*:*:*:*:*:*", "matchCriteriaId": "4B63C618-AC3D-4EF7-AFDF-27B9BF482B78"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.40:*:*:*:*:*:*:*", "matchCriteriaId": "C33DA5A9-5E40-4365-9602-82FB4DCD15B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.41:*:*:*:*:*:*:*", "matchCriteriaId": "EFAFDB74-40BD-46FA-89AC-617EB2C7160B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.42:*:*:*:*:*:*:*", "matchCriteriaId": "CF5F17DA-30A7-40CF-BD7C-CEDF06D64617"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.43:*:*:*:*:*:*:*", "matchCriteriaId": "71A276F5-BD9D-4C1B-90DF-9B0C15B6F7DF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.44:*:*:*:*:*:*:*", "matchCriteriaId": "F8F6EBEC-3C29-444B-BB85-6EF239B59EC1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.45:*:*:*:*:*:*:*", "matchCriteriaId": "FDB91302-FD18-44CF-A8A8-B31483328539"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.46:*:*:*:*:*:*:*", "matchCriteriaId": "9B81DC2B-46FA-4640-AD6C-2A404D94BA0B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.47:*:*:*:*:*:*:*", "matchCriteriaId": "BA6A1663-BC4C-4FC9-B5EB-A52EDED17B26"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.48:*:*:*:*:*:*:*", "matchCriteriaId": "69C33D6C-6B9F-49F4-B505-E7B589CDEC50"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.49:*:*:*:*:*:*:*", "matchCriteriaId": "C464796B-2F31-4159-A132-82A0C74137B7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.50:*:*:*:*:*:*:*", "matchCriteriaId": "1D6C6E46-FE29-4D2D-A0EC-43DA5112BCC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.51:*:*:*:*:*:*:*", "matchCriteriaId": "1A370E91-73A1-4D62-8E7B-696B920203F8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.52:*:*:*:*:*:*:*", "matchCriteriaId": "340197CD-9645-4B7E-B976-F3F5A7D4C5BE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.53:*:*:*:*:*:*:*", "matchCriteriaId": "96030636-0C4A-4A10-B768-525D6A0E18CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.54:*:*:*:*:*:*:*", "matchCriteriaId": "A42D8419-914F-4AD6-B0E9-C1290D514FF1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.55:*:*:*:*:*:*:*", "matchCriteriaId": "F4E2C88B-42EA-4F4F-B1F6-A9332EC6888B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.56:*:*:*:*:*:*:*", "matchCriteriaId": "2449D13B-3314-4182-832F-03F6B11AA31F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.57:*:*:*:*:*:*:*", "matchCriteriaId": "9A35B66C-F050-4462-A58E-FEE061B5582E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.58:*:*:*:*:*:*:*", "matchCriteriaId": "1B551164-0167-49BB-A3AE-4034BDA3DCB4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.59:*:*:*:*:*:*:*", "matchCriteriaId": "7244278E-49B6-4405-A14C-F3540C8F5AF8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.60:*:*:*:*:*:*:*", "matchCriteriaId": "B4C3E4B8-7274-4ABB-B7CE-6A39C183CE18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.61:*:*:*:*:*:*:*", "matchCriteriaId": "6501EDB9-4847-47F8-90EE-B295626E4CDC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.62:*:*:*:*:*:*:*", "matchCriteriaId": "2D676D48-7521-45E2-8563-6B966FF86A35"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.63:*:*:*:*:*:*:*", "matchCriteriaId": "3B69FA17-0AB9-4986-A5A7-2A4C1DD24222"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.64:*:*:*:*:*:*:*", "matchCriteriaId": "7BC35593-96C7-41F0-B738-1568F8129121"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.65:*:*:*:*:*:*:*", "matchCriteriaId": "38D23794-0E7C-4FA5-A7A8-CF940E3FA962"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.66:*:*:*:*:*:*:*", "matchCriteriaId": "008E1E7D-4C20-4560-9288-EF532ADB0029"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.67:*:*:*:*:*:*:*", "matchCriteriaId": "3B3A7044-A92E-47A9-A7BD-35E5B575F5FD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.68:*:*:*:*:*:*:*", "matchCriteriaId": "783E2980-B6AB-489E-B157-B6A2E10A32CA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc2:*:*:*:*:*:*", "matchCriteriaId": "99372D07-C06A-41FA-9843-6D57F99AB5AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc3:*:*:*:*:*:*", "matchCriteriaId": "2B9DC110-D260-4DB4-B8B0-EF1D160ADA07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc4:*:*:*:*:*:*", "matchCriteriaId": "6192FE84-4D53-40D4-AF61-78CE7136141A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc5:*:*:*:*:*:*", "matchCriteriaId": "42FEF3CF-1302-45EB-89CC-3786FE4BAC1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc6:*:*:*:*:*:*", "matchCriteriaId": "AE6A6B58-2C89-4DE4-BA57-78100818095C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc7:*:*:*:*:*:*", "matchCriteriaId": "1D467F87-2F13-4D26-9A93-E0BA526FEA24"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "FE348F7B-02DE-47D5-8011-F83DA9426021"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "E91594EA-F0A3-41B3-A9C6-F7864FC2F229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "9E1ECCDB-0208-48F6-B44F-16CC0ECE3503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "FBA8B5DE-372E-47E0-A0F6-BE286D509CC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "9A1CA083-2CF8-45AE-9E15-1AA3A8352E3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "19D69A49-5290-4C5F-8157-719AD58D253D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "290BD969-42E7-47B0-B21B-06DE4865432C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "23A9E29E-DE78-4C73-9FBD-C2410F5FC8B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "018434C9-E75F-45CB-A169-DAB4B1D864D7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "DC0AC68F-EC58-4C4F-8CBC-A59ECC00CCDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "C123C844-F6D7-471E-A62E-F756042FB1CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.12:*:*:*:*:*:*:*", "matchCriteriaId": "A11C38BB-7FA2-49B0-AAC9-83DB387A06DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.13:*:*:*:*:*:*:*", "matchCriteriaId": "61F3733C-E5F6-4855-B471-DF3FB823613B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.14:*:*:*:*:*:*:*", "matchCriteriaId": "1DDCA75F-9A06-4457-9A45-38A38E7F7086"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.15:*:*:*:*:*:*:*", "matchCriteriaId": "7AEA837E-7864-4003-8DB7-111ED710A7E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.16:*:*:*:*:*:*:*", "matchCriteriaId": "B6FE471F-2D1F-4A1D-A197-7E46B75787E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.17:*:*:*:*:*:*:*", "matchCriteriaId": "FDA9E6AB-58DC-4EC5-A25C-11F9D0B38BF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.18:*:*:*:*:*:*:*", "matchCriteriaId": "DC6B8DB3-B05B-41A2-B091-342D66AAE8F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.19:*:*:*:*:*:*:*", "matchCriteriaId": "958F0FF8-33EF-4A71-A0BD-572C85211DBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.20:*:*:*:*:*:*:*", "matchCriteriaId": "FBA39F48-B02F-4C48-B304-DA9CCA055244"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.21:*:*:*:*:*:*:*", "matchCriteriaId": "1FF841F3-48A7-41D7-9C45-A8170435A5EB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.22:*:*:*:*:*:*:*", "matchCriteriaId": "EF506916-A6DC-4B1E-90E5-959492AF55F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.23:*:*:*:*:*:*:*", "matchCriteriaId": "B3CDAD1F-2C6A-48C0-8FAB-C2659373FA25"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.24:*:*:*:*:*:*:*", "matchCriteriaId": "4FFE4B22-C96A-43D0-B993-F51EDD9C5E0E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.25:*:*:*:*:*:*:*", "matchCriteriaId": "F571CC8B-B212-4553-B463-1DB01D616E8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.26:*:*:*:*:*:*:*", "matchCriteriaId": "84E3E151-D437-48ED-A529-731EEFF88567"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.27:*:*:*:*:*:*:*", "matchCriteriaId": "E9E3EA3C-CCA5-4433-86E0-3D02C4757A0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.28:*:*:*:*:*:*:*", "matchCriteriaId": "F7AC4F7D-9FA6-4CF1-B2E9-70BF7D4D177C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.29:*:*:*:*:*:*:*", "matchCriteriaId": "3CE3A80D-9648-43CC-8F99-D741ED6552BF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.30:*:*:*:*:*:*:*", "matchCriteriaId": "C8A98C03-A465-41B4-A551-A26FEC7FFD94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "AFB76697-1C2F-48C0-9B14-517EC053D4B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc1:*:*:*:*:*:*", "matchCriteriaId": "BED88DFD-1DC5-4505-A441-44ECDEF0252D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc2:*:*:*:*:*:*", "matchCriteriaId": "DBFD2ACD-728A-4082-BB6A-A1EF6E58E47D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc3:*:*:*:*:*:*", "matchCriteriaId": "C31B0E51-F62D-4053-B04F-FC4D5BC373D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc4:*:*:*:*:*:*", "matchCriteriaId": "A914303E-1CB6-4AAD-9F5F-DE5433C4E814"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc5:*:*:*:*:*:*", "matchCriteriaId": "203BBA69-90B2-4C5E-8023-C14180742421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc6:*:*:*:*:*:*", "matchCriteriaId": "0DBFAB53-B889-4028-AC0E-7E165B152A18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc7:*:*:*:*:*:*", "matchCriteriaId": "FE409AEC-F677-4DEF-8EB7-2C35809043CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.1:*:*:*:*:*:*:*", "matchCriteriaId": "578EC12B-402F-4AD4-B8F8-C9B2CAB06891"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.2:*:*:*:*:*:*:*", "matchCriteriaId": "877002ED-8097-4BB4-BB88-6FC6306C38B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.3:*:*:*:*:*:*:*", "matchCriteriaId": "76294CE3-D72C-41D5-9E0F-B693D0042699"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.4:*:*:*:*:*:*:*", "matchCriteriaId": "916E97D4-1FAB-42F5-826B-653B1C0909A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.5:*:*:*:*:*:*:*", "matchCriteriaId": "33FD2217-C5D0-48C1-AD74-3527127FEF9C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.6:*:*:*:*:*:*:*", "matchCriteriaId": "2E92971F-B629-4E0A-9A50-8B235F9704B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.7:*:*:*:*:*:*:*", "matchCriteriaId": "EDD3A069-3829-4EE2-9D5A-29459F29D4C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.8:*:*:*:*:*:*:*", "matchCriteriaId": "A4A0964C-CEB2-41D7-A69C-1599B05B6171"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "0F960FA6-F904-4A4E-B483-44C70090E9A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:*:*", "matchCriteriaId": "261C1B41-C9E0-414F-8368-51C0C0B8AD38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:*:*", "matchCriteriaId": "5CCA261D-2B97-492F-89A0-5F209A804350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:*:*", "matchCriteriaId": "1B1C0C68-9194-473F-BE5E-EC7F184899FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:*:*", "matchCriteriaId": "D7A6AC9E-BEA6-44B0-B3B3-F0F94E32424A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:*:*", "matchCriteriaId": "16038328-9399-4B85-B777-BA4757D02C9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:*:*", "matchCriteriaId": "16CA2757-FA8D-43D9-96E8-D3C0EB6E1DEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:*:*", "matchCriteriaId": "E8CB5481-5EAE-401E-BD7E-D3095CCA9E94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.1:*:*:*:*:*:*:*", "matchCriteriaId": "A0F36FAC-141D-476D-84C5-A558C199F904"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.2:*:*:*:*:*:*:*", "matchCriteriaId": "51D64824-25F6-4761-BD6A-29038A143744"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.3:*:*:*:*:*:*:*", "matchCriteriaId": "E284C8A1-740F-454D-A774-99CD3A21B594"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.4:*:*:*:*:*:*:*", "matchCriteriaId": "C70D72AE-0CBF-4324-9935-57E28EC6279C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.5:*:*:*:*:*:*:*", "matchCriteriaId": "F674B06B-7E86-4E41-9126-8152D0DDABAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.6:*:*:*:*:*:*:*", "matchCriteriaId": "7039B3EC-8B22-413E-B582-B4BEC6181241"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.7:*:*:*:*:*:*:*", "matchCriteriaId": "35CF1DD2-80B9-4476-8963-5C3EF52B33F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.8:*:*:*:*:*:*:*", "matchCriteriaId": "BFB0B05B-A5CE-4B9C-AE7F-83062868D35B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.9:*:*:*:*:*:*:*", "matchCriteriaId": "D166A66E-7454-47EC-BB56-861A9AFEAFE1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.10:*:*:*:*:*:*:*", "matchCriteriaId": "7DA94F50-2A62-4300-BF4D-A342AAE35629"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.11:*:*:*:*:*:*:*", "matchCriteriaId": "252D937B-50DC-444F-AE73-5FCF6203DF27"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.12:*:*:*:*:*:*:*", "matchCriteriaId": "F6D8EE51-02C1-47BC-A92C-0A8ABEFD28FF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.13:*:*:*:*:*:*:*", "matchCriteriaId": "7F20A5D7-3B38-4911-861A-04C8310D5916"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.14:*:*:*:*:*:*:*", "matchCriteriaId": "D472DE3A-71D8-4F40-9DDE-85929A2B047D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.15:*:*:*:*:*:*:*", "matchCriteriaId": "B2AED943-65A8-4FDB-BBD0-CCEF8682A48C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.16:*:*:*:*:*:*:*", "matchCriteriaId": "D4640185-F3D8-4575-A71D-4C889A93DE2C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.17:*:*:*:*:*:*:*", "matchCriteriaId": "144CCF7C-025E-4879-B2E7-ABB8E4390BE5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.18:*:*:*:*:*:*:*", "matchCriteriaId": "B6FAA052-0B2B-40CE-8C98-919B8D08A5ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.19:*:*:*:*:*:*:*", "matchCriteriaId": "4B5A53DE-9C83-4A6B-96F3-23C03BF445D9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.20:*:*:*:*:*:*:*", "matchCriteriaId": "063EB879-CB05-4E33-AA90-9E43516839B5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.21:*:*:*:*:*:*:*", "matchCriteriaId": "2D25764F-4B02-4C65-954E-8C7D6632DE00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.22:*:*:*:*:*:*:*", "matchCriteriaId": "F31F5BF3-CD0A-465C-857F-273841BCD28A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.23:*:*:*:*:*:*:*", "matchCriteriaId": "FF302C8A-079B-42B9-B455-CD9083BFA067"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.24:*:*:*:*:*:*:*", "matchCriteriaId": "744999C0-33D3-4363-B3DB-E0D02CDD3918"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.25:*:*:*:*:*:*:*", "matchCriteriaId": "C2E77A76-2A60-45D8-9337-867BC22C5110"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.26:*:*:*:*:*:*:*", "matchCriteriaId": "C9F4AAE7-C870-46B7-B559-2949737BE777"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.27:*:*:*:*:*:*:*", "matchCriteriaId": "20FA2824-20B0-48B8-BB0A-4904C1D3E8AA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.28:*:*:*:*:*:*:*", "matchCriteriaId": "9F9B347E-61AC-419F-9701-B862BBFA46F2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.29:*:*:*:*:*:*:*", "matchCriteriaId": "989F351C-8B7C-4C1B-AFA2-AE9431576368"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.30:*:*:*:*:*:*:*", "matchCriteriaId": "8D22172A-9FA7-42E0-8451-165D8E47A573"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.31:*:*:*:*:*:*:*", "matchCriteriaId": "CE31624C-94F9-45D8-9B4A-D0028F10602F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.32:*:*:*:*:*:*:*", "matchCriteriaId": "70967A83-28F6-4568-9ADA-6EF232E5BBC2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.1:*:*:*:*:*:*:*", "matchCriteriaId": "962B0C45-AB29-4383-AC16-C6E8245D0FF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.2:*:*:*:*:*:*:*", "matchCriteriaId": "A0EE126B-74B2-4F79-BFE1-3DC169F3F9B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.3:*:*:*:*:*:*:*", "matchCriteriaId": "392075E0-A9C7-4B4A-90F9-7F1ADFF5EFA7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.4:*:*:*:*:*:*:*", "matchCriteriaId": "ECC66968-06F0-4874-A95A-A292C36E45C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.5:*:*:*:*:*:*:*", "matchCriteriaId": "5FE986E6-1068-4E1B-8EAB-DF1EAF32B4E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.6:*:*:*:*:*:*:*", "matchCriteriaId": "543E8536-1A8E-4E76-B89F-1B1F9F26FAB8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.7:*:*:*:*:*:*:*", "matchCriteriaId": "EC2B45E3-31E1-4B46-85FA-3A84E75B8F84"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6:*:*:*:*:*:*:*", "matchCriteriaId": "DDB8CC75-D3EE-417C-A83D-CB6D666FE595"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.1:*:*:*:*:*:*:*", "matchCriteriaId": "09A072F1-7BEE-4236-ACBB-55DB8FEF4A03"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.2:*:*:*:*:*:*:*", "matchCriteriaId": "E19D5A58-17D6-4502-A57A-70B2F84817A4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.3:*:*:*:*:*:*:*", "matchCriteriaId": "D58BA035-1204-4DFA-98A1-12111FB6222E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.4:*:*:*:*:*:*:*", "matchCriteriaId": "A17F2E87-8EB8-476A-B5B5-9AE5CF53D9FE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.5:*:*:*:*:*:*:*", "matchCriteriaId": "A8CCC101-5852-4299-9B67-EA1B149D58C0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.6:*:*:*:*:*:*:*", "matchCriteriaId": "B8074D32-C252-4AD3-A579-1C5EDDD7014B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.7:*:*:*:*:*:*:*", "matchCriteriaId": "962AA802-8179-4606-AAC0-9363BAEABC9F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.8:*:*:*:*:*:*:*", "matchCriteriaId": "1286C858-D5A2-45F3-86D1-E50FE53FB23C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.9:*:*:*:*:*:*:*", "matchCriteriaId": "5AC4A13E-F560-4D01-98A3-E2A2B82EB25B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.10:*:*:*:*:*:*:*", "matchCriteriaId": "942C462A-5398-4BB9-A792-598682E1FEF2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.11:*:*:*:*:*:*:*", "matchCriteriaId": "B852F7E0-0282-483D-BB4D-18CB7A4F1392"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7:*:*:*:*:*:*:*", "matchCriteriaId": "53ED9A31-99CC-41C8-8B72-5B2A9B49AA6C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.1:*:*:*:*:*:*:*", "matchCriteriaId": "EFD646BC-62F7-47CF-B0BE-768F701F7D9A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.2:*:*:*:*:*:*:*", "matchCriteriaId": "F43D418E-87C1-4C83-9FF1-4F45B4F452DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.3:*:*:*:*:*:*:*", "matchCriteriaId": "680D0E00-F29A-487C-8770-8E7EAC672B7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.4:*:*:*:*:*:*:*", "matchCriteriaId": "2DCA96A4-A836-4E94-A39C-3AD3EA1D9611"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.5:*:*:*:*:*:*:*", "matchCriteriaId": "753C05E3-B603-4E36-B9BA-FAEDCBF62A7D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.6:*:*:*:*:*:*:*", "matchCriteriaId": "E385C2E0-B9F1-4564-8E6D-56FD9E762405"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.7:*:*:*:*:*:*:*", "matchCriteriaId": "041335D4-05E1-4004-9381-28AAD5994B47"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.8:*:*:*:*:*:*:*", "matchCriteriaId": "370F2AE5-3DBC-46B9-AC70-F052C9229C00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.9:*:*:*:*:*:*:*", "matchCriteriaId": "7A971BE3-259D-4494-BBC5-12793D92DB57"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.10:*:*:*:*:*:*:*", "matchCriteriaId": "8E4719A6-FDEA-4714-A830-E23A52AE90BC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.0:*:*:*:*:*:*:*", "matchCriteriaId": "1A6E41FB-38CE-49F2-B796-9A5AA648E73F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.1:*:*:*:*:*:*:*", "matchCriteriaId": "93523FE1-5993-46CB-9299-7C8C1A04E873"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.2:*:*:*:*:*:*:*", "matchCriteriaId": "27ADC356-6BE9-43A3-9E0B-393DC4B1559A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.3:*:*:*:*:*:*:*", "matchCriteriaId": "4F543D23-1774-4D14-A7D1-AD49EDEA94DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.4:*:*:*:*:*:*:*", "matchCriteriaId": "FC323F58-CA00-4C3C-BA4D-CC2C0A6E5F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.5:*:*:*:*:*:*:*", "matchCriteriaId": "FEA0B2E3-668D-40ED-9D3D-709EB6449F8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.6:*:*:*:*:*:*:*", "matchCriteriaId": "3431B258-4EC8-4E7F-87BB-4D934880601E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.7:*:*:*:*:*:*:*", "matchCriteriaId": "1B09FA1E-8B28-4F2A-BA7E-8E1C40365970"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.8:*:*:*:*:*:*:*", "matchCriteriaId": "91917120-9D68-41C0-8B5D-85C256BC6200"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.9:*:*:*:*:*:*:*", "matchCriteriaId": "AAD268A0-096C-4C31-BEC5-D47F5149D462"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.10:*:*:*:*:*:*:*", "matchCriteriaId": "32BD2427-C47F-4660-A1D9-448E500EF5B9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.11:*:*:*:*:*:*:*", "matchCriteriaId": "02048CE5-81C7-4DFB-BC40-CE4C86B7E022"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.12:*:*:*:*:*:*:*", "matchCriteriaId": "934D2B37-0575-4A75-B00B-0028316D6DF0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.13:*:*:*:*:*:*:*", "matchCriteriaId": "06754C21-995C-4850-A4DC-F21826C0F8C5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc1:*:*:*:*:*:*", "matchCriteriaId": "42633FF9-FB0C-4095-B4A1-8D623A98683B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc2:*:*:*:*:*:*", "matchCriteriaId": "08C04619-89A2-4B15-82A2-48BCC662C1F1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc3:*:*:*:*:*:*", "matchCriteriaId": "5B039196-7159-476C-876A-C61242CC41DA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc4:*:*:*:*:*:*", "matchCriteriaId": "3A9E0457-53C9-44DD-ACFB-31EE1D1E060E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc5:*:*:*:*:*:*", "matchCriteriaId": "BEE406E7-87BA-44BA-BF61-673E6CC44A2F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc6:*:*:*:*:*:*", "matchCriteriaId": "29FBA173-658F-45DC-8205-934CACD67166"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc7:*:*:*:*:*:*", "matchCriteriaId": "139700F0-BA32-40CF-B9DF-C9C450384FDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.0:*:*:*:*:*:*:*", "matchCriteriaId": "E578085C-3968-4543-BEBA-EE3C3CB4FA02"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.1:*:*:*:*:*:*:*", "matchCriteriaId": "4DCFA441-68FB-4559-A245-FF0B79DE43CA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.2:*:*:*:*:*:*:*", "matchCriteriaId": "8C2508D8-6571-4B81-A0D7-E494CCD039CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.3:*:*:*:*:*:*:*", "matchCriteriaId": "8B516926-5E86-4C0A-85F3-F64E1FCDA249"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.4:*:*:*:*:*:*:*", "matchCriteriaId": "069D774D-79BE-479F-BF4E-F021AD808114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.5:*:*:*:*:*:*:*", "matchCriteriaId": "D15B27A9-46E0-4DDF-A00C-29F8F1F18D73"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.6:*:*:*:*:*:*:*", "matchCriteriaId": "A381BB4A-28B4-4672-87EE-91B3DDD6C71A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.7:*:*:*:*:*:*:*", "matchCriteriaId": "922F80CF-937D-4FA2-AFF2-6E47FFE9E1E9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.8:*:*:*:*:*:*:*", "matchCriteriaId": "A548ADF4-9E3B-407C-A5ED-05150EB3A185"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.9:*:*:*:*:*:*:*", "matchCriteriaId": "9C623230-4497-41B9-9BD2-7A6CFDD77983"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.10:*:*:*:*:*:*:*", "matchCriteriaId": "C72FA8A6-60A6-4486-A245-7BEF8B2A2711"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.11:*:*:*:*:*:*:*", "matchCriteriaId": "0A498D90-BB99-405E-9FA6-1FBFE179787E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.1:*:*:*:*:*:*:*", "matchCriteriaId": "D0D32776-8ADB-4E79-846A-C0C99FED19E0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.2:*:*:*:*:*:*:*", "matchCriteriaId": "B7D01673-D13F-487F-81B6-1279C187277E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.3:*:*:*:*:*:*:*", "matchCriteriaId": "ADB27A3E-78E4-40F7-9716-A1099B0D85FB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.4:*:*:*:*:*:*:*", "matchCriteriaId": "16E7136A-A8A6-4BF5-AF5D-AFB5C7A10712"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.5:*:*:*:*:*:*:*", "matchCriteriaId": "6FE127AC-E61D-427A-B998-D60DF5AABA21"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.6:*:*:*:*:*:*:*", "matchCriteriaId": "3819FF99-AEC5-4466-8542-D395419E4308"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.7:*:*:*:*:*:*:*", "matchCriteriaId": "E621FA1A-464B-4D2A-A0D6-EDA475A3709B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.8:*:*:*:*:*:*:*", "matchCriteriaId": "B760B422-EA11-43AB-B6D2-CA54E7229663"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.9:*:*:*:*:*:*:*", "matchCriteriaId": "D2CA7BBC-917C-4F31-A442-465C30444836"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.10:*:*:*:*:*:*:*", "matchCriteriaId": "AE778000-4FD5-4032-86CE-5930EF4CB7C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.11:*:*:*:*:*:*:*", "matchCriteriaId": "B3344EEB-F037-48FE-81DC-67F6384F7D9A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.12:*:*:*:*:*:*:*", "matchCriteriaId": "0244B0CA-9C67-4F06-BFBA-1F257112AC08"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.13:*:*:*:*:*:*:*", "matchCriteriaId": "2148C13F-4BB0-4D46-A688-F7C726D12497"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.14:*:*:*:*:*:*:*", "matchCriteriaId": "9871AF57-9158-4A41-8340-596B4463289A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.15:*:*:*:*:*:*:*", "matchCriteriaId": "2A875207-DF01-4240-8895-49B62693D27B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.16:*:*:*:*:*:*:*", "matchCriteriaId": "FE04A172-6F3B-4E3B-8D4D-564740FABAAF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.17:*:*:*:*:*:*:*", "matchCriteriaId": "CFEEF8C4-7DC2-4230-B58C-337F39A4DFAF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.18:*:*:*:*:*:*:*", "matchCriteriaId": "9F74DB5C-5096-438C-8C8A-6D337A2FD06A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.11:*:*:*:*:*:*:*", "matchCriteriaId": "639E3A57-A9E7-40E6-8929-81CCC0060EFB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.11.1:*:*:*:*:*:*:*", "matchCriteriaId": "07012ADD-F521-40A8-B067-E87C2238A3D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.11.2:*:*:*:*:*:*:*", "matchCriteriaId": "3F5FF393-3F89-4274-B82B-F671358072ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.11.3:*:*:*:*:*:*:*", "matchCriteriaId": "E348698F-54D1-4F5E-B701-CFAF50881E0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.11.4:*:*:*:*:*:*:*", "matchCriteriaId": "932205D9-3514-4289-9B55-C7A169276930"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.11.5:*:*:*:*:*:*:*", "matchCriteriaId": "2ECB2D33-F517-480F-8A6F-99D9D6C49596"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.11.6:*:*:*:*:*:*:*", "matchCriteriaId": "D16F68DD-E2D4-4AA4-AB81-3796C2947E37"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.11.7:*:*:*:*:*:*:*", "matchCriteriaId": "2422AC23-8410-4524-A733-25E4ABC7515D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.12:*:*:*:*:*:*:*", "matchCriteriaId": "B291154A-4B91-4A0E-AAAE-716A8BB7BF99"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=d049f74f2dbe71354d43d393ac3a188947811348", "source": "cve-coordination@google.com", "tags": ["Exploit", "Patch"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00010.html", "source": "cve-coordination@google.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00025.html", "source": "cve-coordination@google.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-0100.html", "source": "cve-coordination@google.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-0159.html", "source": "cve-coordination@google.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-0285.html", "source": "cve-coordination@google.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.12.2", "source": "cve-coordination@google.com"}, {"url": "http://www.securityfocus.com/bid/64111", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2070-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2075-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2109-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2110-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2111-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2112-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2114-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2115-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2116-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2128-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-2129-1", "source": "cve-coordination@google.com"}, {"url": "https://access.redhat.com/errata/RHSA-2018:1252", "source": "cve-coordination@google.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1028148", "source": "cve-coordination@google.com", "tags": ["Patch"]}, {"url": "https://github.com/torvalds/linux/commit/d049f74f2dbe71354d43d393ac3a188947811348", "source": "cve-coordination@google.com", "tags": ["Exploit", "Patch"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.2.54", "source": "cve-coordination@google.com"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/d049f74f2dbe71354d43d393ac3a188947811348"}}