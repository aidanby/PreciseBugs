{"buggy_code": ["\"\"\"PyPI and direct package downloading.\"\"\"\n\nimport sys\nimport os\nimport re\nimport io\nimport shutil\nimport socket\nimport base64\nimport hashlib\nimport itertools\nimport warnings\nimport configparser\nimport html\nimport http.client\nimport urllib.parse\nimport urllib.request\nimport urllib.error\nfrom functools import wraps\n\nimport setuptools\nfrom pkg_resources import (\n    CHECKOUT_DIST,\n    Distribution,\n    BINARY_DIST,\n    normalize_path,\n    SOURCE_DIST,\n    Environment,\n    find_distributions,\n    safe_name,\n    safe_version,\n    to_filename,\n    Requirement,\n    DEVELOP_DIST,\n    EGG_DIST,\n    parse_version,\n)\nfrom distutils import log\nfrom distutils.errors import DistutilsError\nfrom fnmatch import translate\nfrom setuptools.wheel import Wheel\nfrom setuptools.extern.more_itertools import unique_everseen\n\n\nEGG_FRAGMENT = re.compile(r'^egg=([-A-Za-z0-9_.+!]+)$')\nHREF = re.compile(r\"\"\"href\\s*=\\s*['\"]?([^'\"> ]+)\"\"\", re.I)\nPYPI_MD5 = re.compile(\n    r'<a href=\"([^\"#]+)\">([^<]+)</a>\\n\\s+\\(<a (?:title=\"MD5 hash\"\\n\\s+)'\n    r'href=\"[^?]+\\?:action=show_md5&amp;digest=([0-9a-f]{32})\">md5</a>\\)'\n)\nURL_SCHEME = re.compile('([-+.a-z0-9]{2,}):', re.I).match\nEXTENSIONS = \".tar.gz .tar.bz2 .tar .zip .tgz\".split()\n\n__all__ = [\n    'PackageIndex',\n    'distros_for_url',\n    'parse_bdist_wininst',\n    'interpret_distro_name',\n]\n\n_SOCKET_TIMEOUT = 15\n\n_tmpl = \"setuptools/{setuptools.__version__} Python-urllib/{py_major}\"\nuser_agent = _tmpl.format(\n    py_major='{}.{}'.format(*sys.version_info), setuptools=setuptools\n)\n\n\ndef parse_requirement_arg(spec):\n    try:\n        return Requirement.parse(spec)\n    except ValueError as e:\n        raise DistutilsError(\n            \"Not a URL, existing file, or requirement spec: %r\" % (spec,)\n        ) from e\n\n\ndef parse_bdist_wininst(name):\n    \"\"\"Return (base,pyversion) or (None,None) for possible .exe name\"\"\"\n\n    lower = name.lower()\n    base, py_ver, plat = None, None, None\n\n    if lower.endswith('.exe'):\n        if lower.endswith('.win32.exe'):\n            base = name[:-10]\n            plat = 'win32'\n        elif lower.startswith('.win32-py', -16):\n            py_ver = name[-7:-4]\n            base = name[:-16]\n            plat = 'win32'\n        elif lower.endswith('.win-amd64.exe'):\n            base = name[:-14]\n            plat = 'win-amd64'\n        elif lower.startswith('.win-amd64-py', -20):\n            py_ver = name[-7:-4]\n            base = name[:-20]\n            plat = 'win-amd64'\n    return base, py_ver, plat\n\n\ndef egg_info_for_url(url):\n    parts = urllib.parse.urlparse(url)\n    scheme, server, path, parameters, query, fragment = parts\n    base = urllib.parse.unquote(path.split('/')[-1])\n    if server == 'sourceforge.net' and base == 'download':  # XXX Yuck\n        base = urllib.parse.unquote(path.split('/')[-2])\n    if '#' in base:\n        base, fragment = base.split('#', 1)\n    return base, fragment\n\n\ndef distros_for_url(url, metadata=None):\n    \"\"\"Yield egg or source distribution objects that might be found at a URL\"\"\"\n    base, fragment = egg_info_for_url(url)\n    for dist in distros_for_location(url, base, metadata):\n        yield dist\n    if fragment:\n        match = EGG_FRAGMENT.match(fragment)\n        if match:\n            for dist in interpret_distro_name(\n                url, match.group(1), metadata, precedence=CHECKOUT_DIST\n            ):\n                yield dist\n\n\ndef distros_for_location(location, basename, metadata=None):\n    \"\"\"Yield egg or source distribution objects based on basename\"\"\"\n    if basename.endswith('.egg.zip'):\n        basename = basename[:-4]  # strip the .zip\n    if basename.endswith('.egg') and '-' in basename:\n        # only one, unambiguous interpretation\n        return [Distribution.from_location(location, basename, metadata)]\n    if basename.endswith('.whl') and '-' in basename:\n        wheel = Wheel(basename)\n        if not wheel.is_compatible():\n            return []\n        return [\n            Distribution(\n                location=location,\n                project_name=wheel.project_name,\n                version=wheel.version,\n                # Increase priority over eggs.\n                precedence=EGG_DIST + 1,\n            )\n        ]\n    if basename.endswith('.exe'):\n        win_base, py_ver, platform = parse_bdist_wininst(basename)\n        if win_base is not None:\n            return interpret_distro_name(\n                location, win_base, metadata, py_ver, BINARY_DIST, platform\n            )\n    # Try source distro extensions (.zip, .tgz, etc.)\n    #\n    for ext in EXTENSIONS:\n        if basename.endswith(ext):\n            basename = basename[: -len(ext)]\n            return interpret_distro_name(location, basename, metadata)\n    return []  # no extension matched\n\n\ndef distros_for_filename(filename, metadata=None):\n    \"\"\"Yield possible egg or source distribution objects based on a filename\"\"\"\n    return distros_for_location(\n        normalize_path(filename), os.path.basename(filename), metadata\n    )\n\n\ndef interpret_distro_name(\n    location, basename, metadata, py_version=None, precedence=SOURCE_DIST, platform=None\n):\n    \"\"\"Generate alternative interpretations of a source distro name\n\n    Note: if `location` is a filesystem filename, you should call\n    ``pkg_resources.normalize_path()`` on it before passing it to this\n    routine!\n    \"\"\"\n    # Generate alternative interpretations of a source distro name\n    # Because some packages are ambiguous as to name/versions split\n    # e.g. \"adns-python-1.1.0\", \"egenix-mx-commercial\", etc.\n    # So, we generate each possible interpretation (e.g. \"adns, python-1.1.0\"\n    # \"adns-python, 1.1.0\", and \"adns-python-1.1.0, no version\").  In practice,\n    # the spurious interpretations should be ignored, because in the event\n    # there's also an \"adns\" package, the spurious \"python-1.1.0\" version will\n    # compare lower than any numeric version number, and is therefore unlikely\n    # to match a request for it.  It's still a potential problem, though, and\n    # in the long run PyPI and the distutils should go for \"safe\" names and\n    # versions in distribution archive names (sdist and bdist).\n\n    parts = basename.split('-')\n    if not py_version and any(re.match(r'py\\d\\.\\d$', p) for p in parts[2:]):\n        # it is a bdist_dumb, not an sdist -- bail out\n        return\n\n    for p in range(1, len(parts) + 1):\n        yield Distribution(\n            location,\n            metadata,\n            '-'.join(parts[:p]),\n            '-'.join(parts[p:]),\n            py_version=py_version,\n            precedence=precedence,\n            platform=platform,\n        )\n\n\ndef unique_values(func):\n    \"\"\"\n    Wrap a function returning an iterable such that the resulting iterable\n    only ever yields unique items.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        return unique_everseen(func(*args, **kwargs))\n\n    return wrapper\n\n\nREL = re.compile(r\"\"\"<([^>]*\\srel\\s*=\\s*['\"]?([^'\">]+)[^>]*)>\"\"\", re.I)\n\"\"\"\nRegex for an HTML tag with 'rel=\"val\"' attributes.\n\"\"\"\n\n\n@unique_values\ndef find_external_links(url, page):\n    \"\"\"Find rel=\"homepage\" and rel=\"download\" links in `page`, yielding URLs\"\"\"\n\n    for match in REL.finditer(page):\n        tag, rel = match.groups()\n        rels = set(map(str.strip, rel.lower().split(',')))\n        if 'homepage' in rels or 'download' in rels:\n            for match in HREF.finditer(tag):\n                yield urllib.parse.urljoin(url, htmldecode(match.group(1)))\n\n    for tag in (\"<th>Home Page\", \"<th>Download URL\"):\n        pos = page.find(tag)\n        if pos != -1:\n            match = HREF.search(page, pos)\n            if match:\n                yield urllib.parse.urljoin(url, htmldecode(match.group(1)))\n\n\nclass ContentChecker:\n    \"\"\"\n    A null content checker that defines the interface for checking content\n    \"\"\"\n\n    def feed(self, block):\n        \"\"\"\n        Feed a block of data to the hash.\n        \"\"\"\n        return\n\n    def is_valid(self):\n        \"\"\"\n        Check the hash. Return False if validation fails.\n        \"\"\"\n        return True\n\n    def report(self, reporter, template):\n        \"\"\"\n        Call reporter with information about the checker (hash name)\n        substituted into the template.\n        \"\"\"\n        return\n\n\nclass HashChecker(ContentChecker):\n    pattern = re.compile(\n        r'(?P<hash_name>sha1|sha224|sha384|sha256|sha512|md5)='\n        r'(?P<expected>[a-f0-9]+)'\n    )\n\n    def __init__(self, hash_name, expected):\n        self.hash_name = hash_name\n        self.hash = hashlib.new(hash_name)\n        self.expected = expected\n\n    @classmethod\n    def from_url(cls, url):\n        \"Construct a (possibly null) ContentChecker from a URL\"\n        fragment = urllib.parse.urlparse(url)[-1]\n        if not fragment:\n            return ContentChecker()\n        match = cls.pattern.search(fragment)\n        if not match:\n            return ContentChecker()\n        return cls(**match.groupdict())\n\n    def feed(self, block):\n        self.hash.update(block)\n\n    def is_valid(self):\n        return self.hash.hexdigest() == self.expected\n\n    def report(self, reporter, template):\n        msg = template % self.hash_name\n        return reporter(msg)\n\n\nclass PackageIndex(Environment):\n    \"\"\"A distribution index that scans web pages for download URLs\"\"\"\n\n    def __init__(\n        self,\n        index_url=\"https://pypi.org/simple/\",\n        hosts=('*',),\n        ca_bundle=None,\n        verify_ssl=True,\n        *args,\n        **kw\n    ):\n        super().__init__(*args, **kw)\n        self.index_url = index_url + \"/\"[: not index_url.endswith('/')]\n        self.scanned_urls = {}\n        self.fetched_urls = {}\n        self.package_pages = {}\n        self.allows = re.compile('|'.join(map(translate, hosts))).match\n        self.to_scan = []\n        self.opener = urllib.request.urlopen\n\n    def add(self, dist):\n        # ignore invalid versions\n        try:\n            parse_version(dist.version)\n        except Exception:\n            return\n        return super().add(dist)\n\n    # FIXME: 'PackageIndex.process_url' is too complex (14)\n    def process_url(self, url, retrieve=False):  # noqa: C901\n        \"\"\"Evaluate a URL as a possible download, and maybe retrieve it\"\"\"\n        if url in self.scanned_urls and not retrieve:\n            return\n        self.scanned_urls[url] = True\n        if not URL_SCHEME(url):\n            self.process_filename(url)\n            return\n        else:\n            dists = list(distros_for_url(url))\n            if dists:\n                if not self.url_ok(url):\n                    return\n                self.debug(\"Found link: %s\", url)\n\n        if dists or not retrieve or url in self.fetched_urls:\n            list(map(self.add, dists))\n            return  # don't need the actual page\n\n        if not self.url_ok(url):\n            self.fetched_urls[url] = True\n            return\n\n        self.info(\"Reading %s\", url)\n        self.fetched_urls[url] = True  # prevent multiple fetch attempts\n        tmpl = \"Download error on %s: %%s -- Some packages may not be found!\"\n        f = self.open_url(url, tmpl % url)\n        if f is None:\n            return\n        if isinstance(f, urllib.error.HTTPError) and f.code == 401:\n            self.info(\"Authentication error: %s\" % f.msg)\n        self.fetched_urls[f.url] = True\n        if 'html' not in f.headers.get('content-type', '').lower():\n            f.close()  # not html, we can't process it\n            return\n\n        base = f.url  # handle redirects\n        page = f.read()\n        if not isinstance(page, str):\n            # In Python 3 and got bytes but want str.\n            if isinstance(f, urllib.error.HTTPError):\n                # Errors have no charset, assume latin1:\n                charset = 'latin-1'\n            else:\n                charset = f.headers.get_param('charset') or 'latin-1'\n            page = page.decode(charset, \"ignore\")\n        f.close()\n        for match in HREF.finditer(page):\n            link = urllib.parse.urljoin(base, htmldecode(match.group(1)))\n            self.process_url(link)\n        if url.startswith(self.index_url) and getattr(f, 'code', None) != 404:\n            page = self.process_index(url, page)\n\n    def process_filename(self, fn, nested=False):\n        # process filenames or directories\n        if not os.path.exists(fn):\n            self.warn(\"Not found: %s\", fn)\n            return\n\n        if os.path.isdir(fn) and not nested:\n            path = os.path.realpath(fn)\n            for item in os.listdir(path):\n                self.process_filename(os.path.join(path, item), True)\n\n        dists = distros_for_filename(fn)\n        if dists:\n            self.debug(\"Found: %s\", fn)\n            list(map(self.add, dists))\n\n    def url_ok(self, url, fatal=False):\n        s = URL_SCHEME(url)\n        is_file = s and s.group(1).lower() == 'file'\n        if is_file or self.allows(urllib.parse.urlparse(url)[1]):\n            return True\n        msg = (\n            \"\\nNote: Bypassing %s (disallowed host; see \"\n            \"http://bit.ly/2hrImnY for details).\\n\"\n        )\n        if fatal:\n            raise DistutilsError(msg % url)\n        else:\n            self.warn(msg, url)\n\n    def scan_egg_links(self, search_path):\n        dirs = filter(os.path.isdir, search_path)\n        egg_links = (\n            (path, entry)\n            for path in dirs\n            for entry in os.listdir(path)\n            if entry.endswith('.egg-link')\n        )\n        list(itertools.starmap(self.scan_egg_link, egg_links))\n\n    def scan_egg_link(self, path, entry):\n        with open(os.path.join(path, entry)) as raw_lines:\n            # filter non-empty lines\n            lines = list(filter(None, map(str.strip, raw_lines)))\n\n        if len(lines) != 2:\n            # format is not recognized; punt\n            return\n\n        egg_path, setup_path = lines\n\n        for dist in find_distributions(os.path.join(path, egg_path)):\n            dist.location = os.path.join(path, *lines)\n            dist.precedence = SOURCE_DIST\n            self.add(dist)\n\n    def _scan(self, link):\n        # Process a URL to see if it's for a package page\n        NO_MATCH_SENTINEL = None, None\n        if not link.startswith(self.index_url):\n            return NO_MATCH_SENTINEL\n\n        parts = list(map(urllib.parse.unquote, link[len(self.index_url) :].split('/')))\n        if len(parts) != 2 or '#' in parts[1]:\n            return NO_MATCH_SENTINEL\n\n        # it's a package page, sanitize and index it\n        pkg = safe_name(parts[0])\n        ver = safe_version(parts[1])\n        self.package_pages.setdefault(pkg.lower(), {})[link] = True\n        return to_filename(pkg), to_filename(ver)\n\n    def process_index(self, url, page):\n        \"\"\"Process the contents of a PyPI page\"\"\"\n\n        # process an index page into the package-page index\n        for match in HREF.finditer(page):\n            try:\n                self._scan(urllib.parse.urljoin(url, htmldecode(match.group(1))))\n            except ValueError:\n                pass\n\n        pkg, ver = self._scan(url)  # ensure this page is in the page index\n        if not pkg:\n            return \"\"  # no sense double-scanning non-package pages\n\n        # process individual package page\n        for new_url in find_external_links(url, page):\n            # Process the found URL\n            base, frag = egg_info_for_url(new_url)\n            if base.endswith('.py') and not frag:\n                if ver:\n                    new_url += '#egg=%s-%s' % (pkg, ver)\n                else:\n                    self.need_version_info(url)\n            self.scan_url(new_url)\n\n        return PYPI_MD5.sub(\n            lambda m: '<a href=\"%s#md5=%s\">%s</a>' % m.group(1, 3, 2), page\n        )\n\n    def need_version_info(self, url):\n        self.scan_all(\n            \"Page at %s links to .py file(s) without version info; an index \"\n            \"scan is required.\",\n            url,\n        )\n\n    def scan_all(self, msg=None, *args):\n        if self.index_url not in self.fetched_urls:\n            if msg:\n                self.warn(msg, *args)\n            self.info(\"Scanning index of all packages (this may take a while)\")\n        self.scan_url(self.index_url)\n\n    def find_packages(self, requirement):\n        self.scan_url(self.index_url + requirement.unsafe_name + '/')\n\n        if not self.package_pages.get(requirement.key):\n            # Fall back to safe version of the name\n            self.scan_url(self.index_url + requirement.project_name + '/')\n\n        if not self.package_pages.get(requirement.key):\n            # We couldn't find the target package, so search the index page too\n            self.not_found_in_index(requirement)\n\n        for url in list(self.package_pages.get(requirement.key, ())):\n            # scan each page that might be related to the desired package\n            self.scan_url(url)\n\n    def obtain(self, requirement, installer=None):\n        self.prescan()\n        self.find_packages(requirement)\n        for dist in self[requirement.key]:\n            if dist in requirement:\n                return dist\n            self.debug(\"%s does not match %s\", requirement, dist)\n        return super(PackageIndex, self).obtain(requirement, installer)\n\n    def check_hash(self, checker, filename, tfp):\n        \"\"\"\n        checker is a ContentChecker\n        \"\"\"\n        checker.report(self.debug, \"Validating %%s checksum for %s\" % filename)\n        if not checker.is_valid():\n            tfp.close()\n            os.unlink(filename)\n            raise DistutilsError(\n                \"%s validation failed for %s; \"\n                \"possible download problem?\"\n                % (checker.hash.name, os.path.basename(filename))\n            )\n\n    def add_find_links(self, urls):\n        \"\"\"Add `urls` to the list that will be prescanned for searches\"\"\"\n        for url in urls:\n            if (\n                self.to_scan is None  # if we have already \"gone online\"\n                or not URL_SCHEME(url)  # or it's a local file/directory\n                or url.startswith('file:')\n                or list(distros_for_url(url))  # or a direct package link\n            ):\n                # then go ahead and process it now\n                self.scan_url(url)\n            else:\n                # otherwise, defer retrieval till later\n                self.to_scan.append(url)\n\n    def prescan(self):\n        \"\"\"Scan urls scheduled for prescanning (e.g. --find-links)\"\"\"\n        if self.to_scan:\n            list(map(self.scan_url, self.to_scan))\n        self.to_scan = None  # from now on, go ahead and process immediately\n\n    def not_found_in_index(self, requirement):\n        if self[requirement.key]:  # we've seen at least one distro\n            meth, msg = self.info, \"Couldn't retrieve index page for %r\"\n        else:  # no distros seen for this name, might be misspelled\n            meth, msg = (\n                self.warn,\n                \"Couldn't find index page for %r (maybe misspelled?)\",\n            )\n        meth(msg, requirement.unsafe_name)\n        self.scan_all()\n\n    def download(self, spec, tmpdir):\n        \"\"\"Locate and/or download `spec` to `tmpdir`, returning a local path\n\n        `spec` may be a ``Requirement`` object, or a string containing a URL,\n        an existing local filename, or a project/version requirement spec\n        (i.e. the string form of a ``Requirement`` object).  If it is the URL\n        of a .py file with an unambiguous ``#egg=name-version`` tag (i.e., one\n        that escapes ``-`` as ``_`` throughout), a trivial ``setup.py`` is\n        automatically created alongside the downloaded file.\n\n        If `spec` is a ``Requirement`` object or a string containing a\n        project/version requirement spec, this method returns the location of\n        a matching distribution (possibly after downloading it to `tmpdir`).\n        If `spec` is a locally existing file or directory name, it is simply\n        returned unchanged.  If `spec` is a URL, it is downloaded to a subpath\n        of `tmpdir`, and the local filename is returned.  Various errors may be\n        raised if a problem occurs during downloading.\n        \"\"\"\n        if not isinstance(spec, Requirement):\n            scheme = URL_SCHEME(spec)\n            if scheme:\n                # It's a url, download it to tmpdir\n                found = self._download_url(scheme.group(1), spec, tmpdir)\n                base, fragment = egg_info_for_url(spec)\n                if base.endswith('.py'):\n                    found = self.gen_setup(found, fragment, tmpdir)\n                return found\n            elif os.path.exists(spec):\n                # Existing file or directory, just return it\n                return spec\n            else:\n                spec = parse_requirement_arg(spec)\n        return getattr(self.fetch_distribution(spec, tmpdir), 'location', None)\n\n    def fetch_distribution(  # noqa: C901  # is too complex (14)  # FIXME\n        self,\n        requirement,\n        tmpdir,\n        force_scan=False,\n        source=False,\n        develop_ok=False,\n        local_index=None,\n    ):\n        \"\"\"Obtain a distribution suitable for fulfilling `requirement`\n\n        `requirement` must be a ``pkg_resources.Requirement`` instance.\n        If necessary, or if the `force_scan` flag is set, the requirement is\n        searched for in the (online) package index as well as the locally\n        installed packages.  If a distribution matching `requirement` is found,\n        the returned distribution's ``location`` is the value you would have\n        gotten from calling the ``download()`` method with the matching\n        distribution's URL or filename.  If no matching distribution is found,\n        ``None`` is returned.\n\n        If the `source` flag is set, only source distributions and source\n        checkout links will be considered.  Unless the `develop_ok` flag is\n        set, development and system eggs (i.e., those using the ``.egg-info``\n        format) will be ignored.\n        \"\"\"\n        # process a Requirement\n        self.info(\"Searching for %s\", requirement)\n        skipped = {}\n        dist = None\n\n        def find(req, env=None):\n            if env is None:\n                env = self\n            # Find a matching distribution; may be called more than once\n\n            for dist in env[req.key]:\n\n                if dist.precedence == DEVELOP_DIST and not develop_ok:\n                    if dist not in skipped:\n                        self.warn(\n                            \"Skipping development or system egg: %s\",\n                            dist,\n                        )\n                        skipped[dist] = 1\n                    continue\n\n                test = dist in req and (dist.precedence <= SOURCE_DIST or not source)\n                if test:\n                    loc = self.download(dist.location, tmpdir)\n                    dist.download_location = loc\n                    if os.path.exists(dist.download_location):\n                        return dist\n\n        if force_scan:\n            self.prescan()\n            self.find_packages(requirement)\n            dist = find(requirement)\n\n        if not dist and local_index is not None:\n            dist = find(requirement, local_index)\n\n        if dist is None:\n            if self.to_scan is not None:\n                self.prescan()\n            dist = find(requirement)\n\n        if dist is None and not force_scan:\n            self.find_packages(requirement)\n            dist = find(requirement)\n\n        if dist is None:\n            self.warn(\n                \"No local packages or working download links found for %s%s\",\n                (source and \"a source distribution of \" or \"\"),\n                requirement,\n            )\n        else:\n            self.info(\"Best match: %s\", dist)\n            return dist.clone(location=dist.download_location)\n\n    def fetch(self, requirement, tmpdir, force_scan=False, source=False):\n        \"\"\"Obtain a file suitable for fulfilling `requirement`\n\n        DEPRECATED; use the ``fetch_distribution()`` method now instead.  For\n        backward compatibility, this routine is identical but returns the\n        ``location`` of the downloaded distribution instead of a distribution\n        object.\n        \"\"\"\n        dist = self.fetch_distribution(requirement, tmpdir, force_scan, source)\n        if dist is not None:\n            return dist.location\n        return None\n\n    def gen_setup(self, filename, fragment, tmpdir):\n        match = EGG_FRAGMENT.match(fragment)\n        dists = (\n            match\n            and [\n                d\n                for d in interpret_distro_name(filename, match.group(1), None)\n                if d.version\n            ]\n            or []\n        )\n\n        if len(dists) == 1:  # unambiguous ``#egg`` fragment\n            basename = os.path.basename(filename)\n\n            # Make sure the file has been downloaded to the temp dir.\n            if os.path.dirname(filename) != tmpdir:\n                dst = os.path.join(tmpdir, basename)\n                if not (os.path.exists(dst) and os.path.samefile(filename, dst)):\n                    shutil.copy2(filename, dst)\n                    filename = dst\n\n            with open(os.path.join(tmpdir, 'setup.py'), 'w') as file:\n                file.write(\n                    \"from setuptools import setup\\n\"\n                    \"setup(name=%r, version=%r, py_modules=[%r])\\n\"\n                    % (\n                        dists[0].project_name,\n                        dists[0].version,\n                        os.path.splitext(basename)[0],\n                    )\n                )\n            return filename\n\n        elif match:\n            raise DistutilsError(\n                \"Can't unambiguously interpret project/version identifier %r; \"\n                \"any dashes in the name or version should be escaped using \"\n                \"underscores. %r\" % (fragment, dists)\n            )\n        else:\n            raise DistutilsError(\n                \"Can't process plain .py files without an '#egg=name-version'\"\n                \" suffix to enable automatic setup script generation.\"\n            )\n\n    dl_blocksize = 8192\n\n    def _download_to(self, url, filename):\n        self.info(\"Downloading %s\", url)\n        # Download the file\n        fp = None\n        try:\n            checker = HashChecker.from_url(url)\n            fp = self.open_url(url)\n            if isinstance(fp, urllib.error.HTTPError):\n                raise DistutilsError(\n                    \"Can't download %s: %s %s\" % (url, fp.code, fp.msg)\n                )\n            headers = fp.info()\n            blocknum = 0\n            bs = self.dl_blocksize\n            size = -1\n            if \"content-length\" in headers:\n                # Some servers return multiple Content-Length headers :(\n                sizes = headers.get_all('Content-Length')\n                size = max(map(int, sizes))\n                self.reporthook(url, filename, blocknum, bs, size)\n            with open(filename, 'wb') as tfp:\n                while True:\n                    block = fp.read(bs)\n                    if block:\n                        checker.feed(block)\n                        tfp.write(block)\n                        blocknum += 1\n                        self.reporthook(url, filename, blocknum, bs, size)\n                    else:\n                        break\n                self.check_hash(checker, filename, tfp)\n            return headers\n        finally:\n            if fp:\n                fp.close()\n\n    def reporthook(self, url, filename, blocknum, blksize, size):\n        pass  # no-op\n\n    # FIXME:\n    def open_url(self, url, warning=None):  # noqa: C901  # is too complex (12)\n        if url.startswith('file:'):\n            return local_open(url)\n        try:\n            return open_with_auth(url, self.opener)\n        except (ValueError, http.client.InvalidURL) as v:\n            msg = ' '.join([str(arg) for arg in v.args])\n            if warning:\n                self.warn(warning, msg)\n            else:\n                raise DistutilsError('%s %s' % (url, msg)) from v\n        except urllib.error.HTTPError as v:\n            return v\n        except urllib.error.URLError as v:\n            if warning:\n                self.warn(warning, v.reason)\n            else:\n                raise DistutilsError(\n                    \"Download error for %s: %s\" % (url, v.reason)\n                ) from v\n        except http.client.BadStatusLine as v:\n            if warning:\n                self.warn(warning, v.line)\n            else:\n                raise DistutilsError(\n                    '%s returned a bad status line. The server might be '\n                    'down, %s' % (url, v.line)\n                ) from v\n        except (http.client.HTTPException, socket.error) as v:\n            if warning:\n                self.warn(warning, v)\n            else:\n                raise DistutilsError(\"Download error for %s: %s\" % (url, v)) from v\n\n    def _download_url(self, scheme, url, tmpdir):\n        # Determine download filename\n        #\n        name, fragment = egg_info_for_url(url)\n        if name:\n            while '..' in name:\n                name = name.replace('..', '.').replace('\\\\', '_')\n        else:\n            name = \"__downloaded__\"  # default if URL has no path contents\n\n        if name.endswith('.egg.zip'):\n            name = name[:-4]  # strip the extra .zip before download\n\n        filename = os.path.join(tmpdir, name)\n\n        # Download the file\n        #\n        if scheme == 'svn' or scheme.startswith('svn+'):\n            return self._download_svn(url, filename)\n        elif scheme == 'git' or scheme.startswith('git+'):\n            return self._download_git(url, filename)\n        elif scheme.startswith('hg+'):\n            return self._download_hg(url, filename)\n        elif scheme == 'file':\n            return urllib.request.url2pathname(urllib.parse.urlparse(url)[2])\n        else:\n            self.url_ok(url, True)  # raises error if not allowed\n            return self._attempt_download(url, filename)\n\n    def scan_url(self, url):\n        self.process_url(url, True)\n\n    def _attempt_download(self, url, filename):\n        headers = self._download_to(url, filename)\n        if 'html' in headers.get('content-type', '').lower():\n            return self._download_html(url, headers, filename)\n        else:\n            return filename\n\n    def _download_html(self, url, headers, filename):\n        file = open(filename)\n        for line in file:\n            if line.strip():\n                # Check for a subversion index page\n                if re.search(r'<title>([^- ]+ - )?Revision \\d+:', line):\n                    # it's a subversion index page:\n                    file.close()\n                    os.unlink(filename)\n                    return self._download_svn(url, filename)\n                break  # not an index page\n        file.close()\n        os.unlink(filename)\n        raise DistutilsError(\"Unexpected HTML page found at \" + url)\n\n    def _download_svn(self, url, filename):\n        warnings.warn(\"SVN download support is deprecated\", UserWarning)\n        url = url.split('#', 1)[0]  # remove any fragment for svn's sake\n        creds = ''\n        if url.lower().startswith('svn:') and '@' in url:\n            scheme, netloc, path, p, q, f = urllib.parse.urlparse(url)\n            if not netloc and path.startswith('//') and '/' in path[2:]:\n                netloc, path = path[2:].split('/', 1)\n                auth, host = _splituser(netloc)\n                if auth:\n                    if ':' in auth:\n                        user, pw = auth.split(':', 1)\n                        creds = \" --username=%s --password=%s\" % (user, pw)\n                    else:\n                        creds = \" --username=\" + auth\n                    netloc = host\n                    parts = scheme, netloc, url, p, q, f\n                    url = urllib.parse.urlunparse(parts)\n        self.info(\"Doing subversion checkout from %s to %s\", url, filename)\n        os.system(\"svn checkout%s -q %s %s\" % (creds, url, filename))\n        return filename\n\n    @staticmethod\n    def _vcs_split_rev_from_url(url, pop_prefix=False):\n        scheme, netloc, path, query, frag = urllib.parse.urlsplit(url)\n\n        scheme = scheme.split('+', 1)[-1]\n\n        # Some fragment identification fails\n        path = path.split('#', 1)[0]\n\n        rev = None\n        if '@' in path:\n            path, rev = path.rsplit('@', 1)\n\n        # Also, discard fragment\n        url = urllib.parse.urlunsplit((scheme, netloc, path, query, ''))\n\n        return url, rev\n\n    def _download_git(self, url, filename):\n        filename = filename.split('#', 1)[0]\n        url, rev = self._vcs_split_rev_from_url(url, pop_prefix=True)\n\n        self.info(\"Doing git clone from %s to %s\", url, filename)\n        os.system(\"git clone --quiet %s %s\" % (url, filename))\n\n        if rev is not None:\n            self.info(\"Checking out %s\", rev)\n            os.system(\n                \"git -C %s checkout --quiet %s\"\n                % (\n                    filename,\n                    rev,\n                )\n            )\n\n        return filename\n\n    def _download_hg(self, url, filename):\n        filename = filename.split('#', 1)[0]\n        url, rev = self._vcs_split_rev_from_url(url, pop_prefix=True)\n\n        self.info(\"Doing hg clone from %s to %s\", url, filename)\n        os.system(\"hg clone --quiet %s %s\" % (url, filename))\n\n        if rev is not None:\n            self.info(\"Updating to %s\", rev)\n            os.system(\n                \"hg --cwd %s up -C -r %s -q\"\n                % (\n                    filename,\n                    rev,\n                )\n            )\n\n        return filename\n\n    def debug(self, msg, *args):\n        log.debug(msg, *args)\n\n    def info(self, msg, *args):\n        log.info(msg, *args)\n\n    def warn(self, msg, *args):\n        log.warn(msg, *args)\n\n\n# This pattern matches a character entity reference (a decimal numeric\n# references, a hexadecimal numeric reference, or a named reference).\nentity_sub = re.compile(r'&(#(\\d+|x[\\da-fA-F]+)|[\\w.:-]+);?').sub\n\n\ndef decode_entity(match):\n    what = match.group(0)\n    return html.unescape(what)\n\n\ndef htmldecode(text):\n    \"\"\"\n    Decode HTML entities in the given text.\n\n    >>> htmldecode(\n    ...     'https://../package_name-0.1.2.tar.gz'\n    ...     '?tokena=A&amp;tokenb=B\">package_name-0.1.2.tar.gz')\n    'https://../package_name-0.1.2.tar.gz?tokena=A&tokenb=B\">package_name-0.1.2.tar.gz'\n    \"\"\"\n    return entity_sub(decode_entity, text)\n\n\ndef socket_timeout(timeout=15):\n    def _socket_timeout(func):\n        def _socket_timeout(*args, **kwargs):\n            old_timeout = socket.getdefaulttimeout()\n            socket.setdefaulttimeout(timeout)\n            try:\n                return func(*args, **kwargs)\n            finally:\n                socket.setdefaulttimeout(old_timeout)\n\n        return _socket_timeout\n\n    return _socket_timeout\n\n\ndef _encode_auth(auth):\n    \"\"\"\n    Encode auth from a URL suitable for an HTTP header.\n    >>> str(_encode_auth('username%3Apassword'))\n    'dXNlcm5hbWU6cGFzc3dvcmQ='\n\n    Long auth strings should not cause a newline to be inserted.\n    >>> long_auth = 'username:' + 'password'*10\n    >>> chr(10) in str(_encode_auth(long_auth))\n    False\n    \"\"\"\n    auth_s = urllib.parse.unquote(auth)\n    # convert to bytes\n    auth_bytes = auth_s.encode()\n    encoded_bytes = base64.b64encode(auth_bytes)\n    # convert back to a string\n    encoded = encoded_bytes.decode()\n    # strip the trailing carriage return\n    return encoded.replace('\\n', '')\n\n\nclass Credential:\n    \"\"\"\n    A username/password pair. Use like a namedtuple.\n    \"\"\"\n\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def __iter__(self):\n        yield self.username\n        yield self.password\n\n    def __str__(self):\n        return '%(username)s:%(password)s' % vars(self)\n\n\nclass PyPIConfig(configparser.RawConfigParser):\n    def __init__(self):\n        \"\"\"\n        Load from ~/.pypirc\n        \"\"\"\n        defaults = dict.fromkeys(['username', 'password', 'repository'], '')\n        super().__init__(defaults)\n\n        rc = os.path.join(os.path.expanduser('~'), '.pypirc')\n        if os.path.exists(rc):\n            self.read(rc)\n\n    @property\n    def creds_by_repository(self):\n        sections_with_repositories = [\n            section\n            for section in self.sections()\n            if self.get(section, 'repository').strip()\n        ]\n\n        return dict(map(self._get_repo_cred, sections_with_repositories))\n\n    def _get_repo_cred(self, section):\n        repo = self.get(section, 'repository').strip()\n        return repo, Credential(\n            self.get(section, 'username').strip(),\n            self.get(section, 'password').strip(),\n        )\n\n    def find_credential(self, url):\n        \"\"\"\n        If the URL indicated appears to be a repository defined in this\n        config, return the credential for that repository.\n        \"\"\"\n        for repository, cred in self.creds_by_repository.items():\n            if url.startswith(repository):\n                return cred\n\n\ndef open_with_auth(url, opener=urllib.request.urlopen):\n    \"\"\"Open a urllib2 request, handling HTTP authentication\"\"\"\n\n    parsed = urllib.parse.urlparse(url)\n    scheme, netloc, path, params, query, frag = parsed\n\n    # Double scheme does not raise on macOS as revealed by a\n    # failing test. We would expect \"nonnumeric port\". Refs #20.\n    if netloc.endswith(':'):\n        raise http.client.InvalidURL(\"nonnumeric port: ''\")\n\n    if scheme in ('http', 'https'):\n        auth, address = _splituser(netloc)\n    else:\n        auth = None\n\n    if not auth:\n        cred = PyPIConfig().find_credential(url)\n        if cred:\n            auth = str(cred)\n            info = cred.username, url\n            log.info('Authenticating as %s for %s (from .pypirc)', *info)\n\n    if auth:\n        auth = \"Basic \" + _encode_auth(auth)\n        parts = scheme, address, path, params, query, frag\n        new_url = urllib.parse.urlunparse(parts)\n        request = urllib.request.Request(new_url)\n        request.add_header(\"Authorization\", auth)\n    else:\n        request = urllib.request.Request(url)\n\n    request.add_header('User-Agent', user_agent)\n    fp = opener(request)\n\n    if auth:\n        # Put authentication info back into request URL if same host,\n        # so that links found on the page will work\n        s2, h2, path2, param2, query2, frag2 = urllib.parse.urlparse(fp.url)\n        if s2 == scheme and h2 == address:\n            parts = s2, netloc, path2, param2, query2, frag2\n            fp.url = urllib.parse.urlunparse(parts)\n\n    return fp\n\n\n# copy of urllib.parse._splituser from Python 3.8\ndef _splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]')\n    --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    user, delim, host = host.rpartition('@')\n    return (user if delim else None), host\n\n\n# adding a timeout to avoid freezing package_index\nopen_with_auth = socket_timeout(_SOCKET_TIMEOUT)(open_with_auth)\n\n\ndef fix_sf_url(url):\n    return url  # backward compatibility\n\n\ndef local_open(url):\n    \"\"\"Read a local path, with special support for directories\"\"\"\n    scheme, server, path, param, query, frag = urllib.parse.urlparse(url)\n    filename = urllib.request.url2pathname(path)\n    if os.path.isfile(filename):\n        return urllib.request.urlopen(url)\n    elif path.endswith('/') and os.path.isdir(filename):\n        files = []\n        for f in os.listdir(filename):\n            filepath = os.path.join(filename, f)\n            if f == 'index.html':\n                with open(filepath, 'r') as fp:\n                    body = fp.read()\n                break\n            elif os.path.isdir(filepath):\n                f += '/'\n            files.append('<a href=\"{name}\">{name}</a>'.format(name=f))\n        else:\n            tmpl = (\n                \"<html><head><title>{url}</title>\" \"</head><body>{files}</body></html>\"\n            )\n            body = tmpl.format(url=url, files='\\n'.join(files))\n        status, message = 200, \"OK\"\n    else:\n        status, message, body = 404, \"Path not found\", \"Not found\"\n\n    headers = {'content-type': 'text/html'}\n    body_stream = io.StringIO(body)\n    return urllib.error.HTTPError(url, status, message, headers, body_stream)\n", "import sys\nimport os\nimport distutils.errors\nimport platform\nimport urllib.request\nimport urllib.error\nimport http.client\nfrom unittest import mock\n\nimport pytest\n\nimport setuptools.package_index\nfrom .textwrap import DALS\n\n\nclass TestPackageIndex:\n    def test_regex(self):\n        hash_url = 'http://other_url?:action=show_md5&amp;'\n        hash_url += 'digest=0123456789abcdef0123456789abcdef'\n        doc = \"\"\"\n            <a href=\"http://some_url\">Name</a>\n            (<a title=\"MD5 hash\"\n            href=\"{hash_url}\">md5</a>)\n        \"\"\".lstrip().format(\n            **locals()\n        )\n        assert setuptools.package_index.PYPI_MD5.match(doc)\n\n    def test_bad_url_bad_port(self):\n        index = setuptools.package_index.PackageIndex()\n        url = 'http://127.0.0.1:0/nonesuch/test_package_index'\n        try:\n            v = index.open_url(url)\n        except Exception as v:\n            assert url in str(v)\n        else:\n            assert isinstance(v, urllib.error.HTTPError)\n\n    def test_bad_url_typo(self):\n        # issue 16\n        # easy_install inquant.contentmirror.plone breaks because of a typo\n        # in its home URL\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        url = (\n            'url:%20https://svn.plone.org/svn'\n            '/collective/inquant.contentmirror.plone/trunk'\n        )\n        try:\n            v = index.open_url(url)\n        except Exception as v:\n            assert url in str(v)\n        else:\n            assert isinstance(v, urllib.error.HTTPError)\n\n    def test_bad_url_bad_status_line(self):\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        def _urlopen(*args):\n            raise http.client.BadStatusLine('line')\n\n        index.opener = _urlopen\n        url = 'http://example.com'\n        try:\n            index.open_url(url)\n        except Exception as exc:\n            assert 'line' in str(exc)\n        else:\n            raise AssertionError('Should have raise here!')\n\n    def test_bad_url_double_scheme(self):\n        \"\"\"\n        A bad URL with a double scheme should raise a DistutilsError.\n        \"\"\"\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        # issue 20\n        url = 'http://http://svn.pythonpaste.org/Paste/wphp/trunk'\n        try:\n            index.open_url(url)\n        except distutils.errors.DistutilsError as error:\n            msg = str(error)\n            assert (\n                'nonnumeric port' in msg\n                or 'getaddrinfo failed' in msg\n                or 'Name or service not known' in msg\n            )\n            return\n        raise RuntimeError(\"Did not raise\")\n\n    def test_bad_url_screwy_href(self):\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        # issue #160\n        if sys.version_info[0] == 2 and sys.version_info[1] == 7:\n            # this should not fail\n            url = 'http://example.com'\n            page = '<a href=\"http://www.famfamfam.com](' 'http://www.famfamfam.com/\">'\n            index.process_index(url, page)\n\n    def test_url_ok(self):\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n        url = 'file:///tmp/test_package_index'\n        assert index.url_ok(url, True)\n\n    def test_parse_bdist_wininst(self):\n        parse = setuptools.package_index.parse_bdist_wininst\n\n        actual = parse('reportlab-2.5.win32-py2.4.exe')\n        expected = 'reportlab-2.5', '2.4', 'win32'\n        assert actual == expected\n\n        actual = parse('reportlab-2.5.win32.exe')\n        expected = 'reportlab-2.5', None, 'win32'\n        assert actual == expected\n\n        actual = parse('reportlab-2.5.win-amd64-py2.7.exe')\n        expected = 'reportlab-2.5', '2.7', 'win-amd64'\n        assert actual == expected\n\n        actual = parse('reportlab-2.5.win-amd64.exe')\n        expected = 'reportlab-2.5', None, 'win-amd64'\n        assert actual == expected\n\n    def test__vcs_split_rev_from_url(self):\n        \"\"\"\n        Test the basic usage of _vcs_split_rev_from_url\n        \"\"\"\n        vsrfu = setuptools.package_index.PackageIndex._vcs_split_rev_from_url\n        url, rev = vsrfu('https://example.com/bar@2995')\n        assert url == 'https://example.com/bar'\n        assert rev == '2995'\n\n    def test_local_index(self, tmpdir):\n        \"\"\"\n        local_open should be able to read an index from the file system.\n        \"\"\"\n        index_file = tmpdir / 'index.html'\n        with index_file.open('w') as f:\n            f.write('<div>content</div>')\n        url = 'file:' + urllib.request.pathname2url(str(tmpdir)) + '/'\n        res = setuptools.package_index.local_open(url)\n        assert 'content' in res.read()\n\n    def test_egg_fragment(self):\n        \"\"\"\n        EGG fragments must comply to PEP 440\n        \"\"\"\n        epoch = [\n            '',\n            '1!',\n        ]\n        releases = [\n            '0',\n            '0.0',\n            '0.0.0',\n        ]\n        pre = [\n            'a0',\n            'b0',\n            'rc0',\n        ]\n        post = ['.post0']\n        dev = [\n            '.dev0',\n        ]\n        local = [\n            ('', ''),\n            ('+ubuntu.0', '+ubuntu.0'),\n            ('+ubuntu-0', '+ubuntu.0'),\n            ('+ubuntu_0', '+ubuntu.0'),\n        ]\n        versions = [\n            [''.join([e, r, p, loc]) for loc in locs]\n            for e in epoch\n            for r in releases\n            for p in sum([pre, post, dev], [''])\n            for locs in local\n        ]\n        for v, vc in versions:\n            dists = list(\n                setuptools.package_index.distros_for_url(\n                    'http://example.com/example.zip#egg=example-' + v\n                )\n            )\n            assert dists[0].version == ''\n            assert dists[1].version == vc\n\n    def test_download_git_with_rev(self, tmpdir):\n        url = 'git+https://github.example/group/project@master#egg=foo'\n        index = setuptools.package_index.PackageIndex()\n\n        with mock.patch(\"os.system\") as os_system_mock:\n            result = index.download(url, str(tmpdir))\n\n        os_system_mock.assert_called()\n\n        expected_dir = str(tmpdir / 'project@master')\n        expected = (\n            'git clone --quiet ' 'https://github.example/group/project {expected_dir}'\n        ).format(**locals())\n        first_call_args = os_system_mock.call_args_list[0][0]\n        assert first_call_args == (expected,)\n\n        tmpl = 'git -C {expected_dir} checkout --quiet master'\n        expected = tmpl.format(**locals())\n        assert os_system_mock.call_args_list[1][0] == (expected,)\n        assert result == expected_dir\n\n    def test_download_git_no_rev(self, tmpdir):\n        url = 'git+https://github.example/group/project#egg=foo'\n        index = setuptools.package_index.PackageIndex()\n\n        with mock.patch(\"os.system\") as os_system_mock:\n            result = index.download(url, str(tmpdir))\n\n        os_system_mock.assert_called()\n\n        expected_dir = str(tmpdir / 'project')\n        expected = (\n            'git clone --quiet ' 'https://github.example/group/project {expected_dir}'\n        ).format(**locals())\n        os_system_mock.assert_called_once_with(expected)\n\n    def test_download_svn(self, tmpdir):\n        url = 'svn+https://svn.example/project#egg=foo'\n        index = setuptools.package_index.PackageIndex()\n\n        with pytest.warns(UserWarning):\n            with mock.patch(\"os.system\") as os_system_mock:\n                result = index.download(url, str(tmpdir))\n\n        os_system_mock.assert_called()\n\n        expected_dir = str(tmpdir / 'project')\n        expected = (\n            'svn checkout -q ' 'svn+https://svn.example/project {expected_dir}'\n        ).format(**locals())\n        os_system_mock.assert_called_once_with(expected)\n\n\nclass TestContentCheckers:\n    def test_md5(self):\n        checker = setuptools.package_index.HashChecker.from_url(\n            'http://foo/bar#md5=f12895fdffbd45007040d2e44df98478'\n        )\n        checker.feed('You should probably not be using MD5'.encode('ascii'))\n        assert checker.hash.hexdigest() == 'f12895fdffbd45007040d2e44df98478'\n        assert checker.is_valid()\n\n    def test_other_fragment(self):\n        \"Content checks should succeed silently if no hash is present\"\n        checker = setuptools.package_index.HashChecker.from_url(\n            'http://foo/bar#something%20completely%20different'\n        )\n        checker.feed('anything'.encode('ascii'))\n        assert checker.is_valid()\n\n    def test_blank_md5(self):\n        \"Content checks should succeed if a hash is empty\"\n        checker = setuptools.package_index.HashChecker.from_url('http://foo/bar#md5=')\n        checker.feed('anything'.encode('ascii'))\n        assert checker.is_valid()\n\n    def test_get_hash_name_md5(self):\n        checker = setuptools.package_index.HashChecker.from_url(\n            'http://foo/bar#md5=f12895fdffbd45007040d2e44df98478'\n        )\n        assert checker.hash_name == 'md5'\n\n    def test_report(self):\n        checker = setuptools.package_index.HashChecker.from_url(\n            'http://foo/bar#md5=f12895fdffbd45007040d2e44df98478'\n        )\n        rep = checker.report(lambda x: x, 'My message about %s')\n        assert rep == 'My message about md5'\n\n\n@pytest.fixture\ndef temp_home(tmpdir, monkeypatch):\n    key = (\n        'USERPROFILE'\n        if platform.system() == 'Windows' and sys.version_info > (3, 8)\n        else 'HOME'\n    )\n\n    monkeypatch.setitem(os.environ, key, str(tmpdir))\n    return tmpdir\n\n\nclass TestPyPIConfig:\n    def test_percent_in_password(self, temp_home):\n        pypirc = temp_home / '.pypirc'\n        pypirc.write(\n            DALS(\n                \"\"\"\n            [pypi]\n            repository=https://pypi.org\n            username=jaraco\n            password=pity%\n        \"\"\"\n            )\n        )\n        cfg = setuptools.package_index.PyPIConfig()\n        cred = cfg.creds_by_repository['https://pypi.org']\n        assert cred.username == 'jaraco'\n        assert cred.password == 'pity%'\n\n\n@pytest.mark.xfail(reason=\"#3659\")\n@pytest.mark.timeout(1)\ndef test_REL_DoS():\n    \"\"\"\n    REL should not hang on a contrived attack string.\n    \"\"\"\n    setuptools.package_index.REL.search('< rel=' + ' ' * 2**12)\n"], "fixing_code": ["\"\"\"PyPI and direct package downloading.\"\"\"\n\nimport sys\nimport os\nimport re\nimport io\nimport shutil\nimport socket\nimport base64\nimport hashlib\nimport itertools\nimport warnings\nimport configparser\nimport html\nimport http.client\nimport urllib.parse\nimport urllib.request\nimport urllib.error\nfrom functools import wraps\n\nimport setuptools\nfrom pkg_resources import (\n    CHECKOUT_DIST,\n    Distribution,\n    BINARY_DIST,\n    normalize_path,\n    SOURCE_DIST,\n    Environment,\n    find_distributions,\n    safe_name,\n    safe_version,\n    to_filename,\n    Requirement,\n    DEVELOP_DIST,\n    EGG_DIST,\n    parse_version,\n)\nfrom distutils import log\nfrom distutils.errors import DistutilsError\nfrom fnmatch import translate\nfrom setuptools.wheel import Wheel\nfrom setuptools.extern.more_itertools import unique_everseen\n\n\nEGG_FRAGMENT = re.compile(r'^egg=([-A-Za-z0-9_.+!]+)$')\nHREF = re.compile(r\"\"\"href\\s*=\\s*['\"]?([^'\"> ]+)\"\"\", re.I)\nPYPI_MD5 = re.compile(\n    r'<a href=\"([^\"#]+)\">([^<]+)</a>\\n\\s+\\(<a (?:title=\"MD5 hash\"\\n\\s+)'\n    r'href=\"[^?]+\\?:action=show_md5&amp;digest=([0-9a-f]{32})\">md5</a>\\)'\n)\nURL_SCHEME = re.compile('([-+.a-z0-9]{2,}):', re.I).match\nEXTENSIONS = \".tar.gz .tar.bz2 .tar .zip .tgz\".split()\n\n__all__ = [\n    'PackageIndex',\n    'distros_for_url',\n    'parse_bdist_wininst',\n    'interpret_distro_name',\n]\n\n_SOCKET_TIMEOUT = 15\n\n_tmpl = \"setuptools/{setuptools.__version__} Python-urllib/{py_major}\"\nuser_agent = _tmpl.format(\n    py_major='{}.{}'.format(*sys.version_info), setuptools=setuptools\n)\n\n\ndef parse_requirement_arg(spec):\n    try:\n        return Requirement.parse(spec)\n    except ValueError as e:\n        raise DistutilsError(\n            \"Not a URL, existing file, or requirement spec: %r\" % (spec,)\n        ) from e\n\n\ndef parse_bdist_wininst(name):\n    \"\"\"Return (base,pyversion) or (None,None) for possible .exe name\"\"\"\n\n    lower = name.lower()\n    base, py_ver, plat = None, None, None\n\n    if lower.endswith('.exe'):\n        if lower.endswith('.win32.exe'):\n            base = name[:-10]\n            plat = 'win32'\n        elif lower.startswith('.win32-py', -16):\n            py_ver = name[-7:-4]\n            base = name[:-16]\n            plat = 'win32'\n        elif lower.endswith('.win-amd64.exe'):\n            base = name[:-14]\n            plat = 'win-amd64'\n        elif lower.startswith('.win-amd64-py', -20):\n            py_ver = name[-7:-4]\n            base = name[:-20]\n            plat = 'win-amd64'\n    return base, py_ver, plat\n\n\ndef egg_info_for_url(url):\n    parts = urllib.parse.urlparse(url)\n    scheme, server, path, parameters, query, fragment = parts\n    base = urllib.parse.unquote(path.split('/')[-1])\n    if server == 'sourceforge.net' and base == 'download':  # XXX Yuck\n        base = urllib.parse.unquote(path.split('/')[-2])\n    if '#' in base:\n        base, fragment = base.split('#', 1)\n    return base, fragment\n\n\ndef distros_for_url(url, metadata=None):\n    \"\"\"Yield egg or source distribution objects that might be found at a URL\"\"\"\n    base, fragment = egg_info_for_url(url)\n    for dist in distros_for_location(url, base, metadata):\n        yield dist\n    if fragment:\n        match = EGG_FRAGMENT.match(fragment)\n        if match:\n            for dist in interpret_distro_name(\n                url, match.group(1), metadata, precedence=CHECKOUT_DIST\n            ):\n                yield dist\n\n\ndef distros_for_location(location, basename, metadata=None):\n    \"\"\"Yield egg or source distribution objects based on basename\"\"\"\n    if basename.endswith('.egg.zip'):\n        basename = basename[:-4]  # strip the .zip\n    if basename.endswith('.egg') and '-' in basename:\n        # only one, unambiguous interpretation\n        return [Distribution.from_location(location, basename, metadata)]\n    if basename.endswith('.whl') and '-' in basename:\n        wheel = Wheel(basename)\n        if not wheel.is_compatible():\n            return []\n        return [\n            Distribution(\n                location=location,\n                project_name=wheel.project_name,\n                version=wheel.version,\n                # Increase priority over eggs.\n                precedence=EGG_DIST + 1,\n            )\n        ]\n    if basename.endswith('.exe'):\n        win_base, py_ver, platform = parse_bdist_wininst(basename)\n        if win_base is not None:\n            return interpret_distro_name(\n                location, win_base, metadata, py_ver, BINARY_DIST, platform\n            )\n    # Try source distro extensions (.zip, .tgz, etc.)\n    #\n    for ext in EXTENSIONS:\n        if basename.endswith(ext):\n            basename = basename[: -len(ext)]\n            return interpret_distro_name(location, basename, metadata)\n    return []  # no extension matched\n\n\ndef distros_for_filename(filename, metadata=None):\n    \"\"\"Yield possible egg or source distribution objects based on a filename\"\"\"\n    return distros_for_location(\n        normalize_path(filename), os.path.basename(filename), metadata\n    )\n\n\ndef interpret_distro_name(\n    location, basename, metadata, py_version=None, precedence=SOURCE_DIST, platform=None\n):\n    \"\"\"Generate alternative interpretations of a source distro name\n\n    Note: if `location` is a filesystem filename, you should call\n    ``pkg_resources.normalize_path()`` on it before passing it to this\n    routine!\n    \"\"\"\n    # Generate alternative interpretations of a source distro name\n    # Because some packages are ambiguous as to name/versions split\n    # e.g. \"adns-python-1.1.0\", \"egenix-mx-commercial\", etc.\n    # So, we generate each possible interpretation (e.g. \"adns, python-1.1.0\"\n    # \"adns-python, 1.1.0\", and \"adns-python-1.1.0, no version\").  In practice,\n    # the spurious interpretations should be ignored, because in the event\n    # there's also an \"adns\" package, the spurious \"python-1.1.0\" version will\n    # compare lower than any numeric version number, and is therefore unlikely\n    # to match a request for it.  It's still a potential problem, though, and\n    # in the long run PyPI and the distutils should go for \"safe\" names and\n    # versions in distribution archive names (sdist and bdist).\n\n    parts = basename.split('-')\n    if not py_version and any(re.match(r'py\\d\\.\\d$', p) for p in parts[2:]):\n        # it is a bdist_dumb, not an sdist -- bail out\n        return\n\n    for p in range(1, len(parts) + 1):\n        yield Distribution(\n            location,\n            metadata,\n            '-'.join(parts[:p]),\n            '-'.join(parts[p:]),\n            py_version=py_version,\n            precedence=precedence,\n            platform=platform,\n        )\n\n\ndef unique_values(func):\n    \"\"\"\n    Wrap a function returning an iterable such that the resulting iterable\n    only ever yields unique items.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        return unique_everseen(func(*args, **kwargs))\n\n    return wrapper\n\n\nREL = re.compile(r\"\"\"<([^>]*\\srel\\s{0,10}=\\s{0,10}['\"]?([^'\" >]+)[^>]*)>\"\"\", re.I)\n\"\"\"\nRegex for an HTML tag with 'rel=\"val\"' attributes.\n\"\"\"\n\n\n@unique_values\ndef find_external_links(url, page):\n    \"\"\"Find rel=\"homepage\" and rel=\"download\" links in `page`, yielding URLs\"\"\"\n\n    for match in REL.finditer(page):\n        tag, rel = match.groups()\n        rels = set(map(str.strip, rel.lower().split(',')))\n        if 'homepage' in rels or 'download' in rels:\n            for match in HREF.finditer(tag):\n                yield urllib.parse.urljoin(url, htmldecode(match.group(1)))\n\n    for tag in (\"<th>Home Page\", \"<th>Download URL\"):\n        pos = page.find(tag)\n        if pos != -1:\n            match = HREF.search(page, pos)\n            if match:\n                yield urllib.parse.urljoin(url, htmldecode(match.group(1)))\n\n\nclass ContentChecker:\n    \"\"\"\n    A null content checker that defines the interface for checking content\n    \"\"\"\n\n    def feed(self, block):\n        \"\"\"\n        Feed a block of data to the hash.\n        \"\"\"\n        return\n\n    def is_valid(self):\n        \"\"\"\n        Check the hash. Return False if validation fails.\n        \"\"\"\n        return True\n\n    def report(self, reporter, template):\n        \"\"\"\n        Call reporter with information about the checker (hash name)\n        substituted into the template.\n        \"\"\"\n        return\n\n\nclass HashChecker(ContentChecker):\n    pattern = re.compile(\n        r'(?P<hash_name>sha1|sha224|sha384|sha256|sha512|md5)='\n        r'(?P<expected>[a-f0-9]+)'\n    )\n\n    def __init__(self, hash_name, expected):\n        self.hash_name = hash_name\n        self.hash = hashlib.new(hash_name)\n        self.expected = expected\n\n    @classmethod\n    def from_url(cls, url):\n        \"Construct a (possibly null) ContentChecker from a URL\"\n        fragment = urllib.parse.urlparse(url)[-1]\n        if not fragment:\n            return ContentChecker()\n        match = cls.pattern.search(fragment)\n        if not match:\n            return ContentChecker()\n        return cls(**match.groupdict())\n\n    def feed(self, block):\n        self.hash.update(block)\n\n    def is_valid(self):\n        return self.hash.hexdigest() == self.expected\n\n    def report(self, reporter, template):\n        msg = template % self.hash_name\n        return reporter(msg)\n\n\nclass PackageIndex(Environment):\n    \"\"\"A distribution index that scans web pages for download URLs\"\"\"\n\n    def __init__(\n        self,\n        index_url=\"https://pypi.org/simple/\",\n        hosts=('*',),\n        ca_bundle=None,\n        verify_ssl=True,\n        *args,\n        **kw\n    ):\n        super().__init__(*args, **kw)\n        self.index_url = index_url + \"/\"[: not index_url.endswith('/')]\n        self.scanned_urls = {}\n        self.fetched_urls = {}\n        self.package_pages = {}\n        self.allows = re.compile('|'.join(map(translate, hosts))).match\n        self.to_scan = []\n        self.opener = urllib.request.urlopen\n\n    def add(self, dist):\n        # ignore invalid versions\n        try:\n            parse_version(dist.version)\n        except Exception:\n            return\n        return super().add(dist)\n\n    # FIXME: 'PackageIndex.process_url' is too complex (14)\n    def process_url(self, url, retrieve=False):  # noqa: C901\n        \"\"\"Evaluate a URL as a possible download, and maybe retrieve it\"\"\"\n        if url in self.scanned_urls and not retrieve:\n            return\n        self.scanned_urls[url] = True\n        if not URL_SCHEME(url):\n            self.process_filename(url)\n            return\n        else:\n            dists = list(distros_for_url(url))\n            if dists:\n                if not self.url_ok(url):\n                    return\n                self.debug(\"Found link: %s\", url)\n\n        if dists or not retrieve or url in self.fetched_urls:\n            list(map(self.add, dists))\n            return  # don't need the actual page\n\n        if not self.url_ok(url):\n            self.fetched_urls[url] = True\n            return\n\n        self.info(\"Reading %s\", url)\n        self.fetched_urls[url] = True  # prevent multiple fetch attempts\n        tmpl = \"Download error on %s: %%s -- Some packages may not be found!\"\n        f = self.open_url(url, tmpl % url)\n        if f is None:\n            return\n        if isinstance(f, urllib.error.HTTPError) and f.code == 401:\n            self.info(\"Authentication error: %s\" % f.msg)\n        self.fetched_urls[f.url] = True\n        if 'html' not in f.headers.get('content-type', '').lower():\n            f.close()  # not html, we can't process it\n            return\n\n        base = f.url  # handle redirects\n        page = f.read()\n        if not isinstance(page, str):\n            # In Python 3 and got bytes but want str.\n            if isinstance(f, urllib.error.HTTPError):\n                # Errors have no charset, assume latin1:\n                charset = 'latin-1'\n            else:\n                charset = f.headers.get_param('charset') or 'latin-1'\n            page = page.decode(charset, \"ignore\")\n        f.close()\n        for match in HREF.finditer(page):\n            link = urllib.parse.urljoin(base, htmldecode(match.group(1)))\n            self.process_url(link)\n        if url.startswith(self.index_url) and getattr(f, 'code', None) != 404:\n            page = self.process_index(url, page)\n\n    def process_filename(self, fn, nested=False):\n        # process filenames or directories\n        if not os.path.exists(fn):\n            self.warn(\"Not found: %s\", fn)\n            return\n\n        if os.path.isdir(fn) and not nested:\n            path = os.path.realpath(fn)\n            for item in os.listdir(path):\n                self.process_filename(os.path.join(path, item), True)\n\n        dists = distros_for_filename(fn)\n        if dists:\n            self.debug(\"Found: %s\", fn)\n            list(map(self.add, dists))\n\n    def url_ok(self, url, fatal=False):\n        s = URL_SCHEME(url)\n        is_file = s and s.group(1).lower() == 'file'\n        if is_file or self.allows(urllib.parse.urlparse(url)[1]):\n            return True\n        msg = (\n            \"\\nNote: Bypassing %s (disallowed host; see \"\n            \"http://bit.ly/2hrImnY for details).\\n\"\n        )\n        if fatal:\n            raise DistutilsError(msg % url)\n        else:\n            self.warn(msg, url)\n\n    def scan_egg_links(self, search_path):\n        dirs = filter(os.path.isdir, search_path)\n        egg_links = (\n            (path, entry)\n            for path in dirs\n            for entry in os.listdir(path)\n            if entry.endswith('.egg-link')\n        )\n        list(itertools.starmap(self.scan_egg_link, egg_links))\n\n    def scan_egg_link(self, path, entry):\n        with open(os.path.join(path, entry)) as raw_lines:\n            # filter non-empty lines\n            lines = list(filter(None, map(str.strip, raw_lines)))\n\n        if len(lines) != 2:\n            # format is not recognized; punt\n            return\n\n        egg_path, setup_path = lines\n\n        for dist in find_distributions(os.path.join(path, egg_path)):\n            dist.location = os.path.join(path, *lines)\n            dist.precedence = SOURCE_DIST\n            self.add(dist)\n\n    def _scan(self, link):\n        # Process a URL to see if it's for a package page\n        NO_MATCH_SENTINEL = None, None\n        if not link.startswith(self.index_url):\n            return NO_MATCH_SENTINEL\n\n        parts = list(map(urllib.parse.unquote, link[len(self.index_url) :].split('/')))\n        if len(parts) != 2 or '#' in parts[1]:\n            return NO_MATCH_SENTINEL\n\n        # it's a package page, sanitize and index it\n        pkg = safe_name(parts[0])\n        ver = safe_version(parts[1])\n        self.package_pages.setdefault(pkg.lower(), {})[link] = True\n        return to_filename(pkg), to_filename(ver)\n\n    def process_index(self, url, page):\n        \"\"\"Process the contents of a PyPI page\"\"\"\n\n        # process an index page into the package-page index\n        for match in HREF.finditer(page):\n            try:\n                self._scan(urllib.parse.urljoin(url, htmldecode(match.group(1))))\n            except ValueError:\n                pass\n\n        pkg, ver = self._scan(url)  # ensure this page is in the page index\n        if not pkg:\n            return \"\"  # no sense double-scanning non-package pages\n\n        # process individual package page\n        for new_url in find_external_links(url, page):\n            # Process the found URL\n            base, frag = egg_info_for_url(new_url)\n            if base.endswith('.py') and not frag:\n                if ver:\n                    new_url += '#egg=%s-%s' % (pkg, ver)\n                else:\n                    self.need_version_info(url)\n            self.scan_url(new_url)\n\n        return PYPI_MD5.sub(\n            lambda m: '<a href=\"%s#md5=%s\">%s</a>' % m.group(1, 3, 2), page\n        )\n\n    def need_version_info(self, url):\n        self.scan_all(\n            \"Page at %s links to .py file(s) without version info; an index \"\n            \"scan is required.\",\n            url,\n        )\n\n    def scan_all(self, msg=None, *args):\n        if self.index_url not in self.fetched_urls:\n            if msg:\n                self.warn(msg, *args)\n            self.info(\"Scanning index of all packages (this may take a while)\")\n        self.scan_url(self.index_url)\n\n    def find_packages(self, requirement):\n        self.scan_url(self.index_url + requirement.unsafe_name + '/')\n\n        if not self.package_pages.get(requirement.key):\n            # Fall back to safe version of the name\n            self.scan_url(self.index_url + requirement.project_name + '/')\n\n        if not self.package_pages.get(requirement.key):\n            # We couldn't find the target package, so search the index page too\n            self.not_found_in_index(requirement)\n\n        for url in list(self.package_pages.get(requirement.key, ())):\n            # scan each page that might be related to the desired package\n            self.scan_url(url)\n\n    def obtain(self, requirement, installer=None):\n        self.prescan()\n        self.find_packages(requirement)\n        for dist in self[requirement.key]:\n            if dist in requirement:\n                return dist\n            self.debug(\"%s does not match %s\", requirement, dist)\n        return super(PackageIndex, self).obtain(requirement, installer)\n\n    def check_hash(self, checker, filename, tfp):\n        \"\"\"\n        checker is a ContentChecker\n        \"\"\"\n        checker.report(self.debug, \"Validating %%s checksum for %s\" % filename)\n        if not checker.is_valid():\n            tfp.close()\n            os.unlink(filename)\n            raise DistutilsError(\n                \"%s validation failed for %s; \"\n                \"possible download problem?\"\n                % (checker.hash.name, os.path.basename(filename))\n            )\n\n    def add_find_links(self, urls):\n        \"\"\"Add `urls` to the list that will be prescanned for searches\"\"\"\n        for url in urls:\n            if (\n                self.to_scan is None  # if we have already \"gone online\"\n                or not URL_SCHEME(url)  # or it's a local file/directory\n                or url.startswith('file:')\n                or list(distros_for_url(url))  # or a direct package link\n            ):\n                # then go ahead and process it now\n                self.scan_url(url)\n            else:\n                # otherwise, defer retrieval till later\n                self.to_scan.append(url)\n\n    def prescan(self):\n        \"\"\"Scan urls scheduled for prescanning (e.g. --find-links)\"\"\"\n        if self.to_scan:\n            list(map(self.scan_url, self.to_scan))\n        self.to_scan = None  # from now on, go ahead and process immediately\n\n    def not_found_in_index(self, requirement):\n        if self[requirement.key]:  # we've seen at least one distro\n            meth, msg = self.info, \"Couldn't retrieve index page for %r\"\n        else:  # no distros seen for this name, might be misspelled\n            meth, msg = (\n                self.warn,\n                \"Couldn't find index page for %r (maybe misspelled?)\",\n            )\n        meth(msg, requirement.unsafe_name)\n        self.scan_all()\n\n    def download(self, spec, tmpdir):\n        \"\"\"Locate and/or download `spec` to `tmpdir`, returning a local path\n\n        `spec` may be a ``Requirement`` object, or a string containing a URL,\n        an existing local filename, or a project/version requirement spec\n        (i.e. the string form of a ``Requirement`` object).  If it is the URL\n        of a .py file with an unambiguous ``#egg=name-version`` tag (i.e., one\n        that escapes ``-`` as ``_`` throughout), a trivial ``setup.py`` is\n        automatically created alongside the downloaded file.\n\n        If `spec` is a ``Requirement`` object or a string containing a\n        project/version requirement spec, this method returns the location of\n        a matching distribution (possibly after downloading it to `tmpdir`).\n        If `spec` is a locally existing file or directory name, it is simply\n        returned unchanged.  If `spec` is a URL, it is downloaded to a subpath\n        of `tmpdir`, and the local filename is returned.  Various errors may be\n        raised if a problem occurs during downloading.\n        \"\"\"\n        if not isinstance(spec, Requirement):\n            scheme = URL_SCHEME(spec)\n            if scheme:\n                # It's a url, download it to tmpdir\n                found = self._download_url(scheme.group(1), spec, tmpdir)\n                base, fragment = egg_info_for_url(spec)\n                if base.endswith('.py'):\n                    found = self.gen_setup(found, fragment, tmpdir)\n                return found\n            elif os.path.exists(spec):\n                # Existing file or directory, just return it\n                return spec\n            else:\n                spec = parse_requirement_arg(spec)\n        return getattr(self.fetch_distribution(spec, tmpdir), 'location', None)\n\n    def fetch_distribution(  # noqa: C901  # is too complex (14)  # FIXME\n        self,\n        requirement,\n        tmpdir,\n        force_scan=False,\n        source=False,\n        develop_ok=False,\n        local_index=None,\n    ):\n        \"\"\"Obtain a distribution suitable for fulfilling `requirement`\n\n        `requirement` must be a ``pkg_resources.Requirement`` instance.\n        If necessary, or if the `force_scan` flag is set, the requirement is\n        searched for in the (online) package index as well as the locally\n        installed packages.  If a distribution matching `requirement` is found,\n        the returned distribution's ``location`` is the value you would have\n        gotten from calling the ``download()`` method with the matching\n        distribution's URL or filename.  If no matching distribution is found,\n        ``None`` is returned.\n\n        If the `source` flag is set, only source distributions and source\n        checkout links will be considered.  Unless the `develop_ok` flag is\n        set, development and system eggs (i.e., those using the ``.egg-info``\n        format) will be ignored.\n        \"\"\"\n        # process a Requirement\n        self.info(\"Searching for %s\", requirement)\n        skipped = {}\n        dist = None\n\n        def find(req, env=None):\n            if env is None:\n                env = self\n            # Find a matching distribution; may be called more than once\n\n            for dist in env[req.key]:\n\n                if dist.precedence == DEVELOP_DIST and not develop_ok:\n                    if dist not in skipped:\n                        self.warn(\n                            \"Skipping development or system egg: %s\",\n                            dist,\n                        )\n                        skipped[dist] = 1\n                    continue\n\n                test = dist in req and (dist.precedence <= SOURCE_DIST or not source)\n                if test:\n                    loc = self.download(dist.location, tmpdir)\n                    dist.download_location = loc\n                    if os.path.exists(dist.download_location):\n                        return dist\n\n        if force_scan:\n            self.prescan()\n            self.find_packages(requirement)\n            dist = find(requirement)\n\n        if not dist and local_index is not None:\n            dist = find(requirement, local_index)\n\n        if dist is None:\n            if self.to_scan is not None:\n                self.prescan()\n            dist = find(requirement)\n\n        if dist is None and not force_scan:\n            self.find_packages(requirement)\n            dist = find(requirement)\n\n        if dist is None:\n            self.warn(\n                \"No local packages or working download links found for %s%s\",\n                (source and \"a source distribution of \" or \"\"),\n                requirement,\n            )\n        else:\n            self.info(\"Best match: %s\", dist)\n            return dist.clone(location=dist.download_location)\n\n    def fetch(self, requirement, tmpdir, force_scan=False, source=False):\n        \"\"\"Obtain a file suitable for fulfilling `requirement`\n\n        DEPRECATED; use the ``fetch_distribution()`` method now instead.  For\n        backward compatibility, this routine is identical but returns the\n        ``location`` of the downloaded distribution instead of a distribution\n        object.\n        \"\"\"\n        dist = self.fetch_distribution(requirement, tmpdir, force_scan, source)\n        if dist is not None:\n            return dist.location\n        return None\n\n    def gen_setup(self, filename, fragment, tmpdir):\n        match = EGG_FRAGMENT.match(fragment)\n        dists = (\n            match\n            and [\n                d\n                for d in interpret_distro_name(filename, match.group(1), None)\n                if d.version\n            ]\n            or []\n        )\n\n        if len(dists) == 1:  # unambiguous ``#egg`` fragment\n            basename = os.path.basename(filename)\n\n            # Make sure the file has been downloaded to the temp dir.\n            if os.path.dirname(filename) != tmpdir:\n                dst = os.path.join(tmpdir, basename)\n                if not (os.path.exists(dst) and os.path.samefile(filename, dst)):\n                    shutil.copy2(filename, dst)\n                    filename = dst\n\n            with open(os.path.join(tmpdir, 'setup.py'), 'w') as file:\n                file.write(\n                    \"from setuptools import setup\\n\"\n                    \"setup(name=%r, version=%r, py_modules=[%r])\\n\"\n                    % (\n                        dists[0].project_name,\n                        dists[0].version,\n                        os.path.splitext(basename)[0],\n                    )\n                )\n            return filename\n\n        elif match:\n            raise DistutilsError(\n                \"Can't unambiguously interpret project/version identifier %r; \"\n                \"any dashes in the name or version should be escaped using \"\n                \"underscores. %r\" % (fragment, dists)\n            )\n        else:\n            raise DistutilsError(\n                \"Can't process plain .py files without an '#egg=name-version'\"\n                \" suffix to enable automatic setup script generation.\"\n            )\n\n    dl_blocksize = 8192\n\n    def _download_to(self, url, filename):\n        self.info(\"Downloading %s\", url)\n        # Download the file\n        fp = None\n        try:\n            checker = HashChecker.from_url(url)\n            fp = self.open_url(url)\n            if isinstance(fp, urllib.error.HTTPError):\n                raise DistutilsError(\n                    \"Can't download %s: %s %s\" % (url, fp.code, fp.msg)\n                )\n            headers = fp.info()\n            blocknum = 0\n            bs = self.dl_blocksize\n            size = -1\n            if \"content-length\" in headers:\n                # Some servers return multiple Content-Length headers :(\n                sizes = headers.get_all('Content-Length')\n                size = max(map(int, sizes))\n                self.reporthook(url, filename, blocknum, bs, size)\n            with open(filename, 'wb') as tfp:\n                while True:\n                    block = fp.read(bs)\n                    if block:\n                        checker.feed(block)\n                        tfp.write(block)\n                        blocknum += 1\n                        self.reporthook(url, filename, blocknum, bs, size)\n                    else:\n                        break\n                self.check_hash(checker, filename, tfp)\n            return headers\n        finally:\n            if fp:\n                fp.close()\n\n    def reporthook(self, url, filename, blocknum, blksize, size):\n        pass  # no-op\n\n    # FIXME:\n    def open_url(self, url, warning=None):  # noqa: C901  # is too complex (12)\n        if url.startswith('file:'):\n            return local_open(url)\n        try:\n            return open_with_auth(url, self.opener)\n        except (ValueError, http.client.InvalidURL) as v:\n            msg = ' '.join([str(arg) for arg in v.args])\n            if warning:\n                self.warn(warning, msg)\n            else:\n                raise DistutilsError('%s %s' % (url, msg)) from v\n        except urllib.error.HTTPError as v:\n            return v\n        except urllib.error.URLError as v:\n            if warning:\n                self.warn(warning, v.reason)\n            else:\n                raise DistutilsError(\n                    \"Download error for %s: %s\" % (url, v.reason)\n                ) from v\n        except http.client.BadStatusLine as v:\n            if warning:\n                self.warn(warning, v.line)\n            else:\n                raise DistutilsError(\n                    '%s returned a bad status line. The server might be '\n                    'down, %s' % (url, v.line)\n                ) from v\n        except (http.client.HTTPException, socket.error) as v:\n            if warning:\n                self.warn(warning, v)\n            else:\n                raise DistutilsError(\"Download error for %s: %s\" % (url, v)) from v\n\n    def _download_url(self, scheme, url, tmpdir):\n        # Determine download filename\n        #\n        name, fragment = egg_info_for_url(url)\n        if name:\n            while '..' in name:\n                name = name.replace('..', '.').replace('\\\\', '_')\n        else:\n            name = \"__downloaded__\"  # default if URL has no path contents\n\n        if name.endswith('.egg.zip'):\n            name = name[:-4]  # strip the extra .zip before download\n\n        filename = os.path.join(tmpdir, name)\n\n        # Download the file\n        #\n        if scheme == 'svn' or scheme.startswith('svn+'):\n            return self._download_svn(url, filename)\n        elif scheme == 'git' or scheme.startswith('git+'):\n            return self._download_git(url, filename)\n        elif scheme.startswith('hg+'):\n            return self._download_hg(url, filename)\n        elif scheme == 'file':\n            return urllib.request.url2pathname(urllib.parse.urlparse(url)[2])\n        else:\n            self.url_ok(url, True)  # raises error if not allowed\n            return self._attempt_download(url, filename)\n\n    def scan_url(self, url):\n        self.process_url(url, True)\n\n    def _attempt_download(self, url, filename):\n        headers = self._download_to(url, filename)\n        if 'html' in headers.get('content-type', '').lower():\n            return self._download_html(url, headers, filename)\n        else:\n            return filename\n\n    def _download_html(self, url, headers, filename):\n        file = open(filename)\n        for line in file:\n            if line.strip():\n                # Check for a subversion index page\n                if re.search(r'<title>([^- ]+ - )?Revision \\d+:', line):\n                    # it's a subversion index page:\n                    file.close()\n                    os.unlink(filename)\n                    return self._download_svn(url, filename)\n                break  # not an index page\n        file.close()\n        os.unlink(filename)\n        raise DistutilsError(\"Unexpected HTML page found at \" + url)\n\n    def _download_svn(self, url, filename):\n        warnings.warn(\"SVN download support is deprecated\", UserWarning)\n        url = url.split('#', 1)[0]  # remove any fragment for svn's sake\n        creds = ''\n        if url.lower().startswith('svn:') and '@' in url:\n            scheme, netloc, path, p, q, f = urllib.parse.urlparse(url)\n            if not netloc and path.startswith('//') and '/' in path[2:]:\n                netloc, path = path[2:].split('/', 1)\n                auth, host = _splituser(netloc)\n                if auth:\n                    if ':' in auth:\n                        user, pw = auth.split(':', 1)\n                        creds = \" --username=%s --password=%s\" % (user, pw)\n                    else:\n                        creds = \" --username=\" + auth\n                    netloc = host\n                    parts = scheme, netloc, url, p, q, f\n                    url = urllib.parse.urlunparse(parts)\n        self.info(\"Doing subversion checkout from %s to %s\", url, filename)\n        os.system(\"svn checkout%s -q %s %s\" % (creds, url, filename))\n        return filename\n\n    @staticmethod\n    def _vcs_split_rev_from_url(url, pop_prefix=False):\n        scheme, netloc, path, query, frag = urllib.parse.urlsplit(url)\n\n        scheme = scheme.split('+', 1)[-1]\n\n        # Some fragment identification fails\n        path = path.split('#', 1)[0]\n\n        rev = None\n        if '@' in path:\n            path, rev = path.rsplit('@', 1)\n\n        # Also, discard fragment\n        url = urllib.parse.urlunsplit((scheme, netloc, path, query, ''))\n\n        return url, rev\n\n    def _download_git(self, url, filename):\n        filename = filename.split('#', 1)[0]\n        url, rev = self._vcs_split_rev_from_url(url, pop_prefix=True)\n\n        self.info(\"Doing git clone from %s to %s\", url, filename)\n        os.system(\"git clone --quiet %s %s\" % (url, filename))\n\n        if rev is not None:\n            self.info(\"Checking out %s\", rev)\n            os.system(\n                \"git -C %s checkout --quiet %s\"\n                % (\n                    filename,\n                    rev,\n                )\n            )\n\n        return filename\n\n    def _download_hg(self, url, filename):\n        filename = filename.split('#', 1)[0]\n        url, rev = self._vcs_split_rev_from_url(url, pop_prefix=True)\n\n        self.info(\"Doing hg clone from %s to %s\", url, filename)\n        os.system(\"hg clone --quiet %s %s\" % (url, filename))\n\n        if rev is not None:\n            self.info(\"Updating to %s\", rev)\n            os.system(\n                \"hg --cwd %s up -C -r %s -q\"\n                % (\n                    filename,\n                    rev,\n                )\n            )\n\n        return filename\n\n    def debug(self, msg, *args):\n        log.debug(msg, *args)\n\n    def info(self, msg, *args):\n        log.info(msg, *args)\n\n    def warn(self, msg, *args):\n        log.warn(msg, *args)\n\n\n# This pattern matches a character entity reference (a decimal numeric\n# references, a hexadecimal numeric reference, or a named reference).\nentity_sub = re.compile(r'&(#(\\d+|x[\\da-fA-F]+)|[\\w.:-]+);?').sub\n\n\ndef decode_entity(match):\n    what = match.group(0)\n    return html.unescape(what)\n\n\ndef htmldecode(text):\n    \"\"\"\n    Decode HTML entities in the given text.\n\n    >>> htmldecode(\n    ...     'https://../package_name-0.1.2.tar.gz'\n    ...     '?tokena=A&amp;tokenb=B\">package_name-0.1.2.tar.gz')\n    'https://../package_name-0.1.2.tar.gz?tokena=A&tokenb=B\">package_name-0.1.2.tar.gz'\n    \"\"\"\n    return entity_sub(decode_entity, text)\n\n\ndef socket_timeout(timeout=15):\n    def _socket_timeout(func):\n        def _socket_timeout(*args, **kwargs):\n            old_timeout = socket.getdefaulttimeout()\n            socket.setdefaulttimeout(timeout)\n            try:\n                return func(*args, **kwargs)\n            finally:\n                socket.setdefaulttimeout(old_timeout)\n\n        return _socket_timeout\n\n    return _socket_timeout\n\n\ndef _encode_auth(auth):\n    \"\"\"\n    Encode auth from a URL suitable for an HTTP header.\n    >>> str(_encode_auth('username%3Apassword'))\n    'dXNlcm5hbWU6cGFzc3dvcmQ='\n\n    Long auth strings should not cause a newline to be inserted.\n    >>> long_auth = 'username:' + 'password'*10\n    >>> chr(10) in str(_encode_auth(long_auth))\n    False\n    \"\"\"\n    auth_s = urllib.parse.unquote(auth)\n    # convert to bytes\n    auth_bytes = auth_s.encode()\n    encoded_bytes = base64.b64encode(auth_bytes)\n    # convert back to a string\n    encoded = encoded_bytes.decode()\n    # strip the trailing carriage return\n    return encoded.replace('\\n', '')\n\n\nclass Credential:\n    \"\"\"\n    A username/password pair. Use like a namedtuple.\n    \"\"\"\n\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def __iter__(self):\n        yield self.username\n        yield self.password\n\n    def __str__(self):\n        return '%(username)s:%(password)s' % vars(self)\n\n\nclass PyPIConfig(configparser.RawConfigParser):\n    def __init__(self):\n        \"\"\"\n        Load from ~/.pypirc\n        \"\"\"\n        defaults = dict.fromkeys(['username', 'password', 'repository'], '')\n        super().__init__(defaults)\n\n        rc = os.path.join(os.path.expanduser('~'), '.pypirc')\n        if os.path.exists(rc):\n            self.read(rc)\n\n    @property\n    def creds_by_repository(self):\n        sections_with_repositories = [\n            section\n            for section in self.sections()\n            if self.get(section, 'repository').strip()\n        ]\n\n        return dict(map(self._get_repo_cred, sections_with_repositories))\n\n    def _get_repo_cred(self, section):\n        repo = self.get(section, 'repository').strip()\n        return repo, Credential(\n            self.get(section, 'username').strip(),\n            self.get(section, 'password').strip(),\n        )\n\n    def find_credential(self, url):\n        \"\"\"\n        If the URL indicated appears to be a repository defined in this\n        config, return the credential for that repository.\n        \"\"\"\n        for repository, cred in self.creds_by_repository.items():\n            if url.startswith(repository):\n                return cred\n\n\ndef open_with_auth(url, opener=urllib.request.urlopen):\n    \"\"\"Open a urllib2 request, handling HTTP authentication\"\"\"\n\n    parsed = urllib.parse.urlparse(url)\n    scheme, netloc, path, params, query, frag = parsed\n\n    # Double scheme does not raise on macOS as revealed by a\n    # failing test. We would expect \"nonnumeric port\". Refs #20.\n    if netloc.endswith(':'):\n        raise http.client.InvalidURL(\"nonnumeric port: ''\")\n\n    if scheme in ('http', 'https'):\n        auth, address = _splituser(netloc)\n    else:\n        auth = None\n\n    if not auth:\n        cred = PyPIConfig().find_credential(url)\n        if cred:\n            auth = str(cred)\n            info = cred.username, url\n            log.info('Authenticating as %s for %s (from .pypirc)', *info)\n\n    if auth:\n        auth = \"Basic \" + _encode_auth(auth)\n        parts = scheme, address, path, params, query, frag\n        new_url = urllib.parse.urlunparse(parts)\n        request = urllib.request.Request(new_url)\n        request.add_header(\"Authorization\", auth)\n    else:\n        request = urllib.request.Request(url)\n\n    request.add_header('User-Agent', user_agent)\n    fp = opener(request)\n\n    if auth:\n        # Put authentication info back into request URL if same host,\n        # so that links found on the page will work\n        s2, h2, path2, param2, query2, frag2 = urllib.parse.urlparse(fp.url)\n        if s2 == scheme and h2 == address:\n            parts = s2, netloc, path2, param2, query2, frag2\n            fp.url = urllib.parse.urlunparse(parts)\n\n    return fp\n\n\n# copy of urllib.parse._splituser from Python 3.8\ndef _splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]')\n    --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    user, delim, host = host.rpartition('@')\n    return (user if delim else None), host\n\n\n# adding a timeout to avoid freezing package_index\nopen_with_auth = socket_timeout(_SOCKET_TIMEOUT)(open_with_auth)\n\n\ndef fix_sf_url(url):\n    return url  # backward compatibility\n\n\ndef local_open(url):\n    \"\"\"Read a local path, with special support for directories\"\"\"\n    scheme, server, path, param, query, frag = urllib.parse.urlparse(url)\n    filename = urllib.request.url2pathname(path)\n    if os.path.isfile(filename):\n        return urllib.request.urlopen(url)\n    elif path.endswith('/') and os.path.isdir(filename):\n        files = []\n        for f in os.listdir(filename):\n            filepath = os.path.join(filename, f)\n            if f == 'index.html':\n                with open(filepath, 'r') as fp:\n                    body = fp.read()\n                break\n            elif os.path.isdir(filepath):\n                f += '/'\n            files.append('<a href=\"{name}\">{name}</a>'.format(name=f))\n        else:\n            tmpl = (\n                \"<html><head><title>{url}</title>\" \"</head><body>{files}</body></html>\"\n            )\n            body = tmpl.format(url=url, files='\\n'.join(files))\n        status, message = 200, \"OK\"\n    else:\n        status, message, body = 404, \"Path not found\", \"Not found\"\n\n    headers = {'content-type': 'text/html'}\n    body_stream = io.StringIO(body)\n    return urllib.error.HTTPError(url, status, message, headers, body_stream)\n", "import sys\nimport os\nimport distutils.errors\nimport platform\nimport urllib.request\nimport urllib.error\nimport http.client\nfrom unittest import mock\n\nimport pytest\n\nimport setuptools.package_index\nfrom .textwrap import DALS\n\n\nclass TestPackageIndex:\n    def test_regex(self):\n        hash_url = 'http://other_url?:action=show_md5&amp;'\n        hash_url += 'digest=0123456789abcdef0123456789abcdef'\n        doc = \"\"\"\n            <a href=\"http://some_url\">Name</a>\n            (<a title=\"MD5 hash\"\n            href=\"{hash_url}\">md5</a>)\n        \"\"\".lstrip().format(\n            **locals()\n        )\n        assert setuptools.package_index.PYPI_MD5.match(doc)\n\n    def test_bad_url_bad_port(self):\n        index = setuptools.package_index.PackageIndex()\n        url = 'http://127.0.0.1:0/nonesuch/test_package_index'\n        try:\n            v = index.open_url(url)\n        except Exception as v:\n            assert url in str(v)\n        else:\n            assert isinstance(v, urllib.error.HTTPError)\n\n    def test_bad_url_typo(self):\n        # issue 16\n        # easy_install inquant.contentmirror.plone breaks because of a typo\n        # in its home URL\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        url = (\n            'url:%20https://svn.plone.org/svn'\n            '/collective/inquant.contentmirror.plone/trunk'\n        )\n        try:\n            v = index.open_url(url)\n        except Exception as v:\n            assert url in str(v)\n        else:\n            assert isinstance(v, urllib.error.HTTPError)\n\n    def test_bad_url_bad_status_line(self):\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        def _urlopen(*args):\n            raise http.client.BadStatusLine('line')\n\n        index.opener = _urlopen\n        url = 'http://example.com'\n        try:\n            index.open_url(url)\n        except Exception as exc:\n            assert 'line' in str(exc)\n        else:\n            raise AssertionError('Should have raise here!')\n\n    def test_bad_url_double_scheme(self):\n        \"\"\"\n        A bad URL with a double scheme should raise a DistutilsError.\n        \"\"\"\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        # issue 20\n        url = 'http://http://svn.pythonpaste.org/Paste/wphp/trunk'\n        try:\n            index.open_url(url)\n        except distutils.errors.DistutilsError as error:\n            msg = str(error)\n            assert (\n                'nonnumeric port' in msg\n                or 'getaddrinfo failed' in msg\n                or 'Name or service not known' in msg\n            )\n            return\n        raise RuntimeError(\"Did not raise\")\n\n    def test_bad_url_screwy_href(self):\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n\n        # issue #160\n        if sys.version_info[0] == 2 and sys.version_info[1] == 7:\n            # this should not fail\n            url = 'http://example.com'\n            page = '<a href=\"http://www.famfamfam.com](' 'http://www.famfamfam.com/\">'\n            index.process_index(url, page)\n\n    def test_url_ok(self):\n        index = setuptools.package_index.PackageIndex(hosts=('www.example.com',))\n        url = 'file:///tmp/test_package_index'\n        assert index.url_ok(url, True)\n\n    def test_parse_bdist_wininst(self):\n        parse = setuptools.package_index.parse_bdist_wininst\n\n        actual = parse('reportlab-2.5.win32-py2.4.exe')\n        expected = 'reportlab-2.5', '2.4', 'win32'\n        assert actual == expected\n\n        actual = parse('reportlab-2.5.win32.exe')\n        expected = 'reportlab-2.5', None, 'win32'\n        assert actual == expected\n\n        actual = parse('reportlab-2.5.win-amd64-py2.7.exe')\n        expected = 'reportlab-2.5', '2.7', 'win-amd64'\n        assert actual == expected\n\n        actual = parse('reportlab-2.5.win-amd64.exe')\n        expected = 'reportlab-2.5', None, 'win-amd64'\n        assert actual == expected\n\n    def test__vcs_split_rev_from_url(self):\n        \"\"\"\n        Test the basic usage of _vcs_split_rev_from_url\n        \"\"\"\n        vsrfu = setuptools.package_index.PackageIndex._vcs_split_rev_from_url\n        url, rev = vsrfu('https://example.com/bar@2995')\n        assert url == 'https://example.com/bar'\n        assert rev == '2995'\n\n    def test_local_index(self, tmpdir):\n        \"\"\"\n        local_open should be able to read an index from the file system.\n        \"\"\"\n        index_file = tmpdir / 'index.html'\n        with index_file.open('w') as f:\n            f.write('<div>content</div>')\n        url = 'file:' + urllib.request.pathname2url(str(tmpdir)) + '/'\n        res = setuptools.package_index.local_open(url)\n        assert 'content' in res.read()\n\n    def test_egg_fragment(self):\n        \"\"\"\n        EGG fragments must comply to PEP 440\n        \"\"\"\n        epoch = [\n            '',\n            '1!',\n        ]\n        releases = [\n            '0',\n            '0.0',\n            '0.0.0',\n        ]\n        pre = [\n            'a0',\n            'b0',\n            'rc0',\n        ]\n        post = ['.post0']\n        dev = [\n            '.dev0',\n        ]\n        local = [\n            ('', ''),\n            ('+ubuntu.0', '+ubuntu.0'),\n            ('+ubuntu-0', '+ubuntu.0'),\n            ('+ubuntu_0', '+ubuntu.0'),\n        ]\n        versions = [\n            [''.join([e, r, p, loc]) for loc in locs]\n            for e in epoch\n            for r in releases\n            for p in sum([pre, post, dev], [''])\n            for locs in local\n        ]\n        for v, vc in versions:\n            dists = list(\n                setuptools.package_index.distros_for_url(\n                    'http://example.com/example.zip#egg=example-' + v\n                )\n            )\n            assert dists[0].version == ''\n            assert dists[1].version == vc\n\n    def test_download_git_with_rev(self, tmpdir):\n        url = 'git+https://github.example/group/project@master#egg=foo'\n        index = setuptools.package_index.PackageIndex()\n\n        with mock.patch(\"os.system\") as os_system_mock:\n            result = index.download(url, str(tmpdir))\n\n        os_system_mock.assert_called()\n\n        expected_dir = str(tmpdir / 'project@master')\n        expected = (\n            'git clone --quiet ' 'https://github.example/group/project {expected_dir}'\n        ).format(**locals())\n        first_call_args = os_system_mock.call_args_list[0][0]\n        assert first_call_args == (expected,)\n\n        tmpl = 'git -C {expected_dir} checkout --quiet master'\n        expected = tmpl.format(**locals())\n        assert os_system_mock.call_args_list[1][0] == (expected,)\n        assert result == expected_dir\n\n    def test_download_git_no_rev(self, tmpdir):\n        url = 'git+https://github.example/group/project#egg=foo'\n        index = setuptools.package_index.PackageIndex()\n\n        with mock.patch(\"os.system\") as os_system_mock:\n            result = index.download(url, str(tmpdir))\n\n        os_system_mock.assert_called()\n\n        expected_dir = str(tmpdir / 'project')\n        expected = (\n            'git clone --quiet ' 'https://github.example/group/project {expected_dir}'\n        ).format(**locals())\n        os_system_mock.assert_called_once_with(expected)\n\n    def test_download_svn(self, tmpdir):\n        url = 'svn+https://svn.example/project#egg=foo'\n        index = setuptools.package_index.PackageIndex()\n\n        with pytest.warns(UserWarning):\n            with mock.patch(\"os.system\") as os_system_mock:\n                result = index.download(url, str(tmpdir))\n\n        os_system_mock.assert_called()\n\n        expected_dir = str(tmpdir / 'project')\n        expected = (\n            'svn checkout -q ' 'svn+https://svn.example/project {expected_dir}'\n        ).format(**locals())\n        os_system_mock.assert_called_once_with(expected)\n\n\nclass TestContentCheckers:\n    def test_md5(self):\n        checker = setuptools.package_index.HashChecker.from_url(\n            'http://foo/bar#md5=f12895fdffbd45007040d2e44df98478'\n        )\n        checker.feed('You should probably not be using MD5'.encode('ascii'))\n        assert checker.hash.hexdigest() == 'f12895fdffbd45007040d2e44df98478'\n        assert checker.is_valid()\n\n    def test_other_fragment(self):\n        \"Content checks should succeed silently if no hash is present\"\n        checker = setuptools.package_index.HashChecker.from_url(\n            'http://foo/bar#something%20completely%20different'\n        )\n        checker.feed('anything'.encode('ascii'))\n        assert checker.is_valid()\n\n    def test_blank_md5(self):\n        \"Content checks should succeed if a hash is empty\"\n        checker = setuptools.package_index.HashChecker.from_url('http://foo/bar#md5=')\n        checker.feed('anything'.encode('ascii'))\n        assert checker.is_valid()\n\n    def test_get_hash_name_md5(self):\n        checker = setuptools.package_index.HashChecker.from_url(\n            'http://foo/bar#md5=f12895fdffbd45007040d2e44df98478'\n        )\n        assert checker.hash_name == 'md5'\n\n    def test_report(self):\n        checker = setuptools.package_index.HashChecker.from_url(\n            'http://foo/bar#md5=f12895fdffbd45007040d2e44df98478'\n        )\n        rep = checker.report(lambda x: x, 'My message about %s')\n        assert rep == 'My message about md5'\n\n\n@pytest.fixture\ndef temp_home(tmpdir, monkeypatch):\n    key = (\n        'USERPROFILE'\n        if platform.system() == 'Windows' and sys.version_info > (3, 8)\n        else 'HOME'\n    )\n\n    monkeypatch.setitem(os.environ, key, str(tmpdir))\n    return tmpdir\n\n\nclass TestPyPIConfig:\n    def test_percent_in_password(self, temp_home):\n        pypirc = temp_home / '.pypirc'\n        pypirc.write(\n            DALS(\n                \"\"\"\n            [pypi]\n            repository=https://pypi.org\n            username=jaraco\n            password=pity%\n        \"\"\"\n            )\n        )\n        cfg = setuptools.package_index.PyPIConfig()\n        cred = cfg.creds_by_repository['https://pypi.org']\n        assert cred.username == 'jaraco'\n        assert cred.password == 'pity%'\n\n\n@pytest.mark.timeout(1)\ndef test_REL_DoS():\n    \"\"\"\n    REL should not hang on a contrived attack string.\n    \"\"\"\n    setuptools.package_index.REL.search('< rel=' + ' ' * 2**12)\n"], "filenames": ["setuptools/package_index.py", "setuptools/tests/test_packageindex.py"], "buggy_code_start_loc": [220, 310], "buggy_code_end_loc": [221, 311], "fixing_code_start_loc": [220, 309], "fixing_code_end_loc": [221, 309], "type": "NVD-CWE-Other", "message": "Python Packaging Authority (PyPA) setuptools before 65.5.1 allows remote attackers to cause a denial of service via HTML in a crafted package or custom PackageIndex page. There is a Regular Expression Denial of Service (ReDoS) in package_index.py.", "other": {"cve": {"id": "CVE-2022-40897", "sourceIdentifier": "cve@mitre.org", "published": "2022-12-23T00:15:13.987", "lastModified": "2023-05-01T06:15:12.800", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Python Packaging Authority (PyPA) setuptools before 65.5.1 allows remote attackers to cause a denial of service via HTML in a crafted package or custom PackageIndex page. There is a Regular Expression Denial of Service (ReDoS) in package_index.py."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:python:setuptools:*:*:*:*:*:*:*:*", "versionEndExcluding": "65.5.1", "matchCriteriaId": "F9A94E39-308F-4E14-8A0C-0327B4F50AAD"}]}]}], "references": [{"url": "https://github.com/pypa/setuptools/blob/fe8a98e696241487ba6ac9f91faa38ade939ec5d/setuptools/package_index.py#L200", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/pypa/setuptools/commit/43a9c9bfa6aa626ec2a22540bea28d2ca77964be", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/pypa/setuptools/compare/v65.5.0...v65.5.1", "source": "cve@mitre.org", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/ADES3NLOE5QJKBLGNZNI2RGVOSQXA37R/", "source": "cve@mitre.org"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/YNA2BAH2ACBZ4TVJZKFLCR7L23BG5C3H/", "source": "cve@mitre.org"}, {"url": "https://pyup.io/posts/pyup-discovers-redos-vulnerabilities-in-top-python-packages/", "source": "cve@mitre.org", "tags": ["Exploit", "Patch", "Technical Description", "Vendor Advisory"]}, {"url": "https://pyup.io/vulnerabilities/CVE-2022-40897/52495/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20230214-0001/", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/pypa/setuptools/commit/43a9c9bfa6aa626ec2a22540bea28d2ca77964be"}}