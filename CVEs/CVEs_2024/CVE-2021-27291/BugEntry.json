{"buggy_code": ["Pygments changelog\n==================\n\nSince 2.5.0, issue numbers refer to the tracker at\n<https://github.com/pygments/pygments/issues>,\npull request numbers to the requests at\n<https://github.com/pygments/pygments/pulls>.\n\n\nVersion 2.8.0\n-------------\n(not released yet)\n\n\nVersion 2.7.4\n-------------\n(released January 9, 2021)\n\n- Updated lexers:\n\n  - Apache configurations: Improve handling of malformed tags (#1656)\n  - Crystal (#1650)\n  - Coq (#1648)\n  - Fortran: Add missing ``ONLY`` keyword (#1635)\n  - Ini (#1624)\n  - JavaScript and variants (#1647 -- missing regex flags, #1651)\n  - Markdown (#1623)\n  - Shell\n\n    - Lex trailing whitespace as part of the prompt (#1645)\n    - Add missing ``in`` keyword (#1652)\n    \n  - Typescript: Fix incorrect punctuation handling (#1510, #1511)\n\n- Fix infinite loop in SML lexer (#1625)\n- Fix backtracking string regexes in JavaScript/TypeScript, Modula2\n  and many other lexers (#1637)\n- Limit recursion with nesting Ruby heredocs (#1638)\n- Fix a few inefficient regexes for guessing lexers\n- Fix the raw token lexer handling of Unicode (#1616)\n- Revert a private API change in the HTML formatter (#1655) -- please note that private APIs remain subject to change!\n- Add Dracula theme style (#1636)\n\nThanks to Google's OSS-Fuzz project for finding many of these bugs.\n\n\nVersion 2.7.3\n-------------\n(released December 6, 2020)\n\n- Updated lexers:\n\n  * Ada (#1581)\n  * HTML (#1615, #1614)\n  * Java (#1594, #1586)\n  * JavaScript (#1605, #1589, #1588)\n  * JSON (#1569 -- this is a complete rewrite)\n  * Lean (#1601)\n  * LLVM (#1612)\n  * Mason (#1592)\n  * MySQL (#1555, #1551)\n  * Rust (#1608)\n  * Turtle (#1590, #1553)\n\n- Deprecated JsonBareObjectLexer, which is now identical to JsonLexer (#1600)\n- The ``ImgFormatter`` now calculates the exact character width, which fixes some issues with overlapping text (#1213, #1611)\n- Documentation fixes (#1609, #1599, #1598)\n- Fixed duplicated Juttle language alias (#1604, #1606)\n- Added support for Kotlin scripts (#1587)\n- Removed CSS rule which forced margin to 0\n  \n\nVersion 2.7.2\n-------------\n(released October 24, 2020)\n\n- Updated lexers:\n\n  * Latex (#1517, #1516)\n  * LLVM (#1565)\n  * SPARQL (#1559)\n\n- Fix Python console/traceback lexer problems with custom exceptions without messages (#1548)\n- Allow loading ttc fonts on Mac/image formatter (#1223)\n- Improve ``analyze_text`` across a variety of lexers (#1549)\n- Remove CSS rule which forced the vertical padding to 0 for line numbers (#1583, #1579)\n- Fix ``TNTLexer`` crashing on unexpected EOL (#1568, #1570)\n- ``regexlint`` can be now run locally as part of ``tox`` tests (#1557)\n- Fix typos (#1550, #1562)\n- Add Python 3.9 as a supported version (#1554)\n\n\nVersion 2.7.1\n-------------\n(released September 16, 2020)\n\n- Fixed a regression in the JSON lexer (#1544)\n\n\nVersion 2.7.0\n-------------\n(released September 12, 2020)\n\n- Added lexers:\n\n  * Arrow (#1481, #1499)\n  * BARE (#1488)\n  * Devicetree (#1434)\n  * F* (#1409)\n  * GDScript (#1457)\n  * Pointless (#1494)\n  * PromQL (#1506)\n  * PsySH (#1438)\n  * Singularity (#1285)\n  * TiddlyWiki5 (#1390)\n  * TNT (#1414)\n  * YANG (#1408, #1428)\n\n- Updated lexers:\n\n  * APL (#1503)\n  * C++ (#1350, which also fixes: #1222, #996, #906, #828, #1162, #1166,\n    #1396)\n  * Chapel (#1423)\n  * CMake (#1491)\n  * CSound (#1509)\n  * Cython (#1507)\n  * Dart (#1449)\n  * Fennel (#1535)\n  * Fortran (#1442)\n  * GAS (#1530)\n  * HTTP (#1432, #1520, #1521)\n  * Inform 6 (#1461)\n  * Javascript (#1533)\n  * JSON (#1065, #1528)\n  * Lean (#1415)\n  * Matlab (#1399)\n  * Markdown (#1492, #1495)\n  * MySQL (#975, #1063, #1453, #1527)\n  * NASM (#1465)\n  * Nim (#1426)\n  * PostgreSQL (#1513)\n  * PowerShell (#1398, #1497)\n  * Protobuf (#1505)\n  * Robot (#1480)\n  * SQL (#1402)\n  * SystemVerilog (#1436, #1452, #1454, #1460, #1462, #1463, #1464, #1471, #1496, #1504)\n  * TeraTerm (#1337)\n  * XML (#1502)\n\n- Added a new filter for math symbols (#1406)\n- The Kconfig lexer will match Kconfig derivative names now (#1458)\n- Improved HTML formatter output (#1500)\n- ``.markdown`` is now recognized as an extension for Markdown files (#1476)\n- Fixed line number colors for Solarized (#1477, #1356)\n- Improvements to exception handling (#1478)\n- Improvements to tests (#1532, #1533, #1539)\n- Various code cleanups (#1536, #1537, #1538)\n\n\nVersion 2.6.1\n-------------\n(released March 8, 2020)\n\n- This release fixes a packaging issue. No functional changes.\n\n\nVersion 2.6\n-----------\n(released March 8, 2020)\n\n- Running Pygments on Python 2.x is no longer supported.\n  (The Python 2 lexer still exists.)\n\n- Added lexers:\n\n  * Linux kernel logs (#1310)\n  * LLVM MIR (#1361)\n  * MiniScript (#1397)\n  * Mosel (#1287, #1326)\n  * Parsing Expression Grammar (#1336)\n  * ReasonML (#1386)\n  * Ride (#1319, #1321)\n  * Sieve (#1257)\n  * USD (#1290)\n  * WebIDL (#1309)\n\n- Updated lexers:\n\n  * Apache2 (#1378)\n  * Chapel (#1357)\n  * CSound (#1383)\n  * D (#1375, #1362)\n  * Haskell (#1347, #1177)\n  * Idris (#1360)\n  * Perl6/Raku lexer (#1344)\n  * Python3 (#1382, #1385)\n  * Rust: Updated lexer to cover more builtins (mostly macros) and miscellaneous\n    new syntax (#1320)\n  * SQL: Add temporal support keywords (#1402)\n\n- The 256-color/true-color terminal formatters now support the italic attribute\n  in styles (#1288)\n- Support HTTP 2/3 header (#1308)\n- Support missing reason in HTTP header (#1322)\n- Boogie/Silver: support line continuations and triggers, move contract keywords\n  to separate category (#1299)\n- GAS: support C-style comments (#1291)\n- Fix names in S lexer (#1330, #1333)\n- Fix numeric literals in Ada (#1334)\n- Recognize ``.mjs`` files as Javascript (#1392)\n- Recognize ``.eex`` files as Elixir (#1387)\n- Fix ``re.MULTILINE`` usage (#1388)\n- Recognize ``pipenv`` and ``poetry`` dependency & lock files (PR#1376)\n- Improve font search on Windows (#1247)\n- Remove unused script block (#1401)\n\n\nVersion 2.5.2\n-------------\n(released November 29, 2019)\n\n- Fix incompatibility with some setuptools versions (PR#1316)\n\n- Fix lexing of ReST field lists (PR#1279)\n- Fix lexing of Matlab keywords as field names (PR#1282)\n- Recognize double-quoted strings in Matlab (PR#1278)\n- Avoid slow backtracking in Vim lexer (PR#1312)\n- Fix Scala highlighting of types (PR#1315)\n- Highlight field lists more consistently in ReST (PR#1279)\n- Fix highlighting Matlab keywords in field names (PR#1282)\n- Recognize Matlab double quoted strings (PR#1278)\n- Add some Terraform keywords\n- Update Modelica lexer to 3.4\n- Update Crystal examples\n\n\nVersion 2.5.1\n-------------\n(released November 26, 2019)\n\n- This release fixes a packaging issue. No functional changes.\n\n\nVersion 2.5.0\n-------------\n(released November 26, 2019)\n\n- Added lexers:\n\n  * Email (PR#1246)\n  * Erlang, Elixir shells (PR#823, #1521)\n  * Notmuch (PR#1264)\n  * `Scdoc <https://git.sr.ht/~sircmpwn/scdoc>`_ (PR#1268)\n  * `Solidity <https://solidity.readthedocs.io/>`_ (#1214)\n  * `Zeek <https://www.zeek.org>`_ (new name for Bro) (PR#1269)\n  * `Zig <https://ziglang.org/>`_ (PR#820)\n\n- Updated lexers:\n\n  * Apache2 Configuration (PR#1251)\n  * Bash sessions (#1253)\n  * CSound (PR#1250)\n  * Dart\n  * Dockerfile\n  * Emacs Lisp\n  * Handlebars (PR#773)\n  * Java (#1101, #987)\n  * Logtalk (PR#1261)\n  * Matlab (PR#1271)\n  * Praat (PR#1277)\n  * Python3 (PR#1255, PR#1400)\n  * Ruby\n  * YAML (#1528)\n  * Velocity\n\n- Added styles:\n\n  * Inkpot (PR#1276)\n\n- The ``PythonLexer`` class is now an alias for the former ``Python3Lexer``.\n  The old ``PythonLexer`` is available as ``Python2Lexer``.  Same change has\n  been done for the ``PythonTracebackLexer``.  The ``python3`` option for\n  the ``PythonConsoleLexer`` is now true by default.\n\n- Bump ``NasmLexer`` priority over ``TasmLexer`` for ``.asm`` files\n  (fixes #1326)\n- Default font in the ``ImageFormatter`` has been updated (#928, PR#1245)\n- Test suite switched to py.test, removed nose dependency (#1490)\n- Reduce ``TeraTerm`` lexer score -- it used to match nearly all languages\n  (#1256)\n- Treat ``Skylark``/``Starlark`` files as Python files (PR#1259)\n- Image formatter: actually respect ``line_number_separator`` option\n\n- Add LICENSE file to wheel builds\n- Agda: fix lambda highlighting\n- Dart: support ``@`` annotations\n- Dockerfile: accept ``FROM ... AS`` syntax\n- Emacs Lisp: add more string functions\n- GAS: accept registers in directive arguments\n- Java: make structural punctuation (braces, parens, colon, comma) ``Punctuation``, not ``Operator`` (#987)\n- Java: support ``var`` contextual keyword (#1101)\n- Matlab: Fix recognition of ``function`` keyword (PR#1271)\n- Python: recognize ``.jy`` filenames (#976)\n- Python: recognize ``f`` string prefix (#1156)\n- Ruby: support squiggly heredocs\n- Shell sessions: recognize Virtualenv prompt (PR#1266)\n- Velocity: support silent reference syntax\n\n\nVersion 2.4.2\n-------------\n(released May 28, 2019)\n\n- Fix encoding error when guessing lexer with given ``encoding`` option\n  (#1438)\n\n\nVersion 2.4.1\n-------------\n(released May 24, 2019)\n\n- Updated lexers:\n\n  * Coq (#1430)\n  * MSDOS Session (PR#734)\n  * NASM (#1517)\n  * Objective-C (PR#813, #1508)\n  * Prolog (#1511)\n  * TypeScript (#1515)\n\n- Support CSS variables in stylesheets (PR#814, #1356)\n- Fix F# lexer name (PR#709)\n- Fix ``TerminalFormatter`` using bold for bright text (#1480)\n\n\nVersion 2.4.0\n-------------\n(released May 8, 2019)\n\n- Added lexers:\n\n  * Augeas (PR#807)\n  * BBC Basic (PR#806)\n  * Boa (PR#756)\n  * Charm++ CI (PR#788)\n  * DASM16 (PR#807)\n  * FloScript (PR#750)\n  * FreeFem++ (PR#785)\n  * Hspec (PR#790)\n  * Pony (PR#627)\n  * SGF (PR#780)\n  * Slash (PR#807)\n  * Slurm (PR#760)\n  * Tera Term Language (PR#749)\n  * TOML (PR#807)\n  * Unicon (PR#731)\n  * VBScript (PR#673)\n\n- Updated lexers:\n\n  * Apache2 (PR#766)\n  * Cypher (PR#746)\n  * LLVM (PR#792)\n  * Makefiles (PR#766)\n  * PHP (#1482)\n  * Rust\n  * SQL (PR#672)\n  * Stan (PR#774)\n  * Stata (PR#800)\n  * Terraform (PR#787)\n  * YAML\n\n- Add solarized style (PR#708)\n- Add support for Markdown reference-style links (PR#753)\n- Add license information to generated HTML/CSS files (#1496)\n- Change ANSI color names (PR#777)\n- Fix catastrophic backtracking in the bash lexer (#1494)\n- Fix documentation failing to build using Sphinx 2.0 (#1501)\n- Fix incorrect links in the Lisp and R lexer documentation (PR#775)\n- Fix rare unicode errors on Python 2.7 (PR#798, #1492)\n- Fix lexers popping from an empty stack (#1506)\n- TypoScript uses ``.typoscript`` now (#1498)\n- Updated Trove classifiers and ``pip`` requirements (PR#799)\n\n\n\nVersion 2.3.1\n-------------\n(released Dec 16, 2018)\n\n- Updated lexers:\n\n  * ASM (PR#784)\n  * Chapel (PR#735)\n  * Clean (PR#621)\n  * CSound (PR#684)\n  * Elm (PR#744)\n  * Fortran (PR#747)\n  * GLSL (PR#740)\n  * Haskell (PR#745)\n  * Hy (PR#754)\n  * Igor Pro (PR#764)\n  * PowerShell (PR#705)\n  * Python (PR#720, #1299, PR#715)\n  * SLexer (PR#680)\n  * YAML (PR#762, PR#724)\n\n- Fix invalid string escape sequences\n- Fix `FutureWarning` introduced by regex changes in Python 3.7\n\n\nVersion 2.3.0\n-------------\n(released Nov 25, 2018)\n\n- Added lexers:\n\n  * Fennel (PR#783)\n  * HLSL (PR#675)\n\n- Updated lexers:\n\n  * Dockerfile (PR#714)\n\n- Minimum Python versions changed to 2.7 and 3.5\n- Added support for Python 3.7 generator changes (PR#772)\n- Fix incorrect token type in SCSS for single-quote strings (#1322)\n- Use `terminal256` formatter if `TERM` contains `256` (PR#666)\n- Fix incorrect handling of GitHub style fences in Markdown (PR#741, #1389)\n- Fix `%a` not being highlighted in Python3 strings (PR#727)\n\n\nVersion 2.2.0\n-------------\n(released Jan 22, 2017)\n\n- Added lexers:\n\n  * AMPL\n  * TypoScript (#1173)\n  * Varnish config (PR#554)\n  * Clean (PR#503)\n  * WDiff (PR#513)\n  * Flatline (PR#551)\n  * Silver (PR#537)\n  * HSAIL (PR#518)\n  * JSGF (PR#546)\n  * NCAR command language (PR#536)\n  * Extempore (PR#530)\n  * Cap'n Proto (PR#595)\n  * Whiley (PR#573)\n  * Monte (PR#592)\n  * Crystal (PR#576)\n  * Snowball (PR#589)\n  * CapDL (PR#579)\n  * NuSMV (PR#564)\n  * SAS, Stata (PR#593)\n\n- Added the ability to load lexer and formatter classes directly from files\n  with the `-x` command line option and the `lexers.load_lexer_from_file()`\n  and `formatters.load_formatter_from_file()` functions. (PR#559)\n\n- Added `lexers.find_lexer_class_by_name()`. (#1203)\n\n- Added new token types and lexing for magic methods and variables in Python\n  and PHP.\n\n- Added a new token type for string affixes and lexing for them in Python, C++\n  and Postgresql lexers.\n\n- Added a new token type for heredoc (and similar) string delimiters and\n  lexing for them in C++, Perl, PHP, Postgresql and Ruby lexers.\n\n- Styles can now define colors with ANSI colors for use in the 256-color\n  terminal formatter. (PR#531)\n\n- Improved the CSS lexer. (#1083, #1130)\n\n- Added \"Rainbow Dash\" style. (PR#623)\n\n- Delay loading `pkg_resources`, which takes a long while to import. (PR#690)\n\n\nVersion 2.1.3\n-------------\n(released Mar 2, 2016)\n\n- Fixed regression in Bash lexer (PR#563)\n\n\nVersion 2.1.2\n-------------\n(released Feb 29, 2016)\n\n- Fixed Python 3 regression in image formatter (#1215)\n- Fixed regression in Bash lexer (PR#562)\n\n\nVersion 2.1.1\n-------------\n(relased Feb 14, 2016)\n\n- Fixed Jython compatibility (#1205)\n- Fixed HTML formatter output with leading empty lines (#1111)\n- Added a mapping table for LaTeX encodings and added utf8 (#1152)\n- Fixed image formatter font searching on Macs (#1188)\n- Fixed deepcopy-ing of Token instances (#1168)\n- Fixed Julia string interpolation (#1170)\n- Fixed statefulness of HttpLexer between get_tokens calls\n- Many smaller fixes to various lexers\n\n\nVersion 2.1\n-----------\n(released Jan 17, 2016)\n\n- Added lexers:\n\n  * Emacs Lisp (PR#431)\n  * Arduino (PR#442)\n  * Modula-2 with multi-dialect support (#1090)\n  * Fortran fixed format (PR#213)\n  * Archetype Definition language (PR#483)\n  * Terraform (PR#432)\n  * Jcl, Easytrieve (PR#208)\n  * ParaSail (PR#381)\n  * Boogie (PR#420)\n  * Turtle (PR#425)\n  * Fish Shell (PR#422)\n  * Roboconf (PR#449)\n  * Test Anything Protocol (PR#428)\n  * Shen (PR#385)\n  * Component Pascal (PR#437)\n  * SuperCollider (PR#472)\n  * Shell consoles (Tcsh, PowerShell, MSDOS) (PR#479)\n  * Elm and J (PR#452)\n  * Crmsh (PR#440)\n  * Praat (PR#492)\n  * CSound (PR#494)\n  * Ezhil (PR#443)\n  * Thrift (PR#469)\n  * QVT Operational (PR#204)\n  * Hexdump (PR#508)\n  * CAmkES Configuration (PR#462)\n\n- Added styles:\n\n  * Lovelace (PR#456)\n  * Algol and Algol-nu (#1090)\n\n- Added formatters:\n\n  * IRC (PR#458)\n  * True color (24-bit) terminal ANSI sequences (#1142)\n    (formatter alias: \"16m\")\n\n- New \"filename\" option for HTML formatter (PR#527).\n\n- Improved performance of the HTML formatter for long lines (PR#504).\n\n- Updated autopygmentize script (PR#445).\n\n- Fixed style inheritance for non-standard token types in HTML output.\n\n- Added support for async/await to Python 3 lexer.\n\n- Rewrote linenos option for TerminalFormatter (it's better, but slightly\n  different output than before) (#1147).\n\n- Javascript lexer now supports most of ES6 (#1100).\n\n- Cocoa builtins updated for iOS 8.1 (PR#433).\n\n- Combined BashSessionLexer and ShellSessionLexer, new version should support\n  the prompt styles of either.\n\n- Added option to pygmentize to show a full traceback on exceptions.\n\n- Fixed incomplete output on Windows and Python 3 (e.g. when using iPython\n  Notebook) (#1153).\n\n- Allowed more traceback styles in Python console lexer (PR#253).\n\n- Added decorators to TypeScript (PR#509).\n\n- Fix highlighting of certain IRC logs formats (#1076).\n\n\nVersion 2.0.2\n-------------\n(released Jan 20, 2015)\n\n- Fix Python tracebacks getting duplicated in the console lexer (#1068).\n\n- Backquote-delimited identifiers are now recognized in F# (#1062).\n\n\nVersion 2.0.1\n-------------\n(released Nov 10, 2014)\n\n- Fix an encoding issue when using ``pygmentize`` with the ``-o`` option.\n\n\nVersion 2.0\n-----------\n(released Nov 9, 2014)\n\n- Default lexer encoding is now \"guess\", i.e. UTF-8 / Locale / Latin1 is\n  tried in that order.\n\n- Major update to Swift lexer (PR#410).\n\n- Multiple fixes to lexer guessing in conflicting cases:\n\n  * recognize HTML5 by doctype\n  * recognize XML by XML declaration\n  * don't recognize C/C++ as SystemVerilog\n\n- Simplified regexes and builtin lists.\n\n\nVersion 2.0rc1\n--------------\n(released Oct 16, 2014)\n\n- Dropped Python 2.4 and 2.5 compatibility.  This is in favor of single-source\n  compatibility between Python 2.6, 2.7 and 3.3+.\n\n- New website and documentation based on Sphinx (finally!)\n\n- Lexers added:\n\n  * APL (#969)\n  * Agda and Literate Agda (PR#203)\n  * Alloy (PR#355)\n  * AmbientTalk\n  * BlitzBasic (PR#197)\n  * ChaiScript (PR#24)\n  * Chapel (PR#256)\n  * Cirru (PR#275)\n  * Clay (PR#184)\n  * ColdFusion CFC (PR#283)\n  * Cryptol and Literate Cryptol (PR#344)\n  * Cypher (PR#257)\n  * Docker config files\n  * EBNF (PR#193)\n  * Eiffel (PR#273)\n  * GAP (PR#311)\n  * Golo (PR#309)\n  * Handlebars (PR#186)\n  * Hy (PR#238)\n  * Idris and Literate Idris (PR#210)\n  * Igor Pro (PR#172)\n  * Inform 6/7 (PR#281)\n  * Intel objdump (PR#279)\n  * Isabelle (PR#386)\n  * Jasmin (PR#349)\n  * JSON-LD (PR#289)\n  * Kal (PR#233)\n  * Lean (PR#399)\n  * LSL (PR#296)\n  * Limbo (PR#291)\n  * Liquid (#977)\n  * MQL (PR#285)\n  * MaskJS (PR#280)\n  * Mozilla preprocessors\n  * Mathematica (PR#245)\n  * NesC (PR#166)\n  * Nit (PR#375)\n  * Nix (PR#267)\n  * Pan\n  * Pawn (PR#211)\n  * Perl 6 (PR#181)\n  * Pig (PR#304)\n  * Pike (PR#237)\n  * QBasic (PR#182)\n  * Red (PR#341)\n  * ResourceBundle (#1038)\n  * Rexx (PR#199)\n  * Rql (PR#251)\n  * Rsl\n  * SPARQL (PR#78)\n  * Slim (PR#366)\n  * Swift (PR#371)\n  * Swig (PR#168)\n  * TADS 3 (PR#407)\n  * Todo.txt todo lists\n  * Twig (PR#404)\n\n- Added a helper to \"optimize\" regular expressions that match one of many\n  literal words; this can save 20% and more lexing time with lexers that\n  highlight many keywords or builtins.\n\n- New styles: \"xcode\" and \"igor\", similar to the default highlighting of\n  the respective IDEs.\n\n- The command-line \"pygmentize\" tool now tries a little harder to find the\n  correct encoding for files and the terminal (#979).\n\n- Added \"inencoding\" option for lexers to override \"encoding\" analogous\n  to \"outencoding\" (#800).\n\n- Added line-by-line \"streaming\" mode for pygmentize with the \"-s\" option.\n  (PR#165)  Only fully works for lexers that have no constructs spanning\n  lines!\n\n- Added an \"envname\" option to the LaTeX formatter to select a replacement\n  verbatim environment (PR#235).\n\n- Updated the Makefile lexer to yield a little more useful highlighting.\n\n- Lexer aliases passed to ``get_lexer_by_name()`` are now case-insensitive.\n\n- File name matching in lexers and formatters will now use a regex cache\n  for speed (PR#205).\n\n- Pygments will now recognize \"vim\" modelines when guessing the lexer for\n  a file based on content (PR#118).\n\n- Major restructure of the ``pygments.lexers`` module namespace.  There are now\n  many more modules with less lexers per module.  Old modules are still around\n  and re-export the lexers they previously contained.\n\n- The NameHighlightFilter now works with any Name.* token type (#790).\n\n- Python 3 lexer: add new exceptions from PEP 3151.\n\n- Opa lexer: add new keywords (PR#170).\n\n- Julia lexer: add keywords and underscore-separated number\n  literals (PR#176).\n\n- Lasso lexer: fix method highlighting, update builtins. Fix\n  guessing so that plain XML isn't always taken as Lasso (PR#163).\n\n- Objective C/C++ lexers: allow \"@\" prefixing any expression (#871).\n\n- Ruby lexer: fix lexing of Name::Space tokens (#860) and of symbols\n  in hashes (#873).\n\n- Stan lexer: update for version 2.4.0 of the language (PR#162, PR#255, PR#377).\n\n- JavaScript lexer: add the \"yield\" keyword (PR#196).\n\n- HTTP lexer: support for PATCH method (PR#190).\n\n- Koka lexer: update to newest language spec (PR#201).\n\n- Haxe lexer: rewrite and support for Haxe 3 (PR#174).\n\n- Prolog lexer: add different kinds of numeric literals (#864).\n\n- F# lexer: rewrite with newest spec for F# 3.0 (#842), fix a bug with\n  dotted chains (#948).\n\n- Kotlin lexer: general update (PR#271).\n\n- Rebol lexer: fix comment detection and analyse_text (PR#261).\n\n- LLVM lexer: update keywords to v3.4 (PR#258).\n\n- PHP lexer: add new keywords and binary literals (PR#222).\n\n- external/markdown-processor.py updated to newest python-markdown (PR#221).\n\n- CSS lexer: some highlighting order fixes (PR#231).\n\n- Ceylon lexer: fix parsing of nested multiline comments (#915).\n\n- C family lexers: fix parsing of indented preprocessor directives (#944).\n\n- Rust lexer: update to 0.9 language version (PR#270, PR#388).\n\n- Elixir lexer: update to 0.15 language version (PR#392).\n\n- Fix swallowing incomplete tracebacks in Python console lexer (#874).\n\n\nVersion 1.6\n-----------\n(released Feb 3, 2013)\n\n- Lexers added:\n\n  * Dylan console (PR#149)\n  * Logos (PR#150)\n  * Shell sessions (PR#158)\n\n- Fix guessed lexers not receiving lexer options (#838).\n\n- Fix unquoted HTML attribute lexing in Opa (#841).\n\n- Fixes to the Dart lexer (PR#160).\n\n\nVersion 1.6rc1\n--------------\n(released Jan 9, 2013)\n\n- Lexers added:\n\n  * AspectJ (PR#90)\n  * AutoIt (PR#122)\n  * BUGS-like languages (PR#89)\n  * Ceylon (PR#86)\n  * Croc (new name for MiniD)\n  * CUDA (PR#75)\n  * Dg (PR#116)\n  * IDL (PR#115)\n  * Jags (PR#89)\n  * Julia (PR#61)\n  * Kconfig (#711)\n  * Lasso (PR#95, PR#113)\n  * LiveScript (PR#84)\n  * Monkey (PR#117)\n  * Mscgen (PR#80)\n  * NSIS scripts (PR#136)\n  * OpenCOBOL (PR#72)\n  * QML (PR#123)\n  * Puppet (PR#133)\n  * Racket (PR#94)\n  * Rdoc (PR#99)\n  * Robot Framework (PR#137)\n  * RPM spec files (PR#124)\n  * Rust (PR#67)\n  * Smali (Dalvik assembly)\n  * SourcePawn (PR#39)\n  * Stan (PR#89)\n  * Treetop (PR#125)\n  * TypeScript (PR#114)\n  * VGL (PR#12)\n  * Visual FoxPro (#762)\n  * Windows Registry (#819)\n  * Xtend (PR#68)\n\n- The HTML formatter now supports linking to tags using CTags files, when the\n  python-ctags package is installed (PR#87).\n\n- The HTML formatter now has a \"linespans\" option that wraps every line in a\n  <span> tag with a specific id (PR#82).\n\n- When deriving a lexer from another lexer with token definitions, definitions\n  for states not in the child lexer are now inherited.  If you override a state\n  in the child lexer, an \"inherit\" keyword has been added to insert the base\n  state at that position (PR#141).\n\n- The C family lexers now inherit token definitions from a common base class,\n  removing code duplication (PR#141).\n\n- Use \"colorama\" on Windows for console color output (PR#142).\n\n- Fix Template Haskell highlighting (PR#63).\n\n- Fix some S/R lexer errors (PR#91).\n\n- Fix a bug in the Prolog lexer with names that start with 'is' (#810).\n\n- Rewrite Dylan lexer, add Dylan LID lexer (PR#147).\n\n- Add a Java quickstart document (PR#146).\n\n- Add a \"external/autopygmentize\" file that can be used as .lessfilter (#802).\n\n\nVersion 1.5\n-----------\n(codename Zeitdilatation, released Mar 10, 2012)\n\n- Lexers added:\n\n  * Awk (#630)\n  * Fancy (#633)\n  * PyPy Log\n  * eC\n  * Nimrod\n  * Nemerle (#667)\n  * F# (#353)\n  * Groovy (#501)\n  * PostgreSQL (#660)\n  * DTD\n  * Gosu (#634)\n  * Octave (PR#22)\n  * Standard ML (PR#14)\n  * CFengine3 (#601)\n  * Opa (PR#37)\n  * HTTP sessions (PR#42)\n  * JSON (PR#31)\n  * SNOBOL (PR#30)\n  * MoonScript (PR#43)\n  * ECL (PR#29)\n  * Urbiscript (PR#17)\n  * OpenEdge ABL (PR#27)\n  * SystemVerilog (PR#35)\n  * Coq (#734)\n  * PowerShell (#654)\n  * Dart (#715)\n  * Fantom (PR#36)\n  * Bro (PR#5)\n  * NewLISP (PR#26)\n  * VHDL (PR#45)\n  * Scilab (#740)\n  * Elixir (PR#57)\n  * Tea (PR#56)\n  * Kotlin (PR#58)\n\n- Fix Python 3 terminal highlighting with pygmentize (#691).\n\n- In the LaTeX formatter, escape special &, < and > chars (#648).\n\n- In the LaTeX formatter, fix display problems for styles with token\n  background colors (#670).\n\n- Enhancements to the Squid conf lexer (#664).\n\n- Several fixes to the reStructuredText lexer (#636).\n\n- Recognize methods in the ObjC lexer (#638).\n\n- Fix Lua \"class\" highlighting: it does not have classes (#665).\n\n- Fix degenerate regex in Scala lexer (#671) and highlighting bugs (#713, 708).\n\n- Fix number pattern order in Ocaml lexer (#647).\n\n- Fix generic type highlighting in ActionScript 3 (#666).\n\n- Fixes to the Clojure lexer (PR#9).\n\n- Fix degenerate regex in Nemerle lexer (#706).\n\n- Fix infinite looping in CoffeeScript lexer (#729).\n\n- Fix crashes and analysis with ObjectiveC lexer (#693, #696).\n\n- Add some Fortran 2003 keywords.\n\n- Fix Boo string regexes (#679).\n\n- Add \"rrt\" style (#727).\n\n- Fix infinite looping in Darcs Patch lexer.\n\n- Lots of misc fixes to character-eating bugs and ordering problems in many\n  different lexers.\n\n\nVersion 1.4\n-----------\n(codename Unsch\u00e4rfe, released Jan 03, 2011)\n\n- Lexers added:\n\n  * Factor (#520)\n  * PostScript (#486)\n  * Verilog (#491)\n  * BlitzMax Basic (#478)\n  * Ioke (#465)\n  * Java properties, split out of the INI lexer (#445)\n  * Scss (#509)\n  * Duel/JBST\n  * XQuery (#617)\n  * Mason (#615)\n  * GoodData (#609)\n  * SSP (#473)\n  * Autohotkey (#417)\n  * Google Protocol Buffers\n  * Hybris (#506)\n\n- Do not fail in analyse_text methods (#618).\n\n- Performance improvements in the HTML formatter (#523).\n\n- With the ``noclasses`` option in the HTML formatter, some styles\n  present in the stylesheet were not added as inline styles.\n\n- Four fixes to the Lua lexer (#480, #481, #482, #497).\n\n- More context-sensitive Gherkin lexer with support for more i18n translations.\n\n- Support new OO keywords in Matlab lexer (#521).\n\n- Small fix in the CoffeeScript lexer (#519).\n\n- A bugfix for backslashes in ocaml strings (#499).\n\n- Fix unicode/raw docstrings in the Python lexer (#489).\n\n- Allow PIL to work without PIL.pth (#502).\n\n- Allow seconds as a unit in CSS (#496).\n\n- Support ``application/javascript`` as a JavaScript mime type (#504).\n\n- Support `Offload <https://offload.codeplay.com/>`_ C++ Extensions as\n  keywords in the C++ lexer (#484).\n\n- Escape more characters in LaTeX output (#505).\n\n- Update Haml/Sass lexers to version 3 (#509).\n\n- Small PHP lexer string escaping fix (#515).\n\n- Support comments before preprocessor directives, and unsigned/\n  long long literals in C/C++ (#613, #616).\n\n- Support line continuations in the INI lexer (#494).\n\n- Fix lexing of Dylan string and char literals (#628).\n\n- Fix class/procedure name highlighting in VB.NET lexer (#624).\n\n\nVersion 1.3.1\n-------------\n(bugfix release, released Mar 05, 2010)\n\n- The ``pygmentize`` script was missing from the distribution.\n\n\nVersion 1.3\n-----------\n(codename Schneegl\u00f6ckchen, released Mar 01, 2010)\n\n- Added the ``ensurenl`` lexer option, which can be used to suppress the\n  automatic addition of a newline to the lexer input.\n\n- Lexers added:\n\n  * Ada\n  * Coldfusion\n  * Modula-2\n  * Haxe\n  * R console\n  * Objective-J\n  * Haml and Sass\n  * CoffeeScript\n\n- Enhanced reStructuredText highlighting.\n\n- Added support for PHP 5.3 namespaces in the PHP lexer.\n\n- Added a bash completion script for `pygmentize`, to the external/\n  directory (#466).\n\n- Fixed a bug in `do_insertions()` used for multi-lexer languages.\n\n- Fixed a Ruby regex highlighting bug (#476).\n\n- Fixed regex highlighting bugs in Perl lexer (#258).\n\n- Add small enhancements to the C lexer (#467) and Bash lexer (#469).\n\n- Small fixes for the Tcl, Debian control file, Nginx config,\n  Smalltalk, Objective-C, Clojure, Lua lexers.\n\n- Gherkin lexer: Fixed single apostrophe bug and added new i18n keywords.\n\n\nVersion 1.2.2\n-------------\n(bugfix release, released Jan 02, 2010)\n\n* Removed a backwards incompatibility in the LaTeX formatter that caused\n  Sphinx to produce invalid commands when writing LaTeX output (#463).\n\n* Fixed a forever-backtracking regex in the BashLexer (#462).\n\n\nVersion 1.2.1\n-------------\n(bugfix release, released Jan 02, 2010)\n\n* Fixed mishandling of an ellipsis in place of the frames in a Python\n  console traceback, resulting in clobbered output.\n\n\nVersion 1.2\n-----------\n(codename Neujahr, released Jan 01, 2010)\n\n- Dropped Python 2.3 compatibility.\n\n- Lexers added:\n\n  * Asymptote\n  * Go\n  * Gherkin (Cucumber)\n  * CMake\n  * Ooc\n  * Coldfusion\n  * Haxe\n  * R console\n\n- Added options for rendering LaTeX in source code comments in the\n  LaTeX formatter (#461).\n\n- Updated the Logtalk lexer.\n\n- Added `line_number_start` option to image formatter (#456).\n\n- Added `hl_lines` and `hl_color` options to image formatter (#457).\n\n- Fixed the HtmlFormatter's handling of noclasses=True to not output any\n  classes (#427).\n\n- Added the Monokai style (#453).\n\n- Fixed LLVM lexer identifier syntax and added new keywords (#442).\n\n- Fixed the PythonTracebackLexer to handle non-traceback data in header or\n  trailer, and support more partial tracebacks that start on line 2 (#437).\n\n- Fixed the CLexer to not highlight ternary statements as labels.\n\n- Fixed lexing of some Ruby quoting peculiarities (#460).\n\n- A few ASM lexer fixes (#450).\n\n\nVersion 1.1.1\n-------------\n(bugfix release, released Sep 15, 2009)\n\n- Fixed the BBCode lexer (#435).\n\n- Added support for new Jinja2 keywords.\n\n- Fixed test suite failures.\n\n- Added Gentoo-specific suffixes to Bash lexer.\n\n\nVersion 1.1\n-----------\n(codename Brillouin, released Sep 11, 2009)\n\n- Ported Pygments to Python 3.  This needed a few changes in the way\n  encodings are handled; they may affect corner cases when used with\n  Python 2 as well.\n\n- Lexers added:\n\n  * Antlr/Ragel, thanks to Ana Nelson\n  * (Ba)sh shell\n  * Erlang shell\n  * GLSL\n  * Prolog\n  * Evoque\n  * Modelica\n  * Rebol\n  * MXML\n  * Cython\n  * ABAP\n  * ASP.net (VB/C#)\n  * Vala\n  * Newspeak\n\n- Fixed the LaTeX formatter's output so that output generated for one style\n  can be used with the style definitions of another (#384).\n\n- Added \"anchorlinenos\" and \"noclobber_cssfile\" (#396) options to HTML\n  formatter.\n\n- Support multiline strings in Lua lexer.\n\n- Rewrite of the JavaScript lexer by Pumbaa80 to better support regular\n  expression literals (#403).\n\n- When pygmentize is asked to highlight a file for which multiple lexers\n  match the filename, use the analyse_text guessing engine to determine the\n  winner (#355).\n\n- Fixed minor bugs in the JavaScript lexer (#383), the Matlab lexer (#378),\n  the Scala lexer (#392), the INI lexer (#391), the Clojure lexer (#387)\n  and the AS3 lexer (#389).\n\n- Fixed three Perl heredoc lexing bugs (#379, #400, #422).\n\n- Fixed a bug in the image formatter which misdetected lines (#380).\n\n- Fixed bugs lexing extended Ruby strings and regexes.\n\n- Fixed a bug when lexing git diffs.\n\n- Fixed a bug lexing the empty commit in the PHP lexer (#405).\n\n- Fixed a bug causing Python numbers to be mishighlighted as floats (#397).\n\n- Fixed a bug when backslashes are used in odd locations in Python (#395).\n\n- Fixed various bugs in Matlab and S-Plus lexers, thanks to Winston Chang (#410,\n  #411, #413, #414) and fmarc (#419).\n\n- Fixed a bug in Haskell single-line comment detection (#426).\n\n- Added new-style reStructuredText directive for docutils 0.5+ (#428).\n\n\nVersion 1.0\n-----------\n(codename Dreiundzwanzig, released Nov 23, 2008)\n\n- Don't use join(splitlines()) when converting newlines to ``\\n``,\n  because that doesn't keep all newlines at the end when the\n  ``stripnl`` lexer option is False.\n\n- Added ``-N`` option to command-line interface to get a lexer name\n  for a given filename.\n\n- Added Tango style, written by Andre Roberge for the Crunchy project.\n\n- Added Python3TracebackLexer and ``python3`` option to\n  PythonConsoleLexer.\n\n- Fixed a few bugs in the Haskell lexer.\n\n- Fixed PythonTracebackLexer to be able to recognize SyntaxError and\n  KeyboardInterrupt (#360).\n\n- Provide one formatter class per image format, so that surprises like::\n\n    pygmentize -f gif -o foo.gif foo.py\n\n  creating a PNG file are avoided.\n\n- Actually use the `font_size` option of the image formatter.\n\n- Fixed numpy lexer that it doesn't listen for `*.py` any longer.\n\n- Fixed HTML formatter so that text options can be Unicode\n  strings (#371).\n\n- Unified Diff lexer supports the \"udiff\" alias now.\n\n- Fixed a few issues in Scala lexer (#367).\n\n- RubyConsoleLexer now supports simple prompt mode (#363).\n\n- JavascriptLexer is smarter about what constitutes a regex (#356).\n\n- Add Applescript lexer, thanks to Andreas Amann (#330).\n\n- Make the codetags more strict about matching words (#368).\n\n- NginxConfLexer is a little more accurate on mimetypes and\n  variables (#370).\n\n\nVersion 0.11.1\n--------------\n(released Aug 24, 2008)\n\n- Fixed a Jython compatibility issue in pygments.unistring (#358).\n\n\nVersion 0.11\n------------\n(codename Strau\u00dfenei, released Aug 23, 2008)\n\nMany thanks go to Tim Hatch for writing or integrating most of the bug\nfixes and new features.\n\n- Lexers added:\n\n  * Nasm-style assembly language, thanks to delroth\n  * YAML, thanks to Kirill Simonov\n  * ActionScript 3, thanks to Pierre Bourdon\n  * Cheetah/Spitfire templates, thanks to Matt Good\n  * Lighttpd config files\n  * Nginx config files\n  * Gnuplot plotting scripts\n  * Clojure\n  * POV-Ray scene files\n  * Sqlite3 interactive console sessions\n  * Scala source files, thanks to Krzysiek Goj\n\n- Lexers improved:\n\n  * C lexer highlights standard library functions now and supports C99\n    types.\n  * Bash lexer now correctly highlights heredocs without preceding\n    whitespace.\n  * Vim lexer now highlights hex colors properly and knows a couple\n    more keywords.\n  * Irc logs lexer now handles xchat's default time format (#340) and\n    correctly highlights lines ending in ``>``.\n  * Support more delimiters for perl regular expressions (#258).\n  * ObjectiveC lexer now supports 2.0 features.\n\n- Added \"Visual Studio\" style.\n\n- Updated markdown processor to Markdown 1.7.\n\n- Support roman/sans/mono style defs and use them in the LaTeX\n  formatter.\n\n- The RawTokenFormatter is no longer registered to ``*.raw`` and it's\n  documented that tokenization with this lexer may raise exceptions.\n\n- New option ``hl_lines`` to HTML formatter, to highlight certain\n  lines.\n\n- New option ``prestyles`` to HTML formatter.\n\n- New option *-g* to pygmentize, to allow lexer guessing based on\n  filetext (can be slowish, so file extensions are still checked\n  first).\n\n- ``guess_lexer()`` now makes its decision much faster due to a cache\n  of whether data is xml-like (a check which is used in several\n  versions of ``analyse_text()``.  Several lexers also have more\n  accurate ``analyse_text()`` now.\n\n\nVersion 0.10\n------------\n(codename Malzeug, released May 06, 2008)\n\n- Lexers added:\n\n  * Io\n  * Smalltalk\n  * Darcs patches\n  * Tcl\n  * Matlab\n  * Matlab sessions\n  * FORTRAN\n  * XSLT\n  * tcsh\n  * NumPy\n  * Python 3\n  * S, S-plus, R statistics languages\n  * Logtalk\n\n- In the LatexFormatter, the *commandprefix* option is now by default\n  'PY' instead of 'C', since the latter resulted in several collisions\n  with other packages.  Also, the special meaning of the *arg*\n  argument to ``get_style_defs()`` was removed.\n\n- Added ImageFormatter, to format code as PNG, JPG, GIF or BMP.\n  (Needs the Python Imaging Library.)\n\n- Support doc comments in the PHP lexer.\n\n- Handle format specifications in the Perl lexer.\n\n- Fix comment handling in the Batch lexer.\n\n- Add more file name extensions for the C++, INI and XML lexers.\n\n- Fixes in the IRC and MuPad lexers.\n\n- Fix function and interface name highlighting in the Java lexer.\n\n- Fix at-rule handling in the CSS lexer.\n\n- Handle KeyboardInterrupts gracefully in pygmentize.\n\n- Added BlackWhiteStyle.\n\n- Bash lexer now correctly highlights math, does not require\n  whitespace after semicolons, and correctly highlights boolean\n  operators.\n\n- Makefile lexer is now capable of handling BSD and GNU make syntax.\n\n\nVersion 0.9\n-----------\n(codename Herbstzeitlose, released Oct 14, 2007)\n\n- Lexers added:\n\n  * Erlang\n  * ActionScript\n  * Literate Haskell\n  * Common Lisp\n  * Various assembly languages\n  * Gettext catalogs\n  * Squid configuration\n  * Debian control files\n  * MySQL-style SQL\n  * MOOCode\n\n- Lexers improved:\n\n  * Greatly improved the Haskell and OCaml lexers.\n  * Improved the Bash lexer's handling of nested constructs.\n  * The C# and Java lexers exhibited abysmal performance with some\n    input code; this should now be fixed.\n  * The IRC logs lexer is now able to colorize weechat logs too.\n  * The Lua lexer now recognizes multi-line comments.\n  * Fixed bugs in the D and MiniD lexer.\n\n- The encoding handling of the command line mode (pygmentize) was\n  enhanced. You shouldn't get UnicodeErrors from it anymore if you\n  don't give an encoding option.\n\n- Added a ``-P`` option to the command line mode which can be used to\n  give options whose values contain commas or equals signs.\n\n- Added 256-color terminal formatter.\n\n- Added an experimental SVG formatter.\n\n- Added the ``lineanchors`` option to the HTML formatter, thanks to\n  Ian Charnas for the idea.\n\n- Gave the line numbers table a CSS class in the HTML formatter.\n\n- Added a Vim 7-like style.\n\n\nVersion 0.8.1\n-------------\n(released Jun 27, 2007)\n\n- Fixed POD highlighting in the Ruby lexer.\n\n- Fixed Unicode class and namespace name highlighting in the C# lexer.\n\n- Fixed Unicode string prefix highlighting in the Python lexer.\n\n- Fixed a bug in the D and MiniD lexers.\n\n- Fixed the included MoinMoin parser.\n\n\nVersion 0.8\n-----------\n(codename Maik\u00e4fer, released May 30, 2007)\n\n- Lexers added:\n\n  * Haskell, thanks to Adam Blinkinsop\n  * Redcode, thanks to Adam Blinkinsop\n  * D, thanks to Kirk McDonald\n  * MuPad, thanks to Christopher Creutzig\n  * MiniD, thanks to Jarrett Billingsley\n  * Vim Script, by Tim Hatch\n\n- The HTML formatter now has a second line-numbers mode in which it\n  will just integrate the numbers in the same ``<pre>`` tag as the\n  code.\n\n- The `CSharpLexer` now is Unicode-aware, which means that it has an\n  option that can be set so that it correctly lexes Unicode\n  identifiers allowed by the C# specs.\n\n- Added a `RaiseOnErrorTokenFilter` that raises an exception when the\n  lexer generates an error token, and a `VisibleWhitespaceFilter` that\n  converts whitespace (spaces, tabs, newlines) into visible\n  characters.\n\n- Fixed the `do_insertions()` helper function to yield correct\n  indices.\n\n- The ReST lexer now automatically highlights source code blocks in\n  \".. sourcecode:: language\" and \".. code:: language\" directive\n  blocks.\n\n- Improved the default style (thanks to Tiberius Teng). The old\n  default is still available as the \"emacs\" style (which was an alias\n  before).\n\n- The `get_style_defs` method of HTML formatters now uses the\n  `cssclass` option as the default selector if it was given.\n\n- Improved the ReST and Bash lexers a bit.\n\n- Fixed a few bugs in the Makefile and Bash lexers, thanks to Tim\n  Hatch.\n\n- Fixed a bug in the command line code that disallowed ``-O`` options\n  when using the ``-S`` option.\n\n- Fixed a bug in the `RawTokenFormatter`.\n\n\nVersion 0.7.1\n-------------\n(released Feb 15, 2007)\n\n- Fixed little highlighting bugs in the Python, Java, Scheme and\n  Apache Config lexers.\n\n- Updated the included manpage.\n\n- Included a built version of the documentation in the source tarball.\n\n\nVersion 0.7\n-----------\n(codename Faschingskrapfn, released Feb 14, 2007)\n\n- Added a MoinMoin parser that uses Pygments. With it, you get\n  Pygments highlighting in Moin Wiki pages.\n\n- Changed the exception raised if no suitable lexer, formatter etc. is\n  found in one of the `get_*_by_*` functions to a custom exception,\n  `pygments.util.ClassNotFound`. It is, however, a subclass of\n  `ValueError` in order to retain backwards compatibility.\n\n- Added a `-H` command line option which can be used to get the\n  docstring of a lexer, formatter or filter.\n\n- Made the handling of lexers and formatters more consistent. The\n  aliases and filename patterns of formatters are now attributes on\n  them.\n\n- Added an OCaml lexer, thanks to Adam Blinkinsop.\n\n- Made the HTML formatter more flexible, and easily subclassable in\n  order to make it easy to implement custom wrappers, e.g. alternate\n  line number markup. See the documentation.\n\n- Added an `outencoding` option to all formatters, making it possible\n  to override the `encoding` (which is used by lexers and formatters)\n  when using the command line interface. Also, if using the terminal\n  formatter and the output file is a terminal and has an encoding\n  attribute, use it if no encoding is given.\n\n- Made it possible to just drop style modules into the `styles`\n  subpackage of the Pygments installation.\n\n- Added a \"state\" keyword argument to the `using` helper.\n\n- Added a `commandprefix` option to the `LatexFormatter` which allows\n  to control how the command names are constructed.\n\n- Added quite a few new lexers, thanks to Tim Hatch:\n\n  * Java Server Pages\n  * Windows batch files\n  * Trac Wiki markup\n  * Python tracebacks\n  * ReStructuredText\n  * Dylan\n  * and the Befunge esoteric programming language (yay!)\n\n- Added Mako lexers by Ben Bangert.\n\n- Added \"fruity\" style, another dark background originally vim-based\n  theme.\n\n- Added sources.list lexer by Dennis Kaarsemaker.\n\n- Added token stream filters, and a pygmentize option to use them.\n\n- Changed behavior of `in` Operator for tokens.\n\n- Added mimetypes for all lexers.\n\n- Fixed some problems lexing Python strings.\n\n- Fixed tickets: #167, #178, #179, #180, #185, #201.\n\n\nVersion 0.6\n-----------\n(codename Zimtstern, released Dec 20, 2006)\n\n- Added option for the HTML formatter to write the CSS to an external\n  file in \"full document\" mode.\n\n- Added RTF formatter.\n\n- Added Bash and Apache configuration lexers (thanks to Tim Hatch).\n\n- Improved guessing methods for various lexers.\n\n- Added `@media` support to CSS lexer (thanks to Tim Hatch).\n\n- Added a Groff lexer (thanks to Tim Hatch).\n\n- License change to BSD.\n\n- Added lexers for the Myghty template language.\n\n- Added a Scheme lexer (thanks to Marek Kubica).\n\n- Added some functions to iterate over existing lexers, formatters and\n  lexers.\n\n- The HtmlFormatter's `get_style_defs()` can now take a list as an\n  argument to generate CSS with multiple prefixes.\n\n- Support for guessing input encoding added.\n\n- Encoding support added: all processing is now done with Unicode\n  strings, input and output are converted from and optionally to byte\n  strings (see the ``encoding`` option of lexers and formatters).\n\n- Some improvements in the C(++) lexers handling comments and line\n  continuations.\n\n\nVersion 0.5.1\n-------------\n(released Oct 30, 2006)\n\n- Fixed traceback in ``pygmentize -L`` (thanks to Piotr Ozarowski).\n\n\nVersion 0.5\n-----------\n(codename PyKleur, released Oct 30, 2006)\n\n- Initial public release.\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.archetype\n    ~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexer for Archetype-related syntaxes, including:\n\n    - ODIN syntax <https://github.com/openEHR/odin>\n    - ADL syntax <http://www.openehr.org/releases/trunk/architecture/am/adl2.pdf>\n    - cADL sub-syntax of ADL\n\n    For uses of this syntax, see the openEHR archetypes <http://www.openEHR.org/ckm>\n\n    Contributed by Thomas Beale <https://github.com/wolandscat>,\n    <https://bitbucket.org/thomas_beale>.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pygments.lexer import RegexLexer, include, bygroups, using, default\nfrom pygments.token import Text, Comment, Name, Literal, Number, String, \\\n    Punctuation, Keyword, Operator, Generic\n\n__all__ = ['OdinLexer', 'CadlLexer', 'AdlLexer']\n\n\nclass AtomsLexer(RegexLexer):\n    \"\"\"\n    Lexer for Values used in ADL and ODIN.\n\n    .. versionadded:: 2.1\n    \"\"\"\n\n    tokens = {\n        # ----- pseudo-states for inclusion -----\n        'whitespace': [\n            (r'\\n', Text),\n            (r'\\s+', Text),\n            (r'[ \\t]*--.*$', Comment),\n        ],\n        'archetype_id': [\n            (r'[ \\t]*([a-zA-Z]\\w+(\\.[a-zA-Z]\\w+)*::)?[a-zA-Z]\\w+(-[a-zA-Z]\\w+){2}'\n             r'\\.\\w+[\\w-]*\\.v\\d+(\\.\\d+){,2}((-[a-z]+)(\\.\\d+)?)?', Name.Decorator),\n        ],\n        'date_constraints': [\n            # ISO 8601-based date/time constraints\n            (r'[Xx?YyMmDdHhSs\\d]{2,4}([:-][Xx?YyMmDdHhSs\\d]{2}){2}', Literal.Date),\n            # ISO 8601-based duration constraints + optional trailing slash\n            (r'(P[YyMmWwDd]+(T[HhMmSs]+)?|PT[HhMmSs]+)/?', Literal.Date),\n        ],\n        'ordered_values': [\n            # ISO 8601 date with optional 'T' ligature\n            (r'\\d{4}-\\d{2}-\\d{2}T?', Literal.Date),\n            # ISO 8601 time\n            (r'\\d{2}:\\d{2}:\\d{2}(\\.\\d+)?([+-]\\d{4}|Z)?', Literal.Date),\n            # ISO 8601 duration\n            (r'P((\\d*(\\.\\d+)?[YyMmWwDd]){1,3}(T(\\d*(\\.\\d+)?[HhMmSs]){,3})?|'\n             r'T(\\d*(\\.\\d+)?[HhMmSs]){,3})', Literal.Date),\n            (r'[+-]?(\\d+\\.\\d*|\\.\\d+|\\d+)[eE][+-]?\\d+', Number.Float),\n            (r'[+-]?(\\d+)*\\.\\d+%?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[+-]?\\d+%?', Number.Integer),\n        ],\n        'values': [\n            include('ordered_values'),\n            (r'([Tt]rue|[Ff]alse)', Literal),\n            (r'\"', String, 'string'),\n            (r\"'(\\\\.|\\\\[0-7]{1,3}|\\\\x[a-fA-F0-9]{1,2}|[^\\\\\\'\\n])'\", String.Char),\n            (r'[a-z][a-z0-9+.-]*:', Literal, 'uri'),\n            # term code\n            (r'(\\[)(\\w[\\w-]*(?:\\([^)\\n]+\\))?)(::)(\\w[\\w-]*)(\\])',\n             bygroups(Punctuation, Name.Decorator, Punctuation, Name.Decorator,\n                      Punctuation)),\n            (r'\\|', Punctuation, 'interval'),\n            # list continuation\n            (r'\\.\\.\\.', Punctuation),\n        ],\n        'constraint_values': [\n            (r'(\\[)(\\w[\\w-]*(?:\\([^)\\n]+\\))?)(::)',\n             bygroups(Punctuation, Name.Decorator, Punctuation), 'adl14_code_constraint'),\n            # ADL 1.4 ordinal constraint\n            (r'(\\d*)(\\|)(\\[\\w[\\w-]*::\\w[\\w-]*\\])((?:[,;])?)',\n             bygroups(Number, Punctuation, Name.Decorator, Punctuation)),\n            include('date_constraints'),\n            include('values'),\n        ],\n\n        # ----- real states -----\n        'string': [\n            ('\"', String, '#pop'),\n            (r'\\\\([\\\\abfnrtv\"\\']|x[a-fA-F0-9]{2,4}|'\n             r'u[a-fA-F0-9]{4}|U[a-fA-F0-9]{8}|[0-7]{1,3})', String.Escape),\n            # all other characters\n            (r'[^\\\\\"]+', String),\n            # stray backslash\n            (r'\\\\', String),\n        ],\n        'uri': [\n            # effective URI terminators\n            (r'[,>\\s]', Punctuation, '#pop'),\n            (r'[^>\\s,]+', Literal),\n        ],\n        'interval': [\n            (r'\\|', Punctuation, '#pop'),\n            include('ordered_values'),\n            (r'\\.\\.', Punctuation),\n            (r'[<>=] *', Punctuation),\n            # handle +/-\n            (r'\\+/-', Punctuation),\n            (r'\\s+', Text),\n        ],\n        'any_code': [\n            include('archetype_id'),\n            # if it is a code\n            (r'[a-z_]\\w*[0-9.]+(@[^\\]]+)?', Name.Decorator),\n            # if it is tuple with attribute names\n            (r'[a-z_]\\w*', Name.Class),\n            # if it is an integer, i.e. Xpath child index\n            (r'[0-9]+', Text),\n            (r'\\|', Punctuation, 'code_rubric'),\n            (r'\\]', Punctuation, '#pop'),\n            # handle use_archetype statement\n            (r'\\s*,\\s*', Punctuation),\n        ],\n        'code_rubric': [\n            (r'\\|', Punctuation, '#pop'),\n            (r'[^|]+', String),\n        ],\n        'adl14_code_constraint': [\n            (r'\\]', Punctuation, '#pop'),\n            (r'\\|', Punctuation, 'code_rubric'),\n            (r'(\\w[\\w-]*)([;,]?)', bygroups(Name.Decorator, Punctuation)),\n            include('whitespace'),\n        ],\n    }\n\n\nclass OdinLexer(AtomsLexer):\n    \"\"\"\n    Lexer for ODIN syntax.\n\n    .. versionadded:: 2.1\n    \"\"\"\n    name = 'ODIN'\n    aliases = ['odin']\n    filenames = ['*.odin']\n    mimetypes = ['text/odin']\n\n    tokens = {\n        'path': [\n            (r'>', Punctuation, '#pop'),\n            # attribute name\n            (r'[a-z_]\\w*', Name.Class),\n            (r'/', Punctuation),\n            (r'\\[', Punctuation, 'key'),\n            (r'\\s*,\\s*', Punctuation, '#pop'),\n            (r'\\s+', Text, '#pop'),\n        ],\n        'key': [\n            include('values'),\n            (r'\\]', Punctuation, '#pop'),\n        ],\n        'type_cast': [\n            (r'\\)', Punctuation, '#pop'),\n            (r'[^)]+',  Name.Class),\n        ],\n        'root': [\n            include('whitespace'),\n            (r'([Tt]rue|[Ff]alse)', Literal),\n            include('values'),\n            # x-ref path\n            (r'/', Punctuation, 'path'),\n            # x-ref path starting with key\n            (r'\\[', Punctuation, 'key'),\n            # attribute name\n            (r'[a-z_]\\w*', Name.Class),\n            (r'=', Operator),\n            (r'\\(', Punctuation, 'type_cast'),\n            (r',', Punctuation),\n            (r'<', Punctuation),\n            (r'>', Punctuation),\n            (r';', Punctuation),\n        ],\n    }\n\n\nclass CadlLexer(AtomsLexer):\n    \"\"\"\n    Lexer for cADL syntax.\n\n    .. versionadded:: 2.1\n    \"\"\"\n    name = 'cADL'\n    aliases = ['cadl']\n    filenames = ['*.cadl']\n\n    tokens = {\n        'path': [\n            # attribute name\n            (r'[a-z_]\\w*', Name.Class),\n            (r'/', Punctuation),\n            (r'\\[', Punctuation, 'any_code'),\n            (r'\\s+', Punctuation, '#pop'),\n        ],\n        'root': [\n            include('whitespace'),\n            (r'(cardinality|existence|occurrences|group|include|exclude|'\n             r'allow_archetype|use_archetype|use_node)\\W', Keyword.Type),\n            (r'(and|or|not|there_exists|xor|implies|for_all)\\W', Keyword.Type),\n            (r'(after|before|closed)\\W', Keyword.Type),\n            (r'(not)\\W', Operator),\n            (r'(matches|is_in)\\W', Operator),\n            # is_in / not is_in char\n            ('(\\u2208|\\u2209)', Operator),\n            # there_exists / not there_exists / for_all / and / or\n            ('(\\u2203|\\u2204|\\u2200|\\u2227|\\u2228|\\u22BB|\\223C)',\n             Operator),\n            # regex in slot or as string constraint\n            (r'(\\{)(\\s*/[^}]+/\\s*)(\\})',\n             bygroups(Punctuation, String.Regex, Punctuation)),\n            # regex in slot or as string constraint\n            (r'(\\{)(\\s*\\^[^}]+\\^\\s*)(\\})',\n             bygroups(Punctuation, String.Regex, Punctuation)),\n            (r'/', Punctuation, 'path'),\n            # for cardinality etc\n            (r'(\\{)((?:\\d+\\.\\.)?(?:\\d+|\\*))'\n             r'((?:\\s*;\\s*(?:ordered|unordered|unique)){,2})(\\})',\n             bygroups(Punctuation, Number, Number, Punctuation)),\n            # [{ is start of a tuple value\n            (r'\\[\\{', Punctuation),\n            (r'\\}\\]', Punctuation),\n            (r'\\{', Punctuation),\n            (r'\\}', Punctuation),\n            include('constraint_values'),\n            # type name\n            (r'[A-Z]\\w+(<[A-Z]\\w+([A-Za-z_<>]*)>)?',  Name.Class),\n            # attribute name\n            (r'[a-z_]\\w*', Name.Class),\n            (r'\\[', Punctuation, 'any_code'),\n            (r'(~|//|\\\\\\\\|\\+|-|/|\\*|\\^|!=|=|<=|>=|<|>]?)', Operator),\n            (r'\\(', Punctuation),\n            (r'\\)', Punctuation),\n            # for lists of values\n            (r',', Punctuation),\n            (r'\"', String, 'string'),\n            # for assumed value\n            (r';', Punctuation),\n        ],\n    }\n\n\nclass AdlLexer(AtomsLexer):\n    \"\"\"\n    Lexer for ADL syntax.\n\n    .. versionadded:: 2.1\n    \"\"\"\n\n    name = 'ADL'\n    aliases = ['adl']\n    filenames = ['*.adl', '*.adls', '*.adlf', '*.adlx']\n\n    tokens = {\n        'whitespace': [\n            # blank line ends\n            (r'\\s*\\n', Text),\n            # comment-only line\n            (r'^[ \\t]*--.*$', Comment),\n        ],\n        'odin_section': [\n            # repeating the following two rules from the root state enable multi-line\n            # strings that start in the first column to be dealt with\n            (r'^(language|description|ontology|terminology|annotations|'\n             r'component_terminologies|revision_history)[ \\t]*\\n', Generic.Heading),\n            (r'^(definition)[ \\t]*\\n', Generic.Heading, 'cadl_section'),\n            (r'^([ \\t]*|[ \\t]+.*)\\n', using(OdinLexer)),\n            (r'^([^\"]*\")(>[ \\t]*\\n)', bygroups(String, Punctuation)),\n            # template overlay delimiter\n            (r'^----------*\\n', Text, '#pop'),\n            (r'^.*\\n', String),\n            default('#pop'),\n        ],\n        'cadl_section': [\n            (r'^([ \\t]*|[ \\t]+.*)\\n', using(CadlLexer)),\n            default('#pop'),\n        ],\n        'rules_section': [\n            (r'^[ \\t]+.*\\n', using(CadlLexer)),\n            default('#pop'),\n        ],\n        'metadata': [\n            (r'\\)', Punctuation, '#pop'),\n            (r';', Punctuation),\n            (r'([Tt]rue|[Ff]alse)', Literal),\n            # numbers and version ids\n            (r'\\d+(\\.\\d+)*', Literal),\n            # Guids\n            (r'(\\d|[a-fA-F])+(-(\\d|[a-fA-F])+){3,}', Literal),\n            (r'\\w+', Name.Class),\n            (r'\"', String, 'string'),\n            (r'=', Operator),\n            (r'[ \\t]+', Text),\n            default('#pop'),\n        ],\n        'root': [\n            (r'^(archetype|template_overlay|operational_template|template|'\n             r'speciali[sz]e)', Generic.Heading),\n            (r'^(language|description|ontology|terminology|annotations|'\n             r'component_terminologies|revision_history)[ \\t]*\\n',\n             Generic.Heading, 'odin_section'),\n            (r'^(definition)[ \\t]*\\n', Generic.Heading, 'cadl_section'),\n            (r'^(rules)[ \\t]*\\n', Generic.Heading, 'rules_section'),\n            include('archetype_id'),\n            (r'[ \\t]*\\(', Punctuation, 'metadata'),\n            include('whitespace'),\n        ],\n    }\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.factor\n    ~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for the Factor language.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexer import RegexLexer, bygroups, default, words\nfrom pygments.token import Text, Comment, Keyword, Name, String, Number\n\n__all__ = ['FactorLexer']\n\n\nclass FactorLexer(RegexLexer):\n    \"\"\"\n    Lexer for the `Factor <http://factorcode.org>`_ language.\n\n    .. versionadded:: 1.4\n    \"\"\"\n    name = 'Factor'\n    aliases = ['factor']\n    filenames = ['*.factor']\n    mimetypes = ['text/x-factor']\n\n    flags = re.MULTILINE | re.UNICODE\n\n    builtin_kernel = words((\n        '-rot', '2bi', '2bi@', '2bi*', '2curry', '2dip', '2drop', '2dup', '2keep', '2nip',\n        '2over', '2tri', '2tri@', '2tri*', '3bi', '3curry', '3dip', '3drop', '3dup', '3keep',\n        '3tri', '4dip', '4drop', '4dup', '4keep', '<wrapper>', '=', '>boolean', 'clone',\n        '?', '?execute', '?if', 'and', 'assert', 'assert=', 'assert?', 'bi', 'bi-curry',\n        'bi-curry@', 'bi-curry*', 'bi@', 'bi*', 'boa', 'boolean', 'boolean?', 'both?',\n        'build', 'call', 'callstack', 'callstack>array', 'callstack?', 'clear', '(clone)',\n        'compose', 'compose?', 'curry', 'curry?', 'datastack', 'die', 'dip', 'do', 'drop',\n        'dup', 'dupd', 'either?', 'eq?', 'equal?', 'execute', 'hashcode', 'hashcode*',\n        'identity-hashcode', 'identity-tuple', 'identity-tuple?', 'if', 'if*',\n        'keep', 'loop', 'most', 'new', 'nip', 'not', 'null', 'object', 'or', 'over',\n        'pick', 'prepose', 'retainstack', 'rot', 'same?', 'swap', 'swapd', 'throw',\n        'tri', 'tri-curry', 'tri-curry@', 'tri-curry*', 'tri@', 'tri*', 'tuple',\n        'tuple?', 'unless', 'unless*', 'until', 'when', 'when*', 'while', 'with',\n        'wrapper', 'wrapper?', 'xor'), suffix=r'\\s')\n\n    builtin_assocs = words((\n        '2cache', '<enum>', '>alist', '?at', '?of', 'assoc', 'assoc-all?',\n        'assoc-any?', 'assoc-clone-like', 'assoc-combine', 'assoc-diff',\n        'assoc-diff!', 'assoc-differ', 'assoc-each', 'assoc-empty?',\n        'assoc-filter', 'assoc-filter!', 'assoc-filter-as', 'assoc-find',\n        'assoc-hashcode', 'assoc-intersect', 'assoc-like', 'assoc-map',\n        'assoc-map-as', 'assoc-partition', 'assoc-refine', 'assoc-size',\n        'assoc-stack', 'assoc-subset?', 'assoc-union', 'assoc-union!',\n        'assoc=', 'assoc>map', 'assoc?', 'at', 'at+', 'at*', 'cache', 'change-at',\n        'clear-assoc', 'delete-at', 'delete-at*', 'enum', 'enum?', 'extract-keys',\n        'inc-at', 'key?', 'keys', 'map>assoc', 'maybe-set-at', 'new-assoc', 'of',\n        'push-at', 'rename-at', 'set-at', 'sift-keys', 'sift-values', 'substitute',\n        'unzip', 'value-at', 'value-at*', 'value?', 'values', 'zip'), suffix=r'\\s')\n\n    builtin_combinators = words((\n        '2cleave', '2cleave>quot', '3cleave', '3cleave>quot', '4cleave',\n        '4cleave>quot', 'alist>quot', 'call-effect', 'case', 'case-find',\n        'case>quot', 'cleave', 'cleave>quot', 'cond', 'cond>quot', 'deep-spread>quot',\n        'execute-effect', 'linear-case-quot', 'no-case', 'no-case?', 'no-cond',\n        'no-cond?', 'recursive-hashcode', 'shallow-spread>quot', 'spread',\n        'to-fixed-point', 'wrong-values', 'wrong-values?'), suffix=r'\\s')\n\n    builtin_math = words((\n        '-', '/', '/f', '/i', '/mod', '2/', '2^', '<', '<=', '<fp-nan>', '>',\n        '>=', '>bignum', '>fixnum', '>float', '>integer', '(all-integers?)',\n        '(each-integer)', '(find-integer)', '*', '+', '?1+',\n        'abs', 'align', 'all-integers?', 'bignum', 'bignum?', 'bit?', 'bitand',\n        'bitnot', 'bitor', 'bits>double', 'bits>float', 'bitxor', 'complex',\n        'complex?', 'denominator', 'double>bits', 'each-integer', 'even?',\n        'find-integer', 'find-last-integer', 'fixnum', 'fixnum?', 'float',\n        'float>bits', 'float?', 'fp-bitwise=', 'fp-infinity?', 'fp-nan-payload',\n        'fp-nan?', 'fp-qnan?', 'fp-sign', 'fp-snan?', 'fp-special?',\n        'if-zero', 'imaginary-part', 'integer', 'integer>fixnum',\n        'integer>fixnum-strict', 'integer?', 'log2', 'log2-expects-positive',\n        'log2-expects-positive?', 'mod', 'neg', 'neg?', 'next-float',\n        'next-power-of-2', 'number', 'number=', 'number?', 'numerator', 'odd?',\n        'out-of-fixnum-range', 'out-of-fixnum-range?', 'power-of-2?',\n        'prev-float', 'ratio', 'ratio?', 'rational', 'rational?', 'real',\n        'real-part', 'real?', 'recip', 'rem', 'sgn', 'shift', 'sq', 'times',\n        'u<', 'u<=', 'u>', 'u>=', 'unless-zero', 'unordered?', 'when-zero',\n        'zero?'), suffix=r'\\s')\n\n    builtin_sequences = words((\n        '1sequence', '2all?', '2each', '2map', '2map-as', '2map-reduce', '2reduce',\n        '2selector', '2sequence', '3append', '3append-as', '3each', '3map', '3map-as',\n        '3sequence', '4sequence', '<repetition>', '<reversed>', '<slice>', '?first',\n        '?last', '?nth', '?second', '?set-nth', 'accumulate', 'accumulate!',\n        'accumulate-as', 'all?', 'any?', 'append', 'append!', 'append-as',\n        'assert-sequence', 'assert-sequence=', 'assert-sequence?',\n        'binary-reduce', 'bounds-check', 'bounds-check?', 'bounds-error',\n        'bounds-error?', 'but-last', 'but-last-slice', 'cartesian-each',\n        'cartesian-map', 'cartesian-product', 'change-nth', 'check-slice',\n        'check-slice-error', 'clone-like', 'collapse-slice', 'collector',\n        'collector-for', 'concat', 'concat-as', 'copy', 'count', 'cut', 'cut-slice',\n        'cut*', 'delete-all', 'delete-slice', 'drop-prefix', 'each', 'each-from',\n        'each-index', 'empty?', 'exchange', 'filter', 'filter!', 'filter-as', 'find',\n        'find-from', 'find-index', 'find-index-from', 'find-last', 'find-last-from',\n        'first', 'first2', 'first3', 'first4', 'flip', 'follow', 'fourth', 'glue', 'halves',\n        'harvest', 'head', 'head-slice', 'head-slice*', 'head*', 'head?',\n        'if-empty', 'immutable', 'immutable-sequence', 'immutable-sequence?',\n        'immutable?', 'index', 'index-from', 'indices', 'infimum', 'infimum-by',\n        'insert-nth', 'interleave', 'iota', 'iota-tuple', 'iota-tuple?', 'join',\n        'join-as', 'last', 'last-index', 'last-index-from', 'length', 'lengthen',\n        'like', 'longer', 'longer?', 'longest', 'map', 'map!', 'map-as', 'map-find',\n        'map-find-last', 'map-index', 'map-integers', 'map-reduce', 'map-sum',\n        'max-length', 'member-eq?', 'member?', 'midpoint@', 'min-length',\n        'mismatch', 'move', 'new-like', 'new-resizable', 'new-sequence',\n        'non-negative-integer-expected', 'non-negative-integer-expected?',\n        'nth', 'nths', 'pad-head', 'pad-tail', 'padding', 'partition', 'pop', 'pop*',\n        'prefix', 'prepend', 'prepend-as', 'produce', 'produce-as', 'product', 'push',\n        'push-all', 'push-either', 'push-if', 'reduce', 'reduce-index', 'remove',\n        'remove!', 'remove-eq', 'remove-eq!', 'remove-nth', 'remove-nth!', 'repetition',\n        'repetition?', 'replace-slice', 'replicate', 'replicate-as', 'rest',\n        'rest-slice', 'reverse', 'reverse!', 'reversed', 'reversed?', 'second',\n        'selector', 'selector-for', 'sequence', 'sequence-hashcode', 'sequence=',\n        'sequence?', 'set-first', 'set-fourth', 'set-last', 'set-length', 'set-nth',\n        'set-second', 'set-third', 'short', 'shorten', 'shorter', 'shorter?',\n        'shortest', 'sift', 'slice', 'slice-error', 'slice-error?', 'slice?',\n        'snip', 'snip-slice', 'start', 'start*', 'subseq', 'subseq?', 'suffix',\n        'suffix!', 'sum', 'sum-lengths', 'supremum', 'supremum-by', 'surround', 'tail',\n        'tail-slice', 'tail-slice*', 'tail*', 'tail?', 'third', 'trim',\n        'trim-head', 'trim-head-slice', 'trim-slice', 'trim-tail', 'trim-tail-slice',\n        'unclip', 'unclip-last', 'unclip-last-slice', 'unclip-slice', 'unless-empty',\n        'virtual-exemplar', 'virtual-sequence', 'virtual-sequence?', 'virtual@',\n        'when-empty'), suffix=r'\\s')\n\n    builtin_namespaces = words((\n        '+@', 'change', 'change-global', 'counter', 'dec', 'get', 'get-global',\n        'global', 'inc', 'init-namespaces', 'initialize', 'is-global', 'make-assoc',\n        'namespace', 'namestack', 'off', 'on', 'set', 'set-global', 'set-namestack',\n        'toggle', 'with-global', 'with-scope', 'with-variable', 'with-variables'),\n        suffix=r'\\s')\n\n    builtin_arrays = words((\n        '1array', '2array', '3array', '4array', '<array>', '>array', 'array',\n        'array?', 'pair', 'pair?', 'resize-array'), suffix=r'\\s')\n\n    builtin_io = words((\n        '(each-stream-block-slice)', '(each-stream-block)',\n        '(stream-contents-by-block)', '(stream-contents-by-element)',\n        '(stream-contents-by-length-or-block)',\n        '(stream-contents-by-length)', '+byte+', '+character+',\n        'bad-seek-type', 'bad-seek-type?', 'bl', 'contents', 'each-block',\n        'each-block-size', 'each-block-slice', 'each-line', 'each-morsel',\n        'each-stream-block', 'each-stream-block-slice', 'each-stream-line',\n        'error-stream', 'flush', 'input-stream', 'input-stream?',\n        'invalid-read-buffer', 'invalid-read-buffer?', 'lines', 'nl',\n        'output-stream', 'output-stream?', 'print', 'read', 'read-into',\n        'read-partial', 'read-partial-into', 'read-until', 'read1', 'readln',\n        'seek-absolute', 'seek-absolute?', 'seek-end', 'seek-end?',\n        'seek-input', 'seek-output', 'seek-relative', 'seek-relative?',\n        'stream-bl', 'stream-contents', 'stream-contents*', 'stream-copy',\n        'stream-copy*', 'stream-element-type', 'stream-flush',\n        'stream-length', 'stream-lines', 'stream-nl', 'stream-print',\n        'stream-read', 'stream-read-into', 'stream-read-partial',\n        'stream-read-partial-into', 'stream-read-partial-unsafe',\n        'stream-read-unsafe', 'stream-read-until', 'stream-read1',\n        'stream-readln', 'stream-seek', 'stream-seekable?', 'stream-tell',\n        'stream-write', 'stream-write1', 'tell-input', 'tell-output',\n        'with-error-stream', 'with-error-stream*', 'with-error>output',\n        'with-input-output+error-streams',\n        'with-input-output+error-streams*', 'with-input-stream',\n        'with-input-stream*', 'with-output-stream', 'with-output-stream*',\n        'with-output>error', 'with-output+error-stream',\n        'with-output+error-stream*', 'with-streams', 'with-streams*',\n        'write', 'write1'), suffix=r'\\s')\n\n    builtin_strings = words((\n        '1string', '<string>', '>string', 'resize-string', 'string',\n        'string?'), suffix=r'\\s')\n\n    builtin_vectors = words((\n        '1vector', '<vector>', '>vector', '?push', 'vector', 'vector?'),\n        suffix=r'\\s')\n\n    builtin_continuations = words((\n        '<condition>', '<continuation>', '<restart>', 'attempt-all',\n        'attempt-all-error', 'attempt-all-error?', 'callback-error-hook',\n        'callcc0', 'callcc1', 'cleanup', 'compute-restarts', 'condition',\n        'condition?', 'continuation', 'continuation?', 'continue',\n        'continue-restart', 'continue-with', 'current-continuation',\n        'error', 'error-continuation', 'error-in-thread', 'error-thread',\n        'ifcc', 'ignore-errors', 'in-callback?', 'original-error', 'recover',\n        'restart', 'restart?', 'restarts', 'rethrow', 'rethrow-restarts',\n        'return', 'return-continuation', 'thread-error-hook', 'throw-continue',\n        'throw-restarts', 'with-datastack', 'with-return'), suffix=r'\\s')\n\n    tokens = {\n        'root': [\n            # factor allows a file to start with a shebang\n            (r'#!.*$', Comment.Preproc),\n            default('base'),\n        ],\n        'base': [\n            (r'\\s+', Text),\n\n            # defining words\n            (r'((?:MACRO|MEMO|TYPED)?:[:]?)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function)),\n            (r'(M:[:]?)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class, Text, Name.Function)),\n            (r'(C:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function, Text, Name.Class)),\n            (r'(GENERIC:)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function)),\n            (r'(HOOK:|GENERIC#)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function, Text, Name.Function)),\n            (r'\\(\\s', Name.Function, 'stackeffect'),\n            (r';\\s', Keyword),\n\n            # imports and namespaces\n            (r'(USING:)(\\s+)',\n             bygroups(Keyword.Namespace, Text), 'vocabs'),\n            (r'(USE:|UNUSE:|IN:|QUALIFIED:)(\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Namespace)),\n            (r'(QUALIFIED-WITH:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Namespace, Text, Name.Namespace)),\n            (r'(FROM:|EXCLUDE:)(\\s+)(\\S+)(\\s+=>\\s)',\n             bygroups(Keyword.Namespace, Text, Name.Namespace, Text), 'words'),\n            (r'(RENAME:)(\\s+)(\\S+)(\\s+)(\\S+)(\\s+=>\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Function, Text, Name.Namespace, Text, Name.Function)),\n            (r'(ALIAS:|TYPEDEF:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Function, Text, Name.Function)),\n            (r'(DEFER:|FORGET:|POSTPONE:)(\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Function)),\n\n            # tuples and classes\n            (r'(TUPLE:|ERROR:)(\\s+)(\\S+)(\\s+<\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class, Text, Name.Class), 'slots'),\n            (r'(TUPLE:|ERROR:|BUILTIN:)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class), 'slots'),\n            (r'(MIXIN:|UNION:|INTERSECTION:)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class)),\n            (r'(PREDICATE:)(\\s+)(\\S+)(\\s+<\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class, Text, Name.Class)),\n            (r'(C:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function, Text, Name.Class)),\n            (r'(INSTANCE:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class, Text, Name.Class)),\n            (r'(SLOT:)(\\s+)(\\S+)', bygroups(Keyword, Text, Name.Function)),\n            (r'(SINGLETON:)(\\s+)(\\S+)', bygroups(Keyword, Text, Name.Class)),\n            (r'SINGLETONS:', Keyword, 'classes'),\n\n            # other syntax\n            (r'(CONSTANT:|SYMBOL:|MAIN:|HELP:)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function)),\n            (r'SYMBOLS:\\s', Keyword, 'words'),\n            (r'SYNTAX:\\s', Keyword),\n            (r'ALIEN:\\s', Keyword),\n            (r'(STRUCT:)(\\s+)(\\S+)', bygroups(Keyword, Text, Name.Class)),\n            (r'(FUNCTION:)(\\s+\\S+\\s+)(\\S+)(\\s+\\(\\s+[^)]+\\)\\s)',\n             bygroups(Keyword.Namespace, Text, Name.Function, Text)),\n            (r'(FUNCTION-ALIAS:)(\\s+)(\\S+)(\\s+\\S+\\s+)(\\S+)(\\s+\\(\\s+[^)]+\\)\\s)',\n             bygroups(Keyword.Namespace, Text, Name.Function, Text, Name.Function, Text)),\n\n            # vocab.private\n            (r'(?:<PRIVATE|PRIVATE>)\\s', Keyword.Namespace),\n\n            # strings\n            (r'\"\"\"\\s+(?:.|\\n)*?\\s+\"\"\"', String),\n            (r'\"(?:\\\\\\\\|\\\\\"|[^\"])*\"', String),\n            (r'\\S+\"\\s+(?:\\\\\\\\|\\\\\"|[^\"])*\"', String),\n            (r'CHAR:\\s+(?:\\\\[\\\\abfnrstv]|[^\\\\]\\S*)\\s', String.Char),\n\n            # comments\n            (r'!\\s+.*$', Comment),\n            (r'#!\\s+.*$', Comment),\n            (r'/\\*\\s+(?:.|\\n)*?\\s\\*/\\s', Comment),\n\n            # boolean constants\n            (r'[tf]\\s', Name.Constant),\n\n            # symbols and literals\n            (r'[\\\\$]\\s+\\S+', Name.Constant),\n            (r'M\\\\\\s+\\S+\\s+\\S+', Name.Constant),\n\n            # numbers\n            (r'[+-]?(?:[\\d,]*\\d)?\\.(?:\\d([\\d,]*\\d)?)?(?:[eE][+-]?\\d+)?\\s', Number),\n            (r'[+-]?\\d(?:[\\d,]*\\d)?(?:[eE][+-]?\\d+)?\\s', Number),\n            (r'0x[a-fA-F\\d](?:[a-fA-F\\d,]*[a-fA-F\\d])?(?:p\\d([\\d,]*\\d)?)?\\s', Number),\n            (r'NAN:\\s+[a-fA-F\\d](?:[a-fA-F\\d,]*[a-fA-F\\d])?(?:p\\d([\\d,]*\\d)?)?\\s', Number),\n            (r'0b[01]+\\s', Number.Bin),\n            (r'0o[0-7]+\\s', Number.Oct),\n            (r'(?:\\d([\\d,]*\\d)?)?\\+\\d(?:[\\d,]*\\d)?/\\d(?:[\\d,]*\\d)?\\s', Number),\n            (r'(?:\\-\\d([\\d,]*\\d)?)?\\-\\d(?:[\\d,]*\\d)?/\\d(?:[\\d,]*\\d)?\\s', Number),\n\n            # keywords\n            (r'(?:deprecated|final|foldable|flushable|inline|recursive)\\s',\n             Keyword),\n\n            # builtins\n            (builtin_kernel, Name.Builtin),\n            (builtin_assocs, Name.Builtin),\n            (builtin_combinators, Name.Builtin),\n            (builtin_math, Name.Builtin),\n            (builtin_sequences, Name.Builtin),\n            (builtin_namespaces, Name.Builtin),\n            (builtin_arrays, Name.Builtin),\n            (builtin_io, Name.Builtin),\n            (builtin_strings, Name.Builtin),\n            (builtin_vectors, Name.Builtin),\n            (builtin_continuations, Name.Builtin),\n\n            # everything else is text\n            (r'\\S+', Text),\n        ],\n        'stackeffect': [\n            (r'\\s+', Text),\n            (r'\\(\\s+', Name.Function, 'stackeffect'),\n            (r'\\)\\s', Name.Function, '#pop'),\n            (r'--\\s', Name.Function),\n            (r'\\S+', Name.Variable),\n        ],\n        'slots': [\n            (r'\\s+', Text),\n            (r';\\s', Keyword, '#pop'),\n            (r'(\\{\\s+)(\\S+)(\\s+[^}]+\\s+\\}\\s)',\n             bygroups(Text, Name.Variable, Text)),\n            (r'\\S+', Name.Variable),\n        ],\n        'vocabs': [\n            (r'\\s+', Text),\n            (r';\\s', Keyword, '#pop'),\n            (r'\\S+', Name.Namespace),\n        ],\n        'classes': [\n            (r'\\s+', Text),\n            (r';\\s', Keyword, '#pop'),\n            (r'\\S+', Name.Class),\n        ],\n        'words': [\n            (r'\\s+', Text),\n            (r';\\s', Keyword, '#pop'),\n            (r'\\S+', Name.Function),\n        ],\n    }\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.jvm\n    ~~~~~~~~~~~~~~~~~~~\n\n    Pygments lexers for JVM languages.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexer import Lexer, RegexLexer, include, bygroups, using, \\\n    this, combined, default, words\nfrom pygments.token import Text, Comment, Operator, Keyword, Name, String, \\\n    Number, Punctuation\nfrom pygments.util import shebang_matches\nfrom pygments import unistring as uni\n\n__all__ = ['JavaLexer', 'ScalaLexer', 'GosuLexer', 'GosuTemplateLexer',\n           'GroovyLexer', 'IokeLexer', 'ClojureLexer', 'ClojureScriptLexer',\n           'KotlinLexer', 'XtendLexer', 'AspectJLexer', 'CeylonLexer',\n           'PigLexer', 'GoloLexer', 'JasminLexer', 'SarlLexer']\n\n\nclass JavaLexer(RegexLexer):\n    \"\"\"\n    For `Java <https://www.oracle.com/technetwork/java/>`_ source code.\n    \"\"\"\n\n    name = 'Java'\n    aliases = ['java']\n    filenames = ['*.java']\n    mimetypes = ['text/x-java']\n\n    flags = re.MULTILINE | re.DOTALL | re.UNICODE\n\n    tokens = {\n        'root': [\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            # keywords: go before method names to avoid lexing \"throw new XYZ\"\n            # as a method signature\n            (r'(assert|break|case|catch|continue|default|do|else|finally|for|'\n             r'if|goto|instanceof|new|return|switch|this|throw|try|while)\\b',\n             Keyword),\n            # method names\n            (r'((?:(?:[^\\W\\d]|\\$)[\\w.\\[\\]$<>]*\\s+)+?)'  # return arguments\n             r'((?:[^\\W\\d]|\\$)[\\w$]*)'                  # method name\n             r'(\\s*)(\\()',                              # signature start\n             bygroups(using(this), Name.Function, Text, Punctuation)),\n            (r'@[^\\W\\d][\\w.]*', Name.Decorator),\n            (r'(abstract|const|enum|extends|final|implements|native|private|'\n             r'protected|public|static|strictfp|super|synchronized|throws|'\n             r'transient|volatile)\\b', Keyword.Declaration),\n            (r'(boolean|byte|char|double|float|int|long|short|void)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(class|interface)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'class'),\n            (r'(var)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'var'),\n            (r'(import(?:\\s+static)?)(\\s+)', bygroups(Keyword.Namespace, Text),\n             'import'),\n            (r'\"', String, 'string'),\n            (r\"'\\\\.'|'[^\\\\]'|'\\\\u[0-9a-fA-F]{4}'\", String.Char),\n            (r'(\\.)((?:[^\\W\\d]|\\$)[\\w$]*)', bygroups(Punctuation,\n                                                     Name.Attribute)),\n            (r'^\\s*([^\\W\\d]|\\$)[\\w$]*:', Name.Label),\n            (r'([^\\W\\d]|\\$)[\\w$]*', Name),\n            (r'([0-9][0-9_]*\\.([0-9][0-9_]*)?|'\n             r'\\.[0-9][0-9_]*)'\n             r'([eE][+\\-]?[0-9][0-9_]*)?[fFdD]?|'\n             r'[0-9][eE][+\\-]?[0-9][0-9_]*[fFdD]?|'\n             r'[0-9]([eE][+\\-]?[0-9][0-9_]*)?[fFdD]|'\n             r'0[xX]([0-9a-fA-F][0-9a-fA-F_]*\\.?|'\n             r'([0-9a-fA-F][0-9a-fA-F_]*)?\\.[0-9a-fA-F][0-9a-fA-F_]*)'\n             r'[pP][+\\-]?[0-9][0-9_]*[fFdD]?', Number.Float),\n            (r'0[xX][0-9a-fA-F][0-9a-fA-F_]*[lL]?', Number.Hex),\n            (r'0[bB][01][01_]*[lL]?', Number.Bin),\n            (r'0[0-7_]+[lL]?', Number.Oct),\n            (r'0|[1-9][0-9_]*[lL]?', Number.Integer),\n            (r'[~^*!%&\\[\\]<>|+=/?-]', Operator),\n            (r'[{}();:.,]', Punctuation),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'([^\\W\\d]|\\$)[\\w$]*', Name.Class, '#pop')\n        ],\n        'var': [\n            (r'([^\\W\\d]|\\$)[\\w$]*', Name, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n        'string': [\n            (r'[^\\\\\"]+', String),\n            (r'\\\\\\\\', String),  # Escaped backslash\n            (r'\\\\\"', String),  # Escaped quote\n            (r'\\\\', String),  # Bare backslash\n            (r'\"', String, '#pop'),  # Closing quote\n        ],\n    }\n\n\nclass AspectJLexer(JavaLexer):\n    \"\"\"\n    For `AspectJ <http://www.eclipse.org/aspectj/>`_ source code.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'AspectJ'\n    aliases = ['aspectj']\n    filenames = ['*.aj']\n    mimetypes = ['text/x-aspectj']\n\n    aj_keywords = {\n        'aspect', 'pointcut', 'privileged', 'call', 'execution',\n        'initialization', 'preinitialization', 'handler', 'get', 'set',\n        'staticinitialization', 'target', 'args', 'within', 'withincode',\n        'cflow', 'cflowbelow', 'annotation', 'before', 'after', 'around',\n        'proceed', 'throwing', 'returning', 'adviceexecution', 'declare',\n        'parents', 'warning', 'error', 'soft', 'precedence', 'thisJoinPoint',\n        'thisJoinPointStaticPart', 'thisEnclosingJoinPointStaticPart',\n        'issingleton', 'perthis', 'pertarget', 'percflow', 'percflowbelow',\n        'pertypewithin', 'lock', 'unlock', 'thisAspectInstance'\n    }\n    aj_inter_type = {'parents:', 'warning:', 'error:', 'soft:', 'precedence:'}\n    aj_inter_type_annotation = {'@type', '@method', '@constructor', '@field'}\n\n    def get_tokens_unprocessed(self, text):\n        for index, token, value in JavaLexer.get_tokens_unprocessed(self, text):\n            if token is Name and value in self.aj_keywords:\n                yield index, Keyword, value\n            elif token is Name.Label and value in self.aj_inter_type:\n                yield index, Keyword, value[:-1]\n                yield index, Operator, value[-1]\n            elif token is Name.Decorator and value in self.aj_inter_type_annotation:\n                yield index, Keyword, value\n            else:\n                yield index, token, value\n\n\nclass ScalaLexer(RegexLexer):\n    \"\"\"\n    For `Scala <http://www.scala-lang.org>`_ source code.\n    \"\"\"\n\n    name = 'Scala'\n    aliases = ['scala']\n    filenames = ['*.scala']\n    mimetypes = ['text/x-scala']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    # don't use raw unicode strings!\n    op = ('[-~\\\\^\\\\*!%&\\\\\\\\<>\\\\|+=:/?@\\u00a6-\\u00a7\\u00a9\\u00ac\\u00ae\\u00b0-\\u00b1'\n          '\\u00b6\\u00d7\\u00f7\\u03f6\\u0482\\u0606-\\u0608\\u060e-\\u060f\\u06e9'\n          '\\u06fd-\\u06fe\\u07f6\\u09fa\\u0b70\\u0bf3-\\u0bf8\\u0bfa\\u0c7f\\u0cf1-\\u0cf2'\n          '\\u0d79\\u0f01-\\u0f03\\u0f13-\\u0f17\\u0f1a-\\u0f1f\\u0f34\\u0f36\\u0f38'\n          '\\u0fbe-\\u0fc5\\u0fc7-\\u0fcf\\u109e-\\u109f\\u1360\\u1390-\\u1399\\u1940'\n          '\\u19e0-\\u19ff\\u1b61-\\u1b6a\\u1b74-\\u1b7c\\u2044\\u2052\\u207a-\\u207c'\n          '\\u208a-\\u208c\\u2100-\\u2101\\u2103-\\u2106\\u2108-\\u2109\\u2114\\u2116-\\u2118'\n          '\\u211e-\\u2123\\u2125\\u2127\\u2129\\u212e\\u213a-\\u213b\\u2140-\\u2144'\n          '\\u214a-\\u214d\\u214f\\u2190-\\u2328\\u232b-\\u244a\\u249c-\\u24e9\\u2500-\\u2767'\n          '\\u2794-\\u27c4\\u27c7-\\u27e5\\u27f0-\\u2982\\u2999-\\u29d7\\u29dc-\\u29fb'\n          '\\u29fe-\\u2b54\\u2ce5-\\u2cea\\u2e80-\\u2ffb\\u3004\\u3012-\\u3013\\u3020'\n          '\\u3036-\\u3037\\u303e-\\u303f\\u3190-\\u3191\\u3196-\\u319f\\u31c0-\\u31e3'\n          '\\u3200-\\u321e\\u322a-\\u3250\\u3260-\\u327f\\u328a-\\u32b0\\u32c0-\\u33ff'\n          '\\u4dc0-\\u4dff\\ua490-\\ua4c6\\ua828-\\ua82b\\ufb29\\ufdfd\\ufe62\\ufe64-\\ufe66'\n          '\\uff0b\\uff1c-\\uff1e\\uff5c\\uff5e\\uffe2\\uffe4\\uffe8-\\uffee\\ufffc-\\ufffd]+')\n\n    letter = ('[a-zA-Z\\\\$_\\u00aa\\u00b5\\u00ba\\u00c0-\\u00d6\\u00d8-\\u00f6'\n              '\\u00f8-\\u02af\\u0370-\\u0373\\u0376-\\u0377\\u037b-\\u037d\\u0386'\n              '\\u0388-\\u03f5\\u03f7-\\u0481\\u048a-\\u0556\\u0561-\\u0587\\u05d0-\\u05f2'\n              '\\u0621-\\u063f\\u0641-\\u064a\\u066e-\\u066f\\u0671-\\u06d3\\u06d5'\n              '\\u06ee-\\u06ef\\u06fa-\\u06fc\\u06ff\\u0710\\u0712-\\u072f\\u074d-\\u07a5'\n              '\\u07b1\\u07ca-\\u07ea\\u0904-\\u0939\\u093d\\u0950\\u0958-\\u0961'\n              '\\u0972-\\u097f\\u0985-\\u09b9\\u09bd\\u09ce\\u09dc-\\u09e1\\u09f0-\\u09f1'\n              '\\u0a05-\\u0a39\\u0a59-\\u0a5e\\u0a72-\\u0a74\\u0a85-\\u0ab9\\u0abd'\n              '\\u0ad0-\\u0ae1\\u0b05-\\u0b39\\u0b3d\\u0b5c-\\u0b61\\u0b71\\u0b83-\\u0bb9'\n              '\\u0bd0\\u0c05-\\u0c3d\\u0c58-\\u0c61\\u0c85-\\u0cb9\\u0cbd\\u0cde-\\u0ce1'\n              '\\u0d05-\\u0d3d\\u0d60-\\u0d61\\u0d7a-\\u0d7f\\u0d85-\\u0dc6\\u0e01-\\u0e30'\n              '\\u0e32-\\u0e33\\u0e40-\\u0e45\\u0e81-\\u0eb0\\u0eb2-\\u0eb3\\u0ebd-\\u0ec4'\n              '\\u0edc-\\u0f00\\u0f40-\\u0f6c\\u0f88-\\u0f8b\\u1000-\\u102a\\u103f'\n              '\\u1050-\\u1055\\u105a-\\u105d\\u1061\\u1065-\\u1066\\u106e-\\u1070'\n              '\\u1075-\\u1081\\u108e\\u10a0-\\u10fa\\u1100-\\u135a\\u1380-\\u138f'\n              '\\u13a0-\\u166c\\u166f-\\u1676\\u1681-\\u169a\\u16a0-\\u16ea\\u16ee-\\u1711'\n              '\\u1720-\\u1731\\u1740-\\u1751\\u1760-\\u1770\\u1780-\\u17b3\\u17dc'\n              '\\u1820-\\u1842\\u1844-\\u18a8\\u18aa-\\u191c\\u1950-\\u19a9\\u19c1-\\u19c7'\n              '\\u1a00-\\u1a16\\u1b05-\\u1b33\\u1b45-\\u1b4b\\u1b83-\\u1ba0\\u1bae-\\u1baf'\n              '\\u1c00-\\u1c23\\u1c4d-\\u1c4f\\u1c5a-\\u1c77\\u1d00-\\u1d2b\\u1d62-\\u1d77'\n              '\\u1d79-\\u1d9a\\u1e00-\\u1fbc\\u1fbe\\u1fc2-\\u1fcc\\u1fd0-\\u1fdb'\n              '\\u1fe0-\\u1fec\\u1ff2-\\u1ffc\\u2071\\u207f\\u2102\\u2107\\u210a-\\u2113'\n              '\\u2115\\u2119-\\u211d\\u2124\\u2126\\u2128\\u212a-\\u212d\\u212f-\\u2139'\n              '\\u213c-\\u213f\\u2145-\\u2149\\u214e\\u2160-\\u2188\\u2c00-\\u2c7c'\n              '\\u2c80-\\u2ce4\\u2d00-\\u2d65\\u2d80-\\u2dde\\u3006-\\u3007\\u3021-\\u3029'\n              '\\u3038-\\u303a\\u303c\\u3041-\\u3096\\u309f\\u30a1-\\u30fa\\u30ff-\\u318e'\n              '\\u31a0-\\u31b7\\u31f0-\\u31ff\\u3400-\\u4db5\\u4e00-\\ua014\\ua016-\\ua48c'\n              '\\ua500-\\ua60b\\ua610-\\ua61f\\ua62a-\\ua66e\\ua680-\\ua697\\ua722-\\ua76f'\n              '\\ua771-\\ua787\\ua78b-\\ua801\\ua803-\\ua805\\ua807-\\ua80a\\ua80c-\\ua822'\n              '\\ua840-\\ua873\\ua882-\\ua8b3\\ua90a-\\ua925\\ua930-\\ua946\\uaa00-\\uaa28'\n              '\\uaa40-\\uaa42\\uaa44-\\uaa4b\\uac00-\\ud7a3\\uf900-\\ufb1d\\ufb1f-\\ufb28'\n              '\\ufb2a-\\ufd3d\\ufd50-\\ufdfb\\ufe70-\\ufefc\\uff21-\\uff3a\\uff41-\\uff5a'\n              '\\uff66-\\uff6f\\uff71-\\uff9d\\uffa0-\\uffdc]')\n\n    upper = ('[A-Z\\\\$_\\u00c0-\\u00d6\\u00d8-\\u00de\\u0100\\u0102\\u0104\\u0106\\u0108'\n             '\\u010a\\u010c\\u010e\\u0110\\u0112\\u0114\\u0116\\u0118\\u011a\\u011c'\n             '\\u011e\\u0120\\u0122\\u0124\\u0126\\u0128\\u012a\\u012c\\u012e\\u0130'\n             '\\u0132\\u0134\\u0136\\u0139\\u013b\\u013d\\u013f\\u0141\\u0143\\u0145'\n             '\\u0147\\u014a\\u014c\\u014e\\u0150\\u0152\\u0154\\u0156\\u0158\\u015a'\n             '\\u015c\\u015e\\u0160\\u0162\\u0164\\u0166\\u0168\\u016a\\u016c\\u016e'\n             '\\u0170\\u0172\\u0174\\u0176\\u0178-\\u0179\\u017b\\u017d\\u0181-\\u0182'\n             '\\u0184\\u0186-\\u0187\\u0189-\\u018b\\u018e-\\u0191\\u0193-\\u0194'\n             '\\u0196-\\u0198\\u019c-\\u019d\\u019f-\\u01a0\\u01a2\\u01a4\\u01a6-\\u01a7'\n             '\\u01a9\\u01ac\\u01ae-\\u01af\\u01b1-\\u01b3\\u01b5\\u01b7-\\u01b8\\u01bc'\n             '\\u01c4\\u01c7\\u01ca\\u01cd\\u01cf\\u01d1\\u01d3\\u01d5\\u01d7\\u01d9'\n             '\\u01db\\u01de\\u01e0\\u01e2\\u01e4\\u01e6\\u01e8\\u01ea\\u01ec\\u01ee'\n             '\\u01f1\\u01f4\\u01f6-\\u01f8\\u01fa\\u01fc\\u01fe\\u0200\\u0202\\u0204'\n             '\\u0206\\u0208\\u020a\\u020c\\u020e\\u0210\\u0212\\u0214\\u0216\\u0218'\n             '\\u021a\\u021c\\u021e\\u0220\\u0222\\u0224\\u0226\\u0228\\u022a\\u022c'\n             '\\u022e\\u0230\\u0232\\u023a-\\u023b\\u023d-\\u023e\\u0241\\u0243-\\u0246'\n             '\\u0248\\u024a\\u024c\\u024e\\u0370\\u0372\\u0376\\u0386\\u0388-\\u038f'\n             '\\u0391-\\u03ab\\u03cf\\u03d2-\\u03d4\\u03d8\\u03da\\u03dc\\u03de\\u03e0'\n             '\\u03e2\\u03e4\\u03e6\\u03e8\\u03ea\\u03ec\\u03ee\\u03f4\\u03f7'\n             '\\u03f9-\\u03fa\\u03fd-\\u042f\\u0460\\u0462\\u0464\\u0466\\u0468\\u046a'\n             '\\u046c\\u046e\\u0470\\u0472\\u0474\\u0476\\u0478\\u047a\\u047c\\u047e'\n             '\\u0480\\u048a\\u048c\\u048e\\u0490\\u0492\\u0494\\u0496\\u0498\\u049a'\n             '\\u049c\\u049e\\u04a0\\u04a2\\u04a4\\u04a6\\u04a8\\u04aa\\u04ac\\u04ae'\n             '\\u04b0\\u04b2\\u04b4\\u04b6\\u04b8\\u04ba\\u04bc\\u04be\\u04c0-\\u04c1'\n             '\\u04c3\\u04c5\\u04c7\\u04c9\\u04cb\\u04cd\\u04d0\\u04d2\\u04d4\\u04d6'\n             '\\u04d8\\u04da\\u04dc\\u04de\\u04e0\\u04e2\\u04e4\\u04e6\\u04e8\\u04ea'\n             '\\u04ec\\u04ee\\u04f0\\u04f2\\u04f4\\u04f6\\u04f8\\u04fa\\u04fc\\u04fe'\n             '\\u0500\\u0502\\u0504\\u0506\\u0508\\u050a\\u050c\\u050e\\u0510\\u0512'\n             '\\u0514\\u0516\\u0518\\u051a\\u051c\\u051e\\u0520\\u0522\\u0531-\\u0556'\n             '\\u10a0-\\u10c5\\u1e00\\u1e02\\u1e04\\u1e06\\u1e08\\u1e0a\\u1e0c\\u1e0e'\n             '\\u1e10\\u1e12\\u1e14\\u1e16\\u1e18\\u1e1a\\u1e1c\\u1e1e\\u1e20\\u1e22'\n             '\\u1e24\\u1e26\\u1e28\\u1e2a\\u1e2c\\u1e2e\\u1e30\\u1e32\\u1e34\\u1e36'\n             '\\u1e38\\u1e3a\\u1e3c\\u1e3e\\u1e40\\u1e42\\u1e44\\u1e46\\u1e48\\u1e4a'\n             '\\u1e4c\\u1e4e\\u1e50\\u1e52\\u1e54\\u1e56\\u1e58\\u1e5a\\u1e5c\\u1e5e'\n             '\\u1e60\\u1e62\\u1e64\\u1e66\\u1e68\\u1e6a\\u1e6c\\u1e6e\\u1e70\\u1e72'\n             '\\u1e74\\u1e76\\u1e78\\u1e7a\\u1e7c\\u1e7e\\u1e80\\u1e82\\u1e84\\u1e86'\n             '\\u1e88\\u1e8a\\u1e8c\\u1e8e\\u1e90\\u1e92\\u1e94\\u1e9e\\u1ea0\\u1ea2'\n             '\\u1ea4\\u1ea6\\u1ea8\\u1eaa\\u1eac\\u1eae\\u1eb0\\u1eb2\\u1eb4\\u1eb6'\n             '\\u1eb8\\u1eba\\u1ebc\\u1ebe\\u1ec0\\u1ec2\\u1ec4\\u1ec6\\u1ec8\\u1eca'\n             '\\u1ecc\\u1ece\\u1ed0\\u1ed2\\u1ed4\\u1ed6\\u1ed8\\u1eda\\u1edc\\u1ede'\n             '\\u1ee0\\u1ee2\\u1ee4\\u1ee6\\u1ee8\\u1eea\\u1eec\\u1eee\\u1ef0\\u1ef2'\n             '\\u1ef4\\u1ef6\\u1ef8\\u1efa\\u1efc\\u1efe\\u1f08-\\u1f0f\\u1f18-\\u1f1d'\n             '\\u1f28-\\u1f2f\\u1f38-\\u1f3f\\u1f48-\\u1f4d\\u1f59-\\u1f5f'\n             '\\u1f68-\\u1f6f\\u1fb8-\\u1fbb\\u1fc8-\\u1fcb\\u1fd8-\\u1fdb'\n             '\\u1fe8-\\u1fec\\u1ff8-\\u1ffb\\u2102\\u2107\\u210b-\\u210d\\u2110-\\u2112'\n             '\\u2115\\u2119-\\u211d\\u2124\\u2126\\u2128\\u212a-\\u212d\\u2130-\\u2133'\n             '\\u213e-\\u213f\\u2145\\u2183\\u2c00-\\u2c2e\\u2c60\\u2c62-\\u2c64\\u2c67'\n             '\\u2c69\\u2c6b\\u2c6d-\\u2c6f\\u2c72\\u2c75\\u2c80\\u2c82\\u2c84\\u2c86'\n             '\\u2c88\\u2c8a\\u2c8c\\u2c8e\\u2c90\\u2c92\\u2c94\\u2c96\\u2c98\\u2c9a'\n             '\\u2c9c\\u2c9e\\u2ca0\\u2ca2\\u2ca4\\u2ca6\\u2ca8\\u2caa\\u2cac\\u2cae'\n             '\\u2cb0\\u2cb2\\u2cb4\\u2cb6\\u2cb8\\u2cba\\u2cbc\\u2cbe\\u2cc0\\u2cc2'\n             '\\u2cc4\\u2cc6\\u2cc8\\u2cca\\u2ccc\\u2cce\\u2cd0\\u2cd2\\u2cd4\\u2cd6'\n             '\\u2cd8\\u2cda\\u2cdc\\u2cde\\u2ce0\\u2ce2\\ua640\\ua642\\ua644\\ua646'\n             '\\ua648\\ua64a\\ua64c\\ua64e\\ua650\\ua652\\ua654\\ua656\\ua658\\ua65a'\n             '\\ua65c\\ua65e\\ua662\\ua664\\ua666\\ua668\\ua66a\\ua66c\\ua680\\ua682'\n             '\\ua684\\ua686\\ua688\\ua68a\\ua68c\\ua68e\\ua690\\ua692\\ua694\\ua696'\n             '\\ua722\\ua724\\ua726\\ua728\\ua72a\\ua72c\\ua72e\\ua732\\ua734\\ua736'\n             '\\ua738\\ua73a\\ua73c\\ua73e\\ua740\\ua742\\ua744\\ua746\\ua748\\ua74a'\n             '\\ua74c\\ua74e\\ua750\\ua752\\ua754\\ua756\\ua758\\ua75a\\ua75c\\ua75e'\n             '\\ua760\\ua762\\ua764\\ua766\\ua768\\ua76a\\ua76c\\ua76e\\ua779\\ua77b'\n             '\\ua77d-\\ua77e\\ua780\\ua782\\ua784\\ua786\\ua78b\\uff21-\\uff3a]')\n\n    idrest = '%s(?:%s|[0-9])*(?:(?<=_)%s)?' % (letter, letter, op)\n    letter_letter_digit = '%s(?:%s|\\\\d)*' % (letter, letter)\n\n    tokens = {\n        'root': [\n            # method names\n            (r'(class|trait|object)(\\s+)', bygroups(Keyword, Text), 'class'),\n            (r'[^\\S\\n]+', Text),\n            include('comments'),\n            (r'@%s' % idrest, Name.Decorator),\n            (r'(abstract|ca(?:se|tch)|d(?:ef|o)|e(?:lse|xtends)|'\n             r'f(?:inal(?:ly)?|or(?:Some)?)|i(?:f|mplicit)|'\n             r'lazy|match|new|override|pr(?:ivate|otected)'\n             r'|re(?:quires|turn)|s(?:ealed|uper)|'\n             r't(?:h(?:is|row)|ry)|va[lr]|w(?:hile|ith)|yield)\\b|'\n             r'(<[%:-]|=>|>:|[#=@_\\u21D2\\u2190])\\b', Keyword),\n            (r':(?!%s)' % op, Keyword, 'type'),\n            (r'%s%s\\b' % (upper, idrest), Name.Class),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(import|package)(\\s+)', bygroups(Keyword, Text), 'import'),\n            (r'(type)(\\s+)', bygroups(Keyword, Text), 'type'),\n            (r'\"\"\".*?\"\"\"(?!\")', String),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String),\n            (r\"'\\\\.'|'[^\\\\]'|'\\\\u[0-9a-fA-F]{4}'\", String.Char),\n            (r\"'%s\" % idrest, Text.Symbol),\n            (r'[fs]\"\"\"', String, 'interptriplestring'),  # interpolated strings\n            (r'[fs]\"', String, 'interpstring'),  # interpolated strings\n            (r'raw\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String),  # raw strings\n            # (r'(\\.)(%s|%s|`[^`]+`)' % (idrest, op), bygroups(Operator,\n            # Name.Attribute)),\n            (idrest, Name),\n            (r'`[^`]+`', Name),\n            (r'\\[', Operator, 'typeparam'),\n            (r'[(){};,.#]', Operator),\n            (op, Operator),\n            (r'([0-9][0-9]*\\.[0-9]*|\\.[0-9]+)([eE][+-]?[0-9]+)?[fFdD]?',\n             Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'(%s|%s|`[^`]+`)(\\s*)(\\[)' % (idrest, op),\n             bygroups(Name.Class, Text, Operator), ('#pop', 'typeparam')),\n            (r'\\s+', Text),\n            include('comments'),\n            (r'\\{', Operator, '#pop'),\n            (r'\\(', Operator, '#pop'),\n            (r'%s|%s|`[^`]+`' % (idrest, op), Name.Class, '#pop'),\n        ],\n        'type': [\n            (r'\\s+', Text),\n            include('comments'),\n            (r'<[%:]|>:|[#_]|\\bforSome\\b|\\btype\\b', Keyword),\n            (r'([,);}]|=>|=|\\u21d2)(\\s*)', bygroups(Operator, Text), '#pop'),\n            (r'[({]', Operator, '#push'),\n            (r'((?:%s|%s|`[^`]+`)(?:\\.(?:%s|%s|`[^`]+`))*)(\\s*)(\\[)' %\n             (idrest, op, idrest, op),\n             bygroups(Keyword.Type, Text, Operator), ('#pop', 'typeparam')),\n            (r'((?:%s|%s|`[^`]+`)(?:\\.(?:%s|%s|`[^`]+`))*)(\\s*)$' %\n             (idrest, op, idrest, op),\n             bygroups(Keyword.Type, Text), '#pop'),\n            (r'\\.|%s|%s|`[^`]+`' % (idrest, op), Keyword.Type)\n        ],\n        'typeparam': [\n            (r'\\s+', Text),\n            include('comments'),\n            (r',+', Punctuation),\n            (r'<[%:]|=>|>:|[#_\\u21D2]|\\bforSome\\b|\\btype\\b', Keyword),\n            (r'([\\])}])', Operator, '#pop'),\n            (r'[(\\[{]', Operator, '#push'),\n            (r'\\.|%s|%s|`[^`]+`' % (idrest, op), Keyword.Type)\n        ],\n        'comments': [\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*', Comment.Multiline, 'comment'),\n        ],\n        'comment': [\n            (r'[^/*]+', Comment.Multiline),\n            (r'/\\*', Comment.Multiline, '#push'),\n            (r'\\*/', Comment.Multiline, '#pop'),\n            (r'[*/]', Comment.Multiline)\n        ],\n        'import': [\n            (r'(%s|\\.)+' % idrest, Name.Namespace, '#pop')\n        ],\n        'interpstringcommon': [\n            (r'[^\"$\\\\]+', String),\n            (r'\\$\\$', String),\n            (r'\\$' + letter_letter_digit, String.Interpol),\n            (r'\\$\\{', String.Interpol, 'interpbrace'),\n            (r'\\\\.', String),\n        ],\n        'interptriplestring': [\n            (r'\"\"\"(?!\")', String, '#pop'),\n            (r'\"', String),\n            include('interpstringcommon'),\n        ],\n        'interpstring': [\n            (r'\"', String, '#pop'),\n            include('interpstringcommon'),\n        ],\n        'interpbrace': [\n            (r'\\}', String.Interpol, '#pop'),\n            (r'\\{', String.Interpol, '#push'),\n            include('root'),\n        ],\n    }\n\n\nclass GosuLexer(RegexLexer):\n    \"\"\"\n    For Gosu source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    name = 'Gosu'\n    aliases = ['gosu']\n    filenames = ['*.gs', '*.gsx', '*.gsp', '*.vark']\n    mimetypes = ['text/x-gosu']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # modifiers etc.\n             r'([a-zA-Z_]\\w*)'                       # method name\n             r'(\\s*)(\\()',                           # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w.]*', Name.Decorator),\n            (r'(in|as|typeof|statictypeof|typeis|typeas|if|else|foreach|for|'\n             r'index|while|do|continue|break|return|try|catch|finally|this|'\n             r'throw|new|switch|case|default|eval|super|outer|classpath|'\n             r'using)\\b', Keyword),\n            (r'(var|delegate|construct|function|private|internal|protected|'\n             r'public|abstract|override|final|static|extends|transient|'\n             r'implements|represents|readonly)\\b', Keyword.Declaration),\n            (r'(property\\s+)(get|set)?', Keyword.Declaration),\n            (r'(boolean|byte|char|double|float|int|long|short|void|block)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(true|false|null|NaN|Infinity)\\b', Keyword.Constant),\n            (r'(class|interface|enhancement|enum)(\\s+)([a-zA-Z_]\\w*)',\n             bygroups(Keyword.Declaration, Text, Name.Class)),\n            (r'(uses)(\\s+)([\\w.]+\\*?)',\n             bygroups(Keyword.Namespace, Text, Name.Namespace)),\n            (r'\"', String, 'string'),\n            (r'(\\??[.#])([a-zA-Z_]\\w*)',\n             bygroups(Operator, Name.Attribute)),\n            (r'(:)([a-zA-Z_]\\w*)',\n             bygroups(Operator, Name.Attribute)),\n            (r'[a-zA-Z_$]\\w*', Name),\n            (r'and|or|not|[\\\\~^*!%&\\[\\](){}<>|+=:;,./?-]', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'[0-9]+', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'templateText': [\n            (r'(\\\\<)|(\\\\\\$)', String),\n            (r'(<%@\\s+)(extends|params)',\n             bygroups(Operator, Name.Decorator), 'stringTemplate'),\n            (r'<%!--.*?--%>', Comment.Multiline),\n            (r'(<%)|(<%=)', Operator, 'stringTemplate'),\n            (r'\\$\\{', Operator, 'stringTemplateShorthand'),\n            (r'.', String)\n        ],\n        'string': [\n            (r'\"', String, '#pop'),\n            include('templateText')\n        ],\n        'stringTemplate': [\n            (r'\"', String, 'string'),\n            (r'%>', Operator, '#pop'),\n            include('root')\n        ],\n        'stringTemplateShorthand': [\n            (r'\"', String, 'string'),\n            (r'\\{', Operator, 'stringTemplateShorthand'),\n            (r'\\}', Operator, '#pop'),\n            include('root')\n        ],\n    }\n\n\nclass GosuTemplateLexer(Lexer):\n    \"\"\"\n    For Gosu templates.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    name = 'Gosu Template'\n    aliases = ['gst']\n    filenames = ['*.gst']\n    mimetypes = ['text/x-gosu-template']\n\n    def get_tokens_unprocessed(self, text):\n        lexer = GosuLexer()\n        stack = ['templateText']\n        yield from lexer.get_tokens_unprocessed(text, stack)\n\n\nclass GroovyLexer(RegexLexer):\n    \"\"\"\n    For `Groovy <http://groovy.codehaus.org/>`_ source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    name = 'Groovy'\n    aliases = ['groovy']\n    filenames = ['*.groovy','*.gradle']\n    mimetypes = ['text/x-groovy']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # Groovy allows a file to start with a shebang\n            (r'#!(.*?)$', Comment.Preproc, 'base'),\n            default('base'),\n        ],\n        'base': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_]\\w*)'                      # method name\n             r'(\\s*)(\\()',                          # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w.]*', Name.Decorator),\n            (r'(assert|break|case|catch|continue|default|do|else|finally|for|'\n             r'if|goto|instanceof|new|return|switch|this|throw|try|while|in|as)\\b',\n             Keyword),\n            (r'(abstract|const|enum|extends|final|implements|native|private|'\n             r'protected|public|static|strictfp|super|synchronized|throws|'\n             r'transient|volatile)\\b', Keyword.Declaration),\n            (r'(def|boolean|byte|char|double|float|int|long|short|void)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(class|interface)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'class'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'\"\"\".*?\"\"\"', String.Double),\n            (r\"'''.*?'''\", String.Single),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'\\$/((?!/\\$).)*/\\$', String),\n            (r'/(\\\\\\\\|\\\\[^\\\\]|[^/\\\\])*/', String),\n            (r\"'\\\\.'|'[^\\\\]'|'\\\\u[0-9a-fA-F]{4}'\", String.Char),\n            (r'(\\.)([a-zA-Z_]\\w*)', bygroups(Operator, Name.Attribute)),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_$]\\w*', Name),\n            (r'[~^*!%&\\[\\](){}<>|+=:;,./?-]', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'[a-zA-Z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n    }\n\n    def analyse_text(text):\n        return shebang_matches(text, r'groovy')\n\n\nclass IokeLexer(RegexLexer):\n    \"\"\"\n    For `Ioke <http://ioke.org/>`_ (a strongly typed, dynamic,\n    prototype based programming language) source.\n\n    .. versionadded:: 1.4\n    \"\"\"\n    name = 'Ioke'\n    filenames = ['*.ik']\n    aliases = ['ioke', 'ik']\n    mimetypes = ['text/x-iokesrc']\n    tokens = {\n        'interpolatableText': [\n            (r'(\\\\b|\\\\e|\\\\t|\\\\n|\\\\f|\\\\r|\\\\\"|\\\\\\\\|\\\\#|\\\\\\Z|\\\\u[0-9a-fA-F]{1,4}'\n             r'|\\\\[0-3]?[0-7]?[0-7])', String.Escape),\n            (r'#\\{', Punctuation, 'textInterpolationRoot')\n        ],\n\n        'text': [\n            (r'(?<!\\\\)\"', String, '#pop'),\n            include('interpolatableText'),\n            (r'[^\"]', String)\n        ],\n\n        'documentation': [\n            (r'(?<!\\\\)\"', String.Doc, '#pop'),\n            include('interpolatableText'),\n            (r'[^\"]', String.Doc)\n        ],\n\n        'textInterpolationRoot': [\n            (r'\\}', Punctuation, '#pop'),\n            include('root')\n        ],\n\n        'slashRegexp': [\n            (r'(?<!\\\\)/[im-psux]*', String.Regex, '#pop'),\n            include('interpolatableText'),\n            (r'\\\\/', String.Regex),\n            (r'[^/]', String.Regex)\n        ],\n\n        'squareRegexp': [\n            (r'(?<!\\\\)][im-psux]*', String.Regex, '#pop'),\n            include('interpolatableText'),\n            (r'\\\\]', String.Regex),\n            (r'[^\\]]', String.Regex)\n        ],\n\n        'squareText': [\n            (r'(?<!\\\\)]', String, '#pop'),\n            include('interpolatableText'),\n            (r'[^\\]]', String)\n        ],\n\n        'root': [\n            (r'\\n', Text),\n            (r'\\s+', Text),\n\n            # Comments\n            (r';(.*?)\\n', Comment),\n            (r'\\A#!(.*?)\\n', Comment),\n\n            # Regexps\n            (r'#/', String.Regex, 'slashRegexp'),\n            (r'#r\\[', String.Regex, 'squareRegexp'),\n\n            # Symbols\n            (r':[\\w!:?]+', String.Symbol),\n            (r'[\\w!:?]+:(?![\\w!?])', String.Other),\n            (r':\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Symbol),\n\n            # Documentation\n            (r'((?<=fn\\()|(?<=fnx\\()|(?<=method\\()|(?<=macro\\()|(?<=lecro\\()'\n             r'|(?<=syntax\\()|(?<=dmacro\\()|(?<=dlecro\\()|(?<=dlecrox\\()'\n             r'|(?<=dsyntax\\())\\s*\"', String.Doc, 'documentation'),\n\n            # Text\n            (r'\"', String, 'text'),\n            (r'#\\[', String, 'squareText'),\n\n            # Mimic\n            (r'\\w[\\w!:?]+(?=\\s*=.*mimic\\s)', Name.Entity),\n\n            # Assignment\n            (r'[a-zA-Z_][\\w!:?]*(?=[\\s]*[+*/-]?=[^=].*($|\\.))',\n             Name.Variable),\n\n            # keywords\n            (r'(break|cond|continue|do|ensure|for|for:dict|for:set|if|let|'\n             r'loop|p:for|p:for:dict|p:for:set|return|unless|until|while|'\n             r'with)(?![\\w!:?])', Keyword.Reserved),\n\n            # Origin\n            (r'(eval|mimic|print|println)(?![\\w!:?])', Keyword),\n\n            # Base\n            (r'(cell\\?|cellNames|cellOwner\\?|cellOwner|cells|cell|'\n             r'documentation|hash|identity|mimic|removeCell\\!|undefineCell\\!)'\n             r'(?![\\w!:?])', Keyword),\n\n            # Ground\n            (r'(stackTraceAsText)(?![\\w!:?])', Keyword),\n\n            # DefaultBehaviour Literals\n            (r'(dict|list|message|set)(?![\\w!:?])', Keyword.Reserved),\n\n            # DefaultBehaviour Case\n            (r'(case|case:and|case:else|case:nand|case:nor|case:not|case:or|'\n             r'case:otherwise|case:xor)(?![\\w!:?])', Keyword.Reserved),\n\n            # DefaultBehaviour Reflection\n            (r'(asText|become\\!|derive|freeze\\!|frozen\\?|in\\?|is\\?|kind\\?|'\n             r'mimic\\!|mimics|mimics\\?|prependMimic\\!|removeAllMimics\\!|'\n             r'removeMimic\\!|same\\?|send|thaw\\!|uniqueHexId)'\n             r'(?![\\w!:?])', Keyword),\n\n            # DefaultBehaviour Aspects\n            (r'(after|around|before)(?![\\w!:?])', Keyword.Reserved),\n\n            # DefaultBehaviour\n            (r'(kind|cellDescriptionDict|cellSummary|genSym|inspect|notice)'\n             r'(?![\\w!:?])', Keyword),\n            (r'(use|destructuring)', Keyword.Reserved),\n\n            # DefaultBehavior BaseBehavior\n            (r'(cell\\?|cellOwner\\?|cellOwner|cellNames|cells|cell|'\n             r'documentation|identity|removeCell!|undefineCell)'\n             r'(?![\\w!:?])', Keyword),\n\n            # DefaultBehavior Internal\n            (r'(internal:compositeRegexp|internal:concatenateText|'\n             r'internal:createDecimal|internal:createNumber|'\n             r'internal:createRegexp|internal:createText)'\n             r'(?![\\w!:?])', Keyword.Reserved),\n\n            # DefaultBehaviour Conditions\n            (r'(availableRestarts|bind|error\\!|findRestart|handle|'\n             r'invokeRestart|rescue|restart|signal\\!|warn\\!)'\n             r'(?![\\w!:?])', Keyword.Reserved),\n\n            # constants\n            (r'(nil|false|true)(?![\\w!:?])', Name.Constant),\n\n            # names\n            (r'(Arity|Base|Call|Condition|DateTime|Aspects|Pointcut|'\n             r'Assignment|BaseBehavior|Boolean|Case|AndCombiner|Else|'\n             r'NAndCombiner|NOrCombiner|NotCombiner|OrCombiner|XOrCombiner|'\n             r'Conditions|Definitions|FlowControl|Internal|Literals|'\n             r'Reflection|DefaultMacro|DefaultMethod|DefaultSyntax|Dict|'\n             r'FileSystem|Ground|Handler|Hook|IO|IokeGround|Struct|'\n             r'LexicalBlock|LexicalMacro|List|Message|Method|Mixins|'\n             r'NativeMethod|Number|Origin|Pair|Range|Reflector|Regexp Match|'\n             r'Regexp|Rescue|Restart|Runtime|Sequence|Set|Symbol|'\n             r'System|Text|Tuple)(?![\\w!:?])', Name.Builtin),\n\n            # functions\n            ('(generateMatchMethod|aliasMethod|\\u03bb|\\u028E|fnx|fn|method|'\n             'dmacro|dlecro|syntax|macro|dlecrox|lecrox|lecro|syntax)'\n             '(?![\\\\w!:?])', Name.Function),\n\n            # Numbers\n            (r'-?0[xX][0-9a-fA-F]+', Number.Hex),\n            (r'-?(\\d+\\.?\\d*|\\d*\\.\\d+)([eE][+-]?[0-9]+)?', Number.Float),\n            (r'-?\\d+', Number.Integer),\n\n            (r'#\\(', Punctuation),\n\n            # Operators\n            (r'(&&>>|\\|\\|>>|\\*\\*>>|:::|::|\\.\\.\\.|===|\\*\\*>|\\*\\*=|&&>|&&=|'\n             r'\\|\\|>|\\|\\|=|\\->>|\\+>>|!>>|<>>>|<>>|&>>|%>>|#>>|@>>|/>>|\\*>>|'\n             r'\\?>>|\\|>>|\\^>>|~>>|\\$>>|=>>|<<=|>>=|<=>|<\\->|=~|!~|=>|\\+\\+|'\n             r'\\-\\-|<=|>=|==|!=|&&|\\.\\.|\\+=|\\-=|\\*=|\\/=|%=|&=|\\^=|\\|=|<\\-|'\n             r'\\+>|!>|<>|&>|%>|#>|\\@>|\\/>|\\*>|\\?>|\\|>|\\^>|~>|\\$>|<\\->|\\->|'\n             r'<<|>>|\\*\\*|\\?\\||\\?&|\\|\\||>|<|\\*|\\/|%|\\+|\\-|&|\\^|\\||=|\\$|!|~|'\n             r'\\?|#|\\u2260|\\u2218|\\u2208|\\u2209)', Operator),\n            (r'(and|nand|or|xor|nor|return|import)(?![\\w!?])',\n             Operator),\n\n            # Punctuation\n            (r'(\\`\\`|\\`|\\'\\'|\\'|\\.|\\,|@@|@|\\[|\\]|\\(|\\)|\\{|\\})', Punctuation),\n\n            # kinds\n            (r'[A-Z][\\w!:?]*', Name.Class),\n\n            # default cellnames\n            (r'[a-z_][\\w!:?]*', Name)\n        ]\n    }\n\n\nclass ClojureLexer(RegexLexer):\n    \"\"\"\n    Lexer for `Clojure <http://clojure.org/>`_ source code.\n\n    .. versionadded:: 0.11\n    \"\"\"\n    name = 'Clojure'\n    aliases = ['clojure', 'clj']\n    filenames = ['*.clj']\n    mimetypes = ['text/x-clojure', 'application/x-clojure']\n\n    special_forms = (\n        '.', 'def', 'do', 'fn', 'if', 'let', 'new', 'quote', 'var', 'loop'\n    )\n\n    # It's safe to consider 'ns' a declaration thing because it defines a new\n    # namespace.\n    declarations = (\n        'def-', 'defn', 'defn-', 'defmacro', 'defmulti', 'defmethod',\n        'defstruct', 'defonce', 'declare', 'definline', 'definterface',\n        'defprotocol', 'defrecord', 'deftype', 'defproject', 'ns'\n    )\n\n    builtins = (\n        '*', '+', '-', '->', '/', '<', '<=', '=', '==', '>', '>=', '..',\n        'accessor', 'agent', 'agent-errors', 'aget', 'alength', 'all-ns',\n        'alter', 'and', 'append-child', 'apply', 'array-map', 'aset',\n        'aset-boolean', 'aset-byte', 'aset-char', 'aset-double', 'aset-float',\n        'aset-int', 'aset-long', 'aset-short', 'assert', 'assoc', 'await',\n        'await-for', 'bean', 'binding', 'bit-and', 'bit-not', 'bit-or',\n        'bit-shift-left', 'bit-shift-right', 'bit-xor', 'boolean', 'branch?',\n        'butlast', 'byte', 'cast', 'char', 'children', 'class',\n        'clear-agent-errors', 'comment', 'commute', 'comp', 'comparator',\n        'complement', 'concat', 'conj', 'cons', 'constantly', 'cond', 'if-not',\n        'construct-proxy', 'contains?', 'count', 'create-ns', 'create-struct',\n        'cycle', 'dec',  'deref', 'difference', 'disj', 'dissoc', 'distinct',\n        'doall', 'doc', 'dorun', 'doseq', 'dosync', 'dotimes', 'doto',\n        'double', 'down', 'drop', 'drop-while', 'edit', 'end?', 'ensure',\n        'eval', 'every?', 'false?', 'ffirst', 'file-seq', 'filter', 'find',\n        'find-doc', 'find-ns', 'find-var', 'first', 'float', 'flush', 'for',\n        'fnseq', 'frest', 'gensym', 'get-proxy-class', 'get',\n        'hash-map', 'hash-set', 'identical?', 'identity', 'if-let', 'import',\n        'in-ns', 'inc', 'index', 'insert-child', 'insert-left', 'insert-right',\n        'inspect-table', 'inspect-tree', 'instance?', 'int', 'interleave',\n        'intersection', 'into', 'into-array', 'iterate', 'join', 'key', 'keys',\n        'keyword', 'keyword?', 'last', 'lazy-cat', 'lazy-cons', 'left',\n        'lefts', 'line-seq', 'list*', 'list', 'load', 'load-file',\n        'locking', 'long', 'loop', 'macroexpand', 'macroexpand-1',\n        'make-array', 'make-node', 'map', 'map-invert', 'map?', 'mapcat',\n        'max', 'max-key', 'memfn', 'merge', 'merge-with', 'meta', 'min',\n        'min-key', 'name', 'namespace', 'neg?', 'new', 'newline', 'next',\n        'nil?', 'node', 'not', 'not-any?', 'not-every?', 'not=', 'ns-imports',\n        'ns-interns', 'ns-map', 'ns-name', 'ns-publics', 'ns-refers',\n        'ns-resolve', 'ns-unmap', 'nth', 'nthrest', 'or', 'parse', 'partial',\n        'path', 'peek', 'pop', 'pos?', 'pr', 'pr-str', 'print', 'print-str',\n        'println', 'println-str', 'prn', 'prn-str', 'project', 'proxy',\n        'proxy-mappings', 'quot', 'rand', 'rand-int', 'range', 're-find',\n        're-groups', 're-matcher', 're-matches', 're-pattern', 're-seq',\n        'read', 'read-line', 'reduce', 'ref', 'ref-set', 'refer', 'rem',\n        'remove', 'remove-method', 'remove-ns', 'rename', 'rename-keys',\n        'repeat', 'replace', 'replicate', 'resolve', 'rest', 'resultset-seq',\n        'reverse', 'rfirst', 'right', 'rights', 'root', 'rrest', 'rseq',\n        'second', 'select', 'select-keys', 'send', 'send-off', 'seq',\n        'seq-zip', 'seq?', 'set', 'short', 'slurp', 'some', 'sort',\n        'sort-by', 'sorted-map', 'sorted-map-by', 'sorted-set',\n        'special-symbol?', 'split-at', 'split-with', 'str', 'string?',\n        'struct', 'struct-map', 'subs', 'subvec', 'symbol', 'symbol?',\n        'sync', 'take', 'take-nth', 'take-while', 'test', 'time', 'to-array',\n        'to-array-2d', 'tree-seq', 'true?', 'union', 'up', 'update-proxy',\n        'val', 'vals', 'var-get', 'var-set', 'var?', 'vector', 'vector-zip',\n        'vector?', 'when', 'when-first', 'when-let', 'when-not',\n        'with-local-vars', 'with-meta', 'with-open', 'with-out-str',\n        'xml-seq', 'xml-zip', 'zero?', 'zipmap', 'zipper')\n\n    # valid names for identifiers\n    # well, names can only not consist fully of numbers\n    # but this should be good enough for now\n\n    # TODO / should divide keywords/symbols into namespace/rest\n    # but that's hard, so just pretend / is part of the name\n    valid_name = r'(?!#)[\\w!$%*+<=>?/.#|-]+'\n\n    tokens = {\n        'root': [\n            # the comments - always starting with semicolon\n            # and going to the end of the line\n            (r';.*$', Comment.Single),\n\n            # whitespaces - usually not relevant\n            (r'[,\\s]+', Text),\n\n            # numbers\n            (r'-?\\d+\\.\\d+', Number.Float),\n            (r'-?\\d+', Number.Integer),\n            (r'0x-?[abcdef\\d]+', Number.Hex),\n\n            # strings, symbols and characters\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String),\n            (r\"'\" + valid_name, String.Symbol),\n            (r\"\\\\(.|[a-z]+)\", String.Char),\n\n            # keywords\n            (r'::?#?' + valid_name, String.Symbol),\n\n            # special operators\n            (r'~@|[`\\'#^~&@]', Operator),\n\n            # highlight the special forms\n            (words(special_forms, suffix=' '), Keyword),\n\n            # Technically, only the special forms are 'keywords'. The problem\n            # is that only treating them as keywords means that things like\n            # 'defn' and 'ns' need to be highlighted as builtins. This is ugly\n            # and weird for most styles. So, as a compromise we're going to\n            # highlight them as Keyword.Declarations.\n            (words(declarations, suffix=' '), Keyword.Declaration),\n\n            # highlight the builtins\n            (words(builtins, suffix=' '), Name.Builtin),\n\n            # the remaining functions\n            (r'(?<=\\()' + valid_name, Name.Function),\n\n            # find the remaining variables\n            (valid_name, Name.Variable),\n\n            # Clojure accepts vector notation\n            (r'(\\[|\\])', Punctuation),\n\n            # Clojure accepts map notation\n            (r'(\\{|\\})', Punctuation),\n\n            # the famous parentheses!\n            (r'(\\(|\\))', Punctuation),\n        ],\n    }\n\n\nclass ClojureScriptLexer(ClojureLexer):\n    \"\"\"\n    Lexer for `ClojureScript <http://clojure.org/clojurescript>`_\n    source code.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    name = 'ClojureScript'\n    aliases = ['clojurescript', 'cljs']\n    filenames = ['*.cljs']\n    mimetypes = ['text/x-clojurescript', 'application/x-clojurescript']\n\n\nclass TeaLangLexer(RegexLexer):\n    \"\"\"\n    For `Tea <http://teatrove.org/>`_ source code. Only used within a\n    TeaTemplateLexer.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w\\.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_]\\w*)'                       # method name\n             r'(\\s*)(\\()',                           # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w\\.]*', Name.Decorator),\n            (r'(and|break|else|foreach|if|in|not|or|reverse)\\b',\n             Keyword),\n            (r'(as|call|define)\\b', Keyword.Declaration),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(template)(\\s+)', bygroups(Keyword.Declaration, Text), 'template'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'(\\.)([a-zA-Z_]\\w*)', bygroups(Operator, Name.Attribute)),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_\\$]\\w*', Name),\n            (r'(isa|[.]{3}|[.]{2}|[=#!<>+-/%&;,.\\*\\\\\\(\\)\\[\\]\\{\\}])', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'template': [\n            (r'[a-zA-Z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n    }\n\n\nclass CeylonLexer(RegexLexer):\n    \"\"\"\n    For `Ceylon <http://ceylon-lang.org/>`_ source code.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'Ceylon'\n    aliases = ['ceylon']\n    filenames = ['*.ceylon']\n    mimetypes = ['text/x-ceylon']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    #: optional Comment or Whitespace\n    _ws = r'(?:\\s|//.*?\\n|/[*].*?[*]/)+'\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_]\\w*)'                      # method name\n             r'(\\s*)(\\()',                          # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*', Comment.Multiline, 'comment'),\n            (r'(shared|abstract|formal|default|actual|variable|deprecated|small|'\n             r'late|literal|doc|by|see|throws|optional|license|tagged|final|native|'\n             r'annotation|sealed)\\b', Name.Decorator),\n            (r'(break|case|catch|continue|else|finally|for|in|'\n             r'if|return|switch|this|throw|try|while|is|exists|dynamic|'\n             r'nonempty|then|outer|assert|let)\\b', Keyword),\n            (r'(abstracts|extends|satisfies|'\n             r'super|given|of|out|assign)\\b', Keyword.Declaration),\n            (r'(function|value|void|new)\\b',\n             Keyword.Type),\n            (r'(assembly|module|package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(class|interface|object|alias)(\\s+)',\n             bygroups(Keyword.Declaration, Text), 'class'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String),\n            (r\"'\\\\.'|'[^\\\\]'|'\\\\\\{#[0-9a-fA-F]{4}\\}'\", String.Char),\n            (r'\".*``.*``.*\"', String.Interpol),\n            (r'(\\.)([a-z_]\\w*)',\n             bygroups(Operator, Name.Attribute)),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_]\\w*', Name),\n            (r'[~^*!%&\\[\\](){}<>|+=:;,./?-]', Operator),\n            (r'\\d{1,3}(_\\d{3})+\\.\\d{1,3}(_\\d{3})+[kMGTPmunpf]?', Number.Float),\n            (r'\\d{1,3}(_\\d{3})+\\.[0-9]+([eE][+-]?[0-9]+)?[kMGTPmunpf]?',\n             Number.Float),\n            (r'[0-9][0-9]*\\.\\d{1,3}(_\\d{3})+[kMGTPmunpf]?', Number.Float),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][+-]?[0-9]+)?[kMGTPmunpf]?',\n             Number.Float),\n            (r'#([0-9a-fA-F]{4})(_[0-9a-fA-F]{4})+', Number.Hex),\n            (r'#[0-9a-fA-F]+', Number.Hex),\n            (r'\\$([01]{4})(_[01]{4})+', Number.Bin),\n            (r'\\$[01]+', Number.Bin),\n            (r'\\d{1,3}(_\\d{3})+[kMGTP]?', Number.Integer),\n            (r'[0-9]+[kMGTP]?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'[A-Za-z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[a-z][\\w.]*',\n             Name.Namespace, '#pop')\n        ],\n        'comment': [\n            (r'[^*/]', Comment.Multiline),\n            (r'/\\*', Comment.Multiline, '#push'),\n            (r'\\*/', Comment.Multiline, '#pop'),\n            (r'[*/]', Comment.Multiline)\n        ],\n    }\n\n\nclass KotlinLexer(RegexLexer):\n    \"\"\"\n    For `Kotlin <http://kotlinlang.org/>`_\n    source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    name = 'Kotlin'\n    aliases = ['kotlin']\n    filenames = ['*.kt', '*.kts']\n    mimetypes = ['text/x-kotlin']\n\n    flags = re.MULTILINE | re.DOTALL | re.UNICODE\n\n    kt_name = ('@?[_' + uni.combine('Lu', 'Ll', 'Lt', 'Lm', 'Nl') + ']' +\n               '[' + uni.combine('Lu', 'Ll', 'Lt', 'Lm', 'Nl', 'Nd', 'Pc', 'Cf',\n                                 'Mn', 'Mc') + ']*')\n\n    kt_space_name = ('@?[_' + uni.combine('Lu', 'Ll', 'Lt', 'Lm', 'Nl') + ']' +\n               '[' + uni.combine('Lu', 'Ll', 'Lt', 'Lm', 'Nl', 'Nd', 'Pc', 'Cf',\n                                 'Mn', 'Mc', 'Zs') + ',-]*')\n\n    kt_id = '(' + kt_name + '|`' + kt_space_name + '`)'\n\n    tokens = {\n        'root': [\n            (r'^\\s*\\[.*?\\]', Name.Attribute),\n            (r'[^\\S\\n]+', Text),\n            (r'\\s+', Text),\n            (r'\\\\\\n', Text),  # line continuation\n            (r'//.*?\\n', Comment.Single),\n            (r'^#!/.+?\\n', Comment.Single),  # shebang for kotlin scripts\n            (r'/[*].*?[*]/', Comment.Multiline),\n            (r'\"\"\".*?\"\"\"', String),\n            (r'\\n', Text),\n            (r'::|!!|\\?[:.]', Operator),\n            (r'[~!%^&*()+=|\\[\\]:;,.<>/?-]', Punctuation),\n            (r'[{}]', Punctuation),\n            (r'@\"(\"\"|[^\"])*\"', String),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\\\n])*[\"\\n]', String),\n            (r\"'\\\\.'|'[^\\\\]'\", String.Char),\n            (r\"[0-9](\\.[0-9]*)?([eE][+-][0-9]+)?[flFL]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n            (r'(object)(\\s+)(:)(\\s+)', bygroups(Keyword, Text, Punctuation, Text), 'class'),\n            (r'(companion)(\\s+)(object)', bygroups(Keyword, Text, Keyword)),\n            (r'(class|interface|object)(\\s+)', bygroups(Keyword, Text), 'class'),\n            (r'(package|import)(\\s+)', bygroups(Keyword, Text), 'package'),\n            (r'(val|var)(\\s+)([(])', bygroups(Keyword, Text, Punctuation), 'property_dec'),\n            (r'(val|var)(\\s+)', bygroups(Keyword, Text), 'property'),\n            (r'(fun)(\\s+)', bygroups(Keyword, Text), 'function'),\n            (r'(inline fun)(\\s+)', bygroups(Keyword, Text), 'function'),\n            (r'(abstract|annotation|as|break|by|catch|class|companion|const|'\n             r'constructor|continue|crossinline|data|do|dynamic|else|enum|'\n             r'external|false|final|finally|for|fun|get|if|import|in|infix|'\n             r'inline|inner|interface|internal|is|lateinit|noinline|null|'\n             r'object|open|operator|out|override|package|private|protected|'\n             r'public|reified|return|sealed|set|super|tailrec|this|throw|'\n             r'true|try|val|var|vararg|when|where|while)\\b', Keyword),\n            (kt_id, Name),\n        ],\n        'package': [\n            (r'\\S+', Name.Namespace, '#pop')\n        ],\n        'class': [\n            (kt_id, Name.Class, '#pop')\n        ],\n        'property': [\n            (kt_id, Name.Property, '#pop')\n        ],\n        'property_dec': [\n            (r'(,)(\\s*)', bygroups(Punctuation, Text)),\n            (r'(:)(\\s*)', bygroups(Punctuation, Text)),\n            (r'<', Punctuation, 'generic'),\n            (r'([)])', Punctuation, '#pop'),\n            (kt_id, Name.Property)\n        ],\n        'function': [\n            (r'<', Punctuation, 'generic'),\n            (r''+kt_id+'([.])'+kt_id, bygroups(Name.Class, Punctuation, Name.Function), '#pop'),\n            (kt_id, Name.Function, '#pop')\n        ],\n        'generic': [\n            (r'(>)(\\s*)', bygroups(Punctuation, Text), '#pop'),\n            (r':',Punctuation),\n            (r'(reified|out|in)\\b', Keyword),\n            (r',',Text),\n            (r'\\s+',Text),\n            (kt_id,Name)\n        ]\n    }\n\n\nclass XtendLexer(RegexLexer):\n    \"\"\"\n    For `Xtend <http://xtend-lang.org/>`_ source code.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'Xtend'\n    aliases = ['xtend']\n    filenames = ['*.xtend']\n    mimetypes = ['text/x-xtend']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_$][\\w$]*)'                  # method name\n             r'(\\s*)(\\()',                          # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w.]*', Name.Decorator),\n            (r'(assert|break|case|catch|continue|default|do|else|finally|for|'\n             r'if|goto|instanceof|new|return|switch|this|throw|try|while|IF|'\n             r'ELSE|ELSEIF|ENDIF|FOR|ENDFOR|SEPARATOR|BEFORE|AFTER)\\b',\n             Keyword),\n            (r'(def|abstract|const|enum|extends|final|implements|native|private|'\n             r'protected|public|static|strictfp|super|synchronized|throws|'\n             r'transient|volatile)\\b', Keyword.Declaration),\n            (r'(boolean|byte|char|double|float|int|long|short|void)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(class|interface)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'class'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r\"(''')\", String, 'template'),\n            (r'(\\u00BB)', String, 'template'),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_$]\\w*', Name),\n            (r'[~^*!%&\\[\\](){}<>\\|+=:;,./?-]', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'[a-zA-Z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n        'template': [\n            (r\"'''\", String, '#pop'),\n            (r'\\u00AB', String, '#pop'),\n            (r'.', String)\n        ],\n    }\n\n\nclass PigLexer(RegexLexer):\n    \"\"\"\n    For `Pig Latin <https://pig.apache.org/>`_ source code.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'Pig'\n    aliases = ['pig']\n    filenames = ['*.pig']\n    mimetypes = ['text/x-pig']\n\n    flags = re.MULTILINE | re.IGNORECASE\n\n    tokens = {\n        'root': [\n            (r'\\s+', Text),\n            (r'--.*', Comment),\n            (r'/\\*[\\w\\W]*?\\*/', Comment.Multiline),\n            (r'\\\\\\n', Text),\n            (r'\\\\', Text),\n            (r'\\'(?:\\\\[ntbrf\\\\\\']|\\\\u[0-9a-f]{4}|[^\\'\\\\\\n\\r])*\\'', String),\n            include('keywords'),\n            include('types'),\n            include('builtins'),\n            include('punct'),\n            include('operators'),\n            (r'[0-9]*\\.[0-9]+(e[0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-f]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text),\n            (r'([a-z_]\\w*)(\\s*)(\\()',\n             bygroups(Name.Function, Text, Punctuation)),\n            (r'[()#:]', Text),\n            (r'[^(:#\\'\")\\s]+', Text),\n            (r'\\S+\\s+', Text)   # TODO: make tests pass without \\s+\n        ],\n        'keywords': [\n            (r'(assert|and|any|all|arrange|as|asc|bag|by|cache|CASE|cat|cd|cp|'\n             r'%declare|%default|define|dense|desc|describe|distinct|du|dump|'\n             r'eval|exex|explain|filter|flatten|foreach|full|generate|group|'\n             r'help|if|illustrate|import|inner|input|into|is|join|kill|left|'\n             r'limit|load|ls|map|matches|mkdir|mv|not|null|onschema|or|order|'\n             r'outer|output|parallel|pig|pwd|quit|register|returns|right|rm|'\n             r'rmf|rollup|run|sample|set|ship|split|stderr|stdin|stdout|store|'\n             r'stream|through|union|using|void)\\b', Keyword)\n        ],\n        'builtins': [\n            (r'(AVG|BinStorage|cogroup|CONCAT|copyFromLocal|copyToLocal|COUNT|'\n             r'cross|DIFF|MAX|MIN|PigDump|PigStorage|SIZE|SUM|TextLoader|'\n             r'TOKENIZE)\\b', Name.Builtin)\n        ],\n        'types': [\n            (r'(bytearray|BIGINTEGER|BIGDECIMAL|chararray|datetime|double|float|'\n             r'int|long|tuple)\\b', Keyword.Type)\n        ],\n        'punct': [\n            (r'[;(){}\\[\\]]', Punctuation),\n        ],\n        'operators': [\n            (r'[#=,./%+\\-?]', Operator),\n            (r'(eq|gt|lt|gte|lte|neq|matches)\\b', Operator),\n            (r'(==|<=|<|>=|>|!=)', Operator),\n        ],\n    }\n\n\nclass GoloLexer(RegexLexer):\n    \"\"\"\n    For `Golo <http://golo-lang.org/>`_ source code.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'Golo'\n    filenames = ['*.golo']\n    aliases = ['golo']\n\n    tokens = {\n        'root': [\n            (r'[^\\S\\n]+', Text),\n\n            (r'#.*$', Comment),\n\n            (r'(\\^|\\.\\.\\.|:|\\?:|->|==|!=|=|\\+|\\*|%|/|<=|<|>=|>|=|\\.)',\n                Operator),\n            (r'(?<=[^-])(-)(?=[^-])', Operator),\n\n            (r'(?<=[^`])(is|isnt|and|or|not|oftype|in|orIfNull)\\b', Operator.Word),\n            (r'[]{}|(),[]', Punctuation),\n\n            (r'(module|import)(\\s+)',\n                bygroups(Keyword.Namespace, Text),\n                'modname'),\n            (r'\\b([a-zA-Z_][\\w$.]*)(::)',  bygroups(Name.Namespace, Punctuation)),\n            (r'\\b([a-zA-Z_][\\w$]*(?:\\.[a-zA-Z_][\\w$]*)+)\\b', Name.Namespace),\n\n            (r'(let|var)(\\s+)',\n                bygroups(Keyword.Declaration, Text),\n                'varname'),\n            (r'(struct)(\\s+)',\n                bygroups(Keyword.Declaration, Text),\n                'structname'),\n            (r'(function)(\\s+)',\n                bygroups(Keyword.Declaration, Text),\n                'funcname'),\n\n            (r'(null|true|false)\\b', Keyword.Constant),\n            (r'(augment|pimp'\n             r'|if|else|case|match|return'\n             r'|case|when|then|otherwise'\n             r'|while|for|foreach'\n             r'|try|catch|finally|throw'\n             r'|local'\n             r'|continue|break)\\b', Keyword),\n\n            (r'(map|array|list|set|vector|tuple)(\\[)',\n                bygroups(Name.Builtin, Punctuation)),\n            (r'(print|println|readln|raise|fun'\n             r'|asInterfaceInstance)\\b', Name.Builtin),\n            (r'(`?[a-zA-Z_][\\w$]*)(\\()',\n                bygroups(Name.Function, Punctuation)),\n\n            (r'-?[\\d_]*\\.[\\d_]*([eE][+-]?\\d[\\d_]*)?F?', Number.Float),\n            (r'0[0-7]+j?', Number.Oct),\n            (r'0[xX][a-fA-F0-9]+', Number.Hex),\n            (r'-?\\d[\\d_]*L', Number.Integer.Long),\n            (r'-?\\d[\\d_]*', Number.Integer),\n\n            (r'`?[a-zA-Z_][\\w$]*', Name),\n            (r'@[a-zA-Z_][\\w$.]*', Name.Decorator),\n\n            (r'\"\"\"', String, combined('stringescape', 'triplestring')),\n            (r'\"', String, combined('stringescape', 'doublestring')),\n            (r\"'\", String, combined('stringescape', 'singlestring')),\n            (r'----((.|\\n)*?)----', String.Doc)\n\n        ],\n\n        'funcname': [\n            (r'`?[a-zA-Z_][\\w$]*', Name.Function, '#pop'),\n        ],\n        'modname': [\n            (r'[a-zA-Z_][\\w$.]*\\*?', Name.Namespace, '#pop')\n        ],\n        'structname': [\n            (r'`?[\\w.]+\\*?', Name.Class, '#pop')\n        ],\n        'varname': [\n            (r'`?[a-zA-Z_][\\w$]*', Name.Variable, '#pop'),\n        ],\n        'string': [\n            (r'[^\\\\\\'\"\\n]+', String),\n            (r'[\\'\"\\\\]', String)\n        ],\n        'stringescape': [\n            (r'\\\\([\\\\abfnrtv\"\\']|\\n|N\\{.*?\\}|u[a-fA-F0-9]{4}|'\n             r'U[a-fA-F0-9]{8}|x[a-fA-F0-9]{2}|[0-7]{1,3})', String.Escape)\n        ],\n        'triplestring': [\n            (r'\"\"\"', String, '#pop'),\n            include('string'),\n            (r'\\n', String),\n        ],\n        'doublestring': [\n            (r'\"', String.Double, '#pop'),\n            include('string'),\n        ],\n        'singlestring': [\n            (r\"'\", String, '#pop'),\n            include('string'),\n        ],\n        'operators': [\n            (r'[#=,./%+\\-?]', Operator),\n            (r'(eq|gt|lt|gte|lte|neq|matches)\\b', Operator),\n            (r'(==|<=|<|>=|>|!=)', Operator),\n        ],\n    }\n\n\nclass JasminLexer(RegexLexer):\n    \"\"\"\n    For `Jasmin <http://jasmin.sourceforge.net/>`_ assembly code.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'Jasmin'\n    aliases = ['jasmin', 'jasminxt']\n    filenames = ['*.j']\n\n    _whitespace = r' \\n\\t\\r'\n    _ws = r'(?:[%s]+)' % _whitespace\n    _separator = r'%s:=' % _whitespace\n    _break = r'(?=[%s]|$)' % _separator\n    _name = r'[^%s]+' % _separator\n    _unqualified_name = r'(?:[^%s.;\\[/]+)' % _separator\n\n    tokens = {\n        'default': [\n            (r'\\n', Text, '#pop'),\n            (r\"'\", String.Single, ('#pop', 'quote')),\n            (r'\"', String.Double, 'string'),\n            (r'=', Punctuation),\n            (r':', Punctuation, 'label'),\n            (_ws, Text),\n            (r';.*', Comment.Single),\n            (r'(\\$[-+])?0x-?[\\da-fA-F]+%s' % _break, Number.Hex),\n            (r'(\\$[-+]|\\+)?-?\\d+%s' % _break, Number.Integer),\n            (r'-?(\\d+\\.\\d*|\\.\\d+)([eE][-+]?\\d+)?[fFdD]?'\n             r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]*%s' % _break, Number.Float),\n            (r'\\$%s' % _name, Name.Variable),\n\n            # Directives\n            (r'\\.annotation%s' % _break, Keyword.Reserved, 'annotation'),\n            (r'(\\.attribute|\\.bytecode|\\.debug|\\.deprecated|\\.enclosing|'\n             r'\\.interface|\\.line|\\.signature|\\.source|\\.stack|\\.var|abstract|'\n             r'annotation|bridge|class|default|enum|field|final|fpstrict|'\n             r'interface|native|private|protected|public|signature|static|'\n             r'synchronized|synthetic|transient|varargs|volatile)%s' % _break,\n             Keyword.Reserved),\n            (r'\\.catch%s' % _break, Keyword.Reserved, 'caught-exception'),\n            (r'(\\.class|\\.implements|\\.inner|\\.super|inner|invisible|'\n             r'invisibleparam|outer|visible|visibleparam)%s' % _break,\n             Keyword.Reserved, 'class/convert-dots'),\n            (r'\\.field%s' % _break, Keyword.Reserved,\n             ('descriptor/convert-dots', 'field')),\n            (r'(\\.end|\\.limit|use)%s' % _break, Keyword.Reserved,\n             'no-verification'),\n            (r'\\.method%s' % _break, Keyword.Reserved, 'method'),\n            (r'\\.set%s' % _break, Keyword.Reserved, 'var'),\n            (r'\\.throws%s' % _break, Keyword.Reserved, 'exception'),\n            (r'(from|offset|to|using)%s' % _break, Keyword.Reserved, 'label'),\n            (r'is%s' % _break, Keyword.Reserved,\n             ('descriptor/convert-dots', 'var')),\n            (r'(locals|stack)%s' % _break, Keyword.Reserved, 'verification'),\n            (r'method%s' % _break, Keyword.Reserved, 'enclosing-method'),\n\n            # Instructions\n            (words((\n                'aaload', 'aastore', 'aconst_null', 'aload', 'aload_0', 'aload_1', 'aload_2',\n                'aload_3', 'aload_w', 'areturn', 'arraylength', 'astore', 'astore_0', 'astore_1',\n                'astore_2', 'astore_3', 'astore_w', 'athrow', 'baload', 'bastore', 'bipush',\n                'breakpoint', 'caload', 'castore', 'd2f', 'd2i', 'd2l', 'dadd', 'daload', 'dastore',\n                'dcmpg', 'dcmpl', 'dconst_0', 'dconst_1', 'ddiv', 'dload', 'dload_0', 'dload_1',\n                'dload_2', 'dload_3', 'dload_w', 'dmul', 'dneg', 'drem', 'dreturn', 'dstore', 'dstore_0',\n                'dstore_1', 'dstore_2', 'dstore_3', 'dstore_w', 'dsub', 'dup', 'dup2', 'dup2_x1',\n                'dup2_x2', 'dup_x1', 'dup_x2', 'f2d', 'f2i', 'f2l', 'fadd', 'faload', 'fastore', 'fcmpg',\n                'fcmpl', 'fconst_0', 'fconst_1', 'fconst_2', 'fdiv', 'fload', 'fload_0', 'fload_1',\n                'fload_2', 'fload_3', 'fload_w', 'fmul', 'fneg', 'frem', 'freturn', 'fstore', 'fstore_0',\n                'fstore_1', 'fstore_2', 'fstore_3', 'fstore_w', 'fsub', 'i2b', 'i2c', 'i2d', 'i2f', 'i2l',\n                'i2s', 'iadd', 'iaload', 'iand', 'iastore', 'iconst_0', 'iconst_1', 'iconst_2',\n                'iconst_3', 'iconst_4', 'iconst_5', 'iconst_m1', 'idiv', 'iinc', 'iinc_w', 'iload',\n                'iload_0', 'iload_1', 'iload_2', 'iload_3', 'iload_w', 'imul', 'ineg', 'int2byte',\n                'int2char', 'int2short', 'ior', 'irem', 'ireturn', 'ishl', 'ishr', 'istore', 'istore_0',\n                'istore_1', 'istore_2', 'istore_3', 'istore_w', 'isub', 'iushr', 'ixor', 'l2d', 'l2f',\n                'l2i', 'ladd', 'laload', 'land', 'lastore', 'lcmp', 'lconst_0', 'lconst_1', 'ldc2_w',\n                'ldiv', 'lload', 'lload_0', 'lload_1', 'lload_2', 'lload_3', 'lload_w', 'lmul', 'lneg',\n                'lookupswitch', 'lor', 'lrem', 'lreturn', 'lshl', 'lshr', 'lstore', 'lstore_0',\n                'lstore_1', 'lstore_2', 'lstore_3', 'lstore_w', 'lsub', 'lushr', 'lxor',\n                'monitorenter', 'monitorexit', 'nop', 'pop', 'pop2', 'ret', 'ret_w', 'return', 'saload',\n                'sastore', 'sipush', 'swap'), suffix=_break), Keyword.Reserved),\n            (r'(anewarray|checkcast|instanceof|ldc|ldc_w|new)%s' % _break,\n             Keyword.Reserved, 'class/no-dots'),\n            (r'invoke(dynamic|interface|nonvirtual|special|'\n             r'static|virtual)%s' % _break, Keyword.Reserved,\n             'invocation'),\n            (r'(getfield|putfield)%s' % _break, Keyword.Reserved,\n             ('descriptor/no-dots', 'field')),\n            (r'(getstatic|putstatic)%s' % _break, Keyword.Reserved,\n             ('descriptor/no-dots', 'static')),\n            (words((\n                'goto', 'goto_w', 'if_acmpeq', 'if_acmpne', 'if_icmpeq',\n                'if_icmpge', 'if_icmpgt', 'if_icmple', 'if_icmplt', 'if_icmpne',\n                'ifeq', 'ifge', 'ifgt', 'ifle', 'iflt', 'ifne', 'ifnonnull',\n                'ifnull', 'jsr', 'jsr_w'), suffix=_break),\n             Keyword.Reserved, 'label'),\n            (r'(multianewarray|newarray)%s' % _break, Keyword.Reserved,\n             'descriptor/convert-dots'),\n            (r'tableswitch%s' % _break, Keyword.Reserved, 'table')\n        ],\n        'quote': [\n            (r\"'\", String.Single, '#pop'),\n            (r'\\\\u[\\da-fA-F]{4}', String.Escape),\n            (r\"[^'\\\\]+\", String.Single)\n        ],\n        'string': [\n            (r'\"', String.Double, '#pop'),\n            (r'\\\\([nrtfb\"\\'\\\\]|u[\\da-fA-F]{4}|[0-3]?[0-7]{1,2})',\n             String.Escape),\n            (r'[^\"\\\\]+', String.Double)\n        ],\n        'root': [\n            (r'\\n+', Text),\n            (r\"'\", String.Single, 'quote'),\n            include('default'),\n            (r'(%s)([ \\t\\r]*)(:)' % _name,\n             bygroups(Name.Label, Text, Punctuation)),\n            (_name, String.Other)\n        ],\n        'annotation': [\n            (r'\\n', Text, ('#pop', 'annotation-body')),\n            (r'default%s' % _break, Keyword.Reserved,\n             ('#pop', 'annotation-default')),\n            include('default')\n        ],\n        'annotation-body': [\n            (r'\\n+', Text),\n            (r'\\.end%s' % _break, Keyword.Reserved, '#pop'),\n            include('default'),\n            (_name, String.Other, ('annotation-items', 'descriptor/no-dots'))\n        ],\n        'annotation-default': [\n            (r'\\n+', Text),\n            (r'\\.end%s' % _break, Keyword.Reserved, '#pop'),\n            include('default'),\n            default(('annotation-items', 'descriptor/no-dots'))\n        ],\n        'annotation-items': [\n            (r\"'\", String.Single, 'quote'),\n            include('default'),\n            (_name, String.Other)\n        ],\n        'caught-exception': [\n            (r'all%s' % _break, Keyword, '#pop'),\n            include('exception')\n        ],\n        'class/convert-dots': [\n            include('default'),\n            (r'(L)((?:%s[/.])*)(%s)(;)' % (_unqualified_name, _name),\n             bygroups(Keyword.Type, Name.Namespace, Name.Class, Punctuation),\n             '#pop'),\n            (r'((?:%s[/.])*)(%s)' % (_unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class), '#pop')\n        ],\n        'class/no-dots': [\n            include('default'),\n            (r'\\[+', Punctuation, ('#pop', 'descriptor/no-dots')),\n            (r'(L)((?:%s/)*)(%s)(;)' % (_unqualified_name, _name),\n             bygroups(Keyword.Type, Name.Namespace, Name.Class, Punctuation),\n             '#pop'),\n            (r'((?:%s/)*)(%s)' % (_unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class), '#pop')\n        ],\n        'descriptor/convert-dots': [\n            include('default'),\n            (r'\\[+', Punctuation),\n            (r'(L)((?:%s[/.])*)(%s?)(;)' % (_unqualified_name, _name),\n             bygroups(Keyword.Type, Name.Namespace, Name.Class, Punctuation),\n             '#pop'),\n            (r'[^%s\\[)L]+' % _separator, Keyword.Type, '#pop'),\n            default('#pop')\n        ],\n        'descriptor/no-dots': [\n            include('default'),\n            (r'\\[+', Punctuation),\n            (r'(L)((?:%s/)*)(%s)(;)' % (_unqualified_name, _name),\n             bygroups(Keyword.Type, Name.Namespace, Name.Class, Punctuation),\n             '#pop'),\n            (r'[^%s\\[)L]+' % _separator, Keyword.Type, '#pop'),\n            default('#pop')\n        ],\n        'descriptors/convert-dots': [\n            (r'\\)', Punctuation, '#pop'),\n            default('descriptor/convert-dots')\n        ],\n        'enclosing-method': [\n            (_ws, Text),\n            (r'(?=[^%s]*\\()' % _separator, Text, ('#pop', 'invocation')),\n            default(('#pop', 'class/convert-dots'))\n        ],\n        'exception': [\n            include('default'),\n            (r'((?:%s[/.])*)(%s)' % (_unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Exception), '#pop')\n        ],\n        'field': [\n            (r'static%s' % _break, Keyword.Reserved, ('#pop', 'static')),\n            include('default'),\n            (r'((?:%s[/.](?=[^%s]*[/.]))*)(%s[/.])?(%s)' %\n             (_unqualified_name, _separator, _unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class, Name.Variable.Instance),\n             '#pop')\n        ],\n        'invocation': [\n            include('default'),\n            (r'((?:%s[/.](?=[^%s(]*[/.]))*)(%s[/.])?(%s)(\\()' %\n             (_unqualified_name, _separator, _unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class, Name.Function, Punctuation),\n             ('#pop', 'descriptor/convert-dots', 'descriptors/convert-dots',\n              'descriptor/convert-dots'))\n        ],\n        'label': [\n            include('default'),\n            (_name, Name.Label, '#pop')\n        ],\n        'method': [\n            include('default'),\n            (r'(%s)(\\()' % _name, bygroups(Name.Function, Punctuation),\n             ('#pop', 'descriptor/convert-dots', 'descriptors/convert-dots',\n              'descriptor/convert-dots'))\n        ],\n        'no-verification': [\n            (r'(locals|method|stack)%s' % _break, Keyword.Reserved, '#pop'),\n            include('default')\n        ],\n        'static': [\n            include('default'),\n            (r'((?:%s[/.](?=[^%s]*[/.]))*)(%s[/.])?(%s)' %\n             (_unqualified_name, _separator, _unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class, Name.Variable.Class), '#pop')\n        ],\n        'table': [\n            (r'\\n+', Text),\n            (r'default%s' % _break, Keyword.Reserved, '#pop'),\n            include('default'),\n            (_name, Name.Label)\n        ],\n        'var': [\n            include('default'),\n            (_name, Name.Variable, '#pop')\n        ],\n        'verification': [\n            include('default'),\n            (r'(Double|Float|Integer|Long|Null|Top|UninitializedThis)%s' %\n             _break, Keyword, '#pop'),\n            (r'Object%s' % _break, Keyword, ('#pop', 'class/no-dots')),\n            (r'Uninitialized%s' % _break, Keyword, ('#pop', 'label'))\n        ]\n    }\n\n    def analyse_text(text):\n        score = 0\n        if re.search(r'^\\s*\\.class\\s', text, re.MULTILINE):\n            score += 0.5\n            if re.search(r'^\\s*[a-z]+_[a-z]+\\b', text, re.MULTILINE):\n                score += 0.3\n        if re.search(r'^\\s*\\.(attribute|bytecode|debug|deprecated|enclosing|'\n                     r'inner|interface|limit|set|signature|stack)\\b', text,\n                     re.MULTILINE):\n            score += 0.6\n        return score\n\n\nclass SarlLexer(RegexLexer):\n    \"\"\"\n    For `SARL <http://www.sarl.io>`_ source code.\n\n    .. versionadded:: 2.4\n    \"\"\"\n\n    name = 'SARL'\n    aliases = ['sarl']\n    filenames = ['*.sarl']\n    mimetypes = ['text/x-sarl']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_$][\\w$]*)'                      # method name\n             r'(\\s*)(\\()',                             # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w.]*', Name.Decorator),\n            (r'(as|break|case|catch|default|do|else|extends|extension|finally|'\n             r'fires|for|if|implements|instanceof|new|on|requires|return|super|'\n             r'switch|throw|throws|try|typeof|uses|while|with)\\b',\n             Keyword),\n            (r'(abstract|def|dispatch|final|native|override|private|protected|'\n             r'public|static|strictfp|synchronized|transient|val|var|volatile)\\b',\n             Keyword.Declaration),\n            (r'(boolean|byte|char|double|float|int|long|short|void)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(false|it|null|occurrence|this|true|void)\\b', Keyword.Constant),\n            (r'(agent|annotation|artifact|behavior|capacity|class|enum|event|'\n             r'interface|skill|space)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'class'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_$]\\w*', Name),\n            (r'[~^*!%&\\[\\](){}<>\\|+=:;,./?-]', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'[a-zA-Z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n    }\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.matlab\n    ~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for Matlab and related languages.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexer import Lexer, RegexLexer, bygroups, default, words, \\\n    do_insertions\nfrom pygments.token import Text, Comment, Operator, Keyword, Name, String, \\\n    Number, Punctuation, Generic, Whitespace\n\nfrom pygments.lexers import _scilab_builtins\n\n__all__ = ['MatlabLexer', 'MatlabSessionLexer', 'OctaveLexer', 'ScilabLexer']\n\n\nclass MatlabLexer(RegexLexer):\n    \"\"\"\n    For Matlab source code.\n\n    .. versionadded:: 0.10\n    \"\"\"\n    name = 'Matlab'\n    aliases = ['matlab']\n    filenames = ['*.m']\n    mimetypes = ['text/matlab']\n\n    #\n    # These lists are generated automatically.\n    # Run the following in bash shell:\n    #\n    # for f in elfun specfun elmat; do\n    #   echo -n \"$f = \"\n    #   matlab -nojvm -r \"help $f;exit;\" | perl -ne \\\n    #   'push(@c,$1) if /^    (\\w+)\\s+-/; END {print q{[\"}.join(q{\",\"},@c).qq{\"]\\n};}'\n    # done\n    #\n    # elfun: Elementary math functions\n    # specfun: Special Math functions\n    # elmat: Elementary matrices and matrix manipulation\n    #\n    # taken from Matlab version 9.4 (R2018a)\n    #\n    elfun = (\"sin\", \"sind\", \"sinh\", \"asin\", \"asind\", \"asinh\", \"cos\", \"cosd\", \"cosh\",\n             \"acos\", \"acosd\", \"acosh\", \"tan\", \"tand\", \"tanh\", \"atan\", \"atand\", \"atan2\",\n             \"atan2d\", \"atanh\", \"sec\", \"secd\", \"sech\", \"asec\", \"asecd\", \"asech\", \"csc\", \"cscd\",\n             \"csch\", \"acsc\", \"acscd\", \"acsch\", \"cot\", \"cotd\", \"coth\", \"acot\", \"acotd\",\n             \"acoth\", \"hypot\", \"deg2rad\", \"rad2deg\", \"exp\", \"expm1\", \"log\", \"log1p\", \"log10\", \"log2\", \"pow2\",\n             \"realpow\", \"reallog\", \"realsqrt\", \"sqrt\", \"nthroot\", \"nextpow2\", \"abs\",\n             \"angle\", \"complex\", \"conj\", \"imag\", \"real\", \"unwrap\", \"isreal\", \"cplxpair\",\n             \"fix\", \"floor\", \"ceil\", \"round\", \"mod\", \"rem\", \"sign\")\n    specfun = (\"airy\", \"besselj\", \"bessely\", \"besselh\", \"besseli\", \"besselk\", \"beta\",\n               \"betainc\", \"betaincinv\", \"betaln\", \"ellipj\", \"ellipke\", \"erf\", \"erfc\", \"erfcx\",\n               \"erfinv\", \"erfcinv\", \"expint\", \"gamma\", \"gammainc\", \"gammaincinv\", \"gammaln\", \"psi\", \"legendre\",\n               \"cross\", \"dot\", \"factor\", \"isprime\", \"primes\", \"gcd\", \"lcm\", \"rat\",\n               \"rats\", \"perms\", \"nchoosek\", \"factorial\", \"cart2sph\", \"cart2pol\",\n               \"pol2cart\", \"sph2cart\", \"hsv2rgb\", \"rgb2hsv\")\n    elmat = (\"zeros\", \"ones\", \"eye\", \"repmat\", \"repelem\", \"linspace\", \"logspace\",\n             \"freqspace\", \"meshgrid\", \"accumarray\", \"size\", \"length\", \"ndims\", \"numel\",\n             \"disp\", \"isempty\", \"isequal\", \"isequaln\", \"cat\", \"reshape\",\n             \"diag\", \"blkdiag\", \"tril\", \"triu\", \"fliplr\", \"flipud\", \"flip\", \"rot90\",\n             \"find\", \"end\", \"sub2ind\", \"ind2sub\", \"bsxfun\", \"ndgrid\", \"permute\",\n             \"ipermute\", \"shiftdim\", \"circshift\", \"squeeze\", \"isscalar\", \"isvector\",\n             \"isrow\", \"iscolumn\", \"ismatrix\", \"eps\", \"realmax\", \"realmin\", \"intmax\", \"intmin\", \"flintmax\", \"pi\", \"i\", \"inf\", \"nan\", \"isnan\",\n             \"isinf\", \"isfinite\", \"j\", \"true\", \"false\", \"compan\", \"gallery\", \"hadamard\", \"hankel\",\n             \"hilb\", \"invhilb\", \"magic\", \"pascal\", \"rosser\", \"toeplitz\", \"vander\",\n             \"wilkinson\")\n\n    _operators = r'-|==|~=|<=|>=|<|>|&&|&|~|\\|\\|?|\\.\\*|\\*|\\+|\\.\\^|\\.\\\\|\\./|/|\\\\'\n\n    tokens = {\n        'root': [\n            # line starting with '!' is sent as a system command.  not sure what\n            # label to use...\n            (r'^!.*', String.Other),\n            (r'%\\{\\s*\\n', Comment.Multiline, 'blockcomment'),\n            (r'%.*$', Comment),\n            (r'^\\s*function\\b', Keyword, 'deffunc'),\n\n            # from 'iskeyword' on version 9.4 (R2018a):\n            # Check that there is no preceding dot, as keywords are valid field\n            # names.\n            (words(('break', 'case', 'catch', 'classdef', 'continue', 'else',\n                    'elseif', 'end', 'for', 'function',\n                    'global', 'if', 'otherwise', 'parfor',\n                    'persistent', 'return', 'spmd', 'switch',\n                    'try', 'while'),\n                   prefix=r'(?<!\\.)', suffix=r'\\b'),\n             Keyword),\n\n            (\"(\" + \"|\".join(elfun + specfun + elmat) + r')\\b',  Name.Builtin),\n\n            # line continuation with following comment:\n            (r'(\\.\\.\\.)(.*)$', bygroups(Keyword, Comment)),\n\n            # command form:\n            # \"How MATLAB Recognizes Command Syntax\" specifies that an operator\n            # is recognized if it is either surrounded by spaces or by no\n            # spaces on both sides; only the former case matters for us.  (This\n            # allows distinguishing `cd ./foo` from `cd ./ foo`.)\n            (r'(?:^|(?<=;))(\\s*)(\\w+)(\\s+)(?!=|\\(|(?:%s)\\s+)' % _operators,\n             bygroups(Text, Name, Text), 'commandargs'),\n\n            # operators:\n            (_operators, Operator),\n\n            # numbers (must come before punctuation to handle `.5`; cannot use\n            # `\\b` due to e.g. `5. + .5`).\n            (r'(?<!\\w)((\\d+\\.\\d*)|(\\d*\\.\\d+))([eEf][+-]?\\d+)?(?!\\w)', Number.Float),\n            (r'\\b\\d+[eEf][+-]?[0-9]+\\b', Number.Float),\n            (r'\\b\\d+\\b', Number.Integer),\n\n            # punctuation:\n            (r'\\[|\\]|\\(|\\)|\\{|\\}|:|@|\\.|,', Punctuation),\n            (r'=|:|;', Punctuation),\n\n            # quote can be transpose, instead of string:\n            # (not great, but handles common cases...)\n            (r'(?<=[\\w)\\].])\\'+', Operator),\n\n            (r'\"(\"\"|[^\"])*\"', String),\n\n            (r'(?<![\\w)\\].])\\'', String, 'string'),\n            (r'[a-zA-Z_]\\w*', Name),\n            (r'.', Text),\n        ],\n        'blockcomment': [\n            (r'^\\s*%\\}', Comment.Multiline, '#pop'),\n            (r'^.*\\n', Comment.Multiline),\n            (r'.', Comment.Multiline),\n        ],\n        'deffunc': [\n            (r'(\\s*)(?:(.+)(\\s*)(=)(\\s*))?(.+)(\\()(.*)(\\))(\\s*)',\n             bygroups(Whitespace, Text, Whitespace, Punctuation,\n                      Whitespace, Name.Function, Punctuation, Text,\n                      Punctuation, Whitespace), '#pop'),\n            # function with no args\n            (r'(\\s*)([a-zA-Z_]\\w*)', bygroups(Text, Name.Function), '#pop'),\n        ],\n        'string': [\n            (r\"[^']*'\", String, '#pop'),\n        ],\n        'commandargs': [\n            # If an equal sign or other operator is encountered, this\n            # isn't a command. It might be a variable assignment or\n            # comparison operation with multiple spaces before the\n            # equal sign or operator\n            (r\"=\", Punctuation, '#pop'),\n            (_operators, Operator, '#pop'),\n            (r\"[ \\t]+\", Text),\n            (\"'[^']*'\", String),\n            (r\"[^';\\s]+\", String),\n            (\";\", Punctuation, '#pop'),\n            default('#pop'),\n        ]\n    }\n\n    def analyse_text(text):\n        # function declaration.\n        first_non_comment = next((line for line in text.splitlines()\n                                  if not re.match(r'^\\s*%', text)), '').strip()\n        if (first_non_comment.startswith('function')\n                and '{' not in first_non_comment):\n            return 1.\n        # comment\n        elif re.search(r'^\\s*%', text, re.M):\n            return 0.2\n        # system cmd\n        elif re.search(r'^!\\w+', text, re.M):\n            return 0.2\n\n\nline_re  = re.compile('.*?\\n')\n\n\nclass MatlabSessionLexer(Lexer):\n    \"\"\"\n    For Matlab sessions.  Modeled after PythonConsoleLexer.\n    Contributed by Ken Schutte <kschutte@csail.mit.edu>.\n\n    .. versionadded:: 0.10\n    \"\"\"\n    name = 'Matlab session'\n    aliases = ['matlabsession']\n\n    def get_tokens_unprocessed(self, text):\n        mlexer = MatlabLexer(**self.options)\n\n        curcode = ''\n        insertions = []\n        continuation = False\n\n        for match in line_re.finditer(text):\n            line = match.group()\n\n            if line.startswith('>> '):\n                insertions.append((len(curcode),\n                                   [(0, Generic.Prompt, line[:3])]))\n                curcode += line[3:]\n\n            elif line.startswith('>>'):\n                insertions.append((len(curcode),\n                                   [(0, Generic.Prompt, line[:2])]))\n                curcode += line[2:]\n\n            elif line.startswith('???'):\n\n                idx = len(curcode)\n\n                # without is showing error on same line as before...?\n                # line = \"\\n\" + line\n                token = (0, Generic.Traceback, line)\n                insertions.append((idx, [token]))\n            elif continuation:\n                # line_start is the length of the most recent prompt symbol\n                line_start = len(insertions[-1][-1][-1])\n                # Set leading spaces with the length of the prompt to be a generic prompt\n                # This keeps code aligned when prompts are removed, say with some Javascript\n                if line.startswith(' '*line_start):\n                    insertions.append((len(curcode),\n                                    [(0, Generic.Prompt, line[:line_start])]))\n                    curcode += line[line_start:]\n                else:\n                    curcode += line\n            else:\n                if curcode:\n                    yield from do_insertions(\n                        insertions, mlexer.get_tokens_unprocessed(curcode))\n                    curcode = ''\n                    insertions = []\n\n                yield match.start(), Generic.Output, line\n\n            # Does not allow continuation if a comment is included after the ellipses.\n            # Continues any line that ends with ..., even comments (lines that start with %)\n            if line.strip().endswith('...'):\n                continuation = True\n            else:\n                continuation = False\n\n        if curcode:  # or item:\n            yield from do_insertions(\n                insertions, mlexer.get_tokens_unprocessed(curcode))\n\n\nclass OctaveLexer(RegexLexer):\n    \"\"\"\n    For GNU Octave source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n    name = 'Octave'\n    aliases = ['octave']\n    filenames = ['*.m']\n    mimetypes = ['text/octave']\n\n    # These lists are generated automatically.\n    # Run the following in bash shell:\n    #\n    # First dump all of the Octave manual into a plain text file:\n    #\n    #   $ info octave --subnodes -o octave-manual\n    #\n    # Now grep through it:\n\n    # for i in \\\n    #     \"Built-in Function\" \"Command\" \"Function File\" \\\n    #     \"Loadable Function\" \"Mapping Function\";\n    # do\n    #     perl -e '@name = qw('\"$i\"');\n    #              print lc($name[0]),\"_kw = [\\n\"';\n    #\n    #     perl -n -e 'print \"\\\"$1\\\",\\n\" if /-- '\"$i\"': .* (\\w*) \\(/;' \\\n    #         octave-manual | sort | uniq ;\n    #     echo \"]\" ;\n    #     echo;\n    # done\n\n    # taken from Octave Mercurial changeset 8cc154f45e37 (30-jan-2011)\n\n    builtin_kw = (\n        \"addlistener\", \"addpath\", \"addproperty\", \"all\",\n        \"and\", \"any\", \"argnames\", \"argv\", \"assignin\",\n        \"atexit\", \"autoload\",\n        \"available_graphics_toolkits\", \"beep_on_error\",\n        \"bitand\", \"bitmax\", \"bitor\", \"bitshift\", \"bitxor\",\n        \"cat\", \"cell\", \"cellstr\", \"char\", \"class\", \"clc\",\n        \"columns\", \"command_line_path\",\n        \"completion_append_char\", \"completion_matches\",\n        \"complex\", \"confirm_recursive_rmdir\", \"cputime\",\n        \"crash_dumps_octave_core\", \"ctranspose\", \"cumprod\",\n        \"cumsum\", \"debug_on_error\", \"debug_on_interrupt\",\n        \"debug_on_warning\", \"default_save_options\",\n        \"dellistener\", \"diag\", \"diff\", \"disp\",\n        \"doc_cache_file\", \"do_string_escapes\", \"double\",\n        \"drawnow\", \"e\", \"echo_executing_commands\", \"eps\",\n        \"eq\", \"errno\", \"errno_list\", \"error\", \"eval\",\n        \"evalin\", \"exec\", \"exist\", \"exit\", \"eye\", \"false\",\n        \"fclear\", \"fclose\", \"fcntl\", \"fdisp\", \"feof\",\n        \"ferror\", \"feval\", \"fflush\", \"fgetl\", \"fgets\",\n        \"fieldnames\", \"file_in_loadpath\", \"file_in_path\",\n        \"filemarker\", \"filesep\", \"find_dir_in_path\",\n        \"fixed_point_format\", \"fnmatch\", \"fopen\", \"fork\",\n        \"formula\", \"fprintf\", \"fputs\", \"fread\", \"freport\",\n        \"frewind\", \"fscanf\", \"fseek\", \"fskipl\", \"ftell\",\n        \"functions\", \"fwrite\", \"ge\", \"genpath\", \"get\",\n        \"getegid\", \"getenv\", \"geteuid\", \"getgid\",\n        \"getpgrp\", \"getpid\", \"getppid\", \"getuid\", \"glob\",\n        \"gt\", \"gui_mode\", \"history_control\",\n        \"history_file\", \"history_size\",\n        \"history_timestamp_format_string\", \"home\",\n        \"horzcat\", \"hypot\", \"ifelse\",\n        \"ignore_function_time_stamp\", \"inferiorto\",\n        \"info_file\", \"info_program\", \"inline\", \"input\",\n        \"intmax\", \"intmin\", \"ipermute\",\n        \"is_absolute_filename\", \"isargout\", \"isbool\",\n        \"iscell\", \"iscellstr\", \"ischar\", \"iscomplex\",\n        \"isempty\", \"isfield\", \"isfloat\", \"isglobal\",\n        \"ishandle\", \"isieee\", \"isindex\", \"isinteger\",\n        \"islogical\", \"ismatrix\", \"ismethod\", \"isnull\",\n        \"isnumeric\", \"isobject\", \"isreal\",\n        \"is_rooted_relative_filename\", \"issorted\",\n        \"isstruct\", \"isvarname\", \"kbhit\", \"keyboard\",\n        \"kill\", \"lasterr\", \"lasterror\", \"lastwarn\",\n        \"ldivide\", \"le\", \"length\", \"link\", \"linspace\",\n        \"logical\", \"lstat\", \"lt\", \"make_absolute_filename\",\n        \"makeinfo_program\", \"max_recursion_depth\", \"merge\",\n        \"methods\", \"mfilename\", \"minus\", \"mislocked\",\n        \"mkdir\", \"mkfifo\", \"mkstemp\", \"mldivide\", \"mlock\",\n        \"mouse_wheel_zoom\", \"mpower\", \"mrdivide\", \"mtimes\",\n        \"munlock\", \"nargin\", \"nargout\",\n        \"native_float_format\", \"ndims\", \"ne\", \"nfields\",\n        \"nnz\", \"norm\", \"not\", \"numel\", \"nzmax\",\n        \"octave_config_info\", \"octave_core_file_limit\",\n        \"octave_core_file_name\",\n        \"octave_core_file_options\", \"ones\", \"or\",\n        \"output_max_field_width\", \"output_precision\",\n        \"page_output_immediately\", \"page_screen_output\",\n        \"path\", \"pathsep\", \"pause\", \"pclose\", \"permute\",\n        \"pi\", \"pipe\", \"plus\", \"popen\", \"power\",\n        \"print_empty_dimensions\", \"printf\",\n        \"print_struct_array_contents\", \"prod\",\n        \"program_invocation_name\", \"program_name\",\n        \"putenv\", \"puts\", \"pwd\", \"quit\", \"rats\", \"rdivide\",\n        \"readdir\", \"readlink\", \"read_readline_init_file\",\n        \"realmax\", \"realmin\", \"rehash\", \"rename\",\n        \"repelems\", \"re_read_readline_init_file\", \"reset\",\n        \"reshape\", \"resize\", \"restoredefaultpath\",\n        \"rethrow\", \"rmdir\", \"rmfield\", \"rmpath\", \"rows\",\n        \"save_header_format_string\", \"save_precision\",\n        \"saving_history\", \"scanf\", \"set\", \"setenv\",\n        \"shell_cmd\", \"sighup_dumps_octave_core\",\n        \"sigterm_dumps_octave_core\", \"silent_functions\",\n        \"single\", \"size\", \"size_equal\", \"sizemax\",\n        \"sizeof\", \"sleep\", \"source\", \"sparse_auto_mutate\",\n        \"split_long_rows\", \"sprintf\", \"squeeze\", \"sscanf\",\n        \"stat\", \"stderr\", \"stdin\", \"stdout\", \"strcmp\",\n        \"strcmpi\", \"string_fill_char\", \"strncmp\",\n        \"strncmpi\", \"struct\", \"struct_levels_to_print\",\n        \"strvcat\", \"subsasgn\", \"subsref\", \"sum\", \"sumsq\",\n        \"superiorto\", \"suppress_verbose_help_message\",\n        \"symlink\", \"system\", \"tic\", \"tilde_expand\",\n        \"times\", \"tmpfile\", \"tmpnam\", \"toc\", \"toupper\",\n        \"transpose\", \"true\", \"typeinfo\", \"umask\", \"uminus\",\n        \"uname\", \"undo_string_escapes\", \"unlink\", \"uplus\",\n        \"upper\", \"usage\", \"usleep\", \"vec\", \"vectorize\",\n        \"vertcat\", \"waitpid\", \"warning\", \"warranty\",\n        \"whos_line_format\", \"yes_or_no\", \"zeros\",\n        \"inf\", \"Inf\", \"nan\", \"NaN\")\n\n    command_kw = (\"close\", \"load\", \"who\", \"whos\")\n\n    function_kw = (\n        \"accumarray\", \"accumdim\", \"acosd\", \"acotd\",\n        \"acscd\", \"addtodate\", \"allchild\", \"ancestor\",\n        \"anova\", \"arch_fit\", \"arch_rnd\", \"arch_test\",\n        \"area\", \"arma_rnd\", \"arrayfun\", \"ascii\", \"asctime\",\n        \"asecd\", \"asind\", \"assert\", \"atand\",\n        \"autoreg_matrix\", \"autumn\", \"axes\", \"axis\", \"bar\",\n        \"barh\", \"bartlett\", \"bartlett_test\", \"beep\",\n        \"betacdf\", \"betainv\", \"betapdf\", \"betarnd\",\n        \"bicgstab\", \"bicubic\", \"binary\", \"binocdf\",\n        \"binoinv\", \"binopdf\", \"binornd\", \"bitcmp\",\n        \"bitget\", \"bitset\", \"blackman\", \"blanks\",\n        \"blkdiag\", \"bone\", \"box\", \"brighten\", \"calendar\",\n        \"cast\", \"cauchy_cdf\", \"cauchy_inv\", \"cauchy_pdf\",\n        \"cauchy_rnd\", \"caxis\", \"celldisp\", \"center\", \"cgs\",\n        \"chisquare_test_homogeneity\",\n        \"chisquare_test_independence\", \"circshift\", \"cla\",\n        \"clabel\", \"clf\", \"clock\", \"cloglog\", \"closereq\",\n        \"colon\", \"colorbar\", \"colormap\", \"colperm\",\n        \"comet\", \"common_size\", \"commutation_matrix\",\n        \"compan\", \"compare_versions\", \"compass\",\n        \"computer\", \"cond\", \"condest\", \"contour\",\n        \"contourc\", \"contourf\", \"contrast\", \"conv\",\n        \"convhull\", \"cool\", \"copper\", \"copyfile\", \"cor\",\n        \"corrcoef\", \"cor_test\", \"cosd\", \"cotd\", \"cov\",\n        \"cplxpair\", \"cross\", \"cscd\", \"cstrcat\", \"csvread\",\n        \"csvwrite\", \"ctime\", \"cumtrapz\", \"curl\", \"cut\",\n        \"cylinder\", \"date\", \"datenum\", \"datestr\",\n        \"datetick\", \"datevec\", \"dblquad\", \"deal\",\n        \"deblank\", \"deconv\", \"delaunay\", \"delaunayn\",\n        \"delete\", \"demo\", \"detrend\", \"diffpara\", \"diffuse\",\n        \"dir\", \"discrete_cdf\", \"discrete_inv\",\n        \"discrete_pdf\", \"discrete_rnd\", \"display\",\n        \"divergence\", \"dlmwrite\", \"dos\", \"dsearch\",\n        \"dsearchn\", \"duplication_matrix\", \"durbinlevinson\",\n        \"ellipsoid\", \"empirical_cdf\", \"empirical_inv\",\n        \"empirical_pdf\", \"empirical_rnd\", \"eomday\",\n        \"errorbar\", \"etime\", \"etreeplot\", \"example\",\n        \"expcdf\", \"expinv\", \"expm\", \"exppdf\", \"exprnd\",\n        \"ezcontour\", \"ezcontourf\", \"ezmesh\", \"ezmeshc\",\n        \"ezplot\", \"ezpolar\", \"ezsurf\", \"ezsurfc\", \"factor\",\n        \"factorial\", \"fail\", \"fcdf\", \"feather\", \"fftconv\",\n        \"fftfilt\", \"fftshift\", \"figure\", \"fileattrib\",\n        \"fileparts\", \"fill\", \"findall\", \"findobj\",\n        \"findstr\", \"finv\", \"flag\", \"flipdim\", \"fliplr\",\n        \"flipud\", \"fpdf\", \"fplot\", \"fractdiff\", \"freqz\",\n        \"freqz_plot\", \"frnd\", \"fsolve\",\n        \"f_test_regression\", \"ftp\", \"fullfile\", \"fzero\",\n        \"gamcdf\", \"gaminv\", \"gampdf\", \"gamrnd\", \"gca\",\n        \"gcbf\", \"gcbo\", \"gcf\", \"genvarname\", \"geocdf\",\n        \"geoinv\", \"geopdf\", \"geornd\", \"getfield\", \"ginput\",\n        \"glpk\", \"gls\", \"gplot\", \"gradient\",\n        \"graphics_toolkit\", \"gray\", \"grid\", \"griddata\",\n        \"griddatan\", \"gtext\", \"gunzip\", \"gzip\", \"hadamard\",\n        \"hamming\", \"hankel\", \"hanning\", \"hggroup\",\n        \"hidden\", \"hilb\", \"hist\", \"histc\", \"hold\", \"hot\",\n        \"hotelling_test\", \"housh\", \"hsv\", \"hurst\",\n        \"hygecdf\", \"hygeinv\", \"hygepdf\", \"hygernd\",\n        \"idivide\", \"ifftshift\", \"image\", \"imagesc\",\n        \"imfinfo\", \"imread\", \"imshow\", \"imwrite\", \"index\",\n        \"info\", \"inpolygon\", \"inputname\", \"interpft\",\n        \"interpn\", \"intersect\", \"invhilb\", \"iqr\", \"isa\",\n        \"isdefinite\", \"isdir\", \"is_duplicate_entry\",\n        \"isequal\", \"isequalwithequalnans\", \"isfigure\",\n        \"ishermitian\", \"ishghandle\", \"is_leap_year\",\n        \"isletter\", \"ismac\", \"ismember\", \"ispc\", \"isprime\",\n        \"isprop\", \"isscalar\", \"issquare\", \"isstrprop\",\n        \"issymmetric\", \"isunix\", \"is_valid_file_id\",\n        \"isvector\", \"jet\", \"kendall\",\n        \"kolmogorov_smirnov_cdf\",\n        \"kolmogorov_smirnov_test\", \"kruskal_wallis_test\",\n        \"krylov\", \"kurtosis\", \"laplace_cdf\", \"laplace_inv\",\n        \"laplace_pdf\", \"laplace_rnd\", \"legend\", \"legendre\",\n        \"license\", \"line\", \"linkprop\", \"list_primes\",\n        \"loadaudio\", \"loadobj\", \"logistic_cdf\",\n        \"logistic_inv\", \"logistic_pdf\", \"logistic_rnd\",\n        \"logit\", \"loglog\", \"loglogerr\", \"logm\", \"logncdf\",\n        \"logninv\", \"lognpdf\", \"lognrnd\", \"logspace\",\n        \"lookfor\", \"ls_command\", \"lsqnonneg\", \"magic\",\n        \"mahalanobis\", \"manova\", \"matlabroot\",\n        \"mcnemar_test\", \"mean\", \"meansq\", \"median\", \"menu\",\n        \"mesh\", \"meshc\", \"meshgrid\", \"meshz\", \"mexext\",\n        \"mget\", \"mkpp\", \"mode\", \"moment\", \"movefile\",\n        \"mpoles\", \"mput\", \"namelengthmax\", \"nargchk\",\n        \"nargoutchk\", \"nbincdf\", \"nbininv\", \"nbinpdf\",\n        \"nbinrnd\", \"nchoosek\", \"ndgrid\", \"newplot\", \"news\",\n        \"nonzeros\", \"normcdf\", \"normest\", \"norminv\",\n        \"normpdf\", \"normrnd\", \"now\", \"nthroot\", \"null\",\n        \"ocean\", \"ols\", \"onenormest\", \"optimget\",\n        \"optimset\", \"orderfields\", \"orient\", \"orth\",\n        \"pack\", \"pareto\", \"parseparams\", \"pascal\", \"patch\",\n        \"pathdef\", \"pcg\", \"pchip\", \"pcolor\", \"pcr\",\n        \"peaks\", \"periodogram\", \"perl\", \"perms\", \"pie\",\n        \"pink\", \"planerot\", \"playaudio\", \"plot\",\n        \"plotmatrix\", \"plotyy\", \"poisscdf\", \"poissinv\",\n        \"poisspdf\", \"poissrnd\", \"polar\", \"poly\",\n        \"polyaffine\", \"polyarea\", \"polyderiv\", \"polyfit\",\n        \"polygcd\", \"polyint\", \"polyout\", \"polyreduce\",\n        \"polyval\", \"polyvalm\", \"postpad\", \"powerset\",\n        \"ppder\", \"ppint\", \"ppjumps\", \"ppplot\", \"ppval\",\n        \"pqpnonneg\", \"prepad\", \"primes\", \"print\",\n        \"print_usage\", \"prism\", \"probit\", \"qp\", \"qqplot\",\n        \"quadcc\", \"quadgk\", \"quadl\", \"quadv\", \"quiver\",\n        \"qzhess\", \"rainbow\", \"randi\", \"range\", \"rank\",\n        \"ranks\", \"rat\", \"reallog\", \"realpow\", \"realsqrt\",\n        \"record\", \"rectangle_lw\", \"rectangle_sw\",\n        \"rectint\", \"refresh\", \"refreshdata\",\n        \"regexptranslate\", \"repmat\", \"residue\", \"ribbon\",\n        \"rindex\", \"roots\", \"rose\", \"rosser\", \"rotdim\",\n        \"rref\", \"run\", \"run_count\", \"rundemos\", \"run_test\",\n        \"runtests\", \"saveas\", \"saveaudio\", \"saveobj\",\n        \"savepath\", \"scatter\", \"secd\", \"semilogx\",\n        \"semilogxerr\", \"semilogy\", \"semilogyerr\",\n        \"setaudio\", \"setdiff\", \"setfield\", \"setxor\",\n        \"shading\", \"shift\", \"shiftdim\", \"sign_test\",\n        \"sinc\", \"sind\", \"sinetone\", \"sinewave\", \"skewness\",\n        \"slice\", \"sombrero\", \"sortrows\", \"spaugment\",\n        \"spconvert\", \"spdiags\", \"spearman\", \"spectral_adf\",\n        \"spectral_xdf\", \"specular\", \"speed\", \"spencer\",\n        \"speye\", \"spfun\", \"sphere\", \"spinmap\", \"spline\",\n        \"spones\", \"sprand\", \"sprandn\", \"sprandsym\",\n        \"spring\", \"spstats\", \"spy\", \"sqp\", \"stairs\",\n        \"statistics\", \"std\", \"stdnormal_cdf\",\n        \"stdnormal_inv\", \"stdnormal_pdf\", \"stdnormal_rnd\",\n        \"stem\", \"stft\", \"strcat\", \"strchr\", \"strjust\",\n        \"strmatch\", \"strread\", \"strsplit\", \"strtok\",\n        \"strtrim\", \"strtrunc\", \"structfun\", \"studentize\",\n        \"subplot\", \"subsindex\", \"subspace\", \"substr\",\n        \"substruct\", \"summer\", \"surf\", \"surface\", \"surfc\",\n        \"surfl\", \"surfnorm\", \"svds\", \"swapbytes\",\n        \"sylvester_matrix\", \"symvar\", \"synthesis\", \"table\",\n        \"tand\", \"tar\", \"tcdf\", \"tempdir\", \"tempname\",\n        \"test\", \"text\", \"textread\", \"textscan\", \"tinv\",\n        \"title\", \"toeplitz\", \"tpdf\", \"trace\", \"trapz\",\n        \"treelayout\", \"treeplot\", \"triangle_lw\",\n        \"triangle_sw\", \"tril\", \"trimesh\", \"triplequad\",\n        \"triplot\", \"trisurf\", \"triu\", \"trnd\", \"tsearchn\",\n        \"t_test\", \"t_test_regression\", \"type\", \"unidcdf\",\n        \"unidinv\", \"unidpdf\", \"unidrnd\", \"unifcdf\",\n        \"unifinv\", \"unifpdf\", \"unifrnd\", \"union\", \"unique\",\n        \"unix\", \"unmkpp\", \"unpack\", \"untabify\", \"untar\",\n        \"unwrap\", \"unzip\", \"u_test\", \"validatestring\",\n        \"vander\", \"var\", \"var_test\", \"vech\", \"ver\",\n        \"version\", \"view\", \"voronoi\", \"voronoin\",\n        \"waitforbuttonpress\", \"wavread\", \"wavwrite\",\n        \"wblcdf\", \"wblinv\", \"wblpdf\", \"wblrnd\", \"weekday\",\n        \"welch_test\", \"what\", \"white\", \"whitebg\",\n        \"wienrnd\", \"wilcoxon_test\", \"wilkinson\", \"winter\",\n        \"xlabel\", \"xlim\", \"ylabel\", \"yulewalker\", \"zip\",\n        \"zlabel\", \"z_test\")\n\n    loadable_kw = (\n        \"airy\", \"amd\", \"balance\", \"besselh\", \"besseli\",\n        \"besselj\", \"besselk\", \"bessely\", \"bitpack\",\n        \"bsxfun\", \"builtin\", \"ccolamd\", \"cellfun\",\n        \"cellslices\", \"chol\", \"choldelete\", \"cholinsert\",\n        \"cholinv\", \"cholshift\", \"cholupdate\", \"colamd\",\n        \"colloc\", \"convhulln\", \"convn\", \"csymamd\",\n        \"cummax\", \"cummin\", \"daspk\", \"daspk_options\",\n        \"dasrt\", \"dasrt_options\", \"dassl\", \"dassl_options\",\n        \"dbclear\", \"dbdown\", \"dbstack\", \"dbstatus\",\n        \"dbstop\", \"dbtype\", \"dbup\", \"dbwhere\", \"det\",\n        \"dlmread\", \"dmperm\", \"dot\", \"eig\", \"eigs\",\n        \"endgrent\", \"endpwent\", \"etree\", \"fft\", \"fftn\",\n        \"fftw\", \"filter\", \"find\", \"full\", \"gcd\",\n        \"getgrent\", \"getgrgid\", \"getgrnam\", \"getpwent\",\n        \"getpwnam\", \"getpwuid\", \"getrusage\", \"givens\",\n        \"gmtime\", \"gnuplot_binary\", \"hess\", \"ifft\",\n        \"ifftn\", \"inv\", \"isdebugmode\", \"issparse\", \"kron\",\n        \"localtime\", \"lookup\", \"lsode\", \"lsode_options\",\n        \"lu\", \"luinc\", \"luupdate\", \"matrix_type\", \"max\",\n        \"min\", \"mktime\", \"pinv\", \"qr\", \"qrdelete\",\n        \"qrinsert\", \"qrshift\", \"qrupdate\", \"quad\",\n        \"quad_options\", \"qz\", \"rand\", \"rande\", \"randg\",\n        \"randn\", \"randp\", \"randperm\", \"rcond\", \"regexp\",\n        \"regexpi\", \"regexprep\", \"schur\", \"setgrent\",\n        \"setpwent\", \"sort\", \"spalloc\", \"sparse\", \"spparms\",\n        \"sprank\", \"sqrtm\", \"strfind\", \"strftime\",\n        \"strptime\", \"strrep\", \"svd\", \"svd_driver\", \"syl\",\n        \"symamd\", \"symbfact\", \"symrcm\", \"time\", \"tsearch\",\n        \"typecast\", \"urlread\", \"urlwrite\")\n\n    mapping_kw = (\n        \"abs\", \"acos\", \"acosh\", \"acot\", \"acoth\", \"acsc\",\n        \"acsch\", \"angle\", \"arg\", \"asec\", \"asech\", \"asin\",\n        \"asinh\", \"atan\", \"atanh\", \"beta\", \"betainc\",\n        \"betaln\", \"bincoeff\", \"cbrt\", \"ceil\", \"conj\", \"cos\",\n        \"cosh\", \"cot\", \"coth\", \"csc\", \"csch\", \"erf\", \"erfc\",\n        \"erfcx\", \"erfinv\", \"exp\", \"finite\", \"fix\", \"floor\",\n        \"fmod\", \"gamma\", \"gammainc\", \"gammaln\", \"imag\",\n        \"isalnum\", \"isalpha\", \"isascii\", \"iscntrl\",\n        \"isdigit\", \"isfinite\", \"isgraph\", \"isinf\",\n        \"islower\", \"isna\", \"isnan\", \"isprint\", \"ispunct\",\n        \"isspace\", \"isupper\", \"isxdigit\", \"lcm\", \"lgamma\",\n        \"log\", \"lower\", \"mod\", \"real\", \"rem\", \"round\",\n        \"roundb\", \"sec\", \"sech\", \"sign\", \"sin\", \"sinh\",\n        \"sqrt\", \"tan\", \"tanh\", \"toascii\", \"tolower\", \"xor\")\n\n    builtin_consts = (\n        \"EDITOR\", \"EXEC_PATH\", \"I\", \"IMAGE_PATH\", \"NA\",\n        \"OCTAVE_HOME\", \"OCTAVE_VERSION\", \"PAGER\",\n        \"PAGER_FLAGS\", \"SEEK_CUR\", \"SEEK_END\", \"SEEK_SET\",\n        \"SIG\", \"S_ISBLK\", \"S_ISCHR\", \"S_ISDIR\", \"S_ISFIFO\",\n        \"S_ISLNK\", \"S_ISREG\", \"S_ISSOCK\", \"WCONTINUE\",\n        \"WCOREDUMP\", \"WEXITSTATUS\", \"WIFCONTINUED\",\n        \"WIFEXITED\", \"WIFSIGNALED\", \"WIFSTOPPED\", \"WNOHANG\",\n        \"WSTOPSIG\", \"WTERMSIG\", \"WUNTRACED\")\n\n    tokens = {\n        'root': [\n            # We should look into multiline comments\n            (r'[%#].*$', Comment),\n            (r'^\\s*function\\b', Keyword, 'deffunc'),\n\n            # from 'iskeyword' on hg changeset 8cc154f45e37\n            (words((\n                '__FILE__', '__LINE__', 'break', 'case', 'catch', 'classdef', 'continue', 'do', 'else',\n                'elseif', 'end', 'end_try_catch', 'end_unwind_protect', 'endclassdef',\n                'endevents', 'endfor', 'endfunction', 'endif', 'endmethods', 'endproperties',\n                'endswitch', 'endwhile', 'events', 'for', 'function', 'get', 'global', 'if', 'methods',\n                'otherwise', 'persistent', 'properties', 'return', 'set', 'static', 'switch', 'try',\n                'until', 'unwind_protect', 'unwind_protect_cleanup', 'while'), suffix=r'\\b'),\n             Keyword),\n\n            (words(builtin_kw + command_kw + function_kw + loadable_kw + mapping_kw,\n                   suffix=r'\\b'),  Name.Builtin),\n\n            (words(builtin_consts, suffix=r'\\b'), Name.Constant),\n\n            # operators in Octave but not Matlab:\n            (r'-=|!=|!|/=|--', Operator),\n            # operators:\n            (r'-|==|~=|<|>|<=|>=|&&|&|~|\\|\\|?', Operator),\n            # operators in Octave but not Matlab requiring escape for re:\n            (r'\\*=|\\+=|\\^=|\\/=|\\\\=|\\*\\*|\\+\\+|\\.\\*\\*', Operator),\n            # operators requiring escape for re:\n            (r'\\.\\*|\\*|\\+|\\.\\^|\\.\\\\|\\.\\/|\\/|\\\\', Operator),\n\n\n            # punctuation:\n            (r'[\\[\\](){}:@.,]', Punctuation),\n            (r'=|:|;', Punctuation),\n\n            (r'\"[^\"]*\"', String),\n\n            (r'(\\d+\\.\\d*|\\d*\\.\\d+)([eEf][+-]?[0-9]+)?', Number.Float),\n            (r'\\d+[eEf][+-]?[0-9]+', Number.Float),\n            (r'\\d+', Number.Integer),\n\n            # quote can be transpose, instead of string:\n            # (not great, but handles common cases...)\n            (r'(?<=[\\w)\\].])\\'+', Operator),\n            (r'(?<![\\w)\\].])\\'', String, 'string'),\n\n            (r'[a-zA-Z_]\\w*', Name),\n            (r'.', Text),\n        ],\n        'string': [\n            (r\"[^']*'\", String, '#pop'),\n        ],\n        'deffunc': [\n            (r'(\\s*)(?:(.+)(\\s*)(=)(\\s*))?(.+)(\\()(.*)(\\))(\\s*)',\n             bygroups(Whitespace, Text, Whitespace, Punctuation,\n                      Whitespace, Name.Function, Punctuation, Text,\n                      Punctuation, Whitespace), '#pop'),\n            # function with no args\n            (r'(\\s*)([a-zA-Z_]\\w*)', bygroups(Text, Name.Function), '#pop'),\n        ],\n    }\n\n    def analyse_text(text):\n        \"\"\"Octave is quite hard to spot, and it looks like Matlab as well.\"\"\"\n        return 0\n\n\nclass ScilabLexer(RegexLexer):\n    \"\"\"\n    For Scilab source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n    name = 'Scilab'\n    aliases = ['scilab']\n    filenames = ['*.sci', '*.sce', '*.tst']\n    mimetypes = ['text/scilab']\n\n    tokens = {\n        'root': [\n            (r'//.*?$', Comment.Single),\n            (r'^\\s*function\\b', Keyword, 'deffunc'),\n\n            (words((\n                '__FILE__', '__LINE__', 'break', 'case', 'catch', 'classdef', 'continue', 'do', 'else',\n                'elseif', 'end', 'end_try_catch', 'end_unwind_protect', 'endclassdef',\n                'endevents', 'endfor', 'endfunction', 'endif', 'endmethods', 'endproperties',\n                'endswitch', 'endwhile', 'events', 'for', 'function', 'get', 'global', 'if', 'methods',\n                'otherwise', 'persistent', 'properties', 'return', 'set', 'static', 'switch', 'try',\n                'until', 'unwind_protect', 'unwind_protect_cleanup', 'while'), suffix=r'\\b'),\n             Keyword),\n\n            (words(_scilab_builtins.functions_kw +\n                   _scilab_builtins.commands_kw +\n                   _scilab_builtins.macros_kw, suffix=r'\\b'), Name.Builtin),\n\n            (words(_scilab_builtins.variables_kw, suffix=r'\\b'), Name.Constant),\n\n            # operators:\n            (r'-|==|~=|<|>|<=|>=|&&|&|~|\\|\\|?', Operator),\n            # operators requiring escape for re:\n            (r'\\.\\*|\\*|\\+|\\.\\^|\\.\\\\|\\.\\/|\\/|\\\\', Operator),\n\n            # punctuation:\n            (r'[\\[\\](){}@.,=:;]', Punctuation),\n\n            (r'\"[^\"]*\"', String),\n\n            # quote can be transpose, instead of string:\n            # (not great, but handles common cases...)\n            (r'(?<=[\\w)\\].])\\'+', Operator),\n            (r'(?<![\\w)\\].])\\'', String, 'string'),\n\n            (r'(\\d+\\.\\d*|\\d*\\.\\d+)([eEf][+-]?[0-9]+)?', Number.Float),\n            (r'\\d+[eEf][+-]?[0-9]+', Number.Float),\n            (r'\\d+', Number.Integer),\n\n            (r'[a-zA-Z_]\\w*', Name),\n            (r'.', Text),\n        ],\n        'string': [\n            (r\"[^']*'\", String, '#pop'),\n            (r'.', String, '#pop'),\n        ],\n        'deffunc': [\n            (r'(\\s*)(?:(.+)(\\s*)(=)(\\s*))?(.+)(\\()(.*)(\\))(\\s*)',\n             bygroups(Whitespace, Text, Whitespace, Punctuation,\n                      Whitespace, Name.Function, Punctuation, Text,\n                      Punctuation, Whitespace), '#pop'),\n            # function with no args\n            (r'(\\s*)([a-zA-Z_]\\w*)', bygroups(Text, Name.Function), '#pop'),\n        ],\n    }\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.objective\n    ~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for Objective-C family languages.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexer import RegexLexer, include, bygroups, using, this, words, \\\n    inherit, default\nfrom pygments.token import Text, Keyword, Name, String, Operator, \\\n    Number, Punctuation, Literal, Comment\n\nfrom pygments.lexers.c_cpp import CLexer, CppLexer\n\n__all__ = ['ObjectiveCLexer', 'ObjectiveCppLexer', 'LogosLexer', 'SwiftLexer']\n\n\ndef objective(baselexer):\n    \"\"\"\n    Generate a subclass of baselexer that accepts the Objective-C syntax\n    extensions.\n    \"\"\"\n\n    # Have to be careful not to accidentally match JavaDoc/Doxygen syntax here,\n    # since that's quite common in ordinary C/C++ files.  It's OK to match\n    # JavaDoc/Doxygen keywords that only apply to Objective-C, mind.\n    #\n    # The upshot of this is that we CANNOT match @class or @interface\n    _oc_keywords = re.compile(r'@(?:end|implementation|protocol)')\n\n    # Matches [ <ws>? identifier <ws> ( identifier <ws>? ] |  identifier? : )\n    # (note the identifier is *optional* when there is a ':'!)\n    _oc_message = re.compile(r'\\[\\s*[a-zA-Z_]\\w*\\s+'\n                             r'(?:[a-zA-Z_]\\w*\\s*\\]|'\n                             r'(?:[a-zA-Z_]\\w*)?:)')\n\n    class GeneratedObjectiveCVariant(baselexer):\n        \"\"\"\n        Implements Objective-C syntax on top of an existing C family lexer.\n        \"\"\"\n\n        tokens = {\n            'statements': [\n                (r'@\"', String, 'string'),\n                (r'@(YES|NO)', Number),\n                (r\"@'(\\\\.|\\\\[0-7]{1,3}|\\\\x[a-fA-F0-9]{1,2}|[^\\\\\\'\\n])'\", String.Char),\n                (r'@(\\d+\\.\\d*|\\.\\d+|\\d+)[eE][+-]?\\d+[lL]?', Number.Float),\n                (r'@(\\d+\\.\\d*|\\.\\d+|\\d+[fF])[fF]?', Number.Float),\n                (r'@0x[0-9a-fA-F]+[Ll]?', Number.Hex),\n                (r'@0[0-7]+[Ll]?', Number.Oct),\n                (r'@\\d+[Ll]?', Number.Integer),\n                (r'@\\(', Literal, 'literal_number'),\n                (r'@\\[', Literal, 'literal_array'),\n                (r'@\\{', Literal, 'literal_dictionary'),\n                (words((\n                    '@selector', '@private', '@protected', '@public', '@encode',\n                    '@synchronized', '@try', '@throw', '@catch', '@finally',\n                    '@end', '@property', '@synthesize', '__bridge', '__bridge_transfer',\n                    '__autoreleasing', '__block', '__weak', '__strong', 'weak', 'strong',\n                    'copy', 'retain', 'assign', 'unsafe_unretained', 'atomic', 'nonatomic',\n                    'readonly', 'readwrite', 'setter', 'getter', 'typeof', 'in',\n                    'out', 'inout', 'release', 'class', '@dynamic', '@optional',\n                    '@required', '@autoreleasepool', '@import'), suffix=r'\\b'),\n                 Keyword),\n                (words(('id', 'instancetype', 'Class', 'IMP', 'SEL', 'BOOL',\n                        'IBOutlet', 'IBAction', 'unichar'), suffix=r'\\b'),\n                 Keyword.Type),\n                (r'@(true|false|YES|NO)\\n', Name.Builtin),\n                (r'(YES|NO|nil|self|super)\\b', Name.Builtin),\n                # Carbon types\n                (r'(Boolean|UInt8|SInt8|UInt16|SInt16|UInt32|SInt32)\\b', Keyword.Type),\n                # Carbon built-ins\n                (r'(TRUE|FALSE)\\b', Name.Builtin),\n                (r'(@interface|@implementation)(\\s+)', bygroups(Keyword, Text),\n                 ('#pop', 'oc_classname')),\n                (r'(@class|@protocol)(\\s+)', bygroups(Keyword, Text),\n                 ('#pop', 'oc_forward_classname')),\n                # @ can also prefix other expressions like @{...} or @(...)\n                (r'@', Punctuation),\n                inherit,\n            ],\n            'oc_classname': [\n                # interface definition that inherits\n                (r'([a-zA-Z$_][\\w$]*)(\\s*:\\s*)([a-zA-Z$_][\\w$]*)?(\\s*)(\\{)',\n                 bygroups(Name.Class, Text, Name.Class, Text, Punctuation),\n                 ('#pop', 'oc_ivars')),\n                (r'([a-zA-Z$_][\\w$]*)(\\s*:\\s*)([a-zA-Z$_][\\w$]*)?',\n                 bygroups(Name.Class, Text, Name.Class), '#pop'),\n                # interface definition for a category\n                (r'([a-zA-Z$_][\\w$]*)(\\s*)(\\([a-zA-Z$_][\\w$]*\\))(\\s*)(\\{)',\n                 bygroups(Name.Class, Text, Name.Label, Text, Punctuation),\n                 ('#pop', 'oc_ivars')),\n                (r'([a-zA-Z$_][\\w$]*)(\\s*)(\\([a-zA-Z$_][\\w$]*\\))',\n                 bygroups(Name.Class, Text, Name.Label), '#pop'),\n                # simple interface / implementation\n                (r'([a-zA-Z$_][\\w$]*)(\\s*)(\\{)',\n                 bygroups(Name.Class, Text, Punctuation), ('#pop', 'oc_ivars')),\n                (r'([a-zA-Z$_][\\w$]*)', Name.Class, '#pop')\n            ],\n            'oc_forward_classname': [\n                (r'([a-zA-Z$_][\\w$]*)(\\s*,\\s*)',\n                 bygroups(Name.Class, Text), 'oc_forward_classname'),\n                (r'([a-zA-Z$_][\\w$]*)(\\s*;?)',\n                 bygroups(Name.Class, Text), '#pop')\n            ],\n            'oc_ivars': [\n                include('whitespace'),\n                include('statements'),\n                (';', Punctuation),\n                (r'\\{', Punctuation, '#push'),\n                (r'\\}', Punctuation, '#pop'),\n            ],\n            'root': [\n                # methods\n                (r'^([-+])(\\s*)'                         # method marker\n                 r'(\\(.*?\\))?(\\s*)'                      # return type\n                 r'([a-zA-Z$_][\\w$]*:?)',        # begin of method name\n                 bygroups(Punctuation, Text, using(this),\n                          Text, Name.Function),\n                 'method'),\n                inherit,\n            ],\n            'method': [\n                include('whitespace'),\n                # TODO unsure if ellipses are allowed elsewhere, see\n                # discussion in Issue 789\n                (r',', Punctuation),\n                (r'\\.\\.\\.', Punctuation),\n                (r'(\\(.*?\\))(\\s*)([a-zA-Z$_][\\w$]*)',\n                 bygroups(using(this), Text, Name.Variable)),\n                (r'[a-zA-Z$_][\\w$]*:', Name.Function),\n                (';', Punctuation, '#pop'),\n                (r'\\{', Punctuation, 'function'),\n                default('#pop'),\n            ],\n            'literal_number': [\n                (r'\\(', Punctuation, 'literal_number_inner'),\n                (r'\\)', Literal, '#pop'),\n                include('statement'),\n            ],\n            'literal_number_inner': [\n                (r'\\(', Punctuation, '#push'),\n                (r'\\)', Punctuation, '#pop'),\n                include('statement'),\n            ],\n            'literal_array': [\n                (r'\\[', Punctuation, 'literal_array_inner'),\n                (r'\\]', Literal, '#pop'),\n                include('statement'),\n            ],\n            'literal_array_inner': [\n                (r'\\[', Punctuation, '#push'),\n                (r'\\]', Punctuation, '#pop'),\n                include('statement'),\n            ],\n            'literal_dictionary': [\n                (r'\\}', Literal, '#pop'),\n                include('statement'),\n            ],\n        }\n\n        def analyse_text(text):\n            if _oc_keywords.search(text):\n                return 1.0\n            elif '@\"' in text:  # strings\n                return 0.8\n            elif re.search('@[0-9]+', text):\n                return 0.7\n            elif _oc_message.search(text):\n                return 0.8\n            return 0\n\n        def get_tokens_unprocessed(self, text):\n            from pygments.lexers._cocoa_builtins import COCOA_INTERFACES, \\\n                COCOA_PROTOCOLS, COCOA_PRIMITIVES\n\n            for index, token, value in \\\n                    baselexer.get_tokens_unprocessed(self, text):\n                if token is Name or token is Name.Class:\n                    if value in COCOA_INTERFACES or value in COCOA_PROTOCOLS \\\n                       or value in COCOA_PRIMITIVES:\n                        token = Name.Builtin.Pseudo\n\n                yield index, token, value\n\n    return GeneratedObjectiveCVariant\n\n\nclass ObjectiveCLexer(objective(CLexer)):\n    \"\"\"\n    For Objective-C source code with preprocessor directives.\n    \"\"\"\n\n    name = 'Objective-C'\n    aliases = ['objective-c', 'objectivec', 'obj-c', 'objc']\n    filenames = ['*.m', '*.h']\n    mimetypes = ['text/x-objective-c']\n    priority = 0.05    # Lower than C\n\n\nclass ObjectiveCppLexer(objective(CppLexer)):\n    \"\"\"\n    For Objective-C++ source code with preprocessor directives.\n    \"\"\"\n\n    name = 'Objective-C++'\n    aliases = ['objective-c++', 'objectivec++', 'obj-c++', 'objc++']\n    filenames = ['*.mm', '*.hh']\n    mimetypes = ['text/x-objective-c++']\n    priority = 0.05    # Lower than C++\n\n\nclass LogosLexer(ObjectiveCppLexer):\n    \"\"\"\n    For Logos + Objective-C source code with preprocessor directives.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'Logos'\n    aliases = ['logos']\n    filenames = ['*.x', '*.xi', '*.xm', '*.xmi']\n    mimetypes = ['text/x-logos']\n    priority = 0.25\n\n    tokens = {\n        'statements': [\n            (r'(%orig|%log)\\b', Keyword),\n            (r'(%c)\\b(\\()(\\s*)([a-zA-Z$_][\\w$]*)(\\s*)(\\))',\n             bygroups(Keyword, Punctuation, Text, Name.Class, Text, Punctuation)),\n            (r'(%init)\\b(\\()',\n             bygroups(Keyword, Punctuation), 'logos_init_directive'),\n            (r'(%init)(?=\\s*;)', bygroups(Keyword)),\n            (r'(%hook|%group)(\\s+)([a-zA-Z$_][\\w$]+)',\n             bygroups(Keyword, Text, Name.Class), '#pop'),\n            (r'(%subclass)(\\s+)', bygroups(Keyword, Text),\n             ('#pop', 'logos_classname')),\n            inherit,\n        ],\n        'logos_init_directive': [\n            (r'\\s+', Text),\n            (',', Punctuation, ('logos_init_directive', '#pop')),\n            (r'([a-zA-Z$_][\\w$]*)(\\s*)(=)(\\s*)([^);]*)',\n             bygroups(Name.Class, Text, Punctuation, Text, Text)),\n            (r'([a-zA-Z$_][\\w$]*)', Name.Class),\n            (r'\\)', Punctuation, '#pop'),\n        ],\n        'logos_classname': [\n            (r'([a-zA-Z$_][\\w$]*)(\\s*:\\s*)([a-zA-Z$_][\\w$]*)?',\n             bygroups(Name.Class, Text, Name.Class), '#pop'),\n            (r'([a-zA-Z$_][\\w$]*)', Name.Class, '#pop')\n        ],\n        'root': [\n            (r'(%subclass)(\\s+)', bygroups(Keyword, Text),\n             'logos_classname'),\n            (r'(%hook|%group)(\\s+)([a-zA-Z$_][\\w$]+)',\n             bygroups(Keyword, Text, Name.Class)),\n            (r'(%config)(\\s*\\(\\s*)(\\w+)(\\s*=\\s*)(.*?)(\\s*\\)\\s*)',\n             bygroups(Keyword, Text, Name.Variable, Text, String, Text)),\n            (r'(%ctor)(\\s*)(\\{)', bygroups(Keyword, Text, Punctuation),\n             'function'),\n            (r'(%new)(\\s*)(\\()(\\s*.*?\\s*)(\\))',\n             bygroups(Keyword, Text, Keyword, String, Keyword)),\n            (r'(\\s*)(%end)(\\s*)', bygroups(Text, Keyword, Text)),\n            inherit,\n        ],\n    }\n\n    _logos_keywords = re.compile(r'%(?:hook|ctor|init|c\\()')\n\n    def analyse_text(text):\n        if LogosLexer._logos_keywords.search(text):\n            return 1.0\n        return 0\n\n\nclass SwiftLexer(RegexLexer):\n    \"\"\"\n    For `Swift <https://developer.apple.com/swift/>`_ source.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    name = 'Swift'\n    filenames = ['*.swift']\n    aliases = ['swift']\n    mimetypes = ['text/x-swift']\n\n    tokens = {\n        'root': [\n            # Whitespace and Comments\n            (r'\\n', Text),\n            (r'\\s+', Text),\n            (r'//', Comment.Single, 'comment-single'),\n            (r'/\\*', Comment.Multiline, 'comment-multi'),\n            (r'#(if|elseif|else|endif|available)\\b', Comment.Preproc, 'preproc'),\n\n            # Keywords\n            include('keywords'),\n\n            # Global Types\n            (words((\n                'Array', 'AutoreleasingUnsafeMutablePointer', 'BidirectionalReverseView',\n                'Bit', 'Bool', 'CFunctionPointer', 'COpaquePointer', 'CVaListPointer',\n                'Character', 'ClosedInterval', 'CollectionOfOne', 'ContiguousArray',\n                'Dictionary', 'DictionaryGenerator', 'DictionaryIndex', 'Double',\n                'EmptyCollection', 'EmptyGenerator', 'EnumerateGenerator',\n                'EnumerateSequence', 'FilterCollectionView',\n                'FilterCollectionViewIndex', 'FilterGenerator', 'FilterSequenceView',\n                'Float', 'Float80', 'FloatingPointClassification', 'GeneratorOf',\n                'GeneratorOfOne', 'GeneratorSequence', 'HalfOpenInterval', 'HeapBuffer',\n                'HeapBufferStorage', 'ImplicitlyUnwrappedOptional', 'IndexingGenerator',\n                'Int', 'Int16', 'Int32', 'Int64', 'Int8', 'LazyBidirectionalCollection',\n                'LazyForwardCollection', 'LazyRandomAccessCollection',\n                'LazySequence', 'MapCollectionView', 'MapSequenceGenerator',\n                'MapSequenceView', 'MirrorDisposition', 'ObjectIdentifier', 'OnHeap',\n                'Optional', 'PermutationGenerator', 'QuickLookObject',\n                'RandomAccessReverseView', 'Range', 'RangeGenerator', 'RawByte', 'Repeat',\n                'ReverseBidirectionalIndex', 'ReverseRandomAccessIndex', 'SequenceOf',\n                'SinkOf', 'Slice', 'StaticString', 'StrideThrough', 'StrideThroughGenerator',\n                'StrideTo', 'StrideToGenerator', 'String', 'UInt', 'UInt16', 'UInt32',\n                'UInt64', 'UInt8', 'UTF16', 'UTF32', 'UTF8', 'UnicodeDecodingResult',\n                'UnicodeScalar', 'Unmanaged', 'UnsafeBufferPointer',\n                'UnsafeBufferPointerGenerator', 'UnsafeMutableBufferPointer',\n                'UnsafeMutablePointer', 'UnsafePointer', 'Zip2', 'ZipGenerator2',\n                # Protocols\n                'AbsoluteValuable', 'AnyObject', 'ArrayLiteralConvertible',\n                'BidirectionalIndexType', 'BitwiseOperationsType',\n                'BooleanLiteralConvertible', 'BooleanType', 'CVarArgType',\n                'CollectionType', 'Comparable', 'DebugPrintable',\n                'DictionaryLiteralConvertible', 'Equatable',\n                'ExtendedGraphemeClusterLiteralConvertible',\n                'ExtensibleCollectionType', 'FloatLiteralConvertible',\n                'FloatingPointType', 'ForwardIndexType', 'GeneratorType', 'Hashable',\n                'IntegerArithmeticType', 'IntegerLiteralConvertible', 'IntegerType',\n                'IntervalType', 'MirrorType', 'MutableCollectionType', 'MutableSliceable',\n                'NilLiteralConvertible', 'OutputStreamType', 'Printable',\n                'RandomAccessIndexType', 'RangeReplaceableCollectionType',\n                'RawOptionSetType', 'RawRepresentable', 'Reflectable', 'SequenceType',\n                'SignedIntegerType', 'SignedNumberType', 'SinkType', 'Sliceable',\n                'Streamable', 'Strideable', 'StringInterpolationConvertible',\n                'StringLiteralConvertible', 'UnicodeCodecType',\n                'UnicodeScalarLiteralConvertible', 'UnsignedIntegerType',\n                '_ArrayBufferType', '_BidirectionalIndexType', '_CocoaStringType',\n                '_CollectionType', '_Comparable', '_ExtensibleCollectionType',\n                '_ForwardIndexType', '_Incrementable', '_IntegerArithmeticType',\n                '_IntegerType', '_ObjectiveCBridgeable', '_RandomAccessIndexType',\n                '_RawOptionSetType', '_SequenceType', '_Sequence_Type',\n                '_SignedIntegerType', '_SignedNumberType', '_Sliceable', '_Strideable',\n                '_SwiftNSArrayRequiredOverridesType', '_SwiftNSArrayType',\n                '_SwiftNSCopyingType', '_SwiftNSDictionaryRequiredOverridesType',\n                '_SwiftNSDictionaryType', '_SwiftNSEnumeratorType',\n                '_SwiftNSFastEnumerationType', '_SwiftNSStringRequiredOverridesType',\n                '_SwiftNSStringType', '_UnsignedIntegerType',\n                # Variables\n                'C_ARGC', 'C_ARGV', 'Process',\n                # Typealiases\n                'Any', 'AnyClass', 'BooleanLiteralType', 'CBool', 'CChar', 'CChar16',\n                'CChar32', 'CDouble', 'CFloat', 'CInt', 'CLong', 'CLongLong', 'CShort',\n                'CSignedChar', 'CUnsignedInt', 'CUnsignedLong', 'CUnsignedShort',\n                'CWideChar', 'ExtendedGraphemeClusterType', 'Float32', 'Float64',\n                'FloatLiteralType', 'IntMax', 'IntegerLiteralType', 'StringLiteralType',\n                'UIntMax', 'UWord', 'UnicodeScalarType', 'Void', 'Word',\n                # Foundation/Cocoa\n                'NSErrorPointer', 'NSObjectProtocol', 'Selector'), suffix=r'\\b'),\n             Name.Builtin),\n            # Functions\n            (words((\n                'abs', 'advance', 'alignof', 'alignofValue', 'assert', 'assertionFailure',\n                'contains', 'count', 'countElements', 'debugPrint', 'debugPrintln',\n                'distance', 'dropFirst', 'dropLast', 'dump', 'enumerate', 'equal',\n                'extend', 'fatalError', 'filter', 'find', 'first', 'getVaList', 'indices',\n                'insert', 'isEmpty', 'join', 'last', 'lazy', 'lexicographicalCompare',\n                'map', 'max', 'maxElement', 'min', 'minElement', 'numericCast', 'overlaps',\n                'partition', 'precondition', 'preconditionFailure', 'prefix', 'print',\n                'println', 'reduce', 'reflect', 'removeAll', 'removeAtIndex', 'removeLast',\n                'removeRange', 'reverse', 'sizeof', 'sizeofValue', 'sort', 'sorted',\n                'splice', 'split', 'startsWith', 'stride', 'strideof', 'strideofValue',\n                'suffix', 'swap', 'toDebugString', 'toString', 'transcode',\n                'underestimateCount', 'unsafeAddressOf', 'unsafeBitCast', 'unsafeDowncast',\n                'withExtendedLifetime', 'withUnsafeMutablePointer',\n                'withUnsafeMutablePointers', 'withUnsafePointer', 'withUnsafePointers',\n                'withVaList'), suffix=r'\\b'),\n             Name.Builtin.Pseudo),\n\n            # Implicit Block Variables\n            (r'\\$\\d+', Name.Variable),\n\n            # Binary Literal\n            (r'0b[01_]+', Number.Bin),\n            # Octal Literal\n            (r'0o[0-7_]+', Number.Oct),\n            # Hexadecimal Literal\n            (r'0x[0-9a-fA-F_]+', Number.Hex),\n            # Decimal Literal\n            (r'[0-9][0-9_]*(\\.[0-9_]+[eE][+\\-]?[0-9_]+|'\n             r'\\.[0-9_]*|[eE][+\\-]?[0-9_]+)', Number.Float),\n            (r'[0-9][0-9_]*', Number.Integer),\n            # String Literal\n            (r'\"', String, 'string'),\n\n            # Operators and Punctuation\n            (r'[(){}\\[\\].,:;=@#`?]|->|[<&?](?=\\w)|(?<=\\w)[>!?]', Punctuation),\n            (r'[/=\\-+!*%<>&|^?~]+', Operator),\n\n            # Identifier\n            (r'[a-zA-Z_]\\w*', Name)\n        ],\n        'keywords': [\n            (words((\n                'as', 'break', 'case', 'catch', 'continue', 'default', 'defer',\n                'do', 'else', 'fallthrough', 'for', 'guard', 'if', 'in', 'is',\n                'repeat', 'return', '#selector', 'switch', 'throw', 'try',\n                'where', 'while'), suffix=r'\\b'),\n             Keyword),\n            (r'@availability\\([^)]+\\)', Keyword.Reserved),\n            (words((\n                'associativity', 'convenience', 'dynamic', 'didSet', 'final',\n                'get', 'indirect', 'infix', 'inout', 'lazy', 'left', 'mutating',\n                'none', 'nonmutating', 'optional', 'override', 'postfix',\n                'precedence', 'prefix', 'Protocol', 'required', 'rethrows',\n                'right', 'set', 'throws', 'Type', 'unowned', 'weak', 'willSet',\n                '@availability', '@autoclosure', '@noreturn',\n                '@NSApplicationMain', '@NSCopying', '@NSManaged', '@objc',\n                '@UIApplicationMain', '@IBAction', '@IBDesignable',\n                '@IBInspectable', '@IBOutlet'), suffix=r'\\b'),\n             Keyword.Reserved),\n            (r'(as|dynamicType|false|is|nil|self|Self|super|true|__COLUMN__'\n             r'|__FILE__|__FUNCTION__|__LINE__|_'\n             r'|#(?:file|line|column|function))\\b', Keyword.Constant),\n            (r'import\\b', Keyword.Declaration, 'module'),\n            (r'(class|enum|extension|struct|protocol)(\\s+)([a-zA-Z_]\\w*)',\n             bygroups(Keyword.Declaration, Text, Name.Class)),\n            (r'(func)(\\s+)([a-zA-Z_]\\w*)',\n             bygroups(Keyword.Declaration, Text, Name.Function)),\n            (r'(var|let)(\\s+)([a-zA-Z_]\\w*)', bygroups(Keyword.Declaration,\n             Text, Name.Variable)),\n            (words((\n                'class', 'deinit', 'enum', 'extension', 'func', 'import', 'init',\n                'internal', 'let', 'operator', 'private', 'protocol', 'public',\n                'static', 'struct', 'subscript', 'typealias', 'var'), suffix=r'\\b'),\n             Keyword.Declaration)\n        ],\n        'comment': [\n            (r':param: [a-zA-Z_]\\w*|:returns?:|(FIXME|MARK|TODO):',\n             Comment.Special)\n        ],\n\n        # Nested\n        'comment-single': [\n            (r'\\n', Text, '#pop'),\n            include('comment'),\n            (r'[^\\n]', Comment.Single)\n        ],\n        'comment-multi': [\n            include('comment'),\n            (r'[^*/]', Comment.Multiline),\n            (r'/\\*', Comment.Multiline, '#push'),\n            (r'\\*/', Comment.Multiline, '#pop'),\n            (r'[*/]', Comment.Multiline)\n        ],\n        'module': [\n            (r'\\n', Text, '#pop'),\n            (r'[a-zA-Z_]\\w*', Name.Class),\n            include('root')\n        ],\n        'preproc': [\n            (r'\\n', Text, '#pop'),\n            include('keywords'),\n            (r'[A-Za-z]\\w*', Comment.Preproc),\n            include('root')\n        ],\n        'string': [\n            (r'\\\\\\(', String.Interpol, 'string-intp'),\n            (r'\"', String, '#pop'),\n            (r\"\"\"\\\\['\"\\\\nrt]|\\\\x[0-9a-fA-F]{2}|\\\\[0-7]{1,3}\"\"\"\n             r\"\"\"|\\\\u[0-9a-fA-F]{4}|\\\\U[0-9a-fA-F]{8}\"\"\", String.Escape),\n            (r'[^\\\\\"]+', String),\n            (r'\\\\', String)\n        ],\n        'string-intp': [\n            (r'\\(', String.Interpol, '#push'),\n            (r'\\)', String.Interpol, '#pop'),\n            include('root')\n        ]\n    }\n\n    def get_tokens_unprocessed(self, text):\n        from pygments.lexers._cocoa_builtins import COCOA_INTERFACES, \\\n            COCOA_PROTOCOLS, COCOA_PRIMITIVES\n\n        for index, token, value in \\\n                RegexLexer.get_tokens_unprocessed(self, text):\n            if token is Name or token is Name.Class:\n                if value in COCOA_INTERFACES or value in COCOA_PROTOCOLS \\\n                   or value in COCOA_PRIMITIVES:\n                    token = Name.Builtin.Pseudo\n\n            yield index, token, value\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.templates\n    ~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for various template engines' markup.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexers.html import HtmlLexer, XmlLexer\nfrom pygments.lexers.javascript import JavascriptLexer, LassoLexer\nfrom pygments.lexers.css import CssLexer\nfrom pygments.lexers.php import PhpLexer\nfrom pygments.lexers.python import PythonLexer\nfrom pygments.lexers.perl import PerlLexer\nfrom pygments.lexers.jvm import JavaLexer, TeaLangLexer\nfrom pygments.lexers.data import YamlLexer\nfrom pygments.lexer import Lexer, DelegatingLexer, RegexLexer, bygroups, \\\n    include, using, this, default, combined\nfrom pygments.token import Error, Punctuation, Whitespace, \\\n    Text, Comment, Operator, Keyword, Name, String, Number, Other, Token\nfrom pygments.util import html_doctype_matches, looks_like_xml\n\n__all__ = ['HtmlPhpLexer', 'XmlPhpLexer', 'CssPhpLexer',\n           'JavascriptPhpLexer', 'ErbLexer', 'RhtmlLexer',\n           'XmlErbLexer', 'CssErbLexer', 'JavascriptErbLexer',\n           'SmartyLexer', 'HtmlSmartyLexer', 'XmlSmartyLexer',\n           'CssSmartyLexer', 'JavascriptSmartyLexer', 'DjangoLexer',\n           'HtmlDjangoLexer', 'CssDjangoLexer', 'XmlDjangoLexer',\n           'JavascriptDjangoLexer', 'GenshiLexer', 'HtmlGenshiLexer',\n           'GenshiTextLexer', 'CssGenshiLexer', 'JavascriptGenshiLexer',\n           'MyghtyLexer', 'MyghtyHtmlLexer', 'MyghtyXmlLexer',\n           'MyghtyCssLexer', 'MyghtyJavascriptLexer', 'MasonLexer', 'MakoLexer',\n           'MakoHtmlLexer', 'MakoXmlLexer', 'MakoJavascriptLexer',\n           'MakoCssLexer', 'JspLexer', 'CheetahLexer', 'CheetahHtmlLexer',\n           'CheetahXmlLexer', 'CheetahJavascriptLexer', 'EvoqueLexer',\n           'EvoqueHtmlLexer', 'EvoqueXmlLexer', 'ColdfusionLexer',\n           'ColdfusionHtmlLexer', 'ColdfusionCFCLexer', 'VelocityLexer',\n           'VelocityHtmlLexer', 'VelocityXmlLexer', 'SspLexer',\n           'TeaTemplateLexer', 'LassoHtmlLexer', 'LassoXmlLexer',\n           'LassoCssLexer', 'LassoJavascriptLexer', 'HandlebarsLexer',\n           'HandlebarsHtmlLexer', 'YamlJinjaLexer', 'LiquidLexer',\n           'TwigLexer', 'TwigHtmlLexer', 'Angular2Lexer', 'Angular2HtmlLexer']\n\n\nclass ErbLexer(Lexer):\n    \"\"\"\n    Generic `ERB <http://ruby-doc.org/core/classes/ERB.html>`_ (Ruby Templating)\n    lexer.\n\n    Just highlights ruby code between the preprocessor directives, other data\n    is left untouched by the lexer.\n\n    All options are also forwarded to the `RubyLexer`.\n    \"\"\"\n\n    name = 'ERB'\n    aliases = ['erb']\n    mimetypes = ['application/x-ruby-templating']\n\n    _block_re = re.compile(r'(<%%|%%>|<%=|<%#|<%-|<%|-%>|%>|^%[^%].*?$)', re.M)\n\n    def __init__(self, **options):\n        from pygments.lexers.ruby import RubyLexer\n        self.ruby_lexer = RubyLexer(**options)\n        Lexer.__init__(self, **options)\n\n    def get_tokens_unprocessed(self, text):\n        \"\"\"\n        Since ERB doesn't allow \"<%\" and other tags inside of ruby\n        blocks we have to use a split approach here that fails for\n        that too.\n        \"\"\"\n        tokens = self._block_re.split(text)\n        tokens.reverse()\n        state = idx = 0\n        try:\n            while True:\n                # text\n                if state == 0:\n                    val = tokens.pop()\n                    yield idx, Other, val\n                    idx += len(val)\n                    state = 1\n                # block starts\n                elif state == 1:\n                    tag = tokens.pop()\n                    # literals\n                    if tag in ('<%%', '%%>'):\n                        yield idx, Other, tag\n                        idx += 3\n                        state = 0\n                    # comment\n                    elif tag == '<%#':\n                        yield idx, Comment.Preproc, tag\n                        val = tokens.pop()\n                        yield idx + 3, Comment, val\n                        idx += 3 + len(val)\n                        state = 2\n                    # blocks or output\n                    elif tag in ('<%', '<%=', '<%-'):\n                        yield idx, Comment.Preproc, tag\n                        idx += len(tag)\n                        data = tokens.pop()\n                        r_idx = 0\n                        for r_idx, r_token, r_value in \\\n                                self.ruby_lexer.get_tokens_unprocessed(data):\n                            yield r_idx + idx, r_token, r_value\n                        idx += len(data)\n                        state = 2\n                    elif tag in ('%>', '-%>'):\n                        yield idx, Error, tag\n                        idx += len(tag)\n                        state = 0\n                    # % raw ruby statements\n                    else:\n                        yield idx, Comment.Preproc, tag[0]\n                        r_idx = 0\n                        for r_idx, r_token, r_value in \\\n                                self.ruby_lexer.get_tokens_unprocessed(tag[1:]):\n                            yield idx + 1 + r_idx, r_token, r_value\n                        idx += len(tag)\n                        state = 0\n                # block ends\n                elif state == 2:\n                    tag = tokens.pop()\n                    if tag not in ('%>', '-%>'):\n                        yield idx, Other, tag\n                    else:\n                        yield idx, Comment.Preproc, tag\n                    idx += len(tag)\n                    state = 0\n        except IndexError:\n            return\n\n    def analyse_text(text):\n        if '<%' in text and '%>' in text:\n            return 0.4\n\n\nclass SmartyLexer(RegexLexer):\n    \"\"\"\n    Generic `Smarty <http://smarty.php.net/>`_ template lexer.\n\n    Just highlights smarty code between the preprocessor directives, other\n    data is left untouched by the lexer.\n    \"\"\"\n\n    name = 'Smarty'\n    aliases = ['smarty']\n    filenames = ['*.tpl']\n    mimetypes = ['application/x-smarty']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Other),\n            (r'(\\{)(\\*.*?\\*)(\\})',\n             bygroups(Comment.Preproc, Comment, Comment.Preproc)),\n            (r'(\\{php\\})(.*?)(\\{/php\\})',\n             bygroups(Comment.Preproc, using(PhpLexer, startinline=True),\n                      Comment.Preproc)),\n            (r'(\\{)(/?[a-zA-Z_]\\w*)(\\s*)',\n             bygroups(Comment.Preproc, Name.Function, Text), 'smarty'),\n            (r'\\{', Comment.Preproc, 'smarty')\n        ],\n        'smarty': [\n            (r'\\s+', Text),\n            (r'\\{', Comment.Preproc, '#push'),\n            (r'\\}', Comment.Preproc, '#pop'),\n            (r'#[a-zA-Z_]\\w*#', Name.Variable),\n            (r'\\$[a-zA-Z_]\\w*(\\.\\w+)*', Name.Variable),\n            (r'[~!%^&*()+=|\\[\\]:;,.<>/?@-]', Operator),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'[a-zA-Z_]\\w*', Name.Attribute)\n        ]\n    }\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'\\{if\\s+.*?\\}.*?\\{/if\\}', text):\n            rv += 0.15\n        if re.search(r'\\{include\\s+file=.*?\\}', text):\n            rv += 0.15\n        if re.search(r'\\{foreach\\s+.*?\\}.*?\\{/foreach\\}', text):\n            rv += 0.15\n        if re.search(r'\\{\\$.*?\\}', text):\n            rv += 0.01\n        return rv\n\n\nclass VelocityLexer(RegexLexer):\n    \"\"\"\n    Generic `Velocity <http://velocity.apache.org/>`_ template lexer.\n\n    Just highlights velocity directives and variable references, other\n    data is left untouched by the lexer.\n    \"\"\"\n\n    name = 'Velocity'\n    aliases = ['velocity']\n    filenames = ['*.vm', '*.fhtml']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    identifier = r'[a-zA-Z_]\\w*'\n\n    tokens = {\n        'root': [\n            (r'[^{#$]+', Other),\n            (r'(#)(\\*.*?\\*)(#)',\n             bygroups(Comment.Preproc, Comment, Comment.Preproc)),\n            (r'(##)(.*?$)',\n             bygroups(Comment.Preproc, Comment)),\n            (r'(#\\{?)(' + identifier + r')(\\}?)(\\s?\\()',\n             bygroups(Comment.Preproc, Name.Function, Comment.Preproc, Punctuation),\n             'directiveparams'),\n            (r'(#\\{?)(' + identifier + r')(\\}|\\b)',\n             bygroups(Comment.Preproc, Name.Function, Comment.Preproc)),\n            (r'\\$!?\\{?', Punctuation, 'variable')\n        ],\n        'variable': [\n            (identifier, Name.Variable),\n            (r'\\(', Punctuation, 'funcparams'),\n            (r'(\\.)(' + identifier + r')',\n             bygroups(Punctuation, Name.Variable), '#push'),\n            (r'\\}', Punctuation, '#pop'),\n            default('#pop')\n        ],\n        'directiveparams': [\n            (r'(&&|\\|\\||==?|!=?|[-<>+*%&|^/])|\\b(eq|ne|gt|lt|ge|le|not|in)\\b',\n             Operator),\n            (r'\\[', Operator, 'rangeoperator'),\n            (r'\\b' + identifier + r'\\b', Name.Function),\n            include('funcparams')\n        ],\n        'rangeoperator': [\n            (r'\\.\\.', Operator),\n            include('funcparams'),\n            (r'\\]', Operator, '#pop')\n        ],\n        'funcparams': [\n            (r'\\$!?\\{?', Punctuation, 'variable'),\n            (r'\\s+', Text),\n            (r'[,:]', Punctuation),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n            (r\"\\b[0-9]+\\b\", Number),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'\\(', Punctuation, '#push'),\n            (r'\\)', Punctuation, '#pop'),\n            (r'\\{', Punctuation, '#push'),\n            (r'\\}', Punctuation, '#pop'),\n            (r'\\[', Punctuation, '#push'),\n            (r'\\]', Punctuation, '#pop'),\n        ]\n    }\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'#\\{?macro\\}?\\(.*?\\).*?#\\{?end\\}?', text):\n            rv += 0.25\n        if re.search(r'#\\{?if\\}?\\(.+?\\).*?#\\{?end\\}?', text):\n            rv += 0.15\n        if re.search(r'#\\{?foreach\\}?\\(.+?\\).*?#\\{?end\\}?', text):\n            rv += 0.15\n        if re.search(r'\\$!?\\{?[a-zA-Z_]\\w*(\\([^)]*\\))?'\n                     r'(\\.\\w+(\\([^)]*\\))?)*\\}?', text):\n            rv += 0.01\n        return rv\n\n\nclass VelocityHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `VelocityLexer` that highlights unlexed data\n    with the `HtmlLexer`.\n\n    \"\"\"\n\n    name = 'HTML+Velocity'\n    aliases = ['html+velocity']\n    alias_filenames = ['*.html', '*.fhtml']\n    mimetypes = ['text/html+velocity']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, VelocityLexer, **options)\n\n\nclass VelocityXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `VelocityLexer` that highlights unlexed data\n    with the `XmlLexer`.\n\n    \"\"\"\n\n    name = 'XML+Velocity'\n    aliases = ['xml+velocity']\n    alias_filenames = ['*.xml', '*.vm']\n    mimetypes = ['application/xml+velocity']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, VelocityLexer, **options)\n\n    def analyse_text(text):\n        rv = VelocityLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass DjangoLexer(RegexLexer):\n    \"\"\"\n    Generic `django <http://www.djangoproject.com/documentation/templates/>`_\n    and `jinja <https://jinja.pocoo.org/jinja/>`_ template lexer.\n\n    It just highlights django/jinja code between the preprocessor directives,\n    other data is left untouched by the lexer.\n    \"\"\"\n\n    name = 'Django/Jinja'\n    aliases = ['django', 'jinja']\n    mimetypes = ['application/x-django-templating', 'application/x-jinja']\n\n    flags = re.M | re.S\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Other),\n            (r'\\{\\{', Comment.Preproc, 'var'),\n            # jinja/django comments\n            (r'\\{#.*?#\\}', Comment),\n            # django comments\n            (r'(\\{%)(-?\\s*)(comment)(\\s*-?)(%\\})(.*?)'\n             r'(\\{%)(-?\\s*)(endcomment)(\\s*-?)(%\\})',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Comment.Preproc,\n                      Comment, Comment.Preproc, Text, Keyword, Text,\n                      Comment.Preproc)),\n            # raw jinja blocks\n            (r'(\\{%)(-?\\s*)(raw)(\\s*-?)(%\\})(.*?)'\n             r'(\\{%)(-?\\s*)(endraw)(\\s*-?)(%\\})',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Comment.Preproc,\n                      Text, Comment.Preproc, Text, Keyword, Text,\n                      Comment.Preproc)),\n            # filter blocks\n            (r'(\\{%)(-?\\s*)(filter)(\\s+)([a-zA-Z_]\\w*)',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Name.Function),\n             'block'),\n            (r'(\\{%)(-?\\s*)([a-zA-Z_]\\w*)',\n             bygroups(Comment.Preproc, Text, Keyword), 'block'),\n            (r'\\{', Other)\n        ],\n        'varnames': [\n            (r'(\\|)(\\s*)([a-zA-Z_]\\w*)',\n             bygroups(Operator, Text, Name.Function)),\n            (r'(is)(\\s+)(not)?(\\s+)?([a-zA-Z_]\\w*)',\n             bygroups(Keyword, Text, Keyword, Text, Name.Function)),\n            (r'(_|true|false|none|True|False|None)\\b', Keyword.Pseudo),\n            (r'(in|as|reversed|recursive|not|and|or|is|if|else|import|'\n             r'with(?:(?:out)?\\s*context)?|scoped|ignore\\s+missing)\\b',\n             Keyword),\n            (r'(loop|block|super|forloop)\\b', Name.Builtin),\n            (r'[a-zA-Z_][\\w-]*', Name.Variable),\n            (r'\\.\\w+', Name.Variable),\n            (r':?\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\":?'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'([{}()\\[\\]+\\-*/%,:~]|[><=]=?|!=)', Operator),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n        ],\n        'var': [\n            (r'\\s+', Text),\n            (r'(-?)(\\}\\})', bygroups(Text, Comment.Preproc), '#pop'),\n            include('varnames')\n        ],\n        'block': [\n            (r'\\s+', Text),\n            (r'(-?)(%\\})', bygroups(Text, Comment.Preproc), '#pop'),\n            include('varnames'),\n            (r'.', Punctuation)\n        ]\n    }\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'\\{%\\s*(block|extends)', text) is not None:\n            rv += 0.4\n        if re.search(r'\\{%\\s*if\\s*.*?%\\}', text) is not None:\n            rv += 0.1\n        if re.search(r'\\{\\{.*?\\}\\}', text) is not None:\n            rv += 0.1\n        return rv\n\n\nclass MyghtyLexer(RegexLexer):\n    \"\"\"\n    Generic `myghty templates`_ lexer. Code that isn't Myghty\n    markup is yielded as `Token.Other`.\n\n    .. versionadded:: 0.6\n\n    .. _myghty templates: http://www.myghty.org/\n    \"\"\"\n\n    name = 'Myghty'\n    aliases = ['myghty']\n    filenames = ['*.myt', 'autodelegate']\n    mimetypes = ['application/x-myghty']\n\n    tokens = {\n        'root': [\n            (r'\\s+', Text),\n            (r'(?s)(<%(?:def|method))(\\s*)(.*?)(>)(.*?)(</%\\2\\s*>)',\n             bygroups(Name.Tag, Text, Name.Function, Name.Tag,\n                      using(this), Name.Tag)),\n            (r'(?s)(<%\\w+)(.*?)(>)(.*?)(</%\\2\\s*>)',\n             bygroups(Name.Tag, Name.Function, Name.Tag,\n                      using(PythonLexer), Name.Tag)),\n            (r'(<&[^|])(.*?)(,.*?)?(&>)',\n             bygroups(Name.Tag, Name.Function, using(PythonLexer), Name.Tag)),\n            (r'(?s)(<&\\|)(.*?)(,.*?)?(&>)',\n             bygroups(Name.Tag, Name.Function, using(PythonLexer), Name.Tag)),\n            (r'</&>', Name.Tag),\n            (r'(?s)(<%!?)(.*?)(%>)',\n             bygroups(Name.Tag, using(PythonLexer), Name.Tag)),\n            (r'(?<=^)#[^\\n]*(\\n|\\Z)', Comment),\n            (r'(?<=^)(%)([^\\n]*)(\\n|\\Z)',\n             bygroups(Name.Tag, using(PythonLexer), Other)),\n            (r\"\"\"(?sx)\n                 (.+?)               # anything, followed by:\n                 (?:\n                  (?<=\\n)(?=[%#]) |  # an eval or comment line\n                  (?=</?[%&]) |      # a substitution or block or\n                                     # call start or end\n                                     # - don't consume\n                  (\\\\\\n) |           # an escaped newline\n                  \\Z                 # end of string\n                 )\"\"\", bygroups(Other, Operator)),\n        ]\n    }\n\n\nclass MyghtyHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MyghtyLexer` that highlights unlexed data\n    with the `HtmlLexer`.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    name = 'HTML+Myghty'\n    aliases = ['html+myghty']\n    mimetypes = ['text/html+myghty']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, MyghtyLexer, **options)\n\n\nclass MyghtyXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MyghtyLexer` that highlights unlexed data\n    with the `XmlLexer`.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    name = 'XML+Myghty'\n    aliases = ['xml+myghty']\n    mimetypes = ['application/xml+myghty']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, MyghtyLexer, **options)\n\n\nclass MyghtyJavascriptLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MyghtyLexer` that highlights unlexed data\n    with the `JavascriptLexer`.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    name = 'JavaScript+Myghty'\n    aliases = ['js+myghty', 'javascript+myghty']\n    mimetypes = ['application/x-javascript+myghty',\n                 'text/x-javascript+myghty',\n                 'text/javascript+mygthy']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, MyghtyLexer, **options)\n\n\nclass MyghtyCssLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MyghtyLexer` that highlights unlexed data\n    with the `CssLexer`.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    name = 'CSS+Myghty'\n    aliases = ['css+myghty']\n    mimetypes = ['text/css+myghty']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, MyghtyLexer, **options)\n\n\nclass MasonLexer(RegexLexer):\n    \"\"\"\n    Generic `mason templates`_ lexer. Stolen from Myghty lexer. Code that isn't\n    Mason markup is HTML.\n\n    .. _mason templates: http://www.masonhq.com/\n\n    .. versionadded:: 1.4\n    \"\"\"\n    name = 'Mason'\n    aliases = ['mason']\n    filenames = ['*.m', '*.mhtml', '*.mc', '*.mi', 'autohandler', 'dhandler']\n    mimetypes = ['application/x-mason']\n\n    tokens = {\n        'root': [\n            (r'\\s+', Text),\n            (r'(?s)(<%doc>)(.*?)(</%doc>)',\n             bygroups(Name.Tag, Comment.Multiline, Name.Tag)),\n            (r'(?s)(<%(?:def|method))(\\s*)(.*?)(>)(.*?)(</%\\2\\s*>)',\n             bygroups(Name.Tag, Text, Name.Function, Name.Tag,\n                      using(this), Name.Tag)),\n            (r'(?s)(<%(\\w+)(.*?)(>))(.*?)(</%\\2\\s*>)',\n             bygroups(Name.Tag, None, None, None, using(PerlLexer), Name.Tag)),\n            (r'(?s)(<&[^|])(.*?)(,.*?)?(&>)',\n             bygroups(Name.Tag, Name.Function, using(PerlLexer), Name.Tag)),\n            (r'(?s)(<&\\|)(.*?)(,.*?)?(&>)',\n             bygroups(Name.Tag, Name.Function, using(PerlLexer), Name.Tag)),\n            (r'</&>', Name.Tag),\n            (r'(?s)(<%!?)(.*?)(%>)',\n             bygroups(Name.Tag, using(PerlLexer), Name.Tag)),\n            (r'(?<=^)#[^\\n]*(\\n|\\Z)', Comment),\n            (r'(?<=^)(%)([^\\n]*)(\\n|\\Z)',\n             bygroups(Name.Tag, using(PerlLexer), Other)),\n            (r\"\"\"(?sx)\n                 (.+?)               # anything, followed by:\n                 (?:\n                  (?<=\\n)(?=[%#]) |  # an eval or comment line\n                  (?=</?[%&]) |      # a substitution or block or\n                                     # call start or end\n                                     # - don't consume\n                  (\\\\\\n) |           # an escaped newline\n                  \\Z                 # end of string\n                 )\"\"\", bygroups(using(HtmlLexer), Operator)),\n        ]\n    }\n\n    def analyse_text(text):\n        result = 0.0\n        if re.search(r'</%(class|doc|init)>', text) is not None:\n            result = 1.0\n        elif re.search(r'<&.+&>', text, re.DOTALL) is not None:\n            result = 0.11\n        return result\n\n\nclass MakoLexer(RegexLexer):\n    \"\"\"\n    Generic `mako templates`_ lexer. Code that isn't Mako\n    markup is yielded as `Token.Other`.\n\n    .. versionadded:: 0.7\n\n    .. _mako templates: http://www.makotemplates.org/\n    \"\"\"\n\n    name = 'Mako'\n    aliases = ['mako']\n    filenames = ['*.mao']\n    mimetypes = ['application/x-mako']\n\n    tokens = {\n        'root': [\n            (r'(\\s*)(%)(\\s*end(?:\\w+))(\\n|\\Z)',\n             bygroups(Text, Comment.Preproc, Keyword, Other)),\n            (r'(\\s*)(%)([^\\n]*)(\\n|\\Z)',\n             bygroups(Text, Comment.Preproc, using(PythonLexer), Other)),\n            (r'(\\s*)(##[^\\n]*)(\\n|\\Z)',\n             bygroups(Text, Comment.Preproc, Other)),\n            (r'(?s)<%doc>.*?</%doc>', Comment.Preproc),\n            (r'(<%)([\\w.:]+)',\n             bygroups(Comment.Preproc, Name.Builtin), 'tag'),\n            (r'(</%)([\\w.:]+)(>)',\n             bygroups(Comment.Preproc, Name.Builtin, Comment.Preproc)),\n            (r'<%(?=([\\w.:]+))', Comment.Preproc, 'ondeftags'),\n            (r'(?s)(<%(?:!?))(.*?)(%>)',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            (r'(\\$\\{)(.*?)(\\})',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            (r'''(?sx)\n                (.+?)                # anything, followed by:\n                (?:\n                 (?<=\\n)(?=%|\\#\\#) | # an eval or comment line\n                 (?=\\#\\*) |          # multiline comment\n                 (?=</?%) |          # a python block\n                                     # call start or end\n                 (?=\\$\\{) |          # a substitution\n                 (?<=\\n)(?=\\s*%) |\n                                     # - don't consume\n                 (\\\\\\n) |            # an escaped newline\n                 \\Z                  # end of string\n                )\n            ''', bygroups(Other, Operator)),\n            (r'\\s+', Text),\n        ],\n        'ondeftags': [\n            (r'<%', Comment.Preproc),\n            (r'(?<=<%)(include|inherit|namespace|page)', Name.Builtin),\n            include('tag'),\n        ],\n        'tag': [\n            (r'((?:\\w+)\\s*=)(\\s*)(\".*?\")',\n             bygroups(Name.Attribute, Text, String)),\n            (r'/?\\s*>', Comment.Preproc, '#pop'),\n            (r'\\s+', Text),\n        ],\n        'attr': [\n            ('\".*?\"', String, '#pop'),\n            (\"'.*?'\", String, '#pop'),\n            (r'[^\\s>]+', String, '#pop'),\n        ],\n    }\n\n\nclass MakoHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MakoLexer` that highlights unlexed data\n    with the `HtmlLexer`.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    name = 'HTML+Mako'\n    aliases = ['html+mako']\n    mimetypes = ['text/html+mako']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, MakoLexer, **options)\n\n\nclass MakoXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MakoLexer` that highlights unlexed data\n    with the `XmlLexer`.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    name = 'XML+Mako'\n    aliases = ['xml+mako']\n    mimetypes = ['application/xml+mako']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, MakoLexer, **options)\n\n\nclass MakoJavascriptLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MakoLexer` that highlights unlexed data\n    with the `JavascriptLexer`.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    name = 'JavaScript+Mako'\n    aliases = ['js+mako', 'javascript+mako']\n    mimetypes = ['application/x-javascript+mako',\n                 'text/x-javascript+mako',\n                 'text/javascript+mako']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, MakoLexer, **options)\n\n\nclass MakoCssLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MakoLexer` that highlights unlexed data\n    with the `CssLexer`.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    name = 'CSS+Mako'\n    aliases = ['css+mako']\n    mimetypes = ['text/css+mako']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, MakoLexer, **options)\n\n\n# Genshi and Cheetah lexers courtesy of Matt Good.\n\nclass CheetahPythonLexer(Lexer):\n    \"\"\"\n    Lexer for handling Cheetah's special $ tokens in Python syntax.\n    \"\"\"\n\n    def get_tokens_unprocessed(self, text):\n        pylexer = PythonLexer(**self.options)\n        for pos, type_, value in pylexer.get_tokens_unprocessed(text):\n            if type_ == Token.Error and value == '$':\n                type_ = Comment.Preproc\n            yield pos, type_, value\n\n\nclass CheetahLexer(RegexLexer):\n    \"\"\"\n    Generic `cheetah templates`_ lexer. Code that isn't Cheetah\n    markup is yielded as `Token.Other`.  This also works for\n    `spitfire templates`_ which use the same syntax.\n\n    .. _cheetah templates: http://www.cheetahtemplate.org/\n    .. _spitfire templates: http://code.google.com/p/spitfire/\n    \"\"\"\n\n    name = 'Cheetah'\n    aliases = ['cheetah', 'spitfire']\n    filenames = ['*.tmpl', '*.spt']\n    mimetypes = ['application/x-cheetah', 'application/x-spitfire']\n\n    tokens = {\n        'root': [\n            (r'(##[^\\n]*)$',\n             (bygroups(Comment))),\n            (r'#[*](.|\\n)*?[*]#', Comment),\n            (r'#end[^#\\n]*(?:#|$)', Comment.Preproc),\n            (r'#slurp$', Comment.Preproc),\n            (r'(#[a-zA-Z]+)([^#\\n]*)(#|$)',\n             (bygroups(Comment.Preproc, using(CheetahPythonLexer),\n                       Comment.Preproc))),\n            # TODO support other Python syntax like $foo['bar']\n            (r'(\\$)([a-zA-Z_][\\w.]*\\w)',\n             bygroups(Comment.Preproc, using(CheetahPythonLexer))),\n            (r'(?s)(\\$\\{!?)(.*?)(\\})',\n             bygroups(Comment.Preproc, using(CheetahPythonLexer),\n                      Comment.Preproc)),\n            (r'''(?sx)\n                (.+?)               # anything, followed by:\n                (?:\n                 (?=\\#[#a-zA-Z]*) | # an eval comment\n                 (?=\\$[a-zA-Z_{]) | # a substitution\n                 \\Z                 # end of string\n                )\n            ''', Other),\n            (r'\\s+', Text),\n        ],\n    }\n\n\nclass CheetahHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `CheetahLexer` that highlights unlexed data\n    with the `HtmlLexer`.\n    \"\"\"\n\n    name = 'HTML+Cheetah'\n    aliases = ['html+cheetah', 'html+spitfire', 'htmlcheetah']\n    mimetypes = ['text/html+cheetah', 'text/html+spitfire']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, CheetahLexer, **options)\n\n\nclass CheetahXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `CheetahLexer` that highlights unlexed data\n    with the `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+Cheetah'\n    aliases = ['xml+cheetah', 'xml+spitfire']\n    mimetypes = ['application/xml+cheetah', 'application/xml+spitfire']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, CheetahLexer, **options)\n\n\nclass CheetahJavascriptLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `CheetahLexer` that highlights unlexed data\n    with the `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+Cheetah'\n    aliases = ['js+cheetah', 'javascript+cheetah',\n               'js+spitfire', 'javascript+spitfire']\n    mimetypes = ['application/x-javascript+cheetah',\n                 'text/x-javascript+cheetah',\n                 'text/javascript+cheetah',\n                 'application/x-javascript+spitfire',\n                 'text/x-javascript+spitfire',\n                 'text/javascript+spitfire']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, CheetahLexer, **options)\n\n\nclass GenshiTextLexer(RegexLexer):\n    \"\"\"\n    A lexer that highlights `genshi <http://genshi.edgewall.org/>`_ text\n    templates.\n    \"\"\"\n\n    name = 'Genshi Text'\n    aliases = ['genshitext']\n    mimetypes = ['application/x-genshi-text', 'text/x-genshi']\n\n    tokens = {\n        'root': [\n            (r'[^#$\\s]+', Other),\n            (r'^(\\s*)(##.*)$', bygroups(Text, Comment)),\n            (r'^(\\s*)(#)', bygroups(Text, Comment.Preproc), 'directive'),\n            include('variable'),\n            (r'[#$\\s]', Other),\n        ],\n        'directive': [\n            (r'\\n', Text, '#pop'),\n            (r'(?:def|for|if)\\s+.*', using(PythonLexer), '#pop'),\n            (r'(choose|when|with)([^\\S\\n]+)(.*)',\n             bygroups(Keyword, Text, using(PythonLexer)), '#pop'),\n            (r'(choose|otherwise)\\b', Keyword, '#pop'),\n            (r'(end\\w*)([^\\S\\n]*)(.*)', bygroups(Keyword, Text, Comment), '#pop'),\n        ],\n        'variable': [\n            (r'(?<!\\$)(\\$\\{)(.+?)(\\})',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            (r'(?<!\\$)(\\$)([a-zA-Z_][\\w.]*)',\n             Name.Variable),\n        ]\n    }\n\n\nclass GenshiMarkupLexer(RegexLexer):\n    \"\"\"\n    Base lexer for Genshi markup, used by `HtmlGenshiLexer` and\n    `GenshiLexer`.\n    \"\"\"\n\n    flags = re.DOTALL\n\n    tokens = {\n        'root': [\n            (r'[^<$]+', Other),\n            (r'(<\\?python)(.*?)(\\?>)',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            # yield style and script blocks as Other\n            (r'<\\s*(script|style)\\s*.*?>.*?<\\s*/\\1\\s*>', Other),\n            (r'<\\s*py:[a-zA-Z0-9]+', Name.Tag, 'pytag'),\n            (r'<\\s*[a-zA-Z0-9:.]+', Name.Tag, 'tag'),\n            include('variable'),\n            (r'[<$]', Other),\n        ],\n        'pytag': [\n            (r'\\s+', Text),\n            (r'[\\w:-]+\\s*=', Name.Attribute, 'pyattr'),\n            (r'/?\\s*>', Name.Tag, '#pop'),\n        ],\n        'pyattr': [\n            ('(\")(.*?)(\")', bygroups(String, using(PythonLexer), String), '#pop'),\n            (\"(')(.*?)(')\", bygroups(String, using(PythonLexer), String), '#pop'),\n            (r'[^\\s>]+', String, '#pop'),\n        ],\n        'tag': [\n            (r'\\s+', Text),\n            (r'py:[\\w-]+\\s*=', Name.Attribute, 'pyattr'),\n            (r'[\\w:-]+\\s*=', Name.Attribute, 'attr'),\n            (r'/?\\s*>', Name.Tag, '#pop'),\n        ],\n        'attr': [\n            ('\"', String, 'attr-dstring'),\n            (\"'\", String, 'attr-sstring'),\n            (r'[^\\s>]*', String, '#pop')\n        ],\n        'attr-dstring': [\n            ('\"', String, '#pop'),\n            include('strings'),\n            (\"'\", String)\n        ],\n        'attr-sstring': [\n            (\"'\", String, '#pop'),\n            include('strings'),\n            (\"'\", String)\n        ],\n        'strings': [\n            ('[^\"\\'$]+', String),\n            include('variable')\n        ],\n        'variable': [\n            (r'(?<!\\$)(\\$\\{)(.+?)(\\})',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            (r'(?<!\\$)(\\$)([a-zA-Z_][\\w\\.]*)',\n             Name.Variable),\n        ]\n    }\n\n\nclass HtmlGenshiLexer(DelegatingLexer):\n    \"\"\"\n    A lexer that highlights `genshi <http://genshi.edgewall.org/>`_ and\n    `kid <http://kid-templating.org/>`_ kid HTML templates.\n    \"\"\"\n\n    name = 'HTML+Genshi'\n    aliases = ['html+genshi', 'html+kid']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml']\n    mimetypes = ['text/html+genshi']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, GenshiMarkupLexer, **options)\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'\\$\\{.*?\\}', text) is not None:\n            rv += 0.2\n        if re.search(r'py:(.*?)=[\"\\']', text) is not None:\n            rv += 0.2\n        return rv + HtmlLexer.analyse_text(text) - 0.01\n\n\nclass GenshiLexer(DelegatingLexer):\n    \"\"\"\n    A lexer that highlights `genshi <http://genshi.edgewall.org/>`_ and\n    `kid <http://kid-templating.org/>`_ kid XML templates.\n    \"\"\"\n\n    name = 'Genshi'\n    aliases = ['genshi', 'kid', 'xml+genshi', 'xml+kid']\n    filenames = ['*.kid']\n    alias_filenames = ['*.xml']\n    mimetypes = ['application/x-genshi', 'application/x-kid']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, GenshiMarkupLexer, **options)\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'\\$\\{.*?\\}', text) is not None:\n            rv += 0.2\n        if re.search(r'py:(.*?)=[\"\\']', text) is not None:\n            rv += 0.2\n        return rv + XmlLexer.analyse_text(text) - 0.01\n\n\nclass JavascriptGenshiLexer(DelegatingLexer):\n    \"\"\"\n    A lexer that highlights javascript code in genshi text templates.\n    \"\"\"\n\n    name = 'JavaScript+Genshi Text'\n    aliases = ['js+genshitext', 'js+genshi', 'javascript+genshitext',\n               'javascript+genshi']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+genshi',\n                 'text/x-javascript+genshi',\n                 'text/javascript+genshi']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, GenshiTextLexer, **options)\n\n    def analyse_text(text):\n        return GenshiLexer.analyse_text(text) - 0.05\n\n\nclass CssGenshiLexer(DelegatingLexer):\n    \"\"\"\n    A lexer that highlights CSS definitions in genshi text templates.\n    \"\"\"\n\n    name = 'CSS+Genshi Text'\n    aliases = ['css+genshitext', 'css+genshi']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+genshi']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, GenshiTextLexer, **options)\n\n    def analyse_text(text):\n        return GenshiLexer.analyse_text(text) - 0.05\n\n\nclass RhtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the ERB lexer that highlights the unlexed data with the\n    html lexer.\n\n    Nested Javascript and CSS is highlighted too.\n    \"\"\"\n\n    name = 'RHTML'\n    aliases = ['rhtml', 'html+erb', 'html+ruby']\n    filenames = ['*.rhtml']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml']\n    mimetypes = ['text/html+ruby']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, ErbLexer, **options)\n\n    def analyse_text(text):\n        rv = ErbLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):\n            # one more than the XmlErbLexer returns\n            rv += 0.5\n        return rv\n\n\nclass XmlErbLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `ErbLexer` which highlights data outside preprocessor\n    directives with the `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+Ruby'\n    aliases = ['xml+erb', 'xml+ruby']\n    alias_filenames = ['*.xml']\n    mimetypes = ['application/xml+ruby']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, ErbLexer, **options)\n\n    def analyse_text(text):\n        rv = ErbLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass CssErbLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `ErbLexer` which highlights unlexed data with the `CssLexer`.\n    \"\"\"\n\n    name = 'CSS+Ruby'\n    aliases = ['css+erb', 'css+ruby']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+ruby']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, ErbLexer, **options)\n\n    def analyse_text(text):\n        return ErbLexer.analyse_text(text) - 0.05\n\n\nclass JavascriptErbLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `ErbLexer` which highlights unlexed data with the\n    `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+Ruby'\n    aliases = ['js+erb', 'javascript+erb', 'js+ruby', 'javascript+ruby']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+ruby',\n                 'text/x-javascript+ruby',\n                 'text/javascript+ruby']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, ErbLexer, **options)\n\n    def analyse_text(text):\n        return ErbLexer.analyse_text(text) - 0.05\n\n\nclass HtmlPhpLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `PhpLexer` that highlights unhandled data with the `HtmlLexer`.\n\n    Nested Javascript and CSS is highlighted too.\n    \"\"\"\n\n    name = 'HTML+PHP'\n    aliases = ['html+php']\n    filenames = ['*.phtml']\n    alias_filenames = ['*.php', '*.html', '*.htm', '*.xhtml',\n                       '*.php[345]']\n    mimetypes = ['application/x-php',\n                 'application/x-httpd-php', 'application/x-httpd-php3',\n                 'application/x-httpd-php4', 'application/x-httpd-php5']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, PhpLexer, **options)\n\n    def analyse_text(text):\n        rv = PhpLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):\n            rv += 0.5\n        return rv\n\n\nclass XmlPhpLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `PhpLexer` that highlights unhandled data with the `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+PHP'\n    aliases = ['xml+php']\n    alias_filenames = ['*.xml', '*.php', '*.php[345]']\n    mimetypes = ['application/xml+php']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, PhpLexer, **options)\n\n    def analyse_text(text):\n        rv = PhpLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass CssPhpLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `PhpLexer` which highlights unmatched data with the `CssLexer`.\n    \"\"\"\n\n    name = 'CSS+PHP'\n    aliases = ['css+php']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+php']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, PhpLexer, **options)\n\n    def analyse_text(text):\n        return PhpLexer.analyse_text(text) - 0.05\n\n\nclass JavascriptPhpLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `PhpLexer` which highlights unmatched data with the\n    `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+PHP'\n    aliases = ['js+php', 'javascript+php']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+php',\n                 'text/x-javascript+php',\n                 'text/javascript+php']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, PhpLexer, **options)\n\n    def analyse_text(text):\n        return PhpLexer.analyse_text(text)\n\n\nclass HtmlSmartyLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `SmartyLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    Nested Javascript and CSS is highlighted too.\n    \"\"\"\n\n    name = 'HTML+Smarty'\n    aliases = ['html+smarty']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml', '*.tpl']\n    mimetypes = ['text/html+smarty']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, SmartyLexer, **options)\n\n    def analyse_text(text):\n        rv = SmartyLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):\n            rv += 0.5\n        return rv\n\n\nclass XmlSmartyLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `SmartyLexer` that highlights unlexed data with the\n    `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+Smarty'\n    aliases = ['xml+smarty']\n    alias_filenames = ['*.xml', '*.tpl']\n    mimetypes = ['application/xml+smarty']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, SmartyLexer, **options)\n\n    def analyse_text(text):\n        rv = SmartyLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass CssSmartyLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `SmartyLexer` that highlights unlexed data with the\n    `CssLexer`.\n    \"\"\"\n\n    name = 'CSS+Smarty'\n    aliases = ['css+smarty']\n    alias_filenames = ['*.css', '*.tpl']\n    mimetypes = ['text/css+smarty']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, SmartyLexer, **options)\n\n    def analyse_text(text):\n        return SmartyLexer.analyse_text(text) - 0.05\n\n\nclass JavascriptSmartyLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `SmartyLexer` that highlights unlexed data with the\n    `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+Smarty'\n    aliases = ['js+smarty', 'javascript+smarty']\n    alias_filenames = ['*.js', '*.tpl']\n    mimetypes = ['application/x-javascript+smarty',\n                 'text/x-javascript+smarty',\n                 'text/javascript+smarty']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, SmartyLexer, **options)\n\n    def analyse_text(text):\n        return SmartyLexer.analyse_text(text) - 0.05\n\n\nclass HtmlDjangoLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    Nested Javascript and CSS is highlighted too.\n    \"\"\"\n\n    name = 'HTML+Django/Jinja'\n    aliases = ['html+django', 'html+jinja', 'htmldjango']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml']\n    mimetypes = ['text/html+django', 'text/html+jinja']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, DjangoLexer, **options)\n\n    def analyse_text(text):\n        rv = DjangoLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):\n            rv += 0.5\n        return rv\n\n\nclass XmlDjangoLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+Django/Jinja'\n    aliases = ['xml+django', 'xml+jinja']\n    alias_filenames = ['*.xml']\n    mimetypes = ['application/xml+django', 'application/xml+jinja']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, DjangoLexer, **options)\n\n    def analyse_text(text):\n        rv = DjangoLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass CssDjangoLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `CssLexer`.\n    \"\"\"\n\n    name = 'CSS+Django/Jinja'\n    aliases = ['css+django', 'css+jinja']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+django', 'text/css+jinja']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, DjangoLexer, **options)\n\n    def analyse_text(text):\n        return DjangoLexer.analyse_text(text) - 0.05\n\n\nclass JavascriptDjangoLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+Django/Jinja'\n    aliases = ['js+django', 'javascript+django',\n               'js+jinja', 'javascript+jinja']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+django',\n                 'application/x-javascript+jinja',\n                 'text/x-javascript+django',\n                 'text/x-javascript+jinja',\n                 'text/javascript+django',\n                 'text/javascript+jinja']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, DjangoLexer, **options)\n\n    def analyse_text(text):\n        return DjangoLexer.analyse_text(text) - 0.05\n\n\nclass JspRootLexer(RegexLexer):\n    \"\"\"\n    Base for the `JspLexer`. Yields `Token.Other` for area outside of\n    JSP tags.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    tokens = {\n        'root': [\n            (r'<%\\S?', Keyword, 'sec'),\n            # FIXME: I want to make these keywords but still parse attributes.\n            (r'</?jsp:(forward|getProperty|include|plugin|setProperty|useBean).*?>',\n             Keyword),\n            (r'[^<]+', Other),\n            (r'<', Other),\n        ],\n        'sec': [\n            (r'%>', Keyword, '#pop'),\n            # note: '\\w\\W' != '.' without DOTALL.\n            (r'[\\w\\W]+?(?=%>|\\Z)', using(JavaLexer)),\n        ],\n    }\n\n\nclass JspLexer(DelegatingLexer):\n    \"\"\"\n    Lexer for Java Server Pages.\n\n    .. versionadded:: 0.7\n    \"\"\"\n    name = 'Java Server Page'\n    aliases = ['jsp']\n    filenames = ['*.jsp']\n    mimetypes = ['application/x-jsp']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, JspRootLexer, **options)\n\n    def analyse_text(text):\n        rv = JavaLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        if '<%' in text and '%>' in text:\n            rv += 0.1\n        return rv\n\n\nclass EvoqueLexer(RegexLexer):\n    \"\"\"\n    For files using the Evoque templating system.\n\n    .. versionadded:: 1.1\n    \"\"\"\n    name = 'Evoque'\n    aliases = ['evoque']\n    filenames = ['*.evoque']\n    mimetypes = ['application/x-evoque']\n\n    flags = re.DOTALL\n\n    tokens = {\n        'root': [\n            (r'[^#$]+', Other),\n            (r'#\\[', Comment.Multiline, 'comment'),\n            (r'\\$\\$', Other),\n            # svn keywords\n            (r'\\$\\w+:[^$\\n]*\\$', Comment.Multiline),\n            # directives: begin, end\n            (r'(\\$)(begin|end)(\\{(%)?)(.*?)((?(4)%)\\})',\n             bygroups(Punctuation, Name.Builtin, Punctuation, None,\n                      String, Punctuation)),\n            # directives: evoque, overlay\n            # see doc for handling first name arg: /directives/evoque/\n            # + minor inconsistency: the \"name\" in e.g. $overlay{name=site_base}\n            # should be using(PythonLexer), not passed out as String\n            (r'(\\$)(evoque|overlay)(\\{(%)?)(\\s*[#\\w\\-\"\\'.]+[^=,%}]+?)?'\n             r'(.*?)((?(4)%)\\})',\n             bygroups(Punctuation, Name.Builtin, Punctuation, None,\n                      String, using(PythonLexer), Punctuation)),\n            # directives: if, for, prefer, test\n            (r'(\\$)(\\w+)(\\{(%)?)(.*?)((?(4)%)\\})',\n             bygroups(Punctuation, Name.Builtin, Punctuation, None,\n                      using(PythonLexer), Punctuation)),\n            # directive clauses (no {} expression)\n            (r'(\\$)(else|rof|fi)', bygroups(Punctuation, Name.Builtin)),\n            # expressions\n            (r'(\\$\\{(%)?)(.*?)((!)(.*?))?((?(2)%)\\})',\n             bygroups(Punctuation, None, using(PythonLexer),\n                      Name.Builtin, None, None, Punctuation)),\n            (r'#', Other),\n        ],\n        'comment': [\n            (r'[^\\]#]', Comment.Multiline),\n            (r'#\\[', Comment.Multiline, '#push'),\n            (r'\\]#', Comment.Multiline, '#pop'),\n            (r'[\\]#]', Comment.Multiline)\n        ],\n    }\n\n    def analyse_text(text):\n        \"\"\"Evoque templates use $evoque, which is unique.\"\"\"\n        if '$evoque' in text:\n            return 1\n\nclass EvoqueHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `EvoqueLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    .. versionadded:: 1.1\n    \"\"\"\n    name = 'HTML+Evoque'\n    aliases = ['html+evoque']\n    filenames = ['*.html']\n    mimetypes = ['text/html+evoque']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, EvoqueLexer, **options)\n\n    def analyse_text(text):\n        return EvoqueLexer.analyse_text(text)\n\n\nclass EvoqueXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `EvoqueLexer` that highlights unlexed data with the\n    `XmlLexer`.\n\n    .. versionadded:: 1.1\n    \"\"\"\n    name = 'XML+Evoque'\n    aliases = ['xml+evoque']\n    filenames = ['*.xml']\n    mimetypes = ['application/xml+evoque']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, EvoqueLexer, **options)\n\n    def analyse_text(text):\n        return EvoqueLexer.analyse_text(text)\n\n\nclass ColdfusionLexer(RegexLexer):\n    \"\"\"\n    Coldfusion statements\n    \"\"\"\n    name = 'cfstatement'\n    aliases = ['cfs']\n    filenames = []\n    mimetypes = []\n    flags = re.IGNORECASE\n\n    tokens = {\n        'root': [\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*(?:.|\\n)*?\\*/', Comment.Multiline),\n            (r'\\+\\+|--', Operator),\n            (r'[-+*/^&=!]', Operator),\n            (r'<=|>=|<|>|==', Operator),\n            (r'mod\\b', Operator),\n            (r'(eq|lt|gt|lte|gte|not|is|and|or)\\b', Operator),\n            (r'\\|\\||&&', Operator),\n            (r'\\?', Operator),\n            (r'\"', String.Double, 'string'),\n            # There is a special rule for allowing html in single quoted\n            # strings, evidently.\n            (r\"'.*?'\", String.Single),\n            (r'\\d+', Number),\n            (r'(if|else|len|var|xml|default|break|switch|component|property|function|do|'\n             r'try|catch|in|continue|for|return|while|required|any|array|binary|boolean|'\n             r'component|date|guid|numeric|query|string|struct|uuid|case)\\b', Keyword),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(application|session|client|cookie|super|this|variables|arguments)\\b',\n             Name.Constant),\n            (r'([a-z_$][\\w.]*)(\\s*)(\\()',\n             bygroups(Name.Function, Text, Punctuation)),\n            (r'[a-z_$][\\w.]*', Name.Variable),\n            (r'[()\\[\\]{};:,.\\\\]', Punctuation),\n            (r'\\s+', Text),\n        ],\n        'string': [\n            (r'\"\"', String.Double),\n            (r'#.+?#', String.Interp),\n            (r'[^\"#]+', String.Double),\n            (r'#', String.Double),\n            (r'\"', String.Double, '#pop'),\n        ],\n    }\n\n\nclass ColdfusionMarkupLexer(RegexLexer):\n    \"\"\"\n    Coldfusion markup only\n    \"\"\"\n    name = 'Coldfusion'\n    aliases = ['cf']\n    filenames = []\n    mimetypes = []\n\n    tokens = {\n        'root': [\n            (r'[^<]+', Other),\n            include('tags'),\n            (r'<[^<>]*', Other),\n        ],\n        'tags': [\n            (r'<!---', Comment.Multiline, 'cfcomment'),\n            (r'(?s)<!--.*?-->', Comment),\n            (r'<cfoutput.*?>', Name.Builtin, 'cfoutput'),\n            (r'(?s)(<cfscript.*?>)(.+?)(</cfscript.*?>)',\n             bygroups(Name.Builtin, using(ColdfusionLexer), Name.Builtin)),\n            # negative lookbehind is for strings with embedded >\n            (r'(?s)(</?cf(?:component|include|if|else|elseif|loop|return|'\n             r'dbinfo|dump|abort|location|invoke|throw|file|savecontent|'\n             r'mailpart|mail|header|content|zip|image|lock|argument|try|'\n             r'catch|break|directory|http|set|function|param)\\b)(.*?)((?<!\\\\)>)',\n             bygroups(Name.Builtin, using(ColdfusionLexer), Name.Builtin)),\n        ],\n        'cfoutput': [\n            (r'[^#<]+', Other),\n            (r'(#)(.*?)(#)', bygroups(Punctuation, using(ColdfusionLexer),\n                                      Punctuation)),\n            # (r'<cfoutput.*?>', Name.Builtin, '#push'),\n            (r'</cfoutput.*?>', Name.Builtin, '#pop'),\n            include('tags'),\n            (r'(?s)<[^<>]*', Other),\n            (r'#', Other),\n        ],\n        'cfcomment': [\n            (r'<!---', Comment.Multiline, '#push'),\n            (r'--->', Comment.Multiline, '#pop'),\n            (r'([^<-]|<(?!!---)|-(?!-->))+', Comment.Multiline),\n        ],\n    }\n\n\nclass ColdfusionHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Coldfusion markup in html\n    \"\"\"\n    name = 'Coldfusion HTML'\n    aliases = ['cfm']\n    filenames = ['*.cfm', '*.cfml']\n    mimetypes = ['application/x-coldfusion']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, ColdfusionMarkupLexer, **options)\n\n\nclass ColdfusionCFCLexer(DelegatingLexer):\n    \"\"\"\n    Coldfusion markup/script components\n\n    .. versionadded:: 2.0\n    \"\"\"\n    name = 'Coldfusion CFC'\n    aliases = ['cfc']\n    filenames = ['*.cfc']\n    mimetypes = []\n\n    def __init__(self, **options):\n        super().__init__(ColdfusionHtmlLexer, ColdfusionLexer, **options)\n\n\nclass SspLexer(DelegatingLexer):\n    \"\"\"\n    Lexer for Scalate Server Pages.\n\n    .. versionadded:: 1.4\n    \"\"\"\n    name = 'Scalate Server Page'\n    aliases = ['ssp']\n    filenames = ['*.ssp']\n    mimetypes = ['application/x-ssp']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, JspRootLexer, **options)\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'val \\w+\\s*:', text):\n            rv += 0.6\n        if looks_like_xml(text):\n            rv += 0.2\n        if '<%' in text and '%>' in text:\n            rv += 0.1\n        return rv\n\n\nclass TeaTemplateRootLexer(RegexLexer):\n    \"\"\"\n    Base for the `TeaTemplateLexer`. Yields `Token.Other` for area outside of\n    code blocks.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    tokens = {\n        'root': [\n            (r'<%\\S?', Keyword, 'sec'),\n            (r'[^<]+', Other),\n            (r'<', Other),\n        ],\n        'sec': [\n            (r'%>', Keyword, '#pop'),\n            # note: '\\w\\W' != '.' without DOTALL.\n            (r'[\\w\\W]+?(?=%>|\\Z)', using(TeaLangLexer)),\n        ],\n    }\n\n\nclass TeaTemplateLexer(DelegatingLexer):\n    \"\"\"\n    Lexer for `Tea Templates <http://teatrove.org/>`_.\n\n    .. versionadded:: 1.5\n    \"\"\"\n    name = 'Tea'\n    aliases = ['tea']\n    filenames = ['*.tea']\n    mimetypes = ['text/x-tea']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, TeaTemplateRootLexer, **options)\n\n    def analyse_text(text):\n        rv = TeaLangLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        if '<%' in text and '%>' in text:\n            rv += 0.1\n        return rv\n\n\nclass LassoHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `LassoLexer` which highlights unhandled data with the\n    `HtmlLexer`.\n\n    Nested JavaScript and CSS is also highlighted.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'HTML+Lasso'\n    aliases = ['html+lasso']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml', '*.lasso', '*.lasso[89]',\n                       '*.incl', '*.inc', '*.las']\n    mimetypes = ['text/html+lasso',\n                 'application/x-httpd-lasso',\n                 'application/x-httpd-lasso[89]']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, LassoLexer, **options)\n\n    def analyse_text(text):\n        rv = LassoLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):  # same as HTML lexer\n            rv += 0.5\n        return rv\n\n\nclass LassoXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `LassoLexer` which highlights unhandled data with the\n    `XmlLexer`.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'XML+Lasso'\n    aliases = ['xml+lasso']\n    alias_filenames = ['*.xml', '*.lasso', '*.lasso[89]',\n                       '*.incl', '*.inc', '*.las']\n    mimetypes = ['application/xml+lasso']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, LassoLexer, **options)\n\n    def analyse_text(text):\n        rv = LassoLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass LassoCssLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `LassoLexer` which highlights unhandled data with the\n    `CssLexer`.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'CSS+Lasso'\n    aliases = ['css+lasso']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+lasso']\n\n    def __init__(self, **options):\n        options['requiredelimiters'] = True\n        super().__init__(CssLexer, LassoLexer, **options)\n\n    def analyse_text(text):\n        rv = LassoLexer.analyse_text(text) - 0.05\n        if re.search(r'\\w+:[^;]+;', text):\n            rv += 0.1\n        if 'padding:' in text:\n            rv += 0.1\n        return rv\n\n\nclass LassoJavascriptLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `LassoLexer` which highlights unhandled data with the\n    `JavascriptLexer`.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'JavaScript+Lasso'\n    aliases = ['js+lasso', 'javascript+lasso']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+lasso',\n                 'text/x-javascript+lasso',\n                 'text/javascript+lasso']\n\n    def __init__(self, **options):\n        options['requiredelimiters'] = True\n        super().__init__(JavascriptLexer, LassoLexer, **options)\n\n    def analyse_text(text):\n        rv = LassoLexer.analyse_text(text) - 0.05\n        return rv\n\n\nclass HandlebarsLexer(RegexLexer):\n    \"\"\"\n    Generic `handlebars <http://handlebarsjs.com/>` template lexer.\n\n    Highlights only the Handlebars template tags (stuff between `{{` and `}}`).\n    Everything else is left for a delegating lexer.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = \"Handlebars\"\n    aliases = ['handlebars']\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Other),\n\n            # Comment start {{!  }} or {{!--\n            (r'\\{\\{!.*\\}\\}', Comment),\n\n            # HTML Escaping open {{{expression\n            (r'(\\{\\{\\{)(\\s*)', bygroups(Comment.Special, Text), 'tag'),\n\n            # {{blockOpen {{#blockOpen {{/blockClose with optional tilde ~\n            (r'(\\{\\{)([#~/]+)([^\\s}]*)',\n             bygroups(Comment.Preproc, Number.Attribute, Number.Attribute), 'tag'),\n            (r'(\\{\\{)(\\s*)', bygroups(Comment.Preproc, Text), 'tag'),\n        ],\n\n        'tag': [\n            (r'\\s+', Text),\n            # HTML Escaping close }}}\n            (r'\\}\\}\\}', Comment.Special, '#pop'),\n            # blockClose}}, includes optional tilde ~\n            (r'(~?)(\\}\\})', bygroups(Number, Comment.Preproc), '#pop'),\n\n            # {{opt=something}}\n            (r'([^\\s}]+)(=)', bygroups(Name.Attribute, Operator)),\n\n            # Partials {{> ...}}\n            (r'(>)(\\s*)(@partial-block)', bygroups(Keyword, Text, Keyword)),\n            (r'(#?>)(\\s*)([\\w-]+)', bygroups(Keyword, Text, Name.Variable)),\n            (r'(>)(\\s*)(\\()', bygroups(Keyword, Text, Punctuation),\n             'dynamic-partial'),\n\n            include('generic'),\n        ],\n        'dynamic-partial': [\n            (r'\\s+', Text),\n            (r'\\)', Punctuation, '#pop'),\n\n            (r'(lookup)(\\s+)(\\.|this)(\\s+)', bygroups(Keyword, Text,\n                                                      Name.Variable, Text)),\n            (r'(lookup)(\\s+)(\\S+)', bygroups(Keyword, Text,\n                                             using(this, state='variable'))),\n            (r'[\\w-]+', Name.Function),\n\n            include('generic'),\n        ],\n        'variable': [\n            (r'[()/@a-zA-Z][\\w-]*', Name.Variable),\n            (r'\\.[\\w-]+', Name.Variable),\n            (r'(this\\/|\\.\\/|(\\.\\.\\/)+)[\\w-]+', Name.Variable),\n        ],\n        'generic': [\n            include('variable'),\n\n            # borrowed from DjangoLexer\n            (r':?\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\":?'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n        ]\n    }\n\n\nclass HandlebarsHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `HandlebarsLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = \"HTML+Handlebars\"\n    aliases = [\"html+handlebars\"]\n    filenames = ['*.handlebars', '*.hbs']\n    mimetypes = ['text/html+handlebars', 'text/x-handlebars-template']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, HandlebarsLexer, **options)\n\n\nclass YamlJinjaLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `YamlLexer`.\n\n    Commonly used in Saltstack salt states.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'YAML+Jinja'\n    aliases = ['yaml+jinja', 'salt', 'sls']\n    filenames = ['*.sls']\n    mimetypes = ['text/x-yaml+jinja', 'text/x-sls']\n\n    def __init__(self, **options):\n        super().__init__(YamlLexer, DjangoLexer, **options)\n\n\nclass LiquidLexer(RegexLexer):\n    \"\"\"\n    Lexer for `Liquid templates\n    <http://www.rubydoc.info/github/Shopify/liquid>`_.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    name = 'liquid'\n    aliases = ['liquid']\n    filenames = ['*.liquid']\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Text),\n            # tags and block tags\n            (r'(\\{%)(\\s*)', bygroups(Punctuation, Whitespace), 'tag-or-block'),\n            # output tags\n            (r'(\\{\\{)(\\s*)([^\\s}]+)',\n             bygroups(Punctuation, Whitespace, using(this, state = 'generic')),\n             'output'),\n            (r'\\{', Text)\n        ],\n\n        'tag-or-block': [\n            # builtin logic blocks\n            (r'(if|unless|elsif|case)(?=\\s+)', Keyword.Reserved, 'condition'),\n            (r'(when)(\\s+)', bygroups(Keyword.Reserved, Whitespace),\n             combined('end-of-block', 'whitespace', 'generic')),\n            (r'(else)(\\s*)(%\\})',\n             bygroups(Keyword.Reserved, Whitespace, Punctuation), '#pop'),\n\n            # other builtin blocks\n            (r'(capture)(\\s+)([^\\s%]+)(\\s*)(%\\})',\n             bygroups(Name.Tag, Whitespace, using(this, state = 'variable'),\n                      Whitespace, Punctuation), '#pop'),\n            (r'(comment)(\\s*)(%\\})',\n             bygroups(Name.Tag, Whitespace, Punctuation), 'comment'),\n            (r'(raw)(\\s*)(%\\})',\n             bygroups(Name.Tag, Whitespace, Punctuation), 'raw'),\n\n            # end of block\n            (r'(end(case|unless|if))(\\s*)(%\\})',\n             bygroups(Keyword.Reserved, None, Whitespace, Punctuation), '#pop'),\n            (r'(end([^\\s%]+))(\\s*)(%\\})',\n             bygroups(Name.Tag, None, Whitespace, Punctuation), '#pop'),\n\n            # builtin tags (assign and include are handled together with usual tags)\n            (r'(cycle)(\\s+)(?:([^\\s:]*)(:))?(\\s*)',\n             bygroups(Name.Tag, Whitespace,\n                      using(this, state='generic'), Punctuation, Whitespace),\n             'variable-tag-markup'),\n\n            # other tags or blocks\n            (r'([^\\s%]+)(\\s*)', bygroups(Name.Tag, Whitespace), 'tag-markup')\n        ],\n\n        'output': [\n            include('whitespace'),\n            (r'\\}\\}', Punctuation, '#pop'),  # end of output\n\n            (r'\\|', Punctuation, 'filters')\n        ],\n\n        'filters': [\n            include('whitespace'),\n            (r'\\}\\}', Punctuation, ('#pop', '#pop')),  # end of filters and output\n\n            (r'([^\\s|:]+)(:?)(\\s*)',\n             bygroups(Name.Function, Punctuation, Whitespace), 'filter-markup')\n        ],\n\n        'filter-markup': [\n            (r'\\|', Punctuation, '#pop'),\n            include('end-of-tag'),\n            include('default-param-markup')\n        ],\n\n        'condition': [\n            include('end-of-block'),\n            include('whitespace'),\n\n            (r'([^\\s=!><]+)(\\s*)([=!><]=?)(\\s*)(\\S+)(\\s*)(%\\})',\n             bygroups(using(this, state = 'generic'), Whitespace, Operator,\n                      Whitespace, using(this, state = 'generic'), Whitespace,\n                      Punctuation)),\n            (r'\\b!', Operator),\n            (r'\\bnot\\b', Operator.Word),\n            (r'([\\w.\\'\"]+)(\\s+)(contains)(\\s+)([\\w.\\'\"]+)',\n             bygroups(using(this, state = 'generic'), Whitespace, Operator.Word,\n                      Whitespace, using(this, state = 'generic'))),\n\n            include('generic'),\n            include('whitespace')\n        ],\n\n        'generic-value': [\n            include('generic'),\n            include('end-at-whitespace')\n        ],\n\n        'operator': [\n            (r'(\\s*)((=|!|>|<)=?)(\\s*)',\n             bygroups(Whitespace, Operator, None, Whitespace), '#pop'),\n            (r'(\\s*)(\\bcontains\\b)(\\s*)',\n             bygroups(Whitespace, Operator.Word, Whitespace), '#pop'),\n        ],\n\n        'end-of-tag': [\n            (r'\\}\\}', Punctuation, '#pop')\n        ],\n\n        'end-of-block': [\n            (r'%\\}', Punctuation, ('#pop', '#pop'))\n        ],\n\n        'end-at-whitespace': [\n            (r'\\s+', Whitespace, '#pop')\n        ],\n\n        # states for unknown markup\n        'param-markup': [\n            include('whitespace'),\n            # params with colons or equals\n            (r'([^\\s=:]+)(\\s*)(=|:)',\n             bygroups(Name.Attribute, Whitespace, Operator)),\n            # explicit variables\n            (r'(\\{\\{)(\\s*)([^\\s}])(\\s*)(\\}\\})',\n             bygroups(Punctuation, Whitespace, using(this, state = 'variable'),\n                      Whitespace, Punctuation)),\n\n            include('string'),\n            include('number'),\n            include('keyword'),\n            (r',', Punctuation)\n        ],\n\n        'default-param-markup': [\n            include('param-markup'),\n            (r'.', Text)  # fallback for switches / variables / un-quoted strings / ...\n        ],\n\n        'variable-param-markup': [\n            include('param-markup'),\n            include('variable'),\n            (r'.', Text)  # fallback\n        ],\n\n        'tag-markup': [\n            (r'%\\}', Punctuation, ('#pop', '#pop')),  # end of tag\n            include('default-param-markup')\n        ],\n\n        'variable-tag-markup': [\n            (r'%\\}', Punctuation, ('#pop', '#pop')),  # end of tag\n            include('variable-param-markup')\n        ],\n\n        # states for different values types\n        'keyword': [\n            (r'\\b(false|true)\\b', Keyword.Constant)\n        ],\n\n        'variable': [\n            (r'[a-zA-Z_]\\w*', Name.Variable),\n            (r'(?<=\\w)\\.(?=\\w)', Punctuation)\n        ],\n\n        'string': [\n            (r\"'[^']*'\", String.Single),\n            (r'\"[^\"]*\"', String.Double)\n        ],\n\n        'number': [\n            (r'\\d+\\.\\d+', Number.Float),\n            (r'\\d+', Number.Integer)\n        ],\n\n        'generic': [  # decides for variable, string, keyword or number\n            include('keyword'),\n            include('string'),\n            include('number'),\n            include('variable')\n        ],\n\n        'whitespace': [\n            (r'[ \\t]+', Whitespace)\n        ],\n\n        # states for builtin blocks\n        'comment': [\n            (r'(\\{%)(\\s*)(endcomment)(\\s*)(%\\})',\n             bygroups(Punctuation, Whitespace, Name.Tag, Whitespace,\n                      Punctuation), ('#pop', '#pop')),\n            (r'.', Comment)\n        ],\n\n        'raw': [\n            (r'[^{]+', Text),\n            (r'(\\{%)(\\s*)(endraw)(\\s*)(%\\})',\n             bygroups(Punctuation, Whitespace, Name.Tag, Whitespace,\n                      Punctuation), '#pop'),\n            (r'\\{', Text)\n        ],\n    }\n\n\nclass TwigLexer(RegexLexer):\n    \"\"\"\n    `Twig <http://twig.sensiolabs.org/>`_ template lexer.\n\n    It just highlights Twig code between the preprocessor directives,\n    other data is left untouched by the lexer.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'Twig'\n    aliases = ['twig']\n    mimetypes = ['application/x-twig']\n\n    flags = re.M | re.S\n\n    # Note that a backslash is included in the following two patterns\n    # PHP uses a backslash as a namespace separator\n    _ident_char = r'[\\\\\\w-]|[^\\x00-\\x7f]'\n    _ident_begin = r'(?:[\\\\_a-z]|[^\\x00-\\x7f])'\n    _ident_end = r'(?:' + _ident_char + ')*'\n    _ident_inner = _ident_begin + _ident_end\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Other),\n            (r'\\{\\{', Comment.Preproc, 'var'),\n            # twig comments\n            (r'\\{\\#.*?\\#\\}', Comment),\n            # raw twig blocks\n            (r'(\\{%)(-?\\s*)(raw)(\\s*-?)(%\\})(.*?)'\n             r'(\\{%)(-?\\s*)(endraw)(\\s*-?)(%\\})',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Comment.Preproc,\n                      Other, Comment.Preproc, Text, Keyword, Text,\n                      Comment.Preproc)),\n            (r'(\\{%)(-?\\s*)(verbatim)(\\s*-?)(%\\})(.*?)'\n             r'(\\{%)(-?\\s*)(endverbatim)(\\s*-?)(%\\})',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Comment.Preproc,\n                      Other, Comment.Preproc, Text, Keyword, Text,\n                      Comment.Preproc)),\n            # filter blocks\n            (r'(\\{%%)(-?\\s*)(filter)(\\s+)(%s)' % _ident_inner,\n             bygroups(Comment.Preproc, Text, Keyword, Text, Name.Function),\n             'tag'),\n            (r'(\\{%)(-?\\s*)([a-zA-Z_]\\w*)',\n             bygroups(Comment.Preproc, Text, Keyword), 'tag'),\n            (r'\\{', Other),\n        ],\n        'varnames': [\n            (r'(\\|)(\\s*)(%s)' % _ident_inner,\n             bygroups(Operator, Text, Name.Function)),\n            (r'(is)(\\s+)(not)?(\\s*)(%s)' % _ident_inner,\n             bygroups(Keyword, Text, Keyword, Text, Name.Function)),\n            (r'(?i)(true|false|none|null)\\b', Keyword.Pseudo),\n            (r'(in|not|and|b-and|or|b-or|b-xor|is'\n             r'if|elseif|else|import'\n             r'constant|defined|divisibleby|empty|even|iterable|odd|sameas'\n             r'matches|starts\\s+with|ends\\s+with)\\b',\n             Keyword),\n            (r'(loop|block|parent)\\b', Name.Builtin),\n            (_ident_inner, Name.Variable),\n            (r'\\.' + _ident_inner, Name.Variable),\n            (r'\\.[0-9]+', Number),\n            (r':?\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\":?'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'([{}()\\[\\]+\\-*/,:~%]|\\.\\.|\\?|:|\\*\\*|\\/\\/|!=|[><=]=?)', Operator),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n        ],\n        'var': [\n            (r'\\s+', Text),\n            (r'(-?)(\\}\\})', bygroups(Text, Comment.Preproc), '#pop'),\n            include('varnames')\n        ],\n        'tag': [\n            (r'\\s+', Text),\n            (r'(-?)(%\\})', bygroups(Text, Comment.Preproc), '#pop'),\n            include('varnames'),\n            (r'.', Punctuation),\n        ],\n    }\n\n\nclass TwigHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `TwigLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = \"HTML+Twig\"\n    aliases = [\"html+twig\"]\n    filenames = ['*.twig']\n    mimetypes = ['text/html+twig']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, TwigLexer, **options)\n\n\nclass Angular2Lexer(RegexLexer):\n    \"\"\"\n    Generic\n    `angular2 <http://victorsavkin.com/post/119943127151/angular-2-template-syntax>`_\n    template lexer.\n\n    Highlights only the Angular template tags (stuff between `{{` and `}}` and\n    special attributes: '(event)=', '[property]=', '[(twoWayBinding)]=').\n    Everything else is left for a delegating lexer.\n\n    .. versionadded:: 2.1\n    \"\"\"\n\n    name = \"Angular2\"\n    aliases = ['ng2']\n\n    tokens = {\n        'root': [\n            (r'[^{([*#]+', Other),\n\n            # {{meal.name}}\n            (r'(\\{\\{)(\\s*)', bygroups(Comment.Preproc, Text), 'ngExpression'),\n\n            # (click)=\"deleteOrder()\"; [value]=\"test\"; [(twoWayTest)]=\"foo.bar\"\n            (r'([([]+)([\\w:.-]+)([\\])]+)(\\s*)(=)(\\s*)',\n             bygroups(Punctuation, Name.Attribute, Punctuation, Text, Operator, Text),\n             'attr'),\n            (r'([([]+)([\\w:.-]+)([\\])]+)(\\s*)',\n             bygroups(Punctuation, Name.Attribute, Punctuation, Text)),\n\n            # *ngIf=\"...\"; #f=\"ngForm\"\n            (r'([*#])([\\w:.-]+)(\\s*)(=)(\\s*)',\n             bygroups(Punctuation, Name.Attribute, Text, Operator, Text), 'attr'),\n            (r'([*#])([\\w:.-]+)(\\s*)',\n             bygroups(Punctuation, Name.Attribute, Text)),\n        ],\n\n        'ngExpression': [\n            (r'\\s+(\\|\\s+)?', Text),\n            (r'\\}\\}', Comment.Preproc, '#pop'),\n\n            # Literals\n            (r':?(true|false)', String.Boolean),\n            (r':?\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\":?'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n\n            # Variabletext\n            (r'[a-zA-Z][\\w-]*(\\(.*\\))?', Name.Variable),\n            (r'\\.[\\w-]+(\\(.*\\))?', Name.Variable),\n\n            # inline If\n            (r'(\\?)(\\s*)([^}\\s]+)(\\s*)(:)(\\s*)([^}\\s]+)(\\s*)',\n             bygroups(Operator, Text, String, Text, Operator, Text, String, Text)),\n        ],\n        'attr': [\n            ('\".*?\"', String, '#pop'),\n            (\"'.*?'\", String, '#pop'),\n            (r'[^\\s>]+', String, '#pop'),\n        ],\n    }\n\n\nclass Angular2HtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `Angular2Lexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = \"HTML + Angular2\"\n    aliases = [\"html+ng2\"]\n    filenames = ['*.ng2']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, Angular2Lexer, **options)\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.varnish\n    ~~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for Varnish configuration\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pygments.lexer import RegexLexer, include, bygroups, using, this, \\\n    inherit, words\nfrom pygments.token import Text, Comment, Operator, Keyword, Name, String, \\\n    Number, Punctuation, Literal\n\n__all__ = ['VCLLexer', 'VCLSnippetLexer']\n\n\nclass VCLLexer(RegexLexer):\n    \"\"\"\n    For Varnish Configuration Language (VCL).\n\n    .. versionadded:: 2.2\n    \"\"\"\n    name = 'VCL'\n    aliases = ['vcl']\n    filenames = ['*.vcl']\n    mimetypes = ['text/x-vclsrc']\n\n    def analyse_text(text):\n        # If the very first line is 'vcl 4.0;' it's pretty much guaranteed\n        # that this is VCL\n        if text.startswith('vcl 4.0;'):\n            return 1.0\n        # Skip over comments and blank lines\n        # This is accurate enough that returning 0.9 is reasonable.\n        # Almost no VCL files start without some comments.\n        elif '\\nvcl 4.0;' in text[:1000]:\n            return 0.9\n\n    tokens = {\n        'probe': [\n            include('whitespace'),\n            include('comments'),\n            (r'(\\.\\w+)(\\s*=\\s*)([^;]*)(;)',\n             bygroups(Name.Attribute, Operator, using(this), Punctuation)),\n            (r'\\}', Punctuation, '#pop'),\n        ],\n        'acl': [\n            include('whitespace'),\n            include('comments'),\n            (r'[!/]+', Operator),\n            (r';', Punctuation),\n            (r'\\d+', Number),\n            (r'\\}', Punctuation, '#pop'),\n        ],\n        'backend': [\n            include('whitespace'),\n            (r'(\\.probe)(\\s*=\\s*)(\\w+)(;)',\n             bygroups(Name.Attribute, Operator, Name.Variable.Global, Punctuation)),\n            (r'(\\.probe)(\\s*=\\s*)(\\{)',\n             bygroups(Name.Attribute, Operator, Punctuation), 'probe'),\n            (r'(\\.\\w+\\b)(\\s*=\\s*)([^;]*)(\\s*;)',\n             bygroups(Name.Attribute, Operator, using(this), Punctuation)),\n            (r'\\{', Punctuation, '#push'),\n            (r'\\}', Punctuation, '#pop'),\n        ],\n        'statements': [\n            (r'(\\d\\.)?\\d+[sdwhmy]', Literal.Date),\n            (r'(\\d\\.)?\\d+ms', Literal.Date),\n            (r'(vcl_pass|vcl_hash|vcl_hit|vcl_init|vcl_backend_fetch|vcl_pipe|'\n             r'vcl_backend_response|vcl_synth|vcl_deliver|vcl_backend_error|'\n             r'vcl_fini|vcl_recv|vcl_purge|vcl_miss)\\b', Name.Function),\n            (r'(pipe|retry|hash|synth|deliver|purge|abandon|lookup|pass|fail|ok|'\n             r'miss|fetch|restart)\\b', Name.Constant),\n            (r'(beresp|obj|resp|req|req_top|bereq)\\.http\\.[a-zA-Z_-]+\\b', Name.Variable),\n            (words((\n                'obj.status', 'req.hash_always_miss', 'beresp.backend', 'req.esi_level',\n                'req.can_gzip', 'beresp.ttl', 'obj.uncacheable', 'req.ttl', 'obj.hits',\n                'client.identity', 'req.hash_ignore_busy', 'obj.reason', 'req.xid',\n                'req_top.proto', 'beresp.age', 'obj.proto', 'obj.age', 'local.ip',\n                'beresp.uncacheable', 'req.method', 'beresp.backend.ip', 'now',\n                'obj.grace', 'req.restarts', 'beresp.keep', 'req.proto', 'resp.proto',\n                'bereq.xid', 'bereq.between_bytes_timeout', 'req.esi',\n                'bereq.first_byte_timeout', 'bereq.method', 'bereq.connect_timeout',\n                'beresp.do_gzip',  'resp.status', 'beresp.do_gunzip',\n                'beresp.storage_hint', 'resp.is_streaming', 'beresp.do_stream',\n                'req_top.method', 'bereq.backend', 'beresp.backend.name', 'beresp.status',\n                'req.url', 'obj.keep', 'obj.ttl', 'beresp.reason', 'bereq.retries',\n                'resp.reason', 'bereq.url', 'beresp.do_esi', 'beresp.proto', 'client.ip',\n                'bereq.proto', 'server.hostname', 'remote.ip', 'req.backend_hint',\n                'server.identity', 'req_top.url', 'beresp.grace', 'beresp.was_304',\n                'server.ip', 'bereq.uncacheable'), suffix=r'\\b'),\n             Name.Variable),\n            (r'[!%&+*\\-,/<.}{>=|~]+', Operator),\n            (r'[();]', Punctuation),\n\n            (r'[,]+', Punctuation),\n            (words(('hash_data', 'regsub', 'regsuball', 'if', 'else',\n                    'elsif', 'elif', 'synth', 'synthetic', 'ban',\n                    'return', 'set', 'unset', 'import', 'include', 'new',\n                    'rollback', 'call'), suffix=r'\\b'),\n             Keyword),\n            (r'storage\\.\\w+\\.\\w+\\b', Name.Variable),\n            (words(('true', 'false')), Name.Builtin),\n            (r'\\d+\\b', Number),\n            (r'(backend)(\\s+\\w+)(\\s*\\{)',\n             bygroups(Keyword, Name.Variable.Global, Punctuation), 'backend'),\n            (r'(probe\\s)(\\s*\\w+\\s)(\\{)',\n             bygroups(Keyword, Name.Variable.Global, Punctuation), 'probe'),\n            (r'(acl\\s)(\\s*\\w+\\s)(\\{)',\n             bygroups(Keyword, Name.Variable.Global, Punctuation), 'acl'),\n            (r'(vcl )(4.0)(;)$',\n             bygroups(Keyword.Reserved, Name.Constant, Punctuation)),\n            (r'(sub\\s+)([a-zA-Z]\\w*)(\\s*\\{)',\n                bygroups(Keyword, Name.Function, Punctuation)),\n            (r'([a-zA-Z_]\\w*)'\n             r'(\\.)'\n             r'([a-zA-Z_]\\w*)'\n             r'(\\s*\\(.*\\))',\n             bygroups(Name.Function, Punctuation, Name.Function, using(this))),\n            (r'[a-zA-Z_]\\w*', Name),\n        ],\n        'comment': [\n            (r'[^*/]+', Comment.Multiline),\n            (r'/\\*', Comment.Multiline, '#push'),\n            (r'\\*/', Comment.Multiline, '#pop'),\n            (r'[*/]', Comment.Multiline),\n        ],\n        'comments': [\n            (r'#.*$', Comment),\n            (r'/\\*', Comment.Multiline, 'comment'),\n            (r'//.*$', Comment),\n        ],\n        'string': [\n            (r'\"', String, '#pop'),\n            (r'[^\"\\n]+', String),  # all other characters\n        ],\n        'multistring': [\n            (r'[^\"}]', String),\n            (r'\"\\}', String, '#pop'),\n            (r'[\"}]', String),\n        ],\n        'whitespace': [\n            (r'L?\"', String, 'string'),\n            (r'\\{\"', String, 'multistring'),\n            (r'\\n', Text),\n            (r'\\s+', Text),\n            (r'\\\\\\n', Text),  # line continuation\n        ],\n        'root': [\n            include('whitespace'),\n            include('comments'),\n            include('statements'),\n            (r'\\s+', Text),\n        ],\n    }\n\n\nclass VCLSnippetLexer(VCLLexer):\n    \"\"\"\n    For Varnish Configuration Language snippets.\n\n    .. versionadded:: 2.2\n    \"\"\"\n    name = 'VCLSnippets'\n    aliases = ['vclsnippets', 'vclsnippet']\n    mimetypes = ['text/x-vclsnippet']\n    filenames = []\n\n    def analyse_text(text):\n        # override method inherited from VCLLexer\n        return 0\n\n    tokens = {\n        'snippetspre': [\n            (r'\\.\\.\\.+', Comment),\n            (r'(bereq|req|req_top|resp|beresp|obj|client|server|local|remote|'\n             r'storage)($|\\.\\*)', Name.Variable),\n        ],\n        'snippetspost': [\n            (r'(backend)\\b', Keyword.Reserved),\n        ],\n        'root': [\n            include('snippetspre'),\n            inherit,\n            include('snippetspost'),\n        ],\n    }\n"], "fixing_code": ["Pygments changelog\n==================\n\nSince 2.5.0, issue numbers refer to the tracker at\n<https://github.com/pygments/pygments/issues>,\npull request numbers to the requests at\n<https://github.com/pygments/pygments/pulls>.\n\n\nVersion 2.8.0\n-------------\n(not released yet)\n\n\nVersion 2.7.4\n-------------\n(released January 9, 2021)\n\n- Updated lexers:\n\n  - Apache configurations: Improve handling of malformed tags (#1656)\n  - Crystal (#1650)\n  - Coq (#1648)\n  - Fortran: Add missing ``ONLY`` keyword (#1635)\n  - Ini (#1624)\n  - JavaScript and variants (#1647 -- missing regex flags, #1651)\n  - Markdown (#1623)\n  - Shell\n\n    - Lex trailing whitespace as part of the prompt (#1645)\n    - Add missing ``in`` keyword (#1652)\n    \n  - Typescript: Fix incorrect punctuation handling (#1510, #1511)\n\n- Fix infinite loop in SML lexer (#1625)\n- Fix backtracking string regexes in JavaScript/TypeScript, Modula2\n  and many other lexers (#1637)\n- Limit recursion with nesting Ruby heredocs (#1638)\n- Fix a few inefficient regexes for guessing lexers\n- Fix the raw token lexer handling of Unicode (#1616)\n- Revert a private API change in the HTML formatter (#1655) -- \n  please note that private APIs remain subject to change!\n- Add Dracula theme style (#1636)\n- Fix several exponential/cubic-complexity regexes found by\n  Ben Caller/Doyensec (#1675)\n\nThanks to Google's OSS-Fuzz project for finding many of these bugs.\n\n\nVersion 2.7.3\n-------------\n(released December 6, 2020)\n\n- Updated lexers:\n\n  * Ada (#1581)\n  * HTML (#1615, #1614)\n  * Java (#1594, #1586)\n  * JavaScript (#1605, #1589, #1588)\n  * JSON (#1569 -- this is a complete rewrite)\n  * Lean (#1601)\n  * LLVM (#1612)\n  * Mason (#1592)\n  * MySQL (#1555, #1551)\n  * Rust (#1608)\n  * Turtle (#1590, #1553)\n\n- Deprecated JsonBareObjectLexer, which is now identical to JsonLexer (#1600)\n- The ``ImgFormatter`` now calculates the exact character width, which fixes some issues with overlapping text (#1213, #1611)\n- Documentation fixes (#1609, #1599, #1598)\n- Fixed duplicated Juttle language alias (#1604, #1606)\n- Added support for Kotlin scripts (#1587)\n- Removed CSS rule which forced margin to 0\n  \n\nVersion 2.7.2\n-------------\n(released October 24, 2020)\n\n- Updated lexers:\n\n  * Latex (#1517, #1516)\n  * LLVM (#1565)\n  * SPARQL (#1559)\n\n- Fix Python console/traceback lexer problems with custom exceptions without messages (#1548)\n- Allow loading ttc fonts on Mac/image formatter (#1223)\n- Improve ``analyze_text`` across a variety of lexers (#1549)\n- Remove CSS rule which forced the vertical padding to 0 for line numbers (#1583, #1579)\n- Fix ``TNTLexer`` crashing on unexpected EOL (#1568, #1570)\n- ``regexlint`` can be now run locally as part of ``tox`` tests (#1557)\n- Fix typos (#1550, #1562)\n- Add Python 3.9 as a supported version (#1554)\n\n\nVersion 2.7.1\n-------------\n(released September 16, 2020)\n\n- Fixed a regression in the JSON lexer (#1544)\n\n\nVersion 2.7.0\n-------------\n(released September 12, 2020)\n\n- Added lexers:\n\n  * Arrow (#1481, #1499)\n  * BARE (#1488)\n  * Devicetree (#1434)\n  * F* (#1409)\n  * GDScript (#1457)\n  * Pointless (#1494)\n  * PromQL (#1506)\n  * PsySH (#1438)\n  * Singularity (#1285)\n  * TiddlyWiki5 (#1390)\n  * TNT (#1414)\n  * YANG (#1408, #1428)\n\n- Updated lexers:\n\n  * APL (#1503)\n  * C++ (#1350, which also fixes: #1222, #996, #906, #828, #1162, #1166,\n    #1396)\n  * Chapel (#1423)\n  * CMake (#1491)\n  * CSound (#1509)\n  * Cython (#1507)\n  * Dart (#1449)\n  * Fennel (#1535)\n  * Fortran (#1442)\n  * GAS (#1530)\n  * HTTP (#1432, #1520, #1521)\n  * Inform 6 (#1461)\n  * Javascript (#1533)\n  * JSON (#1065, #1528)\n  * Lean (#1415)\n  * Matlab (#1399)\n  * Markdown (#1492, #1495)\n  * MySQL (#975, #1063, #1453, #1527)\n  * NASM (#1465)\n  * Nim (#1426)\n  * PostgreSQL (#1513)\n  * PowerShell (#1398, #1497)\n  * Protobuf (#1505)\n  * Robot (#1480)\n  * SQL (#1402)\n  * SystemVerilog (#1436, #1452, #1454, #1460, #1462, #1463, #1464, #1471, #1496, #1504)\n  * TeraTerm (#1337)\n  * XML (#1502)\n\n- Added a new filter for math symbols (#1406)\n- The Kconfig lexer will match Kconfig derivative names now (#1458)\n- Improved HTML formatter output (#1500)\n- ``.markdown`` is now recognized as an extension for Markdown files (#1476)\n- Fixed line number colors for Solarized (#1477, #1356)\n- Improvements to exception handling (#1478)\n- Improvements to tests (#1532, #1533, #1539)\n- Various code cleanups (#1536, #1537, #1538)\n\n\nVersion 2.6.1\n-------------\n(released March 8, 2020)\n\n- This release fixes a packaging issue. No functional changes.\n\n\nVersion 2.6\n-----------\n(released March 8, 2020)\n\n- Running Pygments on Python 2.x is no longer supported.\n  (The Python 2 lexer still exists.)\n\n- Added lexers:\n\n  * Linux kernel logs (#1310)\n  * LLVM MIR (#1361)\n  * MiniScript (#1397)\n  * Mosel (#1287, #1326)\n  * Parsing Expression Grammar (#1336)\n  * ReasonML (#1386)\n  * Ride (#1319, #1321)\n  * Sieve (#1257)\n  * USD (#1290)\n  * WebIDL (#1309)\n\n- Updated lexers:\n\n  * Apache2 (#1378)\n  * Chapel (#1357)\n  * CSound (#1383)\n  * D (#1375, #1362)\n  * Haskell (#1347, #1177)\n  * Idris (#1360)\n  * Perl6/Raku lexer (#1344)\n  * Python3 (#1382, #1385)\n  * Rust: Updated lexer to cover more builtins (mostly macros) and miscellaneous\n    new syntax (#1320)\n  * SQL: Add temporal support keywords (#1402)\n\n- The 256-color/true-color terminal formatters now support the italic attribute\n  in styles (#1288)\n- Support HTTP 2/3 header (#1308)\n- Support missing reason in HTTP header (#1322)\n- Boogie/Silver: support line continuations and triggers, move contract keywords\n  to separate category (#1299)\n- GAS: support C-style comments (#1291)\n- Fix names in S lexer (#1330, #1333)\n- Fix numeric literals in Ada (#1334)\n- Recognize ``.mjs`` files as Javascript (#1392)\n- Recognize ``.eex`` files as Elixir (#1387)\n- Fix ``re.MULTILINE`` usage (#1388)\n- Recognize ``pipenv`` and ``poetry`` dependency & lock files (PR#1376)\n- Improve font search on Windows (#1247)\n- Remove unused script block (#1401)\n\n\nVersion 2.5.2\n-------------\n(released November 29, 2019)\n\n- Fix incompatibility with some setuptools versions (PR#1316)\n\n- Fix lexing of ReST field lists (PR#1279)\n- Fix lexing of Matlab keywords as field names (PR#1282)\n- Recognize double-quoted strings in Matlab (PR#1278)\n- Avoid slow backtracking in Vim lexer (PR#1312)\n- Fix Scala highlighting of types (PR#1315)\n- Highlight field lists more consistently in ReST (PR#1279)\n- Fix highlighting Matlab keywords in field names (PR#1282)\n- Recognize Matlab double quoted strings (PR#1278)\n- Add some Terraform keywords\n- Update Modelica lexer to 3.4\n- Update Crystal examples\n\n\nVersion 2.5.1\n-------------\n(released November 26, 2019)\n\n- This release fixes a packaging issue. No functional changes.\n\n\nVersion 2.5.0\n-------------\n(released November 26, 2019)\n\n- Added lexers:\n\n  * Email (PR#1246)\n  * Erlang, Elixir shells (PR#823, #1521)\n  * Notmuch (PR#1264)\n  * `Scdoc <https://git.sr.ht/~sircmpwn/scdoc>`_ (PR#1268)\n  * `Solidity <https://solidity.readthedocs.io/>`_ (#1214)\n  * `Zeek <https://www.zeek.org>`_ (new name for Bro) (PR#1269)\n  * `Zig <https://ziglang.org/>`_ (PR#820)\n\n- Updated lexers:\n\n  * Apache2 Configuration (PR#1251)\n  * Bash sessions (#1253)\n  * CSound (PR#1250)\n  * Dart\n  * Dockerfile\n  * Emacs Lisp\n  * Handlebars (PR#773)\n  * Java (#1101, #987)\n  * Logtalk (PR#1261)\n  * Matlab (PR#1271)\n  * Praat (PR#1277)\n  * Python3 (PR#1255, PR#1400)\n  * Ruby\n  * YAML (#1528)\n  * Velocity\n\n- Added styles:\n\n  * Inkpot (PR#1276)\n\n- The ``PythonLexer`` class is now an alias for the former ``Python3Lexer``.\n  The old ``PythonLexer`` is available as ``Python2Lexer``.  Same change has\n  been done for the ``PythonTracebackLexer``.  The ``python3`` option for\n  the ``PythonConsoleLexer`` is now true by default.\n\n- Bump ``NasmLexer`` priority over ``TasmLexer`` for ``.asm`` files\n  (fixes #1326)\n- Default font in the ``ImageFormatter`` has been updated (#928, PR#1245)\n- Test suite switched to py.test, removed nose dependency (#1490)\n- Reduce ``TeraTerm`` lexer score -- it used to match nearly all languages\n  (#1256)\n- Treat ``Skylark``/``Starlark`` files as Python files (PR#1259)\n- Image formatter: actually respect ``line_number_separator`` option\n\n- Add LICENSE file to wheel builds\n- Agda: fix lambda highlighting\n- Dart: support ``@`` annotations\n- Dockerfile: accept ``FROM ... AS`` syntax\n- Emacs Lisp: add more string functions\n- GAS: accept registers in directive arguments\n- Java: make structural punctuation (braces, parens, colon, comma) ``Punctuation``, not ``Operator`` (#987)\n- Java: support ``var`` contextual keyword (#1101)\n- Matlab: Fix recognition of ``function`` keyword (PR#1271)\n- Python: recognize ``.jy`` filenames (#976)\n- Python: recognize ``f`` string prefix (#1156)\n- Ruby: support squiggly heredocs\n- Shell sessions: recognize Virtualenv prompt (PR#1266)\n- Velocity: support silent reference syntax\n\n\nVersion 2.4.2\n-------------\n(released May 28, 2019)\n\n- Fix encoding error when guessing lexer with given ``encoding`` option\n  (#1438)\n\n\nVersion 2.4.1\n-------------\n(released May 24, 2019)\n\n- Updated lexers:\n\n  * Coq (#1430)\n  * MSDOS Session (PR#734)\n  * NASM (#1517)\n  * Objective-C (PR#813, #1508)\n  * Prolog (#1511)\n  * TypeScript (#1515)\n\n- Support CSS variables in stylesheets (PR#814, #1356)\n- Fix F# lexer name (PR#709)\n- Fix ``TerminalFormatter`` using bold for bright text (#1480)\n\n\nVersion 2.4.0\n-------------\n(released May 8, 2019)\n\n- Added lexers:\n\n  * Augeas (PR#807)\n  * BBC Basic (PR#806)\n  * Boa (PR#756)\n  * Charm++ CI (PR#788)\n  * DASM16 (PR#807)\n  * FloScript (PR#750)\n  * FreeFem++ (PR#785)\n  * Hspec (PR#790)\n  * Pony (PR#627)\n  * SGF (PR#780)\n  * Slash (PR#807)\n  * Slurm (PR#760)\n  * Tera Term Language (PR#749)\n  * TOML (PR#807)\n  * Unicon (PR#731)\n  * VBScript (PR#673)\n\n- Updated lexers:\n\n  * Apache2 (PR#766)\n  * Cypher (PR#746)\n  * LLVM (PR#792)\n  * Makefiles (PR#766)\n  * PHP (#1482)\n  * Rust\n  * SQL (PR#672)\n  * Stan (PR#774)\n  * Stata (PR#800)\n  * Terraform (PR#787)\n  * YAML\n\n- Add solarized style (PR#708)\n- Add support for Markdown reference-style links (PR#753)\n- Add license information to generated HTML/CSS files (#1496)\n- Change ANSI color names (PR#777)\n- Fix catastrophic backtracking in the bash lexer (#1494)\n- Fix documentation failing to build using Sphinx 2.0 (#1501)\n- Fix incorrect links in the Lisp and R lexer documentation (PR#775)\n- Fix rare unicode errors on Python 2.7 (PR#798, #1492)\n- Fix lexers popping from an empty stack (#1506)\n- TypoScript uses ``.typoscript`` now (#1498)\n- Updated Trove classifiers and ``pip`` requirements (PR#799)\n\n\n\nVersion 2.3.1\n-------------\n(released Dec 16, 2018)\n\n- Updated lexers:\n\n  * ASM (PR#784)\n  * Chapel (PR#735)\n  * Clean (PR#621)\n  * CSound (PR#684)\n  * Elm (PR#744)\n  * Fortran (PR#747)\n  * GLSL (PR#740)\n  * Haskell (PR#745)\n  * Hy (PR#754)\n  * Igor Pro (PR#764)\n  * PowerShell (PR#705)\n  * Python (PR#720, #1299, PR#715)\n  * SLexer (PR#680)\n  * YAML (PR#762, PR#724)\n\n- Fix invalid string escape sequences\n- Fix `FutureWarning` introduced by regex changes in Python 3.7\n\n\nVersion 2.3.0\n-------------\n(released Nov 25, 2018)\n\n- Added lexers:\n\n  * Fennel (PR#783)\n  * HLSL (PR#675)\n\n- Updated lexers:\n\n  * Dockerfile (PR#714)\n\n- Minimum Python versions changed to 2.7 and 3.5\n- Added support for Python 3.7 generator changes (PR#772)\n- Fix incorrect token type in SCSS for single-quote strings (#1322)\n- Use `terminal256` formatter if `TERM` contains `256` (PR#666)\n- Fix incorrect handling of GitHub style fences in Markdown (PR#741, #1389)\n- Fix `%a` not being highlighted in Python3 strings (PR#727)\n\n\nVersion 2.2.0\n-------------\n(released Jan 22, 2017)\n\n- Added lexers:\n\n  * AMPL\n  * TypoScript (#1173)\n  * Varnish config (PR#554)\n  * Clean (PR#503)\n  * WDiff (PR#513)\n  * Flatline (PR#551)\n  * Silver (PR#537)\n  * HSAIL (PR#518)\n  * JSGF (PR#546)\n  * NCAR command language (PR#536)\n  * Extempore (PR#530)\n  * Cap'n Proto (PR#595)\n  * Whiley (PR#573)\n  * Monte (PR#592)\n  * Crystal (PR#576)\n  * Snowball (PR#589)\n  * CapDL (PR#579)\n  * NuSMV (PR#564)\n  * SAS, Stata (PR#593)\n\n- Added the ability to load lexer and formatter classes directly from files\n  with the `-x` command line option and the `lexers.load_lexer_from_file()`\n  and `formatters.load_formatter_from_file()` functions. (PR#559)\n\n- Added `lexers.find_lexer_class_by_name()`. (#1203)\n\n- Added new token types and lexing for magic methods and variables in Python\n  and PHP.\n\n- Added a new token type for string affixes and lexing for them in Python, C++\n  and Postgresql lexers.\n\n- Added a new token type for heredoc (and similar) string delimiters and\n  lexing for them in C++, Perl, PHP, Postgresql and Ruby lexers.\n\n- Styles can now define colors with ANSI colors for use in the 256-color\n  terminal formatter. (PR#531)\n\n- Improved the CSS lexer. (#1083, #1130)\n\n- Added \"Rainbow Dash\" style. (PR#623)\n\n- Delay loading `pkg_resources`, which takes a long while to import. (PR#690)\n\n\nVersion 2.1.3\n-------------\n(released Mar 2, 2016)\n\n- Fixed regression in Bash lexer (PR#563)\n\n\nVersion 2.1.2\n-------------\n(released Feb 29, 2016)\n\n- Fixed Python 3 regression in image formatter (#1215)\n- Fixed regression in Bash lexer (PR#562)\n\n\nVersion 2.1.1\n-------------\n(relased Feb 14, 2016)\n\n- Fixed Jython compatibility (#1205)\n- Fixed HTML formatter output with leading empty lines (#1111)\n- Added a mapping table for LaTeX encodings and added utf8 (#1152)\n- Fixed image formatter font searching on Macs (#1188)\n- Fixed deepcopy-ing of Token instances (#1168)\n- Fixed Julia string interpolation (#1170)\n- Fixed statefulness of HttpLexer between get_tokens calls\n- Many smaller fixes to various lexers\n\n\nVersion 2.1\n-----------\n(released Jan 17, 2016)\n\n- Added lexers:\n\n  * Emacs Lisp (PR#431)\n  * Arduino (PR#442)\n  * Modula-2 with multi-dialect support (#1090)\n  * Fortran fixed format (PR#213)\n  * Archetype Definition language (PR#483)\n  * Terraform (PR#432)\n  * Jcl, Easytrieve (PR#208)\n  * ParaSail (PR#381)\n  * Boogie (PR#420)\n  * Turtle (PR#425)\n  * Fish Shell (PR#422)\n  * Roboconf (PR#449)\n  * Test Anything Protocol (PR#428)\n  * Shen (PR#385)\n  * Component Pascal (PR#437)\n  * SuperCollider (PR#472)\n  * Shell consoles (Tcsh, PowerShell, MSDOS) (PR#479)\n  * Elm and J (PR#452)\n  * Crmsh (PR#440)\n  * Praat (PR#492)\n  * CSound (PR#494)\n  * Ezhil (PR#443)\n  * Thrift (PR#469)\n  * QVT Operational (PR#204)\n  * Hexdump (PR#508)\n  * CAmkES Configuration (PR#462)\n\n- Added styles:\n\n  * Lovelace (PR#456)\n  * Algol and Algol-nu (#1090)\n\n- Added formatters:\n\n  * IRC (PR#458)\n  * True color (24-bit) terminal ANSI sequences (#1142)\n    (formatter alias: \"16m\")\n\n- New \"filename\" option for HTML formatter (PR#527).\n\n- Improved performance of the HTML formatter for long lines (PR#504).\n\n- Updated autopygmentize script (PR#445).\n\n- Fixed style inheritance for non-standard token types in HTML output.\n\n- Added support for async/await to Python 3 lexer.\n\n- Rewrote linenos option for TerminalFormatter (it's better, but slightly\n  different output than before) (#1147).\n\n- Javascript lexer now supports most of ES6 (#1100).\n\n- Cocoa builtins updated for iOS 8.1 (PR#433).\n\n- Combined BashSessionLexer and ShellSessionLexer, new version should support\n  the prompt styles of either.\n\n- Added option to pygmentize to show a full traceback on exceptions.\n\n- Fixed incomplete output on Windows and Python 3 (e.g. when using iPython\n  Notebook) (#1153).\n\n- Allowed more traceback styles in Python console lexer (PR#253).\n\n- Added decorators to TypeScript (PR#509).\n\n- Fix highlighting of certain IRC logs formats (#1076).\n\n\nVersion 2.0.2\n-------------\n(released Jan 20, 2015)\n\n- Fix Python tracebacks getting duplicated in the console lexer (#1068).\n\n- Backquote-delimited identifiers are now recognized in F# (#1062).\n\n\nVersion 2.0.1\n-------------\n(released Nov 10, 2014)\n\n- Fix an encoding issue when using ``pygmentize`` with the ``-o`` option.\n\n\nVersion 2.0\n-----------\n(released Nov 9, 2014)\n\n- Default lexer encoding is now \"guess\", i.e. UTF-8 / Locale / Latin1 is\n  tried in that order.\n\n- Major update to Swift lexer (PR#410).\n\n- Multiple fixes to lexer guessing in conflicting cases:\n\n  * recognize HTML5 by doctype\n  * recognize XML by XML declaration\n  * don't recognize C/C++ as SystemVerilog\n\n- Simplified regexes and builtin lists.\n\n\nVersion 2.0rc1\n--------------\n(released Oct 16, 2014)\n\n- Dropped Python 2.4 and 2.5 compatibility.  This is in favor of single-source\n  compatibility between Python 2.6, 2.7 and 3.3+.\n\n- New website and documentation based on Sphinx (finally!)\n\n- Lexers added:\n\n  * APL (#969)\n  * Agda and Literate Agda (PR#203)\n  * Alloy (PR#355)\n  * AmbientTalk\n  * BlitzBasic (PR#197)\n  * ChaiScript (PR#24)\n  * Chapel (PR#256)\n  * Cirru (PR#275)\n  * Clay (PR#184)\n  * ColdFusion CFC (PR#283)\n  * Cryptol and Literate Cryptol (PR#344)\n  * Cypher (PR#257)\n  * Docker config files\n  * EBNF (PR#193)\n  * Eiffel (PR#273)\n  * GAP (PR#311)\n  * Golo (PR#309)\n  * Handlebars (PR#186)\n  * Hy (PR#238)\n  * Idris and Literate Idris (PR#210)\n  * Igor Pro (PR#172)\n  * Inform 6/7 (PR#281)\n  * Intel objdump (PR#279)\n  * Isabelle (PR#386)\n  * Jasmin (PR#349)\n  * JSON-LD (PR#289)\n  * Kal (PR#233)\n  * Lean (PR#399)\n  * LSL (PR#296)\n  * Limbo (PR#291)\n  * Liquid (#977)\n  * MQL (PR#285)\n  * MaskJS (PR#280)\n  * Mozilla preprocessors\n  * Mathematica (PR#245)\n  * NesC (PR#166)\n  * Nit (PR#375)\n  * Nix (PR#267)\n  * Pan\n  * Pawn (PR#211)\n  * Perl 6 (PR#181)\n  * Pig (PR#304)\n  * Pike (PR#237)\n  * QBasic (PR#182)\n  * Red (PR#341)\n  * ResourceBundle (#1038)\n  * Rexx (PR#199)\n  * Rql (PR#251)\n  * Rsl\n  * SPARQL (PR#78)\n  * Slim (PR#366)\n  * Swift (PR#371)\n  * Swig (PR#168)\n  * TADS 3 (PR#407)\n  * Todo.txt todo lists\n  * Twig (PR#404)\n\n- Added a helper to \"optimize\" regular expressions that match one of many\n  literal words; this can save 20% and more lexing time with lexers that\n  highlight many keywords or builtins.\n\n- New styles: \"xcode\" and \"igor\", similar to the default highlighting of\n  the respective IDEs.\n\n- The command-line \"pygmentize\" tool now tries a little harder to find the\n  correct encoding for files and the terminal (#979).\n\n- Added \"inencoding\" option for lexers to override \"encoding\" analogous\n  to \"outencoding\" (#800).\n\n- Added line-by-line \"streaming\" mode for pygmentize with the \"-s\" option.\n  (PR#165)  Only fully works for lexers that have no constructs spanning\n  lines!\n\n- Added an \"envname\" option to the LaTeX formatter to select a replacement\n  verbatim environment (PR#235).\n\n- Updated the Makefile lexer to yield a little more useful highlighting.\n\n- Lexer aliases passed to ``get_lexer_by_name()`` are now case-insensitive.\n\n- File name matching in lexers and formatters will now use a regex cache\n  for speed (PR#205).\n\n- Pygments will now recognize \"vim\" modelines when guessing the lexer for\n  a file based on content (PR#118).\n\n- Major restructure of the ``pygments.lexers`` module namespace.  There are now\n  many more modules with less lexers per module.  Old modules are still around\n  and re-export the lexers they previously contained.\n\n- The NameHighlightFilter now works with any Name.* token type (#790).\n\n- Python 3 lexer: add new exceptions from PEP 3151.\n\n- Opa lexer: add new keywords (PR#170).\n\n- Julia lexer: add keywords and underscore-separated number\n  literals (PR#176).\n\n- Lasso lexer: fix method highlighting, update builtins. Fix\n  guessing so that plain XML isn't always taken as Lasso (PR#163).\n\n- Objective C/C++ lexers: allow \"@\" prefixing any expression (#871).\n\n- Ruby lexer: fix lexing of Name::Space tokens (#860) and of symbols\n  in hashes (#873).\n\n- Stan lexer: update for version 2.4.0 of the language (PR#162, PR#255, PR#377).\n\n- JavaScript lexer: add the \"yield\" keyword (PR#196).\n\n- HTTP lexer: support for PATCH method (PR#190).\n\n- Koka lexer: update to newest language spec (PR#201).\n\n- Haxe lexer: rewrite and support for Haxe 3 (PR#174).\n\n- Prolog lexer: add different kinds of numeric literals (#864).\n\n- F# lexer: rewrite with newest spec for F# 3.0 (#842), fix a bug with\n  dotted chains (#948).\n\n- Kotlin lexer: general update (PR#271).\n\n- Rebol lexer: fix comment detection and analyse_text (PR#261).\n\n- LLVM lexer: update keywords to v3.4 (PR#258).\n\n- PHP lexer: add new keywords and binary literals (PR#222).\n\n- external/markdown-processor.py updated to newest python-markdown (PR#221).\n\n- CSS lexer: some highlighting order fixes (PR#231).\n\n- Ceylon lexer: fix parsing of nested multiline comments (#915).\n\n- C family lexers: fix parsing of indented preprocessor directives (#944).\n\n- Rust lexer: update to 0.9 language version (PR#270, PR#388).\n\n- Elixir lexer: update to 0.15 language version (PR#392).\n\n- Fix swallowing incomplete tracebacks in Python console lexer (#874).\n\n\nVersion 1.6\n-----------\n(released Feb 3, 2013)\n\n- Lexers added:\n\n  * Dylan console (PR#149)\n  * Logos (PR#150)\n  * Shell sessions (PR#158)\n\n- Fix guessed lexers not receiving lexer options (#838).\n\n- Fix unquoted HTML attribute lexing in Opa (#841).\n\n- Fixes to the Dart lexer (PR#160).\n\n\nVersion 1.6rc1\n--------------\n(released Jan 9, 2013)\n\n- Lexers added:\n\n  * AspectJ (PR#90)\n  * AutoIt (PR#122)\n  * BUGS-like languages (PR#89)\n  * Ceylon (PR#86)\n  * Croc (new name for MiniD)\n  * CUDA (PR#75)\n  * Dg (PR#116)\n  * IDL (PR#115)\n  * Jags (PR#89)\n  * Julia (PR#61)\n  * Kconfig (#711)\n  * Lasso (PR#95, PR#113)\n  * LiveScript (PR#84)\n  * Monkey (PR#117)\n  * Mscgen (PR#80)\n  * NSIS scripts (PR#136)\n  * OpenCOBOL (PR#72)\n  * QML (PR#123)\n  * Puppet (PR#133)\n  * Racket (PR#94)\n  * Rdoc (PR#99)\n  * Robot Framework (PR#137)\n  * RPM spec files (PR#124)\n  * Rust (PR#67)\n  * Smali (Dalvik assembly)\n  * SourcePawn (PR#39)\n  * Stan (PR#89)\n  * Treetop (PR#125)\n  * TypeScript (PR#114)\n  * VGL (PR#12)\n  * Visual FoxPro (#762)\n  * Windows Registry (#819)\n  * Xtend (PR#68)\n\n- The HTML formatter now supports linking to tags using CTags files, when the\n  python-ctags package is installed (PR#87).\n\n- The HTML formatter now has a \"linespans\" option that wraps every line in a\n  <span> tag with a specific id (PR#82).\n\n- When deriving a lexer from another lexer with token definitions, definitions\n  for states not in the child lexer are now inherited.  If you override a state\n  in the child lexer, an \"inherit\" keyword has been added to insert the base\n  state at that position (PR#141).\n\n- The C family lexers now inherit token definitions from a common base class,\n  removing code duplication (PR#141).\n\n- Use \"colorama\" on Windows for console color output (PR#142).\n\n- Fix Template Haskell highlighting (PR#63).\n\n- Fix some S/R lexer errors (PR#91).\n\n- Fix a bug in the Prolog lexer with names that start with 'is' (#810).\n\n- Rewrite Dylan lexer, add Dylan LID lexer (PR#147).\n\n- Add a Java quickstart document (PR#146).\n\n- Add a \"external/autopygmentize\" file that can be used as .lessfilter (#802).\n\n\nVersion 1.5\n-----------\n(codename Zeitdilatation, released Mar 10, 2012)\n\n- Lexers added:\n\n  * Awk (#630)\n  * Fancy (#633)\n  * PyPy Log\n  * eC\n  * Nimrod\n  * Nemerle (#667)\n  * F# (#353)\n  * Groovy (#501)\n  * PostgreSQL (#660)\n  * DTD\n  * Gosu (#634)\n  * Octave (PR#22)\n  * Standard ML (PR#14)\n  * CFengine3 (#601)\n  * Opa (PR#37)\n  * HTTP sessions (PR#42)\n  * JSON (PR#31)\n  * SNOBOL (PR#30)\n  * MoonScript (PR#43)\n  * ECL (PR#29)\n  * Urbiscript (PR#17)\n  * OpenEdge ABL (PR#27)\n  * SystemVerilog (PR#35)\n  * Coq (#734)\n  * PowerShell (#654)\n  * Dart (#715)\n  * Fantom (PR#36)\n  * Bro (PR#5)\n  * NewLISP (PR#26)\n  * VHDL (PR#45)\n  * Scilab (#740)\n  * Elixir (PR#57)\n  * Tea (PR#56)\n  * Kotlin (PR#58)\n\n- Fix Python 3 terminal highlighting with pygmentize (#691).\n\n- In the LaTeX formatter, escape special &, < and > chars (#648).\n\n- In the LaTeX formatter, fix display problems for styles with token\n  background colors (#670).\n\n- Enhancements to the Squid conf lexer (#664).\n\n- Several fixes to the reStructuredText lexer (#636).\n\n- Recognize methods in the ObjC lexer (#638).\n\n- Fix Lua \"class\" highlighting: it does not have classes (#665).\n\n- Fix degenerate regex in Scala lexer (#671) and highlighting bugs (#713, 708).\n\n- Fix number pattern order in Ocaml lexer (#647).\n\n- Fix generic type highlighting in ActionScript 3 (#666).\n\n- Fixes to the Clojure lexer (PR#9).\n\n- Fix degenerate regex in Nemerle lexer (#706).\n\n- Fix infinite looping in CoffeeScript lexer (#729).\n\n- Fix crashes and analysis with ObjectiveC lexer (#693, #696).\n\n- Add some Fortran 2003 keywords.\n\n- Fix Boo string regexes (#679).\n\n- Add \"rrt\" style (#727).\n\n- Fix infinite looping in Darcs Patch lexer.\n\n- Lots of misc fixes to character-eating bugs and ordering problems in many\n  different lexers.\n\n\nVersion 1.4\n-----------\n(codename Unsch\u00e4rfe, released Jan 03, 2011)\n\n- Lexers added:\n\n  * Factor (#520)\n  * PostScript (#486)\n  * Verilog (#491)\n  * BlitzMax Basic (#478)\n  * Ioke (#465)\n  * Java properties, split out of the INI lexer (#445)\n  * Scss (#509)\n  * Duel/JBST\n  * XQuery (#617)\n  * Mason (#615)\n  * GoodData (#609)\n  * SSP (#473)\n  * Autohotkey (#417)\n  * Google Protocol Buffers\n  * Hybris (#506)\n\n- Do not fail in analyse_text methods (#618).\n\n- Performance improvements in the HTML formatter (#523).\n\n- With the ``noclasses`` option in the HTML formatter, some styles\n  present in the stylesheet were not added as inline styles.\n\n- Four fixes to the Lua lexer (#480, #481, #482, #497).\n\n- More context-sensitive Gherkin lexer with support for more i18n translations.\n\n- Support new OO keywords in Matlab lexer (#521).\n\n- Small fix in the CoffeeScript lexer (#519).\n\n- A bugfix for backslashes in ocaml strings (#499).\n\n- Fix unicode/raw docstrings in the Python lexer (#489).\n\n- Allow PIL to work without PIL.pth (#502).\n\n- Allow seconds as a unit in CSS (#496).\n\n- Support ``application/javascript`` as a JavaScript mime type (#504).\n\n- Support `Offload <https://offload.codeplay.com/>`_ C++ Extensions as\n  keywords in the C++ lexer (#484).\n\n- Escape more characters in LaTeX output (#505).\n\n- Update Haml/Sass lexers to version 3 (#509).\n\n- Small PHP lexer string escaping fix (#515).\n\n- Support comments before preprocessor directives, and unsigned/\n  long long literals in C/C++ (#613, #616).\n\n- Support line continuations in the INI lexer (#494).\n\n- Fix lexing of Dylan string and char literals (#628).\n\n- Fix class/procedure name highlighting in VB.NET lexer (#624).\n\n\nVersion 1.3.1\n-------------\n(bugfix release, released Mar 05, 2010)\n\n- The ``pygmentize`` script was missing from the distribution.\n\n\nVersion 1.3\n-----------\n(codename Schneegl\u00f6ckchen, released Mar 01, 2010)\n\n- Added the ``ensurenl`` lexer option, which can be used to suppress the\n  automatic addition of a newline to the lexer input.\n\n- Lexers added:\n\n  * Ada\n  * Coldfusion\n  * Modula-2\n  * Haxe\n  * R console\n  * Objective-J\n  * Haml and Sass\n  * CoffeeScript\n\n- Enhanced reStructuredText highlighting.\n\n- Added support for PHP 5.3 namespaces in the PHP lexer.\n\n- Added a bash completion script for `pygmentize`, to the external/\n  directory (#466).\n\n- Fixed a bug in `do_insertions()` used for multi-lexer languages.\n\n- Fixed a Ruby regex highlighting bug (#476).\n\n- Fixed regex highlighting bugs in Perl lexer (#258).\n\n- Add small enhancements to the C lexer (#467) and Bash lexer (#469).\n\n- Small fixes for the Tcl, Debian control file, Nginx config,\n  Smalltalk, Objective-C, Clojure, Lua lexers.\n\n- Gherkin lexer: Fixed single apostrophe bug and added new i18n keywords.\n\n\nVersion 1.2.2\n-------------\n(bugfix release, released Jan 02, 2010)\n\n* Removed a backwards incompatibility in the LaTeX formatter that caused\n  Sphinx to produce invalid commands when writing LaTeX output (#463).\n\n* Fixed a forever-backtracking regex in the BashLexer (#462).\n\n\nVersion 1.2.1\n-------------\n(bugfix release, released Jan 02, 2010)\n\n* Fixed mishandling of an ellipsis in place of the frames in a Python\n  console traceback, resulting in clobbered output.\n\n\nVersion 1.2\n-----------\n(codename Neujahr, released Jan 01, 2010)\n\n- Dropped Python 2.3 compatibility.\n\n- Lexers added:\n\n  * Asymptote\n  * Go\n  * Gherkin (Cucumber)\n  * CMake\n  * Ooc\n  * Coldfusion\n  * Haxe\n  * R console\n\n- Added options for rendering LaTeX in source code comments in the\n  LaTeX formatter (#461).\n\n- Updated the Logtalk lexer.\n\n- Added `line_number_start` option to image formatter (#456).\n\n- Added `hl_lines` and `hl_color` options to image formatter (#457).\n\n- Fixed the HtmlFormatter's handling of noclasses=True to not output any\n  classes (#427).\n\n- Added the Monokai style (#453).\n\n- Fixed LLVM lexer identifier syntax and added new keywords (#442).\n\n- Fixed the PythonTracebackLexer to handle non-traceback data in header or\n  trailer, and support more partial tracebacks that start on line 2 (#437).\n\n- Fixed the CLexer to not highlight ternary statements as labels.\n\n- Fixed lexing of some Ruby quoting peculiarities (#460).\n\n- A few ASM lexer fixes (#450).\n\n\nVersion 1.1.1\n-------------\n(bugfix release, released Sep 15, 2009)\n\n- Fixed the BBCode lexer (#435).\n\n- Added support for new Jinja2 keywords.\n\n- Fixed test suite failures.\n\n- Added Gentoo-specific suffixes to Bash lexer.\n\n\nVersion 1.1\n-----------\n(codename Brillouin, released Sep 11, 2009)\n\n- Ported Pygments to Python 3.  This needed a few changes in the way\n  encodings are handled; they may affect corner cases when used with\n  Python 2 as well.\n\n- Lexers added:\n\n  * Antlr/Ragel, thanks to Ana Nelson\n  * (Ba)sh shell\n  * Erlang shell\n  * GLSL\n  * Prolog\n  * Evoque\n  * Modelica\n  * Rebol\n  * MXML\n  * Cython\n  * ABAP\n  * ASP.net (VB/C#)\n  * Vala\n  * Newspeak\n\n- Fixed the LaTeX formatter's output so that output generated for one style\n  can be used with the style definitions of another (#384).\n\n- Added \"anchorlinenos\" and \"noclobber_cssfile\" (#396) options to HTML\n  formatter.\n\n- Support multiline strings in Lua lexer.\n\n- Rewrite of the JavaScript lexer by Pumbaa80 to better support regular\n  expression literals (#403).\n\n- When pygmentize is asked to highlight a file for which multiple lexers\n  match the filename, use the analyse_text guessing engine to determine the\n  winner (#355).\n\n- Fixed minor bugs in the JavaScript lexer (#383), the Matlab lexer (#378),\n  the Scala lexer (#392), the INI lexer (#391), the Clojure lexer (#387)\n  and the AS3 lexer (#389).\n\n- Fixed three Perl heredoc lexing bugs (#379, #400, #422).\n\n- Fixed a bug in the image formatter which misdetected lines (#380).\n\n- Fixed bugs lexing extended Ruby strings and regexes.\n\n- Fixed a bug when lexing git diffs.\n\n- Fixed a bug lexing the empty commit in the PHP lexer (#405).\n\n- Fixed a bug causing Python numbers to be mishighlighted as floats (#397).\n\n- Fixed a bug when backslashes are used in odd locations in Python (#395).\n\n- Fixed various bugs in Matlab and S-Plus lexers, thanks to Winston Chang (#410,\n  #411, #413, #414) and fmarc (#419).\n\n- Fixed a bug in Haskell single-line comment detection (#426).\n\n- Added new-style reStructuredText directive for docutils 0.5+ (#428).\n\n\nVersion 1.0\n-----------\n(codename Dreiundzwanzig, released Nov 23, 2008)\n\n- Don't use join(splitlines()) when converting newlines to ``\\n``,\n  because that doesn't keep all newlines at the end when the\n  ``stripnl`` lexer option is False.\n\n- Added ``-N`` option to command-line interface to get a lexer name\n  for a given filename.\n\n- Added Tango style, written by Andre Roberge for the Crunchy project.\n\n- Added Python3TracebackLexer and ``python3`` option to\n  PythonConsoleLexer.\n\n- Fixed a few bugs in the Haskell lexer.\n\n- Fixed PythonTracebackLexer to be able to recognize SyntaxError and\n  KeyboardInterrupt (#360).\n\n- Provide one formatter class per image format, so that surprises like::\n\n    pygmentize -f gif -o foo.gif foo.py\n\n  creating a PNG file are avoided.\n\n- Actually use the `font_size` option of the image formatter.\n\n- Fixed numpy lexer that it doesn't listen for `*.py` any longer.\n\n- Fixed HTML formatter so that text options can be Unicode\n  strings (#371).\n\n- Unified Diff lexer supports the \"udiff\" alias now.\n\n- Fixed a few issues in Scala lexer (#367).\n\n- RubyConsoleLexer now supports simple prompt mode (#363).\n\n- JavascriptLexer is smarter about what constitutes a regex (#356).\n\n- Add Applescript lexer, thanks to Andreas Amann (#330).\n\n- Make the codetags more strict about matching words (#368).\n\n- NginxConfLexer is a little more accurate on mimetypes and\n  variables (#370).\n\n\nVersion 0.11.1\n--------------\n(released Aug 24, 2008)\n\n- Fixed a Jython compatibility issue in pygments.unistring (#358).\n\n\nVersion 0.11\n------------\n(codename Strau\u00dfenei, released Aug 23, 2008)\n\nMany thanks go to Tim Hatch for writing or integrating most of the bug\nfixes and new features.\n\n- Lexers added:\n\n  * Nasm-style assembly language, thanks to delroth\n  * YAML, thanks to Kirill Simonov\n  * ActionScript 3, thanks to Pierre Bourdon\n  * Cheetah/Spitfire templates, thanks to Matt Good\n  * Lighttpd config files\n  * Nginx config files\n  * Gnuplot plotting scripts\n  * Clojure\n  * POV-Ray scene files\n  * Sqlite3 interactive console sessions\n  * Scala source files, thanks to Krzysiek Goj\n\n- Lexers improved:\n\n  * C lexer highlights standard library functions now and supports C99\n    types.\n  * Bash lexer now correctly highlights heredocs without preceding\n    whitespace.\n  * Vim lexer now highlights hex colors properly and knows a couple\n    more keywords.\n  * Irc logs lexer now handles xchat's default time format (#340) and\n    correctly highlights lines ending in ``>``.\n  * Support more delimiters for perl regular expressions (#258).\n  * ObjectiveC lexer now supports 2.0 features.\n\n- Added \"Visual Studio\" style.\n\n- Updated markdown processor to Markdown 1.7.\n\n- Support roman/sans/mono style defs and use them in the LaTeX\n  formatter.\n\n- The RawTokenFormatter is no longer registered to ``*.raw`` and it's\n  documented that tokenization with this lexer may raise exceptions.\n\n- New option ``hl_lines`` to HTML formatter, to highlight certain\n  lines.\n\n- New option ``prestyles`` to HTML formatter.\n\n- New option *-g* to pygmentize, to allow lexer guessing based on\n  filetext (can be slowish, so file extensions are still checked\n  first).\n\n- ``guess_lexer()`` now makes its decision much faster due to a cache\n  of whether data is xml-like (a check which is used in several\n  versions of ``analyse_text()``.  Several lexers also have more\n  accurate ``analyse_text()`` now.\n\n\nVersion 0.10\n------------\n(codename Malzeug, released May 06, 2008)\n\n- Lexers added:\n\n  * Io\n  * Smalltalk\n  * Darcs patches\n  * Tcl\n  * Matlab\n  * Matlab sessions\n  * FORTRAN\n  * XSLT\n  * tcsh\n  * NumPy\n  * Python 3\n  * S, S-plus, R statistics languages\n  * Logtalk\n\n- In the LatexFormatter, the *commandprefix* option is now by default\n  'PY' instead of 'C', since the latter resulted in several collisions\n  with other packages.  Also, the special meaning of the *arg*\n  argument to ``get_style_defs()`` was removed.\n\n- Added ImageFormatter, to format code as PNG, JPG, GIF or BMP.\n  (Needs the Python Imaging Library.)\n\n- Support doc comments in the PHP lexer.\n\n- Handle format specifications in the Perl lexer.\n\n- Fix comment handling in the Batch lexer.\n\n- Add more file name extensions for the C++, INI and XML lexers.\n\n- Fixes in the IRC and MuPad lexers.\n\n- Fix function and interface name highlighting in the Java lexer.\n\n- Fix at-rule handling in the CSS lexer.\n\n- Handle KeyboardInterrupts gracefully in pygmentize.\n\n- Added BlackWhiteStyle.\n\n- Bash lexer now correctly highlights math, does not require\n  whitespace after semicolons, and correctly highlights boolean\n  operators.\n\n- Makefile lexer is now capable of handling BSD and GNU make syntax.\n\n\nVersion 0.9\n-----------\n(codename Herbstzeitlose, released Oct 14, 2007)\n\n- Lexers added:\n\n  * Erlang\n  * ActionScript\n  * Literate Haskell\n  * Common Lisp\n  * Various assembly languages\n  * Gettext catalogs\n  * Squid configuration\n  * Debian control files\n  * MySQL-style SQL\n  * MOOCode\n\n- Lexers improved:\n\n  * Greatly improved the Haskell and OCaml lexers.\n  * Improved the Bash lexer's handling of nested constructs.\n  * The C# and Java lexers exhibited abysmal performance with some\n    input code; this should now be fixed.\n  * The IRC logs lexer is now able to colorize weechat logs too.\n  * The Lua lexer now recognizes multi-line comments.\n  * Fixed bugs in the D and MiniD lexer.\n\n- The encoding handling of the command line mode (pygmentize) was\n  enhanced. You shouldn't get UnicodeErrors from it anymore if you\n  don't give an encoding option.\n\n- Added a ``-P`` option to the command line mode which can be used to\n  give options whose values contain commas or equals signs.\n\n- Added 256-color terminal formatter.\n\n- Added an experimental SVG formatter.\n\n- Added the ``lineanchors`` option to the HTML formatter, thanks to\n  Ian Charnas for the idea.\n\n- Gave the line numbers table a CSS class in the HTML formatter.\n\n- Added a Vim 7-like style.\n\n\nVersion 0.8.1\n-------------\n(released Jun 27, 2007)\n\n- Fixed POD highlighting in the Ruby lexer.\n\n- Fixed Unicode class and namespace name highlighting in the C# lexer.\n\n- Fixed Unicode string prefix highlighting in the Python lexer.\n\n- Fixed a bug in the D and MiniD lexers.\n\n- Fixed the included MoinMoin parser.\n\n\nVersion 0.8\n-----------\n(codename Maik\u00e4fer, released May 30, 2007)\n\n- Lexers added:\n\n  * Haskell, thanks to Adam Blinkinsop\n  * Redcode, thanks to Adam Blinkinsop\n  * D, thanks to Kirk McDonald\n  * MuPad, thanks to Christopher Creutzig\n  * MiniD, thanks to Jarrett Billingsley\n  * Vim Script, by Tim Hatch\n\n- The HTML formatter now has a second line-numbers mode in which it\n  will just integrate the numbers in the same ``<pre>`` tag as the\n  code.\n\n- The `CSharpLexer` now is Unicode-aware, which means that it has an\n  option that can be set so that it correctly lexes Unicode\n  identifiers allowed by the C# specs.\n\n- Added a `RaiseOnErrorTokenFilter` that raises an exception when the\n  lexer generates an error token, and a `VisibleWhitespaceFilter` that\n  converts whitespace (spaces, tabs, newlines) into visible\n  characters.\n\n- Fixed the `do_insertions()` helper function to yield correct\n  indices.\n\n- The ReST lexer now automatically highlights source code blocks in\n  \".. sourcecode:: language\" and \".. code:: language\" directive\n  blocks.\n\n- Improved the default style (thanks to Tiberius Teng). The old\n  default is still available as the \"emacs\" style (which was an alias\n  before).\n\n- The `get_style_defs` method of HTML formatters now uses the\n  `cssclass` option as the default selector if it was given.\n\n- Improved the ReST and Bash lexers a bit.\n\n- Fixed a few bugs in the Makefile and Bash lexers, thanks to Tim\n  Hatch.\n\n- Fixed a bug in the command line code that disallowed ``-O`` options\n  when using the ``-S`` option.\n\n- Fixed a bug in the `RawTokenFormatter`.\n\n\nVersion 0.7.1\n-------------\n(released Feb 15, 2007)\n\n- Fixed little highlighting bugs in the Python, Java, Scheme and\n  Apache Config lexers.\n\n- Updated the included manpage.\n\n- Included a built version of the documentation in the source tarball.\n\n\nVersion 0.7\n-----------\n(codename Faschingskrapfn, released Feb 14, 2007)\n\n- Added a MoinMoin parser that uses Pygments. With it, you get\n  Pygments highlighting in Moin Wiki pages.\n\n- Changed the exception raised if no suitable lexer, formatter etc. is\n  found in one of the `get_*_by_*` functions to a custom exception,\n  `pygments.util.ClassNotFound`. It is, however, a subclass of\n  `ValueError` in order to retain backwards compatibility.\n\n- Added a `-H` command line option which can be used to get the\n  docstring of a lexer, formatter or filter.\n\n- Made the handling of lexers and formatters more consistent. The\n  aliases and filename patterns of formatters are now attributes on\n  them.\n\n- Added an OCaml lexer, thanks to Adam Blinkinsop.\n\n- Made the HTML formatter more flexible, and easily subclassable in\n  order to make it easy to implement custom wrappers, e.g. alternate\n  line number markup. See the documentation.\n\n- Added an `outencoding` option to all formatters, making it possible\n  to override the `encoding` (which is used by lexers and formatters)\n  when using the command line interface. Also, if using the terminal\n  formatter and the output file is a terminal and has an encoding\n  attribute, use it if no encoding is given.\n\n- Made it possible to just drop style modules into the `styles`\n  subpackage of the Pygments installation.\n\n- Added a \"state\" keyword argument to the `using` helper.\n\n- Added a `commandprefix` option to the `LatexFormatter` which allows\n  to control how the command names are constructed.\n\n- Added quite a few new lexers, thanks to Tim Hatch:\n\n  * Java Server Pages\n  * Windows batch files\n  * Trac Wiki markup\n  * Python tracebacks\n  * ReStructuredText\n  * Dylan\n  * and the Befunge esoteric programming language (yay!)\n\n- Added Mako lexers by Ben Bangert.\n\n- Added \"fruity\" style, another dark background originally vim-based\n  theme.\n\n- Added sources.list lexer by Dennis Kaarsemaker.\n\n- Added token stream filters, and a pygmentize option to use them.\n\n- Changed behavior of `in` Operator for tokens.\n\n- Added mimetypes for all lexers.\n\n- Fixed some problems lexing Python strings.\n\n- Fixed tickets: #167, #178, #179, #180, #185, #201.\n\n\nVersion 0.6\n-----------\n(codename Zimtstern, released Dec 20, 2006)\n\n- Added option for the HTML formatter to write the CSS to an external\n  file in \"full document\" mode.\n\n- Added RTF formatter.\n\n- Added Bash and Apache configuration lexers (thanks to Tim Hatch).\n\n- Improved guessing methods for various lexers.\n\n- Added `@media` support to CSS lexer (thanks to Tim Hatch).\n\n- Added a Groff lexer (thanks to Tim Hatch).\n\n- License change to BSD.\n\n- Added lexers for the Myghty template language.\n\n- Added a Scheme lexer (thanks to Marek Kubica).\n\n- Added some functions to iterate over existing lexers, formatters and\n  lexers.\n\n- The HtmlFormatter's `get_style_defs()` can now take a list as an\n  argument to generate CSS with multiple prefixes.\n\n- Support for guessing input encoding added.\n\n- Encoding support added: all processing is now done with Unicode\n  strings, input and output are converted from and optionally to byte\n  strings (see the ``encoding`` option of lexers and formatters).\n\n- Some improvements in the C(++) lexers handling comments and line\n  continuations.\n\n\nVersion 0.5.1\n-------------\n(released Oct 30, 2006)\n\n- Fixed traceback in ``pygmentize -L`` (thanks to Piotr Ozarowski).\n\n\nVersion 0.5\n-----------\n(codename PyKleur, released Oct 30, 2006)\n\n- Initial public release.\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.archetype\n    ~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexer for Archetype-related syntaxes, including:\n\n    - ODIN syntax <https://github.com/openEHR/odin>\n    - ADL syntax <http://www.openehr.org/releases/trunk/architecture/am/adl2.pdf>\n    - cADL sub-syntax of ADL\n\n    For uses of this syntax, see the openEHR archetypes <http://www.openEHR.org/ckm>\n\n    Contributed by Thomas Beale <https://github.com/wolandscat>,\n    <https://bitbucket.org/thomas_beale>.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pygments.lexer import RegexLexer, include, bygroups, using, default\nfrom pygments.token import Text, Comment, Name, Literal, Number, String, \\\n    Punctuation, Keyword, Operator, Generic\n\n__all__ = ['OdinLexer', 'CadlLexer', 'AdlLexer']\n\n\nclass AtomsLexer(RegexLexer):\n    \"\"\"\n    Lexer for Values used in ADL and ODIN.\n\n    .. versionadded:: 2.1\n    \"\"\"\n\n    tokens = {\n        # ----- pseudo-states for inclusion -----\n        'whitespace': [\n            (r'\\n', Text),\n            (r'\\s+', Text),\n            (r'[ \\t]*--.*$', Comment),\n        ],\n        'archetype_id': [\n            (r'[ \\t]*([a-zA-Z]\\w+(\\.[a-zA-Z]\\w+)*::)?[a-zA-Z]\\w+(-[a-zA-Z]\\w+){2}'\n             r'\\.\\w+[\\w-]*\\.v\\d+(\\.\\d+){,2}((-[a-z]+)(\\.\\d+)?)?', Name.Decorator),\n        ],\n        'date_constraints': [\n            # ISO 8601-based date/time constraints\n            (r'[Xx?YyMmDdHhSs\\d]{2,4}([:-][Xx?YyMmDdHhSs\\d]{2}){2}', Literal.Date),\n            # ISO 8601-based duration constraints + optional trailing slash\n            (r'(P[YyMmWwDd]+(T[HhMmSs]+)?|PT[HhMmSs]+)/?', Literal.Date),\n        ],\n        'ordered_values': [\n            # ISO 8601 date with optional 'T' ligature\n            (r'\\d{4}-\\d{2}-\\d{2}T?', Literal.Date),\n            # ISO 8601 time\n            (r'\\d{2}:\\d{2}:\\d{2}(\\.\\d+)?([+-]\\d{4}|Z)?', Literal.Date),\n            # ISO 8601 duration\n            (r'P((\\d*(\\.\\d+)?[YyMmWwDd]){1,3}(T(\\d*(\\.\\d+)?[HhMmSs]){,3})?|'\n             r'T(\\d*(\\.\\d+)?[HhMmSs]){,3})', Literal.Date),\n            (r'[+-]?(\\d+\\.\\d*|\\.\\d+|\\d+)[eE][+-]?\\d+', Number.Float),\n            (r'[+-]?\\d*\\.\\d+%?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[+-]?\\d+%?', Number.Integer),\n        ],\n        'values': [\n            include('ordered_values'),\n            (r'([Tt]rue|[Ff]alse)', Literal),\n            (r'\"', String, 'string'),\n            (r\"'(\\\\.|\\\\[0-7]{1,3}|\\\\x[a-fA-F0-9]{1,2}|[^\\\\\\'\\n])'\", String.Char),\n            (r'[a-z][a-z0-9+.-]*:', Literal, 'uri'),\n            # term code\n            (r'(\\[)(\\w[\\w-]*(?:\\([^)\\n]+\\))?)(::)(\\w[\\w-]*)(\\])',\n             bygroups(Punctuation, Name.Decorator, Punctuation, Name.Decorator,\n                      Punctuation)),\n            (r'\\|', Punctuation, 'interval'),\n            # list continuation\n            (r'\\.\\.\\.', Punctuation),\n        ],\n        'constraint_values': [\n            (r'(\\[)(\\w[\\w-]*(?:\\([^)\\n]+\\))?)(::)',\n             bygroups(Punctuation, Name.Decorator, Punctuation), 'adl14_code_constraint'),\n            # ADL 1.4 ordinal constraint\n            (r'(\\d*)(\\|)(\\[\\w[\\w-]*::\\w[\\w-]*\\])((?:[,;])?)',\n             bygroups(Number, Punctuation, Name.Decorator, Punctuation)),\n            include('date_constraints'),\n            include('values'),\n        ],\n\n        # ----- real states -----\n        'string': [\n            ('\"', String, '#pop'),\n            (r'\\\\([\\\\abfnrtv\"\\']|x[a-fA-F0-9]{2,4}|'\n             r'u[a-fA-F0-9]{4}|U[a-fA-F0-9]{8}|[0-7]{1,3})', String.Escape),\n            # all other characters\n            (r'[^\\\\\"]+', String),\n            # stray backslash\n            (r'\\\\', String),\n        ],\n        'uri': [\n            # effective URI terminators\n            (r'[,>\\s]', Punctuation, '#pop'),\n            (r'[^>\\s,]+', Literal),\n        ],\n        'interval': [\n            (r'\\|', Punctuation, '#pop'),\n            include('ordered_values'),\n            (r'\\.\\.', Punctuation),\n            (r'[<>=] *', Punctuation),\n            # handle +/-\n            (r'\\+/-', Punctuation),\n            (r'\\s+', Text),\n        ],\n        'any_code': [\n            include('archetype_id'),\n            # if it is a code\n            (r'[a-z_]\\w*[0-9.]+(@[^\\]]+)?', Name.Decorator),\n            # if it is tuple with attribute names\n            (r'[a-z_]\\w*', Name.Class),\n            # if it is an integer, i.e. Xpath child index\n            (r'[0-9]+', Text),\n            (r'\\|', Punctuation, 'code_rubric'),\n            (r'\\]', Punctuation, '#pop'),\n            # handle use_archetype statement\n            (r'\\s*,\\s*', Punctuation),\n        ],\n        'code_rubric': [\n            (r'\\|', Punctuation, '#pop'),\n            (r'[^|]+', String),\n        ],\n        'adl14_code_constraint': [\n            (r'\\]', Punctuation, '#pop'),\n            (r'\\|', Punctuation, 'code_rubric'),\n            (r'(\\w[\\w-]*)([;,]?)', bygroups(Name.Decorator, Punctuation)),\n            include('whitespace'),\n        ],\n    }\n\n\nclass OdinLexer(AtomsLexer):\n    \"\"\"\n    Lexer for ODIN syntax.\n\n    .. versionadded:: 2.1\n    \"\"\"\n    name = 'ODIN'\n    aliases = ['odin']\n    filenames = ['*.odin']\n    mimetypes = ['text/odin']\n\n    tokens = {\n        'path': [\n            (r'>', Punctuation, '#pop'),\n            # attribute name\n            (r'[a-z_]\\w*', Name.Class),\n            (r'/', Punctuation),\n            (r'\\[', Punctuation, 'key'),\n            (r'\\s*,\\s*', Punctuation, '#pop'),\n            (r'\\s+', Text, '#pop'),\n        ],\n        'key': [\n            include('values'),\n            (r'\\]', Punctuation, '#pop'),\n        ],\n        'type_cast': [\n            (r'\\)', Punctuation, '#pop'),\n            (r'[^)]+',  Name.Class),\n        ],\n        'root': [\n            include('whitespace'),\n            (r'([Tt]rue|[Ff]alse)', Literal),\n            include('values'),\n            # x-ref path\n            (r'/', Punctuation, 'path'),\n            # x-ref path starting with key\n            (r'\\[', Punctuation, 'key'),\n            # attribute name\n            (r'[a-z_]\\w*', Name.Class),\n            (r'=', Operator),\n            (r'\\(', Punctuation, 'type_cast'),\n            (r',', Punctuation),\n            (r'<', Punctuation),\n            (r'>', Punctuation),\n            (r';', Punctuation),\n        ],\n    }\n\n\nclass CadlLexer(AtomsLexer):\n    \"\"\"\n    Lexer for cADL syntax.\n\n    .. versionadded:: 2.1\n    \"\"\"\n    name = 'cADL'\n    aliases = ['cadl']\n    filenames = ['*.cadl']\n\n    tokens = {\n        'path': [\n            # attribute name\n            (r'[a-z_]\\w*', Name.Class),\n            (r'/', Punctuation),\n            (r'\\[', Punctuation, 'any_code'),\n            (r'\\s+', Punctuation, '#pop'),\n        ],\n        'root': [\n            include('whitespace'),\n            (r'(cardinality|existence|occurrences|group|include|exclude|'\n             r'allow_archetype|use_archetype|use_node)\\W', Keyword.Type),\n            (r'(and|or|not|there_exists|xor|implies|for_all)\\W', Keyword.Type),\n            (r'(after|before|closed)\\W', Keyword.Type),\n            (r'(not)\\W', Operator),\n            (r'(matches|is_in)\\W', Operator),\n            # is_in / not is_in char\n            ('(\\u2208|\\u2209)', Operator),\n            # there_exists / not there_exists / for_all / and / or\n            ('(\\u2203|\\u2204|\\u2200|\\u2227|\\u2228|\\u22BB|\\223C)',\n             Operator),\n            # regex in slot or as string constraint\n            (r'(\\{)(\\s*/[^}]+/\\s*)(\\})',\n             bygroups(Punctuation, String.Regex, Punctuation)),\n            # regex in slot or as string constraint\n            (r'(\\{)(\\s*\\^[^}]+\\^\\s*)(\\})',\n             bygroups(Punctuation, String.Regex, Punctuation)),\n            (r'/', Punctuation, 'path'),\n            # for cardinality etc\n            (r'(\\{)((?:\\d+\\.\\.)?(?:\\d+|\\*))'\n             r'((?:\\s*;\\s*(?:ordered|unordered|unique)){,2})(\\})',\n             bygroups(Punctuation, Number, Number, Punctuation)),\n            # [{ is start of a tuple value\n            (r'\\[\\{', Punctuation),\n            (r'\\}\\]', Punctuation),\n            (r'\\{', Punctuation),\n            (r'\\}', Punctuation),\n            include('constraint_values'),\n            # type name\n            (r'[A-Z]\\w+(<[A-Z]\\w+([A-Za-z_<>]*)>)?',  Name.Class),\n            # attribute name\n            (r'[a-z_]\\w*', Name.Class),\n            (r'\\[', Punctuation, 'any_code'),\n            (r'(~|//|\\\\\\\\|\\+|-|/|\\*|\\^|!=|=|<=|>=|<|>]?)', Operator),\n            (r'\\(', Punctuation),\n            (r'\\)', Punctuation),\n            # for lists of values\n            (r',', Punctuation),\n            (r'\"', String, 'string'),\n            # for assumed value\n            (r';', Punctuation),\n        ],\n    }\n\n\nclass AdlLexer(AtomsLexer):\n    \"\"\"\n    Lexer for ADL syntax.\n\n    .. versionadded:: 2.1\n    \"\"\"\n\n    name = 'ADL'\n    aliases = ['adl']\n    filenames = ['*.adl', '*.adls', '*.adlf', '*.adlx']\n\n    tokens = {\n        'whitespace': [\n            # blank line ends\n            (r'\\s*\\n', Text),\n            # comment-only line\n            (r'^[ \\t]*--.*$', Comment),\n        ],\n        'odin_section': [\n            # repeating the following two rules from the root state enable multi-line\n            # strings that start in the first column to be dealt with\n            (r'^(language|description|ontology|terminology|annotations|'\n             r'component_terminologies|revision_history)[ \\t]*\\n', Generic.Heading),\n            (r'^(definition)[ \\t]*\\n', Generic.Heading, 'cadl_section'),\n            (r'^([ \\t]*|[ \\t]+.*)\\n', using(OdinLexer)),\n            (r'^([^\"]*\")(>[ \\t]*\\n)', bygroups(String, Punctuation)),\n            # template overlay delimiter\n            (r'^----------*\\n', Text, '#pop'),\n            (r'^.*\\n', String),\n            default('#pop'),\n        ],\n        'cadl_section': [\n            (r'^([ \\t]*|[ \\t]+.*)\\n', using(CadlLexer)),\n            default('#pop'),\n        ],\n        'rules_section': [\n            (r'^[ \\t]+.*\\n', using(CadlLexer)),\n            default('#pop'),\n        ],\n        'metadata': [\n            (r'\\)', Punctuation, '#pop'),\n            (r';', Punctuation),\n            (r'([Tt]rue|[Ff]alse)', Literal),\n            # numbers and version ids\n            (r'\\d+(\\.\\d+)*', Literal),\n            # Guids\n            (r'(\\d|[a-fA-F])+(-(\\d|[a-fA-F])+){3,}', Literal),\n            (r'\\w+', Name.Class),\n            (r'\"', String, 'string'),\n            (r'=', Operator),\n            (r'[ \\t]+', Text),\n            default('#pop'),\n        ],\n        'root': [\n            (r'^(archetype|template_overlay|operational_template|template|'\n             r'speciali[sz]e)', Generic.Heading),\n            (r'^(language|description|ontology|terminology|annotations|'\n             r'component_terminologies|revision_history)[ \\t]*\\n',\n             Generic.Heading, 'odin_section'),\n            (r'^(definition)[ \\t]*\\n', Generic.Heading, 'cadl_section'),\n            (r'^(rules)[ \\t]*\\n', Generic.Heading, 'rules_section'),\n            include('archetype_id'),\n            (r'[ \\t]*\\(', Punctuation, 'metadata'),\n            include('whitespace'),\n        ],\n    }\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.factor\n    ~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for the Factor language.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexer import RegexLexer, bygroups, default, words\nfrom pygments.token import Text, Comment, Keyword, Name, String, Number\n\n__all__ = ['FactorLexer']\n\n\nclass FactorLexer(RegexLexer):\n    \"\"\"\n    Lexer for the `Factor <http://factorcode.org>`_ language.\n\n    .. versionadded:: 1.4\n    \"\"\"\n    name = 'Factor'\n    aliases = ['factor']\n    filenames = ['*.factor']\n    mimetypes = ['text/x-factor']\n\n    flags = re.MULTILINE | re.UNICODE\n\n    builtin_kernel = words((\n        '-rot', '2bi', '2bi@', '2bi*', '2curry', '2dip', '2drop', '2dup', '2keep', '2nip',\n        '2over', '2tri', '2tri@', '2tri*', '3bi', '3curry', '3dip', '3drop', '3dup', '3keep',\n        '3tri', '4dip', '4drop', '4dup', '4keep', '<wrapper>', '=', '>boolean', 'clone',\n        '?', '?execute', '?if', 'and', 'assert', 'assert=', 'assert?', 'bi', 'bi-curry',\n        'bi-curry@', 'bi-curry*', 'bi@', 'bi*', 'boa', 'boolean', 'boolean?', 'both?',\n        'build', 'call', 'callstack', 'callstack>array', 'callstack?', 'clear', '(clone)',\n        'compose', 'compose?', 'curry', 'curry?', 'datastack', 'die', 'dip', 'do', 'drop',\n        'dup', 'dupd', 'either?', 'eq?', 'equal?', 'execute', 'hashcode', 'hashcode*',\n        'identity-hashcode', 'identity-tuple', 'identity-tuple?', 'if', 'if*',\n        'keep', 'loop', 'most', 'new', 'nip', 'not', 'null', 'object', 'or', 'over',\n        'pick', 'prepose', 'retainstack', 'rot', 'same?', 'swap', 'swapd', 'throw',\n        'tri', 'tri-curry', 'tri-curry@', 'tri-curry*', 'tri@', 'tri*', 'tuple',\n        'tuple?', 'unless', 'unless*', 'until', 'when', 'when*', 'while', 'with',\n        'wrapper', 'wrapper?', 'xor'), suffix=r'\\s')\n\n    builtin_assocs = words((\n        '2cache', '<enum>', '>alist', '?at', '?of', 'assoc', 'assoc-all?',\n        'assoc-any?', 'assoc-clone-like', 'assoc-combine', 'assoc-diff',\n        'assoc-diff!', 'assoc-differ', 'assoc-each', 'assoc-empty?',\n        'assoc-filter', 'assoc-filter!', 'assoc-filter-as', 'assoc-find',\n        'assoc-hashcode', 'assoc-intersect', 'assoc-like', 'assoc-map',\n        'assoc-map-as', 'assoc-partition', 'assoc-refine', 'assoc-size',\n        'assoc-stack', 'assoc-subset?', 'assoc-union', 'assoc-union!',\n        'assoc=', 'assoc>map', 'assoc?', 'at', 'at+', 'at*', 'cache', 'change-at',\n        'clear-assoc', 'delete-at', 'delete-at*', 'enum', 'enum?', 'extract-keys',\n        'inc-at', 'key?', 'keys', 'map>assoc', 'maybe-set-at', 'new-assoc', 'of',\n        'push-at', 'rename-at', 'set-at', 'sift-keys', 'sift-values', 'substitute',\n        'unzip', 'value-at', 'value-at*', 'value?', 'values', 'zip'), suffix=r'\\s')\n\n    builtin_combinators = words((\n        '2cleave', '2cleave>quot', '3cleave', '3cleave>quot', '4cleave',\n        '4cleave>quot', 'alist>quot', 'call-effect', 'case', 'case-find',\n        'case>quot', 'cleave', 'cleave>quot', 'cond', 'cond>quot', 'deep-spread>quot',\n        'execute-effect', 'linear-case-quot', 'no-case', 'no-case?', 'no-cond',\n        'no-cond?', 'recursive-hashcode', 'shallow-spread>quot', 'spread',\n        'to-fixed-point', 'wrong-values', 'wrong-values?'), suffix=r'\\s')\n\n    builtin_math = words((\n        '-', '/', '/f', '/i', '/mod', '2/', '2^', '<', '<=', '<fp-nan>', '>',\n        '>=', '>bignum', '>fixnum', '>float', '>integer', '(all-integers?)',\n        '(each-integer)', '(find-integer)', '*', '+', '?1+',\n        'abs', 'align', 'all-integers?', 'bignum', 'bignum?', 'bit?', 'bitand',\n        'bitnot', 'bitor', 'bits>double', 'bits>float', 'bitxor', 'complex',\n        'complex?', 'denominator', 'double>bits', 'each-integer', 'even?',\n        'find-integer', 'find-last-integer', 'fixnum', 'fixnum?', 'float',\n        'float>bits', 'float?', 'fp-bitwise=', 'fp-infinity?', 'fp-nan-payload',\n        'fp-nan?', 'fp-qnan?', 'fp-sign', 'fp-snan?', 'fp-special?',\n        'if-zero', 'imaginary-part', 'integer', 'integer>fixnum',\n        'integer>fixnum-strict', 'integer?', 'log2', 'log2-expects-positive',\n        'log2-expects-positive?', 'mod', 'neg', 'neg?', 'next-float',\n        'next-power-of-2', 'number', 'number=', 'number?', 'numerator', 'odd?',\n        'out-of-fixnum-range', 'out-of-fixnum-range?', 'power-of-2?',\n        'prev-float', 'ratio', 'ratio?', 'rational', 'rational?', 'real',\n        'real-part', 'real?', 'recip', 'rem', 'sgn', 'shift', 'sq', 'times',\n        'u<', 'u<=', 'u>', 'u>=', 'unless-zero', 'unordered?', 'when-zero',\n        'zero?'), suffix=r'\\s')\n\n    builtin_sequences = words((\n        '1sequence', '2all?', '2each', '2map', '2map-as', '2map-reduce', '2reduce',\n        '2selector', '2sequence', '3append', '3append-as', '3each', '3map', '3map-as',\n        '3sequence', '4sequence', '<repetition>', '<reversed>', '<slice>', '?first',\n        '?last', '?nth', '?second', '?set-nth', 'accumulate', 'accumulate!',\n        'accumulate-as', 'all?', 'any?', 'append', 'append!', 'append-as',\n        'assert-sequence', 'assert-sequence=', 'assert-sequence?',\n        'binary-reduce', 'bounds-check', 'bounds-check?', 'bounds-error',\n        'bounds-error?', 'but-last', 'but-last-slice', 'cartesian-each',\n        'cartesian-map', 'cartesian-product', 'change-nth', 'check-slice',\n        'check-slice-error', 'clone-like', 'collapse-slice', 'collector',\n        'collector-for', 'concat', 'concat-as', 'copy', 'count', 'cut', 'cut-slice',\n        'cut*', 'delete-all', 'delete-slice', 'drop-prefix', 'each', 'each-from',\n        'each-index', 'empty?', 'exchange', 'filter', 'filter!', 'filter-as', 'find',\n        'find-from', 'find-index', 'find-index-from', 'find-last', 'find-last-from',\n        'first', 'first2', 'first3', 'first4', 'flip', 'follow', 'fourth', 'glue', 'halves',\n        'harvest', 'head', 'head-slice', 'head-slice*', 'head*', 'head?',\n        'if-empty', 'immutable', 'immutable-sequence', 'immutable-sequence?',\n        'immutable?', 'index', 'index-from', 'indices', 'infimum', 'infimum-by',\n        'insert-nth', 'interleave', 'iota', 'iota-tuple', 'iota-tuple?', 'join',\n        'join-as', 'last', 'last-index', 'last-index-from', 'length', 'lengthen',\n        'like', 'longer', 'longer?', 'longest', 'map', 'map!', 'map-as', 'map-find',\n        'map-find-last', 'map-index', 'map-integers', 'map-reduce', 'map-sum',\n        'max-length', 'member-eq?', 'member?', 'midpoint@', 'min-length',\n        'mismatch', 'move', 'new-like', 'new-resizable', 'new-sequence',\n        'non-negative-integer-expected', 'non-negative-integer-expected?',\n        'nth', 'nths', 'pad-head', 'pad-tail', 'padding', 'partition', 'pop', 'pop*',\n        'prefix', 'prepend', 'prepend-as', 'produce', 'produce-as', 'product', 'push',\n        'push-all', 'push-either', 'push-if', 'reduce', 'reduce-index', 'remove',\n        'remove!', 'remove-eq', 'remove-eq!', 'remove-nth', 'remove-nth!', 'repetition',\n        'repetition?', 'replace-slice', 'replicate', 'replicate-as', 'rest',\n        'rest-slice', 'reverse', 'reverse!', 'reversed', 'reversed?', 'second',\n        'selector', 'selector-for', 'sequence', 'sequence-hashcode', 'sequence=',\n        'sequence?', 'set-first', 'set-fourth', 'set-last', 'set-length', 'set-nth',\n        'set-second', 'set-third', 'short', 'shorten', 'shorter', 'shorter?',\n        'shortest', 'sift', 'slice', 'slice-error', 'slice-error?', 'slice?',\n        'snip', 'snip-slice', 'start', 'start*', 'subseq', 'subseq?', 'suffix',\n        'suffix!', 'sum', 'sum-lengths', 'supremum', 'supremum-by', 'surround', 'tail',\n        'tail-slice', 'tail-slice*', 'tail*', 'tail?', 'third', 'trim',\n        'trim-head', 'trim-head-slice', 'trim-slice', 'trim-tail', 'trim-tail-slice',\n        'unclip', 'unclip-last', 'unclip-last-slice', 'unclip-slice', 'unless-empty',\n        'virtual-exemplar', 'virtual-sequence', 'virtual-sequence?', 'virtual@',\n        'when-empty'), suffix=r'\\s')\n\n    builtin_namespaces = words((\n        '+@', 'change', 'change-global', 'counter', 'dec', 'get', 'get-global',\n        'global', 'inc', 'init-namespaces', 'initialize', 'is-global', 'make-assoc',\n        'namespace', 'namestack', 'off', 'on', 'set', 'set-global', 'set-namestack',\n        'toggle', 'with-global', 'with-scope', 'with-variable', 'with-variables'),\n        suffix=r'\\s')\n\n    builtin_arrays = words((\n        '1array', '2array', '3array', '4array', '<array>', '>array', 'array',\n        'array?', 'pair', 'pair?', 'resize-array'), suffix=r'\\s')\n\n    builtin_io = words((\n        '(each-stream-block-slice)', '(each-stream-block)',\n        '(stream-contents-by-block)', '(stream-contents-by-element)',\n        '(stream-contents-by-length-or-block)',\n        '(stream-contents-by-length)', '+byte+', '+character+',\n        'bad-seek-type', 'bad-seek-type?', 'bl', 'contents', 'each-block',\n        'each-block-size', 'each-block-slice', 'each-line', 'each-morsel',\n        'each-stream-block', 'each-stream-block-slice', 'each-stream-line',\n        'error-stream', 'flush', 'input-stream', 'input-stream?',\n        'invalid-read-buffer', 'invalid-read-buffer?', 'lines', 'nl',\n        'output-stream', 'output-stream?', 'print', 'read', 'read-into',\n        'read-partial', 'read-partial-into', 'read-until', 'read1', 'readln',\n        'seek-absolute', 'seek-absolute?', 'seek-end', 'seek-end?',\n        'seek-input', 'seek-output', 'seek-relative', 'seek-relative?',\n        'stream-bl', 'stream-contents', 'stream-contents*', 'stream-copy',\n        'stream-copy*', 'stream-element-type', 'stream-flush',\n        'stream-length', 'stream-lines', 'stream-nl', 'stream-print',\n        'stream-read', 'stream-read-into', 'stream-read-partial',\n        'stream-read-partial-into', 'stream-read-partial-unsafe',\n        'stream-read-unsafe', 'stream-read-until', 'stream-read1',\n        'stream-readln', 'stream-seek', 'stream-seekable?', 'stream-tell',\n        'stream-write', 'stream-write1', 'tell-input', 'tell-output',\n        'with-error-stream', 'with-error-stream*', 'with-error>output',\n        'with-input-output+error-streams',\n        'with-input-output+error-streams*', 'with-input-stream',\n        'with-input-stream*', 'with-output-stream', 'with-output-stream*',\n        'with-output>error', 'with-output+error-stream',\n        'with-output+error-stream*', 'with-streams', 'with-streams*',\n        'write', 'write1'), suffix=r'\\s')\n\n    builtin_strings = words((\n        '1string', '<string>', '>string', 'resize-string', 'string',\n        'string?'), suffix=r'\\s')\n\n    builtin_vectors = words((\n        '1vector', '<vector>', '>vector', '?push', 'vector', 'vector?'),\n        suffix=r'\\s')\n\n    builtin_continuations = words((\n        '<condition>', '<continuation>', '<restart>', 'attempt-all',\n        'attempt-all-error', 'attempt-all-error?', 'callback-error-hook',\n        'callcc0', 'callcc1', 'cleanup', 'compute-restarts', 'condition',\n        'condition?', 'continuation', 'continuation?', 'continue',\n        'continue-restart', 'continue-with', 'current-continuation',\n        'error', 'error-continuation', 'error-in-thread', 'error-thread',\n        'ifcc', 'ignore-errors', 'in-callback?', 'original-error', 'recover',\n        'restart', 'restart?', 'restarts', 'rethrow', 'rethrow-restarts',\n        'return', 'return-continuation', 'thread-error-hook', 'throw-continue',\n        'throw-restarts', 'with-datastack', 'with-return'), suffix=r'\\s')\n\n    tokens = {\n        'root': [\n            # factor allows a file to start with a shebang\n            (r'#!.*$', Comment.Preproc),\n            default('base'),\n        ],\n        'base': [\n            (r'\\s+', Text),\n\n            # defining words\n            (r'((?:MACRO|MEMO|TYPED)?:[:]?)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function)),\n            (r'(M:[:]?)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class, Text, Name.Function)),\n            (r'(C:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function, Text, Name.Class)),\n            (r'(GENERIC:)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function)),\n            (r'(HOOK:|GENERIC#)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function, Text, Name.Function)),\n            (r'\\(\\s', Name.Function, 'stackeffect'),\n            (r';\\s', Keyword),\n\n            # imports and namespaces\n            (r'(USING:)(\\s+)',\n             bygroups(Keyword.Namespace, Text), 'vocabs'),\n            (r'(USE:|UNUSE:|IN:|QUALIFIED:)(\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Namespace)),\n            (r'(QUALIFIED-WITH:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Namespace, Text, Name.Namespace)),\n            (r'(FROM:|EXCLUDE:)(\\s+)(\\S+)(\\s+=>\\s)',\n             bygroups(Keyword.Namespace, Text, Name.Namespace, Text), 'words'),\n            (r'(RENAME:)(\\s+)(\\S+)(\\s+)(\\S+)(\\s+=>\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Function, Text, Name.Namespace, Text, Name.Function)),\n            (r'(ALIAS:|TYPEDEF:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Function, Text, Name.Function)),\n            (r'(DEFER:|FORGET:|POSTPONE:)(\\s+)(\\S+)',\n             bygroups(Keyword.Namespace, Text, Name.Function)),\n\n            # tuples and classes\n            (r'(TUPLE:|ERROR:)(\\s+)(\\S+)(\\s+<\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class, Text, Name.Class), 'slots'),\n            (r'(TUPLE:|ERROR:|BUILTIN:)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class), 'slots'),\n            (r'(MIXIN:|UNION:|INTERSECTION:)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class)),\n            (r'(PREDICATE:)(\\s+)(\\S+)(\\s+<\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class, Text, Name.Class)),\n            (r'(C:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function, Text, Name.Class)),\n            (r'(INSTANCE:)(\\s+)(\\S+)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Class, Text, Name.Class)),\n            (r'(SLOT:)(\\s+)(\\S+)', bygroups(Keyword, Text, Name.Function)),\n            (r'(SINGLETON:)(\\s+)(\\S+)', bygroups(Keyword, Text, Name.Class)),\n            (r'SINGLETONS:', Keyword, 'classes'),\n\n            # other syntax\n            (r'(CONSTANT:|SYMBOL:|MAIN:|HELP:)(\\s+)(\\S+)',\n             bygroups(Keyword, Text, Name.Function)),\n            (r'SYMBOLS:\\s', Keyword, 'words'),\n            (r'SYNTAX:\\s', Keyword),\n            (r'ALIEN:\\s', Keyword),\n            (r'(STRUCT:)(\\s+)(\\S+)', bygroups(Keyword, Text, Name.Class)),\n            (r'(FUNCTION:)(\\s+\\S+\\s+)(\\S+)(\\s+\\(\\s+[^)]+\\)\\s)',\n             bygroups(Keyword.Namespace, Text, Name.Function, Text)),\n            (r'(FUNCTION-ALIAS:)(\\s+)(\\S+)(\\s+\\S+\\s+)(\\S+)(\\s+\\(\\s+[^)]+\\)\\s)',\n             bygroups(Keyword.Namespace, Text, Name.Function, Text, Name.Function, Text)),\n\n            # vocab.private\n            (r'(?:<PRIVATE|PRIVATE>)\\s', Keyword.Namespace),\n\n            # strings\n            (r'\"\"\"\\s(?:.|\\n)*?\\s\"\"\"', String),\n            (r'\"(?:\\\\\\\\|\\\\\"|[^\"])*\"', String),\n            (r'\\S+\"\\s+(?:\\\\\\\\|\\\\\"|[^\"])*\"', String),\n            (r'CHAR:\\s+(?:\\\\[\\\\abfnrstv]|[^\\\\]\\S*)\\s', String.Char),\n\n            # comments\n            (r'!\\s+.*$', Comment),\n            (r'#!\\s+.*$', Comment),\n            (r'/\\*\\s+(?:.|\\n)*?\\s\\*/\\s', Comment),\n\n            # boolean constants\n            (r'[tf]\\s', Name.Constant),\n\n            # symbols and literals\n            (r'[\\\\$]\\s+\\S+', Name.Constant),\n            (r'M\\\\\\s+\\S+\\s+\\S+', Name.Constant),\n\n            # numbers\n            (r'[+-]?(?:[\\d,]*\\d)?\\.(?:\\d([\\d,]*\\d)?)?(?:[eE][+-]?\\d+)?\\s', Number),\n            (r'[+-]?\\d(?:[\\d,]*\\d)?(?:[eE][+-]?\\d+)?\\s', Number),\n            (r'0x[a-fA-F\\d](?:[a-fA-F\\d,]*[a-fA-F\\d])?(?:p\\d([\\d,]*\\d)?)?\\s', Number),\n            (r'NAN:\\s+[a-fA-F\\d](?:[a-fA-F\\d,]*[a-fA-F\\d])?(?:p\\d([\\d,]*\\d)?)?\\s', Number),\n            (r'0b[01]+\\s', Number.Bin),\n            (r'0o[0-7]+\\s', Number.Oct),\n            (r'(?:\\d([\\d,]*\\d)?)?\\+\\d(?:[\\d,]*\\d)?/\\d(?:[\\d,]*\\d)?\\s', Number),\n            (r'(?:\\-\\d([\\d,]*\\d)?)?\\-\\d(?:[\\d,]*\\d)?/\\d(?:[\\d,]*\\d)?\\s', Number),\n\n            # keywords\n            (r'(?:deprecated|final|foldable|flushable|inline|recursive)\\s',\n             Keyword),\n\n            # builtins\n            (builtin_kernel, Name.Builtin),\n            (builtin_assocs, Name.Builtin),\n            (builtin_combinators, Name.Builtin),\n            (builtin_math, Name.Builtin),\n            (builtin_sequences, Name.Builtin),\n            (builtin_namespaces, Name.Builtin),\n            (builtin_arrays, Name.Builtin),\n            (builtin_io, Name.Builtin),\n            (builtin_strings, Name.Builtin),\n            (builtin_vectors, Name.Builtin),\n            (builtin_continuations, Name.Builtin),\n\n            # everything else is text\n            (r'\\S+', Text),\n        ],\n        'stackeffect': [\n            (r'\\s+', Text),\n            (r'\\(\\s+', Name.Function, 'stackeffect'),\n            (r'\\)\\s', Name.Function, '#pop'),\n            (r'--\\s', Name.Function),\n            (r'\\S+', Name.Variable),\n        ],\n        'slots': [\n            (r'\\s+', Text),\n            (r';\\s', Keyword, '#pop'),\n            (r'(\\{\\s+)(\\S+)(\\s[^}]+\\s\\}\\s)',\n             bygroups(Text, Name.Variable, Text)),\n            (r'\\S+', Name.Variable),\n        ],\n        'vocabs': [\n            (r'\\s+', Text),\n            (r';\\s', Keyword, '#pop'),\n            (r'\\S+', Name.Namespace),\n        ],\n        'classes': [\n            (r'\\s+', Text),\n            (r';\\s', Keyword, '#pop'),\n            (r'\\S+', Name.Class),\n        ],\n        'words': [\n            (r'\\s+', Text),\n            (r';\\s', Keyword, '#pop'),\n            (r'\\S+', Name.Function),\n        ],\n    }\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.jvm\n    ~~~~~~~~~~~~~~~~~~~\n\n    Pygments lexers for JVM languages.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexer import Lexer, RegexLexer, include, bygroups, using, \\\n    this, combined, default, words\nfrom pygments.token import Text, Comment, Operator, Keyword, Name, String, \\\n    Number, Punctuation\nfrom pygments.util import shebang_matches\nfrom pygments import unistring as uni\n\n__all__ = ['JavaLexer', 'ScalaLexer', 'GosuLexer', 'GosuTemplateLexer',\n           'GroovyLexer', 'IokeLexer', 'ClojureLexer', 'ClojureScriptLexer',\n           'KotlinLexer', 'XtendLexer', 'AspectJLexer', 'CeylonLexer',\n           'PigLexer', 'GoloLexer', 'JasminLexer', 'SarlLexer']\n\n\nclass JavaLexer(RegexLexer):\n    \"\"\"\n    For `Java <https://www.oracle.com/technetwork/java/>`_ source code.\n    \"\"\"\n\n    name = 'Java'\n    aliases = ['java']\n    filenames = ['*.java']\n    mimetypes = ['text/x-java']\n\n    flags = re.MULTILINE | re.DOTALL | re.UNICODE\n\n    tokens = {\n        'root': [\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            # keywords: go before method names to avoid lexing \"throw new XYZ\"\n            # as a method signature\n            (r'(assert|break|case|catch|continue|default|do|else|finally|for|'\n             r'if|goto|instanceof|new|return|switch|this|throw|try|while)\\b',\n             Keyword),\n            # method names\n            (r'((?:(?:[^\\W\\d]|\\$)[\\w.\\[\\]$<>]*\\s+)+?)'  # return arguments\n             r'((?:[^\\W\\d]|\\$)[\\w$]*)'                  # method name\n             r'(\\s*)(\\()',                              # signature start\n             bygroups(using(this), Name.Function, Text, Punctuation)),\n            (r'@[^\\W\\d][\\w.]*', Name.Decorator),\n            (r'(abstract|const|enum|extends|final|implements|native|private|'\n             r'protected|public|static|strictfp|super|synchronized|throws|'\n             r'transient|volatile)\\b', Keyword.Declaration),\n            (r'(boolean|byte|char|double|float|int|long|short|void)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(class|interface)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'class'),\n            (r'(var)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'var'),\n            (r'(import(?:\\s+static)?)(\\s+)', bygroups(Keyword.Namespace, Text),\n             'import'),\n            (r'\"', String, 'string'),\n            (r\"'\\\\.'|'[^\\\\]'|'\\\\u[0-9a-fA-F]{4}'\", String.Char),\n            (r'(\\.)((?:[^\\W\\d]|\\$)[\\w$]*)', bygroups(Punctuation,\n                                                     Name.Attribute)),\n            (r'^\\s*([^\\W\\d]|\\$)[\\w$]*:', Name.Label),\n            (r'([^\\W\\d]|\\$)[\\w$]*', Name),\n            (r'([0-9][0-9_]*\\.([0-9][0-9_]*)?|'\n             r'\\.[0-9][0-9_]*)'\n             r'([eE][+\\-]?[0-9][0-9_]*)?[fFdD]?|'\n             r'[0-9][eE][+\\-]?[0-9][0-9_]*[fFdD]?|'\n             r'[0-9]([eE][+\\-]?[0-9][0-9_]*)?[fFdD]|'\n             r'0[xX]([0-9a-fA-F][0-9a-fA-F_]*\\.?|'\n             r'([0-9a-fA-F][0-9a-fA-F_]*)?\\.[0-9a-fA-F][0-9a-fA-F_]*)'\n             r'[pP][+\\-]?[0-9][0-9_]*[fFdD]?', Number.Float),\n            (r'0[xX][0-9a-fA-F][0-9a-fA-F_]*[lL]?', Number.Hex),\n            (r'0[bB][01][01_]*[lL]?', Number.Bin),\n            (r'0[0-7_]+[lL]?', Number.Oct),\n            (r'0|[1-9][0-9_]*[lL]?', Number.Integer),\n            (r'[~^*!%&\\[\\]<>|+=/?-]', Operator),\n            (r'[{}();:.,]', Punctuation),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'([^\\W\\d]|\\$)[\\w$]*', Name.Class, '#pop')\n        ],\n        'var': [\n            (r'([^\\W\\d]|\\$)[\\w$]*', Name, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n        'string': [\n            (r'[^\\\\\"]+', String),\n            (r'\\\\\\\\', String),  # Escaped backslash\n            (r'\\\\\"', String),  # Escaped quote\n            (r'\\\\', String),  # Bare backslash\n            (r'\"', String, '#pop'),  # Closing quote\n        ],\n    }\n\n\nclass AspectJLexer(JavaLexer):\n    \"\"\"\n    For `AspectJ <http://www.eclipse.org/aspectj/>`_ source code.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'AspectJ'\n    aliases = ['aspectj']\n    filenames = ['*.aj']\n    mimetypes = ['text/x-aspectj']\n\n    aj_keywords = {\n        'aspect', 'pointcut', 'privileged', 'call', 'execution',\n        'initialization', 'preinitialization', 'handler', 'get', 'set',\n        'staticinitialization', 'target', 'args', 'within', 'withincode',\n        'cflow', 'cflowbelow', 'annotation', 'before', 'after', 'around',\n        'proceed', 'throwing', 'returning', 'adviceexecution', 'declare',\n        'parents', 'warning', 'error', 'soft', 'precedence', 'thisJoinPoint',\n        'thisJoinPointStaticPart', 'thisEnclosingJoinPointStaticPart',\n        'issingleton', 'perthis', 'pertarget', 'percflow', 'percflowbelow',\n        'pertypewithin', 'lock', 'unlock', 'thisAspectInstance'\n    }\n    aj_inter_type = {'parents:', 'warning:', 'error:', 'soft:', 'precedence:'}\n    aj_inter_type_annotation = {'@type', '@method', '@constructor', '@field'}\n\n    def get_tokens_unprocessed(self, text):\n        for index, token, value in JavaLexer.get_tokens_unprocessed(self, text):\n            if token is Name and value in self.aj_keywords:\n                yield index, Keyword, value\n            elif token is Name.Label and value in self.aj_inter_type:\n                yield index, Keyword, value[:-1]\n                yield index, Operator, value[-1]\n            elif token is Name.Decorator and value in self.aj_inter_type_annotation:\n                yield index, Keyword, value\n            else:\n                yield index, token, value\n\n\nclass ScalaLexer(RegexLexer):\n    \"\"\"\n    For `Scala <http://www.scala-lang.org>`_ source code.\n    \"\"\"\n\n    name = 'Scala'\n    aliases = ['scala']\n    filenames = ['*.scala']\n    mimetypes = ['text/x-scala']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    # don't use raw unicode strings!\n    op = ('[-~\\\\^\\\\*!%&\\\\\\\\<>\\\\|+=:/?@\\u00a6-\\u00a7\\u00a9\\u00ac\\u00ae\\u00b0-\\u00b1'\n          '\\u00b6\\u00d7\\u00f7\\u03f6\\u0482\\u0606-\\u0608\\u060e-\\u060f\\u06e9'\n          '\\u06fd-\\u06fe\\u07f6\\u09fa\\u0b70\\u0bf3-\\u0bf8\\u0bfa\\u0c7f\\u0cf1-\\u0cf2'\n          '\\u0d79\\u0f01-\\u0f03\\u0f13-\\u0f17\\u0f1a-\\u0f1f\\u0f34\\u0f36\\u0f38'\n          '\\u0fbe-\\u0fc5\\u0fc7-\\u0fcf\\u109e-\\u109f\\u1360\\u1390-\\u1399\\u1940'\n          '\\u19e0-\\u19ff\\u1b61-\\u1b6a\\u1b74-\\u1b7c\\u2044\\u2052\\u207a-\\u207c'\n          '\\u208a-\\u208c\\u2100-\\u2101\\u2103-\\u2106\\u2108-\\u2109\\u2114\\u2116-\\u2118'\n          '\\u211e-\\u2123\\u2125\\u2127\\u2129\\u212e\\u213a-\\u213b\\u2140-\\u2144'\n          '\\u214a-\\u214d\\u214f\\u2190-\\u2328\\u232b-\\u244a\\u249c-\\u24e9\\u2500-\\u2767'\n          '\\u2794-\\u27c4\\u27c7-\\u27e5\\u27f0-\\u2982\\u2999-\\u29d7\\u29dc-\\u29fb'\n          '\\u29fe-\\u2b54\\u2ce5-\\u2cea\\u2e80-\\u2ffb\\u3004\\u3012-\\u3013\\u3020'\n          '\\u3036-\\u3037\\u303e-\\u303f\\u3190-\\u3191\\u3196-\\u319f\\u31c0-\\u31e3'\n          '\\u3200-\\u321e\\u322a-\\u3250\\u3260-\\u327f\\u328a-\\u32b0\\u32c0-\\u33ff'\n          '\\u4dc0-\\u4dff\\ua490-\\ua4c6\\ua828-\\ua82b\\ufb29\\ufdfd\\ufe62\\ufe64-\\ufe66'\n          '\\uff0b\\uff1c-\\uff1e\\uff5c\\uff5e\\uffe2\\uffe4\\uffe8-\\uffee\\ufffc-\\ufffd]+')\n\n    letter = ('[a-zA-Z\\\\$_\\u00aa\\u00b5\\u00ba\\u00c0-\\u00d6\\u00d8-\\u00f6'\n              '\\u00f8-\\u02af\\u0370-\\u0373\\u0376-\\u0377\\u037b-\\u037d\\u0386'\n              '\\u0388-\\u03f5\\u03f7-\\u0481\\u048a-\\u0556\\u0561-\\u0587\\u05d0-\\u05f2'\n              '\\u0621-\\u063f\\u0641-\\u064a\\u066e-\\u066f\\u0671-\\u06d3\\u06d5'\n              '\\u06ee-\\u06ef\\u06fa-\\u06fc\\u06ff\\u0710\\u0712-\\u072f\\u074d-\\u07a5'\n              '\\u07b1\\u07ca-\\u07ea\\u0904-\\u0939\\u093d\\u0950\\u0958-\\u0961'\n              '\\u0972-\\u097f\\u0985-\\u09b9\\u09bd\\u09ce\\u09dc-\\u09e1\\u09f0-\\u09f1'\n              '\\u0a05-\\u0a39\\u0a59-\\u0a5e\\u0a72-\\u0a74\\u0a85-\\u0ab9\\u0abd'\n              '\\u0ad0-\\u0ae1\\u0b05-\\u0b39\\u0b3d\\u0b5c-\\u0b61\\u0b71\\u0b83-\\u0bb9'\n              '\\u0bd0\\u0c05-\\u0c3d\\u0c58-\\u0c61\\u0c85-\\u0cb9\\u0cbd\\u0cde-\\u0ce1'\n              '\\u0d05-\\u0d3d\\u0d60-\\u0d61\\u0d7a-\\u0d7f\\u0d85-\\u0dc6\\u0e01-\\u0e30'\n              '\\u0e32-\\u0e33\\u0e40-\\u0e45\\u0e81-\\u0eb0\\u0eb2-\\u0eb3\\u0ebd-\\u0ec4'\n              '\\u0edc-\\u0f00\\u0f40-\\u0f6c\\u0f88-\\u0f8b\\u1000-\\u102a\\u103f'\n              '\\u1050-\\u1055\\u105a-\\u105d\\u1061\\u1065-\\u1066\\u106e-\\u1070'\n              '\\u1075-\\u1081\\u108e\\u10a0-\\u10fa\\u1100-\\u135a\\u1380-\\u138f'\n              '\\u13a0-\\u166c\\u166f-\\u1676\\u1681-\\u169a\\u16a0-\\u16ea\\u16ee-\\u1711'\n              '\\u1720-\\u1731\\u1740-\\u1751\\u1760-\\u1770\\u1780-\\u17b3\\u17dc'\n              '\\u1820-\\u1842\\u1844-\\u18a8\\u18aa-\\u191c\\u1950-\\u19a9\\u19c1-\\u19c7'\n              '\\u1a00-\\u1a16\\u1b05-\\u1b33\\u1b45-\\u1b4b\\u1b83-\\u1ba0\\u1bae-\\u1baf'\n              '\\u1c00-\\u1c23\\u1c4d-\\u1c4f\\u1c5a-\\u1c77\\u1d00-\\u1d2b\\u1d62-\\u1d77'\n              '\\u1d79-\\u1d9a\\u1e00-\\u1fbc\\u1fbe\\u1fc2-\\u1fcc\\u1fd0-\\u1fdb'\n              '\\u1fe0-\\u1fec\\u1ff2-\\u1ffc\\u2071\\u207f\\u2102\\u2107\\u210a-\\u2113'\n              '\\u2115\\u2119-\\u211d\\u2124\\u2126\\u2128\\u212a-\\u212d\\u212f-\\u2139'\n              '\\u213c-\\u213f\\u2145-\\u2149\\u214e\\u2160-\\u2188\\u2c00-\\u2c7c'\n              '\\u2c80-\\u2ce4\\u2d00-\\u2d65\\u2d80-\\u2dde\\u3006-\\u3007\\u3021-\\u3029'\n              '\\u3038-\\u303a\\u303c\\u3041-\\u3096\\u309f\\u30a1-\\u30fa\\u30ff-\\u318e'\n              '\\u31a0-\\u31b7\\u31f0-\\u31ff\\u3400-\\u4db5\\u4e00-\\ua014\\ua016-\\ua48c'\n              '\\ua500-\\ua60b\\ua610-\\ua61f\\ua62a-\\ua66e\\ua680-\\ua697\\ua722-\\ua76f'\n              '\\ua771-\\ua787\\ua78b-\\ua801\\ua803-\\ua805\\ua807-\\ua80a\\ua80c-\\ua822'\n              '\\ua840-\\ua873\\ua882-\\ua8b3\\ua90a-\\ua925\\ua930-\\ua946\\uaa00-\\uaa28'\n              '\\uaa40-\\uaa42\\uaa44-\\uaa4b\\uac00-\\ud7a3\\uf900-\\ufb1d\\ufb1f-\\ufb28'\n              '\\ufb2a-\\ufd3d\\ufd50-\\ufdfb\\ufe70-\\ufefc\\uff21-\\uff3a\\uff41-\\uff5a'\n              '\\uff66-\\uff6f\\uff71-\\uff9d\\uffa0-\\uffdc]')\n\n    upper = ('[A-Z\\\\$_\\u00c0-\\u00d6\\u00d8-\\u00de\\u0100\\u0102\\u0104\\u0106\\u0108'\n             '\\u010a\\u010c\\u010e\\u0110\\u0112\\u0114\\u0116\\u0118\\u011a\\u011c'\n             '\\u011e\\u0120\\u0122\\u0124\\u0126\\u0128\\u012a\\u012c\\u012e\\u0130'\n             '\\u0132\\u0134\\u0136\\u0139\\u013b\\u013d\\u013f\\u0141\\u0143\\u0145'\n             '\\u0147\\u014a\\u014c\\u014e\\u0150\\u0152\\u0154\\u0156\\u0158\\u015a'\n             '\\u015c\\u015e\\u0160\\u0162\\u0164\\u0166\\u0168\\u016a\\u016c\\u016e'\n             '\\u0170\\u0172\\u0174\\u0176\\u0178-\\u0179\\u017b\\u017d\\u0181-\\u0182'\n             '\\u0184\\u0186-\\u0187\\u0189-\\u018b\\u018e-\\u0191\\u0193-\\u0194'\n             '\\u0196-\\u0198\\u019c-\\u019d\\u019f-\\u01a0\\u01a2\\u01a4\\u01a6-\\u01a7'\n             '\\u01a9\\u01ac\\u01ae-\\u01af\\u01b1-\\u01b3\\u01b5\\u01b7-\\u01b8\\u01bc'\n             '\\u01c4\\u01c7\\u01ca\\u01cd\\u01cf\\u01d1\\u01d3\\u01d5\\u01d7\\u01d9'\n             '\\u01db\\u01de\\u01e0\\u01e2\\u01e4\\u01e6\\u01e8\\u01ea\\u01ec\\u01ee'\n             '\\u01f1\\u01f4\\u01f6-\\u01f8\\u01fa\\u01fc\\u01fe\\u0200\\u0202\\u0204'\n             '\\u0206\\u0208\\u020a\\u020c\\u020e\\u0210\\u0212\\u0214\\u0216\\u0218'\n             '\\u021a\\u021c\\u021e\\u0220\\u0222\\u0224\\u0226\\u0228\\u022a\\u022c'\n             '\\u022e\\u0230\\u0232\\u023a-\\u023b\\u023d-\\u023e\\u0241\\u0243-\\u0246'\n             '\\u0248\\u024a\\u024c\\u024e\\u0370\\u0372\\u0376\\u0386\\u0388-\\u038f'\n             '\\u0391-\\u03ab\\u03cf\\u03d2-\\u03d4\\u03d8\\u03da\\u03dc\\u03de\\u03e0'\n             '\\u03e2\\u03e4\\u03e6\\u03e8\\u03ea\\u03ec\\u03ee\\u03f4\\u03f7'\n             '\\u03f9-\\u03fa\\u03fd-\\u042f\\u0460\\u0462\\u0464\\u0466\\u0468\\u046a'\n             '\\u046c\\u046e\\u0470\\u0472\\u0474\\u0476\\u0478\\u047a\\u047c\\u047e'\n             '\\u0480\\u048a\\u048c\\u048e\\u0490\\u0492\\u0494\\u0496\\u0498\\u049a'\n             '\\u049c\\u049e\\u04a0\\u04a2\\u04a4\\u04a6\\u04a8\\u04aa\\u04ac\\u04ae'\n             '\\u04b0\\u04b2\\u04b4\\u04b6\\u04b8\\u04ba\\u04bc\\u04be\\u04c0-\\u04c1'\n             '\\u04c3\\u04c5\\u04c7\\u04c9\\u04cb\\u04cd\\u04d0\\u04d2\\u04d4\\u04d6'\n             '\\u04d8\\u04da\\u04dc\\u04de\\u04e0\\u04e2\\u04e4\\u04e6\\u04e8\\u04ea'\n             '\\u04ec\\u04ee\\u04f0\\u04f2\\u04f4\\u04f6\\u04f8\\u04fa\\u04fc\\u04fe'\n             '\\u0500\\u0502\\u0504\\u0506\\u0508\\u050a\\u050c\\u050e\\u0510\\u0512'\n             '\\u0514\\u0516\\u0518\\u051a\\u051c\\u051e\\u0520\\u0522\\u0531-\\u0556'\n             '\\u10a0-\\u10c5\\u1e00\\u1e02\\u1e04\\u1e06\\u1e08\\u1e0a\\u1e0c\\u1e0e'\n             '\\u1e10\\u1e12\\u1e14\\u1e16\\u1e18\\u1e1a\\u1e1c\\u1e1e\\u1e20\\u1e22'\n             '\\u1e24\\u1e26\\u1e28\\u1e2a\\u1e2c\\u1e2e\\u1e30\\u1e32\\u1e34\\u1e36'\n             '\\u1e38\\u1e3a\\u1e3c\\u1e3e\\u1e40\\u1e42\\u1e44\\u1e46\\u1e48\\u1e4a'\n             '\\u1e4c\\u1e4e\\u1e50\\u1e52\\u1e54\\u1e56\\u1e58\\u1e5a\\u1e5c\\u1e5e'\n             '\\u1e60\\u1e62\\u1e64\\u1e66\\u1e68\\u1e6a\\u1e6c\\u1e6e\\u1e70\\u1e72'\n             '\\u1e74\\u1e76\\u1e78\\u1e7a\\u1e7c\\u1e7e\\u1e80\\u1e82\\u1e84\\u1e86'\n             '\\u1e88\\u1e8a\\u1e8c\\u1e8e\\u1e90\\u1e92\\u1e94\\u1e9e\\u1ea0\\u1ea2'\n             '\\u1ea4\\u1ea6\\u1ea8\\u1eaa\\u1eac\\u1eae\\u1eb0\\u1eb2\\u1eb4\\u1eb6'\n             '\\u1eb8\\u1eba\\u1ebc\\u1ebe\\u1ec0\\u1ec2\\u1ec4\\u1ec6\\u1ec8\\u1eca'\n             '\\u1ecc\\u1ece\\u1ed0\\u1ed2\\u1ed4\\u1ed6\\u1ed8\\u1eda\\u1edc\\u1ede'\n             '\\u1ee0\\u1ee2\\u1ee4\\u1ee6\\u1ee8\\u1eea\\u1eec\\u1eee\\u1ef0\\u1ef2'\n             '\\u1ef4\\u1ef6\\u1ef8\\u1efa\\u1efc\\u1efe\\u1f08-\\u1f0f\\u1f18-\\u1f1d'\n             '\\u1f28-\\u1f2f\\u1f38-\\u1f3f\\u1f48-\\u1f4d\\u1f59-\\u1f5f'\n             '\\u1f68-\\u1f6f\\u1fb8-\\u1fbb\\u1fc8-\\u1fcb\\u1fd8-\\u1fdb'\n             '\\u1fe8-\\u1fec\\u1ff8-\\u1ffb\\u2102\\u2107\\u210b-\\u210d\\u2110-\\u2112'\n             '\\u2115\\u2119-\\u211d\\u2124\\u2126\\u2128\\u212a-\\u212d\\u2130-\\u2133'\n             '\\u213e-\\u213f\\u2145\\u2183\\u2c00-\\u2c2e\\u2c60\\u2c62-\\u2c64\\u2c67'\n             '\\u2c69\\u2c6b\\u2c6d-\\u2c6f\\u2c72\\u2c75\\u2c80\\u2c82\\u2c84\\u2c86'\n             '\\u2c88\\u2c8a\\u2c8c\\u2c8e\\u2c90\\u2c92\\u2c94\\u2c96\\u2c98\\u2c9a'\n             '\\u2c9c\\u2c9e\\u2ca0\\u2ca2\\u2ca4\\u2ca6\\u2ca8\\u2caa\\u2cac\\u2cae'\n             '\\u2cb0\\u2cb2\\u2cb4\\u2cb6\\u2cb8\\u2cba\\u2cbc\\u2cbe\\u2cc0\\u2cc2'\n             '\\u2cc4\\u2cc6\\u2cc8\\u2cca\\u2ccc\\u2cce\\u2cd0\\u2cd2\\u2cd4\\u2cd6'\n             '\\u2cd8\\u2cda\\u2cdc\\u2cde\\u2ce0\\u2ce2\\ua640\\ua642\\ua644\\ua646'\n             '\\ua648\\ua64a\\ua64c\\ua64e\\ua650\\ua652\\ua654\\ua656\\ua658\\ua65a'\n             '\\ua65c\\ua65e\\ua662\\ua664\\ua666\\ua668\\ua66a\\ua66c\\ua680\\ua682'\n             '\\ua684\\ua686\\ua688\\ua68a\\ua68c\\ua68e\\ua690\\ua692\\ua694\\ua696'\n             '\\ua722\\ua724\\ua726\\ua728\\ua72a\\ua72c\\ua72e\\ua732\\ua734\\ua736'\n             '\\ua738\\ua73a\\ua73c\\ua73e\\ua740\\ua742\\ua744\\ua746\\ua748\\ua74a'\n             '\\ua74c\\ua74e\\ua750\\ua752\\ua754\\ua756\\ua758\\ua75a\\ua75c\\ua75e'\n             '\\ua760\\ua762\\ua764\\ua766\\ua768\\ua76a\\ua76c\\ua76e\\ua779\\ua77b'\n             '\\ua77d-\\ua77e\\ua780\\ua782\\ua784\\ua786\\ua78b\\uff21-\\uff3a]')\n\n    idrest = '%s(?:%s|[0-9])*(?:(?<=_)%s)?' % (letter, letter, op)\n    letter_letter_digit = '%s(?:%s|\\\\d)*' % (letter, letter)\n\n    tokens = {\n        'root': [\n            # method names\n            (r'(class|trait|object)(\\s+)', bygroups(Keyword, Text), 'class'),\n            (r'[^\\S\\n]+', Text),\n            include('comments'),\n            (r'@%s' % idrest, Name.Decorator),\n            (r'(abstract|ca(?:se|tch)|d(?:ef|o)|e(?:lse|xtends)|'\n             r'f(?:inal(?:ly)?|or(?:Some)?)|i(?:f|mplicit)|'\n             r'lazy|match|new|override|pr(?:ivate|otected)'\n             r'|re(?:quires|turn)|s(?:ealed|uper)|'\n             r't(?:h(?:is|row)|ry)|va[lr]|w(?:hile|ith)|yield)\\b|'\n             r'(<[%:-]|=>|>:|[#=@_\\u21D2\\u2190])\\b', Keyword),\n            (r':(?!%s)' % op, Keyword, 'type'),\n            (r'%s%s\\b' % (upper, idrest), Name.Class),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(import|package)(\\s+)', bygroups(Keyword, Text), 'import'),\n            (r'(type)(\\s+)', bygroups(Keyword, Text), 'type'),\n            (r'\"\"\".*?\"\"\"(?!\")', String),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String),\n            (r\"'\\\\.'|'[^\\\\]'|'\\\\u[0-9a-fA-F]{4}'\", String.Char),\n            (r\"'%s\" % idrest, Text.Symbol),\n            (r'[fs]\"\"\"', String, 'interptriplestring'),  # interpolated strings\n            (r'[fs]\"', String, 'interpstring'),  # interpolated strings\n            (r'raw\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String),  # raw strings\n            # (r'(\\.)(%s|%s|`[^`]+`)' % (idrest, op), bygroups(Operator,\n            # Name.Attribute)),\n            (idrest, Name),\n            (r'`[^`]+`', Name),\n            (r'\\[', Operator, 'typeparam'),\n            (r'[(){};,.#]', Operator),\n            (op, Operator),\n            (r'([0-9][0-9]*\\.[0-9]*|\\.[0-9]+)([eE][+-]?[0-9]+)?[fFdD]?',\n             Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'(%s|%s|`[^`]+`)(\\s*)(\\[)' % (idrest, op),\n             bygroups(Name.Class, Text, Operator), ('#pop', 'typeparam')),\n            (r'\\s+', Text),\n            include('comments'),\n            (r'\\{', Operator, '#pop'),\n            (r'\\(', Operator, '#pop'),\n            (r'%s|%s|`[^`]+`' % (idrest, op), Name.Class, '#pop'),\n        ],\n        'type': [\n            (r'\\s+', Text),\n            include('comments'),\n            (r'<[%:]|>:|[#_]|\\bforSome\\b|\\btype\\b', Keyword),\n            (r'([,);}]|=>|=|\\u21d2)(\\s*)', bygroups(Operator, Text), '#pop'),\n            (r'[({]', Operator, '#push'),\n            (r'((?:%s|%s|`[^`]+`)(?:\\.(?:%s|%s|`[^`]+`))*)(\\s*)(\\[)' %\n             (idrest, op, idrest, op),\n             bygroups(Keyword.Type, Text, Operator), ('#pop', 'typeparam')),\n            (r'((?:%s|%s|`[^`]+`)(?:\\.(?:%s|%s|`[^`]+`))*)(\\s*)$' %\n             (idrest, op, idrest, op),\n             bygroups(Keyword.Type, Text), '#pop'),\n            (r'\\.|%s|%s|`[^`]+`' % (idrest, op), Keyword.Type)\n        ],\n        'typeparam': [\n            (r'\\s+', Text),\n            include('comments'),\n            (r',+', Punctuation),\n            (r'<[%:]|=>|>:|[#_\\u21D2]|\\bforSome\\b|\\btype\\b', Keyword),\n            (r'([\\])}])', Operator, '#pop'),\n            (r'[(\\[{]', Operator, '#push'),\n            (r'\\.|%s|%s|`[^`]+`' % (idrest, op), Keyword.Type)\n        ],\n        'comments': [\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*', Comment.Multiline, 'comment'),\n        ],\n        'comment': [\n            (r'[^/*]+', Comment.Multiline),\n            (r'/\\*', Comment.Multiline, '#push'),\n            (r'\\*/', Comment.Multiline, '#pop'),\n            (r'[*/]', Comment.Multiline)\n        ],\n        'import': [\n            (r'(%s|\\.)+' % idrest, Name.Namespace, '#pop')\n        ],\n        'interpstringcommon': [\n            (r'[^\"$\\\\]+', String),\n            (r'\\$\\$', String),\n            (r'\\$' + letter_letter_digit, String.Interpol),\n            (r'\\$\\{', String.Interpol, 'interpbrace'),\n            (r'\\\\.', String),\n        ],\n        'interptriplestring': [\n            (r'\"\"\"(?!\")', String, '#pop'),\n            (r'\"', String),\n            include('interpstringcommon'),\n        ],\n        'interpstring': [\n            (r'\"', String, '#pop'),\n            include('interpstringcommon'),\n        ],\n        'interpbrace': [\n            (r'\\}', String.Interpol, '#pop'),\n            (r'\\{', String.Interpol, '#push'),\n            include('root'),\n        ],\n    }\n\n\nclass GosuLexer(RegexLexer):\n    \"\"\"\n    For Gosu source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    name = 'Gosu'\n    aliases = ['gosu']\n    filenames = ['*.gs', '*.gsx', '*.gsp', '*.vark']\n    mimetypes = ['text/x-gosu']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # modifiers etc.\n             r'([a-zA-Z_]\\w*)'                       # method name\n             r'(\\s*)(\\()',                           # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w.]*', Name.Decorator),\n            (r'(in|as|typeof|statictypeof|typeis|typeas|if|else|foreach|for|'\n             r'index|while|do|continue|break|return|try|catch|finally|this|'\n             r'throw|new|switch|case|default|eval|super|outer|classpath|'\n             r'using)\\b', Keyword),\n            (r'(var|delegate|construct|function|private|internal|protected|'\n             r'public|abstract|override|final|static|extends|transient|'\n             r'implements|represents|readonly)\\b', Keyword.Declaration),\n            (r'(property\\s+)(get|set)?', Keyword.Declaration),\n            (r'(boolean|byte|char|double|float|int|long|short|void|block)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(true|false|null|NaN|Infinity)\\b', Keyword.Constant),\n            (r'(class|interface|enhancement|enum)(\\s+)([a-zA-Z_]\\w*)',\n             bygroups(Keyword.Declaration, Text, Name.Class)),\n            (r'(uses)(\\s+)([\\w.]+\\*?)',\n             bygroups(Keyword.Namespace, Text, Name.Namespace)),\n            (r'\"', String, 'string'),\n            (r'(\\??[.#])([a-zA-Z_]\\w*)',\n             bygroups(Operator, Name.Attribute)),\n            (r'(:)([a-zA-Z_]\\w*)',\n             bygroups(Operator, Name.Attribute)),\n            (r'[a-zA-Z_$]\\w*', Name),\n            (r'and|or|not|[\\\\~^*!%&\\[\\](){}<>|+=:;,./?-]', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'[0-9]+', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'templateText': [\n            (r'(\\\\<)|(\\\\\\$)', String),\n            (r'(<%@\\s+)(extends|params)',\n             bygroups(Operator, Name.Decorator), 'stringTemplate'),\n            (r'<%!--.*?--%>', Comment.Multiline),\n            (r'(<%)|(<%=)', Operator, 'stringTemplate'),\n            (r'\\$\\{', Operator, 'stringTemplateShorthand'),\n            (r'.', String)\n        ],\n        'string': [\n            (r'\"', String, '#pop'),\n            include('templateText')\n        ],\n        'stringTemplate': [\n            (r'\"', String, 'string'),\n            (r'%>', Operator, '#pop'),\n            include('root')\n        ],\n        'stringTemplateShorthand': [\n            (r'\"', String, 'string'),\n            (r'\\{', Operator, 'stringTemplateShorthand'),\n            (r'\\}', Operator, '#pop'),\n            include('root')\n        ],\n    }\n\n\nclass GosuTemplateLexer(Lexer):\n    \"\"\"\n    For Gosu templates.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    name = 'Gosu Template'\n    aliases = ['gst']\n    filenames = ['*.gst']\n    mimetypes = ['text/x-gosu-template']\n\n    def get_tokens_unprocessed(self, text):\n        lexer = GosuLexer()\n        stack = ['templateText']\n        yield from lexer.get_tokens_unprocessed(text, stack)\n\n\nclass GroovyLexer(RegexLexer):\n    \"\"\"\n    For `Groovy <http://groovy.codehaus.org/>`_ source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    name = 'Groovy'\n    aliases = ['groovy']\n    filenames = ['*.groovy','*.gradle']\n    mimetypes = ['text/x-groovy']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # Groovy allows a file to start with a shebang\n            (r'#!(.*?)$', Comment.Preproc, 'base'),\n            default('base'),\n        ],\n        'base': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_]\\w*)'                      # method name\n             r'(\\s*)(\\()',                          # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w.]*', Name.Decorator),\n            (r'(assert|break|case|catch|continue|default|do|else|finally|for|'\n             r'if|goto|instanceof|new|return|switch|this|throw|try|while|in|as)\\b',\n             Keyword),\n            (r'(abstract|const|enum|extends|final|implements|native|private|'\n             r'protected|public|static|strictfp|super|synchronized|throws|'\n             r'transient|volatile)\\b', Keyword.Declaration),\n            (r'(def|boolean|byte|char|double|float|int|long|short|void)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(class|interface)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'class'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'\"\"\".*?\"\"\"', String.Double),\n            (r\"'''.*?'''\", String.Single),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'\\$/((?!/\\$).)*/\\$', String),\n            (r'/(\\\\\\\\|\\\\[^\\\\]|[^/\\\\])*/', String),\n            (r\"'\\\\.'|'[^\\\\]'|'\\\\u[0-9a-fA-F]{4}'\", String.Char),\n            (r'(\\.)([a-zA-Z_]\\w*)', bygroups(Operator, Name.Attribute)),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_$]\\w*', Name),\n            (r'[~^*!%&\\[\\](){}<>|+=:;,./?-]', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'[a-zA-Z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n    }\n\n    def analyse_text(text):\n        return shebang_matches(text, r'groovy')\n\n\nclass IokeLexer(RegexLexer):\n    \"\"\"\n    For `Ioke <http://ioke.org/>`_ (a strongly typed, dynamic,\n    prototype based programming language) source.\n\n    .. versionadded:: 1.4\n    \"\"\"\n    name = 'Ioke'\n    filenames = ['*.ik']\n    aliases = ['ioke', 'ik']\n    mimetypes = ['text/x-iokesrc']\n    tokens = {\n        'interpolatableText': [\n            (r'(\\\\b|\\\\e|\\\\t|\\\\n|\\\\f|\\\\r|\\\\\"|\\\\\\\\|\\\\#|\\\\\\Z|\\\\u[0-9a-fA-F]{1,4}'\n             r'|\\\\[0-3]?[0-7]?[0-7])', String.Escape),\n            (r'#\\{', Punctuation, 'textInterpolationRoot')\n        ],\n\n        'text': [\n            (r'(?<!\\\\)\"', String, '#pop'),\n            include('interpolatableText'),\n            (r'[^\"]', String)\n        ],\n\n        'documentation': [\n            (r'(?<!\\\\)\"', String.Doc, '#pop'),\n            include('interpolatableText'),\n            (r'[^\"]', String.Doc)\n        ],\n\n        'textInterpolationRoot': [\n            (r'\\}', Punctuation, '#pop'),\n            include('root')\n        ],\n\n        'slashRegexp': [\n            (r'(?<!\\\\)/[im-psux]*', String.Regex, '#pop'),\n            include('interpolatableText'),\n            (r'\\\\/', String.Regex),\n            (r'[^/]', String.Regex)\n        ],\n\n        'squareRegexp': [\n            (r'(?<!\\\\)][im-psux]*', String.Regex, '#pop'),\n            include('interpolatableText'),\n            (r'\\\\]', String.Regex),\n            (r'[^\\]]', String.Regex)\n        ],\n\n        'squareText': [\n            (r'(?<!\\\\)]', String, '#pop'),\n            include('interpolatableText'),\n            (r'[^\\]]', String)\n        ],\n\n        'root': [\n            (r'\\n', Text),\n            (r'\\s+', Text),\n\n            # Comments\n            (r';(.*?)\\n', Comment),\n            (r'\\A#!(.*?)\\n', Comment),\n\n            # Regexps\n            (r'#/', String.Regex, 'slashRegexp'),\n            (r'#r\\[', String.Regex, 'squareRegexp'),\n\n            # Symbols\n            (r':[\\w!:?]+', String.Symbol),\n            (r'[\\w!:?]+:(?![\\w!?])', String.Other),\n            (r':\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Symbol),\n\n            # Documentation\n            (r'((?<=fn\\()|(?<=fnx\\()|(?<=method\\()|(?<=macro\\()|(?<=lecro\\()'\n             r'|(?<=syntax\\()|(?<=dmacro\\()|(?<=dlecro\\()|(?<=dlecrox\\()'\n             r'|(?<=dsyntax\\())\\s*\"', String.Doc, 'documentation'),\n\n            # Text\n            (r'\"', String, 'text'),\n            (r'#\\[', String, 'squareText'),\n\n            # Mimic\n            (r'\\w[\\w!:?]+(?=\\s*=.*mimic\\s)', Name.Entity),\n\n            # Assignment\n            (r'[a-zA-Z_][\\w!:?]*(?=[\\s]*[+*/-]?=[^=].*($|\\.))',\n             Name.Variable),\n\n            # keywords\n            (r'(break|cond|continue|do|ensure|for|for:dict|for:set|if|let|'\n             r'loop|p:for|p:for:dict|p:for:set|return|unless|until|while|'\n             r'with)(?![\\w!:?])', Keyword.Reserved),\n\n            # Origin\n            (r'(eval|mimic|print|println)(?![\\w!:?])', Keyword),\n\n            # Base\n            (r'(cell\\?|cellNames|cellOwner\\?|cellOwner|cells|cell|'\n             r'documentation|hash|identity|mimic|removeCell\\!|undefineCell\\!)'\n             r'(?![\\w!:?])', Keyword),\n\n            # Ground\n            (r'(stackTraceAsText)(?![\\w!:?])', Keyword),\n\n            # DefaultBehaviour Literals\n            (r'(dict|list|message|set)(?![\\w!:?])', Keyword.Reserved),\n\n            # DefaultBehaviour Case\n            (r'(case|case:and|case:else|case:nand|case:nor|case:not|case:or|'\n             r'case:otherwise|case:xor)(?![\\w!:?])', Keyword.Reserved),\n\n            # DefaultBehaviour Reflection\n            (r'(asText|become\\!|derive|freeze\\!|frozen\\?|in\\?|is\\?|kind\\?|'\n             r'mimic\\!|mimics|mimics\\?|prependMimic\\!|removeAllMimics\\!|'\n             r'removeMimic\\!|same\\?|send|thaw\\!|uniqueHexId)'\n             r'(?![\\w!:?])', Keyword),\n\n            # DefaultBehaviour Aspects\n            (r'(after|around|before)(?![\\w!:?])', Keyword.Reserved),\n\n            # DefaultBehaviour\n            (r'(kind|cellDescriptionDict|cellSummary|genSym|inspect|notice)'\n             r'(?![\\w!:?])', Keyword),\n            (r'(use|destructuring)', Keyword.Reserved),\n\n            # DefaultBehavior BaseBehavior\n            (r'(cell\\?|cellOwner\\?|cellOwner|cellNames|cells|cell|'\n             r'documentation|identity|removeCell!|undefineCell)'\n             r'(?![\\w!:?])', Keyword),\n\n            # DefaultBehavior Internal\n            (r'(internal:compositeRegexp|internal:concatenateText|'\n             r'internal:createDecimal|internal:createNumber|'\n             r'internal:createRegexp|internal:createText)'\n             r'(?![\\w!:?])', Keyword.Reserved),\n\n            # DefaultBehaviour Conditions\n            (r'(availableRestarts|bind|error\\!|findRestart|handle|'\n             r'invokeRestart|rescue|restart|signal\\!|warn\\!)'\n             r'(?![\\w!:?])', Keyword.Reserved),\n\n            # constants\n            (r'(nil|false|true)(?![\\w!:?])', Name.Constant),\n\n            # names\n            (r'(Arity|Base|Call|Condition|DateTime|Aspects|Pointcut|'\n             r'Assignment|BaseBehavior|Boolean|Case|AndCombiner|Else|'\n             r'NAndCombiner|NOrCombiner|NotCombiner|OrCombiner|XOrCombiner|'\n             r'Conditions|Definitions|FlowControl|Internal|Literals|'\n             r'Reflection|DefaultMacro|DefaultMethod|DefaultSyntax|Dict|'\n             r'FileSystem|Ground|Handler|Hook|IO|IokeGround|Struct|'\n             r'LexicalBlock|LexicalMacro|List|Message|Method|Mixins|'\n             r'NativeMethod|Number|Origin|Pair|Range|Reflector|Regexp Match|'\n             r'Regexp|Rescue|Restart|Runtime|Sequence|Set|Symbol|'\n             r'System|Text|Tuple)(?![\\w!:?])', Name.Builtin),\n\n            # functions\n            ('(generateMatchMethod|aliasMethod|\\u03bb|\\u028E|fnx|fn|method|'\n             'dmacro|dlecro|syntax|macro|dlecrox|lecrox|lecro|syntax)'\n             '(?![\\\\w!:?])', Name.Function),\n\n            # Numbers\n            (r'-?0[xX][0-9a-fA-F]+', Number.Hex),\n            (r'-?(\\d+\\.?\\d*|\\d*\\.\\d+)([eE][+-]?[0-9]+)?', Number.Float),\n            (r'-?\\d+', Number.Integer),\n\n            (r'#\\(', Punctuation),\n\n            # Operators\n            (r'(&&>>|\\|\\|>>|\\*\\*>>|:::|::|\\.\\.\\.|===|\\*\\*>|\\*\\*=|&&>|&&=|'\n             r'\\|\\|>|\\|\\|=|\\->>|\\+>>|!>>|<>>>|<>>|&>>|%>>|#>>|@>>|/>>|\\*>>|'\n             r'\\?>>|\\|>>|\\^>>|~>>|\\$>>|=>>|<<=|>>=|<=>|<\\->|=~|!~|=>|\\+\\+|'\n             r'\\-\\-|<=|>=|==|!=|&&|\\.\\.|\\+=|\\-=|\\*=|\\/=|%=|&=|\\^=|\\|=|<\\-|'\n             r'\\+>|!>|<>|&>|%>|#>|\\@>|\\/>|\\*>|\\?>|\\|>|\\^>|~>|\\$>|<\\->|\\->|'\n             r'<<|>>|\\*\\*|\\?\\||\\?&|\\|\\||>|<|\\*|\\/|%|\\+|\\-|&|\\^|\\||=|\\$|!|~|'\n             r'\\?|#|\\u2260|\\u2218|\\u2208|\\u2209)', Operator),\n            (r'(and|nand|or|xor|nor|return|import)(?![\\w!?])',\n             Operator),\n\n            # Punctuation\n            (r'(\\`\\`|\\`|\\'\\'|\\'|\\.|\\,|@@|@|\\[|\\]|\\(|\\)|\\{|\\})', Punctuation),\n\n            # kinds\n            (r'[A-Z][\\w!:?]*', Name.Class),\n\n            # default cellnames\n            (r'[a-z_][\\w!:?]*', Name)\n        ]\n    }\n\n\nclass ClojureLexer(RegexLexer):\n    \"\"\"\n    Lexer for `Clojure <http://clojure.org/>`_ source code.\n\n    .. versionadded:: 0.11\n    \"\"\"\n    name = 'Clojure'\n    aliases = ['clojure', 'clj']\n    filenames = ['*.clj']\n    mimetypes = ['text/x-clojure', 'application/x-clojure']\n\n    special_forms = (\n        '.', 'def', 'do', 'fn', 'if', 'let', 'new', 'quote', 'var', 'loop'\n    )\n\n    # It's safe to consider 'ns' a declaration thing because it defines a new\n    # namespace.\n    declarations = (\n        'def-', 'defn', 'defn-', 'defmacro', 'defmulti', 'defmethod',\n        'defstruct', 'defonce', 'declare', 'definline', 'definterface',\n        'defprotocol', 'defrecord', 'deftype', 'defproject', 'ns'\n    )\n\n    builtins = (\n        '*', '+', '-', '->', '/', '<', '<=', '=', '==', '>', '>=', '..',\n        'accessor', 'agent', 'agent-errors', 'aget', 'alength', 'all-ns',\n        'alter', 'and', 'append-child', 'apply', 'array-map', 'aset',\n        'aset-boolean', 'aset-byte', 'aset-char', 'aset-double', 'aset-float',\n        'aset-int', 'aset-long', 'aset-short', 'assert', 'assoc', 'await',\n        'await-for', 'bean', 'binding', 'bit-and', 'bit-not', 'bit-or',\n        'bit-shift-left', 'bit-shift-right', 'bit-xor', 'boolean', 'branch?',\n        'butlast', 'byte', 'cast', 'char', 'children', 'class',\n        'clear-agent-errors', 'comment', 'commute', 'comp', 'comparator',\n        'complement', 'concat', 'conj', 'cons', 'constantly', 'cond', 'if-not',\n        'construct-proxy', 'contains?', 'count', 'create-ns', 'create-struct',\n        'cycle', 'dec',  'deref', 'difference', 'disj', 'dissoc', 'distinct',\n        'doall', 'doc', 'dorun', 'doseq', 'dosync', 'dotimes', 'doto',\n        'double', 'down', 'drop', 'drop-while', 'edit', 'end?', 'ensure',\n        'eval', 'every?', 'false?', 'ffirst', 'file-seq', 'filter', 'find',\n        'find-doc', 'find-ns', 'find-var', 'first', 'float', 'flush', 'for',\n        'fnseq', 'frest', 'gensym', 'get-proxy-class', 'get',\n        'hash-map', 'hash-set', 'identical?', 'identity', 'if-let', 'import',\n        'in-ns', 'inc', 'index', 'insert-child', 'insert-left', 'insert-right',\n        'inspect-table', 'inspect-tree', 'instance?', 'int', 'interleave',\n        'intersection', 'into', 'into-array', 'iterate', 'join', 'key', 'keys',\n        'keyword', 'keyword?', 'last', 'lazy-cat', 'lazy-cons', 'left',\n        'lefts', 'line-seq', 'list*', 'list', 'load', 'load-file',\n        'locking', 'long', 'loop', 'macroexpand', 'macroexpand-1',\n        'make-array', 'make-node', 'map', 'map-invert', 'map?', 'mapcat',\n        'max', 'max-key', 'memfn', 'merge', 'merge-with', 'meta', 'min',\n        'min-key', 'name', 'namespace', 'neg?', 'new', 'newline', 'next',\n        'nil?', 'node', 'not', 'not-any?', 'not-every?', 'not=', 'ns-imports',\n        'ns-interns', 'ns-map', 'ns-name', 'ns-publics', 'ns-refers',\n        'ns-resolve', 'ns-unmap', 'nth', 'nthrest', 'or', 'parse', 'partial',\n        'path', 'peek', 'pop', 'pos?', 'pr', 'pr-str', 'print', 'print-str',\n        'println', 'println-str', 'prn', 'prn-str', 'project', 'proxy',\n        'proxy-mappings', 'quot', 'rand', 'rand-int', 'range', 're-find',\n        're-groups', 're-matcher', 're-matches', 're-pattern', 're-seq',\n        'read', 'read-line', 'reduce', 'ref', 'ref-set', 'refer', 'rem',\n        'remove', 'remove-method', 'remove-ns', 'rename', 'rename-keys',\n        'repeat', 'replace', 'replicate', 'resolve', 'rest', 'resultset-seq',\n        'reverse', 'rfirst', 'right', 'rights', 'root', 'rrest', 'rseq',\n        'second', 'select', 'select-keys', 'send', 'send-off', 'seq',\n        'seq-zip', 'seq?', 'set', 'short', 'slurp', 'some', 'sort',\n        'sort-by', 'sorted-map', 'sorted-map-by', 'sorted-set',\n        'special-symbol?', 'split-at', 'split-with', 'str', 'string?',\n        'struct', 'struct-map', 'subs', 'subvec', 'symbol', 'symbol?',\n        'sync', 'take', 'take-nth', 'take-while', 'test', 'time', 'to-array',\n        'to-array-2d', 'tree-seq', 'true?', 'union', 'up', 'update-proxy',\n        'val', 'vals', 'var-get', 'var-set', 'var?', 'vector', 'vector-zip',\n        'vector?', 'when', 'when-first', 'when-let', 'when-not',\n        'with-local-vars', 'with-meta', 'with-open', 'with-out-str',\n        'xml-seq', 'xml-zip', 'zero?', 'zipmap', 'zipper')\n\n    # valid names for identifiers\n    # well, names can only not consist fully of numbers\n    # but this should be good enough for now\n\n    # TODO / should divide keywords/symbols into namespace/rest\n    # but that's hard, so just pretend / is part of the name\n    valid_name = r'(?!#)[\\w!$%*+<=>?/.#|-]+'\n\n    tokens = {\n        'root': [\n            # the comments - always starting with semicolon\n            # and going to the end of the line\n            (r';.*$', Comment.Single),\n\n            # whitespaces - usually not relevant\n            (r'[,\\s]+', Text),\n\n            # numbers\n            (r'-?\\d+\\.\\d+', Number.Float),\n            (r'-?\\d+', Number.Integer),\n            (r'0x-?[abcdef\\d]+', Number.Hex),\n\n            # strings, symbols and characters\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String),\n            (r\"'\" + valid_name, String.Symbol),\n            (r\"\\\\(.|[a-z]+)\", String.Char),\n\n            # keywords\n            (r'::?#?' + valid_name, String.Symbol),\n\n            # special operators\n            (r'~@|[`\\'#^~&@]', Operator),\n\n            # highlight the special forms\n            (words(special_forms, suffix=' '), Keyword),\n\n            # Technically, only the special forms are 'keywords'. The problem\n            # is that only treating them as keywords means that things like\n            # 'defn' and 'ns' need to be highlighted as builtins. This is ugly\n            # and weird for most styles. So, as a compromise we're going to\n            # highlight them as Keyword.Declarations.\n            (words(declarations, suffix=' '), Keyword.Declaration),\n\n            # highlight the builtins\n            (words(builtins, suffix=' '), Name.Builtin),\n\n            # the remaining functions\n            (r'(?<=\\()' + valid_name, Name.Function),\n\n            # find the remaining variables\n            (valid_name, Name.Variable),\n\n            # Clojure accepts vector notation\n            (r'(\\[|\\])', Punctuation),\n\n            # Clojure accepts map notation\n            (r'(\\{|\\})', Punctuation),\n\n            # the famous parentheses!\n            (r'(\\(|\\))', Punctuation),\n        ],\n    }\n\n\nclass ClojureScriptLexer(ClojureLexer):\n    \"\"\"\n    Lexer for `ClojureScript <http://clojure.org/clojurescript>`_\n    source code.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    name = 'ClojureScript'\n    aliases = ['clojurescript', 'cljs']\n    filenames = ['*.cljs']\n    mimetypes = ['text/x-clojurescript', 'application/x-clojurescript']\n\n\nclass TeaLangLexer(RegexLexer):\n    \"\"\"\n    For `Tea <http://teatrove.org/>`_ source code. Only used within a\n    TeaTemplateLexer.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w\\.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_]\\w*)'                       # method name\n             r'(\\s*)(\\()',                           # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w\\.]*', Name.Decorator),\n            (r'(and|break|else|foreach|if|in|not|or|reverse)\\b',\n             Keyword),\n            (r'(as|call|define)\\b', Keyword.Declaration),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(template)(\\s+)', bygroups(Keyword.Declaration, Text), 'template'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'(\\.)([a-zA-Z_]\\w*)', bygroups(Operator, Name.Attribute)),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_\\$]\\w*', Name),\n            (r'(isa|[.]{3}|[.]{2}|[=#!<>+-/%&;,.\\*\\\\\\(\\)\\[\\]\\{\\}])', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'template': [\n            (r'[a-zA-Z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n    }\n\n\nclass CeylonLexer(RegexLexer):\n    \"\"\"\n    For `Ceylon <http://ceylon-lang.org/>`_ source code.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'Ceylon'\n    aliases = ['ceylon']\n    filenames = ['*.ceylon']\n    mimetypes = ['text/x-ceylon']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    #: optional Comment or Whitespace\n    _ws = r'(?:\\s|//.*?\\n|/[*].*?[*]/)+'\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_]\\w*)'                      # method name\n             r'(\\s*)(\\()',                          # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*', Comment.Multiline, 'comment'),\n            (r'(shared|abstract|formal|default|actual|variable|deprecated|small|'\n             r'late|literal|doc|by|see|throws|optional|license|tagged|final|native|'\n             r'annotation|sealed)\\b', Name.Decorator),\n            (r'(break|case|catch|continue|else|finally|for|in|'\n             r'if|return|switch|this|throw|try|while|is|exists|dynamic|'\n             r'nonempty|then|outer|assert|let)\\b', Keyword),\n            (r'(abstracts|extends|satisfies|'\n             r'super|given|of|out|assign)\\b', Keyword.Declaration),\n            (r'(function|value|void|new)\\b',\n             Keyword.Type),\n            (r'(assembly|module|package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(class|interface|object|alias)(\\s+)',\n             bygroups(Keyword.Declaration, Text), 'class'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String),\n            (r\"'\\\\.'|'[^\\\\]'|'\\\\\\{#[0-9a-fA-F]{4}\\}'\", String.Char),\n            (r'(\\.)([a-z_]\\w*)',\n             bygroups(Operator, Name.Attribute)),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_]\\w*', Name),\n            (r'[~^*!%&\\[\\](){}<>|+=:;,./?-]', Operator),\n            (r'\\d{1,3}(_\\d{3})+\\.\\d{1,3}(_\\d{3})+[kMGTPmunpf]?', Number.Float),\n            (r'\\d{1,3}(_\\d{3})+\\.[0-9]+([eE][+-]?[0-9]+)?[kMGTPmunpf]?',\n             Number.Float),\n            (r'[0-9][0-9]*\\.\\d{1,3}(_\\d{3})+[kMGTPmunpf]?', Number.Float),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][+-]?[0-9]+)?[kMGTPmunpf]?',\n             Number.Float),\n            (r'#([0-9a-fA-F]{4})(_[0-9a-fA-F]{4})+', Number.Hex),\n            (r'#[0-9a-fA-F]+', Number.Hex),\n            (r'\\$([01]{4})(_[01]{4})+', Number.Bin),\n            (r'\\$[01]+', Number.Bin),\n            (r'\\d{1,3}(_\\d{3})+[kMGTP]?', Number.Integer),\n            (r'[0-9]+[kMGTP]?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'[A-Za-z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[a-z][\\w.]*',\n             Name.Namespace, '#pop')\n        ],\n        'comment': [\n            (r'[^*/]', Comment.Multiline),\n            (r'/\\*', Comment.Multiline, '#push'),\n            (r'\\*/', Comment.Multiline, '#pop'),\n            (r'[*/]', Comment.Multiline)\n        ],\n    }\n\n\nclass KotlinLexer(RegexLexer):\n    \"\"\"\n    For `Kotlin <http://kotlinlang.org/>`_\n    source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    name = 'Kotlin'\n    aliases = ['kotlin']\n    filenames = ['*.kt', '*.kts']\n    mimetypes = ['text/x-kotlin']\n\n    flags = re.MULTILINE | re.DOTALL | re.UNICODE\n\n    kt_name = ('@?[_' + uni.combine('Lu', 'Ll', 'Lt', 'Lm', 'Nl') + ']' +\n               '[' + uni.combine('Lu', 'Ll', 'Lt', 'Lm', 'Nl', 'Nd', 'Pc', 'Cf',\n                                 'Mn', 'Mc') + ']*')\n\n    kt_space_name = ('@?[_' + uni.combine('Lu', 'Ll', 'Lt', 'Lm', 'Nl') + ']' +\n               '[' + uni.combine('Lu', 'Ll', 'Lt', 'Lm', 'Nl', 'Nd', 'Pc', 'Cf',\n                                 'Mn', 'Mc', 'Zs') + ',-]*')\n\n    kt_id = '(' + kt_name + '|`' + kt_space_name + '`)'\n\n    tokens = {\n        'root': [\n            (r'^\\s*\\[.*?\\]', Name.Attribute),\n            (r'[^\\S\\n]+', Text),\n            (r'\\s+', Text),\n            (r'\\\\\\n', Text),  # line continuation\n            (r'//.*?\\n', Comment.Single),\n            (r'^#!/.+?\\n', Comment.Single),  # shebang for kotlin scripts\n            (r'/[*].*?[*]/', Comment.Multiline),\n            (r'\"\"\".*?\"\"\"', String),\n            (r'\\n', Text),\n            (r'::|!!|\\?[:.]', Operator),\n            (r'[~!%^&*()+=|\\[\\]:;,.<>/?-]', Punctuation),\n            (r'[{}]', Punctuation),\n            (r'@\"(\"\"|[^\"])*\"', String),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\\\n])*[\"\\n]', String),\n            (r\"'\\\\.'|'[^\\\\]'\", String.Char),\n            (r\"[0-9](\\.[0-9]*)?([eE][+-][0-9]+)?[flFL]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n            (r'(object)(\\s+)(:)(\\s+)', bygroups(Keyword, Text, Punctuation, Text), 'class'),\n            (r'(companion)(\\s+)(object)', bygroups(Keyword, Text, Keyword)),\n            (r'(class|interface|object)(\\s+)', bygroups(Keyword, Text), 'class'),\n            (r'(package|import)(\\s+)', bygroups(Keyword, Text), 'package'),\n            (r'(val|var)(\\s+)([(])', bygroups(Keyword, Text, Punctuation), 'property_dec'),\n            (r'(val|var)(\\s+)', bygroups(Keyword, Text), 'property'),\n            (r'(fun)(\\s+)', bygroups(Keyword, Text), 'function'),\n            (r'(inline fun)(\\s+)', bygroups(Keyword, Text), 'function'),\n            (r'(abstract|annotation|as|break|by|catch|class|companion|const|'\n             r'constructor|continue|crossinline|data|do|dynamic|else|enum|'\n             r'external|false|final|finally|for|fun|get|if|import|in|infix|'\n             r'inline|inner|interface|internal|is|lateinit|noinline|null|'\n             r'object|open|operator|out|override|package|private|protected|'\n             r'public|reified|return|sealed|set|super|tailrec|this|throw|'\n             r'true|try|val|var|vararg|when|where|while)\\b', Keyword),\n            (kt_id, Name),\n        ],\n        'package': [\n            (r'\\S+', Name.Namespace, '#pop')\n        ],\n        'class': [\n            (kt_id, Name.Class, '#pop')\n        ],\n        'property': [\n            (kt_id, Name.Property, '#pop')\n        ],\n        'property_dec': [\n            (r'(,)(\\s*)', bygroups(Punctuation, Text)),\n            (r'(:)(\\s*)', bygroups(Punctuation, Text)),\n            (r'<', Punctuation, 'generic'),\n            (r'([)])', Punctuation, '#pop'),\n            (kt_id, Name.Property)\n        ],\n        'function': [\n            (r'<', Punctuation, 'generic'),\n            (r''+kt_id+'([.])'+kt_id, bygroups(Name.Class, Punctuation, Name.Function), '#pop'),\n            (kt_id, Name.Function, '#pop')\n        ],\n        'generic': [\n            (r'(>)(\\s*)', bygroups(Punctuation, Text), '#pop'),\n            (r':',Punctuation),\n            (r'(reified|out|in)\\b', Keyword),\n            (r',',Text),\n            (r'\\s+',Text),\n            (kt_id,Name)\n        ]\n    }\n\n\nclass XtendLexer(RegexLexer):\n    \"\"\"\n    For `Xtend <http://xtend-lang.org/>`_ source code.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'Xtend'\n    aliases = ['xtend']\n    filenames = ['*.xtend']\n    mimetypes = ['text/x-xtend']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_$][\\w$]*)'                  # method name\n             r'(\\s*)(\\()',                          # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w.]*', Name.Decorator),\n            (r'(assert|break|case|catch|continue|default|do|else|finally|for|'\n             r'if|goto|instanceof|new|return|switch|this|throw|try|while|IF|'\n             r'ELSE|ELSEIF|ENDIF|FOR|ENDFOR|SEPARATOR|BEFORE|AFTER)\\b',\n             Keyword),\n            (r'(def|abstract|const|enum|extends|final|implements|native|private|'\n             r'protected|public|static|strictfp|super|synchronized|throws|'\n             r'transient|volatile)\\b', Keyword.Declaration),\n            (r'(boolean|byte|char|double|float|int|long|short|void)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(class|interface)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'class'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r\"(''')\", String, 'template'),\n            (r'(\\u00BB)', String, 'template'),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_$]\\w*', Name),\n            (r'[~^*!%&\\[\\](){}<>\\|+=:;,./?-]', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'[a-zA-Z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n        'template': [\n            (r\"'''\", String, '#pop'),\n            (r'\\u00AB', String, '#pop'),\n            (r'.', String)\n        ],\n    }\n\n\nclass PigLexer(RegexLexer):\n    \"\"\"\n    For `Pig Latin <https://pig.apache.org/>`_ source code.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'Pig'\n    aliases = ['pig']\n    filenames = ['*.pig']\n    mimetypes = ['text/x-pig']\n\n    flags = re.MULTILINE | re.IGNORECASE\n\n    tokens = {\n        'root': [\n            (r'\\s+', Text),\n            (r'--.*', Comment),\n            (r'/\\*[\\w\\W]*?\\*/', Comment.Multiline),\n            (r'\\\\\\n', Text),\n            (r'\\\\', Text),\n            (r'\\'(?:\\\\[ntbrf\\\\\\']|\\\\u[0-9a-f]{4}|[^\\'\\\\\\n\\r])*\\'', String),\n            include('keywords'),\n            include('types'),\n            include('builtins'),\n            include('punct'),\n            include('operators'),\n            (r'[0-9]*\\.[0-9]+(e[0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-f]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text),\n            (r'([a-z_]\\w*)(\\s*)(\\()',\n             bygroups(Name.Function, Text, Punctuation)),\n            (r'[()#:]', Text),\n            (r'[^(:#\\'\")\\s]+', Text),\n            (r'\\S+\\s+', Text)   # TODO: make tests pass without \\s+\n        ],\n        'keywords': [\n            (r'(assert|and|any|all|arrange|as|asc|bag|by|cache|CASE|cat|cd|cp|'\n             r'%declare|%default|define|dense|desc|describe|distinct|du|dump|'\n             r'eval|exex|explain|filter|flatten|foreach|full|generate|group|'\n             r'help|if|illustrate|import|inner|input|into|is|join|kill|left|'\n             r'limit|load|ls|map|matches|mkdir|mv|not|null|onschema|or|order|'\n             r'outer|output|parallel|pig|pwd|quit|register|returns|right|rm|'\n             r'rmf|rollup|run|sample|set|ship|split|stderr|stdin|stdout|store|'\n             r'stream|through|union|using|void)\\b', Keyword)\n        ],\n        'builtins': [\n            (r'(AVG|BinStorage|cogroup|CONCAT|copyFromLocal|copyToLocal|COUNT|'\n             r'cross|DIFF|MAX|MIN|PigDump|PigStorage|SIZE|SUM|TextLoader|'\n             r'TOKENIZE)\\b', Name.Builtin)\n        ],\n        'types': [\n            (r'(bytearray|BIGINTEGER|BIGDECIMAL|chararray|datetime|double|float|'\n             r'int|long|tuple)\\b', Keyword.Type)\n        ],\n        'punct': [\n            (r'[;(){}\\[\\]]', Punctuation),\n        ],\n        'operators': [\n            (r'[#=,./%+\\-?]', Operator),\n            (r'(eq|gt|lt|gte|lte|neq|matches)\\b', Operator),\n            (r'(==|<=|<|>=|>|!=)', Operator),\n        ],\n    }\n\n\nclass GoloLexer(RegexLexer):\n    \"\"\"\n    For `Golo <http://golo-lang.org/>`_ source code.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'Golo'\n    filenames = ['*.golo']\n    aliases = ['golo']\n\n    tokens = {\n        'root': [\n            (r'[^\\S\\n]+', Text),\n\n            (r'#.*$', Comment),\n\n            (r'(\\^|\\.\\.\\.|:|\\?:|->|==|!=|=|\\+|\\*|%|/|<=|<|>=|>|=|\\.)',\n                Operator),\n            (r'(?<=[^-])(-)(?=[^-])', Operator),\n\n            (r'(?<=[^`])(is|isnt|and|or|not|oftype|in|orIfNull)\\b', Operator.Word),\n            (r'[]{}|(),[]', Punctuation),\n\n            (r'(module|import)(\\s+)',\n                bygroups(Keyword.Namespace, Text),\n                'modname'),\n            (r'\\b([a-zA-Z_][\\w$.]*)(::)',  bygroups(Name.Namespace, Punctuation)),\n            (r'\\b([a-zA-Z_][\\w$]*(?:\\.[a-zA-Z_][\\w$]*)+)\\b', Name.Namespace),\n\n            (r'(let|var)(\\s+)',\n                bygroups(Keyword.Declaration, Text),\n                'varname'),\n            (r'(struct)(\\s+)',\n                bygroups(Keyword.Declaration, Text),\n                'structname'),\n            (r'(function)(\\s+)',\n                bygroups(Keyword.Declaration, Text),\n                'funcname'),\n\n            (r'(null|true|false)\\b', Keyword.Constant),\n            (r'(augment|pimp'\n             r'|if|else|case|match|return'\n             r'|case|when|then|otherwise'\n             r'|while|for|foreach'\n             r'|try|catch|finally|throw'\n             r'|local'\n             r'|continue|break)\\b', Keyword),\n\n            (r'(map|array|list|set|vector|tuple)(\\[)',\n                bygroups(Name.Builtin, Punctuation)),\n            (r'(print|println|readln|raise|fun'\n             r'|asInterfaceInstance)\\b', Name.Builtin),\n            (r'(`?[a-zA-Z_][\\w$]*)(\\()',\n                bygroups(Name.Function, Punctuation)),\n\n            (r'-?[\\d_]*\\.[\\d_]*([eE][+-]?\\d[\\d_]*)?F?', Number.Float),\n            (r'0[0-7]+j?', Number.Oct),\n            (r'0[xX][a-fA-F0-9]+', Number.Hex),\n            (r'-?\\d[\\d_]*L', Number.Integer.Long),\n            (r'-?\\d[\\d_]*', Number.Integer),\n\n            (r'`?[a-zA-Z_][\\w$]*', Name),\n            (r'@[a-zA-Z_][\\w$.]*', Name.Decorator),\n\n            (r'\"\"\"', String, combined('stringescape', 'triplestring')),\n            (r'\"', String, combined('stringescape', 'doublestring')),\n            (r\"'\", String, combined('stringescape', 'singlestring')),\n            (r'----((.|\\n)*?)----', String.Doc)\n\n        ],\n\n        'funcname': [\n            (r'`?[a-zA-Z_][\\w$]*', Name.Function, '#pop'),\n        ],\n        'modname': [\n            (r'[a-zA-Z_][\\w$.]*\\*?', Name.Namespace, '#pop')\n        ],\n        'structname': [\n            (r'`?[\\w.]+\\*?', Name.Class, '#pop')\n        ],\n        'varname': [\n            (r'`?[a-zA-Z_][\\w$]*', Name.Variable, '#pop'),\n        ],\n        'string': [\n            (r'[^\\\\\\'\"\\n]+', String),\n            (r'[\\'\"\\\\]', String)\n        ],\n        'stringescape': [\n            (r'\\\\([\\\\abfnrtv\"\\']|\\n|N\\{.*?\\}|u[a-fA-F0-9]{4}|'\n             r'U[a-fA-F0-9]{8}|x[a-fA-F0-9]{2}|[0-7]{1,3})', String.Escape)\n        ],\n        'triplestring': [\n            (r'\"\"\"', String, '#pop'),\n            include('string'),\n            (r'\\n', String),\n        ],\n        'doublestring': [\n            (r'\"', String.Double, '#pop'),\n            include('string'),\n        ],\n        'singlestring': [\n            (r\"'\", String, '#pop'),\n            include('string'),\n        ],\n        'operators': [\n            (r'[#=,./%+\\-?]', Operator),\n            (r'(eq|gt|lt|gte|lte|neq|matches)\\b', Operator),\n            (r'(==|<=|<|>=|>|!=)', Operator),\n        ],\n    }\n\n\nclass JasminLexer(RegexLexer):\n    \"\"\"\n    For `Jasmin <http://jasmin.sourceforge.net/>`_ assembly code.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'Jasmin'\n    aliases = ['jasmin', 'jasminxt']\n    filenames = ['*.j']\n\n    _whitespace = r' \\n\\t\\r'\n    _ws = r'(?:[%s]+)' % _whitespace\n    _separator = r'%s:=' % _whitespace\n    _break = r'(?=[%s]|$)' % _separator\n    _name = r'[^%s]+' % _separator\n    _unqualified_name = r'(?:[^%s.;\\[/]+)' % _separator\n\n    tokens = {\n        'default': [\n            (r'\\n', Text, '#pop'),\n            (r\"'\", String.Single, ('#pop', 'quote')),\n            (r'\"', String.Double, 'string'),\n            (r'=', Punctuation),\n            (r':', Punctuation, 'label'),\n            (_ws, Text),\n            (r';.*', Comment.Single),\n            (r'(\\$[-+])?0x-?[\\da-fA-F]+%s' % _break, Number.Hex),\n            (r'(\\$[-+]|\\+)?-?\\d+%s' % _break, Number.Integer),\n            (r'-?(\\d+\\.\\d*|\\.\\d+)([eE][-+]?\\d+)?[fFdD]?'\n             r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]*%s' % _break, Number.Float),\n            (r'\\$%s' % _name, Name.Variable),\n\n            # Directives\n            (r'\\.annotation%s' % _break, Keyword.Reserved, 'annotation'),\n            (r'(\\.attribute|\\.bytecode|\\.debug|\\.deprecated|\\.enclosing|'\n             r'\\.interface|\\.line|\\.signature|\\.source|\\.stack|\\.var|abstract|'\n             r'annotation|bridge|class|default|enum|field|final|fpstrict|'\n             r'interface|native|private|protected|public|signature|static|'\n             r'synchronized|synthetic|transient|varargs|volatile)%s' % _break,\n             Keyword.Reserved),\n            (r'\\.catch%s' % _break, Keyword.Reserved, 'caught-exception'),\n            (r'(\\.class|\\.implements|\\.inner|\\.super|inner|invisible|'\n             r'invisibleparam|outer|visible|visibleparam)%s' % _break,\n             Keyword.Reserved, 'class/convert-dots'),\n            (r'\\.field%s' % _break, Keyword.Reserved,\n             ('descriptor/convert-dots', 'field')),\n            (r'(\\.end|\\.limit|use)%s' % _break, Keyword.Reserved,\n             'no-verification'),\n            (r'\\.method%s' % _break, Keyword.Reserved, 'method'),\n            (r'\\.set%s' % _break, Keyword.Reserved, 'var'),\n            (r'\\.throws%s' % _break, Keyword.Reserved, 'exception'),\n            (r'(from|offset|to|using)%s' % _break, Keyword.Reserved, 'label'),\n            (r'is%s' % _break, Keyword.Reserved,\n             ('descriptor/convert-dots', 'var')),\n            (r'(locals|stack)%s' % _break, Keyword.Reserved, 'verification'),\n            (r'method%s' % _break, Keyword.Reserved, 'enclosing-method'),\n\n            # Instructions\n            (words((\n                'aaload', 'aastore', 'aconst_null', 'aload', 'aload_0', 'aload_1', 'aload_2',\n                'aload_3', 'aload_w', 'areturn', 'arraylength', 'astore', 'astore_0', 'astore_1',\n                'astore_2', 'astore_3', 'astore_w', 'athrow', 'baload', 'bastore', 'bipush',\n                'breakpoint', 'caload', 'castore', 'd2f', 'd2i', 'd2l', 'dadd', 'daload', 'dastore',\n                'dcmpg', 'dcmpl', 'dconst_0', 'dconst_1', 'ddiv', 'dload', 'dload_0', 'dload_1',\n                'dload_2', 'dload_3', 'dload_w', 'dmul', 'dneg', 'drem', 'dreturn', 'dstore', 'dstore_0',\n                'dstore_1', 'dstore_2', 'dstore_3', 'dstore_w', 'dsub', 'dup', 'dup2', 'dup2_x1',\n                'dup2_x2', 'dup_x1', 'dup_x2', 'f2d', 'f2i', 'f2l', 'fadd', 'faload', 'fastore', 'fcmpg',\n                'fcmpl', 'fconst_0', 'fconst_1', 'fconst_2', 'fdiv', 'fload', 'fload_0', 'fload_1',\n                'fload_2', 'fload_3', 'fload_w', 'fmul', 'fneg', 'frem', 'freturn', 'fstore', 'fstore_0',\n                'fstore_1', 'fstore_2', 'fstore_3', 'fstore_w', 'fsub', 'i2b', 'i2c', 'i2d', 'i2f', 'i2l',\n                'i2s', 'iadd', 'iaload', 'iand', 'iastore', 'iconst_0', 'iconst_1', 'iconst_2',\n                'iconst_3', 'iconst_4', 'iconst_5', 'iconst_m1', 'idiv', 'iinc', 'iinc_w', 'iload',\n                'iload_0', 'iload_1', 'iload_2', 'iload_3', 'iload_w', 'imul', 'ineg', 'int2byte',\n                'int2char', 'int2short', 'ior', 'irem', 'ireturn', 'ishl', 'ishr', 'istore', 'istore_0',\n                'istore_1', 'istore_2', 'istore_3', 'istore_w', 'isub', 'iushr', 'ixor', 'l2d', 'l2f',\n                'l2i', 'ladd', 'laload', 'land', 'lastore', 'lcmp', 'lconst_0', 'lconst_1', 'ldc2_w',\n                'ldiv', 'lload', 'lload_0', 'lload_1', 'lload_2', 'lload_3', 'lload_w', 'lmul', 'lneg',\n                'lookupswitch', 'lor', 'lrem', 'lreturn', 'lshl', 'lshr', 'lstore', 'lstore_0',\n                'lstore_1', 'lstore_2', 'lstore_3', 'lstore_w', 'lsub', 'lushr', 'lxor',\n                'monitorenter', 'monitorexit', 'nop', 'pop', 'pop2', 'ret', 'ret_w', 'return', 'saload',\n                'sastore', 'sipush', 'swap'), suffix=_break), Keyword.Reserved),\n            (r'(anewarray|checkcast|instanceof|ldc|ldc_w|new)%s' % _break,\n             Keyword.Reserved, 'class/no-dots'),\n            (r'invoke(dynamic|interface|nonvirtual|special|'\n             r'static|virtual)%s' % _break, Keyword.Reserved,\n             'invocation'),\n            (r'(getfield|putfield)%s' % _break, Keyword.Reserved,\n             ('descriptor/no-dots', 'field')),\n            (r'(getstatic|putstatic)%s' % _break, Keyword.Reserved,\n             ('descriptor/no-dots', 'static')),\n            (words((\n                'goto', 'goto_w', 'if_acmpeq', 'if_acmpne', 'if_icmpeq',\n                'if_icmpge', 'if_icmpgt', 'if_icmple', 'if_icmplt', 'if_icmpne',\n                'ifeq', 'ifge', 'ifgt', 'ifle', 'iflt', 'ifne', 'ifnonnull',\n                'ifnull', 'jsr', 'jsr_w'), suffix=_break),\n             Keyword.Reserved, 'label'),\n            (r'(multianewarray|newarray)%s' % _break, Keyword.Reserved,\n             'descriptor/convert-dots'),\n            (r'tableswitch%s' % _break, Keyword.Reserved, 'table')\n        ],\n        'quote': [\n            (r\"'\", String.Single, '#pop'),\n            (r'\\\\u[\\da-fA-F]{4}', String.Escape),\n            (r\"[^'\\\\]+\", String.Single)\n        ],\n        'string': [\n            (r'\"', String.Double, '#pop'),\n            (r'\\\\([nrtfb\"\\'\\\\]|u[\\da-fA-F]{4}|[0-3]?[0-7]{1,2})',\n             String.Escape),\n            (r'[^\"\\\\]+', String.Double)\n        ],\n        'root': [\n            (r'\\n+', Text),\n            (r\"'\", String.Single, 'quote'),\n            include('default'),\n            (r'(%s)([ \\t\\r]*)(:)' % _name,\n             bygroups(Name.Label, Text, Punctuation)),\n            (_name, String.Other)\n        ],\n        'annotation': [\n            (r'\\n', Text, ('#pop', 'annotation-body')),\n            (r'default%s' % _break, Keyword.Reserved,\n             ('#pop', 'annotation-default')),\n            include('default')\n        ],\n        'annotation-body': [\n            (r'\\n+', Text),\n            (r'\\.end%s' % _break, Keyword.Reserved, '#pop'),\n            include('default'),\n            (_name, String.Other, ('annotation-items', 'descriptor/no-dots'))\n        ],\n        'annotation-default': [\n            (r'\\n+', Text),\n            (r'\\.end%s' % _break, Keyword.Reserved, '#pop'),\n            include('default'),\n            default(('annotation-items', 'descriptor/no-dots'))\n        ],\n        'annotation-items': [\n            (r\"'\", String.Single, 'quote'),\n            include('default'),\n            (_name, String.Other)\n        ],\n        'caught-exception': [\n            (r'all%s' % _break, Keyword, '#pop'),\n            include('exception')\n        ],\n        'class/convert-dots': [\n            include('default'),\n            (r'(L)((?:%s[/.])*)(%s)(;)' % (_unqualified_name, _name),\n             bygroups(Keyword.Type, Name.Namespace, Name.Class, Punctuation),\n             '#pop'),\n            (r'((?:%s[/.])*)(%s)' % (_unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class), '#pop')\n        ],\n        'class/no-dots': [\n            include('default'),\n            (r'\\[+', Punctuation, ('#pop', 'descriptor/no-dots')),\n            (r'(L)((?:%s/)*)(%s)(;)' % (_unqualified_name, _name),\n             bygroups(Keyword.Type, Name.Namespace, Name.Class, Punctuation),\n             '#pop'),\n            (r'((?:%s/)*)(%s)' % (_unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class), '#pop')\n        ],\n        'descriptor/convert-dots': [\n            include('default'),\n            (r'\\[+', Punctuation),\n            (r'(L)((?:%s[/.])*)(%s?)(;)' % (_unqualified_name, _name),\n             bygroups(Keyword.Type, Name.Namespace, Name.Class, Punctuation),\n             '#pop'),\n            (r'[^%s\\[)L]+' % _separator, Keyword.Type, '#pop'),\n            default('#pop')\n        ],\n        'descriptor/no-dots': [\n            include('default'),\n            (r'\\[+', Punctuation),\n            (r'(L)((?:%s/)*)(%s)(;)' % (_unqualified_name, _name),\n             bygroups(Keyword.Type, Name.Namespace, Name.Class, Punctuation),\n             '#pop'),\n            (r'[^%s\\[)L]+' % _separator, Keyword.Type, '#pop'),\n            default('#pop')\n        ],\n        'descriptors/convert-dots': [\n            (r'\\)', Punctuation, '#pop'),\n            default('descriptor/convert-dots')\n        ],\n        'enclosing-method': [\n            (_ws, Text),\n            (r'(?=[^%s]*\\()' % _separator, Text, ('#pop', 'invocation')),\n            default(('#pop', 'class/convert-dots'))\n        ],\n        'exception': [\n            include('default'),\n            (r'((?:%s[/.])*)(%s)' % (_unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Exception), '#pop')\n        ],\n        'field': [\n            (r'static%s' % _break, Keyword.Reserved, ('#pop', 'static')),\n            include('default'),\n            (r'((?:%s[/.](?=[^%s]*[/.]))*)(%s[/.])?(%s)' %\n             (_unqualified_name, _separator, _unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class, Name.Variable.Instance),\n             '#pop')\n        ],\n        'invocation': [\n            include('default'),\n            (r'((?:%s[/.](?=[^%s(]*[/.]))*)(%s[/.])?(%s)(\\()' %\n             (_unqualified_name, _separator, _unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class, Name.Function, Punctuation),\n             ('#pop', 'descriptor/convert-dots', 'descriptors/convert-dots',\n              'descriptor/convert-dots'))\n        ],\n        'label': [\n            include('default'),\n            (_name, Name.Label, '#pop')\n        ],\n        'method': [\n            include('default'),\n            (r'(%s)(\\()' % _name, bygroups(Name.Function, Punctuation),\n             ('#pop', 'descriptor/convert-dots', 'descriptors/convert-dots',\n              'descriptor/convert-dots'))\n        ],\n        'no-verification': [\n            (r'(locals|method|stack)%s' % _break, Keyword.Reserved, '#pop'),\n            include('default')\n        ],\n        'static': [\n            include('default'),\n            (r'((?:%s[/.](?=[^%s]*[/.]))*)(%s[/.])?(%s)' %\n             (_unqualified_name, _separator, _unqualified_name, _name),\n             bygroups(Name.Namespace, Name.Class, Name.Variable.Class), '#pop')\n        ],\n        'table': [\n            (r'\\n+', Text),\n            (r'default%s' % _break, Keyword.Reserved, '#pop'),\n            include('default'),\n            (_name, Name.Label)\n        ],\n        'var': [\n            include('default'),\n            (_name, Name.Variable, '#pop')\n        ],\n        'verification': [\n            include('default'),\n            (r'(Double|Float|Integer|Long|Null|Top|UninitializedThis)%s' %\n             _break, Keyword, '#pop'),\n            (r'Object%s' % _break, Keyword, ('#pop', 'class/no-dots')),\n            (r'Uninitialized%s' % _break, Keyword, ('#pop', 'label'))\n        ]\n    }\n\n    def analyse_text(text):\n        score = 0\n        if re.search(r'^\\s*\\.class\\s', text, re.MULTILINE):\n            score += 0.5\n            if re.search(r'^\\s*[a-z]+_[a-z]+\\b', text, re.MULTILINE):\n                score += 0.3\n        if re.search(r'^\\s*\\.(attribute|bytecode|debug|deprecated|enclosing|'\n                     r'inner|interface|limit|set|signature|stack)\\b', text,\n                     re.MULTILINE):\n            score += 0.6\n        return score\n\n\nclass SarlLexer(RegexLexer):\n    \"\"\"\n    For `SARL <http://www.sarl.io>`_ source code.\n\n    .. versionadded:: 2.4\n    \"\"\"\n\n    name = 'SARL'\n    aliases = ['sarl']\n    filenames = ['*.sarl']\n    mimetypes = ['text/x-sarl']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            # method names\n            (r'^(\\s*(?:[a-zA-Z_][\\w.\\[\\]]*\\s+)+?)'  # return arguments\n             r'([a-zA-Z_$][\\w$]*)'                      # method name\n             r'(\\s*)(\\()',                             # signature start\n             bygroups(using(this), Name.Function, Text, Operator)),\n            (r'[^\\S\\n]+', Text),\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*.*?\\*/', Comment.Multiline),\n            (r'@[a-zA-Z_][\\w.]*', Name.Decorator),\n            (r'(as|break|case|catch|default|do|else|extends|extension|finally|'\n             r'fires|for|if|implements|instanceof|new|on|requires|return|super|'\n             r'switch|throw|throws|try|typeof|uses|while|with)\\b',\n             Keyword),\n            (r'(abstract|def|dispatch|final|native|override|private|protected|'\n             r'public|static|strictfp|synchronized|transient|val|var|volatile)\\b',\n             Keyword.Declaration),\n            (r'(boolean|byte|char|double|float|int|long|short|void)\\b',\n             Keyword.Type),\n            (r'(package)(\\s+)', bygroups(Keyword.Namespace, Text)),\n            (r'(false|it|null|occurrence|this|true|void)\\b', Keyword.Constant),\n            (r'(agent|annotation|artifact|behavior|capacity|class|enum|event|'\n             r'interface|skill|space)(\\s+)', bygroups(Keyword.Declaration, Text),\n             'class'),\n            (r'(import)(\\s+)', bygroups(Keyword.Namespace, Text), 'import'),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'[a-zA-Z_]\\w*:', Name.Label),\n            (r'[a-zA-Z_$]\\w*', Name),\n            (r'[~^*!%&\\[\\](){}<>\\|+=:;,./?-]', Operator),\n            (r'[0-9][0-9]*\\.[0-9]+([eE][0-9]+)?[fd]?', Number.Float),\n            (r'0x[0-9a-fA-F]+', Number.Hex),\n            (r'[0-9]+L?', Number.Integer),\n            (r'\\n', Text)\n        ],\n        'class': [\n            (r'[a-zA-Z_]\\w*', Name.Class, '#pop')\n        ],\n        'import': [\n            (r'[\\w.]+\\*?', Name.Namespace, '#pop')\n        ],\n    }\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.matlab\n    ~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for Matlab and related languages.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexer import Lexer, RegexLexer, bygroups, default, words, \\\n    do_insertions\nfrom pygments.token import Text, Comment, Operator, Keyword, Name, String, \\\n    Number, Punctuation, Generic, Whitespace\n\nfrom pygments.lexers import _scilab_builtins\n\n__all__ = ['MatlabLexer', 'MatlabSessionLexer', 'OctaveLexer', 'ScilabLexer']\n\n\nclass MatlabLexer(RegexLexer):\n    \"\"\"\n    For Matlab source code.\n\n    .. versionadded:: 0.10\n    \"\"\"\n    name = 'Matlab'\n    aliases = ['matlab']\n    filenames = ['*.m']\n    mimetypes = ['text/matlab']\n\n    #\n    # These lists are generated automatically.\n    # Run the following in bash shell:\n    #\n    # for f in elfun specfun elmat; do\n    #   echo -n \"$f = \"\n    #   matlab -nojvm -r \"help $f;exit;\" | perl -ne \\\n    #   'push(@c,$1) if /^    (\\w+)\\s+-/; END {print q{[\"}.join(q{\",\"},@c).qq{\"]\\n};}'\n    # done\n    #\n    # elfun: Elementary math functions\n    # specfun: Special Math functions\n    # elmat: Elementary matrices and matrix manipulation\n    #\n    # taken from Matlab version 9.4 (R2018a)\n    #\n    elfun = (\"sin\", \"sind\", \"sinh\", \"asin\", \"asind\", \"asinh\", \"cos\", \"cosd\", \"cosh\",\n             \"acos\", \"acosd\", \"acosh\", \"tan\", \"tand\", \"tanh\", \"atan\", \"atand\", \"atan2\",\n             \"atan2d\", \"atanh\", \"sec\", \"secd\", \"sech\", \"asec\", \"asecd\", \"asech\", \"csc\", \"cscd\",\n             \"csch\", \"acsc\", \"acscd\", \"acsch\", \"cot\", \"cotd\", \"coth\", \"acot\", \"acotd\",\n             \"acoth\", \"hypot\", \"deg2rad\", \"rad2deg\", \"exp\", \"expm1\", \"log\", \"log1p\", \"log10\", \"log2\", \"pow2\",\n             \"realpow\", \"reallog\", \"realsqrt\", \"sqrt\", \"nthroot\", \"nextpow2\", \"abs\",\n             \"angle\", \"complex\", \"conj\", \"imag\", \"real\", \"unwrap\", \"isreal\", \"cplxpair\",\n             \"fix\", \"floor\", \"ceil\", \"round\", \"mod\", \"rem\", \"sign\")\n    specfun = (\"airy\", \"besselj\", \"bessely\", \"besselh\", \"besseli\", \"besselk\", \"beta\",\n               \"betainc\", \"betaincinv\", \"betaln\", \"ellipj\", \"ellipke\", \"erf\", \"erfc\", \"erfcx\",\n               \"erfinv\", \"erfcinv\", \"expint\", \"gamma\", \"gammainc\", \"gammaincinv\", \"gammaln\", \"psi\", \"legendre\",\n               \"cross\", \"dot\", \"factor\", \"isprime\", \"primes\", \"gcd\", \"lcm\", \"rat\",\n               \"rats\", \"perms\", \"nchoosek\", \"factorial\", \"cart2sph\", \"cart2pol\",\n               \"pol2cart\", \"sph2cart\", \"hsv2rgb\", \"rgb2hsv\")\n    elmat = (\"zeros\", \"ones\", \"eye\", \"repmat\", \"repelem\", \"linspace\", \"logspace\",\n             \"freqspace\", \"meshgrid\", \"accumarray\", \"size\", \"length\", \"ndims\", \"numel\",\n             \"disp\", \"isempty\", \"isequal\", \"isequaln\", \"cat\", \"reshape\",\n             \"diag\", \"blkdiag\", \"tril\", \"triu\", \"fliplr\", \"flipud\", \"flip\", \"rot90\",\n             \"find\", \"end\", \"sub2ind\", \"ind2sub\", \"bsxfun\", \"ndgrid\", \"permute\",\n             \"ipermute\", \"shiftdim\", \"circshift\", \"squeeze\", \"isscalar\", \"isvector\",\n             \"isrow\", \"iscolumn\", \"ismatrix\", \"eps\", \"realmax\", \"realmin\", \"intmax\", \"intmin\", \"flintmax\", \"pi\", \"i\", \"inf\", \"nan\", \"isnan\",\n             \"isinf\", \"isfinite\", \"j\", \"true\", \"false\", \"compan\", \"gallery\", \"hadamard\", \"hankel\",\n             \"hilb\", \"invhilb\", \"magic\", \"pascal\", \"rosser\", \"toeplitz\", \"vander\",\n             \"wilkinson\")\n\n    _operators = r'-|==|~=|<=|>=|<|>|&&|&|~|\\|\\|?|\\.\\*|\\*|\\+|\\.\\^|\\.\\\\|\\./|/|\\\\'\n\n    tokens = {\n        'root': [\n            # line starting with '!' is sent as a system command.  not sure what\n            # label to use...\n            (r'^!.*', String.Other),\n            (r'%\\{\\s*\\n', Comment.Multiline, 'blockcomment'),\n            (r'%.*$', Comment),\n            (r'^\\s*function\\b', Keyword, 'deffunc'),\n\n            # from 'iskeyword' on version 9.4 (R2018a):\n            # Check that there is no preceding dot, as keywords are valid field\n            # names.\n            (words(('break', 'case', 'catch', 'classdef', 'continue', 'else',\n                    'elseif', 'end', 'for', 'function',\n                    'global', 'if', 'otherwise', 'parfor',\n                    'persistent', 'return', 'spmd', 'switch',\n                    'try', 'while'),\n                   prefix=r'(?<!\\.)', suffix=r'\\b'),\n             Keyword),\n\n            (\"(\" + \"|\".join(elfun + specfun + elmat) + r')\\b',  Name.Builtin),\n\n            # line continuation with following comment:\n            (r'(\\.\\.\\.)(.*)$', bygroups(Keyword, Comment)),\n\n            # command form:\n            # \"How MATLAB Recognizes Command Syntax\" specifies that an operator\n            # is recognized if it is either surrounded by spaces or by no\n            # spaces on both sides; only the former case matters for us.  (This\n            # allows distinguishing `cd ./foo` from `cd ./ foo`.)\n            (r'(?:^|(?<=;))(\\s*)(\\w+)(\\s+)(?!=|\\(|(?:%s)\\s+)' % _operators,\n             bygroups(Text, Name, Text), 'commandargs'),\n\n            # operators:\n            (_operators, Operator),\n\n            # numbers (must come before punctuation to handle `.5`; cannot use\n            # `\\b` due to e.g. `5. + .5`).\n            (r'(?<!\\w)((\\d+\\.\\d*)|(\\d*\\.\\d+))([eEf][+-]?\\d+)?(?!\\w)', Number.Float),\n            (r'\\b\\d+[eEf][+-]?[0-9]+\\b', Number.Float),\n            (r'\\b\\d+\\b', Number.Integer),\n\n            # punctuation:\n            (r'\\[|\\]|\\(|\\)|\\{|\\}|:|@|\\.|,', Punctuation),\n            (r'=|:|;', Punctuation),\n\n            # quote can be transpose, instead of string:\n            # (not great, but handles common cases...)\n            (r'(?<=[\\w)\\].])\\'+', Operator),\n\n            (r'\"(\"\"|[^\"])*\"', String),\n\n            (r'(?<![\\w)\\].])\\'', String, 'string'),\n            (r'[a-zA-Z_]\\w*', Name),\n            (r'.', Text),\n        ],\n        'blockcomment': [\n            (r'^\\s*%\\}', Comment.Multiline, '#pop'),\n            (r'^.*\\n', Comment.Multiline),\n            (r'.', Comment.Multiline),\n        ],\n        'deffunc': [\n            (r'(\\s*)(?:(\\S+)(\\s*)(=)(\\s*))?(.+)(\\()(.*)(\\))(\\s*)',\n             bygroups(Whitespace, Text, Whitespace, Punctuation,\n                      Whitespace, Name.Function, Punctuation, Text,\n                      Punctuation, Whitespace), '#pop'),\n            # function with no args\n            (r'(\\s*)([a-zA-Z_]\\w*)', bygroups(Text, Name.Function), '#pop'),\n        ],\n        'string': [\n            (r\"[^']*'\", String, '#pop'),\n        ],\n        'commandargs': [\n            # If an equal sign or other operator is encountered, this\n            # isn't a command. It might be a variable assignment or\n            # comparison operation with multiple spaces before the\n            # equal sign or operator\n            (r\"=\", Punctuation, '#pop'),\n            (_operators, Operator, '#pop'),\n            (r\"[ \\t]+\", Text),\n            (\"'[^']*'\", String),\n            (r\"[^';\\s]+\", String),\n            (\";\", Punctuation, '#pop'),\n            default('#pop'),\n        ]\n    }\n\n    def analyse_text(text):\n        # function declaration.\n        first_non_comment = next((line for line in text.splitlines()\n                                  if not re.match(r'^\\s*%', text)), '').strip()\n        if (first_non_comment.startswith('function')\n                and '{' not in first_non_comment):\n            return 1.\n        # comment\n        elif re.search(r'^\\s*%', text, re.M):\n            return 0.2\n        # system cmd\n        elif re.search(r'^!\\w+', text, re.M):\n            return 0.2\n\n\nline_re  = re.compile('.*?\\n')\n\n\nclass MatlabSessionLexer(Lexer):\n    \"\"\"\n    For Matlab sessions.  Modeled after PythonConsoleLexer.\n    Contributed by Ken Schutte <kschutte@csail.mit.edu>.\n\n    .. versionadded:: 0.10\n    \"\"\"\n    name = 'Matlab session'\n    aliases = ['matlabsession']\n\n    def get_tokens_unprocessed(self, text):\n        mlexer = MatlabLexer(**self.options)\n\n        curcode = ''\n        insertions = []\n        continuation = False\n\n        for match in line_re.finditer(text):\n            line = match.group()\n\n            if line.startswith('>> '):\n                insertions.append((len(curcode),\n                                   [(0, Generic.Prompt, line[:3])]))\n                curcode += line[3:]\n\n            elif line.startswith('>>'):\n                insertions.append((len(curcode),\n                                   [(0, Generic.Prompt, line[:2])]))\n                curcode += line[2:]\n\n            elif line.startswith('???'):\n\n                idx = len(curcode)\n\n                # without is showing error on same line as before...?\n                # line = \"\\n\" + line\n                token = (0, Generic.Traceback, line)\n                insertions.append((idx, [token]))\n            elif continuation:\n                # line_start is the length of the most recent prompt symbol\n                line_start = len(insertions[-1][-1][-1])\n                # Set leading spaces with the length of the prompt to be a generic prompt\n                # This keeps code aligned when prompts are removed, say with some Javascript\n                if line.startswith(' '*line_start):\n                    insertions.append((len(curcode),\n                                    [(0, Generic.Prompt, line[:line_start])]))\n                    curcode += line[line_start:]\n                else:\n                    curcode += line\n            else:\n                if curcode:\n                    yield from do_insertions(\n                        insertions, mlexer.get_tokens_unprocessed(curcode))\n                    curcode = ''\n                    insertions = []\n\n                yield match.start(), Generic.Output, line\n\n            # Does not allow continuation if a comment is included after the ellipses.\n            # Continues any line that ends with ..., even comments (lines that start with %)\n            if line.strip().endswith('...'):\n                continuation = True\n            else:\n                continuation = False\n\n        if curcode:  # or item:\n            yield from do_insertions(\n                insertions, mlexer.get_tokens_unprocessed(curcode))\n\n\nclass OctaveLexer(RegexLexer):\n    \"\"\"\n    For GNU Octave source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n    name = 'Octave'\n    aliases = ['octave']\n    filenames = ['*.m']\n    mimetypes = ['text/octave']\n\n    # These lists are generated automatically.\n    # Run the following in bash shell:\n    #\n    # First dump all of the Octave manual into a plain text file:\n    #\n    #   $ info octave --subnodes -o octave-manual\n    #\n    # Now grep through it:\n\n    # for i in \\\n    #     \"Built-in Function\" \"Command\" \"Function File\" \\\n    #     \"Loadable Function\" \"Mapping Function\";\n    # do\n    #     perl -e '@name = qw('\"$i\"');\n    #              print lc($name[0]),\"_kw = [\\n\"';\n    #\n    #     perl -n -e 'print \"\\\"$1\\\",\\n\" if /-- '\"$i\"': .* (\\w*) \\(/;' \\\n    #         octave-manual | sort | uniq ;\n    #     echo \"]\" ;\n    #     echo;\n    # done\n\n    # taken from Octave Mercurial changeset 8cc154f45e37 (30-jan-2011)\n\n    builtin_kw = (\n        \"addlistener\", \"addpath\", \"addproperty\", \"all\",\n        \"and\", \"any\", \"argnames\", \"argv\", \"assignin\",\n        \"atexit\", \"autoload\",\n        \"available_graphics_toolkits\", \"beep_on_error\",\n        \"bitand\", \"bitmax\", \"bitor\", \"bitshift\", \"bitxor\",\n        \"cat\", \"cell\", \"cellstr\", \"char\", \"class\", \"clc\",\n        \"columns\", \"command_line_path\",\n        \"completion_append_char\", \"completion_matches\",\n        \"complex\", \"confirm_recursive_rmdir\", \"cputime\",\n        \"crash_dumps_octave_core\", \"ctranspose\", \"cumprod\",\n        \"cumsum\", \"debug_on_error\", \"debug_on_interrupt\",\n        \"debug_on_warning\", \"default_save_options\",\n        \"dellistener\", \"diag\", \"diff\", \"disp\",\n        \"doc_cache_file\", \"do_string_escapes\", \"double\",\n        \"drawnow\", \"e\", \"echo_executing_commands\", \"eps\",\n        \"eq\", \"errno\", \"errno_list\", \"error\", \"eval\",\n        \"evalin\", \"exec\", \"exist\", \"exit\", \"eye\", \"false\",\n        \"fclear\", \"fclose\", \"fcntl\", \"fdisp\", \"feof\",\n        \"ferror\", \"feval\", \"fflush\", \"fgetl\", \"fgets\",\n        \"fieldnames\", \"file_in_loadpath\", \"file_in_path\",\n        \"filemarker\", \"filesep\", \"find_dir_in_path\",\n        \"fixed_point_format\", \"fnmatch\", \"fopen\", \"fork\",\n        \"formula\", \"fprintf\", \"fputs\", \"fread\", \"freport\",\n        \"frewind\", \"fscanf\", \"fseek\", \"fskipl\", \"ftell\",\n        \"functions\", \"fwrite\", \"ge\", \"genpath\", \"get\",\n        \"getegid\", \"getenv\", \"geteuid\", \"getgid\",\n        \"getpgrp\", \"getpid\", \"getppid\", \"getuid\", \"glob\",\n        \"gt\", \"gui_mode\", \"history_control\",\n        \"history_file\", \"history_size\",\n        \"history_timestamp_format_string\", \"home\",\n        \"horzcat\", \"hypot\", \"ifelse\",\n        \"ignore_function_time_stamp\", \"inferiorto\",\n        \"info_file\", \"info_program\", \"inline\", \"input\",\n        \"intmax\", \"intmin\", \"ipermute\",\n        \"is_absolute_filename\", \"isargout\", \"isbool\",\n        \"iscell\", \"iscellstr\", \"ischar\", \"iscomplex\",\n        \"isempty\", \"isfield\", \"isfloat\", \"isglobal\",\n        \"ishandle\", \"isieee\", \"isindex\", \"isinteger\",\n        \"islogical\", \"ismatrix\", \"ismethod\", \"isnull\",\n        \"isnumeric\", \"isobject\", \"isreal\",\n        \"is_rooted_relative_filename\", \"issorted\",\n        \"isstruct\", \"isvarname\", \"kbhit\", \"keyboard\",\n        \"kill\", \"lasterr\", \"lasterror\", \"lastwarn\",\n        \"ldivide\", \"le\", \"length\", \"link\", \"linspace\",\n        \"logical\", \"lstat\", \"lt\", \"make_absolute_filename\",\n        \"makeinfo_program\", \"max_recursion_depth\", \"merge\",\n        \"methods\", \"mfilename\", \"minus\", \"mislocked\",\n        \"mkdir\", \"mkfifo\", \"mkstemp\", \"mldivide\", \"mlock\",\n        \"mouse_wheel_zoom\", \"mpower\", \"mrdivide\", \"mtimes\",\n        \"munlock\", \"nargin\", \"nargout\",\n        \"native_float_format\", \"ndims\", \"ne\", \"nfields\",\n        \"nnz\", \"norm\", \"not\", \"numel\", \"nzmax\",\n        \"octave_config_info\", \"octave_core_file_limit\",\n        \"octave_core_file_name\",\n        \"octave_core_file_options\", \"ones\", \"or\",\n        \"output_max_field_width\", \"output_precision\",\n        \"page_output_immediately\", \"page_screen_output\",\n        \"path\", \"pathsep\", \"pause\", \"pclose\", \"permute\",\n        \"pi\", \"pipe\", \"plus\", \"popen\", \"power\",\n        \"print_empty_dimensions\", \"printf\",\n        \"print_struct_array_contents\", \"prod\",\n        \"program_invocation_name\", \"program_name\",\n        \"putenv\", \"puts\", \"pwd\", \"quit\", \"rats\", \"rdivide\",\n        \"readdir\", \"readlink\", \"read_readline_init_file\",\n        \"realmax\", \"realmin\", \"rehash\", \"rename\",\n        \"repelems\", \"re_read_readline_init_file\", \"reset\",\n        \"reshape\", \"resize\", \"restoredefaultpath\",\n        \"rethrow\", \"rmdir\", \"rmfield\", \"rmpath\", \"rows\",\n        \"save_header_format_string\", \"save_precision\",\n        \"saving_history\", \"scanf\", \"set\", \"setenv\",\n        \"shell_cmd\", \"sighup_dumps_octave_core\",\n        \"sigterm_dumps_octave_core\", \"silent_functions\",\n        \"single\", \"size\", \"size_equal\", \"sizemax\",\n        \"sizeof\", \"sleep\", \"source\", \"sparse_auto_mutate\",\n        \"split_long_rows\", \"sprintf\", \"squeeze\", \"sscanf\",\n        \"stat\", \"stderr\", \"stdin\", \"stdout\", \"strcmp\",\n        \"strcmpi\", \"string_fill_char\", \"strncmp\",\n        \"strncmpi\", \"struct\", \"struct_levels_to_print\",\n        \"strvcat\", \"subsasgn\", \"subsref\", \"sum\", \"sumsq\",\n        \"superiorto\", \"suppress_verbose_help_message\",\n        \"symlink\", \"system\", \"tic\", \"tilde_expand\",\n        \"times\", \"tmpfile\", \"tmpnam\", \"toc\", \"toupper\",\n        \"transpose\", \"true\", \"typeinfo\", \"umask\", \"uminus\",\n        \"uname\", \"undo_string_escapes\", \"unlink\", \"uplus\",\n        \"upper\", \"usage\", \"usleep\", \"vec\", \"vectorize\",\n        \"vertcat\", \"waitpid\", \"warning\", \"warranty\",\n        \"whos_line_format\", \"yes_or_no\", \"zeros\",\n        \"inf\", \"Inf\", \"nan\", \"NaN\")\n\n    command_kw = (\"close\", \"load\", \"who\", \"whos\")\n\n    function_kw = (\n        \"accumarray\", \"accumdim\", \"acosd\", \"acotd\",\n        \"acscd\", \"addtodate\", \"allchild\", \"ancestor\",\n        \"anova\", \"arch_fit\", \"arch_rnd\", \"arch_test\",\n        \"area\", \"arma_rnd\", \"arrayfun\", \"ascii\", \"asctime\",\n        \"asecd\", \"asind\", \"assert\", \"atand\",\n        \"autoreg_matrix\", \"autumn\", \"axes\", \"axis\", \"bar\",\n        \"barh\", \"bartlett\", \"bartlett_test\", \"beep\",\n        \"betacdf\", \"betainv\", \"betapdf\", \"betarnd\",\n        \"bicgstab\", \"bicubic\", \"binary\", \"binocdf\",\n        \"binoinv\", \"binopdf\", \"binornd\", \"bitcmp\",\n        \"bitget\", \"bitset\", \"blackman\", \"blanks\",\n        \"blkdiag\", \"bone\", \"box\", \"brighten\", \"calendar\",\n        \"cast\", \"cauchy_cdf\", \"cauchy_inv\", \"cauchy_pdf\",\n        \"cauchy_rnd\", \"caxis\", \"celldisp\", \"center\", \"cgs\",\n        \"chisquare_test_homogeneity\",\n        \"chisquare_test_independence\", \"circshift\", \"cla\",\n        \"clabel\", \"clf\", \"clock\", \"cloglog\", \"closereq\",\n        \"colon\", \"colorbar\", \"colormap\", \"colperm\",\n        \"comet\", \"common_size\", \"commutation_matrix\",\n        \"compan\", \"compare_versions\", \"compass\",\n        \"computer\", \"cond\", \"condest\", \"contour\",\n        \"contourc\", \"contourf\", \"contrast\", \"conv\",\n        \"convhull\", \"cool\", \"copper\", \"copyfile\", \"cor\",\n        \"corrcoef\", \"cor_test\", \"cosd\", \"cotd\", \"cov\",\n        \"cplxpair\", \"cross\", \"cscd\", \"cstrcat\", \"csvread\",\n        \"csvwrite\", \"ctime\", \"cumtrapz\", \"curl\", \"cut\",\n        \"cylinder\", \"date\", \"datenum\", \"datestr\",\n        \"datetick\", \"datevec\", \"dblquad\", \"deal\",\n        \"deblank\", \"deconv\", \"delaunay\", \"delaunayn\",\n        \"delete\", \"demo\", \"detrend\", \"diffpara\", \"diffuse\",\n        \"dir\", \"discrete_cdf\", \"discrete_inv\",\n        \"discrete_pdf\", \"discrete_rnd\", \"display\",\n        \"divergence\", \"dlmwrite\", \"dos\", \"dsearch\",\n        \"dsearchn\", \"duplication_matrix\", \"durbinlevinson\",\n        \"ellipsoid\", \"empirical_cdf\", \"empirical_inv\",\n        \"empirical_pdf\", \"empirical_rnd\", \"eomday\",\n        \"errorbar\", \"etime\", \"etreeplot\", \"example\",\n        \"expcdf\", \"expinv\", \"expm\", \"exppdf\", \"exprnd\",\n        \"ezcontour\", \"ezcontourf\", \"ezmesh\", \"ezmeshc\",\n        \"ezplot\", \"ezpolar\", \"ezsurf\", \"ezsurfc\", \"factor\",\n        \"factorial\", \"fail\", \"fcdf\", \"feather\", \"fftconv\",\n        \"fftfilt\", \"fftshift\", \"figure\", \"fileattrib\",\n        \"fileparts\", \"fill\", \"findall\", \"findobj\",\n        \"findstr\", \"finv\", \"flag\", \"flipdim\", \"fliplr\",\n        \"flipud\", \"fpdf\", \"fplot\", \"fractdiff\", \"freqz\",\n        \"freqz_plot\", \"frnd\", \"fsolve\",\n        \"f_test_regression\", \"ftp\", \"fullfile\", \"fzero\",\n        \"gamcdf\", \"gaminv\", \"gampdf\", \"gamrnd\", \"gca\",\n        \"gcbf\", \"gcbo\", \"gcf\", \"genvarname\", \"geocdf\",\n        \"geoinv\", \"geopdf\", \"geornd\", \"getfield\", \"ginput\",\n        \"glpk\", \"gls\", \"gplot\", \"gradient\",\n        \"graphics_toolkit\", \"gray\", \"grid\", \"griddata\",\n        \"griddatan\", \"gtext\", \"gunzip\", \"gzip\", \"hadamard\",\n        \"hamming\", \"hankel\", \"hanning\", \"hggroup\",\n        \"hidden\", \"hilb\", \"hist\", \"histc\", \"hold\", \"hot\",\n        \"hotelling_test\", \"housh\", \"hsv\", \"hurst\",\n        \"hygecdf\", \"hygeinv\", \"hygepdf\", \"hygernd\",\n        \"idivide\", \"ifftshift\", \"image\", \"imagesc\",\n        \"imfinfo\", \"imread\", \"imshow\", \"imwrite\", \"index\",\n        \"info\", \"inpolygon\", \"inputname\", \"interpft\",\n        \"interpn\", \"intersect\", \"invhilb\", \"iqr\", \"isa\",\n        \"isdefinite\", \"isdir\", \"is_duplicate_entry\",\n        \"isequal\", \"isequalwithequalnans\", \"isfigure\",\n        \"ishermitian\", \"ishghandle\", \"is_leap_year\",\n        \"isletter\", \"ismac\", \"ismember\", \"ispc\", \"isprime\",\n        \"isprop\", \"isscalar\", \"issquare\", \"isstrprop\",\n        \"issymmetric\", \"isunix\", \"is_valid_file_id\",\n        \"isvector\", \"jet\", \"kendall\",\n        \"kolmogorov_smirnov_cdf\",\n        \"kolmogorov_smirnov_test\", \"kruskal_wallis_test\",\n        \"krylov\", \"kurtosis\", \"laplace_cdf\", \"laplace_inv\",\n        \"laplace_pdf\", \"laplace_rnd\", \"legend\", \"legendre\",\n        \"license\", \"line\", \"linkprop\", \"list_primes\",\n        \"loadaudio\", \"loadobj\", \"logistic_cdf\",\n        \"logistic_inv\", \"logistic_pdf\", \"logistic_rnd\",\n        \"logit\", \"loglog\", \"loglogerr\", \"logm\", \"logncdf\",\n        \"logninv\", \"lognpdf\", \"lognrnd\", \"logspace\",\n        \"lookfor\", \"ls_command\", \"lsqnonneg\", \"magic\",\n        \"mahalanobis\", \"manova\", \"matlabroot\",\n        \"mcnemar_test\", \"mean\", \"meansq\", \"median\", \"menu\",\n        \"mesh\", \"meshc\", \"meshgrid\", \"meshz\", \"mexext\",\n        \"mget\", \"mkpp\", \"mode\", \"moment\", \"movefile\",\n        \"mpoles\", \"mput\", \"namelengthmax\", \"nargchk\",\n        \"nargoutchk\", \"nbincdf\", \"nbininv\", \"nbinpdf\",\n        \"nbinrnd\", \"nchoosek\", \"ndgrid\", \"newplot\", \"news\",\n        \"nonzeros\", \"normcdf\", \"normest\", \"norminv\",\n        \"normpdf\", \"normrnd\", \"now\", \"nthroot\", \"null\",\n        \"ocean\", \"ols\", \"onenormest\", \"optimget\",\n        \"optimset\", \"orderfields\", \"orient\", \"orth\",\n        \"pack\", \"pareto\", \"parseparams\", \"pascal\", \"patch\",\n        \"pathdef\", \"pcg\", \"pchip\", \"pcolor\", \"pcr\",\n        \"peaks\", \"periodogram\", \"perl\", \"perms\", \"pie\",\n        \"pink\", \"planerot\", \"playaudio\", \"plot\",\n        \"plotmatrix\", \"plotyy\", \"poisscdf\", \"poissinv\",\n        \"poisspdf\", \"poissrnd\", \"polar\", \"poly\",\n        \"polyaffine\", \"polyarea\", \"polyderiv\", \"polyfit\",\n        \"polygcd\", \"polyint\", \"polyout\", \"polyreduce\",\n        \"polyval\", \"polyvalm\", \"postpad\", \"powerset\",\n        \"ppder\", \"ppint\", \"ppjumps\", \"ppplot\", \"ppval\",\n        \"pqpnonneg\", \"prepad\", \"primes\", \"print\",\n        \"print_usage\", \"prism\", \"probit\", \"qp\", \"qqplot\",\n        \"quadcc\", \"quadgk\", \"quadl\", \"quadv\", \"quiver\",\n        \"qzhess\", \"rainbow\", \"randi\", \"range\", \"rank\",\n        \"ranks\", \"rat\", \"reallog\", \"realpow\", \"realsqrt\",\n        \"record\", \"rectangle_lw\", \"rectangle_sw\",\n        \"rectint\", \"refresh\", \"refreshdata\",\n        \"regexptranslate\", \"repmat\", \"residue\", \"ribbon\",\n        \"rindex\", \"roots\", \"rose\", \"rosser\", \"rotdim\",\n        \"rref\", \"run\", \"run_count\", \"rundemos\", \"run_test\",\n        \"runtests\", \"saveas\", \"saveaudio\", \"saveobj\",\n        \"savepath\", \"scatter\", \"secd\", \"semilogx\",\n        \"semilogxerr\", \"semilogy\", \"semilogyerr\",\n        \"setaudio\", \"setdiff\", \"setfield\", \"setxor\",\n        \"shading\", \"shift\", \"shiftdim\", \"sign_test\",\n        \"sinc\", \"sind\", \"sinetone\", \"sinewave\", \"skewness\",\n        \"slice\", \"sombrero\", \"sortrows\", \"spaugment\",\n        \"spconvert\", \"spdiags\", \"spearman\", \"spectral_adf\",\n        \"spectral_xdf\", \"specular\", \"speed\", \"spencer\",\n        \"speye\", \"spfun\", \"sphere\", \"spinmap\", \"spline\",\n        \"spones\", \"sprand\", \"sprandn\", \"sprandsym\",\n        \"spring\", \"spstats\", \"spy\", \"sqp\", \"stairs\",\n        \"statistics\", \"std\", \"stdnormal_cdf\",\n        \"stdnormal_inv\", \"stdnormal_pdf\", \"stdnormal_rnd\",\n        \"stem\", \"stft\", \"strcat\", \"strchr\", \"strjust\",\n        \"strmatch\", \"strread\", \"strsplit\", \"strtok\",\n        \"strtrim\", \"strtrunc\", \"structfun\", \"studentize\",\n        \"subplot\", \"subsindex\", \"subspace\", \"substr\",\n        \"substruct\", \"summer\", \"surf\", \"surface\", \"surfc\",\n        \"surfl\", \"surfnorm\", \"svds\", \"swapbytes\",\n        \"sylvester_matrix\", \"symvar\", \"synthesis\", \"table\",\n        \"tand\", \"tar\", \"tcdf\", \"tempdir\", \"tempname\",\n        \"test\", \"text\", \"textread\", \"textscan\", \"tinv\",\n        \"title\", \"toeplitz\", \"tpdf\", \"trace\", \"trapz\",\n        \"treelayout\", \"treeplot\", \"triangle_lw\",\n        \"triangle_sw\", \"tril\", \"trimesh\", \"triplequad\",\n        \"triplot\", \"trisurf\", \"triu\", \"trnd\", \"tsearchn\",\n        \"t_test\", \"t_test_regression\", \"type\", \"unidcdf\",\n        \"unidinv\", \"unidpdf\", \"unidrnd\", \"unifcdf\",\n        \"unifinv\", \"unifpdf\", \"unifrnd\", \"union\", \"unique\",\n        \"unix\", \"unmkpp\", \"unpack\", \"untabify\", \"untar\",\n        \"unwrap\", \"unzip\", \"u_test\", \"validatestring\",\n        \"vander\", \"var\", \"var_test\", \"vech\", \"ver\",\n        \"version\", \"view\", \"voronoi\", \"voronoin\",\n        \"waitforbuttonpress\", \"wavread\", \"wavwrite\",\n        \"wblcdf\", \"wblinv\", \"wblpdf\", \"wblrnd\", \"weekday\",\n        \"welch_test\", \"what\", \"white\", \"whitebg\",\n        \"wienrnd\", \"wilcoxon_test\", \"wilkinson\", \"winter\",\n        \"xlabel\", \"xlim\", \"ylabel\", \"yulewalker\", \"zip\",\n        \"zlabel\", \"z_test\")\n\n    loadable_kw = (\n        \"airy\", \"amd\", \"balance\", \"besselh\", \"besseli\",\n        \"besselj\", \"besselk\", \"bessely\", \"bitpack\",\n        \"bsxfun\", \"builtin\", \"ccolamd\", \"cellfun\",\n        \"cellslices\", \"chol\", \"choldelete\", \"cholinsert\",\n        \"cholinv\", \"cholshift\", \"cholupdate\", \"colamd\",\n        \"colloc\", \"convhulln\", \"convn\", \"csymamd\",\n        \"cummax\", \"cummin\", \"daspk\", \"daspk_options\",\n        \"dasrt\", \"dasrt_options\", \"dassl\", \"dassl_options\",\n        \"dbclear\", \"dbdown\", \"dbstack\", \"dbstatus\",\n        \"dbstop\", \"dbtype\", \"dbup\", \"dbwhere\", \"det\",\n        \"dlmread\", \"dmperm\", \"dot\", \"eig\", \"eigs\",\n        \"endgrent\", \"endpwent\", \"etree\", \"fft\", \"fftn\",\n        \"fftw\", \"filter\", \"find\", \"full\", \"gcd\",\n        \"getgrent\", \"getgrgid\", \"getgrnam\", \"getpwent\",\n        \"getpwnam\", \"getpwuid\", \"getrusage\", \"givens\",\n        \"gmtime\", \"gnuplot_binary\", \"hess\", \"ifft\",\n        \"ifftn\", \"inv\", \"isdebugmode\", \"issparse\", \"kron\",\n        \"localtime\", \"lookup\", \"lsode\", \"lsode_options\",\n        \"lu\", \"luinc\", \"luupdate\", \"matrix_type\", \"max\",\n        \"min\", \"mktime\", \"pinv\", \"qr\", \"qrdelete\",\n        \"qrinsert\", \"qrshift\", \"qrupdate\", \"quad\",\n        \"quad_options\", \"qz\", \"rand\", \"rande\", \"randg\",\n        \"randn\", \"randp\", \"randperm\", \"rcond\", \"regexp\",\n        \"regexpi\", \"regexprep\", \"schur\", \"setgrent\",\n        \"setpwent\", \"sort\", \"spalloc\", \"sparse\", \"spparms\",\n        \"sprank\", \"sqrtm\", \"strfind\", \"strftime\",\n        \"strptime\", \"strrep\", \"svd\", \"svd_driver\", \"syl\",\n        \"symamd\", \"symbfact\", \"symrcm\", \"time\", \"tsearch\",\n        \"typecast\", \"urlread\", \"urlwrite\")\n\n    mapping_kw = (\n        \"abs\", \"acos\", \"acosh\", \"acot\", \"acoth\", \"acsc\",\n        \"acsch\", \"angle\", \"arg\", \"asec\", \"asech\", \"asin\",\n        \"asinh\", \"atan\", \"atanh\", \"beta\", \"betainc\",\n        \"betaln\", \"bincoeff\", \"cbrt\", \"ceil\", \"conj\", \"cos\",\n        \"cosh\", \"cot\", \"coth\", \"csc\", \"csch\", \"erf\", \"erfc\",\n        \"erfcx\", \"erfinv\", \"exp\", \"finite\", \"fix\", \"floor\",\n        \"fmod\", \"gamma\", \"gammainc\", \"gammaln\", \"imag\",\n        \"isalnum\", \"isalpha\", \"isascii\", \"iscntrl\",\n        \"isdigit\", \"isfinite\", \"isgraph\", \"isinf\",\n        \"islower\", \"isna\", \"isnan\", \"isprint\", \"ispunct\",\n        \"isspace\", \"isupper\", \"isxdigit\", \"lcm\", \"lgamma\",\n        \"log\", \"lower\", \"mod\", \"real\", \"rem\", \"round\",\n        \"roundb\", \"sec\", \"sech\", \"sign\", \"sin\", \"sinh\",\n        \"sqrt\", \"tan\", \"tanh\", \"toascii\", \"tolower\", \"xor\")\n\n    builtin_consts = (\n        \"EDITOR\", \"EXEC_PATH\", \"I\", \"IMAGE_PATH\", \"NA\",\n        \"OCTAVE_HOME\", \"OCTAVE_VERSION\", \"PAGER\",\n        \"PAGER_FLAGS\", \"SEEK_CUR\", \"SEEK_END\", \"SEEK_SET\",\n        \"SIG\", \"S_ISBLK\", \"S_ISCHR\", \"S_ISDIR\", \"S_ISFIFO\",\n        \"S_ISLNK\", \"S_ISREG\", \"S_ISSOCK\", \"WCONTINUE\",\n        \"WCOREDUMP\", \"WEXITSTATUS\", \"WIFCONTINUED\",\n        \"WIFEXITED\", \"WIFSIGNALED\", \"WIFSTOPPED\", \"WNOHANG\",\n        \"WSTOPSIG\", \"WTERMSIG\", \"WUNTRACED\")\n\n    tokens = {\n        'root': [\n            # We should look into multiline comments\n            (r'[%#].*$', Comment),\n            (r'^\\s*function\\b', Keyword, 'deffunc'),\n\n            # from 'iskeyword' on hg changeset 8cc154f45e37\n            (words((\n                '__FILE__', '__LINE__', 'break', 'case', 'catch', 'classdef', 'continue', 'do', 'else',\n                'elseif', 'end', 'end_try_catch', 'end_unwind_protect', 'endclassdef',\n                'endevents', 'endfor', 'endfunction', 'endif', 'endmethods', 'endproperties',\n                'endswitch', 'endwhile', 'events', 'for', 'function', 'get', 'global', 'if', 'methods',\n                'otherwise', 'persistent', 'properties', 'return', 'set', 'static', 'switch', 'try',\n                'until', 'unwind_protect', 'unwind_protect_cleanup', 'while'), suffix=r'\\b'),\n             Keyword),\n\n            (words(builtin_kw + command_kw + function_kw + loadable_kw + mapping_kw,\n                   suffix=r'\\b'),  Name.Builtin),\n\n            (words(builtin_consts, suffix=r'\\b'), Name.Constant),\n\n            # operators in Octave but not Matlab:\n            (r'-=|!=|!|/=|--', Operator),\n            # operators:\n            (r'-|==|~=|<|>|<=|>=|&&|&|~|\\|\\|?', Operator),\n            # operators in Octave but not Matlab requiring escape for re:\n            (r'\\*=|\\+=|\\^=|\\/=|\\\\=|\\*\\*|\\+\\+|\\.\\*\\*', Operator),\n            # operators requiring escape for re:\n            (r'\\.\\*|\\*|\\+|\\.\\^|\\.\\\\|\\.\\/|\\/|\\\\', Operator),\n\n\n            # punctuation:\n            (r'[\\[\\](){}:@.,]', Punctuation),\n            (r'=|:|;', Punctuation),\n\n            (r'\"[^\"]*\"', String),\n\n            (r'(\\d+\\.\\d*|\\d*\\.\\d+)([eEf][+-]?[0-9]+)?', Number.Float),\n            (r'\\d+[eEf][+-]?[0-9]+', Number.Float),\n            (r'\\d+', Number.Integer),\n\n            # quote can be transpose, instead of string:\n            # (not great, but handles common cases...)\n            (r'(?<=[\\w)\\].])\\'+', Operator),\n            (r'(?<![\\w)\\].])\\'', String, 'string'),\n\n            (r'[a-zA-Z_]\\w*', Name),\n            (r'.', Text),\n        ],\n        'string': [\n            (r\"[^']*'\", String, '#pop'),\n        ],\n        'deffunc': [\n            (r'(\\s*)(?:(\\S+)(\\s*)(=)(\\s*))?(.+)(\\()(.*)(\\))(\\s*)',\n             bygroups(Whitespace, Text, Whitespace, Punctuation,\n                      Whitespace, Name.Function, Punctuation, Text,\n                      Punctuation, Whitespace), '#pop'),\n            # function with no args\n            (r'(\\s*)([a-zA-Z_]\\w*)', bygroups(Text, Name.Function), '#pop'),\n        ],\n    }\n\n    def analyse_text(text):\n        \"\"\"Octave is quite hard to spot, and it looks like Matlab as well.\"\"\"\n        return 0\n\n\nclass ScilabLexer(RegexLexer):\n    \"\"\"\n    For Scilab source code.\n\n    .. versionadded:: 1.5\n    \"\"\"\n    name = 'Scilab'\n    aliases = ['scilab']\n    filenames = ['*.sci', '*.sce', '*.tst']\n    mimetypes = ['text/scilab']\n\n    tokens = {\n        'root': [\n            (r'//.*?$', Comment.Single),\n            (r'^\\s*function\\b', Keyword, 'deffunc'),\n\n            (words((\n                '__FILE__', '__LINE__', 'break', 'case', 'catch', 'classdef', 'continue', 'do', 'else',\n                'elseif', 'end', 'end_try_catch', 'end_unwind_protect', 'endclassdef',\n                'endevents', 'endfor', 'endfunction', 'endif', 'endmethods', 'endproperties',\n                'endswitch', 'endwhile', 'events', 'for', 'function', 'get', 'global', 'if', 'methods',\n                'otherwise', 'persistent', 'properties', 'return', 'set', 'static', 'switch', 'try',\n                'until', 'unwind_protect', 'unwind_protect_cleanup', 'while'), suffix=r'\\b'),\n             Keyword),\n\n            (words(_scilab_builtins.functions_kw +\n                   _scilab_builtins.commands_kw +\n                   _scilab_builtins.macros_kw, suffix=r'\\b'), Name.Builtin),\n\n            (words(_scilab_builtins.variables_kw, suffix=r'\\b'), Name.Constant),\n\n            # operators:\n            (r'-|==|~=|<|>|<=|>=|&&|&|~|\\|\\|?', Operator),\n            # operators requiring escape for re:\n            (r'\\.\\*|\\*|\\+|\\.\\^|\\.\\\\|\\.\\/|\\/|\\\\', Operator),\n\n            # punctuation:\n            (r'[\\[\\](){}@.,=:;]', Punctuation),\n\n            (r'\"[^\"]*\"', String),\n\n            # quote can be transpose, instead of string:\n            # (not great, but handles common cases...)\n            (r'(?<=[\\w)\\].])\\'+', Operator),\n            (r'(?<![\\w)\\].])\\'', String, 'string'),\n\n            (r'(\\d+\\.\\d*|\\d*\\.\\d+)([eEf][+-]?[0-9]+)?', Number.Float),\n            (r'\\d+[eEf][+-]?[0-9]+', Number.Float),\n            (r'\\d+', Number.Integer),\n\n            (r'[a-zA-Z_]\\w*', Name),\n            (r'.', Text),\n        ],\n        'string': [\n            (r\"[^']*'\", String, '#pop'),\n            (r'.', String, '#pop'),\n        ],\n        'deffunc': [\n            (r'(\\s*)(?:(\\S+)(\\s*)(=)(\\s*))?(.+)(\\()(.*)(\\))(\\s*)',\n             bygroups(Whitespace, Text, Whitespace, Punctuation,\n                      Whitespace, Name.Function, Punctuation, Text,\n                      Punctuation, Whitespace), '#pop'),\n            # function with no args\n            (r'(\\s*)([a-zA-Z_]\\w*)', bygroups(Text, Name.Function), '#pop'),\n        ],\n    }\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.objective\n    ~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for Objective-C family languages.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexer import RegexLexer, include, bygroups, using, this, words, \\\n    inherit, default\nfrom pygments.token import Text, Keyword, Name, String, Operator, \\\n    Number, Punctuation, Literal, Comment\n\nfrom pygments.lexers.c_cpp import CLexer, CppLexer\n\n__all__ = ['ObjectiveCLexer', 'ObjectiveCppLexer', 'LogosLexer', 'SwiftLexer']\n\n\ndef objective(baselexer):\n    \"\"\"\n    Generate a subclass of baselexer that accepts the Objective-C syntax\n    extensions.\n    \"\"\"\n\n    # Have to be careful not to accidentally match JavaDoc/Doxygen syntax here,\n    # since that's quite common in ordinary C/C++ files.  It's OK to match\n    # JavaDoc/Doxygen keywords that only apply to Objective-C, mind.\n    #\n    # The upshot of this is that we CANNOT match @class or @interface\n    _oc_keywords = re.compile(r'@(?:end|implementation|protocol)')\n\n    # Matches [ <ws>? identifier <ws> ( identifier <ws>? ] |  identifier? : )\n    # (note the identifier is *optional* when there is a ':'!)\n    _oc_message = re.compile(r'\\[\\s*[a-zA-Z_]\\w*\\s+'\n                             r'(?:[a-zA-Z_]\\w*\\s*\\]|'\n                             r'(?:[a-zA-Z_]\\w*)?:)')\n\n    class GeneratedObjectiveCVariant(baselexer):\n        \"\"\"\n        Implements Objective-C syntax on top of an existing C family lexer.\n        \"\"\"\n\n        tokens = {\n            'statements': [\n                (r'@\"', String, 'string'),\n                (r'@(YES|NO)', Number),\n                (r\"@'(\\\\.|\\\\[0-7]{1,3}|\\\\x[a-fA-F0-9]{1,2}|[^\\\\\\'\\n])'\", String.Char),\n                (r'@(\\d+\\.\\d*|\\.\\d+|\\d+)[eE][+-]?\\d+[lL]?', Number.Float),\n                (r'@(\\d+\\.\\d*|\\.\\d+|\\d+[fF])[fF]?', Number.Float),\n                (r'@0x[0-9a-fA-F]+[Ll]?', Number.Hex),\n                (r'@0[0-7]+[Ll]?', Number.Oct),\n                (r'@\\d+[Ll]?', Number.Integer),\n                (r'@\\(', Literal, 'literal_number'),\n                (r'@\\[', Literal, 'literal_array'),\n                (r'@\\{', Literal, 'literal_dictionary'),\n                (words((\n                    '@selector', '@private', '@protected', '@public', '@encode',\n                    '@synchronized', '@try', '@throw', '@catch', '@finally',\n                    '@end', '@property', '@synthesize', '__bridge', '__bridge_transfer',\n                    '__autoreleasing', '__block', '__weak', '__strong', 'weak', 'strong',\n                    'copy', 'retain', 'assign', 'unsafe_unretained', 'atomic', 'nonatomic',\n                    'readonly', 'readwrite', 'setter', 'getter', 'typeof', 'in',\n                    'out', 'inout', 'release', 'class', '@dynamic', '@optional',\n                    '@required', '@autoreleasepool', '@import'), suffix=r'\\b'),\n                 Keyword),\n                (words(('id', 'instancetype', 'Class', 'IMP', 'SEL', 'BOOL',\n                        'IBOutlet', 'IBAction', 'unichar'), suffix=r'\\b'),\n                 Keyword.Type),\n                (r'@(true|false|YES|NO)\\n', Name.Builtin),\n                (r'(YES|NO|nil|self|super)\\b', Name.Builtin),\n                # Carbon types\n                (r'(Boolean|UInt8|SInt8|UInt16|SInt16|UInt32|SInt32)\\b', Keyword.Type),\n                # Carbon built-ins\n                (r'(TRUE|FALSE)\\b', Name.Builtin),\n                (r'(@interface|@implementation)(\\s+)', bygroups(Keyword, Text),\n                 ('#pop', 'oc_classname')),\n                (r'(@class|@protocol)(\\s+)', bygroups(Keyword, Text),\n                 ('#pop', 'oc_forward_classname')),\n                # @ can also prefix other expressions like @{...} or @(...)\n                (r'@', Punctuation),\n                inherit,\n            ],\n            'oc_classname': [\n                # interface definition that inherits\n                (r'([a-zA-Z$_][\\w$]*)(\\s*:\\s*)([a-zA-Z$_][\\w$]*)?(\\s*)(\\{)',\n                 bygroups(Name.Class, Text, Name.Class, Text, Punctuation),\n                 ('#pop', 'oc_ivars')),\n                (r'([a-zA-Z$_][\\w$]*)(\\s*:\\s*)([a-zA-Z$_][\\w$]*)?',\n                 bygroups(Name.Class, Text, Name.Class), '#pop'),\n                # interface definition for a category\n                (r'([a-zA-Z$_][\\w$]*)(\\s*)(\\([a-zA-Z$_][\\w$]*\\))(\\s*)(\\{)',\n                 bygroups(Name.Class, Text, Name.Label, Text, Punctuation),\n                 ('#pop', 'oc_ivars')),\n                (r'([a-zA-Z$_][\\w$]*)(\\s*)(\\([a-zA-Z$_][\\w$]*\\))',\n                 bygroups(Name.Class, Text, Name.Label), '#pop'),\n                # simple interface / implementation\n                (r'([a-zA-Z$_][\\w$]*)(\\s*)(\\{)',\n                 bygroups(Name.Class, Text, Punctuation), ('#pop', 'oc_ivars')),\n                (r'([a-zA-Z$_][\\w$]*)', Name.Class, '#pop')\n            ],\n            'oc_forward_classname': [\n                (r'([a-zA-Z$_][\\w$]*)(\\s*,\\s*)',\n                 bygroups(Name.Class, Text), 'oc_forward_classname'),\n                (r'([a-zA-Z$_][\\w$]*)(\\s*;?)',\n                 bygroups(Name.Class, Text), '#pop')\n            ],\n            'oc_ivars': [\n                include('whitespace'),\n                include('statements'),\n                (';', Punctuation),\n                (r'\\{', Punctuation, '#push'),\n                (r'\\}', Punctuation, '#pop'),\n            ],\n            'root': [\n                # methods\n                (r'^([-+])(\\s*)'                         # method marker\n                 r'(\\(.*?\\))?(\\s*)'                      # return type\n                 r'([a-zA-Z$_][\\w$]*:?)',        # begin of method name\n                 bygroups(Punctuation, Text, using(this),\n                          Text, Name.Function),\n                 'method'),\n                inherit,\n            ],\n            'method': [\n                include('whitespace'),\n                # TODO unsure if ellipses are allowed elsewhere, see\n                # discussion in Issue 789\n                (r',', Punctuation),\n                (r'\\.\\.\\.', Punctuation),\n                (r'(\\(.*?\\))(\\s*)([a-zA-Z$_][\\w$]*)',\n                 bygroups(using(this), Text, Name.Variable)),\n                (r'[a-zA-Z$_][\\w$]*:', Name.Function),\n                (';', Punctuation, '#pop'),\n                (r'\\{', Punctuation, 'function'),\n                default('#pop'),\n            ],\n            'literal_number': [\n                (r'\\(', Punctuation, 'literal_number_inner'),\n                (r'\\)', Literal, '#pop'),\n                include('statement'),\n            ],\n            'literal_number_inner': [\n                (r'\\(', Punctuation, '#push'),\n                (r'\\)', Punctuation, '#pop'),\n                include('statement'),\n            ],\n            'literal_array': [\n                (r'\\[', Punctuation, 'literal_array_inner'),\n                (r'\\]', Literal, '#pop'),\n                include('statement'),\n            ],\n            'literal_array_inner': [\n                (r'\\[', Punctuation, '#push'),\n                (r'\\]', Punctuation, '#pop'),\n                include('statement'),\n            ],\n            'literal_dictionary': [\n                (r'\\}', Literal, '#pop'),\n                include('statement'),\n            ],\n        }\n\n        def analyse_text(text):\n            if _oc_keywords.search(text):\n                return 1.0\n            elif '@\"' in text:  # strings\n                return 0.8\n            elif re.search('@[0-9]+', text):\n                return 0.7\n            elif _oc_message.search(text):\n                return 0.8\n            return 0\n\n        def get_tokens_unprocessed(self, text):\n            from pygments.lexers._cocoa_builtins import COCOA_INTERFACES, \\\n                COCOA_PROTOCOLS, COCOA_PRIMITIVES\n\n            for index, token, value in \\\n                    baselexer.get_tokens_unprocessed(self, text):\n                if token is Name or token is Name.Class:\n                    if value in COCOA_INTERFACES or value in COCOA_PROTOCOLS \\\n                       or value in COCOA_PRIMITIVES:\n                        token = Name.Builtin.Pseudo\n\n                yield index, token, value\n\n    return GeneratedObjectiveCVariant\n\n\nclass ObjectiveCLexer(objective(CLexer)):\n    \"\"\"\n    For Objective-C source code with preprocessor directives.\n    \"\"\"\n\n    name = 'Objective-C'\n    aliases = ['objective-c', 'objectivec', 'obj-c', 'objc']\n    filenames = ['*.m', '*.h']\n    mimetypes = ['text/x-objective-c']\n    priority = 0.05    # Lower than C\n\n\nclass ObjectiveCppLexer(objective(CppLexer)):\n    \"\"\"\n    For Objective-C++ source code with preprocessor directives.\n    \"\"\"\n\n    name = 'Objective-C++'\n    aliases = ['objective-c++', 'objectivec++', 'obj-c++', 'objc++']\n    filenames = ['*.mm', '*.hh']\n    mimetypes = ['text/x-objective-c++']\n    priority = 0.05    # Lower than C++\n\n\nclass LogosLexer(ObjectiveCppLexer):\n    \"\"\"\n    For Logos + Objective-C source code with preprocessor directives.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'Logos'\n    aliases = ['logos']\n    filenames = ['*.x', '*.xi', '*.xm', '*.xmi']\n    mimetypes = ['text/x-logos']\n    priority = 0.25\n\n    tokens = {\n        'statements': [\n            (r'(%orig|%log)\\b', Keyword),\n            (r'(%c)\\b(\\()(\\s*)([a-zA-Z$_][\\w$]*)(\\s*)(\\))',\n             bygroups(Keyword, Punctuation, Text, Name.Class, Text, Punctuation)),\n            (r'(%init)\\b(\\()',\n             bygroups(Keyword, Punctuation), 'logos_init_directive'),\n            (r'(%init)(?=\\s*;)', bygroups(Keyword)),\n            (r'(%hook|%group)(\\s+)([a-zA-Z$_][\\w$]+)',\n             bygroups(Keyword, Text, Name.Class), '#pop'),\n            (r'(%subclass)(\\s+)', bygroups(Keyword, Text),\n             ('#pop', 'logos_classname')),\n            inherit,\n        ],\n        'logos_init_directive': [\n            (r'\\s+', Text),\n            (',', Punctuation, ('logos_init_directive', '#pop')),\n            (r'([a-zA-Z$_][\\w$]*)(\\s*)(=)(\\s*)([^);]*)',\n             bygroups(Name.Class, Text, Punctuation, Text, Text)),\n            (r'([a-zA-Z$_][\\w$]*)', Name.Class),\n            (r'\\)', Punctuation, '#pop'),\n        ],\n        'logos_classname': [\n            (r'([a-zA-Z$_][\\w$]*)(\\s*:\\s*)([a-zA-Z$_][\\w$]*)?',\n             bygroups(Name.Class, Text, Name.Class), '#pop'),\n            (r'([a-zA-Z$_][\\w$]*)', Name.Class, '#pop')\n        ],\n        'root': [\n            (r'(%subclass)(\\s+)', bygroups(Keyword, Text),\n             'logos_classname'),\n            (r'(%hook|%group)(\\s+)([a-zA-Z$_][\\w$]+)',\n             bygroups(Keyword, Text, Name.Class)),\n            (r'(%config)(\\s*\\(\\s*)(\\w+)(\\s*=)(.*?)(\\)\\s*)',\n             bygroups(Keyword, Text, Name.Variable, Text, String, Text)),\n            (r'(%ctor)(\\s*)(\\{)', bygroups(Keyword, Text, Punctuation),\n             'function'),\n            (r'(%new)(\\s*)(\\()(.*?)(\\))',\n             bygroups(Keyword, Text, Keyword, String, Keyword)),\n            (r'(\\s*)(%end)(\\s*)', bygroups(Text, Keyword, Text)),\n            inherit,\n        ],\n    }\n\n    _logos_keywords = re.compile(r'%(?:hook|ctor|init|c\\()')\n\n    def analyse_text(text):\n        if LogosLexer._logos_keywords.search(text):\n            return 1.0\n        return 0\n\n\nclass SwiftLexer(RegexLexer):\n    \"\"\"\n    For `Swift <https://developer.apple.com/swift/>`_ source.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    name = 'Swift'\n    filenames = ['*.swift']\n    aliases = ['swift']\n    mimetypes = ['text/x-swift']\n\n    tokens = {\n        'root': [\n            # Whitespace and Comments\n            (r'\\n', Text),\n            (r'\\s+', Text),\n            (r'//', Comment.Single, 'comment-single'),\n            (r'/\\*', Comment.Multiline, 'comment-multi'),\n            (r'#(if|elseif|else|endif|available)\\b', Comment.Preproc, 'preproc'),\n\n            # Keywords\n            include('keywords'),\n\n            # Global Types\n            (words((\n                'Array', 'AutoreleasingUnsafeMutablePointer', 'BidirectionalReverseView',\n                'Bit', 'Bool', 'CFunctionPointer', 'COpaquePointer', 'CVaListPointer',\n                'Character', 'ClosedInterval', 'CollectionOfOne', 'ContiguousArray',\n                'Dictionary', 'DictionaryGenerator', 'DictionaryIndex', 'Double',\n                'EmptyCollection', 'EmptyGenerator', 'EnumerateGenerator',\n                'EnumerateSequence', 'FilterCollectionView',\n                'FilterCollectionViewIndex', 'FilterGenerator', 'FilterSequenceView',\n                'Float', 'Float80', 'FloatingPointClassification', 'GeneratorOf',\n                'GeneratorOfOne', 'GeneratorSequence', 'HalfOpenInterval', 'HeapBuffer',\n                'HeapBufferStorage', 'ImplicitlyUnwrappedOptional', 'IndexingGenerator',\n                'Int', 'Int16', 'Int32', 'Int64', 'Int8', 'LazyBidirectionalCollection',\n                'LazyForwardCollection', 'LazyRandomAccessCollection',\n                'LazySequence', 'MapCollectionView', 'MapSequenceGenerator',\n                'MapSequenceView', 'MirrorDisposition', 'ObjectIdentifier', 'OnHeap',\n                'Optional', 'PermutationGenerator', 'QuickLookObject',\n                'RandomAccessReverseView', 'Range', 'RangeGenerator', 'RawByte', 'Repeat',\n                'ReverseBidirectionalIndex', 'ReverseRandomAccessIndex', 'SequenceOf',\n                'SinkOf', 'Slice', 'StaticString', 'StrideThrough', 'StrideThroughGenerator',\n                'StrideTo', 'StrideToGenerator', 'String', 'UInt', 'UInt16', 'UInt32',\n                'UInt64', 'UInt8', 'UTF16', 'UTF32', 'UTF8', 'UnicodeDecodingResult',\n                'UnicodeScalar', 'Unmanaged', 'UnsafeBufferPointer',\n                'UnsafeBufferPointerGenerator', 'UnsafeMutableBufferPointer',\n                'UnsafeMutablePointer', 'UnsafePointer', 'Zip2', 'ZipGenerator2',\n                # Protocols\n                'AbsoluteValuable', 'AnyObject', 'ArrayLiteralConvertible',\n                'BidirectionalIndexType', 'BitwiseOperationsType',\n                'BooleanLiteralConvertible', 'BooleanType', 'CVarArgType',\n                'CollectionType', 'Comparable', 'DebugPrintable',\n                'DictionaryLiteralConvertible', 'Equatable',\n                'ExtendedGraphemeClusterLiteralConvertible',\n                'ExtensibleCollectionType', 'FloatLiteralConvertible',\n                'FloatingPointType', 'ForwardIndexType', 'GeneratorType', 'Hashable',\n                'IntegerArithmeticType', 'IntegerLiteralConvertible', 'IntegerType',\n                'IntervalType', 'MirrorType', 'MutableCollectionType', 'MutableSliceable',\n                'NilLiteralConvertible', 'OutputStreamType', 'Printable',\n                'RandomAccessIndexType', 'RangeReplaceableCollectionType',\n                'RawOptionSetType', 'RawRepresentable', 'Reflectable', 'SequenceType',\n                'SignedIntegerType', 'SignedNumberType', 'SinkType', 'Sliceable',\n                'Streamable', 'Strideable', 'StringInterpolationConvertible',\n                'StringLiteralConvertible', 'UnicodeCodecType',\n                'UnicodeScalarLiteralConvertible', 'UnsignedIntegerType',\n                '_ArrayBufferType', '_BidirectionalIndexType', '_CocoaStringType',\n                '_CollectionType', '_Comparable', '_ExtensibleCollectionType',\n                '_ForwardIndexType', '_Incrementable', '_IntegerArithmeticType',\n                '_IntegerType', '_ObjectiveCBridgeable', '_RandomAccessIndexType',\n                '_RawOptionSetType', '_SequenceType', '_Sequence_Type',\n                '_SignedIntegerType', '_SignedNumberType', '_Sliceable', '_Strideable',\n                '_SwiftNSArrayRequiredOverridesType', '_SwiftNSArrayType',\n                '_SwiftNSCopyingType', '_SwiftNSDictionaryRequiredOverridesType',\n                '_SwiftNSDictionaryType', '_SwiftNSEnumeratorType',\n                '_SwiftNSFastEnumerationType', '_SwiftNSStringRequiredOverridesType',\n                '_SwiftNSStringType', '_UnsignedIntegerType',\n                # Variables\n                'C_ARGC', 'C_ARGV', 'Process',\n                # Typealiases\n                'Any', 'AnyClass', 'BooleanLiteralType', 'CBool', 'CChar', 'CChar16',\n                'CChar32', 'CDouble', 'CFloat', 'CInt', 'CLong', 'CLongLong', 'CShort',\n                'CSignedChar', 'CUnsignedInt', 'CUnsignedLong', 'CUnsignedShort',\n                'CWideChar', 'ExtendedGraphemeClusterType', 'Float32', 'Float64',\n                'FloatLiteralType', 'IntMax', 'IntegerLiteralType', 'StringLiteralType',\n                'UIntMax', 'UWord', 'UnicodeScalarType', 'Void', 'Word',\n                # Foundation/Cocoa\n                'NSErrorPointer', 'NSObjectProtocol', 'Selector'), suffix=r'\\b'),\n             Name.Builtin),\n            # Functions\n            (words((\n                'abs', 'advance', 'alignof', 'alignofValue', 'assert', 'assertionFailure',\n                'contains', 'count', 'countElements', 'debugPrint', 'debugPrintln',\n                'distance', 'dropFirst', 'dropLast', 'dump', 'enumerate', 'equal',\n                'extend', 'fatalError', 'filter', 'find', 'first', 'getVaList', 'indices',\n                'insert', 'isEmpty', 'join', 'last', 'lazy', 'lexicographicalCompare',\n                'map', 'max', 'maxElement', 'min', 'minElement', 'numericCast', 'overlaps',\n                'partition', 'precondition', 'preconditionFailure', 'prefix', 'print',\n                'println', 'reduce', 'reflect', 'removeAll', 'removeAtIndex', 'removeLast',\n                'removeRange', 'reverse', 'sizeof', 'sizeofValue', 'sort', 'sorted',\n                'splice', 'split', 'startsWith', 'stride', 'strideof', 'strideofValue',\n                'suffix', 'swap', 'toDebugString', 'toString', 'transcode',\n                'underestimateCount', 'unsafeAddressOf', 'unsafeBitCast', 'unsafeDowncast',\n                'withExtendedLifetime', 'withUnsafeMutablePointer',\n                'withUnsafeMutablePointers', 'withUnsafePointer', 'withUnsafePointers',\n                'withVaList'), suffix=r'\\b'),\n             Name.Builtin.Pseudo),\n\n            # Implicit Block Variables\n            (r'\\$\\d+', Name.Variable),\n\n            # Binary Literal\n            (r'0b[01_]+', Number.Bin),\n            # Octal Literal\n            (r'0o[0-7_]+', Number.Oct),\n            # Hexadecimal Literal\n            (r'0x[0-9a-fA-F_]+', Number.Hex),\n            # Decimal Literal\n            (r'[0-9][0-9_]*(\\.[0-9_]+[eE][+\\-]?[0-9_]+|'\n             r'\\.[0-9_]*|[eE][+\\-]?[0-9_]+)', Number.Float),\n            (r'[0-9][0-9_]*', Number.Integer),\n            # String Literal\n            (r'\"', String, 'string'),\n\n            # Operators and Punctuation\n            (r'[(){}\\[\\].,:;=@#`?]|->|[<&?](?=\\w)|(?<=\\w)[>!?]', Punctuation),\n            (r'[/=\\-+!*%<>&|^?~]+', Operator),\n\n            # Identifier\n            (r'[a-zA-Z_]\\w*', Name)\n        ],\n        'keywords': [\n            (words((\n                'as', 'break', 'case', 'catch', 'continue', 'default', 'defer',\n                'do', 'else', 'fallthrough', 'for', 'guard', 'if', 'in', 'is',\n                'repeat', 'return', '#selector', 'switch', 'throw', 'try',\n                'where', 'while'), suffix=r'\\b'),\n             Keyword),\n            (r'@availability\\([^)]+\\)', Keyword.Reserved),\n            (words((\n                'associativity', 'convenience', 'dynamic', 'didSet', 'final',\n                'get', 'indirect', 'infix', 'inout', 'lazy', 'left', 'mutating',\n                'none', 'nonmutating', 'optional', 'override', 'postfix',\n                'precedence', 'prefix', 'Protocol', 'required', 'rethrows',\n                'right', 'set', 'throws', 'Type', 'unowned', 'weak', 'willSet',\n                '@availability', '@autoclosure', '@noreturn',\n                '@NSApplicationMain', '@NSCopying', '@NSManaged', '@objc',\n                '@UIApplicationMain', '@IBAction', '@IBDesignable',\n                '@IBInspectable', '@IBOutlet'), suffix=r'\\b'),\n             Keyword.Reserved),\n            (r'(as|dynamicType|false|is|nil|self|Self|super|true|__COLUMN__'\n             r'|__FILE__|__FUNCTION__|__LINE__|_'\n             r'|#(?:file|line|column|function))\\b', Keyword.Constant),\n            (r'import\\b', Keyword.Declaration, 'module'),\n            (r'(class|enum|extension|struct|protocol)(\\s+)([a-zA-Z_]\\w*)',\n             bygroups(Keyword.Declaration, Text, Name.Class)),\n            (r'(func)(\\s+)([a-zA-Z_]\\w*)',\n             bygroups(Keyword.Declaration, Text, Name.Function)),\n            (r'(var|let)(\\s+)([a-zA-Z_]\\w*)', bygroups(Keyword.Declaration,\n             Text, Name.Variable)),\n            (words((\n                'class', 'deinit', 'enum', 'extension', 'func', 'import', 'init',\n                'internal', 'let', 'operator', 'private', 'protocol', 'public',\n                'static', 'struct', 'subscript', 'typealias', 'var'), suffix=r'\\b'),\n             Keyword.Declaration)\n        ],\n        'comment': [\n            (r':param: [a-zA-Z_]\\w*|:returns?:|(FIXME|MARK|TODO):',\n             Comment.Special)\n        ],\n\n        # Nested\n        'comment-single': [\n            (r'\\n', Text, '#pop'),\n            include('comment'),\n            (r'[^\\n]', Comment.Single)\n        ],\n        'comment-multi': [\n            include('comment'),\n            (r'[^*/]', Comment.Multiline),\n            (r'/\\*', Comment.Multiline, '#push'),\n            (r'\\*/', Comment.Multiline, '#pop'),\n            (r'[*/]', Comment.Multiline)\n        ],\n        'module': [\n            (r'\\n', Text, '#pop'),\n            (r'[a-zA-Z_]\\w*', Name.Class),\n            include('root')\n        ],\n        'preproc': [\n            (r'\\n', Text, '#pop'),\n            include('keywords'),\n            (r'[A-Za-z]\\w*', Comment.Preproc),\n            include('root')\n        ],\n        'string': [\n            (r'\\\\\\(', String.Interpol, 'string-intp'),\n            (r'\"', String, '#pop'),\n            (r\"\"\"\\\\['\"\\\\nrt]|\\\\x[0-9a-fA-F]{2}|\\\\[0-7]{1,3}\"\"\"\n             r\"\"\"|\\\\u[0-9a-fA-F]{4}|\\\\U[0-9a-fA-F]{8}\"\"\", String.Escape),\n            (r'[^\\\\\"]+', String),\n            (r'\\\\', String)\n        ],\n        'string-intp': [\n            (r'\\(', String.Interpol, '#push'),\n            (r'\\)', String.Interpol, '#pop'),\n            include('root')\n        ]\n    }\n\n    def get_tokens_unprocessed(self, text):\n        from pygments.lexers._cocoa_builtins import COCOA_INTERFACES, \\\n            COCOA_PROTOCOLS, COCOA_PRIMITIVES\n\n        for index, token, value in \\\n                RegexLexer.get_tokens_unprocessed(self, text):\n            if token is Name or token is Name.Class:\n                if value in COCOA_INTERFACES or value in COCOA_PROTOCOLS \\\n                   or value in COCOA_PRIMITIVES:\n                    token = Name.Builtin.Pseudo\n\n            yield index, token, value\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.templates\n    ~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for various template engines' markup.\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexers.html import HtmlLexer, XmlLexer\nfrom pygments.lexers.javascript import JavascriptLexer, LassoLexer\nfrom pygments.lexers.css import CssLexer\nfrom pygments.lexers.php import PhpLexer\nfrom pygments.lexers.python import PythonLexer\nfrom pygments.lexers.perl import PerlLexer\nfrom pygments.lexers.jvm import JavaLexer, TeaLangLexer\nfrom pygments.lexers.data import YamlLexer\nfrom pygments.lexer import Lexer, DelegatingLexer, RegexLexer, bygroups, \\\n    include, using, this, default, combined\nfrom pygments.token import Error, Punctuation, Whitespace, \\\n    Text, Comment, Operator, Keyword, Name, String, Number, Other, Token\nfrom pygments.util import html_doctype_matches, looks_like_xml\n\n__all__ = ['HtmlPhpLexer', 'XmlPhpLexer', 'CssPhpLexer',\n           'JavascriptPhpLexer', 'ErbLexer', 'RhtmlLexer',\n           'XmlErbLexer', 'CssErbLexer', 'JavascriptErbLexer',\n           'SmartyLexer', 'HtmlSmartyLexer', 'XmlSmartyLexer',\n           'CssSmartyLexer', 'JavascriptSmartyLexer', 'DjangoLexer',\n           'HtmlDjangoLexer', 'CssDjangoLexer', 'XmlDjangoLexer',\n           'JavascriptDjangoLexer', 'GenshiLexer', 'HtmlGenshiLexer',\n           'GenshiTextLexer', 'CssGenshiLexer', 'JavascriptGenshiLexer',\n           'MyghtyLexer', 'MyghtyHtmlLexer', 'MyghtyXmlLexer',\n           'MyghtyCssLexer', 'MyghtyJavascriptLexer', 'MasonLexer', 'MakoLexer',\n           'MakoHtmlLexer', 'MakoXmlLexer', 'MakoJavascriptLexer',\n           'MakoCssLexer', 'JspLexer', 'CheetahLexer', 'CheetahHtmlLexer',\n           'CheetahXmlLexer', 'CheetahJavascriptLexer', 'EvoqueLexer',\n           'EvoqueHtmlLexer', 'EvoqueXmlLexer', 'ColdfusionLexer',\n           'ColdfusionHtmlLexer', 'ColdfusionCFCLexer', 'VelocityLexer',\n           'VelocityHtmlLexer', 'VelocityXmlLexer', 'SspLexer',\n           'TeaTemplateLexer', 'LassoHtmlLexer', 'LassoXmlLexer',\n           'LassoCssLexer', 'LassoJavascriptLexer', 'HandlebarsLexer',\n           'HandlebarsHtmlLexer', 'YamlJinjaLexer', 'LiquidLexer',\n           'TwigLexer', 'TwigHtmlLexer', 'Angular2Lexer', 'Angular2HtmlLexer']\n\n\nclass ErbLexer(Lexer):\n    \"\"\"\n    Generic `ERB <http://ruby-doc.org/core/classes/ERB.html>`_ (Ruby Templating)\n    lexer.\n\n    Just highlights ruby code between the preprocessor directives, other data\n    is left untouched by the lexer.\n\n    All options are also forwarded to the `RubyLexer`.\n    \"\"\"\n\n    name = 'ERB'\n    aliases = ['erb']\n    mimetypes = ['application/x-ruby-templating']\n\n    _block_re = re.compile(r'(<%%|%%>|<%=|<%#|<%-|<%|-%>|%>|^%[^%].*?$)', re.M)\n\n    def __init__(self, **options):\n        from pygments.lexers.ruby import RubyLexer\n        self.ruby_lexer = RubyLexer(**options)\n        Lexer.__init__(self, **options)\n\n    def get_tokens_unprocessed(self, text):\n        \"\"\"\n        Since ERB doesn't allow \"<%\" and other tags inside of ruby\n        blocks we have to use a split approach here that fails for\n        that too.\n        \"\"\"\n        tokens = self._block_re.split(text)\n        tokens.reverse()\n        state = idx = 0\n        try:\n            while True:\n                # text\n                if state == 0:\n                    val = tokens.pop()\n                    yield idx, Other, val\n                    idx += len(val)\n                    state = 1\n                # block starts\n                elif state == 1:\n                    tag = tokens.pop()\n                    # literals\n                    if tag in ('<%%', '%%>'):\n                        yield idx, Other, tag\n                        idx += 3\n                        state = 0\n                    # comment\n                    elif tag == '<%#':\n                        yield idx, Comment.Preproc, tag\n                        val = tokens.pop()\n                        yield idx + 3, Comment, val\n                        idx += 3 + len(val)\n                        state = 2\n                    # blocks or output\n                    elif tag in ('<%', '<%=', '<%-'):\n                        yield idx, Comment.Preproc, tag\n                        idx += len(tag)\n                        data = tokens.pop()\n                        r_idx = 0\n                        for r_idx, r_token, r_value in \\\n                                self.ruby_lexer.get_tokens_unprocessed(data):\n                            yield r_idx + idx, r_token, r_value\n                        idx += len(data)\n                        state = 2\n                    elif tag in ('%>', '-%>'):\n                        yield idx, Error, tag\n                        idx += len(tag)\n                        state = 0\n                    # % raw ruby statements\n                    else:\n                        yield idx, Comment.Preproc, tag[0]\n                        r_idx = 0\n                        for r_idx, r_token, r_value in \\\n                                self.ruby_lexer.get_tokens_unprocessed(tag[1:]):\n                            yield idx + 1 + r_idx, r_token, r_value\n                        idx += len(tag)\n                        state = 0\n                # block ends\n                elif state == 2:\n                    tag = tokens.pop()\n                    if tag not in ('%>', '-%>'):\n                        yield idx, Other, tag\n                    else:\n                        yield idx, Comment.Preproc, tag\n                    idx += len(tag)\n                    state = 0\n        except IndexError:\n            return\n\n    def analyse_text(text):\n        if '<%' in text and '%>' in text:\n            return 0.4\n\n\nclass SmartyLexer(RegexLexer):\n    \"\"\"\n    Generic `Smarty <http://smarty.php.net/>`_ template lexer.\n\n    Just highlights smarty code between the preprocessor directives, other\n    data is left untouched by the lexer.\n    \"\"\"\n\n    name = 'Smarty'\n    aliases = ['smarty']\n    filenames = ['*.tpl']\n    mimetypes = ['application/x-smarty']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Other),\n            (r'(\\{)(\\*.*?\\*)(\\})',\n             bygroups(Comment.Preproc, Comment, Comment.Preproc)),\n            (r'(\\{php\\})(.*?)(\\{/php\\})',\n             bygroups(Comment.Preproc, using(PhpLexer, startinline=True),\n                      Comment.Preproc)),\n            (r'(\\{)(/?[a-zA-Z_]\\w*)(\\s*)',\n             bygroups(Comment.Preproc, Name.Function, Text), 'smarty'),\n            (r'\\{', Comment.Preproc, 'smarty')\n        ],\n        'smarty': [\n            (r'\\s+', Text),\n            (r'\\{', Comment.Preproc, '#push'),\n            (r'\\}', Comment.Preproc, '#pop'),\n            (r'#[a-zA-Z_]\\w*#', Name.Variable),\n            (r'\\$[a-zA-Z_]\\w*(\\.\\w+)*', Name.Variable),\n            (r'[~!%^&*()+=|\\[\\]:;,.<>/?@-]', Operator),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'[a-zA-Z_]\\w*', Name.Attribute)\n        ]\n    }\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'\\{if\\s+.*?\\}.*?\\{/if\\}', text):\n            rv += 0.15\n        if re.search(r'\\{include\\s+file=.*?\\}', text):\n            rv += 0.15\n        if re.search(r'\\{foreach\\s+.*?\\}.*?\\{/foreach\\}', text):\n            rv += 0.15\n        if re.search(r'\\{\\$.*?\\}', text):\n            rv += 0.01\n        return rv\n\n\nclass VelocityLexer(RegexLexer):\n    \"\"\"\n    Generic `Velocity <http://velocity.apache.org/>`_ template lexer.\n\n    Just highlights velocity directives and variable references, other\n    data is left untouched by the lexer.\n    \"\"\"\n\n    name = 'Velocity'\n    aliases = ['velocity']\n    filenames = ['*.vm', '*.fhtml']\n\n    flags = re.MULTILINE | re.DOTALL\n\n    identifier = r'[a-zA-Z_]\\w*'\n\n    tokens = {\n        'root': [\n            (r'[^{#$]+', Other),\n            (r'(#)(\\*.*?\\*)(#)',\n             bygroups(Comment.Preproc, Comment, Comment.Preproc)),\n            (r'(##)(.*?$)',\n             bygroups(Comment.Preproc, Comment)),\n            (r'(#\\{?)(' + identifier + r')(\\}?)(\\s?\\()',\n             bygroups(Comment.Preproc, Name.Function, Comment.Preproc, Punctuation),\n             'directiveparams'),\n            (r'(#\\{?)(' + identifier + r')(\\}|\\b)',\n             bygroups(Comment.Preproc, Name.Function, Comment.Preproc)),\n            (r'\\$!?\\{?', Punctuation, 'variable')\n        ],\n        'variable': [\n            (identifier, Name.Variable),\n            (r'\\(', Punctuation, 'funcparams'),\n            (r'(\\.)(' + identifier + r')',\n             bygroups(Punctuation, Name.Variable), '#push'),\n            (r'\\}', Punctuation, '#pop'),\n            default('#pop')\n        ],\n        'directiveparams': [\n            (r'(&&|\\|\\||==?|!=?|[-<>+*%&|^/])|\\b(eq|ne|gt|lt|ge|le|not|in)\\b',\n             Operator),\n            (r'\\[', Operator, 'rangeoperator'),\n            (r'\\b' + identifier + r'\\b', Name.Function),\n            include('funcparams')\n        ],\n        'rangeoperator': [\n            (r'\\.\\.', Operator),\n            include('funcparams'),\n            (r'\\]', Operator, '#pop')\n        ],\n        'funcparams': [\n            (r'\\$!?\\{?', Punctuation, 'variable'),\n            (r'\\s+', Text),\n            (r'[,:]', Punctuation),\n            (r'\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\"'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n            (r\"\\b[0-9]+\\b\", Number),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'\\(', Punctuation, '#push'),\n            (r'\\)', Punctuation, '#pop'),\n            (r'\\{', Punctuation, '#push'),\n            (r'\\}', Punctuation, '#pop'),\n            (r'\\[', Punctuation, '#push'),\n            (r'\\]', Punctuation, '#pop'),\n        ]\n    }\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'#\\{?macro\\}?\\(.*?\\).*?#\\{?end\\}?', text):\n            rv += 0.25\n        if re.search(r'#\\{?if\\}?\\(.+?\\).*?#\\{?end\\}?', text):\n            rv += 0.15\n        if re.search(r'#\\{?foreach\\}?\\(.+?\\).*?#\\{?end\\}?', text):\n            rv += 0.15\n        if re.search(r'\\$!?\\{?[a-zA-Z_]\\w*(\\([^)]*\\))?'\n                     r'(\\.\\w+(\\([^)]*\\))?)*\\}?', text):\n            rv += 0.01\n        return rv\n\n\nclass VelocityHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `VelocityLexer` that highlights unlexed data\n    with the `HtmlLexer`.\n\n    \"\"\"\n\n    name = 'HTML+Velocity'\n    aliases = ['html+velocity']\n    alias_filenames = ['*.html', '*.fhtml']\n    mimetypes = ['text/html+velocity']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, VelocityLexer, **options)\n\n\nclass VelocityXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `VelocityLexer` that highlights unlexed data\n    with the `XmlLexer`.\n\n    \"\"\"\n\n    name = 'XML+Velocity'\n    aliases = ['xml+velocity']\n    alias_filenames = ['*.xml', '*.vm']\n    mimetypes = ['application/xml+velocity']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, VelocityLexer, **options)\n\n    def analyse_text(text):\n        rv = VelocityLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass DjangoLexer(RegexLexer):\n    \"\"\"\n    Generic `django <http://www.djangoproject.com/documentation/templates/>`_\n    and `jinja <https://jinja.pocoo.org/jinja/>`_ template lexer.\n\n    It just highlights django/jinja code between the preprocessor directives,\n    other data is left untouched by the lexer.\n    \"\"\"\n\n    name = 'Django/Jinja'\n    aliases = ['django', 'jinja']\n    mimetypes = ['application/x-django-templating', 'application/x-jinja']\n\n    flags = re.M | re.S\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Other),\n            (r'\\{\\{', Comment.Preproc, 'var'),\n            # jinja/django comments\n            (r'\\{#.*?#\\}', Comment),\n            # django comments\n            (r'(\\{%)(-?\\s*)(comment)(\\s*-?)(%\\})(.*?)'\n             r'(\\{%)(-?\\s*)(endcomment)(\\s*-?)(%\\})',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Comment.Preproc,\n                      Comment, Comment.Preproc, Text, Keyword, Text,\n                      Comment.Preproc)),\n            # raw jinja blocks\n            (r'(\\{%)(-?\\s*)(raw)(\\s*-?)(%\\})(.*?)'\n             r'(\\{%)(-?\\s*)(endraw)(\\s*-?)(%\\})',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Comment.Preproc,\n                      Text, Comment.Preproc, Text, Keyword, Text,\n                      Comment.Preproc)),\n            # filter blocks\n            (r'(\\{%)(-?\\s*)(filter)(\\s+)([a-zA-Z_]\\w*)',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Name.Function),\n             'block'),\n            (r'(\\{%)(-?\\s*)([a-zA-Z_]\\w*)',\n             bygroups(Comment.Preproc, Text, Keyword), 'block'),\n            (r'\\{', Other)\n        ],\n        'varnames': [\n            (r'(\\|)(\\s*)([a-zA-Z_]\\w*)',\n             bygroups(Operator, Text, Name.Function)),\n            (r'(is)(\\s+)(not)?(\\s+)?([a-zA-Z_]\\w*)',\n             bygroups(Keyword, Text, Keyword, Text, Name.Function)),\n            (r'(_|true|false|none|True|False|None)\\b', Keyword.Pseudo),\n            (r'(in|as|reversed|recursive|not|and|or|is|if|else|import|'\n             r'with(?:(?:out)?\\s*context)?|scoped|ignore\\s+missing)\\b',\n             Keyword),\n            (r'(loop|block|super|forloop)\\b', Name.Builtin),\n            (r'[a-zA-Z_][\\w-]*', Name.Variable),\n            (r'\\.\\w+', Name.Variable),\n            (r':?\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\":?'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'([{}()\\[\\]+\\-*/%,:~]|[><=]=?|!=)', Operator),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n        ],\n        'var': [\n            (r'\\s+', Text),\n            (r'(-?)(\\}\\})', bygroups(Text, Comment.Preproc), '#pop'),\n            include('varnames')\n        ],\n        'block': [\n            (r'\\s+', Text),\n            (r'(-?)(%\\})', bygroups(Text, Comment.Preproc), '#pop'),\n            include('varnames'),\n            (r'.', Punctuation)\n        ]\n    }\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'\\{%\\s*(block|extends)', text) is not None:\n            rv += 0.4\n        if re.search(r'\\{%\\s*if\\s*.*?%\\}', text) is not None:\n            rv += 0.1\n        if re.search(r'\\{\\{.*?\\}\\}', text) is not None:\n            rv += 0.1\n        return rv\n\n\nclass MyghtyLexer(RegexLexer):\n    \"\"\"\n    Generic `myghty templates`_ lexer. Code that isn't Myghty\n    markup is yielded as `Token.Other`.\n\n    .. versionadded:: 0.6\n\n    .. _myghty templates: http://www.myghty.org/\n    \"\"\"\n\n    name = 'Myghty'\n    aliases = ['myghty']\n    filenames = ['*.myt', 'autodelegate']\n    mimetypes = ['application/x-myghty']\n\n    tokens = {\n        'root': [\n            (r'\\s+', Text),\n            (r'(?s)(<%(?:def|method))(\\s*)(.*?)(>)(.*?)(</%\\2\\s*>)',\n             bygroups(Name.Tag, Text, Name.Function, Name.Tag,\n                      using(this), Name.Tag)),\n            (r'(?s)(<%\\w+)(.*?)(>)(.*?)(</%\\2\\s*>)',\n             bygroups(Name.Tag, Name.Function, Name.Tag,\n                      using(PythonLexer), Name.Tag)),\n            (r'(<&[^|])(.*?)(,.*?)?(&>)',\n             bygroups(Name.Tag, Name.Function, using(PythonLexer), Name.Tag)),\n            (r'(?s)(<&\\|)(.*?)(,.*?)?(&>)',\n             bygroups(Name.Tag, Name.Function, using(PythonLexer), Name.Tag)),\n            (r'</&>', Name.Tag),\n            (r'(?s)(<%!?)(.*?)(%>)',\n             bygroups(Name.Tag, using(PythonLexer), Name.Tag)),\n            (r'(?<=^)#[^\\n]*(\\n|\\Z)', Comment),\n            (r'(?<=^)(%)([^\\n]*)(\\n|\\Z)',\n             bygroups(Name.Tag, using(PythonLexer), Other)),\n            (r\"\"\"(?sx)\n                 (.+?)               # anything, followed by:\n                 (?:\n                  (?<=\\n)(?=[%#]) |  # an eval or comment line\n                  (?=</?[%&]) |      # a substitution or block or\n                                     # call start or end\n                                     # - don't consume\n                  (\\\\\\n) |           # an escaped newline\n                  \\Z                 # end of string\n                 )\"\"\", bygroups(Other, Operator)),\n        ]\n    }\n\n\nclass MyghtyHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MyghtyLexer` that highlights unlexed data\n    with the `HtmlLexer`.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    name = 'HTML+Myghty'\n    aliases = ['html+myghty']\n    mimetypes = ['text/html+myghty']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, MyghtyLexer, **options)\n\n\nclass MyghtyXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MyghtyLexer` that highlights unlexed data\n    with the `XmlLexer`.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    name = 'XML+Myghty'\n    aliases = ['xml+myghty']\n    mimetypes = ['application/xml+myghty']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, MyghtyLexer, **options)\n\n\nclass MyghtyJavascriptLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MyghtyLexer` that highlights unlexed data\n    with the `JavascriptLexer`.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    name = 'JavaScript+Myghty'\n    aliases = ['js+myghty', 'javascript+myghty']\n    mimetypes = ['application/x-javascript+myghty',\n                 'text/x-javascript+myghty',\n                 'text/javascript+mygthy']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, MyghtyLexer, **options)\n\n\nclass MyghtyCssLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MyghtyLexer` that highlights unlexed data\n    with the `CssLexer`.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    name = 'CSS+Myghty'\n    aliases = ['css+myghty']\n    mimetypes = ['text/css+myghty']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, MyghtyLexer, **options)\n\n\nclass MasonLexer(RegexLexer):\n    \"\"\"\n    Generic `mason templates`_ lexer. Stolen from Myghty lexer. Code that isn't\n    Mason markup is HTML.\n\n    .. _mason templates: http://www.masonhq.com/\n\n    .. versionadded:: 1.4\n    \"\"\"\n    name = 'Mason'\n    aliases = ['mason']\n    filenames = ['*.m', '*.mhtml', '*.mc', '*.mi', 'autohandler', 'dhandler']\n    mimetypes = ['application/x-mason']\n\n    tokens = {\n        'root': [\n            (r'\\s+', Text),\n            (r'(?s)(<%doc>)(.*?)(</%doc>)',\n             bygroups(Name.Tag, Comment.Multiline, Name.Tag)),\n            (r'(?s)(<%(?:def|method))(\\s*)(.*?)(>)(.*?)(</%\\2\\s*>)',\n             bygroups(Name.Tag, Text, Name.Function, Name.Tag,\n                      using(this), Name.Tag)),\n            (r'(?s)(<%(\\w+)(.*?)(>))(.*?)(</%\\2\\s*>)',\n             bygroups(Name.Tag, None, None, None, using(PerlLexer), Name.Tag)),\n            (r'(?s)(<&[^|])(.*?)(,.*?)?(&>)',\n             bygroups(Name.Tag, Name.Function, using(PerlLexer), Name.Tag)),\n            (r'(?s)(<&\\|)(.*?)(,.*?)?(&>)',\n             bygroups(Name.Tag, Name.Function, using(PerlLexer), Name.Tag)),\n            (r'</&>', Name.Tag),\n            (r'(?s)(<%!?)(.*?)(%>)',\n             bygroups(Name.Tag, using(PerlLexer), Name.Tag)),\n            (r'(?<=^)#[^\\n]*(\\n|\\Z)', Comment),\n            (r'(?<=^)(%)([^\\n]*)(\\n|\\Z)',\n             bygroups(Name.Tag, using(PerlLexer), Other)),\n            (r\"\"\"(?sx)\n                 (.+?)               # anything, followed by:\n                 (?:\n                  (?<=\\n)(?=[%#]) |  # an eval or comment line\n                  (?=</?[%&]) |      # a substitution or block or\n                                     # call start or end\n                                     # - don't consume\n                  (\\\\\\n) |           # an escaped newline\n                  \\Z                 # end of string\n                 )\"\"\", bygroups(using(HtmlLexer), Operator)),\n        ]\n    }\n\n    def analyse_text(text):\n        result = 0.0\n        if re.search(r'</%(class|doc|init)>', text) is not None:\n            result = 1.0\n        elif re.search(r'<&.+&>', text, re.DOTALL) is not None:\n            result = 0.11\n        return result\n\n\nclass MakoLexer(RegexLexer):\n    \"\"\"\n    Generic `mako templates`_ lexer. Code that isn't Mako\n    markup is yielded as `Token.Other`.\n\n    .. versionadded:: 0.7\n\n    .. _mako templates: http://www.makotemplates.org/\n    \"\"\"\n\n    name = 'Mako'\n    aliases = ['mako']\n    filenames = ['*.mao']\n    mimetypes = ['application/x-mako']\n\n    tokens = {\n        'root': [\n            (r'(\\s*)(%)(\\s*end(?:\\w+))(\\n|\\Z)',\n             bygroups(Text, Comment.Preproc, Keyword, Other)),\n            (r'(\\s*)(%)([^\\n]*)(\\n|\\Z)',\n             bygroups(Text, Comment.Preproc, using(PythonLexer), Other)),\n            (r'(\\s*)(##[^\\n]*)(\\n|\\Z)',\n             bygroups(Text, Comment.Preproc, Other)),\n            (r'(?s)<%doc>.*?</%doc>', Comment.Preproc),\n            (r'(<%)([\\w.:]+)',\n             bygroups(Comment.Preproc, Name.Builtin), 'tag'),\n            (r'(</%)([\\w.:]+)(>)',\n             bygroups(Comment.Preproc, Name.Builtin, Comment.Preproc)),\n            (r'<%(?=([\\w.:]+))', Comment.Preproc, 'ondeftags'),\n            (r'(?s)(<%(?:!?))(.*?)(%>)',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            (r'(\\$\\{)(.*?)(\\})',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            (r'''(?sx)\n                (.+?)                # anything, followed by:\n                (?:\n                 (?<=\\n)(?=%|\\#\\#) | # an eval or comment line\n                 (?=\\#\\*) |          # multiline comment\n                 (?=</?%) |          # a python block\n                                     # call start or end\n                 (?=\\$\\{) |          # a substitution\n                 (?<=\\n)(?=\\s*%) |\n                                     # - don't consume\n                 (\\\\\\n) |            # an escaped newline\n                 \\Z                  # end of string\n                )\n            ''', bygroups(Other, Operator)),\n            (r'\\s+', Text),\n        ],\n        'ondeftags': [\n            (r'<%', Comment.Preproc),\n            (r'(?<=<%)(include|inherit|namespace|page)', Name.Builtin),\n            include('tag'),\n        ],\n        'tag': [\n            (r'((?:\\w+)\\s*=)(\\s*)(\".*?\")',\n             bygroups(Name.Attribute, Text, String)),\n            (r'/?\\s*>', Comment.Preproc, '#pop'),\n            (r'\\s+', Text),\n        ],\n        'attr': [\n            ('\".*?\"', String, '#pop'),\n            (\"'.*?'\", String, '#pop'),\n            (r'[^\\s>]+', String, '#pop'),\n        ],\n    }\n\n\nclass MakoHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MakoLexer` that highlights unlexed data\n    with the `HtmlLexer`.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    name = 'HTML+Mako'\n    aliases = ['html+mako']\n    mimetypes = ['text/html+mako']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, MakoLexer, **options)\n\n\nclass MakoXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MakoLexer` that highlights unlexed data\n    with the `XmlLexer`.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    name = 'XML+Mako'\n    aliases = ['xml+mako']\n    mimetypes = ['application/xml+mako']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, MakoLexer, **options)\n\n\nclass MakoJavascriptLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MakoLexer` that highlights unlexed data\n    with the `JavascriptLexer`.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    name = 'JavaScript+Mako'\n    aliases = ['js+mako', 'javascript+mako']\n    mimetypes = ['application/x-javascript+mako',\n                 'text/x-javascript+mako',\n                 'text/javascript+mako']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, MakoLexer, **options)\n\n\nclass MakoCssLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `MakoLexer` that highlights unlexed data\n    with the `CssLexer`.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    name = 'CSS+Mako'\n    aliases = ['css+mako']\n    mimetypes = ['text/css+mako']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, MakoLexer, **options)\n\n\n# Genshi and Cheetah lexers courtesy of Matt Good.\n\nclass CheetahPythonLexer(Lexer):\n    \"\"\"\n    Lexer for handling Cheetah's special $ tokens in Python syntax.\n    \"\"\"\n\n    def get_tokens_unprocessed(self, text):\n        pylexer = PythonLexer(**self.options)\n        for pos, type_, value in pylexer.get_tokens_unprocessed(text):\n            if type_ == Token.Error and value == '$':\n                type_ = Comment.Preproc\n            yield pos, type_, value\n\n\nclass CheetahLexer(RegexLexer):\n    \"\"\"\n    Generic `cheetah templates`_ lexer. Code that isn't Cheetah\n    markup is yielded as `Token.Other`.  This also works for\n    `spitfire templates`_ which use the same syntax.\n\n    .. _cheetah templates: http://www.cheetahtemplate.org/\n    .. _spitfire templates: http://code.google.com/p/spitfire/\n    \"\"\"\n\n    name = 'Cheetah'\n    aliases = ['cheetah', 'spitfire']\n    filenames = ['*.tmpl', '*.spt']\n    mimetypes = ['application/x-cheetah', 'application/x-spitfire']\n\n    tokens = {\n        'root': [\n            (r'(##[^\\n]*)$',\n             (bygroups(Comment))),\n            (r'#[*](.|\\n)*?[*]#', Comment),\n            (r'#end[^#\\n]*(?:#|$)', Comment.Preproc),\n            (r'#slurp$', Comment.Preproc),\n            (r'(#[a-zA-Z]+)([^#\\n]*)(#|$)',\n             (bygroups(Comment.Preproc, using(CheetahPythonLexer),\n                       Comment.Preproc))),\n            # TODO support other Python syntax like $foo['bar']\n            (r'(\\$)([a-zA-Z_][\\w.]*\\w)',\n             bygroups(Comment.Preproc, using(CheetahPythonLexer))),\n            (r'(?s)(\\$\\{!?)(.*?)(\\})',\n             bygroups(Comment.Preproc, using(CheetahPythonLexer),\n                      Comment.Preproc)),\n            (r'''(?sx)\n                (.+?)               # anything, followed by:\n                (?:\n                 (?=\\#[#a-zA-Z]*) | # an eval comment\n                 (?=\\$[a-zA-Z_{]) | # a substitution\n                 \\Z                 # end of string\n                )\n            ''', Other),\n            (r'\\s+', Text),\n        ],\n    }\n\n\nclass CheetahHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `CheetahLexer` that highlights unlexed data\n    with the `HtmlLexer`.\n    \"\"\"\n\n    name = 'HTML+Cheetah'\n    aliases = ['html+cheetah', 'html+spitfire', 'htmlcheetah']\n    mimetypes = ['text/html+cheetah', 'text/html+spitfire']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, CheetahLexer, **options)\n\n\nclass CheetahXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `CheetahLexer` that highlights unlexed data\n    with the `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+Cheetah'\n    aliases = ['xml+cheetah', 'xml+spitfire']\n    mimetypes = ['application/xml+cheetah', 'application/xml+spitfire']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, CheetahLexer, **options)\n\n\nclass CheetahJavascriptLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `CheetahLexer` that highlights unlexed data\n    with the `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+Cheetah'\n    aliases = ['js+cheetah', 'javascript+cheetah',\n               'js+spitfire', 'javascript+spitfire']\n    mimetypes = ['application/x-javascript+cheetah',\n                 'text/x-javascript+cheetah',\n                 'text/javascript+cheetah',\n                 'application/x-javascript+spitfire',\n                 'text/x-javascript+spitfire',\n                 'text/javascript+spitfire']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, CheetahLexer, **options)\n\n\nclass GenshiTextLexer(RegexLexer):\n    \"\"\"\n    A lexer that highlights `genshi <http://genshi.edgewall.org/>`_ text\n    templates.\n    \"\"\"\n\n    name = 'Genshi Text'\n    aliases = ['genshitext']\n    mimetypes = ['application/x-genshi-text', 'text/x-genshi']\n\n    tokens = {\n        'root': [\n            (r'[^#$\\s]+', Other),\n            (r'^(\\s*)(##.*)$', bygroups(Text, Comment)),\n            (r'^(\\s*)(#)', bygroups(Text, Comment.Preproc), 'directive'),\n            include('variable'),\n            (r'[#$\\s]', Other),\n        ],\n        'directive': [\n            (r'\\n', Text, '#pop'),\n            (r'(?:def|for|if)\\s+.*', using(PythonLexer), '#pop'),\n            (r'(choose|when|with)([^\\S\\n]+)(.*)',\n             bygroups(Keyword, Text, using(PythonLexer)), '#pop'),\n            (r'(choose|otherwise)\\b', Keyword, '#pop'),\n            (r'(end\\w*)([^\\S\\n]*)(.*)', bygroups(Keyword, Text, Comment), '#pop'),\n        ],\n        'variable': [\n            (r'(?<!\\$)(\\$\\{)(.+?)(\\})',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            (r'(?<!\\$)(\\$)([a-zA-Z_][\\w.]*)',\n             Name.Variable),\n        ]\n    }\n\n\nclass GenshiMarkupLexer(RegexLexer):\n    \"\"\"\n    Base lexer for Genshi markup, used by `HtmlGenshiLexer` and\n    `GenshiLexer`.\n    \"\"\"\n\n    flags = re.DOTALL\n\n    tokens = {\n        'root': [\n            (r'[^<$]+', Other),\n            (r'(<\\?python)(.*?)(\\?>)',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            # yield style and script blocks as Other\n            (r'<\\s*(script|style)\\s*.*?>.*?<\\s*/\\1\\s*>', Other),\n            (r'<\\s*py:[a-zA-Z0-9]+', Name.Tag, 'pytag'),\n            (r'<\\s*[a-zA-Z0-9:.]+', Name.Tag, 'tag'),\n            include('variable'),\n            (r'[<$]', Other),\n        ],\n        'pytag': [\n            (r'\\s+', Text),\n            (r'[\\w:-]+\\s*=', Name.Attribute, 'pyattr'),\n            (r'/?\\s*>', Name.Tag, '#pop'),\n        ],\n        'pyattr': [\n            ('(\")(.*?)(\")', bygroups(String, using(PythonLexer), String), '#pop'),\n            (\"(')(.*?)(')\", bygroups(String, using(PythonLexer), String), '#pop'),\n            (r'[^\\s>]+', String, '#pop'),\n        ],\n        'tag': [\n            (r'\\s+', Text),\n            (r'py:[\\w-]+\\s*=', Name.Attribute, 'pyattr'),\n            (r'[\\w:-]+\\s*=', Name.Attribute, 'attr'),\n            (r'/?\\s*>', Name.Tag, '#pop'),\n        ],\n        'attr': [\n            ('\"', String, 'attr-dstring'),\n            (\"'\", String, 'attr-sstring'),\n            (r'[^\\s>]*', String, '#pop')\n        ],\n        'attr-dstring': [\n            ('\"', String, '#pop'),\n            include('strings'),\n            (\"'\", String)\n        ],\n        'attr-sstring': [\n            (\"'\", String, '#pop'),\n            include('strings'),\n            (\"'\", String)\n        ],\n        'strings': [\n            ('[^\"\\'$]+', String),\n            include('variable')\n        ],\n        'variable': [\n            (r'(?<!\\$)(\\$\\{)(.+?)(\\})',\n             bygroups(Comment.Preproc, using(PythonLexer), Comment.Preproc)),\n            (r'(?<!\\$)(\\$)([a-zA-Z_][\\w\\.]*)',\n             Name.Variable),\n        ]\n    }\n\n\nclass HtmlGenshiLexer(DelegatingLexer):\n    \"\"\"\n    A lexer that highlights `genshi <http://genshi.edgewall.org/>`_ and\n    `kid <http://kid-templating.org/>`_ kid HTML templates.\n    \"\"\"\n\n    name = 'HTML+Genshi'\n    aliases = ['html+genshi', 'html+kid']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml']\n    mimetypes = ['text/html+genshi']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, GenshiMarkupLexer, **options)\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'\\$\\{.*?\\}', text) is not None:\n            rv += 0.2\n        if re.search(r'py:(.*?)=[\"\\']', text) is not None:\n            rv += 0.2\n        return rv + HtmlLexer.analyse_text(text) - 0.01\n\n\nclass GenshiLexer(DelegatingLexer):\n    \"\"\"\n    A lexer that highlights `genshi <http://genshi.edgewall.org/>`_ and\n    `kid <http://kid-templating.org/>`_ kid XML templates.\n    \"\"\"\n\n    name = 'Genshi'\n    aliases = ['genshi', 'kid', 'xml+genshi', 'xml+kid']\n    filenames = ['*.kid']\n    alias_filenames = ['*.xml']\n    mimetypes = ['application/x-genshi', 'application/x-kid']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, GenshiMarkupLexer, **options)\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'\\$\\{.*?\\}', text) is not None:\n            rv += 0.2\n        if re.search(r'py:(.*?)=[\"\\']', text) is not None:\n            rv += 0.2\n        return rv + XmlLexer.analyse_text(text) - 0.01\n\n\nclass JavascriptGenshiLexer(DelegatingLexer):\n    \"\"\"\n    A lexer that highlights javascript code in genshi text templates.\n    \"\"\"\n\n    name = 'JavaScript+Genshi Text'\n    aliases = ['js+genshitext', 'js+genshi', 'javascript+genshitext',\n               'javascript+genshi']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+genshi',\n                 'text/x-javascript+genshi',\n                 'text/javascript+genshi']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, GenshiTextLexer, **options)\n\n    def analyse_text(text):\n        return GenshiLexer.analyse_text(text) - 0.05\n\n\nclass CssGenshiLexer(DelegatingLexer):\n    \"\"\"\n    A lexer that highlights CSS definitions in genshi text templates.\n    \"\"\"\n\n    name = 'CSS+Genshi Text'\n    aliases = ['css+genshitext', 'css+genshi']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+genshi']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, GenshiTextLexer, **options)\n\n    def analyse_text(text):\n        return GenshiLexer.analyse_text(text) - 0.05\n\n\nclass RhtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the ERB lexer that highlights the unlexed data with the\n    html lexer.\n\n    Nested Javascript and CSS is highlighted too.\n    \"\"\"\n\n    name = 'RHTML'\n    aliases = ['rhtml', 'html+erb', 'html+ruby']\n    filenames = ['*.rhtml']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml']\n    mimetypes = ['text/html+ruby']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, ErbLexer, **options)\n\n    def analyse_text(text):\n        rv = ErbLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):\n            # one more than the XmlErbLexer returns\n            rv += 0.5\n        return rv\n\n\nclass XmlErbLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `ErbLexer` which highlights data outside preprocessor\n    directives with the `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+Ruby'\n    aliases = ['xml+erb', 'xml+ruby']\n    alias_filenames = ['*.xml']\n    mimetypes = ['application/xml+ruby']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, ErbLexer, **options)\n\n    def analyse_text(text):\n        rv = ErbLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass CssErbLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `ErbLexer` which highlights unlexed data with the `CssLexer`.\n    \"\"\"\n\n    name = 'CSS+Ruby'\n    aliases = ['css+erb', 'css+ruby']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+ruby']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, ErbLexer, **options)\n\n    def analyse_text(text):\n        return ErbLexer.analyse_text(text) - 0.05\n\n\nclass JavascriptErbLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `ErbLexer` which highlights unlexed data with the\n    `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+Ruby'\n    aliases = ['js+erb', 'javascript+erb', 'js+ruby', 'javascript+ruby']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+ruby',\n                 'text/x-javascript+ruby',\n                 'text/javascript+ruby']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, ErbLexer, **options)\n\n    def analyse_text(text):\n        return ErbLexer.analyse_text(text) - 0.05\n\n\nclass HtmlPhpLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `PhpLexer` that highlights unhandled data with the `HtmlLexer`.\n\n    Nested Javascript and CSS is highlighted too.\n    \"\"\"\n\n    name = 'HTML+PHP'\n    aliases = ['html+php']\n    filenames = ['*.phtml']\n    alias_filenames = ['*.php', '*.html', '*.htm', '*.xhtml',\n                       '*.php[345]']\n    mimetypes = ['application/x-php',\n                 'application/x-httpd-php', 'application/x-httpd-php3',\n                 'application/x-httpd-php4', 'application/x-httpd-php5']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, PhpLexer, **options)\n\n    def analyse_text(text):\n        rv = PhpLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):\n            rv += 0.5\n        return rv\n\n\nclass XmlPhpLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `PhpLexer` that highlights unhandled data with the `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+PHP'\n    aliases = ['xml+php']\n    alias_filenames = ['*.xml', '*.php', '*.php[345]']\n    mimetypes = ['application/xml+php']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, PhpLexer, **options)\n\n    def analyse_text(text):\n        rv = PhpLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass CssPhpLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `PhpLexer` which highlights unmatched data with the `CssLexer`.\n    \"\"\"\n\n    name = 'CSS+PHP'\n    aliases = ['css+php']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+php']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, PhpLexer, **options)\n\n    def analyse_text(text):\n        return PhpLexer.analyse_text(text) - 0.05\n\n\nclass JavascriptPhpLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of `PhpLexer` which highlights unmatched data with the\n    `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+PHP'\n    aliases = ['js+php', 'javascript+php']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+php',\n                 'text/x-javascript+php',\n                 'text/javascript+php']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, PhpLexer, **options)\n\n    def analyse_text(text):\n        return PhpLexer.analyse_text(text)\n\n\nclass HtmlSmartyLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `SmartyLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    Nested Javascript and CSS is highlighted too.\n    \"\"\"\n\n    name = 'HTML+Smarty'\n    aliases = ['html+smarty']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml', '*.tpl']\n    mimetypes = ['text/html+smarty']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, SmartyLexer, **options)\n\n    def analyse_text(text):\n        rv = SmartyLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):\n            rv += 0.5\n        return rv\n\n\nclass XmlSmartyLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `SmartyLexer` that highlights unlexed data with the\n    `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+Smarty'\n    aliases = ['xml+smarty']\n    alias_filenames = ['*.xml', '*.tpl']\n    mimetypes = ['application/xml+smarty']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, SmartyLexer, **options)\n\n    def analyse_text(text):\n        rv = SmartyLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass CssSmartyLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `SmartyLexer` that highlights unlexed data with the\n    `CssLexer`.\n    \"\"\"\n\n    name = 'CSS+Smarty'\n    aliases = ['css+smarty']\n    alias_filenames = ['*.css', '*.tpl']\n    mimetypes = ['text/css+smarty']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, SmartyLexer, **options)\n\n    def analyse_text(text):\n        return SmartyLexer.analyse_text(text) - 0.05\n\n\nclass JavascriptSmartyLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `SmartyLexer` that highlights unlexed data with the\n    `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+Smarty'\n    aliases = ['js+smarty', 'javascript+smarty']\n    alias_filenames = ['*.js', '*.tpl']\n    mimetypes = ['application/x-javascript+smarty',\n                 'text/x-javascript+smarty',\n                 'text/javascript+smarty']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, SmartyLexer, **options)\n\n    def analyse_text(text):\n        return SmartyLexer.analyse_text(text) - 0.05\n\n\nclass HtmlDjangoLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    Nested Javascript and CSS is highlighted too.\n    \"\"\"\n\n    name = 'HTML+Django/Jinja'\n    aliases = ['html+django', 'html+jinja', 'htmldjango']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml']\n    mimetypes = ['text/html+django', 'text/html+jinja']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, DjangoLexer, **options)\n\n    def analyse_text(text):\n        rv = DjangoLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):\n            rv += 0.5\n        return rv\n\n\nclass XmlDjangoLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `XmlLexer`.\n    \"\"\"\n\n    name = 'XML+Django/Jinja'\n    aliases = ['xml+django', 'xml+jinja']\n    alias_filenames = ['*.xml']\n    mimetypes = ['application/xml+django', 'application/xml+jinja']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, DjangoLexer, **options)\n\n    def analyse_text(text):\n        rv = DjangoLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass CssDjangoLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `CssLexer`.\n    \"\"\"\n\n    name = 'CSS+Django/Jinja'\n    aliases = ['css+django', 'css+jinja']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+django', 'text/css+jinja']\n\n    def __init__(self, **options):\n        super().__init__(CssLexer, DjangoLexer, **options)\n\n    def analyse_text(text):\n        return DjangoLexer.analyse_text(text) - 0.05\n\n\nclass JavascriptDjangoLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `JavascriptLexer`.\n    \"\"\"\n\n    name = 'JavaScript+Django/Jinja'\n    aliases = ['js+django', 'javascript+django',\n               'js+jinja', 'javascript+jinja']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+django',\n                 'application/x-javascript+jinja',\n                 'text/x-javascript+django',\n                 'text/x-javascript+jinja',\n                 'text/javascript+django',\n                 'text/javascript+jinja']\n\n    def __init__(self, **options):\n        super().__init__(JavascriptLexer, DjangoLexer, **options)\n\n    def analyse_text(text):\n        return DjangoLexer.analyse_text(text) - 0.05\n\n\nclass JspRootLexer(RegexLexer):\n    \"\"\"\n    Base for the `JspLexer`. Yields `Token.Other` for area outside of\n    JSP tags.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    tokens = {\n        'root': [\n            (r'<%\\S?', Keyword, 'sec'),\n            # FIXME: I want to make these keywords but still parse attributes.\n            (r'</?jsp:(forward|getProperty|include|plugin|setProperty|useBean).*?>',\n             Keyword),\n            (r'[^<]+', Other),\n            (r'<', Other),\n        ],\n        'sec': [\n            (r'%>', Keyword, '#pop'),\n            # note: '\\w\\W' != '.' without DOTALL.\n            (r'[\\w\\W]+?(?=%>|\\Z)', using(JavaLexer)),\n        ],\n    }\n\n\nclass JspLexer(DelegatingLexer):\n    \"\"\"\n    Lexer for Java Server Pages.\n\n    .. versionadded:: 0.7\n    \"\"\"\n    name = 'Java Server Page'\n    aliases = ['jsp']\n    filenames = ['*.jsp']\n    mimetypes = ['application/x-jsp']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, JspRootLexer, **options)\n\n    def analyse_text(text):\n        rv = JavaLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        if '<%' in text and '%>' in text:\n            rv += 0.1\n        return rv\n\n\nclass EvoqueLexer(RegexLexer):\n    \"\"\"\n    For files using the Evoque templating system.\n\n    .. versionadded:: 1.1\n    \"\"\"\n    name = 'Evoque'\n    aliases = ['evoque']\n    filenames = ['*.evoque']\n    mimetypes = ['application/x-evoque']\n\n    flags = re.DOTALL\n\n    tokens = {\n        'root': [\n            (r'[^#$]+', Other),\n            (r'#\\[', Comment.Multiline, 'comment'),\n            (r'\\$\\$', Other),\n            # svn keywords\n            (r'\\$\\w+:[^$\\n]*\\$', Comment.Multiline),\n            # directives: begin, end\n            (r'(\\$)(begin|end)(\\{(%)?)(.*?)((?(4)%)\\})',\n             bygroups(Punctuation, Name.Builtin, Punctuation, None,\n                      String, Punctuation)),\n            # directives: evoque, overlay\n            # see doc for handling first name arg: /directives/evoque/\n            # + minor inconsistency: the \"name\" in e.g. $overlay{name=site_base}\n            # should be using(PythonLexer), not passed out as String\n            (r'(\\$)(evoque|overlay)(\\{(%)?)(\\s*[#\\w\\-\"\\'.]+)?'\n             r'(.*?)((?(4)%)\\})',\n             bygroups(Punctuation, Name.Builtin, Punctuation, None,\n                      String, using(PythonLexer), Punctuation)),\n            # directives: if, for, prefer, test\n            (r'(\\$)(\\w+)(\\{(%)?)(.*?)((?(4)%)\\})',\n             bygroups(Punctuation, Name.Builtin, Punctuation, None,\n                      using(PythonLexer), Punctuation)),\n            # directive clauses (no {} expression)\n            (r'(\\$)(else|rof|fi)', bygroups(Punctuation, Name.Builtin)),\n            # expressions\n            (r'(\\$\\{(%)?)(.*?)((!)(.*?))?((?(2)%)\\})',\n             bygroups(Punctuation, None, using(PythonLexer),\n                      Name.Builtin, None, None, Punctuation)),\n            (r'#', Other),\n        ],\n        'comment': [\n            (r'[^\\]#]', Comment.Multiline),\n            (r'#\\[', Comment.Multiline, '#push'),\n            (r'\\]#', Comment.Multiline, '#pop'),\n            (r'[\\]#]', Comment.Multiline)\n        ],\n    }\n\n    def analyse_text(text):\n        \"\"\"Evoque templates use $evoque, which is unique.\"\"\"\n        if '$evoque' in text:\n            return 1\n\nclass EvoqueHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `EvoqueLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    .. versionadded:: 1.1\n    \"\"\"\n    name = 'HTML+Evoque'\n    aliases = ['html+evoque']\n    filenames = ['*.html']\n    mimetypes = ['text/html+evoque']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, EvoqueLexer, **options)\n\n    def analyse_text(text):\n        return EvoqueLexer.analyse_text(text)\n\n\nclass EvoqueXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `EvoqueLexer` that highlights unlexed data with the\n    `XmlLexer`.\n\n    .. versionadded:: 1.1\n    \"\"\"\n    name = 'XML+Evoque'\n    aliases = ['xml+evoque']\n    filenames = ['*.xml']\n    mimetypes = ['application/xml+evoque']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, EvoqueLexer, **options)\n\n    def analyse_text(text):\n        return EvoqueLexer.analyse_text(text)\n\n\nclass ColdfusionLexer(RegexLexer):\n    \"\"\"\n    Coldfusion statements\n    \"\"\"\n    name = 'cfstatement'\n    aliases = ['cfs']\n    filenames = []\n    mimetypes = []\n    flags = re.IGNORECASE\n\n    tokens = {\n        'root': [\n            (r'//.*?\\n', Comment.Single),\n            (r'/\\*(?:.|\\n)*?\\*/', Comment.Multiline),\n            (r'\\+\\+|--', Operator),\n            (r'[-+*/^&=!]', Operator),\n            (r'<=|>=|<|>|==', Operator),\n            (r'mod\\b', Operator),\n            (r'(eq|lt|gt|lte|gte|not|is|and|or)\\b', Operator),\n            (r'\\|\\||&&', Operator),\n            (r'\\?', Operator),\n            (r'\"', String.Double, 'string'),\n            # There is a special rule for allowing html in single quoted\n            # strings, evidently.\n            (r\"'.*?'\", String.Single),\n            (r'\\d+', Number),\n            (r'(if|else|len|var|xml|default|break|switch|component|property|function|do|'\n             r'try|catch|in|continue|for|return|while|required|any|array|binary|boolean|'\n             r'component|date|guid|numeric|query|string|struct|uuid|case)\\b', Keyword),\n            (r'(true|false|null)\\b', Keyword.Constant),\n            (r'(application|session|client|cookie|super|this|variables|arguments)\\b',\n             Name.Constant),\n            (r'([a-z_$][\\w.]*)(\\s*)(\\()',\n             bygroups(Name.Function, Text, Punctuation)),\n            (r'[a-z_$][\\w.]*', Name.Variable),\n            (r'[()\\[\\]{};:,.\\\\]', Punctuation),\n            (r'\\s+', Text),\n        ],\n        'string': [\n            (r'\"\"', String.Double),\n            (r'#.+?#', String.Interp),\n            (r'[^\"#]+', String.Double),\n            (r'#', String.Double),\n            (r'\"', String.Double, '#pop'),\n        ],\n    }\n\n\nclass ColdfusionMarkupLexer(RegexLexer):\n    \"\"\"\n    Coldfusion markup only\n    \"\"\"\n    name = 'Coldfusion'\n    aliases = ['cf']\n    filenames = []\n    mimetypes = []\n\n    tokens = {\n        'root': [\n            (r'[^<]+', Other),\n            include('tags'),\n            (r'<[^<>]*', Other),\n        ],\n        'tags': [\n            (r'<!---', Comment.Multiline, 'cfcomment'),\n            (r'(?s)<!--.*?-->', Comment),\n            (r'<cfoutput.*?>', Name.Builtin, 'cfoutput'),\n            (r'(?s)(<cfscript.*?>)(.+?)(</cfscript.*?>)',\n             bygroups(Name.Builtin, using(ColdfusionLexer), Name.Builtin)),\n            # negative lookbehind is for strings with embedded >\n            (r'(?s)(</?cf(?:component|include|if|else|elseif|loop|return|'\n             r'dbinfo|dump|abort|location|invoke|throw|file|savecontent|'\n             r'mailpart|mail|header|content|zip|image|lock|argument|try|'\n             r'catch|break|directory|http|set|function|param)\\b)(.*?)((?<!\\\\)>)',\n             bygroups(Name.Builtin, using(ColdfusionLexer), Name.Builtin)),\n        ],\n        'cfoutput': [\n            (r'[^#<]+', Other),\n            (r'(#)(.*?)(#)', bygroups(Punctuation, using(ColdfusionLexer),\n                                      Punctuation)),\n            # (r'<cfoutput.*?>', Name.Builtin, '#push'),\n            (r'</cfoutput.*?>', Name.Builtin, '#pop'),\n            include('tags'),\n            (r'(?s)<[^<>]*', Other),\n            (r'#', Other),\n        ],\n        'cfcomment': [\n            (r'<!---', Comment.Multiline, '#push'),\n            (r'--->', Comment.Multiline, '#pop'),\n            (r'([^<-]|<(?!!---)|-(?!-->))+', Comment.Multiline),\n        ],\n    }\n\n\nclass ColdfusionHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Coldfusion markup in html\n    \"\"\"\n    name = 'Coldfusion HTML'\n    aliases = ['cfm']\n    filenames = ['*.cfm', '*.cfml']\n    mimetypes = ['application/x-coldfusion']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, ColdfusionMarkupLexer, **options)\n\n\nclass ColdfusionCFCLexer(DelegatingLexer):\n    \"\"\"\n    Coldfusion markup/script components\n\n    .. versionadded:: 2.0\n    \"\"\"\n    name = 'Coldfusion CFC'\n    aliases = ['cfc']\n    filenames = ['*.cfc']\n    mimetypes = []\n\n    def __init__(self, **options):\n        super().__init__(ColdfusionHtmlLexer, ColdfusionLexer, **options)\n\n\nclass SspLexer(DelegatingLexer):\n    \"\"\"\n    Lexer for Scalate Server Pages.\n\n    .. versionadded:: 1.4\n    \"\"\"\n    name = 'Scalate Server Page'\n    aliases = ['ssp']\n    filenames = ['*.ssp']\n    mimetypes = ['application/x-ssp']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, JspRootLexer, **options)\n\n    def analyse_text(text):\n        rv = 0.0\n        if re.search(r'val \\w+\\s*:', text):\n            rv += 0.6\n        if looks_like_xml(text):\n            rv += 0.2\n        if '<%' in text and '%>' in text:\n            rv += 0.1\n        return rv\n\n\nclass TeaTemplateRootLexer(RegexLexer):\n    \"\"\"\n    Base for the `TeaTemplateLexer`. Yields `Token.Other` for area outside of\n    code blocks.\n\n    .. versionadded:: 1.5\n    \"\"\"\n\n    tokens = {\n        'root': [\n            (r'<%\\S?', Keyword, 'sec'),\n            (r'[^<]+', Other),\n            (r'<', Other),\n        ],\n        'sec': [\n            (r'%>', Keyword, '#pop'),\n            # note: '\\w\\W' != '.' without DOTALL.\n            (r'[\\w\\W]+?(?=%>|\\Z)', using(TeaLangLexer)),\n        ],\n    }\n\n\nclass TeaTemplateLexer(DelegatingLexer):\n    \"\"\"\n    Lexer for `Tea Templates <http://teatrove.org/>`_.\n\n    .. versionadded:: 1.5\n    \"\"\"\n    name = 'Tea'\n    aliases = ['tea']\n    filenames = ['*.tea']\n    mimetypes = ['text/x-tea']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, TeaTemplateRootLexer, **options)\n\n    def analyse_text(text):\n        rv = TeaLangLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        if '<%' in text and '%>' in text:\n            rv += 0.1\n        return rv\n\n\nclass LassoHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `LassoLexer` which highlights unhandled data with the\n    `HtmlLexer`.\n\n    Nested JavaScript and CSS is also highlighted.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'HTML+Lasso'\n    aliases = ['html+lasso']\n    alias_filenames = ['*.html', '*.htm', '*.xhtml', '*.lasso', '*.lasso[89]',\n                       '*.incl', '*.inc', '*.las']\n    mimetypes = ['text/html+lasso',\n                 'application/x-httpd-lasso',\n                 'application/x-httpd-lasso[89]']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, LassoLexer, **options)\n\n    def analyse_text(text):\n        rv = LassoLexer.analyse_text(text) - 0.01\n        if html_doctype_matches(text):  # same as HTML lexer\n            rv += 0.5\n        return rv\n\n\nclass LassoXmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `LassoLexer` which highlights unhandled data with the\n    `XmlLexer`.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'XML+Lasso'\n    aliases = ['xml+lasso']\n    alias_filenames = ['*.xml', '*.lasso', '*.lasso[89]',\n                       '*.incl', '*.inc', '*.las']\n    mimetypes = ['application/xml+lasso']\n\n    def __init__(self, **options):\n        super().__init__(XmlLexer, LassoLexer, **options)\n\n    def analyse_text(text):\n        rv = LassoLexer.analyse_text(text) - 0.01\n        if looks_like_xml(text):\n            rv += 0.4\n        return rv\n\n\nclass LassoCssLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `LassoLexer` which highlights unhandled data with the\n    `CssLexer`.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'CSS+Lasso'\n    aliases = ['css+lasso']\n    alias_filenames = ['*.css']\n    mimetypes = ['text/css+lasso']\n\n    def __init__(self, **options):\n        options['requiredelimiters'] = True\n        super().__init__(CssLexer, LassoLexer, **options)\n\n    def analyse_text(text):\n        rv = LassoLexer.analyse_text(text) - 0.05\n        if re.search(r'\\w+:[^;]+;', text):\n            rv += 0.1\n        if 'padding:' in text:\n            rv += 0.1\n        return rv\n\n\nclass LassoJavascriptLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `LassoLexer` which highlights unhandled data with the\n    `JavascriptLexer`.\n\n    .. versionadded:: 1.6\n    \"\"\"\n\n    name = 'JavaScript+Lasso'\n    aliases = ['js+lasso', 'javascript+lasso']\n    alias_filenames = ['*.js']\n    mimetypes = ['application/x-javascript+lasso',\n                 'text/x-javascript+lasso',\n                 'text/javascript+lasso']\n\n    def __init__(self, **options):\n        options['requiredelimiters'] = True\n        super().__init__(JavascriptLexer, LassoLexer, **options)\n\n    def analyse_text(text):\n        rv = LassoLexer.analyse_text(text) - 0.05\n        return rv\n\n\nclass HandlebarsLexer(RegexLexer):\n    \"\"\"\n    Generic `handlebars <http://handlebarsjs.com/>` template lexer.\n\n    Highlights only the Handlebars template tags (stuff between `{{` and `}}`).\n    Everything else is left for a delegating lexer.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = \"Handlebars\"\n    aliases = ['handlebars']\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Other),\n\n            # Comment start {{!  }} or {{!--\n            (r'\\{\\{!.*\\}\\}', Comment),\n\n            # HTML Escaping open {{{expression\n            (r'(\\{\\{\\{)(\\s*)', bygroups(Comment.Special, Text), 'tag'),\n\n            # {{blockOpen {{#blockOpen {{/blockClose with optional tilde ~\n            (r'(\\{\\{)([#~/]+)([^\\s}]*)',\n             bygroups(Comment.Preproc, Number.Attribute, Number.Attribute), 'tag'),\n            (r'(\\{\\{)(\\s*)', bygroups(Comment.Preproc, Text), 'tag'),\n        ],\n\n        'tag': [\n            (r'\\s+', Text),\n            # HTML Escaping close }}}\n            (r'\\}\\}\\}', Comment.Special, '#pop'),\n            # blockClose}}, includes optional tilde ~\n            (r'(~?)(\\}\\})', bygroups(Number, Comment.Preproc), '#pop'),\n\n            # {{opt=something}}\n            (r'([^\\s}]+)(=)', bygroups(Name.Attribute, Operator)),\n\n            # Partials {{> ...}}\n            (r'(>)(\\s*)(@partial-block)', bygroups(Keyword, Text, Keyword)),\n            (r'(#?>)(\\s*)([\\w-]+)', bygroups(Keyword, Text, Name.Variable)),\n            (r'(>)(\\s*)(\\()', bygroups(Keyword, Text, Punctuation),\n             'dynamic-partial'),\n\n            include('generic'),\n        ],\n        'dynamic-partial': [\n            (r'\\s+', Text),\n            (r'\\)', Punctuation, '#pop'),\n\n            (r'(lookup)(\\s+)(\\.|this)(\\s+)', bygroups(Keyword, Text,\n                                                      Name.Variable, Text)),\n            (r'(lookup)(\\s+)(\\S+)', bygroups(Keyword, Text,\n                                             using(this, state='variable'))),\n            (r'[\\w-]+', Name.Function),\n\n            include('generic'),\n        ],\n        'variable': [\n            (r'[()/@a-zA-Z][\\w-]*', Name.Variable),\n            (r'\\.[\\w-]+', Name.Variable),\n            (r'(this\\/|\\.\\/|(\\.\\.\\/)+)[\\w-]+', Name.Variable),\n        ],\n        'generic': [\n            include('variable'),\n\n            # borrowed from DjangoLexer\n            (r':?\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\":?'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n        ]\n    }\n\n\nclass HandlebarsHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `HandlebarsLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = \"HTML+Handlebars\"\n    aliases = [\"html+handlebars\"]\n    filenames = ['*.handlebars', '*.hbs']\n    mimetypes = ['text/html+handlebars', 'text/x-handlebars-template']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, HandlebarsLexer, **options)\n\n\nclass YamlJinjaLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `DjangoLexer` that highlights unlexed data with the\n    `YamlLexer`.\n\n    Commonly used in Saltstack salt states.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'YAML+Jinja'\n    aliases = ['yaml+jinja', 'salt', 'sls']\n    filenames = ['*.sls']\n    mimetypes = ['text/x-yaml+jinja', 'text/x-sls']\n\n    def __init__(self, **options):\n        super().__init__(YamlLexer, DjangoLexer, **options)\n\n\nclass LiquidLexer(RegexLexer):\n    \"\"\"\n    Lexer for `Liquid templates\n    <http://www.rubydoc.info/github/Shopify/liquid>`_.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    name = 'liquid'\n    aliases = ['liquid']\n    filenames = ['*.liquid']\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Text),\n            # tags and block tags\n            (r'(\\{%)(\\s*)', bygroups(Punctuation, Whitespace), 'tag-or-block'),\n            # output tags\n            (r'(\\{\\{)(\\s*)([^\\s}]+)',\n             bygroups(Punctuation, Whitespace, using(this, state = 'generic')),\n             'output'),\n            (r'\\{', Text)\n        ],\n\n        'tag-or-block': [\n            # builtin logic blocks\n            (r'(if|unless|elsif|case)(?=\\s+)', Keyword.Reserved, 'condition'),\n            (r'(when)(\\s+)', bygroups(Keyword.Reserved, Whitespace),\n             combined('end-of-block', 'whitespace', 'generic')),\n            (r'(else)(\\s*)(%\\})',\n             bygroups(Keyword.Reserved, Whitespace, Punctuation), '#pop'),\n\n            # other builtin blocks\n            (r'(capture)(\\s+)([^\\s%]+)(\\s*)(%\\})',\n             bygroups(Name.Tag, Whitespace, using(this, state = 'variable'),\n                      Whitespace, Punctuation), '#pop'),\n            (r'(comment)(\\s*)(%\\})',\n             bygroups(Name.Tag, Whitespace, Punctuation), 'comment'),\n            (r'(raw)(\\s*)(%\\})',\n             bygroups(Name.Tag, Whitespace, Punctuation), 'raw'),\n\n            # end of block\n            (r'(end(case|unless|if))(\\s*)(%\\})',\n             bygroups(Keyword.Reserved, None, Whitespace, Punctuation), '#pop'),\n            (r'(end([^\\s%]+))(\\s*)(%\\})',\n             bygroups(Name.Tag, None, Whitespace, Punctuation), '#pop'),\n\n            # builtin tags (assign and include are handled together with usual tags)\n            (r'(cycle)(\\s+)(?:([^\\s:]*)(:))?(\\s*)',\n             bygroups(Name.Tag, Whitespace,\n                      using(this, state='generic'), Punctuation, Whitespace),\n             'variable-tag-markup'),\n\n            # other tags or blocks\n            (r'([^\\s%]+)(\\s*)', bygroups(Name.Tag, Whitespace), 'tag-markup')\n        ],\n\n        'output': [\n            include('whitespace'),\n            (r'\\}\\}', Punctuation, '#pop'),  # end of output\n\n            (r'\\|', Punctuation, 'filters')\n        ],\n\n        'filters': [\n            include('whitespace'),\n            (r'\\}\\}', Punctuation, ('#pop', '#pop')),  # end of filters and output\n\n            (r'([^\\s|:]+)(:?)(\\s*)',\n             bygroups(Name.Function, Punctuation, Whitespace), 'filter-markup')\n        ],\n\n        'filter-markup': [\n            (r'\\|', Punctuation, '#pop'),\n            include('end-of-tag'),\n            include('default-param-markup')\n        ],\n\n        'condition': [\n            include('end-of-block'),\n            include('whitespace'),\n\n            (r'([^\\s=!><]+)(\\s*)([=!><]=?)(\\s*)(\\S+)(\\s*)(%\\})',\n             bygroups(using(this, state = 'generic'), Whitespace, Operator,\n                      Whitespace, using(this, state = 'generic'), Whitespace,\n                      Punctuation)),\n            (r'\\b!', Operator),\n            (r'\\bnot\\b', Operator.Word),\n            (r'([\\w.\\'\"]+)(\\s+)(contains)(\\s+)([\\w.\\'\"]+)',\n             bygroups(using(this, state = 'generic'), Whitespace, Operator.Word,\n                      Whitespace, using(this, state = 'generic'))),\n\n            include('generic'),\n            include('whitespace')\n        ],\n\n        'generic-value': [\n            include('generic'),\n            include('end-at-whitespace')\n        ],\n\n        'operator': [\n            (r'(\\s*)((=|!|>|<)=?)(\\s*)',\n             bygroups(Whitespace, Operator, None, Whitespace), '#pop'),\n            (r'(\\s*)(\\bcontains\\b)(\\s*)',\n             bygroups(Whitespace, Operator.Word, Whitespace), '#pop'),\n        ],\n\n        'end-of-tag': [\n            (r'\\}\\}', Punctuation, '#pop')\n        ],\n\n        'end-of-block': [\n            (r'%\\}', Punctuation, ('#pop', '#pop'))\n        ],\n\n        'end-at-whitespace': [\n            (r'\\s+', Whitespace, '#pop')\n        ],\n\n        # states for unknown markup\n        'param-markup': [\n            include('whitespace'),\n            # params with colons or equals\n            (r'([^\\s=:]+)(\\s*)(=|:)',\n             bygroups(Name.Attribute, Whitespace, Operator)),\n            # explicit variables\n            (r'(\\{\\{)(\\s*)([^\\s}])(\\s*)(\\}\\})',\n             bygroups(Punctuation, Whitespace, using(this, state = 'variable'),\n                      Whitespace, Punctuation)),\n\n            include('string'),\n            include('number'),\n            include('keyword'),\n            (r',', Punctuation)\n        ],\n\n        'default-param-markup': [\n            include('param-markup'),\n            (r'.', Text)  # fallback for switches / variables / un-quoted strings / ...\n        ],\n\n        'variable-param-markup': [\n            include('param-markup'),\n            include('variable'),\n            (r'.', Text)  # fallback\n        ],\n\n        'tag-markup': [\n            (r'%\\}', Punctuation, ('#pop', '#pop')),  # end of tag\n            include('default-param-markup')\n        ],\n\n        'variable-tag-markup': [\n            (r'%\\}', Punctuation, ('#pop', '#pop')),  # end of tag\n            include('variable-param-markup')\n        ],\n\n        # states for different values types\n        'keyword': [\n            (r'\\b(false|true)\\b', Keyword.Constant)\n        ],\n\n        'variable': [\n            (r'[a-zA-Z_]\\w*', Name.Variable),\n            (r'(?<=\\w)\\.(?=\\w)', Punctuation)\n        ],\n\n        'string': [\n            (r\"'[^']*'\", String.Single),\n            (r'\"[^\"]*\"', String.Double)\n        ],\n\n        'number': [\n            (r'\\d+\\.\\d+', Number.Float),\n            (r'\\d+', Number.Integer)\n        ],\n\n        'generic': [  # decides for variable, string, keyword or number\n            include('keyword'),\n            include('string'),\n            include('number'),\n            include('variable')\n        ],\n\n        'whitespace': [\n            (r'[ \\t]+', Whitespace)\n        ],\n\n        # states for builtin blocks\n        'comment': [\n            (r'(\\{%)(\\s*)(endcomment)(\\s*)(%\\})',\n             bygroups(Punctuation, Whitespace, Name.Tag, Whitespace,\n                      Punctuation), ('#pop', '#pop')),\n            (r'.', Comment)\n        ],\n\n        'raw': [\n            (r'[^{]+', Text),\n            (r'(\\{%)(\\s*)(endraw)(\\s*)(%\\})',\n             bygroups(Punctuation, Whitespace, Name.Tag, Whitespace,\n                      Punctuation), '#pop'),\n            (r'\\{', Text)\n        ],\n    }\n\n\nclass TwigLexer(RegexLexer):\n    \"\"\"\n    `Twig <http://twig.sensiolabs.org/>`_ template lexer.\n\n    It just highlights Twig code between the preprocessor directives,\n    other data is left untouched by the lexer.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = 'Twig'\n    aliases = ['twig']\n    mimetypes = ['application/x-twig']\n\n    flags = re.M | re.S\n\n    # Note that a backslash is included in the following two patterns\n    # PHP uses a backslash as a namespace separator\n    _ident_char = r'[\\\\\\w-]|[^\\x00-\\x7f]'\n    _ident_begin = r'(?:[\\\\_a-z]|[^\\x00-\\x7f])'\n    _ident_end = r'(?:' + _ident_char + ')*'\n    _ident_inner = _ident_begin + _ident_end\n\n    tokens = {\n        'root': [\n            (r'[^{]+', Other),\n            (r'\\{\\{', Comment.Preproc, 'var'),\n            # twig comments\n            (r'\\{\\#.*?\\#\\}', Comment),\n            # raw twig blocks\n            (r'(\\{%)(-?\\s*)(raw)(\\s*-?)(%\\})(.*?)'\n             r'(\\{%)(-?\\s*)(endraw)(\\s*-?)(%\\})',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Comment.Preproc,\n                      Other, Comment.Preproc, Text, Keyword, Text,\n                      Comment.Preproc)),\n            (r'(\\{%)(-?\\s*)(verbatim)(\\s*-?)(%\\})(.*?)'\n             r'(\\{%)(-?\\s*)(endverbatim)(\\s*-?)(%\\})',\n             bygroups(Comment.Preproc, Text, Keyword, Text, Comment.Preproc,\n                      Other, Comment.Preproc, Text, Keyword, Text,\n                      Comment.Preproc)),\n            # filter blocks\n            (r'(\\{%%)(-?\\s*)(filter)(\\s+)(%s)' % _ident_inner,\n             bygroups(Comment.Preproc, Text, Keyword, Text, Name.Function),\n             'tag'),\n            (r'(\\{%)(-?\\s*)([a-zA-Z_]\\w*)',\n             bygroups(Comment.Preproc, Text, Keyword), 'tag'),\n            (r'\\{', Other),\n        ],\n        'varnames': [\n            (r'(\\|)(\\s*)(%s)' % _ident_inner,\n             bygroups(Operator, Text, Name.Function)),\n            (r'(is)(\\s+)(not)?(\\s*)(%s)' % _ident_inner,\n             bygroups(Keyword, Text, Keyword, Text, Name.Function)),\n            (r'(?i)(true|false|none|null)\\b', Keyword.Pseudo),\n            (r'(in|not|and|b-and|or|b-or|b-xor|is'\n             r'if|elseif|else|import'\n             r'constant|defined|divisibleby|empty|even|iterable|odd|sameas'\n             r'matches|starts\\s+with|ends\\s+with)\\b',\n             Keyword),\n            (r'(loop|block|parent)\\b', Name.Builtin),\n            (_ident_inner, Name.Variable),\n            (r'\\.' + _ident_inner, Name.Variable),\n            (r'\\.[0-9]+', Number),\n            (r':?\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\":?'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r'([{}()\\[\\]+\\-*/,:~%]|\\.\\.|\\?|:|\\*\\*|\\/\\/|!=|[><=]=?)', Operator),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n        ],\n        'var': [\n            (r'\\s+', Text),\n            (r'(-?)(\\}\\})', bygroups(Text, Comment.Preproc), '#pop'),\n            include('varnames')\n        ],\n        'tag': [\n            (r'\\s+', Text),\n            (r'(-?)(%\\})', bygroups(Text, Comment.Preproc), '#pop'),\n            include('varnames'),\n            (r'.', Punctuation),\n        ],\n    }\n\n\nclass TwigHtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `TwigLexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = \"HTML+Twig\"\n    aliases = [\"html+twig\"]\n    filenames = ['*.twig']\n    mimetypes = ['text/html+twig']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, TwigLexer, **options)\n\n\nclass Angular2Lexer(RegexLexer):\n    \"\"\"\n    Generic\n    `angular2 <http://victorsavkin.com/post/119943127151/angular-2-template-syntax>`_\n    template lexer.\n\n    Highlights only the Angular template tags (stuff between `{{` and `}}` and\n    special attributes: '(event)=', '[property]=', '[(twoWayBinding)]=').\n    Everything else is left for a delegating lexer.\n\n    .. versionadded:: 2.1\n    \"\"\"\n\n    name = \"Angular2\"\n    aliases = ['ng2']\n\n    tokens = {\n        'root': [\n            (r'[^{([*#]+', Other),\n\n            # {{meal.name}}\n            (r'(\\{\\{)(\\s*)', bygroups(Comment.Preproc, Text), 'ngExpression'),\n\n            # (click)=\"deleteOrder()\"; [value]=\"test\"; [(twoWayTest)]=\"foo.bar\"\n            (r'([([]+)([\\w:.-]+)([\\])]+)(\\s*)(=)(\\s*)',\n             bygroups(Punctuation, Name.Attribute, Punctuation, Text, Operator, Text),\n             'attr'),\n            (r'([([]+)([\\w:.-]+)([\\])]+)(\\s*)',\n             bygroups(Punctuation, Name.Attribute, Punctuation, Text)),\n\n            # *ngIf=\"...\"; #f=\"ngForm\"\n            (r'([*#])([\\w:.-]+)(\\s*)(=)(\\s*)',\n             bygroups(Punctuation, Name.Attribute, Text, Operator, Text), 'attr'),\n            (r'([*#])([\\w:.-]+)(\\s*)',\n             bygroups(Punctuation, Name.Attribute, Text)),\n        ],\n\n        'ngExpression': [\n            (r'\\s+(\\|\\s+)?', Text),\n            (r'\\}\\}', Comment.Preproc, '#pop'),\n\n            # Literals\n            (r':?(true|false)', String.Boolean),\n            (r':?\"(\\\\\\\\|\\\\[^\\\\]|[^\"\\\\])*\"', String.Double),\n            (r\":?'(\\\\\\\\|\\\\[^\\\\]|[^'\\\\])*'\", String.Single),\n            (r\"[0-9](\\.[0-9]*)?(eE[+-][0-9])?[flFLdD]?|\"\n             r\"0[xX][0-9a-fA-F]+[Ll]?\", Number),\n\n            # Variabletext\n            (r'[a-zA-Z][\\w-]*(\\(.*\\))?', Name.Variable),\n            (r'\\.[\\w-]+(\\(.*\\))?', Name.Variable),\n\n            # inline If\n            (r'(\\?)(\\s*)([^}\\s]+)(\\s*)(:)(\\s*)([^}\\s]+)(\\s*)',\n             bygroups(Operator, Text, String, Text, Operator, Text, String, Text)),\n        ],\n        'attr': [\n            ('\".*?\"', String, '#pop'),\n            (\"'.*?'\", String, '#pop'),\n            (r'[^\\s>]+', String, '#pop'),\n        ],\n    }\n\n\nclass Angular2HtmlLexer(DelegatingLexer):\n    \"\"\"\n    Subclass of the `Angular2Lexer` that highlights unlexed data with the\n    `HtmlLexer`.\n\n    .. versionadded:: 2.0\n    \"\"\"\n\n    name = \"HTML + Angular2\"\n    aliases = [\"html+ng2\"]\n    filenames = ['*.ng2']\n\n    def __init__(self, **options):\n        super().__init__(HtmlLexer, Angular2Lexer, **options)\n", "# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.varnish\n    ~~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for Varnish configuration\n\n    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pygments.lexer import RegexLexer, include, bygroups, using, this, \\\n    inherit, words\nfrom pygments.token import Text, Comment, Operator, Keyword, Name, String, \\\n    Number, Punctuation, Literal\n\n__all__ = ['VCLLexer', 'VCLSnippetLexer']\n\n\nclass VCLLexer(RegexLexer):\n    \"\"\"\n    For Varnish Configuration Language (VCL).\n\n    .. versionadded:: 2.2\n    \"\"\"\n    name = 'VCL'\n    aliases = ['vcl']\n    filenames = ['*.vcl']\n    mimetypes = ['text/x-vclsrc']\n\n    def analyse_text(text):\n        # If the very first line is 'vcl 4.0;' it's pretty much guaranteed\n        # that this is VCL\n        if text.startswith('vcl 4.0;'):\n            return 1.0\n        # Skip over comments and blank lines\n        # This is accurate enough that returning 0.9 is reasonable.\n        # Almost no VCL files start without some comments.\n        elif '\\nvcl 4.0;' in text[:1000]:\n            return 0.9\n\n    tokens = {\n        'probe': [\n            include('whitespace'),\n            include('comments'),\n            (r'(\\.\\w+)(\\s*=\\s*)([^;]*)(;)',\n             bygroups(Name.Attribute, Operator, using(this), Punctuation)),\n            (r'\\}', Punctuation, '#pop'),\n        ],\n        'acl': [\n            include('whitespace'),\n            include('comments'),\n            (r'[!/]+', Operator),\n            (r';', Punctuation),\n            (r'\\d+', Number),\n            (r'\\}', Punctuation, '#pop'),\n        ],\n        'backend': [\n            include('whitespace'),\n            (r'(\\.probe)(\\s*=\\s*)(\\w+)(;)',\n             bygroups(Name.Attribute, Operator, Name.Variable.Global, Punctuation)),\n            (r'(\\.probe)(\\s*=\\s*)(\\{)',\n             bygroups(Name.Attribute, Operator, Punctuation), 'probe'),\n            (r'(\\.\\w+\\b)(\\s*=\\s*)([^;\\s]*)(\\s*;)',\n             bygroups(Name.Attribute, Operator, using(this), Punctuation)),\n            (r'\\{', Punctuation, '#push'),\n            (r'\\}', Punctuation, '#pop'),\n        ],\n        'statements': [\n            (r'(\\d\\.)?\\d+[sdwhmy]', Literal.Date),\n            (r'(\\d\\.)?\\d+ms', Literal.Date),\n            (r'(vcl_pass|vcl_hash|vcl_hit|vcl_init|vcl_backend_fetch|vcl_pipe|'\n             r'vcl_backend_response|vcl_synth|vcl_deliver|vcl_backend_error|'\n             r'vcl_fini|vcl_recv|vcl_purge|vcl_miss)\\b', Name.Function),\n            (r'(pipe|retry|hash|synth|deliver|purge|abandon|lookup|pass|fail|ok|'\n             r'miss|fetch|restart)\\b', Name.Constant),\n            (r'(beresp|obj|resp|req|req_top|bereq)\\.http\\.[a-zA-Z_-]+\\b', Name.Variable),\n            (words((\n                'obj.status', 'req.hash_always_miss', 'beresp.backend', 'req.esi_level',\n                'req.can_gzip', 'beresp.ttl', 'obj.uncacheable', 'req.ttl', 'obj.hits',\n                'client.identity', 'req.hash_ignore_busy', 'obj.reason', 'req.xid',\n                'req_top.proto', 'beresp.age', 'obj.proto', 'obj.age', 'local.ip',\n                'beresp.uncacheable', 'req.method', 'beresp.backend.ip', 'now',\n                'obj.grace', 'req.restarts', 'beresp.keep', 'req.proto', 'resp.proto',\n                'bereq.xid', 'bereq.between_bytes_timeout', 'req.esi',\n                'bereq.first_byte_timeout', 'bereq.method', 'bereq.connect_timeout',\n                'beresp.do_gzip',  'resp.status', 'beresp.do_gunzip',\n                'beresp.storage_hint', 'resp.is_streaming', 'beresp.do_stream',\n                'req_top.method', 'bereq.backend', 'beresp.backend.name', 'beresp.status',\n                'req.url', 'obj.keep', 'obj.ttl', 'beresp.reason', 'bereq.retries',\n                'resp.reason', 'bereq.url', 'beresp.do_esi', 'beresp.proto', 'client.ip',\n                'bereq.proto', 'server.hostname', 'remote.ip', 'req.backend_hint',\n                'server.identity', 'req_top.url', 'beresp.grace', 'beresp.was_304',\n                'server.ip', 'bereq.uncacheable'), suffix=r'\\b'),\n             Name.Variable),\n            (r'[!%&+*\\-,/<.}{>=|~]+', Operator),\n            (r'[();]', Punctuation),\n\n            (r'[,]+', Punctuation),\n            (words(('hash_data', 'regsub', 'regsuball', 'if', 'else',\n                    'elsif', 'elif', 'synth', 'synthetic', 'ban',\n                    'return', 'set', 'unset', 'import', 'include', 'new',\n                    'rollback', 'call'), suffix=r'\\b'),\n             Keyword),\n            (r'storage\\.\\w+\\.\\w+\\b', Name.Variable),\n            (words(('true', 'false')), Name.Builtin),\n            (r'\\d+\\b', Number),\n            (r'(backend)(\\s+\\w+)(\\s*\\{)',\n             bygroups(Keyword, Name.Variable.Global, Punctuation), 'backend'),\n            (r'(probe\\s)(\\s*\\w+\\s)(\\{)',\n             bygroups(Keyword, Name.Variable.Global, Punctuation), 'probe'),\n            (r'(acl\\s)(\\s*\\w+\\s)(\\{)',\n             bygroups(Keyword, Name.Variable.Global, Punctuation), 'acl'),\n            (r'(vcl )(4.0)(;)$',\n             bygroups(Keyword.Reserved, Name.Constant, Punctuation)),\n            (r'(sub\\s+)([a-zA-Z]\\w*)(\\s*\\{)',\n                bygroups(Keyword, Name.Function, Punctuation)),\n            (r'([a-zA-Z_]\\w*)'\n             r'(\\.)'\n             r'([a-zA-Z_]\\w*)'\n             r'(\\s*\\(.*\\))',\n             bygroups(Name.Function, Punctuation, Name.Function, using(this))),\n            (r'[a-zA-Z_]\\w*', Name),\n        ],\n        'comment': [\n            (r'[^*/]+', Comment.Multiline),\n            (r'/\\*', Comment.Multiline, '#push'),\n            (r'\\*/', Comment.Multiline, '#pop'),\n            (r'[*/]', Comment.Multiline),\n        ],\n        'comments': [\n            (r'#.*$', Comment),\n            (r'/\\*', Comment.Multiline, 'comment'),\n            (r'//.*$', Comment),\n        ],\n        'string': [\n            (r'\"', String, '#pop'),\n            (r'[^\"\\n]+', String),  # all other characters\n        ],\n        'multistring': [\n            (r'[^\"}]', String),\n            (r'\"\\}', String, '#pop'),\n            (r'[\"}]', String),\n        ],\n        'whitespace': [\n            (r'L?\"', String, 'string'),\n            (r'\\{\"', String, 'multistring'),\n            (r'\\n', Text),\n            (r'\\s+', Text),\n            (r'\\\\\\n', Text),  # line continuation\n        ],\n        'root': [\n            include('whitespace'),\n            include('comments'),\n            include('statements'),\n            (r'\\s+', Text),\n        ],\n    }\n\n\nclass VCLSnippetLexer(VCLLexer):\n    \"\"\"\n    For Varnish Configuration Language snippets.\n\n    .. versionadded:: 2.2\n    \"\"\"\n    name = 'VCLSnippets'\n    aliases = ['vclsnippets', 'vclsnippet']\n    mimetypes = ['text/x-vclsnippet']\n    filenames = []\n\n    def analyse_text(text):\n        # override method inherited from VCLLexer\n        return 0\n\n    tokens = {\n        'snippetspre': [\n            (r'\\.\\.\\.+', Comment),\n            (r'(bereq|req|req_top|resp|beresp|obj|client|server|local|remote|'\n             r'storage)($|\\.\\*)', Name.Variable),\n        ],\n        'snippetspost': [\n            (r'(backend)\\b', Keyword.Reserved),\n        ],\n        'root': [\n            include('snippetspre'),\n            inherit,\n            include('snippetspost'),\n        ],\n    }\n"], "filenames": ["CHANGES", "pygments/lexers/archetype.py", "pygments/lexers/factor.py", "pygments/lexers/jvm.py", "pygments/lexers/matlab.py", "pygments/lexers/objective.py", "pygments/lexers/templates.py", "pygments/lexers/varnish.py"], "buggy_code_start_loc": [41, 61, 268, 984, 140, 264, 1408, 64], "buggy_code_end_loc": [42, 62, 326, 985, 714, 269, 1409, 65], "fixing_code_start_loc": [41, 61, 268, 983, 140, 264, 1408, 64], "fixing_code_end_loc": [46, 62, 326, 983, 714, 269, 1409, 65], "type": "NVD-CWE-Other", "message": "In pygments 1.1+, fixed in 2.7.4, the lexers used to parse programming languages rely heavily on regular expressions. Some of the regular expressions have exponential or cubic worst-case complexity and are vulnerable to ReDoS. By crafting malicious input, an attacker can cause a denial of service.", "other": {"cve": {"id": "CVE-2021-27291", "sourceIdentifier": "cve@mitre.org", "published": "2021-03-17T13:15:15.137", "lastModified": "2022-05-23T22:35:48.033", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In pygments 1.1+, fixed in 2.7.4, the lexers used to parse programming languages rely heavily on regular expressions. Some of the regular expressions have exponential or cubic worst-case complexity and are vulnerable to ReDoS. By crafting malicious input, an attacker can cause a denial of service."}, {"lang": "es", "value": "En pygments versi\u00f3n 1.1+, corregido en 2.7.4, los lexers usados para analizar unos lenguajes de programaci\u00f3n dependen en gran medida en expresiones regulares.&#xa0;Algunas de las expresiones regulares presentan una complejidad exponencial o c\u00fabica en el peor de los casos y son vulnerables a ReDoS.&#xa0;Al dise\u00f1ar una entrada maliciosa, un atacante puede causar una denegaci\u00f3n de servicio"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:pygments:pygments:*:*:*:*:*:*:*:*", "versionStartIncluding": "1.1", "versionEndExcluding": "2.7.4", "matchCriteriaId": "CD77E0B1-BE2B-4B95-91AA-AD9AAEFD6DA8"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:32:*:*:*:*:*:*:*", "matchCriteriaId": "36D96259-24BD-44E2-96D9-78CE1D41F956"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:33:*:*:*:*:*:*:*", "matchCriteriaId": "E460AA51-FCDA-46B9-AE97-E6676AA5E194"}]}]}], "references": [{"url": "https://gist.github.com/b-c-ds/b1a2cc0c68a35c57188575eb496de5ce", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/pygments/pygments/commit/2e7e8c4a7b318f4032493773732754e418279a14", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2021/03/msg00024.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2021/05/msg00003.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2021/05/msg00006.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/GSJRFHALQ7E3UV4FFMFU2YQ6LUDHAI55/", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/WSLD67LFGXOX2K5YNESSWAS4AGZIJTUQ/", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://www.debian.org/security/2021/dsa-4878", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2021/dsa-4889", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/pygments/pygments/commit/2e7e8c4a7b318f4032493773732754e418279a14"}}