{"buggy_code": ["/*\n * x86 single-step support code, common to 32-bit and 64-bit.\n */\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/ptrace.h>\n#include <asm/desc.h>\n\nunsigned long convert_ip_to_linear(struct task_struct *child, struct pt_regs *regs)\n{\n\tunsigned long addr, seg;\n\n\taddr = regs->ip;\n\tseg = regs->cs & 0xffff;\n\tif (v8086_mode(regs)) {\n\t\taddr = (addr & 0xffff) + (seg << 4);\n\t\treturn addr;\n\t}\n\n\t/*\n\t * We'll assume that the code segments in the GDT\n\t * are all zero-based. That is largely true: the\n\t * TLS segments are used for data, and the PNPBIOS\n\t * and APM bios ones we just ignore here.\n\t */\n\tif ((seg & SEGMENT_TI_MASK) == SEGMENT_LDT) {\n\t\tstruct desc_struct *desc;\n\t\tunsigned long base;\n\n\t\tseg &= ~7UL;\n\n\t\tmutex_lock(&child->mm->context.lock);\n\t\tif (unlikely((seg >> 3) >= child->mm->context.size))\n\t\t\taddr = -1L; /* bogus selector, access would fault */\n\t\telse {\n\t\t\tdesc = child->mm->context.ldt + seg;\n\t\t\tbase = get_desc_base(desc);\n\n\t\t\t/* 16-bit code segment? */\n\t\t\tif (!desc->d)\n\t\t\t\taddr &= 0xffff;\n\t\t\taddr += base;\n\t\t}\n\t\tmutex_unlock(&child->mm->context.lock);\n\t}\n\n\treturn addr;\n}\n\nstatic int is_setting_trap_flag(struct task_struct *child, struct pt_regs *regs)\n{\n\tint i, copied;\n\tunsigned char opcode[15];\n\tunsigned long addr = convert_ip_to_linear(child, regs);\n\n\tcopied = access_process_vm(child, addr, opcode, sizeof(opcode), 0);\n\tfor (i = 0; i < copied; i++) {\n\t\tswitch (opcode[i]) {\n\t\t/* popf and iret */\n\t\tcase 0x9d: case 0xcf:\n\t\t\treturn 1;\n\n\t\t\t/* CHECKME: 64 65 */\n\n\t\t/* opcode and address size prefixes */\n\t\tcase 0x66: case 0x67:\n\t\t\tcontinue;\n\t\t/* irrelevant prefixes (segment overrides and repeats) */\n\t\tcase 0x26: case 0x2e:\n\t\tcase 0x36: case 0x3e:\n\t\tcase 0x64: case 0x65:\n\t\tcase 0xf0: case 0xf2: case 0xf3:\n\t\t\tcontinue;\n\n#ifdef CONFIG_X86_64\n\t\tcase 0x40 ... 0x4f:\n\t\t\tif (!user_64bit_mode(regs))\n\t\t\t\t/* 32-bit mode: register increment */\n\t\t\t\treturn 0;\n\t\t\t/* 64-bit mode: REX prefix */\n\t\t\tcontinue;\n#endif\n\n\t\t\t/* CHECKME: f2, f3 */\n\n\t\t/*\n\t\t * pushf: NOTE! We should probably not let\n\t\t * the user see the TF bit being set. But\n\t\t * it's more pain than it's worth to avoid\n\t\t * it, and a debugger could emulate this\n\t\t * all in user space if it _really_ cares.\n\t\t */\n\t\tcase 0x9c:\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * Enable single-stepping.  Return nonzero if user mode is not using TF itself.\n */\nstatic int enable_single_step(struct task_struct *child)\n{\n\tstruct pt_regs *regs = task_pt_regs(child);\n\tunsigned long oflags;\n\n\t/*\n\t * If we stepped into a sysenter/syscall insn, it trapped in\n\t * kernel mode; do_debug() cleared TF and set TIF_SINGLESTEP.\n\t * If user-mode had set TF itself, then it's still clear from\n\t * do_debug() and we need to set it again to restore the user\n\t * state so we don't wrongly set TIF_FORCED_TF below.\n\t * If enable_single_step() was used last and that is what\n\t * set TIF_SINGLESTEP, then both TF and TIF_FORCED_TF are\n\t * already set and our bookkeeping is fine.\n\t */\n\tif (unlikely(test_tsk_thread_flag(child, TIF_SINGLESTEP)))\n\t\tregs->flags |= X86_EFLAGS_TF;\n\n\t/*\n\t * Always set TIF_SINGLESTEP - this guarantees that\n\t * we single-step system calls etc..  This will also\n\t * cause us to set TF when returning to user mode.\n\t */\n\tset_tsk_thread_flag(child, TIF_SINGLESTEP);\n\n\toflags = regs->flags;\n\n\t/* Set TF on the kernel stack.. */\n\tregs->flags |= X86_EFLAGS_TF;\n\n\t/*\n\t * ..but if TF is changed by the instruction we will trace,\n\t * don't mark it as being \"us\" that set it, so that we\n\t * won't clear it by hand later.\n\t *\n\t * Note that if we don't actually execute the popf because\n\t * of a signal arriving right now or suchlike, we will lose\n\t * track of the fact that it really was \"us\" that set it.\n\t */\n\tif (is_setting_trap_flag(child, regs)) {\n\t\tclear_tsk_thread_flag(child, TIF_FORCED_TF);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If TF was already set, check whether it was us who set it.\n\t * If not, we should never attempt a block step.\n\t */\n\tif (oflags & X86_EFLAGS_TF)\n\t\treturn test_tsk_thread_flag(child, TIF_FORCED_TF);\n\n\tset_tsk_thread_flag(child, TIF_FORCED_TF);\n\n\treturn 1;\n}\n\nvoid set_task_blockstep(struct task_struct *task, bool on)\n{\n\tunsigned long debugctl;\n\n\t/*\n\t * Ensure irq/preemption can't change debugctl in between.\n\t * Note also that both TIF_BLOCKSTEP and debugctl should\n\t * be changed atomically wrt preemption.\n\t * FIXME: this means that set/clear TIF_BLOCKSTEP is simply\n\t * wrong if task != current, SIGKILL can wakeup the stopped\n\t * tracee and set/clear can play with the running task, this\n\t * can confuse the next __switch_to_xtra().\n\t */\n\tlocal_irq_disable();\n\tdebugctl = get_debugctlmsr();\n\tif (on) {\n\t\tdebugctl |= DEBUGCTLMSR_BTF;\n\t\tset_tsk_thread_flag(task, TIF_BLOCKSTEP);\n\t} else {\n\t\tdebugctl &= ~DEBUGCTLMSR_BTF;\n\t\tclear_tsk_thread_flag(task, TIF_BLOCKSTEP);\n\t}\n\tif (task == current)\n\t\tupdate_debugctlmsr(debugctl);\n\tlocal_irq_enable();\n}\n\n/*\n * Enable single or block step.\n */\nstatic void enable_step(struct task_struct *child, bool block)\n{\n\t/*\n\t * Make sure block stepping (BTF) is not enabled unless it should be.\n\t * Note that we don't try to worry about any is_setting_trap_flag()\n\t * instructions after the first when using block stepping.\n\t * So no one should try to use debugger block stepping in a program\n\t * that uses user-mode single stepping itself.\n\t */\n\tif (enable_single_step(child) && block)\n\t\tset_task_blockstep(child, true);\n\telse if (test_tsk_thread_flag(child, TIF_BLOCKSTEP))\n\t\tset_task_blockstep(child, false);\n}\n\nvoid user_enable_single_step(struct task_struct *child)\n{\n\tenable_step(child, 0);\n}\n\nvoid user_enable_block_step(struct task_struct *child)\n{\n\tenable_step(child, 1);\n}\n\nvoid user_disable_single_step(struct task_struct *child)\n{\n\t/*\n\t * Make sure block stepping (BTF) is disabled.\n\t */\n\tif (test_tsk_thread_flag(child, TIF_BLOCKSTEP))\n\t\tset_task_blockstep(child, false);\n\n\t/* Always clear TIF_SINGLESTEP... */\n\tclear_tsk_thread_flag(child, TIF_SINGLESTEP);\n\n\t/* But touch TF only if it was set by us.. */\n\tif (test_and_clear_tsk_thread_flag(child, TIF_FORCED_TF))\n\t\ttask_pt_regs(child)->flags &= ~X86_EFLAGS_TF;\n}\n", "/*\n * linux/kernel/ptrace.c\n *\n * (C) Copyright 1999 Linus Torvalds\n *\n * Common interfaces for \"ptrace()\" which we do not want\n * to continually duplicate across every architecture.\n */\n\n#include <linux/capability.h>\n#include <linux/export.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/ptrace.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/audit.h>\n#include <linux/pid_namespace.h>\n#include <linux/syscalls.h>\n#include <linux/uaccess.h>\n#include <linux/regset.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/cn_proc.h>\n\n\nstatic int ptrace_trapping_sleep_fn(void *flags)\n{\n\tschedule();\n\treturn 0;\n}\n\n/*\n * ptrace a task: make the debugger its new parent and\n * move it to the ptrace list.\n *\n * Must be called with the tasklist lock write-held.\n */\nvoid __ptrace_link(struct task_struct *child, struct task_struct *new_parent)\n{\n\tBUG_ON(!list_empty(&child->ptrace_entry));\n\tlist_add(&child->ptrace_entry, &new_parent->ptraced);\n\tchild->parent = new_parent;\n}\n\n/**\n * __ptrace_unlink - unlink ptracee and restore its execution state\n * @child: ptracee to be unlinked\n *\n * Remove @child from the ptrace list, move it back to the original parent,\n * and restore the execution state so that it conforms to the group stop\n * state.\n *\n * Unlinking can happen via two paths - explicit PTRACE_DETACH or ptracer\n * exiting.  For PTRACE_DETACH, unless the ptracee has been killed between\n * ptrace_check_attach() and here, it's guaranteed to be in TASK_TRACED.\n * If the ptracer is exiting, the ptracee can be in any state.\n *\n * After detach, the ptracee should be in a state which conforms to the\n * group stop.  If the group is stopped or in the process of stopping, the\n * ptracee should be put into TASK_STOPPED; otherwise, it should be woken\n * up from TASK_TRACED.\n *\n * If the ptracee is in TASK_TRACED and needs to be moved to TASK_STOPPED,\n * it goes through TRACED -> RUNNING -> STOPPED transition which is similar\n * to but in the opposite direction of what happens while attaching to a\n * stopped task.  However, in this direction, the intermediate RUNNING\n * state is not hidden even from the current ptracer and if it immediately\n * re-attaches and performs a WNOHANG wait(2), it may fail.\n *\n * CONTEXT:\n * write_lock_irq(tasklist_lock)\n */\nvoid __ptrace_unlink(struct task_struct *child)\n{\n\tBUG_ON(!child->ptrace);\n\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\n\tspin_lock(&child->sighand->siglock);\n\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tptrace_signal_wake_up(child, true);\n\n\tspin_unlock(&child->sighand->siglock);\n}\n\n/**\n * ptrace_check_attach - check whether ptracee is ready for ptrace operation\n * @child: ptracee to check for\n * @ignore_state: don't check whether @child is currently %TASK_TRACED\n *\n * Check whether @child is being ptraced by %current and ready for further\n * ptrace operations.  If @ignore_state is %false, @child also should be in\n * %TASK_TRACED state and on return the child is guaranteed to be traced\n * and not executing.  If @ignore_state is %true, @child can be in any\n * state.\n *\n * CONTEXT:\n * Grabs and releases tasklist_lock and @child->sighand->siglock.\n *\n * RETURNS:\n * 0 on success, -ESRCH if %child is not ready.\n */\nstatic int ptrace_check_attach(struct task_struct *child, bool ignore_state)\n{\n\tint ret = -ESRCH;\n\n\t/*\n\t * We take the read lock around doing both checks to close a\n\t * possible race where someone else was tracing our child and\n\t * detached between these two checks.  After this locked check,\n\t * we are sure that this is our traced child and that can only\n\t * be changed by us so it's not changing right after this.\n\t */\n\tread_lock(&tasklist_lock);\n\tif ((child->ptrace & PT_PTRACED) && child->parent == current) {\n\t\t/*\n\t\t * child->sighand can't be NULL, release_task()\n\t\t * does ptrace_unlink() before __exit_signal().\n\t\t */\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tWARN_ON_ONCE(task_is_stopped(child));\n\t\tif (ignore_state || (task_is_traced(child) &&\n\t\t\t\t     !(child->jobctl & JOBCTL_LISTENING)))\n\t\t\tret = 0;\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tif (!ret && !ignore_state)\n\t\tret = wait_task_inactive(child, TASK_TRACED) ? 0 : -ESRCH;\n\n\t/* All systems go.. */\n\treturn ret;\n}\n\nstatic int ptrace_has_cap(struct user_namespace *ns, unsigned int mode)\n{\n\tif (mode & PTRACE_MODE_NOAUDIT)\n\t\treturn has_ns_capability_noaudit(current, ns, CAP_SYS_PTRACE);\n\telse\n\t\treturn has_ns_capability(current, ns, CAP_SYS_PTRACE);\n}\n\n/* Returns 0 on success, -errno on denial. */\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\tint dumpable = 0;\n\t/* Don't let security modules deny introspection */\n\tif (task == current)\n\t\treturn 0;\n\trcu_read_lock();\n\ttcred = __task_cred(task);\n\tif (uid_eq(cred->uid, tcred->euid) &&\n\t    uid_eq(cred->uid, tcred->suid) &&\n\t    uid_eq(cred->uid, tcred->uid)  &&\n\t    gid_eq(cred->gid, tcred->egid) &&\n\t    gid_eq(cred->gid, tcred->sgid) &&\n\t    gid_eq(cred->gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tsmp_rmb();\n\tif (task->mm)\n\t\tdumpable = get_dumpable(task->mm);\n\trcu_read_lock();\n\tif (!dumpable && !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn security_ptrace_access_check(task, mode);\n}\n\nbool ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tint err;\n\ttask_lock(task);\n\terr = __ptrace_may_access(task, mode);\n\ttask_unlock(task);\n\treturn !err;\n}\n\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT,\n\t\t\t    ptrace_trapping_sleep_fn, TASK_UNINTERRUPTIBLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n\n/**\n * ptrace_traceme  --  helper for PTRACE_TRACEME\n *\n * Performs checks and sets PT_PTRACED.\n * Should be used by all ptrace implementations for PTRACE_TRACEME.\n */\nstatic int ptrace_traceme(void)\n{\n\tint ret = -EPERM;\n\n\twrite_lock_irq(&tasklist_lock);\n\t/* Are we already being traced? */\n\tif (!current->ptrace) {\n\t\tret = security_ptrace_traceme(current->parent);\n\t\t/*\n\t\t * Check PF_EXITING to ensure ->real_parent has not passed\n\t\t * exit_ptrace(). Otherwise we don't report the error but\n\t\t * pretend ->real_parent untraces us right after return.\n\t\t */\n\t\tif (!ret && !(current->real_parent->flags & PF_EXITING)) {\n\t\t\tcurrent->ptrace = PT_PTRACED;\n\t\t\t__ptrace_link(current, current->real_parent);\n\t\t}\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * Called with irqs disabled, returns true if childs should reap themselves.\n */\nstatic int ignoring_children(struct sighand_struct *sigh)\n{\n\tint ret;\n\tspin_lock(&sigh->siglock);\n\tret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||\n\t      (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);\n\tspin_unlock(&sigh->siglock);\n\treturn ret;\n}\n\n/*\n * Called with tasklist_lock held for writing.\n * Unlink a traced task, and clean it up if it was a traced zombie.\n * Return true if it needs to be reaped with release_task().\n * (We can't call release_task() here because we already hold tasklist_lock.)\n *\n * If it's a zombie, our attachedness prevented normal parent notification\n * or self-reaping.  Do notification now if it would have happened earlier.\n * If it should reap itself, return true.\n *\n * If it's our own child, there is no notification to do. But if our normal\n * children self-reap, then this child was prevented by ptrace and we must\n * reap it now, in that case we must also wake up sub-threads sleeping in\n * do_wait().\n */\nstatic bool __ptrace_detach(struct task_struct *tracer, struct task_struct *p)\n{\n\tbool dead;\n\n\t__ptrace_unlink(p);\n\n\tif (p->exit_state != EXIT_ZOMBIE)\n\t\treturn false;\n\n\tdead = !thread_group_leader(p);\n\n\tif (!dead && thread_group_empty(p)) {\n\t\tif (!same_thread_group(p->real_parent, tracer))\n\t\t\tdead = do_notify_parent(p, p->exit_signal);\n\t\telse if (ignoring_children(tracer->sighand)) {\n\t\t\t__wake_up_parent(p, tracer);\n\t\t\tdead = true;\n\t\t}\n\t}\n\t/* Mark it as in the process of being reaped. */\n\tif (dead)\n\t\tp->exit_state = EXIT_DEAD;\n\treturn dead;\n}\n\nstatic int ptrace_detach(struct task_struct *child, unsigned int data)\n{\n\tbool dead = false;\n\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\t/* Architecture-specific hardware disable .. */\n\tptrace_disable(child);\n\tclear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\n\twrite_lock_irq(&tasklist_lock);\n\t/*\n\t * This child can be already killed. Make sure de_thread() or\n\t * our sub-thread doing do_wait() didn't do release_task() yet.\n\t */\n\tif (child->ptrace) {\n\t\tchild->exit_code = data;\n\t\tdead = __ptrace_detach(current, child);\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_ptrace_connector(child, PTRACE_DETACH);\n\tif (unlikely(dead))\n\t\trelease_task(child);\n\n\treturn 0;\n}\n\n/*\n * Detach all tasks we were using ptrace on. Called with tasklist held\n * for writing, and returns with it held too. But note it can release\n * and reacquire the lock.\n */\nvoid exit_ptrace(struct task_struct *tracer)\n\t__releases(&tasklist_lock)\n\t__acquires(&tasklist_lock)\n{\n\tstruct task_struct *p, *n;\n\tLIST_HEAD(ptrace_dead);\n\n\tif (likely(list_empty(&tracer->ptraced)))\n\t\treturn;\n\n\tlist_for_each_entry_safe(p, n, &tracer->ptraced, ptrace_entry) {\n\t\tif (unlikely(p->ptrace & PT_EXITKILL))\n\t\t\tsend_sig_info(SIGKILL, SEND_SIG_FORCED, p);\n\n\t\tif (__ptrace_detach(tracer, p))\n\t\t\tlist_add(&p->ptrace_entry, &ptrace_dead);\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\tBUG_ON(!list_empty(&tracer->ptraced));\n\n\tlist_for_each_entry_safe(p, n, &ptrace_dead, ptrace_entry) {\n\t\tlist_del_init(&p->ptrace_entry);\n\t\trelease_task(p);\n\t}\n\n\twrite_lock_irq(&tasklist_lock);\n}\n\nint ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tretval = access_process_vm(tsk, src, buf, this_len, 0);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tif (copy_to_user(dst, buf, retval))\n\t\t\treturn -EFAULT;\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nint ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tif (copy_from_user(buf, src, this_len))\n\t\t\treturn -EFAULT;\n\t\tretval = access_process_vm(tsk, dst, buf, this_len, 1);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nstatic int ptrace_setoptions(struct task_struct *child, unsigned long data)\n{\n\tunsigned flags;\n\n\tif (data & ~(unsigned long)PTRACE_O_MASK)\n\t\treturn -EINVAL;\n\n\t/* Avoid intermediate state when all opts are cleared */\n\tflags = child->ptrace;\n\tflags &= ~(PTRACE_O_MASK << PT_OPT_FLAG_SHIFT);\n\tflags |= (data << PT_OPT_FLAG_SHIFT);\n\tchild->ptrace = flags;\n\n\treturn 0;\n}\n\nstatic int ptrace_getsiginfo(struct task_struct *child, siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\t*info = *child->last_siginfo;\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_setsiginfo(struct task_struct *child, const siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\t*child->last_siginfo = *info;\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\n\n#ifdef PTRACE_SINGLESTEP\n#define is_singlestep(request)\t\t((request) == PTRACE_SINGLESTEP)\n#else\n#define is_singlestep(request)\t\t0\n#endif\n\n#ifdef PTRACE_SINGLEBLOCK\n#define is_singleblock(request)\t\t((request) == PTRACE_SINGLEBLOCK)\n#else\n#define is_singleblock(request)\t\t0\n#endif\n\n#ifdef PTRACE_SYSEMU\n#define is_sysemu_singlestep(request)\t((request) == PTRACE_SYSEMU_SINGLESTEP)\n#else\n#define is_sysemu_singlestep(request)\t0\n#endif\n\nstatic int ptrace_resume(struct task_struct *child, long request,\n\t\t\t unsigned long data)\n{\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\tif (request == PTRACE_SYSCALL)\n\t\tset_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\telse\n\t\tclear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\n#ifdef TIF_SYSCALL_EMU\n\tif (request == PTRACE_SYSEMU || request == PTRACE_SYSEMU_SINGLESTEP)\n\t\tset_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n\telse\n\t\tclear_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n#endif\n\n\tif (is_singleblock(request)) {\n\t\tif (unlikely(!arch_has_block_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_block_step(child);\n\t} else if (is_singlestep(request) || is_sysemu_singlestep(request)) {\n\t\tif (unlikely(!arch_has_single_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_single_step(child);\n\t} else {\n\t\tuser_disable_single_step(child);\n\t}\n\n\tchild->exit_code = data;\n\twake_up_state(child, __TASK_TRACED);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\nstatic const struct user_regset *\nfind_regset(const struct user_regset_view *view, unsigned int type)\n{\n\tconst struct user_regset *regset;\n\tint n;\n\n\tfor (n = 0; n < view->n; ++n) {\n\t\tregset = view->regsets + n;\n\t\tif (regset->core_note_type == type)\n\t\t\treturn regset;\n\t}\n\n\treturn NULL;\n}\n\nstatic int ptrace_regset(struct task_struct *task, int req, unsigned int type,\n\t\t\t struct iovec *kiov)\n{\n\tconst struct user_regset_view *view = task_user_regset_view(task);\n\tconst struct user_regset *regset = find_regset(view, type);\n\tint regset_no;\n\n\tif (!regset || (kiov->iov_len % regset->size) != 0)\n\t\treturn -EINVAL;\n\n\tregset_no = regset - view->regsets;\n\tkiov->iov_len = min(kiov->iov_len,\n\t\t\t    (__kernel_size_t) (regset->n * regset->size));\n\n\tif (req == PTRACE_GETREGSET)\n\t\treturn copy_regset_to_user(task, view, regset_no, 0,\n\t\t\t\t\t   kiov->iov_len, kiov->iov_base);\n\telse\n\t\treturn copy_regset_from_user(task, view, regset_no, 0,\n\t\t\t\t\t     kiov->iov_len, kiov->iov_base);\n}\n\n#endif\n\nint ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tsiginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tif (copy_from_user(&siginfo, datavp, sizeof siginfo))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tptrace_signal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic struct task_struct *ptrace_get_task_struct(pid_t pid)\n{\n\tstruct task_struct *child;\n\n\trcu_read_lock();\n\tchild = find_task_by_vpid(pid);\n\tif (child)\n\t\tget_task_struct(child);\n\trcu_read_unlock();\n\n\tif (!child)\n\t\treturn ERR_PTR(-ESRCH);\n\treturn child;\n}\n\n#ifndef arch_ptrace_attach\n#define arch_ptrace_attach(child)\tdo { } while (0)\n#endif\n\nSYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,\n\t\tunsigned long, data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(current);\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (ret < 0)\n\t\tgoto out_put_task_struct;\n\n\tret = arch_ptrace(child, request, addr, data);\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n\nint generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tunsigned long tmp;\n\tint copied;\n\n\tcopied = access_process_vm(tsk, addr, &tmp, sizeof(tmp), 0);\n\tif (copied != sizeof(tmp))\n\t\treturn -EIO;\n\treturn put_user(tmp, (unsigned long __user *)data);\n}\n\nint generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tint copied;\n\n\tcopied = access_process_vm(tsk, addr, &data, sizeof(data), 1);\n\treturn (copied == sizeof(data)) ? 0 : -EIO;\n}\n\n#if defined CONFIG_COMPAT\n#include <linux/compat.h>\n\nint compat_ptrace_request(struct task_struct *child, compat_long_t request,\n\t\t\t  compat_ulong_t addr, compat_ulong_t data)\n{\n\tcompat_ulong_t __user *datap = compat_ptr(data);\n\tcompat_ulong_t word;\n\tsiginfo_t siginfo;\n\tint ret;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\tret = access_process_vm(child, addr, &word, sizeof(word), 0);\n\t\tif (ret != sizeof(word))\n\t\t\tret = -EIO;\n\t\telse\n\t\t\tret = put_user(word, datap);\n\t\tbreak;\n\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\tret = access_process_vm(child, addr, &data, sizeof(data), 1);\n\t\tret = (ret != sizeof(data) ? -EIO : 0);\n\t\tbreak;\n\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user((compat_ulong_t) child->ptrace_message, datap);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user32(\n\t\t\t\t(struct compat_siginfo __user *) datap,\n\t\t\t\t&siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tmemset(&siginfo, 0, sizeof siginfo);\n\t\tif (copy_siginfo_from_user32(\n\t\t\t    &siginfo, (struct compat_siginfo __user *) datap))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct compat_iovec __user *uiov =\n\t\t\t(struct compat_iovec __user *) datap;\n\t\tcompat_uptr_t ptr;\n\t\tcompat_size_t len;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(ptr, &uiov->iov_base) ||\n\t\t    __get_user(len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tkiov.iov_base = compat_ptr(ptr);\n\t\tkiov.iov_len = len;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t}\n\n\treturn ret;\n}\n\nasmlinkage long compat_sys_ptrace(compat_long_t request, compat_long_t pid,\n\t\t\t\t  compat_long_t addr, compat_long_t data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (!ret)\n\t\tret = compat_arch_ptrace(child, request, addr, data);\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n#endif\t/* CONFIG_COMPAT */\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\nint ptrace_get_breakpoints(struct task_struct *tsk)\n{\n\tif (atomic_inc_not_zero(&tsk->ptrace_bp_refcnt))\n\t\treturn 0;\n\n\treturn -1;\n}\n\nvoid ptrace_put_breakpoints(struct task_struct *tsk)\n{\n\tif (atomic_dec_and_test(&tsk->ptrace_bp_refcnt))\n\t\tflush_ptrace_hw_breakpoint(tsk);\n}\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n", "/*\n *  linux/kernel/signal.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  1997-11-02  Modified for POSIX.1b signals by Richard Henderson\n *\n *  2003-06-02  Jim Houston - Concurrent Computer Corp.\n *\t\tChanges to use preallocated sigqueue structures\n *\t\tto allow signals to be sent reliably.\n */\n\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/tty.h>\n#include <linux/binfmts.h>\n#include <linux/coredump.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/ptrace.h>\n#include <linux/signal.h>\n#include <linux/signalfd.h>\n#include <linux/ratelimit.h>\n#include <linux/tracehook.h>\n#include <linux/capability.h>\n#include <linux/freezer.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/user_namespace.h>\n#include <linux/uprobes.h>\n#include <linux/compat.h>\n#define CREATE_TRACE_POINTS\n#include <trace/events/signal.h>\n\n#include <asm/param.h>\n#include <asm/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/siginfo.h>\n#include <asm/cacheflush.h>\n#include \"audit.h\"\t/* audit_signal_info() */\n\n/*\n * SLAB caches for signal bits.\n */\n\nstatic struct kmem_cache *sigqueue_cachep;\n\nint print_fatal_signals __read_mostly;\n\nstatic void __user *sig_handler(struct task_struct *t, int sig)\n{\n\treturn t->sighand->action[sig - 1].sa.sa_handler;\n}\n\nstatic int sig_handler_ignored(void __user *handler, int sig)\n{\n\t/* Is it explicitly or implicitly ignored? */\n\treturn handler == SIG_IGN ||\n\t\t(handler == SIG_DFL && sig_kernel_ignore(sig));\n}\n\nstatic int sig_task_ignored(struct task_struct *t, int sig, bool force)\n{\n\tvoid __user *handler;\n\n\thandler = sig_handler(t, sig);\n\n\tif (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\thandler == SIG_DFL && !force)\n\t\treturn 1;\n\n\treturn sig_handler_ignored(handler, sig);\n}\n\nstatic int sig_ignored(struct task_struct *t, int sig, bool force)\n{\n\t/*\n\t * Blocked signals are never ignored, since the\n\t * signal handler may change by the time it is\n\t * unblocked.\n\t */\n\tif (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))\n\t\treturn 0;\n\n\tif (!sig_task_ignored(t, sig, force))\n\t\treturn 0;\n\n\t/*\n\t * Tracers may want to know about even ignored signals.\n\t */\n\treturn !t->ptrace;\n}\n\n/*\n * Re-calculate pending state from the set of locally pending\n * signals, globally pending signals, and blocked signals.\n */\nstatic inline int has_pending_signals(sigset_t *signal, sigset_t *blocked)\n{\n\tunsigned long ready;\n\tlong i;\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = _NSIG_WORDS, ready = 0; --i >= 0 ;)\n\t\t\tready |= signal->sig[i] &~ blocked->sig[i];\n\t\tbreak;\n\n\tcase 4: ready  = signal->sig[3] &~ blocked->sig[3];\n\t\tready |= signal->sig[2] &~ blocked->sig[2];\n\t\tready |= signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 2: ready  = signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 1: ready  = signal->sig[0] &~ blocked->sig[0];\n\t}\n\treturn ready !=\t0;\n}\n\n#define PENDING(p,b) has_pending_signals(&(p)->signal, (b))\n\nstatic int recalc_sigpending_tsk(struct task_struct *t)\n{\n\tif ((t->jobctl & JOBCTL_PENDING_MASK) ||\n\t    PENDING(&t->pending, &t->blocked) ||\n\t    PENDING(&t->signal->shared_pending, &t->blocked)) {\n\t\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t\treturn 1;\n\t}\n\t/*\n\t * We must never clear the flag in another thread, or in current\n\t * when it's possible the current syscall is returning -ERESTART*.\n\t * So we don't clear it here, and only callers who know they should do.\n\t */\n\treturn 0;\n}\n\n/*\n * After recalculating TIF_SIGPENDING, we need to make sure the task wakes up.\n * This is superfluous when called on current, the wakeup is a harmless no-op.\n */\nvoid recalc_sigpending_and_wake(struct task_struct *t)\n{\n\tif (recalc_sigpending_tsk(t))\n\t\tsignal_wake_up(t, 0);\n}\n\nvoid recalc_sigpending(void)\n{\n\tif (!recalc_sigpending_tsk(current) && !freezing(current))\n\t\tclear_thread_flag(TIF_SIGPENDING);\n\n}\n\n/* Given the mask, find the first available signal that should be serviced. */\n\n#define SYNCHRONOUS_MASK \\\n\t(sigmask(SIGSEGV) | sigmask(SIGBUS) | sigmask(SIGILL) | \\\n\t sigmask(SIGTRAP) | sigmask(SIGFPE) | sigmask(SIGSYS))\n\nint next_signal(struct sigpending *pending, sigset_t *mask)\n{\n\tunsigned long i, *s, *m, x;\n\tint sig = 0;\n\n\ts = pending->signal.sig;\n\tm = mask->sig;\n\n\t/*\n\t * Handle the first word specially: it contains the\n\t * synchronous signals that need to be dequeued first.\n\t */\n\tx = *s &~ *m;\n\tif (x) {\n\t\tif (x & SYNCHRONOUS_MASK)\n\t\t\tx &= SYNCHRONOUS_MASK;\n\t\tsig = ffz(~x) + 1;\n\t\treturn sig;\n\t}\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = 1; i < _NSIG_WORDS; ++i) {\n\t\t\tx = *++s &~ *++m;\n\t\t\tif (!x)\n\t\t\t\tcontinue;\n\t\t\tsig = ffz(~x) + i*_NSIG_BPW + 1;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 2:\n\t\tx = s[1] &~ m[1];\n\t\tif (!x)\n\t\t\tbreak;\n\t\tsig = ffz(~x) + _NSIG_BPW + 1;\n\t\tbreak;\n\n\tcase 1:\n\t\t/* Nothing to do */\n\t\tbreak;\n\t}\n\n\treturn sig;\n}\n\nstatic inline void print_dropped_signal(int sig)\n{\n\tstatic DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);\n\n\tif (!print_fatal_signals)\n\t\treturn;\n\n\tif (!__ratelimit(&ratelimit_state))\n\t\treturn;\n\n\tprintk(KERN_INFO \"%s/%d: reached RLIMIT_SIGPENDING, dropped signal %d\\n\",\n\t\t\t\tcurrent->comm, current->pid, sig);\n}\n\n/**\n * task_set_jobctl_pending - set jobctl pending bits\n * @task: target task\n * @mask: pending bits to set\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK | %JOBCTL_STOP_CONSUME | %JOBCTL_STOP_SIGMASK |\n * %JOBCTL_TRAPPING.  If stop signo is being set, the existing signo is\n * cleared.  If @task is already being killed or exiting, this function\n * becomes noop.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if @mask is set, %false if made noop because @task was dying.\n */\nbool task_set_jobctl_pending(struct task_struct *task, unsigned int mask)\n{\n\tBUG_ON(mask & ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |\n\t\t\tJOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));\n\tBUG_ON((mask & JOBCTL_TRAPPING) && !(mask & JOBCTL_PENDING_MASK));\n\n\tif (unlikely(fatal_signal_pending(task) || (task->flags & PF_EXITING)))\n\t\treturn false;\n\n\tif (mask & JOBCTL_STOP_SIGMASK)\n\t\ttask->jobctl &= ~JOBCTL_STOP_SIGMASK;\n\n\ttask->jobctl |= mask;\n\treturn true;\n}\n\n/**\n * task_clear_jobctl_trapping - clear jobctl trapping bit\n * @task: target task\n *\n * If JOBCTL_TRAPPING is set, a ptracer is waiting for us to enter TRACED.\n * Clear it and wake up the ptracer.  Note that we don't need any further\n * locking.  @task->siglock guarantees that @task->parent points to the\n * ptracer.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_trapping(struct task_struct *task)\n{\n\tif (unlikely(task->jobctl & JOBCTL_TRAPPING)) {\n\t\ttask->jobctl &= ~JOBCTL_TRAPPING;\n\t\twake_up_bit(&task->jobctl, JOBCTL_TRAPPING_BIT);\n\t}\n}\n\n/**\n * task_clear_jobctl_pending - clear jobctl pending bits\n * @task: target task\n * @mask: pending bits to clear\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK.  If %JOBCTL_STOP_PENDING is being cleared, other\n * STOP bits are cleared together.\n *\n * If clearing of @mask leaves no stop or trap pending, this function calls\n * task_clear_jobctl_trapping().\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_pending(struct task_struct *task, unsigned int mask)\n{\n\tBUG_ON(mask & ~JOBCTL_PENDING_MASK);\n\n\tif (mask & JOBCTL_STOP_PENDING)\n\t\tmask |= JOBCTL_STOP_CONSUME | JOBCTL_STOP_DEQUEUED;\n\n\ttask->jobctl &= ~mask;\n\n\tif (!(task->jobctl & JOBCTL_PENDING_MASK))\n\t\ttask_clear_jobctl_trapping(task);\n}\n\n/**\n * task_participate_group_stop - participate in a group stop\n * @task: task participating in a group stop\n *\n * @task has %JOBCTL_STOP_PENDING set and is participating in a group stop.\n * Group stop states are cleared and the group stop count is consumed if\n * %JOBCTL_STOP_CONSUME was set.  If the consumption completes the group\n * stop, the appropriate %SIGNAL_* flags are set.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if group stop completion should be notified to the parent, %false\n * otherwise.\n */\nstatic bool task_participate_group_stop(struct task_struct *task)\n{\n\tstruct signal_struct *sig = task->signal;\n\tbool consume = task->jobctl & JOBCTL_STOP_CONSUME;\n\n\tWARN_ON_ONCE(!(task->jobctl & JOBCTL_STOP_PENDING));\n\n\ttask_clear_jobctl_pending(task, JOBCTL_STOP_PENDING);\n\n\tif (!consume)\n\t\treturn false;\n\n\tif (!WARN_ON_ONCE(sig->group_stop_count == 0))\n\t\tsig->group_stop_count--;\n\n\t/*\n\t * Tell the caller to notify completion iff we are entering into a\n\t * fresh group stop.  Read comment in do_signal_stop() for details.\n\t */\n\tif (!sig->group_stop_count && !(sig->flags & SIGNAL_STOP_STOPPED)) {\n\t\tsig->flags = SIGNAL_STOP_STOPPED;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * allocate a new signal queue record\n * - this may be called without locks if and only if t == current, otherwise an\n *   appropriate lock must be held to stop the target task from exiting\n */\nstatic struct sigqueue *\n__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)\n{\n\tstruct sigqueue *q = NULL;\n\tstruct user_struct *user;\n\n\t/*\n\t * Protect access to @t credentials. This can go away when all\n\t * callers hold rcu read lock.\n\t */\n\trcu_read_lock();\n\tuser = get_uid(__task_cred(t)->user);\n\tatomic_inc(&user->sigpending);\n\trcu_read_unlock();\n\n\tif (override_rlimit ||\n\t    atomic_read(&user->sigpending) <=\n\t\t\ttask_rlimit(t, RLIMIT_SIGPENDING)) {\n\t\tq = kmem_cache_alloc(sigqueue_cachep, flags);\n\t} else {\n\t\tprint_dropped_signal(sig);\n\t}\n\n\tif (unlikely(q == NULL)) {\n\t\tatomic_dec(&user->sigpending);\n\t\tfree_uid(user);\n\t} else {\n\t\tINIT_LIST_HEAD(&q->list);\n\t\tq->flags = 0;\n\t\tq->user = user;\n\t}\n\n\treturn q;\n}\n\nstatic void __sigqueue_free(struct sigqueue *q)\n{\n\tif (q->flags & SIGQUEUE_PREALLOC)\n\t\treturn;\n\tatomic_dec(&q->user->sigpending);\n\tfree_uid(q->user);\n\tkmem_cache_free(sigqueue_cachep, q);\n}\n\nvoid flush_sigqueue(struct sigpending *queue)\n{\n\tstruct sigqueue *q;\n\n\tsigemptyset(&queue->signal);\n\twhile (!list_empty(&queue->list)) {\n\t\tq = list_entry(queue->list.next, struct sigqueue , list);\n\t\tlist_del_init(&q->list);\n\t\t__sigqueue_free(q);\n\t}\n}\n\n/*\n * Flush all pending signals for a task.\n */\nvoid __flush_signals(struct task_struct *t)\n{\n\tclear_tsk_thread_flag(t, TIF_SIGPENDING);\n\tflush_sigqueue(&t->pending);\n\tflush_sigqueue(&t->signal->shared_pending);\n}\n\nvoid flush_signals(struct task_struct *t)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\t__flush_signals(t);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n}\n\nstatic void __flush_itimer_signals(struct sigpending *pending)\n{\n\tsigset_t signal, retain;\n\tstruct sigqueue *q, *n;\n\n\tsignal = pending->signal;\n\tsigemptyset(&retain);\n\n\tlist_for_each_entry_safe(q, n, &pending->list, list) {\n\t\tint sig = q->info.si_signo;\n\n\t\tif (likely(q->info.si_code != SI_TIMER)) {\n\t\t\tsigaddset(&retain, sig);\n\t\t} else {\n\t\t\tsigdelset(&signal, sig);\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\n\tsigorsets(&pending->signal, &signal, &retain);\n}\n\nvoid flush_itimer_signals(void)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tsk->sighand->siglock, flags);\n\t__flush_itimer_signals(&tsk->pending);\n\t__flush_itimer_signals(&tsk->signal->shared_pending);\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, flags);\n}\n\nvoid ignore_signals(struct task_struct *t)\n{\n\tint i;\n\n\tfor (i = 0; i < _NSIG; ++i)\n\t\tt->sighand->action[i].sa.sa_handler = SIG_IGN;\n\n\tflush_signals(t);\n}\n\n/*\n * Flush all handlers for a task.\n */\n\nvoid\nflush_signal_handlers(struct task_struct *t, int force_default)\n{\n\tint i;\n\tstruct k_sigaction *ka = &t->sighand->action[0];\n\tfor (i = _NSIG ; i != 0 ; i--) {\n\t\tif (force_default || ka->sa.sa_handler != SIG_IGN)\n\t\t\tka->sa.sa_handler = SIG_DFL;\n\t\tka->sa.sa_flags = 0;\n\t\tsigemptyset(&ka->sa.sa_mask);\n\t\tka++;\n\t}\n}\n\nint unhandled_signal(struct task_struct *tsk, int sig)\n{\n\tvoid __user *handler = tsk->sighand->action[sig-1].sa.sa_handler;\n\tif (is_global_init(tsk))\n\t\treturn 1;\n\tif (handler != SIG_IGN && handler != SIG_DFL)\n\t\treturn 0;\n\t/* if ptraced, let the tracer determine */\n\treturn !tsk->ptrace;\n}\n\n/*\n * Notify the system that a driver wants to block all signals for this\n * process, and wants to be notified if any signals at all were to be\n * sent/acted upon.  If the notifier routine returns non-zero, then the\n * signal will be acted upon after all.  If the notifier routine returns 0,\n * then then signal will be blocked.  Only one block per process is\n * allowed.  priv is a pointer to private data that the notifier routine\n * can use to determine if the signal should be blocked or not.\n */\nvoid\nblock_all_signals(int (*notifier)(void *priv), void *priv, sigset_t *mask)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\tcurrent->notifier_mask = mask;\n\tcurrent->notifier_data = priv;\n\tcurrent->notifier = notifier;\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\n/* Notify the system that blocking has ended. */\n\nvoid\nunblock_all_signals(void)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\tcurrent->notifier = NULL;\n\tcurrent->notifier_data = NULL;\n\trecalc_sigpending();\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\nstatic void collect_signal(int sig, struct sigpending *list, siginfo_t *info)\n{\n\tstruct sigqueue *q, *first = NULL;\n\n\t/*\n\t * Collect the siginfo appropriate to this signal.  Check if\n\t * there is another siginfo for the same signal.\n\t*/\n\tlist_for_each_entry(q, &list->list, list) {\n\t\tif (q->info.si_signo == sig) {\n\t\t\tif (first)\n\t\t\t\tgoto still_pending;\n\t\t\tfirst = q;\n\t\t}\n\t}\n\n\tsigdelset(&list->signal, sig);\n\n\tif (first) {\nstill_pending:\n\t\tlist_del_init(&first->list);\n\t\tcopy_siginfo(info, &first->info);\n\t\t__sigqueue_free(first);\n\t} else {\n\t\t/*\n\t\t * Ok, it wasn't in the queue.  This must be\n\t\t * a fast-pathed signal or we must have been\n\t\t * out of queue space.  So zero out the info.\n\t\t */\n\t\tinfo->si_signo = sig;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\tinfo->si_pid = 0;\n\t\tinfo->si_uid = 0;\n\t}\n}\n\nstatic int __dequeue_signal(struct sigpending *pending, sigset_t *mask,\n\t\t\tsiginfo_t *info)\n{\n\tint sig = next_signal(pending, mask);\n\n\tif (sig) {\n\t\tif (current->notifier) {\n\t\t\tif (sigismember(current->notifier_mask, sig)) {\n\t\t\t\tif (!(current->notifier)(current->notifier_data)) {\n\t\t\t\t\tclear_thread_flag(TIF_SIGPENDING);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tcollect_signal(sig, pending, info);\n\t}\n\n\treturn sig;\n}\n\n/*\n * Dequeue a signal and return the element to the caller, which is\n * expected to free it.\n *\n * All callers have to hold the siglock.\n */\nint dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)\n{\n\tint signr;\n\n\t/* We only dequeue private signals from ourselves, we don't let\n\t * signalfd steal them\n\t */\n\tsignr = __dequeue_signal(&tsk->pending, mask, info);\n\tif (!signr) {\n\t\tsignr = __dequeue_signal(&tsk->signal->shared_pending,\n\t\t\t\t\t mask, info);\n\t\t/*\n\t\t * itimer signal ?\n\t\t *\n\t\t * itimers are process shared and we restart periodic\n\t\t * itimers in the signal delivery path to prevent DoS\n\t\t * attacks in the high resolution timer case. This is\n\t\t * compliant with the old way of self-restarting\n\t\t * itimers, as the SIGALRM is a legacy signal and only\n\t\t * queued once. Changing the restart behaviour to\n\t\t * restart the timer in the signal dequeue path is\n\t\t * reducing the timer noise on heavy loaded !highres\n\t\t * systems too.\n\t\t */\n\t\tif (unlikely(signr == SIGALRM)) {\n\t\t\tstruct hrtimer *tmr = &tsk->signal->real_timer;\n\n\t\t\tif (!hrtimer_is_queued(tmr) &&\n\t\t\t    tsk->signal->it_real_incr.tv64 != 0) {\n\t\t\t\thrtimer_forward(tmr, tmr->base->get_time(),\n\t\t\t\t\t\ttsk->signal->it_real_incr);\n\t\t\t\thrtimer_restart(tmr);\n\t\t\t}\n\t\t}\n\t}\n\n\trecalc_sigpending();\n\tif (!signr)\n\t\treturn 0;\n\n\tif (unlikely(sig_kernel_stop(signr))) {\n\t\t/*\n\t\t * Set a marker that we have dequeued a stop signal.  Our\n\t\t * caller might release the siglock and then the pending\n\t\t * stop signal it is about to process is no longer in the\n\t\t * pending bitmasks, but must still be cleared by a SIGCONT\n\t\t * (and overruled by a SIGKILL).  So those cases clear this\n\t\t * shared flag after we've set it.  Note that this flag may\n\t\t * remain set after the signal we return is ignored or\n\t\t * handled.  That doesn't matter because its only purpose\n\t\t * is to alert stop-signal processing code when another\n\t\t * processor has come along and cleared the flag.\n\t\t */\n\t\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\t}\n\tif ((info->si_code & __SI_MASK) == __SI_TIMER && info->si_sys_private) {\n\t\t/*\n\t\t * Release the siglock to ensure proper locking order\n\t\t * of timer locks outside of siglocks.  Note, we leave\n\t\t * irqs disabled here, since the posix-timers code is\n\t\t * about to disable them again anyway.\n\t\t */\n\t\tspin_unlock(&tsk->sighand->siglock);\n\t\tdo_schedule_next_timer(info);\n\t\tspin_lock(&tsk->sighand->siglock);\n\t}\n\treturn signr;\n}\n\n/*\n * Tell a process that it has a new active signal..\n *\n * NOTE! we rely on the previous spin_lock to\n * lock interrupts for us! We can only be called with\n * \"siglock\" held, and the local interrupt must\n * have been disabled when that got acquired!\n *\n * No need to set need_resched since signal event passing\n * goes through ->blocked\n */\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}\n\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n *\n * This version takes a sigset mask and looks at all signals,\n * not just those in the first mask word.\n */\nstatic int rm_from_queue_full(sigset_t *mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\tsigset_t m;\n\n\tsigandsets(&m, mask, &s->signal);\n\tif (sigisemptyset(&m))\n\t\treturn 0;\n\n\tsigandnsets(&s->signal, &s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (sigismember(mask, q->info.si_signo)) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\treturn 1;\n}\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n */\nstatic int rm_from_queue(unsigned long mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\n\tif (!sigtestsetmask(&s->signal, mask))\n\t\treturn 0;\n\n\tsigdelsetmask(&s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (q->info.si_signo < SIGRTMIN &&\n\t\t    (mask & sigmask(q->info.si_signo))) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic inline int is_si_special(const struct siginfo *info)\n{\n\treturn info <= SEND_SIG_FORCED;\n}\n\nstatic inline bool si_fromuser(const struct siginfo *info)\n{\n\treturn info == SEND_SIG_NOINFO ||\n\t\t(!is_si_special(info) && SI_FROMUSER(info));\n}\n\n/*\n * called with RCU read lock from check_kill_permission()\n */\nstatic int kill_ok_by_cred(struct task_struct *t)\n{\n\tconst struct cred *cred = current_cred();\n\tconst struct cred *tcred = __task_cred(t);\n\n\tif (uid_eq(cred->euid, tcred->suid) ||\n\t    uid_eq(cred->euid, tcred->uid)  ||\n\t    uid_eq(cred->uid,  tcred->suid) ||\n\t    uid_eq(cred->uid,  tcred->uid))\n\t\treturn 1;\n\n\tif (ns_capable(tcred->user_ns, CAP_KILL))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n/*\n * Bad permissions for sending the signal\n * - the caller must hold the RCU read lock\n */\nstatic int check_kill_permission(int sig, struct siginfo *info,\n\t\t\t\t struct task_struct *t)\n{\n\tstruct pid *sid;\n\tint error;\n\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\tif (!si_fromuser(info))\n\t\treturn 0;\n\n\terror = audit_signal_info(sig, t); /* Let audit system see the signal */\n\tif (error)\n\t\treturn error;\n\n\tif (!same_thread_group(current, t) &&\n\t    !kill_ok_by_cred(t)) {\n\t\tswitch (sig) {\n\t\tcase SIGCONT:\n\t\t\tsid = task_session(t);\n\t\t\t/*\n\t\t\t * We don't return the error if sid == NULL. The\n\t\t\t * task was unhashed, the caller must notice this.\n\t\t\t */\n\t\t\tif (!sid || sid == task_session(current))\n\t\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n\n\treturn security_task_kill(t, info, sig, 0);\n}\n\n/**\n * ptrace_trap_notify - schedule trap to notify ptracer\n * @t: tracee wanting to notify tracer\n *\n * This function schedules sticky ptrace trap which is cleared on the next\n * TRAP_STOP to notify ptracer of an event.  @t must have been seized by\n * ptracer.\n *\n * If @t is running, STOP trap will be taken.  If trapped for STOP and\n * ptracer is listening for events, tracee is woken up so that it can\n * re-trap for the new event.  If trapped otherwise, STOP trap will be\n * eventually taken without returning to userland after the existing traps\n * are finished by PTRACE_CONT.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nstatic void ptrace_trap_notify(struct task_struct *t)\n{\n\tWARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\n\tassert_spin_locked(&t->sighand->siglock);\n\n\ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n}\n\n/*\n * Handle magic process-wide effects of stop/continue signals. Unlike\n * the signal actions, these happen immediately at signal-generation\n * time regardless of blocking, ignoring, or handling.  This does the\n * actual continuing for SIGCONT, but not the actual stopping for stop\n * signals. The process stop is done as a signal action for SIG_DFL.\n *\n * Returns true if the signal should be actually delivered, otherwise\n * it should be dropped.\n */\nstatic int prepare_signal(int sig, struct task_struct *p, bool force)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\tif (unlikely(signal->flags & SIGNAL_GROUP_EXIT)) {\n\t\t/*\n\t\t * The process is in the middle of dying, nothing to do.\n\t\t */\n\t} else if (sig_kernel_stop(sig)) {\n\t\t/*\n\t\t * This is a stop signal.  Remove SIGCONT from all queues.\n\t\t */\n\t\trm_from_queue(sigmask(SIGCONT), &signal->shared_pending);\n\t\tt = p;\n\t\tdo {\n\t\t\trm_from_queue(sigmask(SIGCONT), &t->pending);\n\t\t} while_each_thread(p, t);\n\t} else if (sig == SIGCONT) {\n\t\tunsigned int why;\n\t\t/*\n\t\t * Remove all stop signals from all queues, wake all threads.\n\t\t */\n\t\trm_from_queue(SIG_KERNEL_STOP_MASK, &signal->shared_pending);\n\t\tt = p;\n\t\tdo {\n\t\t\ttask_clear_jobctl_pending(t, JOBCTL_STOP_PENDING);\n\t\t\trm_from_queue(SIG_KERNEL_STOP_MASK, &t->pending);\n\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\twake_up_state(t, __TASK_STOPPED);\n\t\t\telse\n\t\t\t\tptrace_trap_notify(t);\n\t\t} while_each_thread(p, t);\n\n\t\t/*\n\t\t * Notify the parent with CLD_CONTINUED if we were stopped.\n\t\t *\n\t\t * If we were in the middle of a group stop, we pretend it\n\t\t * was already finished, and then continued. Since SIGCHLD\n\t\t * doesn't queue we report only CLD_STOPPED, as if the next\n\t\t * CLD_CONTINUED was dropped.\n\t\t */\n\t\twhy = 0;\n\t\tif (signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\twhy |= SIGNAL_CLD_CONTINUED;\n\t\telse if (signal->group_stop_count)\n\t\t\twhy |= SIGNAL_CLD_STOPPED;\n\n\t\tif (why) {\n\t\t\t/*\n\t\t\t * The first thread which returns from do_signal_stop()\n\t\t\t * will take ->siglock, notice SIGNAL_CLD_MASK, and\n\t\t\t * notify its parent. See get_signal_to_deliver().\n\t\t\t */\n\t\t\tsignal->flags = why | SIGNAL_STOP_CONTINUED;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tsignal->group_exit_code = 0;\n\t\t}\n\t}\n\n\treturn !sig_ignored(p, sig, force);\n}\n\n/*\n * Test if P wants to take SIG.  After we've checked all threads with this,\n * it's equivalent to finding no threads not blocking SIG.  Any threads not\n * blocking SIG were ruled out because they are not running and already\n * have pending signals.  Such threads will dequeue from the shared queue\n * as soon as they're available, so putting the signal on the shared queue\n * will be equivalent to sending it to one such thread.\n */\nstatic inline int wants_signal(int sig, struct task_struct *p)\n{\n\tif (sigismember(&p->blocked, sig))\n\t\treturn 0;\n\tif (p->flags & PF_EXITING)\n\t\treturn 0;\n\tif (sig == SIGKILL)\n\t\treturn 1;\n\tif (task_is_stopped_or_traced(p))\n\t\treturn 0;\n\treturn task_curr(p) || !signal_pending(p);\n}\n\nstatic void complete_signal(int sig, struct task_struct *p, int group)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\t/*\n\t * Now find a thread we can wake up to take the signal off the queue.\n\t *\n\t * If the main thread wants the signal, it gets first crack.\n\t * Probably the least surprising to the average bear.\n\t */\n\tif (wants_signal(sig, p))\n\t\tt = p;\n\telse if (!group || thread_group_empty(p))\n\t\t/*\n\t\t * There is just one thread and it does not need to be woken.\n\t\t * It will dequeue unblocked signals before it runs again.\n\t\t */\n\t\treturn;\n\telse {\n\t\t/*\n\t\t * Otherwise try to find a suitable thread.\n\t\t */\n\t\tt = signal->curr_target;\n\t\twhile (!wants_signal(sig, t)) {\n\t\t\tt = next_thread(t);\n\t\t\tif (t == signal->curr_target)\n\t\t\t\t/*\n\t\t\t\t * No thread needs to be woken.\n\t\t\t\t * Any eligible threads will see\n\t\t\t\t * the signal in the queue soon.\n\t\t\t\t */\n\t\t\t\treturn;\n\t\t}\n\t\tsignal->curr_target = t;\n\t}\n\n\t/*\n\t * Found a killable thread.  If the signal will be fatal,\n\t * then start taking the whole group down immediately.\n\t */\n\tif (sig_fatal(p, sig) &&\n\t    !(signal->flags & (SIGNAL_UNKILLABLE | SIGNAL_GROUP_EXIT)) &&\n\t    !sigismember(&t->real_blocked, sig) &&\n\t    (sig == SIGKILL || !t->ptrace)) {\n\t\t/*\n\t\t * This signal will be fatal to the whole group.\n\t\t */\n\t\tif (!sig_kernel_coredump(sig)) {\n\t\t\t/*\n\t\t\t * Start a group exit and wake everybody up.\n\t\t\t * This way we don't have other threads\n\t\t\t * running and doing things after a slower\n\t\t\t * thread has the fatal signal pending.\n\t\t\t */\n\t\t\tsignal->flags = SIGNAL_GROUP_EXIT;\n\t\t\tsignal->group_exit_code = sig;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tt = p;\n\t\t\tdo {\n\t\t\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\t\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\t\t\tsignal_wake_up(t, 1);\n\t\t\t} while_each_thread(p, t);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * The signal is already in the shared-pending queue.\n\t * Tell the chosen thread to wake up and dequeue it.\n\t */\n\tsignal_wake_up(t, sig == SIGKILL);\n\treturn;\n}\n\nstatic inline int legacy_queue(struct sigpending *signals, int sig)\n{\n\treturn (sig < SIGRTMIN) && sigismember(&signals->signal, sig);\n}\n\n#ifdef CONFIG_USER_NS\nstatic inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)\n{\n\tif (current_user_ns() == task_cred_xxx(t, user_ns))\n\t\treturn;\n\n\tif (SI_FROMKERNEL(info))\n\t\treturn;\n\n\trcu_read_lock();\n\tinfo->si_uid = from_kuid_munged(task_cred_xxx(t, user_ns),\n\t\t\t\t\tmake_kuid(current_user_ns(), info->si_uid));\n\trcu_read_unlock();\n}\n#else\nstatic inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)\n{\n\treturn;\n}\n#endif\n\nstatic int __send_signal(int sig, struct siginfo *info, struct task_struct *t,\n\t\t\tint group, int from_ancestor_ns)\n{\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint override_rlimit;\n\tint ret = 0, result;\n\n\tassert_spin_locked(&t->sighand->siglock);\n\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t,\n\t\t\tfrom_ancestor_ns || (info == SEND_SIG_FORCED)))\n\t\tgoto ret;\n\n\tpending = group ? &t->signal->shared_pending : &t->pending;\n\t/*\n\t * Short-circuit ignored signals and support queuing\n\t * exactly one non-rt signal, so that we can get more\n\t * detailed information about the cause of the signal.\n\t */\n\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\tif (legacy_queue(pending, sig))\n\t\tgoto ret;\n\n\tresult = TRACE_SIGNAL_DELIVERED;\n\t/*\n\t * fast-pathed signals for kernel-internal things like SIGSTOP\n\t * or SIGKILL.\n\t */\n\tif (info == SEND_SIG_FORCED)\n\t\tgoto out_set;\n\n\t/*\n\t * Real-time signals must be queued if sent by sigqueue, or\n\t * some other real-time mechanism.  It is implementation\n\t * defined whether kill() does so.  We attempt to do so, on\n\t * the principle of least surprise, but since kill is not\n\t * allowed to fail with EAGAIN when low on memory we just\n\t * make sure at least one signal gets delivered and don't\n\t * pass on the info struct.\n\t */\n\tif (sig < SIGRTMIN)\n\t\toverride_rlimit = (is_si_special(info) || info->si_code >= 0);\n\telse\n\t\toverride_rlimit = 0;\n\n\tq = __sigqueue_alloc(sig, t, GFP_ATOMIC | __GFP_NOTRACK_FALSE_POSITIVE,\n\t\toverride_rlimit);\n\tif (q) {\n\t\tlist_add_tail(&q->list, &pending->list);\n\t\tswitch ((unsigned long) info) {\n\t\tcase (unsigned long) SEND_SIG_NOINFO:\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_USER;\n\t\t\tq->info.si_pid = task_tgid_nr_ns(current,\n\t\t\t\t\t\t\ttask_active_pid_ns(t));\n\t\t\tq->info.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\t\t\tbreak;\n\t\tcase (unsigned long) SEND_SIG_PRIV:\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_KERNEL;\n\t\t\tq->info.si_pid = 0;\n\t\t\tq->info.si_uid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tcopy_siginfo(&q->info, info);\n\t\t\tif (from_ancestor_ns)\n\t\t\t\tq->info.si_pid = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tuserns_fixup_signal_uid(&q->info, t);\n\n\t} else if (!is_si_special(info)) {\n\t\tif (sig >= SIGRTMIN && info->si_code != SI_USER) {\n\t\t\t/*\n\t\t\t * Queue overflow, abort.  We may abort if the\n\t\t\t * signal was rt and sent by user using something\n\t\t\t * other than kill().\n\t\t\t */\n\t\t\tresult = TRACE_SIGNAL_OVERFLOW_FAIL;\n\t\t\tret = -EAGAIN;\n\t\t\tgoto ret;\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is a silent loss of information.  We still\n\t\t\t * send the signal, but the *info bits are lost.\n\t\t\t */\n\t\t\tresult = TRACE_SIGNAL_LOSE_INFO;\n\t\t}\n\t}\n\nout_set:\n\tsignalfd_notify(t, sig);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, group);\nret:\n\ttrace_signal_generate(sig, info, t, group, result);\n\treturn ret;\n}\n\nstatic int send_signal(int sig, struct siginfo *info, struct task_struct *t,\n\t\t\tint group)\n{\n\tint from_ancestor_ns = 0;\n\n#ifdef CONFIG_PID_NS\n\tfrom_ancestor_ns = si_fromuser(info) &&\n\t\t\t   !task_pid_nr_ns(current, task_active_pid_ns(t));\n#endif\n\n\treturn __send_signal(sig, info, t, group, from_ancestor_ns);\n}\n\nstatic void print_fatal_signal(int signr)\n{\n\tstruct pt_regs *regs = signal_pt_regs();\n\tprintk(\"%s/%d: potentially unexpected fatal signal %d.\\n\",\n\t\tcurrent->comm, task_pid_nr(current), signr);\n\n#if defined(__i386__) && !defined(__arch_um__)\n\tprintk(\"code at %08lx: \", regs->ip);\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < 16; i++) {\n\t\t\tunsigned char insn;\n\n\t\t\tif (get_user(insn, (unsigned char *)(regs->ip + i)))\n\t\t\t\tbreak;\n\t\t\tprintk(\"%02x \", insn);\n\t\t}\n\t}\n#endif\n\tprintk(\"\\n\");\n\tpreempt_disable();\n\tshow_regs(regs);\n\tpreempt_enable();\n}\n\nstatic int __init setup_print_fatal_signals(char *str)\n{\n\tget_option (&str, &print_fatal_signals);\n\n\treturn 1;\n}\n\n__setup(\"print-fatal-signals=\", setup_print_fatal_signals);\n\nint\n__group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\treturn send_signal(sig, info, p, 1);\n}\n\nstatic int\nspecific_send_sig_info(int sig, struct siginfo *info, struct task_struct *t)\n{\n\treturn send_signal(sig, info, t, 0);\n}\n\nint do_send_sig_info(int sig, struct siginfo *info, struct task_struct *p,\n\t\t\tbool group)\n{\n\tunsigned long flags;\n\tint ret = -ESRCH;\n\n\tif (lock_task_sighand(p, &flags)) {\n\t\tret = send_signal(sig, info, p, group);\n\t\tunlock_task_sighand(p, &flags);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Force a signal that the process can't ignore: if necessary\n * we unblock the signal and change any SIG_IGN to SIG_DFL.\n *\n * Note: If we unblock the signal, we always reset it to SIG_DFL,\n * since we do not want to have a signal handler that was blocked\n * be invoked when user space had explicitly blocked it.\n *\n * We don't want to have recursive SIGSEGV's etc, for example,\n * that is why we also clear SIGNAL_UNKILLABLE.\n */\nint\nforce_sig_info(int sig, struct siginfo *info, struct task_struct *t)\n{\n\tunsigned long int flags;\n\tint ret, blocked, ignored;\n\tstruct k_sigaction *action;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\taction = &t->sighand->action[sig-1];\n\tignored = action->sa.sa_handler == SIG_IGN;\n\tblocked = sigismember(&t->blocked, sig);\n\tif (blocked || ignored) {\n\t\taction->sa.sa_handler = SIG_DFL;\n\t\tif (blocked) {\n\t\t\tsigdelset(&t->blocked, sig);\n\t\t\trecalc_sigpending_and_wake(t);\n\t\t}\n\t}\n\tif (action->sa.sa_handler == SIG_DFL)\n\t\tt->signal->flags &= ~SIGNAL_UNKILLABLE;\n\tret = specific_send_sig_info(sig, info, t);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n\n\treturn ret;\n}\n\n/*\n * Nuke all other threads in the group.\n */\nint zap_other_threads(struct task_struct *p)\n{\n\tstruct task_struct *t = p;\n\tint count = 0;\n\n\tp->signal->group_stop_count = 0;\n\n\twhile_each_thread(p, t) {\n\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\tcount++;\n\n\t\t/* Don't bother with already dead threads */\n\t\tif (t->exit_state)\n\t\t\tcontinue;\n\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\tsignal_wake_up(t, 1);\n\t}\n\n\treturn count;\n}\n\nstruct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t   unsigned long *flags)\n{\n\tstruct sighand_struct *sighand;\n\n\tfor (;;) {\n\t\tlocal_irq_save(*flags);\n\t\trcu_read_lock();\n\t\tsighand = rcu_dereference(tsk->sighand);\n\t\tif (unlikely(sighand == NULL)) {\n\t\t\trcu_read_unlock();\n\t\t\tlocal_irq_restore(*flags);\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_lock(&sighand->siglock);\n\t\tif (likely(sighand == tsk->sighand)) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&sighand->siglock);\n\t\trcu_read_unlock();\n\t\tlocal_irq_restore(*flags);\n\t}\n\n\treturn sighand;\n}\n\n/*\n * send signal info to all the members of a group\n */\nint group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = check_kill_permission(sig, info, p);\n\trcu_read_unlock();\n\n\tif (!ret && sig)\n\t\tret = do_send_sig_info(sig, info, p, true);\n\n\treturn ret;\n}\n\n/*\n * __kill_pgrp_info() sends a signal to a process group: this is what the tty\n * control characters do (^C, ^Z etc)\n * - the caller must hold at least a readlock on tasklist_lock\n */\nint __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp)\n{\n\tstruct task_struct *p = NULL;\n\tint retval, success;\n\n\tsuccess = 0;\n\tretval = -ESRCH;\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tint err = group_send_sig_info(sig, info, p);\n\t\tsuccess |= !err;\n\t\tretval = err;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\treturn success ? 0 : retval;\n}\n\nint kill_pid_info(int sig, struct siginfo *info, struct pid *pid)\n{\n\tint error = -ESRCH;\n\tstruct task_struct *p;\n\n\trcu_read_lock();\nretry:\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (p) {\n\t\terror = group_send_sig_info(sig, info, p);\n\t\tif (unlikely(error == -ESRCH))\n\t\t\t/*\n\t\t\t * The task was unhashed in between, try again.\n\t\t\t * If it is dead, pid_task() will return NULL,\n\t\t\t * if we race with de_thread() it will find the\n\t\t\t * new leader.\n\t\t\t */\n\t\t\tgoto retry;\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nint kill_proc_info(int sig, struct siginfo *info, pid_t pid)\n{\n\tint error;\n\trcu_read_lock();\n\terror = kill_pid_info(sig, info, find_vpid(pid));\n\trcu_read_unlock();\n\treturn error;\n}\n\nstatic int kill_as_cred_perm(const struct cred *cred,\n\t\t\t     struct task_struct *target)\n{\n\tconst struct cred *pcred = __task_cred(target);\n\tif (!uid_eq(cred->euid, pcred->suid) && !uid_eq(cred->euid, pcred->uid) &&\n\t    !uid_eq(cred->uid,  pcred->suid) && !uid_eq(cred->uid,  pcred->uid))\n\t\treturn 0;\n\treturn 1;\n}\n\n/* like kill_pid_info(), but doesn't use uid/euid of \"current\" */\nint kill_pid_info_as_cred(int sig, struct siginfo *info, struct pid *pid,\n\t\t\t const struct cred *cred, u32 secid)\n{\n\tint ret = -EINVAL;\n\tstruct task_struct *p;\n\tunsigned long flags;\n\n\tif (!valid_signal(sig))\n\t\treturn ret;\n\n\trcu_read_lock();\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (!p) {\n\t\tret = -ESRCH;\n\t\tgoto out_unlock;\n\t}\n\tif (si_fromuser(info) && !kill_as_cred_perm(cred, p)) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\tret = security_task_kill(p, info, sig, secid);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tif (sig) {\n\t\tif (lock_task_sighand(p, &flags)) {\n\t\t\tret = __send_signal(sig, info, p, 1, 0);\n\t\t\tunlock_task_sighand(p, &flags);\n\t\t} else\n\t\t\tret = -ESRCH;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kill_pid_info_as_cred);\n\n/*\n * kill_something_info() interprets pid in interesting ways just like kill(2).\n *\n * POSIX specifies that kill(-1,sig) is unspecified, but what we have\n * is probably wrong.  Should make it like BSD or SYSV.\n */\n\nstatic int kill_something_info(int sig, struct siginfo *info, pid_t pid)\n{\n\tint ret;\n\n\tif (pid > 0) {\n\t\trcu_read_lock();\n\t\tret = kill_pid_info(sig, info, find_vpid(pid));\n\t\trcu_read_unlock();\n\t\treturn ret;\n\t}\n\n\tread_lock(&tasklist_lock);\n\tif (pid != -1) {\n\t\tret = __kill_pgrp_info(sig, info,\n\t\t\t\tpid ? find_vpid(-pid) : task_pgrp(current));\n\t} else {\n\t\tint retval = 0, count = 0;\n\t\tstruct task_struct * p;\n\n\t\tfor_each_process(p) {\n\t\t\tif (task_pid_vnr(p) > 1 &&\n\t\t\t\t\t!same_thread_group(p, current)) {\n\t\t\t\tint err = group_send_sig_info(sig, info, p);\n\t\t\t\t++count;\n\t\t\t\tif (err != -EPERM)\n\t\t\t\t\tretval = err;\n\t\t\t}\n\t\t}\n\t\tret = count ? retval : -ESRCH;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * These are for backward compatibility with the rest of the kernel source.\n */\n\nint send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\t/*\n\t * Make sure legacy kernel users don't send in bad values\n\t * (normal paths check this in check_kill_permission).\n\t */\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\treturn do_send_sig_info(sig, info, p, false);\n}\n\n#define __si_special(priv) \\\n\t((priv) ? SEND_SIG_PRIV : SEND_SIG_NOINFO)\n\nint\nsend_sig(int sig, struct task_struct *p, int priv)\n{\n\treturn send_sig_info(sig, __si_special(priv), p);\n}\n\nvoid\nforce_sig(int sig, struct task_struct *p)\n{\n\tforce_sig_info(sig, SEND_SIG_PRIV, p);\n}\n\n/*\n * When things go south during signal handling, we\n * will force a SIGSEGV. And if the signal that caused\n * the problem was already a SIGSEGV, we'll want to\n * make sure we don't even try to deliver the signal..\n */\nint\nforce_sigsegv(int sig, struct task_struct *p)\n{\n\tif (sig == SIGSEGV) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&p->sighand->siglock, flags);\n\t\tp->sighand->action[sig - 1].sa.sa_handler = SIG_DFL;\n\t\tspin_unlock_irqrestore(&p->sighand->siglock, flags);\n\t}\n\tforce_sig(SIGSEGV, p);\n\treturn 0;\n}\n\nint kill_pgrp(struct pid *pid, int sig, int priv)\n{\n\tint ret;\n\n\tread_lock(&tasklist_lock);\n\tret = __kill_pgrp_info(sig, __si_special(priv), pid);\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kill_pgrp);\n\nint kill_pid(struct pid *pid, int sig, int priv)\n{\n\treturn kill_pid_info(sig, __si_special(priv), pid);\n}\nEXPORT_SYMBOL(kill_pid);\n\n/*\n * These functions support sending signals using preallocated sigqueue\n * structures.  This is needed \"because realtime applications cannot\n * afford to lose notifications of asynchronous events, like timer\n * expirations or I/O completions\".  In the case of POSIX Timers\n * we allocate the sigqueue structure from the timer_create.  If this\n * allocation fails we are able to report the failure to the application\n * with an EAGAIN error.\n */\nstruct sigqueue *sigqueue_alloc(void)\n{\n\tstruct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);\n\n\tif (q)\n\t\tq->flags |= SIGQUEUE_PREALLOC;\n\n\treturn q;\n}\n\nvoid sigqueue_free(struct sigqueue *q)\n{\n\tunsigned long flags;\n\tspinlock_t *lock = &current->sighand->siglock;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\t/*\n\t * We must hold ->siglock while testing q->list\n\t * to serialize with collect_signal() or with\n\t * __exit_signal()->flush_sigqueue().\n\t */\n\tspin_lock_irqsave(lock, flags);\n\tq->flags &= ~SIGQUEUE_PREALLOC;\n\t/*\n\t * If it is queued it will be freed when dequeued,\n\t * like the \"regular\" sigqueue.\n\t */\n\tif (!list_empty(&q->list))\n\t\tq = NULL;\n\tspin_unlock_irqrestore(lock, flags);\n\n\tif (q)\n\t\t__sigqueue_free(q);\n}\n\nint send_sigqueue(struct sigqueue *q, struct task_struct *t, int group)\n{\n\tint sig = q->info.si_signo;\n\tstruct sigpending *pending;\n\tunsigned long flags;\n\tint ret, result;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\n\tret = -1;\n\tif (!likely(lock_task_sighand(t, &flags)))\n\t\tgoto ret;\n\n\tret = 1; /* the signal is ignored */\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t, false))\n\t\tgoto out;\n\n\tret = 0;\n\tif (unlikely(!list_empty(&q->list))) {\n\t\t/*\n\t\t * If an SI_TIMER entry is already queue just increment\n\t\t * the overrun count.\n\t\t */\n\t\tBUG_ON(q->info.si_code != SI_TIMER);\n\t\tq->info.si_overrun++;\n\t\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\t\tgoto out;\n\t}\n\tq->info.si_overrun = 0;\n\n\tsignalfd_notify(t, sig);\n\tpending = group ? &t->signal->shared_pending : &t->pending;\n\tlist_add_tail(&q->list, &pending->list);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, group);\n\tresult = TRACE_SIGNAL_DELIVERED;\nout:\n\ttrace_signal_generate(sig, &q->info, t, group, result);\n\tunlock_task_sighand(t, &flags);\nret:\n\treturn ret;\n}\n\n/*\n * Let a parent know about the death of a child.\n * For a stopped/continued status change, use do_notify_parent_cldstop instead.\n *\n * Returns true if our parent ignored us and so we've switched to\n * self-reaping.\n */\nbool do_notify_parent(struct task_struct *tsk, int sig)\n{\n\tstruct siginfo info;\n\tunsigned long flags;\n\tstruct sighand_struct *psig;\n\tbool autoreap = false;\n\n\tBUG_ON(sig == -1);\n\n \t/* do_notify_parent_cldstop should have been called instead.  */\n \tBUG_ON(task_is_stopped_or_traced(tsk));\n\n\tBUG_ON(!tsk->ptrace &&\n\t       (tsk->group_leader != tsk || !thread_group_empty(tsk)));\n\n\tif (sig != SIGCHLD) {\n\t\t/*\n\t\t * This is only possible if parent == real_parent.\n\t\t * Check if it has changed security domain.\n\t\t */\n\t\tif (tsk->parent_exec_id != tsk->parent->self_exec_id)\n\t\t\tsig = SIGCHLD;\n\t}\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\t/*\n\t * We are under tasklist_lock here so our parent is tied to\n\t * us and cannot change.\n\t *\n\t * task_active_pid_ns will always return the same pid namespace\n\t * until a task passes through release_task.\n\t *\n\t * write_lock() currently calls preempt_disable() which is the\n\t * same as rcu_read_lock(), but according to Oleg, this is not\n\t * correct to rely on this\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(tsk->parent, user_ns),\n\t\t\t\t       task_uid(tsk));\n\trcu_read_unlock();\n\n\tinfo.si_utime = cputime_to_clock_t(tsk->utime + tsk->signal->utime);\n\tinfo.si_stime = cputime_to_clock_t(tsk->stime + tsk->signal->stime);\n\n\tinfo.si_status = tsk->exit_code & 0x7f;\n\tif (tsk->exit_code & 0x80)\n\t\tinfo.si_code = CLD_DUMPED;\n\telse if (tsk->exit_code & 0x7f)\n\t\tinfo.si_code = CLD_KILLED;\n\telse {\n\t\tinfo.si_code = CLD_EXITED;\n\t\tinfo.si_status = tsk->exit_code >> 8;\n\t}\n\n\tpsig = tsk->parent->sighand;\n\tspin_lock_irqsave(&psig->siglock, flags);\n\tif (!tsk->ptrace && sig == SIGCHLD &&\n\t    (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||\n\t     (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT))) {\n\t\t/*\n\t\t * We are exiting and our parent doesn't care.  POSIX.1\n\t\t * defines special semantics for setting SIGCHLD to SIG_IGN\n\t\t * or setting the SA_NOCLDWAIT flag: we should be reaped\n\t\t * automatically and not left for our parent's wait4 call.\n\t\t * Rather than having the parent do it as a magic kind of\n\t\t * signal handler, we just set this to tell do_exit that we\n\t\t * can be cleaned up without becoming a zombie.  Note that\n\t\t * we still call __wake_up_parent in this case, because a\n\t\t * blocked sys_wait4 might now return -ECHILD.\n\t\t *\n\t\t * Whether we send SIGCHLD or not for SA_NOCLDWAIT\n\t\t * is implementation-defined: we do (if you don't want\n\t\t * it, just use SIG_IGN instead).\n\t\t */\n\t\tautoreap = true;\n\t\tif (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN)\n\t\t\tsig = 0;\n\t}\n\tif (valid_signal(sig) && sig)\n\t\t__group_send_sig_info(sig, &info, tsk->parent);\n\t__wake_up_parent(tsk, tsk->parent);\n\tspin_unlock_irqrestore(&psig->siglock, flags);\n\n\treturn autoreap;\n}\n\n/**\n * do_notify_parent_cldstop - notify parent of stopped/continued state change\n * @tsk: task reporting the state change\n * @for_ptracer: the notification is for ptracer\n * @why: CLD_{CONTINUED|STOPPED|TRAPPED} to report\n *\n * Notify @tsk's parent that the stopped/continued state has changed.  If\n * @for_ptracer is %false, @tsk's group leader notifies to its real parent.\n * If %true, @tsk reports to @tsk->parent which should be the ptracer.\n *\n * CONTEXT:\n * Must be called with tasklist_lock at least read locked.\n */\nstatic void do_notify_parent_cldstop(struct task_struct *tsk,\n\t\t\t\t     bool for_ptracer, int why)\n{\n\tstruct siginfo info;\n\tunsigned long flags;\n\tstruct task_struct *parent;\n\tstruct sighand_struct *sighand;\n\n\tif (for_ptracer) {\n\t\tparent = tsk->parent;\n\t} else {\n\t\ttsk = tsk->group_leader;\n\t\tparent = tsk->real_parent;\n\t}\n\n\tinfo.si_signo = SIGCHLD;\n\tinfo.si_errno = 0;\n\t/*\n\t * see comment in do_notify_parent() about the following 4 lines\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(parent, user_ns), task_uid(tsk));\n\trcu_read_unlock();\n\n\tinfo.si_utime = cputime_to_clock_t(tsk->utime);\n\tinfo.si_stime = cputime_to_clock_t(tsk->stime);\n\n \tinfo.si_code = why;\n \tswitch (why) {\n \tcase CLD_CONTINUED:\n \t\tinfo.si_status = SIGCONT;\n \t\tbreak;\n \tcase CLD_STOPPED:\n \t\tinfo.si_status = tsk->signal->group_exit_code & 0x7f;\n \t\tbreak;\n \tcase CLD_TRAPPED:\n \t\tinfo.si_status = tsk->exit_code & 0x7f;\n \t\tbreak;\n \tdefault:\n \t\tBUG();\n \t}\n\n\tsighand = parent->sighand;\n\tspin_lock_irqsave(&sighand->siglock, flags);\n\tif (sighand->action[SIGCHLD-1].sa.sa_handler != SIG_IGN &&\n\t    !(sighand->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))\n\t\t__group_send_sig_info(SIGCHLD, &info, parent);\n\t/*\n\t * Even if SIGCHLD is not generated, we must wake up wait4 calls.\n\t */\n\t__wake_up_parent(tsk, parent);\n\tspin_unlock_irqrestore(&sighand->siglock, flags);\n}\n\nstatic inline int may_ptrace_stop(void)\n{\n\tif (!likely(current->ptrace))\n\t\treturn 0;\n\t/*\n\t * Are we in the middle of do_coredump?\n\t * If so and our tracer is also part of the coredump stopping\n\t * is a deadlock situation, and pointless because our tracer\n\t * is dead so don't allow us to stop.\n\t * If SIGKILL was already sent before the caller unlocked\n\t * ->siglock we must see ->core_state != NULL. Otherwise it\n\t * is safe to enter schedule().\n\t */\n\tif (unlikely(current->mm->core_state) &&\n\t    unlikely(current->mm == current->parent->mm))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * Return non-zero if there is a SIGKILL that should be waking us up.\n * Called with the siglock held.\n */\nstatic int sigkill_pending(struct task_struct *tsk)\n{\n\treturn\tsigismember(&tsk->pending.signal, SIGKILL) ||\n\t\tsigismember(&tsk->signal->shared_pending.signal, SIGKILL);\n}\n\n/*\n * This must be called with current->sighand->siglock held.\n *\n * This should be the path for all ptrace stops.\n * We always set current->last_siginfo while stopped here.\n * That makes it a way to test a stopped process for\n * being ptrace-stopped vs being job-control-stopped.\n *\n * If we actually decide not to stop at all because the tracer\n * is gone, we keep current->exit_code unless clear_code.\n */\nstatic void ptrace_stop(int exit_code, int why, int clear_code, siginfo_t *info)\n\t__releases(&current->sighand->siglock)\n\t__acquires(&current->sighand->siglock)\n{\n\tbool gstop_done = false;\n\n\tif (arch_ptrace_stop_needed(exit_code, info)) {\n\t\t/*\n\t\t * The arch code has something special to do before a\n\t\t * ptrace stop.  This is allowed to block, e.g. for faults\n\t\t * on user stack pages.  We can't keep the siglock while\n\t\t * calling arch_ptrace_stop, so we must release it now.\n\t\t * To preserve proper semantics, we must do this before\n\t\t * any signal bookkeeping like checking group_stop_count.\n\t\t * Meanwhile, a SIGKILL could come in before we retake the\n\t\t * siglock.  That must prevent us from sleeping in TASK_TRACED.\n\t\t * So after regaining the lock, we must check for SIGKILL.\n\t\t */\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tarch_ptrace_stop(exit_code, info);\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\tif (sigkill_pending(current))\n\t\t\treturn;\n\t}\n\n\t/*\n\t * We're committing to trapping.  TRACED should be visible before\n\t * TRAPPING is cleared; otherwise, the tracer might fail do_wait().\n\t * Also, transition to TRACED and updates to ->jobctl should be\n\t * atomic with respect to siglock and should be done after the arch\n\t * hook as siglock is released and regrabbed across it.\n\t */\n\tset_current_state(TASK_TRACED);\n\n\tcurrent->last_siginfo = info;\n\tcurrent->exit_code = exit_code;\n\n\t/*\n\t * If @why is CLD_STOPPED, we're trapping to participate in a group\n\t * stop.  Do the bookkeeping.  Note that if SIGCONT was delievered\n\t * across siglock relocks since INTERRUPT was scheduled, PENDING\n\t * could be clear now.  We act as if SIGCONT is received after\n\t * TASK_TRACED is entered - ignore it.\n\t */\n\tif (why == CLD_STOPPED && (current->jobctl & JOBCTL_STOP_PENDING))\n\t\tgstop_done = task_participate_group_stop(current);\n\n\t/* any trap clears pending STOP trap, STOP trap clears NOTIFY */\n\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\tif (info && info->si_code >> 8 == PTRACE_EVENT_STOP)\n\t\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_NOTIFY);\n\n\t/* entering a trap, clear TRAPPING */\n\ttask_clear_jobctl_trapping(current);\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\tread_lock(&tasklist_lock);\n\tif (may_ptrace_stop()) {\n\t\t/*\n\t\t * Notify parents of the stop.\n\t\t *\n\t\t * While ptraced, there are two parents - the ptracer and\n\t\t * the real_parent of the group_leader.  The ptracer should\n\t\t * know about every stop while the real parent is only\n\t\t * interested in the completion of group stop.  The states\n\t\t * for the two don't interact with each other.  Notify\n\t\t * separately unless they're gonna be duplicates.\n\t\t */\n\t\tdo_notify_parent_cldstop(current, true, why);\n\t\tif (gstop_done && ptrace_reparented(current))\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/*\n\t\t * Don't want to allow preemption here, because\n\t\t * sys_ptrace() needs this task to be inactive.\n\t\t *\n\t\t * XXX: implement read_unlock_no_resched().\n\t\t */\n\t\tpreempt_disable();\n\t\tread_unlock(&tasklist_lock);\n\t\tpreempt_enable_no_resched();\n\t\tfreezable_schedule();\n\t} else {\n\t\t/*\n\t\t * By the time we got the lock, our tracer went away.\n\t\t * Don't drop the lock yet, another tracer may come.\n\t\t *\n\t\t * If @gstop_done, the ptracer went away between group stop\n\t\t * completion and here.  During detach, it would have set\n\t\t * JOBCTL_STOP_PENDING on us and we'll re-enter\n\t\t * TASK_STOPPED in do_signal_stop() on return, so notifying\n\t\t * the real parent of the group stop completion is enough.\n\t\t */\n\t\tif (gstop_done)\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t__set_current_state(TASK_RUNNING);\n\t\tif (clear_code)\n\t\t\tcurrent->exit_code = 0;\n\t\tread_unlock(&tasklist_lock);\n\t}\n\n\t/*\n\t * We are back.  Now reacquire the siglock before touching\n\t * last_siginfo, so that we are sure to have synchronized with\n\t * any signal-sending on another CPU that wants to examine it.\n\t */\n\tspin_lock_irq(&current->sighand->siglock);\n\tcurrent->last_siginfo = NULL;\n\n\t/* LISTENING can be set only during STOP traps, clear it */\n\tcurrent->jobctl &= ~JOBCTL_LISTENING;\n\n\t/*\n\t * Queued signals ignored us while we were stopped for tracing.\n\t * So check for any that we should take before resuming user mode.\n\t * This sets TIF_SIGPENDING, but never clears it.\n\t */\n\trecalc_sigpending_tsk(current);\n}\n\nstatic void ptrace_do_notify(int signr, int exit_code, int why)\n{\n\tsiginfo_t info;\n\n\tmemset(&info, 0, sizeof info);\n\tinfo.si_signo = signr;\n\tinfo.si_code = exit_code;\n\tinfo.si_pid = task_pid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\t/* Let the debugger run.  */\n\tptrace_stop(exit_code, why, 1, &info);\n}\n\nvoid ptrace_notify(int exit_code)\n{\n\tBUG_ON((exit_code & (0x7f | ~0xffff)) != SIGTRAP);\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tptrace_do_notify(SIGTRAP, exit_code, CLD_TRAPPED);\n\tspin_unlock_irq(&current->sighand->siglock);\n}\n\n/**\n * do_signal_stop - handle group stop for SIGSTOP and other stop signals\n * @signr: signr causing group stop if initiating\n *\n * If %JOBCTL_STOP_PENDING is not set yet, initiate group stop with @signr\n * and participate in it.  If already set, participate in the existing\n * group stop.  If participated in a group stop (and thus slept), %true is\n * returned with siglock released.\n *\n * If ptraced, this function doesn't handle stop itself.  Instead,\n * %JOBCTL_TRAP_STOP is scheduled and %false is returned with siglock\n * untouched.  The caller must ensure that INTERRUPT trap handling takes\n * places afterwards.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which is released\n * on %true return.\n *\n * RETURNS:\n * %false if group stop is already cancelled or ptrace trap is scheduled.\n * %true if participated in group stop.\n */\nstatic bool do_signal_stop(int signr)\n\t__releases(&current->sighand->siglock)\n{\n\tstruct signal_struct *sig = current->signal;\n\n\tif (!(current->jobctl & JOBCTL_STOP_PENDING)) {\n\t\tunsigned int gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\n\t\tstruct task_struct *t;\n\n\t\t/* signr will be recorded in task->jobctl for retries */\n\t\tWARN_ON_ONCE(signr & ~JOBCTL_STOP_SIGMASK);\n\n\t\tif (!likely(current->jobctl & JOBCTL_STOP_DEQUEUED) ||\n\t\t    unlikely(signal_group_exit(sig)))\n\t\t\treturn false;\n\t\t/*\n\t\t * There is no group stop already in progress.  We must\n\t\t * initiate one now.\n\t\t *\n\t\t * While ptraced, a task may be resumed while group stop is\n\t\t * still in effect and then receive a stop signal and\n\t\t * initiate another group stop.  This deviates from the\n\t\t * usual behavior as two consecutive stop signals can't\n\t\t * cause two group stops when !ptraced.  That is why we\n\t\t * also check !task_is_stopped(t) below.\n\t\t *\n\t\t * The condition can be distinguished by testing whether\n\t\t * SIGNAL_STOP_STOPPED is already set.  Don't generate\n\t\t * group_exit_code in such case.\n\t\t *\n\t\t * This is not necessary for SIGNAL_STOP_CONTINUED because\n\t\t * an intervening stop signal is required to cause two\n\t\t * continued events regardless of ptrace.\n\t\t */\n\t\tif (!(sig->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsig->group_exit_code = signr;\n\n\t\tsig->group_stop_count = 0;\n\n\t\tif (task_set_jobctl_pending(current, signr | gstop))\n\t\t\tsig->group_stop_count++;\n\n\t\tfor (t = next_thread(current); t != current;\n\t\t     t = next_thread(t)) {\n\t\t\t/*\n\t\t\t * Setting state to TASK_STOPPED for a group\n\t\t\t * stop is always done with the siglock held,\n\t\t\t * so this check has no races.\n\t\t\t */\n\t\t\tif (!task_is_stopped(t) &&\n\t\t\t    task_set_jobctl_pending(t, signr | gstop)) {\n\t\t\t\tsig->group_stop_count++;\n\t\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\t\tsignal_wake_up(t, 0);\n\t\t\t\telse\n\t\t\t\t\tptrace_trap_notify(t);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (likely(!current->ptrace)) {\n\t\tint notify = 0;\n\n\t\t/*\n\t\t * If there are no other threads in the group, or if there\n\t\t * is a group stop in progress and we are the last to stop,\n\t\t * report to the parent.\n\t\t */\n\t\tif (task_participate_group_stop(current))\n\t\t\tnotify = CLD_STOPPED;\n\n\t\t__set_current_state(TASK_STOPPED);\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent of the group stop completion.  Because\n\t\t * we're not holding either the siglock or tasklist_lock\n\t\t * here, ptracer may attach inbetween; however, this is for\n\t\t * group stop and should always be delivered to the real\n\t\t * parent of the group leader.  The new ptracer will get\n\t\t * its notification when this task transitions into\n\t\t * TASK_TRACED.\n\t\t */\n\t\tif (notify) {\n\t\t\tread_lock(&tasklist_lock);\n\t\t\tdo_notify_parent_cldstop(current, false, notify);\n\t\t\tread_unlock(&tasklist_lock);\n\t\t}\n\n\t\t/* Now we don't run again until woken by SIGCONT or SIGKILL */\n\t\tfreezable_schedule();\n\t\treturn true;\n\t} else {\n\t\t/*\n\t\t * While ptraced, group stop is handled by STOP trap.\n\t\t * Schedule it and let the caller deal with it.\n\t\t */\n\t\ttask_set_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\t\treturn false;\n\t}\n}\n\n/**\n * do_jobctl_trap - take care of ptrace jobctl traps\n *\n * When PT_SEIZED, it's used for both group stop and explicit\n * SEIZE/INTERRUPT traps.  Both generate PTRACE_EVENT_STOP trap with\n * accompanying siginfo.  If stopped, lower eight bits of exit_code contain\n * the stop signal; otherwise, %SIGTRAP.\n *\n * When !PT_SEIZED, it's used only for group stop trap with stop signal\n * number as exit_code and no siginfo.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which may be\n * released and re-acquired before returning with intervening sleep.\n */\nstatic void do_jobctl_trap(void)\n{\n\tstruct signal_struct *signal = current->signal;\n\tint signr = current->jobctl & JOBCTL_STOP_SIGMASK;\n\n\tif (current->ptrace & PT_SEIZED) {\n\t\tif (!signal->group_stop_count &&\n\t\t    !(signal->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsignr = SIGTRAP;\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_do_notify(signr, signr | (PTRACE_EVENT_STOP << 8),\n\t\t\t\t CLD_STOPPED);\n\t} else {\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_stop(signr, CLD_STOPPED, 0, NULL);\n\t\tcurrent->exit_code = 0;\n\t}\n}\n\nstatic int ptrace_signal(int signr, siginfo_t *info)\n{\n\tptrace_signal_deliver();\n\t/*\n\t * We do not check sig_kernel_stop(signr) but set this marker\n\t * unconditionally because we do not know whether debugger will\n\t * change signr. This flag has no meaning unless we are going\n\t * to stop after return from ptrace_stop(). In this case it will\n\t * be checked in do_signal_stop(), we should only stop if it was\n\t * not cleared by SIGCONT while we were sleeping. See also the\n\t * comment in dequeue_signal().\n\t */\n\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\tptrace_stop(signr, CLD_TRAPPED, 0, info);\n\n\t/* We're back.  Did the debugger cancel the sig?  */\n\tsignr = current->exit_code;\n\tif (signr == 0)\n\t\treturn signr;\n\n\tcurrent->exit_code = 0;\n\n\t/*\n\t * Update the siginfo structure if the signal has\n\t * changed.  If the debugger wanted something\n\t * specific in the siginfo structure then it should\n\t * have updated *info via PTRACE_SETSIGINFO.\n\t */\n\tif (signr != info->si_signo) {\n\t\tinfo->si_signo = signr;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\trcu_read_lock();\n\t\tinfo->si_pid = task_pid_vnr(current->parent);\n\t\tinfo->si_uid = from_kuid_munged(current_user_ns(),\n\t\t\t\t\t\ttask_uid(current->parent));\n\t\trcu_read_unlock();\n\t}\n\n\t/* If the (new) signal is now blocked, requeue it.  */\n\tif (sigismember(&current->blocked, signr)) {\n\t\tspecific_send_sig_info(signr, info, current);\n\t\tsignr = 0;\n\t}\n\n\treturn signr;\n}\n\nint get_signal_to_deliver(siginfo_t *info, struct k_sigaction *return_ka,\n\t\t\t  struct pt_regs *regs, void *cookie)\n{\n\tstruct sighand_struct *sighand = current->sighand;\n\tstruct signal_struct *signal = current->signal;\n\tint signr;\n\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tif (unlikely(uprobe_deny_signal()))\n\t\treturn 0;\n\n\t/*\n\t * Do this once, we can't return to user-mode if freezing() == T.\n\t * do_signal_stop() and ptrace_stop() do freezable_schedule() and\n\t * thus do not need another check after return.\n\t */\n\ttry_to_freeze();\n\nrelock:\n\tspin_lock_irq(&sighand->siglock);\n\t/*\n\t * Every stopped thread goes here after wakeup. Check to see if\n\t * we should notify the parent, prepare_signal(SIGCONT) encodes\n\t * the CLD_ si_code into SIGNAL_CLD_MASK bits.\n\t */\n\tif (unlikely(signal->flags & SIGNAL_CLD_MASK)) {\n\t\tint why;\n\n\t\tif (signal->flags & SIGNAL_CLD_CONTINUED)\n\t\t\twhy = CLD_CONTINUED;\n\t\telse\n\t\t\twhy = CLD_STOPPED;\n\n\t\tsignal->flags &= ~SIGNAL_CLD_MASK;\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent that we're continuing.  This event is\n\t\t * always per-process and doesn't make whole lot of sense\n\t\t * for ptracers, who shouldn't consume the state via\n\t\t * wait(2) either, but, for backward compatibility, notify\n\t\t * the ptracer of the group leader too unless it's gonna be\n\t\t * a duplicate.\n\t\t */\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\tif (ptrace_reparented(current->group_leader))\n\t\t\tdo_notify_parent_cldstop(current->group_leader,\n\t\t\t\t\t\ttrue, why);\n\t\tread_unlock(&tasklist_lock);\n\n\t\tgoto relock;\n\t}\n\n\tfor (;;) {\n\t\tstruct k_sigaction *ka;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_STOP_PENDING) &&\n\t\t    do_signal_stop(0))\n\t\t\tgoto relock;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_TRAP_MASK)) {\n\t\t\tdo_jobctl_trap();\n\t\t\tspin_unlock_irq(&sighand->siglock);\n\t\t\tgoto relock;\n\t\t}\n\n\t\tsignr = dequeue_signal(current, &current->blocked, info);\n\n\t\tif (!signr)\n\t\t\tbreak; /* will return 0 */\n\n\t\tif (unlikely(current->ptrace) && signr != SIGKILL) {\n\t\t\tsignr = ptrace_signal(signr, info);\n\t\t\tif (!signr)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tka = &sighand->action[signr-1];\n\n\t\t/* Trace actually delivered signals. */\n\t\ttrace_signal_deliver(signr, info, ka);\n\n\t\tif (ka->sa.sa_handler == SIG_IGN) /* Do nothing.  */\n\t\t\tcontinue;\n\t\tif (ka->sa.sa_handler != SIG_DFL) {\n\t\t\t/* Run the handler.  */\n\t\t\t*return_ka = *ka;\n\n\t\t\tif (ka->sa.sa_flags & SA_ONESHOT)\n\t\t\t\tka->sa.sa_handler = SIG_DFL;\n\n\t\t\tbreak; /* will return non-zero \"signr\" value */\n\t\t}\n\n\t\t/*\n\t\t * Now we are doing the default action for this signal.\n\t\t */\n\t\tif (sig_kernel_ignore(signr)) /* Default is nothing. */\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Global init gets no signals it doesn't want.\n\t\t * Container-init gets no signals it doesn't want from same\n\t\t * container.\n\t\t *\n\t\t * Note that if global/container-init sees a sig_kernel_only()\n\t\t * signal here, the signal must have been generated internally\n\t\t * or must have come from an ancestor namespace. In either\n\t\t * case, the signal cannot be dropped.\n\t\t */\n\t\tif (unlikely(signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\t\t!sig_kernel_only(signr))\n\t\t\tcontinue;\n\n\t\tif (sig_kernel_stop(signr)) {\n\t\t\t/*\n\t\t\t * The default action is to stop all threads in\n\t\t\t * the thread group.  The job control signals\n\t\t\t * do nothing in an orphaned pgrp, but SIGSTOP\n\t\t\t * always works.  Note that siglock needs to be\n\t\t\t * dropped during the call to is_orphaned_pgrp()\n\t\t\t * because of lock ordering with tasklist_lock.\n\t\t\t * This allows an intervening SIGCONT to be posted.\n\t\t\t * We need to check for that and bail out if necessary.\n\t\t\t */\n\t\t\tif (signr != SIGSTOP) {\n\t\t\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t\t\t/* signals can be posted during this window */\n\n\t\t\t\tif (is_current_pgrp_orphaned())\n\t\t\t\t\tgoto relock;\n\n\t\t\t\tspin_lock_irq(&sighand->siglock);\n\t\t\t}\n\n\t\t\tif (likely(do_signal_stop(info->si_signo))) {\n\t\t\t\t/* It released the siglock.  */\n\t\t\t\tgoto relock;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We didn't actually stop, due to a race\n\t\t\t * with SIGCONT or something like that.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Anything else is fatal, maybe with a core dump.\n\t\t */\n\t\tcurrent->flags |= PF_SIGNALED;\n\n\t\tif (sig_kernel_coredump(signr)) {\n\t\t\tif (print_fatal_signals)\n\t\t\t\tprint_fatal_signal(info->si_signo);\n\t\t\t/*\n\t\t\t * If it was able to dump core, this kills all\n\t\t\t * other threads in the group and synchronizes with\n\t\t\t * their demise.  If we lost the race with another\n\t\t\t * thread getting here, it set group_exit_code\n\t\t\t * first and our do_group_exit call below will use\n\t\t\t * that value and ignore the one we pass it.\n\t\t\t */\n\t\t\tdo_coredump(info);\n\t\t}\n\n\t\t/*\n\t\t * Death signals, no core dump.\n\t\t */\n\t\tdo_group_exit(info->si_signo);\n\t\t/* NOTREACHED */\n\t}\n\tspin_unlock_irq(&sighand->siglock);\n\treturn signr;\n}\n\n/**\n * signal_delivered - \n * @sig:\t\tnumber of signal being delivered\n * @info:\t\tsiginfo_t of signal being delivered\n * @ka:\t\t\tsigaction setting that chose the handler\n * @regs:\t\tuser register state\n * @stepping:\t\tnonzero if debugger single-step or block-step in use\n *\n * This function should be called when a signal has succesfully been\n * delivered. It updates the blocked signals accordingly (@ka->sa.sa_mask\n * is always blocked, and the signal itself is blocked unless %SA_NODEFER\n * is set in @ka->sa.sa_flags.  Tracing is notified.\n */\nvoid signal_delivered(int sig, siginfo_t *info, struct k_sigaction *ka,\n\t\t\tstruct pt_regs *regs, int stepping)\n{\n\tsigset_t blocked;\n\n\t/* A signal was successfully delivered, and the\n\t   saved sigmask was stored on the signal frame,\n\t   and will be restored by sigreturn.  So we can\n\t   simply clear the restore sigmask flag.  */\n\tclear_restore_sigmask();\n\n\tsigorsets(&blocked, &current->blocked, &ka->sa.sa_mask);\n\tif (!(ka->sa.sa_flags & SA_NODEFER))\n\t\tsigaddset(&blocked, sig);\n\tset_current_blocked(&blocked);\n\ttracehook_signal_handler(sig, info, ka, regs, stepping);\n}\n\n/*\n * It could be that complete_signal() picked us to notify about the\n * group-wide signal. Other threads should be notified now to take\n * the shared signals in @which since we will not.\n */\nstatic void retarget_shared_pending(struct task_struct *tsk, sigset_t *which)\n{\n\tsigset_t retarget;\n\tstruct task_struct *t;\n\n\tsigandsets(&retarget, &tsk->signal->shared_pending.signal, which);\n\tif (sigisemptyset(&retarget))\n\t\treturn;\n\n\tt = tsk;\n\twhile_each_thread(tsk, t) {\n\t\tif (t->flags & PF_EXITING)\n\t\t\tcontinue;\n\n\t\tif (!has_pending_signals(&retarget, &t->blocked))\n\t\t\tcontinue;\n\t\t/* Remove the signals this thread can handle. */\n\t\tsigandsets(&retarget, &retarget, &t->blocked);\n\n\t\tif (!signal_pending(t))\n\t\t\tsignal_wake_up(t, 0);\n\n\t\tif (sigisemptyset(&retarget))\n\t\t\tbreak;\n\t}\n}\n\nvoid exit_signals(struct task_struct *tsk)\n{\n\tint group_stop = 0;\n\tsigset_t unblocked;\n\n\t/*\n\t * @tsk is about to have PF_EXITING set - lock out users which\n\t * expect stable threadgroup.\n\t */\n\tthreadgroup_change_begin(tsk);\n\n\tif (thread_group_empty(tsk) || signal_group_exit(tsk->signal)) {\n\t\ttsk->flags |= PF_EXITING;\n\t\tthreadgroup_change_end(tsk);\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t/*\n\t * From now this task is not visible for group-wide signals,\n\t * see wants_signal(), do_signal_stop().\n\t */\n\ttsk->flags |= PF_EXITING;\n\n\tthreadgroup_change_end(tsk);\n\n\tif (!signal_pending(tsk))\n\t\tgoto out;\n\n\tunblocked = tsk->blocked;\n\tsignotset(&unblocked);\n\tretarget_shared_pending(tsk, &unblocked);\n\n\tif (unlikely(tsk->jobctl & JOBCTL_STOP_PENDING) &&\n\t    task_participate_group_stop(tsk))\n\t\tgroup_stop = CLD_STOPPED;\nout:\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t/*\n\t * If group stop has completed, deliver the notification.  This\n\t * should always go to the real parent of the group leader.\n\t */\n\tif (unlikely(group_stop)) {\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(tsk, false, group_stop);\n\t\tread_unlock(&tasklist_lock);\n\t}\n}\n\nEXPORT_SYMBOL(recalc_sigpending);\nEXPORT_SYMBOL_GPL(dequeue_signal);\nEXPORT_SYMBOL(flush_signals);\nEXPORT_SYMBOL(force_sig);\nEXPORT_SYMBOL(send_sig);\nEXPORT_SYMBOL(send_sig_info);\nEXPORT_SYMBOL(sigprocmask);\nEXPORT_SYMBOL(block_all_signals);\nEXPORT_SYMBOL(unblock_all_signals);\n\n\n/*\n * System call entry points.\n */\n\n/**\n *  sys_restart_syscall - restart a system call\n */\nSYSCALL_DEFINE0(restart_syscall)\n{\n\tstruct restart_block *restart = &current_thread_info()->restart_block;\n\treturn restart->fn(restart);\n}\n\nlong do_no_restart_syscall(struct restart_block *param)\n{\n\treturn -EINTR;\n}\n\nstatic void __set_task_blocked(struct task_struct *tsk, const sigset_t *newset)\n{\n\tif (signal_pending(tsk) && !thread_group_empty(tsk)) {\n\t\tsigset_t newblocked;\n\t\t/* A set of now blocked but previously unblocked signals. */\n\t\tsigandnsets(&newblocked, newset, &current->blocked);\n\t\tretarget_shared_pending(tsk, &newblocked);\n\t}\n\ttsk->blocked = *newset;\n\trecalc_sigpending();\n}\n\n/**\n * set_current_blocked - change current->blocked mask\n * @newset: new mask\n *\n * It is wrong to change ->blocked directly, this helper should be used\n * to ensure the process can't miss a shared signal we are going to block.\n */\nvoid set_current_blocked(sigset_t *newset)\n{\n\tsigdelsetmask(newset, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t__set_current_blocked(newset);\n}\n\nvoid __set_current_blocked(const sigset_t *newset)\n{\n\tstruct task_struct *tsk = current;\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t__set_task_blocked(tsk, newset);\n\tspin_unlock_irq(&tsk->sighand->siglock);\n}\n\n/*\n * This is also useful for kernel threads that want to temporarily\n * (or permanently) block certain signals.\n *\n * NOTE! Unlike the user-mode sys_sigprocmask(), the kernel\n * interface happily blocks \"unblockable\" signals like SIGKILL\n * and friends.\n */\nint sigprocmask(int how, sigset_t *set, sigset_t *oldset)\n{\n\tstruct task_struct *tsk = current;\n\tsigset_t newset;\n\n\t/* Lockless, only current can change ->blocked, never from irq */\n\tif (oldset)\n\t\t*oldset = tsk->blocked;\n\n\tswitch (how) {\n\tcase SIG_BLOCK:\n\t\tsigorsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_UNBLOCK:\n\t\tsigandnsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_SETMASK:\n\t\tnewset = *set;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t__set_current_blocked(&newset);\n\treturn 0;\n}\n\n/**\n *  sys_rt_sigprocmask - change the list of currently blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: stores pending signals\n *  @oset: previous value of signal mask if non-null\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigprocmask, int, how, sigset_t __user *, nset,\n\t\tsigset_t __user *, oset, size_t, sigsetsize)\n{\n\tsigset_t old_set, new_set;\n\tint error;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\told_set = current->blocked;\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nlong do_sigpending(void __user *set, unsigned long sigsetsize)\n{\n\tlong error = -EINVAL;\n\tsigset_t pending;\n\n\tif (sigsetsize > sizeof(sigset_t))\n\t\tgoto out;\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tsigorsets(&pending, &current->pending.signal,\n\t\t  &current->signal->shared_pending.signal);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\t/* Outside the lock because only this thread touches it.  */\n\tsigandsets(&pending, &current->blocked, &pending);\n\n\terror = -EFAULT;\n\tif (!copy_to_user(set, &pending, sigsetsize))\n\t\terror = 0;\n\nout:\n\treturn error;\n}\n\n/**\n *  sys_rt_sigpending - examine a pending signal that has been raised\n *\t\t\twhile blocked\n *  @set: stores pending signals\n *  @sigsetsize: size of sigset_t type or larger\n */\nSYSCALL_DEFINE2(rt_sigpending, sigset_t __user *, set, size_t, sigsetsize)\n{\n\treturn do_sigpending(set, sigsetsize);\n}\n\n#ifndef HAVE_ARCH_COPY_SIGINFO_TO_USER\n\nint copy_siginfo_to_user(siginfo_t __user *to, siginfo_t *from)\n{\n\tint err;\n\n\tif (!access_ok (VERIFY_WRITE, to, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\tif (from->si_code < 0)\n\t\treturn __copy_to_user(to, from, sizeof(siginfo_t))\n\t\t\t? -EFAULT : 0;\n\t/*\n\t * If you change siginfo_t structure, please be sure\n\t * this code is fixed accordingly.\n\t * Please remember to update the signalfd_copyinfo() function\n\t * inside fs/signalfd.c too, in case siginfo_t changes.\n\t * It should never copy any pad contained in the structure\n\t * to avoid security leaks, but must copy the generic\n\t * 3 ints plus the relevant union member.\n\t */\n\terr = __put_user(from->si_signo, &to->si_signo);\n\terr |= __put_user(from->si_errno, &to->si_errno);\n\terr |= __put_user((short)from->si_code, &to->si_code);\n\tswitch (from->si_code & __SI_MASK) {\n\tcase __SI_KILL:\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\tbreak;\n\tcase __SI_TIMER:\n\t\t err |= __put_user(from->si_tid, &to->si_tid);\n\t\t err |= __put_user(from->si_overrun, &to->si_overrun);\n\t\t err |= __put_user(from->si_ptr, &to->si_ptr);\n\t\tbreak;\n\tcase __SI_POLL:\n\t\terr |= __put_user(from->si_band, &to->si_band);\n\t\terr |= __put_user(from->si_fd, &to->si_fd);\n\t\tbreak;\n\tcase __SI_FAULT:\n\t\terr |= __put_user(from->si_addr, &to->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\terr |= __put_user(from->si_trapno, &to->si_trapno);\n#endif\n#ifdef BUS_MCEERR_AO\n\t\t/*\n\t\t * Other callers might not initialize the si_lsb field,\n\t\t * so check explicitly for the right codes here.\n\t\t */\n\t\tif (from->si_code == BUS_MCEERR_AR || from->si_code == BUS_MCEERR_AO)\n\t\t\terr |= __put_user(from->si_addr_lsb, &to->si_addr_lsb);\n#endif\n\t\tbreak;\n\tcase __SI_CHLD:\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\terr |= __put_user(from->si_status, &to->si_status);\n\t\terr |= __put_user(from->si_utime, &to->si_utime);\n\t\terr |= __put_user(from->si_stime, &to->si_stime);\n\t\tbreak;\n\tcase __SI_RT: /* This is not generated by the kernel as of now. */\n\tcase __SI_MESGQ: /* But this is */\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\terr |= __put_user(from->si_ptr, &to->si_ptr);\n\t\tbreak;\n#ifdef __ARCH_SIGSYS\n\tcase __SI_SYS:\n\t\terr |= __put_user(from->si_call_addr, &to->si_call_addr);\n\t\terr |= __put_user(from->si_syscall, &to->si_syscall);\n\t\terr |= __put_user(from->si_arch, &to->si_arch);\n\t\tbreak;\n#endif\n\tdefault: /* this is just in case for now ... */\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\tbreak;\n\t}\n\treturn err;\n}\n\n#endif\n\n/**\n *  do_sigtimedwait - wait for queued signals specified in @which\n *  @which: queued signals to wait for\n *  @info: if non-null, the signal's siginfo is returned here\n *  @ts: upper bound on process time suspension\n */\nint do_sigtimedwait(const sigset_t *which, siginfo_t *info,\n\t\t\tconst struct timespec *ts)\n{\n\tstruct task_struct *tsk = current;\n\tlong timeout = MAX_SCHEDULE_TIMEOUT;\n\tsigset_t mask = *which;\n\tint sig;\n\n\tif (ts) {\n\t\tif (!timespec_valid(ts))\n\t\t\treturn -EINVAL;\n\t\ttimeout = timespec_to_jiffies(ts);\n\t\t/*\n\t\t * We can be close to the next tick, add another one\n\t\t * to ensure we will wait at least the time asked for.\n\t\t */\n\t\tif (ts->tv_sec || ts->tv_nsec)\n\t\t\ttimeout++;\n\t}\n\n\t/*\n\t * Invert the set of allowed signals to get those we want to block.\n\t */\n\tsigdelsetmask(&mask, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\tsignotset(&mask);\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\tsig = dequeue_signal(tsk, &mask, info);\n\tif (!sig && timeout) {\n\t\t/*\n\t\t * None ready, temporarily unblock those we're interested\n\t\t * while we are sleeping in so that we'll be awakened when\n\t\t * they arrive. Unblocking is always fine, we can avoid\n\t\t * set_current_blocked().\n\t\t */\n\t\ttsk->real_blocked = tsk->blocked;\n\t\tsigandsets(&tsk->blocked, &tsk->blocked, &mask);\n\t\trecalc_sigpending();\n\t\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t\ttimeout = schedule_timeout_interruptible(timeout);\n\n\t\tspin_lock_irq(&tsk->sighand->siglock);\n\t\t__set_task_blocked(tsk, &tsk->real_blocked);\n\t\tsiginitset(&tsk->real_blocked, 0);\n\t\tsig = dequeue_signal(tsk, &mask, info);\n\t}\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\tif (sig)\n\t\treturn sig;\n\treturn timeout ? -EINTR : -EAGAIN;\n}\n\n/**\n *  sys_rt_sigtimedwait - synchronously wait for queued signals specified\n *\t\t\tin @uthese\n *  @uthese: queued signals to wait for\n *  @uinfo: if non-null, the signal's siginfo is returned here\n *  @uts: upper bound on process time suspension\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigtimedwait, const sigset_t __user *, uthese,\n\t\tsiginfo_t __user *, uinfo, const struct timespec __user *, uts,\n\t\tsize_t, sigsetsize)\n{\n\tsigset_t these;\n\tstruct timespec ts;\n\tsiginfo_t info;\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&these, uthese, sizeof(these)))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (copy_from_user(&ts, uts, sizeof(ts)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n\n/**\n *  sys_kill - send a signal to a process\n *  @pid: the PID of the process\n *  @sig: signal to be sent\n */\nSYSCALL_DEFINE2(kill, pid_t, pid, int, sig)\n{\n\tstruct siginfo info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_USER;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn kill_something_info(sig, &info, pid);\n}\n\nstatic int\ndo_send_specific(pid_t tgid, pid_t pid, int sig, struct siginfo *info)\n{\n\tstruct task_struct *p;\n\tint error = -ESRCH;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p && (tgid <= 0 || task_tgid_vnr(p) == tgid)) {\n\t\terror = check_kill_permission(sig, info, p);\n\t\t/*\n\t\t * The null signal is a permissions and process existence\n\t\t * probe.  No signal is actually delivered.\n\t\t */\n\t\tif (!error && sig) {\n\t\t\terror = do_send_sig_info(sig, info, p, false);\n\t\t\t/*\n\t\t\t * If lock_task_sighand() failed we pretend the task\n\t\t\t * dies after receiving the signal. The window is tiny,\n\t\t\t * and the signal is private anyway.\n\t\t\t */\n\t\t\tif (unlikely(error == -ESRCH))\n\t\t\t\terror = 0;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nstatic int do_tkill(pid_t tgid, pid_t pid, int sig)\n{\n\tstruct siginfo info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_TKILL;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn do_send_specific(tgid, pid, sig, &info);\n}\n\n/**\n *  sys_tgkill - send signal to one specific thread\n *  @tgid: the thread group ID of the thread\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *\n *  This syscall also checks the @tgid and returns -ESRCH even if the PID\n *  exists but it's not belonging to the target process anymore. This\n *  method solves the problem of threads exiting and PIDs getting reused.\n */\nSYSCALL_DEFINE3(tgkill, pid_t, tgid, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(tgid, pid, sig);\n}\n\n/**\n *  sys_tkill - send signal to one specific task\n *  @pid: the PID of the task\n *  @sig: signal to be sent\n *\n *  Send a signal to only one task, even if it's a CLONE_THREAD task.\n */\nSYSCALL_DEFINE2(tkill, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(0, pid, sig);\n}\n\n/**\n *  sys_rt_sigqueueinfo - send signal information to a signal\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *  @uinfo: signal info to be sent\n */\nSYSCALL_DEFINE3(rt_sigqueueinfo, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tsiginfo_t info;\n\n\tif (copy_from_user(&info, uinfo, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif (info.si_code >= 0 || info.si_code == SI_TKILL) {\n\t\t/* We used to allow any < 0 si_code */\n\t\tWARN_ON_ONCE(info.si_code < 0);\n\t\treturn -EPERM;\n\t}\n\tinfo.si_signo = sig;\n\n\t/* POSIX.1b doesn't mention process groups.  */\n\treturn kill_proc_info(sig, &info, pid);\n}\n\nlong do_rt_tgsigqueueinfo(pid_t tgid, pid_t pid, int sig, siginfo_t *info)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif (info->si_code >= 0 || info->si_code == SI_TKILL) {\n\t\t/* We used to allow any < 0 si_code */\n\t\tWARN_ON_ONCE(info->si_code < 0);\n\t\treturn -EPERM;\n\t}\n\tinfo->si_signo = sig;\n\n\treturn do_send_specific(tgid, pid, sig, info);\n}\n\nSYSCALL_DEFINE4(rt_tgsigqueueinfo, pid_t, tgid, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tsiginfo_t info;\n\n\tif (copy_from_user(&info, uinfo, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n\nint do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact)\n{\n\tstruct task_struct *t = current;\n\tstruct k_sigaction *k;\n\tsigset_t mask;\n\n\tif (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))\n\t\treturn -EINVAL;\n\n\tk = &t->sighand->action[sig-1];\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (oact)\n\t\t*oact = *k;\n\n\tif (act) {\n\t\tsigdelsetmask(&act->sa.sa_mask,\n\t\t\t      sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t\t*k = *act;\n\t\t/*\n\t\t * POSIX 3.3.1.3:\n\t\t *  \"Setting a signal action to SIG_IGN for a signal that is\n\t\t *   pending shall cause the pending signal to be discarded,\n\t\t *   whether or not it is blocked.\"\n\t\t *\n\t\t *  \"Setting a signal action to SIG_DFL for a signal that is\n\t\t *   pending and whose default action is to ignore the signal\n\t\t *   (for example, SIGCHLD), shall cause the pending signal to\n\t\t *   be discarded, whether or not it is blocked\"\n\t\t */\n\t\tif (sig_handler_ignored(sig_handler(t, sig), sig)) {\n\t\t\tsigemptyset(&mask);\n\t\t\tsigaddset(&mask, sig);\n\t\t\trm_from_queue_full(&mask, &t->signal->shared_pending);\n\t\t\tdo {\n\t\t\t\trm_from_queue_full(&mask, &t->pending);\n\t\t\t\tt = next_thread(t);\n\t\t\t} while (t != current);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn 0;\n}\n\nint \ndo_sigaltstack (const stack_t __user *uss, stack_t __user *uoss, unsigned long sp)\n{\n\tstack_t oss;\n\tint error;\n\n\toss.ss_sp = (void __user *) current->sas_ss_sp;\n\toss.ss_size = current->sas_ss_size;\n\toss.ss_flags = sas_ss_flags(sp);\n\n\tif (uss) {\n\t\tvoid __user *ss_sp;\n\t\tsize_t ss_size;\n\t\tint ss_flags;\n\n\t\terror = -EFAULT;\n\t\tif (!access_ok(VERIFY_READ, uss, sizeof(*uss)))\n\t\t\tgoto out;\n\t\terror = __get_user(ss_sp, &uss->ss_sp) |\n\t\t\t__get_user(ss_flags, &uss->ss_flags) |\n\t\t\t__get_user(ss_size, &uss->ss_size);\n\t\tif (error)\n\t\t\tgoto out;\n\n\t\terror = -EPERM;\n\t\tif (on_sig_stack(sp))\n\t\t\tgoto out;\n\n\t\terror = -EINVAL;\n\t\t/*\n\t\t * Note - this code used to test ss_flags incorrectly:\n\t\t *  \t  old code may have been written using ss_flags==0\n\t\t *\t  to mean ss_flags==SS_ONSTACK (as this was the only\n\t\t *\t  way that worked) - this fix preserves that older\n\t\t *\t  mechanism.\n\t\t */\n\t\tif (ss_flags != SS_DISABLE && ss_flags != SS_ONSTACK && ss_flags != 0)\n\t\t\tgoto out;\n\n\t\tif (ss_flags == SS_DISABLE) {\n\t\t\tss_size = 0;\n\t\t\tss_sp = NULL;\n\t\t} else {\n\t\t\terror = -ENOMEM;\n\t\t\tif (ss_size < MINSIGSTKSZ)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tcurrent->sas_ss_sp = (unsigned long) ss_sp;\n\t\tcurrent->sas_ss_size = ss_size;\n\t}\n\n\terror = 0;\n\tif (uoss) {\n\t\terror = -EFAULT;\n\t\tif (!access_ok(VERIFY_WRITE, uoss, sizeof(*uoss)))\n\t\t\tgoto out;\n\t\terror = __put_user(oss.ss_sp, &uoss->ss_sp) |\n\t\t\t__put_user(oss.ss_size, &uoss->ss_size) |\n\t\t\t__put_user(oss.ss_flags, &uoss->ss_flags);\n\t}\n\nout:\n\treturn error;\n}\n#ifdef CONFIG_GENERIC_SIGALTSTACK\nSYSCALL_DEFINE2(sigaltstack,const stack_t __user *,uss, stack_t __user *,uoss)\n{\n\treturn do_sigaltstack(uss, uoss, current_user_stack_pointer());\n}\n#endif\n\nint restore_altstack(const stack_t __user *uss)\n{\n\tint err = do_sigaltstack(uss, NULL, current_user_stack_pointer());\n\t/* squash all but EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __save_altstack(stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\treturn  __put_user((void __user *)t->sas_ss_sp, &uss->ss_sp) |\n\t\t__put_user(sas_ss_flags(sp), &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n}\n\n#ifdef CONFIG_COMPAT\n#ifdef CONFIG_GENERIC_SIGALTSTACK\nCOMPAT_SYSCALL_DEFINE2(sigaltstack,\n\t\t\tconst compat_stack_t __user *, uss_ptr,\n\t\t\tcompat_stack_t __user *, uoss_ptr)\n{\n\tstack_t uss, uoss;\n\tint ret;\n\tmm_segment_t seg;\n\n\tif (uss_ptr) {\n\t\tcompat_stack_t uss32;\n\n\t\tmemset(&uss, 0, sizeof(stack_t));\n\t\tif (copy_from_user(&uss32, uss_ptr, sizeof(compat_stack_t)))\n\t\t\treturn -EFAULT;\n\t\tuss.ss_sp = compat_ptr(uss32.ss_sp);\n\t\tuss.ss_flags = uss32.ss_flags;\n\t\tuss.ss_size = uss32.ss_size;\n\t}\n\tseg = get_fs();\n\tset_fs(KERNEL_DS);\n\tret = do_sigaltstack((stack_t __force __user *) (uss_ptr ? &uss : NULL),\n\t\t\t     (stack_t __force __user *) &uoss,\n\t\t\t     compat_user_stack_pointer());\n\tset_fs(seg);\n\tif (ret >= 0 && uoss_ptr)  {\n\t\tif (!access_ok(VERIFY_WRITE, uoss_ptr, sizeof(compat_stack_t)) ||\n\t\t    __put_user(ptr_to_compat(uoss.ss_sp), &uoss_ptr->ss_sp) ||\n\t\t    __put_user(uoss.ss_flags, &uoss_ptr->ss_flags) ||\n\t\t    __put_user(uoss.ss_size, &uoss_ptr->ss_size))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\nint compat_restore_altstack(const compat_stack_t __user *uss)\n{\n\tint err = compat_sys_sigaltstack(uss, NULL);\n\t/* squash all but -EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __compat_save_altstack(compat_stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\treturn  __put_user(ptr_to_compat((void __user *)t->sas_ss_sp), &uss->ss_sp) |\n\t\t__put_user(sas_ss_flags(sp), &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n}\n#endif\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPENDING\n\n/**\n *  sys_sigpending - examine pending signals\n *  @set: where mask of pending signal is returned\n */\nSYSCALL_DEFINE1(sigpending, old_sigset_t __user *, set)\n{\n\treturn do_sigpending(set, sizeof(*set));\n}\n\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\n/**\n *  sys_sigprocmask - examine and change blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: signals to add or remove (if non-null)\n *  @oset: previous value of signal mask if non-null\n *\n * Some platforms have their own version with special arguments;\n * others support only sys_rt_sigprocmask.\n */\n\nSYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, nset,\n\t\told_sigset_t __user *, oset)\n{\n\told_sigset_t old_set, new_set;\n\tsigset_t new_blocked;\n\n\told_set = current->blocked.sig[0];\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(*nset)))\n\t\t\treturn -EFAULT;\n\n\t\tnew_blocked = current->blocked;\n\n\t\tswitch (how) {\n\t\tcase SIG_BLOCK:\n\t\t\tsigaddsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_UNBLOCK:\n\t\t\tsigdelsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_SETMASK:\n\t\t\tnew_blocked.sig[0] = new_set;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tset_current_blocked(&new_blocked);\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(*oset)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n#endif /* __ARCH_WANT_SYS_SIGPROCMASK */\n\n#ifdef __ARCH_WANT_SYS_RT_SIGACTION\n/**\n *  sys_rt_sigaction - alter an action taken by a process\n *  @sig: signal to be sent\n *  @act: new sigaction\n *  @oact: used to save the previous sigaction\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct sigaction __user *, act,\n\t\tstruct sigaction __user *, oact,\n\t\tsize_t, sigsetsize)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret = -EINVAL;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\tgoto out;\n\n\tif (act) {\n\t\tif (copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigaction(sig, act ? &new_sa : NULL, oact ? &old_sa : NULL);\n\n\tif (!ret && oact) {\n\t\tif (copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa)))\n\t\t\treturn -EFAULT;\n\t}\nout:\n\treturn ret;\n}\n#endif /* __ARCH_WANT_SYS_RT_SIGACTION */\n\n#ifdef __ARCH_WANT_SYS_SGETMASK\n\n/*\n * For backwards compatibility.  Functionality superseded by sigprocmask.\n */\nSYSCALL_DEFINE0(sgetmask)\n{\n\t/* SMP safe */\n\treturn current->blocked.sig[0];\n}\n\nSYSCALL_DEFINE1(ssetmask, int, newmask)\n{\n\tint old = current->blocked.sig[0];\n\tsigset_t newset;\n\n\tsiginitset(&newset, newmask);\n\tset_current_blocked(&newset);\n\n\treturn old;\n}\n#endif /* __ARCH_WANT_SGETMASK */\n\n#ifdef __ARCH_WANT_SYS_SIGNAL\n/*\n * For backwards compatibility.  Functionality superseded by sigaction.\n */\nSYSCALL_DEFINE2(signal, int, sig, __sighandler_t, handler)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret;\n\n\tnew_sa.sa.sa_handler = handler;\n\tnew_sa.sa.sa_flags = SA_ONESHOT | SA_NOMASK;\n\tsigemptyset(&new_sa.sa.sa_mask);\n\n\tret = do_sigaction(sig, &new_sa, &old_sa);\n\n\treturn ret ? ret : (unsigned long)old_sa.sa.sa_handler;\n}\n#endif /* __ARCH_WANT_SYS_SIGNAL */\n\n#ifdef __ARCH_WANT_SYS_PAUSE\n\nSYSCALL_DEFINE0(pause)\n{\n\twhile (!signal_pending(current)) {\n\t\tcurrent->state = TASK_INTERRUPTIBLE;\n\t\tschedule();\n\t}\n\treturn -ERESTARTNOHAND;\n}\n\n#endif\n\nint sigsuspend(sigset_t *set)\n{\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(set);\n\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tschedule();\n\tset_restore_sigmask();\n\treturn -ERESTARTNOHAND;\n}\n\n#ifdef __ARCH_WANT_SYS_RT_SIGSUSPEND\n/**\n *  sys_rt_sigsuspend - replace the signal mask for a value with the\n *\t@unewset value until a signal is received\n *  @unewset: new signal mask value\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE2(rt_sigsuspend, sigset_t __user *, unewset, size_t, sigsetsize)\n{\n\tsigset_t newset;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&newset, unewset, sizeof(newset)))\n\t\treturn -EFAULT;\n\treturn sigsuspend(&newset);\n}\n#endif /* __ARCH_WANT_SYS_RT_SIGSUSPEND */\n\n__attribute__((weak)) const char *arch_vma_name(struct vm_area_struct *vma)\n{\n\treturn NULL;\n}\n\nvoid __init signals_init(void)\n{\n\tsigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC);\n}\n\n#ifdef CONFIG_KGDB_KDB\n#include <linux/kdb.h>\n/*\n * kdb_send_sig_info - Allows kdb to send signals without exposing\n * signal internals.  This function checks if the required locks are\n * available before calling the main signal code, to avoid kdb\n * deadlocks.\n */\nvoid\nkdb_send_sig_info(struct task_struct *t, struct siginfo *info)\n{\n\tstatic struct task_struct *kdb_prev_t;\n\tint sig, new_t;\n\tif (!spin_trylock(&t->sighand->siglock)) {\n\t\tkdb_printf(\"Can't do kill command now.\\n\"\n\t\t\t   \"The sigmask lock is held somewhere else in \"\n\t\t\t   \"kernel, try again later\\n\");\n\t\treturn;\n\t}\n\tspin_unlock(&t->sighand->siglock);\n\tnew_t = kdb_prev_t != t;\n\tkdb_prev_t = t;\n\tif (t->state != TASK_RUNNING && new_t) {\n\t\tkdb_printf(\"Process is not RUNNING, sending a signal from \"\n\t\t\t   \"kdb risks deadlock\\n\"\n\t\t\t   \"on the run queue locks. \"\n\t\t\t   \"The signal has _not_ been sent.\\n\"\n\t\t\t   \"Reissue the kill command if you want to risk \"\n\t\t\t   \"the deadlock.\\n\");\n\t\treturn;\n\t}\n\tsig = info->si_signo;\n\tif (send_sig_info(sig, info, t))\n\t\tkdb_printf(\"Fail to deliver Signal %d to process %d.\\n\",\n\t\t\t   sig, t->pid);\n\telse\n\t\tkdb_printf(\"Signal %d is sent to process %d.\\n\", sig, t->pid);\n}\n#endif\t/* CONFIG_KGDB_KDB */\n"], "fixing_code": ["/*\n * x86 single-step support code, common to 32-bit and 64-bit.\n */\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/ptrace.h>\n#include <asm/desc.h>\n\nunsigned long convert_ip_to_linear(struct task_struct *child, struct pt_regs *regs)\n{\n\tunsigned long addr, seg;\n\n\taddr = regs->ip;\n\tseg = regs->cs & 0xffff;\n\tif (v8086_mode(regs)) {\n\t\taddr = (addr & 0xffff) + (seg << 4);\n\t\treturn addr;\n\t}\n\n\t/*\n\t * We'll assume that the code segments in the GDT\n\t * are all zero-based. That is largely true: the\n\t * TLS segments are used for data, and the PNPBIOS\n\t * and APM bios ones we just ignore here.\n\t */\n\tif ((seg & SEGMENT_TI_MASK) == SEGMENT_LDT) {\n\t\tstruct desc_struct *desc;\n\t\tunsigned long base;\n\n\t\tseg &= ~7UL;\n\n\t\tmutex_lock(&child->mm->context.lock);\n\t\tif (unlikely((seg >> 3) >= child->mm->context.size))\n\t\t\taddr = -1L; /* bogus selector, access would fault */\n\t\telse {\n\t\t\tdesc = child->mm->context.ldt + seg;\n\t\t\tbase = get_desc_base(desc);\n\n\t\t\t/* 16-bit code segment? */\n\t\t\tif (!desc->d)\n\t\t\t\taddr &= 0xffff;\n\t\t\taddr += base;\n\t\t}\n\t\tmutex_unlock(&child->mm->context.lock);\n\t}\n\n\treturn addr;\n}\n\nstatic int is_setting_trap_flag(struct task_struct *child, struct pt_regs *regs)\n{\n\tint i, copied;\n\tunsigned char opcode[15];\n\tunsigned long addr = convert_ip_to_linear(child, regs);\n\n\tcopied = access_process_vm(child, addr, opcode, sizeof(opcode), 0);\n\tfor (i = 0; i < copied; i++) {\n\t\tswitch (opcode[i]) {\n\t\t/* popf and iret */\n\t\tcase 0x9d: case 0xcf:\n\t\t\treturn 1;\n\n\t\t\t/* CHECKME: 64 65 */\n\n\t\t/* opcode and address size prefixes */\n\t\tcase 0x66: case 0x67:\n\t\t\tcontinue;\n\t\t/* irrelevant prefixes (segment overrides and repeats) */\n\t\tcase 0x26: case 0x2e:\n\t\tcase 0x36: case 0x3e:\n\t\tcase 0x64: case 0x65:\n\t\tcase 0xf0: case 0xf2: case 0xf3:\n\t\t\tcontinue;\n\n#ifdef CONFIG_X86_64\n\t\tcase 0x40 ... 0x4f:\n\t\t\tif (!user_64bit_mode(regs))\n\t\t\t\t/* 32-bit mode: register increment */\n\t\t\t\treturn 0;\n\t\t\t/* 64-bit mode: REX prefix */\n\t\t\tcontinue;\n#endif\n\n\t\t\t/* CHECKME: f2, f3 */\n\n\t\t/*\n\t\t * pushf: NOTE! We should probably not let\n\t\t * the user see the TF bit being set. But\n\t\t * it's more pain than it's worth to avoid\n\t\t * it, and a debugger could emulate this\n\t\t * all in user space if it _really_ cares.\n\t\t */\n\t\tcase 0x9c:\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * Enable single-stepping.  Return nonzero if user mode is not using TF itself.\n */\nstatic int enable_single_step(struct task_struct *child)\n{\n\tstruct pt_regs *regs = task_pt_regs(child);\n\tunsigned long oflags;\n\n\t/*\n\t * If we stepped into a sysenter/syscall insn, it trapped in\n\t * kernel mode; do_debug() cleared TF and set TIF_SINGLESTEP.\n\t * If user-mode had set TF itself, then it's still clear from\n\t * do_debug() and we need to set it again to restore the user\n\t * state so we don't wrongly set TIF_FORCED_TF below.\n\t * If enable_single_step() was used last and that is what\n\t * set TIF_SINGLESTEP, then both TF and TIF_FORCED_TF are\n\t * already set and our bookkeeping is fine.\n\t */\n\tif (unlikely(test_tsk_thread_flag(child, TIF_SINGLESTEP)))\n\t\tregs->flags |= X86_EFLAGS_TF;\n\n\t/*\n\t * Always set TIF_SINGLESTEP - this guarantees that\n\t * we single-step system calls etc..  This will also\n\t * cause us to set TF when returning to user mode.\n\t */\n\tset_tsk_thread_flag(child, TIF_SINGLESTEP);\n\n\toflags = regs->flags;\n\n\t/* Set TF on the kernel stack.. */\n\tregs->flags |= X86_EFLAGS_TF;\n\n\t/*\n\t * ..but if TF is changed by the instruction we will trace,\n\t * don't mark it as being \"us\" that set it, so that we\n\t * won't clear it by hand later.\n\t *\n\t * Note that if we don't actually execute the popf because\n\t * of a signal arriving right now or suchlike, we will lose\n\t * track of the fact that it really was \"us\" that set it.\n\t */\n\tif (is_setting_trap_flag(child, regs)) {\n\t\tclear_tsk_thread_flag(child, TIF_FORCED_TF);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If TF was already set, check whether it was us who set it.\n\t * If not, we should never attempt a block step.\n\t */\n\tif (oflags & X86_EFLAGS_TF)\n\t\treturn test_tsk_thread_flag(child, TIF_FORCED_TF);\n\n\tset_tsk_thread_flag(child, TIF_FORCED_TF);\n\n\treturn 1;\n}\n\nvoid set_task_blockstep(struct task_struct *task, bool on)\n{\n\tunsigned long debugctl;\n\n\t/*\n\t * Ensure irq/preemption can't change debugctl in between.\n\t * Note also that both TIF_BLOCKSTEP and debugctl should\n\t * be changed atomically wrt preemption.\n\t *\n\t * NOTE: this means that set/clear TIF_BLOCKSTEP is only safe if\n\t * task is current or it can't be running, otherwise we can race\n\t * with __switch_to_xtra(). We rely on ptrace_freeze_traced() but\n\t * PTRACE_KILL is not safe.\n\t */\n\tlocal_irq_disable();\n\tdebugctl = get_debugctlmsr();\n\tif (on) {\n\t\tdebugctl |= DEBUGCTLMSR_BTF;\n\t\tset_tsk_thread_flag(task, TIF_BLOCKSTEP);\n\t} else {\n\t\tdebugctl &= ~DEBUGCTLMSR_BTF;\n\t\tclear_tsk_thread_flag(task, TIF_BLOCKSTEP);\n\t}\n\tif (task == current)\n\t\tupdate_debugctlmsr(debugctl);\n\tlocal_irq_enable();\n}\n\n/*\n * Enable single or block step.\n */\nstatic void enable_step(struct task_struct *child, bool block)\n{\n\t/*\n\t * Make sure block stepping (BTF) is not enabled unless it should be.\n\t * Note that we don't try to worry about any is_setting_trap_flag()\n\t * instructions after the first when using block stepping.\n\t * So no one should try to use debugger block stepping in a program\n\t * that uses user-mode single stepping itself.\n\t */\n\tif (enable_single_step(child) && block)\n\t\tset_task_blockstep(child, true);\n\telse if (test_tsk_thread_flag(child, TIF_BLOCKSTEP))\n\t\tset_task_blockstep(child, false);\n}\n\nvoid user_enable_single_step(struct task_struct *child)\n{\n\tenable_step(child, 0);\n}\n\nvoid user_enable_block_step(struct task_struct *child)\n{\n\tenable_step(child, 1);\n}\n\nvoid user_disable_single_step(struct task_struct *child)\n{\n\t/*\n\t * Make sure block stepping (BTF) is disabled.\n\t */\n\tif (test_tsk_thread_flag(child, TIF_BLOCKSTEP))\n\t\tset_task_blockstep(child, false);\n\n\t/* Always clear TIF_SINGLESTEP... */\n\tclear_tsk_thread_flag(child, TIF_SINGLESTEP);\n\n\t/* But touch TF only if it was set by us.. */\n\tif (test_and_clear_tsk_thread_flag(child, TIF_FORCED_TF))\n\t\ttask_pt_regs(child)->flags &= ~X86_EFLAGS_TF;\n}\n", "/*\n * linux/kernel/ptrace.c\n *\n * (C) Copyright 1999 Linus Torvalds\n *\n * Common interfaces for \"ptrace()\" which we do not want\n * to continually duplicate across every architecture.\n */\n\n#include <linux/capability.h>\n#include <linux/export.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/ptrace.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/audit.h>\n#include <linux/pid_namespace.h>\n#include <linux/syscalls.h>\n#include <linux/uaccess.h>\n#include <linux/regset.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/cn_proc.h>\n\n\nstatic int ptrace_trapping_sleep_fn(void *flags)\n{\n\tschedule();\n\treturn 0;\n}\n\n/*\n * ptrace a task: make the debugger its new parent and\n * move it to the ptrace list.\n *\n * Must be called with the tasklist lock write-held.\n */\nvoid __ptrace_link(struct task_struct *child, struct task_struct *new_parent)\n{\n\tBUG_ON(!list_empty(&child->ptrace_entry));\n\tlist_add(&child->ptrace_entry, &new_parent->ptraced);\n\tchild->parent = new_parent;\n}\n\n/**\n * __ptrace_unlink - unlink ptracee and restore its execution state\n * @child: ptracee to be unlinked\n *\n * Remove @child from the ptrace list, move it back to the original parent,\n * and restore the execution state so that it conforms to the group stop\n * state.\n *\n * Unlinking can happen via two paths - explicit PTRACE_DETACH or ptracer\n * exiting.  For PTRACE_DETACH, unless the ptracee has been killed between\n * ptrace_check_attach() and here, it's guaranteed to be in TASK_TRACED.\n * If the ptracer is exiting, the ptracee can be in any state.\n *\n * After detach, the ptracee should be in a state which conforms to the\n * group stop.  If the group is stopped or in the process of stopping, the\n * ptracee should be put into TASK_STOPPED; otherwise, it should be woken\n * up from TASK_TRACED.\n *\n * If the ptracee is in TASK_TRACED and needs to be moved to TASK_STOPPED,\n * it goes through TRACED -> RUNNING -> STOPPED transition which is similar\n * to but in the opposite direction of what happens while attaching to a\n * stopped task.  However, in this direction, the intermediate RUNNING\n * state is not hidden even from the current ptracer and if it immediately\n * re-attaches and performs a WNOHANG wait(2), it may fail.\n *\n * CONTEXT:\n * write_lock_irq(tasklist_lock)\n */\nvoid __ptrace_unlink(struct task_struct *child)\n{\n\tBUG_ON(!child->ptrace);\n\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\n\tspin_lock(&child->sighand->siglock);\n\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tptrace_signal_wake_up(child, true);\n\n\tspin_unlock(&child->sighand->siglock);\n}\n\n/* Ensure that nothing can wake it up, even SIGKILL */\nstatic bool ptrace_freeze_traced(struct task_struct *task)\n{\n\tbool ret = false;\n\n\t/* Lockless, nobody but us can set this flag */\n\tif (task->jobctl & JOBCTL_LISTENING)\n\t\treturn ret;\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (task_is_traced(task) && !__fatal_signal_pending(task)) {\n\t\ttask->state = __TASK_TRACED;\n\t\tret = true;\n\t}\n\tspin_unlock_irq(&task->sighand->siglock);\n\n\treturn ret;\n}\n\nstatic void ptrace_unfreeze_traced(struct task_struct *task)\n{\n\tif (task->state != __TASK_TRACED)\n\t\treturn;\n\n\tWARN_ON(!task->ptrace || task->parent != current);\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (__fatal_signal_pending(task))\n\t\twake_up_state(task, __TASK_TRACED);\n\telse\n\t\ttask->state = TASK_TRACED;\n\tspin_unlock_irq(&task->sighand->siglock);\n}\n\n/**\n * ptrace_check_attach - check whether ptracee is ready for ptrace operation\n * @child: ptracee to check for\n * @ignore_state: don't check whether @child is currently %TASK_TRACED\n *\n * Check whether @child is being ptraced by %current and ready for further\n * ptrace operations.  If @ignore_state is %false, @child also should be in\n * %TASK_TRACED state and on return the child is guaranteed to be traced\n * and not executing.  If @ignore_state is %true, @child can be in any\n * state.\n *\n * CONTEXT:\n * Grabs and releases tasklist_lock and @child->sighand->siglock.\n *\n * RETURNS:\n * 0 on success, -ESRCH if %child is not ready.\n */\nstatic int ptrace_check_attach(struct task_struct *child, bool ignore_state)\n{\n\tint ret = -ESRCH;\n\n\t/*\n\t * We take the read lock around doing both checks to close a\n\t * possible race where someone else was tracing our child and\n\t * detached between these two checks.  After this locked check,\n\t * we are sure that this is our traced child and that can only\n\t * be changed by us so it's not changing right after this.\n\t */\n\tread_lock(&tasklist_lock);\n\tif (child->ptrace && child->parent == current) {\n\t\tWARN_ON(child->state == __TASK_TRACED);\n\t\t/*\n\t\t * child->sighand can't be NULL, release_task()\n\t\t * does ptrace_unlink() before __exit_signal().\n\t\t */\n\t\tif (ignore_state || ptrace_freeze_traced(child))\n\t\t\tret = 0;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tif (!ret && !ignore_state) {\n\t\tif (!wait_task_inactive(child, __TASK_TRACED)) {\n\t\t\t/*\n\t\t\t * This can only happen if may_ptrace_stop() fails and\n\t\t\t * ptrace_stop() changes ->state back to TASK_RUNNING,\n\t\t\t * so we should not worry about leaking __TASK_TRACED.\n\t\t\t */\n\t\t\tWARN_ON(child->state == __TASK_TRACED);\n\t\t\tret = -ESRCH;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int ptrace_has_cap(struct user_namespace *ns, unsigned int mode)\n{\n\tif (mode & PTRACE_MODE_NOAUDIT)\n\t\treturn has_ns_capability_noaudit(current, ns, CAP_SYS_PTRACE);\n\telse\n\t\treturn has_ns_capability(current, ns, CAP_SYS_PTRACE);\n}\n\n/* Returns 0 on success, -errno on denial. */\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\tint dumpable = 0;\n\t/* Don't let security modules deny introspection */\n\tif (task == current)\n\t\treturn 0;\n\trcu_read_lock();\n\ttcred = __task_cred(task);\n\tif (uid_eq(cred->uid, tcred->euid) &&\n\t    uid_eq(cred->uid, tcred->suid) &&\n\t    uid_eq(cred->uid, tcred->uid)  &&\n\t    gid_eq(cred->gid, tcred->egid) &&\n\t    gid_eq(cred->gid, tcred->sgid) &&\n\t    gid_eq(cred->gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tsmp_rmb();\n\tif (task->mm)\n\t\tdumpable = get_dumpable(task->mm);\n\trcu_read_lock();\n\tif (!dumpable && !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn security_ptrace_access_check(task, mode);\n}\n\nbool ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tint err;\n\ttask_lock(task);\n\terr = __ptrace_may_access(task, mode);\n\ttask_unlock(task);\n\treturn !err;\n}\n\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT,\n\t\t\t    ptrace_trapping_sleep_fn, TASK_UNINTERRUPTIBLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n\n/**\n * ptrace_traceme  --  helper for PTRACE_TRACEME\n *\n * Performs checks and sets PT_PTRACED.\n * Should be used by all ptrace implementations for PTRACE_TRACEME.\n */\nstatic int ptrace_traceme(void)\n{\n\tint ret = -EPERM;\n\n\twrite_lock_irq(&tasklist_lock);\n\t/* Are we already being traced? */\n\tif (!current->ptrace) {\n\t\tret = security_ptrace_traceme(current->parent);\n\t\t/*\n\t\t * Check PF_EXITING to ensure ->real_parent has not passed\n\t\t * exit_ptrace(). Otherwise we don't report the error but\n\t\t * pretend ->real_parent untraces us right after return.\n\t\t */\n\t\tif (!ret && !(current->real_parent->flags & PF_EXITING)) {\n\t\t\tcurrent->ptrace = PT_PTRACED;\n\t\t\t__ptrace_link(current, current->real_parent);\n\t\t}\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * Called with irqs disabled, returns true if childs should reap themselves.\n */\nstatic int ignoring_children(struct sighand_struct *sigh)\n{\n\tint ret;\n\tspin_lock(&sigh->siglock);\n\tret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||\n\t      (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);\n\tspin_unlock(&sigh->siglock);\n\treturn ret;\n}\n\n/*\n * Called with tasklist_lock held for writing.\n * Unlink a traced task, and clean it up if it was a traced zombie.\n * Return true if it needs to be reaped with release_task().\n * (We can't call release_task() here because we already hold tasklist_lock.)\n *\n * If it's a zombie, our attachedness prevented normal parent notification\n * or self-reaping.  Do notification now if it would have happened earlier.\n * If it should reap itself, return true.\n *\n * If it's our own child, there is no notification to do. But if our normal\n * children self-reap, then this child was prevented by ptrace and we must\n * reap it now, in that case we must also wake up sub-threads sleeping in\n * do_wait().\n */\nstatic bool __ptrace_detach(struct task_struct *tracer, struct task_struct *p)\n{\n\tbool dead;\n\n\t__ptrace_unlink(p);\n\n\tif (p->exit_state != EXIT_ZOMBIE)\n\t\treturn false;\n\n\tdead = !thread_group_leader(p);\n\n\tif (!dead && thread_group_empty(p)) {\n\t\tif (!same_thread_group(p->real_parent, tracer))\n\t\t\tdead = do_notify_parent(p, p->exit_signal);\n\t\telse if (ignoring_children(tracer->sighand)) {\n\t\t\t__wake_up_parent(p, tracer);\n\t\t\tdead = true;\n\t\t}\n\t}\n\t/* Mark it as in the process of being reaped. */\n\tif (dead)\n\t\tp->exit_state = EXIT_DEAD;\n\treturn dead;\n}\n\nstatic int ptrace_detach(struct task_struct *child, unsigned int data)\n{\n\tbool dead = false;\n\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\t/* Architecture-specific hardware disable .. */\n\tptrace_disable(child);\n\tclear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\n\twrite_lock_irq(&tasklist_lock);\n\t/*\n\t * This child can be already killed. Make sure de_thread() or\n\t * our sub-thread doing do_wait() didn't do release_task() yet.\n\t */\n\tif (child->ptrace) {\n\t\tchild->exit_code = data;\n\t\tdead = __ptrace_detach(current, child);\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_ptrace_connector(child, PTRACE_DETACH);\n\tif (unlikely(dead))\n\t\trelease_task(child);\n\n\treturn 0;\n}\n\n/*\n * Detach all tasks we were using ptrace on. Called with tasklist held\n * for writing, and returns with it held too. But note it can release\n * and reacquire the lock.\n */\nvoid exit_ptrace(struct task_struct *tracer)\n\t__releases(&tasklist_lock)\n\t__acquires(&tasklist_lock)\n{\n\tstruct task_struct *p, *n;\n\tLIST_HEAD(ptrace_dead);\n\n\tif (likely(list_empty(&tracer->ptraced)))\n\t\treturn;\n\n\tlist_for_each_entry_safe(p, n, &tracer->ptraced, ptrace_entry) {\n\t\tif (unlikely(p->ptrace & PT_EXITKILL))\n\t\t\tsend_sig_info(SIGKILL, SEND_SIG_FORCED, p);\n\n\t\tif (__ptrace_detach(tracer, p))\n\t\t\tlist_add(&p->ptrace_entry, &ptrace_dead);\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\tBUG_ON(!list_empty(&tracer->ptraced));\n\n\tlist_for_each_entry_safe(p, n, &ptrace_dead, ptrace_entry) {\n\t\tlist_del_init(&p->ptrace_entry);\n\t\trelease_task(p);\n\t}\n\n\twrite_lock_irq(&tasklist_lock);\n}\n\nint ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tretval = access_process_vm(tsk, src, buf, this_len, 0);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tif (copy_to_user(dst, buf, retval))\n\t\t\treturn -EFAULT;\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nint ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tif (copy_from_user(buf, src, this_len))\n\t\t\treturn -EFAULT;\n\t\tretval = access_process_vm(tsk, dst, buf, this_len, 1);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nstatic int ptrace_setoptions(struct task_struct *child, unsigned long data)\n{\n\tunsigned flags;\n\n\tif (data & ~(unsigned long)PTRACE_O_MASK)\n\t\treturn -EINVAL;\n\n\t/* Avoid intermediate state when all opts are cleared */\n\tflags = child->ptrace;\n\tflags &= ~(PTRACE_O_MASK << PT_OPT_FLAG_SHIFT);\n\tflags |= (data << PT_OPT_FLAG_SHIFT);\n\tchild->ptrace = flags;\n\n\treturn 0;\n}\n\nstatic int ptrace_getsiginfo(struct task_struct *child, siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\t*info = *child->last_siginfo;\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_setsiginfo(struct task_struct *child, const siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\t*child->last_siginfo = *info;\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\n\n#ifdef PTRACE_SINGLESTEP\n#define is_singlestep(request)\t\t((request) == PTRACE_SINGLESTEP)\n#else\n#define is_singlestep(request)\t\t0\n#endif\n\n#ifdef PTRACE_SINGLEBLOCK\n#define is_singleblock(request)\t\t((request) == PTRACE_SINGLEBLOCK)\n#else\n#define is_singleblock(request)\t\t0\n#endif\n\n#ifdef PTRACE_SYSEMU\n#define is_sysemu_singlestep(request)\t((request) == PTRACE_SYSEMU_SINGLESTEP)\n#else\n#define is_sysemu_singlestep(request)\t0\n#endif\n\nstatic int ptrace_resume(struct task_struct *child, long request,\n\t\t\t unsigned long data)\n{\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\tif (request == PTRACE_SYSCALL)\n\t\tset_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\telse\n\t\tclear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);\n\n#ifdef TIF_SYSCALL_EMU\n\tif (request == PTRACE_SYSEMU || request == PTRACE_SYSEMU_SINGLESTEP)\n\t\tset_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n\telse\n\t\tclear_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n#endif\n\n\tif (is_singleblock(request)) {\n\t\tif (unlikely(!arch_has_block_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_block_step(child);\n\t} else if (is_singlestep(request) || is_sysemu_singlestep(request)) {\n\t\tif (unlikely(!arch_has_single_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_single_step(child);\n\t} else {\n\t\tuser_disable_single_step(child);\n\t}\n\n\tchild->exit_code = data;\n\twake_up_state(child, __TASK_TRACED);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\nstatic const struct user_regset *\nfind_regset(const struct user_regset_view *view, unsigned int type)\n{\n\tconst struct user_regset *regset;\n\tint n;\n\n\tfor (n = 0; n < view->n; ++n) {\n\t\tregset = view->regsets + n;\n\t\tif (regset->core_note_type == type)\n\t\t\treturn regset;\n\t}\n\n\treturn NULL;\n}\n\nstatic int ptrace_regset(struct task_struct *task, int req, unsigned int type,\n\t\t\t struct iovec *kiov)\n{\n\tconst struct user_regset_view *view = task_user_regset_view(task);\n\tconst struct user_regset *regset = find_regset(view, type);\n\tint regset_no;\n\n\tif (!regset || (kiov->iov_len % regset->size) != 0)\n\t\treturn -EINVAL;\n\n\tregset_no = regset - view->regsets;\n\tkiov->iov_len = min(kiov->iov_len,\n\t\t\t    (__kernel_size_t) (regset->n * regset->size));\n\n\tif (req == PTRACE_GETREGSET)\n\t\treturn copy_regset_to_user(task, view, regset_no, 0,\n\t\t\t\t\t   kiov->iov_len, kiov->iov_base);\n\telse\n\t\treturn copy_regset_from_user(task, view, regset_no, 0,\n\t\t\t\t\t     kiov->iov_len, kiov->iov_base);\n}\n\n#endif\n\nint ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tsiginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tif (copy_from_user(&siginfo, datavp, sizeof siginfo))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tptrace_signal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic struct task_struct *ptrace_get_task_struct(pid_t pid)\n{\n\tstruct task_struct *child;\n\n\trcu_read_lock();\n\tchild = find_task_by_vpid(pid);\n\tif (child)\n\t\tget_task_struct(child);\n\trcu_read_unlock();\n\n\tif (!child)\n\t\treturn ERR_PTR(-ESRCH);\n\treturn child;\n}\n\n#ifndef arch_ptrace_attach\n#define arch_ptrace_attach(child)\tdo { } while (0)\n#endif\n\nSYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,\n\t\tunsigned long, data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(current);\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (ret < 0)\n\t\tgoto out_put_task_struct;\n\n\tret = arch_ptrace(child, request, addr, data);\n\tif (ret || request != PTRACE_DETACH)\n\t\tptrace_unfreeze_traced(child);\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n\nint generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tunsigned long tmp;\n\tint copied;\n\n\tcopied = access_process_vm(tsk, addr, &tmp, sizeof(tmp), 0);\n\tif (copied != sizeof(tmp))\n\t\treturn -EIO;\n\treturn put_user(tmp, (unsigned long __user *)data);\n}\n\nint generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tint copied;\n\n\tcopied = access_process_vm(tsk, addr, &data, sizeof(data), 1);\n\treturn (copied == sizeof(data)) ? 0 : -EIO;\n}\n\n#if defined CONFIG_COMPAT\n#include <linux/compat.h>\n\nint compat_ptrace_request(struct task_struct *child, compat_long_t request,\n\t\t\t  compat_ulong_t addr, compat_ulong_t data)\n{\n\tcompat_ulong_t __user *datap = compat_ptr(data);\n\tcompat_ulong_t word;\n\tsiginfo_t siginfo;\n\tint ret;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\tret = access_process_vm(child, addr, &word, sizeof(word), 0);\n\t\tif (ret != sizeof(word))\n\t\t\tret = -EIO;\n\t\telse\n\t\t\tret = put_user(word, datap);\n\t\tbreak;\n\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\tret = access_process_vm(child, addr, &data, sizeof(data), 1);\n\t\tret = (ret != sizeof(data) ? -EIO : 0);\n\t\tbreak;\n\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user((compat_ulong_t) child->ptrace_message, datap);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user32(\n\t\t\t\t(struct compat_siginfo __user *) datap,\n\t\t\t\t&siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tmemset(&siginfo, 0, sizeof siginfo);\n\t\tif (copy_siginfo_from_user32(\n\t\t\t    &siginfo, (struct compat_siginfo __user *) datap))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct compat_iovec __user *uiov =\n\t\t\t(struct compat_iovec __user *) datap;\n\t\tcompat_uptr_t ptr;\n\t\tcompat_size_t len;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(ptr, &uiov->iov_base) ||\n\t\t    __get_user(len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tkiov.iov_base = compat_ptr(ptr);\n\t\tkiov.iov_len = len;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t}\n\n\treturn ret;\n}\n\nasmlinkage long compat_sys_ptrace(compat_long_t request, compat_long_t pid,\n\t\t\t\t  compat_long_t addr, compat_long_t data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tgoto out;\n\t}\n\n\tchild = ptrace_get_task_struct(pid);\n\tif (IS_ERR(child)) {\n\t\tret = PTR_ERR(child);\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (!ret) {\n\t\tret = compat_arch_ptrace(child, request, addr, data);\n\t\tif (ret || request != PTRACE_DETACH)\n\t\t\tptrace_unfreeze_traced(child);\n\t}\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n#endif\t/* CONFIG_COMPAT */\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\nint ptrace_get_breakpoints(struct task_struct *tsk)\n{\n\tif (atomic_inc_not_zero(&tsk->ptrace_bp_refcnt))\n\t\treturn 0;\n\n\treturn -1;\n}\n\nvoid ptrace_put_breakpoints(struct task_struct *tsk)\n{\n\tif (atomic_dec_and_test(&tsk->ptrace_bp_refcnt))\n\t\tflush_ptrace_hw_breakpoint(tsk);\n}\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n", "/*\n *  linux/kernel/signal.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  1997-11-02  Modified for POSIX.1b signals by Richard Henderson\n *\n *  2003-06-02  Jim Houston - Concurrent Computer Corp.\n *\t\tChanges to use preallocated sigqueue structures\n *\t\tto allow signals to be sent reliably.\n */\n\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/tty.h>\n#include <linux/binfmts.h>\n#include <linux/coredump.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/ptrace.h>\n#include <linux/signal.h>\n#include <linux/signalfd.h>\n#include <linux/ratelimit.h>\n#include <linux/tracehook.h>\n#include <linux/capability.h>\n#include <linux/freezer.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/user_namespace.h>\n#include <linux/uprobes.h>\n#include <linux/compat.h>\n#define CREATE_TRACE_POINTS\n#include <trace/events/signal.h>\n\n#include <asm/param.h>\n#include <asm/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/siginfo.h>\n#include <asm/cacheflush.h>\n#include \"audit.h\"\t/* audit_signal_info() */\n\n/*\n * SLAB caches for signal bits.\n */\n\nstatic struct kmem_cache *sigqueue_cachep;\n\nint print_fatal_signals __read_mostly;\n\nstatic void __user *sig_handler(struct task_struct *t, int sig)\n{\n\treturn t->sighand->action[sig - 1].sa.sa_handler;\n}\n\nstatic int sig_handler_ignored(void __user *handler, int sig)\n{\n\t/* Is it explicitly or implicitly ignored? */\n\treturn handler == SIG_IGN ||\n\t\t(handler == SIG_DFL && sig_kernel_ignore(sig));\n}\n\nstatic int sig_task_ignored(struct task_struct *t, int sig, bool force)\n{\n\tvoid __user *handler;\n\n\thandler = sig_handler(t, sig);\n\n\tif (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\thandler == SIG_DFL && !force)\n\t\treturn 1;\n\n\treturn sig_handler_ignored(handler, sig);\n}\n\nstatic int sig_ignored(struct task_struct *t, int sig, bool force)\n{\n\t/*\n\t * Blocked signals are never ignored, since the\n\t * signal handler may change by the time it is\n\t * unblocked.\n\t */\n\tif (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))\n\t\treturn 0;\n\n\tif (!sig_task_ignored(t, sig, force))\n\t\treturn 0;\n\n\t/*\n\t * Tracers may want to know about even ignored signals.\n\t */\n\treturn !t->ptrace;\n}\n\n/*\n * Re-calculate pending state from the set of locally pending\n * signals, globally pending signals, and blocked signals.\n */\nstatic inline int has_pending_signals(sigset_t *signal, sigset_t *blocked)\n{\n\tunsigned long ready;\n\tlong i;\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = _NSIG_WORDS, ready = 0; --i >= 0 ;)\n\t\t\tready |= signal->sig[i] &~ blocked->sig[i];\n\t\tbreak;\n\n\tcase 4: ready  = signal->sig[3] &~ blocked->sig[3];\n\t\tready |= signal->sig[2] &~ blocked->sig[2];\n\t\tready |= signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 2: ready  = signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 1: ready  = signal->sig[0] &~ blocked->sig[0];\n\t}\n\treturn ready !=\t0;\n}\n\n#define PENDING(p,b) has_pending_signals(&(p)->signal, (b))\n\nstatic int recalc_sigpending_tsk(struct task_struct *t)\n{\n\tif ((t->jobctl & JOBCTL_PENDING_MASK) ||\n\t    PENDING(&t->pending, &t->blocked) ||\n\t    PENDING(&t->signal->shared_pending, &t->blocked)) {\n\t\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t\treturn 1;\n\t}\n\t/*\n\t * We must never clear the flag in another thread, or in current\n\t * when it's possible the current syscall is returning -ERESTART*.\n\t * So we don't clear it here, and only callers who know they should do.\n\t */\n\treturn 0;\n}\n\n/*\n * After recalculating TIF_SIGPENDING, we need to make sure the task wakes up.\n * This is superfluous when called on current, the wakeup is a harmless no-op.\n */\nvoid recalc_sigpending_and_wake(struct task_struct *t)\n{\n\tif (recalc_sigpending_tsk(t))\n\t\tsignal_wake_up(t, 0);\n}\n\nvoid recalc_sigpending(void)\n{\n\tif (!recalc_sigpending_tsk(current) && !freezing(current))\n\t\tclear_thread_flag(TIF_SIGPENDING);\n\n}\n\n/* Given the mask, find the first available signal that should be serviced. */\n\n#define SYNCHRONOUS_MASK \\\n\t(sigmask(SIGSEGV) | sigmask(SIGBUS) | sigmask(SIGILL) | \\\n\t sigmask(SIGTRAP) | sigmask(SIGFPE) | sigmask(SIGSYS))\n\nint next_signal(struct sigpending *pending, sigset_t *mask)\n{\n\tunsigned long i, *s, *m, x;\n\tint sig = 0;\n\n\ts = pending->signal.sig;\n\tm = mask->sig;\n\n\t/*\n\t * Handle the first word specially: it contains the\n\t * synchronous signals that need to be dequeued first.\n\t */\n\tx = *s &~ *m;\n\tif (x) {\n\t\tif (x & SYNCHRONOUS_MASK)\n\t\t\tx &= SYNCHRONOUS_MASK;\n\t\tsig = ffz(~x) + 1;\n\t\treturn sig;\n\t}\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = 1; i < _NSIG_WORDS; ++i) {\n\t\t\tx = *++s &~ *++m;\n\t\t\tif (!x)\n\t\t\t\tcontinue;\n\t\t\tsig = ffz(~x) + i*_NSIG_BPW + 1;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 2:\n\t\tx = s[1] &~ m[1];\n\t\tif (!x)\n\t\t\tbreak;\n\t\tsig = ffz(~x) + _NSIG_BPW + 1;\n\t\tbreak;\n\n\tcase 1:\n\t\t/* Nothing to do */\n\t\tbreak;\n\t}\n\n\treturn sig;\n}\n\nstatic inline void print_dropped_signal(int sig)\n{\n\tstatic DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);\n\n\tif (!print_fatal_signals)\n\t\treturn;\n\n\tif (!__ratelimit(&ratelimit_state))\n\t\treturn;\n\n\tprintk(KERN_INFO \"%s/%d: reached RLIMIT_SIGPENDING, dropped signal %d\\n\",\n\t\t\t\tcurrent->comm, current->pid, sig);\n}\n\n/**\n * task_set_jobctl_pending - set jobctl pending bits\n * @task: target task\n * @mask: pending bits to set\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK | %JOBCTL_STOP_CONSUME | %JOBCTL_STOP_SIGMASK |\n * %JOBCTL_TRAPPING.  If stop signo is being set, the existing signo is\n * cleared.  If @task is already being killed or exiting, this function\n * becomes noop.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if @mask is set, %false if made noop because @task was dying.\n */\nbool task_set_jobctl_pending(struct task_struct *task, unsigned int mask)\n{\n\tBUG_ON(mask & ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |\n\t\t\tJOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));\n\tBUG_ON((mask & JOBCTL_TRAPPING) && !(mask & JOBCTL_PENDING_MASK));\n\n\tif (unlikely(fatal_signal_pending(task) || (task->flags & PF_EXITING)))\n\t\treturn false;\n\n\tif (mask & JOBCTL_STOP_SIGMASK)\n\t\ttask->jobctl &= ~JOBCTL_STOP_SIGMASK;\n\n\ttask->jobctl |= mask;\n\treturn true;\n}\n\n/**\n * task_clear_jobctl_trapping - clear jobctl trapping bit\n * @task: target task\n *\n * If JOBCTL_TRAPPING is set, a ptracer is waiting for us to enter TRACED.\n * Clear it and wake up the ptracer.  Note that we don't need any further\n * locking.  @task->siglock guarantees that @task->parent points to the\n * ptracer.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_trapping(struct task_struct *task)\n{\n\tif (unlikely(task->jobctl & JOBCTL_TRAPPING)) {\n\t\ttask->jobctl &= ~JOBCTL_TRAPPING;\n\t\twake_up_bit(&task->jobctl, JOBCTL_TRAPPING_BIT);\n\t}\n}\n\n/**\n * task_clear_jobctl_pending - clear jobctl pending bits\n * @task: target task\n * @mask: pending bits to clear\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK.  If %JOBCTL_STOP_PENDING is being cleared, other\n * STOP bits are cleared together.\n *\n * If clearing of @mask leaves no stop or trap pending, this function calls\n * task_clear_jobctl_trapping().\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_pending(struct task_struct *task, unsigned int mask)\n{\n\tBUG_ON(mask & ~JOBCTL_PENDING_MASK);\n\n\tif (mask & JOBCTL_STOP_PENDING)\n\t\tmask |= JOBCTL_STOP_CONSUME | JOBCTL_STOP_DEQUEUED;\n\n\ttask->jobctl &= ~mask;\n\n\tif (!(task->jobctl & JOBCTL_PENDING_MASK))\n\t\ttask_clear_jobctl_trapping(task);\n}\n\n/**\n * task_participate_group_stop - participate in a group stop\n * @task: task participating in a group stop\n *\n * @task has %JOBCTL_STOP_PENDING set and is participating in a group stop.\n * Group stop states are cleared and the group stop count is consumed if\n * %JOBCTL_STOP_CONSUME was set.  If the consumption completes the group\n * stop, the appropriate %SIGNAL_* flags are set.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if group stop completion should be notified to the parent, %false\n * otherwise.\n */\nstatic bool task_participate_group_stop(struct task_struct *task)\n{\n\tstruct signal_struct *sig = task->signal;\n\tbool consume = task->jobctl & JOBCTL_STOP_CONSUME;\n\n\tWARN_ON_ONCE(!(task->jobctl & JOBCTL_STOP_PENDING));\n\n\ttask_clear_jobctl_pending(task, JOBCTL_STOP_PENDING);\n\n\tif (!consume)\n\t\treturn false;\n\n\tif (!WARN_ON_ONCE(sig->group_stop_count == 0))\n\t\tsig->group_stop_count--;\n\n\t/*\n\t * Tell the caller to notify completion iff we are entering into a\n\t * fresh group stop.  Read comment in do_signal_stop() for details.\n\t */\n\tif (!sig->group_stop_count && !(sig->flags & SIGNAL_STOP_STOPPED)) {\n\t\tsig->flags = SIGNAL_STOP_STOPPED;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * allocate a new signal queue record\n * - this may be called without locks if and only if t == current, otherwise an\n *   appropriate lock must be held to stop the target task from exiting\n */\nstatic struct sigqueue *\n__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)\n{\n\tstruct sigqueue *q = NULL;\n\tstruct user_struct *user;\n\n\t/*\n\t * Protect access to @t credentials. This can go away when all\n\t * callers hold rcu read lock.\n\t */\n\trcu_read_lock();\n\tuser = get_uid(__task_cred(t)->user);\n\tatomic_inc(&user->sigpending);\n\trcu_read_unlock();\n\n\tif (override_rlimit ||\n\t    atomic_read(&user->sigpending) <=\n\t\t\ttask_rlimit(t, RLIMIT_SIGPENDING)) {\n\t\tq = kmem_cache_alloc(sigqueue_cachep, flags);\n\t} else {\n\t\tprint_dropped_signal(sig);\n\t}\n\n\tif (unlikely(q == NULL)) {\n\t\tatomic_dec(&user->sigpending);\n\t\tfree_uid(user);\n\t} else {\n\t\tINIT_LIST_HEAD(&q->list);\n\t\tq->flags = 0;\n\t\tq->user = user;\n\t}\n\n\treturn q;\n}\n\nstatic void __sigqueue_free(struct sigqueue *q)\n{\n\tif (q->flags & SIGQUEUE_PREALLOC)\n\t\treturn;\n\tatomic_dec(&q->user->sigpending);\n\tfree_uid(q->user);\n\tkmem_cache_free(sigqueue_cachep, q);\n}\n\nvoid flush_sigqueue(struct sigpending *queue)\n{\n\tstruct sigqueue *q;\n\n\tsigemptyset(&queue->signal);\n\twhile (!list_empty(&queue->list)) {\n\t\tq = list_entry(queue->list.next, struct sigqueue , list);\n\t\tlist_del_init(&q->list);\n\t\t__sigqueue_free(q);\n\t}\n}\n\n/*\n * Flush all pending signals for a task.\n */\nvoid __flush_signals(struct task_struct *t)\n{\n\tclear_tsk_thread_flag(t, TIF_SIGPENDING);\n\tflush_sigqueue(&t->pending);\n\tflush_sigqueue(&t->signal->shared_pending);\n}\n\nvoid flush_signals(struct task_struct *t)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\t__flush_signals(t);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n}\n\nstatic void __flush_itimer_signals(struct sigpending *pending)\n{\n\tsigset_t signal, retain;\n\tstruct sigqueue *q, *n;\n\n\tsignal = pending->signal;\n\tsigemptyset(&retain);\n\n\tlist_for_each_entry_safe(q, n, &pending->list, list) {\n\t\tint sig = q->info.si_signo;\n\n\t\tif (likely(q->info.si_code != SI_TIMER)) {\n\t\t\tsigaddset(&retain, sig);\n\t\t} else {\n\t\t\tsigdelset(&signal, sig);\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\n\tsigorsets(&pending->signal, &signal, &retain);\n}\n\nvoid flush_itimer_signals(void)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tsk->sighand->siglock, flags);\n\t__flush_itimer_signals(&tsk->pending);\n\t__flush_itimer_signals(&tsk->signal->shared_pending);\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, flags);\n}\n\nvoid ignore_signals(struct task_struct *t)\n{\n\tint i;\n\n\tfor (i = 0; i < _NSIG; ++i)\n\t\tt->sighand->action[i].sa.sa_handler = SIG_IGN;\n\n\tflush_signals(t);\n}\n\n/*\n * Flush all handlers for a task.\n */\n\nvoid\nflush_signal_handlers(struct task_struct *t, int force_default)\n{\n\tint i;\n\tstruct k_sigaction *ka = &t->sighand->action[0];\n\tfor (i = _NSIG ; i != 0 ; i--) {\n\t\tif (force_default || ka->sa.sa_handler != SIG_IGN)\n\t\t\tka->sa.sa_handler = SIG_DFL;\n\t\tka->sa.sa_flags = 0;\n\t\tsigemptyset(&ka->sa.sa_mask);\n\t\tka++;\n\t}\n}\n\nint unhandled_signal(struct task_struct *tsk, int sig)\n{\n\tvoid __user *handler = tsk->sighand->action[sig-1].sa.sa_handler;\n\tif (is_global_init(tsk))\n\t\treturn 1;\n\tif (handler != SIG_IGN && handler != SIG_DFL)\n\t\treturn 0;\n\t/* if ptraced, let the tracer determine */\n\treturn !tsk->ptrace;\n}\n\n/*\n * Notify the system that a driver wants to block all signals for this\n * process, and wants to be notified if any signals at all were to be\n * sent/acted upon.  If the notifier routine returns non-zero, then the\n * signal will be acted upon after all.  If the notifier routine returns 0,\n * then then signal will be blocked.  Only one block per process is\n * allowed.  priv is a pointer to private data that the notifier routine\n * can use to determine if the signal should be blocked or not.\n */\nvoid\nblock_all_signals(int (*notifier)(void *priv), void *priv, sigset_t *mask)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\tcurrent->notifier_mask = mask;\n\tcurrent->notifier_data = priv;\n\tcurrent->notifier = notifier;\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\n/* Notify the system that blocking has ended. */\n\nvoid\nunblock_all_signals(void)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\tcurrent->notifier = NULL;\n\tcurrent->notifier_data = NULL;\n\trecalc_sigpending();\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\nstatic void collect_signal(int sig, struct sigpending *list, siginfo_t *info)\n{\n\tstruct sigqueue *q, *first = NULL;\n\n\t/*\n\t * Collect the siginfo appropriate to this signal.  Check if\n\t * there is another siginfo for the same signal.\n\t*/\n\tlist_for_each_entry(q, &list->list, list) {\n\t\tif (q->info.si_signo == sig) {\n\t\t\tif (first)\n\t\t\t\tgoto still_pending;\n\t\t\tfirst = q;\n\t\t}\n\t}\n\n\tsigdelset(&list->signal, sig);\n\n\tif (first) {\nstill_pending:\n\t\tlist_del_init(&first->list);\n\t\tcopy_siginfo(info, &first->info);\n\t\t__sigqueue_free(first);\n\t} else {\n\t\t/*\n\t\t * Ok, it wasn't in the queue.  This must be\n\t\t * a fast-pathed signal or we must have been\n\t\t * out of queue space.  So zero out the info.\n\t\t */\n\t\tinfo->si_signo = sig;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\tinfo->si_pid = 0;\n\t\tinfo->si_uid = 0;\n\t}\n}\n\nstatic int __dequeue_signal(struct sigpending *pending, sigset_t *mask,\n\t\t\tsiginfo_t *info)\n{\n\tint sig = next_signal(pending, mask);\n\n\tif (sig) {\n\t\tif (current->notifier) {\n\t\t\tif (sigismember(current->notifier_mask, sig)) {\n\t\t\t\tif (!(current->notifier)(current->notifier_data)) {\n\t\t\t\t\tclear_thread_flag(TIF_SIGPENDING);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tcollect_signal(sig, pending, info);\n\t}\n\n\treturn sig;\n}\n\n/*\n * Dequeue a signal and return the element to the caller, which is\n * expected to free it.\n *\n * All callers have to hold the siglock.\n */\nint dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)\n{\n\tint signr;\n\n\t/* We only dequeue private signals from ourselves, we don't let\n\t * signalfd steal them\n\t */\n\tsignr = __dequeue_signal(&tsk->pending, mask, info);\n\tif (!signr) {\n\t\tsignr = __dequeue_signal(&tsk->signal->shared_pending,\n\t\t\t\t\t mask, info);\n\t\t/*\n\t\t * itimer signal ?\n\t\t *\n\t\t * itimers are process shared and we restart periodic\n\t\t * itimers in the signal delivery path to prevent DoS\n\t\t * attacks in the high resolution timer case. This is\n\t\t * compliant with the old way of self-restarting\n\t\t * itimers, as the SIGALRM is a legacy signal and only\n\t\t * queued once. Changing the restart behaviour to\n\t\t * restart the timer in the signal dequeue path is\n\t\t * reducing the timer noise on heavy loaded !highres\n\t\t * systems too.\n\t\t */\n\t\tif (unlikely(signr == SIGALRM)) {\n\t\t\tstruct hrtimer *tmr = &tsk->signal->real_timer;\n\n\t\t\tif (!hrtimer_is_queued(tmr) &&\n\t\t\t    tsk->signal->it_real_incr.tv64 != 0) {\n\t\t\t\thrtimer_forward(tmr, tmr->base->get_time(),\n\t\t\t\t\t\ttsk->signal->it_real_incr);\n\t\t\t\thrtimer_restart(tmr);\n\t\t\t}\n\t\t}\n\t}\n\n\trecalc_sigpending();\n\tif (!signr)\n\t\treturn 0;\n\n\tif (unlikely(sig_kernel_stop(signr))) {\n\t\t/*\n\t\t * Set a marker that we have dequeued a stop signal.  Our\n\t\t * caller might release the siglock and then the pending\n\t\t * stop signal it is about to process is no longer in the\n\t\t * pending bitmasks, but must still be cleared by a SIGCONT\n\t\t * (and overruled by a SIGKILL).  So those cases clear this\n\t\t * shared flag after we've set it.  Note that this flag may\n\t\t * remain set after the signal we return is ignored or\n\t\t * handled.  That doesn't matter because its only purpose\n\t\t * is to alert stop-signal processing code when another\n\t\t * processor has come along and cleared the flag.\n\t\t */\n\t\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\t}\n\tif ((info->si_code & __SI_MASK) == __SI_TIMER && info->si_sys_private) {\n\t\t/*\n\t\t * Release the siglock to ensure proper locking order\n\t\t * of timer locks outside of siglocks.  Note, we leave\n\t\t * irqs disabled here, since the posix-timers code is\n\t\t * about to disable them again anyway.\n\t\t */\n\t\tspin_unlock(&tsk->sighand->siglock);\n\t\tdo_schedule_next_timer(info);\n\t\tspin_lock(&tsk->sighand->siglock);\n\t}\n\treturn signr;\n}\n\n/*\n * Tell a process that it has a new active signal..\n *\n * NOTE! we rely on the previous spin_lock to\n * lock interrupts for us! We can only be called with\n * \"siglock\" held, and the local interrupt must\n * have been disabled when that got acquired!\n *\n * No need to set need_resched since signal event passing\n * goes through ->blocked\n */\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}\n\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n *\n * This version takes a sigset mask and looks at all signals,\n * not just those in the first mask word.\n */\nstatic int rm_from_queue_full(sigset_t *mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\tsigset_t m;\n\n\tsigandsets(&m, mask, &s->signal);\n\tif (sigisemptyset(&m))\n\t\treturn 0;\n\n\tsigandnsets(&s->signal, &s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (sigismember(mask, q->info.si_signo)) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\treturn 1;\n}\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n */\nstatic int rm_from_queue(unsigned long mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\n\tif (!sigtestsetmask(&s->signal, mask))\n\t\treturn 0;\n\n\tsigdelsetmask(&s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (q->info.si_signo < SIGRTMIN &&\n\t\t    (mask & sigmask(q->info.si_signo))) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic inline int is_si_special(const struct siginfo *info)\n{\n\treturn info <= SEND_SIG_FORCED;\n}\n\nstatic inline bool si_fromuser(const struct siginfo *info)\n{\n\treturn info == SEND_SIG_NOINFO ||\n\t\t(!is_si_special(info) && SI_FROMUSER(info));\n}\n\n/*\n * called with RCU read lock from check_kill_permission()\n */\nstatic int kill_ok_by_cred(struct task_struct *t)\n{\n\tconst struct cred *cred = current_cred();\n\tconst struct cred *tcred = __task_cred(t);\n\n\tif (uid_eq(cred->euid, tcred->suid) ||\n\t    uid_eq(cred->euid, tcred->uid)  ||\n\t    uid_eq(cred->uid,  tcred->suid) ||\n\t    uid_eq(cred->uid,  tcred->uid))\n\t\treturn 1;\n\n\tif (ns_capable(tcred->user_ns, CAP_KILL))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n/*\n * Bad permissions for sending the signal\n * - the caller must hold the RCU read lock\n */\nstatic int check_kill_permission(int sig, struct siginfo *info,\n\t\t\t\t struct task_struct *t)\n{\n\tstruct pid *sid;\n\tint error;\n\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\tif (!si_fromuser(info))\n\t\treturn 0;\n\n\terror = audit_signal_info(sig, t); /* Let audit system see the signal */\n\tif (error)\n\t\treturn error;\n\n\tif (!same_thread_group(current, t) &&\n\t    !kill_ok_by_cred(t)) {\n\t\tswitch (sig) {\n\t\tcase SIGCONT:\n\t\t\tsid = task_session(t);\n\t\t\t/*\n\t\t\t * We don't return the error if sid == NULL. The\n\t\t\t * task was unhashed, the caller must notice this.\n\t\t\t */\n\t\t\tif (!sid || sid == task_session(current))\n\t\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n\n\treturn security_task_kill(t, info, sig, 0);\n}\n\n/**\n * ptrace_trap_notify - schedule trap to notify ptracer\n * @t: tracee wanting to notify tracer\n *\n * This function schedules sticky ptrace trap which is cleared on the next\n * TRAP_STOP to notify ptracer of an event.  @t must have been seized by\n * ptracer.\n *\n * If @t is running, STOP trap will be taken.  If trapped for STOP and\n * ptracer is listening for events, tracee is woken up so that it can\n * re-trap for the new event.  If trapped otherwise, STOP trap will be\n * eventually taken without returning to userland after the existing traps\n * are finished by PTRACE_CONT.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nstatic void ptrace_trap_notify(struct task_struct *t)\n{\n\tWARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\n\tassert_spin_locked(&t->sighand->siglock);\n\n\ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n}\n\n/*\n * Handle magic process-wide effects of stop/continue signals. Unlike\n * the signal actions, these happen immediately at signal-generation\n * time regardless of blocking, ignoring, or handling.  This does the\n * actual continuing for SIGCONT, but not the actual stopping for stop\n * signals. The process stop is done as a signal action for SIG_DFL.\n *\n * Returns true if the signal should be actually delivered, otherwise\n * it should be dropped.\n */\nstatic int prepare_signal(int sig, struct task_struct *p, bool force)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\tif (unlikely(signal->flags & SIGNAL_GROUP_EXIT)) {\n\t\t/*\n\t\t * The process is in the middle of dying, nothing to do.\n\t\t */\n\t} else if (sig_kernel_stop(sig)) {\n\t\t/*\n\t\t * This is a stop signal.  Remove SIGCONT from all queues.\n\t\t */\n\t\trm_from_queue(sigmask(SIGCONT), &signal->shared_pending);\n\t\tt = p;\n\t\tdo {\n\t\t\trm_from_queue(sigmask(SIGCONT), &t->pending);\n\t\t} while_each_thread(p, t);\n\t} else if (sig == SIGCONT) {\n\t\tunsigned int why;\n\t\t/*\n\t\t * Remove all stop signals from all queues, wake all threads.\n\t\t */\n\t\trm_from_queue(SIG_KERNEL_STOP_MASK, &signal->shared_pending);\n\t\tt = p;\n\t\tdo {\n\t\t\ttask_clear_jobctl_pending(t, JOBCTL_STOP_PENDING);\n\t\t\trm_from_queue(SIG_KERNEL_STOP_MASK, &t->pending);\n\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\twake_up_state(t, __TASK_STOPPED);\n\t\t\telse\n\t\t\t\tptrace_trap_notify(t);\n\t\t} while_each_thread(p, t);\n\n\t\t/*\n\t\t * Notify the parent with CLD_CONTINUED if we were stopped.\n\t\t *\n\t\t * If we were in the middle of a group stop, we pretend it\n\t\t * was already finished, and then continued. Since SIGCHLD\n\t\t * doesn't queue we report only CLD_STOPPED, as if the next\n\t\t * CLD_CONTINUED was dropped.\n\t\t */\n\t\twhy = 0;\n\t\tif (signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\twhy |= SIGNAL_CLD_CONTINUED;\n\t\telse if (signal->group_stop_count)\n\t\t\twhy |= SIGNAL_CLD_STOPPED;\n\n\t\tif (why) {\n\t\t\t/*\n\t\t\t * The first thread which returns from do_signal_stop()\n\t\t\t * will take ->siglock, notice SIGNAL_CLD_MASK, and\n\t\t\t * notify its parent. See get_signal_to_deliver().\n\t\t\t */\n\t\t\tsignal->flags = why | SIGNAL_STOP_CONTINUED;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tsignal->group_exit_code = 0;\n\t\t}\n\t}\n\n\treturn !sig_ignored(p, sig, force);\n}\n\n/*\n * Test if P wants to take SIG.  After we've checked all threads with this,\n * it's equivalent to finding no threads not blocking SIG.  Any threads not\n * blocking SIG were ruled out because they are not running and already\n * have pending signals.  Such threads will dequeue from the shared queue\n * as soon as they're available, so putting the signal on the shared queue\n * will be equivalent to sending it to one such thread.\n */\nstatic inline int wants_signal(int sig, struct task_struct *p)\n{\n\tif (sigismember(&p->blocked, sig))\n\t\treturn 0;\n\tif (p->flags & PF_EXITING)\n\t\treturn 0;\n\tif (sig == SIGKILL)\n\t\treturn 1;\n\tif (task_is_stopped_or_traced(p))\n\t\treturn 0;\n\treturn task_curr(p) || !signal_pending(p);\n}\n\nstatic void complete_signal(int sig, struct task_struct *p, int group)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\t/*\n\t * Now find a thread we can wake up to take the signal off the queue.\n\t *\n\t * If the main thread wants the signal, it gets first crack.\n\t * Probably the least surprising to the average bear.\n\t */\n\tif (wants_signal(sig, p))\n\t\tt = p;\n\telse if (!group || thread_group_empty(p))\n\t\t/*\n\t\t * There is just one thread and it does not need to be woken.\n\t\t * It will dequeue unblocked signals before it runs again.\n\t\t */\n\t\treturn;\n\telse {\n\t\t/*\n\t\t * Otherwise try to find a suitable thread.\n\t\t */\n\t\tt = signal->curr_target;\n\t\twhile (!wants_signal(sig, t)) {\n\t\t\tt = next_thread(t);\n\t\t\tif (t == signal->curr_target)\n\t\t\t\t/*\n\t\t\t\t * No thread needs to be woken.\n\t\t\t\t * Any eligible threads will see\n\t\t\t\t * the signal in the queue soon.\n\t\t\t\t */\n\t\t\t\treturn;\n\t\t}\n\t\tsignal->curr_target = t;\n\t}\n\n\t/*\n\t * Found a killable thread.  If the signal will be fatal,\n\t * then start taking the whole group down immediately.\n\t */\n\tif (sig_fatal(p, sig) &&\n\t    !(signal->flags & (SIGNAL_UNKILLABLE | SIGNAL_GROUP_EXIT)) &&\n\t    !sigismember(&t->real_blocked, sig) &&\n\t    (sig == SIGKILL || !t->ptrace)) {\n\t\t/*\n\t\t * This signal will be fatal to the whole group.\n\t\t */\n\t\tif (!sig_kernel_coredump(sig)) {\n\t\t\t/*\n\t\t\t * Start a group exit and wake everybody up.\n\t\t\t * This way we don't have other threads\n\t\t\t * running and doing things after a slower\n\t\t\t * thread has the fatal signal pending.\n\t\t\t */\n\t\t\tsignal->flags = SIGNAL_GROUP_EXIT;\n\t\t\tsignal->group_exit_code = sig;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tt = p;\n\t\t\tdo {\n\t\t\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\t\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\t\t\tsignal_wake_up(t, 1);\n\t\t\t} while_each_thread(p, t);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * The signal is already in the shared-pending queue.\n\t * Tell the chosen thread to wake up and dequeue it.\n\t */\n\tsignal_wake_up(t, sig == SIGKILL);\n\treturn;\n}\n\nstatic inline int legacy_queue(struct sigpending *signals, int sig)\n{\n\treturn (sig < SIGRTMIN) && sigismember(&signals->signal, sig);\n}\n\n#ifdef CONFIG_USER_NS\nstatic inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)\n{\n\tif (current_user_ns() == task_cred_xxx(t, user_ns))\n\t\treturn;\n\n\tif (SI_FROMKERNEL(info))\n\t\treturn;\n\n\trcu_read_lock();\n\tinfo->si_uid = from_kuid_munged(task_cred_xxx(t, user_ns),\n\t\t\t\t\tmake_kuid(current_user_ns(), info->si_uid));\n\trcu_read_unlock();\n}\n#else\nstatic inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)\n{\n\treturn;\n}\n#endif\n\nstatic int __send_signal(int sig, struct siginfo *info, struct task_struct *t,\n\t\t\tint group, int from_ancestor_ns)\n{\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint override_rlimit;\n\tint ret = 0, result;\n\n\tassert_spin_locked(&t->sighand->siglock);\n\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t,\n\t\t\tfrom_ancestor_ns || (info == SEND_SIG_FORCED)))\n\t\tgoto ret;\n\n\tpending = group ? &t->signal->shared_pending : &t->pending;\n\t/*\n\t * Short-circuit ignored signals and support queuing\n\t * exactly one non-rt signal, so that we can get more\n\t * detailed information about the cause of the signal.\n\t */\n\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\tif (legacy_queue(pending, sig))\n\t\tgoto ret;\n\n\tresult = TRACE_SIGNAL_DELIVERED;\n\t/*\n\t * fast-pathed signals for kernel-internal things like SIGSTOP\n\t * or SIGKILL.\n\t */\n\tif (info == SEND_SIG_FORCED)\n\t\tgoto out_set;\n\n\t/*\n\t * Real-time signals must be queued if sent by sigqueue, or\n\t * some other real-time mechanism.  It is implementation\n\t * defined whether kill() does so.  We attempt to do so, on\n\t * the principle of least surprise, but since kill is not\n\t * allowed to fail with EAGAIN when low on memory we just\n\t * make sure at least one signal gets delivered and don't\n\t * pass on the info struct.\n\t */\n\tif (sig < SIGRTMIN)\n\t\toverride_rlimit = (is_si_special(info) || info->si_code >= 0);\n\telse\n\t\toverride_rlimit = 0;\n\n\tq = __sigqueue_alloc(sig, t, GFP_ATOMIC | __GFP_NOTRACK_FALSE_POSITIVE,\n\t\toverride_rlimit);\n\tif (q) {\n\t\tlist_add_tail(&q->list, &pending->list);\n\t\tswitch ((unsigned long) info) {\n\t\tcase (unsigned long) SEND_SIG_NOINFO:\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_USER;\n\t\t\tq->info.si_pid = task_tgid_nr_ns(current,\n\t\t\t\t\t\t\ttask_active_pid_ns(t));\n\t\t\tq->info.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\t\t\tbreak;\n\t\tcase (unsigned long) SEND_SIG_PRIV:\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_KERNEL;\n\t\t\tq->info.si_pid = 0;\n\t\t\tq->info.si_uid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tcopy_siginfo(&q->info, info);\n\t\t\tif (from_ancestor_ns)\n\t\t\t\tq->info.si_pid = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tuserns_fixup_signal_uid(&q->info, t);\n\n\t} else if (!is_si_special(info)) {\n\t\tif (sig >= SIGRTMIN && info->si_code != SI_USER) {\n\t\t\t/*\n\t\t\t * Queue overflow, abort.  We may abort if the\n\t\t\t * signal was rt and sent by user using something\n\t\t\t * other than kill().\n\t\t\t */\n\t\t\tresult = TRACE_SIGNAL_OVERFLOW_FAIL;\n\t\t\tret = -EAGAIN;\n\t\t\tgoto ret;\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is a silent loss of information.  We still\n\t\t\t * send the signal, but the *info bits are lost.\n\t\t\t */\n\t\t\tresult = TRACE_SIGNAL_LOSE_INFO;\n\t\t}\n\t}\n\nout_set:\n\tsignalfd_notify(t, sig);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, group);\nret:\n\ttrace_signal_generate(sig, info, t, group, result);\n\treturn ret;\n}\n\nstatic int send_signal(int sig, struct siginfo *info, struct task_struct *t,\n\t\t\tint group)\n{\n\tint from_ancestor_ns = 0;\n\n#ifdef CONFIG_PID_NS\n\tfrom_ancestor_ns = si_fromuser(info) &&\n\t\t\t   !task_pid_nr_ns(current, task_active_pid_ns(t));\n#endif\n\n\treturn __send_signal(sig, info, t, group, from_ancestor_ns);\n}\n\nstatic void print_fatal_signal(int signr)\n{\n\tstruct pt_regs *regs = signal_pt_regs();\n\tprintk(\"%s/%d: potentially unexpected fatal signal %d.\\n\",\n\t\tcurrent->comm, task_pid_nr(current), signr);\n\n#if defined(__i386__) && !defined(__arch_um__)\n\tprintk(\"code at %08lx: \", regs->ip);\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < 16; i++) {\n\t\t\tunsigned char insn;\n\n\t\t\tif (get_user(insn, (unsigned char *)(regs->ip + i)))\n\t\t\t\tbreak;\n\t\t\tprintk(\"%02x \", insn);\n\t\t}\n\t}\n#endif\n\tprintk(\"\\n\");\n\tpreempt_disable();\n\tshow_regs(regs);\n\tpreempt_enable();\n}\n\nstatic int __init setup_print_fatal_signals(char *str)\n{\n\tget_option (&str, &print_fatal_signals);\n\n\treturn 1;\n}\n\n__setup(\"print-fatal-signals=\", setup_print_fatal_signals);\n\nint\n__group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\treturn send_signal(sig, info, p, 1);\n}\n\nstatic int\nspecific_send_sig_info(int sig, struct siginfo *info, struct task_struct *t)\n{\n\treturn send_signal(sig, info, t, 0);\n}\n\nint do_send_sig_info(int sig, struct siginfo *info, struct task_struct *p,\n\t\t\tbool group)\n{\n\tunsigned long flags;\n\tint ret = -ESRCH;\n\n\tif (lock_task_sighand(p, &flags)) {\n\t\tret = send_signal(sig, info, p, group);\n\t\tunlock_task_sighand(p, &flags);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Force a signal that the process can't ignore: if necessary\n * we unblock the signal and change any SIG_IGN to SIG_DFL.\n *\n * Note: If we unblock the signal, we always reset it to SIG_DFL,\n * since we do not want to have a signal handler that was blocked\n * be invoked when user space had explicitly blocked it.\n *\n * We don't want to have recursive SIGSEGV's etc, for example,\n * that is why we also clear SIGNAL_UNKILLABLE.\n */\nint\nforce_sig_info(int sig, struct siginfo *info, struct task_struct *t)\n{\n\tunsigned long int flags;\n\tint ret, blocked, ignored;\n\tstruct k_sigaction *action;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\taction = &t->sighand->action[sig-1];\n\tignored = action->sa.sa_handler == SIG_IGN;\n\tblocked = sigismember(&t->blocked, sig);\n\tif (blocked || ignored) {\n\t\taction->sa.sa_handler = SIG_DFL;\n\t\tif (blocked) {\n\t\t\tsigdelset(&t->blocked, sig);\n\t\t\trecalc_sigpending_and_wake(t);\n\t\t}\n\t}\n\tif (action->sa.sa_handler == SIG_DFL)\n\t\tt->signal->flags &= ~SIGNAL_UNKILLABLE;\n\tret = specific_send_sig_info(sig, info, t);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n\n\treturn ret;\n}\n\n/*\n * Nuke all other threads in the group.\n */\nint zap_other_threads(struct task_struct *p)\n{\n\tstruct task_struct *t = p;\n\tint count = 0;\n\n\tp->signal->group_stop_count = 0;\n\n\twhile_each_thread(p, t) {\n\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\tcount++;\n\n\t\t/* Don't bother with already dead threads */\n\t\tif (t->exit_state)\n\t\t\tcontinue;\n\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\tsignal_wake_up(t, 1);\n\t}\n\n\treturn count;\n}\n\nstruct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t   unsigned long *flags)\n{\n\tstruct sighand_struct *sighand;\n\n\tfor (;;) {\n\t\tlocal_irq_save(*flags);\n\t\trcu_read_lock();\n\t\tsighand = rcu_dereference(tsk->sighand);\n\t\tif (unlikely(sighand == NULL)) {\n\t\t\trcu_read_unlock();\n\t\t\tlocal_irq_restore(*flags);\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_lock(&sighand->siglock);\n\t\tif (likely(sighand == tsk->sighand)) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&sighand->siglock);\n\t\trcu_read_unlock();\n\t\tlocal_irq_restore(*flags);\n\t}\n\n\treturn sighand;\n}\n\n/*\n * send signal info to all the members of a group\n */\nint group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = check_kill_permission(sig, info, p);\n\trcu_read_unlock();\n\n\tif (!ret && sig)\n\t\tret = do_send_sig_info(sig, info, p, true);\n\n\treturn ret;\n}\n\n/*\n * __kill_pgrp_info() sends a signal to a process group: this is what the tty\n * control characters do (^C, ^Z etc)\n * - the caller must hold at least a readlock on tasklist_lock\n */\nint __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp)\n{\n\tstruct task_struct *p = NULL;\n\tint retval, success;\n\n\tsuccess = 0;\n\tretval = -ESRCH;\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tint err = group_send_sig_info(sig, info, p);\n\t\tsuccess |= !err;\n\t\tretval = err;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\treturn success ? 0 : retval;\n}\n\nint kill_pid_info(int sig, struct siginfo *info, struct pid *pid)\n{\n\tint error = -ESRCH;\n\tstruct task_struct *p;\n\n\trcu_read_lock();\nretry:\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (p) {\n\t\terror = group_send_sig_info(sig, info, p);\n\t\tif (unlikely(error == -ESRCH))\n\t\t\t/*\n\t\t\t * The task was unhashed in between, try again.\n\t\t\t * If it is dead, pid_task() will return NULL,\n\t\t\t * if we race with de_thread() it will find the\n\t\t\t * new leader.\n\t\t\t */\n\t\t\tgoto retry;\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nint kill_proc_info(int sig, struct siginfo *info, pid_t pid)\n{\n\tint error;\n\trcu_read_lock();\n\terror = kill_pid_info(sig, info, find_vpid(pid));\n\trcu_read_unlock();\n\treturn error;\n}\n\nstatic int kill_as_cred_perm(const struct cred *cred,\n\t\t\t     struct task_struct *target)\n{\n\tconst struct cred *pcred = __task_cred(target);\n\tif (!uid_eq(cred->euid, pcred->suid) && !uid_eq(cred->euid, pcred->uid) &&\n\t    !uid_eq(cred->uid,  pcred->suid) && !uid_eq(cred->uid,  pcred->uid))\n\t\treturn 0;\n\treturn 1;\n}\n\n/* like kill_pid_info(), but doesn't use uid/euid of \"current\" */\nint kill_pid_info_as_cred(int sig, struct siginfo *info, struct pid *pid,\n\t\t\t const struct cred *cred, u32 secid)\n{\n\tint ret = -EINVAL;\n\tstruct task_struct *p;\n\tunsigned long flags;\n\n\tif (!valid_signal(sig))\n\t\treturn ret;\n\n\trcu_read_lock();\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (!p) {\n\t\tret = -ESRCH;\n\t\tgoto out_unlock;\n\t}\n\tif (si_fromuser(info) && !kill_as_cred_perm(cred, p)) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\tret = security_task_kill(p, info, sig, secid);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tif (sig) {\n\t\tif (lock_task_sighand(p, &flags)) {\n\t\t\tret = __send_signal(sig, info, p, 1, 0);\n\t\t\tunlock_task_sighand(p, &flags);\n\t\t} else\n\t\t\tret = -ESRCH;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kill_pid_info_as_cred);\n\n/*\n * kill_something_info() interprets pid in interesting ways just like kill(2).\n *\n * POSIX specifies that kill(-1,sig) is unspecified, but what we have\n * is probably wrong.  Should make it like BSD or SYSV.\n */\n\nstatic int kill_something_info(int sig, struct siginfo *info, pid_t pid)\n{\n\tint ret;\n\n\tif (pid > 0) {\n\t\trcu_read_lock();\n\t\tret = kill_pid_info(sig, info, find_vpid(pid));\n\t\trcu_read_unlock();\n\t\treturn ret;\n\t}\n\n\tread_lock(&tasklist_lock);\n\tif (pid != -1) {\n\t\tret = __kill_pgrp_info(sig, info,\n\t\t\t\tpid ? find_vpid(-pid) : task_pgrp(current));\n\t} else {\n\t\tint retval = 0, count = 0;\n\t\tstruct task_struct * p;\n\n\t\tfor_each_process(p) {\n\t\t\tif (task_pid_vnr(p) > 1 &&\n\t\t\t\t\t!same_thread_group(p, current)) {\n\t\t\t\tint err = group_send_sig_info(sig, info, p);\n\t\t\t\t++count;\n\t\t\t\tif (err != -EPERM)\n\t\t\t\t\tretval = err;\n\t\t\t}\n\t\t}\n\t\tret = count ? retval : -ESRCH;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * These are for backward compatibility with the rest of the kernel source.\n */\n\nint send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\t/*\n\t * Make sure legacy kernel users don't send in bad values\n\t * (normal paths check this in check_kill_permission).\n\t */\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\treturn do_send_sig_info(sig, info, p, false);\n}\n\n#define __si_special(priv) \\\n\t((priv) ? SEND_SIG_PRIV : SEND_SIG_NOINFO)\n\nint\nsend_sig(int sig, struct task_struct *p, int priv)\n{\n\treturn send_sig_info(sig, __si_special(priv), p);\n}\n\nvoid\nforce_sig(int sig, struct task_struct *p)\n{\n\tforce_sig_info(sig, SEND_SIG_PRIV, p);\n}\n\n/*\n * When things go south during signal handling, we\n * will force a SIGSEGV. And if the signal that caused\n * the problem was already a SIGSEGV, we'll want to\n * make sure we don't even try to deliver the signal..\n */\nint\nforce_sigsegv(int sig, struct task_struct *p)\n{\n\tif (sig == SIGSEGV) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&p->sighand->siglock, flags);\n\t\tp->sighand->action[sig - 1].sa.sa_handler = SIG_DFL;\n\t\tspin_unlock_irqrestore(&p->sighand->siglock, flags);\n\t}\n\tforce_sig(SIGSEGV, p);\n\treturn 0;\n}\n\nint kill_pgrp(struct pid *pid, int sig, int priv)\n{\n\tint ret;\n\n\tread_lock(&tasklist_lock);\n\tret = __kill_pgrp_info(sig, __si_special(priv), pid);\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kill_pgrp);\n\nint kill_pid(struct pid *pid, int sig, int priv)\n{\n\treturn kill_pid_info(sig, __si_special(priv), pid);\n}\nEXPORT_SYMBOL(kill_pid);\n\n/*\n * These functions support sending signals using preallocated sigqueue\n * structures.  This is needed \"because realtime applications cannot\n * afford to lose notifications of asynchronous events, like timer\n * expirations or I/O completions\".  In the case of POSIX Timers\n * we allocate the sigqueue structure from the timer_create.  If this\n * allocation fails we are able to report the failure to the application\n * with an EAGAIN error.\n */\nstruct sigqueue *sigqueue_alloc(void)\n{\n\tstruct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);\n\n\tif (q)\n\t\tq->flags |= SIGQUEUE_PREALLOC;\n\n\treturn q;\n}\n\nvoid sigqueue_free(struct sigqueue *q)\n{\n\tunsigned long flags;\n\tspinlock_t *lock = &current->sighand->siglock;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\t/*\n\t * We must hold ->siglock while testing q->list\n\t * to serialize with collect_signal() or with\n\t * __exit_signal()->flush_sigqueue().\n\t */\n\tspin_lock_irqsave(lock, flags);\n\tq->flags &= ~SIGQUEUE_PREALLOC;\n\t/*\n\t * If it is queued it will be freed when dequeued,\n\t * like the \"regular\" sigqueue.\n\t */\n\tif (!list_empty(&q->list))\n\t\tq = NULL;\n\tspin_unlock_irqrestore(lock, flags);\n\n\tif (q)\n\t\t__sigqueue_free(q);\n}\n\nint send_sigqueue(struct sigqueue *q, struct task_struct *t, int group)\n{\n\tint sig = q->info.si_signo;\n\tstruct sigpending *pending;\n\tunsigned long flags;\n\tint ret, result;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\n\tret = -1;\n\tif (!likely(lock_task_sighand(t, &flags)))\n\t\tgoto ret;\n\n\tret = 1; /* the signal is ignored */\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t, false))\n\t\tgoto out;\n\n\tret = 0;\n\tif (unlikely(!list_empty(&q->list))) {\n\t\t/*\n\t\t * If an SI_TIMER entry is already queue just increment\n\t\t * the overrun count.\n\t\t */\n\t\tBUG_ON(q->info.si_code != SI_TIMER);\n\t\tq->info.si_overrun++;\n\t\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\t\tgoto out;\n\t}\n\tq->info.si_overrun = 0;\n\n\tsignalfd_notify(t, sig);\n\tpending = group ? &t->signal->shared_pending : &t->pending;\n\tlist_add_tail(&q->list, &pending->list);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, group);\n\tresult = TRACE_SIGNAL_DELIVERED;\nout:\n\ttrace_signal_generate(sig, &q->info, t, group, result);\n\tunlock_task_sighand(t, &flags);\nret:\n\treturn ret;\n}\n\n/*\n * Let a parent know about the death of a child.\n * For a stopped/continued status change, use do_notify_parent_cldstop instead.\n *\n * Returns true if our parent ignored us and so we've switched to\n * self-reaping.\n */\nbool do_notify_parent(struct task_struct *tsk, int sig)\n{\n\tstruct siginfo info;\n\tunsigned long flags;\n\tstruct sighand_struct *psig;\n\tbool autoreap = false;\n\n\tBUG_ON(sig == -1);\n\n \t/* do_notify_parent_cldstop should have been called instead.  */\n \tBUG_ON(task_is_stopped_or_traced(tsk));\n\n\tBUG_ON(!tsk->ptrace &&\n\t       (tsk->group_leader != tsk || !thread_group_empty(tsk)));\n\n\tif (sig != SIGCHLD) {\n\t\t/*\n\t\t * This is only possible if parent == real_parent.\n\t\t * Check if it has changed security domain.\n\t\t */\n\t\tif (tsk->parent_exec_id != tsk->parent->self_exec_id)\n\t\t\tsig = SIGCHLD;\n\t}\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\t/*\n\t * We are under tasklist_lock here so our parent is tied to\n\t * us and cannot change.\n\t *\n\t * task_active_pid_ns will always return the same pid namespace\n\t * until a task passes through release_task.\n\t *\n\t * write_lock() currently calls preempt_disable() which is the\n\t * same as rcu_read_lock(), but according to Oleg, this is not\n\t * correct to rely on this\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(tsk->parent, user_ns),\n\t\t\t\t       task_uid(tsk));\n\trcu_read_unlock();\n\n\tinfo.si_utime = cputime_to_clock_t(tsk->utime + tsk->signal->utime);\n\tinfo.si_stime = cputime_to_clock_t(tsk->stime + tsk->signal->stime);\n\n\tinfo.si_status = tsk->exit_code & 0x7f;\n\tif (tsk->exit_code & 0x80)\n\t\tinfo.si_code = CLD_DUMPED;\n\telse if (tsk->exit_code & 0x7f)\n\t\tinfo.si_code = CLD_KILLED;\n\telse {\n\t\tinfo.si_code = CLD_EXITED;\n\t\tinfo.si_status = tsk->exit_code >> 8;\n\t}\n\n\tpsig = tsk->parent->sighand;\n\tspin_lock_irqsave(&psig->siglock, flags);\n\tif (!tsk->ptrace && sig == SIGCHLD &&\n\t    (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||\n\t     (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT))) {\n\t\t/*\n\t\t * We are exiting and our parent doesn't care.  POSIX.1\n\t\t * defines special semantics for setting SIGCHLD to SIG_IGN\n\t\t * or setting the SA_NOCLDWAIT flag: we should be reaped\n\t\t * automatically and not left for our parent's wait4 call.\n\t\t * Rather than having the parent do it as a magic kind of\n\t\t * signal handler, we just set this to tell do_exit that we\n\t\t * can be cleaned up without becoming a zombie.  Note that\n\t\t * we still call __wake_up_parent in this case, because a\n\t\t * blocked sys_wait4 might now return -ECHILD.\n\t\t *\n\t\t * Whether we send SIGCHLD or not for SA_NOCLDWAIT\n\t\t * is implementation-defined: we do (if you don't want\n\t\t * it, just use SIG_IGN instead).\n\t\t */\n\t\tautoreap = true;\n\t\tif (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN)\n\t\t\tsig = 0;\n\t}\n\tif (valid_signal(sig) && sig)\n\t\t__group_send_sig_info(sig, &info, tsk->parent);\n\t__wake_up_parent(tsk, tsk->parent);\n\tspin_unlock_irqrestore(&psig->siglock, flags);\n\n\treturn autoreap;\n}\n\n/**\n * do_notify_parent_cldstop - notify parent of stopped/continued state change\n * @tsk: task reporting the state change\n * @for_ptracer: the notification is for ptracer\n * @why: CLD_{CONTINUED|STOPPED|TRAPPED} to report\n *\n * Notify @tsk's parent that the stopped/continued state has changed.  If\n * @for_ptracer is %false, @tsk's group leader notifies to its real parent.\n * If %true, @tsk reports to @tsk->parent which should be the ptracer.\n *\n * CONTEXT:\n * Must be called with tasklist_lock at least read locked.\n */\nstatic void do_notify_parent_cldstop(struct task_struct *tsk,\n\t\t\t\t     bool for_ptracer, int why)\n{\n\tstruct siginfo info;\n\tunsigned long flags;\n\tstruct task_struct *parent;\n\tstruct sighand_struct *sighand;\n\n\tif (for_ptracer) {\n\t\tparent = tsk->parent;\n\t} else {\n\t\ttsk = tsk->group_leader;\n\t\tparent = tsk->real_parent;\n\t}\n\n\tinfo.si_signo = SIGCHLD;\n\tinfo.si_errno = 0;\n\t/*\n\t * see comment in do_notify_parent() about the following 4 lines\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(parent, user_ns), task_uid(tsk));\n\trcu_read_unlock();\n\n\tinfo.si_utime = cputime_to_clock_t(tsk->utime);\n\tinfo.si_stime = cputime_to_clock_t(tsk->stime);\n\n \tinfo.si_code = why;\n \tswitch (why) {\n \tcase CLD_CONTINUED:\n \t\tinfo.si_status = SIGCONT;\n \t\tbreak;\n \tcase CLD_STOPPED:\n \t\tinfo.si_status = tsk->signal->group_exit_code & 0x7f;\n \t\tbreak;\n \tcase CLD_TRAPPED:\n \t\tinfo.si_status = tsk->exit_code & 0x7f;\n \t\tbreak;\n \tdefault:\n \t\tBUG();\n \t}\n\n\tsighand = parent->sighand;\n\tspin_lock_irqsave(&sighand->siglock, flags);\n\tif (sighand->action[SIGCHLD-1].sa.sa_handler != SIG_IGN &&\n\t    !(sighand->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))\n\t\t__group_send_sig_info(SIGCHLD, &info, parent);\n\t/*\n\t * Even if SIGCHLD is not generated, we must wake up wait4 calls.\n\t */\n\t__wake_up_parent(tsk, parent);\n\tspin_unlock_irqrestore(&sighand->siglock, flags);\n}\n\nstatic inline int may_ptrace_stop(void)\n{\n\tif (!likely(current->ptrace))\n\t\treturn 0;\n\t/*\n\t * Are we in the middle of do_coredump?\n\t * If so and our tracer is also part of the coredump stopping\n\t * is a deadlock situation, and pointless because our tracer\n\t * is dead so don't allow us to stop.\n\t * If SIGKILL was already sent before the caller unlocked\n\t * ->siglock we must see ->core_state != NULL. Otherwise it\n\t * is safe to enter schedule().\n\t *\n\t * This is almost outdated, a task with the pending SIGKILL can't\n\t * block in TASK_TRACED. But PTRACE_EVENT_EXIT can be reported\n\t * after SIGKILL was already dequeued.\n\t */\n\tif (unlikely(current->mm->core_state) &&\n\t    unlikely(current->mm == current->parent->mm))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * Return non-zero if there is a SIGKILL that should be waking us up.\n * Called with the siglock held.\n */\nstatic int sigkill_pending(struct task_struct *tsk)\n{\n\treturn\tsigismember(&tsk->pending.signal, SIGKILL) ||\n\t\tsigismember(&tsk->signal->shared_pending.signal, SIGKILL);\n}\n\n/*\n * This must be called with current->sighand->siglock held.\n *\n * This should be the path for all ptrace stops.\n * We always set current->last_siginfo while stopped here.\n * That makes it a way to test a stopped process for\n * being ptrace-stopped vs being job-control-stopped.\n *\n * If we actually decide not to stop at all because the tracer\n * is gone, we keep current->exit_code unless clear_code.\n */\nstatic void ptrace_stop(int exit_code, int why, int clear_code, siginfo_t *info)\n\t__releases(&current->sighand->siglock)\n\t__acquires(&current->sighand->siglock)\n{\n\tbool gstop_done = false;\n\n\tif (arch_ptrace_stop_needed(exit_code, info)) {\n\t\t/*\n\t\t * The arch code has something special to do before a\n\t\t * ptrace stop.  This is allowed to block, e.g. for faults\n\t\t * on user stack pages.  We can't keep the siglock while\n\t\t * calling arch_ptrace_stop, so we must release it now.\n\t\t * To preserve proper semantics, we must do this before\n\t\t * any signal bookkeeping like checking group_stop_count.\n\t\t * Meanwhile, a SIGKILL could come in before we retake the\n\t\t * siglock.  That must prevent us from sleeping in TASK_TRACED.\n\t\t * So after regaining the lock, we must check for SIGKILL.\n\t\t */\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tarch_ptrace_stop(exit_code, info);\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\tif (sigkill_pending(current))\n\t\t\treturn;\n\t}\n\n\t/*\n\t * We're committing to trapping.  TRACED should be visible before\n\t * TRAPPING is cleared; otherwise, the tracer might fail do_wait().\n\t * Also, transition to TRACED and updates to ->jobctl should be\n\t * atomic with respect to siglock and should be done after the arch\n\t * hook as siglock is released and regrabbed across it.\n\t */\n\tset_current_state(TASK_TRACED);\n\n\tcurrent->last_siginfo = info;\n\tcurrent->exit_code = exit_code;\n\n\t/*\n\t * If @why is CLD_STOPPED, we're trapping to participate in a group\n\t * stop.  Do the bookkeeping.  Note that if SIGCONT was delievered\n\t * across siglock relocks since INTERRUPT was scheduled, PENDING\n\t * could be clear now.  We act as if SIGCONT is received after\n\t * TASK_TRACED is entered - ignore it.\n\t */\n\tif (why == CLD_STOPPED && (current->jobctl & JOBCTL_STOP_PENDING))\n\t\tgstop_done = task_participate_group_stop(current);\n\n\t/* any trap clears pending STOP trap, STOP trap clears NOTIFY */\n\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\tif (info && info->si_code >> 8 == PTRACE_EVENT_STOP)\n\t\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_NOTIFY);\n\n\t/* entering a trap, clear TRAPPING */\n\ttask_clear_jobctl_trapping(current);\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\tread_lock(&tasklist_lock);\n\tif (may_ptrace_stop()) {\n\t\t/*\n\t\t * Notify parents of the stop.\n\t\t *\n\t\t * While ptraced, there are two parents - the ptracer and\n\t\t * the real_parent of the group_leader.  The ptracer should\n\t\t * know about every stop while the real parent is only\n\t\t * interested in the completion of group stop.  The states\n\t\t * for the two don't interact with each other.  Notify\n\t\t * separately unless they're gonna be duplicates.\n\t\t */\n\t\tdo_notify_parent_cldstop(current, true, why);\n\t\tif (gstop_done && ptrace_reparented(current))\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/*\n\t\t * Don't want to allow preemption here, because\n\t\t * sys_ptrace() needs this task to be inactive.\n\t\t *\n\t\t * XXX: implement read_unlock_no_resched().\n\t\t */\n\t\tpreempt_disable();\n\t\tread_unlock(&tasklist_lock);\n\t\tpreempt_enable_no_resched();\n\t\tfreezable_schedule();\n\t} else {\n\t\t/*\n\t\t * By the time we got the lock, our tracer went away.\n\t\t * Don't drop the lock yet, another tracer may come.\n\t\t *\n\t\t * If @gstop_done, the ptracer went away between group stop\n\t\t * completion and here.  During detach, it would have set\n\t\t * JOBCTL_STOP_PENDING on us and we'll re-enter\n\t\t * TASK_STOPPED in do_signal_stop() on return, so notifying\n\t\t * the real parent of the group stop completion is enough.\n\t\t */\n\t\tif (gstop_done)\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/* tasklist protects us from ptrace_freeze_traced() */\n\t\t__set_current_state(TASK_RUNNING);\n\t\tif (clear_code)\n\t\t\tcurrent->exit_code = 0;\n\t\tread_unlock(&tasklist_lock);\n\t}\n\n\t/*\n\t * We are back.  Now reacquire the siglock before touching\n\t * last_siginfo, so that we are sure to have synchronized with\n\t * any signal-sending on another CPU that wants to examine it.\n\t */\n\tspin_lock_irq(&current->sighand->siglock);\n\tcurrent->last_siginfo = NULL;\n\n\t/* LISTENING can be set only during STOP traps, clear it */\n\tcurrent->jobctl &= ~JOBCTL_LISTENING;\n\n\t/*\n\t * Queued signals ignored us while we were stopped for tracing.\n\t * So check for any that we should take before resuming user mode.\n\t * This sets TIF_SIGPENDING, but never clears it.\n\t */\n\trecalc_sigpending_tsk(current);\n}\n\nstatic void ptrace_do_notify(int signr, int exit_code, int why)\n{\n\tsiginfo_t info;\n\n\tmemset(&info, 0, sizeof info);\n\tinfo.si_signo = signr;\n\tinfo.si_code = exit_code;\n\tinfo.si_pid = task_pid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\t/* Let the debugger run.  */\n\tptrace_stop(exit_code, why, 1, &info);\n}\n\nvoid ptrace_notify(int exit_code)\n{\n\tBUG_ON((exit_code & (0x7f | ~0xffff)) != SIGTRAP);\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tptrace_do_notify(SIGTRAP, exit_code, CLD_TRAPPED);\n\tspin_unlock_irq(&current->sighand->siglock);\n}\n\n/**\n * do_signal_stop - handle group stop for SIGSTOP and other stop signals\n * @signr: signr causing group stop if initiating\n *\n * If %JOBCTL_STOP_PENDING is not set yet, initiate group stop with @signr\n * and participate in it.  If already set, participate in the existing\n * group stop.  If participated in a group stop (and thus slept), %true is\n * returned with siglock released.\n *\n * If ptraced, this function doesn't handle stop itself.  Instead,\n * %JOBCTL_TRAP_STOP is scheduled and %false is returned with siglock\n * untouched.  The caller must ensure that INTERRUPT trap handling takes\n * places afterwards.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which is released\n * on %true return.\n *\n * RETURNS:\n * %false if group stop is already cancelled or ptrace trap is scheduled.\n * %true if participated in group stop.\n */\nstatic bool do_signal_stop(int signr)\n\t__releases(&current->sighand->siglock)\n{\n\tstruct signal_struct *sig = current->signal;\n\n\tif (!(current->jobctl & JOBCTL_STOP_PENDING)) {\n\t\tunsigned int gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\n\t\tstruct task_struct *t;\n\n\t\t/* signr will be recorded in task->jobctl for retries */\n\t\tWARN_ON_ONCE(signr & ~JOBCTL_STOP_SIGMASK);\n\n\t\tif (!likely(current->jobctl & JOBCTL_STOP_DEQUEUED) ||\n\t\t    unlikely(signal_group_exit(sig)))\n\t\t\treturn false;\n\t\t/*\n\t\t * There is no group stop already in progress.  We must\n\t\t * initiate one now.\n\t\t *\n\t\t * While ptraced, a task may be resumed while group stop is\n\t\t * still in effect and then receive a stop signal and\n\t\t * initiate another group stop.  This deviates from the\n\t\t * usual behavior as two consecutive stop signals can't\n\t\t * cause two group stops when !ptraced.  That is why we\n\t\t * also check !task_is_stopped(t) below.\n\t\t *\n\t\t * The condition can be distinguished by testing whether\n\t\t * SIGNAL_STOP_STOPPED is already set.  Don't generate\n\t\t * group_exit_code in such case.\n\t\t *\n\t\t * This is not necessary for SIGNAL_STOP_CONTINUED because\n\t\t * an intervening stop signal is required to cause two\n\t\t * continued events regardless of ptrace.\n\t\t */\n\t\tif (!(sig->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsig->group_exit_code = signr;\n\n\t\tsig->group_stop_count = 0;\n\n\t\tif (task_set_jobctl_pending(current, signr | gstop))\n\t\t\tsig->group_stop_count++;\n\n\t\tfor (t = next_thread(current); t != current;\n\t\t     t = next_thread(t)) {\n\t\t\t/*\n\t\t\t * Setting state to TASK_STOPPED for a group\n\t\t\t * stop is always done with the siglock held,\n\t\t\t * so this check has no races.\n\t\t\t */\n\t\t\tif (!task_is_stopped(t) &&\n\t\t\t    task_set_jobctl_pending(t, signr | gstop)) {\n\t\t\t\tsig->group_stop_count++;\n\t\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\t\tsignal_wake_up(t, 0);\n\t\t\t\telse\n\t\t\t\t\tptrace_trap_notify(t);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (likely(!current->ptrace)) {\n\t\tint notify = 0;\n\n\t\t/*\n\t\t * If there are no other threads in the group, or if there\n\t\t * is a group stop in progress and we are the last to stop,\n\t\t * report to the parent.\n\t\t */\n\t\tif (task_participate_group_stop(current))\n\t\t\tnotify = CLD_STOPPED;\n\n\t\t__set_current_state(TASK_STOPPED);\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent of the group stop completion.  Because\n\t\t * we're not holding either the siglock or tasklist_lock\n\t\t * here, ptracer may attach inbetween; however, this is for\n\t\t * group stop and should always be delivered to the real\n\t\t * parent of the group leader.  The new ptracer will get\n\t\t * its notification when this task transitions into\n\t\t * TASK_TRACED.\n\t\t */\n\t\tif (notify) {\n\t\t\tread_lock(&tasklist_lock);\n\t\t\tdo_notify_parent_cldstop(current, false, notify);\n\t\t\tread_unlock(&tasklist_lock);\n\t\t}\n\n\t\t/* Now we don't run again until woken by SIGCONT or SIGKILL */\n\t\tfreezable_schedule();\n\t\treturn true;\n\t} else {\n\t\t/*\n\t\t * While ptraced, group stop is handled by STOP trap.\n\t\t * Schedule it and let the caller deal with it.\n\t\t */\n\t\ttask_set_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\t\treturn false;\n\t}\n}\n\n/**\n * do_jobctl_trap - take care of ptrace jobctl traps\n *\n * When PT_SEIZED, it's used for both group stop and explicit\n * SEIZE/INTERRUPT traps.  Both generate PTRACE_EVENT_STOP trap with\n * accompanying siginfo.  If stopped, lower eight bits of exit_code contain\n * the stop signal; otherwise, %SIGTRAP.\n *\n * When !PT_SEIZED, it's used only for group stop trap with stop signal\n * number as exit_code and no siginfo.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which may be\n * released and re-acquired before returning with intervening sleep.\n */\nstatic void do_jobctl_trap(void)\n{\n\tstruct signal_struct *signal = current->signal;\n\tint signr = current->jobctl & JOBCTL_STOP_SIGMASK;\n\n\tif (current->ptrace & PT_SEIZED) {\n\t\tif (!signal->group_stop_count &&\n\t\t    !(signal->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsignr = SIGTRAP;\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_do_notify(signr, signr | (PTRACE_EVENT_STOP << 8),\n\t\t\t\t CLD_STOPPED);\n\t} else {\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_stop(signr, CLD_STOPPED, 0, NULL);\n\t\tcurrent->exit_code = 0;\n\t}\n}\n\nstatic int ptrace_signal(int signr, siginfo_t *info)\n{\n\tptrace_signal_deliver();\n\t/*\n\t * We do not check sig_kernel_stop(signr) but set this marker\n\t * unconditionally because we do not know whether debugger will\n\t * change signr. This flag has no meaning unless we are going\n\t * to stop after return from ptrace_stop(). In this case it will\n\t * be checked in do_signal_stop(), we should only stop if it was\n\t * not cleared by SIGCONT while we were sleeping. See also the\n\t * comment in dequeue_signal().\n\t */\n\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\tptrace_stop(signr, CLD_TRAPPED, 0, info);\n\n\t/* We're back.  Did the debugger cancel the sig?  */\n\tsignr = current->exit_code;\n\tif (signr == 0)\n\t\treturn signr;\n\n\tcurrent->exit_code = 0;\n\n\t/*\n\t * Update the siginfo structure if the signal has\n\t * changed.  If the debugger wanted something\n\t * specific in the siginfo structure then it should\n\t * have updated *info via PTRACE_SETSIGINFO.\n\t */\n\tif (signr != info->si_signo) {\n\t\tinfo->si_signo = signr;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\trcu_read_lock();\n\t\tinfo->si_pid = task_pid_vnr(current->parent);\n\t\tinfo->si_uid = from_kuid_munged(current_user_ns(),\n\t\t\t\t\t\ttask_uid(current->parent));\n\t\trcu_read_unlock();\n\t}\n\n\t/* If the (new) signal is now blocked, requeue it.  */\n\tif (sigismember(&current->blocked, signr)) {\n\t\tspecific_send_sig_info(signr, info, current);\n\t\tsignr = 0;\n\t}\n\n\treturn signr;\n}\n\nint get_signal_to_deliver(siginfo_t *info, struct k_sigaction *return_ka,\n\t\t\t  struct pt_regs *regs, void *cookie)\n{\n\tstruct sighand_struct *sighand = current->sighand;\n\tstruct signal_struct *signal = current->signal;\n\tint signr;\n\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tif (unlikely(uprobe_deny_signal()))\n\t\treturn 0;\n\n\t/*\n\t * Do this once, we can't return to user-mode if freezing() == T.\n\t * do_signal_stop() and ptrace_stop() do freezable_schedule() and\n\t * thus do not need another check after return.\n\t */\n\ttry_to_freeze();\n\nrelock:\n\tspin_lock_irq(&sighand->siglock);\n\t/*\n\t * Every stopped thread goes here after wakeup. Check to see if\n\t * we should notify the parent, prepare_signal(SIGCONT) encodes\n\t * the CLD_ si_code into SIGNAL_CLD_MASK bits.\n\t */\n\tif (unlikely(signal->flags & SIGNAL_CLD_MASK)) {\n\t\tint why;\n\n\t\tif (signal->flags & SIGNAL_CLD_CONTINUED)\n\t\t\twhy = CLD_CONTINUED;\n\t\telse\n\t\t\twhy = CLD_STOPPED;\n\n\t\tsignal->flags &= ~SIGNAL_CLD_MASK;\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent that we're continuing.  This event is\n\t\t * always per-process and doesn't make whole lot of sense\n\t\t * for ptracers, who shouldn't consume the state via\n\t\t * wait(2) either, but, for backward compatibility, notify\n\t\t * the ptracer of the group leader too unless it's gonna be\n\t\t * a duplicate.\n\t\t */\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\tif (ptrace_reparented(current->group_leader))\n\t\t\tdo_notify_parent_cldstop(current->group_leader,\n\t\t\t\t\t\ttrue, why);\n\t\tread_unlock(&tasklist_lock);\n\n\t\tgoto relock;\n\t}\n\n\tfor (;;) {\n\t\tstruct k_sigaction *ka;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_STOP_PENDING) &&\n\t\t    do_signal_stop(0))\n\t\t\tgoto relock;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_TRAP_MASK)) {\n\t\t\tdo_jobctl_trap();\n\t\t\tspin_unlock_irq(&sighand->siglock);\n\t\t\tgoto relock;\n\t\t}\n\n\t\tsignr = dequeue_signal(current, &current->blocked, info);\n\n\t\tif (!signr)\n\t\t\tbreak; /* will return 0 */\n\n\t\tif (unlikely(current->ptrace) && signr != SIGKILL) {\n\t\t\tsignr = ptrace_signal(signr, info);\n\t\t\tif (!signr)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tka = &sighand->action[signr-1];\n\n\t\t/* Trace actually delivered signals. */\n\t\ttrace_signal_deliver(signr, info, ka);\n\n\t\tif (ka->sa.sa_handler == SIG_IGN) /* Do nothing.  */\n\t\t\tcontinue;\n\t\tif (ka->sa.sa_handler != SIG_DFL) {\n\t\t\t/* Run the handler.  */\n\t\t\t*return_ka = *ka;\n\n\t\t\tif (ka->sa.sa_flags & SA_ONESHOT)\n\t\t\t\tka->sa.sa_handler = SIG_DFL;\n\n\t\t\tbreak; /* will return non-zero \"signr\" value */\n\t\t}\n\n\t\t/*\n\t\t * Now we are doing the default action for this signal.\n\t\t */\n\t\tif (sig_kernel_ignore(signr)) /* Default is nothing. */\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Global init gets no signals it doesn't want.\n\t\t * Container-init gets no signals it doesn't want from same\n\t\t * container.\n\t\t *\n\t\t * Note that if global/container-init sees a sig_kernel_only()\n\t\t * signal here, the signal must have been generated internally\n\t\t * or must have come from an ancestor namespace. In either\n\t\t * case, the signal cannot be dropped.\n\t\t */\n\t\tif (unlikely(signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\t\t!sig_kernel_only(signr))\n\t\t\tcontinue;\n\n\t\tif (sig_kernel_stop(signr)) {\n\t\t\t/*\n\t\t\t * The default action is to stop all threads in\n\t\t\t * the thread group.  The job control signals\n\t\t\t * do nothing in an orphaned pgrp, but SIGSTOP\n\t\t\t * always works.  Note that siglock needs to be\n\t\t\t * dropped during the call to is_orphaned_pgrp()\n\t\t\t * because of lock ordering with tasklist_lock.\n\t\t\t * This allows an intervening SIGCONT to be posted.\n\t\t\t * We need to check for that and bail out if necessary.\n\t\t\t */\n\t\t\tif (signr != SIGSTOP) {\n\t\t\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t\t\t/* signals can be posted during this window */\n\n\t\t\t\tif (is_current_pgrp_orphaned())\n\t\t\t\t\tgoto relock;\n\n\t\t\t\tspin_lock_irq(&sighand->siglock);\n\t\t\t}\n\n\t\t\tif (likely(do_signal_stop(info->si_signo))) {\n\t\t\t\t/* It released the siglock.  */\n\t\t\t\tgoto relock;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We didn't actually stop, due to a race\n\t\t\t * with SIGCONT or something like that.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Anything else is fatal, maybe with a core dump.\n\t\t */\n\t\tcurrent->flags |= PF_SIGNALED;\n\n\t\tif (sig_kernel_coredump(signr)) {\n\t\t\tif (print_fatal_signals)\n\t\t\t\tprint_fatal_signal(info->si_signo);\n\t\t\t/*\n\t\t\t * If it was able to dump core, this kills all\n\t\t\t * other threads in the group and synchronizes with\n\t\t\t * their demise.  If we lost the race with another\n\t\t\t * thread getting here, it set group_exit_code\n\t\t\t * first and our do_group_exit call below will use\n\t\t\t * that value and ignore the one we pass it.\n\t\t\t */\n\t\t\tdo_coredump(info);\n\t\t}\n\n\t\t/*\n\t\t * Death signals, no core dump.\n\t\t */\n\t\tdo_group_exit(info->si_signo);\n\t\t/* NOTREACHED */\n\t}\n\tspin_unlock_irq(&sighand->siglock);\n\treturn signr;\n}\n\n/**\n * signal_delivered - \n * @sig:\t\tnumber of signal being delivered\n * @info:\t\tsiginfo_t of signal being delivered\n * @ka:\t\t\tsigaction setting that chose the handler\n * @regs:\t\tuser register state\n * @stepping:\t\tnonzero if debugger single-step or block-step in use\n *\n * This function should be called when a signal has succesfully been\n * delivered. It updates the blocked signals accordingly (@ka->sa.sa_mask\n * is always blocked, and the signal itself is blocked unless %SA_NODEFER\n * is set in @ka->sa.sa_flags.  Tracing is notified.\n */\nvoid signal_delivered(int sig, siginfo_t *info, struct k_sigaction *ka,\n\t\t\tstruct pt_regs *regs, int stepping)\n{\n\tsigset_t blocked;\n\n\t/* A signal was successfully delivered, and the\n\t   saved sigmask was stored on the signal frame,\n\t   and will be restored by sigreturn.  So we can\n\t   simply clear the restore sigmask flag.  */\n\tclear_restore_sigmask();\n\n\tsigorsets(&blocked, &current->blocked, &ka->sa.sa_mask);\n\tif (!(ka->sa.sa_flags & SA_NODEFER))\n\t\tsigaddset(&blocked, sig);\n\tset_current_blocked(&blocked);\n\ttracehook_signal_handler(sig, info, ka, regs, stepping);\n}\n\n/*\n * It could be that complete_signal() picked us to notify about the\n * group-wide signal. Other threads should be notified now to take\n * the shared signals in @which since we will not.\n */\nstatic void retarget_shared_pending(struct task_struct *tsk, sigset_t *which)\n{\n\tsigset_t retarget;\n\tstruct task_struct *t;\n\n\tsigandsets(&retarget, &tsk->signal->shared_pending.signal, which);\n\tif (sigisemptyset(&retarget))\n\t\treturn;\n\n\tt = tsk;\n\twhile_each_thread(tsk, t) {\n\t\tif (t->flags & PF_EXITING)\n\t\t\tcontinue;\n\n\t\tif (!has_pending_signals(&retarget, &t->blocked))\n\t\t\tcontinue;\n\t\t/* Remove the signals this thread can handle. */\n\t\tsigandsets(&retarget, &retarget, &t->blocked);\n\n\t\tif (!signal_pending(t))\n\t\t\tsignal_wake_up(t, 0);\n\n\t\tif (sigisemptyset(&retarget))\n\t\t\tbreak;\n\t}\n}\n\nvoid exit_signals(struct task_struct *tsk)\n{\n\tint group_stop = 0;\n\tsigset_t unblocked;\n\n\t/*\n\t * @tsk is about to have PF_EXITING set - lock out users which\n\t * expect stable threadgroup.\n\t */\n\tthreadgroup_change_begin(tsk);\n\n\tif (thread_group_empty(tsk) || signal_group_exit(tsk->signal)) {\n\t\ttsk->flags |= PF_EXITING;\n\t\tthreadgroup_change_end(tsk);\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t/*\n\t * From now this task is not visible for group-wide signals,\n\t * see wants_signal(), do_signal_stop().\n\t */\n\ttsk->flags |= PF_EXITING;\n\n\tthreadgroup_change_end(tsk);\n\n\tif (!signal_pending(tsk))\n\t\tgoto out;\n\n\tunblocked = tsk->blocked;\n\tsignotset(&unblocked);\n\tretarget_shared_pending(tsk, &unblocked);\n\n\tif (unlikely(tsk->jobctl & JOBCTL_STOP_PENDING) &&\n\t    task_participate_group_stop(tsk))\n\t\tgroup_stop = CLD_STOPPED;\nout:\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t/*\n\t * If group stop has completed, deliver the notification.  This\n\t * should always go to the real parent of the group leader.\n\t */\n\tif (unlikely(group_stop)) {\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(tsk, false, group_stop);\n\t\tread_unlock(&tasklist_lock);\n\t}\n}\n\nEXPORT_SYMBOL(recalc_sigpending);\nEXPORT_SYMBOL_GPL(dequeue_signal);\nEXPORT_SYMBOL(flush_signals);\nEXPORT_SYMBOL(force_sig);\nEXPORT_SYMBOL(send_sig);\nEXPORT_SYMBOL(send_sig_info);\nEXPORT_SYMBOL(sigprocmask);\nEXPORT_SYMBOL(block_all_signals);\nEXPORT_SYMBOL(unblock_all_signals);\n\n\n/*\n * System call entry points.\n */\n\n/**\n *  sys_restart_syscall - restart a system call\n */\nSYSCALL_DEFINE0(restart_syscall)\n{\n\tstruct restart_block *restart = &current_thread_info()->restart_block;\n\treturn restart->fn(restart);\n}\n\nlong do_no_restart_syscall(struct restart_block *param)\n{\n\treturn -EINTR;\n}\n\nstatic void __set_task_blocked(struct task_struct *tsk, const sigset_t *newset)\n{\n\tif (signal_pending(tsk) && !thread_group_empty(tsk)) {\n\t\tsigset_t newblocked;\n\t\t/* A set of now blocked but previously unblocked signals. */\n\t\tsigandnsets(&newblocked, newset, &current->blocked);\n\t\tretarget_shared_pending(tsk, &newblocked);\n\t}\n\ttsk->blocked = *newset;\n\trecalc_sigpending();\n}\n\n/**\n * set_current_blocked - change current->blocked mask\n * @newset: new mask\n *\n * It is wrong to change ->blocked directly, this helper should be used\n * to ensure the process can't miss a shared signal we are going to block.\n */\nvoid set_current_blocked(sigset_t *newset)\n{\n\tsigdelsetmask(newset, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t__set_current_blocked(newset);\n}\n\nvoid __set_current_blocked(const sigset_t *newset)\n{\n\tstruct task_struct *tsk = current;\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t__set_task_blocked(tsk, newset);\n\tspin_unlock_irq(&tsk->sighand->siglock);\n}\n\n/*\n * This is also useful for kernel threads that want to temporarily\n * (or permanently) block certain signals.\n *\n * NOTE! Unlike the user-mode sys_sigprocmask(), the kernel\n * interface happily blocks \"unblockable\" signals like SIGKILL\n * and friends.\n */\nint sigprocmask(int how, sigset_t *set, sigset_t *oldset)\n{\n\tstruct task_struct *tsk = current;\n\tsigset_t newset;\n\n\t/* Lockless, only current can change ->blocked, never from irq */\n\tif (oldset)\n\t\t*oldset = tsk->blocked;\n\n\tswitch (how) {\n\tcase SIG_BLOCK:\n\t\tsigorsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_UNBLOCK:\n\t\tsigandnsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_SETMASK:\n\t\tnewset = *set;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t__set_current_blocked(&newset);\n\treturn 0;\n}\n\n/**\n *  sys_rt_sigprocmask - change the list of currently blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: stores pending signals\n *  @oset: previous value of signal mask if non-null\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigprocmask, int, how, sigset_t __user *, nset,\n\t\tsigset_t __user *, oset, size_t, sigsetsize)\n{\n\tsigset_t old_set, new_set;\n\tint error;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\told_set = current->blocked;\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nlong do_sigpending(void __user *set, unsigned long sigsetsize)\n{\n\tlong error = -EINVAL;\n\tsigset_t pending;\n\n\tif (sigsetsize > sizeof(sigset_t))\n\t\tgoto out;\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tsigorsets(&pending, &current->pending.signal,\n\t\t  &current->signal->shared_pending.signal);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\t/* Outside the lock because only this thread touches it.  */\n\tsigandsets(&pending, &current->blocked, &pending);\n\n\terror = -EFAULT;\n\tif (!copy_to_user(set, &pending, sigsetsize))\n\t\terror = 0;\n\nout:\n\treturn error;\n}\n\n/**\n *  sys_rt_sigpending - examine a pending signal that has been raised\n *\t\t\twhile blocked\n *  @set: stores pending signals\n *  @sigsetsize: size of sigset_t type or larger\n */\nSYSCALL_DEFINE2(rt_sigpending, sigset_t __user *, set, size_t, sigsetsize)\n{\n\treturn do_sigpending(set, sigsetsize);\n}\n\n#ifndef HAVE_ARCH_COPY_SIGINFO_TO_USER\n\nint copy_siginfo_to_user(siginfo_t __user *to, siginfo_t *from)\n{\n\tint err;\n\n\tif (!access_ok (VERIFY_WRITE, to, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\tif (from->si_code < 0)\n\t\treturn __copy_to_user(to, from, sizeof(siginfo_t))\n\t\t\t? -EFAULT : 0;\n\t/*\n\t * If you change siginfo_t structure, please be sure\n\t * this code is fixed accordingly.\n\t * Please remember to update the signalfd_copyinfo() function\n\t * inside fs/signalfd.c too, in case siginfo_t changes.\n\t * It should never copy any pad contained in the structure\n\t * to avoid security leaks, but must copy the generic\n\t * 3 ints plus the relevant union member.\n\t */\n\terr = __put_user(from->si_signo, &to->si_signo);\n\terr |= __put_user(from->si_errno, &to->si_errno);\n\terr |= __put_user((short)from->si_code, &to->si_code);\n\tswitch (from->si_code & __SI_MASK) {\n\tcase __SI_KILL:\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\tbreak;\n\tcase __SI_TIMER:\n\t\t err |= __put_user(from->si_tid, &to->si_tid);\n\t\t err |= __put_user(from->si_overrun, &to->si_overrun);\n\t\t err |= __put_user(from->si_ptr, &to->si_ptr);\n\t\tbreak;\n\tcase __SI_POLL:\n\t\terr |= __put_user(from->si_band, &to->si_band);\n\t\terr |= __put_user(from->si_fd, &to->si_fd);\n\t\tbreak;\n\tcase __SI_FAULT:\n\t\terr |= __put_user(from->si_addr, &to->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\terr |= __put_user(from->si_trapno, &to->si_trapno);\n#endif\n#ifdef BUS_MCEERR_AO\n\t\t/*\n\t\t * Other callers might not initialize the si_lsb field,\n\t\t * so check explicitly for the right codes here.\n\t\t */\n\t\tif (from->si_code == BUS_MCEERR_AR || from->si_code == BUS_MCEERR_AO)\n\t\t\terr |= __put_user(from->si_addr_lsb, &to->si_addr_lsb);\n#endif\n\t\tbreak;\n\tcase __SI_CHLD:\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\terr |= __put_user(from->si_status, &to->si_status);\n\t\terr |= __put_user(from->si_utime, &to->si_utime);\n\t\terr |= __put_user(from->si_stime, &to->si_stime);\n\t\tbreak;\n\tcase __SI_RT: /* This is not generated by the kernel as of now. */\n\tcase __SI_MESGQ: /* But this is */\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\terr |= __put_user(from->si_ptr, &to->si_ptr);\n\t\tbreak;\n#ifdef __ARCH_SIGSYS\n\tcase __SI_SYS:\n\t\terr |= __put_user(from->si_call_addr, &to->si_call_addr);\n\t\terr |= __put_user(from->si_syscall, &to->si_syscall);\n\t\terr |= __put_user(from->si_arch, &to->si_arch);\n\t\tbreak;\n#endif\n\tdefault: /* this is just in case for now ... */\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\tbreak;\n\t}\n\treturn err;\n}\n\n#endif\n\n/**\n *  do_sigtimedwait - wait for queued signals specified in @which\n *  @which: queued signals to wait for\n *  @info: if non-null, the signal's siginfo is returned here\n *  @ts: upper bound on process time suspension\n */\nint do_sigtimedwait(const sigset_t *which, siginfo_t *info,\n\t\t\tconst struct timespec *ts)\n{\n\tstruct task_struct *tsk = current;\n\tlong timeout = MAX_SCHEDULE_TIMEOUT;\n\tsigset_t mask = *which;\n\tint sig;\n\n\tif (ts) {\n\t\tif (!timespec_valid(ts))\n\t\t\treturn -EINVAL;\n\t\ttimeout = timespec_to_jiffies(ts);\n\t\t/*\n\t\t * We can be close to the next tick, add another one\n\t\t * to ensure we will wait at least the time asked for.\n\t\t */\n\t\tif (ts->tv_sec || ts->tv_nsec)\n\t\t\ttimeout++;\n\t}\n\n\t/*\n\t * Invert the set of allowed signals to get those we want to block.\n\t */\n\tsigdelsetmask(&mask, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\tsignotset(&mask);\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\tsig = dequeue_signal(tsk, &mask, info);\n\tif (!sig && timeout) {\n\t\t/*\n\t\t * None ready, temporarily unblock those we're interested\n\t\t * while we are sleeping in so that we'll be awakened when\n\t\t * they arrive. Unblocking is always fine, we can avoid\n\t\t * set_current_blocked().\n\t\t */\n\t\ttsk->real_blocked = tsk->blocked;\n\t\tsigandsets(&tsk->blocked, &tsk->blocked, &mask);\n\t\trecalc_sigpending();\n\t\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t\ttimeout = schedule_timeout_interruptible(timeout);\n\n\t\tspin_lock_irq(&tsk->sighand->siglock);\n\t\t__set_task_blocked(tsk, &tsk->real_blocked);\n\t\tsiginitset(&tsk->real_blocked, 0);\n\t\tsig = dequeue_signal(tsk, &mask, info);\n\t}\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\tif (sig)\n\t\treturn sig;\n\treturn timeout ? -EINTR : -EAGAIN;\n}\n\n/**\n *  sys_rt_sigtimedwait - synchronously wait for queued signals specified\n *\t\t\tin @uthese\n *  @uthese: queued signals to wait for\n *  @uinfo: if non-null, the signal's siginfo is returned here\n *  @uts: upper bound on process time suspension\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigtimedwait, const sigset_t __user *, uthese,\n\t\tsiginfo_t __user *, uinfo, const struct timespec __user *, uts,\n\t\tsize_t, sigsetsize)\n{\n\tsigset_t these;\n\tstruct timespec ts;\n\tsiginfo_t info;\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&these, uthese, sizeof(these)))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (copy_from_user(&ts, uts, sizeof(ts)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n\n/**\n *  sys_kill - send a signal to a process\n *  @pid: the PID of the process\n *  @sig: signal to be sent\n */\nSYSCALL_DEFINE2(kill, pid_t, pid, int, sig)\n{\n\tstruct siginfo info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_USER;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn kill_something_info(sig, &info, pid);\n}\n\nstatic int\ndo_send_specific(pid_t tgid, pid_t pid, int sig, struct siginfo *info)\n{\n\tstruct task_struct *p;\n\tint error = -ESRCH;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p && (tgid <= 0 || task_tgid_vnr(p) == tgid)) {\n\t\terror = check_kill_permission(sig, info, p);\n\t\t/*\n\t\t * The null signal is a permissions and process existence\n\t\t * probe.  No signal is actually delivered.\n\t\t */\n\t\tif (!error && sig) {\n\t\t\terror = do_send_sig_info(sig, info, p, false);\n\t\t\t/*\n\t\t\t * If lock_task_sighand() failed we pretend the task\n\t\t\t * dies after receiving the signal. The window is tiny,\n\t\t\t * and the signal is private anyway.\n\t\t\t */\n\t\t\tif (unlikely(error == -ESRCH))\n\t\t\t\terror = 0;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nstatic int do_tkill(pid_t tgid, pid_t pid, int sig)\n{\n\tstruct siginfo info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_TKILL;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn do_send_specific(tgid, pid, sig, &info);\n}\n\n/**\n *  sys_tgkill - send signal to one specific thread\n *  @tgid: the thread group ID of the thread\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *\n *  This syscall also checks the @tgid and returns -ESRCH even if the PID\n *  exists but it's not belonging to the target process anymore. This\n *  method solves the problem of threads exiting and PIDs getting reused.\n */\nSYSCALL_DEFINE3(tgkill, pid_t, tgid, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(tgid, pid, sig);\n}\n\n/**\n *  sys_tkill - send signal to one specific task\n *  @pid: the PID of the task\n *  @sig: signal to be sent\n *\n *  Send a signal to only one task, even if it's a CLONE_THREAD task.\n */\nSYSCALL_DEFINE2(tkill, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(0, pid, sig);\n}\n\n/**\n *  sys_rt_sigqueueinfo - send signal information to a signal\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *  @uinfo: signal info to be sent\n */\nSYSCALL_DEFINE3(rt_sigqueueinfo, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tsiginfo_t info;\n\n\tif (copy_from_user(&info, uinfo, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif (info.si_code >= 0 || info.si_code == SI_TKILL) {\n\t\t/* We used to allow any < 0 si_code */\n\t\tWARN_ON_ONCE(info.si_code < 0);\n\t\treturn -EPERM;\n\t}\n\tinfo.si_signo = sig;\n\n\t/* POSIX.1b doesn't mention process groups.  */\n\treturn kill_proc_info(sig, &info, pid);\n}\n\nlong do_rt_tgsigqueueinfo(pid_t tgid, pid_t pid, int sig, siginfo_t *info)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif (info->si_code >= 0 || info->si_code == SI_TKILL) {\n\t\t/* We used to allow any < 0 si_code */\n\t\tWARN_ON_ONCE(info->si_code < 0);\n\t\treturn -EPERM;\n\t}\n\tinfo->si_signo = sig;\n\n\treturn do_send_specific(tgid, pid, sig, info);\n}\n\nSYSCALL_DEFINE4(rt_tgsigqueueinfo, pid_t, tgid, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tsiginfo_t info;\n\n\tif (copy_from_user(&info, uinfo, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n\nint do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact)\n{\n\tstruct task_struct *t = current;\n\tstruct k_sigaction *k;\n\tsigset_t mask;\n\n\tif (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))\n\t\treturn -EINVAL;\n\n\tk = &t->sighand->action[sig-1];\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (oact)\n\t\t*oact = *k;\n\n\tif (act) {\n\t\tsigdelsetmask(&act->sa.sa_mask,\n\t\t\t      sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t\t*k = *act;\n\t\t/*\n\t\t * POSIX 3.3.1.3:\n\t\t *  \"Setting a signal action to SIG_IGN for a signal that is\n\t\t *   pending shall cause the pending signal to be discarded,\n\t\t *   whether or not it is blocked.\"\n\t\t *\n\t\t *  \"Setting a signal action to SIG_DFL for a signal that is\n\t\t *   pending and whose default action is to ignore the signal\n\t\t *   (for example, SIGCHLD), shall cause the pending signal to\n\t\t *   be discarded, whether or not it is blocked\"\n\t\t */\n\t\tif (sig_handler_ignored(sig_handler(t, sig), sig)) {\n\t\t\tsigemptyset(&mask);\n\t\t\tsigaddset(&mask, sig);\n\t\t\trm_from_queue_full(&mask, &t->signal->shared_pending);\n\t\t\tdo {\n\t\t\t\trm_from_queue_full(&mask, &t->pending);\n\t\t\t\tt = next_thread(t);\n\t\t\t} while (t != current);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn 0;\n}\n\nint \ndo_sigaltstack (const stack_t __user *uss, stack_t __user *uoss, unsigned long sp)\n{\n\tstack_t oss;\n\tint error;\n\n\toss.ss_sp = (void __user *) current->sas_ss_sp;\n\toss.ss_size = current->sas_ss_size;\n\toss.ss_flags = sas_ss_flags(sp);\n\n\tif (uss) {\n\t\tvoid __user *ss_sp;\n\t\tsize_t ss_size;\n\t\tint ss_flags;\n\n\t\terror = -EFAULT;\n\t\tif (!access_ok(VERIFY_READ, uss, sizeof(*uss)))\n\t\t\tgoto out;\n\t\terror = __get_user(ss_sp, &uss->ss_sp) |\n\t\t\t__get_user(ss_flags, &uss->ss_flags) |\n\t\t\t__get_user(ss_size, &uss->ss_size);\n\t\tif (error)\n\t\t\tgoto out;\n\n\t\terror = -EPERM;\n\t\tif (on_sig_stack(sp))\n\t\t\tgoto out;\n\n\t\terror = -EINVAL;\n\t\t/*\n\t\t * Note - this code used to test ss_flags incorrectly:\n\t\t *  \t  old code may have been written using ss_flags==0\n\t\t *\t  to mean ss_flags==SS_ONSTACK (as this was the only\n\t\t *\t  way that worked) - this fix preserves that older\n\t\t *\t  mechanism.\n\t\t */\n\t\tif (ss_flags != SS_DISABLE && ss_flags != SS_ONSTACK && ss_flags != 0)\n\t\t\tgoto out;\n\n\t\tif (ss_flags == SS_DISABLE) {\n\t\t\tss_size = 0;\n\t\t\tss_sp = NULL;\n\t\t} else {\n\t\t\terror = -ENOMEM;\n\t\t\tif (ss_size < MINSIGSTKSZ)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tcurrent->sas_ss_sp = (unsigned long) ss_sp;\n\t\tcurrent->sas_ss_size = ss_size;\n\t}\n\n\terror = 0;\n\tif (uoss) {\n\t\terror = -EFAULT;\n\t\tif (!access_ok(VERIFY_WRITE, uoss, sizeof(*uoss)))\n\t\t\tgoto out;\n\t\terror = __put_user(oss.ss_sp, &uoss->ss_sp) |\n\t\t\t__put_user(oss.ss_size, &uoss->ss_size) |\n\t\t\t__put_user(oss.ss_flags, &uoss->ss_flags);\n\t}\n\nout:\n\treturn error;\n}\n#ifdef CONFIG_GENERIC_SIGALTSTACK\nSYSCALL_DEFINE2(sigaltstack,const stack_t __user *,uss, stack_t __user *,uoss)\n{\n\treturn do_sigaltstack(uss, uoss, current_user_stack_pointer());\n}\n#endif\n\nint restore_altstack(const stack_t __user *uss)\n{\n\tint err = do_sigaltstack(uss, NULL, current_user_stack_pointer());\n\t/* squash all but EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __save_altstack(stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\treturn  __put_user((void __user *)t->sas_ss_sp, &uss->ss_sp) |\n\t\t__put_user(sas_ss_flags(sp), &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n}\n\n#ifdef CONFIG_COMPAT\n#ifdef CONFIG_GENERIC_SIGALTSTACK\nCOMPAT_SYSCALL_DEFINE2(sigaltstack,\n\t\t\tconst compat_stack_t __user *, uss_ptr,\n\t\t\tcompat_stack_t __user *, uoss_ptr)\n{\n\tstack_t uss, uoss;\n\tint ret;\n\tmm_segment_t seg;\n\n\tif (uss_ptr) {\n\t\tcompat_stack_t uss32;\n\n\t\tmemset(&uss, 0, sizeof(stack_t));\n\t\tif (copy_from_user(&uss32, uss_ptr, sizeof(compat_stack_t)))\n\t\t\treturn -EFAULT;\n\t\tuss.ss_sp = compat_ptr(uss32.ss_sp);\n\t\tuss.ss_flags = uss32.ss_flags;\n\t\tuss.ss_size = uss32.ss_size;\n\t}\n\tseg = get_fs();\n\tset_fs(KERNEL_DS);\n\tret = do_sigaltstack((stack_t __force __user *) (uss_ptr ? &uss : NULL),\n\t\t\t     (stack_t __force __user *) &uoss,\n\t\t\t     compat_user_stack_pointer());\n\tset_fs(seg);\n\tif (ret >= 0 && uoss_ptr)  {\n\t\tif (!access_ok(VERIFY_WRITE, uoss_ptr, sizeof(compat_stack_t)) ||\n\t\t    __put_user(ptr_to_compat(uoss.ss_sp), &uoss_ptr->ss_sp) ||\n\t\t    __put_user(uoss.ss_flags, &uoss_ptr->ss_flags) ||\n\t\t    __put_user(uoss.ss_size, &uoss_ptr->ss_size))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\nint compat_restore_altstack(const compat_stack_t __user *uss)\n{\n\tint err = compat_sys_sigaltstack(uss, NULL);\n\t/* squash all but -EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __compat_save_altstack(compat_stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\treturn  __put_user(ptr_to_compat((void __user *)t->sas_ss_sp), &uss->ss_sp) |\n\t\t__put_user(sas_ss_flags(sp), &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n}\n#endif\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPENDING\n\n/**\n *  sys_sigpending - examine pending signals\n *  @set: where mask of pending signal is returned\n */\nSYSCALL_DEFINE1(sigpending, old_sigset_t __user *, set)\n{\n\treturn do_sigpending(set, sizeof(*set));\n}\n\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\n/**\n *  sys_sigprocmask - examine and change blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: signals to add or remove (if non-null)\n *  @oset: previous value of signal mask if non-null\n *\n * Some platforms have their own version with special arguments;\n * others support only sys_rt_sigprocmask.\n */\n\nSYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, nset,\n\t\told_sigset_t __user *, oset)\n{\n\told_sigset_t old_set, new_set;\n\tsigset_t new_blocked;\n\n\told_set = current->blocked.sig[0];\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(*nset)))\n\t\t\treturn -EFAULT;\n\n\t\tnew_blocked = current->blocked;\n\n\t\tswitch (how) {\n\t\tcase SIG_BLOCK:\n\t\t\tsigaddsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_UNBLOCK:\n\t\t\tsigdelsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_SETMASK:\n\t\t\tnew_blocked.sig[0] = new_set;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tset_current_blocked(&new_blocked);\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(*oset)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n#endif /* __ARCH_WANT_SYS_SIGPROCMASK */\n\n#ifdef __ARCH_WANT_SYS_RT_SIGACTION\n/**\n *  sys_rt_sigaction - alter an action taken by a process\n *  @sig: signal to be sent\n *  @act: new sigaction\n *  @oact: used to save the previous sigaction\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct sigaction __user *, act,\n\t\tstruct sigaction __user *, oact,\n\t\tsize_t, sigsetsize)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret = -EINVAL;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\tgoto out;\n\n\tif (act) {\n\t\tif (copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigaction(sig, act ? &new_sa : NULL, oact ? &old_sa : NULL);\n\n\tif (!ret && oact) {\n\t\tif (copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa)))\n\t\t\treturn -EFAULT;\n\t}\nout:\n\treturn ret;\n}\n#endif /* __ARCH_WANT_SYS_RT_SIGACTION */\n\n#ifdef __ARCH_WANT_SYS_SGETMASK\n\n/*\n * For backwards compatibility.  Functionality superseded by sigprocmask.\n */\nSYSCALL_DEFINE0(sgetmask)\n{\n\t/* SMP safe */\n\treturn current->blocked.sig[0];\n}\n\nSYSCALL_DEFINE1(ssetmask, int, newmask)\n{\n\tint old = current->blocked.sig[0];\n\tsigset_t newset;\n\n\tsiginitset(&newset, newmask);\n\tset_current_blocked(&newset);\n\n\treturn old;\n}\n#endif /* __ARCH_WANT_SGETMASK */\n\n#ifdef __ARCH_WANT_SYS_SIGNAL\n/*\n * For backwards compatibility.  Functionality superseded by sigaction.\n */\nSYSCALL_DEFINE2(signal, int, sig, __sighandler_t, handler)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret;\n\n\tnew_sa.sa.sa_handler = handler;\n\tnew_sa.sa.sa_flags = SA_ONESHOT | SA_NOMASK;\n\tsigemptyset(&new_sa.sa.sa_mask);\n\n\tret = do_sigaction(sig, &new_sa, &old_sa);\n\n\treturn ret ? ret : (unsigned long)old_sa.sa.sa_handler;\n}\n#endif /* __ARCH_WANT_SYS_SIGNAL */\n\n#ifdef __ARCH_WANT_SYS_PAUSE\n\nSYSCALL_DEFINE0(pause)\n{\n\twhile (!signal_pending(current)) {\n\t\tcurrent->state = TASK_INTERRUPTIBLE;\n\t\tschedule();\n\t}\n\treturn -ERESTARTNOHAND;\n}\n\n#endif\n\nint sigsuspend(sigset_t *set)\n{\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(set);\n\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tschedule();\n\tset_restore_sigmask();\n\treturn -ERESTARTNOHAND;\n}\n\n#ifdef __ARCH_WANT_SYS_RT_SIGSUSPEND\n/**\n *  sys_rt_sigsuspend - replace the signal mask for a value with the\n *\t@unewset value until a signal is received\n *  @unewset: new signal mask value\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE2(rt_sigsuspend, sigset_t __user *, unewset, size_t, sigsetsize)\n{\n\tsigset_t newset;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&newset, unewset, sizeof(newset)))\n\t\treturn -EFAULT;\n\treturn sigsuspend(&newset);\n}\n#endif /* __ARCH_WANT_SYS_RT_SIGSUSPEND */\n\n__attribute__((weak)) const char *arch_vma_name(struct vm_area_struct *vma)\n{\n\treturn NULL;\n}\n\nvoid __init signals_init(void)\n{\n\tsigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC);\n}\n\n#ifdef CONFIG_KGDB_KDB\n#include <linux/kdb.h>\n/*\n * kdb_send_sig_info - Allows kdb to send signals without exposing\n * signal internals.  This function checks if the required locks are\n * available before calling the main signal code, to avoid kdb\n * deadlocks.\n */\nvoid\nkdb_send_sig_info(struct task_struct *t, struct siginfo *info)\n{\n\tstatic struct task_struct *kdb_prev_t;\n\tint sig, new_t;\n\tif (!spin_trylock(&t->sighand->siglock)) {\n\t\tkdb_printf(\"Can't do kill command now.\\n\"\n\t\t\t   \"The sigmask lock is held somewhere else in \"\n\t\t\t   \"kernel, try again later\\n\");\n\t\treturn;\n\t}\n\tspin_unlock(&t->sighand->siglock);\n\tnew_t = kdb_prev_t != t;\n\tkdb_prev_t = t;\n\tif (t->state != TASK_RUNNING && new_t) {\n\t\tkdb_printf(\"Process is not RUNNING, sending a signal from \"\n\t\t\t   \"kdb risks deadlock\\n\"\n\t\t\t   \"on the run queue locks. \"\n\t\t\t   \"The signal has _not_ been sent.\\n\"\n\t\t\t   \"Reissue the kill command if you want to risk \"\n\t\t\t   \"the deadlock.\\n\");\n\t\treturn;\n\t}\n\tsig = info->si_signo;\n\tif (send_sig_info(sig, info, t))\n\t\tkdb_printf(\"Fail to deliver Signal %d to process %d.\\n\",\n\t\t\t   sig, t->pid);\n\telse\n\t\tkdb_printf(\"Signal %d is sent to process %d.\\n\", sig, t->pid);\n}\n#endif\t/* CONFIG_KGDB_KDB */\n"], "filenames": ["arch/x86/kernel/step.c", "kernel/ptrace.c", "kernel/signal.c"], "buggy_code_start_loc": [168, 124, 1796], "buggy_code_end_loc": [172, 1043, 1921], "fixing_code_start_loc": [168, 125, 1797], "fixing_code_end_loc": [173, 1088, 1927], "type": "CWE-362", "message": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death.", "other": {"cve": {"id": "CVE-2013-0871", "sourceIdentifier": "cve-coordination@google.com", "published": "2013-02-18T04:41:50.463", "lastModified": "2013-06-21T03:16:21.743", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death."}, {"lang": "es", "value": "Condicion de carrera en la funcionalidad ptrace en el kernel de Linux anterior a v3.7.5 permite a usuarios locales ganar privilegios de administrador mediante una llamada PTRACE_SETREGS ptrace en una aplicaci\u00f3n manipulada, como se demostr\u00f3 con ptrace_death."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.7.4", "matchCriteriaId": "AAAEC09C-2D88-4225-8404-B4F37B92CD0E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.35:*:*:*:*:*:*:*", "matchCriteriaId": "F72412E3-8DA9-4CC9-A426-B534202ADBA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.36:*:*:*:*:*:*:*", "matchCriteriaId": "FCAA9D7A-3C3E-4C0B-9D38-EA80E68C2E46"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.37:*:*:*:*:*:*:*", "matchCriteriaId": "4A9E3AE5-3FCF-4CBB-A30B-082BCFBFB0CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.38:*:*:*:*:*:*:*", "matchCriteriaId": "CF715657-4C3A-4392-B85D-1BBF4DE45D89"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.39:*:*:*:*:*:*:*", "matchCriteriaId": "4B63C618-AC3D-4EF7-AFDF-27B9BF482B78"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.40:*:*:*:*:*:*:*", "matchCriteriaId": "C33DA5A9-5E40-4365-9602-82FB4DCD15B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.41:*:*:*:*:*:*:*", "matchCriteriaId": "EFAFDB74-40BD-46FA-89AC-617EB2C7160B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.42:*:*:*:*:*:*:*", "matchCriteriaId": "CF5F17DA-30A7-40CF-BD7C-CEDF06D64617"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.43:*:*:*:*:*:*:*", "matchCriteriaId": "71A276F5-BD9D-4C1B-90DF-9B0C15B6F7DF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.44:*:*:*:*:*:*:*", "matchCriteriaId": "F8F6EBEC-3C29-444B-BB85-6EF239B59EC1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc2:*:*:*:*:*:*", "matchCriteriaId": "99372D07-C06A-41FA-9843-6D57F99AB5AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc3:*:*:*:*:*:*", "matchCriteriaId": "2B9DC110-D260-4DB4-B8B0-EF1D160ADA07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc4:*:*:*:*:*:*", "matchCriteriaId": "6192FE84-4D53-40D4-AF61-78CE7136141A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc5:*:*:*:*:*:*", "matchCriteriaId": "42FEF3CF-1302-45EB-89CC-3786FE4BAC1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc6:*:*:*:*:*:*", "matchCriteriaId": "AE6A6B58-2C89-4DE4-BA57-78100818095C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc7:*:*:*:*:*:*", "matchCriteriaId": "1D467F87-2F13-4D26-9A93-E0BA526FEA24"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "FE348F7B-02DE-47D5-8011-F83DA9426021"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "E91594EA-F0A3-41B3-A9C6-F7864FC2F229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "9E1ECCDB-0208-48F6-B44F-16CC0ECE3503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "FBA8B5DE-372E-47E0-A0F6-BE286D509CC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "9A1CA083-2CF8-45AE-9E15-1AA3A8352E3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "19D69A49-5290-4C5F-8157-719AD58D253D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "290BD969-42E7-47B0-B21B-06DE4865432C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "23A9E29E-DE78-4C73-9FBD-C2410F5FC8B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "018434C9-E75F-45CB-A169-DAB4B1D864D7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "DC0AC68F-EC58-4C4F-8CBC-A59ECC00CCDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "C123C844-F6D7-471E-A62E-F756042FB1CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.12:*:*:*:*:*:*:*", "matchCriteriaId": "A11C38BB-7FA2-49B0-AAC9-83DB387A06DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.13:*:*:*:*:*:*:*", "matchCriteriaId": "61F3733C-E5F6-4855-B471-DF3FB823613B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.14:*:*:*:*:*:*:*", "matchCriteriaId": "1DDCA75F-9A06-4457-9A45-38A38E7F7086"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.15:*:*:*:*:*:*:*", "matchCriteriaId": "7AEA837E-7864-4003-8DB7-111ED710A7E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.16:*:*:*:*:*:*:*", "matchCriteriaId": "B6FE471F-2D1F-4A1D-A197-7E46B75787E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.17:*:*:*:*:*:*:*", "matchCriteriaId": "FDA9E6AB-58DC-4EC5-A25C-11F9D0B38BF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.18:*:*:*:*:*:*:*", "matchCriteriaId": "DC6B8DB3-B05B-41A2-B091-342D66AAE8F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.19:*:*:*:*:*:*:*", "matchCriteriaId": "958F0FF8-33EF-4A71-A0BD-572C85211DBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.20:*:*:*:*:*:*:*", "matchCriteriaId": "FBA39F48-B02F-4C48-B304-DA9CCA055244"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.21:*:*:*:*:*:*:*", "matchCriteriaId": "1FF841F3-48A7-41D7-9C45-A8170435A5EB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.22:*:*:*:*:*:*:*", "matchCriteriaId": "EF506916-A6DC-4B1E-90E5-959492AF55F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.23:*:*:*:*:*:*:*", "matchCriteriaId": "B3CDAD1F-2C6A-48C0-8FAB-C2659373FA25"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.24:*:*:*:*:*:*:*", "matchCriteriaId": "4FFE4B22-C96A-43D0-B993-F51EDD9C5E0E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.25:*:*:*:*:*:*:*", "matchCriteriaId": "F571CC8B-B212-4553-B463-1DB01D616E8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.26:*:*:*:*:*:*:*", "matchCriteriaId": "84E3E151-D437-48ED-A529-731EEFF88567"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.27:*:*:*:*:*:*:*", "matchCriteriaId": "E9E3EA3C-CCA5-4433-86E0-3D02C4757A0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.28:*:*:*:*:*:*:*", "matchCriteriaId": "F7AC4F7D-9FA6-4CF1-B2E9-70BF7D4D177C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.29:*:*:*:*:*:*:*", "matchCriteriaId": "3CE3A80D-9648-43CC-8F99-D741ED6552BF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.30:*:*:*:*:*:*:*", "matchCriteriaId": "C8A98C03-A465-41B4-A551-A26FEC7FFD94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "AFB76697-1C2F-48C0-9B14-517EC053D4B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc1:*:*:*:*:*:*", "matchCriteriaId": "BED88DFD-1DC5-4505-A441-44ECDEF0252D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc2:*:*:*:*:*:*", "matchCriteriaId": "DBFD2ACD-728A-4082-BB6A-A1EF6E58E47D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc3:*:*:*:*:*:*", "matchCriteriaId": "C31B0E51-F62D-4053-B04F-FC4D5BC373D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc4:*:*:*:*:*:*", "matchCriteriaId": "A914303E-1CB6-4AAD-9F5F-DE5433C4E814"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc5:*:*:*:*:*:*", "matchCriteriaId": "203BBA69-90B2-4C5E-8023-C14180742421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc6:*:*:*:*:*:*", "matchCriteriaId": "0DBFAB53-B889-4028-AC0E-7E165B152A18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc7:*:*:*:*:*:*", "matchCriteriaId": "FE409AEC-F677-4DEF-8EB7-2C35809043CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.1:*:*:*:*:*:*:*", "matchCriteriaId": "578EC12B-402F-4AD4-B8F8-C9B2CAB06891"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.2:*:*:*:*:*:*:*", "matchCriteriaId": "877002ED-8097-4BB4-BB88-6FC6306C38B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.3:*:*:*:*:*:*:*", "matchCriteriaId": "76294CE3-D72C-41D5-9E0F-B693D0042699"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.4:*:*:*:*:*:*:*", "matchCriteriaId": "916E97D4-1FAB-42F5-826B-653B1C0909A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.5:*:*:*:*:*:*:*", "matchCriteriaId": "33FD2217-C5D0-48C1-AD74-3527127FEF9C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.6:*:*:*:*:*:*:*", "matchCriteriaId": "2E92971F-B629-4E0A-9A50-8B235F9704B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.7:*:*:*:*:*:*:*", "matchCriteriaId": "EDD3A069-3829-4EE2-9D5A-29459F29D4C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.8:*:*:*:*:*:*:*", "matchCriteriaId": "A4A0964C-CEB2-41D7-A69C-1599B05B6171"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "0F960FA6-F904-4A4E-B483-44C70090E9A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:*:*", "matchCriteriaId": "261C1B41-C9E0-414F-8368-51C0C0B8AD38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:*:*", "matchCriteriaId": "5CCA261D-2B97-492F-89A0-5F209A804350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:*:*", "matchCriteriaId": "1B1C0C68-9194-473F-BE5E-EC7F184899FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:*:*", "matchCriteriaId": "D7A6AC9E-BEA6-44B0-B3B3-F0F94E32424A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:*:*", "matchCriteriaId": "16038328-9399-4B85-B777-BA4757D02C9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:*:*", "matchCriteriaId": "16CA2757-FA8D-43D9-96E8-D3C0EB6E1DEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:*:*", "matchCriteriaId": "E8CB5481-5EAE-401E-BD7E-D3095CCA9E94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.1:*:*:*:*:*:*:*", "matchCriteriaId": "A0F36FAC-141D-476D-84C5-A558C199F904"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.2:*:*:*:*:*:*:*", "matchCriteriaId": "51D64824-25F6-4761-BD6A-29038A143744"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.3:*:*:*:*:*:*:*", "matchCriteriaId": "E284C8A1-740F-454D-A774-99CD3A21B594"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.4:*:*:*:*:*:*:*", "matchCriteriaId": "C70D72AE-0CBF-4324-9935-57E28EC6279C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.5:*:*:*:*:*:*:*", "matchCriteriaId": "F674B06B-7E86-4E41-9126-8152D0DDABAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.6:*:*:*:*:*:*:*", "matchCriteriaId": "7039B3EC-8B22-413E-B582-B4BEC6181241"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.7:*:*:*:*:*:*:*", "matchCriteriaId": "35CF1DD2-80B9-4476-8963-5C3EF52B33F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.8:*:*:*:*:*:*:*", "matchCriteriaId": "BFB0B05B-A5CE-4B9C-AE7F-83062868D35B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.9:*:*:*:*:*:*:*", "matchCriteriaId": "D166A66E-7454-47EC-BB56-861A9AFEAFE1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.10:*:*:*:*:*:*:*", "matchCriteriaId": "7DA94F50-2A62-4300-BF4D-A342AAE35629"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.11:*:*:*:*:*:*:*", "matchCriteriaId": "252D937B-50DC-444F-AE73-5FCF6203DF27"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.12:*:*:*:*:*:*:*", "matchCriteriaId": "F6D8EE51-02C1-47BC-A92C-0A8ABEFD28FF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.13:*:*:*:*:*:*:*", "matchCriteriaId": "7F20A5D7-3B38-4911-861A-04C8310D5916"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.14:*:*:*:*:*:*:*", "matchCriteriaId": "D472DE3A-71D8-4F40-9DDE-85929A2B047D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.15:*:*:*:*:*:*:*", "matchCriteriaId": "B2AED943-65A8-4FDB-BBD0-CCEF8682A48C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.16:*:*:*:*:*:*:*", "matchCriteriaId": "D4640185-F3D8-4575-A71D-4C889A93DE2C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.17:*:*:*:*:*:*:*", "matchCriteriaId": "144CCF7C-025E-4879-B2E7-ABB8E4390BE5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.18:*:*:*:*:*:*:*", "matchCriteriaId": "B6FAA052-0B2B-40CE-8C98-919B8D08A5ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.19:*:*:*:*:*:*:*", "matchCriteriaId": "4B5A53DE-9C83-4A6B-96F3-23C03BF445D9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.20:*:*:*:*:*:*:*", "matchCriteriaId": "063EB879-CB05-4E33-AA90-9E43516839B5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.21:*:*:*:*:*:*:*", "matchCriteriaId": "2D25764F-4B02-4C65-954E-8C7D6632DE00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.22:*:*:*:*:*:*:*", "matchCriteriaId": "F31F5BF3-CD0A-465C-857F-273841BCD28A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.23:*:*:*:*:*:*:*", "matchCriteriaId": "FF302C8A-079B-42B9-B455-CD9083BFA067"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.24:*:*:*:*:*:*:*", "matchCriteriaId": "744999C0-33D3-4363-B3DB-E0D02CDD3918"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.1:*:*:*:*:*:*:*", "matchCriteriaId": "962B0C45-AB29-4383-AC16-C6E8245D0FF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.2:*:*:*:*:*:*:*", "matchCriteriaId": "A0EE126B-74B2-4F79-BFE1-3DC169F3F9B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.3:*:*:*:*:*:*:*", "matchCriteriaId": "392075E0-A9C7-4B4A-90F9-7F1ADFF5EFA7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.4:*:*:*:*:*:*:*", "matchCriteriaId": "ECC66968-06F0-4874-A95A-A292C36E45C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.5:*:*:*:*:*:*:*", "matchCriteriaId": "5FE986E6-1068-4E1B-8EAB-DF1EAF32B4E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.6:*:*:*:*:*:*:*", "matchCriteriaId": "543E8536-1A8E-4E76-B89F-1B1F9F26FAB8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.7:*:*:*:*:*:*:*", "matchCriteriaId": "EC2B45E3-31E1-4B46-85FA-3A84E75B8F84"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.9:*:*:*:*:*:*:*", "matchCriteriaId": "5AC4A13E-F560-4D01-98A3-E2A2B82EB25B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.10:*:*:*:*:*:*:*", "matchCriteriaId": "942C462A-5398-4BB9-A792-598682E1FEF2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.11:*:*:*:*:*:*:*", "matchCriteriaId": "B852F7E0-0282-483D-BB4D-18CB7A4F1392"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7:*:*:*:*:*:*:*", "matchCriteriaId": "53ED9A31-99CC-41C8-8B72-5B2A9B49AA6C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.1:*:*:*:*:*:*:*", "matchCriteriaId": "EFD646BC-62F7-47CF-B0BE-768F701F7D9A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.2:*:*:*:*:*:*:*", "matchCriteriaId": "F43D418E-87C1-4C83-9FF1-4F45B4F452DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.3:*:*:*:*:*:*:*", "matchCriteriaId": "680D0E00-F29A-487C-8770-8E7EAC672B7C"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=9899d11f654474d2d54ea52ceaa2a1f4db3abd68", "source": "cve-coordination@google.com", "tags": ["Patch"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2013-02/msg00022.html", "source": "cve-coordination@google.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2013-04/msg00018.html", "source": "cve-coordination@google.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2013-06/msg00005.html", "source": "cve-coordination@google.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-0567.html", "source": "cve-coordination@google.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-0661.html", "source": "cve-coordination@google.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-0662.html", "source": "cve-coordination@google.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-0695.html", "source": "cve-coordination@google.com"}, {"url": "http://www.debian.org/security/2013/dsa-2632", "source": "cve-coordination@google.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.7.5", "source": "cve-coordination@google.com"}, {"url": "http://www.openwall.com/lists/oss-security/2013/02/15/16", "source": "cve-coordination@google.com", "tags": ["Patch"]}, {"url": "http://www.ubuntu.com/usn/USN-1736-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1737-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1738-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1739-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1740-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1741-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1742-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1743-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1744-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1745-1", "source": "cve-coordination@google.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=911937", "source": "cve-coordination@google.com"}, {"url": "https://github.com/torvalds/linux/commit/9899d11f654474d2d54ea52ceaa2a1f4db3abd68", "source": "cve-coordination@google.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/9899d11f654474d2d54ea52ceaa2a1f4db3abd68"}}