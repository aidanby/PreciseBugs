{"buggy_code": ["#ifndef _ASM_X86_MMU_CONTEXT_H\n#define _ASM_X86_MMU_CONTEXT_H\n\n#include <asm/desc.h>\n#include <linux/atomic.h>\n#include <linux/mm_types.h>\n\n#include <trace/events/tlb.h>\n\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include <asm/paravirt.h>\n#include <asm/mpx.h>\n#ifndef CONFIG_PARAVIRT\nstatic inline void paravirt_activate_mm(struct mm_struct *prev,\n\t\t\t\t\tstruct mm_struct *next)\n{\n}\n#endif\t/* !CONFIG_PARAVIRT */\n\n#ifdef CONFIG_PERF_EVENTS\nextern struct static_key rdpmc_always_available;\n\nstatic inline void load_mm_cr4(struct mm_struct *mm)\n{\n\tif (static_key_false(&rdpmc_always_available) ||\n\t    atomic_read(&mm->context.perf_rdpmc_allowed))\n\t\tcr4_set_bits(X86_CR4_PCE);\n\telse\n\t\tcr4_clear_bits(X86_CR4_PCE);\n}\n#else\nstatic inline void load_mm_cr4(struct mm_struct *mm) {}\n#endif\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n/*\n * ldt_structs can be allocated, used, and freed, but they are never\n * modified while live.\n */\nstruct ldt_struct {\n\t/*\n\t * Xen requires page-aligned LDTs with special permissions.  This is\n\t * needed to prevent us from installing evil descriptors such as\n\t * call gates.  On native, we could merge the ldt_struct and LDT\n\t * allocations, but it's not worth trying to optimize.\n\t */\n\tstruct desc_struct *entries;\n\tint size;\n};\n\n/*\n * Used for LDT copy/destruction.\n */\nint init_new_context(struct task_struct *tsk, struct mm_struct *mm);\nvoid destroy_context(struct mm_struct *mm);\n#else\t/* CONFIG_MODIFY_LDT_SYSCALL */\nstatic inline int init_new_context(struct task_struct *tsk,\n\t\t\t\t   struct mm_struct *mm)\n{\n\treturn 0;\n}\nstatic inline void destroy_context(struct mm_struct *mm) {}\n#endif\n\nstatic inline void load_mm_ldt(struct mm_struct *mm)\n{\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\tstruct ldt_struct *ldt;\n\n\t/* lockless_dereference synchronizes with smp_store_release */\n\tldt = lockless_dereference(mm->context.ldt);\n\n\t/*\n\t * Any change to mm->context.ldt is followed by an IPI to all\n\t * CPUs with the mm active.  The LDT will not be freed until\n\t * after the IPI is handled by all such CPUs.  This means that,\n\t * if the ldt_struct changes before we return, the values we see\n\t * will be safe, and the new values will be loaded before we run\n\t * any user code.\n\t *\n\t * NB: don't try to convert this to use RCU without extreme care.\n\t * We would still need IRQs off, because we don't want to change\n\t * the local LDT after an IPI loaded a newer value than the one\n\t * that we can see.\n\t */\n\n\tif (unlikely(ldt))\n\t\tset_ldt(ldt->entries, ldt->size);\n\telse\n\t\tclear_LDT();\n#else\n\tclear_LDT();\n#endif\n\n\tDEBUG_LOCKS_WARN_ON(preemptible());\n}\n\nstatic inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)\n{\n#ifdef CONFIG_SMP\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);\n#endif\n}\n\nstatic inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t\t\t     struct task_struct *tsk)\n{\n\tunsigned cpu = smp_processor_id();\n\n\tif (likely(prev != next)) {\n#ifdef CONFIG_SMP\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tthis_cpu_write(cpu_tlbstate.active_mm, next);\n#endif\n\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t/* Re-load page tables */\n\t\tload_cr3(next->pgd);\n\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\n\t\t/* Stop flush ipis for the previous mm */\n\t\tcpumask_clear_cpu(cpu, mm_cpumask(prev));\n\n\t\t/* Load per-mm CR4 state */\n\t\tload_mm_cr4(next);\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\t\t/*\n\t\t * Load the LDT, if the LDT is different.\n\t\t *\n\t\t * It's possible that prev->context.ldt doesn't match\n\t\t * the LDT register.  This can happen if leave_mm(prev)\n\t\t * was called and then modify_ldt changed\n\t\t * prev->context.ldt but suppressed an IPI to this CPU.\n\t\t * In this case, prev->context.ldt != NULL, because we\n\t\t * never set context.ldt to NULL while the mm still\n\t\t * exists.  That means that next->context.ldt !=\n\t\t * prev->context.ldt, because mms never share an LDT.\n\t\t */\n\t\tif (unlikely(prev->context.ldt != next->context.ldt))\n\t\t\tload_mm_ldt(next);\n#endif\n\t}\n#ifdef CONFIG_SMP\n\t  else {\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tBUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);\n\n\t\tif (!cpumask_test_cpu(cpu, mm_cpumask(next))) {\n\t\t\t/*\n\t\t\t * On established mms, the mm_cpumask is only changed\n\t\t\t * from irq context, from ptep_clear_flush() while in\n\t\t\t * lazy tlb mode, and here. Irqs are blocked during\n\t\t\t * schedule, protecting us from simultaneous changes.\n\t\t\t */\n\t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\t\t\t/*\n\t\t\t * We were in lazy tlb mode and leave_mm disabled\n\t\t\t * tlb flush IPI delivery. We must reload CR3\n\t\t\t * to make sure to use no freed page tables.\n\t\t\t */\n\t\t\tload_cr3(next->pgd);\n\t\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\t\t\tload_mm_cr4(next);\n\t\t\tload_mm_ldt(next);\n\t\t}\n\t}\n#endif\n}\n\n#define activate_mm(prev, next)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tparavirt_activate_mm((prev), (next));\t\\\n\tswitch_mm((prev), (next), NULL);\t\\\n} while (0);\n\n#ifdef CONFIG_X86_32\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tlazy_load_gs(0);\t\t\t\\\n} while (0)\n#else\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tload_gs_index(0);\t\t\t\\\n\tloadsegment(fs, 0);\t\t\t\\\n} while (0)\n#endif\n\nstatic inline void arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t struct mm_struct *mm)\n{\n\tparavirt_arch_dup_mmap(oldmm, mm);\n}\n\nstatic inline void arch_exit_mmap(struct mm_struct *mm)\n{\n\tparavirt_arch_exit_mmap(mm);\n}\n\n#ifdef CONFIG_X86_64\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn\t!config_enabled(CONFIG_IA32_EMULATION) ||\n\t\t!(mm->context.ia32_compat == TIF_IA32);\n}\n#else\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn false;\n}\n#endif\n\nstatic inline void arch_bprm_mm_init(struct mm_struct *mm,\n\t\tstruct vm_area_struct *vma)\n{\n\tmpx_mm_init(mm);\n}\n\nstatic inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t      unsigned long start, unsigned long end)\n{\n\t/*\n\t * mpx_notify_unmap() goes and reads a rarely-hot\n\t * cacheline in the mm_struct.  That can be expensive\n\t * enough to be seen in profiles.\n\t *\n\t * The mpx_notify_unmap() call and its contents have been\n\t * observed to affect munmap() performance on hardware\n\t * where MPX is not present.\n\t *\n\t * The unlikely() optimizes for the fast case: no MPX\n\t * in the CPU, or no MPX use in the process.  Even if\n\t * we get this wrong (in the unlikely event that MPX\n\t * is widely enabled on some system) the overhead of\n\t * MPX itself (reading bounds tables) is expected to\n\t * overwhelm the overhead of getting this unlikely()\n\t * consistently wrong.\n\t */\n\tif (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))\n\t\tmpx_notify_unmap(mm, vma, start, end);\n}\n\n#endif /* _ASM_X86_MMU_CONTEXT_H */\n", "#include <linux/init.h>\n\n#include <linux/mm.h>\n#include <linux/spinlock.h>\n#include <linux/smp.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/cpu.h>\n\n#include <asm/tlbflush.h>\n#include <asm/mmu_context.h>\n#include <asm/cache.h>\n#include <asm/apic.h>\n#include <asm/uv/uv.h>\n#include <linux/debugfs.h>\n\n/*\n *\tSmarter SMP flushing macros.\n *\t\tc/o Linus Torvalds.\n *\n *\tThese mean you can really definitely utterly forget about\n *\twriting to user space from interrupts. (Its not allowed anyway).\n *\n *\tOptimizations Manfred Spraul <manfred@colorfullife.com>\n *\n *\tMore scalable flush, from Andi Kleen\n *\n *\tImplement flush IPI by CALL_FUNCTION_VECTOR, Alex Shi\n */\n\nstruct flush_tlb_info {\n\tstruct mm_struct *flush_mm;\n\tunsigned long flush_start;\n\tunsigned long flush_end;\n};\n\n/*\n * We cannot call mmdrop() because we are in interrupt context,\n * instead update mm->cpu_vm_mask.\n */\nvoid leave_mm(int cpu)\n{\n\tstruct mm_struct *active_mm = this_cpu_read(cpu_tlbstate.active_mm);\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)\n\t\tBUG();\n\tif (cpumask_test_cpu(cpu, mm_cpumask(active_mm))) {\n\t\tcpumask_clear_cpu(cpu, mm_cpumask(active_mm));\n\t\tload_cr3(swapper_pg_dir);\n\t\t/*\n\t\t * This gets called in the idle path where RCU\n\t\t * functions differently.  Tracing normally\n\t\t * uses RCU, so we have to call the tracepoint\n\t\t * specially here.\n\t\t */\n\t\ttrace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\t}\n}\nEXPORT_SYMBOL_GPL(leave_mm);\n\n/*\n * The flush IPI assumes that a thread switch happens in this order:\n * [cpu0: the cpu that switches]\n * 1) switch_mm() either 1a) or 1b)\n * 1a) thread switch to a different mm\n * 1a1) set cpu_tlbstate to TLBSTATE_OK\n *\tNow the tlb flush NMI handler flush_tlb_func won't call leave_mm\n *\tif cpu0 was in lazy tlb mode.\n * 1a2) update cpu active_mm\n *\tNow cpu0 accepts tlb flushes for the new mm.\n * 1a3) cpu_set(cpu, new_mm->cpu_vm_mask);\n *\tNow the other cpus will send tlb flush ipis.\n * 1a4) change cr3.\n * 1a5) cpu_clear(cpu, old_mm->cpu_vm_mask);\n *\tStop ipi delivery for the old mm. This is not synchronized with\n *\tthe other cpus, but flush_tlb_func ignore flush ipis for the wrong\n *\tmm, and in the worst case we perform a superfluous tlb flush.\n * 1b) thread switch without mm change\n *\tcpu active_mm is correct, cpu0 already handles flush ipis.\n * 1b1) set cpu_tlbstate to TLBSTATE_OK\n * 1b2) test_and_set the cpu bit in cpu_vm_mask.\n *\tAtomically set the bit [other cpus will start sending flush ipis],\n *\tand test the bit.\n * 1b3) if the bit was 0: leave_mm was called, flush the tlb.\n * 2) switch %%esp, ie current\n *\n * The interrupt must handle 2 special cases:\n * - cr3 is changed before %%esp, ie. it cannot use current->{active_,}mm.\n * - the cpu performs speculative tlb reads, i.e. even if the cpu only\n *   runs in kernel space, the cpu could load tlb entries for user space\n *   pages.\n *\n * The good news is that cpu_tlbstate is local to each cpu, no\n * write/read ordering problems.\n */\n\n/*\n * TLB flush funcation:\n * 1) Flush the tlb entries if the cpu uses the mm that's being flushed.\n * 2) Leave the mm if we are in the lazy tlb mode.\n */\nstatic void flush_tlb_func(void *info)\n{\n\tstruct flush_tlb_info *f = info;\n\n\tinc_irq_stat(irq_tlb_count);\n\n\tif (f->flush_mm != this_cpu_read(cpu_tlbstate.active_mm))\n\t\treturn;\n\tif (!f->flush_end)\n\t\tf->flush_end = f->flush_start + PAGE_SIZE;\n\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK) {\n\t\tif (f->flush_end == TLB_FLUSH_ALL) {\n\t\t\tlocal_flush_tlb();\n\t\t\ttrace_tlb_flush(TLB_REMOTE_SHOOTDOWN, TLB_FLUSH_ALL);\n\t\t} else {\n\t\t\tunsigned long addr;\n\t\t\tunsigned long nr_pages =\n\t\t\t\t(f->flush_end - f->flush_start) / PAGE_SIZE;\n\t\t\taddr = f->flush_start;\n\t\t\twhile (addr < f->flush_end) {\n\t\t\t\t__flush_tlb_single(addr);\n\t\t\t\taddr += PAGE_SIZE;\n\t\t\t}\n\t\t\ttrace_tlb_flush(TLB_REMOTE_SHOOTDOWN, nr_pages);\n\t\t}\n\t} else\n\t\tleave_mm(smp_processor_id());\n\n}\n\nvoid native_flush_tlb_others(const struct cpumask *cpumask,\n\t\t\t\t struct mm_struct *mm, unsigned long start,\n\t\t\t\t unsigned long end)\n{\n\tstruct flush_tlb_info info;\n\tinfo.flush_mm = mm;\n\tinfo.flush_start = start;\n\tinfo.flush_end = end;\n\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH);\n\ttrace_tlb_flush(TLB_REMOTE_SEND_IPI, end - start);\n\tif (is_uv_system()) {\n\t\tunsigned int cpu;\n\n\t\tcpu = smp_processor_id();\n\t\tcpumask = uv_flush_tlb_others(cpumask, mm, start, end, cpu);\n\t\tif (cpumask)\n\t\t\tsmp_call_function_many(cpumask, flush_tlb_func,\n\t\t\t\t\t\t\t\t&info, 1);\n\t\treturn;\n\t}\n\tsmp_call_function_many(cpumask, flush_tlb_func, &info, 1);\n}\n\nvoid flush_tlb_current_task(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tpreempt_disable();\n\n\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\tlocal_flush_tlb();\n\ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);\n\tpreempt_enable();\n}\n\n/*\n * See Documentation/x86/tlb.txt for details.  We choose 33\n * because it is large enough to cover the vast majority (at\n * least 95%) of allocations, and is small enough that we are\n * confident it will not cause too much overhead.  Each single\n * flush is about 100 ns, so this caps the maximum overhead at\n * _about_ 3,000 ns.\n *\n * This is in units of pages.\n */\nstatic unsigned long tlb_single_page_flush_ceiling __read_mostly = 33;\n\nvoid flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned long vmflag)\n{\n\tunsigned long addr;\n\t/* do a global flush by default */\n\tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n\n\tpreempt_disable();\n\tif (current->active_mm != mm)\n\t\tgoto out;\n\n\tif (!current->mm) {\n\t\tleave_mm(smp_processor_id());\n\t\tgoto out;\n\t}\n\n\tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n\t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n\n\tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n\t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\t\tlocal_flush_tlb();\n\t} else {\n\t\t/* flush range by one by one 'invlpg' */\n\t\tfor (addr = start; addr < end;\taddr += PAGE_SIZE) {\n\t\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);\n\t\t\t__flush_tlb_single(addr);\n\t\t}\n\t}\n\ttrace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);\nout:\n\tif (base_pages_to_flush == TLB_FLUSH_ALL) {\n\t\tstart = 0UL;\n\t\tend = TLB_FLUSH_ALL;\n\t}\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, end);\n\tpreempt_enable();\n}\n\nvoid flush_tlb_page(struct vm_area_struct *vma, unsigned long start)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpreempt_disable();\n\n\tif (current->active_mm == mm) {\n\t\tif (current->mm)\n\t\t\t__flush_tlb_one(start);\n\t\telse\n\t\t\tleave_mm(smp_processor_id());\n\t}\n\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, 0UL);\n\n\tpreempt_enable();\n}\n\nstatic void do_flush_tlb_all(void *info)\n{\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);\n\t__flush_tlb_all();\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_LAZY)\n\t\tleave_mm(smp_processor_id());\n}\n\nvoid flush_tlb_all(void)\n{\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH);\n\ton_each_cpu(do_flush_tlb_all, NULL, 1);\n}\n\nstatic void do_kernel_range_flush(void *info)\n{\n\tstruct flush_tlb_info *f = info;\n\tunsigned long addr;\n\n\t/* flush range by one by one 'invlpg' */\n\tfor (addr = f->flush_start; addr < f->flush_end; addr += PAGE_SIZE)\n\t\t__flush_tlb_single(addr);\n}\n\nvoid flush_tlb_kernel_range(unsigned long start, unsigned long end)\n{\n\n\t/* Balance as user space task's flush, a bit conservative */\n\tif (end == TLB_FLUSH_ALL ||\n\t    (end - start) > tlb_single_page_flush_ceiling * PAGE_SIZE) {\n\t\ton_each_cpu(do_flush_tlb_all, NULL, 1);\n\t} else {\n\t\tstruct flush_tlb_info info;\n\t\tinfo.flush_start = start;\n\t\tinfo.flush_end = end;\n\t\ton_each_cpu(do_kernel_range_flush, &info, 1);\n\t}\n}\n\nstatic ssize_t tlbflush_read_file(struct file *file, char __user *user_buf,\n\t\t\t     size_t count, loff_t *ppos)\n{\n\tchar buf[32];\n\tunsigned int len;\n\n\tlen = sprintf(buf, \"%ld\\n\", tlb_single_page_flush_ceiling);\n\treturn simple_read_from_buffer(user_buf, count, ppos, buf, len);\n}\n\nstatic ssize_t tlbflush_write_file(struct file *file,\n\t\t const char __user *user_buf, size_t count, loff_t *ppos)\n{\n\tchar buf[32];\n\tssize_t len;\n\tint ceiling;\n\n\tlen = min(count, sizeof(buf) - 1);\n\tif (copy_from_user(buf, user_buf, len))\n\t\treturn -EFAULT;\n\n\tbuf[len] = '\\0';\n\tif (kstrtoint(buf, 0, &ceiling))\n\t\treturn -EINVAL;\n\n\tif (ceiling < 0)\n\t\treturn -EINVAL;\n\n\ttlb_single_page_flush_ceiling = ceiling;\n\treturn count;\n}\n\nstatic const struct file_operations fops_tlbflush = {\n\t.read = tlbflush_read_file,\n\t.write = tlbflush_write_file,\n\t.llseek = default_llseek,\n};\n\nstatic int __init create_tlb_single_page_flush_ceiling(void)\n{\n\tdebugfs_create_file(\"tlb_single_page_flush_ceiling\", S_IRUSR | S_IWUSR,\n\t\t\t    arch_debugfs_dir, NULL, &fops_tlbflush);\n\treturn 0;\n}\nlate_initcall(create_tlb_single_page_flush_ceiling);\n"], "fixing_code": ["#ifndef _ASM_X86_MMU_CONTEXT_H\n#define _ASM_X86_MMU_CONTEXT_H\n\n#include <asm/desc.h>\n#include <linux/atomic.h>\n#include <linux/mm_types.h>\n\n#include <trace/events/tlb.h>\n\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include <asm/paravirt.h>\n#include <asm/mpx.h>\n#ifndef CONFIG_PARAVIRT\nstatic inline void paravirt_activate_mm(struct mm_struct *prev,\n\t\t\t\t\tstruct mm_struct *next)\n{\n}\n#endif\t/* !CONFIG_PARAVIRT */\n\n#ifdef CONFIG_PERF_EVENTS\nextern struct static_key rdpmc_always_available;\n\nstatic inline void load_mm_cr4(struct mm_struct *mm)\n{\n\tif (static_key_false(&rdpmc_always_available) ||\n\t    atomic_read(&mm->context.perf_rdpmc_allowed))\n\t\tcr4_set_bits(X86_CR4_PCE);\n\telse\n\t\tcr4_clear_bits(X86_CR4_PCE);\n}\n#else\nstatic inline void load_mm_cr4(struct mm_struct *mm) {}\n#endif\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n/*\n * ldt_structs can be allocated, used, and freed, but they are never\n * modified while live.\n */\nstruct ldt_struct {\n\t/*\n\t * Xen requires page-aligned LDTs with special permissions.  This is\n\t * needed to prevent us from installing evil descriptors such as\n\t * call gates.  On native, we could merge the ldt_struct and LDT\n\t * allocations, but it's not worth trying to optimize.\n\t */\n\tstruct desc_struct *entries;\n\tint size;\n};\n\n/*\n * Used for LDT copy/destruction.\n */\nint init_new_context(struct task_struct *tsk, struct mm_struct *mm);\nvoid destroy_context(struct mm_struct *mm);\n#else\t/* CONFIG_MODIFY_LDT_SYSCALL */\nstatic inline int init_new_context(struct task_struct *tsk,\n\t\t\t\t   struct mm_struct *mm)\n{\n\treturn 0;\n}\nstatic inline void destroy_context(struct mm_struct *mm) {}\n#endif\n\nstatic inline void load_mm_ldt(struct mm_struct *mm)\n{\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\tstruct ldt_struct *ldt;\n\n\t/* lockless_dereference synchronizes with smp_store_release */\n\tldt = lockless_dereference(mm->context.ldt);\n\n\t/*\n\t * Any change to mm->context.ldt is followed by an IPI to all\n\t * CPUs with the mm active.  The LDT will not be freed until\n\t * after the IPI is handled by all such CPUs.  This means that,\n\t * if the ldt_struct changes before we return, the values we see\n\t * will be safe, and the new values will be loaded before we run\n\t * any user code.\n\t *\n\t * NB: don't try to convert this to use RCU without extreme care.\n\t * We would still need IRQs off, because we don't want to change\n\t * the local LDT after an IPI loaded a newer value than the one\n\t * that we can see.\n\t */\n\n\tif (unlikely(ldt))\n\t\tset_ldt(ldt->entries, ldt->size);\n\telse\n\t\tclear_LDT();\n#else\n\tclear_LDT();\n#endif\n\n\tDEBUG_LOCKS_WARN_ON(preemptible());\n}\n\nstatic inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)\n{\n#ifdef CONFIG_SMP\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);\n#endif\n}\n\nstatic inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t\t\t     struct task_struct *tsk)\n{\n\tunsigned cpu = smp_processor_id();\n\n\tif (likely(prev != next)) {\n#ifdef CONFIG_SMP\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tthis_cpu_write(cpu_tlbstate.active_mm, next);\n#endif\n\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t/*\n\t\t * Re-load page tables.\n\t\t *\n\t\t * This logic has an ordering constraint:\n\t\t *\n\t\t *  CPU 0: Write to a PTE for 'next'\n\t\t *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.\n\t\t *  CPU 1: set bit 1 in next's mm_cpumask\n\t\t *  CPU 1: load from the PTE that CPU 0 writes (implicit)\n\t\t *\n\t\t * We need to prevent an outcome in which CPU 1 observes\n\t\t * the new PTE value and CPU 0 observes bit 1 clear in\n\t\t * mm_cpumask.  (If that occurs, then the IPI will never\n\t\t * be sent, and CPU 0's TLB will contain a stale entry.)\n\t\t *\n\t\t * The bad outcome can occur if either CPU's load is\n\t\t * reordered before that CPU's store, so both CPUs much\n\t\t * execute full barriers to prevent this from happening.\n\t\t *\n\t\t * Thus, switch_mm needs a full barrier between the\n\t\t * store to mm_cpumask and any operation that could load\n\t\t * from next->pgd.  This barrier synchronizes with\n\t\t * remote TLB flushers.  Fortunately, load_cr3 is\n\t\t * serializing and thus acts as a full barrier.\n\t\t *\n\t\t */\n\t\tload_cr3(next->pgd);\n\n\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\n\t\t/* Stop flush ipis for the previous mm */\n\t\tcpumask_clear_cpu(cpu, mm_cpumask(prev));\n\n\t\t/* Load per-mm CR4 state */\n\t\tload_mm_cr4(next);\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\t\t/*\n\t\t * Load the LDT, if the LDT is different.\n\t\t *\n\t\t * It's possible that prev->context.ldt doesn't match\n\t\t * the LDT register.  This can happen if leave_mm(prev)\n\t\t * was called and then modify_ldt changed\n\t\t * prev->context.ldt but suppressed an IPI to this CPU.\n\t\t * In this case, prev->context.ldt != NULL, because we\n\t\t * never set context.ldt to NULL while the mm still\n\t\t * exists.  That means that next->context.ldt !=\n\t\t * prev->context.ldt, because mms never share an LDT.\n\t\t */\n\t\tif (unlikely(prev->context.ldt != next->context.ldt))\n\t\t\tload_mm_ldt(next);\n#endif\n\t}\n#ifdef CONFIG_SMP\n\t  else {\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tBUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);\n\n\t\tif (!cpumask_test_cpu(cpu, mm_cpumask(next))) {\n\t\t\t/*\n\t\t\t * On established mms, the mm_cpumask is only changed\n\t\t\t * from irq context, from ptep_clear_flush() while in\n\t\t\t * lazy tlb mode, and here. Irqs are blocked during\n\t\t\t * schedule, protecting us from simultaneous changes.\n\t\t\t */\n\t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t\t/*\n\t\t\t * We were in lazy tlb mode and leave_mm disabled\n\t\t\t * tlb flush IPI delivery. We must reload CR3\n\t\t\t * to make sure to use no freed page tables.\n\t\t\t *\n\t\t\t * As above, this is a barrier that forces\n\t\t\t * TLB repopulation to be ordered after the\n\t\t\t * store to mm_cpumask.\n\t\t\t */\n\t\t\tload_cr3(next->pgd);\n\t\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\t\t\tload_mm_cr4(next);\n\t\t\tload_mm_ldt(next);\n\t\t}\n\t}\n#endif\n}\n\n#define activate_mm(prev, next)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tparavirt_activate_mm((prev), (next));\t\\\n\tswitch_mm((prev), (next), NULL);\t\\\n} while (0);\n\n#ifdef CONFIG_X86_32\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tlazy_load_gs(0);\t\t\t\\\n} while (0)\n#else\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tload_gs_index(0);\t\t\t\\\n\tloadsegment(fs, 0);\t\t\t\\\n} while (0)\n#endif\n\nstatic inline void arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t struct mm_struct *mm)\n{\n\tparavirt_arch_dup_mmap(oldmm, mm);\n}\n\nstatic inline void arch_exit_mmap(struct mm_struct *mm)\n{\n\tparavirt_arch_exit_mmap(mm);\n}\n\n#ifdef CONFIG_X86_64\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn\t!config_enabled(CONFIG_IA32_EMULATION) ||\n\t\t!(mm->context.ia32_compat == TIF_IA32);\n}\n#else\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn false;\n}\n#endif\n\nstatic inline void arch_bprm_mm_init(struct mm_struct *mm,\n\t\tstruct vm_area_struct *vma)\n{\n\tmpx_mm_init(mm);\n}\n\nstatic inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t      unsigned long start, unsigned long end)\n{\n\t/*\n\t * mpx_notify_unmap() goes and reads a rarely-hot\n\t * cacheline in the mm_struct.  That can be expensive\n\t * enough to be seen in profiles.\n\t *\n\t * The mpx_notify_unmap() call and its contents have been\n\t * observed to affect munmap() performance on hardware\n\t * where MPX is not present.\n\t *\n\t * The unlikely() optimizes for the fast case: no MPX\n\t * in the CPU, or no MPX use in the process.  Even if\n\t * we get this wrong (in the unlikely event that MPX\n\t * is widely enabled on some system) the overhead of\n\t * MPX itself (reading bounds tables) is expected to\n\t * overwhelm the overhead of getting this unlikely()\n\t * consistently wrong.\n\t */\n\tif (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))\n\t\tmpx_notify_unmap(mm, vma, start, end);\n}\n\n#endif /* _ASM_X86_MMU_CONTEXT_H */\n", "#include <linux/init.h>\n\n#include <linux/mm.h>\n#include <linux/spinlock.h>\n#include <linux/smp.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/cpu.h>\n\n#include <asm/tlbflush.h>\n#include <asm/mmu_context.h>\n#include <asm/cache.h>\n#include <asm/apic.h>\n#include <asm/uv/uv.h>\n#include <linux/debugfs.h>\n\n/*\n *\tSmarter SMP flushing macros.\n *\t\tc/o Linus Torvalds.\n *\n *\tThese mean you can really definitely utterly forget about\n *\twriting to user space from interrupts. (Its not allowed anyway).\n *\n *\tOptimizations Manfred Spraul <manfred@colorfullife.com>\n *\n *\tMore scalable flush, from Andi Kleen\n *\n *\tImplement flush IPI by CALL_FUNCTION_VECTOR, Alex Shi\n */\n\nstruct flush_tlb_info {\n\tstruct mm_struct *flush_mm;\n\tunsigned long flush_start;\n\tunsigned long flush_end;\n};\n\n/*\n * We cannot call mmdrop() because we are in interrupt context,\n * instead update mm->cpu_vm_mask.\n */\nvoid leave_mm(int cpu)\n{\n\tstruct mm_struct *active_mm = this_cpu_read(cpu_tlbstate.active_mm);\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)\n\t\tBUG();\n\tif (cpumask_test_cpu(cpu, mm_cpumask(active_mm))) {\n\t\tcpumask_clear_cpu(cpu, mm_cpumask(active_mm));\n\t\tload_cr3(swapper_pg_dir);\n\t\t/*\n\t\t * This gets called in the idle path where RCU\n\t\t * functions differently.  Tracing normally\n\t\t * uses RCU, so we have to call the tracepoint\n\t\t * specially here.\n\t\t */\n\t\ttrace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\t}\n}\nEXPORT_SYMBOL_GPL(leave_mm);\n\n/*\n * The flush IPI assumes that a thread switch happens in this order:\n * [cpu0: the cpu that switches]\n * 1) switch_mm() either 1a) or 1b)\n * 1a) thread switch to a different mm\n * 1a1) set cpu_tlbstate to TLBSTATE_OK\n *\tNow the tlb flush NMI handler flush_tlb_func won't call leave_mm\n *\tif cpu0 was in lazy tlb mode.\n * 1a2) update cpu active_mm\n *\tNow cpu0 accepts tlb flushes for the new mm.\n * 1a3) cpu_set(cpu, new_mm->cpu_vm_mask);\n *\tNow the other cpus will send tlb flush ipis.\n * 1a4) change cr3.\n * 1a5) cpu_clear(cpu, old_mm->cpu_vm_mask);\n *\tStop ipi delivery for the old mm. This is not synchronized with\n *\tthe other cpus, but flush_tlb_func ignore flush ipis for the wrong\n *\tmm, and in the worst case we perform a superfluous tlb flush.\n * 1b) thread switch without mm change\n *\tcpu active_mm is correct, cpu0 already handles flush ipis.\n * 1b1) set cpu_tlbstate to TLBSTATE_OK\n * 1b2) test_and_set the cpu bit in cpu_vm_mask.\n *\tAtomically set the bit [other cpus will start sending flush ipis],\n *\tand test the bit.\n * 1b3) if the bit was 0: leave_mm was called, flush the tlb.\n * 2) switch %%esp, ie current\n *\n * The interrupt must handle 2 special cases:\n * - cr3 is changed before %%esp, ie. it cannot use current->{active_,}mm.\n * - the cpu performs speculative tlb reads, i.e. even if the cpu only\n *   runs in kernel space, the cpu could load tlb entries for user space\n *   pages.\n *\n * The good news is that cpu_tlbstate is local to each cpu, no\n * write/read ordering problems.\n */\n\n/*\n * TLB flush funcation:\n * 1) Flush the tlb entries if the cpu uses the mm that's being flushed.\n * 2) Leave the mm if we are in the lazy tlb mode.\n */\nstatic void flush_tlb_func(void *info)\n{\n\tstruct flush_tlb_info *f = info;\n\n\tinc_irq_stat(irq_tlb_count);\n\n\tif (f->flush_mm != this_cpu_read(cpu_tlbstate.active_mm))\n\t\treturn;\n\tif (!f->flush_end)\n\t\tf->flush_end = f->flush_start + PAGE_SIZE;\n\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK) {\n\t\tif (f->flush_end == TLB_FLUSH_ALL) {\n\t\t\tlocal_flush_tlb();\n\t\t\ttrace_tlb_flush(TLB_REMOTE_SHOOTDOWN, TLB_FLUSH_ALL);\n\t\t} else {\n\t\t\tunsigned long addr;\n\t\t\tunsigned long nr_pages =\n\t\t\t\t(f->flush_end - f->flush_start) / PAGE_SIZE;\n\t\t\taddr = f->flush_start;\n\t\t\twhile (addr < f->flush_end) {\n\t\t\t\t__flush_tlb_single(addr);\n\t\t\t\taddr += PAGE_SIZE;\n\t\t\t}\n\t\t\ttrace_tlb_flush(TLB_REMOTE_SHOOTDOWN, nr_pages);\n\t\t}\n\t} else\n\t\tleave_mm(smp_processor_id());\n\n}\n\nvoid native_flush_tlb_others(const struct cpumask *cpumask,\n\t\t\t\t struct mm_struct *mm, unsigned long start,\n\t\t\t\t unsigned long end)\n{\n\tstruct flush_tlb_info info;\n\tinfo.flush_mm = mm;\n\tinfo.flush_start = start;\n\tinfo.flush_end = end;\n\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH);\n\ttrace_tlb_flush(TLB_REMOTE_SEND_IPI, end - start);\n\tif (is_uv_system()) {\n\t\tunsigned int cpu;\n\n\t\tcpu = smp_processor_id();\n\t\tcpumask = uv_flush_tlb_others(cpumask, mm, start, end, cpu);\n\t\tif (cpumask)\n\t\t\tsmp_call_function_many(cpumask, flush_tlb_func,\n\t\t\t\t\t\t\t\t&info, 1);\n\t\treturn;\n\t}\n\tsmp_call_function_many(cpumask, flush_tlb_func, &info, 1);\n}\n\nvoid flush_tlb_current_task(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tpreempt_disable();\n\n\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\n\t/* This is an implicit full barrier that synchronizes with switch_mm. */\n\tlocal_flush_tlb();\n\n\ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);\n\tpreempt_enable();\n}\n\n/*\n * See Documentation/x86/tlb.txt for details.  We choose 33\n * because it is large enough to cover the vast majority (at\n * least 95%) of allocations, and is small enough that we are\n * confident it will not cause too much overhead.  Each single\n * flush is about 100 ns, so this caps the maximum overhead at\n * _about_ 3,000 ns.\n *\n * This is in units of pages.\n */\nstatic unsigned long tlb_single_page_flush_ceiling __read_mostly = 33;\n\nvoid flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned long vmflag)\n{\n\tunsigned long addr;\n\t/* do a global flush by default */\n\tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n\n\tpreempt_disable();\n\tif (current->active_mm != mm) {\n\t\t/* Synchronize with switch_mm. */\n\t\tsmp_mb();\n\n\t\tgoto out;\n\t}\n\n\tif (!current->mm) {\n\t\tleave_mm(smp_processor_id());\n\n\t\t/* Synchronize with switch_mm. */\n\t\tsmp_mb();\n\n\t\tgoto out;\n\t}\n\n\tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n\t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n\n\t/*\n\t * Both branches below are implicit full barriers (MOV to CR or\n\t * INVLPG) that synchronize with switch_mm.\n\t */\n\tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n\t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\t\tlocal_flush_tlb();\n\t} else {\n\t\t/* flush range by one by one 'invlpg' */\n\t\tfor (addr = start; addr < end;\taddr += PAGE_SIZE) {\n\t\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);\n\t\t\t__flush_tlb_single(addr);\n\t\t}\n\t}\n\ttrace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);\nout:\n\tif (base_pages_to_flush == TLB_FLUSH_ALL) {\n\t\tstart = 0UL;\n\t\tend = TLB_FLUSH_ALL;\n\t}\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, end);\n\tpreempt_enable();\n}\n\nvoid flush_tlb_page(struct vm_area_struct *vma, unsigned long start)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpreempt_disable();\n\n\tif (current->active_mm == mm) {\n\t\tif (current->mm) {\n\t\t\t/*\n\t\t\t * Implicit full barrier (INVLPG) that synchronizes\n\t\t\t * with switch_mm.\n\t\t\t */\n\t\t\t__flush_tlb_one(start);\n\t\t} else {\n\t\t\tleave_mm(smp_processor_id());\n\n\t\t\t/* Synchronize with switch_mm. */\n\t\t\tsmp_mb();\n\t\t}\n\t}\n\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, 0UL);\n\n\tpreempt_enable();\n}\n\nstatic void do_flush_tlb_all(void *info)\n{\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);\n\t__flush_tlb_all();\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_LAZY)\n\t\tleave_mm(smp_processor_id());\n}\n\nvoid flush_tlb_all(void)\n{\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH);\n\ton_each_cpu(do_flush_tlb_all, NULL, 1);\n}\n\nstatic void do_kernel_range_flush(void *info)\n{\n\tstruct flush_tlb_info *f = info;\n\tunsigned long addr;\n\n\t/* flush range by one by one 'invlpg' */\n\tfor (addr = f->flush_start; addr < f->flush_end; addr += PAGE_SIZE)\n\t\t__flush_tlb_single(addr);\n}\n\nvoid flush_tlb_kernel_range(unsigned long start, unsigned long end)\n{\n\n\t/* Balance as user space task's flush, a bit conservative */\n\tif (end == TLB_FLUSH_ALL ||\n\t    (end - start) > tlb_single_page_flush_ceiling * PAGE_SIZE) {\n\t\ton_each_cpu(do_flush_tlb_all, NULL, 1);\n\t} else {\n\t\tstruct flush_tlb_info info;\n\t\tinfo.flush_start = start;\n\t\tinfo.flush_end = end;\n\t\ton_each_cpu(do_kernel_range_flush, &info, 1);\n\t}\n}\n\nstatic ssize_t tlbflush_read_file(struct file *file, char __user *user_buf,\n\t\t\t     size_t count, loff_t *ppos)\n{\n\tchar buf[32];\n\tunsigned int len;\n\n\tlen = sprintf(buf, \"%ld\\n\", tlb_single_page_flush_ceiling);\n\treturn simple_read_from_buffer(user_buf, count, ppos, buf, len);\n}\n\nstatic ssize_t tlbflush_write_file(struct file *file,\n\t\t const char __user *user_buf, size_t count, loff_t *ppos)\n{\n\tchar buf[32];\n\tssize_t len;\n\tint ceiling;\n\n\tlen = min(count, sizeof(buf) - 1);\n\tif (copy_from_user(buf, user_buf, len))\n\t\treturn -EFAULT;\n\n\tbuf[len] = '\\0';\n\tif (kstrtoint(buf, 0, &ceiling))\n\t\treturn -EINVAL;\n\n\tif (ceiling < 0)\n\t\treturn -EINVAL;\n\n\ttlb_single_page_flush_ceiling = ceiling;\n\treturn count;\n}\n\nstatic const struct file_operations fops_tlbflush = {\n\t.read = tlbflush_read_file,\n\t.write = tlbflush_write_file,\n\t.llseek = default_llseek,\n};\n\nstatic int __init create_tlb_single_page_flush_ceiling(void)\n{\n\tdebugfs_create_file(\"tlb_single_page_flush_ceiling\", S_IRUSR | S_IWUSR,\n\t\t\t    arch_debugfs_dir, NULL, &fops_tlbflush);\n\treturn 0;\n}\nlate_initcall(create_tlb_single_page_flush_ceiling);\n"], "filenames": ["arch/x86/include/asm/mmu_context.h", "arch/x86/mm/tlb.c"], "buggy_code_start_loc": [119, 163], "buggy_code_end_loc": [162, 234], "fixing_code_start_loc": [119, 164], "fixing_code_end_loc": [194, 258], "type": "CWE-362", "message": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU.", "other": {"cve": {"id": "CVE-2016-2069", "sourceIdentifier": "cve@mitre.org", "published": "2016-04-27T17:59:07.007", "lastModified": "2018-01-05T02:30:35.823", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU."}, {"lang": "es", "value": "Condici\u00f3n de carrera en arch/x86/mm/tlb.c en el kernel de Linux en versiones anteriores a 4.4.1 permite a usuarios locales obtener privilegios desencadenando el acceso a una estructura de paginaci\u00f3n por un CPU diferente."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.4, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.4, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B6B7CAD7-9D4E-4FDB-88E3-1E583210A01F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.4", "matchCriteriaId": "8B458ACF-17C3-4551-9F11-8D02B6D52B7C"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=71b3c126e61177eb693423f2e18a1914205b165e", "source": "cve@mitre.org", "tags": ["Vendor Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-03/msg00094.html", "source": "cve@mitre.org"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00015.html", "source": "cve@mitre.org"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-04/msg00045.html", "source": "cve@mitre.org"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-08/msg00038.html", "source": "cve@mitre.org"}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2574.html", "source": "cve@mitre.org"}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2584.html", "source": "cve@mitre.org"}, {"url": "http://rhn.redhat.com/errata/RHSA-2017-0817.html", "source": "cve@mitre.org"}, {"url": "http://www.debian.org/security/2016/dsa-3503", "source": "cve@mitre.org"}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.4.1", "source": "cve@mitre.org"}, {"url": "http://www.openwall.com/lists/oss-security/2016/01/25/1", "source": "cve@mitre.org"}, {"url": "http://www.oracle.com/technetwork/topics/security/linuxbulletinjul2016-3090544.html", "source": "cve@mitre.org"}, {"url": "http://www.oracle.com/technetwork/topics/security/ovmbulletinoct2016-3090547.html", "source": "cve@mitre.org"}, {"url": "http://www.securityfocus.com/bid/81809", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-2931-1", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-2932-1", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-2967-1", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-2967-2", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-2989-1", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-2998-1", "source": "cve@mitre.org"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1301893", "source": "cve@mitre.org"}, {"url": "https://github.com/torvalds/linux/commit/71b3c126e61177eb693423f2e18a1914205b165e", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/71b3c126e61177eb693423f2e18a1914205b165e"}}