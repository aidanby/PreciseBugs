{"buggy_code": ["/* Copyright (C) 2009 Red Hat, Inc.\n * Author: Michael S. Tsirkin <mst@redhat.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.\n *\n * virtio-net server in host kernel.\n */\n\n#include <linux/compat.h>\n#include <linux/eventfd.h>\n#include <linux/vhost.h>\n#include <linux/virtio_net.h>\n#include <linux/miscdevice.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/mutex.h>\n#include <linux/workqueue.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n\n#include <linux/net.h>\n#include <linux/if_packet.h>\n#include <linux/if_arp.h>\n#include <linux/if_tun.h>\n#include <linux/if_macvlan.h>\n#include <linux/if_vlan.h>\n\n#include <net/sock.h>\n\n#include \"vhost.h\"\n\nstatic int experimental_zcopytx = 1;\nmodule_param(experimental_zcopytx, int, 0444);\nMODULE_PARM_DESC(experimental_zcopytx, \"Enable Zero Copy TX;\"\n\t\t                       \" 1 -Enable; 0 - Disable\");\n\n/* Max number of bytes transferred before requeueing the job.\n * Using this limit prevents one virtqueue from starving others. */\n#define VHOST_NET_WEIGHT 0x80000\n\n/* MAX number of TX used buffers for outstanding zerocopy */\n#define VHOST_MAX_PEND 128\n#define VHOST_GOODCOPY_LEN 256\n\n/*\n * For transmit, used buffer len is unused; we override it to track buffer\n * status internally; used for zerocopy tx only.\n */\n/* Lower device DMA failed */\n#define VHOST_DMA_FAILED_LEN\t3\n/* Lower device DMA done */\n#define VHOST_DMA_DONE_LEN\t2\n/* Lower device DMA in progress */\n#define VHOST_DMA_IN_PROGRESS\t1\n/* Buffer unused */\n#define VHOST_DMA_CLEAR_LEN\t0\n\n#define VHOST_DMA_IS_DONE(len) ((len) >= VHOST_DMA_DONE_LEN)\n\nenum {\n\tVHOST_NET_FEATURES = VHOST_FEATURES |\n\t\t\t (1ULL << VHOST_NET_F_VIRTIO_NET_HDR) |\n\t\t\t (1ULL << VIRTIO_NET_F_MRG_RXBUF),\n};\n\nenum {\n\tVHOST_NET_VQ_RX = 0,\n\tVHOST_NET_VQ_TX = 1,\n\tVHOST_NET_VQ_MAX = 2,\n};\n\nstruct vhost_net_ubuf_ref {\n\t/* refcount follows semantics similar to kref:\n\t *  0: object is released\n\t *  1: no outstanding ubufs\n\t * >1: outstanding ubufs\n\t */\n\tatomic_t refcount;\n\twait_queue_head_t wait;\n\tstruct vhost_virtqueue *vq;\n};\n\nstruct vhost_net_virtqueue {\n\tstruct vhost_virtqueue vq;\n\t/* hdr is used to store the virtio header.\n\t * Since each iovec has >= 1 byte length, we never need more than\n\t * header length entries to store the header. */\n\tstruct iovec hdr[sizeof(struct virtio_net_hdr_mrg_rxbuf)];\n\tsize_t vhost_hlen;\n\tsize_t sock_hlen;\n\t/* vhost zerocopy support fields below: */\n\t/* last used idx for outstanding DMA zerocopy buffers */\n\tint upend_idx;\n\t/* first used idx for DMA done zerocopy buffers */\n\tint done_idx;\n\t/* an array of userspace buffers info */\n\tstruct ubuf_info *ubuf_info;\n\t/* Reference counting for outstanding ubufs.\n\t * Protected by vq mutex. Writers must also take device mutex. */\n\tstruct vhost_net_ubuf_ref *ubufs;\n};\n\nstruct vhost_net {\n\tstruct vhost_dev dev;\n\tstruct vhost_net_virtqueue vqs[VHOST_NET_VQ_MAX];\n\tstruct vhost_poll poll[VHOST_NET_VQ_MAX];\n\t/* Number of TX recently submitted.\n\t * Protected by tx vq lock. */\n\tunsigned tx_packets;\n\t/* Number of times zerocopy TX recently failed.\n\t * Protected by tx vq lock. */\n\tunsigned tx_zcopy_err;\n\t/* Flush in progress. Protected by tx vq lock. */\n\tbool tx_flush;\n};\n\nstatic unsigned vhost_net_zcopy_mask __read_mostly;\n\nstatic void vhost_net_enable_zcopy(int vq)\n{\n\tvhost_net_zcopy_mask |= 0x1 << vq;\n}\n\nstatic struct vhost_net_ubuf_ref *\nvhost_net_ubuf_alloc(struct vhost_virtqueue *vq, bool zcopy)\n{\n\tstruct vhost_net_ubuf_ref *ubufs;\n\t/* No zero copy backend? Nothing to count. */\n\tif (!zcopy)\n\t\treturn NULL;\n\tubufs = kmalloc(sizeof(*ubufs), GFP_KERNEL);\n\tif (!ubufs)\n\t\treturn ERR_PTR(-ENOMEM);\n\tatomic_set(&ubufs->refcount, 1);\n\tinit_waitqueue_head(&ubufs->wait);\n\tubufs->vq = vq;\n\treturn ubufs;\n}\n\nstatic int vhost_net_ubuf_put(struct vhost_net_ubuf_ref *ubufs)\n{\n\tint r = atomic_sub_return(1, &ubufs->refcount);\n\tif (unlikely(!r))\n\t\twake_up(&ubufs->wait);\n\treturn r;\n}\n\nstatic void vhost_net_ubuf_put_and_wait(struct vhost_net_ubuf_ref *ubufs)\n{\n\tvhost_net_ubuf_put(ubufs);\n\twait_event(ubufs->wait, !atomic_read(&ubufs->refcount));\n}\n\nstatic void vhost_net_ubuf_put_wait_and_free(struct vhost_net_ubuf_ref *ubufs)\n{\n\tvhost_net_ubuf_put_and_wait(ubufs);\n\tkfree(ubufs);\n}\n\nstatic void vhost_net_clear_ubuf_info(struct vhost_net *n)\n{\n\tint i;\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; ++i) {\n\t\tkfree(n->vqs[i].ubuf_info);\n\t\tn->vqs[i].ubuf_info = NULL;\n\t}\n}\n\nstatic int vhost_net_set_ubuf_info(struct vhost_net *n)\n{\n\tbool zcopy;\n\tint i;\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; ++i) {\n\t\tzcopy = vhost_net_zcopy_mask & (0x1 << i);\n\t\tif (!zcopy)\n\t\t\tcontinue;\n\t\tn->vqs[i].ubuf_info = kmalloc(sizeof(*n->vqs[i].ubuf_info) *\n\t\t\t\t\t      UIO_MAXIOV, GFP_KERNEL);\n\t\tif  (!n->vqs[i].ubuf_info)\n\t\t\tgoto err;\n\t}\n\treturn 0;\n\nerr:\n\tvhost_net_clear_ubuf_info(n);\n\treturn -ENOMEM;\n}\n\nstatic void vhost_net_vq_reset(struct vhost_net *n)\n{\n\tint i;\n\n\tvhost_net_clear_ubuf_info(n);\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\n\t\tn->vqs[i].done_idx = 0;\n\t\tn->vqs[i].upend_idx = 0;\n\t\tn->vqs[i].ubufs = NULL;\n\t\tn->vqs[i].vhost_hlen = 0;\n\t\tn->vqs[i].sock_hlen = 0;\n\t}\n\n}\n\nstatic void vhost_net_tx_packet(struct vhost_net *net)\n{\n\t++net->tx_packets;\n\tif (net->tx_packets < 1024)\n\t\treturn;\n\tnet->tx_packets = 0;\n\tnet->tx_zcopy_err = 0;\n}\n\nstatic void vhost_net_tx_err(struct vhost_net *net)\n{\n\t++net->tx_zcopy_err;\n}\n\nstatic bool vhost_net_tx_select_zcopy(struct vhost_net *net)\n{\n\t/* TX flush waits for outstanding DMAs to be done.\n\t * Don't start new DMAs.\n\t */\n\treturn !net->tx_flush &&\n\t\tnet->tx_packets / 64 >= net->tx_zcopy_err;\n}\n\nstatic bool vhost_sock_zcopy(struct socket *sock)\n{\n\treturn unlikely(experimental_zcopytx) &&\n\t\tsock_flag(sock->sk, SOCK_ZEROCOPY);\n}\n\n/* Pop first len bytes from iovec. Return number of segments used. */\nstatic int move_iovec_hdr(struct iovec *from, struct iovec *to,\n\t\t\t  size_t len, int iov_count)\n{\n\tint seg = 0;\n\tsize_t size;\n\n\twhile (len && seg < iov_count) {\n\t\tsize = min(from->iov_len, len);\n\t\tto->iov_base = from->iov_base;\n\t\tto->iov_len = size;\n\t\tfrom->iov_len -= size;\n\t\tfrom->iov_base += size;\n\t\tlen -= size;\n\t\t++from;\n\t\t++to;\n\t\t++seg;\n\t}\n\treturn seg;\n}\n/* Copy iovec entries for len bytes from iovec. */\nstatic void copy_iovec_hdr(const struct iovec *from, struct iovec *to,\n\t\t\t   size_t len, int iovcount)\n{\n\tint seg = 0;\n\tsize_t size;\n\n\twhile (len && seg < iovcount) {\n\t\tsize = min(from->iov_len, len);\n\t\tto->iov_base = from->iov_base;\n\t\tto->iov_len = size;\n\t\tlen -= size;\n\t\t++from;\n\t\t++to;\n\t\t++seg;\n\t}\n}\n\n/* In case of DMA done not in order in lower device driver for some reason.\n * upend_idx is used to track end of used idx, done_idx is used to track head\n * of used idx. Once lower device DMA done contiguously, we will signal KVM\n * guest used idx.\n */\nstatic void vhost_zerocopy_signal_used(struct vhost_net *net,\n\t\t\t\t       struct vhost_virtqueue *vq)\n{\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\tint i, add;\n\tint j = 0;\n\n\tfor (i = nvq->done_idx; i != nvq->upend_idx; i = (i + 1) % UIO_MAXIOV) {\n\t\tif (vq->heads[i].len == VHOST_DMA_FAILED_LEN)\n\t\t\tvhost_net_tx_err(net);\n\t\tif (VHOST_DMA_IS_DONE(vq->heads[i].len)) {\n\t\t\tvq->heads[i].len = VHOST_DMA_CLEAR_LEN;\n\t\t\t++j;\n\t\t} else\n\t\t\tbreak;\n\t}\n\twhile (j) {\n\t\tadd = min(UIO_MAXIOV - nvq->done_idx, j);\n\t\tvhost_add_used_and_signal_n(vq->dev, vq,\n\t\t\t\t\t    &vq->heads[nvq->done_idx], add);\n\t\tnvq->done_idx = (nvq->done_idx + add) % UIO_MAXIOV;\n\t\tj -= add;\n\t}\n}\n\nstatic void vhost_zerocopy_callback(struct ubuf_info *ubuf, bool success)\n{\n\tstruct vhost_net_ubuf_ref *ubufs = ubuf->ctx;\n\tstruct vhost_virtqueue *vq = ubufs->vq;\n\tint cnt;\n\n\trcu_read_lock_bh();\n\n\t/* set len to mark this desc buffers done DMA */\n\tvq->heads[ubuf->desc].len = success ?\n\t\tVHOST_DMA_DONE_LEN : VHOST_DMA_FAILED_LEN;\n\tcnt = vhost_net_ubuf_put(ubufs);\n\n\t/*\n\t * Trigger polling thread if guest stopped submitting new buffers:\n\t * in this case, the refcount after decrement will eventually reach 1.\n\t * We also trigger polling periodically after each 16 packets\n\t * (the value 16 here is more or less arbitrary, it's tuned to trigger\n\t * less than 10% of times).\n\t */\n\tif (cnt <= 1 || !(cnt % 16))\n\t\tvhost_poll_queue(&vq->poll);\n\n\trcu_read_unlock_bh();\n}\n\n/* Expects to be always run from workqueue - which acts as\n * read-size critical section for our kind of RCU. */\nstatic void handle_tx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned out, in, s;\n\tint head;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL,\n\t\t.msg_controllen = 0,\n\t\t.msg_iov = vq->iov,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tsize_t len, total_len = 0;\n\tint err;\n\tsize_t hdr_size;\n\tstruct socket *sock;\n\tstruct vhost_net_ubuf_ref *uninitialized_var(ubufs);\n\tbool zcopy, zcopy_used;\n\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tif (!sock)\n\t\tgoto out;\n\n\tvhost_disable_notify(&net->dev, vq);\n\n\thdr_size = nvq->vhost_hlen;\n\tzcopy = nvq->ubufs;\n\n\tfor (;;) {\n\t\t/* Release DMAs done buffers first */\n\t\tif (zcopy)\n\t\t\tvhost_zerocopy_signal_used(net, vq);\n\n\t\t/* If more outstanding DMAs, queue the work.\n\t\t * Handle upend_idx wrap around\n\t\t */\n\t\tif (unlikely((nvq->upend_idx + vq->num - VHOST_MAX_PEND)\n\t\t\t      % UIO_MAXIOV == nvq->done_idx))\n\t\t\tbreak;\n\n\t\thead = vhost_get_vq_desc(&net->dev, vq, vq->iov,\n\t\t\t\t\t ARRAY_SIZE(vq->iov),\n\t\t\t\t\t &out, &in,\n\t\t\t\t\t NULL, NULL);\n\t\t/* On error, stop handling until the next kick. */\n\t\tif (unlikely(head < 0))\n\t\t\tbreak;\n\t\t/* Nothing new?  Wait for eventfd to tell us they refilled. */\n\t\tif (head == vq->num) {\n\t\t\tif (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tif (in) {\n\t\t\tvq_err(vq, \"Unexpected descriptor format for TX: \"\n\t\t\t       \"out %d, int %d\\n\", out, in);\n\t\t\tbreak;\n\t\t}\n\t\t/* Skip header. TODO: support TSO. */\n\t\ts = move_iovec_hdr(vq->iov, nvq->hdr, hdr_size, out);\n\t\tmsg.msg_iovlen = out;\n\t\tlen = iov_length(vq->iov, out);\n\t\t/* Sanity check */\n\t\tif (!len) {\n\t\t\tvq_err(vq, \"Unexpected header len for TX: \"\n\t\t\t       \"%zd expected %zd\\n\",\n\t\t\t       iov_length(nvq->hdr, s), hdr_size);\n\t\t\tbreak;\n\t\t}\n\n\t\tzcopy_used = zcopy && len >= VHOST_GOODCOPY_LEN\n\t\t\t\t   && (nvq->upend_idx + 1) % UIO_MAXIOV !=\n\t\t\t\t      nvq->done_idx\n\t\t\t\t   && vhost_net_tx_select_zcopy(net);\n\n\t\t/* use msg_control to pass vhost zerocopy ubuf info to skb */\n\t\tif (zcopy_used) {\n\t\t\tstruct ubuf_info *ubuf;\n\t\t\tubuf = nvq->ubuf_info + nvq->upend_idx;\n\n\t\t\tvq->heads[nvq->upend_idx].id = head;\n\t\t\tvq->heads[nvq->upend_idx].len = VHOST_DMA_IN_PROGRESS;\n\t\t\tubuf->callback = vhost_zerocopy_callback;\n\t\t\tubuf->ctx = nvq->ubufs;\n\t\t\tubuf->desc = nvq->upend_idx;\n\t\t\tmsg.msg_control = ubuf;\n\t\t\tmsg.msg_controllen = sizeof(ubuf);\n\t\t\tubufs = nvq->ubufs;\n\t\t\tatomic_inc(&ubufs->refcount);\n\t\t\tnvq->upend_idx = (nvq->upend_idx + 1) % UIO_MAXIOV;\n\t\t} else {\n\t\t\tmsg.msg_control = NULL;\n\t\t\tubufs = NULL;\n\t\t}\n\t\t/* TODO: Check specific error and bomb out unless ENOBUFS? */\n\t\terr = sock->ops->sendmsg(NULL, sock, &msg, len);\n\t\tif (unlikely(err < 0)) {\n\t\t\tif (zcopy_used) {\n\t\t\t\tvhost_net_ubuf_put(ubufs);\n\t\t\t\tnvq->upend_idx = ((unsigned)nvq->upend_idx - 1)\n\t\t\t\t\t% UIO_MAXIOV;\n\t\t\t}\n\t\t\tvhost_discard_vq_desc(vq, 1);\n\t\t\tbreak;\n\t\t}\n\t\tif (err != len)\n\t\t\tpr_debug(\"Truncated TX packet: \"\n\t\t\t\t \" len %d != %zd\\n\", err, len);\n\t\tif (!zcopy_used)\n\t\t\tvhost_add_used_and_signal(&net->dev, vq, head, 0);\n\t\telse\n\t\t\tvhost_zerocopy_signal_used(net, vq);\n\t\ttotal_len += len;\n\t\tvhost_net_tx_packet(net);\n\t\tif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\n\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&vq->mutex);\n}\n\nstatic int peek_head_len(struct sock *sk)\n{\n\tstruct sk_buff *head;\n\tint len = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sk->sk_receive_queue.lock, flags);\n\thead = skb_peek(&sk->sk_receive_queue);\n\tif (likely(head)) {\n\t\tlen = head->len;\n\t\tif (vlan_tx_tag_present(head))\n\t\t\tlen += VLAN_HLEN;\n\t}\n\n\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, flags);\n\treturn len;\n}\n\n/* This is a multi-buffer version of vhost_get_desc, that works if\n *\tvq has read descriptors only.\n * @vq\t\t- the relevant virtqueue\n * @datalen\t- data length we'll be reading\n * @iovcount\t- returned count of io vectors we fill\n * @log\t\t- vhost log\n * @log_num\t- log offset\n * @quota       - headcount quota, 1 for big buffer\n *\treturns number of buffer heads allocated, negative on error\n */\nstatic int get_rx_bufs(struct vhost_virtqueue *vq,\n\t\t       struct vring_used_elem *heads,\n\t\t       int datalen,\n\t\t       unsigned *iovcount,\n\t\t       struct vhost_log *log,\n\t\t       unsigned *log_num,\n\t\t       unsigned int quota)\n{\n\tunsigned int out, in;\n\tint seg = 0;\n\tint headcount = 0;\n\tunsigned d;\n\tint r, nlogs = 0;\n\n\twhile (datalen > 0 && headcount < quota) {\n\t\tif (unlikely(seg >= UIO_MAXIOV)) {\n\t\t\tr = -ENOBUFS;\n\t\t\tgoto err;\n\t\t}\n\t\td = vhost_get_vq_desc(vq->dev, vq, vq->iov + seg,\n\t\t\t\t      ARRAY_SIZE(vq->iov) - seg, &out,\n\t\t\t\t      &in, log, log_num);\n\t\tif (d == vq->num) {\n\t\t\tr = 0;\n\t\t\tgoto err;\n\t\t}\n\t\tif (unlikely(out || in <= 0)) {\n\t\t\tvq_err(vq, \"unexpected descriptor format for RX: \"\n\t\t\t\t\"out %d, in %d\\n\", out, in);\n\t\t\tr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tif (unlikely(log)) {\n\t\t\tnlogs += *log_num;\n\t\t\tlog += *log_num;\n\t\t}\n\t\theads[headcount].id = d;\n\t\theads[headcount].len = iov_length(vq->iov + seg, in);\n\t\tdatalen -= heads[headcount].len;\n\t\t++headcount;\n\t\tseg += in;\n\t}\n\theads[headcount - 1].len += datalen;\n\t*iovcount = seg;\n\tif (unlikely(log))\n\t\t*log_num = nlogs;\n\treturn headcount;\nerr:\n\tvhost_discard_vq_desc(vq, headcount);\n\treturn r;\n}\n\n/* Expects to be always run from workqueue - which acts as\n * read-size critical section for our kind of RCU. */\nstatic void handle_rx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned uninitialized_var(in), log;\n\tstruct vhost_log *vq_log;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL, /* FIXME: get and handle RX aux data. */\n\t\t.msg_controllen = 0,\n\t\t.msg_iov = vq->iov,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tstruct virtio_net_hdr_mrg_rxbuf hdr = {\n\t\t.hdr.flags = 0,\n\t\t.hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE\n\t};\n\tsize_t total_len = 0;\n\tint err, mergeable;\n\ts16 headcount;\n\tsize_t vhost_hlen, sock_hlen;\n\tsize_t vhost_len, sock_len;\n\tstruct socket *sock;\n\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tif (!sock)\n\t\tgoto out;\n\tvhost_disable_notify(&net->dev, vq);\n\n\tvhost_hlen = nvq->vhost_hlen;\n\tsock_hlen = nvq->sock_hlen;\n\n\tvq_log = unlikely(vhost_has_feature(&net->dev, VHOST_F_LOG_ALL)) ?\n\t\tvq->log : NULL;\n\tmergeable = vhost_has_feature(&net->dev, VIRTIO_NET_F_MRG_RXBUF);\n\n\twhile ((sock_len = peek_head_len(sock->sk))) {\n\t\tsock_len += sock_hlen;\n\t\tvhost_len = sock_len + vhost_hlen;\n\t\theadcount = get_rx_bufs(vq, vq->heads, vhost_len,\n\t\t\t\t\t&in, vq_log, &log,\n\t\t\t\t\tlikely(mergeable) ? UIO_MAXIOV : 1);\n\t\t/* On error, stop handling until the next kick. */\n\t\tif (unlikely(headcount < 0))\n\t\t\tbreak;\n\t\t/* OK, now we need to know about added descriptors. */\n\t\tif (!headcount) {\n\t\t\tif (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\t/* They have slipped one in as we were\n\t\t\t\t * doing that: check again. */\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/* Nothing new?  Wait for eventfd to tell us\n\t\t\t * they refilled. */\n\t\t\tbreak;\n\t\t}\n\t\t/* We don't need to be notified again. */\n\t\tif (unlikely((vhost_hlen)))\n\t\t\t/* Skip header. TODO: support TSO. */\n\t\t\tmove_iovec_hdr(vq->iov, nvq->hdr, vhost_hlen, in);\n\t\telse\n\t\t\t/* Copy the header for use in VIRTIO_NET_F_MRG_RXBUF:\n\t\t\t * needed because recvmsg can modify msg_iov. */\n\t\t\tcopy_iovec_hdr(vq->iov, nvq->hdr, sock_hlen, in);\n\t\tmsg.msg_iovlen = in;\n\t\terr = sock->ops->recvmsg(NULL, sock, &msg,\n\t\t\t\t\t sock_len, MSG_DONTWAIT | MSG_TRUNC);\n\t\t/* Userspace might have consumed the packet meanwhile:\n\t\t * it's not supposed to do this usually, but might be hard\n\t\t * to prevent. Discard data we got (if any) and keep going. */\n\t\tif (unlikely(err != sock_len)) {\n\t\t\tpr_debug(\"Discarded rx packet: \"\n\t\t\t\t \" len %d, expected %zd\\n\", err, sock_len);\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(vhost_hlen) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&hdr, 0,\n\t\t\t\t      vhost_hlen)) {\n\t\t\tvq_err(vq, \"Unable to write vnet_hdr at addr %p\\n\",\n\t\t\t       vq->iov->iov_base);\n\t\t\tbreak;\n\t\t}\n\t\t/* TODO: Should check and handle checksum. */\n\t\tif (likely(mergeable) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&headcount,\n\t\t\t\t      offsetof(typeof(hdr), num_buffers),\n\t\t\t\t      sizeof hdr.num_buffers)) {\n\t\t\tvq_err(vq, \"Failed num_buffers write\");\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tbreak;\n\t\t}\n\t\tvhost_add_used_and_signal_n(&net->dev, vq, vq->heads,\n\t\t\t\t\t    headcount);\n\t\tif (unlikely(vq_log))\n\t\t\tvhost_log_write(vq, vq_log, log, vhost_len);\n\t\ttotal_len += vhost_len;\n\t\tif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\n\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&vq->mutex);\n}\n\nstatic void handle_tx_kick(struct vhost_work *work)\n{\n\tstruct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,\n\t\t\t\t\t\t  poll.work);\n\tstruct vhost_net *net = container_of(vq->dev, struct vhost_net, dev);\n\n\thandle_tx(net);\n}\n\nstatic void handle_rx_kick(struct vhost_work *work)\n{\n\tstruct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,\n\t\t\t\t\t\t  poll.work);\n\tstruct vhost_net *net = container_of(vq->dev, struct vhost_net, dev);\n\n\thandle_rx(net);\n}\n\nstatic void handle_tx_net(struct vhost_work *work)\n{\n\tstruct vhost_net *net = container_of(work, struct vhost_net,\n\t\t\t\t\t     poll[VHOST_NET_VQ_TX].work);\n\thandle_tx(net);\n}\n\nstatic void handle_rx_net(struct vhost_work *work)\n{\n\tstruct vhost_net *net = container_of(work, struct vhost_net,\n\t\t\t\t\t     poll[VHOST_NET_VQ_RX].work);\n\thandle_rx(net);\n}\n\nstatic int vhost_net_open(struct inode *inode, struct file *f)\n{\n\tstruct vhost_net *n = kmalloc(sizeof *n, GFP_KERNEL);\n\tstruct vhost_dev *dev;\n\tstruct vhost_virtqueue **vqs;\n\tint i;\n\n\tif (!n)\n\t\treturn -ENOMEM;\n\tvqs = kmalloc(VHOST_NET_VQ_MAX * sizeof(*vqs), GFP_KERNEL);\n\tif (!vqs) {\n\t\tkfree(n);\n\t\treturn -ENOMEM;\n\t}\n\n\tdev = &n->dev;\n\tvqs[VHOST_NET_VQ_TX] = &n->vqs[VHOST_NET_VQ_TX].vq;\n\tvqs[VHOST_NET_VQ_RX] = &n->vqs[VHOST_NET_VQ_RX].vq;\n\tn->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;\n\tn->vqs[VHOST_NET_VQ_RX].vq.handle_kick = handle_rx_kick;\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\n\t\tn->vqs[i].ubufs = NULL;\n\t\tn->vqs[i].ubuf_info = NULL;\n\t\tn->vqs[i].upend_idx = 0;\n\t\tn->vqs[i].done_idx = 0;\n\t\tn->vqs[i].vhost_hlen = 0;\n\t\tn->vqs[i].sock_hlen = 0;\n\t}\n\tvhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX);\n\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, POLLOUT, dev);\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, POLLIN, dev);\n\n\tf->private_data = n;\n\n\treturn 0;\n}\n\nstatic void vhost_net_disable_vq(struct vhost_net *n,\n\t\t\t\t struct vhost_virtqueue *vq)\n{\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\tstruct vhost_poll *poll = n->poll + (nvq - n->vqs);\n\tif (!vq->private_data)\n\t\treturn;\n\tvhost_poll_stop(poll);\n}\n\nstatic int vhost_net_enable_vq(struct vhost_net *n,\n\t\t\t\tstruct vhost_virtqueue *vq)\n{\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\tstruct vhost_poll *poll = n->poll + (nvq - n->vqs);\n\tstruct socket *sock;\n\n\tsock = vq->private_data;\n\tif (!sock)\n\t\treturn 0;\n\n\treturn vhost_poll_start(poll, sock->file);\n}\n\nstatic struct socket *vhost_net_stop_vq(struct vhost_net *n,\n\t\t\t\t\tstruct vhost_virtqueue *vq)\n{\n\tstruct socket *sock;\n\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tvhost_net_disable_vq(n, vq);\n\tvq->private_data = NULL;\n\tmutex_unlock(&vq->mutex);\n\treturn sock;\n}\n\nstatic void vhost_net_stop(struct vhost_net *n, struct socket **tx_sock,\n\t\t\t   struct socket **rx_sock)\n{\n\t*tx_sock = vhost_net_stop_vq(n, &n->vqs[VHOST_NET_VQ_TX].vq);\n\t*rx_sock = vhost_net_stop_vq(n, &n->vqs[VHOST_NET_VQ_RX].vq);\n}\n\nstatic void vhost_net_flush_vq(struct vhost_net *n, int index)\n{\n\tvhost_poll_flush(n->poll + index);\n\tvhost_poll_flush(&n->vqs[index].vq.poll);\n}\n\nstatic void vhost_net_flush(struct vhost_net *n)\n{\n\tvhost_net_flush_vq(n, VHOST_NET_VQ_TX);\n\tvhost_net_flush_vq(n, VHOST_NET_VQ_RX);\n\tif (n->vqs[VHOST_NET_VQ_TX].ubufs) {\n\t\tmutex_lock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t\tn->tx_flush = true;\n\t\tmutex_unlock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t\t/* Wait for all lower device DMAs done. */\n\t\tvhost_net_ubuf_put_and_wait(n->vqs[VHOST_NET_VQ_TX].ubufs);\n\t\tmutex_lock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t\tn->tx_flush = false;\n\t\tatomic_set(&n->vqs[VHOST_NET_VQ_TX].ubufs->refcount, 1);\n\t\tmutex_unlock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t}\n}\n\nstatic int vhost_net_release(struct inode *inode, struct file *f)\n{\n\tstruct vhost_net *n = f->private_data;\n\tstruct socket *tx_sock;\n\tstruct socket *rx_sock;\n\n\tvhost_net_stop(n, &tx_sock, &rx_sock);\n\tvhost_net_flush(n);\n\tvhost_dev_stop(&n->dev);\n\tvhost_dev_cleanup(&n->dev, false);\n\tvhost_net_vq_reset(n);\n\tif (tx_sock)\n\t\tfput(tx_sock->file);\n\tif (rx_sock)\n\t\tfput(rx_sock->file);\n\t/* Make sure no callbacks are outstanding */\n\tsynchronize_rcu_bh();\n\t/* We do an extra flush before freeing memory,\n\t * since jobs can re-queue themselves. */\n\tvhost_net_flush(n);\n\tkfree(n->dev.vqs);\n\tkfree(n);\n\treturn 0;\n}\n\nstatic struct socket *get_raw_socket(int fd)\n{\n\tstruct {\n\t\tstruct sockaddr_ll sa;\n\t\tchar  buf[MAX_ADDR_LEN];\n\t} uaddr;\n\tint uaddr_len = sizeof uaddr, r;\n\tstruct socket *sock = sockfd_lookup(fd, &r);\n\n\tif (!sock)\n\t\treturn ERR_PTR(-ENOTSOCK);\n\n\t/* Parameter checking */\n\tif (sock->sk->sk_type != SOCK_RAW) {\n\t\tr = -ESOCKTNOSUPPORT;\n\t\tgoto err;\n\t}\n\n\tr = sock->ops->getname(sock, (struct sockaddr *)&uaddr.sa,\n\t\t\t       &uaddr_len, 0);\n\tif (r)\n\t\tgoto err;\n\n\tif (uaddr.sa.sll_family != AF_PACKET) {\n\t\tr = -EPFNOSUPPORT;\n\t\tgoto err;\n\t}\n\treturn sock;\nerr:\n\tfput(sock->file);\n\treturn ERR_PTR(r);\n}\n\nstatic struct socket *get_tap_socket(int fd)\n{\n\tstruct file *file = fget(fd);\n\tstruct socket *sock;\n\n\tif (!file)\n\t\treturn ERR_PTR(-EBADF);\n\tsock = tun_get_socket(file);\n\tif (!IS_ERR(sock))\n\t\treturn sock;\n\tsock = macvtap_get_socket(file);\n\tif (IS_ERR(sock))\n\t\tfput(file);\n\treturn sock;\n}\n\nstatic struct socket *get_socket(int fd)\n{\n\tstruct socket *sock;\n\n\t/* special case to disable backend */\n\tif (fd == -1)\n\t\treturn NULL;\n\tsock = get_raw_socket(fd);\n\tif (!IS_ERR(sock))\n\t\treturn sock;\n\tsock = get_tap_socket(fd);\n\tif (!IS_ERR(sock))\n\t\treturn sock;\n\treturn ERR_PTR(-ENOTSOCK);\n}\n\nstatic long vhost_net_set_backend(struct vhost_net *n, unsigned index, int fd)\n{\n\tstruct socket *sock, *oldsock;\n\tstruct vhost_virtqueue *vq;\n\tstruct vhost_net_virtqueue *nvq;\n\tstruct vhost_net_ubuf_ref *ubufs, *oldubufs = NULL;\n\tint r;\n\n\tmutex_lock(&n->dev.mutex);\n\tr = vhost_dev_check_owner(&n->dev);\n\tif (r)\n\t\tgoto err;\n\n\tif (index >= VHOST_NET_VQ_MAX) {\n\t\tr = -ENOBUFS;\n\t\tgoto err;\n\t}\n\tvq = &n->vqs[index].vq;\n\tnvq = &n->vqs[index];\n\tmutex_lock(&vq->mutex);\n\n\t/* Verify that ring has been setup correctly. */\n\tif (!vhost_vq_access_ok(vq)) {\n\t\tr = -EFAULT;\n\t\tgoto err_vq;\n\t}\n\tsock = get_socket(fd);\n\tif (IS_ERR(sock)) {\n\t\tr = PTR_ERR(sock);\n\t\tgoto err_vq;\n\t}\n\n\t/* start polling new socket */\n\toldsock = vq->private_data;\n\tif (sock != oldsock) {\n\t\tubufs = vhost_net_ubuf_alloc(vq,\n\t\t\t\t\t     sock && vhost_sock_zcopy(sock));\n\t\tif (IS_ERR(ubufs)) {\n\t\t\tr = PTR_ERR(ubufs);\n\t\t\tgoto err_ubufs;\n\t\t}\n\n\t\tvhost_net_disable_vq(n, vq);\n\t\tvq->private_data = sock;\n\t\tr = vhost_init_used(vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\t\tr = vhost_net_enable_vq(n, vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\n\t\toldubufs = nvq->ubufs;\n\t\tnvq->ubufs = ubufs;\n\n\t\tn->tx_packets = 0;\n\t\tn->tx_zcopy_err = 0;\n\t\tn->tx_flush = false;\n\t}\n\n\tmutex_unlock(&vq->mutex);\n\n\tif (oldubufs) {\n\t\tvhost_net_ubuf_put_wait_and_free(oldubufs);\n\t\tmutex_lock(&vq->mutex);\n\t\tvhost_zerocopy_signal_used(n, vq);\n\t\tmutex_unlock(&vq->mutex);\n\t}\n\n\tif (oldsock) {\n\t\tvhost_net_flush_vq(n, index);\n\t\tfput(oldsock->file);\n\t}\n\n\tmutex_unlock(&n->dev.mutex);\n\treturn 0;\n\nerr_used:\n\tvq->private_data = oldsock;\n\tvhost_net_enable_vq(n, vq);\n\tif (ubufs)\n\t\tvhost_net_ubuf_put_wait_and_free(ubufs);\nerr_ubufs:\n\tfput(sock->file);\nerr_vq:\n\tmutex_unlock(&vq->mutex);\nerr:\n\tmutex_unlock(&n->dev.mutex);\n\treturn r;\n}\n\nstatic long vhost_net_reset_owner(struct vhost_net *n)\n{\n\tstruct socket *tx_sock = NULL;\n\tstruct socket *rx_sock = NULL;\n\tlong err;\n\tstruct vhost_memory *memory;\n\n\tmutex_lock(&n->dev.mutex);\n\terr = vhost_dev_check_owner(&n->dev);\n\tif (err)\n\t\tgoto done;\n\tmemory = vhost_dev_reset_owner_prepare();\n\tif (!memory) {\n\t\terr = -ENOMEM;\n\t\tgoto done;\n\t}\n\tvhost_net_stop(n, &tx_sock, &rx_sock);\n\tvhost_net_flush(n);\n\tvhost_dev_reset_owner(&n->dev, memory);\n\tvhost_net_vq_reset(n);\ndone:\n\tmutex_unlock(&n->dev.mutex);\n\tif (tx_sock)\n\t\tfput(tx_sock->file);\n\tif (rx_sock)\n\t\tfput(rx_sock->file);\n\treturn err;\n}\n\nstatic int vhost_net_set_features(struct vhost_net *n, u64 features)\n{\n\tsize_t vhost_hlen, sock_hlen, hdr_len;\n\tint i;\n\n\thdr_len = (features & (1 << VIRTIO_NET_F_MRG_RXBUF)) ?\n\t\t\tsizeof(struct virtio_net_hdr_mrg_rxbuf) :\n\t\t\tsizeof(struct virtio_net_hdr);\n\tif (features & (1 << VHOST_NET_F_VIRTIO_NET_HDR)) {\n\t\t/* vhost provides vnet_hdr */\n\t\tvhost_hlen = hdr_len;\n\t\tsock_hlen = 0;\n\t} else {\n\t\t/* socket provides vnet_hdr */\n\t\tvhost_hlen = 0;\n\t\tsock_hlen = hdr_len;\n\t}\n\tmutex_lock(&n->dev.mutex);\n\tif ((features & (1 << VHOST_F_LOG_ALL)) &&\n\t    !vhost_log_access_ok(&n->dev)) {\n\t\tmutex_unlock(&n->dev.mutex);\n\t\treturn -EFAULT;\n\t}\n\tn->dev.acked_features = features;\n\tsmp_wmb();\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; ++i) {\n\t\tmutex_lock(&n->vqs[i].vq.mutex);\n\t\tn->vqs[i].vhost_hlen = vhost_hlen;\n\t\tn->vqs[i].sock_hlen = sock_hlen;\n\t\tmutex_unlock(&n->vqs[i].vq.mutex);\n\t}\n\tvhost_net_flush(n);\n\tmutex_unlock(&n->dev.mutex);\n\treturn 0;\n}\n\nstatic long vhost_net_set_owner(struct vhost_net *n)\n{\n\tint r;\n\n\tmutex_lock(&n->dev.mutex);\n\tif (vhost_dev_has_owner(&n->dev)) {\n\t\tr = -EBUSY;\n\t\tgoto out;\n\t}\n\tr = vhost_net_set_ubuf_info(n);\n\tif (r)\n\t\tgoto out;\n\tr = vhost_dev_set_owner(&n->dev);\n\tif (r)\n\t\tvhost_net_clear_ubuf_info(n);\n\tvhost_net_flush(n);\nout:\n\tmutex_unlock(&n->dev.mutex);\n\treturn r;\n}\n\nstatic long vhost_net_ioctl(struct file *f, unsigned int ioctl,\n\t\t\t    unsigned long arg)\n{\n\tstruct vhost_net *n = f->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tu64 __user *featurep = argp;\n\tstruct vhost_vring_file backend;\n\tu64 features;\n\tint r;\n\n\tswitch (ioctl) {\n\tcase VHOST_NET_SET_BACKEND:\n\t\tif (copy_from_user(&backend, argp, sizeof backend))\n\t\t\treturn -EFAULT;\n\t\treturn vhost_net_set_backend(n, backend.index, backend.fd);\n\tcase VHOST_GET_FEATURES:\n\t\tfeatures = VHOST_NET_FEATURES;\n\t\tif (copy_to_user(featurep, &features, sizeof features))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase VHOST_SET_FEATURES:\n\t\tif (copy_from_user(&features, featurep, sizeof features))\n\t\t\treturn -EFAULT;\n\t\tif (features & ~VHOST_NET_FEATURES)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn vhost_net_set_features(n, features);\n\tcase VHOST_RESET_OWNER:\n\t\treturn vhost_net_reset_owner(n);\n\tcase VHOST_SET_OWNER:\n\t\treturn vhost_net_set_owner(n);\n\tdefault:\n\t\tmutex_lock(&n->dev.mutex);\n\t\tr = vhost_dev_ioctl(&n->dev, ioctl, argp);\n\t\tif (r == -ENOIOCTLCMD)\n\t\t\tr = vhost_vring_ioctl(&n->dev, ioctl, argp);\n\t\telse\n\t\t\tvhost_net_flush(n);\n\t\tmutex_unlock(&n->dev.mutex);\n\t\treturn r;\n\t}\n}\n\n#ifdef CONFIG_COMPAT\nstatic long vhost_net_compat_ioctl(struct file *f, unsigned int ioctl,\n\t\t\t\t   unsigned long arg)\n{\n\treturn vhost_net_ioctl(f, ioctl, (unsigned long)compat_ptr(arg));\n}\n#endif\n\nstatic const struct file_operations vhost_net_fops = {\n\t.owner          = THIS_MODULE,\n\t.release        = vhost_net_release,\n\t.unlocked_ioctl = vhost_net_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl   = vhost_net_compat_ioctl,\n#endif\n\t.open           = vhost_net_open,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic struct miscdevice vhost_net_misc = {\n\t.minor = VHOST_NET_MINOR,\n\t.name = \"vhost-net\",\n\t.fops = &vhost_net_fops,\n};\n\nstatic int vhost_net_init(void)\n{\n\tif (experimental_zcopytx)\n\t\tvhost_net_enable_zcopy(VHOST_NET_VQ_TX);\n\treturn misc_register(&vhost_net_misc);\n}\nmodule_init(vhost_net_init);\n\nstatic void vhost_net_exit(void)\n{\n\tmisc_deregister(&vhost_net_misc);\n}\nmodule_exit(vhost_net_exit);\n\nMODULE_VERSION(\"0.0.1\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Michael S. Tsirkin\");\nMODULE_DESCRIPTION(\"Host kernel accelerator for virtio net\");\nMODULE_ALIAS_MISCDEV(VHOST_NET_MINOR);\nMODULE_ALIAS(\"devname:vhost-net\");\n"], "fixing_code": ["/* Copyright (C) 2009 Red Hat, Inc.\n * Author: Michael S. Tsirkin <mst@redhat.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.\n *\n * virtio-net server in host kernel.\n */\n\n#include <linux/compat.h>\n#include <linux/eventfd.h>\n#include <linux/vhost.h>\n#include <linux/virtio_net.h>\n#include <linux/miscdevice.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/mutex.h>\n#include <linux/workqueue.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n\n#include <linux/net.h>\n#include <linux/if_packet.h>\n#include <linux/if_arp.h>\n#include <linux/if_tun.h>\n#include <linux/if_macvlan.h>\n#include <linux/if_vlan.h>\n\n#include <net/sock.h>\n\n#include \"vhost.h\"\n\nstatic int experimental_zcopytx = 1;\nmodule_param(experimental_zcopytx, int, 0444);\nMODULE_PARM_DESC(experimental_zcopytx, \"Enable Zero Copy TX;\"\n\t\t                       \" 1 -Enable; 0 - Disable\");\n\n/* Max number of bytes transferred before requeueing the job.\n * Using this limit prevents one virtqueue from starving others. */\n#define VHOST_NET_WEIGHT 0x80000\n\n/* MAX number of TX used buffers for outstanding zerocopy */\n#define VHOST_MAX_PEND 128\n#define VHOST_GOODCOPY_LEN 256\n\n/*\n * For transmit, used buffer len is unused; we override it to track buffer\n * status internally; used for zerocopy tx only.\n */\n/* Lower device DMA failed */\n#define VHOST_DMA_FAILED_LEN\t3\n/* Lower device DMA done */\n#define VHOST_DMA_DONE_LEN\t2\n/* Lower device DMA in progress */\n#define VHOST_DMA_IN_PROGRESS\t1\n/* Buffer unused */\n#define VHOST_DMA_CLEAR_LEN\t0\n\n#define VHOST_DMA_IS_DONE(len) ((len) >= VHOST_DMA_DONE_LEN)\n\nenum {\n\tVHOST_NET_FEATURES = VHOST_FEATURES |\n\t\t\t (1ULL << VHOST_NET_F_VIRTIO_NET_HDR) |\n\t\t\t (1ULL << VIRTIO_NET_F_MRG_RXBUF),\n};\n\nenum {\n\tVHOST_NET_VQ_RX = 0,\n\tVHOST_NET_VQ_TX = 1,\n\tVHOST_NET_VQ_MAX = 2,\n};\n\nstruct vhost_net_ubuf_ref {\n\t/* refcount follows semantics similar to kref:\n\t *  0: object is released\n\t *  1: no outstanding ubufs\n\t * >1: outstanding ubufs\n\t */\n\tatomic_t refcount;\n\twait_queue_head_t wait;\n\tstruct vhost_virtqueue *vq;\n};\n\nstruct vhost_net_virtqueue {\n\tstruct vhost_virtqueue vq;\n\t/* hdr is used to store the virtio header.\n\t * Since each iovec has >= 1 byte length, we never need more than\n\t * header length entries to store the header. */\n\tstruct iovec hdr[sizeof(struct virtio_net_hdr_mrg_rxbuf)];\n\tsize_t vhost_hlen;\n\tsize_t sock_hlen;\n\t/* vhost zerocopy support fields below: */\n\t/* last used idx for outstanding DMA zerocopy buffers */\n\tint upend_idx;\n\t/* first used idx for DMA done zerocopy buffers */\n\tint done_idx;\n\t/* an array of userspace buffers info */\n\tstruct ubuf_info *ubuf_info;\n\t/* Reference counting for outstanding ubufs.\n\t * Protected by vq mutex. Writers must also take device mutex. */\n\tstruct vhost_net_ubuf_ref *ubufs;\n};\n\nstruct vhost_net {\n\tstruct vhost_dev dev;\n\tstruct vhost_net_virtqueue vqs[VHOST_NET_VQ_MAX];\n\tstruct vhost_poll poll[VHOST_NET_VQ_MAX];\n\t/* Number of TX recently submitted.\n\t * Protected by tx vq lock. */\n\tunsigned tx_packets;\n\t/* Number of times zerocopy TX recently failed.\n\t * Protected by tx vq lock. */\n\tunsigned tx_zcopy_err;\n\t/* Flush in progress. Protected by tx vq lock. */\n\tbool tx_flush;\n};\n\nstatic unsigned vhost_net_zcopy_mask __read_mostly;\n\nstatic void vhost_net_enable_zcopy(int vq)\n{\n\tvhost_net_zcopy_mask |= 0x1 << vq;\n}\n\nstatic struct vhost_net_ubuf_ref *\nvhost_net_ubuf_alloc(struct vhost_virtqueue *vq, bool zcopy)\n{\n\tstruct vhost_net_ubuf_ref *ubufs;\n\t/* No zero copy backend? Nothing to count. */\n\tif (!zcopy)\n\t\treturn NULL;\n\tubufs = kmalloc(sizeof(*ubufs), GFP_KERNEL);\n\tif (!ubufs)\n\t\treturn ERR_PTR(-ENOMEM);\n\tatomic_set(&ubufs->refcount, 1);\n\tinit_waitqueue_head(&ubufs->wait);\n\tubufs->vq = vq;\n\treturn ubufs;\n}\n\nstatic int vhost_net_ubuf_put(struct vhost_net_ubuf_ref *ubufs)\n{\n\tint r = atomic_sub_return(1, &ubufs->refcount);\n\tif (unlikely(!r))\n\t\twake_up(&ubufs->wait);\n\treturn r;\n}\n\nstatic void vhost_net_ubuf_put_and_wait(struct vhost_net_ubuf_ref *ubufs)\n{\n\tvhost_net_ubuf_put(ubufs);\n\twait_event(ubufs->wait, !atomic_read(&ubufs->refcount));\n}\n\nstatic void vhost_net_ubuf_put_wait_and_free(struct vhost_net_ubuf_ref *ubufs)\n{\n\tvhost_net_ubuf_put_and_wait(ubufs);\n\tkfree(ubufs);\n}\n\nstatic void vhost_net_clear_ubuf_info(struct vhost_net *n)\n{\n\tint i;\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; ++i) {\n\t\tkfree(n->vqs[i].ubuf_info);\n\t\tn->vqs[i].ubuf_info = NULL;\n\t}\n}\n\nstatic int vhost_net_set_ubuf_info(struct vhost_net *n)\n{\n\tbool zcopy;\n\tint i;\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; ++i) {\n\t\tzcopy = vhost_net_zcopy_mask & (0x1 << i);\n\t\tif (!zcopy)\n\t\t\tcontinue;\n\t\tn->vqs[i].ubuf_info = kmalloc(sizeof(*n->vqs[i].ubuf_info) *\n\t\t\t\t\t      UIO_MAXIOV, GFP_KERNEL);\n\t\tif  (!n->vqs[i].ubuf_info)\n\t\t\tgoto err;\n\t}\n\treturn 0;\n\nerr:\n\tvhost_net_clear_ubuf_info(n);\n\treturn -ENOMEM;\n}\n\nstatic void vhost_net_vq_reset(struct vhost_net *n)\n{\n\tint i;\n\n\tvhost_net_clear_ubuf_info(n);\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\n\t\tn->vqs[i].done_idx = 0;\n\t\tn->vqs[i].upend_idx = 0;\n\t\tn->vqs[i].ubufs = NULL;\n\t\tn->vqs[i].vhost_hlen = 0;\n\t\tn->vqs[i].sock_hlen = 0;\n\t}\n\n}\n\nstatic void vhost_net_tx_packet(struct vhost_net *net)\n{\n\t++net->tx_packets;\n\tif (net->tx_packets < 1024)\n\t\treturn;\n\tnet->tx_packets = 0;\n\tnet->tx_zcopy_err = 0;\n}\n\nstatic void vhost_net_tx_err(struct vhost_net *net)\n{\n\t++net->tx_zcopy_err;\n}\n\nstatic bool vhost_net_tx_select_zcopy(struct vhost_net *net)\n{\n\t/* TX flush waits for outstanding DMAs to be done.\n\t * Don't start new DMAs.\n\t */\n\treturn !net->tx_flush &&\n\t\tnet->tx_packets / 64 >= net->tx_zcopy_err;\n}\n\nstatic bool vhost_sock_zcopy(struct socket *sock)\n{\n\treturn unlikely(experimental_zcopytx) &&\n\t\tsock_flag(sock->sk, SOCK_ZEROCOPY);\n}\n\n/* Pop first len bytes from iovec. Return number of segments used. */\nstatic int move_iovec_hdr(struct iovec *from, struct iovec *to,\n\t\t\t  size_t len, int iov_count)\n{\n\tint seg = 0;\n\tsize_t size;\n\n\twhile (len && seg < iov_count) {\n\t\tsize = min(from->iov_len, len);\n\t\tto->iov_base = from->iov_base;\n\t\tto->iov_len = size;\n\t\tfrom->iov_len -= size;\n\t\tfrom->iov_base += size;\n\t\tlen -= size;\n\t\t++from;\n\t\t++to;\n\t\t++seg;\n\t}\n\treturn seg;\n}\n/* Copy iovec entries for len bytes from iovec. */\nstatic void copy_iovec_hdr(const struct iovec *from, struct iovec *to,\n\t\t\t   size_t len, int iovcount)\n{\n\tint seg = 0;\n\tsize_t size;\n\n\twhile (len && seg < iovcount) {\n\t\tsize = min(from->iov_len, len);\n\t\tto->iov_base = from->iov_base;\n\t\tto->iov_len = size;\n\t\tlen -= size;\n\t\t++from;\n\t\t++to;\n\t\t++seg;\n\t}\n}\n\n/* In case of DMA done not in order in lower device driver for some reason.\n * upend_idx is used to track end of used idx, done_idx is used to track head\n * of used idx. Once lower device DMA done contiguously, we will signal KVM\n * guest used idx.\n */\nstatic void vhost_zerocopy_signal_used(struct vhost_net *net,\n\t\t\t\t       struct vhost_virtqueue *vq)\n{\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\tint i, add;\n\tint j = 0;\n\n\tfor (i = nvq->done_idx; i != nvq->upend_idx; i = (i + 1) % UIO_MAXIOV) {\n\t\tif (vq->heads[i].len == VHOST_DMA_FAILED_LEN)\n\t\t\tvhost_net_tx_err(net);\n\t\tif (VHOST_DMA_IS_DONE(vq->heads[i].len)) {\n\t\t\tvq->heads[i].len = VHOST_DMA_CLEAR_LEN;\n\t\t\t++j;\n\t\t} else\n\t\t\tbreak;\n\t}\n\twhile (j) {\n\t\tadd = min(UIO_MAXIOV - nvq->done_idx, j);\n\t\tvhost_add_used_and_signal_n(vq->dev, vq,\n\t\t\t\t\t    &vq->heads[nvq->done_idx], add);\n\t\tnvq->done_idx = (nvq->done_idx + add) % UIO_MAXIOV;\n\t\tj -= add;\n\t}\n}\n\nstatic void vhost_zerocopy_callback(struct ubuf_info *ubuf, bool success)\n{\n\tstruct vhost_net_ubuf_ref *ubufs = ubuf->ctx;\n\tstruct vhost_virtqueue *vq = ubufs->vq;\n\tint cnt;\n\n\trcu_read_lock_bh();\n\n\t/* set len to mark this desc buffers done DMA */\n\tvq->heads[ubuf->desc].len = success ?\n\t\tVHOST_DMA_DONE_LEN : VHOST_DMA_FAILED_LEN;\n\tcnt = vhost_net_ubuf_put(ubufs);\n\n\t/*\n\t * Trigger polling thread if guest stopped submitting new buffers:\n\t * in this case, the refcount after decrement will eventually reach 1.\n\t * We also trigger polling periodically after each 16 packets\n\t * (the value 16 here is more or less arbitrary, it's tuned to trigger\n\t * less than 10% of times).\n\t */\n\tif (cnt <= 1 || !(cnt % 16))\n\t\tvhost_poll_queue(&vq->poll);\n\n\trcu_read_unlock_bh();\n}\n\n/* Expects to be always run from workqueue - which acts as\n * read-size critical section for our kind of RCU. */\nstatic void handle_tx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned out, in, s;\n\tint head;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL,\n\t\t.msg_controllen = 0,\n\t\t.msg_iov = vq->iov,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tsize_t len, total_len = 0;\n\tint err;\n\tsize_t hdr_size;\n\tstruct socket *sock;\n\tstruct vhost_net_ubuf_ref *uninitialized_var(ubufs);\n\tbool zcopy, zcopy_used;\n\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tif (!sock)\n\t\tgoto out;\n\n\tvhost_disable_notify(&net->dev, vq);\n\n\thdr_size = nvq->vhost_hlen;\n\tzcopy = nvq->ubufs;\n\n\tfor (;;) {\n\t\t/* Release DMAs done buffers first */\n\t\tif (zcopy)\n\t\t\tvhost_zerocopy_signal_used(net, vq);\n\n\t\t/* If more outstanding DMAs, queue the work.\n\t\t * Handle upend_idx wrap around\n\t\t */\n\t\tif (unlikely((nvq->upend_idx + vq->num - VHOST_MAX_PEND)\n\t\t\t      % UIO_MAXIOV == nvq->done_idx))\n\t\t\tbreak;\n\n\t\thead = vhost_get_vq_desc(&net->dev, vq, vq->iov,\n\t\t\t\t\t ARRAY_SIZE(vq->iov),\n\t\t\t\t\t &out, &in,\n\t\t\t\t\t NULL, NULL);\n\t\t/* On error, stop handling until the next kick. */\n\t\tif (unlikely(head < 0))\n\t\t\tbreak;\n\t\t/* Nothing new?  Wait for eventfd to tell us they refilled. */\n\t\tif (head == vq->num) {\n\t\t\tif (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tif (in) {\n\t\t\tvq_err(vq, \"Unexpected descriptor format for TX: \"\n\t\t\t       \"out %d, int %d\\n\", out, in);\n\t\t\tbreak;\n\t\t}\n\t\t/* Skip header. TODO: support TSO. */\n\t\ts = move_iovec_hdr(vq->iov, nvq->hdr, hdr_size, out);\n\t\tmsg.msg_iovlen = out;\n\t\tlen = iov_length(vq->iov, out);\n\t\t/* Sanity check */\n\t\tif (!len) {\n\t\t\tvq_err(vq, \"Unexpected header len for TX: \"\n\t\t\t       \"%zd expected %zd\\n\",\n\t\t\t       iov_length(nvq->hdr, s), hdr_size);\n\t\t\tbreak;\n\t\t}\n\n\t\tzcopy_used = zcopy && len >= VHOST_GOODCOPY_LEN\n\t\t\t\t   && (nvq->upend_idx + 1) % UIO_MAXIOV !=\n\t\t\t\t      nvq->done_idx\n\t\t\t\t   && vhost_net_tx_select_zcopy(net);\n\n\t\t/* use msg_control to pass vhost zerocopy ubuf info to skb */\n\t\tif (zcopy_used) {\n\t\t\tstruct ubuf_info *ubuf;\n\t\t\tubuf = nvq->ubuf_info + nvq->upend_idx;\n\n\t\t\tvq->heads[nvq->upend_idx].id = head;\n\t\t\tvq->heads[nvq->upend_idx].len = VHOST_DMA_IN_PROGRESS;\n\t\t\tubuf->callback = vhost_zerocopy_callback;\n\t\t\tubuf->ctx = nvq->ubufs;\n\t\t\tubuf->desc = nvq->upend_idx;\n\t\t\tmsg.msg_control = ubuf;\n\t\t\tmsg.msg_controllen = sizeof(ubuf);\n\t\t\tubufs = nvq->ubufs;\n\t\t\tatomic_inc(&ubufs->refcount);\n\t\t\tnvq->upend_idx = (nvq->upend_idx + 1) % UIO_MAXIOV;\n\t\t} else {\n\t\t\tmsg.msg_control = NULL;\n\t\t\tubufs = NULL;\n\t\t}\n\t\t/* TODO: Check specific error and bomb out unless ENOBUFS? */\n\t\terr = sock->ops->sendmsg(NULL, sock, &msg, len);\n\t\tif (unlikely(err < 0)) {\n\t\t\tif (zcopy_used) {\n\t\t\t\tvhost_net_ubuf_put(ubufs);\n\t\t\t\tnvq->upend_idx = ((unsigned)nvq->upend_idx - 1)\n\t\t\t\t\t% UIO_MAXIOV;\n\t\t\t}\n\t\t\tvhost_discard_vq_desc(vq, 1);\n\t\t\tbreak;\n\t\t}\n\t\tif (err != len)\n\t\t\tpr_debug(\"Truncated TX packet: \"\n\t\t\t\t \" len %d != %zd\\n\", err, len);\n\t\tif (!zcopy_used)\n\t\t\tvhost_add_used_and_signal(&net->dev, vq, head, 0);\n\t\telse\n\t\t\tvhost_zerocopy_signal_used(net, vq);\n\t\ttotal_len += len;\n\t\tvhost_net_tx_packet(net);\n\t\tif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\n\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&vq->mutex);\n}\n\nstatic int peek_head_len(struct sock *sk)\n{\n\tstruct sk_buff *head;\n\tint len = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sk->sk_receive_queue.lock, flags);\n\thead = skb_peek(&sk->sk_receive_queue);\n\tif (likely(head)) {\n\t\tlen = head->len;\n\t\tif (vlan_tx_tag_present(head))\n\t\t\tlen += VLAN_HLEN;\n\t}\n\n\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, flags);\n\treturn len;\n}\n\n/* This is a multi-buffer version of vhost_get_desc, that works if\n *\tvq has read descriptors only.\n * @vq\t\t- the relevant virtqueue\n * @datalen\t- data length we'll be reading\n * @iovcount\t- returned count of io vectors we fill\n * @log\t\t- vhost log\n * @log_num\t- log offset\n * @quota       - headcount quota, 1 for big buffer\n *\treturns number of buffer heads allocated, negative on error\n */\nstatic int get_rx_bufs(struct vhost_virtqueue *vq,\n\t\t       struct vring_used_elem *heads,\n\t\t       int datalen,\n\t\t       unsigned *iovcount,\n\t\t       struct vhost_log *log,\n\t\t       unsigned *log_num,\n\t\t       unsigned int quota)\n{\n\tunsigned int out, in;\n\tint seg = 0;\n\tint headcount = 0;\n\tunsigned d;\n\tint r, nlogs = 0;\n\n\twhile (datalen > 0 && headcount < quota) {\n\t\tif (unlikely(seg >= UIO_MAXIOV)) {\n\t\t\tr = -ENOBUFS;\n\t\t\tgoto err;\n\t\t}\n\t\td = vhost_get_vq_desc(vq->dev, vq, vq->iov + seg,\n\t\t\t\t      ARRAY_SIZE(vq->iov) - seg, &out,\n\t\t\t\t      &in, log, log_num);\n\t\tif (d == vq->num) {\n\t\t\tr = 0;\n\t\t\tgoto err;\n\t\t}\n\t\tif (unlikely(out || in <= 0)) {\n\t\t\tvq_err(vq, \"unexpected descriptor format for RX: \"\n\t\t\t\t\"out %d, in %d\\n\", out, in);\n\t\t\tr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tif (unlikely(log)) {\n\t\t\tnlogs += *log_num;\n\t\t\tlog += *log_num;\n\t\t}\n\t\theads[headcount].id = d;\n\t\theads[headcount].len = iov_length(vq->iov + seg, in);\n\t\tdatalen -= heads[headcount].len;\n\t\t++headcount;\n\t\tseg += in;\n\t}\n\theads[headcount - 1].len += datalen;\n\t*iovcount = seg;\n\tif (unlikely(log))\n\t\t*log_num = nlogs;\n\n\t/* Detect overrun */\n\tif (unlikely(datalen > 0)) {\n\t\tr = UIO_MAXIOV + 1;\n\t\tgoto err;\n\t}\n\treturn headcount;\nerr:\n\tvhost_discard_vq_desc(vq, headcount);\n\treturn r;\n}\n\n/* Expects to be always run from workqueue - which acts as\n * read-size critical section for our kind of RCU. */\nstatic void handle_rx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned uninitialized_var(in), log;\n\tstruct vhost_log *vq_log;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL, /* FIXME: get and handle RX aux data. */\n\t\t.msg_controllen = 0,\n\t\t.msg_iov = vq->iov,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tstruct virtio_net_hdr_mrg_rxbuf hdr = {\n\t\t.hdr.flags = 0,\n\t\t.hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE\n\t};\n\tsize_t total_len = 0;\n\tint err, mergeable;\n\ts16 headcount;\n\tsize_t vhost_hlen, sock_hlen;\n\tsize_t vhost_len, sock_len;\n\tstruct socket *sock;\n\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tif (!sock)\n\t\tgoto out;\n\tvhost_disable_notify(&net->dev, vq);\n\n\tvhost_hlen = nvq->vhost_hlen;\n\tsock_hlen = nvq->sock_hlen;\n\n\tvq_log = unlikely(vhost_has_feature(&net->dev, VHOST_F_LOG_ALL)) ?\n\t\tvq->log : NULL;\n\tmergeable = vhost_has_feature(&net->dev, VIRTIO_NET_F_MRG_RXBUF);\n\n\twhile ((sock_len = peek_head_len(sock->sk))) {\n\t\tsock_len += sock_hlen;\n\t\tvhost_len = sock_len + vhost_hlen;\n\t\theadcount = get_rx_bufs(vq, vq->heads, vhost_len,\n\t\t\t\t\t&in, vq_log, &log,\n\t\t\t\t\tlikely(mergeable) ? UIO_MAXIOV : 1);\n\t\t/* On error, stop handling until the next kick. */\n\t\tif (unlikely(headcount < 0))\n\t\t\tbreak;\n\t\t/* On overrun, truncate and discard */\n\t\tif (unlikely(headcount > UIO_MAXIOV)) {\n\t\t\tmsg.msg_iovlen = 1;\n\t\t\terr = sock->ops->recvmsg(NULL, sock, &msg,\n\t\t\t\t\t\t 1, MSG_DONTWAIT | MSG_TRUNC);\n\t\t\tpr_debug(\"Discarded rx packet: len %zd\\n\", sock_len);\n\t\t\tcontinue;\n\t\t}\n\t\t/* OK, now we need to know about added descriptors. */\n\t\tif (!headcount) {\n\t\t\tif (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\t/* They have slipped one in as we were\n\t\t\t\t * doing that: check again. */\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/* Nothing new?  Wait for eventfd to tell us\n\t\t\t * they refilled. */\n\t\t\tbreak;\n\t\t}\n\t\t/* We don't need to be notified again. */\n\t\tif (unlikely((vhost_hlen)))\n\t\t\t/* Skip header. TODO: support TSO. */\n\t\t\tmove_iovec_hdr(vq->iov, nvq->hdr, vhost_hlen, in);\n\t\telse\n\t\t\t/* Copy the header for use in VIRTIO_NET_F_MRG_RXBUF:\n\t\t\t * needed because recvmsg can modify msg_iov. */\n\t\t\tcopy_iovec_hdr(vq->iov, nvq->hdr, sock_hlen, in);\n\t\tmsg.msg_iovlen = in;\n\t\terr = sock->ops->recvmsg(NULL, sock, &msg,\n\t\t\t\t\t sock_len, MSG_DONTWAIT | MSG_TRUNC);\n\t\t/* Userspace might have consumed the packet meanwhile:\n\t\t * it's not supposed to do this usually, but might be hard\n\t\t * to prevent. Discard data we got (if any) and keep going. */\n\t\tif (unlikely(err != sock_len)) {\n\t\t\tpr_debug(\"Discarded rx packet: \"\n\t\t\t\t \" len %d, expected %zd\\n\", err, sock_len);\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(vhost_hlen) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&hdr, 0,\n\t\t\t\t      vhost_hlen)) {\n\t\t\tvq_err(vq, \"Unable to write vnet_hdr at addr %p\\n\",\n\t\t\t       vq->iov->iov_base);\n\t\t\tbreak;\n\t\t}\n\t\t/* TODO: Should check and handle checksum. */\n\t\tif (likely(mergeable) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&headcount,\n\t\t\t\t      offsetof(typeof(hdr), num_buffers),\n\t\t\t\t      sizeof hdr.num_buffers)) {\n\t\t\tvq_err(vq, \"Failed num_buffers write\");\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tbreak;\n\t\t}\n\t\tvhost_add_used_and_signal_n(&net->dev, vq, vq->heads,\n\t\t\t\t\t    headcount);\n\t\tif (unlikely(vq_log))\n\t\t\tvhost_log_write(vq, vq_log, log, vhost_len);\n\t\ttotal_len += vhost_len;\n\t\tif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\n\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&vq->mutex);\n}\n\nstatic void handle_tx_kick(struct vhost_work *work)\n{\n\tstruct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,\n\t\t\t\t\t\t  poll.work);\n\tstruct vhost_net *net = container_of(vq->dev, struct vhost_net, dev);\n\n\thandle_tx(net);\n}\n\nstatic void handle_rx_kick(struct vhost_work *work)\n{\n\tstruct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,\n\t\t\t\t\t\t  poll.work);\n\tstruct vhost_net *net = container_of(vq->dev, struct vhost_net, dev);\n\n\thandle_rx(net);\n}\n\nstatic void handle_tx_net(struct vhost_work *work)\n{\n\tstruct vhost_net *net = container_of(work, struct vhost_net,\n\t\t\t\t\t     poll[VHOST_NET_VQ_TX].work);\n\thandle_tx(net);\n}\n\nstatic void handle_rx_net(struct vhost_work *work)\n{\n\tstruct vhost_net *net = container_of(work, struct vhost_net,\n\t\t\t\t\t     poll[VHOST_NET_VQ_RX].work);\n\thandle_rx(net);\n}\n\nstatic int vhost_net_open(struct inode *inode, struct file *f)\n{\n\tstruct vhost_net *n = kmalloc(sizeof *n, GFP_KERNEL);\n\tstruct vhost_dev *dev;\n\tstruct vhost_virtqueue **vqs;\n\tint i;\n\n\tif (!n)\n\t\treturn -ENOMEM;\n\tvqs = kmalloc(VHOST_NET_VQ_MAX * sizeof(*vqs), GFP_KERNEL);\n\tif (!vqs) {\n\t\tkfree(n);\n\t\treturn -ENOMEM;\n\t}\n\n\tdev = &n->dev;\n\tvqs[VHOST_NET_VQ_TX] = &n->vqs[VHOST_NET_VQ_TX].vq;\n\tvqs[VHOST_NET_VQ_RX] = &n->vqs[VHOST_NET_VQ_RX].vq;\n\tn->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;\n\tn->vqs[VHOST_NET_VQ_RX].vq.handle_kick = handle_rx_kick;\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\n\t\tn->vqs[i].ubufs = NULL;\n\t\tn->vqs[i].ubuf_info = NULL;\n\t\tn->vqs[i].upend_idx = 0;\n\t\tn->vqs[i].done_idx = 0;\n\t\tn->vqs[i].vhost_hlen = 0;\n\t\tn->vqs[i].sock_hlen = 0;\n\t}\n\tvhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX);\n\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, POLLOUT, dev);\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, POLLIN, dev);\n\n\tf->private_data = n;\n\n\treturn 0;\n}\n\nstatic void vhost_net_disable_vq(struct vhost_net *n,\n\t\t\t\t struct vhost_virtqueue *vq)\n{\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\tstruct vhost_poll *poll = n->poll + (nvq - n->vqs);\n\tif (!vq->private_data)\n\t\treturn;\n\tvhost_poll_stop(poll);\n}\n\nstatic int vhost_net_enable_vq(struct vhost_net *n,\n\t\t\t\tstruct vhost_virtqueue *vq)\n{\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\tstruct vhost_poll *poll = n->poll + (nvq - n->vqs);\n\tstruct socket *sock;\n\n\tsock = vq->private_data;\n\tif (!sock)\n\t\treturn 0;\n\n\treturn vhost_poll_start(poll, sock->file);\n}\n\nstatic struct socket *vhost_net_stop_vq(struct vhost_net *n,\n\t\t\t\t\tstruct vhost_virtqueue *vq)\n{\n\tstruct socket *sock;\n\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tvhost_net_disable_vq(n, vq);\n\tvq->private_data = NULL;\n\tmutex_unlock(&vq->mutex);\n\treturn sock;\n}\n\nstatic void vhost_net_stop(struct vhost_net *n, struct socket **tx_sock,\n\t\t\t   struct socket **rx_sock)\n{\n\t*tx_sock = vhost_net_stop_vq(n, &n->vqs[VHOST_NET_VQ_TX].vq);\n\t*rx_sock = vhost_net_stop_vq(n, &n->vqs[VHOST_NET_VQ_RX].vq);\n}\n\nstatic void vhost_net_flush_vq(struct vhost_net *n, int index)\n{\n\tvhost_poll_flush(n->poll + index);\n\tvhost_poll_flush(&n->vqs[index].vq.poll);\n}\n\nstatic void vhost_net_flush(struct vhost_net *n)\n{\n\tvhost_net_flush_vq(n, VHOST_NET_VQ_TX);\n\tvhost_net_flush_vq(n, VHOST_NET_VQ_RX);\n\tif (n->vqs[VHOST_NET_VQ_TX].ubufs) {\n\t\tmutex_lock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t\tn->tx_flush = true;\n\t\tmutex_unlock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t\t/* Wait for all lower device DMAs done. */\n\t\tvhost_net_ubuf_put_and_wait(n->vqs[VHOST_NET_VQ_TX].ubufs);\n\t\tmutex_lock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t\tn->tx_flush = false;\n\t\tatomic_set(&n->vqs[VHOST_NET_VQ_TX].ubufs->refcount, 1);\n\t\tmutex_unlock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t}\n}\n\nstatic int vhost_net_release(struct inode *inode, struct file *f)\n{\n\tstruct vhost_net *n = f->private_data;\n\tstruct socket *tx_sock;\n\tstruct socket *rx_sock;\n\n\tvhost_net_stop(n, &tx_sock, &rx_sock);\n\tvhost_net_flush(n);\n\tvhost_dev_stop(&n->dev);\n\tvhost_dev_cleanup(&n->dev, false);\n\tvhost_net_vq_reset(n);\n\tif (tx_sock)\n\t\tfput(tx_sock->file);\n\tif (rx_sock)\n\t\tfput(rx_sock->file);\n\t/* Make sure no callbacks are outstanding */\n\tsynchronize_rcu_bh();\n\t/* We do an extra flush before freeing memory,\n\t * since jobs can re-queue themselves. */\n\tvhost_net_flush(n);\n\tkfree(n->dev.vqs);\n\tkfree(n);\n\treturn 0;\n}\n\nstatic struct socket *get_raw_socket(int fd)\n{\n\tstruct {\n\t\tstruct sockaddr_ll sa;\n\t\tchar  buf[MAX_ADDR_LEN];\n\t} uaddr;\n\tint uaddr_len = sizeof uaddr, r;\n\tstruct socket *sock = sockfd_lookup(fd, &r);\n\n\tif (!sock)\n\t\treturn ERR_PTR(-ENOTSOCK);\n\n\t/* Parameter checking */\n\tif (sock->sk->sk_type != SOCK_RAW) {\n\t\tr = -ESOCKTNOSUPPORT;\n\t\tgoto err;\n\t}\n\n\tr = sock->ops->getname(sock, (struct sockaddr *)&uaddr.sa,\n\t\t\t       &uaddr_len, 0);\n\tif (r)\n\t\tgoto err;\n\n\tif (uaddr.sa.sll_family != AF_PACKET) {\n\t\tr = -EPFNOSUPPORT;\n\t\tgoto err;\n\t}\n\treturn sock;\nerr:\n\tfput(sock->file);\n\treturn ERR_PTR(r);\n}\n\nstatic struct socket *get_tap_socket(int fd)\n{\n\tstruct file *file = fget(fd);\n\tstruct socket *sock;\n\n\tif (!file)\n\t\treturn ERR_PTR(-EBADF);\n\tsock = tun_get_socket(file);\n\tif (!IS_ERR(sock))\n\t\treturn sock;\n\tsock = macvtap_get_socket(file);\n\tif (IS_ERR(sock))\n\t\tfput(file);\n\treturn sock;\n}\n\nstatic struct socket *get_socket(int fd)\n{\n\tstruct socket *sock;\n\n\t/* special case to disable backend */\n\tif (fd == -1)\n\t\treturn NULL;\n\tsock = get_raw_socket(fd);\n\tif (!IS_ERR(sock))\n\t\treturn sock;\n\tsock = get_tap_socket(fd);\n\tif (!IS_ERR(sock))\n\t\treturn sock;\n\treturn ERR_PTR(-ENOTSOCK);\n}\n\nstatic long vhost_net_set_backend(struct vhost_net *n, unsigned index, int fd)\n{\n\tstruct socket *sock, *oldsock;\n\tstruct vhost_virtqueue *vq;\n\tstruct vhost_net_virtqueue *nvq;\n\tstruct vhost_net_ubuf_ref *ubufs, *oldubufs = NULL;\n\tint r;\n\n\tmutex_lock(&n->dev.mutex);\n\tr = vhost_dev_check_owner(&n->dev);\n\tif (r)\n\t\tgoto err;\n\n\tif (index >= VHOST_NET_VQ_MAX) {\n\t\tr = -ENOBUFS;\n\t\tgoto err;\n\t}\n\tvq = &n->vqs[index].vq;\n\tnvq = &n->vqs[index];\n\tmutex_lock(&vq->mutex);\n\n\t/* Verify that ring has been setup correctly. */\n\tif (!vhost_vq_access_ok(vq)) {\n\t\tr = -EFAULT;\n\t\tgoto err_vq;\n\t}\n\tsock = get_socket(fd);\n\tif (IS_ERR(sock)) {\n\t\tr = PTR_ERR(sock);\n\t\tgoto err_vq;\n\t}\n\n\t/* start polling new socket */\n\toldsock = vq->private_data;\n\tif (sock != oldsock) {\n\t\tubufs = vhost_net_ubuf_alloc(vq,\n\t\t\t\t\t     sock && vhost_sock_zcopy(sock));\n\t\tif (IS_ERR(ubufs)) {\n\t\t\tr = PTR_ERR(ubufs);\n\t\t\tgoto err_ubufs;\n\t\t}\n\n\t\tvhost_net_disable_vq(n, vq);\n\t\tvq->private_data = sock;\n\t\tr = vhost_init_used(vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\t\tr = vhost_net_enable_vq(n, vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\n\t\toldubufs = nvq->ubufs;\n\t\tnvq->ubufs = ubufs;\n\n\t\tn->tx_packets = 0;\n\t\tn->tx_zcopy_err = 0;\n\t\tn->tx_flush = false;\n\t}\n\n\tmutex_unlock(&vq->mutex);\n\n\tif (oldubufs) {\n\t\tvhost_net_ubuf_put_wait_and_free(oldubufs);\n\t\tmutex_lock(&vq->mutex);\n\t\tvhost_zerocopy_signal_used(n, vq);\n\t\tmutex_unlock(&vq->mutex);\n\t}\n\n\tif (oldsock) {\n\t\tvhost_net_flush_vq(n, index);\n\t\tfput(oldsock->file);\n\t}\n\n\tmutex_unlock(&n->dev.mutex);\n\treturn 0;\n\nerr_used:\n\tvq->private_data = oldsock;\n\tvhost_net_enable_vq(n, vq);\n\tif (ubufs)\n\t\tvhost_net_ubuf_put_wait_and_free(ubufs);\nerr_ubufs:\n\tfput(sock->file);\nerr_vq:\n\tmutex_unlock(&vq->mutex);\nerr:\n\tmutex_unlock(&n->dev.mutex);\n\treturn r;\n}\n\nstatic long vhost_net_reset_owner(struct vhost_net *n)\n{\n\tstruct socket *tx_sock = NULL;\n\tstruct socket *rx_sock = NULL;\n\tlong err;\n\tstruct vhost_memory *memory;\n\n\tmutex_lock(&n->dev.mutex);\n\terr = vhost_dev_check_owner(&n->dev);\n\tif (err)\n\t\tgoto done;\n\tmemory = vhost_dev_reset_owner_prepare();\n\tif (!memory) {\n\t\terr = -ENOMEM;\n\t\tgoto done;\n\t}\n\tvhost_net_stop(n, &tx_sock, &rx_sock);\n\tvhost_net_flush(n);\n\tvhost_dev_reset_owner(&n->dev, memory);\n\tvhost_net_vq_reset(n);\ndone:\n\tmutex_unlock(&n->dev.mutex);\n\tif (tx_sock)\n\t\tfput(tx_sock->file);\n\tif (rx_sock)\n\t\tfput(rx_sock->file);\n\treturn err;\n}\n\nstatic int vhost_net_set_features(struct vhost_net *n, u64 features)\n{\n\tsize_t vhost_hlen, sock_hlen, hdr_len;\n\tint i;\n\n\thdr_len = (features & (1 << VIRTIO_NET_F_MRG_RXBUF)) ?\n\t\t\tsizeof(struct virtio_net_hdr_mrg_rxbuf) :\n\t\t\tsizeof(struct virtio_net_hdr);\n\tif (features & (1 << VHOST_NET_F_VIRTIO_NET_HDR)) {\n\t\t/* vhost provides vnet_hdr */\n\t\tvhost_hlen = hdr_len;\n\t\tsock_hlen = 0;\n\t} else {\n\t\t/* socket provides vnet_hdr */\n\t\tvhost_hlen = 0;\n\t\tsock_hlen = hdr_len;\n\t}\n\tmutex_lock(&n->dev.mutex);\n\tif ((features & (1 << VHOST_F_LOG_ALL)) &&\n\t    !vhost_log_access_ok(&n->dev)) {\n\t\tmutex_unlock(&n->dev.mutex);\n\t\treturn -EFAULT;\n\t}\n\tn->dev.acked_features = features;\n\tsmp_wmb();\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; ++i) {\n\t\tmutex_lock(&n->vqs[i].vq.mutex);\n\t\tn->vqs[i].vhost_hlen = vhost_hlen;\n\t\tn->vqs[i].sock_hlen = sock_hlen;\n\t\tmutex_unlock(&n->vqs[i].vq.mutex);\n\t}\n\tvhost_net_flush(n);\n\tmutex_unlock(&n->dev.mutex);\n\treturn 0;\n}\n\nstatic long vhost_net_set_owner(struct vhost_net *n)\n{\n\tint r;\n\n\tmutex_lock(&n->dev.mutex);\n\tif (vhost_dev_has_owner(&n->dev)) {\n\t\tr = -EBUSY;\n\t\tgoto out;\n\t}\n\tr = vhost_net_set_ubuf_info(n);\n\tif (r)\n\t\tgoto out;\n\tr = vhost_dev_set_owner(&n->dev);\n\tif (r)\n\t\tvhost_net_clear_ubuf_info(n);\n\tvhost_net_flush(n);\nout:\n\tmutex_unlock(&n->dev.mutex);\n\treturn r;\n}\n\nstatic long vhost_net_ioctl(struct file *f, unsigned int ioctl,\n\t\t\t    unsigned long arg)\n{\n\tstruct vhost_net *n = f->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tu64 __user *featurep = argp;\n\tstruct vhost_vring_file backend;\n\tu64 features;\n\tint r;\n\n\tswitch (ioctl) {\n\tcase VHOST_NET_SET_BACKEND:\n\t\tif (copy_from_user(&backend, argp, sizeof backend))\n\t\t\treturn -EFAULT;\n\t\treturn vhost_net_set_backend(n, backend.index, backend.fd);\n\tcase VHOST_GET_FEATURES:\n\t\tfeatures = VHOST_NET_FEATURES;\n\t\tif (copy_to_user(featurep, &features, sizeof features))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase VHOST_SET_FEATURES:\n\t\tif (copy_from_user(&features, featurep, sizeof features))\n\t\t\treturn -EFAULT;\n\t\tif (features & ~VHOST_NET_FEATURES)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn vhost_net_set_features(n, features);\n\tcase VHOST_RESET_OWNER:\n\t\treturn vhost_net_reset_owner(n);\n\tcase VHOST_SET_OWNER:\n\t\treturn vhost_net_set_owner(n);\n\tdefault:\n\t\tmutex_lock(&n->dev.mutex);\n\t\tr = vhost_dev_ioctl(&n->dev, ioctl, argp);\n\t\tif (r == -ENOIOCTLCMD)\n\t\t\tr = vhost_vring_ioctl(&n->dev, ioctl, argp);\n\t\telse\n\t\t\tvhost_net_flush(n);\n\t\tmutex_unlock(&n->dev.mutex);\n\t\treturn r;\n\t}\n}\n\n#ifdef CONFIG_COMPAT\nstatic long vhost_net_compat_ioctl(struct file *f, unsigned int ioctl,\n\t\t\t\t   unsigned long arg)\n{\n\treturn vhost_net_ioctl(f, ioctl, (unsigned long)compat_ptr(arg));\n}\n#endif\n\nstatic const struct file_operations vhost_net_fops = {\n\t.owner          = THIS_MODULE,\n\t.release        = vhost_net_release,\n\t.unlocked_ioctl = vhost_net_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl   = vhost_net_compat_ioctl,\n#endif\n\t.open           = vhost_net_open,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic struct miscdevice vhost_net_misc = {\n\t.minor = VHOST_NET_MINOR,\n\t.name = \"vhost-net\",\n\t.fops = &vhost_net_fops,\n};\n\nstatic int vhost_net_init(void)\n{\n\tif (experimental_zcopytx)\n\t\tvhost_net_enable_zcopy(VHOST_NET_VQ_TX);\n\treturn misc_register(&vhost_net_misc);\n}\nmodule_init(vhost_net_init);\n\nstatic void vhost_net_exit(void)\n{\n\tmisc_deregister(&vhost_net_misc);\n}\nmodule_exit(vhost_net_exit);\n\nMODULE_VERSION(\"0.0.1\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Michael S. Tsirkin\");\nMODULE_DESCRIPTION(\"Host kernel accelerator for virtio net\");\nMODULE_ALIAS_MISCDEV(VHOST_NET_MINOR);\nMODULE_ALIAS(\"devname:vhost-net\");\n"], "filenames": ["drivers/vhost/net.c"], "buggy_code_start_loc": [534], "buggy_code_end_loc": [589], "fixing_code_start_loc": [535], "fixing_code_end_loc": [604], "type": "CWE-787", "message": "drivers/vhost/net.c in the Linux kernel before 3.13.10, when mergeable buffers are disabled, does not properly validate packet lengths, which allows guest OS users to cause a denial of service (memory corruption and host OS crash) or possibly gain privileges on the host OS via crafted packets, related to the handle_rx and get_rx_bufs functions.", "other": {"cve": {"id": "CVE-2014-0077", "sourceIdentifier": "secalert@redhat.com", "published": "2014-04-14T23:55:07.530", "lastModified": "2023-02-13T00:31:13.407", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "drivers/vhost/net.c in the Linux kernel before 3.13.10, when mergeable buffers are disabled, does not properly validate packet lengths, which allows guest OS users to cause a denial of service (memory corruption and host OS crash) or possibly gain privileges on the host OS via crafted packets, related to the handle_rx and get_rx_bufs functions."}, {"lang": "es", "value": "drivers/vhost/net.c en el kernel de Linux anterior a 3.13.10, cuando buffers combinables  est\u00e1n deshabilitados, no valida debidamente los longitudes de paquetes, lo que permite a usuarios invitados del sistema operativo causar una denegaci\u00f3n de servicio (corrupci\u00f3n de memoria y ca\u00edda del sistema operativo anfitri\u00f3n) o posiblemente ganar privilegios en el sistema operativo anfitri\u00f3n a trav\u00e9s de paquetes manipulados, relacionado con las funciones handle_rx y get_rx_bufs."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:A/AC:H/Au:S/C:P/I:P/A:C", "accessVector": "ADJACENT_NETWORK", "accessComplexity": "HIGH", "authentication": "SINGLE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "COMPLETE", "baseScore": 5.5}, "baseSeverity": "MEDIUM", "exploitabilityScore": 2.5, "impactScore": 8.5, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.13.10", "matchCriteriaId": "3BA91F47-37E8-4342-848A-045504854388"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=d8316f3991d207fe32881a9ac20241be8fa2bad0", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.13.10", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/66678", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1064440", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/d8316f3991d207fe32881a9ac20241be8fa2bad0", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/d8316f3991d207fe32881a9ac20241be8fa2bad0"}}