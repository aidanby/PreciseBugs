{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/type_traits.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/meta_support.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/bfloat16.h\"\n\nnamespace {\nenum {\n  QUANTIZE_MODE_MIN_COMBINED,\n  QUANTIZE_MODE_MIN_FIRST,\n  QUANTIZE_MODE_SCALED,\n};\n}  // namespace\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nT Cast(float v) {\n  return v;\n}\n\ntemplate <>\nbfloat16 Cast<bfloat16>(float v) {\n  return bfloat16(v);\n}\n\ntemplate <typename Device, typename T, typename S>\nclass DequantizeOp : public OpKernel {\n public:\n  explicit DequantizeOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    string mode_string;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"mode\", &mode_string));\n    OP_REQUIRES(\n        ctx,\n        (ctx->output_type(0) == DT_FLOAT || ctx->output_type(0) == DT_BFLOAT16),\n        errors::InvalidArgument(\"Output type must be bfloat16 or float,\"\n                                \" is '\" +\n                                DataTypeString(ctx->output_type(0)) + \"'\"));\n\n    need_cast_ = true;\n    if (ctx->output_type(0) == DT_FLOAT) {\n      need_cast_ = false;\n      OP_REQUIRES(ctx,\n                  (mode_string == \"MIN_COMBINED\" ||\n                   mode_string == \"MIN_FIRST\" || mode_string == \"SCALED\"),\n                  errors::InvalidArgument(\"Mode string must be 'MIN_COMBINED',\"\n                                          \" 'MIN_FIRST', or 'SCALED', is '\" +\n                                          mode_string + \"'\"));\n    } else {\n      OP_REQUIRES(\n          ctx, (mode_string == \"MIN_COMBINED\"),\n          errors::InvalidArgument(\"When output type is bfloat16, Mode\"\n                                  \" string must be 'MIN_COMBINED', is '\" +\n                                  mode_string + \"'\"));\n    }\n\n    if (mode_string == \"MIN_COMBINED\") {\n      mode_ = QUANTIZE_MODE_MIN_COMBINED;\n    } else if (mode_string == \"MIN_FIRST\") {\n      mode_ = QUANTIZE_MODE_MIN_FIRST;\n    } else if (mode_string == \"SCALED\") {\n      mode_ = QUANTIZE_MODE_SCALED;\n    }\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"narrow_range\", &narrow_range_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"axis\", &axis_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min_tensor = ctx->input(1);\n    const Tensor& input_max_tensor = ctx->input(2);\n\n    int num_slices = 1;\n    if (axis_ > -1) {\n      num_slices = input.dim_size(axis_);\n    }\n    OP_REQUIRES(ctx, input_min_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_min_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_min_tensor.NumElements(),\n                    \", expected \", num_slices));\n    OP_REQUIRES(ctx, input_max_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_max_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_max_tensor.NumElements(),\n                    \", expected \", num_slices));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor float_output =\n        need_cast_ ? tensorflow::Tensor(DT_FLOAT, input.shape()) : *output;\n    if (num_slices == 1) {\n      const float min_range = input_min_tensor.flat<float>()(0);\n      const float max_range = input_max_tensor.flat<float>()(0);\n      DequantizeTensor(ctx, input, min_range, max_range, &float_output);\n    } else {\n      OP_REQUIRES(ctx, mode_ != QUANTIZE_MODE_MIN_FIRST,\n                  errors::Unimplemented(\"MIN_FIRST mode is not implemented for \"\n                                        \"Dequantize with axis != -1.\"));\n\n      int64_t pre_dim = 1, post_dim = 1;\n      for (int i = 0; i < axis_; ++i) {\n        pre_dim *= float_output.dim_size(i);\n      }\n      for (int i = axis_ + 1; i < float_output.dims(); ++i) {\n        post_dim *= float_output.dim_size(i);\n      }\n      auto input_tensor = input.template bit_casted_shaped<T, 3>(\n          {pre_dim, num_slices, post_dim});\n      auto output_tensor =\n          float_output.flat_inner_outer_dims<float, 3>(axis_ - 1);\n      auto min_ranges = input_min_tensor.vec<float>();\n      auto max_ranges = input_max_tensor.vec<float>();\n      for (int i = 0; i < num_slices; ++i) {\n        DequantizeSlice(ctx->eigen_device<Device>(), ctx,\n                        input_tensor.template chip<1>(i), min_ranges(i),\n                        max_ranges(i), output_tensor.template chip<1>(i));\n      }\n    }\n    if (need_cast_) {\n      S* out_ptr = output->flat<S>().data();\n      float* in_ptr = float_output.flat<float>().data();\n      for (int64_t i = 0; i < float_output.NumElements(); ++i) {\n        out_ptr[i] = static_cast<S>(in_ptr[i]);\n      }\n    }\n  }\n\n  void DequantizeTensor(OpKernelContext* ctx, const Tensor& input,\n                        const float min_range, const float max_range,\n                        Tensor* output) {\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          ((input_tensor.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n\n    } else if (mode_ == QUANTIZE_MODE_MIN_FIRST) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T, quint8>()) {\n        auto input_ui8_array = input.flat<quint8>();\n        meta::Dequantize(ctx, input_ui8_array.data(), input_ui8_array.size(),\n                         min_range, max_range, output->flat<float>().data());\n      } else {\n        QuantizedTensorToFloatInPlaceUsingEigen<T>(\n            ctx->template eigen_device<Device>(), input, min_range, max_range,\n            output);\n      }\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          input_tensor.template cast<int>().template cast<float>() *\n          scale_factor;\n    }\n  }\n\n  template <typename ConstVec, typename Vec>\n  void DequantizeSlice(const Device& d, OpKernelContext* ctx,\n                       const ConstVec& input, float min_range, float max_range,\n                       Vec output) {\n    // TODO(pauldonnelly): Factor out the similar calculations in quantize,\n    //   dequantize and quantize_and_dequantize ops.\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      output.device(d) =\n          ((input.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      output.device(d) = input.template cast<float>() * scale_factor;\n    }\n  }\n\n private:\n  int mode_;\n  int axis_;\n  bool narrow_range_;\n  bool need_cast_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, bfloat16>);\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/type_traits.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/meta_support.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/bfloat16.h\"\n\nnamespace {\nenum {\n  QUANTIZE_MODE_MIN_COMBINED,\n  QUANTIZE_MODE_MIN_FIRST,\n  QUANTIZE_MODE_SCALED,\n};\n}  // namespace\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nT Cast(float v) {\n  return v;\n}\n\ntemplate <>\nbfloat16 Cast<bfloat16>(float v) {\n  return bfloat16(v);\n}\n\ntemplate <typename Device, typename T, typename S>\nclass DequantizeOp : public OpKernel {\n public:\n  explicit DequantizeOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    string mode_string;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"mode\", &mode_string));\n    OP_REQUIRES(\n        ctx,\n        (ctx->output_type(0) == DT_FLOAT || ctx->output_type(0) == DT_BFLOAT16),\n        errors::InvalidArgument(\"Output type must be bfloat16 or float,\"\n                                \" is '\" +\n                                DataTypeString(ctx->output_type(0)) + \"'\"));\n\n    need_cast_ = true;\n    if (ctx->output_type(0) == DT_FLOAT) {\n      need_cast_ = false;\n      OP_REQUIRES(ctx,\n                  (mode_string == \"MIN_COMBINED\" ||\n                   mode_string == \"MIN_FIRST\" || mode_string == \"SCALED\"),\n                  errors::InvalidArgument(\"Mode string must be 'MIN_COMBINED',\"\n                                          \" 'MIN_FIRST', or 'SCALED', is '\" +\n                                          mode_string + \"'\"));\n    } else {\n      OP_REQUIRES(\n          ctx, (mode_string == \"MIN_COMBINED\"),\n          errors::InvalidArgument(\"When output type is bfloat16, Mode\"\n                                  \" string must be 'MIN_COMBINED', is '\" +\n                                  mode_string + \"'\"));\n    }\n\n    if (mode_string == \"MIN_COMBINED\") {\n      mode_ = QUANTIZE_MODE_MIN_COMBINED;\n    } else if (mode_string == \"MIN_FIRST\") {\n      mode_ = QUANTIZE_MODE_MIN_FIRST;\n    } else if (mode_string == \"SCALED\") {\n      mode_ = QUANTIZE_MODE_SCALED;\n    }\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"narrow_range\", &narrow_range_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"axis\", &axis_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min_tensor = ctx->input(1);\n    const Tensor& input_max_tensor = ctx->input(2);\n\n    OP_REQUIRES(\n        ctx, axis_ < input.dims(),\n        errors::InvalidArgument(\"Axis must be less than input dimension(\",\n                                input.dims(), \"), got \", axis_));\n\n    int num_slices = 1;\n    if (axis_ > -1) {\n      num_slices = input.dim_size(axis_);\n    }\n    OP_REQUIRES(ctx, input_min_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_min_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_min_tensor.NumElements(),\n                    \", expected \", num_slices));\n    OP_REQUIRES(ctx, input_max_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_max_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_max_tensor.NumElements(),\n                    \", expected \", num_slices));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor float_output =\n        need_cast_ ? tensorflow::Tensor(DT_FLOAT, input.shape()) : *output;\n    if (num_slices == 1) {\n      const float min_range = input_min_tensor.flat<float>()(0);\n      const float max_range = input_max_tensor.flat<float>()(0);\n      DequantizeTensor(ctx, input, min_range, max_range, &float_output);\n    } else {\n      OP_REQUIRES(ctx, mode_ != QUANTIZE_MODE_MIN_FIRST,\n                  errors::Unimplemented(\"MIN_FIRST mode is not implemented for \"\n                                        \"Dequantize with axis != -1.\"));\n\n      int64_t pre_dim = 1, post_dim = 1;\n      for (int i = 0; i < axis_; ++i) {\n        pre_dim *= float_output.dim_size(i);\n      }\n      for (int i = axis_ + 1; i < float_output.dims(); ++i) {\n        post_dim *= float_output.dim_size(i);\n      }\n      auto input_tensor = input.template bit_casted_shaped<T, 3>(\n          {pre_dim, num_slices, post_dim});\n      auto output_tensor =\n          float_output.flat_inner_outer_dims<float, 3>(axis_ - 1);\n      auto min_ranges = input_min_tensor.vec<float>();\n      auto max_ranges = input_max_tensor.vec<float>();\n      for (int i = 0; i < num_slices; ++i) {\n        DequantizeSlice(ctx->eigen_device<Device>(), ctx,\n                        input_tensor.template chip<1>(i), min_ranges(i),\n                        max_ranges(i), output_tensor.template chip<1>(i));\n      }\n    }\n    if (need_cast_) {\n      S* out_ptr = output->flat<S>().data();\n      float* in_ptr = float_output.flat<float>().data();\n      for (int64_t i = 0; i < float_output.NumElements(); ++i) {\n        out_ptr[i] = static_cast<S>(in_ptr[i]);\n      }\n    }\n  }\n\n  void DequantizeTensor(OpKernelContext* ctx, const Tensor& input,\n                        const float min_range, const float max_range,\n                        Tensor* output) {\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          ((input_tensor.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n\n    } else if (mode_ == QUANTIZE_MODE_MIN_FIRST) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T, quint8>()) {\n        auto input_ui8_array = input.flat<quint8>();\n        meta::Dequantize(ctx, input_ui8_array.data(), input_ui8_array.size(),\n                         min_range, max_range, output->flat<float>().data());\n      } else {\n        QuantizedTensorToFloatInPlaceUsingEigen<T>(\n            ctx->template eigen_device<Device>(), input, min_range, max_range,\n            output);\n      }\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          input_tensor.template cast<int>().template cast<float>() *\n          scale_factor;\n    }\n  }\n\n  template <typename ConstVec, typename Vec>\n  void DequantizeSlice(const Device& d, OpKernelContext* ctx,\n                       const ConstVec& input, float min_range, float max_range,\n                       Vec output) {\n    // TODO(pauldonnelly): Factor out the similar calculations in quantize,\n    //   dequantize and quantize_and_dequantize ops.\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      output.device(d) =\n          ((input.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      output.device(d) = input.template cast<float>() * scale_factor;\n    }\n  }\n\n private:\n  int mode_;\n  int axis_;\n  bool narrow_range_;\n  bool need_cast_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, bfloat16>);\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/dequantize_op.cc"], "buggy_code_start_loc": [96], "buggy_code_end_loc": [96], "fixing_code_start_loc": [97], "fixing_code_end_loc": [102], "type": "CWE-125", "message": "Tensorflow is an Open Source Machine Learning Framework. The implementation of `Dequantize` does not fully validate the value of `axis` and can result in heap OOB accesses. The `axis` argument can be `-1` (the default value for the optional argument) or any other positive value at most the number of dimensions of the input. Unfortunately, the upper bound is not checked and this results in reading past the end of the array containing the dimensions of the input tensor. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2022-21726", "sourceIdentifier": "security-advisories@github.com", "published": "2022-02-03T11:15:07.810", "lastModified": "2022-02-08T20:05:44.557", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Tensorflow is an Open Source Machine Learning Framework. The implementation of `Dequantize` does not fully validate the value of `axis` and can result in heap OOB accesses. The `axis` argument can be `-1` (the default value for the optional argument) or any other positive value at most the number of dimensions of the input. Unfortunately, the upper bound is not checked and this results in reading past the end of the array containing the dimensions of the input tensor. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range."}, {"lang": "es", "value": "Tensorflow es un marco de aprendizaje autom\u00e1tico de c\u00f3digo abierto. La implementaci\u00f3n de \"Dequantize\" no comprueba completamente el valor de \"axis\" y puede resultar en accesos OOB a la pila. El argumento \"axis\" puede ser \"-1\" (el valor por defecto para el argumento opcional) o cualquier otro valor positivo como m\u00e1ximo el n\u00famero de dimensiones de la entrada. Desgraciadamente, el l\u00edmite superior no es comprobado y esto hace que es le\u00eddo m\u00e1s all\u00e1 del final del array que contiene las dimensiones del tensor de entrada. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.8.0. Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.7.1, TensorFlow versi\u00f3n 2.6.3, y TensorFlow versi\u00f3n 2.5.3, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 8.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.5}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.5.2", "matchCriteriaId": "688150BF-477C-48FC-9AEF-A79AC57A6DDC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndIncluding": "2.6.2", "matchCriteriaId": "C9E69B60-8C97-47E2-9027-9598B8392E5D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:*:*:*:*:*:*:*", "matchCriteriaId": "2EDFAAB8-799C-4259-9102-944D4760DA2C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/5100e359aef5c8021f2e71c7b986420b85ce7b3d/tensorflow/core/kernels/dequantize_op.cc#L92-L153", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/23968a8bf65b009120c43b5ebcceaf52dbc9e943", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-23hm-7w47-xw72", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/23968a8bf65b009120c43b5ebcceaf52dbc9e943"}}