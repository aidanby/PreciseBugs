{"buggy_code": ["from __future__ import annotations\n\nimport asyncio\nimport logging\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Union,\n)\n\nimport requests\n\nfrom langchain.docstore.document import Document\nfrom langchain.document_loaders.base import BaseLoader\nfrom langchain.utils.html import extract_sub_links\n\nif TYPE_CHECKING:\n    import aiohttp\n\nlogger = logging.getLogger(__name__)\n\n\ndef _metadata_extractor(raw_html: str, url: str) -> dict:\n    \"\"\"Extract metadata from raw html using BeautifulSoup.\"\"\"\n    metadata = {\"source\": url}\n\n    try:\n        from bs4 import BeautifulSoup\n    except ImportError:\n        logger.warning(\n            \"The bs4 package is required for default metadata extraction. \"\n            \"Please install it with `pip install bs4`.\"\n        )\n        return metadata\n    soup = BeautifulSoup(raw_html, \"html.parser\")\n    if title := soup.find(\"title\"):\n        metadata[\"title\"] = title.get_text()\n    if description := soup.find(\"meta\", attrs={\"name\": \"description\"}):\n        metadata[\"description\"] = description.get(\"content\", None)\n    if html := soup.find(\"html\"):\n        metadata[\"language\"] = html.get(\"lang\", None)\n    return metadata\n\n\nclass RecursiveUrlLoader(BaseLoader):\n    \"\"\"Load all child links from a URL page.\"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        max_depth: Optional[int] = 2,\n        use_async: Optional[bool] = None,\n        extractor: Optional[Callable[[str], str]] = None,\n        metadata_extractor: Optional[Callable[[str, str], str]] = None,\n        exclude_dirs: Optional[Sequence[str]] = (),\n        timeout: Optional[int] = 10,\n        prevent_outside: Optional[bool] = True,\n        link_regex: Union[str, re.Pattern, None] = None,\n        headers: Optional[dict] = None,\n        check_response_status: bool = False,\n    ) -> None:\n        \"\"\"Initialize with URL to crawl and any subdirectories to exclude.\n        Args:\n            url: The URL to crawl.\n            max_depth: The max depth of the recursive loading.\n            use_async: Whether to use asynchronous loading.\n                If True, this function will not be lazy, but it will still work in the\n                expected way, just not lazy.\n            extractor: A function to extract document contents from raw html.\n                When extract function returns an empty string, the document is\n                ignored.\n            metadata_extractor: A function to extract metadata from raw html and the\n                source url (args in that order). Default extractor will attempt\n                to use BeautifulSoup4 to extract the title, description and language\n                of the page.\n            exclude_dirs: A list of subdirectories to exclude.\n            timeout: The timeout for the requests, in the unit of seconds. If None then\n                connection will not timeout.\n            prevent_outside: If True, prevent loading from urls which are not children\n                of the root url.\n            link_regex: Regex for extracting sub-links from the raw html of a web page.\n            check_response_status: If True, check HTTP response status and skip\n                URLs with error responses (400-599).\n        \"\"\"\n\n        self.url = url\n        self.max_depth = max_depth if max_depth is not None else 2\n        self.use_async = use_async if use_async is not None else False\n        self.extractor = extractor if extractor is not None else lambda x: x\n        self.metadata_extractor = (\n            metadata_extractor\n            if metadata_extractor is not None\n            else _metadata_extractor\n        )\n        self.exclude_dirs = exclude_dirs if exclude_dirs is not None else ()\n\n        if any(url.startswith(exclude_dir) for exclude_dir in self.exclude_dirs):\n            raise ValueError(\n                f\"Base url is included in exclude_dirs. Received base_url: {url} and \"\n                f\"exclude_dirs: {self.exclude_dirs}\"\n            )\n\n        self.timeout = timeout\n        self.prevent_outside = prevent_outside if prevent_outside is not None else True\n        self.link_regex = link_regex\n        self._lock = asyncio.Lock() if self.use_async else None\n        self.headers = headers\n        self.check_response_status = check_response_status\n\n    def _get_child_links_recursive(\n        self, url: str, visited: Set[str], *, depth: int = 0\n    ) -> Iterator[Document]:\n        \"\"\"Recursively get all child links starting with the path of the input URL.\n\n        Args:\n            url: The URL to crawl.\n            visited: A set of visited URLs.\n            depth: Current depth of recursion. Stop when depth >= max_depth.\n        \"\"\"\n\n        if depth >= self.max_depth:\n            return\n\n        # Get all links that can be accessed from the current URL\n        visited.add(url)\n        try:\n            response = requests.get(url, timeout=self.timeout, headers=self.headers)\n            if self.check_response_status and 400 <= response.status_code <= 599:\n                raise ValueError(f\"Received HTTP status {response.status_code}\")\n        except Exception as e:\n            logger.warning(\n                f\"Unable to load from {url}. Received error {e} of type \"\n                f\"{e.__class__.__name__}\"\n            )\n            return\n        content = self.extractor(response.text)\n        if content:\n            yield Document(\n                page_content=content,\n                metadata=self.metadata_extractor(response.text, url),\n            )\n\n        # Store the visited links and recursively visit the children\n        sub_links = extract_sub_links(\n            response.text,\n            url,\n            base_url=self.url,\n            pattern=self.link_regex,\n            prevent_outside=self.prevent_outside,\n            exclude_prefixes=self.exclude_dirs,\n        )\n        for link in sub_links:\n            # Check all unvisited links\n            if link not in visited:\n                yield from self._get_child_links_recursive(\n                    link, visited, depth=depth + 1\n                )\n\n    async def _async_get_child_links_recursive(\n        self,\n        url: str,\n        visited: Set[str],\n        *,\n        session: Optional[aiohttp.ClientSession] = None,\n        depth: int = 0,\n    ) -> List[Document]:\n        \"\"\"Recursively get all child links starting with the path of the input URL.\n\n        Args:\n            url: The URL to crawl.\n            visited: A set of visited URLs.\n            depth: To reach the current url, how many pages have been visited.\n        \"\"\"\n        try:\n            import aiohttp\n        except ImportError:\n            raise ImportError(\n                \"The aiohttp package is required for the RecursiveUrlLoader. \"\n                \"Please install it with `pip install aiohttp`.\"\n            )\n        if depth >= self.max_depth:\n            return []\n\n        # Disable SSL verification because websites may have invalid SSL certificates,\n        # but won't cause any security issues for us.\n        close_session = session is None\n        session = (\n            session\n            if session is not None\n            else aiohttp.ClientSession(\n                connector=aiohttp.TCPConnector(ssl=False),\n                timeout=aiohttp.ClientTimeout(total=self.timeout),\n                headers=self.headers,\n            )\n        )\n        async with self._lock:  # type: ignore\n            visited.add(url)\n        try:\n            async with session.get(url) as response:\n                text = await response.text()\n                if self.check_response_status and 400 <= response.status <= 599:\n                    raise ValueError(f\"Received HTTP status {response.status}\")\n        except (aiohttp.client_exceptions.InvalidURL, Exception) as e:\n            logger.warning(\n                f\"Unable to load {url}. Received error {e} of type \"\n                f\"{e.__class__.__name__}\"\n            )\n            if close_session:\n                await session.close()\n            return []\n        results = []\n        content = self.extractor(text)\n        if content:\n            results.append(\n                Document(\n                    page_content=content,\n                    metadata=self.metadata_extractor(text, url),\n                )\n            )\n        if depth < self.max_depth - 1:\n            sub_links = extract_sub_links(\n                text,\n                url,\n                base_url=self.url,\n                pattern=self.link_regex,\n                prevent_outside=self.prevent_outside,\n                exclude_prefixes=self.exclude_dirs,\n            )\n\n            # Recursively call the function to get the children of the children\n            sub_tasks = []\n            async with self._lock:  # type: ignore\n                to_visit = set(sub_links).difference(visited)\n                for link in to_visit:\n                    sub_tasks.append(\n                        self._async_get_child_links_recursive(\n                            link, visited, session=session, depth=depth + 1\n                        )\n                    )\n            next_results = await asyncio.gather(*sub_tasks)\n            for sub_result in next_results:\n                if isinstance(sub_result, Exception) or sub_result is None:\n                    # We don't want to stop the whole process, so just ignore it\n                    # Not standard html format or invalid url or 404 may cause this.\n                    continue\n                # locking not fully working, temporary hack to ensure deduplication\n                results += [r for r in sub_result if r not in results]\n        if close_session:\n            await session.close()\n        return results\n\n    def lazy_load(self) -> Iterator[Document]:\n        \"\"\"Lazy load web pages.\n        When use_async is True, this function will not be lazy,\n        but it will still work in the expected way, just not lazy.\"\"\"\n        visited: Set[str] = set()\n        if self.use_async:\n            results = asyncio.run(\n                self._async_get_child_links_recursive(self.url, visited)\n            )\n            return iter(results or [])\n        else:\n            return self._get_child_links_recursive(self.url, visited)\n\n    def load(self) -> List[Document]:\n        \"\"\"Load web pages.\"\"\"\n        return list(self.lazy_load())\n"], "fixing_code": ["from __future__ import annotations\n\nimport asyncio\nimport logging\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Union,\n)\n\nimport requests\n\nfrom langchain.docstore.document import Document\nfrom langchain.document_loaders.base import BaseLoader\nfrom langchain.utils.html import extract_sub_links\n\nif TYPE_CHECKING:\n    import aiohttp\n\nlogger = logging.getLogger(__name__)\n\n\ndef _metadata_extractor(raw_html: str, url: str) -> dict:\n    \"\"\"Extract metadata from raw html using BeautifulSoup.\"\"\"\n    metadata = {\"source\": url}\n\n    try:\n        from bs4 import BeautifulSoup\n    except ImportError:\n        logger.warning(\n            \"The bs4 package is required for default metadata extraction. \"\n            \"Please install it with `pip install bs4`.\"\n        )\n        return metadata\n    soup = BeautifulSoup(raw_html, \"html.parser\")\n    if title := soup.find(\"title\"):\n        metadata[\"title\"] = title.get_text()\n    if description := soup.find(\"meta\", attrs={\"name\": \"description\"}):\n        metadata[\"description\"] = description.get(\"content\", None)\n    if html := soup.find(\"html\"):\n        metadata[\"language\"] = html.get(\"lang\", None)\n    return metadata\n\n\nclass RecursiveUrlLoader(BaseLoader):\n    \"\"\"Load all child links from a URL page.\n\n    **Security Note**: This loader is a crawler that will start crawling\n        at a given URL and then expand to crawl child links recursively.\n\n        Web crawlers should generally NOT be deployed with network access\n        to any internal servers.\n\n        Control access to who can submit crawling requests and what network access\n        the crawler has.\n\n        While crawling, the crawler may encounter malicious URLs that would lead to a\n        server-side request forgery (SSRF) attack.\n\n        To mitigate risks, the crawler by default will only load URLs from the same\n        domain as the start URL (controlled via prevent_outside named argument).\n\n        This will mitigate the risk of SSRF attacks, but will not eliminate it.\n\n        For example, if crawling a host which hosts several sites:\n\n        https://some_host/alice_site/\n        https://some_host/bob_site/\n\n        A malicious URL on Alice's site could cause the crawler to make a malicious\n        GET request to an endpoint on Bob's site. Both sites are hosted on the\n        same host, so such a request would not be prevented by default.\n\n        See https://python.langchain.com/docs/security\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        max_depth: Optional[int] = 2,\n        use_async: Optional[bool] = None,\n        extractor: Optional[Callable[[str], str]] = None,\n        metadata_extractor: Optional[Callable[[str, str], str]] = None,\n        exclude_dirs: Optional[Sequence[str]] = (),\n        timeout: Optional[int] = 10,\n        prevent_outside: bool = True,\n        link_regex: Union[str, re.Pattern, None] = None,\n        headers: Optional[dict] = None,\n        check_response_status: bool = False,\n    ) -> None:\n        \"\"\"Initialize with URL to crawl and any subdirectories to exclude.\n\n        Args:\n            url: The URL to crawl.\n            max_depth: The max depth of the recursive loading.\n            use_async: Whether to use asynchronous loading.\n                If True, this function will not be lazy, but it will still work in the\n                expected way, just not lazy.\n            extractor: A function to extract document contents from raw html.\n                When extract function returns an empty string, the document is\n                ignored.\n            metadata_extractor: A function to extract metadata from raw html and the\n                source url (args in that order). Default extractor will attempt\n                to use BeautifulSoup4 to extract the title, description and language\n                of the page.\n            exclude_dirs: A list of subdirectories to exclude.\n            timeout: The timeout for the requests, in the unit of seconds. If None then\n                connection will not timeout.\n            prevent_outside: If True, prevent loading from urls which are not children\n                of the root url.\n            link_regex: Regex for extracting sub-links from the raw html of a web page.\n            check_response_status: If True, check HTTP response status and skip\n                URLs with error responses (400-599).\n        \"\"\"\n\n        self.url = url\n        self.max_depth = max_depth if max_depth is not None else 2\n        self.use_async = use_async if use_async is not None else False\n        self.extractor = extractor if extractor is not None else lambda x: x\n        self.metadata_extractor = (\n            metadata_extractor\n            if metadata_extractor is not None\n            else _metadata_extractor\n        )\n        self.exclude_dirs = exclude_dirs if exclude_dirs is not None else ()\n\n        if any(url.startswith(exclude_dir) for exclude_dir in self.exclude_dirs):\n            raise ValueError(\n                f\"Base url is included in exclude_dirs. Received base_url: {url} and \"\n                f\"exclude_dirs: {self.exclude_dirs}\"\n            )\n\n        self.timeout = timeout\n        self.prevent_outside = prevent_outside if prevent_outside is not None else True\n        self.link_regex = link_regex\n        self._lock = asyncio.Lock() if self.use_async else None\n        self.headers = headers\n        self.check_response_status = check_response_status\n\n    def _get_child_links_recursive(\n        self, url: str, visited: Set[str], *, depth: int = 0\n    ) -> Iterator[Document]:\n        \"\"\"Recursively get all child links starting with the path of the input URL.\n\n        Args:\n            url: The URL to crawl.\n            visited: A set of visited URLs.\n            depth: Current depth of recursion. Stop when depth >= max_depth.\n        \"\"\"\n\n        if depth >= self.max_depth:\n            return\n\n        # Get all links that can be accessed from the current URL\n        visited.add(url)\n        try:\n            response = requests.get(url, timeout=self.timeout, headers=self.headers)\n            if self.check_response_status and 400 <= response.status_code <= 599:\n                raise ValueError(f\"Received HTTP status {response.status_code}\")\n        except Exception as e:\n            logger.warning(\n                f\"Unable to load from {url}. Received error {e} of type \"\n                f\"{e.__class__.__name__}\"\n            )\n            return\n        content = self.extractor(response.text)\n        if content:\n            yield Document(\n                page_content=content,\n                metadata=self.metadata_extractor(response.text, url),\n            )\n\n        # Store the visited links and recursively visit the children\n        sub_links = extract_sub_links(\n            response.text,\n            url,\n            base_url=self.url,\n            pattern=self.link_regex,\n            prevent_outside=self.prevent_outside,\n            exclude_prefixes=self.exclude_dirs,\n        )\n        for link in sub_links:\n            # Check all unvisited links\n            if link not in visited:\n                yield from self._get_child_links_recursive(\n                    link, visited, depth=depth + 1\n                )\n\n    async def _async_get_child_links_recursive(\n        self,\n        url: str,\n        visited: Set[str],\n        *,\n        session: Optional[aiohttp.ClientSession] = None,\n        depth: int = 0,\n    ) -> List[Document]:\n        \"\"\"Recursively get all child links starting with the path of the input URL.\n\n        Args:\n            url: The URL to crawl.\n            visited: A set of visited URLs.\n            depth: To reach the current url, how many pages have been visited.\n        \"\"\"\n        try:\n            import aiohttp\n        except ImportError:\n            raise ImportError(\n                \"The aiohttp package is required for the RecursiveUrlLoader. \"\n                \"Please install it with `pip install aiohttp`.\"\n            )\n        if depth >= self.max_depth:\n            return []\n\n        # Disable SSL verification because websites may have invalid SSL certificates,\n        # but won't cause any security issues for us.\n        close_session = session is None\n        session = (\n            session\n            if session is not None\n            else aiohttp.ClientSession(\n                connector=aiohttp.TCPConnector(ssl=False),\n                timeout=aiohttp.ClientTimeout(total=self.timeout),\n                headers=self.headers,\n            )\n        )\n        async with self._lock:  # type: ignore\n            visited.add(url)\n        try:\n            async with session.get(url) as response:\n                text = await response.text()\n                if self.check_response_status and 400 <= response.status <= 599:\n                    raise ValueError(f\"Received HTTP status {response.status}\")\n        except (aiohttp.client_exceptions.InvalidURL, Exception) as e:\n            logger.warning(\n                f\"Unable to load {url}. Received error {e} of type \"\n                f\"{e.__class__.__name__}\"\n            )\n            if close_session:\n                await session.close()\n            return []\n        results = []\n        content = self.extractor(text)\n        if content:\n            results.append(\n                Document(\n                    page_content=content,\n                    metadata=self.metadata_extractor(text, url),\n                )\n            )\n        if depth < self.max_depth - 1:\n            sub_links = extract_sub_links(\n                text,\n                url,\n                base_url=self.url,\n                pattern=self.link_regex,\n                prevent_outside=self.prevent_outside,\n                exclude_prefixes=self.exclude_dirs,\n            )\n\n            # Recursively call the function to get the children of the children\n            sub_tasks = []\n            async with self._lock:  # type: ignore\n                to_visit = set(sub_links).difference(visited)\n                for link in to_visit:\n                    sub_tasks.append(\n                        self._async_get_child_links_recursive(\n                            link, visited, session=session, depth=depth + 1\n                        )\n                    )\n            next_results = await asyncio.gather(*sub_tasks)\n            for sub_result in next_results:\n                if isinstance(sub_result, Exception) or sub_result is None:\n                    # We don't want to stop the whole process, so just ignore it\n                    # Not standard html format or invalid url or 404 may cause this.\n                    continue\n                # locking not fully working, temporary hack to ensure deduplication\n                results += [r for r in sub_result if r not in results]\n        if close_session:\n            await session.close()\n        return results\n\n    def lazy_load(self) -> Iterator[Document]:\n        \"\"\"Lazy load web pages.\n        When use_async is True, this function will not be lazy,\n        but it will still work in the expected way, just not lazy.\"\"\"\n        visited: Set[str] = set()\n        if self.use_async:\n            results = asyncio.run(\n                self._async_get_child_links_recursive(self.url, visited)\n            )\n            return iter(results or [])\n        else:\n            return self._get_child_links_recursive(self.url, visited)\n\n    def load(self) -> List[Document]:\n        \"\"\"Load web pages.\"\"\"\n        return list(self.lazy_load())\n"], "filenames": ["libs/langchain/langchain/document_loaders/recursive_url_loader.py"], "buggy_code_start_loc": [52], "buggy_code_end_loc": [68], "fixing_code_start_loc": [52], "fixing_code_end_loc": [99], "type": "CWE-918", "message": "LangChain before 0.0.317 allows SSRF via document_loaders/recursive_url_loader.py because crawling can proceed from an external server to an internal server.", "other": {"cve": {"id": "CVE-2023-46229", "sourceIdentifier": "cve@mitre.org", "published": "2023-10-19T05:15:58.737", "lastModified": "2023-10-25T14:56:07.537", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "LangChain before 0.0.317 allows SSRF via document_loaders/recursive_url_loader.py because crawling can proceed from an external server to an internal server."}, {"lang": "es", "value": "LangChain anterior a 0.0.317 permite SSRF a trav\u00e9s de document_loaders/recursive_url_loader.py porque el rastreo puede proceder desde un servidor externo a un servidor interno."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-918"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:langchain:langchain:*:*:*:*:*:*:*:*", "versionEndExcluding": "0.0.317", "matchCriteriaId": "96BF88B4-C2B7-476C-8270-73C2DA83CD95"}]}]}], "references": [{"url": "https://github.com/langchain-ai/langchain/commit/9ecb7240a480720ec9d739b3877a52f76098a2b8", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://github.com/langchain-ai/langchain/pull/11925", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/langchain-ai/langchain/commit/9ecb7240a480720ec9d739b3877a52f76098a2b8"}}