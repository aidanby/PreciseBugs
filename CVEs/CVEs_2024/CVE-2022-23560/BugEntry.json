{"buggy_code": ["/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/kernels/internal/utils/sparsity_format_converter.h\"\n\n#include <cstdint>\n#include <utility>\n#include <vector>\n\nnamespace tflite {\nnamespace internal {\nnamespace sparsity {\n\nnamespace {\nuint64_t GetFlattenedIndex(const std::vector<int>& indices,\n                           const std::vector<int>& shape) {\n  uint64_t index = 0;\n  int sub_elements = 1;\n  for (int i = shape.size() - 1; i >= 0; i--) {\n    index += indices[i] * sub_elements;\n    sub_elements *= shape[i];\n  }\n  return index;\n}\n\nstd::vector<int> TfLiteIntArrayToVector(const TfLiteIntArray* int_array) {\n  std::vector<int> values;\n  if (!int_array) {\n    return values;\n  }\n\n  values.resize(int_array->size);\n  for (size_t i = 0; i < int_array->size; i++) {\n    values[i] = int_array->data[i];\n  }\n\n  return values;\n}\n\n}  // namespace\n\ntemplate <typename T>\nFormatConverter<T>::FormatConverter(\n    const std::vector<int>& shape, const std::vector<int>& traversal_order,\n    const std::vector<TfLiteDimensionType>& format,\n    const std::vector<int>& block_size, const std::vector<int>& block_map)\n    : dense_shape_(shape),\n      traversal_order_(traversal_order),\n      block_size_(block_size),\n      block_map_(block_map) {\n  dense_size_ = 1;\n  int block_dim = 0;\n  blocked_shape_.resize(shape.size());\n  format_.resize(shape.size() + block_map.size());\n  for (int i = 0; i < shape.size(); i++) {\n    format_[i] = format[traversal_order[i]];\n    dense_size_ *= shape[i];\n    if (block_dim < block_map.size() && block_map[block_dim] == i) {\n      blocked_shape_[i] = shape[i] / block_size[block_dim];\n      block_dim++;\n    } else {\n      blocked_shape_[i] = shape[i];\n    }\n  }\n\n  // Only dense blocks are supported.\n  for (int i = 0; i < block_map.size(); i++) {\n    format_[i + shape.size()] = kTfLiteDimDense;\n  }\n}\n\ntemplate <typename T>\nTfLiteStatus FormatConverter<T>::DenseToSparse(const T* src_data) {\n  int num_original_dims = dense_shape_.size();\n  int num_block_dims = block_map_.size();\n  int num_expanded_dims = num_original_dims + num_block_dims;\n  std::vector<int> expanded_shape(num_expanded_dims);\n  for (int i = 0; i < num_expanded_dims; i++) {\n    if (i < num_original_dims) {\n      expanded_shape[i] = blocked_shape_[i];\n    } else {\n      expanded_shape[i] = block_size_[i - num_original_dims];\n    }\n  }\n\n  std::vector<int> shape_offset(num_original_dims);\n  shape_offset[shape_offset.size() - 1] = 1;\n  for (int i = num_original_dims - 1; i > 0; --i) {\n    shape_offset[i - 1] = shape_offset[i] * dense_shape_[i];\n  }\n\n  std::vector<int> expanded_shape_offset(num_expanded_dims);\n  for (int i = 0; i < num_original_dims; ++i) {\n    expanded_shape_offset[i] = shape_offset[i];\n  }\n  for (int i = 0; i < num_block_dims; ++i) {\n    int mapped_dim = block_map_[i];\n    expanded_shape_offset[num_original_dims + i] = shape_offset[mapped_dim];\n    expanded_shape_offset[mapped_dim] *= block_size_[i];\n  }\n\n  std::vector<int> dst_ordered_offset(num_expanded_dims);\n  for (int i = 0; i < num_expanded_dims; ++i) {\n    dst_ordered_offset[i] = expanded_shape_offset[traversal_order_[i]];\n  }\n\n  std::vector<bool> dst_dim_has_nonzeroes(num_expanded_dims);\n  std::fill(dst_dim_has_nonzeroes.begin(), dst_dim_has_nonzeroes.end(), false);\n  std::vector<int> inner_compressed_dim(num_expanded_dims);\n  int most_recent_compressed_dim = -1;\n  std::vector<int> num_segments_of_next_compressed_dim(num_expanded_dims);\n  int segment_count = 1;\n  for (int i = num_expanded_dims - 1; i >= 0; --i) {\n    inner_compressed_dim[i] = most_recent_compressed_dim;\n    if (format_[i] == kTfLiteDimSparseCSR) {\n      most_recent_compressed_dim = i;\n      num_segments_of_next_compressed_dim[i] = segment_count;\n      segment_count = 1;\n    } else {\n      num_segments_of_next_compressed_dim[i] = -1;\n      segment_count *= expanded_shape[traversal_order_[i]];\n    }\n  }\n\n  dim_metadata_.resize(num_expanded_dims * 2);\n  std::vector<int> dst_sparse_dims;\n  dst_sparse_dims.reserve(num_expanded_dims);\n  for (int i = 0; i < num_expanded_dims; ++i) {\n    dim_metadata_[i * 2].clear();\n    dim_metadata_[i * 2 + 1].clear();\n    if (format_[i] == kTfLiteDimDense) {\n      // If dimension is dense, just store the shape.\n      dim_metadata_[i * 2].push_back(expanded_shape[traversal_order_[i]]);\n    } else {\n      dim_metadata_[i * 2].push_back(0);  // Segment array always begins with 0.\n      dst_sparse_dims.push_back(i);       // Add dimension to the sparse list.\n    }\n  }\n\n  // This algorithm assumes that the block size is small enough for all the\n  // elements to fit in cache, so the strided accesses from different traversal\n  // order and the write-first-erase-later strategy shouldn't be too slow\n  int dst_dim_idx = num_expanded_dims;\n  std::vector<int> coordinate(num_expanded_dims, 0);\n  int dense_tensor_idx = 0;\n  while (dst_dim_idx >= 0) {\n    if (dst_dim_idx == num_expanded_dims) {\n      // We have a complete coordinate. Add the element to the value array if it\n      // is not zero, or if the last dimension is dense.\n      if (!IsZero(src_data[dense_tensor_idx])) {\n        data_.push_back(src_data[dense_tensor_idx]);\n        // Mark all sparse dimensions that their current indices have nonzeroes.\n        for (auto dst_dim : dst_sparse_dims) {\n          if (!dst_dim_has_nonzeroes[dst_dim]) {\n            // Only add the index to the indices array if the current nonzero\n            // is the first nonzero of the block.\n            dim_metadata_[2 * dst_dim + 1].push_back(coordinate[dst_dim]);\n            dst_dim_has_nonzeroes[dst_dim] = true;\n          }\n        }\n      } else if (format_[num_expanded_dims - 1] == kTfLiteDimDense) {\n        data_.push_back(src_data[dense_tensor_idx]);\n      }\n      --dst_dim_idx;\n    } else {\n      int original_dim_idx = traversal_order_[dst_dim_idx];\n      int dim_size = expanded_shape[original_dim_idx];\n      if (dst_dim_has_nonzeroes[dst_dim_idx]) {\n        // If the previous block has nonzeroes, reset the flag to false since\n        // we have just moved to a new block.\n        dst_dim_has_nonzeroes[dst_dim_idx] = false;\n      } else if (format_[dst_dim_idx] == kTfLiteDimSparseCSR) {\n        // This block is empty. Delete unnecessary values if compressed.\n        int next_compressed_dim = inner_compressed_dim[dst_dim_idx];\n        int erase_offset = dim_metadata_[2 * dst_dim_idx + 1].size() *\n                           num_segments_of_next_compressed_dim[dst_dim_idx];\n        if (next_compressed_dim >= 0) {\n          auto& segments = dim_metadata_[2 * inner_compressed_dim[dst_dim_idx]];\n          segments.erase(segments.begin() + 1 + erase_offset, segments.end());\n        } else {\n          data_.erase(data_.begin() + erase_offset, data_.end());\n        }\n      }\n      if (++coordinate[dst_dim_idx] < dim_size) {\n        // The current dst_dim_idx is valid (not out of bound).\n        dense_tensor_idx += dst_ordered_offset[dst_dim_idx];\n        ++dst_dim_idx;\n      } else {\n        // dst_dim_idx has reached its dim size. Update segment array and go\n        // back to incrementing the previous dimension (dst_dim_idx - 1).\n        if (format_[dst_dim_idx] == kTfLiteDimSparseCSR) {\n          dim_metadata_[2 * dst_dim_idx].push_back(\n              dim_metadata_[2 * dst_dim_idx + 1].size());\n        }\n        coordinate[dst_dim_idx] = -1;\n        dense_tensor_idx -= dst_ordered_offset[dst_dim_idx] * dim_size;\n        --dst_dim_idx;\n      }\n    }\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <typename T>\nFormatConverter<T>::FormatConverter(\n    const std::vector<int>& shape, const std::vector<int>& traversal_order,\n    const std::vector<TfLiteDimensionType>& format,\n    const std::vector<int>& dense_size,\n    const std::vector<std::vector<int>>& segments,\n    const std::vector<std::vector<int>>& indices,\n    const std::vector<int>& block_map) {\n  InitSparseToDenseConverter(shape, traversal_order, format, dense_size,\n                             segments, indices, block_map);\n}\n\ntemplate <typename T>\nFormatConverter<T>::FormatConverter(const std::vector<int>& shape,\n                                    const TfLiteSparsity& sparsity) {\n  auto traversal_order = TfLiteIntArrayToVector(sparsity.traversal_order);\n  auto block_map = TfLiteIntArrayToVector(sparsity.block_map);\n\n  std::vector<TfLiteDimensionType> format(sparsity.dim_metadata_size);\n  std::vector<int> dense_size(sparsity.dim_metadata_size);\n  std::vector<std::vector<int>> segments(sparsity.dim_metadata_size);\n  std::vector<std::vector<int>> indices(sparsity.dim_metadata_size);\n  for (int i = 0; i < sparsity.dim_metadata_size; i++) {\n    format[i] = sparsity.dim_metadata[i].format;\n    dense_size[i] = sparsity.dim_metadata[i].dense_size;\n    segments[i] =\n        TfLiteIntArrayToVector(sparsity.dim_metadata[i].array_segments);\n    indices[i] = TfLiteIntArrayToVector(sparsity.dim_metadata[i].array_indices);\n  }\n\n  InitSparseToDenseConverter(shape, std::move(traversal_order),\n                             std::move(format), std::move(dense_size),\n                             std::move(segments), std::move(indices),\n                             std::move(block_map));\n}\n\ntemplate <typename T>\nvoid FormatConverter<T>::InitSparseToDenseConverter(\n    std::vector<int> shape, std::vector<int> traversal_order,\n    std::vector<TfLiteDimensionType> format, std::vector<int> dense_size,\n    std::vector<std::vector<int>> segments,\n    std::vector<std::vector<int>> indices, std::vector<int> block_map) {\n  dense_shape_ = std::move(shape);\n  traversal_order_ = std::move(traversal_order);\n  block_map_ = std::move(block_map);\n  format_ = std::move(format);\n\n  dense_size_ = 1;\n  for (int i = 0; i < dense_shape_.size(); i++) {\n    dense_size_ *= dense_shape_[i];\n  }\n\n  dim_metadata_.resize(2 * format_.size());\n  for (int i = 0; i < format_.size(); i++) {\n    if (format_[i] == kTfLiteDimDense) {\n      dim_metadata_[2 * i] = {dense_size[i]};\n    } else {\n      dim_metadata_[2 * i] = std::move(segments[i]);\n      dim_metadata_[2 * i + 1] = std::move(indices[i]);\n    }\n  }\n\n  int original_rank = dense_shape_.size();\n  int block_dim = 0;\n\n  blocked_shape_.resize(original_rank);\n  block_size_.resize(block_map_.size());\n  for (int i = 0; i < original_rank; i++) {\n    if (block_dim < block_map_.size() && block_map_[block_dim] == i) {\n      int orig_dim = traversal_order_[original_rank + block_dim];\n      block_size_[block_dim] = dense_size[orig_dim];\n      blocked_shape_[i] = dense_shape_[i] / dense_size[orig_dim];\n      block_dim++;\n    } else {\n      blocked_shape_[i] = dense_shape_[i];\n    }\n  }\n}\n\ntemplate <typename T>\nvoid FormatConverter<T>::Populate(const T* src_data, std::vector<int> indices,\n                                  int level, int prev_idx, int* src_data_ptr,\n                                  T* dest_data) {\n  if (level == indices.size()) {\n    int orig_rank = dense_shape_.size();\n    std::vector<int> orig_idx;\n    orig_idx.resize(orig_rank);\n    int i = 0;\n    for (; i < orig_idx.size(); i++) {\n      int orig_dim = traversal_order_[i];\n      orig_idx[orig_dim] = indices[i];\n    }\n\n    for (; i < indices.size(); i++) {\n      const int block_idx = traversal_order_[i] - orig_rank;\n      const int orig_dim = block_map_[block_idx];\n      orig_idx[orig_dim] =\n          orig_idx[orig_dim] * block_size_[block_idx] + indices[i];\n    }\n\n    dest_data[GetFlattenedIndex(orig_idx, dense_shape_)] =\n        src_data[*src_data_ptr];\n\n    *src_data_ptr = *src_data_ptr + 1;\n    return;\n  }\n\n  const int metadata_idx = 2 * level;\n  const int shape_of_level = dim_metadata_[metadata_idx][0];\n  if (format_[level] == kTfLiteDimDense) {\n    for (int i = 0; i < shape_of_level; i++) {\n      indices[level] = i;\n      Populate(src_data, indices, level + 1, prev_idx * shape_of_level + i,\n               src_data_ptr, dest_data);\n    }\n  } else {\n    const auto& array_segments = dim_metadata_[metadata_idx];\n    const auto& array_indices = dim_metadata_[metadata_idx + 1];\n    for (int i = array_segments[prev_idx]; i < array_segments[prev_idx + 1];\n         i++) {\n      indices[level] = array_indices[i];\n      Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\n    }\n  }\n}\n\ntemplate <typename T>\nTfLiteStatus FormatConverter<T>::SparseToDense(const T* src_data) {\n  data_.resize(dense_size_);\n  std::fill(data_.begin(), data_.end(), T(0));\n\n  int total_rank = traversal_order_.size();\n  int src_data_ptr = 0;\n  std::vector<int> indices(total_rank);\n  Populate(src_data, indices, 0, 0, &src_data_ptr, data_.data());\n\n  return kTfLiteOk;\n}\n\ntemplate <typename T>\nTfLiteStatus FormatConverter<T>::SparseToDense(const T* src_data,\n                                               const size_t dest_size,\n                                               T* dest_data,\n                                               TfLiteContext* context) {\n  if (dest_size != dense_size_) {\n    TF_LITE_MAYBE_KERNEL_LOG(\n        context, \"unexpected buffer size for densified data, expected %lld.\\n\",\n        dense_size_);\n    return kTfLiteError;\n  }\n\n  // For types like Eigen::half, we cannot do a simple memset() with 0 values.\n  for (auto i = 0; i < dest_size; i++) {\n    dest_data[i] = T(0);\n  }\n\n  const int total_rank = traversal_order_.size();\n  int src_data_ptr = 0;\n  std::vector<int> indices(total_rank);\n  Populate(src_data, indices, 0, 0, &src_data_ptr, dest_data);\n\n  return kTfLiteOk;\n}\n\ntemplate <typename T>\nbool FormatConverter<T>::IsZero(const T val) {\n  return (val == static_cast<T>(0));\n}\n\ntemplate class FormatConverter<int32_t>;\ntemplate class FormatConverter<int8_t>;\ntemplate class FormatConverter<float>;\ntemplate class FormatConverter<Eigen::half>;\n\n}  // namespace sparsity\n}  // namespace internal\n}  // namespace tflite\n"], "fixing_code": ["/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/kernels/internal/utils/sparsity_format_converter.h\"\n\n#include <cstdint>\n#include <utility>\n#include <vector>\n\nnamespace tflite {\nnamespace internal {\nnamespace sparsity {\n\nnamespace {\nuint64_t GetFlattenedIndex(const std::vector<int>& indices,\n                           const std::vector<int>& shape) {\n  uint64_t index = 0;\n  int sub_elements = 1;\n  for (int i = shape.size() - 1; i >= 0; i--) {\n    index += indices[i] * sub_elements;\n    sub_elements *= shape[i];\n  }\n  return index;\n}\n\nstd::vector<int> TfLiteIntArrayToVector(const TfLiteIntArray* int_array) {\n  std::vector<int> values;\n  if (!int_array) {\n    return values;\n  }\n\n  values.resize(int_array->size);\n  for (size_t i = 0; i < int_array->size; i++) {\n    values[i] = int_array->data[i];\n  }\n\n  return values;\n}\n\n}  // namespace\n\ntemplate <typename T>\nFormatConverter<T>::FormatConverter(\n    const std::vector<int>& shape, const std::vector<int>& traversal_order,\n    const std::vector<TfLiteDimensionType>& format,\n    const std::vector<int>& block_size, const std::vector<int>& block_map)\n    : dense_shape_(shape),\n      traversal_order_(traversal_order),\n      block_size_(block_size),\n      block_map_(block_map) {\n  dense_size_ = 1;\n  int block_dim = 0;\n  blocked_shape_.resize(shape.size());\n  format_.resize(shape.size() + block_map.size());\n  for (int i = 0; i < shape.size(); i++) {\n    format_[i] = format[traversal_order[i]];\n    dense_size_ *= shape[i];\n    if (block_dim < block_map.size() && block_map[block_dim] == i) {\n      blocked_shape_[i] = shape[i] / block_size[block_dim];\n      block_dim++;\n    } else {\n      blocked_shape_[i] = shape[i];\n    }\n  }\n\n  // Only dense blocks are supported.\n  for (int i = 0; i < block_map.size(); i++) {\n    format_[i + shape.size()] = kTfLiteDimDense;\n  }\n}\n\ntemplate <typename T>\nTfLiteStatus FormatConverter<T>::DenseToSparse(const T* src_data) {\n  int num_original_dims = dense_shape_.size();\n  int num_block_dims = block_map_.size();\n  int num_expanded_dims = num_original_dims + num_block_dims;\n  std::vector<int> expanded_shape(num_expanded_dims);\n  for (int i = 0; i < num_expanded_dims; i++) {\n    if (i < num_original_dims) {\n      expanded_shape[i] = blocked_shape_[i];\n    } else {\n      expanded_shape[i] = block_size_[i - num_original_dims];\n    }\n  }\n\n  std::vector<int> shape_offset(num_original_dims);\n  shape_offset[shape_offset.size() - 1] = 1;\n  for (int i = num_original_dims - 1; i > 0; --i) {\n    shape_offset[i - 1] = shape_offset[i] * dense_shape_[i];\n  }\n\n  std::vector<int> expanded_shape_offset(num_expanded_dims);\n  for (int i = 0; i < num_original_dims; ++i) {\n    expanded_shape_offset[i] = shape_offset[i];\n  }\n  for (int i = 0; i < num_block_dims; ++i) {\n    int mapped_dim = block_map_[i];\n    expanded_shape_offset[num_original_dims + i] = shape_offset[mapped_dim];\n    expanded_shape_offset[mapped_dim] *= block_size_[i];\n  }\n\n  std::vector<int> dst_ordered_offset(num_expanded_dims);\n  for (int i = 0; i < num_expanded_dims; ++i) {\n    dst_ordered_offset[i] = expanded_shape_offset[traversal_order_[i]];\n  }\n\n  std::vector<bool> dst_dim_has_nonzeroes(num_expanded_dims);\n  std::fill(dst_dim_has_nonzeroes.begin(), dst_dim_has_nonzeroes.end(), false);\n  std::vector<int> inner_compressed_dim(num_expanded_dims);\n  int most_recent_compressed_dim = -1;\n  std::vector<int> num_segments_of_next_compressed_dim(num_expanded_dims);\n  int segment_count = 1;\n  for (int i = num_expanded_dims - 1; i >= 0; --i) {\n    inner_compressed_dim[i] = most_recent_compressed_dim;\n    if (format_[i] == kTfLiteDimSparseCSR) {\n      most_recent_compressed_dim = i;\n      num_segments_of_next_compressed_dim[i] = segment_count;\n      segment_count = 1;\n    } else {\n      num_segments_of_next_compressed_dim[i] = -1;\n      segment_count *= expanded_shape[traversal_order_[i]];\n    }\n  }\n\n  dim_metadata_.resize(num_expanded_dims * 2);\n  std::vector<int> dst_sparse_dims;\n  dst_sparse_dims.reserve(num_expanded_dims);\n  for (int i = 0; i < num_expanded_dims; ++i) {\n    dim_metadata_[i * 2].clear();\n    dim_metadata_[i * 2 + 1].clear();\n    if (format_[i] == kTfLiteDimDense) {\n      // If dimension is dense, just store the shape.\n      dim_metadata_[i * 2].push_back(expanded_shape[traversal_order_[i]]);\n    } else {\n      dim_metadata_[i * 2].push_back(0);  // Segment array always begins with 0.\n      dst_sparse_dims.push_back(i);       // Add dimension to the sparse list.\n    }\n  }\n\n  // This algorithm assumes that the block size is small enough for all the\n  // elements to fit in cache, so the strided accesses from different traversal\n  // order and the write-first-erase-later strategy shouldn't be too slow\n  int dst_dim_idx = num_expanded_dims;\n  std::vector<int> coordinate(num_expanded_dims, 0);\n  int dense_tensor_idx = 0;\n  while (dst_dim_idx >= 0) {\n    if (dst_dim_idx == num_expanded_dims) {\n      // We have a complete coordinate. Add the element to the value array if it\n      // is not zero, or if the last dimension is dense.\n      if (!IsZero(src_data[dense_tensor_idx])) {\n        data_.push_back(src_data[dense_tensor_idx]);\n        // Mark all sparse dimensions that their current indices have nonzeroes.\n        for (auto dst_dim : dst_sparse_dims) {\n          if (!dst_dim_has_nonzeroes[dst_dim]) {\n            // Only add the index to the indices array if the current nonzero\n            // is the first nonzero of the block.\n            dim_metadata_[2 * dst_dim + 1].push_back(coordinate[dst_dim]);\n            dst_dim_has_nonzeroes[dst_dim] = true;\n          }\n        }\n      } else if (format_[num_expanded_dims - 1] == kTfLiteDimDense) {\n        data_.push_back(src_data[dense_tensor_idx]);\n      }\n      --dst_dim_idx;\n    } else {\n      int original_dim_idx = traversal_order_[dst_dim_idx];\n      int dim_size = expanded_shape[original_dim_idx];\n      if (dst_dim_has_nonzeroes[dst_dim_idx]) {\n        // If the previous block has nonzeroes, reset the flag to false since\n        // we have just moved to a new block.\n        dst_dim_has_nonzeroes[dst_dim_idx] = false;\n      } else if (format_[dst_dim_idx] == kTfLiteDimSparseCSR) {\n        // This block is empty. Delete unnecessary values if compressed.\n        int next_compressed_dim = inner_compressed_dim[dst_dim_idx];\n        int erase_offset = dim_metadata_[2 * dst_dim_idx + 1].size() *\n                           num_segments_of_next_compressed_dim[dst_dim_idx];\n        if (next_compressed_dim >= 0) {\n          auto& segments = dim_metadata_[2 * inner_compressed_dim[dst_dim_idx]];\n          segments.erase(segments.begin() + 1 + erase_offset, segments.end());\n        } else {\n          data_.erase(data_.begin() + erase_offset, data_.end());\n        }\n      }\n      if (++coordinate[dst_dim_idx] < dim_size) {\n        // The current dst_dim_idx is valid (not out of bound).\n        dense_tensor_idx += dst_ordered_offset[dst_dim_idx];\n        ++dst_dim_idx;\n      } else {\n        // dst_dim_idx has reached its dim size. Update segment array and go\n        // back to incrementing the previous dimension (dst_dim_idx - 1).\n        if (format_[dst_dim_idx] == kTfLiteDimSparseCSR) {\n          dim_metadata_[2 * dst_dim_idx].push_back(\n              dim_metadata_[2 * dst_dim_idx + 1].size());\n        }\n        coordinate[dst_dim_idx] = -1;\n        dense_tensor_idx -= dst_ordered_offset[dst_dim_idx] * dim_size;\n        --dst_dim_idx;\n      }\n    }\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <typename T>\nFormatConverter<T>::FormatConverter(\n    const std::vector<int>& shape, const std::vector<int>& traversal_order,\n    const std::vector<TfLiteDimensionType>& format,\n    const std::vector<int>& dense_size,\n    const std::vector<std::vector<int>>& segments,\n    const std::vector<std::vector<int>>& indices,\n    const std::vector<int>& block_map) {\n  InitSparseToDenseConverter(shape, traversal_order, format, dense_size,\n                             segments, indices, block_map);\n}\n\ntemplate <typename T>\nFormatConverter<T>::FormatConverter(const std::vector<int>& shape,\n                                    const TfLiteSparsity& sparsity) {\n  auto traversal_order = TfLiteIntArrayToVector(sparsity.traversal_order);\n  auto block_map = TfLiteIntArrayToVector(sparsity.block_map);\n\n  std::vector<TfLiteDimensionType> format(sparsity.dim_metadata_size);\n  std::vector<int> dense_size(sparsity.dim_metadata_size);\n  std::vector<std::vector<int>> segments(sparsity.dim_metadata_size);\n  std::vector<std::vector<int>> indices(sparsity.dim_metadata_size);\n  for (int i = 0; i < sparsity.dim_metadata_size; i++) {\n    format[i] = sparsity.dim_metadata[i].format;\n    dense_size[i] = sparsity.dim_metadata[i].dense_size;\n    segments[i] =\n        TfLiteIntArrayToVector(sparsity.dim_metadata[i].array_segments);\n    indices[i] = TfLiteIntArrayToVector(sparsity.dim_metadata[i].array_indices);\n  }\n\n  InitSparseToDenseConverter(shape, std::move(traversal_order),\n                             std::move(format), std::move(dense_size),\n                             std::move(segments), std::move(indices),\n                             std::move(block_map));\n}\n\ntemplate <typename T>\nvoid FormatConverter<T>::InitSparseToDenseConverter(\n    std::vector<int> shape, std::vector<int> traversal_order,\n    std::vector<TfLiteDimensionType> format, std::vector<int> dense_size,\n    std::vector<std::vector<int>> segments,\n    std::vector<std::vector<int>> indices, std::vector<int> block_map) {\n  dense_shape_ = std::move(shape);\n  traversal_order_ = std::move(traversal_order);\n  block_map_ = std::move(block_map);\n  format_ = std::move(format);\n\n  dense_size_ = 1;\n  for (int i = 0; i < dense_shape_.size(); i++) {\n    dense_size_ *= dense_shape_[i];\n  }\n\n  dim_metadata_.resize(2 * format_.size());\n  for (int i = 0; i < format_.size(); i++) {\n    if (format_[i] == kTfLiteDimDense) {\n      dim_metadata_[2 * i] = {dense_size[i]};\n    } else {\n      dim_metadata_[2 * i] = std::move(segments[i]);\n      dim_metadata_[2 * i + 1] = std::move(indices[i]);\n    }\n  }\n\n  int original_rank = dense_shape_.size();\n  int block_dim = 0;\n\n  blocked_shape_.resize(original_rank);\n  block_size_.resize(block_map_.size());\n  for (int i = 0; i < original_rank; i++) {\n    if (block_dim < block_map_.size() && block_map_[block_dim] == i) {\n      if (original_rank + block_dim < traversal_order_.size()) {\n        int orig_dim = traversal_order_[original_rank + block_dim];\n        block_size_[block_dim] = dense_size[orig_dim];\n        blocked_shape_[i] = dense_shape_[i] / dense_size[orig_dim];\n        block_dim++;\n      }\n    } else {\n      blocked_shape_[i] = dense_shape_[i];\n    }\n  }\n}\n\ntemplate <typename T>\nvoid FormatConverter<T>::Populate(const T* src_data, std::vector<int> indices,\n                                  int level, int prev_idx, int* src_data_ptr,\n                                  T* dest_data) {\n  if (level == indices.size()) {\n    int orig_rank = dense_shape_.size();\n    std::vector<int> orig_idx;\n    orig_idx.resize(orig_rank);\n    int i = 0;\n    for (; i < orig_idx.size(); i++) {\n      int orig_dim = traversal_order_[i];\n      orig_idx[orig_dim] = indices[i];\n    }\n\n    for (; i < indices.size(); i++) {\n      const int block_idx = traversal_order_[i] - orig_rank;\n      const int orig_dim = block_map_[block_idx];\n      orig_idx[orig_dim] =\n          orig_idx[orig_dim] * block_size_[block_idx] + indices[i];\n    }\n\n    dest_data[GetFlattenedIndex(orig_idx, dense_shape_)] =\n        src_data[*src_data_ptr];\n\n    *src_data_ptr = *src_data_ptr + 1;\n    return;\n  }\n\n  const int metadata_idx = 2 * level;\n  const int shape_of_level = dim_metadata_[metadata_idx][0];\n  if (format_[level] == kTfLiteDimDense) {\n    for (int i = 0; i < shape_of_level; i++) {\n      indices[level] = i;\n      Populate(src_data, indices, level + 1, prev_idx * shape_of_level + i,\n               src_data_ptr, dest_data);\n    }\n  } else if (prev_idx + 1 < dim_metadata_[metadata_idx].size()) {\n    const auto& array_segments = dim_metadata_[metadata_idx];\n    const auto& array_indices = dim_metadata_[metadata_idx + 1];\n    for (int i = array_segments[prev_idx]; i < array_segments[prev_idx + 1];\n         i++) {\n      if (i < array_indices.size() && level < indices.size()) {\n        indices[level] = array_indices[i];\n        Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\n      }\n    }\n  }\n}\n\ntemplate <typename T>\nTfLiteStatus FormatConverter<T>::SparseToDense(const T* src_data) {\n  data_.resize(dense_size_);\n  std::fill(data_.begin(), data_.end(), T(0));\n\n  int total_rank = traversal_order_.size();\n  int src_data_ptr = 0;\n  std::vector<int> indices(total_rank);\n  Populate(src_data, indices, 0, 0, &src_data_ptr, data_.data());\n\n  return kTfLiteOk;\n}\n\ntemplate <typename T>\nTfLiteStatus FormatConverter<T>::SparseToDense(const T* src_data,\n                                               const size_t dest_size,\n                                               T* dest_data,\n                                               TfLiteContext* context) {\n  if (dest_size != dense_size_) {\n    TF_LITE_MAYBE_KERNEL_LOG(\n        context, \"unexpected buffer size for densified data, expected %lld.\\n\",\n        dense_size_);\n    return kTfLiteError;\n  }\n\n  // For types like Eigen::half, we cannot do a simple memset() with 0 values.\n  for (auto i = 0; i < dest_size; i++) {\n    dest_data[i] = T(0);\n  }\n\n  const int total_rank = traversal_order_.size();\n  int src_data_ptr = 0;\n  std::vector<int> indices(total_rank);\n  Populate(src_data, indices, 0, 0, &src_data_ptr, dest_data);\n\n  return kTfLiteOk;\n}\n\ntemplate <typename T>\nbool FormatConverter<T>::IsZero(const T val) {\n  return (val == static_cast<T>(0));\n}\n\ntemplate class FormatConverter<int32_t>;\ntemplate class FormatConverter<int8_t>;\ntemplate class FormatConverter<float>;\ntemplate class FormatConverter<Eigen::half>;\n\n}  // namespace sparsity\n}  // namespace internal\n}  // namespace tflite\n"], "filenames": ["tensorflow/lite/kernels/internal/utils/sparsity_format_converter.cc"], "buggy_code_start_loc": [285], "buggy_code_end_loc": [338], "fixing_code_start_loc": [285], "fixing_code_end_loc": [342], "type": "CWE-125", "message": "Tensorflow is an Open Source Machine Learning Framework. An attacker can craft a TFLite model that would allow limited reads and writes outside of arrays in TFLite. This exploits missing validation in the conversion from sparse tensors to dense tensors. The fix is included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range. Users are advised to upgrade as soon as possible.", "other": {"cve": {"id": "CVE-2022-23560", "sourceIdentifier": "security-advisories@github.com", "published": "2022-02-04T23:15:13.737", "lastModified": "2022-02-09T18:55:25.457", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Tensorflow is an Open Source Machine Learning Framework. An attacker can craft a TFLite model that would allow limited reads and writes outside of arrays in TFLite. This exploits missing validation in the conversion from sparse tensors to dense tensors. The fix is included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range. Users are advised to upgrade as soon as possible."}, {"lang": "es", "value": "Tensorflow es un Marco de Aprendizaje Autom\u00e1tico de C\u00f3digo Abierto. Un atacante puede dise\u00f1ar un modelo de TFLite que permita lecturas y escrituras limitadas fuera de las matrices en TFLite. Esto explota una falta de comprobaci\u00f3n en la conversi\u00f3n de tensores dispersos a tensores densos. La correcci\u00f3n es incluida en TensorFlow versi\u00f3n 2.8.0. Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.7.1, TensorFlow versi\u00f3n 2.6.3, y TensorFlow versi\u00f3n 2.5.3, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido. Se recomienda a usuarios que actualicen lo antes posible"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.5}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}, {"lang": "en", "value": "CWE-787"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.5.2", "matchCriteriaId": "688150BF-477C-48FC-9AEF-A79AC57A6DDC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndIncluding": "2.6.2", "matchCriteriaId": "C9E69B60-8C97-47E2-9027-9598B8392E5D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:*:*:*:*:*:*:*", "matchCriteriaId": "2EDFAAB8-799C-4259-9102-944D4760DA2C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/ca6f96b62ad84207fbec580404eaa7dd7403a550/tensorflow/lite/kernels/internal/utils/sparsity_format_converter.cc#L252-L293", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/6364463d6f5b6254cac3d6aedf999b6a96225038", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-4hvf-hxvg-f67v", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/6364463d6f5b6254cac3d6aedf999b6a96225038"}}