{"buggy_code": ["/*\n * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers\n * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.\n * Copyright (c) 1999-2004 Hewlett-Packard Development Company, L.P.\n *\n * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED\n * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.\n *\n * Permission is hereby granted to use or copy this program\n * for any purpose,  provided the above notices are retained on all copies.\n * Permission to modify the code and to distribute modified code is granted,\n * provided the above notices are retained, and a notice that the code was\n * modified is included with the above copyright notice.\n */\n\n#include \"private/gc_priv.h\"\n\n#include <stdio.h>\n#include <string.h>\n\n/* Allocate reclaim list for kind:      */\n/* Return TRUE on success               */\nSTATIC GC_bool GC_alloc_reclaim_list(struct obj_kind *kind)\n{\n    struct hblk ** result = (struct hblk **)\n                GC_scratch_alloc((MAXOBJGRANULES+1) * sizeof(struct hblk *));\n    if (result == 0) return(FALSE);\n    BZERO(result, (MAXOBJGRANULES+1)*sizeof(struct hblk *));\n    kind -> ok_reclaim_list = result;\n    return(TRUE);\n}\n\nGC_INNER GC_bool GC_collect_or_expand(word needed_blocks,\n                                      GC_bool ignore_off_page,\n                                      GC_bool retry); /* from alloc.c */\n\n/* Allocate a large block of size lb bytes.     */\n/* The block is not cleared.                    */\n/* Flags is 0 or IGNORE_OFF_PAGE.               */\n/* We hold the allocation lock.                 */\n/* EXTRA_BYTES were already added to lb.        */\nGC_INNER ptr_t GC_alloc_large(size_t lb, int k, unsigned flags)\n{\n    struct hblk * h;\n    word n_blocks;\n    ptr_t result;\n    GC_bool retry = FALSE;\n\n    /* Round up to a multiple of a granule. */\n      lb = (lb + GRANULE_BYTES - 1) & ~(GRANULE_BYTES - 1);\n    n_blocks = OBJ_SZ_TO_BLOCKS(lb);\n    if (!EXPECT(GC_is_initialized, TRUE)) GC_init();\n    /* Do our share of marking work */\n        if (GC_incremental && !GC_dont_gc)\n            GC_collect_a_little_inner((int)n_blocks);\n    h = GC_allochblk(lb, k, flags);\n#   ifdef USE_MUNMAP\n        if (0 == h) {\n            GC_merge_unmapped();\n            h = GC_allochblk(lb, k, flags);\n        }\n#   endif\n    while (0 == h && GC_collect_or_expand(n_blocks, flags != 0, retry)) {\n        h = GC_allochblk(lb, k, flags);\n        retry = TRUE;\n    }\n    if (h == 0) {\n        result = 0;\n    } else {\n        size_t total_bytes = n_blocks * HBLKSIZE;\n        if (n_blocks > 1) {\n            GC_large_allocd_bytes += total_bytes;\n            if (GC_large_allocd_bytes > GC_max_large_allocd_bytes)\n                GC_max_large_allocd_bytes = GC_large_allocd_bytes;\n        }\n        result = h -> hb_body;\n    }\n    return result;\n}\n\n/* Allocate a large block of size lb bytes.  Clear if appropriate.      */\n/* We hold the allocation lock.                                         */\n/* EXTRA_BYTES were already added to lb.                                */\nSTATIC ptr_t GC_alloc_large_and_clear(size_t lb, int k, unsigned flags)\n{\n    ptr_t result = GC_alloc_large(lb, k, flags);\n    word n_blocks = OBJ_SZ_TO_BLOCKS(lb);\n\n    if (0 == result) return 0;\n    if (GC_debugging_started || GC_obj_kinds[k].ok_init) {\n        /* Clear the whole block, in case of GC_realloc call. */\n        BZERO(result, n_blocks * HBLKSIZE);\n    }\n    return result;\n}\n\n/* allocate lb bytes for an object of kind k.   */\n/* Should not be used to directly to allocate   */\n/* objects such as STUBBORN objects that        */\n/* require special handling on allocation.      */\n/* First a version that assumes we already      */\n/* hold lock:                                   */\nGC_INNER void * GC_generic_malloc_inner(size_t lb, int k)\n{\n    void *op;\n\n    if(SMALL_OBJ(lb)) {\n        struct obj_kind * kind = GC_obj_kinds + k;\n        size_t lg = GC_size_map[lb];\n        void ** opp = &(kind -> ok_freelist[lg]);\n\n        op = *opp;\n        if (EXPECT(0 == op, FALSE)) {\n            if (GC_size_map[lb] == 0) {\n              if (!EXPECT(GC_is_initialized, TRUE)) GC_init();\n              if (GC_size_map[lb] == 0) GC_extend_size_map(lb);\n              return(GC_generic_malloc_inner(lb, k));\n            }\n            if (kind -> ok_reclaim_list == 0) {\n                if (!GC_alloc_reclaim_list(kind)) goto out;\n            }\n            op = GC_allocobj(lg, k);\n            if (op == 0) goto out;\n        }\n        *opp = obj_link(op);\n        obj_link(op) = 0;\n        GC_bytes_allocd += GRANULES_TO_BYTES(lg);\n    } else {\n        op = (ptr_t)GC_alloc_large_and_clear(ADD_SLOP(lb), k, 0);\n        GC_bytes_allocd += lb;\n    }\n\nout:\n    return op;\n}\n\n/* Allocate a composite object of size n bytes.  The caller guarantees  */\n/* that pointers past the first page are not relevant.  Caller holds    */\n/* allocation lock.                                                     */\nGC_INNER void * GC_generic_malloc_inner_ignore_off_page(size_t lb, int k)\n{\n    word lb_adjusted;\n    void * op;\n\n    if (lb <= HBLKSIZE)\n        return(GC_generic_malloc_inner(lb, k));\n    lb_adjusted = ADD_SLOP(lb);\n    op = GC_alloc_large_and_clear(lb_adjusted, k, IGNORE_OFF_PAGE);\n    GC_bytes_allocd += lb_adjusted;\n    return op;\n}\n\nGC_API void * GC_CALL GC_generic_malloc(size_t lb, int k)\n{\n    void * result;\n    DCL_LOCK_STATE;\n\n    if (EXPECT(GC_have_errors, FALSE))\n      GC_print_all_errors();\n    GC_INVOKE_FINALIZERS();\n    if (SMALL_OBJ(lb)) {\n        LOCK();\n        result = GC_generic_malloc_inner((word)lb, k);\n        UNLOCK();\n    } else {\n        size_t lg;\n        size_t lb_rounded;\n        word n_blocks;\n        GC_bool init;\n        lg = ROUNDED_UP_GRANULES(lb);\n        lb_rounded = GRANULES_TO_BYTES(lg);\n        if (lb_rounded < lb)\n            return((*GC_get_oom_fn())(lb));\n        n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);\n        init = GC_obj_kinds[k].ok_init;\n        LOCK();\n        result = (ptr_t)GC_alloc_large(lb_rounded, k, 0);\n        if (0 != result) {\n          if (GC_debugging_started) {\n            BZERO(result, n_blocks * HBLKSIZE);\n          } else {\n#           ifdef THREADS\n              /* Clear any memory that might be used for GC descriptors */\n              /* before we release the lock.                            */\n                ((word *)result)[0] = 0;\n                ((word *)result)[1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-2] = 0;\n#           endif\n          }\n        }\n        GC_bytes_allocd += lb_rounded;\n        UNLOCK();\n        if (init && !GC_debugging_started && 0 != result) {\n            BZERO(result, n_blocks * HBLKSIZE);\n        }\n    }\n    if (0 == result) {\n        return((*GC_get_oom_fn())(lb));\n    } else {\n        return(result);\n    }\n}\n\n/* Allocate lb bytes of atomic (pointerfree) data */\n#ifdef THREAD_LOCAL_ALLOC\n  GC_INNER void * GC_core_malloc_atomic(size_t lb)\n#else\n  GC_API void * GC_CALL GC_malloc_atomic(size_t lb)\n#endif\n{\n    void *op;\n    void ** opp;\n    size_t lg;\n    DCL_LOCK_STATE;\n\n    if(SMALL_OBJ(lb)) {\n        lg = GC_size_map[lb];\n        opp = &(GC_aobjfreelist[lg]);\n        LOCK();\n        if (EXPECT((op = *opp) == 0, FALSE)) {\n            UNLOCK();\n            return(GENERAL_MALLOC((word)lb, PTRFREE));\n        }\n        *opp = obj_link(op);\n        GC_bytes_allocd += GRANULES_TO_BYTES(lg);\n        UNLOCK();\n        return((void *) op);\n   } else {\n       return(GENERAL_MALLOC((word)lb, PTRFREE));\n   }\n}\n\n/* Allocate lb bytes of composite (pointerful) data */\n#ifdef THREAD_LOCAL_ALLOC\n  GC_INNER void * GC_core_malloc(size_t lb)\n#else\n  GC_API void * GC_CALL GC_malloc(size_t lb)\n#endif\n{\n    void *op;\n    void **opp;\n    size_t lg;\n    DCL_LOCK_STATE;\n\n    if(SMALL_OBJ(lb)) {\n        lg = GC_size_map[lb];\n        opp = (void **)&(GC_objfreelist[lg]);\n        LOCK();\n        if (EXPECT((op = *opp) == 0, FALSE)) {\n            UNLOCK();\n            return (GENERAL_MALLOC((word)lb, NORMAL));\n        }\n        GC_ASSERT(0 == obj_link(op)\n                  || ((word)obj_link(op)\n                        <= (word)GC_greatest_plausible_heap_addr\n                     && (word)obj_link(op)\n                        >= (word)GC_least_plausible_heap_addr));\n        *opp = obj_link(op);\n        obj_link(op) = 0;\n        GC_bytes_allocd += GRANULES_TO_BYTES(lg);\n        UNLOCK();\n        return op;\n   } else {\n       return(GENERAL_MALLOC(lb, NORMAL));\n   }\n}\n\n/* Allocate lb bytes of pointerful, traced, but not collectable data */\nGC_API void * GC_CALL GC_malloc_uncollectable(size_t lb)\n{\n    void *op;\n    void **opp;\n    size_t lg;\n    DCL_LOCK_STATE;\n\n    if( SMALL_OBJ(lb) ) {\n        if (EXTRA_BYTES != 0 && lb != 0) lb--;\n                  /* We don't need the extra byte, since this won't be  */\n                  /* collected anyway.                                  */\n        lg = GC_size_map[lb];\n        opp = &(GC_uobjfreelist[lg]);\n        LOCK();\n        op = *opp;\n        if (EXPECT(0 != op, TRUE)) {\n            *opp = obj_link(op);\n            obj_link(op) = 0;\n            GC_bytes_allocd += GRANULES_TO_BYTES(lg);\n            /* Mark bit ws already set on free list.  It will be        */\n            /* cleared only temporarily during a collection, as a       */\n            /* result of the normal free list mark bit clearing.        */\n            GC_non_gc_bytes += GRANULES_TO_BYTES(lg);\n            UNLOCK();\n        } else {\n            UNLOCK();\n            op = (ptr_t)GC_generic_malloc((word)lb, UNCOLLECTABLE);\n            /* For small objects, the free lists are completely marked. */\n        }\n        GC_ASSERT(0 == op || GC_is_marked(op));\n        return((void *) op);\n    } else {\n        hdr * hhdr;\n\n        op = (ptr_t)GC_generic_malloc((word)lb, UNCOLLECTABLE);\n        if (0 == op) return(0);\n\n        GC_ASSERT(((word)op & (HBLKSIZE - 1)) == 0); /* large block */\n        hhdr = HDR(op);\n        /* We don't need the lock here, since we have an undisguised    */\n        /* pointer.  We do need to hold the lock while we adjust        */\n        /* mark bits.                                                   */\n        LOCK();\n        set_mark_bit_from_hdr(hhdr, 0); /* Only object. */\n        GC_ASSERT(hhdr -> hb_n_marks == 0);\n        hhdr -> hb_n_marks = 1;\n        UNLOCK();\n        return((void *) op);\n    }\n}\n\n#ifdef REDIRECT_MALLOC\n\n# ifndef MSWINCE\n#  include <errno.h>\n# endif\n\n/* Avoid unnecessary nested procedure calls here, by #defining some     */\n/* malloc replacements.  Otherwise we end up saving a                   */\n/* meaningless return address in the object.  It also speeds things up, */\n/* but it is admittedly quite ugly.                                     */\n\n# define GC_debug_malloc_replacement(lb) \\\n                        GC_debug_malloc(lb, GC_DBG_RA \"unknown\", 0)\n\nvoid * malloc(size_t lb)\n{\n    /* It might help to manually inline the GC_malloc call here.        */\n    /* But any decent compiler should reduce the extra procedure call   */\n    /* to at most a jump instruction in this case.                      */\n#   if defined(I386) && defined(GC_SOLARIS_THREADS)\n      /*\n       * Thread initialisation can call malloc before\n       * we're ready for it.\n       * It's not clear that this is enough to help matters.\n       * The thread implementation may well call malloc at other\n       * inopportune times.\n       */\n      if (!EXPECT(GC_is_initialized, TRUE)) return sbrk(lb);\n#   endif /* I386 && GC_SOLARIS_THREADS */\n    return((void *)REDIRECT_MALLOC(lb));\n}\n\n#if defined(GC_LINUX_THREADS) /* && !defined(USE_PROC_FOR_LIBRARIES) */\n  STATIC ptr_t GC_libpthread_start = 0;\n  STATIC ptr_t GC_libpthread_end = 0;\n  STATIC ptr_t GC_libld_start = 0;\n  STATIC ptr_t GC_libld_end = 0;\n\n  STATIC void GC_init_lib_bounds(void)\n  {\n    if (GC_libpthread_start != 0) return;\n    GC_init(); /* if not called yet */\n    if (!GC_text_mapping(\"libpthread-\",\n                         &GC_libpthread_start, &GC_libpthread_end)) {\n        WARN(\"Failed to find libpthread.so text mapping: Expect crash\\n\", 0);\n        /* This might still work with some versions of libpthread,      */\n        /* so we don't abort.  Perhaps we should.                       */\n        /* Generate message only once:                                  */\n          GC_libpthread_start = (ptr_t)1;\n    }\n    if (!GC_text_mapping(\"ld-\", &GC_libld_start, &GC_libld_end)) {\n        WARN(\"Failed to find ld.so text mapping: Expect crash\\n\", 0);\n    }\n  }\n#endif /* GC_LINUX_THREADS */\n\n#ifndef SIZE_MAX\n#define SIZE_MAX (~(size_t)0)\n#endif\nvoid * calloc(size_t n, size_t lb)\n{\n    if (lb && n > SIZE_MAX / lb)\n      return NULL;\n#   if defined(GC_LINUX_THREADS) /* && !defined(USE_PROC_FOR_LIBRARIES) */\n        /* libpthread allocated some memory that is only pointed to by  */\n        /* mmapped thread stacks.  Make sure it's not collectable.      */\n        {\n          static GC_bool lib_bounds_set = FALSE;\n          ptr_t caller = (ptr_t)__builtin_return_address(0);\n          /* This test does not need to ensure memory visibility, since */\n          /* the bounds will be set when/if we create another thread.   */\n          if (!EXPECT(lib_bounds_set, TRUE)) {\n            GC_init_lib_bounds();\n            lib_bounds_set = TRUE;\n          }\n          if (((word)caller >= (word)GC_libpthread_start\n               && (word)caller < (word)GC_libpthread_end)\n              || ((word)caller >= (word)GC_libld_start\n                  && (word)caller < (word)GC_libld_end))\n            return GC_malloc_uncollectable(n*lb);\n          /* The two ranges are actually usually adjacent, so there may */\n          /* be a way to speed this up.                                 */\n        }\n#   endif\n    return((void *)REDIRECT_MALLOC(n*lb));\n}\n\n#ifndef strdup\n  char *strdup(const char *s)\n  {\n    size_t lb = strlen(s) + 1;\n    char *result = (char *)REDIRECT_MALLOC(lb);\n    if (result == 0) {\n      errno = ENOMEM;\n      return 0;\n    }\n    BCOPY(s, result, lb);\n    return result;\n  }\n#endif /* !defined(strdup) */\n /* If strdup is macro defined, we assume that it actually calls malloc, */\n /* and thus the right thing will happen even without overriding it.     */\n /* This seems to be true on most Linux systems.                         */\n\n#ifndef strndup\n  /* This is similar to strdup().       */\n  char *strndup(const char *str, size_t size)\n  {\n    char *copy;\n    size_t len = strlen(str);\n    if (len > size)\n      len = size;\n    copy = (char *)REDIRECT_MALLOC(len + 1);\n    if (copy == NULL) {\n      errno = ENOMEM;\n      return NULL;\n    }\n    BCOPY(str, copy, len);\n    copy[len] = '\\0';\n    return copy;\n  }\n#endif /* !strndup */\n\n#undef GC_debug_malloc_replacement\n\n#endif /* REDIRECT_MALLOC */\n\n/* Explicitly deallocate an object p.                           */\nGC_API void GC_CALL GC_free(void * p)\n{\n    struct hblk *h;\n    hdr *hhdr;\n    size_t sz; /* In bytes */\n    size_t ngranules;   /* sz in granules */\n    void **flh;\n    int knd;\n    struct obj_kind * ok;\n    DCL_LOCK_STATE;\n\n    if (p == 0) return;\n        /* Required by ANSI.  It's not my fault ...     */\n#   ifdef LOG_ALLOCS\n      GC_err_printf(\"GC_free(%p), GC: %lu\\n\", p, (unsigned long)GC_gc_no);\n#   endif\n    h = HBLKPTR(p);\n    hhdr = HDR(h);\n#   if defined(REDIRECT_MALLOC) && \\\n        (defined(GC_SOLARIS_THREADS) || defined(GC_LINUX_THREADS) \\\n         || defined(MSWIN32))\n        /* For Solaris, we have to redirect malloc calls during         */\n        /* initialization.  For the others, this seems to happen        */\n        /* implicitly.                                                  */\n        /* Don't try to deallocate that memory.                         */\n        if (0 == hhdr) return;\n#   endif\n    GC_ASSERT(GC_base(p) == p);\n    sz = hhdr -> hb_sz;\n    ngranules = BYTES_TO_GRANULES(sz);\n    knd = hhdr -> hb_obj_kind;\n    ok = &GC_obj_kinds[knd];\n    if (EXPECT(ngranules <= MAXOBJGRANULES, TRUE)) {\n        LOCK();\n        GC_bytes_freed += sz;\n        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;\n                /* Its unnecessary to clear the mark bit.  If the       */\n                /* object is reallocated, it doesn't matter.  O.w. the  */\n                /* collector will do it, since it's on a free list.     */\n        if (ok -> ok_init) {\n            BZERO((word *)p + 1, sz-sizeof(word));\n        }\n        flh = &(ok -> ok_freelist[ngranules]);\n        obj_link(p) = *flh;\n        *flh = (ptr_t)p;\n        UNLOCK();\n    } else {\n        size_t nblocks = OBJ_SZ_TO_BLOCKS(sz);\n        LOCK();\n        GC_bytes_freed += sz;\n        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;\n        if (nblocks > 1) {\n          GC_large_allocd_bytes -= nblocks * HBLKSIZE;\n        }\n        GC_freehblk(h);\n        UNLOCK();\n    }\n}\n\n/* Explicitly deallocate an object p when we already hold lock.         */\n/* Only used for internally allocated objects, so we can take some      */\n/* shortcuts.                                                           */\n#ifdef THREADS\n  GC_INNER void GC_free_inner(void * p)\n  {\n    struct hblk *h;\n    hdr *hhdr;\n    size_t sz; /* bytes */\n    size_t ngranules;  /* sz in granules */\n    void ** flh;\n    int knd;\n    struct obj_kind * ok;\n\n    h = HBLKPTR(p);\n    hhdr = HDR(h);\n    knd = hhdr -> hb_obj_kind;\n    sz = hhdr -> hb_sz;\n    ngranules = BYTES_TO_GRANULES(sz);\n    ok = &GC_obj_kinds[knd];\n    if (ngranules <= MAXOBJGRANULES) {\n        GC_bytes_freed += sz;\n        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;\n        if (ok -> ok_init) {\n            BZERO((word *)p + 1, sz-sizeof(word));\n        }\n        flh = &(ok -> ok_freelist[ngranules]);\n        obj_link(p) = *flh;\n        *flh = (ptr_t)p;\n    } else {\n        size_t nblocks = OBJ_SZ_TO_BLOCKS(sz);\n        GC_bytes_freed += sz;\n        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;\n        if (nblocks > 1) {\n          GC_large_allocd_bytes -= nblocks * HBLKSIZE;\n        }\n        GC_freehblk(h);\n    }\n  }\n#endif /* THREADS */\n\n#if defined(REDIRECT_MALLOC) && !defined(REDIRECT_FREE)\n# define REDIRECT_FREE GC_free\n#endif\n\n#ifdef REDIRECT_FREE\n  void free(void * p)\n  {\n#   if defined(GC_LINUX_THREADS) && !defined(USE_PROC_FOR_LIBRARIES)\n        {\n          /* Don't bother with initialization checks.  If nothing       */\n          /* has been initialized, the check fails, and that's safe,    */\n          /* since we haven't allocated uncollectable objects either.   */\n          ptr_t caller = (ptr_t)__builtin_return_address(0);\n          /* This test does not need to ensure memory visibility, since */\n          /* the bounds will be set when/if we create another thread.   */\n          if (((word)caller >= (word)GC_libpthread_start\n               && (word)caller < (word)GC_libpthread_end)\n              || ((word)caller >= (word)GC_libld_start\n                  && (word)caller < (word)GC_libld_end)) {\n            GC_free(p);\n            return;\n          }\n        }\n#   endif\n#   ifndef IGNORE_FREE\n      REDIRECT_FREE(p);\n#   endif\n  }\n#endif /* REDIRECT_FREE */\n"], "fixing_code": ["/*\n * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers\n * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.\n * Copyright (c) 1999-2004 Hewlett-Packard Development Company, L.P.\n *\n * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED\n * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.\n *\n * Permission is hereby granted to use or copy this program\n * for any purpose,  provided the above notices are retained on all copies.\n * Permission to modify the code and to distribute modified code is granted,\n * provided the above notices are retained, and a notice that the code was\n * modified is included with the above copyright notice.\n */\n\n#include \"private/gc_priv.h\"\n\n#include <stdio.h>\n#include <string.h>\n\n/* Allocate reclaim list for kind:      */\n/* Return TRUE on success               */\nSTATIC GC_bool GC_alloc_reclaim_list(struct obj_kind *kind)\n{\n    struct hblk ** result = (struct hblk **)\n                GC_scratch_alloc((MAXOBJGRANULES+1) * sizeof(struct hblk *));\n    if (result == 0) return(FALSE);\n    BZERO(result, (MAXOBJGRANULES+1)*sizeof(struct hblk *));\n    kind -> ok_reclaim_list = result;\n    return(TRUE);\n}\n\nGC_INNER GC_bool GC_collect_or_expand(word needed_blocks,\n                                      GC_bool ignore_off_page,\n                                      GC_bool retry); /* from alloc.c */\n\n/* Allocate a large block of size lb bytes.     */\n/* The block is not cleared.                    */\n/* Flags is 0 or IGNORE_OFF_PAGE.               */\n/* We hold the allocation lock.                 */\n/* EXTRA_BYTES were already added to lb.        */\nGC_INNER ptr_t GC_alloc_large(size_t lb, int k, unsigned flags)\n{\n    struct hblk * h;\n    word n_blocks;\n    ptr_t result;\n    GC_bool retry = FALSE;\n\n    /* Round up to a multiple of a granule. */\n      lb = (lb + GRANULE_BYTES - 1) & ~(GRANULE_BYTES - 1);\n    n_blocks = OBJ_SZ_TO_BLOCKS(lb);\n    if (!EXPECT(GC_is_initialized, TRUE)) GC_init();\n    /* Do our share of marking work */\n        if (GC_incremental && !GC_dont_gc)\n            GC_collect_a_little_inner((int)n_blocks);\n    h = GC_allochblk(lb, k, flags);\n#   ifdef USE_MUNMAP\n        if (0 == h) {\n            GC_merge_unmapped();\n            h = GC_allochblk(lb, k, flags);\n        }\n#   endif\n    while (0 == h && GC_collect_or_expand(n_blocks, flags != 0, retry)) {\n        h = GC_allochblk(lb, k, flags);\n        retry = TRUE;\n    }\n    if (h == 0) {\n        result = 0;\n    } else {\n        size_t total_bytes = n_blocks * HBLKSIZE;\n        if (n_blocks > 1) {\n            GC_large_allocd_bytes += total_bytes;\n            if (GC_large_allocd_bytes > GC_max_large_allocd_bytes)\n                GC_max_large_allocd_bytes = GC_large_allocd_bytes;\n        }\n        result = h -> hb_body;\n    }\n    return result;\n}\n\n/* Allocate a large block of size lb bytes.  Clear if appropriate.      */\n/* We hold the allocation lock.                                         */\n/* EXTRA_BYTES were already added to lb.                                */\nSTATIC ptr_t GC_alloc_large_and_clear(size_t lb, int k, unsigned flags)\n{\n    ptr_t result = GC_alloc_large(lb, k, flags);\n    word n_blocks = OBJ_SZ_TO_BLOCKS(lb);\n\n    if (0 == result) return 0;\n    if (GC_debugging_started || GC_obj_kinds[k].ok_init) {\n        /* Clear the whole block, in case of GC_realloc call. */\n        BZERO(result, n_blocks * HBLKSIZE);\n    }\n    return result;\n}\n\n/* allocate lb bytes for an object of kind k.   */\n/* Should not be used to directly to allocate   */\n/* objects such as STUBBORN objects that        */\n/* require special handling on allocation.      */\n/* First a version that assumes we already      */\n/* hold lock:                                   */\nGC_INNER void * GC_generic_malloc_inner(size_t lb, int k)\n{\n    void *op;\n\n    if(SMALL_OBJ(lb)) {\n        struct obj_kind * kind = GC_obj_kinds + k;\n        size_t lg = GC_size_map[lb];\n        void ** opp = &(kind -> ok_freelist[lg]);\n\n        op = *opp;\n        if (EXPECT(0 == op, FALSE)) {\n            if (GC_size_map[lb] == 0) {\n              if (!EXPECT(GC_is_initialized, TRUE)) GC_init();\n              if (GC_size_map[lb] == 0) GC_extend_size_map(lb);\n              return(GC_generic_malloc_inner(lb, k));\n            }\n            if (kind -> ok_reclaim_list == 0) {\n                if (!GC_alloc_reclaim_list(kind)) goto out;\n            }\n            op = GC_allocobj(lg, k);\n            if (op == 0) goto out;\n        }\n        *opp = obj_link(op);\n        obj_link(op) = 0;\n        GC_bytes_allocd += GRANULES_TO_BYTES(lg);\n    } else {\n        op = (ptr_t)GC_alloc_large_and_clear(ADD_SLOP(lb), k, 0);\n        GC_bytes_allocd += lb;\n    }\n\nout:\n    return op;\n}\n\n/* Allocate a composite object of size n bytes.  The caller guarantees  */\n/* that pointers past the first page are not relevant.  Caller holds    */\n/* allocation lock.                                                     */\nGC_INNER void * GC_generic_malloc_inner_ignore_off_page(size_t lb, int k)\n{\n    word lb_adjusted;\n    void * op;\n\n    if (lb <= HBLKSIZE)\n        return(GC_generic_malloc_inner(lb, k));\n    lb_adjusted = ADD_SLOP(lb);\n    op = GC_alloc_large_and_clear(lb_adjusted, k, IGNORE_OFF_PAGE);\n    GC_bytes_allocd += lb_adjusted;\n    return op;\n}\n\nGC_API void * GC_CALL GC_generic_malloc(size_t lb, int k)\n{\n    void * result;\n    DCL_LOCK_STATE;\n\n    if (EXPECT(GC_have_errors, FALSE))\n      GC_print_all_errors();\n    GC_INVOKE_FINALIZERS();\n    if (SMALL_OBJ(lb)) {\n        LOCK();\n        result = GC_generic_malloc_inner((word)lb, k);\n        UNLOCK();\n    } else {\n        size_t lg;\n        size_t lb_rounded;\n        word n_blocks;\n        GC_bool init;\n        lg = ROUNDED_UP_GRANULES(lb);\n        lb_rounded = GRANULES_TO_BYTES(lg);\n        if (lb_rounded < lb)\n            return((*GC_get_oom_fn())(lb));\n        n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);\n        init = GC_obj_kinds[k].ok_init;\n        LOCK();\n        result = (ptr_t)GC_alloc_large(lb_rounded, k, 0);\n        if (0 != result) {\n          if (GC_debugging_started) {\n            BZERO(result, n_blocks * HBLKSIZE);\n          } else {\n#           ifdef THREADS\n              /* Clear any memory that might be used for GC descriptors */\n              /* before we release the lock.                            */\n                ((word *)result)[0] = 0;\n                ((word *)result)[1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-1] = 0;\n                ((word *)result)[GRANULES_TO_WORDS(lg)-2] = 0;\n#           endif\n          }\n        }\n        GC_bytes_allocd += lb_rounded;\n        UNLOCK();\n        if (init && !GC_debugging_started && 0 != result) {\n            BZERO(result, n_blocks * HBLKSIZE);\n        }\n    }\n    if (0 == result) {\n        return((*GC_get_oom_fn())(lb));\n    } else {\n        return(result);\n    }\n}\n\n/* Allocate lb bytes of atomic (pointerfree) data */\n#ifdef THREAD_LOCAL_ALLOC\n  GC_INNER void * GC_core_malloc_atomic(size_t lb)\n#else\n  GC_API void * GC_CALL GC_malloc_atomic(size_t lb)\n#endif\n{\n    void *op;\n    void ** opp;\n    size_t lg;\n    DCL_LOCK_STATE;\n\n    if(SMALL_OBJ(lb)) {\n        lg = GC_size_map[lb];\n        opp = &(GC_aobjfreelist[lg]);\n        LOCK();\n        if (EXPECT((op = *opp) == 0, FALSE)) {\n            UNLOCK();\n            return(GENERAL_MALLOC((word)lb, PTRFREE));\n        }\n        *opp = obj_link(op);\n        GC_bytes_allocd += GRANULES_TO_BYTES(lg);\n        UNLOCK();\n        return((void *) op);\n   } else {\n       return(GENERAL_MALLOC((word)lb, PTRFREE));\n   }\n}\n\n/* Allocate lb bytes of composite (pointerful) data */\n#ifdef THREAD_LOCAL_ALLOC\n  GC_INNER void * GC_core_malloc(size_t lb)\n#else\n  GC_API void * GC_CALL GC_malloc(size_t lb)\n#endif\n{\n    void *op;\n    void **opp;\n    size_t lg;\n    DCL_LOCK_STATE;\n\n    if(SMALL_OBJ(lb)) {\n        lg = GC_size_map[lb];\n        opp = (void **)&(GC_objfreelist[lg]);\n        LOCK();\n        if (EXPECT((op = *opp) == 0, FALSE)) {\n            UNLOCK();\n            return (GENERAL_MALLOC((word)lb, NORMAL));\n        }\n        GC_ASSERT(0 == obj_link(op)\n                  || ((word)obj_link(op)\n                        <= (word)GC_greatest_plausible_heap_addr\n                     && (word)obj_link(op)\n                        >= (word)GC_least_plausible_heap_addr));\n        *opp = obj_link(op);\n        obj_link(op) = 0;\n        GC_bytes_allocd += GRANULES_TO_BYTES(lg);\n        UNLOCK();\n        return op;\n   } else {\n       return(GENERAL_MALLOC(lb, NORMAL));\n   }\n}\n\n/* Allocate lb bytes of pointerful, traced, but not collectable data */\nGC_API void * GC_CALL GC_malloc_uncollectable(size_t lb)\n{\n    void *op;\n    void **opp;\n    size_t lg;\n    DCL_LOCK_STATE;\n\n    if( SMALL_OBJ(lb) ) {\n        if (EXTRA_BYTES != 0 && lb != 0) lb--;\n                  /* We don't need the extra byte, since this won't be  */\n                  /* collected anyway.                                  */\n        lg = GC_size_map[lb];\n        opp = &(GC_uobjfreelist[lg]);\n        LOCK();\n        op = *opp;\n        if (EXPECT(0 != op, TRUE)) {\n            *opp = obj_link(op);\n            obj_link(op) = 0;\n            GC_bytes_allocd += GRANULES_TO_BYTES(lg);\n            /* Mark bit ws already set on free list.  It will be        */\n            /* cleared only temporarily during a collection, as a       */\n            /* result of the normal free list mark bit clearing.        */\n            GC_non_gc_bytes += GRANULES_TO_BYTES(lg);\n            UNLOCK();\n        } else {\n            UNLOCK();\n            op = (ptr_t)GC_generic_malloc((word)lb, UNCOLLECTABLE);\n            /* For small objects, the free lists are completely marked. */\n        }\n        GC_ASSERT(0 == op || GC_is_marked(op));\n        return((void *) op);\n    } else {\n        hdr * hhdr;\n\n        op = (ptr_t)GC_generic_malloc((word)lb, UNCOLLECTABLE);\n        if (0 == op) return(0);\n\n        GC_ASSERT(((word)op & (HBLKSIZE - 1)) == 0); /* large block */\n        hhdr = HDR(op);\n        /* We don't need the lock here, since we have an undisguised    */\n        /* pointer.  We do need to hold the lock while we adjust        */\n        /* mark bits.                                                   */\n        LOCK();\n        set_mark_bit_from_hdr(hhdr, 0); /* Only object. */\n        GC_ASSERT(hhdr -> hb_n_marks == 0);\n        hhdr -> hb_n_marks = 1;\n        UNLOCK();\n        return((void *) op);\n    }\n}\n\n#ifdef REDIRECT_MALLOC\n\n# ifndef MSWINCE\n#  include <errno.h>\n# endif\n\n/* Avoid unnecessary nested procedure calls here, by #defining some     */\n/* malloc replacements.  Otherwise we end up saving a                   */\n/* meaningless return address in the object.  It also speeds things up, */\n/* but it is admittedly quite ugly.                                     */\n\n# define GC_debug_malloc_replacement(lb) \\\n                        GC_debug_malloc(lb, GC_DBG_RA \"unknown\", 0)\n\nvoid * malloc(size_t lb)\n{\n    /* It might help to manually inline the GC_malloc call here.        */\n    /* But any decent compiler should reduce the extra procedure call   */\n    /* to at most a jump instruction in this case.                      */\n#   if defined(I386) && defined(GC_SOLARIS_THREADS)\n      /*\n       * Thread initialisation can call malloc before\n       * we're ready for it.\n       * It's not clear that this is enough to help matters.\n       * The thread implementation may well call malloc at other\n       * inopportune times.\n       */\n      if (!EXPECT(GC_is_initialized, TRUE)) return sbrk(lb);\n#   endif /* I386 && GC_SOLARIS_THREADS */\n    return((void *)REDIRECT_MALLOC(lb));\n}\n\n#if defined(GC_LINUX_THREADS) /* && !defined(USE_PROC_FOR_LIBRARIES) */\n  STATIC ptr_t GC_libpthread_start = 0;\n  STATIC ptr_t GC_libpthread_end = 0;\n  STATIC ptr_t GC_libld_start = 0;\n  STATIC ptr_t GC_libld_end = 0;\n\n  STATIC void GC_init_lib_bounds(void)\n  {\n    if (GC_libpthread_start != 0) return;\n    GC_init(); /* if not called yet */\n    if (!GC_text_mapping(\"libpthread-\",\n                         &GC_libpthread_start, &GC_libpthread_end)) {\n        WARN(\"Failed to find libpthread.so text mapping: Expect crash\\n\", 0);\n        /* This might still work with some versions of libpthread,      */\n        /* so we don't abort.  Perhaps we should.                       */\n        /* Generate message only once:                                  */\n          GC_libpthread_start = (ptr_t)1;\n    }\n    if (!GC_text_mapping(\"ld-\", &GC_libld_start, &GC_libld_end)) {\n        WARN(\"Failed to find ld.so text mapping: Expect crash\\n\", 0);\n    }\n  }\n#endif /* GC_LINUX_THREADS */\n\n#include <limits.h>\n#ifdef SIZE_MAX\n# define GC_SIZE_MAX SIZE_MAX\n#else\n# define GC_SIZE_MAX (~(size_t)0)\n#endif\n\nvoid * calloc(size_t n, size_t lb)\n{\n    if (lb && n > GC_SIZE_MAX / lb)\n      return NULL;\n#   if defined(GC_LINUX_THREADS) /* && !defined(USE_PROC_FOR_LIBRARIES) */\n        /* libpthread allocated some memory that is only pointed to by  */\n        /* mmapped thread stacks.  Make sure it's not collectable.      */\n        {\n          static GC_bool lib_bounds_set = FALSE;\n          ptr_t caller = (ptr_t)__builtin_return_address(0);\n          /* This test does not need to ensure memory visibility, since */\n          /* the bounds will be set when/if we create another thread.   */\n          if (!EXPECT(lib_bounds_set, TRUE)) {\n            GC_init_lib_bounds();\n            lib_bounds_set = TRUE;\n          }\n          if (((word)caller >= (word)GC_libpthread_start\n               && (word)caller < (word)GC_libpthread_end)\n              || ((word)caller >= (word)GC_libld_start\n                  && (word)caller < (word)GC_libld_end))\n            return GC_malloc_uncollectable(n*lb);\n          /* The two ranges are actually usually adjacent, so there may */\n          /* be a way to speed this up.                                 */\n        }\n#   endif\n    return((void *)REDIRECT_MALLOC(n*lb));\n}\n\n#ifndef strdup\n  char *strdup(const char *s)\n  {\n    size_t lb = strlen(s) + 1;\n    char *result = (char *)REDIRECT_MALLOC(lb);\n    if (result == 0) {\n      errno = ENOMEM;\n      return 0;\n    }\n    BCOPY(s, result, lb);\n    return result;\n  }\n#endif /* !defined(strdup) */\n /* If strdup is macro defined, we assume that it actually calls malloc, */\n /* and thus the right thing will happen even without overriding it.     */\n /* This seems to be true on most Linux systems.                         */\n\n#ifndef strndup\n  /* This is similar to strdup().       */\n  char *strndup(const char *str, size_t size)\n  {\n    char *copy;\n    size_t len = strlen(str);\n    if (len > size)\n      len = size;\n    copy = (char *)REDIRECT_MALLOC(len + 1);\n    if (copy == NULL) {\n      errno = ENOMEM;\n      return NULL;\n    }\n    BCOPY(str, copy, len);\n    copy[len] = '\\0';\n    return copy;\n  }\n#endif /* !strndup */\n\n#undef GC_debug_malloc_replacement\n\n#endif /* REDIRECT_MALLOC */\n\n/* Explicitly deallocate an object p.                           */\nGC_API void GC_CALL GC_free(void * p)\n{\n    struct hblk *h;\n    hdr *hhdr;\n    size_t sz; /* In bytes */\n    size_t ngranules;   /* sz in granules */\n    void **flh;\n    int knd;\n    struct obj_kind * ok;\n    DCL_LOCK_STATE;\n\n    if (p == 0) return;\n        /* Required by ANSI.  It's not my fault ...     */\n#   ifdef LOG_ALLOCS\n      GC_err_printf(\"GC_free(%p), GC: %lu\\n\", p, (unsigned long)GC_gc_no);\n#   endif\n    h = HBLKPTR(p);\n    hhdr = HDR(h);\n#   if defined(REDIRECT_MALLOC) && \\\n        (defined(GC_SOLARIS_THREADS) || defined(GC_LINUX_THREADS) \\\n         || defined(MSWIN32))\n        /* For Solaris, we have to redirect malloc calls during         */\n        /* initialization.  For the others, this seems to happen        */\n        /* implicitly.                                                  */\n        /* Don't try to deallocate that memory.                         */\n        if (0 == hhdr) return;\n#   endif\n    GC_ASSERT(GC_base(p) == p);\n    sz = hhdr -> hb_sz;\n    ngranules = BYTES_TO_GRANULES(sz);\n    knd = hhdr -> hb_obj_kind;\n    ok = &GC_obj_kinds[knd];\n    if (EXPECT(ngranules <= MAXOBJGRANULES, TRUE)) {\n        LOCK();\n        GC_bytes_freed += sz;\n        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;\n                /* Its unnecessary to clear the mark bit.  If the       */\n                /* object is reallocated, it doesn't matter.  O.w. the  */\n                /* collector will do it, since it's on a free list.     */\n        if (ok -> ok_init) {\n            BZERO((word *)p + 1, sz-sizeof(word));\n        }\n        flh = &(ok -> ok_freelist[ngranules]);\n        obj_link(p) = *flh;\n        *flh = (ptr_t)p;\n        UNLOCK();\n    } else {\n        size_t nblocks = OBJ_SZ_TO_BLOCKS(sz);\n        LOCK();\n        GC_bytes_freed += sz;\n        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;\n        if (nblocks > 1) {\n          GC_large_allocd_bytes -= nblocks * HBLKSIZE;\n        }\n        GC_freehblk(h);\n        UNLOCK();\n    }\n}\n\n/* Explicitly deallocate an object p when we already hold lock.         */\n/* Only used for internally allocated objects, so we can take some      */\n/* shortcuts.                                                           */\n#ifdef THREADS\n  GC_INNER void GC_free_inner(void * p)\n  {\n    struct hblk *h;\n    hdr *hhdr;\n    size_t sz; /* bytes */\n    size_t ngranules;  /* sz in granules */\n    void ** flh;\n    int knd;\n    struct obj_kind * ok;\n\n    h = HBLKPTR(p);\n    hhdr = HDR(h);\n    knd = hhdr -> hb_obj_kind;\n    sz = hhdr -> hb_sz;\n    ngranules = BYTES_TO_GRANULES(sz);\n    ok = &GC_obj_kinds[knd];\n    if (ngranules <= MAXOBJGRANULES) {\n        GC_bytes_freed += sz;\n        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;\n        if (ok -> ok_init) {\n            BZERO((word *)p + 1, sz-sizeof(word));\n        }\n        flh = &(ok -> ok_freelist[ngranules]);\n        obj_link(p) = *flh;\n        *flh = (ptr_t)p;\n    } else {\n        size_t nblocks = OBJ_SZ_TO_BLOCKS(sz);\n        GC_bytes_freed += sz;\n        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;\n        if (nblocks > 1) {\n          GC_large_allocd_bytes -= nblocks * HBLKSIZE;\n        }\n        GC_freehblk(h);\n    }\n  }\n#endif /* THREADS */\n\n#if defined(REDIRECT_MALLOC) && !defined(REDIRECT_FREE)\n# define REDIRECT_FREE GC_free\n#endif\n\n#ifdef REDIRECT_FREE\n  void free(void * p)\n  {\n#   if defined(GC_LINUX_THREADS) && !defined(USE_PROC_FOR_LIBRARIES)\n        {\n          /* Don't bother with initialization checks.  If nothing       */\n          /* has been initialized, the check fails, and that's safe,    */\n          /* since we haven't allocated uncollectable objects either.   */\n          ptr_t caller = (ptr_t)__builtin_return_address(0);\n          /* This test does not need to ensure memory visibility, since */\n          /* the bounds will be set when/if we create another thread.   */\n          if (((word)caller >= (word)GC_libpthread_start\n               && (word)caller < (word)GC_libpthread_end)\n              || ((word)caller >= (word)GC_libld_start\n                  && (word)caller < (word)GC_libld_end)) {\n            GC_free(p);\n            return;\n          }\n        }\n#   endif\n#   ifndef IGNORE_FREE\n      REDIRECT_FREE(p);\n#   endif\n  }\n#endif /* REDIRECT_FREE */\n"], "filenames": ["malloc.c"], "buggy_code_start_loc": [377], "buggy_code_end_loc": [383], "fixing_code_start_loc": [377], "fixing_code_end_loc": [387], "type": "CWE-189", "message": "Multiple integer overflows in the (1) GC_generic_malloc and (2) calloc functions in malloc.c, and the (3) GC_generic_malloc_ignore_off_page function in mallocx.c in Boehm-Demers-Weiser GC (libgc) before 7.2 make it easier for context-dependent attackers to perform memory-related attacks such as buffer overflows via a large size value, which causes less memory to be allocated than expected.", "other": {"cve": {"id": "CVE-2012-2673", "sourceIdentifier": "secalert@redhat.com", "published": "2012-07-25T19:55:02.773", "lastModified": "2016-09-29T01:59:06.160", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Multiple integer overflows in the (1) GC_generic_malloc and (2) calloc functions in malloc.c, and the (3) GC_generic_malloc_ignore_off_page function in mallocx.c in Boehm-Demers-Weiser GC (libgc) before 7.2 make it easier for context-dependent attackers to perform memory-related attacks such as buffer overflows via a large size value, which causes less memory to be allocated than expected."}, {"lang": "es", "value": "M\u00faltiples desbordamientos de entero en las funciones (1) GC_generic_malloc y (2) calloc en malloc.c y en la funci\u00f3n (3) GC_generic_malloc_ignore_off_page en mallocx.c en Boehm-Demers-Weiser GC (libgc) en versiones anteriores a 7.2 hace que sea m\u00e1s f\u00e1cil para atacantes dependientes de contexto realizar ataques relacionados con la memoria tales como desbordamientos de b\u00fafer a trav\u00e9s un valor de gran tama\u00f1o, lo que provoca que se asigne menos memoria de los esperado."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:P/A:N", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-189"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:*:alpha6:*:*:*:*:*:*", "versionEndIncluding": "7.2", "matchCriteriaId": "FB7E10E2-C5DB-41EC-97EB-CE8536C43C1A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:1.3:*:*:*:*:*:*:*", "matchCriteriaId": "491073DC-2BFD-4B0C-BDF2-9F5280A66B58"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:1.4:*:*:*:*:*:*:*", "matchCriteriaId": "25A7A8FB-CF57-4E84-B7E6-163AC7CB018B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:1.5:*:*:*:*:*:*:*", "matchCriteriaId": "083750EB-C841-412C-A006-4DD509479289"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:1.8:*:*:*:*:*:*:*", "matchCriteriaId": "031EAB4C-6E1A-4582-8FB8-EAED0AFFA8A8"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:1.9:*:*:*:*:*:*:*", "matchCriteriaId": "4B979DD2-47F2-4F4A-BCAB-3B205142FABA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:2.0:*:*:*:*:*:*:*", "matchCriteriaId": "4717403B-917C-4114-98B7-ECE20DCCC5AC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:2.1:*:*:*:*:*:*:*", "matchCriteriaId": "4E207890-449C-46B9-93E7-A63FF8684A04"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:2.2:*:*:*:*:*:*:*", "matchCriteriaId": "F6BA97C7-7860-4BB3-B066-A42F0FFCD89C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:2.3:*:*:*:*:*:*:*", "matchCriteriaId": "F34B767E-85C3-4647-B945-3D68A47A32C5"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:2.4:*:*:*:*:*:*:*", "matchCriteriaId": "3C1B8A06-29C8-4967-8907-C0049AA0958F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:3.0:*:*:*:*:*:*:*", "matchCriteriaId": "299A66C9-C24B-42E6-A66F-D4CD81DDB7F6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "38B1EF53-DEEF-492B-8ED7-C3317DA8352D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "0D54EC0E-A58E-46E5-8B1D-C05AED9275C7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "46675810-D684-47E2-8110-3E0E676DFD2F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "63943010-1074-46CF-BC8B-2F6D3A65F6E6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:3.5:*:*:*:*:*:*:*", "matchCriteriaId": "15EF17F8-7C98-4196-B982-A232EC8E03B1"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:3.6:*:*:*:*:*:*:*", "matchCriteriaId": "FD38128B-2CED-4E4D-991F-B304E70D28E6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:3.7:*:*:*:*:*:*:*", "matchCriteriaId": "1EA5BC4D-FEE0-410B-A6B0-620404012D67"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.0:*:*:*:*:*:*:*", "matchCriteriaId": "F9EB7E2B-DBBC-42D2-ADFB-780C4719AB6A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.1:*:*:*:*:*:*:*", "matchCriteriaId": "B7E6ACC0-B90C-45C6-A92F-B253CF9E6B1D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.2:*:*:*:*:*:*:*", "matchCriteriaId": "D43D4FE2-8C2F-4295-9201-1C1BA29149AC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.3:*:*:*:*:*:*:*", "matchCriteriaId": "A821A89F-BC65-4902-A654-4653D7F5EB12"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.4:*:*:*:*:*:*:*", "matchCriteriaId": "E26120DA-A675-4D40-986B-80816C354C7A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.5:*:*:*:*:*:*:*", "matchCriteriaId": "77311E02-BA9E-4EE2-B4C8-93924F21B42E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.6:*:*:*:*:*:*:*", "matchCriteriaId": "3903A88C-7D08-4C1A-A9F6-306D3431EED3"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.7:*:*:*:*:*:*:*", "matchCriteriaId": "7E191DDB-847C-4B23-A2E7-92C7FDFAB320"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.8:*:*:*:*:*:*:*", "matchCriteriaId": "016D4E46-79E6-4A10-A8D5-5A0F8ACC031A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.9:*:*:*:*:*:*:*", "matchCriteriaId": "D833B1A6-CDD6-4066-817B-401DFC29CB74"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.10:*:*:*:*:*:*:*", "matchCriteriaId": "7473C96A-B8C4-4A31-BC3C-11BBD44D43CF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.11:*:*:*:*:*:*:*", "matchCriteriaId": "FE91D870-9D11-49D3-BD79-B89D492378E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.12:*:*:*:*:*:*:*", "matchCriteriaId": "345F5BE8-34BB-4670-87C8-E47DCC7D4376"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.13:*:*:*:*:*:*:*", "matchCriteriaId": "E8BFA222-5FC6-4D6A-BEB6-71B1B6C74BB3"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.14:*:*:*:*:*:*:*", "matchCriteriaId": "75A47F20-004B-4F06-A6F3-E48056FB9596"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.14:alpha1:*:*:*:*:*:*", "matchCriteriaId": "AE42C4C6-B198-4E5B-A56C-B76EC37A3B40"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:4.14:alpha2:*:*:*:*:*:*", "matchCriteriaId": "D8D3047B-FD43-4574-8D60-2085D63B5C29"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.0:*:*:*:*:*:*:*", "matchCriteriaId": "6DEE2A39-AD46-485C-99E7-4CF750CC5D1A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.0:alpha1:*:*:*:*:*:*", "matchCriteriaId": "72949DF3-D5D7-437D-8494-C16BECC8A0DE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.0:alpha2:*:*:*:*:*:*", "matchCriteriaId": "1778B6A6-5AF7-420C-B8A0-D5EEA11FF08A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.0:alpha3:*:*:*:*:*:*", "matchCriteriaId": "2D608CBB-DE39-4F89-A6D5-3DD0743E3F3E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.0:alpha4:*:*:*:*:*:*", "matchCriteriaId": "12DF9207-CC7F-42DB-B8F3-DB218117DCF7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.0:alpha6:*:*:*:*:*:*", "matchCriteriaId": "DA18A03D-8237-4BBE-9C10-47EA5FCE8BDA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.0:alpha7:*:*:*:*:*:*", "matchCriteriaId": "9B9A6D84-4809-49A0-86C6-ECBDD1064928"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.1:*:*:*:*:*:*:*", "matchCriteriaId": "6C820C5B-8EA8-4F60-9841-FA5A352532D8"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.2:*:*:*:*:*:*:*", "matchCriteriaId": "342F1990-B47E-4F18-A8EF-0D3A4DA12750"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.3:*:*:*:*:*:*:*", "matchCriteriaId": "40752649-CC20-4A48-9445-9A6834D3E607"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:5.4:*:*:*:*:*:*:*", "matchCriteriaId": "FF823758-7576-42F1-86CB-299D81814C60"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:*:*:*:*:*:*:*", "matchCriteriaId": "62D73130-3452-4099-81D2-35CE27604471"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:alpha1:*:*:*:*:*:*", "matchCriteriaId": "DD35B5F7-CE6B-4611-8C93-6E984BBE9F60"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:alpha2:*:*:*:*:*:*", "matchCriteriaId": "5D47BE29-3D8C-491C-8CE1-11936D290751"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:alpha3:*:*:*:*:*:*", "matchCriteriaId": "8FE9F396-BA3A-4FEA-99CD-82101B2F4D17"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:alpha4:*:*:*:*:*:*", "matchCriteriaId": "0F35B814-E6D9-4339-BBF0-71C8BCB56AC3"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:alpha5:*:*:*:*:*:*", "matchCriteriaId": "09914F81-E73F-40B7-B089-1601CDA2F08A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:alpha6:*:*:*:*:*:*", "matchCriteriaId": "AE9D4C8A-7109-42FB-A3A4-EA275263B51A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:alpha7:*:*:*:*:*:*", "matchCriteriaId": "BAEC8D07-03F3-47BD-BFAF-C68DED57E9E4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:alpha8:*:*:*:*:*:*", "matchCriteriaId": "97F7A53D-9E3B-4FC8-B5D2-8F6098D42154"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.0:alpha9:*:*:*:*:*:*", "matchCriteriaId": "15B91852-E698-44B8-AA4B-3D160F6922EB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.1:*:*:*:*:*:*:*", "matchCriteriaId": "A6C5E554-829A-4DD2-AA08-F1F4086970FF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.1:alpha1:*:*:*:*:*:*", "matchCriteriaId": "EAB8D4F7-FF1F-4A9D-8256-7862DBCC91AE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.1:alpha2:*:*:*:*:*:*", "matchCriteriaId": "1AFF33C7-7DE7-4252-B1F5-1923C97488DB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.1:alpha3:*:*:*:*:*:*", "matchCriteriaId": "85E1EFDA-4A24-4021-A964-BF541692A662"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.1:alpha4:*:*:*:*:*:*", "matchCriteriaId": "EA13626B-D41F-4FD6-99B6-894A550243A1"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.1:alpha5:*:*:*:*:*:*", "matchCriteriaId": "76017823-6C26-4117-BE01-C08326140BF1"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.2:*:*:*:*:*:*:*", "matchCriteriaId": "76CBFFA9-3C32-4912-A80E-092C0CFFFB35"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.2:alpha1:*:*:*:*:*:*", "matchCriteriaId": "3DD6DEDC-065F-4A1D-971B-F6648CCBC524"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.2:alpha2:*:*:*:*:*:*", "matchCriteriaId": "43273650-CE0B-44C1-A666-2EBC7F25699C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.2:alpha3:*:*:*:*:*:*", "matchCriteriaId": "BEB5C494-929A-4A22-8DB8-66D92671CB3B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.2:alpha4:*:*:*:*:*:*", "matchCriteriaId": "8C80F2C3-4F26-4F20-A4FA-DD543F300E4C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.2:alpha5:*:*:*:*:*:*", "matchCriteriaId": "D9854F73-471D-458A-8C96-4F20AC0CABF0"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.2:alpha6:*:*:*:*:*:*", "matchCriteriaId": "17EB51A7-125F-4738-9F40-C43FF24F33CB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.3:*:*:*:*:*:*:*", "matchCriteriaId": "ADBA25B4-1E69-4E4E-8FD3-AC7FF9B75560"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.3:alpha1:*:*:*:*:*:*", "matchCriteriaId": "D3593B9D-B7F7-4AE5-A56F-E1D619EC62C7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.3:alpha2:*:*:*:*:*:*", "matchCriteriaId": "8D94D856-4C71-4BF2-9B99-4CF4B2A3252C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.3:alpha3:*:*:*:*:*:*", "matchCriteriaId": "CD69A3EF-8F68-48AA-A3AB-1A6A5AAF583B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.3:alpha4:*:*:*:*:*:*", "matchCriteriaId": "DB138D34-6BE9-4C89-A8DC-E7166A3C9CDC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.3:alpha5:*:*:*:*:*:*", "matchCriteriaId": "3263B0DE-0494-4E23-B2B1-07D6F7859667"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.3:alpha6:*:*:*:*:*:*", "matchCriteriaId": "B09D5237-C46B-46BF-8261-F06F90A5B363"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.4:*:*:*:*:*:*:*", "matchCriteriaId": "C72EF628-6740-4DF3-9415-DDF1E27439E0"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.5:*:*:*:*:*:*:*", "matchCriteriaId": "95E28717-719E-42C7-95C2-A1640EE25683"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.6:*:*:*:*:*:*:*", "matchCriteriaId": "C83B3862-DA90-4F32-BE45-E973806311CC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.7:*:*:*:*:*:*:*", "matchCriteriaId": "91268295-6262-43BF-BC1C-25CCC4B9E17C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.8:*:*:*:*:*:*:*", "matchCriteriaId": "84EBF00E-BF9F-4E77-9B57-BC88DB70EC0B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:6.9:*:*:*:*:*:*:*", "matchCriteriaId": "5B0666E4-9E33-48C1-97D8-22B5E3A5F96E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "96F70C6D-27B8-44B6-BA07-32B634617475"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.0:alpha1:*:*:*:*:*:*", "matchCriteriaId": "C3133D17-B254-4B52-8C6C-A19FB95679FF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.0:alpha2:*:*:*:*:*:*", "matchCriteriaId": "8E52765E-1FF3-4355-993C-B4AECC7CA3C8"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.0:alpha3:*:*:*:*:*:*", "matchCriteriaId": "C2628958-C5B0-4FC9-BB2C-3ECC0FE3C2A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.0:alpha4:*:*:*:*:*:*", "matchCriteriaId": "AC25F20D-714D-49A1-9F31-E2B08FC34FB7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.0:alpha5:*:*:*:*:*:*", "matchCriteriaId": "555CF08F-350D-4B2E-B24A-2C959A0B5F2D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.0:alpha7:*:*:*:*:*:*", "matchCriteriaId": "19A58A73-DB04-46CB-BA6A-E7C9EF1131D7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.0:alpha9:*:*:*:*:*:*", "matchCriteriaId": "C3167413-D7B5-431F-930B-318F177A1439"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.1:*:*:*:*:*:*:*", "matchCriteriaId": "E8A63539-9978-467A-9A51-5DA2AED49CDA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.1:alpha2:*:*:*:*:*:*", "matchCriteriaId": "7EF6F6B4-991E-4B96-8562-D9510BDF27DB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.2:alpha2:*:*:*:*:*:*", "matchCriteriaId": "4301B5B7-9DC1-4C77-B8E5-5856D9630068"}, {"vulnerable": true, "criteria": "cpe:2.3:a:boehm-demers-weiser:garbage_collector:7.2:alpha4:*:*:*:*:*:*", "matchCriteriaId": "A19FE5A5-2037-44C0-9EB4-C6297F8D5E02"}]}]}], "references": [{"url": "http://kqueue.org/blog/2012/03/05/memory-allocator-security-revisited/", "source": "secalert@redhat.com"}, {"url": "http://lists.fedoraproject.org/pipermail/package-announce/2012-June/082926.html", "source": "secalert@redhat.com"}, {"url": "http://lists.fedoraproject.org/pipermail/package-announce/2012-June/082988.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-1500.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-0149.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-0150.html", "source": "secalert@redhat.com"}, {"url": "http://www.mandriva.com/security/advisories?name=MDVSA-2012:158", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2012/06/05/1", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2012/06/07/13", "source": "secalert@redhat.com"}, {"url": "http://www.securityfocus.com/bid/54227", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1546-1", "source": "secalert@redhat.com"}, {"url": "https://github.com/ivmai/bdwgc/blob/master/ChangeLog", "source": "secalert@redhat.com"}, {"url": "https://github.com/ivmai/bdwgc/commit/6a93f8e5bcad22137f41b6c60a1c7384baaec2b3", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}, {"url": "https://github.com/ivmai/bdwgc/commit/83231d0ab5ed60015797c3d1ad9056295ac3b2bb", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}, {"url": "https://github.com/ivmai/bdwgc/commit/be9df82919960214ee4b9d3313523bff44fd99e1", "source": "secalert@redhat.com", "tags": ["Patch"]}, {"url": "https://github.com/ivmai/bdwgc/commit/e10c1eb9908c2774c16b3148b30d2f3823d66a9a", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/ivmai/bdwgc/commit/6a93f8e5bcad22137f41b6c60a1c7384baaec2b3"}}