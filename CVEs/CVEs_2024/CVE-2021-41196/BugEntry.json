{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/pooling_ops_3d.h\"\n\n#include <array>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_slice.h\"\n#include \"tensorflow/core/kernels/eigen_pooling.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n#include \"tensorflow/core/util/work_sharder.h\"\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/kernels/cudnn_pooling_gpu.h\"\n#include \"tensorflow/core/kernels/pooling_ops_3d_gpu.h\"\n#endif\n\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nPool3dParameters::Pool3dParameters(OpKernelContext* context,\n                                   const std::vector<int32>& ksize,\n                                   const std::vector<int32>& stride,\n                                   Padding padding, TensorFormat data_format,\n                                   const TensorShape& tensor_in_shape) {\n  // For maxpooling, tensor_in should have 4 dimensions.\n  OP_REQUIRES(context, tensor_in_shape.dims() == 5,\n              errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n  this->data_format = data_format;\n  depth = GetTensorDim(tensor_in_shape, data_format, 'C');\n  tensor_in_planes = GetTensorDim(tensor_in_shape, data_format, '0');\n  tensor_in_rows = GetTensorDim(tensor_in_shape, data_format, '1');\n  tensor_in_cols = GetTensorDim(tensor_in_shape, data_format, '2');\n  tensor_in_batch = GetTensorDim(tensor_in_shape, data_format, 'N');\n  window_planes = GetTensorDim(ksize, data_format, '0');\n  window_rows = GetTensorDim(ksize, data_format, '1');\n  window_cols = GetTensorDim(ksize, data_format, '2');\n  depth_window = GetTensorDim(ksize, data_format, 'C');\n  plane_stride = GetTensorDim(stride, data_format, '0');\n  row_stride = GetTensorDim(stride, data_format, '1');\n  col_stride = GetTensorDim(stride, data_format, '2');\n  depth_stride = GetTensorDim(stride, data_format, 'C');\n\n  // We only support 3D pooling across plane/width/height. Depthwise\n  // pooling is not supported.\n  OP_REQUIRES(\n      context, depth_window == 1 && depth_stride == 1,\n      errors::Unimplemented(\n          \"Pooling3d only supports pooling across plane/width/height.\"));\n\n  OP_REQUIRES_OK(context, GetWindowedOutputSize(tensor_in_planes, window_planes,\n                                                plane_stride, padding,\n                                                &out_plane, &pad_planes));\n  OP_REQUIRES_OK(context,\n                 GetWindowedOutputSize(tensor_in_rows, window_rows, row_stride,\n                                       padding, &out_height, &pad_rows));\n  OP_REQUIRES_OK(context,\n                 GetWindowedOutputSize(tensor_in_cols, window_cols, col_stride,\n                                       padding, &out_width, &pad_cols));\n}\n\nTensorShape Pool3dParameters::forward_output_shape() {\n  return ShapeFromFormat(data_format, tensor_in_batch,\n                         {{out_plane, out_height, out_width}}, depth);\n}\n\ntemplate <typename T>\nstruct LaunchPoolingOp<CPUDevice, T, AVG> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =\n        Eigen::CuboidAvgPooling(tensor_in.tensor<T, 5>(), window[0], window[1],\n                                window[2], stride[0], stride[1], stride[2],\n                                BrainPadding2EigenPadding(padding_type));\n  }\n};\n\ntemplate <typename T>\nstruct LaunchPoolingOp<CPUDevice, T, MAX> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =\n        Eigen::CuboidMaxPooling(tensor_in.tensor<T, 5>(), window[0], window[1],\n                                window[2], stride[0], stride[1], stride[2],\n                                BrainPadding2EigenPadding(padding_type));\n  }\n};\n\ntemplate <typename Device, typename T, PoolingType Type>\nclass Pooling3DOp : public UnaryOp<T> {\n public:\n  explicit Pooling3DOp(OpKernelConstruction* context) : UnaryOp<T>(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\"Default Pooling3DOp only supports NDHWC \",\n                                  \"on device type \",\n                                  DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    const int64_t depth = GetTensorDim(tensor_in, data_format_, 'C');\n    const int64_t in_batch = GetTensorDim(tensor_in, data_format_, 'N');\n\n    // Dimension order for these arrays is: x, y, z.\n    std::array<int64_t, 3> input_size{\n        {GetTensorDim(tensor_in, data_format_, '2'),\n         GetTensorDim(tensor_in, data_format_, '1'),\n         GetTensorDim(tensor_in, data_format_, '0')}};\n    std::array<int64_t, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                   GetTensorDim(ksize_, data_format_, '1'),\n                                   GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64_t, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                   GetTensorDim(stride_, data_format_, '1'),\n                                   GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64_t, 3> padding, out;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n\n    TensorShape out_shape = ShapeFromFormat(data_format_, in_batch,\n                                            {{out[2], out[1], out[0]}}, depth);\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n    if (out_shape.num_elements() == 0) return;\n    LaunchPoolingOp<Device, T, Type>::launch(context, tensor_in, window, stride,\n                                             padding, data_format_, padding_,\n                                             output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const Tensor& tensor_out, const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    output->flat<T>().setZero();\n    for (int64_t p = 0; p < out_backprop.dim_size(3); ++p) {\n      // Calculate broadcast size for planes/rows/cols. For SAME padding,\n      // current index could be in the padding area, and\n      //   p * stride_planes + window_planes\n      // could be beyond the input tensor's boundary. In such cases, change\n      // the starting index and reduce the broadcast size.\n      //\n      // The same procedure is repeated for every spatial dimension in the\n      // nested loops below.\n      int pindex, psize;\n      std::array<int64_t, 3> input_size{{tensor_in.dim_size(3),\n                                         tensor_in.dim_size(2),\n                                         tensor_in.dim_size(1)}};\n      OP_REQUIRES_OK(context,\n                     GetBroadcastSize(p, input_size[0], window[0], stride[0],\n                                      padding[0], &pindex, &psize));\n      for (int64_t r = 0; r < out_backprop.dim_size(2); ++r) {\n        int rindex, rsize;\n        OP_REQUIRES_OK(context,\n                       GetBroadcastSize(r, input_size[1], window[1], stride[1],\n                                        padding[1], &rindex, &rsize));\n        for (int64_t c = 0; c < out_backprop.dim_size(1); ++c) {\n          int cindex, csize;\n          OP_REQUIRES_OK(\n              context, GetBroadcastSize(c, input_size[2], window[2], stride[2],\n                                        padding[2], &cindex, &csize));\n          TensorSlice src{{0, -1}, {c, 1}, {r, 1}, {p, 1}, {0, -1}};\n          TensorSlice dst{{0, -1},\n                          {cindex, csize},\n                          {rindex, rsize},\n                          {pindex, psize},\n                          {0, -1}};\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_sizes;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_sizes;\n          src.FillIndicesAndSizes<5>(out_backprop.shape(), &src_indices,\n                                     &src_sizes);\n          dst.FillIndicesAndSizes<5>(tensor_in.shape(), &dst_indices,\n                                     &dst_sizes);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n          Eigen::array<int, 5> bcast = {1, csize, rsize, psize, 1};\n#else\n          Eigen::IndexList<Eigen::type2index<1>, int, int, int,\n                           Eigen::type2index<1>>\n              bcast;\n          bcast.set(1, csize);\n          bcast.set(2, rsize);\n          bcast.set(3, psize);\n#endif\n\n          // Slice from tensor_in.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> tensor_in_slice(dst_sizes);\n          tensor_in_slice.device(context->eigen_cpu_device()) =\n              tensor_in.tensor<T, 5>().slice(dst_indices, dst_sizes);\n\n          // Slice from tensor_out.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> tensor_out_slice(src_sizes);\n          tensor_out_slice.device(context->eigen_cpu_device()) =\n              tensor_out.tensor<T, 5>().slice(src_indices, src_sizes);\n\n          // Backprop slice.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> out_backprop_slice(src_sizes);\n          out_backprop_slice.device(context->eigen_cpu_device()) =\n              out_backprop.tensor<T, 5>().slice(src_indices, src_sizes);\n\n          // The true backprop slice: if an element is the max, choose\n          // the backprop slice; otherwise set to 0.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> select_slice(dst_sizes);\n          Eigen::Tensor<T, 5, Eigen::RowMajor> mat0(dst_sizes);\n          mat0.setZero();\n          select_slice =\n              ((tensor_in_slice - tensor_out_slice.broadcast(bcast)).abs() <\n               tensor_in_slice.constant(1e-5))\n                  .select(out_backprop_slice.broadcast(bcast), mat0);\n\n          output->tensor<T, 5>()\n              .slice(dst_indices, dst_sizes)\n              .device(context->eigen_cpu_device()) += select_slice;\n        }\n      }\n    }\n  }\n};\n\ntemplate <class Device, class T>\nclass MaxPooling3dGradOp : public OpKernel {\n public:\n  explicit MaxPooling3dGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\n              \"Default MaxPooling3dGradOp only supports NDHWC \",\n              \"on device type \", DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    OP_REQUIRES(context, out_backprop.dims() == 5,\n                errors::InvalidArgument(\"out_backprop must be 5-dimensional\"));\n\n    const TensorShape& output_shape = tensor_in.shape();\n    Tensor* input_backprop;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, output_shape, &input_backprop));\n    std::array<int64_t, 3> input_size{\n        {GetTensorDim(output_shape, data_format_, '2'),\n         GetTensorDim(output_shape, data_format_, '1'),\n         GetTensorDim(output_shape, data_format_, '0')}};\n    std::array<int64_t, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                   GetTensorDim(ksize_, data_format_, '1'),\n                                   GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64_t, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                   GetTensorDim(stride_, data_format_, '1'),\n                                   GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64_t, 3> out, padding;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n    LaunchMaxPooling3dGradOp<Device, T>::launch(\n        context, tensor_in, tensor_out, out_backprop, window, stride, out,\n        padding, data_format_, input_backprop);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchAvgPooling3dGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context,\n                     const TensorShape& tensor_in_shape,\n                     const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& output_shape,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    OP_REQUIRES(\n        context, tensor_in_shape.dim_size(0) == out_backprop.dim_size(0),\n        errors::InvalidArgument(\n            \"Expected first dimension of tensor_in_shape and \"\n            \"out_backprop to match, got \",\n            tensor_in_shape.dim_size(0), \" and \", out_backprop.dim_size(0)));\n    OP_REQUIRES(\n        context, tensor_in_shape.dim_size(4) == out_backprop.dim_size(4),\n        errors::InvalidArgument(\n            \"Expected last dimension of tensor_in_shape and \"\n            \"out_backprop to match, got \",\n            tensor_in_shape.dim_size(4), \" and \", out_backprop.dim_size(4)));\n\n    output->flat<T>().setZero();\n    std::array<int64_t, 3> input_size = {{tensor_in_shape.dim_size(3),\n                                          tensor_in_shape.dim_size(2),\n                                          tensor_in_shape.dim_size(1)}};\n    for (int64_t p = 0; p < out_backprop.dim_size(3); ++p) {\n      // Calculate broadcast size for planes/rows/cols. For SAME padding,\n      // current index could be in the padding area, and\n      //   p * stride_planes + window_planes\n      // could be beyond the input tensor's boundary. In such cases, change\n      // the starting index and reduce the broadcast size.\n      //\n      // The same procedure is repeated for every spatial dimension in the\n      // nested loops below.\n      int pindex, psize;\n      OP_REQUIRES_OK(context,\n                     GetBroadcastSize(p, input_size[0], window[0], stride[0],\n                                      padding[0], &pindex, &psize));\n      for (int64_t r = 0; r < out_backprop.dim_size(2); ++r) {\n        int rindex, rsize;\n        OP_REQUIRES_OK(context,\n                       GetBroadcastSize(r, input_size[1], window[1], stride[1],\n                                        padding[1], &rindex, &rsize));\n        for (int64_t c = 0; c < out_backprop.dim_size(1); ++c) {\n          int cindex, csize;\n          OP_REQUIRES_OK(\n              context, GetBroadcastSize(c, input_size[2], window[2], stride[2],\n                                        padding[2], &cindex, &csize));\n          TensorSlice src{{0, -1}, {c, 1}, {r, 1}, {p, 1}, {0, -1}};\n          TensorSlice dst{{0, -1},\n                          {cindex, csize},\n                          {rindex, rsize},\n                          {pindex, psize},\n                          {0, -1}};\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_sizes;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_sizes;\n          src.FillIndicesAndSizes<5>(out_backprop.shape(), &src_indices,\n                                     &src_sizes);\n          dst.FillIndicesAndSizes<5>(tensor_in_shape, &dst_indices, &dst_sizes);\n#if !defined(EIGEN_HAS_INDEX_LIST)\n          Eigen::array<int, 5> bcast = {1, csize, rsize, psize, 1};\n#else\n          Eigen::IndexList<Eigen::type2index<1>, int, int, int,\n                           Eigen::type2index<1>>\n              bcast;\n          bcast.set(1, csize);\n          bcast.set(2, rsize);\n          bcast.set(3, psize);\n#endif\n          Eigen::Tensor<T, 5, Eigen::RowMajor> slices(src_sizes);\n          slices.device(context->eigen_cpu_device()) =\n              out_backprop.tensor<T, 5>().slice(src_indices, src_sizes);\n          // Divide by the size of the actual patch (psize * rsize * csize).\n          float divide_size = rsize * csize * psize * 1.0f;\n          slices *= slices.constant(1.0f / divide_size);\n\n          output->tensor<T, 5>()\n              .slice(dst_indices, dst_sizes)\n              .device(context->eigen_cpu_device()) += slices.broadcast(bcast);\n        }\n      }\n    }\n  }\n};\n\ntemplate <class Device, class T>\nclass AvgPooling3dGradOp : public OpKernel {\n public:\n  explicit AvgPooling3dGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\n              \"Default AvgPooling3dGradOp only supports NDHWC \",\n              \"on device type \", DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in_shape = context->input(0);\n    const Tensor& out_backprop = context->input(1);\n    OP_REQUIRES(\n        context,\n        tensor_in_shape.dims() == 1 && tensor_in_shape.NumElements() == 5,\n        errors::InvalidArgument(\"tensor_in must be 1-dimensional and 5 \"\n                                \"elements\"));\n    OP_REQUIRES(context, out_backprop.dims() == 5,\n                errors::InvalidArgument(\"out_backprop must be 5-dimensional\"));\n\n    TensorShape output_shape;\n    auto shape_vec = tensor_in_shape.vec<int32>();\n    for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\n      output_shape.AddDim(shape_vec(i));\n    }\n\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n\n    // Dimension order for these arrays is x, y, z.\n    std::array<int64_t, 3> input_size{\n        {GetTensorDim(output_shape, data_format_, '2'),\n         GetTensorDim(output_shape, data_format_, '1'),\n         GetTensorDim(output_shape, data_format_, '0')}};\n    std::array<int64_t, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                   GetTensorDim(ksize_, data_format_, '1'),\n                                   GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64_t, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                   GetTensorDim(stride_, data_format_, '1'),\n                                   GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64_t, 3> padding, out;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n\n    LaunchAvgPooling3dGradOp<Device, T>::launch(\n        context, output_shape, out_backprop, window, stride, out, padding,\n        data_format_, output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context, const Pool3dParameters& params,\n                     const Tensor& tensor_in, const Tensor& tensor_out,\n                     const Tensor& tensor_top_diff,\n                     Tensor* tensor_bottom_diff) {\n    OP_REQUIRES(\n        context, params.data_format == FORMAT_NHWC,\n        errors::InvalidArgument(\"Default MaxPooling3dGradGradOp only supports\",\n                                \"NDHWC on CPU device type\"));\n\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), params.depth,\n                               params.tensor_in_planes * params.tensor_in_cols *\n                                   params.tensor_in_rows *\n                                   params.tensor_in_batch);\n    ConstEigenMatrixMap out_mat(tensor_out.flat<T>().data(), params.depth,\n                                params.out_plane * params.out_width *\n                                    params.out_height * params.tensor_in_batch);\n    ConstEigenMatrixMap top_diff_mat(\n        tensor_top_diff.flat<T>().data(), params.depth,\n        params.tensor_in_planes * params.tensor_in_cols *\n            params.tensor_in_rows * params.tensor_in_batch);\n    EigenMatrixMap bottom_diff_mat(\n        tensor_bottom_diff->flat<T>().data(), params.depth,\n        params.out_plane * params.out_width * params.out_height *\n            params.tensor_in_batch);\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n\n    auto shard = [&params, &in_mat, &out_mat, &top_diff_mat, &bottom_diff_mat](\n                     int64_t start, int64_t limit) {\n      const int32_t depth = params.depth;\n      const int32_t in_planes = params.tensor_in_planes;\n      const int32_t in_rows = params.tensor_in_rows;\n      const int32_t in_cols = params.tensor_in_cols;\n      const int32_t pad_planes = params.pad_planes;\n      const int32_t pad_rows = params.pad_rows;\n      const int32_t pad_cols = params.pad_cols;\n      const int32_t window_planes = params.window_planes;\n      const int32_t window_rows = params.window_rows;\n      const int32_t window_cols = params.window_cols;\n      const int32_t plane_stride = params.plane_stride;\n      const int32_t row_stride = params.row_stride;\n      const int32_t col_stride = params.col_stride;\n      const int32_t out_plane = params.out_plane;\n      const int32_t out_height = params.out_height;\n      const int32_t out_width = params.out_width;\n\n      {\n        // Initializes the output grad backprop tensor with 0.\n        const int32_t output_image_size =\n            out_plane * out_height * out_width * params.depth;\n        EigenMatrixMap bottom_diff_shard(\n            bottom_diff_mat.data() + start * output_image_size, 1,\n            (limit - start) * output_image_size);\n        bottom_diff_shard.setZero();\n      }\n\n      for (int b = start; b < limit; ++b) {\n        for (int pp = 0; pp < out_plane; ++pp) {\n          for (int ph = 0; ph < out_height; ++ph) {\n            for (int pw = 0; pw < out_width; ++pw) {\n              // (p_start, p_end) * (h_start, h_end) * (w_start, w_end) is the\n              // range that the input vector projects to.\n              int p_start = pp * plane_stride - pad_planes;\n              const int p_end = std::min(p_start + window_planes, in_planes);\n              int h_start = ph * row_stride - pad_rows;\n              const int h_end = std::min(h_start + window_rows, in_rows);\n              int w_start = pw * col_stride - pad_cols;\n              const int w_end = std::min(w_start + window_cols, in_cols);\n              p_start = std::max(p_start, 0);\n              h_start = std::max(h_start, 0);\n              w_start = std::max(w_start, 0);\n              const int out_index =\n                  ((b * out_plane + pp) * out_height + ph) * out_width + pw;\n              // Find value corresponding to the input maximum in top_diff.\n              for (int d = 0; d < depth; ++d) {\n                const T& output_ref = out_mat.coeffRef(d, out_index);\n                bool should_stop = false;\n                for (int p = p_start; p < p_end && !should_stop; ++p) {\n                  for (int h = h_start; h < h_end && !should_stop; ++h) {\n                    for (int w = w_start; w < w_end && !should_stop; ++w) {\n                      const int in_index =\n                          ((b * in_planes + p) * in_rows + h) * in_cols + w;\n                      const T& input_ref = in_mat.coeffRef(d, in_index);\n                      if (output_ref == input_ref) {\n                        T& bottom_diff_ref =\n                            bottom_diff_mat.coeffRef(d, out_index);\n                        bottom_diff_ref = top_diff_mat.coeffRef(d, in_index);\n                        should_stop = true;\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    };\n    const int64_t shard_cost =\n        params.out_plane * params.out_height * params.out_width * params.depth *\n        params.window_planes * params.window_rows * params.window_cols;\n    Shard(worker_threads.num_threads, worker_threads.workers,\n          params.tensor_in_batch, shard_cost, shard);\n  }\n};\n\ntemplate <class Device, class T>\nclass MaxPooling3dGradGradOp : public OpKernel {\n public:\n  explicit MaxPooling3dGradGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    const int32_t ksize_c = GetTensorDim(ksize_, data_format_, 'C');\n    const int32_t stride_c = GetTensorDim(stride_, data_format_, 'C');\n    OP_REQUIRES(context, ksize_c == 1 && stride_c == 1,\n                errors::Unimplemented(\"MaxPooling3dGradGrad is not yet \"\n                                      \"supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_grad_backprop = context->input(2);\n\n    // For maxpooling3d, tensor_in should have 5 dimensions.\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    // For maxpooling3d, out_grad_backprop should have 5 dimensions.\n    OP_REQUIRES(\n        context, out_grad_backprop.dims() == 5,\n        errors::InvalidArgument(\"out_grad_backprop must be 5-dimensional\"));\n\n    Pool3dParameters params{context,  ksize_,       stride_,\n                            padding_, data_format_, tensor_in.shape()};\n    if (!context->status().ok()) return;  // params is invalid\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {2}, 0, tensor_out.shape(), &output));\n\n    // Given access patterns in LaunchMaxPooling3dGradGradOp, these tensors must\n    // have elements.\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_in: \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_out: \",\n                                        tensor_out.DebugString()));\n    OP_REQUIRES(\n        context, out_grad_backprop.NumElements() > 0,\n        errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                out_grad_backprop.DebugString()));\n    OP_REQUIRES(context,\n                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n                                        \"have same number of elements, got <\",\n                                        tensor_in.DebugString(), \"> and <\",\n                                        out_grad_backprop.DebugString(), \">\"));\n    OP_REQUIRES(\n        context, tensor_out.NumElements() == output->NumElements(),\n        errors::InvalidArgument(\n            \"tensor_out and output must have same number of elements, got <\",\n            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));\n\n    LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n        context, params, tensor_in, tensor_out, out_grad_backprop, output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\n#define REGISTER_KERNELS(D, T)                                             \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, MAX>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"MaxPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .TypeConstraint<T>(\"TInput\"),                \\\n                          MaxPooling3dGradOp<D##Device, T>);               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3DGradGrad\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"), \\\n      MaxPooling3dGradGradOp<D##Device, T>);                               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"AvgPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, AVG>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"AvgPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .HostMemory(\"orig_input_shape\"),             \\\n                          AvgPooling3dGradOp<D##Device, T>);\n\n#define REGISTER_CPU_KERNELS(T) REGISTER_KERNELS(CPU, T)\nTF_CALL_float(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename T>\nstruct LaunchPoolingOp<GPUDevice, T, AVG> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    DnnPooling3dOp<T>::Compute(context, se::dnn::PoolingMode::kAverage, window,\n                               stride, padding, data_format, tensor_in, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchPoolingOp<GPUDevice, T, MAX> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    DnnPooling3dOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum, window,\n                               stride, padding, data_format, tensor_in, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const Tensor& tensor_out, const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* input_backprop) {\n    const TensorShape output_shape = tensor_in.shape();\n    DnnPooling3dGradOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum,\n                                   window, stride, padding, out, data_format,\n                                   out_backprop, output_shape, &tensor_in,\n                                   &tensor_out, input_backprop);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchAvgPooling3dGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context,\n                     const TensorShape& tensor_in_shape,\n                     const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    DnnPooling3dGradOp<T>::Compute(\n        context, se::dnn::PoolingMode::kAverage, window, stride, padding, out,\n        data_format, out_backprop, tensor_in_shape, nullptr, nullptr, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context, const Pool3dParameters& params,\n                     const Tensor& tensor_in, const Tensor& tensor_out,\n                     const Tensor& tensor_top_diff,\n                     Tensor* tensor_bottom_diff) {\n    bool status = functor::MaxPool3dGradBackward<T>()(\n        params.data_format, tensor_in.flat<T>().data(),\n        tensor_out.flat<T>().data(), params.tensor_in_batch, params.out_plane,\n        params.out_height, params.out_width, params.depth,\n        params.tensor_in_planes, params.tensor_in_rows, params.tensor_in_cols,\n        params.window_planes, params.window_rows, params.window_cols,\n        params.plane_stride, params.row_stride, params.col_stride,\n        params.pad_planes, params.pad_rows, params.pad_cols,\n        tensor_top_diff.flat<T>().data(), tensor_bottom_diff->flat<T>().data(),\n        context->eigen_gpu_device());\n    if (!status) {\n      context->SetStatus(\n          errors::Internal(\"Failed launching MaxPool3dGradBackward\"));\n    }\n  }\n};\n\n#define REGISTER_GPU_KERNELS(T) REGISTER_KERNELS(GPU, T)\nTF_CALL_float(REGISTER_GPU_KERNELS) TF_CALL_half(REGISTER_GPU_KERNELS)\n#undef REGISTER_GPU_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n#undef REGISTER_KERNELS\n\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for 3d pooling operations.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\ndef GetTestConfigs():\n  \"\"\"Get all the valid tests configs to run.\n\n  Returns:\n    all the valid test configs as tuples of data_format and use_gpu.\n  \"\"\"\n  test_configs = [(\"NDHWC\", False), (\"NDHWC\", True)]\n  if test.is_gpu_available(cuda_only=True):\n    # \"NCHW\" format is currently supported exclusively on CUDA GPUs.\n    test_configs += [(\"NCDHW\", True)]\n  return test_configs\n\n\n# TODO(mjanusz): Add microbenchmarks for 3d pooling.\nclass PoolingTest(test.TestCase):\n\n  def _VerifyOneTest(self, pool_func, input_sizes, window, strides, padding,\n                     data_format, expected, use_gpu):\n    \"\"\"Verifies the output values of the pooling function.\n\n    Args:\n      pool_func: Function to be called: co.MaxPool, co.AvgPool.\n      input_sizes: Input tensor dimensions.\n      window: Tuple of kernel dims: planes, rows, cols.\n      strides: Tuple of strides for dims: planes, rows, cols.\n      padding: Padding type.\n      data_format: The data format we use to run the pooling operation.\n      expected: An array containing the expected operation outputs.\n      use_gpu: Whether to run ops on GPU.\n    \"\"\"\n    total_size = 1\n    for s in input_sizes:\n      total_size *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x = [f * 1.0 for f in range(1, total_size + 1)]\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      t = constant_op.constant(x, shape=input_sizes)\n      window = [1] + list(window) + [1]\n      strides = [1] + list(strides) + [1]\n      if data_format == \"NCDHW\":\n        t = test_util.NHWCToNCHW(t)\n        window = test_util.NHWCToNCHW(window)\n        strides = test_util.NHWCToNCHW(strides)\n      t = pool_func(\n          t,\n          ksize=window,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n      if data_format == \"NCDHW\":\n        t = test_util.NCHWToNHWC(t)\n      vals = self.evaluate(t)\n    # Verifies values.\n    actual = vals.flatten()\n    self.assertAllClose(expected, actual)\n\n  def _VerifyValues(self, pool_func, input_sizes, window, strides,\n                    padding, expected):\n    for data_format, use_gpu in GetTestConfigs():\n      self._VerifyOneTest(pool_func, input_sizes, window, strides, padding,\n                          data_format, expected, use_gpu)\n\n  def testAvgPool3dValidPadding(self):\n    expected_output = [20.5, 21.5, 22.5]\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"VALID\",\n        expected=expected_output)\n\n  def testAvgPool3dSamePadding(self):\n    expected_output = [20.5, 21.5, 22.5, 26.5, 27.5, 28.5]\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 2, 2, 4, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"SAME\",\n        expected=expected_output)\n\n  def testAvgPool3dSamePaddingDifferentStrides(self):\n    expected_output = [1.5, 4.5, 7.5, 17.5, 20.5, 23.5, 33.5, 36.5, 39.5]\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 5, 8, 1, 1],\n        window=(1, 2, 3),\n        strides=(2, 3, 1),\n        padding=\"SAME\",\n        expected=expected_output)\n\n  def testMaxPool3dValidPadding(self):\n    expected_output = [40.0, 41.0, 42.0]\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 3, 3, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"VALID\",\n        expected=expected_output)\n\n  def testMaxPool3dSamePadding(self):\n    expected_output = [31., 32., 33., 34., 35., 36.]\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 2, 2, 3, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"SAME\",\n        expected=expected_output)\n\n  def testMaxPool3dSamePaddingDifferentStrides(self):\n    expected_output = [2., 5., 8., 18., 21., 24., 34., 37., 40.]\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 5, 8, 1, 1],\n        window=(1, 2, 3),\n        strides=(2, 3, 1),\n        padding=\"SAME\",\n        expected=expected_output)\n\n    # Test pooling on a larger input, with different stride and kernel\n    # size for the 'z' dimension.\n\n    # Simulate max pooling in numpy to get the expected output.\n    input_data = np.arange(1, 5 * 27 * 27 * 64 + 1).reshape((5, 27, 27, 64))\n    input_data = np.pad(input_data, [[0, 0], [0, 1], [0, 1], [0, 0]],\n                        mode=\"constant\")\n    expected_output = input_data[:, 1::2, 1::2, :]\n    expected_output[:, -1, :, :] = input_data[:, -2, 1::2, :]\n    expected_output[:, :, -1, :] = input_data[:, 1::2, -2, :]\n    expected_output[:, -1, -1, :] = input_data[:, -2, -2, :]\n\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 5, 27, 27, 64],\n        window=(1, 2, 2),\n        strides=(1, 2, 2),\n        padding=\"SAME\",\n        expected=expected_output.flatten())\n\n  def testKernelSmallerThanStride(self):\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        window=[1, 1, 1],\n        strides=[2, 2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9, 19, 21, 25, 27])\n\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 7, 7, 7, 1],\n        window=[2, 2, 2],\n        strides=[3, 3, 3],\n        padding=\"VALID\",\n        expected=[58, 61, 79, 82, 205, 208, 226, 229])\n\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        window=[1, 1, 1],\n        strides=[2, 2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9, 19, 21, 25, 27])\n\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 7, 7, 7, 1],\n        window=[2, 2, 2],\n        strides=[3, 3, 3],\n        padding=\"VALID\",\n        expected=[29.5, 32.5, 50.5, 53.5, 176.5, 179.5, 197.5, 200.5])\n\n  def testMaxPool3DEmptyTensorOutputShape(self):\n    \"\"\"Verifies the output shape of the max pooling function when tensor is empty.\n\n    Args: none\n    \"\"\"\n    input_sizes = [0, 112, 112, 112, 64]\n\n    input_data = 1.\n    input_tensor = constant_op.constant(\n        input_data, shape=input_sizes, name=\"input\")\n    max_pool_3d = nn_ops.max_pool3d(\n        input_tensor,\n        ksize=[2, 2, 2],\n        strides=[2, 2, 2],\n        padding=\"VALID\",\n        data_format=\"NDHWC\",\n        name=\"max_pool_3d\")\n    values = self.evaluate(max_pool_3d)\n    self.assertEqual(values.shape, (0, 56, 56, 56, 64))\n\n  def _ConstructAndTestGradientForConfig(self,\n                                         pool_func,\n                                         input_sizes,\n                                         output_sizes,\n                                         window,\n                                         strides,\n                                         padding,\n                                         data_format,\n                                         use_gpu):\n    \"\"\"Verifies the gradients of a pooling function.\n\n    Args:\n      pool_func: Function to be called, co.MaxPool, co.AvgPool,\n        or the Lua version.\n      input_sizes: Input tensor dimensions.\n      output_sizes: Output tensor dimensions.\n      window: Tuple of kernel dims: planes, rows, cols.\n      strides: Tuple of strides for dims: planes, rows, cols.\n      padding: Padding type.\n      data_format: Data format string.\n      use_gpu: Whether to run on GPU.\n    \"\"\"\n    total_size = 1\n    for s in input_sizes:\n      total_size *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x = np.arange(1, total_size + 1, dtype=np.float32)\n    with self.cached_session(use_gpu=use_gpu):\n      input_tensor = constant_op.constant(x, shape=input_sizes, name=\"input\")\n      err_g_margin = 1e-3\n      err_gg_margin = 1.5e-2\n      if pool_func == nn_ops.avg_pool3d:\n        func_name = \"avg_pool3d\"\n        x_init_value = None\n      else:\n        x_init_value = np.asfarray(np.arange(1, total_size + 1),\n                                   dtype=np.float32).reshape(input_sizes)\n        func_name = \"max_pool3d\"\n\n      ksize = [1, window[0], window[1], window[2], 1]\n      strides = [1, strides[0], strides[1], strides[2], 1]\n      t = input_tensor\n\n      if data_format == \"NCDHW\":\n        ksize = test_util.NHWCToNCHW(ksize)\n        strides = test_util.NHWCToNCHW(strides)\n        t = test_util.NHWCToNCHW(t)\n        output_sizes = test_util.NHWCToNCHW(output_sizes)\n\n      t = pool_func(\n          t,\n          ksize=ksize,\n          strides=strides,\n          padding=padding,\n          data_format=data_format,\n          name=func_name)\n      t_g = gradients_impl.gradients(t**2, input_tensor)[0]\n\n      err_g = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_sizes,\n          t,\n          output_sizes,\n          x_init_value=x_init_value,\n          delta=1e-2)\n      err_gg = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_sizes,\n          t_g,\n          input_sizes,\n          x_init_value=x_init_value,\n          delta=1e-2)\n\n    print(\"%s gradient error = \" % func_name, err_g)\n    self.assertLess(err_g, err_g_margin)\n    print(\"%s second-order gradient error = \" % func_name, err_gg)\n    self.assertLess(err_gg, err_gg_margin)\n\n  def _ConstructAndTestGradient(self,\n                                pool_func,\n                                **kwargs):\n    \"\"\"Runs _ConstructAndTestGradientForConfig for all tests configurations.\"\"\"\n\n    for data_format, use_gpu in GetTestConfigs():\n      self._ConstructAndTestGradientForConfig(pool_func,\n                                              data_format=data_format,\n                                              use_gpu=use_gpu,\n                                              **kwargs)\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding1_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        output_sizes=[1, 3, 3, 3, 1],\n        window=(1, 1, 1),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding2_1_6_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 2, 3, 4, 2],\n        output_sizes=[1, 1, 2, 3, 2],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding2_1_7_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 2, 7, 1],\n        output_sizes=[1, 2, 1, 6, 1],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding1_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        output_sizes=[1, 2, 2, 2, 1],\n        window=(1, 1, 1),\n        strides=(2, 2, 2),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding2_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[2, 2, 2, 2, 1],\n        output_sizes=[2, 1, 1, 1, 1],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding1_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 2, 4, 1],\n        output_sizes=[1, 3, 2, 4, 1],\n        window=(1, 1, 1),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding1_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 2, 4, 1],\n        output_sizes=[1, 2, 1, 2, 1],\n        window=(1, 1, 1),\n        strides=(2, 2, 2),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding2_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 2, 4, 1],\n        output_sizes=[1, 3, 2, 4, 1],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding2_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 5, 2, 4, 2],\n        output_sizes=[1, 3, 1, 2, 2],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding3_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 4, 2, 1],\n        output_sizes=[1, 3, 4, 2, 1],\n        window=(3, 3, 3),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradValidPadding1_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        output_sizes=[1, 3, 3, 3, 1],\n        window=(1, 1, 1),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradValidPadding1_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        output_sizes=[1, 2, 2, 2, 1],\n        window=(1, 1, 1),\n        strides=(2, 2, 2),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradValidPadding2_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 2],\n        output_sizes=[1, 2, 2, 2, 2],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradValidPadding2_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[2, 2, 2, 2, 2],\n        output_sizes=[2, 1, 1, 1, 2],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding1_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 2, 4, 2],\n        output_sizes=[1, 3, 2, 4, 2],\n        window=(1, 1, 1),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding1_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 2, 4, 2],\n        output_sizes=[1, 2, 1, 2, 2],\n        window=(1, 1, 1),\n        strides=(2, 2, 2),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding2_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 2, 2, 2, 1],\n        output_sizes=[1, 2, 2, 2, 1],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding2_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 5, 2, 4, 1],\n        output_sizes=[1, 3, 1, 2, 1],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding3_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 6, 2, 1],\n        output_sizes=[1, 3, 6, 2, 1],\n        window=(3, 3, 3),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/pooling_ops_3d.h\"\n\n#include <array>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_slice.h\"\n#include \"tensorflow/core/kernels/eigen_pooling.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n#include \"tensorflow/core/util/work_sharder.h\"\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/kernels/cudnn_pooling_gpu.h\"\n#include \"tensorflow/core/kernels/pooling_ops_3d_gpu.h\"\n#endif\n\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nPool3dParameters::Pool3dParameters(OpKernelContext* context,\n                                   const std::vector<int32>& ksize,\n                                   const std::vector<int32>& stride,\n                                   Padding padding, TensorFormat data_format,\n                                   const TensorShape& tensor_in_shape) {\n  // For maxpooling, tensor_in should have 4 dimensions.\n  OP_REQUIRES(context, tensor_in_shape.dims() == 5,\n              errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n  this->data_format = data_format;\n  depth = GetTensorDim(tensor_in_shape, data_format, 'C');\n  tensor_in_planes = GetTensorDim(tensor_in_shape, data_format, '0');\n  tensor_in_rows = GetTensorDim(tensor_in_shape, data_format, '1');\n  tensor_in_cols = GetTensorDim(tensor_in_shape, data_format, '2');\n  tensor_in_batch = GetTensorDim(tensor_in_shape, data_format, 'N');\n  window_planes = GetTensorDim(ksize, data_format, '0');\n  window_rows = GetTensorDim(ksize, data_format, '1');\n  window_cols = GetTensorDim(ksize, data_format, '2');\n  depth_window = GetTensorDim(ksize, data_format, 'C');\n  plane_stride = GetTensorDim(stride, data_format, '0');\n  row_stride = GetTensorDim(stride, data_format, '1');\n  col_stride = GetTensorDim(stride, data_format, '2');\n  depth_stride = GetTensorDim(stride, data_format, 'C');\n\n  // We only support 3D pooling across plane/width/height. Depthwise\n  // pooling is not supported.\n  OP_REQUIRES(\n      context, depth_window == 1 && depth_stride == 1,\n      errors::Unimplemented(\n          \"Pooling3d only supports pooling across plane/width/height.\"));\n\n  OP_REQUIRES_OK(context, GetWindowedOutputSize(tensor_in_planes, window_planes,\n                                                plane_stride, padding,\n                                                &out_plane, &pad_planes));\n  OP_REQUIRES_OK(context,\n                 GetWindowedOutputSize(tensor_in_rows, window_rows, row_stride,\n                                       padding, &out_height, &pad_rows));\n  OP_REQUIRES_OK(context,\n                 GetWindowedOutputSize(tensor_in_cols, window_cols, col_stride,\n                                       padding, &out_width, &pad_cols));\n}\n\nTensorShape Pool3dParameters::forward_output_shape() {\n  return ShapeFromFormat(data_format, tensor_in_batch,\n                         {{out_plane, out_height, out_width}}, depth);\n}\n\ntemplate <typename T>\nstruct LaunchPoolingOp<CPUDevice, T, AVG> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =\n        Eigen::CuboidAvgPooling(tensor_in.tensor<T, 5>(), window[0], window[1],\n                                window[2], stride[0], stride[1], stride[2],\n                                BrainPadding2EigenPadding(padding_type));\n  }\n};\n\ntemplate <typename T>\nstruct LaunchPoolingOp<CPUDevice, T, MAX> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =\n        Eigen::CuboidMaxPooling(tensor_in.tensor<T, 5>(), window[0], window[1],\n                                window[2], stride[0], stride[1], stride[2],\n                                BrainPadding2EigenPadding(padding_type));\n  }\n};\n\ntemplate <typename Device, typename T, PoolingType Type>\nclass Pooling3DOp : public UnaryOp<T> {\n public:\n  explicit Pooling3DOp(OpKernelConstruction* context) : UnaryOp<T>(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\"Default Pooling3DOp only supports NDHWC \",\n                                  \"on device type \",\n                                  DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    bool non_negative =\n        std::all_of(ksize_.begin(), ksize_.end(), [](int k) { return k > 0; });\n    OP_REQUIRES(context, non_negative,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"have non-negative dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    const int64_t depth = GetTensorDim(tensor_in, data_format_, 'C');\n    const int64_t in_batch = GetTensorDim(tensor_in, data_format_, 'N');\n\n    // Dimension order for these arrays is: x, y, z.\n    std::array<int64_t, 3> input_size{\n        {GetTensorDim(tensor_in, data_format_, '2'),\n         GetTensorDim(tensor_in, data_format_, '1'),\n         GetTensorDim(tensor_in, data_format_, '0')}};\n    std::array<int64_t, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                   GetTensorDim(ksize_, data_format_, '1'),\n                                   GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64_t, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                   GetTensorDim(stride_, data_format_, '1'),\n                                   GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64_t, 3> padding, out;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n\n    TensorShape out_shape = ShapeFromFormat(data_format_, in_batch,\n                                            {{out[2], out[1], out[0]}}, depth);\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n    if (out_shape.num_elements() == 0) return;\n    LaunchPoolingOp<Device, T, Type>::launch(context, tensor_in, window, stride,\n                                             padding, data_format_, padding_,\n                                             output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const Tensor& tensor_out, const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    output->flat<T>().setZero();\n    for (int64_t p = 0; p < out_backprop.dim_size(3); ++p) {\n      // Calculate broadcast size for planes/rows/cols. For SAME padding,\n      // current index could be in the padding area, and\n      //   p * stride_planes + window_planes\n      // could be beyond the input tensor's boundary. In such cases, change\n      // the starting index and reduce the broadcast size.\n      //\n      // The same procedure is repeated for every spatial dimension in the\n      // nested loops below.\n      int pindex, psize;\n      std::array<int64_t, 3> input_size{{tensor_in.dim_size(3),\n                                         tensor_in.dim_size(2),\n                                         tensor_in.dim_size(1)}};\n      OP_REQUIRES_OK(context,\n                     GetBroadcastSize(p, input_size[0], window[0], stride[0],\n                                      padding[0], &pindex, &psize));\n      for (int64_t r = 0; r < out_backprop.dim_size(2); ++r) {\n        int rindex, rsize;\n        OP_REQUIRES_OK(context,\n                       GetBroadcastSize(r, input_size[1], window[1], stride[1],\n                                        padding[1], &rindex, &rsize));\n        for (int64_t c = 0; c < out_backprop.dim_size(1); ++c) {\n          int cindex, csize;\n          OP_REQUIRES_OK(\n              context, GetBroadcastSize(c, input_size[2], window[2], stride[2],\n                                        padding[2], &cindex, &csize));\n          TensorSlice src{{0, -1}, {c, 1}, {r, 1}, {p, 1}, {0, -1}};\n          TensorSlice dst{{0, -1},\n                          {cindex, csize},\n                          {rindex, rsize},\n                          {pindex, psize},\n                          {0, -1}};\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_sizes;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_sizes;\n          src.FillIndicesAndSizes<5>(out_backprop.shape(), &src_indices,\n                                     &src_sizes);\n          dst.FillIndicesAndSizes<5>(tensor_in.shape(), &dst_indices,\n                                     &dst_sizes);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n          Eigen::array<int, 5> bcast = {1, csize, rsize, psize, 1};\n#else\n          Eigen::IndexList<Eigen::type2index<1>, int, int, int,\n                           Eigen::type2index<1>>\n              bcast;\n          bcast.set(1, csize);\n          bcast.set(2, rsize);\n          bcast.set(3, psize);\n#endif\n\n          // Slice from tensor_in.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> tensor_in_slice(dst_sizes);\n          tensor_in_slice.device(context->eigen_cpu_device()) =\n              tensor_in.tensor<T, 5>().slice(dst_indices, dst_sizes);\n\n          // Slice from tensor_out.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> tensor_out_slice(src_sizes);\n          tensor_out_slice.device(context->eigen_cpu_device()) =\n              tensor_out.tensor<T, 5>().slice(src_indices, src_sizes);\n\n          // Backprop slice.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> out_backprop_slice(src_sizes);\n          out_backprop_slice.device(context->eigen_cpu_device()) =\n              out_backprop.tensor<T, 5>().slice(src_indices, src_sizes);\n\n          // The true backprop slice: if an element is the max, choose\n          // the backprop slice; otherwise set to 0.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> select_slice(dst_sizes);\n          Eigen::Tensor<T, 5, Eigen::RowMajor> mat0(dst_sizes);\n          mat0.setZero();\n          select_slice =\n              ((tensor_in_slice - tensor_out_slice.broadcast(bcast)).abs() <\n               tensor_in_slice.constant(1e-5))\n                  .select(out_backprop_slice.broadcast(bcast), mat0);\n\n          output->tensor<T, 5>()\n              .slice(dst_indices, dst_sizes)\n              .device(context->eigen_cpu_device()) += select_slice;\n        }\n      }\n    }\n  }\n};\n\ntemplate <class Device, class T>\nclass MaxPooling3dGradOp : public OpKernel {\n public:\n  explicit MaxPooling3dGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\n              \"Default MaxPooling3dGradOp only supports NDHWC \",\n              \"on device type \", DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    OP_REQUIRES(context, out_backprop.dims() == 5,\n                errors::InvalidArgument(\"out_backprop must be 5-dimensional\"));\n\n    const TensorShape& output_shape = tensor_in.shape();\n    Tensor* input_backprop;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, output_shape, &input_backprop));\n    std::array<int64_t, 3> input_size{\n        {GetTensorDim(output_shape, data_format_, '2'),\n         GetTensorDim(output_shape, data_format_, '1'),\n         GetTensorDim(output_shape, data_format_, '0')}};\n    std::array<int64_t, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                   GetTensorDim(ksize_, data_format_, '1'),\n                                   GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64_t, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                   GetTensorDim(stride_, data_format_, '1'),\n                                   GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64_t, 3> out, padding;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n    LaunchMaxPooling3dGradOp<Device, T>::launch(\n        context, tensor_in, tensor_out, out_backprop, window, stride, out,\n        padding, data_format_, input_backprop);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchAvgPooling3dGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context,\n                     const TensorShape& tensor_in_shape,\n                     const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& output_shape,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    OP_REQUIRES(\n        context, tensor_in_shape.dim_size(0) == out_backprop.dim_size(0),\n        errors::InvalidArgument(\n            \"Expected first dimension of tensor_in_shape and \"\n            \"out_backprop to match, got \",\n            tensor_in_shape.dim_size(0), \" and \", out_backprop.dim_size(0)));\n    OP_REQUIRES(\n        context, tensor_in_shape.dim_size(4) == out_backprop.dim_size(4),\n        errors::InvalidArgument(\n            \"Expected last dimension of tensor_in_shape and \"\n            \"out_backprop to match, got \",\n            tensor_in_shape.dim_size(4), \" and \", out_backprop.dim_size(4)));\n\n    output->flat<T>().setZero();\n    std::array<int64_t, 3> input_size = {{tensor_in_shape.dim_size(3),\n                                          tensor_in_shape.dim_size(2),\n                                          tensor_in_shape.dim_size(1)}};\n    for (int64_t p = 0; p < out_backprop.dim_size(3); ++p) {\n      // Calculate broadcast size for planes/rows/cols. For SAME padding,\n      // current index could be in the padding area, and\n      //   p * stride_planes + window_planes\n      // could be beyond the input tensor's boundary. In such cases, change\n      // the starting index and reduce the broadcast size.\n      //\n      // The same procedure is repeated for every spatial dimension in the\n      // nested loops below.\n      int pindex, psize;\n      OP_REQUIRES_OK(context,\n                     GetBroadcastSize(p, input_size[0], window[0], stride[0],\n                                      padding[0], &pindex, &psize));\n      for (int64_t r = 0; r < out_backprop.dim_size(2); ++r) {\n        int rindex, rsize;\n        OP_REQUIRES_OK(context,\n                       GetBroadcastSize(r, input_size[1], window[1], stride[1],\n                                        padding[1], &rindex, &rsize));\n        for (int64_t c = 0; c < out_backprop.dim_size(1); ++c) {\n          int cindex, csize;\n          OP_REQUIRES_OK(\n              context, GetBroadcastSize(c, input_size[2], window[2], stride[2],\n                                        padding[2], &cindex, &csize));\n          TensorSlice src{{0, -1}, {c, 1}, {r, 1}, {p, 1}, {0, -1}};\n          TensorSlice dst{{0, -1},\n                          {cindex, csize},\n                          {rindex, rsize},\n                          {pindex, psize},\n                          {0, -1}};\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_sizes;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_sizes;\n          src.FillIndicesAndSizes<5>(out_backprop.shape(), &src_indices,\n                                     &src_sizes);\n          dst.FillIndicesAndSizes<5>(tensor_in_shape, &dst_indices, &dst_sizes);\n#if !defined(EIGEN_HAS_INDEX_LIST)\n          Eigen::array<int, 5> bcast = {1, csize, rsize, psize, 1};\n#else\n          Eigen::IndexList<Eigen::type2index<1>, int, int, int,\n                           Eigen::type2index<1>>\n              bcast;\n          bcast.set(1, csize);\n          bcast.set(2, rsize);\n          bcast.set(3, psize);\n#endif\n          Eigen::Tensor<T, 5, Eigen::RowMajor> slices(src_sizes);\n          slices.device(context->eigen_cpu_device()) =\n              out_backprop.tensor<T, 5>().slice(src_indices, src_sizes);\n          // Divide by the size of the actual patch (psize * rsize * csize).\n          float divide_size = rsize * csize * psize * 1.0f;\n          slices *= slices.constant(1.0f / divide_size);\n\n          output->tensor<T, 5>()\n              .slice(dst_indices, dst_sizes)\n              .device(context->eigen_cpu_device()) += slices.broadcast(bcast);\n        }\n      }\n    }\n  }\n};\n\ntemplate <class Device, class T>\nclass AvgPooling3dGradOp : public OpKernel {\n public:\n  explicit AvgPooling3dGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\n              \"Default AvgPooling3dGradOp only supports NDHWC \",\n              \"on device type \", DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in_shape = context->input(0);\n    const Tensor& out_backprop = context->input(1);\n    OP_REQUIRES(\n        context,\n        tensor_in_shape.dims() == 1 && tensor_in_shape.NumElements() == 5,\n        errors::InvalidArgument(\"tensor_in must be 1-dimensional and 5 \"\n                                \"elements\"));\n    OP_REQUIRES(context, out_backprop.dims() == 5,\n                errors::InvalidArgument(\"out_backprop must be 5-dimensional\"));\n\n    TensorShape output_shape;\n    auto shape_vec = tensor_in_shape.vec<int32>();\n    for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\n      output_shape.AddDim(shape_vec(i));\n    }\n\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n\n    // Dimension order for these arrays is x, y, z.\n    std::array<int64_t, 3> input_size{\n        {GetTensorDim(output_shape, data_format_, '2'),\n         GetTensorDim(output_shape, data_format_, '1'),\n         GetTensorDim(output_shape, data_format_, '0')}};\n    std::array<int64_t, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                   GetTensorDim(ksize_, data_format_, '1'),\n                                   GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64_t, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                   GetTensorDim(stride_, data_format_, '1'),\n                                   GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64_t, 3> padding, out;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n\n    LaunchAvgPooling3dGradOp<Device, T>::launch(\n        context, output_shape, out_backprop, window, stride, out, padding,\n        data_format_, output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context, const Pool3dParameters& params,\n                     const Tensor& tensor_in, const Tensor& tensor_out,\n                     const Tensor& tensor_top_diff,\n                     Tensor* tensor_bottom_diff) {\n    OP_REQUIRES(\n        context, params.data_format == FORMAT_NHWC,\n        errors::InvalidArgument(\"Default MaxPooling3dGradGradOp only supports\",\n                                \"NDHWC on CPU device type\"));\n\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), params.depth,\n                               params.tensor_in_planes * params.tensor_in_cols *\n                                   params.tensor_in_rows *\n                                   params.tensor_in_batch);\n    ConstEigenMatrixMap out_mat(tensor_out.flat<T>().data(), params.depth,\n                                params.out_plane * params.out_width *\n                                    params.out_height * params.tensor_in_batch);\n    ConstEigenMatrixMap top_diff_mat(\n        tensor_top_diff.flat<T>().data(), params.depth,\n        params.tensor_in_planes * params.tensor_in_cols *\n            params.tensor_in_rows * params.tensor_in_batch);\n    EigenMatrixMap bottom_diff_mat(\n        tensor_bottom_diff->flat<T>().data(), params.depth,\n        params.out_plane * params.out_width * params.out_height *\n            params.tensor_in_batch);\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n\n    auto shard = [&params, &in_mat, &out_mat, &top_diff_mat, &bottom_diff_mat](\n                     int64_t start, int64_t limit) {\n      const int32_t depth = params.depth;\n      const int32_t in_planes = params.tensor_in_planes;\n      const int32_t in_rows = params.tensor_in_rows;\n      const int32_t in_cols = params.tensor_in_cols;\n      const int32_t pad_planes = params.pad_planes;\n      const int32_t pad_rows = params.pad_rows;\n      const int32_t pad_cols = params.pad_cols;\n      const int32_t window_planes = params.window_planes;\n      const int32_t window_rows = params.window_rows;\n      const int32_t window_cols = params.window_cols;\n      const int32_t plane_stride = params.plane_stride;\n      const int32_t row_stride = params.row_stride;\n      const int32_t col_stride = params.col_stride;\n      const int32_t out_plane = params.out_plane;\n      const int32_t out_height = params.out_height;\n      const int32_t out_width = params.out_width;\n\n      {\n        // Initializes the output grad backprop tensor with 0.\n        const int32_t output_image_size =\n            out_plane * out_height * out_width * params.depth;\n        EigenMatrixMap bottom_diff_shard(\n            bottom_diff_mat.data() + start * output_image_size, 1,\n            (limit - start) * output_image_size);\n        bottom_diff_shard.setZero();\n      }\n\n      for (int b = start; b < limit; ++b) {\n        for (int pp = 0; pp < out_plane; ++pp) {\n          for (int ph = 0; ph < out_height; ++ph) {\n            for (int pw = 0; pw < out_width; ++pw) {\n              // (p_start, p_end) * (h_start, h_end) * (w_start, w_end) is the\n              // range that the input vector projects to.\n              int p_start = pp * plane_stride - pad_planes;\n              const int p_end = std::min(p_start + window_planes, in_planes);\n              int h_start = ph * row_stride - pad_rows;\n              const int h_end = std::min(h_start + window_rows, in_rows);\n              int w_start = pw * col_stride - pad_cols;\n              const int w_end = std::min(w_start + window_cols, in_cols);\n              p_start = std::max(p_start, 0);\n              h_start = std::max(h_start, 0);\n              w_start = std::max(w_start, 0);\n              const int out_index =\n                  ((b * out_plane + pp) * out_height + ph) * out_width + pw;\n              // Find value corresponding to the input maximum in top_diff.\n              for (int d = 0; d < depth; ++d) {\n                const T& output_ref = out_mat.coeffRef(d, out_index);\n                bool should_stop = false;\n                for (int p = p_start; p < p_end && !should_stop; ++p) {\n                  for (int h = h_start; h < h_end && !should_stop; ++h) {\n                    for (int w = w_start; w < w_end && !should_stop; ++w) {\n                      const int in_index =\n                          ((b * in_planes + p) * in_rows + h) * in_cols + w;\n                      const T& input_ref = in_mat.coeffRef(d, in_index);\n                      if (output_ref == input_ref) {\n                        T& bottom_diff_ref =\n                            bottom_diff_mat.coeffRef(d, out_index);\n                        bottom_diff_ref = top_diff_mat.coeffRef(d, in_index);\n                        should_stop = true;\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    };\n    const int64_t shard_cost =\n        params.out_plane * params.out_height * params.out_width * params.depth *\n        params.window_planes * params.window_rows * params.window_cols;\n    Shard(worker_threads.num_threads, worker_threads.workers,\n          params.tensor_in_batch, shard_cost, shard);\n  }\n};\n\ntemplate <class Device, class T>\nclass MaxPooling3dGradGradOp : public OpKernel {\n public:\n  explicit MaxPooling3dGradGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    const int32_t ksize_c = GetTensorDim(ksize_, data_format_, 'C');\n    const int32_t stride_c = GetTensorDim(stride_, data_format_, 'C');\n    OP_REQUIRES(context, ksize_c == 1 && stride_c == 1,\n                errors::Unimplemented(\"MaxPooling3dGradGrad is not yet \"\n                                      \"supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_grad_backprop = context->input(2);\n\n    // For maxpooling3d, tensor_in should have 5 dimensions.\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    // For maxpooling3d, out_grad_backprop should have 5 dimensions.\n    OP_REQUIRES(\n        context, out_grad_backprop.dims() == 5,\n        errors::InvalidArgument(\"out_grad_backprop must be 5-dimensional\"));\n\n    Pool3dParameters params{context,  ksize_,       stride_,\n                            padding_, data_format_, tensor_in.shape()};\n    if (!context->status().ok()) return;  // params is invalid\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {2}, 0, tensor_out.shape(), &output));\n\n    // Given access patterns in LaunchMaxPooling3dGradGradOp, these tensors must\n    // have elements.\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_in: \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_out: \",\n                                        tensor_out.DebugString()));\n    OP_REQUIRES(\n        context, out_grad_backprop.NumElements() > 0,\n        errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                out_grad_backprop.DebugString()));\n    OP_REQUIRES(context,\n                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n                                        \"have same number of elements, got <\",\n                                        tensor_in.DebugString(), \"> and <\",\n                                        out_grad_backprop.DebugString(), \">\"));\n    OP_REQUIRES(\n        context, tensor_out.NumElements() == output->NumElements(),\n        errors::InvalidArgument(\n            \"tensor_out and output must have same number of elements, got <\",\n            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));\n\n    LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n        context, params, tensor_in, tensor_out, out_grad_backprop, output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\n#define REGISTER_KERNELS(D, T)                                             \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, MAX>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"MaxPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .TypeConstraint<T>(\"TInput\"),                \\\n                          MaxPooling3dGradOp<D##Device, T>);               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3DGradGrad\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"), \\\n      MaxPooling3dGradGradOp<D##Device, T>);                               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"AvgPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, AVG>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"AvgPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .HostMemory(\"orig_input_shape\"),             \\\n                          AvgPooling3dGradOp<D##Device, T>);\n\n#define REGISTER_CPU_KERNELS(T) REGISTER_KERNELS(CPU, T)\nTF_CALL_float(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename T>\nstruct LaunchPoolingOp<GPUDevice, T, AVG> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    DnnPooling3dOp<T>::Compute(context, se::dnn::PoolingMode::kAverage, window,\n                               stride, padding, data_format, tensor_in, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchPoolingOp<GPUDevice, T, MAX> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    DnnPooling3dOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum, window,\n                               stride, padding, data_format, tensor_in, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const Tensor& tensor_out, const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* input_backprop) {\n    const TensorShape output_shape = tensor_in.shape();\n    DnnPooling3dGradOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum,\n                                   window, stride, padding, out, data_format,\n                                   out_backprop, output_shape, &tensor_in,\n                                   &tensor_out, input_backprop);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchAvgPooling3dGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context,\n                     const TensorShape& tensor_in_shape,\n                     const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    DnnPooling3dGradOp<T>::Compute(\n        context, se::dnn::PoolingMode::kAverage, window, stride, padding, out,\n        data_format, out_backprop, tensor_in_shape, nullptr, nullptr, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context, const Pool3dParameters& params,\n                     const Tensor& tensor_in, const Tensor& tensor_out,\n                     const Tensor& tensor_top_diff,\n                     Tensor* tensor_bottom_diff) {\n    bool status = functor::MaxPool3dGradBackward<T>()(\n        params.data_format, tensor_in.flat<T>().data(),\n        tensor_out.flat<T>().data(), params.tensor_in_batch, params.out_plane,\n        params.out_height, params.out_width, params.depth,\n        params.tensor_in_planes, params.tensor_in_rows, params.tensor_in_cols,\n        params.window_planes, params.window_rows, params.window_cols,\n        params.plane_stride, params.row_stride, params.col_stride,\n        params.pad_planes, params.pad_rows, params.pad_cols,\n        tensor_top_diff.flat<T>().data(), tensor_bottom_diff->flat<T>().data(),\n        context->eigen_gpu_device());\n    if (!status) {\n      context->SetStatus(\n          errors::Internal(\"Failed launching MaxPool3dGradBackward\"));\n    }\n  }\n};\n\n#define REGISTER_GPU_KERNELS(T) REGISTER_KERNELS(GPU, T)\nTF_CALL_float(REGISTER_GPU_KERNELS) TF_CALL_half(REGISTER_GPU_KERNELS)\n#undef REGISTER_GPU_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n#undef REGISTER_KERNELS\n\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for 3d pooling operations.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\ndef GetTestConfigs():\n  \"\"\"Get all the valid tests configs to run.\n\n  Returns:\n    all the valid test configs as tuples of data_format and use_gpu.\n  \"\"\"\n  test_configs = [(\"NDHWC\", False), (\"NDHWC\", True)]\n  if test.is_gpu_available(cuda_only=True):\n    # \"NCHW\" format is currently supported exclusively on CUDA GPUs.\n    test_configs += [(\"NCDHW\", True)]\n  return test_configs\n\n\n# TODO(mjanusz): Add microbenchmarks for 3d pooling.\nclass PoolingTest(test.TestCase):\n\n  def _VerifyOneTest(self, pool_func, input_sizes, window, strides, padding,\n                     data_format, expected, use_gpu):\n    \"\"\"Verifies the output values of the pooling function.\n\n    Args:\n      pool_func: Function to be called: co.MaxPool, co.AvgPool.\n      input_sizes: Input tensor dimensions.\n      window: Tuple of kernel dims: planes, rows, cols.\n      strides: Tuple of strides for dims: planes, rows, cols.\n      padding: Padding type.\n      data_format: The data format we use to run the pooling operation.\n      expected: An array containing the expected operation outputs.\n      use_gpu: Whether to run ops on GPU.\n    \"\"\"\n    total_size = 1\n    for s in input_sizes:\n      total_size *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x = [f * 1.0 for f in range(1, total_size + 1)]\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      t = constant_op.constant(x, shape=input_sizes)\n      window = [1] + list(window) + [1]\n      strides = [1] + list(strides) + [1]\n      if data_format == \"NCDHW\":\n        t = test_util.NHWCToNCHW(t)\n        window = test_util.NHWCToNCHW(window)\n        strides = test_util.NHWCToNCHW(strides)\n      t = pool_func(\n          t,\n          ksize=window,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n      if data_format == \"NCDHW\":\n        t = test_util.NCHWToNHWC(t)\n      vals = self.evaluate(t)\n    # Verifies values.\n    actual = vals.flatten()\n    self.assertAllClose(expected, actual)\n\n  def _VerifyValues(self, pool_func, input_sizes, window, strides,\n                    padding, expected):\n    for data_format, use_gpu in GetTestConfigs():\n      self._VerifyOneTest(pool_func, input_sizes, window, strides, padding,\n                          data_format, expected, use_gpu)\n\n  def testAvgPool3dValidPadding(self):\n    expected_output = [20.5, 21.5, 22.5]\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"VALID\",\n        expected=expected_output)\n\n  def testAvgPool3dSamePadding(self):\n    expected_output = [20.5, 21.5, 22.5, 26.5, 27.5, 28.5]\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 2, 2, 4, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"SAME\",\n        expected=expected_output)\n\n  def testAvgPool3dSamePaddingDifferentStrides(self):\n    expected_output = [1.5, 4.5, 7.5, 17.5, 20.5, 23.5, 33.5, 36.5, 39.5]\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 5, 8, 1, 1],\n        window=(1, 2, 3),\n        strides=(2, 3, 1),\n        padding=\"SAME\",\n        expected=expected_output)\n\n  def testMaxPool3dValidPadding(self):\n    expected_output = [40.0, 41.0, 42.0]\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 3, 3, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"VALID\",\n        expected=expected_output)\n\n  def testMaxPool3dSamePadding(self):\n    expected_output = [31., 32., 33., 34., 35., 36.]\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 2, 2, 3, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"SAME\",\n        expected=expected_output)\n\n  def testMaxPool3dSamePaddingDifferentStrides(self):\n    expected_output = [2., 5., 8., 18., 21., 24., 34., 37., 40.]\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 5, 8, 1, 1],\n        window=(1, 2, 3),\n        strides=(2, 3, 1),\n        padding=\"SAME\",\n        expected=expected_output)\n\n    # Test pooling on a larger input, with different stride and kernel\n    # size for the 'z' dimension.\n\n    # Simulate max pooling in numpy to get the expected output.\n    input_data = np.arange(1, 5 * 27 * 27 * 64 + 1).reshape((5, 27, 27, 64))\n    input_data = np.pad(input_data, [[0, 0], [0, 1], [0, 1], [0, 0]],\n                        mode=\"constant\")\n    expected_output = input_data[:, 1::2, 1::2, :]\n    expected_output[:, -1, :, :] = input_data[:, -2, 1::2, :]\n    expected_output[:, :, -1, :] = input_data[:, 1::2, -2, :]\n    expected_output[:, -1, -1, :] = input_data[:, -2, -2, :]\n\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 5, 27, 27, 64],\n        window=(1, 2, 2),\n        strides=(1, 2, 2),\n        padding=\"SAME\",\n        expected=expected_output.flatten())\n\n  def testKernelSmallerThanStride(self):\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        window=[1, 1, 1],\n        strides=[2, 2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9, 19, 21, 25, 27])\n\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 7, 7, 7, 1],\n        window=[2, 2, 2],\n        strides=[3, 3, 3],\n        padding=\"VALID\",\n        expected=[58, 61, 79, 82, 205, 208, 226, 229])\n\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        window=[1, 1, 1],\n        strides=[2, 2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9, 19, 21, 25, 27])\n\n    self._VerifyValues(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 7, 7, 7, 1],\n        window=[2, 2, 2],\n        strides=[3, 3, 3],\n        padding=\"VALID\",\n        expected=[29.5, 32.5, 50.5, 53.5, 176.5, 179.5, 197.5, 200.5])\n\n  def testMaxPool3DEmptyTensorOutputShape(self):\n    \"\"\"Verifies the output shape of the max pooling function when tensor is empty.\n\n    Args: none\n    \"\"\"\n    input_sizes = [0, 112, 112, 112, 64]\n\n    input_data = 1.\n    input_tensor = constant_op.constant(\n        input_data, shape=input_sizes, name=\"input\")\n    max_pool_3d = nn_ops.max_pool3d(\n        input_tensor,\n        ksize=[2, 2, 2],\n        strides=[2, 2, 2],\n        padding=\"VALID\",\n        data_format=\"NDHWC\",\n        name=\"max_pool_3d\")\n    values = self.evaluate(max_pool_3d)\n    self.assertEqual(values.shape, (0, 56, 56, 56, 64))\n\n  def _ConstructAndTestGradientForConfig(self,\n                                         pool_func,\n                                         input_sizes,\n                                         output_sizes,\n                                         window,\n                                         strides,\n                                         padding,\n                                         data_format,\n                                         use_gpu):\n    \"\"\"Verifies the gradients of a pooling function.\n\n    Args:\n      pool_func: Function to be called, co.MaxPool, co.AvgPool,\n        or the Lua version.\n      input_sizes: Input tensor dimensions.\n      output_sizes: Output tensor dimensions.\n      window: Tuple of kernel dims: planes, rows, cols.\n      strides: Tuple of strides for dims: planes, rows, cols.\n      padding: Padding type.\n      data_format: Data format string.\n      use_gpu: Whether to run on GPU.\n    \"\"\"\n    total_size = 1\n    for s in input_sizes:\n      total_size *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x = np.arange(1, total_size + 1, dtype=np.float32)\n    with self.cached_session(use_gpu=use_gpu):\n      input_tensor = constant_op.constant(x, shape=input_sizes, name=\"input\")\n      err_g_margin = 1e-3\n      err_gg_margin = 1.5e-2\n      if pool_func == nn_ops.avg_pool3d:\n        func_name = \"avg_pool3d\"\n        x_init_value = None\n      else:\n        x_init_value = np.asfarray(np.arange(1, total_size + 1),\n                                   dtype=np.float32).reshape(input_sizes)\n        func_name = \"max_pool3d\"\n\n      ksize = [1, window[0], window[1], window[2], 1]\n      strides = [1, strides[0], strides[1], strides[2], 1]\n      t = input_tensor\n\n      if data_format == \"NCDHW\":\n        ksize = test_util.NHWCToNCHW(ksize)\n        strides = test_util.NHWCToNCHW(strides)\n        t = test_util.NHWCToNCHW(t)\n        output_sizes = test_util.NHWCToNCHW(output_sizes)\n\n      t = pool_func(\n          t,\n          ksize=ksize,\n          strides=strides,\n          padding=padding,\n          data_format=data_format,\n          name=func_name)\n      t_g = gradients_impl.gradients(t**2, input_tensor)[0]\n\n      err_g = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_sizes,\n          t,\n          output_sizes,\n          x_init_value=x_init_value,\n          delta=1e-2)\n      err_gg = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_sizes,\n          t_g,\n          input_sizes,\n          x_init_value=x_init_value,\n          delta=1e-2)\n\n    print(\"%s gradient error = \" % func_name, err_g)\n    self.assertLess(err_g, err_g_margin)\n    print(\"%s second-order gradient error = \" % func_name, err_gg)\n    self.assertLess(err_gg, err_gg_margin)\n\n  def _ConstructAndTestGradient(self,\n                                pool_func,\n                                **kwargs):\n    \"\"\"Runs _ConstructAndTestGradientForConfig for all tests configurations.\"\"\"\n\n    for data_format, use_gpu in GetTestConfigs():\n      self._ConstructAndTestGradientForConfig(pool_func,\n                                              data_format=data_format,\n                                              use_gpu=use_gpu,\n                                              **kwargs)\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding1_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        output_sizes=[1, 3, 3, 3, 1],\n        window=(1, 1, 1),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding2_1_6_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 2, 3, 4, 2],\n        output_sizes=[1, 1, 2, 3, 2],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding2_1_7_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 2, 7, 1],\n        output_sizes=[1, 2, 1, 6, 1],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding1_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        output_sizes=[1, 2, 2, 2, 1],\n        window=(1, 1, 1),\n        strides=(2, 2, 2),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradValidPadding2_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[2, 2, 2, 2, 1],\n        output_sizes=[2, 1, 1, 1, 1],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding1_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 2, 4, 1],\n        output_sizes=[1, 3, 2, 4, 1],\n        window=(1, 1, 1),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding1_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 2, 4, 1],\n        output_sizes=[1, 2, 1, 2, 1],\n        window=(1, 1, 1),\n        strides=(2, 2, 2),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding2_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 2, 4, 1],\n        output_sizes=[1, 3, 2, 4, 1],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding2_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 5, 2, 4, 2],\n        output_sizes=[1, 3, 1, 2, 2],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradSamePadding3_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 4, 2, 1],\n        output_sizes=[1, 3, 4, 2, 1],\n        window=(3, 3, 3),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradValidPadding1_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        output_sizes=[1, 3, 3, 3, 1],\n        window=(1, 1, 1),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradValidPadding1_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 1],\n        output_sizes=[1, 2, 2, 2, 1],\n        window=(1, 1, 1),\n        strides=(2, 2, 2),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradValidPadding2_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 3, 3, 2],\n        output_sizes=[1, 2, 2, 2, 2],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradValidPadding2_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[2, 2, 2, 2, 2],\n        output_sizes=[2, 1, 1, 1, 2],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding1_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 2, 4, 2],\n        output_sizes=[1, 3, 2, 4, 2],\n        window=(1, 1, 1),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding1_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 2, 4, 2],\n        output_sizes=[1, 2, 1, 2, 2],\n        window=(1, 1, 1),\n        strides=(2, 2, 2),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding2_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 2, 2, 2, 1],\n        output_sizes=[1, 2, 2, 2, 1],\n        window=(2, 2, 2),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding2_2_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 5, 2, 4, 1],\n        output_sizes=[1, 3, 1, 2, 1],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGradSamePadding3_1_3d(self):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool3d,\n        input_sizes=[1, 3, 6, 2, 1],\n        output_sizes=[1, 3, 6, 2, 1],\n        window=(3, 3, 3),\n        strides=(1, 1, 1),\n        padding=\"SAME\")\n\n  def testMaxPool3DZeroPoolSize(self):\n    # Test case for GitHub issue 51936.\n    for f in [nn_ops.max_pool3d, nn_ops.avg_pool3d]:\n      with self.session():\n        with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n          input_sizes = [3, 4, 10, 11, 12]\n\n          input_data = 1.\n          input_tensor = constant_op.constant(\n              input_data, shape=input_sizes, name=\"input\")\n          pool_3d = f(input_tensor, ksize=[2, 2, 0], strides=1, padding=\"VALID\")\n          self.evaluate(pool_3d)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/pooling_ops_3d.cc", "tensorflow/python/kernel_tests/pooling_ops_3d_test.py"], "buggy_code_start_loc": [143, 19], "buggy_code_end_loc": [143, 503], "fixing_code_start_loc": [144, 20], "fixing_code_end_loc": [149, 518], "type": "CWE-191", "message": "TensorFlow is an open source platform for machine learning. In affected versions the Keras pooling layers can trigger a segfault if the size of the pool is 0 or if a dimension is negative. This is due to the TensorFlow's implementation of pooling operations where the values in the sliding window are not checked to be strictly positive. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41196", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T20:15:07.780", "lastModified": "2021-11-09T13:56:01.853", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions the Keras pooling layers can trigger a segfault if the size of the pool is 0 or if a dimension is negative. This is due to the TensorFlow's implementation of pooling operations where the values in the sliding window are not checked to be strictly positive. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas, las capas de pooling de Keras pueden provocar un segfault si el tama\u00f1o del pool es 0 o si una dimensi\u00f3n es negativa. Esto es debido a la implementaci\u00f3n de TensorFlow de las operaciones de pooling donde no es comprobado que los valores de la ventana deslizante sean estrictamente positivos. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n ser\u00e1 incluida este commit en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-191"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-191"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.4.4", "matchCriteriaId": "455FB550-4C9C-4BD6-9F76-A627B62AB332"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndExcluding": "2.6.1", "matchCriteriaId": "5D68D8D1-DB27-4395-9D3D-2BED901B852C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/12b1ff82b3f26ff8de17e58703231d5a02ef1b8b", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/issues/51936", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-m539-j985-hcr8", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/12b1ff82b3f26ff8de17e58703231d5a02ef1b8b"}}