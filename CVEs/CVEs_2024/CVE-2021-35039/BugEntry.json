{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Copyright (C) 2002 Richard Henderson\n * Copyright (C) 2001 Rusty Russell, 2002, 2010 Rusty Russell IBM.\n */\n\n#define INCLUDE_VERMAGIC\n\n#include <linux/export.h>\n#include <linux/extable.h>\n#include <linux/moduleloader.h>\n#include <linux/module_signature.h>\n#include <linux/trace_events.h>\n#include <linux/init.h>\n#include <linux/kallsyms.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/sysfs.h>\n#include <linux/kernel.h>\n#include <linux/kernel_read_file.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/elf.h>\n#include <linux/proc_fs.h>\n#include <linux/security.h>\n#include <linux/seq_file.h>\n#include <linux/syscalls.h>\n#include <linux/fcntl.h>\n#include <linux/rcupdate.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/moduleparam.h>\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/vermagic.h>\n#include <linux/notifier.h>\n#include <linux/sched.h>\n#include <linux/device.h>\n#include <linux/string.h>\n#include <linux/mutex.h>\n#include <linux/rculist.h>\n#include <linux/uaccess.h>\n#include <asm/cacheflush.h>\n#include <linux/set_memory.h>\n#include <asm/mmu_context.h>\n#include <linux/license.h>\n#include <asm/sections.h>\n#include <linux/tracepoint.h>\n#include <linux/ftrace.h>\n#include <linux/livepatch.h>\n#include <linux/async.h>\n#include <linux/percpu.h>\n#include <linux/kmemleak.h>\n#include <linux/jump_label.h>\n#include <linux/pfn.h>\n#include <linux/bsearch.h>\n#include <linux/dynamic_debug.h>\n#include <linux/audit.h>\n#include <uapi/linux/module.h>\n#include \"module-internal.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/module.h>\n\n#ifndef ARCH_SHF_SMALL\n#define ARCH_SHF_SMALL 0\n#endif\n\n/*\n * Modules' sections will be aligned on page boundaries\n * to ensure complete separation of code and data, but\n * only when CONFIG_ARCH_HAS_STRICT_MODULE_RWX=y\n */\n#ifdef CONFIG_ARCH_HAS_STRICT_MODULE_RWX\n# define debug_align(X) ALIGN(X, PAGE_SIZE)\n#else\n# define debug_align(X) (X)\n#endif\n\n/* If this is set, the section belongs in the init part of the module */\n#define INIT_OFFSET_MASK (1UL << (BITS_PER_LONG-1))\n\n/*\n * Mutex protects:\n * 1) List of modules (also safely readable with preempt_disable),\n * 2) module_use links,\n * 3) module_addr_min/module_addr_max.\n * (delete and add uses RCU list operations).\n */\nstatic DEFINE_MUTEX(module_mutex);\nstatic LIST_HEAD(modules);\n\n/* Work queue for freeing init sections in success case */\nstatic void do_free_init(struct work_struct *w);\nstatic DECLARE_WORK(init_free_wq, do_free_init);\nstatic LLIST_HEAD(init_free_list);\n\n#ifdef CONFIG_MODULES_TREE_LOOKUP\n\n/*\n * Use a latched RB-tree for __module_address(); this allows us to use\n * RCU-sched lookups of the address from any context.\n *\n * This is conditional on PERF_EVENTS || TRACING because those can really hit\n * __module_address() hard by doing a lot of stack unwinding; potentially from\n * NMI context.\n */\n\nstatic __always_inline unsigned long __mod_tree_val(struct latch_tree_node *n)\n{\n\tstruct module_layout *layout = container_of(n, struct module_layout, mtn.node);\n\n\treturn (unsigned long)layout->base;\n}\n\nstatic __always_inline unsigned long __mod_tree_size(struct latch_tree_node *n)\n{\n\tstruct module_layout *layout = container_of(n, struct module_layout, mtn.node);\n\n\treturn (unsigned long)layout->size;\n}\n\nstatic __always_inline bool\nmod_tree_less(struct latch_tree_node *a, struct latch_tree_node *b)\n{\n\treturn __mod_tree_val(a) < __mod_tree_val(b);\n}\n\nstatic __always_inline int\nmod_tree_comp(void *key, struct latch_tree_node *n)\n{\n\tunsigned long val = (unsigned long)key;\n\tunsigned long start, end;\n\n\tstart = __mod_tree_val(n);\n\tif (val < start)\n\t\treturn -1;\n\n\tend = start + __mod_tree_size(n);\n\tif (val >= end)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic const struct latch_tree_ops mod_tree_ops = {\n\t.less = mod_tree_less,\n\t.comp = mod_tree_comp,\n};\n\nstatic struct mod_tree_root {\n\tstruct latch_tree_root root;\n\tunsigned long addr_min;\n\tunsigned long addr_max;\n} mod_tree __cacheline_aligned = {\n\t.addr_min = -1UL,\n};\n\n#define module_addr_min mod_tree.addr_min\n#define module_addr_max mod_tree.addr_max\n\nstatic noinline void __mod_tree_insert(struct mod_tree_node *node)\n{\n\tlatch_tree_insert(&node->node, &mod_tree.root, &mod_tree_ops);\n}\n\nstatic void __mod_tree_remove(struct mod_tree_node *node)\n{\n\tlatch_tree_erase(&node->node, &mod_tree.root, &mod_tree_ops);\n}\n\n/*\n * These modifications: insert, remove_init and remove; are serialized by the\n * module_mutex.\n */\nstatic void mod_tree_insert(struct module *mod)\n{\n\tmod->core_layout.mtn.mod = mod;\n\tmod->init_layout.mtn.mod = mod;\n\n\t__mod_tree_insert(&mod->core_layout.mtn);\n\tif (mod->init_layout.size)\n\t\t__mod_tree_insert(&mod->init_layout.mtn);\n}\n\nstatic void mod_tree_remove_init(struct module *mod)\n{\n\tif (mod->init_layout.size)\n\t\t__mod_tree_remove(&mod->init_layout.mtn);\n}\n\nstatic void mod_tree_remove(struct module *mod)\n{\n\t__mod_tree_remove(&mod->core_layout.mtn);\n\tmod_tree_remove_init(mod);\n}\n\nstatic struct module *mod_find(unsigned long addr)\n{\n\tstruct latch_tree_node *ltn;\n\n\tltn = latch_tree_find((void *)addr, &mod_tree.root, &mod_tree_ops);\n\tif (!ltn)\n\t\treturn NULL;\n\n\treturn container_of(ltn, struct mod_tree_node, node)->mod;\n}\n\n#else /* MODULES_TREE_LOOKUP */\n\nstatic unsigned long module_addr_min = -1UL, module_addr_max = 0;\n\nstatic void mod_tree_insert(struct module *mod) { }\nstatic void mod_tree_remove_init(struct module *mod) { }\nstatic void mod_tree_remove(struct module *mod) { }\n\nstatic struct module *mod_find(unsigned long addr)\n{\n\tstruct module *mod;\n\n\tlist_for_each_entry_rcu(mod, &modules, list,\n\t\t\t\tlockdep_is_held(&module_mutex)) {\n\t\tif (within_module(addr, mod))\n\t\t\treturn mod;\n\t}\n\n\treturn NULL;\n}\n\n#endif /* MODULES_TREE_LOOKUP */\n\n/*\n * Bounds of module text, for speeding up __module_address.\n * Protected by module_mutex.\n */\nstatic void __mod_update_bounds(void *base, unsigned int size)\n{\n\tunsigned long min = (unsigned long)base;\n\tunsigned long max = min + size;\n\n\tif (min < module_addr_min)\n\t\tmodule_addr_min = min;\n\tif (max > module_addr_max)\n\t\tmodule_addr_max = max;\n}\n\nstatic void mod_update_bounds(struct module *mod)\n{\n\t__mod_update_bounds(mod->core_layout.base, mod->core_layout.size);\n\tif (mod->init_layout.size)\n\t\t__mod_update_bounds(mod->init_layout.base, mod->init_layout.size);\n}\n\n#ifdef CONFIG_KGDB_KDB\nstruct list_head *kdb_modules = &modules; /* kdb needs the list of modules */\n#endif /* CONFIG_KGDB_KDB */\n\nstatic void module_assert_mutex_or_preempt(void)\n{\n#ifdef CONFIG_LOCKDEP\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\tWARN_ON_ONCE(!rcu_read_lock_sched_held() &&\n\t\t!lockdep_is_held(&module_mutex));\n#endif\n}\n\nstatic bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);\nmodule_param(sig_enforce, bool_enable_only, 0644);\n\n/*\n * Export sig_enforce kernel cmdline parameter to allow other subsystems rely\n * on that instead of directly to CONFIG_MODULE_SIG_FORCE config.\n */\nbool is_module_sig_enforced(void)\n{\n\treturn sig_enforce;\n}\nEXPORT_SYMBOL(is_module_sig_enforced);\n\nvoid set_module_sig_enforced(void)\n{\n\tsig_enforce = true;\n}\n\n/* Block module loading/unloading? */\nint modules_disabled = 0;\ncore_param(nomodule, modules_disabled, bint, 0);\n\n/* Waiting for a module to finish initializing? */\nstatic DECLARE_WAIT_QUEUE_HEAD(module_wq);\n\nstatic BLOCKING_NOTIFIER_HEAD(module_notify_list);\n\nint register_module_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&module_notify_list, nb);\n}\nEXPORT_SYMBOL(register_module_notifier);\n\nint unregister_module_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&module_notify_list, nb);\n}\nEXPORT_SYMBOL(unregister_module_notifier);\n\n/*\n * We require a truly strong try_module_get(): 0 means success.\n * Otherwise an error is returned due to ongoing or failed\n * initialization etc.\n */\nstatic inline int strong_try_module_get(struct module *mod)\n{\n\tBUG_ON(mod && mod->state == MODULE_STATE_UNFORMED);\n\tif (mod && mod->state == MODULE_STATE_COMING)\n\t\treturn -EBUSY;\n\tif (try_module_get(mod))\n\t\treturn 0;\n\telse\n\t\treturn -ENOENT;\n}\n\nstatic inline void add_taint_module(struct module *mod, unsigned flag,\n\t\t\t\t    enum lockdep_ok lockdep_ok)\n{\n\tadd_taint(flag, lockdep_ok);\n\tset_bit(flag, &mod->taints);\n}\n\n/*\n * A thread that wants to hold a reference to a module only while it\n * is running can call this to safely exit.  nfsd and lockd use this.\n */\nvoid __noreturn __module_put_and_exit(struct module *mod, long code)\n{\n\tmodule_put(mod);\n\tdo_exit(code);\n}\nEXPORT_SYMBOL(__module_put_and_exit);\n\n/* Find a module section: 0 means not found. */\nstatic unsigned int find_sec(const struct load_info *info, const char *name)\n{\n\tunsigned int i;\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tElf_Shdr *shdr = &info->sechdrs[i];\n\t\t/* Alloc bit cleared means \"ignore it.\" */\n\t\tif ((shdr->sh_flags & SHF_ALLOC)\n\t\t    && strcmp(info->secstrings + shdr->sh_name, name) == 0)\n\t\t\treturn i;\n\t}\n\treturn 0;\n}\n\n/* Find a module section, or NULL. */\nstatic void *section_addr(const struct load_info *info, const char *name)\n{\n\t/* Section 0 has sh_addr 0. */\n\treturn (void *)info->sechdrs[find_sec(info, name)].sh_addr;\n}\n\n/* Find a module section, or NULL.  Fill in number of \"objects\" in section. */\nstatic void *section_objs(const struct load_info *info,\n\t\t\t  const char *name,\n\t\t\t  size_t object_size,\n\t\t\t  unsigned int *num)\n{\n\tunsigned int sec = find_sec(info, name);\n\n\t/* Section 0 has sh_addr 0 and sh_size 0. */\n\t*num = info->sechdrs[sec].sh_size / object_size;\n\treturn (void *)info->sechdrs[sec].sh_addr;\n}\n\n/* Find a module section: 0 means not found. Ignores SHF_ALLOC flag. */\nstatic unsigned int find_any_sec(const struct load_info *info, const char *name)\n{\n\tunsigned int i;\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tElf_Shdr *shdr = &info->sechdrs[i];\n\t\tif (strcmp(info->secstrings + shdr->sh_name, name) == 0)\n\t\t\treturn i;\n\t}\n\treturn 0;\n}\n\n/*\n * Find a module section, or NULL. Fill in number of \"objects\" in section.\n * Ignores SHF_ALLOC flag.\n */\nstatic __maybe_unused void *any_section_objs(const struct load_info *info,\n\t\t\t\t\t     const char *name,\n\t\t\t\t\t     size_t object_size,\n\t\t\t\t\t     unsigned int *num)\n{\n\tunsigned int sec = find_any_sec(info, name);\n\n\t/* Section 0 has sh_addr 0 and sh_size 0. */\n\t*num = info->sechdrs[sec].sh_size / object_size;\n\treturn (void *)info->sechdrs[sec].sh_addr;\n}\n\n/* Provided by the linker */\nextern const struct kernel_symbol __start___ksymtab[];\nextern const struct kernel_symbol __stop___ksymtab[];\nextern const struct kernel_symbol __start___ksymtab_gpl[];\nextern const struct kernel_symbol __stop___ksymtab_gpl[];\nextern const s32 __start___kcrctab[];\nextern const s32 __start___kcrctab_gpl[];\n\n#ifndef CONFIG_MODVERSIONS\n#define symversion(base, idx) NULL\n#else\n#define symversion(base, idx) ((base != NULL) ? ((base) + (idx)) : NULL)\n#endif\n\nstruct symsearch {\n\tconst struct kernel_symbol *start, *stop;\n\tconst s32 *crcs;\n\tenum mod_license {\n\t\tNOT_GPL_ONLY,\n\t\tGPL_ONLY,\n\t} license;\n};\n\nstruct find_symbol_arg {\n\t/* Input */\n\tconst char *name;\n\tbool gplok;\n\tbool warn;\n\n\t/* Output */\n\tstruct module *owner;\n\tconst s32 *crc;\n\tconst struct kernel_symbol *sym;\n\tenum mod_license license;\n};\n\nstatic bool check_exported_symbol(const struct symsearch *syms,\n\t\t\t\t  struct module *owner,\n\t\t\t\t  unsigned int symnum, void *data)\n{\n\tstruct find_symbol_arg *fsa = data;\n\n\tif (!fsa->gplok && syms->license == GPL_ONLY)\n\t\treturn false;\n\tfsa->owner = owner;\n\tfsa->crc = symversion(syms->crcs, symnum);\n\tfsa->sym = &syms->start[symnum];\n\tfsa->license = syms->license;\n\treturn true;\n}\n\nstatic unsigned long kernel_symbol_value(const struct kernel_symbol *sym)\n{\n#ifdef CONFIG_HAVE_ARCH_PREL32_RELOCATIONS\n\treturn (unsigned long)offset_to_ptr(&sym->value_offset);\n#else\n\treturn sym->value;\n#endif\n}\n\nstatic const char *kernel_symbol_name(const struct kernel_symbol *sym)\n{\n#ifdef CONFIG_HAVE_ARCH_PREL32_RELOCATIONS\n\treturn offset_to_ptr(&sym->name_offset);\n#else\n\treturn sym->name;\n#endif\n}\n\nstatic const char *kernel_symbol_namespace(const struct kernel_symbol *sym)\n{\n#ifdef CONFIG_HAVE_ARCH_PREL32_RELOCATIONS\n\tif (!sym->namespace_offset)\n\t\treturn NULL;\n\treturn offset_to_ptr(&sym->namespace_offset);\n#else\n\treturn sym->namespace;\n#endif\n}\n\nstatic int cmp_name(const void *name, const void *sym)\n{\n\treturn strcmp(name, kernel_symbol_name(sym));\n}\n\nstatic bool find_exported_symbol_in_section(const struct symsearch *syms,\n\t\t\t\t\t    struct module *owner,\n\t\t\t\t\t    void *data)\n{\n\tstruct find_symbol_arg *fsa = data;\n\tstruct kernel_symbol *sym;\n\n\tsym = bsearch(fsa->name, syms->start, syms->stop - syms->start,\n\t\t\tsizeof(struct kernel_symbol), cmp_name);\n\n\tif (sym != NULL && check_exported_symbol(syms, owner,\n\t\t\t\t\t\t sym - syms->start, data))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Find an exported symbol and return it, along with, (optional) crc and\n * (optional) module which owns it.  Needs preempt disabled or module_mutex.\n */\nstatic bool find_symbol(struct find_symbol_arg *fsa)\n{\n\tstatic const struct symsearch arr[] = {\n\t\t{ __start___ksymtab, __stop___ksymtab, __start___kcrctab,\n\t\t  NOT_GPL_ONLY },\n\t\t{ __start___ksymtab_gpl, __stop___ksymtab_gpl,\n\t\t  __start___kcrctab_gpl,\n\t\t  GPL_ONLY },\n\t};\n\tstruct module *mod;\n\tunsigned int i;\n\n\tmodule_assert_mutex_or_preempt();\n\n\tfor (i = 0; i < ARRAY_SIZE(arr); i++)\n\t\tif (find_exported_symbol_in_section(&arr[i], NULL, fsa))\n\t\t\treturn true;\n\n\tlist_for_each_entry_rcu(mod, &modules, list,\n\t\t\t\tlockdep_is_held(&module_mutex)) {\n\t\tstruct symsearch arr[] = {\n\t\t\t{ mod->syms, mod->syms + mod->num_syms, mod->crcs,\n\t\t\t  NOT_GPL_ONLY },\n\t\t\t{ mod->gpl_syms, mod->gpl_syms + mod->num_gpl_syms,\n\t\t\t  mod->gpl_crcs,\n\t\t\t  GPL_ONLY },\n\t\t};\n\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(arr); i++)\n\t\t\tif (find_exported_symbol_in_section(&arr[i], mod, fsa))\n\t\t\t\treturn true;\n\t}\n\n\tpr_debug(\"Failed to find symbol %s\\n\", fsa->name);\n\treturn false;\n}\n\n/*\n * Search for module by name: must hold module_mutex (or preempt disabled\n * for read-only access).\n */\nstatic struct module *find_module_all(const char *name, size_t len,\n\t\t\t\t      bool even_unformed)\n{\n\tstruct module *mod;\n\n\tmodule_assert_mutex_or_preempt();\n\n\tlist_for_each_entry_rcu(mod, &modules, list,\n\t\t\t\tlockdep_is_held(&module_mutex)) {\n\t\tif (!even_unformed && mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tif (strlen(mod->name) == len && !memcmp(mod->name, name, len))\n\t\t\treturn mod;\n\t}\n\treturn NULL;\n}\n\nstruct module *find_module(const char *name)\n{\n\treturn find_module_all(name, strlen(name), false);\n}\n\n#ifdef CONFIG_SMP\n\nstatic inline void __percpu *mod_percpu(struct module *mod)\n{\n\treturn mod->percpu;\n}\n\nstatic int percpu_modalloc(struct module *mod, struct load_info *info)\n{\n\tElf_Shdr *pcpusec = &info->sechdrs[info->index.pcpu];\n\tunsigned long align = pcpusec->sh_addralign;\n\n\tif (!pcpusec->sh_size)\n\t\treturn 0;\n\n\tif (align > PAGE_SIZE) {\n\t\tpr_warn(\"%s: per-cpu alignment %li > %li\\n\",\n\t\t\tmod->name, align, PAGE_SIZE);\n\t\talign = PAGE_SIZE;\n\t}\n\n\tmod->percpu = __alloc_reserved_percpu(pcpusec->sh_size, align);\n\tif (!mod->percpu) {\n\t\tpr_warn(\"%s: Could not allocate %lu bytes percpu data\\n\",\n\t\t\tmod->name, (unsigned long)pcpusec->sh_size);\n\t\treturn -ENOMEM;\n\t}\n\tmod->percpu_size = pcpusec->sh_size;\n\treturn 0;\n}\n\nstatic void percpu_modfree(struct module *mod)\n{\n\tfree_percpu(mod->percpu);\n}\n\nstatic unsigned int find_pcpusec(struct load_info *info)\n{\n\treturn find_sec(info, \".data..percpu\");\n}\n\nstatic void percpu_modcopy(struct module *mod,\n\t\t\t   const void *from, unsigned long size)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tmemcpy(per_cpu_ptr(mod->percpu, cpu), from, size);\n}\n\nbool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)\n{\n\tstruct module *mod;\n\tunsigned int cpu;\n\n\tpreempt_disable();\n\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tif (!mod->percpu_size)\n\t\t\tcontinue;\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tvoid *start = per_cpu_ptr(mod->percpu, cpu);\n\t\t\tvoid *va = (void *)addr;\n\n\t\t\tif (va >= start && va < start + mod->percpu_size) {\n\t\t\t\tif (can_addr) {\n\t\t\t\t\t*can_addr = (unsigned long) (va - start);\n\t\t\t\t\t*can_addr += (unsigned long)\n\t\t\t\t\t\tper_cpu_ptr(mod->percpu,\n\t\t\t\t\t\t\t    get_boot_cpu_id());\n\t\t\t\t}\n\t\t\t\tpreempt_enable();\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\n\tpreempt_enable();\n\treturn false;\n}\n\n/**\n * is_module_percpu_address() - test whether address is from module static percpu\n * @addr: address to test\n *\n * Test whether @addr belongs to module static percpu area.\n *\n * Return: %true if @addr is from module static percpu area\n */\nbool is_module_percpu_address(unsigned long addr)\n{\n\treturn __is_module_percpu_address(addr, NULL);\n}\n\n#else /* ... !CONFIG_SMP */\n\nstatic inline void __percpu *mod_percpu(struct module *mod)\n{\n\treturn NULL;\n}\nstatic int percpu_modalloc(struct module *mod, struct load_info *info)\n{\n\t/* UP modules shouldn't have this section: ENOMEM isn't quite right */\n\tif (info->sechdrs[info->index.pcpu].sh_size != 0)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nstatic inline void percpu_modfree(struct module *mod)\n{\n}\nstatic unsigned int find_pcpusec(struct load_info *info)\n{\n\treturn 0;\n}\nstatic inline void percpu_modcopy(struct module *mod,\n\t\t\t\t  const void *from, unsigned long size)\n{\n\t/* pcpusec should be 0, and size of that section should be 0. */\n\tBUG_ON(size != 0);\n}\nbool is_module_percpu_address(unsigned long addr)\n{\n\treturn false;\n}\n\nbool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)\n{\n\treturn false;\n}\n\n#endif /* CONFIG_SMP */\n\n#define MODINFO_ATTR(field)\t\\\nstatic void setup_modinfo_##field(struct module *mod, const char *s)  \\\n{                                                                     \\\n\tmod->field = kstrdup(s, GFP_KERNEL);                          \\\n}                                                                     \\\nstatic ssize_t show_modinfo_##field(struct module_attribute *mattr,   \\\n\t\t\tstruct module_kobject *mk, char *buffer)      \\\n{                                                                     \\\n\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", mk->mod->field);  \\\n}                                                                     \\\nstatic int modinfo_##field##_exists(struct module *mod)               \\\n{                                                                     \\\n\treturn mod->field != NULL;                                    \\\n}                                                                     \\\nstatic void free_modinfo_##field(struct module *mod)                  \\\n{                                                                     \\\n\tkfree(mod->field);                                            \\\n\tmod->field = NULL;                                            \\\n}                                                                     \\\nstatic struct module_attribute modinfo_##field = {                    \\\n\t.attr = { .name = __stringify(field), .mode = 0444 },         \\\n\t.show = show_modinfo_##field,                                 \\\n\t.setup = setup_modinfo_##field,                               \\\n\t.test = modinfo_##field##_exists,                             \\\n\t.free = free_modinfo_##field,                                 \\\n};\n\nMODINFO_ATTR(version);\nMODINFO_ATTR(srcversion);\n\nstatic char last_unloaded_module[MODULE_NAME_LEN+1];\n\n#ifdef CONFIG_MODULE_UNLOAD\n\nEXPORT_TRACEPOINT_SYMBOL(module_get);\n\n/* MODULE_REF_BASE is the base reference count by kmodule loader. */\n#define MODULE_REF_BASE\t1\n\n/* Init the unload section of the module. */\nstatic int module_unload_init(struct module *mod)\n{\n\t/*\n\t * Initialize reference counter to MODULE_REF_BASE.\n\t * refcnt == 0 means module is going.\n\t */\n\tatomic_set(&mod->refcnt, MODULE_REF_BASE);\n\n\tINIT_LIST_HEAD(&mod->source_list);\n\tINIT_LIST_HEAD(&mod->target_list);\n\n\t/* Hold reference count during initialization. */\n\tatomic_inc(&mod->refcnt);\n\n\treturn 0;\n}\n\n/* Does a already use b? */\nstatic int already_uses(struct module *a, struct module *b)\n{\n\tstruct module_use *use;\n\n\tlist_for_each_entry(use, &b->source_list, source_list) {\n\t\tif (use->source == a) {\n\t\t\tpr_debug(\"%s uses %s!\\n\", a->name, b->name);\n\t\t\treturn 1;\n\t\t}\n\t}\n\tpr_debug(\"%s does not use %s!\\n\", a->name, b->name);\n\treturn 0;\n}\n\n/*\n * Module a uses b\n *  - we add 'a' as a \"source\", 'b' as a \"target\" of module use\n *  - the module_use is added to the list of 'b' sources (so\n *    'b' can walk the list to see who sourced them), and of 'a'\n *    targets (so 'a' can see what modules it targets).\n */\nstatic int add_module_usage(struct module *a, struct module *b)\n{\n\tstruct module_use *use;\n\n\tpr_debug(\"Allocating new usage for %s.\\n\", a->name);\n\tuse = kmalloc(sizeof(*use), GFP_ATOMIC);\n\tif (!use)\n\t\treturn -ENOMEM;\n\n\tuse->source = a;\n\tuse->target = b;\n\tlist_add(&use->source_list, &b->source_list);\n\tlist_add(&use->target_list, &a->target_list);\n\treturn 0;\n}\n\n/* Module a uses b: caller needs module_mutex() */\nstatic int ref_module(struct module *a, struct module *b)\n{\n\tint err;\n\n\tif (b == NULL || already_uses(a, b))\n\t\treturn 0;\n\n\t/* If module isn't available, we fail. */\n\terr = strong_try_module_get(b);\n\tif (err)\n\t\treturn err;\n\n\terr = add_module_usage(a, b);\n\tif (err) {\n\t\tmodule_put(b);\n\t\treturn err;\n\t}\n\treturn 0;\n}\n\n/* Clear the unload stuff of the module. */\nstatic void module_unload_free(struct module *mod)\n{\n\tstruct module_use *use, *tmp;\n\n\tmutex_lock(&module_mutex);\n\tlist_for_each_entry_safe(use, tmp, &mod->target_list, target_list) {\n\t\tstruct module *i = use->target;\n\t\tpr_debug(\"%s unusing %s\\n\", mod->name, i->name);\n\t\tmodule_put(i);\n\t\tlist_del(&use->source_list);\n\t\tlist_del(&use->target_list);\n\t\tkfree(use);\n\t}\n\tmutex_unlock(&module_mutex);\n}\n\n#ifdef CONFIG_MODULE_FORCE_UNLOAD\nstatic inline int try_force_unload(unsigned int flags)\n{\n\tint ret = (flags & O_TRUNC);\n\tif (ret)\n\t\tadd_taint(TAINT_FORCED_RMMOD, LOCKDEP_NOW_UNRELIABLE);\n\treturn ret;\n}\n#else\nstatic inline int try_force_unload(unsigned int flags)\n{\n\treturn 0;\n}\n#endif /* CONFIG_MODULE_FORCE_UNLOAD */\n\n/* Try to release refcount of module, 0 means success. */\nstatic int try_release_module_ref(struct module *mod)\n{\n\tint ret;\n\n\t/* Try to decrement refcnt which we set at loading */\n\tret = atomic_sub_return(MODULE_REF_BASE, &mod->refcnt);\n\tBUG_ON(ret < 0);\n\tif (ret)\n\t\t/* Someone can put this right now, recover with checking */\n\t\tret = atomic_add_unless(&mod->refcnt, MODULE_REF_BASE, 0);\n\n\treturn ret;\n}\n\nstatic int try_stop_module(struct module *mod, int flags, int *forced)\n{\n\t/* If it's not unused, quit unless we're forcing. */\n\tif (try_release_module_ref(mod) != 0) {\n\t\t*forced = try_force_unload(flags);\n\t\tif (!(*forced))\n\t\t\treturn -EWOULDBLOCK;\n\t}\n\n\t/* Mark it as dying. */\n\tmod->state = MODULE_STATE_GOING;\n\n\treturn 0;\n}\n\n/**\n * module_refcount() - return the refcount or -1 if unloading\n * @mod:\tthe module we're checking\n *\n * Return:\n *\t-1 if the module is in the process of unloading\n *\totherwise the number of references in the kernel to the module\n */\nint module_refcount(struct module *mod)\n{\n\treturn atomic_read(&mod->refcnt) - MODULE_REF_BASE;\n}\nEXPORT_SYMBOL(module_refcount);\n\n/* This exists whether we can unload or not */\nstatic void free_module(struct module *mod);\n\nSYSCALL_DEFINE2(delete_module, const char __user *, name_user,\n\t\tunsigned int, flags)\n{\n\tstruct module *mod;\n\tchar name[MODULE_NAME_LEN];\n\tint ret, forced = 0;\n\n\tif (!capable(CAP_SYS_MODULE) || modules_disabled)\n\t\treturn -EPERM;\n\n\tif (strncpy_from_user(name, name_user, MODULE_NAME_LEN-1) < 0)\n\t\treturn -EFAULT;\n\tname[MODULE_NAME_LEN-1] = '\\0';\n\n\taudit_log_kern_module(name);\n\n\tif (mutex_lock_interruptible(&module_mutex) != 0)\n\t\treturn -EINTR;\n\n\tmod = find_module(name);\n\tif (!mod) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tif (!list_empty(&mod->source_list)) {\n\t\t/* Other modules depend on us: get rid of them first. */\n\t\tret = -EWOULDBLOCK;\n\t\tgoto out;\n\t}\n\n\t/* Doing init or already dying? */\n\tif (mod->state != MODULE_STATE_LIVE) {\n\t\t/* FIXME: if (force), slam module count damn the torpedoes */\n\t\tpr_debug(\"%s already dying\\n\", mod->name);\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* If it has an init func, it must have an exit func to unload */\n\tif (mod->init && !mod->exit) {\n\t\tforced = try_force_unload(flags);\n\t\tif (!forced) {\n\t\t\t/* This module can't be removed */\n\t\t\tret = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Stop the machine so refcounts can't move and disable module. */\n\tret = try_stop_module(mod, flags, &forced);\n\tif (ret != 0)\n\t\tgoto out;\n\n\tmutex_unlock(&module_mutex);\n\t/* Final destruction now no one is using it. */\n\tif (mod->exit != NULL)\n\t\tmod->exit();\n\tblocking_notifier_call_chain(&module_notify_list,\n\t\t\t\t     MODULE_STATE_GOING, mod);\n\tklp_module_going(mod);\n\tftrace_release_mod(mod);\n\n\tasync_synchronize_full();\n\n\t/* Store the name of the last unloaded module for diagnostic purposes */\n\tstrlcpy(last_unloaded_module, mod->name, sizeof(last_unloaded_module));\n\n\tfree_module(mod);\n\t/* someone could wait for the module in add_unformed_module() */\n\twake_up_all(&module_wq);\n\treturn 0;\nout:\n\tmutex_unlock(&module_mutex);\n\treturn ret;\n}\n\nstatic inline void print_unload_info(struct seq_file *m, struct module *mod)\n{\n\tstruct module_use *use;\n\tint printed_something = 0;\n\n\tseq_printf(m, \" %i \", module_refcount(mod));\n\n\t/*\n\t * Always include a trailing , so userspace can differentiate\n\t * between this and the old multi-field proc format.\n\t */\n\tlist_for_each_entry(use, &mod->source_list, source_list) {\n\t\tprinted_something = 1;\n\t\tseq_printf(m, \"%s,\", use->source->name);\n\t}\n\n\tif (mod->init != NULL && mod->exit == NULL) {\n\t\tprinted_something = 1;\n\t\tseq_puts(m, \"[permanent],\");\n\t}\n\n\tif (!printed_something)\n\t\tseq_puts(m, \"-\");\n}\n\nvoid __symbol_put(const char *symbol)\n{\n\tstruct find_symbol_arg fsa = {\n\t\t.name\t= symbol,\n\t\t.gplok\t= true,\n\t};\n\n\tpreempt_disable();\n\tif (!find_symbol(&fsa))\n\t\tBUG();\n\tmodule_put(fsa.owner);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL(__symbol_put);\n\n/* Note this assumes addr is a function, which it currently always is. */\nvoid symbol_put_addr(void *addr)\n{\n\tstruct module *modaddr;\n\tunsigned long a = (unsigned long)dereference_function_descriptor(addr);\n\n\tif (core_kernel_text(a))\n\t\treturn;\n\n\t/*\n\t * Even though we hold a reference on the module; we still need to\n\t * disable preemption in order to safely traverse the data structure.\n\t */\n\tpreempt_disable();\n\tmodaddr = __module_text_address(a);\n\tBUG_ON(!modaddr);\n\tmodule_put(modaddr);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(symbol_put_addr);\n\nstatic ssize_t show_refcnt(struct module_attribute *mattr,\n\t\t\t   struct module_kobject *mk, char *buffer)\n{\n\treturn sprintf(buffer, \"%i\\n\", module_refcount(mk->mod));\n}\n\nstatic struct module_attribute modinfo_refcnt =\n\t__ATTR(refcnt, 0444, show_refcnt, NULL);\n\nvoid __module_get(struct module *module)\n{\n\tif (module) {\n\t\tpreempt_disable();\n\t\tatomic_inc(&module->refcnt);\n\t\ttrace_module_get(module, _RET_IP_);\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL(__module_get);\n\nbool try_module_get(struct module *module)\n{\n\tbool ret = true;\n\n\tif (module) {\n\t\tpreempt_disable();\n\t\t/* Note: here, we can fail to get a reference */\n\t\tif (likely(module_is_live(module) &&\n\t\t\t   atomic_inc_not_zero(&module->refcnt) != 0))\n\t\t\ttrace_module_get(module, _RET_IP_);\n\t\telse\n\t\t\tret = false;\n\n\t\tpreempt_enable();\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(try_module_get);\n\nvoid module_put(struct module *module)\n{\n\tint ret;\n\n\tif (module) {\n\t\tpreempt_disable();\n\t\tret = atomic_dec_if_positive(&module->refcnt);\n\t\tWARN_ON(ret < 0);\t/* Failed to put refcount */\n\t\ttrace_module_put(module, _RET_IP_);\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL(module_put);\n\n#else /* !CONFIG_MODULE_UNLOAD */\nstatic inline void print_unload_info(struct seq_file *m, struct module *mod)\n{\n\t/* We don't know the usage count, or what modules are using. */\n\tseq_puts(m, \" - -\");\n}\n\nstatic inline void module_unload_free(struct module *mod)\n{\n}\n\nstatic int ref_module(struct module *a, struct module *b)\n{\n\treturn strong_try_module_get(b);\n}\n\nstatic inline int module_unload_init(struct module *mod)\n{\n\treturn 0;\n}\n#endif /* CONFIG_MODULE_UNLOAD */\n\nstatic size_t module_flags_taint(struct module *mod, char *buf)\n{\n\tsize_t l = 0;\n\tint i;\n\n\tfor (i = 0; i < TAINT_FLAGS_COUNT; i++) {\n\t\tif (taint_flags[i].module && test_bit(i, &mod->taints))\n\t\t\tbuf[l++] = taint_flags[i].c_true;\n\t}\n\n\treturn l;\n}\n\nstatic ssize_t show_initstate(struct module_attribute *mattr,\n\t\t\t      struct module_kobject *mk, char *buffer)\n{\n\tconst char *state = \"unknown\";\n\n\tswitch (mk->mod->state) {\n\tcase MODULE_STATE_LIVE:\n\t\tstate = \"live\";\n\t\tbreak;\n\tcase MODULE_STATE_COMING:\n\t\tstate = \"coming\";\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\tstate = \"going\";\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\treturn sprintf(buffer, \"%s\\n\", state);\n}\n\nstatic struct module_attribute modinfo_initstate =\n\t__ATTR(initstate, 0444, show_initstate, NULL);\n\nstatic ssize_t store_uevent(struct module_attribute *mattr,\n\t\t\t    struct module_kobject *mk,\n\t\t\t    const char *buffer, size_t count)\n{\n\tint rc;\n\n\trc = kobject_synth_uevent(&mk->kobj, buffer, count);\n\treturn rc ? rc : count;\n}\n\nstruct module_attribute module_uevent =\n\t__ATTR(uevent, 0200, NULL, store_uevent);\n\nstatic ssize_t show_coresize(struct module_attribute *mattr,\n\t\t\t     struct module_kobject *mk, char *buffer)\n{\n\treturn sprintf(buffer, \"%u\\n\", mk->mod->core_layout.size);\n}\n\nstatic struct module_attribute modinfo_coresize =\n\t__ATTR(coresize, 0444, show_coresize, NULL);\n\nstatic ssize_t show_initsize(struct module_attribute *mattr,\n\t\t\t     struct module_kobject *mk, char *buffer)\n{\n\treturn sprintf(buffer, \"%u\\n\", mk->mod->init_layout.size);\n}\n\nstatic struct module_attribute modinfo_initsize =\n\t__ATTR(initsize, 0444, show_initsize, NULL);\n\nstatic ssize_t show_taint(struct module_attribute *mattr,\n\t\t\t  struct module_kobject *mk, char *buffer)\n{\n\tsize_t l;\n\n\tl = module_flags_taint(mk->mod, buffer);\n\tbuffer[l++] = '\\n';\n\treturn l;\n}\n\nstatic struct module_attribute modinfo_taint =\n\t__ATTR(taint, 0444, show_taint, NULL);\n\nstatic struct module_attribute *modinfo_attrs[] = {\n\t&module_uevent,\n\t&modinfo_version,\n\t&modinfo_srcversion,\n\t&modinfo_initstate,\n\t&modinfo_coresize,\n\t&modinfo_initsize,\n\t&modinfo_taint,\n#ifdef CONFIG_MODULE_UNLOAD\n\t&modinfo_refcnt,\n#endif\n\tNULL,\n};\n\nstatic const char vermagic[] = VERMAGIC_STRING;\n\nstatic int try_to_force_load(struct module *mod, const char *reason)\n{\n#ifdef CONFIG_MODULE_FORCE_LOAD\n\tif (!test_taint(TAINT_FORCED_MODULE))\n\t\tpr_warn(\"%s: %s: kernel tainted.\\n\", mod->name, reason);\n\tadd_taint_module(mod, TAINT_FORCED_MODULE, LOCKDEP_NOW_UNRELIABLE);\n\treturn 0;\n#else\n\treturn -ENOEXEC;\n#endif\n}\n\n#ifdef CONFIG_MODVERSIONS\n\nstatic u32 resolve_rel_crc(const s32 *crc)\n{\n\treturn *(u32 *)((void *)crc + *crc);\n}\n\nstatic int check_version(const struct load_info *info,\n\t\t\t const char *symname,\n\t\t\t struct module *mod,\n\t\t\t const s32 *crc)\n{\n\tElf_Shdr *sechdrs = info->sechdrs;\n\tunsigned int versindex = info->index.vers;\n\tunsigned int i, num_versions;\n\tstruct modversion_info *versions;\n\n\t/* Exporting module didn't supply crcs?  OK, we're already tainted. */\n\tif (!crc)\n\t\treturn 1;\n\n\t/* No versions at all?  modprobe --force does this. */\n\tif (versindex == 0)\n\t\treturn try_to_force_load(mod, symname) == 0;\n\n\tversions = (void *) sechdrs[versindex].sh_addr;\n\tnum_versions = sechdrs[versindex].sh_size\n\t\t/ sizeof(struct modversion_info);\n\n\tfor (i = 0; i < num_versions; i++) {\n\t\tu32 crcval;\n\n\t\tif (strcmp(versions[i].name, symname) != 0)\n\t\t\tcontinue;\n\n\t\tif (IS_ENABLED(CONFIG_MODULE_REL_CRCS))\n\t\t\tcrcval = resolve_rel_crc(crc);\n\t\telse\n\t\t\tcrcval = *crc;\n\t\tif (versions[i].crc == crcval)\n\t\t\treturn 1;\n\t\tpr_debug(\"Found checksum %X vs module %lX\\n\",\n\t\t\t crcval, versions[i].crc);\n\t\tgoto bad_version;\n\t}\n\n\t/* Broken toolchain. Warn once, then let it go.. */\n\tpr_warn_once(\"%s: no symbol version for %s\\n\", info->name, symname);\n\treturn 1;\n\nbad_version:\n\tpr_warn(\"%s: disagrees about version of symbol %s\\n\",\n\t       info->name, symname);\n\treturn 0;\n}\n\nstatic inline int check_modstruct_version(const struct load_info *info,\n\t\t\t\t\t  struct module *mod)\n{\n\tstruct find_symbol_arg fsa = {\n\t\t.name\t= \"module_layout\",\n\t\t.gplok\t= true,\n\t};\n\n\t/*\n\t * Since this should be found in kernel (which can't be removed), no\n\t * locking is necessary -- use preempt_disable() to placate lockdep.\n\t */\n\tpreempt_disable();\n\tif (!find_symbol(&fsa)) {\n\t\tpreempt_enable();\n\t\tBUG();\n\t}\n\tpreempt_enable();\n\treturn check_version(info, \"module_layout\", mod, fsa.crc);\n}\n\n/* First part is kernel version, which we ignore if module has crcs. */\nstatic inline int same_magic(const char *amagic, const char *bmagic,\n\t\t\t     bool has_crcs)\n{\n\tif (has_crcs) {\n\t\tamagic += strcspn(amagic, \" \");\n\t\tbmagic += strcspn(bmagic, \" \");\n\t}\n\treturn strcmp(amagic, bmagic) == 0;\n}\n#else\nstatic inline int check_version(const struct load_info *info,\n\t\t\t\tconst char *symname,\n\t\t\t\tstruct module *mod,\n\t\t\t\tconst s32 *crc)\n{\n\treturn 1;\n}\n\nstatic inline int check_modstruct_version(const struct load_info *info,\n\t\t\t\t\t  struct module *mod)\n{\n\treturn 1;\n}\n\nstatic inline int same_magic(const char *amagic, const char *bmagic,\n\t\t\t     bool has_crcs)\n{\n\treturn strcmp(amagic, bmagic) == 0;\n}\n#endif /* CONFIG_MODVERSIONS */\n\nstatic char *get_modinfo(const struct load_info *info, const char *tag);\nstatic char *get_next_modinfo(const struct load_info *info, const char *tag,\n\t\t\t      char *prev);\n\nstatic int verify_namespace_is_imported(const struct load_info *info,\n\t\t\t\t\tconst struct kernel_symbol *sym,\n\t\t\t\t\tstruct module *mod)\n{\n\tconst char *namespace;\n\tchar *imported_namespace;\n\n\tnamespace = kernel_symbol_namespace(sym);\n\tif (namespace && namespace[0]) {\n\t\timported_namespace = get_modinfo(info, \"import_ns\");\n\t\twhile (imported_namespace) {\n\t\t\tif (strcmp(namespace, imported_namespace) == 0)\n\t\t\t\treturn 0;\n\t\t\timported_namespace = get_next_modinfo(\n\t\t\t\tinfo, \"import_ns\", imported_namespace);\n\t\t}\n#ifdef CONFIG_MODULE_ALLOW_MISSING_NAMESPACE_IMPORTS\n\t\tpr_warn(\n#else\n\t\tpr_err(\n#endif\n\t\t\t\"%s: module uses symbol (%s) from namespace %s, but does not import it.\\n\",\n\t\t\tmod->name, kernel_symbol_name(sym), namespace);\n#ifndef CONFIG_MODULE_ALLOW_MISSING_NAMESPACE_IMPORTS\n\t\treturn -EINVAL;\n#endif\n\t}\n\treturn 0;\n}\n\nstatic bool inherit_taint(struct module *mod, struct module *owner)\n{\n\tif (!owner || !test_bit(TAINT_PROPRIETARY_MODULE, &owner->taints))\n\t\treturn true;\n\n\tif (mod->using_gplonly_symbols) {\n\t\tpr_err(\"%s: module using GPL-only symbols uses symbols from proprietary module %s.\\n\",\n\t\t\tmod->name, owner->name);\n\t\treturn false;\n\t}\n\n\tif (!test_bit(TAINT_PROPRIETARY_MODULE, &mod->taints)) {\n\t\tpr_warn(\"%s: module uses symbols from proprietary module %s, inheriting taint.\\n\",\n\t\t\tmod->name, owner->name);\n\t\tset_bit(TAINT_PROPRIETARY_MODULE, &mod->taints);\n\t}\n\treturn true;\n}\n\n/* Resolve a symbol for this module.  I.e. if we find one, record usage. */\nstatic const struct kernel_symbol *resolve_symbol(struct module *mod,\n\t\t\t\t\t\t  const struct load_info *info,\n\t\t\t\t\t\t  const char *name,\n\t\t\t\t\t\t  char ownername[])\n{\n\tstruct find_symbol_arg fsa = {\n\t\t.name\t= name,\n\t\t.gplok\t= !(mod->taints & (1 << TAINT_PROPRIETARY_MODULE)),\n\t\t.warn\t= true,\n\t};\n\tint err;\n\n\t/*\n\t * The module_mutex should not be a heavily contended lock;\n\t * if we get the occasional sleep here, we'll go an extra iteration\n\t * in the wait_event_interruptible(), which is harmless.\n\t */\n\tsched_annotate_sleep();\n\tmutex_lock(&module_mutex);\n\tif (!find_symbol(&fsa))\n\t\tgoto unlock;\n\n\tif (fsa.license == GPL_ONLY)\n\t\tmod->using_gplonly_symbols = true;\n\n\tif (!inherit_taint(mod, fsa.owner)) {\n\t\tfsa.sym = NULL;\n\t\tgoto getname;\n\t}\n\n\tif (!check_version(info, name, mod, fsa.crc)) {\n\t\tfsa.sym = ERR_PTR(-EINVAL);\n\t\tgoto getname;\n\t}\n\n\terr = verify_namespace_is_imported(info, fsa.sym, mod);\n\tif (err) {\n\t\tfsa.sym = ERR_PTR(err);\n\t\tgoto getname;\n\t}\n\n\terr = ref_module(mod, fsa.owner);\n\tif (err) {\n\t\tfsa.sym = ERR_PTR(err);\n\t\tgoto getname;\n\t}\n\ngetname:\n\t/* We must make copy under the lock if we failed to get ref. */\n\tstrncpy(ownername, module_name(fsa.owner), MODULE_NAME_LEN);\nunlock:\n\tmutex_unlock(&module_mutex);\n\treturn fsa.sym;\n}\n\nstatic const struct kernel_symbol *\nresolve_symbol_wait(struct module *mod,\n\t\t    const struct load_info *info,\n\t\t    const char *name)\n{\n\tconst struct kernel_symbol *ksym;\n\tchar owner[MODULE_NAME_LEN];\n\n\tif (wait_event_interruptible_timeout(module_wq,\n\t\t\t!IS_ERR(ksym = resolve_symbol(mod, info, name, owner))\n\t\t\t|| PTR_ERR(ksym) != -EBUSY,\n\t\t\t\t\t     30 * HZ) <= 0) {\n\t\tpr_warn(\"%s: gave up waiting for init of module %s.\\n\",\n\t\t\tmod->name, owner);\n\t}\n\treturn ksym;\n}\n\n/*\n * /sys/module/foo/sections stuff\n * J. Corbet <corbet@lwn.net>\n */\n#ifdef CONFIG_SYSFS\n\n#ifdef CONFIG_KALLSYMS\nstatic inline bool sect_empty(const Elf_Shdr *sect)\n{\n\treturn !(sect->sh_flags & SHF_ALLOC) || sect->sh_size == 0;\n}\n\nstruct module_sect_attr {\n\tstruct bin_attribute battr;\n\tunsigned long address;\n};\n\nstruct module_sect_attrs {\n\tstruct attribute_group grp;\n\tunsigned int nsections;\n\tstruct module_sect_attr attrs[];\n};\n\n#define MODULE_SECT_READ_SIZE (3 /* \"0x\", \"\\n\" */ + (BITS_PER_LONG / 4))\nstatic ssize_t module_sect_read(struct file *file, struct kobject *kobj,\n\t\t\t\tstruct bin_attribute *battr,\n\t\t\t\tchar *buf, loff_t pos, size_t count)\n{\n\tstruct module_sect_attr *sattr =\n\t\tcontainer_of(battr, struct module_sect_attr, battr);\n\tchar bounce[MODULE_SECT_READ_SIZE + 1];\n\tsize_t wrote;\n\n\tif (pos != 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Since we're a binary read handler, we must account for the\n\t * trailing NUL byte that sprintf will write: if \"buf\" is\n\t * too small to hold the NUL, or the NUL is exactly the last\n\t * byte, the read will look like it got truncated by one byte.\n\t * Since there is no way to ask sprintf nicely to not write\n\t * the NUL, we have to use a bounce buffer.\n\t */\n\twrote = scnprintf(bounce, sizeof(bounce), \"0x%px\\n\",\n\t\t\t kallsyms_show_value(file->f_cred)\n\t\t\t\t? (void *)sattr->address : NULL);\n\tcount = min(count, wrote);\n\tmemcpy(buf, bounce, count);\n\n\treturn count;\n}\n\nstatic void free_sect_attrs(struct module_sect_attrs *sect_attrs)\n{\n\tunsigned int section;\n\n\tfor (section = 0; section < sect_attrs->nsections; section++)\n\t\tkfree(sect_attrs->attrs[section].battr.attr.name);\n\tkfree(sect_attrs);\n}\n\nstatic void add_sect_attrs(struct module *mod, const struct load_info *info)\n{\n\tunsigned int nloaded = 0, i, size[2];\n\tstruct module_sect_attrs *sect_attrs;\n\tstruct module_sect_attr *sattr;\n\tstruct bin_attribute **gattr;\n\n\t/* Count loaded sections and allocate structures */\n\tfor (i = 0; i < info->hdr->e_shnum; i++)\n\t\tif (!sect_empty(&info->sechdrs[i]))\n\t\t\tnloaded++;\n\tsize[0] = ALIGN(struct_size(sect_attrs, attrs, nloaded),\n\t\t\tsizeof(sect_attrs->grp.bin_attrs[0]));\n\tsize[1] = (nloaded + 1) * sizeof(sect_attrs->grp.bin_attrs[0]);\n\tsect_attrs = kzalloc(size[0] + size[1], GFP_KERNEL);\n\tif (sect_attrs == NULL)\n\t\treturn;\n\n\t/* Setup section attributes. */\n\tsect_attrs->grp.name = \"sections\";\n\tsect_attrs->grp.bin_attrs = (void *)sect_attrs + size[0];\n\n\tsect_attrs->nsections = 0;\n\tsattr = &sect_attrs->attrs[0];\n\tgattr = &sect_attrs->grp.bin_attrs[0];\n\tfor (i = 0; i < info->hdr->e_shnum; i++) {\n\t\tElf_Shdr *sec = &info->sechdrs[i];\n\t\tif (sect_empty(sec))\n\t\t\tcontinue;\n\t\tsysfs_bin_attr_init(&sattr->battr);\n\t\tsattr->address = sec->sh_addr;\n\t\tsattr->battr.attr.name =\n\t\t\tkstrdup(info->secstrings + sec->sh_name, GFP_KERNEL);\n\t\tif (sattr->battr.attr.name == NULL)\n\t\t\tgoto out;\n\t\tsect_attrs->nsections++;\n\t\tsattr->battr.read = module_sect_read;\n\t\tsattr->battr.size = MODULE_SECT_READ_SIZE;\n\t\tsattr->battr.attr.mode = 0400;\n\t\t*(gattr++) = &(sattr++)->battr;\n\t}\n\t*gattr = NULL;\n\n\tif (sysfs_create_group(&mod->mkobj.kobj, &sect_attrs->grp))\n\t\tgoto out;\n\n\tmod->sect_attrs = sect_attrs;\n\treturn;\n  out:\n\tfree_sect_attrs(sect_attrs);\n}\n\nstatic void remove_sect_attrs(struct module *mod)\n{\n\tif (mod->sect_attrs) {\n\t\tsysfs_remove_group(&mod->mkobj.kobj,\n\t\t\t\t   &mod->sect_attrs->grp);\n\t\t/*\n\t\t * We are positive that no one is using any sect attrs\n\t\t * at this point.  Deallocate immediately.\n\t\t */\n\t\tfree_sect_attrs(mod->sect_attrs);\n\t\tmod->sect_attrs = NULL;\n\t}\n}\n\n/*\n * /sys/module/foo/notes/.section.name gives contents of SHT_NOTE sections.\n */\n\nstruct module_notes_attrs {\n\tstruct kobject *dir;\n\tunsigned int notes;\n\tstruct bin_attribute attrs[];\n};\n\nstatic ssize_t module_notes_read(struct file *filp, struct kobject *kobj,\n\t\t\t\t struct bin_attribute *bin_attr,\n\t\t\t\t char *buf, loff_t pos, size_t count)\n{\n\t/*\n\t * The caller checked the pos and count against our size.\n\t */\n\tmemcpy(buf, bin_attr->private + pos, count);\n\treturn count;\n}\n\nstatic void free_notes_attrs(struct module_notes_attrs *notes_attrs,\n\t\t\t     unsigned int i)\n{\n\tif (notes_attrs->dir) {\n\t\twhile (i-- > 0)\n\t\t\tsysfs_remove_bin_file(notes_attrs->dir,\n\t\t\t\t\t      &notes_attrs->attrs[i]);\n\t\tkobject_put(notes_attrs->dir);\n\t}\n\tkfree(notes_attrs);\n}\n\nstatic void add_notes_attrs(struct module *mod, const struct load_info *info)\n{\n\tunsigned int notes, loaded, i;\n\tstruct module_notes_attrs *notes_attrs;\n\tstruct bin_attribute *nattr;\n\n\t/* failed to create section attributes, so can't create notes */\n\tif (!mod->sect_attrs)\n\t\treturn;\n\n\t/* Count notes sections and allocate structures.  */\n\tnotes = 0;\n\tfor (i = 0; i < info->hdr->e_shnum; i++)\n\t\tif (!sect_empty(&info->sechdrs[i]) &&\n\t\t    (info->sechdrs[i].sh_type == SHT_NOTE))\n\t\t\t++notes;\n\n\tif (notes == 0)\n\t\treturn;\n\n\tnotes_attrs = kzalloc(struct_size(notes_attrs, attrs, notes),\n\t\t\t      GFP_KERNEL);\n\tif (notes_attrs == NULL)\n\t\treturn;\n\n\tnotes_attrs->notes = notes;\n\tnattr = &notes_attrs->attrs[0];\n\tfor (loaded = i = 0; i < info->hdr->e_shnum; ++i) {\n\t\tif (sect_empty(&info->sechdrs[i]))\n\t\t\tcontinue;\n\t\tif (info->sechdrs[i].sh_type == SHT_NOTE) {\n\t\t\tsysfs_bin_attr_init(nattr);\n\t\t\tnattr->attr.name = mod->sect_attrs->attrs[loaded].battr.attr.name;\n\t\t\tnattr->attr.mode = S_IRUGO;\n\t\t\tnattr->size = info->sechdrs[i].sh_size;\n\t\t\tnattr->private = (void *) info->sechdrs[i].sh_addr;\n\t\t\tnattr->read = module_notes_read;\n\t\t\t++nattr;\n\t\t}\n\t\t++loaded;\n\t}\n\n\tnotes_attrs->dir = kobject_create_and_add(\"notes\", &mod->mkobj.kobj);\n\tif (!notes_attrs->dir)\n\t\tgoto out;\n\n\tfor (i = 0; i < notes; ++i)\n\t\tif (sysfs_create_bin_file(notes_attrs->dir,\n\t\t\t\t\t  &notes_attrs->attrs[i]))\n\t\t\tgoto out;\n\n\tmod->notes_attrs = notes_attrs;\n\treturn;\n\n  out:\n\tfree_notes_attrs(notes_attrs, i);\n}\n\nstatic void remove_notes_attrs(struct module *mod)\n{\n\tif (mod->notes_attrs)\n\t\tfree_notes_attrs(mod->notes_attrs, mod->notes_attrs->notes);\n}\n\n#else\n\nstatic inline void add_sect_attrs(struct module *mod,\n\t\t\t\t  const struct load_info *info)\n{\n}\n\nstatic inline void remove_sect_attrs(struct module *mod)\n{\n}\n\nstatic inline void add_notes_attrs(struct module *mod,\n\t\t\t\t   const struct load_info *info)\n{\n}\n\nstatic inline void remove_notes_attrs(struct module *mod)\n{\n}\n#endif /* CONFIG_KALLSYMS */\n\nstatic void del_usage_links(struct module *mod)\n{\n#ifdef CONFIG_MODULE_UNLOAD\n\tstruct module_use *use;\n\n\tmutex_lock(&module_mutex);\n\tlist_for_each_entry(use, &mod->target_list, target_list)\n\t\tsysfs_remove_link(use->target->holders_dir, mod->name);\n\tmutex_unlock(&module_mutex);\n#endif\n}\n\nstatic int add_usage_links(struct module *mod)\n{\n\tint ret = 0;\n#ifdef CONFIG_MODULE_UNLOAD\n\tstruct module_use *use;\n\n\tmutex_lock(&module_mutex);\n\tlist_for_each_entry(use, &mod->target_list, target_list) {\n\t\tret = sysfs_create_link(use->target->holders_dir,\n\t\t\t\t\t&mod->mkobj.kobj, mod->name);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&module_mutex);\n\tif (ret)\n\t\tdel_usage_links(mod);\n#endif\n\treturn ret;\n}\n\nstatic void module_remove_modinfo_attrs(struct module *mod, int end);\n\nstatic int module_add_modinfo_attrs(struct module *mod)\n{\n\tstruct module_attribute *attr;\n\tstruct module_attribute *temp_attr;\n\tint error = 0;\n\tint i;\n\n\tmod->modinfo_attrs = kzalloc((sizeof(struct module_attribute) *\n\t\t\t\t\t(ARRAY_SIZE(modinfo_attrs) + 1)),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!mod->modinfo_attrs)\n\t\treturn -ENOMEM;\n\n\ttemp_attr = mod->modinfo_attrs;\n\tfor (i = 0; (attr = modinfo_attrs[i]); i++) {\n\t\tif (!attr->test || attr->test(mod)) {\n\t\t\tmemcpy(temp_attr, attr, sizeof(*temp_attr));\n\t\t\tsysfs_attr_init(&temp_attr->attr);\n\t\t\terror = sysfs_create_file(&mod->mkobj.kobj,\n\t\t\t\t\t&temp_attr->attr);\n\t\t\tif (error)\n\t\t\t\tgoto error_out;\n\t\t\t++temp_attr;\n\t\t}\n\t}\n\n\treturn 0;\n\nerror_out:\n\tif (i > 0)\n\t\tmodule_remove_modinfo_attrs(mod, --i);\n\telse\n\t\tkfree(mod->modinfo_attrs);\n\treturn error;\n}\n\nstatic void module_remove_modinfo_attrs(struct module *mod, int end)\n{\n\tstruct module_attribute *attr;\n\tint i;\n\n\tfor (i = 0; (attr = &mod->modinfo_attrs[i]); i++) {\n\t\tif (end >= 0 && i > end)\n\t\t\tbreak;\n\t\t/* pick a field to test for end of list */\n\t\tif (!attr->attr.name)\n\t\t\tbreak;\n\t\tsysfs_remove_file(&mod->mkobj.kobj, &attr->attr);\n\t\tif (attr->free)\n\t\t\tattr->free(mod);\n\t}\n\tkfree(mod->modinfo_attrs);\n}\n\nstatic void mod_kobject_put(struct module *mod)\n{\n\tDECLARE_COMPLETION_ONSTACK(c);\n\tmod->mkobj.kobj_completion = &c;\n\tkobject_put(&mod->mkobj.kobj);\n\twait_for_completion(&c);\n}\n\nstatic int mod_sysfs_init(struct module *mod)\n{\n\tint err;\n\tstruct kobject *kobj;\n\n\tif (!module_sysfs_initialized) {\n\t\tpr_err(\"%s: module sysfs not initialized\\n\", mod->name);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tkobj = kset_find_obj(module_kset, mod->name);\n\tif (kobj) {\n\t\tpr_err(\"%s: module is already loaded\\n\", mod->name);\n\t\tkobject_put(kobj);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmod->mkobj.mod = mod;\n\n\tmemset(&mod->mkobj.kobj, 0, sizeof(mod->mkobj.kobj));\n\tmod->mkobj.kobj.kset = module_kset;\n\terr = kobject_init_and_add(&mod->mkobj.kobj, &module_ktype, NULL,\n\t\t\t\t   \"%s\", mod->name);\n\tif (err)\n\t\tmod_kobject_put(mod);\n\nout:\n\treturn err;\n}\n\nstatic int mod_sysfs_setup(struct module *mod,\n\t\t\t   const struct load_info *info,\n\t\t\t   struct kernel_param *kparam,\n\t\t\t   unsigned int num_params)\n{\n\tint err;\n\n\terr = mod_sysfs_init(mod);\n\tif (err)\n\t\tgoto out;\n\n\tmod->holders_dir = kobject_create_and_add(\"holders\", &mod->mkobj.kobj);\n\tif (!mod->holders_dir) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unreg;\n\t}\n\n\terr = module_param_sysfs_setup(mod, kparam, num_params);\n\tif (err)\n\t\tgoto out_unreg_holders;\n\n\terr = module_add_modinfo_attrs(mod);\n\tif (err)\n\t\tgoto out_unreg_param;\n\n\terr = add_usage_links(mod);\n\tif (err)\n\t\tgoto out_unreg_modinfo_attrs;\n\n\tadd_sect_attrs(mod, info);\n\tadd_notes_attrs(mod, info);\n\n\treturn 0;\n\nout_unreg_modinfo_attrs:\n\tmodule_remove_modinfo_attrs(mod, -1);\nout_unreg_param:\n\tmodule_param_sysfs_remove(mod);\nout_unreg_holders:\n\tkobject_put(mod->holders_dir);\nout_unreg:\n\tmod_kobject_put(mod);\nout:\n\treturn err;\n}\n\nstatic void mod_sysfs_fini(struct module *mod)\n{\n\tremove_notes_attrs(mod);\n\tremove_sect_attrs(mod);\n\tmod_kobject_put(mod);\n}\n\nstatic void init_param_lock(struct module *mod)\n{\n\tmutex_init(&mod->param_lock);\n}\n#else /* !CONFIG_SYSFS */\n\nstatic int mod_sysfs_setup(struct module *mod,\n\t\t\t   const struct load_info *info,\n\t\t\t   struct kernel_param *kparam,\n\t\t\t   unsigned int num_params)\n{\n\treturn 0;\n}\n\nstatic void mod_sysfs_fini(struct module *mod)\n{\n}\n\nstatic void module_remove_modinfo_attrs(struct module *mod, int end)\n{\n}\n\nstatic void del_usage_links(struct module *mod)\n{\n}\n\nstatic void init_param_lock(struct module *mod)\n{\n}\n#endif /* CONFIG_SYSFS */\n\nstatic void mod_sysfs_teardown(struct module *mod)\n{\n\tdel_usage_links(mod);\n\tmodule_remove_modinfo_attrs(mod, -1);\n\tmodule_param_sysfs_remove(mod);\n\tkobject_put(mod->mkobj.drivers_dir);\n\tkobject_put(mod->holders_dir);\n\tmod_sysfs_fini(mod);\n}\n\n/*\n * LKM RO/NX protection: protect module's text/ro-data\n * from modification and any data from execution.\n *\n * General layout of module is:\n *          [text] [read-only-data] [ro-after-init] [writable data]\n * text_size -----^                ^               ^               ^\n * ro_size ------------------------|               |               |\n * ro_after_init_size -----------------------------|               |\n * size -----------------------------------------------------------|\n *\n * These values are always page-aligned (as is base)\n */\n\n/*\n * Since some arches are moving towards PAGE_KERNEL module allocations instead\n * of PAGE_KERNEL_EXEC, keep frob_text() and module_enable_x() outside of the\n * CONFIG_STRICT_MODULE_RWX block below because they are needed regardless of\n * whether we are strict.\n */\n#ifdef CONFIG_ARCH_HAS_STRICT_MODULE_RWX\nstatic void frob_text(const struct module_layout *layout,\n\t\t      int (*set_memory)(unsigned long start, int num_pages))\n{\n\tBUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->text_size & (PAGE_SIZE-1));\n\tset_memory((unsigned long)layout->base,\n\t\t   layout->text_size >> PAGE_SHIFT);\n}\n\nstatic void module_enable_x(const struct module *mod)\n{\n\tfrob_text(&mod->core_layout, set_memory_x);\n\tfrob_text(&mod->init_layout, set_memory_x);\n}\n#else /* !CONFIG_ARCH_HAS_STRICT_MODULE_RWX */\nstatic void module_enable_x(const struct module *mod) { }\n#endif /* CONFIG_ARCH_HAS_STRICT_MODULE_RWX */\n\n#ifdef CONFIG_STRICT_MODULE_RWX\nstatic void frob_rodata(const struct module_layout *layout,\n\t\t\tint (*set_memory)(unsigned long start, int num_pages))\n{\n\tBUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->text_size & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->ro_size & (PAGE_SIZE-1));\n\tset_memory((unsigned long)layout->base + layout->text_size,\n\t\t   (layout->ro_size - layout->text_size) >> PAGE_SHIFT);\n}\n\nstatic void frob_ro_after_init(const struct module_layout *layout,\n\t\t\t\tint (*set_memory)(unsigned long start, int num_pages))\n{\n\tBUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->ro_size & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->ro_after_init_size & (PAGE_SIZE-1));\n\tset_memory((unsigned long)layout->base + layout->ro_size,\n\t\t   (layout->ro_after_init_size - layout->ro_size) >> PAGE_SHIFT);\n}\n\nstatic void frob_writable_data(const struct module_layout *layout,\n\t\t\t       int (*set_memory)(unsigned long start, int num_pages))\n{\n\tBUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->ro_after_init_size & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->size & (PAGE_SIZE-1));\n\tset_memory((unsigned long)layout->base + layout->ro_after_init_size,\n\t\t   (layout->size - layout->ro_after_init_size) >> PAGE_SHIFT);\n}\n\nstatic void module_enable_ro(const struct module *mod, bool after_init)\n{\n\tif (!rodata_enabled)\n\t\treturn;\n\n\tset_vm_flush_reset_perms(mod->core_layout.base);\n\tset_vm_flush_reset_perms(mod->init_layout.base);\n\tfrob_text(&mod->core_layout, set_memory_ro);\n\n\tfrob_rodata(&mod->core_layout, set_memory_ro);\n\tfrob_text(&mod->init_layout, set_memory_ro);\n\tfrob_rodata(&mod->init_layout, set_memory_ro);\n\n\tif (after_init)\n\t\tfrob_ro_after_init(&mod->core_layout, set_memory_ro);\n}\n\nstatic void module_enable_nx(const struct module *mod)\n{\n\tfrob_rodata(&mod->core_layout, set_memory_nx);\n\tfrob_ro_after_init(&mod->core_layout, set_memory_nx);\n\tfrob_writable_data(&mod->core_layout, set_memory_nx);\n\tfrob_rodata(&mod->init_layout, set_memory_nx);\n\tfrob_writable_data(&mod->init_layout, set_memory_nx);\n}\n\nstatic int module_enforce_rwx_sections(Elf_Ehdr *hdr, Elf_Shdr *sechdrs,\n\t\t\t\t       char *secstrings, struct module *mod)\n{\n\tconst unsigned long shf_wx = SHF_WRITE|SHF_EXECINSTR;\n\tint i;\n\n\tfor (i = 0; i < hdr->e_shnum; i++) {\n\t\tif ((sechdrs[i].sh_flags & shf_wx) == shf_wx) {\n\t\t\tpr_err(\"%s: section %s (index %d) has invalid WRITE|EXEC flags\\n\",\n\t\t\t\tmod->name, secstrings + sechdrs[i].sh_name, i);\n\t\t\treturn -ENOEXEC;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#else /* !CONFIG_STRICT_MODULE_RWX */\nstatic void module_enable_nx(const struct module *mod) { }\nstatic void module_enable_ro(const struct module *mod, bool after_init) {}\nstatic int module_enforce_rwx_sections(Elf_Ehdr *hdr, Elf_Shdr *sechdrs,\n\t\t\t\t       char *secstrings, struct module *mod)\n{\n\treturn 0;\n}\n#endif /*  CONFIG_STRICT_MODULE_RWX */\n\n#ifdef CONFIG_LIVEPATCH\n/*\n * Persist Elf information about a module. Copy the Elf header,\n * section header table, section string table, and symtab section\n * index from info to mod->klp_info.\n */\nstatic int copy_module_elf(struct module *mod, struct load_info *info)\n{\n\tunsigned int size, symndx;\n\tint ret;\n\n\tsize = sizeof(*mod->klp_info);\n\tmod->klp_info = kmalloc(size, GFP_KERNEL);\n\tif (mod->klp_info == NULL)\n\t\treturn -ENOMEM;\n\n\t/* Elf header */\n\tsize = sizeof(mod->klp_info->hdr);\n\tmemcpy(&mod->klp_info->hdr, info->hdr, size);\n\n\t/* Elf section header table */\n\tsize = sizeof(*info->sechdrs) * info->hdr->e_shnum;\n\tmod->klp_info->sechdrs = kmemdup(info->sechdrs, size, GFP_KERNEL);\n\tif (mod->klp_info->sechdrs == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto free_info;\n\t}\n\n\t/* Elf section name string table */\n\tsize = info->sechdrs[info->hdr->e_shstrndx].sh_size;\n\tmod->klp_info->secstrings = kmemdup(info->secstrings, size, GFP_KERNEL);\n\tif (mod->klp_info->secstrings == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto free_sechdrs;\n\t}\n\n\t/* Elf symbol section index */\n\tsymndx = info->index.sym;\n\tmod->klp_info->symndx = symndx;\n\n\t/*\n\t * For livepatch modules, core_kallsyms.symtab is a complete\n\t * copy of the original symbol table. Adjust sh_addr to point\n\t * to core_kallsyms.symtab since the copy of the symtab in module\n\t * init memory is freed at the end of do_init_module().\n\t */\n\tmod->klp_info->sechdrs[symndx].sh_addr = \\\n\t\t(unsigned long) mod->core_kallsyms.symtab;\n\n\treturn 0;\n\nfree_sechdrs:\n\tkfree(mod->klp_info->sechdrs);\nfree_info:\n\tkfree(mod->klp_info);\n\treturn ret;\n}\n\nstatic void free_module_elf(struct module *mod)\n{\n\tkfree(mod->klp_info->sechdrs);\n\tkfree(mod->klp_info->secstrings);\n\tkfree(mod->klp_info);\n}\n#else /* !CONFIG_LIVEPATCH */\nstatic int copy_module_elf(struct module *mod, struct load_info *info)\n{\n\treturn 0;\n}\n\nstatic void free_module_elf(struct module *mod)\n{\n}\n#endif /* CONFIG_LIVEPATCH */\n\nvoid __weak module_memfree(void *module_region)\n{\n\t/*\n\t * This memory may be RO, and freeing RO memory in an interrupt is not\n\t * supported by vmalloc.\n\t */\n\tWARN_ON(in_interrupt());\n\tvfree(module_region);\n}\n\nvoid __weak module_arch_cleanup(struct module *mod)\n{\n}\n\nvoid __weak module_arch_freeing_init(struct module *mod)\n{\n}\n\nstatic void cfi_cleanup(struct module *mod);\n\n/* Free a module, remove from lists, etc. */\nstatic void free_module(struct module *mod)\n{\n\ttrace_module_free(mod);\n\n\tmod_sysfs_teardown(mod);\n\n\t/*\n\t * We leave it in list to prevent duplicate loads, but make sure\n\t * that noone uses it while it's being deconstructed.\n\t */\n\tmutex_lock(&module_mutex);\n\tmod->state = MODULE_STATE_UNFORMED;\n\tmutex_unlock(&module_mutex);\n\n\t/* Remove dynamic debug info */\n\tddebug_remove_module(mod->name);\n\n\t/* Arch-specific cleanup. */\n\tmodule_arch_cleanup(mod);\n\n\t/* Module unload stuff */\n\tmodule_unload_free(mod);\n\n\t/* Free any allocated parameters. */\n\tdestroy_params(mod->kp, mod->num_kp);\n\n\tif (is_livepatch_module(mod))\n\t\tfree_module_elf(mod);\n\n\t/* Now we can delete it from the lists */\n\tmutex_lock(&module_mutex);\n\t/* Unlink carefully: kallsyms could be walking list. */\n\tlist_del_rcu(&mod->list);\n\tmod_tree_remove(mod);\n\t/* Remove this module from bug list, this uses list_del_rcu */\n\tmodule_bug_cleanup(mod);\n\t/* Wait for RCU-sched synchronizing before releasing mod->list and buglist. */\n\tsynchronize_rcu();\n\tmutex_unlock(&module_mutex);\n\n\t/* Clean up CFI for the module. */\n\tcfi_cleanup(mod);\n\n\t/* This may be empty, but that's OK */\n\tmodule_arch_freeing_init(mod);\n\tmodule_memfree(mod->init_layout.base);\n\tkfree(mod->args);\n\tpercpu_modfree(mod);\n\n\t/* Free lock-classes; relies on the preceding sync_rcu(). */\n\tlockdep_free_key_range(mod->core_layout.base, mod->core_layout.size);\n\n\t/* Finally, free the core (containing the module structure) */\n\tmodule_memfree(mod->core_layout.base);\n}\n\nvoid *__symbol_get(const char *symbol)\n{\n\tstruct find_symbol_arg fsa = {\n\t\t.name\t= symbol,\n\t\t.gplok\t= true,\n\t\t.warn\t= true,\n\t};\n\n\tpreempt_disable();\n\tif (!find_symbol(&fsa) || strong_try_module_get(fsa.owner)) {\n\t\tpreempt_enable();\n\t\treturn NULL;\n\t}\n\tpreempt_enable();\n\treturn (void *)kernel_symbol_value(fsa.sym);\n}\nEXPORT_SYMBOL_GPL(__symbol_get);\n\n/*\n * Ensure that an exported symbol [global namespace] does not already exist\n * in the kernel or in some other module's exported symbol table.\n *\n * You must hold the module_mutex.\n */\nstatic int verify_exported_symbols(struct module *mod)\n{\n\tunsigned int i;\n\tconst struct kernel_symbol *s;\n\tstruct {\n\t\tconst struct kernel_symbol *sym;\n\t\tunsigned int num;\n\t} arr[] = {\n\t\t{ mod->syms, mod->num_syms },\n\t\t{ mod->gpl_syms, mod->num_gpl_syms },\n\t};\n\n\tfor (i = 0; i < ARRAY_SIZE(arr); i++) {\n\t\tfor (s = arr[i].sym; s < arr[i].sym + arr[i].num; s++) {\n\t\t\tstruct find_symbol_arg fsa = {\n\t\t\t\t.name\t= kernel_symbol_name(s),\n\t\t\t\t.gplok\t= true,\n\t\t\t};\n\t\t\tif (find_symbol(&fsa)) {\n\t\t\t\tpr_err(\"%s: exports duplicate symbol %s\"\n\t\t\t\t       \" (owned by %s)\\n\",\n\t\t\t\t       mod->name, kernel_symbol_name(s),\n\t\t\t\t       module_name(fsa.owner));\n\t\t\t\treturn -ENOEXEC;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic bool ignore_undef_symbol(Elf_Half emachine, const char *name)\n{\n\t/*\n\t * On x86, PIC code and Clang non-PIC code may have call foo@PLT. GNU as\n\t * before 2.37 produces an unreferenced _GLOBAL_OFFSET_TABLE_ on x86-64.\n\t * i386 has a similar problem but may not deserve a fix.\n\t *\n\t * If we ever have to ignore many symbols, consider refactoring the code to\n\t * only warn if referenced by a relocation.\n\t */\n\tif (emachine == EM_386 || emachine == EM_X86_64)\n\t\treturn !strcmp(name, \"_GLOBAL_OFFSET_TABLE_\");\n\treturn false;\n}\n\n/* Change all symbols so that st_value encodes the pointer directly. */\nstatic int simplify_symbols(struct module *mod, const struct load_info *info)\n{\n\tElf_Shdr *symsec = &info->sechdrs[info->index.sym];\n\tElf_Sym *sym = (void *)symsec->sh_addr;\n\tunsigned long secbase;\n\tunsigned int i;\n\tint ret = 0;\n\tconst struct kernel_symbol *ksym;\n\n\tfor (i = 1; i < symsec->sh_size / sizeof(Elf_Sym); i++) {\n\t\tconst char *name = info->strtab + sym[i].st_name;\n\n\t\tswitch (sym[i].st_shndx) {\n\t\tcase SHN_COMMON:\n\t\t\t/* Ignore common symbols */\n\t\t\tif (!strncmp(name, \"__gnu_lto\", 9))\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * We compiled with -fno-common.  These are not\n\t\t\t * supposed to happen.\n\t\t\t */\n\t\t\tpr_debug(\"Common symbol: %s\\n\", name);\n\t\t\tpr_warn(\"%s: please compile with -fno-common\\n\",\n\t\t\t       mod->name);\n\t\t\tret = -ENOEXEC;\n\t\t\tbreak;\n\n\t\tcase SHN_ABS:\n\t\t\t/* Don't need to do anything */\n\t\t\tpr_debug(\"Absolute symbol: 0x%08lx\\n\",\n\t\t\t       (long)sym[i].st_value);\n\t\t\tbreak;\n\n\t\tcase SHN_LIVEPATCH:\n\t\t\t/* Livepatch symbols are resolved by livepatch */\n\t\t\tbreak;\n\n\t\tcase SHN_UNDEF:\n\t\t\tksym = resolve_symbol_wait(mod, info, name);\n\t\t\t/* Ok if resolved.  */\n\t\t\tif (ksym && !IS_ERR(ksym)) {\n\t\t\t\tsym[i].st_value = kernel_symbol_value(ksym);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Ok if weak or ignored.  */\n\t\t\tif (!ksym &&\n\t\t\t    (ELF_ST_BIND(sym[i].st_info) == STB_WEAK ||\n\t\t\t     ignore_undef_symbol(info->hdr->e_machine, name)))\n\t\t\t\tbreak;\n\n\t\t\tret = PTR_ERR(ksym) ?: -ENOENT;\n\t\t\tpr_warn(\"%s: Unknown symbol %s (err %d)\\n\",\n\t\t\t\tmod->name, name, ret);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\t/* Divert to percpu allocation if a percpu var. */\n\t\t\tif (sym[i].st_shndx == info->index.pcpu)\n\t\t\t\tsecbase = (unsigned long)mod_percpu(mod);\n\t\t\telse\n\t\t\t\tsecbase = info->sechdrs[sym[i].st_shndx].sh_addr;\n\t\t\tsym[i].st_value += secbase;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int apply_relocations(struct module *mod, const struct load_info *info)\n{\n\tunsigned int i;\n\tint err = 0;\n\n\t/* Now do relocations. */\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tunsigned int infosec = info->sechdrs[i].sh_info;\n\n\t\t/* Not a valid relocation section? */\n\t\tif (infosec >= info->hdr->e_shnum)\n\t\t\tcontinue;\n\n\t\t/* Don't bother with non-allocated sections */\n\t\tif (!(info->sechdrs[infosec].sh_flags & SHF_ALLOC))\n\t\t\tcontinue;\n\n\t\tif (info->sechdrs[i].sh_flags & SHF_RELA_LIVEPATCH)\n\t\t\terr = klp_apply_section_relocs(mod, info->sechdrs,\n\t\t\t\t\t\t       info->secstrings,\n\t\t\t\t\t\t       info->strtab,\n\t\t\t\t\t\t       info->index.sym, i,\n\t\t\t\t\t\t       NULL);\n\t\telse if (info->sechdrs[i].sh_type == SHT_REL)\n\t\t\terr = apply_relocate(info->sechdrs, info->strtab,\n\t\t\t\t\t     info->index.sym, i, mod);\n\t\telse if (info->sechdrs[i].sh_type == SHT_RELA)\n\t\t\terr = apply_relocate_add(info->sechdrs, info->strtab,\n\t\t\t\t\t\t info->index.sym, i, mod);\n\t\tif (err < 0)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\n/* Additional bytes needed by arch in front of individual sections */\nunsigned int __weak arch_mod_section_prepend(struct module *mod,\n\t\t\t\t\t     unsigned int section)\n{\n\t/* default implementation just returns zero */\n\treturn 0;\n}\n\n/* Update size with this section: return offset. */\nstatic long get_offset(struct module *mod, unsigned int *size,\n\t\t       Elf_Shdr *sechdr, unsigned int section)\n{\n\tlong ret;\n\n\t*size += arch_mod_section_prepend(mod, section);\n\tret = ALIGN(*size, sechdr->sh_addralign ?: 1);\n\t*size = ret + sechdr->sh_size;\n\treturn ret;\n}\n\nstatic bool module_init_layout_section(const char *sname)\n{\n#ifndef CONFIG_MODULE_UNLOAD\n\tif (module_exit_section(sname))\n\t\treturn true;\n#endif\n\treturn module_init_section(sname);\n}\n\n/*\n * Lay out the SHF_ALLOC sections in a way not dissimilar to how ld\n * might -- code, read-only data, read-write data, small data.  Tally\n * sizes, and place the offsets into sh_entsize fields: high bit means it\n * belongs in init.\n */\nstatic void layout_sections(struct module *mod, struct load_info *info)\n{\n\tstatic unsigned long const masks[][2] = {\n\t\t/*\n\t\t * NOTE: all executable code must be the first section\n\t\t * in this array; otherwise modify the text_size\n\t\t * finder in the two loops below\n\t\t */\n\t\t{ SHF_EXECINSTR | SHF_ALLOC, ARCH_SHF_SMALL },\n\t\t{ SHF_ALLOC, SHF_WRITE | ARCH_SHF_SMALL },\n\t\t{ SHF_RO_AFTER_INIT | SHF_ALLOC, ARCH_SHF_SMALL },\n\t\t{ SHF_WRITE | SHF_ALLOC, ARCH_SHF_SMALL },\n\t\t{ ARCH_SHF_SMALL | SHF_ALLOC, 0 }\n\t};\n\tunsigned int m, i;\n\n\tfor (i = 0; i < info->hdr->e_shnum; i++)\n\t\tinfo->sechdrs[i].sh_entsize = ~0UL;\n\n\tpr_debug(\"Core section allocation order:\\n\");\n\tfor (m = 0; m < ARRAY_SIZE(masks); ++m) {\n\t\tfor (i = 0; i < info->hdr->e_shnum; ++i) {\n\t\t\tElf_Shdr *s = &info->sechdrs[i];\n\t\t\tconst char *sname = info->secstrings + s->sh_name;\n\n\t\t\tif ((s->sh_flags & masks[m][0]) != masks[m][0]\n\t\t\t    || (s->sh_flags & masks[m][1])\n\t\t\t    || s->sh_entsize != ~0UL\n\t\t\t    || module_init_layout_section(sname))\n\t\t\t\tcontinue;\n\t\t\ts->sh_entsize = get_offset(mod, &mod->core_layout.size, s, i);\n\t\t\tpr_debug(\"\\t%s\\n\", sname);\n\t\t}\n\t\tswitch (m) {\n\t\tcase 0: /* executable */\n\t\t\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\t\t\tmod->core_layout.text_size = mod->core_layout.size;\n\t\t\tbreak;\n\t\tcase 1: /* RO: text and ro-data */\n\t\t\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\t\t\tmod->core_layout.ro_size = mod->core_layout.size;\n\t\t\tbreak;\n\t\tcase 2: /* RO after init */\n\t\t\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\t\t\tmod->core_layout.ro_after_init_size = mod->core_layout.size;\n\t\t\tbreak;\n\t\tcase 4: /* whole core */\n\t\t\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tpr_debug(\"Init section allocation order:\\n\");\n\tfor (m = 0; m < ARRAY_SIZE(masks); ++m) {\n\t\tfor (i = 0; i < info->hdr->e_shnum; ++i) {\n\t\t\tElf_Shdr *s = &info->sechdrs[i];\n\t\t\tconst char *sname = info->secstrings + s->sh_name;\n\n\t\t\tif ((s->sh_flags & masks[m][0]) != masks[m][0]\n\t\t\t    || (s->sh_flags & masks[m][1])\n\t\t\t    || s->sh_entsize != ~0UL\n\t\t\t    || !module_init_layout_section(sname))\n\t\t\t\tcontinue;\n\t\t\ts->sh_entsize = (get_offset(mod, &mod->init_layout.size, s, i)\n\t\t\t\t\t | INIT_OFFSET_MASK);\n\t\t\tpr_debug(\"\\t%s\\n\", sname);\n\t\t}\n\t\tswitch (m) {\n\t\tcase 0: /* executable */\n\t\t\tmod->init_layout.size = debug_align(mod->init_layout.size);\n\t\t\tmod->init_layout.text_size = mod->init_layout.size;\n\t\t\tbreak;\n\t\tcase 1: /* RO: text and ro-data */\n\t\t\tmod->init_layout.size = debug_align(mod->init_layout.size);\n\t\t\tmod->init_layout.ro_size = mod->init_layout.size;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t/*\n\t\t\t * RO after init doesn't apply to init_layout (only\n\t\t\t * core_layout), so it just takes the value of ro_size.\n\t\t\t */\n\t\t\tmod->init_layout.ro_after_init_size = mod->init_layout.ro_size;\n\t\t\tbreak;\n\t\tcase 4: /* whole init */\n\t\t\tmod->init_layout.size = debug_align(mod->init_layout.size);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void set_license(struct module *mod, const char *license)\n{\n\tif (!license)\n\t\tlicense = \"unspecified\";\n\n\tif (!license_is_gpl_compatible(license)) {\n\t\tif (!test_taint(TAINT_PROPRIETARY_MODULE))\n\t\t\tpr_warn(\"%s: module license '%s' taints kernel.\\n\",\n\t\t\t\tmod->name, license);\n\t\tadd_taint_module(mod, TAINT_PROPRIETARY_MODULE,\n\t\t\t\t LOCKDEP_NOW_UNRELIABLE);\n\t}\n}\n\n/* Parse tag=value strings from .modinfo section */\nstatic char *next_string(char *string, unsigned long *secsize)\n{\n\t/* Skip non-zero chars */\n\twhile (string[0]) {\n\t\tstring++;\n\t\tif ((*secsize)-- <= 1)\n\t\t\treturn NULL;\n\t}\n\n\t/* Skip any zero padding. */\n\twhile (!string[0]) {\n\t\tstring++;\n\t\tif ((*secsize)-- <= 1)\n\t\t\treturn NULL;\n\t}\n\treturn string;\n}\n\nstatic char *get_next_modinfo(const struct load_info *info, const char *tag,\n\t\t\t      char *prev)\n{\n\tchar *p;\n\tunsigned int taglen = strlen(tag);\n\tElf_Shdr *infosec = &info->sechdrs[info->index.info];\n\tunsigned long size = infosec->sh_size;\n\n\t/*\n\t * get_modinfo() calls made before rewrite_section_headers()\n\t * must use sh_offset, as sh_addr isn't set!\n\t */\n\tchar *modinfo = (char *)info->hdr + infosec->sh_offset;\n\n\tif (prev) {\n\t\tsize -= prev - modinfo;\n\t\tmodinfo = next_string(prev, &size);\n\t}\n\n\tfor (p = modinfo; p; p = next_string(p, &size)) {\n\t\tif (strncmp(p, tag, taglen) == 0 && p[taglen] == '=')\n\t\t\treturn p + taglen + 1;\n\t}\n\treturn NULL;\n}\n\nstatic char *get_modinfo(const struct load_info *info, const char *tag)\n{\n\treturn get_next_modinfo(info, tag, NULL);\n}\n\nstatic void setup_modinfo(struct module *mod, struct load_info *info)\n{\n\tstruct module_attribute *attr;\n\tint i;\n\n\tfor (i = 0; (attr = modinfo_attrs[i]); i++) {\n\t\tif (attr->setup)\n\t\t\tattr->setup(mod, get_modinfo(info, attr->attr.name));\n\t}\n}\n\nstatic void free_modinfo(struct module *mod)\n{\n\tstruct module_attribute *attr;\n\tint i;\n\n\tfor (i = 0; (attr = modinfo_attrs[i]); i++) {\n\t\tif (attr->free)\n\t\t\tattr->free(mod);\n\t}\n}\n\n#ifdef CONFIG_KALLSYMS\n\n/* Lookup exported symbol in given range of kernel_symbols */\nstatic const struct kernel_symbol *lookup_exported_symbol(const char *name,\n\t\t\t\t\t\t\t  const struct kernel_symbol *start,\n\t\t\t\t\t\t\t  const struct kernel_symbol *stop)\n{\n\treturn bsearch(name, start, stop - start,\n\t\t\tsizeof(struct kernel_symbol), cmp_name);\n}\n\nstatic int is_exported(const char *name, unsigned long value,\n\t\t       const struct module *mod)\n{\n\tconst struct kernel_symbol *ks;\n\tif (!mod)\n\t\tks = lookup_exported_symbol(name, __start___ksymtab, __stop___ksymtab);\n\telse\n\t\tks = lookup_exported_symbol(name, mod->syms, mod->syms + mod->num_syms);\n\n\treturn ks != NULL && kernel_symbol_value(ks) == value;\n}\n\n/* As per nm */\nstatic char elf_type(const Elf_Sym *sym, const struct load_info *info)\n{\n\tconst Elf_Shdr *sechdrs = info->sechdrs;\n\n\tif (ELF_ST_BIND(sym->st_info) == STB_WEAK) {\n\t\tif (ELF_ST_TYPE(sym->st_info) == STT_OBJECT)\n\t\t\treturn 'v';\n\t\telse\n\t\t\treturn 'w';\n\t}\n\tif (sym->st_shndx == SHN_UNDEF)\n\t\treturn 'U';\n\tif (sym->st_shndx == SHN_ABS || sym->st_shndx == info->index.pcpu)\n\t\treturn 'a';\n\tif (sym->st_shndx >= SHN_LORESERVE)\n\t\treturn '?';\n\tif (sechdrs[sym->st_shndx].sh_flags & SHF_EXECINSTR)\n\t\treturn 't';\n\tif (sechdrs[sym->st_shndx].sh_flags & SHF_ALLOC\n\t    && sechdrs[sym->st_shndx].sh_type != SHT_NOBITS) {\n\t\tif (!(sechdrs[sym->st_shndx].sh_flags & SHF_WRITE))\n\t\t\treturn 'r';\n\t\telse if (sechdrs[sym->st_shndx].sh_flags & ARCH_SHF_SMALL)\n\t\t\treturn 'g';\n\t\telse\n\t\t\treturn 'd';\n\t}\n\tif (sechdrs[sym->st_shndx].sh_type == SHT_NOBITS) {\n\t\tif (sechdrs[sym->st_shndx].sh_flags & ARCH_SHF_SMALL)\n\t\t\treturn 's';\n\t\telse\n\t\t\treturn 'b';\n\t}\n\tif (strstarts(info->secstrings + sechdrs[sym->st_shndx].sh_name,\n\t\t      \".debug\")) {\n\t\treturn 'n';\n\t}\n\treturn '?';\n}\n\nstatic bool is_core_symbol(const Elf_Sym *src, const Elf_Shdr *sechdrs,\n\t\t\tunsigned int shnum, unsigned int pcpundx)\n{\n\tconst Elf_Shdr *sec;\n\n\tif (src->st_shndx == SHN_UNDEF\n\t    || src->st_shndx >= shnum\n\t    || !src->st_name)\n\t\treturn false;\n\n#ifdef CONFIG_KALLSYMS_ALL\n\tif (src->st_shndx == pcpundx)\n\t\treturn true;\n#endif\n\n\tsec = sechdrs + src->st_shndx;\n\tif (!(sec->sh_flags & SHF_ALLOC)\n#ifndef CONFIG_KALLSYMS_ALL\n\t    || !(sec->sh_flags & SHF_EXECINSTR)\n#endif\n\t    || (sec->sh_entsize & INIT_OFFSET_MASK))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * We only allocate and copy the strings needed by the parts of symtab\n * we keep.  This is simple, but has the effect of making multiple\n * copies of duplicates.  We could be more sophisticated, see\n * linux-kernel thread starting with\n * <73defb5e4bca04a6431392cc341112b1@localhost>.\n */\nstatic void layout_symtab(struct module *mod, struct load_info *info)\n{\n\tElf_Shdr *symsect = info->sechdrs + info->index.sym;\n\tElf_Shdr *strsect = info->sechdrs + info->index.str;\n\tconst Elf_Sym *src;\n\tunsigned int i, nsrc, ndst, strtab_size = 0;\n\n\t/* Put symbol section at end of init part of module. */\n\tsymsect->sh_flags |= SHF_ALLOC;\n\tsymsect->sh_entsize = get_offset(mod, &mod->init_layout.size, symsect,\n\t\t\t\t\t info->index.sym) | INIT_OFFSET_MASK;\n\tpr_debug(\"\\t%s\\n\", info->secstrings + symsect->sh_name);\n\n\tsrc = (void *)info->hdr + symsect->sh_offset;\n\tnsrc = symsect->sh_size / sizeof(*src);\n\n\t/* Compute total space required for the core symbols' strtab. */\n\tfor (ndst = i = 0; i < nsrc; i++) {\n\t\tif (i == 0 || is_livepatch_module(mod) ||\n\t\t    is_core_symbol(src+i, info->sechdrs, info->hdr->e_shnum,\n\t\t\t\t   info->index.pcpu)) {\n\t\t\tstrtab_size += strlen(&info->strtab[src[i].st_name])+1;\n\t\t\tndst++;\n\t\t}\n\t}\n\n\t/* Append room for core symbols at end of core part. */\n\tinfo->symoffs = ALIGN(mod->core_layout.size, symsect->sh_addralign ?: 1);\n\tinfo->stroffs = mod->core_layout.size = info->symoffs + ndst * sizeof(Elf_Sym);\n\tmod->core_layout.size += strtab_size;\n\tinfo->core_typeoffs = mod->core_layout.size;\n\tmod->core_layout.size += ndst * sizeof(char);\n\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\n\t/* Put string table section at end of init part of module. */\n\tstrsect->sh_flags |= SHF_ALLOC;\n\tstrsect->sh_entsize = get_offset(mod, &mod->init_layout.size, strsect,\n\t\t\t\t\t info->index.str) | INIT_OFFSET_MASK;\n\tpr_debug(\"\\t%s\\n\", info->secstrings + strsect->sh_name);\n\n\t/* We'll tack temporary mod_kallsyms on the end. */\n\tmod->init_layout.size = ALIGN(mod->init_layout.size,\n\t\t\t\t      __alignof__(struct mod_kallsyms));\n\tinfo->mod_kallsyms_init_off = mod->init_layout.size;\n\tmod->init_layout.size += sizeof(struct mod_kallsyms);\n\tinfo->init_typeoffs = mod->init_layout.size;\n\tmod->init_layout.size += nsrc * sizeof(char);\n\tmod->init_layout.size = debug_align(mod->init_layout.size);\n}\n\n/*\n * We use the full symtab and strtab which layout_symtab arranged to\n * be appended to the init section.  Later we switch to the cut-down\n * core-only ones.\n */\nstatic void add_kallsyms(struct module *mod, const struct load_info *info)\n{\n\tunsigned int i, ndst;\n\tconst Elf_Sym *src;\n\tElf_Sym *dst;\n\tchar *s;\n\tElf_Shdr *symsec = &info->sechdrs[info->index.sym];\n\n\t/* Set up to point into init section. */\n\tmod->kallsyms = mod->init_layout.base + info->mod_kallsyms_init_off;\n\n\tmod->kallsyms->symtab = (void *)symsec->sh_addr;\n\tmod->kallsyms->num_symtab = symsec->sh_size / sizeof(Elf_Sym);\n\t/* Make sure we get permanent strtab: don't use info->strtab. */\n\tmod->kallsyms->strtab = (void *)info->sechdrs[info->index.str].sh_addr;\n\tmod->kallsyms->typetab = mod->init_layout.base + info->init_typeoffs;\n\n\t/*\n\t * Now populate the cut down core kallsyms for after init\n\t * and set types up while we still have access to sections.\n\t */\n\tmod->core_kallsyms.symtab = dst = mod->core_layout.base + info->symoffs;\n\tmod->core_kallsyms.strtab = s = mod->core_layout.base + info->stroffs;\n\tmod->core_kallsyms.typetab = mod->core_layout.base + info->core_typeoffs;\n\tsrc = mod->kallsyms->symtab;\n\tfor (ndst = i = 0; i < mod->kallsyms->num_symtab; i++) {\n\t\tmod->kallsyms->typetab[i] = elf_type(src + i, info);\n\t\tif (i == 0 || is_livepatch_module(mod) ||\n\t\t    is_core_symbol(src+i, info->sechdrs, info->hdr->e_shnum,\n\t\t\t\t   info->index.pcpu)) {\n\t\t\tmod->core_kallsyms.typetab[ndst] =\n\t\t\t    mod->kallsyms->typetab[i];\n\t\t\tdst[ndst] = src[i];\n\t\t\tdst[ndst++].st_name = s - mod->core_kallsyms.strtab;\n\t\t\ts += strlcpy(s, &mod->kallsyms->strtab[src[i].st_name],\n\t\t\t\t     KSYM_NAME_LEN) + 1;\n\t\t}\n\t}\n\tmod->core_kallsyms.num_symtab = ndst;\n}\n#else\nstatic inline void layout_symtab(struct module *mod, struct load_info *info)\n{\n}\n\nstatic void add_kallsyms(struct module *mod, const struct load_info *info)\n{\n}\n#endif /* CONFIG_KALLSYMS */\n\nstatic void dynamic_debug_setup(struct module *mod, struct _ddebug *debug, unsigned int num)\n{\n\tif (!debug)\n\t\treturn;\n\tddebug_add_module(debug, num, mod->name);\n}\n\nstatic void dynamic_debug_remove(struct module *mod, struct _ddebug *debug)\n{\n\tif (debug)\n\t\tddebug_remove_module(mod->name);\n}\n\nvoid * __weak module_alloc(unsigned long size)\n{\n\treturn __vmalloc_node_range(size, 1, VMALLOC_START, VMALLOC_END,\n\t\t\tGFP_KERNEL, PAGE_KERNEL_EXEC, VM_FLUSH_RESET_PERMS,\n\t\t\tNUMA_NO_NODE, __builtin_return_address(0));\n}\n\nbool __weak module_init_section(const char *name)\n{\n\treturn strstarts(name, \".init\");\n}\n\nbool __weak module_exit_section(const char *name)\n{\n\treturn strstarts(name, \".exit\");\n}\n\n#ifdef CONFIG_DEBUG_KMEMLEAK\nstatic void kmemleak_load_module(const struct module *mod,\n\t\t\t\t const struct load_info *info)\n{\n\tunsigned int i;\n\n\t/* only scan the sections containing data */\n\tkmemleak_scan_area(mod, sizeof(struct module), GFP_KERNEL);\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\t/* Scan all writable sections that's not executable */\n\t\tif (!(info->sechdrs[i].sh_flags & SHF_ALLOC) ||\n\t\t    !(info->sechdrs[i].sh_flags & SHF_WRITE) ||\n\t\t    (info->sechdrs[i].sh_flags & SHF_EXECINSTR))\n\t\t\tcontinue;\n\n\t\tkmemleak_scan_area((void *)info->sechdrs[i].sh_addr,\n\t\t\t\t   info->sechdrs[i].sh_size, GFP_KERNEL);\n\t}\n}\n#else\nstatic inline void kmemleak_load_module(const struct module *mod,\n\t\t\t\t\tconst struct load_info *info)\n{\n}\n#endif\n\n#ifdef CONFIG_MODULE_SIG\nstatic int module_sig_check(struct load_info *info, int flags)\n{\n\tint err = -ENODATA;\n\tconst unsigned long markerlen = sizeof(MODULE_SIG_STRING) - 1;\n\tconst char *reason;\n\tconst void *mod = info->hdr;\n\n\t/*\n\t * Require flags == 0, as a module with version information\n\t * removed is no longer the module that was signed\n\t */\n\tif (flags == 0 &&\n\t    info->len > markerlen &&\n\t    memcmp(mod + info->len - markerlen, MODULE_SIG_STRING, markerlen) == 0) {\n\t\t/* We truncate the module to discard the signature */\n\t\tinfo->len -= markerlen;\n\t\terr = mod_verify_sig(mod, info);\n\t\tif (!err) {\n\t\t\tinfo->sig_ok = true;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * We don't permit modules to be loaded into the trusted kernels\n\t * without a valid signature on them, but if we're not enforcing,\n\t * certain errors are non-fatal.\n\t */\n\tswitch (err) {\n\tcase -ENODATA:\n\t\treason = \"unsigned module\";\n\t\tbreak;\n\tcase -ENOPKG:\n\t\treason = \"module with unsupported crypto\";\n\t\tbreak;\n\tcase -ENOKEY:\n\t\treason = \"module with unavailable key\";\n\t\tbreak;\n\n\tdefault:\n\t\t/*\n\t\t * All other errors are fatal, including lack of memory,\n\t\t * unparseable signatures, and signature check failures --\n\t\t * even if signatures aren't required.\n\t\t */\n\t\treturn err;\n\t}\n\n\tif (is_module_sig_enforced()) {\n\t\tpr_notice(\"Loading of %s is rejected\\n\", reason);\n\t\treturn -EKEYREJECTED;\n\t}\n\n\treturn security_locked_down(LOCKDOWN_MODULE_SIGNATURE);\n}\n#else /* !CONFIG_MODULE_SIG */\nstatic int module_sig_check(struct load_info *info, int flags)\n{\n\treturn 0;\n}\n#endif /* !CONFIG_MODULE_SIG */\n\nstatic int validate_section_offset(struct load_info *info, Elf_Shdr *shdr)\n{\n\tunsigned long secend;\n\n\t/*\n\t * Check for both overflow and offset/size being\n\t * too large.\n\t */\n\tsecend = shdr->sh_offset + shdr->sh_size;\n\tif (secend < shdr->sh_offset || secend > info->len)\n\t\treturn -ENOEXEC;\n\n\treturn 0;\n}\n\n/*\n * Sanity checks against invalid binaries, wrong arch, weird elf version.\n *\n * Also do basic validity checks against section offsets and sizes, the\n * section name string table, and the indices used for it (sh_name).\n */\nstatic int elf_validity_check(struct load_info *info)\n{\n\tunsigned int i;\n\tElf_Shdr *shdr, *strhdr;\n\tint err;\n\n\tif (info->len < sizeof(*(info->hdr)))\n\t\treturn -ENOEXEC;\n\n\tif (memcmp(info->hdr->e_ident, ELFMAG, SELFMAG) != 0\n\t    || info->hdr->e_type != ET_REL\n\t    || !elf_check_arch(info->hdr)\n\t    || info->hdr->e_shentsize != sizeof(Elf_Shdr))\n\t\treturn -ENOEXEC;\n\n\t/*\n\t * e_shnum is 16 bits, and sizeof(Elf_Shdr) is\n\t * known and small. So e_shnum * sizeof(Elf_Shdr)\n\t * will not overflow unsigned long on any platform.\n\t */\n\tif (info->hdr->e_shoff >= info->len\n\t    || (info->hdr->e_shnum * sizeof(Elf_Shdr) >\n\t\tinfo->len - info->hdr->e_shoff))\n\t\treturn -ENOEXEC;\n\n\tinfo->sechdrs = (void *)info->hdr + info->hdr->e_shoff;\n\n\t/*\n\t * Verify if the section name table index is valid.\n\t */\n\tif (info->hdr->e_shstrndx == SHN_UNDEF\n\t    || info->hdr->e_shstrndx >= info->hdr->e_shnum)\n\t\treturn -ENOEXEC;\n\n\tstrhdr = &info->sechdrs[info->hdr->e_shstrndx];\n\terr = validate_section_offset(info, strhdr);\n\tif (err < 0)\n\t\treturn err;\n\n\t/*\n\t * The section name table must be NUL-terminated, as required\n\t * by the spec. This makes strcmp and pr_* calls that access\n\t * strings in the section safe.\n\t */\n\tinfo->secstrings = (void *)info->hdr + strhdr->sh_offset;\n\tif (info->secstrings[strhdr->sh_size - 1] != '\\0')\n\t\treturn -ENOEXEC;\n\n\t/*\n\t * The code assumes that section 0 has a length of zero and\n\t * an addr of zero, so check for it.\n\t */\n\tif (info->sechdrs[0].sh_type != SHT_NULL\n\t    || info->sechdrs[0].sh_size != 0\n\t    || info->sechdrs[0].sh_addr != 0)\n\t\treturn -ENOEXEC;\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tshdr = &info->sechdrs[i];\n\t\tswitch (shdr->sh_type) {\n\t\tcase SHT_NULL:\n\t\tcase SHT_NOBITS:\n\t\t\tcontinue;\n\t\tcase SHT_SYMTAB:\n\t\t\tif (shdr->sh_link == SHN_UNDEF\n\t\t\t    || shdr->sh_link >= info->hdr->e_shnum)\n\t\t\t\treturn -ENOEXEC;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\terr = validate_section_offset(info, shdr);\n\t\t\tif (err < 0) {\n\t\t\t\tpr_err(\"Invalid ELF section in module (section %u type %u)\\n\",\n\t\t\t\t\ti, shdr->sh_type);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\tif (shdr->sh_flags & SHF_ALLOC) {\n\t\t\t\tif (shdr->sh_name >= strhdr->sh_size) {\n\t\t\t\t\tpr_err(\"Invalid ELF section name in module (section %u type %u)\\n\",\n\t\t\t\t\t       i, shdr->sh_type);\n\t\t\t\t\treturn -ENOEXEC;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#define COPY_CHUNK_SIZE (16*PAGE_SIZE)\n\nstatic int copy_chunked_from_user(void *dst, const void __user *usrc, unsigned long len)\n{\n\tdo {\n\t\tunsigned long n = min(len, COPY_CHUNK_SIZE);\n\n\t\tif (copy_from_user(dst, usrc, n) != 0)\n\t\t\treturn -EFAULT;\n\t\tcond_resched();\n\t\tdst += n;\n\t\tusrc += n;\n\t\tlen -= n;\n\t} while (len);\n\treturn 0;\n}\n\n#ifdef CONFIG_LIVEPATCH\nstatic int check_modinfo_livepatch(struct module *mod, struct load_info *info)\n{\n\tif (get_modinfo(info, \"livepatch\")) {\n\t\tmod->klp = true;\n\t\tadd_taint_module(mod, TAINT_LIVEPATCH, LOCKDEP_STILL_OK);\n\t\tpr_notice_once(\"%s: tainting kernel with TAINT_LIVEPATCH\\n\",\n\t\t\t       mod->name);\n\t}\n\n\treturn 0;\n}\n#else /* !CONFIG_LIVEPATCH */\nstatic int check_modinfo_livepatch(struct module *mod, struct load_info *info)\n{\n\tif (get_modinfo(info, \"livepatch\")) {\n\t\tpr_err(\"%s: module is marked as livepatch module, but livepatch support is disabled\",\n\t\t       mod->name);\n\t\treturn -ENOEXEC;\n\t}\n\n\treturn 0;\n}\n#endif /* CONFIG_LIVEPATCH */\n\nstatic void check_modinfo_retpoline(struct module *mod, struct load_info *info)\n{\n\tif (retpoline_module_ok(get_modinfo(info, \"retpoline\")))\n\t\treturn;\n\n\tpr_warn(\"%s: loading module not compiled with retpoline compiler.\\n\",\n\t\tmod->name);\n}\n\n/* Sets info->hdr and info->len. */\nstatic int copy_module_from_user(const void __user *umod, unsigned long len,\n\t\t\t\t  struct load_info *info)\n{\n\tint err;\n\n\tinfo->len = len;\n\tif (info->len < sizeof(*(info->hdr)))\n\t\treturn -ENOEXEC;\n\n\terr = security_kernel_load_data(LOADING_MODULE, true);\n\tif (err)\n\t\treturn err;\n\n\t/* Suck in entire file: we'll want most of it. */\n\tinfo->hdr = __vmalloc(info->len, GFP_KERNEL | __GFP_NOWARN);\n\tif (!info->hdr)\n\t\treturn -ENOMEM;\n\n\tif (copy_chunked_from_user(info->hdr, umod, info->len) != 0) {\n\t\terr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\terr = security_kernel_post_load_data((char *)info->hdr, info->len,\n\t\t\t\t\t     LOADING_MODULE, \"init_module\");\nout:\n\tif (err)\n\t\tvfree(info->hdr);\n\n\treturn err;\n}\n\nstatic void free_copy(struct load_info *info)\n{\n\tvfree(info->hdr);\n}\n\nstatic int rewrite_section_headers(struct load_info *info, int flags)\n{\n\tunsigned int i;\n\n\t/* This should always be true, but let's be sure. */\n\tinfo->sechdrs[0].sh_addr = 0;\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tElf_Shdr *shdr = &info->sechdrs[i];\n\n\t\t/*\n\t\t * Mark all sections sh_addr with their address in the\n\t\t * temporary image.\n\t\t */\n\t\tshdr->sh_addr = (size_t)info->hdr + shdr->sh_offset;\n\n\t}\n\n\t/* Track but don't keep modinfo and version sections. */\n\tinfo->sechdrs[info->index.vers].sh_flags &= ~(unsigned long)SHF_ALLOC;\n\tinfo->sechdrs[info->index.info].sh_flags &= ~(unsigned long)SHF_ALLOC;\n\n\treturn 0;\n}\n\n/*\n * Set up our basic convenience variables (pointers to section headers,\n * search for module section index etc), and do some basic section\n * verification.\n *\n * Set info->mod to the temporary copy of the module in info->hdr. The final one\n * will be allocated in move_module().\n */\nstatic int setup_load_info(struct load_info *info, int flags)\n{\n\tunsigned int i;\n\n\t/* Try to find a name early so we can log errors with a module name */\n\tinfo->index.info = find_sec(info, \".modinfo\");\n\tif (info->index.info)\n\t\tinfo->name = get_modinfo(info, \"name\");\n\n\t/* Find internal symbols and strings. */\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tif (info->sechdrs[i].sh_type == SHT_SYMTAB) {\n\t\t\tinfo->index.sym = i;\n\t\t\tinfo->index.str = info->sechdrs[i].sh_link;\n\t\t\tinfo->strtab = (char *)info->hdr\n\t\t\t\t+ info->sechdrs[info->index.str].sh_offset;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (info->index.sym == 0) {\n\t\tpr_warn(\"%s: module has no symbols (stripped?)\\n\",\n\t\t\tinfo->name ?: \"(missing .modinfo section or name field)\");\n\t\treturn -ENOEXEC;\n\t}\n\n\tinfo->index.mod = find_sec(info, \".gnu.linkonce.this_module\");\n\tif (!info->index.mod) {\n\t\tpr_warn(\"%s: No module found in object\\n\",\n\t\t\tinfo->name ?: \"(missing .modinfo section or name field)\");\n\t\treturn -ENOEXEC;\n\t}\n\t/* This is temporary: point mod into copy of data. */\n\tinfo->mod = (void *)info->hdr + info->sechdrs[info->index.mod].sh_offset;\n\n\t/*\n\t * If we didn't load the .modinfo 'name' field earlier, fall back to\n\t * on-disk struct mod 'name' field.\n\t */\n\tif (!info->name)\n\t\tinfo->name = info->mod->name;\n\n\tif (flags & MODULE_INIT_IGNORE_MODVERSIONS)\n\t\tinfo->index.vers = 0; /* Pretend no __versions section! */\n\telse\n\t\tinfo->index.vers = find_sec(info, \"__versions\");\n\n\tinfo->index.pcpu = find_pcpusec(info);\n\n\treturn 0;\n}\n\nstatic int check_modinfo(struct module *mod, struct load_info *info, int flags)\n{\n\tconst char *modmagic = get_modinfo(info, \"vermagic\");\n\tint err;\n\n\tif (flags & MODULE_INIT_IGNORE_VERMAGIC)\n\t\tmodmagic = NULL;\n\n\t/* This is allowed: modprobe --force will invalidate it. */\n\tif (!modmagic) {\n\t\terr = try_to_force_load(mod, \"bad vermagic\");\n\t\tif (err)\n\t\t\treturn err;\n\t} else if (!same_magic(modmagic, vermagic, info->index.vers)) {\n\t\tpr_err(\"%s: version magic '%s' should be '%s'\\n\",\n\t\t       info->name, modmagic, vermagic);\n\t\treturn -ENOEXEC;\n\t}\n\n\tif (!get_modinfo(info, \"intree\")) {\n\t\tif (!test_taint(TAINT_OOT_MODULE))\n\t\t\tpr_warn(\"%s: loading out-of-tree module taints kernel.\\n\",\n\t\t\t\tmod->name);\n\t\tadd_taint_module(mod, TAINT_OOT_MODULE, LOCKDEP_STILL_OK);\n\t}\n\n\tcheck_modinfo_retpoline(mod, info);\n\n\tif (get_modinfo(info, \"staging\")) {\n\t\tadd_taint_module(mod, TAINT_CRAP, LOCKDEP_STILL_OK);\n\t\tpr_warn(\"%s: module is from the staging directory, the quality \"\n\t\t\t\"is unknown, you have been warned.\\n\", mod->name);\n\t}\n\n\terr = check_modinfo_livepatch(mod, info);\n\tif (err)\n\t\treturn err;\n\n\t/* Set up license info based on the info section */\n\tset_license(mod, get_modinfo(info, \"license\"));\n\n\treturn 0;\n}\n\nstatic int find_module_sections(struct module *mod, struct load_info *info)\n{\n\tmod->kp = section_objs(info, \"__param\",\n\t\t\t       sizeof(*mod->kp), &mod->num_kp);\n\tmod->syms = section_objs(info, \"__ksymtab\",\n\t\t\t\t sizeof(*mod->syms), &mod->num_syms);\n\tmod->crcs = section_addr(info, \"__kcrctab\");\n\tmod->gpl_syms = section_objs(info, \"__ksymtab_gpl\",\n\t\t\t\t     sizeof(*mod->gpl_syms),\n\t\t\t\t     &mod->num_gpl_syms);\n\tmod->gpl_crcs = section_addr(info, \"__kcrctab_gpl\");\n\n#ifdef CONFIG_CONSTRUCTORS\n\tmod->ctors = section_objs(info, \".ctors\",\n\t\t\t\t  sizeof(*mod->ctors), &mod->num_ctors);\n\tif (!mod->ctors)\n\t\tmod->ctors = section_objs(info, \".init_array\",\n\t\t\t\tsizeof(*mod->ctors), &mod->num_ctors);\n\telse if (find_sec(info, \".init_array\")) {\n\t\t/*\n\t\t * This shouldn't happen with same compiler and binutils\n\t\t * building all parts of the module.\n\t\t */\n\t\tpr_warn(\"%s: has both .ctors and .init_array.\\n\",\n\t\t       mod->name);\n\t\treturn -EINVAL;\n\t}\n#endif\n\n\tmod->noinstr_text_start = section_objs(info, \".noinstr.text\", 1,\n\t\t\t\t\t\t&mod->noinstr_text_size);\n\n#ifdef CONFIG_TRACEPOINTS\n\tmod->tracepoints_ptrs = section_objs(info, \"__tracepoints_ptrs\",\n\t\t\t\t\t     sizeof(*mod->tracepoints_ptrs),\n\t\t\t\t\t     &mod->num_tracepoints);\n#endif\n#ifdef CONFIG_TREE_SRCU\n\tmod->srcu_struct_ptrs = section_objs(info, \"___srcu_struct_ptrs\",\n\t\t\t\t\t     sizeof(*mod->srcu_struct_ptrs),\n\t\t\t\t\t     &mod->num_srcu_structs);\n#endif\n#ifdef CONFIG_BPF_EVENTS\n\tmod->bpf_raw_events = section_objs(info, \"__bpf_raw_tp_map\",\n\t\t\t\t\t   sizeof(*mod->bpf_raw_events),\n\t\t\t\t\t   &mod->num_bpf_raw_events);\n#endif\n#ifdef CONFIG_DEBUG_INFO_BTF_MODULES\n\tmod->btf_data = any_section_objs(info, \".BTF\", 1, &mod->btf_data_size);\n#endif\n#ifdef CONFIG_JUMP_LABEL\n\tmod->jump_entries = section_objs(info, \"__jump_table\",\n\t\t\t\t\tsizeof(*mod->jump_entries),\n\t\t\t\t\t&mod->num_jump_entries);\n#endif\n#ifdef CONFIG_EVENT_TRACING\n\tmod->trace_events = section_objs(info, \"_ftrace_events\",\n\t\t\t\t\t sizeof(*mod->trace_events),\n\t\t\t\t\t &mod->num_trace_events);\n\tmod->trace_evals = section_objs(info, \"_ftrace_eval_map\",\n\t\t\t\t\tsizeof(*mod->trace_evals),\n\t\t\t\t\t&mod->num_trace_evals);\n#endif\n#ifdef CONFIG_TRACING\n\tmod->trace_bprintk_fmt_start = section_objs(info, \"__trace_printk_fmt\",\n\t\t\t\t\t sizeof(*mod->trace_bprintk_fmt_start),\n\t\t\t\t\t &mod->num_trace_bprintk_fmt);\n#endif\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n\t/* sechdrs[0].sh_size is always zero */\n\tmod->ftrace_callsites = section_objs(info, FTRACE_CALLSITE_SECTION,\n\t\t\t\t\t     sizeof(*mod->ftrace_callsites),\n\t\t\t\t\t     &mod->num_ftrace_callsites);\n#endif\n#ifdef CONFIG_FUNCTION_ERROR_INJECTION\n\tmod->ei_funcs = section_objs(info, \"_error_injection_whitelist\",\n\t\t\t\t\t    sizeof(*mod->ei_funcs),\n\t\t\t\t\t    &mod->num_ei_funcs);\n#endif\n#ifdef CONFIG_KPROBES\n\tmod->kprobes_text_start = section_objs(info, \".kprobes.text\", 1,\n\t\t\t\t\t\t&mod->kprobes_text_size);\n\tmod->kprobe_blacklist = section_objs(info, \"_kprobe_blacklist\",\n\t\t\t\t\t\tsizeof(unsigned long),\n\t\t\t\t\t\t&mod->num_kprobe_blacklist);\n#endif\n#ifdef CONFIG_HAVE_STATIC_CALL_INLINE\n\tmod->static_call_sites = section_objs(info, \".static_call_sites\",\n\t\t\t\t\t      sizeof(*mod->static_call_sites),\n\t\t\t\t\t      &mod->num_static_call_sites);\n#endif\n\tmod->extable = section_objs(info, \"__ex_table\",\n\t\t\t\t    sizeof(*mod->extable), &mod->num_exentries);\n\n\tif (section_addr(info, \"__obsparm\"))\n\t\tpr_warn(\"%s: Ignoring obsolete parameters\\n\", mod->name);\n\n\tinfo->debug = section_objs(info, \"__dyndbg\",\n\t\t\t\t   sizeof(*info->debug), &info->num_debug);\n\n\treturn 0;\n}\n\nstatic int move_module(struct module *mod, struct load_info *info)\n{\n\tint i;\n\tvoid *ptr;\n\n\t/* Do the allocs. */\n\tptr = module_alloc(mod->core_layout.size);\n\t/*\n\t * The pointer to this block is stored in the module structure\n\t * which is inside the block. Just mark it as not being a\n\t * leak.\n\t */\n\tkmemleak_not_leak(ptr);\n\tif (!ptr)\n\t\treturn -ENOMEM;\n\n\tmemset(ptr, 0, mod->core_layout.size);\n\tmod->core_layout.base = ptr;\n\n\tif (mod->init_layout.size) {\n\t\tptr = module_alloc(mod->init_layout.size);\n\t\t/*\n\t\t * The pointer to this block is stored in the module structure\n\t\t * which is inside the block. This block doesn't need to be\n\t\t * scanned as it contains data and code that will be freed\n\t\t * after the module is initialized.\n\t\t */\n\t\tkmemleak_ignore(ptr);\n\t\tif (!ptr) {\n\t\t\tmodule_memfree(mod->core_layout.base);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tmemset(ptr, 0, mod->init_layout.size);\n\t\tmod->init_layout.base = ptr;\n\t} else\n\t\tmod->init_layout.base = NULL;\n\n\t/* Transfer each section which specifies SHF_ALLOC */\n\tpr_debug(\"final section addresses:\\n\");\n\tfor (i = 0; i < info->hdr->e_shnum; i++) {\n\t\tvoid *dest;\n\t\tElf_Shdr *shdr = &info->sechdrs[i];\n\n\t\tif (!(shdr->sh_flags & SHF_ALLOC))\n\t\t\tcontinue;\n\n\t\tif (shdr->sh_entsize & INIT_OFFSET_MASK)\n\t\t\tdest = mod->init_layout.base\n\t\t\t\t+ (shdr->sh_entsize & ~INIT_OFFSET_MASK);\n\t\telse\n\t\t\tdest = mod->core_layout.base + shdr->sh_entsize;\n\n\t\tif (shdr->sh_type != SHT_NOBITS)\n\t\t\tmemcpy(dest, (void *)shdr->sh_addr, shdr->sh_size);\n\t\t/* Update sh_addr to point to copy in image. */\n\t\tshdr->sh_addr = (unsigned long)dest;\n\t\tpr_debug(\"\\t0x%lx %s\\n\",\n\t\t\t (long)shdr->sh_addr, info->secstrings + shdr->sh_name);\n\t}\n\n\treturn 0;\n}\n\nstatic int check_module_license_and_versions(struct module *mod)\n{\n\tint prev_taint = test_taint(TAINT_PROPRIETARY_MODULE);\n\n\t/*\n\t * ndiswrapper is under GPL by itself, but loads proprietary modules.\n\t * Don't use add_taint_module(), as it would prevent ndiswrapper from\n\t * using GPL-only symbols it needs.\n\t */\n\tif (strcmp(mod->name, \"ndiswrapper\") == 0)\n\t\tadd_taint(TAINT_PROPRIETARY_MODULE, LOCKDEP_NOW_UNRELIABLE);\n\n\t/* driverloader was caught wrongly pretending to be under GPL */\n\tif (strcmp(mod->name, \"driverloader\") == 0)\n\t\tadd_taint_module(mod, TAINT_PROPRIETARY_MODULE,\n\t\t\t\t LOCKDEP_NOW_UNRELIABLE);\n\n\t/* lve claims to be GPL but upstream won't provide source */\n\tif (strcmp(mod->name, \"lve\") == 0)\n\t\tadd_taint_module(mod, TAINT_PROPRIETARY_MODULE,\n\t\t\t\t LOCKDEP_NOW_UNRELIABLE);\n\n\tif (!prev_taint && test_taint(TAINT_PROPRIETARY_MODULE))\n\t\tpr_warn(\"%s: module license taints kernel.\\n\", mod->name);\n\n#ifdef CONFIG_MODVERSIONS\n\tif ((mod->num_syms && !mod->crcs) ||\n\t    (mod->num_gpl_syms && !mod->gpl_crcs)) {\n\t\treturn try_to_force_load(mod,\n\t\t\t\t\t \"no versions for exported symbols\");\n\t}\n#endif\n\treturn 0;\n}\n\nstatic void flush_module_icache(const struct module *mod)\n{\n\t/*\n\t * Flush the instruction cache, since we've played with text.\n\t * Do it before processing of module parameters, so the module\n\t * can provide parameter accessor functions of its own.\n\t */\n\tif (mod->init_layout.base)\n\t\tflush_icache_range((unsigned long)mod->init_layout.base,\n\t\t\t\t   (unsigned long)mod->init_layout.base\n\t\t\t\t   + mod->init_layout.size);\n\tflush_icache_range((unsigned long)mod->core_layout.base,\n\t\t\t   (unsigned long)mod->core_layout.base + mod->core_layout.size);\n}\n\nint __weak module_frob_arch_sections(Elf_Ehdr *hdr,\n\t\t\t\t     Elf_Shdr *sechdrs,\n\t\t\t\t     char *secstrings,\n\t\t\t\t     struct module *mod)\n{\n\treturn 0;\n}\n\n/* module_blacklist is a comma-separated list of module names */\nstatic char *module_blacklist;\nstatic bool blacklisted(const char *module_name)\n{\n\tconst char *p;\n\tsize_t len;\n\n\tif (!module_blacklist)\n\t\treturn false;\n\n\tfor (p = module_blacklist; *p; p += len) {\n\t\tlen = strcspn(p, \",\");\n\t\tif (strlen(module_name) == len && !memcmp(module_name, p, len))\n\t\t\treturn true;\n\t\tif (p[len] == ',')\n\t\t\tlen++;\n\t}\n\treturn false;\n}\ncore_param(module_blacklist, module_blacklist, charp, 0400);\n\nstatic struct module *layout_and_allocate(struct load_info *info, int flags)\n{\n\tstruct module *mod;\n\tunsigned int ndx;\n\tint err;\n\n\terr = check_modinfo(info->mod, info, flags);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* Allow arches to frob section contents and sizes.  */\n\terr = module_frob_arch_sections(info->hdr, info->sechdrs,\n\t\t\t\t\tinfo->secstrings, info->mod);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\terr = module_enforce_rwx_sections(info->hdr, info->sechdrs,\n\t\t\t\t\t  info->secstrings, info->mod);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\t/* We will do a special allocation for per-cpu sections later. */\n\tinfo->sechdrs[info->index.pcpu].sh_flags &= ~(unsigned long)SHF_ALLOC;\n\n\t/*\n\t * Mark ro_after_init section with SHF_RO_AFTER_INIT so that\n\t * layout_sections() can put it in the right place.\n\t * Note: ro_after_init sections also have SHF_{WRITE,ALLOC} set.\n\t */\n\tndx = find_sec(info, \".data..ro_after_init\");\n\tif (ndx)\n\t\tinfo->sechdrs[ndx].sh_flags |= SHF_RO_AFTER_INIT;\n\t/*\n\t * Mark the __jump_table section as ro_after_init as well: these data\n\t * structures are never modified, with the exception of entries that\n\t * refer to code in the __init section, which are annotated as such\n\t * at module load time.\n\t */\n\tndx = find_sec(info, \"__jump_table\");\n\tif (ndx)\n\t\tinfo->sechdrs[ndx].sh_flags |= SHF_RO_AFTER_INIT;\n\n\t/*\n\t * Determine total sizes, and put offsets in sh_entsize.  For now\n\t * this is done generically; there doesn't appear to be any\n\t * special cases for the architectures.\n\t */\n\tlayout_sections(info->mod, info);\n\tlayout_symtab(info->mod, info);\n\n\t/* Allocate and move to the final place */\n\terr = move_module(info->mod, info);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* Module has been copied to its final place now: return it. */\n\tmod = (void *)info->sechdrs[info->index.mod].sh_addr;\n\tkmemleak_load_module(mod, info);\n\treturn mod;\n}\n\n/* mod is no longer valid after this! */\nstatic void module_deallocate(struct module *mod, struct load_info *info)\n{\n\tpercpu_modfree(mod);\n\tmodule_arch_freeing_init(mod);\n\tmodule_memfree(mod->init_layout.base);\n\tmodule_memfree(mod->core_layout.base);\n}\n\nint __weak module_finalize(const Elf_Ehdr *hdr,\n\t\t\t   const Elf_Shdr *sechdrs,\n\t\t\t   struct module *me)\n{\n\treturn 0;\n}\n\nstatic int post_relocation(struct module *mod, const struct load_info *info)\n{\n\t/* Sort exception table now relocations are done. */\n\tsort_extable(mod->extable, mod->extable + mod->num_exentries);\n\n\t/* Copy relocated percpu area over. */\n\tpercpu_modcopy(mod, (void *)info->sechdrs[info->index.pcpu].sh_addr,\n\t\t       info->sechdrs[info->index.pcpu].sh_size);\n\n\t/* Setup kallsyms-specific fields. */\n\tadd_kallsyms(mod, info);\n\n\t/* Arch-specific module finalizing. */\n\treturn module_finalize(info->hdr, info->sechdrs, mod);\n}\n\n/* Is this module of this name done loading?  No locks held. */\nstatic bool finished_loading(const char *name)\n{\n\tstruct module *mod;\n\tbool ret;\n\n\t/*\n\t * The module_mutex should not be a heavily contended lock;\n\t * if we get the occasional sleep here, we'll go an extra iteration\n\t * in the wait_event_interruptible(), which is harmless.\n\t */\n\tsched_annotate_sleep();\n\tmutex_lock(&module_mutex);\n\tmod = find_module_all(name, strlen(name), true);\n\tret = !mod || mod->state == MODULE_STATE_LIVE;\n\tmutex_unlock(&module_mutex);\n\n\treturn ret;\n}\n\n/* Call module constructors. */\nstatic void do_mod_ctors(struct module *mod)\n{\n#ifdef CONFIG_CONSTRUCTORS\n\tunsigned long i;\n\n\tfor (i = 0; i < mod->num_ctors; i++)\n\t\tmod->ctors[i]();\n#endif\n}\n\n/* For freeing module_init on success, in case kallsyms traversing */\nstruct mod_initfree {\n\tstruct llist_node node;\n\tvoid *module_init;\n};\n\nstatic void do_free_init(struct work_struct *w)\n{\n\tstruct llist_node *pos, *n, *list;\n\tstruct mod_initfree *initfree;\n\n\tlist = llist_del_all(&init_free_list);\n\n\tsynchronize_rcu();\n\n\tllist_for_each_safe(pos, n, list) {\n\t\tinitfree = container_of(pos, struct mod_initfree, node);\n\t\tmodule_memfree(initfree->module_init);\n\t\tkfree(initfree);\n\t}\n}\n\n/*\n * This is where the real work happens.\n *\n * Keep it uninlined to provide a reliable breakpoint target, e.g. for the gdb\n * helper command 'lx-symbols'.\n */\nstatic noinline int do_init_module(struct module *mod)\n{\n\tint ret = 0;\n\tstruct mod_initfree *freeinit;\n\n\tfreeinit = kmalloc(sizeof(*freeinit), GFP_KERNEL);\n\tif (!freeinit) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tfreeinit->module_init = mod->init_layout.base;\n\n\t/*\n\t * We want to find out whether @mod uses async during init.  Clear\n\t * PF_USED_ASYNC.  async_schedule*() will set it.\n\t */\n\tcurrent->flags &= ~PF_USED_ASYNC;\n\n\tdo_mod_ctors(mod);\n\t/* Start the module */\n\tif (mod->init != NULL)\n\t\tret = do_one_initcall(mod->init);\n\tif (ret < 0) {\n\t\tgoto fail_free_freeinit;\n\t}\n\tif (ret > 0) {\n\t\tpr_warn(\"%s: '%s'->init suspiciously returned %d, it should \"\n\t\t\t\"follow 0/-E convention\\n\"\n\t\t\t\"%s: loading module anyway...\\n\",\n\t\t\t__func__, mod->name, ret, __func__);\n\t\tdump_stack();\n\t}\n\n\t/* Now it's a first class citizen! */\n\tmod->state = MODULE_STATE_LIVE;\n\tblocking_notifier_call_chain(&module_notify_list,\n\t\t\t\t     MODULE_STATE_LIVE, mod);\n\n\t/* Delay uevent until module has finished its init routine */\n\tkobject_uevent(&mod->mkobj.kobj, KOBJ_ADD);\n\n\t/*\n\t * We need to finish all async code before the module init sequence\n\t * is done.  This has potential to deadlock.  For example, a newly\n\t * detected block device can trigger request_module() of the\n\t * default iosched from async probing task.  Once userland helper\n\t * reaches here, async_synchronize_full() will wait on the async\n\t * task waiting on request_module() and deadlock.\n\t *\n\t * This deadlock is avoided by perfomring async_synchronize_full()\n\t * iff module init queued any async jobs.  This isn't a full\n\t * solution as it will deadlock the same if module loading from\n\t * async jobs nests more than once; however, due to the various\n\t * constraints, this hack seems to be the best option for now.\n\t * Please refer to the following thread for details.\n\t *\n\t * http://thread.gmane.org/gmane.linux.kernel/1420814\n\t */\n\tif (!mod->async_probe_requested && (current->flags & PF_USED_ASYNC))\n\t\tasync_synchronize_full();\n\n\tftrace_free_mem(mod, mod->init_layout.base, mod->init_layout.base +\n\t\t\tmod->init_layout.size);\n\tmutex_lock(&module_mutex);\n\t/* Drop initial reference. */\n\tmodule_put(mod);\n\ttrim_init_extable(mod);\n#ifdef CONFIG_KALLSYMS\n\t/* Switch to core kallsyms now init is done: kallsyms may be walking! */\n\trcu_assign_pointer(mod->kallsyms, &mod->core_kallsyms);\n#endif\n\tmodule_enable_ro(mod, true);\n\tmod_tree_remove_init(mod);\n\tmodule_arch_freeing_init(mod);\n\tmod->init_layout.base = NULL;\n\tmod->init_layout.size = 0;\n\tmod->init_layout.ro_size = 0;\n\tmod->init_layout.ro_after_init_size = 0;\n\tmod->init_layout.text_size = 0;\n#ifdef CONFIG_DEBUG_INFO_BTF_MODULES\n\t/* .BTF is not SHF_ALLOC and will get removed, so sanitize pointer */\n\tmod->btf_data = NULL;\n#endif\n\t/*\n\t * We want to free module_init, but be aware that kallsyms may be\n\t * walking this with preempt disabled.  In all the failure paths, we\n\t * call synchronize_rcu(), but we don't want to slow down the success\n\t * path. module_memfree() cannot be called in an interrupt, so do the\n\t * work and call synchronize_rcu() in a work queue.\n\t *\n\t * Note that module_alloc() on most architectures creates W+X page\n\t * mappings which won't be cleaned up until do_free_init() runs.  Any\n\t * code such as mark_rodata_ro() which depends on those mappings to\n\t * be cleaned up needs to sync with the queued work - ie\n\t * rcu_barrier()\n\t */\n\tif (llist_add(&freeinit->node, &init_free_list))\n\t\tschedule_work(&init_free_wq);\n\n\tmutex_unlock(&module_mutex);\n\twake_up_all(&module_wq);\n\n\treturn 0;\n\nfail_free_freeinit:\n\tkfree(freeinit);\nfail:\n\t/* Try to protect us from buggy refcounters. */\n\tmod->state = MODULE_STATE_GOING;\n\tsynchronize_rcu();\n\tmodule_put(mod);\n\tblocking_notifier_call_chain(&module_notify_list,\n\t\t\t\t     MODULE_STATE_GOING, mod);\n\tklp_module_going(mod);\n\tftrace_release_mod(mod);\n\tfree_module(mod);\n\twake_up_all(&module_wq);\n\treturn ret;\n}\n\nstatic int may_init_module(void)\n{\n\tif (!capable(CAP_SYS_MODULE) || modules_disabled)\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\n/*\n * We try to place it in the list now to make sure it's unique before\n * we dedicate too many resources.  In particular, temporary percpu\n * memory exhaustion.\n */\nstatic int add_unformed_module(struct module *mod)\n{\n\tint err;\n\tstruct module *old;\n\n\tmod->state = MODULE_STATE_UNFORMED;\n\nagain:\n\tmutex_lock(&module_mutex);\n\told = find_module_all(mod->name, strlen(mod->name), true);\n\tif (old != NULL) {\n\t\tif (old->state != MODULE_STATE_LIVE) {\n\t\t\t/* Wait in case it fails to load. */\n\t\t\tmutex_unlock(&module_mutex);\n\t\t\terr = wait_event_interruptible(module_wq,\n\t\t\t\t\t       finished_loading(mod->name));\n\t\t\tif (err)\n\t\t\t\tgoto out_unlocked;\n\t\t\tgoto again;\n\t\t}\n\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\tmod_update_bounds(mod);\n\tlist_add_rcu(&mod->list, &modules);\n\tmod_tree_insert(mod);\n\terr = 0;\n\nout:\n\tmutex_unlock(&module_mutex);\nout_unlocked:\n\treturn err;\n}\n\nstatic int complete_formation(struct module *mod, struct load_info *info)\n{\n\tint err;\n\n\tmutex_lock(&module_mutex);\n\n\t/* Find duplicate symbols (must be called under lock). */\n\terr = verify_exported_symbols(mod);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* This relies on module_mutex for list integrity. */\n\tmodule_bug_finalize(info->hdr, info->sechdrs, mod);\n\n\tmodule_enable_ro(mod, false);\n\tmodule_enable_nx(mod);\n\tmodule_enable_x(mod);\n\n\t/*\n\t * Mark state as coming so strong_try_module_get() ignores us,\n\t * but kallsyms etc. can see us.\n\t */\n\tmod->state = MODULE_STATE_COMING;\n\tmutex_unlock(&module_mutex);\n\n\treturn 0;\n\nout:\n\tmutex_unlock(&module_mutex);\n\treturn err;\n}\n\nstatic int prepare_coming_module(struct module *mod)\n{\n\tint err;\n\n\tftrace_module_enable(mod);\n\terr = klp_module_coming(mod);\n\tif (err)\n\t\treturn err;\n\n\terr = blocking_notifier_call_chain_robust(&module_notify_list,\n\t\t\tMODULE_STATE_COMING, MODULE_STATE_GOING, mod);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\tklp_module_going(mod);\n\n\treturn err;\n}\n\nstatic int unknown_module_param_cb(char *param, char *val, const char *modname,\n\t\t\t\t   void *arg)\n{\n\tstruct module *mod = arg;\n\tint ret;\n\n\tif (strcmp(param, \"async_probe\") == 0) {\n\t\tmod->async_probe_requested = true;\n\t\treturn 0;\n\t}\n\n\t/* Check for magic 'dyndbg' arg */\n\tret = ddebug_dyndbg_module_param_cb(param, val, modname);\n\tif (ret != 0)\n\t\tpr_warn(\"%s: unknown parameter '%s' ignored\\n\", modname, param);\n\treturn 0;\n}\n\nstatic void cfi_init(struct module *mod);\n\n/*\n * Allocate and load the module: note that size of section 0 is always\n * zero, and we rely on this for optional sections.\n */\nstatic int load_module(struct load_info *info, const char __user *uargs,\n\t\t       int flags)\n{\n\tstruct module *mod;\n\tlong err = 0;\n\tchar *after_dashes;\n\n\t/*\n\t * Do the signature check (if any) first. All that\n\t * the signature check needs is info->len, it does\n\t * not need any of the section info. That can be\n\t * set up later. This will minimize the chances\n\t * of a corrupt module causing problems before\n\t * we even get to the signature check.\n\t *\n\t * The check will also adjust info->len by stripping\n\t * off the sig length at the end of the module, making\n\t * checks against info->len more correct.\n\t */\n\terr = module_sig_check(info, flags);\n\tif (err)\n\t\tgoto free_copy;\n\n\t/*\n\t * Do basic sanity checks against the ELF header and\n\t * sections.\n\t */\n\terr = elf_validity_check(info);\n\tif (err) {\n\t\tpr_err(\"Module has invalid ELF structures\\n\");\n\t\tgoto free_copy;\n\t}\n\n\t/*\n\t * Everything checks out, so set up the section info\n\t * in the info structure.\n\t */\n\terr = setup_load_info(info, flags);\n\tif (err)\n\t\tgoto free_copy;\n\n\t/*\n\t * Now that we know we have the correct module name, check\n\t * if it's blacklisted.\n\t */\n\tif (blacklisted(info->name)) {\n\t\terr = -EPERM;\n\t\tpr_err(\"Module %s is blacklisted\\n\", info->name);\n\t\tgoto free_copy;\n\t}\n\n\terr = rewrite_section_headers(info, flags);\n\tif (err)\n\t\tgoto free_copy;\n\n\t/* Check module struct version now, before we try to use module. */\n\tif (!check_modstruct_version(info, info->mod)) {\n\t\terr = -ENOEXEC;\n\t\tgoto free_copy;\n\t}\n\n\t/* Figure out module layout, and allocate all the memory. */\n\tmod = layout_and_allocate(info, flags);\n\tif (IS_ERR(mod)) {\n\t\terr = PTR_ERR(mod);\n\t\tgoto free_copy;\n\t}\n\n\taudit_log_kern_module(mod->name);\n\n\t/* Reserve our place in the list. */\n\terr = add_unformed_module(mod);\n\tif (err)\n\t\tgoto free_module;\n\n#ifdef CONFIG_MODULE_SIG\n\tmod->sig_ok = info->sig_ok;\n\tif (!mod->sig_ok) {\n\t\tpr_notice_once(\"%s: module verification failed: signature \"\n\t\t\t       \"and/or required key missing - tainting \"\n\t\t\t       \"kernel\\n\", mod->name);\n\t\tadd_taint_module(mod, TAINT_UNSIGNED_MODULE, LOCKDEP_STILL_OK);\n\t}\n#endif\n\n\t/* To avoid stressing percpu allocator, do this once we're unique. */\n\terr = percpu_modalloc(mod, info);\n\tif (err)\n\t\tgoto unlink_mod;\n\n\t/* Now module is in final location, initialize linked lists, etc. */\n\terr = module_unload_init(mod);\n\tif (err)\n\t\tgoto unlink_mod;\n\n\tinit_param_lock(mod);\n\n\t/*\n\t * Now we've got everything in the final locations, we can\n\t * find optional sections.\n\t */\n\terr = find_module_sections(mod, info);\n\tif (err)\n\t\tgoto free_unload;\n\n\terr = check_module_license_and_versions(mod);\n\tif (err)\n\t\tgoto free_unload;\n\n\t/* Set up MODINFO_ATTR fields */\n\tsetup_modinfo(mod, info);\n\n\t/* Fix up syms, so that st_value is a pointer to location. */\n\terr = simplify_symbols(mod, info);\n\tif (err < 0)\n\t\tgoto free_modinfo;\n\n\terr = apply_relocations(mod, info);\n\tif (err < 0)\n\t\tgoto free_modinfo;\n\n\terr = post_relocation(mod, info);\n\tif (err < 0)\n\t\tgoto free_modinfo;\n\n\tflush_module_icache(mod);\n\n\t/* Setup CFI for the module. */\n\tcfi_init(mod);\n\n\t/* Now copy in args */\n\tmod->args = strndup_user(uargs, ~0UL >> 1);\n\tif (IS_ERR(mod->args)) {\n\t\terr = PTR_ERR(mod->args);\n\t\tgoto free_arch_cleanup;\n\t}\n\n\tdynamic_debug_setup(mod, info->debug, info->num_debug);\n\n\t/* Ftrace init must be called in the MODULE_STATE_UNFORMED state */\n\tftrace_module_init(mod);\n\n\t/* Finally it's fully formed, ready to start executing. */\n\terr = complete_formation(mod, info);\n\tif (err)\n\t\tgoto ddebug_cleanup;\n\n\terr = prepare_coming_module(mod);\n\tif (err)\n\t\tgoto bug_cleanup;\n\n\t/* Module is ready to execute: parsing args may do that. */\n\tafter_dashes = parse_args(mod->name, mod->args, mod->kp, mod->num_kp,\n\t\t\t\t  -32768, 32767, mod,\n\t\t\t\t  unknown_module_param_cb);\n\tif (IS_ERR(after_dashes)) {\n\t\terr = PTR_ERR(after_dashes);\n\t\tgoto coming_cleanup;\n\t} else if (after_dashes) {\n\t\tpr_warn(\"%s: parameters '%s' after `--' ignored\\n\",\n\t\t       mod->name, after_dashes);\n\t}\n\n\t/* Link in to sysfs. */\n\terr = mod_sysfs_setup(mod, info, mod->kp, mod->num_kp);\n\tif (err < 0)\n\t\tgoto coming_cleanup;\n\n\tif (is_livepatch_module(mod)) {\n\t\terr = copy_module_elf(mod, info);\n\t\tif (err < 0)\n\t\t\tgoto sysfs_cleanup;\n\t}\n\n\t/* Get rid of temporary copy. */\n\tfree_copy(info);\n\n\t/* Done! */\n\ttrace_module_load(mod);\n\n\treturn do_init_module(mod);\n\n sysfs_cleanup:\n\tmod_sysfs_teardown(mod);\n coming_cleanup:\n\tmod->state = MODULE_STATE_GOING;\n\tdestroy_params(mod->kp, mod->num_kp);\n\tblocking_notifier_call_chain(&module_notify_list,\n\t\t\t\t     MODULE_STATE_GOING, mod);\n\tklp_module_going(mod);\n bug_cleanup:\n\tmod->state = MODULE_STATE_GOING;\n\t/* module_bug_cleanup needs module_mutex protection */\n\tmutex_lock(&module_mutex);\n\tmodule_bug_cleanup(mod);\n\tmutex_unlock(&module_mutex);\n\n ddebug_cleanup:\n\tftrace_release_mod(mod);\n\tdynamic_debug_remove(mod, info->debug);\n\tsynchronize_rcu();\n\tkfree(mod->args);\n free_arch_cleanup:\n\tcfi_cleanup(mod);\n\tmodule_arch_cleanup(mod);\n free_modinfo:\n\tfree_modinfo(mod);\n free_unload:\n\tmodule_unload_free(mod);\n unlink_mod:\n\tmutex_lock(&module_mutex);\n\t/* Unlink carefully: kallsyms could be walking list. */\n\tlist_del_rcu(&mod->list);\n\tmod_tree_remove(mod);\n\twake_up_all(&module_wq);\n\t/* Wait for RCU-sched synchronizing before releasing mod->list. */\n\tsynchronize_rcu();\n\tmutex_unlock(&module_mutex);\n free_module:\n\t/* Free lock-classes; relies on the preceding sync_rcu() */\n\tlockdep_free_key_range(mod->core_layout.base, mod->core_layout.size);\n\n\tmodule_deallocate(mod, info);\n free_copy:\n\tfree_copy(info);\n\treturn err;\n}\n\nSYSCALL_DEFINE3(init_module, void __user *, umod,\n\t\tunsigned long, len, const char __user *, uargs)\n{\n\tint err;\n\tstruct load_info info = { };\n\n\terr = may_init_module();\n\tif (err)\n\t\treturn err;\n\n\tpr_debug(\"init_module: umod=%p, len=%lu, uargs=%p\\n\",\n\t       umod, len, uargs);\n\n\terr = copy_module_from_user(umod, len, &info);\n\tif (err)\n\t\treturn err;\n\n\treturn load_module(&info, uargs, 0);\n}\n\nSYSCALL_DEFINE3(finit_module, int, fd, const char __user *, uargs, int, flags)\n{\n\tstruct load_info info = { };\n\tvoid *hdr = NULL;\n\tint err;\n\n\terr = may_init_module();\n\tif (err)\n\t\treturn err;\n\n\tpr_debug(\"finit_module: fd=%d, uargs=%p, flags=%i\\n\", fd, uargs, flags);\n\n\tif (flags & ~(MODULE_INIT_IGNORE_MODVERSIONS\n\t\t      |MODULE_INIT_IGNORE_VERMAGIC))\n\t\treturn -EINVAL;\n\n\terr = kernel_read_file_from_fd(fd, 0, &hdr, INT_MAX, NULL,\n\t\t\t\t       READING_MODULE);\n\tif (err < 0)\n\t\treturn err;\n\tinfo.hdr = hdr;\n\tinfo.len = err;\n\n\treturn load_module(&info, uargs, flags);\n}\n\nstatic inline int within(unsigned long addr, void *start, unsigned long size)\n{\n\treturn ((void *)addr >= start && (void *)addr < start + size);\n}\n\n#ifdef CONFIG_KALLSYMS\n/*\n * This ignores the intensely annoying \"mapping symbols\" found\n * in ARM ELF files: $a, $t and $d.\n */\nstatic inline int is_arm_mapping_symbol(const char *str)\n{\n\tif (str[0] == '.' && str[1] == 'L')\n\t\treturn true;\n\treturn str[0] == '$' && strchr(\"axtd\", str[1])\n\t       && (str[2] == '\\0' || str[2] == '.');\n}\n\nstatic const char *kallsyms_symbol_name(struct mod_kallsyms *kallsyms, unsigned int symnum)\n{\n\treturn kallsyms->strtab + kallsyms->symtab[symnum].st_name;\n}\n\n/*\n * Given a module and address, find the corresponding symbol and return its name\n * while providing its size and offset if needed.\n */\nstatic const char *find_kallsyms_symbol(struct module *mod,\n\t\t\t\t\tunsigned long addr,\n\t\t\t\t\tunsigned long *size,\n\t\t\t\t\tunsigned long *offset)\n{\n\tunsigned int i, best = 0;\n\tunsigned long nextval, bestval;\n\tstruct mod_kallsyms *kallsyms = rcu_dereference_sched(mod->kallsyms);\n\n\t/* At worse, next value is at end of module */\n\tif (within_module_init(addr, mod))\n\t\tnextval = (unsigned long)mod->init_layout.base+mod->init_layout.text_size;\n\telse\n\t\tnextval = (unsigned long)mod->core_layout.base+mod->core_layout.text_size;\n\n\tbestval = kallsyms_symbol_value(&kallsyms->symtab[best]);\n\n\t/*\n\t * Scan for closest preceding symbol, and next symbol. (ELF\n\t * starts real symbols at 1).\n\t */\n\tfor (i = 1; i < kallsyms->num_symtab; i++) {\n\t\tconst Elf_Sym *sym = &kallsyms->symtab[i];\n\t\tunsigned long thisval = kallsyms_symbol_value(sym);\n\n\t\tif (sym->st_shndx == SHN_UNDEF)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We ignore unnamed symbols: they're uninformative\n\t\t * and inserted at a whim.\n\t\t */\n\t\tif (*kallsyms_symbol_name(kallsyms, i) == '\\0'\n\t\t    || is_arm_mapping_symbol(kallsyms_symbol_name(kallsyms, i)))\n\t\t\tcontinue;\n\n\t\tif (thisval <= addr && thisval > bestval) {\n\t\t\tbest = i;\n\t\t\tbestval = thisval;\n\t\t}\n\t\tif (thisval > addr && thisval < nextval)\n\t\t\tnextval = thisval;\n\t}\n\n\tif (!best)\n\t\treturn NULL;\n\n\tif (size)\n\t\t*size = nextval - bestval;\n\tif (offset)\n\t\t*offset = addr - bestval;\n\n\treturn kallsyms_symbol_name(kallsyms, best);\n}\n\nvoid * __weak dereference_module_function_descriptor(struct module *mod,\n\t\t\t\t\t\t     void *ptr)\n{\n\treturn ptr;\n}\n\n/*\n * For kallsyms to ask for address resolution.  NULL means not found.  Careful\n * not to lock to avoid deadlock on oopses, simply disable preemption.\n */\nconst char *module_address_lookup(unsigned long addr,\n\t\t\t    unsigned long *size,\n\t\t\t    unsigned long *offset,\n\t\t\t    char **modname,\n\t\t\t    char *namebuf)\n{\n\tconst char *ret = NULL;\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tmod = __module_address(addr);\n\tif (mod) {\n\t\tif (modname)\n\t\t\t*modname = mod->name;\n\n\t\tret = find_kallsyms_symbol(mod, addr, size, offset);\n\t}\n\t/* Make a copy in here where it's safe */\n\tif (ret) {\n\t\tstrncpy(namebuf, ret, KSYM_NAME_LEN - 1);\n\t\tret = namebuf;\n\t}\n\tpreempt_enable();\n\n\treturn ret;\n}\n\nint lookup_module_symbol_name(unsigned long addr, char *symname)\n{\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tif (within_module(addr, mod)) {\n\t\t\tconst char *sym;\n\n\t\t\tsym = find_kallsyms_symbol(mod, addr, NULL, NULL);\n\t\t\tif (!sym)\n\t\t\t\tgoto out;\n\n\t\t\tstrlcpy(symname, sym, KSYM_NAME_LEN);\n\t\t\tpreempt_enable();\n\t\t\treturn 0;\n\t\t}\n\t}\nout:\n\tpreempt_enable();\n\treturn -ERANGE;\n}\n\nint lookup_module_symbol_attrs(unsigned long addr, unsigned long *size,\n\t\t\tunsigned long *offset, char *modname, char *name)\n{\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tif (within_module(addr, mod)) {\n\t\t\tconst char *sym;\n\n\t\t\tsym = find_kallsyms_symbol(mod, addr, size, offset);\n\t\t\tif (!sym)\n\t\t\t\tgoto out;\n\t\t\tif (modname)\n\t\t\t\tstrlcpy(modname, mod->name, MODULE_NAME_LEN);\n\t\t\tif (name)\n\t\t\t\tstrlcpy(name, sym, KSYM_NAME_LEN);\n\t\t\tpreempt_enable();\n\t\t\treturn 0;\n\t\t}\n\t}\nout:\n\tpreempt_enable();\n\treturn -ERANGE;\n}\n\nint module_get_kallsym(unsigned int symnum, unsigned long *value, char *type,\n\t\t\tchar *name, char *module_name, int *exported)\n{\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tstruct mod_kallsyms *kallsyms;\n\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tkallsyms = rcu_dereference_sched(mod->kallsyms);\n\t\tif (symnum < kallsyms->num_symtab) {\n\t\t\tconst Elf_Sym *sym = &kallsyms->symtab[symnum];\n\n\t\t\t*value = kallsyms_symbol_value(sym);\n\t\t\t*type = kallsyms->typetab[symnum];\n\t\t\tstrlcpy(name, kallsyms_symbol_name(kallsyms, symnum), KSYM_NAME_LEN);\n\t\t\tstrlcpy(module_name, mod->name, MODULE_NAME_LEN);\n\t\t\t*exported = is_exported(name, *value, mod);\n\t\t\tpreempt_enable();\n\t\t\treturn 0;\n\t\t}\n\t\tsymnum -= kallsyms->num_symtab;\n\t}\n\tpreempt_enable();\n\treturn -ERANGE;\n}\n\n/* Given a module and name of symbol, find and return the symbol's value */\nstatic unsigned long find_kallsyms_symbol_value(struct module *mod, const char *name)\n{\n\tunsigned int i;\n\tstruct mod_kallsyms *kallsyms = rcu_dereference_sched(mod->kallsyms);\n\n\tfor (i = 0; i < kallsyms->num_symtab; i++) {\n\t\tconst Elf_Sym *sym = &kallsyms->symtab[i];\n\n\t\tif (strcmp(name, kallsyms_symbol_name(kallsyms, i)) == 0 &&\n\t\t    sym->st_shndx != SHN_UNDEF)\n\t\t\treturn kallsyms_symbol_value(sym);\n\t}\n\treturn 0;\n}\n\n/* Look for this name: can be of form module:name. */\nunsigned long module_kallsyms_lookup_name(const char *name)\n{\n\tstruct module *mod;\n\tchar *colon;\n\tunsigned long ret = 0;\n\n\t/* Don't lock: we're in enough trouble already. */\n\tpreempt_disable();\n\tif ((colon = strnchr(name, MODULE_NAME_LEN, ':')) != NULL) {\n\t\tif ((mod = find_module_all(name, colon - name, false)) != NULL)\n\t\t\tret = find_kallsyms_symbol_value(mod, colon+1);\n\t} else {\n\t\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\t\tcontinue;\n\t\t\tif ((ret = find_kallsyms_symbol_value(mod, name)) != 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tpreempt_enable();\n\treturn ret;\n}\n\n#ifdef CONFIG_LIVEPATCH\nint module_kallsyms_on_each_symbol(int (*fn)(void *, const char *,\n\t\t\t\t\t     struct module *, unsigned long),\n\t\t\t\t   void *data)\n{\n\tstruct module *mod;\n\tunsigned int i;\n\tint ret = 0;\n\n\tmutex_lock(&module_mutex);\n\tlist_for_each_entry(mod, &modules, list) {\n\t\t/* We hold module_mutex: no need for rcu_dereference_sched */\n\t\tstruct mod_kallsyms *kallsyms = mod->kallsyms;\n\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tfor (i = 0; i < kallsyms->num_symtab; i++) {\n\t\t\tconst Elf_Sym *sym = &kallsyms->symtab[i];\n\n\t\t\tif (sym->st_shndx == SHN_UNDEF)\n\t\t\t\tcontinue;\n\n\t\t\tret = fn(data, kallsyms_symbol_name(kallsyms, i),\n\t\t\t\t mod, kallsyms_symbol_value(sym));\n\t\t\tif (ret != 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&module_mutex);\n\treturn ret;\n}\n#endif /* CONFIG_LIVEPATCH */\n#endif /* CONFIG_KALLSYMS */\n\nstatic void cfi_init(struct module *mod)\n{\n#ifdef CONFIG_CFI_CLANG\n\tinitcall_t *init;\n\texitcall_t *exit;\n\n\trcu_read_lock_sched();\n\tmod->cfi_check = (cfi_check_fn)\n\t\tfind_kallsyms_symbol_value(mod, \"__cfi_check\");\n\tinit = (initcall_t *)\n\t\tfind_kallsyms_symbol_value(mod, \"__cfi_jt_init_module\");\n\texit = (exitcall_t *)\n\t\tfind_kallsyms_symbol_value(mod, \"__cfi_jt_cleanup_module\");\n\trcu_read_unlock_sched();\n\n\t/* Fix init/exit functions to point to the CFI jump table */\n\tif (init)\n\t\tmod->init = *init;\n\tif (exit)\n\t\tmod->exit = *exit;\n\n\tcfi_module_add(mod, module_addr_min);\n#endif\n}\n\nstatic void cfi_cleanup(struct module *mod)\n{\n#ifdef CONFIG_CFI_CLANG\n\tcfi_module_remove(mod, module_addr_min);\n#endif\n}\n\n/* Maximum number of characters written by module_flags() */\n#define MODULE_FLAGS_BUF_SIZE (TAINT_FLAGS_COUNT + 4)\n\n/* Keep in sync with MODULE_FLAGS_BUF_SIZE !!! */\nstatic char *module_flags(struct module *mod, char *buf)\n{\n\tint bx = 0;\n\n\tBUG_ON(mod->state == MODULE_STATE_UNFORMED);\n\tif (mod->taints ||\n\t    mod->state == MODULE_STATE_GOING ||\n\t    mod->state == MODULE_STATE_COMING) {\n\t\tbuf[bx++] = '(';\n\t\tbx += module_flags_taint(mod, buf + bx);\n\t\t/* Show a - for module-is-being-unloaded */\n\t\tif (mod->state == MODULE_STATE_GOING)\n\t\t\tbuf[bx++] = '-';\n\t\t/* Show a + for module-is-being-loaded */\n\t\tif (mod->state == MODULE_STATE_COMING)\n\t\t\tbuf[bx++] = '+';\n\t\tbuf[bx++] = ')';\n\t}\n\tbuf[bx] = '\\0';\n\n\treturn buf;\n}\n\n#ifdef CONFIG_PROC_FS\n/* Called by the /proc file system to return a list of modules. */\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tmutex_lock(&module_mutex);\n\treturn seq_list_start(&modules, *pos);\n}\n\nstatic void *m_next(struct seq_file *m, void *p, loff_t *pos)\n{\n\treturn seq_list_next(p, &modules, pos);\n}\n\nstatic void m_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&module_mutex);\n}\n\nstatic int m_show(struct seq_file *m, void *p)\n{\n\tstruct module *mod = list_entry(p, struct module, list);\n\tchar buf[MODULE_FLAGS_BUF_SIZE];\n\tvoid *value;\n\n\t/* We always ignore unformed modules. */\n\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\treturn 0;\n\n\tseq_printf(m, \"%s %u\",\n\t\t   mod->name, mod->init_layout.size + mod->core_layout.size);\n\tprint_unload_info(m, mod);\n\n\t/* Informative for users. */\n\tseq_printf(m, \" %s\",\n\t\t   mod->state == MODULE_STATE_GOING ? \"Unloading\" :\n\t\t   mod->state == MODULE_STATE_COMING ? \"Loading\" :\n\t\t   \"Live\");\n\t/* Used by oprofile and other similar tools. */\n\tvalue = m->private ? NULL : mod->core_layout.base;\n\tseq_printf(m, \" 0x%px\", value);\n\n\t/* Taints info */\n\tif (mod->taints)\n\t\tseq_printf(m, \" %s\", module_flags(mod, buf));\n\n\tseq_puts(m, \"\\n\");\n\treturn 0;\n}\n\n/*\n * Format: modulename size refcount deps address\n *\n * Where refcount is a number or -, and deps is a comma-separated list\n * of depends or -.\n */\nstatic const struct seq_operations modules_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= m_show\n};\n\n/*\n * This also sets the \"private\" pointer to non-NULL if the\n * kernel pointers should be hidden (so you can just test\n * \"m->private\" to see if you should keep the values private).\n *\n * We use the same logic as for /proc/kallsyms.\n */\nstatic int modules_open(struct inode *inode, struct file *file)\n{\n\tint err = seq_open(file, &modules_op);\n\n\tif (!err) {\n\t\tstruct seq_file *m = file->private_data;\n\t\tm->private = kallsyms_show_value(file->f_cred) ? NULL : (void *)8ul;\n\t}\n\n\treturn err;\n}\n\nstatic const struct proc_ops modules_proc_ops = {\n\t.proc_flags\t= PROC_ENTRY_PERMANENT,\n\t.proc_open\t= modules_open,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_release\t= seq_release,\n};\n\nstatic int __init proc_modules_init(void)\n{\n\tproc_create(\"modules\", 0, NULL, &modules_proc_ops);\n\treturn 0;\n}\nmodule_init(proc_modules_init);\n#endif\n\n/* Given an address, look for it in the module exception tables. */\nconst struct exception_table_entry *search_module_extables(unsigned long addr)\n{\n\tconst struct exception_table_entry *e = NULL;\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tmod = __module_address(addr);\n\tif (!mod)\n\t\tgoto out;\n\n\tif (!mod->num_exentries)\n\t\tgoto out;\n\n\te = search_extable(mod->extable,\n\t\t\t   mod->num_exentries,\n\t\t\t   addr);\nout:\n\tpreempt_enable();\n\n\t/*\n\t * Now, if we found one, we are running inside it now, hence\n\t * we cannot unload the module, hence no refcnt needed.\n\t */\n\treturn e;\n}\n\n/**\n * is_module_address() - is this address inside a module?\n * @addr: the address to check.\n *\n * See is_module_text_address() if you simply want to see if the address\n * is code (not data).\n */\nbool is_module_address(unsigned long addr)\n{\n\tbool ret;\n\n\tpreempt_disable();\n\tret = __module_address(addr) != NULL;\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * __module_address() - get the module which contains an address.\n * @addr: the address.\n *\n * Must be called with preempt disabled or module mutex held so that\n * module doesn't get freed during this.\n */\nstruct module *__module_address(unsigned long addr)\n{\n\tstruct module *mod;\n\n\tif (addr < module_addr_min || addr > module_addr_max)\n\t\treturn NULL;\n\n\tmodule_assert_mutex_or_preempt();\n\n\tmod = mod_find(addr);\n\tif (mod) {\n\t\tBUG_ON(!within_module(addr, mod));\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tmod = NULL;\n\t}\n\treturn mod;\n}\n\n/**\n * is_module_text_address() - is this address inside module code?\n * @addr: the address to check.\n *\n * See is_module_address() if you simply want to see if the address is\n * anywhere in a module.  See kernel_text_address() for testing if an\n * address corresponds to kernel or module code.\n */\nbool is_module_text_address(unsigned long addr)\n{\n\tbool ret;\n\n\tpreempt_disable();\n\tret = __module_text_address(addr) != NULL;\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * __module_text_address() - get the module whose code contains an address.\n * @addr: the address.\n *\n * Must be called with preempt disabled or module mutex held so that\n * module doesn't get freed during this.\n */\nstruct module *__module_text_address(unsigned long addr)\n{\n\tstruct module *mod = __module_address(addr);\n\tif (mod) {\n\t\t/* Make sure it's within the text section. */\n\t\tif (!within(addr, mod->init_layout.base, mod->init_layout.text_size)\n\t\t    && !within(addr, mod->core_layout.base, mod->core_layout.text_size))\n\t\t\tmod = NULL;\n\t}\n\treturn mod;\n}\n\n/* Don't grab lock, we're oopsing. */\nvoid print_modules(void)\n{\n\tstruct module *mod;\n\tchar buf[MODULE_FLAGS_BUF_SIZE];\n\n\tprintk(KERN_DEFAULT \"Modules linked in:\");\n\t/* Most callers should already have preempt disabled, but make sure */\n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tpr_cont(\" %s%s\", mod->name, module_flags(mod, buf));\n\t}\n\tpreempt_enable();\n\tif (last_unloaded_module[0])\n\t\tpr_cont(\" [last unloaded: %s]\", last_unloaded_module);\n\tpr_cont(\"\\n\");\n}\n\n#ifdef CONFIG_MODVERSIONS\n/*\n * Generate the signature for all relevant module structures here.\n * If these change, we don't want to try to parse the module.\n */\nvoid module_layout(struct module *mod,\n\t\t   struct modversion_info *ver,\n\t\t   struct kernel_param *kp,\n\t\t   struct kernel_symbol *ks,\n\t\t   struct tracepoint * const *tp)\n{\n}\nEXPORT_SYMBOL(module_layout);\n#endif\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Copyright (C) 2002 Richard Henderson\n * Copyright (C) 2001 Rusty Russell, 2002, 2010 Rusty Russell IBM.\n */\n\n#define INCLUDE_VERMAGIC\n\n#include <linux/export.h>\n#include <linux/extable.h>\n#include <linux/moduleloader.h>\n#include <linux/module_signature.h>\n#include <linux/trace_events.h>\n#include <linux/init.h>\n#include <linux/kallsyms.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/sysfs.h>\n#include <linux/kernel.h>\n#include <linux/kernel_read_file.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/elf.h>\n#include <linux/proc_fs.h>\n#include <linux/security.h>\n#include <linux/seq_file.h>\n#include <linux/syscalls.h>\n#include <linux/fcntl.h>\n#include <linux/rcupdate.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/moduleparam.h>\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/vermagic.h>\n#include <linux/notifier.h>\n#include <linux/sched.h>\n#include <linux/device.h>\n#include <linux/string.h>\n#include <linux/mutex.h>\n#include <linux/rculist.h>\n#include <linux/uaccess.h>\n#include <asm/cacheflush.h>\n#include <linux/set_memory.h>\n#include <asm/mmu_context.h>\n#include <linux/license.h>\n#include <asm/sections.h>\n#include <linux/tracepoint.h>\n#include <linux/ftrace.h>\n#include <linux/livepatch.h>\n#include <linux/async.h>\n#include <linux/percpu.h>\n#include <linux/kmemleak.h>\n#include <linux/jump_label.h>\n#include <linux/pfn.h>\n#include <linux/bsearch.h>\n#include <linux/dynamic_debug.h>\n#include <linux/audit.h>\n#include <uapi/linux/module.h>\n#include \"module-internal.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/module.h>\n\n#ifndef ARCH_SHF_SMALL\n#define ARCH_SHF_SMALL 0\n#endif\n\n/*\n * Modules' sections will be aligned on page boundaries\n * to ensure complete separation of code and data, but\n * only when CONFIG_ARCH_HAS_STRICT_MODULE_RWX=y\n */\n#ifdef CONFIG_ARCH_HAS_STRICT_MODULE_RWX\n# define debug_align(X) ALIGN(X, PAGE_SIZE)\n#else\n# define debug_align(X) (X)\n#endif\n\n/* If this is set, the section belongs in the init part of the module */\n#define INIT_OFFSET_MASK (1UL << (BITS_PER_LONG-1))\n\n/*\n * Mutex protects:\n * 1) List of modules (also safely readable with preempt_disable),\n * 2) module_use links,\n * 3) module_addr_min/module_addr_max.\n * (delete and add uses RCU list operations).\n */\nstatic DEFINE_MUTEX(module_mutex);\nstatic LIST_HEAD(modules);\n\n/* Work queue for freeing init sections in success case */\nstatic void do_free_init(struct work_struct *w);\nstatic DECLARE_WORK(init_free_wq, do_free_init);\nstatic LLIST_HEAD(init_free_list);\n\n#ifdef CONFIG_MODULES_TREE_LOOKUP\n\n/*\n * Use a latched RB-tree for __module_address(); this allows us to use\n * RCU-sched lookups of the address from any context.\n *\n * This is conditional on PERF_EVENTS || TRACING because those can really hit\n * __module_address() hard by doing a lot of stack unwinding; potentially from\n * NMI context.\n */\n\nstatic __always_inline unsigned long __mod_tree_val(struct latch_tree_node *n)\n{\n\tstruct module_layout *layout = container_of(n, struct module_layout, mtn.node);\n\n\treturn (unsigned long)layout->base;\n}\n\nstatic __always_inline unsigned long __mod_tree_size(struct latch_tree_node *n)\n{\n\tstruct module_layout *layout = container_of(n, struct module_layout, mtn.node);\n\n\treturn (unsigned long)layout->size;\n}\n\nstatic __always_inline bool\nmod_tree_less(struct latch_tree_node *a, struct latch_tree_node *b)\n{\n\treturn __mod_tree_val(a) < __mod_tree_val(b);\n}\n\nstatic __always_inline int\nmod_tree_comp(void *key, struct latch_tree_node *n)\n{\n\tunsigned long val = (unsigned long)key;\n\tunsigned long start, end;\n\n\tstart = __mod_tree_val(n);\n\tif (val < start)\n\t\treturn -1;\n\n\tend = start + __mod_tree_size(n);\n\tif (val >= end)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic const struct latch_tree_ops mod_tree_ops = {\n\t.less = mod_tree_less,\n\t.comp = mod_tree_comp,\n};\n\nstatic struct mod_tree_root {\n\tstruct latch_tree_root root;\n\tunsigned long addr_min;\n\tunsigned long addr_max;\n} mod_tree __cacheline_aligned = {\n\t.addr_min = -1UL,\n};\n\n#define module_addr_min mod_tree.addr_min\n#define module_addr_max mod_tree.addr_max\n\nstatic noinline void __mod_tree_insert(struct mod_tree_node *node)\n{\n\tlatch_tree_insert(&node->node, &mod_tree.root, &mod_tree_ops);\n}\n\nstatic void __mod_tree_remove(struct mod_tree_node *node)\n{\n\tlatch_tree_erase(&node->node, &mod_tree.root, &mod_tree_ops);\n}\n\n/*\n * These modifications: insert, remove_init and remove; are serialized by the\n * module_mutex.\n */\nstatic void mod_tree_insert(struct module *mod)\n{\n\tmod->core_layout.mtn.mod = mod;\n\tmod->init_layout.mtn.mod = mod;\n\n\t__mod_tree_insert(&mod->core_layout.mtn);\n\tif (mod->init_layout.size)\n\t\t__mod_tree_insert(&mod->init_layout.mtn);\n}\n\nstatic void mod_tree_remove_init(struct module *mod)\n{\n\tif (mod->init_layout.size)\n\t\t__mod_tree_remove(&mod->init_layout.mtn);\n}\n\nstatic void mod_tree_remove(struct module *mod)\n{\n\t__mod_tree_remove(&mod->core_layout.mtn);\n\tmod_tree_remove_init(mod);\n}\n\nstatic struct module *mod_find(unsigned long addr)\n{\n\tstruct latch_tree_node *ltn;\n\n\tltn = latch_tree_find((void *)addr, &mod_tree.root, &mod_tree_ops);\n\tif (!ltn)\n\t\treturn NULL;\n\n\treturn container_of(ltn, struct mod_tree_node, node)->mod;\n}\n\n#else /* MODULES_TREE_LOOKUP */\n\nstatic unsigned long module_addr_min = -1UL, module_addr_max = 0;\n\nstatic void mod_tree_insert(struct module *mod) { }\nstatic void mod_tree_remove_init(struct module *mod) { }\nstatic void mod_tree_remove(struct module *mod) { }\n\nstatic struct module *mod_find(unsigned long addr)\n{\n\tstruct module *mod;\n\n\tlist_for_each_entry_rcu(mod, &modules, list,\n\t\t\t\tlockdep_is_held(&module_mutex)) {\n\t\tif (within_module(addr, mod))\n\t\t\treturn mod;\n\t}\n\n\treturn NULL;\n}\n\n#endif /* MODULES_TREE_LOOKUP */\n\n/*\n * Bounds of module text, for speeding up __module_address.\n * Protected by module_mutex.\n */\nstatic void __mod_update_bounds(void *base, unsigned int size)\n{\n\tunsigned long min = (unsigned long)base;\n\tunsigned long max = min + size;\n\n\tif (min < module_addr_min)\n\t\tmodule_addr_min = min;\n\tif (max > module_addr_max)\n\t\tmodule_addr_max = max;\n}\n\nstatic void mod_update_bounds(struct module *mod)\n{\n\t__mod_update_bounds(mod->core_layout.base, mod->core_layout.size);\n\tif (mod->init_layout.size)\n\t\t__mod_update_bounds(mod->init_layout.base, mod->init_layout.size);\n}\n\n#ifdef CONFIG_KGDB_KDB\nstruct list_head *kdb_modules = &modules; /* kdb needs the list of modules */\n#endif /* CONFIG_KGDB_KDB */\n\nstatic void module_assert_mutex_or_preempt(void)\n{\n#ifdef CONFIG_LOCKDEP\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\tWARN_ON_ONCE(!rcu_read_lock_sched_held() &&\n\t\t!lockdep_is_held(&module_mutex));\n#endif\n}\n\n#ifdef CONFIG_MODULE_SIG\nstatic bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);\nmodule_param(sig_enforce, bool_enable_only, 0644);\n\nvoid set_module_sig_enforced(void)\n{\n\tsig_enforce = true;\n}\n#else\n#define sig_enforce false\n#endif\n\n/*\n * Export sig_enforce kernel cmdline parameter to allow other subsystems rely\n * on that instead of directly to CONFIG_MODULE_SIG_FORCE config.\n */\nbool is_module_sig_enforced(void)\n{\n\treturn sig_enforce;\n}\nEXPORT_SYMBOL(is_module_sig_enforced);\n\n/* Block module loading/unloading? */\nint modules_disabled = 0;\ncore_param(nomodule, modules_disabled, bint, 0);\n\n/* Waiting for a module to finish initializing? */\nstatic DECLARE_WAIT_QUEUE_HEAD(module_wq);\n\nstatic BLOCKING_NOTIFIER_HEAD(module_notify_list);\n\nint register_module_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&module_notify_list, nb);\n}\nEXPORT_SYMBOL(register_module_notifier);\n\nint unregister_module_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&module_notify_list, nb);\n}\nEXPORT_SYMBOL(unregister_module_notifier);\n\n/*\n * We require a truly strong try_module_get(): 0 means success.\n * Otherwise an error is returned due to ongoing or failed\n * initialization etc.\n */\nstatic inline int strong_try_module_get(struct module *mod)\n{\n\tBUG_ON(mod && mod->state == MODULE_STATE_UNFORMED);\n\tif (mod && mod->state == MODULE_STATE_COMING)\n\t\treturn -EBUSY;\n\tif (try_module_get(mod))\n\t\treturn 0;\n\telse\n\t\treturn -ENOENT;\n}\n\nstatic inline void add_taint_module(struct module *mod, unsigned flag,\n\t\t\t\t    enum lockdep_ok lockdep_ok)\n{\n\tadd_taint(flag, lockdep_ok);\n\tset_bit(flag, &mod->taints);\n}\n\n/*\n * A thread that wants to hold a reference to a module only while it\n * is running can call this to safely exit.  nfsd and lockd use this.\n */\nvoid __noreturn __module_put_and_exit(struct module *mod, long code)\n{\n\tmodule_put(mod);\n\tdo_exit(code);\n}\nEXPORT_SYMBOL(__module_put_and_exit);\n\n/* Find a module section: 0 means not found. */\nstatic unsigned int find_sec(const struct load_info *info, const char *name)\n{\n\tunsigned int i;\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tElf_Shdr *shdr = &info->sechdrs[i];\n\t\t/* Alloc bit cleared means \"ignore it.\" */\n\t\tif ((shdr->sh_flags & SHF_ALLOC)\n\t\t    && strcmp(info->secstrings + shdr->sh_name, name) == 0)\n\t\t\treturn i;\n\t}\n\treturn 0;\n}\n\n/* Find a module section, or NULL. */\nstatic void *section_addr(const struct load_info *info, const char *name)\n{\n\t/* Section 0 has sh_addr 0. */\n\treturn (void *)info->sechdrs[find_sec(info, name)].sh_addr;\n}\n\n/* Find a module section, or NULL.  Fill in number of \"objects\" in section. */\nstatic void *section_objs(const struct load_info *info,\n\t\t\t  const char *name,\n\t\t\t  size_t object_size,\n\t\t\t  unsigned int *num)\n{\n\tunsigned int sec = find_sec(info, name);\n\n\t/* Section 0 has sh_addr 0 and sh_size 0. */\n\t*num = info->sechdrs[sec].sh_size / object_size;\n\treturn (void *)info->sechdrs[sec].sh_addr;\n}\n\n/* Find a module section: 0 means not found. Ignores SHF_ALLOC flag. */\nstatic unsigned int find_any_sec(const struct load_info *info, const char *name)\n{\n\tunsigned int i;\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tElf_Shdr *shdr = &info->sechdrs[i];\n\t\tif (strcmp(info->secstrings + shdr->sh_name, name) == 0)\n\t\t\treturn i;\n\t}\n\treturn 0;\n}\n\n/*\n * Find a module section, or NULL. Fill in number of \"objects\" in section.\n * Ignores SHF_ALLOC flag.\n */\nstatic __maybe_unused void *any_section_objs(const struct load_info *info,\n\t\t\t\t\t     const char *name,\n\t\t\t\t\t     size_t object_size,\n\t\t\t\t\t     unsigned int *num)\n{\n\tunsigned int sec = find_any_sec(info, name);\n\n\t/* Section 0 has sh_addr 0 and sh_size 0. */\n\t*num = info->sechdrs[sec].sh_size / object_size;\n\treturn (void *)info->sechdrs[sec].sh_addr;\n}\n\n/* Provided by the linker */\nextern const struct kernel_symbol __start___ksymtab[];\nextern const struct kernel_symbol __stop___ksymtab[];\nextern const struct kernel_symbol __start___ksymtab_gpl[];\nextern const struct kernel_symbol __stop___ksymtab_gpl[];\nextern const s32 __start___kcrctab[];\nextern const s32 __start___kcrctab_gpl[];\n\n#ifndef CONFIG_MODVERSIONS\n#define symversion(base, idx) NULL\n#else\n#define symversion(base, idx) ((base != NULL) ? ((base) + (idx)) : NULL)\n#endif\n\nstruct symsearch {\n\tconst struct kernel_symbol *start, *stop;\n\tconst s32 *crcs;\n\tenum mod_license {\n\t\tNOT_GPL_ONLY,\n\t\tGPL_ONLY,\n\t} license;\n};\n\nstruct find_symbol_arg {\n\t/* Input */\n\tconst char *name;\n\tbool gplok;\n\tbool warn;\n\n\t/* Output */\n\tstruct module *owner;\n\tconst s32 *crc;\n\tconst struct kernel_symbol *sym;\n\tenum mod_license license;\n};\n\nstatic bool check_exported_symbol(const struct symsearch *syms,\n\t\t\t\t  struct module *owner,\n\t\t\t\t  unsigned int symnum, void *data)\n{\n\tstruct find_symbol_arg *fsa = data;\n\n\tif (!fsa->gplok && syms->license == GPL_ONLY)\n\t\treturn false;\n\tfsa->owner = owner;\n\tfsa->crc = symversion(syms->crcs, symnum);\n\tfsa->sym = &syms->start[symnum];\n\tfsa->license = syms->license;\n\treturn true;\n}\n\nstatic unsigned long kernel_symbol_value(const struct kernel_symbol *sym)\n{\n#ifdef CONFIG_HAVE_ARCH_PREL32_RELOCATIONS\n\treturn (unsigned long)offset_to_ptr(&sym->value_offset);\n#else\n\treturn sym->value;\n#endif\n}\n\nstatic const char *kernel_symbol_name(const struct kernel_symbol *sym)\n{\n#ifdef CONFIG_HAVE_ARCH_PREL32_RELOCATIONS\n\treturn offset_to_ptr(&sym->name_offset);\n#else\n\treturn sym->name;\n#endif\n}\n\nstatic const char *kernel_symbol_namespace(const struct kernel_symbol *sym)\n{\n#ifdef CONFIG_HAVE_ARCH_PREL32_RELOCATIONS\n\tif (!sym->namespace_offset)\n\t\treturn NULL;\n\treturn offset_to_ptr(&sym->namespace_offset);\n#else\n\treturn sym->namespace;\n#endif\n}\n\nstatic int cmp_name(const void *name, const void *sym)\n{\n\treturn strcmp(name, kernel_symbol_name(sym));\n}\n\nstatic bool find_exported_symbol_in_section(const struct symsearch *syms,\n\t\t\t\t\t    struct module *owner,\n\t\t\t\t\t    void *data)\n{\n\tstruct find_symbol_arg *fsa = data;\n\tstruct kernel_symbol *sym;\n\n\tsym = bsearch(fsa->name, syms->start, syms->stop - syms->start,\n\t\t\tsizeof(struct kernel_symbol), cmp_name);\n\n\tif (sym != NULL && check_exported_symbol(syms, owner,\n\t\t\t\t\t\t sym - syms->start, data))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Find an exported symbol and return it, along with, (optional) crc and\n * (optional) module which owns it.  Needs preempt disabled or module_mutex.\n */\nstatic bool find_symbol(struct find_symbol_arg *fsa)\n{\n\tstatic const struct symsearch arr[] = {\n\t\t{ __start___ksymtab, __stop___ksymtab, __start___kcrctab,\n\t\t  NOT_GPL_ONLY },\n\t\t{ __start___ksymtab_gpl, __stop___ksymtab_gpl,\n\t\t  __start___kcrctab_gpl,\n\t\t  GPL_ONLY },\n\t};\n\tstruct module *mod;\n\tunsigned int i;\n\n\tmodule_assert_mutex_or_preempt();\n\n\tfor (i = 0; i < ARRAY_SIZE(arr); i++)\n\t\tif (find_exported_symbol_in_section(&arr[i], NULL, fsa))\n\t\t\treturn true;\n\n\tlist_for_each_entry_rcu(mod, &modules, list,\n\t\t\t\tlockdep_is_held(&module_mutex)) {\n\t\tstruct symsearch arr[] = {\n\t\t\t{ mod->syms, mod->syms + mod->num_syms, mod->crcs,\n\t\t\t  NOT_GPL_ONLY },\n\t\t\t{ mod->gpl_syms, mod->gpl_syms + mod->num_gpl_syms,\n\t\t\t  mod->gpl_crcs,\n\t\t\t  GPL_ONLY },\n\t\t};\n\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(arr); i++)\n\t\t\tif (find_exported_symbol_in_section(&arr[i], mod, fsa))\n\t\t\t\treturn true;\n\t}\n\n\tpr_debug(\"Failed to find symbol %s\\n\", fsa->name);\n\treturn false;\n}\n\n/*\n * Search for module by name: must hold module_mutex (or preempt disabled\n * for read-only access).\n */\nstatic struct module *find_module_all(const char *name, size_t len,\n\t\t\t\t      bool even_unformed)\n{\n\tstruct module *mod;\n\n\tmodule_assert_mutex_or_preempt();\n\n\tlist_for_each_entry_rcu(mod, &modules, list,\n\t\t\t\tlockdep_is_held(&module_mutex)) {\n\t\tif (!even_unformed && mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tif (strlen(mod->name) == len && !memcmp(mod->name, name, len))\n\t\t\treturn mod;\n\t}\n\treturn NULL;\n}\n\nstruct module *find_module(const char *name)\n{\n\treturn find_module_all(name, strlen(name), false);\n}\n\n#ifdef CONFIG_SMP\n\nstatic inline void __percpu *mod_percpu(struct module *mod)\n{\n\treturn mod->percpu;\n}\n\nstatic int percpu_modalloc(struct module *mod, struct load_info *info)\n{\n\tElf_Shdr *pcpusec = &info->sechdrs[info->index.pcpu];\n\tunsigned long align = pcpusec->sh_addralign;\n\n\tif (!pcpusec->sh_size)\n\t\treturn 0;\n\n\tif (align > PAGE_SIZE) {\n\t\tpr_warn(\"%s: per-cpu alignment %li > %li\\n\",\n\t\t\tmod->name, align, PAGE_SIZE);\n\t\talign = PAGE_SIZE;\n\t}\n\n\tmod->percpu = __alloc_reserved_percpu(pcpusec->sh_size, align);\n\tif (!mod->percpu) {\n\t\tpr_warn(\"%s: Could not allocate %lu bytes percpu data\\n\",\n\t\t\tmod->name, (unsigned long)pcpusec->sh_size);\n\t\treturn -ENOMEM;\n\t}\n\tmod->percpu_size = pcpusec->sh_size;\n\treturn 0;\n}\n\nstatic void percpu_modfree(struct module *mod)\n{\n\tfree_percpu(mod->percpu);\n}\n\nstatic unsigned int find_pcpusec(struct load_info *info)\n{\n\treturn find_sec(info, \".data..percpu\");\n}\n\nstatic void percpu_modcopy(struct module *mod,\n\t\t\t   const void *from, unsigned long size)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tmemcpy(per_cpu_ptr(mod->percpu, cpu), from, size);\n}\n\nbool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)\n{\n\tstruct module *mod;\n\tunsigned int cpu;\n\n\tpreempt_disable();\n\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tif (!mod->percpu_size)\n\t\t\tcontinue;\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tvoid *start = per_cpu_ptr(mod->percpu, cpu);\n\t\t\tvoid *va = (void *)addr;\n\n\t\t\tif (va >= start && va < start + mod->percpu_size) {\n\t\t\t\tif (can_addr) {\n\t\t\t\t\t*can_addr = (unsigned long) (va - start);\n\t\t\t\t\t*can_addr += (unsigned long)\n\t\t\t\t\t\tper_cpu_ptr(mod->percpu,\n\t\t\t\t\t\t\t    get_boot_cpu_id());\n\t\t\t\t}\n\t\t\t\tpreempt_enable();\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\n\tpreempt_enable();\n\treturn false;\n}\n\n/**\n * is_module_percpu_address() - test whether address is from module static percpu\n * @addr: address to test\n *\n * Test whether @addr belongs to module static percpu area.\n *\n * Return: %true if @addr is from module static percpu area\n */\nbool is_module_percpu_address(unsigned long addr)\n{\n\treturn __is_module_percpu_address(addr, NULL);\n}\n\n#else /* ... !CONFIG_SMP */\n\nstatic inline void __percpu *mod_percpu(struct module *mod)\n{\n\treturn NULL;\n}\nstatic int percpu_modalloc(struct module *mod, struct load_info *info)\n{\n\t/* UP modules shouldn't have this section: ENOMEM isn't quite right */\n\tif (info->sechdrs[info->index.pcpu].sh_size != 0)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nstatic inline void percpu_modfree(struct module *mod)\n{\n}\nstatic unsigned int find_pcpusec(struct load_info *info)\n{\n\treturn 0;\n}\nstatic inline void percpu_modcopy(struct module *mod,\n\t\t\t\t  const void *from, unsigned long size)\n{\n\t/* pcpusec should be 0, and size of that section should be 0. */\n\tBUG_ON(size != 0);\n}\nbool is_module_percpu_address(unsigned long addr)\n{\n\treturn false;\n}\n\nbool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)\n{\n\treturn false;\n}\n\n#endif /* CONFIG_SMP */\n\n#define MODINFO_ATTR(field)\t\\\nstatic void setup_modinfo_##field(struct module *mod, const char *s)  \\\n{                                                                     \\\n\tmod->field = kstrdup(s, GFP_KERNEL);                          \\\n}                                                                     \\\nstatic ssize_t show_modinfo_##field(struct module_attribute *mattr,   \\\n\t\t\tstruct module_kobject *mk, char *buffer)      \\\n{                                                                     \\\n\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", mk->mod->field);  \\\n}                                                                     \\\nstatic int modinfo_##field##_exists(struct module *mod)               \\\n{                                                                     \\\n\treturn mod->field != NULL;                                    \\\n}                                                                     \\\nstatic void free_modinfo_##field(struct module *mod)                  \\\n{                                                                     \\\n\tkfree(mod->field);                                            \\\n\tmod->field = NULL;                                            \\\n}                                                                     \\\nstatic struct module_attribute modinfo_##field = {                    \\\n\t.attr = { .name = __stringify(field), .mode = 0444 },         \\\n\t.show = show_modinfo_##field,                                 \\\n\t.setup = setup_modinfo_##field,                               \\\n\t.test = modinfo_##field##_exists,                             \\\n\t.free = free_modinfo_##field,                                 \\\n};\n\nMODINFO_ATTR(version);\nMODINFO_ATTR(srcversion);\n\nstatic char last_unloaded_module[MODULE_NAME_LEN+1];\n\n#ifdef CONFIG_MODULE_UNLOAD\n\nEXPORT_TRACEPOINT_SYMBOL(module_get);\n\n/* MODULE_REF_BASE is the base reference count by kmodule loader. */\n#define MODULE_REF_BASE\t1\n\n/* Init the unload section of the module. */\nstatic int module_unload_init(struct module *mod)\n{\n\t/*\n\t * Initialize reference counter to MODULE_REF_BASE.\n\t * refcnt == 0 means module is going.\n\t */\n\tatomic_set(&mod->refcnt, MODULE_REF_BASE);\n\n\tINIT_LIST_HEAD(&mod->source_list);\n\tINIT_LIST_HEAD(&mod->target_list);\n\n\t/* Hold reference count during initialization. */\n\tatomic_inc(&mod->refcnt);\n\n\treturn 0;\n}\n\n/* Does a already use b? */\nstatic int already_uses(struct module *a, struct module *b)\n{\n\tstruct module_use *use;\n\n\tlist_for_each_entry(use, &b->source_list, source_list) {\n\t\tif (use->source == a) {\n\t\t\tpr_debug(\"%s uses %s!\\n\", a->name, b->name);\n\t\t\treturn 1;\n\t\t}\n\t}\n\tpr_debug(\"%s does not use %s!\\n\", a->name, b->name);\n\treturn 0;\n}\n\n/*\n * Module a uses b\n *  - we add 'a' as a \"source\", 'b' as a \"target\" of module use\n *  - the module_use is added to the list of 'b' sources (so\n *    'b' can walk the list to see who sourced them), and of 'a'\n *    targets (so 'a' can see what modules it targets).\n */\nstatic int add_module_usage(struct module *a, struct module *b)\n{\n\tstruct module_use *use;\n\n\tpr_debug(\"Allocating new usage for %s.\\n\", a->name);\n\tuse = kmalloc(sizeof(*use), GFP_ATOMIC);\n\tif (!use)\n\t\treturn -ENOMEM;\n\n\tuse->source = a;\n\tuse->target = b;\n\tlist_add(&use->source_list, &b->source_list);\n\tlist_add(&use->target_list, &a->target_list);\n\treturn 0;\n}\n\n/* Module a uses b: caller needs module_mutex() */\nstatic int ref_module(struct module *a, struct module *b)\n{\n\tint err;\n\n\tif (b == NULL || already_uses(a, b))\n\t\treturn 0;\n\n\t/* If module isn't available, we fail. */\n\terr = strong_try_module_get(b);\n\tif (err)\n\t\treturn err;\n\n\terr = add_module_usage(a, b);\n\tif (err) {\n\t\tmodule_put(b);\n\t\treturn err;\n\t}\n\treturn 0;\n}\n\n/* Clear the unload stuff of the module. */\nstatic void module_unload_free(struct module *mod)\n{\n\tstruct module_use *use, *tmp;\n\n\tmutex_lock(&module_mutex);\n\tlist_for_each_entry_safe(use, tmp, &mod->target_list, target_list) {\n\t\tstruct module *i = use->target;\n\t\tpr_debug(\"%s unusing %s\\n\", mod->name, i->name);\n\t\tmodule_put(i);\n\t\tlist_del(&use->source_list);\n\t\tlist_del(&use->target_list);\n\t\tkfree(use);\n\t}\n\tmutex_unlock(&module_mutex);\n}\n\n#ifdef CONFIG_MODULE_FORCE_UNLOAD\nstatic inline int try_force_unload(unsigned int flags)\n{\n\tint ret = (flags & O_TRUNC);\n\tif (ret)\n\t\tadd_taint(TAINT_FORCED_RMMOD, LOCKDEP_NOW_UNRELIABLE);\n\treturn ret;\n}\n#else\nstatic inline int try_force_unload(unsigned int flags)\n{\n\treturn 0;\n}\n#endif /* CONFIG_MODULE_FORCE_UNLOAD */\n\n/* Try to release refcount of module, 0 means success. */\nstatic int try_release_module_ref(struct module *mod)\n{\n\tint ret;\n\n\t/* Try to decrement refcnt which we set at loading */\n\tret = atomic_sub_return(MODULE_REF_BASE, &mod->refcnt);\n\tBUG_ON(ret < 0);\n\tif (ret)\n\t\t/* Someone can put this right now, recover with checking */\n\t\tret = atomic_add_unless(&mod->refcnt, MODULE_REF_BASE, 0);\n\n\treturn ret;\n}\n\nstatic int try_stop_module(struct module *mod, int flags, int *forced)\n{\n\t/* If it's not unused, quit unless we're forcing. */\n\tif (try_release_module_ref(mod) != 0) {\n\t\t*forced = try_force_unload(flags);\n\t\tif (!(*forced))\n\t\t\treturn -EWOULDBLOCK;\n\t}\n\n\t/* Mark it as dying. */\n\tmod->state = MODULE_STATE_GOING;\n\n\treturn 0;\n}\n\n/**\n * module_refcount() - return the refcount or -1 if unloading\n * @mod:\tthe module we're checking\n *\n * Return:\n *\t-1 if the module is in the process of unloading\n *\totherwise the number of references in the kernel to the module\n */\nint module_refcount(struct module *mod)\n{\n\treturn atomic_read(&mod->refcnt) - MODULE_REF_BASE;\n}\nEXPORT_SYMBOL(module_refcount);\n\n/* This exists whether we can unload or not */\nstatic void free_module(struct module *mod);\n\nSYSCALL_DEFINE2(delete_module, const char __user *, name_user,\n\t\tunsigned int, flags)\n{\n\tstruct module *mod;\n\tchar name[MODULE_NAME_LEN];\n\tint ret, forced = 0;\n\n\tif (!capable(CAP_SYS_MODULE) || modules_disabled)\n\t\treturn -EPERM;\n\n\tif (strncpy_from_user(name, name_user, MODULE_NAME_LEN-1) < 0)\n\t\treturn -EFAULT;\n\tname[MODULE_NAME_LEN-1] = '\\0';\n\n\taudit_log_kern_module(name);\n\n\tif (mutex_lock_interruptible(&module_mutex) != 0)\n\t\treturn -EINTR;\n\n\tmod = find_module(name);\n\tif (!mod) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tif (!list_empty(&mod->source_list)) {\n\t\t/* Other modules depend on us: get rid of them first. */\n\t\tret = -EWOULDBLOCK;\n\t\tgoto out;\n\t}\n\n\t/* Doing init or already dying? */\n\tif (mod->state != MODULE_STATE_LIVE) {\n\t\t/* FIXME: if (force), slam module count damn the torpedoes */\n\t\tpr_debug(\"%s already dying\\n\", mod->name);\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* If it has an init func, it must have an exit func to unload */\n\tif (mod->init && !mod->exit) {\n\t\tforced = try_force_unload(flags);\n\t\tif (!forced) {\n\t\t\t/* This module can't be removed */\n\t\t\tret = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Stop the machine so refcounts can't move and disable module. */\n\tret = try_stop_module(mod, flags, &forced);\n\tif (ret != 0)\n\t\tgoto out;\n\n\tmutex_unlock(&module_mutex);\n\t/* Final destruction now no one is using it. */\n\tif (mod->exit != NULL)\n\t\tmod->exit();\n\tblocking_notifier_call_chain(&module_notify_list,\n\t\t\t\t     MODULE_STATE_GOING, mod);\n\tklp_module_going(mod);\n\tftrace_release_mod(mod);\n\n\tasync_synchronize_full();\n\n\t/* Store the name of the last unloaded module for diagnostic purposes */\n\tstrlcpy(last_unloaded_module, mod->name, sizeof(last_unloaded_module));\n\n\tfree_module(mod);\n\t/* someone could wait for the module in add_unformed_module() */\n\twake_up_all(&module_wq);\n\treturn 0;\nout:\n\tmutex_unlock(&module_mutex);\n\treturn ret;\n}\n\nstatic inline void print_unload_info(struct seq_file *m, struct module *mod)\n{\n\tstruct module_use *use;\n\tint printed_something = 0;\n\n\tseq_printf(m, \" %i \", module_refcount(mod));\n\n\t/*\n\t * Always include a trailing , so userspace can differentiate\n\t * between this and the old multi-field proc format.\n\t */\n\tlist_for_each_entry(use, &mod->source_list, source_list) {\n\t\tprinted_something = 1;\n\t\tseq_printf(m, \"%s,\", use->source->name);\n\t}\n\n\tif (mod->init != NULL && mod->exit == NULL) {\n\t\tprinted_something = 1;\n\t\tseq_puts(m, \"[permanent],\");\n\t}\n\n\tif (!printed_something)\n\t\tseq_puts(m, \"-\");\n}\n\nvoid __symbol_put(const char *symbol)\n{\n\tstruct find_symbol_arg fsa = {\n\t\t.name\t= symbol,\n\t\t.gplok\t= true,\n\t};\n\n\tpreempt_disable();\n\tif (!find_symbol(&fsa))\n\t\tBUG();\n\tmodule_put(fsa.owner);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL(__symbol_put);\n\n/* Note this assumes addr is a function, which it currently always is. */\nvoid symbol_put_addr(void *addr)\n{\n\tstruct module *modaddr;\n\tunsigned long a = (unsigned long)dereference_function_descriptor(addr);\n\n\tif (core_kernel_text(a))\n\t\treturn;\n\n\t/*\n\t * Even though we hold a reference on the module; we still need to\n\t * disable preemption in order to safely traverse the data structure.\n\t */\n\tpreempt_disable();\n\tmodaddr = __module_text_address(a);\n\tBUG_ON(!modaddr);\n\tmodule_put(modaddr);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(symbol_put_addr);\n\nstatic ssize_t show_refcnt(struct module_attribute *mattr,\n\t\t\t   struct module_kobject *mk, char *buffer)\n{\n\treturn sprintf(buffer, \"%i\\n\", module_refcount(mk->mod));\n}\n\nstatic struct module_attribute modinfo_refcnt =\n\t__ATTR(refcnt, 0444, show_refcnt, NULL);\n\nvoid __module_get(struct module *module)\n{\n\tif (module) {\n\t\tpreempt_disable();\n\t\tatomic_inc(&module->refcnt);\n\t\ttrace_module_get(module, _RET_IP_);\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL(__module_get);\n\nbool try_module_get(struct module *module)\n{\n\tbool ret = true;\n\n\tif (module) {\n\t\tpreempt_disable();\n\t\t/* Note: here, we can fail to get a reference */\n\t\tif (likely(module_is_live(module) &&\n\t\t\t   atomic_inc_not_zero(&module->refcnt) != 0))\n\t\t\ttrace_module_get(module, _RET_IP_);\n\t\telse\n\t\t\tret = false;\n\n\t\tpreempt_enable();\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(try_module_get);\n\nvoid module_put(struct module *module)\n{\n\tint ret;\n\n\tif (module) {\n\t\tpreempt_disable();\n\t\tret = atomic_dec_if_positive(&module->refcnt);\n\t\tWARN_ON(ret < 0);\t/* Failed to put refcount */\n\t\ttrace_module_put(module, _RET_IP_);\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL(module_put);\n\n#else /* !CONFIG_MODULE_UNLOAD */\nstatic inline void print_unload_info(struct seq_file *m, struct module *mod)\n{\n\t/* We don't know the usage count, or what modules are using. */\n\tseq_puts(m, \" - -\");\n}\n\nstatic inline void module_unload_free(struct module *mod)\n{\n}\n\nstatic int ref_module(struct module *a, struct module *b)\n{\n\treturn strong_try_module_get(b);\n}\n\nstatic inline int module_unload_init(struct module *mod)\n{\n\treturn 0;\n}\n#endif /* CONFIG_MODULE_UNLOAD */\n\nstatic size_t module_flags_taint(struct module *mod, char *buf)\n{\n\tsize_t l = 0;\n\tint i;\n\n\tfor (i = 0; i < TAINT_FLAGS_COUNT; i++) {\n\t\tif (taint_flags[i].module && test_bit(i, &mod->taints))\n\t\t\tbuf[l++] = taint_flags[i].c_true;\n\t}\n\n\treturn l;\n}\n\nstatic ssize_t show_initstate(struct module_attribute *mattr,\n\t\t\t      struct module_kobject *mk, char *buffer)\n{\n\tconst char *state = \"unknown\";\n\n\tswitch (mk->mod->state) {\n\tcase MODULE_STATE_LIVE:\n\t\tstate = \"live\";\n\t\tbreak;\n\tcase MODULE_STATE_COMING:\n\t\tstate = \"coming\";\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\tstate = \"going\";\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\treturn sprintf(buffer, \"%s\\n\", state);\n}\n\nstatic struct module_attribute modinfo_initstate =\n\t__ATTR(initstate, 0444, show_initstate, NULL);\n\nstatic ssize_t store_uevent(struct module_attribute *mattr,\n\t\t\t    struct module_kobject *mk,\n\t\t\t    const char *buffer, size_t count)\n{\n\tint rc;\n\n\trc = kobject_synth_uevent(&mk->kobj, buffer, count);\n\treturn rc ? rc : count;\n}\n\nstruct module_attribute module_uevent =\n\t__ATTR(uevent, 0200, NULL, store_uevent);\n\nstatic ssize_t show_coresize(struct module_attribute *mattr,\n\t\t\t     struct module_kobject *mk, char *buffer)\n{\n\treturn sprintf(buffer, \"%u\\n\", mk->mod->core_layout.size);\n}\n\nstatic struct module_attribute modinfo_coresize =\n\t__ATTR(coresize, 0444, show_coresize, NULL);\n\nstatic ssize_t show_initsize(struct module_attribute *mattr,\n\t\t\t     struct module_kobject *mk, char *buffer)\n{\n\treturn sprintf(buffer, \"%u\\n\", mk->mod->init_layout.size);\n}\n\nstatic struct module_attribute modinfo_initsize =\n\t__ATTR(initsize, 0444, show_initsize, NULL);\n\nstatic ssize_t show_taint(struct module_attribute *mattr,\n\t\t\t  struct module_kobject *mk, char *buffer)\n{\n\tsize_t l;\n\n\tl = module_flags_taint(mk->mod, buffer);\n\tbuffer[l++] = '\\n';\n\treturn l;\n}\n\nstatic struct module_attribute modinfo_taint =\n\t__ATTR(taint, 0444, show_taint, NULL);\n\nstatic struct module_attribute *modinfo_attrs[] = {\n\t&module_uevent,\n\t&modinfo_version,\n\t&modinfo_srcversion,\n\t&modinfo_initstate,\n\t&modinfo_coresize,\n\t&modinfo_initsize,\n\t&modinfo_taint,\n#ifdef CONFIG_MODULE_UNLOAD\n\t&modinfo_refcnt,\n#endif\n\tNULL,\n};\n\nstatic const char vermagic[] = VERMAGIC_STRING;\n\nstatic int try_to_force_load(struct module *mod, const char *reason)\n{\n#ifdef CONFIG_MODULE_FORCE_LOAD\n\tif (!test_taint(TAINT_FORCED_MODULE))\n\t\tpr_warn(\"%s: %s: kernel tainted.\\n\", mod->name, reason);\n\tadd_taint_module(mod, TAINT_FORCED_MODULE, LOCKDEP_NOW_UNRELIABLE);\n\treturn 0;\n#else\n\treturn -ENOEXEC;\n#endif\n}\n\n#ifdef CONFIG_MODVERSIONS\n\nstatic u32 resolve_rel_crc(const s32 *crc)\n{\n\treturn *(u32 *)((void *)crc + *crc);\n}\n\nstatic int check_version(const struct load_info *info,\n\t\t\t const char *symname,\n\t\t\t struct module *mod,\n\t\t\t const s32 *crc)\n{\n\tElf_Shdr *sechdrs = info->sechdrs;\n\tunsigned int versindex = info->index.vers;\n\tunsigned int i, num_versions;\n\tstruct modversion_info *versions;\n\n\t/* Exporting module didn't supply crcs?  OK, we're already tainted. */\n\tif (!crc)\n\t\treturn 1;\n\n\t/* No versions at all?  modprobe --force does this. */\n\tif (versindex == 0)\n\t\treturn try_to_force_load(mod, symname) == 0;\n\n\tversions = (void *) sechdrs[versindex].sh_addr;\n\tnum_versions = sechdrs[versindex].sh_size\n\t\t/ sizeof(struct modversion_info);\n\n\tfor (i = 0; i < num_versions; i++) {\n\t\tu32 crcval;\n\n\t\tif (strcmp(versions[i].name, symname) != 0)\n\t\t\tcontinue;\n\n\t\tif (IS_ENABLED(CONFIG_MODULE_REL_CRCS))\n\t\t\tcrcval = resolve_rel_crc(crc);\n\t\telse\n\t\t\tcrcval = *crc;\n\t\tif (versions[i].crc == crcval)\n\t\t\treturn 1;\n\t\tpr_debug(\"Found checksum %X vs module %lX\\n\",\n\t\t\t crcval, versions[i].crc);\n\t\tgoto bad_version;\n\t}\n\n\t/* Broken toolchain. Warn once, then let it go.. */\n\tpr_warn_once(\"%s: no symbol version for %s\\n\", info->name, symname);\n\treturn 1;\n\nbad_version:\n\tpr_warn(\"%s: disagrees about version of symbol %s\\n\",\n\t       info->name, symname);\n\treturn 0;\n}\n\nstatic inline int check_modstruct_version(const struct load_info *info,\n\t\t\t\t\t  struct module *mod)\n{\n\tstruct find_symbol_arg fsa = {\n\t\t.name\t= \"module_layout\",\n\t\t.gplok\t= true,\n\t};\n\n\t/*\n\t * Since this should be found in kernel (which can't be removed), no\n\t * locking is necessary -- use preempt_disable() to placate lockdep.\n\t */\n\tpreempt_disable();\n\tif (!find_symbol(&fsa)) {\n\t\tpreempt_enable();\n\t\tBUG();\n\t}\n\tpreempt_enable();\n\treturn check_version(info, \"module_layout\", mod, fsa.crc);\n}\n\n/* First part is kernel version, which we ignore if module has crcs. */\nstatic inline int same_magic(const char *amagic, const char *bmagic,\n\t\t\t     bool has_crcs)\n{\n\tif (has_crcs) {\n\t\tamagic += strcspn(amagic, \" \");\n\t\tbmagic += strcspn(bmagic, \" \");\n\t}\n\treturn strcmp(amagic, bmagic) == 0;\n}\n#else\nstatic inline int check_version(const struct load_info *info,\n\t\t\t\tconst char *symname,\n\t\t\t\tstruct module *mod,\n\t\t\t\tconst s32 *crc)\n{\n\treturn 1;\n}\n\nstatic inline int check_modstruct_version(const struct load_info *info,\n\t\t\t\t\t  struct module *mod)\n{\n\treturn 1;\n}\n\nstatic inline int same_magic(const char *amagic, const char *bmagic,\n\t\t\t     bool has_crcs)\n{\n\treturn strcmp(amagic, bmagic) == 0;\n}\n#endif /* CONFIG_MODVERSIONS */\n\nstatic char *get_modinfo(const struct load_info *info, const char *tag);\nstatic char *get_next_modinfo(const struct load_info *info, const char *tag,\n\t\t\t      char *prev);\n\nstatic int verify_namespace_is_imported(const struct load_info *info,\n\t\t\t\t\tconst struct kernel_symbol *sym,\n\t\t\t\t\tstruct module *mod)\n{\n\tconst char *namespace;\n\tchar *imported_namespace;\n\n\tnamespace = kernel_symbol_namespace(sym);\n\tif (namespace && namespace[0]) {\n\t\timported_namespace = get_modinfo(info, \"import_ns\");\n\t\twhile (imported_namespace) {\n\t\t\tif (strcmp(namespace, imported_namespace) == 0)\n\t\t\t\treturn 0;\n\t\t\timported_namespace = get_next_modinfo(\n\t\t\t\tinfo, \"import_ns\", imported_namespace);\n\t\t}\n#ifdef CONFIG_MODULE_ALLOW_MISSING_NAMESPACE_IMPORTS\n\t\tpr_warn(\n#else\n\t\tpr_err(\n#endif\n\t\t\t\"%s: module uses symbol (%s) from namespace %s, but does not import it.\\n\",\n\t\t\tmod->name, kernel_symbol_name(sym), namespace);\n#ifndef CONFIG_MODULE_ALLOW_MISSING_NAMESPACE_IMPORTS\n\t\treturn -EINVAL;\n#endif\n\t}\n\treturn 0;\n}\n\nstatic bool inherit_taint(struct module *mod, struct module *owner)\n{\n\tif (!owner || !test_bit(TAINT_PROPRIETARY_MODULE, &owner->taints))\n\t\treturn true;\n\n\tif (mod->using_gplonly_symbols) {\n\t\tpr_err(\"%s: module using GPL-only symbols uses symbols from proprietary module %s.\\n\",\n\t\t\tmod->name, owner->name);\n\t\treturn false;\n\t}\n\n\tif (!test_bit(TAINT_PROPRIETARY_MODULE, &mod->taints)) {\n\t\tpr_warn(\"%s: module uses symbols from proprietary module %s, inheriting taint.\\n\",\n\t\t\tmod->name, owner->name);\n\t\tset_bit(TAINT_PROPRIETARY_MODULE, &mod->taints);\n\t}\n\treturn true;\n}\n\n/* Resolve a symbol for this module.  I.e. if we find one, record usage. */\nstatic const struct kernel_symbol *resolve_symbol(struct module *mod,\n\t\t\t\t\t\t  const struct load_info *info,\n\t\t\t\t\t\t  const char *name,\n\t\t\t\t\t\t  char ownername[])\n{\n\tstruct find_symbol_arg fsa = {\n\t\t.name\t= name,\n\t\t.gplok\t= !(mod->taints & (1 << TAINT_PROPRIETARY_MODULE)),\n\t\t.warn\t= true,\n\t};\n\tint err;\n\n\t/*\n\t * The module_mutex should not be a heavily contended lock;\n\t * if we get the occasional sleep here, we'll go an extra iteration\n\t * in the wait_event_interruptible(), which is harmless.\n\t */\n\tsched_annotate_sleep();\n\tmutex_lock(&module_mutex);\n\tif (!find_symbol(&fsa))\n\t\tgoto unlock;\n\n\tif (fsa.license == GPL_ONLY)\n\t\tmod->using_gplonly_symbols = true;\n\n\tif (!inherit_taint(mod, fsa.owner)) {\n\t\tfsa.sym = NULL;\n\t\tgoto getname;\n\t}\n\n\tif (!check_version(info, name, mod, fsa.crc)) {\n\t\tfsa.sym = ERR_PTR(-EINVAL);\n\t\tgoto getname;\n\t}\n\n\terr = verify_namespace_is_imported(info, fsa.sym, mod);\n\tif (err) {\n\t\tfsa.sym = ERR_PTR(err);\n\t\tgoto getname;\n\t}\n\n\terr = ref_module(mod, fsa.owner);\n\tif (err) {\n\t\tfsa.sym = ERR_PTR(err);\n\t\tgoto getname;\n\t}\n\ngetname:\n\t/* We must make copy under the lock if we failed to get ref. */\n\tstrncpy(ownername, module_name(fsa.owner), MODULE_NAME_LEN);\nunlock:\n\tmutex_unlock(&module_mutex);\n\treturn fsa.sym;\n}\n\nstatic const struct kernel_symbol *\nresolve_symbol_wait(struct module *mod,\n\t\t    const struct load_info *info,\n\t\t    const char *name)\n{\n\tconst struct kernel_symbol *ksym;\n\tchar owner[MODULE_NAME_LEN];\n\n\tif (wait_event_interruptible_timeout(module_wq,\n\t\t\t!IS_ERR(ksym = resolve_symbol(mod, info, name, owner))\n\t\t\t|| PTR_ERR(ksym) != -EBUSY,\n\t\t\t\t\t     30 * HZ) <= 0) {\n\t\tpr_warn(\"%s: gave up waiting for init of module %s.\\n\",\n\t\t\tmod->name, owner);\n\t}\n\treturn ksym;\n}\n\n/*\n * /sys/module/foo/sections stuff\n * J. Corbet <corbet@lwn.net>\n */\n#ifdef CONFIG_SYSFS\n\n#ifdef CONFIG_KALLSYMS\nstatic inline bool sect_empty(const Elf_Shdr *sect)\n{\n\treturn !(sect->sh_flags & SHF_ALLOC) || sect->sh_size == 0;\n}\n\nstruct module_sect_attr {\n\tstruct bin_attribute battr;\n\tunsigned long address;\n};\n\nstruct module_sect_attrs {\n\tstruct attribute_group grp;\n\tunsigned int nsections;\n\tstruct module_sect_attr attrs[];\n};\n\n#define MODULE_SECT_READ_SIZE (3 /* \"0x\", \"\\n\" */ + (BITS_PER_LONG / 4))\nstatic ssize_t module_sect_read(struct file *file, struct kobject *kobj,\n\t\t\t\tstruct bin_attribute *battr,\n\t\t\t\tchar *buf, loff_t pos, size_t count)\n{\n\tstruct module_sect_attr *sattr =\n\t\tcontainer_of(battr, struct module_sect_attr, battr);\n\tchar bounce[MODULE_SECT_READ_SIZE + 1];\n\tsize_t wrote;\n\n\tif (pos != 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Since we're a binary read handler, we must account for the\n\t * trailing NUL byte that sprintf will write: if \"buf\" is\n\t * too small to hold the NUL, or the NUL is exactly the last\n\t * byte, the read will look like it got truncated by one byte.\n\t * Since there is no way to ask sprintf nicely to not write\n\t * the NUL, we have to use a bounce buffer.\n\t */\n\twrote = scnprintf(bounce, sizeof(bounce), \"0x%px\\n\",\n\t\t\t kallsyms_show_value(file->f_cred)\n\t\t\t\t? (void *)sattr->address : NULL);\n\tcount = min(count, wrote);\n\tmemcpy(buf, bounce, count);\n\n\treturn count;\n}\n\nstatic void free_sect_attrs(struct module_sect_attrs *sect_attrs)\n{\n\tunsigned int section;\n\n\tfor (section = 0; section < sect_attrs->nsections; section++)\n\t\tkfree(sect_attrs->attrs[section].battr.attr.name);\n\tkfree(sect_attrs);\n}\n\nstatic void add_sect_attrs(struct module *mod, const struct load_info *info)\n{\n\tunsigned int nloaded = 0, i, size[2];\n\tstruct module_sect_attrs *sect_attrs;\n\tstruct module_sect_attr *sattr;\n\tstruct bin_attribute **gattr;\n\n\t/* Count loaded sections and allocate structures */\n\tfor (i = 0; i < info->hdr->e_shnum; i++)\n\t\tif (!sect_empty(&info->sechdrs[i]))\n\t\t\tnloaded++;\n\tsize[0] = ALIGN(struct_size(sect_attrs, attrs, nloaded),\n\t\t\tsizeof(sect_attrs->grp.bin_attrs[0]));\n\tsize[1] = (nloaded + 1) * sizeof(sect_attrs->grp.bin_attrs[0]);\n\tsect_attrs = kzalloc(size[0] + size[1], GFP_KERNEL);\n\tif (sect_attrs == NULL)\n\t\treturn;\n\n\t/* Setup section attributes. */\n\tsect_attrs->grp.name = \"sections\";\n\tsect_attrs->grp.bin_attrs = (void *)sect_attrs + size[0];\n\n\tsect_attrs->nsections = 0;\n\tsattr = &sect_attrs->attrs[0];\n\tgattr = &sect_attrs->grp.bin_attrs[0];\n\tfor (i = 0; i < info->hdr->e_shnum; i++) {\n\t\tElf_Shdr *sec = &info->sechdrs[i];\n\t\tif (sect_empty(sec))\n\t\t\tcontinue;\n\t\tsysfs_bin_attr_init(&sattr->battr);\n\t\tsattr->address = sec->sh_addr;\n\t\tsattr->battr.attr.name =\n\t\t\tkstrdup(info->secstrings + sec->sh_name, GFP_KERNEL);\n\t\tif (sattr->battr.attr.name == NULL)\n\t\t\tgoto out;\n\t\tsect_attrs->nsections++;\n\t\tsattr->battr.read = module_sect_read;\n\t\tsattr->battr.size = MODULE_SECT_READ_SIZE;\n\t\tsattr->battr.attr.mode = 0400;\n\t\t*(gattr++) = &(sattr++)->battr;\n\t}\n\t*gattr = NULL;\n\n\tif (sysfs_create_group(&mod->mkobj.kobj, &sect_attrs->grp))\n\t\tgoto out;\n\n\tmod->sect_attrs = sect_attrs;\n\treturn;\n  out:\n\tfree_sect_attrs(sect_attrs);\n}\n\nstatic void remove_sect_attrs(struct module *mod)\n{\n\tif (mod->sect_attrs) {\n\t\tsysfs_remove_group(&mod->mkobj.kobj,\n\t\t\t\t   &mod->sect_attrs->grp);\n\t\t/*\n\t\t * We are positive that no one is using any sect attrs\n\t\t * at this point.  Deallocate immediately.\n\t\t */\n\t\tfree_sect_attrs(mod->sect_attrs);\n\t\tmod->sect_attrs = NULL;\n\t}\n}\n\n/*\n * /sys/module/foo/notes/.section.name gives contents of SHT_NOTE sections.\n */\n\nstruct module_notes_attrs {\n\tstruct kobject *dir;\n\tunsigned int notes;\n\tstruct bin_attribute attrs[];\n};\n\nstatic ssize_t module_notes_read(struct file *filp, struct kobject *kobj,\n\t\t\t\t struct bin_attribute *bin_attr,\n\t\t\t\t char *buf, loff_t pos, size_t count)\n{\n\t/*\n\t * The caller checked the pos and count against our size.\n\t */\n\tmemcpy(buf, bin_attr->private + pos, count);\n\treturn count;\n}\n\nstatic void free_notes_attrs(struct module_notes_attrs *notes_attrs,\n\t\t\t     unsigned int i)\n{\n\tif (notes_attrs->dir) {\n\t\twhile (i-- > 0)\n\t\t\tsysfs_remove_bin_file(notes_attrs->dir,\n\t\t\t\t\t      &notes_attrs->attrs[i]);\n\t\tkobject_put(notes_attrs->dir);\n\t}\n\tkfree(notes_attrs);\n}\n\nstatic void add_notes_attrs(struct module *mod, const struct load_info *info)\n{\n\tunsigned int notes, loaded, i;\n\tstruct module_notes_attrs *notes_attrs;\n\tstruct bin_attribute *nattr;\n\n\t/* failed to create section attributes, so can't create notes */\n\tif (!mod->sect_attrs)\n\t\treturn;\n\n\t/* Count notes sections and allocate structures.  */\n\tnotes = 0;\n\tfor (i = 0; i < info->hdr->e_shnum; i++)\n\t\tif (!sect_empty(&info->sechdrs[i]) &&\n\t\t    (info->sechdrs[i].sh_type == SHT_NOTE))\n\t\t\t++notes;\n\n\tif (notes == 0)\n\t\treturn;\n\n\tnotes_attrs = kzalloc(struct_size(notes_attrs, attrs, notes),\n\t\t\t      GFP_KERNEL);\n\tif (notes_attrs == NULL)\n\t\treturn;\n\n\tnotes_attrs->notes = notes;\n\tnattr = &notes_attrs->attrs[0];\n\tfor (loaded = i = 0; i < info->hdr->e_shnum; ++i) {\n\t\tif (sect_empty(&info->sechdrs[i]))\n\t\t\tcontinue;\n\t\tif (info->sechdrs[i].sh_type == SHT_NOTE) {\n\t\t\tsysfs_bin_attr_init(nattr);\n\t\t\tnattr->attr.name = mod->sect_attrs->attrs[loaded].battr.attr.name;\n\t\t\tnattr->attr.mode = S_IRUGO;\n\t\t\tnattr->size = info->sechdrs[i].sh_size;\n\t\t\tnattr->private = (void *) info->sechdrs[i].sh_addr;\n\t\t\tnattr->read = module_notes_read;\n\t\t\t++nattr;\n\t\t}\n\t\t++loaded;\n\t}\n\n\tnotes_attrs->dir = kobject_create_and_add(\"notes\", &mod->mkobj.kobj);\n\tif (!notes_attrs->dir)\n\t\tgoto out;\n\n\tfor (i = 0; i < notes; ++i)\n\t\tif (sysfs_create_bin_file(notes_attrs->dir,\n\t\t\t\t\t  &notes_attrs->attrs[i]))\n\t\t\tgoto out;\n\n\tmod->notes_attrs = notes_attrs;\n\treturn;\n\n  out:\n\tfree_notes_attrs(notes_attrs, i);\n}\n\nstatic void remove_notes_attrs(struct module *mod)\n{\n\tif (mod->notes_attrs)\n\t\tfree_notes_attrs(mod->notes_attrs, mod->notes_attrs->notes);\n}\n\n#else\n\nstatic inline void add_sect_attrs(struct module *mod,\n\t\t\t\t  const struct load_info *info)\n{\n}\n\nstatic inline void remove_sect_attrs(struct module *mod)\n{\n}\n\nstatic inline void add_notes_attrs(struct module *mod,\n\t\t\t\t   const struct load_info *info)\n{\n}\n\nstatic inline void remove_notes_attrs(struct module *mod)\n{\n}\n#endif /* CONFIG_KALLSYMS */\n\nstatic void del_usage_links(struct module *mod)\n{\n#ifdef CONFIG_MODULE_UNLOAD\n\tstruct module_use *use;\n\n\tmutex_lock(&module_mutex);\n\tlist_for_each_entry(use, &mod->target_list, target_list)\n\t\tsysfs_remove_link(use->target->holders_dir, mod->name);\n\tmutex_unlock(&module_mutex);\n#endif\n}\n\nstatic int add_usage_links(struct module *mod)\n{\n\tint ret = 0;\n#ifdef CONFIG_MODULE_UNLOAD\n\tstruct module_use *use;\n\n\tmutex_lock(&module_mutex);\n\tlist_for_each_entry(use, &mod->target_list, target_list) {\n\t\tret = sysfs_create_link(use->target->holders_dir,\n\t\t\t\t\t&mod->mkobj.kobj, mod->name);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&module_mutex);\n\tif (ret)\n\t\tdel_usage_links(mod);\n#endif\n\treturn ret;\n}\n\nstatic void module_remove_modinfo_attrs(struct module *mod, int end);\n\nstatic int module_add_modinfo_attrs(struct module *mod)\n{\n\tstruct module_attribute *attr;\n\tstruct module_attribute *temp_attr;\n\tint error = 0;\n\tint i;\n\n\tmod->modinfo_attrs = kzalloc((sizeof(struct module_attribute) *\n\t\t\t\t\t(ARRAY_SIZE(modinfo_attrs) + 1)),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!mod->modinfo_attrs)\n\t\treturn -ENOMEM;\n\n\ttemp_attr = mod->modinfo_attrs;\n\tfor (i = 0; (attr = modinfo_attrs[i]); i++) {\n\t\tif (!attr->test || attr->test(mod)) {\n\t\t\tmemcpy(temp_attr, attr, sizeof(*temp_attr));\n\t\t\tsysfs_attr_init(&temp_attr->attr);\n\t\t\terror = sysfs_create_file(&mod->mkobj.kobj,\n\t\t\t\t\t&temp_attr->attr);\n\t\t\tif (error)\n\t\t\t\tgoto error_out;\n\t\t\t++temp_attr;\n\t\t}\n\t}\n\n\treturn 0;\n\nerror_out:\n\tif (i > 0)\n\t\tmodule_remove_modinfo_attrs(mod, --i);\n\telse\n\t\tkfree(mod->modinfo_attrs);\n\treturn error;\n}\n\nstatic void module_remove_modinfo_attrs(struct module *mod, int end)\n{\n\tstruct module_attribute *attr;\n\tint i;\n\n\tfor (i = 0; (attr = &mod->modinfo_attrs[i]); i++) {\n\t\tif (end >= 0 && i > end)\n\t\t\tbreak;\n\t\t/* pick a field to test for end of list */\n\t\tif (!attr->attr.name)\n\t\t\tbreak;\n\t\tsysfs_remove_file(&mod->mkobj.kobj, &attr->attr);\n\t\tif (attr->free)\n\t\t\tattr->free(mod);\n\t}\n\tkfree(mod->modinfo_attrs);\n}\n\nstatic void mod_kobject_put(struct module *mod)\n{\n\tDECLARE_COMPLETION_ONSTACK(c);\n\tmod->mkobj.kobj_completion = &c;\n\tkobject_put(&mod->mkobj.kobj);\n\twait_for_completion(&c);\n}\n\nstatic int mod_sysfs_init(struct module *mod)\n{\n\tint err;\n\tstruct kobject *kobj;\n\n\tif (!module_sysfs_initialized) {\n\t\tpr_err(\"%s: module sysfs not initialized\\n\", mod->name);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tkobj = kset_find_obj(module_kset, mod->name);\n\tif (kobj) {\n\t\tpr_err(\"%s: module is already loaded\\n\", mod->name);\n\t\tkobject_put(kobj);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmod->mkobj.mod = mod;\n\n\tmemset(&mod->mkobj.kobj, 0, sizeof(mod->mkobj.kobj));\n\tmod->mkobj.kobj.kset = module_kset;\n\terr = kobject_init_and_add(&mod->mkobj.kobj, &module_ktype, NULL,\n\t\t\t\t   \"%s\", mod->name);\n\tif (err)\n\t\tmod_kobject_put(mod);\n\nout:\n\treturn err;\n}\n\nstatic int mod_sysfs_setup(struct module *mod,\n\t\t\t   const struct load_info *info,\n\t\t\t   struct kernel_param *kparam,\n\t\t\t   unsigned int num_params)\n{\n\tint err;\n\n\terr = mod_sysfs_init(mod);\n\tif (err)\n\t\tgoto out;\n\n\tmod->holders_dir = kobject_create_and_add(\"holders\", &mod->mkobj.kobj);\n\tif (!mod->holders_dir) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unreg;\n\t}\n\n\terr = module_param_sysfs_setup(mod, kparam, num_params);\n\tif (err)\n\t\tgoto out_unreg_holders;\n\n\terr = module_add_modinfo_attrs(mod);\n\tif (err)\n\t\tgoto out_unreg_param;\n\n\terr = add_usage_links(mod);\n\tif (err)\n\t\tgoto out_unreg_modinfo_attrs;\n\n\tadd_sect_attrs(mod, info);\n\tadd_notes_attrs(mod, info);\n\n\treturn 0;\n\nout_unreg_modinfo_attrs:\n\tmodule_remove_modinfo_attrs(mod, -1);\nout_unreg_param:\n\tmodule_param_sysfs_remove(mod);\nout_unreg_holders:\n\tkobject_put(mod->holders_dir);\nout_unreg:\n\tmod_kobject_put(mod);\nout:\n\treturn err;\n}\n\nstatic void mod_sysfs_fini(struct module *mod)\n{\n\tremove_notes_attrs(mod);\n\tremove_sect_attrs(mod);\n\tmod_kobject_put(mod);\n}\n\nstatic void init_param_lock(struct module *mod)\n{\n\tmutex_init(&mod->param_lock);\n}\n#else /* !CONFIG_SYSFS */\n\nstatic int mod_sysfs_setup(struct module *mod,\n\t\t\t   const struct load_info *info,\n\t\t\t   struct kernel_param *kparam,\n\t\t\t   unsigned int num_params)\n{\n\treturn 0;\n}\n\nstatic void mod_sysfs_fini(struct module *mod)\n{\n}\n\nstatic void module_remove_modinfo_attrs(struct module *mod, int end)\n{\n}\n\nstatic void del_usage_links(struct module *mod)\n{\n}\n\nstatic void init_param_lock(struct module *mod)\n{\n}\n#endif /* CONFIG_SYSFS */\n\nstatic void mod_sysfs_teardown(struct module *mod)\n{\n\tdel_usage_links(mod);\n\tmodule_remove_modinfo_attrs(mod, -1);\n\tmodule_param_sysfs_remove(mod);\n\tkobject_put(mod->mkobj.drivers_dir);\n\tkobject_put(mod->holders_dir);\n\tmod_sysfs_fini(mod);\n}\n\n/*\n * LKM RO/NX protection: protect module's text/ro-data\n * from modification and any data from execution.\n *\n * General layout of module is:\n *          [text] [read-only-data] [ro-after-init] [writable data]\n * text_size -----^                ^               ^               ^\n * ro_size ------------------------|               |               |\n * ro_after_init_size -----------------------------|               |\n * size -----------------------------------------------------------|\n *\n * These values are always page-aligned (as is base)\n */\n\n/*\n * Since some arches are moving towards PAGE_KERNEL module allocations instead\n * of PAGE_KERNEL_EXEC, keep frob_text() and module_enable_x() outside of the\n * CONFIG_STRICT_MODULE_RWX block below because they are needed regardless of\n * whether we are strict.\n */\n#ifdef CONFIG_ARCH_HAS_STRICT_MODULE_RWX\nstatic void frob_text(const struct module_layout *layout,\n\t\t      int (*set_memory)(unsigned long start, int num_pages))\n{\n\tBUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->text_size & (PAGE_SIZE-1));\n\tset_memory((unsigned long)layout->base,\n\t\t   layout->text_size >> PAGE_SHIFT);\n}\n\nstatic void module_enable_x(const struct module *mod)\n{\n\tfrob_text(&mod->core_layout, set_memory_x);\n\tfrob_text(&mod->init_layout, set_memory_x);\n}\n#else /* !CONFIG_ARCH_HAS_STRICT_MODULE_RWX */\nstatic void module_enable_x(const struct module *mod) { }\n#endif /* CONFIG_ARCH_HAS_STRICT_MODULE_RWX */\n\n#ifdef CONFIG_STRICT_MODULE_RWX\nstatic void frob_rodata(const struct module_layout *layout,\n\t\t\tint (*set_memory)(unsigned long start, int num_pages))\n{\n\tBUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->text_size & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->ro_size & (PAGE_SIZE-1));\n\tset_memory((unsigned long)layout->base + layout->text_size,\n\t\t   (layout->ro_size - layout->text_size) >> PAGE_SHIFT);\n}\n\nstatic void frob_ro_after_init(const struct module_layout *layout,\n\t\t\t\tint (*set_memory)(unsigned long start, int num_pages))\n{\n\tBUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->ro_size & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->ro_after_init_size & (PAGE_SIZE-1));\n\tset_memory((unsigned long)layout->base + layout->ro_size,\n\t\t   (layout->ro_after_init_size - layout->ro_size) >> PAGE_SHIFT);\n}\n\nstatic void frob_writable_data(const struct module_layout *layout,\n\t\t\t       int (*set_memory)(unsigned long start, int num_pages))\n{\n\tBUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->ro_after_init_size & (PAGE_SIZE-1));\n\tBUG_ON((unsigned long)layout->size & (PAGE_SIZE-1));\n\tset_memory((unsigned long)layout->base + layout->ro_after_init_size,\n\t\t   (layout->size - layout->ro_after_init_size) >> PAGE_SHIFT);\n}\n\nstatic void module_enable_ro(const struct module *mod, bool after_init)\n{\n\tif (!rodata_enabled)\n\t\treturn;\n\n\tset_vm_flush_reset_perms(mod->core_layout.base);\n\tset_vm_flush_reset_perms(mod->init_layout.base);\n\tfrob_text(&mod->core_layout, set_memory_ro);\n\n\tfrob_rodata(&mod->core_layout, set_memory_ro);\n\tfrob_text(&mod->init_layout, set_memory_ro);\n\tfrob_rodata(&mod->init_layout, set_memory_ro);\n\n\tif (after_init)\n\t\tfrob_ro_after_init(&mod->core_layout, set_memory_ro);\n}\n\nstatic void module_enable_nx(const struct module *mod)\n{\n\tfrob_rodata(&mod->core_layout, set_memory_nx);\n\tfrob_ro_after_init(&mod->core_layout, set_memory_nx);\n\tfrob_writable_data(&mod->core_layout, set_memory_nx);\n\tfrob_rodata(&mod->init_layout, set_memory_nx);\n\tfrob_writable_data(&mod->init_layout, set_memory_nx);\n}\n\nstatic int module_enforce_rwx_sections(Elf_Ehdr *hdr, Elf_Shdr *sechdrs,\n\t\t\t\t       char *secstrings, struct module *mod)\n{\n\tconst unsigned long shf_wx = SHF_WRITE|SHF_EXECINSTR;\n\tint i;\n\n\tfor (i = 0; i < hdr->e_shnum; i++) {\n\t\tif ((sechdrs[i].sh_flags & shf_wx) == shf_wx) {\n\t\t\tpr_err(\"%s: section %s (index %d) has invalid WRITE|EXEC flags\\n\",\n\t\t\t\tmod->name, secstrings + sechdrs[i].sh_name, i);\n\t\t\treturn -ENOEXEC;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#else /* !CONFIG_STRICT_MODULE_RWX */\nstatic void module_enable_nx(const struct module *mod) { }\nstatic void module_enable_ro(const struct module *mod, bool after_init) {}\nstatic int module_enforce_rwx_sections(Elf_Ehdr *hdr, Elf_Shdr *sechdrs,\n\t\t\t\t       char *secstrings, struct module *mod)\n{\n\treturn 0;\n}\n#endif /*  CONFIG_STRICT_MODULE_RWX */\n\n#ifdef CONFIG_LIVEPATCH\n/*\n * Persist Elf information about a module. Copy the Elf header,\n * section header table, section string table, and symtab section\n * index from info to mod->klp_info.\n */\nstatic int copy_module_elf(struct module *mod, struct load_info *info)\n{\n\tunsigned int size, symndx;\n\tint ret;\n\n\tsize = sizeof(*mod->klp_info);\n\tmod->klp_info = kmalloc(size, GFP_KERNEL);\n\tif (mod->klp_info == NULL)\n\t\treturn -ENOMEM;\n\n\t/* Elf header */\n\tsize = sizeof(mod->klp_info->hdr);\n\tmemcpy(&mod->klp_info->hdr, info->hdr, size);\n\n\t/* Elf section header table */\n\tsize = sizeof(*info->sechdrs) * info->hdr->e_shnum;\n\tmod->klp_info->sechdrs = kmemdup(info->sechdrs, size, GFP_KERNEL);\n\tif (mod->klp_info->sechdrs == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto free_info;\n\t}\n\n\t/* Elf section name string table */\n\tsize = info->sechdrs[info->hdr->e_shstrndx].sh_size;\n\tmod->klp_info->secstrings = kmemdup(info->secstrings, size, GFP_KERNEL);\n\tif (mod->klp_info->secstrings == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto free_sechdrs;\n\t}\n\n\t/* Elf symbol section index */\n\tsymndx = info->index.sym;\n\tmod->klp_info->symndx = symndx;\n\n\t/*\n\t * For livepatch modules, core_kallsyms.symtab is a complete\n\t * copy of the original symbol table. Adjust sh_addr to point\n\t * to core_kallsyms.symtab since the copy of the symtab in module\n\t * init memory is freed at the end of do_init_module().\n\t */\n\tmod->klp_info->sechdrs[symndx].sh_addr = \\\n\t\t(unsigned long) mod->core_kallsyms.symtab;\n\n\treturn 0;\n\nfree_sechdrs:\n\tkfree(mod->klp_info->sechdrs);\nfree_info:\n\tkfree(mod->klp_info);\n\treturn ret;\n}\n\nstatic void free_module_elf(struct module *mod)\n{\n\tkfree(mod->klp_info->sechdrs);\n\tkfree(mod->klp_info->secstrings);\n\tkfree(mod->klp_info);\n}\n#else /* !CONFIG_LIVEPATCH */\nstatic int copy_module_elf(struct module *mod, struct load_info *info)\n{\n\treturn 0;\n}\n\nstatic void free_module_elf(struct module *mod)\n{\n}\n#endif /* CONFIG_LIVEPATCH */\n\nvoid __weak module_memfree(void *module_region)\n{\n\t/*\n\t * This memory may be RO, and freeing RO memory in an interrupt is not\n\t * supported by vmalloc.\n\t */\n\tWARN_ON(in_interrupt());\n\tvfree(module_region);\n}\n\nvoid __weak module_arch_cleanup(struct module *mod)\n{\n}\n\nvoid __weak module_arch_freeing_init(struct module *mod)\n{\n}\n\nstatic void cfi_cleanup(struct module *mod);\n\n/* Free a module, remove from lists, etc. */\nstatic void free_module(struct module *mod)\n{\n\ttrace_module_free(mod);\n\n\tmod_sysfs_teardown(mod);\n\n\t/*\n\t * We leave it in list to prevent duplicate loads, but make sure\n\t * that noone uses it while it's being deconstructed.\n\t */\n\tmutex_lock(&module_mutex);\n\tmod->state = MODULE_STATE_UNFORMED;\n\tmutex_unlock(&module_mutex);\n\n\t/* Remove dynamic debug info */\n\tddebug_remove_module(mod->name);\n\n\t/* Arch-specific cleanup. */\n\tmodule_arch_cleanup(mod);\n\n\t/* Module unload stuff */\n\tmodule_unload_free(mod);\n\n\t/* Free any allocated parameters. */\n\tdestroy_params(mod->kp, mod->num_kp);\n\n\tif (is_livepatch_module(mod))\n\t\tfree_module_elf(mod);\n\n\t/* Now we can delete it from the lists */\n\tmutex_lock(&module_mutex);\n\t/* Unlink carefully: kallsyms could be walking list. */\n\tlist_del_rcu(&mod->list);\n\tmod_tree_remove(mod);\n\t/* Remove this module from bug list, this uses list_del_rcu */\n\tmodule_bug_cleanup(mod);\n\t/* Wait for RCU-sched synchronizing before releasing mod->list and buglist. */\n\tsynchronize_rcu();\n\tmutex_unlock(&module_mutex);\n\n\t/* Clean up CFI for the module. */\n\tcfi_cleanup(mod);\n\n\t/* This may be empty, but that's OK */\n\tmodule_arch_freeing_init(mod);\n\tmodule_memfree(mod->init_layout.base);\n\tkfree(mod->args);\n\tpercpu_modfree(mod);\n\n\t/* Free lock-classes; relies on the preceding sync_rcu(). */\n\tlockdep_free_key_range(mod->core_layout.base, mod->core_layout.size);\n\n\t/* Finally, free the core (containing the module structure) */\n\tmodule_memfree(mod->core_layout.base);\n}\n\nvoid *__symbol_get(const char *symbol)\n{\n\tstruct find_symbol_arg fsa = {\n\t\t.name\t= symbol,\n\t\t.gplok\t= true,\n\t\t.warn\t= true,\n\t};\n\n\tpreempt_disable();\n\tif (!find_symbol(&fsa) || strong_try_module_get(fsa.owner)) {\n\t\tpreempt_enable();\n\t\treturn NULL;\n\t}\n\tpreempt_enable();\n\treturn (void *)kernel_symbol_value(fsa.sym);\n}\nEXPORT_SYMBOL_GPL(__symbol_get);\n\n/*\n * Ensure that an exported symbol [global namespace] does not already exist\n * in the kernel or in some other module's exported symbol table.\n *\n * You must hold the module_mutex.\n */\nstatic int verify_exported_symbols(struct module *mod)\n{\n\tunsigned int i;\n\tconst struct kernel_symbol *s;\n\tstruct {\n\t\tconst struct kernel_symbol *sym;\n\t\tunsigned int num;\n\t} arr[] = {\n\t\t{ mod->syms, mod->num_syms },\n\t\t{ mod->gpl_syms, mod->num_gpl_syms },\n\t};\n\n\tfor (i = 0; i < ARRAY_SIZE(arr); i++) {\n\t\tfor (s = arr[i].sym; s < arr[i].sym + arr[i].num; s++) {\n\t\t\tstruct find_symbol_arg fsa = {\n\t\t\t\t.name\t= kernel_symbol_name(s),\n\t\t\t\t.gplok\t= true,\n\t\t\t};\n\t\t\tif (find_symbol(&fsa)) {\n\t\t\t\tpr_err(\"%s: exports duplicate symbol %s\"\n\t\t\t\t       \" (owned by %s)\\n\",\n\t\t\t\t       mod->name, kernel_symbol_name(s),\n\t\t\t\t       module_name(fsa.owner));\n\t\t\t\treturn -ENOEXEC;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic bool ignore_undef_symbol(Elf_Half emachine, const char *name)\n{\n\t/*\n\t * On x86, PIC code and Clang non-PIC code may have call foo@PLT. GNU as\n\t * before 2.37 produces an unreferenced _GLOBAL_OFFSET_TABLE_ on x86-64.\n\t * i386 has a similar problem but may not deserve a fix.\n\t *\n\t * If we ever have to ignore many symbols, consider refactoring the code to\n\t * only warn if referenced by a relocation.\n\t */\n\tif (emachine == EM_386 || emachine == EM_X86_64)\n\t\treturn !strcmp(name, \"_GLOBAL_OFFSET_TABLE_\");\n\treturn false;\n}\n\n/* Change all symbols so that st_value encodes the pointer directly. */\nstatic int simplify_symbols(struct module *mod, const struct load_info *info)\n{\n\tElf_Shdr *symsec = &info->sechdrs[info->index.sym];\n\tElf_Sym *sym = (void *)symsec->sh_addr;\n\tunsigned long secbase;\n\tunsigned int i;\n\tint ret = 0;\n\tconst struct kernel_symbol *ksym;\n\n\tfor (i = 1; i < symsec->sh_size / sizeof(Elf_Sym); i++) {\n\t\tconst char *name = info->strtab + sym[i].st_name;\n\n\t\tswitch (sym[i].st_shndx) {\n\t\tcase SHN_COMMON:\n\t\t\t/* Ignore common symbols */\n\t\t\tif (!strncmp(name, \"__gnu_lto\", 9))\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * We compiled with -fno-common.  These are not\n\t\t\t * supposed to happen.\n\t\t\t */\n\t\t\tpr_debug(\"Common symbol: %s\\n\", name);\n\t\t\tpr_warn(\"%s: please compile with -fno-common\\n\",\n\t\t\t       mod->name);\n\t\t\tret = -ENOEXEC;\n\t\t\tbreak;\n\n\t\tcase SHN_ABS:\n\t\t\t/* Don't need to do anything */\n\t\t\tpr_debug(\"Absolute symbol: 0x%08lx\\n\",\n\t\t\t       (long)sym[i].st_value);\n\t\t\tbreak;\n\n\t\tcase SHN_LIVEPATCH:\n\t\t\t/* Livepatch symbols are resolved by livepatch */\n\t\t\tbreak;\n\n\t\tcase SHN_UNDEF:\n\t\t\tksym = resolve_symbol_wait(mod, info, name);\n\t\t\t/* Ok if resolved.  */\n\t\t\tif (ksym && !IS_ERR(ksym)) {\n\t\t\t\tsym[i].st_value = kernel_symbol_value(ksym);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Ok if weak or ignored.  */\n\t\t\tif (!ksym &&\n\t\t\t    (ELF_ST_BIND(sym[i].st_info) == STB_WEAK ||\n\t\t\t     ignore_undef_symbol(info->hdr->e_machine, name)))\n\t\t\t\tbreak;\n\n\t\t\tret = PTR_ERR(ksym) ?: -ENOENT;\n\t\t\tpr_warn(\"%s: Unknown symbol %s (err %d)\\n\",\n\t\t\t\tmod->name, name, ret);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\t/* Divert to percpu allocation if a percpu var. */\n\t\t\tif (sym[i].st_shndx == info->index.pcpu)\n\t\t\t\tsecbase = (unsigned long)mod_percpu(mod);\n\t\t\telse\n\t\t\t\tsecbase = info->sechdrs[sym[i].st_shndx].sh_addr;\n\t\t\tsym[i].st_value += secbase;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int apply_relocations(struct module *mod, const struct load_info *info)\n{\n\tunsigned int i;\n\tint err = 0;\n\n\t/* Now do relocations. */\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tunsigned int infosec = info->sechdrs[i].sh_info;\n\n\t\t/* Not a valid relocation section? */\n\t\tif (infosec >= info->hdr->e_shnum)\n\t\t\tcontinue;\n\n\t\t/* Don't bother with non-allocated sections */\n\t\tif (!(info->sechdrs[infosec].sh_flags & SHF_ALLOC))\n\t\t\tcontinue;\n\n\t\tif (info->sechdrs[i].sh_flags & SHF_RELA_LIVEPATCH)\n\t\t\terr = klp_apply_section_relocs(mod, info->sechdrs,\n\t\t\t\t\t\t       info->secstrings,\n\t\t\t\t\t\t       info->strtab,\n\t\t\t\t\t\t       info->index.sym, i,\n\t\t\t\t\t\t       NULL);\n\t\telse if (info->sechdrs[i].sh_type == SHT_REL)\n\t\t\terr = apply_relocate(info->sechdrs, info->strtab,\n\t\t\t\t\t     info->index.sym, i, mod);\n\t\telse if (info->sechdrs[i].sh_type == SHT_RELA)\n\t\t\terr = apply_relocate_add(info->sechdrs, info->strtab,\n\t\t\t\t\t\t info->index.sym, i, mod);\n\t\tif (err < 0)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\n/* Additional bytes needed by arch in front of individual sections */\nunsigned int __weak arch_mod_section_prepend(struct module *mod,\n\t\t\t\t\t     unsigned int section)\n{\n\t/* default implementation just returns zero */\n\treturn 0;\n}\n\n/* Update size with this section: return offset. */\nstatic long get_offset(struct module *mod, unsigned int *size,\n\t\t       Elf_Shdr *sechdr, unsigned int section)\n{\n\tlong ret;\n\n\t*size += arch_mod_section_prepend(mod, section);\n\tret = ALIGN(*size, sechdr->sh_addralign ?: 1);\n\t*size = ret + sechdr->sh_size;\n\treturn ret;\n}\n\nstatic bool module_init_layout_section(const char *sname)\n{\n#ifndef CONFIG_MODULE_UNLOAD\n\tif (module_exit_section(sname))\n\t\treturn true;\n#endif\n\treturn module_init_section(sname);\n}\n\n/*\n * Lay out the SHF_ALLOC sections in a way not dissimilar to how ld\n * might -- code, read-only data, read-write data, small data.  Tally\n * sizes, and place the offsets into sh_entsize fields: high bit means it\n * belongs in init.\n */\nstatic void layout_sections(struct module *mod, struct load_info *info)\n{\n\tstatic unsigned long const masks[][2] = {\n\t\t/*\n\t\t * NOTE: all executable code must be the first section\n\t\t * in this array; otherwise modify the text_size\n\t\t * finder in the two loops below\n\t\t */\n\t\t{ SHF_EXECINSTR | SHF_ALLOC, ARCH_SHF_SMALL },\n\t\t{ SHF_ALLOC, SHF_WRITE | ARCH_SHF_SMALL },\n\t\t{ SHF_RO_AFTER_INIT | SHF_ALLOC, ARCH_SHF_SMALL },\n\t\t{ SHF_WRITE | SHF_ALLOC, ARCH_SHF_SMALL },\n\t\t{ ARCH_SHF_SMALL | SHF_ALLOC, 0 }\n\t};\n\tunsigned int m, i;\n\n\tfor (i = 0; i < info->hdr->e_shnum; i++)\n\t\tinfo->sechdrs[i].sh_entsize = ~0UL;\n\n\tpr_debug(\"Core section allocation order:\\n\");\n\tfor (m = 0; m < ARRAY_SIZE(masks); ++m) {\n\t\tfor (i = 0; i < info->hdr->e_shnum; ++i) {\n\t\t\tElf_Shdr *s = &info->sechdrs[i];\n\t\t\tconst char *sname = info->secstrings + s->sh_name;\n\n\t\t\tif ((s->sh_flags & masks[m][0]) != masks[m][0]\n\t\t\t    || (s->sh_flags & masks[m][1])\n\t\t\t    || s->sh_entsize != ~0UL\n\t\t\t    || module_init_layout_section(sname))\n\t\t\t\tcontinue;\n\t\t\ts->sh_entsize = get_offset(mod, &mod->core_layout.size, s, i);\n\t\t\tpr_debug(\"\\t%s\\n\", sname);\n\t\t}\n\t\tswitch (m) {\n\t\tcase 0: /* executable */\n\t\t\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\t\t\tmod->core_layout.text_size = mod->core_layout.size;\n\t\t\tbreak;\n\t\tcase 1: /* RO: text and ro-data */\n\t\t\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\t\t\tmod->core_layout.ro_size = mod->core_layout.size;\n\t\t\tbreak;\n\t\tcase 2: /* RO after init */\n\t\t\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\t\t\tmod->core_layout.ro_after_init_size = mod->core_layout.size;\n\t\t\tbreak;\n\t\tcase 4: /* whole core */\n\t\t\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tpr_debug(\"Init section allocation order:\\n\");\n\tfor (m = 0; m < ARRAY_SIZE(masks); ++m) {\n\t\tfor (i = 0; i < info->hdr->e_shnum; ++i) {\n\t\t\tElf_Shdr *s = &info->sechdrs[i];\n\t\t\tconst char *sname = info->secstrings + s->sh_name;\n\n\t\t\tif ((s->sh_flags & masks[m][0]) != masks[m][0]\n\t\t\t    || (s->sh_flags & masks[m][1])\n\t\t\t    || s->sh_entsize != ~0UL\n\t\t\t    || !module_init_layout_section(sname))\n\t\t\t\tcontinue;\n\t\t\ts->sh_entsize = (get_offset(mod, &mod->init_layout.size, s, i)\n\t\t\t\t\t | INIT_OFFSET_MASK);\n\t\t\tpr_debug(\"\\t%s\\n\", sname);\n\t\t}\n\t\tswitch (m) {\n\t\tcase 0: /* executable */\n\t\t\tmod->init_layout.size = debug_align(mod->init_layout.size);\n\t\t\tmod->init_layout.text_size = mod->init_layout.size;\n\t\t\tbreak;\n\t\tcase 1: /* RO: text and ro-data */\n\t\t\tmod->init_layout.size = debug_align(mod->init_layout.size);\n\t\t\tmod->init_layout.ro_size = mod->init_layout.size;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t/*\n\t\t\t * RO after init doesn't apply to init_layout (only\n\t\t\t * core_layout), so it just takes the value of ro_size.\n\t\t\t */\n\t\t\tmod->init_layout.ro_after_init_size = mod->init_layout.ro_size;\n\t\t\tbreak;\n\t\tcase 4: /* whole init */\n\t\t\tmod->init_layout.size = debug_align(mod->init_layout.size);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void set_license(struct module *mod, const char *license)\n{\n\tif (!license)\n\t\tlicense = \"unspecified\";\n\n\tif (!license_is_gpl_compatible(license)) {\n\t\tif (!test_taint(TAINT_PROPRIETARY_MODULE))\n\t\t\tpr_warn(\"%s: module license '%s' taints kernel.\\n\",\n\t\t\t\tmod->name, license);\n\t\tadd_taint_module(mod, TAINT_PROPRIETARY_MODULE,\n\t\t\t\t LOCKDEP_NOW_UNRELIABLE);\n\t}\n}\n\n/* Parse tag=value strings from .modinfo section */\nstatic char *next_string(char *string, unsigned long *secsize)\n{\n\t/* Skip non-zero chars */\n\twhile (string[0]) {\n\t\tstring++;\n\t\tif ((*secsize)-- <= 1)\n\t\t\treturn NULL;\n\t}\n\n\t/* Skip any zero padding. */\n\twhile (!string[0]) {\n\t\tstring++;\n\t\tif ((*secsize)-- <= 1)\n\t\t\treturn NULL;\n\t}\n\treturn string;\n}\n\nstatic char *get_next_modinfo(const struct load_info *info, const char *tag,\n\t\t\t      char *prev)\n{\n\tchar *p;\n\tunsigned int taglen = strlen(tag);\n\tElf_Shdr *infosec = &info->sechdrs[info->index.info];\n\tunsigned long size = infosec->sh_size;\n\n\t/*\n\t * get_modinfo() calls made before rewrite_section_headers()\n\t * must use sh_offset, as sh_addr isn't set!\n\t */\n\tchar *modinfo = (char *)info->hdr + infosec->sh_offset;\n\n\tif (prev) {\n\t\tsize -= prev - modinfo;\n\t\tmodinfo = next_string(prev, &size);\n\t}\n\n\tfor (p = modinfo; p; p = next_string(p, &size)) {\n\t\tif (strncmp(p, tag, taglen) == 0 && p[taglen] == '=')\n\t\t\treturn p + taglen + 1;\n\t}\n\treturn NULL;\n}\n\nstatic char *get_modinfo(const struct load_info *info, const char *tag)\n{\n\treturn get_next_modinfo(info, tag, NULL);\n}\n\nstatic void setup_modinfo(struct module *mod, struct load_info *info)\n{\n\tstruct module_attribute *attr;\n\tint i;\n\n\tfor (i = 0; (attr = modinfo_attrs[i]); i++) {\n\t\tif (attr->setup)\n\t\t\tattr->setup(mod, get_modinfo(info, attr->attr.name));\n\t}\n}\n\nstatic void free_modinfo(struct module *mod)\n{\n\tstruct module_attribute *attr;\n\tint i;\n\n\tfor (i = 0; (attr = modinfo_attrs[i]); i++) {\n\t\tif (attr->free)\n\t\t\tattr->free(mod);\n\t}\n}\n\n#ifdef CONFIG_KALLSYMS\n\n/* Lookup exported symbol in given range of kernel_symbols */\nstatic const struct kernel_symbol *lookup_exported_symbol(const char *name,\n\t\t\t\t\t\t\t  const struct kernel_symbol *start,\n\t\t\t\t\t\t\t  const struct kernel_symbol *stop)\n{\n\treturn bsearch(name, start, stop - start,\n\t\t\tsizeof(struct kernel_symbol), cmp_name);\n}\n\nstatic int is_exported(const char *name, unsigned long value,\n\t\t       const struct module *mod)\n{\n\tconst struct kernel_symbol *ks;\n\tif (!mod)\n\t\tks = lookup_exported_symbol(name, __start___ksymtab, __stop___ksymtab);\n\telse\n\t\tks = lookup_exported_symbol(name, mod->syms, mod->syms + mod->num_syms);\n\n\treturn ks != NULL && kernel_symbol_value(ks) == value;\n}\n\n/* As per nm */\nstatic char elf_type(const Elf_Sym *sym, const struct load_info *info)\n{\n\tconst Elf_Shdr *sechdrs = info->sechdrs;\n\n\tif (ELF_ST_BIND(sym->st_info) == STB_WEAK) {\n\t\tif (ELF_ST_TYPE(sym->st_info) == STT_OBJECT)\n\t\t\treturn 'v';\n\t\telse\n\t\t\treturn 'w';\n\t}\n\tif (sym->st_shndx == SHN_UNDEF)\n\t\treturn 'U';\n\tif (sym->st_shndx == SHN_ABS || sym->st_shndx == info->index.pcpu)\n\t\treturn 'a';\n\tif (sym->st_shndx >= SHN_LORESERVE)\n\t\treturn '?';\n\tif (sechdrs[sym->st_shndx].sh_flags & SHF_EXECINSTR)\n\t\treturn 't';\n\tif (sechdrs[sym->st_shndx].sh_flags & SHF_ALLOC\n\t    && sechdrs[sym->st_shndx].sh_type != SHT_NOBITS) {\n\t\tif (!(sechdrs[sym->st_shndx].sh_flags & SHF_WRITE))\n\t\t\treturn 'r';\n\t\telse if (sechdrs[sym->st_shndx].sh_flags & ARCH_SHF_SMALL)\n\t\t\treturn 'g';\n\t\telse\n\t\t\treturn 'd';\n\t}\n\tif (sechdrs[sym->st_shndx].sh_type == SHT_NOBITS) {\n\t\tif (sechdrs[sym->st_shndx].sh_flags & ARCH_SHF_SMALL)\n\t\t\treturn 's';\n\t\telse\n\t\t\treturn 'b';\n\t}\n\tif (strstarts(info->secstrings + sechdrs[sym->st_shndx].sh_name,\n\t\t      \".debug\")) {\n\t\treturn 'n';\n\t}\n\treturn '?';\n}\n\nstatic bool is_core_symbol(const Elf_Sym *src, const Elf_Shdr *sechdrs,\n\t\t\tunsigned int shnum, unsigned int pcpundx)\n{\n\tconst Elf_Shdr *sec;\n\n\tif (src->st_shndx == SHN_UNDEF\n\t    || src->st_shndx >= shnum\n\t    || !src->st_name)\n\t\treturn false;\n\n#ifdef CONFIG_KALLSYMS_ALL\n\tif (src->st_shndx == pcpundx)\n\t\treturn true;\n#endif\n\n\tsec = sechdrs + src->st_shndx;\n\tif (!(sec->sh_flags & SHF_ALLOC)\n#ifndef CONFIG_KALLSYMS_ALL\n\t    || !(sec->sh_flags & SHF_EXECINSTR)\n#endif\n\t    || (sec->sh_entsize & INIT_OFFSET_MASK))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * We only allocate and copy the strings needed by the parts of symtab\n * we keep.  This is simple, but has the effect of making multiple\n * copies of duplicates.  We could be more sophisticated, see\n * linux-kernel thread starting with\n * <73defb5e4bca04a6431392cc341112b1@localhost>.\n */\nstatic void layout_symtab(struct module *mod, struct load_info *info)\n{\n\tElf_Shdr *symsect = info->sechdrs + info->index.sym;\n\tElf_Shdr *strsect = info->sechdrs + info->index.str;\n\tconst Elf_Sym *src;\n\tunsigned int i, nsrc, ndst, strtab_size = 0;\n\n\t/* Put symbol section at end of init part of module. */\n\tsymsect->sh_flags |= SHF_ALLOC;\n\tsymsect->sh_entsize = get_offset(mod, &mod->init_layout.size, symsect,\n\t\t\t\t\t info->index.sym) | INIT_OFFSET_MASK;\n\tpr_debug(\"\\t%s\\n\", info->secstrings + symsect->sh_name);\n\n\tsrc = (void *)info->hdr + symsect->sh_offset;\n\tnsrc = symsect->sh_size / sizeof(*src);\n\n\t/* Compute total space required for the core symbols' strtab. */\n\tfor (ndst = i = 0; i < nsrc; i++) {\n\t\tif (i == 0 || is_livepatch_module(mod) ||\n\t\t    is_core_symbol(src+i, info->sechdrs, info->hdr->e_shnum,\n\t\t\t\t   info->index.pcpu)) {\n\t\t\tstrtab_size += strlen(&info->strtab[src[i].st_name])+1;\n\t\t\tndst++;\n\t\t}\n\t}\n\n\t/* Append room for core symbols at end of core part. */\n\tinfo->symoffs = ALIGN(mod->core_layout.size, symsect->sh_addralign ?: 1);\n\tinfo->stroffs = mod->core_layout.size = info->symoffs + ndst * sizeof(Elf_Sym);\n\tmod->core_layout.size += strtab_size;\n\tinfo->core_typeoffs = mod->core_layout.size;\n\tmod->core_layout.size += ndst * sizeof(char);\n\tmod->core_layout.size = debug_align(mod->core_layout.size);\n\n\t/* Put string table section at end of init part of module. */\n\tstrsect->sh_flags |= SHF_ALLOC;\n\tstrsect->sh_entsize = get_offset(mod, &mod->init_layout.size, strsect,\n\t\t\t\t\t info->index.str) | INIT_OFFSET_MASK;\n\tpr_debug(\"\\t%s\\n\", info->secstrings + strsect->sh_name);\n\n\t/* We'll tack temporary mod_kallsyms on the end. */\n\tmod->init_layout.size = ALIGN(mod->init_layout.size,\n\t\t\t\t      __alignof__(struct mod_kallsyms));\n\tinfo->mod_kallsyms_init_off = mod->init_layout.size;\n\tmod->init_layout.size += sizeof(struct mod_kallsyms);\n\tinfo->init_typeoffs = mod->init_layout.size;\n\tmod->init_layout.size += nsrc * sizeof(char);\n\tmod->init_layout.size = debug_align(mod->init_layout.size);\n}\n\n/*\n * We use the full symtab and strtab which layout_symtab arranged to\n * be appended to the init section.  Later we switch to the cut-down\n * core-only ones.\n */\nstatic void add_kallsyms(struct module *mod, const struct load_info *info)\n{\n\tunsigned int i, ndst;\n\tconst Elf_Sym *src;\n\tElf_Sym *dst;\n\tchar *s;\n\tElf_Shdr *symsec = &info->sechdrs[info->index.sym];\n\n\t/* Set up to point into init section. */\n\tmod->kallsyms = mod->init_layout.base + info->mod_kallsyms_init_off;\n\n\tmod->kallsyms->symtab = (void *)symsec->sh_addr;\n\tmod->kallsyms->num_symtab = symsec->sh_size / sizeof(Elf_Sym);\n\t/* Make sure we get permanent strtab: don't use info->strtab. */\n\tmod->kallsyms->strtab = (void *)info->sechdrs[info->index.str].sh_addr;\n\tmod->kallsyms->typetab = mod->init_layout.base + info->init_typeoffs;\n\n\t/*\n\t * Now populate the cut down core kallsyms for after init\n\t * and set types up while we still have access to sections.\n\t */\n\tmod->core_kallsyms.symtab = dst = mod->core_layout.base + info->symoffs;\n\tmod->core_kallsyms.strtab = s = mod->core_layout.base + info->stroffs;\n\tmod->core_kallsyms.typetab = mod->core_layout.base + info->core_typeoffs;\n\tsrc = mod->kallsyms->symtab;\n\tfor (ndst = i = 0; i < mod->kallsyms->num_symtab; i++) {\n\t\tmod->kallsyms->typetab[i] = elf_type(src + i, info);\n\t\tif (i == 0 || is_livepatch_module(mod) ||\n\t\t    is_core_symbol(src+i, info->sechdrs, info->hdr->e_shnum,\n\t\t\t\t   info->index.pcpu)) {\n\t\t\tmod->core_kallsyms.typetab[ndst] =\n\t\t\t    mod->kallsyms->typetab[i];\n\t\t\tdst[ndst] = src[i];\n\t\t\tdst[ndst++].st_name = s - mod->core_kallsyms.strtab;\n\t\t\ts += strlcpy(s, &mod->kallsyms->strtab[src[i].st_name],\n\t\t\t\t     KSYM_NAME_LEN) + 1;\n\t\t}\n\t}\n\tmod->core_kallsyms.num_symtab = ndst;\n}\n#else\nstatic inline void layout_symtab(struct module *mod, struct load_info *info)\n{\n}\n\nstatic void add_kallsyms(struct module *mod, const struct load_info *info)\n{\n}\n#endif /* CONFIG_KALLSYMS */\n\nstatic void dynamic_debug_setup(struct module *mod, struct _ddebug *debug, unsigned int num)\n{\n\tif (!debug)\n\t\treturn;\n\tddebug_add_module(debug, num, mod->name);\n}\n\nstatic void dynamic_debug_remove(struct module *mod, struct _ddebug *debug)\n{\n\tif (debug)\n\t\tddebug_remove_module(mod->name);\n}\n\nvoid * __weak module_alloc(unsigned long size)\n{\n\treturn __vmalloc_node_range(size, 1, VMALLOC_START, VMALLOC_END,\n\t\t\tGFP_KERNEL, PAGE_KERNEL_EXEC, VM_FLUSH_RESET_PERMS,\n\t\t\tNUMA_NO_NODE, __builtin_return_address(0));\n}\n\nbool __weak module_init_section(const char *name)\n{\n\treturn strstarts(name, \".init\");\n}\n\nbool __weak module_exit_section(const char *name)\n{\n\treturn strstarts(name, \".exit\");\n}\n\n#ifdef CONFIG_DEBUG_KMEMLEAK\nstatic void kmemleak_load_module(const struct module *mod,\n\t\t\t\t const struct load_info *info)\n{\n\tunsigned int i;\n\n\t/* only scan the sections containing data */\n\tkmemleak_scan_area(mod, sizeof(struct module), GFP_KERNEL);\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\t/* Scan all writable sections that's not executable */\n\t\tif (!(info->sechdrs[i].sh_flags & SHF_ALLOC) ||\n\t\t    !(info->sechdrs[i].sh_flags & SHF_WRITE) ||\n\t\t    (info->sechdrs[i].sh_flags & SHF_EXECINSTR))\n\t\t\tcontinue;\n\n\t\tkmemleak_scan_area((void *)info->sechdrs[i].sh_addr,\n\t\t\t\t   info->sechdrs[i].sh_size, GFP_KERNEL);\n\t}\n}\n#else\nstatic inline void kmemleak_load_module(const struct module *mod,\n\t\t\t\t\tconst struct load_info *info)\n{\n}\n#endif\n\n#ifdef CONFIG_MODULE_SIG\nstatic int module_sig_check(struct load_info *info, int flags)\n{\n\tint err = -ENODATA;\n\tconst unsigned long markerlen = sizeof(MODULE_SIG_STRING) - 1;\n\tconst char *reason;\n\tconst void *mod = info->hdr;\n\n\t/*\n\t * Require flags == 0, as a module with version information\n\t * removed is no longer the module that was signed\n\t */\n\tif (flags == 0 &&\n\t    info->len > markerlen &&\n\t    memcmp(mod + info->len - markerlen, MODULE_SIG_STRING, markerlen) == 0) {\n\t\t/* We truncate the module to discard the signature */\n\t\tinfo->len -= markerlen;\n\t\terr = mod_verify_sig(mod, info);\n\t\tif (!err) {\n\t\t\tinfo->sig_ok = true;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * We don't permit modules to be loaded into the trusted kernels\n\t * without a valid signature on them, but if we're not enforcing,\n\t * certain errors are non-fatal.\n\t */\n\tswitch (err) {\n\tcase -ENODATA:\n\t\treason = \"unsigned module\";\n\t\tbreak;\n\tcase -ENOPKG:\n\t\treason = \"module with unsupported crypto\";\n\t\tbreak;\n\tcase -ENOKEY:\n\t\treason = \"module with unavailable key\";\n\t\tbreak;\n\n\tdefault:\n\t\t/*\n\t\t * All other errors are fatal, including lack of memory,\n\t\t * unparseable signatures, and signature check failures --\n\t\t * even if signatures aren't required.\n\t\t */\n\t\treturn err;\n\t}\n\n\tif (is_module_sig_enforced()) {\n\t\tpr_notice(\"Loading of %s is rejected\\n\", reason);\n\t\treturn -EKEYREJECTED;\n\t}\n\n\treturn security_locked_down(LOCKDOWN_MODULE_SIGNATURE);\n}\n#else /* !CONFIG_MODULE_SIG */\nstatic int module_sig_check(struct load_info *info, int flags)\n{\n\treturn 0;\n}\n#endif /* !CONFIG_MODULE_SIG */\n\nstatic int validate_section_offset(struct load_info *info, Elf_Shdr *shdr)\n{\n\tunsigned long secend;\n\n\t/*\n\t * Check for both overflow and offset/size being\n\t * too large.\n\t */\n\tsecend = shdr->sh_offset + shdr->sh_size;\n\tif (secend < shdr->sh_offset || secend > info->len)\n\t\treturn -ENOEXEC;\n\n\treturn 0;\n}\n\n/*\n * Sanity checks against invalid binaries, wrong arch, weird elf version.\n *\n * Also do basic validity checks against section offsets and sizes, the\n * section name string table, and the indices used for it (sh_name).\n */\nstatic int elf_validity_check(struct load_info *info)\n{\n\tunsigned int i;\n\tElf_Shdr *shdr, *strhdr;\n\tint err;\n\n\tif (info->len < sizeof(*(info->hdr)))\n\t\treturn -ENOEXEC;\n\n\tif (memcmp(info->hdr->e_ident, ELFMAG, SELFMAG) != 0\n\t    || info->hdr->e_type != ET_REL\n\t    || !elf_check_arch(info->hdr)\n\t    || info->hdr->e_shentsize != sizeof(Elf_Shdr))\n\t\treturn -ENOEXEC;\n\n\t/*\n\t * e_shnum is 16 bits, and sizeof(Elf_Shdr) is\n\t * known and small. So e_shnum * sizeof(Elf_Shdr)\n\t * will not overflow unsigned long on any platform.\n\t */\n\tif (info->hdr->e_shoff >= info->len\n\t    || (info->hdr->e_shnum * sizeof(Elf_Shdr) >\n\t\tinfo->len - info->hdr->e_shoff))\n\t\treturn -ENOEXEC;\n\n\tinfo->sechdrs = (void *)info->hdr + info->hdr->e_shoff;\n\n\t/*\n\t * Verify if the section name table index is valid.\n\t */\n\tif (info->hdr->e_shstrndx == SHN_UNDEF\n\t    || info->hdr->e_shstrndx >= info->hdr->e_shnum)\n\t\treturn -ENOEXEC;\n\n\tstrhdr = &info->sechdrs[info->hdr->e_shstrndx];\n\terr = validate_section_offset(info, strhdr);\n\tif (err < 0)\n\t\treturn err;\n\n\t/*\n\t * The section name table must be NUL-terminated, as required\n\t * by the spec. This makes strcmp and pr_* calls that access\n\t * strings in the section safe.\n\t */\n\tinfo->secstrings = (void *)info->hdr + strhdr->sh_offset;\n\tif (info->secstrings[strhdr->sh_size - 1] != '\\0')\n\t\treturn -ENOEXEC;\n\n\t/*\n\t * The code assumes that section 0 has a length of zero and\n\t * an addr of zero, so check for it.\n\t */\n\tif (info->sechdrs[0].sh_type != SHT_NULL\n\t    || info->sechdrs[0].sh_size != 0\n\t    || info->sechdrs[0].sh_addr != 0)\n\t\treturn -ENOEXEC;\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tshdr = &info->sechdrs[i];\n\t\tswitch (shdr->sh_type) {\n\t\tcase SHT_NULL:\n\t\tcase SHT_NOBITS:\n\t\t\tcontinue;\n\t\tcase SHT_SYMTAB:\n\t\t\tif (shdr->sh_link == SHN_UNDEF\n\t\t\t    || shdr->sh_link >= info->hdr->e_shnum)\n\t\t\t\treturn -ENOEXEC;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\terr = validate_section_offset(info, shdr);\n\t\t\tif (err < 0) {\n\t\t\t\tpr_err(\"Invalid ELF section in module (section %u type %u)\\n\",\n\t\t\t\t\ti, shdr->sh_type);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\tif (shdr->sh_flags & SHF_ALLOC) {\n\t\t\t\tif (shdr->sh_name >= strhdr->sh_size) {\n\t\t\t\t\tpr_err(\"Invalid ELF section name in module (section %u type %u)\\n\",\n\t\t\t\t\t       i, shdr->sh_type);\n\t\t\t\t\treturn -ENOEXEC;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#define COPY_CHUNK_SIZE (16*PAGE_SIZE)\n\nstatic int copy_chunked_from_user(void *dst, const void __user *usrc, unsigned long len)\n{\n\tdo {\n\t\tunsigned long n = min(len, COPY_CHUNK_SIZE);\n\n\t\tif (copy_from_user(dst, usrc, n) != 0)\n\t\t\treturn -EFAULT;\n\t\tcond_resched();\n\t\tdst += n;\n\t\tusrc += n;\n\t\tlen -= n;\n\t} while (len);\n\treturn 0;\n}\n\n#ifdef CONFIG_LIVEPATCH\nstatic int check_modinfo_livepatch(struct module *mod, struct load_info *info)\n{\n\tif (get_modinfo(info, \"livepatch\")) {\n\t\tmod->klp = true;\n\t\tadd_taint_module(mod, TAINT_LIVEPATCH, LOCKDEP_STILL_OK);\n\t\tpr_notice_once(\"%s: tainting kernel with TAINT_LIVEPATCH\\n\",\n\t\t\t       mod->name);\n\t}\n\n\treturn 0;\n}\n#else /* !CONFIG_LIVEPATCH */\nstatic int check_modinfo_livepatch(struct module *mod, struct load_info *info)\n{\n\tif (get_modinfo(info, \"livepatch\")) {\n\t\tpr_err(\"%s: module is marked as livepatch module, but livepatch support is disabled\",\n\t\t       mod->name);\n\t\treturn -ENOEXEC;\n\t}\n\n\treturn 0;\n}\n#endif /* CONFIG_LIVEPATCH */\n\nstatic void check_modinfo_retpoline(struct module *mod, struct load_info *info)\n{\n\tif (retpoline_module_ok(get_modinfo(info, \"retpoline\")))\n\t\treturn;\n\n\tpr_warn(\"%s: loading module not compiled with retpoline compiler.\\n\",\n\t\tmod->name);\n}\n\n/* Sets info->hdr and info->len. */\nstatic int copy_module_from_user(const void __user *umod, unsigned long len,\n\t\t\t\t  struct load_info *info)\n{\n\tint err;\n\n\tinfo->len = len;\n\tif (info->len < sizeof(*(info->hdr)))\n\t\treturn -ENOEXEC;\n\n\terr = security_kernel_load_data(LOADING_MODULE, true);\n\tif (err)\n\t\treturn err;\n\n\t/* Suck in entire file: we'll want most of it. */\n\tinfo->hdr = __vmalloc(info->len, GFP_KERNEL | __GFP_NOWARN);\n\tif (!info->hdr)\n\t\treturn -ENOMEM;\n\n\tif (copy_chunked_from_user(info->hdr, umod, info->len) != 0) {\n\t\terr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\terr = security_kernel_post_load_data((char *)info->hdr, info->len,\n\t\t\t\t\t     LOADING_MODULE, \"init_module\");\nout:\n\tif (err)\n\t\tvfree(info->hdr);\n\n\treturn err;\n}\n\nstatic void free_copy(struct load_info *info)\n{\n\tvfree(info->hdr);\n}\n\nstatic int rewrite_section_headers(struct load_info *info, int flags)\n{\n\tunsigned int i;\n\n\t/* This should always be true, but let's be sure. */\n\tinfo->sechdrs[0].sh_addr = 0;\n\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tElf_Shdr *shdr = &info->sechdrs[i];\n\n\t\t/*\n\t\t * Mark all sections sh_addr with their address in the\n\t\t * temporary image.\n\t\t */\n\t\tshdr->sh_addr = (size_t)info->hdr + shdr->sh_offset;\n\n\t}\n\n\t/* Track but don't keep modinfo and version sections. */\n\tinfo->sechdrs[info->index.vers].sh_flags &= ~(unsigned long)SHF_ALLOC;\n\tinfo->sechdrs[info->index.info].sh_flags &= ~(unsigned long)SHF_ALLOC;\n\n\treturn 0;\n}\n\n/*\n * Set up our basic convenience variables (pointers to section headers,\n * search for module section index etc), and do some basic section\n * verification.\n *\n * Set info->mod to the temporary copy of the module in info->hdr. The final one\n * will be allocated in move_module().\n */\nstatic int setup_load_info(struct load_info *info, int flags)\n{\n\tunsigned int i;\n\n\t/* Try to find a name early so we can log errors with a module name */\n\tinfo->index.info = find_sec(info, \".modinfo\");\n\tif (info->index.info)\n\t\tinfo->name = get_modinfo(info, \"name\");\n\n\t/* Find internal symbols and strings. */\n\tfor (i = 1; i < info->hdr->e_shnum; i++) {\n\t\tif (info->sechdrs[i].sh_type == SHT_SYMTAB) {\n\t\t\tinfo->index.sym = i;\n\t\t\tinfo->index.str = info->sechdrs[i].sh_link;\n\t\t\tinfo->strtab = (char *)info->hdr\n\t\t\t\t+ info->sechdrs[info->index.str].sh_offset;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (info->index.sym == 0) {\n\t\tpr_warn(\"%s: module has no symbols (stripped?)\\n\",\n\t\t\tinfo->name ?: \"(missing .modinfo section or name field)\");\n\t\treturn -ENOEXEC;\n\t}\n\n\tinfo->index.mod = find_sec(info, \".gnu.linkonce.this_module\");\n\tif (!info->index.mod) {\n\t\tpr_warn(\"%s: No module found in object\\n\",\n\t\t\tinfo->name ?: \"(missing .modinfo section or name field)\");\n\t\treturn -ENOEXEC;\n\t}\n\t/* This is temporary: point mod into copy of data. */\n\tinfo->mod = (void *)info->hdr + info->sechdrs[info->index.mod].sh_offset;\n\n\t/*\n\t * If we didn't load the .modinfo 'name' field earlier, fall back to\n\t * on-disk struct mod 'name' field.\n\t */\n\tif (!info->name)\n\t\tinfo->name = info->mod->name;\n\n\tif (flags & MODULE_INIT_IGNORE_MODVERSIONS)\n\t\tinfo->index.vers = 0; /* Pretend no __versions section! */\n\telse\n\t\tinfo->index.vers = find_sec(info, \"__versions\");\n\n\tinfo->index.pcpu = find_pcpusec(info);\n\n\treturn 0;\n}\n\nstatic int check_modinfo(struct module *mod, struct load_info *info, int flags)\n{\n\tconst char *modmagic = get_modinfo(info, \"vermagic\");\n\tint err;\n\n\tif (flags & MODULE_INIT_IGNORE_VERMAGIC)\n\t\tmodmagic = NULL;\n\n\t/* This is allowed: modprobe --force will invalidate it. */\n\tif (!modmagic) {\n\t\terr = try_to_force_load(mod, \"bad vermagic\");\n\t\tif (err)\n\t\t\treturn err;\n\t} else if (!same_magic(modmagic, vermagic, info->index.vers)) {\n\t\tpr_err(\"%s: version magic '%s' should be '%s'\\n\",\n\t\t       info->name, modmagic, vermagic);\n\t\treturn -ENOEXEC;\n\t}\n\n\tif (!get_modinfo(info, \"intree\")) {\n\t\tif (!test_taint(TAINT_OOT_MODULE))\n\t\t\tpr_warn(\"%s: loading out-of-tree module taints kernel.\\n\",\n\t\t\t\tmod->name);\n\t\tadd_taint_module(mod, TAINT_OOT_MODULE, LOCKDEP_STILL_OK);\n\t}\n\n\tcheck_modinfo_retpoline(mod, info);\n\n\tif (get_modinfo(info, \"staging\")) {\n\t\tadd_taint_module(mod, TAINT_CRAP, LOCKDEP_STILL_OK);\n\t\tpr_warn(\"%s: module is from the staging directory, the quality \"\n\t\t\t\"is unknown, you have been warned.\\n\", mod->name);\n\t}\n\n\terr = check_modinfo_livepatch(mod, info);\n\tif (err)\n\t\treturn err;\n\n\t/* Set up license info based on the info section */\n\tset_license(mod, get_modinfo(info, \"license\"));\n\n\treturn 0;\n}\n\nstatic int find_module_sections(struct module *mod, struct load_info *info)\n{\n\tmod->kp = section_objs(info, \"__param\",\n\t\t\t       sizeof(*mod->kp), &mod->num_kp);\n\tmod->syms = section_objs(info, \"__ksymtab\",\n\t\t\t\t sizeof(*mod->syms), &mod->num_syms);\n\tmod->crcs = section_addr(info, \"__kcrctab\");\n\tmod->gpl_syms = section_objs(info, \"__ksymtab_gpl\",\n\t\t\t\t     sizeof(*mod->gpl_syms),\n\t\t\t\t     &mod->num_gpl_syms);\n\tmod->gpl_crcs = section_addr(info, \"__kcrctab_gpl\");\n\n#ifdef CONFIG_CONSTRUCTORS\n\tmod->ctors = section_objs(info, \".ctors\",\n\t\t\t\t  sizeof(*mod->ctors), &mod->num_ctors);\n\tif (!mod->ctors)\n\t\tmod->ctors = section_objs(info, \".init_array\",\n\t\t\t\tsizeof(*mod->ctors), &mod->num_ctors);\n\telse if (find_sec(info, \".init_array\")) {\n\t\t/*\n\t\t * This shouldn't happen with same compiler and binutils\n\t\t * building all parts of the module.\n\t\t */\n\t\tpr_warn(\"%s: has both .ctors and .init_array.\\n\",\n\t\t       mod->name);\n\t\treturn -EINVAL;\n\t}\n#endif\n\n\tmod->noinstr_text_start = section_objs(info, \".noinstr.text\", 1,\n\t\t\t\t\t\t&mod->noinstr_text_size);\n\n#ifdef CONFIG_TRACEPOINTS\n\tmod->tracepoints_ptrs = section_objs(info, \"__tracepoints_ptrs\",\n\t\t\t\t\t     sizeof(*mod->tracepoints_ptrs),\n\t\t\t\t\t     &mod->num_tracepoints);\n#endif\n#ifdef CONFIG_TREE_SRCU\n\tmod->srcu_struct_ptrs = section_objs(info, \"___srcu_struct_ptrs\",\n\t\t\t\t\t     sizeof(*mod->srcu_struct_ptrs),\n\t\t\t\t\t     &mod->num_srcu_structs);\n#endif\n#ifdef CONFIG_BPF_EVENTS\n\tmod->bpf_raw_events = section_objs(info, \"__bpf_raw_tp_map\",\n\t\t\t\t\t   sizeof(*mod->bpf_raw_events),\n\t\t\t\t\t   &mod->num_bpf_raw_events);\n#endif\n#ifdef CONFIG_DEBUG_INFO_BTF_MODULES\n\tmod->btf_data = any_section_objs(info, \".BTF\", 1, &mod->btf_data_size);\n#endif\n#ifdef CONFIG_JUMP_LABEL\n\tmod->jump_entries = section_objs(info, \"__jump_table\",\n\t\t\t\t\tsizeof(*mod->jump_entries),\n\t\t\t\t\t&mod->num_jump_entries);\n#endif\n#ifdef CONFIG_EVENT_TRACING\n\tmod->trace_events = section_objs(info, \"_ftrace_events\",\n\t\t\t\t\t sizeof(*mod->trace_events),\n\t\t\t\t\t &mod->num_trace_events);\n\tmod->trace_evals = section_objs(info, \"_ftrace_eval_map\",\n\t\t\t\t\tsizeof(*mod->trace_evals),\n\t\t\t\t\t&mod->num_trace_evals);\n#endif\n#ifdef CONFIG_TRACING\n\tmod->trace_bprintk_fmt_start = section_objs(info, \"__trace_printk_fmt\",\n\t\t\t\t\t sizeof(*mod->trace_bprintk_fmt_start),\n\t\t\t\t\t &mod->num_trace_bprintk_fmt);\n#endif\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n\t/* sechdrs[0].sh_size is always zero */\n\tmod->ftrace_callsites = section_objs(info, FTRACE_CALLSITE_SECTION,\n\t\t\t\t\t     sizeof(*mod->ftrace_callsites),\n\t\t\t\t\t     &mod->num_ftrace_callsites);\n#endif\n#ifdef CONFIG_FUNCTION_ERROR_INJECTION\n\tmod->ei_funcs = section_objs(info, \"_error_injection_whitelist\",\n\t\t\t\t\t    sizeof(*mod->ei_funcs),\n\t\t\t\t\t    &mod->num_ei_funcs);\n#endif\n#ifdef CONFIG_KPROBES\n\tmod->kprobes_text_start = section_objs(info, \".kprobes.text\", 1,\n\t\t\t\t\t\t&mod->kprobes_text_size);\n\tmod->kprobe_blacklist = section_objs(info, \"_kprobe_blacklist\",\n\t\t\t\t\t\tsizeof(unsigned long),\n\t\t\t\t\t\t&mod->num_kprobe_blacklist);\n#endif\n#ifdef CONFIG_HAVE_STATIC_CALL_INLINE\n\tmod->static_call_sites = section_objs(info, \".static_call_sites\",\n\t\t\t\t\t      sizeof(*mod->static_call_sites),\n\t\t\t\t\t      &mod->num_static_call_sites);\n#endif\n\tmod->extable = section_objs(info, \"__ex_table\",\n\t\t\t\t    sizeof(*mod->extable), &mod->num_exentries);\n\n\tif (section_addr(info, \"__obsparm\"))\n\t\tpr_warn(\"%s: Ignoring obsolete parameters\\n\", mod->name);\n\n\tinfo->debug = section_objs(info, \"__dyndbg\",\n\t\t\t\t   sizeof(*info->debug), &info->num_debug);\n\n\treturn 0;\n}\n\nstatic int move_module(struct module *mod, struct load_info *info)\n{\n\tint i;\n\tvoid *ptr;\n\n\t/* Do the allocs. */\n\tptr = module_alloc(mod->core_layout.size);\n\t/*\n\t * The pointer to this block is stored in the module structure\n\t * which is inside the block. Just mark it as not being a\n\t * leak.\n\t */\n\tkmemleak_not_leak(ptr);\n\tif (!ptr)\n\t\treturn -ENOMEM;\n\n\tmemset(ptr, 0, mod->core_layout.size);\n\tmod->core_layout.base = ptr;\n\n\tif (mod->init_layout.size) {\n\t\tptr = module_alloc(mod->init_layout.size);\n\t\t/*\n\t\t * The pointer to this block is stored in the module structure\n\t\t * which is inside the block. This block doesn't need to be\n\t\t * scanned as it contains data and code that will be freed\n\t\t * after the module is initialized.\n\t\t */\n\t\tkmemleak_ignore(ptr);\n\t\tif (!ptr) {\n\t\t\tmodule_memfree(mod->core_layout.base);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tmemset(ptr, 0, mod->init_layout.size);\n\t\tmod->init_layout.base = ptr;\n\t} else\n\t\tmod->init_layout.base = NULL;\n\n\t/* Transfer each section which specifies SHF_ALLOC */\n\tpr_debug(\"final section addresses:\\n\");\n\tfor (i = 0; i < info->hdr->e_shnum; i++) {\n\t\tvoid *dest;\n\t\tElf_Shdr *shdr = &info->sechdrs[i];\n\n\t\tif (!(shdr->sh_flags & SHF_ALLOC))\n\t\t\tcontinue;\n\n\t\tif (shdr->sh_entsize & INIT_OFFSET_MASK)\n\t\t\tdest = mod->init_layout.base\n\t\t\t\t+ (shdr->sh_entsize & ~INIT_OFFSET_MASK);\n\t\telse\n\t\t\tdest = mod->core_layout.base + shdr->sh_entsize;\n\n\t\tif (shdr->sh_type != SHT_NOBITS)\n\t\t\tmemcpy(dest, (void *)shdr->sh_addr, shdr->sh_size);\n\t\t/* Update sh_addr to point to copy in image. */\n\t\tshdr->sh_addr = (unsigned long)dest;\n\t\tpr_debug(\"\\t0x%lx %s\\n\",\n\t\t\t (long)shdr->sh_addr, info->secstrings + shdr->sh_name);\n\t}\n\n\treturn 0;\n}\n\nstatic int check_module_license_and_versions(struct module *mod)\n{\n\tint prev_taint = test_taint(TAINT_PROPRIETARY_MODULE);\n\n\t/*\n\t * ndiswrapper is under GPL by itself, but loads proprietary modules.\n\t * Don't use add_taint_module(), as it would prevent ndiswrapper from\n\t * using GPL-only symbols it needs.\n\t */\n\tif (strcmp(mod->name, \"ndiswrapper\") == 0)\n\t\tadd_taint(TAINT_PROPRIETARY_MODULE, LOCKDEP_NOW_UNRELIABLE);\n\n\t/* driverloader was caught wrongly pretending to be under GPL */\n\tif (strcmp(mod->name, \"driverloader\") == 0)\n\t\tadd_taint_module(mod, TAINT_PROPRIETARY_MODULE,\n\t\t\t\t LOCKDEP_NOW_UNRELIABLE);\n\n\t/* lve claims to be GPL but upstream won't provide source */\n\tif (strcmp(mod->name, \"lve\") == 0)\n\t\tadd_taint_module(mod, TAINT_PROPRIETARY_MODULE,\n\t\t\t\t LOCKDEP_NOW_UNRELIABLE);\n\n\tif (!prev_taint && test_taint(TAINT_PROPRIETARY_MODULE))\n\t\tpr_warn(\"%s: module license taints kernel.\\n\", mod->name);\n\n#ifdef CONFIG_MODVERSIONS\n\tif ((mod->num_syms && !mod->crcs) ||\n\t    (mod->num_gpl_syms && !mod->gpl_crcs)) {\n\t\treturn try_to_force_load(mod,\n\t\t\t\t\t \"no versions for exported symbols\");\n\t}\n#endif\n\treturn 0;\n}\n\nstatic void flush_module_icache(const struct module *mod)\n{\n\t/*\n\t * Flush the instruction cache, since we've played with text.\n\t * Do it before processing of module parameters, so the module\n\t * can provide parameter accessor functions of its own.\n\t */\n\tif (mod->init_layout.base)\n\t\tflush_icache_range((unsigned long)mod->init_layout.base,\n\t\t\t\t   (unsigned long)mod->init_layout.base\n\t\t\t\t   + mod->init_layout.size);\n\tflush_icache_range((unsigned long)mod->core_layout.base,\n\t\t\t   (unsigned long)mod->core_layout.base + mod->core_layout.size);\n}\n\nint __weak module_frob_arch_sections(Elf_Ehdr *hdr,\n\t\t\t\t     Elf_Shdr *sechdrs,\n\t\t\t\t     char *secstrings,\n\t\t\t\t     struct module *mod)\n{\n\treturn 0;\n}\n\n/* module_blacklist is a comma-separated list of module names */\nstatic char *module_blacklist;\nstatic bool blacklisted(const char *module_name)\n{\n\tconst char *p;\n\tsize_t len;\n\n\tif (!module_blacklist)\n\t\treturn false;\n\n\tfor (p = module_blacklist; *p; p += len) {\n\t\tlen = strcspn(p, \",\");\n\t\tif (strlen(module_name) == len && !memcmp(module_name, p, len))\n\t\t\treturn true;\n\t\tif (p[len] == ',')\n\t\t\tlen++;\n\t}\n\treturn false;\n}\ncore_param(module_blacklist, module_blacklist, charp, 0400);\n\nstatic struct module *layout_and_allocate(struct load_info *info, int flags)\n{\n\tstruct module *mod;\n\tunsigned int ndx;\n\tint err;\n\n\terr = check_modinfo(info->mod, info, flags);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* Allow arches to frob section contents and sizes.  */\n\terr = module_frob_arch_sections(info->hdr, info->sechdrs,\n\t\t\t\t\tinfo->secstrings, info->mod);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\terr = module_enforce_rwx_sections(info->hdr, info->sechdrs,\n\t\t\t\t\t  info->secstrings, info->mod);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\t/* We will do a special allocation for per-cpu sections later. */\n\tinfo->sechdrs[info->index.pcpu].sh_flags &= ~(unsigned long)SHF_ALLOC;\n\n\t/*\n\t * Mark ro_after_init section with SHF_RO_AFTER_INIT so that\n\t * layout_sections() can put it in the right place.\n\t * Note: ro_after_init sections also have SHF_{WRITE,ALLOC} set.\n\t */\n\tndx = find_sec(info, \".data..ro_after_init\");\n\tif (ndx)\n\t\tinfo->sechdrs[ndx].sh_flags |= SHF_RO_AFTER_INIT;\n\t/*\n\t * Mark the __jump_table section as ro_after_init as well: these data\n\t * structures are never modified, with the exception of entries that\n\t * refer to code in the __init section, which are annotated as such\n\t * at module load time.\n\t */\n\tndx = find_sec(info, \"__jump_table\");\n\tif (ndx)\n\t\tinfo->sechdrs[ndx].sh_flags |= SHF_RO_AFTER_INIT;\n\n\t/*\n\t * Determine total sizes, and put offsets in sh_entsize.  For now\n\t * this is done generically; there doesn't appear to be any\n\t * special cases for the architectures.\n\t */\n\tlayout_sections(info->mod, info);\n\tlayout_symtab(info->mod, info);\n\n\t/* Allocate and move to the final place */\n\terr = move_module(info->mod, info);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* Module has been copied to its final place now: return it. */\n\tmod = (void *)info->sechdrs[info->index.mod].sh_addr;\n\tkmemleak_load_module(mod, info);\n\treturn mod;\n}\n\n/* mod is no longer valid after this! */\nstatic void module_deallocate(struct module *mod, struct load_info *info)\n{\n\tpercpu_modfree(mod);\n\tmodule_arch_freeing_init(mod);\n\tmodule_memfree(mod->init_layout.base);\n\tmodule_memfree(mod->core_layout.base);\n}\n\nint __weak module_finalize(const Elf_Ehdr *hdr,\n\t\t\t   const Elf_Shdr *sechdrs,\n\t\t\t   struct module *me)\n{\n\treturn 0;\n}\n\nstatic int post_relocation(struct module *mod, const struct load_info *info)\n{\n\t/* Sort exception table now relocations are done. */\n\tsort_extable(mod->extable, mod->extable + mod->num_exentries);\n\n\t/* Copy relocated percpu area over. */\n\tpercpu_modcopy(mod, (void *)info->sechdrs[info->index.pcpu].sh_addr,\n\t\t       info->sechdrs[info->index.pcpu].sh_size);\n\n\t/* Setup kallsyms-specific fields. */\n\tadd_kallsyms(mod, info);\n\n\t/* Arch-specific module finalizing. */\n\treturn module_finalize(info->hdr, info->sechdrs, mod);\n}\n\n/* Is this module of this name done loading?  No locks held. */\nstatic bool finished_loading(const char *name)\n{\n\tstruct module *mod;\n\tbool ret;\n\n\t/*\n\t * The module_mutex should not be a heavily contended lock;\n\t * if we get the occasional sleep here, we'll go an extra iteration\n\t * in the wait_event_interruptible(), which is harmless.\n\t */\n\tsched_annotate_sleep();\n\tmutex_lock(&module_mutex);\n\tmod = find_module_all(name, strlen(name), true);\n\tret = !mod || mod->state == MODULE_STATE_LIVE;\n\tmutex_unlock(&module_mutex);\n\n\treturn ret;\n}\n\n/* Call module constructors. */\nstatic void do_mod_ctors(struct module *mod)\n{\n#ifdef CONFIG_CONSTRUCTORS\n\tunsigned long i;\n\n\tfor (i = 0; i < mod->num_ctors; i++)\n\t\tmod->ctors[i]();\n#endif\n}\n\n/* For freeing module_init on success, in case kallsyms traversing */\nstruct mod_initfree {\n\tstruct llist_node node;\n\tvoid *module_init;\n};\n\nstatic void do_free_init(struct work_struct *w)\n{\n\tstruct llist_node *pos, *n, *list;\n\tstruct mod_initfree *initfree;\n\n\tlist = llist_del_all(&init_free_list);\n\n\tsynchronize_rcu();\n\n\tllist_for_each_safe(pos, n, list) {\n\t\tinitfree = container_of(pos, struct mod_initfree, node);\n\t\tmodule_memfree(initfree->module_init);\n\t\tkfree(initfree);\n\t}\n}\n\n/*\n * This is where the real work happens.\n *\n * Keep it uninlined to provide a reliable breakpoint target, e.g. for the gdb\n * helper command 'lx-symbols'.\n */\nstatic noinline int do_init_module(struct module *mod)\n{\n\tint ret = 0;\n\tstruct mod_initfree *freeinit;\n\n\tfreeinit = kmalloc(sizeof(*freeinit), GFP_KERNEL);\n\tif (!freeinit) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tfreeinit->module_init = mod->init_layout.base;\n\n\t/*\n\t * We want to find out whether @mod uses async during init.  Clear\n\t * PF_USED_ASYNC.  async_schedule*() will set it.\n\t */\n\tcurrent->flags &= ~PF_USED_ASYNC;\n\n\tdo_mod_ctors(mod);\n\t/* Start the module */\n\tif (mod->init != NULL)\n\t\tret = do_one_initcall(mod->init);\n\tif (ret < 0) {\n\t\tgoto fail_free_freeinit;\n\t}\n\tif (ret > 0) {\n\t\tpr_warn(\"%s: '%s'->init suspiciously returned %d, it should \"\n\t\t\t\"follow 0/-E convention\\n\"\n\t\t\t\"%s: loading module anyway...\\n\",\n\t\t\t__func__, mod->name, ret, __func__);\n\t\tdump_stack();\n\t}\n\n\t/* Now it's a first class citizen! */\n\tmod->state = MODULE_STATE_LIVE;\n\tblocking_notifier_call_chain(&module_notify_list,\n\t\t\t\t     MODULE_STATE_LIVE, mod);\n\n\t/* Delay uevent until module has finished its init routine */\n\tkobject_uevent(&mod->mkobj.kobj, KOBJ_ADD);\n\n\t/*\n\t * We need to finish all async code before the module init sequence\n\t * is done.  This has potential to deadlock.  For example, a newly\n\t * detected block device can trigger request_module() of the\n\t * default iosched from async probing task.  Once userland helper\n\t * reaches here, async_synchronize_full() will wait on the async\n\t * task waiting on request_module() and deadlock.\n\t *\n\t * This deadlock is avoided by perfomring async_synchronize_full()\n\t * iff module init queued any async jobs.  This isn't a full\n\t * solution as it will deadlock the same if module loading from\n\t * async jobs nests more than once; however, due to the various\n\t * constraints, this hack seems to be the best option for now.\n\t * Please refer to the following thread for details.\n\t *\n\t * http://thread.gmane.org/gmane.linux.kernel/1420814\n\t */\n\tif (!mod->async_probe_requested && (current->flags & PF_USED_ASYNC))\n\t\tasync_synchronize_full();\n\n\tftrace_free_mem(mod, mod->init_layout.base, mod->init_layout.base +\n\t\t\tmod->init_layout.size);\n\tmutex_lock(&module_mutex);\n\t/* Drop initial reference. */\n\tmodule_put(mod);\n\ttrim_init_extable(mod);\n#ifdef CONFIG_KALLSYMS\n\t/* Switch to core kallsyms now init is done: kallsyms may be walking! */\n\trcu_assign_pointer(mod->kallsyms, &mod->core_kallsyms);\n#endif\n\tmodule_enable_ro(mod, true);\n\tmod_tree_remove_init(mod);\n\tmodule_arch_freeing_init(mod);\n\tmod->init_layout.base = NULL;\n\tmod->init_layout.size = 0;\n\tmod->init_layout.ro_size = 0;\n\tmod->init_layout.ro_after_init_size = 0;\n\tmod->init_layout.text_size = 0;\n#ifdef CONFIG_DEBUG_INFO_BTF_MODULES\n\t/* .BTF is not SHF_ALLOC and will get removed, so sanitize pointer */\n\tmod->btf_data = NULL;\n#endif\n\t/*\n\t * We want to free module_init, but be aware that kallsyms may be\n\t * walking this with preempt disabled.  In all the failure paths, we\n\t * call synchronize_rcu(), but we don't want to slow down the success\n\t * path. module_memfree() cannot be called in an interrupt, so do the\n\t * work and call synchronize_rcu() in a work queue.\n\t *\n\t * Note that module_alloc() on most architectures creates W+X page\n\t * mappings which won't be cleaned up until do_free_init() runs.  Any\n\t * code such as mark_rodata_ro() which depends on those mappings to\n\t * be cleaned up needs to sync with the queued work - ie\n\t * rcu_barrier()\n\t */\n\tif (llist_add(&freeinit->node, &init_free_list))\n\t\tschedule_work(&init_free_wq);\n\n\tmutex_unlock(&module_mutex);\n\twake_up_all(&module_wq);\n\n\treturn 0;\n\nfail_free_freeinit:\n\tkfree(freeinit);\nfail:\n\t/* Try to protect us from buggy refcounters. */\n\tmod->state = MODULE_STATE_GOING;\n\tsynchronize_rcu();\n\tmodule_put(mod);\n\tblocking_notifier_call_chain(&module_notify_list,\n\t\t\t\t     MODULE_STATE_GOING, mod);\n\tklp_module_going(mod);\n\tftrace_release_mod(mod);\n\tfree_module(mod);\n\twake_up_all(&module_wq);\n\treturn ret;\n}\n\nstatic int may_init_module(void)\n{\n\tif (!capable(CAP_SYS_MODULE) || modules_disabled)\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\n/*\n * We try to place it in the list now to make sure it's unique before\n * we dedicate too many resources.  In particular, temporary percpu\n * memory exhaustion.\n */\nstatic int add_unformed_module(struct module *mod)\n{\n\tint err;\n\tstruct module *old;\n\n\tmod->state = MODULE_STATE_UNFORMED;\n\nagain:\n\tmutex_lock(&module_mutex);\n\told = find_module_all(mod->name, strlen(mod->name), true);\n\tif (old != NULL) {\n\t\tif (old->state != MODULE_STATE_LIVE) {\n\t\t\t/* Wait in case it fails to load. */\n\t\t\tmutex_unlock(&module_mutex);\n\t\t\terr = wait_event_interruptible(module_wq,\n\t\t\t\t\t       finished_loading(mod->name));\n\t\t\tif (err)\n\t\t\t\tgoto out_unlocked;\n\t\t\tgoto again;\n\t\t}\n\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\tmod_update_bounds(mod);\n\tlist_add_rcu(&mod->list, &modules);\n\tmod_tree_insert(mod);\n\terr = 0;\n\nout:\n\tmutex_unlock(&module_mutex);\nout_unlocked:\n\treturn err;\n}\n\nstatic int complete_formation(struct module *mod, struct load_info *info)\n{\n\tint err;\n\n\tmutex_lock(&module_mutex);\n\n\t/* Find duplicate symbols (must be called under lock). */\n\terr = verify_exported_symbols(mod);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* This relies on module_mutex for list integrity. */\n\tmodule_bug_finalize(info->hdr, info->sechdrs, mod);\n\n\tmodule_enable_ro(mod, false);\n\tmodule_enable_nx(mod);\n\tmodule_enable_x(mod);\n\n\t/*\n\t * Mark state as coming so strong_try_module_get() ignores us,\n\t * but kallsyms etc. can see us.\n\t */\n\tmod->state = MODULE_STATE_COMING;\n\tmutex_unlock(&module_mutex);\n\n\treturn 0;\n\nout:\n\tmutex_unlock(&module_mutex);\n\treturn err;\n}\n\nstatic int prepare_coming_module(struct module *mod)\n{\n\tint err;\n\n\tftrace_module_enable(mod);\n\terr = klp_module_coming(mod);\n\tif (err)\n\t\treturn err;\n\n\terr = blocking_notifier_call_chain_robust(&module_notify_list,\n\t\t\tMODULE_STATE_COMING, MODULE_STATE_GOING, mod);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\tklp_module_going(mod);\n\n\treturn err;\n}\n\nstatic int unknown_module_param_cb(char *param, char *val, const char *modname,\n\t\t\t\t   void *arg)\n{\n\tstruct module *mod = arg;\n\tint ret;\n\n\tif (strcmp(param, \"async_probe\") == 0) {\n\t\tmod->async_probe_requested = true;\n\t\treturn 0;\n\t}\n\n\t/* Check for magic 'dyndbg' arg */\n\tret = ddebug_dyndbg_module_param_cb(param, val, modname);\n\tif (ret != 0)\n\t\tpr_warn(\"%s: unknown parameter '%s' ignored\\n\", modname, param);\n\treturn 0;\n}\n\nstatic void cfi_init(struct module *mod);\n\n/*\n * Allocate and load the module: note that size of section 0 is always\n * zero, and we rely on this for optional sections.\n */\nstatic int load_module(struct load_info *info, const char __user *uargs,\n\t\t       int flags)\n{\n\tstruct module *mod;\n\tlong err = 0;\n\tchar *after_dashes;\n\n\t/*\n\t * Do the signature check (if any) first. All that\n\t * the signature check needs is info->len, it does\n\t * not need any of the section info. That can be\n\t * set up later. This will minimize the chances\n\t * of a corrupt module causing problems before\n\t * we even get to the signature check.\n\t *\n\t * The check will also adjust info->len by stripping\n\t * off the sig length at the end of the module, making\n\t * checks against info->len more correct.\n\t */\n\terr = module_sig_check(info, flags);\n\tif (err)\n\t\tgoto free_copy;\n\n\t/*\n\t * Do basic sanity checks against the ELF header and\n\t * sections.\n\t */\n\terr = elf_validity_check(info);\n\tif (err) {\n\t\tpr_err(\"Module has invalid ELF structures\\n\");\n\t\tgoto free_copy;\n\t}\n\n\t/*\n\t * Everything checks out, so set up the section info\n\t * in the info structure.\n\t */\n\terr = setup_load_info(info, flags);\n\tif (err)\n\t\tgoto free_copy;\n\n\t/*\n\t * Now that we know we have the correct module name, check\n\t * if it's blacklisted.\n\t */\n\tif (blacklisted(info->name)) {\n\t\terr = -EPERM;\n\t\tpr_err(\"Module %s is blacklisted\\n\", info->name);\n\t\tgoto free_copy;\n\t}\n\n\terr = rewrite_section_headers(info, flags);\n\tif (err)\n\t\tgoto free_copy;\n\n\t/* Check module struct version now, before we try to use module. */\n\tif (!check_modstruct_version(info, info->mod)) {\n\t\terr = -ENOEXEC;\n\t\tgoto free_copy;\n\t}\n\n\t/* Figure out module layout, and allocate all the memory. */\n\tmod = layout_and_allocate(info, flags);\n\tif (IS_ERR(mod)) {\n\t\terr = PTR_ERR(mod);\n\t\tgoto free_copy;\n\t}\n\n\taudit_log_kern_module(mod->name);\n\n\t/* Reserve our place in the list. */\n\terr = add_unformed_module(mod);\n\tif (err)\n\t\tgoto free_module;\n\n#ifdef CONFIG_MODULE_SIG\n\tmod->sig_ok = info->sig_ok;\n\tif (!mod->sig_ok) {\n\t\tpr_notice_once(\"%s: module verification failed: signature \"\n\t\t\t       \"and/or required key missing - tainting \"\n\t\t\t       \"kernel\\n\", mod->name);\n\t\tadd_taint_module(mod, TAINT_UNSIGNED_MODULE, LOCKDEP_STILL_OK);\n\t}\n#endif\n\n\t/* To avoid stressing percpu allocator, do this once we're unique. */\n\terr = percpu_modalloc(mod, info);\n\tif (err)\n\t\tgoto unlink_mod;\n\n\t/* Now module is in final location, initialize linked lists, etc. */\n\terr = module_unload_init(mod);\n\tif (err)\n\t\tgoto unlink_mod;\n\n\tinit_param_lock(mod);\n\n\t/*\n\t * Now we've got everything in the final locations, we can\n\t * find optional sections.\n\t */\n\terr = find_module_sections(mod, info);\n\tif (err)\n\t\tgoto free_unload;\n\n\terr = check_module_license_and_versions(mod);\n\tif (err)\n\t\tgoto free_unload;\n\n\t/* Set up MODINFO_ATTR fields */\n\tsetup_modinfo(mod, info);\n\n\t/* Fix up syms, so that st_value is a pointer to location. */\n\terr = simplify_symbols(mod, info);\n\tif (err < 0)\n\t\tgoto free_modinfo;\n\n\terr = apply_relocations(mod, info);\n\tif (err < 0)\n\t\tgoto free_modinfo;\n\n\terr = post_relocation(mod, info);\n\tif (err < 0)\n\t\tgoto free_modinfo;\n\n\tflush_module_icache(mod);\n\n\t/* Setup CFI for the module. */\n\tcfi_init(mod);\n\n\t/* Now copy in args */\n\tmod->args = strndup_user(uargs, ~0UL >> 1);\n\tif (IS_ERR(mod->args)) {\n\t\terr = PTR_ERR(mod->args);\n\t\tgoto free_arch_cleanup;\n\t}\n\n\tdynamic_debug_setup(mod, info->debug, info->num_debug);\n\n\t/* Ftrace init must be called in the MODULE_STATE_UNFORMED state */\n\tftrace_module_init(mod);\n\n\t/* Finally it's fully formed, ready to start executing. */\n\terr = complete_formation(mod, info);\n\tif (err)\n\t\tgoto ddebug_cleanup;\n\n\terr = prepare_coming_module(mod);\n\tif (err)\n\t\tgoto bug_cleanup;\n\n\t/* Module is ready to execute: parsing args may do that. */\n\tafter_dashes = parse_args(mod->name, mod->args, mod->kp, mod->num_kp,\n\t\t\t\t  -32768, 32767, mod,\n\t\t\t\t  unknown_module_param_cb);\n\tif (IS_ERR(after_dashes)) {\n\t\terr = PTR_ERR(after_dashes);\n\t\tgoto coming_cleanup;\n\t} else if (after_dashes) {\n\t\tpr_warn(\"%s: parameters '%s' after `--' ignored\\n\",\n\t\t       mod->name, after_dashes);\n\t}\n\n\t/* Link in to sysfs. */\n\terr = mod_sysfs_setup(mod, info, mod->kp, mod->num_kp);\n\tif (err < 0)\n\t\tgoto coming_cleanup;\n\n\tif (is_livepatch_module(mod)) {\n\t\terr = copy_module_elf(mod, info);\n\t\tif (err < 0)\n\t\t\tgoto sysfs_cleanup;\n\t}\n\n\t/* Get rid of temporary copy. */\n\tfree_copy(info);\n\n\t/* Done! */\n\ttrace_module_load(mod);\n\n\treturn do_init_module(mod);\n\n sysfs_cleanup:\n\tmod_sysfs_teardown(mod);\n coming_cleanup:\n\tmod->state = MODULE_STATE_GOING;\n\tdestroy_params(mod->kp, mod->num_kp);\n\tblocking_notifier_call_chain(&module_notify_list,\n\t\t\t\t     MODULE_STATE_GOING, mod);\n\tklp_module_going(mod);\n bug_cleanup:\n\tmod->state = MODULE_STATE_GOING;\n\t/* module_bug_cleanup needs module_mutex protection */\n\tmutex_lock(&module_mutex);\n\tmodule_bug_cleanup(mod);\n\tmutex_unlock(&module_mutex);\n\n ddebug_cleanup:\n\tftrace_release_mod(mod);\n\tdynamic_debug_remove(mod, info->debug);\n\tsynchronize_rcu();\n\tkfree(mod->args);\n free_arch_cleanup:\n\tcfi_cleanup(mod);\n\tmodule_arch_cleanup(mod);\n free_modinfo:\n\tfree_modinfo(mod);\n free_unload:\n\tmodule_unload_free(mod);\n unlink_mod:\n\tmutex_lock(&module_mutex);\n\t/* Unlink carefully: kallsyms could be walking list. */\n\tlist_del_rcu(&mod->list);\n\tmod_tree_remove(mod);\n\twake_up_all(&module_wq);\n\t/* Wait for RCU-sched synchronizing before releasing mod->list. */\n\tsynchronize_rcu();\n\tmutex_unlock(&module_mutex);\n free_module:\n\t/* Free lock-classes; relies on the preceding sync_rcu() */\n\tlockdep_free_key_range(mod->core_layout.base, mod->core_layout.size);\n\n\tmodule_deallocate(mod, info);\n free_copy:\n\tfree_copy(info);\n\treturn err;\n}\n\nSYSCALL_DEFINE3(init_module, void __user *, umod,\n\t\tunsigned long, len, const char __user *, uargs)\n{\n\tint err;\n\tstruct load_info info = { };\n\n\terr = may_init_module();\n\tif (err)\n\t\treturn err;\n\n\tpr_debug(\"init_module: umod=%p, len=%lu, uargs=%p\\n\",\n\t       umod, len, uargs);\n\n\terr = copy_module_from_user(umod, len, &info);\n\tif (err)\n\t\treturn err;\n\n\treturn load_module(&info, uargs, 0);\n}\n\nSYSCALL_DEFINE3(finit_module, int, fd, const char __user *, uargs, int, flags)\n{\n\tstruct load_info info = { };\n\tvoid *hdr = NULL;\n\tint err;\n\n\terr = may_init_module();\n\tif (err)\n\t\treturn err;\n\n\tpr_debug(\"finit_module: fd=%d, uargs=%p, flags=%i\\n\", fd, uargs, flags);\n\n\tif (flags & ~(MODULE_INIT_IGNORE_MODVERSIONS\n\t\t      |MODULE_INIT_IGNORE_VERMAGIC))\n\t\treturn -EINVAL;\n\n\terr = kernel_read_file_from_fd(fd, 0, &hdr, INT_MAX, NULL,\n\t\t\t\t       READING_MODULE);\n\tif (err < 0)\n\t\treturn err;\n\tinfo.hdr = hdr;\n\tinfo.len = err;\n\n\treturn load_module(&info, uargs, flags);\n}\n\nstatic inline int within(unsigned long addr, void *start, unsigned long size)\n{\n\treturn ((void *)addr >= start && (void *)addr < start + size);\n}\n\n#ifdef CONFIG_KALLSYMS\n/*\n * This ignores the intensely annoying \"mapping symbols\" found\n * in ARM ELF files: $a, $t and $d.\n */\nstatic inline int is_arm_mapping_symbol(const char *str)\n{\n\tif (str[0] == '.' && str[1] == 'L')\n\t\treturn true;\n\treturn str[0] == '$' && strchr(\"axtd\", str[1])\n\t       && (str[2] == '\\0' || str[2] == '.');\n}\n\nstatic const char *kallsyms_symbol_name(struct mod_kallsyms *kallsyms, unsigned int symnum)\n{\n\treturn kallsyms->strtab + kallsyms->symtab[symnum].st_name;\n}\n\n/*\n * Given a module and address, find the corresponding symbol and return its name\n * while providing its size and offset if needed.\n */\nstatic const char *find_kallsyms_symbol(struct module *mod,\n\t\t\t\t\tunsigned long addr,\n\t\t\t\t\tunsigned long *size,\n\t\t\t\t\tunsigned long *offset)\n{\n\tunsigned int i, best = 0;\n\tunsigned long nextval, bestval;\n\tstruct mod_kallsyms *kallsyms = rcu_dereference_sched(mod->kallsyms);\n\n\t/* At worse, next value is at end of module */\n\tif (within_module_init(addr, mod))\n\t\tnextval = (unsigned long)mod->init_layout.base+mod->init_layout.text_size;\n\telse\n\t\tnextval = (unsigned long)mod->core_layout.base+mod->core_layout.text_size;\n\n\tbestval = kallsyms_symbol_value(&kallsyms->symtab[best]);\n\n\t/*\n\t * Scan for closest preceding symbol, and next symbol. (ELF\n\t * starts real symbols at 1).\n\t */\n\tfor (i = 1; i < kallsyms->num_symtab; i++) {\n\t\tconst Elf_Sym *sym = &kallsyms->symtab[i];\n\t\tunsigned long thisval = kallsyms_symbol_value(sym);\n\n\t\tif (sym->st_shndx == SHN_UNDEF)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We ignore unnamed symbols: they're uninformative\n\t\t * and inserted at a whim.\n\t\t */\n\t\tif (*kallsyms_symbol_name(kallsyms, i) == '\\0'\n\t\t    || is_arm_mapping_symbol(kallsyms_symbol_name(kallsyms, i)))\n\t\t\tcontinue;\n\n\t\tif (thisval <= addr && thisval > bestval) {\n\t\t\tbest = i;\n\t\t\tbestval = thisval;\n\t\t}\n\t\tif (thisval > addr && thisval < nextval)\n\t\t\tnextval = thisval;\n\t}\n\n\tif (!best)\n\t\treturn NULL;\n\n\tif (size)\n\t\t*size = nextval - bestval;\n\tif (offset)\n\t\t*offset = addr - bestval;\n\n\treturn kallsyms_symbol_name(kallsyms, best);\n}\n\nvoid * __weak dereference_module_function_descriptor(struct module *mod,\n\t\t\t\t\t\t     void *ptr)\n{\n\treturn ptr;\n}\n\n/*\n * For kallsyms to ask for address resolution.  NULL means not found.  Careful\n * not to lock to avoid deadlock on oopses, simply disable preemption.\n */\nconst char *module_address_lookup(unsigned long addr,\n\t\t\t    unsigned long *size,\n\t\t\t    unsigned long *offset,\n\t\t\t    char **modname,\n\t\t\t    char *namebuf)\n{\n\tconst char *ret = NULL;\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tmod = __module_address(addr);\n\tif (mod) {\n\t\tif (modname)\n\t\t\t*modname = mod->name;\n\n\t\tret = find_kallsyms_symbol(mod, addr, size, offset);\n\t}\n\t/* Make a copy in here where it's safe */\n\tif (ret) {\n\t\tstrncpy(namebuf, ret, KSYM_NAME_LEN - 1);\n\t\tret = namebuf;\n\t}\n\tpreempt_enable();\n\n\treturn ret;\n}\n\nint lookup_module_symbol_name(unsigned long addr, char *symname)\n{\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tif (within_module(addr, mod)) {\n\t\t\tconst char *sym;\n\n\t\t\tsym = find_kallsyms_symbol(mod, addr, NULL, NULL);\n\t\t\tif (!sym)\n\t\t\t\tgoto out;\n\n\t\t\tstrlcpy(symname, sym, KSYM_NAME_LEN);\n\t\t\tpreempt_enable();\n\t\t\treturn 0;\n\t\t}\n\t}\nout:\n\tpreempt_enable();\n\treturn -ERANGE;\n}\n\nint lookup_module_symbol_attrs(unsigned long addr, unsigned long *size,\n\t\t\tunsigned long *offset, char *modname, char *name)\n{\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tif (within_module(addr, mod)) {\n\t\t\tconst char *sym;\n\n\t\t\tsym = find_kallsyms_symbol(mod, addr, size, offset);\n\t\t\tif (!sym)\n\t\t\t\tgoto out;\n\t\t\tif (modname)\n\t\t\t\tstrlcpy(modname, mod->name, MODULE_NAME_LEN);\n\t\t\tif (name)\n\t\t\t\tstrlcpy(name, sym, KSYM_NAME_LEN);\n\t\t\tpreempt_enable();\n\t\t\treturn 0;\n\t\t}\n\t}\nout:\n\tpreempt_enable();\n\treturn -ERANGE;\n}\n\nint module_get_kallsym(unsigned int symnum, unsigned long *value, char *type,\n\t\t\tchar *name, char *module_name, int *exported)\n{\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tstruct mod_kallsyms *kallsyms;\n\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tkallsyms = rcu_dereference_sched(mod->kallsyms);\n\t\tif (symnum < kallsyms->num_symtab) {\n\t\t\tconst Elf_Sym *sym = &kallsyms->symtab[symnum];\n\n\t\t\t*value = kallsyms_symbol_value(sym);\n\t\t\t*type = kallsyms->typetab[symnum];\n\t\t\tstrlcpy(name, kallsyms_symbol_name(kallsyms, symnum), KSYM_NAME_LEN);\n\t\t\tstrlcpy(module_name, mod->name, MODULE_NAME_LEN);\n\t\t\t*exported = is_exported(name, *value, mod);\n\t\t\tpreempt_enable();\n\t\t\treturn 0;\n\t\t}\n\t\tsymnum -= kallsyms->num_symtab;\n\t}\n\tpreempt_enable();\n\treturn -ERANGE;\n}\n\n/* Given a module and name of symbol, find and return the symbol's value */\nstatic unsigned long find_kallsyms_symbol_value(struct module *mod, const char *name)\n{\n\tunsigned int i;\n\tstruct mod_kallsyms *kallsyms = rcu_dereference_sched(mod->kallsyms);\n\n\tfor (i = 0; i < kallsyms->num_symtab; i++) {\n\t\tconst Elf_Sym *sym = &kallsyms->symtab[i];\n\n\t\tif (strcmp(name, kallsyms_symbol_name(kallsyms, i)) == 0 &&\n\t\t    sym->st_shndx != SHN_UNDEF)\n\t\t\treturn kallsyms_symbol_value(sym);\n\t}\n\treturn 0;\n}\n\n/* Look for this name: can be of form module:name. */\nunsigned long module_kallsyms_lookup_name(const char *name)\n{\n\tstruct module *mod;\n\tchar *colon;\n\tunsigned long ret = 0;\n\n\t/* Don't lock: we're in enough trouble already. */\n\tpreempt_disable();\n\tif ((colon = strnchr(name, MODULE_NAME_LEN, ':')) != NULL) {\n\t\tif ((mod = find_module_all(name, colon - name, false)) != NULL)\n\t\t\tret = find_kallsyms_symbol_value(mod, colon+1);\n\t} else {\n\t\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\t\tcontinue;\n\t\t\tif ((ret = find_kallsyms_symbol_value(mod, name)) != 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tpreempt_enable();\n\treturn ret;\n}\n\n#ifdef CONFIG_LIVEPATCH\nint module_kallsyms_on_each_symbol(int (*fn)(void *, const char *,\n\t\t\t\t\t     struct module *, unsigned long),\n\t\t\t\t   void *data)\n{\n\tstruct module *mod;\n\tunsigned int i;\n\tint ret = 0;\n\n\tmutex_lock(&module_mutex);\n\tlist_for_each_entry(mod, &modules, list) {\n\t\t/* We hold module_mutex: no need for rcu_dereference_sched */\n\t\tstruct mod_kallsyms *kallsyms = mod->kallsyms;\n\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tfor (i = 0; i < kallsyms->num_symtab; i++) {\n\t\t\tconst Elf_Sym *sym = &kallsyms->symtab[i];\n\n\t\t\tif (sym->st_shndx == SHN_UNDEF)\n\t\t\t\tcontinue;\n\n\t\t\tret = fn(data, kallsyms_symbol_name(kallsyms, i),\n\t\t\t\t mod, kallsyms_symbol_value(sym));\n\t\t\tif (ret != 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&module_mutex);\n\treturn ret;\n}\n#endif /* CONFIG_LIVEPATCH */\n#endif /* CONFIG_KALLSYMS */\n\nstatic void cfi_init(struct module *mod)\n{\n#ifdef CONFIG_CFI_CLANG\n\tinitcall_t *init;\n\texitcall_t *exit;\n\n\trcu_read_lock_sched();\n\tmod->cfi_check = (cfi_check_fn)\n\t\tfind_kallsyms_symbol_value(mod, \"__cfi_check\");\n\tinit = (initcall_t *)\n\t\tfind_kallsyms_symbol_value(mod, \"__cfi_jt_init_module\");\n\texit = (exitcall_t *)\n\t\tfind_kallsyms_symbol_value(mod, \"__cfi_jt_cleanup_module\");\n\trcu_read_unlock_sched();\n\n\t/* Fix init/exit functions to point to the CFI jump table */\n\tif (init)\n\t\tmod->init = *init;\n\tif (exit)\n\t\tmod->exit = *exit;\n\n\tcfi_module_add(mod, module_addr_min);\n#endif\n}\n\nstatic void cfi_cleanup(struct module *mod)\n{\n#ifdef CONFIG_CFI_CLANG\n\tcfi_module_remove(mod, module_addr_min);\n#endif\n}\n\n/* Maximum number of characters written by module_flags() */\n#define MODULE_FLAGS_BUF_SIZE (TAINT_FLAGS_COUNT + 4)\n\n/* Keep in sync with MODULE_FLAGS_BUF_SIZE !!! */\nstatic char *module_flags(struct module *mod, char *buf)\n{\n\tint bx = 0;\n\n\tBUG_ON(mod->state == MODULE_STATE_UNFORMED);\n\tif (mod->taints ||\n\t    mod->state == MODULE_STATE_GOING ||\n\t    mod->state == MODULE_STATE_COMING) {\n\t\tbuf[bx++] = '(';\n\t\tbx += module_flags_taint(mod, buf + bx);\n\t\t/* Show a - for module-is-being-unloaded */\n\t\tif (mod->state == MODULE_STATE_GOING)\n\t\t\tbuf[bx++] = '-';\n\t\t/* Show a + for module-is-being-loaded */\n\t\tif (mod->state == MODULE_STATE_COMING)\n\t\t\tbuf[bx++] = '+';\n\t\tbuf[bx++] = ')';\n\t}\n\tbuf[bx] = '\\0';\n\n\treturn buf;\n}\n\n#ifdef CONFIG_PROC_FS\n/* Called by the /proc file system to return a list of modules. */\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tmutex_lock(&module_mutex);\n\treturn seq_list_start(&modules, *pos);\n}\n\nstatic void *m_next(struct seq_file *m, void *p, loff_t *pos)\n{\n\treturn seq_list_next(p, &modules, pos);\n}\n\nstatic void m_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&module_mutex);\n}\n\nstatic int m_show(struct seq_file *m, void *p)\n{\n\tstruct module *mod = list_entry(p, struct module, list);\n\tchar buf[MODULE_FLAGS_BUF_SIZE];\n\tvoid *value;\n\n\t/* We always ignore unformed modules. */\n\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\treturn 0;\n\n\tseq_printf(m, \"%s %u\",\n\t\t   mod->name, mod->init_layout.size + mod->core_layout.size);\n\tprint_unload_info(m, mod);\n\n\t/* Informative for users. */\n\tseq_printf(m, \" %s\",\n\t\t   mod->state == MODULE_STATE_GOING ? \"Unloading\" :\n\t\t   mod->state == MODULE_STATE_COMING ? \"Loading\" :\n\t\t   \"Live\");\n\t/* Used by oprofile and other similar tools. */\n\tvalue = m->private ? NULL : mod->core_layout.base;\n\tseq_printf(m, \" 0x%px\", value);\n\n\t/* Taints info */\n\tif (mod->taints)\n\t\tseq_printf(m, \" %s\", module_flags(mod, buf));\n\n\tseq_puts(m, \"\\n\");\n\treturn 0;\n}\n\n/*\n * Format: modulename size refcount deps address\n *\n * Where refcount is a number or -, and deps is a comma-separated list\n * of depends or -.\n */\nstatic const struct seq_operations modules_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= m_show\n};\n\n/*\n * This also sets the \"private\" pointer to non-NULL if the\n * kernel pointers should be hidden (so you can just test\n * \"m->private\" to see if you should keep the values private).\n *\n * We use the same logic as for /proc/kallsyms.\n */\nstatic int modules_open(struct inode *inode, struct file *file)\n{\n\tint err = seq_open(file, &modules_op);\n\n\tif (!err) {\n\t\tstruct seq_file *m = file->private_data;\n\t\tm->private = kallsyms_show_value(file->f_cred) ? NULL : (void *)8ul;\n\t}\n\n\treturn err;\n}\n\nstatic const struct proc_ops modules_proc_ops = {\n\t.proc_flags\t= PROC_ENTRY_PERMANENT,\n\t.proc_open\t= modules_open,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_release\t= seq_release,\n};\n\nstatic int __init proc_modules_init(void)\n{\n\tproc_create(\"modules\", 0, NULL, &modules_proc_ops);\n\treturn 0;\n}\nmodule_init(proc_modules_init);\n#endif\n\n/* Given an address, look for it in the module exception tables. */\nconst struct exception_table_entry *search_module_extables(unsigned long addr)\n{\n\tconst struct exception_table_entry *e = NULL;\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tmod = __module_address(addr);\n\tif (!mod)\n\t\tgoto out;\n\n\tif (!mod->num_exentries)\n\t\tgoto out;\n\n\te = search_extable(mod->extable,\n\t\t\t   mod->num_exentries,\n\t\t\t   addr);\nout:\n\tpreempt_enable();\n\n\t/*\n\t * Now, if we found one, we are running inside it now, hence\n\t * we cannot unload the module, hence no refcnt needed.\n\t */\n\treturn e;\n}\n\n/**\n * is_module_address() - is this address inside a module?\n * @addr: the address to check.\n *\n * See is_module_text_address() if you simply want to see if the address\n * is code (not data).\n */\nbool is_module_address(unsigned long addr)\n{\n\tbool ret;\n\n\tpreempt_disable();\n\tret = __module_address(addr) != NULL;\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * __module_address() - get the module which contains an address.\n * @addr: the address.\n *\n * Must be called with preempt disabled or module mutex held so that\n * module doesn't get freed during this.\n */\nstruct module *__module_address(unsigned long addr)\n{\n\tstruct module *mod;\n\n\tif (addr < module_addr_min || addr > module_addr_max)\n\t\treturn NULL;\n\n\tmodule_assert_mutex_or_preempt();\n\n\tmod = mod_find(addr);\n\tif (mod) {\n\t\tBUG_ON(!within_module(addr, mod));\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tmod = NULL;\n\t}\n\treturn mod;\n}\n\n/**\n * is_module_text_address() - is this address inside module code?\n * @addr: the address to check.\n *\n * See is_module_address() if you simply want to see if the address is\n * anywhere in a module.  See kernel_text_address() for testing if an\n * address corresponds to kernel or module code.\n */\nbool is_module_text_address(unsigned long addr)\n{\n\tbool ret;\n\n\tpreempt_disable();\n\tret = __module_text_address(addr) != NULL;\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * __module_text_address() - get the module whose code contains an address.\n * @addr: the address.\n *\n * Must be called with preempt disabled or module mutex held so that\n * module doesn't get freed during this.\n */\nstruct module *__module_text_address(unsigned long addr)\n{\n\tstruct module *mod = __module_address(addr);\n\tif (mod) {\n\t\t/* Make sure it's within the text section. */\n\t\tif (!within(addr, mod->init_layout.base, mod->init_layout.text_size)\n\t\t    && !within(addr, mod->core_layout.base, mod->core_layout.text_size))\n\t\t\tmod = NULL;\n\t}\n\treturn mod;\n}\n\n/* Don't grab lock, we're oopsing. */\nvoid print_modules(void)\n{\n\tstruct module *mod;\n\tchar buf[MODULE_FLAGS_BUF_SIZE];\n\n\tprintk(KERN_DEFAULT \"Modules linked in:\");\n\t/* Most callers should already have preempt disabled, but make sure */\n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod, &modules, list) {\n\t\tif (mod->state == MODULE_STATE_UNFORMED)\n\t\t\tcontinue;\n\t\tpr_cont(\" %s%s\", mod->name, module_flags(mod, buf));\n\t}\n\tpreempt_enable();\n\tif (last_unloaded_module[0])\n\t\tpr_cont(\" [last unloaded: %s]\", last_unloaded_module);\n\tpr_cont(\"\\n\");\n}\n\n#ifdef CONFIG_MODVERSIONS\n/*\n * Generate the signature for all relevant module structures here.\n * If these change, we don't want to try to parse the module.\n */\nvoid module_layout(struct module *mod,\n\t\t   struct modversion_info *ver,\n\t\t   struct kernel_param *kp,\n\t\t   struct kernel_symbol *ks,\n\t\t   struct tracepoint * const *tp)\n{\n}\nEXPORT_SYMBOL(module_layout);\n#endif\n"], "filenames": ["kernel/module.c"], "buggy_code_start_loc": [268], "buggy_code_end_loc": [286], "fixing_code_start_loc": [269], "fixing_code_end_loc": [289], "type": "CWE-347", "message": "kernel/module.c in the Linux kernel before 5.12.14 mishandles Signature Verification, aka CID-0c18f29aae7c. Without CONFIG_MODULE_SIG, verification that a kernel module is signed, for loading via init_module, does not occur for a module.sig_enforce=1 command-line argument.", "other": {"cve": {"id": "CVE-2021-35039", "sourceIdentifier": "cve@mitre.org", "published": "2021-07-07T01:15:07.517", "lastModified": "2023-05-16T10:59:26.917", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "kernel/module.c in the Linux kernel before 5.12.14 mishandles Signature Verification, aka CID-0c18f29aae7c. Without CONFIG_MODULE_SIG, verification that a kernel module is signed, for loading via init_module, does not occur for a module.sig_enforce=1 command-line argument."}, {"lang": "es", "value": "El archivo kernel/module.c en el kernel de Linux versiones anteriores a 5.12.14, maneja inapropiadamente la Verificaci\u00f3n de firmas, tambi\u00e9n se conoce como CID-0c18f29aae7c.&#xa0;Sin CONFIG_MODULE_SIG, la verificaci\u00f3n de que un m\u00f3dulo del kernel est\u00e1 firmado, para cargar por medio de la funci\u00f3n init_module, no ocurre para un argumento module.sig_enforce=1 en la l\u00ednea de comandos"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-347"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.15", "versionEndExcluding": "4.19.196", "matchCriteriaId": "F3CAB837-7D38-4934-AD4F-195CEFD754E6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.20", "versionEndExcluding": "5.4.129", "matchCriteriaId": "3BA224A1-00EE-4171-B259-921A4A3E2A26"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.5", "versionEndExcluding": "5.10.47", "matchCriteriaId": "7E7C0CCD-69D3-4E4B-A278-9DA1DC6464ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.11", "versionEndExcluding": "5.12.14", "matchCriteriaId": "AE921D8E-1E68-4B26-B8BB-2C48BF1BE23C"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}], "references": [{"url": "http://www.openwall.com/lists/oss-security/2021/07/06/3", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.12.14", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=0c18f29aae7ce3dadd26d8ee3505d07cc982df75", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/0c18f29aae7ce3dadd26d8ee3505d07cc982df75", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2021/10/msg00010.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20210813-0004/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.openwall.com/lists/oss-security/2021/07/06/3", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0c18f29aae7ce3dadd26d8ee3505d07cc982df75"}}