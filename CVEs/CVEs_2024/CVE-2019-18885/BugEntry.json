{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) STRATO AG 2012.  All rights reserved.\n */\n\n#include <linux/sched.h>\n#include <linux/bio.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/kthread.h>\n#include <linux/math64.h>\n#include \"ctree.h\"\n#include \"extent_map.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"async-thread.h\"\n#include \"check-integrity.h\"\n#include \"rcu-string.h\"\n#include \"dev-replace.h\"\n#include \"sysfs.h\"\n\nstatic int btrfs_dev_replace_finishing(struct btrfs_fs_info *fs_info,\n\t\t\t\t       int scrub_ret);\nstatic void btrfs_dev_replace_update_device_in_mapping_tree(\n\t\t\t\t\t\tstruct btrfs_fs_info *fs_info,\n\t\t\t\t\t\tstruct btrfs_device *srcdev,\n\t\t\t\t\t\tstruct btrfs_device *tgtdev);\nstatic int btrfs_dev_replace_kthread(void *data);\n\nint btrfs_init_dev_replace(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tint ret = 0;\n\tstruct btrfs_path *path = NULL;\n\tint item_size;\n\tstruct btrfs_dev_replace_item *ptr;\n\tu64 src_devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_DEV_REPLACE_KEY;\n\tkey.offset = 0;\n\tret = btrfs_search_slot(NULL, dev_root, &key, path, 0, 0);\n\tif (ret) {\nno_valid_dev_replace_entry_found:\n\t\tret = 0;\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_DEV_REPLACE_ITEM_STATE_NEVER_STARTED;\n\t\tdev_replace->cont_reading_from_srcdev_mode =\n\t\t    BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_ALWAYS;\n\t\tdev_replace->time_started = 0;\n\t\tdev_replace->time_stopped = 0;\n\t\tatomic64_set(&dev_replace->num_write_errors, 0);\n\t\tatomic64_set(&dev_replace->num_uncorrectable_read_errors, 0);\n\t\tdev_replace->cursor_left = 0;\n\t\tdev_replace->committed_cursor_left = 0;\n\t\tdev_replace->cursor_left_last_write_of_item = 0;\n\t\tdev_replace->cursor_right = 0;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tdev_replace->is_valid = 0;\n\t\tdev_replace->item_needs_writeback = 0;\n\t\tgoto out;\n\t}\n\tslot = path->slots[0];\n\teb = path->nodes[0];\n\titem_size = btrfs_item_size_nr(eb, slot);\n\tptr = btrfs_item_ptr(eb, slot, struct btrfs_dev_replace_item);\n\n\tif (item_size != sizeof(struct btrfs_dev_replace_item)) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t\"dev_replace entry found has unexpected size, ignore entry\");\n\t\tgoto no_valid_dev_replace_entry_found;\n\t}\n\n\tsrc_devid = btrfs_dev_replace_src_devid(eb, ptr);\n\tdev_replace->cont_reading_from_srcdev_mode =\n\t\tbtrfs_dev_replace_cont_reading_from_srcdev_mode(eb, ptr);\n\tdev_replace->replace_state = btrfs_dev_replace_replace_state(eb, ptr);\n\tdev_replace->time_started = btrfs_dev_replace_time_started(eb, ptr);\n\tdev_replace->time_stopped =\n\t\tbtrfs_dev_replace_time_stopped(eb, ptr);\n\tatomic64_set(&dev_replace->num_write_errors,\n\t\t     btrfs_dev_replace_num_write_errors(eb, ptr));\n\tatomic64_set(&dev_replace->num_uncorrectable_read_errors,\n\t\t     btrfs_dev_replace_num_uncorrectable_read_errors(eb, ptr));\n\tdev_replace->cursor_left = btrfs_dev_replace_cursor_left(eb, ptr);\n\tdev_replace->committed_cursor_left = dev_replace->cursor_left;\n\tdev_replace->cursor_left_last_write_of_item = dev_replace->cursor_left;\n\tdev_replace->cursor_right = btrfs_dev_replace_cursor_right(eb, ptr);\n\tdev_replace->is_valid = 1;\n\n\tdev_replace->item_needs_writeback = 0;\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tdev_replace->srcdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tsrc_devid, NULL, NULL);\n\t\tdev_replace->tgtdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tBTRFS_DEV_REPLACE_DEVID,\n\t\t\t\t\t\t\tNULL, NULL);\n\t\t/*\n\t\t * allow 'btrfs dev replace_cancel' if src/tgt device is\n\t\t * missing\n\t\t */\n\t\tif (!dev_replace->srcdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"srcdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t   src_devid);\n\t\t}\n\t\tif (!dev_replace->tgtdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"tgtdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t\tBTRFS_DEV_REPLACE_DEVID);\n\t\t}\n\t\tif (dev_replace->tgtdev) {\n\t\t\tif (dev_replace->srcdev) {\n\t\t\t\tdev_replace->tgtdev->total_bytes =\n\t\t\t\t\tdev_replace->srcdev->total_bytes;\n\t\t\t\tdev_replace->tgtdev->disk_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->disk_total_bytes;\n\t\t\t\tdev_replace->tgtdev->commit_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->commit_total_bytes;\n\t\t\t\tdev_replace->tgtdev->bytes_used =\n\t\t\t\t\tdev_replace->srcdev->bytes_used;\n\t\t\t\tdev_replace->tgtdev->commit_bytes_used =\n\t\t\t\t\tdev_replace->srcdev->commit_bytes_used;\n\t\t\t}\n\t\t\tset_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\n\t\t\tWARN_ON(fs_info->fs_devices->rw_devices == 0);\n\t\t\tdev_replace->tgtdev->io_width = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->io_align = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->sector_size = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->fs_info = fs_info;\n\t\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\t\t}\n\t\tbreak;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Initialize a new device for device replace target from a given source dev\n * and path.\n *\n * Return 0 and new device in @device_out, otherwise return < 0\n */\nstatic int btrfs_init_dev_replace_tgtdev(struct btrfs_fs_info *fs_info,\n\t\t\t\t  const char *device_path,\n\t\t\t\t  struct btrfs_device *srcdev,\n\t\t\t\t  struct btrfs_device **device_out)\n{\n\tstruct btrfs_device *device;\n\tstruct block_device *bdev;\n\tstruct list_head *devices;\n\tstruct rcu_string *name;\n\tu64 devid = BTRFS_DEV_REPLACE_DEVID;\n\tint ret = 0;\n\n\t*device_out = NULL;\n\tif (fs_info->fs_devices->seeding) {\n\t\tbtrfs_err(fs_info, \"the filesystem is a seed filesystem!\");\n\t\treturn -EINVAL;\n\t}\n\n\tbdev = blkdev_get_by_path(device_path, FMODE_WRITE | FMODE_EXCL,\n\t\t\t\t  fs_info->bdev_holder);\n\tif (IS_ERR(bdev)) {\n\t\tbtrfs_err(fs_info, \"target device %s is invalid!\", device_path);\n\t\treturn PTR_ERR(bdev);\n\t}\n\n\tfilemap_write_and_wait(bdev->bd_inode->i_mapping);\n\n\tdevices = &fs_info->fs_devices->devices;\n\tlist_for_each_entry(device, devices, dev_list) {\n\t\tif (device->bdev == bdev) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t  \"target device is in the filesystem!\");\n\t\t\tret = -EEXIST;\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\n\tif (i_size_read(bdev->bd_inode) <\n\t    btrfs_device_get_total_bytes(srcdev)) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"target device is smaller than source device!\");\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\n\n\tdevice = btrfs_alloc_device(NULL, &devid, NULL);\n\tif (IS_ERR(device)) {\n\t\tret = PTR_ERR(device);\n\t\tgoto error;\n\t}\n\n\tname = rcu_string_strdup(device_path, GFP_KERNEL);\n\tif (!name) {\n\t\tbtrfs_free_device(device);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\trcu_assign_pointer(device->name, name);\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tdevice->generation = 0;\n\tdevice->io_width = fs_info->sectorsize;\n\tdevice->io_align = fs_info->sectorsize;\n\tdevice->sector_size = fs_info->sectorsize;\n\tdevice->total_bytes = btrfs_device_get_total_bytes(srcdev);\n\tdevice->disk_total_bytes = btrfs_device_get_disk_total_bytes(srcdev);\n\tdevice->bytes_used = btrfs_device_get_bytes_used(srcdev);\n\tdevice->commit_total_bytes = srcdev->commit_total_bytes;\n\tdevice->commit_bytes_used = device->bytes_used;\n\tdevice->fs_info = fs_info;\n\tdevice->bdev = bdev;\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tset_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\tdevice->mode = FMODE_EXCL;\n\tdevice->dev_stats_valid = 1;\n\tset_blocksize(device->bdev, BTRFS_BDEV_BLOCKSIZE);\n\tdevice->fs_devices = fs_info->fs_devices;\n\tlist_add(&device->dev_list, &fs_info->fs_devices->devices);\n\tfs_info->fs_devices->num_devices++;\n\tfs_info->fs_devices->open_devices++;\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t*device_out = device;\n\treturn 0;\n\nerror:\n\tblkdev_put(bdev, FMODE_EXCL);\n\treturn ret;\n}\n\n/*\n * called from commit_transaction. Writes changed device replace state to\n * disk.\n */\nint btrfs_run_dev_replace(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_fs_info *fs_info)\n{\n\tint ret;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_dev_replace_item *ptr;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tdown_read(&dev_replace->rwsem);\n\tif (!dev_replace->is_valid ||\n\t    !dev_replace->item_needs_writeback) {\n\t\tup_read(&dev_replace->rwsem);\n\t\treturn 0;\n\t}\n\tup_read(&dev_replace->rwsem);\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_DEV_REPLACE_KEY;\n\tkey.offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tret = btrfs_search_slot(trans, dev_root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t   \"error %d while searching for dev_replace item!\",\n\t\t\t   ret);\n\t\tgoto out;\n\t}\n\n\tif (ret == 0 &&\n\t    btrfs_item_size_nr(path->nodes[0], path->slots[0]) < sizeof(*ptr)) {\n\t\t/*\n\t\t * need to delete old one and insert a new one.\n\t\t * Since no attempt is made to recover any old state, if the\n\t\t * dev_replace state is 'running', the data on the target\n\t\t * drive is lost.\n\t\t * It would be possible to recover the state: just make sure\n\t\t * that the beginning of the item is never changed and always\n\t\t * contains all the essential information. Then read this\n\t\t * minimal set of information and use it as a base for the\n\t\t * new state.\n\t\t */\n\t\tret = btrfs_del_item(trans, dev_root, path);\n\t\tif (ret != 0) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"delete too small dev_replace item failed %d!\",\n\t\t\t\t   ret);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 1;\n\t}\n\n\tif (ret == 1) {\n\t\t/* need to insert a new item */\n\t\tbtrfs_release_path(path);\n\t\tret = btrfs_insert_empty_item(trans, dev_root, path,\n\t\t\t\t\t      &key, sizeof(*ptr));\n\t\tif (ret < 0) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"insert dev_replace item failed %d!\", ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\teb = path->nodes[0];\n\tptr = btrfs_item_ptr(eb, path->slots[0],\n\t\t\t     struct btrfs_dev_replace_item);\n\n\tdown_write(&dev_replace->rwsem);\n\tif (dev_replace->srcdev)\n\t\tbtrfs_set_dev_replace_src_devid(eb, ptr,\n\t\t\tdev_replace->srcdev->devid);\n\telse\n\t\tbtrfs_set_dev_replace_src_devid(eb, ptr, (u64)-1);\n\tbtrfs_set_dev_replace_cont_reading_from_srcdev_mode(eb, ptr,\n\t\tdev_replace->cont_reading_from_srcdev_mode);\n\tbtrfs_set_dev_replace_replace_state(eb, ptr,\n\t\tdev_replace->replace_state);\n\tbtrfs_set_dev_replace_time_started(eb, ptr, dev_replace->time_started);\n\tbtrfs_set_dev_replace_time_stopped(eb, ptr, dev_replace->time_stopped);\n\tbtrfs_set_dev_replace_num_write_errors(eb, ptr,\n\t\tatomic64_read(&dev_replace->num_write_errors));\n\tbtrfs_set_dev_replace_num_uncorrectable_read_errors(eb, ptr,\n\t\tatomic64_read(&dev_replace->num_uncorrectable_read_errors));\n\tdev_replace->cursor_left_last_write_of_item =\n\t\tdev_replace->cursor_left;\n\tbtrfs_set_dev_replace_cursor_left(eb, ptr,\n\t\tdev_replace->cursor_left_last_write_of_item);\n\tbtrfs_set_dev_replace_cursor_right(eb, ptr,\n\t\tdev_replace->cursor_right);\n\tdev_replace->item_needs_writeback = 0;\n\tup_write(&dev_replace->rwsem);\n\n\tbtrfs_mark_buffer_dirty(eb);\n\nout:\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\nstatic char* btrfs_dev_name(struct btrfs_device *device)\n{\n\tif (!device || test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\treturn \"<missing disk>\";\n\telse\n\t\treturn rcu_str_deref(device->name);\n}\n\nstatic int btrfs_dev_replace_start(struct btrfs_fs_info *fs_info,\n\t\tconst char *tgtdev_name, u64 srcdevid, const char *srcdev_name,\n\t\tint read_src)\n{\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tint ret;\n\tstruct btrfs_device *tgt_device = NULL;\n\tstruct btrfs_device *src_device = NULL;\n\tbool need_unlock;\n\n\tsrc_device = btrfs_find_device_by_devspec(fs_info, srcdevid,\n\t\t\t\t\t\t  srcdev_name);\n\tif (IS_ERR(src_device))\n\t\treturn PTR_ERR(src_device);\n\n\tif (btrfs_pinned_by_swapfile(fs_info, src_device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t  \"cannot replace device %s (devid %llu) due to active swapfile\",\n\t\t\tbtrfs_dev_name(src_device), src_device->devid);\n\t\treturn -ETXTBSY;\n\t}\n\n\tret = btrfs_init_dev_replace_tgtdev(fs_info, tgtdev_name,\n\t\t\t\t\t    src_device, &tgt_device);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Here we commit the transaction to make sure commit_total_bytes\n\t * of all the devices are updated.\n\t */\n\ttrans = btrfs_attach_transaction(root);\n\tif (!IS_ERR(trans)) {\n\t\tret = btrfs_commit_transaction(trans);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else if (PTR_ERR(trans) != -ENOENT) {\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tneed_unlock = true;\n\tdown_write(&dev_replace->rwsem);\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tASSERT(0);\n\t\tret = BTRFS_IOCTL_DEV_REPLACE_RESULT_ALREADY_STARTED;\n\t\tgoto leave;\n\t}\n\n\tdev_replace->cont_reading_from_srcdev_mode = read_src;\n\tWARN_ON(!src_device);\n\tdev_replace->srcdev = src_device;\n\tdev_replace->tgtdev = tgt_device;\n\n\tbtrfs_info_in_rcu(fs_info,\n\t\t      \"dev_replace from %s (devid %llu) to %s started\",\n\t\t      btrfs_dev_name(src_device),\n\t\t      src_device->devid,\n\t\t      rcu_str_deref(tgt_device->name));\n\n\t/*\n\t * from now on, the writes to the srcdev are all duplicated to\n\t * go to the tgtdev as well (refer to btrfs_map_block()).\n\t */\n\tdev_replace->replace_state = BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED;\n\tdev_replace->time_started = ktime_get_real_seconds();\n\tdev_replace->cursor_left = 0;\n\tdev_replace->committed_cursor_left = 0;\n\tdev_replace->cursor_left_last_write_of_item = 0;\n\tdev_replace->cursor_right = 0;\n\tdev_replace->is_valid = 1;\n\tdev_replace->item_needs_writeback = 1;\n\tatomic64_set(&dev_replace->num_write_errors, 0);\n\tatomic64_set(&dev_replace->num_uncorrectable_read_errors, 0);\n\tup_write(&dev_replace->rwsem);\n\tneed_unlock = false;\n\n\tret = btrfs_sysfs_add_device_link(tgt_device->fs_devices, tgt_device);\n\tif (ret)\n\t\tbtrfs_err(fs_info, \"kobj add dev failed %d\", ret);\n\n\tbtrfs_wait_ordered_roots(fs_info, U64_MAX, 0, (u64)-1);\n\n\t/* force writing the updated state information to disk */\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tneed_unlock = true;\n\t\tdown_write(&dev_replace->rwsem);\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tgoto leave;\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\tWARN_ON(ret);\n\n\t/* the disk copy procedure reuses the scrub code */\n\tret = btrfs_scrub_dev(fs_info, src_device->devid, 0,\n\t\t\t      btrfs_device_get_total_bytes(src_device),\n\t\t\t      &dev_replace->scrub_progress, 0, 1);\n\n\tret = btrfs_dev_replace_finishing(fs_info, ret);\n\tif (ret == -EINPROGRESS) {\n\t\tret = BTRFS_IOCTL_DEV_REPLACE_RESULT_SCRUB_INPROGRESS;\n\t} else if (ret != -ECANCELED) {\n\t\tWARN_ON(ret);\n\t}\n\n\treturn ret;\n\nleave:\n\tif (need_unlock)\n\t\tup_write(&dev_replace->rwsem);\n\tbtrfs_destroy_dev_replace_tgtdev(tgt_device);\n\treturn ret;\n}\n\nint btrfs_dev_replace_by_ioctl(struct btrfs_fs_info *fs_info,\n\t\t\t    struct btrfs_ioctl_dev_replace_args *args)\n{\n\tint ret;\n\n\tswitch (args->start.cont_reading_from_srcdev_mode) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_CONT_READING_FROM_SRCDEV_MODE_ALWAYS:\n\tcase BTRFS_IOCTL_DEV_REPLACE_CONT_READING_FROM_SRCDEV_MODE_AVOID:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->start.srcdevid == 0 && args->start.srcdev_name[0] == '\\0') ||\n\t    args->start.tgtdev_name[0] == '\\0')\n\t\treturn -EINVAL;\n\n\tret = btrfs_dev_replace_start(fs_info, args->start.tgtdev_name,\n\t\t\t\t\targs->start.srcdevid,\n\t\t\t\t\targs->start.srcdev_name,\n\t\t\t\t\targs->start.cont_reading_from_srcdev_mode);\n\targs->result = ret;\n\t/* don't warn if EINPROGRESS, someone else might be running scrub */\n\tif (ret == BTRFS_IOCTL_DEV_REPLACE_RESULT_SCRUB_INPROGRESS ||\n\t    ret == BTRFS_IOCTL_DEV_REPLACE_RESULT_NO_ERROR)\n\t\treturn 0;\n\n\treturn ret;\n}\n\n/*\n * blocked until all in-flight bios operations are finished.\n */\nstatic void btrfs_rm_dev_replace_blocked(struct btrfs_fs_info *fs_info)\n{\n\tset_bit(BTRFS_FS_STATE_DEV_REPLACING, &fs_info->fs_state);\n\twait_event(fs_info->dev_replace.replace_wait, !percpu_counter_sum(\n\t\t   &fs_info->dev_replace.bio_counter));\n}\n\n/*\n * we have removed target device, it is safe to allow new bios request.\n */\nstatic void btrfs_rm_dev_replace_unblocked(struct btrfs_fs_info *fs_info)\n{\n\tclear_bit(BTRFS_FS_STATE_DEV_REPLACING, &fs_info->fs_state);\n\twake_up(&fs_info->dev_replace.replace_wait);\n}\n\nstatic int btrfs_dev_replace_finishing(struct btrfs_fs_info *fs_info,\n\t\t\t\t       int scrub_ret)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct btrfs_device *tgt_device;\n\tstruct btrfs_device *src_device;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tu8 uuid_tmp[BTRFS_UUID_SIZE];\n\tstruct btrfs_trans_handle *trans;\n\tint ret = 0;\n\n\t/* don't allow cancel or unmount to disturb the finishing procedure */\n\tmutex_lock(&dev_replace->lock_finishing_cancel_unmount);\n\n\tdown_read(&dev_replace->rwsem);\n\t/* was the operation canceled, or is it finished? */\n\tif (dev_replace->replace_state !=\n\t    BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED) {\n\t\tup_read(&dev_replace->rwsem);\n\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\t\treturn 0;\n\t}\n\n\ttgt_device = dev_replace->tgtdev;\n\tsrc_device = dev_replace->srcdev;\n\tup_read(&dev_replace->rwsem);\n\n\t/*\n\t * flush all outstanding I/O and inode extent mappings before the\n\t * copy operation is declared as being finished\n\t */\n\tret = btrfs_start_delalloc_roots(fs_info, -1);\n\tif (ret) {\n\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\t\treturn ret;\n\t}\n\tbtrfs_wait_ordered_roots(fs_info, U64_MAX, 0, (u64)-1);\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\t\treturn PTR_ERR(trans);\n\t}\n\tret = btrfs_commit_transaction(trans);\n\tWARN_ON(ret);\n\n\t/* keep away write_all_supers() during the finishing procedure */\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tdown_write(&dev_replace->rwsem);\n\tdev_replace->replace_state =\n\t\tscrub_ret ? BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED\n\t\t\t  : BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED;\n\tdev_replace->tgtdev = NULL;\n\tdev_replace->srcdev = NULL;\n\tdev_replace->time_stopped = ktime_get_real_seconds();\n\tdev_replace->item_needs_writeback = 1;\n\n\t/* replace old device with new one in mapping tree */\n\tif (!scrub_ret) {\n\t\tbtrfs_dev_replace_update_device_in_mapping_tree(fs_info,\n\t\t\t\t\t\t\t\tsrc_device,\n\t\t\t\t\t\t\t\ttgt_device);\n\t} else {\n\t\tif (scrub_ret != -ECANCELED)\n\t\t\tbtrfs_err_in_rcu(fs_info,\n\t\t\t\t \"btrfs_scrub_dev(%s, %llu, %s) failed %d\",\n\t\t\t\t btrfs_dev_name(src_device),\n\t\t\t\t src_device->devid,\n\t\t\t\t rcu_str_deref(tgt_device->name), scrub_ret);\n\t\tup_write(&dev_replace->rwsem);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tbtrfs_rm_dev_replace_blocked(fs_info);\n\t\tif (tgt_device)\n\t\t\tbtrfs_destroy_dev_replace_tgtdev(tgt_device);\n\t\tbtrfs_rm_dev_replace_unblocked(fs_info);\n\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\n\t\treturn scrub_ret;\n\t}\n\n\tbtrfs_info_in_rcu(fs_info,\n\t\t\t  \"dev_replace from %s (devid %llu) to %s finished\",\n\t\t\t  btrfs_dev_name(src_device),\n\t\t\t  src_device->devid,\n\t\t\t  rcu_str_deref(tgt_device->name));\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &tgt_device->dev_state);\n\ttgt_device->devid = src_device->devid;\n\tsrc_device->devid = BTRFS_DEV_REPLACE_DEVID;\n\tmemcpy(uuid_tmp, tgt_device->uuid, sizeof(uuid_tmp));\n\tmemcpy(tgt_device->uuid, src_device->uuid, sizeof(tgt_device->uuid));\n\tmemcpy(src_device->uuid, uuid_tmp, sizeof(src_device->uuid));\n\tbtrfs_device_set_total_bytes(tgt_device, src_device->total_bytes);\n\tbtrfs_device_set_disk_total_bytes(tgt_device,\n\t\t\t\t\t  src_device->disk_total_bytes);\n\tbtrfs_device_set_bytes_used(tgt_device, src_device->bytes_used);\n\tASSERT(list_empty(&src_device->resized_list));\n\ttgt_device->commit_total_bytes = src_device->commit_total_bytes;\n\ttgt_device->commit_bytes_used = src_device->bytes_used;\n\n\tbtrfs_assign_next_active_device(src_device, tgt_device);\n\n\tlist_add(&tgt_device->dev_alloc_list, &fs_info->fs_devices->alloc_list);\n\tfs_info->fs_devices->rw_devices++;\n\n\tup_write(&dev_replace->rwsem);\n\tbtrfs_rm_dev_replace_blocked(fs_info);\n\n\tbtrfs_rm_dev_replace_remove_srcdev(src_device);\n\n\tbtrfs_rm_dev_replace_unblocked(fs_info);\n\n\t/*\n\t * Increment dev_stats_ccnt so that btrfs_run_dev_stats() will\n\t * update on-disk dev stats value during commit transaction\n\t */\n\tatomic_inc(&tgt_device->dev_stats_ccnt);\n\n\t/*\n\t * this is again a consistent state where no dev_replace procedure\n\t * is running, the target device is part of the filesystem, the\n\t * source device is not part of the filesystem anymore and its 1st\n\t * superblock is scratched out so that it is no longer marked to\n\t * belong to this filesystem.\n\t */\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t/* replace the sysfs entry */\n\tbtrfs_sysfs_rm_device_link(fs_info->fs_devices, src_device);\n\tbtrfs_rm_dev_replace_free_srcdev(fs_info, src_device);\n\n\t/* write back the superblocks */\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (!IS_ERR(trans))\n\t\tbtrfs_commit_transaction(trans);\n\n\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\n\treturn 0;\n}\n\nstatic void btrfs_dev_replace_update_device_in_mapping_tree(\n\t\t\t\t\t\tstruct btrfs_fs_info *fs_info,\n\t\t\t\t\t\tstruct btrfs_device *srcdev,\n\t\t\t\t\t\tstruct btrfs_device *tgtdev)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 start = 0;\n\tint i;\n\n\twrite_lock(&em_tree->lock);\n\tdo {\n\t\tem = lookup_extent_mapping(em_tree, start, (u64)-1);\n\t\tif (!em)\n\t\t\tbreak;\n\t\tmap = em->map_lookup;\n\t\tfor (i = 0; i < map->num_stripes; i++)\n\t\t\tif (srcdev == map->stripes[i].dev)\n\t\t\t\tmap->stripes[i].dev = tgtdev;\n\t\tstart = em->start + em->len;\n\t\tfree_extent_map(em);\n\t} while (start);\n\twrite_unlock(&em_tree->lock);\n}\n\n/*\n * Read progress of device replace status according to the state and last\n * stored position. The value format is the same as for\n * btrfs_dev_replace::progress_1000\n */\nstatic u64 btrfs_dev_replace_progress(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tu64 ret = 0;\n\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tret = 0;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\t\tret = 1000;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tret = div64_u64(dev_replace->cursor_left,\n\t\t\t\tdiv_u64(btrfs_device_get_total_bytes(\n\t\t\t\t\t\tdev_replace->srcdev), 1000));\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nvoid btrfs_dev_replace_status(struct btrfs_fs_info *fs_info,\n\t\t\t      struct btrfs_ioctl_dev_replace_args *args)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tdown_read(&dev_replace->rwsem);\n\t/* even if !dev_replace_is_valid, the values are good enough for\n\t * the replace_status ioctl */\n\targs->result = BTRFS_IOCTL_DEV_REPLACE_RESULT_NO_ERROR;\n\targs->status.replace_state = dev_replace->replace_state;\n\targs->status.time_started = dev_replace->time_started;\n\targs->status.time_stopped = dev_replace->time_stopped;\n\targs->status.num_write_errors =\n\t\tatomic64_read(&dev_replace->num_write_errors);\n\targs->status.num_uncorrectable_read_errors =\n\t\tatomic64_read(&dev_replace->num_uncorrectable_read_errors);\n\targs->status.progress_1000 = btrfs_dev_replace_progress(fs_info);\n\tup_read(&dev_replace->rwsem);\n}\n\nint btrfs_dev_replace_cancel(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct btrfs_device *tgt_device = NULL;\n\tstruct btrfs_device *src_device = NULL;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tint result;\n\tint ret;\n\n\tif (sb_rdonly(fs_info->sb))\n\t\treturn -EROFS;\n\n\tmutex_lock(&dev_replace->lock_finishing_cancel_unmount);\n\tdown_write(&dev_replace->rwsem);\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tresult = BTRFS_IOCTL_DEV_REPLACE_RESULT_NOT_STARTED;\n\t\tup_write(&dev_replace->rwsem);\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\t\ttgt_device = dev_replace->tgtdev;\n\t\tsrc_device = dev_replace->srcdev;\n\t\tup_write(&dev_replace->rwsem);\n\t\tret = btrfs_scrub_cancel(fs_info);\n\t\tif (ret < 0) {\n\t\t\tresult = BTRFS_IOCTL_DEV_REPLACE_RESULT_NOT_STARTED;\n\t\t} else {\n\t\t\tresult = BTRFS_IOCTL_DEV_REPLACE_RESULT_NO_ERROR;\n\t\t\t/*\n\t\t\t * btrfs_dev_replace_finishing() will handle the\n\t\t\t * cleanup part\n\t\t\t */\n\t\t\tbtrfs_info_in_rcu(fs_info,\n\t\t\t\t\"dev_replace from %s (devid %llu) to %s canceled\",\n\t\t\t\tbtrfs_dev_name(src_device), src_device->devid,\n\t\t\t\tbtrfs_dev_name(tgt_device));\n\t\t}\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\t/*\n\t\t * Scrub doing the replace isn't running so we need to do the\n\t\t * cleanup step of btrfs_dev_replace_finishing() here\n\t\t */\n\t\tresult = BTRFS_IOCTL_DEV_REPLACE_RESULT_NO_ERROR;\n\t\ttgt_device = dev_replace->tgtdev;\n\t\tsrc_device = dev_replace->srcdev;\n\t\tdev_replace->tgtdev = NULL;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->replace_state =\n\t\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED;\n\t\tdev_replace->time_stopped = ktime_get_real_seconds();\n\t\tdev_replace->item_needs_writeback = 1;\n\n\t\tup_write(&dev_replace->rwsem);\n\n\t\t/* Scrub for replace must not be running in suspended state */\n\t\tret = btrfs_scrub_cancel(fs_info);\n\t\tASSERT(ret != -ENOTCONN);\n\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\t\t\treturn PTR_ERR(trans);\n\t\t}\n\t\tret = btrfs_commit_transaction(trans);\n\t\tWARN_ON(ret);\n\n\t\tbtrfs_info_in_rcu(fs_info,\n\t\t\"suspended dev_replace from %s (devid %llu) to %s canceled\",\n\t\t\tbtrfs_dev_name(src_device), src_device->devid,\n\t\t\tbtrfs_dev_name(tgt_device));\n\n\t\tif (tgt_device)\n\t\t\tbtrfs_destroy_dev_replace_tgtdev(tgt_device);\n\t\tbreak;\n\tdefault:\n\t\tresult = -EINVAL;\n\t}\n\n\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\treturn result;\n}\n\nvoid btrfs_dev_replace_suspend_for_unmount(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tmutex_lock(&dev_replace->lock_finishing_cancel_unmount);\n\tdown_write(&dev_replace->rwsem);\n\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED;\n\t\tdev_replace->time_stopped = ktime_get_real_seconds();\n\t\tdev_replace->item_needs_writeback = 1;\n\t\tbtrfs_info(fs_info, \"suspending dev_replace for unmount\");\n\t\tbreak;\n\t}\n\n\tup_write(&dev_replace->rwsem);\n\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n}\n\n/* resume dev_replace procedure that was interrupted by unmount */\nint btrfs_resume_dev_replace_async(struct btrfs_fs_info *fs_info)\n{\n\tstruct task_struct *task;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tdown_write(&dev_replace->rwsem);\n\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tup_write(&dev_replace->rwsem);\n\t\treturn 0;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_STARTED;\n\t\tbreak;\n\t}\n\tif (!dev_replace->tgtdev || !dev_replace->tgtdev->bdev) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"cannot continue dev_replace, tgtdev is missing\");\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"you may cancel the operation after 'mount -o degraded'\");\n\t\tdev_replace->replace_state =\n\t\t\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED;\n\t\tup_write(&dev_replace->rwsem);\n\t\treturn 0;\n\t}\n\tup_write(&dev_replace->rwsem);\n\n\t/*\n\t * This could collide with a paused balance, but the exclusive op logic\n\t * should never allow both to start and pause. We don't want to allow\n\t * dev-replace to start anyway.\n\t */\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tdown_write(&dev_replace->rwsem);\n\t\tdev_replace->replace_state =\n\t\t\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED;\n\t\tup_write(&dev_replace->rwsem);\n\t\tbtrfs_info(fs_info,\n\t\t\"cannot resume dev-replace, other exclusive operation running\");\n\t\treturn 0;\n\t}\n\n\ttask = kthread_run(btrfs_dev_replace_kthread, fs_info, \"btrfs-devrepl\");\n\treturn PTR_ERR_OR_ZERO(task);\n}\n\nstatic int btrfs_dev_replace_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tu64 progress;\n\tint ret;\n\n\tprogress = btrfs_dev_replace_progress(fs_info);\n\tprogress = div_u64(progress, 10);\n\tbtrfs_info_in_rcu(fs_info,\n\t\t\"continuing dev_replace from %s (devid %llu) to target %s @%u%%\",\n\t\tbtrfs_dev_name(dev_replace->srcdev),\n\t\tdev_replace->srcdev->devid,\n\t\tbtrfs_dev_name(dev_replace->tgtdev),\n\t\t(unsigned int)progress);\n\n\tret = btrfs_scrub_dev(fs_info, dev_replace->srcdev->devid,\n\t\t\t      dev_replace->committed_cursor_left,\n\t\t\t      btrfs_device_get_total_bytes(dev_replace->srcdev),\n\t\t\t      &dev_replace->scrub_progress, 0, 1);\n\tret = btrfs_dev_replace_finishing(fs_info, ret);\n\tWARN_ON(ret && ret != -ECANCELED);\n\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\treturn 0;\n}\n\nint btrfs_dev_replace_is_ongoing(struct btrfs_dev_replace *dev_replace)\n{\n\tif (!dev_replace->is_valid)\n\t\treturn 0;\n\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\treturn 0;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\t/*\n\t\t * return true even if tgtdev is missing (this is\n\t\t * something that can happen if the dev_replace\n\t\t * procedure is suspended by an umount and then\n\t\t * the tgtdev is missing (or \"btrfs dev scan\") was\n\t\t * not called and the filesystem is remounted\n\t\t * in degraded state. This does not stop the\n\t\t * dev_replace procedure. It needs to be canceled\n\t\t * manually if the cancellation is wanted.\n\t\t */\n\t\tbreak;\n\t}\n\treturn 1;\n}\n\nvoid btrfs_bio_counter_inc_noblocked(struct btrfs_fs_info *fs_info)\n{\n\tpercpu_counter_inc(&fs_info->dev_replace.bio_counter);\n}\n\nvoid btrfs_bio_counter_sub(struct btrfs_fs_info *fs_info, s64 amount)\n{\n\tpercpu_counter_sub(&fs_info->dev_replace.bio_counter, amount);\n\tcond_wake_up_nomb(&fs_info->dev_replace.replace_wait);\n}\n\nvoid btrfs_bio_counter_inc_blocked(struct btrfs_fs_info *fs_info)\n{\n\twhile (1) {\n\t\tpercpu_counter_inc(&fs_info->dev_replace.bio_counter);\n\t\tif (likely(!test_bit(BTRFS_FS_STATE_DEV_REPLACING,\n\t\t\t\t     &fs_info->fs_state)))\n\t\t\tbreak;\n\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\twait_event(fs_info->dev_replace.replace_wait,\n\t\t\t   !test_bit(BTRFS_FS_STATE_DEV_REPLACING,\n\t\t\t\t     &fs_info->fs_state));\n\t}\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n */\n\n#include <linux/kernel.h>\n#include <linux/bio.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/fsnotify.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/time.h>\n#include <linux/string.h>\n#include <linux/backing-dev.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/writeback.h>\n#include <linux/compat.h>\n#include <linux/security.h>\n#include <linux/xattr.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/uuid.h>\n#include <linux/btrfs.h>\n#include <linux/uaccess.h>\n#include <linux/iversion.h>\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"btrfs_inode.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"locking.h\"\n#include \"inode-map.h\"\n#include \"backref.h\"\n#include \"rcu-string.h\"\n#include \"send.h\"\n#include \"dev-replace.h\"\n#include \"props.h\"\n#include \"sysfs.h\"\n#include \"qgroup.h\"\n#include \"tree-log.h\"\n#include \"compression.h\"\n\n#ifdef CONFIG_64BIT\n/* If we have a 32-bit userspace and 64-bit kernel, then the UAPI\n * structures are incorrect, as the timespec structure from userspace\n * is 4 bytes too small. We define these alternatives here to teach\n * the kernel about the 32-bit struct packing.\n */\nstruct btrfs_ioctl_timespec_32 {\n\t__u64 sec;\n\t__u32 nsec;\n} __attribute__ ((__packed__));\n\nstruct btrfs_ioctl_received_subvol_args_32 {\n\tchar\tuuid[BTRFS_UUID_SIZE];\t/* in */\n\t__u64\tstransid;\t\t/* in */\n\t__u64\trtransid;\t\t/* out */\n\tstruct btrfs_ioctl_timespec_32 stime; /* in */\n\tstruct btrfs_ioctl_timespec_32 rtime; /* out */\n\t__u64\tflags;\t\t\t/* in */\n\t__u64\treserved[16];\t\t/* in */\n} __attribute__ ((__packed__));\n\n#define BTRFS_IOC_SET_RECEIVED_SUBVOL_32 _IOWR(BTRFS_IOCTL_MAGIC, 37, \\\n\t\t\t\tstruct btrfs_ioctl_received_subvol_args_32)\n#endif\n\n#if defined(CONFIG_64BIT) && defined(CONFIG_COMPAT)\nstruct btrfs_ioctl_send_args_32 {\n\t__s64 send_fd;\t\t\t/* in */\n\t__u64 clone_sources_count;\t/* in */\n\tcompat_uptr_t clone_sources;\t/* in */\n\t__u64 parent_root;\t\t/* in */\n\t__u64 flags;\t\t\t/* in */\n\t__u64 reserved[4];\t\t/* in */\n} __attribute__ ((__packed__));\n\n#define BTRFS_IOC_SEND_32 _IOW(BTRFS_IOCTL_MAGIC, 38, \\\n\t\t\t       struct btrfs_ioctl_send_args_32)\n#endif\n\nstatic int btrfs_clone(struct inode *src, struct inode *inode,\n\t\t       u64 off, u64 olen, u64 olen_aligned, u64 destoff,\n\t\t       int no_time_update);\n\n/* Mask out flags that are inappropriate for the given type of inode. */\nstatic unsigned int btrfs_mask_fsflags_for_type(struct inode *inode,\n\t\tunsigned int flags)\n{\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn flags;\n\telse if (S_ISREG(inode->i_mode))\n\t\treturn flags & ~FS_DIRSYNC_FL;\n\telse\n\t\treturn flags & (FS_NODUMP_FL | FS_NOATIME_FL);\n}\n\n/*\n * Export internal inode flags to the format expected by the FS_IOC_GETFLAGS\n * ioctl.\n */\nstatic unsigned int btrfs_inode_flags_to_fsflags(unsigned int flags)\n{\n\tunsigned int iflags = 0;\n\n\tif (flags & BTRFS_INODE_SYNC)\n\t\tiflags |= FS_SYNC_FL;\n\tif (flags & BTRFS_INODE_IMMUTABLE)\n\t\tiflags |= FS_IMMUTABLE_FL;\n\tif (flags & BTRFS_INODE_APPEND)\n\t\tiflags |= FS_APPEND_FL;\n\tif (flags & BTRFS_INODE_NODUMP)\n\t\tiflags |= FS_NODUMP_FL;\n\tif (flags & BTRFS_INODE_NOATIME)\n\t\tiflags |= FS_NOATIME_FL;\n\tif (flags & BTRFS_INODE_DIRSYNC)\n\t\tiflags |= FS_DIRSYNC_FL;\n\tif (flags & BTRFS_INODE_NODATACOW)\n\t\tiflags |= FS_NOCOW_FL;\n\n\tif (flags & BTRFS_INODE_NOCOMPRESS)\n\t\tiflags |= FS_NOCOMP_FL;\n\telse if (flags & BTRFS_INODE_COMPRESS)\n\t\tiflags |= FS_COMPR_FL;\n\n\treturn iflags;\n}\n\n/*\n * Update inode->i_flags based on the btrfs internal flags.\n */\nvoid btrfs_sync_inode_flags_to_i_flags(struct inode *inode)\n{\n\tstruct btrfs_inode *binode = BTRFS_I(inode);\n\tunsigned int new_fl = 0;\n\n\tif (binode->flags & BTRFS_INODE_SYNC)\n\t\tnew_fl |= S_SYNC;\n\tif (binode->flags & BTRFS_INODE_IMMUTABLE)\n\t\tnew_fl |= S_IMMUTABLE;\n\tif (binode->flags & BTRFS_INODE_APPEND)\n\t\tnew_fl |= S_APPEND;\n\tif (binode->flags & BTRFS_INODE_NOATIME)\n\t\tnew_fl |= S_NOATIME;\n\tif (binode->flags & BTRFS_INODE_DIRSYNC)\n\t\tnew_fl |= S_DIRSYNC;\n\n\tset_mask_bits(&inode->i_flags,\n\t\t      S_SYNC | S_APPEND | S_IMMUTABLE | S_NOATIME | S_DIRSYNC,\n\t\t      new_fl);\n}\n\nstatic int btrfs_ioctl_getflags(struct file *file, void __user *arg)\n{\n\tstruct btrfs_inode *binode = BTRFS_I(file_inode(file));\n\tunsigned int flags = btrfs_inode_flags_to_fsflags(binode->flags);\n\n\tif (copy_to_user(arg, &flags, sizeof(flags)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/* Check if @flags are a supported and valid set of FS_*_FL flags */\nstatic int check_fsflags(unsigned int flags)\n{\n\tif (flags & ~(FS_IMMUTABLE_FL | FS_APPEND_FL | \\\n\t\t      FS_NOATIME_FL | FS_NODUMP_FL | \\\n\t\t      FS_SYNC_FL | FS_DIRSYNC_FL | \\\n\t\t      FS_NOCOMP_FL | FS_COMPR_FL |\n\t\t      FS_NOCOW_FL))\n\t\treturn -EOPNOTSUPP;\n\n\tif ((flags & FS_NOCOMP_FL) && (flags & FS_COMPR_FL))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int btrfs_ioctl_setflags(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_inode *binode = BTRFS_I(inode);\n\tstruct btrfs_root *root = binode->root;\n\tstruct btrfs_trans_handle *trans;\n\tunsigned int fsflags, old_fsflags;\n\tint ret;\n\tu64 old_flags;\n\tunsigned int old_i_flags;\n\tumode_t mode;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EPERM;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tif (copy_from_user(&fsflags, arg, sizeof(fsflags)))\n\t\treturn -EFAULT;\n\n\tret = check_fsflags(fsflags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tinode_lock(inode);\n\n\told_flags = binode->flags;\n\told_i_flags = inode->i_flags;\n\tmode = inode->i_mode;\n\n\tfsflags = btrfs_mask_fsflags_for_type(inode, fsflags);\n\told_fsflags = btrfs_inode_flags_to_fsflags(binode->flags);\n\tif ((fsflags ^ old_fsflags) & (FS_APPEND_FL | FS_IMMUTABLE_FL)) {\n\t\tif (!capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (fsflags & FS_SYNC_FL)\n\t\tbinode->flags |= BTRFS_INODE_SYNC;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_SYNC;\n\tif (fsflags & FS_IMMUTABLE_FL)\n\t\tbinode->flags |= BTRFS_INODE_IMMUTABLE;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_IMMUTABLE;\n\tif (fsflags & FS_APPEND_FL)\n\t\tbinode->flags |= BTRFS_INODE_APPEND;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_APPEND;\n\tif (fsflags & FS_NODUMP_FL)\n\t\tbinode->flags |= BTRFS_INODE_NODUMP;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_NODUMP;\n\tif (fsflags & FS_NOATIME_FL)\n\t\tbinode->flags |= BTRFS_INODE_NOATIME;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_NOATIME;\n\tif (fsflags & FS_DIRSYNC_FL)\n\t\tbinode->flags |= BTRFS_INODE_DIRSYNC;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_DIRSYNC;\n\tif (fsflags & FS_NOCOW_FL) {\n\t\tif (S_ISREG(mode)) {\n\t\t\t/*\n\t\t\t * It's safe to turn csums off here, no extents exist.\n\t\t\t * Otherwise we want the flag to reflect the real COW\n\t\t\t * status of the file and will not set it.\n\t\t\t */\n\t\t\tif (inode->i_size == 0)\n\t\t\t\tbinode->flags |= BTRFS_INODE_NODATACOW\n\t\t\t\t\t      | BTRFS_INODE_NODATASUM;\n\t\t} else {\n\t\t\tbinode->flags |= BTRFS_INODE_NODATACOW;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * Revert back under same assumptions as above\n\t\t */\n\t\tif (S_ISREG(mode)) {\n\t\t\tif (inode->i_size == 0)\n\t\t\t\tbinode->flags &= ~(BTRFS_INODE_NODATACOW\n\t\t\t\t             | BTRFS_INODE_NODATASUM);\n\t\t} else {\n\t\t\tbinode->flags &= ~BTRFS_INODE_NODATACOW;\n\t\t}\n\t}\n\n\t/*\n\t * The COMPRESS flag can only be changed by users, while the NOCOMPRESS\n\t * flag may be changed automatically if compression code won't make\n\t * things smaller.\n\t */\n\tif (fsflags & FS_NOCOMP_FL) {\n\t\tbinode->flags &= ~BTRFS_INODE_COMPRESS;\n\t\tbinode->flags |= BTRFS_INODE_NOCOMPRESS;\n\n\t\tret = btrfs_set_prop(inode, \"btrfs.compression\", NULL, 0, 0);\n\t\tif (ret && ret != -ENODATA)\n\t\t\tgoto out_drop;\n\t} else if (fsflags & FS_COMPR_FL) {\n\t\tconst char *comp;\n\n\t\tif (IS_SWAPFILE(inode)) {\n\t\t\tret = -ETXTBSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tbinode->flags |= BTRFS_INODE_COMPRESS;\n\t\tbinode->flags &= ~BTRFS_INODE_NOCOMPRESS;\n\n\t\tcomp = btrfs_compress_type2str(fs_info->compress_type);\n\t\tif (!comp || comp[0] == 0)\n\t\t\tcomp = btrfs_compress_type2str(BTRFS_COMPRESS_ZLIB);\n\n\t\tret = btrfs_set_prop(inode, \"btrfs.compression\",\n\t\t\t\t     comp, strlen(comp), 0);\n\t\tif (ret)\n\t\t\tgoto out_drop;\n\n\t} else {\n\t\tret = btrfs_set_prop(inode, \"btrfs.compression\", NULL, 0, 0);\n\t\tif (ret && ret != -ENODATA)\n\t\t\tgoto out_drop;\n\t\tbinode->flags &= ~(BTRFS_INODE_COMPRESS | BTRFS_INODE_NOCOMPRESS);\n\t}\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_drop;\n\t}\n\n\tbtrfs_sync_inode_flags_to_i_flags(inode);\n\tinode_inc_iversion(inode);\n\tinode->i_ctime = current_time(inode);\n\tret = btrfs_update_inode(trans, root, inode);\n\n\tbtrfs_end_transaction(trans);\n out_drop:\n\tif (ret) {\n\t\tbinode->flags = old_flags;\n\t\tinode->i_flags = old_i_flags;\n\t}\n\n out_unlock:\n\tinode_unlock(inode);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\n/*\n * Translate btrfs internal inode flags to xflags as expected by the\n * FS_IOC_FSGETXATT ioctl. Filter only the supported ones, unknown flags are\n * silently dropped.\n */\nstatic unsigned int btrfs_inode_flags_to_xflags(unsigned int flags)\n{\n\tunsigned int xflags = 0;\n\n\tif (flags & BTRFS_INODE_APPEND)\n\t\txflags |= FS_XFLAG_APPEND;\n\tif (flags & BTRFS_INODE_IMMUTABLE)\n\t\txflags |= FS_XFLAG_IMMUTABLE;\n\tif (flags & BTRFS_INODE_NOATIME)\n\t\txflags |= FS_XFLAG_NOATIME;\n\tif (flags & BTRFS_INODE_NODUMP)\n\t\txflags |= FS_XFLAG_NODUMP;\n\tif (flags & BTRFS_INODE_SYNC)\n\t\txflags |= FS_XFLAG_SYNC;\n\n\treturn xflags;\n}\n\n/* Check if @flags are a supported and valid set of FS_XFLAGS_* flags */\nstatic int check_xflags(unsigned int flags)\n{\n\tif (flags & ~(FS_XFLAG_APPEND | FS_XFLAG_IMMUTABLE | FS_XFLAG_NOATIME |\n\t\t      FS_XFLAG_NODUMP | FS_XFLAG_SYNC))\n\t\treturn -EOPNOTSUPP;\n\treturn 0;\n}\n\n/*\n * Set the xflags from the internal inode flags. The remaining items of fsxattr\n * are zeroed.\n */\nstatic int btrfs_ioctl_fsgetxattr(struct file *file, void __user *arg)\n{\n\tstruct btrfs_inode *binode = BTRFS_I(file_inode(file));\n\tstruct fsxattr fa;\n\n\tmemset(&fa, 0, sizeof(fa));\n\tfa.fsx_xflags = btrfs_inode_flags_to_xflags(binode->flags);\n\n\tif (copy_to_user(arg, &fa, sizeof(fa)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int btrfs_ioctl_fssetxattr(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_inode *binode = BTRFS_I(inode);\n\tstruct btrfs_root *root = binode->root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct fsxattr fa;\n\tunsigned old_flags;\n\tunsigned old_i_flags;\n\tint ret = 0;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EPERM;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tmemset(&fa, 0, sizeof(fa));\n\tif (copy_from_user(&fa, arg, sizeof(fa)))\n\t\treturn -EFAULT;\n\n\tret = check_xflags(fa.fsx_xflags);\n\tif (ret)\n\t\treturn ret;\n\n\tif (fa.fsx_extsize != 0 || fa.fsx_projid != 0 || fa.fsx_cowextsize != 0)\n\t\treturn -EOPNOTSUPP;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tinode_lock(inode);\n\n\told_flags = binode->flags;\n\told_i_flags = inode->i_flags;\n\n\t/* We need the capabilities to change append-only or immutable inode */\n\tif (((old_flags & (BTRFS_INODE_APPEND | BTRFS_INODE_IMMUTABLE)) ||\n\t     (fa.fsx_xflags & (FS_XFLAG_APPEND | FS_XFLAG_IMMUTABLE))) &&\n\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (fa.fsx_xflags & FS_XFLAG_SYNC)\n\t\tbinode->flags |= BTRFS_INODE_SYNC;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_SYNC;\n\tif (fa.fsx_xflags & FS_XFLAG_IMMUTABLE)\n\t\tbinode->flags |= BTRFS_INODE_IMMUTABLE;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_IMMUTABLE;\n\tif (fa.fsx_xflags & FS_XFLAG_APPEND)\n\t\tbinode->flags |= BTRFS_INODE_APPEND;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_APPEND;\n\tif (fa.fsx_xflags & FS_XFLAG_NODUMP)\n\t\tbinode->flags |= BTRFS_INODE_NODUMP;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_NODUMP;\n\tif (fa.fsx_xflags & FS_XFLAG_NOATIME)\n\t\tbinode->flags |= BTRFS_INODE_NOATIME;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_NOATIME;\n\n\t/* 1 item for the inode */\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_unlock;\n\t}\n\n\tbtrfs_sync_inode_flags_to_i_flags(inode);\n\tinode_inc_iversion(inode);\n\tinode->i_ctime = current_time(inode);\n\tret = btrfs_update_inode(trans, root, inode);\n\n\tbtrfs_end_transaction(trans);\n\nout_unlock:\n\tif (ret) {\n\t\tbinode->flags = old_flags;\n\t\tinode->i_flags = old_i_flags;\n\t}\n\n\tinode_unlock(inode);\n\tmnt_drop_write_file(file);\n\n\treturn ret;\n}\n\nstatic int btrfs_ioctl_getversion(struct file *file, int __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\n\treturn put_user(inode->i_generation, arg);\n}\n\nstatic noinline int btrfs_ioctl_fitrim(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_device *device;\n\tstruct request_queue *q;\n\tstruct fstrim_range range;\n\tu64 minlen = ULLONG_MAX;\n\tu64 num_devices = 0;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(device, &fs_info->fs_devices->devices,\n\t\t\t\tdev_list) {\n\t\tif (!device->bdev)\n\t\t\tcontinue;\n\t\tq = bdev_get_queue(device->bdev);\n\t\tif (blk_queue_discard(q)) {\n\t\t\tnum_devices++;\n\t\t\tminlen = min_t(u64, q->limits.discard_granularity,\n\t\t\t\t     minlen);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (!num_devices)\n\t\treturn -EOPNOTSUPP;\n\tif (copy_from_user(&range, arg, sizeof(range)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * NOTE: Don't truncate the range using super->total_bytes.  Bytenr of\n\t * block group is in the logical address space, which can be any\n\t * sectorsize aligned bytenr in  the range [0, U64_MAX].\n\t */\n\tif (range.len < fs_info->sb->s_blocksize)\n\t\treturn -EINVAL;\n\n\trange.minlen = max(range.minlen, minlen);\n\tret = btrfs_trim_fs(fs_info, &range);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (copy_to_user(arg, &range, sizeof(range)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nint btrfs_is_empty_uuid(u8 *uuid)\n{\n\tint i;\n\n\tfor (i = 0; i < BTRFS_UUID_SIZE; i++) {\n\t\tif (uuid[i])\n\t\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic noinline int create_subvol(struct inode *dir,\n\t\t\t\t  struct dentry *dentry,\n\t\t\t\t  const char *name, int namelen,\n\t\t\t\t  u64 *async_transid,\n\t\t\t\t  struct btrfs_qgroup_inherit *inherit)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_key key;\n\tstruct btrfs_root_item *root_item;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *new_root;\n\tstruct btrfs_block_rsv block_rsv;\n\tstruct timespec64 cur_time = current_time(dir);\n\tstruct inode *inode;\n\tint ret;\n\tint err;\n\tu64 objectid;\n\tu64 new_dirid = BTRFS_FIRST_FREE_OBJECTID;\n\tu64 index = 0;\n\tuuid_le new_uuid;\n\n\troot_item = kzalloc(sizeof(*root_item), GFP_KERNEL);\n\tif (!root_item)\n\t\treturn -ENOMEM;\n\n\tret = btrfs_find_free_objectid(fs_info->tree_root, &objectid);\n\tif (ret)\n\t\tgoto fail_free;\n\n\t/*\n\t * Don't create subvolume whose level is not zero. Or qgroup will be\n\t * screwed up since it assumes subvolume qgroup's level to be 0.\n\t */\n\tif (btrfs_qgroup_level(objectid)) {\n\t\tret = -ENOSPC;\n\t\tgoto fail_free;\n\t}\n\n\tbtrfs_init_block_rsv(&block_rsv, BTRFS_BLOCK_RSV_TEMP);\n\t/*\n\t * The same as the snapshot creation, please see the comment\n\t * of create_snapshot().\n\t */\n\tret = btrfs_subvolume_reserve_metadata(root, &block_rsv, 8, false);\n\tif (ret)\n\t\tgoto fail_free;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tbtrfs_subvolume_release_metadata(fs_info, &block_rsv);\n\t\tgoto fail_free;\n\t}\n\ttrans->block_rsv = &block_rsv;\n\ttrans->bytes_reserved = block_rsv.size;\n\n\tret = btrfs_qgroup_inherit(trans, 0, objectid, inherit);\n\tif (ret)\n\t\tgoto fail;\n\n\tleaf = btrfs_alloc_tree_block(trans, root, 0, objectid, NULL, 0, 0, 0);\n\tif (IS_ERR(leaf)) {\n\t\tret = PTR_ERR(leaf);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_mark_buffer_dirty(leaf);\n\n\tinode_item = &root_item->inode;\n\tbtrfs_set_stack_inode_generation(inode_item, 1);\n\tbtrfs_set_stack_inode_size(inode_item, 3);\n\tbtrfs_set_stack_inode_nlink(inode_item, 1);\n\tbtrfs_set_stack_inode_nbytes(inode_item,\n\t\t\t\t     fs_info->nodesize);\n\tbtrfs_set_stack_inode_mode(inode_item, S_IFDIR | 0755);\n\n\tbtrfs_set_root_flags(root_item, 0);\n\tbtrfs_set_root_limit(root_item, 0);\n\tbtrfs_set_stack_inode_flags(inode_item, BTRFS_INODE_ROOT_ITEM_INIT);\n\n\tbtrfs_set_root_bytenr(root_item, leaf->start);\n\tbtrfs_set_root_generation(root_item, trans->transid);\n\tbtrfs_set_root_level(root_item, 0);\n\tbtrfs_set_root_refs(root_item, 1);\n\tbtrfs_set_root_used(root_item, leaf->len);\n\tbtrfs_set_root_last_snapshot(root_item, 0);\n\n\tbtrfs_set_root_generation_v2(root_item,\n\t\t\tbtrfs_root_generation(root_item));\n\tuuid_le_gen(&new_uuid);\n\tmemcpy(root_item->uuid, new_uuid.b, BTRFS_UUID_SIZE);\n\tbtrfs_set_stack_timespec_sec(&root_item->otime, cur_time.tv_sec);\n\tbtrfs_set_stack_timespec_nsec(&root_item->otime, cur_time.tv_nsec);\n\troot_item->ctime = root_item->otime;\n\tbtrfs_set_root_ctransid(root_item, trans->transid);\n\tbtrfs_set_root_otransid(root_item, trans->transid);\n\n\tbtrfs_tree_unlock(leaf);\n\tfree_extent_buffer(leaf);\n\tleaf = NULL;\n\n\tbtrfs_set_root_dirid(root_item, new_dirid);\n\n\tkey.objectid = objectid;\n\tkey.offset = 0;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tret = btrfs_insert_root(trans, fs_info->tree_root, &key,\n\t\t\t\troot_item);\n\tif (ret)\n\t\tgoto fail;\n\n\tkey.offset = (u64)-1;\n\tnew_root = btrfs_read_fs_root_no_name(fs_info, &key);\n\tif (IS_ERR(new_root)) {\n\t\tret = PTR_ERR(new_root);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_record_root_in_trans(trans, new_root);\n\n\tret = btrfs_create_subvol_root(trans, new_root, root, new_dirid);\n\tif (ret) {\n\t\t/* We potentially lose an unused inode item here */\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto fail;\n\t}\n\n\tmutex_lock(&new_root->objectid_mutex);\n\tnew_root->highest_objectid = new_dirid;\n\tmutex_unlock(&new_root->objectid_mutex);\n\n\t/*\n\t * insert the directory item\n\t */\n\tret = btrfs_set_inode_index(BTRFS_I(dir), &index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_insert_dir_item(trans, name, namelen, BTRFS_I(dir), &key,\n\t\t\t\t    BTRFS_FT_DIR, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_i_size_write(BTRFS_I(dir), dir->i_size + namelen * 2);\n\tret = btrfs_update_inode(trans, root, dir);\n\tBUG_ON(ret);\n\n\tret = btrfs_add_root_ref(trans, objectid, root->root_key.objectid,\n\t\t\t\t btrfs_ino(BTRFS_I(dir)), index, name, namelen);\n\tBUG_ON(ret);\n\n\tret = btrfs_uuid_tree_add(trans, root_item->uuid,\n\t\t\t\t  BTRFS_UUID_KEY_SUBVOL, objectid);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, ret);\n\nfail:\n\tkfree(root_item);\n\ttrans->block_rsv = NULL;\n\ttrans->bytes_reserved = 0;\n\tbtrfs_subvolume_release_metadata(fs_info, &block_rsv);\n\n\tif (async_transid) {\n\t\t*async_transid = trans->transid;\n\t\terr = btrfs_commit_transaction_async(trans, 1);\n\t\tif (err)\n\t\t\terr = btrfs_commit_transaction(trans);\n\t} else {\n\t\terr = btrfs_commit_transaction(trans);\n\t}\n\tif (err && !ret)\n\t\tret = err;\n\n\tif (!ret) {\n\t\tinode = btrfs_lookup_dentry(dir, dentry);\n\t\tif (IS_ERR(inode))\n\t\t\treturn PTR_ERR(inode);\n\t\td_instantiate(dentry, inode);\n\t}\n\treturn ret;\n\nfail_free:\n\tkfree(root_item);\n\treturn ret;\n}\n\nstatic int create_snapshot(struct btrfs_root *root, struct inode *dir,\n\t\t\t   struct dentry *dentry,\n\t\t\t   u64 *async_transid, bool readonly,\n\t\t\t   struct btrfs_qgroup_inherit *inherit)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct inode *inode;\n\tstruct btrfs_pending_snapshot *pending_snapshot;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tbool snapshot_force_cow = false;\n\n\tif (!test_bit(BTRFS_ROOT_REF_COWS, &root->state))\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&root->nr_swapfiles)) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot snapshot subvolume with active swapfile\");\n\t\treturn -ETXTBSY;\n\t}\n\n\tpending_snapshot = kzalloc(sizeof(*pending_snapshot), GFP_KERNEL);\n\tif (!pending_snapshot)\n\t\treturn -ENOMEM;\n\n\tpending_snapshot->root_item = kzalloc(sizeof(struct btrfs_root_item),\n\t\t\tGFP_KERNEL);\n\tpending_snapshot->path = btrfs_alloc_path();\n\tif (!pending_snapshot->root_item || !pending_snapshot->path) {\n\t\tret = -ENOMEM;\n\t\tgoto free_pending;\n\t}\n\n\t/*\n\t * Force new buffered writes to reserve space even when NOCOW is\n\t * possible. This is to avoid later writeback (running dealloc) to\n\t * fallback to COW mode and unexpectedly fail with ENOSPC.\n\t */\n\tatomic_inc(&root->will_be_snapshotted);\n\tsmp_mb__after_atomic();\n\t/* wait for no snapshot writes */\n\twait_event(root->subv_writers->wait,\n\t\t   percpu_counter_sum(&root->subv_writers->counter) == 0);\n\n\tret = btrfs_start_delalloc_snapshot(root);\n\tif (ret)\n\t\tgoto dec_and_free;\n\n\t/*\n\t * All previous writes have started writeback in NOCOW mode, so now\n\t * we force future writes to fallback to COW mode during snapshot\n\t * creation.\n\t */\n\tatomic_inc(&root->snapshot_force_cow);\n\tsnapshot_force_cow = true;\n\n\tbtrfs_wait_ordered_extents(root, U64_MAX, 0, (u64)-1);\n\n\tbtrfs_init_block_rsv(&pending_snapshot->block_rsv,\n\t\t\t     BTRFS_BLOCK_RSV_TEMP);\n\t/*\n\t * 1 - parent dir inode\n\t * 2 - dir entries\n\t * 1 - root item\n\t * 2 - root ref/backref\n\t * 1 - root of snapshot\n\t * 1 - UUID item\n\t */\n\tret = btrfs_subvolume_reserve_metadata(BTRFS_I(dir)->root,\n\t\t\t\t\t&pending_snapshot->block_rsv, 8,\n\t\t\t\t\tfalse);\n\tif (ret)\n\t\tgoto dec_and_free;\n\n\tpending_snapshot->dentry = dentry;\n\tpending_snapshot->root = root;\n\tpending_snapshot->readonly = readonly;\n\tpending_snapshot->dir = dir;\n\tpending_snapshot->inherit = inherit;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto fail;\n\t}\n\n\tspin_lock(&fs_info->trans_lock);\n\tlist_add(&pending_snapshot->list,\n\t\t &trans->transaction->pending_snapshots);\n\tspin_unlock(&fs_info->trans_lock);\n\tif (async_transid) {\n\t\t*async_transid = trans->transid;\n\t\tret = btrfs_commit_transaction_async(trans, 1);\n\t\tif (ret)\n\t\t\tret = btrfs_commit_transaction(trans);\n\t} else {\n\t\tret = btrfs_commit_transaction(trans);\n\t}\n\tif (ret)\n\t\tgoto fail;\n\n\tret = pending_snapshot->error;\n\tif (ret)\n\t\tgoto fail;\n\n\tret = btrfs_orphan_cleanup(pending_snapshot->snap);\n\tif (ret)\n\t\tgoto fail;\n\n\tinode = btrfs_lookup_dentry(d_inode(dentry->d_parent), dentry);\n\tif (IS_ERR(inode)) {\n\t\tret = PTR_ERR(inode);\n\t\tgoto fail;\n\t}\n\n\td_instantiate(dentry, inode);\n\tret = 0;\nfail:\n\tbtrfs_subvolume_release_metadata(fs_info, &pending_snapshot->block_rsv);\ndec_and_free:\n\tif (snapshot_force_cow)\n\t\tatomic_dec(&root->snapshot_force_cow);\n\tif (atomic_dec_and_test(&root->will_be_snapshotted))\n\t\twake_up_var(&root->will_be_snapshotted);\nfree_pending:\n\tkfree(pending_snapshot->root_item);\n\tbtrfs_free_path(pending_snapshot->path);\n\tkfree(pending_snapshot);\n\n\treturn ret;\n}\n\n/*  copy of may_delete in fs/namei.c()\n *\tCheck whether we can remove a link victim from directory dir, check\n *  whether the type of victim is right.\n *  1. We can't do it if dir is read-only (done in permission())\n *  2. We should have write and exec permissions on dir\n *  3. We can't remove anything from append-only dir\n *  4. We can't do anything with immutable dir (done in permission())\n *  5. If the sticky bit on dir is set we should either\n *\ta. be owner of dir, or\n *\tb. be owner of victim, or\n *\tc. have CAP_FOWNER capability\n *  6. If the victim is append-only or immutable we can't do anything with\n *     links pointing to it.\n *  7. If we were asked to remove a directory and victim isn't one - ENOTDIR.\n *  8. If we were asked to remove a non-directory and victim isn't one - EISDIR.\n *  9. We can't remove a root or mountpoint.\n * 10. We don't allow removal of NFS sillyrenamed files; it's handled by\n *     nfs_async_unlink().\n */\n\nstatic int btrfs_may_delete(struct inode *dir, struct dentry *victim, int isdir)\n{\n\tint error;\n\n\tif (d_really_is_negative(victim))\n\t\treturn -ENOENT;\n\n\tBUG_ON(d_inode(victim->d_parent) != dir);\n\taudit_inode_child(dir, victim, AUDIT_TYPE_CHILD_DELETE);\n\n\terror = inode_permission(dir, MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\treturn error;\n\tif (IS_APPEND(dir))\n\t\treturn -EPERM;\n\tif (check_sticky(dir, d_inode(victim)) || IS_APPEND(d_inode(victim)) ||\n\t    IS_IMMUTABLE(d_inode(victim)) || IS_SWAPFILE(d_inode(victim)))\n\t\treturn -EPERM;\n\tif (isdir) {\n\t\tif (!d_is_dir(victim))\n\t\t\treturn -ENOTDIR;\n\t\tif (IS_ROOT(victim))\n\t\t\treturn -EBUSY;\n\t} else if (d_is_dir(victim))\n\t\treturn -EISDIR;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\tif (victim->d_flags & DCACHE_NFSFS_RENAMED)\n\t\treturn -EBUSY;\n\treturn 0;\n}\n\n/* copy of may_create in fs/namei.c() */\nstatic inline int btrfs_may_create(struct inode *dir, struct dentry *child)\n{\n\tif (d_really_is_positive(child))\n\t\treturn -EEXIST;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\treturn inode_permission(dir, MAY_WRITE | MAY_EXEC);\n}\n\n/*\n * Create a new subvolume below @parent.  This is largely modeled after\n * sys_mkdirat and vfs_mkdir, but we only do a single component lookup\n * inside this filesystem so it's quite a bit simpler.\n */\nstatic noinline int btrfs_mksubvol(const struct path *parent,\n\t\t\t\t   const char *name, int namelen,\n\t\t\t\t   struct btrfs_root *snap_src,\n\t\t\t\t   u64 *async_transid, bool readonly,\n\t\t\t\t   struct btrfs_qgroup_inherit *inherit)\n{\n\tstruct inode *dir = d_inode(parent->dentry);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct dentry *dentry;\n\tint error;\n\n\terror = down_write_killable_nested(&dir->i_rwsem, I_MUTEX_PARENT);\n\tif (error == -EINTR)\n\t\treturn error;\n\n\tdentry = lookup_one_len(name, parent->dentry, namelen);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto out_unlock;\n\n\terror = btrfs_may_create(dir, dentry);\n\tif (error)\n\t\tgoto out_dput;\n\n\t/*\n\t * even if this name doesn't exist, we may get hash collisions.\n\t * check for them now when we can safely fail\n\t */\n\terror = btrfs_check_dir_item_collision(BTRFS_I(dir)->root,\n\t\t\t\t\t       dir->i_ino, name,\n\t\t\t\t\t       namelen);\n\tif (error)\n\t\tgoto out_dput;\n\n\tdown_read(&fs_info->subvol_sem);\n\n\tif (btrfs_root_refs(&BTRFS_I(dir)->root->root_item) == 0)\n\t\tgoto out_up_read;\n\n\tif (snap_src) {\n\t\terror = create_snapshot(snap_src, dir, dentry,\n\t\t\t\t\tasync_transid, readonly, inherit);\n\t} else {\n\t\terror = create_subvol(dir, dentry, name, namelen,\n\t\t\t\t      async_transid, inherit);\n\t}\n\tif (!error)\n\t\tfsnotify_mkdir(dir, dentry);\nout_up_read:\n\tup_read(&fs_info->subvol_sem);\nout_dput:\n\tdput(dentry);\nout_unlock:\n\tinode_unlock(dir);\n\treturn error;\n}\n\n/*\n * When we're defragging a range, we don't want to kick it off again\n * if it is really just waiting for delalloc to send it down.\n * If we find a nice big extent or delalloc range for the bytes in the\n * file you want to defrag, we return 0 to let you know to skip this\n * part of the file\n */\nstatic int check_defrag_in_cache(struct inode *inode, u64 offset, u32 thresh)\n{\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tu64 end;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, offset, PAGE_SIZE);\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tend = extent_map_end(em);\n\t\tfree_extent_map(em);\n\t\tif (end - offset > thresh)\n\t\t\treturn 0;\n\t}\n\t/* if we already have a nice delalloc here, just stop */\n\tthresh /= 2;\n\tend = count_range_bits(io_tree, &offset, offset + thresh,\n\t\t\t       thresh, EXTENT_DELALLOC, 1);\n\tif (end >= thresh)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * helper function to walk through a file and find extents\n * newer than a specific transid, and smaller than thresh.\n *\n * This is used by the defragging code to find new and small\n * extents\n */\nstatic int find_new_extents(struct btrfs_root *root,\n\t\t\t    struct inode *inode, u64 newer_than,\n\t\t\t    u64 *off, u32 thresh)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_key min_key;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_file_extent_item *extent;\n\tint type;\n\tint ret;\n\tu64 ino = btrfs_ino(BTRFS_I(inode));\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmin_key.objectid = ino;\n\tmin_key.type = BTRFS_EXTENT_DATA_KEY;\n\tmin_key.offset = *off;\n\n\twhile (1) {\n\t\tret = btrfs_search_forward(root, &min_key, path, newer_than);\n\t\tif (ret != 0)\n\t\t\tgoto none;\nprocess_slot:\n\t\tif (min_key.objectid != ino)\n\t\t\tgoto none;\n\t\tif (min_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto none;\n\n\t\tleaf = path->nodes[0];\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_file_extent_item);\n\n\t\ttype = btrfs_file_extent_type(leaf, extent);\n\t\tif (type == BTRFS_FILE_EXTENT_REG &&\n\t\t    btrfs_file_extent_num_bytes(leaf, extent) < thresh &&\n\t\t    check_defrag_in_cache(inode, min_key.offset, thresh)) {\n\t\t\t*off = min_key.offset;\n\t\t\tbtrfs_free_path(path);\n\t\t\treturn 0;\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tif (path->slots[0] < btrfs_header_nritems(leaf)) {\n\t\t\tbtrfs_item_key_to_cpu(leaf, &min_key, path->slots[0]);\n\t\t\tgoto process_slot;\n\t\t}\n\n\t\tif (min_key.offset == (u64)-1)\n\t\t\tgoto none;\n\n\t\tmin_key.offset++;\n\t\tbtrfs_release_path(path);\n\t}\nnone:\n\tbtrfs_free_path(path);\n\treturn -ENOENT;\n}\n\nstatic struct extent_map *defrag_lookup_extent(struct inode *inode, u64 start)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em;\n\tu64 len = PAGE_SIZE;\n\n\t/*\n\t * hopefully we have this extent in the tree already, try without\n\t * the full extent lock\n\t */\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tstruct extent_state *cached = NULL;\n\t\tu64 end = start + len - 1;\n\n\t\t/* get the big lock and read metadata off disk */\n\t\tlock_extent_bits(io_tree, start, end, &cached);\n\t\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, start, len, 0);\n\t\tunlock_extent_cached(io_tree, start, end, &cached);\n\n\t\tif (IS_ERR(em))\n\t\t\treturn NULL;\n\t}\n\n\treturn em;\n}\n\nstatic bool defrag_check_next_extent(struct inode *inode, struct extent_map *em)\n{\n\tstruct extent_map *next;\n\tbool ret = true;\n\n\t/* this is the last extent */\n\tif (em->start + em->len >= i_size_read(inode))\n\t\treturn false;\n\n\tnext = defrag_lookup_extent(inode, em->start + em->len);\n\tif (!next || next->block_start >= EXTENT_MAP_LAST_BYTE)\n\t\tret = false;\n\telse if ((em->block_start + em->block_len == next->block_start) &&\n\t\t (em->block_len > SZ_128K && next->block_len > SZ_128K))\n\t\tret = false;\n\n\tfree_extent_map(next);\n\treturn ret;\n}\n\nstatic int should_defrag_range(struct inode *inode, u64 start, u32 thresh,\n\t\t\t       u64 *last_len, u64 *skip, u64 *defrag_end,\n\t\t\t       int compress)\n{\n\tstruct extent_map *em;\n\tint ret = 1;\n\tbool next_mergeable = true;\n\tbool prev_mergeable = true;\n\n\t/*\n\t * make sure that once we start defragging an extent, we keep on\n\t * defragging it\n\t */\n\tif (start < *defrag_end)\n\t\treturn 1;\n\n\t*skip = 0;\n\n\tem = defrag_lookup_extent(inode, start);\n\tif (!em)\n\t\treturn 0;\n\n\t/* this will cover holes, and inline extents */\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (!*defrag_end)\n\t\tprev_mergeable = false;\n\n\tnext_mergeable = defrag_check_next_extent(inode, em);\n\t/*\n\t * we hit a real extent, if it is big or the next extent is not a\n\t * real extent, don't bother defragging it\n\t */\n\tif (!compress && (*last_len == 0 || *last_len >= thresh) &&\n\t    (em->len >= thresh || (!next_mergeable && !prev_mergeable)))\n\t\tret = 0;\nout:\n\t/*\n\t * last_len ends up being a counter of how many bytes we've defragged.\n\t * every time we choose not to defrag an extent, we reset *last_len\n\t * so that the next tiny extent will force a defrag.\n\t *\n\t * The end result of this is that tiny extents before a single big\n\t * extent will force at least part of that big extent to be defragged.\n\t */\n\tif (ret) {\n\t\t*defrag_end = extent_map_end(em);\n\t} else {\n\t\t*last_len = 0;\n\t\t*skip = extent_map_end(em);\n\t\t*defrag_end = 0;\n\t}\n\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * it doesn't do much good to defrag one or two pages\n * at a time.  This pulls in a nice chunk of pages\n * to COW and defrag.\n *\n * It also makes sure the delalloc code has enough\n * dirty data to avoid making new small extents as part\n * of the defrag\n *\n * It's a good idea to start RA on this range\n * before calling this.\n */\nstatic int cluster_pages_for_defrag(struct inode *inode,\n\t\t\t\t    struct page **pages,\n\t\t\t\t    unsigned long start_index,\n\t\t\t\t    unsigned long num_pages)\n{\n\tunsigned long file_end;\n\tu64 isize = i_size_read(inode);\n\tu64 page_start;\n\tu64 page_end;\n\tu64 page_cnt;\n\tint ret;\n\tint i;\n\tint i_done;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tstruct extent_io_tree *tree;\n\tstruct extent_changeset *data_reserved = NULL;\n\tgfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);\n\n\tfile_end = (isize - 1) >> PAGE_SHIFT;\n\tif (!isize || start_index > file_end)\n\t\treturn 0;\n\n\tpage_cnt = min_t(u64, (u64)num_pages, (u64)file_end - start_index + 1);\n\n\tret = btrfs_delalloc_reserve_space(inode, &data_reserved,\n\t\t\tstart_index << PAGE_SHIFT,\n\t\t\tpage_cnt << PAGE_SHIFT);\n\tif (ret)\n\t\treturn ret;\n\ti_done = 0;\n\ttree = &BTRFS_I(inode)->io_tree;\n\n\t/* step one, lock all the pages */\n\tfor (i = 0; i < page_cnt; i++) {\n\t\tstruct page *page;\nagain:\n\t\tpage = find_or_create_page(inode->i_mapping,\n\t\t\t\t\t   start_index + i, mask);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tpage_start = page_offset(page);\n\t\tpage_end = page_start + PAGE_SIZE - 1;\n\t\twhile (1) {\n\t\t\tlock_extent_bits(tree, page_start, page_end,\n\t\t\t\t\t &cached_state);\n\t\t\tordered = btrfs_lookup_ordered_extent(inode,\n\t\t\t\t\t\t\t      page_start);\n\t\t\tunlock_extent_cached(tree, page_start, page_end,\n\t\t\t\t\t     &cached_state);\n\t\t\tif (!ordered)\n\t\t\t\tbreak;\n\n\t\t\tunlock_page(page);\n\t\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t\tlock_page(page);\n\t\t\t/*\n\t\t\t * we unlocked the page above, so we need check if\n\t\t\t * it was released or not.\n\t\t\t */\n\t\t\tif (page->mapping != inode->i_mapping) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\n\t\tif (!PageUptodate(page)) {\n\t\t\tbtrfs_readpage(NULL, page);\n\t\t\tlock_page(page);\n\t\t\tif (!PageUptodate(page)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page);\n\t\t\t\tret = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (page->mapping != inode->i_mapping) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto again;\n\t\t}\n\n\t\tpages[i] = page;\n\t\ti_done++;\n\t}\n\tif (!i_done || ret)\n\t\tgoto out;\n\n\tif (!(inode->i_sb->s_flags & SB_ACTIVE))\n\t\tgoto out;\n\n\t/*\n\t * so now we have a nice long stream of locked\n\t * and up to date pages, lets wait on them\n\t */\n\tfor (i = 0; i < i_done; i++)\n\t\twait_on_page_writeback(pages[i]);\n\n\tpage_start = page_offset(pages[0]);\n\tpage_end = page_offset(pages[i_done - 1]) + PAGE_SIZE;\n\n\tlock_extent_bits(&BTRFS_I(inode)->io_tree,\n\t\t\t page_start, page_end - 1, &cached_state);\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, page_start,\n\t\t\t  page_end - 1, EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0,\n\t\t\t  &cached_state);\n\n\tif (i_done != page_cnt) {\n\t\tspin_lock(&BTRFS_I(inode)->lock);\n\t\tbtrfs_mod_outstanding_extents(BTRFS_I(inode), 1);\n\t\tspin_unlock(&BTRFS_I(inode)->lock);\n\t\tbtrfs_delalloc_release_space(inode, data_reserved,\n\t\t\t\tstart_index << PAGE_SHIFT,\n\t\t\t\t(page_cnt - i_done) << PAGE_SHIFT, true);\n\t}\n\n\n\tset_extent_defrag(&BTRFS_I(inode)->io_tree, page_start, page_end - 1,\n\t\t\t  &cached_state);\n\n\tunlock_extent_cached(&BTRFS_I(inode)->io_tree,\n\t\t\t     page_start, page_end - 1, &cached_state);\n\n\tfor (i = 0; i < i_done; i++) {\n\t\tclear_page_dirty_for_io(pages[i]);\n\t\tClearPageChecked(pages[i]);\n\t\tset_page_extent_mapped(pages[i]);\n\t\tset_page_dirty(pages[i]);\n\t\tunlock_page(pages[i]);\n\t\tput_page(pages[i]);\n\t}\n\tbtrfs_delalloc_release_extents(BTRFS_I(inode), page_cnt << PAGE_SHIFT,\n\t\t\t\t       false);\n\textent_changeset_free(data_reserved);\n\treturn i_done;\nout:\n\tfor (i = 0; i < i_done; i++) {\n\t\tunlock_page(pages[i]);\n\t\tput_page(pages[i]);\n\t}\n\tbtrfs_delalloc_release_space(inode, data_reserved,\n\t\t\tstart_index << PAGE_SHIFT,\n\t\t\tpage_cnt << PAGE_SHIFT, true);\n\tbtrfs_delalloc_release_extents(BTRFS_I(inode), page_cnt << PAGE_SHIFT,\n\t\t\t\t       true);\n\textent_changeset_free(data_reserved);\n\treturn ret;\n\n}\n\nint btrfs_defrag_file(struct inode *inode, struct file *file,\n\t\t      struct btrfs_ioctl_defrag_range_args *range,\n\t\t      u64 newer_than, unsigned long max_to_defrag)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct file_ra_state *ra = NULL;\n\tunsigned long last_index;\n\tu64 isize = i_size_read(inode);\n\tu64 last_len = 0;\n\tu64 skip = 0;\n\tu64 defrag_end = 0;\n\tu64 newer_off = range->start;\n\tunsigned long i;\n\tunsigned long ra_index = 0;\n\tint ret;\n\tint defrag_count = 0;\n\tint compress_type = BTRFS_COMPRESS_ZLIB;\n\tu32 extent_thresh = range->extent_thresh;\n\tunsigned long max_cluster = SZ_256K >> PAGE_SHIFT;\n\tunsigned long cluster = max_cluster;\n\tu64 new_align = ~((u64)SZ_128K - 1);\n\tstruct page **pages = NULL;\n\tbool do_compress = range->flags & BTRFS_DEFRAG_RANGE_COMPRESS;\n\n\tif (isize == 0)\n\t\treturn 0;\n\n\tif (range->start >= isize)\n\t\treturn -EINVAL;\n\n\tif (do_compress) {\n\t\tif (range->compress_type > BTRFS_COMPRESS_TYPES)\n\t\t\treturn -EINVAL;\n\t\tif (range->compress_type)\n\t\t\tcompress_type = range->compress_type;\n\t}\n\n\tif (extent_thresh == 0)\n\t\textent_thresh = SZ_256K;\n\n\t/*\n\t * If we were not given a file, allocate a readahead context. As\n\t * readahead is just an optimization, defrag will work without it so\n\t * we don't error out.\n\t */\n\tif (!file) {\n\t\tra = kzalloc(sizeof(*ra), GFP_KERNEL);\n\t\tif (ra)\n\t\t\tfile_ra_state_init(ra, inode->i_mapping);\n\t} else {\n\t\tra = &file->f_ra;\n\t}\n\n\tpages = kmalloc_array(max_cluster, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out_ra;\n\t}\n\n\t/* find the last page to defrag */\n\tif (range->start + range->len > range->start) {\n\t\tlast_index = min_t(u64, isize - 1,\n\t\t\t range->start + range->len - 1) >> PAGE_SHIFT;\n\t} else {\n\t\tlast_index = (isize - 1) >> PAGE_SHIFT;\n\t}\n\n\tif (newer_than) {\n\t\tret = find_new_extents(root, inode, newer_than,\n\t\t\t\t       &newer_off, SZ_64K);\n\t\tif (!ret) {\n\t\t\trange->start = newer_off;\n\t\t\t/*\n\t\t\t * we always align our defrag to help keep\n\t\t\t * the extents in the file evenly spaced\n\t\t\t */\n\t\t\ti = (newer_off & new_align) >> PAGE_SHIFT;\n\t\t} else\n\t\t\tgoto out_ra;\n\t} else {\n\t\ti = range->start >> PAGE_SHIFT;\n\t}\n\tif (!max_to_defrag)\n\t\tmax_to_defrag = last_index - i + 1;\n\n\t/*\n\t * make writeback starts from i, so the defrag range can be\n\t * written sequentially.\n\t */\n\tif (i < inode->i_mapping->writeback_index)\n\t\tinode->i_mapping->writeback_index = i;\n\n\twhile (i <= last_index && defrag_count < max_to_defrag &&\n\t       (i < DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE))) {\n\t\t/*\n\t\t * make sure we stop running if someone unmounts\n\t\t * the FS\n\t\t */\n\t\tif (!(inode->i_sb->s_flags & SB_ACTIVE))\n\t\t\tbreak;\n\n\t\tif (btrfs_defrag_cancelled(fs_info)) {\n\t\t\tbtrfs_debug(fs_info, \"defrag_file cancelled\");\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!should_defrag_range(inode, (u64)i << PAGE_SHIFT,\n\t\t\t\t\t extent_thresh, &last_len, &skip,\n\t\t\t\t\t &defrag_end, do_compress)){\n\t\t\tunsigned long next;\n\t\t\t/*\n\t\t\t * the should_defrag function tells us how much to skip\n\t\t\t * bump our counter by the suggested amount\n\t\t\t */\n\t\t\tnext = DIV_ROUND_UP(skip, PAGE_SIZE);\n\t\t\ti = max(i + 1, next);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!newer_than) {\n\t\t\tcluster = (PAGE_ALIGN(defrag_end) >>\n\t\t\t\t   PAGE_SHIFT) - i;\n\t\t\tcluster = min(cluster, max_cluster);\n\t\t} else {\n\t\t\tcluster = max_cluster;\n\t\t}\n\n\t\tif (i + cluster > ra_index) {\n\t\t\tra_index = max(i, ra_index);\n\t\t\tif (ra)\n\t\t\t\tpage_cache_sync_readahead(inode->i_mapping, ra,\n\t\t\t\t\t\tfile, ra_index, cluster);\n\t\t\tra_index += cluster;\n\t\t}\n\n\t\tinode_lock(inode);\n\t\tif (IS_SWAPFILE(inode)) {\n\t\t\tret = -ETXTBSY;\n\t\t} else {\n\t\t\tif (do_compress)\n\t\t\t\tBTRFS_I(inode)->defrag_compress = compress_type;\n\t\t\tret = cluster_pages_for_defrag(inode, pages, i, cluster);\n\t\t}\n\t\tif (ret < 0) {\n\t\t\tinode_unlock(inode);\n\t\t\tgoto out_ra;\n\t\t}\n\n\t\tdefrag_count += ret;\n\t\tbalance_dirty_pages_ratelimited(inode->i_mapping);\n\t\tinode_unlock(inode);\n\n\t\tif (newer_than) {\n\t\t\tif (newer_off == (u64)-1)\n\t\t\t\tbreak;\n\n\t\t\tif (ret > 0)\n\t\t\t\ti += ret;\n\n\t\t\tnewer_off = max(newer_off + 1,\n\t\t\t\t\t(u64)i << PAGE_SHIFT);\n\n\t\t\tret = find_new_extents(root, inode, newer_than,\n\t\t\t\t\t       &newer_off, SZ_64K);\n\t\t\tif (!ret) {\n\t\t\t\trange->start = newer_off;\n\t\t\t\ti = (newer_off & new_align) >> PAGE_SHIFT;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (ret > 0) {\n\t\t\t\ti += ret;\n\t\t\t\tlast_len += ret << PAGE_SHIFT;\n\t\t\t} else {\n\t\t\t\ti++;\n\t\t\t\tlast_len = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((range->flags & BTRFS_DEFRAG_RANGE_START_IO)) {\n\t\tfilemap_flush(inode->i_mapping);\n\t\tif (test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,\n\t\t\t     &BTRFS_I(inode)->runtime_flags))\n\t\t\tfilemap_flush(inode->i_mapping);\n\t}\n\n\tif (range->compress_type == BTRFS_COMPRESS_LZO) {\n\t\tbtrfs_set_fs_incompat(fs_info, COMPRESS_LZO);\n\t} else if (range->compress_type == BTRFS_COMPRESS_ZSTD) {\n\t\tbtrfs_set_fs_incompat(fs_info, COMPRESS_ZSTD);\n\t}\n\n\tret = defrag_count;\n\nout_ra:\n\tif (do_compress) {\n\t\tinode_lock(inode);\n\t\tBTRFS_I(inode)->defrag_compress = BTRFS_COMPRESS_NONE;\n\t\tinode_unlock(inode);\n\t}\n\tif (!file)\n\t\tkfree(ra);\n\tkfree(pages);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_resize(struct file *file,\n\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tu64 new_size;\n\tu64 old_size;\n\tu64 devid = 1;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device = NULL;\n\tchar *sizestr;\n\tchar *retptr;\n\tchar *devstr = NULL;\n\tint ret = 0;\n\tint mod = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tmnt_drop_write_file(file);\n\t\treturn BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t}\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tsizestr = vol_args->name;\n\tdevstr = strchr(sizestr, ':');\n\tif (devstr) {\n\t\tsizestr = devstr + 1;\n\t\t*devstr = '\\0';\n\t\tdevstr = vol_args->name;\n\t\tret = kstrtoull(devstr, 10, &devid);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\tif (!devid) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tbtrfs_info(fs_info, \"resizing devid %llu\", devid);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!device) {\n\t\tbtrfs_info(fs_info, \"resizer unable to find device %llu\",\n\t\t\t   devid);\n\t\tret = -ENODEV;\n\t\tgoto out_free;\n\t}\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"resizer unable to apply on readonly device %llu\",\n\t\t       devid);\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\tif (!strcmp(sizestr, \"max\"))\n\t\tnew_size = device->bdev->bd_inode->i_size;\n\telse {\n\t\tif (sizestr[0] == '-') {\n\t\t\tmod = -1;\n\t\t\tsizestr++;\n\t\t} else if (sizestr[0] == '+') {\n\t\t\tmod = 1;\n\t\t\tsizestr++;\n\t\t}\n\t\tnew_size = memparse(sizestr, &retptr);\n\t\tif (*retptr != '\\0' || new_size == 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\told_size = btrfs_device_get_total_bytes(device);\n\n\tif (mod < 0) {\n\t\tif (new_size > old_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size - new_size;\n\t} else if (mod > 0) {\n\t\tif (new_size > ULLONG_MAX - old_size) {\n\t\t\tret = -ERANGE;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size + new_size;\n\t}\n\n\tif (new_size < SZ_256M) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (new_size > device->bdev->bd_inode->i_size) {\n\t\tret = -EFBIG;\n\t\tgoto out_free;\n\t}\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tbtrfs_info_in_rcu(fs_info, \"new size for %s is %llu\",\n\t\t\t  rcu_str_deref(device->name), new_size);\n\n\tif (new_size > old_size) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = btrfs_grow_device(trans, device, new_size);\n\t\tbtrfs_commit_transaction(trans);\n\t} else if (new_size < old_size) {\n\t\tret = btrfs_shrink_device(device, new_size);\n\t} /* equal, nothing need to do */\n\nout_free:\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create_transid(struct file *file,\n\t\t\t\tconst char *name, unsigned long fd, int subvol,\n\t\t\t\tu64 *transid, bool readonly,\n\t\t\t\tstruct btrfs_qgroup_inherit *inherit)\n{\n\tint namelen;\n\tint ret = 0;\n\n\tif (!S_ISDIR(file_inode(file)->i_mode))\n\t\treturn -ENOTDIR;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tnamelen = strlen(name);\n\tif (strchr(name, '/')) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (name[0] == '.' &&\n\t   (namelen == 1 || (name[1] == '.' && namelen == 2))) {\n\t\tret = -EEXIST;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (subvol) {\n\t\tret = btrfs_mksubvol(&file->f_path, name, namelen,\n\t\t\t\t     NULL, transid, readonly, inherit);\n\t} else {\n\t\tstruct fd src = fdget(fd);\n\t\tstruct inode *src_inode;\n\t\tif (!src.file) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_drop_write;\n\t\t}\n\n\t\tsrc_inode = file_inode(src.file);\n\t\tif (src_inode->i_sb != file_inode(file)->i_sb) {\n\t\t\tbtrfs_info(BTRFS_I(file_inode(file))->root->fs_info,\n\t\t\t\t   \"Snapshot src from another FS\");\n\t\t\tret = -EXDEV;\n\t\t} else if (!inode_owner_or_capable(src_inode)) {\n\t\t\t/*\n\t\t\t * Subvolume creation is not restricted, but snapshots\n\t\t\t * are limited to own subvolumes only\n\t\t\t */\n\t\t\tret = -EPERM;\n\t\t} else {\n\t\t\tret = btrfs_mksubvol(&file->f_path, name, namelen,\n\t\t\t\t\t     BTRFS_I(src_inode)->root,\n\t\t\t\t\t     transid, readonly, inherit);\n\t\t}\n\t\tfdput(src);\n\t}\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create(struct file *file,\n\t\t\t\t\t    void __user *arg, int subvol)\n{\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!S_ISDIR(file_inode(file)->i_mode))\n\t\treturn -ENOTDIR;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tret = btrfs_ioctl_snap_create_transid(file, vol_args->name,\n\t\t\t\t\t      vol_args->fd, subvol,\n\t\t\t\t\t      NULL, false, NULL);\n\n\tkfree(vol_args);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create_v2(struct file *file,\n\t\t\t\t\t       void __user *arg, int subvol)\n{\n\tstruct btrfs_ioctl_vol_args_v2 *vol_args;\n\tint ret;\n\tu64 transid = 0;\n\tu64 *ptr = NULL;\n\tbool readonly = false;\n\tstruct btrfs_qgroup_inherit *inherit = NULL;\n\n\tif (!S_ISDIR(file_inode(file)->i_mode))\n\t\treturn -ENOTDIR;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\tvol_args->name[BTRFS_SUBVOL_NAME_MAX] = '\\0';\n\n\tif (vol_args->flags &\n\t    ~(BTRFS_SUBVOL_CREATE_ASYNC | BTRFS_SUBVOL_RDONLY |\n\t      BTRFS_SUBVOL_QGROUP_INHERIT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto free_args;\n\t}\n\n\tif (vol_args->flags & BTRFS_SUBVOL_CREATE_ASYNC)\n\t\tptr = &transid;\n\tif (vol_args->flags & BTRFS_SUBVOL_RDONLY)\n\t\treadonly = true;\n\tif (vol_args->flags & BTRFS_SUBVOL_QGROUP_INHERIT) {\n\t\tif (vol_args->size > PAGE_SIZE) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_args;\n\t\t}\n\t\tinherit = memdup_user(vol_args->qgroup_inherit, vol_args->size);\n\t\tif (IS_ERR(inherit)) {\n\t\t\tret = PTR_ERR(inherit);\n\t\t\tgoto free_args;\n\t\t}\n\t}\n\n\tret = btrfs_ioctl_snap_create_transid(file, vol_args->name,\n\t\t\t\t\t      vol_args->fd, subvol, ptr,\n\t\t\t\t\t      readonly, inherit);\n\tif (ret)\n\t\tgoto free_inherit;\n\n\tif (ptr && copy_to_user(arg +\n\t\t\t\toffsetof(struct btrfs_ioctl_vol_args_v2,\n\t\t\t\t\ttransid),\n\t\t\t\tptr, sizeof(*ptr)))\n\t\tret = -EFAULT;\n\nfree_inherit:\n\tkfree(inherit);\nfree_args:\n\tkfree(vol_args);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_subvol_getflags(struct file *file,\n\t\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret = 0;\n\tu64 flags = 0;\n\n\tif (btrfs_ino(BTRFS_I(inode)) != BTRFS_FIRST_FREE_OBJECTID)\n\t\treturn -EINVAL;\n\n\tdown_read(&fs_info->subvol_sem);\n\tif (btrfs_root_readonly(root))\n\t\tflags |= BTRFS_SUBVOL_RDONLY;\n\tup_read(&fs_info->subvol_sem);\n\n\tif (copy_to_user(arg, &flags, sizeof(flags)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_subvol_setflags(struct file *file,\n\t\t\t\t\t      void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tu64 root_flags;\n\tu64 flags;\n\tint ret = 0;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tif (btrfs_ino(BTRFS_I(inode)) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (copy_from_user(&flags, arg, sizeof(flags))) {\n\t\tret = -EFAULT;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (flags & BTRFS_SUBVOL_CREATE_ASYNC) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (flags & ~BTRFS_SUBVOL_RDONLY) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_drop_write;\n\t}\n\n\tdown_write(&fs_info->subvol_sem);\n\n\t/* nothing to do */\n\tif (!!(flags & BTRFS_SUBVOL_RDONLY) == btrfs_root_readonly(root))\n\t\tgoto out_drop_sem;\n\n\troot_flags = btrfs_root_flags(&root->root_item);\n\tif (flags & BTRFS_SUBVOL_RDONLY) {\n\t\tbtrfs_set_root_flags(&root->root_item,\n\t\t\t\t     root_flags | BTRFS_ROOT_SUBVOL_RDONLY);\n\t} else {\n\t\t/*\n\t\t * Block RO -> RW transition if this subvolume is involved in\n\t\t * send\n\t\t */\n\t\tspin_lock(&root->root_item_lock);\n\t\tif (root->send_in_progress == 0) {\n\t\t\tbtrfs_set_root_flags(&root->root_item,\n\t\t\t\t     root_flags & ~BTRFS_ROOT_SUBVOL_RDONLY);\n\t\t\tspin_unlock(&root->root_item_lock);\n\t\t} else {\n\t\t\tspin_unlock(&root->root_item_lock);\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"Attempt to set subvolume %llu read-write during send\",\n\t\t\t\t   root->root_key.objectid);\n\t\t\tret = -EPERM;\n\t\t\tgoto out_drop_sem;\n\t\t}\n\t}\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_reset;\n\t}\n\n\tret = btrfs_update_root(trans, fs_info->tree_root,\n\t\t\t\t&root->root_key, &root->root_item);\n\tif (ret < 0) {\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out_reset;\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\nout_reset:\n\tif (ret)\n\t\tbtrfs_set_root_flags(&root->root_item, root_flags);\nout_drop_sem:\n\tup_write(&fs_info->subvol_sem);\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\nstatic noinline int key_in_sk(struct btrfs_key *key,\n\t\t\t      struct btrfs_ioctl_search_key *sk)\n{\n\tstruct btrfs_key test;\n\tint ret;\n\n\ttest.objectid = sk->min_objectid;\n\ttest.type = sk->min_type;\n\ttest.offset = sk->min_offset;\n\n\tret = btrfs_comp_cpu_keys(key, &test);\n\tif (ret < 0)\n\t\treturn 0;\n\n\ttest.objectid = sk->max_objectid;\n\ttest.type = sk->max_type;\n\ttest.offset = sk->max_offset;\n\n\tret = btrfs_comp_cpu_keys(key, &test);\n\tif (ret > 0)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic noinline int copy_to_sk(struct btrfs_path *path,\n\t\t\t       struct btrfs_key *key,\n\t\t\t       struct btrfs_ioctl_search_key *sk,\n\t\t\t       size_t *buf_size,\n\t\t\t       char __user *ubuf,\n\t\t\t       unsigned long *sk_offset,\n\t\t\t       int *num_found)\n{\n\tu64 found_transid;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_ioctl_search_header sh;\n\tstruct btrfs_key test;\n\tunsigned long item_off;\n\tunsigned long item_len;\n\tint nritems;\n\tint i;\n\tint slot;\n\tint ret = 0;\n\n\tleaf = path->nodes[0];\n\tslot = path->slots[0];\n\tnritems = btrfs_header_nritems(leaf);\n\n\tif (btrfs_header_generation(leaf) > sk->max_transid) {\n\t\ti = nritems;\n\t\tgoto advance_key;\n\t}\n\tfound_transid = btrfs_header_generation(leaf);\n\n\tfor (i = slot; i < nritems; i++) {\n\t\titem_off = btrfs_item_ptr_offset(leaf, i);\n\t\titem_len = btrfs_item_size_nr(leaf, i);\n\n\t\tbtrfs_item_key_to_cpu(leaf, key, i);\n\t\tif (!key_in_sk(key, sk))\n\t\t\tcontinue;\n\n\t\tif (sizeof(sh) + item_len > *buf_size) {\n\t\t\tif (*num_found) {\n\t\t\t\tret = 1;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * return one empty item back for v1, which does not\n\t\t\t * handle -EOVERFLOW\n\t\t\t */\n\n\t\t\t*buf_size = sizeof(sh) + item_len;\n\t\t\titem_len = 0;\n\t\t\tret = -EOVERFLOW;\n\t\t}\n\n\t\tif (sizeof(sh) + item_len + *sk_offset > *buf_size) {\n\t\t\tret = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsh.objectid = key->objectid;\n\t\tsh.offset = key->offset;\n\t\tsh.type = key->type;\n\t\tsh.len = item_len;\n\t\tsh.transid = found_transid;\n\n\t\t/* copy search result header */\n\t\tif (copy_to_user(ubuf + *sk_offset, &sh, sizeof(sh))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\t*sk_offset += sizeof(sh);\n\n\t\tif (item_len) {\n\t\t\tchar __user *up = ubuf + *sk_offset;\n\t\t\t/* copy the item */\n\t\t\tif (read_extent_buffer_to_user(leaf, up,\n\t\t\t\t\t\t       item_off, item_len)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t*sk_offset += item_len;\n\t\t}\n\t\t(*num_found)++;\n\n\t\tif (ret) /* -EOVERFLOW from above */\n\t\t\tgoto out;\n\n\t\tif (*num_found >= sk->nr_items) {\n\t\t\tret = 1;\n\t\t\tgoto out;\n\t\t}\n\t}\nadvance_key:\n\tret = 0;\n\ttest.objectid = sk->max_objectid;\n\ttest.type = sk->max_type;\n\ttest.offset = sk->max_offset;\n\tif (btrfs_comp_cpu_keys(key, &test) >= 0)\n\t\tret = 1;\n\telse if (key->offset < (u64)-1)\n\t\tkey->offset++;\n\telse if (key->type < (u8)-1) {\n\t\tkey->offset = 0;\n\t\tkey->type++;\n\t} else if (key->objectid < (u64)-1) {\n\t\tkey->offset = 0;\n\t\tkey->type = 0;\n\t\tkey->objectid++;\n\t} else\n\t\tret = 1;\nout:\n\t/*\n\t *  0: all items from this leaf copied, continue with next\n\t *  1: * more items can be copied, but unused buffer is too small\n\t *     * all items were found\n\t *     Either way, it will stops the loop which iterates to the next\n\t *     leaf\n\t *  -EOVERFLOW: item was to large for buffer\n\t *  -EFAULT: could not copy extent buffer back to userspace\n\t */\n\treturn ret;\n}\n\nstatic noinline int search_ioctl(struct inode *inode,\n\t\t\t\t struct btrfs_ioctl_search_key *sk,\n\t\t\t\t size_t *buf_size,\n\t\t\t\t char __user *ubuf)\n{\n\tstruct btrfs_fs_info *info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path;\n\tint ret;\n\tint num_found = 0;\n\tunsigned long sk_offset = 0;\n\n\tif (*buf_size < sizeof(struct btrfs_ioctl_search_header)) {\n\t\t*buf_size = sizeof(struct btrfs_ioctl_search_header);\n\t\treturn -EOVERFLOW;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tif (sk->tree_id == 0) {\n\t\t/* search the root of the inode that was passed */\n\t\troot = BTRFS_I(inode)->root;\n\t} else {\n\t\tkey.objectid = sk->tree_id;\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\t\troot = btrfs_read_fs_root_no_name(info, &key);\n\t\tif (IS_ERR(root)) {\n\t\t\tbtrfs_free_path(path);\n\t\t\treturn PTR_ERR(root);\n\t\t}\n\t}\n\n\tkey.objectid = sk->min_objectid;\n\tkey.type = sk->min_type;\n\tkey.offset = sk->min_offset;\n\n\twhile (1) {\n\t\tret = btrfs_search_forward(root, &key, path, sk->min_transid);\n\t\tif (ret != 0) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = 0;\n\t\t\tgoto err;\n\t\t}\n\t\tret = copy_to_sk(path, &key, sk, buf_size, ubuf,\n\t\t\t\t &sk_offset, &num_found);\n\t\tbtrfs_release_path(path);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t}\n\tif (ret > 0)\n\t\tret = 0;\nerr:\n\tsk->nr_items = num_found;\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_tree_search(struct file *file,\n\t\t\t\t\t   void __user *argp)\n{\n\tstruct btrfs_ioctl_search_args __user *uargs;\n\tstruct btrfs_ioctl_search_key sk;\n\tstruct inode *inode;\n\tint ret;\n\tsize_t buf_size;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tuargs = (struct btrfs_ioctl_search_args __user *)argp;\n\n\tif (copy_from_user(&sk, &uargs->key, sizeof(sk)))\n\t\treturn -EFAULT;\n\n\tbuf_size = sizeof(uargs->buf);\n\n\tinode = file_inode(file);\n\tret = search_ioctl(inode, &sk, &buf_size, uargs->buf);\n\n\t/*\n\t * In the origin implementation an overflow is handled by returning a\n\t * search header with a len of zero, so reset ret.\n\t */\n\tif (ret == -EOVERFLOW)\n\t\tret = 0;\n\n\tif (ret == 0 && copy_to_user(&uargs->key, &sk, sizeof(sk)))\n\t\tret = -EFAULT;\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_tree_search_v2(struct file *file,\n\t\t\t\t\t       void __user *argp)\n{\n\tstruct btrfs_ioctl_search_args_v2 __user *uarg;\n\tstruct btrfs_ioctl_search_args_v2 args;\n\tstruct inode *inode;\n\tint ret;\n\tsize_t buf_size;\n\tconst size_t buf_limit = SZ_16M;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* copy search header and buffer size */\n\tuarg = (struct btrfs_ioctl_search_args_v2 __user *)argp;\n\tif (copy_from_user(&args, uarg, sizeof(args)))\n\t\treturn -EFAULT;\n\n\tbuf_size = args.buf_size;\n\n\t/* limit result size to 16MB */\n\tif (buf_size > buf_limit)\n\t\tbuf_size = buf_limit;\n\n\tinode = file_inode(file);\n\tret = search_ioctl(inode, &args.key, &buf_size,\n\t\t\t   (char __user *)(&uarg->buf[0]));\n\tif (ret == 0 && copy_to_user(&uarg->key, &args.key, sizeof(args.key)))\n\t\tret = -EFAULT;\n\telse if (ret == -EOVERFLOW &&\n\t\tcopy_to_user(&uarg->buf_size, &buf_size, sizeof(buf_size)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\n/*\n * Search INODE_REFs to identify path name of 'dirid' directory\n * in a 'tree_id' tree. and sets path name to 'name'.\n */\nstatic noinline int btrfs_search_path_in_tree(struct btrfs_fs_info *info,\n\t\t\t\tu64 tree_id, u64 dirid, char *name)\n{\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tchar *ptr;\n\tint ret = -1;\n\tint slot;\n\tint len;\n\tint total_len = 0;\n\tstruct btrfs_inode_ref *iref;\n\tstruct extent_buffer *l;\n\tstruct btrfs_path *path;\n\n\tif (dirid == BTRFS_FIRST_FREE_OBJECTID) {\n\t\tname[0]='\\0';\n\t\treturn 0;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tptr = &name[BTRFS_INO_LOOKUP_PATH_MAX - 1];\n\n\tkey.objectid = tree_id;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\troot = btrfs_read_fs_root_no_name(info, &key);\n\tif (IS_ERR(root)) {\n\t\tret = PTR_ERR(root);\n\t\tgoto out;\n\t}\n\n\tkey.objectid = dirid;\n\tkey.type = BTRFS_INODE_REF_KEY;\n\tkey.offset = (u64)-1;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\telse if (ret > 0) {\n\t\t\tret = btrfs_previous_item(root, path, dirid,\n\t\t\t\t\t\t  BTRFS_INODE_REF_KEY);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\telse if (ret > 0) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\tiref = btrfs_item_ptr(l, slot, struct btrfs_inode_ref);\n\t\tlen = btrfs_inode_ref_name_len(l, iref);\n\t\tptr -= len + 1;\n\t\ttotal_len += len + 1;\n\t\tif (ptr < name) {\n\t\t\tret = -ENAMETOOLONG;\n\t\t\tgoto out;\n\t\t}\n\n\t\t*(ptr + len) = '/';\n\t\tread_extent_buffer(l, ptr, (unsigned long)(iref + 1), len);\n\n\t\tif (key.offset == BTRFS_FIRST_FREE_OBJECTID)\n\t\t\tbreak;\n\n\t\tbtrfs_release_path(path);\n\t\tkey.objectid = key.offset;\n\t\tkey.offset = (u64)-1;\n\t\tdirid = key.objectid;\n\t}\n\tmemmove(name, ptr, total_len);\n\tname[total_len] = '\\0';\n\tret = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_search_path_in_tree_user(struct inode *inode,\n\t\t\t\tstruct btrfs_ioctl_ino_lookup_user_args *args)\n{\n\tstruct btrfs_fs_info *fs_info = BTRFS_I(inode)->root->fs_info;\n\tstruct super_block *sb = inode->i_sb;\n\tstruct btrfs_key upper_limit = BTRFS_I(inode)->location;\n\tu64 treeid = BTRFS_I(inode)->root->root_key.objectid;\n\tu64 dirid = args->dirid;\n\tunsigned long item_off;\n\tunsigned long item_len;\n\tstruct btrfs_inode_ref *iref;\n\tstruct btrfs_root_ref *rref;\n\tstruct btrfs_root *root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key, key2;\n\tstruct extent_buffer *leaf;\n\tstruct inode *temp_inode;\n\tchar *ptr;\n\tint slot;\n\tint len;\n\tint total_len = 0;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * If the bottom subvolume does not exist directly under upper_limit,\n\t * construct the path in from the bottom up.\n\t */\n\tif (dirid != upper_limit.objectid) {\n\t\tptr = &args->path[BTRFS_INO_LOOKUP_USER_PATH_MAX - 1];\n\n\t\tkey.objectid = treeid;\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\t\troot = btrfs_read_fs_root_no_name(fs_info, &key);\n\t\tif (IS_ERR(root)) {\n\t\t\tret = PTR_ERR(root);\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey.objectid = dirid;\n\t\tkey.type = BTRFS_INODE_REF_KEY;\n\t\tkey.offset = (u64)-1;\n\t\twhile (1) {\n\t\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\t\tif (ret < 0) {\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tret = btrfs_previous_item(root, path, dirid,\n\t\t\t\t\t\t\t  BTRFS_INODE_REF_KEY);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tgoto out;\n\t\t\t\t} else if (ret > 0) {\n\t\t\t\t\tret = -ENOENT;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tleaf = path->nodes[0];\n\t\t\tslot = path->slots[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t\tiref = btrfs_item_ptr(leaf, slot, struct btrfs_inode_ref);\n\t\t\tlen = btrfs_inode_ref_name_len(leaf, iref);\n\t\t\tptr -= len + 1;\n\t\t\ttotal_len += len + 1;\n\t\t\tif (ptr < args->path) {\n\t\t\t\tret = -ENAMETOOLONG;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t*(ptr + len) = '/';\n\t\t\tread_extent_buffer(leaf, ptr,\n\t\t\t\t\t(unsigned long)(iref + 1), len);\n\n\t\t\t/* Check the read+exec permission of this directory */\n\t\t\tret = btrfs_previous_item(root, path, dirid,\n\t\t\t\t\t\t  BTRFS_INODE_ITEM_KEY);\n\t\t\tif (ret < 0) {\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tleaf = path->nodes[0];\n\t\t\tslot = path->slots[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key2, slot);\n\t\t\tif (key2.objectid != dirid) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\ttemp_inode = btrfs_iget(sb, &key2, root, NULL);\n\t\t\tif (IS_ERR(temp_inode)) {\n\t\t\t\tret = PTR_ERR(temp_inode);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = inode_permission(temp_inode, MAY_READ | MAY_EXEC);\n\t\t\tiput(temp_inode);\n\t\t\tif (ret) {\n\t\t\t\tret = -EACCES;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (key.offset == upper_limit.objectid)\n\t\t\t\tbreak;\n\t\t\tif (key.objectid == BTRFS_FIRST_FREE_OBJECTID) {\n\t\t\t\tret = -EACCES;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tbtrfs_release_path(path);\n\t\t\tkey.objectid = key.offset;\n\t\t\tkey.offset = (u64)-1;\n\t\t\tdirid = key.objectid;\n\t\t}\n\n\t\tmemmove(args->path, ptr, total_len);\n\t\targs->path[total_len] = '\\0';\n\t\tbtrfs_release_path(path);\n\t}\n\n\t/* Get the bottom subvolume's name from ROOT_REF */\n\troot = fs_info->tree_root;\n\tkey.objectid = treeid;\n\tkey.type = BTRFS_ROOT_REF_KEY;\n\tkey.offset = args->treeid;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0) {\n\t\tgoto out;\n\t} else if (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tslot = path->slots[0];\n\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\titem_off = btrfs_item_ptr_offset(leaf, slot);\n\titem_len = btrfs_item_size_nr(leaf, slot);\n\t/* Check if dirid in ROOT_REF corresponds to passed dirid */\n\trref = btrfs_item_ptr(leaf, slot, struct btrfs_root_ref);\n\tif (args->dirid != btrfs_root_ref_dirid(leaf, rref)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Copy subvolume's name */\n\titem_off += sizeof(struct btrfs_root_ref);\n\titem_len -= sizeof(struct btrfs_root_ref);\n\tread_extent_buffer(leaf, args->name, item_off, item_len);\n\targs->name[item_len] = 0;\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_ino_lookup(struct file *file,\n\t\t\t\t\t   void __user *argp)\n{\n\tstruct btrfs_ioctl_ino_lookup_args *args;\n\tstruct inode *inode;\n\tint ret = 0;\n\n\targs = memdup_user(argp, sizeof(*args));\n\tif (IS_ERR(args))\n\t\treturn PTR_ERR(args);\n\n\tinode = file_inode(file);\n\n\t/*\n\t * Unprivileged query to obtain the containing subvolume root id. The\n\t * path is reset so it's consistent with btrfs_search_path_in_tree.\n\t */\n\tif (args->treeid == 0)\n\t\targs->treeid = BTRFS_I(inode)->root->root_key.objectid;\n\n\tif (args->objectid == BTRFS_FIRST_FREE_OBJECTID) {\n\t\targs->name[0] = 0;\n\t\tgoto out;\n\t}\n\n\tif (!capable(CAP_SYS_ADMIN)) {\n\t\tret = -EPERM;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_search_path_in_tree(BTRFS_I(inode)->root->fs_info,\n\t\t\t\t\targs->treeid, args->objectid,\n\t\t\t\t\targs->name);\n\nout:\n\tif (ret == 0 && copy_to_user(argp, args, sizeof(*args)))\n\t\tret = -EFAULT;\n\n\tkfree(args);\n\treturn ret;\n}\n\n/*\n * Version of ino_lookup ioctl (unprivileged)\n *\n * The main differences from ino_lookup ioctl are:\n *\n *   1. Read + Exec permission will be checked using inode_permission() during\n *      path construction. -EACCES will be returned in case of failure.\n *   2. Path construction will be stopped at the inode number which corresponds\n *      to the fd with which this ioctl is called. If constructed path does not\n *      exist under fd's inode, -EACCES will be returned.\n *   3. The name of bottom subvolume is also searched and filled.\n */\nstatic int btrfs_ioctl_ino_lookup_user(struct file *file, void __user *argp)\n{\n\tstruct btrfs_ioctl_ino_lookup_user_args *args;\n\tstruct inode *inode;\n\tint ret;\n\n\targs = memdup_user(argp, sizeof(*args));\n\tif (IS_ERR(args))\n\t\treturn PTR_ERR(args);\n\n\tinode = file_inode(file);\n\n\tif (args->dirid == BTRFS_FIRST_FREE_OBJECTID &&\n\t    BTRFS_I(inode)->location.objectid != BTRFS_FIRST_FREE_OBJECTID) {\n\t\t/*\n\t\t * The subvolume does not exist under fd with which this is\n\t\t * called\n\t\t */\n\t\tkfree(args);\n\t\treturn -EACCES;\n\t}\n\n\tret = btrfs_search_path_in_tree_user(inode, args);\n\n\tif (ret == 0 && copy_to_user(argp, args, sizeof(*args)))\n\t\tret = -EFAULT;\n\n\tkfree(args);\n\treturn ret;\n}\n\n/* Get the subvolume information in BTRFS_ROOT_ITEM and BTRFS_ROOT_BACKREF */\nstatic int btrfs_ioctl_get_subvol_info(struct file *file, void __user *argp)\n{\n\tstruct btrfs_ioctl_get_subvol_info_args *subvol_info;\n\tstruct btrfs_fs_info *fs_info;\n\tstruct btrfs_root *root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_root_item *root_item;\n\tstruct btrfs_root_ref *rref;\n\tstruct extent_buffer *leaf;\n\tunsigned long item_off;\n\tunsigned long item_len;\n\tstruct inode *inode;\n\tint slot;\n\tint ret = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tsubvol_info = kzalloc(sizeof(*subvol_info), GFP_KERNEL);\n\tif (!subvol_info) {\n\t\tbtrfs_free_path(path);\n\t\treturn -ENOMEM;\n\t}\n\n\tinode = file_inode(file);\n\tfs_info = BTRFS_I(inode)->root->fs_info;\n\n\t/* Get root_item of inode's subvolume */\n\tkey.objectid = BTRFS_I(inode)->root->root_key.objectid;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\troot = btrfs_read_fs_root_no_name(fs_info, &key);\n\tif (IS_ERR(root)) {\n\t\tret = PTR_ERR(root);\n\t\tgoto out;\n\t}\n\troot_item = &root->root_item;\n\n\tsubvol_info->treeid = key.objectid;\n\n\tsubvol_info->generation = btrfs_root_generation(root_item);\n\tsubvol_info->flags = btrfs_root_flags(root_item);\n\n\tmemcpy(subvol_info->uuid, root_item->uuid, BTRFS_UUID_SIZE);\n\tmemcpy(subvol_info->parent_uuid, root_item->parent_uuid,\n\t\t\t\t\t\t    BTRFS_UUID_SIZE);\n\tmemcpy(subvol_info->received_uuid, root_item->received_uuid,\n\t\t\t\t\t\t    BTRFS_UUID_SIZE);\n\n\tsubvol_info->ctransid = btrfs_root_ctransid(root_item);\n\tsubvol_info->ctime.sec = btrfs_stack_timespec_sec(&root_item->ctime);\n\tsubvol_info->ctime.nsec = btrfs_stack_timespec_nsec(&root_item->ctime);\n\n\tsubvol_info->otransid = btrfs_root_otransid(root_item);\n\tsubvol_info->otime.sec = btrfs_stack_timespec_sec(&root_item->otime);\n\tsubvol_info->otime.nsec = btrfs_stack_timespec_nsec(&root_item->otime);\n\n\tsubvol_info->stransid = btrfs_root_stransid(root_item);\n\tsubvol_info->stime.sec = btrfs_stack_timespec_sec(&root_item->stime);\n\tsubvol_info->stime.nsec = btrfs_stack_timespec_nsec(&root_item->stime);\n\n\tsubvol_info->rtransid = btrfs_root_rtransid(root_item);\n\tsubvol_info->rtime.sec = btrfs_stack_timespec_sec(&root_item->rtime);\n\tsubvol_info->rtime.nsec = btrfs_stack_timespec_nsec(&root_item->rtime);\n\n\tif (key.objectid != BTRFS_FS_TREE_OBJECTID) {\n\t\t/* Search root tree for ROOT_BACKREF of this subvolume */\n\t\troot = fs_info->tree_root;\n\n\t\tkey.type = BTRFS_ROOT_BACKREF_KEY;\n\t\tkey.offset = 0;\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tgoto out;\n\t\t} else if (path->slots[0] >=\n\t\t\t   btrfs_header_nritems(path->nodes[0])) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.objectid == subvol_info->treeid &&\n\t\t    key.type == BTRFS_ROOT_BACKREF_KEY) {\n\t\t\tsubvol_info->parent_id = key.offset;\n\n\t\t\trref = btrfs_item_ptr(leaf, slot, struct btrfs_root_ref);\n\t\t\tsubvol_info->dirid = btrfs_root_ref_dirid(leaf, rref);\n\n\t\t\titem_off = btrfs_item_ptr_offset(leaf, slot)\n\t\t\t\t\t+ sizeof(struct btrfs_root_ref);\n\t\t\titem_len = btrfs_item_size_nr(leaf, slot)\n\t\t\t\t\t- sizeof(struct btrfs_root_ref);\n\t\t\tread_extent_buffer(leaf, subvol_info->name,\n\t\t\t\t\t   item_off, item_len);\n\t\t} else {\n\t\t\tret = -ENOENT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (copy_to_user(argp, subvol_info, sizeof(*subvol_info)))\n\t\tret = -EFAULT;\n\nout:\n\tbtrfs_free_path(path);\n\tkzfree(subvol_info);\n\treturn ret;\n}\n\n/*\n * Return ROOT_REF information of the subvolume containing this inode\n * except the subvolume name.\n */\nstatic int btrfs_ioctl_get_subvol_rootref(struct file *file, void __user *argp)\n{\n\tstruct btrfs_ioctl_get_subvol_rootref_args *rootrefs;\n\tstruct btrfs_root_ref *rref;\n\tstruct btrfs_root *root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *leaf;\n\tstruct inode *inode;\n\tu64 objectid;\n\tint slot;\n\tint ret;\n\tu8 found;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\trootrefs = memdup_user(argp, sizeof(*rootrefs));\n\tif (IS_ERR(rootrefs)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(rootrefs);\n\t}\n\n\tinode = file_inode(file);\n\troot = BTRFS_I(inode)->root->fs_info->tree_root;\n\tobjectid = BTRFS_I(inode)->root->root_key.objectid;\n\n\tkey.objectid = objectid;\n\tkey.type = BTRFS_ROOT_REF_KEY;\n\tkey.offset = rootrefs->min_treeid;\n\tfound = 0;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0) {\n\t\tgoto out;\n\t} else if (path->slots[0] >=\n\t\t   btrfs_header_nritems(path->nodes[0])) {\n\t\tret = btrfs_next_leaf(root, path);\n\t\tif (ret < 0) {\n\t\t\tgoto out;\n\t\t} else if (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\twhile (1) {\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.objectid != objectid || key.type != BTRFS_ROOT_REF_KEY) {\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (found == BTRFS_MAX_ROOTREF_BUFFER_NUM) {\n\t\t\tret = -EOVERFLOW;\n\t\t\tgoto out;\n\t\t}\n\n\t\trref = btrfs_item_ptr(leaf, slot, struct btrfs_root_ref);\n\t\trootrefs->rootref[found].treeid = key.offset;\n\t\trootrefs->rootref[found].dirid =\n\t\t\t\t  btrfs_root_ref_dirid(leaf, rref);\n\t\tfound++;\n\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0) {\n\t\t\tgoto out;\n\t\t} else if (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tif (!ret || ret == -EOVERFLOW) {\n\t\trootrefs->num_items = found;\n\t\t/* update min_treeid for next search */\n\t\tif (found)\n\t\t\trootrefs->min_treeid =\n\t\t\t\trootrefs->rootref[found - 1].treeid + 1;\n\t\tif (copy_to_user(argp, rootrefs, sizeof(*rootrefs)))\n\t\t\tret = -EFAULT;\n\t}\n\n\tkfree(rootrefs);\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_destroy(struct file *file,\n\t\t\t\t\t     void __user *arg)\n{\n\tstruct dentry *parent = file->f_path.dentry;\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(parent->d_sb);\n\tstruct dentry *dentry;\n\tstruct inode *dir = d_inode(parent);\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *dest = NULL;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint namelen;\n\tint err = 0;\n\n\tif (!S_ISDIR(dir->i_mode))\n\t\treturn -ENOTDIR;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tnamelen = strlen(vol_args->name);\n\tif (strchr(vol_args->name, '/') ||\n\t    strncmp(vol_args->name, \"..\", namelen) == 0) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = mnt_want_write_file(file);\n\tif (err)\n\t\tgoto out;\n\n\n\terr = down_write_killable_nested(&dir->i_rwsem, I_MUTEX_PARENT);\n\tif (err == -EINTR)\n\t\tgoto out_drop_write;\n\tdentry = lookup_one_len(vol_args->name, parent, namelen);\n\tif (IS_ERR(dentry)) {\n\t\terr = PTR_ERR(dentry);\n\t\tgoto out_unlock_dir;\n\t}\n\n\tif (d_really_is_negative(dentry)) {\n\t\terr = -ENOENT;\n\t\tgoto out_dput;\n\t}\n\n\tinode = d_inode(dentry);\n\tdest = BTRFS_I(inode)->root;\n\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t/*\n\t\t * Regular user.  Only allow this with a special mount\n\t\t * option, when the user has write+exec access to the\n\t\t * subvol root, and when rmdir(2) would have been\n\t\t * allowed.\n\t\t *\n\t\t * Note that this is _not_ check that the subvol is\n\t\t * empty or doesn't contain data that we wouldn't\n\t\t * otherwise be able to delete.\n\t\t *\n\t\t * Users who want to delete empty subvols should try\n\t\t * rmdir(2).\n\t\t */\n\t\terr = -EPERM;\n\t\tif (!btrfs_test_opt(fs_info, USER_SUBVOL_RM_ALLOWED))\n\t\t\tgoto out_dput;\n\n\t\t/*\n\t\t * Do not allow deletion if the parent dir is the same\n\t\t * as the dir to be deleted.  That means the ioctl\n\t\t * must be called on the dentry referencing the root\n\t\t * of the subvol, not a random directory contained\n\t\t * within it.\n\t\t */\n\t\terr = -EINVAL;\n\t\tif (root == dest)\n\t\t\tgoto out_dput;\n\n\t\terr = inode_permission(inode, MAY_WRITE | MAY_EXEC);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\n\t/* check if subvolume may be deleted by a user */\n\terr = btrfs_may_delete(dir, dentry, 1);\n\tif (err)\n\t\tgoto out_dput;\n\n\tif (btrfs_ino(BTRFS_I(inode)) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\terr = -EINVAL;\n\t\tgoto out_dput;\n\t}\n\n\tinode_lock(inode);\n\terr = btrfs_delete_subvolume(dir, dentry);\n\tinode_unlock(inode);\n\tif (!err)\n\t\td_delete(dentry);\n\nout_dput:\n\tdput(dentry);\nout_unlock_dir:\n\tinode_unlock(dir);\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\tkfree(vol_args);\n\treturn err;\n}\n\nstatic int btrfs_ioctl_defrag(struct file *file, void __user *argp)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_defrag_range_args *range;\n\tint ret;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (btrfs_root_readonly(root)) {\n\t\tret = -EROFS;\n\t\tgoto out;\n\t}\n\n\tswitch (inode->i_mode & S_IFMT) {\n\tcase S_IFDIR:\n\t\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = btrfs_defrag_root(root);\n\t\tbreak;\n\tcase S_IFREG:\n\t\t/*\n\t\t * Note that this does not check the file descriptor for write\n\t\t * access. This prevents defragmenting executables that are\n\t\t * running and allows defrag on files open in read-only mode.\n\t\t */\n\t\tif (!capable(CAP_SYS_ADMIN) &&\n\t\t    inode_permission(inode, MAY_WRITE)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\n\t\trange = kzalloc(sizeof(*range), GFP_KERNEL);\n\t\tif (!range) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (argp) {\n\t\t\tif (copy_from_user(range, argp,\n\t\t\t\t\t   sizeof(*range))) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tkfree(range);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t/* compression requires us to start the IO */\n\t\t\tif ((range->flags & BTRFS_DEFRAG_RANGE_COMPRESS)) {\n\t\t\t\trange->flags |= BTRFS_DEFRAG_RANGE_START_IO;\n\t\t\t\trange->extent_thresh = (u32)-1;\n\t\t\t}\n\t\t} else {\n\t\t\t/* the rest are all set to zero by kzalloc */\n\t\t\trange->len = (u64)-1;\n\t\t}\n\t\tret = btrfs_defrag_file(file_inode(file), file,\n\t\t\t\t\trange, BTRFS_OLDEST_GENERATION, 0);\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t\tkfree(range);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\nout:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_add_dev(struct btrfs_fs_info *fs_info, void __user *arg)\n{\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags))\n\t\treturn BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tret = btrfs_init_new_device(fs_info, vol_args->name);\n\n\tif (!ret)\n\t\tbtrfs_info(fs_info, \"disk added %s\", vol_args->name);\n\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_rm_dev_v2(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_vol_args_v2 *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto err_drop;\n\t}\n\n\t/* Check for compatibility reject unknown flags */\n\tif (vol_args->flags & ~BTRFS_VOL_ARG_V2_FLAGS_SUPPORTED) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tret = BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t\tgoto out;\n\t}\n\n\tif (vol_args->flags & BTRFS_DEVICE_SPEC_BY_ID) {\n\t\tret = btrfs_rm_device(fs_info, NULL, vol_args->devid);\n\t} else {\n\t\tvol_args->name[BTRFS_SUBVOL_NAME_MAX] = '\\0';\n\t\tret = btrfs_rm_device(fs_info, vol_args->name, 0);\n\t}\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\n\tif (!ret) {\n\t\tif (vol_args->flags & BTRFS_DEVICE_SPEC_BY_ID)\n\t\t\tbtrfs_info(fs_info, \"device deleted: id %llu\",\n\t\t\t\t\tvol_args->devid);\n\t\telse\n\t\t\tbtrfs_info(fs_info, \"device deleted: %s\",\n\t\t\t\t\tvol_args->name);\n\t}\nout:\n\tkfree(vol_args);\nerr_drop:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_rm_dev(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tret = BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t\tgoto out_drop_write;\n\t}\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tret = btrfs_rm_device(fs_info, vol_args->name, 0);\n\n\tif (!ret)\n\t\tbtrfs_info(fs_info, \"disk deleted %s\", vol_args->name);\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\nout_drop_write:\n\tmnt_drop_write_file(file);\n\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_fs_info(struct btrfs_fs_info *fs_info,\n\t\t\t\tvoid __user *arg)\n{\n\tstruct btrfs_ioctl_fs_info_args *fi_args;\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tint ret = 0;\n\n\tfi_args = kzalloc(sizeof(*fi_args), GFP_KERNEL);\n\tif (!fi_args)\n\t\treturn -ENOMEM;\n\n\trcu_read_lock();\n\tfi_args->num_devices = fs_devices->num_devices;\n\n\tlist_for_each_entry_rcu(device, &fs_devices->devices, dev_list) {\n\t\tif (device->devid > fi_args->max_id)\n\t\t\tfi_args->max_id = device->devid;\n\t}\n\trcu_read_unlock();\n\n\tmemcpy(&fi_args->fsid, fs_devices->fsid, sizeof(fi_args->fsid));\n\tfi_args->nodesize = fs_info->nodesize;\n\tfi_args->sectorsize = fs_info->sectorsize;\n\tfi_args->clone_alignment = fs_info->sectorsize;\n\n\tif (copy_to_user(arg, fi_args, sizeof(*fi_args)))\n\t\tret = -EFAULT;\n\n\tkfree(fi_args);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_dev_info(struct btrfs_fs_info *fs_info,\n\t\t\t\t void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_info_args *di_args;\n\tstruct btrfs_device *dev;\n\tint ret = 0;\n\tchar *s_uuid = NULL;\n\n\tdi_args = memdup_user(arg, sizeof(*di_args));\n\tif (IS_ERR(di_args))\n\t\treturn PTR_ERR(di_args);\n\n\tif (!btrfs_is_empty_uuid(di_args->uuid))\n\t\ts_uuid = di_args->uuid;\n\n\trcu_read_lock();\n\tdev = btrfs_find_device(fs_info->fs_devices, di_args->devid, s_uuid,\n\t\t\t\tNULL);\n\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tdi_args->devid = dev->devid;\n\tdi_args->bytes_used = btrfs_device_get_bytes_used(dev);\n\tdi_args->total_bytes = btrfs_device_get_total_bytes(dev);\n\tmemcpy(di_args->uuid, dev->uuid, sizeof(di_args->uuid));\n\tif (dev->name) {\n\t\tstrncpy(di_args->path, rcu_str_deref(dev->name),\n\t\t\t\tsizeof(di_args->path) - 1);\n\t\tdi_args->path[sizeof(di_args->path) - 1] = 0;\n\t} else {\n\t\tdi_args->path[0] = '\\0';\n\t}\n\nout:\n\trcu_read_unlock();\n\tif (ret == 0 && copy_to_user(arg, di_args, sizeof(*di_args)))\n\t\tret = -EFAULT;\n\n\tkfree(di_args);\n\treturn ret;\n}\n\nstatic void btrfs_double_inode_unlock(struct inode *inode1, struct inode *inode2)\n{\n\tinode_unlock(inode1);\n\tinode_unlock(inode2);\n}\n\nstatic void btrfs_double_inode_lock(struct inode *inode1, struct inode *inode2)\n{\n\tif (inode1 < inode2)\n\t\tswap(inode1, inode2);\n\n\tinode_lock_nested(inode1, I_MUTEX_PARENT);\n\tinode_lock_nested(inode2, I_MUTEX_CHILD);\n}\n\nstatic void btrfs_double_extent_unlock(struct inode *inode1, u64 loff1,\n\t\t\t\t       struct inode *inode2, u64 loff2, u64 len)\n{\n\tunlock_extent(&BTRFS_I(inode1)->io_tree, loff1, loff1 + len - 1);\n\tunlock_extent(&BTRFS_I(inode2)->io_tree, loff2, loff2 + len - 1);\n}\n\nstatic void btrfs_double_extent_lock(struct inode *inode1, u64 loff1,\n\t\t\t\t     struct inode *inode2, u64 loff2, u64 len)\n{\n\tif (inode1 < inode2) {\n\t\tswap(inode1, inode2);\n\t\tswap(loff1, loff2);\n\t} else if (inode1 == inode2 && loff2 < loff1) {\n\t\tswap(loff1, loff2);\n\t}\n\tlock_extent(&BTRFS_I(inode1)->io_tree, loff1, loff1 + len - 1);\n\tlock_extent(&BTRFS_I(inode2)->io_tree, loff2, loff2 + len - 1);\n}\n\nstatic int btrfs_extent_same_range(struct inode *src, u64 loff, u64 olen,\n\t\t\t\t   struct inode *dst, u64 dst_loff)\n{\n\tu64 bs = BTRFS_I(src)->root->fs_info->sb->s_blocksize;\n\tint ret;\n\tu64 len = olen;\n\n\tif (loff + len == src->i_size)\n\t\tlen = ALIGN(src->i_size, bs) - loff;\n\t/*\n\t * For same inode case we don't want our length pushed out past i_size\n\t * as comparing that data range makes no sense.\n\t *\n\t * This effectively means we require aligned extents for the single\n\t * inode case, whereas the other cases allow an unaligned length so long\n\t * as it ends at i_size.\n\t */\n\tif (dst == src && len != olen)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Lock destination range to serialize with concurrent readpages() and\n\t * source range to serialize with relocation.\n\t */\n\tbtrfs_double_extent_lock(src, loff, dst, dst_loff, len);\n\tret = btrfs_clone(src, dst, loff, olen, len, dst_loff, 1);\n\tbtrfs_double_extent_unlock(src, loff, dst, dst_loff, len);\n\n\treturn ret;\n}\n\n#define BTRFS_MAX_DEDUPE_LEN\tSZ_16M\n\nstatic int btrfs_extent_same(struct inode *src, u64 loff, u64 olen,\n\t\t\t     struct inode *dst, u64 dst_loff)\n{\n\tint ret;\n\tu64 i, tail_len, chunk_count;\n\n\ttail_len = olen % BTRFS_MAX_DEDUPE_LEN;\n\tchunk_count = div_u64(olen, BTRFS_MAX_DEDUPE_LEN);\n\n\tfor (i = 0; i < chunk_count; i++) {\n\t\tret = btrfs_extent_same_range(src, loff, BTRFS_MAX_DEDUPE_LEN,\n\t\t\t\t\t      dst, dst_loff);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tloff += BTRFS_MAX_DEDUPE_LEN;\n\t\tdst_loff += BTRFS_MAX_DEDUPE_LEN;\n\t}\n\n\tif (tail_len > 0)\n\t\tret = btrfs_extent_same_range(src, loff, tail_len, dst,\n\t\t\t\t\t      dst_loff);\n\n\treturn ret;\n}\n\nstatic int clone_finish_inode_update(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     u64 endoff,\n\t\t\t\t     const u64 destoff,\n\t\t\t\t     const u64 olen,\n\t\t\t\t     int no_time_update)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\tinode_inc_iversion(inode);\n\tif (!no_time_update)\n\t\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\t/*\n\t * We round up to the block size at eof when determining which\n\t * extents to clone above, but shouldn't round up the file size.\n\t */\n\tif (endoff > destoff + olen)\n\t\tendoff = destoff + olen;\n\tif (endoff > inode->i_size)\n\t\tbtrfs_i_size_write(BTRFS_I(inode), endoff);\n\n\tret = btrfs_update_inode(trans, root, inode);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out;\n\t}\n\tret = btrfs_end_transaction(trans);\nout:\n\treturn ret;\n}\n\nstatic void clone_update_extent_map(struct btrfs_inode *inode,\n\t\t\t\t    const struct btrfs_trans_handle *trans,\n\t\t\t\t    const struct btrfs_path *path,\n\t\t\t\t    const u64 hole_offset,\n\t\t\t\t    const u64 hole_len)\n{\n\tstruct extent_map_tree *em_tree = &inode->extent_tree;\n\tstruct extent_map *em;\n\tint ret;\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &inode->runtime_flags);\n\t\treturn;\n\t}\n\n\tif (path) {\n\t\tstruct btrfs_file_extent_item *fi;\n\n\t\tfi = btrfs_item_ptr(path->nodes[0], path->slots[0],\n\t\t\t\t    struct btrfs_file_extent_item);\n\t\tbtrfs_extent_item_to_extent_map(inode, path, fi, false, em);\n\t\tem->generation = -1;\n\t\tif (btrfs_file_extent_type(path->nodes[0], fi) ==\n\t\t    BTRFS_FILE_EXTENT_INLINE)\n\t\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC,\n\t\t\t\t\t&inode->runtime_flags);\n\t} else {\n\t\tem->start = hole_offset;\n\t\tem->len = hole_len;\n\t\tem->ram_bytes = em->len;\n\t\tem->orig_start = hole_offset;\n\t\tem->block_start = EXTENT_MAP_HOLE;\n\t\tem->block_len = 0;\n\t\tem->orig_block_len = 0;\n\t\tem->compress_type = BTRFS_COMPRESS_NONE;\n\t\tem->generation = trans->transid;\n\t}\n\n\twhile (1) {\n\t\twrite_lock(&em_tree->lock);\n\t\tret = add_extent_mapping(em_tree, em, 1);\n\t\twrite_unlock(&em_tree->lock);\n\t\tif (ret != -EEXIST) {\n\t\t\tfree_extent_map(em);\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_drop_extent_cache(inode, em->start,\n\t\t\t\t\tem->start + em->len - 1, 0);\n\t}\n\n\tif (ret)\n\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &inode->runtime_flags);\n}\n\n/*\n * Make sure we do not end up inserting an inline extent into a file that has\n * already other (non-inline) extents. If a file has an inline extent it can\n * not have any other extents and the (single) inline extent must start at the\n * file offset 0. Failing to respect these rules will lead to file corruption,\n * resulting in EIO errors on read/write operations, hitting BUG_ON's in mm, etc\n *\n * We can have extents that have been already written to disk or we can have\n * dirty ranges still in delalloc, in which case the extent maps and items are\n * created only when we run delalloc, and the delalloc ranges might fall outside\n * the range we are currently locking in the inode's io tree. So we check the\n * inode's i_size because of that (i_size updates are done while holding the\n * i_mutex, which we are holding here).\n * We also check to see if the inode has a size not greater than \"datal\" but has\n * extents beyond it, due to an fallocate with FALLOC_FL_KEEP_SIZE (and we are\n * protected against such concurrent fallocate calls by the i_mutex).\n *\n * If the file has no extents but a size greater than datal, do not allow the\n * copy because we would need turn the inline extent into a non-inline one (even\n * with NO_HOLES enabled). If we find our destination inode only has one inline\n * extent, just overwrite it with the source inline extent if its size is less\n * than the source extent's size, or we could copy the source inline extent's\n * data into the destination inode's inline extent if the later is greater then\n * the former.\n */\nstatic int clone_copy_inline_extent(struct inode *dst,\n\t\t\t\t    struct btrfs_trans_handle *trans,\n\t\t\t\t    struct btrfs_path *path,\n\t\t\t\t    struct btrfs_key *new_key,\n\t\t\t\t    const u64 drop_start,\n\t\t\t\t    const u64 datal,\n\t\t\t\t    const u64 skip,\n\t\t\t\t    const u64 size,\n\t\t\t\t    char *inline_data)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dst->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(dst)->root;\n\tconst u64 aligned_end = ALIGN(new_key->offset + datal,\n\t\t\t\t      fs_info->sectorsize);\n\tint ret;\n\tstruct btrfs_key key;\n\n\tif (new_key->offset > 0)\n\t\treturn -EOPNOTSUPP;\n\n\tkey.objectid = btrfs_ino(BTRFS_I(dst));\n\tkey.type = BTRFS_EXTENT_DATA_KEY;\n\tkey.offset = 0;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0) {\n\t\treturn ret;\n\t} else if (ret > 0) {\n\t\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\telse if (ret > 0)\n\t\t\t\tgoto copy_inline_extent;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n\t\tif (key.objectid == btrfs_ino(BTRFS_I(dst)) &&\n\t\t    key.type == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tASSERT(key.offset > 0);\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t} else if (i_size_read(dst) <= datal) {\n\t\tstruct btrfs_file_extent_item *ei;\n\t\tu64 ext_len;\n\n\t\t/*\n\t\t * If the file size is <= datal, make sure there are no other\n\t\t * extents following (can happen do to an fallocate call with\n\t\t * the flag FALLOC_FL_KEEP_SIZE).\n\t\t */\n\t\tei = btrfs_item_ptr(path->nodes[0], path->slots[0],\n\t\t\t\t    struct btrfs_file_extent_item);\n\t\t/*\n\t\t * If it's an inline extent, it can not have other extents\n\t\t * following it.\n\t\t */\n\t\tif (btrfs_file_extent_type(path->nodes[0], ei) ==\n\t\t    BTRFS_FILE_EXTENT_INLINE)\n\t\t\tgoto copy_inline_extent;\n\n\t\text_len = btrfs_file_extent_num_bytes(path->nodes[0], ei);\n\t\tif (ext_len > aligned_end)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0) {\n\t\t\treturn ret;\n\t\t} else if (ret == 0) {\n\t\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key,\n\t\t\t\t\t      path->slots[0]);\n\t\t\tif (key.objectid == btrfs_ino(BTRFS_I(dst)) &&\n\t\t\t    key.type == BTRFS_EXTENT_DATA_KEY)\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t}\n\ncopy_inline_extent:\n\t/*\n\t * We have no extent items, or we have an extent at offset 0 which may\n\t * or may not be inlined. All these cases are dealt the same way.\n\t */\n\tif (i_size_read(dst) > datal) {\n\t\t/*\n\t\t * If the destination inode has an inline extent...\n\t\t * This would require copying the data from the source inline\n\t\t * extent into the beginning of the destination's inline extent.\n\t\t * But this is really complex, both extents can be compressed\n\t\t * or just one of them, which would require decompressing and\n\t\t * re-compressing data (which could increase the new compressed\n\t\t * size, not allowing the compressed data to fit anymore in an\n\t\t * inline extent).\n\t\t * So just don't support this case for now (it should be rare,\n\t\t * we are not really saving space when cloning inline extents).\n\t\t */\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tbtrfs_release_path(path);\n\tret = btrfs_drop_extents(trans, root, dst, drop_start, aligned_end, 1);\n\tif (ret)\n\t\treturn ret;\n\tret = btrfs_insert_empty_item(trans, root, path, new_key, size);\n\tif (ret)\n\t\treturn ret;\n\n\tif (skip) {\n\t\tconst u32 start = btrfs_file_extent_calc_inline_size(0);\n\n\t\tmemmove(inline_data + start, inline_data + start + skip, datal);\n\t}\n\n\twrite_extent_buffer(path->nodes[0], inline_data,\n\t\t\t    btrfs_item_ptr_offset(path->nodes[0],\n\t\t\t\t\t\t  path->slots[0]),\n\t\t\t    size);\n\tinode_add_bytes(dst, datal);\n\n\treturn 0;\n}\n\n/**\n * btrfs_clone() - clone a range from inode file to another\n *\n * @src: Inode to clone from\n * @inode: Inode to clone to\n * @off: Offset within source to start clone from\n * @olen: Original length, passed by user, of range to clone\n * @olen_aligned: Block-aligned value of olen\n * @destoff: Offset within @inode to start clone\n * @no_time_update: Whether to update mtime/ctime on the target inode\n */\nstatic int btrfs_clone(struct inode *src, struct inode *inode,\n\t\t       const u64 off, const u64 olen, const u64 olen_aligned,\n\t\t       const u64 destoff, int no_time_update)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_path *path = NULL;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_trans_handle *trans;\n\tchar *buf = NULL;\n\tstruct btrfs_key key;\n\tu32 nritems;\n\tint slot;\n\tint ret;\n\tconst u64 len = olen_aligned;\n\tu64 last_dest_end = destoff;\n\n\tret = -ENOMEM;\n\tbuf = kvmalloc(fs_info->nodesize, GFP_KERNEL);\n\tif (!buf)\n\t\treturn ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tkvfree(buf);\n\t\treturn ret;\n\t}\n\n\tpath->reada = READA_FORWARD;\n\t/* clone data */\n\tkey.objectid = btrfs_ino(BTRFS_I(src));\n\tkey.type = BTRFS_EXTENT_DATA_KEY;\n\tkey.offset = off;\n\n\twhile (1) {\n\t\tu64 next_key_min_offset = key.offset + 1;\n\n\t\t/*\n\t\t * note the key will change type as we walk through the\n\t\t * tree.\n\t\t */\n\t\tpath->leave_spinning = 1;\n\t\tret = btrfs_search_slot(NULL, BTRFS_I(src)->root, &key, path,\n\t\t\t\t0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * First search, if no extent item that starts at offset off was\n\t\t * found but the previous item is an extent item, it's possible\n\t\t * it might overlap our target range, therefore process it.\n\t\t */\n\t\tif (key.offset == off && ret > 0 && path->slots[0] > 0) {\n\t\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key,\n\t\t\t\t\t      path->slots[0] - 1);\n\t\t\tif (key.type == BTRFS_EXTENT_DATA_KEY)\n\t\t\t\tpath->slots[0]--;\n\t\t}\n\n\t\tnritems = btrfs_header_nritems(path->nodes[0]);\nprocess_slot:\n\t\tif (path->slots[0] >= nritems) {\n\t\t\tret = btrfs_next_leaf(BTRFS_I(src)->root, path);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tnritems = btrfs_header_nritems(path->nodes[0]);\n\t\t}\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.type > BTRFS_EXTENT_DATA_KEY ||\n\t\t    key.objectid != btrfs_ino(BTRFS_I(src)))\n\t\t\tbreak;\n\n\t\tif (key.type == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tstruct btrfs_file_extent_item *extent;\n\t\t\tint type;\n\t\t\tu32 size;\n\t\t\tstruct btrfs_key new_key;\n\t\t\tu64 disko = 0, diskl = 0;\n\t\t\tu64 datao = 0, datal = 0;\n\t\t\tu8 comp;\n\t\t\tu64 drop_start;\n\n\t\t\textent = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\tstruct btrfs_file_extent_item);\n\t\t\tcomp = btrfs_file_extent_compression(leaf, extent);\n\t\t\ttype = btrfs_file_extent_type(leaf, extent);\n\t\t\tif (type == BTRFS_FILE_EXTENT_REG ||\n\t\t\t    type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\t\tdisko = btrfs_file_extent_disk_bytenr(leaf,\n\t\t\t\t\t\t\t\t      extent);\n\t\t\t\tdiskl = btrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t extent);\n\t\t\t\tdatao = btrfs_file_extent_offset(leaf, extent);\n\t\t\t\tdatal = btrfs_file_extent_num_bytes(leaf,\n\t\t\t\t\t\t\t\t    extent);\n\t\t\t} else if (type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\t/* take upper bound, may be compressed */\n\t\t\t\tdatal = btrfs_file_extent_ram_bytes(leaf,\n\t\t\t\t\t\t\t\t    extent);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * The first search might have left us at an extent\n\t\t\t * item that ends before our target range's start, can\n\t\t\t * happen if we have holes and NO_HOLES feature enabled.\n\t\t\t */\n\t\t\tif (key.offset + datal <= off) {\n\t\t\t\tpath->slots[0]++;\n\t\t\t\tgoto process_slot;\n\t\t\t} else if (key.offset >= off + len) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tnext_key_min_offset = key.offset + datal;\n\t\t\tsize = btrfs_item_size_nr(leaf, slot);\n\t\t\tread_extent_buffer(leaf, buf,\n\t\t\t\t\t   btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t   size);\n\n\t\t\tbtrfs_release_path(path);\n\t\t\tpath->leave_spinning = 0;\n\n\t\t\tmemcpy(&new_key, &key, sizeof(new_key));\n\t\t\tnew_key.objectid = btrfs_ino(BTRFS_I(inode));\n\t\t\tif (off <= key.offset)\n\t\t\t\tnew_key.offset = key.offset + destoff - off;\n\t\t\telse\n\t\t\t\tnew_key.offset = destoff;\n\n\t\t\t/*\n\t\t\t * Deal with a hole that doesn't have an extent item\n\t\t\t * that represents it (NO_HOLES feature enabled).\n\t\t\t * This hole is either in the middle of the cloning\n\t\t\t * range or at the beginning (fully overlaps it or\n\t\t\t * partially overlaps it).\n\t\t\t */\n\t\t\tif (new_key.offset != last_dest_end)\n\t\t\t\tdrop_start = last_dest_end;\n\t\t\telse\n\t\t\t\tdrop_start = new_key.offset;\n\n\t\t\t/*\n\t\t\t * 1 - adjusting old extent (we may have to split it)\n\t\t\t * 1 - add new extent\n\t\t\t * 1 - inode update\n\t\t\t */\n\t\t\ttrans = btrfs_start_transaction(root, 3);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (type == BTRFS_FILE_EXTENT_REG ||\n\t\t\t    type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\t\t/*\n\t\t\t\t *    a  | --- range to clone ---|  b\n\t\t\t\t * | ------------- extent ------------- |\n\t\t\t\t */\n\n\t\t\t\t/* subtract range b */\n\t\t\t\tif (key.offset + datal > off + len)\n\t\t\t\t\tdatal = off + len - key.offset;\n\n\t\t\t\t/* subtract range a */\n\t\t\t\tif (off > key.offset) {\n\t\t\t\t\tdatao += off - key.offset;\n\t\t\t\t\tdatal -= off - key.offset;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t\t\t drop_start,\n\t\t\t\t\t\t\t new_key.offset + datal,\n\t\t\t\t\t\t\t 1);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (ret != -EOPNOTSUPP)\n\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_insert_empty_item(trans, root, path,\n\t\t\t\t\t\t\t      &new_key, size);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tleaf = path->nodes[0];\n\t\t\t\tslot = path->slots[0];\n\t\t\t\twrite_extent_buffer(leaf, buf,\n\t\t\t\t\t    btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t    size);\n\n\t\t\t\textent = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\tstruct btrfs_file_extent_item);\n\n\t\t\t\t/* disko == 0 means it's a hole */\n\t\t\t\tif (!disko)\n\t\t\t\t\tdatao = 0;\n\n\t\t\t\tbtrfs_set_file_extent_offset(leaf, extent,\n\t\t\t\t\t\t\t     datao);\n\t\t\t\tbtrfs_set_file_extent_num_bytes(leaf, extent,\n\t\t\t\t\t\t\t\tdatal);\n\n\t\t\t\tif (disko) {\n\t\t\t\t\tinode_add_bytes(inode, datal);\n\t\t\t\t\tret = btrfs_inc_extent_ref(trans,\n\t\t\t\t\t\t\troot,\n\t\t\t\t\t\t\tdisko, diskl, 0,\n\t\t\t\t\t\t\troot->root_key.objectid,\n\t\t\t\t\t\t\tbtrfs_ino(BTRFS_I(inode)),\n\t\t\t\t\t\t\tnew_key.offset - datao);\n\t\t\t\t\tif (ret) {\n\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\t\tret);\n\t\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\t\tgoto out;\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if (type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\tu64 skip = 0;\n\t\t\t\tu64 trim = 0;\n\n\t\t\t\tif (off > key.offset) {\n\t\t\t\t\tskip = off - key.offset;\n\t\t\t\t\tnew_key.offset += skip;\n\t\t\t\t}\n\n\t\t\t\tif (key.offset + datal > off + len)\n\t\t\t\t\ttrim = key.offset + datal - (off + len);\n\n\t\t\t\tif (comp && (skip || trim)) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tsize -= skip + trim;\n\t\t\t\tdatal -= skip + trim;\n\n\t\t\t\tret = clone_copy_inline_extent(inode,\n\t\t\t\t\t\t\t       trans, path,\n\t\t\t\t\t\t\t       &new_key,\n\t\t\t\t\t\t\t       drop_start,\n\t\t\t\t\t\t\t       datal,\n\t\t\t\t\t\t\t       skip, size, buf);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (ret != -EOPNOTSUPP)\n\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tleaf = path->nodes[0];\n\t\t\t\tslot = path->slots[0];\n\t\t\t}\n\n\t\t\t/* If we have an implicit hole (NO_HOLES feature). */\n\t\t\tif (drop_start < new_key.offset)\n\t\t\t\tclone_update_extent_map(BTRFS_I(inode), trans,\n\t\t\t\t\t\tNULL, drop_start,\n\t\t\t\t\t\tnew_key.offset - drop_start);\n\n\t\t\tclone_update_extent_map(BTRFS_I(inode), trans,\n\t\t\t\t\tpath, 0, 0);\n\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t\tbtrfs_release_path(path);\n\n\t\t\tlast_dest_end = ALIGN(new_key.offset + datal,\n\t\t\t\t\t      fs_info->sectorsize);\n\t\t\tret = clone_finish_inode_update(trans, inode,\n\t\t\t\t\t\t\tlast_dest_end,\n\t\t\t\t\t\t\tdestoff, olen,\n\t\t\t\t\t\t\tno_time_update);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tif (new_key.offset + datal >= destoff + len)\n\t\t\t\tbreak;\n\t\t}\n\t\tbtrfs_release_path(path);\n\t\tkey.offset = next_key_min_offset;\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tret = 0;\n\n\tif (last_dest_end < destoff + len) {\n\t\t/*\n\t\t * We have an implicit hole (NO_HOLES feature is enabled) that\n\t\t * fully or partially overlaps our cloning range at its end.\n\t\t */\n\t\tbtrfs_release_path(path);\n\n\t\t/*\n\t\t * 1 - remove extent(s)\n\t\t * 1 - inode update\n\t\t */\n\t\ttrans = btrfs_start_transaction(root, 2);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out;\n\t\t}\n\t\tret = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t last_dest_end, destoff + len, 1);\n\t\tif (ret) {\n\t\t\tif (ret != -EOPNOTSUPP)\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tbtrfs_end_transaction(trans);\n\t\t\tgoto out;\n\t\t}\n\t\tclone_update_extent_map(BTRFS_I(inode), trans, NULL,\n\t\t\t\tlast_dest_end,\n\t\t\t\tdestoff + len - last_dest_end);\n\t\tret = clone_finish_inode_update(trans, inode, destoff + len,\n\t\t\t\t\t\tdestoff, olen, no_time_update);\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tkvfree(buf);\n\treturn ret;\n}\n\nstatic noinline int btrfs_clone_files(struct file *file, struct file *file_src,\n\t\t\t\t\tu64 off, u64 olen, u64 destoff)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct inode *src = file_inode(file_src);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tint ret;\n\tu64 len = olen;\n\tu64 bs = fs_info->sb->s_blocksize;\n\n\t/*\n\t * TODO:\n\t * - split compressed inline extents.  annoying: we need to\n\t *   decompress into destination's address_space (the file offset\n\t *   may change, so source mapping won't do), then recompress (or\n\t *   otherwise reinsert) a subrange.\n\t *\n\t * - split destination inode's inline extents.  The inline extents can\n\t *   be either compressed or non-compressed.\n\t */\n\n\t/*\n\t * VFS's generic_remap_file_range_prep() protects us from cloning the\n\t * eof block into the middle of a file, which would result in corruption\n\t * if the file size is not blocksize aligned. So we don't need to check\n\t * for that case here.\n\t */\n\tif (off + len == src->i_size)\n\t\tlen = ALIGN(src->i_size, bs) - off;\n\n\tif (destoff > inode->i_size) {\n\t\tconst u64 wb_start = ALIGN_DOWN(inode->i_size, bs);\n\n\t\tret = btrfs_cont_expand(inode, inode->i_size, destoff);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\t/*\n\t\t * We may have truncated the last block if the inode's size is\n\t\t * not sector size aligned, so we need to wait for writeback to\n\t\t * complete before proceeding further, otherwise we can race\n\t\t * with cloning and attempt to increment a reference to an\n\t\t * extent that no longer exists (writeback completed right after\n\t\t * we found the previous extent covering eof and before we\n\t\t * attempted to increment its reference count).\n\t\t */\n\t\tret = btrfs_wait_ordered_range(inode, wb_start,\n\t\t\t\t\t       destoff - wb_start);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Lock destination range to serialize with concurrent readpages() and\n\t * source range to serialize with relocation.\n\t */\n\tbtrfs_double_extent_lock(src, off, inode, destoff, len);\n\tret = btrfs_clone(src, inode, off, olen, len, destoff, 0);\n\tbtrfs_double_extent_unlock(src, off, inode, destoff, len);\n\t/*\n\t * Truncate page cache pages so that future reads will see the cloned\n\t * data immediately and not the previous data.\n\t */\n\ttruncate_inode_pages_range(&inode->i_data,\n\t\t\t\tround_down(destoff, PAGE_SIZE),\n\t\t\t\tround_up(destoff + len, PAGE_SIZE) - 1);\n\n\treturn ret;\n}\n\nstatic int btrfs_remap_file_range_prep(struct file *file_in, loff_t pos_in,\n\t\t\t\t       struct file *file_out, loff_t pos_out,\n\t\t\t\t       loff_t *len, unsigned int remap_flags)\n{\n\tstruct inode *inode_in = file_inode(file_in);\n\tstruct inode *inode_out = file_inode(file_out);\n\tu64 bs = BTRFS_I(inode_out)->root->fs_info->sb->s_blocksize;\n\tbool same_inode = inode_out == inode_in;\n\tu64 wb_len;\n\tint ret;\n\n\tif (!(remap_flags & REMAP_FILE_DEDUP)) {\n\t\tstruct btrfs_root *root_out = BTRFS_I(inode_out)->root;\n\n\t\tif (btrfs_root_readonly(root_out))\n\t\t\treturn -EROFS;\n\n\t\tif (file_in->f_path.mnt != file_out->f_path.mnt ||\n\t\t    inode_in->i_sb != inode_out->i_sb)\n\t\t\treturn -EXDEV;\n\t}\n\n\tif (same_inode)\n\t\tinode_lock(inode_in);\n\telse\n\t\tbtrfs_double_inode_lock(inode_in, inode_out);\n\n\t/* don't make the dst file partly checksummed */\n\tif ((BTRFS_I(inode_in)->flags & BTRFS_INODE_NODATASUM) !=\n\t    (BTRFS_I(inode_out)->flags & BTRFS_INODE_NODATASUM)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Now that the inodes are locked, we need to start writeback ourselves\n\t * and can not rely on the writeback from the VFS's generic helper\n\t * generic_remap_file_range_prep() because:\n\t *\n\t * 1) For compression we must call filemap_fdatawrite_range() range\n\t *    twice (btrfs_fdatawrite_range() does it for us), and the generic\n\t *    helper only calls it once;\n\t *\n\t * 2) filemap_fdatawrite_range(), called by the generic helper only\n\t *    waits for the writeback to complete, i.e. for IO to be done, and\n\t *    not for the ordered extents to complete. We need to wait for them\n\t *    to complete so that new file extent items are in the fs tree.\n\t */\n\tif (*len == 0 && !(remap_flags & REMAP_FILE_DEDUP))\n\t\twb_len = ALIGN(inode_in->i_size, bs) - ALIGN_DOWN(pos_in, bs);\n\telse\n\t\twb_len = ALIGN(*len, bs);\n\n\t/*\n\t * Since we don't lock ranges, wait for ongoing lockless dio writes (as\n\t * any in progress could create its ordered extents after we wait for\n\t * existing ordered extents below).\n\t */\n\tinode_dio_wait(inode_in);\n\tif (!same_inode)\n\t\tinode_dio_wait(inode_out);\n\n\tret = btrfs_wait_ordered_range(inode_in, ALIGN_DOWN(pos_in, bs),\n\t\t\t\t       wb_len);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\tret = btrfs_wait_ordered_range(inode_out, ALIGN_DOWN(pos_out, bs),\n\t\t\t\t       wb_len);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\n\tret = generic_remap_file_range_prep(file_in, pos_in, file_out, pos_out,\n\t\t\t\t\t    len, remap_flags);\n\tif (ret < 0 || *len == 0)\n\t\tgoto out_unlock;\n\n\treturn 0;\n\n out_unlock:\n\tif (same_inode)\n\t\tinode_unlock(inode_in);\n\telse\n\t\tbtrfs_double_inode_unlock(inode_in, inode_out);\n\n\treturn ret;\n}\n\nloff_t btrfs_remap_file_range(struct file *src_file, loff_t off,\n\t\tstruct file *dst_file, loff_t destoff, loff_t len,\n\t\tunsigned int remap_flags)\n{\n\tstruct inode *src_inode = file_inode(src_file);\n\tstruct inode *dst_inode = file_inode(dst_file);\n\tbool same_inode = dst_inode == src_inode;\n\tint ret;\n\n\tif (remap_flags & ~(REMAP_FILE_DEDUP | REMAP_FILE_ADVISORY))\n\t\treturn -EINVAL;\n\n\tret = btrfs_remap_file_range_prep(src_file, off, dst_file, destoff,\n\t\t\t\t\t  &len, remap_flags);\n\tif (ret < 0 || len == 0)\n\t\treturn ret;\n\n\tif (remap_flags & REMAP_FILE_DEDUP)\n\t\tret = btrfs_extent_same(src_inode, off, len, dst_inode, destoff);\n\telse\n\t\tret = btrfs_clone_files(dst_file, src_file, off, len, destoff);\n\n\tif (same_inode)\n\t\tinode_unlock(src_inode);\n\telse\n\t\tbtrfs_double_inode_unlock(src_inode, dst_inode);\n\n\treturn ret < 0 ? ret : len;\n}\n\nstatic long btrfs_ioctl_default_subvol(struct file *file, void __user *argp)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_root *new_root;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key location;\n\tstruct btrfs_disk_key disk_key;\n\tu64 objectid = 0;\n\tu64 dir_id;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (copy_from_user(&objectid, argp, sizeof(objectid))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (!objectid)\n\t\tobjectid = BTRFS_FS_TREE_OBJECTID;\n\n\tlocation.objectid = objectid;\n\tlocation.type = BTRFS_ROOT_ITEM_KEY;\n\tlocation.offset = (u64)-1;\n\n\tnew_root = btrfs_read_fs_root_no_name(fs_info, &location);\n\tif (IS_ERR(new_root)) {\n\t\tret = PTR_ERR(new_root);\n\t\tgoto out;\n\t}\n\tif (!is_fstree(new_root->root_key.objectid)) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tpath->leave_spinning = 1;\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tdir_id = btrfs_super_root_dir(fs_info->super_copy);\n\tdi = btrfs_lookup_dir_item(trans, fs_info->tree_root, path,\n\t\t\t\t   dir_id, \"default\", 7, 1);\n\tif (IS_ERR_OR_NULL(di)) {\n\t\tbtrfs_free_path(path);\n\t\tbtrfs_end_transaction(trans);\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"Umm, you don't have the default diritem, this isn't going to work\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tbtrfs_cpu_key_to_disk(&disk_key, &new_root->root_key);\n\tbtrfs_set_dir_item_key(path->nodes[0], di, &disk_key);\n\tbtrfs_mark_buffer_dirty(path->nodes[0]);\n\tbtrfs_free_path(path);\n\n\tbtrfs_set_fs_incompat(fs_info, DEFAULT_SUBVOL);\n\tbtrfs_end_transaction(trans);\nout:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic void get_block_group_info(struct list_head *groups_list,\n\t\t\t\t struct btrfs_ioctl_space_info *space)\n{\n\tstruct btrfs_block_group_cache *block_group;\n\n\tspace->total_bytes = 0;\n\tspace->used_bytes = 0;\n\tspace->flags = 0;\n\tlist_for_each_entry(block_group, groups_list, list) {\n\t\tspace->flags = block_group->flags;\n\t\tspace->total_bytes += block_group->key.offset;\n\t\tspace->used_bytes +=\n\t\t\tbtrfs_block_group_used(&block_group->item);\n\t}\n}\n\nstatic long btrfs_ioctl_space_info(struct btrfs_fs_info *fs_info,\n\t\t\t\t   void __user *arg)\n{\n\tstruct btrfs_ioctl_space_args space_args;\n\tstruct btrfs_ioctl_space_info space;\n\tstruct btrfs_ioctl_space_info *dest;\n\tstruct btrfs_ioctl_space_info *dest_orig;\n\tstruct btrfs_ioctl_space_info __user *user_dest;\n\tstruct btrfs_space_info *info;\n\tstatic const u64 types[] = {\n\t\tBTRFS_BLOCK_GROUP_DATA,\n\t\tBTRFS_BLOCK_GROUP_SYSTEM,\n\t\tBTRFS_BLOCK_GROUP_METADATA,\n\t\tBTRFS_BLOCK_GROUP_DATA | BTRFS_BLOCK_GROUP_METADATA\n\t};\n\tint num_types = 4;\n\tint alloc_size;\n\tint ret = 0;\n\tu64 slot_count = 0;\n\tint i, c;\n\n\tif (copy_from_user(&space_args,\n\t\t\t   (struct btrfs_ioctl_space_args __user *)arg,\n\t\t\t   sizeof(space_args)))\n\t\treturn -EFAULT;\n\n\tfor (i = 0; i < num_types; i++) {\n\t\tstruct btrfs_space_info *tmp;\n\n\t\tinfo = NULL;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(tmp, &fs_info->space_info,\n\t\t\t\t\tlist) {\n\t\t\tif (tmp->flags == types[i]) {\n\t\t\t\tinfo = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (!info)\n\t\t\tcontinue;\n\n\t\tdown_read(&info->groups_sem);\n\t\tfor (c = 0; c < BTRFS_NR_RAID_TYPES; c++) {\n\t\t\tif (!list_empty(&info->block_groups[c]))\n\t\t\t\tslot_count++;\n\t\t}\n\t\tup_read(&info->groups_sem);\n\t}\n\n\t/*\n\t * Global block reserve, exported as a space_info\n\t */\n\tslot_count++;\n\n\t/* space_slots == 0 means they are asking for a count */\n\tif (space_args.space_slots == 0) {\n\t\tspace_args.total_spaces = slot_count;\n\t\tgoto out;\n\t}\n\n\tslot_count = min_t(u64, space_args.space_slots, slot_count);\n\n\talloc_size = sizeof(*dest) * slot_count;\n\n\t/* we generally have at most 6 or so space infos, one for each raid\n\t * level.  So, a whole page should be more than enough for everyone\n\t */\n\tif (alloc_size > PAGE_SIZE)\n\t\treturn -ENOMEM;\n\n\tspace_args.total_spaces = 0;\n\tdest = kmalloc(alloc_size, GFP_KERNEL);\n\tif (!dest)\n\t\treturn -ENOMEM;\n\tdest_orig = dest;\n\n\t/* now we have a buffer to copy into */\n\tfor (i = 0; i < num_types; i++) {\n\t\tstruct btrfs_space_info *tmp;\n\n\t\tif (!slot_count)\n\t\t\tbreak;\n\n\t\tinfo = NULL;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(tmp, &fs_info->space_info,\n\t\t\t\t\tlist) {\n\t\t\tif (tmp->flags == types[i]) {\n\t\t\t\tinfo = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (!info)\n\t\t\tcontinue;\n\t\tdown_read(&info->groups_sem);\n\t\tfor (c = 0; c < BTRFS_NR_RAID_TYPES; c++) {\n\t\t\tif (!list_empty(&info->block_groups[c])) {\n\t\t\t\tget_block_group_info(&info->block_groups[c],\n\t\t\t\t\t\t     &space);\n\t\t\t\tmemcpy(dest, &space, sizeof(space));\n\t\t\t\tdest++;\n\t\t\t\tspace_args.total_spaces++;\n\t\t\t\tslot_count--;\n\t\t\t}\n\t\t\tif (!slot_count)\n\t\t\t\tbreak;\n\t\t}\n\t\tup_read(&info->groups_sem);\n\t}\n\n\t/*\n\t * Add global block reserve\n\t */\n\tif (slot_count) {\n\t\tstruct btrfs_block_rsv *block_rsv = &fs_info->global_block_rsv;\n\n\t\tspin_lock(&block_rsv->lock);\n\t\tspace.total_bytes = block_rsv->size;\n\t\tspace.used_bytes = block_rsv->size - block_rsv->reserved;\n\t\tspin_unlock(&block_rsv->lock);\n\t\tspace.flags = BTRFS_SPACE_INFO_GLOBAL_RSV;\n\t\tmemcpy(dest, &space, sizeof(space));\n\t\tspace_args.total_spaces++;\n\t}\n\n\tuser_dest = (struct btrfs_ioctl_space_info __user *)\n\t\t(arg + sizeof(struct btrfs_ioctl_space_args));\n\n\tif (copy_to_user(user_dest, dest_orig, alloc_size))\n\t\tret = -EFAULT;\n\n\tkfree(dest_orig);\nout:\n\tif (ret == 0 && copy_to_user(arg, &space_args, sizeof(space_args)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nstatic noinline long btrfs_ioctl_start_sync(struct btrfs_root *root,\n\t\t\t\t\t    void __user *argp)\n{\n\tstruct btrfs_trans_handle *trans;\n\tu64 transid;\n\tint ret;\n\n\ttrans = btrfs_attach_transaction_barrier(root);\n\tif (IS_ERR(trans)) {\n\t\tif (PTR_ERR(trans) != -ENOENT)\n\t\t\treturn PTR_ERR(trans);\n\n\t\t/* No running transaction, don't bother */\n\t\ttransid = root->fs_info->last_trans_committed;\n\t\tgoto out;\n\t}\n\ttransid = trans->transid;\n\tret = btrfs_commit_transaction_async(trans, 0);\n\tif (ret) {\n\t\tbtrfs_end_transaction(trans);\n\t\treturn ret;\n\t}\nout:\n\tif (argp)\n\t\tif (copy_to_user(argp, &transid, sizeof(transid)))\n\t\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic noinline long btrfs_ioctl_wait_sync(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t   void __user *argp)\n{\n\tu64 transid;\n\n\tif (argp) {\n\t\tif (copy_from_user(&transid, argp, sizeof(transid)))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\ttransid = 0;  /* current trans */\n\t}\n\treturn btrfs_wait_for_commit(fs_info, transid);\n}\n\nstatic long btrfs_ioctl_scrub(struct file *file, void __user *arg)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(file_inode(file)->i_sb);\n\tstruct btrfs_ioctl_scrub_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tif (!(sa->flags & BTRFS_SCRUB_READONLY)) {\n\t\tret = mnt_want_write_file(file);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = btrfs_scrub_dev(fs_info, sa->devid, sa->start, sa->end,\n\t\t\t      &sa->progress, sa->flags & BTRFS_SCRUB_READONLY,\n\t\t\t      0);\n\n\tif (ret == 0 && copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tif (!(sa->flags & BTRFS_SCRUB_READONLY))\n\t\tmnt_drop_write_file(file);\nout:\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_scrub_cancel(struct btrfs_fs_info *fs_info)\n{\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn btrfs_scrub_cancel(fs_info);\n}\n\nstatic long btrfs_ioctl_scrub_progress(struct btrfs_fs_info *fs_info,\n\t\t\t\t       void __user *arg)\n{\n\tstruct btrfs_ioctl_scrub_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tret = btrfs_scrub_progress(fs_info, sa->devid, &sa->progress);\n\n\tif (ret == 0 && copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\t\t      void __user *arg)\n{\n\tstruct btrfs_ioctl_get_dev_stats *sa;\n\tint ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tif ((sa->flags & BTRFS_DEV_STATS_RESET) && !capable(CAP_SYS_ADMIN)) {\n\t\tkfree(sa);\n\t\treturn -EPERM;\n\t}\n\n\tret = btrfs_get_dev_stats(fs_info, sa);\n\n\tif (ret == 0 && copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_dev_replace(struct btrfs_fs_info *fs_info,\n\t\t\t\t    void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_replace_args *p;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tp = memdup_user(arg, sizeof(*p));\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\tswitch (p->cmd) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_START:\n\t\tif (sb_rdonly(fs_info->sb)) {\n\t\t\tret = -EROFS;\n\t\t\tgoto out;\n\t\t}\n\t\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\t\tret = BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t\t} else {\n\t\t\tret = btrfs_dev_replace_by_ioctl(fs_info, p);\n\t\t\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\t\t}\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_STATUS:\n\t\tbtrfs_dev_replace_status(fs_info, p);\n\t\tret = 0;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_CANCEL:\n\t\tp->result = btrfs_dev_replace_cancel(fs_info);\n\t\tret = 0;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif ((ret == 0 || ret == -ECANCELED) && copy_to_user(arg, p, sizeof(*p)))\n\t\tret = -EFAULT;\nout:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_ino_to_path(struct btrfs_root *root, void __user *arg)\n{\n\tint ret = 0;\n\tint i;\n\tu64 rel_ptr;\n\tint size;\n\tstruct btrfs_ioctl_ino_path_args *ipa = NULL;\n\tstruct inode_fs_paths *ipath = NULL;\n\tstruct btrfs_path *path;\n\n\tif (!capable(CAP_DAC_READ_SEARCH))\n\t\treturn -EPERM;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tipa = memdup_user(arg, sizeof(*ipa));\n\tif (IS_ERR(ipa)) {\n\t\tret = PTR_ERR(ipa);\n\t\tipa = NULL;\n\t\tgoto out;\n\t}\n\n\tsize = min_t(u32, ipa->size, 4096);\n\tipath = init_ipath(size, root, path);\n\tif (IS_ERR(ipath)) {\n\t\tret = PTR_ERR(ipath);\n\t\tipath = NULL;\n\t\tgoto out;\n\t}\n\n\tret = paths_from_inode(ipa->inum, ipath);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tfor (i = 0; i < ipath->fspath->elem_cnt; ++i) {\n\t\trel_ptr = ipath->fspath->val[i] -\n\t\t\t  (u64)(unsigned long)ipath->fspath->val;\n\t\tipath->fspath->val[i] = rel_ptr;\n\t}\n\n\tret = copy_to_user((void __user *)(unsigned long)ipa->fspath,\n\t\t\t   ipath->fspath, size);\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tfree_ipath(ipath);\n\tkfree(ipa);\n\n\treturn ret;\n}\n\nstatic int build_ino_list(u64 inum, u64 offset, u64 root, void *ctx)\n{\n\tstruct btrfs_data_container *inodes = ctx;\n\tconst size_t c = 3 * sizeof(u64);\n\n\tif (inodes->bytes_left >= c) {\n\t\tinodes->bytes_left -= c;\n\t\tinodes->val[inodes->elem_cnt] = inum;\n\t\tinodes->val[inodes->elem_cnt + 1] = offset;\n\t\tinodes->val[inodes->elem_cnt + 2] = root;\n\t\tinodes->elem_cnt += 3;\n\t} else {\n\t\tinodes->bytes_missing += c - inodes->bytes_left;\n\t\tinodes->bytes_left = 0;\n\t\tinodes->elem_missed += 3;\n\t}\n\n\treturn 0;\n}\n\nstatic long btrfs_ioctl_logical_to_ino(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tvoid __user *arg, int version)\n{\n\tint ret = 0;\n\tint size;\n\tstruct btrfs_ioctl_logical_ino_args *loi;\n\tstruct btrfs_data_container *inodes = NULL;\n\tstruct btrfs_path *path = NULL;\n\tbool ignore_offset;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tloi = memdup_user(arg, sizeof(*loi));\n\tif (IS_ERR(loi))\n\t\treturn PTR_ERR(loi);\n\n\tif (version == 1) {\n\t\tignore_offset = false;\n\t\tsize = min_t(u32, loi->size, SZ_64K);\n\t} else {\n\t\t/* All reserved bits must be 0 for now */\n\t\tif (memchr_inv(loi->reserved, 0, sizeof(loi->reserved))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_loi;\n\t\t}\n\t\t/* Only accept flags we have defined so far */\n\t\tif (loi->flags & ~(BTRFS_LOGICAL_INO_ARGS_IGNORE_OFFSET)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_loi;\n\t\t}\n\t\tignore_offset = loi->flags & BTRFS_LOGICAL_INO_ARGS_IGNORE_OFFSET;\n\t\tsize = min_t(u32, loi->size, SZ_16M);\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tinodes = init_data_container(size);\n\tif (IS_ERR(inodes)) {\n\t\tret = PTR_ERR(inodes);\n\t\tinodes = NULL;\n\t\tgoto out;\n\t}\n\n\tret = iterate_inodes_from_logical(loi->logical, fs_info, path,\n\t\t\t\t\t  build_ino_list, inodes, ignore_offset);\n\tif (ret == -EINVAL)\n\t\tret = -ENOENT;\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = copy_to_user((void __user *)(unsigned long)loi->inodes, inodes,\n\t\t\t   size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tbtrfs_free_path(path);\n\tkvfree(inodes);\nout_loi:\n\tkfree(loi);\n\n\treturn ret;\n}\n\nvoid btrfs_update_ioctl_balance_args(struct btrfs_fs_info *fs_info,\n\t\t\t       struct btrfs_ioctl_balance_args *bargs)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\n\tbargs->flags = bctl->flags;\n\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_RUNNING;\n\tif (atomic_read(&fs_info->balance_pause_req))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_PAUSE_REQ;\n\tif (atomic_read(&fs_info->balance_cancel_req))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_CANCEL_REQ;\n\n\tmemcpy(&bargs->data, &bctl->data, sizeof(bargs->data));\n\tmemcpy(&bargs->meta, &bctl->meta, sizeof(bargs->meta));\n\tmemcpy(&bargs->sys, &bctl->sys, sizeof(bargs->sys));\n\n\tspin_lock(&fs_info->balance_lock);\n\tmemcpy(&bargs->stat, &bctl->stat, sizeof(bargs->stat));\n\tspin_unlock(&fs_info->balance_lock);\n}\n\nstatic long btrfs_ioctl_balance(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(file_inode(file))->root;\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_ioctl_balance_args *bargs;\n\tstruct btrfs_balance_control *bctl;\n\tbool need_unlock; /* for mut. excl. ops lock */\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\nagain:\n\tif (!test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t\tneed_unlock = true;\n\t\tgoto locked;\n\t}\n\n\t/*\n\t * mut. excl. ops lock is locked.  Three possibilities:\n\t *   (1) some other op is running\n\t *   (2) balance is running\n\t *   (3) balance is paused -- special case (think resume)\n\t */\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (fs_info->balance_ctl) {\n\t\t/* this is either (2) or (3) */\n\t\tif (!test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t\t/*\n\t\t\t * Lock released to allow other waiters to continue,\n\t\t\t * we'll reexamine the status again.\n\t\t\t */\n\t\t\tmutex_lock(&fs_info->balance_mutex);\n\n\t\t\tif (fs_info->balance_ctl &&\n\t\t\t    !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\t\t\t/* this is (3) */\n\t\t\t\tneed_unlock = false;\n\t\t\t\tgoto locked;\n\t\t\t}\n\n\t\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t\tgoto again;\n\t\t} else {\n\t\t\t/* this is (2) */\n\t\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t\tret = -EINPROGRESS;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t/* this is (1) */\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\tret = BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t\tgoto out;\n\t}\n\nlocked:\n\tBUG_ON(!test_bit(BTRFS_FS_EXCL_OP, &fs_info->flags));\n\n\tif (arg) {\n\t\tbargs = memdup_user(arg, sizeof(*bargs));\n\t\tif (IS_ERR(bargs)) {\n\t\t\tret = PTR_ERR(bargs);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (bargs->flags & BTRFS_BALANCE_RESUME) {\n\t\t\tif (!fs_info->balance_ctl) {\n\t\t\t\tret = -ENOTCONN;\n\t\t\t\tgoto out_bargs;\n\t\t\t}\n\n\t\t\tbctl = fs_info->balance_ctl;\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->flags |= BTRFS_BALANCE_RESUME;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\n\t\t\tgoto do_balance;\n\t\t}\n\t} else {\n\t\tbargs = NULL;\n\t}\n\n\tif (fs_info->balance_ctl) {\n\t\tret = -EINPROGRESS;\n\t\tgoto out_bargs;\n\t}\n\n\tbctl = kzalloc(sizeof(*bctl), GFP_KERNEL);\n\tif (!bctl) {\n\t\tret = -ENOMEM;\n\t\tgoto out_bargs;\n\t}\n\n\tif (arg) {\n\t\tmemcpy(&bctl->data, &bargs->data, sizeof(bctl->data));\n\t\tmemcpy(&bctl->meta, &bargs->meta, sizeof(bctl->meta));\n\t\tmemcpy(&bctl->sys, &bargs->sys, sizeof(bctl->sys));\n\n\t\tbctl->flags = bargs->flags;\n\t} else {\n\t\t/* balance everything - no filters */\n\t\tbctl->flags |= BTRFS_BALANCE_TYPE_MASK;\n\t}\n\n\tif (bctl->flags & ~(BTRFS_BALANCE_ARGS_MASK | BTRFS_BALANCE_TYPE_MASK)) {\n\t\tret = -EINVAL;\n\t\tgoto out_bctl;\n\t}\n\ndo_balance:\n\t/*\n\t * Ownership of bctl and filesystem flag BTRFS_FS_EXCL_OP goes to\n\t * btrfs_balance.  bctl is freed in reset_balance_state, or, if\n\t * restriper was paused all the way until unmount, in free_fs_info.\n\t * The flag should be cleared after reset_balance_state.\n\t */\n\tneed_unlock = false;\n\n\tret = btrfs_balance(fs_info, bctl, bargs);\n\tbctl = NULL;\n\n\tif ((ret == 0 || ret == -ECANCELED) && arg) {\n\t\tif (copy_to_user(arg, bargs, sizeof(*bargs)))\n\t\t\tret = -EFAULT;\n\t}\n\nout_bctl:\n\tkfree(bctl);\nout_bargs:\n\tkfree(bargs);\nout_unlock:\n\tmutex_unlock(&fs_info->balance_mutex);\n\tif (need_unlock)\n\t\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\nout:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_balance_ctl(struct btrfs_fs_info *fs_info, int cmd)\n{\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tswitch (cmd) {\n\tcase BTRFS_BALANCE_CTL_PAUSE:\n\t\treturn btrfs_pause_balance(fs_info);\n\tcase BTRFS_BALANCE_CTL_CANCEL:\n\t\treturn btrfs_cancel_balance(fs_info);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic long btrfs_ioctl_balance_progress(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t void __user *arg)\n{\n\tstruct btrfs_ioctl_balance_args *bargs;\n\tint ret = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tret = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\tbargs = kzalloc(sizeof(*bargs), GFP_KERNEL);\n\tif (!bargs) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tbtrfs_update_ioctl_balance_args(fs_info, bargs);\n\n\tif (copy_to_user(arg, bargs, sizeof(*bargs)))\n\t\tret = -EFAULT;\n\n\tkfree(bargs);\nout:\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_ctl(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_quota_ctl_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\tdown_write(&fs_info->subvol_sem);\n\n\tswitch (sa->cmd) {\n\tcase BTRFS_QUOTA_CTL_ENABLE:\n\t\tret = btrfs_quota_enable(fs_info);\n\t\tbreak;\n\tcase BTRFS_QUOTA_CTL_DISABLE:\n\t\tret = btrfs_quota_disable(fs_info);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tkfree(sa);\n\tup_write(&fs_info->subvol_sem);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_assign(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_qgroup_assign_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tif (sa->assign) {\n\t\tret = btrfs_add_qgroup_relation(trans, sa->src, sa->dst);\n\t} else {\n\t\tret = btrfs_del_qgroup_relation(trans, sa->src, sa->dst);\n\t}\n\n\t/* update qgroup status and info */\n\terr = btrfs_run_qgroups(trans);\n\tif (err < 0)\n\t\tbtrfs_handle_fs_error(fs_info, err,\n\t\t\t\t      \"failed to update qgroup status and info\");\n\terr = btrfs_end_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_create(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_qgroup_create_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\tif (!sa->qgroupid) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tif (sa->create) {\n\t\tret = btrfs_create_qgroup(trans, sa->qgroupid);\n\t} else {\n\t\tret = btrfs_remove_qgroup(trans, sa->qgroupid);\n\t}\n\n\terr = btrfs_end_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_limit(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_qgroup_limit_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\tu64 qgroupid;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tqgroupid = sa->qgroupid;\n\tif (!qgroupid) {\n\t\t/* take the current subvol as qgroup */\n\t\tqgroupid = root->root_key.objectid;\n\t}\n\n\tret = btrfs_limit_qgroup(trans, qgroupid, &sa->lim);\n\n\terr = btrfs_end_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_rescan(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_quota_rescan_args *qsa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tqsa = memdup_user(arg, sizeof(*qsa));\n\tif (IS_ERR(qsa)) {\n\t\tret = PTR_ERR(qsa);\n\t\tgoto drop_write;\n\t}\n\n\tif (qsa->flags) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_qgroup_rescan(fs_info);\n\nout:\n\tkfree(qsa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_rescan_status(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_quota_rescan_args *qsa;\n\tint ret = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tqsa = kzalloc(sizeof(*qsa), GFP_KERNEL);\n\tif (!qsa)\n\t\treturn -ENOMEM;\n\n\tif (fs_info->qgroup_flags & BTRFS_QGROUP_STATUS_FLAG_RESCAN) {\n\t\tqsa->flags = 1;\n\t\tqsa->progress = fs_info->qgroup_rescan_progress.objectid;\n\t}\n\n\tif (copy_to_user(arg, qsa, sizeof(*qsa)))\n\t\tret = -EFAULT;\n\n\tkfree(qsa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_rescan_wait(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn btrfs_qgroup_wait_for_completion(fs_info, true);\n}\n\nstatic long _btrfs_ioctl_set_received_subvol(struct file *file,\n\t\t\t\t\t    struct btrfs_ioctl_received_subvol_args *sa)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_root_item *root_item = &root->root_item;\n\tstruct btrfs_trans_handle *trans;\n\tstruct timespec64 ct = current_time(inode);\n\tint ret = 0;\n\tint received_uuid_changed;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tdown_write(&fs_info->subvol_sem);\n\n\tif (btrfs_ino(BTRFS_I(inode)) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (btrfs_root_readonly(root)) {\n\t\tret = -EROFS;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * 1 - root item\n\t * 2 - uuid items (received uuid + subvol uuid)\n\t */\n\ttrans = btrfs_start_transaction(root, 3);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\ttrans = NULL;\n\t\tgoto out;\n\t}\n\n\tsa->rtransid = trans->transid;\n\tsa->rtime.sec = ct.tv_sec;\n\tsa->rtime.nsec = ct.tv_nsec;\n\n\treceived_uuid_changed = memcmp(root_item->received_uuid, sa->uuid,\n\t\t\t\t       BTRFS_UUID_SIZE);\n\tif (received_uuid_changed &&\n\t    !btrfs_is_empty_uuid(root_item->received_uuid)) {\n\t\tret = btrfs_uuid_tree_remove(trans, root_item->received_uuid,\n\t\t\t\t\t  BTRFS_UUID_KEY_RECEIVED_SUBVOL,\n\t\t\t\t\t  root->root_key.objectid);\n\t\tif (ret && ret != -ENOENT) {\n\t\t        btrfs_abort_transaction(trans, ret);\n\t\t        btrfs_end_transaction(trans);\n\t\t        goto out;\n\t\t}\n\t}\n\tmemcpy(root_item->received_uuid, sa->uuid, BTRFS_UUID_SIZE);\n\tbtrfs_set_root_stransid(root_item, sa->stransid);\n\tbtrfs_set_root_rtransid(root_item, sa->rtransid);\n\tbtrfs_set_stack_timespec_sec(&root_item->stime, sa->stime.sec);\n\tbtrfs_set_stack_timespec_nsec(&root_item->stime, sa->stime.nsec);\n\tbtrfs_set_stack_timespec_sec(&root_item->rtime, sa->rtime.sec);\n\tbtrfs_set_stack_timespec_nsec(&root_item->rtime, sa->rtime.nsec);\n\n\tret = btrfs_update_root(trans, fs_info->tree_root,\n\t\t\t\t&root->root_key, &root->root_item);\n\tif (ret < 0) {\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out;\n\t}\n\tif (received_uuid_changed && !btrfs_is_empty_uuid(sa->uuid)) {\n\t\tret = btrfs_uuid_tree_add(trans, sa->uuid,\n\t\t\t\t\t  BTRFS_UUID_KEY_RECEIVED_SUBVOL,\n\t\t\t\t\t  root->root_key.objectid);\n\t\tif (ret < 0 && ret != -EEXIST) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tbtrfs_end_transaction(trans);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tret = btrfs_commit_transaction(trans);\nout:\n\tup_write(&fs_info->subvol_sem);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\n#ifdef CONFIG_64BIT\nstatic long btrfs_ioctl_set_received_subvol_32(struct file *file,\n\t\t\t\t\t\tvoid __user *arg)\n{\n\tstruct btrfs_ioctl_received_subvol_args_32 *args32 = NULL;\n\tstruct btrfs_ioctl_received_subvol_args *args64 = NULL;\n\tint ret = 0;\n\n\targs32 = memdup_user(arg, sizeof(*args32));\n\tif (IS_ERR(args32))\n\t\treturn PTR_ERR(args32);\n\n\targs64 = kmalloc(sizeof(*args64), GFP_KERNEL);\n\tif (!args64) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmemcpy(args64->uuid, args32->uuid, BTRFS_UUID_SIZE);\n\targs64->stransid = args32->stransid;\n\targs64->rtransid = args32->rtransid;\n\targs64->stime.sec = args32->stime.sec;\n\targs64->stime.nsec = args32->stime.nsec;\n\targs64->rtime.sec = args32->rtime.sec;\n\targs64->rtime.nsec = args32->rtime.nsec;\n\targs64->flags = args32->flags;\n\n\tret = _btrfs_ioctl_set_received_subvol(file, args64);\n\tif (ret)\n\t\tgoto out;\n\n\tmemcpy(args32->uuid, args64->uuid, BTRFS_UUID_SIZE);\n\targs32->stransid = args64->stransid;\n\targs32->rtransid = args64->rtransid;\n\targs32->stime.sec = args64->stime.sec;\n\targs32->stime.nsec = args64->stime.nsec;\n\targs32->rtime.sec = args64->rtime.sec;\n\targs32->rtime.nsec = args64->rtime.nsec;\n\targs32->flags = args64->flags;\n\n\tret = copy_to_user(arg, args32, sizeof(*args32));\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(args32);\n\tkfree(args64);\n\treturn ret;\n}\n#endif\n\nstatic long btrfs_ioctl_set_received_subvol(struct file *file,\n\t\t\t\t\t    void __user *arg)\n{\n\tstruct btrfs_ioctl_received_subvol_args *sa = NULL;\n\tint ret = 0;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tret = _btrfs_ioctl_set_received_subvol(file, sa);\n\n\tif (ret)\n\t\tgoto out;\n\n\tret = copy_to_user(arg, sa, sizeof(*sa));\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic int btrfs_ioctl_get_fslabel(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tsize_t len;\n\tint ret;\n\tchar label[BTRFS_LABEL_SIZE];\n\n\tspin_lock(&fs_info->super_lock);\n\tmemcpy(label, fs_info->super_copy->label, BTRFS_LABEL_SIZE);\n\tspin_unlock(&fs_info->super_lock);\n\n\tlen = strnlen(label, BTRFS_LABEL_SIZE);\n\n\tif (len == BTRFS_LABEL_SIZE) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t   \"label is too long, return the first %zu bytes\",\n\t\t\t   --len);\n\t}\n\n\tret = copy_to_user(arg, label, len);\n\n\treturn ret ? -EFAULT : 0;\n}\n\nstatic int btrfs_ioctl_set_fslabel(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_super_block *super_block = fs_info->super_copy;\n\tstruct btrfs_trans_handle *trans;\n\tchar label[BTRFS_LABEL_SIZE];\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (copy_from_user(label, arg, sizeof(label)))\n\t\treturn -EFAULT;\n\n\tif (strnlen(label, BTRFS_LABEL_SIZE) == BTRFS_LABEL_SIZE) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"unable to set label with more than %d bytes\",\n\t\t\t  BTRFS_LABEL_SIZE - 1);\n\t\treturn -EINVAL;\n\t}\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_unlock;\n\t}\n\n\tspin_lock(&fs_info->super_lock);\n\tstrcpy(super_block->label, label);\n\tspin_unlock(&fs_info->super_lock);\n\tret = btrfs_commit_transaction(trans);\n\nout_unlock:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\n#define INIT_FEATURE_FLAGS(suffix) \\\n\t{ .compat_flags = BTRFS_FEATURE_COMPAT_##suffix, \\\n\t  .compat_ro_flags = BTRFS_FEATURE_COMPAT_RO_##suffix, \\\n\t  .incompat_flags = BTRFS_FEATURE_INCOMPAT_##suffix }\n\nint btrfs_ioctl_get_supported_features(void __user *arg)\n{\n\tstatic const struct btrfs_ioctl_feature_flags features[3] = {\n\t\tINIT_FEATURE_FLAGS(SUPP),\n\t\tINIT_FEATURE_FLAGS(SAFE_SET),\n\t\tINIT_FEATURE_FLAGS(SAFE_CLEAR)\n\t};\n\n\tif (copy_to_user(arg, &features, sizeof(features)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int btrfs_ioctl_get_features(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_super_block *super_block = fs_info->super_copy;\n\tstruct btrfs_ioctl_feature_flags features;\n\n\tfeatures.compat_flags = btrfs_super_compat_flags(super_block);\n\tfeatures.compat_ro_flags = btrfs_super_compat_ro_flags(super_block);\n\tfeatures.incompat_flags = btrfs_super_incompat_flags(super_block);\n\n\tif (copy_to_user(arg, &features, sizeof(features)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int check_feature_bits(struct btrfs_fs_info *fs_info,\n\t\t\t      enum btrfs_feature_set set,\n\t\t\t      u64 change_mask, u64 flags, u64 supported_flags,\n\t\t\t      u64 safe_set, u64 safe_clear)\n{\n\tconst char *type = btrfs_feature_set_names[set];\n\tchar *names;\n\tu64 disallowed, unsupported;\n\tu64 set_mask = flags & change_mask;\n\tu64 clear_mask = ~flags & change_mask;\n\n\tunsupported = set_mask & ~supported_flags;\n\tif (unsupported) {\n\t\tnames = btrfs_printable_features(set, unsupported);\n\t\tif (names) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"this kernel does not support the %s feature bit%s\",\n\t\t\t\t   names, strchr(names, ',') ? \"s\" : \"\");\n\t\t\tkfree(names);\n\t\t} else\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"this kernel does not support %s bits 0x%llx\",\n\t\t\t\t   type, unsupported);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tdisallowed = set_mask & ~safe_set;\n\tif (disallowed) {\n\t\tnames = btrfs_printable_features(set, disallowed);\n\t\tif (names) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"can't set the %s feature bit%s while mounted\",\n\t\t\t\t   names, strchr(names, ',') ? \"s\" : \"\");\n\t\t\tkfree(names);\n\t\t} else\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"can't set %s bits 0x%llx while mounted\",\n\t\t\t\t   type, disallowed);\n\t\treturn -EPERM;\n\t}\n\n\tdisallowed = clear_mask & ~safe_clear;\n\tif (disallowed) {\n\t\tnames = btrfs_printable_features(set, disallowed);\n\t\tif (names) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"can't clear the %s feature bit%s while mounted\",\n\t\t\t\t   names, strchr(names, ',') ? \"s\" : \"\");\n\t\t\tkfree(names);\n\t\t} else\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"can't clear %s bits 0x%llx while mounted\",\n\t\t\t\t   type, disallowed);\n\t\treturn -EPERM;\n\t}\n\n\treturn 0;\n}\n\n#define check_feature(fs_info, change_mask, flags, mask_base)\t\\\ncheck_feature_bits(fs_info, FEAT_##mask_base, change_mask, flags,\t\\\n\t\t   BTRFS_FEATURE_ ## mask_base ## _SUPP,\t\\\n\t\t   BTRFS_FEATURE_ ## mask_base ## _SAFE_SET,\t\\\n\t\t   BTRFS_FEATURE_ ## mask_base ## _SAFE_CLEAR)\n\nstatic int btrfs_ioctl_set_features(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_super_block *super_block = fs_info->super_copy;\n\tstruct btrfs_ioctl_feature_flags flags[2];\n\tstruct btrfs_trans_handle *trans;\n\tu64 newflags;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (copy_from_user(flags, arg, sizeof(flags)))\n\t\treturn -EFAULT;\n\n\t/* Nothing to do */\n\tif (!flags[0].compat_flags && !flags[0].compat_ro_flags &&\n\t    !flags[0].incompat_flags)\n\t\treturn 0;\n\n\tret = check_feature(fs_info, flags[0].compat_flags,\n\t\t\t    flags[1].compat_flags, COMPAT);\n\tif (ret)\n\t\treturn ret;\n\n\tret = check_feature(fs_info, flags[0].compat_ro_flags,\n\t\t\t    flags[1].compat_ro_flags, COMPAT_RO);\n\tif (ret)\n\t\treturn ret;\n\n\tret = check_feature(fs_info, flags[0].incompat_flags,\n\t\t\t    flags[1].incompat_flags, INCOMPAT);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_drop_write;\n\t}\n\n\tspin_lock(&fs_info->super_lock);\n\tnewflags = btrfs_super_compat_flags(super_block);\n\tnewflags |= flags[0].compat_flags & flags[1].compat_flags;\n\tnewflags &= ~(flags[0].compat_flags & ~flags[1].compat_flags);\n\tbtrfs_set_super_compat_flags(super_block, newflags);\n\n\tnewflags = btrfs_super_compat_ro_flags(super_block);\n\tnewflags |= flags[0].compat_ro_flags & flags[1].compat_ro_flags;\n\tnewflags &= ~(flags[0].compat_ro_flags & ~flags[1].compat_ro_flags);\n\tbtrfs_set_super_compat_ro_flags(super_block, newflags);\n\n\tnewflags = btrfs_super_incompat_flags(super_block);\n\tnewflags |= flags[0].incompat_flags & flags[1].incompat_flags;\n\tnewflags &= ~(flags[0].incompat_flags & ~flags[1].incompat_flags);\n\tbtrfs_set_super_incompat_flags(super_block, newflags);\n\tspin_unlock(&fs_info->super_lock);\n\n\tret = btrfs_commit_transaction(trans);\nout_drop_write:\n\tmnt_drop_write_file(file);\n\n\treturn ret;\n}\n\nstatic int _btrfs_ioctl_send(struct file *file, void __user *argp, bool compat)\n{\n\tstruct btrfs_ioctl_send_args *arg;\n\tint ret;\n\n\tif (compat) {\n#if defined(CONFIG_64BIT) && defined(CONFIG_COMPAT)\n\t\tstruct btrfs_ioctl_send_args_32 args32;\n\n\t\tret = copy_from_user(&args32, argp, sizeof(args32));\n\t\tif (ret)\n\t\t\treturn -EFAULT;\n\t\targ = kzalloc(sizeof(*arg), GFP_KERNEL);\n\t\tif (!arg)\n\t\t\treturn -ENOMEM;\n\t\targ->send_fd = args32.send_fd;\n\t\targ->clone_sources_count = args32.clone_sources_count;\n\t\targ->clone_sources = compat_ptr(args32.clone_sources);\n\t\targ->parent_root = args32.parent_root;\n\t\targ->flags = args32.flags;\n\t\tmemcpy(arg->reserved, args32.reserved,\n\t\t       sizeof(args32.reserved));\n#else\n\t\treturn -ENOTTY;\n#endif\n\t} else {\n\t\targ = memdup_user(argp, sizeof(*arg));\n\t\tif (IS_ERR(arg))\n\t\t\treturn PTR_ERR(arg);\n\t}\n\tret = btrfs_ioctl_send(file, arg);\n\tkfree(arg);\n\treturn ret;\n}\n\nlong btrfs_ioctl(struct file *file, unsigned int\n\t\tcmd, unsigned long arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (cmd) {\n\tcase FS_IOC_GETFLAGS:\n\t\treturn btrfs_ioctl_getflags(file, argp);\n\tcase FS_IOC_SETFLAGS:\n\t\treturn btrfs_ioctl_setflags(file, argp);\n\tcase FS_IOC_GETVERSION:\n\t\treturn btrfs_ioctl_getversion(file, argp);\n\tcase FITRIM:\n\t\treturn btrfs_ioctl_fitrim(file, argp);\n\tcase BTRFS_IOC_SNAP_CREATE:\n\t\treturn btrfs_ioctl_snap_create(file, argp, 0);\n\tcase BTRFS_IOC_SNAP_CREATE_V2:\n\t\treturn btrfs_ioctl_snap_create_v2(file, argp, 0);\n\tcase BTRFS_IOC_SUBVOL_CREATE:\n\t\treturn btrfs_ioctl_snap_create(file, argp, 1);\n\tcase BTRFS_IOC_SUBVOL_CREATE_V2:\n\t\treturn btrfs_ioctl_snap_create_v2(file, argp, 1);\n\tcase BTRFS_IOC_SNAP_DESTROY:\n\t\treturn btrfs_ioctl_snap_destroy(file, argp);\n\tcase BTRFS_IOC_SUBVOL_GETFLAGS:\n\t\treturn btrfs_ioctl_subvol_getflags(file, argp);\n\tcase BTRFS_IOC_SUBVOL_SETFLAGS:\n\t\treturn btrfs_ioctl_subvol_setflags(file, argp);\n\tcase BTRFS_IOC_DEFAULT_SUBVOL:\n\t\treturn btrfs_ioctl_default_subvol(file, argp);\n\tcase BTRFS_IOC_DEFRAG:\n\t\treturn btrfs_ioctl_defrag(file, NULL);\n\tcase BTRFS_IOC_DEFRAG_RANGE:\n\t\treturn btrfs_ioctl_defrag(file, argp);\n\tcase BTRFS_IOC_RESIZE:\n\t\treturn btrfs_ioctl_resize(file, argp);\n\tcase BTRFS_IOC_ADD_DEV:\n\t\treturn btrfs_ioctl_add_dev(fs_info, argp);\n\tcase BTRFS_IOC_RM_DEV:\n\t\treturn btrfs_ioctl_rm_dev(file, argp);\n\tcase BTRFS_IOC_RM_DEV_V2:\n\t\treturn btrfs_ioctl_rm_dev_v2(file, argp);\n\tcase BTRFS_IOC_FS_INFO:\n\t\treturn btrfs_ioctl_fs_info(fs_info, argp);\n\tcase BTRFS_IOC_DEV_INFO:\n\t\treturn btrfs_ioctl_dev_info(fs_info, argp);\n\tcase BTRFS_IOC_BALANCE:\n\t\treturn btrfs_ioctl_balance(file, NULL);\n\tcase BTRFS_IOC_TREE_SEARCH:\n\t\treturn btrfs_ioctl_tree_search(file, argp);\n\tcase BTRFS_IOC_TREE_SEARCH_V2:\n\t\treturn btrfs_ioctl_tree_search_v2(file, argp);\n\tcase BTRFS_IOC_INO_LOOKUP:\n\t\treturn btrfs_ioctl_ino_lookup(file, argp);\n\tcase BTRFS_IOC_INO_PATHS:\n\t\treturn btrfs_ioctl_ino_to_path(root, argp);\n\tcase BTRFS_IOC_LOGICAL_INO:\n\t\treturn btrfs_ioctl_logical_to_ino(fs_info, argp, 1);\n\tcase BTRFS_IOC_LOGICAL_INO_V2:\n\t\treturn btrfs_ioctl_logical_to_ino(fs_info, argp, 2);\n\tcase BTRFS_IOC_SPACE_INFO:\n\t\treturn btrfs_ioctl_space_info(fs_info, argp);\n\tcase BTRFS_IOC_SYNC: {\n\t\tint ret;\n\n\t\tret = btrfs_start_delalloc_roots(fs_info, -1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = btrfs_sync_fs(inode->i_sb, 1);\n\t\t/*\n\t\t * The transaction thread may want to do more work,\n\t\t * namely it pokes the cleaner kthread that will start\n\t\t * processing uncleaned subvols.\n\t\t */\n\t\twake_up_process(fs_info->transaction_kthread);\n\t\treturn ret;\n\t}\n\tcase BTRFS_IOC_START_SYNC:\n\t\treturn btrfs_ioctl_start_sync(root, argp);\n\tcase BTRFS_IOC_WAIT_SYNC:\n\t\treturn btrfs_ioctl_wait_sync(fs_info, argp);\n\tcase BTRFS_IOC_SCRUB:\n\t\treturn btrfs_ioctl_scrub(file, argp);\n\tcase BTRFS_IOC_SCRUB_CANCEL:\n\t\treturn btrfs_ioctl_scrub_cancel(fs_info);\n\tcase BTRFS_IOC_SCRUB_PROGRESS:\n\t\treturn btrfs_ioctl_scrub_progress(fs_info, argp);\n\tcase BTRFS_IOC_BALANCE_V2:\n\t\treturn btrfs_ioctl_balance(file, argp);\n\tcase BTRFS_IOC_BALANCE_CTL:\n\t\treturn btrfs_ioctl_balance_ctl(fs_info, arg);\n\tcase BTRFS_IOC_BALANCE_PROGRESS:\n\t\treturn btrfs_ioctl_balance_progress(fs_info, argp);\n\tcase BTRFS_IOC_SET_RECEIVED_SUBVOL:\n\t\treturn btrfs_ioctl_set_received_subvol(file, argp);\n#ifdef CONFIG_64BIT\n\tcase BTRFS_IOC_SET_RECEIVED_SUBVOL_32:\n\t\treturn btrfs_ioctl_set_received_subvol_32(file, argp);\n#endif\n\tcase BTRFS_IOC_SEND:\n\t\treturn _btrfs_ioctl_send(file, argp, false);\n#if defined(CONFIG_64BIT) && defined(CONFIG_COMPAT)\n\tcase BTRFS_IOC_SEND_32:\n\t\treturn _btrfs_ioctl_send(file, argp, true);\n#endif\n\tcase BTRFS_IOC_GET_DEV_STATS:\n\t\treturn btrfs_ioctl_get_dev_stats(fs_info, argp);\n\tcase BTRFS_IOC_QUOTA_CTL:\n\t\treturn btrfs_ioctl_quota_ctl(file, argp);\n\tcase BTRFS_IOC_QGROUP_ASSIGN:\n\t\treturn btrfs_ioctl_qgroup_assign(file, argp);\n\tcase BTRFS_IOC_QGROUP_CREATE:\n\t\treturn btrfs_ioctl_qgroup_create(file, argp);\n\tcase BTRFS_IOC_QGROUP_LIMIT:\n\t\treturn btrfs_ioctl_qgroup_limit(file, argp);\n\tcase BTRFS_IOC_QUOTA_RESCAN:\n\t\treturn btrfs_ioctl_quota_rescan(file, argp);\n\tcase BTRFS_IOC_QUOTA_RESCAN_STATUS:\n\t\treturn btrfs_ioctl_quota_rescan_status(file, argp);\n\tcase BTRFS_IOC_QUOTA_RESCAN_WAIT:\n\t\treturn btrfs_ioctl_quota_rescan_wait(file, argp);\n\tcase BTRFS_IOC_DEV_REPLACE:\n\t\treturn btrfs_ioctl_dev_replace(fs_info, argp);\n\tcase BTRFS_IOC_GET_FSLABEL:\n\t\treturn btrfs_ioctl_get_fslabel(file, argp);\n\tcase BTRFS_IOC_SET_FSLABEL:\n\t\treturn btrfs_ioctl_set_fslabel(file, argp);\n\tcase BTRFS_IOC_GET_SUPPORTED_FEATURES:\n\t\treturn btrfs_ioctl_get_supported_features(argp);\n\tcase BTRFS_IOC_GET_FEATURES:\n\t\treturn btrfs_ioctl_get_features(file, argp);\n\tcase BTRFS_IOC_SET_FEATURES:\n\t\treturn btrfs_ioctl_set_features(file, argp);\n\tcase FS_IOC_FSGETXATTR:\n\t\treturn btrfs_ioctl_fsgetxattr(file, argp);\n\tcase FS_IOC_FSSETXATTR:\n\t\treturn btrfs_ioctl_fssetxattr(file, argp);\n\tcase BTRFS_IOC_GET_SUBVOL_INFO:\n\t\treturn btrfs_ioctl_get_subvol_info(file, argp);\n\tcase BTRFS_IOC_GET_SUBVOL_ROOTREF:\n\t\treturn btrfs_ioctl_get_subvol_rootref(file, argp);\n\tcase BTRFS_IOC_INO_LOOKUP_USER:\n\t\treturn btrfs_ioctl_ino_lookup_user(file, argp);\n\t}\n\n\treturn -ENOTTY;\n}\n\n#ifdef CONFIG_COMPAT\nlong btrfs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\t/*\n\t * These all access 32-bit values anyway so no further\n\t * handling is necessary.\n\t */\n\tswitch (cmd) {\n\tcase FS_IOC32_GETFLAGS:\n\t\tcmd = FS_IOC_GETFLAGS;\n\t\tbreak;\n\tcase FS_IOC32_SETFLAGS:\n\t\tcmd = FS_IOC_SETFLAGS;\n\t\tbreak;\n\tcase FS_IOC32_GETVERSION:\n\t\tcmd = FS_IOC_GETVERSION;\n\t\tbreak;\n\t}\n\n\treturn btrfs_ioctl(file, cmd, (unsigned long) compat_ptr(arg));\n}\n#endif\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2011, 2012 STRATO.  All rights reserved.\n */\n\n#include <linux/blkdev.h>\n#include <linux/ratelimit.h>\n#include <linux/sched/mm.h>\n#include \"ctree.h\"\n#include \"volumes.h\"\n#include \"disk-io.h\"\n#include \"ordered-data.h\"\n#include \"transaction.h\"\n#include \"backref.h\"\n#include \"extent_io.h\"\n#include \"dev-replace.h\"\n#include \"check-integrity.h\"\n#include \"rcu-string.h\"\n#include \"raid56.h\"\n\n/*\n * This is only the first step towards a full-features scrub. It reads all\n * extent and super block and verifies the checksums. In case a bad checksum\n * is found or the extent cannot be read, good data will be written back if\n * any can be found.\n *\n * Future enhancements:\n *  - In case an unrepairable extent is encountered, track which files are\n *    affected and report them\n *  - track and record media errors, throw out bad devices\n *  - add a mode to also read unallocated space\n */\n\nstruct scrub_block;\nstruct scrub_ctx;\n\n/*\n * the following three values only influence the performance.\n * The last one configures the number of parallel and outstanding I/O\n * operations. The first two values configure an upper limit for the number\n * of (dynamically allocated) pages that are added to a bio.\n */\n#define SCRUB_PAGES_PER_RD_BIO\t32\t/* 128k per bio */\n#define SCRUB_PAGES_PER_WR_BIO\t32\t/* 128k per bio */\n#define SCRUB_BIOS_PER_SCTX\t64\t/* 8MB per device in flight */\n\n/*\n * the following value times PAGE_SIZE needs to be large enough to match the\n * largest node/leaf/sector size that shall be supported.\n * Values larger than BTRFS_STRIPE_LEN are not supported.\n */\n#define SCRUB_MAX_PAGES_PER_BLOCK\t16\t/* 64k per node/leaf/sector */\n\nstruct scrub_recover {\n\trefcount_t\t\trefs;\n\tstruct btrfs_bio\t*bbio;\n\tu64\t\t\tmap_length;\n};\n\nstruct scrub_page {\n\tstruct scrub_block\t*sblock;\n\tstruct page\t\t*page;\n\tstruct btrfs_device\t*dev;\n\tstruct list_head\tlist;\n\tu64\t\t\tflags;  /* extent flags */\n\tu64\t\t\tgeneration;\n\tu64\t\t\tlogical;\n\tu64\t\t\tphysical;\n\tu64\t\t\tphysical_for_dev_replace;\n\tatomic_t\t\trefs;\n\tstruct {\n\t\tunsigned int\tmirror_num:8;\n\t\tunsigned int\thave_csum:1;\n\t\tunsigned int\tio_error:1;\n\t};\n\tu8\t\t\tcsum[BTRFS_CSUM_SIZE];\n\n\tstruct scrub_recover\t*recover;\n};\n\nstruct scrub_bio {\n\tint\t\t\tindex;\n\tstruct scrub_ctx\t*sctx;\n\tstruct btrfs_device\t*dev;\n\tstruct bio\t\t*bio;\n\tblk_status_t\t\tstatus;\n\tu64\t\t\tlogical;\n\tu64\t\t\tphysical;\n#if SCRUB_PAGES_PER_WR_BIO >= SCRUB_PAGES_PER_RD_BIO\n\tstruct scrub_page\t*pagev[SCRUB_PAGES_PER_WR_BIO];\n#else\n\tstruct scrub_page\t*pagev[SCRUB_PAGES_PER_RD_BIO];\n#endif\n\tint\t\t\tpage_count;\n\tint\t\t\tnext_free;\n\tstruct btrfs_work\twork;\n};\n\nstruct scrub_block {\n\tstruct scrub_page\t*pagev[SCRUB_MAX_PAGES_PER_BLOCK];\n\tint\t\t\tpage_count;\n\tatomic_t\t\toutstanding_pages;\n\trefcount_t\t\trefs; /* free mem on transition to zero */\n\tstruct scrub_ctx\t*sctx;\n\tstruct scrub_parity\t*sparity;\n\tstruct {\n\t\tunsigned int\theader_error:1;\n\t\tunsigned int\tchecksum_error:1;\n\t\tunsigned int\tno_io_error_seen:1;\n\t\tunsigned int\tgeneration_error:1; /* also sets header_error */\n\n\t\t/* The following is for the data used to check parity */\n\t\t/* It is for the data with checksum */\n\t\tunsigned int\tdata_corrected:1;\n\t};\n\tstruct btrfs_work\twork;\n};\n\n/* Used for the chunks with parity stripe such RAID5/6 */\nstruct scrub_parity {\n\tstruct scrub_ctx\t*sctx;\n\n\tstruct btrfs_device\t*scrub_dev;\n\n\tu64\t\t\tlogic_start;\n\n\tu64\t\t\tlogic_end;\n\n\tint\t\t\tnsectors;\n\n\tu64\t\t\tstripe_len;\n\n\trefcount_t\t\trefs;\n\n\tstruct list_head\tspages;\n\n\t/* Work of parity check and repair */\n\tstruct btrfs_work\twork;\n\n\t/* Mark the parity blocks which have data */\n\tunsigned long\t\t*dbitmap;\n\n\t/*\n\t * Mark the parity blocks which have data, but errors happen when\n\t * read data or check data\n\t */\n\tunsigned long\t\t*ebitmap;\n\n\tunsigned long\t\tbitmap[0];\n};\n\nstruct scrub_ctx {\n\tstruct scrub_bio\t*bios[SCRUB_BIOS_PER_SCTX];\n\tstruct btrfs_fs_info\t*fs_info;\n\tint\t\t\tfirst_free;\n\tint\t\t\tcurr;\n\tatomic_t\t\tbios_in_flight;\n\tatomic_t\t\tworkers_pending;\n\tspinlock_t\t\tlist_lock;\n\twait_queue_head_t\tlist_wait;\n\tu16\t\t\tcsum_size;\n\tstruct list_head\tcsum_list;\n\tatomic_t\t\tcancel_req;\n\tint\t\t\treadonly;\n\tint\t\t\tpages_per_rd_bio;\n\n\tint\t\t\tis_dev_replace;\n\n\tstruct scrub_bio        *wr_curr_bio;\n\tstruct mutex            wr_lock;\n\tint                     pages_per_wr_bio; /* <= SCRUB_PAGES_PER_WR_BIO */\n\tstruct btrfs_device     *wr_tgtdev;\n\tbool                    flush_all_writes;\n\n\t/*\n\t * statistics\n\t */\n\tstruct btrfs_scrub_progress stat;\n\tspinlock_t\t\tstat_lock;\n\n\t/*\n\t * Use a ref counter to avoid use-after-free issues. Scrub workers\n\t * decrement bios_in_flight and workers_pending and then do a wakeup\n\t * on the list_wait wait queue. We must ensure the main scrub task\n\t * doesn't free the scrub context before or while the workers are\n\t * doing the wakeup() call.\n\t */\n\trefcount_t              refs;\n};\n\nstruct scrub_warning {\n\tstruct btrfs_path\t*path;\n\tu64\t\t\textent_item_size;\n\tconst char\t\t*errstr;\n\tu64\t\t\tphysical;\n\tu64\t\t\tlogical;\n\tstruct btrfs_device\t*dev;\n};\n\nstruct full_stripe_lock {\n\tstruct rb_node node;\n\tu64 logical;\n\tu64 refs;\n\tstruct mutex mutex;\n};\n\nstatic void scrub_pending_bio_inc(struct scrub_ctx *sctx);\nstatic void scrub_pending_bio_dec(struct scrub_ctx *sctx);\nstatic int scrub_handle_errored_block(struct scrub_block *sblock_to_check);\nstatic int scrub_setup_recheck_block(struct scrub_block *original_sblock,\n\t\t\t\t     struct scrub_block *sblocks_for_recheck);\nstatic void scrub_recheck_block(struct btrfs_fs_info *fs_info,\n\t\t\t\tstruct scrub_block *sblock,\n\t\t\t\tint retry_failed_mirror);\nstatic void scrub_recheck_block_checksum(struct scrub_block *sblock);\nstatic int scrub_repair_block_from_good_copy(struct scrub_block *sblock_bad,\n\t\t\t\t\t     struct scrub_block *sblock_good);\nstatic int scrub_repair_page_from_good_copy(struct scrub_block *sblock_bad,\n\t\t\t\t\t    struct scrub_block *sblock_good,\n\t\t\t\t\t    int page_num, int force_write);\nstatic void scrub_write_block_to_dev_replace(struct scrub_block *sblock);\nstatic int scrub_write_page_to_dev_replace(struct scrub_block *sblock,\n\t\t\t\t\t   int page_num);\nstatic int scrub_checksum_data(struct scrub_block *sblock);\nstatic int scrub_checksum_tree_block(struct scrub_block *sblock);\nstatic int scrub_checksum_super(struct scrub_block *sblock);\nstatic void scrub_block_get(struct scrub_block *sblock);\nstatic void scrub_block_put(struct scrub_block *sblock);\nstatic void scrub_page_get(struct scrub_page *spage);\nstatic void scrub_page_put(struct scrub_page *spage);\nstatic void scrub_parity_get(struct scrub_parity *sparity);\nstatic void scrub_parity_put(struct scrub_parity *sparity);\nstatic int scrub_add_page_to_rd_bio(struct scrub_ctx *sctx,\n\t\t\t\t    struct scrub_page *spage);\nstatic int scrub_pages(struct scrub_ctx *sctx, u64 logical, u64 len,\n\t\t       u64 physical, struct btrfs_device *dev, u64 flags,\n\t\t       u64 gen, int mirror_num, u8 *csum, int force,\n\t\t       u64 physical_for_dev_replace);\nstatic void scrub_bio_end_io(struct bio *bio);\nstatic void scrub_bio_end_io_worker(struct btrfs_work *work);\nstatic void scrub_block_complete(struct scrub_block *sblock);\nstatic void scrub_remap_extent(struct btrfs_fs_info *fs_info,\n\t\t\t       u64 extent_logical, u64 extent_len,\n\t\t\t       u64 *extent_physical,\n\t\t\t       struct btrfs_device **extent_dev,\n\t\t\t       int *extent_mirror_num);\nstatic int scrub_add_page_to_wr_bio(struct scrub_ctx *sctx,\n\t\t\t\t    struct scrub_page *spage);\nstatic void scrub_wr_submit(struct scrub_ctx *sctx);\nstatic void scrub_wr_bio_end_io(struct bio *bio);\nstatic void scrub_wr_bio_end_io_worker(struct btrfs_work *work);\nstatic void __scrub_blocked_if_needed(struct btrfs_fs_info *fs_info);\nstatic void scrub_blocked_if_needed(struct btrfs_fs_info *fs_info);\nstatic void scrub_put_ctx(struct scrub_ctx *sctx);\n\nstatic inline int scrub_is_page_on_raid56(struct scrub_page *page)\n{\n\treturn page->recover &&\n\t       (page->recover->bbio->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK);\n}\n\nstatic void scrub_pending_bio_inc(struct scrub_ctx *sctx)\n{\n\trefcount_inc(&sctx->refs);\n\tatomic_inc(&sctx->bios_in_flight);\n}\n\nstatic void scrub_pending_bio_dec(struct scrub_ctx *sctx)\n{\n\tatomic_dec(&sctx->bios_in_flight);\n\twake_up(&sctx->list_wait);\n\tscrub_put_ctx(sctx);\n}\n\nstatic void __scrub_blocked_if_needed(struct btrfs_fs_info *fs_info)\n{\n\twhile (atomic_read(&fs_info->scrub_pause_req)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t   atomic_read(&fs_info->scrub_pause_req) == 0);\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n}\n\nstatic void scrub_pause_on(struct btrfs_fs_info *fs_info)\n{\n\tatomic_inc(&fs_info->scrubs_paused);\n\twake_up(&fs_info->scrub_pause_wait);\n}\n\nstatic void scrub_pause_off(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->scrub_lock);\n\t__scrub_blocked_if_needed(fs_info);\n\tatomic_dec(&fs_info->scrubs_paused);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\twake_up(&fs_info->scrub_pause_wait);\n}\n\nstatic void scrub_blocked_if_needed(struct btrfs_fs_info *fs_info)\n{\n\tscrub_pause_on(fs_info);\n\tscrub_pause_off(fs_info);\n}\n\n/*\n * Insert new full stripe lock into full stripe locks tree\n *\n * Return pointer to existing or newly inserted full_stripe_lock structure if\n * everything works well.\n * Return ERR_PTR(-ENOMEM) if we failed to allocate memory\n *\n * NOTE: caller must hold full_stripe_locks_root->lock before calling this\n * function\n */\nstatic struct full_stripe_lock *insert_full_stripe_lock(\n\t\tstruct btrfs_full_stripe_locks_tree *locks_root,\n\t\tu64 fstripe_logical)\n{\n\tstruct rb_node **p;\n\tstruct rb_node *parent = NULL;\n\tstruct full_stripe_lock *entry;\n\tstruct full_stripe_lock *ret;\n\n\tlockdep_assert_held(&locks_root->lock);\n\n\tp = &locks_root->root.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct full_stripe_lock, node);\n\t\tif (fstripe_logical < entry->logical) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (fstripe_logical > entry->logical) {\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\tentry->refs++;\n\t\t\treturn entry;\n\t\t}\n\t}\n\n\t/*\n\t * Insert new lock.\n\t */\n\tret = kmalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn ERR_PTR(-ENOMEM);\n\tret->logical = fstripe_logical;\n\tret->refs = 1;\n\tmutex_init(&ret->mutex);\n\n\trb_link_node(&ret->node, parent, p);\n\trb_insert_color(&ret->node, &locks_root->root);\n\treturn ret;\n}\n\n/*\n * Search for a full stripe lock of a block group\n *\n * Return pointer to existing full stripe lock if found\n * Return NULL if not found\n */\nstatic struct full_stripe_lock *search_full_stripe_lock(\n\t\tstruct btrfs_full_stripe_locks_tree *locks_root,\n\t\tu64 fstripe_logical)\n{\n\tstruct rb_node *node;\n\tstruct full_stripe_lock *entry;\n\n\tlockdep_assert_held(&locks_root->lock);\n\n\tnode = locks_root->root.rb_node;\n\twhile (node) {\n\t\tentry = rb_entry(node, struct full_stripe_lock, node);\n\t\tif (fstripe_logical < entry->logical)\n\t\t\tnode = node->rb_left;\n\t\telse if (fstripe_logical > entry->logical)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn entry;\n\t}\n\treturn NULL;\n}\n\n/*\n * Helper to get full stripe logical from a normal bytenr.\n *\n * Caller must ensure @cache is a RAID56 block group.\n */\nstatic u64 get_full_stripe_logical(struct btrfs_block_group_cache *cache,\n\t\t\t\t   u64 bytenr)\n{\n\tu64 ret;\n\n\t/*\n\t * Due to chunk item size limit, full stripe length should not be\n\t * larger than U32_MAX. Just a sanity check here.\n\t */\n\tWARN_ON_ONCE(cache->full_stripe_len >= U32_MAX);\n\n\t/*\n\t * round_down() can only handle power of 2, while RAID56 full\n\t * stripe length can be 64KiB * n, so we need to manually round down.\n\t */\n\tret = div64_u64(bytenr - cache->key.objectid, cache->full_stripe_len) *\n\t\tcache->full_stripe_len + cache->key.objectid;\n\treturn ret;\n}\n\n/*\n * Lock a full stripe to avoid concurrency of recovery and read\n *\n * It's only used for profiles with parities (RAID5/6), for other profiles it\n * does nothing.\n *\n * Return 0 if we locked full stripe covering @bytenr, with a mutex held.\n * So caller must call unlock_full_stripe() at the same context.\n *\n * Return <0 if encounters error.\n */\nstatic int lock_full_stripe(struct btrfs_fs_info *fs_info, u64 bytenr,\n\t\t\t    bool *locked_ret)\n{\n\tstruct btrfs_block_group_cache *bg_cache;\n\tstruct btrfs_full_stripe_locks_tree *locks_root;\n\tstruct full_stripe_lock *existing;\n\tu64 fstripe_start;\n\tint ret = 0;\n\n\t*locked_ret = false;\n\tbg_cache = btrfs_lookup_block_group(fs_info, bytenr);\n\tif (!bg_cache) {\n\t\tASSERT(0);\n\t\treturn -ENOENT;\n\t}\n\n\t/* Profiles not based on parity don't need full stripe lock */\n\tif (!(bg_cache->flags & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\tgoto out;\n\tlocks_root = &bg_cache->full_stripe_locks_root;\n\n\tfstripe_start = get_full_stripe_logical(bg_cache, bytenr);\n\n\t/* Now insert the full stripe lock */\n\tmutex_lock(&locks_root->lock);\n\texisting = insert_full_stripe_lock(locks_root, fstripe_start);\n\tmutex_unlock(&locks_root->lock);\n\tif (IS_ERR(existing)) {\n\t\tret = PTR_ERR(existing);\n\t\tgoto out;\n\t}\n\tmutex_lock(&existing->mutex);\n\t*locked_ret = true;\nout:\n\tbtrfs_put_block_group(bg_cache);\n\treturn ret;\n}\n\n/*\n * Unlock a full stripe.\n *\n * NOTE: Caller must ensure it's the same context calling corresponding\n * lock_full_stripe().\n *\n * Return 0 if we unlock full stripe without problem.\n * Return <0 for error\n */\nstatic int unlock_full_stripe(struct btrfs_fs_info *fs_info, u64 bytenr,\n\t\t\t      bool locked)\n{\n\tstruct btrfs_block_group_cache *bg_cache;\n\tstruct btrfs_full_stripe_locks_tree *locks_root;\n\tstruct full_stripe_lock *fstripe_lock;\n\tu64 fstripe_start;\n\tbool freeit = false;\n\tint ret = 0;\n\n\t/* If we didn't acquire full stripe lock, no need to continue */\n\tif (!locked)\n\t\treturn 0;\n\n\tbg_cache = btrfs_lookup_block_group(fs_info, bytenr);\n\tif (!bg_cache) {\n\t\tASSERT(0);\n\t\treturn -ENOENT;\n\t}\n\tif (!(bg_cache->flags & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\tgoto out;\n\n\tlocks_root = &bg_cache->full_stripe_locks_root;\n\tfstripe_start = get_full_stripe_logical(bg_cache, bytenr);\n\n\tmutex_lock(&locks_root->lock);\n\tfstripe_lock = search_full_stripe_lock(locks_root, fstripe_start);\n\t/* Unpaired unlock_full_stripe() detected */\n\tif (!fstripe_lock) {\n\t\tWARN_ON(1);\n\t\tret = -ENOENT;\n\t\tmutex_unlock(&locks_root->lock);\n\t\tgoto out;\n\t}\n\n\tif (fstripe_lock->refs == 0) {\n\t\tWARN_ON(1);\n\t\tbtrfs_warn(fs_info, \"full stripe lock at %llu refcount underflow\",\n\t\t\tfstripe_lock->logical);\n\t} else {\n\t\tfstripe_lock->refs--;\n\t}\n\n\tif (fstripe_lock->refs == 0) {\n\t\trb_erase(&fstripe_lock->node, &locks_root->root);\n\t\tfreeit = true;\n\t}\n\tmutex_unlock(&locks_root->lock);\n\n\tmutex_unlock(&fstripe_lock->mutex);\n\tif (freeit)\n\t\tkfree(fstripe_lock);\nout:\n\tbtrfs_put_block_group(bg_cache);\n\treturn ret;\n}\n\nstatic void scrub_free_csums(struct scrub_ctx *sctx)\n{\n\twhile (!list_empty(&sctx->csum_list)) {\n\t\tstruct btrfs_ordered_sum *sum;\n\t\tsum = list_first_entry(&sctx->csum_list,\n\t\t\t\t       struct btrfs_ordered_sum, list);\n\t\tlist_del(&sum->list);\n\t\tkfree(sum);\n\t}\n}\n\nstatic noinline_for_stack void scrub_free_ctx(struct scrub_ctx *sctx)\n{\n\tint i;\n\n\tif (!sctx)\n\t\treturn;\n\n\t/* this can happen when scrub is cancelled */\n\tif (sctx->curr != -1) {\n\t\tstruct scrub_bio *sbio = sctx->bios[sctx->curr];\n\n\t\tfor (i = 0; i < sbio->page_count; i++) {\n\t\t\tWARN_ON(!sbio->pagev[i]->page);\n\t\t\tscrub_block_put(sbio->pagev[i]->sblock);\n\t\t}\n\t\tbio_put(sbio->bio);\n\t}\n\n\tfor (i = 0; i < SCRUB_BIOS_PER_SCTX; ++i) {\n\t\tstruct scrub_bio *sbio = sctx->bios[i];\n\n\t\tif (!sbio)\n\t\t\tbreak;\n\t\tkfree(sbio);\n\t}\n\n\tkfree(sctx->wr_curr_bio);\n\tscrub_free_csums(sctx);\n\tkfree(sctx);\n}\n\nstatic void scrub_put_ctx(struct scrub_ctx *sctx)\n{\n\tif (refcount_dec_and_test(&sctx->refs))\n\t\tscrub_free_ctx(sctx);\n}\n\nstatic noinline_for_stack struct scrub_ctx *scrub_setup_ctx(\n\t\tstruct btrfs_fs_info *fs_info, int is_dev_replace)\n{\n\tstruct scrub_ctx *sctx;\n\tint\t\ti;\n\n\tsctx = kzalloc(sizeof(*sctx), GFP_KERNEL);\n\tif (!sctx)\n\t\tgoto nomem;\n\trefcount_set(&sctx->refs, 1);\n\tsctx->is_dev_replace = is_dev_replace;\n\tsctx->pages_per_rd_bio = SCRUB_PAGES_PER_RD_BIO;\n\tsctx->curr = -1;\n\tsctx->fs_info = fs_info;\n\tfor (i = 0; i < SCRUB_BIOS_PER_SCTX; ++i) {\n\t\tstruct scrub_bio *sbio;\n\n\t\tsbio = kzalloc(sizeof(*sbio), GFP_KERNEL);\n\t\tif (!sbio)\n\t\t\tgoto nomem;\n\t\tsctx->bios[i] = sbio;\n\n\t\tsbio->index = i;\n\t\tsbio->sctx = sctx;\n\t\tsbio->page_count = 0;\n\t\tbtrfs_init_work(&sbio->work, btrfs_scrub_helper,\n\t\t\t\tscrub_bio_end_io_worker, NULL, NULL);\n\n\t\tif (i != SCRUB_BIOS_PER_SCTX - 1)\n\t\t\tsctx->bios[i]->next_free = i + 1;\n\t\telse\n\t\t\tsctx->bios[i]->next_free = -1;\n\t}\n\tsctx->first_free = 0;\n\tatomic_set(&sctx->bios_in_flight, 0);\n\tatomic_set(&sctx->workers_pending, 0);\n\tatomic_set(&sctx->cancel_req, 0);\n\tsctx->csum_size = btrfs_super_csum_size(fs_info->super_copy);\n\tINIT_LIST_HEAD(&sctx->csum_list);\n\n\tspin_lock_init(&sctx->list_lock);\n\tspin_lock_init(&sctx->stat_lock);\n\tinit_waitqueue_head(&sctx->list_wait);\n\n\tWARN_ON(sctx->wr_curr_bio != NULL);\n\tmutex_init(&sctx->wr_lock);\n\tsctx->wr_curr_bio = NULL;\n\tif (is_dev_replace) {\n\t\tWARN_ON(!fs_info->dev_replace.tgtdev);\n\t\tsctx->pages_per_wr_bio = SCRUB_PAGES_PER_WR_BIO;\n\t\tsctx->wr_tgtdev = fs_info->dev_replace.tgtdev;\n\t\tsctx->flush_all_writes = false;\n\t}\n\n\treturn sctx;\n\nnomem:\n\tscrub_free_ctx(sctx);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic int scrub_print_warning_inode(u64 inum, u64 offset, u64 root,\n\t\t\t\t     void *warn_ctx)\n{\n\tu64 isize;\n\tu32 nlink;\n\tint ret;\n\tint i;\n\tunsigned nofs_flag;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct scrub_warning *swarn = warn_ctx;\n\tstruct btrfs_fs_info *fs_info = swarn->dev->fs_info;\n\tstruct inode_fs_paths *ipath = NULL;\n\tstruct btrfs_root *local_root;\n\tstruct btrfs_key root_key;\n\tstruct btrfs_key key;\n\n\troot_key.objectid = root;\n\troot_key.type = BTRFS_ROOT_ITEM_KEY;\n\troot_key.offset = (u64)-1;\n\tlocal_root = btrfs_read_fs_root_no_name(fs_info, &root_key);\n\tif (IS_ERR(local_root)) {\n\t\tret = PTR_ERR(local_root);\n\t\tgoto err;\n\t}\n\n\t/*\n\t * this makes the path point to (inum INODE_ITEM ioff)\n\t */\n\tkey.objectid = inum;\n\tkey.type = BTRFS_INODE_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, local_root, &key, swarn->path, 0, 0);\n\tif (ret) {\n\t\tbtrfs_release_path(swarn->path);\n\t\tgoto err;\n\t}\n\n\teb = swarn->path->nodes[0];\n\tinode_item = btrfs_item_ptr(eb, swarn->path->slots[0],\n\t\t\t\t\tstruct btrfs_inode_item);\n\tisize = btrfs_inode_size(eb, inode_item);\n\tnlink = btrfs_inode_nlink(eb, inode_item);\n\tbtrfs_release_path(swarn->path);\n\n\t/*\n\t * init_path might indirectly call vmalloc, or use GFP_KERNEL. Scrub\n\t * uses GFP_NOFS in this context, so we keep it consistent but it does\n\t * not seem to be strictly necessary.\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\tipath = init_ipath(4096, local_root, swarn->path);\n\tmemalloc_nofs_restore(nofs_flag);\n\tif (IS_ERR(ipath)) {\n\t\tret = PTR_ERR(ipath);\n\t\tipath = NULL;\n\t\tgoto err;\n\t}\n\tret = paths_from_inode(inum, ipath);\n\n\tif (ret < 0)\n\t\tgoto err;\n\n\t/*\n\t * we deliberately ignore the bit ipath might have been too small to\n\t * hold all of the paths here\n\t */\n\tfor (i = 0; i < ipath->fspath->elem_cnt; ++i)\n\t\tbtrfs_warn_in_rcu(fs_info,\n\"%s at logical %llu on dev %s, physical %llu, root %llu, inode %llu, offset %llu, length %llu, links %u (path: %s)\",\n\t\t\t\t  swarn->errstr, swarn->logical,\n\t\t\t\t  rcu_str_deref(swarn->dev->name),\n\t\t\t\t  swarn->physical,\n\t\t\t\t  root, inum, offset,\n\t\t\t\t  min(isize - offset, (u64)PAGE_SIZE), nlink,\n\t\t\t\t  (char *)(unsigned long)ipath->fspath->val[i]);\n\n\tfree_ipath(ipath);\n\treturn 0;\n\nerr:\n\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t  \"%s at logical %llu on dev %s, physical %llu, root %llu, inode %llu, offset %llu: path resolving failed with ret=%d\",\n\t\t\t  swarn->errstr, swarn->logical,\n\t\t\t  rcu_str_deref(swarn->dev->name),\n\t\t\t  swarn->physical,\n\t\t\t  root, inum, offset, ret);\n\n\tfree_ipath(ipath);\n\treturn 0;\n}\n\nstatic void scrub_print_warning(const char *errstr, struct scrub_block *sblock)\n{\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_info *fs_info;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_extent_item *ei;\n\tstruct scrub_warning swarn;\n\tunsigned long ptr = 0;\n\tu64 extent_item_pos;\n\tu64 flags = 0;\n\tu64 ref_root;\n\tu32 item_size;\n\tu8 ref_level = 0;\n\tint ret;\n\n\tWARN_ON(sblock->page_count < 1);\n\tdev = sblock->pagev[0]->dev;\n\tfs_info = sblock->sctx->fs_info;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn;\n\n\tswarn.physical = sblock->pagev[0]->physical;\n\tswarn.logical = sblock->pagev[0]->logical;\n\tswarn.errstr = errstr;\n\tswarn.dev = NULL;\n\n\tret = extent_from_logical(fs_info, swarn.logical, path, &found_key,\n\t\t\t\t  &flags);\n\tif (ret < 0)\n\t\tgoto out;\n\n\textent_item_pos = swarn.logical - found_key.objectid;\n\tswarn.extent_item_size = found_key.offset;\n\n\teb = path->nodes[0];\n\tei = btrfs_item_ptr(eb, path->slots[0], struct btrfs_extent_item);\n\titem_size = btrfs_item_size_nr(eb, path->slots[0]);\n\n\tif (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\n\t\tdo {\n\t\t\tret = tree_backref_for_extent(&ptr, eb, &found_key, ei,\n\t\t\t\t\t\t      item_size, &ref_root,\n\t\t\t\t\t\t      &ref_level);\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\"%s at logical %llu on dev %s, physical %llu: metadata %s (level %d) in tree %llu\",\n\t\t\t\terrstr, swarn.logical,\n\t\t\t\trcu_str_deref(dev->name),\n\t\t\t\tswarn.physical,\n\t\t\t\tref_level ? \"node\" : \"leaf\",\n\t\t\t\tret < 0 ? -1 : ref_level,\n\t\t\t\tret < 0 ? -1 : ref_root);\n\t\t} while (ret != 1);\n\t\tbtrfs_release_path(path);\n\t} else {\n\t\tbtrfs_release_path(path);\n\t\tswarn.path = path;\n\t\tswarn.dev = dev;\n\t\titerate_extent_inodes(fs_info, found_key.objectid,\n\t\t\t\t\textent_item_pos, 1,\n\t\t\t\t\tscrub_print_warning_inode, &swarn, false);\n\t}\n\nout:\n\tbtrfs_free_path(path);\n}\n\nstatic inline void scrub_get_recover(struct scrub_recover *recover)\n{\n\trefcount_inc(&recover->refs);\n}\n\nstatic inline void scrub_put_recover(struct btrfs_fs_info *fs_info,\n\t\t\t\t     struct scrub_recover *recover)\n{\n\tif (refcount_dec_and_test(&recover->refs)) {\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\tbtrfs_put_bbio(recover->bbio);\n\t\tkfree(recover);\n\t}\n}\n\n/*\n * scrub_handle_errored_block gets called when either verification of the\n * pages failed or the bio failed to read, e.g. with EIO. In the latter\n * case, this function handles all pages in the bio, even though only one\n * may be bad.\n * The goal of this function is to repair the errored block by using the\n * contents of one of the mirrors.\n */\nstatic int scrub_handle_errored_block(struct scrub_block *sblock_to_check)\n{\n\tstruct scrub_ctx *sctx = sblock_to_check->sctx;\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_info *fs_info;\n\tu64 logical;\n\tunsigned int failed_mirror_index;\n\tunsigned int is_metadata;\n\tunsigned int have_csum;\n\tstruct scrub_block *sblocks_for_recheck; /* holds one for each mirror */\n\tstruct scrub_block *sblock_bad;\n\tint ret;\n\tint mirror_index;\n\tint page_num;\n\tint success;\n\tbool full_stripe_locked;\n\tunsigned int nofs_flag;\n\tstatic DEFINE_RATELIMIT_STATE(_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\tBUG_ON(sblock_to_check->page_count < 1);\n\tfs_info = sctx->fs_info;\n\tif (sblock_to_check->pagev[0]->flags & BTRFS_EXTENT_FLAG_SUPER) {\n\t\t/*\n\t\t * if we find an error in a super block, we just report it.\n\t\t * They will get written with the next transaction commit\n\t\t * anyway\n\t\t */\n\t\tspin_lock(&sctx->stat_lock);\n\t\t++sctx->stat.super_errors;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn 0;\n\t}\n\tlogical = sblock_to_check->pagev[0]->logical;\n\tBUG_ON(sblock_to_check->pagev[0]->mirror_num < 1);\n\tfailed_mirror_index = sblock_to_check->pagev[0]->mirror_num - 1;\n\tis_metadata = !(sblock_to_check->pagev[0]->flags &\n\t\t\tBTRFS_EXTENT_FLAG_DATA);\n\thave_csum = sblock_to_check->pagev[0]->have_csum;\n\tdev = sblock_to_check->pagev[0]->dev;\n\n\t/*\n\t * We must use GFP_NOFS because the scrub task might be waiting for a\n\t * worker task executing this function and in turn a transaction commit\n\t * might be waiting the scrub task to pause (which needs to wait for all\n\t * the worker tasks to complete before pausing).\n\t * We do allocations in the workers through insert_full_stripe_lock()\n\t * and scrub_add_page_to_wr_bio(), which happens down the call chain of\n\t * this function.\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\t/*\n\t * For RAID5/6, race can happen for a different device scrub thread.\n\t * For data corruption, Parity and Data threads will both try\n\t * to recovery the data.\n\t * Race can lead to doubly added csum error, or even unrecoverable\n\t * error.\n\t */\n\tret = lock_full_stripe(fs_info, logical, &full_stripe_locked);\n\tif (ret < 0) {\n\t\tmemalloc_nofs_restore(nofs_flag);\n\t\tspin_lock(&sctx->stat_lock);\n\t\tif (ret == -ENOMEM)\n\t\t\tsctx->stat.malloc_errors++;\n\t\tsctx->stat.read_errors++;\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * read all mirrors one after the other. This includes to\n\t * re-read the extent or metadata block that failed (that was\n\t * the cause that this fixup code is called) another time,\n\t * page by page this time in order to know which pages\n\t * caused I/O errors and which ones are good (for all mirrors).\n\t * It is the goal to handle the situation when more than one\n\t * mirror contains I/O errors, but the errors do not\n\t * overlap, i.e. the data can be repaired by selecting the\n\t * pages from those mirrors without I/O error on the\n\t * particular pages. One example (with blocks >= 2 * PAGE_SIZE)\n\t * would be that mirror #1 has an I/O error on the first page,\n\t * the second page is good, and mirror #2 has an I/O error on\n\t * the second page, but the first page is good.\n\t * Then the first page of the first mirror can be repaired by\n\t * taking the first page of the second mirror, and the\n\t * second page of the second mirror can be repaired by\n\t * copying the contents of the 2nd page of the 1st mirror.\n\t * One more note: if the pages of one mirror contain I/O\n\t * errors, the checksum cannot be verified. In order to get\n\t * the best data for repairing, the first attempt is to find\n\t * a mirror without I/O errors and with a validated checksum.\n\t * Only if this is not possible, the pages are picked from\n\t * mirrors with I/O errors without considering the checksum.\n\t * If the latter is the case, at the end, the checksum of the\n\t * repaired area is verified in order to correctly maintain\n\t * the statistics.\n\t */\n\n\tsblocks_for_recheck = kcalloc(BTRFS_MAX_MIRRORS,\n\t\t\t\t      sizeof(*sblocks_for_recheck), GFP_KERNEL);\n\tif (!sblocks_for_recheck) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.malloc_errors++;\n\t\tsctx->stat.read_errors++;\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_dev_stat_inc_and_print(dev, BTRFS_DEV_STAT_READ_ERRS);\n\t\tgoto out;\n\t}\n\n\t/* setup the context, map the logical blocks and alloc the pages */\n\tret = scrub_setup_recheck_block(sblock_to_check, sblocks_for_recheck);\n\tif (ret) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.read_errors++;\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_dev_stat_inc_and_print(dev, BTRFS_DEV_STAT_READ_ERRS);\n\t\tgoto out;\n\t}\n\tBUG_ON(failed_mirror_index >= BTRFS_MAX_MIRRORS);\n\tsblock_bad = sblocks_for_recheck + failed_mirror_index;\n\n\t/* build and submit the bios for the failed mirror, check checksums */\n\tscrub_recheck_block(fs_info, sblock_bad, 1);\n\n\tif (!sblock_bad->header_error && !sblock_bad->checksum_error &&\n\t    sblock_bad->no_io_error_seen) {\n\t\t/*\n\t\t * the error disappeared after reading page by page, or\n\t\t * the area was part of a huge bio and other parts of the\n\t\t * bio caused I/O errors, or the block layer merged several\n\t\t * read requests into one and the error is caused by a\n\t\t * different bio (usually one of the two latter cases is\n\t\t * the cause)\n\t\t */\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.unverified_errors++;\n\t\tsblock_to_check->data_corrected = 1;\n\t\tspin_unlock(&sctx->stat_lock);\n\n\t\tif (sctx->is_dev_replace)\n\t\t\tscrub_write_block_to_dev_replace(sblock_bad);\n\t\tgoto out;\n\t}\n\n\tif (!sblock_bad->no_io_error_seen) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.read_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (__ratelimit(&_rs))\n\t\t\tscrub_print_warning(\"i/o error\", sblock_to_check);\n\t\tbtrfs_dev_stat_inc_and_print(dev, BTRFS_DEV_STAT_READ_ERRS);\n\t} else if (sblock_bad->checksum_error) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.csum_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (__ratelimit(&_rs))\n\t\t\tscrub_print_warning(\"checksum error\", sblock_to_check);\n\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t     BTRFS_DEV_STAT_CORRUPTION_ERRS);\n\t} else if (sblock_bad->header_error) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.verify_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (__ratelimit(&_rs))\n\t\t\tscrub_print_warning(\"checksum/header error\",\n\t\t\t\t\t    sblock_to_check);\n\t\tif (sblock_bad->generation_error)\n\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\tBTRFS_DEV_STAT_GENERATION_ERRS);\n\t\telse\n\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\tBTRFS_DEV_STAT_CORRUPTION_ERRS);\n\t}\n\n\tif (sctx->readonly) {\n\t\tASSERT(!sctx->is_dev_replace);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * now build and submit the bios for the other mirrors, check\n\t * checksums.\n\t * First try to pick the mirror which is completely without I/O\n\t * errors and also does not have a checksum error.\n\t * If one is found, and if a checksum is present, the full block\n\t * that is known to contain an error is rewritten. Afterwards\n\t * the block is known to be corrected.\n\t * If a mirror is found which is completely correct, and no\n\t * checksum is present, only those pages are rewritten that had\n\t * an I/O error in the block to be repaired, since it cannot be\n\t * determined, which copy of the other pages is better (and it\n\t * could happen otherwise that a correct page would be\n\t * overwritten by a bad one).\n\t */\n\tfor (mirror_index = 0; ;mirror_index++) {\n\t\tstruct scrub_block *sblock_other;\n\n\t\tif (mirror_index == failed_mirror_index)\n\t\t\tcontinue;\n\n\t\t/* raid56's mirror can be more than BTRFS_MAX_MIRRORS */\n\t\tif (!scrub_is_page_on_raid56(sblock_bad->pagev[0])) {\n\t\t\tif (mirror_index >= BTRFS_MAX_MIRRORS)\n\t\t\t\tbreak;\n\t\t\tif (!sblocks_for_recheck[mirror_index].page_count)\n\t\t\t\tbreak;\n\n\t\t\tsblock_other = sblocks_for_recheck + mirror_index;\n\t\t} else {\n\t\t\tstruct scrub_recover *r = sblock_bad->pagev[0]->recover;\n\t\t\tint max_allowed = r->bbio->num_stripes -\n\t\t\t\t\t\tr->bbio->num_tgtdevs;\n\n\t\t\tif (mirror_index >= max_allowed)\n\t\t\t\tbreak;\n\t\t\tif (!sblocks_for_recheck[1].page_count)\n\t\t\t\tbreak;\n\n\t\t\tASSERT(failed_mirror_index == 0);\n\t\t\tsblock_other = sblocks_for_recheck + 1;\n\t\t\tsblock_other->pagev[0]->mirror_num = 1 + mirror_index;\n\t\t}\n\n\t\t/* build and submit the bios, check checksums */\n\t\tscrub_recheck_block(fs_info, sblock_other, 0);\n\n\t\tif (!sblock_other->header_error &&\n\t\t    !sblock_other->checksum_error &&\n\t\t    sblock_other->no_io_error_seen) {\n\t\t\tif (sctx->is_dev_replace) {\n\t\t\t\tscrub_write_block_to_dev_replace(sblock_other);\n\t\t\t\tgoto corrected_error;\n\t\t\t} else {\n\t\t\t\tret = scrub_repair_block_from_good_copy(\n\t\t\t\t\t\tsblock_bad, sblock_other);\n\t\t\t\tif (!ret)\n\t\t\t\t\tgoto corrected_error;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (sblock_bad->no_io_error_seen && !sctx->is_dev_replace)\n\t\tgoto did_not_correct_error;\n\n\t/*\n\t * In case of I/O errors in the area that is supposed to be\n\t * repaired, continue by picking good copies of those pages.\n\t * Select the good pages from mirrors to rewrite bad pages from\n\t * the area to fix. Afterwards verify the checksum of the block\n\t * that is supposed to be repaired. This verification step is\n\t * only done for the purpose of statistic counting and for the\n\t * final scrub report, whether errors remain.\n\t * A perfect algorithm could make use of the checksum and try\n\t * all possible combinations of pages from the different mirrors\n\t * until the checksum verification succeeds. For example, when\n\t * the 2nd page of mirror #1 faces I/O errors, and the 2nd page\n\t * of mirror #2 is readable but the final checksum test fails,\n\t * then the 2nd page of mirror #3 could be tried, whether now\n\t * the final checksum succeeds. But this would be a rare\n\t * exception and is therefore not implemented. At least it is\n\t * avoided that the good copy is overwritten.\n\t * A more useful improvement would be to pick the sectors\n\t * without I/O error based on sector sizes (512 bytes on legacy\n\t * disks) instead of on PAGE_SIZE. Then maybe 512 byte of one\n\t * mirror could be repaired by taking 512 byte of a different\n\t * mirror, even if other 512 byte sectors in the same PAGE_SIZE\n\t * area are unreadable.\n\t */\n\tsuccess = 1;\n\tfor (page_num = 0; page_num < sblock_bad->page_count;\n\t     page_num++) {\n\t\tstruct scrub_page *page_bad = sblock_bad->pagev[page_num];\n\t\tstruct scrub_block *sblock_other = NULL;\n\n\t\t/* skip no-io-error page in scrub */\n\t\tif (!page_bad->io_error && !sctx->is_dev_replace)\n\t\t\tcontinue;\n\n\t\tif (scrub_is_page_on_raid56(sblock_bad->pagev[0])) {\n\t\t\t/*\n\t\t\t * In case of dev replace, if raid56 rebuild process\n\t\t\t * didn't work out correct data, then copy the content\n\t\t\t * in sblock_bad to make sure target device is identical\n\t\t\t * to source device, instead of writing garbage data in\n\t\t\t * sblock_for_recheck array to target device.\n\t\t\t */\n\t\t\tsblock_other = NULL;\n\t\t} else if (page_bad->io_error) {\n\t\t\t/* try to find no-io-error page in mirrors */\n\t\t\tfor (mirror_index = 0;\n\t\t\t     mirror_index < BTRFS_MAX_MIRRORS &&\n\t\t\t     sblocks_for_recheck[mirror_index].page_count > 0;\n\t\t\t     mirror_index++) {\n\t\t\t\tif (!sblocks_for_recheck[mirror_index].\n\t\t\t\t    pagev[page_num]->io_error) {\n\t\t\t\t\tsblock_other = sblocks_for_recheck +\n\t\t\t\t\t\t       mirror_index;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!sblock_other)\n\t\t\t\tsuccess = 0;\n\t\t}\n\n\t\tif (sctx->is_dev_replace) {\n\t\t\t/*\n\t\t\t * did not find a mirror to fetch the page\n\t\t\t * from. scrub_write_page_to_dev_replace()\n\t\t\t * handles this case (page->io_error), by\n\t\t\t * filling the block with zeros before\n\t\t\t * submitting the write request\n\t\t\t */\n\t\t\tif (!sblock_other)\n\t\t\t\tsblock_other = sblock_bad;\n\n\t\t\tif (scrub_write_page_to_dev_replace(sblock_other,\n\t\t\t\t\t\t\t    page_num) != 0) {\n\t\t\t\tatomic64_inc(\n\t\t\t\t\t&fs_info->dev_replace.num_write_errors);\n\t\t\t\tsuccess = 0;\n\t\t\t}\n\t\t} else if (sblock_other) {\n\t\t\tret = scrub_repair_page_from_good_copy(sblock_bad,\n\t\t\t\t\t\t\t       sblock_other,\n\t\t\t\t\t\t\t       page_num, 0);\n\t\t\tif (0 == ret)\n\t\t\t\tpage_bad->io_error = 0;\n\t\t\telse\n\t\t\t\tsuccess = 0;\n\t\t}\n\t}\n\n\tif (success && !sctx->is_dev_replace) {\n\t\tif (is_metadata || have_csum) {\n\t\t\t/*\n\t\t\t * need to verify the checksum now that all\n\t\t\t * sectors on disk are repaired (the write\n\t\t\t * request for data to be repaired is on its way).\n\t\t\t * Just be lazy and use scrub_recheck_block()\n\t\t\t * which re-reads the data before the checksum\n\t\t\t * is verified, but most likely the data comes out\n\t\t\t * of the page cache.\n\t\t\t */\n\t\t\tscrub_recheck_block(fs_info, sblock_bad, 1);\n\t\t\tif (!sblock_bad->header_error &&\n\t\t\t    !sblock_bad->checksum_error &&\n\t\t\t    sblock_bad->no_io_error_seen)\n\t\t\t\tgoto corrected_error;\n\t\t\telse\n\t\t\t\tgoto did_not_correct_error;\n\t\t} else {\ncorrected_error:\n\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\tsctx->stat.corrected_errors++;\n\t\t\tsblock_to_check->data_corrected = 1;\n\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\t\"fixed up error at logical %llu on dev %s\",\n\t\t\t\tlogical, rcu_str_deref(dev->name));\n\t\t}\n\t} else {\ndid_not_correct_error:\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\"unable to fixup (regular) error at logical %llu on dev %s\",\n\t\t\tlogical, rcu_str_deref(dev->name));\n\t}\n\nout:\n\tif (sblocks_for_recheck) {\n\t\tfor (mirror_index = 0; mirror_index < BTRFS_MAX_MIRRORS;\n\t\t     mirror_index++) {\n\t\t\tstruct scrub_block *sblock = sblocks_for_recheck +\n\t\t\t\t\t\t     mirror_index;\n\t\t\tstruct scrub_recover *recover;\n\t\t\tint page_index;\n\n\t\t\tfor (page_index = 0; page_index < sblock->page_count;\n\t\t\t     page_index++) {\n\t\t\t\tsblock->pagev[page_index]->sblock = NULL;\n\t\t\t\trecover = sblock->pagev[page_index]->recover;\n\t\t\t\tif (recover) {\n\t\t\t\t\tscrub_put_recover(fs_info, recover);\n\t\t\t\t\tsblock->pagev[page_index]->recover =\n\t\t\t\t\t\t\t\t\tNULL;\n\t\t\t\t}\n\t\t\t\tscrub_page_put(sblock->pagev[page_index]);\n\t\t\t}\n\t\t}\n\t\tkfree(sblocks_for_recheck);\n\t}\n\n\tret = unlock_full_stripe(fs_info, logical, full_stripe_locked);\n\tmemalloc_nofs_restore(nofs_flag);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn 0;\n}\n\nstatic inline int scrub_nr_raid_mirrors(struct btrfs_bio *bbio)\n{\n\tif (bbio->map_type & BTRFS_BLOCK_GROUP_RAID5)\n\t\treturn 2;\n\telse if (bbio->map_type & BTRFS_BLOCK_GROUP_RAID6)\n\t\treturn 3;\n\telse\n\t\treturn (int)bbio->num_stripes;\n}\n\nstatic inline void scrub_stripe_index_and_offset(u64 logical, u64 map_type,\n\t\t\t\t\t\t u64 *raid_map,\n\t\t\t\t\t\t u64 mapped_length,\n\t\t\t\t\t\t int nstripes, int mirror,\n\t\t\t\t\t\t int *stripe_index,\n\t\t\t\t\t\t u64 *stripe_offset)\n{\n\tint i;\n\n\tif (map_type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\t/* RAID5/6 */\n\t\tfor (i = 0; i < nstripes; i++) {\n\t\t\tif (raid_map[i] == RAID6_Q_STRIPE ||\n\t\t\t    raid_map[i] == RAID5_P_STRIPE)\n\t\t\t\tcontinue;\n\n\t\t\tif (logical >= raid_map[i] &&\n\t\t\t    logical < raid_map[i] + mapped_length)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t*stripe_index = i;\n\t\t*stripe_offset = logical - raid_map[i];\n\t} else {\n\t\t/* The other RAID type */\n\t\t*stripe_index = mirror;\n\t\t*stripe_offset = 0;\n\t}\n}\n\nstatic int scrub_setup_recheck_block(struct scrub_block *original_sblock,\n\t\t\t\t     struct scrub_block *sblocks_for_recheck)\n{\n\tstruct scrub_ctx *sctx = original_sblock->sctx;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tu64 length = original_sblock->page_count * PAGE_SIZE;\n\tu64 logical = original_sblock->pagev[0]->logical;\n\tu64 generation = original_sblock->pagev[0]->generation;\n\tu64 flags = original_sblock->pagev[0]->flags;\n\tu64 have_csum = original_sblock->pagev[0]->have_csum;\n\tstruct scrub_recover *recover;\n\tstruct btrfs_bio *bbio;\n\tu64 sublen;\n\tu64 mapped_length;\n\tu64 stripe_offset;\n\tint stripe_index;\n\tint page_index = 0;\n\tint mirror_index;\n\tint nmirrors;\n\tint ret;\n\n\t/*\n\t * note: the two members refs and outstanding_pages\n\t * are not used (and not set) in the blocks that are used for\n\t * the recheck procedure\n\t */\n\n\twhile (length > 0) {\n\t\tsublen = min_t(u64, length, PAGE_SIZE);\n\t\tmapped_length = sublen;\n\t\tbbio = NULL;\n\n\t\t/*\n\t\t * with a length of PAGE_SIZE, each returned stripe\n\t\t * represents one mirror\n\t\t */\n\t\tbtrfs_bio_counter_inc_blocked(fs_info);\n\t\tret = btrfs_map_sblock(fs_info, BTRFS_MAP_GET_READ_MIRRORS,\n\t\t\t\tlogical, &mapped_length, &bbio);\n\t\tif (ret || !bbio || mapped_length < sublen) {\n\t\t\tbtrfs_put_bbio(bbio);\n\t\t\tbtrfs_bio_counter_dec(fs_info);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\trecover = kzalloc(sizeof(struct scrub_recover), GFP_NOFS);\n\t\tif (!recover) {\n\t\t\tbtrfs_put_bbio(bbio);\n\t\t\tbtrfs_bio_counter_dec(fs_info);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\trefcount_set(&recover->refs, 1);\n\t\trecover->bbio = bbio;\n\t\trecover->map_length = mapped_length;\n\n\t\tBUG_ON(page_index >= SCRUB_MAX_PAGES_PER_BLOCK);\n\n\t\tnmirrors = min(scrub_nr_raid_mirrors(bbio), BTRFS_MAX_MIRRORS);\n\n\t\tfor (mirror_index = 0; mirror_index < nmirrors;\n\t\t     mirror_index++) {\n\t\t\tstruct scrub_block *sblock;\n\t\t\tstruct scrub_page *page;\n\n\t\t\tsblock = sblocks_for_recheck + mirror_index;\n\t\t\tsblock->sctx = sctx;\n\n\t\t\tpage = kzalloc(sizeof(*page), GFP_NOFS);\n\t\t\tif (!page) {\nleave_nomem:\n\t\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\t\tsctx->stat.malloc_errors++;\n\t\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\t\tscrub_put_recover(fs_info, recover);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tscrub_page_get(page);\n\t\t\tsblock->pagev[page_index] = page;\n\t\t\tpage->sblock = sblock;\n\t\t\tpage->flags = flags;\n\t\t\tpage->generation = generation;\n\t\t\tpage->logical = logical;\n\t\t\tpage->have_csum = have_csum;\n\t\t\tif (have_csum)\n\t\t\t\tmemcpy(page->csum,\n\t\t\t\t       original_sblock->pagev[0]->csum,\n\t\t\t\t       sctx->csum_size);\n\n\t\t\tscrub_stripe_index_and_offset(logical,\n\t\t\t\t\t\t      bbio->map_type,\n\t\t\t\t\t\t      bbio->raid_map,\n\t\t\t\t\t\t      mapped_length,\n\t\t\t\t\t\t      bbio->num_stripes -\n\t\t\t\t\t\t      bbio->num_tgtdevs,\n\t\t\t\t\t\t      mirror_index,\n\t\t\t\t\t\t      &stripe_index,\n\t\t\t\t\t\t      &stripe_offset);\n\t\t\tpage->physical = bbio->stripes[stripe_index].physical +\n\t\t\t\t\t stripe_offset;\n\t\t\tpage->dev = bbio->stripes[stripe_index].dev;\n\n\t\t\tBUG_ON(page_index >= original_sblock->page_count);\n\t\t\tpage->physical_for_dev_replace =\n\t\t\t\toriginal_sblock->pagev[page_index]->\n\t\t\t\tphysical_for_dev_replace;\n\t\t\t/* for missing devices, dev->bdev is NULL */\n\t\t\tpage->mirror_num = mirror_index + 1;\n\t\t\tsblock->page_count++;\n\t\t\tpage->page = alloc_page(GFP_NOFS);\n\t\t\tif (!page->page)\n\t\t\t\tgoto leave_nomem;\n\n\t\t\tscrub_get_recover(recover);\n\t\t\tpage->recover = recover;\n\t\t}\n\t\tscrub_put_recover(fs_info, recover);\n\t\tlength -= sublen;\n\t\tlogical += sublen;\n\t\tpage_index++;\n\t}\n\n\treturn 0;\n}\n\nstatic void scrub_bio_wait_endio(struct bio *bio)\n{\n\tcomplete(bio->bi_private);\n}\n\nstatic int scrub_submit_raid56_bio_wait(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct bio *bio,\n\t\t\t\t\tstruct scrub_page *page)\n{\n\tDECLARE_COMPLETION_ONSTACK(done);\n\tint ret;\n\tint mirror_num;\n\n\tbio->bi_iter.bi_sector = page->logical >> 9;\n\tbio->bi_private = &done;\n\tbio->bi_end_io = scrub_bio_wait_endio;\n\n\tmirror_num = page->sblock->pagev[0]->mirror_num;\n\tret = raid56_parity_recover(fs_info, bio, page->recover->bbio,\n\t\t\t\t    page->recover->map_length,\n\t\t\t\t    mirror_num, 0);\n\tif (ret)\n\t\treturn ret;\n\n\twait_for_completion_io(&done);\n\treturn blk_status_to_errno(bio->bi_status);\n}\n\nstatic void scrub_recheck_block_on_raid56(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t  struct scrub_block *sblock)\n{\n\tstruct scrub_page *first_page = sblock->pagev[0];\n\tstruct bio *bio;\n\tint page_num;\n\n\t/* All pages in sblock belong to the same stripe on the same device. */\n\tASSERT(first_page->dev);\n\tif (!first_page->dev->bdev)\n\t\tgoto out;\n\n\tbio = btrfs_io_bio_alloc(BIO_MAX_PAGES);\n\tbio_set_dev(bio, first_page->dev->bdev);\n\n\tfor (page_num = 0; page_num < sblock->page_count; page_num++) {\n\t\tstruct scrub_page *page = sblock->pagev[page_num];\n\n\t\tWARN_ON(!page->page);\n\t\tbio_add_page(bio, page->page, PAGE_SIZE, 0);\n\t}\n\n\tif (scrub_submit_raid56_bio_wait(fs_info, bio, first_page)) {\n\t\tbio_put(bio);\n\t\tgoto out;\n\t}\n\n\tbio_put(bio);\n\n\tscrub_recheck_block_checksum(sblock);\n\n\treturn;\nout:\n\tfor (page_num = 0; page_num < sblock->page_count; page_num++)\n\t\tsblock->pagev[page_num]->io_error = 1;\n\n\tsblock->no_io_error_seen = 0;\n}\n\n/*\n * this function will check the on disk data for checksum errors, header\n * errors and read I/O errors. If any I/O errors happen, the exact pages\n * which are errored are marked as being bad. The goal is to enable scrub\n * to take those pages that are not errored from all the mirrors so that\n * the pages that are errored in the just handled mirror can be repaired.\n */\nstatic void scrub_recheck_block(struct btrfs_fs_info *fs_info,\n\t\t\t\tstruct scrub_block *sblock,\n\t\t\t\tint retry_failed_mirror)\n{\n\tint page_num;\n\n\tsblock->no_io_error_seen = 1;\n\n\t/* short cut for raid56 */\n\tif (!retry_failed_mirror && scrub_is_page_on_raid56(sblock->pagev[0]))\n\t\treturn scrub_recheck_block_on_raid56(fs_info, sblock);\n\n\tfor (page_num = 0; page_num < sblock->page_count; page_num++) {\n\t\tstruct bio *bio;\n\t\tstruct scrub_page *page = sblock->pagev[page_num];\n\n\t\tif (page->dev->bdev == NULL) {\n\t\t\tpage->io_error = 1;\n\t\t\tsblock->no_io_error_seen = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON(!page->page);\n\t\tbio = btrfs_io_bio_alloc(1);\n\t\tbio_set_dev(bio, page->dev->bdev);\n\n\t\tbio_add_page(bio, page->page, PAGE_SIZE, 0);\n\t\tbio->bi_iter.bi_sector = page->physical >> 9;\n\t\tbio->bi_opf = REQ_OP_READ;\n\n\t\tif (btrfsic_submit_bio_wait(bio)) {\n\t\t\tpage->io_error = 1;\n\t\t\tsblock->no_io_error_seen = 0;\n\t\t}\n\n\t\tbio_put(bio);\n\t}\n\n\tif (sblock->no_io_error_seen)\n\t\tscrub_recheck_block_checksum(sblock);\n}\n\nstatic inline int scrub_check_fsid(u8 fsid[],\n\t\t\t\t   struct scrub_page *spage)\n{\n\tstruct btrfs_fs_devices *fs_devices = spage->dev->fs_devices;\n\tint ret;\n\n\tret = memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\treturn !ret;\n}\n\nstatic void scrub_recheck_block_checksum(struct scrub_block *sblock)\n{\n\tsblock->header_error = 0;\n\tsblock->checksum_error = 0;\n\tsblock->generation_error = 0;\n\n\tif (sblock->pagev[0]->flags & BTRFS_EXTENT_FLAG_DATA)\n\t\tscrub_checksum_data(sblock);\n\telse\n\t\tscrub_checksum_tree_block(sblock);\n}\n\nstatic int scrub_repair_block_from_good_copy(struct scrub_block *sblock_bad,\n\t\t\t\t\t     struct scrub_block *sblock_good)\n{\n\tint page_num;\n\tint ret = 0;\n\n\tfor (page_num = 0; page_num < sblock_bad->page_count; page_num++) {\n\t\tint ret_sub;\n\n\t\tret_sub = scrub_repair_page_from_good_copy(sblock_bad,\n\t\t\t\t\t\t\t   sblock_good,\n\t\t\t\t\t\t\t   page_num, 1);\n\t\tif (ret_sub)\n\t\t\tret = ret_sub;\n\t}\n\n\treturn ret;\n}\n\nstatic int scrub_repair_page_from_good_copy(struct scrub_block *sblock_bad,\n\t\t\t\t\t    struct scrub_block *sblock_good,\n\t\t\t\t\t    int page_num, int force_write)\n{\n\tstruct scrub_page *page_bad = sblock_bad->pagev[page_num];\n\tstruct scrub_page *page_good = sblock_good->pagev[page_num];\n\tstruct btrfs_fs_info *fs_info = sblock_bad->sctx->fs_info;\n\n\tBUG_ON(page_bad->page == NULL);\n\tBUG_ON(page_good->page == NULL);\n\tif (force_write || sblock_bad->header_error ||\n\t    sblock_bad->checksum_error || page_bad->io_error) {\n\t\tstruct bio *bio;\n\t\tint ret;\n\n\t\tif (!page_bad->dev->bdev) {\n\t\t\tbtrfs_warn_rl(fs_info,\n\t\t\t\t\"scrub_repair_page_from_good_copy(bdev == NULL) is unexpected\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tbio = btrfs_io_bio_alloc(1);\n\t\tbio_set_dev(bio, page_bad->dev->bdev);\n\t\tbio->bi_iter.bi_sector = page_bad->physical >> 9;\n\t\tbio->bi_opf = REQ_OP_WRITE;\n\n\t\tret = bio_add_page(bio, page_good->page, PAGE_SIZE, 0);\n\t\tif (PAGE_SIZE != ret) {\n\t\t\tbio_put(bio);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (btrfsic_submit_bio_wait(bio)) {\n\t\t\tbtrfs_dev_stat_inc_and_print(page_bad->dev,\n\t\t\t\tBTRFS_DEV_STAT_WRITE_ERRS);\n\t\t\tatomic64_inc(&fs_info->dev_replace.num_write_errors);\n\t\t\tbio_put(bio);\n\t\t\treturn -EIO;\n\t\t}\n\t\tbio_put(bio);\n\t}\n\n\treturn 0;\n}\n\nstatic void scrub_write_block_to_dev_replace(struct scrub_block *sblock)\n{\n\tstruct btrfs_fs_info *fs_info = sblock->sctx->fs_info;\n\tint page_num;\n\n\t/*\n\t * This block is used for the check of the parity on the source device,\n\t * so the data needn't be written into the destination device.\n\t */\n\tif (sblock->sparity)\n\t\treturn;\n\n\tfor (page_num = 0; page_num < sblock->page_count; page_num++) {\n\t\tint ret;\n\n\t\tret = scrub_write_page_to_dev_replace(sblock, page_num);\n\t\tif (ret)\n\t\t\tatomic64_inc(&fs_info->dev_replace.num_write_errors);\n\t}\n}\n\nstatic int scrub_write_page_to_dev_replace(struct scrub_block *sblock,\n\t\t\t\t\t   int page_num)\n{\n\tstruct scrub_page *spage = sblock->pagev[page_num];\n\n\tBUG_ON(spage->page == NULL);\n\tif (spage->io_error) {\n\t\tvoid *mapped_buffer = kmap_atomic(spage->page);\n\n\t\tclear_page(mapped_buffer);\n\t\tflush_dcache_page(spage->page);\n\t\tkunmap_atomic(mapped_buffer);\n\t}\n\treturn scrub_add_page_to_wr_bio(sblock->sctx, spage);\n}\n\nstatic int scrub_add_page_to_wr_bio(struct scrub_ctx *sctx,\n\t\t\t\t    struct scrub_page *spage)\n{\n\tstruct scrub_bio *sbio;\n\tint ret;\n\n\tmutex_lock(&sctx->wr_lock);\nagain:\n\tif (!sctx->wr_curr_bio) {\n\t\tsctx->wr_curr_bio = kzalloc(sizeof(*sctx->wr_curr_bio),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!sctx->wr_curr_bio) {\n\t\t\tmutex_unlock(&sctx->wr_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tsctx->wr_curr_bio->sctx = sctx;\n\t\tsctx->wr_curr_bio->page_count = 0;\n\t}\n\tsbio = sctx->wr_curr_bio;\n\tif (sbio->page_count == 0) {\n\t\tstruct bio *bio;\n\n\t\tsbio->physical = spage->physical_for_dev_replace;\n\t\tsbio->logical = spage->logical;\n\t\tsbio->dev = sctx->wr_tgtdev;\n\t\tbio = sbio->bio;\n\t\tif (!bio) {\n\t\t\tbio = btrfs_io_bio_alloc(sctx->pages_per_wr_bio);\n\t\t\tsbio->bio = bio;\n\t\t}\n\n\t\tbio->bi_private = sbio;\n\t\tbio->bi_end_io = scrub_wr_bio_end_io;\n\t\tbio_set_dev(bio, sbio->dev->bdev);\n\t\tbio->bi_iter.bi_sector = sbio->physical >> 9;\n\t\tbio->bi_opf = REQ_OP_WRITE;\n\t\tsbio->status = 0;\n\t} else if (sbio->physical + sbio->page_count * PAGE_SIZE !=\n\t\t   spage->physical_for_dev_replace ||\n\t\t   sbio->logical + sbio->page_count * PAGE_SIZE !=\n\t\t   spage->logical) {\n\t\tscrub_wr_submit(sctx);\n\t\tgoto again;\n\t}\n\n\tret = bio_add_page(sbio->bio, spage->page, PAGE_SIZE, 0);\n\tif (ret != PAGE_SIZE) {\n\t\tif (sbio->page_count < 1) {\n\t\t\tbio_put(sbio->bio);\n\t\t\tsbio->bio = NULL;\n\t\t\tmutex_unlock(&sctx->wr_lock);\n\t\t\treturn -EIO;\n\t\t}\n\t\tscrub_wr_submit(sctx);\n\t\tgoto again;\n\t}\n\n\tsbio->pagev[sbio->page_count] = spage;\n\tscrub_page_get(spage);\n\tsbio->page_count++;\n\tif (sbio->page_count == sctx->pages_per_wr_bio)\n\t\tscrub_wr_submit(sctx);\n\tmutex_unlock(&sctx->wr_lock);\n\n\treturn 0;\n}\n\nstatic void scrub_wr_submit(struct scrub_ctx *sctx)\n{\n\tstruct scrub_bio *sbio;\n\n\tif (!sctx->wr_curr_bio)\n\t\treturn;\n\n\tsbio = sctx->wr_curr_bio;\n\tsctx->wr_curr_bio = NULL;\n\tWARN_ON(!sbio->bio->bi_disk);\n\tscrub_pending_bio_inc(sctx);\n\t/* process all writes in a single worker thread. Then the block layer\n\t * orders the requests before sending them to the driver which\n\t * doubled the write performance on spinning disks when measured\n\t * with Linux 3.5 */\n\tbtrfsic_submit_bio(sbio->bio);\n}\n\nstatic void scrub_wr_bio_end_io(struct bio *bio)\n{\n\tstruct scrub_bio *sbio = bio->bi_private;\n\tstruct btrfs_fs_info *fs_info = sbio->dev->fs_info;\n\n\tsbio->status = bio->bi_status;\n\tsbio->bio = bio;\n\n\tbtrfs_init_work(&sbio->work, btrfs_scrubwrc_helper,\n\t\t\t scrub_wr_bio_end_io_worker, NULL, NULL);\n\tbtrfs_queue_work(fs_info->scrub_wr_completion_workers, &sbio->work);\n}\n\nstatic void scrub_wr_bio_end_io_worker(struct btrfs_work *work)\n{\n\tstruct scrub_bio *sbio = container_of(work, struct scrub_bio, work);\n\tstruct scrub_ctx *sctx = sbio->sctx;\n\tint i;\n\n\tWARN_ON(sbio->page_count > SCRUB_PAGES_PER_WR_BIO);\n\tif (sbio->status) {\n\t\tstruct btrfs_dev_replace *dev_replace =\n\t\t\t&sbio->sctx->fs_info->dev_replace;\n\n\t\tfor (i = 0; i < sbio->page_count; i++) {\n\t\t\tstruct scrub_page *spage = sbio->pagev[i];\n\n\t\t\tspage->io_error = 1;\n\t\t\tatomic64_inc(&dev_replace->num_write_errors);\n\t\t}\n\t}\n\n\tfor (i = 0; i < sbio->page_count; i++)\n\t\tscrub_page_put(sbio->pagev[i]);\n\n\tbio_put(sbio->bio);\n\tkfree(sbio);\n\tscrub_pending_bio_dec(sctx);\n}\n\nstatic int scrub_checksum(struct scrub_block *sblock)\n{\n\tu64 flags;\n\tint ret;\n\n\t/*\n\t * No need to initialize these stats currently,\n\t * because this function only use return value\n\t * instead of these stats value.\n\t *\n\t * Todo:\n\t * always use stats\n\t */\n\tsblock->header_error = 0;\n\tsblock->generation_error = 0;\n\tsblock->checksum_error = 0;\n\n\tWARN_ON(sblock->page_count < 1);\n\tflags = sblock->pagev[0]->flags;\n\tret = 0;\n\tif (flags & BTRFS_EXTENT_FLAG_DATA)\n\t\tret = scrub_checksum_data(sblock);\n\telse if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK)\n\t\tret = scrub_checksum_tree_block(sblock);\n\telse if (flags & BTRFS_EXTENT_FLAG_SUPER)\n\t\t(void)scrub_checksum_super(sblock);\n\telse\n\t\tWARN_ON(1);\n\tif (ret)\n\t\tscrub_handle_errored_block(sblock);\n\n\treturn ret;\n}\n\nstatic int scrub_checksum_data(struct scrub_block *sblock)\n{\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tu8 csum[BTRFS_CSUM_SIZE];\n\tu8 *on_disk_csum;\n\tstruct page *page;\n\tvoid *buffer;\n\tu32 crc = ~(u32)0;\n\tu64 len;\n\tint index;\n\n\tBUG_ON(sblock->page_count < 1);\n\tif (!sblock->pagev[0]->have_csum)\n\t\treturn 0;\n\n\ton_disk_csum = sblock->pagev[0]->csum;\n\tpage = sblock->pagev[0]->page;\n\tbuffer = kmap_atomic(page);\n\n\tlen = sctx->fs_info->sectorsize;\n\tindex = 0;\n\tfor (;;) {\n\t\tu64 l = min_t(u64, len, PAGE_SIZE);\n\n\t\tcrc = btrfs_csum_data(buffer, crc, l);\n\t\tkunmap_atomic(buffer);\n\t\tlen -= l;\n\t\tif (len == 0)\n\t\t\tbreak;\n\t\tindex++;\n\t\tBUG_ON(index >= sblock->page_count);\n\t\tBUG_ON(!sblock->pagev[index]->page);\n\t\tpage = sblock->pagev[index]->page;\n\t\tbuffer = kmap_atomic(page);\n\t}\n\n\tbtrfs_csum_final(crc, csum);\n\tif (memcmp(csum, on_disk_csum, sctx->csum_size))\n\t\tsblock->checksum_error = 1;\n\n\treturn sblock->checksum_error;\n}\n\nstatic int scrub_checksum_tree_block(struct scrub_block *sblock)\n{\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tstruct btrfs_header *h;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tu8 calculated_csum[BTRFS_CSUM_SIZE];\n\tu8 on_disk_csum[BTRFS_CSUM_SIZE];\n\tstruct page *page;\n\tvoid *mapped_buffer;\n\tu64 mapped_size;\n\tvoid *p;\n\tu32 crc = ~(u32)0;\n\tu64 len;\n\tint index;\n\n\tBUG_ON(sblock->page_count < 1);\n\tpage = sblock->pagev[0]->page;\n\tmapped_buffer = kmap_atomic(page);\n\th = (struct btrfs_header *)mapped_buffer;\n\tmemcpy(on_disk_csum, h->csum, sctx->csum_size);\n\n\t/*\n\t * we don't use the getter functions here, as we\n\t * a) don't have an extent buffer and\n\t * b) the page is already kmapped\n\t */\n\tif (sblock->pagev[0]->logical != btrfs_stack_header_bytenr(h))\n\t\tsblock->header_error = 1;\n\n\tif (sblock->pagev[0]->generation != btrfs_stack_header_generation(h)) {\n\t\tsblock->header_error = 1;\n\t\tsblock->generation_error = 1;\n\t}\n\n\tif (!scrub_check_fsid(h->fsid, sblock->pagev[0]))\n\t\tsblock->header_error = 1;\n\n\tif (memcmp(h->chunk_tree_uuid, fs_info->chunk_tree_uuid,\n\t\t   BTRFS_UUID_SIZE))\n\t\tsblock->header_error = 1;\n\n\tlen = sctx->fs_info->nodesize - BTRFS_CSUM_SIZE;\n\tmapped_size = PAGE_SIZE - BTRFS_CSUM_SIZE;\n\tp = ((u8 *)mapped_buffer) + BTRFS_CSUM_SIZE;\n\tindex = 0;\n\tfor (;;) {\n\t\tu64 l = min_t(u64, len, mapped_size);\n\n\t\tcrc = btrfs_csum_data(p, crc, l);\n\t\tkunmap_atomic(mapped_buffer);\n\t\tlen -= l;\n\t\tif (len == 0)\n\t\t\tbreak;\n\t\tindex++;\n\t\tBUG_ON(index >= sblock->page_count);\n\t\tBUG_ON(!sblock->pagev[index]->page);\n\t\tpage = sblock->pagev[index]->page;\n\t\tmapped_buffer = kmap_atomic(page);\n\t\tmapped_size = PAGE_SIZE;\n\t\tp = mapped_buffer;\n\t}\n\n\tbtrfs_csum_final(crc, calculated_csum);\n\tif (memcmp(calculated_csum, on_disk_csum, sctx->csum_size))\n\t\tsblock->checksum_error = 1;\n\n\treturn sblock->header_error || sblock->checksum_error;\n}\n\nstatic int scrub_checksum_super(struct scrub_block *sblock)\n{\n\tstruct btrfs_super_block *s;\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tu8 calculated_csum[BTRFS_CSUM_SIZE];\n\tu8 on_disk_csum[BTRFS_CSUM_SIZE];\n\tstruct page *page;\n\tvoid *mapped_buffer;\n\tu64 mapped_size;\n\tvoid *p;\n\tu32 crc = ~(u32)0;\n\tint fail_gen = 0;\n\tint fail_cor = 0;\n\tu64 len;\n\tint index;\n\n\tBUG_ON(sblock->page_count < 1);\n\tpage = sblock->pagev[0]->page;\n\tmapped_buffer = kmap_atomic(page);\n\ts = (struct btrfs_super_block *)mapped_buffer;\n\tmemcpy(on_disk_csum, s->csum, sctx->csum_size);\n\n\tif (sblock->pagev[0]->logical != btrfs_super_bytenr(s))\n\t\t++fail_cor;\n\n\tif (sblock->pagev[0]->generation != btrfs_super_generation(s))\n\t\t++fail_gen;\n\n\tif (!scrub_check_fsid(s->fsid, sblock->pagev[0]))\n\t\t++fail_cor;\n\n\tlen = BTRFS_SUPER_INFO_SIZE - BTRFS_CSUM_SIZE;\n\tmapped_size = PAGE_SIZE - BTRFS_CSUM_SIZE;\n\tp = ((u8 *)mapped_buffer) + BTRFS_CSUM_SIZE;\n\tindex = 0;\n\tfor (;;) {\n\t\tu64 l = min_t(u64, len, mapped_size);\n\n\t\tcrc = btrfs_csum_data(p, crc, l);\n\t\tkunmap_atomic(mapped_buffer);\n\t\tlen -= l;\n\t\tif (len == 0)\n\t\t\tbreak;\n\t\tindex++;\n\t\tBUG_ON(index >= sblock->page_count);\n\t\tBUG_ON(!sblock->pagev[index]->page);\n\t\tpage = sblock->pagev[index]->page;\n\t\tmapped_buffer = kmap_atomic(page);\n\t\tmapped_size = PAGE_SIZE;\n\t\tp = mapped_buffer;\n\t}\n\n\tbtrfs_csum_final(crc, calculated_csum);\n\tif (memcmp(calculated_csum, on_disk_csum, sctx->csum_size))\n\t\t++fail_cor;\n\n\tif (fail_cor + fail_gen) {\n\t\t/*\n\t\t * if we find an error in a super block, we just report it.\n\t\t * They will get written with the next transaction commit\n\t\t * anyway\n\t\t */\n\t\tspin_lock(&sctx->stat_lock);\n\t\t++sctx->stat.super_errors;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (fail_cor)\n\t\t\tbtrfs_dev_stat_inc_and_print(sblock->pagev[0]->dev,\n\t\t\t\tBTRFS_DEV_STAT_CORRUPTION_ERRS);\n\t\telse\n\t\t\tbtrfs_dev_stat_inc_and_print(sblock->pagev[0]->dev,\n\t\t\t\tBTRFS_DEV_STAT_GENERATION_ERRS);\n\t}\n\n\treturn fail_cor + fail_gen;\n}\n\nstatic void scrub_block_get(struct scrub_block *sblock)\n{\n\trefcount_inc(&sblock->refs);\n}\n\nstatic void scrub_block_put(struct scrub_block *sblock)\n{\n\tif (refcount_dec_and_test(&sblock->refs)) {\n\t\tint i;\n\n\t\tif (sblock->sparity)\n\t\t\tscrub_parity_put(sblock->sparity);\n\n\t\tfor (i = 0; i < sblock->page_count; i++)\n\t\t\tscrub_page_put(sblock->pagev[i]);\n\t\tkfree(sblock);\n\t}\n}\n\nstatic void scrub_page_get(struct scrub_page *spage)\n{\n\tatomic_inc(&spage->refs);\n}\n\nstatic void scrub_page_put(struct scrub_page *spage)\n{\n\tif (atomic_dec_and_test(&spage->refs)) {\n\t\tif (spage->page)\n\t\t\t__free_page(spage->page);\n\t\tkfree(spage);\n\t}\n}\n\nstatic void scrub_submit(struct scrub_ctx *sctx)\n{\n\tstruct scrub_bio *sbio;\n\n\tif (sctx->curr == -1)\n\t\treturn;\n\n\tsbio = sctx->bios[sctx->curr];\n\tsctx->curr = -1;\n\tscrub_pending_bio_inc(sctx);\n\tbtrfsic_submit_bio(sbio->bio);\n}\n\nstatic int scrub_add_page_to_rd_bio(struct scrub_ctx *sctx,\n\t\t\t\t    struct scrub_page *spage)\n{\n\tstruct scrub_block *sblock = spage->sblock;\n\tstruct scrub_bio *sbio;\n\tint ret;\n\nagain:\n\t/*\n\t * grab a fresh bio or wait for one to become available\n\t */\n\twhile (sctx->curr == -1) {\n\t\tspin_lock(&sctx->list_lock);\n\t\tsctx->curr = sctx->first_free;\n\t\tif (sctx->curr != -1) {\n\t\t\tsctx->first_free = sctx->bios[sctx->curr]->next_free;\n\t\t\tsctx->bios[sctx->curr]->next_free = -1;\n\t\t\tsctx->bios[sctx->curr]->page_count = 0;\n\t\t\tspin_unlock(&sctx->list_lock);\n\t\t} else {\n\t\t\tspin_unlock(&sctx->list_lock);\n\t\t\twait_event(sctx->list_wait, sctx->first_free != -1);\n\t\t}\n\t}\n\tsbio = sctx->bios[sctx->curr];\n\tif (sbio->page_count == 0) {\n\t\tstruct bio *bio;\n\n\t\tsbio->physical = spage->physical;\n\t\tsbio->logical = spage->logical;\n\t\tsbio->dev = spage->dev;\n\t\tbio = sbio->bio;\n\t\tif (!bio) {\n\t\t\tbio = btrfs_io_bio_alloc(sctx->pages_per_rd_bio);\n\t\t\tsbio->bio = bio;\n\t\t}\n\n\t\tbio->bi_private = sbio;\n\t\tbio->bi_end_io = scrub_bio_end_io;\n\t\tbio_set_dev(bio, sbio->dev->bdev);\n\t\tbio->bi_iter.bi_sector = sbio->physical >> 9;\n\t\tbio->bi_opf = REQ_OP_READ;\n\t\tsbio->status = 0;\n\t} else if (sbio->physical + sbio->page_count * PAGE_SIZE !=\n\t\t   spage->physical ||\n\t\t   sbio->logical + sbio->page_count * PAGE_SIZE !=\n\t\t   spage->logical ||\n\t\t   sbio->dev != spage->dev) {\n\t\tscrub_submit(sctx);\n\t\tgoto again;\n\t}\n\n\tsbio->pagev[sbio->page_count] = spage;\n\tret = bio_add_page(sbio->bio, spage->page, PAGE_SIZE, 0);\n\tif (ret != PAGE_SIZE) {\n\t\tif (sbio->page_count < 1) {\n\t\t\tbio_put(sbio->bio);\n\t\t\tsbio->bio = NULL;\n\t\t\treturn -EIO;\n\t\t}\n\t\tscrub_submit(sctx);\n\t\tgoto again;\n\t}\n\n\tscrub_block_get(sblock); /* one for the page added to the bio */\n\tatomic_inc(&sblock->outstanding_pages);\n\tsbio->page_count++;\n\tif (sbio->page_count == sctx->pages_per_rd_bio)\n\t\tscrub_submit(sctx);\n\n\treturn 0;\n}\n\nstatic void scrub_missing_raid56_end_io(struct bio *bio)\n{\n\tstruct scrub_block *sblock = bio->bi_private;\n\tstruct btrfs_fs_info *fs_info = sblock->sctx->fs_info;\n\n\tif (bio->bi_status)\n\t\tsblock->no_io_error_seen = 0;\n\n\tbio_put(bio);\n\n\tbtrfs_queue_work(fs_info->scrub_workers, &sblock->work);\n}\n\nstatic void scrub_missing_raid56_worker(struct btrfs_work *work)\n{\n\tstruct scrub_block *sblock = container_of(work, struct scrub_block, work);\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tu64 logical;\n\tstruct btrfs_device *dev;\n\n\tlogical = sblock->pagev[0]->logical;\n\tdev = sblock->pagev[0]->dev;\n\n\tif (sblock->no_io_error_seen)\n\t\tscrub_recheck_block_checksum(sblock);\n\n\tif (!sblock->no_io_error_seen) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.read_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\"IO error rebuilding logical %llu for dev %s\",\n\t\t\tlogical, rcu_str_deref(dev->name));\n\t} else if (sblock->header_error || sblock->checksum_error) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\"failed to rebuild valid logical %llu for dev %s\",\n\t\t\tlogical, rcu_str_deref(dev->name));\n\t} else {\n\t\tscrub_write_block_to_dev_replace(sblock);\n\t}\n\n\tscrub_block_put(sblock);\n\n\tif (sctx->is_dev_replace && sctx->flush_all_writes) {\n\t\tmutex_lock(&sctx->wr_lock);\n\t\tscrub_wr_submit(sctx);\n\t\tmutex_unlock(&sctx->wr_lock);\n\t}\n\n\tscrub_pending_bio_dec(sctx);\n}\n\nstatic void scrub_missing_raid56_pages(struct scrub_block *sblock)\n{\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tu64 length = sblock->page_count * PAGE_SIZE;\n\tu64 logical = sblock->pagev[0]->logical;\n\tstruct btrfs_bio *bbio = NULL;\n\tstruct bio *bio;\n\tstruct btrfs_raid_bio *rbio;\n\tint ret;\n\tint i;\n\n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\tret = btrfs_map_sblock(fs_info, BTRFS_MAP_GET_READ_MIRRORS, logical,\n\t\t\t&length, &bbio);\n\tif (ret || !bbio || !bbio->raid_map)\n\t\tgoto bbio_out;\n\n\tif (WARN_ON(!sctx->is_dev_replace ||\n\t\t    !(bbio->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK))) {\n\t\t/*\n\t\t * We shouldn't be scrubbing a missing device. Even for dev\n\t\t * replace, we should only get here for RAID 5/6. We either\n\t\t * managed to mount something with no mirrors remaining or\n\t\t * there's a bug in scrub_remap_extent()/btrfs_map_block().\n\t\t */\n\t\tgoto bbio_out;\n\t}\n\n\tbio = btrfs_io_bio_alloc(0);\n\tbio->bi_iter.bi_sector = logical >> 9;\n\tbio->bi_private = sblock;\n\tbio->bi_end_io = scrub_missing_raid56_end_io;\n\n\trbio = raid56_alloc_missing_rbio(fs_info, bio, bbio, length);\n\tif (!rbio)\n\t\tgoto rbio_out;\n\n\tfor (i = 0; i < sblock->page_count; i++) {\n\t\tstruct scrub_page *spage = sblock->pagev[i];\n\n\t\traid56_add_scrub_pages(rbio, spage->page, spage->logical);\n\t}\n\n\tbtrfs_init_work(&sblock->work, btrfs_scrub_helper,\n\t\t\tscrub_missing_raid56_worker, NULL, NULL);\n\tscrub_block_get(sblock);\n\tscrub_pending_bio_inc(sctx);\n\traid56_submit_missing_rbio(rbio);\n\treturn;\n\nrbio_out:\n\tbio_put(bio);\nbbio_out:\n\tbtrfs_bio_counter_dec(fs_info);\n\tbtrfs_put_bbio(bbio);\n\tspin_lock(&sctx->stat_lock);\n\tsctx->stat.malloc_errors++;\n\tspin_unlock(&sctx->stat_lock);\n}\n\nstatic int scrub_pages(struct scrub_ctx *sctx, u64 logical, u64 len,\n\t\t       u64 physical, struct btrfs_device *dev, u64 flags,\n\t\t       u64 gen, int mirror_num, u8 *csum, int force,\n\t\t       u64 physical_for_dev_replace)\n{\n\tstruct scrub_block *sblock;\n\tint index;\n\n\tsblock = kzalloc(sizeof(*sblock), GFP_KERNEL);\n\tif (!sblock) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.malloc_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* one ref inside this function, plus one for each page added to\n\t * a bio later on */\n\trefcount_set(&sblock->refs, 1);\n\tsblock->sctx = sctx;\n\tsblock->no_io_error_seen = 1;\n\n\tfor (index = 0; len > 0; index++) {\n\t\tstruct scrub_page *spage;\n\t\tu64 l = min_t(u64, len, PAGE_SIZE);\n\n\t\tspage = kzalloc(sizeof(*spage), GFP_KERNEL);\n\t\tif (!spage) {\nleave_nomem:\n\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\tsctx->stat.malloc_errors++;\n\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\tscrub_block_put(sblock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tBUG_ON(index >= SCRUB_MAX_PAGES_PER_BLOCK);\n\t\tscrub_page_get(spage);\n\t\tsblock->pagev[index] = spage;\n\t\tspage->sblock = sblock;\n\t\tspage->dev = dev;\n\t\tspage->flags = flags;\n\t\tspage->generation = gen;\n\t\tspage->logical = logical;\n\t\tspage->physical = physical;\n\t\tspage->physical_for_dev_replace = physical_for_dev_replace;\n\t\tspage->mirror_num = mirror_num;\n\t\tif (csum) {\n\t\t\tspage->have_csum = 1;\n\t\t\tmemcpy(spage->csum, csum, sctx->csum_size);\n\t\t} else {\n\t\t\tspage->have_csum = 0;\n\t\t}\n\t\tsblock->page_count++;\n\t\tspage->page = alloc_page(GFP_KERNEL);\n\t\tif (!spage->page)\n\t\t\tgoto leave_nomem;\n\t\tlen -= l;\n\t\tlogical += l;\n\t\tphysical += l;\n\t\tphysical_for_dev_replace += l;\n\t}\n\n\tWARN_ON(sblock->page_count == 0);\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state)) {\n\t\t/*\n\t\t * This case should only be hit for RAID 5/6 device replace. See\n\t\t * the comment in scrub_missing_raid56_pages() for details.\n\t\t */\n\t\tscrub_missing_raid56_pages(sblock);\n\t} else {\n\t\tfor (index = 0; index < sblock->page_count; index++) {\n\t\t\tstruct scrub_page *spage = sblock->pagev[index];\n\t\t\tint ret;\n\n\t\t\tret = scrub_add_page_to_rd_bio(sctx, spage);\n\t\t\tif (ret) {\n\t\t\t\tscrub_block_put(sblock);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\tif (force)\n\t\t\tscrub_submit(sctx);\n\t}\n\n\t/* last one frees, either here or in bio completion for last page */\n\tscrub_block_put(sblock);\n\treturn 0;\n}\n\nstatic void scrub_bio_end_io(struct bio *bio)\n{\n\tstruct scrub_bio *sbio = bio->bi_private;\n\tstruct btrfs_fs_info *fs_info = sbio->dev->fs_info;\n\n\tsbio->status = bio->bi_status;\n\tsbio->bio = bio;\n\n\tbtrfs_queue_work(fs_info->scrub_workers, &sbio->work);\n}\n\nstatic void scrub_bio_end_io_worker(struct btrfs_work *work)\n{\n\tstruct scrub_bio *sbio = container_of(work, struct scrub_bio, work);\n\tstruct scrub_ctx *sctx = sbio->sctx;\n\tint i;\n\n\tBUG_ON(sbio->page_count > SCRUB_PAGES_PER_RD_BIO);\n\tif (sbio->status) {\n\t\tfor (i = 0; i < sbio->page_count; i++) {\n\t\t\tstruct scrub_page *spage = sbio->pagev[i];\n\n\t\t\tspage->io_error = 1;\n\t\t\tspage->sblock->no_io_error_seen = 0;\n\t\t}\n\t}\n\n\t/* now complete the scrub_block items that have all pages completed */\n\tfor (i = 0; i < sbio->page_count; i++) {\n\t\tstruct scrub_page *spage = sbio->pagev[i];\n\t\tstruct scrub_block *sblock = spage->sblock;\n\n\t\tif (atomic_dec_and_test(&sblock->outstanding_pages))\n\t\t\tscrub_block_complete(sblock);\n\t\tscrub_block_put(sblock);\n\t}\n\n\tbio_put(sbio->bio);\n\tsbio->bio = NULL;\n\tspin_lock(&sctx->list_lock);\n\tsbio->next_free = sctx->first_free;\n\tsctx->first_free = sbio->index;\n\tspin_unlock(&sctx->list_lock);\n\n\tif (sctx->is_dev_replace && sctx->flush_all_writes) {\n\t\tmutex_lock(&sctx->wr_lock);\n\t\tscrub_wr_submit(sctx);\n\t\tmutex_unlock(&sctx->wr_lock);\n\t}\n\n\tscrub_pending_bio_dec(sctx);\n}\n\nstatic inline void __scrub_mark_bitmap(struct scrub_parity *sparity,\n\t\t\t\t       unsigned long *bitmap,\n\t\t\t\t       u64 start, u64 len)\n{\n\tu64 offset;\n\tu64 nsectors64;\n\tu32 nsectors;\n\tint sectorsize = sparity->sctx->fs_info->sectorsize;\n\n\tif (len >= sparity->stripe_len) {\n\t\tbitmap_set(bitmap, 0, sparity->nsectors);\n\t\treturn;\n\t}\n\n\tstart -= sparity->logic_start;\n\tstart = div64_u64_rem(start, sparity->stripe_len, &offset);\n\toffset = div_u64(offset, sectorsize);\n\tnsectors64 = div_u64(len, sectorsize);\n\n\tASSERT(nsectors64 < UINT_MAX);\n\tnsectors = (u32)nsectors64;\n\n\tif (offset + nsectors <= sparity->nsectors) {\n\t\tbitmap_set(bitmap, offset, nsectors);\n\t\treturn;\n\t}\n\n\tbitmap_set(bitmap, offset, sparity->nsectors - offset);\n\tbitmap_set(bitmap, 0, nsectors - (sparity->nsectors - offset));\n}\n\nstatic inline void scrub_parity_mark_sectors_error(struct scrub_parity *sparity,\n\t\t\t\t\t\t   u64 start, u64 len)\n{\n\t__scrub_mark_bitmap(sparity, sparity->ebitmap, start, len);\n}\n\nstatic inline void scrub_parity_mark_sectors_data(struct scrub_parity *sparity,\n\t\t\t\t\t\t  u64 start, u64 len)\n{\n\t__scrub_mark_bitmap(sparity, sparity->dbitmap, start, len);\n}\n\nstatic void scrub_block_complete(struct scrub_block *sblock)\n{\n\tint corrupted = 0;\n\n\tif (!sblock->no_io_error_seen) {\n\t\tcorrupted = 1;\n\t\tscrub_handle_errored_block(sblock);\n\t} else {\n\t\t/*\n\t\t * if has checksum error, write via repair mechanism in\n\t\t * dev replace case, otherwise write here in dev replace\n\t\t * case.\n\t\t */\n\t\tcorrupted = scrub_checksum(sblock);\n\t\tif (!corrupted && sblock->sctx->is_dev_replace)\n\t\t\tscrub_write_block_to_dev_replace(sblock);\n\t}\n\n\tif (sblock->sparity && corrupted && !sblock->data_corrected) {\n\t\tu64 start = sblock->pagev[0]->logical;\n\t\tu64 end = sblock->pagev[sblock->page_count - 1]->logical +\n\t\t\t  PAGE_SIZE;\n\n\t\tscrub_parity_mark_sectors_error(sblock->sparity,\n\t\t\t\t\t\tstart, end - start);\n\t}\n}\n\nstatic int scrub_find_csum(struct scrub_ctx *sctx, u64 logical, u8 *csum)\n{\n\tstruct btrfs_ordered_sum *sum = NULL;\n\tunsigned long index;\n\tunsigned long num_sectors;\n\n\twhile (!list_empty(&sctx->csum_list)) {\n\t\tsum = list_first_entry(&sctx->csum_list,\n\t\t\t\t       struct btrfs_ordered_sum, list);\n\t\tif (sum->bytenr > logical)\n\t\t\treturn 0;\n\t\tif (sum->bytenr + sum->len > logical)\n\t\t\tbreak;\n\n\t\t++sctx->stat.csum_discards;\n\t\tlist_del(&sum->list);\n\t\tkfree(sum);\n\t\tsum = NULL;\n\t}\n\tif (!sum)\n\t\treturn 0;\n\n\tindex = div_u64(logical - sum->bytenr, sctx->fs_info->sectorsize);\n\tASSERT(index < UINT_MAX);\n\n\tnum_sectors = sum->len / sctx->fs_info->sectorsize;\n\tmemcpy(csum, sum->sums + index, sctx->csum_size);\n\tif (index == num_sectors - 1) {\n\t\tlist_del(&sum->list);\n\t\tkfree(sum);\n\t}\n\treturn 1;\n}\n\n/* scrub extent tries to collect up to 64 kB for each bio */\nstatic int scrub_extent(struct scrub_ctx *sctx, struct map_lookup *map,\n\t\t\tu64 logical, u64 len,\n\t\t\tu64 physical, struct btrfs_device *dev, u64 flags,\n\t\t\tu64 gen, int mirror_num, u64 physical_for_dev_replace)\n{\n\tint ret;\n\tu8 csum[BTRFS_CSUM_SIZE];\n\tu32 blocksize;\n\n\tif (flags & BTRFS_EXTENT_FLAG_DATA) {\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tblocksize = map->stripe_len;\n\t\telse\n\t\t\tblocksize = sctx->fs_info->sectorsize;\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.data_extents_scrubbed++;\n\t\tsctx->stat.data_bytes_scrubbed += len;\n\t\tspin_unlock(&sctx->stat_lock);\n\t} else if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tblocksize = map->stripe_len;\n\t\telse\n\t\t\tblocksize = sctx->fs_info->nodesize;\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.tree_extents_scrubbed++;\n\t\tsctx->stat.tree_bytes_scrubbed += len;\n\t\tspin_unlock(&sctx->stat_lock);\n\t} else {\n\t\tblocksize = sctx->fs_info->sectorsize;\n\t\tWARN_ON(1);\n\t}\n\n\twhile (len) {\n\t\tu64 l = min_t(u64, len, blocksize);\n\t\tint have_csum = 0;\n\n\t\tif (flags & BTRFS_EXTENT_FLAG_DATA) {\n\t\t\t/* push csums to sbio */\n\t\t\thave_csum = scrub_find_csum(sctx, logical, csum);\n\t\t\tif (have_csum == 0)\n\t\t\t\t++sctx->stat.no_csum;\n\t\t}\n\t\tret = scrub_pages(sctx, logical, l, physical, dev, flags, gen,\n\t\t\t\t  mirror_num, have_csum ? csum : NULL, 0,\n\t\t\t\t  physical_for_dev_replace);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tlen -= l;\n\t\tlogical += l;\n\t\tphysical += l;\n\t\tphysical_for_dev_replace += l;\n\t}\n\treturn 0;\n}\n\nstatic int scrub_pages_for_parity(struct scrub_parity *sparity,\n\t\t\t\t  u64 logical, u64 len,\n\t\t\t\t  u64 physical, struct btrfs_device *dev,\n\t\t\t\t  u64 flags, u64 gen, int mirror_num, u8 *csum)\n{\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\tstruct scrub_block *sblock;\n\tint index;\n\n\tsblock = kzalloc(sizeof(*sblock), GFP_KERNEL);\n\tif (!sblock) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.malloc_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* one ref inside this function, plus one for each page added to\n\t * a bio later on */\n\trefcount_set(&sblock->refs, 1);\n\tsblock->sctx = sctx;\n\tsblock->no_io_error_seen = 1;\n\tsblock->sparity = sparity;\n\tscrub_parity_get(sparity);\n\n\tfor (index = 0; len > 0; index++) {\n\t\tstruct scrub_page *spage;\n\t\tu64 l = min_t(u64, len, PAGE_SIZE);\n\n\t\tspage = kzalloc(sizeof(*spage), GFP_KERNEL);\n\t\tif (!spage) {\nleave_nomem:\n\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\tsctx->stat.malloc_errors++;\n\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\tscrub_block_put(sblock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tBUG_ON(index >= SCRUB_MAX_PAGES_PER_BLOCK);\n\t\t/* For scrub block */\n\t\tscrub_page_get(spage);\n\t\tsblock->pagev[index] = spage;\n\t\t/* For scrub parity */\n\t\tscrub_page_get(spage);\n\t\tlist_add_tail(&spage->list, &sparity->spages);\n\t\tspage->sblock = sblock;\n\t\tspage->dev = dev;\n\t\tspage->flags = flags;\n\t\tspage->generation = gen;\n\t\tspage->logical = logical;\n\t\tspage->physical = physical;\n\t\tspage->mirror_num = mirror_num;\n\t\tif (csum) {\n\t\t\tspage->have_csum = 1;\n\t\t\tmemcpy(spage->csum, csum, sctx->csum_size);\n\t\t} else {\n\t\t\tspage->have_csum = 0;\n\t\t}\n\t\tsblock->page_count++;\n\t\tspage->page = alloc_page(GFP_KERNEL);\n\t\tif (!spage->page)\n\t\t\tgoto leave_nomem;\n\t\tlen -= l;\n\t\tlogical += l;\n\t\tphysical += l;\n\t}\n\n\tWARN_ON(sblock->page_count == 0);\n\tfor (index = 0; index < sblock->page_count; index++) {\n\t\tstruct scrub_page *spage = sblock->pagev[index];\n\t\tint ret;\n\n\t\tret = scrub_add_page_to_rd_bio(sctx, spage);\n\t\tif (ret) {\n\t\t\tscrub_block_put(sblock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* last one frees, either here or in bio completion for last page */\n\tscrub_block_put(sblock);\n\treturn 0;\n}\n\nstatic int scrub_extent_for_parity(struct scrub_parity *sparity,\n\t\t\t\t   u64 logical, u64 len,\n\t\t\t\t   u64 physical, struct btrfs_device *dev,\n\t\t\t\t   u64 flags, u64 gen, int mirror_num)\n{\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\tint ret;\n\tu8 csum[BTRFS_CSUM_SIZE];\n\tu32 blocksize;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state)) {\n\t\tscrub_parity_mark_sectors_error(sparity, logical, len);\n\t\treturn 0;\n\t}\n\n\tif (flags & BTRFS_EXTENT_FLAG_DATA) {\n\t\tblocksize = sparity->stripe_len;\n\t} else if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\n\t\tblocksize = sparity->stripe_len;\n\t} else {\n\t\tblocksize = sctx->fs_info->sectorsize;\n\t\tWARN_ON(1);\n\t}\n\n\twhile (len) {\n\t\tu64 l = min_t(u64, len, blocksize);\n\t\tint have_csum = 0;\n\n\t\tif (flags & BTRFS_EXTENT_FLAG_DATA) {\n\t\t\t/* push csums to sbio */\n\t\t\thave_csum = scrub_find_csum(sctx, logical, csum);\n\t\t\tif (have_csum == 0)\n\t\t\t\tgoto skip;\n\t\t}\n\t\tret = scrub_pages_for_parity(sparity, logical, l, physical, dev,\n\t\t\t\t\t     flags, gen, mirror_num,\n\t\t\t\t\t     have_csum ? csum : NULL);\n\t\tif (ret)\n\t\t\treturn ret;\nskip:\n\t\tlen -= l;\n\t\tlogical += l;\n\t\tphysical += l;\n\t}\n\treturn 0;\n}\n\n/*\n * Given a physical address, this will calculate it's\n * logical offset. if this is a parity stripe, it will return\n * the most left data stripe's logical offset.\n *\n * return 0 if it is a data stripe, 1 means parity stripe.\n */\nstatic int get_raid56_logic_offset(u64 physical, int num,\n\t\t\t\t   struct map_lookup *map, u64 *offset,\n\t\t\t\t   u64 *stripe_start)\n{\n\tint i;\n\tint j = 0;\n\tu64 stripe_nr;\n\tu64 last_offset;\n\tu32 stripe_index;\n\tu32 rot;\n\n\tlast_offset = (physical - map->stripes[num].physical) *\n\t\t      nr_data_stripes(map);\n\tif (stripe_start)\n\t\t*stripe_start = last_offset;\n\n\t*offset = last_offset;\n\tfor (i = 0; i < nr_data_stripes(map); i++) {\n\t\t*offset = last_offset + i * map->stripe_len;\n\n\t\tstripe_nr = div64_u64(*offset, map->stripe_len);\n\t\tstripe_nr = div_u64(stripe_nr, nr_data_stripes(map));\n\n\t\t/* Work out the disk rotation on this stripe-set */\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes, &rot);\n\t\t/* calculate which stripe this data locates */\n\t\trot += i;\n\t\tstripe_index = rot % map->num_stripes;\n\t\tif (stripe_index == num)\n\t\t\treturn 0;\n\t\tif (stripe_index < num)\n\t\t\tj++;\n\t}\n\t*offset = last_offset + j * map->stripe_len;\n\treturn 1;\n}\n\nstatic void scrub_free_parity(struct scrub_parity *sparity)\n{\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\tstruct scrub_page *curr, *next;\n\tint nbits;\n\n\tnbits = bitmap_weight(sparity->ebitmap, sparity->nsectors);\n\tif (nbits) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.read_errors += nbits;\n\t\tsctx->stat.uncorrectable_errors += nbits;\n\t\tspin_unlock(&sctx->stat_lock);\n\t}\n\n\tlist_for_each_entry_safe(curr, next, &sparity->spages, list) {\n\t\tlist_del_init(&curr->list);\n\t\tscrub_page_put(curr);\n\t}\n\n\tkfree(sparity);\n}\n\nstatic void scrub_parity_bio_endio_worker(struct btrfs_work *work)\n{\n\tstruct scrub_parity *sparity = container_of(work, struct scrub_parity,\n\t\t\t\t\t\t    work);\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\n\tscrub_free_parity(sparity);\n\tscrub_pending_bio_dec(sctx);\n}\n\nstatic void scrub_parity_bio_endio(struct bio *bio)\n{\n\tstruct scrub_parity *sparity = (struct scrub_parity *)bio->bi_private;\n\tstruct btrfs_fs_info *fs_info = sparity->sctx->fs_info;\n\n\tif (bio->bi_status)\n\t\tbitmap_or(sparity->ebitmap, sparity->ebitmap, sparity->dbitmap,\n\t\t\t  sparity->nsectors);\n\n\tbio_put(bio);\n\n\tbtrfs_init_work(&sparity->work, btrfs_scrubparity_helper,\n\t\t\tscrub_parity_bio_endio_worker, NULL, NULL);\n\tbtrfs_queue_work(fs_info->scrub_parity_workers, &sparity->work);\n}\n\nstatic void scrub_parity_check_and_repair(struct scrub_parity *sparity)\n{\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct bio *bio;\n\tstruct btrfs_raid_bio *rbio;\n\tstruct btrfs_bio *bbio = NULL;\n\tu64 length;\n\tint ret;\n\n\tif (!bitmap_andnot(sparity->dbitmap, sparity->dbitmap, sparity->ebitmap,\n\t\t\t   sparity->nsectors))\n\t\tgoto out;\n\n\tlength = sparity->logic_end - sparity->logic_start;\n\n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\tret = btrfs_map_sblock(fs_info, BTRFS_MAP_WRITE, sparity->logic_start,\n\t\t\t       &length, &bbio);\n\tif (ret || !bbio || !bbio->raid_map)\n\t\tgoto bbio_out;\n\n\tbio = btrfs_io_bio_alloc(0);\n\tbio->bi_iter.bi_sector = sparity->logic_start >> 9;\n\tbio->bi_private = sparity;\n\tbio->bi_end_io = scrub_parity_bio_endio;\n\n\trbio = raid56_parity_alloc_scrub_rbio(fs_info, bio, bbio,\n\t\t\t\t\t      length, sparity->scrub_dev,\n\t\t\t\t\t      sparity->dbitmap,\n\t\t\t\t\t      sparity->nsectors);\n\tif (!rbio)\n\t\tgoto rbio_out;\n\n\tscrub_pending_bio_inc(sctx);\n\traid56_parity_submit_scrub_rbio(rbio);\n\treturn;\n\nrbio_out:\n\tbio_put(bio);\nbbio_out:\n\tbtrfs_bio_counter_dec(fs_info);\n\tbtrfs_put_bbio(bbio);\n\tbitmap_or(sparity->ebitmap, sparity->ebitmap, sparity->dbitmap,\n\t\t  sparity->nsectors);\n\tspin_lock(&sctx->stat_lock);\n\tsctx->stat.malloc_errors++;\n\tspin_unlock(&sctx->stat_lock);\nout:\n\tscrub_free_parity(sparity);\n}\n\nstatic inline int scrub_calc_parity_bitmap_len(int nsectors)\n{\n\treturn DIV_ROUND_UP(nsectors, BITS_PER_LONG) * sizeof(long);\n}\n\nstatic void scrub_parity_get(struct scrub_parity *sparity)\n{\n\trefcount_inc(&sparity->refs);\n}\n\nstatic void scrub_parity_put(struct scrub_parity *sparity)\n{\n\tif (!refcount_dec_and_test(&sparity->refs))\n\t\treturn;\n\n\tscrub_parity_check_and_repair(sparity);\n}\n\nstatic noinline_for_stack int scrub_raid56_parity(struct scrub_ctx *sctx,\n\t\t\t\t\t\t  struct map_lookup *map,\n\t\t\t\t\t\t  struct btrfs_device *sdev,\n\t\t\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t\t\t  u64 logic_start,\n\t\t\t\t\t\t  u64 logic_end)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_root *root = fs_info->extent_root;\n\tstruct btrfs_root *csum_root = fs_info->csum_root;\n\tstruct btrfs_extent_item *extent;\n\tstruct btrfs_bio *bbio = NULL;\n\tu64 flags;\n\tint ret;\n\tint slot;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tu64 generation;\n\tu64 extent_logical;\n\tu64 extent_physical;\n\tu64 extent_len;\n\tu64 mapped_length;\n\tstruct btrfs_device *extent_dev;\n\tstruct scrub_parity *sparity;\n\tint nsectors;\n\tint bitmap_len;\n\tint extent_mirror_num;\n\tint stop_loop = 0;\n\n\tnsectors = div_u64(map->stripe_len, fs_info->sectorsize);\n\tbitmap_len = scrub_calc_parity_bitmap_len(nsectors);\n\tsparity = kzalloc(sizeof(struct scrub_parity) + 2 * bitmap_len,\n\t\t\t  GFP_NOFS);\n\tif (!sparity) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.malloc_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\tsparity->stripe_len = map->stripe_len;\n\tsparity->nsectors = nsectors;\n\tsparity->sctx = sctx;\n\tsparity->scrub_dev = sdev;\n\tsparity->logic_start = logic_start;\n\tsparity->logic_end = logic_end;\n\trefcount_set(&sparity->refs, 1);\n\tINIT_LIST_HEAD(&sparity->spages);\n\tsparity->dbitmap = sparity->bitmap;\n\tsparity->ebitmap = (void *)sparity->bitmap + bitmap_len;\n\n\tret = 0;\n\twhile (logic_start < logic_end) {\n\t\tif (btrfs_fs_incompat(fs_info, SKINNY_METADATA))\n\t\t\tkey.type = BTRFS_METADATA_ITEM_KEY;\n\t\telse\n\t\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\tkey.objectid = logic_start;\n\t\tkey.offset = (u64)-1;\n\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tif (ret > 0) {\n\t\t\tret = btrfs_previous_extent_item(root, path, 0);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tif (ret > 0) {\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\tret = btrfs_search_slot(NULL, root, &key,\n\t\t\t\t\t\t\tpath, 0, 0);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tstop_loop = 0;\n\t\twhile (1) {\n\t\t\tu64 bytes;\n\n\t\t\tl = path->nodes[0];\n\t\t\tslot = path->slots[0];\n\t\t\tif (slot >= btrfs_header_nritems(l)) {\n\t\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto out;\n\n\t\t\t\tstop_loop = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\t\tif (key.type != BTRFS_EXTENT_ITEM_KEY &&\n\t\t\t    key.type != BTRFS_METADATA_ITEM_KEY)\n\t\t\t\tgoto next;\n\n\t\t\tif (key.type == BTRFS_METADATA_ITEM_KEY)\n\t\t\t\tbytes = fs_info->nodesize;\n\t\t\telse\n\t\t\t\tbytes = key.offset;\n\n\t\t\tif (key.objectid + bytes <= logic_start)\n\t\t\t\tgoto next;\n\n\t\t\tif (key.objectid >= logic_end) {\n\t\t\t\tstop_loop = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\twhile (key.objectid >= logic_start + map->stripe_len)\n\t\t\t\tlogic_start += map->stripe_len;\n\n\t\t\textent = btrfs_item_ptr(l, slot,\n\t\t\t\t\t\tstruct btrfs_extent_item);\n\t\t\tflags = btrfs_extent_flags(l, extent);\n\t\t\tgeneration = btrfs_extent_generation(l, extent);\n\n\t\t\tif ((flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) &&\n\t\t\t    (key.objectid < logic_start ||\n\t\t\t     key.objectid + bytes >\n\t\t\t     logic_start + map->stripe_len)) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t  \"scrub: tree block %llu spanning stripes, ignored. logical=%llu\",\n\t\t\t\t\t  key.objectid, logic_start);\n\t\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\t\tsctx->stat.uncorrectable_errors++;\n\t\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\t\tgoto next;\n\t\t\t}\nagain:\n\t\t\textent_logical = key.objectid;\n\t\t\textent_len = bytes;\n\n\t\t\tif (extent_logical < logic_start) {\n\t\t\t\textent_len -= logic_start - extent_logical;\n\t\t\t\textent_logical = logic_start;\n\t\t\t}\n\n\t\t\tif (extent_logical + extent_len >\n\t\t\t    logic_start + map->stripe_len)\n\t\t\t\textent_len = logic_start + map->stripe_len -\n\t\t\t\t\t     extent_logical;\n\n\t\t\tscrub_parity_mark_sectors_data(sparity, extent_logical,\n\t\t\t\t\t\t       extent_len);\n\n\t\t\tmapped_length = extent_len;\n\t\t\tbbio = NULL;\n\t\t\tret = btrfs_map_block(fs_info, BTRFS_MAP_READ,\n\t\t\t\t\textent_logical, &mapped_length, &bbio,\n\t\t\t\t\t0);\n\t\t\tif (!ret) {\n\t\t\t\tif (!bbio || mapped_length < extent_len)\n\t\t\t\t\tret = -EIO;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_put_bbio(bbio);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\textent_physical = bbio->stripes[0].physical;\n\t\t\textent_mirror_num = bbio->mirror_num;\n\t\t\textent_dev = bbio->stripes[0].dev;\n\t\t\tbtrfs_put_bbio(bbio);\n\n\t\t\tret = btrfs_lookup_csums_range(csum_root,\n\t\t\t\t\t\textent_logical,\n\t\t\t\t\t\textent_logical + extent_len - 1,\n\t\t\t\t\t\t&sctx->csum_list, 1);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tret = scrub_extent_for_parity(sparity, extent_logical,\n\t\t\t\t\t\t      extent_len,\n\t\t\t\t\t\t      extent_physical,\n\t\t\t\t\t\t      extent_dev, flags,\n\t\t\t\t\t\t      generation,\n\t\t\t\t\t\t      extent_mirror_num);\n\n\t\t\tscrub_free_csums(sctx);\n\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tif (extent_logical + extent_len <\n\t\t\t    key.objectid + bytes) {\n\t\t\t\tlogic_start += map->stripe_len;\n\n\t\t\t\tif (logic_start >= logic_end) {\n\t\t\t\t\tstop_loop = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (logic_start < key.objectid + bytes) {\n\t\t\t\t\tcond_resched();\n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\t\t\t}\nnext:\n\t\t\tpath->slots[0]++;\n\t\t}\n\n\t\tbtrfs_release_path(path);\n\n\t\tif (stop_loop)\n\t\t\tbreak;\n\n\t\tlogic_start += map->stripe_len;\n\t}\nout:\n\tif (ret < 0)\n\t\tscrub_parity_mark_sectors_error(sparity, logic_start,\n\t\t\t\t\t\tlogic_end - logic_start);\n\tscrub_parity_put(sparity);\n\tscrub_submit(sctx);\n\tmutex_lock(&sctx->wr_lock);\n\tscrub_wr_submit(sctx);\n\tmutex_unlock(&sctx->wr_lock);\n\n\tbtrfs_release_path(path);\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic noinline_for_stack int scrub_stripe(struct scrub_ctx *sctx,\n\t\t\t\t\t   struct map_lookup *map,\n\t\t\t\t\t   struct btrfs_device *scrub_dev,\n\t\t\t\t\t   int num, u64 base, u64 length)\n{\n\tstruct btrfs_path *path, *ppath;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_root *root = fs_info->extent_root;\n\tstruct btrfs_root *csum_root = fs_info->csum_root;\n\tstruct btrfs_extent_item *extent;\n\tstruct blk_plug plug;\n\tu64 flags;\n\tint ret;\n\tint slot;\n\tu64 nstripes;\n\tstruct extent_buffer *l;\n\tu64 physical;\n\tu64 logical;\n\tu64 logic_end;\n\tu64 physical_end;\n\tu64 generation;\n\tint mirror_num;\n\tstruct reada_control *reada1;\n\tstruct reada_control *reada2;\n\tstruct btrfs_key key;\n\tstruct btrfs_key key_end;\n\tu64 increment = map->stripe_len;\n\tu64 offset;\n\tu64 extent_logical;\n\tu64 extent_physical;\n\tu64 extent_len;\n\tu64 stripe_logical;\n\tu64 stripe_end;\n\tstruct btrfs_device *extent_dev;\n\tint extent_mirror_num;\n\tint stop_loop = 0;\n\n\tphysical = map->stripes[num].physical;\n\toffset = 0;\n\tnstripes = div64_u64(length, map->stripe_len);\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID0) {\n\t\toffset = map->stripe_len * num;\n\t\tincrement = map->stripe_len * map->num_stripes;\n\t\tmirror_num = 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID10) {\n\t\tint factor = map->num_stripes / map->sub_stripes;\n\t\toffset = map->stripe_len * (num / map->sub_stripes);\n\t\tincrement = map->stripe_len * factor;\n\t\tmirror_num = num % map->sub_stripes + 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID1) {\n\t\tincrement = map->stripe_len;\n\t\tmirror_num = num % map->num_stripes + 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_DUP) {\n\t\tincrement = map->stripe_len;\n\t\tmirror_num = num % map->num_stripes + 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tget_raid56_logic_offset(physical, num, map, &offset, NULL);\n\t\tincrement = map->stripe_len * nr_data_stripes(map);\n\t\tmirror_num = 1;\n\t} else {\n\t\tincrement = map->stripe_len;\n\t\tmirror_num = 1;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tppath = btrfs_alloc_path();\n\tif (!ppath) {\n\t\tbtrfs_free_path(path);\n\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * work on commit root. The related disk blocks are static as\n\t * long as COW is applied. This means, it is save to rewrite\n\t * them to repair disk errors without any race conditions\n\t */\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tppath->search_commit_root = 1;\n\tppath->skip_locking = 1;\n\t/*\n\t * trigger the readahead for extent tree csum tree and wait for\n\t * completion. During readahead, the scrub is officially paused\n\t * to not hold off transaction commits\n\t */\n\tlogical = base + offset;\n\tphysical_end = physical + nstripes * map->stripe_len;\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tget_raid56_logic_offset(physical_end, num,\n\t\t\t\t\tmap, &logic_end, NULL);\n\t\tlogic_end += base;\n\t} else {\n\t\tlogic_end = logical + increment * nstripes;\n\t}\n\twait_event(sctx->list_wait,\n\t\t   atomic_read(&sctx->bios_in_flight) == 0);\n\tscrub_blocked_if_needed(fs_info);\n\n\t/* FIXME it might be better to start readahead at commit root */\n\tkey.objectid = logical;\n\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\tkey.offset = (u64)0;\n\tkey_end.objectid = logic_end;\n\tkey_end.type = BTRFS_METADATA_ITEM_KEY;\n\tkey_end.offset = (u64)-1;\n\treada1 = btrfs_reada_add(root, &key, &key_end);\n\n\tkey.objectid = BTRFS_EXTENT_CSUM_OBJECTID;\n\tkey.type = BTRFS_EXTENT_CSUM_KEY;\n\tkey.offset = logical;\n\tkey_end.objectid = BTRFS_EXTENT_CSUM_OBJECTID;\n\tkey_end.type = BTRFS_EXTENT_CSUM_KEY;\n\tkey_end.offset = logic_end;\n\treada2 = btrfs_reada_add(csum_root, &key, &key_end);\n\n\tif (!IS_ERR(reada1))\n\t\tbtrfs_reada_wait(reada1);\n\tif (!IS_ERR(reada2))\n\t\tbtrfs_reada_wait(reada2);\n\n\n\t/*\n\t * collect all data csums for the stripe to avoid seeking during\n\t * the scrub. This might currently (crc32) end up to be about 1MB\n\t */\n\tblk_start_plug(&plug);\n\n\t/*\n\t * now find all extents for each stripe and scrub them\n\t */\n\tret = 0;\n\twhile (physical < physical_end) {\n\t\t/*\n\t\t * canceled?\n\t\t */\n\t\tif (atomic_read(&fs_info->scrub_cancel_req) ||\n\t\t    atomic_read(&sctx->cancel_req)) {\n\t\t\tret = -ECANCELED;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * check to see if we have to pause\n\t\t */\n\t\tif (atomic_read(&fs_info->scrub_pause_req)) {\n\t\t\t/* push queued extents */\n\t\t\tsctx->flush_all_writes = true;\n\t\t\tscrub_submit(sctx);\n\t\t\tmutex_lock(&sctx->wr_lock);\n\t\t\tscrub_wr_submit(sctx);\n\t\t\tmutex_unlock(&sctx->wr_lock);\n\t\t\twait_event(sctx->list_wait,\n\t\t\t\t   atomic_read(&sctx->bios_in_flight) == 0);\n\t\t\tsctx->flush_all_writes = false;\n\t\t\tscrub_blocked_if_needed(fs_info);\n\t\t}\n\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\t\tret = get_raid56_logic_offset(physical, num, map,\n\t\t\t\t\t\t      &logical,\n\t\t\t\t\t\t      &stripe_logical);\n\t\t\tlogical += base;\n\t\t\tif (ret) {\n\t\t\t\t/* it is parity strip */\n\t\t\t\tstripe_logical += base;\n\t\t\t\tstripe_end = stripe_logical + increment;\n\t\t\t\tret = scrub_raid56_parity(sctx, map, scrub_dev,\n\t\t\t\t\t\t\t  ppath, stripe_logical,\n\t\t\t\t\t\t\t  stripe_end);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tgoto skip;\n\t\t\t}\n\t\t}\n\n\t\tif (btrfs_fs_incompat(fs_info, SKINNY_METADATA))\n\t\t\tkey.type = BTRFS_METADATA_ITEM_KEY;\n\t\telse\n\t\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\tkey.objectid = logical;\n\t\tkey.offset = (u64)-1;\n\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tif (ret > 0) {\n\t\t\tret = btrfs_previous_extent_item(root, path, 0);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tif (ret > 0) {\n\t\t\t\t/* there's no smaller item, so stick with the\n\t\t\t\t * larger one */\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\tret = btrfs_search_slot(NULL, root, &key,\n\t\t\t\t\t\t\tpath, 0, 0);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tstop_loop = 0;\n\t\twhile (1) {\n\t\t\tu64 bytes;\n\n\t\t\tl = path->nodes[0];\n\t\t\tslot = path->slots[0];\n\t\t\tif (slot >= btrfs_header_nritems(l)) {\n\t\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto out;\n\n\t\t\t\tstop_loop = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\t\tif (key.type != BTRFS_EXTENT_ITEM_KEY &&\n\t\t\t    key.type != BTRFS_METADATA_ITEM_KEY)\n\t\t\t\tgoto next;\n\n\t\t\tif (key.type == BTRFS_METADATA_ITEM_KEY)\n\t\t\t\tbytes = fs_info->nodesize;\n\t\t\telse\n\t\t\t\tbytes = key.offset;\n\n\t\t\tif (key.objectid + bytes <= logical)\n\t\t\t\tgoto next;\n\n\t\t\tif (key.objectid >= logical + map->stripe_len) {\n\t\t\t\t/* out of this device extent */\n\t\t\t\tif (key.objectid >= logic_end)\n\t\t\t\t\tstop_loop = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\textent = btrfs_item_ptr(l, slot,\n\t\t\t\t\t\tstruct btrfs_extent_item);\n\t\t\tflags = btrfs_extent_flags(l, extent);\n\t\t\tgeneration = btrfs_extent_generation(l, extent);\n\n\t\t\tif ((flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) &&\n\t\t\t    (key.objectid < logical ||\n\t\t\t     key.objectid + bytes >\n\t\t\t     logical + map->stripe_len)) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t   \"scrub: tree block %llu spanning stripes, ignored. logical=%llu\",\n\t\t\t\t       key.objectid, logical);\n\t\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\t\tsctx->stat.uncorrectable_errors++;\n\t\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\t\tgoto next;\n\t\t\t}\n\nagain:\n\t\t\textent_logical = key.objectid;\n\t\t\textent_len = bytes;\n\n\t\t\t/*\n\t\t\t * trim extent to this stripe\n\t\t\t */\n\t\t\tif (extent_logical < logical) {\n\t\t\t\textent_len -= logical - extent_logical;\n\t\t\t\textent_logical = logical;\n\t\t\t}\n\t\t\tif (extent_logical + extent_len >\n\t\t\t    logical + map->stripe_len) {\n\t\t\t\textent_len = logical + map->stripe_len -\n\t\t\t\t\t     extent_logical;\n\t\t\t}\n\n\t\t\textent_physical = extent_logical - logical + physical;\n\t\t\textent_dev = scrub_dev;\n\t\t\textent_mirror_num = mirror_num;\n\t\t\tif (sctx->is_dev_replace)\n\t\t\t\tscrub_remap_extent(fs_info, extent_logical,\n\t\t\t\t\t\t   extent_len, &extent_physical,\n\t\t\t\t\t\t   &extent_dev,\n\t\t\t\t\t\t   &extent_mirror_num);\n\n\t\t\tret = btrfs_lookup_csums_range(csum_root,\n\t\t\t\t\t\t       extent_logical,\n\t\t\t\t\t\t       extent_logical +\n\t\t\t\t\t\t       extent_len - 1,\n\t\t\t\t\t\t       &sctx->csum_list, 1);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tret = scrub_extent(sctx, map, extent_logical, extent_len,\n\t\t\t\t\t   extent_physical, extent_dev, flags,\n\t\t\t\t\t   generation, extent_mirror_num,\n\t\t\t\t\t   extent_logical - logical + physical);\n\n\t\t\tscrub_free_csums(sctx);\n\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tif (extent_logical + extent_len <\n\t\t\t    key.objectid + bytes) {\n\t\t\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\t\t\t\t/*\n\t\t\t\t\t * loop until we find next data stripe\n\t\t\t\t\t * or we have finished all stripes.\n\t\t\t\t\t */\nloop:\n\t\t\t\t\tphysical += map->stripe_len;\n\t\t\t\t\tret = get_raid56_logic_offset(physical,\n\t\t\t\t\t\t\tnum, map, &logical,\n\t\t\t\t\t\t\t&stripe_logical);\n\t\t\t\t\tlogical += base;\n\n\t\t\t\t\tif (ret && physical < physical_end) {\n\t\t\t\t\t\tstripe_logical += base;\n\t\t\t\t\t\tstripe_end = stripe_logical +\n\t\t\t\t\t\t\t\tincrement;\n\t\t\t\t\t\tret = scrub_raid56_parity(sctx,\n\t\t\t\t\t\t\tmap, scrub_dev, ppath,\n\t\t\t\t\t\t\tstripe_logical,\n\t\t\t\t\t\t\tstripe_end);\n\t\t\t\t\t\tif (ret)\n\t\t\t\t\t\t\tgoto out;\n\t\t\t\t\t\tgoto loop;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tphysical += map->stripe_len;\n\t\t\t\t\tlogical += increment;\n\t\t\t\t}\n\t\t\t\tif (logical < key.objectid + bytes) {\n\t\t\t\t\tcond_resched();\n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\n\t\t\t\tif (physical >= physical_end) {\n\t\t\t\t\tstop_loop = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\nnext:\n\t\t\tpath->slots[0]++;\n\t\t}\n\t\tbtrfs_release_path(path);\nskip:\n\t\tlogical += increment;\n\t\tphysical += map->stripe_len;\n\t\tspin_lock(&sctx->stat_lock);\n\t\tif (stop_loop)\n\t\t\tsctx->stat.last_physical = map->stripes[num].physical +\n\t\t\t\t\t\t   length;\n\t\telse\n\t\t\tsctx->stat.last_physical = physical;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (stop_loop)\n\t\t\tbreak;\n\t}\nout:\n\t/* push queued extents */\n\tscrub_submit(sctx);\n\tmutex_lock(&sctx->wr_lock);\n\tscrub_wr_submit(sctx);\n\tmutex_unlock(&sctx->wr_lock);\n\n\tblk_finish_plug(&plug);\n\tbtrfs_free_path(path);\n\tbtrfs_free_path(ppath);\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic noinline_for_stack int scrub_chunk(struct scrub_ctx *sctx,\n\t\t\t\t\t  struct btrfs_device *scrub_dev,\n\t\t\t\t\t  u64 chunk_offset, u64 length,\n\t\t\t\t\t  u64 dev_offset,\n\t\t\t\t\t  struct btrfs_block_group_cache *cache)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tint i;\n\tint ret = 0;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, chunk_offset, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\tif (!em) {\n\t\t/*\n\t\t * Might have been an unused block group deleted by the cleaner\n\t\t * kthread or relocation.\n\t\t */\n\t\tspin_lock(&cache->lock);\n\t\tif (!cache->removed)\n\t\t\tret = -EINVAL;\n\t\tspin_unlock(&cache->lock);\n\n\t\treturn ret;\n\t}\n\n\tmap = em->map_lookup;\n\tif (em->start != chunk_offset)\n\t\tgoto out;\n\n\tif (em->len < length)\n\t\tgoto out;\n\n\tfor (i = 0; i < map->num_stripes; ++i) {\n\t\tif (map->stripes[i].dev->bdev == scrub_dev->bdev &&\n\t\t    map->stripes[i].physical == dev_offset) {\n\t\t\tret = scrub_stripe(sctx, map, scrub_dev, i,\n\t\t\t\t\t   chunk_offset, length);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tfree_extent_map(em);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack\nint scrub_enumerate_chunks(struct scrub_ctx *sctx,\n\t\t\t   struct btrfs_device *scrub_dev, u64 start, u64 end)\n{\n\tstruct btrfs_dev_extent *dev_extent = NULL;\n\tstruct btrfs_path *path;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tu64 length;\n\tu64 chunk_offset;\n\tint ret = 0;\n\tint ro_set;\n\tint slot;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_block_group_cache *cache;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_FORWARD;\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tkey.objectid = scrub_dev->devid;\n\tkey.offset = 0ull;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tif (ret > 0) {\n\t\t\tif (path->slots[0] >=\n\t\t\t    btrfs_header_nritems(path->nodes[0])) {\n\t\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tbreak;\n\t\t\t\tif (ret > 0) {\n\t\t\t\t\tret = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tret = 0;\n\t\t\t}\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tbtrfs_item_key_to_cpu(l, &found_key, slot);\n\n\t\tif (found_key.objectid != scrub_dev->devid)\n\t\t\tbreak;\n\n\t\tif (found_key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tbreak;\n\n\t\tif (found_key.offset >= end)\n\t\t\tbreak;\n\n\t\tif (found_key.offset < key.offset)\n\t\t\tbreak;\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\tlength = btrfs_dev_extent_length(l, dev_extent);\n\n\t\tif (found_key.offset + length <= start)\n\t\t\tgoto skip;\n\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\n\n\t\t/*\n\t\t * get a reference on the corresponding block group to prevent\n\t\t * the chunk from going away while we scrub it\n\t\t */\n\t\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\n\t\t/* some chunks are removed but not committed to disk yet,\n\t\t * continue scrubbing */\n\t\tif (!cache)\n\t\t\tgoto skip;\n\n\t\t/*\n\t\t * we need call btrfs_inc_block_group_ro() with scrubs_paused,\n\t\t * to avoid deadlock caused by:\n\t\t * btrfs_inc_block_group_ro()\n\t\t * -> btrfs_wait_for_commit()\n\t\t * -> btrfs_commit_transaction()\n\t\t * -> btrfs_scrub_pause()\n\t\t */\n\t\tscrub_pause_on(fs_info);\n\t\tret = btrfs_inc_block_group_ro(cache);\n\t\tif (!ret && sctx->is_dev_replace) {\n\t\t\t/*\n\t\t\t * If we are doing a device replace wait for any tasks\n\t\t\t * that started delalloc right before we set the block\n\t\t\t * group to RO mode, as they might have just allocated\n\t\t\t * an extent from it or decided they could do a nocow\n\t\t\t * write. And if any such tasks did that, wait for their\n\t\t\t * ordered extents to complete and then commit the\n\t\t\t * current transaction, so that we can later see the new\n\t\t\t * extent items in the extent tree - the ordered extents\n\t\t\t * create delayed data references (for cow writes) when\n\t\t\t * they complete, which will be run and insert the\n\t\t\t * corresponding extent items into the extent tree when\n\t\t\t * we commit the transaction they used when running\n\t\t\t * inode.c:btrfs_finish_ordered_io(). We later use\n\t\t\t * the commit root of the extent tree to find extents\n\t\t\t * to copy from the srcdev into the tgtdev, and we don't\n\t\t\t * want to miss any new extents.\n\t\t\t */\n\t\t\tbtrfs_wait_block_group_reservations(cache);\n\t\t\tbtrfs_wait_nocow_writers(cache);\n\t\t\tret = btrfs_wait_ordered_roots(fs_info, U64_MAX,\n\t\t\t\t\t\t       cache->key.objectid,\n\t\t\t\t\t\t       cache->key.offset);\n\t\t\tif (ret > 0) {\n\t\t\t\tstruct btrfs_trans_handle *trans;\n\n\t\t\t\ttrans = btrfs_join_transaction(root);\n\t\t\t\tif (IS_ERR(trans))\n\t\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\telse\n\t\t\t\t\tret = btrfs_commit_transaction(trans);\n\t\t\t\tif (ret) {\n\t\t\t\t\tscrub_pause_off(fs_info);\n\t\t\t\t\tbtrfs_put_block_group(cache);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tscrub_pause_off(fs_info);\n\n\t\tif (ret == 0) {\n\t\t\tro_set = 1;\n\t\t} else if (ret == -ENOSPC) {\n\t\t\t/*\n\t\t\t * btrfs_inc_block_group_ro return -ENOSPC when it\n\t\t\t * failed in creating new chunk for metadata.\n\t\t\t * It is not a problem for scrub/replace, because\n\t\t\t * metadata are always cowed, and our scrub paused\n\t\t\t * commit_transactions.\n\t\t\t */\n\t\t\tro_set = 0;\n\t\t} else {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"failed setting block group ro: %d\", ret);\n\t\t\tbtrfs_put_block_group(cache);\n\t\t\tbreak;\n\t\t}\n\n\t\tdown_write(&fs_info->dev_replace.rwsem);\n\t\tdev_replace->cursor_right = found_key.offset + length;\n\t\tdev_replace->cursor_left = found_key.offset;\n\t\tdev_replace->item_needs_writeback = 1;\n\t\tup_write(&dev_replace->rwsem);\n\n\t\tret = scrub_chunk(sctx, scrub_dev, chunk_offset, length,\n\t\t\t\t  found_key.offset, cache);\n\n\t\t/*\n\t\t * flush, submit all pending read and write bios, afterwards\n\t\t * wait for them.\n\t\t * Note that in the dev replace case, a read request causes\n\t\t * write requests that are submitted in the read completion\n\t\t * worker. Therefore in the current situation, it is required\n\t\t * that all write requests are flushed, so that all read and\n\t\t * write requests are really completed when bios_in_flight\n\t\t * changes to 0.\n\t\t */\n\t\tsctx->flush_all_writes = true;\n\t\tscrub_submit(sctx);\n\t\tmutex_lock(&sctx->wr_lock);\n\t\tscrub_wr_submit(sctx);\n\t\tmutex_unlock(&sctx->wr_lock);\n\n\t\twait_event(sctx->list_wait,\n\t\t\t   atomic_read(&sctx->bios_in_flight) == 0);\n\n\t\tscrub_pause_on(fs_info);\n\n\t\t/*\n\t\t * must be called before we decrease @scrub_paused.\n\t\t * make sure we don't block transaction commit while\n\t\t * we are waiting pending workers finished.\n\t\t */\n\t\twait_event(sctx->list_wait,\n\t\t\t   atomic_read(&sctx->workers_pending) == 0);\n\t\tsctx->flush_all_writes = false;\n\n\t\tscrub_pause_off(fs_info);\n\n\t\tdown_write(&fs_info->dev_replace.rwsem);\n\t\tdev_replace->cursor_left = dev_replace->cursor_right;\n\t\tdev_replace->item_needs_writeback = 1;\n\t\tup_write(&fs_info->dev_replace.rwsem);\n\n\t\tif (ro_set)\n\t\t\tbtrfs_dec_block_group_ro(cache);\n\n\t\t/*\n\t\t * We might have prevented the cleaner kthread from deleting\n\t\t * this block group if it was already unused because we raced\n\t\t * and set it to RO mode first. So add it back to the unused\n\t\t * list, otherwise it might not ever be deleted unless a manual\n\t\t * balance is triggered or it becomes used and unused again.\n\t\t */\n\t\tspin_lock(&cache->lock);\n\t\tif (!cache->removed && !cache->ro && cache->reserved == 0 &&\n\t\t    btrfs_block_group_used(&cache->item) == 0) {\n\t\t\tspin_unlock(&cache->lock);\n\t\t\tbtrfs_mark_bg_unused(cache);\n\t\t} else {\n\t\t\tspin_unlock(&cache->lock);\n\t\t}\n\n\t\tbtrfs_put_block_group(cache);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (sctx->is_dev_replace &&\n\t\t    atomic64_read(&dev_replace->num_write_errors) > 0) {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (sctx->stat.malloc_errors > 0) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\nskip:\n\t\tkey.offset = found_key.offset + length;\n\t\tbtrfs_release_path(path);\n\t}\n\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int scrub_supers(struct scrub_ctx *sctx,\n\t\t\t\t\t   struct btrfs_device *scrub_dev)\n{\n\tint\ti;\n\tu64\tbytenr;\n\tu64\tgen;\n\tint\tret;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\n\tif (test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state))\n\t\treturn -EIO;\n\n\t/* Seed devices of a new filesystem has their own generation. */\n\tif (scrub_dev->fs_devices != fs_info->fs_devices)\n\t\tgen = scrub_dev->generation;\n\telse\n\t\tgen = fs_info->last_trans_committed;\n\n\tfor (i = 0; i < BTRFS_SUPER_MIRROR_MAX; i++) {\n\t\tbytenr = btrfs_sb_offset(i);\n\t\tif (bytenr + BTRFS_SUPER_INFO_SIZE >\n\t\t    scrub_dev->commit_total_bytes)\n\t\t\tbreak;\n\n\t\tret = scrub_pages(sctx, bytenr, BTRFS_SUPER_INFO_SIZE, bytenr,\n\t\t\t\t  scrub_dev, BTRFS_EXTENT_FLAG_SUPER, gen, i,\n\t\t\t\t  NULL, 1, bytenr);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\twait_event(sctx->list_wait, atomic_read(&sctx->bios_in_flight) == 0);\n\n\treturn 0;\n}\n\n/*\n * get a reference count on fs_info->scrub_workers. start worker if necessary\n */\nstatic noinline_for_stack int scrub_workers_get(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\tint is_dev_replace)\n{\n\tunsigned int flags = WQ_FREEZABLE | WQ_UNBOUND;\n\tint max_active = fs_info->thread_pool_size;\n\n\tif (fs_info->scrub_workers_refcnt == 0) {\n\t\tfs_info->scrub_workers = btrfs_alloc_workqueue(fs_info, \"scrub\",\n\t\t\t\tflags, is_dev_replace ? 1 : max_active, 4);\n\t\tif (!fs_info->scrub_workers)\n\t\t\tgoto fail_scrub_workers;\n\n\t\tfs_info->scrub_wr_completion_workers =\n\t\t\tbtrfs_alloc_workqueue(fs_info, \"scrubwrc\", flags,\n\t\t\t\t\t      max_active, 2);\n\t\tif (!fs_info->scrub_wr_completion_workers)\n\t\t\tgoto fail_scrub_wr_completion_workers;\n\n\t\tfs_info->scrub_parity_workers =\n\t\t\tbtrfs_alloc_workqueue(fs_info, \"scrubparity\", flags,\n\t\t\t\t\t      max_active, 2);\n\t\tif (!fs_info->scrub_parity_workers)\n\t\t\tgoto fail_scrub_parity_workers;\n\t}\n\t++fs_info->scrub_workers_refcnt;\n\treturn 0;\n\nfail_scrub_parity_workers:\n\tbtrfs_destroy_workqueue(fs_info->scrub_wr_completion_workers);\nfail_scrub_wr_completion_workers:\n\tbtrfs_destroy_workqueue(fs_info->scrub_workers);\nfail_scrub_workers:\n\treturn -ENOMEM;\n}\n\nstatic noinline_for_stack void scrub_workers_put(struct btrfs_fs_info *fs_info)\n{\n\tif (--fs_info->scrub_workers_refcnt == 0) {\n\t\tbtrfs_destroy_workqueue(fs_info->scrub_workers);\n\t\tbtrfs_destroy_workqueue(fs_info->scrub_wr_completion_workers);\n\t\tbtrfs_destroy_workqueue(fs_info->scrub_parity_workers);\n\t}\n\tWARN_ON(fs_info->scrub_workers_refcnt < 0);\n}\n\nint btrfs_scrub_dev(struct btrfs_fs_info *fs_info, u64 devid, u64 start,\n\t\t    u64 end, struct btrfs_scrub_progress *progress,\n\t\t    int readonly, int is_dev_replace)\n{\n\tstruct scrub_ctx *sctx;\n\tint ret;\n\tstruct btrfs_device *dev;\n\tunsigned int nofs_flag;\n\n\tif (btrfs_fs_closing(fs_info))\n\t\treturn -EINVAL;\n\n\tif (fs_info->nodesize > BTRFS_STRIPE_LEN) {\n\t\t/*\n\t\t * in this case scrub is unable to calculate the checksum\n\t\t * the way scrub is implemented. Do not handle this\n\t\t * situation at all because it won't ever happen.\n\t\t */\n\t\tbtrfs_err(fs_info,\n\t\t\t   \"scrub: size assumption nodesize <= BTRFS_STRIPE_LEN (%d <= %d) fails\",\n\t\t       fs_info->nodesize,\n\t\t       BTRFS_STRIPE_LEN);\n\t\treturn -EINVAL;\n\t}\n\n\tif (fs_info->sectorsize != PAGE_SIZE) {\n\t\t/* not supported for data w/o checksums */\n\t\tbtrfs_err_rl(fs_info,\n\t\t\t   \"scrub: size assumption sectorsize != PAGE_SIZE (%d != %lu) fails\",\n\t\t       fs_info->sectorsize, PAGE_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\tif (fs_info->nodesize >\n\t    PAGE_SIZE * SCRUB_MAX_PAGES_PER_BLOCK ||\n\t    fs_info->sectorsize > PAGE_SIZE * SCRUB_MAX_PAGES_PER_BLOCK) {\n\t\t/*\n\t\t * would exhaust the array bounds of pagev member in\n\t\t * struct scrub_block\n\t\t */\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"scrub: size assumption nodesize and sectorsize <= SCRUB_MAX_PAGES_PER_BLOCK (%d <= %d && %d <= %d) fails\",\n\t\t       fs_info->nodesize,\n\t\t       SCRUB_MAX_PAGES_PER_BLOCK,\n\t\t       fs_info->sectorsize,\n\t\t       SCRUB_MAX_PAGES_PER_BLOCK);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Allocate outside of device_list_mutex */\n\tsctx = scrub_setup_ctx(fs_info, is_dev_replace);\n\tif (IS_ERR(sctx))\n\t\treturn PTR_ERR(sctx);\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev || (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) &&\n\t\t     !is_dev_replace)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -ENODEV;\n\t\tgoto out_free_ctx;\n\t}\n\n\tif (!is_dev_replace && !readonly &&\n\t    !test_bit(BTRFS_DEV_STATE_WRITEABLE, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tbtrfs_err_in_rcu(fs_info, \"scrub: device %s is not writable\",\n\t\t\t\trcu_str_deref(dev->name));\n\t\tret = -EROFS;\n\t\tgoto out_free_ctx;\n\t}\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &dev->dev_state) ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EIO;\n\t\tgoto out_free_ctx;\n\t}\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (dev->scrub_ctx ||\n\t    (!is_dev_replace &&\n\t     btrfs_dev_replace_is_ongoing(&fs_info->dev_replace))) {\n\t\tup_read(&fs_info->dev_replace.rwsem);\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EINPROGRESS;\n\t\tgoto out_free_ctx;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\tret = scrub_workers_get(fs_info, is_dev_replace);\n\tif (ret) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tgoto out_free_ctx;\n\t}\n\n\tsctx->readonly = readonly;\n\tdev->scrub_ctx = sctx;\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t/*\n\t * checking @scrub_pause_req here, we can avoid\n\t * race between committing transaction and scrubbing.\n\t */\n\t__scrub_blocked_if_needed(fs_info);\n\tatomic_inc(&fs_info->scrubs_running);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\t/*\n\t * In order to avoid deadlock with reclaim when there is a transaction\n\t * trying to pause scrub, make sure we use GFP_NOFS for all the\n\t * allocations done at btrfs_scrub_pages() and scrub_pages_for_parity()\n\t * invoked by our callees. The pausing request is done when the\n\t * transaction commit starts, and it blocks the transaction until scrub\n\t * is paused (done at specific points at scrub_stripe() or right above\n\t * before incrementing fs_info->scrubs_running).\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\tif (!is_dev_replace) {\n\t\t/*\n\t\t * by holding device list mutex, we can\n\t\t * kick off writing super in log tree sync.\n\t\t */\n\t\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = scrub_supers(sctx, dev);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t}\n\n\tif (!ret)\n\t\tret = scrub_enumerate_chunks(sctx, dev, start, end);\n\tmemalloc_nofs_restore(nofs_flag);\n\n\twait_event(sctx->list_wait, atomic_read(&sctx->bios_in_flight) == 0);\n\tatomic_dec(&fs_info->scrubs_running);\n\twake_up(&fs_info->scrub_pause_wait);\n\n\twait_event(sctx->list_wait, atomic_read(&sctx->workers_pending) == 0);\n\n\tif (progress)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tdev->scrub_ctx = NULL;\n\tscrub_workers_put(fs_info);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\tscrub_put_ctx(sctx);\n\n\treturn ret;\n\nout_free_ctx:\n\tscrub_free_ctx(sctx);\n\n\treturn ret;\n}\n\nvoid btrfs_scrub_pause(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->scrub_lock);\n\tatomic_inc(&fs_info->scrub_pause_req);\n\twhile (atomic_read(&fs_info->scrubs_paused) !=\n\t       atomic_read(&fs_info->scrubs_running)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t\t   atomic_read(&fs_info->scrubs_paused) ==\n\t\t\t   atomic_read(&fs_info->scrubs_running));\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n\tmutex_unlock(&fs_info->scrub_lock);\n}\n\nvoid btrfs_scrub_continue(struct btrfs_fs_info *fs_info)\n{\n\tatomic_dec(&fs_info->scrub_pause_req);\n\twake_up(&fs_info->scrub_pause_wait);\n}\n\nint btrfs_scrub_cancel(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->scrub_lock);\n\tif (!atomic_read(&fs_info->scrubs_running)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\treturn -ENOTCONN;\n\t}\n\n\tatomic_inc(&fs_info->scrub_cancel_req);\n\twhile (atomic_read(&fs_info->scrubs_running)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t\t   atomic_read(&fs_info->scrubs_running) == 0);\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n\tatomic_dec(&fs_info->scrub_cancel_req);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\treturn 0;\n}\n\nint btrfs_scrub_cancel_dev(struct btrfs_fs_info *fs_info,\n\t\t\t   struct btrfs_device *dev)\n{\n\tstruct scrub_ctx *sctx;\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tsctx = dev->scrub_ctx;\n\tif (!sctx) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\treturn -ENOTCONN;\n\t}\n\tatomic_inc(&sctx->cancel_req);\n\twhile (dev->scrub_ctx) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t\t   dev->scrub_ctx == NULL);\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\treturn 0;\n}\n\nint btrfs_scrub_progress(struct btrfs_fs_info *fs_info, u64 devid,\n\t\t\t struct btrfs_scrub_progress *progress)\n{\n\tstruct btrfs_device *dev;\n\tstruct scrub_ctx *sctx = NULL;\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (dev)\n\t\tsctx = dev->scrub_ctx;\n\tif (sctx)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\treturn dev ? (sctx ? 0 : -ENOTCONN) : -ENODEV;\n}\n\nstatic void scrub_remap_extent(struct btrfs_fs_info *fs_info,\n\t\t\t       u64 extent_logical, u64 extent_len,\n\t\t\t       u64 *extent_physical,\n\t\t\t       struct btrfs_device **extent_dev,\n\t\t\t       int *extent_mirror_num)\n{\n\tu64 mapped_length;\n\tstruct btrfs_bio *bbio = NULL;\n\tint ret;\n\n\tmapped_length = extent_len;\n\tret = btrfs_map_block(fs_info, BTRFS_MAP_READ, extent_logical,\n\t\t\t      &mapped_length, &bbio, 0);\n\tif (ret || !bbio || mapped_length < extent_len ||\n\t    !bbio->stripes[0].dev->bdev) {\n\t\tbtrfs_put_bbio(bbio);\n\t\treturn;\n\t}\n\n\t*extent_physical = bbio->stripes[0].physical;\n\t*extent_mirror_num = bbio->mirror_num;\n\t*extent_dev = bbio->stripes[0].dev;\n\tbtrfs_put_bbio(bbio);\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n */\n\n#include <linux/sched.h>\n#include <linux/bio.h>\n#include <linux/slab.h>\n#include <linux/buffer_head.h>\n#include <linux/blkdev.h>\n#include <linux/ratelimit.h>\n#include <linux/kthread.h>\n#include <linux/raid/pq.h>\n#include <linux/semaphore.h>\n#include <linux/uuid.h>\n#include <linux/list_sort.h>\n#include \"ctree.h\"\n#include \"extent_map.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"raid56.h\"\n#include \"async-thread.h\"\n#include \"check-integrity.h\"\n#include \"rcu-string.h\"\n#include \"math.h\"\n#include \"dev-replace.h\"\n#include \"sysfs.h\"\n\nconst struct btrfs_raid_attr btrfs_raid_array[BTRFS_NR_RAID_TYPES] = {\n\t[BTRFS_RAID_RAID10] = {\n\t\t.sub_stripes\t= 2,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\t/* 0 == as many as possible */\n\t\t.devs_min\t= 4,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid10\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID10,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID10_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 2,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_DUP] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 2,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"dup\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_DUP,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID0] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid0\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_SINGLE] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"single\",\n\t\t.bg_flag\t= 0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID5] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 1,\n\t\t.raid_name\t= \"raid5\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID5,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID5_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID6] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 3,\n\t\t.tolerated_failures = 2,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 2,\n\t\t.raid_name\t= \"raid6\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID6,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID6_MIN_NOT_MET,\n\t},\n};\n\nconst char *get_raid_name(enum btrfs_raid_types type)\n{\n\tif (type >= BTRFS_NR_RAID_TYPES)\n\t\treturn NULL;\n\n\treturn btrfs_raid_array[type].raid_name;\n}\n\n/*\n * Fill @buf with textual description of @bg_flags, no more than @size_buf\n * bytes including terminating null byte.\n */\nvoid btrfs_describe_block_groups(u64 bg_flags, char *buf, u32 size_buf)\n{\n\tint i;\n\tint ret;\n\tchar *bp = buf;\n\tu64 flags = bg_flags;\n\tu32 size_bp = size_buf;\n\n\tif (!flags) {\n\t\tstrcpy(bp, \"NONE\");\n\t\treturn;\n\t}\n\n#define DESCRIBE_FLAG(flag, desc)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (flags & (flag)) {\t\t\t\t\t\\\n\t\t\tret = snprintf(bp, size_bp, \"%s|\", (desc));\t\\\n\t\t\tif (ret < 0 || ret >= size_bp)\t\t\t\\\n\t\t\t\tgoto out_overflow;\t\t\t\\\n\t\t\tsize_bp -= ret;\t\t\t\t\t\\\n\t\t\tbp += ret;\t\t\t\t\t\\\n\t\t\tflags &= ~(flag);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_DATA, \"data\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_SYSTEM, \"system\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_METADATA, \"metadata\");\n\n\tDESCRIBE_FLAG(BTRFS_AVAIL_ALLOC_BIT_SINGLE, \"single\");\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++)\n\t\tDESCRIBE_FLAG(btrfs_raid_array[i].bg_flag,\n\t\t\t      btrfs_raid_array[i].raid_name);\n#undef DESCRIBE_FLAG\n\n\tif (flags) {\n\t\tret = snprintf(bp, size_bp, \"0x%llx|\", flags);\n\t\tsize_bp -= ret;\n\t}\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last | */\n\n\t/*\n\t * The text is trimmed, it's up to the caller to provide sufficiently\n\t * large buffer\n\t */\nout_overflow:;\n}\n\nstatic int init_first_rw_device(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_fs_info *fs_info);\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info);\nstatic void __btrfs_reset_dev_stats(struct btrfs_device *dev);\nstatic void btrfs_dev_stat_print_on_error(struct btrfs_device *dev);\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *device);\nstatic int __btrfs_map_block(struct btrfs_fs_info *fs_info,\n\t\t\t     enum btrfs_map_op op,\n\t\t\t     u64 logical, u64 *length,\n\t\t\t     struct btrfs_bio **bbio_ret,\n\t\t\t     int mirror_num, int need_raid_map);\n\n/*\n * Device locking\n * ==============\n *\n * There are several mutexes that protect manipulation of devices and low-level\n * structures like chunks but not block groups, extents or files\n *\n * uuid_mutex (global lock)\n * ------------------------\n * protects the fs_uuids list that tracks all per-fs fs_devices, resulting from\n * the SCAN_DEV ioctl registration or from mount either implicitly (the first\n * device) or requested by the device= mount option\n *\n * the mutex can be very coarse and can cover long-running operations\n *\n * protects: updates to fs_devices counters like missing devices, rw devices,\n * seeding, structure cloning, opening/closing devices at mount/umount time\n *\n * global::fs_devs - add, remove, updates to the global list\n *\n * does not protect: manipulation of the fs_devices::devices list!\n *\n * btrfs_device::name - renames (write side), read is RCU\n *\n * fs_devices::device_list_mutex (per-fs, with RCU)\n * ------------------------------------------------\n * protects updates to fs_devices::devices, ie. adding and deleting\n *\n * simple list traversal with read-only actions can be done with RCU protection\n *\n * may be used to exclude some operations from running concurrently without any\n * modifications to the list (see write_all_supers)\n *\n * balance_mutex\n * -------------\n * protects balance structures (status, state) and context accessed from\n * several places (internally, ioctl)\n *\n * chunk_mutex\n * -----------\n * protects chunks, adding or removing during allocation, trim or when a new\n * device is added/removed\n *\n * cleaner_mutex\n * -------------\n * a big lock that is held by the cleaner thread and prevents running subvolume\n * cleaning together with relocation or delayed iputs\n *\n *\n * Lock nesting\n * ============\n *\n * uuid_mutex\n *   volume_mutex\n *     device_list_mutex\n *       chunk_mutex\n *     balance_mutex\n *\n *\n * Exclusive operations, BTRFS_FS_EXCL_OP\n * ======================================\n *\n * Maintains the exclusivity of the following operations that apply to the\n * whole filesystem and cannot run in parallel.\n *\n * - Balance (*)\n * - Device add\n * - Device remove\n * - Device replace (*)\n * - Resize\n *\n * The device operations (as above) can be in one of the following states:\n *\n * - Running state\n * - Paused state\n * - Completed state\n *\n * Only device operations marked with (*) can go into the Paused state for the\n * following reasons:\n *\n * - ioctl (only Balance can be Paused through ioctl)\n * - filesystem remounted as read-only\n * - filesystem unmounted and mounted as read-only\n * - system power-cycle and filesystem mounted as read-only\n * - filesystem or device errors leading to forced read-only\n *\n * BTRFS_FS_EXCL_OP flag is set and cleared using atomic operations.\n * During the course of Paused state, the BTRFS_FS_EXCL_OP remains set.\n * A device operation in Paused or Running state can be canceled or resumed\n * either by ioctl (Balance only) or when remounted as read-write.\n * BTRFS_FS_EXCL_OP flag is cleared when the device operation is canceled or\n * completed.\n */\n\nDEFINE_MUTEX(uuid_mutex);\nstatic LIST_HEAD(fs_uuids);\nstruct list_head *btrfs_get_fs_uuids(void)\n{\n\treturn &fs_uuids;\n}\n\n/*\n * alloc_fs_devices - allocate struct btrfs_fs_devices\n * @fsid:\t\tif not NULL, copy the UUID to fs_devices::fsid\n * @metadata_fsid:\tif not NULL, copy the UUID to fs_devices::metadata_fsid\n *\n * Return a pointer to a new struct btrfs_fs_devices on success, or ERR_PTR().\n * The returned struct is not linked onto any lists and can be destroyed with\n * kfree() right away.\n */\nstatic struct btrfs_fs_devices *alloc_fs_devices(const u8 *fsid,\n\t\t\t\t\t\t const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devs;\n\n\tfs_devs = kzalloc(sizeof(*fs_devs), GFP_KERNEL);\n\tif (!fs_devs)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_init(&fs_devs->device_list_mutex);\n\n\tINIT_LIST_HEAD(&fs_devs->devices);\n\tINIT_LIST_HEAD(&fs_devs->resized_devices);\n\tINIT_LIST_HEAD(&fs_devs->alloc_list);\n\tINIT_LIST_HEAD(&fs_devs->fs_list);\n\tif (fsid)\n\t\tmemcpy(fs_devs->fsid, fsid, BTRFS_FSID_SIZE);\n\n\tif (metadata_fsid)\n\t\tmemcpy(fs_devs->metadata_uuid, metadata_fsid, BTRFS_FSID_SIZE);\n\telse if (fsid)\n\t\tmemcpy(fs_devs->metadata_uuid, fsid, BTRFS_FSID_SIZE);\n\n\treturn fs_devs;\n}\n\nvoid btrfs_free_device(struct btrfs_device *device)\n{\n\trcu_string_free(device->name);\n\tbio_put(device->flush_bio);\n\tkfree(device);\n}\n\nstatic void free_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device;\n\tWARN_ON(fs_devices->opened);\n\twhile (!list_empty(&fs_devices->devices)) {\n\t\tdevice = list_entry(fs_devices->devices.next,\n\t\t\t\t    struct btrfs_device, dev_list);\n\t\tlist_del(&device->dev_list);\n\t\tbtrfs_free_device(device);\n\t}\n\tkfree(fs_devices);\n}\n\nstatic void btrfs_kobject_uevent(struct block_device *bdev,\n\t\t\t\t enum kobject_action action)\n{\n\tint ret;\n\n\tret = kobject_uevent(&disk_to_dev(bdev->bd_disk)->kobj, action);\n\tif (ret)\n\t\tpr_warn(\"BTRFS: Sending event '%d' to kobject: '%s' (%p): failed\\n\",\n\t\t\taction,\n\t\t\tkobject_name(&disk_to_dev(bdev->bd_disk)->kobj),\n\t\t\t&disk_to_dev(bdev->bd_disk)->kobj);\n}\n\nvoid __exit btrfs_cleanup_fs_uuids(void)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\twhile (!list_empty(&fs_uuids)) {\n\t\tfs_devices = list_entry(fs_uuids.next,\n\t\t\t\t\tstruct btrfs_fs_devices, fs_list);\n\t\tlist_del(&fs_devices->fs_list);\n\t\tfree_fs_devices(fs_devices);\n\t}\n}\n\n/*\n * Returns a pointer to a new btrfs_device on success; ERR_PTR() on error.\n * Returned struct is not linked onto any lists and must be destroyed using\n * btrfs_free_device.\n */\nstatic struct btrfs_device *__alloc_device(void)\n{\n\tstruct btrfs_device *dev;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * Preallocate a bio that's always going to be used for flushing device\n\t * barriers and matches the device lifespan\n\t */\n\tdev->flush_bio = bio_alloc_bioset(GFP_KERNEL, 0, NULL);\n\tif (!dev->flush_bio) {\n\t\tkfree(dev);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tINIT_LIST_HEAD(&dev->dev_list);\n\tINIT_LIST_HEAD(&dev->dev_alloc_list);\n\tINIT_LIST_HEAD(&dev->resized_list);\n\n\tspin_lock_init(&dev->io_lock);\n\n\tatomic_set(&dev->reada_in_flight, 0);\n\tatomic_set(&dev->dev_stats_ccnt, 0);\n\tbtrfs_device_data_ordered_init(dev);\n\tINIT_RADIX_TREE(&dev->reada_zones, GFP_NOFS & ~__GFP_DIRECT_RECLAIM);\n\tINIT_RADIX_TREE(&dev->reada_extents, GFP_NOFS & ~__GFP_DIRECT_RECLAIM);\n\n\treturn dev;\n}\n\n/*\n * Find a device specified by @devid or @uuid in the list of @fs_devices, or\n * return NULL.\n *\n * If devid and uuid are both specified, the match must be exact, otherwise\n * only devid is used.\n */\nstatic struct btrfs_device *find_device(struct btrfs_fs_devices *fs_devices,\n\t\tu64 devid, const u8 *uuid)\n{\n\tstruct btrfs_device *dev;\n\n\tlist_for_each_entry(dev, &fs_devices->devices, dev_list) {\n\t\tif (dev->devid == devid &&\n\t\t    (!uuid || !memcmp(dev->uuid, uuid, BTRFS_UUID_SIZE))) {\n\t\t\treturn dev;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic noinline struct btrfs_fs_devices *find_fsid(\n\t\tconst u8 *fsid, const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tASSERT(fsid);\n\n\tif (metadata_fsid) {\n\t\t/*\n\t\t * Handle scanned device having completed its fsid change but\n\t\t * belonging to a fs_devices that was created by first scanning\n\t\t * a device which didn't have its fsid/metadata_uuid changed\n\t\t * at all and the CHANGING_FSID_V2 flag set.\n\t\t */\n\t\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\t\tif (fs_devices->fsid_change &&\n\t\t\t    memcmp(metadata_fsid, fs_devices->fsid,\n\t\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t\t    memcmp(fs_devices->fsid, fs_devices->metadata_uuid,\n\t\t\t\t   BTRFS_FSID_SIZE) == 0) {\n\t\t\t\treturn fs_devices;\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Handle scanned device having completed its fsid change but\n\t\t * belonging to a fs_devices that was created by a device that\n\t\t * has an outdated pair of fsid/metadata_uuid and\n\t\t * CHANGING_FSID_V2 flag set.\n\t\t */\n\t\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\t\tif (fs_devices->fsid_change &&\n\t\t\t    memcmp(fs_devices->metadata_uuid,\n\t\t\t\t   fs_devices->fsid, BTRFS_FSID_SIZE) != 0 &&\n\t\t\t    memcmp(metadata_fsid, fs_devices->metadata_uuid,\n\t\t\t\t   BTRFS_FSID_SIZE) == 0) {\n\t\t\t\treturn fs_devices;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Handle non-split brain cases */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (metadata_fsid) {\n\t\t\tif (memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE) == 0\n\t\t\t    && memcmp(metadata_fsid, fs_devices->metadata_uuid,\n\t\t\t\t      BTRFS_FSID_SIZE) == 0)\n\t\t\t\treturn fs_devices;\n\t\t} else {\n\t\t\tif (memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE) == 0)\n\t\t\t\treturn fs_devices;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic int\nbtrfs_get_bdev_and_sb(const char *device_path, fmode_t flags, void *holder,\n\t\t      int flush, struct block_device **bdev,\n\t\t      struct buffer_head **bh)\n{\n\tint ret;\n\n\t*bdev = blkdev_get_by_path(device_path, flags, holder);\n\n\tif (IS_ERR(*bdev)) {\n\t\tret = PTR_ERR(*bdev);\n\t\tgoto error;\n\t}\n\n\tif (flush)\n\t\tfilemap_write_and_wait((*bdev)->bd_inode->i_mapping);\n\tret = set_blocksize(*bdev, BTRFS_BDEV_BLOCKSIZE);\n\tif (ret) {\n\t\tblkdev_put(*bdev, flags);\n\t\tgoto error;\n\t}\n\tinvalidate_bdev(*bdev);\n\t*bh = btrfs_read_dev_super(*bdev);\n\tif (IS_ERR(*bh)) {\n\t\tret = PTR_ERR(*bh);\n\t\tblkdev_put(*bdev, flags);\n\t\tgoto error;\n\t}\n\n\treturn 0;\n\nerror:\n\t*bdev = NULL;\n\t*bh = NULL;\n\treturn ret;\n}\n\nstatic void requeue_list(struct btrfs_pending_bios *pending_bios,\n\t\t\tstruct bio *head, struct bio *tail)\n{\n\n\tstruct bio *old_head;\n\n\told_head = pending_bios->head;\n\tpending_bios->head = head;\n\tif (pending_bios->tail)\n\t\ttail->bi_next = old_head;\n\telse\n\t\tpending_bios->tail = tail;\n}\n\n/*\n * we try to collect pending bios for a device so we don't get a large\n * number of procs sending bios down to the same device.  This greatly\n * improves the schedulers ability to collect and merge the bios.\n *\n * But, it also turns into a long list of bios to process and that is sure\n * to eventually make the worker thread block.  The solution here is to\n * make some progress and then put this work struct back at the end of\n * the list if the block device is congested.  This way, multiple devices\n * can make progress from a single worker thread.\n */\nstatic noinline void run_scheduled_bios(struct btrfs_device *device)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct bio *pending;\n\tstruct backing_dev_info *bdi;\n\tstruct btrfs_pending_bios *pending_bios;\n\tstruct bio *tail;\n\tstruct bio *cur;\n\tint again = 0;\n\tunsigned long num_run;\n\tunsigned long batch_run = 0;\n\tunsigned long last_waited = 0;\n\tint force_reg = 0;\n\tint sync_pending = 0;\n\tstruct blk_plug plug;\n\n\t/*\n\t * this function runs all the bios we've collected for\n\t * a particular device.  We don't want to wander off to\n\t * another device without first sending all of these down.\n\t * So, setup a plug here and finish it off before we return\n\t */\n\tblk_start_plug(&plug);\n\n\tbdi = device->bdev->bd_bdi;\n\nloop:\n\tspin_lock(&device->io_lock);\n\nloop_lock:\n\tnum_run = 0;\n\n\t/* take all the bios off the list at once and process them\n\t * later on (without the lock held).  But, remember the\n\t * tail and other pointers so the bios can be properly reinserted\n\t * into the list if we hit congestion\n\t */\n\tif (!force_reg && device->pending_sync_bios.head) {\n\t\tpending_bios = &device->pending_sync_bios;\n\t\tforce_reg = 1;\n\t} else {\n\t\tpending_bios = &device->pending_bios;\n\t\tforce_reg = 0;\n\t}\n\n\tpending = pending_bios->head;\n\ttail = pending_bios->tail;\n\tWARN_ON(pending && !tail);\n\n\t/*\n\t * if pending was null this time around, no bios need processing\n\t * at all and we can stop.  Otherwise it'll loop back up again\n\t * and do an additional check so no bios are missed.\n\t *\n\t * device->running_pending is used to synchronize with the\n\t * schedule_bio code.\n\t */\n\tif (device->pending_sync_bios.head == NULL &&\n\t    device->pending_bios.head == NULL) {\n\t\tagain = 0;\n\t\tdevice->running_pending = 0;\n\t} else {\n\t\tagain = 1;\n\t\tdevice->running_pending = 1;\n\t}\n\n\tpending_bios->head = NULL;\n\tpending_bios->tail = NULL;\n\n\tspin_unlock(&device->io_lock);\n\n\twhile (pending) {\n\n\t\trmb();\n\t\t/* we want to work on both lists, but do more bios on the\n\t\t * sync list than the regular list\n\t\t */\n\t\tif ((num_run > 32 &&\n\t\t    pending_bios != &device->pending_sync_bios &&\n\t\t    device->pending_sync_bios.head) ||\n\t\t   (num_run > 64 && pending_bios == &device->pending_sync_bios &&\n\t\t    device->pending_bios.head)) {\n\t\t\tspin_lock(&device->io_lock);\n\t\t\trequeue_list(pending_bios, pending, tail);\n\t\t\tgoto loop_lock;\n\t\t}\n\n\t\tcur = pending;\n\t\tpending = pending->bi_next;\n\t\tcur->bi_next = NULL;\n\n\t\tBUG_ON(atomic_read(&cur->__bi_cnt) == 0);\n\n\t\t/*\n\t\t * if we're doing the sync list, record that our\n\t\t * plug has some sync requests on it\n\t\t *\n\t\t * If we're doing the regular list and there are\n\t\t * sync requests sitting around, unplug before\n\t\t * we add more\n\t\t */\n\t\tif (pending_bios == &device->pending_sync_bios) {\n\t\t\tsync_pending = 1;\n\t\t} else if (sync_pending) {\n\t\t\tblk_finish_plug(&plug);\n\t\t\tblk_start_plug(&plug);\n\t\t\tsync_pending = 0;\n\t\t}\n\n\t\tbtrfsic_submit_bio(cur);\n\t\tnum_run++;\n\t\tbatch_run++;\n\n\t\tcond_resched();\n\n\t\t/*\n\t\t * we made progress, there is more work to do and the bdi\n\t\t * is now congested.  Back off and let other work structs\n\t\t * run instead\n\t\t */\n\t\tif (pending && bdi_write_congested(bdi) && batch_run > 8 &&\n\t\t    fs_info->fs_devices->open_devices > 1) {\n\t\t\tstruct io_context *ioc;\n\n\t\t\tioc = current->io_context;\n\n\t\t\t/*\n\t\t\t * the main goal here is that we don't want to\n\t\t\t * block if we're going to be able to submit\n\t\t\t * more requests without blocking.\n\t\t\t *\n\t\t\t * This code does two great things, it pokes into\n\t\t\t * the elevator code from a filesystem _and_\n\t\t\t * it makes assumptions about how batching works.\n\t\t\t */\n\t\t\tif (ioc && ioc->nr_batch_requests > 0 &&\n\t\t\t    time_before(jiffies, ioc->last_waited + HZ/50UL) &&\n\t\t\t    (last_waited == 0 ||\n\t\t\t     ioc->last_waited == last_waited)) {\n\t\t\t\t/*\n\t\t\t\t * we want to go through our batch of\n\t\t\t\t * requests and stop.  So, we copy out\n\t\t\t\t * the ioc->last_waited time and test\n\t\t\t\t * against it before looping\n\t\t\t\t */\n\t\t\t\tlast_waited = ioc->last_waited;\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tspin_lock(&device->io_lock);\n\t\t\trequeue_list(pending_bios, pending, tail);\n\t\t\tdevice->running_pending = 1;\n\n\t\t\tspin_unlock(&device->io_lock);\n\t\t\tbtrfs_queue_work(fs_info->submit_workers,\n\t\t\t\t\t &device->work);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tcond_resched();\n\tif (again)\n\t\tgoto loop;\n\n\tspin_lock(&device->io_lock);\n\tif (device->pending_bios.head || device->pending_sync_bios.head)\n\t\tgoto loop_lock;\n\tspin_unlock(&device->io_lock);\n\ndone:\n\tblk_finish_plug(&plug);\n}\n\nstatic void pending_bios_fn(struct btrfs_work *work)\n{\n\tstruct btrfs_device *device;\n\n\tdevice = container_of(work, struct btrfs_device, work);\n\trun_scheduled_bios(device);\n}\n\nstatic bool device_path_matched(const char *path, struct btrfs_device *device)\n{\n\tint found;\n\n\trcu_read_lock();\n\tfound = strcmp(rcu_str_deref(device->name), path);\n\trcu_read_unlock();\n\n\treturn found == 0;\n}\n\n/*\n *  Search and remove all stale (devices which are not mounted) devices.\n *  When both inputs are NULL, it will search and release all stale devices.\n *  path:\tOptional. When provided will it release all unmounted devices\n *\t\tmatching this path only.\n *  skip_dev:\tOptional. Will skip this device when searching for the stale\n *\t\tdevices.\n *  Return:\t0 for success or if @path is NULL.\n * \t\t-EBUSY if @path is a mounted device.\n * \t\t-ENOENT if @path does not match any device in the list.\n */\nstatic int btrfs_free_stale_devices(const char *path,\n\t\t\t\t     struct btrfs_device *skip_device)\n{\n\tstruct btrfs_fs_devices *fs_devices, *tmp_fs_devices;\n\tstruct btrfs_device *device, *tmp_device;\n\tint ret = 0;\n\n\tif (path)\n\t\tret = -ENOENT;\n\n\tlist_for_each_entry_safe(fs_devices, tmp_fs_devices, &fs_uuids, fs_list) {\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_for_each_entry_safe(device, tmp_device,\n\t\t\t\t\t &fs_devices->devices, dev_list) {\n\t\t\tif (skip_device && skip_device == device)\n\t\t\t\tcontinue;\n\t\t\tif (path && !device->name)\n\t\t\t\tcontinue;\n\t\t\tif (path && !device_path_matched(path, device))\n\t\t\t\tcontinue;\n\t\t\tif (fs_devices->opened) {\n\t\t\t\t/* for an already deleted device return 0 */\n\t\t\t\tif (path && ret != 0)\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* delete the stale device */\n\t\t\tfs_devices->num_devices--;\n\t\t\tlist_del(&device->dev_list);\n\t\t\tbtrfs_free_device(device);\n\n\t\t\tret = 0;\n\t\t\tif (fs_devices->num_devices == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t\tif (fs_devices->num_devices == 0) {\n\t\t\tbtrfs_sysfs_remove_fsid(fs_devices);\n\t\t\tlist_del(&fs_devices->fs_list);\n\t\t\tfree_fs_devices(fs_devices);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int btrfs_open_one_device(struct btrfs_fs_devices *fs_devices,\n\t\t\tstruct btrfs_device *device, fmode_t flags,\n\t\t\tvoid *holder)\n{\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tstruct buffer_head *bh;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tint ret;\n\n\tif (device->bdev)\n\t\treturn -EINVAL;\n\tif (!device->name)\n\t\treturn -EINVAL;\n\n\tret = btrfs_get_bdev_and_sb(device->name->str, flags, holder, 1,\n\t\t\t\t    &bdev, &bh);\n\tif (ret)\n\t\treturn ret;\n\n\tdisk_super = (struct btrfs_super_block *)bh->b_data;\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tif (devid != device->devid)\n\t\tgoto error_brelse;\n\n\tif (memcmp(device->uuid, disk_super->dev_item.uuid, BTRFS_UUID_SIZE))\n\t\tgoto error_brelse;\n\n\tdevice->generation = btrfs_super_generation(disk_super);\n\n\tif (btrfs_super_flags(disk_super) & BTRFS_SUPER_FLAG_SEEDING) {\n\t\tif (btrfs_super_incompat_flags(disk_super) &\n\t\t    BTRFS_FEATURE_INCOMPAT_METADATA_UUID) {\n\t\t\tpr_err(\n\t\t\"BTRFS: Invalid seeding and uuid-changed device detected\\n\");\n\t\t\tgoto error_brelse;\n\t\t}\n\n\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\tfs_devices->seeding = 1;\n\t} else {\n\t\tif (bdev_read_only(bdev))\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\telse\n\t\t\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t}\n\n\tq = bdev_get_queue(bdev);\n\tif (!blk_queue_nonrot(q))\n\t\tfs_devices->rotating = 1;\n\n\tdevice->bdev = bdev;\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tdevice->mode = flags;\n\n\tfs_devices->open_devices++;\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tfs_devices->rw_devices++;\n\t\tlist_add_tail(&device->dev_alloc_list, &fs_devices->alloc_list);\n\t}\n\tbrelse(bh);\n\n\treturn 0;\n\nerror_brelse:\n\tbrelse(bh);\n\tblkdev_put(bdev, flags);\n\n\treturn -EINVAL;\n}\n\n/*\n * Handle scanned device having its CHANGING_FSID_V2 flag set and the fs_devices\n * being created with a disk that has already completed its fsid change.\n */\nstatic struct btrfs_fs_devices *find_fsid_inprogress(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 && !fs_devices->fsid_change) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n\nstatic struct btrfs_fs_devices *find_fsid_changed(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * Handles the case where scanned device is part of an fs that had\n\t * multiple successful changes of FSID but curently device didn't\n\t * observe it. Meaning our fsid will be different than theirs.\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    memcmp(fs_devices->fsid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n/*\n * Add new device to list of registered devices\n *\n * Returns:\n * device pointer which was just added or updated when successful\n * error pointer when failed\n */\nstatic noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}\n\nstatic struct btrfs_fs_devices *clone_fs_devices(struct btrfs_fs_devices *orig)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *orig_dev;\n\n\tfs_devices = alloc_fs_devices(orig->fsid, NULL);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tmutex_lock(&orig->device_list_mutex);\n\tfs_devices->total_devices = orig->total_devices;\n\n\t/* We have held the volume lock, it is safe to get the devices. */\n\tlist_for_each_entry(orig_dev, &orig->devices, dev_list) {\n\t\tstruct rcu_string *name;\n\n\t\tdevice = btrfs_alloc_device(NULL, &orig_dev->devid,\n\t\t\t\t\t    orig_dev->uuid);\n\t\tif (IS_ERR(device))\n\t\t\tgoto error;\n\n\t\t/*\n\t\t * This is ok to do without rcu read locked because we hold the\n\t\t * uuid mutex so nothing we touch in here is going to disappear.\n\t\t */\n\t\tif (orig_dev->name) {\n\t\t\tname = rcu_string_strdup(orig_dev->name->str,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!name) {\n\t\t\t\tbtrfs_free_device(device);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\trcu_assign_pointer(device->name, name);\n\t\t}\n\n\t\tlist_add(&device->dev_list, &fs_devices->devices);\n\t\tdevice->fs_devices = fs_devices;\n\t\tfs_devices->num_devices++;\n\t}\n\tmutex_unlock(&orig->device_list_mutex);\n\treturn fs_devices;\nerror:\n\tmutex_unlock(&orig->device_list_mutex);\n\tfree_fs_devices(fs_devices);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/*\n * After we have read the system tree and know devids belonging to\n * this filesystem, remove the device which does not belong there.\n */\nvoid btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices, int step)\n{\n\tstruct btrfs_device *device, *next;\n\tstruct btrfs_device *latest_dev = NULL;\n\n\tmutex_lock(&uuid_mutex);\nagain:\n\t/* This is the initialized path, it is safe to release the devices. */\n\tlist_for_each_entry_safe(device, next, &fs_devices->devices, dev_list) {\n\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t\t\t&device->dev_state)) {\n\t\t\tif (!test_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t     &device->dev_state) &&\n\t\t\t     (!latest_dev ||\n\t\t\t      device->generation > latest_dev->generation)) {\n\t\t\t\tlatest_dev = device;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (device->devid == BTRFS_DEV_REPLACE_DEVID) {\n\t\t\t/*\n\t\t\t * In the first step, keep the device which has\n\t\t\t * the correct fsid and the devid that is used\n\t\t\t * for the dev_replace procedure.\n\t\t\t * In the second step, the dev_replace state is\n\t\t\t * read from the device tree and it is known\n\t\t\t * whether the procedure is really active or\n\t\t\t * not, which means whether this device is\n\t\t\t * used or whether it should be removed.\n\t\t\t */\n\t\t\tif (step == 0 || test_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t\t\t  &device->dev_state)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (device->bdev) {\n\t\t\tblkdev_put(device->bdev, device->mode);\n\t\t\tdevice->bdev = NULL;\n\t\t\tfs_devices->open_devices--;\n\t\t}\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tlist_del_init(&device->dev_alloc_list);\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\t\tif (!test_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t      &device->dev_state))\n\t\t\t\tfs_devices->rw_devices--;\n\t\t}\n\t\tlist_del_init(&device->dev_list);\n\t\tfs_devices->num_devices--;\n\t\tbtrfs_free_device(device);\n\t}\n\n\tif (fs_devices->seed) {\n\t\tfs_devices = fs_devices->seed;\n\t\tgoto again;\n\t}\n\n\tfs_devices->latest_bdev = latest_dev->bdev;\n\n\tmutex_unlock(&uuid_mutex);\n}\n\nstatic void free_device_rcu(struct rcu_head *head)\n{\n\tstruct btrfs_device *device;\n\n\tdevice = container_of(head, struct btrfs_device, rcu);\n\tbtrfs_free_device(device);\n}\n\nstatic void btrfs_close_bdev(struct btrfs_device *device)\n{\n\tif (!device->bdev)\n\t\treturn;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tsync_blockdev(device->bdev);\n\t\tinvalidate_bdev(device->bdev);\n\t}\n\n\tblkdev_put(device->bdev, device->mode);\n}\n\nstatic void btrfs_close_one_device(struct btrfs_device *device)\n{\n\tstruct btrfs_fs_devices *fs_devices = device->fs_devices;\n\tstruct btrfs_device *new_device;\n\tstruct rcu_string *name;\n\n\tif (device->bdev)\n\t\tfs_devices->open_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tfs_devices->rw_devices--;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tfs_devices->missing_devices--;\n\n\tbtrfs_close_bdev(device);\n\n\tnew_device = btrfs_alloc_device(NULL, &device->devid,\n\t\t\t\t\tdevice->uuid);\n\tBUG_ON(IS_ERR(new_device)); /* -ENOMEM */\n\n\t/* Safe because we are under uuid_mutex */\n\tif (device->name) {\n\t\tname = rcu_string_strdup(device->name->str, GFP_NOFS);\n\t\tBUG_ON(!name); /* -ENOMEM */\n\t\trcu_assign_pointer(new_device->name, name);\n\t}\n\n\tlist_replace_rcu(&device->dev_list, &new_device->dev_list);\n\tnew_device->fs_devices = device->fs_devices;\n\n\tcall_rcu(&device->rcu, free_device_rcu);\n}\n\nstatic int close_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device, *tmp;\n\n\tif (--fs_devices->opened > 0)\n\t\treturn 0;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry_safe(device, tmp, &fs_devices->devices, dev_list) {\n\t\tbtrfs_close_one_device(device);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tWARN_ON(fs_devices->open_devices);\n\tWARN_ON(fs_devices->rw_devices);\n\tfs_devices->opened = 0;\n\tfs_devices->seeding = 0;\n\n\treturn 0;\n}\n\nint btrfs_close_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_fs_devices *seed_devices = NULL;\n\tint ret;\n\n\tmutex_lock(&uuid_mutex);\n\tret = close_fs_devices(fs_devices);\n\tif (!fs_devices->opened) {\n\t\tseed_devices = fs_devices->seed;\n\t\tfs_devices->seed = NULL;\n\t}\n\tmutex_unlock(&uuid_mutex);\n\n\twhile (seed_devices) {\n\t\tfs_devices = seed_devices;\n\t\tseed_devices = fs_devices->seed;\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t}\n\treturn ret;\n}\n\nstatic int open_fs_devices(struct btrfs_fs_devices *fs_devices,\n\t\t\t\tfmode_t flags, void *holder)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *latest_dev = NULL;\n\tint ret = 0;\n\n\tflags |= FMODE_EXCL;\n\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\t/* Just open everything we can; ignore failures here */\n\t\tif (btrfs_open_one_device(fs_devices, device, flags, holder))\n\t\t\tcontinue;\n\n\t\tif (!latest_dev ||\n\t\t    device->generation > latest_dev->generation)\n\t\t\tlatest_dev = device;\n\t}\n\tif (fs_devices->open_devices == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tfs_devices->opened = 1;\n\tfs_devices->latest_bdev = latest_dev->bdev;\n\tfs_devices->total_rw_bytes = 0;\nout:\n\treturn ret;\n}\n\nstatic int devid_cmp(void *priv, struct list_head *a, struct list_head *b)\n{\n\tstruct btrfs_device *dev1, *dev2;\n\n\tdev1 = list_entry(a, struct btrfs_device, dev_list);\n\tdev2 = list_entry(b, struct btrfs_device, dev_list);\n\n\tif (dev1->devid < dev2->devid)\n\t\treturn -1;\n\telse if (dev1->devid > dev2->devid)\n\t\treturn 1;\n\treturn 0;\n}\n\nint btrfs_open_devices(struct btrfs_fs_devices *fs_devices,\n\t\t       fmode_t flags, void *holder)\n{\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tif (fs_devices->opened) {\n\t\tfs_devices->opened++;\n\t\tret = 0;\n\t} else {\n\t\tlist_sort(NULL, &fs_devices->devices, devid_cmp);\n\t\tret = open_fs_devices(fs_devices, flags, holder);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\treturn ret;\n}\n\nstatic void btrfs_release_disk_super(struct page *page)\n{\n\tkunmap(page);\n\tput_page(page);\n}\n\nstatic int btrfs_read_disk_super(struct block_device *bdev, u64 bytenr,\n\t\t\t\t struct page **page,\n\t\t\t\t struct btrfs_super_block **disk_super)\n{\n\tvoid *p;\n\tpgoff_t index;\n\n\t/* make sure our super fits in the device */\n\tif (bytenr + PAGE_SIZE >= i_size_read(bdev->bd_inode))\n\t\treturn 1;\n\n\t/* make sure our super fits in the page */\n\tif (sizeof(**disk_super) > PAGE_SIZE)\n\t\treturn 1;\n\n\t/* make sure our super doesn't straddle pages on disk */\n\tindex = bytenr >> PAGE_SHIFT;\n\tif ((bytenr + sizeof(**disk_super) - 1) >> PAGE_SHIFT != index)\n\t\treturn 1;\n\n\t/* pull in the page with our super */\n\t*page = read_cache_page_gfp(bdev->bd_inode->i_mapping,\n\t\t\t\t   index, GFP_KERNEL);\n\n\tif (IS_ERR_OR_NULL(*page))\n\t\treturn 1;\n\n\tp = kmap(*page);\n\n\t/* align our pointer to the offset of the super block */\n\t*disk_super = p + offset_in_page(bytenr);\n\n\tif (btrfs_super_bytenr(*disk_super) != bytenr ||\n\t    btrfs_super_magic(*disk_super) != BTRFS_MAGIC) {\n\t\tbtrfs_release_disk_super(*page);\n\t\treturn 1;\n\t}\n\n\tif ((*disk_super)->label[0] &&\n\t\t(*disk_super)->label[BTRFS_LABEL_SIZE - 1])\n\t\t(*disk_super)->label[BTRFS_LABEL_SIZE - 1] = '\\0';\n\n\treturn 0;\n}\n\n/*\n * Look for a btrfs signature on a device. This may be called out of the mount path\n * and we are not allowed to call set_blocksize during the scan. The superblock\n * is read via pagecache\n */\nstruct btrfs_device *btrfs_scan_one_device(const char *path, fmode_t flags,\n\t\t\t\t\t   void *holder)\n{\n\tstruct btrfs_super_block *disk_super;\n\tbool new_device_added = false;\n\tstruct btrfs_device *device = NULL;\n\tstruct block_device *bdev;\n\tstruct page *page;\n\tu64 bytenr;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\t/*\n\t * we would like to check all the supers, but that would make\n\t * a btrfs mount succeed after a mkfs from a different FS.\n\t * So, we need to add a special mount option to scan for\n\t * later supers, using BTRFS_SUPER_MIRROR_MAX instead\n\t */\n\tbytenr = btrfs_sb_offset(0);\n\tflags |= FMODE_EXCL;\n\n\tbdev = blkdev_get_by_path(path, flags, holder);\n\tif (IS_ERR(bdev))\n\t\treturn ERR_CAST(bdev);\n\n\tif (btrfs_read_disk_super(bdev, bytenr, &page, &disk_super)) {\n\t\tdevice = ERR_PTR(-EINVAL);\n\t\tgoto error_bdev_put;\n\t}\n\n\tdevice = device_list_add(path, disk_super, &new_device_added);\n\tif (!IS_ERR(device)) {\n\t\tif (new_device_added)\n\t\t\tbtrfs_free_stale_devices(path, device);\n\t}\n\n\tbtrfs_release_disk_super(page);\n\nerror_bdev_put:\n\tblkdev_put(bdev, flags);\n\n\treturn device;\n}\n\nstatic int contains_pending_extent(struct btrfs_transaction *transaction,\n\t\t\t\t   struct btrfs_device *device,\n\t\t\t\t   u64 *start, u64 len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct extent_map *em;\n\tstruct list_head *search_list = &fs_info->pinned_chunks;\n\tint ret = 0;\n\tu64 physical_start = *start;\n\n\tif (transaction)\n\t\tsearch_list = &transaction->pending_chunks;\nagain:\n\tlist_for_each_entry(em, search_list, list) {\n\t\tstruct map_lookup *map;\n\t\tint i;\n\n\t\tmap = em->map_lookup;\n\t\tfor (i = 0; i < map->num_stripes; i++) {\n\t\t\tu64 end;\n\n\t\t\tif (map->stripes[i].dev != device)\n\t\t\t\tcontinue;\n\t\t\tif (map->stripes[i].physical >= physical_start + len ||\n\t\t\t    map->stripes[i].physical + em->orig_block_len <=\n\t\t\t    physical_start)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Make sure that while processing the pinned list we do\n\t\t\t * not override our *start with a lower value, because\n\t\t\t * we can have pinned chunks that fall within this\n\t\t\t * device hole and that have lower physical addresses\n\t\t\t * than the pending chunks we processed before. If we\n\t\t\t * do not take this special care we can end up getting\n\t\t\t * 2 pending chunks that start at the same physical\n\t\t\t * device offsets because the end offset of a pinned\n\t\t\t * chunk can be equal to the start offset of some\n\t\t\t * pending chunk.\n\t\t\t */\n\t\t\tend = map->stripes[i].physical + em->orig_block_len;\n\t\t\tif (end > *start) {\n\t\t\t\t*start = end;\n\t\t\t\tret = 1;\n\t\t\t}\n\t\t}\n\t}\n\tif (search_list != &fs_info->pinned_chunks) {\n\t\tsearch_list = &fs_info->pinned_chunks;\n\t\tgoto again;\n\t}\n\n\treturn ret;\n}\n\n\n/*\n * find_free_dev_extent_start - find free space in the specified device\n * @device:\t  the device which we search the free space in\n * @num_bytes:\t  the size of the free space that we need\n * @search_start: the position from which to begin the search\n * @start:\t  store the start of the free space.\n * @len:\t  the size of the free space. that we find, or the size\n *\t\t  of the max free space if we don't find suitable free space\n *\n * this uses a pretty simple search, the expectation is that it is\n * called very infrequently and that a given device has a small number\n * of extents\n *\n * @start is used to store the start of the free space if we find. But if we\n * don't find suitable free space, it will be used to store the start position\n * of the max free space.\n *\n * @len is used to store the size of the free space that we find.\n * But if we don't find suitable free space, it is used to store the size of\n * the max free space.\n */\nint find_free_dev_extent_start(struct btrfs_transaction *transaction,\n\t\t\t       struct btrfs_device *device, u64 num_bytes,\n\t\t\t       u64 search_start, u64 *start, u64 *len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_dev_extent *dev_extent;\n\tstruct btrfs_path *path;\n\tu64 hole_size;\n\tu64 max_hole_start;\n\tu64 max_hole_size;\n\tu64 extent_end;\n\tu64 search_end = device->total_bytes;\n\tint ret;\n\tint slot;\n\tstruct extent_buffer *l;\n\n\t/*\n\t * We don't want to overwrite the superblock on the drive nor any area\n\t * used by the boot loader (grub for example), so we make sure to start\n\t * at an offset of at least 1MB.\n\t */\n\tsearch_start = max_t(u64, search_start, SZ_1M);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmax_hole_start = search_start;\n\tmax_hole_size = 0;\n\nagain:\n\tif (search_start >= search_end ||\n\t\ttest_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tpath->reada = READA_FORWARD;\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tkey.objectid = device->devid;\n\tkey.offset = search_start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) {\n\t\tret = btrfs_previous_item(root, path, key.objectid, key.type);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\twhile (1) {\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(l)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\tif (key.objectid < device->devid)\n\t\t\tgoto next;\n\n\t\tif (key.objectid > device->devid)\n\t\t\tbreak;\n\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tgoto next;\n\n\t\tif (key.offset > search_start) {\n\t\t\thole_size = key.offset - search_start;\n\n\t\t\t/*\n\t\t\t * Have to check before we set max_hole_start, otherwise\n\t\t\t * we could end up sending back this offset anyway.\n\t\t\t */\n\t\t\tif (contains_pending_extent(transaction, device,\n\t\t\t\t\t\t    &search_start,\n\t\t\t\t\t\t    hole_size)) {\n\t\t\t\tif (key.offset >= search_start) {\n\t\t\t\t\thole_size = key.offset - search_start;\n\t\t\t\t} else {\n\t\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\t\thole_size = 0;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (hole_size > max_hole_size) {\n\t\t\t\tmax_hole_start = search_start;\n\t\t\t\tmax_hole_size = hole_size;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If this free space is greater than which we need,\n\t\t\t * it must be the max free space that we have found\n\t\t\t * until now, so max_hole_start must point to the start\n\t\t\t * of this free space and the length of this free space\n\t\t\t * is stored in max_hole_size. Thus, we return\n\t\t\t * max_hole_start and max_hole_size and go back to the\n\t\t\t * caller.\n\t\t\t */\n\t\t\tif (hole_size >= num_bytes) {\n\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\textent_end = key.offset + btrfs_dev_extent_length(l,\n\t\t\t\t\t\t\t\t  dev_extent);\n\t\tif (extent_end > search_start)\n\t\t\tsearch_start = extent_end;\nnext:\n\t\tpath->slots[0]++;\n\t\tcond_resched();\n\t}\n\n\t/*\n\t * At this point, search_start should be the end of\n\t * allocated dev extents, and when shrinking the device,\n\t * search_end may be smaller than search_start.\n\t */\n\tif (search_end > search_start) {\n\t\thole_size = search_end - search_start;\n\n\t\tif (contains_pending_extent(transaction, device, &search_start,\n\t\t\t\t\t    hole_size)) {\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (hole_size > max_hole_size) {\n\t\t\tmax_hole_start = search_start;\n\t\t\tmax_hole_size = hole_size;\n\t\t}\n\t}\n\n\t/* See above. */\n\tif (max_hole_size < num_bytes)\n\t\tret = -ENOSPC;\n\telse\n\t\tret = 0;\n\nout:\n\tbtrfs_free_path(path);\n\t*start = max_hole_start;\n\tif (len)\n\t\t*len = max_hole_size;\n\treturn ret;\n}\n\nint find_free_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_device *device, u64 num_bytes,\n\t\t\t u64 *start, u64 *len)\n{\n\t/* FIXME use last free of some kind */\n\treturn find_free_dev_extent_start(trans->transaction, device,\n\t\t\t\t\t  num_bytes, 0, start, len);\n}\n\nstatic int btrfs_free_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_device *device,\n\t\t\t  u64 start, u64 *dev_extent_len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf = NULL;\n\tstruct btrfs_dev_extent *extent = NULL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = device->devid;\n\tkey.offset = start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\nagain:\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret > 0) {\n\t\tret = btrfs_previous_item(root, path, key.objectid,\n\t\t\t\t\t  BTRFS_DEV_EXTENT_KEY);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t\tBUG_ON(found_key.offset > start || found_key.offset +\n\t\t       btrfs_dev_extent_length(leaf, extent) < start);\n\t\tkey = found_key;\n\t\tbtrfs_release_path(path);\n\t\tgoto again;\n\t} else if (ret == 0) {\n\t\tleaf = path->nodes[0];\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t} else {\n\t\tbtrfs_handle_fs_error(fs_info, ret, \"Slot search failed\");\n\t\tgoto out;\n\t}\n\n\t*dev_extent_len = btrfs_dev_extent_length(leaf, extent);\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret) {\n\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t      \"Failed to remove dev extent item\");\n\t} else {\n\t\tset_bit(BTRFS_TRANS_HAVE_FREE_BGS, &trans->transaction->flags);\n\t}\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_alloc_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct btrfs_device *device,\n\t\t\t\t  u64 chunk_offset, u64 start, u64 num_bytes)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_dev_extent *extent;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\n\tWARN_ON(!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state));\n\tWARN_ON(test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state));\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = device->devid;\n\tkey.offset = start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      sizeof(*extent));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\tstruct btrfs_dev_extent);\n\tbtrfs_set_dev_extent_chunk_tree(leaf, extent,\n\t\t\t\t\tBTRFS_CHUNK_TREE_OBJECTID);\n\tbtrfs_set_dev_extent_chunk_objectid(leaf, extent,\n\t\t\t\t\t    BTRFS_FIRST_CHUNK_TREE_OBJECTID);\n\tbtrfs_set_dev_extent_chunk_offset(leaf, extent, chunk_offset);\n\n\tbtrfs_set_dev_extent_length(leaf, extent, num_bytes);\n\tbtrfs_mark_buffer_dirty(leaf);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic u64 find_next_chunk(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *n;\n\tu64 ret = 0;\n\n\tem_tree = &fs_info->mapping_tree.map_tree;\n\tread_lock(&em_tree->lock);\n\tn = rb_last(&em_tree->map.rb_root);\n\tif (n) {\n\t\tem = rb_entry(n, struct extent_map, rb_node);\n\t\tret = em->start + em->len;\n\t}\n\tread_unlock(&em_tree->lock);\n\n\treturn ret;\n}\n\nstatic noinline int find_next_devid(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 *devid_ret)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_path *path;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\n\tret = btrfs_search_slot(NULL, fs_info->chunk_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto error;\n\n\tBUG_ON(ret == 0); /* Corruption */\n\n\tret = btrfs_previous_item(fs_info->chunk_root, path,\n\t\t\t\t  BTRFS_DEV_ITEMS_OBJECTID,\n\t\t\t\t  BTRFS_DEV_ITEM_KEY);\n\tif (ret) {\n\t\t*devid_ret = 1;\n\t} else {\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &found_key,\n\t\t\t\t      path->slots[0]);\n\t\t*devid_ret = found_key.offset + 1;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * the device information is stored in the chunk root\n * the btrfs_device struct should be fully filled in\n */\nstatic int btrfs_add_dev_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tunsigned long ptr;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_insert_empty_item(trans, trans->fs_info->chunk_root, path,\n\t\t\t\t      &key, sizeof(*dev_item));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_generation(leaf, dev_item, 0);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_set_device_group(leaf, dev_item, 0);\n\tbtrfs_set_device_seek_speed(leaf, dev_item, 0);\n\tbtrfs_set_device_bandwidth(leaf, dev_item, 0);\n\tbtrfs_set_device_start_offset(leaf, dev_item, 0);\n\n\tptr = btrfs_device_uuid(dev_item);\n\twrite_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n\tptr = btrfs_device_fsid(dev_item);\n\twrite_extent_buffer(leaf, trans->fs_info->fs_devices->metadata_uuid,\n\t\t\t    ptr, BTRFS_FSID_SIZE);\n\tbtrfs_mark_buffer_dirty(leaf);\n\n\tret = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Function to update ctime/mtime for a given device path.\n * Mainly used for ctime/mtime based probe like libblkid.\n */\nstatic void update_dev_time(const char *path_name)\n{\n\tstruct file *filp;\n\n\tfilp = filp_open(path_name, O_RDWR, 0);\n\tif (IS_ERR(filp))\n\t\treturn;\n\tfile_update_time(filp);\n\tfilp_close(filp, NULL);\n}\n\nstatic int btrfs_rm_dev_item(struct btrfs_fs_info *fs_info,\n\t\t\t     struct btrfs_device *device)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_trans_handle *trans;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret) {\n\t\tif (ret > 0)\n\t\t\tret = -ENOENT;\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tif (!ret)\n\t\tret = btrfs_commit_transaction(trans);\n\treturn ret;\n}\n\n/*\n * Verify that @num_devices satisfies the RAID profile constraints in the whole\n * filesystem. It's up to the caller to adjust that number regarding eg. device\n * replace.\n */\nstatic int btrfs_check_raid_min_devices(struct btrfs_fs_info *fs_info,\n\t\tu64 num_devices)\n{\n\tu64 all_avail;\n\tunsigned seq;\n\tint i;\n\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tall_avail = fs_info->avail_data_alloc_bits |\n\t\t\t    fs_info->avail_system_alloc_bits |\n\t\t\t    fs_info->avail_metadata_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++) {\n\t\tif (!(all_avail & btrfs_raid_array[i].bg_flag))\n\t\t\tcontinue;\n\n\t\tif (num_devices < btrfs_raid_array[i].devs_min) {\n\t\t\tint ret = btrfs_raid_array[i].mindev_error;\n\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic struct btrfs_device * btrfs_find_next_active_device(\n\t\tstruct btrfs_fs_devices *fs_devs, struct btrfs_device *device)\n{\n\tstruct btrfs_device *next_device;\n\n\tlist_for_each_entry(next_device, &fs_devs->devices, dev_list) {\n\t\tif (next_device != device &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &next_device->dev_state)\n\t\t    && next_device->bdev)\n\t\t\treturn next_device;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * Helper function to check if the given device is part of s_bdev / latest_bdev\n * and replace it with the provided or the next active device, in the context\n * where this function called, there should be always be another device (or\n * this_dev) which is active.\n */\nvoid btrfs_assign_next_active_device(struct btrfs_device *device,\n\t\t\t\t     struct btrfs_device *this_dev)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_device *next_device;\n\n\tif (this_dev)\n\t\tnext_device = this_dev;\n\telse\n\t\tnext_device = btrfs_find_next_active_device(fs_info->fs_devices,\n\t\t\t\t\t\t\t\tdevice);\n\tASSERT(next_device);\n\n\tif (fs_info->sb->s_bdev &&\n\t\t\t(fs_info->sb->s_bdev == device->bdev))\n\t\tfs_info->sb->s_bdev = next_device->bdev;\n\n\tif (fs_info->fs_devices->latest_bdev == device->bdev)\n\t\tfs_info->fs_devices->latest_bdev = next_device->bdev;\n}\n\n/*\n * Return btrfs_fs_devices::num_devices excluding the device that's being\n * currently replaced.\n */\nstatic u64 btrfs_num_devices(struct btrfs_fs_info *fs_info)\n{\n\tu64 num_devices = fs_info->fs_devices->num_devices;\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (btrfs_dev_replace_is_ongoing(&fs_info->dev_replace)) {\n\t\tASSERT(num_devices > 1);\n\t\tnum_devices--;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\treturn num_devices;\n}\n\nint btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\tu64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(fs_info, device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(fs_info, device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_rm_device_link(fs_devices, device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(device->bdev, device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tcall_rcu(&device->rcu, free_device_rcu);\n\n\tif (cur_devices->open_devices == 0) {\n\t\twhile (fs_devices) {\n\t\t\tif (fs_devices->seed == cur_devices) {\n\t\t\t\tfs_devices->seed = cur_devices->seed;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tfs_devices = fs_devices->seed;\n\t\t}\n\t\tcur_devices->seed = NULL;\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}\n\nvoid btrfs_rm_dev_replace_remove_srcdev(struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlockdep_assert_held(&srcdev->fs_info->fs_devices->device_list_mutex);\n\n\t/*\n\t * in case of fs with no seed, srcdev->fs_devices will point\n\t * to fs_devices of fs_info. However when the dev being replaced is\n\t * a seed dev it will point to the seed's local fs_devices. In short\n\t * srcdev will have its correct fs_devices in both the cases.\n\t */\n\tfs_devices = srcdev->fs_devices;\n\n\tlist_del_rcu(&srcdev->dev_list);\n\tlist_del(&srcdev->dev_alloc_list);\n\tfs_devices->num_devices--;\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &srcdev->dev_state))\n\t\tfs_devices->missing_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &srcdev->dev_state))\n\t\tfs_devices->rw_devices--;\n\n\tif (srcdev->bdev)\n\t\tfs_devices->open_devices--;\n}\n\nvoid btrfs_rm_dev_replace_free_srcdev(struct btrfs_fs_info *fs_info,\n\t\t\t\t      struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = srcdev->fs_devices;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &srcdev->dev_state)) {\n\t\t/* zero out the old super if it is writable */\n\t\tbtrfs_scratch_superblocks(srcdev->bdev, srcdev->name->str);\n\t}\n\n\tbtrfs_close_bdev(srcdev);\n\tcall_rcu(&srcdev->rcu, free_device_rcu);\n\n\t/* if this is no devs we rather delete the fs_devices */\n\tif (!fs_devices->num_devices) {\n\t\tstruct btrfs_fs_devices *tmp_fs_devices;\n\n\t\t/*\n\t\t * On a mounted FS, num_devices can't be zero unless it's a\n\t\t * seed. In case of a seed device being replaced, the replace\n\t\t * target added to the sprout FS, so there will be no more\n\t\t * device left under the seed FS.\n\t\t */\n\t\tASSERT(fs_devices->seeding);\n\n\t\ttmp_fs_devices = fs_info->fs_devices;\n\t\twhile (tmp_fs_devices) {\n\t\t\tif (tmp_fs_devices->seed == fs_devices) {\n\t\t\t\ttmp_fs_devices->seed = fs_devices->seed;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttmp_fs_devices = tmp_fs_devices->seed;\n\t\t}\n\t\tfs_devices->seed = NULL;\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t}\n}\n\nvoid btrfs_destroy_dev_replace_tgtdev(struct btrfs_device *tgtdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = tgtdev->fs_info->fs_devices;\n\n\tWARN_ON(!tgtdev);\n\tmutex_lock(&fs_devices->device_list_mutex);\n\n\tbtrfs_sysfs_rm_device_link(fs_devices, tgtdev);\n\n\tif (tgtdev->bdev)\n\t\tfs_devices->open_devices--;\n\n\tfs_devices->num_devices--;\n\n\tbtrfs_assign_next_active_device(tgtdev, NULL);\n\n\tlist_del_rcu(&tgtdev->dev_list);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * The update_dev_time() with in btrfs_scratch_superblocks()\n\t * may lead to a call to btrfs_show_devname() which will try\n\t * to hold device_list_mutex. And here this device\n\t * is already out of device list, so we don't have to hold\n\t * the device_list_mutex lock.\n\t */\n\tbtrfs_scratch_superblocks(tgtdev->bdev, tgtdev->name->str);\n\n\tbtrfs_close_bdev(tgtdev);\n\tcall_rcu(&tgtdev->rcu, free_device_rcu);\n}\n\nstatic struct btrfs_device *btrfs_find_device_by_path(\n\t\tstruct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tint ret = 0;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tu8 *dev_uuid;\n\tstruct block_device *bdev;\n\tstruct buffer_head *bh;\n\tstruct btrfs_device *device;\n\n\tret = btrfs_get_bdev_and_sb(device_path, FMODE_READ,\n\t\t\t\t    fs_info->bdev_holder, 0, &bdev, &bh);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\tdisk_super = (struct btrfs_super_block *)bh->b_data;\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tdev_uuid = disk_super->dev_item.uuid;\n\tif (btrfs_fs_incompat(fs_info, METADATA_UUID))\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->metadata_uuid);\n\telse\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->fsid);\n\n\tbrelse(bh);\n\tif (!device)\n\t\tdevice = ERR_PTR(-ENOENT);\n\tblkdev_put(bdev, FMODE_READ);\n\treturn device;\n}\n\n/*\n * Lookup a device given by device id, or the path if the id is 0.\n */\nstruct btrfs_device *btrfs_find_device_by_devspec(\n\t\tstruct btrfs_fs_info *fs_info, u64 devid,\n\t\tconst char *device_path)\n{\n\tstruct btrfs_device *device;\n\n\tif (devid) {\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL,\n\t\t\t\t\t   NULL);\n\t\tif (!device)\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\treturn device;\n\t}\n\n\tif (!device_path || !device_path[0])\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (strcmp(device_path, \"missing\") == 0) {\n\t\t/* Find first missing device */\n\t\tlist_for_each_entry(device, &fs_info->fs_devices->devices,\n\t\t\t\t    dev_list) {\n\t\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t     &device->dev_state) && !device->bdev)\n\t\t\t\treturn device;\n\t\t}\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\treturn btrfs_find_device_by_path(fs_info, device_path);\n}\n\n/*\n * does all the dirty work required for changing file system's UUID.\n */\nstatic int btrfs_prepare_sprout(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_fs_devices *old_devices;\n\tstruct btrfs_fs_devices *seed_devices;\n\tstruct btrfs_super_block *disk_super = fs_info->super_copy;\n\tstruct btrfs_device *device;\n\tu64 super_flags;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tif (!fs_devices->seeding)\n\t\treturn -EINVAL;\n\n\tseed_devices = alloc_fs_devices(NULL, NULL);\n\tif (IS_ERR(seed_devices))\n\t\treturn PTR_ERR(seed_devices);\n\n\told_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(old_devices)) {\n\t\tkfree(seed_devices);\n\t\treturn PTR_ERR(old_devices);\n\t}\n\n\tlist_add(&old_devices->fs_list, &fs_uuids);\n\n\tmemcpy(seed_devices, fs_devices, sizeof(*seed_devices));\n\tseed_devices->opened = 1;\n\tINIT_LIST_HEAD(&seed_devices->devices);\n\tINIT_LIST_HEAD(&seed_devices->alloc_list);\n\tmutex_init(&seed_devices->device_list_mutex);\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_splice_init_rcu(&fs_devices->devices, &seed_devices->devices,\n\t\t\t      synchronize_rcu);\n\tlist_for_each_entry(device, &seed_devices->devices, dev_list)\n\t\tdevice->fs_devices = seed_devices;\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_splice_init(&fs_devices->alloc_list, &seed_devices->alloc_list);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\tfs_devices->seeding = 0;\n\tfs_devices->num_devices = 0;\n\tfs_devices->open_devices = 0;\n\tfs_devices->missing_devices = 0;\n\tfs_devices->rotating = 0;\n\tfs_devices->seed = seed_devices;\n\n\tgenerate_random_uuid(fs_devices->fsid);\n\tmemcpy(fs_devices->metadata_uuid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\tmemcpy(disk_super->fsid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tsuper_flags = btrfs_super_flags(disk_super) &\n\t\t      ~BTRFS_SUPER_FLAG_SEEDING;\n\tbtrfs_set_super_flags(disk_super, super_flags);\n\n\treturn 0;\n}\n\n/*\n * Store the expected generation for seed devices in device items.\n */\nstatic int btrfs_finish_sprout(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct btrfs_device *device;\n\tstruct btrfs_key key;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\tu64 devid;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\nnext_slot:\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != BTRFS_DEV_ITEMS_OBJECTID ||\n\t\t    key.type != BTRFS_DEV_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tdev_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t  struct btrfs_dev_item);\n\t\tdevid = btrfs_device_id(leaf, dev_item);\n\t\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t\t   BTRFS_FSID_SIZE);\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   fs_uuid);\n\t\tBUG_ON(!device); /* Logic error */\n\n\t\tif (device->fs_devices->seeding) {\n\t\t\tbtrfs_set_device_generation(leaf, dev_item,\n\t\t\t\t\t\t    device->generation);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tgoto next_slot;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct request_queue *q;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device;\n\tstruct block_device *bdev;\n\tstruct super_block *sb = fs_info->sb;\n\tstruct rcu_string *name;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 orig_super_total_bytes;\n\tu64 orig_super_num_devices;\n\tint seeding_dev = 0;\n\tint ret = 0;\n\tbool unlocked = false;\n\n\tif (sb_rdonly(sb) && !fs_devices->seeding)\n\t\treturn -EROFS;\n\n\tbdev = blkdev_get_by_path(device_path, FMODE_WRITE | FMODE_EXCL,\n\t\t\t\t  fs_info->bdev_holder);\n\tif (IS_ERR(bdev))\n\t\treturn PTR_ERR(bdev);\n\n\tif (fs_devices->seeding) {\n\t\tseeding_dev = 1;\n\t\tdown_write(&sb->s_umount);\n\t\tmutex_lock(&uuid_mutex);\n\t}\n\n\tfilemap_write_and_wait(bdev->bd_inode->i_mapping);\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tif (device->bdev == bdev) {\n\t\t\tret = -EEXIST;\n\t\t\tmutex_unlock(\n\t\t\t\t&fs_devices->device_list_mutex);\n\t\t\tgoto error;\n\t\t}\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tdevice = btrfs_alloc_device(fs_info, NULL, NULL);\n\tif (IS_ERR(device)) {\n\t\t/* we can safely leave the fs_devices entry around */\n\t\tret = PTR_ERR(device);\n\t\tgoto error;\n\t}\n\n\tname = rcu_string_strdup(device_path, GFP_KERNEL);\n\tif (!name) {\n\t\tret = -ENOMEM;\n\t\tgoto error_free_device;\n\t}\n\trcu_assign_pointer(device->name, name);\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto error_free_device;\n\t}\n\n\tq = bdev_get_queue(bdev);\n\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tdevice->generation = trans->transid;\n\tdevice->io_width = fs_info->sectorsize;\n\tdevice->io_align = fs_info->sectorsize;\n\tdevice->sector_size = fs_info->sectorsize;\n\tdevice->total_bytes = round_down(i_size_read(bdev->bd_inode),\n\t\t\t\t\t fs_info->sectorsize);\n\tdevice->disk_total_bytes = device->total_bytes;\n\tdevice->commit_total_bytes = device->total_bytes;\n\tdevice->fs_info = fs_info;\n\tdevice->bdev = bdev;\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\tdevice->mode = FMODE_EXCL;\n\tdevice->dev_stats_valid = 1;\n\tset_blocksize(device->bdev, BTRFS_BDEV_BLOCKSIZE);\n\n\tif (seeding_dev) {\n\t\tsb->s_flags &= ~SB_RDONLY;\n\t\tret = btrfs_prepare_sprout(fs_info);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_trans;\n\t\t}\n\t}\n\n\tdevice->fs_devices = fs_devices;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\tlist_add(&device->dev_alloc_list, &fs_devices->alloc_list);\n\tfs_devices->num_devices++;\n\tfs_devices->open_devices++;\n\tfs_devices->rw_devices++;\n\tfs_devices->total_devices++;\n\tfs_devices->total_rw_bytes += device->total_bytes;\n\n\tatomic64_add(device->total_bytes, &fs_info->free_chunk_space);\n\n\tif (!blk_queue_nonrot(q))\n\t\tfs_devices->rotating = 1;\n\n\torig_super_total_bytes = btrfs_super_total_bytes(fs_info->super_copy);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\tround_down(orig_super_total_bytes + device->total_bytes,\n\t\t\t   fs_info->sectorsize));\n\n\torig_super_num_devices = btrfs_super_num_devices(fs_info->super_copy);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices + 1);\n\n\t/* add sysfs device entry */\n\tbtrfs_sysfs_add_device_link(fs_devices, device);\n\n\t/*\n\t * we've got more storage, clear any full flags on the space\n\t * infos\n\t */\n\tbtrfs_clear_space_info_full(fs_info);\n\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (seeding_dev) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tret = init_first_rw_device(trans, fs_info);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\t}\n\n\tret = btrfs_add_dev_item(trans, device);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto error_sysfs;\n\t}\n\n\tif (seeding_dev) {\n\t\tchar fsid_buf[BTRFS_UUID_UNPARSED_SIZE];\n\n\t\tret = btrfs_finish_sprout(trans, fs_info);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\n\t\t/* Sprouting would change fsid of the mounted root,\n\t\t * so rename the fsid on the sysfs\n\t\t */\n\t\tsnprintf(fsid_buf, BTRFS_UUID_UNPARSED_SIZE, \"%pU\",\n\t\t\t\t\t\tfs_info->fs_devices->fsid);\n\t\tif (kobject_rename(&fs_devices->fsid_kobj, fsid_buf))\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"sysfs: failed to create fsid for sprout\");\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\n\tif (seeding_dev) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t\tunlocked = true;\n\n\t\tif (ret) /* transaction commit */\n\t\t\treturn ret;\n\n\t\tret = btrfs_relocate_sys_chunks(fs_info);\n\t\tif (ret < 0)\n\t\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t    \"Failed to relocate sys chunks after device initialization. This can be fixed using the \\\"btrfs balance\\\" command.\");\n\t\ttrans = btrfs_attach_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tif (PTR_ERR(trans) == -ENOENT)\n\t\t\t\treturn 0;\n\t\t\tret = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tgoto error_sysfs;\n\t\t}\n\t\tret = btrfs_commit_transaction(trans);\n\t}\n\n\t/* Update ctime/mtime for libblkid */\n\tupdate_dev_time(device_path);\n\treturn ret;\n\nerror_sysfs:\n\tbtrfs_sysfs_rm_device_link(fs_devices, device);\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_del_rcu(&device->dev_list);\n\tlist_del(&device->dev_alloc_list);\n\tfs_info->fs_devices->num_devices--;\n\tfs_info->fs_devices->open_devices--;\n\tfs_info->fs_devices->rw_devices--;\n\tfs_info->fs_devices->total_devices--;\n\tfs_info->fs_devices->total_rw_bytes -= device->total_bytes;\n\tatomic64_sub(device->total_bytes, &fs_info->free_chunk_space);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\t\t\t    orig_super_total_bytes);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\nerror_trans:\n\tif (seeding_dev)\n\t\tsb->s_flags |= SB_RDONLY;\n\tif (trans)\n\t\tbtrfs_end_transaction(trans);\nerror_free_device:\n\tbtrfs_free_device(device);\nerror:\n\tblkdev_put(bdev, FMODE_EXCL);\n\tif (seeding_dev && !unlocked) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t}\n\treturn ret;\n}\n\nstatic noinline int btrfs_update_device(struct btrfs_trans_handle *trans,\n\t\t\t\t\tstruct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = device->fs_info->chunk_root;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_mark_buffer_dirty(leaf);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_grow_device(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_fs_devices *fs_devices;\n\tu64 old_total;\n\tu64 diff;\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\treturn -EACCES;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\told_total = btrfs_super_total_bytes(super_copy);\n\tdiff = round_down(new_size - device->total_bytes, fs_info->sectorsize);\n\n\tif (new_size <= device->total_bytes ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tfs_devices = fs_info->fs_devices;\n\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total + diff, fs_info->sectorsize));\n\tdevice->fs_devices->total_rw_bytes += diff;\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tbtrfs_clear_space_info_full(device->fs_info);\n\tif (list_empty(&device->resized_list))\n\t\tlist_add_tail(&device->resized_list,\n\t\t\t      &fs_devices->resized_devices);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\treturn btrfs_update_device(trans, device);\n}\n\nstatic int btrfs_free_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = chunk_offset;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\telse if (ret > 0) { /* Logic error or corruption */\n\t\tbtrfs_handle_fs_error(fs_info, -ENOENT,\n\t\t\t\t      \"Failed lookup while freeing chunk.\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret < 0)\n\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t      \"Failed to delete chunk item.\");\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_del_sys_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *ptr;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur;\n\tstruct btrfs_key key;\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tptr = super_copy->sys_chunk_array;\n\tcur = 0;\n\n\twhile (cur < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)ptr;\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tlen = sizeof(*disk_key);\n\n\t\tif (key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tchunk = (struct btrfs_chunk *)(ptr + len);\n\t\t\tnum_stripes = btrfs_stack_chunk_num_stripes(chunk);\n\t\t\tlen += btrfs_chunk_item_size(num_stripes);\n\t\t} else {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (key.objectid == BTRFS_FIRST_CHUNK_TREE_OBJECTID &&\n\t\t    key.offset == chunk_offset) {\n\t\t\tmemmove(ptr, ptr + len, array_size - (cur + len));\n\t\t\tarray_size -= len;\n\t\t\tbtrfs_set_super_sys_array_size(super_copy, array_size);\n\t\t} else {\n\t\t\tptr += len;\n\t\t\tcur += len;\n\t\t}\n\t}\n\tmutex_unlock(&fs_info->chunk_mutex);\n\treturn ret;\n}\n\n/*\n * btrfs_get_chunk_map() - Find the mapping containing the given logical extent.\n * @logical: Logical block offset in bytes.\n * @length: Length of extent in bytes.\n *\n * Return: Chunk mapping or ERR_PTR.\n */\nstruct extent_map *btrfs_get_chunk_map(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u64 logical, u64 length)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\n\tem_tree = &fs_info->mapping_tree.map_tree;\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, logical, length);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_crit(fs_info, \"unable to find logical %llu length %llu\",\n\t\t\t   logical, length);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (em->start > logical || em->start + em->len < logical) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"found a bad mapping, wanted %llu-%llu, found %llu-%llu\",\n\t\t\t   logical, length, em->start, em->start + em->len);\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/* callers are responsible for dropping em's ref. */\n\treturn em;\n}\n\nint btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 dev_extent_len = 0;\n\tint i, ret = 0;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em)) {\n\t\t/*\n\t\t * This is a logic error, but we don't want to just rely on the\n\t\t * user having built with ASSERT enabled, so if ASSERT doesn't\n\t\t * do anything we still error out.\n\t\t */\n\t\tASSERT(0);\n\t\treturn PTR_ERR(em);\n\t}\n\tmap = em->map_lookup;\n\tmutex_lock(&fs_info->chunk_mutex);\n\tcheck_system_chunk(trans, map->type);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t/*\n\t * Take the device list mutex to prevent races with the final phase of\n\t * a device replace operation that replaces the device object associated\n\t * with map stripes (dev-replace.c:btrfs_dev_replace_finishing()).\n\t */\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\t\tret = btrfs_free_dev_extent(trans, device,\n\t\t\t\t\t    map->stripes[i].physical,\n\t\t\t\t\t    &dev_extent_len);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (device->bytes_used > 0) {\n\t\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\t\tbtrfs_device_set_bytes_used(device,\n\t\t\t\t\tdevice->bytes_used - dev_extent_len);\n\t\t\tatomic64_add(dev_extent_len, &fs_info->free_chunk_space);\n\t\t\tbtrfs_clear_space_info_full(fs_info);\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t}\n\n\t\tret = btrfs_update_device(trans, device);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tret = btrfs_free_chunk(trans, chunk_offset);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\ttrace_btrfs_chunk_free(fs_info, map, chunk_offset, em->len);\n\n\tif (map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tret = btrfs_del_sys_chunk(fs_info, chunk_offset);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = btrfs_remove_block_group(trans, chunk_offset, em);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\nout:\n\t/* once for us */\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic int btrfs_relocate_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\t/*\n\t * Prevent races with automatic removal of unused block groups.\n\t * After we relocate and before we remove the chunk with offset\n\t * chunk_offset, automatic removal of the block group can kick in,\n\t * resulting in a failure when calling btrfs_remove_chunk() below.\n\t *\n\t * Make sure to acquire this mutex before doing a tree search (dev\n\t * or chunk trees) to find chunks. Otherwise the cleaner kthread might\n\t * call btrfs_remove_chunk() (through btrfs_delete_unused_bgs()) after\n\t * we release the path used to search the chunk/dev tree and before\n\t * the current task acquires this mutex and calls us.\n\t */\n\tlockdep_assert_held(&fs_info->delete_unused_bgs_mutex);\n\n\tret = btrfs_can_relocate(fs_info, chunk_offset);\n\tif (ret)\n\t\treturn -ENOSPC;\n\n\t/* step one, relocate all the extents inside this chunk */\n\tbtrfs_scrub_pause(fs_info);\n\tret = btrfs_relocate_block_group(fs_info, chunk_offset);\n\tbtrfs_scrub_continue(fs_info);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * We add the kobjects here (and after forcing data chunk creation)\n\t * since relocation is the only place we'll create chunks of a new\n\t * type at runtime.  The only place where we'll remove the last\n\t * chunk of a type is the call immediately below this one.  Even\n\t * so, we're protected against races with the cleaner thread since\n\t * we're covered by the delete_unused_bgs_mutex.\n\t */\n\tbtrfs_add_raid_kobjects(fs_info);\n\n\ttrans = btrfs_start_trans_remove_block_group(root->fs_info,\n\t\t\t\t\t\t     chunk_offset);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tbtrfs_handle_fs_error(root->fs_info, ret, NULL);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * step two, delete the device extents and the\n\t * chunk tree entries\n\t */\n\tret = btrfs_remove_chunk(trans, chunk_offset);\n\tbtrfs_end_transaction(trans);\n\treturn ret;\n}\n\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tu64 chunk_type;\n\tbool retried = false;\n\tint failed = 0;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\nagain:\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tmutex_lock(&fs_info->delete_unused_bgs_mutex);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto error;\n\t\t}\n\t\tBUG_ON(ret == 0); /* Corruption */\n\n\t\tret = btrfs_previous_item(chunk_root, path, key.objectid,\n\t\t\t\t\t  key.type);\n\t\tif (ret)\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\t\tif (ret > 0)\n\t\t\tbreak;\n\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\t\tchunk = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t       struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\t\tbtrfs_release_path(path);\n\n\t\tif (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\t\tif (ret == -ENOSPC)\n\t\t\t\tfailed++;\n\t\t\telse\n\t\t\t\tBUG_ON(ret);\n\t\t}\n\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\tret = 0;\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (WARN_ON(failed && retried)) {\n\t\tret = -ENOSPC;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * return 1 : allocate a data chunk successfully,\n * return <0: errors during allocating a data chunk,\n * return 0 : no need to allocate a data chunk.\n */\nstatic int btrfs_may_alloc_data_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t      u64 chunk_offset)\n{\n\tstruct btrfs_block_group_cache *cache;\n\tu64 bytes_used;\n\tu64 chunk_type;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tASSERT(cache);\n\tchunk_type = cache->flags;\n\tbtrfs_put_block_group(cache);\n\n\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tspin_lock(&fs_info->data_sinfo->lock);\n\t\tbytes_used = fs_info->data_sinfo->bytes_used;\n\t\tspin_unlock(&fs_info->data_sinfo->lock);\n\n\t\tif (!bytes_used) {\n\t\t\tstruct btrfs_trans_handle *trans;\n\t\t\tint ret;\n\n\t\t\ttrans =\tbtrfs_join_transaction(fs_info->tree_root);\n\t\t\tif (IS_ERR(trans))\n\t\t\t\treturn PTR_ERR(trans);\n\n\t\t\tret = btrfs_force_chunk_alloc(trans,\n\t\t\t\t\t\t      BTRFS_BLOCK_GROUP_DATA);\n\t\t\tbtrfs_end_transaction(trans);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\n\t\t\tbtrfs_add_raid_kobjects(fs_info);\n\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int insert_balance_item(struct btrfs_fs_info *fs_info,\n\t\t\t       struct btrfs_balance_control *bctl)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      sizeof(*item));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tmemzero_extent_buffer(leaf, (unsigned long)item, sizeof(*item));\n\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->data);\n\tbtrfs_set_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->meta);\n\tbtrfs_set_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->sys);\n\tbtrfs_set_balance_sys(leaf, item, &disk_bargs);\n\n\tbtrfs_set_balance_flags(leaf, item, bctl->flags);\n\n\tbtrfs_mark_buffer_dirty(leaf);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int del_balance_item(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\n/*\n * This is a heuristic used to reduce the number of chunks balanced on\n * resume after balance was interrupted.\n */\nstatic void update_balance_args(struct btrfs_balance_control *bctl)\n{\n\t/*\n\t * Turn on soft mode for chunk types that were being converted.\n\t */\n\tif (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\n\t/*\n\t * Turn on usage filter if is not already used.  The idea is\n\t * that chunks that we have already balanced should be\n\t * reasonably full.  Don't do it for chunks that are being\n\t * converted - that will keep us from relocating unconverted\n\t * (albeit full) chunks.\n\t */\n\tif (!(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->data.usage = 90;\n\t}\n\tif (!(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->sys.usage = 90;\n\t}\n\tif (!(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->meta.usage = 90;\n\t}\n}\n\n/*\n * Clear the balance status in fs_info and delete the balance item from disk.\n */\nstatic void reset_balance_state(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tint ret;\n\n\tBUG_ON(!fs_info->balance_ctl);\n\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = NULL;\n\tspin_unlock(&fs_info->balance_lock);\n\n\tkfree(bctl);\n\tret = del_balance_item(fs_info);\n\tif (ret)\n\t\tbtrfs_handle_fs_error(fs_info, ret, NULL);\n}\n\n/*\n * Balance filters.  Return 1 if chunk should be filtered out\n * (should not be balanced).\n */\nstatic int chunk_profiles_filter(u64 chunk_type,\n\t\t\t\t struct btrfs_balance_args *bargs)\n{\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->profiles & chunk_type)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_usage_range_filter(struct btrfs_fs_info *fs_info, u64 chunk_offset,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group_cache *cache;\n\tu64 chunk_used;\n\tu64 user_thresh_min;\n\tu64 user_thresh_max;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = btrfs_block_group_used(&cache->item);\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh_min = 0;\n\telse\n\t\tuser_thresh_min = div_factor_fine(cache->key.offset,\n\t\t\t\t\tbargs->usage_min);\n\n\tif (bargs->usage_max == 0)\n\t\tuser_thresh_max = 1;\n\telse if (bargs->usage_max > 100)\n\t\tuser_thresh_max = cache->key.offset;\n\telse\n\t\tuser_thresh_max = div_factor_fine(cache->key.offset,\n\t\t\t\t\tbargs->usage_max);\n\n\tif (user_thresh_min <= chunk_used && chunk_used < user_thresh_max)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_usage_filter(struct btrfs_fs_info *fs_info,\n\t\tu64 chunk_offset, struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group_cache *cache;\n\tu64 chunk_used, user_thresh;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = btrfs_block_group_used(&cache->item);\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh = 1;\n\telse if (bargs->usage > 100)\n\t\tuser_thresh = cache->key.offset;\n\telse\n\t\tuser_thresh = div_factor_fine(cache->key.offset,\n\t\t\t\t\t      bargs->usage);\n\n\tif (chunk_used < user_thresh)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_devid_filter(struct extent_buffer *leaf,\n\t\t\t      struct btrfs_chunk *chunk,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tint i;\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) == bargs->devid)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/* [pstart, pend) */\nstatic int chunk_drange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tu64 stripe_offset;\n\tu64 stripe_length;\n\tint factor;\n\tint i;\n\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_DEVID))\n\t\treturn 0;\n\n\tif (btrfs_chunk_type(leaf, chunk) & (BTRFS_BLOCK_GROUP_DUP |\n\t     BTRFS_BLOCK_GROUP_RAID1 | BTRFS_BLOCK_GROUP_RAID10)) {\n\t\tfactor = num_stripes / 2;\n\t} else if (btrfs_chunk_type(leaf, chunk) & BTRFS_BLOCK_GROUP_RAID5) {\n\t\tfactor = num_stripes - 1;\n\t} else if (btrfs_chunk_type(leaf, chunk) & BTRFS_BLOCK_GROUP_RAID6) {\n\t\tfactor = num_stripes - 2;\n\t} else {\n\t\tfactor = num_stripes;\n\t}\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) != bargs->devid)\n\t\t\tcontinue;\n\n\t\tstripe_offset = btrfs_stripe_offset(leaf, stripe);\n\t\tstripe_length = btrfs_chunk_length(leaf, chunk);\n\t\tstripe_length = div_u64(stripe_length, factor);\n\n\t\tif (stripe_offset < bargs->pend &&\n\t\t    stripe_offset + stripe_length > bargs->pstart)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/* [vstart, vend) */\nstatic int chunk_vrange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       u64 chunk_offset,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tif (chunk_offset < bargs->vend &&\n\t    chunk_offset + btrfs_chunk_length(leaf, chunk) > bargs->vstart)\n\t\t/* at least part of the chunk is inside this vrange */\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_stripes_range_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tif (bargs->stripes_min <= num_stripes\n\t\t\t&& num_stripes <= bargs->stripes_max)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_soft_convert_filter(u64 chunk_type,\n\t\t\t\t     struct btrfs_balance_args *bargs)\n{\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_CONVERT))\n\t\treturn 0;\n\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->target == chunk_type)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int should_balance_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\tstruct extent_buffer *leaf,\n\t\t\t\tstruct btrfs_chunk *chunk, u64 chunk_offset)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_balance_args *bargs = NULL;\n\tu64 chunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t/* type filter */\n\tif (!((chunk_type & BTRFS_BLOCK_GROUP_TYPE_MASK) &\n\t      (bctl->flags & BTRFS_BALANCE_TYPE_MASK))) {\n\t\treturn 0;\n\t}\n\n\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\tbargs = &bctl->data;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\tbargs = &bctl->sys;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\tbargs = &bctl->meta;\n\n\t/* profiles filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_PROFILES) &&\n\t    chunk_profiles_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* usage filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    chunk_usage_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    chunk_usage_range_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* devid filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DEVID) &&\n\t    chunk_devid_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* drange filter, makes sense only with devid filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DRANGE) &&\n\t    chunk_drange_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* vrange filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_VRANGE) &&\n\t    chunk_vrange_filter(leaf, chunk, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* stripes filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE) &&\n\t    chunk_stripes_range_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* soft profile changing mode */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_SOFT) &&\n\t    chunk_soft_convert_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/*\n\t * limited by count, must be the last filter\n\t */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT)) {\n\t\tif (bargs->limit == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit--;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)) {\n\t\t/*\n\t\t * Same logic as the 'limit' filter; the minimum cannot be\n\t\t * determined here because we do not have the global information\n\t\t * about the count of all chunks that satisfy the filters.\n\t\t */\n\t\tif (bargs->limit_max == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit_max--;\n\t}\n\n\treturn 1;\n}\n\nstatic int __btrfs_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tu64 chunk_type;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\tint ret;\n\tint enospc_errors = 0;\n\tbool counting = true;\n\t/* The single value limit and min/max limits use the same bytes in the */\n\tu64 limit_data = bctl->data.limit;\n\tu64 limit_meta = bctl->meta.limit;\n\tu64 limit_sys = bctl->sys.limit;\n\tu32 count_data = 0;\n\tu32 count_meta = 0;\n\tu32 count_sys = 0;\n\tint chunk_reserved = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\t/* zero out stat counters */\n\tspin_lock(&fs_info->balance_lock);\n\tmemset(&bctl->stat, 0, sizeof(bctl->stat));\n\tspin_unlock(&fs_info->balance_lock);\nagain:\n\tif (!counting) {\n\t\t/*\n\t\t * The single value limit and min/max limits use the same bytes\n\t\t * in the\n\t\t */\n\t\tbctl->data.limit = limit_data;\n\t\tbctl->meta.limit = limit_meta;\n\t\tbctl->sys.limit = limit_sys;\n\t}\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tif ((!counting && atomic_read(&fs_info->balance_pause_req)) ||\n\t\t    atomic_read(&fs_info->balance_cancel_req)) {\n\t\t\tret = -ECANCELED;\n\t\t\tgoto error;\n\t\t}\n\n\t\tmutex_lock(&fs_info->delete_unused_bgs_mutex);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * this shouldn't happen, it means the last relocate\n\t\t * failed\n\t\t */\n\t\tif (ret == 0)\n\t\t\tBUG(); /* FIXME break ? */\n\n\t\tret = btrfs_previous_item(chunk_root, path, 0,\n\t\t\t\t\t  BTRFS_CHUNK_ITEM_KEY);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\tif (found_key.objectid != key.objectid) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t\tif (!counting) {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.considered++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\n\n\t\tret = should_balance_chunk(fs_info, leaf, chunk,\n\t\t\t\t\t   found_key.offset);\n\n\t\tbtrfs_release_path(path);\n\t\tif (!ret) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (counting) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.expected++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\n\t\t\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\t\t\tcount_data++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\t\t\tcount_sys++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\t\t\tcount_meta++;\n\n\t\t\tgoto loop;\n\t\t}\n\n\t\t/*\n\t\t * Apply limit_min filter, no need to check if the LIMITS\n\t\t * filter is used, limit_min is 0 by default\n\t\t */\n\t\tif (((chunk_type & BTRFS_BLOCK_GROUP_DATA) &&\n\t\t\t\t\tcount_data < bctl->data.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t\t\t\tcount_meta < bctl->meta.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) &&\n\t\t\t\t\tcount_sys < bctl->sys.limit_min)) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (!chunk_reserved) {\n\t\t\t/*\n\t\t\t * We may be relocating the only data chunk we have,\n\t\t\t * which could potentially end up with losing data's\n\t\t\t * raid profile, so lets allocate an empty one in\n\t\t\t * advance.\n\t\t\t */\n\t\t\tret = btrfs_may_alloc_data_chunk(fs_info,\n\t\t\t\t\t\t\t found_key.offset);\n\t\t\tif (ret < 0) {\n\t\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\t\tgoto error;\n\t\t\t} else if (ret == 1) {\n\t\t\t\tchunk_reserved = 1;\n\t\t\t}\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\tif (ret == -ENOSPC) {\n\t\t\tenospc_errors++;\n\t\t} else if (ret == -ETXTBSY) {\n\t\t\tbtrfs_info(fs_info,\n\t   \"skipping relocation of block group %llu due to active swapfile\",\n\t\t\t\t   found_key.offset);\n\t\t\tret = 0;\n\t\t} else if (ret) {\n\t\t\tgoto error;\n\t\t} else {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.completed++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\nloop:\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\n\tif (counting) {\n\t\tbtrfs_release_path(path);\n\t\tcounting = false;\n\t\tgoto again;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\tif (enospc_errors) {\n\t\tbtrfs_info(fs_info, \"%d enospc errors during balance\",\n\t\t\t   enospc_errors);\n\t\tif (!ret)\n\t\t\tret = -ENOSPC;\n\t}\n\n\treturn ret;\n}\n\n/**\n * alloc_profile_is_valid - see if a given profile is valid and reduced\n * @flags: profile to validate\n * @extended: if true @flags is treated as an extended profile\n */\nstatic int alloc_profile_is_valid(u64 flags, int extended)\n{\n\tu64 mask = (extended ? BTRFS_EXTENDED_PROFILE_MASK :\n\t\t\t       BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\n\tflags &= ~BTRFS_BLOCK_GROUP_TYPE_MASK;\n\n\t/* 1) check that all other bits are zeroed */\n\tif (flags & ~mask)\n\t\treturn 0;\n\n\t/* 2) see if profile is reduced */\n\tif (flags == 0)\n\t\treturn !extended; /* \"0\" is valid for usual profiles */\n\n\t/* true if exactly one bit set */\n\treturn is_power_of_2(flags);\n}\n\nstatic inline int balance_need_close(struct btrfs_fs_info *fs_info)\n{\n\t/* cancel requested || normal exit path */\n\treturn atomic_read(&fs_info->balance_cancel_req) ||\n\t\t(atomic_read(&fs_info->balance_pause_req) == 0 &&\n\t\t atomic_read(&fs_info->balance_cancel_req) == 0);\n}\n\n/* Non-zero return value signifies invalidity */\nstatic inline int validate_convert_profile(struct btrfs_balance_args *bctl_arg,\n\t\tu64 allowed)\n{\n\treturn ((bctl_arg->flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t(!alloc_profile_is_valid(bctl_arg->target, 1) ||\n\t\t (bctl_arg->target & ~allowed)));\n}\n\n/*\n * Fill @buf with textual description of balance filter flags @bargs, up to\n * @size_buf including the terminating null. The output may be trimmed if it\n * does not fit into the provided buffer.\n */\nstatic void describe_balance_args(struct btrfs_balance_args *bargs, char *buf,\n\t\t\t\t u32 size_buf)\n{\n\tint ret;\n\tu32 size_bp = size_buf;\n\tchar *bp = buf;\n\tu64 flags = bargs->flags;\n\tchar tmp_buf[128] = {'\\0'};\n\n\tif (!flags)\n\t\treturn;\n\n#define CHECK_APPEND_NOARG(a)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_2ARG(a, v1, v2)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1), (v2));\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (flags & BTRFS_BALANCE_ARGS_CONVERT) {\n\t\tint index = btrfs_bg_flags_to_raid_index(bargs->target);\n\n\t\tCHECK_APPEND_1ARG(\"convert=%s,\", get_raid_name(index));\n\t}\n\n\tif (flags & BTRFS_BALANCE_ARGS_SOFT)\n\t\tCHECK_APPEND_NOARG(\"soft,\");\n\n\tif (flags & BTRFS_BALANCE_ARGS_PROFILES) {\n\t\tbtrfs_describe_block_groups(bargs->profiles, tmp_buf,\n\t\t\t\t\t    sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"profiles=%s,\", tmp_buf);\n\t}\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE)\n\t\tCHECK_APPEND_1ARG(\"usage=%llu,\", bargs->usage);\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE_RANGE)\n\t\tCHECK_APPEND_2ARG(\"usage=%u..%u,\",\n\t\t\t\t  bargs->usage_min, bargs->usage_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DEVID)\n\t\tCHECK_APPEND_1ARG(\"devid=%llu,\", bargs->devid);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DRANGE)\n\t\tCHECK_APPEND_2ARG(\"drange=%llu..%llu,\",\n\t\t\t\t  bargs->pstart, bargs->pend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_VRANGE)\n\t\tCHECK_APPEND_2ARG(\"vrange=%llu..%llu,\",\n\t\t\t\t  bargs->vstart, bargs->vend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT)\n\t\tCHECK_APPEND_1ARG(\"limit=%llu,\", bargs->limit);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)\n\t\tCHECK_APPEND_2ARG(\"limit=%u..%u,\",\n\t\t\t\tbargs->limit_min, bargs->limit_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE)\n\t\tCHECK_APPEND_2ARG(\"stripes=%u..%u,\",\n\t\t\t\t  bargs->stripes_min, bargs->stripes_max);\n\n#undef CHECK_APPEND_2ARG\n#undef CHECK_APPEND_1ARG\n#undef CHECK_APPEND_NOARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last , */\n\telse\n\t\tbuf[0] = '\\0';\n}\n\nstatic void describe_balance_start_or_resume(struct btrfs_fs_info *fs_info)\n{\n\tu32 size_buf = 1024;\n\tchar tmp_buf[192] = {'\\0'};\n\tchar *buf;\n\tchar *bp;\n\tu32 size_bp = size_buf;\n\tint ret;\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\n\tbuf = kzalloc(size_buf, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\n\tbp = buf;\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (bctl->flags & BTRFS_BALANCE_FORCE)\n\t\tCHECK_APPEND_1ARG(\"%s\", \"-f \");\n\n\tif (bctl->flags & BTRFS_BALANCE_DATA) {\n\t\tdescribe_balance_args(&bctl->data, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-d%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_METADATA) {\n\t\tdescribe_balance_args(&bctl->meta, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-m%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_SYSTEM) {\n\t\tdescribe_balance_args(&bctl->sys, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-s%s \", tmp_buf);\n\t}\n\n#undef CHECK_APPEND_1ARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last \" \" */\n\tbtrfs_info(fs_info, \"balance: %s %s\",\n\t\t   (bctl->flags & BTRFS_BALANCE_RESUME) ?\n\t\t   \"resume\" : \"start\", buf);\n\n\tkfree(buf);\n}\n\n/*\n * Should be called with balance mutexe held\n */\nint btrfs_balance(struct btrfs_fs_info *fs_info,\n\t\t  struct btrfs_balance_control *bctl,\n\t\t  struct btrfs_ioctl_balance_args *bargs)\n{\n\tu64 meta_target, data_target;\n\tu64 allowed;\n\tint mixed = 0;\n\tint ret;\n\tu64 num_devices;\n\tunsigned seq;\n\tbool reducing_integrity;\n\n\tif (btrfs_fs_closing(fs_info) ||\n\t    atomic_read(&fs_info->balance_pause_req) ||\n\t    atomic_read(&fs_info->balance_cancel_req)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tallowed = btrfs_super_incompat_flags(fs_info->super_copy);\n\tif (allowed & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)\n\t\tmixed = 1;\n\n\t/*\n\t * In case of mixed groups both data and meta should be picked,\n\t * and identical options should be given for both of them.\n\t */\n\tallowed = BTRFS_BALANCE_DATA | BTRFS_BALANCE_METADATA;\n\tif (mixed && (bctl->flags & allowed)) {\n\t\tif (!(bctl->flags & BTRFS_BALANCE_DATA) ||\n\t\t    !(bctl->flags & BTRFS_BALANCE_METADATA) ||\n\t\t    memcmp(&bctl->data, &bctl->meta, sizeof(bctl->data))) {\n\t\t\tbtrfs_err(fs_info,\n\t  \"balance: mixed groups data and metadata options must be the same\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tallowed = BTRFS_AVAIL_ALLOC_BIT_SINGLE | BTRFS_BLOCK_GROUP_DUP;\n\tif (num_devices > 1)\n\t\tallowed |= (BTRFS_BLOCK_GROUP_RAID0 | BTRFS_BLOCK_GROUP_RAID1);\n\tif (num_devices > 2)\n\t\tallowed |= BTRFS_BLOCK_GROUP_RAID5;\n\tif (num_devices > 3)\n\t\tallowed |= (BTRFS_BLOCK_GROUP_RAID10 |\n\t\t\t    BTRFS_BLOCK_GROUP_RAID6);\n\tif (validate_convert_profile(&bctl->data, allowed)) {\n\t\tint index = btrfs_bg_flags_to_raid_index(bctl->data.target);\n\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"balance: invalid convert data profile %s\",\n\t\t\t  get_raid_name(index));\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (validate_convert_profile(&bctl->meta, allowed)) {\n\t\tint index = btrfs_bg_flags_to_raid_index(bctl->meta.target);\n\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"balance: invalid convert metadata profile %s\",\n\t\t\t  get_raid_name(index));\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (validate_convert_profile(&bctl->sys, allowed)) {\n\t\tint index = btrfs_bg_flags_to_raid_index(bctl->sys.target);\n\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"balance: invalid convert system profile %s\",\n\t\t\t  get_raid_name(index));\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* allow to reduce meta or sys integrity only if force set */\n\tallowed = BTRFS_BLOCK_GROUP_DUP | BTRFS_BLOCK_GROUP_RAID1 |\n\t\t\tBTRFS_BLOCK_GROUP_RAID10 |\n\t\t\tBTRFS_BLOCK_GROUP_RAID5 |\n\t\t\tBTRFS_BLOCK_GROUP_RAID6;\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tif (((bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_system_alloc_bits & allowed) &&\n\t\t     !(bctl->sys.target & allowed)) ||\n\t\t    ((bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_metadata_alloc_bits & allowed) &&\n\t\t     !(bctl->meta.target & allowed)))\n\t\t\treducing_integrity = true;\n\t\telse\n\t\t\treducing_integrity = false;\n\n\t\t/* if we're not converting, the target field is uninitialized */\n\t\tmeta_target = (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->meta.target : fs_info->avail_metadata_alloc_bits;\n\t\tdata_target = (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->data.target : fs_info->avail_data_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tif (reducing_integrity) {\n\t\tif (bctl->flags & BTRFS_BALANCE_FORCE) {\n\t\t\tbtrfs_info(fs_info,\n\t\t\t\t   \"balance: force reducing metadata integrity\");\n\t\t} else {\n\t\t\tbtrfs_err(fs_info,\n\t  \"balance: reduces metadata integrity, use --force if you want this\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (btrfs_get_num_tolerated_disk_barrier_failures(meta_target) <\n\t\tbtrfs_get_num_tolerated_disk_barrier_failures(data_target)) {\n\t\tint meta_index = btrfs_bg_flags_to_raid_index(meta_target);\n\t\tint data_index = btrfs_bg_flags_to_raid_index(data_target);\n\n\t\tbtrfs_warn(fs_info,\n\t\"balance: metadata profile %s has lower redundancy than data profile %s\",\n\t\t\t   get_raid_name(meta_index), get_raid_name(data_index));\n\t}\n\n\tret = insert_balance_item(fs_info, bctl);\n\tif (ret && ret != -EEXIST)\n\t\tgoto out;\n\n\tif (!(bctl->flags & BTRFS_BALANCE_RESUME)) {\n\t\tBUG_ON(ret == -EEXIST);\n\t\tBUG_ON(fs_info->balance_ctl);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tfs_info->balance_ctl = bctl;\n\t\tspin_unlock(&fs_info->balance_lock);\n\t} else {\n\t\tBUG_ON(ret != -EEXIST);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tupdate_balance_args(bctl);\n\t\tspin_unlock(&fs_info->balance_lock);\n\t}\n\n\tASSERT(!test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tset_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\tdescribe_balance_start_or_resume(fs_info);\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tret = __btrfs_balance(fs_info);\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (ret == -ECANCELED && atomic_read(&fs_info->balance_pause_req))\n\t\tbtrfs_info(fs_info, \"balance: paused\");\n\telse if (ret == -ECANCELED && atomic_read(&fs_info->balance_cancel_req))\n\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\telse\n\t\tbtrfs_info(fs_info, \"balance: ended with status: %d\", ret);\n\n\tclear_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\n\tif (bargs) {\n\t\tmemset(bargs, 0, sizeof(*bargs));\n\t\tbtrfs_update_ioctl_balance_args(fs_info, bargs);\n\t}\n\n\tif ((ret && ret != -ECANCELED && ret != -ENOSPC) ||\n\t    balance_need_close(fs_info)) {\n\t\treset_balance_state(fs_info);\n\t\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\t}\n\n\twake_up(&fs_info->balance_wait_q);\n\n\treturn ret;\nout:\n\tif (bctl->flags & BTRFS_BALANCE_RESUME)\n\t\treset_balance_state(fs_info);\n\telse\n\t\tkfree(bctl);\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\n\treturn ret;\n}\n\nstatic int balance_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tint ret = 0;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (fs_info->balance_ctl)\n\t\tret = btrfs_balance(fs_info, fs_info->balance_ctl, NULL);\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\treturn ret;\n}\n\nint btrfs_resume_balance_async(struct btrfs_fs_info *fs_info)\n{\n\tstruct task_struct *tsk;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn 0;\n\t}\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tif (btrfs_test_opt(fs_info, SKIP_BALANCE)) {\n\t\tbtrfs_info(fs_info, \"balance: resume skipped\");\n\t\treturn 0;\n\t}\n\n\t/*\n\t * A ro->rw remount sequence should continue with the paused balance\n\t * regardless of who pauses it, system or the user as of now, so set\n\t * the resume flag.\n\t */\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl->flags |= BTRFS_BALANCE_RESUME;\n\tspin_unlock(&fs_info->balance_lock);\n\n\ttsk = kthread_run(balance_kthread, fs_info, \"btrfs-balance\");\n\treturn PTR_ERR_OR_ZERO(tsk);\n}\n\nint btrfs_recover_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, fs_info->tree_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) { /* ret = -ENOENT; */\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tbctl = kzalloc(sizeof(*bctl), GFP_NOFS);\n\tif (!bctl) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tbctl->flags = btrfs_balance_flags(leaf, item);\n\tbctl->flags |= BTRFS_BALANCE_RESUME;\n\n\tbtrfs_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->data, &disk_bargs);\n\tbtrfs_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->meta, &disk_bargs);\n\tbtrfs_balance_sys(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->sys, &disk_bargs);\n\n\t/*\n\t * This should never happen, as the paused balance state is recovered\n\t * during mount without any chance of other exclusive ops to collide.\n\t *\n\t * This gives the exclusive op status to balance and keeps in paused\n\t * state until user intervention (cancel or umount). If the ownership\n\t * cannot be assigned, show a message but do not fail. The balance\n\t * is in a paused state and must have fs_info::balance_ctl properly\n\t * set up.\n\t */\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags))\n\t\tbtrfs_warn(fs_info,\n\t\"balance: cannot set exclusive op status, resume manually\");\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tBUG_ON(fs_info->balance_ctl);\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = bctl;\n\tspin_unlock(&fs_info->balance_lock);\n\tmutex_unlock(&fs_info->balance_mutex);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_pause_balance(struct btrfs_fs_info *fs_info)\n{\n\tint ret = 0;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tatomic_inc(&fs_info->balance_pause_req);\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t\t/* we are good with balance_ctl ripped off from under us */\n\t\tBUG_ON(test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tatomic_dec(&fs_info->balance_pause_req);\n\t} else {\n\t\tret = -ENOTCONN;\n\t}\n\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn ret;\n}\n\nint btrfs_cancel_balance(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\t/*\n\t * A paused balance with the item stored on disk can be resumed at\n\t * mount time if the mount is read-write. Otherwise it's still paused\n\t * and we must not allow cancelling as it deletes the item.\n\t */\n\tif (sb_rdonly(fs_info->sb)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -EROFS;\n\t}\n\n\tatomic_inc(&fs_info->balance_cancel_req);\n\t/*\n\t * if we are running just wait and return, balance item is\n\t * deleted in btrfs_balance in this case\n\t */\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t} else {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t/*\n\t\t * Lock released to allow other waiters to continue, we'll\n\t\t * reexamine the status again.\n\t\t */\n\t\tmutex_lock(&fs_info->balance_mutex);\n\n\t\tif (fs_info->balance_ctl) {\n\t\t\treset_balance_state(fs_info);\n\t\t\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\t\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\t\t}\n\t}\n\n\tBUG_ON(fs_info->balance_ctl ||\n\t\ttest_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tatomic_dec(&fs_info->balance_cancel_req);\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn 0;\n}\n\nstatic int btrfs_uuid_scan_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path = NULL;\n\tint ret = 0;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tstruct btrfs_root_item root_item;\n\tu32 item_size;\n\tstruct btrfs_trans_handle *trans = NULL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = 0;\n\n\twhile (1) {\n\t\tret = btrfs_search_forward(root, &key, path,\n\t\t\t\tBTRFS_OLDEST_GENERATION);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (key.type != BTRFS_ROOT_ITEM_KEY ||\n\t\t    (key.objectid < BTRFS_FIRST_FREE_OBJECTID &&\n\t\t     key.objectid != BTRFS_FS_TREE_OBJECTID) ||\n\t\t    key.objectid > BTRFS_LAST_FREE_OBJECTID)\n\t\t\tgoto skip;\n\n\t\teb = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\titem_size = btrfs_item_size_nr(eb, slot);\n\t\tif (item_size < sizeof(root_item))\n\t\t\tgoto skip;\n\n\t\tread_extent_buffer(eb, &root_item,\n\t\t\t\t   btrfs_item_ptr_offset(eb, slot),\n\t\t\t\t   (int)sizeof(root_item));\n\t\tif (btrfs_root_refs(&root_item) == 0)\n\t\t\tgoto skip;\n\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid) ||\n\t\t    !btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tif (trans)\n\t\t\t\tgoto update_tree;\n\n\t\t\tbtrfs_release_path(path);\n\t\t\t/*\n\t\t\t * 1 - subvol uuid item\n\t\t\t * 1 - received_subvol uuid item\n\t\t\t */\n\t\t\ttrans = btrfs_start_transaction(fs_info->uuid_root, 2);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tgoto skip;\n\t\t}\nupdate_tree:\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans, root_item.uuid,\n\t\t\t\t\t\t  BTRFS_UUID_KEY_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans,\n\t\t\t\t\t\t  root_item.received_uuid,\n\t\t\t\t\t\t BTRFS_UUID_KEY_RECEIVED_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\nskip:\n\t\tif (trans) {\n\t\t\tret = btrfs_end_transaction(trans);\n\t\t\ttrans = NULL;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tbtrfs_release_path(path);\n\t\tif (key.offset < (u64)-1) {\n\t\t\tkey.offset++;\n\t\t} else if (key.type < BTRFS_ROOT_ITEM_KEY) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t} else if (key.objectid < (u64)-1) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t\tkey.objectid++;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tif (trans && !IS_ERR(trans))\n\t\tbtrfs_end_transaction(trans);\n\tif (ret)\n\t\tbtrfs_warn(fs_info, \"btrfs_uuid_scan_kthread failed %d\", ret);\n\telse\n\t\tset_bit(BTRFS_FS_UPDATE_UUID_TREE_GEN, &fs_info->flags);\n\tup(&fs_info->uuid_tree_rescan_sem);\n\treturn 0;\n}\n\n/*\n * Callback for btrfs_uuid_tree_iterate().\n * returns:\n * 0\tcheck succeeded, the entry is not outdated.\n * < 0\tif an error occurred.\n * > 0\tif the check failed, which means the caller shall remove the entry.\n */\nstatic int btrfs_check_uuid_tree_entry(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u8 *uuid, u8 type, u64 subid)\n{\n\tstruct btrfs_key key;\n\tint ret = 0;\n\tstruct btrfs_root *subvol_root;\n\n\tif (type != BTRFS_UUID_KEY_SUBVOL &&\n\t    type != BTRFS_UUID_KEY_RECEIVED_SUBVOL)\n\t\tgoto out;\n\n\tkey.objectid = subid;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\tsubvol_root = btrfs_read_fs_root_no_name(fs_info, &key);\n\tif (IS_ERR(subvol_root)) {\n\t\tret = PTR_ERR(subvol_root);\n\t\tif (ret == -ENOENT)\n\t\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase BTRFS_UUID_KEY_SUBVOL:\n\t\tif (memcmp(uuid, subvol_root->root_item.uuid, BTRFS_UUID_SIZE))\n\t\t\tret = 1;\n\t\tbreak;\n\tcase BTRFS_UUID_KEY_RECEIVED_SUBVOL:\n\t\tif (memcmp(uuid, subvol_root->root_item.received_uuid,\n\t\t\t   BTRFS_UUID_SIZE))\n\t\t\tret = 1;\n\t\tbreak;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic int btrfs_uuid_rescan_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = (struct btrfs_fs_info *)data;\n\tint ret;\n\n\t/*\n\t * 1st step is to iterate through the existing UUID tree and\n\t * to delete all entries that contain outdated data.\n\t * 2nd step is to add all missing entries to the UUID tree.\n\t */\n\tret = btrfs_uuid_tree_iterate(fs_info, btrfs_check_uuid_tree_entry);\n\tif (ret < 0) {\n\t\tbtrfs_warn(fs_info, \"iterating uuid_tree failed %d\", ret);\n\t\tup(&fs_info->uuid_tree_rescan_sem);\n\t\treturn ret;\n\t}\n\treturn btrfs_uuid_scan_kthread(data);\n}\n\nint btrfs_create_uuid_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *tree_root = fs_info->tree_root;\n\tstruct btrfs_root *uuid_root;\n\tstruct task_struct *task;\n\tint ret;\n\n\t/*\n\t * 1 - root node\n\t * 1 - root item\n\t */\n\ttrans = btrfs_start_transaction(tree_root, 2);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tuuid_root = btrfs_create_tree(trans, fs_info,\n\t\t\t\t      BTRFS_UUID_TREE_OBJECTID);\n\tif (IS_ERR(uuid_root)) {\n\t\tret = PTR_ERR(uuid_root);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\treturn ret;\n\t}\n\n\tfs_info->uuid_root = uuid_root;\n\n\tret = btrfs_commit_transaction(trans);\n\tif (ret)\n\t\treturn ret;\n\n\tdown(&fs_info->uuid_tree_rescan_sem);\n\ttask = kthread_run(btrfs_uuid_scan_kthread, fs_info, \"btrfs-uuid\");\n\tif (IS_ERR(task)) {\n\t\t/* fs_info->update_uuid_tree_gen remains 0 in all error case */\n\t\tbtrfs_warn(fs_info, \"failed to start uuid_scan task\");\n\t\tup(&fs_info->uuid_tree_rescan_sem);\n\t\treturn PTR_ERR(task);\n\t}\n\n\treturn 0;\n}\n\nint btrfs_check_uuid_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct task_struct *task;\n\n\tdown(&fs_info->uuid_tree_rescan_sem);\n\ttask = kthread_run(btrfs_uuid_rescan_kthread, fs_info, \"btrfs-uuid\");\n\tif (IS_ERR(task)) {\n\t\t/* fs_info->update_uuid_tree_gen remains 0 in all error case */\n\t\tbtrfs_warn(fs_info, \"failed to start uuid_rescan task\");\n\t\tup(&fs_info->uuid_tree_rescan_sem);\n\t\treturn PTR_ERR(task);\n\t}\n\n\treturn 0;\n}\n\n/*\n * shrinking a device means finding all of the device extents past\n * the new size, and then following the back refs to the chunks.\n * The chunk relocation code actually frees the device extent\n */\nint btrfs_shrink_device(struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_dev_extent *dev_extent = NULL;\n\tstruct btrfs_path *path;\n\tu64 length;\n\tu64 chunk_offset;\n\tint ret;\n\tint slot;\n\tint failed = 0;\n\tbool retried = false;\n\tbool checked_pending_chunks = false;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tu64 old_total = btrfs_super_total_bytes(super_copy);\n\tu64 old_size = btrfs_device_get_total_bytes(device);\n\tu64 diff;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\tdiff = round_down(old_size - new_size, fs_info->sectorsize);\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\treturn -EINVAL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_BACK;\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes -= diff;\n\t\tatomic64_sub(diff, &fs_info->free_chunk_space);\n\t}\n\tmutex_unlock(&fs_info->chunk_mutex);\n\nagain:\n\tkey.objectid = device->devid;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tdo {\n\t\tmutex_lock(&fs_info->delete_unused_bgs_mutex);\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_previous_item(root, path, 0, key.type);\n\t\tif (ret)\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\tif (ret < 0)\n\t\t\tgoto done;\n\t\tif (ret) {\n\t\t\tret = 0;\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(l, &key, path->slots[0]);\n\n\t\tif (key.objectid != device->devid) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\tlength = btrfs_dev_extent_length(l, dev_extent);\n\n\t\tif (key.offset + length <= new_size) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\n\t\tbtrfs_release_path(path);\n\n\t\t/*\n\t\t * We may be relocating the only data chunk we have,\n\t\t * which could potentially end up with losing data's\n\t\t * raid profile, so lets allocate an empty one in\n\t\t * advance.\n\t\t */\n\t\tret = btrfs_may_alloc_data_chunk(fs_info, chunk_offset);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, chunk_offset);\n\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\tif (ret == -ENOSPC) {\n\t\t\tfailed++;\n\t\t} else if (ret) {\n\t\t\tif (ret == -ETXTBSY) {\n\t\t\t\tbtrfs_warn(fs_info,\n\t\t   \"could not shrink block group %llu due to active swapfile\",\n\t\t\t\t\t   chunk_offset);\n\t\t\t}\n\t\t\tgoto done;\n\t\t}\n\t} while (key.offset-- > 0);\n\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (failed && retried) {\n\t\tret = -ENOSPC;\n\t\tgoto done;\n\t}\n\n\t/* Shrinking succeeded, else we would be at \"done\". */\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto done;\n\t}\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\t/*\n\t * We checked in the above loop all device extents that were already in\n\t * the device tree. However before we have updated the device's\n\t * total_bytes to the new size, we might have had chunk allocations that\n\t * have not complete yet (new block groups attached to transaction\n\t * handles), and therefore their device extents were not yet in the\n\t * device tree and we missed them in the loop above. So if we have any\n\t * pending chunk using a device extent that overlaps the device range\n\t * that we can not use anymore, commit the current transaction and\n\t * repeat the search on the device tree - this way we guarantee we will\n\t * not have chunks using device extents that end beyond 'new_size'.\n\t */\n\tif (!checked_pending_chunks) {\n\t\tu64 start = new_size;\n\t\tu64 len = old_size - new_size;\n\n\t\tif (contains_pending_extent(trans->transaction, device,\n\t\t\t\t\t    &start, len)) {\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t\tchecked_pending_chunks = true;\n\t\t\tfailed = 0;\n\t\t\tretried = false;\n\t\t\tret = btrfs_commit_transaction(trans);\n\t\t\tif (ret)\n\t\t\t\tgoto done;\n\t\t\tgoto again;\n\t\t}\n\t}\n\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tif (list_empty(&device->resized_list))\n\t\tlist_add_tail(&device->resized_list,\n\t\t\t      &fs_info->fs_devices->resized_devices);\n\n\tWARN_ON(diff > old_total);\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total - diff, fs_info->sectorsize));\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t/* Now btrfs_update_device() will change the on-disk size. */\n\tret = btrfs_update_device(trans, device);\n\tif (ret < 0) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t} else {\n\t\tret = btrfs_commit_transaction(trans);\n\t}\ndone:\n\tbtrfs_free_path(path);\n\tif (ret) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tbtrfs_device_set_total_bytes(device, old_size);\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\t\tdevice->fs_devices->total_rw_bytes += diff;\n\t\tatomic64_add(diff, &fs_info->free_chunk_space);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_add_system_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t   struct btrfs_key *key,\n\t\t\t   struct btrfs_chunk *chunk, int item_size)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key disk_key;\n\tu32 array_size;\n\tu8 *ptr;\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\tif (array_size + item_size + sizeof(disk_key)\n\t\t\t> BTRFS_SYSTEM_CHUNK_ARRAY_SIZE) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\treturn -EFBIG;\n\t}\n\n\tptr = super_copy->sys_chunk_array + array_size;\n\tbtrfs_cpu_key_to_disk(&disk_key, key);\n\tmemcpy(ptr, &disk_key, sizeof(disk_key));\n\tptr += sizeof(disk_key);\n\tmemcpy(ptr, chunk, item_size);\n\titem_size += sizeof(disk_key);\n\tbtrfs_set_super_sys_array_size(super_copy, array_size + item_size);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\treturn 0;\n}\n\n/*\n * sort the devices in descending order by max_avail, total_avail\n */\nstatic int btrfs_cmp_device_info(const void *a, const void *b)\n{\n\tconst struct btrfs_device_info *di_a = a;\n\tconst struct btrfs_device_info *di_b = b;\n\n\tif (di_a->max_avail > di_b->max_avail)\n\t\treturn -1;\n\tif (di_a->max_avail < di_b->max_avail)\n\t\treturn 1;\n\tif (di_a->total_avail > di_b->total_avail)\n\t\treturn -1;\n\tif (di_a->total_avail < di_b->total_avail)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void check_raid56_incompat_flag(struct btrfs_fs_info *info, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\treturn;\n\n\tbtrfs_set_fs_incompat(info, RAID56);\n}\n\n#define BTRFS_MAX_DEVS(info) ((BTRFS_MAX_ITEM_SIZE(info)\t\\\n\t\t\t- sizeof(struct btrfs_chunk))\t\t\\\n\t\t\t/ sizeof(struct btrfs_stripe) + 1)\n\n#define BTRFS_MAX_DEVS_SYS_CHUNK ((BTRFS_SYSTEM_CHUNK_ARRAY_SIZE\t\\\n\t\t\t\t- 2 * sizeof(struct btrfs_disk_key)\t\\\n\t\t\t\t- 2 * sizeof(struct btrfs_chunk))\t\\\n\t\t\t\t/ sizeof(struct btrfs_stripe) + 1)\n\nstatic int __btrfs_alloc_chunk(struct btrfs_trans_handle *trans,\n\t\t\t       u64 start, u64 type)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = info->fs_devices;\n\tstruct btrfs_device *device;\n\tstruct map_lookup *map = NULL;\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_device_info *devices_info = NULL;\n\tu64 total_avail;\n\tint num_stripes;\t/* total number of stripes to allocate */\n\tint data_stripes;\t/* number of stripes that count for\n\t\t\t\t   block group size */\n\tint sub_stripes;\t/* sub_stripes info for map */\n\tint dev_stripes;\t/* stripes per dev */\n\tint devs_max;\t\t/* max devs to use */\n\tint devs_min;\t\t/* min devs needed */\n\tint devs_increment;\t/* ndevs has to be a multiple of this */\n\tint ncopies;\t\t/* how many copies to data has */\n\tint nparity;\t\t/* number of stripes worth of bytes to\n\t\t\t\t   store parity information */\n\tint ret;\n\tu64 max_stripe_size;\n\tu64 max_chunk_size;\n\tu64 stripe_size;\n\tu64 chunk_size;\n\tint ndevs;\n\tint i;\n\tint j;\n\tint index;\n\n\tBUG_ON(!alloc_profile_is_valid(type, 0));\n\n\tif (list_empty(&fs_devices->alloc_list)) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\tbtrfs_debug(info, \"%s: no writable device\", __func__);\n\t\treturn -ENOSPC;\n\t}\n\n\tindex = btrfs_bg_flags_to_raid_index(type);\n\n\tsub_stripes = btrfs_raid_array[index].sub_stripes;\n\tdev_stripes = btrfs_raid_array[index].dev_stripes;\n\tdevs_max = btrfs_raid_array[index].devs_max;\n\tdevs_min = btrfs_raid_array[index].devs_min;\n\tdevs_increment = btrfs_raid_array[index].devs_increment;\n\tncopies = btrfs_raid_array[index].ncopies;\n\tnparity = btrfs_raid_array[index].nparity;\n\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tmax_stripe_size = SZ_1G;\n\t\tmax_chunk_size = BTRFS_MAX_DATA_CHUNK_SIZE;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\t/* for larger filesystems, use larger metadata chunks */\n\t\tif (fs_devices->total_rw_bytes > 50ULL * SZ_1G)\n\t\t\tmax_stripe_size = SZ_1G;\n\t\telse\n\t\t\tmax_stripe_size = SZ_256M;\n\t\tmax_chunk_size = max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tmax_stripe_size = SZ_32M;\n\t\tmax_chunk_size = 2 * max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS_SYS_CHUNK;\n\t} else {\n\t\tbtrfs_err(info, \"invalid chunk type 0x%llx requested\",\n\t\t       type);\n\t\tBUG_ON(1);\n\t}\n\n\t/* We don't want a chunk larger than 10% of writable space */\n\tmax_chunk_size = min(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t     max_chunk_size);\n\n\tdevices_info = kcalloc(fs_devices->rw_devices, sizeof(*devices_info),\n\t\t\t       GFP_NOFS);\n\tif (!devices_info)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * in the first pass through the devices list, we gather information\n\t * about the available holes on each device.\n\t */\n\tndevs = 0;\n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tu64 max_avail;\n\t\tu64 dev_offset;\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tWARN(1, KERN_ERR\n\t\t\t       \"BTRFS: read-only device in alloc_list\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t&device->dev_state) ||\n\t\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tif (device->total_bytes > device->bytes_used)\n\t\t\ttotal_avail = device->total_bytes - device->bytes_used;\n\t\telse\n\t\t\ttotal_avail = 0;\n\n\t\t/* If there is no space on this device, skip it. */\n\t\tif (total_avail == 0)\n\t\t\tcontinue;\n\n\t\tret = find_free_dev_extent(trans, device,\n\t\t\t\t\t   max_stripe_size * dev_stripes,\n\t\t\t\t\t   &dev_offset, &max_avail);\n\t\tif (ret && ret != -ENOSPC)\n\t\t\tgoto error;\n\n\t\tif (ret == 0)\n\t\t\tmax_avail = max_stripe_size * dev_stripes;\n\n\t\tif (max_avail < BTRFS_STRIPE_LEN * dev_stripes) {\n\t\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\t\tbtrfs_debug(info,\n\t\t\t\"%s: devid %llu has no free space, have=%llu want=%u\",\n\t\t\t\t\t    __func__, device->devid, max_avail,\n\t\t\t\t\t    BTRFS_STRIPE_LEN * dev_stripes);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ndevs == fs_devices->rw_devices) {\n\t\t\tWARN(1, \"%s: found more than %llu devices\\n\",\n\t\t\t     __func__, fs_devices->rw_devices);\n\t\t\tbreak;\n\t\t}\n\t\tdevices_info[ndevs].dev_offset = dev_offset;\n\t\tdevices_info[ndevs].max_avail = max_avail;\n\t\tdevices_info[ndevs].total_avail = total_avail;\n\t\tdevices_info[ndevs].dev = device;\n\t\t++ndevs;\n\t}\n\n\t/*\n\t * now sort the devices by hole size / available space\n\t */\n\tsort(devices_info, ndevs, sizeof(struct btrfs_device_info),\n\t     btrfs_cmp_device_info, NULL);\n\n\t/* round down to number of usable stripes */\n\tndevs = round_down(ndevs, devs_increment);\n\n\tif (ndevs < devs_min) {\n\t\tret = -ENOSPC;\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG)) {\n\t\t\tbtrfs_debug(info,\n\t\"%s: not enough devices with free space: have=%d minimum required=%d\",\n\t\t\t\t    __func__, ndevs, devs_min);\n\t\t}\n\t\tgoto error;\n\t}\n\n\tndevs = min(ndevs, devs_max);\n\n\t/*\n\t * The primary goal is to maximize the number of stripes, so use as\n\t * many devices as possible, even if the stripes are not maximum sized.\n\t *\n\t * The DUP profile stores more than one stripe per device, the\n\t * max_avail is the total size so we have to adjust.\n\t */\n\tstripe_size = div_u64(devices_info[ndevs - 1].max_avail, dev_stripes);\n\tnum_stripes = ndevs * dev_stripes;\n\n\t/*\n\t * this will have to be fixed for RAID1 and RAID10 over\n\t * more drives\n\t */\n\tdata_stripes = (num_stripes - nparity) / ncopies;\n\n\t/*\n\t * Use the number of data stripes to figure out how big this chunk\n\t * is really going to be in terms of logical address space,\n\t * and compare that answer with the max chunk size. If it's higher,\n\t * we try to reduce stripe_size.\n\t */\n\tif (stripe_size * data_stripes > max_chunk_size) {\n\t\t/*\n\t\t * Reduce stripe_size, round it up to a 16MB boundary again and\n\t\t * then use it, unless it ends up being even bigger than the\n\t\t * previous value we had already.\n\t\t */\n\t\tstripe_size = min(round_up(div_u64(max_chunk_size,\n\t\t\t\t\t\t   data_stripes), SZ_16M),\n\t\t\t\t  stripe_size);\n\t}\n\n\t/* align to BTRFS_STRIPE_LEN */\n\tstripe_size = round_down(stripe_size, BTRFS_STRIPE_LEN);\n\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tmap->num_stripes = num_stripes;\n\n\tfor (i = 0; i < ndevs; ++i) {\n\t\tfor (j = 0; j < dev_stripes; ++j) {\n\t\t\tint s = i * dev_stripes + j;\n\t\t\tmap->stripes[s].dev = devices_info[i].dev;\n\t\t\tmap->stripes[s].physical = devices_info[i].dev_offset +\n\t\t\t\t\t\t   j * stripe_size;\n\t\t}\n\t}\n\tmap->stripe_len = BTRFS_STRIPE_LEN;\n\tmap->io_align = BTRFS_STRIPE_LEN;\n\tmap->io_width = BTRFS_STRIPE_LEN;\n\tmap->type = type;\n\tmap->sub_stripes = sub_stripes;\n\n\tchunk_size = stripe_size * data_stripes;\n\n\ttrace_btrfs_chunk_alloc(info, map, start, chunk_size);\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tkfree(map);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = start;\n\tem->len = chunk_size;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\tem->orig_block_len = stripe_size;\n\n\tem_tree = &info->mapping_tree.map_tree;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em, 0);\n\tif (ret) {\n\t\twrite_unlock(&em_tree->lock);\n\t\tfree_extent_map(em);\n\t\tgoto error;\n\t}\n\n\tlist_add_tail(&em->list, &trans->transaction->pending_chunks);\n\trefcount_inc(&em->refs);\n\twrite_unlock(&em_tree->lock);\n\n\tret = btrfs_make_block_group(trans, 0, type, start, chunk_size);\n\tif (ret)\n\t\tgoto error_del_extent;\n\n\tfor (i = 0; i < map->num_stripes; i++)\n\t\tbtrfs_device_set_bytes_used(map->stripes[i].dev,\n\t\t\t\tmap->stripes[i].dev->bytes_used + stripe_size);\n\n\tatomic64_sub(stripe_size * map->num_stripes, &info->free_chunk_space);\n\n\tfree_extent_map(em);\n\tcheck_raid56_incompat_flag(info, type);\n\n\tkfree(devices_info);\n\treturn 0;\n\nerror_del_extent:\n\twrite_lock(&em_tree->lock);\n\tremove_extent_mapping(em_tree, em);\n\twrite_unlock(&em_tree->lock);\n\n\t/* One for our allocation */\n\tfree_extent_map(em);\n\t/* One for the tree reference */\n\tfree_extent_map(em);\n\t/* One for the pending_chunks list reference */\n\tfree_extent_map(em);\nerror:\n\tkfree(devices_info);\n\treturn ret;\n}\n\nint btrfs_finish_chunk_alloc(struct btrfs_trans_handle *trans,\n\t\t\t     u64 chunk_offset, u64 chunk_size)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *extent_root = fs_info->extent_root;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_device *device;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_stripe *stripe;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tsize_t item_size;\n\tu64 dev_offset;\n\tu64 stripe_size;\n\tint i = 0;\n\tint ret = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, chunk_size);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\titem_size = btrfs_chunk_item_size(map->num_stripes);\n\tstripe_size = em->orig_block_len;\n\n\tchunk = kzalloc(item_size, GFP_NOFS);\n\tif (!chunk) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Take the device list mutex to prevent races with the final phase of\n\t * a device replace operation that replaces the device object associated\n\t * with the map's stripes, because the device object's id can change\n\t * at any time during that final phase of the device replace operation\n\t * (dev-replace.c:btrfs_dev_replace_finishing()).\n\t */\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tdevice = map->stripes[i].dev;\n\t\tdev_offset = map->stripes[i].physical;\n\n\t\tret = btrfs_update_device(trans, device);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = btrfs_alloc_dev_extent(trans, device, chunk_offset,\n\t\t\t\t\t     dev_offset, stripe_size);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tif (ret) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tgoto out;\n\t}\n\n\tstripe = &chunk->stripe;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tdevice = map->stripes[i].dev;\n\t\tdev_offset = map->stripes[i].physical;\n\n\t\tbtrfs_set_stack_stripe_devid(stripe, device->devid);\n\t\tbtrfs_set_stack_stripe_offset(stripe, dev_offset);\n\t\tmemcpy(stripe->dev_uuid, device->uuid, BTRFS_UUID_SIZE);\n\t\tstripe++;\n\t}\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\tbtrfs_set_stack_chunk_length(chunk, chunk_size);\n\tbtrfs_set_stack_chunk_owner(chunk, extent_root->root_key.objectid);\n\tbtrfs_set_stack_chunk_stripe_len(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_type(chunk, map->type);\n\tbtrfs_set_stack_chunk_num_stripes(chunk, map->num_stripes);\n\tbtrfs_set_stack_chunk_io_align(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_io_width(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_sector_size(chunk, fs_info->sectorsize);\n\tbtrfs_set_stack_chunk_sub_stripes(chunk, map->sub_stripes);\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\tkey.offset = chunk_offset;\n\n\tret = btrfs_insert_item(trans, chunk_root, &key, chunk, item_size);\n\tif (ret == 0 && map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\t/*\n\t\t * TODO: Cleanup of inserted chunk root in case of\n\t\t * failure.\n\t\t */\n\t\tret = btrfs_add_system_chunk(fs_info, &key, chunk, item_size);\n\t}\n\nout:\n\tkfree(chunk);\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * Chunk allocation falls into two parts. The first part does work\n * that makes the new allocated chunk usable, but does not do any operation\n * that modifies the chunk tree. The second part does the work that\n * requires modifying the chunk tree. This division is important for the\n * bootstrap process of adding storage to a seed btrfs.\n */\nint btrfs_alloc_chunk(struct btrfs_trans_handle *trans, u64 type)\n{\n\tu64 chunk_offset;\n\n\tlockdep_assert_held(&trans->fs_info->chunk_mutex);\n\tchunk_offset = find_next_chunk(trans->fs_info);\n\treturn __btrfs_alloc_chunk(trans, chunk_offset, type);\n}\n\nstatic noinline int init_first_rw_device(struct btrfs_trans_handle *trans,\n\t\t\t\t\t struct btrfs_fs_info *fs_info)\n{\n\tu64 chunk_offset;\n\tu64 sys_chunk_offset;\n\tu64 alloc_profile;\n\tint ret;\n\n\tchunk_offset = find_next_chunk(fs_info);\n\talloc_profile = btrfs_metadata_alloc_profile(fs_info);\n\tret = __btrfs_alloc_chunk(trans, chunk_offset, alloc_profile);\n\tif (ret)\n\t\treturn ret;\n\n\tsys_chunk_offset = find_next_chunk(fs_info);\n\talloc_profile = btrfs_system_alloc_profile(fs_info);\n\tret = __btrfs_alloc_chunk(trans, sys_chunk_offset, alloc_profile);\n\treturn ret;\n}\n\nstatic inline int btrfs_chunk_max_errors(struct map_lookup *map)\n{\n\tint max_errors;\n\n\tif (map->type & (BTRFS_BLOCK_GROUP_RAID1 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID10 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID5 |\n\t\t\t BTRFS_BLOCK_GROUP_DUP)) {\n\t\tmax_errors = 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID6) {\n\t\tmax_errors = 2;\n\t} else {\n\t\tmax_errors = 0;\n\t}\n\n\treturn max_errors;\n}\n\nint btrfs_chunk_readonly(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint readonly = 0;\n\tint miss_ndevs = 0;\n\tint i;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em))\n\t\treturn 1;\n\n\tmap = em->map_lookup;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\tmiss_ndevs++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\treadonly = 1;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\t/*\n\t * If the number of missing devices is larger than max errors,\n\t * we can not write the data into that chunk successfully, so\n\t * set it readonly.\n\t */\n\tif (miss_ndevs > btrfs_chunk_max_errors(map))\n\t\treadonly = 1;\nend:\n\tfree_extent_map(em);\n\treturn readonly;\n}\n\nvoid btrfs_mapping_init(struct btrfs_mapping_tree *tree)\n{\n\textent_map_tree_init(&tree->map_tree);\n}\n\nvoid btrfs_mapping_tree_free(struct btrfs_mapping_tree *tree)\n{\n\tstruct extent_map *em;\n\n\twhile (1) {\n\t\twrite_lock(&tree->map_tree.lock);\n\t\tem = lookup_extent_mapping(&tree->map_tree, 0, (u64)-1);\n\t\tif (em)\n\t\t\tremove_extent_mapping(&tree->map_tree, em);\n\t\twrite_unlock(&tree->map_tree.lock);\n\t\tif (!em)\n\t\t\tbreak;\n\t\t/* once for us */\n\t\tfree_extent_map(em);\n\t\t/* once for the tree */\n\t\tfree_extent_map(em);\n\t}\n}\n\nint btrfs_num_copies(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint ret;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\tif (IS_ERR(em))\n\t\t/*\n\t\t * We could return errors for these cases, but that could get\n\t\t * ugly and we'd probably do the same thing which is just not do\n\t\t * anything else and exit, so return 1 so the callers don't try\n\t\t * to use other copies.\n\t\t */\n\t\treturn 1;\n\n\tmap = em->map_lookup;\n\tif (map->type & (BTRFS_BLOCK_GROUP_DUP | BTRFS_BLOCK_GROUP_RAID1))\n\t\tret = map->num_stripes;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tret = map->sub_stripes;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID5)\n\t\tret = 2;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID6)\n\t\t/*\n\t\t * There could be two corrupted data stripes, we need\n\t\t * to loop retry in order to rebuild the correct data.\n\t\t *\n\t\t * Fail a stripe at a time on every retry except the\n\t\t * stripe under reconstruction.\n\t\t */\n\t\tret = map->num_stripes;\n\telse\n\t\tret = 1;\n\tfree_extent_map(em);\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (btrfs_dev_replace_is_ongoing(&fs_info->dev_replace) &&\n\t    fs_info->dev_replace.tgtdev)\n\t\tret++;\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\treturn ret;\n}\n\nunsigned long btrfs_full_stripe_len(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 logical)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tunsigned long len = fs_info->sectorsize;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif (!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tlen = map->stripe_len * nr_data_stripes(map);\n\t\tfree_extent_map(em);\n\t}\n\treturn len;\n}\n\nint btrfs_is_parity_mirror(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint ret = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif(!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tret = 1;\n\t\tfree_extent_map(em);\n\t}\n\treturn ret;\n}\n\nstatic int find_live_mirror(struct btrfs_fs_info *fs_info,\n\t\t\t    struct map_lookup *map, int first,\n\t\t\t    int dev_replace_is_ongoing)\n{\n\tint i;\n\tint num_stripes;\n\tint preferred_mirror;\n\tint tolerance;\n\tstruct btrfs_device *srcdev;\n\n\tASSERT((map->type &\n\t\t (BTRFS_BLOCK_GROUP_RAID1 | BTRFS_BLOCK_GROUP_RAID10)));\n\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tnum_stripes = map->sub_stripes;\n\telse\n\t\tnum_stripes = map->num_stripes;\n\n\tpreferred_mirror = first + current->pid % num_stripes;\n\n\tif (dev_replace_is_ongoing &&\n\t    fs_info->dev_replace.cont_reading_from_srcdev_mode ==\n\t     BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_AVOID)\n\t\tsrcdev = fs_info->dev_replace.srcdev;\n\telse\n\t\tsrcdev = NULL;\n\n\t/*\n\t * try to avoid the drive that is the source drive for a\n\t * dev-replace procedure, only choose it if no other non-missing\n\t * mirror is available\n\t */\n\tfor (tolerance = 0; tolerance < 2; tolerance++) {\n\t\tif (map->stripes[preferred_mirror].dev->bdev &&\n\t\t    (tolerance || map->stripes[preferred_mirror].dev != srcdev))\n\t\t\treturn preferred_mirror;\n\t\tfor (i = first; i < first + num_stripes; i++) {\n\t\t\tif (map->stripes[i].dev->bdev &&\n\t\t\t    (tolerance || map->stripes[i].dev != srcdev))\n\t\t\t\treturn i;\n\t\t}\n\t}\n\n\t/* we couldn't find one that doesn't fail.  Just return something\n\t * and the io error handling code will clean up eventually\n\t */\n\treturn preferred_mirror;\n}\n\nstatic inline int parity_smaller(u64 a, u64 b)\n{\n\treturn a > b;\n}\n\n/* Bubble-sort the stripe set to put the parity/syndrome stripes last */\nstatic void sort_parity_stripes(struct btrfs_bio *bbio, int num_stripes)\n{\n\tstruct btrfs_bio_stripe s;\n\tint i;\n\tu64 l;\n\tint again = 1;\n\n\twhile (again) {\n\t\tagain = 0;\n\t\tfor (i = 0; i < num_stripes - 1; i++) {\n\t\t\tif (parity_smaller(bbio->raid_map[i],\n\t\t\t\t\t   bbio->raid_map[i+1])) {\n\t\t\t\ts = bbio->stripes[i];\n\t\t\t\tl = bbio->raid_map[i];\n\t\t\t\tbbio->stripes[i] = bbio->stripes[i+1];\n\t\t\t\tbbio->raid_map[i] = bbio->raid_map[i+1];\n\t\t\t\tbbio->stripes[i+1] = s;\n\t\t\t\tbbio->raid_map[i+1] = l;\n\n\t\t\t\tagain = 1;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic struct btrfs_bio *alloc_btrfs_bio(int total_stripes, int real_stripes)\n{\n\tstruct btrfs_bio *bbio = kzalloc(\n\t\t /* the size of the btrfs_bio */\n\t\tsizeof(struct btrfs_bio) +\n\t\t/* plus the variable array for the stripes */\n\t\tsizeof(struct btrfs_bio_stripe) * (total_stripes) +\n\t\t/* plus the variable array for the tgt dev */\n\t\tsizeof(int) * (real_stripes) +\n\t\t/*\n\t\t * plus the raid_map, which includes both the tgt dev\n\t\t * and the stripes\n\t\t */\n\t\tsizeof(u64) * (total_stripes),\n\t\tGFP_NOFS|__GFP_NOFAIL);\n\n\tatomic_set(&bbio->error, 0);\n\trefcount_set(&bbio->refs, 1);\n\n\treturn bbio;\n}\n\nvoid btrfs_get_bbio(struct btrfs_bio *bbio)\n{\n\tWARN_ON(!refcount_read(&bbio->refs));\n\trefcount_inc(&bbio->refs);\n}\n\nvoid btrfs_put_bbio(struct btrfs_bio *bbio)\n{\n\tif (!bbio)\n\t\treturn;\n\tif (refcount_dec_and_test(&bbio->refs))\n\t\tkfree(bbio);\n}\n\n/* can REQ_OP_DISCARD be sent with other REQ like REQ_OP_WRITE? */\n/*\n * Please note that, discard won't be sent to target device of device\n * replace.\n */\nstatic int __btrfs_map_block_for_discard(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t u64 logical, u64 length,\n\t\t\t\t\t struct btrfs_bio **bbio_ret)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_bio *bbio;\n\tu64 offset;\n\tu64 stripe_nr;\n\tu64 stripe_nr_end;\n\tu64 stripe_end_offset;\n\tu64 stripe_cnt;\n\tu64 stripe_len;\n\tu64 stripe_offset;\n\tu64 num_stripes;\n\tu32 stripe_index;\n\tu32 factor = 0;\n\tu32 sub_stripes = 0;\n\tu64 stripes_per_dev = 0;\n\tu32 remaining_stripes = 0;\n\tu32 last_stripe = 0;\n\tint ret = 0;\n\tint i;\n\n\t/* discard always return a bbio */\n\tASSERT(bbio_ret);\n\n\tem = btrfs_get_chunk_map(fs_info, logical, length);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\t/* we don't discard raid56 yet */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\toffset = logical - em->start;\n\tlength = min_t(u64, em->len - offset, length);\n\n\tstripe_len = map->stripe_len;\n\t/*\n\t * stripe_nr counts the total number of stripes we have to stride\n\t * to get to this block\n\t */\n\tstripe_nr = div64_u64(offset, stripe_len);\n\n\t/* stripe_offset is the offset of this block in its stripe */\n\tstripe_offset = offset - stripe_nr * stripe_len;\n\n\tstripe_nr_end = round_up(offset + length, map->stripe_len);\n\tstripe_nr_end = div64_u64(stripe_nr_end, map->stripe_len);\n\tstripe_cnt = stripe_nr_end - stripe_nr;\n\tstripe_end_offset = stripe_nr_end * map->stripe_len -\n\t\t\t    (offset + length);\n\t/*\n\t * after this, stripe_nr is the number of stripes on this\n\t * device we have to walk to find the data, and stripe_index is\n\t * the number of our device in the stripe array\n\t */\n\tnum_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID0)\n\t\t\tsub_stripes = 1;\n\t\telse\n\t\t\tsub_stripes = map->sub_stripes;\n\n\t\tfactor = map->num_stripes / sub_stripes;\n\t\tnum_stripes = min_t(u64, map->num_stripes,\n\t\t\t\t    sub_stripes * stripe_cnt);\n\t\tstripe_nr = div_u64_rem(stripe_nr, factor, &stripe_index);\n\t\tstripe_index *= sub_stripes;\n\t\tstripes_per_dev = div_u64_rem(stripe_cnt, factor,\n\t\t\t\t\t      &remaining_stripes);\n\t\tdiv_u64_rem(stripe_nr_end - 1, factor, &last_stripe);\n\t\tlast_stripe *= sub_stripes;\n\t} else if (map->type & (BTRFS_BLOCK_GROUP_RAID1 |\n\t\t\t\tBTRFS_BLOCK_GROUP_DUP)) {\n\t\tnum_stripes = map->num_stripes;\n\t} else {\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t\t&stripe_index);\n\t}\n\n\tbbio = alloc_btrfs_bio(num_stripes, 0);\n\tif (!bbio) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tbbio->stripes[i].physical =\n\t\t\tmap->stripes[stripe_index].physical +\n\t\t\tstripe_offset + stripe_nr * map->stripe_len;\n\t\tbbio->stripes[i].dev = map->stripes[stripe_index].dev;\n\n\t\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\t\tbbio->stripes[i].length = stripes_per_dev *\n\t\t\t\tmap->stripe_len;\n\n\t\t\tif (i / sub_stripes < remaining_stripes)\n\t\t\t\tbbio->stripes[i].length +=\n\t\t\t\t\tmap->stripe_len;\n\n\t\t\t/*\n\t\t\t * Special for the first stripe and\n\t\t\t * the last stripe:\n\t\t\t *\n\t\t\t * |-------|...|-------|\n\t\t\t *     |----------|\n\t\t\t *    off     end_off\n\t\t\t */\n\t\t\tif (i < sub_stripes)\n\t\t\t\tbbio->stripes[i].length -=\n\t\t\t\t\tstripe_offset;\n\n\t\t\tif (stripe_index >= last_stripe &&\n\t\t\t    stripe_index <= (last_stripe +\n\t\t\t\t\t     sub_stripes - 1))\n\t\t\t\tbbio->stripes[i].length -=\n\t\t\t\t\tstripe_end_offset;\n\n\t\t\tif (i == sub_stripes - 1)\n\t\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\tbbio->stripes[i].length = length;\n\t\t}\n\n\t\tstripe_index++;\n\t\tif (stripe_index == map->num_stripes) {\n\t\t\tstripe_index = 0;\n\t\t\tstripe_nr++;\n\t\t}\n\t}\n\n\t*bbio_ret = bbio;\n\tbbio->map_type = map->type;\n\tbbio->num_stripes = num_stripes;\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * In dev-replace case, for repair case (that's the only case where the mirror\n * is selected explicitly when calling btrfs_map_block), blocks left of the\n * left cursor can also be read from the target drive.\n *\n * For REQ_GET_READ_MIRRORS, the target drive is added as the last one to the\n * array of stripes.\n * For READ, it also needs to be supported using the same mirror number.\n *\n * If the requested block is not left of the left cursor, EIO is returned. This\n * can happen because btrfs_num_copies() returns one more in the dev-replace\n * case.\n */\nstatic int get_extra_mirror_from_replace(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t u64 logical, u64 length,\n\t\t\t\t\t u64 srcdev_devid, int *mirror_num,\n\t\t\t\t\t u64 *physical)\n{\n\tstruct btrfs_bio *bbio = NULL;\n\tint num_stripes;\n\tint index_srcdev = 0;\n\tint found = 0;\n\tu64 physical_of_found = 0;\n\tint i;\n\tint ret = 0;\n\n\tret = __btrfs_map_block(fs_info, BTRFS_MAP_GET_READ_MIRRORS,\n\t\t\t\tlogical, &length, &bbio, 0, 0);\n\tif (ret) {\n\t\tASSERT(bbio == NULL);\n\t\treturn ret;\n\t}\n\n\tnum_stripes = bbio->num_stripes;\n\tif (*mirror_num > num_stripes) {\n\t\t/*\n\t\t * BTRFS_MAP_GET_READ_MIRRORS does not contain this mirror,\n\t\t * that means that the requested area is not left of the left\n\t\t * cursor\n\t\t */\n\t\tbtrfs_put_bbio(bbio);\n\t\treturn -EIO;\n\t}\n\n\t/*\n\t * process the rest of the function using the mirror_num of the source\n\t * drive. Therefore look it up first.  At the end, patch the device\n\t * pointer to the one of the target drive.\n\t */\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tif (bbio->stripes[i].dev->devid != srcdev_devid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * In case of DUP, in order to keep it simple, only add the\n\t\t * mirror with the lowest physical address\n\t\t */\n\t\tif (found &&\n\t\t    physical_of_found <= bbio->stripes[i].physical)\n\t\t\tcontinue;\n\n\t\tindex_srcdev = i;\n\t\tfound = 1;\n\t\tphysical_of_found = bbio->stripes[i].physical;\n\t}\n\n\tbtrfs_put_bbio(bbio);\n\n\tASSERT(found);\n\tif (!found)\n\t\treturn -EIO;\n\n\t*mirror_num = index_srcdev + 1;\n\t*physical = physical_of_found;\n\treturn ret;\n}\n\nstatic void handle_ops_on_dev_replace(enum btrfs_map_op op,\n\t\t\t\t      struct btrfs_bio **bbio_ret,\n\t\t\t\t      struct btrfs_dev_replace *dev_replace,\n\t\t\t\t      int *num_stripes_ret, int *max_errors_ret)\n{\n\tstruct btrfs_bio *bbio = *bbio_ret;\n\tu64 srcdev_devid = dev_replace->srcdev->devid;\n\tint tgtdev_indexes = 0;\n\tint num_stripes = *num_stripes_ret;\n\tint max_errors = *max_errors_ret;\n\tint i;\n\n\tif (op == BTRFS_MAP_WRITE) {\n\t\tint index_where_to_add;\n\n\t\t/*\n\t\t * duplicate the write operations while the dev replace\n\t\t * procedure is running. Since the copying of the old disk to\n\t\t * the new disk takes place at run time while the filesystem is\n\t\t * mounted writable, the regular write operations to the old\n\t\t * disk have to be duplicated to go to the new disk as well.\n\t\t *\n\t\t * Note that device->missing is handled by the caller, and that\n\t\t * the write to the old disk is already set up in the stripes\n\t\t * array.\n\t\t */\n\t\tindex_where_to_add = num_stripes;\n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tif (bbio->stripes[i].dev->devid == srcdev_devid) {\n\t\t\t\t/* write to new disk, too */\n\t\t\t\tstruct btrfs_bio_stripe *new =\n\t\t\t\t\tbbio->stripes + index_where_to_add;\n\t\t\t\tstruct btrfs_bio_stripe *old =\n\t\t\t\t\tbbio->stripes + i;\n\n\t\t\t\tnew->physical = old->physical;\n\t\t\t\tnew->length = old->length;\n\t\t\t\tnew->dev = dev_replace->tgtdev;\n\t\t\t\tbbio->tgtdev_map[i] = index_where_to_add;\n\t\t\t\tindex_where_to_add++;\n\t\t\t\tmax_errors++;\n\t\t\t\ttgtdev_indexes++;\n\t\t\t}\n\t\t}\n\t\tnum_stripes = index_where_to_add;\n\t} else if (op == BTRFS_MAP_GET_READ_MIRRORS) {\n\t\tint index_srcdev = 0;\n\t\tint found = 0;\n\t\tu64 physical_of_found = 0;\n\n\t\t/*\n\t\t * During the dev-replace procedure, the target drive can also\n\t\t * be used to read data in case it is needed to repair a corrupt\n\t\t * block elsewhere. This is possible if the requested area is\n\t\t * left of the left cursor. In this area, the target drive is a\n\t\t * full copy of the source drive.\n\t\t */\n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tif (bbio->stripes[i].dev->devid == srcdev_devid) {\n\t\t\t\t/*\n\t\t\t\t * In case of DUP, in order to keep it simple,\n\t\t\t\t * only add the mirror with the lowest physical\n\t\t\t\t * address\n\t\t\t\t */\n\t\t\t\tif (found &&\n\t\t\t\t    physical_of_found <=\n\t\t\t\t     bbio->stripes[i].physical)\n\t\t\t\t\tcontinue;\n\t\t\t\tindex_srcdev = i;\n\t\t\t\tfound = 1;\n\t\t\t\tphysical_of_found = bbio->stripes[i].physical;\n\t\t\t}\n\t\t}\n\t\tif (found) {\n\t\t\tstruct btrfs_bio_stripe *tgtdev_stripe =\n\t\t\t\tbbio->stripes + num_stripes;\n\n\t\t\ttgtdev_stripe->physical = physical_of_found;\n\t\t\ttgtdev_stripe->length =\n\t\t\t\tbbio->stripes[index_srcdev].length;\n\t\t\ttgtdev_stripe->dev = dev_replace->tgtdev;\n\t\t\tbbio->tgtdev_map[index_srcdev] = num_stripes;\n\n\t\t\ttgtdev_indexes++;\n\t\t\tnum_stripes++;\n\t\t}\n\t}\n\n\t*num_stripes_ret = num_stripes;\n\t*max_errors_ret = max_errors;\n\tbbio->num_tgtdevs = tgtdev_indexes;\n\t*bbio_ret = bbio;\n}\n\nstatic bool need_full_stripe(enum btrfs_map_op op)\n{\n\treturn (op == BTRFS_MAP_WRITE || op == BTRFS_MAP_GET_READ_MIRRORS);\n}\n\nstatic int __btrfs_map_block(struct btrfs_fs_info *fs_info,\n\t\t\t     enum btrfs_map_op op,\n\t\t\t     u64 logical, u64 *length,\n\t\t\t     struct btrfs_bio **bbio_ret,\n\t\t\t     int mirror_num, int need_raid_map)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 offset;\n\tu64 stripe_offset;\n\tu64 stripe_nr;\n\tu64 stripe_len;\n\tu32 stripe_index;\n\tint i;\n\tint ret = 0;\n\tint num_stripes;\n\tint max_errors = 0;\n\tint tgtdev_indexes = 0;\n\tstruct btrfs_bio *bbio = NULL;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tint dev_replace_is_ongoing = 0;\n\tint num_alloc_stripes;\n\tint patch_the_first_stripe_for_dev_replace = 0;\n\tu64 physical_to_patch_in_first_stripe = 0;\n\tu64 raid56_full_stripe_start = (u64)-1;\n\n\tif (op == BTRFS_MAP_DISCARD)\n\t\treturn __btrfs_map_block_for_discard(fs_info, logical,\n\t\t\t\t\t\t     *length, bbio_ret);\n\n\tem = btrfs_get_chunk_map(fs_info, logical, *length);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\toffset = logical - em->start;\n\n\tstripe_len = map->stripe_len;\n\tstripe_nr = offset;\n\t/*\n\t * stripe_nr counts the total number of stripes we have to stride\n\t * to get to this block\n\t */\n\tstripe_nr = div64_u64(stripe_nr, stripe_len);\n\n\tstripe_offset = stripe_nr * stripe_len;\n\tif (offset < stripe_offset) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"stripe math has gone wrong, stripe_offset=%llu, offset=%llu, start=%llu, logical=%llu, stripe_len=%llu\",\n\t\t\t   stripe_offset, offset, em->start, logical,\n\t\t\t   stripe_len);\n\t\tfree_extent_map(em);\n\t\treturn -EINVAL;\n\t}\n\n\t/* stripe_offset is the offset of this block in its stripe*/\n\tstripe_offset = offset - stripe_offset;\n\n\t/* if we're here for raid56, we need to know the stripe aligned start */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tunsigned long full_stripe_len = stripe_len * nr_data_stripes(map);\n\t\traid56_full_stripe_start = offset;\n\n\t\t/* allow a write of a full stripe, but make sure we don't\n\t\t * allow straddling of stripes\n\t\t */\n\t\traid56_full_stripe_start = div64_u64(raid56_full_stripe_start,\n\t\t\t\tfull_stripe_len);\n\t\traid56_full_stripe_start *= full_stripe_len;\n\t}\n\n\tif (map->type & BTRFS_BLOCK_GROUP_PROFILE_MASK) {\n\t\tu64 max_len;\n\t\t/* For writes to RAID[56], allow a full stripeset across all disks.\n\t\t   For other RAID types and for RAID[56] reads, just allow a single\n\t\t   stripe (on a single disk). */\n\t\tif ((map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) &&\n\t\t    (op == BTRFS_MAP_WRITE)) {\n\t\t\tmax_len = stripe_len * nr_data_stripes(map) -\n\t\t\t\t(offset - raid56_full_stripe_start);\n\t\t} else {\n\t\t\t/* we limit the length of each bio to what fits in a stripe */\n\t\t\tmax_len = stripe_len - stripe_offset;\n\t\t}\n\t\t*length = min_t(u64, em->len - offset, max_len);\n\t} else {\n\t\t*length = em->len - offset;\n\t}\n\n\t/*\n\t * This is for when we're called from btrfs_bio_fits_in_stripe and all\n\t * it cares about is the length\n\t */\n\tif (!bbio_ret)\n\t\tgoto out;\n\n\tdown_read(&dev_replace->rwsem);\n\tdev_replace_is_ongoing = btrfs_dev_replace_is_ongoing(dev_replace);\n\t/*\n\t * Hold the semaphore for read during the whole operation, write is\n\t * requested at commit time but must wait.\n\t */\n\tif (!dev_replace_is_ongoing)\n\t\tup_read(&dev_replace->rwsem);\n\n\tif (dev_replace_is_ongoing && mirror_num == map->num_stripes + 1 &&\n\t    !need_full_stripe(op) && dev_replace->tgtdev != NULL) {\n\t\tret = get_extra_mirror_from_replace(fs_info, logical, *length,\n\t\t\t\t\t\t    dev_replace->srcdev->devid,\n\t\t\t\t\t\t    &mirror_num,\n\t\t\t\t\t    &physical_to_patch_in_first_stripe);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\telse\n\t\t\tpatch_the_first_stripe_for_dev_replace = 1;\n\t} else if (mirror_num > map->num_stripes) {\n\t\tmirror_num = 0;\n\t}\n\n\tnum_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID0) {\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t&stripe_index);\n\t\tif (!need_full_stripe(op))\n\t\t\tmirror_num = 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID1) {\n\t\tif (need_full_stripe(op))\n\t\t\tnum_stripes = map->num_stripes;\n\t\telse if (mirror_num)\n\t\t\tstripe_index = mirror_num - 1;\n\t\telse {\n\t\t\tstripe_index = find_live_mirror(fs_info, map, 0,\n\t\t\t\t\t    dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_DUP) {\n\t\tif (need_full_stripe(op)) {\n\t\t\tnum_stripes = map->num_stripes;\n\t\t} else if (mirror_num) {\n\t\t\tstripe_index = mirror_num - 1;\n\t\t} else {\n\t\t\tmirror_num = 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID10) {\n\t\tu32 factor = map->num_stripes / map->sub_stripes;\n\n\t\tstripe_nr = div_u64_rem(stripe_nr, factor, &stripe_index);\n\t\tstripe_index *= map->sub_stripes;\n\n\t\tif (need_full_stripe(op))\n\t\t\tnum_stripes = map->sub_stripes;\n\t\telse if (mirror_num)\n\t\t\tstripe_index += mirror_num - 1;\n\t\telse {\n\t\t\tint old_stripe_index = stripe_index;\n\t\t\tstripe_index = find_live_mirror(fs_info, map,\n\t\t\t\t\t      stripe_index,\n\t\t\t\t\t      dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index - old_stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tif (need_raid_map && (need_full_stripe(op) || mirror_num > 1)) {\n\t\t\t/* push stripe_nr back to the start of the full stripe */\n\t\t\tstripe_nr = div64_u64(raid56_full_stripe_start,\n\t\t\t\t\tstripe_len * nr_data_stripes(map));\n\n\t\t\t/* RAID[56] write or recovery. Return all stripes */\n\t\t\tnum_stripes = map->num_stripes;\n\t\t\tmax_errors = nr_parity_stripes(map);\n\n\t\t\t*length = map->stripe_len;\n\t\t\tstripe_index = 0;\n\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Mirror #0 or #1 means the original data block.\n\t\t\t * Mirror #2 is RAID5 parity block.\n\t\t\t * Mirror #3 is RAID6 Q block.\n\t\t\t */\n\t\t\tstripe_nr = div_u64_rem(stripe_nr,\n\t\t\t\t\tnr_data_stripes(map), &stripe_index);\n\t\t\tif (mirror_num > 1)\n\t\t\t\tstripe_index = nr_data_stripes(map) +\n\t\t\t\t\t\tmirror_num - 2;\n\n\t\t\t/* We distribute the parity blocks across stripes */\n\t\t\tdiv_u64_rem(stripe_nr + stripe_index, map->num_stripes,\n\t\t\t\t\t&stripe_index);\n\t\t\tif (!need_full_stripe(op) && mirror_num <= 1)\n\t\t\t\tmirror_num = 1;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * after this, stripe_nr is the number of stripes on this\n\t\t * device we have to walk to find the data, and stripe_index is\n\t\t * the number of our device in the stripe array\n\t\t */\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t&stripe_index);\n\t\tmirror_num = stripe_index + 1;\n\t}\n\tif (stripe_index >= map->num_stripes) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"stripe index math went horribly wrong, got stripe_index=%u, num_stripes=%u\",\n\t\t\t   stripe_index, map->num_stripes);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tnum_alloc_stripes = num_stripes;\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL) {\n\t\tif (op == BTRFS_MAP_WRITE)\n\t\t\tnum_alloc_stripes <<= 1;\n\t\tif (op == BTRFS_MAP_GET_READ_MIRRORS)\n\t\t\tnum_alloc_stripes++;\n\t\ttgtdev_indexes = num_stripes;\n\t}\n\n\tbbio = alloc_btrfs_bio(num_alloc_stripes, tgtdev_indexes);\n\tif (!bbio) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL)\n\t\tbbio->tgtdev_map = (int *)(bbio->stripes + num_alloc_stripes);\n\n\t/* build raid_map */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK && need_raid_map &&\n\t    (need_full_stripe(op) || mirror_num > 1)) {\n\t\tu64 tmp;\n\t\tunsigned rot;\n\n\t\tbbio->raid_map = (u64 *)((void *)bbio->stripes +\n\t\t\t\t sizeof(struct btrfs_bio_stripe) *\n\t\t\t\t num_alloc_stripes +\n\t\t\t\t sizeof(int) * tgtdev_indexes);\n\n\t\t/* Work out the disk rotation on this stripe-set */\n\t\tdiv_u64_rem(stripe_nr, num_stripes, &rot);\n\n\t\t/* Fill in the logical address of each stripe */\n\t\ttmp = stripe_nr * nr_data_stripes(map);\n\t\tfor (i = 0; i < nr_data_stripes(map); i++)\n\t\t\tbbio->raid_map[(i+rot) % num_stripes] =\n\t\t\t\tem->start + (tmp + i) * map->stripe_len;\n\n\t\tbbio->raid_map[(i+rot) % map->num_stripes] = RAID5_P_STRIPE;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID6)\n\t\t\tbbio->raid_map[(i+rot+1) % num_stripes] =\n\t\t\t\tRAID6_Q_STRIPE;\n\t}\n\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tbbio->stripes[i].physical =\n\t\t\tmap->stripes[stripe_index].physical +\n\t\t\tstripe_offset +\n\t\t\tstripe_nr * map->stripe_len;\n\t\tbbio->stripes[i].dev =\n\t\t\tmap->stripes[stripe_index].dev;\n\t\tstripe_index++;\n\t}\n\n\tif (need_full_stripe(op))\n\t\tmax_errors = btrfs_chunk_max_errors(map);\n\n\tif (bbio->raid_map)\n\t\tsort_parity_stripes(bbio, num_stripes);\n\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL &&\n\t    need_full_stripe(op)) {\n\t\thandle_ops_on_dev_replace(op, &bbio, dev_replace, &num_stripes,\n\t\t\t\t\t  &max_errors);\n\t}\n\n\t*bbio_ret = bbio;\n\tbbio->map_type = map->type;\n\tbbio->num_stripes = num_stripes;\n\tbbio->max_errors = max_errors;\n\tbbio->mirror_num = mirror_num;\n\n\t/*\n\t * this is the case that REQ_READ && dev_replace_is_ongoing &&\n\t * mirror_num == num_stripes + 1 && dev_replace target drive is\n\t * available as a mirror\n\t */\n\tif (patch_the_first_stripe_for_dev_replace && num_stripes > 0) {\n\t\tWARN_ON(num_stripes > 1);\n\t\tbbio->stripes[0].dev = dev_replace->tgtdev;\n\t\tbbio->stripes[0].physical = physical_to_patch_in_first_stripe;\n\t\tbbio->mirror_num = map->num_stripes + 1;\n\t}\nout:\n\tif (dev_replace_is_ongoing) {\n\t\tlockdep_assert_held(&dev_replace->rwsem);\n\t\t/* Unlock and let waiting writers proceed */\n\t\tup_read(&dev_replace->rwsem);\n\t}\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nint btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t      u64 logical, u64 *length,\n\t\t      struct btrfs_bio **bbio_ret, int mirror_num)\n{\n\treturn __btrfs_map_block(fs_info, op, logical, length, bbio_ret,\n\t\t\t\t mirror_num, 0);\n}\n\n/* For Scrub/replace */\nint btrfs_map_sblock(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t     u64 logical, u64 *length,\n\t\t     struct btrfs_bio **bbio_ret)\n{\n\treturn __btrfs_map_block(fs_info, op, logical, length, bbio_ret, 0, 1);\n}\n\nint btrfs_rmap_block(struct btrfs_fs_info *fs_info, u64 chunk_start,\n\t\t     u64 physical, u64 **logical, int *naddrs, int *stripe_len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 *buf;\n\tu64 bytenr;\n\tu64 length;\n\tu64 stripe_nr;\n\tu64 rmap_len;\n\tint i, j, nr = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_start, 1);\n\tif (IS_ERR(em))\n\t\treturn -EIO;\n\n\tmap = em->map_lookup;\n\tlength = em->len;\n\trmap_len = map->stripe_len;\n\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tlength = div_u64(length, map->num_stripes / map->sub_stripes);\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID0)\n\t\tlength = div_u64(length, map->num_stripes);\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tlength = div_u64(length, nr_data_stripes(map));\n\t\trmap_len = map->stripe_len * nr_data_stripes(map);\n\t}\n\n\tbuf = kcalloc(map->num_stripes, sizeof(u64), GFP_NOFS);\n\tBUG_ON(!buf); /* -ENOMEM */\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].physical > physical ||\n\t\t    map->stripes[i].physical + length <= physical)\n\t\t\tcontinue;\n\n\t\tstripe_nr = physical - map->stripes[i].physical;\n\t\tstripe_nr = div64_u64(stripe_nr, map->stripe_len);\n\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID10) {\n\t\t\tstripe_nr = stripe_nr * map->num_stripes + i;\n\t\t\tstripe_nr = div_u64(stripe_nr, map->sub_stripes);\n\t\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID0) {\n\t\t\tstripe_nr = stripe_nr * map->num_stripes + i;\n\t\t} /* else if RAID[56], multiply by nr_data_stripes().\n\t\t   * Alternatively, just use rmap_len below instead of\n\t\t   * map->stripe_len */\n\n\t\tbytenr = chunk_start + stripe_nr * rmap_len;\n\t\tWARN_ON(nr >= map->num_stripes);\n\t\tfor (j = 0; j < nr; j++) {\n\t\t\tif (buf[j] == bytenr)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j == nr) {\n\t\t\tWARN_ON(nr >= map->num_stripes);\n\t\t\tbuf[nr++] = bytenr;\n\t\t}\n\t}\n\n\t*logical = buf;\n\t*naddrs = nr;\n\t*stripe_len = rmap_len;\n\n\tfree_extent_map(em);\n\treturn 0;\n}\n\nstatic inline void btrfs_end_bbio(struct btrfs_bio *bbio, struct bio *bio)\n{\n\tbio->bi_private = bbio->private;\n\tbio->bi_end_io = bbio->end_io;\n\tbio_endio(bio);\n\n\tbtrfs_put_bbio(bbio);\n}\n\nstatic void btrfs_end_bio(struct bio *bio)\n{\n\tstruct btrfs_bio *bbio = bio->bi_private;\n\tint is_orig_bio = 0;\n\n\tif (bio->bi_status) {\n\t\tatomic_inc(&bbio->error);\n\t\tif (bio->bi_status == BLK_STS_IOERR ||\n\t\t    bio->bi_status == BLK_STS_TARGET) {\n\t\t\tunsigned int stripe_index =\n\t\t\t\tbtrfs_io_bio(bio)->stripe_index;\n\t\t\tstruct btrfs_device *dev;\n\n\t\t\tBUG_ON(stripe_index >= bbio->num_stripes);\n\t\t\tdev = bbio->stripes[stripe_index].dev;\n\t\t\tif (dev->bdev) {\n\t\t\t\tif (bio_op(bio) == REQ_OP_WRITE)\n\t\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_WRITE_ERRS);\n\t\t\t\telse\n\t\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_READ_ERRS);\n\t\t\t\tif (bio->bi_opf & REQ_PREFLUSH)\n\t\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_FLUSH_ERRS);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (bio == bbio->orig_bio)\n\t\tis_orig_bio = 1;\n\n\tbtrfs_bio_counter_dec(bbio->fs_info);\n\n\tif (atomic_dec_and_test(&bbio->stripes_pending)) {\n\t\tif (!is_orig_bio) {\n\t\t\tbio_put(bio);\n\t\t\tbio = bbio->orig_bio;\n\t\t}\n\n\t\tbtrfs_io_bio(bio)->mirror_num = bbio->mirror_num;\n\t\t/* only send an error to the higher layers if it is\n\t\t * beyond the tolerance of the btrfs bio\n\t\t */\n\t\tif (atomic_read(&bbio->error) > bbio->max_errors) {\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t} else {\n\t\t\t/*\n\t\t\t * this bio is actually up to date, we didn't\n\t\t\t * go over the max number of errors\n\t\t\t */\n\t\t\tbio->bi_status = BLK_STS_OK;\n\t\t}\n\n\t\tbtrfs_end_bbio(bbio, bio);\n\t} else if (!is_orig_bio) {\n\t\tbio_put(bio);\n\t}\n}\n\n/*\n * see run_scheduled_bios for a description of why bios are collected for\n * async submit.\n *\n * This will add one bio to the pending list for a device and make sure\n * the work struct is scheduled.\n */\nstatic noinline void btrfs_schedule_bio(struct btrfs_device *device,\n\t\t\t\t\tstruct bio *bio)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tint should_queue = 1;\n\tstruct btrfs_pending_bios *pending_bios;\n\n\t/* don't bother with additional async steps for reads, right now */\n\tif (bio_op(bio) == REQ_OP_READ) {\n\t\tbtrfsic_submit_bio(bio);\n\t\treturn;\n\t}\n\n\tWARN_ON(bio->bi_next);\n\tbio->bi_next = NULL;\n\n\tspin_lock(&device->io_lock);\n\tif (op_is_sync(bio->bi_opf))\n\t\tpending_bios = &device->pending_sync_bios;\n\telse\n\t\tpending_bios = &device->pending_bios;\n\n\tif (pending_bios->tail)\n\t\tpending_bios->tail->bi_next = bio;\n\n\tpending_bios->tail = bio;\n\tif (!pending_bios->head)\n\t\tpending_bios->head = bio;\n\tif (device->running_pending)\n\t\tshould_queue = 0;\n\n\tspin_unlock(&device->io_lock);\n\n\tif (should_queue)\n\t\tbtrfs_queue_work(fs_info->submit_workers, &device->work);\n}\n\nstatic void submit_stripe_bio(struct btrfs_bio *bbio, struct bio *bio,\n\t\t\t      u64 physical, int dev_nr, int async)\n{\n\tstruct btrfs_device *dev = bbio->stripes[dev_nr].dev;\n\tstruct btrfs_fs_info *fs_info = bbio->fs_info;\n\n\tbio->bi_private = bbio;\n\tbtrfs_io_bio(bio)->stripe_index = dev_nr;\n\tbio->bi_end_io = btrfs_end_bio;\n\tbio->bi_iter.bi_sector = physical >> 9;\n\tbtrfs_debug_in_rcu(fs_info,\n\t\"btrfs_map_bio: rw %d 0x%x, sector=%llu, dev=%lu (%s id %llu), size=%u\",\n\t\tbio_op(bio), bio->bi_opf, (u64)bio->bi_iter.bi_sector,\n\t\t(u_long)dev->bdev->bd_dev, rcu_str_deref(dev->name), dev->devid,\n\t\tbio->bi_iter.bi_size);\n\tbio_set_dev(bio, dev->bdev);\n\n\tbtrfs_bio_counter_inc_noblocked(fs_info);\n\n\tif (async)\n\t\tbtrfs_schedule_bio(dev, bio);\n\telse\n\t\tbtrfsic_submit_bio(bio);\n}\n\nstatic void bbio_error(struct btrfs_bio *bbio, struct bio *bio, u64 logical)\n{\n\tatomic_inc(&bbio->error);\n\tif (atomic_dec_and_test(&bbio->stripes_pending)) {\n\t\t/* Should be the original bio. */\n\t\tWARN_ON(bio != bbio->orig_bio);\n\n\t\tbtrfs_io_bio(bio)->mirror_num = bbio->mirror_num;\n\t\tbio->bi_iter.bi_sector = logical >> 9;\n\t\tif (atomic_read(&bbio->error) > bbio->max_errors)\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\telse\n\t\t\tbio->bi_status = BLK_STS_OK;\n\t\tbtrfs_end_bbio(bbio, bio);\n\t}\n}\n\nblk_status_t btrfs_map_bio(struct btrfs_fs_info *fs_info, struct bio *bio,\n\t\t\t   int mirror_num, int async_submit)\n{\n\tstruct btrfs_device *dev;\n\tstruct bio *first_bio = bio;\n\tu64 logical = (u64)bio->bi_iter.bi_sector << 9;\n\tu64 length = 0;\n\tu64 map_length;\n\tint ret;\n\tint dev_nr;\n\tint total_devs;\n\tstruct btrfs_bio *bbio = NULL;\n\n\tlength = bio->bi_iter.bi_size;\n\tmap_length = length;\n\n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\tret = __btrfs_map_block(fs_info, btrfs_op(bio), logical,\n\t\t\t\t&map_length, &bbio, mirror_num, 1);\n\tif (ret) {\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\treturn errno_to_blk_status(ret);\n\t}\n\n\ttotal_devs = bbio->num_stripes;\n\tbbio->orig_bio = first_bio;\n\tbbio->private = first_bio->bi_private;\n\tbbio->end_io = first_bio->bi_end_io;\n\tbbio->fs_info = fs_info;\n\tatomic_set(&bbio->stripes_pending, bbio->num_stripes);\n\n\tif ((bbio->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK) &&\n\t    ((bio_op(bio) == REQ_OP_WRITE) || (mirror_num > 1))) {\n\t\t/* In this case, map_length has been set to the length of\n\t\t   a single stripe; not the whole write */\n\t\tif (bio_op(bio) == REQ_OP_WRITE) {\n\t\t\tret = raid56_parity_write(fs_info, bio, bbio,\n\t\t\t\t\t\t  map_length);\n\t\t} else {\n\t\t\tret = raid56_parity_recover(fs_info, bio, bbio,\n\t\t\t\t\t\t    map_length, mirror_num, 1);\n\t\t}\n\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\treturn errno_to_blk_status(ret);\n\t}\n\n\tif (map_length < length) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"mapping failed logical %llu bio len %llu len %llu\",\n\t\t\t   logical, length, map_length);\n\t\tBUG();\n\t}\n\n\tfor (dev_nr = 0; dev_nr < total_devs; dev_nr++) {\n\t\tdev = bbio->stripes[dev_nr].dev;\n\t\tif (!dev || !dev->bdev || test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t   &dev->dev_state) ||\n\t\t    (bio_op(first_bio) == REQ_OP_WRITE &&\n\t\t    !test_bit(BTRFS_DEV_STATE_WRITEABLE, &dev->dev_state))) {\n\t\t\tbbio_error(bbio, first_bio, logical);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (dev_nr < total_devs - 1)\n\t\t\tbio = btrfs_bio_clone(first_bio);\n\t\telse\n\t\t\tbio = first_bio;\n\n\t\tsubmit_stripe_bio(bbio, bio, bbio->stripes[dev_nr].physical,\n\t\t\t\t  dev_nr, async_submit);\n\t}\n\tbtrfs_bio_counter_dec(fs_info);\n\treturn BLK_STS_OK;\n}\n\nstruct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t       u64 devid, u8 *uuid, u8 *fsid)\n{\n\tstruct btrfs_device *device;\n\n\twhile (fs_devices) {\n\t\tif (!fsid ||\n\t\t    !memcmp(fs_devices->metadata_uuid, fsid, BTRFS_FSID_SIZE)) {\n\t\t\tdevice = find_device(fs_devices, devid, uuid);\n\t\t\tif (device)\n\t\t\t\treturn device;\n\t\t}\n\t\tfs_devices = fs_devices->seed;\n\t}\n\treturn NULL;\n}\n\nstatic struct btrfs_device *add_missing_dev(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t\t    u64 devid, u8 *dev_uuid)\n{\n\tstruct btrfs_device *device;\n\n\tdevice = btrfs_alloc_device(NULL, &devid, dev_uuid);\n\tif (IS_ERR(device))\n\t\treturn device;\n\n\tlist_add(&device->dev_list, &fs_devices->devices);\n\tdevice->fs_devices = fs_devices;\n\tfs_devices->num_devices++;\n\n\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\tfs_devices->missing_devices++;\n\n\treturn device;\n}\n\n/**\n * btrfs_alloc_device - allocate struct btrfs_device\n * @fs_info:\tused only for generating a new devid, can be NULL if\n *\t\tdevid is provided (i.e. @devid != NULL).\n * @devid:\ta pointer to devid for this device.  If NULL a new devid\n *\t\tis generated.\n * @uuid:\ta pointer to UUID for this device.  If NULL a new UUID\n *\t\tis generated.\n *\n * Return: a pointer to a new &struct btrfs_device on success; ERR_PTR()\n * on error.  Returned struct is not linked onto any lists and must be\n * destroyed with btrfs_free_device.\n */\nstruct btrfs_device *btrfs_alloc_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tconst u64 *devid,\n\t\t\t\t\tconst u8 *uuid)\n{\n\tstruct btrfs_device *dev;\n\tu64 tmp;\n\n\tif (WARN_ON(!devid && !fs_info))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdev = __alloc_device();\n\tif (IS_ERR(dev))\n\t\treturn dev;\n\n\tif (devid)\n\t\ttmp = *devid;\n\telse {\n\t\tint ret;\n\n\t\tret = find_next_devid(fs_info, &tmp);\n\t\tif (ret) {\n\t\t\tbtrfs_free_device(dev);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\tdev->devid = tmp;\n\n\tif (uuid)\n\t\tmemcpy(dev->uuid, uuid, BTRFS_UUID_SIZE);\n\telse\n\t\tgenerate_random_uuid(dev->uuid);\n\n\tbtrfs_init_work(&dev->work, btrfs_submit_helper,\n\t\t\tpending_bios_fn, NULL, NULL);\n\n\treturn dev;\n}\n\n/* Return -EIO if any error, otherwise return 0. */\nstatic int btrfs_check_chunk_valid(struct btrfs_fs_info *fs_info,\n\t\t\t\t   struct extent_buffer *leaf,\n\t\t\t\t   struct btrfs_chunk *chunk, u64 logical)\n{\n\tu64 length;\n\tu64 stripe_len;\n\tu16 num_stripes;\n\tu16 sub_stripes;\n\tu64 type;\n\tu64 features;\n\tbool mixed = false;\n\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tstripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tsub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\ttype = btrfs_chunk_type(leaf, chunk);\n\n\tif (!num_stripes) {\n\t\tbtrfs_err(fs_info, \"invalid chunk num_stripes: %u\",\n\t\t\t  num_stripes);\n\t\treturn -EIO;\n\t}\n\tif (!IS_ALIGNED(logical, fs_info->sectorsize)) {\n\t\tbtrfs_err(fs_info, \"invalid chunk logical %llu\", logical);\n\t\treturn -EIO;\n\t}\n\tif (btrfs_chunk_sector_size(leaf, chunk) != fs_info->sectorsize) {\n\t\tbtrfs_err(fs_info, \"invalid chunk sectorsize %u\",\n\t\t\t  btrfs_chunk_sector_size(leaf, chunk));\n\t\treturn -EIO;\n\t}\n\tif (!length || !IS_ALIGNED(length, fs_info->sectorsize)) {\n\t\tbtrfs_err(fs_info, \"invalid chunk length %llu\", length);\n\t\treturn -EIO;\n\t}\n\tif (!is_power_of_2(stripe_len) || stripe_len != BTRFS_STRIPE_LEN) {\n\t\tbtrfs_err(fs_info, \"invalid chunk stripe length: %llu\",\n\t\t\t  stripe_len);\n\t\treturn -EIO;\n\t}\n\tif (~(BTRFS_BLOCK_GROUP_TYPE_MASK | BTRFS_BLOCK_GROUP_PROFILE_MASK) &\n\t    type) {\n\t\tbtrfs_err(fs_info, \"unrecognized chunk type: %llu\",\n\t\t\t  ~(BTRFS_BLOCK_GROUP_TYPE_MASK |\n\t\t\t    BTRFS_BLOCK_GROUP_PROFILE_MASK) &\n\t\t\t  btrfs_chunk_type(leaf, chunk));\n\t\treturn -EIO;\n\t}\n\n\tif ((type & BTRFS_BLOCK_GROUP_TYPE_MASK) == 0) {\n\t\tbtrfs_err(fs_info, \"missing chunk type flag: 0x%llx\", type);\n\t\treturn -EIO;\n\t}\n\n\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) &&\n\t    (type & (BTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_DATA))) {\n\t\tbtrfs_err(fs_info,\n\t\t\t\"system chunk with data or metadata type: 0x%llx\", type);\n\t\treturn -EIO;\n\t}\n\n\tfeatures = btrfs_super_incompat_flags(fs_info->super_copy);\n\tif (features & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)\n\t\tmixed = true;\n\n\tif (!mixed) {\n\t\tif ((type & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t    (type & BTRFS_BLOCK_GROUP_DATA)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"mixed chunk type in non-mixed mode: 0x%llx\", type);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tif ((type & BTRFS_BLOCK_GROUP_RAID10 && sub_stripes != 2) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID1 && num_stripes < 1) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID5 && num_stripes < 2) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID6 && num_stripes < 3) ||\n\t    (type & BTRFS_BLOCK_GROUP_DUP && num_stripes > 2) ||\n\t    ((type & BTRFS_BLOCK_GROUP_PROFILE_MASK) == 0 &&\n\t     num_stripes != 1)) {\n\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid num_stripes:sub_stripes %u:%u for profile %llu\",\n\t\t\tnum_stripes, sub_stripes,\n\t\t\ttype & BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic void btrfs_report_missing_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tu64 devid, u8 *uuid, bool error)\n{\n\tif (error)\n\t\tbtrfs_err_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n\telse\n\t\tbtrfs_warn_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n}\n\nstatic int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}\n\nstatic void fill_device_from_item(struct extent_buffer *leaf,\n\t\t\t\t struct btrfs_dev_item *dev_item,\n\t\t\t\t struct btrfs_device *device)\n{\n\tunsigned long ptr;\n\n\tdevice->devid = btrfs_device_id(leaf, dev_item);\n\tdevice->disk_total_bytes = btrfs_device_total_bytes(leaf, dev_item);\n\tdevice->total_bytes = device->disk_total_bytes;\n\tdevice->commit_total_bytes = device->disk_total_bytes;\n\tdevice->bytes_used = btrfs_device_bytes_used(leaf, dev_item);\n\tdevice->commit_bytes_used = device->bytes_used;\n\tdevice->type = btrfs_device_type(leaf, dev_item);\n\tdevice->io_align = btrfs_device_io_align(leaf, dev_item);\n\tdevice->io_width = btrfs_device_io_width(leaf, dev_item);\n\tdevice->sector_size = btrfs_device_sector_size(leaf, dev_item);\n\tWARN_ON(device->devid == BTRFS_DEV_REPLACE_DEVID);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\n\tptr = btrfs_device_uuid(dev_item);\n\tread_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n}\n\nstatic struct btrfs_fs_devices *open_seed_devices(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u8 *fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tASSERT(fsid);\n\n\tfs_devices = fs_info->fs_devices->seed;\n\twhile (fs_devices) {\n\t\tif (!memcmp(fs_devices->fsid, fsid, BTRFS_FSID_SIZE))\n\t\t\treturn fs_devices;\n\n\t\tfs_devices = fs_devices->seed;\n\t}\n\n\tfs_devices = find_fsid(fsid, NULL);\n\tif (!fs_devices) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED))\n\t\t\treturn ERR_PTR(-ENOENT);\n\n\t\tfs_devices = alloc_fs_devices(fsid, NULL);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn fs_devices;\n\n\t\tfs_devices->seeding = 1;\n\t\tfs_devices->opened = 1;\n\t\treturn fs_devices;\n\t}\n\n\tfs_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tret = open_fs_devices(fs_devices, FMODE_READ, fs_info->bdev_holder);\n\tif (ret) {\n\t\tfree_fs_devices(fs_devices);\n\t\tfs_devices = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tif (!fs_devices->seeding) {\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t\tfs_devices = ERR_PTR(-EINVAL);\n\t\tgoto out;\n\t}\n\n\tfs_devices->seed = fs_info->fs_devices->seed;\n\tfs_info->fs_devices->seed = fs_devices;\nout:\n\treturn fs_devices;\n}\n\nstatic int read_one_dev(struct btrfs_fs_info *fs_info,\n\t\t\tstruct extent_buffer *leaf,\n\t\t\tstruct btrfs_dev_item *dev_item)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 devid;\n\tint ret;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\n\tdevid = btrfs_device_id(leaf, dev_item);\n\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t   BTRFS_UUID_SIZE);\n\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t   BTRFS_FSID_SIZE);\n\n\tif (memcmp(fs_uuid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE)) {\n\t\tfs_devices = open_seed_devices(fs_info, fs_uuid);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn PTR_ERR(fs_devices);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t   fs_uuid);\n\tif (!device) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tdevice = add_missing_dev(fs_devices, devid, dev_uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"failed to add missing dev %llu: %ld\",\n\t\t\t\tdevid, PTR_ERR(device));\n\t\t\treturn PTR_ERR(device);\n\t\t}\n\t\tbtrfs_report_missing_device(fs_info, devid, dev_uuid, false);\n\t} else {\n\t\tif (!device->bdev) {\n\t\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\t\tbtrfs_report_missing_device(fs_info,\n\t\t\t\t\t\tdevid, dev_uuid, true);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, false);\n\t\t}\n\n\t\tif (!device->bdev &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\t/*\n\t\t\t * this happens when a device that was properly setup\n\t\t\t * in the device info lists suddenly goes bad.\n\t\t\t * device->bdev is NULL, and so we have to set\n\t\t\t * device->missing to one here\n\t\t\t */\n\t\t\tdevice->fs_devices->missing_devices++;\n\t\t\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\n\t\t/* Move the device to its own fs_devices */\n\t\tif (device->fs_devices != fs_devices) {\n\t\t\tASSERT(test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t\t&device->dev_state));\n\n\t\t\tlist_move(&device->dev_list, &fs_devices->devices);\n\t\t\tdevice->fs_devices->num_devices--;\n\t\t\tfs_devices->num_devices++;\n\n\t\t\tdevice->fs_devices->missing_devices--;\n\t\t\tfs_devices->missing_devices++;\n\n\t\t\tdevice->fs_devices = fs_devices;\n\t\t}\n\t}\n\n\tif (device->fs_devices != fs_info->fs_devices) {\n\t\tBUG_ON(test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state));\n\t\tif (device->generation !=\n\t\t    btrfs_device_generation(leaf, dev_item))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfill_device_from_item(leaf, dev_item, device);\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t   !test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes += device->total_bytes;\n\t\tatomic64_add(device->total_bytes - device->bytes_used,\n\t\t\t\t&fs_info->free_chunk_space);\n\t}\n\tret = 0;\n\treturn ret;\n}\n\nint btrfs_read_sys_array(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct extent_buffer *sb;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *array_ptr;\n\tunsigned long sb_array_offset;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur_offset;\n\tu64 type;\n\tstruct btrfs_key key;\n\n\tASSERT(BTRFS_SUPER_INFO_SIZE <= fs_info->nodesize);\n\t/*\n\t * This will create extent buffer of nodesize, superblock size is\n\t * fixed to BTRFS_SUPER_INFO_SIZE. If nodesize > sb size, this will\n\t * overallocate but we can keep it as-is, only the first page is used.\n\t */\n\tsb = btrfs_find_create_tree_block(fs_info, BTRFS_SUPER_INFO_OFFSET);\n\tif (IS_ERR(sb))\n\t\treturn PTR_ERR(sb);\n\tset_extent_buffer_uptodate(sb);\n\tbtrfs_set_buffer_lockdep_class(root->root_key.objectid, sb, 0);\n\t/*\n\t * The sb extent buffer is artificial and just used to read the system array.\n\t * set_extent_buffer_uptodate() call does not properly mark all it's\n\t * pages up-to-date when the page is larger: extent does not cover the\n\t * whole page and consequently check_page_uptodate does not find all\n\t * the page's extents up-to-date (the hole beyond sb),\n\t * write_extent_buffer then triggers a WARN_ON.\n\t *\n\t * Regular short extents go through mark_extent_buffer_dirty/writeback cycle,\n\t * but sb spans only this function. Add an explicit SetPageUptodate call\n\t * to silence the warning eg. on PowerPC 64.\n\t */\n\tif (PAGE_SIZE > BTRFS_SUPER_INFO_SIZE)\n\t\tSetPageUptodate(sb->pages[0]);\n\n\twrite_extent_buffer(sb, super_copy, 0, BTRFS_SUPER_INFO_SIZE);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tarray_ptr = super_copy->sys_chunk_array;\n\tsb_array_offset = offsetof(struct btrfs_super_block, sys_chunk_array);\n\tcur_offset = 0;\n\n\twhile (cur_offset < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)array_ptr;\n\t\tlen = sizeof(*disk_key);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\n\t\tif (key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tchunk = (struct btrfs_chunk *)sb_array_offset;\n\t\t\t/*\n\t\t\t * At least one btrfs_chunk with one stripe must be\n\t\t\t * present, exact stripe count check comes afterwards\n\t\t\t */\n\t\t\tlen = btrfs_chunk_item_size(1);\n\t\t\tif (cur_offset + len > array_size)\n\t\t\t\tgoto out_short_read;\n\n\t\t\tnum_stripes = btrfs_chunk_num_stripes(sb, chunk);\n\t\t\tif (!num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"invalid number of stripes %u in sys_array at offset %u\",\n\t\t\t\t\tnum_stripes, cur_offset);\n\t\t\t\tret = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\ttype = btrfs_chunk_type(sb, chunk);\n\t\t\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) == 0) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t    \"invalid chunk type %llu in sys_array at offset %u\",\n\t\t\t\t\ttype, cur_offset);\n\t\t\t\tret = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tlen = btrfs_chunk_item_size(num_stripes);\n\t\t\tif (cur_offset + len > array_size)\n\t\t\t\tgoto out_short_read;\n\n\t\t\tret = read_one_chunk(fs_info, &key, sb, chunk);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t    \"unexpected item type %u in sys_array at offset %u\",\n\t\t\t\t  (u32)key.type, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\t}\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn ret;\n\nout_short_read:\n\tbtrfs_err(fs_info, \"sys_array too short to read %u bytes at offset %u\",\n\t\t\tlen, cur_offset);\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn -EIO;\n}\n\n/*\n * Check if all chunks in the fs are OK for read-write degraded mount\n *\n * If the @failing_dev is specified, it's accounted as missing.\n *\n * Return true if all chunks meet the minimal RW mount requirements.\n * Return false if any chunk doesn't meet the minimal RW mount requirements.\n */\nbool btrfs_check_rw_degradable(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct btrfs_device *failing_dev)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tu64 next_start = 0;\n\tbool ret = true;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, 0, (u64)-1);\n\tread_unlock(&map_tree->map_tree.lock);\n\t/* No chunk at all? Return false anyway */\n\tif (!em) {\n\t\tret = false;\n\t\tgoto out;\n\t}\n\twhile (em) {\n\t\tstruct map_lookup *map;\n\t\tint missing = 0;\n\t\tint max_tolerated;\n\t\tint i;\n\n\t\tmap = em->map_lookup;\n\t\tmax_tolerated =\n\t\t\tbtrfs_get_num_tolerated_disk_barrier_failures(\n\t\t\t\t\tmap->type);\n\t\tfor (i = 0; i < map->num_stripes; i++) {\n\t\t\tstruct btrfs_device *dev = map->stripes[i].dev;\n\n\t\t\tif (!dev || !dev->bdev ||\n\t\t\t    test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) ||\n\t\t\t    dev->last_flush_error)\n\t\t\t\tmissing++;\n\t\t\telse if (failing_dev && failing_dev == dev)\n\t\t\t\tmissing++;\n\t\t}\n\t\tif (missing > max_tolerated) {\n\t\t\tif (!failing_dev)\n\t\t\t\tbtrfs_warn(fs_info,\n\t\"chunk %llu missing %d devices, max tolerance is %d for writable mount\",\n\t\t\t\t   em->start, missing, max_tolerated);\n\t\t\tfree_extent_map(em);\n\t\t\tret = false;\n\t\t\tgoto out;\n\t\t}\n\t\tnext_start = extent_map_end(em);\n\t\tfree_extent_map(em);\n\n\t\tread_lock(&map_tree->map_tree.lock);\n\t\tem = lookup_extent_mapping(&map_tree->map_tree, next_start,\n\t\t\t\t\t   (u64)(-1) - next_start);\n\t\tread_unlock(&map_tree->map_tree.lock);\n\t}\nout:\n\treturn ret;\n}\n\nint btrfs_read_chunk_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tint ret;\n\tint slot;\n\tu64 total_dev = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * uuid_mutex is needed only if we are mounting a sprout FS\n\t * otherwise we don't need it.\n\t */\n\tmutex_lock(&uuid_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\t/*\n\t * Read all device items, and then all the chunk items. All\n\t * device items are found before any chunk item (their object id\n\t * is smaller than the lowest possible object id for a chunk\n\t * item - BTRFS_FIRST_CHUNK_TREE_OBJECTID).\n\t */\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = 0;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto error;\n\twhile (1) {\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\t\tif (found_key.type == BTRFS_DEV_ITEM_KEY) {\n\t\t\tstruct btrfs_dev_item *dev_item;\n\t\t\tdev_item = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\t  struct btrfs_dev_item);\n\t\t\tret = read_one_dev(fs_info, leaf, dev_item);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t\ttotal_dev++;\n\t\t} else if (found_key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tstruct btrfs_chunk *chunk;\n\t\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\t\tret = read_one_chunk(fs_info, &found_key, leaf, chunk);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t}\n\t\tpath->slots[0]++;\n\t}\n\n\t/*\n\t * After loading chunk tree, we've got all device information,\n\t * do another round of validation checks.\n\t */\n\tif (total_dev != fs_info->fs_devices->total_devices) {\n\t\tbtrfs_err(fs_info,\n\t   \"super_num_devices %llu mismatch with num_devices %llu found here\",\n\t\t\t  btrfs_super_num_devices(fs_info->super_copy),\n\t\t\t  total_dev);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tif (btrfs_super_total_bytes(fs_info->super_copy) <\n\t    fs_info->fs_devices->total_rw_bytes) {\n\t\tbtrfs_err(fs_info,\n\t\"super_total_bytes %llu mismatch with fs_devices total_rw_bytes %llu\",\n\t\t\t  btrfs_super_total_bytes(fs_info->super_copy),\n\t\t\t  fs_info->fs_devices->total_rw_bytes);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tret = 0;\nerror:\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&uuid_mutex);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nvoid btrfs_init_devices_late(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\n\twhile (fs_devices) {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_for_each_entry(device, &fs_devices->devices, dev_list)\n\t\t\tdevice->fs_info = fs_info;\n\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t\tfs_devices = fs_devices->seed;\n\t}\n}\n\nstatic void __btrfs_reset_dev_stats(struct btrfs_device *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tbtrfs_dev_stat_reset(dev, i);\n}\n\nint btrfs_init_dev_stats(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tint ret = 0;\n\tstruct btrfs_device *device;\n\tstruct btrfs_path *path = NULL;\n\tint i;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tint item_size;\n\t\tstruct btrfs_dev_stats_item *ptr;\n\n\t\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\t\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\t\tkey.offset = device->devid;\n\t\tret = btrfs_search_slot(NULL, dev_root, &key, path, 0, 0);\n\t\tif (ret) {\n\t\t\t__btrfs_reset_dev_stats(device);\n\t\t\tdevice->dev_stats_valid = 1;\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\t\tslot = path->slots[0];\n\t\teb = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(eb, &found_key, slot);\n\t\titem_size = btrfs_item_size_nr(eb, slot);\n\n\t\tptr = btrfs_item_ptr(eb, slot,\n\t\t\t\t     struct btrfs_dev_stats_item);\n\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\t\tif (item_size >= (1 + i) * sizeof(__le64))\n\t\t\t\tbtrfs_dev_stat_set(device, i,\n\t\t\t\t\tbtrfs_dev_stats_value(eb, ptr, i));\n\t\t\telse\n\t\t\t\tbtrfs_dev_stat_reset(device, i);\n\t\t}\n\n\t\tdevice->dev_stats_valid = 1;\n\t\tbtrfs_dev_stat_print_on_load(device);\n\t\tbtrfs_release_path(path);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic int update_dev_stat_item(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_device *device)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_dev_stats_item *ptr;\n\tint ret;\n\tint i;\n\n\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tret = btrfs_search_slot(trans, dev_root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\"error %d while searching for dev_stats item for device %s\",\n\t\t\t      ret, rcu_str_deref(device->name));\n\t\tgoto out;\n\t}\n\n\tif (ret == 0 &&\n\t    btrfs_item_size_nr(path->nodes[0], path->slots[0]) < sizeof(*ptr)) {\n\t\t/* need to delete old one and insert a new one */\n\t\tret = btrfs_del_item(trans, dev_root, path);\n\t\tif (ret != 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"delete too small dev_stats item for device %s failed %d\",\n\t\t\t\t      rcu_str_deref(device->name), ret);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 1;\n\t}\n\n\tif (ret == 1) {\n\t\t/* need to insert a new item */\n\t\tbtrfs_release_path(path);\n\t\tret = btrfs_insert_empty_item(trans, dev_root, path,\n\t\t\t\t\t      &key, sizeof(*ptr));\n\t\tif (ret < 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"insert dev_stats item for device %s failed %d\",\n\t\t\t\trcu_str_deref(device->name), ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\teb = path->nodes[0];\n\tptr = btrfs_item_ptr(eb, path->slots[0], struct btrfs_dev_stats_item);\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tbtrfs_set_dev_stats_value(eb, ptr, i,\n\t\t\t\t\t  btrfs_dev_stat_read(device, i));\n\tbtrfs_mark_buffer_dirty(eb);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * called from commit_transaction. Writes all changed device stats to disk.\n */\nint btrfs_run_dev_stats(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tint stats_cnt;\n\tint ret = 0;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tstats_cnt = atomic_read(&device->dev_stats_ccnt);\n\t\tif (!device->dev_stats_valid || stats_cnt == 0)\n\t\t\tcontinue;\n\n\n\t\t/*\n\t\t * There is a LOAD-LOAD control dependency between the value of\n\t\t * dev_stats_ccnt and updating the on-disk values which requires\n\t\t * reading the in-memory counters. Such control dependencies\n\t\t * require explicit read memory barriers.\n\t\t *\n\t\t * This memory barriers pairs with smp_mb__before_atomic in\n\t\t * btrfs_dev_stat_inc/btrfs_dev_stat_set and with the full\n\t\t * barrier implied by atomic_xchg in\n\t\t * btrfs_dev_stats_read_and_reset\n\t\t */\n\t\tsmp_rmb();\n\n\t\tret = update_dev_stat_item(trans, device);\n\t\tif (!ret)\n\t\t\tatomic_sub(stats_cnt, &device->dev_stats_ccnt);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\treturn ret;\n}\n\nvoid btrfs_dev_stat_inc_and_print(struct btrfs_device *dev, int index)\n{\n\tbtrfs_dev_stat_inc(dev, index);\n\tbtrfs_dev_stat_print_on_error(dev);\n}\n\nstatic void btrfs_dev_stat_print_on_error(struct btrfs_device *dev)\n{\n\tif (!dev->dev_stats_valid)\n\t\treturn;\n\tbtrfs_err_rl_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t\t\t   rcu_str_deref(dev->name),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tif (btrfs_dev_stat_read(dev, i) != 0)\n\t\t\tbreak;\n\tif (i == BTRFS_DEV_STAT_VALUES_MAX)\n\t\treturn; /* all values == 0, suppress message */\n\n\tbtrfs_info_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t       rcu_str_deref(dev->name),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nint btrfs_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\tstruct btrfs_ioctl_get_dev_stats *stats)\n{\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tint i;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, stats->devid, NULL, NULL);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (!dev) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, device not found\");\n\t\treturn -ENODEV;\n\t} else if (!dev->dev_stats_valid) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, not yet valid\");\n\t\treturn -ENODEV;\n\t} else if (stats->flags & BTRFS_DEV_STATS_RESET) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] =\n\t\t\t\t\tbtrfs_dev_stat_read_and_reset(dev, i);\n\t\t\telse\n\t\t\t\tbtrfs_dev_stat_reset(dev, i);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] = btrfs_dev_stat_read(dev, i);\n\t}\n\tif (stats->nr_items > BTRFS_DEV_STAT_VALUES_MAX)\n\t\tstats->nr_items = BTRFS_DEV_STAT_VALUES_MAX;\n\treturn 0;\n}\n\nvoid btrfs_scratch_superblocks(struct block_device *bdev, const char *device_path)\n{\n\tstruct buffer_head *bh;\n\tstruct btrfs_super_block *disk_super;\n\tint copy_num;\n\n\tif (!bdev)\n\t\treturn;\n\n\tfor (copy_num = 0; copy_num < BTRFS_SUPER_MIRROR_MAX;\n\t\tcopy_num++) {\n\n\t\tif (btrfs_read_dev_one_super(bdev, copy_num, &bh))\n\t\t\tcontinue;\n\n\t\tdisk_super = (struct btrfs_super_block *)bh->b_data;\n\n\t\tmemset(&disk_super->magic, 0, sizeof(disk_super->magic));\n\t\tset_buffer_dirty(bh);\n\t\tsync_dirty_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\t/* Notify udev that device has changed */\n\tbtrfs_kobject_uevent(bdev, KOBJ_CHANGE);\n\n\t/* Update ctime/mtime for device path for libblkid */\n\tupdate_dev_time(device_path);\n}\n\n/*\n * Update the size of all devices, which is used for writing out the\n * super blocks.\n */\nvoid btrfs_update_commit_device_size(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *curr, *next;\n\n\tif (list_empty(&fs_devices->resized_devices))\n\t\treturn;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_for_each_entry_safe(curr, next, &fs_devices->resized_devices,\n\t\t\t\t resized_list) {\n\t\tlist_del_init(&curr->resized_list);\n\t\tcurr->commit_total_bytes = curr->disk_total_bytes;\n\t}\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n}\n\n/* Must be invoked during the transaction commit */\nvoid btrfs_update_commit_device_bytes_used(struct btrfs_transaction *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tint i;\n\n\tif (list_empty(&trans->pending_chunks))\n\t\treturn;\n\n\t/* In order to kick the device replace finish process */\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_for_each_entry(em, &trans->pending_chunks, list) {\n\t\tmap = em->map_lookup;\n\n\t\tfor (i = 0; i < map->num_stripes; i++) {\n\t\t\tdev = map->stripes[i].dev;\n\t\t\tdev->commit_bytes_used = dev->bytes_used;\n\t\t}\n\t}\n\tmutex_unlock(&fs_info->chunk_mutex);\n}\n\nvoid btrfs_set_fs_info_ptr(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\twhile (fs_devices) {\n\t\tfs_devices->fs_info = fs_info;\n\t\tfs_devices = fs_devices->seed;\n\t}\n}\n\nvoid btrfs_reset_fs_info_ptr(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\twhile (fs_devices) {\n\t\tfs_devices->fs_info = NULL;\n\t\tfs_devices = fs_devices->seed;\n\t}\n}\n\n/*\n * Multiplicity factor for simple profiles: DUP, RAID1-like and RAID10.\n */\nint btrfs_bg_type_to_factor(u64 flags)\n{\n\tif (flags & (BTRFS_BLOCK_GROUP_DUP | BTRFS_BLOCK_GROUP_RAID1 |\n\t\t     BTRFS_BLOCK_GROUP_RAID10))\n\t\treturn 2;\n\treturn 1;\n}\n\n\nstatic u64 calc_stripe_length(u64 type, u64 chunk_len, int num_stripes)\n{\n\tint index = btrfs_bg_flags_to_raid_index(type);\n\tint ncopies = btrfs_raid_array[index].ncopies;\n\tint data_stripes;\n\n\tswitch (type & BTRFS_BLOCK_GROUP_PROFILE_MASK) {\n\tcase BTRFS_BLOCK_GROUP_RAID5:\n\t\tdata_stripes = num_stripes - 1;\n\t\tbreak;\n\tcase BTRFS_BLOCK_GROUP_RAID6:\n\t\tdata_stripes = num_stripes - 2;\n\t\tbreak;\n\tdefault:\n\t\tdata_stripes = num_stripes / ncopies;\n\t\tbreak;\n\t}\n\treturn div_u64(chunk_len, data_stripes);\n}\n\nstatic int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = find_device(fs_info->fs_devices->seed, devid, NULL);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic int verify_chunk_dev_extent_mapping(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *node;\n\tint ret = 0;\n\n\tread_lock(&em_tree->lock);\n\tfor (node = rb_first_cached(&em_tree->map); node; node = rb_next(node)) {\n\t\tem = rb_entry(node, struct extent_map, rb_node);\n\t\tif (em->map_lookup->num_stripes !=\n\t\t    em->map_lookup->verified_stripes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"chunk %llu has missing dev extent, have %d expect %d\",\n\t\t\t\t  em->start, em->map_lookup->verified_stripes,\n\t\t\t\t  em->map_lookup->num_stripes);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tread_unlock(&em_tree->lock);\n\treturn ret;\n}\n\n/*\n * Ensure that all dev extents are mapped to correct chunk, otherwise\n * later chunk allocation/free would cause unexpected behavior.\n *\n * NOTE: This will iterate through the whole device tree, which should be of\n * the same size level as the chunk tree.  This slightly increases mount time.\n */\nint btrfs_verify_dev_extents(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tu64 prev_devid = 0;\n\tu64 prev_dev_ext_end = 0;\n\tint ret = 0;\n\n\tkey.objectid = 1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\tkey.offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_FORWARD;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/* No dev extents at all? Not good */\n\t\tif (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\twhile (1) {\n\t\tstruct extent_buffer *leaf = path->nodes[0];\n\t\tstruct btrfs_dev_extent *dext;\n\t\tint slot = path->slots[0];\n\t\tu64 chunk_offset;\n\t\tu64 physical_offset;\n\t\tu64 physical_len;\n\t\tu64 devid;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tbreak;\n\t\tdevid = key.objectid;\n\t\tphysical_offset = key.offset;\n\n\t\tdext = btrfs_item_ptr(leaf, slot, struct btrfs_dev_extent);\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(leaf, dext);\n\t\tphysical_len = btrfs_dev_extent_length(leaf, dext);\n\n\t\t/* Check if this dev extent overlaps with the previous one */\n\t\tif (devid == prev_devid && physical_offset < prev_dev_ext_end) {\n\t\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu overlap with previous dev extent end %llu\",\n\t\t\t\t  devid, physical_offset, prev_dev_ext_end);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = verify_one_dev_extent(fs_info, chunk_offset, devid,\n\t\t\t\t\t    physical_offset, physical_len);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tprev_devid = devid;\n\t\tprev_dev_ext_end = physical_offset + physical_len;\n\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (ret > 0) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ensure all chunks have corresponding dev extents */\n\tret = verify_chunk_dev_extent_mapping(fs_info);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Check whether the given block group or device is pinned by any inode being\n * used as a swapfile.\n */\nbool btrfs_pinned_by_swapfile(struct btrfs_fs_info *fs_info, void *ptr)\n{\n\tstruct btrfs_swapfile_pin *sp;\n\tstruct rb_node *node;\n\n\tspin_lock(&fs_info->swapfile_pins_lock);\n\tnode = fs_info->swapfile_pins.rb_node;\n\twhile (node) {\n\t\tsp = rb_entry(node, struct btrfs_swapfile_pin, node);\n\t\tif (ptr < sp->ptr)\n\t\t\tnode = node->rb_left;\n\t\telse if (ptr > sp->ptr)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\tbreak;\n\t}\n\tspin_unlock(&fs_info->swapfile_pins_lock);\n\treturn node != NULL;\n}\n", "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n */\n\n#ifndef BTRFS_VOLUMES_H\n#define BTRFS_VOLUMES_H\n\n#include <linux/bio.h>\n#include <linux/sort.h>\n#include <linux/btrfs.h>\n#include \"async-thread.h\"\n\n#define BTRFS_MAX_DATA_CHUNK_SIZE\t(10ULL * SZ_1G)\n\nextern struct mutex uuid_mutex;\n\n#define BTRFS_STRIPE_LEN\tSZ_64K\n\nstruct buffer_head;\nstruct btrfs_pending_bios {\n\tstruct bio *head;\n\tstruct bio *tail;\n};\n\n/*\n * Use sequence counter to get consistent device stat data on\n * 32-bit processors.\n */\n#if BITS_PER_LONG==32 && defined(CONFIG_SMP)\n#include <linux/seqlock.h>\n#define __BTRFS_NEED_DEVICE_DATA_ORDERED\n#define btrfs_device_data_ordered_init(device)\t\\\n\tseqcount_init(&device->data_seqcount)\n#else\n#define btrfs_device_data_ordered_init(device) do { } while (0)\n#endif\n\n#define BTRFS_DEV_STATE_WRITEABLE\t(0)\n#define BTRFS_DEV_STATE_IN_FS_METADATA\t(1)\n#define BTRFS_DEV_STATE_MISSING\t\t(2)\n#define BTRFS_DEV_STATE_REPLACE_TGT\t(3)\n#define BTRFS_DEV_STATE_FLUSH_SENT\t(4)\n\nstruct btrfs_device {\n\tstruct list_head dev_list;\n\tstruct list_head dev_alloc_list;\n\tstruct btrfs_fs_devices *fs_devices;\n\tstruct btrfs_fs_info *fs_info;\n\n\tstruct rcu_string *name;\n\n\tu64 generation;\n\n\tspinlock_t io_lock ____cacheline_aligned;\n\tint running_pending;\n\t/* regular prio bios */\n\tstruct btrfs_pending_bios pending_bios;\n\t/* sync bios */\n\tstruct btrfs_pending_bios pending_sync_bios;\n\n\tstruct block_device *bdev;\n\n\t/* the mode sent to blkdev_get */\n\tfmode_t mode;\n\n\tunsigned long dev_state;\n\tblk_status_t last_flush_error;\n\tint flush_bio_sent;\n\n#ifdef __BTRFS_NEED_DEVICE_DATA_ORDERED\n\tseqcount_t data_seqcount;\n#endif\n\n\t/* the internal btrfs device id */\n\tu64 devid;\n\n\t/* size of the device in memory */\n\tu64 total_bytes;\n\n\t/* size of the device on disk */\n\tu64 disk_total_bytes;\n\n\t/* bytes used */\n\tu64 bytes_used;\n\n\t/* optimal io alignment for this device */\n\tu32 io_align;\n\n\t/* optimal io width for this device */\n\tu32 io_width;\n\t/* type and info about this device */\n\tu64 type;\n\n\t/* minimal io size for this device */\n\tu32 sector_size;\n\n\t/* physical drive uuid (or lvm uuid) */\n\tu8 uuid[BTRFS_UUID_SIZE];\n\n\t/*\n\t * size of the device on the current transaction\n\t *\n\t * This variant is update when committing the transaction,\n\t * and protected by device_list_mutex\n\t */\n\tu64 commit_total_bytes;\n\n\t/* bytes used on the current transaction */\n\tu64 commit_bytes_used;\n\t/*\n\t * used to manage the device which is resized\n\t *\n\t * It is protected by chunk_lock.\n\t */\n\tstruct list_head resized_list;\n\n\t/* for sending down flush barriers */\n\tstruct bio *flush_bio;\n\tstruct completion flush_wait;\n\n\t/* per-device scrub information */\n\tstruct scrub_ctx *scrub_ctx;\n\n\tstruct btrfs_work work;\n\tstruct rcu_head rcu;\n\n\t/* readahead state */\n\tatomic_t reada_in_flight;\n\tu64 reada_next;\n\tstruct reada_zone *reada_curr_zone;\n\tstruct radix_tree_root reada_zones;\n\tstruct radix_tree_root reada_extents;\n\n\t/* disk I/O failure stats. For detailed description refer to\n\t * enum btrfs_dev_stat_values in ioctl.h */\n\tint dev_stats_valid;\n\n\t/* Counter to record the change of device stats */\n\tatomic_t dev_stats_ccnt;\n\tatomic_t dev_stat_values[BTRFS_DEV_STAT_VALUES_MAX];\n};\n\n/*\n * If we read those variants at the context of their own lock, we needn't\n * use the following helpers, reading them directly is safe.\n */\n#if BITS_PER_LONG==32 && defined(CONFIG_SMP)\n#define BTRFS_DEVICE_GETSET_FUNCS(name)\t\t\t\t\t\\\nstatic inline u64\t\t\t\t\t\t\t\\\nbtrfs_device_get_##name(const struct btrfs_device *dev)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu64 size;\t\t\t\t\t\t\t\\\n\tunsigned int seq;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tseq = read_seqcount_begin(&dev->data_seqcount);\t\t\\\n\t\tsize = dev->name;\t\t\t\t\t\\\n\t} while (read_seqcount_retry(&dev->data_seqcount, seq));\t\\\n\treturn size;\t\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic inline void\t\t\t\t\t\t\t\\\nbtrfs_device_set_##name(struct btrfs_device *dev, u64 size)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\twrite_seqcount_begin(&dev->data_seqcount);\t\t\t\\\n\tdev->name = size;\t\t\t\t\t\t\\\n\twrite_seqcount_end(&dev->data_seqcount);\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n}\n#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPT)\n#define BTRFS_DEVICE_GETSET_FUNCS(name)\t\t\t\t\t\\\nstatic inline u64\t\t\t\t\t\t\t\\\nbtrfs_device_get_##name(const struct btrfs_device *dev)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu64 size;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tsize = dev->name;\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n\treturn size;\t\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic inline void\t\t\t\t\t\t\t\\\nbtrfs_device_set_##name(struct btrfs_device *dev, u64 size)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tdev->name = size;\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n}\n#else\n#define BTRFS_DEVICE_GETSET_FUNCS(name)\t\t\t\t\t\\\nstatic inline u64\t\t\t\t\t\t\t\\\nbtrfs_device_get_##name(const struct btrfs_device *dev)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn dev->name;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic inline void\t\t\t\t\t\t\t\\\nbtrfs_device_set_##name(struct btrfs_device *dev, u64 size)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tdev->name = size;\t\t\t\t\t\t\\\n}\n#endif\n\nBTRFS_DEVICE_GETSET_FUNCS(total_bytes);\nBTRFS_DEVICE_GETSET_FUNCS(disk_total_bytes);\nBTRFS_DEVICE_GETSET_FUNCS(bytes_used);\n\nstruct btrfs_fs_devices {\n\tu8 fsid[BTRFS_FSID_SIZE]; /* FS specific uuid */\n\tu8 metadata_uuid[BTRFS_FSID_SIZE];\n\tbool fsid_change;\n\tstruct list_head fs_list;\n\n\tu64 num_devices;\n\tu64 open_devices;\n\tu64 rw_devices;\n\tu64 missing_devices;\n\tu64 total_rw_bytes;\n\tu64 total_devices;\n\n\t/* Highest generation number of seen devices */\n\tu64 latest_generation;\n\n\tstruct block_device *latest_bdev;\n\n\t/* all of the devices in the FS, protected by a mutex\n\t * so we can safely walk it to write out the supers without\n\t * worrying about add/remove by the multi-device code.\n\t * Scrubbing super can kick off supers writing by holding\n\t * this mutex lock.\n\t */\n\tstruct mutex device_list_mutex;\n\tstruct list_head devices;\n\n\tstruct list_head resized_devices;\n\t/* devices not currently being allocated */\n\tstruct list_head alloc_list;\n\n\tstruct btrfs_fs_devices *seed;\n\tint seeding;\n\n\tint opened;\n\n\t/* set when we find or add a device that doesn't have the\n\t * nonrot flag set\n\t */\n\tint rotating;\n\n\tstruct btrfs_fs_info *fs_info;\n\t/* sysfs kobjects */\n\tstruct kobject fsid_kobj;\n\tstruct kobject *device_dir_kobj;\n\tstruct completion kobj_unregister;\n};\n\n#define BTRFS_BIO_INLINE_CSUM_SIZE\t64\n\n/*\n * we need the mirror number and stripe index to be passed around\n * the call chain while we are processing end_io (especially errors).\n * Really, what we need is a btrfs_bio structure that has this info\n * and is properly sized with its stripe array, but we're not there\n * quite yet.  We have our own btrfs bioset, and all of the bios\n * we allocate are actually btrfs_io_bios.  We'll cram as much of\n * struct btrfs_bio as we can into this over time.\n */\nstruct btrfs_io_bio {\n\tunsigned int mirror_num;\n\tunsigned int stripe_index;\n\tu64 logical;\n\tu8 *csum;\n\tu8 csum_inline[BTRFS_BIO_INLINE_CSUM_SIZE];\n\tstruct bvec_iter iter;\n\t/*\n\t * This member must come last, bio_alloc_bioset will allocate enough\n\t * bytes for entire btrfs_io_bio but relies on bio being last.\n\t */\n\tstruct bio bio;\n};\n\nstatic inline struct btrfs_io_bio *btrfs_io_bio(struct bio *bio)\n{\n\treturn container_of(bio, struct btrfs_io_bio, bio);\n}\n\nstatic inline void btrfs_io_bio_free_csum(struct btrfs_io_bio *io_bio)\n{\n\tif (io_bio->csum != io_bio->csum_inline) {\n\t\tkfree(io_bio->csum);\n\t\tio_bio->csum = NULL;\n\t}\n}\n\nstruct btrfs_bio_stripe {\n\tstruct btrfs_device *dev;\n\tu64 physical;\n\tu64 length; /* only used for discard mappings */\n};\n\nstruct btrfs_bio {\n\trefcount_t refs;\n\tatomic_t stripes_pending;\n\tstruct btrfs_fs_info *fs_info;\n\tu64 map_type; /* get from map_lookup->type */\n\tbio_end_io_t *end_io;\n\tstruct bio *orig_bio;\n\tunsigned long flags;\n\tvoid *private;\n\tatomic_t error;\n\tint max_errors;\n\tint num_stripes;\n\tint mirror_num;\n\tint num_tgtdevs;\n\tint *tgtdev_map;\n\t/*\n\t * logical block numbers for the start of each stripe\n\t * The last one or two are p/q.  These are sorted,\n\t * so raid_map[0] is the start of our full stripe\n\t */\n\tu64 *raid_map;\n\tstruct btrfs_bio_stripe stripes[];\n};\n\nstruct btrfs_device_info {\n\tstruct btrfs_device *dev;\n\tu64 dev_offset;\n\tu64 max_avail;\n\tu64 total_avail;\n};\n\nstruct btrfs_raid_attr {\n\tint sub_stripes;\t/* sub_stripes info for map */\n\tint dev_stripes;\t/* stripes per dev */\n\tint devs_max;\t\t/* max devs to use */\n\tint devs_min;\t\t/* min devs needed */\n\tint tolerated_failures; /* max tolerated fail devs */\n\tint devs_increment;\t/* ndevs has to be a multiple of this */\n\tint ncopies;\t\t/* how many copies to data has */\n\tint nparity;\t\t/* number of stripes worth of bytes to store\n\t\t\t\t * parity information */\n\tint mindev_error;\t/* error code if min devs requisite is unmet */\n\tconst char raid_name[8]; /* name of the raid */\n\tu64 bg_flag;\t\t/* block group flag of the raid */\n};\n\nextern const struct btrfs_raid_attr btrfs_raid_array[BTRFS_NR_RAID_TYPES];\n\nstruct map_lookup {\n\tu64 type;\n\tint io_align;\n\tint io_width;\n\tu64 stripe_len;\n\tint num_stripes;\n\tint sub_stripes;\n\tint verified_stripes; /* For mount time dev extent verification */\n\tstruct btrfs_bio_stripe stripes[];\n};\n\n#define map_lookup_size(n) (sizeof(struct map_lookup) + \\\n\t\t\t    (sizeof(struct btrfs_bio_stripe) * (n)))\n\nstruct btrfs_balance_args;\nstruct btrfs_balance_progress;\nstruct btrfs_balance_control {\n\tstruct btrfs_balance_args data;\n\tstruct btrfs_balance_args meta;\n\tstruct btrfs_balance_args sys;\n\n\tu64 flags;\n\n\tstruct btrfs_balance_progress stat;\n};\n\nenum btrfs_map_op {\n\tBTRFS_MAP_READ,\n\tBTRFS_MAP_WRITE,\n\tBTRFS_MAP_DISCARD,\n\tBTRFS_MAP_GET_READ_MIRRORS,\n};\n\nstatic inline enum btrfs_map_op btrfs_op(struct bio *bio)\n{\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\t\treturn BTRFS_MAP_DISCARD;\n\tcase REQ_OP_WRITE:\n\t\treturn BTRFS_MAP_WRITE;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\tcase REQ_OP_READ:\n\t\treturn BTRFS_MAP_READ;\n\t}\n}\n\nvoid btrfs_get_bbio(struct btrfs_bio *bbio);\nvoid btrfs_put_bbio(struct btrfs_bio *bbio);\nint btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t    u64 logical, u64 *length,\n\t\t    struct btrfs_bio **bbio_ret, int mirror_num);\nint btrfs_map_sblock(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t     u64 logical, u64 *length,\n\t\t     struct btrfs_bio **bbio_ret);\nint btrfs_rmap_block(struct btrfs_fs_info *fs_info, u64 chunk_start,\n\t\t     u64 physical, u64 **logical, int *naddrs, int *stripe_len);\nint btrfs_read_sys_array(struct btrfs_fs_info *fs_info);\nint btrfs_read_chunk_tree(struct btrfs_fs_info *fs_info);\nint btrfs_alloc_chunk(struct btrfs_trans_handle *trans, u64 type);\nvoid btrfs_mapping_init(struct btrfs_mapping_tree *tree);\nvoid btrfs_mapping_tree_free(struct btrfs_mapping_tree *tree);\nblk_status_t btrfs_map_bio(struct btrfs_fs_info *fs_info, struct bio *bio,\n\t\t\t   int mirror_num, int async_submit);\nint btrfs_open_devices(struct btrfs_fs_devices *fs_devices,\n\t\t       fmode_t flags, void *holder);\nstruct btrfs_device *btrfs_scan_one_device(const char *path,\n\t\t\t\t\t   fmode_t flags, void *holder);\nint btrfs_close_devices(struct btrfs_fs_devices *fs_devices);\nvoid btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices, int step);\nvoid btrfs_assign_next_active_device(struct btrfs_device *device,\n\t\t\t\t     struct btrfs_device *this_dev);\nstruct btrfs_device *btrfs_find_device_by_devspec(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u64 devid,\n\t\t\t\t\t\t  const char *devpath);\nstruct btrfs_device *btrfs_alloc_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tconst u64 *devid,\n\t\t\t\t\tconst u8 *uuid);\nvoid btrfs_free_device(struct btrfs_device *device);\nint btrfs_rm_device(struct btrfs_fs_info *fs_info,\n\t\t    const char *device_path, u64 devid);\nvoid __exit btrfs_cleanup_fs_uuids(void);\nint btrfs_num_copies(struct btrfs_fs_info *fs_info, u64 logical, u64 len);\nint btrfs_grow_device(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_device *device, u64 new_size);\nstruct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t       u64 devid, u8 *uuid, u8 *fsid);\nint btrfs_shrink_device(struct btrfs_device *device, u64 new_size);\nint btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *path);\nint btrfs_balance(struct btrfs_fs_info *fs_info,\n\t\t  struct btrfs_balance_control *bctl,\n\t\t  struct btrfs_ioctl_balance_args *bargs);\nvoid btrfs_describe_block_groups(u64 flags, char *buf, u32 size_buf);\nint btrfs_resume_balance_async(struct btrfs_fs_info *fs_info);\nint btrfs_recover_balance(struct btrfs_fs_info *fs_info);\nint btrfs_pause_balance(struct btrfs_fs_info *fs_info);\nint btrfs_cancel_balance(struct btrfs_fs_info *fs_info);\nint btrfs_create_uuid_tree(struct btrfs_fs_info *fs_info);\nint btrfs_check_uuid_tree(struct btrfs_fs_info *fs_info);\nint btrfs_chunk_readonly(struct btrfs_fs_info *fs_info, u64 chunk_offset);\nint find_free_dev_extent_start(struct btrfs_transaction *transaction,\n\t\t\t struct btrfs_device *device, u64 num_bytes,\n\t\t\t u64 search_start, u64 *start, u64 *max_avail);\nint find_free_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_device *device, u64 num_bytes,\n\t\t\t u64 *start, u64 *max_avail);\nvoid btrfs_dev_stat_inc_and_print(struct btrfs_device *dev, int index);\nint btrfs_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\tstruct btrfs_ioctl_get_dev_stats *stats);\nvoid btrfs_init_devices_late(struct btrfs_fs_info *fs_info);\nint btrfs_init_dev_stats(struct btrfs_fs_info *fs_info);\nint btrfs_run_dev_stats(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_fs_info *fs_info);\nvoid btrfs_rm_dev_replace_remove_srcdev(struct btrfs_device *srcdev);\nvoid btrfs_rm_dev_replace_free_srcdev(struct btrfs_fs_info *fs_info,\n\t\t\t\t      struct btrfs_device *srcdev);\nvoid btrfs_destroy_dev_replace_tgtdev(struct btrfs_device *tgtdev);\nvoid btrfs_scratch_superblocks(struct block_device *bdev, const char *device_path);\nint btrfs_is_parity_mirror(struct btrfs_fs_info *fs_info,\n\t\t\t   u64 logical, u64 len);\nunsigned long btrfs_full_stripe_len(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 logical);\nint btrfs_finish_chunk_alloc(struct btrfs_trans_handle *trans,\n\t\t\t     u64 chunk_offset, u64 chunk_size);\nint btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset);\nstruct extent_map *btrfs_get_chunk_map(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u64 logical, u64 length);\n\nstatic inline void btrfs_dev_stat_inc(struct btrfs_device *dev,\n\t\t\t\t      int index)\n{\n\tatomic_inc(dev->dev_stat_values + index);\n\t/*\n\t * This memory barrier orders stores updating statistics before stores\n\t * updating dev_stats_ccnt.\n\t *\n\t * It pairs with smp_rmb() in btrfs_run_dev_stats().\n\t */\n\tsmp_mb__before_atomic();\n\tatomic_inc(&dev->dev_stats_ccnt);\n}\n\nstatic inline int btrfs_dev_stat_read(struct btrfs_device *dev,\n\t\t\t\t      int index)\n{\n\treturn atomic_read(dev->dev_stat_values + index);\n}\n\nstatic inline int btrfs_dev_stat_read_and_reset(struct btrfs_device *dev,\n\t\t\t\t\t\tint index)\n{\n\tint ret;\n\n\tret = atomic_xchg(dev->dev_stat_values + index, 0);\n\t/*\n\t * atomic_xchg implies a full memory barriers as per atomic_t.txt:\n\t * - RMW operations that have a return value are fully ordered;\n\t *\n\t * This implicit memory barriers is paired with the smp_rmb in\n\t * btrfs_run_dev_stats\n\t */\n\tatomic_inc(&dev->dev_stats_ccnt);\n\treturn ret;\n}\n\nstatic inline void btrfs_dev_stat_set(struct btrfs_device *dev,\n\t\t\t\t      int index, unsigned long val)\n{\n\tatomic_set(dev->dev_stat_values + index, val);\n\t/*\n\t * This memory barrier orders stores updating statistics before stores\n\t * updating dev_stats_ccnt.\n\t *\n\t * It pairs with smp_rmb() in btrfs_run_dev_stats().\n\t */\n\tsmp_mb__before_atomic();\n\tatomic_inc(&dev->dev_stats_ccnt);\n}\n\nstatic inline void btrfs_dev_stat_reset(struct btrfs_device *dev,\n\t\t\t\t\tint index)\n{\n\tbtrfs_dev_stat_set(dev, index, 0);\n}\n\n/*\n * Convert block group flags (BTRFS_BLOCK_GROUP_*) to btrfs_raid_types, which\n * can be used as index to access btrfs_raid_array[].\n */\nstatic inline enum btrfs_raid_types btrfs_bg_flags_to_raid_index(u64 flags)\n{\n\tif (flags & BTRFS_BLOCK_GROUP_RAID10)\n\t\treturn BTRFS_RAID_RAID10;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID1)\n\t\treturn BTRFS_RAID_RAID1;\n\telse if (flags & BTRFS_BLOCK_GROUP_DUP)\n\t\treturn BTRFS_RAID_DUP;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID0)\n\t\treturn BTRFS_RAID_RAID0;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID5)\n\t\treturn BTRFS_RAID_RAID5;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID6)\n\t\treturn BTRFS_RAID_RAID6;\n\n\treturn BTRFS_RAID_SINGLE; /* BTRFS_BLOCK_GROUP_SINGLE */\n}\n\nconst char *get_raid_name(enum btrfs_raid_types type);\n\nvoid btrfs_update_commit_device_size(struct btrfs_fs_info *fs_info);\nvoid btrfs_update_commit_device_bytes_used(struct btrfs_transaction *trans);\n\nstruct list_head *btrfs_get_fs_uuids(void);\nvoid btrfs_set_fs_info_ptr(struct btrfs_fs_info *fs_info);\nvoid btrfs_reset_fs_info_ptr(struct btrfs_fs_info *fs_info);\nbool btrfs_check_rw_degradable(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct btrfs_device *failing_dev);\n\nint btrfs_bg_type_to_factor(u64 flags);\nint btrfs_verify_dev_extents(struct btrfs_fs_info *fs_info);\n\n#endif\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) STRATO AG 2012.  All rights reserved.\n */\n\n#include <linux/sched.h>\n#include <linux/bio.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/kthread.h>\n#include <linux/math64.h>\n#include \"ctree.h\"\n#include \"extent_map.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"async-thread.h\"\n#include \"check-integrity.h\"\n#include \"rcu-string.h\"\n#include \"dev-replace.h\"\n#include \"sysfs.h\"\n\nstatic int btrfs_dev_replace_finishing(struct btrfs_fs_info *fs_info,\n\t\t\t\t       int scrub_ret);\nstatic void btrfs_dev_replace_update_device_in_mapping_tree(\n\t\t\t\t\t\tstruct btrfs_fs_info *fs_info,\n\t\t\t\t\t\tstruct btrfs_device *srcdev,\n\t\t\t\t\t\tstruct btrfs_device *tgtdev);\nstatic int btrfs_dev_replace_kthread(void *data);\n\nint btrfs_init_dev_replace(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tint ret = 0;\n\tstruct btrfs_path *path = NULL;\n\tint item_size;\n\tstruct btrfs_dev_replace_item *ptr;\n\tu64 src_devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_DEV_REPLACE_KEY;\n\tkey.offset = 0;\n\tret = btrfs_search_slot(NULL, dev_root, &key, path, 0, 0);\n\tif (ret) {\nno_valid_dev_replace_entry_found:\n\t\tret = 0;\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_DEV_REPLACE_ITEM_STATE_NEVER_STARTED;\n\t\tdev_replace->cont_reading_from_srcdev_mode =\n\t\t    BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_ALWAYS;\n\t\tdev_replace->time_started = 0;\n\t\tdev_replace->time_stopped = 0;\n\t\tatomic64_set(&dev_replace->num_write_errors, 0);\n\t\tatomic64_set(&dev_replace->num_uncorrectable_read_errors, 0);\n\t\tdev_replace->cursor_left = 0;\n\t\tdev_replace->committed_cursor_left = 0;\n\t\tdev_replace->cursor_left_last_write_of_item = 0;\n\t\tdev_replace->cursor_right = 0;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tdev_replace->is_valid = 0;\n\t\tdev_replace->item_needs_writeback = 0;\n\t\tgoto out;\n\t}\n\tslot = path->slots[0];\n\teb = path->nodes[0];\n\titem_size = btrfs_item_size_nr(eb, slot);\n\tptr = btrfs_item_ptr(eb, slot, struct btrfs_dev_replace_item);\n\n\tif (item_size != sizeof(struct btrfs_dev_replace_item)) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t\"dev_replace entry found has unexpected size, ignore entry\");\n\t\tgoto no_valid_dev_replace_entry_found;\n\t}\n\n\tsrc_devid = btrfs_dev_replace_src_devid(eb, ptr);\n\tdev_replace->cont_reading_from_srcdev_mode =\n\t\tbtrfs_dev_replace_cont_reading_from_srcdev_mode(eb, ptr);\n\tdev_replace->replace_state = btrfs_dev_replace_replace_state(eb, ptr);\n\tdev_replace->time_started = btrfs_dev_replace_time_started(eb, ptr);\n\tdev_replace->time_stopped =\n\t\tbtrfs_dev_replace_time_stopped(eb, ptr);\n\tatomic64_set(&dev_replace->num_write_errors,\n\t\t     btrfs_dev_replace_num_write_errors(eb, ptr));\n\tatomic64_set(&dev_replace->num_uncorrectable_read_errors,\n\t\t     btrfs_dev_replace_num_uncorrectable_read_errors(eb, ptr));\n\tdev_replace->cursor_left = btrfs_dev_replace_cursor_left(eb, ptr);\n\tdev_replace->committed_cursor_left = dev_replace->cursor_left;\n\tdev_replace->cursor_left_last_write_of_item = dev_replace->cursor_left;\n\tdev_replace->cursor_right = btrfs_dev_replace_cursor_right(eb, ptr);\n\tdev_replace->is_valid = 1;\n\n\tdev_replace->item_needs_writeback = 0;\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tdev_replace->srcdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\tsrc_devid, NULL, NULL, true);\n\t\tdev_replace->tgtdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tBTRFS_DEV_REPLACE_DEVID,\n\t\t\t\t\t\t\tNULL, NULL, true);\n\t\t/*\n\t\t * allow 'btrfs dev replace_cancel' if src/tgt device is\n\t\t * missing\n\t\t */\n\t\tif (!dev_replace->srcdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"srcdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t   src_devid);\n\t\t}\n\t\tif (!dev_replace->tgtdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"tgtdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t\tBTRFS_DEV_REPLACE_DEVID);\n\t\t}\n\t\tif (dev_replace->tgtdev) {\n\t\t\tif (dev_replace->srcdev) {\n\t\t\t\tdev_replace->tgtdev->total_bytes =\n\t\t\t\t\tdev_replace->srcdev->total_bytes;\n\t\t\t\tdev_replace->tgtdev->disk_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->disk_total_bytes;\n\t\t\t\tdev_replace->tgtdev->commit_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->commit_total_bytes;\n\t\t\t\tdev_replace->tgtdev->bytes_used =\n\t\t\t\t\tdev_replace->srcdev->bytes_used;\n\t\t\t\tdev_replace->tgtdev->commit_bytes_used =\n\t\t\t\t\tdev_replace->srcdev->commit_bytes_used;\n\t\t\t}\n\t\t\tset_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\n\t\t\tWARN_ON(fs_info->fs_devices->rw_devices == 0);\n\t\t\tdev_replace->tgtdev->io_width = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->io_align = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->sector_size = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->fs_info = fs_info;\n\t\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\t\t}\n\t\tbreak;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Initialize a new device for device replace target from a given source dev\n * and path.\n *\n * Return 0 and new device in @device_out, otherwise return < 0\n */\nstatic int btrfs_init_dev_replace_tgtdev(struct btrfs_fs_info *fs_info,\n\t\t\t\t  const char *device_path,\n\t\t\t\t  struct btrfs_device *srcdev,\n\t\t\t\t  struct btrfs_device **device_out)\n{\n\tstruct btrfs_device *device;\n\tstruct block_device *bdev;\n\tstruct list_head *devices;\n\tstruct rcu_string *name;\n\tu64 devid = BTRFS_DEV_REPLACE_DEVID;\n\tint ret = 0;\n\n\t*device_out = NULL;\n\tif (fs_info->fs_devices->seeding) {\n\t\tbtrfs_err(fs_info, \"the filesystem is a seed filesystem!\");\n\t\treturn -EINVAL;\n\t}\n\n\tbdev = blkdev_get_by_path(device_path, FMODE_WRITE | FMODE_EXCL,\n\t\t\t\t  fs_info->bdev_holder);\n\tif (IS_ERR(bdev)) {\n\t\tbtrfs_err(fs_info, \"target device %s is invalid!\", device_path);\n\t\treturn PTR_ERR(bdev);\n\t}\n\n\tfilemap_write_and_wait(bdev->bd_inode->i_mapping);\n\n\tdevices = &fs_info->fs_devices->devices;\n\tlist_for_each_entry(device, devices, dev_list) {\n\t\tif (device->bdev == bdev) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t  \"target device is in the filesystem!\");\n\t\t\tret = -EEXIST;\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\n\tif (i_size_read(bdev->bd_inode) <\n\t    btrfs_device_get_total_bytes(srcdev)) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"target device is smaller than source device!\");\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\n\n\tdevice = btrfs_alloc_device(NULL, &devid, NULL);\n\tif (IS_ERR(device)) {\n\t\tret = PTR_ERR(device);\n\t\tgoto error;\n\t}\n\n\tname = rcu_string_strdup(device_path, GFP_KERNEL);\n\tif (!name) {\n\t\tbtrfs_free_device(device);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\trcu_assign_pointer(device->name, name);\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tdevice->generation = 0;\n\tdevice->io_width = fs_info->sectorsize;\n\tdevice->io_align = fs_info->sectorsize;\n\tdevice->sector_size = fs_info->sectorsize;\n\tdevice->total_bytes = btrfs_device_get_total_bytes(srcdev);\n\tdevice->disk_total_bytes = btrfs_device_get_disk_total_bytes(srcdev);\n\tdevice->bytes_used = btrfs_device_get_bytes_used(srcdev);\n\tdevice->commit_total_bytes = srcdev->commit_total_bytes;\n\tdevice->commit_bytes_used = device->bytes_used;\n\tdevice->fs_info = fs_info;\n\tdevice->bdev = bdev;\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tset_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\tdevice->mode = FMODE_EXCL;\n\tdevice->dev_stats_valid = 1;\n\tset_blocksize(device->bdev, BTRFS_BDEV_BLOCKSIZE);\n\tdevice->fs_devices = fs_info->fs_devices;\n\tlist_add(&device->dev_list, &fs_info->fs_devices->devices);\n\tfs_info->fs_devices->num_devices++;\n\tfs_info->fs_devices->open_devices++;\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t*device_out = device;\n\treturn 0;\n\nerror:\n\tblkdev_put(bdev, FMODE_EXCL);\n\treturn ret;\n}\n\n/*\n * called from commit_transaction. Writes changed device replace state to\n * disk.\n */\nint btrfs_run_dev_replace(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_fs_info *fs_info)\n{\n\tint ret;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_dev_replace_item *ptr;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tdown_read(&dev_replace->rwsem);\n\tif (!dev_replace->is_valid ||\n\t    !dev_replace->item_needs_writeback) {\n\t\tup_read(&dev_replace->rwsem);\n\t\treturn 0;\n\t}\n\tup_read(&dev_replace->rwsem);\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_DEV_REPLACE_KEY;\n\tkey.offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tret = btrfs_search_slot(trans, dev_root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t   \"error %d while searching for dev_replace item!\",\n\t\t\t   ret);\n\t\tgoto out;\n\t}\n\n\tif (ret == 0 &&\n\t    btrfs_item_size_nr(path->nodes[0], path->slots[0]) < sizeof(*ptr)) {\n\t\t/*\n\t\t * need to delete old one and insert a new one.\n\t\t * Since no attempt is made to recover any old state, if the\n\t\t * dev_replace state is 'running', the data on the target\n\t\t * drive is lost.\n\t\t * It would be possible to recover the state: just make sure\n\t\t * that the beginning of the item is never changed and always\n\t\t * contains all the essential information. Then read this\n\t\t * minimal set of information and use it as a base for the\n\t\t * new state.\n\t\t */\n\t\tret = btrfs_del_item(trans, dev_root, path);\n\t\tif (ret != 0) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"delete too small dev_replace item failed %d!\",\n\t\t\t\t   ret);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 1;\n\t}\n\n\tif (ret == 1) {\n\t\t/* need to insert a new item */\n\t\tbtrfs_release_path(path);\n\t\tret = btrfs_insert_empty_item(trans, dev_root, path,\n\t\t\t\t\t      &key, sizeof(*ptr));\n\t\tif (ret < 0) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"insert dev_replace item failed %d!\", ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\teb = path->nodes[0];\n\tptr = btrfs_item_ptr(eb, path->slots[0],\n\t\t\t     struct btrfs_dev_replace_item);\n\n\tdown_write(&dev_replace->rwsem);\n\tif (dev_replace->srcdev)\n\t\tbtrfs_set_dev_replace_src_devid(eb, ptr,\n\t\t\tdev_replace->srcdev->devid);\n\telse\n\t\tbtrfs_set_dev_replace_src_devid(eb, ptr, (u64)-1);\n\tbtrfs_set_dev_replace_cont_reading_from_srcdev_mode(eb, ptr,\n\t\tdev_replace->cont_reading_from_srcdev_mode);\n\tbtrfs_set_dev_replace_replace_state(eb, ptr,\n\t\tdev_replace->replace_state);\n\tbtrfs_set_dev_replace_time_started(eb, ptr, dev_replace->time_started);\n\tbtrfs_set_dev_replace_time_stopped(eb, ptr, dev_replace->time_stopped);\n\tbtrfs_set_dev_replace_num_write_errors(eb, ptr,\n\t\tatomic64_read(&dev_replace->num_write_errors));\n\tbtrfs_set_dev_replace_num_uncorrectable_read_errors(eb, ptr,\n\t\tatomic64_read(&dev_replace->num_uncorrectable_read_errors));\n\tdev_replace->cursor_left_last_write_of_item =\n\t\tdev_replace->cursor_left;\n\tbtrfs_set_dev_replace_cursor_left(eb, ptr,\n\t\tdev_replace->cursor_left_last_write_of_item);\n\tbtrfs_set_dev_replace_cursor_right(eb, ptr,\n\t\tdev_replace->cursor_right);\n\tdev_replace->item_needs_writeback = 0;\n\tup_write(&dev_replace->rwsem);\n\n\tbtrfs_mark_buffer_dirty(eb);\n\nout:\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\nstatic char* btrfs_dev_name(struct btrfs_device *device)\n{\n\tif (!device || test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\treturn \"<missing disk>\";\n\telse\n\t\treturn rcu_str_deref(device->name);\n}\n\nstatic int btrfs_dev_replace_start(struct btrfs_fs_info *fs_info,\n\t\tconst char *tgtdev_name, u64 srcdevid, const char *srcdev_name,\n\t\tint read_src)\n{\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tint ret;\n\tstruct btrfs_device *tgt_device = NULL;\n\tstruct btrfs_device *src_device = NULL;\n\tbool need_unlock;\n\n\tsrc_device = btrfs_find_device_by_devspec(fs_info, srcdevid,\n\t\t\t\t\t\t  srcdev_name);\n\tif (IS_ERR(src_device))\n\t\treturn PTR_ERR(src_device);\n\n\tif (btrfs_pinned_by_swapfile(fs_info, src_device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t  \"cannot replace device %s (devid %llu) due to active swapfile\",\n\t\t\tbtrfs_dev_name(src_device), src_device->devid);\n\t\treturn -ETXTBSY;\n\t}\n\n\tret = btrfs_init_dev_replace_tgtdev(fs_info, tgtdev_name,\n\t\t\t\t\t    src_device, &tgt_device);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Here we commit the transaction to make sure commit_total_bytes\n\t * of all the devices are updated.\n\t */\n\ttrans = btrfs_attach_transaction(root);\n\tif (!IS_ERR(trans)) {\n\t\tret = btrfs_commit_transaction(trans);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else if (PTR_ERR(trans) != -ENOENT) {\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tneed_unlock = true;\n\tdown_write(&dev_replace->rwsem);\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tASSERT(0);\n\t\tret = BTRFS_IOCTL_DEV_REPLACE_RESULT_ALREADY_STARTED;\n\t\tgoto leave;\n\t}\n\n\tdev_replace->cont_reading_from_srcdev_mode = read_src;\n\tWARN_ON(!src_device);\n\tdev_replace->srcdev = src_device;\n\tdev_replace->tgtdev = tgt_device;\n\n\tbtrfs_info_in_rcu(fs_info,\n\t\t      \"dev_replace from %s (devid %llu) to %s started\",\n\t\t      btrfs_dev_name(src_device),\n\t\t      src_device->devid,\n\t\t      rcu_str_deref(tgt_device->name));\n\n\t/*\n\t * from now on, the writes to the srcdev are all duplicated to\n\t * go to the tgtdev as well (refer to btrfs_map_block()).\n\t */\n\tdev_replace->replace_state = BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED;\n\tdev_replace->time_started = ktime_get_real_seconds();\n\tdev_replace->cursor_left = 0;\n\tdev_replace->committed_cursor_left = 0;\n\tdev_replace->cursor_left_last_write_of_item = 0;\n\tdev_replace->cursor_right = 0;\n\tdev_replace->is_valid = 1;\n\tdev_replace->item_needs_writeback = 1;\n\tatomic64_set(&dev_replace->num_write_errors, 0);\n\tatomic64_set(&dev_replace->num_uncorrectable_read_errors, 0);\n\tup_write(&dev_replace->rwsem);\n\tneed_unlock = false;\n\n\tret = btrfs_sysfs_add_device_link(tgt_device->fs_devices, tgt_device);\n\tif (ret)\n\t\tbtrfs_err(fs_info, \"kobj add dev failed %d\", ret);\n\n\tbtrfs_wait_ordered_roots(fs_info, U64_MAX, 0, (u64)-1);\n\n\t/* force writing the updated state information to disk */\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tneed_unlock = true;\n\t\tdown_write(&dev_replace->rwsem);\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tgoto leave;\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\tWARN_ON(ret);\n\n\t/* the disk copy procedure reuses the scrub code */\n\tret = btrfs_scrub_dev(fs_info, src_device->devid, 0,\n\t\t\t      btrfs_device_get_total_bytes(src_device),\n\t\t\t      &dev_replace->scrub_progress, 0, 1);\n\n\tret = btrfs_dev_replace_finishing(fs_info, ret);\n\tif (ret == -EINPROGRESS) {\n\t\tret = BTRFS_IOCTL_DEV_REPLACE_RESULT_SCRUB_INPROGRESS;\n\t} else if (ret != -ECANCELED) {\n\t\tWARN_ON(ret);\n\t}\n\n\treturn ret;\n\nleave:\n\tif (need_unlock)\n\t\tup_write(&dev_replace->rwsem);\n\tbtrfs_destroy_dev_replace_tgtdev(tgt_device);\n\treturn ret;\n}\n\nint btrfs_dev_replace_by_ioctl(struct btrfs_fs_info *fs_info,\n\t\t\t    struct btrfs_ioctl_dev_replace_args *args)\n{\n\tint ret;\n\n\tswitch (args->start.cont_reading_from_srcdev_mode) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_CONT_READING_FROM_SRCDEV_MODE_ALWAYS:\n\tcase BTRFS_IOCTL_DEV_REPLACE_CONT_READING_FROM_SRCDEV_MODE_AVOID:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->start.srcdevid == 0 && args->start.srcdev_name[0] == '\\0') ||\n\t    args->start.tgtdev_name[0] == '\\0')\n\t\treturn -EINVAL;\n\n\tret = btrfs_dev_replace_start(fs_info, args->start.tgtdev_name,\n\t\t\t\t\targs->start.srcdevid,\n\t\t\t\t\targs->start.srcdev_name,\n\t\t\t\t\targs->start.cont_reading_from_srcdev_mode);\n\targs->result = ret;\n\t/* don't warn if EINPROGRESS, someone else might be running scrub */\n\tif (ret == BTRFS_IOCTL_DEV_REPLACE_RESULT_SCRUB_INPROGRESS ||\n\t    ret == BTRFS_IOCTL_DEV_REPLACE_RESULT_NO_ERROR)\n\t\treturn 0;\n\n\treturn ret;\n}\n\n/*\n * blocked until all in-flight bios operations are finished.\n */\nstatic void btrfs_rm_dev_replace_blocked(struct btrfs_fs_info *fs_info)\n{\n\tset_bit(BTRFS_FS_STATE_DEV_REPLACING, &fs_info->fs_state);\n\twait_event(fs_info->dev_replace.replace_wait, !percpu_counter_sum(\n\t\t   &fs_info->dev_replace.bio_counter));\n}\n\n/*\n * we have removed target device, it is safe to allow new bios request.\n */\nstatic void btrfs_rm_dev_replace_unblocked(struct btrfs_fs_info *fs_info)\n{\n\tclear_bit(BTRFS_FS_STATE_DEV_REPLACING, &fs_info->fs_state);\n\twake_up(&fs_info->dev_replace.replace_wait);\n}\n\nstatic int btrfs_dev_replace_finishing(struct btrfs_fs_info *fs_info,\n\t\t\t\t       int scrub_ret)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct btrfs_device *tgt_device;\n\tstruct btrfs_device *src_device;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tu8 uuid_tmp[BTRFS_UUID_SIZE];\n\tstruct btrfs_trans_handle *trans;\n\tint ret = 0;\n\n\t/* don't allow cancel or unmount to disturb the finishing procedure */\n\tmutex_lock(&dev_replace->lock_finishing_cancel_unmount);\n\n\tdown_read(&dev_replace->rwsem);\n\t/* was the operation canceled, or is it finished? */\n\tif (dev_replace->replace_state !=\n\t    BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED) {\n\t\tup_read(&dev_replace->rwsem);\n\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\t\treturn 0;\n\t}\n\n\ttgt_device = dev_replace->tgtdev;\n\tsrc_device = dev_replace->srcdev;\n\tup_read(&dev_replace->rwsem);\n\n\t/*\n\t * flush all outstanding I/O and inode extent mappings before the\n\t * copy operation is declared as being finished\n\t */\n\tret = btrfs_start_delalloc_roots(fs_info, -1);\n\tif (ret) {\n\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\t\treturn ret;\n\t}\n\tbtrfs_wait_ordered_roots(fs_info, U64_MAX, 0, (u64)-1);\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\t\treturn PTR_ERR(trans);\n\t}\n\tret = btrfs_commit_transaction(trans);\n\tWARN_ON(ret);\n\n\t/* keep away write_all_supers() during the finishing procedure */\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tdown_write(&dev_replace->rwsem);\n\tdev_replace->replace_state =\n\t\tscrub_ret ? BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED\n\t\t\t  : BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED;\n\tdev_replace->tgtdev = NULL;\n\tdev_replace->srcdev = NULL;\n\tdev_replace->time_stopped = ktime_get_real_seconds();\n\tdev_replace->item_needs_writeback = 1;\n\n\t/* replace old device with new one in mapping tree */\n\tif (!scrub_ret) {\n\t\tbtrfs_dev_replace_update_device_in_mapping_tree(fs_info,\n\t\t\t\t\t\t\t\tsrc_device,\n\t\t\t\t\t\t\t\ttgt_device);\n\t} else {\n\t\tif (scrub_ret != -ECANCELED)\n\t\t\tbtrfs_err_in_rcu(fs_info,\n\t\t\t\t \"btrfs_scrub_dev(%s, %llu, %s) failed %d\",\n\t\t\t\t btrfs_dev_name(src_device),\n\t\t\t\t src_device->devid,\n\t\t\t\t rcu_str_deref(tgt_device->name), scrub_ret);\n\t\tup_write(&dev_replace->rwsem);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tbtrfs_rm_dev_replace_blocked(fs_info);\n\t\tif (tgt_device)\n\t\t\tbtrfs_destroy_dev_replace_tgtdev(tgt_device);\n\t\tbtrfs_rm_dev_replace_unblocked(fs_info);\n\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\n\t\treturn scrub_ret;\n\t}\n\n\tbtrfs_info_in_rcu(fs_info,\n\t\t\t  \"dev_replace from %s (devid %llu) to %s finished\",\n\t\t\t  btrfs_dev_name(src_device),\n\t\t\t  src_device->devid,\n\t\t\t  rcu_str_deref(tgt_device->name));\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &tgt_device->dev_state);\n\ttgt_device->devid = src_device->devid;\n\tsrc_device->devid = BTRFS_DEV_REPLACE_DEVID;\n\tmemcpy(uuid_tmp, tgt_device->uuid, sizeof(uuid_tmp));\n\tmemcpy(tgt_device->uuid, src_device->uuid, sizeof(tgt_device->uuid));\n\tmemcpy(src_device->uuid, uuid_tmp, sizeof(src_device->uuid));\n\tbtrfs_device_set_total_bytes(tgt_device, src_device->total_bytes);\n\tbtrfs_device_set_disk_total_bytes(tgt_device,\n\t\t\t\t\t  src_device->disk_total_bytes);\n\tbtrfs_device_set_bytes_used(tgt_device, src_device->bytes_used);\n\tASSERT(list_empty(&src_device->resized_list));\n\ttgt_device->commit_total_bytes = src_device->commit_total_bytes;\n\ttgt_device->commit_bytes_used = src_device->bytes_used;\n\n\tbtrfs_assign_next_active_device(src_device, tgt_device);\n\n\tlist_add(&tgt_device->dev_alloc_list, &fs_info->fs_devices->alloc_list);\n\tfs_info->fs_devices->rw_devices++;\n\n\tup_write(&dev_replace->rwsem);\n\tbtrfs_rm_dev_replace_blocked(fs_info);\n\n\tbtrfs_rm_dev_replace_remove_srcdev(src_device);\n\n\tbtrfs_rm_dev_replace_unblocked(fs_info);\n\n\t/*\n\t * Increment dev_stats_ccnt so that btrfs_run_dev_stats() will\n\t * update on-disk dev stats value during commit transaction\n\t */\n\tatomic_inc(&tgt_device->dev_stats_ccnt);\n\n\t/*\n\t * this is again a consistent state where no dev_replace procedure\n\t * is running, the target device is part of the filesystem, the\n\t * source device is not part of the filesystem anymore and its 1st\n\t * superblock is scratched out so that it is no longer marked to\n\t * belong to this filesystem.\n\t */\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t/* replace the sysfs entry */\n\tbtrfs_sysfs_rm_device_link(fs_info->fs_devices, src_device);\n\tbtrfs_rm_dev_replace_free_srcdev(fs_info, src_device);\n\n\t/* write back the superblocks */\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (!IS_ERR(trans))\n\t\tbtrfs_commit_transaction(trans);\n\n\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\n\treturn 0;\n}\n\nstatic void btrfs_dev_replace_update_device_in_mapping_tree(\n\t\t\t\t\t\tstruct btrfs_fs_info *fs_info,\n\t\t\t\t\t\tstruct btrfs_device *srcdev,\n\t\t\t\t\t\tstruct btrfs_device *tgtdev)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 start = 0;\n\tint i;\n\n\twrite_lock(&em_tree->lock);\n\tdo {\n\t\tem = lookup_extent_mapping(em_tree, start, (u64)-1);\n\t\tif (!em)\n\t\t\tbreak;\n\t\tmap = em->map_lookup;\n\t\tfor (i = 0; i < map->num_stripes; i++)\n\t\t\tif (srcdev == map->stripes[i].dev)\n\t\t\t\tmap->stripes[i].dev = tgtdev;\n\t\tstart = em->start + em->len;\n\t\tfree_extent_map(em);\n\t} while (start);\n\twrite_unlock(&em_tree->lock);\n}\n\n/*\n * Read progress of device replace status according to the state and last\n * stored position. The value format is the same as for\n * btrfs_dev_replace::progress_1000\n */\nstatic u64 btrfs_dev_replace_progress(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tu64 ret = 0;\n\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tret = 0;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\t\tret = 1000;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tret = div64_u64(dev_replace->cursor_left,\n\t\t\t\tdiv_u64(btrfs_device_get_total_bytes(\n\t\t\t\t\t\tdev_replace->srcdev), 1000));\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nvoid btrfs_dev_replace_status(struct btrfs_fs_info *fs_info,\n\t\t\t      struct btrfs_ioctl_dev_replace_args *args)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tdown_read(&dev_replace->rwsem);\n\t/* even if !dev_replace_is_valid, the values are good enough for\n\t * the replace_status ioctl */\n\targs->result = BTRFS_IOCTL_DEV_REPLACE_RESULT_NO_ERROR;\n\targs->status.replace_state = dev_replace->replace_state;\n\targs->status.time_started = dev_replace->time_started;\n\targs->status.time_stopped = dev_replace->time_stopped;\n\targs->status.num_write_errors =\n\t\tatomic64_read(&dev_replace->num_write_errors);\n\targs->status.num_uncorrectable_read_errors =\n\t\tatomic64_read(&dev_replace->num_uncorrectable_read_errors);\n\targs->status.progress_1000 = btrfs_dev_replace_progress(fs_info);\n\tup_read(&dev_replace->rwsem);\n}\n\nint btrfs_dev_replace_cancel(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct btrfs_device *tgt_device = NULL;\n\tstruct btrfs_device *src_device = NULL;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tint result;\n\tint ret;\n\n\tif (sb_rdonly(fs_info->sb))\n\t\treturn -EROFS;\n\n\tmutex_lock(&dev_replace->lock_finishing_cancel_unmount);\n\tdown_write(&dev_replace->rwsem);\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tresult = BTRFS_IOCTL_DEV_REPLACE_RESULT_NOT_STARTED;\n\t\tup_write(&dev_replace->rwsem);\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\t\ttgt_device = dev_replace->tgtdev;\n\t\tsrc_device = dev_replace->srcdev;\n\t\tup_write(&dev_replace->rwsem);\n\t\tret = btrfs_scrub_cancel(fs_info);\n\t\tif (ret < 0) {\n\t\t\tresult = BTRFS_IOCTL_DEV_REPLACE_RESULT_NOT_STARTED;\n\t\t} else {\n\t\t\tresult = BTRFS_IOCTL_DEV_REPLACE_RESULT_NO_ERROR;\n\t\t\t/*\n\t\t\t * btrfs_dev_replace_finishing() will handle the\n\t\t\t * cleanup part\n\t\t\t */\n\t\t\tbtrfs_info_in_rcu(fs_info,\n\t\t\t\t\"dev_replace from %s (devid %llu) to %s canceled\",\n\t\t\t\tbtrfs_dev_name(src_device), src_device->devid,\n\t\t\t\tbtrfs_dev_name(tgt_device));\n\t\t}\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\t/*\n\t\t * Scrub doing the replace isn't running so we need to do the\n\t\t * cleanup step of btrfs_dev_replace_finishing() here\n\t\t */\n\t\tresult = BTRFS_IOCTL_DEV_REPLACE_RESULT_NO_ERROR;\n\t\ttgt_device = dev_replace->tgtdev;\n\t\tsrc_device = dev_replace->srcdev;\n\t\tdev_replace->tgtdev = NULL;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->replace_state =\n\t\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED;\n\t\tdev_replace->time_stopped = ktime_get_real_seconds();\n\t\tdev_replace->item_needs_writeback = 1;\n\n\t\tup_write(&dev_replace->rwsem);\n\n\t\t/* Scrub for replace must not be running in suspended state */\n\t\tret = btrfs_scrub_cancel(fs_info);\n\t\tASSERT(ret != -ENOTCONN);\n\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\t\t\treturn PTR_ERR(trans);\n\t\t}\n\t\tret = btrfs_commit_transaction(trans);\n\t\tWARN_ON(ret);\n\n\t\tbtrfs_info_in_rcu(fs_info,\n\t\t\"suspended dev_replace from %s (devid %llu) to %s canceled\",\n\t\t\tbtrfs_dev_name(src_device), src_device->devid,\n\t\t\tbtrfs_dev_name(tgt_device));\n\n\t\tif (tgt_device)\n\t\t\tbtrfs_destroy_dev_replace_tgtdev(tgt_device);\n\t\tbreak;\n\tdefault:\n\t\tresult = -EINVAL;\n\t}\n\n\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n\treturn result;\n}\n\nvoid btrfs_dev_replace_suspend_for_unmount(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tmutex_lock(&dev_replace->lock_finishing_cancel_unmount);\n\tdown_write(&dev_replace->rwsem);\n\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED;\n\t\tdev_replace->time_stopped = ktime_get_real_seconds();\n\t\tdev_replace->item_needs_writeback = 1;\n\t\tbtrfs_info(fs_info, \"suspending dev_replace for unmount\");\n\t\tbreak;\n\t}\n\n\tup_write(&dev_replace->rwsem);\n\tmutex_unlock(&dev_replace->lock_finishing_cancel_unmount);\n}\n\n/* resume dev_replace procedure that was interrupted by unmount */\nint btrfs_resume_dev_replace_async(struct btrfs_fs_info *fs_info)\n{\n\tstruct task_struct *task;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tdown_write(&dev_replace->rwsem);\n\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tup_write(&dev_replace->rwsem);\n\t\treturn 0;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_STARTED;\n\t\tbreak;\n\t}\n\tif (!dev_replace->tgtdev || !dev_replace->tgtdev->bdev) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"cannot continue dev_replace, tgtdev is missing\");\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"you may cancel the operation after 'mount -o degraded'\");\n\t\tdev_replace->replace_state =\n\t\t\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED;\n\t\tup_write(&dev_replace->rwsem);\n\t\treturn 0;\n\t}\n\tup_write(&dev_replace->rwsem);\n\n\t/*\n\t * This could collide with a paused balance, but the exclusive op logic\n\t * should never allow both to start and pause. We don't want to allow\n\t * dev-replace to start anyway.\n\t */\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tdown_write(&dev_replace->rwsem);\n\t\tdev_replace->replace_state =\n\t\t\t\t\tBTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED;\n\t\tup_write(&dev_replace->rwsem);\n\t\tbtrfs_info(fs_info,\n\t\t\"cannot resume dev-replace, other exclusive operation running\");\n\t\treturn 0;\n\t}\n\n\ttask = kthread_run(btrfs_dev_replace_kthread, fs_info, \"btrfs-devrepl\");\n\treturn PTR_ERR_OR_ZERO(task);\n}\n\nstatic int btrfs_dev_replace_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tu64 progress;\n\tint ret;\n\n\tprogress = btrfs_dev_replace_progress(fs_info);\n\tprogress = div_u64(progress, 10);\n\tbtrfs_info_in_rcu(fs_info,\n\t\t\"continuing dev_replace from %s (devid %llu) to target %s @%u%%\",\n\t\tbtrfs_dev_name(dev_replace->srcdev),\n\t\tdev_replace->srcdev->devid,\n\t\tbtrfs_dev_name(dev_replace->tgtdev),\n\t\t(unsigned int)progress);\n\n\tret = btrfs_scrub_dev(fs_info, dev_replace->srcdev->devid,\n\t\t\t      dev_replace->committed_cursor_left,\n\t\t\t      btrfs_device_get_total_bytes(dev_replace->srcdev),\n\t\t\t      &dev_replace->scrub_progress, 0, 1);\n\tret = btrfs_dev_replace_finishing(fs_info, ret);\n\tWARN_ON(ret && ret != -ECANCELED);\n\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\treturn 0;\n}\n\nint btrfs_dev_replace_is_ongoing(struct btrfs_dev_replace *dev_replace)\n{\n\tif (!dev_replace->is_valid)\n\t\treturn 0;\n\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\treturn 0;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\t/*\n\t\t * return true even if tgtdev is missing (this is\n\t\t * something that can happen if the dev_replace\n\t\t * procedure is suspended by an umount and then\n\t\t * the tgtdev is missing (or \"btrfs dev scan\") was\n\t\t * not called and the filesystem is remounted\n\t\t * in degraded state. This does not stop the\n\t\t * dev_replace procedure. It needs to be canceled\n\t\t * manually if the cancellation is wanted.\n\t\t */\n\t\tbreak;\n\t}\n\treturn 1;\n}\n\nvoid btrfs_bio_counter_inc_noblocked(struct btrfs_fs_info *fs_info)\n{\n\tpercpu_counter_inc(&fs_info->dev_replace.bio_counter);\n}\n\nvoid btrfs_bio_counter_sub(struct btrfs_fs_info *fs_info, s64 amount)\n{\n\tpercpu_counter_sub(&fs_info->dev_replace.bio_counter, amount);\n\tcond_wake_up_nomb(&fs_info->dev_replace.replace_wait);\n}\n\nvoid btrfs_bio_counter_inc_blocked(struct btrfs_fs_info *fs_info)\n{\n\twhile (1) {\n\t\tpercpu_counter_inc(&fs_info->dev_replace.bio_counter);\n\t\tif (likely(!test_bit(BTRFS_FS_STATE_DEV_REPLACING,\n\t\t\t\t     &fs_info->fs_state)))\n\t\t\tbreak;\n\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\twait_event(fs_info->dev_replace.replace_wait,\n\t\t\t   !test_bit(BTRFS_FS_STATE_DEV_REPLACING,\n\t\t\t\t     &fs_info->fs_state));\n\t}\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n */\n\n#include <linux/kernel.h>\n#include <linux/bio.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/fsnotify.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/time.h>\n#include <linux/string.h>\n#include <linux/backing-dev.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/writeback.h>\n#include <linux/compat.h>\n#include <linux/security.h>\n#include <linux/xattr.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/uuid.h>\n#include <linux/btrfs.h>\n#include <linux/uaccess.h>\n#include <linux/iversion.h>\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"btrfs_inode.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"locking.h\"\n#include \"inode-map.h\"\n#include \"backref.h\"\n#include \"rcu-string.h\"\n#include \"send.h\"\n#include \"dev-replace.h\"\n#include \"props.h\"\n#include \"sysfs.h\"\n#include \"qgroup.h\"\n#include \"tree-log.h\"\n#include \"compression.h\"\n\n#ifdef CONFIG_64BIT\n/* If we have a 32-bit userspace and 64-bit kernel, then the UAPI\n * structures are incorrect, as the timespec structure from userspace\n * is 4 bytes too small. We define these alternatives here to teach\n * the kernel about the 32-bit struct packing.\n */\nstruct btrfs_ioctl_timespec_32 {\n\t__u64 sec;\n\t__u32 nsec;\n} __attribute__ ((__packed__));\n\nstruct btrfs_ioctl_received_subvol_args_32 {\n\tchar\tuuid[BTRFS_UUID_SIZE];\t/* in */\n\t__u64\tstransid;\t\t/* in */\n\t__u64\trtransid;\t\t/* out */\n\tstruct btrfs_ioctl_timespec_32 stime; /* in */\n\tstruct btrfs_ioctl_timespec_32 rtime; /* out */\n\t__u64\tflags;\t\t\t/* in */\n\t__u64\treserved[16];\t\t/* in */\n} __attribute__ ((__packed__));\n\n#define BTRFS_IOC_SET_RECEIVED_SUBVOL_32 _IOWR(BTRFS_IOCTL_MAGIC, 37, \\\n\t\t\t\tstruct btrfs_ioctl_received_subvol_args_32)\n#endif\n\n#if defined(CONFIG_64BIT) && defined(CONFIG_COMPAT)\nstruct btrfs_ioctl_send_args_32 {\n\t__s64 send_fd;\t\t\t/* in */\n\t__u64 clone_sources_count;\t/* in */\n\tcompat_uptr_t clone_sources;\t/* in */\n\t__u64 parent_root;\t\t/* in */\n\t__u64 flags;\t\t\t/* in */\n\t__u64 reserved[4];\t\t/* in */\n} __attribute__ ((__packed__));\n\n#define BTRFS_IOC_SEND_32 _IOW(BTRFS_IOCTL_MAGIC, 38, \\\n\t\t\t       struct btrfs_ioctl_send_args_32)\n#endif\n\nstatic int btrfs_clone(struct inode *src, struct inode *inode,\n\t\t       u64 off, u64 olen, u64 olen_aligned, u64 destoff,\n\t\t       int no_time_update);\n\n/* Mask out flags that are inappropriate for the given type of inode. */\nstatic unsigned int btrfs_mask_fsflags_for_type(struct inode *inode,\n\t\tunsigned int flags)\n{\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn flags;\n\telse if (S_ISREG(inode->i_mode))\n\t\treturn flags & ~FS_DIRSYNC_FL;\n\telse\n\t\treturn flags & (FS_NODUMP_FL | FS_NOATIME_FL);\n}\n\n/*\n * Export internal inode flags to the format expected by the FS_IOC_GETFLAGS\n * ioctl.\n */\nstatic unsigned int btrfs_inode_flags_to_fsflags(unsigned int flags)\n{\n\tunsigned int iflags = 0;\n\n\tif (flags & BTRFS_INODE_SYNC)\n\t\tiflags |= FS_SYNC_FL;\n\tif (flags & BTRFS_INODE_IMMUTABLE)\n\t\tiflags |= FS_IMMUTABLE_FL;\n\tif (flags & BTRFS_INODE_APPEND)\n\t\tiflags |= FS_APPEND_FL;\n\tif (flags & BTRFS_INODE_NODUMP)\n\t\tiflags |= FS_NODUMP_FL;\n\tif (flags & BTRFS_INODE_NOATIME)\n\t\tiflags |= FS_NOATIME_FL;\n\tif (flags & BTRFS_INODE_DIRSYNC)\n\t\tiflags |= FS_DIRSYNC_FL;\n\tif (flags & BTRFS_INODE_NODATACOW)\n\t\tiflags |= FS_NOCOW_FL;\n\n\tif (flags & BTRFS_INODE_NOCOMPRESS)\n\t\tiflags |= FS_NOCOMP_FL;\n\telse if (flags & BTRFS_INODE_COMPRESS)\n\t\tiflags |= FS_COMPR_FL;\n\n\treturn iflags;\n}\n\n/*\n * Update inode->i_flags based on the btrfs internal flags.\n */\nvoid btrfs_sync_inode_flags_to_i_flags(struct inode *inode)\n{\n\tstruct btrfs_inode *binode = BTRFS_I(inode);\n\tunsigned int new_fl = 0;\n\n\tif (binode->flags & BTRFS_INODE_SYNC)\n\t\tnew_fl |= S_SYNC;\n\tif (binode->flags & BTRFS_INODE_IMMUTABLE)\n\t\tnew_fl |= S_IMMUTABLE;\n\tif (binode->flags & BTRFS_INODE_APPEND)\n\t\tnew_fl |= S_APPEND;\n\tif (binode->flags & BTRFS_INODE_NOATIME)\n\t\tnew_fl |= S_NOATIME;\n\tif (binode->flags & BTRFS_INODE_DIRSYNC)\n\t\tnew_fl |= S_DIRSYNC;\n\n\tset_mask_bits(&inode->i_flags,\n\t\t      S_SYNC | S_APPEND | S_IMMUTABLE | S_NOATIME | S_DIRSYNC,\n\t\t      new_fl);\n}\n\nstatic int btrfs_ioctl_getflags(struct file *file, void __user *arg)\n{\n\tstruct btrfs_inode *binode = BTRFS_I(file_inode(file));\n\tunsigned int flags = btrfs_inode_flags_to_fsflags(binode->flags);\n\n\tif (copy_to_user(arg, &flags, sizeof(flags)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/* Check if @flags are a supported and valid set of FS_*_FL flags */\nstatic int check_fsflags(unsigned int flags)\n{\n\tif (flags & ~(FS_IMMUTABLE_FL | FS_APPEND_FL | \\\n\t\t      FS_NOATIME_FL | FS_NODUMP_FL | \\\n\t\t      FS_SYNC_FL | FS_DIRSYNC_FL | \\\n\t\t      FS_NOCOMP_FL | FS_COMPR_FL |\n\t\t      FS_NOCOW_FL))\n\t\treturn -EOPNOTSUPP;\n\n\tif ((flags & FS_NOCOMP_FL) && (flags & FS_COMPR_FL))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int btrfs_ioctl_setflags(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_inode *binode = BTRFS_I(inode);\n\tstruct btrfs_root *root = binode->root;\n\tstruct btrfs_trans_handle *trans;\n\tunsigned int fsflags, old_fsflags;\n\tint ret;\n\tu64 old_flags;\n\tunsigned int old_i_flags;\n\tumode_t mode;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EPERM;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tif (copy_from_user(&fsflags, arg, sizeof(fsflags)))\n\t\treturn -EFAULT;\n\n\tret = check_fsflags(fsflags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tinode_lock(inode);\n\n\told_flags = binode->flags;\n\told_i_flags = inode->i_flags;\n\tmode = inode->i_mode;\n\n\tfsflags = btrfs_mask_fsflags_for_type(inode, fsflags);\n\told_fsflags = btrfs_inode_flags_to_fsflags(binode->flags);\n\tif ((fsflags ^ old_fsflags) & (FS_APPEND_FL | FS_IMMUTABLE_FL)) {\n\t\tif (!capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (fsflags & FS_SYNC_FL)\n\t\tbinode->flags |= BTRFS_INODE_SYNC;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_SYNC;\n\tif (fsflags & FS_IMMUTABLE_FL)\n\t\tbinode->flags |= BTRFS_INODE_IMMUTABLE;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_IMMUTABLE;\n\tif (fsflags & FS_APPEND_FL)\n\t\tbinode->flags |= BTRFS_INODE_APPEND;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_APPEND;\n\tif (fsflags & FS_NODUMP_FL)\n\t\tbinode->flags |= BTRFS_INODE_NODUMP;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_NODUMP;\n\tif (fsflags & FS_NOATIME_FL)\n\t\tbinode->flags |= BTRFS_INODE_NOATIME;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_NOATIME;\n\tif (fsflags & FS_DIRSYNC_FL)\n\t\tbinode->flags |= BTRFS_INODE_DIRSYNC;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_DIRSYNC;\n\tif (fsflags & FS_NOCOW_FL) {\n\t\tif (S_ISREG(mode)) {\n\t\t\t/*\n\t\t\t * It's safe to turn csums off here, no extents exist.\n\t\t\t * Otherwise we want the flag to reflect the real COW\n\t\t\t * status of the file and will not set it.\n\t\t\t */\n\t\t\tif (inode->i_size == 0)\n\t\t\t\tbinode->flags |= BTRFS_INODE_NODATACOW\n\t\t\t\t\t      | BTRFS_INODE_NODATASUM;\n\t\t} else {\n\t\t\tbinode->flags |= BTRFS_INODE_NODATACOW;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * Revert back under same assumptions as above\n\t\t */\n\t\tif (S_ISREG(mode)) {\n\t\t\tif (inode->i_size == 0)\n\t\t\t\tbinode->flags &= ~(BTRFS_INODE_NODATACOW\n\t\t\t\t             | BTRFS_INODE_NODATASUM);\n\t\t} else {\n\t\t\tbinode->flags &= ~BTRFS_INODE_NODATACOW;\n\t\t}\n\t}\n\n\t/*\n\t * The COMPRESS flag can only be changed by users, while the NOCOMPRESS\n\t * flag may be changed automatically if compression code won't make\n\t * things smaller.\n\t */\n\tif (fsflags & FS_NOCOMP_FL) {\n\t\tbinode->flags &= ~BTRFS_INODE_COMPRESS;\n\t\tbinode->flags |= BTRFS_INODE_NOCOMPRESS;\n\n\t\tret = btrfs_set_prop(inode, \"btrfs.compression\", NULL, 0, 0);\n\t\tif (ret && ret != -ENODATA)\n\t\t\tgoto out_drop;\n\t} else if (fsflags & FS_COMPR_FL) {\n\t\tconst char *comp;\n\n\t\tif (IS_SWAPFILE(inode)) {\n\t\t\tret = -ETXTBSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tbinode->flags |= BTRFS_INODE_COMPRESS;\n\t\tbinode->flags &= ~BTRFS_INODE_NOCOMPRESS;\n\n\t\tcomp = btrfs_compress_type2str(fs_info->compress_type);\n\t\tif (!comp || comp[0] == 0)\n\t\t\tcomp = btrfs_compress_type2str(BTRFS_COMPRESS_ZLIB);\n\n\t\tret = btrfs_set_prop(inode, \"btrfs.compression\",\n\t\t\t\t     comp, strlen(comp), 0);\n\t\tif (ret)\n\t\t\tgoto out_drop;\n\n\t} else {\n\t\tret = btrfs_set_prop(inode, \"btrfs.compression\", NULL, 0, 0);\n\t\tif (ret && ret != -ENODATA)\n\t\t\tgoto out_drop;\n\t\tbinode->flags &= ~(BTRFS_INODE_COMPRESS | BTRFS_INODE_NOCOMPRESS);\n\t}\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_drop;\n\t}\n\n\tbtrfs_sync_inode_flags_to_i_flags(inode);\n\tinode_inc_iversion(inode);\n\tinode->i_ctime = current_time(inode);\n\tret = btrfs_update_inode(trans, root, inode);\n\n\tbtrfs_end_transaction(trans);\n out_drop:\n\tif (ret) {\n\t\tbinode->flags = old_flags;\n\t\tinode->i_flags = old_i_flags;\n\t}\n\n out_unlock:\n\tinode_unlock(inode);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\n/*\n * Translate btrfs internal inode flags to xflags as expected by the\n * FS_IOC_FSGETXATT ioctl. Filter only the supported ones, unknown flags are\n * silently dropped.\n */\nstatic unsigned int btrfs_inode_flags_to_xflags(unsigned int flags)\n{\n\tunsigned int xflags = 0;\n\n\tif (flags & BTRFS_INODE_APPEND)\n\t\txflags |= FS_XFLAG_APPEND;\n\tif (flags & BTRFS_INODE_IMMUTABLE)\n\t\txflags |= FS_XFLAG_IMMUTABLE;\n\tif (flags & BTRFS_INODE_NOATIME)\n\t\txflags |= FS_XFLAG_NOATIME;\n\tif (flags & BTRFS_INODE_NODUMP)\n\t\txflags |= FS_XFLAG_NODUMP;\n\tif (flags & BTRFS_INODE_SYNC)\n\t\txflags |= FS_XFLAG_SYNC;\n\n\treturn xflags;\n}\n\n/* Check if @flags are a supported and valid set of FS_XFLAGS_* flags */\nstatic int check_xflags(unsigned int flags)\n{\n\tif (flags & ~(FS_XFLAG_APPEND | FS_XFLAG_IMMUTABLE | FS_XFLAG_NOATIME |\n\t\t      FS_XFLAG_NODUMP | FS_XFLAG_SYNC))\n\t\treturn -EOPNOTSUPP;\n\treturn 0;\n}\n\n/*\n * Set the xflags from the internal inode flags. The remaining items of fsxattr\n * are zeroed.\n */\nstatic int btrfs_ioctl_fsgetxattr(struct file *file, void __user *arg)\n{\n\tstruct btrfs_inode *binode = BTRFS_I(file_inode(file));\n\tstruct fsxattr fa;\n\n\tmemset(&fa, 0, sizeof(fa));\n\tfa.fsx_xflags = btrfs_inode_flags_to_xflags(binode->flags);\n\n\tif (copy_to_user(arg, &fa, sizeof(fa)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int btrfs_ioctl_fssetxattr(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_inode *binode = BTRFS_I(inode);\n\tstruct btrfs_root *root = binode->root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct fsxattr fa;\n\tunsigned old_flags;\n\tunsigned old_i_flags;\n\tint ret = 0;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EPERM;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tmemset(&fa, 0, sizeof(fa));\n\tif (copy_from_user(&fa, arg, sizeof(fa)))\n\t\treturn -EFAULT;\n\n\tret = check_xflags(fa.fsx_xflags);\n\tif (ret)\n\t\treturn ret;\n\n\tif (fa.fsx_extsize != 0 || fa.fsx_projid != 0 || fa.fsx_cowextsize != 0)\n\t\treturn -EOPNOTSUPP;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tinode_lock(inode);\n\n\told_flags = binode->flags;\n\told_i_flags = inode->i_flags;\n\n\t/* We need the capabilities to change append-only or immutable inode */\n\tif (((old_flags & (BTRFS_INODE_APPEND | BTRFS_INODE_IMMUTABLE)) ||\n\t     (fa.fsx_xflags & (FS_XFLAG_APPEND | FS_XFLAG_IMMUTABLE))) &&\n\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (fa.fsx_xflags & FS_XFLAG_SYNC)\n\t\tbinode->flags |= BTRFS_INODE_SYNC;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_SYNC;\n\tif (fa.fsx_xflags & FS_XFLAG_IMMUTABLE)\n\t\tbinode->flags |= BTRFS_INODE_IMMUTABLE;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_IMMUTABLE;\n\tif (fa.fsx_xflags & FS_XFLAG_APPEND)\n\t\tbinode->flags |= BTRFS_INODE_APPEND;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_APPEND;\n\tif (fa.fsx_xflags & FS_XFLAG_NODUMP)\n\t\tbinode->flags |= BTRFS_INODE_NODUMP;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_NODUMP;\n\tif (fa.fsx_xflags & FS_XFLAG_NOATIME)\n\t\tbinode->flags |= BTRFS_INODE_NOATIME;\n\telse\n\t\tbinode->flags &= ~BTRFS_INODE_NOATIME;\n\n\t/* 1 item for the inode */\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_unlock;\n\t}\n\n\tbtrfs_sync_inode_flags_to_i_flags(inode);\n\tinode_inc_iversion(inode);\n\tinode->i_ctime = current_time(inode);\n\tret = btrfs_update_inode(trans, root, inode);\n\n\tbtrfs_end_transaction(trans);\n\nout_unlock:\n\tif (ret) {\n\t\tbinode->flags = old_flags;\n\t\tinode->i_flags = old_i_flags;\n\t}\n\n\tinode_unlock(inode);\n\tmnt_drop_write_file(file);\n\n\treturn ret;\n}\n\nstatic int btrfs_ioctl_getversion(struct file *file, int __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\n\treturn put_user(inode->i_generation, arg);\n}\n\nstatic noinline int btrfs_ioctl_fitrim(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_device *device;\n\tstruct request_queue *q;\n\tstruct fstrim_range range;\n\tu64 minlen = ULLONG_MAX;\n\tu64 num_devices = 0;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(device, &fs_info->fs_devices->devices,\n\t\t\t\tdev_list) {\n\t\tif (!device->bdev)\n\t\t\tcontinue;\n\t\tq = bdev_get_queue(device->bdev);\n\t\tif (blk_queue_discard(q)) {\n\t\t\tnum_devices++;\n\t\t\tminlen = min_t(u64, q->limits.discard_granularity,\n\t\t\t\t     minlen);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (!num_devices)\n\t\treturn -EOPNOTSUPP;\n\tif (copy_from_user(&range, arg, sizeof(range)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * NOTE: Don't truncate the range using super->total_bytes.  Bytenr of\n\t * block group is in the logical address space, which can be any\n\t * sectorsize aligned bytenr in  the range [0, U64_MAX].\n\t */\n\tif (range.len < fs_info->sb->s_blocksize)\n\t\treturn -EINVAL;\n\n\trange.minlen = max(range.minlen, minlen);\n\tret = btrfs_trim_fs(fs_info, &range);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (copy_to_user(arg, &range, sizeof(range)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nint btrfs_is_empty_uuid(u8 *uuid)\n{\n\tint i;\n\n\tfor (i = 0; i < BTRFS_UUID_SIZE; i++) {\n\t\tif (uuid[i])\n\t\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic noinline int create_subvol(struct inode *dir,\n\t\t\t\t  struct dentry *dentry,\n\t\t\t\t  const char *name, int namelen,\n\t\t\t\t  u64 *async_transid,\n\t\t\t\t  struct btrfs_qgroup_inherit *inherit)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_key key;\n\tstruct btrfs_root_item *root_item;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *new_root;\n\tstruct btrfs_block_rsv block_rsv;\n\tstruct timespec64 cur_time = current_time(dir);\n\tstruct inode *inode;\n\tint ret;\n\tint err;\n\tu64 objectid;\n\tu64 new_dirid = BTRFS_FIRST_FREE_OBJECTID;\n\tu64 index = 0;\n\tuuid_le new_uuid;\n\n\troot_item = kzalloc(sizeof(*root_item), GFP_KERNEL);\n\tif (!root_item)\n\t\treturn -ENOMEM;\n\n\tret = btrfs_find_free_objectid(fs_info->tree_root, &objectid);\n\tif (ret)\n\t\tgoto fail_free;\n\n\t/*\n\t * Don't create subvolume whose level is not zero. Or qgroup will be\n\t * screwed up since it assumes subvolume qgroup's level to be 0.\n\t */\n\tif (btrfs_qgroup_level(objectid)) {\n\t\tret = -ENOSPC;\n\t\tgoto fail_free;\n\t}\n\n\tbtrfs_init_block_rsv(&block_rsv, BTRFS_BLOCK_RSV_TEMP);\n\t/*\n\t * The same as the snapshot creation, please see the comment\n\t * of create_snapshot().\n\t */\n\tret = btrfs_subvolume_reserve_metadata(root, &block_rsv, 8, false);\n\tif (ret)\n\t\tgoto fail_free;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tbtrfs_subvolume_release_metadata(fs_info, &block_rsv);\n\t\tgoto fail_free;\n\t}\n\ttrans->block_rsv = &block_rsv;\n\ttrans->bytes_reserved = block_rsv.size;\n\n\tret = btrfs_qgroup_inherit(trans, 0, objectid, inherit);\n\tif (ret)\n\t\tgoto fail;\n\n\tleaf = btrfs_alloc_tree_block(trans, root, 0, objectid, NULL, 0, 0, 0);\n\tif (IS_ERR(leaf)) {\n\t\tret = PTR_ERR(leaf);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_mark_buffer_dirty(leaf);\n\n\tinode_item = &root_item->inode;\n\tbtrfs_set_stack_inode_generation(inode_item, 1);\n\tbtrfs_set_stack_inode_size(inode_item, 3);\n\tbtrfs_set_stack_inode_nlink(inode_item, 1);\n\tbtrfs_set_stack_inode_nbytes(inode_item,\n\t\t\t\t     fs_info->nodesize);\n\tbtrfs_set_stack_inode_mode(inode_item, S_IFDIR | 0755);\n\n\tbtrfs_set_root_flags(root_item, 0);\n\tbtrfs_set_root_limit(root_item, 0);\n\tbtrfs_set_stack_inode_flags(inode_item, BTRFS_INODE_ROOT_ITEM_INIT);\n\n\tbtrfs_set_root_bytenr(root_item, leaf->start);\n\tbtrfs_set_root_generation(root_item, trans->transid);\n\tbtrfs_set_root_level(root_item, 0);\n\tbtrfs_set_root_refs(root_item, 1);\n\tbtrfs_set_root_used(root_item, leaf->len);\n\tbtrfs_set_root_last_snapshot(root_item, 0);\n\n\tbtrfs_set_root_generation_v2(root_item,\n\t\t\tbtrfs_root_generation(root_item));\n\tuuid_le_gen(&new_uuid);\n\tmemcpy(root_item->uuid, new_uuid.b, BTRFS_UUID_SIZE);\n\tbtrfs_set_stack_timespec_sec(&root_item->otime, cur_time.tv_sec);\n\tbtrfs_set_stack_timespec_nsec(&root_item->otime, cur_time.tv_nsec);\n\troot_item->ctime = root_item->otime;\n\tbtrfs_set_root_ctransid(root_item, trans->transid);\n\tbtrfs_set_root_otransid(root_item, trans->transid);\n\n\tbtrfs_tree_unlock(leaf);\n\tfree_extent_buffer(leaf);\n\tleaf = NULL;\n\n\tbtrfs_set_root_dirid(root_item, new_dirid);\n\n\tkey.objectid = objectid;\n\tkey.offset = 0;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tret = btrfs_insert_root(trans, fs_info->tree_root, &key,\n\t\t\t\troot_item);\n\tif (ret)\n\t\tgoto fail;\n\n\tkey.offset = (u64)-1;\n\tnew_root = btrfs_read_fs_root_no_name(fs_info, &key);\n\tif (IS_ERR(new_root)) {\n\t\tret = PTR_ERR(new_root);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_record_root_in_trans(trans, new_root);\n\n\tret = btrfs_create_subvol_root(trans, new_root, root, new_dirid);\n\tif (ret) {\n\t\t/* We potentially lose an unused inode item here */\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto fail;\n\t}\n\n\tmutex_lock(&new_root->objectid_mutex);\n\tnew_root->highest_objectid = new_dirid;\n\tmutex_unlock(&new_root->objectid_mutex);\n\n\t/*\n\t * insert the directory item\n\t */\n\tret = btrfs_set_inode_index(BTRFS_I(dir), &index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_insert_dir_item(trans, name, namelen, BTRFS_I(dir), &key,\n\t\t\t\t    BTRFS_FT_DIR, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_i_size_write(BTRFS_I(dir), dir->i_size + namelen * 2);\n\tret = btrfs_update_inode(trans, root, dir);\n\tBUG_ON(ret);\n\n\tret = btrfs_add_root_ref(trans, objectid, root->root_key.objectid,\n\t\t\t\t btrfs_ino(BTRFS_I(dir)), index, name, namelen);\n\tBUG_ON(ret);\n\n\tret = btrfs_uuid_tree_add(trans, root_item->uuid,\n\t\t\t\t  BTRFS_UUID_KEY_SUBVOL, objectid);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, ret);\n\nfail:\n\tkfree(root_item);\n\ttrans->block_rsv = NULL;\n\ttrans->bytes_reserved = 0;\n\tbtrfs_subvolume_release_metadata(fs_info, &block_rsv);\n\n\tif (async_transid) {\n\t\t*async_transid = trans->transid;\n\t\terr = btrfs_commit_transaction_async(trans, 1);\n\t\tif (err)\n\t\t\terr = btrfs_commit_transaction(trans);\n\t} else {\n\t\terr = btrfs_commit_transaction(trans);\n\t}\n\tif (err && !ret)\n\t\tret = err;\n\n\tif (!ret) {\n\t\tinode = btrfs_lookup_dentry(dir, dentry);\n\t\tif (IS_ERR(inode))\n\t\t\treturn PTR_ERR(inode);\n\t\td_instantiate(dentry, inode);\n\t}\n\treturn ret;\n\nfail_free:\n\tkfree(root_item);\n\treturn ret;\n}\n\nstatic int create_snapshot(struct btrfs_root *root, struct inode *dir,\n\t\t\t   struct dentry *dentry,\n\t\t\t   u64 *async_transid, bool readonly,\n\t\t\t   struct btrfs_qgroup_inherit *inherit)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct inode *inode;\n\tstruct btrfs_pending_snapshot *pending_snapshot;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tbool snapshot_force_cow = false;\n\n\tif (!test_bit(BTRFS_ROOT_REF_COWS, &root->state))\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&root->nr_swapfiles)) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot snapshot subvolume with active swapfile\");\n\t\treturn -ETXTBSY;\n\t}\n\n\tpending_snapshot = kzalloc(sizeof(*pending_snapshot), GFP_KERNEL);\n\tif (!pending_snapshot)\n\t\treturn -ENOMEM;\n\n\tpending_snapshot->root_item = kzalloc(sizeof(struct btrfs_root_item),\n\t\t\tGFP_KERNEL);\n\tpending_snapshot->path = btrfs_alloc_path();\n\tif (!pending_snapshot->root_item || !pending_snapshot->path) {\n\t\tret = -ENOMEM;\n\t\tgoto free_pending;\n\t}\n\n\t/*\n\t * Force new buffered writes to reserve space even when NOCOW is\n\t * possible. This is to avoid later writeback (running dealloc) to\n\t * fallback to COW mode and unexpectedly fail with ENOSPC.\n\t */\n\tatomic_inc(&root->will_be_snapshotted);\n\tsmp_mb__after_atomic();\n\t/* wait for no snapshot writes */\n\twait_event(root->subv_writers->wait,\n\t\t   percpu_counter_sum(&root->subv_writers->counter) == 0);\n\n\tret = btrfs_start_delalloc_snapshot(root);\n\tif (ret)\n\t\tgoto dec_and_free;\n\n\t/*\n\t * All previous writes have started writeback in NOCOW mode, so now\n\t * we force future writes to fallback to COW mode during snapshot\n\t * creation.\n\t */\n\tatomic_inc(&root->snapshot_force_cow);\n\tsnapshot_force_cow = true;\n\n\tbtrfs_wait_ordered_extents(root, U64_MAX, 0, (u64)-1);\n\n\tbtrfs_init_block_rsv(&pending_snapshot->block_rsv,\n\t\t\t     BTRFS_BLOCK_RSV_TEMP);\n\t/*\n\t * 1 - parent dir inode\n\t * 2 - dir entries\n\t * 1 - root item\n\t * 2 - root ref/backref\n\t * 1 - root of snapshot\n\t * 1 - UUID item\n\t */\n\tret = btrfs_subvolume_reserve_metadata(BTRFS_I(dir)->root,\n\t\t\t\t\t&pending_snapshot->block_rsv, 8,\n\t\t\t\t\tfalse);\n\tif (ret)\n\t\tgoto dec_and_free;\n\n\tpending_snapshot->dentry = dentry;\n\tpending_snapshot->root = root;\n\tpending_snapshot->readonly = readonly;\n\tpending_snapshot->dir = dir;\n\tpending_snapshot->inherit = inherit;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto fail;\n\t}\n\n\tspin_lock(&fs_info->trans_lock);\n\tlist_add(&pending_snapshot->list,\n\t\t &trans->transaction->pending_snapshots);\n\tspin_unlock(&fs_info->trans_lock);\n\tif (async_transid) {\n\t\t*async_transid = trans->transid;\n\t\tret = btrfs_commit_transaction_async(trans, 1);\n\t\tif (ret)\n\t\t\tret = btrfs_commit_transaction(trans);\n\t} else {\n\t\tret = btrfs_commit_transaction(trans);\n\t}\n\tif (ret)\n\t\tgoto fail;\n\n\tret = pending_snapshot->error;\n\tif (ret)\n\t\tgoto fail;\n\n\tret = btrfs_orphan_cleanup(pending_snapshot->snap);\n\tif (ret)\n\t\tgoto fail;\n\n\tinode = btrfs_lookup_dentry(d_inode(dentry->d_parent), dentry);\n\tif (IS_ERR(inode)) {\n\t\tret = PTR_ERR(inode);\n\t\tgoto fail;\n\t}\n\n\td_instantiate(dentry, inode);\n\tret = 0;\nfail:\n\tbtrfs_subvolume_release_metadata(fs_info, &pending_snapshot->block_rsv);\ndec_and_free:\n\tif (snapshot_force_cow)\n\t\tatomic_dec(&root->snapshot_force_cow);\n\tif (atomic_dec_and_test(&root->will_be_snapshotted))\n\t\twake_up_var(&root->will_be_snapshotted);\nfree_pending:\n\tkfree(pending_snapshot->root_item);\n\tbtrfs_free_path(pending_snapshot->path);\n\tkfree(pending_snapshot);\n\n\treturn ret;\n}\n\n/*  copy of may_delete in fs/namei.c()\n *\tCheck whether we can remove a link victim from directory dir, check\n *  whether the type of victim is right.\n *  1. We can't do it if dir is read-only (done in permission())\n *  2. We should have write and exec permissions on dir\n *  3. We can't remove anything from append-only dir\n *  4. We can't do anything with immutable dir (done in permission())\n *  5. If the sticky bit on dir is set we should either\n *\ta. be owner of dir, or\n *\tb. be owner of victim, or\n *\tc. have CAP_FOWNER capability\n *  6. If the victim is append-only or immutable we can't do anything with\n *     links pointing to it.\n *  7. If we were asked to remove a directory and victim isn't one - ENOTDIR.\n *  8. If we were asked to remove a non-directory and victim isn't one - EISDIR.\n *  9. We can't remove a root or mountpoint.\n * 10. We don't allow removal of NFS sillyrenamed files; it's handled by\n *     nfs_async_unlink().\n */\n\nstatic int btrfs_may_delete(struct inode *dir, struct dentry *victim, int isdir)\n{\n\tint error;\n\n\tif (d_really_is_negative(victim))\n\t\treturn -ENOENT;\n\n\tBUG_ON(d_inode(victim->d_parent) != dir);\n\taudit_inode_child(dir, victim, AUDIT_TYPE_CHILD_DELETE);\n\n\terror = inode_permission(dir, MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\treturn error;\n\tif (IS_APPEND(dir))\n\t\treturn -EPERM;\n\tif (check_sticky(dir, d_inode(victim)) || IS_APPEND(d_inode(victim)) ||\n\t    IS_IMMUTABLE(d_inode(victim)) || IS_SWAPFILE(d_inode(victim)))\n\t\treturn -EPERM;\n\tif (isdir) {\n\t\tif (!d_is_dir(victim))\n\t\t\treturn -ENOTDIR;\n\t\tif (IS_ROOT(victim))\n\t\t\treturn -EBUSY;\n\t} else if (d_is_dir(victim))\n\t\treturn -EISDIR;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\tif (victim->d_flags & DCACHE_NFSFS_RENAMED)\n\t\treturn -EBUSY;\n\treturn 0;\n}\n\n/* copy of may_create in fs/namei.c() */\nstatic inline int btrfs_may_create(struct inode *dir, struct dentry *child)\n{\n\tif (d_really_is_positive(child))\n\t\treturn -EEXIST;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\treturn inode_permission(dir, MAY_WRITE | MAY_EXEC);\n}\n\n/*\n * Create a new subvolume below @parent.  This is largely modeled after\n * sys_mkdirat and vfs_mkdir, but we only do a single component lookup\n * inside this filesystem so it's quite a bit simpler.\n */\nstatic noinline int btrfs_mksubvol(const struct path *parent,\n\t\t\t\t   const char *name, int namelen,\n\t\t\t\t   struct btrfs_root *snap_src,\n\t\t\t\t   u64 *async_transid, bool readonly,\n\t\t\t\t   struct btrfs_qgroup_inherit *inherit)\n{\n\tstruct inode *dir = d_inode(parent->dentry);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct dentry *dentry;\n\tint error;\n\n\terror = down_write_killable_nested(&dir->i_rwsem, I_MUTEX_PARENT);\n\tif (error == -EINTR)\n\t\treturn error;\n\n\tdentry = lookup_one_len(name, parent->dentry, namelen);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto out_unlock;\n\n\terror = btrfs_may_create(dir, dentry);\n\tif (error)\n\t\tgoto out_dput;\n\n\t/*\n\t * even if this name doesn't exist, we may get hash collisions.\n\t * check for them now when we can safely fail\n\t */\n\terror = btrfs_check_dir_item_collision(BTRFS_I(dir)->root,\n\t\t\t\t\t       dir->i_ino, name,\n\t\t\t\t\t       namelen);\n\tif (error)\n\t\tgoto out_dput;\n\n\tdown_read(&fs_info->subvol_sem);\n\n\tif (btrfs_root_refs(&BTRFS_I(dir)->root->root_item) == 0)\n\t\tgoto out_up_read;\n\n\tif (snap_src) {\n\t\terror = create_snapshot(snap_src, dir, dentry,\n\t\t\t\t\tasync_transid, readonly, inherit);\n\t} else {\n\t\terror = create_subvol(dir, dentry, name, namelen,\n\t\t\t\t      async_transid, inherit);\n\t}\n\tif (!error)\n\t\tfsnotify_mkdir(dir, dentry);\nout_up_read:\n\tup_read(&fs_info->subvol_sem);\nout_dput:\n\tdput(dentry);\nout_unlock:\n\tinode_unlock(dir);\n\treturn error;\n}\n\n/*\n * When we're defragging a range, we don't want to kick it off again\n * if it is really just waiting for delalloc to send it down.\n * If we find a nice big extent or delalloc range for the bytes in the\n * file you want to defrag, we return 0 to let you know to skip this\n * part of the file\n */\nstatic int check_defrag_in_cache(struct inode *inode, u64 offset, u32 thresh)\n{\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tu64 end;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, offset, PAGE_SIZE);\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tend = extent_map_end(em);\n\t\tfree_extent_map(em);\n\t\tif (end - offset > thresh)\n\t\t\treturn 0;\n\t}\n\t/* if we already have a nice delalloc here, just stop */\n\tthresh /= 2;\n\tend = count_range_bits(io_tree, &offset, offset + thresh,\n\t\t\t       thresh, EXTENT_DELALLOC, 1);\n\tif (end >= thresh)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * helper function to walk through a file and find extents\n * newer than a specific transid, and smaller than thresh.\n *\n * This is used by the defragging code to find new and small\n * extents\n */\nstatic int find_new_extents(struct btrfs_root *root,\n\t\t\t    struct inode *inode, u64 newer_than,\n\t\t\t    u64 *off, u32 thresh)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_key min_key;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_file_extent_item *extent;\n\tint type;\n\tint ret;\n\tu64 ino = btrfs_ino(BTRFS_I(inode));\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmin_key.objectid = ino;\n\tmin_key.type = BTRFS_EXTENT_DATA_KEY;\n\tmin_key.offset = *off;\n\n\twhile (1) {\n\t\tret = btrfs_search_forward(root, &min_key, path, newer_than);\n\t\tif (ret != 0)\n\t\t\tgoto none;\nprocess_slot:\n\t\tif (min_key.objectid != ino)\n\t\t\tgoto none;\n\t\tif (min_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto none;\n\n\t\tleaf = path->nodes[0];\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_file_extent_item);\n\n\t\ttype = btrfs_file_extent_type(leaf, extent);\n\t\tif (type == BTRFS_FILE_EXTENT_REG &&\n\t\t    btrfs_file_extent_num_bytes(leaf, extent) < thresh &&\n\t\t    check_defrag_in_cache(inode, min_key.offset, thresh)) {\n\t\t\t*off = min_key.offset;\n\t\t\tbtrfs_free_path(path);\n\t\t\treturn 0;\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tif (path->slots[0] < btrfs_header_nritems(leaf)) {\n\t\t\tbtrfs_item_key_to_cpu(leaf, &min_key, path->slots[0]);\n\t\t\tgoto process_slot;\n\t\t}\n\n\t\tif (min_key.offset == (u64)-1)\n\t\t\tgoto none;\n\n\t\tmin_key.offset++;\n\t\tbtrfs_release_path(path);\n\t}\nnone:\n\tbtrfs_free_path(path);\n\treturn -ENOENT;\n}\n\nstatic struct extent_map *defrag_lookup_extent(struct inode *inode, u64 start)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em;\n\tu64 len = PAGE_SIZE;\n\n\t/*\n\t * hopefully we have this extent in the tree already, try without\n\t * the full extent lock\n\t */\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tstruct extent_state *cached = NULL;\n\t\tu64 end = start + len - 1;\n\n\t\t/* get the big lock and read metadata off disk */\n\t\tlock_extent_bits(io_tree, start, end, &cached);\n\t\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, start, len, 0);\n\t\tunlock_extent_cached(io_tree, start, end, &cached);\n\n\t\tif (IS_ERR(em))\n\t\t\treturn NULL;\n\t}\n\n\treturn em;\n}\n\nstatic bool defrag_check_next_extent(struct inode *inode, struct extent_map *em)\n{\n\tstruct extent_map *next;\n\tbool ret = true;\n\n\t/* this is the last extent */\n\tif (em->start + em->len >= i_size_read(inode))\n\t\treturn false;\n\n\tnext = defrag_lookup_extent(inode, em->start + em->len);\n\tif (!next || next->block_start >= EXTENT_MAP_LAST_BYTE)\n\t\tret = false;\n\telse if ((em->block_start + em->block_len == next->block_start) &&\n\t\t (em->block_len > SZ_128K && next->block_len > SZ_128K))\n\t\tret = false;\n\n\tfree_extent_map(next);\n\treturn ret;\n}\n\nstatic int should_defrag_range(struct inode *inode, u64 start, u32 thresh,\n\t\t\t       u64 *last_len, u64 *skip, u64 *defrag_end,\n\t\t\t       int compress)\n{\n\tstruct extent_map *em;\n\tint ret = 1;\n\tbool next_mergeable = true;\n\tbool prev_mergeable = true;\n\n\t/*\n\t * make sure that once we start defragging an extent, we keep on\n\t * defragging it\n\t */\n\tif (start < *defrag_end)\n\t\treturn 1;\n\n\t*skip = 0;\n\n\tem = defrag_lookup_extent(inode, start);\n\tif (!em)\n\t\treturn 0;\n\n\t/* this will cover holes, and inline extents */\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (!*defrag_end)\n\t\tprev_mergeable = false;\n\n\tnext_mergeable = defrag_check_next_extent(inode, em);\n\t/*\n\t * we hit a real extent, if it is big or the next extent is not a\n\t * real extent, don't bother defragging it\n\t */\n\tif (!compress && (*last_len == 0 || *last_len >= thresh) &&\n\t    (em->len >= thresh || (!next_mergeable && !prev_mergeable)))\n\t\tret = 0;\nout:\n\t/*\n\t * last_len ends up being a counter of how many bytes we've defragged.\n\t * every time we choose not to defrag an extent, we reset *last_len\n\t * so that the next tiny extent will force a defrag.\n\t *\n\t * The end result of this is that tiny extents before a single big\n\t * extent will force at least part of that big extent to be defragged.\n\t */\n\tif (ret) {\n\t\t*defrag_end = extent_map_end(em);\n\t} else {\n\t\t*last_len = 0;\n\t\t*skip = extent_map_end(em);\n\t\t*defrag_end = 0;\n\t}\n\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * it doesn't do much good to defrag one or two pages\n * at a time.  This pulls in a nice chunk of pages\n * to COW and defrag.\n *\n * It also makes sure the delalloc code has enough\n * dirty data to avoid making new small extents as part\n * of the defrag\n *\n * It's a good idea to start RA on this range\n * before calling this.\n */\nstatic int cluster_pages_for_defrag(struct inode *inode,\n\t\t\t\t    struct page **pages,\n\t\t\t\t    unsigned long start_index,\n\t\t\t\t    unsigned long num_pages)\n{\n\tunsigned long file_end;\n\tu64 isize = i_size_read(inode);\n\tu64 page_start;\n\tu64 page_end;\n\tu64 page_cnt;\n\tint ret;\n\tint i;\n\tint i_done;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tstruct extent_io_tree *tree;\n\tstruct extent_changeset *data_reserved = NULL;\n\tgfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);\n\n\tfile_end = (isize - 1) >> PAGE_SHIFT;\n\tif (!isize || start_index > file_end)\n\t\treturn 0;\n\n\tpage_cnt = min_t(u64, (u64)num_pages, (u64)file_end - start_index + 1);\n\n\tret = btrfs_delalloc_reserve_space(inode, &data_reserved,\n\t\t\tstart_index << PAGE_SHIFT,\n\t\t\tpage_cnt << PAGE_SHIFT);\n\tif (ret)\n\t\treturn ret;\n\ti_done = 0;\n\ttree = &BTRFS_I(inode)->io_tree;\n\n\t/* step one, lock all the pages */\n\tfor (i = 0; i < page_cnt; i++) {\n\t\tstruct page *page;\nagain:\n\t\tpage = find_or_create_page(inode->i_mapping,\n\t\t\t\t\t   start_index + i, mask);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tpage_start = page_offset(page);\n\t\tpage_end = page_start + PAGE_SIZE - 1;\n\t\twhile (1) {\n\t\t\tlock_extent_bits(tree, page_start, page_end,\n\t\t\t\t\t &cached_state);\n\t\t\tordered = btrfs_lookup_ordered_extent(inode,\n\t\t\t\t\t\t\t      page_start);\n\t\t\tunlock_extent_cached(tree, page_start, page_end,\n\t\t\t\t\t     &cached_state);\n\t\t\tif (!ordered)\n\t\t\t\tbreak;\n\n\t\t\tunlock_page(page);\n\t\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t\tlock_page(page);\n\t\t\t/*\n\t\t\t * we unlocked the page above, so we need check if\n\t\t\t * it was released or not.\n\t\t\t */\n\t\t\tif (page->mapping != inode->i_mapping) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\n\t\tif (!PageUptodate(page)) {\n\t\t\tbtrfs_readpage(NULL, page);\n\t\t\tlock_page(page);\n\t\t\tif (!PageUptodate(page)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page);\n\t\t\t\tret = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (page->mapping != inode->i_mapping) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto again;\n\t\t}\n\n\t\tpages[i] = page;\n\t\ti_done++;\n\t}\n\tif (!i_done || ret)\n\t\tgoto out;\n\n\tif (!(inode->i_sb->s_flags & SB_ACTIVE))\n\t\tgoto out;\n\n\t/*\n\t * so now we have a nice long stream of locked\n\t * and up to date pages, lets wait on them\n\t */\n\tfor (i = 0; i < i_done; i++)\n\t\twait_on_page_writeback(pages[i]);\n\n\tpage_start = page_offset(pages[0]);\n\tpage_end = page_offset(pages[i_done - 1]) + PAGE_SIZE;\n\n\tlock_extent_bits(&BTRFS_I(inode)->io_tree,\n\t\t\t page_start, page_end - 1, &cached_state);\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, page_start,\n\t\t\t  page_end - 1, EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0,\n\t\t\t  &cached_state);\n\n\tif (i_done != page_cnt) {\n\t\tspin_lock(&BTRFS_I(inode)->lock);\n\t\tbtrfs_mod_outstanding_extents(BTRFS_I(inode), 1);\n\t\tspin_unlock(&BTRFS_I(inode)->lock);\n\t\tbtrfs_delalloc_release_space(inode, data_reserved,\n\t\t\t\tstart_index << PAGE_SHIFT,\n\t\t\t\t(page_cnt - i_done) << PAGE_SHIFT, true);\n\t}\n\n\n\tset_extent_defrag(&BTRFS_I(inode)->io_tree, page_start, page_end - 1,\n\t\t\t  &cached_state);\n\n\tunlock_extent_cached(&BTRFS_I(inode)->io_tree,\n\t\t\t     page_start, page_end - 1, &cached_state);\n\n\tfor (i = 0; i < i_done; i++) {\n\t\tclear_page_dirty_for_io(pages[i]);\n\t\tClearPageChecked(pages[i]);\n\t\tset_page_extent_mapped(pages[i]);\n\t\tset_page_dirty(pages[i]);\n\t\tunlock_page(pages[i]);\n\t\tput_page(pages[i]);\n\t}\n\tbtrfs_delalloc_release_extents(BTRFS_I(inode), page_cnt << PAGE_SHIFT,\n\t\t\t\t       false);\n\textent_changeset_free(data_reserved);\n\treturn i_done;\nout:\n\tfor (i = 0; i < i_done; i++) {\n\t\tunlock_page(pages[i]);\n\t\tput_page(pages[i]);\n\t}\n\tbtrfs_delalloc_release_space(inode, data_reserved,\n\t\t\tstart_index << PAGE_SHIFT,\n\t\t\tpage_cnt << PAGE_SHIFT, true);\n\tbtrfs_delalloc_release_extents(BTRFS_I(inode), page_cnt << PAGE_SHIFT,\n\t\t\t\t       true);\n\textent_changeset_free(data_reserved);\n\treturn ret;\n\n}\n\nint btrfs_defrag_file(struct inode *inode, struct file *file,\n\t\t      struct btrfs_ioctl_defrag_range_args *range,\n\t\t      u64 newer_than, unsigned long max_to_defrag)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct file_ra_state *ra = NULL;\n\tunsigned long last_index;\n\tu64 isize = i_size_read(inode);\n\tu64 last_len = 0;\n\tu64 skip = 0;\n\tu64 defrag_end = 0;\n\tu64 newer_off = range->start;\n\tunsigned long i;\n\tunsigned long ra_index = 0;\n\tint ret;\n\tint defrag_count = 0;\n\tint compress_type = BTRFS_COMPRESS_ZLIB;\n\tu32 extent_thresh = range->extent_thresh;\n\tunsigned long max_cluster = SZ_256K >> PAGE_SHIFT;\n\tunsigned long cluster = max_cluster;\n\tu64 new_align = ~((u64)SZ_128K - 1);\n\tstruct page **pages = NULL;\n\tbool do_compress = range->flags & BTRFS_DEFRAG_RANGE_COMPRESS;\n\n\tif (isize == 0)\n\t\treturn 0;\n\n\tif (range->start >= isize)\n\t\treturn -EINVAL;\n\n\tif (do_compress) {\n\t\tif (range->compress_type > BTRFS_COMPRESS_TYPES)\n\t\t\treturn -EINVAL;\n\t\tif (range->compress_type)\n\t\t\tcompress_type = range->compress_type;\n\t}\n\n\tif (extent_thresh == 0)\n\t\textent_thresh = SZ_256K;\n\n\t/*\n\t * If we were not given a file, allocate a readahead context. As\n\t * readahead is just an optimization, defrag will work without it so\n\t * we don't error out.\n\t */\n\tif (!file) {\n\t\tra = kzalloc(sizeof(*ra), GFP_KERNEL);\n\t\tif (ra)\n\t\t\tfile_ra_state_init(ra, inode->i_mapping);\n\t} else {\n\t\tra = &file->f_ra;\n\t}\n\n\tpages = kmalloc_array(max_cluster, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out_ra;\n\t}\n\n\t/* find the last page to defrag */\n\tif (range->start + range->len > range->start) {\n\t\tlast_index = min_t(u64, isize - 1,\n\t\t\t range->start + range->len - 1) >> PAGE_SHIFT;\n\t} else {\n\t\tlast_index = (isize - 1) >> PAGE_SHIFT;\n\t}\n\n\tif (newer_than) {\n\t\tret = find_new_extents(root, inode, newer_than,\n\t\t\t\t       &newer_off, SZ_64K);\n\t\tif (!ret) {\n\t\t\trange->start = newer_off;\n\t\t\t/*\n\t\t\t * we always align our defrag to help keep\n\t\t\t * the extents in the file evenly spaced\n\t\t\t */\n\t\t\ti = (newer_off & new_align) >> PAGE_SHIFT;\n\t\t} else\n\t\t\tgoto out_ra;\n\t} else {\n\t\ti = range->start >> PAGE_SHIFT;\n\t}\n\tif (!max_to_defrag)\n\t\tmax_to_defrag = last_index - i + 1;\n\n\t/*\n\t * make writeback starts from i, so the defrag range can be\n\t * written sequentially.\n\t */\n\tif (i < inode->i_mapping->writeback_index)\n\t\tinode->i_mapping->writeback_index = i;\n\n\twhile (i <= last_index && defrag_count < max_to_defrag &&\n\t       (i < DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE))) {\n\t\t/*\n\t\t * make sure we stop running if someone unmounts\n\t\t * the FS\n\t\t */\n\t\tif (!(inode->i_sb->s_flags & SB_ACTIVE))\n\t\t\tbreak;\n\n\t\tif (btrfs_defrag_cancelled(fs_info)) {\n\t\t\tbtrfs_debug(fs_info, \"defrag_file cancelled\");\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!should_defrag_range(inode, (u64)i << PAGE_SHIFT,\n\t\t\t\t\t extent_thresh, &last_len, &skip,\n\t\t\t\t\t &defrag_end, do_compress)){\n\t\t\tunsigned long next;\n\t\t\t/*\n\t\t\t * the should_defrag function tells us how much to skip\n\t\t\t * bump our counter by the suggested amount\n\t\t\t */\n\t\t\tnext = DIV_ROUND_UP(skip, PAGE_SIZE);\n\t\t\ti = max(i + 1, next);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!newer_than) {\n\t\t\tcluster = (PAGE_ALIGN(defrag_end) >>\n\t\t\t\t   PAGE_SHIFT) - i;\n\t\t\tcluster = min(cluster, max_cluster);\n\t\t} else {\n\t\t\tcluster = max_cluster;\n\t\t}\n\n\t\tif (i + cluster > ra_index) {\n\t\t\tra_index = max(i, ra_index);\n\t\t\tif (ra)\n\t\t\t\tpage_cache_sync_readahead(inode->i_mapping, ra,\n\t\t\t\t\t\tfile, ra_index, cluster);\n\t\t\tra_index += cluster;\n\t\t}\n\n\t\tinode_lock(inode);\n\t\tif (IS_SWAPFILE(inode)) {\n\t\t\tret = -ETXTBSY;\n\t\t} else {\n\t\t\tif (do_compress)\n\t\t\t\tBTRFS_I(inode)->defrag_compress = compress_type;\n\t\t\tret = cluster_pages_for_defrag(inode, pages, i, cluster);\n\t\t}\n\t\tif (ret < 0) {\n\t\t\tinode_unlock(inode);\n\t\t\tgoto out_ra;\n\t\t}\n\n\t\tdefrag_count += ret;\n\t\tbalance_dirty_pages_ratelimited(inode->i_mapping);\n\t\tinode_unlock(inode);\n\n\t\tif (newer_than) {\n\t\t\tif (newer_off == (u64)-1)\n\t\t\t\tbreak;\n\n\t\t\tif (ret > 0)\n\t\t\t\ti += ret;\n\n\t\t\tnewer_off = max(newer_off + 1,\n\t\t\t\t\t(u64)i << PAGE_SHIFT);\n\n\t\t\tret = find_new_extents(root, inode, newer_than,\n\t\t\t\t\t       &newer_off, SZ_64K);\n\t\t\tif (!ret) {\n\t\t\t\trange->start = newer_off;\n\t\t\t\ti = (newer_off & new_align) >> PAGE_SHIFT;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (ret > 0) {\n\t\t\t\ti += ret;\n\t\t\t\tlast_len += ret << PAGE_SHIFT;\n\t\t\t} else {\n\t\t\t\ti++;\n\t\t\t\tlast_len = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((range->flags & BTRFS_DEFRAG_RANGE_START_IO)) {\n\t\tfilemap_flush(inode->i_mapping);\n\t\tif (test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,\n\t\t\t     &BTRFS_I(inode)->runtime_flags))\n\t\t\tfilemap_flush(inode->i_mapping);\n\t}\n\n\tif (range->compress_type == BTRFS_COMPRESS_LZO) {\n\t\tbtrfs_set_fs_incompat(fs_info, COMPRESS_LZO);\n\t} else if (range->compress_type == BTRFS_COMPRESS_ZSTD) {\n\t\tbtrfs_set_fs_incompat(fs_info, COMPRESS_ZSTD);\n\t}\n\n\tret = defrag_count;\n\nout_ra:\n\tif (do_compress) {\n\t\tinode_lock(inode);\n\t\tBTRFS_I(inode)->defrag_compress = BTRFS_COMPRESS_NONE;\n\t\tinode_unlock(inode);\n\t}\n\tif (!file)\n\t\tkfree(ra);\n\tkfree(pages);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_resize(struct file *file,\n\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tu64 new_size;\n\tu64 old_size;\n\tu64 devid = 1;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device = NULL;\n\tchar *sizestr;\n\tchar *retptr;\n\tchar *devstr = NULL;\n\tint ret = 0;\n\tint mod = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tmnt_drop_write_file(file);\n\t\treturn BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t}\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tsizestr = vol_args->name;\n\tdevstr = strchr(sizestr, ':');\n\tif (devstr) {\n\t\tsizestr = devstr + 1;\n\t\t*devstr = '\\0';\n\t\tdevstr = vol_args->name;\n\t\tret = kstrtoull(devstr, 10, &devid);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\tif (!devid) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tbtrfs_info(fs_info, \"resizing devid %llu\", devid);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!device) {\n\t\tbtrfs_info(fs_info, \"resizer unable to find device %llu\",\n\t\t\t   devid);\n\t\tret = -ENODEV;\n\t\tgoto out_free;\n\t}\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"resizer unable to apply on readonly device %llu\",\n\t\t       devid);\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\tif (!strcmp(sizestr, \"max\"))\n\t\tnew_size = device->bdev->bd_inode->i_size;\n\telse {\n\t\tif (sizestr[0] == '-') {\n\t\t\tmod = -1;\n\t\t\tsizestr++;\n\t\t} else if (sizestr[0] == '+') {\n\t\t\tmod = 1;\n\t\t\tsizestr++;\n\t\t}\n\t\tnew_size = memparse(sizestr, &retptr);\n\t\tif (*retptr != '\\0' || new_size == 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\told_size = btrfs_device_get_total_bytes(device);\n\n\tif (mod < 0) {\n\t\tif (new_size > old_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size - new_size;\n\t} else if (mod > 0) {\n\t\tif (new_size > ULLONG_MAX - old_size) {\n\t\t\tret = -ERANGE;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size + new_size;\n\t}\n\n\tif (new_size < SZ_256M) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (new_size > device->bdev->bd_inode->i_size) {\n\t\tret = -EFBIG;\n\t\tgoto out_free;\n\t}\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tbtrfs_info_in_rcu(fs_info, \"new size for %s is %llu\",\n\t\t\t  rcu_str_deref(device->name), new_size);\n\n\tif (new_size > old_size) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = btrfs_grow_device(trans, device, new_size);\n\t\tbtrfs_commit_transaction(trans);\n\t} else if (new_size < old_size) {\n\t\tret = btrfs_shrink_device(device, new_size);\n\t} /* equal, nothing need to do */\n\nout_free:\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create_transid(struct file *file,\n\t\t\t\tconst char *name, unsigned long fd, int subvol,\n\t\t\t\tu64 *transid, bool readonly,\n\t\t\t\tstruct btrfs_qgroup_inherit *inherit)\n{\n\tint namelen;\n\tint ret = 0;\n\n\tif (!S_ISDIR(file_inode(file)->i_mode))\n\t\treturn -ENOTDIR;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tnamelen = strlen(name);\n\tif (strchr(name, '/')) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (name[0] == '.' &&\n\t   (namelen == 1 || (name[1] == '.' && namelen == 2))) {\n\t\tret = -EEXIST;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (subvol) {\n\t\tret = btrfs_mksubvol(&file->f_path, name, namelen,\n\t\t\t\t     NULL, transid, readonly, inherit);\n\t} else {\n\t\tstruct fd src = fdget(fd);\n\t\tstruct inode *src_inode;\n\t\tif (!src.file) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_drop_write;\n\t\t}\n\n\t\tsrc_inode = file_inode(src.file);\n\t\tif (src_inode->i_sb != file_inode(file)->i_sb) {\n\t\t\tbtrfs_info(BTRFS_I(file_inode(file))->root->fs_info,\n\t\t\t\t   \"Snapshot src from another FS\");\n\t\t\tret = -EXDEV;\n\t\t} else if (!inode_owner_or_capable(src_inode)) {\n\t\t\t/*\n\t\t\t * Subvolume creation is not restricted, but snapshots\n\t\t\t * are limited to own subvolumes only\n\t\t\t */\n\t\t\tret = -EPERM;\n\t\t} else {\n\t\t\tret = btrfs_mksubvol(&file->f_path, name, namelen,\n\t\t\t\t\t     BTRFS_I(src_inode)->root,\n\t\t\t\t\t     transid, readonly, inherit);\n\t\t}\n\t\tfdput(src);\n\t}\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create(struct file *file,\n\t\t\t\t\t    void __user *arg, int subvol)\n{\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!S_ISDIR(file_inode(file)->i_mode))\n\t\treturn -ENOTDIR;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tret = btrfs_ioctl_snap_create_transid(file, vol_args->name,\n\t\t\t\t\t      vol_args->fd, subvol,\n\t\t\t\t\t      NULL, false, NULL);\n\n\tkfree(vol_args);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create_v2(struct file *file,\n\t\t\t\t\t       void __user *arg, int subvol)\n{\n\tstruct btrfs_ioctl_vol_args_v2 *vol_args;\n\tint ret;\n\tu64 transid = 0;\n\tu64 *ptr = NULL;\n\tbool readonly = false;\n\tstruct btrfs_qgroup_inherit *inherit = NULL;\n\n\tif (!S_ISDIR(file_inode(file)->i_mode))\n\t\treturn -ENOTDIR;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\tvol_args->name[BTRFS_SUBVOL_NAME_MAX] = '\\0';\n\n\tif (vol_args->flags &\n\t    ~(BTRFS_SUBVOL_CREATE_ASYNC | BTRFS_SUBVOL_RDONLY |\n\t      BTRFS_SUBVOL_QGROUP_INHERIT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto free_args;\n\t}\n\n\tif (vol_args->flags & BTRFS_SUBVOL_CREATE_ASYNC)\n\t\tptr = &transid;\n\tif (vol_args->flags & BTRFS_SUBVOL_RDONLY)\n\t\treadonly = true;\n\tif (vol_args->flags & BTRFS_SUBVOL_QGROUP_INHERIT) {\n\t\tif (vol_args->size > PAGE_SIZE) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_args;\n\t\t}\n\t\tinherit = memdup_user(vol_args->qgroup_inherit, vol_args->size);\n\t\tif (IS_ERR(inherit)) {\n\t\t\tret = PTR_ERR(inherit);\n\t\t\tgoto free_args;\n\t\t}\n\t}\n\n\tret = btrfs_ioctl_snap_create_transid(file, vol_args->name,\n\t\t\t\t\t      vol_args->fd, subvol, ptr,\n\t\t\t\t\t      readonly, inherit);\n\tif (ret)\n\t\tgoto free_inherit;\n\n\tif (ptr && copy_to_user(arg +\n\t\t\t\toffsetof(struct btrfs_ioctl_vol_args_v2,\n\t\t\t\t\ttransid),\n\t\t\t\tptr, sizeof(*ptr)))\n\t\tret = -EFAULT;\n\nfree_inherit:\n\tkfree(inherit);\nfree_args:\n\tkfree(vol_args);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_subvol_getflags(struct file *file,\n\t\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret = 0;\n\tu64 flags = 0;\n\n\tif (btrfs_ino(BTRFS_I(inode)) != BTRFS_FIRST_FREE_OBJECTID)\n\t\treturn -EINVAL;\n\n\tdown_read(&fs_info->subvol_sem);\n\tif (btrfs_root_readonly(root))\n\t\tflags |= BTRFS_SUBVOL_RDONLY;\n\tup_read(&fs_info->subvol_sem);\n\n\tif (copy_to_user(arg, &flags, sizeof(flags)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_subvol_setflags(struct file *file,\n\t\t\t\t\t      void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tu64 root_flags;\n\tu64 flags;\n\tint ret = 0;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tif (btrfs_ino(BTRFS_I(inode)) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (copy_from_user(&flags, arg, sizeof(flags))) {\n\t\tret = -EFAULT;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (flags & BTRFS_SUBVOL_CREATE_ASYNC) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (flags & ~BTRFS_SUBVOL_RDONLY) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_drop_write;\n\t}\n\n\tdown_write(&fs_info->subvol_sem);\n\n\t/* nothing to do */\n\tif (!!(flags & BTRFS_SUBVOL_RDONLY) == btrfs_root_readonly(root))\n\t\tgoto out_drop_sem;\n\n\troot_flags = btrfs_root_flags(&root->root_item);\n\tif (flags & BTRFS_SUBVOL_RDONLY) {\n\t\tbtrfs_set_root_flags(&root->root_item,\n\t\t\t\t     root_flags | BTRFS_ROOT_SUBVOL_RDONLY);\n\t} else {\n\t\t/*\n\t\t * Block RO -> RW transition if this subvolume is involved in\n\t\t * send\n\t\t */\n\t\tspin_lock(&root->root_item_lock);\n\t\tif (root->send_in_progress == 0) {\n\t\t\tbtrfs_set_root_flags(&root->root_item,\n\t\t\t\t     root_flags & ~BTRFS_ROOT_SUBVOL_RDONLY);\n\t\t\tspin_unlock(&root->root_item_lock);\n\t\t} else {\n\t\t\tspin_unlock(&root->root_item_lock);\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"Attempt to set subvolume %llu read-write during send\",\n\t\t\t\t   root->root_key.objectid);\n\t\t\tret = -EPERM;\n\t\t\tgoto out_drop_sem;\n\t\t}\n\t}\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_reset;\n\t}\n\n\tret = btrfs_update_root(trans, fs_info->tree_root,\n\t\t\t\t&root->root_key, &root->root_item);\n\tif (ret < 0) {\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out_reset;\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\nout_reset:\n\tif (ret)\n\t\tbtrfs_set_root_flags(&root->root_item, root_flags);\nout_drop_sem:\n\tup_write(&fs_info->subvol_sem);\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\nstatic noinline int key_in_sk(struct btrfs_key *key,\n\t\t\t      struct btrfs_ioctl_search_key *sk)\n{\n\tstruct btrfs_key test;\n\tint ret;\n\n\ttest.objectid = sk->min_objectid;\n\ttest.type = sk->min_type;\n\ttest.offset = sk->min_offset;\n\n\tret = btrfs_comp_cpu_keys(key, &test);\n\tif (ret < 0)\n\t\treturn 0;\n\n\ttest.objectid = sk->max_objectid;\n\ttest.type = sk->max_type;\n\ttest.offset = sk->max_offset;\n\n\tret = btrfs_comp_cpu_keys(key, &test);\n\tif (ret > 0)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic noinline int copy_to_sk(struct btrfs_path *path,\n\t\t\t       struct btrfs_key *key,\n\t\t\t       struct btrfs_ioctl_search_key *sk,\n\t\t\t       size_t *buf_size,\n\t\t\t       char __user *ubuf,\n\t\t\t       unsigned long *sk_offset,\n\t\t\t       int *num_found)\n{\n\tu64 found_transid;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_ioctl_search_header sh;\n\tstruct btrfs_key test;\n\tunsigned long item_off;\n\tunsigned long item_len;\n\tint nritems;\n\tint i;\n\tint slot;\n\tint ret = 0;\n\n\tleaf = path->nodes[0];\n\tslot = path->slots[0];\n\tnritems = btrfs_header_nritems(leaf);\n\n\tif (btrfs_header_generation(leaf) > sk->max_transid) {\n\t\ti = nritems;\n\t\tgoto advance_key;\n\t}\n\tfound_transid = btrfs_header_generation(leaf);\n\n\tfor (i = slot; i < nritems; i++) {\n\t\titem_off = btrfs_item_ptr_offset(leaf, i);\n\t\titem_len = btrfs_item_size_nr(leaf, i);\n\n\t\tbtrfs_item_key_to_cpu(leaf, key, i);\n\t\tif (!key_in_sk(key, sk))\n\t\t\tcontinue;\n\n\t\tif (sizeof(sh) + item_len > *buf_size) {\n\t\t\tif (*num_found) {\n\t\t\t\tret = 1;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * return one empty item back for v1, which does not\n\t\t\t * handle -EOVERFLOW\n\t\t\t */\n\n\t\t\t*buf_size = sizeof(sh) + item_len;\n\t\t\titem_len = 0;\n\t\t\tret = -EOVERFLOW;\n\t\t}\n\n\t\tif (sizeof(sh) + item_len + *sk_offset > *buf_size) {\n\t\t\tret = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsh.objectid = key->objectid;\n\t\tsh.offset = key->offset;\n\t\tsh.type = key->type;\n\t\tsh.len = item_len;\n\t\tsh.transid = found_transid;\n\n\t\t/* copy search result header */\n\t\tif (copy_to_user(ubuf + *sk_offset, &sh, sizeof(sh))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\t*sk_offset += sizeof(sh);\n\n\t\tif (item_len) {\n\t\t\tchar __user *up = ubuf + *sk_offset;\n\t\t\t/* copy the item */\n\t\t\tif (read_extent_buffer_to_user(leaf, up,\n\t\t\t\t\t\t       item_off, item_len)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t*sk_offset += item_len;\n\t\t}\n\t\t(*num_found)++;\n\n\t\tif (ret) /* -EOVERFLOW from above */\n\t\t\tgoto out;\n\n\t\tif (*num_found >= sk->nr_items) {\n\t\t\tret = 1;\n\t\t\tgoto out;\n\t\t}\n\t}\nadvance_key:\n\tret = 0;\n\ttest.objectid = sk->max_objectid;\n\ttest.type = sk->max_type;\n\ttest.offset = sk->max_offset;\n\tif (btrfs_comp_cpu_keys(key, &test) >= 0)\n\t\tret = 1;\n\telse if (key->offset < (u64)-1)\n\t\tkey->offset++;\n\telse if (key->type < (u8)-1) {\n\t\tkey->offset = 0;\n\t\tkey->type++;\n\t} else if (key->objectid < (u64)-1) {\n\t\tkey->offset = 0;\n\t\tkey->type = 0;\n\t\tkey->objectid++;\n\t} else\n\t\tret = 1;\nout:\n\t/*\n\t *  0: all items from this leaf copied, continue with next\n\t *  1: * more items can be copied, but unused buffer is too small\n\t *     * all items were found\n\t *     Either way, it will stops the loop which iterates to the next\n\t *     leaf\n\t *  -EOVERFLOW: item was to large for buffer\n\t *  -EFAULT: could not copy extent buffer back to userspace\n\t */\n\treturn ret;\n}\n\nstatic noinline int search_ioctl(struct inode *inode,\n\t\t\t\t struct btrfs_ioctl_search_key *sk,\n\t\t\t\t size_t *buf_size,\n\t\t\t\t char __user *ubuf)\n{\n\tstruct btrfs_fs_info *info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path;\n\tint ret;\n\tint num_found = 0;\n\tunsigned long sk_offset = 0;\n\n\tif (*buf_size < sizeof(struct btrfs_ioctl_search_header)) {\n\t\t*buf_size = sizeof(struct btrfs_ioctl_search_header);\n\t\treturn -EOVERFLOW;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tif (sk->tree_id == 0) {\n\t\t/* search the root of the inode that was passed */\n\t\troot = BTRFS_I(inode)->root;\n\t} else {\n\t\tkey.objectid = sk->tree_id;\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\t\troot = btrfs_read_fs_root_no_name(info, &key);\n\t\tif (IS_ERR(root)) {\n\t\t\tbtrfs_free_path(path);\n\t\t\treturn PTR_ERR(root);\n\t\t}\n\t}\n\n\tkey.objectid = sk->min_objectid;\n\tkey.type = sk->min_type;\n\tkey.offset = sk->min_offset;\n\n\twhile (1) {\n\t\tret = btrfs_search_forward(root, &key, path, sk->min_transid);\n\t\tif (ret != 0) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = 0;\n\t\t\tgoto err;\n\t\t}\n\t\tret = copy_to_sk(path, &key, sk, buf_size, ubuf,\n\t\t\t\t &sk_offset, &num_found);\n\t\tbtrfs_release_path(path);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t}\n\tif (ret > 0)\n\t\tret = 0;\nerr:\n\tsk->nr_items = num_found;\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_tree_search(struct file *file,\n\t\t\t\t\t   void __user *argp)\n{\n\tstruct btrfs_ioctl_search_args __user *uargs;\n\tstruct btrfs_ioctl_search_key sk;\n\tstruct inode *inode;\n\tint ret;\n\tsize_t buf_size;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tuargs = (struct btrfs_ioctl_search_args __user *)argp;\n\n\tif (copy_from_user(&sk, &uargs->key, sizeof(sk)))\n\t\treturn -EFAULT;\n\n\tbuf_size = sizeof(uargs->buf);\n\n\tinode = file_inode(file);\n\tret = search_ioctl(inode, &sk, &buf_size, uargs->buf);\n\n\t/*\n\t * In the origin implementation an overflow is handled by returning a\n\t * search header with a len of zero, so reset ret.\n\t */\n\tif (ret == -EOVERFLOW)\n\t\tret = 0;\n\n\tif (ret == 0 && copy_to_user(&uargs->key, &sk, sizeof(sk)))\n\t\tret = -EFAULT;\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_tree_search_v2(struct file *file,\n\t\t\t\t\t       void __user *argp)\n{\n\tstruct btrfs_ioctl_search_args_v2 __user *uarg;\n\tstruct btrfs_ioctl_search_args_v2 args;\n\tstruct inode *inode;\n\tint ret;\n\tsize_t buf_size;\n\tconst size_t buf_limit = SZ_16M;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* copy search header and buffer size */\n\tuarg = (struct btrfs_ioctl_search_args_v2 __user *)argp;\n\tif (copy_from_user(&args, uarg, sizeof(args)))\n\t\treturn -EFAULT;\n\n\tbuf_size = args.buf_size;\n\n\t/* limit result size to 16MB */\n\tif (buf_size > buf_limit)\n\t\tbuf_size = buf_limit;\n\n\tinode = file_inode(file);\n\tret = search_ioctl(inode, &args.key, &buf_size,\n\t\t\t   (char __user *)(&uarg->buf[0]));\n\tif (ret == 0 && copy_to_user(&uarg->key, &args.key, sizeof(args.key)))\n\t\tret = -EFAULT;\n\telse if (ret == -EOVERFLOW &&\n\t\tcopy_to_user(&uarg->buf_size, &buf_size, sizeof(buf_size)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\n/*\n * Search INODE_REFs to identify path name of 'dirid' directory\n * in a 'tree_id' tree. and sets path name to 'name'.\n */\nstatic noinline int btrfs_search_path_in_tree(struct btrfs_fs_info *info,\n\t\t\t\tu64 tree_id, u64 dirid, char *name)\n{\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tchar *ptr;\n\tint ret = -1;\n\tint slot;\n\tint len;\n\tint total_len = 0;\n\tstruct btrfs_inode_ref *iref;\n\tstruct extent_buffer *l;\n\tstruct btrfs_path *path;\n\n\tif (dirid == BTRFS_FIRST_FREE_OBJECTID) {\n\t\tname[0]='\\0';\n\t\treturn 0;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tptr = &name[BTRFS_INO_LOOKUP_PATH_MAX - 1];\n\n\tkey.objectid = tree_id;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\troot = btrfs_read_fs_root_no_name(info, &key);\n\tif (IS_ERR(root)) {\n\t\tret = PTR_ERR(root);\n\t\tgoto out;\n\t}\n\n\tkey.objectid = dirid;\n\tkey.type = BTRFS_INODE_REF_KEY;\n\tkey.offset = (u64)-1;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\telse if (ret > 0) {\n\t\t\tret = btrfs_previous_item(root, path, dirid,\n\t\t\t\t\t\t  BTRFS_INODE_REF_KEY);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\telse if (ret > 0) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\tiref = btrfs_item_ptr(l, slot, struct btrfs_inode_ref);\n\t\tlen = btrfs_inode_ref_name_len(l, iref);\n\t\tptr -= len + 1;\n\t\ttotal_len += len + 1;\n\t\tif (ptr < name) {\n\t\t\tret = -ENAMETOOLONG;\n\t\t\tgoto out;\n\t\t}\n\n\t\t*(ptr + len) = '/';\n\t\tread_extent_buffer(l, ptr, (unsigned long)(iref + 1), len);\n\n\t\tif (key.offset == BTRFS_FIRST_FREE_OBJECTID)\n\t\t\tbreak;\n\n\t\tbtrfs_release_path(path);\n\t\tkey.objectid = key.offset;\n\t\tkey.offset = (u64)-1;\n\t\tdirid = key.objectid;\n\t}\n\tmemmove(name, ptr, total_len);\n\tname[total_len] = '\\0';\n\tret = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_search_path_in_tree_user(struct inode *inode,\n\t\t\t\tstruct btrfs_ioctl_ino_lookup_user_args *args)\n{\n\tstruct btrfs_fs_info *fs_info = BTRFS_I(inode)->root->fs_info;\n\tstruct super_block *sb = inode->i_sb;\n\tstruct btrfs_key upper_limit = BTRFS_I(inode)->location;\n\tu64 treeid = BTRFS_I(inode)->root->root_key.objectid;\n\tu64 dirid = args->dirid;\n\tunsigned long item_off;\n\tunsigned long item_len;\n\tstruct btrfs_inode_ref *iref;\n\tstruct btrfs_root_ref *rref;\n\tstruct btrfs_root *root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key, key2;\n\tstruct extent_buffer *leaf;\n\tstruct inode *temp_inode;\n\tchar *ptr;\n\tint slot;\n\tint len;\n\tint total_len = 0;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * If the bottom subvolume does not exist directly under upper_limit,\n\t * construct the path in from the bottom up.\n\t */\n\tif (dirid != upper_limit.objectid) {\n\t\tptr = &args->path[BTRFS_INO_LOOKUP_USER_PATH_MAX - 1];\n\n\t\tkey.objectid = treeid;\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\t\troot = btrfs_read_fs_root_no_name(fs_info, &key);\n\t\tif (IS_ERR(root)) {\n\t\t\tret = PTR_ERR(root);\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey.objectid = dirid;\n\t\tkey.type = BTRFS_INODE_REF_KEY;\n\t\tkey.offset = (u64)-1;\n\t\twhile (1) {\n\t\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\t\tif (ret < 0) {\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tret = btrfs_previous_item(root, path, dirid,\n\t\t\t\t\t\t\t  BTRFS_INODE_REF_KEY);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tgoto out;\n\t\t\t\t} else if (ret > 0) {\n\t\t\t\t\tret = -ENOENT;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tleaf = path->nodes[0];\n\t\t\tslot = path->slots[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t\tiref = btrfs_item_ptr(leaf, slot, struct btrfs_inode_ref);\n\t\t\tlen = btrfs_inode_ref_name_len(leaf, iref);\n\t\t\tptr -= len + 1;\n\t\t\ttotal_len += len + 1;\n\t\t\tif (ptr < args->path) {\n\t\t\t\tret = -ENAMETOOLONG;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t*(ptr + len) = '/';\n\t\t\tread_extent_buffer(leaf, ptr,\n\t\t\t\t\t(unsigned long)(iref + 1), len);\n\n\t\t\t/* Check the read+exec permission of this directory */\n\t\t\tret = btrfs_previous_item(root, path, dirid,\n\t\t\t\t\t\t  BTRFS_INODE_ITEM_KEY);\n\t\t\tif (ret < 0) {\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tleaf = path->nodes[0];\n\t\t\tslot = path->slots[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key2, slot);\n\t\t\tif (key2.objectid != dirid) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\ttemp_inode = btrfs_iget(sb, &key2, root, NULL);\n\t\t\tif (IS_ERR(temp_inode)) {\n\t\t\t\tret = PTR_ERR(temp_inode);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = inode_permission(temp_inode, MAY_READ | MAY_EXEC);\n\t\t\tiput(temp_inode);\n\t\t\tif (ret) {\n\t\t\t\tret = -EACCES;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (key.offset == upper_limit.objectid)\n\t\t\t\tbreak;\n\t\t\tif (key.objectid == BTRFS_FIRST_FREE_OBJECTID) {\n\t\t\t\tret = -EACCES;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tbtrfs_release_path(path);\n\t\t\tkey.objectid = key.offset;\n\t\t\tkey.offset = (u64)-1;\n\t\t\tdirid = key.objectid;\n\t\t}\n\n\t\tmemmove(args->path, ptr, total_len);\n\t\targs->path[total_len] = '\\0';\n\t\tbtrfs_release_path(path);\n\t}\n\n\t/* Get the bottom subvolume's name from ROOT_REF */\n\troot = fs_info->tree_root;\n\tkey.objectid = treeid;\n\tkey.type = BTRFS_ROOT_REF_KEY;\n\tkey.offset = args->treeid;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0) {\n\t\tgoto out;\n\t} else if (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tslot = path->slots[0];\n\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\titem_off = btrfs_item_ptr_offset(leaf, slot);\n\titem_len = btrfs_item_size_nr(leaf, slot);\n\t/* Check if dirid in ROOT_REF corresponds to passed dirid */\n\trref = btrfs_item_ptr(leaf, slot, struct btrfs_root_ref);\n\tif (args->dirid != btrfs_root_ref_dirid(leaf, rref)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Copy subvolume's name */\n\titem_off += sizeof(struct btrfs_root_ref);\n\titem_len -= sizeof(struct btrfs_root_ref);\n\tread_extent_buffer(leaf, args->name, item_off, item_len);\n\targs->name[item_len] = 0;\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_ino_lookup(struct file *file,\n\t\t\t\t\t   void __user *argp)\n{\n\tstruct btrfs_ioctl_ino_lookup_args *args;\n\tstruct inode *inode;\n\tint ret = 0;\n\n\targs = memdup_user(argp, sizeof(*args));\n\tif (IS_ERR(args))\n\t\treturn PTR_ERR(args);\n\n\tinode = file_inode(file);\n\n\t/*\n\t * Unprivileged query to obtain the containing subvolume root id. The\n\t * path is reset so it's consistent with btrfs_search_path_in_tree.\n\t */\n\tif (args->treeid == 0)\n\t\targs->treeid = BTRFS_I(inode)->root->root_key.objectid;\n\n\tif (args->objectid == BTRFS_FIRST_FREE_OBJECTID) {\n\t\targs->name[0] = 0;\n\t\tgoto out;\n\t}\n\n\tif (!capable(CAP_SYS_ADMIN)) {\n\t\tret = -EPERM;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_search_path_in_tree(BTRFS_I(inode)->root->fs_info,\n\t\t\t\t\targs->treeid, args->objectid,\n\t\t\t\t\targs->name);\n\nout:\n\tif (ret == 0 && copy_to_user(argp, args, sizeof(*args)))\n\t\tret = -EFAULT;\n\n\tkfree(args);\n\treturn ret;\n}\n\n/*\n * Version of ino_lookup ioctl (unprivileged)\n *\n * The main differences from ino_lookup ioctl are:\n *\n *   1. Read + Exec permission will be checked using inode_permission() during\n *      path construction. -EACCES will be returned in case of failure.\n *   2. Path construction will be stopped at the inode number which corresponds\n *      to the fd with which this ioctl is called. If constructed path does not\n *      exist under fd's inode, -EACCES will be returned.\n *   3. The name of bottom subvolume is also searched and filled.\n */\nstatic int btrfs_ioctl_ino_lookup_user(struct file *file, void __user *argp)\n{\n\tstruct btrfs_ioctl_ino_lookup_user_args *args;\n\tstruct inode *inode;\n\tint ret;\n\n\targs = memdup_user(argp, sizeof(*args));\n\tif (IS_ERR(args))\n\t\treturn PTR_ERR(args);\n\n\tinode = file_inode(file);\n\n\tif (args->dirid == BTRFS_FIRST_FREE_OBJECTID &&\n\t    BTRFS_I(inode)->location.objectid != BTRFS_FIRST_FREE_OBJECTID) {\n\t\t/*\n\t\t * The subvolume does not exist under fd with which this is\n\t\t * called\n\t\t */\n\t\tkfree(args);\n\t\treturn -EACCES;\n\t}\n\n\tret = btrfs_search_path_in_tree_user(inode, args);\n\n\tif (ret == 0 && copy_to_user(argp, args, sizeof(*args)))\n\t\tret = -EFAULT;\n\n\tkfree(args);\n\treturn ret;\n}\n\n/* Get the subvolume information in BTRFS_ROOT_ITEM and BTRFS_ROOT_BACKREF */\nstatic int btrfs_ioctl_get_subvol_info(struct file *file, void __user *argp)\n{\n\tstruct btrfs_ioctl_get_subvol_info_args *subvol_info;\n\tstruct btrfs_fs_info *fs_info;\n\tstruct btrfs_root *root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_root_item *root_item;\n\tstruct btrfs_root_ref *rref;\n\tstruct extent_buffer *leaf;\n\tunsigned long item_off;\n\tunsigned long item_len;\n\tstruct inode *inode;\n\tint slot;\n\tint ret = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tsubvol_info = kzalloc(sizeof(*subvol_info), GFP_KERNEL);\n\tif (!subvol_info) {\n\t\tbtrfs_free_path(path);\n\t\treturn -ENOMEM;\n\t}\n\n\tinode = file_inode(file);\n\tfs_info = BTRFS_I(inode)->root->fs_info;\n\n\t/* Get root_item of inode's subvolume */\n\tkey.objectid = BTRFS_I(inode)->root->root_key.objectid;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\troot = btrfs_read_fs_root_no_name(fs_info, &key);\n\tif (IS_ERR(root)) {\n\t\tret = PTR_ERR(root);\n\t\tgoto out;\n\t}\n\troot_item = &root->root_item;\n\n\tsubvol_info->treeid = key.objectid;\n\n\tsubvol_info->generation = btrfs_root_generation(root_item);\n\tsubvol_info->flags = btrfs_root_flags(root_item);\n\n\tmemcpy(subvol_info->uuid, root_item->uuid, BTRFS_UUID_SIZE);\n\tmemcpy(subvol_info->parent_uuid, root_item->parent_uuid,\n\t\t\t\t\t\t    BTRFS_UUID_SIZE);\n\tmemcpy(subvol_info->received_uuid, root_item->received_uuid,\n\t\t\t\t\t\t    BTRFS_UUID_SIZE);\n\n\tsubvol_info->ctransid = btrfs_root_ctransid(root_item);\n\tsubvol_info->ctime.sec = btrfs_stack_timespec_sec(&root_item->ctime);\n\tsubvol_info->ctime.nsec = btrfs_stack_timespec_nsec(&root_item->ctime);\n\n\tsubvol_info->otransid = btrfs_root_otransid(root_item);\n\tsubvol_info->otime.sec = btrfs_stack_timespec_sec(&root_item->otime);\n\tsubvol_info->otime.nsec = btrfs_stack_timespec_nsec(&root_item->otime);\n\n\tsubvol_info->stransid = btrfs_root_stransid(root_item);\n\tsubvol_info->stime.sec = btrfs_stack_timespec_sec(&root_item->stime);\n\tsubvol_info->stime.nsec = btrfs_stack_timespec_nsec(&root_item->stime);\n\n\tsubvol_info->rtransid = btrfs_root_rtransid(root_item);\n\tsubvol_info->rtime.sec = btrfs_stack_timespec_sec(&root_item->rtime);\n\tsubvol_info->rtime.nsec = btrfs_stack_timespec_nsec(&root_item->rtime);\n\n\tif (key.objectid != BTRFS_FS_TREE_OBJECTID) {\n\t\t/* Search root tree for ROOT_BACKREF of this subvolume */\n\t\troot = fs_info->tree_root;\n\n\t\tkey.type = BTRFS_ROOT_BACKREF_KEY;\n\t\tkey.offset = 0;\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tgoto out;\n\t\t} else if (path->slots[0] >=\n\t\t\t   btrfs_header_nritems(path->nodes[0])) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.objectid == subvol_info->treeid &&\n\t\t    key.type == BTRFS_ROOT_BACKREF_KEY) {\n\t\t\tsubvol_info->parent_id = key.offset;\n\n\t\t\trref = btrfs_item_ptr(leaf, slot, struct btrfs_root_ref);\n\t\t\tsubvol_info->dirid = btrfs_root_ref_dirid(leaf, rref);\n\n\t\t\titem_off = btrfs_item_ptr_offset(leaf, slot)\n\t\t\t\t\t+ sizeof(struct btrfs_root_ref);\n\t\t\titem_len = btrfs_item_size_nr(leaf, slot)\n\t\t\t\t\t- sizeof(struct btrfs_root_ref);\n\t\t\tread_extent_buffer(leaf, subvol_info->name,\n\t\t\t\t\t   item_off, item_len);\n\t\t} else {\n\t\t\tret = -ENOENT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (copy_to_user(argp, subvol_info, sizeof(*subvol_info)))\n\t\tret = -EFAULT;\n\nout:\n\tbtrfs_free_path(path);\n\tkzfree(subvol_info);\n\treturn ret;\n}\n\n/*\n * Return ROOT_REF information of the subvolume containing this inode\n * except the subvolume name.\n */\nstatic int btrfs_ioctl_get_subvol_rootref(struct file *file, void __user *argp)\n{\n\tstruct btrfs_ioctl_get_subvol_rootref_args *rootrefs;\n\tstruct btrfs_root_ref *rref;\n\tstruct btrfs_root *root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *leaf;\n\tstruct inode *inode;\n\tu64 objectid;\n\tint slot;\n\tint ret;\n\tu8 found;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\trootrefs = memdup_user(argp, sizeof(*rootrefs));\n\tif (IS_ERR(rootrefs)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(rootrefs);\n\t}\n\n\tinode = file_inode(file);\n\troot = BTRFS_I(inode)->root->fs_info->tree_root;\n\tobjectid = BTRFS_I(inode)->root->root_key.objectid;\n\n\tkey.objectid = objectid;\n\tkey.type = BTRFS_ROOT_REF_KEY;\n\tkey.offset = rootrefs->min_treeid;\n\tfound = 0;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0) {\n\t\tgoto out;\n\t} else if (path->slots[0] >=\n\t\t   btrfs_header_nritems(path->nodes[0])) {\n\t\tret = btrfs_next_leaf(root, path);\n\t\tif (ret < 0) {\n\t\t\tgoto out;\n\t\t} else if (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\twhile (1) {\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.objectid != objectid || key.type != BTRFS_ROOT_REF_KEY) {\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (found == BTRFS_MAX_ROOTREF_BUFFER_NUM) {\n\t\t\tret = -EOVERFLOW;\n\t\t\tgoto out;\n\t\t}\n\n\t\trref = btrfs_item_ptr(leaf, slot, struct btrfs_root_ref);\n\t\trootrefs->rootref[found].treeid = key.offset;\n\t\trootrefs->rootref[found].dirid =\n\t\t\t\t  btrfs_root_ref_dirid(leaf, rref);\n\t\tfound++;\n\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0) {\n\t\t\tgoto out;\n\t\t} else if (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tif (!ret || ret == -EOVERFLOW) {\n\t\trootrefs->num_items = found;\n\t\t/* update min_treeid for next search */\n\t\tif (found)\n\t\t\trootrefs->min_treeid =\n\t\t\t\trootrefs->rootref[found - 1].treeid + 1;\n\t\tif (copy_to_user(argp, rootrefs, sizeof(*rootrefs)))\n\t\t\tret = -EFAULT;\n\t}\n\n\tkfree(rootrefs);\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_destroy(struct file *file,\n\t\t\t\t\t     void __user *arg)\n{\n\tstruct dentry *parent = file->f_path.dentry;\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(parent->d_sb);\n\tstruct dentry *dentry;\n\tstruct inode *dir = d_inode(parent);\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *dest = NULL;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint namelen;\n\tint err = 0;\n\n\tif (!S_ISDIR(dir->i_mode))\n\t\treturn -ENOTDIR;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tnamelen = strlen(vol_args->name);\n\tif (strchr(vol_args->name, '/') ||\n\t    strncmp(vol_args->name, \"..\", namelen) == 0) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = mnt_want_write_file(file);\n\tif (err)\n\t\tgoto out;\n\n\n\terr = down_write_killable_nested(&dir->i_rwsem, I_MUTEX_PARENT);\n\tif (err == -EINTR)\n\t\tgoto out_drop_write;\n\tdentry = lookup_one_len(vol_args->name, parent, namelen);\n\tif (IS_ERR(dentry)) {\n\t\terr = PTR_ERR(dentry);\n\t\tgoto out_unlock_dir;\n\t}\n\n\tif (d_really_is_negative(dentry)) {\n\t\terr = -ENOENT;\n\t\tgoto out_dput;\n\t}\n\n\tinode = d_inode(dentry);\n\tdest = BTRFS_I(inode)->root;\n\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t/*\n\t\t * Regular user.  Only allow this with a special mount\n\t\t * option, when the user has write+exec access to the\n\t\t * subvol root, and when rmdir(2) would have been\n\t\t * allowed.\n\t\t *\n\t\t * Note that this is _not_ check that the subvol is\n\t\t * empty or doesn't contain data that we wouldn't\n\t\t * otherwise be able to delete.\n\t\t *\n\t\t * Users who want to delete empty subvols should try\n\t\t * rmdir(2).\n\t\t */\n\t\terr = -EPERM;\n\t\tif (!btrfs_test_opt(fs_info, USER_SUBVOL_RM_ALLOWED))\n\t\t\tgoto out_dput;\n\n\t\t/*\n\t\t * Do not allow deletion if the parent dir is the same\n\t\t * as the dir to be deleted.  That means the ioctl\n\t\t * must be called on the dentry referencing the root\n\t\t * of the subvol, not a random directory contained\n\t\t * within it.\n\t\t */\n\t\terr = -EINVAL;\n\t\tif (root == dest)\n\t\t\tgoto out_dput;\n\n\t\terr = inode_permission(inode, MAY_WRITE | MAY_EXEC);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\n\t/* check if subvolume may be deleted by a user */\n\terr = btrfs_may_delete(dir, dentry, 1);\n\tif (err)\n\t\tgoto out_dput;\n\n\tif (btrfs_ino(BTRFS_I(inode)) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\terr = -EINVAL;\n\t\tgoto out_dput;\n\t}\n\n\tinode_lock(inode);\n\terr = btrfs_delete_subvolume(dir, dentry);\n\tinode_unlock(inode);\n\tif (!err)\n\t\td_delete(dentry);\n\nout_dput:\n\tdput(dentry);\nout_unlock_dir:\n\tinode_unlock(dir);\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\tkfree(vol_args);\n\treturn err;\n}\n\nstatic int btrfs_ioctl_defrag(struct file *file, void __user *argp)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_defrag_range_args *range;\n\tint ret;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (btrfs_root_readonly(root)) {\n\t\tret = -EROFS;\n\t\tgoto out;\n\t}\n\n\tswitch (inode->i_mode & S_IFMT) {\n\tcase S_IFDIR:\n\t\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = btrfs_defrag_root(root);\n\t\tbreak;\n\tcase S_IFREG:\n\t\t/*\n\t\t * Note that this does not check the file descriptor for write\n\t\t * access. This prevents defragmenting executables that are\n\t\t * running and allows defrag on files open in read-only mode.\n\t\t */\n\t\tif (!capable(CAP_SYS_ADMIN) &&\n\t\t    inode_permission(inode, MAY_WRITE)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\n\t\trange = kzalloc(sizeof(*range), GFP_KERNEL);\n\t\tif (!range) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (argp) {\n\t\t\tif (copy_from_user(range, argp,\n\t\t\t\t\t   sizeof(*range))) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tkfree(range);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t/* compression requires us to start the IO */\n\t\t\tif ((range->flags & BTRFS_DEFRAG_RANGE_COMPRESS)) {\n\t\t\t\trange->flags |= BTRFS_DEFRAG_RANGE_START_IO;\n\t\t\t\trange->extent_thresh = (u32)-1;\n\t\t\t}\n\t\t} else {\n\t\t\t/* the rest are all set to zero by kzalloc */\n\t\t\trange->len = (u64)-1;\n\t\t}\n\t\tret = btrfs_defrag_file(file_inode(file), file,\n\t\t\t\t\trange, BTRFS_OLDEST_GENERATION, 0);\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t\tkfree(range);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\nout:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_add_dev(struct btrfs_fs_info *fs_info, void __user *arg)\n{\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags))\n\t\treturn BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tret = btrfs_init_new_device(fs_info, vol_args->name);\n\n\tif (!ret)\n\t\tbtrfs_info(fs_info, \"disk added %s\", vol_args->name);\n\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_rm_dev_v2(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_vol_args_v2 *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto err_drop;\n\t}\n\n\t/* Check for compatibility reject unknown flags */\n\tif (vol_args->flags & ~BTRFS_VOL_ARG_V2_FLAGS_SUPPORTED) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tret = BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t\tgoto out;\n\t}\n\n\tif (vol_args->flags & BTRFS_DEVICE_SPEC_BY_ID) {\n\t\tret = btrfs_rm_device(fs_info, NULL, vol_args->devid);\n\t} else {\n\t\tvol_args->name[BTRFS_SUBVOL_NAME_MAX] = '\\0';\n\t\tret = btrfs_rm_device(fs_info, vol_args->name, 0);\n\t}\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\n\tif (!ret) {\n\t\tif (vol_args->flags & BTRFS_DEVICE_SPEC_BY_ID)\n\t\t\tbtrfs_info(fs_info, \"device deleted: id %llu\",\n\t\t\t\t\tvol_args->devid);\n\t\telse\n\t\t\tbtrfs_info(fs_info, \"device deleted: %s\",\n\t\t\t\t\tvol_args->name);\n\t}\nout:\n\tkfree(vol_args);\nerr_drop:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_rm_dev(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tret = BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t\tgoto out_drop_write;\n\t}\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tret = btrfs_rm_device(fs_info, vol_args->name, 0);\n\n\tif (!ret)\n\t\tbtrfs_info(fs_info, \"disk deleted %s\", vol_args->name);\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\nout_drop_write:\n\tmnt_drop_write_file(file);\n\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_fs_info(struct btrfs_fs_info *fs_info,\n\t\t\t\tvoid __user *arg)\n{\n\tstruct btrfs_ioctl_fs_info_args *fi_args;\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tint ret = 0;\n\n\tfi_args = kzalloc(sizeof(*fi_args), GFP_KERNEL);\n\tif (!fi_args)\n\t\treturn -ENOMEM;\n\n\trcu_read_lock();\n\tfi_args->num_devices = fs_devices->num_devices;\n\n\tlist_for_each_entry_rcu(device, &fs_devices->devices, dev_list) {\n\t\tif (device->devid > fi_args->max_id)\n\t\t\tfi_args->max_id = device->devid;\n\t}\n\trcu_read_unlock();\n\n\tmemcpy(&fi_args->fsid, fs_devices->fsid, sizeof(fi_args->fsid));\n\tfi_args->nodesize = fs_info->nodesize;\n\tfi_args->sectorsize = fs_info->sectorsize;\n\tfi_args->clone_alignment = fs_info->sectorsize;\n\n\tif (copy_to_user(arg, fi_args, sizeof(*fi_args)))\n\t\tret = -EFAULT;\n\n\tkfree(fi_args);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_dev_info(struct btrfs_fs_info *fs_info,\n\t\t\t\t void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_info_args *di_args;\n\tstruct btrfs_device *dev;\n\tint ret = 0;\n\tchar *s_uuid = NULL;\n\n\tdi_args = memdup_user(arg, sizeof(*di_args));\n\tif (IS_ERR(di_args))\n\t\treturn PTR_ERR(di_args);\n\n\tif (!btrfs_is_empty_uuid(di_args->uuid))\n\t\ts_uuid = di_args->uuid;\n\n\trcu_read_lock();\n\tdev = btrfs_find_device(fs_info->fs_devices, di_args->devid, s_uuid,\n\t\t\t\tNULL, true);\n\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tdi_args->devid = dev->devid;\n\tdi_args->bytes_used = btrfs_device_get_bytes_used(dev);\n\tdi_args->total_bytes = btrfs_device_get_total_bytes(dev);\n\tmemcpy(di_args->uuid, dev->uuid, sizeof(di_args->uuid));\n\tif (dev->name) {\n\t\tstrncpy(di_args->path, rcu_str_deref(dev->name),\n\t\t\t\tsizeof(di_args->path) - 1);\n\t\tdi_args->path[sizeof(di_args->path) - 1] = 0;\n\t} else {\n\t\tdi_args->path[0] = '\\0';\n\t}\n\nout:\n\trcu_read_unlock();\n\tif (ret == 0 && copy_to_user(arg, di_args, sizeof(*di_args)))\n\t\tret = -EFAULT;\n\n\tkfree(di_args);\n\treturn ret;\n}\n\nstatic void btrfs_double_inode_unlock(struct inode *inode1, struct inode *inode2)\n{\n\tinode_unlock(inode1);\n\tinode_unlock(inode2);\n}\n\nstatic void btrfs_double_inode_lock(struct inode *inode1, struct inode *inode2)\n{\n\tif (inode1 < inode2)\n\t\tswap(inode1, inode2);\n\n\tinode_lock_nested(inode1, I_MUTEX_PARENT);\n\tinode_lock_nested(inode2, I_MUTEX_CHILD);\n}\n\nstatic void btrfs_double_extent_unlock(struct inode *inode1, u64 loff1,\n\t\t\t\t       struct inode *inode2, u64 loff2, u64 len)\n{\n\tunlock_extent(&BTRFS_I(inode1)->io_tree, loff1, loff1 + len - 1);\n\tunlock_extent(&BTRFS_I(inode2)->io_tree, loff2, loff2 + len - 1);\n}\n\nstatic void btrfs_double_extent_lock(struct inode *inode1, u64 loff1,\n\t\t\t\t     struct inode *inode2, u64 loff2, u64 len)\n{\n\tif (inode1 < inode2) {\n\t\tswap(inode1, inode2);\n\t\tswap(loff1, loff2);\n\t} else if (inode1 == inode2 && loff2 < loff1) {\n\t\tswap(loff1, loff2);\n\t}\n\tlock_extent(&BTRFS_I(inode1)->io_tree, loff1, loff1 + len - 1);\n\tlock_extent(&BTRFS_I(inode2)->io_tree, loff2, loff2 + len - 1);\n}\n\nstatic int btrfs_extent_same_range(struct inode *src, u64 loff, u64 olen,\n\t\t\t\t   struct inode *dst, u64 dst_loff)\n{\n\tu64 bs = BTRFS_I(src)->root->fs_info->sb->s_blocksize;\n\tint ret;\n\tu64 len = olen;\n\n\tif (loff + len == src->i_size)\n\t\tlen = ALIGN(src->i_size, bs) - loff;\n\t/*\n\t * For same inode case we don't want our length pushed out past i_size\n\t * as comparing that data range makes no sense.\n\t *\n\t * This effectively means we require aligned extents for the single\n\t * inode case, whereas the other cases allow an unaligned length so long\n\t * as it ends at i_size.\n\t */\n\tif (dst == src && len != olen)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Lock destination range to serialize with concurrent readpages() and\n\t * source range to serialize with relocation.\n\t */\n\tbtrfs_double_extent_lock(src, loff, dst, dst_loff, len);\n\tret = btrfs_clone(src, dst, loff, olen, len, dst_loff, 1);\n\tbtrfs_double_extent_unlock(src, loff, dst, dst_loff, len);\n\n\treturn ret;\n}\n\n#define BTRFS_MAX_DEDUPE_LEN\tSZ_16M\n\nstatic int btrfs_extent_same(struct inode *src, u64 loff, u64 olen,\n\t\t\t     struct inode *dst, u64 dst_loff)\n{\n\tint ret;\n\tu64 i, tail_len, chunk_count;\n\n\ttail_len = olen % BTRFS_MAX_DEDUPE_LEN;\n\tchunk_count = div_u64(olen, BTRFS_MAX_DEDUPE_LEN);\n\n\tfor (i = 0; i < chunk_count; i++) {\n\t\tret = btrfs_extent_same_range(src, loff, BTRFS_MAX_DEDUPE_LEN,\n\t\t\t\t\t      dst, dst_loff);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tloff += BTRFS_MAX_DEDUPE_LEN;\n\t\tdst_loff += BTRFS_MAX_DEDUPE_LEN;\n\t}\n\n\tif (tail_len > 0)\n\t\tret = btrfs_extent_same_range(src, loff, tail_len, dst,\n\t\t\t\t\t      dst_loff);\n\n\treturn ret;\n}\n\nstatic int clone_finish_inode_update(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     u64 endoff,\n\t\t\t\t     const u64 destoff,\n\t\t\t\t     const u64 olen,\n\t\t\t\t     int no_time_update)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\tinode_inc_iversion(inode);\n\tif (!no_time_update)\n\t\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\t/*\n\t * We round up to the block size at eof when determining which\n\t * extents to clone above, but shouldn't round up the file size.\n\t */\n\tif (endoff > destoff + olen)\n\t\tendoff = destoff + olen;\n\tif (endoff > inode->i_size)\n\t\tbtrfs_i_size_write(BTRFS_I(inode), endoff);\n\n\tret = btrfs_update_inode(trans, root, inode);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out;\n\t}\n\tret = btrfs_end_transaction(trans);\nout:\n\treturn ret;\n}\n\nstatic void clone_update_extent_map(struct btrfs_inode *inode,\n\t\t\t\t    const struct btrfs_trans_handle *trans,\n\t\t\t\t    const struct btrfs_path *path,\n\t\t\t\t    const u64 hole_offset,\n\t\t\t\t    const u64 hole_len)\n{\n\tstruct extent_map_tree *em_tree = &inode->extent_tree;\n\tstruct extent_map *em;\n\tint ret;\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &inode->runtime_flags);\n\t\treturn;\n\t}\n\n\tif (path) {\n\t\tstruct btrfs_file_extent_item *fi;\n\n\t\tfi = btrfs_item_ptr(path->nodes[0], path->slots[0],\n\t\t\t\t    struct btrfs_file_extent_item);\n\t\tbtrfs_extent_item_to_extent_map(inode, path, fi, false, em);\n\t\tem->generation = -1;\n\t\tif (btrfs_file_extent_type(path->nodes[0], fi) ==\n\t\t    BTRFS_FILE_EXTENT_INLINE)\n\t\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC,\n\t\t\t\t\t&inode->runtime_flags);\n\t} else {\n\t\tem->start = hole_offset;\n\t\tem->len = hole_len;\n\t\tem->ram_bytes = em->len;\n\t\tem->orig_start = hole_offset;\n\t\tem->block_start = EXTENT_MAP_HOLE;\n\t\tem->block_len = 0;\n\t\tem->orig_block_len = 0;\n\t\tem->compress_type = BTRFS_COMPRESS_NONE;\n\t\tem->generation = trans->transid;\n\t}\n\n\twhile (1) {\n\t\twrite_lock(&em_tree->lock);\n\t\tret = add_extent_mapping(em_tree, em, 1);\n\t\twrite_unlock(&em_tree->lock);\n\t\tif (ret != -EEXIST) {\n\t\t\tfree_extent_map(em);\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_drop_extent_cache(inode, em->start,\n\t\t\t\t\tem->start + em->len - 1, 0);\n\t}\n\n\tif (ret)\n\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &inode->runtime_flags);\n}\n\n/*\n * Make sure we do not end up inserting an inline extent into a file that has\n * already other (non-inline) extents. If a file has an inline extent it can\n * not have any other extents and the (single) inline extent must start at the\n * file offset 0. Failing to respect these rules will lead to file corruption,\n * resulting in EIO errors on read/write operations, hitting BUG_ON's in mm, etc\n *\n * We can have extents that have been already written to disk or we can have\n * dirty ranges still in delalloc, in which case the extent maps and items are\n * created only when we run delalloc, and the delalloc ranges might fall outside\n * the range we are currently locking in the inode's io tree. So we check the\n * inode's i_size because of that (i_size updates are done while holding the\n * i_mutex, which we are holding here).\n * We also check to see if the inode has a size not greater than \"datal\" but has\n * extents beyond it, due to an fallocate with FALLOC_FL_KEEP_SIZE (and we are\n * protected against such concurrent fallocate calls by the i_mutex).\n *\n * If the file has no extents but a size greater than datal, do not allow the\n * copy because we would need turn the inline extent into a non-inline one (even\n * with NO_HOLES enabled). If we find our destination inode only has one inline\n * extent, just overwrite it with the source inline extent if its size is less\n * than the source extent's size, or we could copy the source inline extent's\n * data into the destination inode's inline extent if the later is greater then\n * the former.\n */\nstatic int clone_copy_inline_extent(struct inode *dst,\n\t\t\t\t    struct btrfs_trans_handle *trans,\n\t\t\t\t    struct btrfs_path *path,\n\t\t\t\t    struct btrfs_key *new_key,\n\t\t\t\t    const u64 drop_start,\n\t\t\t\t    const u64 datal,\n\t\t\t\t    const u64 skip,\n\t\t\t\t    const u64 size,\n\t\t\t\t    char *inline_data)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dst->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(dst)->root;\n\tconst u64 aligned_end = ALIGN(new_key->offset + datal,\n\t\t\t\t      fs_info->sectorsize);\n\tint ret;\n\tstruct btrfs_key key;\n\n\tif (new_key->offset > 0)\n\t\treturn -EOPNOTSUPP;\n\n\tkey.objectid = btrfs_ino(BTRFS_I(dst));\n\tkey.type = BTRFS_EXTENT_DATA_KEY;\n\tkey.offset = 0;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0) {\n\t\treturn ret;\n\t} else if (ret > 0) {\n\t\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\telse if (ret > 0)\n\t\t\t\tgoto copy_inline_extent;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n\t\tif (key.objectid == btrfs_ino(BTRFS_I(dst)) &&\n\t\t    key.type == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tASSERT(key.offset > 0);\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t} else if (i_size_read(dst) <= datal) {\n\t\tstruct btrfs_file_extent_item *ei;\n\t\tu64 ext_len;\n\n\t\t/*\n\t\t * If the file size is <= datal, make sure there are no other\n\t\t * extents following (can happen do to an fallocate call with\n\t\t * the flag FALLOC_FL_KEEP_SIZE).\n\t\t */\n\t\tei = btrfs_item_ptr(path->nodes[0], path->slots[0],\n\t\t\t\t    struct btrfs_file_extent_item);\n\t\t/*\n\t\t * If it's an inline extent, it can not have other extents\n\t\t * following it.\n\t\t */\n\t\tif (btrfs_file_extent_type(path->nodes[0], ei) ==\n\t\t    BTRFS_FILE_EXTENT_INLINE)\n\t\t\tgoto copy_inline_extent;\n\n\t\text_len = btrfs_file_extent_num_bytes(path->nodes[0], ei);\n\t\tif (ext_len > aligned_end)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0) {\n\t\t\treturn ret;\n\t\t} else if (ret == 0) {\n\t\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key,\n\t\t\t\t\t      path->slots[0]);\n\t\t\tif (key.objectid == btrfs_ino(BTRFS_I(dst)) &&\n\t\t\t    key.type == BTRFS_EXTENT_DATA_KEY)\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t}\n\ncopy_inline_extent:\n\t/*\n\t * We have no extent items, or we have an extent at offset 0 which may\n\t * or may not be inlined. All these cases are dealt the same way.\n\t */\n\tif (i_size_read(dst) > datal) {\n\t\t/*\n\t\t * If the destination inode has an inline extent...\n\t\t * This would require copying the data from the source inline\n\t\t * extent into the beginning of the destination's inline extent.\n\t\t * But this is really complex, both extents can be compressed\n\t\t * or just one of them, which would require decompressing and\n\t\t * re-compressing data (which could increase the new compressed\n\t\t * size, not allowing the compressed data to fit anymore in an\n\t\t * inline extent).\n\t\t * So just don't support this case for now (it should be rare,\n\t\t * we are not really saving space when cloning inline extents).\n\t\t */\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tbtrfs_release_path(path);\n\tret = btrfs_drop_extents(trans, root, dst, drop_start, aligned_end, 1);\n\tif (ret)\n\t\treturn ret;\n\tret = btrfs_insert_empty_item(trans, root, path, new_key, size);\n\tif (ret)\n\t\treturn ret;\n\n\tif (skip) {\n\t\tconst u32 start = btrfs_file_extent_calc_inline_size(0);\n\n\t\tmemmove(inline_data + start, inline_data + start + skip, datal);\n\t}\n\n\twrite_extent_buffer(path->nodes[0], inline_data,\n\t\t\t    btrfs_item_ptr_offset(path->nodes[0],\n\t\t\t\t\t\t  path->slots[0]),\n\t\t\t    size);\n\tinode_add_bytes(dst, datal);\n\n\treturn 0;\n}\n\n/**\n * btrfs_clone() - clone a range from inode file to another\n *\n * @src: Inode to clone from\n * @inode: Inode to clone to\n * @off: Offset within source to start clone from\n * @olen: Original length, passed by user, of range to clone\n * @olen_aligned: Block-aligned value of olen\n * @destoff: Offset within @inode to start clone\n * @no_time_update: Whether to update mtime/ctime on the target inode\n */\nstatic int btrfs_clone(struct inode *src, struct inode *inode,\n\t\t       const u64 off, const u64 olen, const u64 olen_aligned,\n\t\t       const u64 destoff, int no_time_update)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_path *path = NULL;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_trans_handle *trans;\n\tchar *buf = NULL;\n\tstruct btrfs_key key;\n\tu32 nritems;\n\tint slot;\n\tint ret;\n\tconst u64 len = olen_aligned;\n\tu64 last_dest_end = destoff;\n\n\tret = -ENOMEM;\n\tbuf = kvmalloc(fs_info->nodesize, GFP_KERNEL);\n\tif (!buf)\n\t\treturn ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tkvfree(buf);\n\t\treturn ret;\n\t}\n\n\tpath->reada = READA_FORWARD;\n\t/* clone data */\n\tkey.objectid = btrfs_ino(BTRFS_I(src));\n\tkey.type = BTRFS_EXTENT_DATA_KEY;\n\tkey.offset = off;\n\n\twhile (1) {\n\t\tu64 next_key_min_offset = key.offset + 1;\n\n\t\t/*\n\t\t * note the key will change type as we walk through the\n\t\t * tree.\n\t\t */\n\t\tpath->leave_spinning = 1;\n\t\tret = btrfs_search_slot(NULL, BTRFS_I(src)->root, &key, path,\n\t\t\t\t0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * First search, if no extent item that starts at offset off was\n\t\t * found but the previous item is an extent item, it's possible\n\t\t * it might overlap our target range, therefore process it.\n\t\t */\n\t\tif (key.offset == off && ret > 0 && path->slots[0] > 0) {\n\t\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key,\n\t\t\t\t\t      path->slots[0] - 1);\n\t\t\tif (key.type == BTRFS_EXTENT_DATA_KEY)\n\t\t\t\tpath->slots[0]--;\n\t\t}\n\n\t\tnritems = btrfs_header_nritems(path->nodes[0]);\nprocess_slot:\n\t\tif (path->slots[0] >= nritems) {\n\t\t\tret = btrfs_next_leaf(BTRFS_I(src)->root, path);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tnritems = btrfs_header_nritems(path->nodes[0]);\n\t\t}\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.type > BTRFS_EXTENT_DATA_KEY ||\n\t\t    key.objectid != btrfs_ino(BTRFS_I(src)))\n\t\t\tbreak;\n\n\t\tif (key.type == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tstruct btrfs_file_extent_item *extent;\n\t\t\tint type;\n\t\t\tu32 size;\n\t\t\tstruct btrfs_key new_key;\n\t\t\tu64 disko = 0, diskl = 0;\n\t\t\tu64 datao = 0, datal = 0;\n\t\t\tu8 comp;\n\t\t\tu64 drop_start;\n\n\t\t\textent = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\tstruct btrfs_file_extent_item);\n\t\t\tcomp = btrfs_file_extent_compression(leaf, extent);\n\t\t\ttype = btrfs_file_extent_type(leaf, extent);\n\t\t\tif (type == BTRFS_FILE_EXTENT_REG ||\n\t\t\t    type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\t\tdisko = btrfs_file_extent_disk_bytenr(leaf,\n\t\t\t\t\t\t\t\t      extent);\n\t\t\t\tdiskl = btrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t extent);\n\t\t\t\tdatao = btrfs_file_extent_offset(leaf, extent);\n\t\t\t\tdatal = btrfs_file_extent_num_bytes(leaf,\n\t\t\t\t\t\t\t\t    extent);\n\t\t\t} else if (type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\t/* take upper bound, may be compressed */\n\t\t\t\tdatal = btrfs_file_extent_ram_bytes(leaf,\n\t\t\t\t\t\t\t\t    extent);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * The first search might have left us at an extent\n\t\t\t * item that ends before our target range's start, can\n\t\t\t * happen if we have holes and NO_HOLES feature enabled.\n\t\t\t */\n\t\t\tif (key.offset + datal <= off) {\n\t\t\t\tpath->slots[0]++;\n\t\t\t\tgoto process_slot;\n\t\t\t} else if (key.offset >= off + len) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tnext_key_min_offset = key.offset + datal;\n\t\t\tsize = btrfs_item_size_nr(leaf, slot);\n\t\t\tread_extent_buffer(leaf, buf,\n\t\t\t\t\t   btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t   size);\n\n\t\t\tbtrfs_release_path(path);\n\t\t\tpath->leave_spinning = 0;\n\n\t\t\tmemcpy(&new_key, &key, sizeof(new_key));\n\t\t\tnew_key.objectid = btrfs_ino(BTRFS_I(inode));\n\t\t\tif (off <= key.offset)\n\t\t\t\tnew_key.offset = key.offset + destoff - off;\n\t\t\telse\n\t\t\t\tnew_key.offset = destoff;\n\n\t\t\t/*\n\t\t\t * Deal with a hole that doesn't have an extent item\n\t\t\t * that represents it (NO_HOLES feature enabled).\n\t\t\t * This hole is either in the middle of the cloning\n\t\t\t * range or at the beginning (fully overlaps it or\n\t\t\t * partially overlaps it).\n\t\t\t */\n\t\t\tif (new_key.offset != last_dest_end)\n\t\t\t\tdrop_start = last_dest_end;\n\t\t\telse\n\t\t\t\tdrop_start = new_key.offset;\n\n\t\t\t/*\n\t\t\t * 1 - adjusting old extent (we may have to split it)\n\t\t\t * 1 - add new extent\n\t\t\t * 1 - inode update\n\t\t\t */\n\t\t\ttrans = btrfs_start_transaction(root, 3);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (type == BTRFS_FILE_EXTENT_REG ||\n\t\t\t    type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\t\t/*\n\t\t\t\t *    a  | --- range to clone ---|  b\n\t\t\t\t * | ------------- extent ------------- |\n\t\t\t\t */\n\n\t\t\t\t/* subtract range b */\n\t\t\t\tif (key.offset + datal > off + len)\n\t\t\t\t\tdatal = off + len - key.offset;\n\n\t\t\t\t/* subtract range a */\n\t\t\t\tif (off > key.offset) {\n\t\t\t\t\tdatao += off - key.offset;\n\t\t\t\t\tdatal -= off - key.offset;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t\t\t drop_start,\n\t\t\t\t\t\t\t new_key.offset + datal,\n\t\t\t\t\t\t\t 1);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (ret != -EOPNOTSUPP)\n\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_insert_empty_item(trans, root, path,\n\t\t\t\t\t\t\t      &new_key, size);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tleaf = path->nodes[0];\n\t\t\t\tslot = path->slots[0];\n\t\t\t\twrite_extent_buffer(leaf, buf,\n\t\t\t\t\t    btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t    size);\n\n\t\t\t\textent = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\tstruct btrfs_file_extent_item);\n\n\t\t\t\t/* disko == 0 means it's a hole */\n\t\t\t\tif (!disko)\n\t\t\t\t\tdatao = 0;\n\n\t\t\t\tbtrfs_set_file_extent_offset(leaf, extent,\n\t\t\t\t\t\t\t     datao);\n\t\t\t\tbtrfs_set_file_extent_num_bytes(leaf, extent,\n\t\t\t\t\t\t\t\tdatal);\n\n\t\t\t\tif (disko) {\n\t\t\t\t\tinode_add_bytes(inode, datal);\n\t\t\t\t\tret = btrfs_inc_extent_ref(trans,\n\t\t\t\t\t\t\troot,\n\t\t\t\t\t\t\tdisko, diskl, 0,\n\t\t\t\t\t\t\troot->root_key.objectid,\n\t\t\t\t\t\t\tbtrfs_ino(BTRFS_I(inode)),\n\t\t\t\t\t\t\tnew_key.offset - datao);\n\t\t\t\t\tif (ret) {\n\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\t\tret);\n\t\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\t\tgoto out;\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if (type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\tu64 skip = 0;\n\t\t\t\tu64 trim = 0;\n\n\t\t\t\tif (off > key.offset) {\n\t\t\t\t\tskip = off - key.offset;\n\t\t\t\t\tnew_key.offset += skip;\n\t\t\t\t}\n\n\t\t\t\tif (key.offset + datal > off + len)\n\t\t\t\t\ttrim = key.offset + datal - (off + len);\n\n\t\t\t\tif (comp && (skip || trim)) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tsize -= skip + trim;\n\t\t\t\tdatal -= skip + trim;\n\n\t\t\t\tret = clone_copy_inline_extent(inode,\n\t\t\t\t\t\t\t       trans, path,\n\t\t\t\t\t\t\t       &new_key,\n\t\t\t\t\t\t\t       drop_start,\n\t\t\t\t\t\t\t       datal,\n\t\t\t\t\t\t\t       skip, size, buf);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (ret != -EOPNOTSUPP)\n\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tleaf = path->nodes[0];\n\t\t\t\tslot = path->slots[0];\n\t\t\t}\n\n\t\t\t/* If we have an implicit hole (NO_HOLES feature). */\n\t\t\tif (drop_start < new_key.offset)\n\t\t\t\tclone_update_extent_map(BTRFS_I(inode), trans,\n\t\t\t\t\t\tNULL, drop_start,\n\t\t\t\t\t\tnew_key.offset - drop_start);\n\n\t\t\tclone_update_extent_map(BTRFS_I(inode), trans,\n\t\t\t\t\tpath, 0, 0);\n\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t\tbtrfs_release_path(path);\n\n\t\t\tlast_dest_end = ALIGN(new_key.offset + datal,\n\t\t\t\t\t      fs_info->sectorsize);\n\t\t\tret = clone_finish_inode_update(trans, inode,\n\t\t\t\t\t\t\tlast_dest_end,\n\t\t\t\t\t\t\tdestoff, olen,\n\t\t\t\t\t\t\tno_time_update);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tif (new_key.offset + datal >= destoff + len)\n\t\t\t\tbreak;\n\t\t}\n\t\tbtrfs_release_path(path);\n\t\tkey.offset = next_key_min_offset;\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tret = 0;\n\n\tif (last_dest_end < destoff + len) {\n\t\t/*\n\t\t * We have an implicit hole (NO_HOLES feature is enabled) that\n\t\t * fully or partially overlaps our cloning range at its end.\n\t\t */\n\t\tbtrfs_release_path(path);\n\n\t\t/*\n\t\t * 1 - remove extent(s)\n\t\t * 1 - inode update\n\t\t */\n\t\ttrans = btrfs_start_transaction(root, 2);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out;\n\t\t}\n\t\tret = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t last_dest_end, destoff + len, 1);\n\t\tif (ret) {\n\t\t\tif (ret != -EOPNOTSUPP)\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tbtrfs_end_transaction(trans);\n\t\t\tgoto out;\n\t\t}\n\t\tclone_update_extent_map(BTRFS_I(inode), trans, NULL,\n\t\t\t\tlast_dest_end,\n\t\t\t\tdestoff + len - last_dest_end);\n\t\tret = clone_finish_inode_update(trans, inode, destoff + len,\n\t\t\t\t\t\tdestoff, olen, no_time_update);\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tkvfree(buf);\n\treturn ret;\n}\n\nstatic noinline int btrfs_clone_files(struct file *file, struct file *file_src,\n\t\t\t\t\tu64 off, u64 olen, u64 destoff)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct inode *src = file_inode(file_src);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tint ret;\n\tu64 len = olen;\n\tu64 bs = fs_info->sb->s_blocksize;\n\n\t/*\n\t * TODO:\n\t * - split compressed inline extents.  annoying: we need to\n\t *   decompress into destination's address_space (the file offset\n\t *   may change, so source mapping won't do), then recompress (or\n\t *   otherwise reinsert) a subrange.\n\t *\n\t * - split destination inode's inline extents.  The inline extents can\n\t *   be either compressed or non-compressed.\n\t */\n\n\t/*\n\t * VFS's generic_remap_file_range_prep() protects us from cloning the\n\t * eof block into the middle of a file, which would result in corruption\n\t * if the file size is not blocksize aligned. So we don't need to check\n\t * for that case here.\n\t */\n\tif (off + len == src->i_size)\n\t\tlen = ALIGN(src->i_size, bs) - off;\n\n\tif (destoff > inode->i_size) {\n\t\tconst u64 wb_start = ALIGN_DOWN(inode->i_size, bs);\n\n\t\tret = btrfs_cont_expand(inode, inode->i_size, destoff);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\t/*\n\t\t * We may have truncated the last block if the inode's size is\n\t\t * not sector size aligned, so we need to wait for writeback to\n\t\t * complete before proceeding further, otherwise we can race\n\t\t * with cloning and attempt to increment a reference to an\n\t\t * extent that no longer exists (writeback completed right after\n\t\t * we found the previous extent covering eof and before we\n\t\t * attempted to increment its reference count).\n\t\t */\n\t\tret = btrfs_wait_ordered_range(inode, wb_start,\n\t\t\t\t\t       destoff - wb_start);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Lock destination range to serialize with concurrent readpages() and\n\t * source range to serialize with relocation.\n\t */\n\tbtrfs_double_extent_lock(src, off, inode, destoff, len);\n\tret = btrfs_clone(src, inode, off, olen, len, destoff, 0);\n\tbtrfs_double_extent_unlock(src, off, inode, destoff, len);\n\t/*\n\t * Truncate page cache pages so that future reads will see the cloned\n\t * data immediately and not the previous data.\n\t */\n\ttruncate_inode_pages_range(&inode->i_data,\n\t\t\t\tround_down(destoff, PAGE_SIZE),\n\t\t\t\tround_up(destoff + len, PAGE_SIZE) - 1);\n\n\treturn ret;\n}\n\nstatic int btrfs_remap_file_range_prep(struct file *file_in, loff_t pos_in,\n\t\t\t\t       struct file *file_out, loff_t pos_out,\n\t\t\t\t       loff_t *len, unsigned int remap_flags)\n{\n\tstruct inode *inode_in = file_inode(file_in);\n\tstruct inode *inode_out = file_inode(file_out);\n\tu64 bs = BTRFS_I(inode_out)->root->fs_info->sb->s_blocksize;\n\tbool same_inode = inode_out == inode_in;\n\tu64 wb_len;\n\tint ret;\n\n\tif (!(remap_flags & REMAP_FILE_DEDUP)) {\n\t\tstruct btrfs_root *root_out = BTRFS_I(inode_out)->root;\n\n\t\tif (btrfs_root_readonly(root_out))\n\t\t\treturn -EROFS;\n\n\t\tif (file_in->f_path.mnt != file_out->f_path.mnt ||\n\t\t    inode_in->i_sb != inode_out->i_sb)\n\t\t\treturn -EXDEV;\n\t}\n\n\tif (same_inode)\n\t\tinode_lock(inode_in);\n\telse\n\t\tbtrfs_double_inode_lock(inode_in, inode_out);\n\n\t/* don't make the dst file partly checksummed */\n\tif ((BTRFS_I(inode_in)->flags & BTRFS_INODE_NODATASUM) !=\n\t    (BTRFS_I(inode_out)->flags & BTRFS_INODE_NODATASUM)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Now that the inodes are locked, we need to start writeback ourselves\n\t * and can not rely on the writeback from the VFS's generic helper\n\t * generic_remap_file_range_prep() because:\n\t *\n\t * 1) For compression we must call filemap_fdatawrite_range() range\n\t *    twice (btrfs_fdatawrite_range() does it for us), and the generic\n\t *    helper only calls it once;\n\t *\n\t * 2) filemap_fdatawrite_range(), called by the generic helper only\n\t *    waits for the writeback to complete, i.e. for IO to be done, and\n\t *    not for the ordered extents to complete. We need to wait for them\n\t *    to complete so that new file extent items are in the fs tree.\n\t */\n\tif (*len == 0 && !(remap_flags & REMAP_FILE_DEDUP))\n\t\twb_len = ALIGN(inode_in->i_size, bs) - ALIGN_DOWN(pos_in, bs);\n\telse\n\t\twb_len = ALIGN(*len, bs);\n\n\t/*\n\t * Since we don't lock ranges, wait for ongoing lockless dio writes (as\n\t * any in progress could create its ordered extents after we wait for\n\t * existing ordered extents below).\n\t */\n\tinode_dio_wait(inode_in);\n\tif (!same_inode)\n\t\tinode_dio_wait(inode_out);\n\n\tret = btrfs_wait_ordered_range(inode_in, ALIGN_DOWN(pos_in, bs),\n\t\t\t\t       wb_len);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\tret = btrfs_wait_ordered_range(inode_out, ALIGN_DOWN(pos_out, bs),\n\t\t\t\t       wb_len);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\n\tret = generic_remap_file_range_prep(file_in, pos_in, file_out, pos_out,\n\t\t\t\t\t    len, remap_flags);\n\tif (ret < 0 || *len == 0)\n\t\tgoto out_unlock;\n\n\treturn 0;\n\n out_unlock:\n\tif (same_inode)\n\t\tinode_unlock(inode_in);\n\telse\n\t\tbtrfs_double_inode_unlock(inode_in, inode_out);\n\n\treturn ret;\n}\n\nloff_t btrfs_remap_file_range(struct file *src_file, loff_t off,\n\t\tstruct file *dst_file, loff_t destoff, loff_t len,\n\t\tunsigned int remap_flags)\n{\n\tstruct inode *src_inode = file_inode(src_file);\n\tstruct inode *dst_inode = file_inode(dst_file);\n\tbool same_inode = dst_inode == src_inode;\n\tint ret;\n\n\tif (remap_flags & ~(REMAP_FILE_DEDUP | REMAP_FILE_ADVISORY))\n\t\treturn -EINVAL;\n\n\tret = btrfs_remap_file_range_prep(src_file, off, dst_file, destoff,\n\t\t\t\t\t  &len, remap_flags);\n\tif (ret < 0 || len == 0)\n\t\treturn ret;\n\n\tif (remap_flags & REMAP_FILE_DEDUP)\n\t\tret = btrfs_extent_same(src_inode, off, len, dst_inode, destoff);\n\telse\n\t\tret = btrfs_clone_files(dst_file, src_file, off, len, destoff);\n\n\tif (same_inode)\n\t\tinode_unlock(src_inode);\n\telse\n\t\tbtrfs_double_inode_unlock(src_inode, dst_inode);\n\n\treturn ret < 0 ? ret : len;\n}\n\nstatic long btrfs_ioctl_default_subvol(struct file *file, void __user *argp)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_root *new_root;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key location;\n\tstruct btrfs_disk_key disk_key;\n\tu64 objectid = 0;\n\tu64 dir_id;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (copy_from_user(&objectid, argp, sizeof(objectid))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (!objectid)\n\t\tobjectid = BTRFS_FS_TREE_OBJECTID;\n\n\tlocation.objectid = objectid;\n\tlocation.type = BTRFS_ROOT_ITEM_KEY;\n\tlocation.offset = (u64)-1;\n\n\tnew_root = btrfs_read_fs_root_no_name(fs_info, &location);\n\tif (IS_ERR(new_root)) {\n\t\tret = PTR_ERR(new_root);\n\t\tgoto out;\n\t}\n\tif (!is_fstree(new_root->root_key.objectid)) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tpath->leave_spinning = 1;\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tdir_id = btrfs_super_root_dir(fs_info->super_copy);\n\tdi = btrfs_lookup_dir_item(trans, fs_info->tree_root, path,\n\t\t\t\t   dir_id, \"default\", 7, 1);\n\tif (IS_ERR_OR_NULL(di)) {\n\t\tbtrfs_free_path(path);\n\t\tbtrfs_end_transaction(trans);\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"Umm, you don't have the default diritem, this isn't going to work\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tbtrfs_cpu_key_to_disk(&disk_key, &new_root->root_key);\n\tbtrfs_set_dir_item_key(path->nodes[0], di, &disk_key);\n\tbtrfs_mark_buffer_dirty(path->nodes[0]);\n\tbtrfs_free_path(path);\n\n\tbtrfs_set_fs_incompat(fs_info, DEFAULT_SUBVOL);\n\tbtrfs_end_transaction(trans);\nout:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic void get_block_group_info(struct list_head *groups_list,\n\t\t\t\t struct btrfs_ioctl_space_info *space)\n{\n\tstruct btrfs_block_group_cache *block_group;\n\n\tspace->total_bytes = 0;\n\tspace->used_bytes = 0;\n\tspace->flags = 0;\n\tlist_for_each_entry(block_group, groups_list, list) {\n\t\tspace->flags = block_group->flags;\n\t\tspace->total_bytes += block_group->key.offset;\n\t\tspace->used_bytes +=\n\t\t\tbtrfs_block_group_used(&block_group->item);\n\t}\n}\n\nstatic long btrfs_ioctl_space_info(struct btrfs_fs_info *fs_info,\n\t\t\t\t   void __user *arg)\n{\n\tstruct btrfs_ioctl_space_args space_args;\n\tstruct btrfs_ioctl_space_info space;\n\tstruct btrfs_ioctl_space_info *dest;\n\tstruct btrfs_ioctl_space_info *dest_orig;\n\tstruct btrfs_ioctl_space_info __user *user_dest;\n\tstruct btrfs_space_info *info;\n\tstatic const u64 types[] = {\n\t\tBTRFS_BLOCK_GROUP_DATA,\n\t\tBTRFS_BLOCK_GROUP_SYSTEM,\n\t\tBTRFS_BLOCK_GROUP_METADATA,\n\t\tBTRFS_BLOCK_GROUP_DATA | BTRFS_BLOCK_GROUP_METADATA\n\t};\n\tint num_types = 4;\n\tint alloc_size;\n\tint ret = 0;\n\tu64 slot_count = 0;\n\tint i, c;\n\n\tif (copy_from_user(&space_args,\n\t\t\t   (struct btrfs_ioctl_space_args __user *)arg,\n\t\t\t   sizeof(space_args)))\n\t\treturn -EFAULT;\n\n\tfor (i = 0; i < num_types; i++) {\n\t\tstruct btrfs_space_info *tmp;\n\n\t\tinfo = NULL;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(tmp, &fs_info->space_info,\n\t\t\t\t\tlist) {\n\t\t\tif (tmp->flags == types[i]) {\n\t\t\t\tinfo = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (!info)\n\t\t\tcontinue;\n\n\t\tdown_read(&info->groups_sem);\n\t\tfor (c = 0; c < BTRFS_NR_RAID_TYPES; c++) {\n\t\t\tif (!list_empty(&info->block_groups[c]))\n\t\t\t\tslot_count++;\n\t\t}\n\t\tup_read(&info->groups_sem);\n\t}\n\n\t/*\n\t * Global block reserve, exported as a space_info\n\t */\n\tslot_count++;\n\n\t/* space_slots == 0 means they are asking for a count */\n\tif (space_args.space_slots == 0) {\n\t\tspace_args.total_spaces = slot_count;\n\t\tgoto out;\n\t}\n\n\tslot_count = min_t(u64, space_args.space_slots, slot_count);\n\n\talloc_size = sizeof(*dest) * slot_count;\n\n\t/* we generally have at most 6 or so space infos, one for each raid\n\t * level.  So, a whole page should be more than enough for everyone\n\t */\n\tif (alloc_size > PAGE_SIZE)\n\t\treturn -ENOMEM;\n\n\tspace_args.total_spaces = 0;\n\tdest = kmalloc(alloc_size, GFP_KERNEL);\n\tif (!dest)\n\t\treturn -ENOMEM;\n\tdest_orig = dest;\n\n\t/* now we have a buffer to copy into */\n\tfor (i = 0; i < num_types; i++) {\n\t\tstruct btrfs_space_info *tmp;\n\n\t\tif (!slot_count)\n\t\t\tbreak;\n\n\t\tinfo = NULL;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(tmp, &fs_info->space_info,\n\t\t\t\t\tlist) {\n\t\t\tif (tmp->flags == types[i]) {\n\t\t\t\tinfo = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (!info)\n\t\t\tcontinue;\n\t\tdown_read(&info->groups_sem);\n\t\tfor (c = 0; c < BTRFS_NR_RAID_TYPES; c++) {\n\t\t\tif (!list_empty(&info->block_groups[c])) {\n\t\t\t\tget_block_group_info(&info->block_groups[c],\n\t\t\t\t\t\t     &space);\n\t\t\t\tmemcpy(dest, &space, sizeof(space));\n\t\t\t\tdest++;\n\t\t\t\tspace_args.total_spaces++;\n\t\t\t\tslot_count--;\n\t\t\t}\n\t\t\tif (!slot_count)\n\t\t\t\tbreak;\n\t\t}\n\t\tup_read(&info->groups_sem);\n\t}\n\n\t/*\n\t * Add global block reserve\n\t */\n\tif (slot_count) {\n\t\tstruct btrfs_block_rsv *block_rsv = &fs_info->global_block_rsv;\n\n\t\tspin_lock(&block_rsv->lock);\n\t\tspace.total_bytes = block_rsv->size;\n\t\tspace.used_bytes = block_rsv->size - block_rsv->reserved;\n\t\tspin_unlock(&block_rsv->lock);\n\t\tspace.flags = BTRFS_SPACE_INFO_GLOBAL_RSV;\n\t\tmemcpy(dest, &space, sizeof(space));\n\t\tspace_args.total_spaces++;\n\t}\n\n\tuser_dest = (struct btrfs_ioctl_space_info __user *)\n\t\t(arg + sizeof(struct btrfs_ioctl_space_args));\n\n\tif (copy_to_user(user_dest, dest_orig, alloc_size))\n\t\tret = -EFAULT;\n\n\tkfree(dest_orig);\nout:\n\tif (ret == 0 && copy_to_user(arg, &space_args, sizeof(space_args)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nstatic noinline long btrfs_ioctl_start_sync(struct btrfs_root *root,\n\t\t\t\t\t    void __user *argp)\n{\n\tstruct btrfs_trans_handle *trans;\n\tu64 transid;\n\tint ret;\n\n\ttrans = btrfs_attach_transaction_barrier(root);\n\tif (IS_ERR(trans)) {\n\t\tif (PTR_ERR(trans) != -ENOENT)\n\t\t\treturn PTR_ERR(trans);\n\n\t\t/* No running transaction, don't bother */\n\t\ttransid = root->fs_info->last_trans_committed;\n\t\tgoto out;\n\t}\n\ttransid = trans->transid;\n\tret = btrfs_commit_transaction_async(trans, 0);\n\tif (ret) {\n\t\tbtrfs_end_transaction(trans);\n\t\treturn ret;\n\t}\nout:\n\tif (argp)\n\t\tif (copy_to_user(argp, &transid, sizeof(transid)))\n\t\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic noinline long btrfs_ioctl_wait_sync(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t   void __user *argp)\n{\n\tu64 transid;\n\n\tif (argp) {\n\t\tif (copy_from_user(&transid, argp, sizeof(transid)))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\ttransid = 0;  /* current trans */\n\t}\n\treturn btrfs_wait_for_commit(fs_info, transid);\n}\n\nstatic long btrfs_ioctl_scrub(struct file *file, void __user *arg)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(file_inode(file)->i_sb);\n\tstruct btrfs_ioctl_scrub_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tif (!(sa->flags & BTRFS_SCRUB_READONLY)) {\n\t\tret = mnt_want_write_file(file);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = btrfs_scrub_dev(fs_info, sa->devid, sa->start, sa->end,\n\t\t\t      &sa->progress, sa->flags & BTRFS_SCRUB_READONLY,\n\t\t\t      0);\n\n\tif (ret == 0 && copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tif (!(sa->flags & BTRFS_SCRUB_READONLY))\n\t\tmnt_drop_write_file(file);\nout:\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_scrub_cancel(struct btrfs_fs_info *fs_info)\n{\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn btrfs_scrub_cancel(fs_info);\n}\n\nstatic long btrfs_ioctl_scrub_progress(struct btrfs_fs_info *fs_info,\n\t\t\t\t       void __user *arg)\n{\n\tstruct btrfs_ioctl_scrub_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tret = btrfs_scrub_progress(fs_info, sa->devid, &sa->progress);\n\n\tif (ret == 0 && copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\t\t      void __user *arg)\n{\n\tstruct btrfs_ioctl_get_dev_stats *sa;\n\tint ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tif ((sa->flags & BTRFS_DEV_STATS_RESET) && !capable(CAP_SYS_ADMIN)) {\n\t\tkfree(sa);\n\t\treturn -EPERM;\n\t}\n\n\tret = btrfs_get_dev_stats(fs_info, sa);\n\n\tif (ret == 0 && copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_dev_replace(struct btrfs_fs_info *fs_info,\n\t\t\t\t    void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_replace_args *p;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tp = memdup_user(arg, sizeof(*p));\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\tswitch (p->cmd) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_START:\n\t\tif (sb_rdonly(fs_info->sb)) {\n\t\t\tret = -EROFS;\n\t\t\tgoto out;\n\t\t}\n\t\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\t\tret = BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t\t} else {\n\t\t\tret = btrfs_dev_replace_by_ioctl(fs_info, p);\n\t\t\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\t\t}\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_STATUS:\n\t\tbtrfs_dev_replace_status(fs_info, p);\n\t\tret = 0;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_CANCEL:\n\t\tp->result = btrfs_dev_replace_cancel(fs_info);\n\t\tret = 0;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif ((ret == 0 || ret == -ECANCELED) && copy_to_user(arg, p, sizeof(*p)))\n\t\tret = -EFAULT;\nout:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_ino_to_path(struct btrfs_root *root, void __user *arg)\n{\n\tint ret = 0;\n\tint i;\n\tu64 rel_ptr;\n\tint size;\n\tstruct btrfs_ioctl_ino_path_args *ipa = NULL;\n\tstruct inode_fs_paths *ipath = NULL;\n\tstruct btrfs_path *path;\n\n\tif (!capable(CAP_DAC_READ_SEARCH))\n\t\treturn -EPERM;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tipa = memdup_user(arg, sizeof(*ipa));\n\tif (IS_ERR(ipa)) {\n\t\tret = PTR_ERR(ipa);\n\t\tipa = NULL;\n\t\tgoto out;\n\t}\n\n\tsize = min_t(u32, ipa->size, 4096);\n\tipath = init_ipath(size, root, path);\n\tif (IS_ERR(ipath)) {\n\t\tret = PTR_ERR(ipath);\n\t\tipath = NULL;\n\t\tgoto out;\n\t}\n\n\tret = paths_from_inode(ipa->inum, ipath);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tfor (i = 0; i < ipath->fspath->elem_cnt; ++i) {\n\t\trel_ptr = ipath->fspath->val[i] -\n\t\t\t  (u64)(unsigned long)ipath->fspath->val;\n\t\tipath->fspath->val[i] = rel_ptr;\n\t}\n\n\tret = copy_to_user((void __user *)(unsigned long)ipa->fspath,\n\t\t\t   ipath->fspath, size);\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tfree_ipath(ipath);\n\tkfree(ipa);\n\n\treturn ret;\n}\n\nstatic int build_ino_list(u64 inum, u64 offset, u64 root, void *ctx)\n{\n\tstruct btrfs_data_container *inodes = ctx;\n\tconst size_t c = 3 * sizeof(u64);\n\n\tif (inodes->bytes_left >= c) {\n\t\tinodes->bytes_left -= c;\n\t\tinodes->val[inodes->elem_cnt] = inum;\n\t\tinodes->val[inodes->elem_cnt + 1] = offset;\n\t\tinodes->val[inodes->elem_cnt + 2] = root;\n\t\tinodes->elem_cnt += 3;\n\t} else {\n\t\tinodes->bytes_missing += c - inodes->bytes_left;\n\t\tinodes->bytes_left = 0;\n\t\tinodes->elem_missed += 3;\n\t}\n\n\treturn 0;\n}\n\nstatic long btrfs_ioctl_logical_to_ino(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tvoid __user *arg, int version)\n{\n\tint ret = 0;\n\tint size;\n\tstruct btrfs_ioctl_logical_ino_args *loi;\n\tstruct btrfs_data_container *inodes = NULL;\n\tstruct btrfs_path *path = NULL;\n\tbool ignore_offset;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tloi = memdup_user(arg, sizeof(*loi));\n\tif (IS_ERR(loi))\n\t\treturn PTR_ERR(loi);\n\n\tif (version == 1) {\n\t\tignore_offset = false;\n\t\tsize = min_t(u32, loi->size, SZ_64K);\n\t} else {\n\t\t/* All reserved bits must be 0 for now */\n\t\tif (memchr_inv(loi->reserved, 0, sizeof(loi->reserved))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_loi;\n\t\t}\n\t\t/* Only accept flags we have defined so far */\n\t\tif (loi->flags & ~(BTRFS_LOGICAL_INO_ARGS_IGNORE_OFFSET)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_loi;\n\t\t}\n\t\tignore_offset = loi->flags & BTRFS_LOGICAL_INO_ARGS_IGNORE_OFFSET;\n\t\tsize = min_t(u32, loi->size, SZ_16M);\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tinodes = init_data_container(size);\n\tif (IS_ERR(inodes)) {\n\t\tret = PTR_ERR(inodes);\n\t\tinodes = NULL;\n\t\tgoto out;\n\t}\n\n\tret = iterate_inodes_from_logical(loi->logical, fs_info, path,\n\t\t\t\t\t  build_ino_list, inodes, ignore_offset);\n\tif (ret == -EINVAL)\n\t\tret = -ENOENT;\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = copy_to_user((void __user *)(unsigned long)loi->inodes, inodes,\n\t\t\t   size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tbtrfs_free_path(path);\n\tkvfree(inodes);\nout_loi:\n\tkfree(loi);\n\n\treturn ret;\n}\n\nvoid btrfs_update_ioctl_balance_args(struct btrfs_fs_info *fs_info,\n\t\t\t       struct btrfs_ioctl_balance_args *bargs)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\n\tbargs->flags = bctl->flags;\n\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_RUNNING;\n\tif (atomic_read(&fs_info->balance_pause_req))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_PAUSE_REQ;\n\tif (atomic_read(&fs_info->balance_cancel_req))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_CANCEL_REQ;\n\n\tmemcpy(&bargs->data, &bctl->data, sizeof(bargs->data));\n\tmemcpy(&bargs->meta, &bctl->meta, sizeof(bargs->meta));\n\tmemcpy(&bargs->sys, &bctl->sys, sizeof(bargs->sys));\n\n\tspin_lock(&fs_info->balance_lock);\n\tmemcpy(&bargs->stat, &bctl->stat, sizeof(bargs->stat));\n\tspin_unlock(&fs_info->balance_lock);\n}\n\nstatic long btrfs_ioctl_balance(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(file_inode(file))->root;\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_ioctl_balance_args *bargs;\n\tstruct btrfs_balance_control *bctl;\n\tbool need_unlock; /* for mut. excl. ops lock */\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\nagain:\n\tif (!test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t\tneed_unlock = true;\n\t\tgoto locked;\n\t}\n\n\t/*\n\t * mut. excl. ops lock is locked.  Three possibilities:\n\t *   (1) some other op is running\n\t *   (2) balance is running\n\t *   (3) balance is paused -- special case (think resume)\n\t */\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (fs_info->balance_ctl) {\n\t\t/* this is either (2) or (3) */\n\t\tif (!test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t\t/*\n\t\t\t * Lock released to allow other waiters to continue,\n\t\t\t * we'll reexamine the status again.\n\t\t\t */\n\t\t\tmutex_lock(&fs_info->balance_mutex);\n\n\t\t\tif (fs_info->balance_ctl &&\n\t\t\t    !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\t\t\t/* this is (3) */\n\t\t\t\tneed_unlock = false;\n\t\t\t\tgoto locked;\n\t\t\t}\n\n\t\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t\tgoto again;\n\t\t} else {\n\t\t\t/* this is (2) */\n\t\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t\tret = -EINPROGRESS;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t/* this is (1) */\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\tret = BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t\tgoto out;\n\t}\n\nlocked:\n\tBUG_ON(!test_bit(BTRFS_FS_EXCL_OP, &fs_info->flags));\n\n\tif (arg) {\n\t\tbargs = memdup_user(arg, sizeof(*bargs));\n\t\tif (IS_ERR(bargs)) {\n\t\t\tret = PTR_ERR(bargs);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (bargs->flags & BTRFS_BALANCE_RESUME) {\n\t\t\tif (!fs_info->balance_ctl) {\n\t\t\t\tret = -ENOTCONN;\n\t\t\t\tgoto out_bargs;\n\t\t\t}\n\n\t\t\tbctl = fs_info->balance_ctl;\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->flags |= BTRFS_BALANCE_RESUME;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\n\t\t\tgoto do_balance;\n\t\t}\n\t} else {\n\t\tbargs = NULL;\n\t}\n\n\tif (fs_info->balance_ctl) {\n\t\tret = -EINPROGRESS;\n\t\tgoto out_bargs;\n\t}\n\n\tbctl = kzalloc(sizeof(*bctl), GFP_KERNEL);\n\tif (!bctl) {\n\t\tret = -ENOMEM;\n\t\tgoto out_bargs;\n\t}\n\n\tif (arg) {\n\t\tmemcpy(&bctl->data, &bargs->data, sizeof(bctl->data));\n\t\tmemcpy(&bctl->meta, &bargs->meta, sizeof(bctl->meta));\n\t\tmemcpy(&bctl->sys, &bargs->sys, sizeof(bctl->sys));\n\n\t\tbctl->flags = bargs->flags;\n\t} else {\n\t\t/* balance everything - no filters */\n\t\tbctl->flags |= BTRFS_BALANCE_TYPE_MASK;\n\t}\n\n\tif (bctl->flags & ~(BTRFS_BALANCE_ARGS_MASK | BTRFS_BALANCE_TYPE_MASK)) {\n\t\tret = -EINVAL;\n\t\tgoto out_bctl;\n\t}\n\ndo_balance:\n\t/*\n\t * Ownership of bctl and filesystem flag BTRFS_FS_EXCL_OP goes to\n\t * btrfs_balance.  bctl is freed in reset_balance_state, or, if\n\t * restriper was paused all the way until unmount, in free_fs_info.\n\t * The flag should be cleared after reset_balance_state.\n\t */\n\tneed_unlock = false;\n\n\tret = btrfs_balance(fs_info, bctl, bargs);\n\tbctl = NULL;\n\n\tif ((ret == 0 || ret == -ECANCELED) && arg) {\n\t\tif (copy_to_user(arg, bargs, sizeof(*bargs)))\n\t\t\tret = -EFAULT;\n\t}\n\nout_bctl:\n\tkfree(bctl);\nout_bargs:\n\tkfree(bargs);\nout_unlock:\n\tmutex_unlock(&fs_info->balance_mutex);\n\tif (need_unlock)\n\t\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\nout:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_balance_ctl(struct btrfs_fs_info *fs_info, int cmd)\n{\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tswitch (cmd) {\n\tcase BTRFS_BALANCE_CTL_PAUSE:\n\t\treturn btrfs_pause_balance(fs_info);\n\tcase BTRFS_BALANCE_CTL_CANCEL:\n\t\treturn btrfs_cancel_balance(fs_info);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic long btrfs_ioctl_balance_progress(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t void __user *arg)\n{\n\tstruct btrfs_ioctl_balance_args *bargs;\n\tint ret = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tret = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\tbargs = kzalloc(sizeof(*bargs), GFP_KERNEL);\n\tif (!bargs) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tbtrfs_update_ioctl_balance_args(fs_info, bargs);\n\n\tif (copy_to_user(arg, bargs, sizeof(*bargs)))\n\t\tret = -EFAULT;\n\n\tkfree(bargs);\nout:\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_ctl(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_quota_ctl_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\tdown_write(&fs_info->subvol_sem);\n\n\tswitch (sa->cmd) {\n\tcase BTRFS_QUOTA_CTL_ENABLE:\n\t\tret = btrfs_quota_enable(fs_info);\n\t\tbreak;\n\tcase BTRFS_QUOTA_CTL_DISABLE:\n\t\tret = btrfs_quota_disable(fs_info);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tkfree(sa);\n\tup_write(&fs_info->subvol_sem);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_assign(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_qgroup_assign_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tif (sa->assign) {\n\t\tret = btrfs_add_qgroup_relation(trans, sa->src, sa->dst);\n\t} else {\n\t\tret = btrfs_del_qgroup_relation(trans, sa->src, sa->dst);\n\t}\n\n\t/* update qgroup status and info */\n\terr = btrfs_run_qgroups(trans);\n\tif (err < 0)\n\t\tbtrfs_handle_fs_error(fs_info, err,\n\t\t\t\t      \"failed to update qgroup status and info\");\n\terr = btrfs_end_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_create(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_qgroup_create_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\tif (!sa->qgroupid) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tif (sa->create) {\n\t\tret = btrfs_create_qgroup(trans, sa->qgroupid);\n\t} else {\n\t\tret = btrfs_remove_qgroup(trans, sa->qgroupid);\n\t}\n\n\terr = btrfs_end_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_limit(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_qgroup_limit_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\tu64 qgroupid;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tqgroupid = sa->qgroupid;\n\tif (!qgroupid) {\n\t\t/* take the current subvol as qgroup */\n\t\tqgroupid = root->root_key.objectid;\n\t}\n\n\tret = btrfs_limit_qgroup(trans, qgroupid, &sa->lim);\n\n\terr = btrfs_end_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_rescan(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_quota_rescan_args *qsa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tqsa = memdup_user(arg, sizeof(*qsa));\n\tif (IS_ERR(qsa)) {\n\t\tret = PTR_ERR(qsa);\n\t\tgoto drop_write;\n\t}\n\n\tif (qsa->flags) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_qgroup_rescan(fs_info);\n\nout:\n\tkfree(qsa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_rescan_status(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_ioctl_quota_rescan_args *qsa;\n\tint ret = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tqsa = kzalloc(sizeof(*qsa), GFP_KERNEL);\n\tif (!qsa)\n\t\treturn -ENOMEM;\n\n\tif (fs_info->qgroup_flags & BTRFS_QGROUP_STATUS_FLAG_RESCAN) {\n\t\tqsa->flags = 1;\n\t\tqsa->progress = fs_info->qgroup_rescan_progress.objectid;\n\t}\n\n\tif (copy_to_user(arg, qsa, sizeof(*qsa)))\n\t\tret = -EFAULT;\n\n\tkfree(qsa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_rescan_wait(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn btrfs_qgroup_wait_for_completion(fs_info, true);\n}\n\nstatic long _btrfs_ioctl_set_received_subvol(struct file *file,\n\t\t\t\t\t    struct btrfs_ioctl_received_subvol_args *sa)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_root_item *root_item = &root->root_item;\n\tstruct btrfs_trans_handle *trans;\n\tstruct timespec64 ct = current_time(inode);\n\tint ret = 0;\n\tint received_uuid_changed;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tdown_write(&fs_info->subvol_sem);\n\n\tif (btrfs_ino(BTRFS_I(inode)) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (btrfs_root_readonly(root)) {\n\t\tret = -EROFS;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * 1 - root item\n\t * 2 - uuid items (received uuid + subvol uuid)\n\t */\n\ttrans = btrfs_start_transaction(root, 3);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\ttrans = NULL;\n\t\tgoto out;\n\t}\n\n\tsa->rtransid = trans->transid;\n\tsa->rtime.sec = ct.tv_sec;\n\tsa->rtime.nsec = ct.tv_nsec;\n\n\treceived_uuid_changed = memcmp(root_item->received_uuid, sa->uuid,\n\t\t\t\t       BTRFS_UUID_SIZE);\n\tif (received_uuid_changed &&\n\t    !btrfs_is_empty_uuid(root_item->received_uuid)) {\n\t\tret = btrfs_uuid_tree_remove(trans, root_item->received_uuid,\n\t\t\t\t\t  BTRFS_UUID_KEY_RECEIVED_SUBVOL,\n\t\t\t\t\t  root->root_key.objectid);\n\t\tif (ret && ret != -ENOENT) {\n\t\t        btrfs_abort_transaction(trans, ret);\n\t\t        btrfs_end_transaction(trans);\n\t\t        goto out;\n\t\t}\n\t}\n\tmemcpy(root_item->received_uuid, sa->uuid, BTRFS_UUID_SIZE);\n\tbtrfs_set_root_stransid(root_item, sa->stransid);\n\tbtrfs_set_root_rtransid(root_item, sa->rtransid);\n\tbtrfs_set_stack_timespec_sec(&root_item->stime, sa->stime.sec);\n\tbtrfs_set_stack_timespec_nsec(&root_item->stime, sa->stime.nsec);\n\tbtrfs_set_stack_timespec_sec(&root_item->rtime, sa->rtime.sec);\n\tbtrfs_set_stack_timespec_nsec(&root_item->rtime, sa->rtime.nsec);\n\n\tret = btrfs_update_root(trans, fs_info->tree_root,\n\t\t\t\t&root->root_key, &root->root_item);\n\tif (ret < 0) {\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out;\n\t}\n\tif (received_uuid_changed && !btrfs_is_empty_uuid(sa->uuid)) {\n\t\tret = btrfs_uuid_tree_add(trans, sa->uuid,\n\t\t\t\t\t  BTRFS_UUID_KEY_RECEIVED_SUBVOL,\n\t\t\t\t\t  root->root_key.objectid);\n\t\tif (ret < 0 && ret != -EEXIST) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tbtrfs_end_transaction(trans);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tret = btrfs_commit_transaction(trans);\nout:\n\tup_write(&fs_info->subvol_sem);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\n#ifdef CONFIG_64BIT\nstatic long btrfs_ioctl_set_received_subvol_32(struct file *file,\n\t\t\t\t\t\tvoid __user *arg)\n{\n\tstruct btrfs_ioctl_received_subvol_args_32 *args32 = NULL;\n\tstruct btrfs_ioctl_received_subvol_args *args64 = NULL;\n\tint ret = 0;\n\n\targs32 = memdup_user(arg, sizeof(*args32));\n\tif (IS_ERR(args32))\n\t\treturn PTR_ERR(args32);\n\n\targs64 = kmalloc(sizeof(*args64), GFP_KERNEL);\n\tif (!args64) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmemcpy(args64->uuid, args32->uuid, BTRFS_UUID_SIZE);\n\targs64->stransid = args32->stransid;\n\targs64->rtransid = args32->rtransid;\n\targs64->stime.sec = args32->stime.sec;\n\targs64->stime.nsec = args32->stime.nsec;\n\targs64->rtime.sec = args32->rtime.sec;\n\targs64->rtime.nsec = args32->rtime.nsec;\n\targs64->flags = args32->flags;\n\n\tret = _btrfs_ioctl_set_received_subvol(file, args64);\n\tif (ret)\n\t\tgoto out;\n\n\tmemcpy(args32->uuid, args64->uuid, BTRFS_UUID_SIZE);\n\targs32->stransid = args64->stransid;\n\targs32->rtransid = args64->rtransid;\n\targs32->stime.sec = args64->stime.sec;\n\targs32->stime.nsec = args64->stime.nsec;\n\targs32->rtime.sec = args64->rtime.sec;\n\targs32->rtime.nsec = args64->rtime.nsec;\n\targs32->flags = args64->flags;\n\n\tret = copy_to_user(arg, args32, sizeof(*args32));\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(args32);\n\tkfree(args64);\n\treturn ret;\n}\n#endif\n\nstatic long btrfs_ioctl_set_received_subvol(struct file *file,\n\t\t\t\t\t    void __user *arg)\n{\n\tstruct btrfs_ioctl_received_subvol_args *sa = NULL;\n\tint ret = 0;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tret = _btrfs_ioctl_set_received_subvol(file, sa);\n\n\tif (ret)\n\t\tgoto out;\n\n\tret = copy_to_user(arg, sa, sizeof(*sa));\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic int btrfs_ioctl_get_fslabel(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tsize_t len;\n\tint ret;\n\tchar label[BTRFS_LABEL_SIZE];\n\n\tspin_lock(&fs_info->super_lock);\n\tmemcpy(label, fs_info->super_copy->label, BTRFS_LABEL_SIZE);\n\tspin_unlock(&fs_info->super_lock);\n\n\tlen = strnlen(label, BTRFS_LABEL_SIZE);\n\n\tif (len == BTRFS_LABEL_SIZE) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t   \"label is too long, return the first %zu bytes\",\n\t\t\t   --len);\n\t}\n\n\tret = copy_to_user(arg, label, len);\n\n\treturn ret ? -EFAULT : 0;\n}\n\nstatic int btrfs_ioctl_set_fslabel(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_super_block *super_block = fs_info->super_copy;\n\tstruct btrfs_trans_handle *trans;\n\tchar label[BTRFS_LABEL_SIZE];\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (copy_from_user(label, arg, sizeof(label)))\n\t\treturn -EFAULT;\n\n\tif (strnlen(label, BTRFS_LABEL_SIZE) == BTRFS_LABEL_SIZE) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"unable to set label with more than %d bytes\",\n\t\t\t  BTRFS_LABEL_SIZE - 1);\n\t\treturn -EINVAL;\n\t}\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_unlock;\n\t}\n\n\tspin_lock(&fs_info->super_lock);\n\tstrcpy(super_block->label, label);\n\tspin_unlock(&fs_info->super_lock);\n\tret = btrfs_commit_transaction(trans);\n\nout_unlock:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\n#define INIT_FEATURE_FLAGS(suffix) \\\n\t{ .compat_flags = BTRFS_FEATURE_COMPAT_##suffix, \\\n\t  .compat_ro_flags = BTRFS_FEATURE_COMPAT_RO_##suffix, \\\n\t  .incompat_flags = BTRFS_FEATURE_INCOMPAT_##suffix }\n\nint btrfs_ioctl_get_supported_features(void __user *arg)\n{\n\tstatic const struct btrfs_ioctl_feature_flags features[3] = {\n\t\tINIT_FEATURE_FLAGS(SUPP),\n\t\tINIT_FEATURE_FLAGS(SAFE_SET),\n\t\tINIT_FEATURE_FLAGS(SAFE_CLEAR)\n\t};\n\n\tif (copy_to_user(arg, &features, sizeof(features)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int btrfs_ioctl_get_features(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_super_block *super_block = fs_info->super_copy;\n\tstruct btrfs_ioctl_feature_flags features;\n\n\tfeatures.compat_flags = btrfs_super_compat_flags(super_block);\n\tfeatures.compat_ro_flags = btrfs_super_compat_ro_flags(super_block);\n\tfeatures.incompat_flags = btrfs_super_incompat_flags(super_block);\n\n\tif (copy_to_user(arg, &features, sizeof(features)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int check_feature_bits(struct btrfs_fs_info *fs_info,\n\t\t\t      enum btrfs_feature_set set,\n\t\t\t      u64 change_mask, u64 flags, u64 supported_flags,\n\t\t\t      u64 safe_set, u64 safe_clear)\n{\n\tconst char *type = btrfs_feature_set_names[set];\n\tchar *names;\n\tu64 disallowed, unsupported;\n\tu64 set_mask = flags & change_mask;\n\tu64 clear_mask = ~flags & change_mask;\n\n\tunsupported = set_mask & ~supported_flags;\n\tif (unsupported) {\n\t\tnames = btrfs_printable_features(set, unsupported);\n\t\tif (names) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"this kernel does not support the %s feature bit%s\",\n\t\t\t\t   names, strchr(names, ',') ? \"s\" : \"\");\n\t\t\tkfree(names);\n\t\t} else\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"this kernel does not support %s bits 0x%llx\",\n\t\t\t\t   type, unsupported);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tdisallowed = set_mask & ~safe_set;\n\tif (disallowed) {\n\t\tnames = btrfs_printable_features(set, disallowed);\n\t\tif (names) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"can't set the %s feature bit%s while mounted\",\n\t\t\t\t   names, strchr(names, ',') ? \"s\" : \"\");\n\t\t\tkfree(names);\n\t\t} else\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"can't set %s bits 0x%llx while mounted\",\n\t\t\t\t   type, disallowed);\n\t\treturn -EPERM;\n\t}\n\n\tdisallowed = clear_mask & ~safe_clear;\n\tif (disallowed) {\n\t\tnames = btrfs_printable_features(set, disallowed);\n\t\tif (names) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"can't clear the %s feature bit%s while mounted\",\n\t\t\t\t   names, strchr(names, ',') ? \"s\" : \"\");\n\t\t\tkfree(names);\n\t\t} else\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"can't clear %s bits 0x%llx while mounted\",\n\t\t\t\t   type, disallowed);\n\t\treturn -EPERM;\n\t}\n\n\treturn 0;\n}\n\n#define check_feature(fs_info, change_mask, flags, mask_base)\t\\\ncheck_feature_bits(fs_info, FEAT_##mask_base, change_mask, flags,\t\\\n\t\t   BTRFS_FEATURE_ ## mask_base ## _SUPP,\t\\\n\t\t   BTRFS_FEATURE_ ## mask_base ## _SAFE_SET,\t\\\n\t\t   BTRFS_FEATURE_ ## mask_base ## _SAFE_CLEAR)\n\nstatic int btrfs_ioctl_set_features(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_super_block *super_block = fs_info->super_copy;\n\tstruct btrfs_ioctl_feature_flags flags[2];\n\tstruct btrfs_trans_handle *trans;\n\tu64 newflags;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (copy_from_user(flags, arg, sizeof(flags)))\n\t\treturn -EFAULT;\n\n\t/* Nothing to do */\n\tif (!flags[0].compat_flags && !flags[0].compat_ro_flags &&\n\t    !flags[0].incompat_flags)\n\t\treturn 0;\n\n\tret = check_feature(fs_info, flags[0].compat_flags,\n\t\t\t    flags[1].compat_flags, COMPAT);\n\tif (ret)\n\t\treturn ret;\n\n\tret = check_feature(fs_info, flags[0].compat_ro_flags,\n\t\t\t    flags[1].compat_ro_flags, COMPAT_RO);\n\tif (ret)\n\t\treturn ret;\n\n\tret = check_feature(fs_info, flags[0].incompat_flags,\n\t\t\t    flags[1].incompat_flags, INCOMPAT);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_drop_write;\n\t}\n\n\tspin_lock(&fs_info->super_lock);\n\tnewflags = btrfs_super_compat_flags(super_block);\n\tnewflags |= flags[0].compat_flags & flags[1].compat_flags;\n\tnewflags &= ~(flags[0].compat_flags & ~flags[1].compat_flags);\n\tbtrfs_set_super_compat_flags(super_block, newflags);\n\n\tnewflags = btrfs_super_compat_ro_flags(super_block);\n\tnewflags |= flags[0].compat_ro_flags & flags[1].compat_ro_flags;\n\tnewflags &= ~(flags[0].compat_ro_flags & ~flags[1].compat_ro_flags);\n\tbtrfs_set_super_compat_ro_flags(super_block, newflags);\n\n\tnewflags = btrfs_super_incompat_flags(super_block);\n\tnewflags |= flags[0].incompat_flags & flags[1].incompat_flags;\n\tnewflags &= ~(flags[0].incompat_flags & ~flags[1].incompat_flags);\n\tbtrfs_set_super_incompat_flags(super_block, newflags);\n\tspin_unlock(&fs_info->super_lock);\n\n\tret = btrfs_commit_transaction(trans);\nout_drop_write:\n\tmnt_drop_write_file(file);\n\n\treturn ret;\n}\n\nstatic int _btrfs_ioctl_send(struct file *file, void __user *argp, bool compat)\n{\n\tstruct btrfs_ioctl_send_args *arg;\n\tint ret;\n\n\tif (compat) {\n#if defined(CONFIG_64BIT) && defined(CONFIG_COMPAT)\n\t\tstruct btrfs_ioctl_send_args_32 args32;\n\n\t\tret = copy_from_user(&args32, argp, sizeof(args32));\n\t\tif (ret)\n\t\t\treturn -EFAULT;\n\t\targ = kzalloc(sizeof(*arg), GFP_KERNEL);\n\t\tif (!arg)\n\t\t\treturn -ENOMEM;\n\t\targ->send_fd = args32.send_fd;\n\t\targ->clone_sources_count = args32.clone_sources_count;\n\t\targ->clone_sources = compat_ptr(args32.clone_sources);\n\t\targ->parent_root = args32.parent_root;\n\t\targ->flags = args32.flags;\n\t\tmemcpy(arg->reserved, args32.reserved,\n\t\t       sizeof(args32.reserved));\n#else\n\t\treturn -ENOTTY;\n#endif\n\t} else {\n\t\targ = memdup_user(argp, sizeof(*arg));\n\t\tif (IS_ERR(arg))\n\t\t\treturn PTR_ERR(arg);\n\t}\n\tret = btrfs_ioctl_send(file, arg);\n\tkfree(arg);\n\treturn ret;\n}\n\nlong btrfs_ioctl(struct file *file, unsigned int\n\t\tcmd, unsigned long arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (cmd) {\n\tcase FS_IOC_GETFLAGS:\n\t\treturn btrfs_ioctl_getflags(file, argp);\n\tcase FS_IOC_SETFLAGS:\n\t\treturn btrfs_ioctl_setflags(file, argp);\n\tcase FS_IOC_GETVERSION:\n\t\treturn btrfs_ioctl_getversion(file, argp);\n\tcase FITRIM:\n\t\treturn btrfs_ioctl_fitrim(file, argp);\n\tcase BTRFS_IOC_SNAP_CREATE:\n\t\treturn btrfs_ioctl_snap_create(file, argp, 0);\n\tcase BTRFS_IOC_SNAP_CREATE_V2:\n\t\treturn btrfs_ioctl_snap_create_v2(file, argp, 0);\n\tcase BTRFS_IOC_SUBVOL_CREATE:\n\t\treturn btrfs_ioctl_snap_create(file, argp, 1);\n\tcase BTRFS_IOC_SUBVOL_CREATE_V2:\n\t\treturn btrfs_ioctl_snap_create_v2(file, argp, 1);\n\tcase BTRFS_IOC_SNAP_DESTROY:\n\t\treturn btrfs_ioctl_snap_destroy(file, argp);\n\tcase BTRFS_IOC_SUBVOL_GETFLAGS:\n\t\treturn btrfs_ioctl_subvol_getflags(file, argp);\n\tcase BTRFS_IOC_SUBVOL_SETFLAGS:\n\t\treturn btrfs_ioctl_subvol_setflags(file, argp);\n\tcase BTRFS_IOC_DEFAULT_SUBVOL:\n\t\treturn btrfs_ioctl_default_subvol(file, argp);\n\tcase BTRFS_IOC_DEFRAG:\n\t\treturn btrfs_ioctl_defrag(file, NULL);\n\tcase BTRFS_IOC_DEFRAG_RANGE:\n\t\treturn btrfs_ioctl_defrag(file, argp);\n\tcase BTRFS_IOC_RESIZE:\n\t\treturn btrfs_ioctl_resize(file, argp);\n\tcase BTRFS_IOC_ADD_DEV:\n\t\treturn btrfs_ioctl_add_dev(fs_info, argp);\n\tcase BTRFS_IOC_RM_DEV:\n\t\treturn btrfs_ioctl_rm_dev(file, argp);\n\tcase BTRFS_IOC_RM_DEV_V2:\n\t\treturn btrfs_ioctl_rm_dev_v2(file, argp);\n\tcase BTRFS_IOC_FS_INFO:\n\t\treturn btrfs_ioctl_fs_info(fs_info, argp);\n\tcase BTRFS_IOC_DEV_INFO:\n\t\treturn btrfs_ioctl_dev_info(fs_info, argp);\n\tcase BTRFS_IOC_BALANCE:\n\t\treturn btrfs_ioctl_balance(file, NULL);\n\tcase BTRFS_IOC_TREE_SEARCH:\n\t\treturn btrfs_ioctl_tree_search(file, argp);\n\tcase BTRFS_IOC_TREE_SEARCH_V2:\n\t\treturn btrfs_ioctl_tree_search_v2(file, argp);\n\tcase BTRFS_IOC_INO_LOOKUP:\n\t\treturn btrfs_ioctl_ino_lookup(file, argp);\n\tcase BTRFS_IOC_INO_PATHS:\n\t\treturn btrfs_ioctl_ino_to_path(root, argp);\n\tcase BTRFS_IOC_LOGICAL_INO:\n\t\treturn btrfs_ioctl_logical_to_ino(fs_info, argp, 1);\n\tcase BTRFS_IOC_LOGICAL_INO_V2:\n\t\treturn btrfs_ioctl_logical_to_ino(fs_info, argp, 2);\n\tcase BTRFS_IOC_SPACE_INFO:\n\t\treturn btrfs_ioctl_space_info(fs_info, argp);\n\tcase BTRFS_IOC_SYNC: {\n\t\tint ret;\n\n\t\tret = btrfs_start_delalloc_roots(fs_info, -1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = btrfs_sync_fs(inode->i_sb, 1);\n\t\t/*\n\t\t * The transaction thread may want to do more work,\n\t\t * namely it pokes the cleaner kthread that will start\n\t\t * processing uncleaned subvols.\n\t\t */\n\t\twake_up_process(fs_info->transaction_kthread);\n\t\treturn ret;\n\t}\n\tcase BTRFS_IOC_START_SYNC:\n\t\treturn btrfs_ioctl_start_sync(root, argp);\n\tcase BTRFS_IOC_WAIT_SYNC:\n\t\treturn btrfs_ioctl_wait_sync(fs_info, argp);\n\tcase BTRFS_IOC_SCRUB:\n\t\treturn btrfs_ioctl_scrub(file, argp);\n\tcase BTRFS_IOC_SCRUB_CANCEL:\n\t\treturn btrfs_ioctl_scrub_cancel(fs_info);\n\tcase BTRFS_IOC_SCRUB_PROGRESS:\n\t\treturn btrfs_ioctl_scrub_progress(fs_info, argp);\n\tcase BTRFS_IOC_BALANCE_V2:\n\t\treturn btrfs_ioctl_balance(file, argp);\n\tcase BTRFS_IOC_BALANCE_CTL:\n\t\treturn btrfs_ioctl_balance_ctl(fs_info, arg);\n\tcase BTRFS_IOC_BALANCE_PROGRESS:\n\t\treturn btrfs_ioctl_balance_progress(fs_info, argp);\n\tcase BTRFS_IOC_SET_RECEIVED_SUBVOL:\n\t\treturn btrfs_ioctl_set_received_subvol(file, argp);\n#ifdef CONFIG_64BIT\n\tcase BTRFS_IOC_SET_RECEIVED_SUBVOL_32:\n\t\treturn btrfs_ioctl_set_received_subvol_32(file, argp);\n#endif\n\tcase BTRFS_IOC_SEND:\n\t\treturn _btrfs_ioctl_send(file, argp, false);\n#if defined(CONFIG_64BIT) && defined(CONFIG_COMPAT)\n\tcase BTRFS_IOC_SEND_32:\n\t\treturn _btrfs_ioctl_send(file, argp, true);\n#endif\n\tcase BTRFS_IOC_GET_DEV_STATS:\n\t\treturn btrfs_ioctl_get_dev_stats(fs_info, argp);\n\tcase BTRFS_IOC_QUOTA_CTL:\n\t\treturn btrfs_ioctl_quota_ctl(file, argp);\n\tcase BTRFS_IOC_QGROUP_ASSIGN:\n\t\treturn btrfs_ioctl_qgroup_assign(file, argp);\n\tcase BTRFS_IOC_QGROUP_CREATE:\n\t\treturn btrfs_ioctl_qgroup_create(file, argp);\n\tcase BTRFS_IOC_QGROUP_LIMIT:\n\t\treturn btrfs_ioctl_qgroup_limit(file, argp);\n\tcase BTRFS_IOC_QUOTA_RESCAN:\n\t\treturn btrfs_ioctl_quota_rescan(file, argp);\n\tcase BTRFS_IOC_QUOTA_RESCAN_STATUS:\n\t\treturn btrfs_ioctl_quota_rescan_status(file, argp);\n\tcase BTRFS_IOC_QUOTA_RESCAN_WAIT:\n\t\treturn btrfs_ioctl_quota_rescan_wait(file, argp);\n\tcase BTRFS_IOC_DEV_REPLACE:\n\t\treturn btrfs_ioctl_dev_replace(fs_info, argp);\n\tcase BTRFS_IOC_GET_FSLABEL:\n\t\treturn btrfs_ioctl_get_fslabel(file, argp);\n\tcase BTRFS_IOC_SET_FSLABEL:\n\t\treturn btrfs_ioctl_set_fslabel(file, argp);\n\tcase BTRFS_IOC_GET_SUPPORTED_FEATURES:\n\t\treturn btrfs_ioctl_get_supported_features(argp);\n\tcase BTRFS_IOC_GET_FEATURES:\n\t\treturn btrfs_ioctl_get_features(file, argp);\n\tcase BTRFS_IOC_SET_FEATURES:\n\t\treturn btrfs_ioctl_set_features(file, argp);\n\tcase FS_IOC_FSGETXATTR:\n\t\treturn btrfs_ioctl_fsgetxattr(file, argp);\n\tcase FS_IOC_FSSETXATTR:\n\t\treturn btrfs_ioctl_fssetxattr(file, argp);\n\tcase BTRFS_IOC_GET_SUBVOL_INFO:\n\t\treturn btrfs_ioctl_get_subvol_info(file, argp);\n\tcase BTRFS_IOC_GET_SUBVOL_ROOTREF:\n\t\treturn btrfs_ioctl_get_subvol_rootref(file, argp);\n\tcase BTRFS_IOC_INO_LOOKUP_USER:\n\t\treturn btrfs_ioctl_ino_lookup_user(file, argp);\n\t}\n\n\treturn -ENOTTY;\n}\n\n#ifdef CONFIG_COMPAT\nlong btrfs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\t/*\n\t * These all access 32-bit values anyway so no further\n\t * handling is necessary.\n\t */\n\tswitch (cmd) {\n\tcase FS_IOC32_GETFLAGS:\n\t\tcmd = FS_IOC_GETFLAGS;\n\t\tbreak;\n\tcase FS_IOC32_SETFLAGS:\n\t\tcmd = FS_IOC_SETFLAGS;\n\t\tbreak;\n\tcase FS_IOC32_GETVERSION:\n\t\tcmd = FS_IOC_GETVERSION;\n\t\tbreak;\n\t}\n\n\treturn btrfs_ioctl(file, cmd, (unsigned long) compat_ptr(arg));\n}\n#endif\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2011, 2012 STRATO.  All rights reserved.\n */\n\n#include <linux/blkdev.h>\n#include <linux/ratelimit.h>\n#include <linux/sched/mm.h>\n#include \"ctree.h\"\n#include \"volumes.h\"\n#include \"disk-io.h\"\n#include \"ordered-data.h\"\n#include \"transaction.h\"\n#include \"backref.h\"\n#include \"extent_io.h\"\n#include \"dev-replace.h\"\n#include \"check-integrity.h\"\n#include \"rcu-string.h\"\n#include \"raid56.h\"\n\n/*\n * This is only the first step towards a full-features scrub. It reads all\n * extent and super block and verifies the checksums. In case a bad checksum\n * is found or the extent cannot be read, good data will be written back if\n * any can be found.\n *\n * Future enhancements:\n *  - In case an unrepairable extent is encountered, track which files are\n *    affected and report them\n *  - track and record media errors, throw out bad devices\n *  - add a mode to also read unallocated space\n */\n\nstruct scrub_block;\nstruct scrub_ctx;\n\n/*\n * the following three values only influence the performance.\n * The last one configures the number of parallel and outstanding I/O\n * operations. The first two values configure an upper limit for the number\n * of (dynamically allocated) pages that are added to a bio.\n */\n#define SCRUB_PAGES_PER_RD_BIO\t32\t/* 128k per bio */\n#define SCRUB_PAGES_PER_WR_BIO\t32\t/* 128k per bio */\n#define SCRUB_BIOS_PER_SCTX\t64\t/* 8MB per device in flight */\n\n/*\n * the following value times PAGE_SIZE needs to be large enough to match the\n * largest node/leaf/sector size that shall be supported.\n * Values larger than BTRFS_STRIPE_LEN are not supported.\n */\n#define SCRUB_MAX_PAGES_PER_BLOCK\t16\t/* 64k per node/leaf/sector */\n\nstruct scrub_recover {\n\trefcount_t\t\trefs;\n\tstruct btrfs_bio\t*bbio;\n\tu64\t\t\tmap_length;\n};\n\nstruct scrub_page {\n\tstruct scrub_block\t*sblock;\n\tstruct page\t\t*page;\n\tstruct btrfs_device\t*dev;\n\tstruct list_head\tlist;\n\tu64\t\t\tflags;  /* extent flags */\n\tu64\t\t\tgeneration;\n\tu64\t\t\tlogical;\n\tu64\t\t\tphysical;\n\tu64\t\t\tphysical_for_dev_replace;\n\tatomic_t\t\trefs;\n\tstruct {\n\t\tunsigned int\tmirror_num:8;\n\t\tunsigned int\thave_csum:1;\n\t\tunsigned int\tio_error:1;\n\t};\n\tu8\t\t\tcsum[BTRFS_CSUM_SIZE];\n\n\tstruct scrub_recover\t*recover;\n};\n\nstruct scrub_bio {\n\tint\t\t\tindex;\n\tstruct scrub_ctx\t*sctx;\n\tstruct btrfs_device\t*dev;\n\tstruct bio\t\t*bio;\n\tblk_status_t\t\tstatus;\n\tu64\t\t\tlogical;\n\tu64\t\t\tphysical;\n#if SCRUB_PAGES_PER_WR_BIO >= SCRUB_PAGES_PER_RD_BIO\n\tstruct scrub_page\t*pagev[SCRUB_PAGES_PER_WR_BIO];\n#else\n\tstruct scrub_page\t*pagev[SCRUB_PAGES_PER_RD_BIO];\n#endif\n\tint\t\t\tpage_count;\n\tint\t\t\tnext_free;\n\tstruct btrfs_work\twork;\n};\n\nstruct scrub_block {\n\tstruct scrub_page\t*pagev[SCRUB_MAX_PAGES_PER_BLOCK];\n\tint\t\t\tpage_count;\n\tatomic_t\t\toutstanding_pages;\n\trefcount_t\t\trefs; /* free mem on transition to zero */\n\tstruct scrub_ctx\t*sctx;\n\tstruct scrub_parity\t*sparity;\n\tstruct {\n\t\tunsigned int\theader_error:1;\n\t\tunsigned int\tchecksum_error:1;\n\t\tunsigned int\tno_io_error_seen:1;\n\t\tunsigned int\tgeneration_error:1; /* also sets header_error */\n\n\t\t/* The following is for the data used to check parity */\n\t\t/* It is for the data with checksum */\n\t\tunsigned int\tdata_corrected:1;\n\t};\n\tstruct btrfs_work\twork;\n};\n\n/* Used for the chunks with parity stripe such RAID5/6 */\nstruct scrub_parity {\n\tstruct scrub_ctx\t*sctx;\n\n\tstruct btrfs_device\t*scrub_dev;\n\n\tu64\t\t\tlogic_start;\n\n\tu64\t\t\tlogic_end;\n\n\tint\t\t\tnsectors;\n\n\tu64\t\t\tstripe_len;\n\n\trefcount_t\t\trefs;\n\n\tstruct list_head\tspages;\n\n\t/* Work of parity check and repair */\n\tstruct btrfs_work\twork;\n\n\t/* Mark the parity blocks which have data */\n\tunsigned long\t\t*dbitmap;\n\n\t/*\n\t * Mark the parity blocks which have data, but errors happen when\n\t * read data or check data\n\t */\n\tunsigned long\t\t*ebitmap;\n\n\tunsigned long\t\tbitmap[0];\n};\n\nstruct scrub_ctx {\n\tstruct scrub_bio\t*bios[SCRUB_BIOS_PER_SCTX];\n\tstruct btrfs_fs_info\t*fs_info;\n\tint\t\t\tfirst_free;\n\tint\t\t\tcurr;\n\tatomic_t\t\tbios_in_flight;\n\tatomic_t\t\tworkers_pending;\n\tspinlock_t\t\tlist_lock;\n\twait_queue_head_t\tlist_wait;\n\tu16\t\t\tcsum_size;\n\tstruct list_head\tcsum_list;\n\tatomic_t\t\tcancel_req;\n\tint\t\t\treadonly;\n\tint\t\t\tpages_per_rd_bio;\n\n\tint\t\t\tis_dev_replace;\n\n\tstruct scrub_bio        *wr_curr_bio;\n\tstruct mutex            wr_lock;\n\tint                     pages_per_wr_bio; /* <= SCRUB_PAGES_PER_WR_BIO */\n\tstruct btrfs_device     *wr_tgtdev;\n\tbool                    flush_all_writes;\n\n\t/*\n\t * statistics\n\t */\n\tstruct btrfs_scrub_progress stat;\n\tspinlock_t\t\tstat_lock;\n\n\t/*\n\t * Use a ref counter to avoid use-after-free issues. Scrub workers\n\t * decrement bios_in_flight and workers_pending and then do a wakeup\n\t * on the list_wait wait queue. We must ensure the main scrub task\n\t * doesn't free the scrub context before or while the workers are\n\t * doing the wakeup() call.\n\t */\n\trefcount_t              refs;\n};\n\nstruct scrub_warning {\n\tstruct btrfs_path\t*path;\n\tu64\t\t\textent_item_size;\n\tconst char\t\t*errstr;\n\tu64\t\t\tphysical;\n\tu64\t\t\tlogical;\n\tstruct btrfs_device\t*dev;\n};\n\nstruct full_stripe_lock {\n\tstruct rb_node node;\n\tu64 logical;\n\tu64 refs;\n\tstruct mutex mutex;\n};\n\nstatic void scrub_pending_bio_inc(struct scrub_ctx *sctx);\nstatic void scrub_pending_bio_dec(struct scrub_ctx *sctx);\nstatic int scrub_handle_errored_block(struct scrub_block *sblock_to_check);\nstatic int scrub_setup_recheck_block(struct scrub_block *original_sblock,\n\t\t\t\t     struct scrub_block *sblocks_for_recheck);\nstatic void scrub_recheck_block(struct btrfs_fs_info *fs_info,\n\t\t\t\tstruct scrub_block *sblock,\n\t\t\t\tint retry_failed_mirror);\nstatic void scrub_recheck_block_checksum(struct scrub_block *sblock);\nstatic int scrub_repair_block_from_good_copy(struct scrub_block *sblock_bad,\n\t\t\t\t\t     struct scrub_block *sblock_good);\nstatic int scrub_repair_page_from_good_copy(struct scrub_block *sblock_bad,\n\t\t\t\t\t    struct scrub_block *sblock_good,\n\t\t\t\t\t    int page_num, int force_write);\nstatic void scrub_write_block_to_dev_replace(struct scrub_block *sblock);\nstatic int scrub_write_page_to_dev_replace(struct scrub_block *sblock,\n\t\t\t\t\t   int page_num);\nstatic int scrub_checksum_data(struct scrub_block *sblock);\nstatic int scrub_checksum_tree_block(struct scrub_block *sblock);\nstatic int scrub_checksum_super(struct scrub_block *sblock);\nstatic void scrub_block_get(struct scrub_block *sblock);\nstatic void scrub_block_put(struct scrub_block *sblock);\nstatic void scrub_page_get(struct scrub_page *spage);\nstatic void scrub_page_put(struct scrub_page *spage);\nstatic void scrub_parity_get(struct scrub_parity *sparity);\nstatic void scrub_parity_put(struct scrub_parity *sparity);\nstatic int scrub_add_page_to_rd_bio(struct scrub_ctx *sctx,\n\t\t\t\t    struct scrub_page *spage);\nstatic int scrub_pages(struct scrub_ctx *sctx, u64 logical, u64 len,\n\t\t       u64 physical, struct btrfs_device *dev, u64 flags,\n\t\t       u64 gen, int mirror_num, u8 *csum, int force,\n\t\t       u64 physical_for_dev_replace);\nstatic void scrub_bio_end_io(struct bio *bio);\nstatic void scrub_bio_end_io_worker(struct btrfs_work *work);\nstatic void scrub_block_complete(struct scrub_block *sblock);\nstatic void scrub_remap_extent(struct btrfs_fs_info *fs_info,\n\t\t\t       u64 extent_logical, u64 extent_len,\n\t\t\t       u64 *extent_physical,\n\t\t\t       struct btrfs_device **extent_dev,\n\t\t\t       int *extent_mirror_num);\nstatic int scrub_add_page_to_wr_bio(struct scrub_ctx *sctx,\n\t\t\t\t    struct scrub_page *spage);\nstatic void scrub_wr_submit(struct scrub_ctx *sctx);\nstatic void scrub_wr_bio_end_io(struct bio *bio);\nstatic void scrub_wr_bio_end_io_worker(struct btrfs_work *work);\nstatic void __scrub_blocked_if_needed(struct btrfs_fs_info *fs_info);\nstatic void scrub_blocked_if_needed(struct btrfs_fs_info *fs_info);\nstatic void scrub_put_ctx(struct scrub_ctx *sctx);\n\nstatic inline int scrub_is_page_on_raid56(struct scrub_page *page)\n{\n\treturn page->recover &&\n\t       (page->recover->bbio->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK);\n}\n\nstatic void scrub_pending_bio_inc(struct scrub_ctx *sctx)\n{\n\trefcount_inc(&sctx->refs);\n\tatomic_inc(&sctx->bios_in_flight);\n}\n\nstatic void scrub_pending_bio_dec(struct scrub_ctx *sctx)\n{\n\tatomic_dec(&sctx->bios_in_flight);\n\twake_up(&sctx->list_wait);\n\tscrub_put_ctx(sctx);\n}\n\nstatic void __scrub_blocked_if_needed(struct btrfs_fs_info *fs_info)\n{\n\twhile (atomic_read(&fs_info->scrub_pause_req)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t   atomic_read(&fs_info->scrub_pause_req) == 0);\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n}\n\nstatic void scrub_pause_on(struct btrfs_fs_info *fs_info)\n{\n\tatomic_inc(&fs_info->scrubs_paused);\n\twake_up(&fs_info->scrub_pause_wait);\n}\n\nstatic void scrub_pause_off(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->scrub_lock);\n\t__scrub_blocked_if_needed(fs_info);\n\tatomic_dec(&fs_info->scrubs_paused);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\twake_up(&fs_info->scrub_pause_wait);\n}\n\nstatic void scrub_blocked_if_needed(struct btrfs_fs_info *fs_info)\n{\n\tscrub_pause_on(fs_info);\n\tscrub_pause_off(fs_info);\n}\n\n/*\n * Insert new full stripe lock into full stripe locks tree\n *\n * Return pointer to existing or newly inserted full_stripe_lock structure if\n * everything works well.\n * Return ERR_PTR(-ENOMEM) if we failed to allocate memory\n *\n * NOTE: caller must hold full_stripe_locks_root->lock before calling this\n * function\n */\nstatic struct full_stripe_lock *insert_full_stripe_lock(\n\t\tstruct btrfs_full_stripe_locks_tree *locks_root,\n\t\tu64 fstripe_logical)\n{\n\tstruct rb_node **p;\n\tstruct rb_node *parent = NULL;\n\tstruct full_stripe_lock *entry;\n\tstruct full_stripe_lock *ret;\n\n\tlockdep_assert_held(&locks_root->lock);\n\n\tp = &locks_root->root.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct full_stripe_lock, node);\n\t\tif (fstripe_logical < entry->logical) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (fstripe_logical > entry->logical) {\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\tentry->refs++;\n\t\t\treturn entry;\n\t\t}\n\t}\n\n\t/*\n\t * Insert new lock.\n\t */\n\tret = kmalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn ERR_PTR(-ENOMEM);\n\tret->logical = fstripe_logical;\n\tret->refs = 1;\n\tmutex_init(&ret->mutex);\n\n\trb_link_node(&ret->node, parent, p);\n\trb_insert_color(&ret->node, &locks_root->root);\n\treturn ret;\n}\n\n/*\n * Search for a full stripe lock of a block group\n *\n * Return pointer to existing full stripe lock if found\n * Return NULL if not found\n */\nstatic struct full_stripe_lock *search_full_stripe_lock(\n\t\tstruct btrfs_full_stripe_locks_tree *locks_root,\n\t\tu64 fstripe_logical)\n{\n\tstruct rb_node *node;\n\tstruct full_stripe_lock *entry;\n\n\tlockdep_assert_held(&locks_root->lock);\n\n\tnode = locks_root->root.rb_node;\n\twhile (node) {\n\t\tentry = rb_entry(node, struct full_stripe_lock, node);\n\t\tif (fstripe_logical < entry->logical)\n\t\t\tnode = node->rb_left;\n\t\telse if (fstripe_logical > entry->logical)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn entry;\n\t}\n\treturn NULL;\n}\n\n/*\n * Helper to get full stripe logical from a normal bytenr.\n *\n * Caller must ensure @cache is a RAID56 block group.\n */\nstatic u64 get_full_stripe_logical(struct btrfs_block_group_cache *cache,\n\t\t\t\t   u64 bytenr)\n{\n\tu64 ret;\n\n\t/*\n\t * Due to chunk item size limit, full stripe length should not be\n\t * larger than U32_MAX. Just a sanity check here.\n\t */\n\tWARN_ON_ONCE(cache->full_stripe_len >= U32_MAX);\n\n\t/*\n\t * round_down() can only handle power of 2, while RAID56 full\n\t * stripe length can be 64KiB * n, so we need to manually round down.\n\t */\n\tret = div64_u64(bytenr - cache->key.objectid, cache->full_stripe_len) *\n\t\tcache->full_stripe_len + cache->key.objectid;\n\treturn ret;\n}\n\n/*\n * Lock a full stripe to avoid concurrency of recovery and read\n *\n * It's only used for profiles with parities (RAID5/6), for other profiles it\n * does nothing.\n *\n * Return 0 if we locked full stripe covering @bytenr, with a mutex held.\n * So caller must call unlock_full_stripe() at the same context.\n *\n * Return <0 if encounters error.\n */\nstatic int lock_full_stripe(struct btrfs_fs_info *fs_info, u64 bytenr,\n\t\t\t    bool *locked_ret)\n{\n\tstruct btrfs_block_group_cache *bg_cache;\n\tstruct btrfs_full_stripe_locks_tree *locks_root;\n\tstruct full_stripe_lock *existing;\n\tu64 fstripe_start;\n\tint ret = 0;\n\n\t*locked_ret = false;\n\tbg_cache = btrfs_lookup_block_group(fs_info, bytenr);\n\tif (!bg_cache) {\n\t\tASSERT(0);\n\t\treturn -ENOENT;\n\t}\n\n\t/* Profiles not based on parity don't need full stripe lock */\n\tif (!(bg_cache->flags & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\tgoto out;\n\tlocks_root = &bg_cache->full_stripe_locks_root;\n\n\tfstripe_start = get_full_stripe_logical(bg_cache, bytenr);\n\n\t/* Now insert the full stripe lock */\n\tmutex_lock(&locks_root->lock);\n\texisting = insert_full_stripe_lock(locks_root, fstripe_start);\n\tmutex_unlock(&locks_root->lock);\n\tif (IS_ERR(existing)) {\n\t\tret = PTR_ERR(existing);\n\t\tgoto out;\n\t}\n\tmutex_lock(&existing->mutex);\n\t*locked_ret = true;\nout:\n\tbtrfs_put_block_group(bg_cache);\n\treturn ret;\n}\n\n/*\n * Unlock a full stripe.\n *\n * NOTE: Caller must ensure it's the same context calling corresponding\n * lock_full_stripe().\n *\n * Return 0 if we unlock full stripe without problem.\n * Return <0 for error\n */\nstatic int unlock_full_stripe(struct btrfs_fs_info *fs_info, u64 bytenr,\n\t\t\t      bool locked)\n{\n\tstruct btrfs_block_group_cache *bg_cache;\n\tstruct btrfs_full_stripe_locks_tree *locks_root;\n\tstruct full_stripe_lock *fstripe_lock;\n\tu64 fstripe_start;\n\tbool freeit = false;\n\tint ret = 0;\n\n\t/* If we didn't acquire full stripe lock, no need to continue */\n\tif (!locked)\n\t\treturn 0;\n\n\tbg_cache = btrfs_lookup_block_group(fs_info, bytenr);\n\tif (!bg_cache) {\n\t\tASSERT(0);\n\t\treturn -ENOENT;\n\t}\n\tif (!(bg_cache->flags & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\tgoto out;\n\n\tlocks_root = &bg_cache->full_stripe_locks_root;\n\tfstripe_start = get_full_stripe_logical(bg_cache, bytenr);\n\n\tmutex_lock(&locks_root->lock);\n\tfstripe_lock = search_full_stripe_lock(locks_root, fstripe_start);\n\t/* Unpaired unlock_full_stripe() detected */\n\tif (!fstripe_lock) {\n\t\tWARN_ON(1);\n\t\tret = -ENOENT;\n\t\tmutex_unlock(&locks_root->lock);\n\t\tgoto out;\n\t}\n\n\tif (fstripe_lock->refs == 0) {\n\t\tWARN_ON(1);\n\t\tbtrfs_warn(fs_info, \"full stripe lock at %llu refcount underflow\",\n\t\t\tfstripe_lock->logical);\n\t} else {\n\t\tfstripe_lock->refs--;\n\t}\n\n\tif (fstripe_lock->refs == 0) {\n\t\trb_erase(&fstripe_lock->node, &locks_root->root);\n\t\tfreeit = true;\n\t}\n\tmutex_unlock(&locks_root->lock);\n\n\tmutex_unlock(&fstripe_lock->mutex);\n\tif (freeit)\n\t\tkfree(fstripe_lock);\nout:\n\tbtrfs_put_block_group(bg_cache);\n\treturn ret;\n}\n\nstatic void scrub_free_csums(struct scrub_ctx *sctx)\n{\n\twhile (!list_empty(&sctx->csum_list)) {\n\t\tstruct btrfs_ordered_sum *sum;\n\t\tsum = list_first_entry(&sctx->csum_list,\n\t\t\t\t       struct btrfs_ordered_sum, list);\n\t\tlist_del(&sum->list);\n\t\tkfree(sum);\n\t}\n}\n\nstatic noinline_for_stack void scrub_free_ctx(struct scrub_ctx *sctx)\n{\n\tint i;\n\n\tif (!sctx)\n\t\treturn;\n\n\t/* this can happen when scrub is cancelled */\n\tif (sctx->curr != -1) {\n\t\tstruct scrub_bio *sbio = sctx->bios[sctx->curr];\n\n\t\tfor (i = 0; i < sbio->page_count; i++) {\n\t\t\tWARN_ON(!sbio->pagev[i]->page);\n\t\t\tscrub_block_put(sbio->pagev[i]->sblock);\n\t\t}\n\t\tbio_put(sbio->bio);\n\t}\n\n\tfor (i = 0; i < SCRUB_BIOS_PER_SCTX; ++i) {\n\t\tstruct scrub_bio *sbio = sctx->bios[i];\n\n\t\tif (!sbio)\n\t\t\tbreak;\n\t\tkfree(sbio);\n\t}\n\n\tkfree(sctx->wr_curr_bio);\n\tscrub_free_csums(sctx);\n\tkfree(sctx);\n}\n\nstatic void scrub_put_ctx(struct scrub_ctx *sctx)\n{\n\tif (refcount_dec_and_test(&sctx->refs))\n\t\tscrub_free_ctx(sctx);\n}\n\nstatic noinline_for_stack struct scrub_ctx *scrub_setup_ctx(\n\t\tstruct btrfs_fs_info *fs_info, int is_dev_replace)\n{\n\tstruct scrub_ctx *sctx;\n\tint\t\ti;\n\n\tsctx = kzalloc(sizeof(*sctx), GFP_KERNEL);\n\tif (!sctx)\n\t\tgoto nomem;\n\trefcount_set(&sctx->refs, 1);\n\tsctx->is_dev_replace = is_dev_replace;\n\tsctx->pages_per_rd_bio = SCRUB_PAGES_PER_RD_BIO;\n\tsctx->curr = -1;\n\tsctx->fs_info = fs_info;\n\tfor (i = 0; i < SCRUB_BIOS_PER_SCTX; ++i) {\n\t\tstruct scrub_bio *sbio;\n\n\t\tsbio = kzalloc(sizeof(*sbio), GFP_KERNEL);\n\t\tif (!sbio)\n\t\t\tgoto nomem;\n\t\tsctx->bios[i] = sbio;\n\n\t\tsbio->index = i;\n\t\tsbio->sctx = sctx;\n\t\tsbio->page_count = 0;\n\t\tbtrfs_init_work(&sbio->work, btrfs_scrub_helper,\n\t\t\t\tscrub_bio_end_io_worker, NULL, NULL);\n\n\t\tif (i != SCRUB_BIOS_PER_SCTX - 1)\n\t\t\tsctx->bios[i]->next_free = i + 1;\n\t\telse\n\t\t\tsctx->bios[i]->next_free = -1;\n\t}\n\tsctx->first_free = 0;\n\tatomic_set(&sctx->bios_in_flight, 0);\n\tatomic_set(&sctx->workers_pending, 0);\n\tatomic_set(&sctx->cancel_req, 0);\n\tsctx->csum_size = btrfs_super_csum_size(fs_info->super_copy);\n\tINIT_LIST_HEAD(&sctx->csum_list);\n\n\tspin_lock_init(&sctx->list_lock);\n\tspin_lock_init(&sctx->stat_lock);\n\tinit_waitqueue_head(&sctx->list_wait);\n\n\tWARN_ON(sctx->wr_curr_bio != NULL);\n\tmutex_init(&sctx->wr_lock);\n\tsctx->wr_curr_bio = NULL;\n\tif (is_dev_replace) {\n\t\tWARN_ON(!fs_info->dev_replace.tgtdev);\n\t\tsctx->pages_per_wr_bio = SCRUB_PAGES_PER_WR_BIO;\n\t\tsctx->wr_tgtdev = fs_info->dev_replace.tgtdev;\n\t\tsctx->flush_all_writes = false;\n\t}\n\n\treturn sctx;\n\nnomem:\n\tscrub_free_ctx(sctx);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic int scrub_print_warning_inode(u64 inum, u64 offset, u64 root,\n\t\t\t\t     void *warn_ctx)\n{\n\tu64 isize;\n\tu32 nlink;\n\tint ret;\n\tint i;\n\tunsigned nofs_flag;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct scrub_warning *swarn = warn_ctx;\n\tstruct btrfs_fs_info *fs_info = swarn->dev->fs_info;\n\tstruct inode_fs_paths *ipath = NULL;\n\tstruct btrfs_root *local_root;\n\tstruct btrfs_key root_key;\n\tstruct btrfs_key key;\n\n\troot_key.objectid = root;\n\troot_key.type = BTRFS_ROOT_ITEM_KEY;\n\troot_key.offset = (u64)-1;\n\tlocal_root = btrfs_read_fs_root_no_name(fs_info, &root_key);\n\tif (IS_ERR(local_root)) {\n\t\tret = PTR_ERR(local_root);\n\t\tgoto err;\n\t}\n\n\t/*\n\t * this makes the path point to (inum INODE_ITEM ioff)\n\t */\n\tkey.objectid = inum;\n\tkey.type = BTRFS_INODE_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, local_root, &key, swarn->path, 0, 0);\n\tif (ret) {\n\t\tbtrfs_release_path(swarn->path);\n\t\tgoto err;\n\t}\n\n\teb = swarn->path->nodes[0];\n\tinode_item = btrfs_item_ptr(eb, swarn->path->slots[0],\n\t\t\t\t\tstruct btrfs_inode_item);\n\tisize = btrfs_inode_size(eb, inode_item);\n\tnlink = btrfs_inode_nlink(eb, inode_item);\n\tbtrfs_release_path(swarn->path);\n\n\t/*\n\t * init_path might indirectly call vmalloc, or use GFP_KERNEL. Scrub\n\t * uses GFP_NOFS in this context, so we keep it consistent but it does\n\t * not seem to be strictly necessary.\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\tipath = init_ipath(4096, local_root, swarn->path);\n\tmemalloc_nofs_restore(nofs_flag);\n\tif (IS_ERR(ipath)) {\n\t\tret = PTR_ERR(ipath);\n\t\tipath = NULL;\n\t\tgoto err;\n\t}\n\tret = paths_from_inode(inum, ipath);\n\n\tif (ret < 0)\n\t\tgoto err;\n\n\t/*\n\t * we deliberately ignore the bit ipath might have been too small to\n\t * hold all of the paths here\n\t */\n\tfor (i = 0; i < ipath->fspath->elem_cnt; ++i)\n\t\tbtrfs_warn_in_rcu(fs_info,\n\"%s at logical %llu on dev %s, physical %llu, root %llu, inode %llu, offset %llu, length %llu, links %u (path: %s)\",\n\t\t\t\t  swarn->errstr, swarn->logical,\n\t\t\t\t  rcu_str_deref(swarn->dev->name),\n\t\t\t\t  swarn->physical,\n\t\t\t\t  root, inum, offset,\n\t\t\t\t  min(isize - offset, (u64)PAGE_SIZE), nlink,\n\t\t\t\t  (char *)(unsigned long)ipath->fspath->val[i]);\n\n\tfree_ipath(ipath);\n\treturn 0;\n\nerr:\n\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t  \"%s at logical %llu on dev %s, physical %llu, root %llu, inode %llu, offset %llu: path resolving failed with ret=%d\",\n\t\t\t  swarn->errstr, swarn->logical,\n\t\t\t  rcu_str_deref(swarn->dev->name),\n\t\t\t  swarn->physical,\n\t\t\t  root, inum, offset, ret);\n\n\tfree_ipath(ipath);\n\treturn 0;\n}\n\nstatic void scrub_print_warning(const char *errstr, struct scrub_block *sblock)\n{\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_info *fs_info;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_extent_item *ei;\n\tstruct scrub_warning swarn;\n\tunsigned long ptr = 0;\n\tu64 extent_item_pos;\n\tu64 flags = 0;\n\tu64 ref_root;\n\tu32 item_size;\n\tu8 ref_level = 0;\n\tint ret;\n\n\tWARN_ON(sblock->page_count < 1);\n\tdev = sblock->pagev[0]->dev;\n\tfs_info = sblock->sctx->fs_info;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn;\n\n\tswarn.physical = sblock->pagev[0]->physical;\n\tswarn.logical = sblock->pagev[0]->logical;\n\tswarn.errstr = errstr;\n\tswarn.dev = NULL;\n\n\tret = extent_from_logical(fs_info, swarn.logical, path, &found_key,\n\t\t\t\t  &flags);\n\tif (ret < 0)\n\t\tgoto out;\n\n\textent_item_pos = swarn.logical - found_key.objectid;\n\tswarn.extent_item_size = found_key.offset;\n\n\teb = path->nodes[0];\n\tei = btrfs_item_ptr(eb, path->slots[0], struct btrfs_extent_item);\n\titem_size = btrfs_item_size_nr(eb, path->slots[0]);\n\n\tif (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\n\t\tdo {\n\t\t\tret = tree_backref_for_extent(&ptr, eb, &found_key, ei,\n\t\t\t\t\t\t      item_size, &ref_root,\n\t\t\t\t\t\t      &ref_level);\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\"%s at logical %llu on dev %s, physical %llu: metadata %s (level %d) in tree %llu\",\n\t\t\t\terrstr, swarn.logical,\n\t\t\t\trcu_str_deref(dev->name),\n\t\t\t\tswarn.physical,\n\t\t\t\tref_level ? \"node\" : \"leaf\",\n\t\t\t\tret < 0 ? -1 : ref_level,\n\t\t\t\tret < 0 ? -1 : ref_root);\n\t\t} while (ret != 1);\n\t\tbtrfs_release_path(path);\n\t} else {\n\t\tbtrfs_release_path(path);\n\t\tswarn.path = path;\n\t\tswarn.dev = dev;\n\t\titerate_extent_inodes(fs_info, found_key.objectid,\n\t\t\t\t\textent_item_pos, 1,\n\t\t\t\t\tscrub_print_warning_inode, &swarn, false);\n\t}\n\nout:\n\tbtrfs_free_path(path);\n}\n\nstatic inline void scrub_get_recover(struct scrub_recover *recover)\n{\n\trefcount_inc(&recover->refs);\n}\n\nstatic inline void scrub_put_recover(struct btrfs_fs_info *fs_info,\n\t\t\t\t     struct scrub_recover *recover)\n{\n\tif (refcount_dec_and_test(&recover->refs)) {\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\tbtrfs_put_bbio(recover->bbio);\n\t\tkfree(recover);\n\t}\n}\n\n/*\n * scrub_handle_errored_block gets called when either verification of the\n * pages failed or the bio failed to read, e.g. with EIO. In the latter\n * case, this function handles all pages in the bio, even though only one\n * may be bad.\n * The goal of this function is to repair the errored block by using the\n * contents of one of the mirrors.\n */\nstatic int scrub_handle_errored_block(struct scrub_block *sblock_to_check)\n{\n\tstruct scrub_ctx *sctx = sblock_to_check->sctx;\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_info *fs_info;\n\tu64 logical;\n\tunsigned int failed_mirror_index;\n\tunsigned int is_metadata;\n\tunsigned int have_csum;\n\tstruct scrub_block *sblocks_for_recheck; /* holds one for each mirror */\n\tstruct scrub_block *sblock_bad;\n\tint ret;\n\tint mirror_index;\n\tint page_num;\n\tint success;\n\tbool full_stripe_locked;\n\tunsigned int nofs_flag;\n\tstatic DEFINE_RATELIMIT_STATE(_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\tBUG_ON(sblock_to_check->page_count < 1);\n\tfs_info = sctx->fs_info;\n\tif (sblock_to_check->pagev[0]->flags & BTRFS_EXTENT_FLAG_SUPER) {\n\t\t/*\n\t\t * if we find an error in a super block, we just report it.\n\t\t * They will get written with the next transaction commit\n\t\t * anyway\n\t\t */\n\t\tspin_lock(&sctx->stat_lock);\n\t\t++sctx->stat.super_errors;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn 0;\n\t}\n\tlogical = sblock_to_check->pagev[0]->logical;\n\tBUG_ON(sblock_to_check->pagev[0]->mirror_num < 1);\n\tfailed_mirror_index = sblock_to_check->pagev[0]->mirror_num - 1;\n\tis_metadata = !(sblock_to_check->pagev[0]->flags &\n\t\t\tBTRFS_EXTENT_FLAG_DATA);\n\thave_csum = sblock_to_check->pagev[0]->have_csum;\n\tdev = sblock_to_check->pagev[0]->dev;\n\n\t/*\n\t * We must use GFP_NOFS because the scrub task might be waiting for a\n\t * worker task executing this function and in turn a transaction commit\n\t * might be waiting the scrub task to pause (which needs to wait for all\n\t * the worker tasks to complete before pausing).\n\t * We do allocations in the workers through insert_full_stripe_lock()\n\t * and scrub_add_page_to_wr_bio(), which happens down the call chain of\n\t * this function.\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\t/*\n\t * For RAID5/6, race can happen for a different device scrub thread.\n\t * For data corruption, Parity and Data threads will both try\n\t * to recovery the data.\n\t * Race can lead to doubly added csum error, or even unrecoverable\n\t * error.\n\t */\n\tret = lock_full_stripe(fs_info, logical, &full_stripe_locked);\n\tif (ret < 0) {\n\t\tmemalloc_nofs_restore(nofs_flag);\n\t\tspin_lock(&sctx->stat_lock);\n\t\tif (ret == -ENOMEM)\n\t\t\tsctx->stat.malloc_errors++;\n\t\tsctx->stat.read_errors++;\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * read all mirrors one after the other. This includes to\n\t * re-read the extent or metadata block that failed (that was\n\t * the cause that this fixup code is called) another time,\n\t * page by page this time in order to know which pages\n\t * caused I/O errors and which ones are good (for all mirrors).\n\t * It is the goal to handle the situation when more than one\n\t * mirror contains I/O errors, but the errors do not\n\t * overlap, i.e. the data can be repaired by selecting the\n\t * pages from those mirrors without I/O error on the\n\t * particular pages. One example (with blocks >= 2 * PAGE_SIZE)\n\t * would be that mirror #1 has an I/O error on the first page,\n\t * the second page is good, and mirror #2 has an I/O error on\n\t * the second page, but the first page is good.\n\t * Then the first page of the first mirror can be repaired by\n\t * taking the first page of the second mirror, and the\n\t * second page of the second mirror can be repaired by\n\t * copying the contents of the 2nd page of the 1st mirror.\n\t * One more note: if the pages of one mirror contain I/O\n\t * errors, the checksum cannot be verified. In order to get\n\t * the best data for repairing, the first attempt is to find\n\t * a mirror without I/O errors and with a validated checksum.\n\t * Only if this is not possible, the pages are picked from\n\t * mirrors with I/O errors without considering the checksum.\n\t * If the latter is the case, at the end, the checksum of the\n\t * repaired area is verified in order to correctly maintain\n\t * the statistics.\n\t */\n\n\tsblocks_for_recheck = kcalloc(BTRFS_MAX_MIRRORS,\n\t\t\t\t      sizeof(*sblocks_for_recheck), GFP_KERNEL);\n\tif (!sblocks_for_recheck) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.malloc_errors++;\n\t\tsctx->stat.read_errors++;\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_dev_stat_inc_and_print(dev, BTRFS_DEV_STAT_READ_ERRS);\n\t\tgoto out;\n\t}\n\n\t/* setup the context, map the logical blocks and alloc the pages */\n\tret = scrub_setup_recheck_block(sblock_to_check, sblocks_for_recheck);\n\tif (ret) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.read_errors++;\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_dev_stat_inc_and_print(dev, BTRFS_DEV_STAT_READ_ERRS);\n\t\tgoto out;\n\t}\n\tBUG_ON(failed_mirror_index >= BTRFS_MAX_MIRRORS);\n\tsblock_bad = sblocks_for_recheck + failed_mirror_index;\n\n\t/* build and submit the bios for the failed mirror, check checksums */\n\tscrub_recheck_block(fs_info, sblock_bad, 1);\n\n\tif (!sblock_bad->header_error && !sblock_bad->checksum_error &&\n\t    sblock_bad->no_io_error_seen) {\n\t\t/*\n\t\t * the error disappeared after reading page by page, or\n\t\t * the area was part of a huge bio and other parts of the\n\t\t * bio caused I/O errors, or the block layer merged several\n\t\t * read requests into one and the error is caused by a\n\t\t * different bio (usually one of the two latter cases is\n\t\t * the cause)\n\t\t */\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.unverified_errors++;\n\t\tsblock_to_check->data_corrected = 1;\n\t\tspin_unlock(&sctx->stat_lock);\n\n\t\tif (sctx->is_dev_replace)\n\t\t\tscrub_write_block_to_dev_replace(sblock_bad);\n\t\tgoto out;\n\t}\n\n\tif (!sblock_bad->no_io_error_seen) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.read_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (__ratelimit(&_rs))\n\t\t\tscrub_print_warning(\"i/o error\", sblock_to_check);\n\t\tbtrfs_dev_stat_inc_and_print(dev, BTRFS_DEV_STAT_READ_ERRS);\n\t} else if (sblock_bad->checksum_error) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.csum_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (__ratelimit(&_rs))\n\t\t\tscrub_print_warning(\"checksum error\", sblock_to_check);\n\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t     BTRFS_DEV_STAT_CORRUPTION_ERRS);\n\t} else if (sblock_bad->header_error) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.verify_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (__ratelimit(&_rs))\n\t\t\tscrub_print_warning(\"checksum/header error\",\n\t\t\t\t\t    sblock_to_check);\n\t\tif (sblock_bad->generation_error)\n\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\tBTRFS_DEV_STAT_GENERATION_ERRS);\n\t\telse\n\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\tBTRFS_DEV_STAT_CORRUPTION_ERRS);\n\t}\n\n\tif (sctx->readonly) {\n\t\tASSERT(!sctx->is_dev_replace);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * now build and submit the bios for the other mirrors, check\n\t * checksums.\n\t * First try to pick the mirror which is completely without I/O\n\t * errors and also does not have a checksum error.\n\t * If one is found, and if a checksum is present, the full block\n\t * that is known to contain an error is rewritten. Afterwards\n\t * the block is known to be corrected.\n\t * If a mirror is found which is completely correct, and no\n\t * checksum is present, only those pages are rewritten that had\n\t * an I/O error in the block to be repaired, since it cannot be\n\t * determined, which copy of the other pages is better (and it\n\t * could happen otherwise that a correct page would be\n\t * overwritten by a bad one).\n\t */\n\tfor (mirror_index = 0; ;mirror_index++) {\n\t\tstruct scrub_block *sblock_other;\n\n\t\tif (mirror_index == failed_mirror_index)\n\t\t\tcontinue;\n\n\t\t/* raid56's mirror can be more than BTRFS_MAX_MIRRORS */\n\t\tif (!scrub_is_page_on_raid56(sblock_bad->pagev[0])) {\n\t\t\tif (mirror_index >= BTRFS_MAX_MIRRORS)\n\t\t\t\tbreak;\n\t\t\tif (!sblocks_for_recheck[mirror_index].page_count)\n\t\t\t\tbreak;\n\n\t\t\tsblock_other = sblocks_for_recheck + mirror_index;\n\t\t} else {\n\t\t\tstruct scrub_recover *r = sblock_bad->pagev[0]->recover;\n\t\t\tint max_allowed = r->bbio->num_stripes -\n\t\t\t\t\t\tr->bbio->num_tgtdevs;\n\n\t\t\tif (mirror_index >= max_allowed)\n\t\t\t\tbreak;\n\t\t\tif (!sblocks_for_recheck[1].page_count)\n\t\t\t\tbreak;\n\n\t\t\tASSERT(failed_mirror_index == 0);\n\t\t\tsblock_other = sblocks_for_recheck + 1;\n\t\t\tsblock_other->pagev[0]->mirror_num = 1 + mirror_index;\n\t\t}\n\n\t\t/* build and submit the bios, check checksums */\n\t\tscrub_recheck_block(fs_info, sblock_other, 0);\n\n\t\tif (!sblock_other->header_error &&\n\t\t    !sblock_other->checksum_error &&\n\t\t    sblock_other->no_io_error_seen) {\n\t\t\tif (sctx->is_dev_replace) {\n\t\t\t\tscrub_write_block_to_dev_replace(sblock_other);\n\t\t\t\tgoto corrected_error;\n\t\t\t} else {\n\t\t\t\tret = scrub_repair_block_from_good_copy(\n\t\t\t\t\t\tsblock_bad, sblock_other);\n\t\t\t\tif (!ret)\n\t\t\t\t\tgoto corrected_error;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (sblock_bad->no_io_error_seen && !sctx->is_dev_replace)\n\t\tgoto did_not_correct_error;\n\n\t/*\n\t * In case of I/O errors in the area that is supposed to be\n\t * repaired, continue by picking good copies of those pages.\n\t * Select the good pages from mirrors to rewrite bad pages from\n\t * the area to fix. Afterwards verify the checksum of the block\n\t * that is supposed to be repaired. This verification step is\n\t * only done for the purpose of statistic counting and for the\n\t * final scrub report, whether errors remain.\n\t * A perfect algorithm could make use of the checksum and try\n\t * all possible combinations of pages from the different mirrors\n\t * until the checksum verification succeeds. For example, when\n\t * the 2nd page of mirror #1 faces I/O errors, and the 2nd page\n\t * of mirror #2 is readable but the final checksum test fails,\n\t * then the 2nd page of mirror #3 could be tried, whether now\n\t * the final checksum succeeds. But this would be a rare\n\t * exception and is therefore not implemented. At least it is\n\t * avoided that the good copy is overwritten.\n\t * A more useful improvement would be to pick the sectors\n\t * without I/O error based on sector sizes (512 bytes on legacy\n\t * disks) instead of on PAGE_SIZE. Then maybe 512 byte of one\n\t * mirror could be repaired by taking 512 byte of a different\n\t * mirror, even if other 512 byte sectors in the same PAGE_SIZE\n\t * area are unreadable.\n\t */\n\tsuccess = 1;\n\tfor (page_num = 0; page_num < sblock_bad->page_count;\n\t     page_num++) {\n\t\tstruct scrub_page *page_bad = sblock_bad->pagev[page_num];\n\t\tstruct scrub_block *sblock_other = NULL;\n\n\t\t/* skip no-io-error page in scrub */\n\t\tif (!page_bad->io_error && !sctx->is_dev_replace)\n\t\t\tcontinue;\n\n\t\tif (scrub_is_page_on_raid56(sblock_bad->pagev[0])) {\n\t\t\t/*\n\t\t\t * In case of dev replace, if raid56 rebuild process\n\t\t\t * didn't work out correct data, then copy the content\n\t\t\t * in sblock_bad to make sure target device is identical\n\t\t\t * to source device, instead of writing garbage data in\n\t\t\t * sblock_for_recheck array to target device.\n\t\t\t */\n\t\t\tsblock_other = NULL;\n\t\t} else if (page_bad->io_error) {\n\t\t\t/* try to find no-io-error page in mirrors */\n\t\t\tfor (mirror_index = 0;\n\t\t\t     mirror_index < BTRFS_MAX_MIRRORS &&\n\t\t\t     sblocks_for_recheck[mirror_index].page_count > 0;\n\t\t\t     mirror_index++) {\n\t\t\t\tif (!sblocks_for_recheck[mirror_index].\n\t\t\t\t    pagev[page_num]->io_error) {\n\t\t\t\t\tsblock_other = sblocks_for_recheck +\n\t\t\t\t\t\t       mirror_index;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!sblock_other)\n\t\t\t\tsuccess = 0;\n\t\t}\n\n\t\tif (sctx->is_dev_replace) {\n\t\t\t/*\n\t\t\t * did not find a mirror to fetch the page\n\t\t\t * from. scrub_write_page_to_dev_replace()\n\t\t\t * handles this case (page->io_error), by\n\t\t\t * filling the block with zeros before\n\t\t\t * submitting the write request\n\t\t\t */\n\t\t\tif (!sblock_other)\n\t\t\t\tsblock_other = sblock_bad;\n\n\t\t\tif (scrub_write_page_to_dev_replace(sblock_other,\n\t\t\t\t\t\t\t    page_num) != 0) {\n\t\t\t\tatomic64_inc(\n\t\t\t\t\t&fs_info->dev_replace.num_write_errors);\n\t\t\t\tsuccess = 0;\n\t\t\t}\n\t\t} else if (sblock_other) {\n\t\t\tret = scrub_repair_page_from_good_copy(sblock_bad,\n\t\t\t\t\t\t\t       sblock_other,\n\t\t\t\t\t\t\t       page_num, 0);\n\t\t\tif (0 == ret)\n\t\t\t\tpage_bad->io_error = 0;\n\t\t\telse\n\t\t\t\tsuccess = 0;\n\t\t}\n\t}\n\n\tif (success && !sctx->is_dev_replace) {\n\t\tif (is_metadata || have_csum) {\n\t\t\t/*\n\t\t\t * need to verify the checksum now that all\n\t\t\t * sectors on disk are repaired (the write\n\t\t\t * request for data to be repaired is on its way).\n\t\t\t * Just be lazy and use scrub_recheck_block()\n\t\t\t * which re-reads the data before the checksum\n\t\t\t * is verified, but most likely the data comes out\n\t\t\t * of the page cache.\n\t\t\t */\n\t\t\tscrub_recheck_block(fs_info, sblock_bad, 1);\n\t\t\tif (!sblock_bad->header_error &&\n\t\t\t    !sblock_bad->checksum_error &&\n\t\t\t    sblock_bad->no_io_error_seen)\n\t\t\t\tgoto corrected_error;\n\t\t\telse\n\t\t\t\tgoto did_not_correct_error;\n\t\t} else {\ncorrected_error:\n\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\tsctx->stat.corrected_errors++;\n\t\t\tsblock_to_check->data_corrected = 1;\n\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\t\"fixed up error at logical %llu on dev %s\",\n\t\t\t\tlogical, rcu_str_deref(dev->name));\n\t\t}\n\t} else {\ndid_not_correct_error:\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\"unable to fixup (regular) error at logical %llu on dev %s\",\n\t\t\tlogical, rcu_str_deref(dev->name));\n\t}\n\nout:\n\tif (sblocks_for_recheck) {\n\t\tfor (mirror_index = 0; mirror_index < BTRFS_MAX_MIRRORS;\n\t\t     mirror_index++) {\n\t\t\tstruct scrub_block *sblock = sblocks_for_recheck +\n\t\t\t\t\t\t     mirror_index;\n\t\t\tstruct scrub_recover *recover;\n\t\t\tint page_index;\n\n\t\t\tfor (page_index = 0; page_index < sblock->page_count;\n\t\t\t     page_index++) {\n\t\t\t\tsblock->pagev[page_index]->sblock = NULL;\n\t\t\t\trecover = sblock->pagev[page_index]->recover;\n\t\t\t\tif (recover) {\n\t\t\t\t\tscrub_put_recover(fs_info, recover);\n\t\t\t\t\tsblock->pagev[page_index]->recover =\n\t\t\t\t\t\t\t\t\tNULL;\n\t\t\t\t}\n\t\t\t\tscrub_page_put(sblock->pagev[page_index]);\n\t\t\t}\n\t\t}\n\t\tkfree(sblocks_for_recheck);\n\t}\n\n\tret = unlock_full_stripe(fs_info, logical, full_stripe_locked);\n\tmemalloc_nofs_restore(nofs_flag);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn 0;\n}\n\nstatic inline int scrub_nr_raid_mirrors(struct btrfs_bio *bbio)\n{\n\tif (bbio->map_type & BTRFS_BLOCK_GROUP_RAID5)\n\t\treturn 2;\n\telse if (bbio->map_type & BTRFS_BLOCK_GROUP_RAID6)\n\t\treturn 3;\n\telse\n\t\treturn (int)bbio->num_stripes;\n}\n\nstatic inline void scrub_stripe_index_and_offset(u64 logical, u64 map_type,\n\t\t\t\t\t\t u64 *raid_map,\n\t\t\t\t\t\t u64 mapped_length,\n\t\t\t\t\t\t int nstripes, int mirror,\n\t\t\t\t\t\t int *stripe_index,\n\t\t\t\t\t\t u64 *stripe_offset)\n{\n\tint i;\n\n\tif (map_type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\t/* RAID5/6 */\n\t\tfor (i = 0; i < nstripes; i++) {\n\t\t\tif (raid_map[i] == RAID6_Q_STRIPE ||\n\t\t\t    raid_map[i] == RAID5_P_STRIPE)\n\t\t\t\tcontinue;\n\n\t\t\tif (logical >= raid_map[i] &&\n\t\t\t    logical < raid_map[i] + mapped_length)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t*stripe_index = i;\n\t\t*stripe_offset = logical - raid_map[i];\n\t} else {\n\t\t/* The other RAID type */\n\t\t*stripe_index = mirror;\n\t\t*stripe_offset = 0;\n\t}\n}\n\nstatic int scrub_setup_recheck_block(struct scrub_block *original_sblock,\n\t\t\t\t     struct scrub_block *sblocks_for_recheck)\n{\n\tstruct scrub_ctx *sctx = original_sblock->sctx;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tu64 length = original_sblock->page_count * PAGE_SIZE;\n\tu64 logical = original_sblock->pagev[0]->logical;\n\tu64 generation = original_sblock->pagev[0]->generation;\n\tu64 flags = original_sblock->pagev[0]->flags;\n\tu64 have_csum = original_sblock->pagev[0]->have_csum;\n\tstruct scrub_recover *recover;\n\tstruct btrfs_bio *bbio;\n\tu64 sublen;\n\tu64 mapped_length;\n\tu64 stripe_offset;\n\tint stripe_index;\n\tint page_index = 0;\n\tint mirror_index;\n\tint nmirrors;\n\tint ret;\n\n\t/*\n\t * note: the two members refs and outstanding_pages\n\t * are not used (and not set) in the blocks that are used for\n\t * the recheck procedure\n\t */\n\n\twhile (length > 0) {\n\t\tsublen = min_t(u64, length, PAGE_SIZE);\n\t\tmapped_length = sublen;\n\t\tbbio = NULL;\n\n\t\t/*\n\t\t * with a length of PAGE_SIZE, each returned stripe\n\t\t * represents one mirror\n\t\t */\n\t\tbtrfs_bio_counter_inc_blocked(fs_info);\n\t\tret = btrfs_map_sblock(fs_info, BTRFS_MAP_GET_READ_MIRRORS,\n\t\t\t\tlogical, &mapped_length, &bbio);\n\t\tif (ret || !bbio || mapped_length < sublen) {\n\t\t\tbtrfs_put_bbio(bbio);\n\t\t\tbtrfs_bio_counter_dec(fs_info);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\trecover = kzalloc(sizeof(struct scrub_recover), GFP_NOFS);\n\t\tif (!recover) {\n\t\t\tbtrfs_put_bbio(bbio);\n\t\t\tbtrfs_bio_counter_dec(fs_info);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\trefcount_set(&recover->refs, 1);\n\t\trecover->bbio = bbio;\n\t\trecover->map_length = mapped_length;\n\n\t\tBUG_ON(page_index >= SCRUB_MAX_PAGES_PER_BLOCK);\n\n\t\tnmirrors = min(scrub_nr_raid_mirrors(bbio), BTRFS_MAX_MIRRORS);\n\n\t\tfor (mirror_index = 0; mirror_index < nmirrors;\n\t\t     mirror_index++) {\n\t\t\tstruct scrub_block *sblock;\n\t\t\tstruct scrub_page *page;\n\n\t\t\tsblock = sblocks_for_recheck + mirror_index;\n\t\t\tsblock->sctx = sctx;\n\n\t\t\tpage = kzalloc(sizeof(*page), GFP_NOFS);\n\t\t\tif (!page) {\nleave_nomem:\n\t\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\t\tsctx->stat.malloc_errors++;\n\t\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\t\tscrub_put_recover(fs_info, recover);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tscrub_page_get(page);\n\t\t\tsblock->pagev[page_index] = page;\n\t\t\tpage->sblock = sblock;\n\t\t\tpage->flags = flags;\n\t\t\tpage->generation = generation;\n\t\t\tpage->logical = logical;\n\t\t\tpage->have_csum = have_csum;\n\t\t\tif (have_csum)\n\t\t\t\tmemcpy(page->csum,\n\t\t\t\t       original_sblock->pagev[0]->csum,\n\t\t\t\t       sctx->csum_size);\n\n\t\t\tscrub_stripe_index_and_offset(logical,\n\t\t\t\t\t\t      bbio->map_type,\n\t\t\t\t\t\t      bbio->raid_map,\n\t\t\t\t\t\t      mapped_length,\n\t\t\t\t\t\t      bbio->num_stripes -\n\t\t\t\t\t\t      bbio->num_tgtdevs,\n\t\t\t\t\t\t      mirror_index,\n\t\t\t\t\t\t      &stripe_index,\n\t\t\t\t\t\t      &stripe_offset);\n\t\t\tpage->physical = bbio->stripes[stripe_index].physical +\n\t\t\t\t\t stripe_offset;\n\t\t\tpage->dev = bbio->stripes[stripe_index].dev;\n\n\t\t\tBUG_ON(page_index >= original_sblock->page_count);\n\t\t\tpage->physical_for_dev_replace =\n\t\t\t\toriginal_sblock->pagev[page_index]->\n\t\t\t\tphysical_for_dev_replace;\n\t\t\t/* for missing devices, dev->bdev is NULL */\n\t\t\tpage->mirror_num = mirror_index + 1;\n\t\t\tsblock->page_count++;\n\t\t\tpage->page = alloc_page(GFP_NOFS);\n\t\t\tif (!page->page)\n\t\t\t\tgoto leave_nomem;\n\n\t\t\tscrub_get_recover(recover);\n\t\t\tpage->recover = recover;\n\t\t}\n\t\tscrub_put_recover(fs_info, recover);\n\t\tlength -= sublen;\n\t\tlogical += sublen;\n\t\tpage_index++;\n\t}\n\n\treturn 0;\n}\n\nstatic void scrub_bio_wait_endio(struct bio *bio)\n{\n\tcomplete(bio->bi_private);\n}\n\nstatic int scrub_submit_raid56_bio_wait(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct bio *bio,\n\t\t\t\t\tstruct scrub_page *page)\n{\n\tDECLARE_COMPLETION_ONSTACK(done);\n\tint ret;\n\tint mirror_num;\n\n\tbio->bi_iter.bi_sector = page->logical >> 9;\n\tbio->bi_private = &done;\n\tbio->bi_end_io = scrub_bio_wait_endio;\n\n\tmirror_num = page->sblock->pagev[0]->mirror_num;\n\tret = raid56_parity_recover(fs_info, bio, page->recover->bbio,\n\t\t\t\t    page->recover->map_length,\n\t\t\t\t    mirror_num, 0);\n\tif (ret)\n\t\treturn ret;\n\n\twait_for_completion_io(&done);\n\treturn blk_status_to_errno(bio->bi_status);\n}\n\nstatic void scrub_recheck_block_on_raid56(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t  struct scrub_block *sblock)\n{\n\tstruct scrub_page *first_page = sblock->pagev[0];\n\tstruct bio *bio;\n\tint page_num;\n\n\t/* All pages in sblock belong to the same stripe on the same device. */\n\tASSERT(first_page->dev);\n\tif (!first_page->dev->bdev)\n\t\tgoto out;\n\n\tbio = btrfs_io_bio_alloc(BIO_MAX_PAGES);\n\tbio_set_dev(bio, first_page->dev->bdev);\n\n\tfor (page_num = 0; page_num < sblock->page_count; page_num++) {\n\t\tstruct scrub_page *page = sblock->pagev[page_num];\n\n\t\tWARN_ON(!page->page);\n\t\tbio_add_page(bio, page->page, PAGE_SIZE, 0);\n\t}\n\n\tif (scrub_submit_raid56_bio_wait(fs_info, bio, first_page)) {\n\t\tbio_put(bio);\n\t\tgoto out;\n\t}\n\n\tbio_put(bio);\n\n\tscrub_recheck_block_checksum(sblock);\n\n\treturn;\nout:\n\tfor (page_num = 0; page_num < sblock->page_count; page_num++)\n\t\tsblock->pagev[page_num]->io_error = 1;\n\n\tsblock->no_io_error_seen = 0;\n}\n\n/*\n * this function will check the on disk data for checksum errors, header\n * errors and read I/O errors. If any I/O errors happen, the exact pages\n * which are errored are marked as being bad. The goal is to enable scrub\n * to take those pages that are not errored from all the mirrors so that\n * the pages that are errored in the just handled mirror can be repaired.\n */\nstatic void scrub_recheck_block(struct btrfs_fs_info *fs_info,\n\t\t\t\tstruct scrub_block *sblock,\n\t\t\t\tint retry_failed_mirror)\n{\n\tint page_num;\n\n\tsblock->no_io_error_seen = 1;\n\n\t/* short cut for raid56 */\n\tif (!retry_failed_mirror && scrub_is_page_on_raid56(sblock->pagev[0]))\n\t\treturn scrub_recheck_block_on_raid56(fs_info, sblock);\n\n\tfor (page_num = 0; page_num < sblock->page_count; page_num++) {\n\t\tstruct bio *bio;\n\t\tstruct scrub_page *page = sblock->pagev[page_num];\n\n\t\tif (page->dev->bdev == NULL) {\n\t\t\tpage->io_error = 1;\n\t\t\tsblock->no_io_error_seen = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON(!page->page);\n\t\tbio = btrfs_io_bio_alloc(1);\n\t\tbio_set_dev(bio, page->dev->bdev);\n\n\t\tbio_add_page(bio, page->page, PAGE_SIZE, 0);\n\t\tbio->bi_iter.bi_sector = page->physical >> 9;\n\t\tbio->bi_opf = REQ_OP_READ;\n\n\t\tif (btrfsic_submit_bio_wait(bio)) {\n\t\t\tpage->io_error = 1;\n\t\t\tsblock->no_io_error_seen = 0;\n\t\t}\n\n\t\tbio_put(bio);\n\t}\n\n\tif (sblock->no_io_error_seen)\n\t\tscrub_recheck_block_checksum(sblock);\n}\n\nstatic inline int scrub_check_fsid(u8 fsid[],\n\t\t\t\t   struct scrub_page *spage)\n{\n\tstruct btrfs_fs_devices *fs_devices = spage->dev->fs_devices;\n\tint ret;\n\n\tret = memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\treturn !ret;\n}\n\nstatic void scrub_recheck_block_checksum(struct scrub_block *sblock)\n{\n\tsblock->header_error = 0;\n\tsblock->checksum_error = 0;\n\tsblock->generation_error = 0;\n\n\tif (sblock->pagev[0]->flags & BTRFS_EXTENT_FLAG_DATA)\n\t\tscrub_checksum_data(sblock);\n\telse\n\t\tscrub_checksum_tree_block(sblock);\n}\n\nstatic int scrub_repair_block_from_good_copy(struct scrub_block *sblock_bad,\n\t\t\t\t\t     struct scrub_block *sblock_good)\n{\n\tint page_num;\n\tint ret = 0;\n\n\tfor (page_num = 0; page_num < sblock_bad->page_count; page_num++) {\n\t\tint ret_sub;\n\n\t\tret_sub = scrub_repair_page_from_good_copy(sblock_bad,\n\t\t\t\t\t\t\t   sblock_good,\n\t\t\t\t\t\t\t   page_num, 1);\n\t\tif (ret_sub)\n\t\t\tret = ret_sub;\n\t}\n\n\treturn ret;\n}\n\nstatic int scrub_repair_page_from_good_copy(struct scrub_block *sblock_bad,\n\t\t\t\t\t    struct scrub_block *sblock_good,\n\t\t\t\t\t    int page_num, int force_write)\n{\n\tstruct scrub_page *page_bad = sblock_bad->pagev[page_num];\n\tstruct scrub_page *page_good = sblock_good->pagev[page_num];\n\tstruct btrfs_fs_info *fs_info = sblock_bad->sctx->fs_info;\n\n\tBUG_ON(page_bad->page == NULL);\n\tBUG_ON(page_good->page == NULL);\n\tif (force_write || sblock_bad->header_error ||\n\t    sblock_bad->checksum_error || page_bad->io_error) {\n\t\tstruct bio *bio;\n\t\tint ret;\n\n\t\tif (!page_bad->dev->bdev) {\n\t\t\tbtrfs_warn_rl(fs_info,\n\t\t\t\t\"scrub_repair_page_from_good_copy(bdev == NULL) is unexpected\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tbio = btrfs_io_bio_alloc(1);\n\t\tbio_set_dev(bio, page_bad->dev->bdev);\n\t\tbio->bi_iter.bi_sector = page_bad->physical >> 9;\n\t\tbio->bi_opf = REQ_OP_WRITE;\n\n\t\tret = bio_add_page(bio, page_good->page, PAGE_SIZE, 0);\n\t\tif (PAGE_SIZE != ret) {\n\t\t\tbio_put(bio);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (btrfsic_submit_bio_wait(bio)) {\n\t\t\tbtrfs_dev_stat_inc_and_print(page_bad->dev,\n\t\t\t\tBTRFS_DEV_STAT_WRITE_ERRS);\n\t\t\tatomic64_inc(&fs_info->dev_replace.num_write_errors);\n\t\t\tbio_put(bio);\n\t\t\treturn -EIO;\n\t\t}\n\t\tbio_put(bio);\n\t}\n\n\treturn 0;\n}\n\nstatic void scrub_write_block_to_dev_replace(struct scrub_block *sblock)\n{\n\tstruct btrfs_fs_info *fs_info = sblock->sctx->fs_info;\n\tint page_num;\n\n\t/*\n\t * This block is used for the check of the parity on the source device,\n\t * so the data needn't be written into the destination device.\n\t */\n\tif (sblock->sparity)\n\t\treturn;\n\n\tfor (page_num = 0; page_num < sblock->page_count; page_num++) {\n\t\tint ret;\n\n\t\tret = scrub_write_page_to_dev_replace(sblock, page_num);\n\t\tif (ret)\n\t\t\tatomic64_inc(&fs_info->dev_replace.num_write_errors);\n\t}\n}\n\nstatic int scrub_write_page_to_dev_replace(struct scrub_block *sblock,\n\t\t\t\t\t   int page_num)\n{\n\tstruct scrub_page *spage = sblock->pagev[page_num];\n\n\tBUG_ON(spage->page == NULL);\n\tif (spage->io_error) {\n\t\tvoid *mapped_buffer = kmap_atomic(spage->page);\n\n\t\tclear_page(mapped_buffer);\n\t\tflush_dcache_page(spage->page);\n\t\tkunmap_atomic(mapped_buffer);\n\t}\n\treturn scrub_add_page_to_wr_bio(sblock->sctx, spage);\n}\n\nstatic int scrub_add_page_to_wr_bio(struct scrub_ctx *sctx,\n\t\t\t\t    struct scrub_page *spage)\n{\n\tstruct scrub_bio *sbio;\n\tint ret;\n\n\tmutex_lock(&sctx->wr_lock);\nagain:\n\tif (!sctx->wr_curr_bio) {\n\t\tsctx->wr_curr_bio = kzalloc(sizeof(*sctx->wr_curr_bio),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!sctx->wr_curr_bio) {\n\t\t\tmutex_unlock(&sctx->wr_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tsctx->wr_curr_bio->sctx = sctx;\n\t\tsctx->wr_curr_bio->page_count = 0;\n\t}\n\tsbio = sctx->wr_curr_bio;\n\tif (sbio->page_count == 0) {\n\t\tstruct bio *bio;\n\n\t\tsbio->physical = spage->physical_for_dev_replace;\n\t\tsbio->logical = spage->logical;\n\t\tsbio->dev = sctx->wr_tgtdev;\n\t\tbio = sbio->bio;\n\t\tif (!bio) {\n\t\t\tbio = btrfs_io_bio_alloc(sctx->pages_per_wr_bio);\n\t\t\tsbio->bio = bio;\n\t\t}\n\n\t\tbio->bi_private = sbio;\n\t\tbio->bi_end_io = scrub_wr_bio_end_io;\n\t\tbio_set_dev(bio, sbio->dev->bdev);\n\t\tbio->bi_iter.bi_sector = sbio->physical >> 9;\n\t\tbio->bi_opf = REQ_OP_WRITE;\n\t\tsbio->status = 0;\n\t} else if (sbio->physical + sbio->page_count * PAGE_SIZE !=\n\t\t   spage->physical_for_dev_replace ||\n\t\t   sbio->logical + sbio->page_count * PAGE_SIZE !=\n\t\t   spage->logical) {\n\t\tscrub_wr_submit(sctx);\n\t\tgoto again;\n\t}\n\n\tret = bio_add_page(sbio->bio, spage->page, PAGE_SIZE, 0);\n\tif (ret != PAGE_SIZE) {\n\t\tif (sbio->page_count < 1) {\n\t\t\tbio_put(sbio->bio);\n\t\t\tsbio->bio = NULL;\n\t\t\tmutex_unlock(&sctx->wr_lock);\n\t\t\treturn -EIO;\n\t\t}\n\t\tscrub_wr_submit(sctx);\n\t\tgoto again;\n\t}\n\n\tsbio->pagev[sbio->page_count] = spage;\n\tscrub_page_get(spage);\n\tsbio->page_count++;\n\tif (sbio->page_count == sctx->pages_per_wr_bio)\n\t\tscrub_wr_submit(sctx);\n\tmutex_unlock(&sctx->wr_lock);\n\n\treturn 0;\n}\n\nstatic void scrub_wr_submit(struct scrub_ctx *sctx)\n{\n\tstruct scrub_bio *sbio;\n\n\tif (!sctx->wr_curr_bio)\n\t\treturn;\n\n\tsbio = sctx->wr_curr_bio;\n\tsctx->wr_curr_bio = NULL;\n\tWARN_ON(!sbio->bio->bi_disk);\n\tscrub_pending_bio_inc(sctx);\n\t/* process all writes in a single worker thread. Then the block layer\n\t * orders the requests before sending them to the driver which\n\t * doubled the write performance on spinning disks when measured\n\t * with Linux 3.5 */\n\tbtrfsic_submit_bio(sbio->bio);\n}\n\nstatic void scrub_wr_bio_end_io(struct bio *bio)\n{\n\tstruct scrub_bio *sbio = bio->bi_private;\n\tstruct btrfs_fs_info *fs_info = sbio->dev->fs_info;\n\n\tsbio->status = bio->bi_status;\n\tsbio->bio = bio;\n\n\tbtrfs_init_work(&sbio->work, btrfs_scrubwrc_helper,\n\t\t\t scrub_wr_bio_end_io_worker, NULL, NULL);\n\tbtrfs_queue_work(fs_info->scrub_wr_completion_workers, &sbio->work);\n}\n\nstatic void scrub_wr_bio_end_io_worker(struct btrfs_work *work)\n{\n\tstruct scrub_bio *sbio = container_of(work, struct scrub_bio, work);\n\tstruct scrub_ctx *sctx = sbio->sctx;\n\tint i;\n\n\tWARN_ON(sbio->page_count > SCRUB_PAGES_PER_WR_BIO);\n\tif (sbio->status) {\n\t\tstruct btrfs_dev_replace *dev_replace =\n\t\t\t&sbio->sctx->fs_info->dev_replace;\n\n\t\tfor (i = 0; i < sbio->page_count; i++) {\n\t\t\tstruct scrub_page *spage = sbio->pagev[i];\n\n\t\t\tspage->io_error = 1;\n\t\t\tatomic64_inc(&dev_replace->num_write_errors);\n\t\t}\n\t}\n\n\tfor (i = 0; i < sbio->page_count; i++)\n\t\tscrub_page_put(sbio->pagev[i]);\n\n\tbio_put(sbio->bio);\n\tkfree(sbio);\n\tscrub_pending_bio_dec(sctx);\n}\n\nstatic int scrub_checksum(struct scrub_block *sblock)\n{\n\tu64 flags;\n\tint ret;\n\n\t/*\n\t * No need to initialize these stats currently,\n\t * because this function only use return value\n\t * instead of these stats value.\n\t *\n\t * Todo:\n\t * always use stats\n\t */\n\tsblock->header_error = 0;\n\tsblock->generation_error = 0;\n\tsblock->checksum_error = 0;\n\n\tWARN_ON(sblock->page_count < 1);\n\tflags = sblock->pagev[0]->flags;\n\tret = 0;\n\tif (flags & BTRFS_EXTENT_FLAG_DATA)\n\t\tret = scrub_checksum_data(sblock);\n\telse if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK)\n\t\tret = scrub_checksum_tree_block(sblock);\n\telse if (flags & BTRFS_EXTENT_FLAG_SUPER)\n\t\t(void)scrub_checksum_super(sblock);\n\telse\n\t\tWARN_ON(1);\n\tif (ret)\n\t\tscrub_handle_errored_block(sblock);\n\n\treturn ret;\n}\n\nstatic int scrub_checksum_data(struct scrub_block *sblock)\n{\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tu8 csum[BTRFS_CSUM_SIZE];\n\tu8 *on_disk_csum;\n\tstruct page *page;\n\tvoid *buffer;\n\tu32 crc = ~(u32)0;\n\tu64 len;\n\tint index;\n\n\tBUG_ON(sblock->page_count < 1);\n\tif (!sblock->pagev[0]->have_csum)\n\t\treturn 0;\n\n\ton_disk_csum = sblock->pagev[0]->csum;\n\tpage = sblock->pagev[0]->page;\n\tbuffer = kmap_atomic(page);\n\n\tlen = sctx->fs_info->sectorsize;\n\tindex = 0;\n\tfor (;;) {\n\t\tu64 l = min_t(u64, len, PAGE_SIZE);\n\n\t\tcrc = btrfs_csum_data(buffer, crc, l);\n\t\tkunmap_atomic(buffer);\n\t\tlen -= l;\n\t\tif (len == 0)\n\t\t\tbreak;\n\t\tindex++;\n\t\tBUG_ON(index >= sblock->page_count);\n\t\tBUG_ON(!sblock->pagev[index]->page);\n\t\tpage = sblock->pagev[index]->page;\n\t\tbuffer = kmap_atomic(page);\n\t}\n\n\tbtrfs_csum_final(crc, csum);\n\tif (memcmp(csum, on_disk_csum, sctx->csum_size))\n\t\tsblock->checksum_error = 1;\n\n\treturn sblock->checksum_error;\n}\n\nstatic int scrub_checksum_tree_block(struct scrub_block *sblock)\n{\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tstruct btrfs_header *h;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tu8 calculated_csum[BTRFS_CSUM_SIZE];\n\tu8 on_disk_csum[BTRFS_CSUM_SIZE];\n\tstruct page *page;\n\tvoid *mapped_buffer;\n\tu64 mapped_size;\n\tvoid *p;\n\tu32 crc = ~(u32)0;\n\tu64 len;\n\tint index;\n\n\tBUG_ON(sblock->page_count < 1);\n\tpage = sblock->pagev[0]->page;\n\tmapped_buffer = kmap_atomic(page);\n\th = (struct btrfs_header *)mapped_buffer;\n\tmemcpy(on_disk_csum, h->csum, sctx->csum_size);\n\n\t/*\n\t * we don't use the getter functions here, as we\n\t * a) don't have an extent buffer and\n\t * b) the page is already kmapped\n\t */\n\tif (sblock->pagev[0]->logical != btrfs_stack_header_bytenr(h))\n\t\tsblock->header_error = 1;\n\n\tif (sblock->pagev[0]->generation != btrfs_stack_header_generation(h)) {\n\t\tsblock->header_error = 1;\n\t\tsblock->generation_error = 1;\n\t}\n\n\tif (!scrub_check_fsid(h->fsid, sblock->pagev[0]))\n\t\tsblock->header_error = 1;\n\n\tif (memcmp(h->chunk_tree_uuid, fs_info->chunk_tree_uuid,\n\t\t   BTRFS_UUID_SIZE))\n\t\tsblock->header_error = 1;\n\n\tlen = sctx->fs_info->nodesize - BTRFS_CSUM_SIZE;\n\tmapped_size = PAGE_SIZE - BTRFS_CSUM_SIZE;\n\tp = ((u8 *)mapped_buffer) + BTRFS_CSUM_SIZE;\n\tindex = 0;\n\tfor (;;) {\n\t\tu64 l = min_t(u64, len, mapped_size);\n\n\t\tcrc = btrfs_csum_data(p, crc, l);\n\t\tkunmap_atomic(mapped_buffer);\n\t\tlen -= l;\n\t\tif (len == 0)\n\t\t\tbreak;\n\t\tindex++;\n\t\tBUG_ON(index >= sblock->page_count);\n\t\tBUG_ON(!sblock->pagev[index]->page);\n\t\tpage = sblock->pagev[index]->page;\n\t\tmapped_buffer = kmap_atomic(page);\n\t\tmapped_size = PAGE_SIZE;\n\t\tp = mapped_buffer;\n\t}\n\n\tbtrfs_csum_final(crc, calculated_csum);\n\tif (memcmp(calculated_csum, on_disk_csum, sctx->csum_size))\n\t\tsblock->checksum_error = 1;\n\n\treturn sblock->header_error || sblock->checksum_error;\n}\n\nstatic int scrub_checksum_super(struct scrub_block *sblock)\n{\n\tstruct btrfs_super_block *s;\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tu8 calculated_csum[BTRFS_CSUM_SIZE];\n\tu8 on_disk_csum[BTRFS_CSUM_SIZE];\n\tstruct page *page;\n\tvoid *mapped_buffer;\n\tu64 mapped_size;\n\tvoid *p;\n\tu32 crc = ~(u32)0;\n\tint fail_gen = 0;\n\tint fail_cor = 0;\n\tu64 len;\n\tint index;\n\n\tBUG_ON(sblock->page_count < 1);\n\tpage = sblock->pagev[0]->page;\n\tmapped_buffer = kmap_atomic(page);\n\ts = (struct btrfs_super_block *)mapped_buffer;\n\tmemcpy(on_disk_csum, s->csum, sctx->csum_size);\n\n\tif (sblock->pagev[0]->logical != btrfs_super_bytenr(s))\n\t\t++fail_cor;\n\n\tif (sblock->pagev[0]->generation != btrfs_super_generation(s))\n\t\t++fail_gen;\n\n\tif (!scrub_check_fsid(s->fsid, sblock->pagev[0]))\n\t\t++fail_cor;\n\n\tlen = BTRFS_SUPER_INFO_SIZE - BTRFS_CSUM_SIZE;\n\tmapped_size = PAGE_SIZE - BTRFS_CSUM_SIZE;\n\tp = ((u8 *)mapped_buffer) + BTRFS_CSUM_SIZE;\n\tindex = 0;\n\tfor (;;) {\n\t\tu64 l = min_t(u64, len, mapped_size);\n\n\t\tcrc = btrfs_csum_data(p, crc, l);\n\t\tkunmap_atomic(mapped_buffer);\n\t\tlen -= l;\n\t\tif (len == 0)\n\t\t\tbreak;\n\t\tindex++;\n\t\tBUG_ON(index >= sblock->page_count);\n\t\tBUG_ON(!sblock->pagev[index]->page);\n\t\tpage = sblock->pagev[index]->page;\n\t\tmapped_buffer = kmap_atomic(page);\n\t\tmapped_size = PAGE_SIZE;\n\t\tp = mapped_buffer;\n\t}\n\n\tbtrfs_csum_final(crc, calculated_csum);\n\tif (memcmp(calculated_csum, on_disk_csum, sctx->csum_size))\n\t\t++fail_cor;\n\n\tif (fail_cor + fail_gen) {\n\t\t/*\n\t\t * if we find an error in a super block, we just report it.\n\t\t * They will get written with the next transaction commit\n\t\t * anyway\n\t\t */\n\t\tspin_lock(&sctx->stat_lock);\n\t\t++sctx->stat.super_errors;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (fail_cor)\n\t\t\tbtrfs_dev_stat_inc_and_print(sblock->pagev[0]->dev,\n\t\t\t\tBTRFS_DEV_STAT_CORRUPTION_ERRS);\n\t\telse\n\t\t\tbtrfs_dev_stat_inc_and_print(sblock->pagev[0]->dev,\n\t\t\t\tBTRFS_DEV_STAT_GENERATION_ERRS);\n\t}\n\n\treturn fail_cor + fail_gen;\n}\n\nstatic void scrub_block_get(struct scrub_block *sblock)\n{\n\trefcount_inc(&sblock->refs);\n}\n\nstatic void scrub_block_put(struct scrub_block *sblock)\n{\n\tif (refcount_dec_and_test(&sblock->refs)) {\n\t\tint i;\n\n\t\tif (sblock->sparity)\n\t\t\tscrub_parity_put(sblock->sparity);\n\n\t\tfor (i = 0; i < sblock->page_count; i++)\n\t\t\tscrub_page_put(sblock->pagev[i]);\n\t\tkfree(sblock);\n\t}\n}\n\nstatic void scrub_page_get(struct scrub_page *spage)\n{\n\tatomic_inc(&spage->refs);\n}\n\nstatic void scrub_page_put(struct scrub_page *spage)\n{\n\tif (atomic_dec_and_test(&spage->refs)) {\n\t\tif (spage->page)\n\t\t\t__free_page(spage->page);\n\t\tkfree(spage);\n\t}\n}\n\nstatic void scrub_submit(struct scrub_ctx *sctx)\n{\n\tstruct scrub_bio *sbio;\n\n\tif (sctx->curr == -1)\n\t\treturn;\n\n\tsbio = sctx->bios[sctx->curr];\n\tsctx->curr = -1;\n\tscrub_pending_bio_inc(sctx);\n\tbtrfsic_submit_bio(sbio->bio);\n}\n\nstatic int scrub_add_page_to_rd_bio(struct scrub_ctx *sctx,\n\t\t\t\t    struct scrub_page *spage)\n{\n\tstruct scrub_block *sblock = spage->sblock;\n\tstruct scrub_bio *sbio;\n\tint ret;\n\nagain:\n\t/*\n\t * grab a fresh bio or wait for one to become available\n\t */\n\twhile (sctx->curr == -1) {\n\t\tspin_lock(&sctx->list_lock);\n\t\tsctx->curr = sctx->first_free;\n\t\tif (sctx->curr != -1) {\n\t\t\tsctx->first_free = sctx->bios[sctx->curr]->next_free;\n\t\t\tsctx->bios[sctx->curr]->next_free = -1;\n\t\t\tsctx->bios[sctx->curr]->page_count = 0;\n\t\t\tspin_unlock(&sctx->list_lock);\n\t\t} else {\n\t\t\tspin_unlock(&sctx->list_lock);\n\t\t\twait_event(sctx->list_wait, sctx->first_free != -1);\n\t\t}\n\t}\n\tsbio = sctx->bios[sctx->curr];\n\tif (sbio->page_count == 0) {\n\t\tstruct bio *bio;\n\n\t\tsbio->physical = spage->physical;\n\t\tsbio->logical = spage->logical;\n\t\tsbio->dev = spage->dev;\n\t\tbio = sbio->bio;\n\t\tif (!bio) {\n\t\t\tbio = btrfs_io_bio_alloc(sctx->pages_per_rd_bio);\n\t\t\tsbio->bio = bio;\n\t\t}\n\n\t\tbio->bi_private = sbio;\n\t\tbio->bi_end_io = scrub_bio_end_io;\n\t\tbio_set_dev(bio, sbio->dev->bdev);\n\t\tbio->bi_iter.bi_sector = sbio->physical >> 9;\n\t\tbio->bi_opf = REQ_OP_READ;\n\t\tsbio->status = 0;\n\t} else if (sbio->physical + sbio->page_count * PAGE_SIZE !=\n\t\t   spage->physical ||\n\t\t   sbio->logical + sbio->page_count * PAGE_SIZE !=\n\t\t   spage->logical ||\n\t\t   sbio->dev != spage->dev) {\n\t\tscrub_submit(sctx);\n\t\tgoto again;\n\t}\n\n\tsbio->pagev[sbio->page_count] = spage;\n\tret = bio_add_page(sbio->bio, spage->page, PAGE_SIZE, 0);\n\tif (ret != PAGE_SIZE) {\n\t\tif (sbio->page_count < 1) {\n\t\t\tbio_put(sbio->bio);\n\t\t\tsbio->bio = NULL;\n\t\t\treturn -EIO;\n\t\t}\n\t\tscrub_submit(sctx);\n\t\tgoto again;\n\t}\n\n\tscrub_block_get(sblock); /* one for the page added to the bio */\n\tatomic_inc(&sblock->outstanding_pages);\n\tsbio->page_count++;\n\tif (sbio->page_count == sctx->pages_per_rd_bio)\n\t\tscrub_submit(sctx);\n\n\treturn 0;\n}\n\nstatic void scrub_missing_raid56_end_io(struct bio *bio)\n{\n\tstruct scrub_block *sblock = bio->bi_private;\n\tstruct btrfs_fs_info *fs_info = sblock->sctx->fs_info;\n\n\tif (bio->bi_status)\n\t\tsblock->no_io_error_seen = 0;\n\n\tbio_put(bio);\n\n\tbtrfs_queue_work(fs_info->scrub_workers, &sblock->work);\n}\n\nstatic void scrub_missing_raid56_worker(struct btrfs_work *work)\n{\n\tstruct scrub_block *sblock = container_of(work, struct scrub_block, work);\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tu64 logical;\n\tstruct btrfs_device *dev;\n\n\tlogical = sblock->pagev[0]->logical;\n\tdev = sblock->pagev[0]->dev;\n\n\tif (sblock->no_io_error_seen)\n\t\tscrub_recheck_block_checksum(sblock);\n\n\tif (!sblock->no_io_error_seen) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.read_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\"IO error rebuilding logical %llu for dev %s\",\n\t\t\tlogical, rcu_str_deref(dev->name));\n\t} else if (sblock->header_error || sblock->checksum_error) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.uncorrectable_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\"failed to rebuild valid logical %llu for dev %s\",\n\t\t\tlogical, rcu_str_deref(dev->name));\n\t} else {\n\t\tscrub_write_block_to_dev_replace(sblock);\n\t}\n\n\tscrub_block_put(sblock);\n\n\tif (sctx->is_dev_replace && sctx->flush_all_writes) {\n\t\tmutex_lock(&sctx->wr_lock);\n\t\tscrub_wr_submit(sctx);\n\t\tmutex_unlock(&sctx->wr_lock);\n\t}\n\n\tscrub_pending_bio_dec(sctx);\n}\n\nstatic void scrub_missing_raid56_pages(struct scrub_block *sblock)\n{\n\tstruct scrub_ctx *sctx = sblock->sctx;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tu64 length = sblock->page_count * PAGE_SIZE;\n\tu64 logical = sblock->pagev[0]->logical;\n\tstruct btrfs_bio *bbio = NULL;\n\tstruct bio *bio;\n\tstruct btrfs_raid_bio *rbio;\n\tint ret;\n\tint i;\n\n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\tret = btrfs_map_sblock(fs_info, BTRFS_MAP_GET_READ_MIRRORS, logical,\n\t\t\t&length, &bbio);\n\tif (ret || !bbio || !bbio->raid_map)\n\t\tgoto bbio_out;\n\n\tif (WARN_ON(!sctx->is_dev_replace ||\n\t\t    !(bbio->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK))) {\n\t\t/*\n\t\t * We shouldn't be scrubbing a missing device. Even for dev\n\t\t * replace, we should only get here for RAID 5/6. We either\n\t\t * managed to mount something with no mirrors remaining or\n\t\t * there's a bug in scrub_remap_extent()/btrfs_map_block().\n\t\t */\n\t\tgoto bbio_out;\n\t}\n\n\tbio = btrfs_io_bio_alloc(0);\n\tbio->bi_iter.bi_sector = logical >> 9;\n\tbio->bi_private = sblock;\n\tbio->bi_end_io = scrub_missing_raid56_end_io;\n\n\trbio = raid56_alloc_missing_rbio(fs_info, bio, bbio, length);\n\tif (!rbio)\n\t\tgoto rbio_out;\n\n\tfor (i = 0; i < sblock->page_count; i++) {\n\t\tstruct scrub_page *spage = sblock->pagev[i];\n\n\t\traid56_add_scrub_pages(rbio, spage->page, spage->logical);\n\t}\n\n\tbtrfs_init_work(&sblock->work, btrfs_scrub_helper,\n\t\t\tscrub_missing_raid56_worker, NULL, NULL);\n\tscrub_block_get(sblock);\n\tscrub_pending_bio_inc(sctx);\n\traid56_submit_missing_rbio(rbio);\n\treturn;\n\nrbio_out:\n\tbio_put(bio);\nbbio_out:\n\tbtrfs_bio_counter_dec(fs_info);\n\tbtrfs_put_bbio(bbio);\n\tspin_lock(&sctx->stat_lock);\n\tsctx->stat.malloc_errors++;\n\tspin_unlock(&sctx->stat_lock);\n}\n\nstatic int scrub_pages(struct scrub_ctx *sctx, u64 logical, u64 len,\n\t\t       u64 physical, struct btrfs_device *dev, u64 flags,\n\t\t       u64 gen, int mirror_num, u8 *csum, int force,\n\t\t       u64 physical_for_dev_replace)\n{\n\tstruct scrub_block *sblock;\n\tint index;\n\n\tsblock = kzalloc(sizeof(*sblock), GFP_KERNEL);\n\tif (!sblock) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.malloc_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* one ref inside this function, plus one for each page added to\n\t * a bio later on */\n\trefcount_set(&sblock->refs, 1);\n\tsblock->sctx = sctx;\n\tsblock->no_io_error_seen = 1;\n\n\tfor (index = 0; len > 0; index++) {\n\t\tstruct scrub_page *spage;\n\t\tu64 l = min_t(u64, len, PAGE_SIZE);\n\n\t\tspage = kzalloc(sizeof(*spage), GFP_KERNEL);\n\t\tif (!spage) {\nleave_nomem:\n\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\tsctx->stat.malloc_errors++;\n\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\tscrub_block_put(sblock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tBUG_ON(index >= SCRUB_MAX_PAGES_PER_BLOCK);\n\t\tscrub_page_get(spage);\n\t\tsblock->pagev[index] = spage;\n\t\tspage->sblock = sblock;\n\t\tspage->dev = dev;\n\t\tspage->flags = flags;\n\t\tspage->generation = gen;\n\t\tspage->logical = logical;\n\t\tspage->physical = physical;\n\t\tspage->physical_for_dev_replace = physical_for_dev_replace;\n\t\tspage->mirror_num = mirror_num;\n\t\tif (csum) {\n\t\t\tspage->have_csum = 1;\n\t\t\tmemcpy(spage->csum, csum, sctx->csum_size);\n\t\t} else {\n\t\t\tspage->have_csum = 0;\n\t\t}\n\t\tsblock->page_count++;\n\t\tspage->page = alloc_page(GFP_KERNEL);\n\t\tif (!spage->page)\n\t\t\tgoto leave_nomem;\n\t\tlen -= l;\n\t\tlogical += l;\n\t\tphysical += l;\n\t\tphysical_for_dev_replace += l;\n\t}\n\n\tWARN_ON(sblock->page_count == 0);\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state)) {\n\t\t/*\n\t\t * This case should only be hit for RAID 5/6 device replace. See\n\t\t * the comment in scrub_missing_raid56_pages() for details.\n\t\t */\n\t\tscrub_missing_raid56_pages(sblock);\n\t} else {\n\t\tfor (index = 0; index < sblock->page_count; index++) {\n\t\t\tstruct scrub_page *spage = sblock->pagev[index];\n\t\t\tint ret;\n\n\t\t\tret = scrub_add_page_to_rd_bio(sctx, spage);\n\t\t\tif (ret) {\n\t\t\t\tscrub_block_put(sblock);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\tif (force)\n\t\t\tscrub_submit(sctx);\n\t}\n\n\t/* last one frees, either here or in bio completion for last page */\n\tscrub_block_put(sblock);\n\treturn 0;\n}\n\nstatic void scrub_bio_end_io(struct bio *bio)\n{\n\tstruct scrub_bio *sbio = bio->bi_private;\n\tstruct btrfs_fs_info *fs_info = sbio->dev->fs_info;\n\n\tsbio->status = bio->bi_status;\n\tsbio->bio = bio;\n\n\tbtrfs_queue_work(fs_info->scrub_workers, &sbio->work);\n}\n\nstatic void scrub_bio_end_io_worker(struct btrfs_work *work)\n{\n\tstruct scrub_bio *sbio = container_of(work, struct scrub_bio, work);\n\tstruct scrub_ctx *sctx = sbio->sctx;\n\tint i;\n\n\tBUG_ON(sbio->page_count > SCRUB_PAGES_PER_RD_BIO);\n\tif (sbio->status) {\n\t\tfor (i = 0; i < sbio->page_count; i++) {\n\t\t\tstruct scrub_page *spage = sbio->pagev[i];\n\n\t\t\tspage->io_error = 1;\n\t\t\tspage->sblock->no_io_error_seen = 0;\n\t\t}\n\t}\n\n\t/* now complete the scrub_block items that have all pages completed */\n\tfor (i = 0; i < sbio->page_count; i++) {\n\t\tstruct scrub_page *spage = sbio->pagev[i];\n\t\tstruct scrub_block *sblock = spage->sblock;\n\n\t\tif (atomic_dec_and_test(&sblock->outstanding_pages))\n\t\t\tscrub_block_complete(sblock);\n\t\tscrub_block_put(sblock);\n\t}\n\n\tbio_put(sbio->bio);\n\tsbio->bio = NULL;\n\tspin_lock(&sctx->list_lock);\n\tsbio->next_free = sctx->first_free;\n\tsctx->first_free = sbio->index;\n\tspin_unlock(&sctx->list_lock);\n\n\tif (sctx->is_dev_replace && sctx->flush_all_writes) {\n\t\tmutex_lock(&sctx->wr_lock);\n\t\tscrub_wr_submit(sctx);\n\t\tmutex_unlock(&sctx->wr_lock);\n\t}\n\n\tscrub_pending_bio_dec(sctx);\n}\n\nstatic inline void __scrub_mark_bitmap(struct scrub_parity *sparity,\n\t\t\t\t       unsigned long *bitmap,\n\t\t\t\t       u64 start, u64 len)\n{\n\tu64 offset;\n\tu64 nsectors64;\n\tu32 nsectors;\n\tint sectorsize = sparity->sctx->fs_info->sectorsize;\n\n\tif (len >= sparity->stripe_len) {\n\t\tbitmap_set(bitmap, 0, sparity->nsectors);\n\t\treturn;\n\t}\n\n\tstart -= sparity->logic_start;\n\tstart = div64_u64_rem(start, sparity->stripe_len, &offset);\n\toffset = div_u64(offset, sectorsize);\n\tnsectors64 = div_u64(len, sectorsize);\n\n\tASSERT(nsectors64 < UINT_MAX);\n\tnsectors = (u32)nsectors64;\n\n\tif (offset + nsectors <= sparity->nsectors) {\n\t\tbitmap_set(bitmap, offset, nsectors);\n\t\treturn;\n\t}\n\n\tbitmap_set(bitmap, offset, sparity->nsectors - offset);\n\tbitmap_set(bitmap, 0, nsectors - (sparity->nsectors - offset));\n}\n\nstatic inline void scrub_parity_mark_sectors_error(struct scrub_parity *sparity,\n\t\t\t\t\t\t   u64 start, u64 len)\n{\n\t__scrub_mark_bitmap(sparity, sparity->ebitmap, start, len);\n}\n\nstatic inline void scrub_parity_mark_sectors_data(struct scrub_parity *sparity,\n\t\t\t\t\t\t  u64 start, u64 len)\n{\n\t__scrub_mark_bitmap(sparity, sparity->dbitmap, start, len);\n}\n\nstatic void scrub_block_complete(struct scrub_block *sblock)\n{\n\tint corrupted = 0;\n\n\tif (!sblock->no_io_error_seen) {\n\t\tcorrupted = 1;\n\t\tscrub_handle_errored_block(sblock);\n\t} else {\n\t\t/*\n\t\t * if has checksum error, write via repair mechanism in\n\t\t * dev replace case, otherwise write here in dev replace\n\t\t * case.\n\t\t */\n\t\tcorrupted = scrub_checksum(sblock);\n\t\tif (!corrupted && sblock->sctx->is_dev_replace)\n\t\t\tscrub_write_block_to_dev_replace(sblock);\n\t}\n\n\tif (sblock->sparity && corrupted && !sblock->data_corrected) {\n\t\tu64 start = sblock->pagev[0]->logical;\n\t\tu64 end = sblock->pagev[sblock->page_count - 1]->logical +\n\t\t\t  PAGE_SIZE;\n\n\t\tscrub_parity_mark_sectors_error(sblock->sparity,\n\t\t\t\t\t\tstart, end - start);\n\t}\n}\n\nstatic int scrub_find_csum(struct scrub_ctx *sctx, u64 logical, u8 *csum)\n{\n\tstruct btrfs_ordered_sum *sum = NULL;\n\tunsigned long index;\n\tunsigned long num_sectors;\n\n\twhile (!list_empty(&sctx->csum_list)) {\n\t\tsum = list_first_entry(&sctx->csum_list,\n\t\t\t\t       struct btrfs_ordered_sum, list);\n\t\tif (sum->bytenr > logical)\n\t\t\treturn 0;\n\t\tif (sum->bytenr + sum->len > logical)\n\t\t\tbreak;\n\n\t\t++sctx->stat.csum_discards;\n\t\tlist_del(&sum->list);\n\t\tkfree(sum);\n\t\tsum = NULL;\n\t}\n\tif (!sum)\n\t\treturn 0;\n\n\tindex = div_u64(logical - sum->bytenr, sctx->fs_info->sectorsize);\n\tASSERT(index < UINT_MAX);\n\n\tnum_sectors = sum->len / sctx->fs_info->sectorsize;\n\tmemcpy(csum, sum->sums + index, sctx->csum_size);\n\tif (index == num_sectors - 1) {\n\t\tlist_del(&sum->list);\n\t\tkfree(sum);\n\t}\n\treturn 1;\n}\n\n/* scrub extent tries to collect up to 64 kB for each bio */\nstatic int scrub_extent(struct scrub_ctx *sctx, struct map_lookup *map,\n\t\t\tu64 logical, u64 len,\n\t\t\tu64 physical, struct btrfs_device *dev, u64 flags,\n\t\t\tu64 gen, int mirror_num, u64 physical_for_dev_replace)\n{\n\tint ret;\n\tu8 csum[BTRFS_CSUM_SIZE];\n\tu32 blocksize;\n\n\tif (flags & BTRFS_EXTENT_FLAG_DATA) {\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tblocksize = map->stripe_len;\n\t\telse\n\t\t\tblocksize = sctx->fs_info->sectorsize;\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.data_extents_scrubbed++;\n\t\tsctx->stat.data_bytes_scrubbed += len;\n\t\tspin_unlock(&sctx->stat_lock);\n\t} else if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tblocksize = map->stripe_len;\n\t\telse\n\t\t\tblocksize = sctx->fs_info->nodesize;\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.tree_extents_scrubbed++;\n\t\tsctx->stat.tree_bytes_scrubbed += len;\n\t\tspin_unlock(&sctx->stat_lock);\n\t} else {\n\t\tblocksize = sctx->fs_info->sectorsize;\n\t\tWARN_ON(1);\n\t}\n\n\twhile (len) {\n\t\tu64 l = min_t(u64, len, blocksize);\n\t\tint have_csum = 0;\n\n\t\tif (flags & BTRFS_EXTENT_FLAG_DATA) {\n\t\t\t/* push csums to sbio */\n\t\t\thave_csum = scrub_find_csum(sctx, logical, csum);\n\t\t\tif (have_csum == 0)\n\t\t\t\t++sctx->stat.no_csum;\n\t\t}\n\t\tret = scrub_pages(sctx, logical, l, physical, dev, flags, gen,\n\t\t\t\t  mirror_num, have_csum ? csum : NULL, 0,\n\t\t\t\t  physical_for_dev_replace);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tlen -= l;\n\t\tlogical += l;\n\t\tphysical += l;\n\t\tphysical_for_dev_replace += l;\n\t}\n\treturn 0;\n}\n\nstatic int scrub_pages_for_parity(struct scrub_parity *sparity,\n\t\t\t\t  u64 logical, u64 len,\n\t\t\t\t  u64 physical, struct btrfs_device *dev,\n\t\t\t\t  u64 flags, u64 gen, int mirror_num, u8 *csum)\n{\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\tstruct scrub_block *sblock;\n\tint index;\n\n\tsblock = kzalloc(sizeof(*sblock), GFP_KERNEL);\n\tif (!sblock) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.malloc_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* one ref inside this function, plus one for each page added to\n\t * a bio later on */\n\trefcount_set(&sblock->refs, 1);\n\tsblock->sctx = sctx;\n\tsblock->no_io_error_seen = 1;\n\tsblock->sparity = sparity;\n\tscrub_parity_get(sparity);\n\n\tfor (index = 0; len > 0; index++) {\n\t\tstruct scrub_page *spage;\n\t\tu64 l = min_t(u64, len, PAGE_SIZE);\n\n\t\tspage = kzalloc(sizeof(*spage), GFP_KERNEL);\n\t\tif (!spage) {\nleave_nomem:\n\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\tsctx->stat.malloc_errors++;\n\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\tscrub_block_put(sblock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tBUG_ON(index >= SCRUB_MAX_PAGES_PER_BLOCK);\n\t\t/* For scrub block */\n\t\tscrub_page_get(spage);\n\t\tsblock->pagev[index] = spage;\n\t\t/* For scrub parity */\n\t\tscrub_page_get(spage);\n\t\tlist_add_tail(&spage->list, &sparity->spages);\n\t\tspage->sblock = sblock;\n\t\tspage->dev = dev;\n\t\tspage->flags = flags;\n\t\tspage->generation = gen;\n\t\tspage->logical = logical;\n\t\tspage->physical = physical;\n\t\tspage->mirror_num = mirror_num;\n\t\tif (csum) {\n\t\t\tspage->have_csum = 1;\n\t\t\tmemcpy(spage->csum, csum, sctx->csum_size);\n\t\t} else {\n\t\t\tspage->have_csum = 0;\n\t\t}\n\t\tsblock->page_count++;\n\t\tspage->page = alloc_page(GFP_KERNEL);\n\t\tif (!spage->page)\n\t\t\tgoto leave_nomem;\n\t\tlen -= l;\n\t\tlogical += l;\n\t\tphysical += l;\n\t}\n\n\tWARN_ON(sblock->page_count == 0);\n\tfor (index = 0; index < sblock->page_count; index++) {\n\t\tstruct scrub_page *spage = sblock->pagev[index];\n\t\tint ret;\n\n\t\tret = scrub_add_page_to_rd_bio(sctx, spage);\n\t\tif (ret) {\n\t\t\tscrub_block_put(sblock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* last one frees, either here or in bio completion for last page */\n\tscrub_block_put(sblock);\n\treturn 0;\n}\n\nstatic int scrub_extent_for_parity(struct scrub_parity *sparity,\n\t\t\t\t   u64 logical, u64 len,\n\t\t\t\t   u64 physical, struct btrfs_device *dev,\n\t\t\t\t   u64 flags, u64 gen, int mirror_num)\n{\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\tint ret;\n\tu8 csum[BTRFS_CSUM_SIZE];\n\tu32 blocksize;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state)) {\n\t\tscrub_parity_mark_sectors_error(sparity, logical, len);\n\t\treturn 0;\n\t}\n\n\tif (flags & BTRFS_EXTENT_FLAG_DATA) {\n\t\tblocksize = sparity->stripe_len;\n\t} else if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\n\t\tblocksize = sparity->stripe_len;\n\t} else {\n\t\tblocksize = sctx->fs_info->sectorsize;\n\t\tWARN_ON(1);\n\t}\n\n\twhile (len) {\n\t\tu64 l = min_t(u64, len, blocksize);\n\t\tint have_csum = 0;\n\n\t\tif (flags & BTRFS_EXTENT_FLAG_DATA) {\n\t\t\t/* push csums to sbio */\n\t\t\thave_csum = scrub_find_csum(sctx, logical, csum);\n\t\t\tif (have_csum == 0)\n\t\t\t\tgoto skip;\n\t\t}\n\t\tret = scrub_pages_for_parity(sparity, logical, l, physical, dev,\n\t\t\t\t\t     flags, gen, mirror_num,\n\t\t\t\t\t     have_csum ? csum : NULL);\n\t\tif (ret)\n\t\t\treturn ret;\nskip:\n\t\tlen -= l;\n\t\tlogical += l;\n\t\tphysical += l;\n\t}\n\treturn 0;\n}\n\n/*\n * Given a physical address, this will calculate it's\n * logical offset. if this is a parity stripe, it will return\n * the most left data stripe's logical offset.\n *\n * return 0 if it is a data stripe, 1 means parity stripe.\n */\nstatic int get_raid56_logic_offset(u64 physical, int num,\n\t\t\t\t   struct map_lookup *map, u64 *offset,\n\t\t\t\t   u64 *stripe_start)\n{\n\tint i;\n\tint j = 0;\n\tu64 stripe_nr;\n\tu64 last_offset;\n\tu32 stripe_index;\n\tu32 rot;\n\n\tlast_offset = (physical - map->stripes[num].physical) *\n\t\t      nr_data_stripes(map);\n\tif (stripe_start)\n\t\t*stripe_start = last_offset;\n\n\t*offset = last_offset;\n\tfor (i = 0; i < nr_data_stripes(map); i++) {\n\t\t*offset = last_offset + i * map->stripe_len;\n\n\t\tstripe_nr = div64_u64(*offset, map->stripe_len);\n\t\tstripe_nr = div_u64(stripe_nr, nr_data_stripes(map));\n\n\t\t/* Work out the disk rotation on this stripe-set */\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes, &rot);\n\t\t/* calculate which stripe this data locates */\n\t\trot += i;\n\t\tstripe_index = rot % map->num_stripes;\n\t\tif (stripe_index == num)\n\t\t\treturn 0;\n\t\tif (stripe_index < num)\n\t\t\tj++;\n\t}\n\t*offset = last_offset + j * map->stripe_len;\n\treturn 1;\n}\n\nstatic void scrub_free_parity(struct scrub_parity *sparity)\n{\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\tstruct scrub_page *curr, *next;\n\tint nbits;\n\n\tnbits = bitmap_weight(sparity->ebitmap, sparity->nsectors);\n\tif (nbits) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.read_errors += nbits;\n\t\tsctx->stat.uncorrectable_errors += nbits;\n\t\tspin_unlock(&sctx->stat_lock);\n\t}\n\n\tlist_for_each_entry_safe(curr, next, &sparity->spages, list) {\n\t\tlist_del_init(&curr->list);\n\t\tscrub_page_put(curr);\n\t}\n\n\tkfree(sparity);\n}\n\nstatic void scrub_parity_bio_endio_worker(struct btrfs_work *work)\n{\n\tstruct scrub_parity *sparity = container_of(work, struct scrub_parity,\n\t\t\t\t\t\t    work);\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\n\tscrub_free_parity(sparity);\n\tscrub_pending_bio_dec(sctx);\n}\n\nstatic void scrub_parity_bio_endio(struct bio *bio)\n{\n\tstruct scrub_parity *sparity = (struct scrub_parity *)bio->bi_private;\n\tstruct btrfs_fs_info *fs_info = sparity->sctx->fs_info;\n\n\tif (bio->bi_status)\n\t\tbitmap_or(sparity->ebitmap, sparity->ebitmap, sparity->dbitmap,\n\t\t\t  sparity->nsectors);\n\n\tbio_put(bio);\n\n\tbtrfs_init_work(&sparity->work, btrfs_scrubparity_helper,\n\t\t\tscrub_parity_bio_endio_worker, NULL, NULL);\n\tbtrfs_queue_work(fs_info->scrub_parity_workers, &sparity->work);\n}\n\nstatic void scrub_parity_check_and_repair(struct scrub_parity *sparity)\n{\n\tstruct scrub_ctx *sctx = sparity->sctx;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct bio *bio;\n\tstruct btrfs_raid_bio *rbio;\n\tstruct btrfs_bio *bbio = NULL;\n\tu64 length;\n\tint ret;\n\n\tif (!bitmap_andnot(sparity->dbitmap, sparity->dbitmap, sparity->ebitmap,\n\t\t\t   sparity->nsectors))\n\t\tgoto out;\n\n\tlength = sparity->logic_end - sparity->logic_start;\n\n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\tret = btrfs_map_sblock(fs_info, BTRFS_MAP_WRITE, sparity->logic_start,\n\t\t\t       &length, &bbio);\n\tif (ret || !bbio || !bbio->raid_map)\n\t\tgoto bbio_out;\n\n\tbio = btrfs_io_bio_alloc(0);\n\tbio->bi_iter.bi_sector = sparity->logic_start >> 9;\n\tbio->bi_private = sparity;\n\tbio->bi_end_io = scrub_parity_bio_endio;\n\n\trbio = raid56_parity_alloc_scrub_rbio(fs_info, bio, bbio,\n\t\t\t\t\t      length, sparity->scrub_dev,\n\t\t\t\t\t      sparity->dbitmap,\n\t\t\t\t\t      sparity->nsectors);\n\tif (!rbio)\n\t\tgoto rbio_out;\n\n\tscrub_pending_bio_inc(sctx);\n\traid56_parity_submit_scrub_rbio(rbio);\n\treturn;\n\nrbio_out:\n\tbio_put(bio);\nbbio_out:\n\tbtrfs_bio_counter_dec(fs_info);\n\tbtrfs_put_bbio(bbio);\n\tbitmap_or(sparity->ebitmap, sparity->ebitmap, sparity->dbitmap,\n\t\t  sparity->nsectors);\n\tspin_lock(&sctx->stat_lock);\n\tsctx->stat.malloc_errors++;\n\tspin_unlock(&sctx->stat_lock);\nout:\n\tscrub_free_parity(sparity);\n}\n\nstatic inline int scrub_calc_parity_bitmap_len(int nsectors)\n{\n\treturn DIV_ROUND_UP(nsectors, BITS_PER_LONG) * sizeof(long);\n}\n\nstatic void scrub_parity_get(struct scrub_parity *sparity)\n{\n\trefcount_inc(&sparity->refs);\n}\n\nstatic void scrub_parity_put(struct scrub_parity *sparity)\n{\n\tif (!refcount_dec_and_test(&sparity->refs))\n\t\treturn;\n\n\tscrub_parity_check_and_repair(sparity);\n}\n\nstatic noinline_for_stack int scrub_raid56_parity(struct scrub_ctx *sctx,\n\t\t\t\t\t\t  struct map_lookup *map,\n\t\t\t\t\t\t  struct btrfs_device *sdev,\n\t\t\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t\t\t  u64 logic_start,\n\t\t\t\t\t\t  u64 logic_end)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_root *root = fs_info->extent_root;\n\tstruct btrfs_root *csum_root = fs_info->csum_root;\n\tstruct btrfs_extent_item *extent;\n\tstruct btrfs_bio *bbio = NULL;\n\tu64 flags;\n\tint ret;\n\tint slot;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tu64 generation;\n\tu64 extent_logical;\n\tu64 extent_physical;\n\tu64 extent_len;\n\tu64 mapped_length;\n\tstruct btrfs_device *extent_dev;\n\tstruct scrub_parity *sparity;\n\tint nsectors;\n\tint bitmap_len;\n\tint extent_mirror_num;\n\tint stop_loop = 0;\n\n\tnsectors = div_u64(map->stripe_len, fs_info->sectorsize);\n\tbitmap_len = scrub_calc_parity_bitmap_len(nsectors);\n\tsparity = kzalloc(sizeof(struct scrub_parity) + 2 * bitmap_len,\n\t\t\t  GFP_NOFS);\n\tif (!sparity) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.malloc_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\tsparity->stripe_len = map->stripe_len;\n\tsparity->nsectors = nsectors;\n\tsparity->sctx = sctx;\n\tsparity->scrub_dev = sdev;\n\tsparity->logic_start = logic_start;\n\tsparity->logic_end = logic_end;\n\trefcount_set(&sparity->refs, 1);\n\tINIT_LIST_HEAD(&sparity->spages);\n\tsparity->dbitmap = sparity->bitmap;\n\tsparity->ebitmap = (void *)sparity->bitmap + bitmap_len;\n\n\tret = 0;\n\twhile (logic_start < logic_end) {\n\t\tif (btrfs_fs_incompat(fs_info, SKINNY_METADATA))\n\t\t\tkey.type = BTRFS_METADATA_ITEM_KEY;\n\t\telse\n\t\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\tkey.objectid = logic_start;\n\t\tkey.offset = (u64)-1;\n\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tif (ret > 0) {\n\t\t\tret = btrfs_previous_extent_item(root, path, 0);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tif (ret > 0) {\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\tret = btrfs_search_slot(NULL, root, &key,\n\t\t\t\t\t\t\tpath, 0, 0);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tstop_loop = 0;\n\t\twhile (1) {\n\t\t\tu64 bytes;\n\n\t\t\tl = path->nodes[0];\n\t\t\tslot = path->slots[0];\n\t\t\tif (slot >= btrfs_header_nritems(l)) {\n\t\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto out;\n\n\t\t\t\tstop_loop = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\t\tif (key.type != BTRFS_EXTENT_ITEM_KEY &&\n\t\t\t    key.type != BTRFS_METADATA_ITEM_KEY)\n\t\t\t\tgoto next;\n\n\t\t\tif (key.type == BTRFS_METADATA_ITEM_KEY)\n\t\t\t\tbytes = fs_info->nodesize;\n\t\t\telse\n\t\t\t\tbytes = key.offset;\n\n\t\t\tif (key.objectid + bytes <= logic_start)\n\t\t\t\tgoto next;\n\n\t\t\tif (key.objectid >= logic_end) {\n\t\t\t\tstop_loop = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\twhile (key.objectid >= logic_start + map->stripe_len)\n\t\t\t\tlogic_start += map->stripe_len;\n\n\t\t\textent = btrfs_item_ptr(l, slot,\n\t\t\t\t\t\tstruct btrfs_extent_item);\n\t\t\tflags = btrfs_extent_flags(l, extent);\n\t\t\tgeneration = btrfs_extent_generation(l, extent);\n\n\t\t\tif ((flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) &&\n\t\t\t    (key.objectid < logic_start ||\n\t\t\t     key.objectid + bytes >\n\t\t\t     logic_start + map->stripe_len)) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t  \"scrub: tree block %llu spanning stripes, ignored. logical=%llu\",\n\t\t\t\t\t  key.objectid, logic_start);\n\t\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\t\tsctx->stat.uncorrectable_errors++;\n\t\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\t\tgoto next;\n\t\t\t}\nagain:\n\t\t\textent_logical = key.objectid;\n\t\t\textent_len = bytes;\n\n\t\t\tif (extent_logical < logic_start) {\n\t\t\t\textent_len -= logic_start - extent_logical;\n\t\t\t\textent_logical = logic_start;\n\t\t\t}\n\n\t\t\tif (extent_logical + extent_len >\n\t\t\t    logic_start + map->stripe_len)\n\t\t\t\textent_len = logic_start + map->stripe_len -\n\t\t\t\t\t     extent_logical;\n\n\t\t\tscrub_parity_mark_sectors_data(sparity, extent_logical,\n\t\t\t\t\t\t       extent_len);\n\n\t\t\tmapped_length = extent_len;\n\t\t\tbbio = NULL;\n\t\t\tret = btrfs_map_block(fs_info, BTRFS_MAP_READ,\n\t\t\t\t\textent_logical, &mapped_length, &bbio,\n\t\t\t\t\t0);\n\t\t\tif (!ret) {\n\t\t\t\tif (!bbio || mapped_length < extent_len)\n\t\t\t\t\tret = -EIO;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_put_bbio(bbio);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\textent_physical = bbio->stripes[0].physical;\n\t\t\textent_mirror_num = bbio->mirror_num;\n\t\t\textent_dev = bbio->stripes[0].dev;\n\t\t\tbtrfs_put_bbio(bbio);\n\n\t\t\tret = btrfs_lookup_csums_range(csum_root,\n\t\t\t\t\t\textent_logical,\n\t\t\t\t\t\textent_logical + extent_len - 1,\n\t\t\t\t\t\t&sctx->csum_list, 1);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tret = scrub_extent_for_parity(sparity, extent_logical,\n\t\t\t\t\t\t      extent_len,\n\t\t\t\t\t\t      extent_physical,\n\t\t\t\t\t\t      extent_dev, flags,\n\t\t\t\t\t\t      generation,\n\t\t\t\t\t\t      extent_mirror_num);\n\n\t\t\tscrub_free_csums(sctx);\n\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tif (extent_logical + extent_len <\n\t\t\t    key.objectid + bytes) {\n\t\t\t\tlogic_start += map->stripe_len;\n\n\t\t\t\tif (logic_start >= logic_end) {\n\t\t\t\t\tstop_loop = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (logic_start < key.objectid + bytes) {\n\t\t\t\t\tcond_resched();\n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\t\t\t}\nnext:\n\t\t\tpath->slots[0]++;\n\t\t}\n\n\t\tbtrfs_release_path(path);\n\n\t\tif (stop_loop)\n\t\t\tbreak;\n\n\t\tlogic_start += map->stripe_len;\n\t}\nout:\n\tif (ret < 0)\n\t\tscrub_parity_mark_sectors_error(sparity, logic_start,\n\t\t\t\t\t\tlogic_end - logic_start);\n\tscrub_parity_put(sparity);\n\tscrub_submit(sctx);\n\tmutex_lock(&sctx->wr_lock);\n\tscrub_wr_submit(sctx);\n\tmutex_unlock(&sctx->wr_lock);\n\n\tbtrfs_release_path(path);\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic noinline_for_stack int scrub_stripe(struct scrub_ctx *sctx,\n\t\t\t\t\t   struct map_lookup *map,\n\t\t\t\t\t   struct btrfs_device *scrub_dev,\n\t\t\t\t\t   int num, u64 base, u64 length)\n{\n\tstruct btrfs_path *path, *ppath;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_root *root = fs_info->extent_root;\n\tstruct btrfs_root *csum_root = fs_info->csum_root;\n\tstruct btrfs_extent_item *extent;\n\tstruct blk_plug plug;\n\tu64 flags;\n\tint ret;\n\tint slot;\n\tu64 nstripes;\n\tstruct extent_buffer *l;\n\tu64 physical;\n\tu64 logical;\n\tu64 logic_end;\n\tu64 physical_end;\n\tu64 generation;\n\tint mirror_num;\n\tstruct reada_control *reada1;\n\tstruct reada_control *reada2;\n\tstruct btrfs_key key;\n\tstruct btrfs_key key_end;\n\tu64 increment = map->stripe_len;\n\tu64 offset;\n\tu64 extent_logical;\n\tu64 extent_physical;\n\tu64 extent_len;\n\tu64 stripe_logical;\n\tu64 stripe_end;\n\tstruct btrfs_device *extent_dev;\n\tint extent_mirror_num;\n\tint stop_loop = 0;\n\n\tphysical = map->stripes[num].physical;\n\toffset = 0;\n\tnstripes = div64_u64(length, map->stripe_len);\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID0) {\n\t\toffset = map->stripe_len * num;\n\t\tincrement = map->stripe_len * map->num_stripes;\n\t\tmirror_num = 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID10) {\n\t\tint factor = map->num_stripes / map->sub_stripes;\n\t\toffset = map->stripe_len * (num / map->sub_stripes);\n\t\tincrement = map->stripe_len * factor;\n\t\tmirror_num = num % map->sub_stripes + 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID1) {\n\t\tincrement = map->stripe_len;\n\t\tmirror_num = num % map->num_stripes + 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_DUP) {\n\t\tincrement = map->stripe_len;\n\t\tmirror_num = num % map->num_stripes + 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tget_raid56_logic_offset(physical, num, map, &offset, NULL);\n\t\tincrement = map->stripe_len * nr_data_stripes(map);\n\t\tmirror_num = 1;\n\t} else {\n\t\tincrement = map->stripe_len;\n\t\tmirror_num = 1;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tppath = btrfs_alloc_path();\n\tif (!ppath) {\n\t\tbtrfs_free_path(path);\n\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * work on commit root. The related disk blocks are static as\n\t * long as COW is applied. This means, it is save to rewrite\n\t * them to repair disk errors without any race conditions\n\t */\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tppath->search_commit_root = 1;\n\tppath->skip_locking = 1;\n\t/*\n\t * trigger the readahead for extent tree csum tree and wait for\n\t * completion. During readahead, the scrub is officially paused\n\t * to not hold off transaction commits\n\t */\n\tlogical = base + offset;\n\tphysical_end = physical + nstripes * map->stripe_len;\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tget_raid56_logic_offset(physical_end, num,\n\t\t\t\t\tmap, &logic_end, NULL);\n\t\tlogic_end += base;\n\t} else {\n\t\tlogic_end = logical + increment * nstripes;\n\t}\n\twait_event(sctx->list_wait,\n\t\t   atomic_read(&sctx->bios_in_flight) == 0);\n\tscrub_blocked_if_needed(fs_info);\n\n\t/* FIXME it might be better to start readahead at commit root */\n\tkey.objectid = logical;\n\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\tkey.offset = (u64)0;\n\tkey_end.objectid = logic_end;\n\tkey_end.type = BTRFS_METADATA_ITEM_KEY;\n\tkey_end.offset = (u64)-1;\n\treada1 = btrfs_reada_add(root, &key, &key_end);\n\n\tkey.objectid = BTRFS_EXTENT_CSUM_OBJECTID;\n\tkey.type = BTRFS_EXTENT_CSUM_KEY;\n\tkey.offset = logical;\n\tkey_end.objectid = BTRFS_EXTENT_CSUM_OBJECTID;\n\tkey_end.type = BTRFS_EXTENT_CSUM_KEY;\n\tkey_end.offset = logic_end;\n\treada2 = btrfs_reada_add(csum_root, &key, &key_end);\n\n\tif (!IS_ERR(reada1))\n\t\tbtrfs_reada_wait(reada1);\n\tif (!IS_ERR(reada2))\n\t\tbtrfs_reada_wait(reada2);\n\n\n\t/*\n\t * collect all data csums for the stripe to avoid seeking during\n\t * the scrub. This might currently (crc32) end up to be about 1MB\n\t */\n\tblk_start_plug(&plug);\n\n\t/*\n\t * now find all extents for each stripe and scrub them\n\t */\n\tret = 0;\n\twhile (physical < physical_end) {\n\t\t/*\n\t\t * canceled?\n\t\t */\n\t\tif (atomic_read(&fs_info->scrub_cancel_req) ||\n\t\t    atomic_read(&sctx->cancel_req)) {\n\t\t\tret = -ECANCELED;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * check to see if we have to pause\n\t\t */\n\t\tif (atomic_read(&fs_info->scrub_pause_req)) {\n\t\t\t/* push queued extents */\n\t\t\tsctx->flush_all_writes = true;\n\t\t\tscrub_submit(sctx);\n\t\t\tmutex_lock(&sctx->wr_lock);\n\t\t\tscrub_wr_submit(sctx);\n\t\t\tmutex_unlock(&sctx->wr_lock);\n\t\t\twait_event(sctx->list_wait,\n\t\t\t\t   atomic_read(&sctx->bios_in_flight) == 0);\n\t\t\tsctx->flush_all_writes = false;\n\t\t\tscrub_blocked_if_needed(fs_info);\n\t\t}\n\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\t\tret = get_raid56_logic_offset(physical, num, map,\n\t\t\t\t\t\t      &logical,\n\t\t\t\t\t\t      &stripe_logical);\n\t\t\tlogical += base;\n\t\t\tif (ret) {\n\t\t\t\t/* it is parity strip */\n\t\t\t\tstripe_logical += base;\n\t\t\t\tstripe_end = stripe_logical + increment;\n\t\t\t\tret = scrub_raid56_parity(sctx, map, scrub_dev,\n\t\t\t\t\t\t\t  ppath, stripe_logical,\n\t\t\t\t\t\t\t  stripe_end);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tgoto skip;\n\t\t\t}\n\t\t}\n\n\t\tif (btrfs_fs_incompat(fs_info, SKINNY_METADATA))\n\t\t\tkey.type = BTRFS_METADATA_ITEM_KEY;\n\t\telse\n\t\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\tkey.objectid = logical;\n\t\tkey.offset = (u64)-1;\n\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tif (ret > 0) {\n\t\t\tret = btrfs_previous_extent_item(root, path, 0);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tif (ret > 0) {\n\t\t\t\t/* there's no smaller item, so stick with the\n\t\t\t\t * larger one */\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\tret = btrfs_search_slot(NULL, root, &key,\n\t\t\t\t\t\t\tpath, 0, 0);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tstop_loop = 0;\n\t\twhile (1) {\n\t\t\tu64 bytes;\n\n\t\t\tl = path->nodes[0];\n\t\t\tslot = path->slots[0];\n\t\t\tif (slot >= btrfs_header_nritems(l)) {\n\t\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto out;\n\n\t\t\t\tstop_loop = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\t\tif (key.type != BTRFS_EXTENT_ITEM_KEY &&\n\t\t\t    key.type != BTRFS_METADATA_ITEM_KEY)\n\t\t\t\tgoto next;\n\n\t\t\tif (key.type == BTRFS_METADATA_ITEM_KEY)\n\t\t\t\tbytes = fs_info->nodesize;\n\t\t\telse\n\t\t\t\tbytes = key.offset;\n\n\t\t\tif (key.objectid + bytes <= logical)\n\t\t\t\tgoto next;\n\n\t\t\tif (key.objectid >= logical + map->stripe_len) {\n\t\t\t\t/* out of this device extent */\n\t\t\t\tif (key.objectid >= logic_end)\n\t\t\t\t\tstop_loop = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\textent = btrfs_item_ptr(l, slot,\n\t\t\t\t\t\tstruct btrfs_extent_item);\n\t\t\tflags = btrfs_extent_flags(l, extent);\n\t\t\tgeneration = btrfs_extent_generation(l, extent);\n\n\t\t\tif ((flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) &&\n\t\t\t    (key.objectid < logical ||\n\t\t\t     key.objectid + bytes >\n\t\t\t     logical + map->stripe_len)) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t   \"scrub: tree block %llu spanning stripes, ignored. logical=%llu\",\n\t\t\t\t       key.objectid, logical);\n\t\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\t\tsctx->stat.uncorrectable_errors++;\n\t\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t\t\tgoto next;\n\t\t\t}\n\nagain:\n\t\t\textent_logical = key.objectid;\n\t\t\textent_len = bytes;\n\n\t\t\t/*\n\t\t\t * trim extent to this stripe\n\t\t\t */\n\t\t\tif (extent_logical < logical) {\n\t\t\t\textent_len -= logical - extent_logical;\n\t\t\t\textent_logical = logical;\n\t\t\t}\n\t\t\tif (extent_logical + extent_len >\n\t\t\t    logical + map->stripe_len) {\n\t\t\t\textent_len = logical + map->stripe_len -\n\t\t\t\t\t     extent_logical;\n\t\t\t}\n\n\t\t\textent_physical = extent_logical - logical + physical;\n\t\t\textent_dev = scrub_dev;\n\t\t\textent_mirror_num = mirror_num;\n\t\t\tif (sctx->is_dev_replace)\n\t\t\t\tscrub_remap_extent(fs_info, extent_logical,\n\t\t\t\t\t\t   extent_len, &extent_physical,\n\t\t\t\t\t\t   &extent_dev,\n\t\t\t\t\t\t   &extent_mirror_num);\n\n\t\t\tret = btrfs_lookup_csums_range(csum_root,\n\t\t\t\t\t\t       extent_logical,\n\t\t\t\t\t\t       extent_logical +\n\t\t\t\t\t\t       extent_len - 1,\n\t\t\t\t\t\t       &sctx->csum_list, 1);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tret = scrub_extent(sctx, map, extent_logical, extent_len,\n\t\t\t\t\t   extent_physical, extent_dev, flags,\n\t\t\t\t\t   generation, extent_mirror_num,\n\t\t\t\t\t   extent_logical - logical + physical);\n\n\t\t\tscrub_free_csums(sctx);\n\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tif (extent_logical + extent_len <\n\t\t\t    key.objectid + bytes) {\n\t\t\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\t\t\t\t/*\n\t\t\t\t\t * loop until we find next data stripe\n\t\t\t\t\t * or we have finished all stripes.\n\t\t\t\t\t */\nloop:\n\t\t\t\t\tphysical += map->stripe_len;\n\t\t\t\t\tret = get_raid56_logic_offset(physical,\n\t\t\t\t\t\t\tnum, map, &logical,\n\t\t\t\t\t\t\t&stripe_logical);\n\t\t\t\t\tlogical += base;\n\n\t\t\t\t\tif (ret && physical < physical_end) {\n\t\t\t\t\t\tstripe_logical += base;\n\t\t\t\t\t\tstripe_end = stripe_logical +\n\t\t\t\t\t\t\t\tincrement;\n\t\t\t\t\t\tret = scrub_raid56_parity(sctx,\n\t\t\t\t\t\t\tmap, scrub_dev, ppath,\n\t\t\t\t\t\t\tstripe_logical,\n\t\t\t\t\t\t\tstripe_end);\n\t\t\t\t\t\tif (ret)\n\t\t\t\t\t\t\tgoto out;\n\t\t\t\t\t\tgoto loop;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tphysical += map->stripe_len;\n\t\t\t\t\tlogical += increment;\n\t\t\t\t}\n\t\t\t\tif (logical < key.objectid + bytes) {\n\t\t\t\t\tcond_resched();\n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\n\t\t\t\tif (physical >= physical_end) {\n\t\t\t\t\tstop_loop = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\nnext:\n\t\t\tpath->slots[0]++;\n\t\t}\n\t\tbtrfs_release_path(path);\nskip:\n\t\tlogical += increment;\n\t\tphysical += map->stripe_len;\n\t\tspin_lock(&sctx->stat_lock);\n\t\tif (stop_loop)\n\t\t\tsctx->stat.last_physical = map->stripes[num].physical +\n\t\t\t\t\t\t   length;\n\t\telse\n\t\t\tsctx->stat.last_physical = physical;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (stop_loop)\n\t\t\tbreak;\n\t}\nout:\n\t/* push queued extents */\n\tscrub_submit(sctx);\n\tmutex_lock(&sctx->wr_lock);\n\tscrub_wr_submit(sctx);\n\tmutex_unlock(&sctx->wr_lock);\n\n\tblk_finish_plug(&plug);\n\tbtrfs_free_path(path);\n\tbtrfs_free_path(ppath);\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic noinline_for_stack int scrub_chunk(struct scrub_ctx *sctx,\n\t\t\t\t\t  struct btrfs_device *scrub_dev,\n\t\t\t\t\t  u64 chunk_offset, u64 length,\n\t\t\t\t\t  u64 dev_offset,\n\t\t\t\t\t  struct btrfs_block_group_cache *cache)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tint i;\n\tint ret = 0;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, chunk_offset, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\tif (!em) {\n\t\t/*\n\t\t * Might have been an unused block group deleted by the cleaner\n\t\t * kthread or relocation.\n\t\t */\n\t\tspin_lock(&cache->lock);\n\t\tif (!cache->removed)\n\t\t\tret = -EINVAL;\n\t\tspin_unlock(&cache->lock);\n\n\t\treturn ret;\n\t}\n\n\tmap = em->map_lookup;\n\tif (em->start != chunk_offset)\n\t\tgoto out;\n\n\tif (em->len < length)\n\t\tgoto out;\n\n\tfor (i = 0; i < map->num_stripes; ++i) {\n\t\tif (map->stripes[i].dev->bdev == scrub_dev->bdev &&\n\t\t    map->stripes[i].physical == dev_offset) {\n\t\t\tret = scrub_stripe(sctx, map, scrub_dev, i,\n\t\t\t\t\t   chunk_offset, length);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tfree_extent_map(em);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack\nint scrub_enumerate_chunks(struct scrub_ctx *sctx,\n\t\t\t   struct btrfs_device *scrub_dev, u64 start, u64 end)\n{\n\tstruct btrfs_dev_extent *dev_extent = NULL;\n\tstruct btrfs_path *path;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tu64 length;\n\tu64 chunk_offset;\n\tint ret = 0;\n\tint ro_set;\n\tint slot;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_block_group_cache *cache;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_FORWARD;\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tkey.objectid = scrub_dev->devid;\n\tkey.offset = 0ull;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tif (ret > 0) {\n\t\t\tif (path->slots[0] >=\n\t\t\t    btrfs_header_nritems(path->nodes[0])) {\n\t\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tbreak;\n\t\t\t\tif (ret > 0) {\n\t\t\t\t\tret = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tret = 0;\n\t\t\t}\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tbtrfs_item_key_to_cpu(l, &found_key, slot);\n\n\t\tif (found_key.objectid != scrub_dev->devid)\n\t\t\tbreak;\n\n\t\tif (found_key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tbreak;\n\n\t\tif (found_key.offset >= end)\n\t\t\tbreak;\n\n\t\tif (found_key.offset < key.offset)\n\t\t\tbreak;\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\tlength = btrfs_dev_extent_length(l, dev_extent);\n\n\t\tif (found_key.offset + length <= start)\n\t\t\tgoto skip;\n\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\n\n\t\t/*\n\t\t * get a reference on the corresponding block group to prevent\n\t\t * the chunk from going away while we scrub it\n\t\t */\n\t\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\n\t\t/* some chunks are removed but not committed to disk yet,\n\t\t * continue scrubbing */\n\t\tif (!cache)\n\t\t\tgoto skip;\n\n\t\t/*\n\t\t * we need call btrfs_inc_block_group_ro() with scrubs_paused,\n\t\t * to avoid deadlock caused by:\n\t\t * btrfs_inc_block_group_ro()\n\t\t * -> btrfs_wait_for_commit()\n\t\t * -> btrfs_commit_transaction()\n\t\t * -> btrfs_scrub_pause()\n\t\t */\n\t\tscrub_pause_on(fs_info);\n\t\tret = btrfs_inc_block_group_ro(cache);\n\t\tif (!ret && sctx->is_dev_replace) {\n\t\t\t/*\n\t\t\t * If we are doing a device replace wait for any tasks\n\t\t\t * that started delalloc right before we set the block\n\t\t\t * group to RO mode, as they might have just allocated\n\t\t\t * an extent from it or decided they could do a nocow\n\t\t\t * write. And if any such tasks did that, wait for their\n\t\t\t * ordered extents to complete and then commit the\n\t\t\t * current transaction, so that we can later see the new\n\t\t\t * extent items in the extent tree - the ordered extents\n\t\t\t * create delayed data references (for cow writes) when\n\t\t\t * they complete, which will be run and insert the\n\t\t\t * corresponding extent items into the extent tree when\n\t\t\t * we commit the transaction they used when running\n\t\t\t * inode.c:btrfs_finish_ordered_io(). We later use\n\t\t\t * the commit root of the extent tree to find extents\n\t\t\t * to copy from the srcdev into the tgtdev, and we don't\n\t\t\t * want to miss any new extents.\n\t\t\t */\n\t\t\tbtrfs_wait_block_group_reservations(cache);\n\t\t\tbtrfs_wait_nocow_writers(cache);\n\t\t\tret = btrfs_wait_ordered_roots(fs_info, U64_MAX,\n\t\t\t\t\t\t       cache->key.objectid,\n\t\t\t\t\t\t       cache->key.offset);\n\t\t\tif (ret > 0) {\n\t\t\t\tstruct btrfs_trans_handle *trans;\n\n\t\t\t\ttrans = btrfs_join_transaction(root);\n\t\t\t\tif (IS_ERR(trans))\n\t\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\telse\n\t\t\t\t\tret = btrfs_commit_transaction(trans);\n\t\t\t\tif (ret) {\n\t\t\t\t\tscrub_pause_off(fs_info);\n\t\t\t\t\tbtrfs_put_block_group(cache);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tscrub_pause_off(fs_info);\n\n\t\tif (ret == 0) {\n\t\t\tro_set = 1;\n\t\t} else if (ret == -ENOSPC) {\n\t\t\t/*\n\t\t\t * btrfs_inc_block_group_ro return -ENOSPC when it\n\t\t\t * failed in creating new chunk for metadata.\n\t\t\t * It is not a problem for scrub/replace, because\n\t\t\t * metadata are always cowed, and our scrub paused\n\t\t\t * commit_transactions.\n\t\t\t */\n\t\t\tro_set = 0;\n\t\t} else {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"failed setting block group ro: %d\", ret);\n\t\t\tbtrfs_put_block_group(cache);\n\t\t\tbreak;\n\t\t}\n\n\t\tdown_write(&fs_info->dev_replace.rwsem);\n\t\tdev_replace->cursor_right = found_key.offset + length;\n\t\tdev_replace->cursor_left = found_key.offset;\n\t\tdev_replace->item_needs_writeback = 1;\n\t\tup_write(&dev_replace->rwsem);\n\n\t\tret = scrub_chunk(sctx, scrub_dev, chunk_offset, length,\n\t\t\t\t  found_key.offset, cache);\n\n\t\t/*\n\t\t * flush, submit all pending read and write bios, afterwards\n\t\t * wait for them.\n\t\t * Note that in the dev replace case, a read request causes\n\t\t * write requests that are submitted in the read completion\n\t\t * worker. Therefore in the current situation, it is required\n\t\t * that all write requests are flushed, so that all read and\n\t\t * write requests are really completed when bios_in_flight\n\t\t * changes to 0.\n\t\t */\n\t\tsctx->flush_all_writes = true;\n\t\tscrub_submit(sctx);\n\t\tmutex_lock(&sctx->wr_lock);\n\t\tscrub_wr_submit(sctx);\n\t\tmutex_unlock(&sctx->wr_lock);\n\n\t\twait_event(sctx->list_wait,\n\t\t\t   atomic_read(&sctx->bios_in_flight) == 0);\n\n\t\tscrub_pause_on(fs_info);\n\n\t\t/*\n\t\t * must be called before we decrease @scrub_paused.\n\t\t * make sure we don't block transaction commit while\n\t\t * we are waiting pending workers finished.\n\t\t */\n\t\twait_event(sctx->list_wait,\n\t\t\t   atomic_read(&sctx->workers_pending) == 0);\n\t\tsctx->flush_all_writes = false;\n\n\t\tscrub_pause_off(fs_info);\n\n\t\tdown_write(&fs_info->dev_replace.rwsem);\n\t\tdev_replace->cursor_left = dev_replace->cursor_right;\n\t\tdev_replace->item_needs_writeback = 1;\n\t\tup_write(&fs_info->dev_replace.rwsem);\n\n\t\tif (ro_set)\n\t\t\tbtrfs_dec_block_group_ro(cache);\n\n\t\t/*\n\t\t * We might have prevented the cleaner kthread from deleting\n\t\t * this block group if it was already unused because we raced\n\t\t * and set it to RO mode first. So add it back to the unused\n\t\t * list, otherwise it might not ever be deleted unless a manual\n\t\t * balance is triggered or it becomes used and unused again.\n\t\t */\n\t\tspin_lock(&cache->lock);\n\t\tif (!cache->removed && !cache->ro && cache->reserved == 0 &&\n\t\t    btrfs_block_group_used(&cache->item) == 0) {\n\t\t\tspin_unlock(&cache->lock);\n\t\t\tbtrfs_mark_bg_unused(cache);\n\t\t} else {\n\t\t\tspin_unlock(&cache->lock);\n\t\t}\n\n\t\tbtrfs_put_block_group(cache);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (sctx->is_dev_replace &&\n\t\t    atomic64_read(&dev_replace->num_write_errors) > 0) {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (sctx->stat.malloc_errors > 0) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\nskip:\n\t\tkey.offset = found_key.offset + length;\n\t\tbtrfs_release_path(path);\n\t}\n\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int scrub_supers(struct scrub_ctx *sctx,\n\t\t\t\t\t   struct btrfs_device *scrub_dev)\n{\n\tint\ti;\n\tu64\tbytenr;\n\tu64\tgen;\n\tint\tret;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\n\tif (test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state))\n\t\treturn -EIO;\n\n\t/* Seed devices of a new filesystem has their own generation. */\n\tif (scrub_dev->fs_devices != fs_info->fs_devices)\n\t\tgen = scrub_dev->generation;\n\telse\n\t\tgen = fs_info->last_trans_committed;\n\n\tfor (i = 0; i < BTRFS_SUPER_MIRROR_MAX; i++) {\n\t\tbytenr = btrfs_sb_offset(i);\n\t\tif (bytenr + BTRFS_SUPER_INFO_SIZE >\n\t\t    scrub_dev->commit_total_bytes)\n\t\t\tbreak;\n\n\t\tret = scrub_pages(sctx, bytenr, BTRFS_SUPER_INFO_SIZE, bytenr,\n\t\t\t\t  scrub_dev, BTRFS_EXTENT_FLAG_SUPER, gen, i,\n\t\t\t\t  NULL, 1, bytenr);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\twait_event(sctx->list_wait, atomic_read(&sctx->bios_in_flight) == 0);\n\n\treturn 0;\n}\n\n/*\n * get a reference count on fs_info->scrub_workers. start worker if necessary\n */\nstatic noinline_for_stack int scrub_workers_get(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\tint is_dev_replace)\n{\n\tunsigned int flags = WQ_FREEZABLE | WQ_UNBOUND;\n\tint max_active = fs_info->thread_pool_size;\n\n\tif (fs_info->scrub_workers_refcnt == 0) {\n\t\tfs_info->scrub_workers = btrfs_alloc_workqueue(fs_info, \"scrub\",\n\t\t\t\tflags, is_dev_replace ? 1 : max_active, 4);\n\t\tif (!fs_info->scrub_workers)\n\t\t\tgoto fail_scrub_workers;\n\n\t\tfs_info->scrub_wr_completion_workers =\n\t\t\tbtrfs_alloc_workqueue(fs_info, \"scrubwrc\", flags,\n\t\t\t\t\t      max_active, 2);\n\t\tif (!fs_info->scrub_wr_completion_workers)\n\t\t\tgoto fail_scrub_wr_completion_workers;\n\n\t\tfs_info->scrub_parity_workers =\n\t\t\tbtrfs_alloc_workqueue(fs_info, \"scrubparity\", flags,\n\t\t\t\t\t      max_active, 2);\n\t\tif (!fs_info->scrub_parity_workers)\n\t\t\tgoto fail_scrub_parity_workers;\n\t}\n\t++fs_info->scrub_workers_refcnt;\n\treturn 0;\n\nfail_scrub_parity_workers:\n\tbtrfs_destroy_workqueue(fs_info->scrub_wr_completion_workers);\nfail_scrub_wr_completion_workers:\n\tbtrfs_destroy_workqueue(fs_info->scrub_workers);\nfail_scrub_workers:\n\treturn -ENOMEM;\n}\n\nstatic noinline_for_stack void scrub_workers_put(struct btrfs_fs_info *fs_info)\n{\n\tif (--fs_info->scrub_workers_refcnt == 0) {\n\t\tbtrfs_destroy_workqueue(fs_info->scrub_workers);\n\t\tbtrfs_destroy_workqueue(fs_info->scrub_wr_completion_workers);\n\t\tbtrfs_destroy_workqueue(fs_info->scrub_parity_workers);\n\t}\n\tWARN_ON(fs_info->scrub_workers_refcnt < 0);\n}\n\nint btrfs_scrub_dev(struct btrfs_fs_info *fs_info, u64 devid, u64 start,\n\t\t    u64 end, struct btrfs_scrub_progress *progress,\n\t\t    int readonly, int is_dev_replace)\n{\n\tstruct scrub_ctx *sctx;\n\tint ret;\n\tstruct btrfs_device *dev;\n\tunsigned int nofs_flag;\n\n\tif (btrfs_fs_closing(fs_info))\n\t\treturn -EINVAL;\n\n\tif (fs_info->nodesize > BTRFS_STRIPE_LEN) {\n\t\t/*\n\t\t * in this case scrub is unable to calculate the checksum\n\t\t * the way scrub is implemented. Do not handle this\n\t\t * situation at all because it won't ever happen.\n\t\t */\n\t\tbtrfs_err(fs_info,\n\t\t\t   \"scrub: size assumption nodesize <= BTRFS_STRIPE_LEN (%d <= %d) fails\",\n\t\t       fs_info->nodesize,\n\t\t       BTRFS_STRIPE_LEN);\n\t\treturn -EINVAL;\n\t}\n\n\tif (fs_info->sectorsize != PAGE_SIZE) {\n\t\t/* not supported for data w/o checksums */\n\t\tbtrfs_err_rl(fs_info,\n\t\t\t   \"scrub: size assumption sectorsize != PAGE_SIZE (%d != %lu) fails\",\n\t\t       fs_info->sectorsize, PAGE_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\tif (fs_info->nodesize >\n\t    PAGE_SIZE * SCRUB_MAX_PAGES_PER_BLOCK ||\n\t    fs_info->sectorsize > PAGE_SIZE * SCRUB_MAX_PAGES_PER_BLOCK) {\n\t\t/*\n\t\t * would exhaust the array bounds of pagev member in\n\t\t * struct scrub_block\n\t\t */\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"scrub: size assumption nodesize and sectorsize <= SCRUB_MAX_PAGES_PER_BLOCK (%d <= %d && %d <= %d) fails\",\n\t\t       fs_info->nodesize,\n\t\t       SCRUB_MAX_PAGES_PER_BLOCK,\n\t\t       fs_info->sectorsize,\n\t\t       SCRUB_MAX_PAGES_PER_BLOCK);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Allocate outside of device_list_mutex */\n\tsctx = scrub_setup_ctx(fs_info, is_dev_replace);\n\tif (IS_ERR(sctx))\n\t\treturn PTR_ERR(sctx);\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev || (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) &&\n\t\t     !is_dev_replace)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -ENODEV;\n\t\tgoto out_free_ctx;\n\t}\n\n\tif (!is_dev_replace && !readonly &&\n\t    !test_bit(BTRFS_DEV_STATE_WRITEABLE, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tbtrfs_err_in_rcu(fs_info, \"scrub: device %s is not writable\",\n\t\t\t\trcu_str_deref(dev->name));\n\t\tret = -EROFS;\n\t\tgoto out_free_ctx;\n\t}\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &dev->dev_state) ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EIO;\n\t\tgoto out_free_ctx;\n\t}\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (dev->scrub_ctx ||\n\t    (!is_dev_replace &&\n\t     btrfs_dev_replace_is_ongoing(&fs_info->dev_replace))) {\n\t\tup_read(&fs_info->dev_replace.rwsem);\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EINPROGRESS;\n\t\tgoto out_free_ctx;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\tret = scrub_workers_get(fs_info, is_dev_replace);\n\tif (ret) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tgoto out_free_ctx;\n\t}\n\n\tsctx->readonly = readonly;\n\tdev->scrub_ctx = sctx;\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t/*\n\t * checking @scrub_pause_req here, we can avoid\n\t * race between committing transaction and scrubbing.\n\t */\n\t__scrub_blocked_if_needed(fs_info);\n\tatomic_inc(&fs_info->scrubs_running);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\t/*\n\t * In order to avoid deadlock with reclaim when there is a transaction\n\t * trying to pause scrub, make sure we use GFP_NOFS for all the\n\t * allocations done at btrfs_scrub_pages() and scrub_pages_for_parity()\n\t * invoked by our callees. The pausing request is done when the\n\t * transaction commit starts, and it blocks the transaction until scrub\n\t * is paused (done at specific points at scrub_stripe() or right above\n\t * before incrementing fs_info->scrubs_running).\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\tif (!is_dev_replace) {\n\t\t/*\n\t\t * by holding device list mutex, we can\n\t\t * kick off writing super in log tree sync.\n\t\t */\n\t\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = scrub_supers(sctx, dev);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t}\n\n\tif (!ret)\n\t\tret = scrub_enumerate_chunks(sctx, dev, start, end);\n\tmemalloc_nofs_restore(nofs_flag);\n\n\twait_event(sctx->list_wait, atomic_read(&sctx->bios_in_flight) == 0);\n\tatomic_dec(&fs_info->scrubs_running);\n\twake_up(&fs_info->scrub_pause_wait);\n\n\twait_event(sctx->list_wait, atomic_read(&sctx->workers_pending) == 0);\n\n\tif (progress)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tdev->scrub_ctx = NULL;\n\tscrub_workers_put(fs_info);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\tscrub_put_ctx(sctx);\n\n\treturn ret;\n\nout_free_ctx:\n\tscrub_free_ctx(sctx);\n\n\treturn ret;\n}\n\nvoid btrfs_scrub_pause(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->scrub_lock);\n\tatomic_inc(&fs_info->scrub_pause_req);\n\twhile (atomic_read(&fs_info->scrubs_paused) !=\n\t       atomic_read(&fs_info->scrubs_running)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t\t   atomic_read(&fs_info->scrubs_paused) ==\n\t\t\t   atomic_read(&fs_info->scrubs_running));\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n\tmutex_unlock(&fs_info->scrub_lock);\n}\n\nvoid btrfs_scrub_continue(struct btrfs_fs_info *fs_info)\n{\n\tatomic_dec(&fs_info->scrub_pause_req);\n\twake_up(&fs_info->scrub_pause_wait);\n}\n\nint btrfs_scrub_cancel(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->scrub_lock);\n\tif (!atomic_read(&fs_info->scrubs_running)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\treturn -ENOTCONN;\n\t}\n\n\tatomic_inc(&fs_info->scrub_cancel_req);\n\twhile (atomic_read(&fs_info->scrubs_running)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t\t   atomic_read(&fs_info->scrubs_running) == 0);\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n\tatomic_dec(&fs_info->scrub_cancel_req);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\treturn 0;\n}\n\nint btrfs_scrub_cancel_dev(struct btrfs_fs_info *fs_info,\n\t\t\t   struct btrfs_device *dev)\n{\n\tstruct scrub_ctx *sctx;\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tsctx = dev->scrub_ctx;\n\tif (!sctx) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\treturn -ENOTCONN;\n\t}\n\tatomic_inc(&sctx->cancel_req);\n\twhile (dev->scrub_ctx) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t\t   dev->scrub_ctx == NULL);\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\treturn 0;\n}\n\nint btrfs_scrub_progress(struct btrfs_fs_info *fs_info, u64 devid,\n\t\t\t struct btrfs_scrub_progress *progress)\n{\n\tstruct btrfs_device *dev;\n\tstruct scrub_ctx *sctx = NULL;\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (dev)\n\t\tsctx = dev->scrub_ctx;\n\tif (sctx)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\treturn dev ? (sctx ? 0 : -ENOTCONN) : -ENODEV;\n}\n\nstatic void scrub_remap_extent(struct btrfs_fs_info *fs_info,\n\t\t\t       u64 extent_logical, u64 extent_len,\n\t\t\t       u64 *extent_physical,\n\t\t\t       struct btrfs_device **extent_dev,\n\t\t\t       int *extent_mirror_num)\n{\n\tu64 mapped_length;\n\tstruct btrfs_bio *bbio = NULL;\n\tint ret;\n\n\tmapped_length = extent_len;\n\tret = btrfs_map_block(fs_info, BTRFS_MAP_READ, extent_logical,\n\t\t\t      &mapped_length, &bbio, 0);\n\tif (ret || !bbio || mapped_length < extent_len ||\n\t    !bbio->stripes[0].dev->bdev) {\n\t\tbtrfs_put_bbio(bbio);\n\t\treturn;\n\t}\n\n\t*extent_physical = bbio->stripes[0].physical;\n\t*extent_mirror_num = bbio->mirror_num;\n\t*extent_dev = bbio->stripes[0].dev;\n\tbtrfs_put_bbio(bbio);\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n */\n\n#include <linux/sched.h>\n#include <linux/bio.h>\n#include <linux/slab.h>\n#include <linux/buffer_head.h>\n#include <linux/blkdev.h>\n#include <linux/ratelimit.h>\n#include <linux/kthread.h>\n#include <linux/raid/pq.h>\n#include <linux/semaphore.h>\n#include <linux/uuid.h>\n#include <linux/list_sort.h>\n#include \"ctree.h\"\n#include \"extent_map.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"raid56.h\"\n#include \"async-thread.h\"\n#include \"check-integrity.h\"\n#include \"rcu-string.h\"\n#include \"math.h\"\n#include \"dev-replace.h\"\n#include \"sysfs.h\"\n\nconst struct btrfs_raid_attr btrfs_raid_array[BTRFS_NR_RAID_TYPES] = {\n\t[BTRFS_RAID_RAID10] = {\n\t\t.sub_stripes\t= 2,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\t/* 0 == as many as possible */\n\t\t.devs_min\t= 4,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid10\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID10,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID10_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 2,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_DUP] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 2,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"dup\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_DUP,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID0] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid0\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_SINGLE] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"single\",\n\t\t.bg_flag\t= 0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID5] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 1,\n\t\t.raid_name\t= \"raid5\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID5,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID5_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID6] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 3,\n\t\t.tolerated_failures = 2,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 2,\n\t\t.raid_name\t= \"raid6\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID6,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID6_MIN_NOT_MET,\n\t},\n};\n\nconst char *get_raid_name(enum btrfs_raid_types type)\n{\n\tif (type >= BTRFS_NR_RAID_TYPES)\n\t\treturn NULL;\n\n\treturn btrfs_raid_array[type].raid_name;\n}\n\n/*\n * Fill @buf with textual description of @bg_flags, no more than @size_buf\n * bytes including terminating null byte.\n */\nvoid btrfs_describe_block_groups(u64 bg_flags, char *buf, u32 size_buf)\n{\n\tint i;\n\tint ret;\n\tchar *bp = buf;\n\tu64 flags = bg_flags;\n\tu32 size_bp = size_buf;\n\n\tif (!flags) {\n\t\tstrcpy(bp, \"NONE\");\n\t\treturn;\n\t}\n\n#define DESCRIBE_FLAG(flag, desc)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (flags & (flag)) {\t\t\t\t\t\\\n\t\t\tret = snprintf(bp, size_bp, \"%s|\", (desc));\t\\\n\t\t\tif (ret < 0 || ret >= size_bp)\t\t\t\\\n\t\t\t\tgoto out_overflow;\t\t\t\\\n\t\t\tsize_bp -= ret;\t\t\t\t\t\\\n\t\t\tbp += ret;\t\t\t\t\t\\\n\t\t\tflags &= ~(flag);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_DATA, \"data\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_SYSTEM, \"system\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_METADATA, \"metadata\");\n\n\tDESCRIBE_FLAG(BTRFS_AVAIL_ALLOC_BIT_SINGLE, \"single\");\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++)\n\t\tDESCRIBE_FLAG(btrfs_raid_array[i].bg_flag,\n\t\t\t      btrfs_raid_array[i].raid_name);\n#undef DESCRIBE_FLAG\n\n\tif (flags) {\n\t\tret = snprintf(bp, size_bp, \"0x%llx|\", flags);\n\t\tsize_bp -= ret;\n\t}\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last | */\n\n\t/*\n\t * The text is trimmed, it's up to the caller to provide sufficiently\n\t * large buffer\n\t */\nout_overflow:;\n}\n\nstatic int init_first_rw_device(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_fs_info *fs_info);\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info);\nstatic void __btrfs_reset_dev_stats(struct btrfs_device *dev);\nstatic void btrfs_dev_stat_print_on_error(struct btrfs_device *dev);\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *device);\nstatic int __btrfs_map_block(struct btrfs_fs_info *fs_info,\n\t\t\t     enum btrfs_map_op op,\n\t\t\t     u64 logical, u64 *length,\n\t\t\t     struct btrfs_bio **bbio_ret,\n\t\t\t     int mirror_num, int need_raid_map);\n\n/*\n * Device locking\n * ==============\n *\n * There are several mutexes that protect manipulation of devices and low-level\n * structures like chunks but not block groups, extents or files\n *\n * uuid_mutex (global lock)\n * ------------------------\n * protects the fs_uuids list that tracks all per-fs fs_devices, resulting from\n * the SCAN_DEV ioctl registration or from mount either implicitly (the first\n * device) or requested by the device= mount option\n *\n * the mutex can be very coarse and can cover long-running operations\n *\n * protects: updates to fs_devices counters like missing devices, rw devices,\n * seeding, structure cloning, opening/closing devices at mount/umount time\n *\n * global::fs_devs - add, remove, updates to the global list\n *\n * does not protect: manipulation of the fs_devices::devices list!\n *\n * btrfs_device::name - renames (write side), read is RCU\n *\n * fs_devices::device_list_mutex (per-fs, with RCU)\n * ------------------------------------------------\n * protects updates to fs_devices::devices, ie. adding and deleting\n *\n * simple list traversal with read-only actions can be done with RCU protection\n *\n * may be used to exclude some operations from running concurrently without any\n * modifications to the list (see write_all_supers)\n *\n * balance_mutex\n * -------------\n * protects balance structures (status, state) and context accessed from\n * several places (internally, ioctl)\n *\n * chunk_mutex\n * -----------\n * protects chunks, adding or removing during allocation, trim or when a new\n * device is added/removed\n *\n * cleaner_mutex\n * -------------\n * a big lock that is held by the cleaner thread and prevents running subvolume\n * cleaning together with relocation or delayed iputs\n *\n *\n * Lock nesting\n * ============\n *\n * uuid_mutex\n *   volume_mutex\n *     device_list_mutex\n *       chunk_mutex\n *     balance_mutex\n *\n *\n * Exclusive operations, BTRFS_FS_EXCL_OP\n * ======================================\n *\n * Maintains the exclusivity of the following operations that apply to the\n * whole filesystem and cannot run in parallel.\n *\n * - Balance (*)\n * - Device add\n * - Device remove\n * - Device replace (*)\n * - Resize\n *\n * The device operations (as above) can be in one of the following states:\n *\n * - Running state\n * - Paused state\n * - Completed state\n *\n * Only device operations marked with (*) can go into the Paused state for the\n * following reasons:\n *\n * - ioctl (only Balance can be Paused through ioctl)\n * - filesystem remounted as read-only\n * - filesystem unmounted and mounted as read-only\n * - system power-cycle and filesystem mounted as read-only\n * - filesystem or device errors leading to forced read-only\n *\n * BTRFS_FS_EXCL_OP flag is set and cleared using atomic operations.\n * During the course of Paused state, the BTRFS_FS_EXCL_OP remains set.\n * A device operation in Paused or Running state can be canceled or resumed\n * either by ioctl (Balance only) or when remounted as read-write.\n * BTRFS_FS_EXCL_OP flag is cleared when the device operation is canceled or\n * completed.\n */\n\nDEFINE_MUTEX(uuid_mutex);\nstatic LIST_HEAD(fs_uuids);\nstruct list_head *btrfs_get_fs_uuids(void)\n{\n\treturn &fs_uuids;\n}\n\n/*\n * alloc_fs_devices - allocate struct btrfs_fs_devices\n * @fsid:\t\tif not NULL, copy the UUID to fs_devices::fsid\n * @metadata_fsid:\tif not NULL, copy the UUID to fs_devices::metadata_fsid\n *\n * Return a pointer to a new struct btrfs_fs_devices on success, or ERR_PTR().\n * The returned struct is not linked onto any lists and can be destroyed with\n * kfree() right away.\n */\nstatic struct btrfs_fs_devices *alloc_fs_devices(const u8 *fsid,\n\t\t\t\t\t\t const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devs;\n\n\tfs_devs = kzalloc(sizeof(*fs_devs), GFP_KERNEL);\n\tif (!fs_devs)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_init(&fs_devs->device_list_mutex);\n\n\tINIT_LIST_HEAD(&fs_devs->devices);\n\tINIT_LIST_HEAD(&fs_devs->resized_devices);\n\tINIT_LIST_HEAD(&fs_devs->alloc_list);\n\tINIT_LIST_HEAD(&fs_devs->fs_list);\n\tif (fsid)\n\t\tmemcpy(fs_devs->fsid, fsid, BTRFS_FSID_SIZE);\n\n\tif (metadata_fsid)\n\t\tmemcpy(fs_devs->metadata_uuid, metadata_fsid, BTRFS_FSID_SIZE);\n\telse if (fsid)\n\t\tmemcpy(fs_devs->metadata_uuid, fsid, BTRFS_FSID_SIZE);\n\n\treturn fs_devs;\n}\n\nvoid btrfs_free_device(struct btrfs_device *device)\n{\n\trcu_string_free(device->name);\n\tbio_put(device->flush_bio);\n\tkfree(device);\n}\n\nstatic void free_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device;\n\tWARN_ON(fs_devices->opened);\n\twhile (!list_empty(&fs_devices->devices)) {\n\t\tdevice = list_entry(fs_devices->devices.next,\n\t\t\t\t    struct btrfs_device, dev_list);\n\t\tlist_del(&device->dev_list);\n\t\tbtrfs_free_device(device);\n\t}\n\tkfree(fs_devices);\n}\n\nstatic void btrfs_kobject_uevent(struct block_device *bdev,\n\t\t\t\t enum kobject_action action)\n{\n\tint ret;\n\n\tret = kobject_uevent(&disk_to_dev(bdev->bd_disk)->kobj, action);\n\tif (ret)\n\t\tpr_warn(\"BTRFS: Sending event '%d' to kobject: '%s' (%p): failed\\n\",\n\t\t\taction,\n\t\t\tkobject_name(&disk_to_dev(bdev->bd_disk)->kobj),\n\t\t\t&disk_to_dev(bdev->bd_disk)->kobj);\n}\n\nvoid __exit btrfs_cleanup_fs_uuids(void)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\twhile (!list_empty(&fs_uuids)) {\n\t\tfs_devices = list_entry(fs_uuids.next,\n\t\t\t\t\tstruct btrfs_fs_devices, fs_list);\n\t\tlist_del(&fs_devices->fs_list);\n\t\tfree_fs_devices(fs_devices);\n\t}\n}\n\n/*\n * Returns a pointer to a new btrfs_device on success; ERR_PTR() on error.\n * Returned struct is not linked onto any lists and must be destroyed using\n * btrfs_free_device.\n */\nstatic struct btrfs_device *__alloc_device(void)\n{\n\tstruct btrfs_device *dev;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * Preallocate a bio that's always going to be used for flushing device\n\t * barriers and matches the device lifespan\n\t */\n\tdev->flush_bio = bio_alloc_bioset(GFP_KERNEL, 0, NULL);\n\tif (!dev->flush_bio) {\n\t\tkfree(dev);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tINIT_LIST_HEAD(&dev->dev_list);\n\tINIT_LIST_HEAD(&dev->dev_alloc_list);\n\tINIT_LIST_HEAD(&dev->resized_list);\n\n\tspin_lock_init(&dev->io_lock);\n\n\tatomic_set(&dev->reada_in_flight, 0);\n\tatomic_set(&dev->dev_stats_ccnt, 0);\n\tbtrfs_device_data_ordered_init(dev);\n\tINIT_RADIX_TREE(&dev->reada_zones, GFP_NOFS & ~__GFP_DIRECT_RECLAIM);\n\tINIT_RADIX_TREE(&dev->reada_extents, GFP_NOFS & ~__GFP_DIRECT_RECLAIM);\n\n\treturn dev;\n}\n\nstatic noinline struct btrfs_fs_devices *find_fsid(\n\t\tconst u8 *fsid, const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tASSERT(fsid);\n\n\tif (metadata_fsid) {\n\t\t/*\n\t\t * Handle scanned device having completed its fsid change but\n\t\t * belonging to a fs_devices that was created by first scanning\n\t\t * a device which didn't have its fsid/metadata_uuid changed\n\t\t * at all and the CHANGING_FSID_V2 flag set.\n\t\t */\n\t\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\t\tif (fs_devices->fsid_change &&\n\t\t\t    memcmp(metadata_fsid, fs_devices->fsid,\n\t\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t\t    memcmp(fs_devices->fsid, fs_devices->metadata_uuid,\n\t\t\t\t   BTRFS_FSID_SIZE) == 0) {\n\t\t\t\treturn fs_devices;\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Handle scanned device having completed its fsid change but\n\t\t * belonging to a fs_devices that was created by a device that\n\t\t * has an outdated pair of fsid/metadata_uuid and\n\t\t * CHANGING_FSID_V2 flag set.\n\t\t */\n\t\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\t\tif (fs_devices->fsid_change &&\n\t\t\t    memcmp(fs_devices->metadata_uuid,\n\t\t\t\t   fs_devices->fsid, BTRFS_FSID_SIZE) != 0 &&\n\t\t\t    memcmp(metadata_fsid, fs_devices->metadata_uuid,\n\t\t\t\t   BTRFS_FSID_SIZE) == 0) {\n\t\t\t\treturn fs_devices;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Handle non-split brain cases */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (metadata_fsid) {\n\t\t\tif (memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE) == 0\n\t\t\t    && memcmp(metadata_fsid, fs_devices->metadata_uuid,\n\t\t\t\t      BTRFS_FSID_SIZE) == 0)\n\t\t\t\treturn fs_devices;\n\t\t} else {\n\t\t\tif (memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE) == 0)\n\t\t\t\treturn fs_devices;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic int\nbtrfs_get_bdev_and_sb(const char *device_path, fmode_t flags, void *holder,\n\t\t      int flush, struct block_device **bdev,\n\t\t      struct buffer_head **bh)\n{\n\tint ret;\n\n\t*bdev = blkdev_get_by_path(device_path, flags, holder);\n\n\tif (IS_ERR(*bdev)) {\n\t\tret = PTR_ERR(*bdev);\n\t\tgoto error;\n\t}\n\n\tif (flush)\n\t\tfilemap_write_and_wait((*bdev)->bd_inode->i_mapping);\n\tret = set_blocksize(*bdev, BTRFS_BDEV_BLOCKSIZE);\n\tif (ret) {\n\t\tblkdev_put(*bdev, flags);\n\t\tgoto error;\n\t}\n\tinvalidate_bdev(*bdev);\n\t*bh = btrfs_read_dev_super(*bdev);\n\tif (IS_ERR(*bh)) {\n\t\tret = PTR_ERR(*bh);\n\t\tblkdev_put(*bdev, flags);\n\t\tgoto error;\n\t}\n\n\treturn 0;\n\nerror:\n\t*bdev = NULL;\n\t*bh = NULL;\n\treturn ret;\n}\n\nstatic void requeue_list(struct btrfs_pending_bios *pending_bios,\n\t\t\tstruct bio *head, struct bio *tail)\n{\n\n\tstruct bio *old_head;\n\n\told_head = pending_bios->head;\n\tpending_bios->head = head;\n\tif (pending_bios->tail)\n\t\ttail->bi_next = old_head;\n\telse\n\t\tpending_bios->tail = tail;\n}\n\n/*\n * we try to collect pending bios for a device so we don't get a large\n * number of procs sending bios down to the same device.  This greatly\n * improves the schedulers ability to collect and merge the bios.\n *\n * But, it also turns into a long list of bios to process and that is sure\n * to eventually make the worker thread block.  The solution here is to\n * make some progress and then put this work struct back at the end of\n * the list if the block device is congested.  This way, multiple devices\n * can make progress from a single worker thread.\n */\nstatic noinline void run_scheduled_bios(struct btrfs_device *device)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct bio *pending;\n\tstruct backing_dev_info *bdi;\n\tstruct btrfs_pending_bios *pending_bios;\n\tstruct bio *tail;\n\tstruct bio *cur;\n\tint again = 0;\n\tunsigned long num_run;\n\tunsigned long batch_run = 0;\n\tunsigned long last_waited = 0;\n\tint force_reg = 0;\n\tint sync_pending = 0;\n\tstruct blk_plug plug;\n\n\t/*\n\t * this function runs all the bios we've collected for\n\t * a particular device.  We don't want to wander off to\n\t * another device without first sending all of these down.\n\t * So, setup a plug here and finish it off before we return\n\t */\n\tblk_start_plug(&plug);\n\n\tbdi = device->bdev->bd_bdi;\n\nloop:\n\tspin_lock(&device->io_lock);\n\nloop_lock:\n\tnum_run = 0;\n\n\t/* take all the bios off the list at once and process them\n\t * later on (without the lock held).  But, remember the\n\t * tail and other pointers so the bios can be properly reinserted\n\t * into the list if we hit congestion\n\t */\n\tif (!force_reg && device->pending_sync_bios.head) {\n\t\tpending_bios = &device->pending_sync_bios;\n\t\tforce_reg = 1;\n\t} else {\n\t\tpending_bios = &device->pending_bios;\n\t\tforce_reg = 0;\n\t}\n\n\tpending = pending_bios->head;\n\ttail = pending_bios->tail;\n\tWARN_ON(pending && !tail);\n\n\t/*\n\t * if pending was null this time around, no bios need processing\n\t * at all and we can stop.  Otherwise it'll loop back up again\n\t * and do an additional check so no bios are missed.\n\t *\n\t * device->running_pending is used to synchronize with the\n\t * schedule_bio code.\n\t */\n\tif (device->pending_sync_bios.head == NULL &&\n\t    device->pending_bios.head == NULL) {\n\t\tagain = 0;\n\t\tdevice->running_pending = 0;\n\t} else {\n\t\tagain = 1;\n\t\tdevice->running_pending = 1;\n\t}\n\n\tpending_bios->head = NULL;\n\tpending_bios->tail = NULL;\n\n\tspin_unlock(&device->io_lock);\n\n\twhile (pending) {\n\n\t\trmb();\n\t\t/* we want to work on both lists, but do more bios on the\n\t\t * sync list than the regular list\n\t\t */\n\t\tif ((num_run > 32 &&\n\t\t    pending_bios != &device->pending_sync_bios &&\n\t\t    device->pending_sync_bios.head) ||\n\t\t   (num_run > 64 && pending_bios == &device->pending_sync_bios &&\n\t\t    device->pending_bios.head)) {\n\t\t\tspin_lock(&device->io_lock);\n\t\t\trequeue_list(pending_bios, pending, tail);\n\t\t\tgoto loop_lock;\n\t\t}\n\n\t\tcur = pending;\n\t\tpending = pending->bi_next;\n\t\tcur->bi_next = NULL;\n\n\t\tBUG_ON(atomic_read(&cur->__bi_cnt) == 0);\n\n\t\t/*\n\t\t * if we're doing the sync list, record that our\n\t\t * plug has some sync requests on it\n\t\t *\n\t\t * If we're doing the regular list and there are\n\t\t * sync requests sitting around, unplug before\n\t\t * we add more\n\t\t */\n\t\tif (pending_bios == &device->pending_sync_bios) {\n\t\t\tsync_pending = 1;\n\t\t} else if (sync_pending) {\n\t\t\tblk_finish_plug(&plug);\n\t\t\tblk_start_plug(&plug);\n\t\t\tsync_pending = 0;\n\t\t}\n\n\t\tbtrfsic_submit_bio(cur);\n\t\tnum_run++;\n\t\tbatch_run++;\n\n\t\tcond_resched();\n\n\t\t/*\n\t\t * we made progress, there is more work to do and the bdi\n\t\t * is now congested.  Back off and let other work structs\n\t\t * run instead\n\t\t */\n\t\tif (pending && bdi_write_congested(bdi) && batch_run > 8 &&\n\t\t    fs_info->fs_devices->open_devices > 1) {\n\t\t\tstruct io_context *ioc;\n\n\t\t\tioc = current->io_context;\n\n\t\t\t/*\n\t\t\t * the main goal here is that we don't want to\n\t\t\t * block if we're going to be able to submit\n\t\t\t * more requests without blocking.\n\t\t\t *\n\t\t\t * This code does two great things, it pokes into\n\t\t\t * the elevator code from a filesystem _and_\n\t\t\t * it makes assumptions about how batching works.\n\t\t\t */\n\t\t\tif (ioc && ioc->nr_batch_requests > 0 &&\n\t\t\t    time_before(jiffies, ioc->last_waited + HZ/50UL) &&\n\t\t\t    (last_waited == 0 ||\n\t\t\t     ioc->last_waited == last_waited)) {\n\t\t\t\t/*\n\t\t\t\t * we want to go through our batch of\n\t\t\t\t * requests and stop.  So, we copy out\n\t\t\t\t * the ioc->last_waited time and test\n\t\t\t\t * against it before looping\n\t\t\t\t */\n\t\t\t\tlast_waited = ioc->last_waited;\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tspin_lock(&device->io_lock);\n\t\t\trequeue_list(pending_bios, pending, tail);\n\t\t\tdevice->running_pending = 1;\n\n\t\t\tspin_unlock(&device->io_lock);\n\t\t\tbtrfs_queue_work(fs_info->submit_workers,\n\t\t\t\t\t &device->work);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tcond_resched();\n\tif (again)\n\t\tgoto loop;\n\n\tspin_lock(&device->io_lock);\n\tif (device->pending_bios.head || device->pending_sync_bios.head)\n\t\tgoto loop_lock;\n\tspin_unlock(&device->io_lock);\n\ndone:\n\tblk_finish_plug(&plug);\n}\n\nstatic void pending_bios_fn(struct btrfs_work *work)\n{\n\tstruct btrfs_device *device;\n\n\tdevice = container_of(work, struct btrfs_device, work);\n\trun_scheduled_bios(device);\n}\n\nstatic bool device_path_matched(const char *path, struct btrfs_device *device)\n{\n\tint found;\n\n\trcu_read_lock();\n\tfound = strcmp(rcu_str_deref(device->name), path);\n\trcu_read_unlock();\n\n\treturn found == 0;\n}\n\n/*\n *  Search and remove all stale (devices which are not mounted) devices.\n *  When both inputs are NULL, it will search and release all stale devices.\n *  path:\tOptional. When provided will it release all unmounted devices\n *\t\tmatching this path only.\n *  skip_dev:\tOptional. Will skip this device when searching for the stale\n *\t\tdevices.\n *  Return:\t0 for success or if @path is NULL.\n * \t\t-EBUSY if @path is a mounted device.\n * \t\t-ENOENT if @path does not match any device in the list.\n */\nstatic int btrfs_free_stale_devices(const char *path,\n\t\t\t\t     struct btrfs_device *skip_device)\n{\n\tstruct btrfs_fs_devices *fs_devices, *tmp_fs_devices;\n\tstruct btrfs_device *device, *tmp_device;\n\tint ret = 0;\n\n\tif (path)\n\t\tret = -ENOENT;\n\n\tlist_for_each_entry_safe(fs_devices, tmp_fs_devices, &fs_uuids, fs_list) {\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_for_each_entry_safe(device, tmp_device,\n\t\t\t\t\t &fs_devices->devices, dev_list) {\n\t\t\tif (skip_device && skip_device == device)\n\t\t\t\tcontinue;\n\t\t\tif (path && !device->name)\n\t\t\t\tcontinue;\n\t\t\tif (path && !device_path_matched(path, device))\n\t\t\t\tcontinue;\n\t\t\tif (fs_devices->opened) {\n\t\t\t\t/* for an already deleted device return 0 */\n\t\t\t\tif (path && ret != 0)\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* delete the stale device */\n\t\t\tfs_devices->num_devices--;\n\t\t\tlist_del(&device->dev_list);\n\t\t\tbtrfs_free_device(device);\n\n\t\t\tret = 0;\n\t\t\tif (fs_devices->num_devices == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t\tif (fs_devices->num_devices == 0) {\n\t\t\tbtrfs_sysfs_remove_fsid(fs_devices);\n\t\t\tlist_del(&fs_devices->fs_list);\n\t\t\tfree_fs_devices(fs_devices);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int btrfs_open_one_device(struct btrfs_fs_devices *fs_devices,\n\t\t\tstruct btrfs_device *device, fmode_t flags,\n\t\t\tvoid *holder)\n{\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tstruct buffer_head *bh;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tint ret;\n\n\tif (device->bdev)\n\t\treturn -EINVAL;\n\tif (!device->name)\n\t\treturn -EINVAL;\n\n\tret = btrfs_get_bdev_and_sb(device->name->str, flags, holder, 1,\n\t\t\t\t    &bdev, &bh);\n\tif (ret)\n\t\treturn ret;\n\n\tdisk_super = (struct btrfs_super_block *)bh->b_data;\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tif (devid != device->devid)\n\t\tgoto error_brelse;\n\n\tif (memcmp(device->uuid, disk_super->dev_item.uuid, BTRFS_UUID_SIZE))\n\t\tgoto error_brelse;\n\n\tdevice->generation = btrfs_super_generation(disk_super);\n\n\tif (btrfs_super_flags(disk_super) & BTRFS_SUPER_FLAG_SEEDING) {\n\t\tif (btrfs_super_incompat_flags(disk_super) &\n\t\t    BTRFS_FEATURE_INCOMPAT_METADATA_UUID) {\n\t\t\tpr_err(\n\t\t\"BTRFS: Invalid seeding and uuid-changed device detected\\n\");\n\t\t\tgoto error_brelse;\n\t\t}\n\n\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\tfs_devices->seeding = 1;\n\t} else {\n\t\tif (bdev_read_only(bdev))\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\telse\n\t\t\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t}\n\n\tq = bdev_get_queue(bdev);\n\tif (!blk_queue_nonrot(q))\n\t\tfs_devices->rotating = 1;\n\n\tdevice->bdev = bdev;\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tdevice->mode = flags;\n\n\tfs_devices->open_devices++;\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tfs_devices->rw_devices++;\n\t\tlist_add_tail(&device->dev_alloc_list, &fs_devices->alloc_list);\n\t}\n\tbrelse(bh);\n\n\treturn 0;\n\nerror_brelse:\n\tbrelse(bh);\n\tblkdev_put(bdev, flags);\n\n\treturn -EINVAL;\n}\n\n/*\n * Handle scanned device having its CHANGING_FSID_V2 flag set and the fs_devices\n * being created with a disk that has already completed its fsid change.\n */\nstatic struct btrfs_fs_devices *find_fsid_inprogress(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 && !fs_devices->fsid_change) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n\nstatic struct btrfs_fs_devices *find_fsid_changed(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * Handles the case where scanned device is part of an fs that had\n\t * multiple successful changes of FSID but curently device didn't\n\t * observe it. Meaning our fsid will be different than theirs.\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    memcmp(fs_devices->fsid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n/*\n * Add new device to list of registered devices\n *\n * Returns:\n * device pointer which was just added or updated when successful\n * error pointer when failed\n */\nstatic noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL, false);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}\n\nstatic struct btrfs_fs_devices *clone_fs_devices(struct btrfs_fs_devices *orig)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *orig_dev;\n\n\tfs_devices = alloc_fs_devices(orig->fsid, NULL);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tmutex_lock(&orig->device_list_mutex);\n\tfs_devices->total_devices = orig->total_devices;\n\n\t/* We have held the volume lock, it is safe to get the devices. */\n\tlist_for_each_entry(orig_dev, &orig->devices, dev_list) {\n\t\tstruct rcu_string *name;\n\n\t\tdevice = btrfs_alloc_device(NULL, &orig_dev->devid,\n\t\t\t\t\t    orig_dev->uuid);\n\t\tif (IS_ERR(device))\n\t\t\tgoto error;\n\n\t\t/*\n\t\t * This is ok to do without rcu read locked because we hold the\n\t\t * uuid mutex so nothing we touch in here is going to disappear.\n\t\t */\n\t\tif (orig_dev->name) {\n\t\t\tname = rcu_string_strdup(orig_dev->name->str,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!name) {\n\t\t\t\tbtrfs_free_device(device);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\trcu_assign_pointer(device->name, name);\n\t\t}\n\n\t\tlist_add(&device->dev_list, &fs_devices->devices);\n\t\tdevice->fs_devices = fs_devices;\n\t\tfs_devices->num_devices++;\n\t}\n\tmutex_unlock(&orig->device_list_mutex);\n\treturn fs_devices;\nerror:\n\tmutex_unlock(&orig->device_list_mutex);\n\tfree_fs_devices(fs_devices);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/*\n * After we have read the system tree and know devids belonging to\n * this filesystem, remove the device which does not belong there.\n */\nvoid btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices, int step)\n{\n\tstruct btrfs_device *device, *next;\n\tstruct btrfs_device *latest_dev = NULL;\n\n\tmutex_lock(&uuid_mutex);\nagain:\n\t/* This is the initialized path, it is safe to release the devices. */\n\tlist_for_each_entry_safe(device, next, &fs_devices->devices, dev_list) {\n\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t\t\t&device->dev_state)) {\n\t\t\tif (!test_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t     &device->dev_state) &&\n\t\t\t     (!latest_dev ||\n\t\t\t      device->generation > latest_dev->generation)) {\n\t\t\t\tlatest_dev = device;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (device->devid == BTRFS_DEV_REPLACE_DEVID) {\n\t\t\t/*\n\t\t\t * In the first step, keep the device which has\n\t\t\t * the correct fsid and the devid that is used\n\t\t\t * for the dev_replace procedure.\n\t\t\t * In the second step, the dev_replace state is\n\t\t\t * read from the device tree and it is known\n\t\t\t * whether the procedure is really active or\n\t\t\t * not, which means whether this device is\n\t\t\t * used or whether it should be removed.\n\t\t\t */\n\t\t\tif (step == 0 || test_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t\t\t  &device->dev_state)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (device->bdev) {\n\t\t\tblkdev_put(device->bdev, device->mode);\n\t\t\tdevice->bdev = NULL;\n\t\t\tfs_devices->open_devices--;\n\t\t}\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tlist_del_init(&device->dev_alloc_list);\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\t\tif (!test_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t      &device->dev_state))\n\t\t\t\tfs_devices->rw_devices--;\n\t\t}\n\t\tlist_del_init(&device->dev_list);\n\t\tfs_devices->num_devices--;\n\t\tbtrfs_free_device(device);\n\t}\n\n\tif (fs_devices->seed) {\n\t\tfs_devices = fs_devices->seed;\n\t\tgoto again;\n\t}\n\n\tfs_devices->latest_bdev = latest_dev->bdev;\n\n\tmutex_unlock(&uuid_mutex);\n}\n\nstatic void free_device_rcu(struct rcu_head *head)\n{\n\tstruct btrfs_device *device;\n\n\tdevice = container_of(head, struct btrfs_device, rcu);\n\tbtrfs_free_device(device);\n}\n\nstatic void btrfs_close_bdev(struct btrfs_device *device)\n{\n\tif (!device->bdev)\n\t\treturn;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tsync_blockdev(device->bdev);\n\t\tinvalidate_bdev(device->bdev);\n\t}\n\n\tblkdev_put(device->bdev, device->mode);\n}\n\nstatic void btrfs_close_one_device(struct btrfs_device *device)\n{\n\tstruct btrfs_fs_devices *fs_devices = device->fs_devices;\n\tstruct btrfs_device *new_device;\n\tstruct rcu_string *name;\n\n\tif (device->bdev)\n\t\tfs_devices->open_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tfs_devices->rw_devices--;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tfs_devices->missing_devices--;\n\n\tbtrfs_close_bdev(device);\n\n\tnew_device = btrfs_alloc_device(NULL, &device->devid,\n\t\t\t\t\tdevice->uuid);\n\tBUG_ON(IS_ERR(new_device)); /* -ENOMEM */\n\n\t/* Safe because we are under uuid_mutex */\n\tif (device->name) {\n\t\tname = rcu_string_strdup(device->name->str, GFP_NOFS);\n\t\tBUG_ON(!name); /* -ENOMEM */\n\t\trcu_assign_pointer(new_device->name, name);\n\t}\n\n\tlist_replace_rcu(&device->dev_list, &new_device->dev_list);\n\tnew_device->fs_devices = device->fs_devices;\n\n\tcall_rcu(&device->rcu, free_device_rcu);\n}\n\nstatic int close_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device, *tmp;\n\n\tif (--fs_devices->opened > 0)\n\t\treturn 0;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry_safe(device, tmp, &fs_devices->devices, dev_list) {\n\t\tbtrfs_close_one_device(device);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tWARN_ON(fs_devices->open_devices);\n\tWARN_ON(fs_devices->rw_devices);\n\tfs_devices->opened = 0;\n\tfs_devices->seeding = 0;\n\n\treturn 0;\n}\n\nint btrfs_close_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_fs_devices *seed_devices = NULL;\n\tint ret;\n\n\tmutex_lock(&uuid_mutex);\n\tret = close_fs_devices(fs_devices);\n\tif (!fs_devices->opened) {\n\t\tseed_devices = fs_devices->seed;\n\t\tfs_devices->seed = NULL;\n\t}\n\tmutex_unlock(&uuid_mutex);\n\n\twhile (seed_devices) {\n\t\tfs_devices = seed_devices;\n\t\tseed_devices = fs_devices->seed;\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t}\n\treturn ret;\n}\n\nstatic int open_fs_devices(struct btrfs_fs_devices *fs_devices,\n\t\t\t\tfmode_t flags, void *holder)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *latest_dev = NULL;\n\tint ret = 0;\n\n\tflags |= FMODE_EXCL;\n\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\t/* Just open everything we can; ignore failures here */\n\t\tif (btrfs_open_one_device(fs_devices, device, flags, holder))\n\t\t\tcontinue;\n\n\t\tif (!latest_dev ||\n\t\t    device->generation > latest_dev->generation)\n\t\t\tlatest_dev = device;\n\t}\n\tif (fs_devices->open_devices == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tfs_devices->opened = 1;\n\tfs_devices->latest_bdev = latest_dev->bdev;\n\tfs_devices->total_rw_bytes = 0;\nout:\n\treturn ret;\n}\n\nstatic int devid_cmp(void *priv, struct list_head *a, struct list_head *b)\n{\n\tstruct btrfs_device *dev1, *dev2;\n\n\tdev1 = list_entry(a, struct btrfs_device, dev_list);\n\tdev2 = list_entry(b, struct btrfs_device, dev_list);\n\n\tif (dev1->devid < dev2->devid)\n\t\treturn -1;\n\telse if (dev1->devid > dev2->devid)\n\t\treturn 1;\n\treturn 0;\n}\n\nint btrfs_open_devices(struct btrfs_fs_devices *fs_devices,\n\t\t       fmode_t flags, void *holder)\n{\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tif (fs_devices->opened) {\n\t\tfs_devices->opened++;\n\t\tret = 0;\n\t} else {\n\t\tlist_sort(NULL, &fs_devices->devices, devid_cmp);\n\t\tret = open_fs_devices(fs_devices, flags, holder);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\treturn ret;\n}\n\nstatic void btrfs_release_disk_super(struct page *page)\n{\n\tkunmap(page);\n\tput_page(page);\n}\n\nstatic int btrfs_read_disk_super(struct block_device *bdev, u64 bytenr,\n\t\t\t\t struct page **page,\n\t\t\t\t struct btrfs_super_block **disk_super)\n{\n\tvoid *p;\n\tpgoff_t index;\n\n\t/* make sure our super fits in the device */\n\tif (bytenr + PAGE_SIZE >= i_size_read(bdev->bd_inode))\n\t\treturn 1;\n\n\t/* make sure our super fits in the page */\n\tif (sizeof(**disk_super) > PAGE_SIZE)\n\t\treturn 1;\n\n\t/* make sure our super doesn't straddle pages on disk */\n\tindex = bytenr >> PAGE_SHIFT;\n\tif ((bytenr + sizeof(**disk_super) - 1) >> PAGE_SHIFT != index)\n\t\treturn 1;\n\n\t/* pull in the page with our super */\n\t*page = read_cache_page_gfp(bdev->bd_inode->i_mapping,\n\t\t\t\t   index, GFP_KERNEL);\n\n\tif (IS_ERR_OR_NULL(*page))\n\t\treturn 1;\n\n\tp = kmap(*page);\n\n\t/* align our pointer to the offset of the super block */\n\t*disk_super = p + offset_in_page(bytenr);\n\n\tif (btrfs_super_bytenr(*disk_super) != bytenr ||\n\t    btrfs_super_magic(*disk_super) != BTRFS_MAGIC) {\n\t\tbtrfs_release_disk_super(*page);\n\t\treturn 1;\n\t}\n\n\tif ((*disk_super)->label[0] &&\n\t\t(*disk_super)->label[BTRFS_LABEL_SIZE - 1])\n\t\t(*disk_super)->label[BTRFS_LABEL_SIZE - 1] = '\\0';\n\n\treturn 0;\n}\n\n/*\n * Look for a btrfs signature on a device. This may be called out of the mount path\n * and we are not allowed to call set_blocksize during the scan. The superblock\n * is read via pagecache\n */\nstruct btrfs_device *btrfs_scan_one_device(const char *path, fmode_t flags,\n\t\t\t\t\t   void *holder)\n{\n\tstruct btrfs_super_block *disk_super;\n\tbool new_device_added = false;\n\tstruct btrfs_device *device = NULL;\n\tstruct block_device *bdev;\n\tstruct page *page;\n\tu64 bytenr;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\t/*\n\t * we would like to check all the supers, but that would make\n\t * a btrfs mount succeed after a mkfs from a different FS.\n\t * So, we need to add a special mount option to scan for\n\t * later supers, using BTRFS_SUPER_MIRROR_MAX instead\n\t */\n\tbytenr = btrfs_sb_offset(0);\n\tflags |= FMODE_EXCL;\n\n\tbdev = blkdev_get_by_path(path, flags, holder);\n\tif (IS_ERR(bdev))\n\t\treturn ERR_CAST(bdev);\n\n\tif (btrfs_read_disk_super(bdev, bytenr, &page, &disk_super)) {\n\t\tdevice = ERR_PTR(-EINVAL);\n\t\tgoto error_bdev_put;\n\t}\n\n\tdevice = device_list_add(path, disk_super, &new_device_added);\n\tif (!IS_ERR(device)) {\n\t\tif (new_device_added)\n\t\t\tbtrfs_free_stale_devices(path, device);\n\t}\n\n\tbtrfs_release_disk_super(page);\n\nerror_bdev_put:\n\tblkdev_put(bdev, flags);\n\n\treturn device;\n}\n\nstatic int contains_pending_extent(struct btrfs_transaction *transaction,\n\t\t\t\t   struct btrfs_device *device,\n\t\t\t\t   u64 *start, u64 len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct extent_map *em;\n\tstruct list_head *search_list = &fs_info->pinned_chunks;\n\tint ret = 0;\n\tu64 physical_start = *start;\n\n\tif (transaction)\n\t\tsearch_list = &transaction->pending_chunks;\nagain:\n\tlist_for_each_entry(em, search_list, list) {\n\t\tstruct map_lookup *map;\n\t\tint i;\n\n\t\tmap = em->map_lookup;\n\t\tfor (i = 0; i < map->num_stripes; i++) {\n\t\t\tu64 end;\n\n\t\t\tif (map->stripes[i].dev != device)\n\t\t\t\tcontinue;\n\t\t\tif (map->stripes[i].physical >= physical_start + len ||\n\t\t\t    map->stripes[i].physical + em->orig_block_len <=\n\t\t\t    physical_start)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Make sure that while processing the pinned list we do\n\t\t\t * not override our *start with a lower value, because\n\t\t\t * we can have pinned chunks that fall within this\n\t\t\t * device hole and that have lower physical addresses\n\t\t\t * than the pending chunks we processed before. If we\n\t\t\t * do not take this special care we can end up getting\n\t\t\t * 2 pending chunks that start at the same physical\n\t\t\t * device offsets because the end offset of a pinned\n\t\t\t * chunk can be equal to the start offset of some\n\t\t\t * pending chunk.\n\t\t\t */\n\t\t\tend = map->stripes[i].physical + em->orig_block_len;\n\t\t\tif (end > *start) {\n\t\t\t\t*start = end;\n\t\t\t\tret = 1;\n\t\t\t}\n\t\t}\n\t}\n\tif (search_list != &fs_info->pinned_chunks) {\n\t\tsearch_list = &fs_info->pinned_chunks;\n\t\tgoto again;\n\t}\n\n\treturn ret;\n}\n\n\n/*\n * find_free_dev_extent_start - find free space in the specified device\n * @device:\t  the device which we search the free space in\n * @num_bytes:\t  the size of the free space that we need\n * @search_start: the position from which to begin the search\n * @start:\t  store the start of the free space.\n * @len:\t  the size of the free space. that we find, or the size\n *\t\t  of the max free space if we don't find suitable free space\n *\n * this uses a pretty simple search, the expectation is that it is\n * called very infrequently and that a given device has a small number\n * of extents\n *\n * @start is used to store the start of the free space if we find. But if we\n * don't find suitable free space, it will be used to store the start position\n * of the max free space.\n *\n * @len is used to store the size of the free space that we find.\n * But if we don't find suitable free space, it is used to store the size of\n * the max free space.\n */\nint find_free_dev_extent_start(struct btrfs_transaction *transaction,\n\t\t\t       struct btrfs_device *device, u64 num_bytes,\n\t\t\t       u64 search_start, u64 *start, u64 *len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_dev_extent *dev_extent;\n\tstruct btrfs_path *path;\n\tu64 hole_size;\n\tu64 max_hole_start;\n\tu64 max_hole_size;\n\tu64 extent_end;\n\tu64 search_end = device->total_bytes;\n\tint ret;\n\tint slot;\n\tstruct extent_buffer *l;\n\n\t/*\n\t * We don't want to overwrite the superblock on the drive nor any area\n\t * used by the boot loader (grub for example), so we make sure to start\n\t * at an offset of at least 1MB.\n\t */\n\tsearch_start = max_t(u64, search_start, SZ_1M);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmax_hole_start = search_start;\n\tmax_hole_size = 0;\n\nagain:\n\tif (search_start >= search_end ||\n\t\ttest_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tpath->reada = READA_FORWARD;\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tkey.objectid = device->devid;\n\tkey.offset = search_start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) {\n\t\tret = btrfs_previous_item(root, path, key.objectid, key.type);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\twhile (1) {\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(l)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\tif (key.objectid < device->devid)\n\t\t\tgoto next;\n\n\t\tif (key.objectid > device->devid)\n\t\t\tbreak;\n\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tgoto next;\n\n\t\tif (key.offset > search_start) {\n\t\t\thole_size = key.offset - search_start;\n\n\t\t\t/*\n\t\t\t * Have to check before we set max_hole_start, otherwise\n\t\t\t * we could end up sending back this offset anyway.\n\t\t\t */\n\t\t\tif (contains_pending_extent(transaction, device,\n\t\t\t\t\t\t    &search_start,\n\t\t\t\t\t\t    hole_size)) {\n\t\t\t\tif (key.offset >= search_start) {\n\t\t\t\t\thole_size = key.offset - search_start;\n\t\t\t\t} else {\n\t\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\t\thole_size = 0;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (hole_size > max_hole_size) {\n\t\t\t\tmax_hole_start = search_start;\n\t\t\t\tmax_hole_size = hole_size;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If this free space is greater than which we need,\n\t\t\t * it must be the max free space that we have found\n\t\t\t * until now, so max_hole_start must point to the start\n\t\t\t * of this free space and the length of this free space\n\t\t\t * is stored in max_hole_size. Thus, we return\n\t\t\t * max_hole_start and max_hole_size and go back to the\n\t\t\t * caller.\n\t\t\t */\n\t\t\tif (hole_size >= num_bytes) {\n\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\textent_end = key.offset + btrfs_dev_extent_length(l,\n\t\t\t\t\t\t\t\t  dev_extent);\n\t\tif (extent_end > search_start)\n\t\t\tsearch_start = extent_end;\nnext:\n\t\tpath->slots[0]++;\n\t\tcond_resched();\n\t}\n\n\t/*\n\t * At this point, search_start should be the end of\n\t * allocated dev extents, and when shrinking the device,\n\t * search_end may be smaller than search_start.\n\t */\n\tif (search_end > search_start) {\n\t\thole_size = search_end - search_start;\n\n\t\tif (contains_pending_extent(transaction, device, &search_start,\n\t\t\t\t\t    hole_size)) {\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (hole_size > max_hole_size) {\n\t\t\tmax_hole_start = search_start;\n\t\t\tmax_hole_size = hole_size;\n\t\t}\n\t}\n\n\t/* See above. */\n\tif (max_hole_size < num_bytes)\n\t\tret = -ENOSPC;\n\telse\n\t\tret = 0;\n\nout:\n\tbtrfs_free_path(path);\n\t*start = max_hole_start;\n\tif (len)\n\t\t*len = max_hole_size;\n\treturn ret;\n}\n\nint find_free_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_device *device, u64 num_bytes,\n\t\t\t u64 *start, u64 *len)\n{\n\t/* FIXME use last free of some kind */\n\treturn find_free_dev_extent_start(trans->transaction, device,\n\t\t\t\t\t  num_bytes, 0, start, len);\n}\n\nstatic int btrfs_free_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_device *device,\n\t\t\t  u64 start, u64 *dev_extent_len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf = NULL;\n\tstruct btrfs_dev_extent *extent = NULL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = device->devid;\n\tkey.offset = start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\nagain:\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret > 0) {\n\t\tret = btrfs_previous_item(root, path, key.objectid,\n\t\t\t\t\t  BTRFS_DEV_EXTENT_KEY);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t\tBUG_ON(found_key.offset > start || found_key.offset +\n\t\t       btrfs_dev_extent_length(leaf, extent) < start);\n\t\tkey = found_key;\n\t\tbtrfs_release_path(path);\n\t\tgoto again;\n\t} else if (ret == 0) {\n\t\tleaf = path->nodes[0];\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t} else {\n\t\tbtrfs_handle_fs_error(fs_info, ret, \"Slot search failed\");\n\t\tgoto out;\n\t}\n\n\t*dev_extent_len = btrfs_dev_extent_length(leaf, extent);\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret) {\n\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t      \"Failed to remove dev extent item\");\n\t} else {\n\t\tset_bit(BTRFS_TRANS_HAVE_FREE_BGS, &trans->transaction->flags);\n\t}\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_alloc_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct btrfs_device *device,\n\t\t\t\t  u64 chunk_offset, u64 start, u64 num_bytes)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_dev_extent *extent;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\n\tWARN_ON(!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state));\n\tWARN_ON(test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state));\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = device->devid;\n\tkey.offset = start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      sizeof(*extent));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\tstruct btrfs_dev_extent);\n\tbtrfs_set_dev_extent_chunk_tree(leaf, extent,\n\t\t\t\t\tBTRFS_CHUNK_TREE_OBJECTID);\n\tbtrfs_set_dev_extent_chunk_objectid(leaf, extent,\n\t\t\t\t\t    BTRFS_FIRST_CHUNK_TREE_OBJECTID);\n\tbtrfs_set_dev_extent_chunk_offset(leaf, extent, chunk_offset);\n\n\tbtrfs_set_dev_extent_length(leaf, extent, num_bytes);\n\tbtrfs_mark_buffer_dirty(leaf);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic u64 find_next_chunk(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *n;\n\tu64 ret = 0;\n\n\tem_tree = &fs_info->mapping_tree.map_tree;\n\tread_lock(&em_tree->lock);\n\tn = rb_last(&em_tree->map.rb_root);\n\tif (n) {\n\t\tem = rb_entry(n, struct extent_map, rb_node);\n\t\tret = em->start + em->len;\n\t}\n\tread_unlock(&em_tree->lock);\n\n\treturn ret;\n}\n\nstatic noinline int find_next_devid(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 *devid_ret)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_path *path;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\n\tret = btrfs_search_slot(NULL, fs_info->chunk_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto error;\n\n\tBUG_ON(ret == 0); /* Corruption */\n\n\tret = btrfs_previous_item(fs_info->chunk_root, path,\n\t\t\t\t  BTRFS_DEV_ITEMS_OBJECTID,\n\t\t\t\t  BTRFS_DEV_ITEM_KEY);\n\tif (ret) {\n\t\t*devid_ret = 1;\n\t} else {\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &found_key,\n\t\t\t\t      path->slots[0]);\n\t\t*devid_ret = found_key.offset + 1;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * the device information is stored in the chunk root\n * the btrfs_device struct should be fully filled in\n */\nstatic int btrfs_add_dev_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tunsigned long ptr;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_insert_empty_item(trans, trans->fs_info->chunk_root, path,\n\t\t\t\t      &key, sizeof(*dev_item));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_generation(leaf, dev_item, 0);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_set_device_group(leaf, dev_item, 0);\n\tbtrfs_set_device_seek_speed(leaf, dev_item, 0);\n\tbtrfs_set_device_bandwidth(leaf, dev_item, 0);\n\tbtrfs_set_device_start_offset(leaf, dev_item, 0);\n\n\tptr = btrfs_device_uuid(dev_item);\n\twrite_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n\tptr = btrfs_device_fsid(dev_item);\n\twrite_extent_buffer(leaf, trans->fs_info->fs_devices->metadata_uuid,\n\t\t\t    ptr, BTRFS_FSID_SIZE);\n\tbtrfs_mark_buffer_dirty(leaf);\n\n\tret = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Function to update ctime/mtime for a given device path.\n * Mainly used for ctime/mtime based probe like libblkid.\n */\nstatic void update_dev_time(const char *path_name)\n{\n\tstruct file *filp;\n\n\tfilp = filp_open(path_name, O_RDWR, 0);\n\tif (IS_ERR(filp))\n\t\treturn;\n\tfile_update_time(filp);\n\tfilp_close(filp, NULL);\n}\n\nstatic int btrfs_rm_dev_item(struct btrfs_fs_info *fs_info,\n\t\t\t     struct btrfs_device *device)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_trans_handle *trans;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret) {\n\t\tif (ret > 0)\n\t\t\tret = -ENOENT;\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tif (!ret)\n\t\tret = btrfs_commit_transaction(trans);\n\treturn ret;\n}\n\n/*\n * Verify that @num_devices satisfies the RAID profile constraints in the whole\n * filesystem. It's up to the caller to adjust that number regarding eg. device\n * replace.\n */\nstatic int btrfs_check_raid_min_devices(struct btrfs_fs_info *fs_info,\n\t\tu64 num_devices)\n{\n\tu64 all_avail;\n\tunsigned seq;\n\tint i;\n\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tall_avail = fs_info->avail_data_alloc_bits |\n\t\t\t    fs_info->avail_system_alloc_bits |\n\t\t\t    fs_info->avail_metadata_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++) {\n\t\tif (!(all_avail & btrfs_raid_array[i].bg_flag))\n\t\t\tcontinue;\n\n\t\tif (num_devices < btrfs_raid_array[i].devs_min) {\n\t\t\tint ret = btrfs_raid_array[i].mindev_error;\n\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic struct btrfs_device * btrfs_find_next_active_device(\n\t\tstruct btrfs_fs_devices *fs_devs, struct btrfs_device *device)\n{\n\tstruct btrfs_device *next_device;\n\n\tlist_for_each_entry(next_device, &fs_devs->devices, dev_list) {\n\t\tif (next_device != device &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &next_device->dev_state)\n\t\t    && next_device->bdev)\n\t\t\treturn next_device;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * Helper function to check if the given device is part of s_bdev / latest_bdev\n * and replace it with the provided or the next active device, in the context\n * where this function called, there should be always be another device (or\n * this_dev) which is active.\n */\nvoid btrfs_assign_next_active_device(struct btrfs_device *device,\n\t\t\t\t     struct btrfs_device *this_dev)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_device *next_device;\n\n\tif (this_dev)\n\t\tnext_device = this_dev;\n\telse\n\t\tnext_device = btrfs_find_next_active_device(fs_info->fs_devices,\n\t\t\t\t\t\t\t\tdevice);\n\tASSERT(next_device);\n\n\tif (fs_info->sb->s_bdev &&\n\t\t\t(fs_info->sb->s_bdev == device->bdev))\n\t\tfs_info->sb->s_bdev = next_device->bdev;\n\n\tif (fs_info->fs_devices->latest_bdev == device->bdev)\n\t\tfs_info->fs_devices->latest_bdev = next_device->bdev;\n}\n\n/*\n * Return btrfs_fs_devices::num_devices excluding the device that's being\n * currently replaced.\n */\nstatic u64 btrfs_num_devices(struct btrfs_fs_info *fs_info)\n{\n\tu64 num_devices = fs_info->fs_devices->num_devices;\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (btrfs_dev_replace_is_ongoing(&fs_info->dev_replace)) {\n\t\tASSERT(num_devices > 1);\n\t\tnum_devices--;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\treturn num_devices;\n}\n\nint btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\tu64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(fs_info, device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(fs_info, device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_rm_device_link(fs_devices, device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(device->bdev, device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tcall_rcu(&device->rcu, free_device_rcu);\n\n\tif (cur_devices->open_devices == 0) {\n\t\twhile (fs_devices) {\n\t\t\tif (fs_devices->seed == cur_devices) {\n\t\t\t\tfs_devices->seed = cur_devices->seed;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tfs_devices = fs_devices->seed;\n\t\t}\n\t\tcur_devices->seed = NULL;\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}\n\nvoid btrfs_rm_dev_replace_remove_srcdev(struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlockdep_assert_held(&srcdev->fs_info->fs_devices->device_list_mutex);\n\n\t/*\n\t * in case of fs with no seed, srcdev->fs_devices will point\n\t * to fs_devices of fs_info. However when the dev being replaced is\n\t * a seed dev it will point to the seed's local fs_devices. In short\n\t * srcdev will have its correct fs_devices in both the cases.\n\t */\n\tfs_devices = srcdev->fs_devices;\n\n\tlist_del_rcu(&srcdev->dev_list);\n\tlist_del(&srcdev->dev_alloc_list);\n\tfs_devices->num_devices--;\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &srcdev->dev_state))\n\t\tfs_devices->missing_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &srcdev->dev_state))\n\t\tfs_devices->rw_devices--;\n\n\tif (srcdev->bdev)\n\t\tfs_devices->open_devices--;\n}\n\nvoid btrfs_rm_dev_replace_free_srcdev(struct btrfs_fs_info *fs_info,\n\t\t\t\t      struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = srcdev->fs_devices;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &srcdev->dev_state)) {\n\t\t/* zero out the old super if it is writable */\n\t\tbtrfs_scratch_superblocks(srcdev->bdev, srcdev->name->str);\n\t}\n\n\tbtrfs_close_bdev(srcdev);\n\tcall_rcu(&srcdev->rcu, free_device_rcu);\n\n\t/* if this is no devs we rather delete the fs_devices */\n\tif (!fs_devices->num_devices) {\n\t\tstruct btrfs_fs_devices *tmp_fs_devices;\n\n\t\t/*\n\t\t * On a mounted FS, num_devices can't be zero unless it's a\n\t\t * seed. In case of a seed device being replaced, the replace\n\t\t * target added to the sprout FS, so there will be no more\n\t\t * device left under the seed FS.\n\t\t */\n\t\tASSERT(fs_devices->seeding);\n\n\t\ttmp_fs_devices = fs_info->fs_devices;\n\t\twhile (tmp_fs_devices) {\n\t\t\tif (tmp_fs_devices->seed == fs_devices) {\n\t\t\t\ttmp_fs_devices->seed = fs_devices->seed;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttmp_fs_devices = tmp_fs_devices->seed;\n\t\t}\n\t\tfs_devices->seed = NULL;\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t}\n}\n\nvoid btrfs_destroy_dev_replace_tgtdev(struct btrfs_device *tgtdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = tgtdev->fs_info->fs_devices;\n\n\tWARN_ON(!tgtdev);\n\tmutex_lock(&fs_devices->device_list_mutex);\n\n\tbtrfs_sysfs_rm_device_link(fs_devices, tgtdev);\n\n\tif (tgtdev->bdev)\n\t\tfs_devices->open_devices--;\n\n\tfs_devices->num_devices--;\n\n\tbtrfs_assign_next_active_device(tgtdev, NULL);\n\n\tlist_del_rcu(&tgtdev->dev_list);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * The update_dev_time() with in btrfs_scratch_superblocks()\n\t * may lead to a call to btrfs_show_devname() which will try\n\t * to hold device_list_mutex. And here this device\n\t * is already out of device list, so we don't have to hold\n\t * the device_list_mutex lock.\n\t */\n\tbtrfs_scratch_superblocks(tgtdev->bdev, tgtdev->name->str);\n\n\tbtrfs_close_bdev(tgtdev);\n\tcall_rcu(&tgtdev->rcu, free_device_rcu);\n}\n\nstatic struct btrfs_device *btrfs_find_device_by_path(\n\t\tstruct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tint ret = 0;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tu8 *dev_uuid;\n\tstruct block_device *bdev;\n\tstruct buffer_head *bh;\n\tstruct btrfs_device *device;\n\n\tret = btrfs_get_bdev_and_sb(device_path, FMODE_READ,\n\t\t\t\t    fs_info->bdev_holder, 0, &bdev, &bh);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\tdisk_super = (struct btrfs_super_block *)bh->b_data;\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tdev_uuid = disk_super->dev_item.uuid;\n\tif (btrfs_fs_incompat(fs_info, METADATA_UUID))\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->metadata_uuid, true);\n\telse\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->fsid, true);\n\n\tbrelse(bh);\n\tif (!device)\n\t\tdevice = ERR_PTR(-ENOENT);\n\tblkdev_put(bdev, FMODE_READ);\n\treturn device;\n}\n\n/*\n * Lookup a device given by device id, or the path if the id is 0.\n */\nstruct btrfs_device *btrfs_find_device_by_devspec(\n\t\tstruct btrfs_fs_info *fs_info, u64 devid,\n\t\tconst char *device_path)\n{\n\tstruct btrfs_device *device;\n\n\tif (devid) {\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL,\n\t\t\t\t\t   NULL, true);\n\t\tif (!device)\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\treturn device;\n\t}\n\n\tif (!device_path || !device_path[0])\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (strcmp(device_path, \"missing\") == 0) {\n\t\t/* Find first missing device */\n\t\tlist_for_each_entry(device, &fs_info->fs_devices->devices,\n\t\t\t\t    dev_list) {\n\t\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t     &device->dev_state) && !device->bdev)\n\t\t\t\treturn device;\n\t\t}\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\treturn btrfs_find_device_by_path(fs_info, device_path);\n}\n\n/*\n * does all the dirty work required for changing file system's UUID.\n */\nstatic int btrfs_prepare_sprout(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_fs_devices *old_devices;\n\tstruct btrfs_fs_devices *seed_devices;\n\tstruct btrfs_super_block *disk_super = fs_info->super_copy;\n\tstruct btrfs_device *device;\n\tu64 super_flags;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tif (!fs_devices->seeding)\n\t\treturn -EINVAL;\n\n\tseed_devices = alloc_fs_devices(NULL, NULL);\n\tif (IS_ERR(seed_devices))\n\t\treturn PTR_ERR(seed_devices);\n\n\told_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(old_devices)) {\n\t\tkfree(seed_devices);\n\t\treturn PTR_ERR(old_devices);\n\t}\n\n\tlist_add(&old_devices->fs_list, &fs_uuids);\n\n\tmemcpy(seed_devices, fs_devices, sizeof(*seed_devices));\n\tseed_devices->opened = 1;\n\tINIT_LIST_HEAD(&seed_devices->devices);\n\tINIT_LIST_HEAD(&seed_devices->alloc_list);\n\tmutex_init(&seed_devices->device_list_mutex);\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_splice_init_rcu(&fs_devices->devices, &seed_devices->devices,\n\t\t\t      synchronize_rcu);\n\tlist_for_each_entry(device, &seed_devices->devices, dev_list)\n\t\tdevice->fs_devices = seed_devices;\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_splice_init(&fs_devices->alloc_list, &seed_devices->alloc_list);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\tfs_devices->seeding = 0;\n\tfs_devices->num_devices = 0;\n\tfs_devices->open_devices = 0;\n\tfs_devices->missing_devices = 0;\n\tfs_devices->rotating = 0;\n\tfs_devices->seed = seed_devices;\n\n\tgenerate_random_uuid(fs_devices->fsid);\n\tmemcpy(fs_devices->metadata_uuid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\tmemcpy(disk_super->fsid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tsuper_flags = btrfs_super_flags(disk_super) &\n\t\t      ~BTRFS_SUPER_FLAG_SEEDING;\n\tbtrfs_set_super_flags(disk_super, super_flags);\n\n\treturn 0;\n}\n\n/*\n * Store the expected generation for seed devices in device items.\n */\nstatic int btrfs_finish_sprout(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct btrfs_device *device;\n\tstruct btrfs_key key;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\tu64 devid;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\nnext_slot:\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != BTRFS_DEV_ITEMS_OBJECTID ||\n\t\t    key.type != BTRFS_DEV_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tdev_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t  struct btrfs_dev_item);\n\t\tdevid = btrfs_device_id(leaf, dev_item);\n\t\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t\t   BTRFS_FSID_SIZE);\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   fs_uuid, true);\n\t\tBUG_ON(!device); /* Logic error */\n\n\t\tif (device->fs_devices->seeding) {\n\t\t\tbtrfs_set_device_generation(leaf, dev_item,\n\t\t\t\t\t\t    device->generation);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tgoto next_slot;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct request_queue *q;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device;\n\tstruct block_device *bdev;\n\tstruct super_block *sb = fs_info->sb;\n\tstruct rcu_string *name;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 orig_super_total_bytes;\n\tu64 orig_super_num_devices;\n\tint seeding_dev = 0;\n\tint ret = 0;\n\tbool unlocked = false;\n\n\tif (sb_rdonly(sb) && !fs_devices->seeding)\n\t\treturn -EROFS;\n\n\tbdev = blkdev_get_by_path(device_path, FMODE_WRITE | FMODE_EXCL,\n\t\t\t\t  fs_info->bdev_holder);\n\tif (IS_ERR(bdev))\n\t\treturn PTR_ERR(bdev);\n\n\tif (fs_devices->seeding) {\n\t\tseeding_dev = 1;\n\t\tdown_write(&sb->s_umount);\n\t\tmutex_lock(&uuid_mutex);\n\t}\n\n\tfilemap_write_and_wait(bdev->bd_inode->i_mapping);\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tif (device->bdev == bdev) {\n\t\t\tret = -EEXIST;\n\t\t\tmutex_unlock(\n\t\t\t\t&fs_devices->device_list_mutex);\n\t\t\tgoto error;\n\t\t}\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tdevice = btrfs_alloc_device(fs_info, NULL, NULL);\n\tif (IS_ERR(device)) {\n\t\t/* we can safely leave the fs_devices entry around */\n\t\tret = PTR_ERR(device);\n\t\tgoto error;\n\t}\n\n\tname = rcu_string_strdup(device_path, GFP_KERNEL);\n\tif (!name) {\n\t\tret = -ENOMEM;\n\t\tgoto error_free_device;\n\t}\n\trcu_assign_pointer(device->name, name);\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto error_free_device;\n\t}\n\n\tq = bdev_get_queue(bdev);\n\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tdevice->generation = trans->transid;\n\tdevice->io_width = fs_info->sectorsize;\n\tdevice->io_align = fs_info->sectorsize;\n\tdevice->sector_size = fs_info->sectorsize;\n\tdevice->total_bytes = round_down(i_size_read(bdev->bd_inode),\n\t\t\t\t\t fs_info->sectorsize);\n\tdevice->disk_total_bytes = device->total_bytes;\n\tdevice->commit_total_bytes = device->total_bytes;\n\tdevice->fs_info = fs_info;\n\tdevice->bdev = bdev;\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\tdevice->mode = FMODE_EXCL;\n\tdevice->dev_stats_valid = 1;\n\tset_blocksize(device->bdev, BTRFS_BDEV_BLOCKSIZE);\n\n\tif (seeding_dev) {\n\t\tsb->s_flags &= ~SB_RDONLY;\n\t\tret = btrfs_prepare_sprout(fs_info);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_trans;\n\t\t}\n\t}\n\n\tdevice->fs_devices = fs_devices;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\tlist_add(&device->dev_alloc_list, &fs_devices->alloc_list);\n\tfs_devices->num_devices++;\n\tfs_devices->open_devices++;\n\tfs_devices->rw_devices++;\n\tfs_devices->total_devices++;\n\tfs_devices->total_rw_bytes += device->total_bytes;\n\n\tatomic64_add(device->total_bytes, &fs_info->free_chunk_space);\n\n\tif (!blk_queue_nonrot(q))\n\t\tfs_devices->rotating = 1;\n\n\torig_super_total_bytes = btrfs_super_total_bytes(fs_info->super_copy);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\tround_down(orig_super_total_bytes + device->total_bytes,\n\t\t\t   fs_info->sectorsize));\n\n\torig_super_num_devices = btrfs_super_num_devices(fs_info->super_copy);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices + 1);\n\n\t/* add sysfs device entry */\n\tbtrfs_sysfs_add_device_link(fs_devices, device);\n\n\t/*\n\t * we've got more storage, clear any full flags on the space\n\t * infos\n\t */\n\tbtrfs_clear_space_info_full(fs_info);\n\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (seeding_dev) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tret = init_first_rw_device(trans, fs_info);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\t}\n\n\tret = btrfs_add_dev_item(trans, device);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto error_sysfs;\n\t}\n\n\tif (seeding_dev) {\n\t\tchar fsid_buf[BTRFS_UUID_UNPARSED_SIZE];\n\n\t\tret = btrfs_finish_sprout(trans, fs_info);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\n\t\t/* Sprouting would change fsid of the mounted root,\n\t\t * so rename the fsid on the sysfs\n\t\t */\n\t\tsnprintf(fsid_buf, BTRFS_UUID_UNPARSED_SIZE, \"%pU\",\n\t\t\t\t\t\tfs_info->fs_devices->fsid);\n\t\tif (kobject_rename(&fs_devices->fsid_kobj, fsid_buf))\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"sysfs: failed to create fsid for sprout\");\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\n\tif (seeding_dev) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t\tunlocked = true;\n\n\t\tif (ret) /* transaction commit */\n\t\t\treturn ret;\n\n\t\tret = btrfs_relocate_sys_chunks(fs_info);\n\t\tif (ret < 0)\n\t\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t    \"Failed to relocate sys chunks after device initialization. This can be fixed using the \\\"btrfs balance\\\" command.\");\n\t\ttrans = btrfs_attach_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tif (PTR_ERR(trans) == -ENOENT)\n\t\t\t\treturn 0;\n\t\t\tret = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tgoto error_sysfs;\n\t\t}\n\t\tret = btrfs_commit_transaction(trans);\n\t}\n\n\t/* Update ctime/mtime for libblkid */\n\tupdate_dev_time(device_path);\n\treturn ret;\n\nerror_sysfs:\n\tbtrfs_sysfs_rm_device_link(fs_devices, device);\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_del_rcu(&device->dev_list);\n\tlist_del(&device->dev_alloc_list);\n\tfs_info->fs_devices->num_devices--;\n\tfs_info->fs_devices->open_devices--;\n\tfs_info->fs_devices->rw_devices--;\n\tfs_info->fs_devices->total_devices--;\n\tfs_info->fs_devices->total_rw_bytes -= device->total_bytes;\n\tatomic64_sub(device->total_bytes, &fs_info->free_chunk_space);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\t\t\t    orig_super_total_bytes);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\nerror_trans:\n\tif (seeding_dev)\n\t\tsb->s_flags |= SB_RDONLY;\n\tif (trans)\n\t\tbtrfs_end_transaction(trans);\nerror_free_device:\n\tbtrfs_free_device(device);\nerror:\n\tblkdev_put(bdev, FMODE_EXCL);\n\tif (seeding_dev && !unlocked) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t}\n\treturn ret;\n}\n\nstatic noinline int btrfs_update_device(struct btrfs_trans_handle *trans,\n\t\t\t\t\tstruct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = device->fs_info->chunk_root;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_mark_buffer_dirty(leaf);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_grow_device(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_fs_devices *fs_devices;\n\tu64 old_total;\n\tu64 diff;\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\treturn -EACCES;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\told_total = btrfs_super_total_bytes(super_copy);\n\tdiff = round_down(new_size - device->total_bytes, fs_info->sectorsize);\n\n\tif (new_size <= device->total_bytes ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tfs_devices = fs_info->fs_devices;\n\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total + diff, fs_info->sectorsize));\n\tdevice->fs_devices->total_rw_bytes += diff;\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tbtrfs_clear_space_info_full(device->fs_info);\n\tif (list_empty(&device->resized_list))\n\t\tlist_add_tail(&device->resized_list,\n\t\t\t      &fs_devices->resized_devices);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\treturn btrfs_update_device(trans, device);\n}\n\nstatic int btrfs_free_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = chunk_offset;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\telse if (ret > 0) { /* Logic error or corruption */\n\t\tbtrfs_handle_fs_error(fs_info, -ENOENT,\n\t\t\t\t      \"Failed lookup while freeing chunk.\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret < 0)\n\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t      \"Failed to delete chunk item.\");\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_del_sys_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *ptr;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur;\n\tstruct btrfs_key key;\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tptr = super_copy->sys_chunk_array;\n\tcur = 0;\n\n\twhile (cur < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)ptr;\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tlen = sizeof(*disk_key);\n\n\t\tif (key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tchunk = (struct btrfs_chunk *)(ptr + len);\n\t\t\tnum_stripes = btrfs_stack_chunk_num_stripes(chunk);\n\t\t\tlen += btrfs_chunk_item_size(num_stripes);\n\t\t} else {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (key.objectid == BTRFS_FIRST_CHUNK_TREE_OBJECTID &&\n\t\t    key.offset == chunk_offset) {\n\t\t\tmemmove(ptr, ptr + len, array_size - (cur + len));\n\t\t\tarray_size -= len;\n\t\t\tbtrfs_set_super_sys_array_size(super_copy, array_size);\n\t\t} else {\n\t\t\tptr += len;\n\t\t\tcur += len;\n\t\t}\n\t}\n\tmutex_unlock(&fs_info->chunk_mutex);\n\treturn ret;\n}\n\n/*\n * btrfs_get_chunk_map() - Find the mapping containing the given logical extent.\n * @logical: Logical block offset in bytes.\n * @length: Length of extent in bytes.\n *\n * Return: Chunk mapping or ERR_PTR.\n */\nstruct extent_map *btrfs_get_chunk_map(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u64 logical, u64 length)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\n\tem_tree = &fs_info->mapping_tree.map_tree;\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, logical, length);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_crit(fs_info, \"unable to find logical %llu length %llu\",\n\t\t\t   logical, length);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (em->start > logical || em->start + em->len < logical) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"found a bad mapping, wanted %llu-%llu, found %llu-%llu\",\n\t\t\t   logical, length, em->start, em->start + em->len);\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/* callers are responsible for dropping em's ref. */\n\treturn em;\n}\n\nint btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 dev_extent_len = 0;\n\tint i, ret = 0;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em)) {\n\t\t/*\n\t\t * This is a logic error, but we don't want to just rely on the\n\t\t * user having built with ASSERT enabled, so if ASSERT doesn't\n\t\t * do anything we still error out.\n\t\t */\n\t\tASSERT(0);\n\t\treturn PTR_ERR(em);\n\t}\n\tmap = em->map_lookup;\n\tmutex_lock(&fs_info->chunk_mutex);\n\tcheck_system_chunk(trans, map->type);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t/*\n\t * Take the device list mutex to prevent races with the final phase of\n\t * a device replace operation that replaces the device object associated\n\t * with map stripes (dev-replace.c:btrfs_dev_replace_finishing()).\n\t */\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\t\tret = btrfs_free_dev_extent(trans, device,\n\t\t\t\t\t    map->stripes[i].physical,\n\t\t\t\t\t    &dev_extent_len);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (device->bytes_used > 0) {\n\t\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\t\tbtrfs_device_set_bytes_used(device,\n\t\t\t\t\tdevice->bytes_used - dev_extent_len);\n\t\t\tatomic64_add(dev_extent_len, &fs_info->free_chunk_space);\n\t\t\tbtrfs_clear_space_info_full(fs_info);\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t}\n\n\t\tret = btrfs_update_device(trans, device);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tret = btrfs_free_chunk(trans, chunk_offset);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\ttrace_btrfs_chunk_free(fs_info, map, chunk_offset, em->len);\n\n\tif (map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tret = btrfs_del_sys_chunk(fs_info, chunk_offset);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = btrfs_remove_block_group(trans, chunk_offset, em);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\nout:\n\t/* once for us */\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic int btrfs_relocate_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\t/*\n\t * Prevent races with automatic removal of unused block groups.\n\t * After we relocate and before we remove the chunk with offset\n\t * chunk_offset, automatic removal of the block group can kick in,\n\t * resulting in a failure when calling btrfs_remove_chunk() below.\n\t *\n\t * Make sure to acquire this mutex before doing a tree search (dev\n\t * or chunk trees) to find chunks. Otherwise the cleaner kthread might\n\t * call btrfs_remove_chunk() (through btrfs_delete_unused_bgs()) after\n\t * we release the path used to search the chunk/dev tree and before\n\t * the current task acquires this mutex and calls us.\n\t */\n\tlockdep_assert_held(&fs_info->delete_unused_bgs_mutex);\n\n\tret = btrfs_can_relocate(fs_info, chunk_offset);\n\tif (ret)\n\t\treturn -ENOSPC;\n\n\t/* step one, relocate all the extents inside this chunk */\n\tbtrfs_scrub_pause(fs_info);\n\tret = btrfs_relocate_block_group(fs_info, chunk_offset);\n\tbtrfs_scrub_continue(fs_info);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * We add the kobjects here (and after forcing data chunk creation)\n\t * since relocation is the only place we'll create chunks of a new\n\t * type at runtime.  The only place where we'll remove the last\n\t * chunk of a type is the call immediately below this one.  Even\n\t * so, we're protected against races with the cleaner thread since\n\t * we're covered by the delete_unused_bgs_mutex.\n\t */\n\tbtrfs_add_raid_kobjects(fs_info);\n\n\ttrans = btrfs_start_trans_remove_block_group(root->fs_info,\n\t\t\t\t\t\t     chunk_offset);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tbtrfs_handle_fs_error(root->fs_info, ret, NULL);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * step two, delete the device extents and the\n\t * chunk tree entries\n\t */\n\tret = btrfs_remove_chunk(trans, chunk_offset);\n\tbtrfs_end_transaction(trans);\n\treturn ret;\n}\n\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tu64 chunk_type;\n\tbool retried = false;\n\tint failed = 0;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\nagain:\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tmutex_lock(&fs_info->delete_unused_bgs_mutex);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto error;\n\t\t}\n\t\tBUG_ON(ret == 0); /* Corruption */\n\n\t\tret = btrfs_previous_item(chunk_root, path, key.objectid,\n\t\t\t\t\t  key.type);\n\t\tif (ret)\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\t\tif (ret > 0)\n\t\t\tbreak;\n\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\t\tchunk = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t       struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\t\tbtrfs_release_path(path);\n\n\t\tif (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\t\tif (ret == -ENOSPC)\n\t\t\t\tfailed++;\n\t\t\telse\n\t\t\t\tBUG_ON(ret);\n\t\t}\n\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\tret = 0;\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (WARN_ON(failed && retried)) {\n\t\tret = -ENOSPC;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * return 1 : allocate a data chunk successfully,\n * return <0: errors during allocating a data chunk,\n * return 0 : no need to allocate a data chunk.\n */\nstatic int btrfs_may_alloc_data_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t      u64 chunk_offset)\n{\n\tstruct btrfs_block_group_cache *cache;\n\tu64 bytes_used;\n\tu64 chunk_type;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tASSERT(cache);\n\tchunk_type = cache->flags;\n\tbtrfs_put_block_group(cache);\n\n\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tspin_lock(&fs_info->data_sinfo->lock);\n\t\tbytes_used = fs_info->data_sinfo->bytes_used;\n\t\tspin_unlock(&fs_info->data_sinfo->lock);\n\n\t\tif (!bytes_used) {\n\t\t\tstruct btrfs_trans_handle *trans;\n\t\t\tint ret;\n\n\t\t\ttrans =\tbtrfs_join_transaction(fs_info->tree_root);\n\t\t\tif (IS_ERR(trans))\n\t\t\t\treturn PTR_ERR(trans);\n\n\t\t\tret = btrfs_force_chunk_alloc(trans,\n\t\t\t\t\t\t      BTRFS_BLOCK_GROUP_DATA);\n\t\t\tbtrfs_end_transaction(trans);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\n\t\t\tbtrfs_add_raid_kobjects(fs_info);\n\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int insert_balance_item(struct btrfs_fs_info *fs_info,\n\t\t\t       struct btrfs_balance_control *bctl)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      sizeof(*item));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tmemzero_extent_buffer(leaf, (unsigned long)item, sizeof(*item));\n\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->data);\n\tbtrfs_set_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->meta);\n\tbtrfs_set_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->sys);\n\tbtrfs_set_balance_sys(leaf, item, &disk_bargs);\n\n\tbtrfs_set_balance_flags(leaf, item, bctl->flags);\n\n\tbtrfs_mark_buffer_dirty(leaf);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int del_balance_item(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\n/*\n * This is a heuristic used to reduce the number of chunks balanced on\n * resume after balance was interrupted.\n */\nstatic void update_balance_args(struct btrfs_balance_control *bctl)\n{\n\t/*\n\t * Turn on soft mode for chunk types that were being converted.\n\t */\n\tif (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\n\t/*\n\t * Turn on usage filter if is not already used.  The idea is\n\t * that chunks that we have already balanced should be\n\t * reasonably full.  Don't do it for chunks that are being\n\t * converted - that will keep us from relocating unconverted\n\t * (albeit full) chunks.\n\t */\n\tif (!(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->data.usage = 90;\n\t}\n\tif (!(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->sys.usage = 90;\n\t}\n\tif (!(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->meta.usage = 90;\n\t}\n}\n\n/*\n * Clear the balance status in fs_info and delete the balance item from disk.\n */\nstatic void reset_balance_state(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tint ret;\n\n\tBUG_ON(!fs_info->balance_ctl);\n\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = NULL;\n\tspin_unlock(&fs_info->balance_lock);\n\n\tkfree(bctl);\n\tret = del_balance_item(fs_info);\n\tif (ret)\n\t\tbtrfs_handle_fs_error(fs_info, ret, NULL);\n}\n\n/*\n * Balance filters.  Return 1 if chunk should be filtered out\n * (should not be balanced).\n */\nstatic int chunk_profiles_filter(u64 chunk_type,\n\t\t\t\t struct btrfs_balance_args *bargs)\n{\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->profiles & chunk_type)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_usage_range_filter(struct btrfs_fs_info *fs_info, u64 chunk_offset,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group_cache *cache;\n\tu64 chunk_used;\n\tu64 user_thresh_min;\n\tu64 user_thresh_max;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = btrfs_block_group_used(&cache->item);\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh_min = 0;\n\telse\n\t\tuser_thresh_min = div_factor_fine(cache->key.offset,\n\t\t\t\t\tbargs->usage_min);\n\n\tif (bargs->usage_max == 0)\n\t\tuser_thresh_max = 1;\n\telse if (bargs->usage_max > 100)\n\t\tuser_thresh_max = cache->key.offset;\n\telse\n\t\tuser_thresh_max = div_factor_fine(cache->key.offset,\n\t\t\t\t\tbargs->usage_max);\n\n\tif (user_thresh_min <= chunk_used && chunk_used < user_thresh_max)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_usage_filter(struct btrfs_fs_info *fs_info,\n\t\tu64 chunk_offset, struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group_cache *cache;\n\tu64 chunk_used, user_thresh;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = btrfs_block_group_used(&cache->item);\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh = 1;\n\telse if (bargs->usage > 100)\n\t\tuser_thresh = cache->key.offset;\n\telse\n\t\tuser_thresh = div_factor_fine(cache->key.offset,\n\t\t\t\t\t      bargs->usage);\n\n\tif (chunk_used < user_thresh)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_devid_filter(struct extent_buffer *leaf,\n\t\t\t      struct btrfs_chunk *chunk,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tint i;\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) == bargs->devid)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/* [pstart, pend) */\nstatic int chunk_drange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tu64 stripe_offset;\n\tu64 stripe_length;\n\tint factor;\n\tint i;\n\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_DEVID))\n\t\treturn 0;\n\n\tif (btrfs_chunk_type(leaf, chunk) & (BTRFS_BLOCK_GROUP_DUP |\n\t     BTRFS_BLOCK_GROUP_RAID1 | BTRFS_BLOCK_GROUP_RAID10)) {\n\t\tfactor = num_stripes / 2;\n\t} else if (btrfs_chunk_type(leaf, chunk) & BTRFS_BLOCK_GROUP_RAID5) {\n\t\tfactor = num_stripes - 1;\n\t} else if (btrfs_chunk_type(leaf, chunk) & BTRFS_BLOCK_GROUP_RAID6) {\n\t\tfactor = num_stripes - 2;\n\t} else {\n\t\tfactor = num_stripes;\n\t}\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) != bargs->devid)\n\t\t\tcontinue;\n\n\t\tstripe_offset = btrfs_stripe_offset(leaf, stripe);\n\t\tstripe_length = btrfs_chunk_length(leaf, chunk);\n\t\tstripe_length = div_u64(stripe_length, factor);\n\n\t\tif (stripe_offset < bargs->pend &&\n\t\t    stripe_offset + stripe_length > bargs->pstart)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/* [vstart, vend) */\nstatic int chunk_vrange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       u64 chunk_offset,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tif (chunk_offset < bargs->vend &&\n\t    chunk_offset + btrfs_chunk_length(leaf, chunk) > bargs->vstart)\n\t\t/* at least part of the chunk is inside this vrange */\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_stripes_range_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tif (bargs->stripes_min <= num_stripes\n\t\t\t&& num_stripes <= bargs->stripes_max)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_soft_convert_filter(u64 chunk_type,\n\t\t\t\t     struct btrfs_balance_args *bargs)\n{\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_CONVERT))\n\t\treturn 0;\n\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->target == chunk_type)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int should_balance_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\tstruct extent_buffer *leaf,\n\t\t\t\tstruct btrfs_chunk *chunk, u64 chunk_offset)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_balance_args *bargs = NULL;\n\tu64 chunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t/* type filter */\n\tif (!((chunk_type & BTRFS_BLOCK_GROUP_TYPE_MASK) &\n\t      (bctl->flags & BTRFS_BALANCE_TYPE_MASK))) {\n\t\treturn 0;\n\t}\n\n\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\tbargs = &bctl->data;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\tbargs = &bctl->sys;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\tbargs = &bctl->meta;\n\n\t/* profiles filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_PROFILES) &&\n\t    chunk_profiles_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* usage filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    chunk_usage_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    chunk_usage_range_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* devid filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DEVID) &&\n\t    chunk_devid_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* drange filter, makes sense only with devid filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DRANGE) &&\n\t    chunk_drange_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* vrange filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_VRANGE) &&\n\t    chunk_vrange_filter(leaf, chunk, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* stripes filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE) &&\n\t    chunk_stripes_range_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* soft profile changing mode */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_SOFT) &&\n\t    chunk_soft_convert_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/*\n\t * limited by count, must be the last filter\n\t */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT)) {\n\t\tif (bargs->limit == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit--;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)) {\n\t\t/*\n\t\t * Same logic as the 'limit' filter; the minimum cannot be\n\t\t * determined here because we do not have the global information\n\t\t * about the count of all chunks that satisfy the filters.\n\t\t */\n\t\tif (bargs->limit_max == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit_max--;\n\t}\n\n\treturn 1;\n}\n\nstatic int __btrfs_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tu64 chunk_type;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\tint ret;\n\tint enospc_errors = 0;\n\tbool counting = true;\n\t/* The single value limit and min/max limits use the same bytes in the */\n\tu64 limit_data = bctl->data.limit;\n\tu64 limit_meta = bctl->meta.limit;\n\tu64 limit_sys = bctl->sys.limit;\n\tu32 count_data = 0;\n\tu32 count_meta = 0;\n\tu32 count_sys = 0;\n\tint chunk_reserved = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\t/* zero out stat counters */\n\tspin_lock(&fs_info->balance_lock);\n\tmemset(&bctl->stat, 0, sizeof(bctl->stat));\n\tspin_unlock(&fs_info->balance_lock);\nagain:\n\tif (!counting) {\n\t\t/*\n\t\t * The single value limit and min/max limits use the same bytes\n\t\t * in the\n\t\t */\n\t\tbctl->data.limit = limit_data;\n\t\tbctl->meta.limit = limit_meta;\n\t\tbctl->sys.limit = limit_sys;\n\t}\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tif ((!counting && atomic_read(&fs_info->balance_pause_req)) ||\n\t\t    atomic_read(&fs_info->balance_cancel_req)) {\n\t\t\tret = -ECANCELED;\n\t\t\tgoto error;\n\t\t}\n\n\t\tmutex_lock(&fs_info->delete_unused_bgs_mutex);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * this shouldn't happen, it means the last relocate\n\t\t * failed\n\t\t */\n\t\tif (ret == 0)\n\t\t\tBUG(); /* FIXME break ? */\n\n\t\tret = btrfs_previous_item(chunk_root, path, 0,\n\t\t\t\t\t  BTRFS_CHUNK_ITEM_KEY);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\tif (found_key.objectid != key.objectid) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t\tif (!counting) {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.considered++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\n\n\t\tret = should_balance_chunk(fs_info, leaf, chunk,\n\t\t\t\t\t   found_key.offset);\n\n\t\tbtrfs_release_path(path);\n\t\tif (!ret) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (counting) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.expected++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\n\t\t\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\t\t\tcount_data++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\t\t\tcount_sys++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\t\t\tcount_meta++;\n\n\t\t\tgoto loop;\n\t\t}\n\n\t\t/*\n\t\t * Apply limit_min filter, no need to check if the LIMITS\n\t\t * filter is used, limit_min is 0 by default\n\t\t */\n\t\tif (((chunk_type & BTRFS_BLOCK_GROUP_DATA) &&\n\t\t\t\t\tcount_data < bctl->data.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t\t\t\tcount_meta < bctl->meta.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) &&\n\t\t\t\t\tcount_sys < bctl->sys.limit_min)) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (!chunk_reserved) {\n\t\t\t/*\n\t\t\t * We may be relocating the only data chunk we have,\n\t\t\t * which could potentially end up with losing data's\n\t\t\t * raid profile, so lets allocate an empty one in\n\t\t\t * advance.\n\t\t\t */\n\t\t\tret = btrfs_may_alloc_data_chunk(fs_info,\n\t\t\t\t\t\t\t found_key.offset);\n\t\t\tif (ret < 0) {\n\t\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\t\tgoto error;\n\t\t\t} else if (ret == 1) {\n\t\t\t\tchunk_reserved = 1;\n\t\t\t}\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\tif (ret == -ENOSPC) {\n\t\t\tenospc_errors++;\n\t\t} else if (ret == -ETXTBSY) {\n\t\t\tbtrfs_info(fs_info,\n\t   \"skipping relocation of block group %llu due to active swapfile\",\n\t\t\t\t   found_key.offset);\n\t\t\tret = 0;\n\t\t} else if (ret) {\n\t\t\tgoto error;\n\t\t} else {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.completed++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\nloop:\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\n\tif (counting) {\n\t\tbtrfs_release_path(path);\n\t\tcounting = false;\n\t\tgoto again;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\tif (enospc_errors) {\n\t\tbtrfs_info(fs_info, \"%d enospc errors during balance\",\n\t\t\t   enospc_errors);\n\t\tif (!ret)\n\t\t\tret = -ENOSPC;\n\t}\n\n\treturn ret;\n}\n\n/**\n * alloc_profile_is_valid - see if a given profile is valid and reduced\n * @flags: profile to validate\n * @extended: if true @flags is treated as an extended profile\n */\nstatic int alloc_profile_is_valid(u64 flags, int extended)\n{\n\tu64 mask = (extended ? BTRFS_EXTENDED_PROFILE_MASK :\n\t\t\t       BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\n\tflags &= ~BTRFS_BLOCK_GROUP_TYPE_MASK;\n\n\t/* 1) check that all other bits are zeroed */\n\tif (flags & ~mask)\n\t\treturn 0;\n\n\t/* 2) see if profile is reduced */\n\tif (flags == 0)\n\t\treturn !extended; /* \"0\" is valid for usual profiles */\n\n\t/* true if exactly one bit set */\n\treturn is_power_of_2(flags);\n}\n\nstatic inline int balance_need_close(struct btrfs_fs_info *fs_info)\n{\n\t/* cancel requested || normal exit path */\n\treturn atomic_read(&fs_info->balance_cancel_req) ||\n\t\t(atomic_read(&fs_info->balance_pause_req) == 0 &&\n\t\t atomic_read(&fs_info->balance_cancel_req) == 0);\n}\n\n/* Non-zero return value signifies invalidity */\nstatic inline int validate_convert_profile(struct btrfs_balance_args *bctl_arg,\n\t\tu64 allowed)\n{\n\treturn ((bctl_arg->flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t(!alloc_profile_is_valid(bctl_arg->target, 1) ||\n\t\t (bctl_arg->target & ~allowed)));\n}\n\n/*\n * Fill @buf with textual description of balance filter flags @bargs, up to\n * @size_buf including the terminating null. The output may be trimmed if it\n * does not fit into the provided buffer.\n */\nstatic void describe_balance_args(struct btrfs_balance_args *bargs, char *buf,\n\t\t\t\t u32 size_buf)\n{\n\tint ret;\n\tu32 size_bp = size_buf;\n\tchar *bp = buf;\n\tu64 flags = bargs->flags;\n\tchar tmp_buf[128] = {'\\0'};\n\n\tif (!flags)\n\t\treturn;\n\n#define CHECK_APPEND_NOARG(a)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_2ARG(a, v1, v2)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1), (v2));\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (flags & BTRFS_BALANCE_ARGS_CONVERT) {\n\t\tint index = btrfs_bg_flags_to_raid_index(bargs->target);\n\n\t\tCHECK_APPEND_1ARG(\"convert=%s,\", get_raid_name(index));\n\t}\n\n\tif (flags & BTRFS_BALANCE_ARGS_SOFT)\n\t\tCHECK_APPEND_NOARG(\"soft,\");\n\n\tif (flags & BTRFS_BALANCE_ARGS_PROFILES) {\n\t\tbtrfs_describe_block_groups(bargs->profiles, tmp_buf,\n\t\t\t\t\t    sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"profiles=%s,\", tmp_buf);\n\t}\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE)\n\t\tCHECK_APPEND_1ARG(\"usage=%llu,\", bargs->usage);\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE_RANGE)\n\t\tCHECK_APPEND_2ARG(\"usage=%u..%u,\",\n\t\t\t\t  bargs->usage_min, bargs->usage_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DEVID)\n\t\tCHECK_APPEND_1ARG(\"devid=%llu,\", bargs->devid);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DRANGE)\n\t\tCHECK_APPEND_2ARG(\"drange=%llu..%llu,\",\n\t\t\t\t  bargs->pstart, bargs->pend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_VRANGE)\n\t\tCHECK_APPEND_2ARG(\"vrange=%llu..%llu,\",\n\t\t\t\t  bargs->vstart, bargs->vend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT)\n\t\tCHECK_APPEND_1ARG(\"limit=%llu,\", bargs->limit);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)\n\t\tCHECK_APPEND_2ARG(\"limit=%u..%u,\",\n\t\t\t\tbargs->limit_min, bargs->limit_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE)\n\t\tCHECK_APPEND_2ARG(\"stripes=%u..%u,\",\n\t\t\t\t  bargs->stripes_min, bargs->stripes_max);\n\n#undef CHECK_APPEND_2ARG\n#undef CHECK_APPEND_1ARG\n#undef CHECK_APPEND_NOARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last , */\n\telse\n\t\tbuf[0] = '\\0';\n}\n\nstatic void describe_balance_start_or_resume(struct btrfs_fs_info *fs_info)\n{\n\tu32 size_buf = 1024;\n\tchar tmp_buf[192] = {'\\0'};\n\tchar *buf;\n\tchar *bp;\n\tu32 size_bp = size_buf;\n\tint ret;\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\n\tbuf = kzalloc(size_buf, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\n\tbp = buf;\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (bctl->flags & BTRFS_BALANCE_FORCE)\n\t\tCHECK_APPEND_1ARG(\"%s\", \"-f \");\n\n\tif (bctl->flags & BTRFS_BALANCE_DATA) {\n\t\tdescribe_balance_args(&bctl->data, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-d%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_METADATA) {\n\t\tdescribe_balance_args(&bctl->meta, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-m%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_SYSTEM) {\n\t\tdescribe_balance_args(&bctl->sys, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-s%s \", tmp_buf);\n\t}\n\n#undef CHECK_APPEND_1ARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last \" \" */\n\tbtrfs_info(fs_info, \"balance: %s %s\",\n\t\t   (bctl->flags & BTRFS_BALANCE_RESUME) ?\n\t\t   \"resume\" : \"start\", buf);\n\n\tkfree(buf);\n}\n\n/*\n * Should be called with balance mutexe held\n */\nint btrfs_balance(struct btrfs_fs_info *fs_info,\n\t\t  struct btrfs_balance_control *bctl,\n\t\t  struct btrfs_ioctl_balance_args *bargs)\n{\n\tu64 meta_target, data_target;\n\tu64 allowed;\n\tint mixed = 0;\n\tint ret;\n\tu64 num_devices;\n\tunsigned seq;\n\tbool reducing_integrity;\n\n\tif (btrfs_fs_closing(fs_info) ||\n\t    atomic_read(&fs_info->balance_pause_req) ||\n\t    atomic_read(&fs_info->balance_cancel_req)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tallowed = btrfs_super_incompat_flags(fs_info->super_copy);\n\tif (allowed & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)\n\t\tmixed = 1;\n\n\t/*\n\t * In case of mixed groups both data and meta should be picked,\n\t * and identical options should be given for both of them.\n\t */\n\tallowed = BTRFS_BALANCE_DATA | BTRFS_BALANCE_METADATA;\n\tif (mixed && (bctl->flags & allowed)) {\n\t\tif (!(bctl->flags & BTRFS_BALANCE_DATA) ||\n\t\t    !(bctl->flags & BTRFS_BALANCE_METADATA) ||\n\t\t    memcmp(&bctl->data, &bctl->meta, sizeof(bctl->data))) {\n\t\t\tbtrfs_err(fs_info,\n\t  \"balance: mixed groups data and metadata options must be the same\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tallowed = BTRFS_AVAIL_ALLOC_BIT_SINGLE | BTRFS_BLOCK_GROUP_DUP;\n\tif (num_devices > 1)\n\t\tallowed |= (BTRFS_BLOCK_GROUP_RAID0 | BTRFS_BLOCK_GROUP_RAID1);\n\tif (num_devices > 2)\n\t\tallowed |= BTRFS_BLOCK_GROUP_RAID5;\n\tif (num_devices > 3)\n\t\tallowed |= (BTRFS_BLOCK_GROUP_RAID10 |\n\t\t\t    BTRFS_BLOCK_GROUP_RAID6);\n\tif (validate_convert_profile(&bctl->data, allowed)) {\n\t\tint index = btrfs_bg_flags_to_raid_index(bctl->data.target);\n\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"balance: invalid convert data profile %s\",\n\t\t\t  get_raid_name(index));\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (validate_convert_profile(&bctl->meta, allowed)) {\n\t\tint index = btrfs_bg_flags_to_raid_index(bctl->meta.target);\n\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"balance: invalid convert metadata profile %s\",\n\t\t\t  get_raid_name(index));\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (validate_convert_profile(&bctl->sys, allowed)) {\n\t\tint index = btrfs_bg_flags_to_raid_index(bctl->sys.target);\n\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"balance: invalid convert system profile %s\",\n\t\t\t  get_raid_name(index));\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* allow to reduce meta or sys integrity only if force set */\n\tallowed = BTRFS_BLOCK_GROUP_DUP | BTRFS_BLOCK_GROUP_RAID1 |\n\t\t\tBTRFS_BLOCK_GROUP_RAID10 |\n\t\t\tBTRFS_BLOCK_GROUP_RAID5 |\n\t\t\tBTRFS_BLOCK_GROUP_RAID6;\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tif (((bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_system_alloc_bits & allowed) &&\n\t\t     !(bctl->sys.target & allowed)) ||\n\t\t    ((bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_metadata_alloc_bits & allowed) &&\n\t\t     !(bctl->meta.target & allowed)))\n\t\t\treducing_integrity = true;\n\t\telse\n\t\t\treducing_integrity = false;\n\n\t\t/* if we're not converting, the target field is uninitialized */\n\t\tmeta_target = (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->meta.target : fs_info->avail_metadata_alloc_bits;\n\t\tdata_target = (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->data.target : fs_info->avail_data_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tif (reducing_integrity) {\n\t\tif (bctl->flags & BTRFS_BALANCE_FORCE) {\n\t\t\tbtrfs_info(fs_info,\n\t\t\t\t   \"balance: force reducing metadata integrity\");\n\t\t} else {\n\t\t\tbtrfs_err(fs_info,\n\t  \"balance: reduces metadata integrity, use --force if you want this\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (btrfs_get_num_tolerated_disk_barrier_failures(meta_target) <\n\t\tbtrfs_get_num_tolerated_disk_barrier_failures(data_target)) {\n\t\tint meta_index = btrfs_bg_flags_to_raid_index(meta_target);\n\t\tint data_index = btrfs_bg_flags_to_raid_index(data_target);\n\n\t\tbtrfs_warn(fs_info,\n\t\"balance: metadata profile %s has lower redundancy than data profile %s\",\n\t\t\t   get_raid_name(meta_index), get_raid_name(data_index));\n\t}\n\n\tret = insert_balance_item(fs_info, bctl);\n\tif (ret && ret != -EEXIST)\n\t\tgoto out;\n\n\tif (!(bctl->flags & BTRFS_BALANCE_RESUME)) {\n\t\tBUG_ON(ret == -EEXIST);\n\t\tBUG_ON(fs_info->balance_ctl);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tfs_info->balance_ctl = bctl;\n\t\tspin_unlock(&fs_info->balance_lock);\n\t} else {\n\t\tBUG_ON(ret != -EEXIST);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tupdate_balance_args(bctl);\n\t\tspin_unlock(&fs_info->balance_lock);\n\t}\n\n\tASSERT(!test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tset_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\tdescribe_balance_start_or_resume(fs_info);\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tret = __btrfs_balance(fs_info);\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (ret == -ECANCELED && atomic_read(&fs_info->balance_pause_req))\n\t\tbtrfs_info(fs_info, \"balance: paused\");\n\telse if (ret == -ECANCELED && atomic_read(&fs_info->balance_cancel_req))\n\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\telse\n\t\tbtrfs_info(fs_info, \"balance: ended with status: %d\", ret);\n\n\tclear_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\n\tif (bargs) {\n\t\tmemset(bargs, 0, sizeof(*bargs));\n\t\tbtrfs_update_ioctl_balance_args(fs_info, bargs);\n\t}\n\n\tif ((ret && ret != -ECANCELED && ret != -ENOSPC) ||\n\t    balance_need_close(fs_info)) {\n\t\treset_balance_state(fs_info);\n\t\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\t}\n\n\twake_up(&fs_info->balance_wait_q);\n\n\treturn ret;\nout:\n\tif (bctl->flags & BTRFS_BALANCE_RESUME)\n\t\treset_balance_state(fs_info);\n\telse\n\t\tkfree(bctl);\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\n\treturn ret;\n}\n\nstatic int balance_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tint ret = 0;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (fs_info->balance_ctl)\n\t\tret = btrfs_balance(fs_info, fs_info->balance_ctl, NULL);\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\treturn ret;\n}\n\nint btrfs_resume_balance_async(struct btrfs_fs_info *fs_info)\n{\n\tstruct task_struct *tsk;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn 0;\n\t}\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tif (btrfs_test_opt(fs_info, SKIP_BALANCE)) {\n\t\tbtrfs_info(fs_info, \"balance: resume skipped\");\n\t\treturn 0;\n\t}\n\n\t/*\n\t * A ro->rw remount sequence should continue with the paused balance\n\t * regardless of who pauses it, system or the user as of now, so set\n\t * the resume flag.\n\t */\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl->flags |= BTRFS_BALANCE_RESUME;\n\tspin_unlock(&fs_info->balance_lock);\n\n\ttsk = kthread_run(balance_kthread, fs_info, \"btrfs-balance\");\n\treturn PTR_ERR_OR_ZERO(tsk);\n}\n\nint btrfs_recover_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, fs_info->tree_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) { /* ret = -ENOENT; */\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tbctl = kzalloc(sizeof(*bctl), GFP_NOFS);\n\tif (!bctl) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tbctl->flags = btrfs_balance_flags(leaf, item);\n\tbctl->flags |= BTRFS_BALANCE_RESUME;\n\n\tbtrfs_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->data, &disk_bargs);\n\tbtrfs_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->meta, &disk_bargs);\n\tbtrfs_balance_sys(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->sys, &disk_bargs);\n\n\t/*\n\t * This should never happen, as the paused balance state is recovered\n\t * during mount without any chance of other exclusive ops to collide.\n\t *\n\t * This gives the exclusive op status to balance and keeps in paused\n\t * state until user intervention (cancel or umount). If the ownership\n\t * cannot be assigned, show a message but do not fail. The balance\n\t * is in a paused state and must have fs_info::balance_ctl properly\n\t * set up.\n\t */\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags))\n\t\tbtrfs_warn(fs_info,\n\t\"balance: cannot set exclusive op status, resume manually\");\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tBUG_ON(fs_info->balance_ctl);\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = bctl;\n\tspin_unlock(&fs_info->balance_lock);\n\tmutex_unlock(&fs_info->balance_mutex);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_pause_balance(struct btrfs_fs_info *fs_info)\n{\n\tint ret = 0;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tatomic_inc(&fs_info->balance_pause_req);\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t\t/* we are good with balance_ctl ripped off from under us */\n\t\tBUG_ON(test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tatomic_dec(&fs_info->balance_pause_req);\n\t} else {\n\t\tret = -ENOTCONN;\n\t}\n\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn ret;\n}\n\nint btrfs_cancel_balance(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\t/*\n\t * A paused balance with the item stored on disk can be resumed at\n\t * mount time if the mount is read-write. Otherwise it's still paused\n\t * and we must not allow cancelling as it deletes the item.\n\t */\n\tif (sb_rdonly(fs_info->sb)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -EROFS;\n\t}\n\n\tatomic_inc(&fs_info->balance_cancel_req);\n\t/*\n\t * if we are running just wait and return, balance item is\n\t * deleted in btrfs_balance in this case\n\t */\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t} else {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t/*\n\t\t * Lock released to allow other waiters to continue, we'll\n\t\t * reexamine the status again.\n\t\t */\n\t\tmutex_lock(&fs_info->balance_mutex);\n\n\t\tif (fs_info->balance_ctl) {\n\t\t\treset_balance_state(fs_info);\n\t\t\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\t\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\t\t}\n\t}\n\n\tBUG_ON(fs_info->balance_ctl ||\n\t\ttest_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tatomic_dec(&fs_info->balance_cancel_req);\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn 0;\n}\n\nstatic int btrfs_uuid_scan_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path = NULL;\n\tint ret = 0;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tstruct btrfs_root_item root_item;\n\tu32 item_size;\n\tstruct btrfs_trans_handle *trans = NULL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = 0;\n\n\twhile (1) {\n\t\tret = btrfs_search_forward(root, &key, path,\n\t\t\t\tBTRFS_OLDEST_GENERATION);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (key.type != BTRFS_ROOT_ITEM_KEY ||\n\t\t    (key.objectid < BTRFS_FIRST_FREE_OBJECTID &&\n\t\t     key.objectid != BTRFS_FS_TREE_OBJECTID) ||\n\t\t    key.objectid > BTRFS_LAST_FREE_OBJECTID)\n\t\t\tgoto skip;\n\n\t\teb = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\titem_size = btrfs_item_size_nr(eb, slot);\n\t\tif (item_size < sizeof(root_item))\n\t\t\tgoto skip;\n\n\t\tread_extent_buffer(eb, &root_item,\n\t\t\t\t   btrfs_item_ptr_offset(eb, slot),\n\t\t\t\t   (int)sizeof(root_item));\n\t\tif (btrfs_root_refs(&root_item) == 0)\n\t\t\tgoto skip;\n\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid) ||\n\t\t    !btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tif (trans)\n\t\t\t\tgoto update_tree;\n\n\t\t\tbtrfs_release_path(path);\n\t\t\t/*\n\t\t\t * 1 - subvol uuid item\n\t\t\t * 1 - received_subvol uuid item\n\t\t\t */\n\t\t\ttrans = btrfs_start_transaction(fs_info->uuid_root, 2);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tgoto skip;\n\t\t}\nupdate_tree:\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans, root_item.uuid,\n\t\t\t\t\t\t  BTRFS_UUID_KEY_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans,\n\t\t\t\t\t\t  root_item.received_uuid,\n\t\t\t\t\t\t BTRFS_UUID_KEY_RECEIVED_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\nskip:\n\t\tif (trans) {\n\t\t\tret = btrfs_end_transaction(trans);\n\t\t\ttrans = NULL;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tbtrfs_release_path(path);\n\t\tif (key.offset < (u64)-1) {\n\t\t\tkey.offset++;\n\t\t} else if (key.type < BTRFS_ROOT_ITEM_KEY) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t} else if (key.objectid < (u64)-1) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t\tkey.objectid++;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tif (trans && !IS_ERR(trans))\n\t\tbtrfs_end_transaction(trans);\n\tif (ret)\n\t\tbtrfs_warn(fs_info, \"btrfs_uuid_scan_kthread failed %d\", ret);\n\telse\n\t\tset_bit(BTRFS_FS_UPDATE_UUID_TREE_GEN, &fs_info->flags);\n\tup(&fs_info->uuid_tree_rescan_sem);\n\treturn 0;\n}\n\n/*\n * Callback for btrfs_uuid_tree_iterate().\n * returns:\n * 0\tcheck succeeded, the entry is not outdated.\n * < 0\tif an error occurred.\n * > 0\tif the check failed, which means the caller shall remove the entry.\n */\nstatic int btrfs_check_uuid_tree_entry(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u8 *uuid, u8 type, u64 subid)\n{\n\tstruct btrfs_key key;\n\tint ret = 0;\n\tstruct btrfs_root *subvol_root;\n\n\tif (type != BTRFS_UUID_KEY_SUBVOL &&\n\t    type != BTRFS_UUID_KEY_RECEIVED_SUBVOL)\n\t\tgoto out;\n\n\tkey.objectid = subid;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\tsubvol_root = btrfs_read_fs_root_no_name(fs_info, &key);\n\tif (IS_ERR(subvol_root)) {\n\t\tret = PTR_ERR(subvol_root);\n\t\tif (ret == -ENOENT)\n\t\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase BTRFS_UUID_KEY_SUBVOL:\n\t\tif (memcmp(uuid, subvol_root->root_item.uuid, BTRFS_UUID_SIZE))\n\t\t\tret = 1;\n\t\tbreak;\n\tcase BTRFS_UUID_KEY_RECEIVED_SUBVOL:\n\t\tif (memcmp(uuid, subvol_root->root_item.received_uuid,\n\t\t\t   BTRFS_UUID_SIZE))\n\t\t\tret = 1;\n\t\tbreak;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic int btrfs_uuid_rescan_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = (struct btrfs_fs_info *)data;\n\tint ret;\n\n\t/*\n\t * 1st step is to iterate through the existing UUID tree and\n\t * to delete all entries that contain outdated data.\n\t * 2nd step is to add all missing entries to the UUID tree.\n\t */\n\tret = btrfs_uuid_tree_iterate(fs_info, btrfs_check_uuid_tree_entry);\n\tif (ret < 0) {\n\t\tbtrfs_warn(fs_info, \"iterating uuid_tree failed %d\", ret);\n\t\tup(&fs_info->uuid_tree_rescan_sem);\n\t\treturn ret;\n\t}\n\treturn btrfs_uuid_scan_kthread(data);\n}\n\nint btrfs_create_uuid_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *tree_root = fs_info->tree_root;\n\tstruct btrfs_root *uuid_root;\n\tstruct task_struct *task;\n\tint ret;\n\n\t/*\n\t * 1 - root node\n\t * 1 - root item\n\t */\n\ttrans = btrfs_start_transaction(tree_root, 2);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tuuid_root = btrfs_create_tree(trans, fs_info,\n\t\t\t\t      BTRFS_UUID_TREE_OBJECTID);\n\tif (IS_ERR(uuid_root)) {\n\t\tret = PTR_ERR(uuid_root);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\treturn ret;\n\t}\n\n\tfs_info->uuid_root = uuid_root;\n\n\tret = btrfs_commit_transaction(trans);\n\tif (ret)\n\t\treturn ret;\n\n\tdown(&fs_info->uuid_tree_rescan_sem);\n\ttask = kthread_run(btrfs_uuid_scan_kthread, fs_info, \"btrfs-uuid\");\n\tif (IS_ERR(task)) {\n\t\t/* fs_info->update_uuid_tree_gen remains 0 in all error case */\n\t\tbtrfs_warn(fs_info, \"failed to start uuid_scan task\");\n\t\tup(&fs_info->uuid_tree_rescan_sem);\n\t\treturn PTR_ERR(task);\n\t}\n\n\treturn 0;\n}\n\nint btrfs_check_uuid_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct task_struct *task;\n\n\tdown(&fs_info->uuid_tree_rescan_sem);\n\ttask = kthread_run(btrfs_uuid_rescan_kthread, fs_info, \"btrfs-uuid\");\n\tif (IS_ERR(task)) {\n\t\t/* fs_info->update_uuid_tree_gen remains 0 in all error case */\n\t\tbtrfs_warn(fs_info, \"failed to start uuid_rescan task\");\n\t\tup(&fs_info->uuid_tree_rescan_sem);\n\t\treturn PTR_ERR(task);\n\t}\n\n\treturn 0;\n}\n\n/*\n * shrinking a device means finding all of the device extents past\n * the new size, and then following the back refs to the chunks.\n * The chunk relocation code actually frees the device extent\n */\nint btrfs_shrink_device(struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_dev_extent *dev_extent = NULL;\n\tstruct btrfs_path *path;\n\tu64 length;\n\tu64 chunk_offset;\n\tint ret;\n\tint slot;\n\tint failed = 0;\n\tbool retried = false;\n\tbool checked_pending_chunks = false;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tu64 old_total = btrfs_super_total_bytes(super_copy);\n\tu64 old_size = btrfs_device_get_total_bytes(device);\n\tu64 diff;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\tdiff = round_down(old_size - new_size, fs_info->sectorsize);\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\treturn -EINVAL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_BACK;\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes -= diff;\n\t\tatomic64_sub(diff, &fs_info->free_chunk_space);\n\t}\n\tmutex_unlock(&fs_info->chunk_mutex);\n\nagain:\n\tkey.objectid = device->devid;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tdo {\n\t\tmutex_lock(&fs_info->delete_unused_bgs_mutex);\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_previous_item(root, path, 0, key.type);\n\t\tif (ret)\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\tif (ret < 0)\n\t\t\tgoto done;\n\t\tif (ret) {\n\t\t\tret = 0;\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(l, &key, path->slots[0]);\n\n\t\tif (key.objectid != device->devid) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\tlength = btrfs_dev_extent_length(l, dev_extent);\n\n\t\tif (key.offset + length <= new_size) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\n\t\tbtrfs_release_path(path);\n\n\t\t/*\n\t\t * We may be relocating the only data chunk we have,\n\t\t * which could potentially end up with losing data's\n\t\t * raid profile, so lets allocate an empty one in\n\t\t * advance.\n\t\t */\n\t\tret = btrfs_may_alloc_data_chunk(fs_info, chunk_offset);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, chunk_offset);\n\t\tmutex_unlock(&fs_info->delete_unused_bgs_mutex);\n\t\tif (ret == -ENOSPC) {\n\t\t\tfailed++;\n\t\t} else if (ret) {\n\t\t\tif (ret == -ETXTBSY) {\n\t\t\t\tbtrfs_warn(fs_info,\n\t\t   \"could not shrink block group %llu due to active swapfile\",\n\t\t\t\t\t   chunk_offset);\n\t\t\t}\n\t\t\tgoto done;\n\t\t}\n\t} while (key.offset-- > 0);\n\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (failed && retried) {\n\t\tret = -ENOSPC;\n\t\tgoto done;\n\t}\n\n\t/* Shrinking succeeded, else we would be at \"done\". */\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto done;\n\t}\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\t/*\n\t * We checked in the above loop all device extents that were already in\n\t * the device tree. However before we have updated the device's\n\t * total_bytes to the new size, we might have had chunk allocations that\n\t * have not complete yet (new block groups attached to transaction\n\t * handles), and therefore their device extents were not yet in the\n\t * device tree and we missed them in the loop above. So if we have any\n\t * pending chunk using a device extent that overlaps the device range\n\t * that we can not use anymore, commit the current transaction and\n\t * repeat the search on the device tree - this way we guarantee we will\n\t * not have chunks using device extents that end beyond 'new_size'.\n\t */\n\tif (!checked_pending_chunks) {\n\t\tu64 start = new_size;\n\t\tu64 len = old_size - new_size;\n\n\t\tif (contains_pending_extent(trans->transaction, device,\n\t\t\t\t\t    &start, len)) {\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t\tchecked_pending_chunks = true;\n\t\t\tfailed = 0;\n\t\t\tretried = false;\n\t\t\tret = btrfs_commit_transaction(trans);\n\t\t\tif (ret)\n\t\t\t\tgoto done;\n\t\t\tgoto again;\n\t\t}\n\t}\n\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tif (list_empty(&device->resized_list))\n\t\tlist_add_tail(&device->resized_list,\n\t\t\t      &fs_info->fs_devices->resized_devices);\n\n\tWARN_ON(diff > old_total);\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total - diff, fs_info->sectorsize));\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t/* Now btrfs_update_device() will change the on-disk size. */\n\tret = btrfs_update_device(trans, device);\n\tif (ret < 0) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t} else {\n\t\tret = btrfs_commit_transaction(trans);\n\t}\ndone:\n\tbtrfs_free_path(path);\n\tif (ret) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tbtrfs_device_set_total_bytes(device, old_size);\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\t\tdevice->fs_devices->total_rw_bytes += diff;\n\t\tatomic64_add(diff, &fs_info->free_chunk_space);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_add_system_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t   struct btrfs_key *key,\n\t\t\t   struct btrfs_chunk *chunk, int item_size)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key disk_key;\n\tu32 array_size;\n\tu8 *ptr;\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\tif (array_size + item_size + sizeof(disk_key)\n\t\t\t> BTRFS_SYSTEM_CHUNK_ARRAY_SIZE) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\treturn -EFBIG;\n\t}\n\n\tptr = super_copy->sys_chunk_array + array_size;\n\tbtrfs_cpu_key_to_disk(&disk_key, key);\n\tmemcpy(ptr, &disk_key, sizeof(disk_key));\n\tptr += sizeof(disk_key);\n\tmemcpy(ptr, chunk, item_size);\n\titem_size += sizeof(disk_key);\n\tbtrfs_set_super_sys_array_size(super_copy, array_size + item_size);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\treturn 0;\n}\n\n/*\n * sort the devices in descending order by max_avail, total_avail\n */\nstatic int btrfs_cmp_device_info(const void *a, const void *b)\n{\n\tconst struct btrfs_device_info *di_a = a;\n\tconst struct btrfs_device_info *di_b = b;\n\n\tif (di_a->max_avail > di_b->max_avail)\n\t\treturn -1;\n\tif (di_a->max_avail < di_b->max_avail)\n\t\treturn 1;\n\tif (di_a->total_avail > di_b->total_avail)\n\t\treturn -1;\n\tif (di_a->total_avail < di_b->total_avail)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void check_raid56_incompat_flag(struct btrfs_fs_info *info, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\treturn;\n\n\tbtrfs_set_fs_incompat(info, RAID56);\n}\n\n#define BTRFS_MAX_DEVS(info) ((BTRFS_MAX_ITEM_SIZE(info)\t\\\n\t\t\t- sizeof(struct btrfs_chunk))\t\t\\\n\t\t\t/ sizeof(struct btrfs_stripe) + 1)\n\n#define BTRFS_MAX_DEVS_SYS_CHUNK ((BTRFS_SYSTEM_CHUNK_ARRAY_SIZE\t\\\n\t\t\t\t- 2 * sizeof(struct btrfs_disk_key)\t\\\n\t\t\t\t- 2 * sizeof(struct btrfs_chunk))\t\\\n\t\t\t\t/ sizeof(struct btrfs_stripe) + 1)\n\nstatic int __btrfs_alloc_chunk(struct btrfs_trans_handle *trans,\n\t\t\t       u64 start, u64 type)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = info->fs_devices;\n\tstruct btrfs_device *device;\n\tstruct map_lookup *map = NULL;\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_device_info *devices_info = NULL;\n\tu64 total_avail;\n\tint num_stripes;\t/* total number of stripes to allocate */\n\tint data_stripes;\t/* number of stripes that count for\n\t\t\t\t   block group size */\n\tint sub_stripes;\t/* sub_stripes info for map */\n\tint dev_stripes;\t/* stripes per dev */\n\tint devs_max;\t\t/* max devs to use */\n\tint devs_min;\t\t/* min devs needed */\n\tint devs_increment;\t/* ndevs has to be a multiple of this */\n\tint ncopies;\t\t/* how many copies to data has */\n\tint nparity;\t\t/* number of stripes worth of bytes to\n\t\t\t\t   store parity information */\n\tint ret;\n\tu64 max_stripe_size;\n\tu64 max_chunk_size;\n\tu64 stripe_size;\n\tu64 chunk_size;\n\tint ndevs;\n\tint i;\n\tint j;\n\tint index;\n\n\tBUG_ON(!alloc_profile_is_valid(type, 0));\n\n\tif (list_empty(&fs_devices->alloc_list)) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\tbtrfs_debug(info, \"%s: no writable device\", __func__);\n\t\treturn -ENOSPC;\n\t}\n\n\tindex = btrfs_bg_flags_to_raid_index(type);\n\n\tsub_stripes = btrfs_raid_array[index].sub_stripes;\n\tdev_stripes = btrfs_raid_array[index].dev_stripes;\n\tdevs_max = btrfs_raid_array[index].devs_max;\n\tdevs_min = btrfs_raid_array[index].devs_min;\n\tdevs_increment = btrfs_raid_array[index].devs_increment;\n\tncopies = btrfs_raid_array[index].ncopies;\n\tnparity = btrfs_raid_array[index].nparity;\n\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tmax_stripe_size = SZ_1G;\n\t\tmax_chunk_size = BTRFS_MAX_DATA_CHUNK_SIZE;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\t/* for larger filesystems, use larger metadata chunks */\n\t\tif (fs_devices->total_rw_bytes > 50ULL * SZ_1G)\n\t\t\tmax_stripe_size = SZ_1G;\n\t\telse\n\t\t\tmax_stripe_size = SZ_256M;\n\t\tmax_chunk_size = max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tmax_stripe_size = SZ_32M;\n\t\tmax_chunk_size = 2 * max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS_SYS_CHUNK;\n\t} else {\n\t\tbtrfs_err(info, \"invalid chunk type 0x%llx requested\",\n\t\t       type);\n\t\tBUG_ON(1);\n\t}\n\n\t/* We don't want a chunk larger than 10% of writable space */\n\tmax_chunk_size = min(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t     max_chunk_size);\n\n\tdevices_info = kcalloc(fs_devices->rw_devices, sizeof(*devices_info),\n\t\t\t       GFP_NOFS);\n\tif (!devices_info)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * in the first pass through the devices list, we gather information\n\t * about the available holes on each device.\n\t */\n\tndevs = 0;\n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tu64 max_avail;\n\t\tu64 dev_offset;\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tWARN(1, KERN_ERR\n\t\t\t       \"BTRFS: read-only device in alloc_list\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t&device->dev_state) ||\n\t\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tif (device->total_bytes > device->bytes_used)\n\t\t\ttotal_avail = device->total_bytes - device->bytes_used;\n\t\telse\n\t\t\ttotal_avail = 0;\n\n\t\t/* If there is no space on this device, skip it. */\n\t\tif (total_avail == 0)\n\t\t\tcontinue;\n\n\t\tret = find_free_dev_extent(trans, device,\n\t\t\t\t\t   max_stripe_size * dev_stripes,\n\t\t\t\t\t   &dev_offset, &max_avail);\n\t\tif (ret && ret != -ENOSPC)\n\t\t\tgoto error;\n\n\t\tif (ret == 0)\n\t\t\tmax_avail = max_stripe_size * dev_stripes;\n\n\t\tif (max_avail < BTRFS_STRIPE_LEN * dev_stripes) {\n\t\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\t\tbtrfs_debug(info,\n\t\t\t\"%s: devid %llu has no free space, have=%llu want=%u\",\n\t\t\t\t\t    __func__, device->devid, max_avail,\n\t\t\t\t\t    BTRFS_STRIPE_LEN * dev_stripes);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ndevs == fs_devices->rw_devices) {\n\t\t\tWARN(1, \"%s: found more than %llu devices\\n\",\n\t\t\t     __func__, fs_devices->rw_devices);\n\t\t\tbreak;\n\t\t}\n\t\tdevices_info[ndevs].dev_offset = dev_offset;\n\t\tdevices_info[ndevs].max_avail = max_avail;\n\t\tdevices_info[ndevs].total_avail = total_avail;\n\t\tdevices_info[ndevs].dev = device;\n\t\t++ndevs;\n\t}\n\n\t/*\n\t * now sort the devices by hole size / available space\n\t */\n\tsort(devices_info, ndevs, sizeof(struct btrfs_device_info),\n\t     btrfs_cmp_device_info, NULL);\n\n\t/* round down to number of usable stripes */\n\tndevs = round_down(ndevs, devs_increment);\n\n\tif (ndevs < devs_min) {\n\t\tret = -ENOSPC;\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG)) {\n\t\t\tbtrfs_debug(info,\n\t\"%s: not enough devices with free space: have=%d minimum required=%d\",\n\t\t\t\t    __func__, ndevs, devs_min);\n\t\t}\n\t\tgoto error;\n\t}\n\n\tndevs = min(ndevs, devs_max);\n\n\t/*\n\t * The primary goal is to maximize the number of stripes, so use as\n\t * many devices as possible, even if the stripes are not maximum sized.\n\t *\n\t * The DUP profile stores more than one stripe per device, the\n\t * max_avail is the total size so we have to adjust.\n\t */\n\tstripe_size = div_u64(devices_info[ndevs - 1].max_avail, dev_stripes);\n\tnum_stripes = ndevs * dev_stripes;\n\n\t/*\n\t * this will have to be fixed for RAID1 and RAID10 over\n\t * more drives\n\t */\n\tdata_stripes = (num_stripes - nparity) / ncopies;\n\n\t/*\n\t * Use the number of data stripes to figure out how big this chunk\n\t * is really going to be in terms of logical address space,\n\t * and compare that answer with the max chunk size. If it's higher,\n\t * we try to reduce stripe_size.\n\t */\n\tif (stripe_size * data_stripes > max_chunk_size) {\n\t\t/*\n\t\t * Reduce stripe_size, round it up to a 16MB boundary again and\n\t\t * then use it, unless it ends up being even bigger than the\n\t\t * previous value we had already.\n\t\t */\n\t\tstripe_size = min(round_up(div_u64(max_chunk_size,\n\t\t\t\t\t\t   data_stripes), SZ_16M),\n\t\t\t\t  stripe_size);\n\t}\n\n\t/* align to BTRFS_STRIPE_LEN */\n\tstripe_size = round_down(stripe_size, BTRFS_STRIPE_LEN);\n\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tmap->num_stripes = num_stripes;\n\n\tfor (i = 0; i < ndevs; ++i) {\n\t\tfor (j = 0; j < dev_stripes; ++j) {\n\t\t\tint s = i * dev_stripes + j;\n\t\t\tmap->stripes[s].dev = devices_info[i].dev;\n\t\t\tmap->stripes[s].physical = devices_info[i].dev_offset +\n\t\t\t\t\t\t   j * stripe_size;\n\t\t}\n\t}\n\tmap->stripe_len = BTRFS_STRIPE_LEN;\n\tmap->io_align = BTRFS_STRIPE_LEN;\n\tmap->io_width = BTRFS_STRIPE_LEN;\n\tmap->type = type;\n\tmap->sub_stripes = sub_stripes;\n\n\tchunk_size = stripe_size * data_stripes;\n\n\ttrace_btrfs_chunk_alloc(info, map, start, chunk_size);\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tkfree(map);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = start;\n\tem->len = chunk_size;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\tem->orig_block_len = stripe_size;\n\n\tem_tree = &info->mapping_tree.map_tree;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em, 0);\n\tif (ret) {\n\t\twrite_unlock(&em_tree->lock);\n\t\tfree_extent_map(em);\n\t\tgoto error;\n\t}\n\n\tlist_add_tail(&em->list, &trans->transaction->pending_chunks);\n\trefcount_inc(&em->refs);\n\twrite_unlock(&em_tree->lock);\n\n\tret = btrfs_make_block_group(trans, 0, type, start, chunk_size);\n\tif (ret)\n\t\tgoto error_del_extent;\n\n\tfor (i = 0; i < map->num_stripes; i++)\n\t\tbtrfs_device_set_bytes_used(map->stripes[i].dev,\n\t\t\t\tmap->stripes[i].dev->bytes_used + stripe_size);\n\n\tatomic64_sub(stripe_size * map->num_stripes, &info->free_chunk_space);\n\n\tfree_extent_map(em);\n\tcheck_raid56_incompat_flag(info, type);\n\n\tkfree(devices_info);\n\treturn 0;\n\nerror_del_extent:\n\twrite_lock(&em_tree->lock);\n\tremove_extent_mapping(em_tree, em);\n\twrite_unlock(&em_tree->lock);\n\n\t/* One for our allocation */\n\tfree_extent_map(em);\n\t/* One for the tree reference */\n\tfree_extent_map(em);\n\t/* One for the pending_chunks list reference */\n\tfree_extent_map(em);\nerror:\n\tkfree(devices_info);\n\treturn ret;\n}\n\nint btrfs_finish_chunk_alloc(struct btrfs_trans_handle *trans,\n\t\t\t     u64 chunk_offset, u64 chunk_size)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *extent_root = fs_info->extent_root;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_device *device;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_stripe *stripe;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tsize_t item_size;\n\tu64 dev_offset;\n\tu64 stripe_size;\n\tint i = 0;\n\tint ret = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, chunk_size);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\titem_size = btrfs_chunk_item_size(map->num_stripes);\n\tstripe_size = em->orig_block_len;\n\n\tchunk = kzalloc(item_size, GFP_NOFS);\n\tif (!chunk) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Take the device list mutex to prevent races with the final phase of\n\t * a device replace operation that replaces the device object associated\n\t * with the map's stripes, because the device object's id can change\n\t * at any time during that final phase of the device replace operation\n\t * (dev-replace.c:btrfs_dev_replace_finishing()).\n\t */\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tdevice = map->stripes[i].dev;\n\t\tdev_offset = map->stripes[i].physical;\n\n\t\tret = btrfs_update_device(trans, device);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = btrfs_alloc_dev_extent(trans, device, chunk_offset,\n\t\t\t\t\t     dev_offset, stripe_size);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tif (ret) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tgoto out;\n\t}\n\n\tstripe = &chunk->stripe;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tdevice = map->stripes[i].dev;\n\t\tdev_offset = map->stripes[i].physical;\n\n\t\tbtrfs_set_stack_stripe_devid(stripe, device->devid);\n\t\tbtrfs_set_stack_stripe_offset(stripe, dev_offset);\n\t\tmemcpy(stripe->dev_uuid, device->uuid, BTRFS_UUID_SIZE);\n\t\tstripe++;\n\t}\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\tbtrfs_set_stack_chunk_length(chunk, chunk_size);\n\tbtrfs_set_stack_chunk_owner(chunk, extent_root->root_key.objectid);\n\tbtrfs_set_stack_chunk_stripe_len(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_type(chunk, map->type);\n\tbtrfs_set_stack_chunk_num_stripes(chunk, map->num_stripes);\n\tbtrfs_set_stack_chunk_io_align(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_io_width(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_sector_size(chunk, fs_info->sectorsize);\n\tbtrfs_set_stack_chunk_sub_stripes(chunk, map->sub_stripes);\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\tkey.offset = chunk_offset;\n\n\tret = btrfs_insert_item(trans, chunk_root, &key, chunk, item_size);\n\tif (ret == 0 && map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\t/*\n\t\t * TODO: Cleanup of inserted chunk root in case of\n\t\t * failure.\n\t\t */\n\t\tret = btrfs_add_system_chunk(fs_info, &key, chunk, item_size);\n\t}\n\nout:\n\tkfree(chunk);\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * Chunk allocation falls into two parts. The first part does work\n * that makes the new allocated chunk usable, but does not do any operation\n * that modifies the chunk tree. The second part does the work that\n * requires modifying the chunk tree. This division is important for the\n * bootstrap process of adding storage to a seed btrfs.\n */\nint btrfs_alloc_chunk(struct btrfs_trans_handle *trans, u64 type)\n{\n\tu64 chunk_offset;\n\n\tlockdep_assert_held(&trans->fs_info->chunk_mutex);\n\tchunk_offset = find_next_chunk(trans->fs_info);\n\treturn __btrfs_alloc_chunk(trans, chunk_offset, type);\n}\n\nstatic noinline int init_first_rw_device(struct btrfs_trans_handle *trans,\n\t\t\t\t\t struct btrfs_fs_info *fs_info)\n{\n\tu64 chunk_offset;\n\tu64 sys_chunk_offset;\n\tu64 alloc_profile;\n\tint ret;\n\n\tchunk_offset = find_next_chunk(fs_info);\n\talloc_profile = btrfs_metadata_alloc_profile(fs_info);\n\tret = __btrfs_alloc_chunk(trans, chunk_offset, alloc_profile);\n\tif (ret)\n\t\treturn ret;\n\n\tsys_chunk_offset = find_next_chunk(fs_info);\n\talloc_profile = btrfs_system_alloc_profile(fs_info);\n\tret = __btrfs_alloc_chunk(trans, sys_chunk_offset, alloc_profile);\n\treturn ret;\n}\n\nstatic inline int btrfs_chunk_max_errors(struct map_lookup *map)\n{\n\tint max_errors;\n\n\tif (map->type & (BTRFS_BLOCK_GROUP_RAID1 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID10 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID5 |\n\t\t\t BTRFS_BLOCK_GROUP_DUP)) {\n\t\tmax_errors = 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID6) {\n\t\tmax_errors = 2;\n\t} else {\n\t\tmax_errors = 0;\n\t}\n\n\treturn max_errors;\n}\n\nint btrfs_chunk_readonly(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint readonly = 0;\n\tint miss_ndevs = 0;\n\tint i;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em))\n\t\treturn 1;\n\n\tmap = em->map_lookup;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\tmiss_ndevs++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\treadonly = 1;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\t/*\n\t * If the number of missing devices is larger than max errors,\n\t * we can not write the data into that chunk successfully, so\n\t * set it readonly.\n\t */\n\tif (miss_ndevs > btrfs_chunk_max_errors(map))\n\t\treadonly = 1;\nend:\n\tfree_extent_map(em);\n\treturn readonly;\n}\n\nvoid btrfs_mapping_init(struct btrfs_mapping_tree *tree)\n{\n\textent_map_tree_init(&tree->map_tree);\n}\n\nvoid btrfs_mapping_tree_free(struct btrfs_mapping_tree *tree)\n{\n\tstruct extent_map *em;\n\n\twhile (1) {\n\t\twrite_lock(&tree->map_tree.lock);\n\t\tem = lookup_extent_mapping(&tree->map_tree, 0, (u64)-1);\n\t\tif (em)\n\t\t\tremove_extent_mapping(&tree->map_tree, em);\n\t\twrite_unlock(&tree->map_tree.lock);\n\t\tif (!em)\n\t\t\tbreak;\n\t\t/* once for us */\n\t\tfree_extent_map(em);\n\t\t/* once for the tree */\n\t\tfree_extent_map(em);\n\t}\n}\n\nint btrfs_num_copies(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint ret;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\tif (IS_ERR(em))\n\t\t/*\n\t\t * We could return errors for these cases, but that could get\n\t\t * ugly and we'd probably do the same thing which is just not do\n\t\t * anything else and exit, so return 1 so the callers don't try\n\t\t * to use other copies.\n\t\t */\n\t\treturn 1;\n\n\tmap = em->map_lookup;\n\tif (map->type & (BTRFS_BLOCK_GROUP_DUP | BTRFS_BLOCK_GROUP_RAID1))\n\t\tret = map->num_stripes;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tret = map->sub_stripes;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID5)\n\t\tret = 2;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID6)\n\t\t/*\n\t\t * There could be two corrupted data stripes, we need\n\t\t * to loop retry in order to rebuild the correct data.\n\t\t *\n\t\t * Fail a stripe at a time on every retry except the\n\t\t * stripe under reconstruction.\n\t\t */\n\t\tret = map->num_stripes;\n\telse\n\t\tret = 1;\n\tfree_extent_map(em);\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (btrfs_dev_replace_is_ongoing(&fs_info->dev_replace) &&\n\t    fs_info->dev_replace.tgtdev)\n\t\tret++;\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\treturn ret;\n}\n\nunsigned long btrfs_full_stripe_len(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 logical)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tunsigned long len = fs_info->sectorsize;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif (!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tlen = map->stripe_len * nr_data_stripes(map);\n\t\tfree_extent_map(em);\n\t}\n\treturn len;\n}\n\nint btrfs_is_parity_mirror(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint ret = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif(!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tret = 1;\n\t\tfree_extent_map(em);\n\t}\n\treturn ret;\n}\n\nstatic int find_live_mirror(struct btrfs_fs_info *fs_info,\n\t\t\t    struct map_lookup *map, int first,\n\t\t\t    int dev_replace_is_ongoing)\n{\n\tint i;\n\tint num_stripes;\n\tint preferred_mirror;\n\tint tolerance;\n\tstruct btrfs_device *srcdev;\n\n\tASSERT((map->type &\n\t\t (BTRFS_BLOCK_GROUP_RAID1 | BTRFS_BLOCK_GROUP_RAID10)));\n\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tnum_stripes = map->sub_stripes;\n\telse\n\t\tnum_stripes = map->num_stripes;\n\n\tpreferred_mirror = first + current->pid % num_stripes;\n\n\tif (dev_replace_is_ongoing &&\n\t    fs_info->dev_replace.cont_reading_from_srcdev_mode ==\n\t     BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_AVOID)\n\t\tsrcdev = fs_info->dev_replace.srcdev;\n\telse\n\t\tsrcdev = NULL;\n\n\t/*\n\t * try to avoid the drive that is the source drive for a\n\t * dev-replace procedure, only choose it if no other non-missing\n\t * mirror is available\n\t */\n\tfor (tolerance = 0; tolerance < 2; tolerance++) {\n\t\tif (map->stripes[preferred_mirror].dev->bdev &&\n\t\t    (tolerance || map->stripes[preferred_mirror].dev != srcdev))\n\t\t\treturn preferred_mirror;\n\t\tfor (i = first; i < first + num_stripes; i++) {\n\t\t\tif (map->stripes[i].dev->bdev &&\n\t\t\t    (tolerance || map->stripes[i].dev != srcdev))\n\t\t\t\treturn i;\n\t\t}\n\t}\n\n\t/* we couldn't find one that doesn't fail.  Just return something\n\t * and the io error handling code will clean up eventually\n\t */\n\treturn preferred_mirror;\n}\n\nstatic inline int parity_smaller(u64 a, u64 b)\n{\n\treturn a > b;\n}\n\n/* Bubble-sort the stripe set to put the parity/syndrome stripes last */\nstatic void sort_parity_stripes(struct btrfs_bio *bbio, int num_stripes)\n{\n\tstruct btrfs_bio_stripe s;\n\tint i;\n\tu64 l;\n\tint again = 1;\n\n\twhile (again) {\n\t\tagain = 0;\n\t\tfor (i = 0; i < num_stripes - 1; i++) {\n\t\t\tif (parity_smaller(bbio->raid_map[i],\n\t\t\t\t\t   bbio->raid_map[i+1])) {\n\t\t\t\ts = bbio->stripes[i];\n\t\t\t\tl = bbio->raid_map[i];\n\t\t\t\tbbio->stripes[i] = bbio->stripes[i+1];\n\t\t\t\tbbio->raid_map[i] = bbio->raid_map[i+1];\n\t\t\t\tbbio->stripes[i+1] = s;\n\t\t\t\tbbio->raid_map[i+1] = l;\n\n\t\t\t\tagain = 1;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic struct btrfs_bio *alloc_btrfs_bio(int total_stripes, int real_stripes)\n{\n\tstruct btrfs_bio *bbio = kzalloc(\n\t\t /* the size of the btrfs_bio */\n\t\tsizeof(struct btrfs_bio) +\n\t\t/* plus the variable array for the stripes */\n\t\tsizeof(struct btrfs_bio_stripe) * (total_stripes) +\n\t\t/* plus the variable array for the tgt dev */\n\t\tsizeof(int) * (real_stripes) +\n\t\t/*\n\t\t * plus the raid_map, which includes both the tgt dev\n\t\t * and the stripes\n\t\t */\n\t\tsizeof(u64) * (total_stripes),\n\t\tGFP_NOFS|__GFP_NOFAIL);\n\n\tatomic_set(&bbio->error, 0);\n\trefcount_set(&bbio->refs, 1);\n\n\treturn bbio;\n}\n\nvoid btrfs_get_bbio(struct btrfs_bio *bbio)\n{\n\tWARN_ON(!refcount_read(&bbio->refs));\n\trefcount_inc(&bbio->refs);\n}\n\nvoid btrfs_put_bbio(struct btrfs_bio *bbio)\n{\n\tif (!bbio)\n\t\treturn;\n\tif (refcount_dec_and_test(&bbio->refs))\n\t\tkfree(bbio);\n}\n\n/* can REQ_OP_DISCARD be sent with other REQ like REQ_OP_WRITE? */\n/*\n * Please note that, discard won't be sent to target device of device\n * replace.\n */\nstatic int __btrfs_map_block_for_discard(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t u64 logical, u64 length,\n\t\t\t\t\t struct btrfs_bio **bbio_ret)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_bio *bbio;\n\tu64 offset;\n\tu64 stripe_nr;\n\tu64 stripe_nr_end;\n\tu64 stripe_end_offset;\n\tu64 stripe_cnt;\n\tu64 stripe_len;\n\tu64 stripe_offset;\n\tu64 num_stripes;\n\tu32 stripe_index;\n\tu32 factor = 0;\n\tu32 sub_stripes = 0;\n\tu64 stripes_per_dev = 0;\n\tu32 remaining_stripes = 0;\n\tu32 last_stripe = 0;\n\tint ret = 0;\n\tint i;\n\n\t/* discard always return a bbio */\n\tASSERT(bbio_ret);\n\n\tem = btrfs_get_chunk_map(fs_info, logical, length);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\t/* we don't discard raid56 yet */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\toffset = logical - em->start;\n\tlength = min_t(u64, em->len - offset, length);\n\n\tstripe_len = map->stripe_len;\n\t/*\n\t * stripe_nr counts the total number of stripes we have to stride\n\t * to get to this block\n\t */\n\tstripe_nr = div64_u64(offset, stripe_len);\n\n\t/* stripe_offset is the offset of this block in its stripe */\n\tstripe_offset = offset - stripe_nr * stripe_len;\n\n\tstripe_nr_end = round_up(offset + length, map->stripe_len);\n\tstripe_nr_end = div64_u64(stripe_nr_end, map->stripe_len);\n\tstripe_cnt = stripe_nr_end - stripe_nr;\n\tstripe_end_offset = stripe_nr_end * map->stripe_len -\n\t\t\t    (offset + length);\n\t/*\n\t * after this, stripe_nr is the number of stripes on this\n\t * device we have to walk to find the data, and stripe_index is\n\t * the number of our device in the stripe array\n\t */\n\tnum_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID0)\n\t\t\tsub_stripes = 1;\n\t\telse\n\t\t\tsub_stripes = map->sub_stripes;\n\n\t\tfactor = map->num_stripes / sub_stripes;\n\t\tnum_stripes = min_t(u64, map->num_stripes,\n\t\t\t\t    sub_stripes * stripe_cnt);\n\t\tstripe_nr = div_u64_rem(stripe_nr, factor, &stripe_index);\n\t\tstripe_index *= sub_stripes;\n\t\tstripes_per_dev = div_u64_rem(stripe_cnt, factor,\n\t\t\t\t\t      &remaining_stripes);\n\t\tdiv_u64_rem(stripe_nr_end - 1, factor, &last_stripe);\n\t\tlast_stripe *= sub_stripes;\n\t} else if (map->type & (BTRFS_BLOCK_GROUP_RAID1 |\n\t\t\t\tBTRFS_BLOCK_GROUP_DUP)) {\n\t\tnum_stripes = map->num_stripes;\n\t} else {\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t\t&stripe_index);\n\t}\n\n\tbbio = alloc_btrfs_bio(num_stripes, 0);\n\tif (!bbio) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tbbio->stripes[i].physical =\n\t\t\tmap->stripes[stripe_index].physical +\n\t\t\tstripe_offset + stripe_nr * map->stripe_len;\n\t\tbbio->stripes[i].dev = map->stripes[stripe_index].dev;\n\n\t\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\t\tbbio->stripes[i].length = stripes_per_dev *\n\t\t\t\tmap->stripe_len;\n\n\t\t\tif (i / sub_stripes < remaining_stripes)\n\t\t\t\tbbio->stripes[i].length +=\n\t\t\t\t\tmap->stripe_len;\n\n\t\t\t/*\n\t\t\t * Special for the first stripe and\n\t\t\t * the last stripe:\n\t\t\t *\n\t\t\t * |-------|...|-------|\n\t\t\t *     |----------|\n\t\t\t *    off     end_off\n\t\t\t */\n\t\t\tif (i < sub_stripes)\n\t\t\t\tbbio->stripes[i].length -=\n\t\t\t\t\tstripe_offset;\n\n\t\t\tif (stripe_index >= last_stripe &&\n\t\t\t    stripe_index <= (last_stripe +\n\t\t\t\t\t     sub_stripes - 1))\n\t\t\t\tbbio->stripes[i].length -=\n\t\t\t\t\tstripe_end_offset;\n\n\t\t\tif (i == sub_stripes - 1)\n\t\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\tbbio->stripes[i].length = length;\n\t\t}\n\n\t\tstripe_index++;\n\t\tif (stripe_index == map->num_stripes) {\n\t\t\tstripe_index = 0;\n\t\t\tstripe_nr++;\n\t\t}\n\t}\n\n\t*bbio_ret = bbio;\n\tbbio->map_type = map->type;\n\tbbio->num_stripes = num_stripes;\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * In dev-replace case, for repair case (that's the only case where the mirror\n * is selected explicitly when calling btrfs_map_block), blocks left of the\n * left cursor can also be read from the target drive.\n *\n * For REQ_GET_READ_MIRRORS, the target drive is added as the last one to the\n * array of stripes.\n * For READ, it also needs to be supported using the same mirror number.\n *\n * If the requested block is not left of the left cursor, EIO is returned. This\n * can happen because btrfs_num_copies() returns one more in the dev-replace\n * case.\n */\nstatic int get_extra_mirror_from_replace(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t u64 logical, u64 length,\n\t\t\t\t\t u64 srcdev_devid, int *mirror_num,\n\t\t\t\t\t u64 *physical)\n{\n\tstruct btrfs_bio *bbio = NULL;\n\tint num_stripes;\n\tint index_srcdev = 0;\n\tint found = 0;\n\tu64 physical_of_found = 0;\n\tint i;\n\tint ret = 0;\n\n\tret = __btrfs_map_block(fs_info, BTRFS_MAP_GET_READ_MIRRORS,\n\t\t\t\tlogical, &length, &bbio, 0, 0);\n\tif (ret) {\n\t\tASSERT(bbio == NULL);\n\t\treturn ret;\n\t}\n\n\tnum_stripes = bbio->num_stripes;\n\tif (*mirror_num > num_stripes) {\n\t\t/*\n\t\t * BTRFS_MAP_GET_READ_MIRRORS does not contain this mirror,\n\t\t * that means that the requested area is not left of the left\n\t\t * cursor\n\t\t */\n\t\tbtrfs_put_bbio(bbio);\n\t\treturn -EIO;\n\t}\n\n\t/*\n\t * process the rest of the function using the mirror_num of the source\n\t * drive. Therefore look it up first.  At the end, patch the device\n\t * pointer to the one of the target drive.\n\t */\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tif (bbio->stripes[i].dev->devid != srcdev_devid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * In case of DUP, in order to keep it simple, only add the\n\t\t * mirror with the lowest physical address\n\t\t */\n\t\tif (found &&\n\t\t    physical_of_found <= bbio->stripes[i].physical)\n\t\t\tcontinue;\n\n\t\tindex_srcdev = i;\n\t\tfound = 1;\n\t\tphysical_of_found = bbio->stripes[i].physical;\n\t}\n\n\tbtrfs_put_bbio(bbio);\n\n\tASSERT(found);\n\tif (!found)\n\t\treturn -EIO;\n\n\t*mirror_num = index_srcdev + 1;\n\t*physical = physical_of_found;\n\treturn ret;\n}\n\nstatic void handle_ops_on_dev_replace(enum btrfs_map_op op,\n\t\t\t\t      struct btrfs_bio **bbio_ret,\n\t\t\t\t      struct btrfs_dev_replace *dev_replace,\n\t\t\t\t      int *num_stripes_ret, int *max_errors_ret)\n{\n\tstruct btrfs_bio *bbio = *bbio_ret;\n\tu64 srcdev_devid = dev_replace->srcdev->devid;\n\tint tgtdev_indexes = 0;\n\tint num_stripes = *num_stripes_ret;\n\tint max_errors = *max_errors_ret;\n\tint i;\n\n\tif (op == BTRFS_MAP_WRITE) {\n\t\tint index_where_to_add;\n\n\t\t/*\n\t\t * duplicate the write operations while the dev replace\n\t\t * procedure is running. Since the copying of the old disk to\n\t\t * the new disk takes place at run time while the filesystem is\n\t\t * mounted writable, the regular write operations to the old\n\t\t * disk have to be duplicated to go to the new disk as well.\n\t\t *\n\t\t * Note that device->missing is handled by the caller, and that\n\t\t * the write to the old disk is already set up in the stripes\n\t\t * array.\n\t\t */\n\t\tindex_where_to_add = num_stripes;\n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tif (bbio->stripes[i].dev->devid == srcdev_devid) {\n\t\t\t\t/* write to new disk, too */\n\t\t\t\tstruct btrfs_bio_stripe *new =\n\t\t\t\t\tbbio->stripes + index_where_to_add;\n\t\t\t\tstruct btrfs_bio_stripe *old =\n\t\t\t\t\tbbio->stripes + i;\n\n\t\t\t\tnew->physical = old->physical;\n\t\t\t\tnew->length = old->length;\n\t\t\t\tnew->dev = dev_replace->tgtdev;\n\t\t\t\tbbio->tgtdev_map[i] = index_where_to_add;\n\t\t\t\tindex_where_to_add++;\n\t\t\t\tmax_errors++;\n\t\t\t\ttgtdev_indexes++;\n\t\t\t}\n\t\t}\n\t\tnum_stripes = index_where_to_add;\n\t} else if (op == BTRFS_MAP_GET_READ_MIRRORS) {\n\t\tint index_srcdev = 0;\n\t\tint found = 0;\n\t\tu64 physical_of_found = 0;\n\n\t\t/*\n\t\t * During the dev-replace procedure, the target drive can also\n\t\t * be used to read data in case it is needed to repair a corrupt\n\t\t * block elsewhere. This is possible if the requested area is\n\t\t * left of the left cursor. In this area, the target drive is a\n\t\t * full copy of the source drive.\n\t\t */\n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tif (bbio->stripes[i].dev->devid == srcdev_devid) {\n\t\t\t\t/*\n\t\t\t\t * In case of DUP, in order to keep it simple,\n\t\t\t\t * only add the mirror with the lowest physical\n\t\t\t\t * address\n\t\t\t\t */\n\t\t\t\tif (found &&\n\t\t\t\t    physical_of_found <=\n\t\t\t\t     bbio->stripes[i].physical)\n\t\t\t\t\tcontinue;\n\t\t\t\tindex_srcdev = i;\n\t\t\t\tfound = 1;\n\t\t\t\tphysical_of_found = bbio->stripes[i].physical;\n\t\t\t}\n\t\t}\n\t\tif (found) {\n\t\t\tstruct btrfs_bio_stripe *tgtdev_stripe =\n\t\t\t\tbbio->stripes + num_stripes;\n\n\t\t\ttgtdev_stripe->physical = physical_of_found;\n\t\t\ttgtdev_stripe->length =\n\t\t\t\tbbio->stripes[index_srcdev].length;\n\t\t\ttgtdev_stripe->dev = dev_replace->tgtdev;\n\t\t\tbbio->tgtdev_map[index_srcdev] = num_stripes;\n\n\t\t\ttgtdev_indexes++;\n\t\t\tnum_stripes++;\n\t\t}\n\t}\n\n\t*num_stripes_ret = num_stripes;\n\t*max_errors_ret = max_errors;\n\tbbio->num_tgtdevs = tgtdev_indexes;\n\t*bbio_ret = bbio;\n}\n\nstatic bool need_full_stripe(enum btrfs_map_op op)\n{\n\treturn (op == BTRFS_MAP_WRITE || op == BTRFS_MAP_GET_READ_MIRRORS);\n}\n\nstatic int __btrfs_map_block(struct btrfs_fs_info *fs_info,\n\t\t\t     enum btrfs_map_op op,\n\t\t\t     u64 logical, u64 *length,\n\t\t\t     struct btrfs_bio **bbio_ret,\n\t\t\t     int mirror_num, int need_raid_map)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 offset;\n\tu64 stripe_offset;\n\tu64 stripe_nr;\n\tu64 stripe_len;\n\tu32 stripe_index;\n\tint i;\n\tint ret = 0;\n\tint num_stripes;\n\tint max_errors = 0;\n\tint tgtdev_indexes = 0;\n\tstruct btrfs_bio *bbio = NULL;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tint dev_replace_is_ongoing = 0;\n\tint num_alloc_stripes;\n\tint patch_the_first_stripe_for_dev_replace = 0;\n\tu64 physical_to_patch_in_first_stripe = 0;\n\tu64 raid56_full_stripe_start = (u64)-1;\n\n\tif (op == BTRFS_MAP_DISCARD)\n\t\treturn __btrfs_map_block_for_discard(fs_info, logical,\n\t\t\t\t\t\t     *length, bbio_ret);\n\n\tem = btrfs_get_chunk_map(fs_info, logical, *length);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\toffset = logical - em->start;\n\n\tstripe_len = map->stripe_len;\n\tstripe_nr = offset;\n\t/*\n\t * stripe_nr counts the total number of stripes we have to stride\n\t * to get to this block\n\t */\n\tstripe_nr = div64_u64(stripe_nr, stripe_len);\n\n\tstripe_offset = stripe_nr * stripe_len;\n\tif (offset < stripe_offset) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"stripe math has gone wrong, stripe_offset=%llu, offset=%llu, start=%llu, logical=%llu, stripe_len=%llu\",\n\t\t\t   stripe_offset, offset, em->start, logical,\n\t\t\t   stripe_len);\n\t\tfree_extent_map(em);\n\t\treturn -EINVAL;\n\t}\n\n\t/* stripe_offset is the offset of this block in its stripe*/\n\tstripe_offset = offset - stripe_offset;\n\n\t/* if we're here for raid56, we need to know the stripe aligned start */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tunsigned long full_stripe_len = stripe_len * nr_data_stripes(map);\n\t\traid56_full_stripe_start = offset;\n\n\t\t/* allow a write of a full stripe, but make sure we don't\n\t\t * allow straddling of stripes\n\t\t */\n\t\traid56_full_stripe_start = div64_u64(raid56_full_stripe_start,\n\t\t\t\tfull_stripe_len);\n\t\traid56_full_stripe_start *= full_stripe_len;\n\t}\n\n\tif (map->type & BTRFS_BLOCK_GROUP_PROFILE_MASK) {\n\t\tu64 max_len;\n\t\t/* For writes to RAID[56], allow a full stripeset across all disks.\n\t\t   For other RAID types and for RAID[56] reads, just allow a single\n\t\t   stripe (on a single disk). */\n\t\tif ((map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) &&\n\t\t    (op == BTRFS_MAP_WRITE)) {\n\t\t\tmax_len = stripe_len * nr_data_stripes(map) -\n\t\t\t\t(offset - raid56_full_stripe_start);\n\t\t} else {\n\t\t\t/* we limit the length of each bio to what fits in a stripe */\n\t\t\tmax_len = stripe_len - stripe_offset;\n\t\t}\n\t\t*length = min_t(u64, em->len - offset, max_len);\n\t} else {\n\t\t*length = em->len - offset;\n\t}\n\n\t/*\n\t * This is for when we're called from btrfs_bio_fits_in_stripe and all\n\t * it cares about is the length\n\t */\n\tif (!bbio_ret)\n\t\tgoto out;\n\n\tdown_read(&dev_replace->rwsem);\n\tdev_replace_is_ongoing = btrfs_dev_replace_is_ongoing(dev_replace);\n\t/*\n\t * Hold the semaphore for read during the whole operation, write is\n\t * requested at commit time but must wait.\n\t */\n\tif (!dev_replace_is_ongoing)\n\t\tup_read(&dev_replace->rwsem);\n\n\tif (dev_replace_is_ongoing && mirror_num == map->num_stripes + 1 &&\n\t    !need_full_stripe(op) && dev_replace->tgtdev != NULL) {\n\t\tret = get_extra_mirror_from_replace(fs_info, logical, *length,\n\t\t\t\t\t\t    dev_replace->srcdev->devid,\n\t\t\t\t\t\t    &mirror_num,\n\t\t\t\t\t    &physical_to_patch_in_first_stripe);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\telse\n\t\t\tpatch_the_first_stripe_for_dev_replace = 1;\n\t} else if (mirror_num > map->num_stripes) {\n\t\tmirror_num = 0;\n\t}\n\n\tnum_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID0) {\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t&stripe_index);\n\t\tif (!need_full_stripe(op))\n\t\t\tmirror_num = 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID1) {\n\t\tif (need_full_stripe(op))\n\t\t\tnum_stripes = map->num_stripes;\n\t\telse if (mirror_num)\n\t\t\tstripe_index = mirror_num - 1;\n\t\telse {\n\t\t\tstripe_index = find_live_mirror(fs_info, map, 0,\n\t\t\t\t\t    dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_DUP) {\n\t\tif (need_full_stripe(op)) {\n\t\t\tnum_stripes = map->num_stripes;\n\t\t} else if (mirror_num) {\n\t\t\tstripe_index = mirror_num - 1;\n\t\t} else {\n\t\t\tmirror_num = 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID10) {\n\t\tu32 factor = map->num_stripes / map->sub_stripes;\n\n\t\tstripe_nr = div_u64_rem(stripe_nr, factor, &stripe_index);\n\t\tstripe_index *= map->sub_stripes;\n\n\t\tif (need_full_stripe(op))\n\t\t\tnum_stripes = map->sub_stripes;\n\t\telse if (mirror_num)\n\t\t\tstripe_index += mirror_num - 1;\n\t\telse {\n\t\t\tint old_stripe_index = stripe_index;\n\t\t\tstripe_index = find_live_mirror(fs_info, map,\n\t\t\t\t\t      stripe_index,\n\t\t\t\t\t      dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index - old_stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tif (need_raid_map && (need_full_stripe(op) || mirror_num > 1)) {\n\t\t\t/* push stripe_nr back to the start of the full stripe */\n\t\t\tstripe_nr = div64_u64(raid56_full_stripe_start,\n\t\t\t\t\tstripe_len * nr_data_stripes(map));\n\n\t\t\t/* RAID[56] write or recovery. Return all stripes */\n\t\t\tnum_stripes = map->num_stripes;\n\t\t\tmax_errors = nr_parity_stripes(map);\n\n\t\t\t*length = map->stripe_len;\n\t\t\tstripe_index = 0;\n\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Mirror #0 or #1 means the original data block.\n\t\t\t * Mirror #2 is RAID5 parity block.\n\t\t\t * Mirror #3 is RAID6 Q block.\n\t\t\t */\n\t\t\tstripe_nr = div_u64_rem(stripe_nr,\n\t\t\t\t\tnr_data_stripes(map), &stripe_index);\n\t\t\tif (mirror_num > 1)\n\t\t\t\tstripe_index = nr_data_stripes(map) +\n\t\t\t\t\t\tmirror_num - 2;\n\n\t\t\t/* We distribute the parity blocks across stripes */\n\t\t\tdiv_u64_rem(stripe_nr + stripe_index, map->num_stripes,\n\t\t\t\t\t&stripe_index);\n\t\t\tif (!need_full_stripe(op) && mirror_num <= 1)\n\t\t\t\tmirror_num = 1;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * after this, stripe_nr is the number of stripes on this\n\t\t * device we have to walk to find the data, and stripe_index is\n\t\t * the number of our device in the stripe array\n\t\t */\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t&stripe_index);\n\t\tmirror_num = stripe_index + 1;\n\t}\n\tif (stripe_index >= map->num_stripes) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"stripe index math went horribly wrong, got stripe_index=%u, num_stripes=%u\",\n\t\t\t   stripe_index, map->num_stripes);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tnum_alloc_stripes = num_stripes;\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL) {\n\t\tif (op == BTRFS_MAP_WRITE)\n\t\t\tnum_alloc_stripes <<= 1;\n\t\tif (op == BTRFS_MAP_GET_READ_MIRRORS)\n\t\t\tnum_alloc_stripes++;\n\t\ttgtdev_indexes = num_stripes;\n\t}\n\n\tbbio = alloc_btrfs_bio(num_alloc_stripes, tgtdev_indexes);\n\tif (!bbio) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL)\n\t\tbbio->tgtdev_map = (int *)(bbio->stripes + num_alloc_stripes);\n\n\t/* build raid_map */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK && need_raid_map &&\n\t    (need_full_stripe(op) || mirror_num > 1)) {\n\t\tu64 tmp;\n\t\tunsigned rot;\n\n\t\tbbio->raid_map = (u64 *)((void *)bbio->stripes +\n\t\t\t\t sizeof(struct btrfs_bio_stripe) *\n\t\t\t\t num_alloc_stripes +\n\t\t\t\t sizeof(int) * tgtdev_indexes);\n\n\t\t/* Work out the disk rotation on this stripe-set */\n\t\tdiv_u64_rem(stripe_nr, num_stripes, &rot);\n\n\t\t/* Fill in the logical address of each stripe */\n\t\ttmp = stripe_nr * nr_data_stripes(map);\n\t\tfor (i = 0; i < nr_data_stripes(map); i++)\n\t\t\tbbio->raid_map[(i+rot) % num_stripes] =\n\t\t\t\tem->start + (tmp + i) * map->stripe_len;\n\n\t\tbbio->raid_map[(i+rot) % map->num_stripes] = RAID5_P_STRIPE;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID6)\n\t\t\tbbio->raid_map[(i+rot+1) % num_stripes] =\n\t\t\t\tRAID6_Q_STRIPE;\n\t}\n\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tbbio->stripes[i].physical =\n\t\t\tmap->stripes[stripe_index].physical +\n\t\t\tstripe_offset +\n\t\t\tstripe_nr * map->stripe_len;\n\t\tbbio->stripes[i].dev =\n\t\t\tmap->stripes[stripe_index].dev;\n\t\tstripe_index++;\n\t}\n\n\tif (need_full_stripe(op))\n\t\tmax_errors = btrfs_chunk_max_errors(map);\n\n\tif (bbio->raid_map)\n\t\tsort_parity_stripes(bbio, num_stripes);\n\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL &&\n\t    need_full_stripe(op)) {\n\t\thandle_ops_on_dev_replace(op, &bbio, dev_replace, &num_stripes,\n\t\t\t\t\t  &max_errors);\n\t}\n\n\t*bbio_ret = bbio;\n\tbbio->map_type = map->type;\n\tbbio->num_stripes = num_stripes;\n\tbbio->max_errors = max_errors;\n\tbbio->mirror_num = mirror_num;\n\n\t/*\n\t * this is the case that REQ_READ && dev_replace_is_ongoing &&\n\t * mirror_num == num_stripes + 1 && dev_replace target drive is\n\t * available as a mirror\n\t */\n\tif (patch_the_first_stripe_for_dev_replace && num_stripes > 0) {\n\t\tWARN_ON(num_stripes > 1);\n\t\tbbio->stripes[0].dev = dev_replace->tgtdev;\n\t\tbbio->stripes[0].physical = physical_to_patch_in_first_stripe;\n\t\tbbio->mirror_num = map->num_stripes + 1;\n\t}\nout:\n\tif (dev_replace_is_ongoing) {\n\t\tlockdep_assert_held(&dev_replace->rwsem);\n\t\t/* Unlock and let waiting writers proceed */\n\t\tup_read(&dev_replace->rwsem);\n\t}\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nint btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t      u64 logical, u64 *length,\n\t\t      struct btrfs_bio **bbio_ret, int mirror_num)\n{\n\treturn __btrfs_map_block(fs_info, op, logical, length, bbio_ret,\n\t\t\t\t mirror_num, 0);\n}\n\n/* For Scrub/replace */\nint btrfs_map_sblock(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t     u64 logical, u64 *length,\n\t\t     struct btrfs_bio **bbio_ret)\n{\n\treturn __btrfs_map_block(fs_info, op, logical, length, bbio_ret, 0, 1);\n}\n\nint btrfs_rmap_block(struct btrfs_fs_info *fs_info, u64 chunk_start,\n\t\t     u64 physical, u64 **logical, int *naddrs, int *stripe_len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 *buf;\n\tu64 bytenr;\n\tu64 length;\n\tu64 stripe_nr;\n\tu64 rmap_len;\n\tint i, j, nr = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_start, 1);\n\tif (IS_ERR(em))\n\t\treturn -EIO;\n\n\tmap = em->map_lookup;\n\tlength = em->len;\n\trmap_len = map->stripe_len;\n\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tlength = div_u64(length, map->num_stripes / map->sub_stripes);\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID0)\n\t\tlength = div_u64(length, map->num_stripes);\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tlength = div_u64(length, nr_data_stripes(map));\n\t\trmap_len = map->stripe_len * nr_data_stripes(map);\n\t}\n\n\tbuf = kcalloc(map->num_stripes, sizeof(u64), GFP_NOFS);\n\tBUG_ON(!buf); /* -ENOMEM */\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].physical > physical ||\n\t\t    map->stripes[i].physical + length <= physical)\n\t\t\tcontinue;\n\n\t\tstripe_nr = physical - map->stripes[i].physical;\n\t\tstripe_nr = div64_u64(stripe_nr, map->stripe_len);\n\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID10) {\n\t\t\tstripe_nr = stripe_nr * map->num_stripes + i;\n\t\t\tstripe_nr = div_u64(stripe_nr, map->sub_stripes);\n\t\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID0) {\n\t\t\tstripe_nr = stripe_nr * map->num_stripes + i;\n\t\t} /* else if RAID[56], multiply by nr_data_stripes().\n\t\t   * Alternatively, just use rmap_len below instead of\n\t\t   * map->stripe_len */\n\n\t\tbytenr = chunk_start + stripe_nr * rmap_len;\n\t\tWARN_ON(nr >= map->num_stripes);\n\t\tfor (j = 0; j < nr; j++) {\n\t\t\tif (buf[j] == bytenr)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j == nr) {\n\t\t\tWARN_ON(nr >= map->num_stripes);\n\t\t\tbuf[nr++] = bytenr;\n\t\t}\n\t}\n\n\t*logical = buf;\n\t*naddrs = nr;\n\t*stripe_len = rmap_len;\n\n\tfree_extent_map(em);\n\treturn 0;\n}\n\nstatic inline void btrfs_end_bbio(struct btrfs_bio *bbio, struct bio *bio)\n{\n\tbio->bi_private = bbio->private;\n\tbio->bi_end_io = bbio->end_io;\n\tbio_endio(bio);\n\n\tbtrfs_put_bbio(bbio);\n}\n\nstatic void btrfs_end_bio(struct bio *bio)\n{\n\tstruct btrfs_bio *bbio = bio->bi_private;\n\tint is_orig_bio = 0;\n\n\tif (bio->bi_status) {\n\t\tatomic_inc(&bbio->error);\n\t\tif (bio->bi_status == BLK_STS_IOERR ||\n\t\t    bio->bi_status == BLK_STS_TARGET) {\n\t\t\tunsigned int stripe_index =\n\t\t\t\tbtrfs_io_bio(bio)->stripe_index;\n\t\t\tstruct btrfs_device *dev;\n\n\t\t\tBUG_ON(stripe_index >= bbio->num_stripes);\n\t\t\tdev = bbio->stripes[stripe_index].dev;\n\t\t\tif (dev->bdev) {\n\t\t\t\tif (bio_op(bio) == REQ_OP_WRITE)\n\t\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_WRITE_ERRS);\n\t\t\t\telse\n\t\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_READ_ERRS);\n\t\t\t\tif (bio->bi_opf & REQ_PREFLUSH)\n\t\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_FLUSH_ERRS);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (bio == bbio->orig_bio)\n\t\tis_orig_bio = 1;\n\n\tbtrfs_bio_counter_dec(bbio->fs_info);\n\n\tif (atomic_dec_and_test(&bbio->stripes_pending)) {\n\t\tif (!is_orig_bio) {\n\t\t\tbio_put(bio);\n\t\t\tbio = bbio->orig_bio;\n\t\t}\n\n\t\tbtrfs_io_bio(bio)->mirror_num = bbio->mirror_num;\n\t\t/* only send an error to the higher layers if it is\n\t\t * beyond the tolerance of the btrfs bio\n\t\t */\n\t\tif (atomic_read(&bbio->error) > bbio->max_errors) {\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t} else {\n\t\t\t/*\n\t\t\t * this bio is actually up to date, we didn't\n\t\t\t * go over the max number of errors\n\t\t\t */\n\t\t\tbio->bi_status = BLK_STS_OK;\n\t\t}\n\n\t\tbtrfs_end_bbio(bbio, bio);\n\t} else if (!is_orig_bio) {\n\t\tbio_put(bio);\n\t}\n}\n\n/*\n * see run_scheduled_bios for a description of why bios are collected for\n * async submit.\n *\n * This will add one bio to the pending list for a device and make sure\n * the work struct is scheduled.\n */\nstatic noinline void btrfs_schedule_bio(struct btrfs_device *device,\n\t\t\t\t\tstruct bio *bio)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tint should_queue = 1;\n\tstruct btrfs_pending_bios *pending_bios;\n\n\t/* don't bother with additional async steps for reads, right now */\n\tif (bio_op(bio) == REQ_OP_READ) {\n\t\tbtrfsic_submit_bio(bio);\n\t\treturn;\n\t}\n\n\tWARN_ON(bio->bi_next);\n\tbio->bi_next = NULL;\n\n\tspin_lock(&device->io_lock);\n\tif (op_is_sync(bio->bi_opf))\n\t\tpending_bios = &device->pending_sync_bios;\n\telse\n\t\tpending_bios = &device->pending_bios;\n\n\tif (pending_bios->tail)\n\t\tpending_bios->tail->bi_next = bio;\n\n\tpending_bios->tail = bio;\n\tif (!pending_bios->head)\n\t\tpending_bios->head = bio;\n\tif (device->running_pending)\n\t\tshould_queue = 0;\n\n\tspin_unlock(&device->io_lock);\n\n\tif (should_queue)\n\t\tbtrfs_queue_work(fs_info->submit_workers, &device->work);\n}\n\nstatic void submit_stripe_bio(struct btrfs_bio *bbio, struct bio *bio,\n\t\t\t      u64 physical, int dev_nr, int async)\n{\n\tstruct btrfs_device *dev = bbio->stripes[dev_nr].dev;\n\tstruct btrfs_fs_info *fs_info = bbio->fs_info;\n\n\tbio->bi_private = bbio;\n\tbtrfs_io_bio(bio)->stripe_index = dev_nr;\n\tbio->bi_end_io = btrfs_end_bio;\n\tbio->bi_iter.bi_sector = physical >> 9;\n\tbtrfs_debug_in_rcu(fs_info,\n\t\"btrfs_map_bio: rw %d 0x%x, sector=%llu, dev=%lu (%s id %llu), size=%u\",\n\t\tbio_op(bio), bio->bi_opf, (u64)bio->bi_iter.bi_sector,\n\t\t(u_long)dev->bdev->bd_dev, rcu_str_deref(dev->name), dev->devid,\n\t\tbio->bi_iter.bi_size);\n\tbio_set_dev(bio, dev->bdev);\n\n\tbtrfs_bio_counter_inc_noblocked(fs_info);\n\n\tif (async)\n\t\tbtrfs_schedule_bio(dev, bio);\n\telse\n\t\tbtrfsic_submit_bio(bio);\n}\n\nstatic void bbio_error(struct btrfs_bio *bbio, struct bio *bio, u64 logical)\n{\n\tatomic_inc(&bbio->error);\n\tif (atomic_dec_and_test(&bbio->stripes_pending)) {\n\t\t/* Should be the original bio. */\n\t\tWARN_ON(bio != bbio->orig_bio);\n\n\t\tbtrfs_io_bio(bio)->mirror_num = bbio->mirror_num;\n\t\tbio->bi_iter.bi_sector = logical >> 9;\n\t\tif (atomic_read(&bbio->error) > bbio->max_errors)\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\telse\n\t\t\tbio->bi_status = BLK_STS_OK;\n\t\tbtrfs_end_bbio(bbio, bio);\n\t}\n}\n\nblk_status_t btrfs_map_bio(struct btrfs_fs_info *fs_info, struct bio *bio,\n\t\t\t   int mirror_num, int async_submit)\n{\n\tstruct btrfs_device *dev;\n\tstruct bio *first_bio = bio;\n\tu64 logical = (u64)bio->bi_iter.bi_sector << 9;\n\tu64 length = 0;\n\tu64 map_length;\n\tint ret;\n\tint dev_nr;\n\tint total_devs;\n\tstruct btrfs_bio *bbio = NULL;\n\n\tlength = bio->bi_iter.bi_size;\n\tmap_length = length;\n\n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\tret = __btrfs_map_block(fs_info, btrfs_op(bio), logical,\n\t\t\t\t&map_length, &bbio, mirror_num, 1);\n\tif (ret) {\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\treturn errno_to_blk_status(ret);\n\t}\n\n\ttotal_devs = bbio->num_stripes;\n\tbbio->orig_bio = first_bio;\n\tbbio->private = first_bio->bi_private;\n\tbbio->end_io = first_bio->bi_end_io;\n\tbbio->fs_info = fs_info;\n\tatomic_set(&bbio->stripes_pending, bbio->num_stripes);\n\n\tif ((bbio->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK) &&\n\t    ((bio_op(bio) == REQ_OP_WRITE) || (mirror_num > 1))) {\n\t\t/* In this case, map_length has been set to the length of\n\t\t   a single stripe; not the whole write */\n\t\tif (bio_op(bio) == REQ_OP_WRITE) {\n\t\t\tret = raid56_parity_write(fs_info, bio, bbio,\n\t\t\t\t\t\t  map_length);\n\t\t} else {\n\t\t\tret = raid56_parity_recover(fs_info, bio, bbio,\n\t\t\t\t\t\t    map_length, mirror_num, 1);\n\t\t}\n\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\treturn errno_to_blk_status(ret);\n\t}\n\n\tif (map_length < length) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"mapping failed logical %llu bio len %llu len %llu\",\n\t\t\t   logical, length, map_length);\n\t\tBUG();\n\t}\n\n\tfor (dev_nr = 0; dev_nr < total_devs; dev_nr++) {\n\t\tdev = bbio->stripes[dev_nr].dev;\n\t\tif (!dev || !dev->bdev || test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t   &dev->dev_state) ||\n\t\t    (bio_op(first_bio) == REQ_OP_WRITE &&\n\t\t    !test_bit(BTRFS_DEV_STATE_WRITEABLE, &dev->dev_state))) {\n\t\t\tbbio_error(bbio, first_bio, logical);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (dev_nr < total_devs - 1)\n\t\t\tbio = btrfs_bio_clone(first_bio);\n\t\telse\n\t\t\tbio = first_bio;\n\n\t\tsubmit_stripe_bio(bbio, bio, bbio->stripes[dev_nr].physical,\n\t\t\t\t  dev_nr, async_submit);\n\t}\n\tbtrfs_bio_counter_dec(fs_info);\n\treturn BLK_STS_OK;\n}\n\n/*\n * Find a device specified by @devid or @uuid in the list of @fs_devices, or\n * return NULL.\n *\n * If devid and uuid are both specified, the match must be exact, otherwise\n * only devid is used.\n *\n * If @seed is true, traverse through the seed devices.\n */\nstruct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t       u64 devid, u8 *uuid, u8 *fsid,\n\t\t\t\t       bool seed)\n{\n\tstruct btrfs_device *device;\n\n\twhile (fs_devices) {\n\t\tif (!fsid ||\n\t\t    !memcmp(fs_devices->metadata_uuid, fsid, BTRFS_FSID_SIZE)) {\n\t\t\tlist_for_each_entry(device, &fs_devices->devices,\n\t\t\t\t\t    dev_list) {\n\t\t\t\tif (device->devid == devid &&\n\t\t\t\t    (!uuid || memcmp(device->uuid, uuid,\n\t\t\t\t\t\t     BTRFS_UUID_SIZE) == 0))\n\t\t\t\t\treturn device;\n\t\t\t}\n\t\t}\n\t\tif (seed)\n\t\t\tfs_devices = fs_devices->seed;\n\t\telse\n\t\t\treturn NULL;\n\t}\n\treturn NULL;\n}\n\nstatic struct btrfs_device *add_missing_dev(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t\t    u64 devid, u8 *dev_uuid)\n{\n\tstruct btrfs_device *device;\n\n\tdevice = btrfs_alloc_device(NULL, &devid, dev_uuid);\n\tif (IS_ERR(device))\n\t\treturn device;\n\n\tlist_add(&device->dev_list, &fs_devices->devices);\n\tdevice->fs_devices = fs_devices;\n\tfs_devices->num_devices++;\n\n\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\tfs_devices->missing_devices++;\n\n\treturn device;\n}\n\n/**\n * btrfs_alloc_device - allocate struct btrfs_device\n * @fs_info:\tused only for generating a new devid, can be NULL if\n *\t\tdevid is provided (i.e. @devid != NULL).\n * @devid:\ta pointer to devid for this device.  If NULL a new devid\n *\t\tis generated.\n * @uuid:\ta pointer to UUID for this device.  If NULL a new UUID\n *\t\tis generated.\n *\n * Return: a pointer to a new &struct btrfs_device on success; ERR_PTR()\n * on error.  Returned struct is not linked onto any lists and must be\n * destroyed with btrfs_free_device.\n */\nstruct btrfs_device *btrfs_alloc_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tconst u64 *devid,\n\t\t\t\t\tconst u8 *uuid)\n{\n\tstruct btrfs_device *dev;\n\tu64 tmp;\n\n\tif (WARN_ON(!devid && !fs_info))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdev = __alloc_device();\n\tif (IS_ERR(dev))\n\t\treturn dev;\n\n\tif (devid)\n\t\ttmp = *devid;\n\telse {\n\t\tint ret;\n\n\t\tret = find_next_devid(fs_info, &tmp);\n\t\tif (ret) {\n\t\t\tbtrfs_free_device(dev);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\tdev->devid = tmp;\n\n\tif (uuid)\n\t\tmemcpy(dev->uuid, uuid, BTRFS_UUID_SIZE);\n\telse\n\t\tgenerate_random_uuid(dev->uuid);\n\n\tbtrfs_init_work(&dev->work, btrfs_submit_helper,\n\t\t\tpending_bios_fn, NULL, NULL);\n\n\treturn dev;\n}\n\n/* Return -EIO if any error, otherwise return 0. */\nstatic int btrfs_check_chunk_valid(struct btrfs_fs_info *fs_info,\n\t\t\t\t   struct extent_buffer *leaf,\n\t\t\t\t   struct btrfs_chunk *chunk, u64 logical)\n{\n\tu64 length;\n\tu64 stripe_len;\n\tu16 num_stripes;\n\tu16 sub_stripes;\n\tu64 type;\n\tu64 features;\n\tbool mixed = false;\n\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tstripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tsub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\ttype = btrfs_chunk_type(leaf, chunk);\n\n\tif (!num_stripes) {\n\t\tbtrfs_err(fs_info, \"invalid chunk num_stripes: %u\",\n\t\t\t  num_stripes);\n\t\treturn -EIO;\n\t}\n\tif (!IS_ALIGNED(logical, fs_info->sectorsize)) {\n\t\tbtrfs_err(fs_info, \"invalid chunk logical %llu\", logical);\n\t\treturn -EIO;\n\t}\n\tif (btrfs_chunk_sector_size(leaf, chunk) != fs_info->sectorsize) {\n\t\tbtrfs_err(fs_info, \"invalid chunk sectorsize %u\",\n\t\t\t  btrfs_chunk_sector_size(leaf, chunk));\n\t\treturn -EIO;\n\t}\n\tif (!length || !IS_ALIGNED(length, fs_info->sectorsize)) {\n\t\tbtrfs_err(fs_info, \"invalid chunk length %llu\", length);\n\t\treturn -EIO;\n\t}\n\tif (!is_power_of_2(stripe_len) || stripe_len != BTRFS_STRIPE_LEN) {\n\t\tbtrfs_err(fs_info, \"invalid chunk stripe length: %llu\",\n\t\t\t  stripe_len);\n\t\treturn -EIO;\n\t}\n\tif (~(BTRFS_BLOCK_GROUP_TYPE_MASK | BTRFS_BLOCK_GROUP_PROFILE_MASK) &\n\t    type) {\n\t\tbtrfs_err(fs_info, \"unrecognized chunk type: %llu\",\n\t\t\t  ~(BTRFS_BLOCK_GROUP_TYPE_MASK |\n\t\t\t    BTRFS_BLOCK_GROUP_PROFILE_MASK) &\n\t\t\t  btrfs_chunk_type(leaf, chunk));\n\t\treturn -EIO;\n\t}\n\n\tif ((type & BTRFS_BLOCK_GROUP_TYPE_MASK) == 0) {\n\t\tbtrfs_err(fs_info, \"missing chunk type flag: 0x%llx\", type);\n\t\treturn -EIO;\n\t}\n\n\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) &&\n\t    (type & (BTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_DATA))) {\n\t\tbtrfs_err(fs_info,\n\t\t\t\"system chunk with data or metadata type: 0x%llx\", type);\n\t\treturn -EIO;\n\t}\n\n\tfeatures = btrfs_super_incompat_flags(fs_info->super_copy);\n\tif (features & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)\n\t\tmixed = true;\n\n\tif (!mixed) {\n\t\tif ((type & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t    (type & BTRFS_BLOCK_GROUP_DATA)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"mixed chunk type in non-mixed mode: 0x%llx\", type);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tif ((type & BTRFS_BLOCK_GROUP_RAID10 && sub_stripes != 2) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID1 && num_stripes < 1) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID5 && num_stripes < 2) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID6 && num_stripes < 3) ||\n\t    (type & BTRFS_BLOCK_GROUP_DUP && num_stripes > 2) ||\n\t    ((type & BTRFS_BLOCK_GROUP_PROFILE_MASK) == 0 &&\n\t     num_stripes != 1)) {\n\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid num_stripes:sub_stripes %u:%u for profile %llu\",\n\t\t\tnum_stripes, sub_stripes,\n\t\t\ttype & BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic void btrfs_report_missing_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tu64 devid, u8 *uuid, bool error)\n{\n\tif (error)\n\t\tbtrfs_err_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n\telse\n\t\tbtrfs_warn_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n}\n\nstatic int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL, true);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}\n\nstatic void fill_device_from_item(struct extent_buffer *leaf,\n\t\t\t\t struct btrfs_dev_item *dev_item,\n\t\t\t\t struct btrfs_device *device)\n{\n\tunsigned long ptr;\n\n\tdevice->devid = btrfs_device_id(leaf, dev_item);\n\tdevice->disk_total_bytes = btrfs_device_total_bytes(leaf, dev_item);\n\tdevice->total_bytes = device->disk_total_bytes;\n\tdevice->commit_total_bytes = device->disk_total_bytes;\n\tdevice->bytes_used = btrfs_device_bytes_used(leaf, dev_item);\n\tdevice->commit_bytes_used = device->bytes_used;\n\tdevice->type = btrfs_device_type(leaf, dev_item);\n\tdevice->io_align = btrfs_device_io_align(leaf, dev_item);\n\tdevice->io_width = btrfs_device_io_width(leaf, dev_item);\n\tdevice->sector_size = btrfs_device_sector_size(leaf, dev_item);\n\tWARN_ON(device->devid == BTRFS_DEV_REPLACE_DEVID);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\n\tptr = btrfs_device_uuid(dev_item);\n\tread_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n}\n\nstatic struct btrfs_fs_devices *open_seed_devices(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u8 *fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tASSERT(fsid);\n\n\tfs_devices = fs_info->fs_devices->seed;\n\twhile (fs_devices) {\n\t\tif (!memcmp(fs_devices->fsid, fsid, BTRFS_FSID_SIZE))\n\t\t\treturn fs_devices;\n\n\t\tfs_devices = fs_devices->seed;\n\t}\n\n\tfs_devices = find_fsid(fsid, NULL);\n\tif (!fs_devices) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED))\n\t\t\treturn ERR_PTR(-ENOENT);\n\n\t\tfs_devices = alloc_fs_devices(fsid, NULL);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn fs_devices;\n\n\t\tfs_devices->seeding = 1;\n\t\tfs_devices->opened = 1;\n\t\treturn fs_devices;\n\t}\n\n\tfs_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tret = open_fs_devices(fs_devices, FMODE_READ, fs_info->bdev_holder);\n\tif (ret) {\n\t\tfree_fs_devices(fs_devices);\n\t\tfs_devices = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tif (!fs_devices->seeding) {\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t\tfs_devices = ERR_PTR(-EINVAL);\n\t\tgoto out;\n\t}\n\n\tfs_devices->seed = fs_info->fs_devices->seed;\n\tfs_info->fs_devices->seed = fs_devices;\nout:\n\treturn fs_devices;\n}\n\nstatic int read_one_dev(struct btrfs_fs_info *fs_info,\n\t\t\tstruct extent_buffer *leaf,\n\t\t\tstruct btrfs_dev_item *dev_item)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 devid;\n\tint ret;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\n\tdevid = btrfs_device_id(leaf, dev_item);\n\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t   BTRFS_UUID_SIZE);\n\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t   BTRFS_FSID_SIZE);\n\n\tif (memcmp(fs_uuid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE)) {\n\t\tfs_devices = open_seed_devices(fs_info, fs_uuid);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn PTR_ERR(fs_devices);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t   fs_uuid, true);\n\tif (!device) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tdevice = add_missing_dev(fs_devices, devid, dev_uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"failed to add missing dev %llu: %ld\",\n\t\t\t\tdevid, PTR_ERR(device));\n\t\t\treturn PTR_ERR(device);\n\t\t}\n\t\tbtrfs_report_missing_device(fs_info, devid, dev_uuid, false);\n\t} else {\n\t\tif (!device->bdev) {\n\t\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\t\tbtrfs_report_missing_device(fs_info,\n\t\t\t\t\t\tdevid, dev_uuid, true);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, false);\n\t\t}\n\n\t\tif (!device->bdev &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\t/*\n\t\t\t * this happens when a device that was properly setup\n\t\t\t * in the device info lists suddenly goes bad.\n\t\t\t * device->bdev is NULL, and so we have to set\n\t\t\t * device->missing to one here\n\t\t\t */\n\t\t\tdevice->fs_devices->missing_devices++;\n\t\t\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\n\t\t/* Move the device to its own fs_devices */\n\t\tif (device->fs_devices != fs_devices) {\n\t\t\tASSERT(test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t\t&device->dev_state));\n\n\t\t\tlist_move(&device->dev_list, &fs_devices->devices);\n\t\t\tdevice->fs_devices->num_devices--;\n\t\t\tfs_devices->num_devices++;\n\n\t\t\tdevice->fs_devices->missing_devices--;\n\t\t\tfs_devices->missing_devices++;\n\n\t\t\tdevice->fs_devices = fs_devices;\n\t\t}\n\t}\n\n\tif (device->fs_devices != fs_info->fs_devices) {\n\t\tBUG_ON(test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state));\n\t\tif (device->generation !=\n\t\t    btrfs_device_generation(leaf, dev_item))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfill_device_from_item(leaf, dev_item, device);\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t   !test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes += device->total_bytes;\n\t\tatomic64_add(device->total_bytes - device->bytes_used,\n\t\t\t\t&fs_info->free_chunk_space);\n\t}\n\tret = 0;\n\treturn ret;\n}\n\nint btrfs_read_sys_array(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct extent_buffer *sb;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *array_ptr;\n\tunsigned long sb_array_offset;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur_offset;\n\tu64 type;\n\tstruct btrfs_key key;\n\n\tASSERT(BTRFS_SUPER_INFO_SIZE <= fs_info->nodesize);\n\t/*\n\t * This will create extent buffer of nodesize, superblock size is\n\t * fixed to BTRFS_SUPER_INFO_SIZE. If nodesize > sb size, this will\n\t * overallocate but we can keep it as-is, only the first page is used.\n\t */\n\tsb = btrfs_find_create_tree_block(fs_info, BTRFS_SUPER_INFO_OFFSET);\n\tif (IS_ERR(sb))\n\t\treturn PTR_ERR(sb);\n\tset_extent_buffer_uptodate(sb);\n\tbtrfs_set_buffer_lockdep_class(root->root_key.objectid, sb, 0);\n\t/*\n\t * The sb extent buffer is artificial and just used to read the system array.\n\t * set_extent_buffer_uptodate() call does not properly mark all it's\n\t * pages up-to-date when the page is larger: extent does not cover the\n\t * whole page and consequently check_page_uptodate does not find all\n\t * the page's extents up-to-date (the hole beyond sb),\n\t * write_extent_buffer then triggers a WARN_ON.\n\t *\n\t * Regular short extents go through mark_extent_buffer_dirty/writeback cycle,\n\t * but sb spans only this function. Add an explicit SetPageUptodate call\n\t * to silence the warning eg. on PowerPC 64.\n\t */\n\tif (PAGE_SIZE > BTRFS_SUPER_INFO_SIZE)\n\t\tSetPageUptodate(sb->pages[0]);\n\n\twrite_extent_buffer(sb, super_copy, 0, BTRFS_SUPER_INFO_SIZE);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tarray_ptr = super_copy->sys_chunk_array;\n\tsb_array_offset = offsetof(struct btrfs_super_block, sys_chunk_array);\n\tcur_offset = 0;\n\n\twhile (cur_offset < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)array_ptr;\n\t\tlen = sizeof(*disk_key);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\n\t\tif (key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tchunk = (struct btrfs_chunk *)sb_array_offset;\n\t\t\t/*\n\t\t\t * At least one btrfs_chunk with one stripe must be\n\t\t\t * present, exact stripe count check comes afterwards\n\t\t\t */\n\t\t\tlen = btrfs_chunk_item_size(1);\n\t\t\tif (cur_offset + len > array_size)\n\t\t\t\tgoto out_short_read;\n\n\t\t\tnum_stripes = btrfs_chunk_num_stripes(sb, chunk);\n\t\t\tif (!num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"invalid number of stripes %u in sys_array at offset %u\",\n\t\t\t\t\tnum_stripes, cur_offset);\n\t\t\t\tret = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\ttype = btrfs_chunk_type(sb, chunk);\n\t\t\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) == 0) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t    \"invalid chunk type %llu in sys_array at offset %u\",\n\t\t\t\t\ttype, cur_offset);\n\t\t\t\tret = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tlen = btrfs_chunk_item_size(num_stripes);\n\t\t\tif (cur_offset + len > array_size)\n\t\t\t\tgoto out_short_read;\n\n\t\t\tret = read_one_chunk(fs_info, &key, sb, chunk);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t    \"unexpected item type %u in sys_array at offset %u\",\n\t\t\t\t  (u32)key.type, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\t}\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn ret;\n\nout_short_read:\n\tbtrfs_err(fs_info, \"sys_array too short to read %u bytes at offset %u\",\n\t\t\tlen, cur_offset);\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn -EIO;\n}\n\n/*\n * Check if all chunks in the fs are OK for read-write degraded mount\n *\n * If the @failing_dev is specified, it's accounted as missing.\n *\n * Return true if all chunks meet the minimal RW mount requirements.\n * Return false if any chunk doesn't meet the minimal RW mount requirements.\n */\nbool btrfs_check_rw_degradable(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct btrfs_device *failing_dev)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tu64 next_start = 0;\n\tbool ret = true;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, 0, (u64)-1);\n\tread_unlock(&map_tree->map_tree.lock);\n\t/* No chunk at all? Return false anyway */\n\tif (!em) {\n\t\tret = false;\n\t\tgoto out;\n\t}\n\twhile (em) {\n\t\tstruct map_lookup *map;\n\t\tint missing = 0;\n\t\tint max_tolerated;\n\t\tint i;\n\n\t\tmap = em->map_lookup;\n\t\tmax_tolerated =\n\t\t\tbtrfs_get_num_tolerated_disk_barrier_failures(\n\t\t\t\t\tmap->type);\n\t\tfor (i = 0; i < map->num_stripes; i++) {\n\t\t\tstruct btrfs_device *dev = map->stripes[i].dev;\n\n\t\t\tif (!dev || !dev->bdev ||\n\t\t\t    test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) ||\n\t\t\t    dev->last_flush_error)\n\t\t\t\tmissing++;\n\t\t\telse if (failing_dev && failing_dev == dev)\n\t\t\t\tmissing++;\n\t\t}\n\t\tif (missing > max_tolerated) {\n\t\t\tif (!failing_dev)\n\t\t\t\tbtrfs_warn(fs_info,\n\t\"chunk %llu missing %d devices, max tolerance is %d for writable mount\",\n\t\t\t\t   em->start, missing, max_tolerated);\n\t\t\tfree_extent_map(em);\n\t\t\tret = false;\n\t\t\tgoto out;\n\t\t}\n\t\tnext_start = extent_map_end(em);\n\t\tfree_extent_map(em);\n\n\t\tread_lock(&map_tree->map_tree.lock);\n\t\tem = lookup_extent_mapping(&map_tree->map_tree, next_start,\n\t\t\t\t\t   (u64)(-1) - next_start);\n\t\tread_unlock(&map_tree->map_tree.lock);\n\t}\nout:\n\treturn ret;\n}\n\nint btrfs_read_chunk_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tint ret;\n\tint slot;\n\tu64 total_dev = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * uuid_mutex is needed only if we are mounting a sprout FS\n\t * otherwise we don't need it.\n\t */\n\tmutex_lock(&uuid_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\t/*\n\t * Read all device items, and then all the chunk items. All\n\t * device items are found before any chunk item (their object id\n\t * is smaller than the lowest possible object id for a chunk\n\t * item - BTRFS_FIRST_CHUNK_TREE_OBJECTID).\n\t */\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = 0;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto error;\n\twhile (1) {\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\t\tif (found_key.type == BTRFS_DEV_ITEM_KEY) {\n\t\t\tstruct btrfs_dev_item *dev_item;\n\t\t\tdev_item = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\t  struct btrfs_dev_item);\n\t\t\tret = read_one_dev(fs_info, leaf, dev_item);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t\ttotal_dev++;\n\t\t} else if (found_key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tstruct btrfs_chunk *chunk;\n\t\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\t\tret = read_one_chunk(fs_info, &found_key, leaf, chunk);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t}\n\t\tpath->slots[0]++;\n\t}\n\n\t/*\n\t * After loading chunk tree, we've got all device information,\n\t * do another round of validation checks.\n\t */\n\tif (total_dev != fs_info->fs_devices->total_devices) {\n\t\tbtrfs_err(fs_info,\n\t   \"super_num_devices %llu mismatch with num_devices %llu found here\",\n\t\t\t  btrfs_super_num_devices(fs_info->super_copy),\n\t\t\t  total_dev);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tif (btrfs_super_total_bytes(fs_info->super_copy) <\n\t    fs_info->fs_devices->total_rw_bytes) {\n\t\tbtrfs_err(fs_info,\n\t\"super_total_bytes %llu mismatch with fs_devices total_rw_bytes %llu\",\n\t\t\t  btrfs_super_total_bytes(fs_info->super_copy),\n\t\t\t  fs_info->fs_devices->total_rw_bytes);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tret = 0;\nerror:\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&uuid_mutex);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nvoid btrfs_init_devices_late(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\n\twhile (fs_devices) {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_for_each_entry(device, &fs_devices->devices, dev_list)\n\t\t\tdevice->fs_info = fs_info;\n\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t\tfs_devices = fs_devices->seed;\n\t}\n}\n\nstatic void __btrfs_reset_dev_stats(struct btrfs_device *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tbtrfs_dev_stat_reset(dev, i);\n}\n\nint btrfs_init_dev_stats(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tint ret = 0;\n\tstruct btrfs_device *device;\n\tstruct btrfs_path *path = NULL;\n\tint i;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tint item_size;\n\t\tstruct btrfs_dev_stats_item *ptr;\n\n\t\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\t\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\t\tkey.offset = device->devid;\n\t\tret = btrfs_search_slot(NULL, dev_root, &key, path, 0, 0);\n\t\tif (ret) {\n\t\t\t__btrfs_reset_dev_stats(device);\n\t\t\tdevice->dev_stats_valid = 1;\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\t\tslot = path->slots[0];\n\t\teb = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(eb, &found_key, slot);\n\t\titem_size = btrfs_item_size_nr(eb, slot);\n\n\t\tptr = btrfs_item_ptr(eb, slot,\n\t\t\t\t     struct btrfs_dev_stats_item);\n\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\t\tif (item_size >= (1 + i) * sizeof(__le64))\n\t\t\t\tbtrfs_dev_stat_set(device, i,\n\t\t\t\t\tbtrfs_dev_stats_value(eb, ptr, i));\n\t\t\telse\n\t\t\t\tbtrfs_dev_stat_reset(device, i);\n\t\t}\n\n\t\tdevice->dev_stats_valid = 1;\n\t\tbtrfs_dev_stat_print_on_load(device);\n\t\tbtrfs_release_path(path);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic int update_dev_stat_item(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_device *device)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_dev_stats_item *ptr;\n\tint ret;\n\tint i;\n\n\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tret = btrfs_search_slot(trans, dev_root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\"error %d while searching for dev_stats item for device %s\",\n\t\t\t      ret, rcu_str_deref(device->name));\n\t\tgoto out;\n\t}\n\n\tif (ret == 0 &&\n\t    btrfs_item_size_nr(path->nodes[0], path->slots[0]) < sizeof(*ptr)) {\n\t\t/* need to delete old one and insert a new one */\n\t\tret = btrfs_del_item(trans, dev_root, path);\n\t\tif (ret != 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"delete too small dev_stats item for device %s failed %d\",\n\t\t\t\t      rcu_str_deref(device->name), ret);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 1;\n\t}\n\n\tif (ret == 1) {\n\t\t/* need to insert a new item */\n\t\tbtrfs_release_path(path);\n\t\tret = btrfs_insert_empty_item(trans, dev_root, path,\n\t\t\t\t\t      &key, sizeof(*ptr));\n\t\tif (ret < 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"insert dev_stats item for device %s failed %d\",\n\t\t\t\trcu_str_deref(device->name), ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\teb = path->nodes[0];\n\tptr = btrfs_item_ptr(eb, path->slots[0], struct btrfs_dev_stats_item);\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tbtrfs_set_dev_stats_value(eb, ptr, i,\n\t\t\t\t\t  btrfs_dev_stat_read(device, i));\n\tbtrfs_mark_buffer_dirty(eb);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * called from commit_transaction. Writes all changed device stats to disk.\n */\nint btrfs_run_dev_stats(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tint stats_cnt;\n\tint ret = 0;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tstats_cnt = atomic_read(&device->dev_stats_ccnt);\n\t\tif (!device->dev_stats_valid || stats_cnt == 0)\n\t\t\tcontinue;\n\n\n\t\t/*\n\t\t * There is a LOAD-LOAD control dependency between the value of\n\t\t * dev_stats_ccnt and updating the on-disk values which requires\n\t\t * reading the in-memory counters. Such control dependencies\n\t\t * require explicit read memory barriers.\n\t\t *\n\t\t * This memory barriers pairs with smp_mb__before_atomic in\n\t\t * btrfs_dev_stat_inc/btrfs_dev_stat_set and with the full\n\t\t * barrier implied by atomic_xchg in\n\t\t * btrfs_dev_stats_read_and_reset\n\t\t */\n\t\tsmp_rmb();\n\n\t\tret = update_dev_stat_item(trans, device);\n\t\tif (!ret)\n\t\t\tatomic_sub(stats_cnt, &device->dev_stats_ccnt);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\treturn ret;\n}\n\nvoid btrfs_dev_stat_inc_and_print(struct btrfs_device *dev, int index)\n{\n\tbtrfs_dev_stat_inc(dev, index);\n\tbtrfs_dev_stat_print_on_error(dev);\n}\n\nstatic void btrfs_dev_stat_print_on_error(struct btrfs_device *dev)\n{\n\tif (!dev->dev_stats_valid)\n\t\treturn;\n\tbtrfs_err_rl_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t\t\t   rcu_str_deref(dev->name),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tif (btrfs_dev_stat_read(dev, i) != 0)\n\t\t\tbreak;\n\tif (i == BTRFS_DEV_STAT_VALUES_MAX)\n\t\treturn; /* all values == 0, suppress message */\n\n\tbtrfs_info_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t       rcu_str_deref(dev->name),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nint btrfs_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\tstruct btrfs_ioctl_get_dev_stats *stats)\n{\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tint i;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, stats->devid, NULL, NULL,\n\t\t\t\ttrue);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (!dev) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, device not found\");\n\t\treturn -ENODEV;\n\t} else if (!dev->dev_stats_valid) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, not yet valid\");\n\t\treturn -ENODEV;\n\t} else if (stats->flags & BTRFS_DEV_STATS_RESET) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] =\n\t\t\t\t\tbtrfs_dev_stat_read_and_reset(dev, i);\n\t\t\telse\n\t\t\t\tbtrfs_dev_stat_reset(dev, i);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] = btrfs_dev_stat_read(dev, i);\n\t}\n\tif (stats->nr_items > BTRFS_DEV_STAT_VALUES_MAX)\n\t\tstats->nr_items = BTRFS_DEV_STAT_VALUES_MAX;\n\treturn 0;\n}\n\nvoid btrfs_scratch_superblocks(struct block_device *bdev, const char *device_path)\n{\n\tstruct buffer_head *bh;\n\tstruct btrfs_super_block *disk_super;\n\tint copy_num;\n\n\tif (!bdev)\n\t\treturn;\n\n\tfor (copy_num = 0; copy_num < BTRFS_SUPER_MIRROR_MAX;\n\t\tcopy_num++) {\n\n\t\tif (btrfs_read_dev_one_super(bdev, copy_num, &bh))\n\t\t\tcontinue;\n\n\t\tdisk_super = (struct btrfs_super_block *)bh->b_data;\n\n\t\tmemset(&disk_super->magic, 0, sizeof(disk_super->magic));\n\t\tset_buffer_dirty(bh);\n\t\tsync_dirty_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\t/* Notify udev that device has changed */\n\tbtrfs_kobject_uevent(bdev, KOBJ_CHANGE);\n\n\t/* Update ctime/mtime for device path for libblkid */\n\tupdate_dev_time(device_path);\n}\n\n/*\n * Update the size of all devices, which is used for writing out the\n * super blocks.\n */\nvoid btrfs_update_commit_device_size(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *curr, *next;\n\n\tif (list_empty(&fs_devices->resized_devices))\n\t\treturn;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_for_each_entry_safe(curr, next, &fs_devices->resized_devices,\n\t\t\t\t resized_list) {\n\t\tlist_del_init(&curr->resized_list);\n\t\tcurr->commit_total_bytes = curr->disk_total_bytes;\n\t}\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n}\n\n/* Must be invoked during the transaction commit */\nvoid btrfs_update_commit_device_bytes_used(struct btrfs_transaction *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tint i;\n\n\tif (list_empty(&trans->pending_chunks))\n\t\treturn;\n\n\t/* In order to kick the device replace finish process */\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_for_each_entry(em, &trans->pending_chunks, list) {\n\t\tmap = em->map_lookup;\n\n\t\tfor (i = 0; i < map->num_stripes; i++) {\n\t\t\tdev = map->stripes[i].dev;\n\t\t\tdev->commit_bytes_used = dev->bytes_used;\n\t\t}\n\t}\n\tmutex_unlock(&fs_info->chunk_mutex);\n}\n\nvoid btrfs_set_fs_info_ptr(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\twhile (fs_devices) {\n\t\tfs_devices->fs_info = fs_info;\n\t\tfs_devices = fs_devices->seed;\n\t}\n}\n\nvoid btrfs_reset_fs_info_ptr(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\twhile (fs_devices) {\n\t\tfs_devices->fs_info = NULL;\n\t\tfs_devices = fs_devices->seed;\n\t}\n}\n\n/*\n * Multiplicity factor for simple profiles: DUP, RAID1-like and RAID10.\n */\nint btrfs_bg_type_to_factor(u64 flags)\n{\n\tif (flags & (BTRFS_BLOCK_GROUP_DUP | BTRFS_BLOCK_GROUP_RAID1 |\n\t\t     BTRFS_BLOCK_GROUP_RAID10))\n\t\treturn 2;\n\treturn 1;\n}\n\n\nstatic u64 calc_stripe_length(u64 type, u64 chunk_len, int num_stripes)\n{\n\tint index = btrfs_bg_flags_to_raid_index(type);\n\tint ncopies = btrfs_raid_array[index].ncopies;\n\tint data_stripes;\n\n\tswitch (type & BTRFS_BLOCK_GROUP_PROFILE_MASK) {\n\tcase BTRFS_BLOCK_GROUP_RAID5:\n\t\tdata_stripes = num_stripes - 1;\n\t\tbreak;\n\tcase BTRFS_BLOCK_GROUP_RAID6:\n\t\tdata_stripes = num_stripes - 2;\n\t\tbreak;\n\tdefault:\n\t\tdata_stripes = num_stripes / ncopies;\n\t\tbreak;\n\t}\n\treturn div_u64(chunk_len, data_stripes);\n}\n\nstatic int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = btrfs_find_device(fs_info->fs_devices->seed, devid, NULL,\n\t\t\t\t\tNULL, false);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic int verify_chunk_dev_extent_mapping(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *node;\n\tint ret = 0;\n\n\tread_lock(&em_tree->lock);\n\tfor (node = rb_first_cached(&em_tree->map); node; node = rb_next(node)) {\n\t\tem = rb_entry(node, struct extent_map, rb_node);\n\t\tif (em->map_lookup->num_stripes !=\n\t\t    em->map_lookup->verified_stripes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"chunk %llu has missing dev extent, have %d expect %d\",\n\t\t\t\t  em->start, em->map_lookup->verified_stripes,\n\t\t\t\t  em->map_lookup->num_stripes);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tread_unlock(&em_tree->lock);\n\treturn ret;\n}\n\n/*\n * Ensure that all dev extents are mapped to correct chunk, otherwise\n * later chunk allocation/free would cause unexpected behavior.\n *\n * NOTE: This will iterate through the whole device tree, which should be of\n * the same size level as the chunk tree.  This slightly increases mount time.\n */\nint btrfs_verify_dev_extents(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tu64 prev_devid = 0;\n\tu64 prev_dev_ext_end = 0;\n\tint ret = 0;\n\n\tkey.objectid = 1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\tkey.offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_FORWARD;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/* No dev extents at all? Not good */\n\t\tif (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\twhile (1) {\n\t\tstruct extent_buffer *leaf = path->nodes[0];\n\t\tstruct btrfs_dev_extent *dext;\n\t\tint slot = path->slots[0];\n\t\tu64 chunk_offset;\n\t\tu64 physical_offset;\n\t\tu64 physical_len;\n\t\tu64 devid;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tbreak;\n\t\tdevid = key.objectid;\n\t\tphysical_offset = key.offset;\n\n\t\tdext = btrfs_item_ptr(leaf, slot, struct btrfs_dev_extent);\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(leaf, dext);\n\t\tphysical_len = btrfs_dev_extent_length(leaf, dext);\n\n\t\t/* Check if this dev extent overlaps with the previous one */\n\t\tif (devid == prev_devid && physical_offset < prev_dev_ext_end) {\n\t\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu overlap with previous dev extent end %llu\",\n\t\t\t\t  devid, physical_offset, prev_dev_ext_end);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = verify_one_dev_extent(fs_info, chunk_offset, devid,\n\t\t\t\t\t    physical_offset, physical_len);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tprev_devid = devid;\n\t\tprev_dev_ext_end = physical_offset + physical_len;\n\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (ret > 0) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ensure all chunks have corresponding dev extents */\n\tret = verify_chunk_dev_extent_mapping(fs_info);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Check whether the given block group or device is pinned by any inode being\n * used as a swapfile.\n */\nbool btrfs_pinned_by_swapfile(struct btrfs_fs_info *fs_info, void *ptr)\n{\n\tstruct btrfs_swapfile_pin *sp;\n\tstruct rb_node *node;\n\n\tspin_lock(&fs_info->swapfile_pins_lock);\n\tnode = fs_info->swapfile_pins.rb_node;\n\twhile (node) {\n\t\tsp = rb_entry(node, struct btrfs_swapfile_pin, node);\n\t\tif (ptr < sp->ptr)\n\t\t\tnode = node->rb_left;\n\t\telse if (ptr > sp->ptr)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\tbreak;\n\t}\n\tspin_unlock(&fs_info->swapfile_pins_lock);\n\treturn node != NULL;\n}\n", "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n */\n\n#ifndef BTRFS_VOLUMES_H\n#define BTRFS_VOLUMES_H\n\n#include <linux/bio.h>\n#include <linux/sort.h>\n#include <linux/btrfs.h>\n#include \"async-thread.h\"\n\n#define BTRFS_MAX_DATA_CHUNK_SIZE\t(10ULL * SZ_1G)\n\nextern struct mutex uuid_mutex;\n\n#define BTRFS_STRIPE_LEN\tSZ_64K\n\nstruct buffer_head;\nstruct btrfs_pending_bios {\n\tstruct bio *head;\n\tstruct bio *tail;\n};\n\n/*\n * Use sequence counter to get consistent device stat data on\n * 32-bit processors.\n */\n#if BITS_PER_LONG==32 && defined(CONFIG_SMP)\n#include <linux/seqlock.h>\n#define __BTRFS_NEED_DEVICE_DATA_ORDERED\n#define btrfs_device_data_ordered_init(device)\t\\\n\tseqcount_init(&device->data_seqcount)\n#else\n#define btrfs_device_data_ordered_init(device) do { } while (0)\n#endif\n\n#define BTRFS_DEV_STATE_WRITEABLE\t(0)\n#define BTRFS_DEV_STATE_IN_FS_METADATA\t(1)\n#define BTRFS_DEV_STATE_MISSING\t\t(2)\n#define BTRFS_DEV_STATE_REPLACE_TGT\t(3)\n#define BTRFS_DEV_STATE_FLUSH_SENT\t(4)\n\nstruct btrfs_device {\n\tstruct list_head dev_list;\n\tstruct list_head dev_alloc_list;\n\tstruct btrfs_fs_devices *fs_devices;\n\tstruct btrfs_fs_info *fs_info;\n\n\tstruct rcu_string *name;\n\n\tu64 generation;\n\n\tspinlock_t io_lock ____cacheline_aligned;\n\tint running_pending;\n\t/* regular prio bios */\n\tstruct btrfs_pending_bios pending_bios;\n\t/* sync bios */\n\tstruct btrfs_pending_bios pending_sync_bios;\n\n\tstruct block_device *bdev;\n\n\t/* the mode sent to blkdev_get */\n\tfmode_t mode;\n\n\tunsigned long dev_state;\n\tblk_status_t last_flush_error;\n\tint flush_bio_sent;\n\n#ifdef __BTRFS_NEED_DEVICE_DATA_ORDERED\n\tseqcount_t data_seqcount;\n#endif\n\n\t/* the internal btrfs device id */\n\tu64 devid;\n\n\t/* size of the device in memory */\n\tu64 total_bytes;\n\n\t/* size of the device on disk */\n\tu64 disk_total_bytes;\n\n\t/* bytes used */\n\tu64 bytes_used;\n\n\t/* optimal io alignment for this device */\n\tu32 io_align;\n\n\t/* optimal io width for this device */\n\tu32 io_width;\n\t/* type and info about this device */\n\tu64 type;\n\n\t/* minimal io size for this device */\n\tu32 sector_size;\n\n\t/* physical drive uuid (or lvm uuid) */\n\tu8 uuid[BTRFS_UUID_SIZE];\n\n\t/*\n\t * size of the device on the current transaction\n\t *\n\t * This variant is update when committing the transaction,\n\t * and protected by device_list_mutex\n\t */\n\tu64 commit_total_bytes;\n\n\t/* bytes used on the current transaction */\n\tu64 commit_bytes_used;\n\t/*\n\t * used to manage the device which is resized\n\t *\n\t * It is protected by chunk_lock.\n\t */\n\tstruct list_head resized_list;\n\n\t/* for sending down flush barriers */\n\tstruct bio *flush_bio;\n\tstruct completion flush_wait;\n\n\t/* per-device scrub information */\n\tstruct scrub_ctx *scrub_ctx;\n\n\tstruct btrfs_work work;\n\tstruct rcu_head rcu;\n\n\t/* readahead state */\n\tatomic_t reada_in_flight;\n\tu64 reada_next;\n\tstruct reada_zone *reada_curr_zone;\n\tstruct radix_tree_root reada_zones;\n\tstruct radix_tree_root reada_extents;\n\n\t/* disk I/O failure stats. For detailed description refer to\n\t * enum btrfs_dev_stat_values in ioctl.h */\n\tint dev_stats_valid;\n\n\t/* Counter to record the change of device stats */\n\tatomic_t dev_stats_ccnt;\n\tatomic_t dev_stat_values[BTRFS_DEV_STAT_VALUES_MAX];\n};\n\n/*\n * If we read those variants at the context of their own lock, we needn't\n * use the following helpers, reading them directly is safe.\n */\n#if BITS_PER_LONG==32 && defined(CONFIG_SMP)\n#define BTRFS_DEVICE_GETSET_FUNCS(name)\t\t\t\t\t\\\nstatic inline u64\t\t\t\t\t\t\t\\\nbtrfs_device_get_##name(const struct btrfs_device *dev)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu64 size;\t\t\t\t\t\t\t\\\n\tunsigned int seq;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tseq = read_seqcount_begin(&dev->data_seqcount);\t\t\\\n\t\tsize = dev->name;\t\t\t\t\t\\\n\t} while (read_seqcount_retry(&dev->data_seqcount, seq));\t\\\n\treturn size;\t\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic inline void\t\t\t\t\t\t\t\\\nbtrfs_device_set_##name(struct btrfs_device *dev, u64 size)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\twrite_seqcount_begin(&dev->data_seqcount);\t\t\t\\\n\tdev->name = size;\t\t\t\t\t\t\\\n\twrite_seqcount_end(&dev->data_seqcount);\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n}\n#elif BITS_PER_LONG==32 && defined(CONFIG_PREEMPT)\n#define BTRFS_DEVICE_GETSET_FUNCS(name)\t\t\t\t\t\\\nstatic inline u64\t\t\t\t\t\t\t\\\nbtrfs_device_get_##name(const struct btrfs_device *dev)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu64 size;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tsize = dev->name;\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n\treturn size;\t\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic inline void\t\t\t\t\t\t\t\\\nbtrfs_device_set_##name(struct btrfs_device *dev, u64 size)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tdev->name = size;\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n}\n#else\n#define BTRFS_DEVICE_GETSET_FUNCS(name)\t\t\t\t\t\\\nstatic inline u64\t\t\t\t\t\t\t\\\nbtrfs_device_get_##name(const struct btrfs_device *dev)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn dev->name;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic inline void\t\t\t\t\t\t\t\\\nbtrfs_device_set_##name(struct btrfs_device *dev, u64 size)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tdev->name = size;\t\t\t\t\t\t\\\n}\n#endif\n\nBTRFS_DEVICE_GETSET_FUNCS(total_bytes);\nBTRFS_DEVICE_GETSET_FUNCS(disk_total_bytes);\nBTRFS_DEVICE_GETSET_FUNCS(bytes_used);\n\nstruct btrfs_fs_devices {\n\tu8 fsid[BTRFS_FSID_SIZE]; /* FS specific uuid */\n\tu8 metadata_uuid[BTRFS_FSID_SIZE];\n\tbool fsid_change;\n\tstruct list_head fs_list;\n\n\tu64 num_devices;\n\tu64 open_devices;\n\tu64 rw_devices;\n\tu64 missing_devices;\n\tu64 total_rw_bytes;\n\tu64 total_devices;\n\n\t/* Highest generation number of seen devices */\n\tu64 latest_generation;\n\n\tstruct block_device *latest_bdev;\n\n\t/* all of the devices in the FS, protected by a mutex\n\t * so we can safely walk it to write out the supers without\n\t * worrying about add/remove by the multi-device code.\n\t * Scrubbing super can kick off supers writing by holding\n\t * this mutex lock.\n\t */\n\tstruct mutex device_list_mutex;\n\tstruct list_head devices;\n\n\tstruct list_head resized_devices;\n\t/* devices not currently being allocated */\n\tstruct list_head alloc_list;\n\n\tstruct btrfs_fs_devices *seed;\n\tint seeding;\n\n\tint opened;\n\n\t/* set when we find or add a device that doesn't have the\n\t * nonrot flag set\n\t */\n\tint rotating;\n\n\tstruct btrfs_fs_info *fs_info;\n\t/* sysfs kobjects */\n\tstruct kobject fsid_kobj;\n\tstruct kobject *device_dir_kobj;\n\tstruct completion kobj_unregister;\n};\n\n#define BTRFS_BIO_INLINE_CSUM_SIZE\t64\n\n/*\n * we need the mirror number and stripe index to be passed around\n * the call chain while we are processing end_io (especially errors).\n * Really, what we need is a btrfs_bio structure that has this info\n * and is properly sized with its stripe array, but we're not there\n * quite yet.  We have our own btrfs bioset, and all of the bios\n * we allocate are actually btrfs_io_bios.  We'll cram as much of\n * struct btrfs_bio as we can into this over time.\n */\nstruct btrfs_io_bio {\n\tunsigned int mirror_num;\n\tunsigned int stripe_index;\n\tu64 logical;\n\tu8 *csum;\n\tu8 csum_inline[BTRFS_BIO_INLINE_CSUM_SIZE];\n\tstruct bvec_iter iter;\n\t/*\n\t * This member must come last, bio_alloc_bioset will allocate enough\n\t * bytes for entire btrfs_io_bio but relies on bio being last.\n\t */\n\tstruct bio bio;\n};\n\nstatic inline struct btrfs_io_bio *btrfs_io_bio(struct bio *bio)\n{\n\treturn container_of(bio, struct btrfs_io_bio, bio);\n}\n\nstatic inline void btrfs_io_bio_free_csum(struct btrfs_io_bio *io_bio)\n{\n\tif (io_bio->csum != io_bio->csum_inline) {\n\t\tkfree(io_bio->csum);\n\t\tio_bio->csum = NULL;\n\t}\n}\n\nstruct btrfs_bio_stripe {\n\tstruct btrfs_device *dev;\n\tu64 physical;\n\tu64 length; /* only used for discard mappings */\n};\n\nstruct btrfs_bio {\n\trefcount_t refs;\n\tatomic_t stripes_pending;\n\tstruct btrfs_fs_info *fs_info;\n\tu64 map_type; /* get from map_lookup->type */\n\tbio_end_io_t *end_io;\n\tstruct bio *orig_bio;\n\tunsigned long flags;\n\tvoid *private;\n\tatomic_t error;\n\tint max_errors;\n\tint num_stripes;\n\tint mirror_num;\n\tint num_tgtdevs;\n\tint *tgtdev_map;\n\t/*\n\t * logical block numbers for the start of each stripe\n\t * The last one or two are p/q.  These are sorted,\n\t * so raid_map[0] is the start of our full stripe\n\t */\n\tu64 *raid_map;\n\tstruct btrfs_bio_stripe stripes[];\n};\n\nstruct btrfs_device_info {\n\tstruct btrfs_device *dev;\n\tu64 dev_offset;\n\tu64 max_avail;\n\tu64 total_avail;\n};\n\nstruct btrfs_raid_attr {\n\tint sub_stripes;\t/* sub_stripes info for map */\n\tint dev_stripes;\t/* stripes per dev */\n\tint devs_max;\t\t/* max devs to use */\n\tint devs_min;\t\t/* min devs needed */\n\tint tolerated_failures; /* max tolerated fail devs */\n\tint devs_increment;\t/* ndevs has to be a multiple of this */\n\tint ncopies;\t\t/* how many copies to data has */\n\tint nparity;\t\t/* number of stripes worth of bytes to store\n\t\t\t\t * parity information */\n\tint mindev_error;\t/* error code if min devs requisite is unmet */\n\tconst char raid_name[8]; /* name of the raid */\n\tu64 bg_flag;\t\t/* block group flag of the raid */\n};\n\nextern const struct btrfs_raid_attr btrfs_raid_array[BTRFS_NR_RAID_TYPES];\n\nstruct map_lookup {\n\tu64 type;\n\tint io_align;\n\tint io_width;\n\tu64 stripe_len;\n\tint num_stripes;\n\tint sub_stripes;\n\tint verified_stripes; /* For mount time dev extent verification */\n\tstruct btrfs_bio_stripe stripes[];\n};\n\n#define map_lookup_size(n) (sizeof(struct map_lookup) + \\\n\t\t\t    (sizeof(struct btrfs_bio_stripe) * (n)))\n\nstruct btrfs_balance_args;\nstruct btrfs_balance_progress;\nstruct btrfs_balance_control {\n\tstruct btrfs_balance_args data;\n\tstruct btrfs_balance_args meta;\n\tstruct btrfs_balance_args sys;\n\n\tu64 flags;\n\n\tstruct btrfs_balance_progress stat;\n};\n\nenum btrfs_map_op {\n\tBTRFS_MAP_READ,\n\tBTRFS_MAP_WRITE,\n\tBTRFS_MAP_DISCARD,\n\tBTRFS_MAP_GET_READ_MIRRORS,\n};\n\nstatic inline enum btrfs_map_op btrfs_op(struct bio *bio)\n{\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\t\treturn BTRFS_MAP_DISCARD;\n\tcase REQ_OP_WRITE:\n\t\treturn BTRFS_MAP_WRITE;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\tcase REQ_OP_READ:\n\t\treturn BTRFS_MAP_READ;\n\t}\n}\n\nvoid btrfs_get_bbio(struct btrfs_bio *bbio);\nvoid btrfs_put_bbio(struct btrfs_bio *bbio);\nint btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t    u64 logical, u64 *length,\n\t\t    struct btrfs_bio **bbio_ret, int mirror_num);\nint btrfs_map_sblock(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t     u64 logical, u64 *length,\n\t\t     struct btrfs_bio **bbio_ret);\nint btrfs_rmap_block(struct btrfs_fs_info *fs_info, u64 chunk_start,\n\t\t     u64 physical, u64 **logical, int *naddrs, int *stripe_len);\nint btrfs_read_sys_array(struct btrfs_fs_info *fs_info);\nint btrfs_read_chunk_tree(struct btrfs_fs_info *fs_info);\nint btrfs_alloc_chunk(struct btrfs_trans_handle *trans, u64 type);\nvoid btrfs_mapping_init(struct btrfs_mapping_tree *tree);\nvoid btrfs_mapping_tree_free(struct btrfs_mapping_tree *tree);\nblk_status_t btrfs_map_bio(struct btrfs_fs_info *fs_info, struct bio *bio,\n\t\t\t   int mirror_num, int async_submit);\nint btrfs_open_devices(struct btrfs_fs_devices *fs_devices,\n\t\t       fmode_t flags, void *holder);\nstruct btrfs_device *btrfs_scan_one_device(const char *path,\n\t\t\t\t\t   fmode_t flags, void *holder);\nint btrfs_close_devices(struct btrfs_fs_devices *fs_devices);\nvoid btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices, int step);\nvoid btrfs_assign_next_active_device(struct btrfs_device *device,\n\t\t\t\t     struct btrfs_device *this_dev);\nstruct btrfs_device *btrfs_find_device_by_devspec(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u64 devid,\n\t\t\t\t\t\t  const char *devpath);\nstruct btrfs_device *btrfs_alloc_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tconst u64 *devid,\n\t\t\t\t\tconst u8 *uuid);\nvoid btrfs_free_device(struct btrfs_device *device);\nint btrfs_rm_device(struct btrfs_fs_info *fs_info,\n\t\t    const char *device_path, u64 devid);\nvoid __exit btrfs_cleanup_fs_uuids(void);\nint btrfs_num_copies(struct btrfs_fs_info *fs_info, u64 logical, u64 len);\nint btrfs_grow_device(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_device *device, u64 new_size);\nstruct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t       u64 devid, u8 *uuid, u8 *fsid, bool seed);\nint btrfs_shrink_device(struct btrfs_device *device, u64 new_size);\nint btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *path);\nint btrfs_balance(struct btrfs_fs_info *fs_info,\n\t\t  struct btrfs_balance_control *bctl,\n\t\t  struct btrfs_ioctl_balance_args *bargs);\nvoid btrfs_describe_block_groups(u64 flags, char *buf, u32 size_buf);\nint btrfs_resume_balance_async(struct btrfs_fs_info *fs_info);\nint btrfs_recover_balance(struct btrfs_fs_info *fs_info);\nint btrfs_pause_balance(struct btrfs_fs_info *fs_info);\nint btrfs_cancel_balance(struct btrfs_fs_info *fs_info);\nint btrfs_create_uuid_tree(struct btrfs_fs_info *fs_info);\nint btrfs_check_uuid_tree(struct btrfs_fs_info *fs_info);\nint btrfs_chunk_readonly(struct btrfs_fs_info *fs_info, u64 chunk_offset);\nint find_free_dev_extent_start(struct btrfs_transaction *transaction,\n\t\t\t struct btrfs_device *device, u64 num_bytes,\n\t\t\t u64 search_start, u64 *start, u64 *max_avail);\nint find_free_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_device *device, u64 num_bytes,\n\t\t\t u64 *start, u64 *max_avail);\nvoid btrfs_dev_stat_inc_and_print(struct btrfs_device *dev, int index);\nint btrfs_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\tstruct btrfs_ioctl_get_dev_stats *stats);\nvoid btrfs_init_devices_late(struct btrfs_fs_info *fs_info);\nint btrfs_init_dev_stats(struct btrfs_fs_info *fs_info);\nint btrfs_run_dev_stats(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_fs_info *fs_info);\nvoid btrfs_rm_dev_replace_remove_srcdev(struct btrfs_device *srcdev);\nvoid btrfs_rm_dev_replace_free_srcdev(struct btrfs_fs_info *fs_info,\n\t\t\t\t      struct btrfs_device *srcdev);\nvoid btrfs_destroy_dev_replace_tgtdev(struct btrfs_device *tgtdev);\nvoid btrfs_scratch_superblocks(struct block_device *bdev, const char *device_path);\nint btrfs_is_parity_mirror(struct btrfs_fs_info *fs_info,\n\t\t\t   u64 logical, u64 len);\nunsigned long btrfs_full_stripe_len(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 logical);\nint btrfs_finish_chunk_alloc(struct btrfs_trans_handle *trans,\n\t\t\t     u64 chunk_offset, u64 chunk_size);\nint btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset);\nstruct extent_map *btrfs_get_chunk_map(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u64 logical, u64 length);\n\nstatic inline void btrfs_dev_stat_inc(struct btrfs_device *dev,\n\t\t\t\t      int index)\n{\n\tatomic_inc(dev->dev_stat_values + index);\n\t/*\n\t * This memory barrier orders stores updating statistics before stores\n\t * updating dev_stats_ccnt.\n\t *\n\t * It pairs with smp_rmb() in btrfs_run_dev_stats().\n\t */\n\tsmp_mb__before_atomic();\n\tatomic_inc(&dev->dev_stats_ccnt);\n}\n\nstatic inline int btrfs_dev_stat_read(struct btrfs_device *dev,\n\t\t\t\t      int index)\n{\n\treturn atomic_read(dev->dev_stat_values + index);\n}\n\nstatic inline int btrfs_dev_stat_read_and_reset(struct btrfs_device *dev,\n\t\t\t\t\t\tint index)\n{\n\tint ret;\n\n\tret = atomic_xchg(dev->dev_stat_values + index, 0);\n\t/*\n\t * atomic_xchg implies a full memory barriers as per atomic_t.txt:\n\t * - RMW operations that have a return value are fully ordered;\n\t *\n\t * This implicit memory barriers is paired with the smp_rmb in\n\t * btrfs_run_dev_stats\n\t */\n\tatomic_inc(&dev->dev_stats_ccnt);\n\treturn ret;\n}\n\nstatic inline void btrfs_dev_stat_set(struct btrfs_device *dev,\n\t\t\t\t      int index, unsigned long val)\n{\n\tatomic_set(dev->dev_stat_values + index, val);\n\t/*\n\t * This memory barrier orders stores updating statistics before stores\n\t * updating dev_stats_ccnt.\n\t *\n\t * It pairs with smp_rmb() in btrfs_run_dev_stats().\n\t */\n\tsmp_mb__before_atomic();\n\tatomic_inc(&dev->dev_stats_ccnt);\n}\n\nstatic inline void btrfs_dev_stat_reset(struct btrfs_device *dev,\n\t\t\t\t\tint index)\n{\n\tbtrfs_dev_stat_set(dev, index, 0);\n}\n\n/*\n * Convert block group flags (BTRFS_BLOCK_GROUP_*) to btrfs_raid_types, which\n * can be used as index to access btrfs_raid_array[].\n */\nstatic inline enum btrfs_raid_types btrfs_bg_flags_to_raid_index(u64 flags)\n{\n\tif (flags & BTRFS_BLOCK_GROUP_RAID10)\n\t\treturn BTRFS_RAID_RAID10;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID1)\n\t\treturn BTRFS_RAID_RAID1;\n\telse if (flags & BTRFS_BLOCK_GROUP_DUP)\n\t\treturn BTRFS_RAID_DUP;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID0)\n\t\treturn BTRFS_RAID_RAID0;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID5)\n\t\treturn BTRFS_RAID_RAID5;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID6)\n\t\treturn BTRFS_RAID_RAID6;\n\n\treturn BTRFS_RAID_SINGLE; /* BTRFS_BLOCK_GROUP_SINGLE */\n}\n\nconst char *get_raid_name(enum btrfs_raid_types type);\n\nvoid btrfs_update_commit_device_size(struct btrfs_fs_info *fs_info);\nvoid btrfs_update_commit_device_bytes_used(struct btrfs_transaction *trans);\n\nstruct list_head *btrfs_get_fs_uuids(void);\nvoid btrfs_set_fs_info_ptr(struct btrfs_fs_info *fs_info);\nvoid btrfs_reset_fs_info_ptr(struct btrfs_fs_info *fs_info);\nbool btrfs_check_rw_degradable(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct btrfs_device *failing_dev);\n\nint btrfs_bg_type_to_factor(u64 flags);\nint btrfs_verify_dev_extents(struct btrfs_fs_info *fs_info);\n\n#endif\n"], "filenames": ["fs/btrfs/dev-replace.c", "fs/btrfs/ioctl.c", "fs/btrfs/scrub.c", "fs/btrfs/volumes.c", "fs/btrfs/volumes.h"], "buggy_code_start_loc": [115, 1645, 3838, 418, 437], "buggy_code_end_loc": [119, 3183, 4016, 7836, 438], "fixing_code_start_loc": [115, 1645, 3838, 417, 437], "fixing_code_end_loc": [119, 3183, 4016, 7834, 438], "type": "CWE-476", "message": "fs/btrfs/volumes.c in the Linux kernel before 5.1 allows a btrfs_verify_dev_extents NULL pointer dereference via a crafted btrfs image because fs_devices->devices is mishandled within find_device, aka CID-09ba3bc9dd15.", "other": {"cve": {"id": "CVE-2019-18885", "sourceIdentifier": "cve@mitre.org", "published": "2019-11-14T14:15:11.630", "lastModified": "2021-06-14T18:15:18.097", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "fs/btrfs/volumes.c in the Linux kernel before 5.1 allows a btrfs_verify_dev_extents NULL pointer dereference via a crafted btrfs image because fs_devices->devices is mishandled within find_device, aka CID-09ba3bc9dd15."}, {"lang": "es", "value": "El archivo fs/btrfs/volumes.c en el kernel de Linux versiones anteriores a la versi\u00f3n  5.1, permite una desreferencia del puntero NULL de la funci\u00f3n btrfs_verify_dev_extents por medio de una imagen btrfs especialmente dise\u00f1ada porque fs_devices-)devices es manejada inapropiadamente dentro de find_device, tambi\u00e9n se conoce como CID-09ba3bc9dd15."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.1", "matchCriteriaId": "B5857A73-CFC4-477B-8E21-2EDC12FABFDE"}]}]}], "references": [{"url": "http://packetstormsecurity.com/files/156185/Kernel-Live-Patch-Security-Notice-LSN-0062-1.html", "source": "cve@mitre.org"}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=09ba3bc9dd150457c506e4661380a6183af651c1", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/bobfuzzer/CVE-2019-18885", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/09ba3bc9dd150457c506e4661380a6183af651c1", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2020/08/msg00019.html", "source": "cve@mitre.org"}, {"url": "https://security.netapp.com/advisory/ntap-20191205-0001/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4254-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4254-2/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4258-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4287-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4287-2/", "source": "cve@mitre.org"}, {"url": "https://www.oracle.com/security-alerts/cpuApr2021.html", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/09ba3bc9dd150457c506e4661380a6183af651c1"}}