{"buggy_code": ["/*\n * Memory Migration functionality - linux/mm/migrate.c\n *\n * Copyright (C) 2006 Silicon Graphics, Inc., Christoph Lameter\n *\n * Page migration was first developed in the context of the memory hotplug\n * project. The main authors of the migration code are:\n *\n * IWAMOTO Toshihiro <iwamoto@valinux.co.jp>\n * Hirokazu Takahashi <taka@valinux.co.jp>\n * Dave Hansen <haveblue@us.ibm.com>\n * Christoph Lameter\n */\n\n#include <linux/migrate.h>\n#include <linux/export.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/pagemap.h>\n#include <linux/buffer_head.h>\n#include <linux/mm_inline.h>\n#include <linux/nsproxy.h>\n#include <linux/pagevec.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/topology.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/writeback.h>\n#include <linux/mempolicy.h>\n#include <linux/vmalloc.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/hugetlb.h>\n#include <linux/hugetlb_cgroup.h>\n#include <linux/gfp.h>\n#include <linux/balloon_compaction.h>\n#include <linux/mmu_notifier.h>\n#include <linux/page_idle.h>\n\n#include <asm/tlbflush.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/migrate.h>\n\n#include \"internal.h\"\n\n/*\n * migrate_prep() needs to be called before we start compiling a list of pages\n * to be migrated using isolate_lru_page(). If scheduling work on other CPUs is\n * undesirable, use migrate_prep_local()\n */\nint migrate_prep(void)\n{\n\t/*\n\t * Clear the LRU lists so pages can be isolated.\n\t * Note that pages may be moved off the LRU after we have\n\t * drained them. Those pages will fail to migrate like other\n\t * pages that may be busy.\n\t */\n\tlru_add_drain_all();\n\n\treturn 0;\n}\n\n/* Do the necessary work of migrate_prep but not if it involves other CPUs */\nint migrate_prep_local(void)\n{\n\tlru_add_drain();\n\n\treturn 0;\n}\n\n/*\n * Put previously isolated pages back onto the appropriate lists\n * from where they were once taken off for compaction/migration.\n *\n * This function shall be used whenever the isolated pageset has been\n * built from lru, balloon, hugetlbfs page. See isolate_migratepages_range()\n * and isolate_huge_page().\n */\nvoid putback_movable_pages(struct list_head *l)\n{\n\tstruct page *page;\n\tstruct page *page2;\n\n\tlist_for_each_entry_safe(page, page2, l, lru) {\n\t\tif (unlikely(PageHuge(page))) {\n\t\t\tputback_active_hugepage(page);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_del(&page->lru);\n\t\tdec_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\tpage_is_file_cache(page));\n\t\tif (unlikely(isolated_balloon_page(page)))\n\t\t\tballoon_page_putback(page);\n\t\telse\n\t\t\tputback_lru_page(page);\n\t}\n}\n\n/*\n * Restore a potential migration pte to a working pte entry\n */\nstatic int remove_migration_pte(struct page *new, struct vm_area_struct *vma,\n\t\t\t\t unsigned long addr, void *old)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tswp_entry_t entry;\n \tpmd_t *pmd;\n\tpte_t *ptep, pte;\n \tspinlock_t *ptl;\n\n\tif (unlikely(PageHuge(new))) {\n\t\tptep = huge_pte_offset(mm, addr);\n\t\tif (!ptep)\n\t\t\tgoto out;\n\t\tptl = huge_pte_lockptr(hstate_vma(vma), mm, ptep);\n\t} else {\n\t\tpmd = mm_find_pmd(mm, addr);\n\t\tif (!pmd)\n\t\t\tgoto out;\n\n\t\tptep = pte_offset_map(pmd, addr);\n\n\t\t/*\n\t\t * Peek to check is_swap_pte() before taking ptlock?  No, we\n\t\t * can race mremap's move_ptes(), which skips anon_vma lock.\n\t\t */\n\n\t\tptl = pte_lockptr(mm, pmd);\n\t}\n\n \tspin_lock(ptl);\n\tpte = *ptep;\n\tif (!is_swap_pte(pte))\n\t\tgoto unlock;\n\n\tentry = pte_to_swp_entry(pte);\n\n\tif (!is_migration_entry(entry) ||\n\t    migration_entry_to_page(entry) != old)\n\t\tgoto unlock;\n\n\tget_page(new);\n\tpte = pte_mkold(mk_pte(new, vma->vm_page_prot));\n\tif (pte_swp_soft_dirty(*ptep))\n\t\tpte = pte_mksoft_dirty(pte);\n\n\t/* Recheck VMA as permissions can change since migration started  */\n\tif (is_write_migration_entry(entry))\n\t\tpte = maybe_mkwrite(pte, vma);\n\n#ifdef CONFIG_HUGETLB_PAGE\n\tif (PageHuge(new)) {\n\t\tpte = pte_mkhuge(pte);\n\t\tpte = arch_make_huge_pte(pte, vma, new, 0);\n\t}\n#endif\n\tflush_dcache_page(new);\n\tset_pte_at(mm, addr, ptep, pte);\n\n\tif (PageHuge(new)) {\n\t\tif (PageAnon(new))\n\t\t\thugepage_add_anon_rmap(new, vma, addr);\n\t\telse\n\t\t\tpage_dup_rmap(new);\n\t} else if (PageAnon(new))\n\t\tpage_add_anon_rmap(new, vma, addr);\n\telse\n\t\tpage_add_file_rmap(new);\n\n\tif (vma->vm_flags & VM_LOCKED)\n\t\tmlock_vma_page(new);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, addr, ptep);\nunlock:\n\tpte_unmap_unlock(ptep, ptl);\nout:\n\treturn SWAP_AGAIN;\n}\n\n/*\n * Get rid of all migration entries and replace them by\n * references to the indicated page.\n */\nstatic void remove_migration_ptes(struct page *old, struct page *new)\n{\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = remove_migration_pte,\n\t\t.arg = old,\n\t};\n\n\trmap_walk(new, &rwc);\n}\n\n/*\n * Something used the pte of a page under migration. We need to\n * get to the page and wait until migration is finished.\n * When we return from this function the fault will be retried.\n */\nvoid __migration_entry_wait(struct mm_struct *mm, pte_t *ptep,\n\t\t\t\tspinlock_t *ptl)\n{\n\tpte_t pte;\n\tswp_entry_t entry;\n\tstruct page *page;\n\n\tspin_lock(ptl);\n\tpte = *ptep;\n\tif (!is_swap_pte(pte))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(pte);\n\tif (!is_migration_entry(entry))\n\t\tgoto out;\n\n\tpage = migration_entry_to_page(entry);\n\n\t/*\n\t * Once radix-tree replacement of page migration started, page_count\n\t * *must* be zero. And, we don't want to call wait_on_page_locked()\n\t * against a page without get_page().\n\t * So, we use get_page_unless_zero(), here. Even failed, page fault\n\t * will occur again.\n\t */\n\tif (!get_page_unless_zero(page))\n\t\tgoto out;\n\tpte_unmap_unlock(ptep, ptl);\n\twait_on_page_locked(page);\n\tput_page(page);\n\treturn;\nout:\n\tpte_unmap_unlock(ptep, ptl);\n}\n\nvoid migration_entry_wait(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t\tunsigned long address)\n{\n\tspinlock_t *ptl = pte_lockptr(mm, pmd);\n\tpte_t *ptep = pte_offset_map(pmd, address);\n\t__migration_entry_wait(mm, ptep, ptl);\n}\n\nvoid migration_entry_wait_huge(struct vm_area_struct *vma,\n\t\tstruct mm_struct *mm, pte_t *pte)\n{\n\tspinlock_t *ptl = huge_pte_lockptr(hstate_vma(vma), mm, pte);\n\t__migration_entry_wait(mm, pte, ptl);\n}\n\n#ifdef CONFIG_BLOCK\n/* Returns true if all buffers are successfully locked */\nstatic bool buffer_migrate_lock_buffers(struct buffer_head *head,\n\t\t\t\t\t\t\tenum migrate_mode mode)\n{\n\tstruct buffer_head *bh = head;\n\n\t/* Simple case, sync compaction */\n\tif (mode != MIGRATE_ASYNC) {\n\t\tdo {\n\t\t\tget_bh(bh);\n\t\t\tlock_buffer(bh);\n\t\t\tbh = bh->b_this_page;\n\n\t\t} while (bh != head);\n\n\t\treturn true;\n\t}\n\n\t/* async case, we cannot block on lock_buffer so use trylock_buffer */\n\tdo {\n\t\tget_bh(bh);\n\t\tif (!trylock_buffer(bh)) {\n\t\t\t/*\n\t\t\t * We failed to lock the buffer and cannot stall in\n\t\t\t * async migration. Release the taken locks\n\t\t\t */\n\t\t\tstruct buffer_head *failed_bh = bh;\n\t\t\tput_bh(failed_bh);\n\t\t\tbh = head;\n\t\t\twhile (bh != failed_bh) {\n\t\t\t\tunlock_buffer(bh);\n\t\t\t\tput_bh(bh);\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\treturn true;\n}\n#else\nstatic inline bool buffer_migrate_lock_buffers(struct buffer_head *head,\n\t\t\t\t\t\t\tenum migrate_mode mode)\n{\n\treturn true;\n}\n#endif /* CONFIG_BLOCK */\n\n/*\n * Replace the page in the mapping.\n *\n * The number of remaining references must be:\n * 1 for anonymous pages without a mapping\n * 2 for pages with a mapping\n * 3 for pages with a mapping and PagePrivate/PagePrivate2 set.\n */\nint migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\t__dec_zone_page_state(page, NR_FILE_PAGES);\n\t__inc_zone_page_state(newpage, NR_FILE_PAGES);\n\tif (!PageSwapCache(page) && PageSwapBacked(page)) {\n\t\t__dec_zone_page_state(page, NR_SHMEM);\n\t\t__inc_zone_page_state(newpage, NR_SHMEM);\n\t}\n\tspin_unlock_irq(&mapping->tree_lock);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\n\n/*\n * The expected number of remaining references is the same as that\n * of migrate_page_move_mapping().\n */\nint migrate_huge_page_move_mapping(struct address_space *mapping,\n\t\t\t\t   struct page *newpage, struct page *page)\n{\n\tint expected_count;\n\tvoid **pslot;\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n\t\t\t\t\tpage_index(page));\n\n\texpected_count = 2 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tget_page(newpage);\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\tspin_unlock_irq(&mapping->tree_lock);\n\treturn MIGRATEPAGE_SUCCESS;\n}\n\n/*\n * Gigantic pages are so large that we do not guarantee that page++ pointer\n * arithmetic will work across the entire page.  We need something more\n * specialized.\n */\nstatic void __copy_gigantic_page(struct page *dst, struct page *src,\n\t\t\t\tint nr_pages)\n{\n\tint i;\n\tstruct page *dst_base = dst;\n\tstruct page *src_base = src;\n\n\tfor (i = 0; i < nr_pages; ) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst, src);\n\n\t\ti++;\n\t\tdst = mem_map_next(dst, dst_base, i);\n\t\tsrc = mem_map_next(src, src_base, i);\n\t}\n}\n\nstatic void copy_huge_page(struct page *dst, struct page *src)\n{\n\tint i;\n\tint nr_pages;\n\n\tif (PageHuge(src)) {\n\t\t/* hugetlbfs page */\n\t\tstruct hstate *h = page_hstate(src);\n\t\tnr_pages = pages_per_huge_page(h);\n\n\t\tif (unlikely(nr_pages > MAX_ORDER_NR_PAGES)) {\n\t\t\t__copy_gigantic_page(dst, src, nr_pages);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\t/* thp page */\n\t\tBUG_ON(!PageTransHuge(src));\n\t\tnr_pages = hpage_nr_pages(src);\n\t}\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst + i, src + i);\n\t}\n}\n\n/*\n * Copy the page to its new location\n */\nvoid migrate_page_copy(struct page *newpage, struct page *page)\n{\n\tint cpupid;\n\n\tif (PageHuge(page) || PageTransHuge(page))\n\t\tcopy_huge_page(newpage, page);\n\telse\n\t\tcopy_highpage(newpage, page);\n\n\tif (PageError(page))\n\t\tSetPageError(newpage);\n\tif (PageReferenced(page))\n\t\tSetPageReferenced(newpage);\n\tif (PageUptodate(page))\n\t\tSetPageUptodate(newpage);\n\tif (TestClearPageActive(page)) {\n\t\tVM_BUG_ON_PAGE(PageUnevictable(page), page);\n\t\tSetPageActive(newpage);\n\t} else if (TestClearPageUnevictable(page))\n\t\tSetPageUnevictable(newpage);\n\tif (PageChecked(page))\n\t\tSetPageChecked(newpage);\n\tif (PageMappedToDisk(page))\n\t\tSetPageMappedToDisk(newpage);\n\n\tif (PageDirty(page)) {\n\t\tclear_page_dirty_for_io(page);\n\t\t/*\n\t\t * Want to mark the page and the radix tree as dirty, and\n\t\t * redo the accounting that clear_page_dirty_for_io undid,\n\t\t * but we can't use set_page_dirty because that function\n\t\t * is actually a signal that all of the page has become dirty.\n\t\t * Whereas only part of our page may be dirty.\n\t\t */\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageDirty(newpage);\n\t\telse\n\t\t\t__set_page_dirty_nobuffers(newpage);\n \t}\n\n\tif (page_is_young(page))\n\t\tset_page_young(newpage);\n\tif (page_is_idle(page))\n\t\tset_page_idle(newpage);\n\n\t/*\n\t * Copy NUMA information to the new page, to prevent over-eager\n\t * future migrations of this same page.\n\t */\n\tcpupid = page_cpupid_xchg_last(page, -1);\n\tpage_cpupid_xchg_last(newpage, cpupid);\n\n\tksm_migrate_page(newpage, page);\n\t/*\n\t * Please do not reorder this without considering how mm/ksm.c's\n\t * get_ksm_page() depends upon ksm_migrate_page() and PageSwapCache().\n\t */\n\tif (PageSwapCache(page))\n\t\tClearPageSwapCache(page);\n\tClearPagePrivate(page);\n\tset_page_private(page, 0);\n\n\t/*\n\t * If any waiters have accumulated on the new page then\n\t * wake them up.\n\t */\n\tif (PageWriteback(newpage))\n\t\tend_page_writeback(newpage);\n}\n\n/************************************************************\n *                    Migration functions\n ***********************************************************/\n\n/*\n * Common logic to directly migrate a single page suitable for\n * pages that do not use PagePrivate/PagePrivate2.\n *\n * Pages are locked upon entry and exit.\n */\nint migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tenum migrate_mode mode)\n{\n\tint rc;\n\n\tBUG_ON(PageWriteback(page));\t/* Writeback must be complete */\n\n\trc = migrate_page_move_mapping(mapping, newpage, page, NULL, mode, 0);\n\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\treturn rc;\n\n\tmigrate_page_copy(newpage, page);\n\treturn MIGRATEPAGE_SUCCESS;\n}\nEXPORT_SYMBOL(migrate_page);\n\n#ifdef CONFIG_BLOCK\n/*\n * Migration function for pages with buffers. This function can only be used\n * if the underlying filesystem guarantees that no other references to \"page\"\n * exist.\n */\nint buffer_migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tstruct buffer_head *bh, *head;\n\tint rc;\n\n\tif (!page_has_buffers(page))\n\t\treturn migrate_page(mapping, newpage, page, mode);\n\n\thead = page_buffers(page);\n\n\trc = migrate_page_move_mapping(mapping, newpage, page, head, mode, 0);\n\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\treturn rc;\n\n\t/*\n\t * In the async case, migrate_page_move_mapping locked the buffers\n\t * with an IRQ-safe spinlock held. In the sync case, the buffers\n\t * need to be locked now\n\t */\n\tif (mode != MIGRATE_ASYNC)\n\t\tBUG_ON(!buffer_migrate_lock_buffers(head, mode));\n\n\tClearPagePrivate(page);\n\tset_page_private(newpage, page_private(page));\n\tset_page_private(page, 0);\n\tput_page(page);\n\tget_page(newpage);\n\n\tbh = head;\n\tdo {\n\t\tset_bh_page(bh, newpage, bh_offset(bh));\n\t\tbh = bh->b_this_page;\n\n\t} while (bh != head);\n\n\tSetPagePrivate(newpage);\n\n\tmigrate_page_copy(newpage, page);\n\n\tbh = head;\n\tdo {\n\t\tunlock_buffer(bh);\n \t\tput_bh(bh);\n\t\tbh = bh->b_this_page;\n\n\t} while (bh != head);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\nEXPORT_SYMBOL(buffer_migrate_page);\n#endif\n\n/*\n * Writeback a page to clean the dirty state\n */\nstatic int writeout(struct address_space *mapping, struct page *page)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_NONE,\n\t\t.nr_to_write = 1,\n\t\t.range_start = 0,\n\t\t.range_end = LLONG_MAX,\n\t\t.for_reclaim = 1\n\t};\n\tint rc;\n\n\tif (!mapping->a_ops->writepage)\n\t\t/* No write method for the address space */\n\t\treturn -EINVAL;\n\n\tif (!clear_page_dirty_for_io(page))\n\t\t/* Someone else already triggered a write */\n\t\treturn -EAGAIN;\n\n\t/*\n\t * A dirty page may imply that the underlying filesystem has\n\t * the page on some queue. So the page must be clean for\n\t * migration. Writeout may mean we loose the lock and the\n\t * page state is no longer what we checked for earlier.\n\t * At this point we know that the migration attempt cannot\n\t * be successful.\n\t */\n\tremove_migration_ptes(page, page);\n\n\trc = mapping->a_ops->writepage(page, &wbc);\n\n\tif (rc != AOP_WRITEPAGE_ACTIVATE)\n\t\t/* unlocked. Relock */\n\t\tlock_page(page);\n\n\treturn (rc < 0) ? -EIO : -EAGAIN;\n}\n\n/*\n * Default handling if a filesystem does not provide a migration function.\n */\nstatic int fallback_migrate_page(struct address_space *mapping,\n\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tif (PageDirty(page)) {\n\t\t/* Only writeback pages in full synchronous migration */\n\t\tif (mode != MIGRATE_SYNC)\n\t\t\treturn -EBUSY;\n\t\treturn writeout(mapping, page);\n\t}\n\n\t/*\n\t * Buffers may be managed in a filesystem specific way.\n\t * We must have no buffers or drop them.\n\t */\n\tif (page_has_private(page) &&\n\t    !try_to_release_page(page, GFP_KERNEL))\n\t\treturn -EAGAIN;\n\n\treturn migrate_page(mapping, newpage, page, mode);\n}\n\n/*\n * Move a page to a newly allocated page\n * The page is locked and all ptes have been successfully removed.\n *\n * The new page will have replaced the old page if this function\n * is successful.\n *\n * Return value:\n *   < 0 - error code\n *  MIGRATEPAGE_SUCCESS - success\n */\nstatic int move_to_new_page(struct page *newpage, struct page *page,\n\t\t\t\tenum migrate_mode mode)\n{\n\tstruct address_space *mapping;\n\tint rc;\n\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tVM_BUG_ON_PAGE(!PageLocked(newpage), newpage);\n\n\tmapping = page_mapping(page);\n\tif (!mapping)\n\t\trc = migrate_page(mapping, newpage, page, mode);\n\telse if (mapping->a_ops->migratepage)\n\t\t/*\n\t\t * Most pages have a mapping and most filesystems provide a\n\t\t * migratepage callback. Anonymous pages are part of swap\n\t\t * space which also has its own migratepage callback. This\n\t\t * is the most common path for page migration.\n\t\t */\n\t\trc = mapping->a_ops->migratepage(mapping, newpage, page, mode);\n\telse\n\t\trc = fallback_migrate_page(mapping, newpage, page, mode);\n\n\t/*\n\t * When successful, old pagecache page->mapping must be cleared before\n\t * page is freed; but stats require that PageAnon be left as PageAnon.\n\t */\n\tif (rc == MIGRATEPAGE_SUCCESS) {\n\t\tset_page_memcg(page, NULL);\n\t\tif (!PageAnon(page))\n\t\t\tpage->mapping = NULL;\n\t}\n\treturn rc;\n}\n\nstatic int __unmap_and_move(struct page *page, struct page *newpage,\n\t\t\t\tint force, enum migrate_mode mode)\n{\n\tint rc = -EAGAIN;\n\tint page_was_mapped = 0;\n\tstruct anon_vma *anon_vma = NULL;\n\n\tif (!trylock_page(page)) {\n\t\tif (!force || mode == MIGRATE_ASYNC)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * It's not safe for direct compaction to call lock_page.\n\t\t * For example, during page readahead pages are added locked\n\t\t * to the LRU. Later, when the IO completes the pages are\n\t\t * marked uptodate and unlocked. However, the queueing\n\t\t * could be merging multiple pages for one bio (e.g.\n\t\t * mpage_readpages). If an allocation happens for the\n\t\t * second or third page, the process can end up locking\n\t\t * the same page twice and deadlocking. Rather than\n\t\t * trying to be clever about what pages can be locked,\n\t\t * avoid the use of lock_page for direct compaction\n\t\t * altogether.\n\t\t */\n\t\tif (current->flags & PF_MEMALLOC)\n\t\t\tgoto out;\n\n\t\tlock_page(page);\n\t}\n\n\tif (PageWriteback(page)) {\n\t\t/*\n\t\t * Only in the case of a full synchronous migration is it\n\t\t * necessary to wait for PageWriteback. In the async case,\n\t\t * the retry loop is too short and in the sync-light case,\n\t\t * the overhead of stalling is too much\n\t\t */\n\t\tif (mode != MIGRATE_SYNC) {\n\t\t\trc = -EBUSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (!force)\n\t\t\tgoto out_unlock;\n\t\twait_on_page_writeback(page);\n\t}\n\n\t/*\n\t * By try_to_unmap(), page->mapcount goes down to 0 here. In this case,\n\t * we cannot notice that anon_vma is freed while we migrates a page.\n\t * This get_anon_vma() delays freeing anon_vma pointer until the end\n\t * of migration. File cache pages are no problem because of page_lock()\n\t * File Caches may use write_page() or lock_page() in migration, then,\n\t * just care Anon page here.\n\t *\n\t * Only page_get_anon_vma() understands the subtleties of\n\t * getting a hold on an anon_vma from outside one of its mms.\n\t * But if we cannot get anon_vma, then we won't need it anyway,\n\t * because that implies that the anon page is no longer mapped\n\t * (and cannot be remapped so long as we hold the page lock).\n\t */\n\tif (PageAnon(page) && !PageKsm(page))\n\t\tanon_vma = page_get_anon_vma(page);\n\n\t/*\n\t * Block others from accessing the new page when we get around to\n\t * establishing additional references. We are usually the only one\n\t * holding a reference to newpage at this point. We used to have a BUG\n\t * here if trylock_page(newpage) fails, but would like to allow for\n\t * cases where there might be a race with the previous use of newpage.\n\t * This is much like races on refcount of oldpage: just don't BUG().\n\t */\n\tif (unlikely(!trylock_page(newpage)))\n\t\tgoto out_unlock;\n\n\tif (unlikely(isolated_balloon_page(page))) {\n\t\t/*\n\t\t * A ballooned page does not need any special attention from\n\t\t * physical to virtual reverse mapping procedures.\n\t\t * Skip any attempt to unmap PTEs or to remap swap cache,\n\t\t * in order to avoid burning cycles at rmap level, and perform\n\t\t * the page migration right away (proteced by page lock).\n\t\t */\n\t\trc = balloon_page_migrate(newpage, page, mode);\n\t\tgoto out_unlock_both;\n\t}\n\n\t/*\n\t * Corner case handling:\n\t * 1. When a new swap-cache page is read into, it is added to the LRU\n\t * and treated as swapcache but it has no rmap yet.\n\t * Calling try_to_unmap() against a page->mapping==NULL page will\n\t * trigger a BUG.  So handle it here.\n\t * 2. An orphaned page (see truncate_complete_page) might have\n\t * fs-private metadata. The page can be picked up due to memory\n\t * offlining.  Everywhere else except page reclaim, the page is\n\t * invisible to the vm, so the page can not be migrated.  So try to\n\t * free the metadata, so the page can be freed.\n\t */\n\tif (!page->mapping) {\n\t\tVM_BUG_ON_PAGE(PageAnon(page), page);\n\t\tif (page_has_private(page)) {\n\t\t\ttry_to_free_buffers(page);\n\t\t\tgoto out_unlock_both;\n\t\t}\n\t} else if (page_mapped(page)) {\n\t\t/* Establish migration ptes */\n\t\tVM_BUG_ON_PAGE(PageAnon(page) && !PageKsm(page) && !anon_vma,\n\t\t\t\tpage);\n\t\ttry_to_unmap(page,\n\t\t\tTTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);\n\t\tpage_was_mapped = 1;\n\t}\n\n\tif (!page_mapped(page))\n\t\trc = move_to_new_page(newpage, page, mode);\n\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(page,\n\t\t\trc == MIGRATEPAGE_SUCCESS ? newpage : page);\n\nout_unlock_both:\n\tunlock_page(newpage);\nout_unlock:\n\t/* Drop an anon_vma reference if we took one */\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\tunlock_page(page);\nout:\n\treturn rc;\n}\n\n/*\n * gcc 4.7 and 4.8 on arm get an ICEs when inlining unmap_and_move().  Work\n * around it.\n */\n#if (GCC_VERSION >= 40700 && GCC_VERSION < 40900) && defined(CONFIG_ARM)\n#define ICE_noinline noinline\n#else\n#define ICE_noinline\n#endif\n\n/*\n * Obtain the lock on page, remove all ptes and migrate the page\n * to the newly allocated page in newpage.\n */\nstatic ICE_noinline int unmap_and_move(new_page_t get_new_page,\n\t\t\t\t   free_page_t put_new_page,\n\t\t\t\t   unsigned long private, struct page *page,\n\t\t\t\t   int force, enum migrate_mode mode,\n\t\t\t\t   enum migrate_reason reason)\n{\n\tint rc = MIGRATEPAGE_SUCCESS;\n\tint *result = NULL;\n\tstruct page *newpage;\n\n\tnewpage = get_new_page(page, private, &result);\n\tif (!newpage)\n\t\treturn -ENOMEM;\n\n\tif (page_count(page) == 1) {\n\t\t/* page was freed from under us. So we are done. */\n\t\tgoto out;\n\t}\n\n\tif (unlikely(PageTransHuge(page)))\n\t\tif (unlikely(split_huge_page(page)))\n\t\t\tgoto out;\n\n\trc = __unmap_and_move(page, newpage, force, mode);\n\tif (rc == MIGRATEPAGE_SUCCESS)\n\t\tput_new_page = NULL;\n\nout:\n\tif (rc != -EAGAIN) {\n\t\t/*\n\t\t * A page that has been migrated has all references\n\t\t * removed and will be freed. A page that has not been\n\t\t * migrated will have kepts its references and be\n\t\t * restored.\n\t\t */\n\t\tlist_del(&page->lru);\n\t\tdec_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\tpage_is_file_cache(page));\n\t\t/* Soft-offlined page shouldn't go through lru cache list */\n\t\tif (reason == MR_MEMORY_FAILURE) {\n\t\t\tput_page(page);\n\t\t\tif (!test_set_page_hwpoison(page))\n\t\t\t\tnum_poisoned_pages_inc();\n\t\t} else\n\t\t\tputback_lru_page(page);\n\t}\n\n\t/*\n\t * If migration was not successful and there's a freeing callback, use\n\t * it.  Otherwise, putback_lru_page() will drop the reference grabbed\n\t * during isolation.\n\t */\n\tif (put_new_page)\n\t\tput_new_page(newpage, private);\n\telse if (unlikely(__is_movable_balloon_page(newpage))) {\n\t\t/* drop our reference, page already in the balloon */\n\t\tput_page(newpage);\n\t} else\n\t\tputback_lru_page(newpage);\n\n\tif (result) {\n\t\tif (rc)\n\t\t\t*result = rc;\n\t\telse\n\t\t\t*result = page_to_nid(newpage);\n\t}\n\treturn rc;\n}\n\n/*\n * Counterpart of unmap_and_move_page() for hugepage migration.\n *\n * This function doesn't wait the completion of hugepage I/O\n * because there is no race between I/O and migration for hugepage.\n * Note that currently hugepage I/O occurs only in direct I/O\n * where no lock is held and PG_writeback is irrelevant,\n * and writeback status of all subpages are counted in the reference\n * count of the head page (i.e. if all subpages of a 2MB hugepage are\n * under direct I/O, the reference of the head page is 512 and a bit more.)\n * This means that when we try to migrate hugepage whose subpages are\n * doing direct I/O, some references remain after try_to_unmap() and\n * hugepage migration fails without data corruption.\n *\n * There is also no race when direct I/O is issued on the page under migration,\n * because then pte is replaced with migration swap entry and direct I/O code\n * will wait in the page fault for migration to complete.\n */\nstatic int unmap_and_move_huge_page(new_page_t get_new_page,\n\t\t\t\tfree_page_t put_new_page, unsigned long private,\n\t\t\t\tstruct page *hpage, int force,\n\t\t\t\tenum migrate_mode mode)\n{\n\tint rc = -EAGAIN;\n\tint *result = NULL;\n\tint page_was_mapped = 0;\n\tstruct page *new_hpage;\n\tstruct anon_vma *anon_vma = NULL;\n\n\t/*\n\t * Movability of hugepages depends on architectures and hugepage size.\n\t * This check is necessary because some callers of hugepage migration\n\t * like soft offline and memory hotremove don't walk through page\n\t * tables or check whether the hugepage is pmd-based or not before\n\t * kicking migration.\n\t */\n\tif (!hugepage_migration_supported(page_hstate(hpage))) {\n\t\tputback_active_hugepage(hpage);\n\t\treturn -ENOSYS;\n\t}\n\n\tnew_hpage = get_new_page(hpage, private, &result);\n\tif (!new_hpage)\n\t\treturn -ENOMEM;\n\n\tif (!trylock_page(hpage)) {\n\t\tif (!force || mode != MIGRATE_SYNC)\n\t\t\tgoto out;\n\t\tlock_page(hpage);\n\t}\n\n\tif (PageAnon(hpage))\n\t\tanon_vma = page_get_anon_vma(hpage);\n\n\tif (unlikely(!trylock_page(new_hpage)))\n\t\tgoto put_anon;\n\n\tif (page_mapped(hpage)) {\n\t\ttry_to_unmap(hpage,\n\t\t\tTTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);\n\t\tpage_was_mapped = 1;\n\t}\n\n\tif (!page_mapped(hpage))\n\t\trc = move_to_new_page(new_hpage, hpage, mode);\n\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(hpage,\n\t\t\trc == MIGRATEPAGE_SUCCESS ? new_hpage : hpage);\n\n\tunlock_page(new_hpage);\n\nput_anon:\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\n\tif (rc == MIGRATEPAGE_SUCCESS) {\n\t\thugetlb_cgroup_migrate(hpage, new_hpage);\n\t\tput_new_page = NULL;\n\t}\n\n\tunlock_page(hpage);\nout:\n\tif (rc != -EAGAIN)\n\t\tputback_active_hugepage(hpage);\n\n\t/*\n\t * If migration was not successful and there's a freeing callback, use\n\t * it.  Otherwise, put_page() will drop the reference grabbed during\n\t * isolation.\n\t */\n\tif (put_new_page)\n\t\tput_new_page(new_hpage, private);\n\telse\n\t\tputback_active_hugepage(new_hpage);\n\n\tif (result) {\n\t\tif (rc)\n\t\t\t*result = rc;\n\t\telse\n\t\t\t*result = page_to_nid(new_hpage);\n\t}\n\treturn rc;\n}\n\n/*\n * migrate_pages - migrate the pages specified in a list, to the free pages\n *\t\t   supplied as the target for the page migration\n *\n * @from:\t\tThe list of pages to be migrated.\n * @get_new_page:\tThe function used to allocate free pages to be used\n *\t\t\tas the target of the page migration.\n * @put_new_page:\tThe function used to free target pages if migration\n *\t\t\tfails, or NULL if no special handling is necessary.\n * @private:\t\tPrivate data to be passed on to get_new_page()\n * @mode:\t\tThe migration mode that specifies the constraints for\n *\t\t\tpage migration, if any.\n * @reason:\t\tThe reason for page migration.\n *\n * The function returns after 10 attempts or if no pages are movable any more\n * because the list has become empty or no retryable pages exist any more.\n * The caller should call putback_movable_pages() to return pages to the LRU\n * or free list only if ret != 0.\n *\n * Returns the number of pages that were not migrated, or an error code.\n */\nint migrate_pages(struct list_head *from, new_page_t get_new_page,\n\t\tfree_page_t put_new_page, unsigned long private,\n\t\tenum migrate_mode mode, int reason)\n{\n\tint retry = 1;\n\tint nr_failed = 0;\n\tint nr_succeeded = 0;\n\tint pass = 0;\n\tstruct page *page;\n\tstruct page *page2;\n\tint swapwrite = current->flags & PF_SWAPWRITE;\n\tint rc;\n\n\tif (!swapwrite)\n\t\tcurrent->flags |= PF_SWAPWRITE;\n\n\tfor(pass = 0; pass < 10 && retry; pass++) {\n\t\tretry = 0;\n\n\t\tlist_for_each_entry_safe(page, page2, from, lru) {\n\t\t\tcond_resched();\n\n\t\t\tif (PageHuge(page))\n\t\t\t\trc = unmap_and_move_huge_page(get_new_page,\n\t\t\t\t\t\tput_new_page, private, page,\n\t\t\t\t\t\tpass > 2, mode);\n\t\t\telse\n\t\t\t\trc = unmap_and_move(get_new_page, put_new_page,\n\t\t\t\t\t\tprivate, page, pass > 2, mode,\n\t\t\t\t\t\treason);\n\n\t\t\tswitch(rc) {\n\t\t\tcase -ENOMEM:\n\t\t\t\tgoto out;\n\t\t\tcase -EAGAIN:\n\t\t\t\tretry++;\n\t\t\t\tbreak;\n\t\t\tcase MIGRATEPAGE_SUCCESS:\n\t\t\t\tnr_succeeded++;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/*\n\t\t\t\t * Permanent failure (-EBUSY, -ENOSYS, etc.):\n\t\t\t\t * unlike -EAGAIN case, the failed page is\n\t\t\t\t * removed from migration page list and not\n\t\t\t\t * retried in the next outer loop.\n\t\t\t\t */\n\t\t\t\tnr_failed++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tnr_failed += retry;\n\trc = nr_failed;\nout:\n\tif (nr_succeeded)\n\t\tcount_vm_events(PGMIGRATE_SUCCESS, nr_succeeded);\n\tif (nr_failed)\n\t\tcount_vm_events(PGMIGRATE_FAIL, nr_failed);\n\ttrace_mm_migrate_pages(nr_succeeded, nr_failed, mode, reason);\n\n\tif (!swapwrite)\n\t\tcurrent->flags &= ~PF_SWAPWRITE;\n\n\treturn rc;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * Move a list of individual pages\n */\nstruct page_to_node {\n\tunsigned long addr;\n\tstruct page *page;\n\tint node;\n\tint status;\n};\n\nstatic struct page *new_page_node(struct page *p, unsigned long private,\n\t\tint **result)\n{\n\tstruct page_to_node *pm = (struct page_to_node *)private;\n\n\twhile (pm->node != MAX_NUMNODES && pm->page != p)\n\t\tpm++;\n\n\tif (pm->node == MAX_NUMNODES)\n\t\treturn NULL;\n\n\t*result = &pm->status;\n\n\tif (PageHuge(p))\n\t\treturn alloc_huge_page_node(page_hstate(compound_head(p)),\n\t\t\t\t\tpm->node);\n\telse\n\t\treturn __alloc_pages_node(pm->node,\n\t\t\t\tGFP_HIGHUSER_MOVABLE | __GFP_THISNODE, 0);\n}\n\n/*\n * Move a set of pages as indicated in the pm array. The addr\n * field must be set to the virtual address of the page to be moved\n * and the node number must contain a valid target node.\n * The pm array ends with node = MAX_NUMNODES.\n */\nstatic int do_move_page_to_node_array(struct mm_struct *mm,\n\t\t\t\t      struct page_to_node *pm,\n\t\t\t\t      int migrate_all)\n{\n\tint err;\n\tstruct page_to_node *pp;\n\tLIST_HEAD(pagelist);\n\n\tdown_read(&mm->mmap_sem);\n\n\t/*\n\t * Build a list of pages to migrate\n\t */\n\tfor (pp = pm; pp->node != MAX_NUMNODES; pp++) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct page *page;\n\n\t\terr = -EFAULT;\n\t\tvma = find_vma(mm, pp->addr);\n\t\tif (!vma || pp->addr < vma->vm_start || !vma_migratable(vma))\n\t\t\tgoto set_status;\n\n\t\t/* FOLL_DUMP to ignore special (like zero) pages */\n\t\tpage = follow_page(vma, pp->addr,\n\t\t\t\tFOLL_GET | FOLL_SPLIT | FOLL_DUMP);\n\n\t\terr = PTR_ERR(page);\n\t\tif (IS_ERR(page))\n\t\t\tgoto set_status;\n\n\t\terr = -ENOENT;\n\t\tif (!page)\n\t\t\tgoto set_status;\n\n\t\tpp->page = page;\n\t\terr = page_to_nid(page);\n\n\t\tif (err == pp->node)\n\t\t\t/*\n\t\t\t * Node already in the right place\n\t\t\t */\n\t\t\tgoto put_and_set;\n\n\t\terr = -EACCES;\n\t\tif (page_mapcount(page) > 1 &&\n\t\t\t\t!migrate_all)\n\t\t\tgoto put_and_set;\n\n\t\tif (PageHuge(page)) {\n\t\t\tif (PageHead(page))\n\t\t\t\tisolate_huge_page(page, &pagelist);\n\t\t\tgoto put_and_set;\n\t\t}\n\n\t\terr = isolate_lru_page(page);\n\t\tif (!err) {\n\t\t\tlist_add_tail(&page->lru, &pagelist);\n\t\t\tinc_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\t\t    page_is_file_cache(page));\n\t\t}\nput_and_set:\n\t\t/*\n\t\t * Either remove the duplicate refcount from\n\t\t * isolate_lru_page() or drop the page ref if it was\n\t\t * not isolated.\n\t\t */\n\t\tput_page(page);\nset_status:\n\t\tpp->status = err;\n\t}\n\n\terr = 0;\n\tif (!list_empty(&pagelist)) {\n\t\terr = migrate_pages(&pagelist, new_page_node, NULL,\n\t\t\t\t(unsigned long)pm, MIGRATE_SYNC, MR_SYSCALL);\n\t\tif (err)\n\t\t\tputback_movable_pages(&pagelist);\n\t}\n\n\tup_read(&mm->mmap_sem);\n\treturn err;\n}\n\n/*\n * Migrate an array of page address onto an array of nodes and fill\n * the corresponding array of status.\n */\nstatic int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n\t\t\t unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t const int __user *nodes,\n\t\t\t int __user *status, int flags)\n{\n\tstruct page_to_node *pm;\n\tunsigned long chunk_nr_pages;\n\tunsigned long chunk_start;\n\tint err;\n\n\terr = -ENOMEM;\n\tpm = (struct page_to_node *)__get_free_page(GFP_KERNEL);\n\tif (!pm)\n\t\tgoto out;\n\n\tmigrate_prep();\n\n\t/*\n\t * Store a chunk of page_to_node array in a page,\n\t * but keep the last one as a marker\n\t */\n\tchunk_nr_pages = (PAGE_SIZE / sizeof(struct page_to_node)) - 1;\n\n\tfor (chunk_start = 0;\n\t     chunk_start < nr_pages;\n\t     chunk_start += chunk_nr_pages) {\n\t\tint j;\n\n\t\tif (chunk_start + chunk_nr_pages > nr_pages)\n\t\t\tchunk_nr_pages = nr_pages - chunk_start;\n\n\t\t/* fill the chunk pm with addrs and nodes from user-space */\n\t\tfor (j = 0; j < chunk_nr_pages; j++) {\n\t\t\tconst void __user *p;\n\t\t\tint node;\n\n\t\t\terr = -EFAULT;\n\t\t\tif (get_user(p, pages + j + chunk_start))\n\t\t\t\tgoto out_pm;\n\t\t\tpm[j].addr = (unsigned long) p;\n\n\t\t\tif (get_user(node, nodes + j + chunk_start))\n\t\t\t\tgoto out_pm;\n\n\t\t\terr = -ENODEV;\n\t\t\tif (node < 0 || node >= MAX_NUMNODES)\n\t\t\t\tgoto out_pm;\n\n\t\t\tif (!node_state(node, N_MEMORY))\n\t\t\t\tgoto out_pm;\n\n\t\t\terr = -EACCES;\n\t\t\tif (!node_isset(node, task_nodes))\n\t\t\t\tgoto out_pm;\n\n\t\t\tpm[j].node = node;\n\t\t}\n\n\t\t/* End marker for this chunk */\n\t\tpm[chunk_nr_pages].node = MAX_NUMNODES;\n\n\t\t/* Migrate this chunk */\n\t\terr = do_move_page_to_node_array(mm, pm,\n\t\t\t\t\t\t flags & MPOL_MF_MOVE_ALL);\n\t\tif (err < 0)\n\t\t\tgoto out_pm;\n\n\t\t/* Return status information */\n\t\tfor (j = 0; j < chunk_nr_pages; j++)\n\t\t\tif (put_user(pm[j].status, status + j + chunk_start)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto out_pm;\n\t\t\t}\n\t}\n\terr = 0;\n\nout_pm:\n\tfree_page((unsigned long)pm);\nout:\n\treturn err;\n}\n\n/*\n * Determine the nodes of an array of pages and store it in an array of status.\n */\nstatic void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t\tconst void __user **pages, int *status)\n{\n\tunsigned long i;\n\n\tdown_read(&mm->mmap_sem);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long addr = (unsigned long)(*pages);\n\t\tstruct vm_area_struct *vma;\n\t\tstruct page *page;\n\t\tint err = -EFAULT;\n\n\t\tvma = find_vma(mm, addr);\n\t\tif (!vma || addr < vma->vm_start)\n\t\t\tgoto set_status;\n\n\t\t/* FOLL_DUMP to ignore special (like zero) pages */\n\t\tpage = follow_page(vma, addr, FOLL_DUMP);\n\n\t\terr = PTR_ERR(page);\n\t\tif (IS_ERR(page))\n\t\t\tgoto set_status;\n\n\t\terr = page ? page_to_nid(page) : -ENOENT;\nset_status:\n\t\t*status = err;\n\n\t\tpages++;\n\t\tstatus++;\n\t}\n\n\tup_read(&mm->mmap_sem);\n}\n\n/*\n * Determine the nodes of a user array of pages and store it in\n * a user array of status.\n */\nstatic int do_pages_stat(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t int __user *status)\n{\n#define DO_PAGES_STAT_CHUNK_NR 16\n\tconst void __user *chunk_pages[DO_PAGES_STAT_CHUNK_NR];\n\tint chunk_status[DO_PAGES_STAT_CHUNK_NR];\n\n\twhile (nr_pages) {\n\t\tunsigned long chunk_nr;\n\n\t\tchunk_nr = nr_pages;\n\t\tif (chunk_nr > DO_PAGES_STAT_CHUNK_NR)\n\t\t\tchunk_nr = DO_PAGES_STAT_CHUNK_NR;\n\n\t\tif (copy_from_user(chunk_pages, pages, chunk_nr * sizeof(*chunk_pages)))\n\t\t\tbreak;\n\n\t\tdo_pages_stat_array(mm, chunk_nr, chunk_pages, chunk_status);\n\n\t\tif (copy_to_user(status, chunk_status, chunk_nr * sizeof(*status)))\n\t\t\tbreak;\n\n\t\tpages += chunk_nr;\n\t\tstatus += chunk_nr;\n\t\tnr_pages -= chunk_nr;\n\t}\n\treturn nr_pages ? -EFAULT : 0;\n}\n\n/*\n * Move a list of pages in the address space of the currently executing\n * process.\n */\nSYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,\n\t\tconst void __user * __user *, pages,\n\t\tconst int __user *, nodes,\n\t\tint __user *, status, int, flags)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct task_struct *task;\n\tstruct mm_struct *mm;\n\tint err;\n\tnodemask_t task_nodes;\n\n\t/* Check flags */\n\tif (flags & ~(MPOL_MF_MOVE|MPOL_MF_MOVE_ALL))\n\t\treturn -EINVAL;\n\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\t/* Find the mm_struct */\n\trcu_read_lock();\n\ttask = pid ? find_task_by_vpid(pid) : current;\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn -ESRCH;\n\t}\n\tget_task_struct(task);\n\n\t/*\n\t * Check if this process has the right to modify the specified\n\t * process. The right exists if the process has administrative\n\t * capabilities, superuser privileges or the same\n\t * userid as the target process.\n\t */\n\ttcred = __task_cred(task);\n\tif (!uid_eq(cred->euid, tcred->suid) && !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->uid,  tcred->suid) && !uid_eq(cred->uid,  tcred->uid) &&\n\t    !capable(CAP_SYS_NICE)) {\n\t\trcu_read_unlock();\n\t\terr = -EPERM;\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n \terr = security_task_movememory(task);\n \tif (err)\n\t\tgoto out;\n\n\ttask_nodes = cpuset_mems_allowed(task);\n\tmm = get_task_mm(task);\n\tput_task_struct(task);\n\n\tif (!mm)\n\t\treturn -EINVAL;\n\n\tif (nodes)\n\t\terr = do_pages_move(mm, task_nodes, nr_pages, pages,\n\t\t\t\t    nodes, status, flags);\n\telse\n\t\terr = do_pages_stat(mm, nr_pages, pages, status);\n\n\tmmput(mm);\n\treturn err;\n\nout:\n\tput_task_struct(task);\n\treturn err;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Returns true if this is a safe migration target node for misplaced NUMA\n * pages. Currently it only checks the watermarks which crude\n */\nstatic bool migrate_balanced_pgdat(struct pglist_data *pgdat,\n\t\t\t\t   unsigned long nr_migrate_pages)\n{\n\tint z;\n\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n\t\tstruct zone *zone = pgdat->node_zones + z;\n\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (!zone_reclaimable(zone))\n\t\t\tcontinue;\n\n\t\t/* Avoid waking kswapd by allocating pages_to_migrate pages. */\n\t\tif (!zone_watermark_ok(zone, 0,\n\t\t\t\t       high_wmark_pages(zone) +\n\t\t\t\t       nr_migrate_pages,\n\t\t\t\t       0, 0))\n\t\t\tcontinue;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic struct page *alloc_misplaced_dst_page(struct page *page,\n\t\t\t\t\t   unsigned long data,\n\t\t\t\t\t   int **result)\n{\n\tint nid = (int) data;\n\tstruct page *newpage;\n\n\tnewpage = __alloc_pages_node(nid,\n\t\t\t\t\t (GFP_HIGHUSER_MOVABLE |\n\t\t\t\t\t  __GFP_THISNODE | __GFP_NOMEMALLOC |\n\t\t\t\t\t  __GFP_NORETRY | __GFP_NOWARN) &\n\t\t\t\t\t ~GFP_IOFS, 0);\n\n\treturn newpage;\n}\n\n/*\n * page migration rate limiting control.\n * Do not migrate more than @pages_to_migrate in a @migrate_interval_millisecs\n * window of time. Default here says do not migrate more than 1280M per second.\n */\nstatic unsigned int migrate_interval_millisecs __read_mostly = 100;\nstatic unsigned int ratelimit_pages __read_mostly = 128 << (20 - PAGE_SHIFT);\n\n/* Returns true if the node is migrate rate-limited after the update */\nstatic bool numamigrate_update_ratelimit(pg_data_t *pgdat,\n\t\t\t\t\tunsigned long nr_pages)\n{\n\t/*\n\t * Rate-limit the amount of data that is being migrated to a node.\n\t * Optimal placement is no good if the memory bus is saturated and\n\t * all the time is being spent migrating!\n\t */\n\tif (time_after(jiffies, pgdat->numabalancing_migrate_next_window)) {\n\t\tspin_lock(&pgdat->numabalancing_migrate_lock);\n\t\tpgdat->numabalancing_migrate_nr_pages = 0;\n\t\tpgdat->numabalancing_migrate_next_window = jiffies +\n\t\t\tmsecs_to_jiffies(migrate_interval_millisecs);\n\t\tspin_unlock(&pgdat->numabalancing_migrate_lock);\n\t}\n\tif (pgdat->numabalancing_migrate_nr_pages > ratelimit_pages) {\n\t\ttrace_mm_numa_migrate_ratelimit(current, pgdat->node_id,\n\t\t\t\t\t\t\t\tnr_pages);\n\t\treturn true;\n\t}\n\n\t/*\n\t * This is an unlocked non-atomic update so errors are possible.\n\t * The consequences are failing to migrate when we potentiall should\n\t * have which is not severe enough to warrant locking. If it is ever\n\t * a problem, it can be converted to a per-cpu counter.\n\t */\n\tpgdat->numabalancing_migrate_nr_pages += nr_pages;\n\treturn false;\n}\n\nstatic int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)\n{\n\tint page_lru;\n\n\tVM_BUG_ON_PAGE(compound_order(page) && !PageTransHuge(page), page);\n\n\t/* Avoid migrating to a node that is nearly full */\n\tif (!migrate_balanced_pgdat(pgdat, 1UL << compound_order(page)))\n\t\treturn 0;\n\n\tif (isolate_lru_page(page))\n\t\treturn 0;\n\n\t/*\n\t * migrate_misplaced_transhuge_page() skips page migration's usual\n\t * check on page_count(), so we must do it here, now that the page\n\t * has been isolated: a GUP pin, or any other pin, prevents migration.\n\t * The expected page count is 3: 1 for page's mapcount and 1 for the\n\t * caller's pin and 1 for the reference taken by isolate_lru_page().\n\t */\n\tif (PageTransHuge(page) && page_count(page) != 3) {\n\t\tputback_lru_page(page);\n\t\treturn 0;\n\t}\n\n\tpage_lru = page_is_file_cache(page);\n\tmod_zone_page_state(page_zone(page), NR_ISOLATED_ANON + page_lru,\n\t\t\t\thpage_nr_pages(page));\n\n\t/*\n\t * Isolating the page has taken another reference, so the\n\t * caller's reference can be safely dropped without the page\n\t * disappearing underneath us during migration.\n\t */\n\tput_page(page);\n\treturn 1;\n}\n\nbool pmd_trans_migrating(pmd_t pmd)\n{\n\tstruct page *page = pmd_page(pmd);\n\treturn PageLocked(page);\n}\n\n/*\n * Attempt to migrate a misplaced page to the specified destination\n * node. Caller is expected to have an elevated reference count on\n * the page that will be dropped by this function before returning.\n */\nint migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,\n\t\t\t   int node)\n{\n\tpg_data_t *pgdat = NODE_DATA(node);\n\tint isolated;\n\tint nr_remaining;\n\tLIST_HEAD(migratepages);\n\n\t/*\n\t * Don't migrate file pages that are mapped in multiple processes\n\t * with execute permissions as they are probably shared libraries.\n\t */\n\tif (page_mapcount(page) != 1 && page_is_file_cache(page) &&\n\t    (vma->vm_flags & VM_EXEC))\n\t\tgoto out;\n\n\t/*\n\t * Rate-limit the amount of data that is being migrated to a node.\n\t * Optimal placement is no good if the memory bus is saturated and\n\t * all the time is being spent migrating!\n\t */\n\tif (numamigrate_update_ratelimit(pgdat, 1))\n\t\tgoto out;\n\n\tisolated = numamigrate_isolate_page(pgdat, page);\n\tif (!isolated)\n\t\tgoto out;\n\n\tlist_add(&page->lru, &migratepages);\n\tnr_remaining = migrate_pages(&migratepages, alloc_misplaced_dst_page,\n\t\t\t\t     NULL, node, MIGRATE_ASYNC,\n\t\t\t\t     MR_NUMA_MISPLACED);\n\tif (nr_remaining) {\n\t\tif (!list_empty(&migratepages)) {\n\t\t\tlist_del(&page->lru);\n\t\t\tdec_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\t\tpage_is_file_cache(page));\n\t\t\tputback_lru_page(page);\n\t\t}\n\t\tisolated = 0;\n\t} else\n\t\tcount_vm_numa_event(NUMA_PAGE_MIGRATE);\n\tBUG_ON(!list_empty(&migratepages));\n\treturn isolated;\n\nout:\n\tput_page(page);\n\treturn 0;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n#if defined(CONFIG_NUMA_BALANCING) && defined(CONFIG_TRANSPARENT_HUGEPAGE)\n/*\n * Migrates a THP to a given target node. page must be locked and is unlocked\n * before returning.\n */\nint migrate_misplaced_transhuge_page(struct mm_struct *mm,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tpmd_t *pmd, pmd_t entry,\n\t\t\t\tunsigned long address,\n\t\t\t\tstruct page *page, int node)\n{\n\tspinlock_t *ptl;\n\tpg_data_t *pgdat = NODE_DATA(node);\n\tint isolated = 0;\n\tstruct page *new_page = NULL;\n\tint page_lru = page_is_file_cache(page);\n\tunsigned long mmun_start = address & HPAGE_PMD_MASK;\n\tunsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;\n\tpmd_t orig_entry;\n\n\t/*\n\t * Rate-limit the amount of data that is being migrated to a node.\n\t * Optimal placement is no good if the memory bus is saturated and\n\t * all the time is being spent migrating!\n\t */\n\tif (numamigrate_update_ratelimit(pgdat, HPAGE_PMD_NR))\n\t\tgoto out_dropref;\n\n\tnew_page = alloc_pages_node(node,\n\t\t(GFP_TRANSHUGE | __GFP_THISNODE) & ~__GFP_WAIT,\n\t\tHPAGE_PMD_ORDER);\n\tif (!new_page)\n\t\tgoto out_fail;\n\n\tisolated = numamigrate_isolate_page(pgdat, page);\n\tif (!isolated) {\n\t\tput_page(new_page);\n\t\tgoto out_fail;\n\t}\n\n\tif (mm_tlb_flush_pending(mm))\n\t\tflush_tlb_range(vma, mmun_start, mmun_end);\n\n\t/* Prepare a page as a migration target */\n\t__set_page_locked(new_page);\n\tSetPageSwapBacked(new_page);\n\n\t/* anon mapping, we can simply copy page->mapping to the new page: */\n\tnew_page->mapping = page->mapping;\n\tnew_page->index = page->index;\n\tmigrate_page_copy(new_page, page);\n\tWARN_ON(PageLRU(new_page));\n\n\t/* Recheck the target PMD */\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {\nfail_putback:\n\t\tspin_unlock(ptl);\n\t\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\n\t\t/* Reverse changes made by migrate_page_copy() */\n\t\tif (TestClearPageActive(new_page))\n\t\t\tSetPageActive(page);\n\t\tif (TestClearPageUnevictable(new_page))\n\t\t\tSetPageUnevictable(page);\n\n\t\tunlock_page(new_page);\n\t\tput_page(new_page);\t\t/* Free it */\n\n\t\t/* Retake the callers reference and putback on LRU */\n\t\tget_page(page);\n\t\tputback_lru_page(page);\n\t\tmod_zone_page_state(page_zone(page),\n\t\t\t NR_ISOLATED_ANON + page_lru, -HPAGE_PMD_NR);\n\n\t\tgoto out_unlock;\n\t}\n\n\torig_entry = *pmd;\n\tentry = mk_pmd(new_page, vma->vm_page_prot);\n\tentry = pmd_mkhuge(entry);\n\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\n\t/*\n\t * Clear the old entry under pagetable lock and establish the new PTE.\n\t * Any parallel GUP will either observe the old page blocking on the\n\t * page lock, block on the page table lock or observe the new page.\n\t * The SetPageUptodate on the new page and page_add_new_anon_rmap\n\t * guarantee the copy is visible before the pagetable update.\n\t */\n\tflush_cache_range(vma, mmun_start, mmun_end);\n\tpage_add_anon_rmap(new_page, vma, mmun_start);\n\tpmdp_huge_clear_flush_notify(vma, mmun_start, pmd);\n\tset_pmd_at(mm, mmun_start, pmd, entry);\n\tflush_tlb_range(vma, mmun_start, mmun_end);\n\tupdate_mmu_cache_pmd(vma, address, &entry);\n\n\tif (page_count(page) != 2) {\n\t\tset_pmd_at(mm, mmun_start, pmd, orig_entry);\n\t\tflush_tlb_range(vma, mmun_start, mmun_end);\n\t\tmmu_notifier_invalidate_range(mm, mmun_start, mmun_end);\n\t\tupdate_mmu_cache_pmd(vma, address, &entry);\n\t\tpage_remove_rmap(new_page);\n\t\tgoto fail_putback;\n\t}\n\n\tmlock_migrate_page(new_page, page);\n\tset_page_memcg(new_page, page_memcg(page));\n\tset_page_memcg(page, NULL);\n\tpage_remove_rmap(page);\n\n\tspin_unlock(ptl);\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\n\t/* Take an \"isolate\" reference and put new page on the LRU. */\n\tget_page(new_page);\n\tputback_lru_page(new_page);\n\n\tunlock_page(new_page);\n\tunlock_page(page);\n\tput_page(page);\t\t\t/* Drop the rmap reference */\n\tput_page(page);\t\t\t/* Drop the LRU isolation reference */\n\n\tcount_vm_events(PGMIGRATE_SUCCESS, HPAGE_PMD_NR);\n\tcount_vm_numa_events(NUMA_PAGE_MIGRATE, HPAGE_PMD_NR);\n\n\tmod_zone_page_state(page_zone(page),\n\t\t\tNR_ISOLATED_ANON + page_lru,\n\t\t\t-HPAGE_PMD_NR);\n\treturn isolated;\n\nout_fail:\n\tcount_vm_events(PGMIGRATE_FAIL, HPAGE_PMD_NR);\nout_dropref:\n\tptl = pmd_lock(mm, pmd);\n\tif (pmd_same(*pmd, entry)) {\n\t\tentry = pmd_modify(entry, vma->vm_page_prot);\n\t\tset_pmd_at(mm, mmun_start, pmd, entry);\n\t\tupdate_mmu_cache_pmd(vma, address, &entry);\n\t}\n\tspin_unlock(ptl);\n\nout_unlock:\n\tunlock_page(page);\n\tput_page(page);\n\treturn 0;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n#endif /* CONFIG_NUMA */\n"], "fixing_code": ["/*\n * Memory Migration functionality - linux/mm/migrate.c\n *\n * Copyright (C) 2006 Silicon Graphics, Inc., Christoph Lameter\n *\n * Page migration was first developed in the context of the memory hotplug\n * project. The main authors of the migration code are:\n *\n * IWAMOTO Toshihiro <iwamoto@valinux.co.jp>\n * Hirokazu Takahashi <taka@valinux.co.jp>\n * Dave Hansen <haveblue@us.ibm.com>\n * Christoph Lameter\n */\n\n#include <linux/migrate.h>\n#include <linux/export.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/pagemap.h>\n#include <linux/buffer_head.h>\n#include <linux/mm_inline.h>\n#include <linux/nsproxy.h>\n#include <linux/pagevec.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/topology.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/writeback.h>\n#include <linux/mempolicy.h>\n#include <linux/vmalloc.h>\n#include <linux/security.h>\n#include <linux/backing-dev.h>\n#include <linux/syscalls.h>\n#include <linux/hugetlb.h>\n#include <linux/hugetlb_cgroup.h>\n#include <linux/gfp.h>\n#include <linux/balloon_compaction.h>\n#include <linux/mmu_notifier.h>\n#include <linux/page_idle.h>\n\n#include <asm/tlbflush.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/migrate.h>\n\n#include \"internal.h\"\n\n/*\n * migrate_prep() needs to be called before we start compiling a list of pages\n * to be migrated using isolate_lru_page(). If scheduling work on other CPUs is\n * undesirable, use migrate_prep_local()\n */\nint migrate_prep(void)\n{\n\t/*\n\t * Clear the LRU lists so pages can be isolated.\n\t * Note that pages may be moved off the LRU after we have\n\t * drained them. Those pages will fail to migrate like other\n\t * pages that may be busy.\n\t */\n\tlru_add_drain_all();\n\n\treturn 0;\n}\n\n/* Do the necessary work of migrate_prep but not if it involves other CPUs */\nint migrate_prep_local(void)\n{\n\tlru_add_drain();\n\n\treturn 0;\n}\n\n/*\n * Put previously isolated pages back onto the appropriate lists\n * from where they were once taken off for compaction/migration.\n *\n * This function shall be used whenever the isolated pageset has been\n * built from lru, balloon, hugetlbfs page. See isolate_migratepages_range()\n * and isolate_huge_page().\n */\nvoid putback_movable_pages(struct list_head *l)\n{\n\tstruct page *page;\n\tstruct page *page2;\n\n\tlist_for_each_entry_safe(page, page2, l, lru) {\n\t\tif (unlikely(PageHuge(page))) {\n\t\t\tputback_active_hugepage(page);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_del(&page->lru);\n\t\tdec_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\tpage_is_file_cache(page));\n\t\tif (unlikely(isolated_balloon_page(page)))\n\t\t\tballoon_page_putback(page);\n\t\telse\n\t\t\tputback_lru_page(page);\n\t}\n}\n\n/*\n * Restore a potential migration pte to a working pte entry\n */\nstatic int remove_migration_pte(struct page *new, struct vm_area_struct *vma,\n\t\t\t\t unsigned long addr, void *old)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tswp_entry_t entry;\n \tpmd_t *pmd;\n\tpte_t *ptep, pte;\n \tspinlock_t *ptl;\n\n\tif (unlikely(PageHuge(new))) {\n\t\tptep = huge_pte_offset(mm, addr);\n\t\tif (!ptep)\n\t\t\tgoto out;\n\t\tptl = huge_pte_lockptr(hstate_vma(vma), mm, ptep);\n\t} else {\n\t\tpmd = mm_find_pmd(mm, addr);\n\t\tif (!pmd)\n\t\t\tgoto out;\n\n\t\tptep = pte_offset_map(pmd, addr);\n\n\t\t/*\n\t\t * Peek to check is_swap_pte() before taking ptlock?  No, we\n\t\t * can race mremap's move_ptes(), which skips anon_vma lock.\n\t\t */\n\n\t\tptl = pte_lockptr(mm, pmd);\n\t}\n\n \tspin_lock(ptl);\n\tpte = *ptep;\n\tif (!is_swap_pte(pte))\n\t\tgoto unlock;\n\n\tentry = pte_to_swp_entry(pte);\n\n\tif (!is_migration_entry(entry) ||\n\t    migration_entry_to_page(entry) != old)\n\t\tgoto unlock;\n\n\tget_page(new);\n\tpte = pte_mkold(mk_pte(new, vma->vm_page_prot));\n\tif (pte_swp_soft_dirty(*ptep))\n\t\tpte = pte_mksoft_dirty(pte);\n\n\t/* Recheck VMA as permissions can change since migration started  */\n\tif (is_write_migration_entry(entry))\n\t\tpte = maybe_mkwrite(pte, vma);\n\n#ifdef CONFIG_HUGETLB_PAGE\n\tif (PageHuge(new)) {\n\t\tpte = pte_mkhuge(pte);\n\t\tpte = arch_make_huge_pte(pte, vma, new, 0);\n\t}\n#endif\n\tflush_dcache_page(new);\n\tset_pte_at(mm, addr, ptep, pte);\n\n\tif (PageHuge(new)) {\n\t\tif (PageAnon(new))\n\t\t\thugepage_add_anon_rmap(new, vma, addr);\n\t\telse\n\t\t\tpage_dup_rmap(new);\n\t} else if (PageAnon(new))\n\t\tpage_add_anon_rmap(new, vma, addr);\n\telse\n\t\tpage_add_file_rmap(new);\n\n\tif (vma->vm_flags & VM_LOCKED)\n\t\tmlock_vma_page(new);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, addr, ptep);\nunlock:\n\tpte_unmap_unlock(ptep, ptl);\nout:\n\treturn SWAP_AGAIN;\n}\n\n/*\n * Get rid of all migration entries and replace them by\n * references to the indicated page.\n */\nstatic void remove_migration_ptes(struct page *old, struct page *new)\n{\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = remove_migration_pte,\n\t\t.arg = old,\n\t};\n\n\trmap_walk(new, &rwc);\n}\n\n/*\n * Something used the pte of a page under migration. We need to\n * get to the page and wait until migration is finished.\n * When we return from this function the fault will be retried.\n */\nvoid __migration_entry_wait(struct mm_struct *mm, pte_t *ptep,\n\t\t\t\tspinlock_t *ptl)\n{\n\tpte_t pte;\n\tswp_entry_t entry;\n\tstruct page *page;\n\n\tspin_lock(ptl);\n\tpte = *ptep;\n\tif (!is_swap_pte(pte))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(pte);\n\tif (!is_migration_entry(entry))\n\t\tgoto out;\n\n\tpage = migration_entry_to_page(entry);\n\n\t/*\n\t * Once radix-tree replacement of page migration started, page_count\n\t * *must* be zero. And, we don't want to call wait_on_page_locked()\n\t * against a page without get_page().\n\t * So, we use get_page_unless_zero(), here. Even failed, page fault\n\t * will occur again.\n\t */\n\tif (!get_page_unless_zero(page))\n\t\tgoto out;\n\tpte_unmap_unlock(ptep, ptl);\n\twait_on_page_locked(page);\n\tput_page(page);\n\treturn;\nout:\n\tpte_unmap_unlock(ptep, ptl);\n}\n\nvoid migration_entry_wait(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t\tunsigned long address)\n{\n\tspinlock_t *ptl = pte_lockptr(mm, pmd);\n\tpte_t *ptep = pte_offset_map(pmd, address);\n\t__migration_entry_wait(mm, ptep, ptl);\n}\n\nvoid migration_entry_wait_huge(struct vm_area_struct *vma,\n\t\tstruct mm_struct *mm, pte_t *pte)\n{\n\tspinlock_t *ptl = huge_pte_lockptr(hstate_vma(vma), mm, pte);\n\t__migration_entry_wait(mm, pte, ptl);\n}\n\n#ifdef CONFIG_BLOCK\n/* Returns true if all buffers are successfully locked */\nstatic bool buffer_migrate_lock_buffers(struct buffer_head *head,\n\t\t\t\t\t\t\tenum migrate_mode mode)\n{\n\tstruct buffer_head *bh = head;\n\n\t/* Simple case, sync compaction */\n\tif (mode != MIGRATE_ASYNC) {\n\t\tdo {\n\t\t\tget_bh(bh);\n\t\t\tlock_buffer(bh);\n\t\t\tbh = bh->b_this_page;\n\n\t\t} while (bh != head);\n\n\t\treturn true;\n\t}\n\n\t/* async case, we cannot block on lock_buffer so use trylock_buffer */\n\tdo {\n\t\tget_bh(bh);\n\t\tif (!trylock_buffer(bh)) {\n\t\t\t/*\n\t\t\t * We failed to lock the buffer and cannot stall in\n\t\t\t * async migration. Release the taken locks\n\t\t\t */\n\t\t\tstruct buffer_head *failed_bh = bh;\n\t\t\tput_bh(failed_bh);\n\t\t\tbh = head;\n\t\t\twhile (bh != failed_bh) {\n\t\t\t\tunlock_buffer(bh);\n\t\t\t\tput_bh(bh);\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\treturn true;\n}\n#else\nstatic inline bool buffer_migrate_lock_buffers(struct buffer_head *head,\n\t\t\t\t\t\t\tenum migrate_mode mode)\n{\n\treturn true;\n}\n#endif /* CONFIG_BLOCK */\n\n/*\n * Replace the page in the mapping.\n *\n * The number of remaining references must be:\n * 1 for anonymous pages without a mapping\n * 2 for pages with a mapping\n * 3 for pages with a mapping and PagePrivate/PagePrivate2 set.\n */\nint migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tstruct zone *oldzone, *newzone;\n\tint dirty;\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\toldzone = page_zone(page);\n\tnewzone = page_zone(newpage);\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\t/* Move dirty while page refs frozen and newpage not yet exposed */\n\tdirty = PageDirty(page);\n\tif (dirty) {\n\t\tClearPageDirty(page);\n\t\tSetPageDirty(newpage);\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\tspin_unlock(&mapping->tree_lock);\n\t/* Leave irq disabled to prevent preemption while updating stats */\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\tif (newzone != oldzone) {\n\t\t__dec_zone_state(oldzone, NR_FILE_PAGES);\n\t\t__inc_zone_state(newzone, NR_FILE_PAGES);\n\t\tif (PageSwapBacked(page) && !PageSwapCache(page)) {\n\t\t\t__dec_zone_state(oldzone, NR_SHMEM);\n\t\t\t__inc_zone_state(newzone, NR_SHMEM);\n\t\t}\n\t\tif (dirty && mapping_cap_account_dirty(mapping)) {\n\t\t\t__dec_zone_state(oldzone, NR_FILE_DIRTY);\n\t\t\t__inc_zone_state(newzone, NR_FILE_DIRTY);\n\t\t}\n\t}\n\tlocal_irq_enable();\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\n\n/*\n * The expected number of remaining references is the same as that\n * of migrate_page_move_mapping().\n */\nint migrate_huge_page_move_mapping(struct address_space *mapping,\n\t\t\t\t   struct page *newpage, struct page *page)\n{\n\tint expected_count;\n\tvoid **pslot;\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n\t\t\t\t\tpage_index(page));\n\n\texpected_count = 2 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tget_page(newpage);\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\tspin_unlock_irq(&mapping->tree_lock);\n\treturn MIGRATEPAGE_SUCCESS;\n}\n\n/*\n * Gigantic pages are so large that we do not guarantee that page++ pointer\n * arithmetic will work across the entire page.  We need something more\n * specialized.\n */\nstatic void __copy_gigantic_page(struct page *dst, struct page *src,\n\t\t\t\tint nr_pages)\n{\n\tint i;\n\tstruct page *dst_base = dst;\n\tstruct page *src_base = src;\n\n\tfor (i = 0; i < nr_pages; ) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst, src);\n\n\t\ti++;\n\t\tdst = mem_map_next(dst, dst_base, i);\n\t\tsrc = mem_map_next(src, src_base, i);\n\t}\n}\n\nstatic void copy_huge_page(struct page *dst, struct page *src)\n{\n\tint i;\n\tint nr_pages;\n\n\tif (PageHuge(src)) {\n\t\t/* hugetlbfs page */\n\t\tstruct hstate *h = page_hstate(src);\n\t\tnr_pages = pages_per_huge_page(h);\n\n\t\tif (unlikely(nr_pages > MAX_ORDER_NR_PAGES)) {\n\t\t\t__copy_gigantic_page(dst, src, nr_pages);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\t/* thp page */\n\t\tBUG_ON(!PageTransHuge(src));\n\t\tnr_pages = hpage_nr_pages(src);\n\t}\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst + i, src + i);\n\t}\n}\n\n/*\n * Copy the page to its new location\n */\nvoid migrate_page_copy(struct page *newpage, struct page *page)\n{\n\tint cpupid;\n\n\tif (PageHuge(page) || PageTransHuge(page))\n\t\tcopy_huge_page(newpage, page);\n\telse\n\t\tcopy_highpage(newpage, page);\n\n\tif (PageError(page))\n\t\tSetPageError(newpage);\n\tif (PageReferenced(page))\n\t\tSetPageReferenced(newpage);\n\tif (PageUptodate(page))\n\t\tSetPageUptodate(newpage);\n\tif (TestClearPageActive(page)) {\n\t\tVM_BUG_ON_PAGE(PageUnevictable(page), page);\n\t\tSetPageActive(newpage);\n\t} else if (TestClearPageUnevictable(page))\n\t\tSetPageUnevictable(newpage);\n\tif (PageChecked(page))\n\t\tSetPageChecked(newpage);\n\tif (PageMappedToDisk(page))\n\t\tSetPageMappedToDisk(newpage);\n\n\t/* Move dirty on pages not done by migrate_page_move_mapping() */\n\tif (PageDirty(page))\n\t\tSetPageDirty(newpage);\n\n\tif (page_is_young(page))\n\t\tset_page_young(newpage);\n\tif (page_is_idle(page))\n\t\tset_page_idle(newpage);\n\n\t/*\n\t * Copy NUMA information to the new page, to prevent over-eager\n\t * future migrations of this same page.\n\t */\n\tcpupid = page_cpupid_xchg_last(page, -1);\n\tpage_cpupid_xchg_last(newpage, cpupid);\n\n\tksm_migrate_page(newpage, page);\n\t/*\n\t * Please do not reorder this without considering how mm/ksm.c's\n\t * get_ksm_page() depends upon ksm_migrate_page() and PageSwapCache().\n\t */\n\tif (PageSwapCache(page))\n\t\tClearPageSwapCache(page);\n\tClearPagePrivate(page);\n\tset_page_private(page, 0);\n\n\t/*\n\t * If any waiters have accumulated on the new page then\n\t * wake them up.\n\t */\n\tif (PageWriteback(newpage))\n\t\tend_page_writeback(newpage);\n}\n\n/************************************************************\n *                    Migration functions\n ***********************************************************/\n\n/*\n * Common logic to directly migrate a single page suitable for\n * pages that do not use PagePrivate/PagePrivate2.\n *\n * Pages are locked upon entry and exit.\n */\nint migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tenum migrate_mode mode)\n{\n\tint rc;\n\n\tBUG_ON(PageWriteback(page));\t/* Writeback must be complete */\n\n\trc = migrate_page_move_mapping(mapping, newpage, page, NULL, mode, 0);\n\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\treturn rc;\n\n\tmigrate_page_copy(newpage, page);\n\treturn MIGRATEPAGE_SUCCESS;\n}\nEXPORT_SYMBOL(migrate_page);\n\n#ifdef CONFIG_BLOCK\n/*\n * Migration function for pages with buffers. This function can only be used\n * if the underlying filesystem guarantees that no other references to \"page\"\n * exist.\n */\nint buffer_migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tstruct buffer_head *bh, *head;\n\tint rc;\n\n\tif (!page_has_buffers(page))\n\t\treturn migrate_page(mapping, newpage, page, mode);\n\n\thead = page_buffers(page);\n\n\trc = migrate_page_move_mapping(mapping, newpage, page, head, mode, 0);\n\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\treturn rc;\n\n\t/*\n\t * In the async case, migrate_page_move_mapping locked the buffers\n\t * with an IRQ-safe spinlock held. In the sync case, the buffers\n\t * need to be locked now\n\t */\n\tif (mode != MIGRATE_ASYNC)\n\t\tBUG_ON(!buffer_migrate_lock_buffers(head, mode));\n\n\tClearPagePrivate(page);\n\tset_page_private(newpage, page_private(page));\n\tset_page_private(page, 0);\n\tput_page(page);\n\tget_page(newpage);\n\n\tbh = head;\n\tdo {\n\t\tset_bh_page(bh, newpage, bh_offset(bh));\n\t\tbh = bh->b_this_page;\n\n\t} while (bh != head);\n\n\tSetPagePrivate(newpage);\n\n\tmigrate_page_copy(newpage, page);\n\n\tbh = head;\n\tdo {\n\t\tunlock_buffer(bh);\n \t\tput_bh(bh);\n\t\tbh = bh->b_this_page;\n\n\t} while (bh != head);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\nEXPORT_SYMBOL(buffer_migrate_page);\n#endif\n\n/*\n * Writeback a page to clean the dirty state\n */\nstatic int writeout(struct address_space *mapping, struct page *page)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_NONE,\n\t\t.nr_to_write = 1,\n\t\t.range_start = 0,\n\t\t.range_end = LLONG_MAX,\n\t\t.for_reclaim = 1\n\t};\n\tint rc;\n\n\tif (!mapping->a_ops->writepage)\n\t\t/* No write method for the address space */\n\t\treturn -EINVAL;\n\n\tif (!clear_page_dirty_for_io(page))\n\t\t/* Someone else already triggered a write */\n\t\treturn -EAGAIN;\n\n\t/*\n\t * A dirty page may imply that the underlying filesystem has\n\t * the page on some queue. So the page must be clean for\n\t * migration. Writeout may mean we loose the lock and the\n\t * page state is no longer what we checked for earlier.\n\t * At this point we know that the migration attempt cannot\n\t * be successful.\n\t */\n\tremove_migration_ptes(page, page);\n\n\trc = mapping->a_ops->writepage(page, &wbc);\n\n\tif (rc != AOP_WRITEPAGE_ACTIVATE)\n\t\t/* unlocked. Relock */\n\t\tlock_page(page);\n\n\treturn (rc < 0) ? -EIO : -EAGAIN;\n}\n\n/*\n * Default handling if a filesystem does not provide a migration function.\n */\nstatic int fallback_migrate_page(struct address_space *mapping,\n\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tif (PageDirty(page)) {\n\t\t/* Only writeback pages in full synchronous migration */\n\t\tif (mode != MIGRATE_SYNC)\n\t\t\treturn -EBUSY;\n\t\treturn writeout(mapping, page);\n\t}\n\n\t/*\n\t * Buffers may be managed in a filesystem specific way.\n\t * We must have no buffers or drop them.\n\t */\n\tif (page_has_private(page) &&\n\t    !try_to_release_page(page, GFP_KERNEL))\n\t\treturn -EAGAIN;\n\n\treturn migrate_page(mapping, newpage, page, mode);\n}\n\n/*\n * Move a page to a newly allocated page\n * The page is locked and all ptes have been successfully removed.\n *\n * The new page will have replaced the old page if this function\n * is successful.\n *\n * Return value:\n *   < 0 - error code\n *  MIGRATEPAGE_SUCCESS - success\n */\nstatic int move_to_new_page(struct page *newpage, struct page *page,\n\t\t\t\tenum migrate_mode mode)\n{\n\tstruct address_space *mapping;\n\tint rc;\n\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tVM_BUG_ON_PAGE(!PageLocked(newpage), newpage);\n\n\tmapping = page_mapping(page);\n\tif (!mapping)\n\t\trc = migrate_page(mapping, newpage, page, mode);\n\telse if (mapping->a_ops->migratepage)\n\t\t/*\n\t\t * Most pages have a mapping and most filesystems provide a\n\t\t * migratepage callback. Anonymous pages are part of swap\n\t\t * space which also has its own migratepage callback. This\n\t\t * is the most common path for page migration.\n\t\t */\n\t\trc = mapping->a_ops->migratepage(mapping, newpage, page, mode);\n\telse\n\t\trc = fallback_migrate_page(mapping, newpage, page, mode);\n\n\t/*\n\t * When successful, old pagecache page->mapping must be cleared before\n\t * page is freed; but stats require that PageAnon be left as PageAnon.\n\t */\n\tif (rc == MIGRATEPAGE_SUCCESS) {\n\t\tset_page_memcg(page, NULL);\n\t\tif (!PageAnon(page))\n\t\t\tpage->mapping = NULL;\n\t}\n\treturn rc;\n}\n\nstatic int __unmap_and_move(struct page *page, struct page *newpage,\n\t\t\t\tint force, enum migrate_mode mode)\n{\n\tint rc = -EAGAIN;\n\tint page_was_mapped = 0;\n\tstruct anon_vma *anon_vma = NULL;\n\n\tif (!trylock_page(page)) {\n\t\tif (!force || mode == MIGRATE_ASYNC)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * It's not safe for direct compaction to call lock_page.\n\t\t * For example, during page readahead pages are added locked\n\t\t * to the LRU. Later, when the IO completes the pages are\n\t\t * marked uptodate and unlocked. However, the queueing\n\t\t * could be merging multiple pages for one bio (e.g.\n\t\t * mpage_readpages). If an allocation happens for the\n\t\t * second or third page, the process can end up locking\n\t\t * the same page twice and deadlocking. Rather than\n\t\t * trying to be clever about what pages can be locked,\n\t\t * avoid the use of lock_page for direct compaction\n\t\t * altogether.\n\t\t */\n\t\tif (current->flags & PF_MEMALLOC)\n\t\t\tgoto out;\n\n\t\tlock_page(page);\n\t}\n\n\tif (PageWriteback(page)) {\n\t\t/*\n\t\t * Only in the case of a full synchronous migration is it\n\t\t * necessary to wait for PageWriteback. In the async case,\n\t\t * the retry loop is too short and in the sync-light case,\n\t\t * the overhead of stalling is too much\n\t\t */\n\t\tif (mode != MIGRATE_SYNC) {\n\t\t\trc = -EBUSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (!force)\n\t\t\tgoto out_unlock;\n\t\twait_on_page_writeback(page);\n\t}\n\n\t/*\n\t * By try_to_unmap(), page->mapcount goes down to 0 here. In this case,\n\t * we cannot notice that anon_vma is freed while we migrates a page.\n\t * This get_anon_vma() delays freeing anon_vma pointer until the end\n\t * of migration. File cache pages are no problem because of page_lock()\n\t * File Caches may use write_page() or lock_page() in migration, then,\n\t * just care Anon page here.\n\t *\n\t * Only page_get_anon_vma() understands the subtleties of\n\t * getting a hold on an anon_vma from outside one of its mms.\n\t * But if we cannot get anon_vma, then we won't need it anyway,\n\t * because that implies that the anon page is no longer mapped\n\t * (and cannot be remapped so long as we hold the page lock).\n\t */\n\tif (PageAnon(page) && !PageKsm(page))\n\t\tanon_vma = page_get_anon_vma(page);\n\n\t/*\n\t * Block others from accessing the new page when we get around to\n\t * establishing additional references. We are usually the only one\n\t * holding a reference to newpage at this point. We used to have a BUG\n\t * here if trylock_page(newpage) fails, but would like to allow for\n\t * cases where there might be a race with the previous use of newpage.\n\t * This is much like races on refcount of oldpage: just don't BUG().\n\t */\n\tif (unlikely(!trylock_page(newpage)))\n\t\tgoto out_unlock;\n\n\tif (unlikely(isolated_balloon_page(page))) {\n\t\t/*\n\t\t * A ballooned page does not need any special attention from\n\t\t * physical to virtual reverse mapping procedures.\n\t\t * Skip any attempt to unmap PTEs or to remap swap cache,\n\t\t * in order to avoid burning cycles at rmap level, and perform\n\t\t * the page migration right away (proteced by page lock).\n\t\t */\n\t\trc = balloon_page_migrate(newpage, page, mode);\n\t\tgoto out_unlock_both;\n\t}\n\n\t/*\n\t * Corner case handling:\n\t * 1. When a new swap-cache page is read into, it is added to the LRU\n\t * and treated as swapcache but it has no rmap yet.\n\t * Calling try_to_unmap() against a page->mapping==NULL page will\n\t * trigger a BUG.  So handle it here.\n\t * 2. An orphaned page (see truncate_complete_page) might have\n\t * fs-private metadata. The page can be picked up due to memory\n\t * offlining.  Everywhere else except page reclaim, the page is\n\t * invisible to the vm, so the page can not be migrated.  So try to\n\t * free the metadata, so the page can be freed.\n\t */\n\tif (!page->mapping) {\n\t\tVM_BUG_ON_PAGE(PageAnon(page), page);\n\t\tif (page_has_private(page)) {\n\t\t\ttry_to_free_buffers(page);\n\t\t\tgoto out_unlock_both;\n\t\t}\n\t} else if (page_mapped(page)) {\n\t\t/* Establish migration ptes */\n\t\tVM_BUG_ON_PAGE(PageAnon(page) && !PageKsm(page) && !anon_vma,\n\t\t\t\tpage);\n\t\ttry_to_unmap(page,\n\t\t\tTTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);\n\t\tpage_was_mapped = 1;\n\t}\n\n\tif (!page_mapped(page))\n\t\trc = move_to_new_page(newpage, page, mode);\n\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(page,\n\t\t\trc == MIGRATEPAGE_SUCCESS ? newpage : page);\n\nout_unlock_both:\n\tunlock_page(newpage);\nout_unlock:\n\t/* Drop an anon_vma reference if we took one */\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\tunlock_page(page);\nout:\n\treturn rc;\n}\n\n/*\n * gcc 4.7 and 4.8 on arm get an ICEs when inlining unmap_and_move().  Work\n * around it.\n */\n#if (GCC_VERSION >= 40700 && GCC_VERSION < 40900) && defined(CONFIG_ARM)\n#define ICE_noinline noinline\n#else\n#define ICE_noinline\n#endif\n\n/*\n * Obtain the lock on page, remove all ptes and migrate the page\n * to the newly allocated page in newpage.\n */\nstatic ICE_noinline int unmap_and_move(new_page_t get_new_page,\n\t\t\t\t   free_page_t put_new_page,\n\t\t\t\t   unsigned long private, struct page *page,\n\t\t\t\t   int force, enum migrate_mode mode,\n\t\t\t\t   enum migrate_reason reason)\n{\n\tint rc = MIGRATEPAGE_SUCCESS;\n\tint *result = NULL;\n\tstruct page *newpage;\n\n\tnewpage = get_new_page(page, private, &result);\n\tif (!newpage)\n\t\treturn -ENOMEM;\n\n\tif (page_count(page) == 1) {\n\t\t/* page was freed from under us. So we are done. */\n\t\tgoto out;\n\t}\n\n\tif (unlikely(PageTransHuge(page)))\n\t\tif (unlikely(split_huge_page(page)))\n\t\t\tgoto out;\n\n\trc = __unmap_and_move(page, newpage, force, mode);\n\tif (rc == MIGRATEPAGE_SUCCESS)\n\t\tput_new_page = NULL;\n\nout:\n\tif (rc != -EAGAIN) {\n\t\t/*\n\t\t * A page that has been migrated has all references\n\t\t * removed and will be freed. A page that has not been\n\t\t * migrated will have kepts its references and be\n\t\t * restored.\n\t\t */\n\t\tlist_del(&page->lru);\n\t\tdec_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\tpage_is_file_cache(page));\n\t\t/* Soft-offlined page shouldn't go through lru cache list */\n\t\tif (reason == MR_MEMORY_FAILURE) {\n\t\t\tput_page(page);\n\t\t\tif (!test_set_page_hwpoison(page))\n\t\t\t\tnum_poisoned_pages_inc();\n\t\t} else\n\t\t\tputback_lru_page(page);\n\t}\n\n\t/*\n\t * If migration was not successful and there's a freeing callback, use\n\t * it.  Otherwise, putback_lru_page() will drop the reference grabbed\n\t * during isolation.\n\t */\n\tif (put_new_page)\n\t\tput_new_page(newpage, private);\n\telse if (unlikely(__is_movable_balloon_page(newpage))) {\n\t\t/* drop our reference, page already in the balloon */\n\t\tput_page(newpage);\n\t} else\n\t\tputback_lru_page(newpage);\n\n\tif (result) {\n\t\tif (rc)\n\t\t\t*result = rc;\n\t\telse\n\t\t\t*result = page_to_nid(newpage);\n\t}\n\treturn rc;\n}\n\n/*\n * Counterpart of unmap_and_move_page() for hugepage migration.\n *\n * This function doesn't wait the completion of hugepage I/O\n * because there is no race between I/O and migration for hugepage.\n * Note that currently hugepage I/O occurs only in direct I/O\n * where no lock is held and PG_writeback is irrelevant,\n * and writeback status of all subpages are counted in the reference\n * count of the head page (i.e. if all subpages of a 2MB hugepage are\n * under direct I/O, the reference of the head page is 512 and a bit more.)\n * This means that when we try to migrate hugepage whose subpages are\n * doing direct I/O, some references remain after try_to_unmap() and\n * hugepage migration fails without data corruption.\n *\n * There is also no race when direct I/O is issued on the page under migration,\n * because then pte is replaced with migration swap entry and direct I/O code\n * will wait in the page fault for migration to complete.\n */\nstatic int unmap_and_move_huge_page(new_page_t get_new_page,\n\t\t\t\tfree_page_t put_new_page, unsigned long private,\n\t\t\t\tstruct page *hpage, int force,\n\t\t\t\tenum migrate_mode mode)\n{\n\tint rc = -EAGAIN;\n\tint *result = NULL;\n\tint page_was_mapped = 0;\n\tstruct page *new_hpage;\n\tstruct anon_vma *anon_vma = NULL;\n\n\t/*\n\t * Movability of hugepages depends on architectures and hugepage size.\n\t * This check is necessary because some callers of hugepage migration\n\t * like soft offline and memory hotremove don't walk through page\n\t * tables or check whether the hugepage is pmd-based or not before\n\t * kicking migration.\n\t */\n\tif (!hugepage_migration_supported(page_hstate(hpage))) {\n\t\tputback_active_hugepage(hpage);\n\t\treturn -ENOSYS;\n\t}\n\n\tnew_hpage = get_new_page(hpage, private, &result);\n\tif (!new_hpage)\n\t\treturn -ENOMEM;\n\n\tif (!trylock_page(hpage)) {\n\t\tif (!force || mode != MIGRATE_SYNC)\n\t\t\tgoto out;\n\t\tlock_page(hpage);\n\t}\n\n\tif (PageAnon(hpage))\n\t\tanon_vma = page_get_anon_vma(hpage);\n\n\tif (unlikely(!trylock_page(new_hpage)))\n\t\tgoto put_anon;\n\n\tif (page_mapped(hpage)) {\n\t\ttry_to_unmap(hpage,\n\t\t\tTTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);\n\t\tpage_was_mapped = 1;\n\t}\n\n\tif (!page_mapped(hpage))\n\t\trc = move_to_new_page(new_hpage, hpage, mode);\n\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(hpage,\n\t\t\trc == MIGRATEPAGE_SUCCESS ? new_hpage : hpage);\n\n\tunlock_page(new_hpage);\n\nput_anon:\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\n\tif (rc == MIGRATEPAGE_SUCCESS) {\n\t\thugetlb_cgroup_migrate(hpage, new_hpage);\n\t\tput_new_page = NULL;\n\t}\n\n\tunlock_page(hpage);\nout:\n\tif (rc != -EAGAIN)\n\t\tputback_active_hugepage(hpage);\n\n\t/*\n\t * If migration was not successful and there's a freeing callback, use\n\t * it.  Otherwise, put_page() will drop the reference grabbed during\n\t * isolation.\n\t */\n\tif (put_new_page)\n\t\tput_new_page(new_hpage, private);\n\telse\n\t\tputback_active_hugepage(new_hpage);\n\n\tif (result) {\n\t\tif (rc)\n\t\t\t*result = rc;\n\t\telse\n\t\t\t*result = page_to_nid(new_hpage);\n\t}\n\treturn rc;\n}\n\n/*\n * migrate_pages - migrate the pages specified in a list, to the free pages\n *\t\t   supplied as the target for the page migration\n *\n * @from:\t\tThe list of pages to be migrated.\n * @get_new_page:\tThe function used to allocate free pages to be used\n *\t\t\tas the target of the page migration.\n * @put_new_page:\tThe function used to free target pages if migration\n *\t\t\tfails, or NULL if no special handling is necessary.\n * @private:\t\tPrivate data to be passed on to get_new_page()\n * @mode:\t\tThe migration mode that specifies the constraints for\n *\t\t\tpage migration, if any.\n * @reason:\t\tThe reason for page migration.\n *\n * The function returns after 10 attempts or if no pages are movable any more\n * because the list has become empty or no retryable pages exist any more.\n * The caller should call putback_movable_pages() to return pages to the LRU\n * or free list only if ret != 0.\n *\n * Returns the number of pages that were not migrated, or an error code.\n */\nint migrate_pages(struct list_head *from, new_page_t get_new_page,\n\t\tfree_page_t put_new_page, unsigned long private,\n\t\tenum migrate_mode mode, int reason)\n{\n\tint retry = 1;\n\tint nr_failed = 0;\n\tint nr_succeeded = 0;\n\tint pass = 0;\n\tstruct page *page;\n\tstruct page *page2;\n\tint swapwrite = current->flags & PF_SWAPWRITE;\n\tint rc;\n\n\tif (!swapwrite)\n\t\tcurrent->flags |= PF_SWAPWRITE;\n\n\tfor(pass = 0; pass < 10 && retry; pass++) {\n\t\tretry = 0;\n\n\t\tlist_for_each_entry_safe(page, page2, from, lru) {\n\t\t\tcond_resched();\n\n\t\t\tif (PageHuge(page))\n\t\t\t\trc = unmap_and_move_huge_page(get_new_page,\n\t\t\t\t\t\tput_new_page, private, page,\n\t\t\t\t\t\tpass > 2, mode);\n\t\t\telse\n\t\t\t\trc = unmap_and_move(get_new_page, put_new_page,\n\t\t\t\t\t\tprivate, page, pass > 2, mode,\n\t\t\t\t\t\treason);\n\n\t\t\tswitch(rc) {\n\t\t\tcase -ENOMEM:\n\t\t\t\tgoto out;\n\t\t\tcase -EAGAIN:\n\t\t\t\tretry++;\n\t\t\t\tbreak;\n\t\t\tcase MIGRATEPAGE_SUCCESS:\n\t\t\t\tnr_succeeded++;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/*\n\t\t\t\t * Permanent failure (-EBUSY, -ENOSYS, etc.):\n\t\t\t\t * unlike -EAGAIN case, the failed page is\n\t\t\t\t * removed from migration page list and not\n\t\t\t\t * retried in the next outer loop.\n\t\t\t\t */\n\t\t\t\tnr_failed++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tnr_failed += retry;\n\trc = nr_failed;\nout:\n\tif (nr_succeeded)\n\t\tcount_vm_events(PGMIGRATE_SUCCESS, nr_succeeded);\n\tif (nr_failed)\n\t\tcount_vm_events(PGMIGRATE_FAIL, nr_failed);\n\ttrace_mm_migrate_pages(nr_succeeded, nr_failed, mode, reason);\n\n\tif (!swapwrite)\n\t\tcurrent->flags &= ~PF_SWAPWRITE;\n\n\treturn rc;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * Move a list of individual pages\n */\nstruct page_to_node {\n\tunsigned long addr;\n\tstruct page *page;\n\tint node;\n\tint status;\n};\n\nstatic struct page *new_page_node(struct page *p, unsigned long private,\n\t\tint **result)\n{\n\tstruct page_to_node *pm = (struct page_to_node *)private;\n\n\twhile (pm->node != MAX_NUMNODES && pm->page != p)\n\t\tpm++;\n\n\tif (pm->node == MAX_NUMNODES)\n\t\treturn NULL;\n\n\t*result = &pm->status;\n\n\tif (PageHuge(p))\n\t\treturn alloc_huge_page_node(page_hstate(compound_head(p)),\n\t\t\t\t\tpm->node);\n\telse\n\t\treturn __alloc_pages_node(pm->node,\n\t\t\t\tGFP_HIGHUSER_MOVABLE | __GFP_THISNODE, 0);\n}\n\n/*\n * Move a set of pages as indicated in the pm array. The addr\n * field must be set to the virtual address of the page to be moved\n * and the node number must contain a valid target node.\n * The pm array ends with node = MAX_NUMNODES.\n */\nstatic int do_move_page_to_node_array(struct mm_struct *mm,\n\t\t\t\t      struct page_to_node *pm,\n\t\t\t\t      int migrate_all)\n{\n\tint err;\n\tstruct page_to_node *pp;\n\tLIST_HEAD(pagelist);\n\n\tdown_read(&mm->mmap_sem);\n\n\t/*\n\t * Build a list of pages to migrate\n\t */\n\tfor (pp = pm; pp->node != MAX_NUMNODES; pp++) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct page *page;\n\n\t\terr = -EFAULT;\n\t\tvma = find_vma(mm, pp->addr);\n\t\tif (!vma || pp->addr < vma->vm_start || !vma_migratable(vma))\n\t\t\tgoto set_status;\n\n\t\t/* FOLL_DUMP to ignore special (like zero) pages */\n\t\tpage = follow_page(vma, pp->addr,\n\t\t\t\tFOLL_GET | FOLL_SPLIT | FOLL_DUMP);\n\n\t\terr = PTR_ERR(page);\n\t\tif (IS_ERR(page))\n\t\t\tgoto set_status;\n\n\t\terr = -ENOENT;\n\t\tif (!page)\n\t\t\tgoto set_status;\n\n\t\tpp->page = page;\n\t\terr = page_to_nid(page);\n\n\t\tif (err == pp->node)\n\t\t\t/*\n\t\t\t * Node already in the right place\n\t\t\t */\n\t\t\tgoto put_and_set;\n\n\t\terr = -EACCES;\n\t\tif (page_mapcount(page) > 1 &&\n\t\t\t\t!migrate_all)\n\t\t\tgoto put_and_set;\n\n\t\tif (PageHuge(page)) {\n\t\t\tif (PageHead(page))\n\t\t\t\tisolate_huge_page(page, &pagelist);\n\t\t\tgoto put_and_set;\n\t\t}\n\n\t\terr = isolate_lru_page(page);\n\t\tif (!err) {\n\t\t\tlist_add_tail(&page->lru, &pagelist);\n\t\t\tinc_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\t\t    page_is_file_cache(page));\n\t\t}\nput_and_set:\n\t\t/*\n\t\t * Either remove the duplicate refcount from\n\t\t * isolate_lru_page() or drop the page ref if it was\n\t\t * not isolated.\n\t\t */\n\t\tput_page(page);\nset_status:\n\t\tpp->status = err;\n\t}\n\n\terr = 0;\n\tif (!list_empty(&pagelist)) {\n\t\terr = migrate_pages(&pagelist, new_page_node, NULL,\n\t\t\t\t(unsigned long)pm, MIGRATE_SYNC, MR_SYSCALL);\n\t\tif (err)\n\t\t\tputback_movable_pages(&pagelist);\n\t}\n\n\tup_read(&mm->mmap_sem);\n\treturn err;\n}\n\n/*\n * Migrate an array of page address onto an array of nodes and fill\n * the corresponding array of status.\n */\nstatic int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n\t\t\t unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t const int __user *nodes,\n\t\t\t int __user *status, int flags)\n{\n\tstruct page_to_node *pm;\n\tunsigned long chunk_nr_pages;\n\tunsigned long chunk_start;\n\tint err;\n\n\terr = -ENOMEM;\n\tpm = (struct page_to_node *)__get_free_page(GFP_KERNEL);\n\tif (!pm)\n\t\tgoto out;\n\n\tmigrate_prep();\n\n\t/*\n\t * Store a chunk of page_to_node array in a page,\n\t * but keep the last one as a marker\n\t */\n\tchunk_nr_pages = (PAGE_SIZE / sizeof(struct page_to_node)) - 1;\n\n\tfor (chunk_start = 0;\n\t     chunk_start < nr_pages;\n\t     chunk_start += chunk_nr_pages) {\n\t\tint j;\n\n\t\tif (chunk_start + chunk_nr_pages > nr_pages)\n\t\t\tchunk_nr_pages = nr_pages - chunk_start;\n\n\t\t/* fill the chunk pm with addrs and nodes from user-space */\n\t\tfor (j = 0; j < chunk_nr_pages; j++) {\n\t\t\tconst void __user *p;\n\t\t\tint node;\n\n\t\t\terr = -EFAULT;\n\t\t\tif (get_user(p, pages + j + chunk_start))\n\t\t\t\tgoto out_pm;\n\t\t\tpm[j].addr = (unsigned long) p;\n\n\t\t\tif (get_user(node, nodes + j + chunk_start))\n\t\t\t\tgoto out_pm;\n\n\t\t\terr = -ENODEV;\n\t\t\tif (node < 0 || node >= MAX_NUMNODES)\n\t\t\t\tgoto out_pm;\n\n\t\t\tif (!node_state(node, N_MEMORY))\n\t\t\t\tgoto out_pm;\n\n\t\t\terr = -EACCES;\n\t\t\tif (!node_isset(node, task_nodes))\n\t\t\t\tgoto out_pm;\n\n\t\t\tpm[j].node = node;\n\t\t}\n\n\t\t/* End marker for this chunk */\n\t\tpm[chunk_nr_pages].node = MAX_NUMNODES;\n\n\t\t/* Migrate this chunk */\n\t\terr = do_move_page_to_node_array(mm, pm,\n\t\t\t\t\t\t flags & MPOL_MF_MOVE_ALL);\n\t\tif (err < 0)\n\t\t\tgoto out_pm;\n\n\t\t/* Return status information */\n\t\tfor (j = 0; j < chunk_nr_pages; j++)\n\t\t\tif (put_user(pm[j].status, status + j + chunk_start)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto out_pm;\n\t\t\t}\n\t}\n\terr = 0;\n\nout_pm:\n\tfree_page((unsigned long)pm);\nout:\n\treturn err;\n}\n\n/*\n * Determine the nodes of an array of pages and store it in an array of status.\n */\nstatic void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t\tconst void __user **pages, int *status)\n{\n\tunsigned long i;\n\n\tdown_read(&mm->mmap_sem);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long addr = (unsigned long)(*pages);\n\t\tstruct vm_area_struct *vma;\n\t\tstruct page *page;\n\t\tint err = -EFAULT;\n\n\t\tvma = find_vma(mm, addr);\n\t\tif (!vma || addr < vma->vm_start)\n\t\t\tgoto set_status;\n\n\t\t/* FOLL_DUMP to ignore special (like zero) pages */\n\t\tpage = follow_page(vma, addr, FOLL_DUMP);\n\n\t\terr = PTR_ERR(page);\n\t\tif (IS_ERR(page))\n\t\t\tgoto set_status;\n\n\t\terr = page ? page_to_nid(page) : -ENOENT;\nset_status:\n\t\t*status = err;\n\n\t\tpages++;\n\t\tstatus++;\n\t}\n\n\tup_read(&mm->mmap_sem);\n}\n\n/*\n * Determine the nodes of a user array of pages and store it in\n * a user array of status.\n */\nstatic int do_pages_stat(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t int __user *status)\n{\n#define DO_PAGES_STAT_CHUNK_NR 16\n\tconst void __user *chunk_pages[DO_PAGES_STAT_CHUNK_NR];\n\tint chunk_status[DO_PAGES_STAT_CHUNK_NR];\n\n\twhile (nr_pages) {\n\t\tunsigned long chunk_nr;\n\n\t\tchunk_nr = nr_pages;\n\t\tif (chunk_nr > DO_PAGES_STAT_CHUNK_NR)\n\t\t\tchunk_nr = DO_PAGES_STAT_CHUNK_NR;\n\n\t\tif (copy_from_user(chunk_pages, pages, chunk_nr * sizeof(*chunk_pages)))\n\t\t\tbreak;\n\n\t\tdo_pages_stat_array(mm, chunk_nr, chunk_pages, chunk_status);\n\n\t\tif (copy_to_user(status, chunk_status, chunk_nr * sizeof(*status)))\n\t\t\tbreak;\n\n\t\tpages += chunk_nr;\n\t\tstatus += chunk_nr;\n\t\tnr_pages -= chunk_nr;\n\t}\n\treturn nr_pages ? -EFAULT : 0;\n}\n\n/*\n * Move a list of pages in the address space of the currently executing\n * process.\n */\nSYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,\n\t\tconst void __user * __user *, pages,\n\t\tconst int __user *, nodes,\n\t\tint __user *, status, int, flags)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct task_struct *task;\n\tstruct mm_struct *mm;\n\tint err;\n\tnodemask_t task_nodes;\n\n\t/* Check flags */\n\tif (flags & ~(MPOL_MF_MOVE|MPOL_MF_MOVE_ALL))\n\t\treturn -EINVAL;\n\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\t/* Find the mm_struct */\n\trcu_read_lock();\n\ttask = pid ? find_task_by_vpid(pid) : current;\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn -ESRCH;\n\t}\n\tget_task_struct(task);\n\n\t/*\n\t * Check if this process has the right to modify the specified\n\t * process. The right exists if the process has administrative\n\t * capabilities, superuser privileges or the same\n\t * userid as the target process.\n\t */\n\ttcred = __task_cred(task);\n\tif (!uid_eq(cred->euid, tcred->suid) && !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->uid,  tcred->suid) && !uid_eq(cred->uid,  tcred->uid) &&\n\t    !capable(CAP_SYS_NICE)) {\n\t\trcu_read_unlock();\n\t\terr = -EPERM;\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n \terr = security_task_movememory(task);\n \tif (err)\n\t\tgoto out;\n\n\ttask_nodes = cpuset_mems_allowed(task);\n\tmm = get_task_mm(task);\n\tput_task_struct(task);\n\n\tif (!mm)\n\t\treturn -EINVAL;\n\n\tif (nodes)\n\t\terr = do_pages_move(mm, task_nodes, nr_pages, pages,\n\t\t\t\t    nodes, status, flags);\n\telse\n\t\terr = do_pages_stat(mm, nr_pages, pages, status);\n\n\tmmput(mm);\n\treturn err;\n\nout:\n\tput_task_struct(task);\n\treturn err;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Returns true if this is a safe migration target node for misplaced NUMA\n * pages. Currently it only checks the watermarks which crude\n */\nstatic bool migrate_balanced_pgdat(struct pglist_data *pgdat,\n\t\t\t\t   unsigned long nr_migrate_pages)\n{\n\tint z;\n\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n\t\tstruct zone *zone = pgdat->node_zones + z;\n\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (!zone_reclaimable(zone))\n\t\t\tcontinue;\n\n\t\t/* Avoid waking kswapd by allocating pages_to_migrate pages. */\n\t\tif (!zone_watermark_ok(zone, 0,\n\t\t\t\t       high_wmark_pages(zone) +\n\t\t\t\t       nr_migrate_pages,\n\t\t\t\t       0, 0))\n\t\t\tcontinue;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic struct page *alloc_misplaced_dst_page(struct page *page,\n\t\t\t\t\t   unsigned long data,\n\t\t\t\t\t   int **result)\n{\n\tint nid = (int) data;\n\tstruct page *newpage;\n\n\tnewpage = __alloc_pages_node(nid,\n\t\t\t\t\t (GFP_HIGHUSER_MOVABLE |\n\t\t\t\t\t  __GFP_THISNODE | __GFP_NOMEMALLOC |\n\t\t\t\t\t  __GFP_NORETRY | __GFP_NOWARN) &\n\t\t\t\t\t ~GFP_IOFS, 0);\n\n\treturn newpage;\n}\n\n/*\n * page migration rate limiting control.\n * Do not migrate more than @pages_to_migrate in a @migrate_interval_millisecs\n * window of time. Default here says do not migrate more than 1280M per second.\n */\nstatic unsigned int migrate_interval_millisecs __read_mostly = 100;\nstatic unsigned int ratelimit_pages __read_mostly = 128 << (20 - PAGE_SHIFT);\n\n/* Returns true if the node is migrate rate-limited after the update */\nstatic bool numamigrate_update_ratelimit(pg_data_t *pgdat,\n\t\t\t\t\tunsigned long nr_pages)\n{\n\t/*\n\t * Rate-limit the amount of data that is being migrated to a node.\n\t * Optimal placement is no good if the memory bus is saturated and\n\t * all the time is being spent migrating!\n\t */\n\tif (time_after(jiffies, pgdat->numabalancing_migrate_next_window)) {\n\t\tspin_lock(&pgdat->numabalancing_migrate_lock);\n\t\tpgdat->numabalancing_migrate_nr_pages = 0;\n\t\tpgdat->numabalancing_migrate_next_window = jiffies +\n\t\t\tmsecs_to_jiffies(migrate_interval_millisecs);\n\t\tspin_unlock(&pgdat->numabalancing_migrate_lock);\n\t}\n\tif (pgdat->numabalancing_migrate_nr_pages > ratelimit_pages) {\n\t\ttrace_mm_numa_migrate_ratelimit(current, pgdat->node_id,\n\t\t\t\t\t\t\t\tnr_pages);\n\t\treturn true;\n\t}\n\n\t/*\n\t * This is an unlocked non-atomic update so errors are possible.\n\t * The consequences are failing to migrate when we potentiall should\n\t * have which is not severe enough to warrant locking. If it is ever\n\t * a problem, it can be converted to a per-cpu counter.\n\t */\n\tpgdat->numabalancing_migrate_nr_pages += nr_pages;\n\treturn false;\n}\n\nstatic int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)\n{\n\tint page_lru;\n\n\tVM_BUG_ON_PAGE(compound_order(page) && !PageTransHuge(page), page);\n\n\t/* Avoid migrating to a node that is nearly full */\n\tif (!migrate_balanced_pgdat(pgdat, 1UL << compound_order(page)))\n\t\treturn 0;\n\n\tif (isolate_lru_page(page))\n\t\treturn 0;\n\n\t/*\n\t * migrate_misplaced_transhuge_page() skips page migration's usual\n\t * check on page_count(), so we must do it here, now that the page\n\t * has been isolated: a GUP pin, or any other pin, prevents migration.\n\t * The expected page count is 3: 1 for page's mapcount and 1 for the\n\t * caller's pin and 1 for the reference taken by isolate_lru_page().\n\t */\n\tif (PageTransHuge(page) && page_count(page) != 3) {\n\t\tputback_lru_page(page);\n\t\treturn 0;\n\t}\n\n\tpage_lru = page_is_file_cache(page);\n\tmod_zone_page_state(page_zone(page), NR_ISOLATED_ANON + page_lru,\n\t\t\t\thpage_nr_pages(page));\n\n\t/*\n\t * Isolating the page has taken another reference, so the\n\t * caller's reference can be safely dropped without the page\n\t * disappearing underneath us during migration.\n\t */\n\tput_page(page);\n\treturn 1;\n}\n\nbool pmd_trans_migrating(pmd_t pmd)\n{\n\tstruct page *page = pmd_page(pmd);\n\treturn PageLocked(page);\n}\n\n/*\n * Attempt to migrate a misplaced page to the specified destination\n * node. Caller is expected to have an elevated reference count on\n * the page that will be dropped by this function before returning.\n */\nint migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,\n\t\t\t   int node)\n{\n\tpg_data_t *pgdat = NODE_DATA(node);\n\tint isolated;\n\tint nr_remaining;\n\tLIST_HEAD(migratepages);\n\n\t/*\n\t * Don't migrate file pages that are mapped in multiple processes\n\t * with execute permissions as they are probably shared libraries.\n\t */\n\tif (page_mapcount(page) != 1 && page_is_file_cache(page) &&\n\t    (vma->vm_flags & VM_EXEC))\n\t\tgoto out;\n\n\t/*\n\t * Rate-limit the amount of data that is being migrated to a node.\n\t * Optimal placement is no good if the memory bus is saturated and\n\t * all the time is being spent migrating!\n\t */\n\tif (numamigrate_update_ratelimit(pgdat, 1))\n\t\tgoto out;\n\n\tisolated = numamigrate_isolate_page(pgdat, page);\n\tif (!isolated)\n\t\tgoto out;\n\n\tlist_add(&page->lru, &migratepages);\n\tnr_remaining = migrate_pages(&migratepages, alloc_misplaced_dst_page,\n\t\t\t\t     NULL, node, MIGRATE_ASYNC,\n\t\t\t\t     MR_NUMA_MISPLACED);\n\tif (nr_remaining) {\n\t\tif (!list_empty(&migratepages)) {\n\t\t\tlist_del(&page->lru);\n\t\t\tdec_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\t\tpage_is_file_cache(page));\n\t\t\tputback_lru_page(page);\n\t\t}\n\t\tisolated = 0;\n\t} else\n\t\tcount_vm_numa_event(NUMA_PAGE_MIGRATE);\n\tBUG_ON(!list_empty(&migratepages));\n\treturn isolated;\n\nout:\n\tput_page(page);\n\treturn 0;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n#if defined(CONFIG_NUMA_BALANCING) && defined(CONFIG_TRANSPARENT_HUGEPAGE)\n/*\n * Migrates a THP to a given target node. page must be locked and is unlocked\n * before returning.\n */\nint migrate_misplaced_transhuge_page(struct mm_struct *mm,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tpmd_t *pmd, pmd_t entry,\n\t\t\t\tunsigned long address,\n\t\t\t\tstruct page *page, int node)\n{\n\tspinlock_t *ptl;\n\tpg_data_t *pgdat = NODE_DATA(node);\n\tint isolated = 0;\n\tstruct page *new_page = NULL;\n\tint page_lru = page_is_file_cache(page);\n\tunsigned long mmun_start = address & HPAGE_PMD_MASK;\n\tunsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;\n\tpmd_t orig_entry;\n\n\t/*\n\t * Rate-limit the amount of data that is being migrated to a node.\n\t * Optimal placement is no good if the memory bus is saturated and\n\t * all the time is being spent migrating!\n\t */\n\tif (numamigrate_update_ratelimit(pgdat, HPAGE_PMD_NR))\n\t\tgoto out_dropref;\n\n\tnew_page = alloc_pages_node(node,\n\t\t(GFP_TRANSHUGE | __GFP_THISNODE) & ~__GFP_WAIT,\n\t\tHPAGE_PMD_ORDER);\n\tif (!new_page)\n\t\tgoto out_fail;\n\n\tisolated = numamigrate_isolate_page(pgdat, page);\n\tif (!isolated) {\n\t\tput_page(new_page);\n\t\tgoto out_fail;\n\t}\n\n\tif (mm_tlb_flush_pending(mm))\n\t\tflush_tlb_range(vma, mmun_start, mmun_end);\n\n\t/* Prepare a page as a migration target */\n\t__set_page_locked(new_page);\n\tSetPageSwapBacked(new_page);\n\n\t/* anon mapping, we can simply copy page->mapping to the new page: */\n\tnew_page->mapping = page->mapping;\n\tnew_page->index = page->index;\n\tmigrate_page_copy(new_page, page);\n\tWARN_ON(PageLRU(new_page));\n\n\t/* Recheck the target PMD */\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {\nfail_putback:\n\t\tspin_unlock(ptl);\n\t\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\n\t\t/* Reverse changes made by migrate_page_copy() */\n\t\tif (TestClearPageActive(new_page))\n\t\t\tSetPageActive(page);\n\t\tif (TestClearPageUnevictable(new_page))\n\t\t\tSetPageUnevictable(page);\n\n\t\tunlock_page(new_page);\n\t\tput_page(new_page);\t\t/* Free it */\n\n\t\t/* Retake the callers reference and putback on LRU */\n\t\tget_page(page);\n\t\tputback_lru_page(page);\n\t\tmod_zone_page_state(page_zone(page),\n\t\t\t NR_ISOLATED_ANON + page_lru, -HPAGE_PMD_NR);\n\n\t\tgoto out_unlock;\n\t}\n\n\torig_entry = *pmd;\n\tentry = mk_pmd(new_page, vma->vm_page_prot);\n\tentry = pmd_mkhuge(entry);\n\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\n\t/*\n\t * Clear the old entry under pagetable lock and establish the new PTE.\n\t * Any parallel GUP will either observe the old page blocking on the\n\t * page lock, block on the page table lock or observe the new page.\n\t * The SetPageUptodate on the new page and page_add_new_anon_rmap\n\t * guarantee the copy is visible before the pagetable update.\n\t */\n\tflush_cache_range(vma, mmun_start, mmun_end);\n\tpage_add_anon_rmap(new_page, vma, mmun_start);\n\tpmdp_huge_clear_flush_notify(vma, mmun_start, pmd);\n\tset_pmd_at(mm, mmun_start, pmd, entry);\n\tflush_tlb_range(vma, mmun_start, mmun_end);\n\tupdate_mmu_cache_pmd(vma, address, &entry);\n\n\tif (page_count(page) != 2) {\n\t\tset_pmd_at(mm, mmun_start, pmd, orig_entry);\n\t\tflush_tlb_range(vma, mmun_start, mmun_end);\n\t\tmmu_notifier_invalidate_range(mm, mmun_start, mmun_end);\n\t\tupdate_mmu_cache_pmd(vma, address, &entry);\n\t\tpage_remove_rmap(new_page);\n\t\tgoto fail_putback;\n\t}\n\n\tmlock_migrate_page(new_page, page);\n\tset_page_memcg(new_page, page_memcg(page));\n\tset_page_memcg(page, NULL);\n\tpage_remove_rmap(page);\n\n\tspin_unlock(ptl);\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\n\t/* Take an \"isolate\" reference and put new page on the LRU. */\n\tget_page(new_page);\n\tputback_lru_page(new_page);\n\n\tunlock_page(new_page);\n\tunlock_page(page);\n\tput_page(page);\t\t\t/* Drop the rmap reference */\n\tput_page(page);\t\t\t/* Drop the LRU isolation reference */\n\n\tcount_vm_events(PGMIGRATE_SUCCESS, HPAGE_PMD_NR);\n\tcount_vm_numa_events(NUMA_PAGE_MIGRATE, HPAGE_PMD_NR);\n\n\tmod_zone_page_state(page_zone(page),\n\t\t\tNR_ISOLATED_ANON + page_lru,\n\t\t\t-HPAGE_PMD_NR);\n\treturn isolated;\n\nout_fail:\n\tcount_vm_events(PGMIGRATE_FAIL, HPAGE_PMD_NR);\nout_dropref:\n\tptl = pmd_lock(mm, pmd);\n\tif (pmd_same(*pmd, entry)) {\n\t\tentry = pmd_modify(entry, vma->vm_page_prot);\n\t\tset_pmd_at(mm, mmun_start, pmd, entry);\n\t\tupdate_mmu_cache_pmd(vma, address, &entry);\n\t}\n\tspin_unlock(ptl);\n\nout_unlock:\n\tunlock_page(page);\n\tput_page(page);\n\treturn 0;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n#endif /* CONFIG_NUMA */\n"], "filenames": ["mm/migrate.c"], "buggy_code_start_loc": [32], "buggy_code_end_loc": [541], "fixing_code_start_loc": [33], "fixing_code_end_loc": [552], "type": "CWE-476", "message": "The trace_writeback_dirty_page implementation in include/trace/events/writeback.h in the Linux kernel before 4.4 improperly interacts with mm/migrate.c, which allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by triggering a certain page move.", "other": {"cve": {"id": "CVE-2016-3070", "sourceIdentifier": "secalert@redhat.com", "published": "2016-08-06T20:59:00.157", "lastModified": "2023-02-12T23:18:06.500", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The trace_writeback_dirty_page implementation in include/trace/events/writeback.h in the Linux kernel before 4.4 improperly interacts with mm/migrate.c, which allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by triggering a certain page move."}, {"lang": "es", "value": "La implementaci\u00f3n trace_writeback_dirty_page en include/trace/events/writeback.h en el kernel de Linux en versiones anteriores a 4.4 interact\u00faa incorrectamente con mm/migrate.c, lo que permite a usuarios locales provocar una denegaci\u00f3n de servicio (referencia a puntero NULL y ca\u00edda de sistema) o posiblemente tener otro impacto no especificado desencadenando un cierto movimiento de p\u00e1gina."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.3.6", "matchCriteriaId": "2B389602-4271-4CF2-BA64-4B0DAD8AB4A9"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=42cb14b110a5698ccf26ce59c4441722605a3743", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2574.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2584.html", "source": "secalert@redhat.com"}, {"url": "http://www.debian.org/security/2016/dsa-3607", "source": "secalert@redhat.com"}, {"url": "http://www.securityfocus.com/bid/90518", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-3034-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-3034-2", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-3035-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-3035-2", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-3035-3", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-3036-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-3037-1", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1308846", "source": "secalert@redhat.com", "tags": ["Issue Tracking"]}, {"url": "https://github.com/torvalds/linux/commit/42cb14b110a5698ccf26ce59c4441722605a3743", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://security-tracker.debian.org/tracker/CVE-2016-3070", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/42cb14b110a5698ccf26ce59c4441722605a3743"}}