{"buggy_code": ["# frozen_string_literal: true\n\nrequire \"sidekiq\"\n\nrequire \"zlib\"\nrequire \"base64\"\n\nmodule Sidekiq\n  class Stats\n    def initialize\n      fetch_stats_fast!\n    end\n\n    def processed\n      stat :processed\n    end\n\n    def failed\n      stat :failed\n    end\n\n    def scheduled_size\n      stat :scheduled_size\n    end\n\n    def retry_size\n      stat :retry_size\n    end\n\n    def dead_size\n      stat :dead_size\n    end\n\n    def enqueued\n      stat :enqueued\n    end\n\n    def processes_size\n      stat :processes_size\n    end\n\n    def workers_size\n      stat :workers_size\n    end\n\n    def default_queue_latency\n      stat :default_queue_latency\n    end\n\n    def queues\n      Sidekiq::Stats::Queues.new.lengths\n    end\n\n    # O(1) redis calls\n    def fetch_stats_fast!\n      pipe1_res = Sidekiq.redis { |conn|\n        conn.pipelined do\n          conn.get(\"stat:processed\")\n          conn.get(\"stat:failed\")\n          conn.zcard(\"schedule\")\n          conn.zcard(\"retry\")\n          conn.zcard(\"dead\")\n          conn.scard(\"processes\")\n          conn.lrange(\"queue:default\", -1, -1)\n        end\n      }\n\n      default_queue_latency = if (entry = pipe1_res[6].first)\n        job = begin\n          Sidekiq.load_json(entry)\n        rescue\n          {}\n        end\n        now = Time.now.to_f\n        thence = job[\"enqueued_at\"] || now\n        now - thence\n      else\n        0\n      end\n\n      @stats = {\n        processed: pipe1_res[0].to_i,\n        failed: pipe1_res[1].to_i,\n        scheduled_size: pipe1_res[2],\n        retry_size: pipe1_res[3],\n        dead_size: pipe1_res[4],\n        processes_size: pipe1_res[5],\n\n        default_queue_latency: default_queue_latency\n      }\n    end\n\n    # O(number of processes + number of queues) redis calls\n    def fetch_stats_slow!\n      processes = Sidekiq.redis { |conn|\n        conn.sscan_each(\"processes\").to_a\n      }\n\n      queues = Sidekiq.redis { |conn|\n        conn.sscan_each(\"queues\").to_a\n      }\n\n      pipe2_res = Sidekiq.redis { |conn|\n        conn.pipelined do\n          processes.each { |key| conn.hget(key, \"busy\") }\n          queues.each { |queue| conn.llen(\"queue:#{queue}\") }\n        end\n      }\n\n      s = processes.size\n      workers_size = pipe2_res[0...s].sum(&:to_i)\n      enqueued = pipe2_res[s..-1].sum(&:to_i)\n\n      @stats[:workers_size] = workers_size\n      @stats[:enqueued] = enqueued\n      @stats\n    end\n\n    def fetch_stats!\n      fetch_stats_fast!\n      fetch_stats_slow!\n    end\n\n    def reset(*stats)\n      all = %w[failed processed]\n      stats = stats.empty? ? all : all & stats.flatten.compact.map(&:to_s)\n\n      mset_args = []\n      stats.each do |stat|\n        mset_args << \"stat:#{stat}\"\n        mset_args << 0\n      end\n      Sidekiq.redis do |conn|\n        conn.mset(*mset_args)\n      end\n    end\n\n    private\n\n    def stat(s)\n      fetch_stats_slow! if @stats[s].nil?\n      @stats[s] || raise(ArgumentError, \"Unknown stat #{s}\")\n    end\n\n    class Queues\n      def lengths\n        Sidekiq.redis do |conn|\n          queues = conn.sscan_each(\"queues\").to_a\n\n          lengths = conn.pipelined {\n            queues.each do |queue|\n              conn.llen(\"queue:#{queue}\")\n            end\n          }\n\n          array_of_arrays = queues.zip(lengths).sort_by { |_, size| -size }\n          array_of_arrays.to_h\n        end\n      end\n    end\n\n    class History\n      def initialize(days_previous, start_date = nil)\n        @days_previous = days_previous\n        @start_date = start_date || Time.now.utc.to_date\n      end\n\n      def processed\n        @processed ||= date_stat_hash(\"processed\")\n      end\n\n      def failed\n        @failed ||= date_stat_hash(\"failed\")\n      end\n\n      private\n\n      def date_stat_hash(stat)\n        stat_hash = {}\n        dates = @start_date.downto(@start_date - @days_previous + 1).map { |date|\n          date.strftime(\"%Y-%m-%d\")\n        }\n\n        keys = dates.map { |datestr| \"stat:#{stat}:#{datestr}\" }\n\n        begin\n          Sidekiq.redis do |conn|\n            conn.mget(keys).each_with_index do |value, idx|\n              stat_hash[dates[idx]] = value ? value.to_i : 0\n            end\n          end\n        rescue Redis::CommandError\n          # mget will trigger a CROSSSLOT error when run against a Cluster\n          # TODO Someone want to add Cluster support?\n        end\n\n        stat_hash\n      end\n    end\n  end\n\n  ##\n  # Encapsulates a queue within Sidekiq.\n  # Allows enumeration of all jobs within the queue\n  # and deletion of jobs.\n  #\n  #   queue = Sidekiq::Queue.new(\"mailer\")\n  #   queue.each do |job|\n  #     job.klass # => 'MyWorker'\n  #     job.args # => [1, 2, 3]\n  #     job.delete if job.jid == 'abcdef1234567890'\n  #   end\n  #\n  class Queue\n    include Enumerable\n\n    ##\n    # Return all known queues within Redis.\n    #\n    def self.all\n      Sidekiq.redis { |c| c.sscan_each(\"queues\").to_a }.sort.map { |q| Sidekiq::Queue.new(q) }\n    end\n\n    attr_reader :name\n\n    def initialize(name = \"default\")\n      @name = name.to_s\n      @rname = \"queue:#{name}\"\n    end\n\n    def size\n      Sidekiq.redis { |con| con.llen(@rname) }\n    end\n\n    # Sidekiq Pro overrides this\n    def paused?\n      false\n    end\n\n    ##\n    # Calculates this queue's latency, the difference in seconds since the oldest\n    # job in the queue was enqueued.\n    #\n    # @return Float\n    def latency\n      entry = Sidekiq.redis { |conn|\n        conn.lrange(@rname, -1, -1)\n      }.first\n      return 0 unless entry\n      job = Sidekiq.load_json(entry)\n      now = Time.now.to_f\n      thence = job[\"enqueued_at\"] || now\n      now - thence\n    end\n\n    def each\n      initial_size = size\n      deleted_size = 0\n      page = 0\n      page_size = 50\n\n      loop do\n        range_start = page * page_size - deleted_size\n        range_end = range_start + page_size - 1\n        entries = Sidekiq.redis { |conn|\n          conn.lrange @rname, range_start, range_end\n        }\n        break if entries.empty?\n        page += 1\n        entries.each do |entry|\n          yield JobRecord.new(entry, @name)\n        end\n        deleted_size = initial_size - size\n      end\n    end\n\n    ##\n    # Find the job with the given JID within this queue.\n    #\n    # This is a slow, inefficient operation.  Do not use under\n    # normal conditions.\n    def find_job(jid)\n      detect { |j| j.jid == jid }\n    end\n\n    def clear\n      Sidekiq.redis do |conn|\n        conn.multi do\n          conn.unlink(@rname)\n          conn.srem(\"queues\", name)\n        end\n      end\n    end\n    alias_method :\ud83d\udca3, :clear\n  end\n\n  ##\n  # Encapsulates a pending job within a Sidekiq queue or\n  # sorted set.\n  #\n  # The job should be considered immutable but may be\n  # removed from the queue via JobRecord#delete.\n  #\n  class JobRecord\n    attr_reader :item\n    attr_reader :value\n\n    def initialize(item, queue_name = nil)\n      @args = nil\n      @value = item\n      @item = item.is_a?(Hash) ? item : parse(item)\n      @queue = queue_name || @item[\"queue\"]\n    end\n\n    def parse(item)\n      Sidekiq.load_json(item)\n    rescue JSON::ParserError\n      # If the job payload in Redis is invalid JSON, we'll load\n      # the item as an empty hash and store the invalid JSON as\n      # the job 'args' for display in the Web UI.\n      @invalid = true\n      @args = [item]\n      {}\n    end\n\n    def klass\n      self[\"class\"]\n    end\n\n    def display_class\n      # Unwrap known wrappers so they show up in a human-friendly manner in the Web UI\n      @klass ||= self[\"display_class\"] || begin\n        case klass\n        when /\\ASidekiq::Extensions::Delayed/\n          safe_load(args[0], klass) do |target, method, _|\n            \"#{target}.#{method}\"\n          end\n        when \"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\"\n          job_class = @item[\"wrapped\"] || args[0]\n          if job_class == \"ActionMailer::DeliveryJob\" || job_class == \"ActionMailer::MailDeliveryJob\"\n            # MailerClass#mailer_method\n            args[0][\"arguments\"][0..1].join(\"#\")\n          else\n            job_class\n          end\n        else\n          klass\n        end\n      end\n    end\n\n    def display_args\n      # Unwrap known wrappers so they show up in a human-friendly manner in the Web UI\n      @display_args ||= case klass\n                when /\\ASidekiq::Extensions::Delayed/\n                  safe_load(args[0], args) do |_, _, arg|\n                    arg\n                  end\n                when \"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\"\n                  job_args = self[\"wrapped\"] ? args[0][\"arguments\"] : []\n                  if (self[\"wrapped\"] || args[0]) == \"ActionMailer::DeliveryJob\"\n                    # remove MailerClass, mailer_method and 'deliver_now'\n                    job_args.drop(3)\n                  elsif (self[\"wrapped\"] || args[0]) == \"ActionMailer::MailDeliveryJob\"\n                    # remove MailerClass, mailer_method and 'deliver_now'\n                    job_args.drop(3).first[\"args\"]\n                  else\n                    job_args\n                  end\n                else\n                  if self[\"encrypt\"]\n                    # no point in showing 150+ bytes of random garbage\n                    args[-1] = \"[encrypted data]\"\n                  end\n                  args\n      end\n    end\n\n    def args\n      @args || @item[\"args\"]\n    end\n\n    def jid\n      self[\"jid\"]\n    end\n\n    def enqueued_at\n      self[\"enqueued_at\"] ? Time.at(self[\"enqueued_at\"]).utc : nil\n    end\n\n    def created_at\n      Time.at(self[\"created_at\"] || self[\"enqueued_at\"] || 0).utc\n    end\n\n    def tags\n      self[\"tags\"] || []\n    end\n\n    def error_backtrace\n      # Cache nil values\n      if defined?(@error_backtrace)\n        @error_backtrace\n      else\n        value = self[\"error_backtrace\"]\n        @error_backtrace = value && uncompress_backtrace(value)\n      end\n    end\n\n    attr_reader :queue\n\n    def latency\n      now = Time.now.to_f\n      now - (@item[\"enqueued_at\"] || @item[\"created_at\"] || now)\n    end\n\n    ##\n    # Remove this job from the queue.\n    def delete\n      count = Sidekiq.redis { |conn|\n        conn.lrem(\"queue:#{@queue}\", 1, @value)\n      }\n      count != 0\n    end\n\n    def [](name)\n      # nil will happen if the JSON fails to parse.\n      # We don't guarantee Sidekiq will work with bad job JSON but we should\n      # make a best effort to minimize the damage.\n      @item ? @item[name] : nil\n    end\n\n    private\n\n    def safe_load(content, default)\n      yield(*YAML.load(content))\n    rescue => ex\n      # #1761 in dev mode, it's possible to have jobs enqueued which haven't been loaded into\n      # memory yet so the YAML can't be loaded.\n      Sidekiq.logger.warn \"Unable to load YAML: #{ex.message}\" unless Sidekiq.options[:environment] == \"development\"\n      default\n    end\n\n    def uncompress_backtrace(backtrace)\n      if backtrace.is_a?(Array)\n        # Handle old jobs with raw Array backtrace format\n        backtrace\n      else\n        decoded = Base64.decode64(backtrace)\n        uncompressed = Zlib::Inflate.inflate(decoded)\n        begin\n          Sidekiq.load_json(uncompressed)\n        rescue\n          # Handle old jobs with marshalled backtrace format\n          # TODO Remove in 7.x\n          Marshal.load(uncompressed)\n        end\n      end\n    end\n  end\n\n  class SortedEntry < JobRecord\n    attr_reader :score\n    attr_reader :parent\n\n    def initialize(parent, score, item)\n      super(item)\n      @score = score\n      @parent = parent\n    end\n\n    def at\n      Time.at(score).utc\n    end\n\n    def delete\n      if @value\n        @parent.delete_by_value(@parent.name, @value)\n      else\n        @parent.delete_by_jid(score, jid)\n      end\n    end\n\n    def reschedule(at)\n      Sidekiq.redis do |conn|\n        conn.zincrby(@parent.name, at.to_f - @score, Sidekiq.dump_json(@item))\n      end\n    end\n\n    def add_to_queue\n      remove_job do |message|\n        msg = Sidekiq.load_json(message)\n        Sidekiq::Client.push(msg)\n      end\n    end\n\n    def retry\n      remove_job do |message|\n        msg = Sidekiq.load_json(message)\n        msg[\"retry_count\"] -= 1 if msg[\"retry_count\"]\n        Sidekiq::Client.push(msg)\n      end\n    end\n\n    ##\n    # Place job in the dead set\n    def kill\n      remove_job do |message|\n        DeadSet.new.kill(message)\n      end\n    end\n\n    def error?\n      !!item[\"error_class\"]\n    end\n\n    private\n\n    def remove_job\n      Sidekiq.redis do |conn|\n        results = conn.multi {\n          conn.zrangebyscore(parent.name, score, score)\n          conn.zremrangebyscore(parent.name, score, score)\n        }.first\n\n        if results.size == 1\n          yield results.first\n        else\n          # multiple jobs with the same score\n          # find the one with the right JID and push it\n          matched, nonmatched = results.partition { |message|\n            if message.index(jid)\n              msg = Sidekiq.load_json(message)\n              msg[\"jid\"] == jid\n            else\n              false\n            end\n          }\n\n          msg = matched.first\n          yield msg if msg\n\n          # push the rest back onto the sorted set\n          conn.multi do\n            nonmatched.each do |message|\n              conn.zadd(parent.name, score.to_f.to_s, message)\n            end\n          end\n        end\n      end\n    end\n  end\n\n  class SortedSet\n    include Enumerable\n\n    attr_reader :name\n\n    def initialize(name)\n      @name = name\n      @_size = size\n    end\n\n    def size\n      Sidekiq.redis { |c| c.zcard(name) }\n    end\n\n    def scan(match, count = 100)\n      return to_enum(:scan, match, count) unless block_given?\n\n      match = \"*#{match}*\" unless match.include?(\"*\")\n      Sidekiq.redis do |conn|\n        conn.zscan_each(name, match: match, count: count) do |entry, score|\n          yield SortedEntry.new(self, score, entry)\n        end\n      end\n    end\n\n    def clear\n      Sidekiq.redis do |conn|\n        conn.unlink(name)\n      end\n    end\n    alias_method :\ud83d\udca3, :clear\n  end\n\n  class JobSet < SortedSet\n    def schedule(timestamp, message)\n      Sidekiq.redis do |conn|\n        conn.zadd(name, timestamp.to_f.to_s, Sidekiq.dump_json(message))\n      end\n    end\n\n    def each\n      initial_size = @_size\n      offset_size = 0\n      page = -1\n      page_size = 50\n\n      loop do\n        range_start = page * page_size + offset_size\n        range_end = range_start + page_size - 1\n        elements = Sidekiq.redis { |conn|\n          conn.zrange name, range_start, range_end, with_scores: true\n        }\n        break if elements.empty?\n        page -= 1\n        elements.reverse_each do |element, score|\n          yield SortedEntry.new(self, score, element)\n        end\n        offset_size = initial_size - @_size\n      end\n    end\n\n    ##\n    # Fetch jobs that match a given time or Range. Job ID is an\n    # optional second argument.\n    def fetch(score, jid = nil)\n      begin_score, end_score =\n        if score.is_a?(Range)\n          [score.first, score.last]\n        else\n          [score, score]\n        end\n\n      elements = Sidekiq.redis { |conn|\n        conn.zrangebyscore(name, begin_score, end_score, with_scores: true)\n      }\n\n      elements.each_with_object([]) do |element, result|\n        data, job_score = element\n        entry = SortedEntry.new(self, job_score, data)\n        result << entry if jid.nil? || entry.jid == jid\n      end\n    end\n\n    ##\n    # Find the job with the given JID within this sorted set.\n    # This is a slower O(n) operation.  Do not use for app logic.\n    def find_job(jid)\n      Sidekiq.redis do |conn|\n        conn.zscan_each(name, match: \"*#{jid}*\", count: 100) do |entry, score|\n          job = JSON.parse(entry)\n          matched = job[\"jid\"] == jid\n          return SortedEntry.new(self, score, entry) if matched\n        end\n      end\n      nil\n    end\n\n    def delete_by_value(name, value)\n      Sidekiq.redis do |conn|\n        ret = conn.zrem(name, value)\n        @_size -= 1 if ret\n        ret\n      end\n    end\n\n    def delete_by_jid(score, jid)\n      Sidekiq.redis do |conn|\n        elements = conn.zrangebyscore(name, score, score)\n        elements.each do |element|\n          if element.index(jid)\n            message = Sidekiq.load_json(element)\n            if message[\"jid\"] == jid\n              ret = conn.zrem(name, element)\n              @_size -= 1 if ret\n              break ret\n            end\n          end\n        end\n      end\n    end\n\n    alias_method :delete, :delete_by_jid\n  end\n\n  ##\n  # Allows enumeration of scheduled jobs within Sidekiq.\n  # Based on this, you can search/filter for jobs.  Here's an\n  # example where I'm selecting all jobs of a certain type\n  # and deleting them from the schedule queue.\n  #\n  #   r = Sidekiq::ScheduledSet.new\n  #   r.select do |scheduled|\n  #     scheduled.klass == 'Sidekiq::Extensions::DelayedClass' &&\n  #     scheduled.args[0] == 'User' &&\n  #     scheduled.args[1] == 'setup_new_subscriber'\n  #   end.map(&:delete)\n  class ScheduledSet < JobSet\n    def initialize\n      super \"schedule\"\n    end\n  end\n\n  ##\n  # Allows enumeration of retries within Sidekiq.\n  # Based on this, you can search/filter for jobs.  Here's an\n  # example where I'm selecting all jobs of a certain type\n  # and deleting them from the retry queue.\n  #\n  #   r = Sidekiq::RetrySet.new\n  #   r.select do |retri|\n  #     retri.klass == 'Sidekiq::Extensions::DelayedClass' &&\n  #     retri.args[0] == 'User' &&\n  #     retri.args[1] == 'setup_new_subscriber'\n  #   end.map(&:delete)\n  class RetrySet < JobSet\n    def initialize\n      super \"retry\"\n    end\n\n    def retry_all\n      each(&:retry) while size > 0\n    end\n\n    def kill_all\n      each(&:kill) while size > 0\n    end\n  end\n\n  ##\n  # Allows enumeration of dead jobs within Sidekiq.\n  #\n  class DeadSet < JobSet\n    def initialize\n      super \"dead\"\n    end\n\n    def kill(message, opts = {})\n      now = Time.now.to_f\n      Sidekiq.redis do |conn|\n        conn.multi do\n          conn.zadd(name, now.to_s, message)\n          conn.zremrangebyscore(name, \"-inf\", now - self.class.timeout)\n          conn.zremrangebyrank(name, 0, - self.class.max_jobs)\n        end\n      end\n\n      if opts[:notify_failure] != false\n        job = Sidekiq.load_json(message)\n        r = RuntimeError.new(\"Job killed by API\")\n        r.set_backtrace(caller)\n        Sidekiq.death_handlers.each do |handle|\n          handle.call(job, r)\n        end\n      end\n      true\n    end\n\n    def retry_all\n      each(&:retry) while size > 0\n    end\n\n    def self.max_jobs\n      Sidekiq.options[:dead_max_jobs]\n    end\n\n    def self.timeout\n      Sidekiq.options[:dead_timeout_in_seconds]\n    end\n  end\n\n  ##\n  # Enumerates the set of Sidekiq processes which are actively working\n  # right now.  Each process sends a heartbeat to Redis every 5 seconds\n  # so this set should be relatively accurate, barring network partitions.\n  #\n  # Yields a Sidekiq::Process.\n  #\n  class ProcessSet\n    include Enumerable\n\n    def initialize(clean_plz = true)\n      cleanup if clean_plz\n    end\n\n    # Cleans up dead processes recorded in Redis.\n    # Returns the number of processes cleaned.\n    def cleanup\n      count = 0\n      Sidekiq.redis do |conn|\n        procs = conn.sscan_each(\"processes\").to_a.sort\n        heartbeats = conn.pipelined {\n          procs.each do |key|\n            conn.hget(key, \"info\")\n          end\n        }\n\n        # the hash named key has an expiry of 60 seconds.\n        # if it's not found, that means the process has not reported\n        # in to Redis and probably died.\n        to_prune = procs.select.with_index { |proc, i|\n          heartbeats[i].nil?\n        }\n        count = conn.srem(\"processes\", to_prune) unless to_prune.empty?\n      end\n      count\n    end\n\n    def each\n      result = Sidekiq.redis { |conn|\n        procs = conn.sscan_each(\"processes\").to_a.sort\n\n        # We're making a tradeoff here between consuming more memory instead of\n        # making more roundtrips to Redis, but if you have hundreds or thousands of workers,\n        # you'll be happier this way\n        conn.pipelined do\n          procs.each do |key|\n            conn.hmget(key, \"info\", \"busy\", \"beat\", \"quiet\", \"rss\", \"rtt_us\")\n          end\n        end\n      }\n\n      result.each do |info, busy, at_s, quiet, rss, rtt|\n        # If a process is stopped between when we query Redis for `procs` and\n        # when we query for `result`, we will have an item in `result` that is\n        # composed of `nil` values.\n        next if info.nil?\n\n        hash = Sidekiq.load_json(info)\n        yield Process.new(hash.merge(\"busy\" => busy.to_i,\n          \"beat\" => at_s.to_f,\n          \"quiet\" => quiet,\n          \"rss\" => rss.to_i,\n          \"rtt_us\" => rtt.to_i))\n      end\n    end\n\n    # This method is not guaranteed accurate since it does not prune the set\n    # based on current heartbeat.  #each does that and ensures the set only\n    # contains Sidekiq processes which have sent a heartbeat within the last\n    # 60 seconds.\n    def size\n      Sidekiq.redis { |conn| conn.scard(\"processes\") }\n    end\n\n    # Total number of threads available to execute jobs.\n    # For Sidekiq Enterprise customers this number (in production) must be\n    # less than or equal to your licensed concurrency.\n    def total_concurrency\n      sum { |x| x[\"concurrency\"].to_i }\n    end\n\n    def total_rss_in_kb\n      sum { |x| x[\"rss\"].to_i }\n    end\n    alias_method :total_rss, :total_rss_in_kb\n\n    # Returns the identity of the current cluster leader or \"\" if no leader.\n    # This is a Sidekiq Enterprise feature, will always return \"\" in Sidekiq\n    # or Sidekiq Pro.\n    def leader\n      @leader ||= begin\n        x = Sidekiq.redis { |c| c.get(\"dear-leader\") }\n        # need a non-falsy value so we can memoize\n        x ||= \"\"\n        x\n      end\n    end\n  end\n\n  #\n  # Sidekiq::Process represents an active Sidekiq process talking with Redis.\n  # Each process has a set of attributes which look like this:\n  #\n  # {\n  #   'hostname' => 'app-1.example.com',\n  #   'started_at' => <process start time>,\n  #   'pid' => 12345,\n  #   'tag' => 'myapp'\n  #   'concurrency' => 25,\n  #   'queues' => ['default', 'low'],\n  #   'busy' => 10,\n  #   'beat' => <last heartbeat>,\n  #   'identity' => <unique string identifying the process>,\n  # }\n  class Process\n    def initialize(hash)\n      @attribs = hash\n    end\n\n    def tag\n      self[\"tag\"]\n    end\n\n    def labels\n      Array(self[\"labels\"])\n    end\n\n    def [](key)\n      @attribs[key]\n    end\n\n    def identity\n      self[\"identity\"]\n    end\n\n    def queues\n      self[\"queues\"]\n    end\n\n    def quiet!\n      signal(\"TSTP\")\n    end\n\n    def stop!\n      signal(\"TERM\")\n    end\n\n    def dump_threads\n      signal(\"TTIN\")\n    end\n\n    def stopping?\n      self[\"quiet\"] == \"true\"\n    end\n\n    private\n\n    def signal(sig)\n      key = \"#{identity}-signals\"\n      Sidekiq.redis do |c|\n        c.multi do\n          c.lpush(key, sig)\n          c.expire(key, 60)\n        end\n      end\n    end\n  end\n\n  ##\n  # The WorkSet stores the work being done by this Sidekiq cluster.\n  # It tracks the process and thread working on each job.\n  #\n  # WARNING WARNING WARNING\n  #\n  # This is live data that can change every millisecond.\n  # If you call #size => 5 and then expect #each to be\n  # called 5 times, you're going to have a bad time.\n  #\n  #    works = Sidekiq::WorkSet.new\n  #    works.size => 2\n  #    works.each do |process_id, thread_id, work|\n  #      # process_id is a unique identifier per Sidekiq process\n  #      # thread_id is a unique identifier per thread\n  #      # work is a Hash which looks like:\n  #      # { 'queue' => name, 'run_at' => timestamp, 'payload' => job_hash }\n  #      # run_at is an epoch Integer.\n  #    end\n  #\n  class WorkSet\n    include Enumerable\n\n    def each(&block)\n      results = []\n      Sidekiq.redis do |conn|\n        procs = conn.sscan_each(\"processes\").to_a\n        procs.sort.each do |key|\n          valid, workers = conn.pipelined {\n            conn.exists?(key)\n            conn.hgetall(\"#{key}:workers\")\n          }\n          next unless valid\n          workers.each_pair do |tid, json|\n            hsh = Sidekiq.load_json(json)\n            p = hsh[\"payload\"]\n            # avoid breaking API, this is a side effect of the JSON optimization in #4316\n            hsh[\"payload\"] = Sidekiq.load_json(p) if p.is_a?(String)\n            results << [key, tid, hsh]\n          end\n        end\n      end\n\n      results.sort_by { |(_, _, hsh)| hsh[\"run_at\"] }.each(&block)\n    end\n\n    # Note that #size is only as accurate as Sidekiq's heartbeat,\n    # which happens every 5 seconds.  It is NOT real-time.\n    #\n    # Not very efficient if you have lots of Sidekiq\n    # processes but the alternative is a global counter\n    # which can easily get out of sync with crashy processes.\n    def size\n      Sidekiq.redis do |conn|\n        procs = conn.sscan_each(\"processes\").to_a\n        if procs.empty?\n          0\n        else\n          conn.pipelined {\n            procs.each do |key|\n              conn.hget(key, \"busy\")\n            end\n          }.sum(&:to_i)\n        end\n      end\n    end\n  end\n  # Since \"worker\" is a nebulous term, we've deprecated the use of this class name.\n  # Is \"worker\" a process, a type of job, a thread? Undefined!\n  # WorkSet better describes the data.\n  Workers = WorkSet\nend\n", "# frozen_string_literal: true\n\nmodule Sidekiq\n  class WebApplication\n    extend WebRouter\n\n    REDIS_KEYS = %w[redis_version uptime_in_days connected_clients used_memory_human used_memory_peak_human]\n    CSP_HEADER = [\n      \"default-src 'self' https: http:\",\n      \"child-src 'self'\",\n      \"connect-src 'self' https: http: wss: ws:\",\n      \"font-src 'self' https: http:\",\n      \"frame-src 'self'\",\n      \"img-src 'self' https: http: data:\",\n      \"manifest-src 'self'\",\n      \"media-src 'self'\",\n      \"object-src 'none'\",\n      \"script-src 'self' https: http: 'unsafe-inline'\",\n      \"style-src 'self' https: http: 'unsafe-inline'\",\n      \"worker-src 'self'\",\n      \"base-uri 'self'\"\n    ].join(\"; \").freeze\n\n    def initialize(klass)\n      @klass = klass\n    end\n\n    def settings\n      @klass.settings\n    end\n\n    def self.settings\n      Sidekiq::Web.settings\n    end\n\n    def self.tabs\n      Sidekiq::Web.tabs\n    end\n\n    def self.set(key, val)\n      # nothing, backwards compatibility\n    end\n\n    head \"/\" do\n      # HEAD / is the cheapest heartbeat possible,\n      # it hits Redis to ensure connectivity\n      Sidekiq.redis { |c| c.llen(\"queue:default\") }\n      \"\"\n    end\n\n    get \"/\" do\n      @redis_info = redis_info.select { |k, v| REDIS_KEYS.include? k }\n      stats_history = Sidekiq::Stats::History.new((params[\"days\"] || 30).to_i)\n      @processed_history = stats_history.processed\n      @failed_history = stats_history.failed\n\n      erb(:dashboard)\n    end\n\n    get \"/busy\" do\n      erb(:busy)\n    end\n\n    post \"/busy\" do\n      if params[\"identity\"]\n        p = Sidekiq::Process.new(\"identity\" => params[\"identity\"])\n        p.quiet! if params[\"quiet\"]\n        p.stop! if params[\"stop\"]\n      else\n        processes.each do |pro|\n          pro.quiet! if params[\"quiet\"]\n          pro.stop! if params[\"stop\"]\n        end\n      end\n\n      redirect \"#{root_path}busy\"\n    end\n\n    get \"/queues\" do\n      @queues = Sidekiq::Queue.all\n\n      erb(:queues)\n    end\n\n    QUEUE_NAME = /\\A[a-z_:.\\-0-9]+\\z/i\n\n    get \"/queues/:name\" do\n      @name = route_params[:name]\n\n      halt(404) if !@name || @name !~ QUEUE_NAME\n\n      @count = (params[\"count\"] || 25).to_i\n      @queue = Sidekiq::Queue.new(@name)\n      (@current_page, @total_size, @jobs) = page(\"queue:#{@name}\", params[\"page\"], @count, reverse: params[\"direction\"] == \"asc\")\n      @jobs = @jobs.map { |msg| Sidekiq::JobRecord.new(msg, @name) }\n\n      erb(:queue)\n    end\n\n    post \"/queues/:name\" do\n      queue = Sidekiq::Queue.new(route_params[:name])\n\n      if Sidekiq.pro? && params[\"pause\"]\n        queue.pause!\n      elsif Sidekiq.pro? && params[\"unpause\"]\n        queue.unpause!\n      else\n        queue.clear\n      end\n\n      redirect \"#{root_path}queues\"\n    end\n\n    post \"/queues/:name/delete\" do\n      name = route_params[:name]\n      Sidekiq::JobRecord.new(params[\"key_val\"], name).delete\n\n      redirect_with_query(\"#{root_path}queues/#{CGI.escape(name)}\")\n    end\n\n    get \"/morgue\" do\n      @count = (params[\"count\"] || 25).to_i\n      (@current_page, @total_size, @dead) = page(\"dead\", params[\"page\"], @count, reverse: true)\n      @dead = @dead.map { |msg, score| Sidekiq::SortedEntry.new(nil, score, msg) }\n\n      erb(:morgue)\n    end\n\n    get \"/morgue/:key\" do\n      key = route_params[:key]\n      halt(404) unless key\n\n      @dead = Sidekiq::DeadSet.new.fetch(*parse_params(key)).first\n\n      if @dead.nil?\n        redirect \"#{root_path}morgue\"\n      else\n        erb(:dead)\n      end\n    end\n\n    post \"/morgue\" do\n      redirect(request.path) unless params[\"key\"]\n\n      params[\"key\"].each do |key|\n        job = Sidekiq::DeadSet.new.fetch(*parse_params(key)).first\n        retry_or_delete_or_kill job, params if job\n      end\n\n      redirect_with_query(\"#{root_path}morgue\")\n    end\n\n    post \"/morgue/all/delete\" do\n      Sidekiq::DeadSet.new.clear\n\n      redirect \"#{root_path}morgue\"\n    end\n\n    post \"/morgue/all/retry\" do\n      Sidekiq::DeadSet.new.retry_all\n\n      redirect \"#{root_path}morgue\"\n    end\n\n    post \"/morgue/:key\" do\n      key = route_params[:key]\n      halt(404) unless key\n\n      job = Sidekiq::DeadSet.new.fetch(*parse_params(key)).first\n      retry_or_delete_or_kill job, params if job\n\n      redirect_with_query(\"#{root_path}morgue\")\n    end\n\n    get \"/retries\" do\n      @count = (params[\"count\"] || 25).to_i\n      (@current_page, @total_size, @retries) = page(\"retry\", params[\"page\"], @count)\n      @retries = @retries.map { |msg, score| Sidekiq::SortedEntry.new(nil, score, msg) }\n\n      erb(:retries)\n    end\n\n    get \"/retries/:key\" do\n      @retry = Sidekiq::RetrySet.new.fetch(*parse_params(route_params[:key])).first\n\n      if @retry.nil?\n        redirect \"#{root_path}retries\"\n      else\n        erb(:retry)\n      end\n    end\n\n    post \"/retries\" do\n      redirect(request.path) unless params[\"key\"]\n\n      params[\"key\"].each do |key|\n        job = Sidekiq::RetrySet.new.fetch(*parse_params(key)).first\n        retry_or_delete_or_kill job, params if job\n      end\n\n      redirect_with_query(\"#{root_path}retries\")\n    end\n\n    post \"/retries/all/delete\" do\n      Sidekiq::RetrySet.new.clear\n\n      redirect \"#{root_path}retries\"\n    end\n\n    post \"/retries/all/retry\" do\n      Sidekiq::RetrySet.new.retry_all\n\n      redirect \"#{root_path}retries\"\n    end\n\n    post \"/retries/all/kill\" do\n      Sidekiq::RetrySet.new.kill_all\n\n      redirect \"#{root_path}retries\"\n    end\n\n    post \"/retries/:key\" do\n      job = Sidekiq::RetrySet.new.fetch(*parse_params(route_params[:key])).first\n\n      retry_or_delete_or_kill job, params if job\n\n      redirect_with_query(\"#{root_path}retries\")\n    end\n\n    get \"/scheduled\" do\n      @count = (params[\"count\"] || 25).to_i\n      (@current_page, @total_size, @scheduled) = page(\"schedule\", params[\"page\"], @count)\n      @scheduled = @scheduled.map { |msg, score| Sidekiq::SortedEntry.new(nil, score, msg) }\n\n      erb(:scheduled)\n    end\n\n    get \"/scheduled/:key\" do\n      @job = Sidekiq::ScheduledSet.new.fetch(*parse_params(route_params[:key])).first\n\n      if @job.nil?\n        redirect \"#{root_path}scheduled\"\n      else\n        erb(:scheduled_job_info)\n      end\n    end\n\n    post \"/scheduled\" do\n      redirect(request.path) unless params[\"key\"]\n\n      params[\"key\"].each do |key|\n        job = Sidekiq::ScheduledSet.new.fetch(*parse_params(key)).first\n        delete_or_add_queue job, params if job\n      end\n\n      redirect_with_query(\"#{root_path}scheduled\")\n    end\n\n    post \"/scheduled/:key\" do\n      key = route_params[:key]\n      halt(404) unless key\n\n      job = Sidekiq::ScheduledSet.new.fetch(*parse_params(key)).first\n      delete_or_add_queue job, params if job\n\n      redirect_with_query(\"#{root_path}scheduled\")\n    end\n\n    get \"/dashboard/stats\" do\n      redirect \"#{root_path}stats\"\n    end\n\n    get \"/stats\" do\n      sidekiq_stats = Sidekiq::Stats.new\n      redis_stats = redis_info.select { |k, v| REDIS_KEYS.include? k }\n      json(\n        sidekiq: {\n          processed: sidekiq_stats.processed,\n          failed: sidekiq_stats.failed,\n          busy: sidekiq_stats.workers_size,\n          processes: sidekiq_stats.processes_size,\n          enqueued: sidekiq_stats.enqueued,\n          scheduled: sidekiq_stats.scheduled_size,\n          retries: sidekiq_stats.retry_size,\n          dead: sidekiq_stats.dead_size,\n          default_latency: sidekiq_stats.default_queue_latency\n        },\n        redis: redis_stats,\n        server_utc_time: server_utc_time\n      )\n    end\n\n    get \"/stats/queues\" do\n      json Sidekiq::Stats::Queues.new.lengths\n    end\n\n    def call(env)\n      action = self.class.match(env)\n      return [404, {\"Content-Type\" => \"text/plain\", \"X-Cascade\" => \"pass\"}, [\"Not Found\"]] unless action\n\n      app = @klass\n      resp = catch(:halt) do\n        self.class.run_befores(app, action)\n        action.instance_exec env, &action.block\n      ensure\n        self.class.run_afters(app, action)\n      end\n\n      case resp\n      when Array\n        # redirects go here\n        resp\n      else\n        # rendered content goes here\n        headers = {\n          \"Content-Type\" => \"text/html\",\n          \"Cache-Control\" => \"private, no-store\",\n          \"Content-Language\" => action.locale,\n          \"Content-Security-Policy\" => CSP_HEADER\n        }\n        # we'll let Rack calculate Content-Length for us.\n        [200, headers, [resp]]\n      end\n    end\n\n    def self.helpers(mod = nil, &block)\n      if block\n        WebAction.class_eval(&block)\n      else\n        WebAction.send(:include, mod)\n      end\n    end\n\n    def self.before(path = nil, &block)\n      befores << [path && Regexp.new(\"\\\\A#{path.gsub(\"*\", \".*\")}\\\\z\"), block]\n    end\n\n    def self.after(path = nil, &block)\n      afters << [path && Regexp.new(\"\\\\A#{path.gsub(\"*\", \".*\")}\\\\z\"), block]\n    end\n\n    def self.run_befores(app, action)\n      run_hooks(befores, app, action)\n    end\n\n    def self.run_afters(app, action)\n      run_hooks(afters, app, action)\n    end\n\n    def self.run_hooks(hooks, app, action)\n      hooks.select { |p, _| !p || p =~ action.env[WebRouter::PATH_INFO] }\n        .each { |_, b| action.instance_exec(action.env, app, &b) }\n    end\n\n    def self.befores\n      @befores ||= []\n    end\n\n    def self.afters\n      @afters ||= []\n    end\n  end\nend\n", "# frozen_string_literal: true\nrequire_relative 'helper'\nrequire 'sidekiq/api'\nrequire 'active_job'\nrequire 'action_mailer'\n\ndescribe 'API' do\n  before do\n    Sidekiq.redis {|c| c.flushdb }\n  end\n\n  describe \"stats\" do\n    it \"is initially zero\" do\n      s = Sidekiq::Stats.new\n      assert_equal 0, s.processed\n      assert_equal 0, s.failed\n      assert_equal 0, s.enqueued\n      assert_equal 0, s.default_queue_latency\n      assert_equal 0, s.workers_size\n    end\n\n    describe \"processed\" do\n      it \"returns number of processed jobs\" do\n        Sidekiq.redis { |conn| conn.set(\"stat:processed\", 5) }\n        s = Sidekiq::Stats.new\n        assert_equal 5, s.processed\n      end\n    end\n\n    describe \"failed\" do\n      it \"returns number of failed jobs\" do\n        Sidekiq.redis { |conn| conn.set(\"stat:failed\", 5) }\n        s = Sidekiq::Stats.new\n        assert_equal 5, s.failed\n      end\n    end\n\n    describe \"reset\" do\n      before do\n        Sidekiq.redis do |conn|\n          conn.set('stat:processed', 5)\n          conn.set('stat:failed', 10)\n        end\n      end\n\n      it 'will reset all stats by default' do\n        Sidekiq::Stats.new.reset\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.failed\n        assert_equal 0, s.processed\n      end\n\n      it 'can reset individual stats' do\n        Sidekiq::Stats.new.reset('failed')\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.failed\n        assert_equal 5, s.processed\n      end\n\n      it 'can accept anything that responds to #to_s' do\n        Sidekiq::Stats.new.reset(:failed)\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.failed\n        assert_equal 5, s.processed\n      end\n\n      it 'ignores anything other than \"failed\" or \"processed\"' do\n        Sidekiq::Stats.new.reset((1..10).to_a, ['failed'])\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.failed\n        assert_equal 5, s.processed\n      end\n    end\n\n    describe \"workers_size\" do\n      it 'retrieves the number of busy workers' do\n        Sidekiq.redis do |c|\n          c.sadd(\"processes\", \"process_1\")\n          c.sadd(\"processes\", \"process_2\")\n          c.hset(\"process_1\", \"busy\", 1)\n          c.hset(\"process_2\", \"busy\", 2)\n        end\n        s = Sidekiq::Stats.new\n        assert_equal 3, s.workers_size\n      end\n    end\n\n    describe \"queues\" do\n      it \"is initially empty\" do\n        s = Sidekiq::Stats::Queues.new\n        assert_equal 0, s.lengths.size\n      end\n\n      it \"returns a hash of queue and size in order\" do\n        Sidekiq.redis do |conn|\n          conn.rpush 'queue:foo', '{}'\n          conn.sadd 'queues', 'foo'\n\n          3.times { conn.rpush 'queue:bar', '{}' }\n          conn.sadd 'queues', 'bar'\n        end\n\n        s = Sidekiq::Stats::Queues.new\n        assert_equal ({ \"foo\" => 1, \"bar\" => 3 }), s.lengths\n        assert_equal \"bar\", s.lengths.first.first\n\n        assert_equal Sidekiq::Stats.new.queues, Sidekiq::Stats::Queues.new.lengths\n      end\n    end\n\n    describe \"enqueued\" do\n      it 'handles latency for good jobs' do\n        Sidekiq.redis do |conn|\n          conn.rpush 'queue:default', \"{\\\"enqueued_at\\\": #{Time.now.to_f}}\"\n          conn.sadd 'queues', 'default'\n        end\n        s = Sidekiq::Stats.new\n        assert s.default_queue_latency > 0\n        q = Sidekiq::Queue.new\n        assert q.latency > 0\n      end\n\n      it 'handles latency for incomplete jobs' do\n        Sidekiq.redis do |conn|\n          conn.rpush 'queue:default', '{}'\n          conn.sadd 'queues', 'default'\n        end\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.default_queue_latency\n        q = Sidekiq::Queue.new\n        assert_equal 0, q.latency\n      end\n\n      it \"returns total enqueued jobs\" do\n        Sidekiq.redis do |conn|\n          conn.rpush 'queue:foo', '{}'\n          conn.sadd 'queues', 'foo'\n\n          3.times { conn.rpush 'queue:bar', '{}' }\n          conn.sadd 'queues', 'bar'\n        end\n\n        s = Sidekiq::Stats.new\n        assert_equal 4, s.enqueued\n      end\n    end\n\n    describe \"over time\" do\n      before do\n        require 'active_support/core_ext/time/conversions'\n        @before = Time::DATE_FORMATS[:default]\n        Time::DATE_FORMATS[:default] = \"%d/%m/%Y %H:%M:%S\"\n      end\n\n      after do\n        Time::DATE_FORMATS[:default] = @before\n      end\n\n      describe \"processed\" do\n        it 'retrieves hash of dates' do\n          Sidekiq.redis do |c|\n            c.incrby(\"stat:processed:2012-12-24\", 4)\n            c.incrby(\"stat:processed:2012-12-25\", 1)\n            c.incrby(\"stat:processed:2012-12-26\", 6)\n            c.incrby(\"stat:processed:2012-12-27\", 2)\n          end\n          Time.stub(:now, Time.parse(\"2012-12-26 1:00:00 -0500\")) do\n            s = Sidekiq::Stats::History.new(2)\n            assert_equal({ \"2012-12-26\" => 6, \"2012-12-25\" => 1 }, s.processed)\n\n            s = Sidekiq::Stats::History.new(3)\n            assert_equal({ \"2012-12-26\" => 6, \"2012-12-25\" => 1, \"2012-12-24\" => 4 }, s.processed)\n\n            s = Sidekiq::Stats::History.new(2, Date.parse(\"2012-12-25\"))\n            assert_equal({ \"2012-12-25\" => 1, \"2012-12-24\" => 4 }, s.processed)\n          end\n        end\n      end\n\n      describe \"failed\" do\n        it 'retrieves hash of dates' do\n          Sidekiq.redis do |c|\n            c.incrby(\"stat:failed:2012-12-24\", 4)\n            c.incrby(\"stat:failed:2012-12-25\", 1)\n            c.incrby(\"stat:failed:2012-12-26\", 6)\n            c.incrby(\"stat:failed:2012-12-27\", 2)\n          end\n          Time.stub(:now, Time.parse(\"2012-12-26 1:00:00 -0500\")) do\n            s = Sidekiq::Stats::History.new(2)\n            assert_equal ({ \"2012-12-26\" => 6, \"2012-12-25\" => 1 }), s.failed\n\n            s = Sidekiq::Stats::History.new(3)\n            assert_equal ({ \"2012-12-26\" => 6, \"2012-12-25\" => 1, \"2012-12-24\" => 4 }), s.failed\n\n            s = Sidekiq::Stats::History.new(2, Date.parse(\"2012-12-25\"))\n            assert_equal ({ \"2012-12-25\" => 1, \"2012-12-24\" => 4 }), s.failed\n          end\n        end\n      end\n    end\n  end\n\n  describe 'with an empty database' do\n    it 'shows queue as empty' do\n      q = Sidekiq::Queue.new\n      assert_equal 0, q.size\n      assert_equal 0, q.latency\n    end\n\n    before do\n      ActiveJob::Base.queue_adapter = :sidekiq\n      ActiveJob::Base.logger = nil\n    end\n\n    class ApiMailer < ActionMailer::Base\n      def test_email(*)\n      end\n    end\n\n    class ApiJob < ActiveJob::Base\n      def perform(*)\n      end\n    end\n\n    class ApiWorker\n      include Sidekiq::Worker\n    end\n\n    class WorkerWithTags\n      include Sidekiq::Worker\n      sidekiq_options tags: ['foo']\n    end\n\n    it 'can enumerate jobs' do\n      q = Sidekiq::Queue.new\n      Time.stub(:now, Time.new(2012, 12, 26)) do\n        ApiWorker.perform_async(1, 'mike')\n        assert_equal [ApiWorker.name], q.map(&:klass)\n\n        job = q.first\n        assert_equal 24, job.jid.size\n        assert_equal [1, 'mike'], job.args\n        assert_equal Time.new(2012, 12, 26), job.enqueued_at\n      end\n      assert q.latency > 10_000_000\n\n      q = Sidekiq::Queue.new('other')\n      assert_equal 0, q.size\n    end\n\n    it 'enumerates jobs in descending score order' do\n      # We need to enqueue more than 50 items, which is the page size when retrieving\n      # from Redis to ensure everything is sorted: the pages and the items withing them.\n      51.times { ApiWorker.perform_in(100, 1, 'foo') }\n\n      set = Sidekiq::ScheduledSet.new.to_a\n\n      assert_equal set.sort_by { |job| -job.score }, set\n    end\n\n    it 'has no enqueued_at time for jobs enqueued in the future' do\n      job_id = ApiWorker.perform_in(100, 1, 'foo')\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      assert_nil job.enqueued_at\n    end\n\n    it 'unwraps delayed jobs' do\n      Sidekiq::Extensions.enable_delay!\n      Sidekiq::Queue.delay.foo(1,2,3)\n      q = Sidekiq::Queue.new\n      x = q.first\n      assert_equal \"Sidekiq::Queue.foo\", x.display_class\n      assert_equal [1,2,3], x.display_args\n    end\n\n    it 'handles previous (raw Array) error_backtrace format' do\n      add_retry\n      job = Sidekiq::RetrySet.new.first\n      assert_equal ['line1', 'line2'], job.error_backtrace\n    end\n\n    it 'handles previous (marshalled Array) error_backtrace format' do\n      backtrace = ['line1', 'line2']\n      serialized = Marshal.dump(backtrace)\n      compressed = Zlib::Deflate.deflate(serialized)\n      encoded = Base64.encode64(compressed)\n\n      payload = Sidekiq.dump_json('class' => 'ApiWorker', 'args' => [1], 'queue' => 'default', 'jid' => 'jid', 'error_backtrace' => encoded)\n      Sidekiq.redis do |conn|\n        conn.zadd('retry', Time.now.to_f.to_s, payload)\n      end\n\n      job = Sidekiq::RetrySet.new.first\n      assert_equal backtrace, job.error_backtrace\n    end\n\n    describe \"Rails unwrapping\" do\n      SERIALIZED_JOBS = {\n        \"5.x\" => [\n          '{\"class\":\"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\",\"wrapped\":\"ApiJob\",\"queue\":\"default\",\"args\":[{\"job_class\":\"ApiJob\",\"job_id\":\"f1bde53f-3852-4ae4-a879-c12eacebbbb0\",\"provider_job_id\":null,\"queue_name\":\"default\",\"priority\":null,\"arguments\":[1,2,3],\"executions\":0,\"locale\":\"en\"}],\"retry\":true,\"jid\":\"099eee72911085a511d0e312\",\"created_at\":1568305542.339916,\"enqueued_at\":1568305542.339947}',\n          '{\"class\":\"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\",\"wrapped\":\"ActionMailer::DeliveryJob\",\"queue\":\"mailers\",\"args\":[{\"job_class\":\"ActionMailer::DeliveryJob\",\"job_id\":\"19cc0115-3d1c-4bbe-a51e-bfa1385895d1\",\"provider_job_id\":null,\"queue_name\":\"mailers\",\"priority\":null,\"arguments\":[\"ApiMailer\",\"test_email\",\"deliver_now\",1,2,3],\"executions\":0,\"locale\":\"en\"}],\"retry\":true,\"jid\":\"37436e5504936400e8cf98db\",\"created_at\":1568305542.370133,\"enqueued_at\":1568305542.370241}',\n        ],\n        \"6.x\" => [\n          '{\"class\":\"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\",\"wrapped\":\"ApiJob\",\"queue\":\"default\",\"args\":[{\"job_class\":\"ApiJob\",\"job_id\":\"ff2b48d4-bdce-4825-af6b-ef8c11ab651e\",\"provider_job_id\":null,\"queue_name\":\"default\",\"priority\":null,\"arguments\":[1,2,3],\"executions\":0,\"exception_executions\":{},\"locale\":\"en\",\"timezone\":\"UTC\",\"enqueued_at\":\"2019-09-12T16:28:37Z\"}],\"retry\":true,\"jid\":\"ce121bf77b37ae81fe61b6dc\",\"created_at\":1568305717.9469702,\"enqueued_at\":1568305717.947005}',\n          '{\"class\":\"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\",\"wrapped\":\"ActionMailer::MailDeliveryJob\",\"queue\":\"mailers\",\"args\":[{\"job_class\":\"ActionMailer::MailDeliveryJob\",\"job_id\":\"2f967da1-a389-479c-9a4e-5cc059e6d65c\",\"provider_job_id\":null,\"queue_name\":\"mailers\",\"priority\":null,\"arguments\":[\"ApiMailer\",\"test_email\",\"deliver_now\",{\"args\":[1,2,3],\"_aj_symbol_keys\":[\"args\"]}],\"executions\":0,\"exception_executions\":{},\"locale\":\"en\",\"timezone\":\"UTC\",\"enqueued_at\":\"2019-09-12T16:28:37Z\"}],\"retry\":true,\"jid\":\"469979df52bb9ef9f48b49e1\",\"created_at\":1568305717.9457421,\"enqueued_at\":1568305717.9457731}',\n        ],\n      }.each_pair do |ver,jobs|\n        it \"unwraps ActiveJob #{ver} jobs\" do\n          #ApiJob.perform_later(1,2,3)\n          #puts Sidekiq::Queue.new.first.value\n          x = Sidekiq::JobRecord.new(jobs[0], \"default\")\n          assert_equal ApiJob.name, x.display_class\n          assert_equal [1,2,3], x.display_args\n        end\n\n        it \"unwraps ActionMailer #{ver} jobs\" do\n          #ApiMailer.test_email(1,2,3).deliver_later\n          #puts Sidekiq::Queue.new(\"mailers\").first.value\n          x = Sidekiq::JobRecord.new(jobs[1], \"mailers\")\n          assert_equal \"#{ApiMailer.name}#test_email\", x.display_class\n          assert_equal [1,2,3], x.display_args\n        end\n      end\n    end\n\n    it 'has no enqueued_at time for jobs enqueued in the future' do\n      job_id = ApiWorker.perform_in(100, 1, 'foo')\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      assert_nil job.enqueued_at\n    end\n\n    it 'returns tags field for jobs' do\n      job_id = ApiWorker.perform_async\n      assert_equal [], Sidekiq::Queue.new.find_job(job_id).tags\n\n      job_id = WorkerWithTags.perform_async\n      assert_equal ['foo'], Sidekiq::Queue.new.find_job(job_id).tags\n    end\n\n    it 'can delete jobs' do\n      q = Sidekiq::Queue.new\n      ApiWorker.perform_async(1, 'mike')\n      assert_equal 1, q.size\n\n      x = q.first\n      assert_equal ApiWorker.name, x.display_class\n      assert_equal [1,'mike'], x.display_args\n\n      assert_equal [true], q.map(&:delete)\n      assert_equal 0, q.size\n    end\n\n    it \"can move scheduled job to queue\" do\n      remain_id = ApiWorker.perform_in(100, 1, 'jason')\n      job_id = ApiWorker.perform_in(100, 1, 'jason')\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      q = Sidekiq::Queue.new\n      job.add_to_queue\n      queued_job = q.find_job(job_id)\n      refute_nil queued_job\n      assert_equal queued_job.jid, job_id\n      assert_nil Sidekiq::ScheduledSet.new.find_job(job_id)\n      refute_nil Sidekiq::ScheduledSet.new.find_job(remain_id)\n    end\n\n    it \"handles multiple scheduled jobs when moving to queue\" do\n      jids = Sidekiq::Client.push_bulk('class' => ApiWorker,\n                                       'args' => [[1, 'jason'], [2, 'jason']],\n                                       'at' => Time.now.to_f)\n      assert_equal 2, jids.size\n      (remain_id, job_id) = jids\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      q = Sidekiq::Queue.new\n      job.add_to_queue\n      queued_job = q.find_job(job_id)\n      refute_nil queued_job\n      assert_equal queued_job.jid, job_id\n      assert_nil Sidekiq::ScheduledSet.new.find_job(job_id)\n      refute_nil Sidekiq::ScheduledSet.new.find_job(remain_id)\n    end\n\n    it 'can kill a scheduled job' do\n      job_id = ApiWorker.perform_in(100, 1, '{\"foo\":123}')\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      ds = Sidekiq::DeadSet.new\n      assert_equal 0, ds.size\n      job.kill\n      assert_equal 1, ds.size\n    end\n\n    it 'can find a scheduled job by jid' do\n      10.times do |idx|\n        ApiWorker.perform_in(idx, 1)\n      end\n\n      job_id = ApiWorker.perform_in(5, 1)\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      assert_equal job_id, job.jid\n\n      ApiWorker.perform_in(100, 1, 'jid' => 'jid_in_args')\n      assert_nil Sidekiq::ScheduledSet.new.find_job('jid_in_args')\n    end\n\n    it 'can remove jobs when iterating over a sorted set' do\n      # scheduled jobs must be greater than SortedSet#each underlying page size\n      51.times do\n        ApiWorker.perform_in(100, 'aaron')\n      end\n      set = Sidekiq::ScheduledSet.new\n      set.map(&:delete)\n      assert_equal set.size, 0\n    end\n\n    it 'can remove jobs when iterating over a queue' do\n      # initial queue size must be greater than Queue#each underlying page size\n      51.times do\n        ApiWorker.perform_async(1, 'aaron')\n      end\n      q = Sidekiq::Queue.new\n      q.map(&:delete)\n      assert_equal q.size, 0\n    end\n\n    it 'can find job by id in queues' do\n      q = Sidekiq::Queue.new\n      job_id = ApiWorker.perform_async(1, 'jason')\n      job = q.find_job(job_id)\n      refute_nil job\n      assert_equal job_id, job.jid\n    end\n\n    it 'can clear a queue' do\n      q = Sidekiq::Queue.new\n      2.times { ApiWorker.perform_async(1, 'mike') }\n      q.clear\n\n      Sidekiq.redis do |conn|\n        refute conn.smembers('queues').include?('foo')\n        refute conn.exists?('queue:foo')\n      end\n    end\n\n    it 'can fetch by score' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time)\n      r = Sidekiq::RetrySet.new\n      assert_equal 2, r.fetch(same_time).size\n    end\n\n    it 'can fetch by score and jid' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time)\n      r = Sidekiq::RetrySet.new\n      assert_equal 1, r.fetch(same_time, 'bob1').size\n    end\n\n    it 'can fetch by score range' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time + 1)\n      add_retry('bob3', same_time + 2)\n      r = Sidekiq::RetrySet.new\n      range = (same_time..(same_time + 1))\n      assert_equal 2, r.fetch(range).size\n    end\n\n    it 'can fetch by score range and jid' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time + 1)\n      add_retry('bob3', same_time + 2)\n      r = Sidekiq::RetrySet.new\n      range = (same_time..(same_time + 1))\n      jobs = r.fetch(range, 'bob2')\n      assert_equal 1, jobs.size\n      assert_equal jobs[0].jid, 'bob2'\n    end\n\n    it 'shows empty retries' do\n      r = Sidekiq::RetrySet.new\n      assert_equal 0, r.size\n    end\n\n    it 'can enumerate retries' do\n      add_retry\n\n      r = Sidekiq::RetrySet.new\n      assert_equal 1, r.size\n      array = r.to_a\n      assert_equal 1, array.size\n\n      retri = array.first\n      assert_equal 'ApiWorker', retri.klass\n      assert_equal 'default', retri.queue\n      assert_equal 'bob', retri.jid\n      assert_in_delta Time.now.to_f, retri.at.to_f, 0.02\n    end\n\n    it 'requires a jid to delete an entry' do\n      start_time = Time.now.to_f\n      add_retry('bob2', Time.now.to_f)\n      assert_raises(ArgumentError) do\n        Sidekiq::RetrySet.new.delete(start_time)\n      end\n    end\n\n    it 'can delete a single retry from score and jid' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time)\n      r = Sidekiq::RetrySet.new\n      assert_equal 2, r.size\n      Sidekiq::RetrySet.new.delete(same_time, 'bob1')\n      assert_equal 1, r.size\n    end\n\n    it 'can retry a retry' do\n      add_retry\n      r = Sidekiq::RetrySet.new\n      assert_equal 1, r.size\n      r.first.retry\n      assert_equal 0, r.size\n      assert_equal 1, Sidekiq::Queue.new('default').size\n      job = Sidekiq::Queue.new('default').first\n      assert_equal 'bob', job.jid\n      assert_equal 1, job['retry_count']\n    end\n\n    it 'can clear retries' do\n      add_retry\n      add_retry('test')\n      r = Sidekiq::RetrySet.new\n      assert_equal 2, r.size\n      r.clear\n      assert_equal 0, r.size\n    end\n\n    it 'can scan retries' do\n      add_retry\n      add_retry('test')\n      r = Sidekiq::RetrySet.new\n      assert_instance_of Enumerator, r.scan('Worker')\n      assert_equal 2, r.scan('ApiWorker').to_a.size\n      assert_equal 1, r.scan('*test*').to_a.size\n    end\n\n    it 'can enumerate processes' do\n      identity_string = \"identity_string\"\n      odata = {\n        'pid' => 123,\n        'hostname' => Socket.gethostname,\n        'key' => identity_string,\n        'identity' => identity_string,\n        'started_at' => Time.now.to_f - 15,\n        'queues' => ['foo', 'bar']\n      }\n\n      time = Time.now.to_f\n      Sidekiq.redis do |conn|\n        conn.multi do\n          conn.sadd('processes', odata['key'])\n          conn.hmset(odata['key'], 'info', Sidekiq.dump_json(odata), 'busy', 10, 'beat', time)\n          conn.sadd('processes', 'fake:pid')\n        end\n      end\n\n      ps = Sidekiq::ProcessSet.new.to_a\n      assert_equal 1, ps.size\n      data = ps.first\n      assert_equal 10, data['busy']\n      assert_equal time, data['beat']\n      assert_equal 123, data['pid']\n      assert_equal ['foo', 'bar'], data.queues\n      data.quiet!\n      data.stop!\n      signals_string = \"#{odata['key']}-signals\"\n      assert_equal \"TERM\", Sidekiq.redis{|c| c.lpop(signals_string) }\n      assert_equal \"TSTP\", Sidekiq.redis{|c| c.lpop(signals_string) }\n    end\n\n    it 'can enumerate workers' do\n      w = Sidekiq::Workers.new\n      assert_equal 0, w.size\n      w.each do\n        assert false\n      end\n\n      hn = Socket.gethostname\n      key = \"#{hn}:#{$$}\"\n      pdata = { 'pid' => $$, 'hostname' => hn, 'started_at' => Time.now.to_i }\n      Sidekiq.redis do |conn|\n        conn.sadd('processes', key)\n        conn.hmset(key, 'info', Sidekiq.dump_json(pdata), 'busy', 0, 'beat', Time.now.to_f)\n      end\n\n      s = \"#{key}:workers\"\n      data = Sidekiq.dump_json({ 'payload' => \"{}\", 'queue' => 'default', 'run_at' => Time.now.to_i })\n      Sidekiq.redis do |c|\n        c.hmset(s, '1234', data)\n      end\n\n      w.each do |p, x, y|\n        assert_equal key, p\n        assert_equal \"1234\", x\n        assert_equal 'default', y['queue']\n        assert_equal({}, y['payload'])\n        assert_equal Time.now.year, Time.at(y['run_at']).year\n      end\n\n      s = \"#{key}:workers\"\n      data = Sidekiq.dump_json({ 'payload' => {}, 'queue' => 'default', 'run_at' => (Time.now.to_i - 2*60*60) })\n      Sidekiq.redis do |c|\n        c.multi do\n          c.hmset(s, '5678', data)\n          c.hmset(\"b#{s}\", '5678', data)\n        end\n      end\n\n      assert_equal ['5678', '1234'], w.map { |_, tid, _| tid }\n    end\n\n    it 'can reschedule jobs' do\n      add_retry('foo1')\n      add_retry('foo2')\n\n      retries = Sidekiq::RetrySet.new\n      assert_equal 2, retries.size\n      refute(retries.map { |r| r.score > (Time.now.to_f + 9) }.any?)\n\n      retries.each do |retri|\n        retri.reschedule(Time.now + 15) if retri.jid == 'foo1'\n        retri.reschedule(Time.now.to_f + 10) if retri.jid == 'foo2'\n      end\n\n      assert_equal 2, retries.size\n      assert(retries.map { |r| r.score > (Time.now.to_f + 9) }.any?)\n      assert(retries.map { |r| r.score > (Time.now.to_f + 14) }.any?)\n    end\n\n    it 'prunes processes which have died' do\n      data = { 'pid' => rand(10_000), 'hostname' => \"app#{rand(1_000)}\", 'started_at' => Time.now.to_f }\n      key = \"#{data['hostname']}:#{data['pid']}\"\n      Sidekiq.redis do |conn|\n        conn.sadd('processes', key)\n        conn.hmset(key, 'info', Sidekiq.dump_json(data), 'busy', 0, 'beat', Time.now.to_f)\n      end\n\n      ps = Sidekiq::ProcessSet.new\n      assert_equal 1, ps.size\n      assert_equal 1, ps.to_a.size\n\n      Sidekiq.redis do |conn|\n        conn.sadd('processes', \"bar:987\")\n        conn.sadd('processes', \"bar:986\")\n      end\n\n      ps = Sidekiq::ProcessSet.new\n      assert_equal 1, ps.size\n      assert_equal 1, ps.to_a.size\n    end\n\n    def add_retry(jid = 'bob', at = Time.now.to_f)\n      payload = Sidekiq.dump_json('class' => 'ApiWorker', 'args' => [1, 'mike'], 'queue' => 'default', 'jid' => jid, 'retry_count' => 2, 'failed_at' => Time.now.to_f, 'error_backtrace' => ['line1', 'line2'])\n      Sidekiq.redis do |conn|\n        conn.zadd('retry', at.to_s, payload)\n      end\n    end\n  end\nend\n", "# encoding: utf-8\n# frozen_string_literal: true\nrequire_relative 'helper'\nrequire 'sidekiq/web'\nrequire 'sidekiq/util'\nrequire 'rack/test'\n\ndescribe Sidekiq::Web do\n  include Rack::Test::Methods\n\n  def app\n    @app ||= Sidekiq::Web.new\n  end\n\n  def job_params(job, score)\n    \"#{score}-#{job['jid']}\"\n  end\n\n  before do\n    Sidekiq.redis {|c| c.flushdb }\n    app.middlewares.clear\n  end\n\n  class WebWorker\n    include Sidekiq::Worker\n\n    def perform(a, b)\n      a + b\n    end\n  end\n\n  it 'can show text with any locales' do\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'ru,en'}\n    get '/', {}, rackenv\n    assert_match(/\u041f\u0430\u043d\u0435\u043b\u044c \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'es,en'}\n    get '/', {}, rackenv\n    assert_match(/Panel de Control/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'en-us'}\n    get '/', {}, rackenv\n    assert_match(/Dashboard/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'zh-cn'}\n    get '/', {}, rackenv\n    assert_match(/\u4fe1\u606f\u677f/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'zh-tw'}\n    get '/', {}, rackenv\n    assert_match(/\u8cc7\u8a0a\u4e3b\u9801/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'nb'}\n    get '/', {}, rackenv\n    assert_match(/Oversikt/, last_response.body)\n  end\n\n  it 'can provide a default, appropriate CSP for its content' do\n    get '/', {}\n    policies = last_response.headers[\"Content-Security-Policy\"].split('; ')\n    assert_includes(policies, \"connect-src 'self' https: http: wss: ws:\")\n    assert_includes(policies, \"style-src 'self' https: http: 'unsafe-inline'\")\n    assert_includes(policies, \"script-src 'self' https: http: 'unsafe-inline'\")\n    assert_includes(policies, \"object-src 'none'\")\n  end\n\n  describe 'busy' do\n\n    it 'can display workers' do\n      Sidekiq.redis do |conn|\n        conn.incr('busy')\n        conn.sadd('processes', 'foo:1234')\n        conn.hmset('foo:1234', 'info', Sidekiq.dump_json('hostname' => 'foo', 'started_at' => Time.now.to_f, \"queues\" => [], 'concurrency' => 10), 'at', Time.now.to_f, 'busy', 4)\n        identity = 'foo:1234:workers'\n        hash = {:queue => 'critical', :payload => { 'class' => WebWorker.name, 'args' => [1,'abc'] }, :run_at => Time.now.to_i }\n        conn.hmset(identity, 1001, Sidekiq.dump_json(hash))\n      end\n      assert_equal ['1001'], Sidekiq::Workers.new.map { |pid, tid, data| tid }\n\n      get '/busy'\n      assert_equal 200, last_response.status\n      assert_match(/status-active/, last_response.body)\n      assert_match(/critical/, last_response.body)\n      assert_match(/WebWorker/, last_response.body)\n    end\n\n    it 'can quiet a process' do\n      identity = 'identity'\n      signals_key = \"#{identity}-signals\"\n\n      assert_nil Sidekiq.redis { |c| c.lpop signals_key }\n      post '/busy', 'quiet' => '1', 'identity' => identity\n      assert_equal 302, last_response.status\n      assert_equal 'TSTP', Sidekiq.redis { |c| c.lpop signals_key }\n    end\n\n    it 'can stop a process' do\n      identity = 'identity'\n      signals_key = \"#{identity}-signals\"\n\n      assert_nil Sidekiq.redis { |c| c.lpop signals_key }\n      post '/busy', 'stop' => '1', 'identity' => identity\n      assert_equal 302, last_response.status\n      assert_equal 'TERM', Sidekiq.redis { |c| c.lpop signals_key }\n    end\n  end\n\n  it 'can display queues' do\n    assert Sidekiq::Client.push('queue' => :foo, 'class' => WebWorker, 'args' => [1, 3])\n\n    get '/queues'\n    assert_equal 200, last_response.status\n    assert_match(/foo/, last_response.body)\n    refute_match(/HardWorker/, last_response.body)\n    assert_match(/0.0/, last_response.body)\n    refute_match(/datetime/, last_response.body)\n    Sidekiq::Queue.new(\"foo\").clear\n\n    Time.stub(:now, Time.now - 65) do\n      assert Sidekiq::Client.push('queue' => :foo, 'class' => WebWorker, 'args' => [1, 3])\n    end\n\n    get '/queues'\n    assert_equal 200, last_response.status\n    assert_match(/foo/, last_response.body)\n    refute_match(/HardWorker/, last_response.body)\n    assert_match(/65.0/, last_response.body)\n    assert_match(/datetime/, last_response.body)\n  end\n\n  it 'handles queue view' do\n    get '/queues/onmouseover=alert()'\n    assert_equal 404, last_response.status\n\n    get '/queues/foo_bar:123-wow.'\n    assert_equal 200, last_response.status\n    assert_match(/foo_bar:123-wow\\./, last_response.body)\n\n    get '/queues/default'\n    assert_equal 200, last_response.status\n  end\n\n  it 'can sort on enqueued_at column' do\n    Sidekiq.redis do |conn|\n      (1000..1005).each do |i|\n        conn.lpush('queue:default', Sidekiq.dump_json(args: [i], enqueued_at: Time.now.to_i + i))\n      end\n    end\n\n    get '/queues/default?count=3' # direction is 'desc' by default\n    assert_match(/1005/, last_response.body)\n    refute_match(/1002/, last_response.body)\n\n    get '/queues/default?count=3&direction=asc'\n    assert_match(/1000/, last_response.body)\n    refute_match(/1003/, last_response.body)\n  end\n\n  it 'can delete a queue' do\n    Sidekiq.redis do |conn|\n      conn.rpush('queue:foo', \"{\\\"args\\\":[],\\\"enqueued_at\\\":1567894960}\")\n      conn.sadd('queues', 'foo')\n    end\n\n    get '/queues/foo'\n    assert_equal 200, last_response.status\n\n    post '/queues/foo'\n    assert_equal 302, last_response.status\n\n    Sidekiq.redis do |conn|\n      refute conn.smembers('queues').include?('foo')\n      refute conn.exists?('queue:foo')\n    end\n  end\n\n  it 'can attempt to pause a queue' do\n    Sidekiq.stub(:pro?, true) do\n      mock = Minitest::Mock.new\n      mock.expect :pause!, true\n\n      stub = lambda do |queue_name|\n        assert_equal 'foo', queue_name\n        mock\n      end\n\n      Sidekiq::Queue.stub :new, stub do\n        post '/queues/foo', 'pause' => 'pause'\n        assert_equal 302, last_response.status\n      end\n\n      assert_mock mock\n    end\n  end\n\n  it 'can attempt to unpause a queue' do\n    Sidekiq.stub(:pro?, true) do\n      mock = Minitest::Mock.new\n      mock.expect :unpause!, true\n\n      stub = lambda do |queue_name|\n        assert_equal 'foo', queue_name\n        mock\n      end\n\n      Sidekiq::Queue.stub :new, stub do\n        post '/queues/foo', 'unpause' => 'unpause'\n        assert_equal 302, last_response.status\n      end\n\n      assert_mock mock\n    end\n  end\n\n  it 'ignores to attempt to pause a queue with pro disabled' do\n    mock = Minitest::Mock.new\n    mock.expect :clear, true\n\n    stub = lambda do |queue_name|\n      assert_equal 'foo', queue_name\n      mock\n    end\n\n    Sidekiq::Queue.stub :new, stub do\n      post '/queues/foo', 'pause' => 'pause'\n      assert_equal 302, last_response.status\n    end\n\n    assert_mock mock\n  end\n\n  it 'ignores to attempt to unpause a queue with pro disabled' do\n    mock = Minitest::Mock.new\n    mock.expect :clear, true\n\n    stub = lambda do |queue_name|\n      assert_equal 'foo', queue_name\n      mock\n    end\n\n    Sidekiq::Queue.stub :new, stub do\n      post '/queues/foo', 'unpause' => 'unpause'\n      assert_equal 302, last_response.status\n    end\n\n    assert_mock mock\n  end\n\n  it 'can delete a job' do\n    Sidekiq.redis do |conn|\n      conn.rpush('queue:foo', '{\"args\":[],\"enqueued_at\":1567894960}')\n      conn.rpush('queue:foo', '{\"foo\":\"bar\",\"args\":[],\"enqueued_at\":1567894960}')\n      conn.rpush('queue:foo', '{\"foo2\":\"bar2\",\"args\":[],\"enqueued_at\":1567894960}')\n    end\n\n    get '/queues/foo'\n    assert_equal 200, last_response.status\n\n    post '/queues/foo/delete', key_val: \"{\\\"foo\\\":\\\"bar\\\"}\"\n    assert_equal 302, last_response.status\n\n    Sidekiq.redis do |conn|\n      refute conn.lrange('queue:foo', 0, -1).include?(\"{\\\"foo\\\":\\\"bar\\\"}\")\n    end\n  end\n\n  it 'can display retries' do\n    get '/retries'\n    assert_equal 200, last_response.status\n    assert_match(/found/, last_response.body)\n    refute_match(/HardWorker/, last_response.body)\n\n    add_retry\n\n    get '/retries'\n    assert_equal 200, last_response.status\n    refute_match(/found/, last_response.body)\n    assert_match(/HardWorker/, last_response.body)\n  end\n\n  it 'can display a single retry' do\n    params = add_retry\n    get '/retries/0-shouldntexist'\n    assert_equal 302, last_response.status\n    get \"/retries/#{job_params(*params)}\"\n    assert_equal 200, last_response.status\n    assert_match(/HardWorker/, last_response.body)\n  end\n\n  it 'handles missing retry' do\n    get \"/retries/0-shouldntexist\"\n    assert_equal 302, last_response.status\n  end\n\n  it 'can delete a single retry' do\n    params = add_retry\n    post \"/retries/#{job_params(*params)}\", 'delete' => 'Delete'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n\n    get \"/retries\"\n    assert_equal 200, last_response.status\n    refute_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can delete all retries' do\n    3.times { add_retry }\n\n    post \"/retries/all/delete\", 'delete' => 'Delete'\n    assert_equal 0, Sidekiq::RetrySet.new.size\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n  end\n\n  it 'can retry a single retry now' do\n    params = add_retry\n    post \"/retries/#{job_params(*params)}\", 'retry' => 'Retry'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n\n    get '/queues/default'\n    assert_equal 200, last_response.status\n    assert_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can kill a single retry now' do\n    params = add_retry\n    post \"/retries/#{job_params(*params)}\", 'kill' => 'Kill'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n\n    get '/morgue'\n    assert_equal 200, last_response.status\n    assert_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can display scheduled' do\n    get '/scheduled'\n    assert_equal 200, last_response.status\n    assert_match(/found/, last_response.body)\n    refute_match(/HardWorker/, last_response.body)\n\n    add_scheduled\n\n    get '/scheduled'\n    assert_equal 200, last_response.status\n    refute_match(/found/, last_response.body)\n    assert_match(/HardWorker/, last_response.body)\n  end\n\n  it 'can display a single scheduled job' do\n    params = add_scheduled\n    get '/scheduled/0-shouldntexist'\n    assert_equal 302, last_response.status\n    get \"/scheduled/#{job_params(*params)}\"\n    assert_equal 200, last_response.status\n    assert_match(/HardWorker/, last_response.body)\n  end\n\n  it 'can display a single scheduled job tags' do\n    params = add_scheduled\n    get \"/scheduled/#{job_params(*params)}\"\n    assert_match(/tag1/, last_response.body)\n    assert_match(/tag2/, last_response.body)\n  end\n\n  it 'handles missing scheduled job' do\n    get \"/scheduled/0-shouldntexist\"\n    assert_equal 302, last_response.status\n  end\n\n  it 'can add to queue a single scheduled job' do\n    params = add_scheduled\n    post \"/scheduled/#{job_params(*params)}\", 'add_to_queue' => true\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/scheduled', last_response.header['Location']\n\n    get '/queues/default'\n    assert_equal 200, last_response.status\n    assert_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can delete a single scheduled job' do\n    params = add_scheduled\n    post \"/scheduled/#{job_params(*params)}\", 'delete' => 'Delete'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/scheduled', last_response.header['Location']\n\n    get \"/scheduled\"\n    assert_equal 200, last_response.status\n    refute_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can delete scheduled' do\n    params = add_scheduled\n    Sidekiq.redis do |conn|\n      assert_equal 1, conn.zcard('schedule')\n      post '/scheduled', 'key' => [job_params(*params)], 'delete' => 'Delete'\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/scheduled', last_response.header['Location']\n      assert_equal 0, conn.zcard('schedule')\n    end\n  end\n\n  it \"can move scheduled to default queue\" do\n    q = Sidekiq::Queue.new\n    params = add_scheduled\n    Sidekiq.redis do |conn|\n      assert_equal 1, conn.zcard('schedule')\n      assert_equal 0, q.size\n      post '/scheduled', 'key' => [job_params(*params)], 'add_to_queue' => 'AddToQueue'\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/scheduled', last_response.header['Location']\n      assert_equal 0, conn.zcard('schedule')\n      assert_equal 1, q.size\n      get '/queues/default'\n      assert_equal 200, last_response.status\n      assert_match(/#{params[0]['args'][2]}/, last_response.body)\n    end\n  end\n\n  it 'can retry all retries' do\n    msg = add_retry.first\n    add_retry\n\n    post \"/retries/all/retry\", 'retry' => 'Retry'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n    assert_equal 2, Sidekiq::Queue.new(\"default\").size\n\n    get '/queues/default'\n    assert_equal 200, last_response.status\n    assert_match(/#{msg['args'][2]}/, last_response.body)\n  end\n\n  it 'escape job args and error messages' do\n    # on /retries page\n    params = add_xss_retry\n    get '/retries'\n    assert_equal 200, last_response.status\n    assert_match(/FailWorker/, last_response.body)\n\n    assert last_response.body.include?( \"fail message: &lt;a&gt;hello&lt;&#x2F;a&gt;\" )\n    assert !last_response.body.include?( \"fail message: <a>hello</a>\" )\n\n    assert last_response.body.include?( \"args\\\">&quot;&lt;a&gt;hello&lt;&#x2F;a&gt;&quot;<\" )\n    assert !last_response.body.include?( \"args\\\"><a>hello</a><\" )\n\n    # on /workers page\n    Sidekiq.redis do |conn|\n      pro = 'foo:1234'\n      conn.sadd('processes', pro)\n      conn.hmset(pro, 'info', Sidekiq.dump_json('started_at' => Time.now.to_f, 'labels' => ['frumduz'], 'queues' =>[], 'concurrency' => 10), 'busy', 1, 'beat', Time.now.to_f)\n      identity = \"#{pro}:workers\"\n      hash = {:queue => 'critical', :payload => { 'class' => \"FailWorker\", 'args' => [\"<a>hello</a>\"] }, :run_at => Time.now.to_i }\n      conn.hmset(identity, 100001, Sidekiq.dump_json(hash))\n      conn.incr('busy')\n    end\n\n    get '/busy'\n    assert_equal 200, last_response.status\n    assert_match(/FailWorker/, last_response.body)\n    assert_match(/frumduz/, last_response.body)\n    assert last_response.body.include?( \"&lt;a&gt;hello&lt;&#x2F;a&gt;\" )\n    assert !last_response.body.include?( \"<a>hello</a>\" )\n\n    # on /queues page\n    params = add_xss_retry # sorry, don't know how to easily make this show up on queues page otherwise.\n    post \"/retries/#{job_params(*params)}\", 'retry' => 'Retry'\n    assert_equal 302, last_response.status\n\n    get '/queues/foo'\n    assert_equal 200, last_response.status\n    assert last_response.body.include?( \"&lt;a&gt;hello&lt;&#x2F;a&gt;\" )\n    assert !last_response.body.include?( \"<a>hello</a>\" )\n  end\n\n  it 'can show user defined tab' do\n    begin\n      Sidekiq::Web.tabs['Custom Tab'] = '/custom'\n\n      get '/'\n      assert_match 'Custom Tab', last_response.body\n\n    ensure\n      Sidekiq::Web.tabs.delete 'Custom Tab'\n    end\n  end\n\n  it 'can display home' do\n    get '/'\n    assert_equal 200, last_response.status\n  end\n\n  describe 'custom locales' do\n    before do\n      Sidekiq::Web.settings.locales << File.join(File.dirname(__FILE__), \"fixtures\")\n      Sidekiq::Web.tabs['Custom Tab'] = '/custom'\n      Sidekiq::WebApplication.get('/custom') do\n        clear_caches # ugly hack since I can't figure out how to access WebHelpers outside of this context\n        t('translated_text')\n      end\n    end\n\n    after do\n      Sidekiq::Web.tabs.delete 'Custom Tab'\n      Sidekiq::Web.settings.locales.pop\n    end\n\n    it 'can show user defined tab with custom locales' do\n      get '/custom'\n\n      assert_match(/Changed text/, last_response.body)\n    end\n  end\n\n  describe 'dashboard/stats' do\n    it 'redirects to stats' do\n      get '/dashboard/stats'\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/stats', last_response.header['Location']\n    end\n  end\n\n  describe 'stats' do\n    include Sidekiq::Util\n\n    before do\n      Sidekiq.redis do |conn|\n        conn.set(\"stat:processed\", 5)\n        conn.set(\"stat:failed\", 2)\n        conn.sadd(\"queues\", \"default\")\n      end\n      2.times { add_retry }\n      3.times { add_scheduled }\n      4.times { add_worker }\n    end\n\n    it 'works' do\n      get '/stats'\n      @response = Sidekiq.load_json(last_response.body)\n\n      assert_equal 200, last_response.status\n      assert_includes @response.keys, \"sidekiq\"\n      assert_equal 5, @response[\"sidekiq\"][\"processed\"]\n      assert_equal 2, @response[\"sidekiq\"][\"failed\"]\n      assert_equal 4, @response[\"sidekiq\"][\"busy\"]\n      assert_equal 1, @response[\"sidekiq\"][\"processes\"]\n      assert_equal 2, @response[\"sidekiq\"][\"retries\"]\n      assert_equal 3, @response[\"sidekiq\"][\"scheduled\"]\n      assert_equal 0, @response[\"sidekiq\"][\"default_latency\"]\n      assert_includes @response.keys, \"redis\"\n      assert_includes @response[\"redis\"].keys, \"redis_version\"\n      assert_includes @response[\"redis\"].keys, \"uptime_in_days\"\n      assert_includes @response[\"redis\"].keys, \"connected_clients\"\n      assert_includes @response[\"redis\"].keys, \"used_memory_human\"\n      assert_includes @response[\"redis\"].keys, \"used_memory_peak_human\"\n      assert_includes @response.keys, \"server_utc_time\"\n    end\n  end\n\n  describe 'bad JSON' do\n    it 'displays without error' do\n      s = Sidekiq::DeadSet.new\n      (_, score) = kill_bad\n      assert_equal 1, s.size\n\n      get '/morgue'\n      assert_equal 200, last_response.status\n      assert_match(/#{score.to_i}/, last_response.body)\n      assert_match(\"something bad\", last_response.body)\n      assert_equal 1, s.size\n\n      post \"/morgue/#{score}-\", 'delete' => 'Delete'\n      assert_equal 302, last_response.status\n      assert_equal 1, s.size\n    end\n  end\n\n  describe 'stats/queues' do\n    include Sidekiq::Util\n\n    before do\n      Sidekiq.redis do |conn|\n        conn.set(\"stat:processed\", 5)\n        conn.set(\"stat:failed\", 2)\n        conn.sadd(\"queues\", \"default\")\n        conn.sadd(\"queues\", \"queue2\")\n      end\n      2.times { add_retry }\n      3.times { add_scheduled }\n      4.times { add_worker }\n\n      get '/stats/queues'\n      @response = Sidekiq.load_json(last_response.body)\n    end\n\n    it 'reports the queue depth' do\n      assert_equal 0, @response[\"default\"]\n      assert_equal 0, @response[\"queue2\"]\n    end\n  end\n\n  describe 'dead jobs' do\n    it 'shows empty index' do\n      get 'morgue'\n      assert_equal 200, last_response.status\n    end\n\n    it 'shows index with jobs' do\n      (_, score) = add_dead\n      get 'morgue'\n      assert_equal 200, last_response.status\n      assert_match(/#{score}/, last_response.body)\n    end\n\n    it 'can delete all dead' do\n      3.times { add_dead }\n\n      assert_equal 3, Sidekiq::DeadSet.new.size\n      post \"/morgue/all/delete\", 'delete' => 'Delete'\n      assert_equal 0, Sidekiq::DeadSet.new.size\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/morgue', last_response.header['Location']\n    end\n\n    it 'can display a dead job' do\n      params = add_dead\n      get \"/morgue/#{job_params(*params)}\"\n      assert_equal 200, last_response.status\n    end\n\n    it 'can retry a dead job' do\n      params = add_dead\n      post \"/morgue/#{job_params(*params)}\", 'retry' => 'Retry'\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/morgue', last_response.header['Location']\n      assert_equal 0, Sidekiq::DeadSet.new.size\n\n      params = add_dead('jid-with-hyphen')\n      post \"/morgue/#{job_params(*params)}\", 'retry' => 'Retry'\n      assert_equal 302, last_response.status\n      assert_equal 0, Sidekiq::DeadSet.new.size\n\n      get '/queues/foo'\n      assert_equal 200, last_response.status\n      assert_match(/#{params.first['args'][2]}/, last_response.body)\n    end\n  end\n\n  def add_scheduled\n    score = Time.now.to_f\n    msg = { 'class' => 'HardWorker',\n            'args' => ['bob', 1, Time.now.to_f],\n            'jid' => SecureRandom.hex(12),\n            'tags' => ['tag1', 'tag2'], }\n    Sidekiq.redis do |conn|\n      conn.zadd('schedule', score, Sidekiq.dump_json(msg))\n    end\n    [msg, score]\n  end\n\n  def add_retry\n    msg = { 'class' => 'HardWorker',\n            'args' => ['bob', 1, Time.now.to_f],\n            'queue' => 'default',\n            'error_message' => 'Some fake message',\n            'error_class' => 'RuntimeError',\n            'retry_count' => 0,\n            'failed_at' => Time.now.to_f,\n            'jid' => SecureRandom.hex(12) }\n    score = Time.now.to_f\n    Sidekiq.redis do |conn|\n      conn.zadd('retry', score, Sidekiq.dump_json(msg))\n    end\n\n    [msg, score]\n  end\n\n  def add_dead(jid = SecureRandom.hex(12))\n    msg = { 'class' => 'HardWorker',\n            'args' => ['bob', 1, Time.now.to_f],\n            'queue' => 'foo',\n            'error_message' => 'Some fake message',\n            'error_class' => 'RuntimeError',\n            'retry_count' => 0,\n            'failed_at' => Time.now.utc,\n            'jid' => jid }\n    score = Time.now.to_f\n    Sidekiq.redis do |conn|\n      conn.zadd('dead', score, Sidekiq.dump_json(msg))\n    end\n    [msg, score]\n  end\n\n  def kill_bad\n    job = \"{ something bad }\"\n    score = Time.now.to_f\n    Sidekiq.redis do |conn|\n      conn.zadd('dead', score, job)\n    end\n    [job, score]\n  end\n\n  def add_xss_retry(job_id=SecureRandom.hex(12))\n    msg = { 'class' => 'FailWorker',\n            'args' => ['<a>hello</a>'],\n            'queue' => 'foo',\n            'error_message' => 'fail message: <a>hello</a>',\n            'error_class' => 'RuntimeError',\n            'retry_count' => 0,\n            'failed_at' => Time.now.to_f,\n            'jid' => SecureRandom.hex(12) }\n    score = Time.now.to_f\n    Sidekiq.redis do |conn|\n      conn.zadd('retry', score, Sidekiq.dump_json(msg))\n    end\n\n    [msg, score]\n  end\n\n  def add_worker\n    key = \"#{hostname}:#{$$}\"\n    msg = \"{\\\"queue\\\":\\\"default\\\",\\\"payload\\\":{\\\"retry\\\":true,\\\"queue\\\":\\\"default\\\",\\\"timeout\\\":20,\\\"backtrace\\\":5,\\\"class\\\":\\\"HardWorker\\\",\\\"args\\\":[\\\"bob\\\",10,5],\\\"jid\\\":\\\"2b5ad2b016f5e063a1c62872\\\"},\\\"run_at\\\":1361208995}\"\n    Sidekiq.redis do |conn|\n      conn.multi do\n        conn.sadd(\"processes\", key)\n        conn.hmset(key, 'info', Sidekiq.dump_json('hostname' => 'foo', 'started_at' => Time.now.to_f, \"queues\" => []), 'at', Time.now.to_f, 'busy', 4)\n        conn.hmset(\"#{key}:workers\", Time.now.to_f, msg)\n      end\n    end\n  end\n\n  describe 'basic auth' do\n    include Rack::Test::Methods\n\n    def app\n      app = Sidekiq::Web.new\n      app.use(Rack::Auth::Basic) { |user, pass| user == \"a\" && pass == \"b\" }\n      app.use(Rack::Session::Cookie, secret: SecureRandom.hex(32))\n\n      app\n    end\n\n    it 'requires basic authentication' do\n      get '/'\n\n      assert_equal 401, last_response.status\n      refute_nil last_response.header[\"WWW-Authenticate\"]\n    end\n\n    it 'authenticates successfuly' do\n      basic_authorize 'a', 'b'\n\n      get '/'\n\n      assert_equal 200, last_response.status\n    end\n  end\n\n  describe 'custom session' do\n    include Rack::Test::Methods\n\n    def app\n      app = Sidekiq::Web.new\n      app.use Rack::Session::Cookie, secret: 'v3rys3cr31', host: 'nicehost.org'\n      app\n    end\n\n    it 'requires uses session options' do\n      get '/'\n\n      session_options = last_request.env['rack.session'].options\n\n      assert_equal 'v3rys3cr31', session_options[:secret]\n      assert_equal 'nicehost.org', session_options[:host]\n    end\n  end\n\n  describe \"redirecting in before\" do\n    include Rack::Test::Methods\n\n    before do\n      Sidekiq::WebApplication.before { Thread.current[:some_setting] = :before }\n      Sidekiq::WebApplication.before { redirect '/' }\n      Sidekiq::WebApplication.after { Thread.current[:some_setting] = :after }\n    end\n\n    after do\n      Sidekiq::WebApplication.remove_instance_variable(:@befores)\n      Sidekiq::WebApplication.remove_instance_variable(:@afters)\n    end\n\n    def app\n      app = Sidekiq::Web.new\n      app.use Rack::Session::Cookie, secret: 'v3rys3cr31', host: 'nicehost.org'\n      app\n    end\n\n    it \"allows afters to run\" do\n      get '/'\n      assert_equal :after, Thread.current[:some_setting]\n    end\n  end\nend\n"], "fixing_code": ["# frozen_string_literal: true\n\nrequire \"sidekiq\"\n\nrequire \"zlib\"\nrequire \"base64\"\n\nmodule Sidekiq\n  class Stats\n    def initialize\n      fetch_stats_fast!\n    end\n\n    def processed\n      stat :processed\n    end\n\n    def failed\n      stat :failed\n    end\n\n    def scheduled_size\n      stat :scheduled_size\n    end\n\n    def retry_size\n      stat :retry_size\n    end\n\n    def dead_size\n      stat :dead_size\n    end\n\n    def enqueued\n      stat :enqueued\n    end\n\n    def processes_size\n      stat :processes_size\n    end\n\n    def workers_size\n      stat :workers_size\n    end\n\n    def default_queue_latency\n      stat :default_queue_latency\n    end\n\n    def queues\n      Sidekiq::Stats::Queues.new.lengths\n    end\n\n    # O(1) redis calls\n    def fetch_stats_fast!\n      pipe1_res = Sidekiq.redis { |conn|\n        conn.pipelined do\n          conn.get(\"stat:processed\")\n          conn.get(\"stat:failed\")\n          conn.zcard(\"schedule\")\n          conn.zcard(\"retry\")\n          conn.zcard(\"dead\")\n          conn.scard(\"processes\")\n          conn.lrange(\"queue:default\", -1, -1)\n        end\n      }\n\n      default_queue_latency = if (entry = pipe1_res[6].first)\n        job = begin\n          Sidekiq.load_json(entry)\n        rescue\n          {}\n        end\n        now = Time.now.to_f\n        thence = job[\"enqueued_at\"] || now\n        now - thence\n      else\n        0\n      end\n\n      @stats = {\n        processed: pipe1_res[0].to_i,\n        failed: pipe1_res[1].to_i,\n        scheduled_size: pipe1_res[2],\n        retry_size: pipe1_res[3],\n        dead_size: pipe1_res[4],\n        processes_size: pipe1_res[5],\n\n        default_queue_latency: default_queue_latency\n      }\n    end\n\n    # O(number of processes + number of queues) redis calls\n    def fetch_stats_slow!\n      processes = Sidekiq.redis { |conn|\n        conn.sscan_each(\"processes\").to_a\n      }\n\n      queues = Sidekiq.redis { |conn|\n        conn.sscan_each(\"queues\").to_a\n      }\n\n      pipe2_res = Sidekiq.redis { |conn|\n        conn.pipelined do\n          processes.each { |key| conn.hget(key, \"busy\") }\n          queues.each { |queue| conn.llen(\"queue:#{queue}\") }\n        end\n      }\n\n      s = processes.size\n      workers_size = pipe2_res[0...s].sum(&:to_i)\n      enqueued = pipe2_res[s..-1].sum(&:to_i)\n\n      @stats[:workers_size] = workers_size\n      @stats[:enqueued] = enqueued\n      @stats\n    end\n\n    def fetch_stats!\n      fetch_stats_fast!\n      fetch_stats_slow!\n    end\n\n    def reset(*stats)\n      all = %w[failed processed]\n      stats = stats.empty? ? all : all & stats.flatten.compact.map(&:to_s)\n\n      mset_args = []\n      stats.each do |stat|\n        mset_args << \"stat:#{stat}\"\n        mset_args << 0\n      end\n      Sidekiq.redis do |conn|\n        conn.mset(*mset_args)\n      end\n    end\n\n    private\n\n    def stat(s)\n      fetch_stats_slow! if @stats[s].nil?\n      @stats[s] || raise(ArgumentError, \"Unknown stat #{s}\")\n    end\n\n    class Queues\n      def lengths\n        Sidekiq.redis do |conn|\n          queues = conn.sscan_each(\"queues\").to_a\n\n          lengths = conn.pipelined {\n            queues.each do |queue|\n              conn.llen(\"queue:#{queue}\")\n            end\n          }\n\n          array_of_arrays = queues.zip(lengths).sort_by { |_, size| -size }\n          array_of_arrays.to_h\n        end\n      end\n    end\n\n    class History\n      def initialize(days_previous, start_date = nil)\n        # we only store five years of data in Redis\n        raise ArgumentError if days_previous < 1 || days_previous > (5 * 365)\n        @days_previous = days_previous\n        @start_date = start_date || Time.now.utc.to_date\n      end\n\n      def processed\n        @processed ||= date_stat_hash(\"processed\")\n      end\n\n      def failed\n        @failed ||= date_stat_hash(\"failed\")\n      end\n\n      private\n\n      def date_stat_hash(stat)\n        stat_hash = {}\n        dates = @start_date.downto(@start_date - @days_previous + 1).map { |date|\n          date.strftime(\"%Y-%m-%d\")\n        }\n\n        keys = dates.map { |datestr| \"stat:#{stat}:#{datestr}\" }\n\n        begin\n          Sidekiq.redis do |conn|\n            conn.mget(keys).each_with_index do |value, idx|\n              stat_hash[dates[idx]] = value ? value.to_i : 0\n            end\n          end\n        rescue Redis::CommandError\n          # mget will trigger a CROSSSLOT error when run against a Cluster\n          # TODO Someone want to add Cluster support?\n        end\n\n        stat_hash\n      end\n    end\n  end\n\n  ##\n  # Encapsulates a queue within Sidekiq.\n  # Allows enumeration of all jobs within the queue\n  # and deletion of jobs.\n  #\n  #   queue = Sidekiq::Queue.new(\"mailer\")\n  #   queue.each do |job|\n  #     job.klass # => 'MyWorker'\n  #     job.args # => [1, 2, 3]\n  #     job.delete if job.jid == 'abcdef1234567890'\n  #   end\n  #\n  class Queue\n    include Enumerable\n\n    ##\n    # Return all known queues within Redis.\n    #\n    def self.all\n      Sidekiq.redis { |c| c.sscan_each(\"queues\").to_a }.sort.map { |q| Sidekiq::Queue.new(q) }\n    end\n\n    attr_reader :name\n\n    def initialize(name = \"default\")\n      @name = name.to_s\n      @rname = \"queue:#{name}\"\n    end\n\n    def size\n      Sidekiq.redis { |con| con.llen(@rname) }\n    end\n\n    # Sidekiq Pro overrides this\n    def paused?\n      false\n    end\n\n    ##\n    # Calculates this queue's latency, the difference in seconds since the oldest\n    # job in the queue was enqueued.\n    #\n    # @return Float\n    def latency\n      entry = Sidekiq.redis { |conn|\n        conn.lrange(@rname, -1, -1)\n      }.first\n      return 0 unless entry\n      job = Sidekiq.load_json(entry)\n      now = Time.now.to_f\n      thence = job[\"enqueued_at\"] || now\n      now - thence\n    end\n\n    def each\n      initial_size = size\n      deleted_size = 0\n      page = 0\n      page_size = 50\n\n      loop do\n        range_start = page * page_size - deleted_size\n        range_end = range_start + page_size - 1\n        entries = Sidekiq.redis { |conn|\n          conn.lrange @rname, range_start, range_end\n        }\n        break if entries.empty?\n        page += 1\n        entries.each do |entry|\n          yield JobRecord.new(entry, @name)\n        end\n        deleted_size = initial_size - size\n      end\n    end\n\n    ##\n    # Find the job with the given JID within this queue.\n    #\n    # This is a slow, inefficient operation.  Do not use under\n    # normal conditions.\n    def find_job(jid)\n      detect { |j| j.jid == jid }\n    end\n\n    def clear\n      Sidekiq.redis do |conn|\n        conn.multi do\n          conn.unlink(@rname)\n          conn.srem(\"queues\", name)\n        end\n      end\n    end\n    alias_method :\ud83d\udca3, :clear\n  end\n\n  ##\n  # Encapsulates a pending job within a Sidekiq queue or\n  # sorted set.\n  #\n  # The job should be considered immutable but may be\n  # removed from the queue via JobRecord#delete.\n  #\n  class JobRecord\n    attr_reader :item\n    attr_reader :value\n\n    def initialize(item, queue_name = nil)\n      @args = nil\n      @value = item\n      @item = item.is_a?(Hash) ? item : parse(item)\n      @queue = queue_name || @item[\"queue\"]\n    end\n\n    def parse(item)\n      Sidekiq.load_json(item)\n    rescue JSON::ParserError\n      # If the job payload in Redis is invalid JSON, we'll load\n      # the item as an empty hash and store the invalid JSON as\n      # the job 'args' for display in the Web UI.\n      @invalid = true\n      @args = [item]\n      {}\n    end\n\n    def klass\n      self[\"class\"]\n    end\n\n    def display_class\n      # Unwrap known wrappers so they show up in a human-friendly manner in the Web UI\n      @klass ||= self[\"display_class\"] || begin\n        case klass\n        when /\\ASidekiq::Extensions::Delayed/\n          safe_load(args[0], klass) do |target, method, _|\n            \"#{target}.#{method}\"\n          end\n        when \"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\"\n          job_class = @item[\"wrapped\"] || args[0]\n          if job_class == \"ActionMailer::DeliveryJob\" || job_class == \"ActionMailer::MailDeliveryJob\"\n            # MailerClass#mailer_method\n            args[0][\"arguments\"][0..1].join(\"#\")\n          else\n            job_class\n          end\n        else\n          klass\n        end\n      end\n    end\n\n    def display_args\n      # Unwrap known wrappers so they show up in a human-friendly manner in the Web UI\n      @display_args ||= case klass\n                when /\\ASidekiq::Extensions::Delayed/\n                  safe_load(args[0], args) do |_, _, arg|\n                    arg\n                  end\n                when \"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\"\n                  job_args = self[\"wrapped\"] ? args[0][\"arguments\"] : []\n                  if (self[\"wrapped\"] || args[0]) == \"ActionMailer::DeliveryJob\"\n                    # remove MailerClass, mailer_method and 'deliver_now'\n                    job_args.drop(3)\n                  elsif (self[\"wrapped\"] || args[0]) == \"ActionMailer::MailDeliveryJob\"\n                    # remove MailerClass, mailer_method and 'deliver_now'\n                    job_args.drop(3).first[\"args\"]\n                  else\n                    job_args\n                  end\n                else\n                  if self[\"encrypt\"]\n                    # no point in showing 150+ bytes of random garbage\n                    args[-1] = \"[encrypted data]\"\n                  end\n                  args\n      end\n    end\n\n    def args\n      @args || @item[\"args\"]\n    end\n\n    def jid\n      self[\"jid\"]\n    end\n\n    def enqueued_at\n      self[\"enqueued_at\"] ? Time.at(self[\"enqueued_at\"]).utc : nil\n    end\n\n    def created_at\n      Time.at(self[\"created_at\"] || self[\"enqueued_at\"] || 0).utc\n    end\n\n    def tags\n      self[\"tags\"] || []\n    end\n\n    def error_backtrace\n      # Cache nil values\n      if defined?(@error_backtrace)\n        @error_backtrace\n      else\n        value = self[\"error_backtrace\"]\n        @error_backtrace = value && uncompress_backtrace(value)\n      end\n    end\n\n    attr_reader :queue\n\n    def latency\n      now = Time.now.to_f\n      now - (@item[\"enqueued_at\"] || @item[\"created_at\"] || now)\n    end\n\n    ##\n    # Remove this job from the queue.\n    def delete\n      count = Sidekiq.redis { |conn|\n        conn.lrem(\"queue:#{@queue}\", 1, @value)\n      }\n      count != 0\n    end\n\n    def [](name)\n      # nil will happen if the JSON fails to parse.\n      # We don't guarantee Sidekiq will work with bad job JSON but we should\n      # make a best effort to minimize the damage.\n      @item ? @item[name] : nil\n    end\n\n    private\n\n    def safe_load(content, default)\n      yield(*YAML.load(content))\n    rescue => ex\n      # #1761 in dev mode, it's possible to have jobs enqueued which haven't been loaded into\n      # memory yet so the YAML can't be loaded.\n      Sidekiq.logger.warn \"Unable to load YAML: #{ex.message}\" unless Sidekiq.options[:environment] == \"development\"\n      default\n    end\n\n    def uncompress_backtrace(backtrace)\n      if backtrace.is_a?(Array)\n        # Handle old jobs with raw Array backtrace format\n        backtrace\n      else\n        decoded = Base64.decode64(backtrace)\n        uncompressed = Zlib::Inflate.inflate(decoded)\n        begin\n          Sidekiq.load_json(uncompressed)\n        rescue\n          # Handle old jobs with marshalled backtrace format\n          # TODO Remove in 7.x\n          Marshal.load(uncompressed)\n        end\n      end\n    end\n  end\n\n  class SortedEntry < JobRecord\n    attr_reader :score\n    attr_reader :parent\n\n    def initialize(parent, score, item)\n      super(item)\n      @score = score\n      @parent = parent\n    end\n\n    def at\n      Time.at(score).utc\n    end\n\n    def delete\n      if @value\n        @parent.delete_by_value(@parent.name, @value)\n      else\n        @parent.delete_by_jid(score, jid)\n      end\n    end\n\n    def reschedule(at)\n      Sidekiq.redis do |conn|\n        conn.zincrby(@parent.name, at.to_f - @score, Sidekiq.dump_json(@item))\n      end\n    end\n\n    def add_to_queue\n      remove_job do |message|\n        msg = Sidekiq.load_json(message)\n        Sidekiq::Client.push(msg)\n      end\n    end\n\n    def retry\n      remove_job do |message|\n        msg = Sidekiq.load_json(message)\n        msg[\"retry_count\"] -= 1 if msg[\"retry_count\"]\n        Sidekiq::Client.push(msg)\n      end\n    end\n\n    ##\n    # Place job in the dead set\n    def kill\n      remove_job do |message|\n        DeadSet.new.kill(message)\n      end\n    end\n\n    def error?\n      !!item[\"error_class\"]\n    end\n\n    private\n\n    def remove_job\n      Sidekiq.redis do |conn|\n        results = conn.multi {\n          conn.zrangebyscore(parent.name, score, score)\n          conn.zremrangebyscore(parent.name, score, score)\n        }.first\n\n        if results.size == 1\n          yield results.first\n        else\n          # multiple jobs with the same score\n          # find the one with the right JID and push it\n          matched, nonmatched = results.partition { |message|\n            if message.index(jid)\n              msg = Sidekiq.load_json(message)\n              msg[\"jid\"] == jid\n            else\n              false\n            end\n          }\n\n          msg = matched.first\n          yield msg if msg\n\n          # push the rest back onto the sorted set\n          conn.multi do\n            nonmatched.each do |message|\n              conn.zadd(parent.name, score.to_f.to_s, message)\n            end\n          end\n        end\n      end\n    end\n  end\n\n  class SortedSet\n    include Enumerable\n\n    attr_reader :name\n\n    def initialize(name)\n      @name = name\n      @_size = size\n    end\n\n    def size\n      Sidekiq.redis { |c| c.zcard(name) }\n    end\n\n    def scan(match, count = 100)\n      return to_enum(:scan, match, count) unless block_given?\n\n      match = \"*#{match}*\" unless match.include?(\"*\")\n      Sidekiq.redis do |conn|\n        conn.zscan_each(name, match: match, count: count) do |entry, score|\n          yield SortedEntry.new(self, score, entry)\n        end\n      end\n    end\n\n    def clear\n      Sidekiq.redis do |conn|\n        conn.unlink(name)\n      end\n    end\n    alias_method :\ud83d\udca3, :clear\n  end\n\n  class JobSet < SortedSet\n    def schedule(timestamp, message)\n      Sidekiq.redis do |conn|\n        conn.zadd(name, timestamp.to_f.to_s, Sidekiq.dump_json(message))\n      end\n    end\n\n    def each\n      initial_size = @_size\n      offset_size = 0\n      page = -1\n      page_size = 50\n\n      loop do\n        range_start = page * page_size + offset_size\n        range_end = range_start + page_size - 1\n        elements = Sidekiq.redis { |conn|\n          conn.zrange name, range_start, range_end, with_scores: true\n        }\n        break if elements.empty?\n        page -= 1\n        elements.reverse_each do |element, score|\n          yield SortedEntry.new(self, score, element)\n        end\n        offset_size = initial_size - @_size\n      end\n    end\n\n    ##\n    # Fetch jobs that match a given time or Range. Job ID is an\n    # optional second argument.\n    def fetch(score, jid = nil)\n      begin_score, end_score =\n        if score.is_a?(Range)\n          [score.first, score.last]\n        else\n          [score, score]\n        end\n\n      elements = Sidekiq.redis { |conn|\n        conn.zrangebyscore(name, begin_score, end_score, with_scores: true)\n      }\n\n      elements.each_with_object([]) do |element, result|\n        data, job_score = element\n        entry = SortedEntry.new(self, job_score, data)\n        result << entry if jid.nil? || entry.jid == jid\n      end\n    end\n\n    ##\n    # Find the job with the given JID within this sorted set.\n    # This is a slower O(n) operation.  Do not use for app logic.\n    def find_job(jid)\n      Sidekiq.redis do |conn|\n        conn.zscan_each(name, match: \"*#{jid}*\", count: 100) do |entry, score|\n          job = JSON.parse(entry)\n          matched = job[\"jid\"] == jid\n          return SortedEntry.new(self, score, entry) if matched\n        end\n      end\n      nil\n    end\n\n    def delete_by_value(name, value)\n      Sidekiq.redis do |conn|\n        ret = conn.zrem(name, value)\n        @_size -= 1 if ret\n        ret\n      end\n    end\n\n    def delete_by_jid(score, jid)\n      Sidekiq.redis do |conn|\n        elements = conn.zrangebyscore(name, score, score)\n        elements.each do |element|\n          if element.index(jid)\n            message = Sidekiq.load_json(element)\n            if message[\"jid\"] == jid\n              ret = conn.zrem(name, element)\n              @_size -= 1 if ret\n              break ret\n            end\n          end\n        end\n      end\n    end\n\n    alias_method :delete, :delete_by_jid\n  end\n\n  ##\n  # Allows enumeration of scheduled jobs within Sidekiq.\n  # Based on this, you can search/filter for jobs.  Here's an\n  # example where I'm selecting all jobs of a certain type\n  # and deleting them from the schedule queue.\n  #\n  #   r = Sidekiq::ScheduledSet.new\n  #   r.select do |scheduled|\n  #     scheduled.klass == 'Sidekiq::Extensions::DelayedClass' &&\n  #     scheduled.args[0] == 'User' &&\n  #     scheduled.args[1] == 'setup_new_subscriber'\n  #   end.map(&:delete)\n  class ScheduledSet < JobSet\n    def initialize\n      super \"schedule\"\n    end\n  end\n\n  ##\n  # Allows enumeration of retries within Sidekiq.\n  # Based on this, you can search/filter for jobs.  Here's an\n  # example where I'm selecting all jobs of a certain type\n  # and deleting them from the retry queue.\n  #\n  #   r = Sidekiq::RetrySet.new\n  #   r.select do |retri|\n  #     retri.klass == 'Sidekiq::Extensions::DelayedClass' &&\n  #     retri.args[0] == 'User' &&\n  #     retri.args[1] == 'setup_new_subscriber'\n  #   end.map(&:delete)\n  class RetrySet < JobSet\n    def initialize\n      super \"retry\"\n    end\n\n    def retry_all\n      each(&:retry) while size > 0\n    end\n\n    def kill_all\n      each(&:kill) while size > 0\n    end\n  end\n\n  ##\n  # Allows enumeration of dead jobs within Sidekiq.\n  #\n  class DeadSet < JobSet\n    def initialize\n      super \"dead\"\n    end\n\n    def kill(message, opts = {})\n      now = Time.now.to_f\n      Sidekiq.redis do |conn|\n        conn.multi do\n          conn.zadd(name, now.to_s, message)\n          conn.zremrangebyscore(name, \"-inf\", now - self.class.timeout)\n          conn.zremrangebyrank(name, 0, - self.class.max_jobs)\n        end\n      end\n\n      if opts[:notify_failure] != false\n        job = Sidekiq.load_json(message)\n        r = RuntimeError.new(\"Job killed by API\")\n        r.set_backtrace(caller)\n        Sidekiq.death_handlers.each do |handle|\n          handle.call(job, r)\n        end\n      end\n      true\n    end\n\n    def retry_all\n      each(&:retry) while size > 0\n    end\n\n    def self.max_jobs\n      Sidekiq.options[:dead_max_jobs]\n    end\n\n    def self.timeout\n      Sidekiq.options[:dead_timeout_in_seconds]\n    end\n  end\n\n  ##\n  # Enumerates the set of Sidekiq processes which are actively working\n  # right now.  Each process sends a heartbeat to Redis every 5 seconds\n  # so this set should be relatively accurate, barring network partitions.\n  #\n  # Yields a Sidekiq::Process.\n  #\n  class ProcessSet\n    include Enumerable\n\n    def initialize(clean_plz = true)\n      cleanup if clean_plz\n    end\n\n    # Cleans up dead processes recorded in Redis.\n    # Returns the number of processes cleaned.\n    def cleanup\n      count = 0\n      Sidekiq.redis do |conn|\n        procs = conn.sscan_each(\"processes\").to_a.sort\n        heartbeats = conn.pipelined {\n          procs.each do |key|\n            conn.hget(key, \"info\")\n          end\n        }\n\n        # the hash named key has an expiry of 60 seconds.\n        # if it's not found, that means the process has not reported\n        # in to Redis and probably died.\n        to_prune = procs.select.with_index { |proc, i|\n          heartbeats[i].nil?\n        }\n        count = conn.srem(\"processes\", to_prune) unless to_prune.empty?\n      end\n      count\n    end\n\n    def each\n      result = Sidekiq.redis { |conn|\n        procs = conn.sscan_each(\"processes\").to_a.sort\n\n        # We're making a tradeoff here between consuming more memory instead of\n        # making more roundtrips to Redis, but if you have hundreds or thousands of workers,\n        # you'll be happier this way\n        conn.pipelined do\n          procs.each do |key|\n            conn.hmget(key, \"info\", \"busy\", \"beat\", \"quiet\", \"rss\", \"rtt_us\")\n          end\n        end\n      }\n\n      result.each do |info, busy, at_s, quiet, rss, rtt|\n        # If a process is stopped between when we query Redis for `procs` and\n        # when we query for `result`, we will have an item in `result` that is\n        # composed of `nil` values.\n        next if info.nil?\n\n        hash = Sidekiq.load_json(info)\n        yield Process.new(hash.merge(\"busy\" => busy.to_i,\n          \"beat\" => at_s.to_f,\n          \"quiet\" => quiet,\n          \"rss\" => rss.to_i,\n          \"rtt_us\" => rtt.to_i))\n      end\n    end\n\n    # This method is not guaranteed accurate since it does not prune the set\n    # based on current heartbeat.  #each does that and ensures the set only\n    # contains Sidekiq processes which have sent a heartbeat within the last\n    # 60 seconds.\n    def size\n      Sidekiq.redis { |conn| conn.scard(\"processes\") }\n    end\n\n    # Total number of threads available to execute jobs.\n    # For Sidekiq Enterprise customers this number (in production) must be\n    # less than or equal to your licensed concurrency.\n    def total_concurrency\n      sum { |x| x[\"concurrency\"].to_i }\n    end\n\n    def total_rss_in_kb\n      sum { |x| x[\"rss\"].to_i }\n    end\n    alias_method :total_rss, :total_rss_in_kb\n\n    # Returns the identity of the current cluster leader or \"\" if no leader.\n    # This is a Sidekiq Enterprise feature, will always return \"\" in Sidekiq\n    # or Sidekiq Pro.\n    def leader\n      @leader ||= begin\n        x = Sidekiq.redis { |c| c.get(\"dear-leader\") }\n        # need a non-falsy value so we can memoize\n        x ||= \"\"\n        x\n      end\n    end\n  end\n\n  #\n  # Sidekiq::Process represents an active Sidekiq process talking with Redis.\n  # Each process has a set of attributes which look like this:\n  #\n  # {\n  #   'hostname' => 'app-1.example.com',\n  #   'started_at' => <process start time>,\n  #   'pid' => 12345,\n  #   'tag' => 'myapp'\n  #   'concurrency' => 25,\n  #   'queues' => ['default', 'low'],\n  #   'busy' => 10,\n  #   'beat' => <last heartbeat>,\n  #   'identity' => <unique string identifying the process>,\n  # }\n  class Process\n    def initialize(hash)\n      @attribs = hash\n    end\n\n    def tag\n      self[\"tag\"]\n    end\n\n    def labels\n      Array(self[\"labels\"])\n    end\n\n    def [](key)\n      @attribs[key]\n    end\n\n    def identity\n      self[\"identity\"]\n    end\n\n    def queues\n      self[\"queues\"]\n    end\n\n    def quiet!\n      signal(\"TSTP\")\n    end\n\n    def stop!\n      signal(\"TERM\")\n    end\n\n    def dump_threads\n      signal(\"TTIN\")\n    end\n\n    def stopping?\n      self[\"quiet\"] == \"true\"\n    end\n\n    private\n\n    def signal(sig)\n      key = \"#{identity}-signals\"\n      Sidekiq.redis do |c|\n        c.multi do\n          c.lpush(key, sig)\n          c.expire(key, 60)\n        end\n      end\n    end\n  end\n\n  ##\n  # The WorkSet stores the work being done by this Sidekiq cluster.\n  # It tracks the process and thread working on each job.\n  #\n  # WARNING WARNING WARNING\n  #\n  # This is live data that can change every millisecond.\n  # If you call #size => 5 and then expect #each to be\n  # called 5 times, you're going to have a bad time.\n  #\n  #    works = Sidekiq::WorkSet.new\n  #    works.size => 2\n  #    works.each do |process_id, thread_id, work|\n  #      # process_id is a unique identifier per Sidekiq process\n  #      # thread_id is a unique identifier per thread\n  #      # work is a Hash which looks like:\n  #      # { 'queue' => name, 'run_at' => timestamp, 'payload' => job_hash }\n  #      # run_at is an epoch Integer.\n  #    end\n  #\n  class WorkSet\n    include Enumerable\n\n    def each(&block)\n      results = []\n      Sidekiq.redis do |conn|\n        procs = conn.sscan_each(\"processes\").to_a\n        procs.sort.each do |key|\n          valid, workers = conn.pipelined {\n            conn.exists?(key)\n            conn.hgetall(\"#{key}:workers\")\n          }\n          next unless valid\n          workers.each_pair do |tid, json|\n            hsh = Sidekiq.load_json(json)\n            p = hsh[\"payload\"]\n            # avoid breaking API, this is a side effect of the JSON optimization in #4316\n            hsh[\"payload\"] = Sidekiq.load_json(p) if p.is_a?(String)\n            results << [key, tid, hsh]\n          end\n        end\n      end\n\n      results.sort_by { |(_, _, hsh)| hsh[\"run_at\"] }.each(&block)\n    end\n\n    # Note that #size is only as accurate as Sidekiq's heartbeat,\n    # which happens every 5 seconds.  It is NOT real-time.\n    #\n    # Not very efficient if you have lots of Sidekiq\n    # processes but the alternative is a global counter\n    # which can easily get out of sync with crashy processes.\n    def size\n      Sidekiq.redis do |conn|\n        procs = conn.sscan_each(\"processes\").to_a\n        if procs.empty?\n          0\n        else\n          conn.pipelined {\n            procs.each do |key|\n              conn.hget(key, \"busy\")\n            end\n          }.sum(&:to_i)\n        end\n      end\n    end\n  end\n  # Since \"worker\" is a nebulous term, we've deprecated the use of this class name.\n  # Is \"worker\" a process, a type of job, a thread? Undefined!\n  # WorkSet better describes the data.\n  Workers = WorkSet\nend\n", "# frozen_string_literal: true\n\nmodule Sidekiq\n  class WebApplication\n    extend WebRouter\n\n    REDIS_KEYS = %w[redis_version uptime_in_days connected_clients used_memory_human used_memory_peak_human]\n    CSP_HEADER = [\n      \"default-src 'self' https: http:\",\n      \"child-src 'self'\",\n      \"connect-src 'self' https: http: wss: ws:\",\n      \"font-src 'self' https: http:\",\n      \"frame-src 'self'\",\n      \"img-src 'self' https: http: data:\",\n      \"manifest-src 'self'\",\n      \"media-src 'self'\",\n      \"object-src 'none'\",\n      \"script-src 'self' https: http: 'unsafe-inline'\",\n      \"style-src 'self' https: http: 'unsafe-inline'\",\n      \"worker-src 'self'\",\n      \"base-uri 'self'\"\n    ].join(\"; \").freeze\n\n    def initialize(klass)\n      @klass = klass\n    end\n\n    def settings\n      @klass.settings\n    end\n\n    def self.settings\n      Sidekiq::Web.settings\n    end\n\n    def self.tabs\n      Sidekiq::Web.tabs\n    end\n\n    def self.set(key, val)\n      # nothing, backwards compatibility\n    end\n\n    head \"/\" do\n      # HEAD / is the cheapest heartbeat possible,\n      # it hits Redis to ensure connectivity\n      Sidekiq.redis { |c| c.llen(\"queue:default\") }\n      \"\"\n    end\n\n    get \"/\" do\n      @redis_info = redis_info.select { |k, v| REDIS_KEYS.include? k }\n      days = (params[\"days\"] || 30).to_i\n      return halt(401) if days < 1 || days > 180\n\n      stats_history = Sidekiq::Stats::History.new(days)\n      @processed_history = stats_history.processed\n      @failed_history = stats_history.failed\n\n      erb(:dashboard)\n    end\n\n    get \"/busy\" do\n      erb(:busy)\n    end\n\n    post \"/busy\" do\n      if params[\"identity\"]\n        p = Sidekiq::Process.new(\"identity\" => params[\"identity\"])\n        p.quiet! if params[\"quiet\"]\n        p.stop! if params[\"stop\"]\n      else\n        processes.each do |pro|\n          pro.quiet! if params[\"quiet\"]\n          pro.stop! if params[\"stop\"]\n        end\n      end\n\n      redirect \"#{root_path}busy\"\n    end\n\n    get \"/queues\" do\n      @queues = Sidekiq::Queue.all\n\n      erb(:queues)\n    end\n\n    QUEUE_NAME = /\\A[a-z_:.\\-0-9]+\\z/i\n\n    get \"/queues/:name\" do\n      @name = route_params[:name]\n\n      halt(404) if !@name || @name !~ QUEUE_NAME\n\n      @count = (params[\"count\"] || 25).to_i\n      @queue = Sidekiq::Queue.new(@name)\n      (@current_page, @total_size, @jobs) = page(\"queue:#{@name}\", params[\"page\"], @count, reverse: params[\"direction\"] == \"asc\")\n      @jobs = @jobs.map { |msg| Sidekiq::JobRecord.new(msg, @name) }\n\n      erb(:queue)\n    end\n\n    post \"/queues/:name\" do\n      queue = Sidekiq::Queue.new(route_params[:name])\n\n      if Sidekiq.pro? && params[\"pause\"]\n        queue.pause!\n      elsif Sidekiq.pro? && params[\"unpause\"]\n        queue.unpause!\n      else\n        queue.clear\n      end\n\n      redirect \"#{root_path}queues\"\n    end\n\n    post \"/queues/:name/delete\" do\n      name = route_params[:name]\n      Sidekiq::JobRecord.new(params[\"key_val\"], name).delete\n\n      redirect_with_query(\"#{root_path}queues/#{CGI.escape(name)}\")\n    end\n\n    get \"/morgue\" do\n      @count = (params[\"count\"] || 25).to_i\n      (@current_page, @total_size, @dead) = page(\"dead\", params[\"page\"], @count, reverse: true)\n      @dead = @dead.map { |msg, score| Sidekiq::SortedEntry.new(nil, score, msg) }\n\n      erb(:morgue)\n    end\n\n    get \"/morgue/:key\" do\n      key = route_params[:key]\n      halt(404) unless key\n\n      @dead = Sidekiq::DeadSet.new.fetch(*parse_params(key)).first\n\n      if @dead.nil?\n        redirect \"#{root_path}morgue\"\n      else\n        erb(:dead)\n      end\n    end\n\n    post \"/morgue\" do\n      redirect(request.path) unless params[\"key\"]\n\n      params[\"key\"].each do |key|\n        job = Sidekiq::DeadSet.new.fetch(*parse_params(key)).first\n        retry_or_delete_or_kill job, params if job\n      end\n\n      redirect_with_query(\"#{root_path}morgue\")\n    end\n\n    post \"/morgue/all/delete\" do\n      Sidekiq::DeadSet.new.clear\n\n      redirect \"#{root_path}morgue\"\n    end\n\n    post \"/morgue/all/retry\" do\n      Sidekiq::DeadSet.new.retry_all\n\n      redirect \"#{root_path}morgue\"\n    end\n\n    post \"/morgue/:key\" do\n      key = route_params[:key]\n      halt(404) unless key\n\n      job = Sidekiq::DeadSet.new.fetch(*parse_params(key)).first\n      retry_or_delete_or_kill job, params if job\n\n      redirect_with_query(\"#{root_path}morgue\")\n    end\n\n    get \"/retries\" do\n      @count = (params[\"count\"] || 25).to_i\n      (@current_page, @total_size, @retries) = page(\"retry\", params[\"page\"], @count)\n      @retries = @retries.map { |msg, score| Sidekiq::SortedEntry.new(nil, score, msg) }\n\n      erb(:retries)\n    end\n\n    get \"/retries/:key\" do\n      @retry = Sidekiq::RetrySet.new.fetch(*parse_params(route_params[:key])).first\n\n      if @retry.nil?\n        redirect \"#{root_path}retries\"\n      else\n        erb(:retry)\n      end\n    end\n\n    post \"/retries\" do\n      redirect(request.path) unless params[\"key\"]\n\n      params[\"key\"].each do |key|\n        job = Sidekiq::RetrySet.new.fetch(*parse_params(key)).first\n        retry_or_delete_or_kill job, params if job\n      end\n\n      redirect_with_query(\"#{root_path}retries\")\n    end\n\n    post \"/retries/all/delete\" do\n      Sidekiq::RetrySet.new.clear\n\n      redirect \"#{root_path}retries\"\n    end\n\n    post \"/retries/all/retry\" do\n      Sidekiq::RetrySet.new.retry_all\n\n      redirect \"#{root_path}retries\"\n    end\n\n    post \"/retries/all/kill\" do\n      Sidekiq::RetrySet.new.kill_all\n\n      redirect \"#{root_path}retries\"\n    end\n\n    post \"/retries/:key\" do\n      job = Sidekiq::RetrySet.new.fetch(*parse_params(route_params[:key])).first\n\n      retry_or_delete_or_kill job, params if job\n\n      redirect_with_query(\"#{root_path}retries\")\n    end\n\n    get \"/scheduled\" do\n      @count = (params[\"count\"] || 25).to_i\n      (@current_page, @total_size, @scheduled) = page(\"schedule\", params[\"page\"], @count)\n      @scheduled = @scheduled.map { |msg, score| Sidekiq::SortedEntry.new(nil, score, msg) }\n\n      erb(:scheduled)\n    end\n\n    get \"/scheduled/:key\" do\n      @job = Sidekiq::ScheduledSet.new.fetch(*parse_params(route_params[:key])).first\n\n      if @job.nil?\n        redirect \"#{root_path}scheduled\"\n      else\n        erb(:scheduled_job_info)\n      end\n    end\n\n    post \"/scheduled\" do\n      redirect(request.path) unless params[\"key\"]\n\n      params[\"key\"].each do |key|\n        job = Sidekiq::ScheduledSet.new.fetch(*parse_params(key)).first\n        delete_or_add_queue job, params if job\n      end\n\n      redirect_with_query(\"#{root_path}scheduled\")\n    end\n\n    post \"/scheduled/:key\" do\n      key = route_params[:key]\n      halt(404) unless key\n\n      job = Sidekiq::ScheduledSet.new.fetch(*parse_params(key)).first\n      delete_or_add_queue job, params if job\n\n      redirect_with_query(\"#{root_path}scheduled\")\n    end\n\n    get \"/dashboard/stats\" do\n      redirect \"#{root_path}stats\"\n    end\n\n    get \"/stats\" do\n      sidekiq_stats = Sidekiq::Stats.new\n      redis_stats = redis_info.select { |k, v| REDIS_KEYS.include? k }\n      json(\n        sidekiq: {\n          processed: sidekiq_stats.processed,\n          failed: sidekiq_stats.failed,\n          busy: sidekiq_stats.workers_size,\n          processes: sidekiq_stats.processes_size,\n          enqueued: sidekiq_stats.enqueued,\n          scheduled: sidekiq_stats.scheduled_size,\n          retries: sidekiq_stats.retry_size,\n          dead: sidekiq_stats.dead_size,\n          default_latency: sidekiq_stats.default_queue_latency\n        },\n        redis: redis_stats,\n        server_utc_time: server_utc_time\n      )\n    end\n\n    get \"/stats/queues\" do\n      json Sidekiq::Stats::Queues.new.lengths\n    end\n\n    def call(env)\n      action = self.class.match(env)\n      return [404, {\"Content-Type\" => \"text/plain\", \"X-Cascade\" => \"pass\"}, [\"Not Found\"]] unless action\n\n      app = @klass\n      resp = catch(:halt) do\n        self.class.run_befores(app, action)\n        action.instance_exec env, &action.block\n      ensure\n        self.class.run_afters(app, action)\n      end\n\n      case resp\n      when Array\n        # redirects go here\n        resp\n      else\n        # rendered content goes here\n        headers = {\n          \"Content-Type\" => \"text/html\",\n          \"Cache-Control\" => \"private, no-store\",\n          \"Content-Language\" => action.locale,\n          \"Content-Security-Policy\" => CSP_HEADER\n        }\n        # we'll let Rack calculate Content-Length for us.\n        [200, headers, [resp]]\n      end\n    end\n\n    def self.helpers(mod = nil, &block)\n      if block\n        WebAction.class_eval(&block)\n      else\n        WebAction.send(:include, mod)\n      end\n    end\n\n    def self.before(path = nil, &block)\n      befores << [path && Regexp.new(\"\\\\A#{path.gsub(\"*\", \".*\")}\\\\z\"), block]\n    end\n\n    def self.after(path = nil, &block)\n      afters << [path && Regexp.new(\"\\\\A#{path.gsub(\"*\", \".*\")}\\\\z\"), block]\n    end\n\n    def self.run_befores(app, action)\n      run_hooks(befores, app, action)\n    end\n\n    def self.run_afters(app, action)\n      run_hooks(afters, app, action)\n    end\n\n    def self.run_hooks(hooks, app, action)\n      hooks.select { |p, _| !p || p =~ action.env[WebRouter::PATH_INFO] }\n        .each { |_, b| action.instance_exec(action.env, app, &b) }\n    end\n\n    def self.befores\n      @befores ||= []\n    end\n\n    def self.afters\n      @afters ||= []\n    end\n  end\nend\n", "# frozen_string_literal: true\nrequire_relative 'helper'\nrequire 'sidekiq/api'\nrequire 'active_job'\nrequire 'action_mailer'\n\ndescribe 'API' do\n  before do\n    Sidekiq.redis {|c| c.flushdb }\n  end\n\n  describe \"stats\" do\n    it \"is initially zero\" do\n      s = Sidekiq::Stats.new\n      assert_equal 0, s.processed\n      assert_equal 0, s.failed\n      assert_equal 0, s.enqueued\n      assert_equal 0, s.default_queue_latency\n      assert_equal 0, s.workers_size\n    end\n\n    describe \"processed\" do\n      it \"returns number of processed jobs\" do\n        Sidekiq.redis { |conn| conn.set(\"stat:processed\", 5) }\n        s = Sidekiq::Stats.new\n        assert_equal 5, s.processed\n      end\n    end\n\n    describe \"failed\" do\n      it \"returns number of failed jobs\" do\n        Sidekiq.redis { |conn| conn.set(\"stat:failed\", 5) }\n        s = Sidekiq::Stats.new\n        assert_equal 5, s.failed\n      end\n    end\n\n    describe \"reset\" do\n      before do\n        Sidekiq.redis do |conn|\n          conn.set('stat:processed', 5)\n          conn.set('stat:failed', 10)\n        end\n      end\n\n      it 'will reset all stats by default' do\n        Sidekiq::Stats.new.reset\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.failed\n        assert_equal 0, s.processed\n      end\n\n      it 'can reset individual stats' do\n        Sidekiq::Stats.new.reset('failed')\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.failed\n        assert_equal 5, s.processed\n      end\n\n      it 'can accept anything that responds to #to_s' do\n        Sidekiq::Stats.new.reset(:failed)\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.failed\n        assert_equal 5, s.processed\n      end\n\n      it 'ignores anything other than \"failed\" or \"processed\"' do\n        Sidekiq::Stats.new.reset((1..10).to_a, ['failed'])\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.failed\n        assert_equal 5, s.processed\n      end\n    end\n\n    describe \"workers_size\" do\n      it 'retrieves the number of busy workers' do\n        Sidekiq.redis do |c|\n          c.sadd(\"processes\", \"process_1\")\n          c.sadd(\"processes\", \"process_2\")\n          c.hset(\"process_1\", \"busy\", 1)\n          c.hset(\"process_2\", \"busy\", 2)\n        end\n        s = Sidekiq::Stats.new\n        assert_equal 3, s.workers_size\n      end\n    end\n\n    describe \"queues\" do\n      it \"is initially empty\" do\n        s = Sidekiq::Stats::Queues.new\n        assert_equal 0, s.lengths.size\n      end\n\n      it \"returns a hash of queue and size in order\" do\n        Sidekiq.redis do |conn|\n          conn.rpush 'queue:foo', '{}'\n          conn.sadd 'queues', 'foo'\n\n          3.times { conn.rpush 'queue:bar', '{}' }\n          conn.sadd 'queues', 'bar'\n        end\n\n        s = Sidekiq::Stats::Queues.new\n        assert_equal ({ \"foo\" => 1, \"bar\" => 3 }), s.lengths\n        assert_equal \"bar\", s.lengths.first.first\n\n        assert_equal Sidekiq::Stats.new.queues, Sidekiq::Stats::Queues.new.lengths\n      end\n    end\n\n    describe \"enqueued\" do\n      it 'handles latency for good jobs' do\n        Sidekiq.redis do |conn|\n          conn.rpush 'queue:default', \"{\\\"enqueued_at\\\": #{Time.now.to_f}}\"\n          conn.sadd 'queues', 'default'\n        end\n        s = Sidekiq::Stats.new\n        assert s.default_queue_latency > 0\n        q = Sidekiq::Queue.new\n        assert q.latency > 0\n      end\n\n      it 'handles latency for incomplete jobs' do\n        Sidekiq.redis do |conn|\n          conn.rpush 'queue:default', '{}'\n          conn.sadd 'queues', 'default'\n        end\n        s = Sidekiq::Stats.new\n        assert_equal 0, s.default_queue_latency\n        q = Sidekiq::Queue.new\n        assert_equal 0, q.latency\n      end\n\n      it \"returns total enqueued jobs\" do\n        Sidekiq.redis do |conn|\n          conn.rpush 'queue:foo', '{}'\n          conn.sadd 'queues', 'foo'\n\n          3.times { conn.rpush 'queue:bar', '{}' }\n          conn.sadd 'queues', 'bar'\n        end\n\n        s = Sidekiq::Stats.new\n        assert_equal 4, s.enqueued\n      end\n    end\n\n    describe \"over time\" do\n      before do\n        require 'active_support/core_ext/time/conversions'\n        @before = Time::DATE_FORMATS[:default]\n        Time::DATE_FORMATS[:default] = \"%d/%m/%Y %H:%M:%S\"\n      end\n\n      after do\n        Time::DATE_FORMATS[:default] = @before\n      end\n\n      describe \"history\" do\n        it \"does not allow invalid input\" do\n          assert_raises(ArgumentError) { Sidekiq::Stats::History.new(-1) }\n          assert_raises(ArgumentError) { Sidekiq::Stats::History.new(0) }\n          assert_raises(ArgumentError) { Sidekiq::Stats::History.new(2000) }\n          assert Sidekiq::Stats::History.new(200)\n        end\n      end\n\n      describe \"processed\" do\n        it 'retrieves hash of dates' do\n          Sidekiq.redis do |c|\n            c.incrby(\"stat:processed:2012-12-24\", 4)\n            c.incrby(\"stat:processed:2012-12-25\", 1)\n            c.incrby(\"stat:processed:2012-12-26\", 6)\n            c.incrby(\"stat:processed:2012-12-27\", 2)\n          end\n          Time.stub(:now, Time.parse(\"2012-12-26 1:00:00 -0500\")) do\n            s = Sidekiq::Stats::History.new(2)\n            assert_equal({ \"2012-12-26\" => 6, \"2012-12-25\" => 1 }, s.processed)\n\n            s = Sidekiq::Stats::History.new(3)\n            assert_equal({ \"2012-12-26\" => 6, \"2012-12-25\" => 1, \"2012-12-24\" => 4 }, s.processed)\n\n            s = Sidekiq::Stats::History.new(2, Date.parse(\"2012-12-25\"))\n            assert_equal({ \"2012-12-25\" => 1, \"2012-12-24\" => 4 }, s.processed)\n          end\n        end\n      end\n\n      describe \"failed\" do\n        it 'retrieves hash of dates' do\n          Sidekiq.redis do |c|\n            c.incrby(\"stat:failed:2012-12-24\", 4)\n            c.incrby(\"stat:failed:2012-12-25\", 1)\n            c.incrby(\"stat:failed:2012-12-26\", 6)\n            c.incrby(\"stat:failed:2012-12-27\", 2)\n          end\n          Time.stub(:now, Time.parse(\"2012-12-26 1:00:00 -0500\")) do\n            s = Sidekiq::Stats::History.new(2)\n            assert_equal ({ \"2012-12-26\" => 6, \"2012-12-25\" => 1 }), s.failed\n\n            s = Sidekiq::Stats::History.new(3)\n            assert_equal ({ \"2012-12-26\" => 6, \"2012-12-25\" => 1, \"2012-12-24\" => 4 }), s.failed\n\n            s = Sidekiq::Stats::History.new(2, Date.parse(\"2012-12-25\"))\n            assert_equal ({ \"2012-12-25\" => 1, \"2012-12-24\" => 4 }), s.failed\n          end\n        end\n      end\n    end\n  end\n\n  describe 'with an empty database' do\n    it 'shows queue as empty' do\n      q = Sidekiq::Queue.new\n      assert_equal 0, q.size\n      assert_equal 0, q.latency\n    end\n\n    before do\n      ActiveJob::Base.queue_adapter = :sidekiq\n      ActiveJob::Base.logger = nil\n    end\n\n    class ApiMailer < ActionMailer::Base\n      def test_email(*)\n      end\n    end\n\n    class ApiJob < ActiveJob::Base\n      def perform(*)\n      end\n    end\n\n    class ApiWorker\n      include Sidekiq::Worker\n    end\n\n    class WorkerWithTags\n      include Sidekiq::Worker\n      sidekiq_options tags: ['foo']\n    end\n\n    it 'can enumerate jobs' do\n      q = Sidekiq::Queue.new\n      Time.stub(:now, Time.new(2012, 12, 26)) do\n        ApiWorker.perform_async(1, 'mike')\n        assert_equal [ApiWorker.name], q.map(&:klass)\n\n        job = q.first\n        assert_equal 24, job.jid.size\n        assert_equal [1, 'mike'], job.args\n        assert_equal Time.new(2012, 12, 26), job.enqueued_at\n      end\n      assert q.latency > 10_000_000\n\n      q = Sidekiq::Queue.new('other')\n      assert_equal 0, q.size\n    end\n\n    it 'enumerates jobs in descending score order' do\n      # We need to enqueue more than 50 items, which is the page size when retrieving\n      # from Redis to ensure everything is sorted: the pages and the items withing them.\n      51.times { ApiWorker.perform_in(100, 1, 'foo') }\n\n      set = Sidekiq::ScheduledSet.new.to_a\n\n      assert_equal set.sort_by { |job| -job.score }, set\n    end\n\n    it 'has no enqueued_at time for jobs enqueued in the future' do\n      job_id = ApiWorker.perform_in(100, 1, 'foo')\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      assert_nil job.enqueued_at\n    end\n\n    it 'unwraps delayed jobs' do\n      Sidekiq::Extensions.enable_delay!\n      Sidekiq::Queue.delay.foo(1,2,3)\n      q = Sidekiq::Queue.new\n      x = q.first\n      assert_equal \"Sidekiq::Queue.foo\", x.display_class\n      assert_equal [1,2,3], x.display_args\n    end\n\n    it 'handles previous (raw Array) error_backtrace format' do\n      add_retry\n      job = Sidekiq::RetrySet.new.first\n      assert_equal ['line1', 'line2'], job.error_backtrace\n    end\n\n    it 'handles previous (marshalled Array) error_backtrace format' do\n      backtrace = ['line1', 'line2']\n      serialized = Marshal.dump(backtrace)\n      compressed = Zlib::Deflate.deflate(serialized)\n      encoded = Base64.encode64(compressed)\n\n      payload = Sidekiq.dump_json('class' => 'ApiWorker', 'args' => [1], 'queue' => 'default', 'jid' => 'jid', 'error_backtrace' => encoded)\n      Sidekiq.redis do |conn|\n        conn.zadd('retry', Time.now.to_f.to_s, payload)\n      end\n\n      job = Sidekiq::RetrySet.new.first\n      assert_equal backtrace, job.error_backtrace\n    end\n\n    describe \"Rails unwrapping\" do\n      SERIALIZED_JOBS = {\n        \"5.x\" => [\n          '{\"class\":\"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\",\"wrapped\":\"ApiJob\",\"queue\":\"default\",\"args\":[{\"job_class\":\"ApiJob\",\"job_id\":\"f1bde53f-3852-4ae4-a879-c12eacebbbb0\",\"provider_job_id\":null,\"queue_name\":\"default\",\"priority\":null,\"arguments\":[1,2,3],\"executions\":0,\"locale\":\"en\"}],\"retry\":true,\"jid\":\"099eee72911085a511d0e312\",\"created_at\":1568305542.339916,\"enqueued_at\":1568305542.339947}',\n          '{\"class\":\"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\",\"wrapped\":\"ActionMailer::DeliveryJob\",\"queue\":\"mailers\",\"args\":[{\"job_class\":\"ActionMailer::DeliveryJob\",\"job_id\":\"19cc0115-3d1c-4bbe-a51e-bfa1385895d1\",\"provider_job_id\":null,\"queue_name\":\"mailers\",\"priority\":null,\"arguments\":[\"ApiMailer\",\"test_email\",\"deliver_now\",1,2,3],\"executions\":0,\"locale\":\"en\"}],\"retry\":true,\"jid\":\"37436e5504936400e8cf98db\",\"created_at\":1568305542.370133,\"enqueued_at\":1568305542.370241}',\n        ],\n        \"6.x\" => [\n          '{\"class\":\"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\",\"wrapped\":\"ApiJob\",\"queue\":\"default\",\"args\":[{\"job_class\":\"ApiJob\",\"job_id\":\"ff2b48d4-bdce-4825-af6b-ef8c11ab651e\",\"provider_job_id\":null,\"queue_name\":\"default\",\"priority\":null,\"arguments\":[1,2,3],\"executions\":0,\"exception_executions\":{},\"locale\":\"en\",\"timezone\":\"UTC\",\"enqueued_at\":\"2019-09-12T16:28:37Z\"}],\"retry\":true,\"jid\":\"ce121bf77b37ae81fe61b6dc\",\"created_at\":1568305717.9469702,\"enqueued_at\":1568305717.947005}',\n          '{\"class\":\"ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper\",\"wrapped\":\"ActionMailer::MailDeliveryJob\",\"queue\":\"mailers\",\"args\":[{\"job_class\":\"ActionMailer::MailDeliveryJob\",\"job_id\":\"2f967da1-a389-479c-9a4e-5cc059e6d65c\",\"provider_job_id\":null,\"queue_name\":\"mailers\",\"priority\":null,\"arguments\":[\"ApiMailer\",\"test_email\",\"deliver_now\",{\"args\":[1,2,3],\"_aj_symbol_keys\":[\"args\"]}],\"executions\":0,\"exception_executions\":{},\"locale\":\"en\",\"timezone\":\"UTC\",\"enqueued_at\":\"2019-09-12T16:28:37Z\"}],\"retry\":true,\"jid\":\"469979df52bb9ef9f48b49e1\",\"created_at\":1568305717.9457421,\"enqueued_at\":1568305717.9457731}',\n        ],\n      }.each_pair do |ver,jobs|\n        it \"unwraps ActiveJob #{ver} jobs\" do\n          #ApiJob.perform_later(1,2,3)\n          #puts Sidekiq::Queue.new.first.value\n          x = Sidekiq::JobRecord.new(jobs[0], \"default\")\n          assert_equal ApiJob.name, x.display_class\n          assert_equal [1,2,3], x.display_args\n        end\n\n        it \"unwraps ActionMailer #{ver} jobs\" do\n          #ApiMailer.test_email(1,2,3).deliver_later\n          #puts Sidekiq::Queue.new(\"mailers\").first.value\n          x = Sidekiq::JobRecord.new(jobs[1], \"mailers\")\n          assert_equal \"#{ApiMailer.name}#test_email\", x.display_class\n          assert_equal [1,2,3], x.display_args\n        end\n      end\n    end\n\n    it 'has no enqueued_at time for jobs enqueued in the future' do\n      job_id = ApiWorker.perform_in(100, 1, 'foo')\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      assert_nil job.enqueued_at\n    end\n\n    it 'returns tags field for jobs' do\n      job_id = ApiWorker.perform_async\n      assert_equal [], Sidekiq::Queue.new.find_job(job_id).tags\n\n      job_id = WorkerWithTags.perform_async\n      assert_equal ['foo'], Sidekiq::Queue.new.find_job(job_id).tags\n    end\n\n    it 'can delete jobs' do\n      q = Sidekiq::Queue.new\n      ApiWorker.perform_async(1, 'mike')\n      assert_equal 1, q.size\n\n      x = q.first\n      assert_equal ApiWorker.name, x.display_class\n      assert_equal [1,'mike'], x.display_args\n\n      assert_equal [true], q.map(&:delete)\n      assert_equal 0, q.size\n    end\n\n    it \"can move scheduled job to queue\" do\n      remain_id = ApiWorker.perform_in(100, 1, 'jason')\n      job_id = ApiWorker.perform_in(100, 1, 'jason')\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      q = Sidekiq::Queue.new\n      job.add_to_queue\n      queued_job = q.find_job(job_id)\n      refute_nil queued_job\n      assert_equal queued_job.jid, job_id\n      assert_nil Sidekiq::ScheduledSet.new.find_job(job_id)\n      refute_nil Sidekiq::ScheduledSet.new.find_job(remain_id)\n    end\n\n    it \"handles multiple scheduled jobs when moving to queue\" do\n      jids = Sidekiq::Client.push_bulk('class' => ApiWorker,\n                                       'args' => [[1, 'jason'], [2, 'jason']],\n                                       'at' => Time.now.to_f)\n      assert_equal 2, jids.size\n      (remain_id, job_id) = jids\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      q = Sidekiq::Queue.new\n      job.add_to_queue\n      queued_job = q.find_job(job_id)\n      refute_nil queued_job\n      assert_equal queued_job.jid, job_id\n      assert_nil Sidekiq::ScheduledSet.new.find_job(job_id)\n      refute_nil Sidekiq::ScheduledSet.new.find_job(remain_id)\n    end\n\n    it 'can kill a scheduled job' do\n      job_id = ApiWorker.perform_in(100, 1, '{\"foo\":123}')\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      ds = Sidekiq::DeadSet.new\n      assert_equal 0, ds.size\n      job.kill\n      assert_equal 1, ds.size\n    end\n\n    it 'can find a scheduled job by jid' do\n      10.times do |idx|\n        ApiWorker.perform_in(idx, 1)\n      end\n\n      job_id = ApiWorker.perform_in(5, 1)\n      job = Sidekiq::ScheduledSet.new.find_job(job_id)\n      assert_equal job_id, job.jid\n\n      ApiWorker.perform_in(100, 1, 'jid' => 'jid_in_args')\n      assert_nil Sidekiq::ScheduledSet.new.find_job('jid_in_args')\n    end\n\n    it 'can remove jobs when iterating over a sorted set' do\n      # scheduled jobs must be greater than SortedSet#each underlying page size\n      51.times do\n        ApiWorker.perform_in(100, 'aaron')\n      end\n      set = Sidekiq::ScheduledSet.new\n      set.map(&:delete)\n      assert_equal set.size, 0\n    end\n\n    it 'can remove jobs when iterating over a queue' do\n      # initial queue size must be greater than Queue#each underlying page size\n      51.times do\n        ApiWorker.perform_async(1, 'aaron')\n      end\n      q = Sidekiq::Queue.new\n      q.map(&:delete)\n      assert_equal q.size, 0\n    end\n\n    it 'can find job by id in queues' do\n      q = Sidekiq::Queue.new\n      job_id = ApiWorker.perform_async(1, 'jason')\n      job = q.find_job(job_id)\n      refute_nil job\n      assert_equal job_id, job.jid\n    end\n\n    it 'can clear a queue' do\n      q = Sidekiq::Queue.new\n      2.times { ApiWorker.perform_async(1, 'mike') }\n      q.clear\n\n      Sidekiq.redis do |conn|\n        refute conn.smembers('queues').include?('foo')\n        refute conn.exists?('queue:foo')\n      end\n    end\n\n    it 'can fetch by score' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time)\n      r = Sidekiq::RetrySet.new\n      assert_equal 2, r.fetch(same_time).size\n    end\n\n    it 'can fetch by score and jid' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time)\n      r = Sidekiq::RetrySet.new\n      assert_equal 1, r.fetch(same_time, 'bob1').size\n    end\n\n    it 'can fetch by score range' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time + 1)\n      add_retry('bob3', same_time + 2)\n      r = Sidekiq::RetrySet.new\n      range = (same_time..(same_time + 1))\n      assert_equal 2, r.fetch(range).size\n    end\n\n    it 'can fetch by score range and jid' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time + 1)\n      add_retry('bob3', same_time + 2)\n      r = Sidekiq::RetrySet.new\n      range = (same_time..(same_time + 1))\n      jobs = r.fetch(range, 'bob2')\n      assert_equal 1, jobs.size\n      assert_equal jobs[0].jid, 'bob2'\n    end\n\n    it 'shows empty retries' do\n      r = Sidekiq::RetrySet.new\n      assert_equal 0, r.size\n    end\n\n    it 'can enumerate retries' do\n      add_retry\n\n      r = Sidekiq::RetrySet.new\n      assert_equal 1, r.size\n      array = r.to_a\n      assert_equal 1, array.size\n\n      retri = array.first\n      assert_equal 'ApiWorker', retri.klass\n      assert_equal 'default', retri.queue\n      assert_equal 'bob', retri.jid\n      assert_in_delta Time.now.to_f, retri.at.to_f, 0.02\n    end\n\n    it 'requires a jid to delete an entry' do\n      start_time = Time.now.to_f\n      add_retry('bob2', Time.now.to_f)\n      assert_raises(ArgumentError) do\n        Sidekiq::RetrySet.new.delete(start_time)\n      end\n    end\n\n    it 'can delete a single retry from score and jid' do\n      same_time = Time.now.to_f\n      add_retry('bob1', same_time)\n      add_retry('bob2', same_time)\n      r = Sidekiq::RetrySet.new\n      assert_equal 2, r.size\n      Sidekiq::RetrySet.new.delete(same_time, 'bob1')\n      assert_equal 1, r.size\n    end\n\n    it 'can retry a retry' do\n      add_retry\n      r = Sidekiq::RetrySet.new\n      assert_equal 1, r.size\n      r.first.retry\n      assert_equal 0, r.size\n      assert_equal 1, Sidekiq::Queue.new('default').size\n      job = Sidekiq::Queue.new('default').first\n      assert_equal 'bob', job.jid\n      assert_equal 1, job['retry_count']\n    end\n\n    it 'can clear retries' do\n      add_retry\n      add_retry('test')\n      r = Sidekiq::RetrySet.new\n      assert_equal 2, r.size\n      r.clear\n      assert_equal 0, r.size\n    end\n\n    it 'can scan retries' do\n      add_retry\n      add_retry('test')\n      r = Sidekiq::RetrySet.new\n      assert_instance_of Enumerator, r.scan('Worker')\n      assert_equal 2, r.scan('ApiWorker').to_a.size\n      assert_equal 1, r.scan('*test*').to_a.size\n    end\n\n    it 'can enumerate processes' do\n      identity_string = \"identity_string\"\n      odata = {\n        'pid' => 123,\n        'hostname' => Socket.gethostname,\n        'key' => identity_string,\n        'identity' => identity_string,\n        'started_at' => Time.now.to_f - 15,\n        'queues' => ['foo', 'bar']\n      }\n\n      time = Time.now.to_f\n      Sidekiq.redis do |conn|\n        conn.multi do\n          conn.sadd('processes', odata['key'])\n          conn.hmset(odata['key'], 'info', Sidekiq.dump_json(odata), 'busy', 10, 'beat', time)\n          conn.sadd('processes', 'fake:pid')\n        end\n      end\n\n      ps = Sidekiq::ProcessSet.new.to_a\n      assert_equal 1, ps.size\n      data = ps.first\n      assert_equal 10, data['busy']\n      assert_equal time, data['beat']\n      assert_equal 123, data['pid']\n      assert_equal ['foo', 'bar'], data.queues\n      data.quiet!\n      data.stop!\n      signals_string = \"#{odata['key']}-signals\"\n      assert_equal \"TERM\", Sidekiq.redis{|c| c.lpop(signals_string) }\n      assert_equal \"TSTP\", Sidekiq.redis{|c| c.lpop(signals_string) }\n    end\n\n    it 'can enumerate workers' do\n      w = Sidekiq::Workers.new\n      assert_equal 0, w.size\n      w.each do\n        assert false\n      end\n\n      hn = Socket.gethostname\n      key = \"#{hn}:#{$$}\"\n      pdata = { 'pid' => $$, 'hostname' => hn, 'started_at' => Time.now.to_i }\n      Sidekiq.redis do |conn|\n        conn.sadd('processes', key)\n        conn.hmset(key, 'info', Sidekiq.dump_json(pdata), 'busy', 0, 'beat', Time.now.to_f)\n      end\n\n      s = \"#{key}:workers\"\n      data = Sidekiq.dump_json({ 'payload' => \"{}\", 'queue' => 'default', 'run_at' => Time.now.to_i })\n      Sidekiq.redis do |c|\n        c.hmset(s, '1234', data)\n      end\n\n      w.each do |p, x, y|\n        assert_equal key, p\n        assert_equal \"1234\", x\n        assert_equal 'default', y['queue']\n        assert_equal({}, y['payload'])\n        assert_equal Time.now.year, Time.at(y['run_at']).year\n      end\n\n      s = \"#{key}:workers\"\n      data = Sidekiq.dump_json({ 'payload' => {}, 'queue' => 'default', 'run_at' => (Time.now.to_i - 2*60*60) })\n      Sidekiq.redis do |c|\n        c.multi do\n          c.hmset(s, '5678', data)\n          c.hmset(\"b#{s}\", '5678', data)\n        end\n      end\n\n      assert_equal ['5678', '1234'], w.map { |_, tid, _| tid }\n    end\n\n    it 'can reschedule jobs' do\n      add_retry('foo1')\n      add_retry('foo2')\n\n      retries = Sidekiq::RetrySet.new\n      assert_equal 2, retries.size\n      refute(retries.map { |r| r.score > (Time.now.to_f + 9) }.any?)\n\n      retries.each do |retri|\n        retri.reschedule(Time.now + 15) if retri.jid == 'foo1'\n        retri.reschedule(Time.now.to_f + 10) if retri.jid == 'foo2'\n      end\n\n      assert_equal 2, retries.size\n      assert(retries.map { |r| r.score > (Time.now.to_f + 9) }.any?)\n      assert(retries.map { |r| r.score > (Time.now.to_f + 14) }.any?)\n    end\n\n    it 'prunes processes which have died' do\n      data = { 'pid' => rand(10_000), 'hostname' => \"app#{rand(1_000)}\", 'started_at' => Time.now.to_f }\n      key = \"#{data['hostname']}:#{data['pid']}\"\n      Sidekiq.redis do |conn|\n        conn.sadd('processes', key)\n        conn.hmset(key, 'info', Sidekiq.dump_json(data), 'busy', 0, 'beat', Time.now.to_f)\n      end\n\n      ps = Sidekiq::ProcessSet.new\n      assert_equal 1, ps.size\n      assert_equal 1, ps.to_a.size\n\n      Sidekiq.redis do |conn|\n        conn.sadd('processes', \"bar:987\")\n        conn.sadd('processes', \"bar:986\")\n      end\n\n      ps = Sidekiq::ProcessSet.new\n      assert_equal 1, ps.size\n      assert_equal 1, ps.to_a.size\n    end\n\n    def add_retry(jid = 'bob', at = Time.now.to_f)\n      payload = Sidekiq.dump_json('class' => 'ApiWorker', 'args' => [1, 'mike'], 'queue' => 'default', 'jid' => jid, 'retry_count' => 2, 'failed_at' => Time.now.to_f, 'error_backtrace' => ['line1', 'line2'])\n      Sidekiq.redis do |conn|\n        conn.zadd('retry', at.to_s, payload)\n      end\n    end\n  end\nend\n", "# encoding: utf-8\n# frozen_string_literal: true\nrequire_relative 'helper'\nrequire 'sidekiq/web'\nrequire 'sidekiq/util'\nrequire 'rack/test'\n\ndescribe Sidekiq::Web do\n  include Rack::Test::Methods\n\n  def app\n    @app ||= Sidekiq::Web.new\n  end\n\n  def job_params(job, score)\n    \"#{score}-#{job['jid']}\"\n  end\n\n  before do\n    Sidekiq.redis {|c| c.flushdb }\n    app.middlewares.clear\n  end\n\n  class WebWorker\n    include Sidekiq::Worker\n\n    def perform(a, b)\n      a + b\n    end\n  end\n\n  it 'can show text with any locales' do\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'ru,en'}\n    get '/', {}, rackenv\n    assert_match(/\u041f\u0430\u043d\u0435\u043b\u044c \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'es,en'}\n    get '/', {}, rackenv\n    assert_match(/Panel de Control/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'en-us'}\n    get '/', {}, rackenv\n    assert_match(/Dashboard/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'zh-cn'}\n    get '/', {}, rackenv\n    assert_match(/\u4fe1\u606f\u677f/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'zh-tw'}\n    get '/', {}, rackenv\n    assert_match(/\u8cc7\u8a0a\u4e3b\u9801/, last_response.body)\n    rackenv = {'HTTP_ACCEPT_LANGUAGE' => 'nb'}\n    get '/', {}, rackenv\n    assert_match(/Oversikt/, last_response.body)\n  end\n\n  it 'can provide a default, appropriate CSP for its content' do\n    get '/', {}\n    policies = last_response.headers[\"Content-Security-Policy\"].split('; ')\n    assert_includes(policies, \"connect-src 'self' https: http: wss: ws:\")\n    assert_includes(policies, \"style-src 'self' https: http: 'unsafe-inline'\")\n    assert_includes(policies, \"script-src 'self' https: http: 'unsafe-inline'\")\n    assert_includes(policies, \"object-src 'none'\")\n  end\n\n  describe 'busy' do\n\n    it 'can display workers' do\n      Sidekiq.redis do |conn|\n        conn.incr('busy')\n        conn.sadd('processes', 'foo:1234')\n        conn.hmset('foo:1234', 'info', Sidekiq.dump_json('hostname' => 'foo', 'started_at' => Time.now.to_f, \"queues\" => [], 'concurrency' => 10), 'at', Time.now.to_f, 'busy', 4)\n        identity = 'foo:1234:workers'\n        hash = {:queue => 'critical', :payload => { 'class' => WebWorker.name, 'args' => [1,'abc'] }, :run_at => Time.now.to_i }\n        conn.hmset(identity, 1001, Sidekiq.dump_json(hash))\n      end\n      assert_equal ['1001'], Sidekiq::Workers.new.map { |pid, tid, data| tid }\n\n      get '/busy'\n      assert_equal 200, last_response.status\n      assert_match(/status-active/, last_response.body)\n      assert_match(/critical/, last_response.body)\n      assert_match(/WebWorker/, last_response.body)\n    end\n\n    it 'can quiet a process' do\n      identity = 'identity'\n      signals_key = \"#{identity}-signals\"\n\n      assert_nil Sidekiq.redis { |c| c.lpop signals_key }\n      post '/busy', 'quiet' => '1', 'identity' => identity\n      assert_equal 302, last_response.status\n      assert_equal 'TSTP', Sidekiq.redis { |c| c.lpop signals_key }\n    end\n\n    it 'can stop a process' do\n      identity = 'identity'\n      signals_key = \"#{identity}-signals\"\n\n      assert_nil Sidekiq.redis { |c| c.lpop signals_key }\n      post '/busy', 'stop' => '1', 'identity' => identity\n      assert_equal 302, last_response.status\n      assert_equal 'TERM', Sidekiq.redis { |c| c.lpop signals_key }\n    end\n  end\n\n  it 'can display queues' do\n    assert Sidekiq::Client.push('queue' => :foo, 'class' => WebWorker, 'args' => [1, 3])\n\n    get '/queues'\n    assert_equal 200, last_response.status\n    assert_match(/foo/, last_response.body)\n    refute_match(/HardWorker/, last_response.body)\n    assert_match(/0.0/, last_response.body)\n    refute_match(/datetime/, last_response.body)\n    Sidekiq::Queue.new(\"foo\").clear\n\n    Time.stub(:now, Time.now - 65) do\n      assert Sidekiq::Client.push('queue' => :foo, 'class' => WebWorker, 'args' => [1, 3])\n    end\n\n    get '/queues'\n    assert_equal 200, last_response.status\n    assert_match(/foo/, last_response.body)\n    refute_match(/HardWorker/, last_response.body)\n    assert_match(/65.0/, last_response.body)\n    assert_match(/datetime/, last_response.body)\n  end\n\n  it 'handles queue view' do\n    get '/queues/onmouseover=alert()'\n    assert_equal 404, last_response.status\n\n    get '/queues/foo_bar:123-wow.'\n    assert_equal 200, last_response.status\n    assert_match(/foo_bar:123-wow\\./, last_response.body)\n\n    get '/queues/default'\n    assert_equal 200, last_response.status\n  end\n\n  it 'can sort on enqueued_at column' do\n    Sidekiq.redis do |conn|\n      (1000..1005).each do |i|\n        conn.lpush('queue:default', Sidekiq.dump_json(args: [i], enqueued_at: Time.now.to_i + i))\n      end\n    end\n\n    get '/queues/default?count=3' # direction is 'desc' by default\n    assert_match(/1005/, last_response.body)\n    refute_match(/1002/, last_response.body)\n\n    get '/queues/default?count=3&direction=asc'\n    assert_match(/1000/, last_response.body)\n    refute_match(/1003/, last_response.body)\n  end\n\n  it 'can delete a queue' do\n    Sidekiq.redis do |conn|\n      conn.rpush('queue:foo', \"{\\\"args\\\":[],\\\"enqueued_at\\\":1567894960}\")\n      conn.sadd('queues', 'foo')\n    end\n\n    get '/queues/foo'\n    assert_equal 200, last_response.status\n\n    post '/queues/foo'\n    assert_equal 302, last_response.status\n\n    Sidekiq.redis do |conn|\n      refute conn.smembers('queues').include?('foo')\n      refute conn.exists?('queue:foo')\n    end\n  end\n\n  it 'can attempt to pause a queue' do\n    Sidekiq.stub(:pro?, true) do\n      mock = Minitest::Mock.new\n      mock.expect :pause!, true\n\n      stub = lambda do |queue_name|\n        assert_equal 'foo', queue_name\n        mock\n      end\n\n      Sidekiq::Queue.stub :new, stub do\n        post '/queues/foo', 'pause' => 'pause'\n        assert_equal 302, last_response.status\n      end\n\n      assert_mock mock\n    end\n  end\n\n  it 'can attempt to unpause a queue' do\n    Sidekiq.stub(:pro?, true) do\n      mock = Minitest::Mock.new\n      mock.expect :unpause!, true\n\n      stub = lambda do |queue_name|\n        assert_equal 'foo', queue_name\n        mock\n      end\n\n      Sidekiq::Queue.stub :new, stub do\n        post '/queues/foo', 'unpause' => 'unpause'\n        assert_equal 302, last_response.status\n      end\n\n      assert_mock mock\n    end\n  end\n\n  it 'ignores to attempt to pause a queue with pro disabled' do\n    mock = Minitest::Mock.new\n    mock.expect :clear, true\n\n    stub = lambda do |queue_name|\n      assert_equal 'foo', queue_name\n      mock\n    end\n\n    Sidekiq::Queue.stub :new, stub do\n      post '/queues/foo', 'pause' => 'pause'\n      assert_equal 302, last_response.status\n    end\n\n    assert_mock mock\n  end\n\n  it 'ignores to attempt to unpause a queue with pro disabled' do\n    mock = Minitest::Mock.new\n    mock.expect :clear, true\n\n    stub = lambda do |queue_name|\n      assert_equal 'foo', queue_name\n      mock\n    end\n\n    Sidekiq::Queue.stub :new, stub do\n      post '/queues/foo', 'unpause' => 'unpause'\n      assert_equal 302, last_response.status\n    end\n\n    assert_mock mock\n  end\n\n  it 'can delete a job' do\n    Sidekiq.redis do |conn|\n      conn.rpush('queue:foo', '{\"args\":[],\"enqueued_at\":1567894960}')\n      conn.rpush('queue:foo', '{\"foo\":\"bar\",\"args\":[],\"enqueued_at\":1567894960}')\n      conn.rpush('queue:foo', '{\"foo2\":\"bar2\",\"args\":[],\"enqueued_at\":1567894960}')\n    end\n\n    get '/queues/foo'\n    assert_equal 200, last_response.status\n\n    post '/queues/foo/delete', key_val: \"{\\\"foo\\\":\\\"bar\\\"}\"\n    assert_equal 302, last_response.status\n\n    Sidekiq.redis do |conn|\n      refute conn.lrange('queue:foo', 0, -1).include?(\"{\\\"foo\\\":\\\"bar\\\"}\")\n    end\n  end\n\n  it 'can display retries' do\n    get '/retries'\n    assert_equal 200, last_response.status\n    assert_match(/found/, last_response.body)\n    refute_match(/HardWorker/, last_response.body)\n\n    add_retry\n\n    get '/retries'\n    assert_equal 200, last_response.status\n    refute_match(/found/, last_response.body)\n    assert_match(/HardWorker/, last_response.body)\n  end\n\n  it 'can display a single retry' do\n    params = add_retry\n    get '/retries/0-shouldntexist'\n    assert_equal 302, last_response.status\n    get \"/retries/#{job_params(*params)}\"\n    assert_equal 200, last_response.status\n    assert_match(/HardWorker/, last_response.body)\n  end\n\n  it 'handles missing retry' do\n    get \"/retries/0-shouldntexist\"\n    assert_equal 302, last_response.status\n  end\n\n  it 'can delete a single retry' do\n    params = add_retry\n    post \"/retries/#{job_params(*params)}\", 'delete' => 'Delete'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n\n    get \"/retries\"\n    assert_equal 200, last_response.status\n    refute_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can delete all retries' do\n    3.times { add_retry }\n\n    post \"/retries/all/delete\", 'delete' => 'Delete'\n    assert_equal 0, Sidekiq::RetrySet.new.size\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n  end\n\n  it 'can retry a single retry now' do\n    params = add_retry\n    post \"/retries/#{job_params(*params)}\", 'retry' => 'Retry'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n\n    get '/queues/default'\n    assert_equal 200, last_response.status\n    assert_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can kill a single retry now' do\n    params = add_retry\n    post \"/retries/#{job_params(*params)}\", 'kill' => 'Kill'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n\n    get '/morgue'\n    assert_equal 200, last_response.status\n    assert_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can display scheduled' do\n    get '/scheduled'\n    assert_equal 200, last_response.status\n    assert_match(/found/, last_response.body)\n    refute_match(/HardWorker/, last_response.body)\n\n    add_scheduled\n\n    get '/scheduled'\n    assert_equal 200, last_response.status\n    refute_match(/found/, last_response.body)\n    assert_match(/HardWorker/, last_response.body)\n  end\n\n  it 'can display a single scheduled job' do\n    params = add_scheduled\n    get '/scheduled/0-shouldntexist'\n    assert_equal 302, last_response.status\n    get \"/scheduled/#{job_params(*params)}\"\n    assert_equal 200, last_response.status\n    assert_match(/HardWorker/, last_response.body)\n  end\n\n  it 'can display a single scheduled job tags' do\n    params = add_scheduled\n    get \"/scheduled/#{job_params(*params)}\"\n    assert_match(/tag1/, last_response.body)\n    assert_match(/tag2/, last_response.body)\n  end\n\n  it 'handles missing scheduled job' do\n    get \"/scheduled/0-shouldntexist\"\n    assert_equal 302, last_response.status\n  end\n\n  it 'can add to queue a single scheduled job' do\n    params = add_scheduled\n    post \"/scheduled/#{job_params(*params)}\", 'add_to_queue' => true\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/scheduled', last_response.header['Location']\n\n    get '/queues/default'\n    assert_equal 200, last_response.status\n    assert_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can delete a single scheduled job' do\n    params = add_scheduled\n    post \"/scheduled/#{job_params(*params)}\", 'delete' => 'Delete'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/scheduled', last_response.header['Location']\n\n    get \"/scheduled\"\n    assert_equal 200, last_response.status\n    refute_match(/#{params.first['args'][2]}/, last_response.body)\n  end\n\n  it 'can delete scheduled' do\n    params = add_scheduled\n    Sidekiq.redis do |conn|\n      assert_equal 1, conn.zcard('schedule')\n      post '/scheduled', 'key' => [job_params(*params)], 'delete' => 'Delete'\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/scheduled', last_response.header['Location']\n      assert_equal 0, conn.zcard('schedule')\n    end\n  end\n\n  it \"can move scheduled to default queue\" do\n    q = Sidekiq::Queue.new\n    params = add_scheduled\n    Sidekiq.redis do |conn|\n      assert_equal 1, conn.zcard('schedule')\n      assert_equal 0, q.size\n      post '/scheduled', 'key' => [job_params(*params)], 'add_to_queue' => 'AddToQueue'\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/scheduled', last_response.header['Location']\n      assert_equal 0, conn.zcard('schedule')\n      assert_equal 1, q.size\n      get '/queues/default'\n      assert_equal 200, last_response.status\n      assert_match(/#{params[0]['args'][2]}/, last_response.body)\n    end\n  end\n\n  it 'can retry all retries' do\n    msg = add_retry.first\n    add_retry\n\n    post \"/retries/all/retry\", 'retry' => 'Retry'\n    assert_equal 302, last_response.status\n    assert_equal 'http://example.org/retries', last_response.header['Location']\n    assert_equal 2, Sidekiq::Queue.new(\"default\").size\n\n    get '/queues/default'\n    assert_equal 200, last_response.status\n    assert_match(/#{msg['args'][2]}/, last_response.body)\n  end\n\n  it 'escape job args and error messages' do\n    # on /retries page\n    params = add_xss_retry\n    get '/retries'\n    assert_equal 200, last_response.status\n    assert_match(/FailWorker/, last_response.body)\n\n    assert last_response.body.include?( \"fail message: &lt;a&gt;hello&lt;&#x2F;a&gt;\" )\n    assert !last_response.body.include?( \"fail message: <a>hello</a>\" )\n\n    assert last_response.body.include?( \"args\\\">&quot;&lt;a&gt;hello&lt;&#x2F;a&gt;&quot;<\" )\n    assert !last_response.body.include?( \"args\\\"><a>hello</a><\" )\n\n    # on /workers page\n    Sidekiq.redis do |conn|\n      pro = 'foo:1234'\n      conn.sadd('processes', pro)\n      conn.hmset(pro, 'info', Sidekiq.dump_json('started_at' => Time.now.to_f, 'labels' => ['frumduz'], 'queues' =>[], 'concurrency' => 10), 'busy', 1, 'beat', Time.now.to_f)\n      identity = \"#{pro}:workers\"\n      hash = {:queue => 'critical', :payload => { 'class' => \"FailWorker\", 'args' => [\"<a>hello</a>\"] }, :run_at => Time.now.to_i }\n      conn.hmset(identity, 100001, Sidekiq.dump_json(hash))\n      conn.incr('busy')\n    end\n\n    get '/busy'\n    assert_equal 200, last_response.status\n    assert_match(/FailWorker/, last_response.body)\n    assert_match(/frumduz/, last_response.body)\n    assert last_response.body.include?( \"&lt;a&gt;hello&lt;&#x2F;a&gt;\" )\n    assert !last_response.body.include?( \"<a>hello</a>\" )\n\n    # on /queues page\n    params = add_xss_retry # sorry, don't know how to easily make this show up on queues page otherwise.\n    post \"/retries/#{job_params(*params)}\", 'retry' => 'Retry'\n    assert_equal 302, last_response.status\n\n    get '/queues/foo'\n    assert_equal 200, last_response.status\n    assert last_response.body.include?( \"&lt;a&gt;hello&lt;&#x2F;a&gt;\" )\n    assert !last_response.body.include?( \"<a>hello</a>\" )\n  end\n\n  it 'can show user defined tab' do\n    begin\n      Sidekiq::Web.tabs['Custom Tab'] = '/custom'\n\n      get '/'\n      assert_match 'Custom Tab', last_response.body\n\n    ensure\n      Sidekiq::Web.tabs.delete 'Custom Tab'\n    end\n  end\n\n  it 'can display home' do\n    get '/'\n    assert_equal 200, last_response.status\n  end\n\n  describe 'custom locales' do\n    before do\n      Sidekiq::Web.settings.locales << File.join(File.dirname(__FILE__), \"fixtures\")\n      Sidekiq::Web.tabs['Custom Tab'] = '/custom'\n      Sidekiq::WebApplication.get('/custom') do\n        clear_caches # ugly hack since I can't figure out how to access WebHelpers outside of this context\n        t('translated_text')\n      end\n    end\n\n    after do\n      Sidekiq::Web.tabs.delete 'Custom Tab'\n      Sidekiq::Web.settings.locales.pop\n    end\n\n    it 'can show user defined tab with custom locales' do\n      get '/custom'\n\n      assert_match(/Changed text/, last_response.body)\n    end\n  end\n\n  describe 'dashboard/stats' do\n    it 'redirects to stats' do\n      get '/dashboard/stats'\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/stats', last_response.header['Location']\n    end\n  end\n\n  describe 'stats' do\n    include Sidekiq::Util\n\n    before do\n      Sidekiq.redis do |conn|\n        conn.set(\"stat:processed\", 5)\n        conn.set(\"stat:failed\", 2)\n        conn.sadd(\"queues\", \"default\")\n      end\n      2.times { add_retry }\n      3.times { add_scheduled }\n      4.times { add_worker }\n    end\n\n    it 'works' do\n      get '/stats'\n      @response = Sidekiq.load_json(last_response.body)\n\n      assert_equal 200, last_response.status\n      assert_includes @response.keys, \"sidekiq\"\n      assert_equal 5, @response[\"sidekiq\"][\"processed\"]\n      assert_equal 2, @response[\"sidekiq\"][\"failed\"]\n      assert_equal 4, @response[\"sidekiq\"][\"busy\"]\n      assert_equal 1, @response[\"sidekiq\"][\"processes\"]\n      assert_equal 2, @response[\"sidekiq\"][\"retries\"]\n      assert_equal 3, @response[\"sidekiq\"][\"scheduled\"]\n      assert_equal 0, @response[\"sidekiq\"][\"default_latency\"]\n      assert_includes @response.keys, \"redis\"\n      assert_includes @response[\"redis\"].keys, \"redis_version\"\n      assert_includes @response[\"redis\"].keys, \"uptime_in_days\"\n      assert_includes @response[\"redis\"].keys, \"connected_clients\"\n      assert_includes @response[\"redis\"].keys, \"used_memory_human\"\n      assert_includes @response[\"redis\"].keys, \"used_memory_peak_human\"\n      assert_includes @response.keys, \"server_utc_time\"\n    end\n  end\n\n  describe 'bad JSON' do\n    it 'displays without error' do\n      s = Sidekiq::DeadSet.new\n      (_, score) = kill_bad\n      assert_equal 1, s.size\n\n      get '/morgue'\n      assert_equal 200, last_response.status\n      assert_match(/#{score.to_i}/, last_response.body)\n      assert_match(\"something bad\", last_response.body)\n      assert_equal 1, s.size\n\n      post \"/morgue/#{score}-\", 'delete' => 'Delete'\n      assert_equal 302, last_response.status\n      assert_equal 1, s.size\n    end\n  end\n\n  describe 'stats/queues' do\n    include Sidekiq::Util\n\n    before do\n      Sidekiq.redis do |conn|\n        conn.set(\"stat:processed\", 5)\n        conn.set(\"stat:failed\", 2)\n        conn.sadd(\"queues\", \"default\")\n        conn.sadd(\"queues\", \"queue2\")\n      end\n      2.times { add_retry }\n      3.times { add_scheduled }\n      4.times { add_worker }\n\n      get '/stats/queues'\n      @response = Sidekiq.load_json(last_response.body)\n    end\n\n    it 'reports the queue depth' do\n      assert_equal 0, @response[\"default\"]\n      assert_equal 0, @response[\"queue2\"]\n    end\n  end\n\n  describe 'dead jobs' do\n    it 'shows empty index' do\n      get 'morgue'\n      assert_equal 200, last_response.status\n    end\n\n    it 'shows index with jobs' do\n      (_, score) = add_dead\n      get 'morgue'\n      assert_equal 200, last_response.status\n      assert_match(/#{score}/, last_response.body)\n    end\n\n    it 'can delete all dead' do\n      3.times { add_dead }\n\n      assert_equal 3, Sidekiq::DeadSet.new.size\n      post \"/morgue/all/delete\", 'delete' => 'Delete'\n      assert_equal 0, Sidekiq::DeadSet.new.size\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/morgue', last_response.header['Location']\n    end\n\n    it 'can display a dead job' do\n      params = add_dead\n      get \"/morgue/#{job_params(*params)}\"\n      assert_equal 200, last_response.status\n    end\n\n    it 'can retry a dead job' do\n      params = add_dead\n      post \"/morgue/#{job_params(*params)}\", 'retry' => 'Retry'\n      assert_equal 302, last_response.status\n      assert_equal 'http://example.org/morgue', last_response.header['Location']\n      assert_equal 0, Sidekiq::DeadSet.new.size\n\n      params = add_dead('jid-with-hyphen')\n      post \"/morgue/#{job_params(*params)}\", 'retry' => 'Retry'\n      assert_equal 302, last_response.status\n      assert_equal 0, Sidekiq::DeadSet.new.size\n\n      get '/queues/foo'\n      assert_equal 200, last_response.status\n      assert_match(/#{params.first['args'][2]}/, last_response.body)\n    end\n  end\n\n  def add_scheduled\n    score = Time.now.to_f\n    msg = { 'class' => 'HardWorker',\n            'args' => ['bob', 1, Time.now.to_f],\n            'jid' => SecureRandom.hex(12),\n            'tags' => ['tag1', 'tag2'], }\n    Sidekiq.redis do |conn|\n      conn.zadd('schedule', score, Sidekiq.dump_json(msg))\n    end\n    [msg, score]\n  end\n\n  def add_retry\n    msg = { 'class' => 'HardWorker',\n            'args' => ['bob', 1, Time.now.to_f],\n            'queue' => 'default',\n            'error_message' => 'Some fake message',\n            'error_class' => 'RuntimeError',\n            'retry_count' => 0,\n            'failed_at' => Time.now.to_f,\n            'jid' => SecureRandom.hex(12) }\n    score = Time.now.to_f\n    Sidekiq.redis do |conn|\n      conn.zadd('retry', score, Sidekiq.dump_json(msg))\n    end\n\n    [msg, score]\n  end\n\n  def add_dead(jid = SecureRandom.hex(12))\n    msg = { 'class' => 'HardWorker',\n            'args' => ['bob', 1, Time.now.to_f],\n            'queue' => 'foo',\n            'error_message' => 'Some fake message',\n            'error_class' => 'RuntimeError',\n            'retry_count' => 0,\n            'failed_at' => Time.now.utc,\n            'jid' => jid }\n    score = Time.now.to_f\n    Sidekiq.redis do |conn|\n      conn.zadd('dead', score, Sidekiq.dump_json(msg))\n    end\n    [msg, score]\n  end\n\n  def kill_bad\n    job = \"{ something bad }\"\n    score = Time.now.to_f\n    Sidekiq.redis do |conn|\n      conn.zadd('dead', score, job)\n    end\n    [job, score]\n  end\n\n  def add_xss_retry(job_id=SecureRandom.hex(12))\n    msg = { 'class' => 'FailWorker',\n            'args' => ['<a>hello</a>'],\n            'queue' => 'foo',\n            'error_message' => 'fail message: <a>hello</a>',\n            'error_class' => 'RuntimeError',\n            'retry_count' => 0,\n            'failed_at' => Time.now.to_f,\n            'jid' => SecureRandom.hex(12) }\n    score = Time.now.to_f\n    Sidekiq.redis do |conn|\n      conn.zadd('retry', score, Sidekiq.dump_json(msg))\n    end\n\n    [msg, score]\n  end\n\n  def add_worker\n    key = \"#{hostname}:#{$$}\"\n    msg = \"{\\\"queue\\\":\\\"default\\\",\\\"payload\\\":{\\\"retry\\\":true,\\\"queue\\\":\\\"default\\\",\\\"timeout\\\":20,\\\"backtrace\\\":5,\\\"class\\\":\\\"HardWorker\\\",\\\"args\\\":[\\\"bob\\\",10,5],\\\"jid\\\":\\\"2b5ad2b016f5e063a1c62872\\\"},\\\"run_at\\\":1361208995}\"\n    Sidekiq.redis do |conn|\n      conn.multi do\n        conn.sadd(\"processes\", key)\n        conn.hmset(key, 'info', Sidekiq.dump_json('hostname' => 'foo', 'started_at' => Time.now.to_f, \"queues\" => []), 'at', Time.now.to_f, 'busy', 4)\n        conn.hmset(\"#{key}:workers\", Time.now.to_f, msg)\n      end\n    end\n  end\n\n  describe 'basic auth' do\n    include Rack::Test::Methods\n\n    def app\n      app = Sidekiq::Web.new\n      app.use(Rack::Auth::Basic) { |user, pass| user == \"a\" && pass == \"b\" }\n      app.use(Rack::Session::Cookie, secret: SecureRandom.hex(32))\n\n      app\n    end\n\n    it 'requires basic authentication' do\n      get '/'\n\n      assert_equal 401, last_response.status\n      refute_nil last_response.header[\"WWW-Authenticate\"]\n    end\n\n    it 'authenticates successfuly' do\n      basic_authorize 'a', 'b'\n\n      get '/'\n      assert_equal 200, last_response.status\n      get '/?days=1000000'\n      assert_equal 401, last_response.status\n    end\n  end\n\n  describe 'custom session' do\n    include Rack::Test::Methods\n\n    def app\n      app = Sidekiq::Web.new\n      app.use Rack::Session::Cookie, secret: 'v3rys3cr31', host: 'nicehost.org'\n      app\n    end\n\n    it 'requires uses session options' do\n      get '/'\n\n      session_options = last_request.env['rack.session'].options\n\n      assert_equal 'v3rys3cr31', session_options[:secret]\n      assert_equal 'nicehost.org', session_options[:host]\n    end\n  end\n\n  describe \"redirecting in before\" do\n    include Rack::Test::Methods\n\n    before do\n      Sidekiq::WebApplication.before { Thread.current[:some_setting] = :before }\n      Sidekiq::WebApplication.before { redirect '/' }\n      Sidekiq::WebApplication.after { Thread.current[:some_setting] = :after }\n    end\n\n    after do\n      Sidekiq::WebApplication.remove_instance_variable(:@befores)\n      Sidekiq::WebApplication.remove_instance_variable(:@afters)\n    end\n\n    def app\n      app = Sidekiq::Web.new\n      app.use Rack::Session::Cookie, secret: 'v3rys3cr31', host: 'nicehost.org'\n      app\n    end\n\n    it \"allows afters to run\" do\n      get '/'\n      assert_equal :after, Thread.current[:some_setting]\n    end\n  end\nend\n"], "filenames": ["lib/sidekiq/api.rb", "lib/sidekiq/web/application.rb", "test/test_api.rb", "test/test_web.rb"], "buggy_code_start_loc": [163, 53, 156, 751], "buggy_code_end_loc": [163, 54, 156, 752], "fixing_code_start_loc": [164, 53, 157, 750], "fixing_code_end_loc": [166, 57, 166, 754], "type": "CWE-770", "message": "In api.rb in Sidekiq before 5.2.10 and 6.4.0, there is no limit on the number of days when requesting stats for the graph. This overloads the system, affecting the Web UI, and makes it unavailable to users.", "other": {"cve": {"id": "CVE-2022-23837", "sourceIdentifier": "cve@mitre.org", "published": "2022-01-21T21:15:09.283", "lastModified": "2023-03-13T00:15:22.160", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "In api.rb in Sidekiq before 5.2.10 and 6.4.0, there is no limit on the number of days when requesting stats for the graph. This overloads the system, affecting the Web UI, and makes it unavailable to users."}, {"lang": "es", "value": "En api.rb en Sidekiq antes de la versi\u00f3n 5.2.10 y 6.4.0, no hay l\u00edmite en el n\u00famero de d\u00edas cuando se solicitan estad\u00edsticas para el gr\u00e1fico. Esto sobrecarga el sistema, afectando a la interfaz web, y hace que no est\u00e9 disponible para los usuarios"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-770"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:contribsys:sidekiq:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.2.10", "matchCriteriaId": "C6B15EF8-61E6-4DED-B338-1BAB5FA46E67"}, {"vulnerable": true, "criteria": "cpe:2.3:a:contribsys:sidekiq:*:*:*:*:*:*:*:*", "versionStartIncluding": "6.0.0", "versionEndExcluding": "6.4.0", "matchCriteriaId": "8B68F3B1-8E41-4B23-9461-0DC222E0A92A"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}], "references": [{"url": "https://github.com/TUTUMSPACE/exploits/blob/main/sidekiq.md", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/mperham/sidekiq/commit/7785ac1399f1b28992adb56055f6acd88fd1d956", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/rubysec/ruby-advisory-db/pull/495", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/03/msg00015.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2023/03/msg00011.html", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/mperham/sidekiq/commit/7785ac1399f1b28992adb56055f6acd88fd1d956"}}