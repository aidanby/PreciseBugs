{"buggy_code": ["#include \"CMOS.h\"\n#include \"Process.h\"\n#include \"StdLib.h\"\n#include <AK/Assertions.h>\n#include <AK/kstdio.h>\n#include <Kernel/Arch/i386/CPU.h>\n#include <Kernel/FileSystem/Inode.h>\n#include <Kernel/Multiboot.h>\n#include <Kernel/VM/AnonymousVMObject.h>\n#include <Kernel/VM/InodeVMObject.h>\n#include <Kernel/VM/MemoryManager.h>\n#include <Kernel/VM/PurgeableVMObject.h>\n\n//#define MM_DEBUG\n//#define PAGE_FAULT_DEBUG\n\nstatic MemoryManager* s_the;\n\nMemoryManager& MM\n{\n    return *s_the;\n}\n\nMemoryManager::MemoryManager(u32 physical_address_for_kernel_page_tables)\n{\n    CPUID id(0x80000001);\n    m_has_nx_support = (id.edx() & (1 << 20)) != 0;\n\n    m_kernel_page_directory = PageDirectory::create_at_fixed_address(PhysicalAddress(physical_address_for_kernel_page_tables));\n    for (size_t i = 0; i < 4; ++i) {\n        m_low_page_tables[i] = (PageTableEntry*)(physical_address_for_kernel_page_tables + PAGE_SIZE * (5 + i));\n        memset(m_low_page_tables[i], 0, PAGE_SIZE);\n    }\n\n    initialize_paging();\n\n    kprintf(\"MM initialized.\\n\");\n}\n\nMemoryManager::~MemoryManager()\n{\n}\n\nvoid MemoryManager::initialize_paging()\n{\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Kernel page directory @ %p\\n\", kernel_page_directory().cr3());\n#endif\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Protect against null dereferences\\n\");\n#endif\n    // Make null dereferences crash.\n    map_protected(VirtualAddress(0), PAGE_SIZE);\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Identity map bottom 8MB\\n\");\n#endif\n    // The bottom 8 MB (except for the null page) are identity mapped & supervisor only.\n    // Every process shares these mappings.\n    create_identity_mapping(kernel_page_directory(), VirtualAddress(PAGE_SIZE), (8 * MB) - PAGE_SIZE);\n\n    // Disable execution from 0MB through 1MB (BIOS data, legacy things, ...)\n    for (size_t i = 0; i < (1 * MB); ++i) {\n        auto& pte = ensure_pte(kernel_page_directory(), VirtualAddress(i));\n        if (m_has_nx_support)\n            pte.set_execute_disabled(true);\n    }\n\n    // Disable execution from 2MB through 8MB (kmalloc, kmalloc_eternal, slabs, page tables, ...)\n    for (size_t i = 1; i < 4; ++i) {\n        auto& pte = kernel_page_directory().table().directory(0)[i];\n        if (m_has_nx_support)\n            pte.set_execute_disabled(true);\n    }\n\n    // FIXME: We should move everything kernel-related above the 0xc0000000 virtual mark.\n\n    // Basic physical memory map:\n    // 0      -> 1 MB           We're just leaving this alone for now.\n    // 1      -> 3 MB           Kernel image.\n    // (last page before 2MB)   Used by quickmap_page().\n    // 2 MB   -> 4 MB           kmalloc_eternal() space.\n    // 4 MB   -> 7 MB           kmalloc() space.\n    // 7 MB   -> 8 MB           Supervisor physical pages (available for allocation!)\n    // 8 MB   -> MAX            Userspace physical pages (available for allocation!)\n\n    // Basic virtual memory map:\n    // 0 -> 4 KB                Null page (so nullptr dereferences crash!)\n    // 4 KB -> 8 MB             Identity mapped.\n    // 8 MB -> 3 GB             Available to userspace.\n    // 3GB  -> 4 GB             Kernel-only virtual address space (>0xc0000000)\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Quickmap will use %p\\n\", m_quickmap_addr.get());\n#endif\n    m_quickmap_addr = VirtualAddress((2 * MB) - PAGE_SIZE);\n\n    RefPtr<PhysicalRegion> region;\n    bool region_is_super = false;\n\n    for (auto* mmap = (multiboot_memory_map_t*)multiboot_info_ptr->mmap_addr; (unsigned long)mmap < multiboot_info_ptr->mmap_addr + multiboot_info_ptr->mmap_length; mmap = (multiboot_memory_map_t*)((unsigned long)mmap + mmap->size + sizeof(mmap->size))) {\n        kprintf(\"MM: Multiboot mmap: base_addr = 0x%x%08x, length = 0x%x%08x, type = 0x%x\\n\",\n            (u32)(mmap->addr >> 32),\n            (u32)(mmap->addr & 0xffffffff),\n            (u32)(mmap->len >> 32),\n            (u32)(mmap->len & 0xffffffff),\n            (u32)mmap->type);\n\n        if (mmap->type != MULTIBOOT_MEMORY_AVAILABLE)\n            continue;\n\n        // FIXME: Maybe make use of stuff below the 1MB mark?\n        if (mmap->addr < (1 * MB))\n            continue;\n\n        if ((mmap->addr + mmap->len) > 0xffffffff)\n            continue;\n\n        auto diff = (u32)mmap->addr % PAGE_SIZE;\n        if (diff != 0) {\n            kprintf(\"MM: got an unaligned region base from the bootloader; correcting %p by %d bytes\\n\", mmap->addr, diff);\n            diff = PAGE_SIZE - diff;\n            mmap->addr += diff;\n            mmap->len -= diff;\n        }\n        if ((mmap->len % PAGE_SIZE) != 0) {\n            kprintf(\"MM: got an unaligned region length from the bootloader; correcting %d by %d bytes\\n\", mmap->len, mmap->len % PAGE_SIZE);\n            mmap->len -= mmap->len % PAGE_SIZE;\n        }\n        if (mmap->len < PAGE_SIZE) {\n            kprintf(\"MM: memory region from bootloader is too small; we want >= %d bytes, but got %d bytes\\n\", PAGE_SIZE, mmap->len);\n            continue;\n        }\n\n#ifdef MM_DEBUG\n        kprintf(\"MM: considering memory at %p - %p\\n\",\n            (u32)mmap->addr, (u32)(mmap->addr + mmap->len));\n#endif\n\n        for (size_t page_base = mmap->addr; page_base < (mmap->addr + mmap->len); page_base += PAGE_SIZE) {\n            auto addr = PhysicalAddress(page_base);\n\n            if (page_base < 7 * MB) {\n                // nothing\n            } else if (page_base >= 7 * MB && page_base < 8 * MB) {\n                if (region.is_null() || !region_is_super || region->upper().offset(PAGE_SIZE) != addr) {\n                    m_super_physical_regions.append(PhysicalRegion::create(addr, addr));\n                    region = m_super_physical_regions.last();\n                    region_is_super = true;\n                } else {\n                    region->expand(region->lower(), addr);\n                }\n            } else {\n                if (region.is_null() || region_is_super || region->upper().offset(PAGE_SIZE) != addr) {\n                    m_user_physical_regions.append(PhysicalRegion::create(addr, addr));\n                    region = m_user_physical_regions.last();\n                    region_is_super = false;\n                } else {\n                    region->expand(region->lower(), addr);\n                }\n            }\n        }\n    }\n\n    for (auto& region : m_super_physical_regions)\n        m_super_physical_pages += region.finalize_capacity();\n\n    for (auto& region : m_user_physical_regions)\n        m_user_physical_pages += region.finalize_capacity();\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Installing page directory\\n\");\n#endif\n\n    // Turn on CR4.PGE so the CPU will respect the G bit in page tables.\n    asm volatile(\n        \"mov %cr4, %eax\\n\"\n        \"orl $0x80, %eax\\n\"\n        \"mov %eax, %cr4\\n\");\n\n    // Turn on CR4.PAE\n    asm volatile(\n        \"mov %cr4, %eax\\n\"\n        \"orl $0x20, %eax\\n\"\n        \"mov %eax, %cr4\\n\");\n\n    if (m_has_nx_support) {\n        kprintf(\"MM: NX support detected; enabling NXE flag\\n\");\n\n        // Turn on IA32_EFER.NXE\n        asm volatile(\n            \"movl $0xc0000080, %ecx\\n\"\n            \"rdmsr\\n\"\n            \"orl $0x800, %eax\\n\"\n            \"wrmsr\\n\");\n    } else {\n        kprintf(\"MM: NX support not detected\\n\");\n    }\n\n    asm volatile(\"movl %%eax, %%cr3\" ::\"a\"(kernel_page_directory().cr3()));\n    asm volatile(\n        \"movl %%cr0, %%eax\\n\"\n        \"orl $0x80010001, %%eax\\n\"\n        \"movl %%eax, %%cr0\\n\" ::\n            : \"%eax\", \"memory\");\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Paging initialized.\\n\");\n#endif\n}\n\nPageTableEntry& MemoryManager::ensure_pte(PageDirectory& page_directory, VirtualAddress vaddr)\n{\n    ASSERT_INTERRUPTS_DISABLED();\n    u32 page_directory_table_index = (vaddr.get() >> 30) & 0x3;\n    u32 page_directory_index = (vaddr.get() >> 21) & 0x1ff;\n    u32 page_table_index = (vaddr.get() >> 12) & 0x1ff;\n\n    PageDirectoryEntry& pde = page_directory.table().directory(page_directory_table_index)[page_directory_index];\n    if (!pde.is_present()) {\n#ifdef MM_DEBUG\n        dbgprintf(\"MM: PDE %u not present (requested for V%p), allocating\\n\", page_directory_index, vaddr.get());\n#endif\n        if (page_directory_table_index == 0 && page_directory_index < 4) {\n            ASSERT(&page_directory == m_kernel_page_directory);\n            pde.set_page_table_base((u32)m_low_page_tables[page_directory_index]);\n            pde.set_user_allowed(false);\n            pde.set_present(true);\n            pde.set_writable(true);\n            pde.set_global(true);\n        } else {\n            auto page_table = allocate_supervisor_physical_page();\n#ifdef MM_DEBUG\n            dbgprintf(\"MM: PD K%p (%s) at P%p allocated page table #%u (for V%p) at P%p\\n\",\n                &page_directory,\n                &page_directory == m_kernel_page_directory ? \"Kernel\" : \"User\",\n                page_directory.cr3(),\n                page_directory_index,\n                vaddr.get(),\n                page_table->paddr().get());\n#endif\n            pde.set_page_table_base(page_table->paddr().get());\n            pde.set_user_allowed(true);\n            pde.set_present(true);\n            pde.set_writable(true);\n            pde.set_global(&page_directory == m_kernel_page_directory.ptr());\n            page_directory.m_physical_pages.set(page_directory_index, move(page_table));\n        }\n    }\n    return pde.page_table_base()[page_table_index];\n}\n\nvoid MemoryManager::map_protected(VirtualAddress vaddr, size_t length)\n{\n    InterruptDisabler disabler;\n    ASSERT(vaddr.is_page_aligned());\n    for (u32 offset = 0; offset < length; offset += PAGE_SIZE) {\n        auto pte_address = vaddr.offset(offset);\n        auto& pte = ensure_pte(kernel_page_directory(), pte_address);\n        pte.set_physical_page_base(pte_address.get());\n        pte.set_user_allowed(false);\n        pte.set_present(false);\n        pte.set_writable(false);\n        flush_tlb(pte_address);\n    }\n}\n\nvoid MemoryManager::create_identity_mapping(PageDirectory& page_directory, VirtualAddress vaddr, size_t size)\n{\n    InterruptDisabler disabler;\n    ASSERT((vaddr.get() & ~PAGE_MASK) == 0);\n    for (u32 offset = 0; offset < size; offset += PAGE_SIZE) {\n        auto pte_address = vaddr.offset(offset);\n        auto& pte = ensure_pte(page_directory, pte_address);\n        pte.set_physical_page_base(pte_address.get());\n        pte.set_user_allowed(false);\n        pte.set_present(true);\n        pte.set_writable(true);\n        page_directory.flush(pte_address);\n    }\n}\n\nvoid MemoryManager::initialize(u32 physical_address_for_kernel_page_tables)\n{\n    s_the = new MemoryManager(physical_address_for_kernel_page_tables);\n}\n\nRegion* MemoryManager::kernel_region_from_vaddr(VirtualAddress vaddr)\n{\n    if (vaddr.get() < 0xc0000000)\n        return nullptr;\n    for (auto& region : MM.m_kernel_regions) {\n        if (region.contains(vaddr))\n            return &region;\n    }\n    return nullptr;\n}\n\nRegion* MemoryManager::user_region_from_vaddr(Process& process, VirtualAddress vaddr)\n{\n    // FIXME: Use a binary search tree (maybe red/black?) or some other more appropriate data structure!\n    for (auto& region : process.m_regions) {\n        if (region.contains(vaddr))\n            return &region;\n    }\n    dbg() << process << \" Couldn't find user region for \" << vaddr;\n    return nullptr;\n}\n\nRegion* MemoryManager::region_from_vaddr(Process& process, VirtualAddress vaddr)\n{\n    if (auto* region = kernel_region_from_vaddr(vaddr))\n        return region;\n    return user_region_from_vaddr(process, vaddr);\n}\n\nconst Region* MemoryManager::region_from_vaddr(const Process& process, VirtualAddress vaddr)\n{\n    if (auto* region = kernel_region_from_vaddr(vaddr))\n        return region;\n    return user_region_from_vaddr(const_cast<Process&>(process), vaddr);\n}\n\nRegion* MemoryManager::region_from_vaddr(VirtualAddress vaddr)\n{\n    if (auto* region = kernel_region_from_vaddr(vaddr))\n        return region;\n    auto page_directory = PageDirectory::find_by_cr3(cpu_cr3());\n    if (!page_directory)\n        return nullptr;\n    ASSERT(page_directory->process());\n    return user_region_from_vaddr(*page_directory->process(), vaddr);\n}\n\nPageFaultResponse MemoryManager::handle_page_fault(const PageFault& fault)\n{\n    ASSERT_INTERRUPTS_DISABLED();\n    ASSERT(current);\n#ifdef PAGE_FAULT_DEBUG\n    dbgprintf(\"MM: handle_page_fault(%w) at V%p\\n\", fault.code(), fault.vaddr().get());\n#endif\n    ASSERT(fault.vaddr() != m_quickmap_addr);\n    auto* region = region_from_vaddr(fault.vaddr());\n    if (!region) {\n        kprintf(\"NP(error) fault at invalid address V%p\\n\", fault.vaddr().get());\n        return PageFaultResponse::ShouldCrash;\n    }\n\n    return region->handle_fault(fault);\n}\n\nOwnPtr<Region> MemoryManager::allocate_kernel_region(size_t size, const StringView& name, u8 access, bool user_accessible, bool should_commit)\n{\n    InterruptDisabler disabler;\n    ASSERT(!(size % PAGE_SIZE));\n    auto range = kernel_page_directory().range_allocator().allocate_anywhere(size);\n    ASSERT(range.is_valid());\n    OwnPtr<Region> region;\n    if (user_accessible)\n        region = Region::create_user_accessible(range, name, access);\n    else\n        region = Region::create_kernel_only(range, name, access);\n    region->map(kernel_page_directory());\n    // FIXME: It would be cool if these could zero-fill on demand instead.\n    if (should_commit)\n        region->commit();\n    return region;\n}\n\nOwnPtr<Region> MemoryManager::allocate_user_accessible_kernel_region(size_t size, const StringView& name, u8 access)\n{\n    return allocate_kernel_region(size, name, access, true);\n}\n\nOwnPtr<Region> MemoryManager::allocate_kernel_region_with_vmobject(VMObject& vmobject, size_t size, const StringView& name, u8 access)\n{\n    InterruptDisabler disabler;\n    ASSERT(!(size % PAGE_SIZE));\n    auto range = kernel_page_directory().range_allocator().allocate_anywhere(size);\n    ASSERT(range.is_valid());\n    auto region = make<Region>(range, vmobject, 0, name, access);\n    region->map(kernel_page_directory());\n    return region;\n}\n\nvoid MemoryManager::deallocate_user_physical_page(PhysicalPage&& page)\n{\n    for (auto& region : m_user_physical_regions) {\n        if (!region.contains(page)) {\n            kprintf(\n                \"MM: deallocate_user_physical_page: %p not in %p -> %p\\n\",\n                page.paddr().get(), region.lower().get(), region.upper().get());\n            continue;\n        }\n\n        region.return_page(move(page));\n        --m_user_physical_pages_used;\n\n        return;\n    }\n\n    kprintf(\"MM: deallocate_user_physical_page couldn't figure out region for user page @ %p\\n\", page.paddr().get());\n    ASSERT_NOT_REACHED();\n}\n\nRefPtr<PhysicalPage> MemoryManager::find_free_user_physical_page()\n{\n    RefPtr<PhysicalPage> page;\n    for (auto& region : m_user_physical_regions) {\n        page = region.take_free_page(false);\n        if (!page.is_null())\n            break;\n    }\n    return page;\n}\n\nRefPtr<PhysicalPage> MemoryManager::allocate_user_physical_page(ShouldZeroFill should_zero_fill)\n{\n    InterruptDisabler disabler;\n    RefPtr<PhysicalPage> page = find_free_user_physical_page();\n\n    if (!page) {\n        if (m_user_physical_regions.is_empty()) {\n            kprintf(\"MM: no user physical regions available (?)\\n\");\n        }\n\n        for_each_vmobject([&](auto& vmobject) {\n            if (vmobject.is_purgeable()) {\n                auto& purgeable_vmobject = static_cast<PurgeableVMObject&>(vmobject);\n                int purged_page_count = purgeable_vmobject.purge_with_interrupts_disabled({});\n                if (purged_page_count) {\n                    kprintf(\"MM: Purge saved the day! Purged %d pages from PurgeableVMObject{%p}\\n\", purged_page_count, &purgeable_vmobject);\n                    page = find_free_user_physical_page();\n                    ASSERT(page);\n                    return IterationDecision::Break;\n                }\n            }\n            return IterationDecision::Continue;\n        });\n\n        if (!page) {\n            kprintf(\"MM: no user physical pages available\\n\");\n            ASSERT_NOT_REACHED();\n            return {};\n        }\n    }\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: allocate_user_physical_page vending P%p\\n\", page->paddr().get());\n#endif\n\n    if (should_zero_fill == ShouldZeroFill::Yes) {\n        auto* ptr = (u32*)quickmap_page(*page);\n        fast_u32_fill(ptr, 0, PAGE_SIZE / sizeof(u32));\n        unquickmap_page();\n    }\n\n    ++m_user_physical_pages_used;\n    return page;\n}\n\nvoid MemoryManager::deallocate_supervisor_physical_page(PhysicalPage&& page)\n{\n    for (auto& region : m_super_physical_regions) {\n        if (!region.contains(page)) {\n            kprintf(\n                \"MM: deallocate_supervisor_physical_page: %p not in %p -> %p\\n\",\n                page.paddr().get(), region.lower().get(), region.upper().get());\n            continue;\n        }\n\n        region.return_page(move(page));\n        --m_super_physical_pages_used;\n        return;\n    }\n\n    kprintf(\"MM: deallocate_supervisor_physical_page couldn't figure out region for super page @ %p\\n\", page.paddr().get());\n    ASSERT_NOT_REACHED();\n}\n\nRefPtr<PhysicalPage> MemoryManager::allocate_supervisor_physical_page()\n{\n    InterruptDisabler disabler;\n    RefPtr<PhysicalPage> page;\n\n    for (auto& region : m_super_physical_regions) {\n        page = region.take_free_page(true);\n        if (page.is_null())\n            continue;\n    }\n\n    if (!page) {\n        if (m_super_physical_regions.is_empty()) {\n            kprintf(\"MM: no super physical regions available (?)\\n\");\n        }\n\n        kprintf(\"MM: no super physical pages available\\n\");\n        ASSERT_NOT_REACHED();\n        return {};\n    }\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: allocate_supervisor_physical_page vending P%p\\n\", page->paddr().get());\n#endif\n\n    fast_u32_fill((u32*)page->paddr().as_ptr(), 0, PAGE_SIZE / sizeof(u32));\n    ++m_super_physical_pages_used;\n    return page;\n}\n\nvoid MemoryManager::enter_process_paging_scope(Process& process)\n{\n    ASSERT(current);\n    InterruptDisabler disabler;\n\n    current->tss().cr3 = process.page_directory().cr3();\n    asm volatile(\"movl %%eax, %%cr3\" ::\"a\"(process.page_directory().cr3())\n                 : \"memory\");\n}\n\nvoid MemoryManager::flush_entire_tlb()\n{\n    asm volatile(\n        \"mov %%cr3, %%eax\\n\"\n        \"mov %%eax, %%cr3\\n\" ::\n            : \"%eax\", \"memory\");\n}\n\nvoid MemoryManager::flush_tlb(VirtualAddress vaddr)\n{\n    asm volatile(\"invlpg %0\"\n                 :\n                 : \"m\"(*(char*)vaddr.get())\n                 : \"memory\");\n}\n\nvoid MemoryManager::map_for_kernel(VirtualAddress vaddr, PhysicalAddress paddr, bool cache_disabled)\n{\n    auto& pte = ensure_pte(kernel_page_directory(), vaddr);\n    pte.set_physical_page_base(paddr.get());\n    pte.set_present(true);\n    pte.set_writable(true);\n    pte.set_user_allowed(false);\n    pte.set_cache_disabled(cache_disabled);\n    flush_tlb(vaddr);\n}\n\nu8* MemoryManager::quickmap_page(PhysicalPage& physical_page)\n{\n    ASSERT_INTERRUPTS_DISABLED();\n    ASSERT(!m_quickmap_in_use);\n    m_quickmap_in_use = true;\n    auto page_vaddr = m_quickmap_addr;\n    auto& pte = ensure_pte(kernel_page_directory(), page_vaddr);\n    pte.set_physical_page_base(physical_page.paddr().get());\n    pte.set_present(true);\n    pte.set_writable(true);\n    pte.set_user_allowed(false);\n    flush_tlb(page_vaddr);\n    ASSERT((u32)pte.physical_page_base() == physical_page.paddr().get());\n#ifdef MM_DEBUG\n    dbg() << \"MM: >> quickmap_page \" << page_vaddr << \" => \" << physical_page.paddr() << \" @ PTE=\" << (void*)pte.raw() << \" {\" << &pte << \"}\";\n#endif\n    return page_vaddr.as_ptr();\n}\n\nvoid MemoryManager::unquickmap_page()\n{\n    ASSERT_INTERRUPTS_DISABLED();\n    ASSERT(m_quickmap_in_use);\n    auto page_vaddr = m_quickmap_addr;\n    auto& pte = ensure_pte(kernel_page_directory(), page_vaddr);\n#ifdef MM_DEBUG\n    auto old_physical_address = pte.physical_page_base();\n#endif\n    pte.set_physical_page_base(0);\n    pte.set_present(false);\n    pte.set_writable(false);\n    flush_tlb(page_vaddr);\n#ifdef MM_DEBUG\n    dbg() << \"MM: >> unquickmap_page \" << page_vaddr << \" =/> \" << old_physical_address;\n#endif\n    m_quickmap_in_use = false;\n}\n\nbool MemoryManager::validate_user_stack(const Process& process, VirtualAddress vaddr) const\n{\n    auto* region = region_from_vaddr(process, vaddr);\n    return region && region->is_stack();\n}\n\nbool MemoryManager::validate_user_read(const Process& process, VirtualAddress vaddr) const\n{\n    auto* region = region_from_vaddr(process, vaddr);\n    return region && region->is_readable();\n}\n\nbool MemoryManager::validate_user_write(const Process& process, VirtualAddress vaddr) const\n{\n    auto* region = region_from_vaddr(process, vaddr);\n    return region && region->is_writable();\n}\n\nvoid MemoryManager::register_vmobject(VMObject& vmobject)\n{\n    InterruptDisabler disabler;\n    m_vmobjects.append(&vmobject);\n}\n\nvoid MemoryManager::unregister_vmobject(VMObject& vmobject)\n{\n    InterruptDisabler disabler;\n    m_vmobjects.remove(&vmobject);\n}\n\nvoid MemoryManager::register_region(Region& region)\n{\n    InterruptDisabler disabler;\n    if (region.vaddr().get() >= 0xc0000000)\n        m_kernel_regions.append(&region);\n    else\n        m_user_regions.append(&region);\n}\n\nvoid MemoryManager::unregister_region(Region& region)\n{\n    InterruptDisabler disabler;\n    if (region.vaddr().get() >= 0xc0000000)\n        m_kernel_regions.remove(&region);\n    else\n        m_user_regions.remove(&region);\n}\n\nProcessPagingScope::ProcessPagingScope(Process& process)\n{\n    ASSERT(current);\n    MM.enter_process_paging_scope(process);\n}\n\nProcessPagingScope::~ProcessPagingScope()\n{\n    MM.enter_process_paging_scope(current->process());\n}\n"], "fixing_code": ["#include \"CMOS.h\"\n#include \"Process.h\"\n#include \"StdLib.h\"\n#include <AK/Assertions.h>\n#include <AK/kstdio.h>\n#include <Kernel/Arch/i386/CPU.h>\n#include <Kernel/FileSystem/Inode.h>\n#include <Kernel/Multiboot.h>\n#include <Kernel/VM/AnonymousVMObject.h>\n#include <Kernel/VM/InodeVMObject.h>\n#include <Kernel/VM/MemoryManager.h>\n#include <Kernel/VM/PurgeableVMObject.h>\n\n//#define MM_DEBUG\n//#define PAGE_FAULT_DEBUG\n\nstatic MemoryManager* s_the;\n\nMemoryManager& MM\n{\n    return *s_the;\n}\n\nMemoryManager::MemoryManager(u32 physical_address_for_kernel_page_tables)\n{\n    CPUID id(0x80000001);\n    m_has_nx_support = (id.edx() & (1 << 20)) != 0;\n\n    m_kernel_page_directory = PageDirectory::create_at_fixed_address(PhysicalAddress(physical_address_for_kernel_page_tables));\n    for (size_t i = 0; i < 4; ++i) {\n        m_low_page_tables[i] = (PageTableEntry*)(physical_address_for_kernel_page_tables + PAGE_SIZE * (5 + i));\n        memset(m_low_page_tables[i], 0, PAGE_SIZE);\n    }\n\n    initialize_paging();\n\n    kprintf(\"MM initialized.\\n\");\n}\n\nMemoryManager::~MemoryManager()\n{\n}\n\nvoid MemoryManager::initialize_paging()\n{\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Kernel page directory @ %p\\n\", kernel_page_directory().cr3());\n#endif\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Protect against null dereferences\\n\");\n#endif\n    // Make null dereferences crash.\n    map_protected(VirtualAddress(0), PAGE_SIZE);\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Identity map bottom 8MB\\n\");\n#endif\n    // The bottom 8 MB (except for the null page) are identity mapped & supervisor only.\n    // Every process shares these mappings.\n    create_identity_mapping(kernel_page_directory(), VirtualAddress(PAGE_SIZE), (8 * MB) - PAGE_SIZE);\n\n    // Disable execution from 0MB through 1MB (BIOS data, legacy things, ...)\n    for (size_t i = 0; i < (1 * MB); ++i) {\n        auto& pte = ensure_pte(kernel_page_directory(), VirtualAddress(i));\n        if (m_has_nx_support)\n            pte.set_execute_disabled(true);\n    }\n\n    // Disable execution from 2MB through 8MB (kmalloc, kmalloc_eternal, slabs, page tables, ...)\n    for (size_t i = 1; i < 4; ++i) {\n        auto& pte = kernel_page_directory().table().directory(0)[i];\n        if (m_has_nx_support)\n            pte.set_execute_disabled(true);\n    }\n\n    // FIXME: We should move everything kernel-related above the 0xc0000000 virtual mark.\n\n    // Basic physical memory map:\n    // 0      -> 1 MB           We're just leaving this alone for now.\n    // 1      -> 3 MB           Kernel image.\n    // (last page before 2MB)   Used by quickmap_page().\n    // 2 MB   -> 4 MB           kmalloc_eternal() space.\n    // 4 MB   -> 7 MB           kmalloc() space.\n    // 7 MB   -> 8 MB           Supervisor physical pages (available for allocation!)\n    // 8 MB   -> MAX            Userspace physical pages (available for allocation!)\n\n    // Basic virtual memory map:\n    // 0 -> 4 KB                Null page (so nullptr dereferences crash!)\n    // 4 KB -> 8 MB             Identity mapped.\n    // 8 MB -> 3 GB             Available to userspace.\n    // 3GB  -> 4 GB             Kernel-only virtual address space (>0xc0000000)\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Quickmap will use %p\\n\", m_quickmap_addr.get());\n#endif\n    m_quickmap_addr = VirtualAddress((2 * MB) - PAGE_SIZE);\n\n    RefPtr<PhysicalRegion> region;\n    bool region_is_super = false;\n\n    for (auto* mmap = (multiboot_memory_map_t*)multiboot_info_ptr->mmap_addr; (unsigned long)mmap < multiboot_info_ptr->mmap_addr + multiboot_info_ptr->mmap_length; mmap = (multiboot_memory_map_t*)((unsigned long)mmap + mmap->size + sizeof(mmap->size))) {\n        kprintf(\"MM: Multiboot mmap: base_addr = 0x%x%08x, length = 0x%x%08x, type = 0x%x\\n\",\n            (u32)(mmap->addr >> 32),\n            (u32)(mmap->addr & 0xffffffff),\n            (u32)(mmap->len >> 32),\n            (u32)(mmap->len & 0xffffffff),\n            (u32)mmap->type);\n\n        if (mmap->type != MULTIBOOT_MEMORY_AVAILABLE)\n            continue;\n\n        // FIXME: Maybe make use of stuff below the 1MB mark?\n        if (mmap->addr < (1 * MB))\n            continue;\n\n        if ((mmap->addr + mmap->len) > 0xffffffff)\n            continue;\n\n        auto diff = (u32)mmap->addr % PAGE_SIZE;\n        if (diff != 0) {\n            kprintf(\"MM: got an unaligned region base from the bootloader; correcting %p by %d bytes\\n\", mmap->addr, diff);\n            diff = PAGE_SIZE - diff;\n            mmap->addr += diff;\n            mmap->len -= diff;\n        }\n        if ((mmap->len % PAGE_SIZE) != 0) {\n            kprintf(\"MM: got an unaligned region length from the bootloader; correcting %d by %d bytes\\n\", mmap->len, mmap->len % PAGE_SIZE);\n            mmap->len -= mmap->len % PAGE_SIZE;\n        }\n        if (mmap->len < PAGE_SIZE) {\n            kprintf(\"MM: memory region from bootloader is too small; we want >= %d bytes, but got %d bytes\\n\", PAGE_SIZE, mmap->len);\n            continue;\n        }\n\n#ifdef MM_DEBUG\n        kprintf(\"MM: considering memory at %p - %p\\n\",\n            (u32)mmap->addr, (u32)(mmap->addr + mmap->len));\n#endif\n\n        for (size_t page_base = mmap->addr; page_base < (mmap->addr + mmap->len); page_base += PAGE_SIZE) {\n            auto addr = PhysicalAddress(page_base);\n\n            if (page_base < 7 * MB) {\n                // nothing\n            } else if (page_base >= 7 * MB && page_base < 8 * MB) {\n                if (region.is_null() || !region_is_super || region->upper().offset(PAGE_SIZE) != addr) {\n                    m_super_physical_regions.append(PhysicalRegion::create(addr, addr));\n                    region = m_super_physical_regions.last();\n                    region_is_super = true;\n                } else {\n                    region->expand(region->lower(), addr);\n                }\n            } else {\n                if (region.is_null() || region_is_super || region->upper().offset(PAGE_SIZE) != addr) {\n                    m_user_physical_regions.append(PhysicalRegion::create(addr, addr));\n                    region = m_user_physical_regions.last();\n                    region_is_super = false;\n                } else {\n                    region->expand(region->lower(), addr);\n                }\n            }\n        }\n    }\n\n    for (auto& region : m_super_physical_regions)\n        m_super_physical_pages += region.finalize_capacity();\n\n    for (auto& region : m_user_physical_regions)\n        m_user_physical_pages += region.finalize_capacity();\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Installing page directory\\n\");\n#endif\n\n    // Turn on CR4.PGE so the CPU will respect the G bit in page tables.\n    asm volatile(\n        \"mov %cr4, %eax\\n\"\n        \"orl $0x80, %eax\\n\"\n        \"mov %eax, %cr4\\n\");\n\n    // Turn on CR4.PAE\n    asm volatile(\n        \"mov %cr4, %eax\\n\"\n        \"orl $0x20, %eax\\n\"\n        \"mov %eax, %cr4\\n\");\n\n    if (m_has_nx_support) {\n        kprintf(\"MM: NX support detected; enabling NXE flag\\n\");\n\n        // Turn on IA32_EFER.NXE\n        asm volatile(\n            \"movl $0xc0000080, %ecx\\n\"\n            \"rdmsr\\n\"\n            \"orl $0x800, %eax\\n\"\n            \"wrmsr\\n\");\n    } else {\n        kprintf(\"MM: NX support not detected\\n\");\n    }\n\n    asm volatile(\"movl %%eax, %%cr3\" ::\"a\"(kernel_page_directory().cr3()));\n    asm volatile(\n        \"movl %%cr0, %%eax\\n\"\n        \"orl $0x80010001, %%eax\\n\"\n        \"movl %%eax, %%cr0\\n\" ::\n            : \"%eax\", \"memory\");\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: Paging initialized.\\n\");\n#endif\n}\n\nPageTableEntry& MemoryManager::ensure_pte(PageDirectory& page_directory, VirtualAddress vaddr)\n{\n    ASSERT_INTERRUPTS_DISABLED();\n    u32 page_directory_table_index = (vaddr.get() >> 30) & 0x3;\n    u32 page_directory_index = (vaddr.get() >> 21) & 0x1ff;\n    u32 page_table_index = (vaddr.get() >> 12) & 0x1ff;\n\n    PageDirectoryEntry& pde = page_directory.table().directory(page_directory_table_index)[page_directory_index];\n    if (!pde.is_present()) {\n#ifdef MM_DEBUG\n        dbgprintf(\"MM: PDE %u not present (requested for V%p), allocating\\n\", page_directory_index, vaddr.get());\n#endif\n        if (page_directory_table_index == 0 && page_directory_index < 4) {\n            ASSERT(&page_directory == m_kernel_page_directory);\n            pde.set_page_table_base((u32)m_low_page_tables[page_directory_index]);\n            pde.set_user_allowed(false);\n            pde.set_present(true);\n            pde.set_writable(true);\n            pde.set_global(true);\n        } else {\n            auto page_table = allocate_supervisor_physical_page();\n#ifdef MM_DEBUG\n            dbgprintf(\"MM: PD K%p (%s) at P%p allocated page table #%u (for V%p) at P%p\\n\",\n                &page_directory,\n                &page_directory == m_kernel_page_directory ? \"Kernel\" : \"User\",\n                page_directory.cr3(),\n                page_directory_index,\n                vaddr.get(),\n                page_table->paddr().get());\n#endif\n            pde.set_page_table_base(page_table->paddr().get());\n            pde.set_user_allowed(true);\n            pde.set_present(true);\n            pde.set_writable(true);\n            pde.set_global(&page_directory == m_kernel_page_directory.ptr());\n            page_directory.m_physical_pages.set(page_directory_index, move(page_table));\n        }\n    }\n    return pde.page_table_base()[page_table_index];\n}\n\nvoid MemoryManager::map_protected(VirtualAddress vaddr, size_t length)\n{\n    InterruptDisabler disabler;\n    ASSERT(vaddr.is_page_aligned());\n    for (u32 offset = 0; offset < length; offset += PAGE_SIZE) {\n        auto pte_address = vaddr.offset(offset);\n        auto& pte = ensure_pte(kernel_page_directory(), pte_address);\n        pte.set_physical_page_base(pte_address.get());\n        pte.set_user_allowed(false);\n        pte.set_present(false);\n        pte.set_writable(false);\n        flush_tlb(pte_address);\n    }\n}\n\nvoid MemoryManager::create_identity_mapping(PageDirectory& page_directory, VirtualAddress vaddr, size_t size)\n{\n    InterruptDisabler disabler;\n    ASSERT((vaddr.get() & ~PAGE_MASK) == 0);\n    for (u32 offset = 0; offset < size; offset += PAGE_SIZE) {\n        auto pte_address = vaddr.offset(offset);\n        auto& pte = ensure_pte(page_directory, pte_address);\n        pte.set_physical_page_base(pte_address.get());\n        pte.set_user_allowed(false);\n        pte.set_present(true);\n        pte.set_writable(true);\n        page_directory.flush(pte_address);\n    }\n}\n\nvoid MemoryManager::initialize(u32 physical_address_for_kernel_page_tables)\n{\n    s_the = new MemoryManager(physical_address_for_kernel_page_tables);\n}\n\nRegion* MemoryManager::kernel_region_from_vaddr(VirtualAddress vaddr)\n{\n    if (vaddr.get() < 0xc0000000)\n        return nullptr;\n    for (auto& region : MM.m_kernel_regions) {\n        if (region.contains(vaddr))\n            return &region;\n    }\n    return nullptr;\n}\n\nRegion* MemoryManager::user_region_from_vaddr(Process& process, VirtualAddress vaddr)\n{\n    // FIXME: Use a binary search tree (maybe red/black?) or some other more appropriate data structure!\n    for (auto& region : process.m_regions) {\n        if (region.contains(vaddr))\n            return &region;\n    }\n    dbg() << process << \" Couldn't find user region for \" << vaddr;\n    return nullptr;\n}\n\nRegion* MemoryManager::region_from_vaddr(Process& process, VirtualAddress vaddr)\n{\n    if (auto* region = kernel_region_from_vaddr(vaddr))\n        return region;\n    return user_region_from_vaddr(process, vaddr);\n}\n\nconst Region* MemoryManager::region_from_vaddr(const Process& process, VirtualAddress vaddr)\n{\n    if (auto* region = kernel_region_from_vaddr(vaddr))\n        return region;\n    return user_region_from_vaddr(const_cast<Process&>(process), vaddr);\n}\n\nRegion* MemoryManager::region_from_vaddr(VirtualAddress vaddr)\n{\n    if (auto* region = kernel_region_from_vaddr(vaddr))\n        return region;\n    auto page_directory = PageDirectory::find_by_cr3(cpu_cr3());\n    if (!page_directory)\n        return nullptr;\n    ASSERT(page_directory->process());\n    return user_region_from_vaddr(*page_directory->process(), vaddr);\n}\n\nPageFaultResponse MemoryManager::handle_page_fault(const PageFault& fault)\n{\n    ASSERT_INTERRUPTS_DISABLED();\n    ASSERT(current);\n#ifdef PAGE_FAULT_DEBUG\n    dbgprintf(\"MM: handle_page_fault(%w) at V%p\\n\", fault.code(), fault.vaddr().get());\n#endif\n    ASSERT(fault.vaddr() != m_quickmap_addr);\n    auto* region = region_from_vaddr(fault.vaddr());\n    if (!region) {\n        kprintf(\"NP(error) fault at invalid address V%p\\n\", fault.vaddr().get());\n        return PageFaultResponse::ShouldCrash;\n    }\n\n    return region->handle_fault(fault);\n}\n\nOwnPtr<Region> MemoryManager::allocate_kernel_region(size_t size, const StringView& name, u8 access, bool user_accessible, bool should_commit)\n{\n    InterruptDisabler disabler;\n    ASSERT(!(size % PAGE_SIZE));\n    auto range = kernel_page_directory().range_allocator().allocate_anywhere(size);\n    ASSERT(range.is_valid());\n    OwnPtr<Region> region;\n    if (user_accessible)\n        region = Region::create_user_accessible(range, name, access);\n    else\n        region = Region::create_kernel_only(range, name, access);\n    region->map(kernel_page_directory());\n    // FIXME: It would be cool if these could zero-fill on demand instead.\n    if (should_commit)\n        region->commit();\n    return region;\n}\n\nOwnPtr<Region> MemoryManager::allocate_user_accessible_kernel_region(size_t size, const StringView& name, u8 access)\n{\n    return allocate_kernel_region(size, name, access, true);\n}\n\nOwnPtr<Region> MemoryManager::allocate_kernel_region_with_vmobject(VMObject& vmobject, size_t size, const StringView& name, u8 access)\n{\n    InterruptDisabler disabler;\n    ASSERT(!(size % PAGE_SIZE));\n    auto range = kernel_page_directory().range_allocator().allocate_anywhere(size);\n    ASSERT(range.is_valid());\n    auto region = make<Region>(range, vmobject, 0, name, access);\n    region->map(kernel_page_directory());\n    return region;\n}\n\nvoid MemoryManager::deallocate_user_physical_page(PhysicalPage&& page)\n{\n    for (auto& region : m_user_physical_regions) {\n        if (!region.contains(page)) {\n            kprintf(\n                \"MM: deallocate_user_physical_page: %p not in %p -> %p\\n\",\n                page.paddr().get(), region.lower().get(), region.upper().get());\n            continue;\n        }\n\n        region.return_page(move(page));\n        --m_user_physical_pages_used;\n\n        return;\n    }\n\n    kprintf(\"MM: deallocate_user_physical_page couldn't figure out region for user page @ %p\\n\", page.paddr().get());\n    ASSERT_NOT_REACHED();\n}\n\nRefPtr<PhysicalPage> MemoryManager::find_free_user_physical_page()\n{\n    RefPtr<PhysicalPage> page;\n    for (auto& region : m_user_physical_regions) {\n        page = region.take_free_page(false);\n        if (!page.is_null())\n            break;\n    }\n    return page;\n}\n\nRefPtr<PhysicalPage> MemoryManager::allocate_user_physical_page(ShouldZeroFill should_zero_fill)\n{\n    InterruptDisabler disabler;\n    RefPtr<PhysicalPage> page = find_free_user_physical_page();\n\n    if (!page) {\n        if (m_user_physical_regions.is_empty()) {\n            kprintf(\"MM: no user physical regions available (?)\\n\");\n        }\n\n        for_each_vmobject([&](auto& vmobject) {\n            if (vmobject.is_purgeable()) {\n                auto& purgeable_vmobject = static_cast<PurgeableVMObject&>(vmobject);\n                int purged_page_count = purgeable_vmobject.purge_with_interrupts_disabled({});\n                if (purged_page_count) {\n                    kprintf(\"MM: Purge saved the day! Purged %d pages from PurgeableVMObject{%p}\\n\", purged_page_count, &purgeable_vmobject);\n                    page = find_free_user_physical_page();\n                    ASSERT(page);\n                    return IterationDecision::Break;\n                }\n            }\n            return IterationDecision::Continue;\n        });\n\n        if (!page) {\n            kprintf(\"MM: no user physical pages available\\n\");\n            ASSERT_NOT_REACHED();\n            return {};\n        }\n    }\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: allocate_user_physical_page vending P%p\\n\", page->paddr().get());\n#endif\n\n    if (should_zero_fill == ShouldZeroFill::Yes) {\n        auto* ptr = (u32*)quickmap_page(*page);\n        fast_u32_fill(ptr, 0, PAGE_SIZE / sizeof(u32));\n        unquickmap_page();\n    }\n\n    ++m_user_physical_pages_used;\n    return page;\n}\n\nvoid MemoryManager::deallocate_supervisor_physical_page(PhysicalPage&& page)\n{\n    for (auto& region : m_super_physical_regions) {\n        if (!region.contains(page)) {\n            kprintf(\n                \"MM: deallocate_supervisor_physical_page: %p not in %p -> %p\\n\",\n                page.paddr().get(), region.lower().get(), region.upper().get());\n            continue;\n        }\n\n        region.return_page(move(page));\n        --m_super_physical_pages_used;\n        return;\n    }\n\n    kprintf(\"MM: deallocate_supervisor_physical_page couldn't figure out region for super page @ %p\\n\", page.paddr().get());\n    ASSERT_NOT_REACHED();\n}\n\nRefPtr<PhysicalPage> MemoryManager::allocate_supervisor_physical_page()\n{\n    InterruptDisabler disabler;\n    RefPtr<PhysicalPage> page;\n\n    for (auto& region : m_super_physical_regions) {\n        page = region.take_free_page(true);\n        if (page.is_null())\n            continue;\n    }\n\n    if (!page) {\n        if (m_super_physical_regions.is_empty()) {\n            kprintf(\"MM: no super physical regions available (?)\\n\");\n        }\n\n        kprintf(\"MM: no super physical pages available\\n\");\n        ASSERT_NOT_REACHED();\n        return {};\n    }\n\n#ifdef MM_DEBUG\n    dbgprintf(\"MM: allocate_supervisor_physical_page vending P%p\\n\", page->paddr().get());\n#endif\n\n    fast_u32_fill((u32*)page->paddr().as_ptr(), 0, PAGE_SIZE / sizeof(u32));\n    ++m_super_physical_pages_used;\n    return page;\n}\n\nvoid MemoryManager::enter_process_paging_scope(Process& process)\n{\n    ASSERT(current);\n    InterruptDisabler disabler;\n\n    current->tss().cr3 = process.page_directory().cr3();\n    asm volatile(\"movl %%eax, %%cr3\" ::\"a\"(process.page_directory().cr3())\n                 : \"memory\");\n}\n\nvoid MemoryManager::flush_entire_tlb()\n{\n    asm volatile(\n        \"mov %%cr3, %%eax\\n\"\n        \"mov %%eax, %%cr3\\n\" ::\n            : \"%eax\", \"memory\");\n}\n\nvoid MemoryManager::flush_tlb(VirtualAddress vaddr)\n{\n    asm volatile(\"invlpg %0\"\n                 :\n                 : \"m\"(*(char*)vaddr.get())\n                 : \"memory\");\n}\n\nvoid MemoryManager::map_for_kernel(VirtualAddress vaddr, PhysicalAddress paddr, bool cache_disabled)\n{\n    auto& pte = ensure_pte(kernel_page_directory(), vaddr);\n    pte.set_physical_page_base(paddr.get());\n    pte.set_present(true);\n    pte.set_writable(true);\n    pte.set_user_allowed(false);\n    pte.set_cache_disabled(cache_disabled);\n    flush_tlb(vaddr);\n}\n\nu8* MemoryManager::quickmap_page(PhysicalPage& physical_page)\n{\n    ASSERT_INTERRUPTS_DISABLED();\n    ASSERT(!m_quickmap_in_use);\n    m_quickmap_in_use = true;\n    auto page_vaddr = m_quickmap_addr;\n    auto& pte = ensure_pte(kernel_page_directory(), page_vaddr);\n    pte.set_physical_page_base(physical_page.paddr().get());\n    pte.set_present(true);\n    pte.set_writable(true);\n    pte.set_user_allowed(false);\n    flush_tlb(page_vaddr);\n    ASSERT((u32)pte.physical_page_base() == physical_page.paddr().get());\n#ifdef MM_DEBUG\n    dbg() << \"MM: >> quickmap_page \" << page_vaddr << \" => \" << physical_page.paddr() << \" @ PTE=\" << (void*)pte.raw() << \" {\" << &pte << \"}\";\n#endif\n    return page_vaddr.as_ptr();\n}\n\nvoid MemoryManager::unquickmap_page()\n{\n    ASSERT_INTERRUPTS_DISABLED();\n    ASSERT(m_quickmap_in_use);\n    auto page_vaddr = m_quickmap_addr;\n    auto& pte = ensure_pte(kernel_page_directory(), page_vaddr);\n#ifdef MM_DEBUG\n    auto old_physical_address = pte.physical_page_base();\n#endif\n    pte.set_physical_page_base(0);\n    pte.set_present(false);\n    pte.set_writable(false);\n    flush_tlb(page_vaddr);\n#ifdef MM_DEBUG\n    dbg() << \"MM: >> unquickmap_page \" << page_vaddr << \" =/> \" << old_physical_address;\n#endif\n    m_quickmap_in_use = false;\n}\n\nbool MemoryManager::validate_user_stack(const Process& process, VirtualAddress vaddr) const\n{\n    auto* region = region_from_vaddr(process, vaddr);\n    return region && region->is_stack();\n}\n\nbool MemoryManager::validate_user_read(const Process& process, VirtualAddress vaddr) const\n{\n    auto* region = user_region_from_vaddr(const_cast<Process&>(process), vaddr);\n    return region && region->is_user_accessible() && region->is_readable();\n}\n\nbool MemoryManager::validate_user_write(const Process& process, VirtualAddress vaddr) const\n{\n    auto* region = user_region_from_vaddr(const_cast<Process&>(process), vaddr);\n    return region && region->is_user_accessible() && region->is_writable();\n}\n\nvoid MemoryManager::register_vmobject(VMObject& vmobject)\n{\n    InterruptDisabler disabler;\n    m_vmobjects.append(&vmobject);\n}\n\nvoid MemoryManager::unregister_vmobject(VMObject& vmobject)\n{\n    InterruptDisabler disabler;\n    m_vmobjects.remove(&vmobject);\n}\n\nvoid MemoryManager::register_region(Region& region)\n{\n    InterruptDisabler disabler;\n    if (region.vaddr().get() >= 0xc0000000)\n        m_kernel_regions.append(&region);\n    else\n        m_user_regions.append(&region);\n}\n\nvoid MemoryManager::unregister_region(Region& region)\n{\n    InterruptDisabler disabler;\n    if (region.vaddr().get() >= 0xc0000000)\n        m_kernel_regions.remove(&region);\n    else\n        m_user_regions.remove(&region);\n}\n\nProcessPagingScope::ProcessPagingScope(Process& process)\n{\n    ASSERT(current);\n    MM.enter_process_paging_scope(process);\n}\n\nProcessPagingScope::~ProcessPagingScope()\n{\n    MM.enter_process_paging_scope(current->process());\n}\n"], "filenames": ["Kernel/VM/MemoryManager.cpp"], "buggy_code_start_loc": [595], "buggy_code_end_loc": [603], "fixing_code_start_loc": [595], "fixing_code_end_loc": [603], "type": "CWE-119", "message": "Kernel/VM/MemoryManager.cpp in SerenityOS before 2019-12-30 does not reject syscalls with pointers into the kernel-only virtual address space, which allows local users to gain privileges by overwriting a return address that was found on the kernel stack.", "other": {"cve": {"id": "CVE-2019-20172", "sourceIdentifier": "cve@mitre.org", "published": "2019-12-31T03:15:10.467", "lastModified": "2022-12-08T18:55:41.790", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Kernel/VM/MemoryManager.cpp in SerenityOS before 2019-12-30 does not reject syscalls with pointers into the kernel-only virtual address space, which allows local users to gain privileges by overwriting a return address that was found on the kernel stack."}, {"lang": "es", "value": "El archivo Kernel/VM/MemoryManager.cpp en SerenityOS antes de 30-12-2019 no rechaza las llamadas al sistema con punteros en el espacio de direcciones virtuales solo del kernel, lo que permite a usuarios locales alcanzar privilegios sobrescribiendo una direcci\u00f3n de retorno que fue encontrada en la pila del kernel."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:serenityos:serenityos:*:*:*:*:*:*:*:*", "versionEndExcluding": "2019-12-30", "matchCriteriaId": "5EA34FF9-B6AD-4751-86A8-7D8ED9EC6DA0"}]}]}], "references": [{"url": "https://github.com/Fire30/CTF-WRITEUPS/tree/master/36c3_ctf/wisdom", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/SerenityOS/serenity/commit/0fc24fe2564736689859e7edfa177a86dac36bf9", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/SerenityOS/serenity/commit/0fc24fe2564736689859e7edfa177a86dac36bf9"}}