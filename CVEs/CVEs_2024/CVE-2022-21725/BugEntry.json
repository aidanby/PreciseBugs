{"buggy_code": ["load(\"//tensorflow:tensorflow.bzl\", \"filegroup\")\nload(\"//tensorflow/core/platform:rules_cc.bzl\", \"cc_library\")\nload(\n    \"//tensorflow:tensorflow.bzl\",\n    \"tf_cc_test\",\n    \"tf_cuda_library\",\n)\nload(\n    \"//tensorflow/core/platform:build_config.bzl\",\n    \"tf_proto_library\",\n    \"tf_protos_grappler\",\n    \"tf_pyclif_proto_library\",\n)\n\npackage(\n    licenses = [\"notice\"],\n)\n\nalias(\n    name = \"graph_properties_testdata\",\n    actual = \"//tensorflow/core/grappler/costs/graph_properties_testdata:graph_properties_testdata\",\n    visibility = [\"//visibility:public\"],\n)\n\ntf_proto_library(\n    name = \"op_performance_data\",\n    srcs = [\"op_performance_data.proto\"],\n    cc_api_version = 2,\n    make_default_target_header_only = True,\n    protodeps = [\n        \"//tensorflow/core/framework:attr_value_proto\",\n        \"//tensorflow/core/framework:resource_handle_proto\",\n        \"//tensorflow/core/framework:tensor_proto\",\n        \"//tensorflow/core/framework:tensor_shape_proto\",\n        \"//tensorflow/core/framework:types_proto\",\n        \"//tensorflow/core/protobuf:for_core_protos\",\n    ],\n    visibility = [\"//visibility:public\"],\n)\n\ntf_pyclif_proto_library(\n    name = \"op_performance_data_pyclif\",\n    proto_lib = \":op_performance_data\",\n    proto_srcfile = \"op_performance_data.proto\",\n    visibility = [\"//visibility:public\"],\n)\n\nfilegroup(\n    name = \"pywrap_required_hdrs\",\n    srcs = [\n        \"analytical_cost_estimator.h\",\n        \"cost_estimator.h\",\n        \"graph_memory.h\",\n        \"graph_properties.h\",\n        \"measuring_cost_estimator.h\",\n        \"op_context.h\",\n        \"op_level_cost_estimator.h\",\n        \"utils.h\",\n        \"virtual_placer.h\",\n        \"virtual_scheduler.h\",\n    ],\n    visibility = [\n        \"//tensorflow/python/grappler:__pkg__\",\n    ],\n)\n\ncc_library(\n    name = \"graph_properties\",\n    srcs = [\"graph_properties.cc\"],\n    hdrs = [\"graph_properties.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":utils\",\n        \"@com_google_absl//absl/container:flat_hash_map\",\n        \"@com_google_absl//absl/types:optional\",\n        \"//tensorflow/core/grappler/utils:functions\",\n        \"//tensorflow/core/grappler/utils:topological_sort\",\n        \"//tensorflow/core/grappler:mutable_graph_view\",\n        \"//tensorflow/core/grappler:op_types\",\n        \"//tensorflow/core:core_cpu_base\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/grappler/clusters:cluster\",\n        \"//tensorflow/core/grappler/optimizers:evaluation_utils\",\n    ] + tf_protos_grappler(),\n)\n\ntf_cc_test(\n    name = \"graph_properties_test\",\n    srcs = [\"graph_properties_test.cc\"],\n    args = [\"--heap_check=\"],  # The GPU tracer leaks memory. TODO(b/185483595): use a dependency instead of a flag\n    data = [\":graph_properties_testdata\"],\n    tags = [\n        \"nomsan\",  # TODO(b/160921160): broken by NOAUTOROLLBACK CL\n    ],\n    deps = [\n        \":graph_properties\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/cc:functional_ops\",\n        \"//tensorflow/cc:scope\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:lib_proto_parsing\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/framework:tensor_testutil\",\n        \"//tensorflow/core/graph:mkl_graph_util\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler/clusters:single_machine\",\n        \"//tensorflow/core/grappler/inputs:trivial_test_graph_input_yielder\",\n        \"//tensorflow/core/grappler/inputs:utils\",\n    ],\n)\n\ncc_library(\n    name = \"graph_memory\",\n    srcs = [\"graph_memory.cc\"],\n    hdrs = [\"graph_memory.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \":graph_properties\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/grappler/clusters:cluster\",\n        \"//tensorflow/core/grappler/clusters:virtual_cluster\",\n    ],\n)\n\ntf_cc_test(\n    name = \"graph_memory_test\",\n    srcs = [\"graph_memory_test.cc\"],\n    args = [\"--heap_check=\"],  # The GPU tracer leaks memory. TODO(b/185483595): use a dependency instead of a flag\n    deps = [\n        \":graph_memory\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler/inputs:trivial_test_graph_input_yielder\",\n    ],\n)\n\ncc_library(\n    name = \"robust_stats\",\n    srcs = [\"robust_stats.cc\"],\n    hdrs = [\"robust_stats.h\"],\n    visibility = [\"//visibility:public\"],\n)\n\ntf_cc_test(\n    name = \"robust_stats_test\",\n    srcs = [\"robust_stats_test.cc\"],\n    deps = [\n        \":robust_stats\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n    ],\n)\n\ntf_cuda_library(\n    name = \"utils\",\n    srcs = [\"utils.cc\"],\n    hdrs = [\"utils.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \"//third_party/eigen3\",\n        \"@com_google_absl//absl/container:node_hash_map\",\n        \"@com_google_absl//absl/strings\",\n        \"@com_google_absl//absl/strings:str_format\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:graph\",\n        \"//tensorflow/core/common_runtime/gpu:gpu_id\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:lib_proto_parsing\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/util:overflow\",\n        \"//tensorflow/core/grappler/clusters:utils\",\n    ] + tf_protos_grappler(),\n)\n\ntf_cc_test(\n    name = \"utils_test\",\n    srcs = [\"utils_test.cc\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":utils\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/core:all_kernels\",\n        \"//tensorflow/core:core_cpu_internal\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core:testlib\",\n        \"//tensorflow/core/framework:tensor_testutil\",\n    ],\n)\n\ncc_library(\n    name = \"cost_estimator\",\n    srcs = [\"cost_estimator.cc\"],\n    hdrs = [\"cost_estimator.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n    ],\n)\n\ntf_cc_test(\n    name = \"cost_estimator_test\",\n    srcs = [\"cost_estimator_test.cc\"],\n    deps = [\n        \":cost_estimator\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n    ],\n)\n\ncc_library(\n    name = \"virtual_placer\",\n    srcs = [\"virtual_placer.cc\"],\n    hdrs = [\"virtual_placer.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:devices\",\n        \"//tensorflow/core/grappler/clusters:cluster\",\n    ],\n)\n\ntf_cc_test(\n    name = \"virtual_placer_test\",\n    srcs = [\"virtual_placer_test.cc\"],\n    deps = [\n        \":virtual_placer\",\n        \"//tensorflow/core:core_cpu\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/grappler/clusters:virtual_cluster\",\n    ],\n)\n\ncc_library(\n    name = \"op_context\",\n    hdrs = [\"op_context.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"@com_google_absl//absl/container:flat_hash_map\",\n        \"//tensorflow/core:protos_all_cc\",\n    ] + tf_protos_grappler(),\n)\n\ncc_library(\n    name = \"virtual_scheduler\",\n    srcs = [\"virtual_scheduler.cc\"],\n    hdrs = [\"virtual_scheduler.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \":graph_properties\",\n        \":op_context\",\n        \":utils\",\n        \":virtual_placer\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler:op_types\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/grappler/clusters:utils\",\n        \"//tensorflow/core/grappler/utils:transitive_fanin\",\n        \"@com_google_absl//absl/strings\",\n        \"@com_google_absl//absl/strings:str_format\",\n    ],\n)\n\ntf_cc_test(\n    name = \"virtual_scheduler_test\",\n    srcs = [\"virtual_scheduler_test.cc\"],\n    deps = [\n        \":graph_properties\",\n        \":utils\",\n        \":virtual_placer\",\n        \":virtual_scheduler\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:tensorflow\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/grappler/clusters:virtual_cluster\",\n    ],\n)\n\ncc_library(\n    name = \"measuring_cost_estimator\",\n    srcs = [\"measuring_cost_estimator.cc\"],\n    hdrs = [\"measuring_cost_estimator.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":robust_stats\",\n        \"//tensorflow/core:core_cpu\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:lib_internal\",\n        \"//tensorflow/core:lib_proto_parsing\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler/clusters:cluster\",\n        \"//tensorflow/core/grappler/costs:cost_estimator\",\n        \"//tensorflow/core/kernels:ops_util\",\n    ],\n    alwayslink = 1,\n)\n\ncc_library(\n    name = \"op_level_cost_estimator\",\n    srcs = [\"op_level_cost_estimator.cc\"],\n    hdrs = [\"op_level_cost_estimator.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \":op_context\",\n        \":utils\",\n        \"@com_google_absl//absl/strings\",\n        \"//third_party/eigen3\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler/clusters:utils\",\n        \"//tensorflow/core/util:overflow\",\n    ] + tf_protos_grappler(),\n)\n\ntf_cc_test(\n    name = \"op_level_cost_estimator_test\",\n    srcs = [\"op_level_cost_estimator_test.cc\"],\n    tags = [\"no_oss\"],  # b/163222310\n    deps = [\n        \":op_level_cost_estimator\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n    ],\n)\n\ncc_library(\n    name = \"analytical_cost_estimator\",\n    srcs = [\"analytical_cost_estimator.cc\"],\n    hdrs = [\"analytical_cost_estimator.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \":graph_properties\",\n        \":op_level_cost_estimator\",\n        \":utils\",\n        \":virtual_placer\",\n        \":virtual_scheduler\",\n        \"//tensorflow/core:core_cpu_base\",\n        \"//tensorflow/core:graph\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler:op_types\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/util:overflow\",\n    ] + tf_protos_grappler(),\n    alwayslink = 1,\n)\n\ntf_cc_test(\n    name = \"analytical_cost_estimator_test\",\n    srcs = [\"analytical_cost_estimator_test.cc\"],\n    deps = [\n        \":analytical_cost_estimator\",\n        \":virtual_scheduler\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:tensorflow\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/grappler/clusters:virtual_cluster\",\n    ],\n)\n\n# copybara:uncomment_begin(google-only)\n# py_proto_library(\n#     name = \"op_performance_data_py_pb2\",\n#     has_services = 0,\n#     api_version = 2,\n#     visibility = [\"//visibility:public\"],\n#     deps = [\":op_performance_data\"],\n# )\n# copybara:uncomment_end\n", "\n/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/grappler/costs/op_level_cost_estimator.h\"\n\n#include \"absl/strings/match.h\"\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/framework/attr_value.pb.h\"\n#include \"tensorflow/core/framework/attr_value_util.h\"\n#include \"tensorflow/core/framework/tensor.pb.h\"\n#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/grappler/clusters/utils.h\"\n#include \"tensorflow/core/grappler/costs/op_context.h\"\n#include \"tensorflow/core/grappler/costs/utils.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/util/overflow.h\"\n\nnamespace tensorflow {\nnamespace grappler {\n\n// TODO(dyoon): update op to Predict method map for TF ops with V2 or V3 suffix.\nconstexpr int kOpsPerMac = 2;\nconstexpr char kGuaranteeConst[] = \"GuaranteeConst\";\nconstexpr char kAddN[] = \"AddN\";\nconstexpr char kBitCast[] = \"BitCast\";\nconstexpr char kConcatV2[] = \"ConcatV2\";\nconstexpr char kConv2d[] = \"Conv2D\";\nconstexpr char kConv2dBackpropFilter[] = \"Conv2DBackpropFilter\";\nconstexpr char kConv2dBackpropInput[] = \"Conv2DBackpropInput\";\nconstexpr char kFusedConv2dBiasActivation[] = \"FusedConv2DBiasActivation\";\nconstexpr char kDataFormatVecPermute[] = \"DataFormatVecPermute\";\nconstexpr char kDepthToSpace[] = \"DepthToSpace\";\nconstexpr char kDepthwiseConv2dNative[] = \"DepthwiseConv2dNative\";\nconstexpr char kDepthwiseConv2dNativeBackpropFilter[] =\n    \"DepthwiseConv2dNativeBackpropFilter\";\nconstexpr char kDepthwiseConv2dNativeBackpropInput[] =\n    \"DepthwiseConv2dNativeBackpropInput\";\nconstexpr char kMatMul[] = \"MatMul\";\nconstexpr char kXlaEinsum[] = \"XlaEinsum\";\nconstexpr char kEinsum[] = \"Einsum\";\nconstexpr char kExpandDims[] = \"ExpandDims\";\nconstexpr char kFill[] = \"Fill\";\nconstexpr char kSparseMatMul[] = \"SparseMatMul\";\nconstexpr char kSparseTensorDenseMatMul[] = \"SparseTensorDenseMatMul\";\nconstexpr char kPlaceholder[] = \"Placeholder\";\nconstexpr char kIdentity[] = \"Identity\";\nconstexpr char kIdentityN[] = \"IdentityN\";\nconstexpr char kRefIdentity[] = \"RefIdentity\";\nconstexpr char kNoOp[] = \"NoOp\";\nconstexpr char kReshape[] = \"Reshape\";\nconstexpr char kSplit[] = \"Split\";\nconstexpr char kSqueeze[] = \"Squeeze\";\nconstexpr char kRecv[] = \"_Recv\";\nconstexpr char kSend[] = \"_Send\";\nconstexpr char kBatchMatMul[] = \"BatchMatMul\";\nconstexpr char kBatchMatMulV2[] = \"BatchMatMulV2\";\nconstexpr char kOneHot[] = \"OneHot\";\nconstexpr char kPack[] = \"Pack\";\nconstexpr char kRank[] = \"Rank\";\nconstexpr char kRange[] = \"Range\";\nconstexpr char kShape[] = \"Shape\";\nconstexpr char kShapeN[] = \"ShapeN\";\nconstexpr char kSize[] = \"Size\";\nconstexpr char kStopGradient[] = \"StopGradient\";\nconstexpr char kPreventGradient[] = \"PreventGradient\";\nconstexpr char kGather[] = \"Gather\";\nconstexpr char kGatherNd[] = \"GatherNd\";\nconstexpr char kGatherV2[] = \"GatherV2\";\nconstexpr char kScatterAdd[] = \"ScatterAdd\";\nconstexpr char kScatterDiv[] = \"ScatterDiv\";\nconstexpr char kScatterMax[] = \"ScatterMax\";\nconstexpr char kScatterMin[] = \"ScatterMin\";\nconstexpr char kScatterMul[] = \"ScatterMul\";\nconstexpr char kScatterSub[] = \"ScatterSub\";\nconstexpr char kScatterUpdate[] = \"ScatterUpdate\";\nconstexpr char kSlice[] = \"Slice\";\nconstexpr char kStridedSlice[] = \"StridedSlice\";\nconstexpr char kSpaceToDepth[] = \"SpaceToDepth\";\nconstexpr char kTranspose[] = \"Transpose\";\nconstexpr char kTile[] = \"Tile\";\nconstexpr char kMaxPool[] = \"MaxPool\";\nconstexpr char kMaxPoolGrad[] = \"MaxPoolGrad\";\nconstexpr char kAvgPool[] = \"AvgPool\";\nconstexpr char kAvgPoolGrad[] = \"AvgPoolGrad\";\nconstexpr char kFusedBatchNorm[] = \"FusedBatchNorm\";\nconstexpr char kFusedBatchNormGrad[] = \"FusedBatchNormGrad\";\nconstexpr char kQuantizedMatMul[] = \"QuantizedMatMul\";\nconstexpr char kQuantizedMatMulV2[] = \"QuantizedMatMulV2\";\nconstexpr char kUnpack[] = \"Unpack\";\nconstexpr char kSoftmax[] = \"Softmax\";\nconstexpr char kResizeBilinear[] = \"ResizeBilinear\";\nconstexpr char kCropAndResize[] = \"CropAndResize\";\n// Dynamic control flow ops.\nconstexpr char kSwitch[] = \"Switch\";\nconstexpr char kMerge[] = \"Merge\";\nconstexpr char kEnter[] = \"Enter\";\nconstexpr char kExit[] = \"Exit\";\nconstexpr char kNextIteration[] = \"NextIteration\";\n// Persistent ops.\nconstexpr char kConst[] = \"Const\";\nconstexpr char kVariable[] = \"Variable\";\nconstexpr char kVariableV2[] = \"VariableV2\";\nconstexpr char kAutoReloadVariable[] = \"AutoReloadVariable\";\nconstexpr char kVarHandleOp[] = \"VarHandleOp\";\nconstexpr char kVarHandlesOp[] = \"_VarHandlesOp\";\nconstexpr char kReadVariableOp[] = \"ReadVariableOp\";\nconstexpr char kReadVariablesOp[] = \"_ReadVariablesOp\";\nconstexpr char kAssignVariableOp[] = \"AssignVariableOp\";\nconstexpr char kAssignAddVariableOp[] = \"AssignAddVariableOp\";\nconstexpr char kAssignSubVariableOp[] = \"AssignSubVariableOp\";\n\nstatic const Costs::Duration kMinComputeTime(1);\nstatic const int64_t kMinComputeOp = 1;\n\nnamespace {\n\nstd::string GetDataFormat(const OpInfo& op_info) {\n  std::string data_format = \"NHWC\";  // Default format.\n  if (op_info.attr().find(\"data_format\") != op_info.attr().end()) {\n    data_format = op_info.attr().at(\"data_format\").s();\n  }\n  return data_format;\n}\n\nstd::string GetFilterFormat(const OpInfo& op_info) {\n  std::string filter_format = \"HWIO\";  // Default format.\n  if (op_info.attr().find(\"filter_format\") != op_info.attr().end()) {\n    filter_format = op_info.attr().at(\"filter_format\").s();\n  }\n  return filter_format;\n}\n\nPadding GetPadding(const OpInfo& op_info) {\n  if (op_info.attr().find(\"padding\") != op_info.attr().end() &&\n      op_info.attr().at(\"padding\").s() == \"VALID\") {\n    return Padding::VALID;\n  }\n  return Padding::SAME;  // Default padding.\n}\n\nbool IsTraining(const OpInfo& op_info) {\n  if (op_info.attr().find(\"is_training\") != op_info.attr().end() &&\n      op_info.attr().at(\"is_training\").b()) {\n    return true;\n  }\n  return false;\n}\n\n// TODO(dyoon): support non-4D tensors in the cost functions of convolution\n// related ops (Conv, Pool, BatchNorm, and their backprops) and the related\n// helper functions.\nstd::vector<int64_t> GetStrides(const OpInfo& op_info) {\n  if (op_info.attr().find(\"strides\") != op_info.attr().end()) {\n    const auto strides = op_info.attr().at(\"strides\").list().i();\n    DCHECK(strides.size() == 4)\n        << \"Attr strides is not a length-4 vector: \" << op_info.DebugString();\n    if (strides.size() != 4) return {1, 1, 1, 1};\n    return {strides[0], strides[1], strides[2], strides[3]};\n  }\n  return {1, 1, 1, 1};\n}\n\nstd::vector<int64_t> GetKernelSize(const OpInfo& op_info) {\n  if (op_info.attr().find(\"ksize\") != op_info.attr().end()) {\n    const auto ksize = op_info.attr().at(\"ksize\").list().i();\n    DCHECK(ksize.size() == 4)\n        << \"Attr ksize is not a length-4 vector: \" << op_info.DebugString();\n    if (ksize.size() != 4) return {1, 1, 1, 1};\n    return {ksize[0], ksize[1], ksize[2], ksize[3]};\n  }\n  // Note that FusedBatchNorm doesn't have ksize attr, but GetKernelSize returns\n  // {1, 1, 1, 1} in that case.\n  return {1, 1, 1, 1};\n}\n\nint64_t GetOutputSize(const int64_t input, const int64_t filter,\n                      const int64_t stride, const Padding& padding) {\n  // Logic for calculating output shape is from GetWindowedOutputSizeVerbose()\n  // function in third_party/tensorflow/core/framework/common_shape_fns.cc.\n  if (padding == Padding::VALID) {\n    return (input - filter + stride) / stride;\n  } else {  // SAME.\n    return (input + stride - 1) / stride;\n  }\n}\n\n// Return the output element count of a multi-input element-wise op considering\n// broadcasting.\nint64_t CwiseOutputElementCount(const OpInfo& op_info) {\n  int max_rank = 1;\n  for (const OpInfo::TensorProperties& input_properties : op_info.inputs()) {\n    max_rank = std::max(max_rank, input_properties.shape().dim_size());\n  }\n\n  TensorShapeProto output_shape;\n  output_shape.mutable_dim()->Reserve(max_rank);\n  for (int i = 0; i < max_rank; ++i) {\n    output_shape.add_dim();\n  }\n\n  // Expand the shape of the output to follow the numpy-style broadcast rule\n  // which matches each input starting with the trailing dimensions and working\n  // its way forward. To do this, iterate through each input shape's dimensions\n  // in reverse order, and potentially increase the corresponding output\n  // dimension.\n  for (const OpInfo::TensorProperties& input_properties : op_info.inputs()) {\n    const TensorShapeProto& input_shape = input_properties.shape();\n    for (int i = input_shape.dim_size() - 1; i >= 0; --i) {\n      int output_shape_dim_index =\n          i + output_shape.dim_size() - input_shape.dim_size();\n      output_shape.mutable_dim(output_shape_dim_index)\n          ->set_size(std::max(output_shape.dim(output_shape_dim_index).size(),\n                              input_shape.dim(i).size()));\n    }\n  }\n\n  int64_t count = 1;\n  for (int i = 0; i < output_shape.dim_size(); i++) {\n    count *= output_shape.dim(i).size();\n  }\n  return count;\n}\n\n// Helper function for determining whether there are repeated indices in the\n// input Einsum equation.\nbool CheckRepeatedDimensions(const absl::string_view dim_str) {\n  int str_size = dim_str.size();\n  for (int idx = 0; idx < str_size - 1; idx++) {\n    if (dim_str.find(dim_str[idx], idx + 1) != std::string::npos) {\n      return true;\n    }\n  }\n  return false;\n}\n\n// Auxiliary function for determining whether OpLevelCostEstimator is compatible\n// with a given Einsum.\nbool IsEinsumCorrectlyFormed(const OpContext& einsum_context) {\n  const auto& op_info = einsum_context.op_info;\n\n  auto it = op_info.attr().find(\"equation\");\n  if (it == op_info.attr().end()) return false;\n  const absl::string_view equation = it->second.s();\n  std::vector<std::string> equation_split = absl::StrSplit(equation, \"->\");\n\n  if (equation_split.empty()) {\n    LOG(WARNING) << \"Einsum with malformed equation\";\n    return false;\n  }\n  std::vector<absl::string_view> input_split =\n      absl::StrSplit(equation_split[0], ',');\n\n  // The current model covers Einsum operations with two operands and a RHS\n  if (op_info.inputs_size() != 2 || equation_split.size() != 2) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n    return false;\n  }\n  const auto& a_input = op_info.inputs(0);\n  const auto& b_input = op_info.inputs(1);\n  absl::string_view rhs_str = equation_split[1];\n  absl::string_view a_input_str = input_split[0];\n  absl::string_view b_input_str = input_split[1];\n\n  // Ellipsis are not currently supported\n  if (absl::StrContains(a_input_str, \"...\") ||\n      absl::StrContains(b_input_str, \"...\")) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op()\n            << \", ellipsis not supported\";\n    return false;\n  }\n\n  constexpr int kMatrixRank = 2;\n\n  bool a_input_shape_unknown = false;\n  bool b_input_shape_unknown = false;\n\n  TensorShapeProto a_input_shape = MaybeGetMinimumShape(\n      a_input.shape(), std::max(kMatrixRank, a_input.shape().dim_size()),\n      &a_input_shape_unknown);\n  TensorShapeProto b_input_shape = MaybeGetMinimumShape(\n      b_input.shape(), std::max(kMatrixRank, b_input.shape().dim_size()),\n      &b_input_shape_unknown);\n\n  if (a_input_str.size() != static_cast<size_t>(a_input_shape.dim_size()) ||\n      b_input_str.size() != static_cast<size_t>(b_input_shape.dim_size())) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op()\n            << \", equation subscripts don't match tensor rank.\";\n    return false;\n  }\n\n  // Subscripts where axis appears more than once for a single input are not yet\n  // supported\n  if (CheckRepeatedDimensions(a_input_str) ||\n      CheckRepeatedDimensions(b_input_str) ||\n      CheckRepeatedDimensions(rhs_str)) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op()\n            << \", Subscripts where axis appears more than once for a single \"\n               \"input are not yet supported\";\n    return false;\n  }\n\n  return true;\n}\n\n}  // namespace\n\n// Return a minimum shape if the shape is unknown. If known, return the original\n// shape.\nTensorShapeProto MaybeGetMinimumShape(const TensorShapeProto& original_shape,\n                                      int rank, bool* found_unknown_shapes) {\n  auto shape = original_shape;\n  bool is_scalar = !shape.unknown_rank() && shape.dim_size() == 0;\n\n  if (shape.unknown_rank() || (!is_scalar && shape.dim_size() < rank)) {\n    *found_unknown_shapes = true;\n    VLOG(2) << \"Use minimum shape because the rank is unknown.\";\n    // The size of each dimension is at least 1, if unknown.\n    for (int i = shape.dim_size(); i < rank; i++) {\n      shape.add_dim()->set_size(1);\n    }\n  } else if (is_scalar) {\n    for (int i = 0; i < rank; i++) {\n      shape.add_dim()->set_size(1);\n    }\n  } else if (shape.dim_size() > rank) {\n    *found_unknown_shapes = true;\n    shape.clear_dim();\n    for (int i = 0; i < rank; i++) {\n      shape.add_dim()->set_size(original_shape.dim(i).size());\n    }\n  } else {\n    for (int i = 0; i < shape.dim_size(); i++) {\n      if (shape.dim(i).size() < 0) {\n        *found_unknown_shapes = true;\n        VLOG(2) << \"Use minimum dim size 1 because the shape is unknown.\";\n        // The size of each dimension is at least 1, if unknown.\n        shape.mutable_dim(i)->set_size(1);\n      }\n    }\n  }\n  return shape;\n}\n\nOpLevelCostEstimator::OpLevelCostEstimator() {\n  // Syntactic sugar to build and return a lambda that takes an OpInfo and\n  // returns a cost.\n  typedef Status (OpLevelCostEstimator::*CostImpl)(const OpContext& op_context,\n                                                   NodeCosts*) const;\n  auto wrap = [this](CostImpl impl)\n      -> std::function<Status(const OpContext&, NodeCosts*)> {\n    return [this, impl](const OpContext& op_context, NodeCosts* node_costs) {\n      return (this->*impl)(op_context, node_costs);\n    };\n  };\n\n  device_cost_impl_.emplace(kConv2d,\n                            wrap(&OpLevelCostEstimator::PredictConv2D));\n  device_cost_impl_.emplace(\n      kConv2dBackpropFilter,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropFilter));\n  device_cost_impl_.emplace(\n      kConv2dBackpropInput,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropInput));\n  device_cost_impl_.emplace(\n      kFusedConv2dBiasActivation,\n      wrap(&OpLevelCostEstimator::PredictFusedConv2DBiasActivation));\n  // reuse Conv2D for DepthwiseConv2dNative because the calculation is the\n  // same although the actual meaning of the parameters are different. See\n  // comments in PredictConv2D and related functions\n  device_cost_impl_.emplace(kDepthwiseConv2dNative,\n                            wrap(&OpLevelCostEstimator::PredictConv2D));\n  device_cost_impl_.emplace(\n      kDepthwiseConv2dNativeBackpropFilter,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropFilter));\n  device_cost_impl_.emplace(\n      kDepthwiseConv2dNativeBackpropInput,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropInput));\n  device_cost_impl_.emplace(kMatMul,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(kSparseMatMul,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(\n      kSparseTensorDenseMatMul,\n      wrap(&OpLevelCostEstimator::PredictSparseTensorDenseMatMul));\n  device_cost_impl_.emplace(kBatchMatMul,\n                            wrap(&OpLevelCostEstimator::PredictBatchMatMul));\n  device_cost_impl_.emplace(kBatchMatMulV2,\n                            wrap(&OpLevelCostEstimator::PredictBatchMatMul));\n  device_cost_impl_.emplace(kQuantizedMatMul,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(kQuantizedMatMulV2,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(kXlaEinsum,\n                            wrap(&OpLevelCostEstimator::PredictEinsum));\n  device_cost_impl_.emplace(kEinsum,\n                            wrap(&OpLevelCostEstimator::PredictEinsum));\n\n  device_cost_impl_.emplace(kNoOp, wrap(&OpLevelCostEstimator::PredictNoOp));\n  device_cost_impl_.emplace(kGuaranteeConst,\n                            wrap(&OpLevelCostEstimator::PredictNoOp));\n\n  device_cost_impl_.emplace(kGather,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kGatherNd,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kGatherV2,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kScatterAdd,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterDiv,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterMax,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterMin,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterMul,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterSub,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterUpdate,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n\n  device_cost_impl_.emplace(kSlice,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kStridedSlice,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n\n  device_cost_impl_.emplace(kPlaceholder,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kIdentity,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kIdentityN,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kRefIdentity,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kStopGradient,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kPreventGradient,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kReshape,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kRecv,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kSend,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kSwitch,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kMerge,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kEnter,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kExit,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kNextIteration,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kBitCast,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n\n  device_cost_impl_.emplace(kConcatV2,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kDataFormatVecPermute,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kDepthToSpace,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kExpandDims,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kFill,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kOneHot,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kPack,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kRange,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kSpaceToDepth,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kSplit,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kSqueeze,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kTranspose,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kTile,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kUnpack,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n\n  device_cost_impl_.emplace(kRank,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kShape,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kShapeN,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kSize,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kMaxPool,\n                            wrap(&OpLevelCostEstimator::PredictMaxPool));\n  device_cost_impl_.emplace(kMaxPoolGrad,\n                            wrap(&OpLevelCostEstimator::PredictMaxPoolGrad));\n  device_cost_impl_.emplace(kAvgPool,\n                            wrap(&OpLevelCostEstimator::PredictAvgPool));\n  device_cost_impl_.emplace(kAvgPoolGrad,\n                            wrap(&OpLevelCostEstimator::PredictAvgPoolGrad));\n  device_cost_impl_.emplace(kFusedBatchNorm,\n                            wrap(&OpLevelCostEstimator::PredictFusedBatchNorm));\n  device_cost_impl_.emplace(\n      kFusedBatchNormGrad,\n      wrap(&OpLevelCostEstimator::PredictFusedBatchNormGrad));\n  device_cost_impl_.emplace(kSoftmax,\n                            wrap(&OpLevelCostEstimator::PredictSoftmax));\n  device_cost_impl_.emplace(kResizeBilinear,\n                            wrap(&OpLevelCostEstimator::PredictResizeBilinear));\n  device_cost_impl_.emplace(kCropAndResize,\n                            wrap(&OpLevelCostEstimator::PredictCropAndResize));\n  device_cost_impl_.emplace(\n      kAssignVariableOp, wrap(&OpLevelCostEstimator::PredictAssignVariableOps));\n  device_cost_impl_.emplace(\n      kAssignAddVariableOp,\n      wrap(&OpLevelCostEstimator::PredictAssignVariableOps));\n  device_cost_impl_.emplace(\n      kAssignSubVariableOp,\n      wrap(&OpLevelCostEstimator::PredictAssignVariableOps));\n  device_cost_impl_.emplace(kAddN, wrap(&OpLevelCostEstimator::PredictNaryOp));\n\n  persistent_ops_ = {\n      kConst,       kVariable,       kVariableV2,   kAutoReloadVariable,\n      kVarHandleOp, kReadVariableOp, kVarHandlesOp, kReadVariablesOp};\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n\n  // Quantize = apply min and max bounds, multiply by scale factor and round.\n  const int quantize_v2_cost =\n      EIGEN_COST(scalar_product_op<float>) + EIGEN_COST(scalar_max_op<float>) +\n      EIGEN_COST(scalar_min_op<float>) + EIGEN_COST(scalar_round_op<float>);\n  const int quantize_and_dequantize_v2_cost =\n      quantize_v2_cost + EIGEN_COST(scalar_product_op<float>);\n\n  // Unary ops alphabetically sorted\n  elementwise_ops_.emplace(\"Acos\", EIGEN_COST(scalar_acos_op<float>));\n  elementwise_ops_.emplace(\"All\", EIGEN_COST(scalar_boolean_and_op));\n  elementwise_ops_.emplace(\"ArgMax\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Asin\", EIGEN_COST(scalar_asin_op<float>));\n  elementwise_ops_.emplace(\"Atan\", EIGEN_COST(scalar_atan_op<float>));\n  elementwise_ops_.emplace(\"Atan2\", EIGEN_COST(scalar_quotient_op<float>) +\n                                        EIGEN_COST(scalar_atan_op<float>));\n  // For now, we use Eigen cost model for float to int16 cast as an example\n  // case; Eigen cost model is zero when src and dst types are identical,\n  // and it uses AddCost (1) when different. We may implement a separate\n  // cost functions for cast ops, using the actual input and output types.\n  elementwise_ops_.emplace(\n      \"Cast\", Eigen::internal::functor_traits<\n                  Eigen::internal::scalar_cast_op<float, int16>>::Cost);\n  elementwise_ops_.emplace(\"Ceil\", EIGEN_COST(scalar_ceil_op<float>));\n  elementwise_ops_.emplace(\"Cos\", EIGEN_COST(scalar_cos_op<float>));\n  elementwise_ops_.emplace(\"Dequantize\", EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"Erf\", 1);\n  elementwise_ops_.emplace(\"Erfc\", 1);\n  elementwise_ops_.emplace(\"Exp\", EIGEN_COST(scalar_exp_op<float>));\n  elementwise_ops_.emplace(\"Expm1\", EIGEN_COST(scalar_expm1_op<float>));\n  elementwise_ops_.emplace(\"Floor\", EIGEN_COST(scalar_floor_op<float>));\n  elementwise_ops_.emplace(\"Inv\", EIGEN_COST(scalar_inverse_op<float>));\n  elementwise_ops_.emplace(\"InvGrad\", 1);\n  elementwise_ops_.emplace(\"Lgamma\", 1);\n  elementwise_ops_.emplace(\"Log\", EIGEN_COST(scalar_log_op<float>));\n  elementwise_ops_.emplace(\"Log1p\", EIGEN_COST(scalar_log1p_op<float>));\n  elementwise_ops_.emplace(\"Max\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Min\", EIGEN_COST(scalar_min_op<float>));\n  elementwise_ops_.emplace(\"Neg\", EIGEN_COST(scalar_opposite_op<float>));\n  elementwise_ops_.emplace(\"Prod\", EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"QuantizeAndDequantizeV2\",\n                           quantize_and_dequantize_v2_cost);\n  elementwise_ops_.emplace(\"QuantizeAndDequantizeV4\",\n                           quantize_and_dequantize_v2_cost);\n  elementwise_ops_.emplace(\"QuantizedSigmoid\",\n                           EIGEN_COST(scalar_logistic_op<float>));\n  elementwise_ops_.emplace(\"QuantizeV2\", quantize_v2_cost);\n  elementwise_ops_.emplace(\"Reciprocal\", EIGEN_COST(scalar_inverse_op<float>));\n  elementwise_ops_.emplace(\"Relu\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Relu6\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Rint\", 1);\n  elementwise_ops_.emplace(\"Round\", EIGEN_COST(scalar_round_op<float>));\n  elementwise_ops_.emplace(\"Rsqrt\", EIGEN_COST(scalar_rsqrt_op<float>));\n  elementwise_ops_.emplace(\"Sigmoid\", EIGEN_COST(scalar_logistic_op<float>));\n  elementwise_ops_.emplace(\"Sign\", EIGEN_COST(scalar_sign_op<float>));\n  elementwise_ops_.emplace(\"Sin\", EIGEN_COST(scalar_sin_op<float>));\n  elementwise_ops_.emplace(\"Sqrt\", EIGEN_COST(scalar_sqrt_op<float>));\n  elementwise_ops_.emplace(\"Square\", EIGEN_COST(scalar_square_op<float>));\n  elementwise_ops_.emplace(\"Sum\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"Tan\", EIGEN_COST(scalar_tan_op<float>));\n  elementwise_ops_.emplace(\"Tanh\", EIGEN_COST(scalar_tanh_op<float>));\n  elementwise_ops_.emplace(\"TopKV2\", EIGEN_COST(scalar_max_op<float>));\n  // Binary ops alphabetically sorted\n  elementwise_ops_.emplace(\"Add\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"AddV2\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"ApproximateEqual\", 1);\n  elementwise_ops_.emplace(\"BiasAdd\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"QuantizedBiasAdd\",\n                           EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"Div\", EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"Equal\", 1);\n  elementwise_ops_.emplace(\"FloorDiv\", EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"FloorMod\", EIGEN_COST(scalar_mod_op<float>));\n  elementwise_ops_.emplace(\"Greater\", 1);\n  elementwise_ops_.emplace(\"GreaterEqual\", 1);\n  elementwise_ops_.emplace(\"Less\", 1);\n  elementwise_ops_.emplace(\"LessEqual\", 1);\n  elementwise_ops_.emplace(\"LogicalAnd\", EIGEN_COST(scalar_boolean_and_op));\n  elementwise_ops_.emplace(\"LogicalNot\", 1);\n  elementwise_ops_.emplace(\"LogicalOr\", EIGEN_COST(scalar_boolean_or_op));\n  elementwise_ops_.emplace(\"Maximum\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Minimum\", EIGEN_COST(scalar_min_op<float>));\n  elementwise_ops_.emplace(\"Mod\", EIGEN_COST(scalar_mod_op<float>));\n  elementwise_ops_.emplace(\"Mul\", EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"NotEqual\", 1);\n  elementwise_ops_.emplace(\"QuantizedAdd\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"QuantizedMul\",\n                           EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"RealDiv\", EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"ReluGrad\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Select\", EIGEN_COST(scalar_boolean_or_op));\n  elementwise_ops_.emplace(\"SelectV2\", EIGEN_COST(scalar_boolean_or_op));\n  elementwise_ops_.emplace(\"SquaredDifference\",\n                           EIGEN_COST(scalar_square_op<float>) +\n                               EIGEN_COST(scalar_difference_op<float>));\n  elementwise_ops_.emplace(\"Sub\", EIGEN_COST(scalar_difference_op<float>));\n  elementwise_ops_.emplace(\"TruncateDiv\",\n                           EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"TruncateMod\", EIGEN_COST(scalar_mod_op<float>));\n  elementwise_ops_.emplace(\"Where\", 1);\n\n#undef EIGEN_COST\n\n  // By default, use sum of memory_time and compute_time for execution_time.\n  compute_memory_overlap_ = false;\n}\n\nCosts OpLevelCostEstimator::PredictCosts(const OpContext& op_context) const {\n  Costs costs;\n  NodeCosts node_costs;\n  if (PredictNodeCosts(op_context, &node_costs).ok()) {\n    if (node_costs.has_costs) {\n      return node_costs.costs;\n    }\n    // Convert NodeCosts to Costs.\n    if (node_costs.minimum_cost_op) {\n      // Override to minimum cost; Note that some ops with minimum cost may have\n      // non-typical device (e.g., channel for _Send), which may fail with\n      // GetDeviceInfo(), called from PredictOpCountBasedCost(). Make sure we\n      // directly set minimum values to Costs here, not calling\n      // PredictOpCountBasedCost().\n      costs.compute_time = kMinComputeTime;\n      costs.execution_time = kMinComputeTime;\n      costs.memory_time = 0;\n      costs.intermediate_memory_time = 0;\n      costs.intermediate_memory_read_time = 0;\n      costs.intermediate_memory_write_time = 0;\n    } else {\n      // Convert NodeCosts to Costs.\n      costs = PredictOpCountBasedCost(\n          node_costs.num_compute_ops, node_costs.num_total_read_bytes(),\n          node_costs.num_total_write_bytes(), op_context.op_info);\n    }\n    VLOG(1) << \"Operation \" << op_context.op_info.op() << \" takes \"\n            << costs.execution_time.count() << \" ns.\";\n    // Copy additional stats from NodeCosts to Costs.\n    costs.max_memory = node_costs.max_memory;\n    costs.persistent_memory = node_costs.persistent_memory;\n    costs.temporary_memory = node_costs.temporary_memory;\n    costs.inaccurate = node_costs.inaccurate;\n    costs.num_ops_with_unknown_shapes =\n        node_costs.num_nodes_with_unknown_shapes;\n    costs.num_ops_total = node_costs.num_nodes;\n    return costs;\n  }\n  // Errors during node cost estimate.\n  LOG(WARNING) << \"Error in PredictCost() for the op: \"\n               << op_context.op_info.ShortDebugString();\n  costs = Costs::ZeroCosts(/*inaccurate=*/true);\n  costs.num_ops_with_unknown_shapes = node_costs.num_nodes_with_unknown_shapes;\n  return costs;\n}\n\nStatus OpLevelCostEstimator::PredictNodeCosts(const OpContext& op_context,\n                                              NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  auto it = device_cost_impl_.find(op_info.op());\n  if (it != device_cost_impl_.end()) {\n    std::function<Status(const OpContext&, NodeCosts*)> estimator = it->second;\n    return estimator(op_context, node_costs);\n  }\n\n  if (persistent_ops_.find(op_info.op()) != persistent_ops_.end()) {\n    return PredictVariable(op_context, node_costs);\n  }\n\n  if (elementwise_ops_.find(op_info.op()) != elementwise_ops_.end()) {\n    return PredictCwiseOp(op_context, node_costs);\n  }\n\n  VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n\n  node_costs->num_nodes_with_unknown_op_type = 1;\n  return PredictCostOfAnUnknownOp(op_context, node_costs);\n}\n\n// This method assumes a typical system composed of CPUs and GPUs, connected\n// through PCIe. To define device info more precisely, override this method.\nDeviceInfo OpLevelCostEstimator::GetDeviceInfo(\n    const DeviceProperties& device) const {\n  double gflops = -1;\n  double gb_per_sec = -1;\n\n  if (device.type() == \"CPU\") {\n    // Check if vector instructions are available, and refine performance\n    // prediction based on this.\n    // Frequencies are stored in MHz in the DeviceProperties.\n    gflops = device.num_cores() * device.frequency() * 1e-3;\n    if (gb_per_sec < 0) {\n      if (device.bandwidth() > 0) {\n        gb_per_sec = device.bandwidth() / 1e6;\n      } else {\n        gb_per_sec = 32;\n      }\n    }\n  } else if (device.type() == \"GPU\") {\n    const auto& device_env = device.environment();\n    auto it = device_env.find(\"architecture\");\n    if (it != device_env.end()) {\n      const std::string architecture = device_env.at(\"architecture\");\n      int cores_per_multiprocessor;\n      if (architecture < \"3\") {\n        // Fermi\n        cores_per_multiprocessor = 32;\n      } else if (architecture < \"4\") {\n        // Kepler\n        cores_per_multiprocessor = 192;\n      } else if (architecture < \"6\") {\n        // Maxwell\n        cores_per_multiprocessor = 128;\n      } else {\n        // Pascal (compute capability version 6) and Volta (compute capability\n        // version 7)\n        cores_per_multiprocessor = 64;\n      }\n      gflops = device.num_cores() * device.frequency() * 1e-3 *\n               cores_per_multiprocessor * kOpsPerMac;\n      if (device.bandwidth() > 0) {\n        gb_per_sec = device.bandwidth() / 1e6;\n      } else {\n        gb_per_sec = 100;\n      }\n    } else {\n      // Architecture is not available (ex: pluggable device), return default\n      // value.\n      gflops = 100;     // Dummy value;\n      gb_per_sec = 12;  // default PCIe x16 gen3.\n    }\n  } else {\n    LOG_EVERY_N(WARNING, 1000) << \"Unknown device type: \" << device.type()\n                               << \", assuming PCIe between CPU and GPU.\";\n    gflops = 1;  // Dummy value; data transfer ops would not have compute ops.\n    gb_per_sec = 12;  // default PCIe x16 gen3.\n  }\n  VLOG(1) << \"Device: \" << device.type() << \" gflops: \" << gflops\n          << \" gb_per_sec: \" << gb_per_sec;\n\n  return DeviceInfo(gflops, gb_per_sec);\n}\n\nStatus OpLevelCostEstimator::PredictCwiseOp(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  // For element-wise operations, op count is the element count of any input. We\n  // use the count for the largest input here to be more robust in case that the\n  // shape is unknown or partially known for other input.\n  int64_t op_count = CalculateLargestInputCount(op_info, &found_unknown_shapes);\n  // If output shape is available, try to use the element count calculated from\n  // that.\n  if (op_info.outputs_size() > 0) {\n    op_count = std::max(\n        op_count,\n        CalculateTensorElementCount(op_info.outputs(0), &found_unknown_shapes));\n  }\n  // Calculate the output shape possibly resulting from broadcasting.\n  if (op_info.inputs_size() >= 2) {\n    op_count = std::max(op_count, CwiseOutputElementCount(op_info));\n  }\n\n  int op_cost = 1;\n  auto it = elementwise_ops_.find(op_info.op());\n  if (it != elementwise_ops_.end()) {\n    op_cost = it->second;\n  } else {\n    return errors::InvalidArgument(\"Not a cwise op: \", op_info.op());\n  }\n\n  return PredictDefaultNodeCosts(op_count * op_cost, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictCostOfAnUnknownOp(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  // Don't assume the operation is cwise, return cost based on input/output size\n  // and admit that it is inaccurate...\n  bool found_unknown_shapes = false;\n  node_costs->inaccurate = true;\n  return PredictDefaultNodeCosts(0, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nCosts OpLevelCostEstimator::PredictOpCountBasedCost(\n    double operations, const OpInfo& op_info) const {\n  bool unknown_shapes = false;\n  const double input_size = CalculateInputSize(op_info, &unknown_shapes);\n  const double output_size = CalculateOutputSize(op_info, &unknown_shapes);\n  Costs costs =\n      PredictOpCountBasedCost(operations, input_size, output_size, op_info);\n  costs.inaccurate = unknown_shapes;\n  costs.num_ops_with_unknown_shapes = unknown_shapes;\n  costs.max_memory = output_size;\n  return costs;\n}\n\nCosts OpLevelCostEstimator::PredictOpCountBasedCost(\n    double operations, double input_io_bytes, double output_io_bytes,\n    const OpInfo& op_info) const {\n  double total_io_bytes = input_io_bytes + output_io_bytes;\n  const DeviceInfo device_info = GetDeviceInfo(op_info.device());\n  if (device_info.gigaops <= 0 || device_info.gb_per_sec <= 0 ||\n      device_info.intermediate_read_gb_per_sec <= 0 ||\n      device_info.intermediate_write_gb_per_sec <= 0) {\n    VLOG(1) << \"BAD DEVICE. Op:\" << op_info.op()\n            << \" device type:\" << op_info.device().type()\n            << \" device model:\" << op_info.device().model();\n  }\n\n  Costs::NanoSeconds compute_cost(std::ceil(operations / device_info.gigaops));\n  VLOG(1) << \"Op:\" << op_info.op() << \" GOps:\" << operations / 1e9\n          << \" Compute Time (ns):\" << compute_cost.count();\n\n  Costs::NanoSeconds memory_cost(\n      std::ceil(total_io_bytes / device_info.gb_per_sec));\n  VLOG(1) << \"Op:\" << op_info.op() << \" Size (KB):\" << (total_io_bytes) / 1e3\n          << \" Memory Time (ns):\" << memory_cost.count();\n\n  // Check if bytes > 0.  If it's not and the bandwidth is set to infinity\n  // then the result would be undefined.\n  double intermediate_read_time =\n      (input_io_bytes > 0)\n          ? std::ceil(input_io_bytes / device_info.intermediate_read_gb_per_sec)\n          : 0;\n\n  double intermediate_write_time =\n      (output_io_bytes > 0)\n          ? std::ceil(output_io_bytes /\n                      device_info.intermediate_write_gb_per_sec)\n          : 0;\n\n  Costs::NanoSeconds intermediate_memory_cost =\n      compute_memory_overlap_\n          ? std::max(intermediate_read_time, intermediate_write_time)\n          : (intermediate_read_time + intermediate_write_time);\n  VLOG(1) << \"Op:\" << op_info.op() << \" Size (KB):\" << (total_io_bytes) / 1e3\n          << \" Intermediate Memory Time (ns):\"\n          << intermediate_memory_cost.count();\n\n  Costs costs = Costs::ZeroCosts();\n  costs.compute_time = compute_cost;\n  costs.memory_time = memory_cost;\n  costs.intermediate_memory_time = intermediate_memory_cost;\n  costs.intermediate_memory_read_time =\n      Costs::NanoSeconds(intermediate_read_time);\n  costs.intermediate_memory_write_time =\n      Costs::NanoSeconds(intermediate_write_time);\n  CombineCostsAndUpdateExecutionTime(compute_memory_overlap_, &costs);\n  return costs;\n}\n\nint64_t OpLevelCostEstimator::CountConv2DOperations(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  return CountConv2DOperations(op_info, nullptr, found_unknown_shapes);\n}\n\n// Helper to translate the positional arguments into named fields.\n/* static */\nOpLevelCostEstimator::ConvolutionDimensions\nOpLevelCostEstimator::ConvolutionDimensionsFromInputs(\n    const TensorShapeProto& original_image_shape,\n    const TensorShapeProto& original_filter_shape, const OpInfo& op_info,\n    bool* found_unknown_shapes) {\n  VLOG(2) << \"op features: \" << op_info.DebugString();\n  VLOG(2) << \"Original image shape: \" << original_image_shape.DebugString();\n  VLOG(2) << \"Original filter shape: \" << original_filter_shape.DebugString();\n\n  int x_index, y_index, major_channel_index, minor_channel_index = -1;\n  const std::string& data_format = GetDataFormat(op_info);\n  if (data_format == \"NCHW\") {\n    major_channel_index = 1;\n    y_index = 2;\n    x_index = 3;\n  } else if (data_format == \"NCHW_VECT_C\") {\n    // Use NCHW_VECT_C\n    minor_channel_index = 1;\n    y_index = 2;\n    x_index = 3;\n    major_channel_index = 4;\n  } else {\n    // Use NHWC.\n    y_index = 1;\n    x_index = 2;\n    major_channel_index = 3;\n  }\n  const std::string& filter_format = GetFilterFormat(op_info);\n  int filter_x_index, filter_y_index, in_major_channel_index, out_channel_index,\n      in_minor_channel_index = -1;\n  if (filter_format == \"HWIO\") {\n    filter_y_index = 0;\n    filter_x_index = 1;\n    in_major_channel_index = 2;\n    out_channel_index = 3;\n  } else if (filter_format == \"OIHW_VECT_I\") {\n    out_channel_index = 0;\n    in_minor_channel_index = 1;\n    filter_y_index = 2;\n    filter_x_index = 3;\n    in_major_channel_index = 4;\n  } else {\n    // Use OIHW\n    out_channel_index = 0;\n    in_major_channel_index = 1;\n    filter_y_index = 2;\n    filter_x_index = 3;\n  }\n\n  auto image_shape = MaybeGetMinimumShape(original_image_shape,\n                                          minor_channel_index >= 0 ? 5 : 4,\n                                          found_unknown_shapes);\n  auto filter_shape = MaybeGetMinimumShape(original_filter_shape,\n                                           in_minor_channel_index >= 0 ? 5 : 4,\n                                           found_unknown_shapes);\n  VLOG(2) << \"Image shape: \" << image_shape.DebugString();\n  VLOG(2) << \"Filter shape: \" << filter_shape.DebugString();\n\n  int64_t batch = image_shape.dim(0).size();\n  int64_t ix = image_shape.dim(x_index).size();\n  int64_t iy = image_shape.dim(y_index).size();\n  int64_t iz = minor_channel_index >= 0\n                   ? image_shape.dim(minor_channel_index).size() *\n                         image_shape.dim(major_channel_index).size()\n                   : image_shape.dim(major_channel_index).size();\n  int64_t kx = filter_shape.dim(filter_x_index).size();\n  int64_t ky = filter_shape.dim(filter_y_index).size();\n  int64_t kz = in_minor_channel_index >= 0\n                   ? filter_shape.dim(in_major_channel_index).size() *\n                         filter_shape.dim(in_minor_channel_index).size()\n                   : filter_shape.dim(in_major_channel_index).size();\n  std::vector<int64_t> strides = GetStrides(op_info);\n  const auto padding = GetPadding(op_info);\n  int64_t sx = strides[x_index];\n  int64_t sy = strides[y_index];\n  int64_t ox = GetOutputSize(ix, kx, sx, padding);\n  int64_t oy = GetOutputSize(iy, ky, sy, padding);\n  int64_t oz = filter_shape.dim(out_channel_index).size();\n  // Only check equality when both sizes are known (in other words, when\n  // neither is set to a minimum dimension size of 1).\n  if (iz != 1 && kz != 1) {\n    DCHECK_EQ(iz % kz, 0) << \"Input channel \" << iz\n                          << \" is not a multiple of filter channel \" << kz\n                          << \".\";\n    if (iz % kz) {\n      *found_unknown_shapes = true;\n    }\n  } else {\n    iz = kz = std::max<int64_t>(iz, kz);\n  }\n  OpLevelCostEstimator::ConvolutionDimensions conv_dims = {\n      batch, ix, iy, iz, kx, ky, kz, oz, ox, oy, sx, sy, padding};\n\n  VLOG(1) << \"Batch Size:\" << batch;\n  VLOG(1) << \"Image Dims:\" << ix << \",\" << iy;\n  VLOG(1) << \"Input Depth:\" << iz;\n  VLOG(1) << \"Kernel Dims:\" << kx << \",\" << ky;\n  VLOG(1) << \"Kernel Depth:\" << kz;\n  VLOG(1) << \"Output Dims:\" << ox << \",\" << oy;\n  VLOG(1) << \"Output Depth:\" << oz;\n  VLOG(1) << \"Strides:\" << sx << \",\" << sy;\n  VLOG(1) << \"Padding:\" << (padding == Padding::VALID ? \"VALID\" : \"SAME\");\n  return conv_dims;\n}\n\nint64_t OpLevelCostEstimator::CountConv2DOperations(\n    const OpInfo& op_info, ConvolutionDimensions* conv_info,\n    bool* found_unknown_shapes) {\n  DCHECK(op_info.op() == kConv2d || op_info.op() == kDepthwiseConv2dNative)\n      << \"Invalid Operation: not Conv2D nor DepthwiseConv2dNative\";\n\n  if (op_info.inputs_size() < 2) {  // Unexpected inputs.\n    *found_unknown_shapes = true;\n    return 0;\n  }\n\n  ConvolutionDimensions conv_dims = ConvolutionDimensionsFromInputs(\n      op_info.inputs(0).shape(), op_info.inputs(1).shape(), op_info,\n      found_unknown_shapes);\n\n  //  in DepthwiseConv2dNative conv_dims.oz is actually the channel depth\n  //  multiplier; The effective output channel depth oz_effective is\n  //  conv_dims.iz * conv_dims.oz. thus # ops = N x H x W x oz_effective x 2RS.\n  //  Compare to Conv2D where # ops =  N x H x W x kz x oz x 2RS,\n  //  oz = oz_effective,  then Conv2D_ops / Depthwise_conv2d_native_ops = kz.\n  int64_t ops = conv_dims.batch;\n  ops *= conv_dims.ox * conv_dims.oy;\n  ops *= conv_dims.kx * conv_dims.ky;\n  if (op_info.op() == kConv2d) {\n    ops *= conv_dims.kz * conv_dims.oz;\n  } else {\n    // To ensure output tensor dims to be correct for DepthwiseConv2DNative,\n    // although ops are the same as Conv2D.\n    conv_dims.oz *= conv_dims.iz;\n    ops *= conv_dims.oz;\n  }\n  ops *= kOpsPerMac;\n\n  if (conv_info != nullptr) {\n    *conv_info = conv_dims;\n  }\n  return ops;\n}\n\nint64_t OpLevelCostEstimator::CountMatMulOperations(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  return CountMatMulOperations(op_info, nullptr, found_unknown_shapes);\n}\n\n// TODO(nishantpatil): Create separate estimator for Sparse Matmul\nint64_t OpLevelCostEstimator::CountMatMulOperations(\n    const OpInfo& op_info, MatMulDimensions* mat_mul,\n    bool* found_unknown_shapes) {\n  double ops = 0;\n\n  if (op_info.inputs_size() < 2) {\n    LOG(ERROR) << \"Need 2 inputs but got \" << op_info.inputs_size();\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return 0;\n  }\n\n  auto& a_matrix = op_info.inputs(0);\n  auto& b_matrix = op_info.inputs(1);\n\n  bool transpose_a = false;\n  bool transpose_b = false;\n\n  double m_dim, n_dim, k_dim, k_dim_b = 0;\n\n  for (const auto& item : op_info.attr()) {\n    VLOG(1) << \"Key:\" << item.first\n            << \" Value:\" << SummarizeAttrValue(item.second);\n    if (item.first == \"transpose_a\" && item.second.b() == true)\n      transpose_a = true;\n    if (item.first == \"transpose_b\" && item.second.b() == true)\n      transpose_b = true;\n  }\n  VLOG(1) << \"transpose_a:\" << transpose_a;\n  VLOG(1) << \"transpose_b:\" << transpose_b;\n  auto a_matrix_shape =\n      MaybeGetMinimumShape(a_matrix.shape(), 2, found_unknown_shapes);\n  auto b_matrix_shape =\n      MaybeGetMinimumShape(b_matrix.shape(), 2, found_unknown_shapes);\n  if (transpose_a) {\n    m_dim = a_matrix_shape.dim(1).size();\n    k_dim = a_matrix_shape.dim(0).size();\n  } else {\n    m_dim = a_matrix_shape.dim(0).size();\n    k_dim = a_matrix_shape.dim(1).size();\n  }\n  if (transpose_b) {\n    k_dim_b = b_matrix_shape.dim(1).size();\n    n_dim = b_matrix_shape.dim(0).size();\n  } else {\n    k_dim_b = b_matrix_shape.dim(0).size();\n    n_dim = b_matrix_shape.dim(1).size();\n  }\n\n  VLOG(1) << \"M, N, K: \" << m_dim << \",\" << n_dim << \",\" << k_dim;\n  // Only check equality when both sizes are known (in other words, when\n  // neither is set to a minimum dimension size of 1).\n  if (k_dim_b != 1 && k_dim != 1 && k_dim_b != k_dim) {\n    LOG(ERROR) << \"Incompatible Matrix dimensions\";\n    return ops;\n  } else {\n    // One of k_dim and k_dim_b might be 1 (minimum dimension size).\n    k_dim = std::max(k_dim, k_dim_b);\n  }\n\n  ops = m_dim * n_dim * k_dim * 2;\n  VLOG(1) << \"Operations for Matmul: \" << ops;\n\n  if (mat_mul != nullptr) {\n    mat_mul->m = m_dim;\n    mat_mul->n = n_dim;\n    mat_mul->k = k_dim;\n  }\n  return ops;\n}\n\nbool OpLevelCostEstimator::GenerateBatchMatmulContextFromEinsum(\n    const OpContext& einsum_context, OpContext* batch_matmul_context,\n    bool* found_unknown_shapes) const {\n  // This auxiliary function transforms an einsum OpContext into its equivalent\n  // Batch Matmul OpContext. The function returns a boolean, which determines\n  // whether it was successful in generating the output OpContext or not.\n\n  // Einsum computes a generalized contraction between tensors of arbitrary\n  // dimension as defined by the equation written in the Einstein summation\n  // convention. The number of tensors in the computation and the number of\n  // contractions can be arbitrarily long. The current model only contemplates\n  // Einsum equations, which can be translated into a single BatchMatMul\n  // operation. Einsum operations with more than two operands are not currently\n  // supported. Subscripts where an axis appears more than once for a single\n  // input and ellipsis are currently also excluded. See:\n  // https://www.tensorflow.org/api_docs/python/tf/einsum\n  // We distinguish four kinds of dimensions, depending on their placement in\n  // the equation:\n  // + B: Batch dimensions: Dimensions which appear in both operands and RHS.\n  // + K: Contracting dimensions: These appear in both inputs but not RHS.\n  // + M: Operand A dimensions: These appear in the first operand and the RHS.\n  // + N: Operand B dimensions: These appear in the second operand and the RHS.\n  // Then, the operation to estimate is BatchMatMul([B,M,K],[B,K,N])\n\n  if (batch_matmul_context == nullptr) {\n    VLOG(1) << \"Output context should not be a nullptr.\";\n    return false;\n  }\n  if (!IsEinsumCorrectlyFormed(einsum_context)) return false;\n  const auto& op_info = einsum_context.op_info;\n  std::vector<std::string> equation_split =\n      absl::StrSplit(op_info.attr().find(\"equation\")->second.s(), \"->\");\n  std::vector<absl::string_view> input_split =\n      absl::StrSplit(equation_split[0], ',');\n  const auto& a_input = op_info.inputs(0);\n  const auto& b_input = op_info.inputs(1);\n  absl::string_view rhs_str = equation_split[1];\n  absl::string_view a_input_str = input_split[0];\n  absl::string_view b_input_str = input_split[1];\n\n  constexpr int kMatrixRank = 2;\n\n  bool a_input_shape_unknown = false;\n  bool b_input_shape_unknown = false;\n\n  TensorShapeProto a_input_shape = MaybeGetMinimumShape(\n      a_input.shape(), std::max(kMatrixRank, a_input.shape().dim_size()),\n      &a_input_shape_unknown);\n  TensorShapeProto b_input_shape = MaybeGetMinimumShape(\n      b_input.shape(), std::max(kMatrixRank, b_input.shape().dim_size()),\n      &b_input_shape_unknown);\n\n  *found_unknown_shapes = a_input_shape_unknown || b_input_shape_unknown ||\n                          (a_input.shape().dim_size() < kMatrixRank) ||\n                          (b_input.shape().dim_size() < kMatrixRank);\n\n  OpInfo batch_matmul_op_info = op_info;\n  batch_matmul_op_info.mutable_inputs()->Clear();\n  batch_matmul_op_info.set_op(\"BatchMatMul\");\n\n  AttrValue transpose_attribute;\n  transpose_attribute.set_b(false);\n  (*batch_matmul_op_info.mutable_attr())[\"transpose_a\"] = transpose_attribute;\n  (*batch_matmul_op_info.mutable_attr())[\"transpose_b\"] = transpose_attribute;\n\n  OpInfo::TensorProperties* a_matrix = batch_matmul_op_info.add_inputs();\n  TensorShapeProto* a_matrix_shape = a_matrix->mutable_shape();\n  a_matrix->set_dtype(a_input.dtype());\n\n  OpInfo::TensorProperties* b_matrix = batch_matmul_op_info.add_inputs();\n  b_matrix->set_dtype(b_input.dtype());\n  TensorShapeProto* b_matrix_shape = b_matrix->mutable_shape();\n\n  TensorShapeProto_Dim m_dim;\n  TensorShapeProto_Dim n_dim;\n  TensorShapeProto_Dim k_dim;\n\n  m_dim.set_size(1);\n  n_dim.set_size(1);\n  k_dim.set_size(1);\n\n  for (int i_idx = 0, a_input_str_size = a_input_str.size();\n       i_idx < a_input_str_size; ++i_idx) {\n    if (b_input_str.find(a_input_str[i_idx]) == std::string::npos) {\n      if (rhs_str.find(a_input_str[i_idx]) == std::string::npos) {\n        VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n        return false;\n      }\n\n      m_dim.set_size(m_dim.size() * a_input_shape.dim(i_idx).size());\n      continue;\n    } else if (rhs_str.find(a_input_str[i_idx]) == std::string::npos) {\n      // The dimension does not appear in the RHS, therefore it is a contracting\n      // dimension.\n      k_dim.set_size(k_dim.size() * a_input_shape.dim(i_idx).size());\n      continue;\n    }\n    // It appears in both input operands, therefore we place it as an outer\n    // dimension for the Batch Matmul.\n    *(a_matrix_shape->add_dim()) = a_input_shape.dim(i_idx);\n    *(b_matrix_shape->add_dim()) = a_input_shape.dim(i_idx);\n  }\n  for (int i_idx = 0, b_input_str_size = b_input_str.size();\n       i_idx < b_input_str_size; ++i_idx) {\n    if (a_input_str.find(b_input_str[i_idx]) == std::string::npos) {\n      if (rhs_str.find(b_input_str[i_idx]) == std::string::npos) {\n        VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n        return false;\n      }\n      n_dim.set_size(n_dim.size() * b_input_shape.dim(i_idx).size());\n    }\n  }\n\n  // The two inner-most dimensions of the Batch Matmul are added.\n  *(a_matrix_shape->add_dim()) = m_dim;\n  *(a_matrix_shape->add_dim()) = k_dim;\n  *(b_matrix_shape->add_dim()) = k_dim;\n  *(b_matrix_shape->add_dim()) = n_dim;\n\n  *batch_matmul_context = einsum_context;\n  batch_matmul_context->op_info = batch_matmul_op_info;\n  return true;\n}\n\nint64_t OpLevelCostEstimator::CountBatchMatMulOperations(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  return CountBatchMatMulOperations(op_info, nullptr, found_unknown_shapes);\n}\n\nint64_t OpLevelCostEstimator::CountBatchMatMulOperations(\n    const OpInfo& op_info, BatchMatMulDimensions* batch_mat_mul,\n    bool* found_unknown_shapes) {\n  if (op_info.op() != kBatchMatMul && op_info.op() != kBatchMatMulV2) {\n    LOG(ERROR) << \"Invalid Operation: \" << op_info.op();\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return 0;\n  }\n  if (op_info.inputs_size() != 2) {\n    LOG(ERROR) << \"Expected 2 inputs but got \" << op_info.inputs_size();\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return 0;\n  }\n\n  double ops = 0;\n  const auto& a_input = op_info.inputs(0);\n  const auto& b_input = op_info.inputs(1);\n\n  // BatchMatMul requires inputs of at least matrix shape (rank 2).\n  // The two most minor dimensions of each input are matrices that\n  // need to be multiplied together. The other dimensions determine\n  // the number of such MatMuls.  For example, if the BatchMatMul has\n  // inputs of shape:\n  //   a_input_shape = [2, 3, 4, 5]\n  //   b_input_shape = [2, 3, 5, 6]\n  // then there are 2*3 = 6 MatMuls of dimensions m = 4, k = 5, n = 6\n  // in this BatchMatMul.\n  const int matrix_rank = 2;\n\n  bool a_input_shape_unknown = false;\n  bool b_input_shape_unknown = false;\n\n  TensorShapeProto a_input_shape = MaybeGetMinimumShape(\n      a_input.shape(), std::max(matrix_rank, a_input.shape().dim_size()),\n      &a_input_shape_unknown);\n  TensorShapeProto b_input_shape = MaybeGetMinimumShape(\n      b_input.shape(), std::max(matrix_rank, b_input.shape().dim_size()),\n      &b_input_shape_unknown);\n\n  *found_unknown_shapes = a_input_shape_unknown || b_input_shape_unknown ||\n                          (a_input.shape().dim_size() < matrix_rank) ||\n                          (b_input.shape().dim_size() < matrix_rank);\n\n  // Compute the number of matmuls as the max indicated at each dimension\n  // by either input. Note that the shapes do not have to have\n  // the same rank due to incompleteness.\n  TensorShapeProto* bigger_rank_shape = &a_input_shape;\n  TensorShapeProto* smaller_rank_shape = &b_input_shape;\n  if (b_input_shape.dim_size() > a_input_shape.dim_size()) {\n    bigger_rank_shape = &b_input_shape;\n    smaller_rank_shape = &a_input_shape;\n  }\n  int num_matmuls = 1;\n  for (int b_i = 0,\n           s_i = smaller_rank_shape->dim_size() - bigger_rank_shape->dim_size();\n       b_i < bigger_rank_shape->dim_size() - matrix_rank; ++b_i, ++s_i) {\n    int b_dim = bigger_rank_shape->dim(b_i).size();\n    int s_dim = 1;\n    if (s_i >= 0) {\n      s_dim = smaller_rank_shape->dim(s_i).size();\n    }\n    if (batch_mat_mul != nullptr) {\n      batch_mat_mul->batch_dims.push_back(s_dim);\n    }\n    num_matmuls *= std::max(b_dim, s_dim);\n  }\n\n  // Build the MatMul. Note that values are ignored here since we are just\n  // counting ops (e.g. only shapes matter).\n  OpInfo matmul_op_info;\n  matmul_op_info.set_op(\"MatMul\");\n\n  AttrValue transpose_a;\n  transpose_a.set_b(false);\n  if (op_info.attr().find(\"adj_x\") != op_info.attr().end()) {\n    transpose_a.set_b(op_info.attr().at(\"adj_x\").b());\n  }\n  (*matmul_op_info.mutable_attr())[\"transpose_a\"] = transpose_a;\n\n  AttrValue transpose_b;\n  transpose_b.set_b(false);\n  if (op_info.attr().find(\"adj_y\") != op_info.attr().end()) {\n    transpose_b.set_b(op_info.attr().at(\"adj_y\").b());\n  }\n  (*matmul_op_info.mutable_attr())[\"transpose_b\"] = transpose_b;\n\n  OpInfo::TensorProperties* a_matrix = matmul_op_info.add_inputs();\n  a_matrix->set_dtype(a_input.dtype());\n  TensorShapeProto* a_matrix_shape = a_matrix->mutable_shape();\n  for (int i = std::max(0, a_input_shape.dim_size() - matrix_rank);\n       i < a_input_shape.dim_size(); ++i) {\n    *(a_matrix_shape->add_dim()) = a_input_shape.dim(i);\n  }\n\n  OpInfo::TensorProperties* b_matrix = matmul_op_info.add_inputs();\n  b_matrix->set_dtype(b_input.dtype());\n  TensorShapeProto* b_matrix_shape = b_matrix->mutable_shape();\n  for (int i = std::max(0, b_input_shape.dim_size() - matrix_rank);\n       i < b_input_shape.dim_size(); ++i) {\n    *(b_matrix_shape->add_dim()) = b_input_shape.dim(i);\n  }\n  if (batch_mat_mul != nullptr) {\n    batch_mat_mul->matmul_dims.m = (transpose_a.b())\n                                       ? a_matrix_shape->dim(1).size()\n                                       : a_matrix_shape->dim(0).size();\n    batch_mat_mul->matmul_dims.k = (transpose_a.b())\n                                       ? a_matrix_shape->dim(0).size()\n                                       : a_matrix_shape->dim(1).size();\n    batch_mat_mul->matmul_dims.n = (transpose_b.b())\n                                       ? b_matrix_shape->dim(0).size()\n                                       : b_matrix_shape->dim(1).size();\n  }\n\n  for (int i = 0; i < num_matmuls; ++i) {\n    bool matmul_unknown_shapes = false;\n    ops += CountMatMulOperations(matmul_op_info, &matmul_unknown_shapes);\n    *found_unknown_shapes |= matmul_unknown_shapes;\n  }\n  return ops;\n}\n\nbool GetTensorShapeProtoFromTensorProto(const TensorProto& tensor_proto,\n                                        TensorShapeProto* tensor_shape_proto) {\n  tensor_shape_proto->Clear();\n  // First convert TensorProto into Tensor class so that it correctly parses\n  // data values within TensorProto (whether it's in int_val, int64_val,\n  // tensor_content, or anything.\n  Tensor tensor(tensor_proto.dtype());\n  if (!tensor.FromProto(tensor_proto)) {\n    LOG(WARNING) << \"GetTensorShapeProtoFromTensorProto() -- \"\n                 << \"failed to parse TensorProto: \"\n                 << tensor_proto.DebugString();\n    return false;\n  }\n  if (tensor.dims() != 1) {\n    LOG(WARNING) << \"GetTensorShapeProtoFromTensorProto() -- \"\n                 << \"tensor is not 1D: \" << tensor.dims();\n    return false;\n  }\n  // Then, convert it back to TensorProto using AsProtoField, which makes sure\n  // the data is in int_val, int64_val, or such repeated data fields, not in\n  // tensor_content.\n  TensorProto temp_tensor;\n  tensor.AsProtoField(&temp_tensor);\n\n#define TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(type)        \\\n  do {                                                   \\\n    for (const auto& value : temp_tensor.type##_val()) { \\\n      tensor_shape_proto->add_dim()->set_size(value);    \\\n    }                                                    \\\n  } while (0)\n\n  if (tensor.dtype() == DT_INT32 || tensor.dtype() == DT_INT16 ||\n      tensor.dtype() == DT_INT8 || tensor.dtype() == DT_UINT8) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(int);\n  } else if (tensor.dtype() == DT_INT64) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(int64);\n  } else if (tensor.dtype() == DT_UINT32) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(uint32);\n  } else if (tensor.dtype() == DT_UINT64) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(uint64);\n  } else {\n    LOG(WARNING) << \"GetTensorShapeProtoFromTensorProto() -- \"\n                 << \"Unsupported dtype: \" << tensor.dtype();\n    return false;\n  }\n#undef TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO\n\n  return true;\n}\n\n// TODO(cliffy): Dedup this method and CountConv2DBackpropFilterOperations.\nint64_t OpLevelCostEstimator::CountConv2DBackpropInputOperations(\n    const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n    bool* found_unknown_shapes) {\n  int64_t ops = 0;\n\n  DCHECK(op_info.op() == kConv2dBackpropInput ||\n         op_info.op() == kDepthwiseConv2dNativeBackpropInput)\n      << \"Invalid Operation: not kConv2dBackpropInput nor\"\n         \"kDepthwiseConv2dNativeBackpropInput\";\n\n  if (op_info.inputs_size() < 2) {\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return ops;\n  }\n\n  TensorShapeProto input_shape;\n  bool shape_found = false;\n  if (op_info.inputs(0).has_value()) {\n    const TensorProto& value = op_info.inputs(0).value();\n    shape_found = GetTensorShapeProtoFromTensorProto(value, &input_shape);\n  }\n  if (!shape_found && op_info.outputs_size() == 1) {\n    input_shape = op_info.outputs(0).shape();\n    shape_found = true;\n  }\n  if (!shape_found) {\n    // Set the minimum filter size that's feasible.\n    input_shape.Clear();\n    for (int i = 0; i < 4; ++i) {\n      input_shape.add_dim()->set_size(1);\n    }\n    *found_unknown_shapes = true;\n  }\n\n  ConvolutionDimensions conv_dims = ConvolutionDimensionsFromInputs(\n      input_shape, op_info.inputs(1).shape(), op_info, found_unknown_shapes);\n\n  ops = conv_dims.batch;\n  ops *= conv_dims.ox * conv_dims.oy;\n  ops *= conv_dims.kx * conv_dims.ky;\n  if (op_info.op() == kConv2dBackpropInput) {\n    ops *= conv_dims.kz * conv_dims.oz;\n  } else {\n    // conv_dims always use forward path definition regardless\n    conv_dims.oz *= conv_dims.iz;\n    ops *= conv_dims.oz;\n  }\n  ops *= kOpsPerMac;\n\n  VLOG(1) << \"Operations for\" << op_info.op() << \"  \" << ops;\n\n  if (returned_conv_dims != nullptr) {\n    *returned_conv_dims = conv_dims;\n  }\n  return ops;\n}\n\nint64_t OpLevelCostEstimator::CountConv2DBackpropFilterOperations(\n    const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n    bool* found_unknown_shapes) {\n  int64_t ops = 0;\n\n  DCHECK(op_info.op() == kConv2dBackpropFilter ||\n         op_info.op() == kDepthwiseConv2dNativeBackpropFilter)\n      << \"Invalid Operation: not kConv2dBackpropFilter nor\"\n         \"kDepthwiseConv2dNativeBackpropFilter\";\n\n  TensorShapeProto filter_shape;\n  bool shape_found = false;\n  if (op_info.inputs_size() >= 2 && op_info.inputs(1).has_value()) {\n    const TensorProto& value = op_info.inputs(1).value();\n    shape_found = GetTensorShapeProtoFromTensorProto(value, &filter_shape);\n  }\n  if (!shape_found && op_info.outputs_size() == 1) {\n    filter_shape = op_info.outputs(0).shape();\n    shape_found = true;\n  }\n  if (!shape_found) {\n    // Set the minimum filter size that's feasible.\n    filter_shape.Clear();\n    for (int i = 0; i < 4; ++i) {\n      filter_shape.add_dim()->set_size(1);\n    }\n    *found_unknown_shapes = true;\n  }\n\n  if (op_info.inputs_size() < 1) {\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return ops;\n  }\n  ConvolutionDimensions conv_dims = ConvolutionDimensionsFromInputs(\n      op_info.inputs(0).shape(), filter_shape, op_info, found_unknown_shapes);\n\n  ops = conv_dims.batch;\n  ops *= conv_dims.ox * conv_dims.oy;\n  ops *= conv_dims.kx * conv_dims.ky;\n  if (op_info.op() == kConv2dBackpropFilter) {\n    ops *= conv_dims.kz * conv_dims.oz;\n  } else {\n    // conv_dims always use forward path definition regardless\n    conv_dims.oz *= conv_dims.iz;\n    ops *= conv_dims.oz;\n  }\n  ops *= kOpsPerMac;\n  VLOG(1) << \"Operations for\" << op_info.op() << \"  \" << ops;\n\n  if (returned_conv_dims != nullptr) {\n    *returned_conv_dims = conv_dims;\n  }\n  return ops;\n}\n\nint64_t OpLevelCostEstimator::CalculateTensorElementCount(\n    const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes) {\n  VLOG(2) << \"   with \" << DataTypeString(tensor.dtype()) << \" tensor of shape \"\n          << tensor.shape().DebugString();\n  int64_t tensor_size = 1;\n  int num_dims = std::max(1, tensor.shape().dim_size());\n  auto tensor_shape =\n      MaybeGetMinimumShape(tensor.shape(), num_dims, found_unknown_shapes);\n  for (const auto& dim : tensor_shape.dim()) {\n    int64_t new_tensor_size = MultiplyWithoutOverflow(tensor_size, dim.size());\n    if (new_tensor_size < 0) {\n      VLOG(1) << \"Overflow encountered when computing element count of a \"\n                 \"tensor, multiplying \"\n              << tensor_size << \" with \" << dim.size();\n      return -1;\n    }\n    tensor_size = new_tensor_size;\n  }\n  return tensor_size;\n}\n\nint64_t OpLevelCostEstimator::CalculateTensorSize(\n    const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes) {\n  int64_t count = CalculateTensorElementCount(tensor, found_unknown_shapes);\n  int size = DataTypeSize(BaseType(tensor.dtype()));\n  VLOG(2) << \"Count: \" << count << \" DataTypeSize: \" << size;\n  int64_t tensor_size = MultiplyWithoutOverflow(count, size);\n  if (tensor_size < 0) {\n    VLOG(1) << \"Overflow encountered when computing tensor size, multiplying \"\n            << count << \" with \" << size;\n    return -1;\n  }\n  return tensor_size;\n}\n\nint64_t OpLevelCostEstimator::CalculateInputSize(const OpInfo& op_info,\n                                                 bool* found_unknown_shapes) {\n  int64_t total_input_size = 0;\n  for (auto& input : op_info.inputs()) {\n    int64_t input_size = CalculateTensorSize(input, found_unknown_shapes);\n    total_input_size += input_size;\n    VLOG(1) << \"Input Size: \" << input_size\n            << \" Total Input Size:\" << total_input_size;\n  }\n  return total_input_size;\n}\n\nstd::vector<int64_t> OpLevelCostEstimator::CalculateInputTensorSize(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  std::vector<int64_t> input_tensor_size;\n  input_tensor_size.reserve(op_info.inputs().size());\n  for (auto& input : op_info.inputs()) {\n    input_tensor_size.push_back(\n        CalculateTensorSize(input, found_unknown_shapes));\n  }\n  return input_tensor_size;\n}\n\nint64_t OpLevelCostEstimator::CalculateLargestInputCount(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  int64_t largest_input_count = 0;\n  for (auto& input : op_info.inputs()) {\n    int64_t input_count =\n        CalculateTensorElementCount(input, found_unknown_shapes);\n    if (input_count > largest_input_count) {\n      largest_input_count = input_count;\n    }\n    VLOG(1) << \"Input Count: \" << input_count\n            << \" Largest Input Count:\" << largest_input_count;\n  }\n  return largest_input_count;\n}\n\nint64_t OpLevelCostEstimator::CalculateOutputSize(const OpInfo& op_info,\n                                                  bool* found_unknown_shapes) {\n  int64_t total_output_size = 0;\n  // Use float as default for calculations.\n  for (const auto& output : op_info.outputs()) {\n    DataType dt = output.dtype();\n    const auto& original_output_shape = output.shape();\n    int64_t output_size = DataTypeSize(BaseType(dt));\n    int num_dims = std::max(1, original_output_shape.dim_size());\n    auto output_shape = MaybeGetMinimumShape(original_output_shape, num_dims,\n                                             found_unknown_shapes);\n    for (const auto& dim : output_shape.dim()) {\n      int64_t new_output_size =\n          MultiplyWithoutOverflow(output_size, dim.size());\n      if (new_output_size < 0) {\n        VLOG(1) << \"Overflow encountered when estimating cost, multiplying \"\n                << output_size << \" with \" << dim.size();\n        return -1;\n      }\n      output_size = new_output_size;\n    }\n    total_output_size += output_size;\n    VLOG(1) << \"Output Size: \" << output_size\n            << \" Total Output Size:\" << total_output_size;\n  }\n  return total_output_size;\n}\n\nstd::vector<int64_t> OpLevelCostEstimator::CalculateOutputTensorSize(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  std::vector<int64_t> output_tensor_size;\n  output_tensor_size.reserve(op_info.outputs().size());\n  // Use float as default for calculations.\n  for (const auto& output : op_info.outputs()) {\n    DataType dt = output.dtype();\n    const auto& original_output_shape = output.shape();\n    int64_t output_size = DataTypeSize(BaseType(dt));\n    int num_dims = std::max(1, original_output_shape.dim_size());\n    auto output_shape = MaybeGetMinimumShape(original_output_shape, num_dims,\n                                             found_unknown_shapes);\n    for (const auto& dim : output_shape.dim()) {\n      output_size *= dim.size();\n    }\n    output_tensor_size.push_back(output_size);\n  }\n  return output_tensor_size;\n}\n\nStatus OpLevelCostEstimator::PredictDefaultNodeCosts(\n    const int64_t num_compute_ops, const OpContext& op_context,\n    bool* found_unknown_shapes, NodeCosts* node_costs) {\n  const auto& op_info = op_context.op_info;\n  node_costs->num_compute_ops = num_compute_ops;\n  node_costs->num_input_bytes_accessed =\n      CalculateInputTensorSize(op_info, found_unknown_shapes);\n  node_costs->num_output_bytes_accessed =\n      CalculateOutputTensorSize(op_info, found_unknown_shapes);\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n  if (*found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nbool HasZeroDim(const OpInfo& op_info) {\n  for (int i = 0; i < op_info.inputs_size(); ++i) {\n    const auto& input = op_info.inputs(i);\n    for (int j = 0; j < input.shape().dim_size(); ++j) {\n      const auto& dim = input.shape().dim(j);\n      if (dim.size() == 0) {\n        VLOG(1) << \"Convolution config has zero dim \"\n                << op_info.ShortDebugString();\n        return true;\n      }\n    }\n  }\n  return false;\n}\n\nStatus OpLevelCostEstimator::PredictConv2D(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  if (HasZeroDim(op_info)) {\n    node_costs->num_nodes_with_unknown_shapes = 1;\n    return errors::InvalidArgument(\"Conv2D op includes zero dimension: \",\n                                   op_info.ShortDebugString());\n  }\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops =\n      CountConv2DOperations(op_info, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictConv2DBackpropInput(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  if (HasZeroDim(op_info)) {\n    node_costs->num_nodes_with_unknown_shapes = 1;\n    return errors::InvalidArgument(\n        \"Conv2DBackpropInput op includes zero dimension\",\n        op_info.ShortDebugString());\n  }\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops = CountConv2DBackpropInputOperations(\n      op_info, nullptr, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictConv2DBackpropFilter(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  if (HasZeroDim(op_info)) {\n    node_costs->num_nodes_with_unknown_shapes = 1;\n    return errors::InvalidArgument(\n        \"Conv2DBackpropFilter op includes zero dimension\",\n        op_info.ShortDebugString());\n  }\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops = CountConv2DBackpropFilterOperations(\n      op_info, nullptr, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictFusedConv2DBiasActivation(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  // FusedConv2DBiasActivation computes a fused kernel which implements:\n  // 2D convolution, adds side input with separate scaling on convolution and\n  // side inputs, then adds bias, and finally applies the ReLU activation\n  // function to the result:\n  //\n  // Input -> Conv2D  ->  Add  -> BiasAdd  -> ReLU\n  //            ^          ^         ^\n  //          Filter   Side Input   Bias\n  //\n  // Note that when adding the side input, the operation multiplies the output\n  // of Conv2D by conv_input_scale, confusingly, and the side_input by\n  // side_input_scale.\n  //\n  // Note that in the special case that side_input_scale is 0, which we infer\n  // from side_input having dimensions [], we skip that addition operation.\n  //\n  // For more information, see\n  // contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc\n\n  // TODO(yaozhang): Support NHWC_VECT_W.\n  std::string data_format = GetDataFormat(op_context.op_info);\n  if (data_format != \"NCHW\" && data_format != \"NHWC\" &&\n      data_format != \"NCHW_VECT_C\") {\n    return errors::InvalidArgument(\n        \"Unsupported data format (\", data_format,\n        \") for op: \", op_context.op_info.ShortDebugString());\n  }\n  std::string filter_format = GetFilterFormat(op_context.op_info);\n  if (filter_format != \"HWIO\" && filter_format != \"OIHW\" &&\n      filter_format != \"OIHW_VECT_I\") {\n    return errors::InvalidArgument(\n        \"Unsupported filter format (\", filter_format,\n        \") for op: \", op_context.op_info.ShortDebugString());\n  }\n\n  auto& conv_input = op_context.op_info.inputs(0);\n  auto& filter = op_context.op_info.inputs(1);\n  auto& side_input = op_context.op_info.inputs(3);\n  auto& conv_input_scale = op_context.op_info.inputs(4);\n  auto& side_input_scale = op_context.op_info.inputs(5);\n\n  // Manually compute our convolution dimensions.\n  bool found_unknown_shapes = false;\n  auto dims = ConvolutionDimensionsFromInputs(\n      conv_input.shape(), filter.shape(), op_context.op_info,\n      &found_unknown_shapes);\n  OpInfo::TensorProperties output;\n  if (data_format == \"NCHW\" || data_format == \"NCHW_VECT_C\") {\n    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.oz, dims.oy, dims.ox});\n  } else if (data_format == \"NHWC\") {\n    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.oy, dims.ox, dims.oz});\n  }\n\n  // Add the operations the fused op always computes.\n  std::vector<OpContext> component_ops = {\n      FusedChildContext(op_context, \"Conv2D\", output, {conv_input, filter}),\n      FusedChildContext(op_context, \"Mul\", output, {output, conv_input_scale}),\n      FusedChildContext(\n          op_context, \"BiasAdd\", output,\n          {output, output}),  // Note we're no longer using bias at all\n      FusedChildContext(op_context, \"Relu\", output, {output})};\n\n  // Add our side_input iff it's non-empty.\n  if (side_input.shape().dim_size() > 0) {\n    component_ops.push_back(FusedChildContext(op_context, \"Mul\", side_input,\n                                              {side_input, side_input_scale}));\n    component_ops.push_back(FusedChildContext(\n        op_context, \"Add\", output,\n        {output, output}));  // Note that we're not using side_input here\n  }\n\n  // Construct an op_context which definitely has our output shape.\n  auto op_context_with_output = op_context;\n  op_context_with_output.op_info.mutable_outputs()->Clear();\n  *op_context_with_output.op_info.mutable_outputs()->Add() = output;\n\n  // Construct component operations and run the cost computation.\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return PredictFusedOp(op_context_with_output, component_ops, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictMatMul(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops =\n      CountMatMulOperations(op_info, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictEinsum(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n\n  auto it = op_info.attr().find(\"equation\");\n  if (it == op_info.attr().end()) {\n    return errors::InvalidArgument(\"Einsum op doesn't have equation attr: \",\n                                   op_info.ShortDebugString());\n  }\n\n  OpContext batch_matmul_op_context;\n  bool found_unknown_shapes = false;\n  bool success = GenerateBatchMatmulContextFromEinsum(\n      op_context, &batch_matmul_op_context, &found_unknown_shapes);\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  if (!success) {\n    return PredictCostOfAnUnknownOp(op_context, node_costs);\n  }\n  return PredictNodeCosts(batch_matmul_op_context, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictSparseTensorDenseMatMul(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  // input[0]: indices in sparse matrix a\n  // input[1]: values in sparse matrix a\n  // input[2]: shape of matrix a\n  // input[3]: matrix b\n  // See\n  // https://github.com/tensorflow/tensorflow/blob/9a43dfeac5/tensorflow/core/ops/sparse_ops.cc#L85\n  int64_t num_elems_in_a =\n      CalculateTensorElementCount(op_info.inputs(1), &found_unknown_shapes);\n  auto b_matrix = op_info.inputs(3);\n  auto b_matrix_shape =\n      MaybeGetMinimumShape(b_matrix.shape(), 2, &found_unknown_shapes);\n  int64_t n_dim = b_matrix_shape.dim(1).size();\n\n  // Each element in A is multiplied and added with an element from each column\n  // in b.\n  const int64_t op_count = kOpsPerMac * num_elems_in_a * n_dim;\n\n  int64_t a_indices_input_size =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  int64_t a_values_input_size =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  int64_t a_shape_input_size =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  int64_t b_input_size =\n      num_elems_in_a * n_dim * DataTypeSize(BaseType(b_matrix.dtype()));\n  int64_t output_size = CalculateOutputSize(op_info, &found_unknown_shapes);\n\n  node_costs->num_compute_ops = op_count;\n  node_costs->num_input_bytes_accessed = {a_indices_input_size,\n                                          a_values_input_size,\n                                          a_shape_input_size, b_input_size};\n  node_costs->num_output_bytes_accessed = {output_size};\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictNoOp(const OpContext& op_context,\n                                         NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  VLOG(1) << \"Op:\" << op_info.op() << \" Execution Time 0 (ns)\";\n  // By default, NodeCosts is initialized to zero ops and bytes.\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictPureMemoryOp(const OpContext& op_context,\n                                                 NodeCosts* node_costs) const {\n  // Each output element is a copy of some element from input, with no required\n  // computation, so just compute memory costs.\n  bool found_unknown_shapes = false;\n  node_costs->num_nodes_with_pure_memory_op = 1;\n  return PredictDefaultNodeCosts(0, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictIdentity(const OpContext& op_context,\n                                             NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  VLOG(1) << \"Op:\" << op_info.op() << \" Minimum cost for Identity\";\n  node_costs->minimum_cost_op = true;\n  node_costs->num_compute_ops = kMinComputeOp;\n  // Identity op internally pass input tensor buffer's pointer to the output\n  // tensor buffer; no actual memory operation.\n  node_costs->num_input_bytes_accessed = {0};\n  node_costs->num_output_bytes_accessed = {0};\n  bool inaccurate = false;\n  node_costs->max_memory = CalculateOutputSize(op_info, &inaccurate);\n  if (inaccurate) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictVariable(const OpContext& op_context,\n                                             NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  VLOG(1) << \"Op:\" << op_info.op() << \" Minimum cost for Variable\";\n  node_costs->minimum_cost_op = true;\n  node_costs->num_compute_ops = kMinComputeOp;\n  // Variables are persistent ops; initialized before step; hence, no memory\n  // cost.\n  node_costs->num_input_bytes_accessed = {0};\n  node_costs->num_output_bytes_accessed = {0};\n  bool inaccurate = false;\n  node_costs->persistent_memory = CalculateOutputSize(op_info, &inaccurate);\n  if (inaccurate) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictBatchMatMul(const OpContext& op_context,\n                                                NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops =\n      CountBatchMatMulOperations(op_info, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictMetadata(const OpContext& op_context,\n                                             NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  node_costs->minimum_cost_op = true;\n  node_costs->num_compute_ops = kMinComputeOp;\n  node_costs->num_input_bytes_accessed = {0};\n  node_costs->num_output_bytes_accessed = {0};\n  bool inaccurate = false;\n  node_costs->max_memory = CalculateOutputSize(op_info, &inaccurate);\n  if (inaccurate) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictGatherOrSlice(const OpContext& op_context,\n                                                  NodeCosts* node_costs) const {\n  // Gather & Slice ops can have a very large input, but only access a small\n  // part of it. For these op the size of the output determines the memory cost.\n  const auto& op_info = op_context.op_info;\n\n  const int inputs_needed = op_info.op() == \"Slice\" ? 3 : 2;\n  if (op_info.outputs_size() == 0 || op_info.inputs_size() < inputs_needed) {\n    return errors::InvalidArgument(\n        op_info.op(),\n        \" Op doesn't have valid input / output: \", op_info.ShortDebugString());\n  }\n\n  bool unknown_shapes = false;\n\n  // Each output element is a copy of some element from input.\n  // For roofline estimate we assume each copy has a unit cost.\n  const int64_t op_count =\n      CalculateTensorElementCount(op_info.outputs(0), &unknown_shapes);\n  node_costs->num_compute_ops = op_count;\n\n  const int64_t output_size = CalculateOutputSize(op_info, &unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n\n  node_costs->num_input_bytes_accessed.reserve(op_info.inputs().size());\n  int64_t input_size = output_size;\n  // Note that input(0) byte accessed is not equal to input(0) tensor size.\n  // It's equal to the output size; though, input access is indexed gather or\n  // slice (ignore duplicate indices).\n  node_costs->num_input_bytes_accessed.push_back(input_size);\n  int begin_input_index = 1;\n  int end_input_index;\n  if (op_info.op() == \"Slice\") {\n    // Slice: 'input' (omitted), 'begin', 'size'\n    end_input_index = 3;\n  } else if (op_info.op() == \"StridedSlice\") {\n    // StridedSlice: 'input' (omitted), 'begin', 'end', 'strides'\n    end_input_index = 4;\n  } else {\n    // Gather, GatherV2, GatherNd: 'params' (omitted), 'indices'\n    end_input_index = 2;\n  }\n  for (int i = begin_input_index; i < end_input_index; ++i) {\n    node_costs->num_input_bytes_accessed.push_back(\n        CalculateTensorElementCount(op_info.inputs(i), &unknown_shapes));\n  }\n  if (unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictScatter(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  // Scatter ops sparsely access a reference input and output tensor.\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n\n  // input[0]: ref tensor that will be sparsely accessed\n  // input[1]: indices - A tensor of indices into the first dimension of ref.\n  // input[2]: updates where updates.shape = indices.shape + ref.shape[1:]\n  // See\n  // https://www.tensorflow.org/api_docs/python/tf/scatter_add and\n  // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/state_ops.cc#L146\n\n  const int64_t num_indices =\n      CalculateTensorElementCount(op_info.inputs(1), &found_unknown_shapes);\n\n  int64_t num_elems_in_ref_per_index = 1;\n  auto ref_tensor_shape = MaybeGetMinimumShape(\n      op_info.inputs(0).shape(), op_info.inputs(0).shape().dim_size(),\n      &found_unknown_shapes);\n  for (int i = 1; i < ref_tensor_shape.dim().size(); ++i) {\n    num_elems_in_ref_per_index *= ref_tensor_shape.dim(i).size();\n  }\n  const int64_t op_count = num_indices * num_elems_in_ref_per_index;\n  node_costs->num_compute_ops = op_count;\n\n  // Sparsely access ref so input size depends on the number of operations\n  int64_t ref_input_size =\n      op_count * DataTypeSize(BaseType(op_info.inputs(0).dtype()));\n  int64_t indices_input_size =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  int64_t updates_input_size =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  node_costs->num_input_bytes_accessed = {ref_input_size, indices_input_size,\n                                          updates_input_size};\n\n  // Sparsely access ref so output size depends on the number of operations\n  int64_t output_size =\n      op_count * DataTypeSize(BaseType(op_info.outputs(0).dtype()));\n  node_costs->num_output_bytes_accessed = {output_size};\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictFusedOp(\n    const OpContext& op_context,\n    const std::vector<OpContext>& fused_op_contexts,\n    NodeCosts* node_costs) const {\n  // Note that PredictDefaultNodeCosts will get the correct memory costs from\n  // the node's inputs and outputs; but we don't want to have to re-implement\n  // the logic for computing the operation count of each of our component\n  // operations here; so we simply add the compute times of each component\n  // operation, then update the cost.\n  bool found_unknown_shapes = false;\n  Status s =\n      PredictDefaultNodeCosts(0, op_context, &found_unknown_shapes, node_costs);\n\n  for (auto& fused_op : fused_op_contexts) {\n    NodeCosts fused_node_costs;\n    s.Update(PredictNodeCosts(fused_op, &fused_node_costs));\n    node_costs->num_compute_ops += fused_node_costs.num_compute_ops;\n    node_costs->inaccurate |= fused_node_costs.inaccurate;\n    // Set, not increment. Note that we are predicting the cost of one fused\n    // node, not a function node composed of many nodes.\n    node_costs->num_nodes_with_unknown_shapes |=\n        fused_node_costs.num_nodes_with_unknown_shapes;\n    node_costs->num_nodes_with_unknown_op_type |=\n        fused_node_costs.num_nodes_with_unknown_op_type;\n    node_costs->num_nodes_with_pure_memory_op |=\n        fused_node_costs.num_nodes_with_pure_memory_op;\n  }\n\n  return Status::OK();\n}\n\n/* static */\nOpContext OpLevelCostEstimator::FusedChildContext(\n    const OpContext& parent, const std::string& op_name,\n    const OpInfo::TensorProperties& output,\n    const std::vector<OpInfo::TensorProperties>& inputs) {\n  // Setup the base parameters of our new context.\n  OpContext new_context;\n  new_context.name = op_name;\n  new_context.device_name = parent.device_name;\n  new_context.op_info = parent.op_info;\n  new_context.op_info.set_op(op_name);\n\n  // Setup the inputs of our new context.\n  new_context.op_info.mutable_inputs()->Clear();\n  for (const auto& input : inputs) {\n    *new_context.op_info.mutable_inputs()->Add() = input;\n  }\n\n  // Setup the output of our new context.\n  new_context.op_info.mutable_outputs()->Clear();\n  *new_context.op_info.mutable_outputs()->Add() = output;\n\n  return new_context;\n}\n\n/* static */\nOpInfo::TensorProperties OpLevelCostEstimator::DescribeTensor(\n    DataType type, const std::vector<int64_t>& dims) {\n  OpInfo::TensorProperties ret;\n  ret.set_dtype(type);\n\n  auto shape = ret.mutable_shape();\n  for (const int dim : dims) {\n    shape->add_dim()->set_size(dim);\n  }\n\n  return ret;\n}\n\n/* static */\nOpLevelCostEstimator::ConvolutionDimensions\nOpLevelCostEstimator::OpDimensionsFromInputs(\n    const TensorShapeProto& original_image_shape, const OpInfo& op_info,\n    bool* found_unknown_shapes) {\n  VLOG(2) << \"op features: \" << op_info.DebugString();\n  VLOG(2) << \"Original image shape: \" << original_image_shape.DebugString();\n  auto image_shape =\n      MaybeGetMinimumShape(original_image_shape, 4, found_unknown_shapes);\n  VLOG(2) << \"Image shape: \" << image_shape.DebugString();\n\n  int x_index, y_index, channel_index;\n  const std::string& data_format = GetDataFormat(op_info);\n  if (data_format == \"NCHW\") {\n    channel_index = 1;\n    y_index = 2;\n    x_index = 3;\n  } else {\n    y_index = 1;\n    x_index = 2;\n    channel_index = 3;\n  }\n  int64_t batch = image_shape.dim(0).size();\n  int64_t ix = image_shape.dim(x_index).size();\n  int64_t iy = image_shape.dim(y_index).size();\n  int64_t iz = image_shape.dim(channel_index).size();\n\n  // Note that FusedBatchNorm doesn't have ksize attr, but GetKernelSize returns\n  // {1, 1, 1, 1} in that case.\n  std::vector<int64_t> ksize = GetKernelSize(op_info);\n  int64_t kx = ksize[x_index];\n  int64_t ky = ksize[y_index];\n  // These ops don't support groupwise operation, therefore kz == iz.\n  int64_t kz = iz;\n\n  std::vector<int64_t> strides = GetStrides(op_info);\n  int64_t sx = strides[x_index];\n  int64_t sy = strides[y_index];\n  const auto padding = GetPadding(op_info);\n\n  int64_t ox = GetOutputSize(ix, kx, sx, padding);\n  int64_t oy = GetOutputSize(iy, ky, sy, padding);\n  int64_t oz = iz;\n\n  OpLevelCostEstimator::ConvolutionDimensions conv_dims = {\n      batch, ix, iy, iz, kx, ky, kz, oz, ox, oy, sx, sy, padding};\n  return conv_dims;\n}\n\nStatus OpLevelCostEstimator::PredictMaxPool(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  ConvolutionDimensions dims = OpDimensionsFromInputs(\n      op_info.inputs(0).shape(), op_info, &found_unknown_shapes);\n  // kx * ky - 1 comparisons per output (kx * xy > 1)\n  // or 1 copy per output (kx * k1 = 1).\n  int per_output_ops = dims.kx * dims.ky == 1 ? 1 : dims.kx * dims.ky - 1;\n  int64_t ops = dims.batch * dims.ox * dims.oy * dims.oz * per_output_ops;\n  node_costs->num_compute_ops = ops;\n\n  int64_t input_size = 0;\n  if (dims.ky >= dims.sy) {\n    input_size = CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  } else {  // dims.ky < dims.sy\n    // Vertical stride is larger than vertical kernel; assuming row-major\n    // format, skip unnecessary rows (or read every kx rows per sy rows, as the\n    // others are not used for output).\n    const auto data_size = DataTypeSize(BaseType(op_info.inputs(0).dtype()));\n    input_size = data_size * dims.batch * dims.ix * dims.ky * dims.oy * dims.iz;\n  }\n  node_costs->num_input_bytes_accessed = {input_size};\n  const int64_t output_size =\n      CalculateOutputSize(op_info, &found_unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n  node_costs->max_memory = output_size;\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictMaxPoolGrad(const OpContext& op_context,\n                                                NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  // y: op_info.inputs(1)\n  // y_grad: op_info.inputs(2)\n  if (op_info.inputs_size() < 3) {\n    return errors::InvalidArgument(\"MaxPoolGrad op has invalid inputs: \",\n                                   op_info.ShortDebugString());\n  }\n\n  ConvolutionDimensions dims = OpDimensionsFromInputs(\n      op_info.inputs(0).shape(), op_info, &found_unknown_shapes);\n\n  int64_t ops = 0;\n  if (dims.kx == 1 && dims.ky == 1) {\n    // 1x1 window. No need to know which input was max.\n    ops = dims.batch * dims.ix * dims.iy * dims.iz;\n  } else if (dims.kx <= dims.sx && dims.ky <= dims.sy) {\n    // Non-overlapping window: re-run maxpool, then assign zero or y_grad.\n    ops = dims.batch * dims.iz *\n          (dims.ox * dims.oy * (dims.kx * dims.ky - 1) + dims.ix * dims.iy);\n  } else {\n    // Overlapping window: initialize with zeros, re-run maxpool, then\n    // accumulate y_gad to proper x_grad locations.\n    ops = dims.batch * dims.iz *\n          (dims.ox * dims.oy * (dims.kx * dims.ky - 1) + dims.ix * dims.iy * 2);\n  }\n  node_costs->num_compute_ops = ops;\n\n  // Just read x and y_grad; no need to read y as we assume MaxPoolGrad re-run\n  // MaxPool internally.\n  const int64_t input0_size =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  const int64_t input2_size =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  node_costs->num_input_bytes_accessed = {input0_size, 0, input2_size};\n  // Write x_grad; size equal to x.\n  const int64_t output_size =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n  node_costs->max_memory = output_size;\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\n/* This predict function handles three types of tensorflow ops\n * AssignVariableOp/AssignAddVariableOp/AssignSubVariableOp, broadcasting\n * was not possible for these ops, therefore the input tensor's shapes is\n * enough to compute the cost */\nStatus OpLevelCostEstimator::PredictAssignVariableOps(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  /* First input of these ops are reference to the assignee. */\n  if (op_info.inputs_size() != 2) {\n    return errors::InvalidArgument(\"AssignVariable op has invalid input: \",\n                                   op_info.ShortDebugString());\n  }\n\n  const int64_t ops = op_info.op() == kAssignVariableOp\n                          ? 0\n                          : CalculateTensorElementCount(op_info.inputs(1),\n                                                        &found_unknown_shapes);\n  node_costs->num_compute_ops = ops;\n  const int64_t input_size = CalculateInputSize(op_info, &found_unknown_shapes);\n  node_costs->num_input_bytes_accessed = {input_size};\n  // TODO(dyoon): check these ops' behavior whether it writes data;\n  // Op itself doesn't have output tensor, but it may modify the input (ref or\n  // resource). Maybe use node_costs->internal_write_bytes.\n  node_costs->num_output_bytes_accessed = {0};\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictAvgPool(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  ConvolutionDimensions dims = OpDimensionsFromInputs(\n      op_info.inputs(0).shape(), op_info, &found_unknown_shapes);\n\n  // kx * ky - 1 additions and 1 multiplication per output.\n  int64_t ops = dims.batch * dims.ox * dims.oy * dims.oz * dims.kx * dims.ky;\n  node_costs->num_compute_ops = ops;\n\n  int64_t input_size;\n  if (dims.ky >= dims.sy) {\n    input_size = CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  } else {  // dims.ky < dims.sy\n    // vertical stride is larger than vertical kernel; assuming row-major\n    // format, skip unnecessary rows (or read every kx rows per sy rows, as the\n    // others are not used for output).\n    const auto data_size = DataTypeSize(BaseType(op_info.inputs(0).dtype()));\n    input_size = data_size * dims.batch * dims.ix * dims.ky * dims.oy * dims.iz;\n  }\n  node_costs->num_input_bytes_accessed = {input_size};\n\n  const int64_t output_size =\n      CalculateOutputSize(op_info, &found_unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n  node_costs->max_memory = output_size;\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictAvgPoolGrad(const OpContext& op_context,\n                                                NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x's shape: op_info.inputs(0)\n  // y_grad: op_info.inputs(1)\n\n  // Extract x_shape from op_info.inputs(0).value() or op_info.outputs(0).\n  bool shape_found = false;\n  TensorShapeProto x_shape;\n  if (op_info.inputs_size() >= 1 && op_info.inputs(0).has_value()) {\n    const TensorProto& value = op_info.inputs(0).value();\n    shape_found = GetTensorShapeProtoFromTensorProto(value, &x_shape);\n  }\n  if (!shape_found && op_info.outputs_size() > 0) {\n    x_shape = op_info.outputs(0).shape();\n    shape_found = true;\n  }\n  if (!shape_found) {\n    // Set the minimum shape that's feasible.\n    x_shape.Clear();\n    for (int i = 0; i < 4; ++i) {\n      x_shape.add_dim()->set_size(1);\n    }\n    found_unknown_shapes = true;\n  }\n\n  ConvolutionDimensions dims =\n      OpDimensionsFromInputs(x_shape, op_info, &found_unknown_shapes);\n\n  int64_t ops = 0;\n  if (dims.kx <= dims.sx && dims.ky <= dims.sy) {\n    // Non-overlapping window.\n    ops = dims.batch * dims.iz * (dims.ix * dims.iy + dims.ox * dims.oy);\n  } else {\n    // Overlapping window.\n    ops = dims.batch * dims.iz *\n          (dims.ix * dims.iy + dims.ox * dims.oy * (dims.kx * dims.ky + 1));\n  }\n  auto s = PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                   node_costs);\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n  return s;\n}\n\nStatus OpLevelCostEstimator::PredictFusedBatchNorm(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  // scale: op_info.inputs(1)\n  // offset: op_info.inputs(2)\n  // mean: op_info.inputs(3)  --> only for inference\n  // variance: op_info.inputs(4) --> only for inference\n  ConvolutionDimensions dims = OpDimensionsFromInputs(\n      op_info.inputs(0).shape(), op_info, &found_unknown_shapes);\n  const bool is_training = IsTraining(op_info);\n\n  int64_t ops = 0;\n  const auto rsqrt_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_rsqrt_op<float>>::Cost;\n  if (is_training) {\n    ops = dims.iz * (dims.batch * dims.ix * dims.iy * 4 + 6 + rsqrt_cost);\n  } else {\n    ops = dims.batch * dims.ix * dims.iy * dims.iz * 2;\n  }\n  node_costs->num_compute_ops = ops;\n\n  const int64_t size_nhwc =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  const int64_t size_c =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  if (is_training) {\n    node_costs->num_input_bytes_accessed = {size_nhwc, size_c, size_c};\n    node_costs->num_output_bytes_accessed = {size_nhwc, size_c, size_c, size_c,\n                                             size_c};\n    // FusedBatchNorm in training mode internally re-reads the input tensor:\n    // one for mean/variance, and the 2nd internal read forthe actual scaling.\n    // Assume small intermediate data such as mean / variance (size_c) can be\n    // cached on-chip.\n    node_costs->internal_read_bytes = size_nhwc;\n  } else {\n    node_costs->num_input_bytes_accessed = {size_nhwc, size_c, size_c, size_c,\n                                            size_c};\n    node_costs->num_output_bytes_accessed = {size_nhwc};\n  }\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictFusedBatchNormGrad(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // y_backprop: op_info.inputs(0)\n  // x: op_info.inputs(1)\n  // scale: op_info.inputs(2)\n  // mean: op_info.inputs(3)\n  // variance or inverse of variance: op_info.inputs(4)\n  ConvolutionDimensions dims = OpDimensionsFromInputs(\n      op_info.inputs(1).shape(), op_info, &found_unknown_shapes);\n\n  int64_t ops = 0;\n  const auto rsqrt_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_rsqrt_op<float>>::Cost;\n  ops = dims.iz * (dims.batch * dims.ix * dims.iy * 11 + 5 + rsqrt_cost);\n  node_costs->num_compute_ops = ops;\n\n  const int64_t size_nhwc =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  const int64_t size_c =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  // TODO(dyoon): fix missing memory cost for variance input (size_c) and\n  // yet another read of y_backprop (size_nhwc) internally.\n  node_costs->num_input_bytes_accessed = {size_nhwc, size_nhwc, size_c, size_c};\n  node_costs->num_output_bytes_accessed = {size_nhwc, size_c, size_c};\n  // FusedBatchNormGrad has to read y_backprop internally.\n  node_costs->internal_read_bytes = size_nhwc;\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictNaryOp(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  // Calculate the largest known tensor size across all inputs and output.\n  int64_t op_count = CalculateLargestInputCount(op_info, &found_unknown_shapes);\n  // If output shape is available, try to use the element count calculated from\n  // that.\n  if (op_info.outputs_size() > 0) {\n    op_count = std::max(\n        op_count,\n        CalculateTensorElementCount(op_info.outputs(0), &found_unknown_shapes));\n  }\n  // Also calculate the output shape possibly resulting from broadcasting.\n  // Note that the some Nary ops (such as AddN) do not support broadcasting,\n  // but we're including this here for completeness.\n  if (op_info.inputs_size() >= 2) {\n    op_count = std::max(op_count, CwiseOutputElementCount(op_info));\n  }\n\n  // Nary ops perform one operation for every element in every input tensor.\n  op_count *= op_info.inputs_size() - 1;\n\n  const auto sum_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_sum_op<float>>::Cost;\n  return PredictDefaultNodeCosts(op_count * sum_cost, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\n// softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))\nStatus OpLevelCostEstimator::PredictSoftmax(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const int64_t logits_size = CalculateTensorElementCount(\n      op_context.op_info.inputs(0), &found_unknown_shapes);\n  // Softmax input rank should be >=1.\n  TensorShapeProto logits_shape = op_context.op_info.inputs(0).shape();\n  if (logits_shape.unknown_rank() || logits_shape.dim_size() == 0) {\n    return errors::InvalidArgument(\"Softmax op has invalid input: \",\n                                   op_context.op_info.ShortDebugString());\n  }\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n\n  // Every element of <logits> will be exponentiated, have that result included\n  // in a sum across j, and also have that result multiplied by the reciprocal\n  // of the sum_j. In addition, we'll compute 1/sum_j for every i.\n  auto ops =\n      (EIGEN_COST(scalar_exp_op<float>) + EIGEN_COST(scalar_sum_op<float>) +\n       EIGEN_COST(scalar_product_op<float>)) *\n          logits_size +\n      EIGEN_COST(scalar_inverse_op<float>) * logits_shape.dim(0).size();\n\n#undef EIGEN_COST\n  return PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictResizeBilinear(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n\n  if (op_context.op_info.outputs().empty() ||\n      op_context.op_info.inputs().empty()) {\n    return errors::InvalidArgument(\n        \"ResizeBilinear op has invalid input / output \",\n        op_context.op_info.ShortDebugString());\n  }\n\n  const int64_t output_elements = CalculateTensorElementCount(\n      op_context.op_info.outputs(0), &found_unknown_shapes);\n\n  const auto half_pixel_centers =\n      op_context.op_info.attr().find(\"half_pixel_centers\");\n  bool use_half_pixel_centers = false;\n  if (half_pixel_centers == op_context.op_info.attr().end()) {\n    LOG(WARNING) << \"half_pixel_centers attr not set for ResizeBilinear.\";\n    return PredictCostOfAnUnknownOp(op_context, node_costs);\n  } else {\n    use_half_pixel_centers = half_pixel_centers->second.b();\n  }\n\n  // Compose cost of bilinear interpolation.\n  int64_t ops = 0;\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n  const auto sub_cost_float = EIGEN_COST(scalar_difference_op<float>);\n  const auto sub_cost_int = EIGEN_COST(scalar_difference_op<int64_t>);\n  const auto add_cost = EIGEN_COST(scalar_sum_op<float>);\n  const auto mul_cost = EIGEN_COST(scalar_product_op<float>);\n  const auto floor_cost = EIGEN_COST(scalar_floor_op<float>);\n  const auto max_cost = EIGEN_COST(scalar_max_op<int64_t>);\n  const auto min_cost = EIGEN_COST(scalar_min_op<int64_t>);\n  const auto cast_to_int_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_cast_op<float, int64_t>>::Cost;\n  const auto cast_to_float_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_cast_op<int64_t, float>>::Cost;\n  const auto ceil_cost = EIGEN_COST(scalar_ceil_op<float>);\n#undef EIGEN_COST\n\n  // Ops calculated from tensorflow/core/kernels/image/resize_bilinear_op.cc.\n\n  // Op counts taken from resize_bilinear implementation on 07/21/2020.\n  // Computed op counts may become inaccurate if resize_bilinear implementation\n  // changes.\n\n  // resize_bilinear has an optimization where the interpolation weights are\n  // precomputed and cached. Given input tensors of size [B,H1,W1,C] and output\n  // tensors of size [B,H2,W2,C], the last dimension C that needs to be accessed\n  // in the input for interpolation are identical at every point in the output.\n  // These values are cached in the compute_interpolation_weights function. For\n  // a particular y in [0...H2-1], the rows to be accessed in the input are the\n  // same. Likewise, for a particular x in [0...H2-1], the columns to be accsed\n  // are the same. So the precomputation only needs to be done for H2 + W2\n  // values.\n  const auto output_shape = MaybeGetMinimumShape(\n      op_context.op_info.outputs(0).shape(), 4, &found_unknown_shapes);\n  // Assume H is dim 1 and W is dim 2 to match logic in resize_bilinear, which\n  // also makes this assumption.\n  const int64_t output_height = output_shape.dim(1).size();\n  const int64_t output_width = output_shape.dim(2).size();\n  // Add the ops done outside of the scaler function in\n  // compute_interpolation_weights.\n  int64_t interp_weight_cost = floor_cost + max_cost + min_cost +\n                               sub_cost_float + sub_cost_int + ceil_cost +\n                               cast_to_int_cost * 2;\n  // There are two options for computing the weight of each pixel in the\n  // interpolation. Algorithm can use pixel centers, or corners, for the\n  // weight. Ops depend on the scaler function passed into\n  // compute_interpolation_weights.\n  if (use_half_pixel_centers) {\n    // Ops for HalfPixelScalaer.\n    interp_weight_cost +=\n        add_cost + mul_cost + sub_cost_float + cast_to_float_cost;\n  } else {\n    // Ops for LegacyScaler.\n    interp_weight_cost += cast_to_float_cost + mul_cost;\n  }\n  // Cost for the interpolation is multiplied by (H2 + w2), as mentioned above.\n  ops += interp_weight_cost * (output_height + output_width);\n\n  // Ops for computing the new values, done for every element. Logic is from\n  // compute_lerp in the inner loop of resize_image which consists of:\n  //   const float top = top_left + (top_right - top_left) * x_lerp;\n  //   const float bottom = bottom_left + (bottom_right - bottom_left) * x_lerp;\n  //   return top + (bottom - top) * y_lerp;\n  ops += (add_cost * 3 + sub_cost_float * 3 + mul_cost * 3) * output_elements;\n\n  return PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictCropAndResize(const OpContext& op_context,\n                                                  NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n\n  const auto method = op_context.op_info.attr().find(\"method\");\n  bool use_bilinear_interp;\n  if (method == op_context.op_info.attr().end() ||\n      method->second.s() == \"bilinear\") {\n    use_bilinear_interp = true;\n  } else if (method->second.s() == \"nearest\") {\n    use_bilinear_interp = false;\n  } else {\n    LOG(WARNING) << \"method attr in CropAndResize invalid; expected bilinear \"\n                    \"or nearest.\";\n    return PredictCostOfAnUnknownOp(op_context, node_costs);\n  }\n\n  const int64_t num_boxes = op_context.op_info.inputs(1).shape().dim(0).size();\n  const auto crop_shape = MaybeGetMinimumShape(\n      op_context.op_info.outputs(0).shape(), 4, &found_unknown_shapes);\n  const int64_t crop_height = crop_shape.dim(1).size();\n  const int64_t crop_width = crop_shape.dim(2).size();\n  const int64_t output_elements = CalculateTensorElementCount(\n      op_context.op_info.outputs(0), &found_unknown_shapes);\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n  const auto sub_cost = EIGEN_COST(scalar_difference_op<float>);\n  const auto add_cost = EIGEN_COST(scalar_sum_op<float>);\n  const auto mul_cost = EIGEN_COST(scalar_product_op<float>);\n  auto div_cost = EIGEN_COST(scalar_div_cost<float>);\n  const auto floor_cost = EIGEN_COST(scalar_floor_op<float>);\n  const auto ceil_cost = EIGEN_COST(scalar_ceil_op<float>);\n  auto round_cost = EIGEN_COST(scalar_round_op<float>);\n  const auto cast_to_float_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_cast_op<int64_t, float>>::Cost;\n#undef EIGEN_COST\n\n  // Computing ops following\n  // tensorflow/core/kernels/image/crop_and_resize_op.cc at 08/25/2020. Op\n  // calculation differs from rough estimate in implementation, as it separates\n  // out cost per box from cost per pixel and cost per element.\n\n  // Since crop arguments are user controlled, check for overflow.\n  int64_t crop_area = MultiplyWithoutOverflow(crop_height, crop_width);\n  if (crop_area < 0)\n    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n                                   crop_height, \" with \", crop_width,\n                                   \" would overflow\");\n  int64_t crop_volume = MultiplyWithoutOverflow(crop_area, num_boxes);\n  if (crop_volume < 0)\n    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n                                   crop_area, \" with \", num_boxes,\n                                   \" would overflow\");\n  int64_t crop_depth = MultiplyWithoutOverflow(crop_height, num_boxes);\n  if (crop_depth < 0)\n    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n                                   crop_height, \" with \", num_boxes,\n                                   \" would overflow\");\n\n  // Ops for variables height_scale and width_scale.\n  int64_t ops = (sub_cost * 6 + mul_cost * 2 + div_cost * 2) * num_boxes;\n  // Ops for variable in_y.\n  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_depth;\n  // Ops for variable in_x (same computation across both branches).\n  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_volume;\n  // Specify op_cost based on the method.\n  if (use_bilinear_interp) {\n    // Ops for variables top_y_index, bottom_y_index, y_lerp.\n    ops += (floor_cost + ceil_cost + sub_cost) * crop_depth;\n    // Ops for variables left_x, right_x, x_lerp;\n    ops += (floor_cost + ceil_cost + sub_cost) * crop_volume;\n    // Ops for innermost loop across depth.\n    ops +=\n        (cast_to_float_cost * 4 + add_cost * 3 + sub_cost * 3 + mul_cost * 3) *\n        output_elements;\n  } else /* method == \"nearest\" */ {\n    // Ops for variables closest_x_index and closest_y_index.\n    ops += round_cost * 2 * crop_volume;\n    // Ops for innermost loop across depth.\n    ops += cast_to_float_cost * output_elements;\n  }\n  return PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\n}  // end namespace grappler\n}  // end namespace tensorflow\n", "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#ifndef TENSORFLOW_CORE_GRAPPLER_COSTS_OP_LEVEL_COST_ESTIMATOR_H_\n#define TENSORFLOW_CORE_GRAPPLER_COSTS_OP_LEVEL_COST_ESTIMATOR_H_\n\n#include <numeric>\n\n#include \"tensorflow/core/grappler/costs/cost_estimator.h\"\n#include \"tensorflow/core/grappler/costs/op_context.h\"\n#include \"tensorflow/core/grappler/costs/op_performance_data.pb.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/util/padding.h\"\n\nnamespace tensorflow {\nnamespace grappler {\n\nbool GetTensorShapeProtoFromTensorProto(const TensorProto& tensor_proto,\n                                        TensorShapeProto* tensor_shape_proto);\nTensorShapeProto MaybeGetMinimumShape(const TensorShapeProto& original_shape,\n                                      int rank, bool* found_unknown_shapes);\n\n// Node costs; an intermediate structure used within op level cost estimator.\nstruct NodeCosts {\n  // If this FLAG is true, override calculated compute time with a minimum\n  // value, instead of calculating it from num_compute_ops and compute ops/sec.\n  // For example, PredictIdentity, PredictVariable, PredictMetadata set this\n  // FLAG.\n  bool minimum_cost_op = false;\n\n  // Compute ops.\n  int64_t num_compute_ops = 0;\n\n  // Memory bytes accessed; note that these may be different to the size of\n  // tensors.\n  std::vector<int64_t> num_input_bytes_accessed;   // ordered by input tensors.\n  std::vector<int64_t> num_output_bytes_accessed;  // ordered by output ports.\n  int64_t internal_read_bytes = 0;\n  int64_t internal_write_bytes = 0;\n\n  // Convenience functions.\n  int64_t num_total_input_bytes() const {\n    return std::accumulate(num_input_bytes_accessed.begin(),\n                           num_input_bytes_accessed.end(), 0LL);\n  }\n  int64_t num_total_read_bytes() const {\n    return num_total_input_bytes() + internal_read_bytes;\n  }\n  int64_t num_total_output_bytes() const {\n    return std::accumulate(num_output_bytes_accessed.begin(),\n                           num_output_bytes_accessed.end(), 0LL);\n  }\n  int64_t num_total_write_bytes() const {\n    return num_total_output_bytes() + internal_write_bytes;\n  }\n  int64_t num_bytes_accessed() const {\n    return num_total_read_bytes() + num_total_write_bytes();\n  }\n\n  // Memory usage.\n  int64_t max_memory = 0;\n  int64_t persistent_memory = 0;\n  int64_t temporary_memory = 0;\n\n  // Stats.\n  int64_t num_nodes = 1;\n  int64_t num_nodes_with_unknown_shapes = 0;\n  int64_t num_nodes_with_unknown_op_type = 0;\n  int64_t num_nodes_with_pure_memory_op = 0;\n  bool inaccurate = false;\n\n  // TODO(dyoon): this is added for compatibility; some old code is hard to\n  // migrate; hence, using these as a backup. Once we clean up, we'll delete\n  // these fields. New code should not use these.\n  bool has_costs = false;\n  Costs costs;\n};\n\nclass OpLevelCostEstimator {\n public:\n  OpLevelCostEstimator();\n  virtual ~OpLevelCostEstimator() {}\n\n  virtual Costs PredictCosts(const OpContext& op_context) const;\n\n  // Returns basic device performance info.\n  virtual DeviceInfo GetDeviceInfo(const DeviceProperties& device) const;\n\n protected:\n  // TODO(dyoon): Consider to remove PredictOpCountBasedCosts() with OpInfo.\n  // Naive cost estimate based on the given operations count and total\n  // input/output tensor sizes of the given op_info combined.\n  Costs PredictOpCountBasedCost(double operations, const OpInfo& op_info) const;\n\n  // Naive cost estimate based on the given operations count and the given total\n  // io size in bytes. Sizes of op_info inputs and outputs are not taken into\n  // consideration.\n  Costs PredictOpCountBasedCost(double operations, double input_io_bytes,\n                                double output_io_bytes,\n                                const OpInfo& op_info) const;\n\n  // Top-level method cost function (PredictCosts calls this method to get\n  // NodeCosts, and then converts it to Costs). PredictNodeCosts() calls other\n  // Predict methods depending on op types.\n  Status PredictNodeCosts(const OpContext& op_context,\n                          NodeCosts* node_costs) const;\n\n  // Predict cost of an op for which no accurate estimator is defined.\n  Status PredictCostOfAnUnknownOp(const OpContext& op_context,\n                                  NodeCosts* node_costs) const;\n\n  // This family of routines predicts the costs to\n  // perform the specified TensorFlow Op on the\n  // device represented by a subclass. The default\n  // implementation just divides the operations to\n  // perform the op (from the \"Count\" routines,\n  // above) by the device peak operations per\n  // second.\n  // Implementation of costs other than\n  // execution_time is optional, depending on the\n  // device.\n  Status PredictNaryOp(const OpContext& op_context,\n                       NodeCosts* node_costs) const;\n  Status PredictConv2D(const OpContext& op_context,\n                       NodeCosts* node_costs) const;\n  Status PredictCwiseOp(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictConv2DBackpropInput(const OpContext& op_context,\n                                    NodeCosts* node_costs) const;\n  Status PredictConv2DBackpropFilter(const OpContext& op_context,\n                                     NodeCosts* node_costs) const;\n  Status PredictFusedConv2DBiasActivation(const OpContext& op_context,\n                                          NodeCosts* node_costs) const;\n  Status PredictMatMul(const OpContext& op_context,\n                       NodeCosts* node_costs) const;\n  Status PredictSparseTensorDenseMatMul(const OpContext& op_context,\n                                        NodeCosts* node_costs) const;\n  Status PredictNoOp(const OpContext& op_context, NodeCosts* node_costs) const;\n  Status PredictIdentity(const OpContext& op_context,\n                         NodeCosts* node_costs) const;\n  Status PredictVariable(const OpContext& op_context,\n                         NodeCosts* node_costs) const;\n  Status PredictBatchMatMul(const OpContext& op_context,\n                            NodeCosts* node_costs) const;\n  Status PredictMetadata(const OpContext& op_context,\n                         NodeCosts* node_costs) const;\n  Status PredictGatherOrSlice(const OpContext& op_context,\n                              NodeCosts* node_costs) const;\n  Status PredictScatter(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictMaxPool(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictMaxPoolGrad(const OpContext& op_context,\n                            NodeCosts* node_costs) const;\n  Status PredictAvgPool(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictAvgPoolGrad(const OpContext& op_context,\n                            NodeCosts* node_costs) const;\n  Status PredictFusedBatchNorm(const OpContext& op_context,\n                               NodeCosts* node_costs) const;\n  Status PredictFusedBatchNormGrad(const OpContext& op_context,\n                                   NodeCosts* node_costs) const;\n  Status PredictEinsum(const OpContext& op_context,\n                       NodeCosts* node_costs) const;\n  Status PredictAssignVariableOps(const OpContext& op_context,\n                                  NodeCosts* node_costs) const;\n  Status PredictPureMemoryOp(const OpContext& op_context,\n                             NodeCosts* node_costs) const;\n  Status PredictSoftmax(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictResizeBilinear(const OpContext& op_context,\n                               NodeCosts* node_costs) const;\n  Status PredictCropAndResize(const OpContext& op_context,\n                              NodeCosts* node_costs) const;\n\n  // Generic cost prediction method for fused operations.\n  Status PredictFusedOp(const OpContext& op_context,\n                        const std::vector<OpContext>& fused_op_contexts,\n                        NodeCosts* node_costs) const;\n\n  // Utility function for safe division. Returns 0\n  // if rhs is 0 or negative.\n  static double SafeDiv(const double lhs, const double rhs) {\n    if (rhs > 0) {\n      return lhs / rhs;\n    } else {\n      return 0.0;\n    }\n  }\n\n  // This family of routines counts the number of operations to perform the\n  // specified TensorFlow Op.\n  struct MatMulDimensions {\n    int m;\n    int n;\n    int k;\n  };\n  struct BatchMatMulDimensions {\n    std::vector<int> batch_dims;\n    MatMulDimensions matmul_dims;\n  };\n  struct ConvolutionDimensions {\n    int64_t batch;  // Batch size.\n    int64_t ix;     // Input size x.\n    int64_t iy;     // Input size y.\n    int64_t iz;     // Input depth.\n    int64_t kx;     // Kernel x.\n    int64_t ky;     // Kernel y.\n    int64_t kz;     // Kernel depth (in case of group convolution, this will be\n                    // smaller than input depth).\n    int64_t oz;     // Output depth.\n    int64_t ox;     // Output size x.\n    int64_t oy;     // Output size y.\n    int64_t sx;     // Stride x.\n    int64_t sy;     // Stride y.\n    Padding padding;  // SAME or VALID.\n  };\n  static int64_t CountConv2DOperations(const OpInfo& op_info,\n                                       bool* found_unknown_shapes);\n  static int64_t CountConv2DOperations(const OpInfo& op_info,\n                                       ConvolutionDimensions* conv_info,\n                                       bool* found_unknown_shapes);\n  static int64_t CountMatMulOperations(const OpInfo& op_info,\n                                       bool* found_unknown_shapes);\n  static int64_t CountMatMulOperations(const OpInfo& op_info,\n                                       MatMulDimensions* mat_mul,\n                                       bool* found_unknown_shapes);\n  bool GenerateBatchMatmulContextFromEinsum(const OpContext& einsum_context,\n                                            OpContext* batch_matmul_context,\n                                            bool* found_unknown_shapes) const;\n  static int64_t CountBatchMatMulOperations(const OpInfo& op_info,\n                                            bool* found_unknown_shapes);\n  static int64_t CountBatchMatMulOperations(\n      const OpInfo& op_info, BatchMatMulDimensions* batch_mat_mul,\n      bool* found_unknown_shapes);\n  static int64_t CountConv2DBackpropInputOperations(\n      const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n      bool* found_unknown_shapes);\n  static int64_t CountConv2DBackpropFilterOperations(\n      const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n      bool* found_unknown_shapes);\n\n  // Calculate the element count of an input/output tensor.\n  static int64_t CalculateTensorElementCount(\n      const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes);\n\n  // Calculate the total size in bytes of an input/output tensor.\n  static int64_t CalculateTensorSize(const OpInfo::TensorProperties& tensor,\n                                     bool* found_unknown_shapes);\n\n  // Calculate the element count of the largest\n  // input of specified TensorFlow op.\n  static int64_t CalculateLargestInputCount(const OpInfo& op_info,\n                                            bool* found_unknown_shapes);\n\n  // Calculate the total size in bytes of the all\n  // the inputs of specified TensorFlow op.\n  static int64_t CalculateInputSize(const OpInfo& op_info,\n                                    bool* found_unknown_shapes);\n\n  // Same, but a vector format: one for each input.\n  static std::vector<int64_t> CalculateInputTensorSize(\n      const OpInfo& op_info, bool* found_unknown_shapes);\n\n  // Calculate the total size in bytes of the all\n  // the outputs of specified TensorFlow op.\n  static int64_t CalculateOutputSize(const OpInfo& op_info,\n                                     bool* found_unknown_shapes);\n\n  // Same, but a vector format: one for each output.\n  static std::vector<int64_t> CalculateOutputTensorSize(\n      const OpInfo& op_info, bool* found_unknown_shapes);\n\n  // For convolution and its grad ops.\n  static ConvolutionDimensions ConvolutionDimensionsFromInputs(\n      const TensorShapeProto& original_image_shape,\n      const TensorShapeProto& original_filter_shape, const OpInfo& op_info,\n      bool* found_unknown_shapes);\n\n  // For Pooling, FusedBatchNorm, and their grad ops.\n  static ConvolutionDimensions OpDimensionsFromInputs(\n      const TensorShapeProto& original_image_shape, const OpInfo& op_info,\n      bool* found_unknown_shapes);\n\n  // Helper to construct child operation contexts for the component operations\n  // of fused ops.\n  static OpContext FusedChildContext(\n      const OpContext& parent, const string& op_name,\n      const OpInfo::TensorProperties& output,\n      const std::vector<OpInfo::TensorProperties>& inputs);\n\n  // Helper to construct tensor shapes.\n  static OpInfo::TensorProperties DescribeTensor(\n      DataType type, const std::vector<int64_t>& dims);\n\n  // Helper method for building common case NodeCosts.\n  static Status PredictDefaultNodeCosts(const int64_t num_compute_ops,\n                                        const OpContext& op_context,\n                                        bool* found_unknown_shapes,\n                                        NodeCosts* node_costs);\n\n protected:\n  std::map<string, int> elementwise_ops_;\n  typedef std::function<Status(const OpContext& op_context, NodeCosts*)>\n      CostImpl;\n  std::map<string, CostImpl> device_cost_impl_;\n  // If true, assume compute and memory overlap; hence, the op cost is max of\n  // compute_time and memory_time, instead of sum of those two.\n  bool compute_memory_overlap_;\n  std::set<string> persistent_ops_;\n\n private:\n  friend class OpLevelCostEstimatorTest;\n};\n\n}  // end namespace grappler\n}  // end namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_GRAPPLER_COSTS_OP_LEVEL_COST_ESTIMATOR_H_\n", "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/grappler/costs/op_level_cost_estimator.h\"\n\n#include <unordered_set>\n\n#include \"tensorflow/core/framework/attr_value.pb.h\"\n#include \"tensorflow/core/framework/attr_value_util.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor.pb.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/platform/test.h\"\n#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n\nnamespace tensorflow {\nnamespace grappler {\n\nnamespace {\n\n// TODO(dyoon): Consider to use this Test class for all the test cases, and then\n// remove friend in the OpLevelCostEstimator class header.\nclass TestOpLevelCostEstimator : public OpLevelCostEstimator {\n public:\n  TestOpLevelCostEstimator() {\n    compute_memory_overlap_ = true;\n    device_info_ = DeviceInfo();\n  }\n  ~TestOpLevelCostEstimator() override {}\n\n  void SetDeviceInfo(const DeviceInfo& device_info) {\n    device_info_ = device_info;\n  }\n\n  void SetComputeMemoryOverlap(bool value) { compute_memory_overlap_ = value; }\n\n protected:\n  DeviceInfo GetDeviceInfo(const DeviceProperties& device) const override {\n    return device_info_;\n  }\n\n  DeviceInfo device_info_;\n};\n\nvoid ExpectZeroCost(const Costs& cost) {\n  EXPECT_TRUE(cost.inaccurate);\n  EXPECT_EQ(cost.compute_time, Costs::Duration::zero());\n  EXPECT_EQ(cost.execution_time, Costs::Duration::zero());\n  EXPECT_EQ(cost.memory_time, Costs::Duration::zero());\n}\n\n// Wrangles the minimum number of proto fields to set up a matrix.\nvoid DescribeMatrix(int rows, int columns, OpInfo* op_info) {\n  auto input = op_info->add_inputs();\n  auto shape = input->mutable_shape();\n  auto shape_rows = shape->add_dim();\n  shape_rows->set_size(rows);\n  auto shape_columns = shape->add_dim();\n  shape_columns->set_size(columns);\n  input->set_dtype(DT_FLOAT);\n}\n\nvoid SetCpuDevice(OpInfo* op_info) {\n  auto device = op_info->mutable_device();\n  device->set_type(\"CPU\");\n  device->set_num_cores(10);\n  device->set_bandwidth(10000000);  // 10000000 KB/s = 10 GB/s\n  device->set_frequency(1000);      // 1000 Mhz = 1 GHz\n}\n\n// Returns an OpInfo for MatMul with the minimum set of fields set up.\nOpContext DescribeMatMul(int m, int n, int l, int k) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"MatMul\");\n\n  DescribeMatrix(m, l, &op_context.op_info);\n  DescribeMatrix(k, n, &op_context.op_info);\n  return op_context;\n}\n\n// Wrangles the minimum number of proto fields to set up an input of\n// arbitrary rank and type.\nvoid DescribeArbitraryRankInput(const std::vector<int>& dims, DataType dtype,\n                                OpInfo* op_info) {\n  auto input = op_info->add_inputs();\n  input->set_dtype(dtype);\n  auto shape = input->mutable_shape();\n  for (auto d : dims) {\n    shape->add_dim()->set_size(d);\n  }\n}\n\n// Wrangles the minimum number of proto fields to set up an output of\n// arbitrary rank and type.\nvoid DescribeArbitraryRankOutput(const std::vector<int>& dims, DataType dtype,\n                                 OpInfo* op_info) {\n  auto output = op_info->add_outputs();\n  output->set_dtype(dtype);\n  auto shape = output->mutable_shape();\n  for (auto d : dims) {\n    shape->add_dim()->set_size(d);\n  }\n}\n\n// Returns an OpInfo for a SparseTensorDenseMatMul\nOpContext DescribeSparseTensorDenseMatMul(const int nnz_a,\n                                          const std::vector<int>& dims_b,\n                                          const std::vector<int>& dims_out) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"SparseTensorDenseMatMul\");\n\n  DescribeArbitraryRankInput({nnz_a, 2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput({nnz_a}, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput(dims_b, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankOutput(dims_out, DT_FLOAT, &op_context.op_info);\n  return op_context;\n}\n\n// Returns an OpInfo for an XlaEinsum\nOpContext DescribeXlaEinsum(const std::vector<int>& dims_a,\n                            const std::vector<int>& dims_b,\n                            const string& equation) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"XlaEinsum\");\n  AttrValue equation_attribute;\n  equation_attribute.set_s(equation);\n  (*op_context.op_info.mutable_attr())[\"equation\"] = equation_attribute;\n  if (!dims_a.empty())\n    DescribeArbitraryRankInput(dims_a, DT_FLOAT, &op_context.op_info);\n  if (!dims_b.empty())\n    DescribeArbitraryRankInput(dims_b, DT_FLOAT, &op_context.op_info);\n  return op_context;\n}\n\n// Returns an OpInfo for an Einsum\nOpContext DescribeEinsum(const std::vector<int>& dims_a,\n                         const std::vector<int>& dims_b,\n                         const string& equation) {\n  OpContext op_context = DescribeXlaEinsum(dims_a, dims_b, equation);\n  op_context.op_info.set_op(\"Einsum\");\n  return op_context;\n}\n\nvoid DescribeDummyTensor(OpInfo::TensorProperties* tensor) {\n  // Intentionally leave the tensor shape and type information missing.\n}\n\n// Wrangles the minimum number of proto fields to set up a 1D Tensor for cost\n// estimation purposes.\nvoid DescribeTensor1D(int dim0, OpInfo::TensorProperties* tensor) {\n  auto shape = tensor->mutable_shape();\n  shape->add_dim()->set_size(dim0);\n  tensor->set_dtype(DT_FLOAT);\n}\n\n// Wrangles the minimum number of proto fields to set up a 4D Tensor for cost\n// estimation purposes.\nvoid DescribeTensor4D(int dim0, int dim1, int dim2, int dim3,\n                      OpInfo::TensorProperties* tensor) {\n  auto shape = tensor->mutable_shape();\n  shape->add_dim()->set_size(dim0);\n  shape->add_dim()->set_size(dim1);\n  shape->add_dim()->set_size(dim2);\n  shape->add_dim()->set_size(dim3);\n  tensor->set_dtype(DT_FLOAT);\n}\n\n// Wrangles the minimum number of proto fields to set up a 4D Tensor for cost\n// estimation purposes.\nvoid DescribeTensor5D(int dim0, int dim1, int dim2, int dim3, int dim4,\n                      OpInfo::TensorProperties* tensor) {\n  auto shape = tensor->mutable_shape();\n  shape->add_dim()->set_size(dim0);\n  shape->add_dim()->set_size(dim1);\n  shape->add_dim()->set_size(dim2);\n  shape->add_dim()->set_size(dim3);\n  shape->add_dim()->set_size(dim4);\n  tensor->set_dtype(DT_FLOAT);\n}\n\n// DescribeConvolution constructs an OpContext for a Conv2D applied to an input\n// tensor with shape (batch, ix, iy, iz1) and a kernel tensor with shape\n// (kx, ky, iz2, oz).\nOpContext DescribeConvolution(int batch, int ix, int iy, int iz1, int iz2,\n                              int kx, int ky, int oz) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"Conv2D\");\n\n  DescribeTensor4D(batch, ix, iy, iz1, op_context.op_info.add_inputs());\n  DescribeTensor4D(kx, ky, iz2, oz, op_context.op_info.add_inputs());\n\n  return op_context;\n}\n\n// Describe DepthwiseConvolution constructs an OpContext for a\n// DepthwiseConv2dNative applied to an input\n// tensor with shape (batch, ix, iy, iz1) and a kernel tensor with shape\n// (kx, ky, iz2, cm). cm is channel multiplier\n\nOpContext DescribeDepthwiseConv2dNative(int batch, int ix, int iy, int iz1,\n                                        int iz2, int kx, int ky, int cm) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"DepthwiseConv2dNative\");\n\n  DescribeTensor4D(batch, ix, iy, iz1, op_context.op_info.add_inputs());\n  DescribeTensor4D(kx, ky, iz2, cm, op_context.op_info.add_inputs());\n\n  return op_context;\n}\n\n// DescribeFusedConv2DBiasActivation constructs an OpContext for a\n// FusedConv2DBiasActivation applied to a convolution input tensor with shape\n// (batch, ix, iy, iz1), a kernel tensor with shape (kx, ky, iz2, oz), a\n// bias tensor with shape (oz), a side input tensor with shape\n// (batch, ox, oy, oz) if has_side_input is set, and two scaling tensors with\n// shape (1). If a vectorized channel format is chosen (NCHW_VECT_C, e.g.) we'll\n// default to 4 (the vector size most often used with this format on NVIDIA\n// platforms) for the major channel size, and divide the input channel size by\n// that amount.\n//\n// Note that this assumes the NHWC data format.\nOpContext DescribeFusedConv2DBiasActivation(int batch, int ix, int iy, int iz1,\n                                            int iz2, int kx, int ky, int ox,\n                                            int oy, int oz, bool has_side_input,\n                                            const string& data_format,\n                                            const string& filter_format) {\n  const int kVecWidth = 4;\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"FusedConv2DBiasActivation\");\n  auto* attr_data_format = op_context.op_info.mutable_attr();\n  SetAttrValue(data_format, &(*attr_data_format)[\"data_format\"]);\n  auto* attr_filter_format = op_context.op_info.mutable_attr();\n  SetAttrValue(filter_format, &(*attr_filter_format)[\"filter_format\"]);\n  if (data_format == \"NHWC\") {\n    DescribeTensor4D(batch, ix, iy, iz1, op_context.op_info.add_inputs());\n  } else if (data_format == \"NCHW\") {\n    DescribeTensor4D(batch, iz1, ix, iy, op_context.op_info.add_inputs());\n  } else {\n    // Use the NCHW_VECT_C format.\n    EXPECT_EQ(data_format, \"NCHW_VECT_C\");\n    EXPECT_EQ(iz1 % kVecWidth, 0);\n    DescribeTensor5D(batch, iz1 / kVecWidth, ix, iy, kVecWidth,\n                     op_context.op_info.add_inputs());\n  }\n  if (filter_format == \"HWIO\") {\n    DescribeTensor4D(kx, ky, iz2, oz, op_context.op_info.add_inputs());\n  } else if (filter_format == \"OIHW\") {\n    DescribeTensor4D(oz, iz2, kx, ky, op_context.op_info.add_inputs());\n  } else {\n    EXPECT_EQ(filter_format, \"OIHW_VECT_I\");\n    EXPECT_EQ(iz2 % kVecWidth, 0);\n    // Use the OIHW_VECT_I format.\n    DescribeTensor5D(oz, iz2 / kVecWidth, kx, ky, kVecWidth,\n                     op_context.op_info.add_inputs());\n  }\n  DescribeTensor1D(oz, op_context.op_info.add_inputs());\n\n  // Add the side_input, if any.\n  auto side_input = op_context.op_info.add_inputs();\n  if (has_side_input) {\n    if (data_format == \"NHWC\") {\n      DescribeTensor4D(batch, ox, oy, oz, side_input);\n    } else if (data_format == \"NCHW\") {\n      DescribeTensor4D(batch, oz, ox, oy, side_input);\n    } else {\n      // Use the NCHW_VECT_C format.\n      EXPECT_EQ(data_format, \"NCHW_VECT_C\");\n      EXPECT_EQ(oz % kVecWidth, 0);\n      DescribeTensor5D(batch, oz / kVecWidth, ox, oy, kVecWidth, side_input);\n    }\n  }\n\n  // Add the scaling tensors.\n  DescribeTensor1D(1, op_context.op_info.add_inputs());\n  DescribeTensor1D(1, op_context.op_info.add_inputs());\n\n  return op_context;\n}\n\n// DescribeUnaryOp constructs an OpContext for the given operation applied to\n// a 4-tensor with shape (size1, 1, 1, 1).\nOpContext DescribeUnaryOp(const string& op, int size1) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(op);\n\n  DescribeTensor4D(size1, 1, 1, 1, op_context.op_info.add_inputs());\n  DescribeTensor4D(size1, 1, 1, 1, op_context.op_info.add_outputs());\n\n  return op_context;\n}\n\n// DescribeBinaryOp constructs an OpContext for the given operation applied to\n// a 4-tensor with dimensions (size1, 1, 1, 1) and a 4-tensor with dimensions\n// (2 * size1, size2, 1, 1).\n//\n// The choice of dimension here is arbitrary, and is used strictly to test the\n// cost model for applying elementwise operations to tensors with unequal\n// dimension values.\nOpContext DescribeBinaryOp(const string& op, int size1, int size2) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(op);\n\n  DescribeTensor4D(size1, 1, 1, 1, op_context.op_info.add_inputs());\n  DescribeTensor4D(2 * size1, size2, 1, 1, op_context.op_info.add_inputs());\n  DescribeTensor4D(2 * size1, size2, 1, 1, op_context.op_info.add_outputs());\n\n  return op_context;\n}\n\n// DescribeBiasAdd constructs an OpContext for a BiasAdd applied to a 4-tensor\n// with dimensions (1, 1, size2, size1) and a bias with dimension (size1),\n// according to the constraint that the bias must be 1D with size equal to that\n// of the last dimension of the input value.\nOpContext DescribeBiasAdd(int size1, int size2) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"BiasAdd\");\n\n  DescribeTensor4D(1, 1, size2, size1, op_context.op_info.add_inputs());\n  DescribeTensor1D(size1, op_context.op_info.add_inputs());\n  DescribeTensor4D(1, 1, size2, size1, op_context.op_info.add_outputs());\n\n  return op_context;\n}\n\nint GetOutputSize(const int x, const int k, const int s,\n                  const string& padding) {\n  if (padding == \"SAME\") {\n    return (x + s - 1) / s;\n  } else {\n    return (x - k + s) / s;\n  }\n}\n\nstd::vector<int> GetPoolingOutputSize(const std::vector<int>& input,\n                                      const std::vector<int>& ksize,\n                                      const std::vector<int>& strides,\n                                      const string& data_format,\n                                      const string& padding) {\n  // h, w, and c indices: default with NHWC.\n  int h_index = 1;\n  int w_index = 2;\n  int c_index = 3;\n  if (data_format == \"NCHW\") {\n    h_index = 2;\n    w_index = 3;\n    c_index = 1;\n  }\n  // Extract parameters.\n  int n = input[0];\n  int h = input[h_index];\n  int w = input[w_index];\n  int c = input[c_index];\n  int sx = strides[h_index];\n  int sy = strides[w_index];\n  int kx = ksize[h_index];\n  int ky = ksize[w_index];\n\n  // Output activation size: default with VALID padding.\n  int ho = GetOutputSize(h, kx, sx, padding);\n  int wo = GetOutputSize(w, ky, sy, padding);\n\n  std::vector<int> output;\n  if (data_format == \"NHWC\") {\n    output = {n, ho, wo, c};\n  } else {\n    output = {n, c, ho, wo};\n  }\n  return output;\n}\n\n// Helper functions for testing GetTensorShapeProtoFromTensorProto().\nvoid GetTensorProto(const DataType dtype, const std::vector<int64_t>& shape,\n                    const std::vector<int64_t> values,\n                    const bool tensor_content, TensorProto* tensor_proto) {\n  tensor_proto->Clear();\n  TensorProto temp_tensor_proto;\n  temp_tensor_proto.set_dtype(dtype);\n  for (const auto& x : shape) {\n    temp_tensor_proto.mutable_tensor_shape()->add_dim()->set_size(x);\n  }\n  for (const auto& x : values) {\n    if (dtype == DT_INT64) {\n      temp_tensor_proto.add_int64_val(x);\n    } else if (dtype == DT_INT32 || dtype == DT_INT16 || dtype == DT_INT8 ||\n               dtype == DT_UINT8) {\n      temp_tensor_proto.add_int_val(x);\n    } else if (dtype == DT_UINT32) {\n      temp_tensor_proto.add_uint32_val(x);\n    } else if (dtype == DT_UINT64) {\n      temp_tensor_proto.add_uint64_val(x);\n    } else {\n      CHECK(false) << \"Unsupported dtype: \" << dtype;\n    }\n  }\n  Tensor tensor(dtype);\n  CHECK(tensor.FromProto(temp_tensor_proto));\n  if (tensor_content) {\n    tensor.AsProtoTensorContent(tensor_proto);\n  } else {\n    tensor.AsProtoField(tensor_proto);\n  }\n}\n\nOpContext DescribePoolingOp(const string& op_name, const std::vector<int>& x,\n                            const std::vector<int>& ksize,\n                            const std::vector<int>& strides,\n                            const string& data_format, const string& padding) {\n  OpContext op_context;\n  auto& op_info = op_context.op_info;\n  SetCpuDevice(&op_info);\n  op_info.set_op(op_name);\n\n  const std::vector<int> y =\n      GetPoolingOutputSize(x, ksize, strides, data_format, padding);\n  if (op_name == \"AvgPool\" || op_name == \"MaxPool\") {\n    // input: x, output: y.\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_inputs());\n    DescribeTensor4D(y[0], y[1], y[2], y[3], op_info.add_outputs());\n  } else if (op_name == \"AvgPoolGrad\") {\n    // input: x's shape, y_grad, output: x_grad.\n    DescribeArbitraryRankInput({4}, DT_INT32, &op_info);\n    auto* tensor_proto = op_info.mutable_inputs(0)->mutable_value();\n    GetTensorProto(DT_INT32, {4}, {x[0], x[1], x[2], x[3]},\n                   /*tensor_content=*/false, tensor_proto);\n    DescribeTensor4D(y[0], y[1], y[2], y[3], op_info.add_inputs());\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_outputs());\n  } else if (op_name == \"MaxPoolGrad\") {\n    // input: x, y, y_grad, output: x_grad.\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_inputs());\n    DescribeTensor4D(y[0], y[1], y[2], y[3], op_info.add_inputs());\n    DescribeTensor4D(y[0], y[1], y[2], y[3], op_info.add_inputs());\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_outputs());\n  }\n  auto* attr = op_info.mutable_attr();\n  SetAttrValue(data_format, &(*attr)[\"data_format\"]);\n  SetAttrValue(padding, &(*attr)[\"padding\"]);\n  SetAttrValue(strides, &(*attr)[\"strides\"]);\n  SetAttrValue(ksize, &(*attr)[\"ksize\"]);\n  return op_context;\n}\n\nOpContext DescribeFusedBatchNorm(const bool is_training, const bool is_grad,\n                                 const std::vector<int>& x,\n                                 const string& data_format) {\n  // First, get MaxPool op info with unit stride and unit window.\n  OpContext op_context = DescribePoolingOp(\"MaxPool\", x, {1, 1, 1, 1},\n                                           {1, 1, 1, 1}, data_format, \"SAME\");\n  auto& op_info = op_context.op_info;\n  // Override op name.\n  if (is_grad) {\n    op_info.set_op(\"FusedBatchNormGrad\");\n  } else {\n    op_info.set_op(\"FusedBatchNorm\");\n  }\n\n  // Add additional input output tensors.\n  if (is_grad) {\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_inputs());\n  }\n  int num_1d_inputs = is_grad ? 3 : 4;\n  for (int i = 0; i < num_1d_inputs; i++) {\n    auto* tensor = op_info.add_inputs();\n    auto* shape = tensor->mutable_shape();\n    shape->add_dim()->set_size(x[3]);\n    tensor->set_dtype(DT_FLOAT);\n  }\n  for (int i = 0; i < 4; i++) {\n    auto* tensor = op_info.add_outputs();\n    auto* shape = tensor->mutable_shape();\n    shape->add_dim()->set_size(x[3]);\n    tensor->set_dtype(DT_FLOAT);\n  }\n\n  // Delete unnecessary attr.\n  auto* attr = op_context.op_info.mutable_attr();\n  attr->erase(\"ksize\");\n  attr->erase(\"strides\");\n  attr->erase(\"padding\");\n\n  // Additional attrs for FusedBatchNorm.\n  SetAttrValue(is_training, &(*attr)[\"is_training\"]);\n\n  return op_context;\n}\n}  // namespace\n\nclass OpLevelCostEstimatorTest : public ::testing::Test {\n protected:\n  using BatchMatMulDimensions = OpLevelCostEstimator::BatchMatMulDimensions;\n\n  Costs PredictCosts(const OpContext& op_context) const {\n    return estimator_.PredictCosts(op_context);\n  }\n\n  int64_t CountMatMulOperations(const OpInfo& op_info,\n                                bool* found_unknown_shapes) const {\n    return estimator_.CountMatMulOperations(op_info, found_unknown_shapes);\n  }\n\n  int64_t CountBatchMatMulOperations(const OpInfo& op_info,\n                                     bool* found_unknown_shapes) const {\n    return estimator_.CountBatchMatMulOperations(op_info, found_unknown_shapes);\n  }\n\n  int64_t CountBatchMatMulOperations(const OpInfo& op_info,\n                                     BatchMatMulDimensions* batch_mat_mul,\n                                     bool* found_unknown_shapes) const {\n    return estimator_.CountBatchMatMulOperations(op_info, batch_mat_mul,\n                                                 found_unknown_shapes);\n  }\n\n  void SetComputeMemoryOverlap(bool value) {\n    estimator_.compute_memory_overlap_ = value;\n  }\n\n  void ValidateOpDimensionsFromInputs(const int n, const int h, const int w,\n                                      const int c, const int kx, const int ky,\n                                      const int sx, const int sy,\n                                      const string& data_format,\n                                      const string& padding) {\n    OpContext op_context;\n    int ho;\n    int wo;\n    if (data_format == \"NHWC\") {\n      op_context = DescribePoolingOp(\"MaxPool\", {n, h, w, c}, {1, kx, ky, 1},\n                                     {1, sx, sy, 1}, \"NHWC\", padding);\n      ho = op_context.op_info.outputs(0).shape().dim(1).size();\n      wo = op_context.op_info.outputs(0).shape().dim(2).size();\n    } else {\n      op_context = DescribePoolingOp(\"MaxPool\", {n, c, h, w}, {1, 1, kx, ky},\n                                     {1, 1, sx, sy}, \"NCHW\", padding);\n      ho = op_context.op_info.outputs(0).shape().dim(2).size();\n      wo = op_context.op_info.outputs(0).shape().dim(3).size();\n    }\n\n    bool found_unknown_shapes;\n    auto dims = OpLevelCostEstimator::OpDimensionsFromInputs(\n        op_context.op_info.inputs(0).shape(), op_context.op_info,\n        &found_unknown_shapes);\n    Padding padding_enum;\n    if (padding == \"VALID\") {\n      padding_enum = Padding::VALID;\n    } else {\n      padding_enum = Padding::SAME;\n    }\n    EXPECT_EQ(n, dims.batch);\n    EXPECT_EQ(h, dims.ix);\n    EXPECT_EQ(w, dims.iy);\n    EXPECT_EQ(c, dims.iz);\n    EXPECT_EQ(kx, dims.kx);\n    EXPECT_EQ(ky, dims.ky);\n    EXPECT_EQ(sx, dims.sx);\n    EXPECT_EQ(sy, dims.sy);\n    EXPECT_EQ(ho, dims.ox);\n    EXPECT_EQ(wo, dims.oy);\n    EXPECT_EQ(c, dims.oz);\n    EXPECT_EQ(padding_enum, dims.padding);\n  }\n\n  OpLevelCostEstimator estimator_;\n};\n\nclass OpLevelBatchMatMulCostEstimatorTest\n    : public OpLevelCostEstimatorTest,\n      public ::testing::WithParamInterface<const char*> {\n protected:\n  // Returns an OpInfo for a BatchMatMul\n  OpContext DescribeBatchMatMul(const std::vector<int>& dims_a,\n                                const std::vector<int>& dims_b) {\n    OpContext op_context;\n    SetCpuDevice(&op_context.op_info);\n    op_context.op_info.set_op(GetParam());\n\n    DescribeArbitraryRankInput(dims_a, DT_FLOAT, &op_context.op_info);\n    DescribeArbitraryRankInput(dims_b, DT_FLOAT, &op_context.op_info);\n    return op_context;\n  }\n\n  int64_t CountBatchMatMulOperations(const OpInfo& op_info,\n                                     bool* found_unknown_shapes) const {\n    return OpLevelCostEstimatorTest::CountBatchMatMulOperations(\n        op_info, found_unknown_shapes);\n  }\n\n  int64_t CountBatchMatMulDimProduct(const OpInfo& op_info,\n                                     bool* found_unknown_shapes) const {\n    BatchMatMulDimensions batch_mat_mul;\n\n    batch_mat_mul.matmul_dims.n = 0;\n    batch_mat_mul.matmul_dims.m = 0;\n    batch_mat_mul.matmul_dims.k = 0;\n\n    OpLevelCostEstimatorTest::CountBatchMatMulOperations(\n        op_info, &batch_mat_mul, found_unknown_shapes);\n    int dimension_product = 1;\n    for (auto dim : batch_mat_mul.batch_dims) dimension_product *= dim;\n\n    dimension_product *= batch_mat_mul.matmul_dims.n;\n    dimension_product *= batch_mat_mul.matmul_dims.m;\n    dimension_product *= batch_mat_mul.matmul_dims.k;\n\n    return dimension_product;\n  }\n};\n\nTEST_F(OpLevelCostEstimatorTest, TestPersistentOpCosts) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  std::unordered_set<string> persistent_ops = {\n      \"Const\",       \"Variable\",       \"VariableV2\", \"AutoReloadVariable\",\n      \"VarHandleOp\", \"ReadVariableOp\",\n  };\n  // Minimum cost for all persistent ops.\n  for (const auto& op : persistent_ops) {\n    op_context.op_info.set_op(op);\n    auto cost = estimator_.PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(0), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(1), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(1), cost.execution_time);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestGatherCosts) {\n  std::vector<std::string> gather_ops = {\"Gather\", \"GatherNd\", \"GatherV2\"};\n\n  for (const auto& op : gather_ops) {\n    OpContext op_context;\n    SetCpuDevice(&op_context.op_info);\n    op_context.op_info.set_op(op);\n\n    // Huge first input shouldn't affect Gather execution and memory costs.\n    DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n    DescribeArbitraryRankInput({16}, DT_INT64, &op_context.op_info);\n    DescribeArbitraryRankOutput({16, 10}, DT_FLOAT, &op_context.op_info);\n\n    auto cost = estimator_.PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(130), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(16), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(146), cost.execution_time);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestGatherCostsWithoutOutput) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"Gather\");\n\n  // Huge first input shouldn't affect Gather execution and memory costs.\n  DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankInput({16}, DT_INT64, &op_context.op_info);\n\n  auto cost = estimator_.PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(0), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(0), cost.execution_time);\n  EXPECT_EQ(1, cost.num_ops_total);\n  EXPECT_TRUE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestSliceCosts) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"Slice\");\n\n  // Huge first input shouldn't affect Slice execution and memory costs.\n  DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankOutput({10, 10}, DT_FLOAT, &op_context.op_info);\n\n  auto cost = estimator_.PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(81), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(10), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(91), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestStridedSliceCosts) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"StridedSlice\");\n\n  // Huge first input shouldn't affect StridedSlice execution and memory costs.\n  DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankOutput({10, 10}, DT_FLOAT, &op_context.op_info);\n\n  auto cost = estimator_.PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(81), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(10), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(91), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestScatterOps) {\n  std::vector<string> scatter_ops = {\"ScatterAdd\",   \"ScatterDiv\", \"ScatterMax\",\n                                     \"ScatterMin\",   \"ScatterMul\", \"ScatterSub\",\n                                     \"ScatterUpdate\"};\n  for (const auto& op : scatter_ops) {\n    // Test updates.shape = indices.shape + ref.shape[1:]\n    {\n      OpContext op_context;\n      SetCpuDevice(&op_context.op_info);\n      op_context.op_info.set_op(op);\n      // Huge first dimension in input shouldn't affect Scatter execution and\n      // memory costs.\n      DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n      DescribeArbitraryRankInput({16}, DT_INT64, &op_context.op_info);\n      DescribeArbitraryRankInput({16, 10}, DT_FLOAT, &op_context.op_info);\n      DescribeArbitraryRankOutput({10000000, 10}, DT_FLOAT,\n                                  &op_context.op_info);\n\n      auto cost = estimator_.PredictCosts(op_context);\n      EXPECT_EQ(Costs::Duration(205), cost.memory_time);\n      EXPECT_EQ(Costs::Duration(16), cost.compute_time);\n      EXPECT_EQ(Costs::Duration(221), cost.execution_time);\n      EXPECT_EQ(cost.num_ops_total, 1);\n      EXPECT_FALSE(cost.inaccurate);\n      EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n      EXPECT_EQ(cost.temporary_memory, 0);\n      EXPECT_EQ(cost.persistent_memory, 0);\n    }\n\n    // Test updates.shape = [] and INT32 indices\n    {\n      OpContext op_context;\n      SetCpuDevice(&op_context.op_info);\n      op_context.op_info.set_op(op);\n      // Huge first dimension in input shouldn't affect Scatter execution and\n      // memory costs.\n      DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n      DescribeArbitraryRankInput({16}, DT_INT32, &op_context.op_info);\n      DescribeArbitraryRankInput({}, DT_FLOAT, &op_context.op_info);\n      DescribeArbitraryRankOutput({10000000, 10}, DT_FLOAT,\n                                  &op_context.op_info);\n\n      auto cost = estimator_.PredictCosts(op_context);\n      EXPECT_EQ(Costs::Duration(135), cost.memory_time);\n      EXPECT_EQ(Costs::Duration(16), cost.compute_time);\n      EXPECT_EQ(Costs::Duration(151), cost.execution_time);\n      EXPECT_EQ(1, cost.num_ops_total);\n      EXPECT_FALSE(cost.inaccurate);\n      EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n    }\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, BiasAddExecutionTime) {\n  auto cost = PredictCosts(DescribeBiasAdd(1000, 10));\n  EXPECT_EQ(Costs::Duration(8400), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(1000), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(9400), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, Conv2DExecutionTime) {\n  auto cost = PredictCosts(DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(233780), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(354877440), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(355111220), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, InvalidConv2DConfig) {\n  // Convolution ops.\n  const std::vector<std::string> conv_ops = {\n      \"Conv2D\",\n      \"Conv2DBackpropFilter\",\n      \"Conv2DBackpropInput\",\n      \"DepthwiseConv2dNative\",\n      \"DepthwiseConv2dNativeBackpropFilter\",\n      \"DepthwiseConv2dNativeBackpropInput\",\n  };\n  // A valid Conv2D config.\n  const std::vector<int> valid_conv_config = {16, 19, 19, 48, 48, 5, 5, 256};\n  for (const auto& op : conv_ops) {\n    // Test with setting one value in conv config to zero.\n    // PredictCosts() should return zero costs.\n    for (int i = 0; i < valid_conv_config.size(); ++i) {\n      std::vector<int> conv_config(valid_conv_config);\n      conv_config[i] = 0;\n      auto op_context = DescribeConvolution(\n          conv_config[0], conv_config[1], conv_config[2], conv_config[3],\n          conv_config[4], conv_config[5], conv_config[6], conv_config[7]);\n      op_context.op_info.set_op(op);\n      auto cost = PredictCosts(op_context);\n      EXPECT_EQ(Costs::Duration(0), cost.memory_time);\n      EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n      EXPECT_EQ(Costs::Duration(0), cost.execution_time);\n      EXPECT_EQ(1, cost.num_ops_total);\n      EXPECT_TRUE(cost.inaccurate);\n      EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n    }\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, DepthwiseConv2dNativeExecutionTime) {\n  auto cost =\n      PredictCosts(DescribeDepthwiseConv2dNative(16, 19, 19, 48, 48, 5, 5, 3));\n  EXPECT_EQ(Costs::Duration(112340), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(4158720), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(4271060), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, DummyExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"Dummy\", 1000, 1));\n  EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(2000), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_TRUE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, ExecutionTimeSumOrMax) {\n  SetComputeMemoryOverlap(true);\n  auto cost = PredictCosts(DescribeBinaryOp(\"Dummy\", 1000, 1));\n  EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(2000), cost.execution_time);  // max(2000, 200)\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_TRUE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n  SetComputeMemoryOverlap(false);  // Set it back to default.\n}\n\nTEST_F(OpLevelCostEstimatorTest,\n       FusedConv2DBiasActivationNCHW_HWIO_NoSideInput) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ false,\n      \"NCHW\", \"HWIO\"));\n  EXPECT_EQ(Costs::Duration(825345), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355321037), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(356146382), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNCHW_HWIO) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW\", \"HWIO\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNCHW_OIHW) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW\", \"OIHW\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNHWC_HWIO) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NHWC\", \"HWIO\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNHWC_OIHW) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NHWC\", \"OIHW\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNCHW_VECT_C_OIHW) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW_VECT_C\", \"OIHW\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNCHW_OIHW_VECT_I) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW\", \"OIHW_VECT_I\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest,\n       FusedConv2DBiasActivationNCHW_VECT_C_OIHW_VECT_I) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW_VECT_C\", \"OIHW_VECT_I\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, MulExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"Mul\", 1000, 1));\n  EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(200), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(2200), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, MulBroadcastExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"Mul\", 1000, 2));\n  EXPECT_EQ(Costs::Duration(3600), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(400), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(4000), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, ModExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"Mod\", 1000, 1));\n  EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(1600), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(3600), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, SquaredDifferenceExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"SquaredDifference\", 1000, 2));\n  EXPECT_EQ(cost.memory_time, Costs::Duration(3600));\n  EXPECT_EQ(cost.compute_time, Costs::Duration(800));\n  EXPECT_EQ(cost.execution_time, Costs::Duration(4400));\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, UnaryOpExecutionTime) {\n  std::vector<std::pair<std::string, int>> unary_ops = {\n      {\"All\", 1},      {\"ArgMax\", 1}, {\"Cast\", 1},  {\"Max\", 1},\n      {\"Min\", 1},      {\"Prod\", 1},   {\"Relu\", 1},  {\"Relu6\", 1},\n      {\"Softmax\", 43}, {\"Sum\", 1},    {\"TopKV2\", 1}};\n\n  const int kTensorSize = 1000;\n  for (auto unary_op : unary_ops) {\n    OpContext op_context = DescribeUnaryOp(unary_op.first, kTensorSize);\n\n    const int kExpectedMemoryTime = 800;\n    int expected_compute_time = std::ceil(\n        unary_op.second * kTensorSize /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time))\n        << unary_op.first;\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(expected_compute_time + kExpectedMemoryTime));\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, BinaryOpExecutionTime) {\n  std::vector<std::pair<std::string, int>> binary_ops = {\n      {\"Select\", 1},\n      {\"SelectV2\", 1},\n      {\"SquaredDifference\", 2},\n      {\"Where\", 1},\n  };\n\n  const int kTensorSize1 = 1000;\n  const int kTensorSize2 = 2;\n  for (auto binary_op : binary_ops) {\n    OpContext op_context =\n        DescribeBinaryOp(binary_op.first, kTensorSize1, kTensorSize2);\n\n    const int kExpectedMemoryTime = 3600;\n    int expected_compute_time = std::ceil(\n        binary_op.second * kTensorSize1 * kTensorSize2 * 2 /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(kExpectedMemoryTime), cost.memory_time)\n        << binary_op.first;\n    EXPECT_EQ(Costs::Duration(expected_compute_time), cost.compute_time)\n        << binary_op.first;\n    EXPECT_EQ(Costs::Duration(expected_compute_time + kExpectedMemoryTime),\n              cost.execution_time)\n        << binary_op.first;\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, BroadcastAddExecutionTime) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"Add\");\n\n  DescribeTensor1D(100, op_context.op_info.add_inputs());\n  DescribeTensor4D(1, 10, 1, 1, op_context.op_info.add_inputs());\n\n  auto cost = PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(44), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(100), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(144), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, UnknownOrPartialShape) {\n  {\n    auto cost = PredictCosts(DescribeMatMul(2, 4, 7, 7));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeMatMul(-1, 4, 7, 7));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeMatMul(2, 4, -1, 7));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeConvolution(16, -1, 19, 48, 48, 5, 5, 256));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_P(OpLevelBatchMatMulCostEstimatorTest, TestBatchMatMul) {\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({}, {}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({2, 4}, {}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({2, 4}, {4, 2}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({1, 2, 4}, {1, 4, 2}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({2, 4}, {1, 3, 4, 2}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  bool matmul_inaccurate = false;\n  bool batch_matmul_inaccurate = false;\n  EXPECT_EQ(\n      CountMatMulOperations(DescribeMatMul(2, 2, 4, 4).op_info,\n                            &matmul_inaccurate),\n      CountBatchMatMulOperations(DescribeBatchMatMul({2, 4}, {4, 2}).op_info,\n                                 &batch_matmul_inaccurate));\n  EXPECT_EQ(matmul_inaccurate, batch_matmul_inaccurate);\n  EXPECT_EQ(10 * CountMatMulOperations(DescribeMatMul(2, 2, 4, 4).op_info,\n                                       &matmul_inaccurate),\n            CountBatchMatMulOperations(\n                DescribeBatchMatMul({10, 2, 4}, {-1, 10, 4, 2}).op_info,\n                &batch_matmul_inaccurate));\n  EXPECT_NE(matmul_inaccurate, batch_matmul_inaccurate);\n  EXPECT_EQ(20 * CountMatMulOperations(DescribeMatMul(2, 2, 4, 4).op_info,\n                                       &matmul_inaccurate),\n            CountBatchMatMulOperations(\n                DescribeBatchMatMul({2, 10, 2, 4}, {-1, 10, 4, 2}).op_info,\n                &batch_matmul_inaccurate));\n  EXPECT_NE(matmul_inaccurate, batch_matmul_inaccurate);\n\n  // Test the count to make sure that they extracted the dimensions correctly\n  int prod = CountBatchMatMulDimProduct(\n      DescribeBatchMatMul({2, 4}, {1, 3, 4, 2}).op_info,\n      &batch_matmul_inaccurate);\n  EXPECT_EQ(prod, 16);\n  EXPECT_FALSE(batch_matmul_inaccurate);\n\n  // Exercise the bad cases of a batchMatMul.\n  OpContext bad_batch = DescribeBatchMatMul({2, 4}, {4, 2});\n  bad_batch.op_info.set_op(\"notBatchMatMul\");\n  prod =\n      CountBatchMatMulDimProduct(bad_batch.op_info, &batch_matmul_inaccurate);\n\n  EXPECT_EQ(prod, 0);\n  EXPECT_TRUE(batch_matmul_inaccurate);\n\n  // Exercise a transpose case of a batchMatMul\n  OpContext transpose_batch = DescribeBatchMatMul({2, 4, 3, 1}, {4, 2});\n  auto attr = transpose_batch.op_info.mutable_attr();\n  (*attr)[\"adj_x\"].set_b(true);\n  (*attr)[\"adj_y\"].set_b(true);\n\n  prod = CountBatchMatMulDimProduct(transpose_batch.op_info,\n                                    &batch_matmul_inaccurate);\n  EXPECT_EQ(prod, 12);\n}\nINSTANTIATE_TEST_SUITE_P(TestBatchMatMul, OpLevelBatchMatMulCostEstimatorTest,\n                         ::testing::Values(\"BatchMatMul\", \"BatchMatMulV2\"));\n\nTEST_F(OpLevelCostEstimatorTest, SparseTensorDenseMatMul) {\n  // Unknown shape cases\n  {\n    auto cost =\n        PredictCosts(DescribeSparseTensorDenseMatMul(-1, {1, 1}, {1, 1}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeSparseTensorDenseMatMul(1, {-1, 1}, {1, 1}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeSparseTensorDenseMatMul(1, {1, -1}, {1, -1}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeSparseTensorDenseMatMul(1, {1, 1}, {-1, 1}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  // Known shape cases\n  {\n    auto cost = PredictCosts(\n        DescribeSparseTensorDenseMatMul(10, {1000, 100}, {50, 100}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n    EXPECT_EQ(Costs::Duration(200), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(2422), cost.memory_time);\n  }\n  {\n    // Same cost as above case because cost does not depend on k_dim\n    auto cost = PredictCosts(\n        DescribeSparseTensorDenseMatMul(10, {100000, 100}, {50, 100}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n    EXPECT_EQ(Costs::Duration(200), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(2422), cost.memory_time);\n  }\n}\n\nvoid ExpectTensorShape(const std::vector<int64_t>& expected,\n                       const TensorShapeProto& tensor_shape_proto) {\n  TensorShape tensor_shape_expected(expected);\n  TensorShape tensor_shape(tensor_shape_proto);\n\n  EXPECT_EQ(tensor_shape_expected, tensor_shape);\n}\n\nTEST_F(OpLevelCostEstimatorTest, GetTensorShapeProtoFromTensorProto) {\n  TensorProto tensor_proto;\n  TensorShapeProto tensor_shape_proto;\n\n  // Dimension larger than max value; should fail while converting to\n  // Tensor class.\n  tensor_proto.mutable_tensor_shape()->add_dim()->set_size(255);\n  EXPECT_FALSE(\n      GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n\n  tensor_proto.Clear();\n  // Expect only 1D shape.\n  tensor_proto.mutable_tensor_shape()->add_dim()->set_size(1);\n  tensor_proto.mutable_tensor_shape()->add_dim()->set_size(2);\n  EXPECT_FALSE(\n      GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n\n  // Expect only handle integer data types.\n  GetTensorProto(DT_FLOAT, {}, {}, /*tensor_content=*/false, &tensor_proto);\n  EXPECT_FALSE(\n      GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n\n  // Check GetTensorShapeProtoFromTensorProto() returns correct values.\n  {\n    std::vector<int64_t> shape_expected = {10, 20, 30, 40};\n    GetTensorProto(DT_INT32, {4}, shape_expected,\n                   /*tensor_content=*/false, &tensor_proto);\n    EXPECT_TRUE(\n        GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n    ExpectTensorShape(shape_expected, tensor_shape_proto);\n  }\n\n  {\n    std::vector<int64_t> shape_expected = {40, 20, 90, 40};\n    GetTensorProto(DT_INT64, {4}, shape_expected,\n                   /*tensor_content=*/false, &tensor_proto);\n    EXPECT_TRUE(\n        GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n    ExpectTensorShape(shape_expected, tensor_shape_proto);\n  }\n\n  {\n    std::vector<int64_t> shape_expected = {10, 20, 30, 40};\n    GetTensorProto(DT_INT32, {4}, shape_expected,\n                   /*tensor_content=*/true, &tensor_proto);\n    EXPECT_TRUE(\n        GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n    ExpectTensorShape(shape_expected, tensor_shape_proto);\n  }\n\n  {\n    std::vector<int64_t> shape_expected = {40, 20, 90, 40};\n    GetTensorProto(DT_INT64, {4}, shape_expected,\n                   /*tensor_content=*/true, &tensor_proto);\n    EXPECT_TRUE(\n        GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n    ExpectTensorShape(shape_expected, tensor_shape_proto);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, OpDimensionsFromInputs) {\n  std::vector<string> paddings = {\"VALID\", \"SAME\"};\n  std::vector<string> formats = {\"NHWC\", \"NCHW\"};\n  for (const auto& p : paddings) {\n    for (const auto& f : formats) {\n      // n, h, w, c, kx, ky, sx, sy, data_format, padding.\n      ValidateOpDimensionsFromInputs(10, 20, 20, 100, 3, 3, 2, 2, f, p);\n      ValidateOpDimensionsFromInputs(10, 20, 20, 100, 1, 1, 3, 3, f, p);\n      ValidateOpDimensionsFromInputs(10, 200, 200, 100, 5, 5, 3, 3, f, p);\n      ValidateOpDimensionsFromInputs(10, 14, 14, 3840, 3, 3, 2, 2, f, p);\n    }\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictMaxPool) {\n  auto predict_max_pool = [this](const int n, const int in, const int c,\n                                 const int k, const int s,\n                                 const string& padding) -> Costs {\n    OpContext op_context = DescribePoolingOp(\n        \"MaxPool\", {n, in, in, c}, {1, k, k, 1}, {1, s, s, 1}, \"NHWC\", padding);\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    // Typical 3xz3 window with 2x2 stride.\n    auto costs = predict_max_pool(10, 20, 384, 3, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1075200), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(307200), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(768000), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n  {\n    // 1x1 window with 2x2 stride: used for shortcut in resnet-50.\n    auto costs = predict_max_pool(10, 20, 384, 1, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(499200), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(38400), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(460800), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n  {\n    // 2x2 window with 3x3 stride.\n    auto costs = predict_max_pool(10, 20, 384, 2, 3, \"VALID\");\n    EXPECT_EQ(Costs::Duration(561792), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(56448), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(505344), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictMaxPoolGrad) {\n  auto predict_max_pool_grad = [this](const int n, const int in, const int c,\n                                      const int k, const int s,\n                                      const string& padding) -> Costs {\n    OpContext op_context =\n        DescribePoolingOp(\"MaxPoolGrad\", {n, in, in, c}, {1, k, k, 1},\n                          {1, s, s, 1}, \"NHWC\", padding);\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    // Typical 3x3 window with 2x2 stride.\n    auto costs = predict_max_pool_grad(10, 20, 384, 3, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1996800), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(614400), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(1382400), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n  {\n    // 1x1 window with 2x2 stride: used for shortcut in resnet-50.\n    auto costs = predict_max_pool_grad(10, 20, 384, 1, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1536000), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(153600), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(1382400), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n  {\n    // 2x2 window with 3x3 stride.\n    auto costs = predict_max_pool_grad(10, 20, 384, 2, 3, \"VALID\");\n    EXPECT_EQ(Costs::Duration(1514112), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(210048), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(1304064), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictAvgPool) {\n  auto predict_avg_pool = [this](const int n, const int in, const int c,\n                                 const int k, const int s,\n                                 const string& padding) -> Costs {\n    OpContext op_context = DescribePoolingOp(\n        \"AvgPool\", {n, in, in, c}, {1, k, k, 1}, {1, s, s, 1}, \"NHWC\", padding);\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    // Typical 3x3 window with 2x2 stride.\n    auto costs = predict_avg_pool(10, 20, 384, 3, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1113600), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(345600), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(768000), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n  {\n    // 1x1 window with 2x2 stride: used for shortcut in resnet-50.\n    auto costs = predict_avg_pool(10, 20, 384, 1, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(499200), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(38400), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(460800), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n  {\n    // 2x2 window with 3x3 stride.\n    auto costs = predict_avg_pool(10, 20, 384, 2, 3, \"VALID\");\n    EXPECT_EQ(Costs::Duration(580608), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(75264), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(505344), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictAvgPoolGrad) {\n  auto predict_avg_pool_grad = [this](const int n, const int in, const int c,\n                                      const int k, const int s,\n                                      const string& padding) -> Costs {\n    OpContext op_context =\n        DescribePoolingOp(\"AvgPoolGrad\", {n, in, in, c}, {1, k, k, 1},\n                          {1, s, s, 1}, \"NHWC\", padding);\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    // Typical 3xz3 window with 2x2 stride.\n    auto costs = predict_avg_pool_grad(10, 20, 384, 3, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1305602), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(537600), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(768002), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n  {\n    // 1x1 window with 2x2 stride: used for shortcut in resnet-50.\n    auto costs = predict_avg_pool_grad(10, 20, 384, 1, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(960002), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(192000), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(768002), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n  {\n    // 2x2 window with 3x3 stride.\n    auto costs = predict_avg_pool_grad(10, 20, 384, 2, 3, \"VALID\");\n    EXPECT_EQ(Costs::Duration(862082), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(172416), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(689666), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictFusedBatchNorm) {\n  auto predict_fused_bn = [this](const int n, const int in, const int c,\n                                 const bool is_training) -> Costs {\n    OpContext op_context = DescribeFusedBatchNorm(\n        is_training, /*is_grad=*/false, {n, in, in, c}, \"NHWC\");\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    auto costs = predict_fused_bn(10, 20, 96, /*is_training=*/true);\n    EXPECT_EQ(Costs::Duration(614737), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(153706), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(461031), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n\n  {\n    auto costs = predict_fused_bn(10, 20, 32, /*is_training=*/true);\n    EXPECT_EQ(Costs::Duration(204913), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(51236), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(153677), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n\n  {\n    auto costs = predict_fused_bn(10, 20, 96, /*is_training=*/false);\n    EXPECT_EQ(Costs::Duration(384154), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(76800), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(307354), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n\n  {\n    auto costs = predict_fused_bn(10, 20, 32, /*is_training=*/false);\n    EXPECT_EQ(Costs::Duration(128052), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(25600), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(102452), costs.memory_time);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictFusedBatchNormGrad) {\n  auto predict_fused_bn_grad = [this](const int n, const int in,\n                                      const int c) -> Costs {\n    OpContext op_context = DescribeFusedBatchNorm(\n        /*is_training=*/false, /*is_grad=*/true, {n, in, in, c}, \"NHWC\");\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    auto costs = predict_fused_bn_grad(10, 20, 96);\n    EXPECT_EQ(Costs::Duration(1037050), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(422496), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(614554), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n\n  {\n    auto costs = predict_fused_bn_grad(128, 7, 384);\n    EXPECT_EQ(Costs::Duration(6503809), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(2649677), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(3854132), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, MaybeGetMinimumShape) {\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(true);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 4, &unknown_shapes);\n    EXPECT_TRUE(unknown_shapes);\n    ExpectTensorShape({1, 1, 1, 1}, y);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 1, &unknown_shapes);\n    EXPECT_FALSE(unknown_shapes);\n    ExpectTensorShape({1}, y);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 2, &unknown_shapes);\n    EXPECT_FALSE(unknown_shapes);\n    ExpectTensorShape({1, 1}, y);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    x.add_dim()->set_size(10);\n    x.add_dim()->set_size(20);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 2, &unknown_shapes);\n    EXPECT_FALSE(unknown_shapes);\n    ExpectTensorShape({10, 20}, y);\n\n    unknown_shapes = false;\n    TensorShapeProto z = MaybeGetMinimumShape(x, 4, &unknown_shapes);\n    EXPECT_TRUE(unknown_shapes);\n    EXPECT_EQ(4, z.dim_size());\n    ExpectTensorShape({10, 20, 1, 1}, z);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    x.add_dim()->set_size(10);\n    x.add_dim()->set_size(20);\n    x.add_dim()->set_size(-1);\n    x.add_dim()->set_size(20);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 4, &unknown_shapes);\n    EXPECT_TRUE(unknown_shapes);\n    ExpectTensorShape({10, 20, 1, 20}, y);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    x.add_dim()->set_size(10);\n    x.add_dim()->set_size(20);\n    x.add_dim()->set_size(30);\n    x.add_dim()->set_size(20);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 2, &unknown_shapes);\n    EXPECT_TRUE(unknown_shapes);\n    ExpectTensorShape({10, 20}, y);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, IntermediateRdWrBandwidth) {\n  TestOpLevelCostEstimator estimator;\n\n  // Compute limited.\n  estimator.SetDeviceInfo(DeviceInfo(/*gigaops=*/1,\n                                     /*gb_per_sec=*/1));\n  estimator.SetComputeMemoryOverlap(true);\n  auto cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(3548774400), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.compute_time);\n\n  estimator.SetComputeMemoryOverlap(false);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(3551112192), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.compute_time + cost.memory_time +\n                                     cost.intermediate_memory_time);\n\n  // Memory limited.\n  estimator.SetDeviceInfo(DeviceInfo(/*gigaops=*/99999,\n                                     /*gb_per_sec=*/1));\n  estimator.SetComputeMemoryOverlap(true);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(2337792), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.memory_time);\n\n  estimator.SetComputeMemoryOverlap(false);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(2373281), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.compute_time + cost.memory_time +\n                                     cost.intermediate_memory_time);\n\n  // Intermediate memory bandwidth limited.\n  estimator.SetDeviceInfo(DeviceInfo(/*gigaops=*/99999,\n                                     /*gb_per_sec=*/9999,\n                                     /*intermediate_read_gb_per_sec=*/1,\n                                     /*intermediate_write_gb_per_sec=*/1));\n  estimator.SetComputeMemoryOverlap(true);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(2337792), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.intermediate_memory_time);\n\n  estimator.SetComputeMemoryOverlap(false);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(2373515), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.compute_time + cost.memory_time +\n                                     cost.intermediate_memory_time);\n}\n\nTEST_F(OpLevelCostEstimatorTest, Einsum) {\n  {  // Test a simple matrix multiplication.\n    auto cost = PredictCosts(DescribeEinsum({100, 50}, {100, 50}, \"ik,jk->ij\"));\n    EXPECT_EQ(Costs::Duration(104000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(4000), cost.memory_time);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(PredictCosts(DescribeEinsum({100, 50}, {100, 50}, \"ik,jk->ij\"))\n                  .execution_time,\n              PredictCosts(DescribeXlaEinsum({100, 50}, {100, 50}, \"ik,jk->ij\"))\n                  .execution_time);\n  }\n  {  // Test a simple batch matrix multiplication.\n    auto cost = PredictCosts(\n        DescribeEinsum({25, 100, 50}, {100, 50, 25}, \"Bik,jkB->Bij\"));\n    EXPECT_EQ(Costs::Duration(25 * 104000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(25 * 100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(25 * 4000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(PredictCosts(\n                  DescribeEinsum({25, 100, 50}, {100, 50, 25}, \"Bik,jkB->Bij\"))\n                  .execution_time,\n              PredictCosts(DescribeXlaEinsum({25, 100, 50}, {100, 50, 25},\n                                             \"Bik,jkB->Bij\"))\n                  .execution_time);\n  }\n  {  // Test multiple batch dimensions.\n    auto cost = PredictCosts(DescribeEinsum(\n        {25, 16, 100, 50}, {16, 100, 50, 25}, \"BNik,NjkB->BNij\"));\n    EXPECT_EQ(Costs::Duration(16 * 25 * 104000), cost.execution_time);\n    EXPECT_EQ(\n        Costs::Duration(16 * 25 * 100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n        cost.compute_time);\n    EXPECT_EQ(Costs::Duration(16 * 25 * 4000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({25, 16, 100, 50}, {16, 100, 50, 25},\n                                    \"BNik,NjkB->BNij\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({25, 16, 100, 50}, {16, 100, 50, 25},\n                                       \"BNik,NjkB->BNij\"))\n            .execution_time);\n  }\n  {  // Test multiple M dimensions.\n    auto cost =\n        PredictCosts(DescribeEinsum({25, 100, 50}, {100, 50}, \"Aik,jk->Aij\"));\n    EXPECT_EQ(Costs::Duration(2552000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(25 * 100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(52000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({25, 100, 50}, {100, 50}, \"Aik,jk->Aij\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({25, 100, 50}, {100, 50}, \"Aik,jk->Aij\"))\n            .execution_time);\n  }\n  {  // Test multiple N dimensions.\n    auto cost =\n        PredictCosts(DescribeEinsum({100, 50}, {25, 100, 50}, \"ik,Bjk->ijB\"));\n    EXPECT_EQ(Costs::Duration(2552000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(25 * 100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(52000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50}, {25, 100, 50}, \"ik,Bjk->ijB\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50}, {25, 100, 50}, \"ik,Bjk->ijB\"))\n            .execution_time);\n  }\n  {  // Test multiple contracting dimensions.\n    auto cost = PredictCosts(\n        DescribeEinsum({100, 50, 25}, {100, 50, 25}, \"ikl,jkl->ij\"));\n    EXPECT_EQ(Costs::Duration(2600000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(100 * 50 * 25 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(100000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(PredictCosts(\n                  DescribeEinsum({100, 50, 25}, {100, 50, 25}, \"ikl,jkl->ij\"))\n                  .execution_time,\n              PredictCosts(DescribeXlaEinsum({100, 50, 25}, {100, 50, 25},\n                                             \"ikl,jkl->ij\"))\n                  .execution_time);\n  }\n  {  // Test a simple matrix transpose.\n    auto cost = PredictCosts(DescribeEinsum({100, 50}, {}, \"ij->ji\"));\n    EXPECT_EQ(Costs::Duration(2000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50}, {}, \"ij->ji\")).execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50}, {}, \"ij->ji\"))\n            .execution_time);\n  }\n  {  // Test a malformed Einsum equation: Mismatch between shapes and equation.\n    auto cost =\n        PredictCosts(DescribeEinsum({100, 50, 25}, {50, 100}, \"ik,kl->il\"));\n    EXPECT_EQ(Costs::Duration(52000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(52000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50, 25}, {50, 100}, \"ik,kl->il\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50, 25}, {50, 100}, \"ik,kl->il\"))\n            .execution_time);\n\n    cost = PredictCosts(DescribeEinsum({100, 50}, {50, 100, 25}, \"ik,kl->il\"));\n    EXPECT_EQ(Costs::Duration(52000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(52000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50}, {50, 100, 25}, \"ik,kl->il\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50}, {50, 100, 25}, \"ik,kl->il\"))\n            .execution_time);\n  }\n  {  // Test an unsupported Einsum: ellipsis\n    auto cost = PredictCosts(DescribeEinsum(\n        {100, 50, 25, 16}, {50, 100, 32, 12}, \"ik...,kl...->il...\"));\n    EXPECT_EQ(Costs::Duration(1568000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(1568000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50, 25, 16}, {50, 100, 32, 12},\n                                    \"ik...,kl...->il...\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50, 25, 16}, {50, 100, 32, 12},\n                                       \"ik...,kl...->il...\"))\n            .execution_time);\n  }\n  {  // Test a malformed/unsupported Einsum: repeated indices\n    auto cost =\n        PredictCosts(DescribeEinsum({100, 100, 50}, {50, 100}, \"iik,kl->il\"));\n    EXPECT_EQ(Costs::Duration(202000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(202000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 100, 50}, {50, 100}, \"iik,kl->il\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 100, 50}, {50, 100}, \"iik,kl->il\"))\n            .execution_time);\n  }\n  {  // Test missing shapes.\n    auto cost = PredictCosts(DescribeEinsum({-1, 50}, {100, 50}, \"ik,jk->ij\"));\n    EXPECT_EQ(Costs::Duration(3020), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(1 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(2020), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(PredictCosts(DescribeEinsum({-1, 50}, {100, 50}, \"ik,jk->ij\"))\n                  .execution_time,\n              PredictCosts(DescribeXlaEinsum({-1, 50}, {100, 50}, \"ik,jk->ij\"))\n                  .execution_time);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictResourceVariableOps) {\n  TestOpLevelCostEstimator estimator;\n  estimator.SetDeviceInfo(DeviceInfo(/*gigaops=*/1, /*gb_per_sec=*/1));\n\n  {\n    OpContext op_context;\n    op_context.op_info.set_op(\"AssignVariableOp\");\n    DescribeDummyTensor(op_context.op_info.add_inputs());\n    DescribeTensor1D(100, op_context.op_info.add_inputs());\n    auto cost = estimator.PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(400), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(400), cost.execution_time);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n\n  {\n    OpContext op_context;\n    op_context.op_info.set_op(\"AssignSubVariableOp\");\n    DescribeDummyTensor(op_context.op_info.add_inputs());\n    DescribeTensor1D(100, op_context.op_info.add_inputs());\n    auto cost = estimator.PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(400), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(100), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(400), cost.execution_time);\n    EXPECT_FALSE(cost.inaccurate);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, AddNExecutionTime) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"AddN\");\n\n  DescribeTensor4D(1, 10, 10, 10, op_context.op_info.add_inputs());\n  DescribeTensor4D(1, 10, 10, 10, op_context.op_info.add_inputs());\n  DescribeTensor4D(1, 10, 10, 10, op_context.op_info.add_inputs());\n\n  auto cost = PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(1200), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(200), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(1400), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, IdentityOpExecutionTime) {\n  std::vector<std::string> identity_ops = {\n      \"_Recv\",         \"_Send\",        \"BitCast\",         \"Identity\",\n      \"Enter\",         \"Exit\",         \"IdentityN\",       \"Merge\",\n      \"NextIteration\", \"Placeholder\",  \"PreventGradient\", \"RefIdentity\",\n      \"Reshape\",       \"StopGradient\", \"Switch\"};\n\n  const int kTensorSize = 1000;\n  for (auto identity_op : identity_ops) {\n    OpContext op_context = DescribeUnaryOp(identity_op, kTensorSize);\n\n    const int kExpectedMemoryTime = 0;\n    const int kExpectedComputeTime = 1;\n\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(kExpectedMemoryTime), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(kExpectedComputeTime), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(kExpectedComputeTime + kExpectedMemoryTime),\n              cost.execution_time);\n    EXPECT_EQ(cost.max_memory, kTensorSize * 4);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PureMemoryOpExecutionTime) {\n  std::vector<std::string> reshape_ops = {\n      \"ConcatV2\",     \"DataFormatVecPermute\",\n      \"DepthToSpace\", \"ExpandDims\",\n      \"Fill\",         \"OneHot\",\n      \"Pack\",         \"Range\",\n      \"SpaceToDepth\", \"Split\",\n      \"Squeeze\",      \"Transpose\",\n      \"Tile\",         \"Unpack\"};\n\n  const int kTensorSize = 1000;\n  for (auto reshape_op : reshape_ops) {\n    OpContext op_context = DescribeUnaryOp(reshape_op, kTensorSize);\n\n    const int kExpectedMemoryTime = 800;\n    const int kExpectedComputeTime = 0;\n\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(kExpectedMemoryTime), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(kExpectedComputeTime), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(kExpectedComputeTime + kExpectedMemoryTime),\n              cost.execution_time);\n    EXPECT_EQ(cost.max_memory, kTensorSize * 4);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, ResizeBilinearExecutionTime) {\n  const int kImageDim = 255;\n  const int kChannelSize = 10;\n  const int kComputeLerpCost = 9;\n  {\n    OpContext op_context;\n    SetCpuDevice(&op_context.op_info);\n    op_context.op_info.set_op(\"ResizeBilinear\");\n    DescribeTensor4D(1, kImageDim, kImageDim, kChannelSize,\n                     op_context.op_info.add_inputs());\n    // Test with no output.\n    auto cost = PredictCosts(op_context);\n    ExpectZeroCost(cost);\n    op_context.op_info.clear_inputs();\n\n    DescribeTensor4D(0, 0, 0, 0, op_context.op_info.add_outputs());\n    // Test with no input.\n    cost = PredictCosts(op_context);\n    ExpectZeroCost(cost);\n  }\n  {\n    // Test with size 0 output.\n    OpContext op_context;\n    SetCpuDevice(&op_context.op_info);\n    op_context.op_info.set_op(\"ResizeBilinear\");\n\n    DescribeTensor4D(1, kImageDim, kImageDim, kChannelSize,\n                     op_context.op_info.add_inputs());\n    const int kExpectedMemoryTime = kImageDim * kImageDim * 4;\n    DescribeTensor4D(0, 0, 0, 0, op_context.op_info.add_outputs());\n\n    // As the half_pixel_centers attr was not set, cost should be inaccurate\n    // with 0 compute time.\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(0));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n\n    AttrValue half_pixel_centers;\n    half_pixel_centers.set_b(false);\n    (*op_context.op_info.mutable_attr())[\"half_pixel_centers\"] =\n        half_pixel_centers;\n    cost = PredictCosts(op_context);\n    // Compute time depends only on output size, so compute time is 0.\n    EXPECT_EQ(cost.compute_time, Costs::Duration(0));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n\n  // Test with non-zero output size.\n  const int kOutputImageDim = 100;\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"ResizeBilinear\");\n  DescribeTensor4D(1, kImageDim, kImageDim, kChannelSize,\n                   op_context.op_info.add_inputs());\n  DescribeTensor4D(1, kOutputImageDim, kOutputImageDim, kChannelSize,\n                   op_context.op_info.add_outputs());\n  const int kExpectedMemoryTime =\n      (kImageDim * kImageDim + kOutputImageDim * kOutputImageDim) * 4;\n\n  {\n    // Cost of calculating weights without using half_pixel_centers.\n    AttrValue half_pixel_centers;\n    half_pixel_centers.set_b(false);\n    (*op_context.op_info.mutable_attr())[\"half_pixel_centers\"] =\n        half_pixel_centers;\n    const int kInterpWeightCost = 10;\n    const int num_ops =\n        kInterpWeightCost * (kOutputImageDim * 2) +\n        kComputeLerpCost * (kOutputImageDim * kOutputImageDim * kChannelSize);\n    const int expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(kExpectedMemoryTime + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n\n  {\n    // Cost of calculating weights using half_pixel_centers.\n    AttrValue half_pixel_centers;\n    half_pixel_centers.set_b(true);\n    (*op_context.op_info.mutable_attr())[\"half_pixel_centers\"] =\n        half_pixel_centers;\n    const int kInterpWeightCost = 12;\n    const int num_ops =\n        kInterpWeightCost * (kOutputImageDim * 2) +\n        kComputeLerpCost * (kOutputImageDim * kOutputImageDim * kChannelSize);\n    const int expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(kExpectedMemoryTime + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n\n  {\n    // Cost with very large tensor.\n    op_context.op_info.clear_outputs();\n    // Number of elements in tensor exceeds 2^32.\n    constexpr int64_t kLargeOutputImageDim = 40000;\n    DescribeTensor4D(1, kLargeOutputImageDim, kLargeOutputImageDim,\n                     kChannelSize, op_context.op_info.add_outputs());\n    const int64_t kInterpWeightCost = 12;\n    // Using half_pixel_centers.\n    AttrValue half_pixel_centers;\n    half_pixel_centers.set_b(true);\n    (*op_context.op_info.mutable_attr())[\"half_pixel_centers\"] =\n        half_pixel_centers;\n\n    const int64_t num_ops =\n        kInterpWeightCost * (kLargeOutputImageDim * 2) +\n        kComputeLerpCost *\n            (kLargeOutputImageDim * kLargeOutputImageDim * kChannelSize);\n    const int64_t expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const int64_t expected_memory_time =\n        (kImageDim * kImageDim + kLargeOutputImageDim * kLargeOutputImageDim) *\n        4;\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(expected_memory_time));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(expected_memory_time + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, CropAndResizeExecutionTime) {\n  const int kImageDim = 255;\n  const int kChannelSize = 10;\n  const int kOutputImageDim = 100;\n  const int kNumBoxes = 10;\n  const int kOutputElements =\n      kNumBoxes * kOutputImageDim * kOutputImageDim * kChannelSize;\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"CropAndResize\");\n  DescribeTensor4D(1, kImageDim, kImageDim, kChannelSize,\n                   op_context.op_info.add_inputs());\n  DescribeArbitraryRankInput({kNumBoxes, 4}, DT_INT64, &op_context.op_info);\n  DescribeTensor4D(kNumBoxes, kOutputImageDim, kOutputImageDim, kChannelSize,\n                   op_context.op_info.add_outputs());\n\n  // Note this is time [ns, default in Duration in Costs], not bytes;\n  // whereas memory bandwidth from SetCpuDevice() is 10GB/s.\n  const int kExpectedMemoryTime =\n      (kImageDim * kImageDim * 4 +  // input image in float.\n       kNumBoxes * 4 * 8 / 10 +     // boxes (kNumBoxes x 4) in int64.\n       kNumBoxes * kOutputImageDim * kOutputImageDim * 4);  // output in float.\n  // Note that input image and output image has kChannelSize dim, which is 10,\n  // hence, no need to divide it by 10 (bandwidth).\n\n  {\n    // Cost of CropAndResize with bilinear interpolation.\n    AttrValue method;\n    method.set_s(\"bilinear\");\n    (*op_context.op_info.mutable_attr())[\"method\"] = method;\n    int num_ops = 28 * kNumBoxes + 4 * kNumBoxes * kOutputImageDim +\n                  4 * kNumBoxes * kOutputImageDim * kOutputImageDim +\n                  3 * kNumBoxes * kOutputImageDim +\n                  3 * kNumBoxes * kOutputImageDim * kOutputImageDim +\n                  13 * kOutputElements;\n    const int expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(kExpectedMemoryTime + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n\n  {\n    // Cost of CropAndResize when nearest pixel is taken.\n    AttrValue method;\n    method.set_s(\"nearest\");\n    (*op_context.op_info.mutable_attr())[\"method\"] = method;\n    int num_ops = 28 * kNumBoxes + 4 * kNumBoxes * kOutputImageDim +\n                  4 * kNumBoxes * kOutputImageDim * kOutputImageDim +\n                  2 * kNumBoxes * kOutputImageDim * kOutputImageDim +\n                  kOutputElements;\n    const int expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(kExpectedMemoryTime + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n}\n\n}  // end namespace grappler\n}  // end namespace tensorflow\n"], "fixing_code": ["load(\"//tensorflow:tensorflow.bzl\", \"filegroup\")\nload(\"//tensorflow/core/platform:rules_cc.bzl\", \"cc_library\")\nload(\n    \"//tensorflow:tensorflow.bzl\",\n    \"tf_cc_test\",\n    \"tf_cuda_library\",\n)\nload(\n    \"//tensorflow/core/platform:build_config.bzl\",\n    \"tf_proto_library\",\n    \"tf_protos_grappler\",\n    \"tf_pyclif_proto_library\",\n)\n\npackage(\n    licenses = [\"notice\"],\n)\n\nalias(\n    name = \"graph_properties_testdata\",\n    actual = \"//tensorflow/core/grappler/costs/graph_properties_testdata:graph_properties_testdata\",\n    visibility = [\"//visibility:public\"],\n)\n\ntf_proto_library(\n    name = \"op_performance_data\",\n    srcs = [\"op_performance_data.proto\"],\n    cc_api_version = 2,\n    make_default_target_header_only = True,\n    protodeps = [\n        \"//tensorflow/core/framework:attr_value_proto\",\n        \"//tensorflow/core/framework:resource_handle_proto\",\n        \"//tensorflow/core/framework:tensor_proto\",\n        \"//tensorflow/core/framework:tensor_shape_proto\",\n        \"//tensorflow/core/framework:types_proto\",\n        \"//tensorflow/core/protobuf:for_core_protos\",\n    ],\n    visibility = [\"//visibility:public\"],\n)\n\ntf_pyclif_proto_library(\n    name = \"op_performance_data_pyclif\",\n    proto_lib = \":op_performance_data\",\n    proto_srcfile = \"op_performance_data.proto\",\n    visibility = [\"//visibility:public\"],\n)\n\nfilegroup(\n    name = \"pywrap_required_hdrs\",\n    srcs = [\n        \"analytical_cost_estimator.h\",\n        \"cost_estimator.h\",\n        \"graph_memory.h\",\n        \"graph_properties.h\",\n        \"measuring_cost_estimator.h\",\n        \"op_context.h\",\n        \"op_level_cost_estimator.h\",\n        \"utils.h\",\n        \"virtual_placer.h\",\n        \"virtual_scheduler.h\",\n    ],\n    visibility = [\n        \"//tensorflow/python/grappler:__pkg__\",\n    ],\n)\n\ncc_library(\n    name = \"graph_properties\",\n    srcs = [\"graph_properties.cc\"],\n    hdrs = [\"graph_properties.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":utils\",\n        \"@com_google_absl//absl/container:flat_hash_map\",\n        \"@com_google_absl//absl/types:optional\",\n        \"//tensorflow/core/grappler/utils:functions\",\n        \"//tensorflow/core/grappler/utils:topological_sort\",\n        \"//tensorflow/core/grappler:mutable_graph_view\",\n        \"//tensorflow/core/grappler:op_types\",\n        \"//tensorflow/core:core_cpu_base\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/grappler/clusters:cluster\",\n        \"//tensorflow/core/grappler/optimizers:evaluation_utils\",\n    ] + tf_protos_grappler(),\n)\n\ntf_cc_test(\n    name = \"graph_properties_test\",\n    srcs = [\"graph_properties_test.cc\"],\n    args = [\"--heap_check=\"],  # The GPU tracer leaks memory. TODO(b/185483595): use a dependency instead of a flag\n    data = [\":graph_properties_testdata\"],\n    tags = [\n        \"nomsan\",  # TODO(b/160921160): broken by NOAUTOROLLBACK CL\n    ],\n    deps = [\n        \":graph_properties\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/cc:functional_ops\",\n        \"//tensorflow/cc:scope\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:lib_proto_parsing\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/framework:tensor_testutil\",\n        \"//tensorflow/core/graph:mkl_graph_util\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler/clusters:single_machine\",\n        \"//tensorflow/core/grappler/inputs:trivial_test_graph_input_yielder\",\n        \"//tensorflow/core/grappler/inputs:utils\",\n    ],\n)\n\ncc_library(\n    name = \"graph_memory\",\n    srcs = [\"graph_memory.cc\"],\n    hdrs = [\"graph_memory.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \":graph_properties\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/grappler/clusters:cluster\",\n        \"//tensorflow/core/grappler/clusters:virtual_cluster\",\n    ],\n)\n\ntf_cc_test(\n    name = \"graph_memory_test\",\n    srcs = [\"graph_memory_test.cc\"],\n    args = [\"--heap_check=\"],  # The GPU tracer leaks memory. TODO(b/185483595): use a dependency instead of a flag\n    deps = [\n        \":graph_memory\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler/inputs:trivial_test_graph_input_yielder\",\n    ],\n)\n\ncc_library(\n    name = \"robust_stats\",\n    srcs = [\"robust_stats.cc\"],\n    hdrs = [\"robust_stats.h\"],\n    visibility = [\"//visibility:public\"],\n)\n\ntf_cc_test(\n    name = \"robust_stats_test\",\n    srcs = [\"robust_stats_test.cc\"],\n    deps = [\n        \":robust_stats\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n    ],\n)\n\ntf_cuda_library(\n    name = \"utils\",\n    srcs = [\"utils.cc\"],\n    hdrs = [\"utils.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \"//third_party/eigen3\",\n        \"@com_google_absl//absl/container:node_hash_map\",\n        \"@com_google_absl//absl/strings\",\n        \"@com_google_absl//absl/strings:str_format\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:graph\",\n        \"//tensorflow/core/common_runtime/gpu:gpu_id\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:lib_proto_parsing\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/util:overflow\",\n        \"//tensorflow/core/grappler/clusters:utils\",\n    ] + tf_protos_grappler(),\n)\n\ntf_cc_test(\n    name = \"utils_test\",\n    srcs = [\"utils_test.cc\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":utils\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/core:all_kernels\",\n        \"//tensorflow/core:core_cpu_internal\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core:testlib\",\n        \"//tensorflow/core/framework:tensor_testutil\",\n    ],\n)\n\ncc_library(\n    name = \"cost_estimator\",\n    srcs = [\"cost_estimator.cc\"],\n    hdrs = [\"cost_estimator.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n    ],\n)\n\ntf_cc_test(\n    name = \"cost_estimator_test\",\n    srcs = [\"cost_estimator_test.cc\"],\n    deps = [\n        \":cost_estimator\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n    ],\n)\n\ncc_library(\n    name = \"virtual_placer\",\n    srcs = [\"virtual_placer.cc\"],\n    hdrs = [\"virtual_placer.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:devices\",\n        \"//tensorflow/core/grappler/clusters:cluster\",\n    ],\n)\n\ntf_cc_test(\n    name = \"virtual_placer_test\",\n    srcs = [\"virtual_placer_test.cc\"],\n    deps = [\n        \":virtual_placer\",\n        \"//tensorflow/core:core_cpu\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/grappler/clusters:virtual_cluster\",\n    ],\n)\n\ncc_library(\n    name = \"op_context\",\n    hdrs = [\"op_context.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"@com_google_absl//absl/container:flat_hash_map\",\n        \"//tensorflow/core:protos_all_cc\",\n    ] + tf_protos_grappler(),\n)\n\ncc_library(\n    name = \"virtual_scheduler\",\n    srcs = [\"virtual_scheduler.cc\"],\n    hdrs = [\"virtual_scheduler.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \":graph_properties\",\n        \":op_context\",\n        \":utils\",\n        \":virtual_placer\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler:op_types\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/grappler/clusters:utils\",\n        \"//tensorflow/core/grappler/utils:transitive_fanin\",\n        \"@com_google_absl//absl/strings\",\n        \"@com_google_absl//absl/strings:str_format\",\n    ],\n)\n\ntf_cc_test(\n    name = \"virtual_scheduler_test\",\n    srcs = [\"virtual_scheduler_test.cc\"],\n    deps = [\n        \":graph_properties\",\n        \":utils\",\n        \":virtual_placer\",\n        \":virtual_scheduler\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:tensorflow\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/grappler/clusters:virtual_cluster\",\n    ],\n)\n\ncc_library(\n    name = \"measuring_cost_estimator\",\n    srcs = [\"measuring_cost_estimator.cc\"],\n    hdrs = [\"measuring_cost_estimator.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":robust_stats\",\n        \"//tensorflow/core:core_cpu\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:lib_internal\",\n        \"//tensorflow/core:lib_proto_parsing\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler/clusters:cluster\",\n        \"//tensorflow/core/grappler/costs:cost_estimator\",\n        \"//tensorflow/core/kernels:ops_util\",\n    ],\n    alwayslink = 1,\n)\n\ncc_library(\n    name = \"op_level_cost_estimator\",\n    srcs = [\"op_level_cost_estimator.cc\"],\n    hdrs = [\"op_level_cost_estimator.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \":op_context\",\n        \":utils\",\n        \"@com_google_absl//absl/strings\",\n        \"//third_party/eigen3\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler/clusters:utils\",\n        \"//tensorflow/core/util:overflow\",\n    ] + tf_protos_grappler(),\n)\n\ntf_cc_test(\n    name = \"op_level_cost_estimator_test\",\n    srcs = [\"op_level_cost_estimator_test.cc\"],\n    tags = [\"no_oss\"],  # b/163222310\n    deps = [\n        \":op_level_cost_estimator\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/platform:status_matchers\",\n    ],\n)\n\ncc_library(\n    name = \"analytical_cost_estimator\",\n    srcs = [\"analytical_cost_estimator.cc\"],\n    hdrs = [\"analytical_cost_estimator.h\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":cost_estimator\",\n        \":graph_properties\",\n        \":op_level_cost_estimator\",\n        \":utils\",\n        \":virtual_placer\",\n        \":virtual_scheduler\",\n        \"//tensorflow/core:core_cpu_base\",\n        \"//tensorflow/core:graph\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core/grappler:grappler_item\",\n        \"//tensorflow/core/grappler:op_types\",\n        \"//tensorflow/core/grappler:utils\",\n        \"//tensorflow/core/util:overflow\",\n    ] + tf_protos_grappler(),\n    alwayslink = 1,\n)\n\ntf_cc_test(\n    name = \"analytical_cost_estimator_test\",\n    srcs = [\"analytical_cost_estimator_test.cc\"],\n    deps = [\n        \":analytical_cost_estimator\",\n        \":virtual_scheduler\",\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/core:protos_all_cc\",\n        \"//tensorflow/core:tensorflow\",\n        \"//tensorflow/core:test\",\n        \"//tensorflow/core:test_main\",\n        \"//tensorflow/core/grappler/clusters:virtual_cluster\",\n    ],\n)\n\n# copybara:uncomment_begin(google-only)\n# py_proto_library(\n#     name = \"op_performance_data_py_pb2\",\n#     has_services = 0,\n#     api_version = 2,\n#     visibility = [\"//visibility:public\"],\n#     deps = [\":op_performance_data\"],\n# )\n# copybara:uncomment_end\n", "\n/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/grappler/costs/op_level_cost_estimator.h\"\n\n#include \"absl/strings/match.h\"\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/framework/attr_value.pb.h\"\n#include \"tensorflow/core/framework/attr_value_util.h\"\n#include \"tensorflow/core/framework/tensor.pb.h\"\n#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/grappler/clusters/utils.h\"\n#include \"tensorflow/core/grappler/costs/op_context.h\"\n#include \"tensorflow/core/grappler/costs/utils.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/util/overflow.h\"\n\nnamespace tensorflow {\nnamespace grappler {\n\n// TODO(dyoon): update op to Predict method map for TF ops with V2 or V3 suffix.\nconstexpr int kOpsPerMac = 2;\nconstexpr char kGuaranteeConst[] = \"GuaranteeConst\";\nconstexpr char kAddN[] = \"AddN\";\nconstexpr char kBitCast[] = \"BitCast\";\nconstexpr char kConcatV2[] = \"ConcatV2\";\nconstexpr char kConv2d[] = \"Conv2D\";\nconstexpr char kConv2dBackpropFilter[] = \"Conv2DBackpropFilter\";\nconstexpr char kConv2dBackpropInput[] = \"Conv2DBackpropInput\";\nconstexpr char kFusedConv2dBiasActivation[] = \"FusedConv2DBiasActivation\";\nconstexpr char kDataFormatVecPermute[] = \"DataFormatVecPermute\";\nconstexpr char kDepthToSpace[] = \"DepthToSpace\";\nconstexpr char kDepthwiseConv2dNative[] = \"DepthwiseConv2dNative\";\nconstexpr char kDepthwiseConv2dNativeBackpropFilter[] =\n    \"DepthwiseConv2dNativeBackpropFilter\";\nconstexpr char kDepthwiseConv2dNativeBackpropInput[] =\n    \"DepthwiseConv2dNativeBackpropInput\";\nconstexpr char kMatMul[] = \"MatMul\";\nconstexpr char kXlaEinsum[] = \"XlaEinsum\";\nconstexpr char kEinsum[] = \"Einsum\";\nconstexpr char kExpandDims[] = \"ExpandDims\";\nconstexpr char kFill[] = \"Fill\";\nconstexpr char kSparseMatMul[] = \"SparseMatMul\";\nconstexpr char kSparseTensorDenseMatMul[] = \"SparseTensorDenseMatMul\";\nconstexpr char kPlaceholder[] = \"Placeholder\";\nconstexpr char kIdentity[] = \"Identity\";\nconstexpr char kIdentityN[] = \"IdentityN\";\nconstexpr char kRefIdentity[] = \"RefIdentity\";\nconstexpr char kNoOp[] = \"NoOp\";\nconstexpr char kReshape[] = \"Reshape\";\nconstexpr char kSplit[] = \"Split\";\nconstexpr char kSqueeze[] = \"Squeeze\";\nconstexpr char kRecv[] = \"_Recv\";\nconstexpr char kSend[] = \"_Send\";\nconstexpr char kBatchMatMul[] = \"BatchMatMul\";\nconstexpr char kBatchMatMulV2[] = \"BatchMatMulV2\";\nconstexpr char kOneHot[] = \"OneHot\";\nconstexpr char kPack[] = \"Pack\";\nconstexpr char kRank[] = \"Rank\";\nconstexpr char kRange[] = \"Range\";\nconstexpr char kShape[] = \"Shape\";\nconstexpr char kShapeN[] = \"ShapeN\";\nconstexpr char kSize[] = \"Size\";\nconstexpr char kStopGradient[] = \"StopGradient\";\nconstexpr char kPreventGradient[] = \"PreventGradient\";\nconstexpr char kGather[] = \"Gather\";\nconstexpr char kGatherNd[] = \"GatherNd\";\nconstexpr char kGatherV2[] = \"GatherV2\";\nconstexpr char kScatterAdd[] = \"ScatterAdd\";\nconstexpr char kScatterDiv[] = \"ScatterDiv\";\nconstexpr char kScatterMax[] = \"ScatterMax\";\nconstexpr char kScatterMin[] = \"ScatterMin\";\nconstexpr char kScatterMul[] = \"ScatterMul\";\nconstexpr char kScatterSub[] = \"ScatterSub\";\nconstexpr char kScatterUpdate[] = \"ScatterUpdate\";\nconstexpr char kSlice[] = \"Slice\";\nconstexpr char kStridedSlice[] = \"StridedSlice\";\nconstexpr char kSpaceToDepth[] = \"SpaceToDepth\";\nconstexpr char kTranspose[] = \"Transpose\";\nconstexpr char kTile[] = \"Tile\";\nconstexpr char kMaxPool[] = \"MaxPool\";\nconstexpr char kMaxPoolGrad[] = \"MaxPoolGrad\";\nconstexpr char kAvgPool[] = \"AvgPool\";\nconstexpr char kAvgPoolGrad[] = \"AvgPoolGrad\";\nconstexpr char kFusedBatchNorm[] = \"FusedBatchNorm\";\nconstexpr char kFusedBatchNormGrad[] = \"FusedBatchNormGrad\";\nconstexpr char kQuantizedMatMul[] = \"QuantizedMatMul\";\nconstexpr char kQuantizedMatMulV2[] = \"QuantizedMatMulV2\";\nconstexpr char kUnpack[] = \"Unpack\";\nconstexpr char kSoftmax[] = \"Softmax\";\nconstexpr char kResizeBilinear[] = \"ResizeBilinear\";\nconstexpr char kCropAndResize[] = \"CropAndResize\";\n// Dynamic control flow ops.\nconstexpr char kSwitch[] = \"Switch\";\nconstexpr char kMerge[] = \"Merge\";\nconstexpr char kEnter[] = \"Enter\";\nconstexpr char kExit[] = \"Exit\";\nconstexpr char kNextIteration[] = \"NextIteration\";\n// Persistent ops.\nconstexpr char kConst[] = \"Const\";\nconstexpr char kVariable[] = \"Variable\";\nconstexpr char kVariableV2[] = \"VariableV2\";\nconstexpr char kAutoReloadVariable[] = \"AutoReloadVariable\";\nconstexpr char kVarHandleOp[] = \"VarHandleOp\";\nconstexpr char kVarHandlesOp[] = \"_VarHandlesOp\";\nconstexpr char kReadVariableOp[] = \"ReadVariableOp\";\nconstexpr char kReadVariablesOp[] = \"_ReadVariablesOp\";\nconstexpr char kAssignVariableOp[] = \"AssignVariableOp\";\nconstexpr char kAssignAddVariableOp[] = \"AssignAddVariableOp\";\nconstexpr char kAssignSubVariableOp[] = \"AssignSubVariableOp\";\n\nstatic const Costs::Duration kMinComputeTime(1);\nstatic const int64_t kMinComputeOp = 1;\n\nnamespace {\n\nstd::string GetDataFormat(const OpInfo& op_info) {\n  std::string data_format = \"NHWC\";  // Default format.\n  if (op_info.attr().find(\"data_format\") != op_info.attr().end()) {\n    data_format = op_info.attr().at(\"data_format\").s();\n  }\n  return data_format;\n}\n\nstd::string GetFilterFormat(const OpInfo& op_info) {\n  std::string filter_format = \"HWIO\";  // Default format.\n  if (op_info.attr().find(\"filter_format\") != op_info.attr().end()) {\n    filter_format = op_info.attr().at(\"filter_format\").s();\n  }\n  return filter_format;\n}\n\nPadding GetPadding(const OpInfo& op_info) {\n  if (op_info.attr().find(\"padding\") != op_info.attr().end() &&\n      op_info.attr().at(\"padding\").s() == \"VALID\") {\n    return Padding::VALID;\n  }\n  return Padding::SAME;  // Default padding.\n}\n\nbool IsTraining(const OpInfo& op_info) {\n  if (op_info.attr().find(\"is_training\") != op_info.attr().end() &&\n      op_info.attr().at(\"is_training\").b()) {\n    return true;\n  }\n  return false;\n}\n\n// TODO(dyoon): support non-4D tensors in the cost functions of convolution\n// related ops (Conv, Pool, BatchNorm, and their backprops) and the related\n// helper functions.\nstd::vector<int64_t> GetStrides(const OpInfo& op_info) {\n  if (op_info.attr().find(\"strides\") != op_info.attr().end()) {\n    const auto strides = op_info.attr().at(\"strides\").list().i();\n    DCHECK(strides.size() == 4)\n        << \"Attr strides is not a length-4 vector: \" << op_info.DebugString();\n    if (strides.size() != 4) return {1, 1, 1, 1};\n    return {strides[0], strides[1], strides[2], strides[3]};\n  }\n  return {1, 1, 1, 1};\n}\n\nstd::vector<int64_t> GetKernelSize(const OpInfo& op_info) {\n  if (op_info.attr().find(\"ksize\") != op_info.attr().end()) {\n    const auto ksize = op_info.attr().at(\"ksize\").list().i();\n    DCHECK(ksize.size() == 4)\n        << \"Attr ksize is not a length-4 vector: \" << op_info.DebugString();\n    if (ksize.size() != 4) return {1, 1, 1, 1};\n    return {ksize[0], ksize[1], ksize[2], ksize[3]};\n  }\n  // Note that FusedBatchNorm doesn't have ksize attr, but GetKernelSize returns\n  // {1, 1, 1, 1} in that case.\n  return {1, 1, 1, 1};\n}\n\nint64_t GetOutputSize(const int64_t input, const int64_t filter,\n                      const int64_t stride, const Padding& padding) {\n  // Logic for calculating output shape is from GetWindowedOutputSizeVerbose()\n  // function in third_party/tensorflow/core/framework/common_shape_fns.cc.\n  if (padding == Padding::VALID) {\n    return (input - filter + stride) / stride;\n  } else {  // SAME.\n    return (input + stride - 1) / stride;\n  }\n}\n\n// Return the output element count of a multi-input element-wise op considering\n// broadcasting.\nint64_t CwiseOutputElementCount(const OpInfo& op_info) {\n  int max_rank = 1;\n  for (const OpInfo::TensorProperties& input_properties : op_info.inputs()) {\n    max_rank = std::max(max_rank, input_properties.shape().dim_size());\n  }\n\n  TensorShapeProto output_shape;\n  output_shape.mutable_dim()->Reserve(max_rank);\n  for (int i = 0; i < max_rank; ++i) {\n    output_shape.add_dim();\n  }\n\n  // Expand the shape of the output to follow the numpy-style broadcast rule\n  // which matches each input starting with the trailing dimensions and working\n  // its way forward. To do this, iterate through each input shape's dimensions\n  // in reverse order, and potentially increase the corresponding output\n  // dimension.\n  for (const OpInfo::TensorProperties& input_properties : op_info.inputs()) {\n    const TensorShapeProto& input_shape = input_properties.shape();\n    for (int i = input_shape.dim_size() - 1; i >= 0; --i) {\n      int output_shape_dim_index =\n          i + output_shape.dim_size() - input_shape.dim_size();\n      output_shape.mutable_dim(output_shape_dim_index)\n          ->set_size(std::max(output_shape.dim(output_shape_dim_index).size(),\n                              input_shape.dim(i).size()));\n    }\n  }\n\n  int64_t count = 1;\n  for (int i = 0; i < output_shape.dim_size(); i++) {\n    count *= output_shape.dim(i).size();\n  }\n  return count;\n}\n\n// Helper function for determining whether there are repeated indices in the\n// input Einsum equation.\nbool CheckRepeatedDimensions(const absl::string_view dim_str) {\n  int str_size = dim_str.size();\n  for (int idx = 0; idx < str_size - 1; idx++) {\n    if (dim_str.find(dim_str[idx], idx + 1) != std::string::npos) {\n      return true;\n    }\n  }\n  return false;\n}\n\n// Auxiliary function for determining whether OpLevelCostEstimator is compatible\n// with a given Einsum.\nbool IsEinsumCorrectlyFormed(const OpContext& einsum_context) {\n  const auto& op_info = einsum_context.op_info;\n\n  auto it = op_info.attr().find(\"equation\");\n  if (it == op_info.attr().end()) return false;\n  const absl::string_view equation = it->second.s();\n  std::vector<std::string> equation_split = absl::StrSplit(equation, \"->\");\n\n  if (equation_split.empty()) {\n    LOG(WARNING) << \"Einsum with malformed equation\";\n    return false;\n  }\n  std::vector<absl::string_view> input_split =\n      absl::StrSplit(equation_split[0], ',');\n\n  // The current model covers Einsum operations with two operands and a RHS\n  if (op_info.inputs_size() != 2 || equation_split.size() != 2) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n    return false;\n  }\n  const auto& a_input = op_info.inputs(0);\n  const auto& b_input = op_info.inputs(1);\n  absl::string_view rhs_str = equation_split[1];\n  absl::string_view a_input_str = input_split[0];\n  absl::string_view b_input_str = input_split[1];\n\n  // Ellipsis are not currently supported\n  if (absl::StrContains(a_input_str, \"...\") ||\n      absl::StrContains(b_input_str, \"...\")) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op()\n            << \", ellipsis not supported\";\n    return false;\n  }\n\n  constexpr int kMatrixRank = 2;\n\n  bool a_input_shape_unknown = false;\n  bool b_input_shape_unknown = false;\n\n  TensorShapeProto a_input_shape = MaybeGetMinimumShape(\n      a_input.shape(), std::max(kMatrixRank, a_input.shape().dim_size()),\n      &a_input_shape_unknown);\n  TensorShapeProto b_input_shape = MaybeGetMinimumShape(\n      b_input.shape(), std::max(kMatrixRank, b_input.shape().dim_size()),\n      &b_input_shape_unknown);\n\n  if (a_input_str.size() != static_cast<size_t>(a_input_shape.dim_size()) ||\n      b_input_str.size() != static_cast<size_t>(b_input_shape.dim_size())) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op()\n            << \", equation subscripts don't match tensor rank.\";\n    return false;\n  }\n\n  // Subscripts where axis appears more than once for a single input are not yet\n  // supported\n  if (CheckRepeatedDimensions(a_input_str) ||\n      CheckRepeatedDimensions(b_input_str) ||\n      CheckRepeatedDimensions(rhs_str)) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op()\n            << \", Subscripts where axis appears more than once for a single \"\n               \"input are not yet supported\";\n    return false;\n  }\n\n  return true;\n}\n\n}  // namespace\n\n// Return a minimum shape if the shape is unknown. If known, return the original\n// shape.\nTensorShapeProto MaybeGetMinimumShape(const TensorShapeProto& original_shape,\n                                      int rank, bool* found_unknown_shapes) {\n  auto shape = original_shape;\n  bool is_scalar = !shape.unknown_rank() && shape.dim_size() == 0;\n\n  if (shape.unknown_rank() || (!is_scalar && shape.dim_size() < rank)) {\n    *found_unknown_shapes = true;\n    VLOG(2) << \"Use minimum shape because the rank is unknown.\";\n    // The size of each dimension is at least 1, if unknown.\n    for (int i = shape.dim_size(); i < rank; i++) {\n      shape.add_dim()->set_size(1);\n    }\n  } else if (is_scalar) {\n    for (int i = 0; i < rank; i++) {\n      shape.add_dim()->set_size(1);\n    }\n  } else if (shape.dim_size() > rank) {\n    *found_unknown_shapes = true;\n    shape.clear_dim();\n    for (int i = 0; i < rank; i++) {\n      shape.add_dim()->set_size(original_shape.dim(i).size());\n    }\n  } else {\n    for (int i = 0; i < shape.dim_size(); i++) {\n      if (shape.dim(i).size() < 0) {\n        *found_unknown_shapes = true;\n        VLOG(2) << \"Use minimum dim size 1 because the shape is unknown.\";\n        // The size of each dimension is at least 1, if unknown.\n        shape.mutable_dim(i)->set_size(1);\n      }\n    }\n  }\n  return shape;\n}\n\nOpLevelCostEstimator::OpLevelCostEstimator() {\n  // Syntactic sugar to build and return a lambda that takes an OpInfo and\n  // returns a cost.\n  typedef Status (OpLevelCostEstimator::*CostImpl)(const OpContext& op_context,\n                                                   NodeCosts*) const;\n  auto wrap = [this](CostImpl impl)\n      -> std::function<Status(const OpContext&, NodeCosts*)> {\n    return [this, impl](const OpContext& op_context, NodeCosts* node_costs) {\n      return (this->*impl)(op_context, node_costs);\n    };\n  };\n\n  device_cost_impl_.emplace(kConv2d,\n                            wrap(&OpLevelCostEstimator::PredictConv2D));\n  device_cost_impl_.emplace(\n      kConv2dBackpropFilter,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropFilter));\n  device_cost_impl_.emplace(\n      kConv2dBackpropInput,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropInput));\n  device_cost_impl_.emplace(\n      kFusedConv2dBiasActivation,\n      wrap(&OpLevelCostEstimator::PredictFusedConv2DBiasActivation));\n  // reuse Conv2D for DepthwiseConv2dNative because the calculation is the\n  // same although the actual meaning of the parameters are different. See\n  // comments in PredictConv2D and related functions\n  device_cost_impl_.emplace(kDepthwiseConv2dNative,\n                            wrap(&OpLevelCostEstimator::PredictConv2D));\n  device_cost_impl_.emplace(\n      kDepthwiseConv2dNativeBackpropFilter,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropFilter));\n  device_cost_impl_.emplace(\n      kDepthwiseConv2dNativeBackpropInput,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropInput));\n  device_cost_impl_.emplace(kMatMul,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(kSparseMatMul,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(\n      kSparseTensorDenseMatMul,\n      wrap(&OpLevelCostEstimator::PredictSparseTensorDenseMatMul));\n  device_cost_impl_.emplace(kBatchMatMul,\n                            wrap(&OpLevelCostEstimator::PredictBatchMatMul));\n  device_cost_impl_.emplace(kBatchMatMulV2,\n                            wrap(&OpLevelCostEstimator::PredictBatchMatMul));\n  device_cost_impl_.emplace(kQuantizedMatMul,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(kQuantizedMatMulV2,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(kXlaEinsum,\n                            wrap(&OpLevelCostEstimator::PredictEinsum));\n  device_cost_impl_.emplace(kEinsum,\n                            wrap(&OpLevelCostEstimator::PredictEinsum));\n\n  device_cost_impl_.emplace(kNoOp, wrap(&OpLevelCostEstimator::PredictNoOp));\n  device_cost_impl_.emplace(kGuaranteeConst,\n                            wrap(&OpLevelCostEstimator::PredictNoOp));\n\n  device_cost_impl_.emplace(kGather,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kGatherNd,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kGatherV2,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kScatterAdd,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterDiv,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterMax,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterMin,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterMul,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterSub,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterUpdate,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n\n  device_cost_impl_.emplace(kSlice,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kStridedSlice,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n\n  device_cost_impl_.emplace(kPlaceholder,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kIdentity,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kIdentityN,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kRefIdentity,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kStopGradient,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kPreventGradient,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kReshape,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kRecv,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kSend,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kSwitch,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kMerge,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kEnter,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kExit,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kNextIteration,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kBitCast,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n\n  device_cost_impl_.emplace(kConcatV2,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kDataFormatVecPermute,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kDepthToSpace,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kExpandDims,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kFill,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kOneHot,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kPack,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kRange,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kSpaceToDepth,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kSplit,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kSqueeze,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kTranspose,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kTile,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kUnpack,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n\n  device_cost_impl_.emplace(kRank,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kShape,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kShapeN,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kSize,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kMaxPool,\n                            wrap(&OpLevelCostEstimator::PredictMaxPool));\n  device_cost_impl_.emplace(kMaxPoolGrad,\n                            wrap(&OpLevelCostEstimator::PredictMaxPoolGrad));\n  device_cost_impl_.emplace(kAvgPool,\n                            wrap(&OpLevelCostEstimator::PredictAvgPool));\n  device_cost_impl_.emplace(kAvgPoolGrad,\n                            wrap(&OpLevelCostEstimator::PredictAvgPoolGrad));\n  device_cost_impl_.emplace(kFusedBatchNorm,\n                            wrap(&OpLevelCostEstimator::PredictFusedBatchNorm));\n  device_cost_impl_.emplace(\n      kFusedBatchNormGrad,\n      wrap(&OpLevelCostEstimator::PredictFusedBatchNormGrad));\n  device_cost_impl_.emplace(kSoftmax,\n                            wrap(&OpLevelCostEstimator::PredictSoftmax));\n  device_cost_impl_.emplace(kResizeBilinear,\n                            wrap(&OpLevelCostEstimator::PredictResizeBilinear));\n  device_cost_impl_.emplace(kCropAndResize,\n                            wrap(&OpLevelCostEstimator::PredictCropAndResize));\n  device_cost_impl_.emplace(\n      kAssignVariableOp, wrap(&OpLevelCostEstimator::PredictAssignVariableOps));\n  device_cost_impl_.emplace(\n      kAssignAddVariableOp,\n      wrap(&OpLevelCostEstimator::PredictAssignVariableOps));\n  device_cost_impl_.emplace(\n      kAssignSubVariableOp,\n      wrap(&OpLevelCostEstimator::PredictAssignVariableOps));\n  device_cost_impl_.emplace(kAddN, wrap(&OpLevelCostEstimator::PredictNaryOp));\n\n  persistent_ops_ = {\n      kConst,       kVariable,       kVariableV2,   kAutoReloadVariable,\n      kVarHandleOp, kReadVariableOp, kVarHandlesOp, kReadVariablesOp};\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n\n  // Quantize = apply min and max bounds, multiply by scale factor and round.\n  const int quantize_v2_cost =\n      EIGEN_COST(scalar_product_op<float>) + EIGEN_COST(scalar_max_op<float>) +\n      EIGEN_COST(scalar_min_op<float>) + EIGEN_COST(scalar_round_op<float>);\n  const int quantize_and_dequantize_v2_cost =\n      quantize_v2_cost + EIGEN_COST(scalar_product_op<float>);\n\n  // Unary ops alphabetically sorted\n  elementwise_ops_.emplace(\"Acos\", EIGEN_COST(scalar_acos_op<float>));\n  elementwise_ops_.emplace(\"All\", EIGEN_COST(scalar_boolean_and_op));\n  elementwise_ops_.emplace(\"ArgMax\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Asin\", EIGEN_COST(scalar_asin_op<float>));\n  elementwise_ops_.emplace(\"Atan\", EIGEN_COST(scalar_atan_op<float>));\n  elementwise_ops_.emplace(\"Atan2\", EIGEN_COST(scalar_quotient_op<float>) +\n                                        EIGEN_COST(scalar_atan_op<float>));\n  // For now, we use Eigen cost model for float to int16 cast as an example\n  // case; Eigen cost model is zero when src and dst types are identical,\n  // and it uses AddCost (1) when different. We may implement a separate\n  // cost functions for cast ops, using the actual input and output types.\n  elementwise_ops_.emplace(\n      \"Cast\", Eigen::internal::functor_traits<\n                  Eigen::internal::scalar_cast_op<float, int16>>::Cost);\n  elementwise_ops_.emplace(\"Ceil\", EIGEN_COST(scalar_ceil_op<float>));\n  elementwise_ops_.emplace(\"Cos\", EIGEN_COST(scalar_cos_op<float>));\n  elementwise_ops_.emplace(\"Dequantize\", EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"Erf\", 1);\n  elementwise_ops_.emplace(\"Erfc\", 1);\n  elementwise_ops_.emplace(\"Exp\", EIGEN_COST(scalar_exp_op<float>));\n  elementwise_ops_.emplace(\"Expm1\", EIGEN_COST(scalar_expm1_op<float>));\n  elementwise_ops_.emplace(\"Floor\", EIGEN_COST(scalar_floor_op<float>));\n  elementwise_ops_.emplace(\"Inv\", EIGEN_COST(scalar_inverse_op<float>));\n  elementwise_ops_.emplace(\"InvGrad\", 1);\n  elementwise_ops_.emplace(\"Lgamma\", 1);\n  elementwise_ops_.emplace(\"Log\", EIGEN_COST(scalar_log_op<float>));\n  elementwise_ops_.emplace(\"Log1p\", EIGEN_COST(scalar_log1p_op<float>));\n  elementwise_ops_.emplace(\"Max\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Min\", EIGEN_COST(scalar_min_op<float>));\n  elementwise_ops_.emplace(\"Neg\", EIGEN_COST(scalar_opposite_op<float>));\n  elementwise_ops_.emplace(\"Prod\", EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"QuantizeAndDequantizeV2\",\n                           quantize_and_dequantize_v2_cost);\n  elementwise_ops_.emplace(\"QuantizeAndDequantizeV4\",\n                           quantize_and_dequantize_v2_cost);\n  elementwise_ops_.emplace(\"QuantizedSigmoid\",\n                           EIGEN_COST(scalar_logistic_op<float>));\n  elementwise_ops_.emplace(\"QuantizeV2\", quantize_v2_cost);\n  elementwise_ops_.emplace(\"Reciprocal\", EIGEN_COST(scalar_inverse_op<float>));\n  elementwise_ops_.emplace(\"Relu\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Relu6\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Rint\", 1);\n  elementwise_ops_.emplace(\"Round\", EIGEN_COST(scalar_round_op<float>));\n  elementwise_ops_.emplace(\"Rsqrt\", EIGEN_COST(scalar_rsqrt_op<float>));\n  elementwise_ops_.emplace(\"Sigmoid\", EIGEN_COST(scalar_logistic_op<float>));\n  elementwise_ops_.emplace(\"Sign\", EIGEN_COST(scalar_sign_op<float>));\n  elementwise_ops_.emplace(\"Sin\", EIGEN_COST(scalar_sin_op<float>));\n  elementwise_ops_.emplace(\"Sqrt\", EIGEN_COST(scalar_sqrt_op<float>));\n  elementwise_ops_.emplace(\"Square\", EIGEN_COST(scalar_square_op<float>));\n  elementwise_ops_.emplace(\"Sum\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"Tan\", EIGEN_COST(scalar_tan_op<float>));\n  elementwise_ops_.emplace(\"Tanh\", EIGEN_COST(scalar_tanh_op<float>));\n  elementwise_ops_.emplace(\"TopKV2\", EIGEN_COST(scalar_max_op<float>));\n  // Binary ops alphabetically sorted\n  elementwise_ops_.emplace(\"Add\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"AddV2\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"ApproximateEqual\", 1);\n  elementwise_ops_.emplace(\"BiasAdd\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"QuantizedBiasAdd\",\n                           EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"Div\", EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"Equal\", 1);\n  elementwise_ops_.emplace(\"FloorDiv\", EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"FloorMod\", EIGEN_COST(scalar_mod_op<float>));\n  elementwise_ops_.emplace(\"Greater\", 1);\n  elementwise_ops_.emplace(\"GreaterEqual\", 1);\n  elementwise_ops_.emplace(\"Less\", 1);\n  elementwise_ops_.emplace(\"LessEqual\", 1);\n  elementwise_ops_.emplace(\"LogicalAnd\", EIGEN_COST(scalar_boolean_and_op));\n  elementwise_ops_.emplace(\"LogicalNot\", 1);\n  elementwise_ops_.emplace(\"LogicalOr\", EIGEN_COST(scalar_boolean_or_op));\n  elementwise_ops_.emplace(\"Maximum\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Minimum\", EIGEN_COST(scalar_min_op<float>));\n  elementwise_ops_.emplace(\"Mod\", EIGEN_COST(scalar_mod_op<float>));\n  elementwise_ops_.emplace(\"Mul\", EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"NotEqual\", 1);\n  elementwise_ops_.emplace(\"QuantizedAdd\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"QuantizedMul\",\n                           EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"RealDiv\", EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"ReluGrad\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Select\", EIGEN_COST(scalar_boolean_or_op));\n  elementwise_ops_.emplace(\"SelectV2\", EIGEN_COST(scalar_boolean_or_op));\n  elementwise_ops_.emplace(\"SquaredDifference\",\n                           EIGEN_COST(scalar_square_op<float>) +\n                               EIGEN_COST(scalar_difference_op<float>));\n  elementwise_ops_.emplace(\"Sub\", EIGEN_COST(scalar_difference_op<float>));\n  elementwise_ops_.emplace(\"TruncateDiv\",\n                           EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"TruncateMod\", EIGEN_COST(scalar_mod_op<float>));\n  elementwise_ops_.emplace(\"Where\", 1);\n\n#undef EIGEN_COST\n\n  // By default, use sum of memory_time and compute_time for execution_time.\n  compute_memory_overlap_ = false;\n}\n\nCosts OpLevelCostEstimator::PredictCosts(const OpContext& op_context) const {\n  Costs costs;\n  NodeCosts node_costs;\n  if (PredictNodeCosts(op_context, &node_costs).ok()) {\n    if (node_costs.has_costs) {\n      return node_costs.costs;\n    }\n    // Convert NodeCosts to Costs.\n    if (node_costs.minimum_cost_op) {\n      // Override to minimum cost; Note that some ops with minimum cost may have\n      // non-typical device (e.g., channel for _Send), which may fail with\n      // GetDeviceInfo(), called from PredictOpCountBasedCost(). Make sure we\n      // directly set minimum values to Costs here, not calling\n      // PredictOpCountBasedCost().\n      costs.compute_time = kMinComputeTime;\n      costs.execution_time = kMinComputeTime;\n      costs.memory_time = 0;\n      costs.intermediate_memory_time = 0;\n      costs.intermediate_memory_read_time = 0;\n      costs.intermediate_memory_write_time = 0;\n    } else {\n      // Convert NodeCosts to Costs.\n      costs = PredictOpCountBasedCost(\n          node_costs.num_compute_ops, node_costs.num_total_read_bytes(),\n          node_costs.num_total_write_bytes(), op_context.op_info);\n    }\n    VLOG(1) << \"Operation \" << op_context.op_info.op() << \" takes \"\n            << costs.execution_time.count() << \" ns.\";\n    // Copy additional stats from NodeCosts to Costs.\n    costs.max_memory = node_costs.max_memory;\n    costs.persistent_memory = node_costs.persistent_memory;\n    costs.temporary_memory = node_costs.temporary_memory;\n    costs.inaccurate = node_costs.inaccurate;\n    costs.num_ops_with_unknown_shapes =\n        node_costs.num_nodes_with_unknown_shapes;\n    costs.num_ops_total = node_costs.num_nodes;\n    return costs;\n  }\n  // Errors during node cost estimate.\n  LOG(WARNING) << \"Error in PredictCost() for the op: \"\n               << op_context.op_info.ShortDebugString();\n  costs = Costs::ZeroCosts(/*inaccurate=*/true);\n  costs.num_ops_with_unknown_shapes = node_costs.num_nodes_with_unknown_shapes;\n  return costs;\n}\n\nStatus OpLevelCostEstimator::PredictNodeCosts(const OpContext& op_context,\n                                              NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  auto it = device_cost_impl_.find(op_info.op());\n  if (it != device_cost_impl_.end()) {\n    std::function<Status(const OpContext&, NodeCosts*)> estimator = it->second;\n    return estimator(op_context, node_costs);\n  }\n\n  if (persistent_ops_.find(op_info.op()) != persistent_ops_.end()) {\n    return PredictVariable(op_context, node_costs);\n  }\n\n  if (elementwise_ops_.find(op_info.op()) != elementwise_ops_.end()) {\n    return PredictCwiseOp(op_context, node_costs);\n  }\n\n  VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n\n  node_costs->num_nodes_with_unknown_op_type = 1;\n  return PredictCostOfAnUnknownOp(op_context, node_costs);\n}\n\n// This method assumes a typical system composed of CPUs and GPUs, connected\n// through PCIe. To define device info more precisely, override this method.\nDeviceInfo OpLevelCostEstimator::GetDeviceInfo(\n    const DeviceProperties& device) const {\n  double gflops = -1;\n  double gb_per_sec = -1;\n\n  if (device.type() == \"CPU\") {\n    // Check if vector instructions are available, and refine performance\n    // prediction based on this.\n    // Frequencies are stored in MHz in the DeviceProperties.\n    gflops = device.num_cores() * device.frequency() * 1e-3;\n    if (gb_per_sec < 0) {\n      if (device.bandwidth() > 0) {\n        gb_per_sec = device.bandwidth() / 1e6;\n      } else {\n        gb_per_sec = 32;\n      }\n    }\n  } else if (device.type() == \"GPU\") {\n    const auto& device_env = device.environment();\n    auto it = device_env.find(\"architecture\");\n    if (it != device_env.end()) {\n      const std::string architecture = device_env.at(\"architecture\");\n      int cores_per_multiprocessor;\n      if (architecture < \"3\") {\n        // Fermi\n        cores_per_multiprocessor = 32;\n      } else if (architecture < \"4\") {\n        // Kepler\n        cores_per_multiprocessor = 192;\n      } else if (architecture < \"6\") {\n        // Maxwell\n        cores_per_multiprocessor = 128;\n      } else {\n        // Pascal (compute capability version 6) and Volta (compute capability\n        // version 7)\n        cores_per_multiprocessor = 64;\n      }\n      gflops = device.num_cores() * device.frequency() * 1e-3 *\n               cores_per_multiprocessor * kOpsPerMac;\n      if (device.bandwidth() > 0) {\n        gb_per_sec = device.bandwidth() / 1e6;\n      } else {\n        gb_per_sec = 100;\n      }\n    } else {\n      // Architecture is not available (ex: pluggable device), return default\n      // value.\n      gflops = 100;     // Dummy value;\n      gb_per_sec = 12;  // default PCIe x16 gen3.\n    }\n  } else {\n    LOG_EVERY_N(WARNING, 1000) << \"Unknown device type: \" << device.type()\n                               << \", assuming PCIe between CPU and GPU.\";\n    gflops = 1;  // Dummy value; data transfer ops would not have compute ops.\n    gb_per_sec = 12;  // default PCIe x16 gen3.\n  }\n  VLOG(1) << \"Device: \" << device.type() << \" gflops: \" << gflops\n          << \" gb_per_sec: \" << gb_per_sec;\n\n  return DeviceInfo(gflops, gb_per_sec);\n}\n\nStatus OpLevelCostEstimator::PredictCwiseOp(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  // For element-wise operations, op count is the element count of any input. We\n  // use the count for the largest input here to be more robust in case that the\n  // shape is unknown or partially known for other input.\n  int64_t op_count = CalculateLargestInputCount(op_info, &found_unknown_shapes);\n  // If output shape is available, try to use the element count calculated from\n  // that.\n  if (op_info.outputs_size() > 0) {\n    op_count = std::max(\n        op_count,\n        CalculateTensorElementCount(op_info.outputs(0), &found_unknown_shapes));\n  }\n  // Calculate the output shape possibly resulting from broadcasting.\n  if (op_info.inputs_size() >= 2) {\n    op_count = std::max(op_count, CwiseOutputElementCount(op_info));\n  }\n\n  int op_cost = 1;\n  auto it = elementwise_ops_.find(op_info.op());\n  if (it != elementwise_ops_.end()) {\n    op_cost = it->second;\n  } else {\n    return errors::InvalidArgument(\"Not a cwise op: \", op_info.op());\n  }\n\n  return PredictDefaultNodeCosts(op_count * op_cost, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictCostOfAnUnknownOp(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  // Don't assume the operation is cwise, return cost based on input/output size\n  // and admit that it is inaccurate...\n  bool found_unknown_shapes = false;\n  node_costs->inaccurate = true;\n  return PredictDefaultNodeCosts(0, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nCosts OpLevelCostEstimator::PredictOpCountBasedCost(\n    double operations, const OpInfo& op_info) const {\n  bool unknown_shapes = false;\n  const double input_size = CalculateInputSize(op_info, &unknown_shapes);\n  const double output_size = CalculateOutputSize(op_info, &unknown_shapes);\n  Costs costs =\n      PredictOpCountBasedCost(operations, input_size, output_size, op_info);\n  costs.inaccurate = unknown_shapes;\n  costs.num_ops_with_unknown_shapes = unknown_shapes;\n  costs.max_memory = output_size;\n  return costs;\n}\n\nCosts OpLevelCostEstimator::PredictOpCountBasedCost(\n    double operations, double input_io_bytes, double output_io_bytes,\n    const OpInfo& op_info) const {\n  double total_io_bytes = input_io_bytes + output_io_bytes;\n  const DeviceInfo device_info = GetDeviceInfo(op_info.device());\n  if (device_info.gigaops <= 0 || device_info.gb_per_sec <= 0 ||\n      device_info.intermediate_read_gb_per_sec <= 0 ||\n      device_info.intermediate_write_gb_per_sec <= 0) {\n    VLOG(1) << \"BAD DEVICE. Op:\" << op_info.op()\n            << \" device type:\" << op_info.device().type()\n            << \" device model:\" << op_info.device().model();\n  }\n\n  Costs::NanoSeconds compute_cost(std::ceil(operations / device_info.gigaops));\n  VLOG(1) << \"Op:\" << op_info.op() << \" GOps:\" << operations / 1e9\n          << \" Compute Time (ns):\" << compute_cost.count();\n\n  Costs::NanoSeconds memory_cost(\n      std::ceil(total_io_bytes / device_info.gb_per_sec));\n  VLOG(1) << \"Op:\" << op_info.op() << \" Size (KB):\" << (total_io_bytes) / 1e3\n          << \" Memory Time (ns):\" << memory_cost.count();\n\n  // Check if bytes > 0.  If it's not and the bandwidth is set to infinity\n  // then the result would be undefined.\n  double intermediate_read_time =\n      (input_io_bytes > 0)\n          ? std::ceil(input_io_bytes / device_info.intermediate_read_gb_per_sec)\n          : 0;\n\n  double intermediate_write_time =\n      (output_io_bytes > 0)\n          ? std::ceil(output_io_bytes /\n                      device_info.intermediate_write_gb_per_sec)\n          : 0;\n\n  Costs::NanoSeconds intermediate_memory_cost =\n      compute_memory_overlap_\n          ? std::max(intermediate_read_time, intermediate_write_time)\n          : (intermediate_read_time + intermediate_write_time);\n  VLOG(1) << \"Op:\" << op_info.op() << \" Size (KB):\" << (total_io_bytes) / 1e3\n          << \" Intermediate Memory Time (ns):\"\n          << intermediate_memory_cost.count();\n\n  Costs costs = Costs::ZeroCosts();\n  costs.compute_time = compute_cost;\n  costs.memory_time = memory_cost;\n  costs.intermediate_memory_time = intermediate_memory_cost;\n  costs.intermediate_memory_read_time =\n      Costs::NanoSeconds(intermediate_read_time);\n  costs.intermediate_memory_write_time =\n      Costs::NanoSeconds(intermediate_write_time);\n  CombineCostsAndUpdateExecutionTime(compute_memory_overlap_, &costs);\n  return costs;\n}\n\nint64_t OpLevelCostEstimator::CountConv2DOperations(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  return CountConv2DOperations(op_info, nullptr, found_unknown_shapes);\n}\n\n// Helper to translate the positional arguments into named fields.\n/* static */\nOpLevelCostEstimator::ConvolutionDimensions\nOpLevelCostEstimator::ConvolutionDimensionsFromInputs(\n    const TensorShapeProto& original_image_shape,\n    const TensorShapeProto& original_filter_shape, const OpInfo& op_info,\n    bool* found_unknown_shapes) {\n  VLOG(2) << \"op features: \" << op_info.DebugString();\n  VLOG(2) << \"Original image shape: \" << original_image_shape.DebugString();\n  VLOG(2) << \"Original filter shape: \" << original_filter_shape.DebugString();\n\n  int x_index, y_index, major_channel_index, minor_channel_index = -1;\n  const std::string& data_format = GetDataFormat(op_info);\n  if (data_format == \"NCHW\") {\n    major_channel_index = 1;\n    y_index = 2;\n    x_index = 3;\n  } else if (data_format == \"NCHW_VECT_C\") {\n    // Use NCHW_VECT_C\n    minor_channel_index = 1;\n    y_index = 2;\n    x_index = 3;\n    major_channel_index = 4;\n  } else {\n    // Use NHWC.\n    y_index = 1;\n    x_index = 2;\n    major_channel_index = 3;\n  }\n  const std::string& filter_format = GetFilterFormat(op_info);\n  int filter_x_index, filter_y_index, in_major_channel_index, out_channel_index,\n      in_minor_channel_index = -1;\n  if (filter_format == \"HWIO\") {\n    filter_y_index = 0;\n    filter_x_index = 1;\n    in_major_channel_index = 2;\n    out_channel_index = 3;\n  } else if (filter_format == \"OIHW_VECT_I\") {\n    out_channel_index = 0;\n    in_minor_channel_index = 1;\n    filter_y_index = 2;\n    filter_x_index = 3;\n    in_major_channel_index = 4;\n  } else {\n    // Use OIHW\n    out_channel_index = 0;\n    in_major_channel_index = 1;\n    filter_y_index = 2;\n    filter_x_index = 3;\n  }\n\n  auto image_shape = MaybeGetMinimumShape(original_image_shape,\n                                          minor_channel_index >= 0 ? 5 : 4,\n                                          found_unknown_shapes);\n  auto filter_shape = MaybeGetMinimumShape(original_filter_shape,\n                                           in_minor_channel_index >= 0 ? 5 : 4,\n                                           found_unknown_shapes);\n  VLOG(2) << \"Image shape: \" << image_shape.DebugString();\n  VLOG(2) << \"Filter shape: \" << filter_shape.DebugString();\n\n  int64_t batch = image_shape.dim(0).size();\n  int64_t ix = image_shape.dim(x_index).size();\n  int64_t iy = image_shape.dim(y_index).size();\n  int64_t iz = minor_channel_index >= 0\n                   ? image_shape.dim(minor_channel_index).size() *\n                         image_shape.dim(major_channel_index).size()\n                   : image_shape.dim(major_channel_index).size();\n  int64_t kx = filter_shape.dim(filter_x_index).size();\n  int64_t ky = filter_shape.dim(filter_y_index).size();\n  int64_t kz = in_minor_channel_index >= 0\n                   ? filter_shape.dim(in_major_channel_index).size() *\n                         filter_shape.dim(in_minor_channel_index).size()\n                   : filter_shape.dim(in_major_channel_index).size();\n  std::vector<int64_t> strides = GetStrides(op_info);\n  const auto padding = GetPadding(op_info);\n  int64_t sx = strides[x_index];\n  int64_t sy = strides[y_index];\n  int64_t ox = GetOutputSize(ix, kx, sx, padding);\n  int64_t oy = GetOutputSize(iy, ky, sy, padding);\n  int64_t oz = filter_shape.dim(out_channel_index).size();\n  // Only check equality when both sizes are known (in other words, when\n  // neither is set to a minimum dimension size of 1).\n  if (iz != 1 && kz != 1) {\n    DCHECK_EQ(iz % kz, 0) << \"Input channel \" << iz\n                          << \" is not a multiple of filter channel \" << kz\n                          << \".\";\n    if (iz % kz) {\n      *found_unknown_shapes = true;\n    }\n  } else {\n    iz = kz = std::max<int64_t>(iz, kz);\n  }\n  OpLevelCostEstimator::ConvolutionDimensions conv_dims = {\n      batch, ix, iy, iz, kx, ky, kz, oz, ox, oy, sx, sy, padding};\n\n  VLOG(1) << \"Batch Size:\" << batch;\n  VLOG(1) << \"Image Dims:\" << ix << \",\" << iy;\n  VLOG(1) << \"Input Depth:\" << iz;\n  VLOG(1) << \"Kernel Dims:\" << kx << \",\" << ky;\n  VLOG(1) << \"Kernel Depth:\" << kz;\n  VLOG(1) << \"Output Dims:\" << ox << \",\" << oy;\n  VLOG(1) << \"Output Depth:\" << oz;\n  VLOG(1) << \"Strides:\" << sx << \",\" << sy;\n  VLOG(1) << \"Padding:\" << (padding == Padding::VALID ? \"VALID\" : \"SAME\");\n  return conv_dims;\n}\n\nint64_t OpLevelCostEstimator::CountConv2DOperations(\n    const OpInfo& op_info, ConvolutionDimensions* conv_info,\n    bool* found_unknown_shapes) {\n  DCHECK(op_info.op() == kConv2d || op_info.op() == kDepthwiseConv2dNative)\n      << \"Invalid Operation: not Conv2D nor DepthwiseConv2dNative\";\n\n  if (op_info.inputs_size() < 2) {  // Unexpected inputs.\n    *found_unknown_shapes = true;\n    return 0;\n  }\n\n  ConvolutionDimensions conv_dims = ConvolutionDimensionsFromInputs(\n      op_info.inputs(0).shape(), op_info.inputs(1).shape(), op_info,\n      found_unknown_shapes);\n\n  //  in DepthwiseConv2dNative conv_dims.oz is actually the channel depth\n  //  multiplier; The effective output channel depth oz_effective is\n  //  conv_dims.iz * conv_dims.oz. thus # ops = N x H x W x oz_effective x 2RS.\n  //  Compare to Conv2D where # ops =  N x H x W x kz x oz x 2RS,\n  //  oz = oz_effective,  then Conv2D_ops / Depthwise_conv2d_native_ops = kz.\n  int64_t ops = conv_dims.batch;\n  ops *= conv_dims.ox * conv_dims.oy;\n  ops *= conv_dims.kx * conv_dims.ky;\n  if (op_info.op() == kConv2d) {\n    ops *= conv_dims.kz * conv_dims.oz;\n  } else {\n    // To ensure output tensor dims to be correct for DepthwiseConv2DNative,\n    // although ops are the same as Conv2D.\n    conv_dims.oz *= conv_dims.iz;\n    ops *= conv_dims.oz;\n  }\n  ops *= kOpsPerMac;\n\n  if (conv_info != nullptr) {\n    *conv_info = conv_dims;\n  }\n  return ops;\n}\n\nint64_t OpLevelCostEstimator::CountMatMulOperations(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  return CountMatMulOperations(op_info, nullptr, found_unknown_shapes);\n}\n\n// TODO(nishantpatil): Create separate estimator for Sparse Matmul\nint64_t OpLevelCostEstimator::CountMatMulOperations(\n    const OpInfo& op_info, MatMulDimensions* mat_mul,\n    bool* found_unknown_shapes) {\n  double ops = 0;\n\n  if (op_info.inputs_size() < 2) {\n    LOG(ERROR) << \"Need 2 inputs but got \" << op_info.inputs_size();\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return 0;\n  }\n\n  auto& a_matrix = op_info.inputs(0);\n  auto& b_matrix = op_info.inputs(1);\n\n  bool transpose_a = false;\n  bool transpose_b = false;\n\n  double m_dim, n_dim, k_dim, k_dim_b = 0;\n\n  for (const auto& item : op_info.attr()) {\n    VLOG(1) << \"Key:\" << item.first\n            << \" Value:\" << SummarizeAttrValue(item.second);\n    if (item.first == \"transpose_a\" && item.second.b() == true)\n      transpose_a = true;\n    if (item.first == \"transpose_b\" && item.second.b() == true)\n      transpose_b = true;\n  }\n  VLOG(1) << \"transpose_a:\" << transpose_a;\n  VLOG(1) << \"transpose_b:\" << transpose_b;\n  auto a_matrix_shape =\n      MaybeGetMinimumShape(a_matrix.shape(), 2, found_unknown_shapes);\n  auto b_matrix_shape =\n      MaybeGetMinimumShape(b_matrix.shape(), 2, found_unknown_shapes);\n  if (transpose_a) {\n    m_dim = a_matrix_shape.dim(1).size();\n    k_dim = a_matrix_shape.dim(0).size();\n  } else {\n    m_dim = a_matrix_shape.dim(0).size();\n    k_dim = a_matrix_shape.dim(1).size();\n  }\n  if (transpose_b) {\n    k_dim_b = b_matrix_shape.dim(1).size();\n    n_dim = b_matrix_shape.dim(0).size();\n  } else {\n    k_dim_b = b_matrix_shape.dim(0).size();\n    n_dim = b_matrix_shape.dim(1).size();\n  }\n\n  VLOG(1) << \"M, N, K: \" << m_dim << \",\" << n_dim << \",\" << k_dim;\n  // Only check equality when both sizes are known (in other words, when\n  // neither is set to a minimum dimension size of 1).\n  if (k_dim_b != 1 && k_dim != 1 && k_dim_b != k_dim) {\n    LOG(ERROR) << \"Incompatible Matrix dimensions\";\n    return ops;\n  } else {\n    // One of k_dim and k_dim_b might be 1 (minimum dimension size).\n    k_dim = std::max(k_dim, k_dim_b);\n  }\n\n  ops = m_dim * n_dim * k_dim * 2;\n  VLOG(1) << \"Operations for Matmul: \" << ops;\n\n  if (mat_mul != nullptr) {\n    mat_mul->m = m_dim;\n    mat_mul->n = n_dim;\n    mat_mul->k = k_dim;\n  }\n  return ops;\n}\n\nbool OpLevelCostEstimator::GenerateBatchMatmulContextFromEinsum(\n    const OpContext& einsum_context, OpContext* batch_matmul_context,\n    bool* found_unknown_shapes) const {\n  // This auxiliary function transforms an einsum OpContext into its equivalent\n  // Batch Matmul OpContext. The function returns a boolean, which determines\n  // whether it was successful in generating the output OpContext or not.\n\n  // Einsum computes a generalized contraction between tensors of arbitrary\n  // dimension as defined by the equation written in the Einstein summation\n  // convention. The number of tensors in the computation and the number of\n  // contractions can be arbitrarily long. The current model only contemplates\n  // Einsum equations, which can be translated into a single BatchMatMul\n  // operation. Einsum operations with more than two operands are not currently\n  // supported. Subscripts where an axis appears more than once for a single\n  // input and ellipsis are currently also excluded. See:\n  // https://www.tensorflow.org/api_docs/python/tf/einsum\n  // We distinguish four kinds of dimensions, depending on their placement in\n  // the equation:\n  // + B: Batch dimensions: Dimensions which appear in both operands and RHS.\n  // + K: Contracting dimensions: These appear in both inputs but not RHS.\n  // + M: Operand A dimensions: These appear in the first operand and the RHS.\n  // + N: Operand B dimensions: These appear in the second operand and the RHS.\n  // Then, the operation to estimate is BatchMatMul([B,M,K],[B,K,N])\n\n  if (batch_matmul_context == nullptr) {\n    VLOG(1) << \"Output context should not be a nullptr.\";\n    return false;\n  }\n  if (!IsEinsumCorrectlyFormed(einsum_context)) return false;\n  const auto& op_info = einsum_context.op_info;\n  std::vector<std::string> equation_split =\n      absl::StrSplit(op_info.attr().find(\"equation\")->second.s(), \"->\");\n  std::vector<absl::string_view> input_split =\n      absl::StrSplit(equation_split[0], ',');\n  const auto& a_input = op_info.inputs(0);\n  const auto& b_input = op_info.inputs(1);\n  absl::string_view rhs_str = equation_split[1];\n  absl::string_view a_input_str = input_split[0];\n  absl::string_view b_input_str = input_split[1];\n\n  constexpr int kMatrixRank = 2;\n\n  bool a_input_shape_unknown = false;\n  bool b_input_shape_unknown = false;\n\n  TensorShapeProto a_input_shape = MaybeGetMinimumShape(\n      a_input.shape(), std::max(kMatrixRank, a_input.shape().dim_size()),\n      &a_input_shape_unknown);\n  TensorShapeProto b_input_shape = MaybeGetMinimumShape(\n      b_input.shape(), std::max(kMatrixRank, b_input.shape().dim_size()),\n      &b_input_shape_unknown);\n\n  *found_unknown_shapes = a_input_shape_unknown || b_input_shape_unknown ||\n                          (a_input.shape().dim_size() < kMatrixRank) ||\n                          (b_input.shape().dim_size() < kMatrixRank);\n\n  OpInfo batch_matmul_op_info = op_info;\n  batch_matmul_op_info.mutable_inputs()->Clear();\n  batch_matmul_op_info.set_op(\"BatchMatMul\");\n\n  AttrValue transpose_attribute;\n  transpose_attribute.set_b(false);\n  (*batch_matmul_op_info.mutable_attr())[\"transpose_a\"] = transpose_attribute;\n  (*batch_matmul_op_info.mutable_attr())[\"transpose_b\"] = transpose_attribute;\n\n  OpInfo::TensorProperties* a_matrix = batch_matmul_op_info.add_inputs();\n  TensorShapeProto* a_matrix_shape = a_matrix->mutable_shape();\n  a_matrix->set_dtype(a_input.dtype());\n\n  OpInfo::TensorProperties* b_matrix = batch_matmul_op_info.add_inputs();\n  b_matrix->set_dtype(b_input.dtype());\n  TensorShapeProto* b_matrix_shape = b_matrix->mutable_shape();\n\n  TensorShapeProto_Dim m_dim;\n  TensorShapeProto_Dim n_dim;\n  TensorShapeProto_Dim k_dim;\n\n  m_dim.set_size(1);\n  n_dim.set_size(1);\n  k_dim.set_size(1);\n\n  for (int i_idx = 0, a_input_str_size = a_input_str.size();\n       i_idx < a_input_str_size; ++i_idx) {\n    if (b_input_str.find(a_input_str[i_idx]) == std::string::npos) {\n      if (rhs_str.find(a_input_str[i_idx]) == std::string::npos) {\n        VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n        return false;\n      }\n\n      m_dim.set_size(m_dim.size() * a_input_shape.dim(i_idx).size());\n      continue;\n    } else if (rhs_str.find(a_input_str[i_idx]) == std::string::npos) {\n      // The dimension does not appear in the RHS, therefore it is a contracting\n      // dimension.\n      k_dim.set_size(k_dim.size() * a_input_shape.dim(i_idx).size());\n      continue;\n    }\n    // It appears in both input operands, therefore we place it as an outer\n    // dimension for the Batch Matmul.\n    *(a_matrix_shape->add_dim()) = a_input_shape.dim(i_idx);\n    *(b_matrix_shape->add_dim()) = a_input_shape.dim(i_idx);\n  }\n  for (int i_idx = 0, b_input_str_size = b_input_str.size();\n       i_idx < b_input_str_size; ++i_idx) {\n    if (a_input_str.find(b_input_str[i_idx]) == std::string::npos) {\n      if (rhs_str.find(b_input_str[i_idx]) == std::string::npos) {\n        VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n        return false;\n      }\n      n_dim.set_size(n_dim.size() * b_input_shape.dim(i_idx).size());\n    }\n  }\n\n  // The two inner-most dimensions of the Batch Matmul are added.\n  *(a_matrix_shape->add_dim()) = m_dim;\n  *(a_matrix_shape->add_dim()) = k_dim;\n  *(b_matrix_shape->add_dim()) = k_dim;\n  *(b_matrix_shape->add_dim()) = n_dim;\n\n  *batch_matmul_context = einsum_context;\n  batch_matmul_context->op_info = batch_matmul_op_info;\n  return true;\n}\n\nint64_t OpLevelCostEstimator::CountBatchMatMulOperations(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  return CountBatchMatMulOperations(op_info, nullptr, found_unknown_shapes);\n}\n\nint64_t OpLevelCostEstimator::CountBatchMatMulOperations(\n    const OpInfo& op_info, BatchMatMulDimensions* batch_mat_mul,\n    bool* found_unknown_shapes) {\n  if (op_info.op() != kBatchMatMul && op_info.op() != kBatchMatMulV2) {\n    LOG(ERROR) << \"Invalid Operation: \" << op_info.op();\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return 0;\n  }\n  if (op_info.inputs_size() != 2) {\n    LOG(ERROR) << \"Expected 2 inputs but got \" << op_info.inputs_size();\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return 0;\n  }\n\n  double ops = 0;\n  const auto& a_input = op_info.inputs(0);\n  const auto& b_input = op_info.inputs(1);\n\n  // BatchMatMul requires inputs of at least matrix shape (rank 2).\n  // The two most minor dimensions of each input are matrices that\n  // need to be multiplied together. The other dimensions determine\n  // the number of such MatMuls.  For example, if the BatchMatMul has\n  // inputs of shape:\n  //   a_input_shape = [2, 3, 4, 5]\n  //   b_input_shape = [2, 3, 5, 6]\n  // then there are 2*3 = 6 MatMuls of dimensions m = 4, k = 5, n = 6\n  // in this BatchMatMul.\n  const int matrix_rank = 2;\n\n  bool a_input_shape_unknown = false;\n  bool b_input_shape_unknown = false;\n\n  TensorShapeProto a_input_shape = MaybeGetMinimumShape(\n      a_input.shape(), std::max(matrix_rank, a_input.shape().dim_size()),\n      &a_input_shape_unknown);\n  TensorShapeProto b_input_shape = MaybeGetMinimumShape(\n      b_input.shape(), std::max(matrix_rank, b_input.shape().dim_size()),\n      &b_input_shape_unknown);\n\n  *found_unknown_shapes = a_input_shape_unknown || b_input_shape_unknown ||\n                          (a_input.shape().dim_size() < matrix_rank) ||\n                          (b_input.shape().dim_size() < matrix_rank);\n\n  // Compute the number of matmuls as the max indicated at each dimension\n  // by either input. Note that the shapes do not have to have\n  // the same rank due to incompleteness.\n  TensorShapeProto* bigger_rank_shape = &a_input_shape;\n  TensorShapeProto* smaller_rank_shape = &b_input_shape;\n  if (b_input_shape.dim_size() > a_input_shape.dim_size()) {\n    bigger_rank_shape = &b_input_shape;\n    smaller_rank_shape = &a_input_shape;\n  }\n  int num_matmuls = 1;\n  for (int b_i = 0,\n           s_i = smaller_rank_shape->dim_size() - bigger_rank_shape->dim_size();\n       b_i < bigger_rank_shape->dim_size() - matrix_rank; ++b_i, ++s_i) {\n    int b_dim = bigger_rank_shape->dim(b_i).size();\n    int s_dim = 1;\n    if (s_i >= 0) {\n      s_dim = smaller_rank_shape->dim(s_i).size();\n    }\n    if (batch_mat_mul != nullptr) {\n      batch_mat_mul->batch_dims.push_back(s_dim);\n    }\n    num_matmuls *= std::max(b_dim, s_dim);\n  }\n\n  // Build the MatMul. Note that values are ignored here since we are just\n  // counting ops (e.g. only shapes matter).\n  OpInfo matmul_op_info;\n  matmul_op_info.set_op(\"MatMul\");\n\n  AttrValue transpose_a;\n  transpose_a.set_b(false);\n  if (op_info.attr().find(\"adj_x\") != op_info.attr().end()) {\n    transpose_a.set_b(op_info.attr().at(\"adj_x\").b());\n  }\n  (*matmul_op_info.mutable_attr())[\"transpose_a\"] = transpose_a;\n\n  AttrValue transpose_b;\n  transpose_b.set_b(false);\n  if (op_info.attr().find(\"adj_y\") != op_info.attr().end()) {\n    transpose_b.set_b(op_info.attr().at(\"adj_y\").b());\n  }\n  (*matmul_op_info.mutable_attr())[\"transpose_b\"] = transpose_b;\n\n  OpInfo::TensorProperties* a_matrix = matmul_op_info.add_inputs();\n  a_matrix->set_dtype(a_input.dtype());\n  TensorShapeProto* a_matrix_shape = a_matrix->mutable_shape();\n  for (int i = std::max(0, a_input_shape.dim_size() - matrix_rank);\n       i < a_input_shape.dim_size(); ++i) {\n    *(a_matrix_shape->add_dim()) = a_input_shape.dim(i);\n  }\n\n  OpInfo::TensorProperties* b_matrix = matmul_op_info.add_inputs();\n  b_matrix->set_dtype(b_input.dtype());\n  TensorShapeProto* b_matrix_shape = b_matrix->mutable_shape();\n  for (int i = std::max(0, b_input_shape.dim_size() - matrix_rank);\n       i < b_input_shape.dim_size(); ++i) {\n    *(b_matrix_shape->add_dim()) = b_input_shape.dim(i);\n  }\n  if (batch_mat_mul != nullptr) {\n    batch_mat_mul->matmul_dims.m = (transpose_a.b())\n                                       ? a_matrix_shape->dim(1).size()\n                                       : a_matrix_shape->dim(0).size();\n    batch_mat_mul->matmul_dims.k = (transpose_a.b())\n                                       ? a_matrix_shape->dim(0).size()\n                                       : a_matrix_shape->dim(1).size();\n    batch_mat_mul->matmul_dims.n = (transpose_b.b())\n                                       ? b_matrix_shape->dim(0).size()\n                                       : b_matrix_shape->dim(1).size();\n  }\n\n  for (int i = 0; i < num_matmuls; ++i) {\n    bool matmul_unknown_shapes = false;\n    ops += CountMatMulOperations(matmul_op_info, &matmul_unknown_shapes);\n    *found_unknown_shapes |= matmul_unknown_shapes;\n  }\n  return ops;\n}\n\nbool GetTensorShapeProtoFromTensorProto(const TensorProto& tensor_proto,\n                                        TensorShapeProto* tensor_shape_proto) {\n  tensor_shape_proto->Clear();\n  // First convert TensorProto into Tensor class so that it correctly parses\n  // data values within TensorProto (whether it's in int_val, int64_val,\n  // tensor_content, or anything.\n  Tensor tensor(tensor_proto.dtype());\n  if (!tensor.FromProto(tensor_proto)) {\n    LOG(WARNING) << \"GetTensorShapeProtoFromTensorProto() -- \"\n                 << \"failed to parse TensorProto: \"\n                 << tensor_proto.DebugString();\n    return false;\n  }\n  if (tensor.dims() != 1) {\n    LOG(WARNING) << \"GetTensorShapeProtoFromTensorProto() -- \"\n                 << \"tensor is not 1D: \" << tensor.dims();\n    return false;\n  }\n  // Then, convert it back to TensorProto using AsProtoField, which makes sure\n  // the data is in int_val, int64_val, or such repeated data fields, not in\n  // tensor_content.\n  TensorProto temp_tensor;\n  tensor.AsProtoField(&temp_tensor);\n\n#define TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(type)        \\\n  do {                                                   \\\n    for (const auto& value : temp_tensor.type##_val()) { \\\n      tensor_shape_proto->add_dim()->set_size(value);    \\\n    }                                                    \\\n  } while (0)\n\n  if (tensor.dtype() == DT_INT32 || tensor.dtype() == DT_INT16 ||\n      tensor.dtype() == DT_INT8 || tensor.dtype() == DT_UINT8) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(int);\n  } else if (tensor.dtype() == DT_INT64) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(int64);\n  } else if (tensor.dtype() == DT_UINT32) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(uint32);\n  } else if (tensor.dtype() == DT_UINT64) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(uint64);\n  } else {\n    LOG(WARNING) << \"GetTensorShapeProtoFromTensorProto() -- \"\n                 << \"Unsupported dtype: \" << tensor.dtype();\n    return false;\n  }\n#undef TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO\n\n  return true;\n}\n\n// TODO(cliffy): Dedup this method and CountConv2DBackpropFilterOperations.\nint64_t OpLevelCostEstimator::CountConv2DBackpropInputOperations(\n    const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n    bool* found_unknown_shapes) {\n  int64_t ops = 0;\n\n  DCHECK(op_info.op() == kConv2dBackpropInput ||\n         op_info.op() == kDepthwiseConv2dNativeBackpropInput)\n      << \"Invalid Operation: not kConv2dBackpropInput nor\"\n         \"kDepthwiseConv2dNativeBackpropInput\";\n\n  if (op_info.inputs_size() < 2) {\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return ops;\n  }\n\n  TensorShapeProto input_shape;\n  bool shape_found = false;\n  if (op_info.inputs(0).has_value()) {\n    const TensorProto& value = op_info.inputs(0).value();\n    shape_found = GetTensorShapeProtoFromTensorProto(value, &input_shape);\n  }\n  if (!shape_found && op_info.outputs_size() == 1) {\n    input_shape = op_info.outputs(0).shape();\n    shape_found = true;\n  }\n  if (!shape_found) {\n    // Set the minimum filter size that's feasible.\n    input_shape.Clear();\n    for (int i = 0; i < 4; ++i) {\n      input_shape.add_dim()->set_size(1);\n    }\n    *found_unknown_shapes = true;\n  }\n\n  ConvolutionDimensions conv_dims = ConvolutionDimensionsFromInputs(\n      input_shape, op_info.inputs(1).shape(), op_info, found_unknown_shapes);\n\n  ops = conv_dims.batch;\n  ops *= conv_dims.ox * conv_dims.oy;\n  ops *= conv_dims.kx * conv_dims.ky;\n  if (op_info.op() == kConv2dBackpropInput) {\n    ops *= conv_dims.kz * conv_dims.oz;\n  } else {\n    // conv_dims always use forward path definition regardless\n    conv_dims.oz *= conv_dims.iz;\n    ops *= conv_dims.oz;\n  }\n  ops *= kOpsPerMac;\n\n  VLOG(1) << \"Operations for\" << op_info.op() << \"  \" << ops;\n\n  if (returned_conv_dims != nullptr) {\n    *returned_conv_dims = conv_dims;\n  }\n  return ops;\n}\n\nint64_t OpLevelCostEstimator::CountConv2DBackpropFilterOperations(\n    const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n    bool* found_unknown_shapes) {\n  int64_t ops = 0;\n\n  DCHECK(op_info.op() == kConv2dBackpropFilter ||\n         op_info.op() == kDepthwiseConv2dNativeBackpropFilter)\n      << \"Invalid Operation: not kConv2dBackpropFilter nor\"\n         \"kDepthwiseConv2dNativeBackpropFilter\";\n\n  TensorShapeProto filter_shape;\n  bool shape_found = false;\n  if (op_info.inputs_size() >= 2 && op_info.inputs(1).has_value()) {\n    const TensorProto& value = op_info.inputs(1).value();\n    shape_found = GetTensorShapeProtoFromTensorProto(value, &filter_shape);\n  }\n  if (!shape_found && op_info.outputs_size() == 1) {\n    filter_shape = op_info.outputs(0).shape();\n    shape_found = true;\n  }\n  if (!shape_found) {\n    // Set the minimum filter size that's feasible.\n    filter_shape.Clear();\n    for (int i = 0; i < 4; ++i) {\n      filter_shape.add_dim()->set_size(1);\n    }\n    *found_unknown_shapes = true;\n  }\n\n  if (op_info.inputs_size() < 1) {\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return ops;\n  }\n  ConvolutionDimensions conv_dims = ConvolutionDimensionsFromInputs(\n      op_info.inputs(0).shape(), filter_shape, op_info, found_unknown_shapes);\n\n  ops = conv_dims.batch;\n  ops *= conv_dims.ox * conv_dims.oy;\n  ops *= conv_dims.kx * conv_dims.ky;\n  if (op_info.op() == kConv2dBackpropFilter) {\n    ops *= conv_dims.kz * conv_dims.oz;\n  } else {\n    // conv_dims always use forward path definition regardless\n    conv_dims.oz *= conv_dims.iz;\n    ops *= conv_dims.oz;\n  }\n  ops *= kOpsPerMac;\n  VLOG(1) << \"Operations for\" << op_info.op() << \"  \" << ops;\n\n  if (returned_conv_dims != nullptr) {\n    *returned_conv_dims = conv_dims;\n  }\n  return ops;\n}\n\nint64_t OpLevelCostEstimator::CalculateTensorElementCount(\n    const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes) {\n  VLOG(2) << \"   with \" << DataTypeString(tensor.dtype()) << \" tensor of shape \"\n          << tensor.shape().DebugString();\n  int64_t tensor_size = 1;\n  int num_dims = std::max(1, tensor.shape().dim_size());\n  auto tensor_shape =\n      MaybeGetMinimumShape(tensor.shape(), num_dims, found_unknown_shapes);\n  for (const auto& dim : tensor_shape.dim()) {\n    int64_t new_tensor_size = MultiplyWithoutOverflow(tensor_size, dim.size());\n    if (new_tensor_size < 0) {\n      VLOG(1) << \"Overflow encountered when computing element count of a \"\n                 \"tensor, multiplying \"\n              << tensor_size << \" with \" << dim.size();\n      return -1;\n    }\n    tensor_size = new_tensor_size;\n  }\n  return tensor_size;\n}\n\nint64_t OpLevelCostEstimator::CalculateTensorSize(\n    const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes) {\n  int64_t count = CalculateTensorElementCount(tensor, found_unknown_shapes);\n  int size = DataTypeSize(BaseType(tensor.dtype()));\n  VLOG(2) << \"Count: \" << count << \" DataTypeSize: \" << size;\n  int64_t tensor_size = MultiplyWithoutOverflow(count, size);\n  if (tensor_size < 0) {\n    VLOG(1) << \"Overflow encountered when computing tensor size, multiplying \"\n            << count << \" with \" << size;\n    return -1;\n  }\n  return tensor_size;\n}\n\nint64_t OpLevelCostEstimator::CalculateInputSize(const OpInfo& op_info,\n                                                 bool* found_unknown_shapes) {\n  int64_t total_input_size = 0;\n  for (auto& input : op_info.inputs()) {\n    int64_t input_size = CalculateTensorSize(input, found_unknown_shapes);\n    total_input_size += input_size;\n    VLOG(1) << \"Input Size: \" << input_size\n            << \" Total Input Size:\" << total_input_size;\n  }\n  return total_input_size;\n}\n\nstd::vector<int64_t> OpLevelCostEstimator::CalculateInputTensorSize(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  std::vector<int64_t> input_tensor_size;\n  input_tensor_size.reserve(op_info.inputs().size());\n  for (auto& input : op_info.inputs()) {\n    input_tensor_size.push_back(\n        CalculateTensorSize(input, found_unknown_shapes));\n  }\n  return input_tensor_size;\n}\n\nint64_t OpLevelCostEstimator::CalculateLargestInputCount(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  int64_t largest_input_count = 0;\n  for (auto& input : op_info.inputs()) {\n    int64_t input_count =\n        CalculateTensorElementCount(input, found_unknown_shapes);\n    if (input_count > largest_input_count) {\n      largest_input_count = input_count;\n    }\n    VLOG(1) << \"Input Count: \" << input_count\n            << \" Largest Input Count:\" << largest_input_count;\n  }\n  return largest_input_count;\n}\n\nint64_t OpLevelCostEstimator::CalculateOutputSize(const OpInfo& op_info,\n                                                  bool* found_unknown_shapes) {\n  int64_t total_output_size = 0;\n  // Use float as default for calculations.\n  for (const auto& output : op_info.outputs()) {\n    DataType dt = output.dtype();\n    const auto& original_output_shape = output.shape();\n    int64_t output_size = DataTypeSize(BaseType(dt));\n    int num_dims = std::max(1, original_output_shape.dim_size());\n    auto output_shape = MaybeGetMinimumShape(original_output_shape, num_dims,\n                                             found_unknown_shapes);\n    for (const auto& dim : output_shape.dim()) {\n      int64_t new_output_size =\n          MultiplyWithoutOverflow(output_size, dim.size());\n      if (new_output_size < 0) {\n        VLOG(1) << \"Overflow encountered when estimating cost, multiplying \"\n                << output_size << \" with \" << dim.size();\n        return -1;\n      }\n      output_size = new_output_size;\n    }\n    total_output_size += output_size;\n    VLOG(1) << \"Output Size: \" << output_size\n            << \" Total Output Size:\" << total_output_size;\n  }\n  return total_output_size;\n}\n\nstd::vector<int64_t> OpLevelCostEstimator::CalculateOutputTensorSize(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  std::vector<int64_t> output_tensor_size;\n  output_tensor_size.reserve(op_info.outputs().size());\n  // Use float as default for calculations.\n  for (const auto& output : op_info.outputs()) {\n    DataType dt = output.dtype();\n    const auto& original_output_shape = output.shape();\n    int64_t output_size = DataTypeSize(BaseType(dt));\n    int num_dims = std::max(1, original_output_shape.dim_size());\n    auto output_shape = MaybeGetMinimumShape(original_output_shape, num_dims,\n                                             found_unknown_shapes);\n    for (const auto& dim : output_shape.dim()) {\n      output_size *= dim.size();\n    }\n    output_tensor_size.push_back(output_size);\n  }\n  return output_tensor_size;\n}\n\nStatus OpLevelCostEstimator::PredictDefaultNodeCosts(\n    const int64_t num_compute_ops, const OpContext& op_context,\n    bool* found_unknown_shapes, NodeCosts* node_costs) {\n  const auto& op_info = op_context.op_info;\n  node_costs->num_compute_ops = num_compute_ops;\n  node_costs->num_input_bytes_accessed =\n      CalculateInputTensorSize(op_info, found_unknown_shapes);\n  node_costs->num_output_bytes_accessed =\n      CalculateOutputTensorSize(op_info, found_unknown_shapes);\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n  if (*found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nbool HasZeroDim(const OpInfo& op_info) {\n  for (int i = 0; i < op_info.inputs_size(); ++i) {\n    const auto& input = op_info.inputs(i);\n    for (int j = 0; j < input.shape().dim_size(); ++j) {\n      const auto& dim = input.shape().dim(j);\n      if (dim.size() == 0) {\n        VLOG(1) << \"Convolution config has zero dim \"\n                << op_info.ShortDebugString();\n        return true;\n      }\n    }\n  }\n  return false;\n}\n\nStatus OpLevelCostEstimator::PredictConv2D(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  if (HasZeroDim(op_info)) {\n    node_costs->num_nodes_with_unknown_shapes = 1;\n    return errors::InvalidArgument(\"Conv2D op includes zero dimension: \",\n                                   op_info.ShortDebugString());\n  }\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops =\n      CountConv2DOperations(op_info, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictConv2DBackpropInput(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  if (HasZeroDim(op_info)) {\n    node_costs->num_nodes_with_unknown_shapes = 1;\n    return errors::InvalidArgument(\n        \"Conv2DBackpropInput op includes zero dimension\",\n        op_info.ShortDebugString());\n  }\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops = CountConv2DBackpropInputOperations(\n      op_info, nullptr, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictConv2DBackpropFilter(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  if (HasZeroDim(op_info)) {\n    node_costs->num_nodes_with_unknown_shapes = 1;\n    return errors::InvalidArgument(\n        \"Conv2DBackpropFilter op includes zero dimension\",\n        op_info.ShortDebugString());\n  }\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops = CountConv2DBackpropFilterOperations(\n      op_info, nullptr, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictFusedConv2DBiasActivation(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  // FusedConv2DBiasActivation computes a fused kernel which implements:\n  // 2D convolution, adds side input with separate scaling on convolution and\n  // side inputs, then adds bias, and finally applies the ReLU activation\n  // function to the result:\n  //\n  // Input -> Conv2D  ->  Add  -> BiasAdd  -> ReLU\n  //            ^          ^         ^\n  //          Filter   Side Input   Bias\n  //\n  // Note that when adding the side input, the operation multiplies the output\n  // of Conv2D by conv_input_scale, confusingly, and the side_input by\n  // side_input_scale.\n  //\n  // Note that in the special case that side_input_scale is 0, which we infer\n  // from side_input having dimensions [], we skip that addition operation.\n  //\n  // For more information, see\n  // contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc\n\n  // TODO(yaozhang): Support NHWC_VECT_W.\n  std::string data_format = GetDataFormat(op_context.op_info);\n  if (data_format != \"NCHW\" && data_format != \"NHWC\" &&\n      data_format != \"NCHW_VECT_C\") {\n    return errors::InvalidArgument(\n        \"Unsupported data format (\", data_format,\n        \") for op: \", op_context.op_info.ShortDebugString());\n  }\n  std::string filter_format = GetFilterFormat(op_context.op_info);\n  if (filter_format != \"HWIO\" && filter_format != \"OIHW\" &&\n      filter_format != \"OIHW_VECT_I\") {\n    return errors::InvalidArgument(\n        \"Unsupported filter format (\", filter_format,\n        \") for op: \", op_context.op_info.ShortDebugString());\n  }\n\n  auto& conv_input = op_context.op_info.inputs(0);\n  auto& filter = op_context.op_info.inputs(1);\n  auto& side_input = op_context.op_info.inputs(3);\n  auto& conv_input_scale = op_context.op_info.inputs(4);\n  auto& side_input_scale = op_context.op_info.inputs(5);\n\n  // Manually compute our convolution dimensions.\n  bool found_unknown_shapes = false;\n  auto dims = ConvolutionDimensionsFromInputs(\n      conv_input.shape(), filter.shape(), op_context.op_info,\n      &found_unknown_shapes);\n  OpInfo::TensorProperties output;\n  if (data_format == \"NCHW\" || data_format == \"NCHW_VECT_C\") {\n    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.oz, dims.oy, dims.ox});\n  } else if (data_format == \"NHWC\") {\n    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.oy, dims.ox, dims.oz});\n  }\n\n  // Add the operations the fused op always computes.\n  std::vector<OpContext> component_ops = {\n      FusedChildContext(op_context, \"Conv2D\", output, {conv_input, filter}),\n      FusedChildContext(op_context, \"Mul\", output, {output, conv_input_scale}),\n      FusedChildContext(\n          op_context, \"BiasAdd\", output,\n          {output, output}),  // Note we're no longer using bias at all\n      FusedChildContext(op_context, \"Relu\", output, {output})};\n\n  // Add our side_input iff it's non-empty.\n  if (side_input.shape().dim_size() > 0) {\n    component_ops.push_back(FusedChildContext(op_context, \"Mul\", side_input,\n                                              {side_input, side_input_scale}));\n    component_ops.push_back(FusedChildContext(\n        op_context, \"Add\", output,\n        {output, output}));  // Note that we're not using side_input here\n  }\n\n  // Construct an op_context which definitely has our output shape.\n  auto op_context_with_output = op_context;\n  op_context_with_output.op_info.mutable_outputs()->Clear();\n  *op_context_with_output.op_info.mutable_outputs()->Add() = output;\n\n  // Construct component operations and run the cost computation.\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return PredictFusedOp(op_context_with_output, component_ops, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictMatMul(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops =\n      CountMatMulOperations(op_info, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictEinsum(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n\n  auto it = op_info.attr().find(\"equation\");\n  if (it == op_info.attr().end()) {\n    return errors::InvalidArgument(\"Einsum op doesn't have equation attr: \",\n                                   op_info.ShortDebugString());\n  }\n\n  OpContext batch_matmul_op_context;\n  bool found_unknown_shapes = false;\n  bool success = GenerateBatchMatmulContextFromEinsum(\n      op_context, &batch_matmul_op_context, &found_unknown_shapes);\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  if (!success) {\n    return PredictCostOfAnUnknownOp(op_context, node_costs);\n  }\n  return PredictNodeCosts(batch_matmul_op_context, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictSparseTensorDenseMatMul(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  // input[0]: indices in sparse matrix a\n  // input[1]: values in sparse matrix a\n  // input[2]: shape of matrix a\n  // input[3]: matrix b\n  // See\n  // https://github.com/tensorflow/tensorflow/blob/9a43dfeac5/tensorflow/core/ops/sparse_ops.cc#L85\n  int64_t num_elems_in_a =\n      CalculateTensorElementCount(op_info.inputs(1), &found_unknown_shapes);\n  auto b_matrix = op_info.inputs(3);\n  auto b_matrix_shape =\n      MaybeGetMinimumShape(b_matrix.shape(), 2, &found_unknown_shapes);\n  int64_t n_dim = b_matrix_shape.dim(1).size();\n\n  // Each element in A is multiplied and added with an element from each column\n  // in b.\n  const int64_t op_count = kOpsPerMac * num_elems_in_a * n_dim;\n\n  int64_t a_indices_input_size =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  int64_t a_values_input_size =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  int64_t a_shape_input_size =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  int64_t b_input_size =\n      num_elems_in_a * n_dim * DataTypeSize(BaseType(b_matrix.dtype()));\n  int64_t output_size = CalculateOutputSize(op_info, &found_unknown_shapes);\n\n  node_costs->num_compute_ops = op_count;\n  node_costs->num_input_bytes_accessed = {a_indices_input_size,\n                                          a_values_input_size,\n                                          a_shape_input_size, b_input_size};\n  node_costs->num_output_bytes_accessed = {output_size};\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictNoOp(const OpContext& op_context,\n                                         NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  VLOG(1) << \"Op:\" << op_info.op() << \" Execution Time 0 (ns)\";\n  // By default, NodeCosts is initialized to zero ops and bytes.\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictPureMemoryOp(const OpContext& op_context,\n                                                 NodeCosts* node_costs) const {\n  // Each output element is a copy of some element from input, with no required\n  // computation, so just compute memory costs.\n  bool found_unknown_shapes = false;\n  node_costs->num_nodes_with_pure_memory_op = 1;\n  return PredictDefaultNodeCosts(0, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictIdentity(const OpContext& op_context,\n                                             NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  VLOG(1) << \"Op:\" << op_info.op() << \" Minimum cost for Identity\";\n  node_costs->minimum_cost_op = true;\n  node_costs->num_compute_ops = kMinComputeOp;\n  // Identity op internally pass input tensor buffer's pointer to the output\n  // tensor buffer; no actual memory operation.\n  node_costs->num_input_bytes_accessed = {0};\n  node_costs->num_output_bytes_accessed = {0};\n  bool inaccurate = false;\n  node_costs->max_memory = CalculateOutputSize(op_info, &inaccurate);\n  if (inaccurate) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictVariable(const OpContext& op_context,\n                                             NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  VLOG(1) << \"Op:\" << op_info.op() << \" Minimum cost for Variable\";\n  node_costs->minimum_cost_op = true;\n  node_costs->num_compute_ops = kMinComputeOp;\n  // Variables are persistent ops; initialized before step; hence, no memory\n  // cost.\n  node_costs->num_input_bytes_accessed = {0};\n  node_costs->num_output_bytes_accessed = {0};\n  bool inaccurate = false;\n  node_costs->persistent_memory = CalculateOutputSize(op_info, &inaccurate);\n  if (inaccurate) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictBatchMatMul(const OpContext& op_context,\n                                                NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops =\n      CountBatchMatMulOperations(op_info, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictMetadata(const OpContext& op_context,\n                                             NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  node_costs->minimum_cost_op = true;\n  node_costs->num_compute_ops = kMinComputeOp;\n  node_costs->num_input_bytes_accessed = {0};\n  node_costs->num_output_bytes_accessed = {0};\n  bool inaccurate = false;\n  node_costs->max_memory = CalculateOutputSize(op_info, &inaccurate);\n  if (inaccurate) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictGatherOrSlice(const OpContext& op_context,\n                                                  NodeCosts* node_costs) const {\n  // Gather & Slice ops can have a very large input, but only access a small\n  // part of it. For these op the size of the output determines the memory cost.\n  const auto& op_info = op_context.op_info;\n\n  const int inputs_needed = op_info.op() == \"Slice\" ? 3 : 2;\n  if (op_info.outputs_size() == 0 || op_info.inputs_size() < inputs_needed) {\n    return errors::InvalidArgument(\n        op_info.op(),\n        \" Op doesn't have valid input / output: \", op_info.ShortDebugString());\n  }\n\n  bool unknown_shapes = false;\n\n  // Each output element is a copy of some element from input.\n  // For roofline estimate we assume each copy has a unit cost.\n  const int64_t op_count =\n      CalculateTensorElementCount(op_info.outputs(0), &unknown_shapes);\n  node_costs->num_compute_ops = op_count;\n\n  const int64_t output_size = CalculateOutputSize(op_info, &unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n\n  node_costs->num_input_bytes_accessed.reserve(op_info.inputs().size());\n  int64_t input_size = output_size;\n  // Note that input(0) byte accessed is not equal to input(0) tensor size.\n  // It's equal to the output size; though, input access is indexed gather or\n  // slice (ignore duplicate indices).\n  node_costs->num_input_bytes_accessed.push_back(input_size);\n  int begin_input_index = 1;\n  int end_input_index;\n  if (op_info.op() == \"Slice\") {\n    // Slice: 'input' (omitted), 'begin', 'size'\n    end_input_index = 3;\n  } else if (op_info.op() == \"StridedSlice\") {\n    // StridedSlice: 'input' (omitted), 'begin', 'end', 'strides'\n    end_input_index = 4;\n  } else {\n    // Gather, GatherV2, GatherNd: 'params' (omitted), 'indices'\n    end_input_index = 2;\n  }\n  for (int i = begin_input_index; i < end_input_index; ++i) {\n    node_costs->num_input_bytes_accessed.push_back(\n        CalculateTensorElementCount(op_info.inputs(i), &unknown_shapes));\n  }\n  if (unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictScatter(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  // Scatter ops sparsely access a reference input and output tensor.\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n\n  // input[0]: ref tensor that will be sparsely accessed\n  // input[1]: indices - A tensor of indices into the first dimension of ref.\n  // input[2]: updates where updates.shape = indices.shape + ref.shape[1:]\n  // See\n  // https://www.tensorflow.org/api_docs/python/tf/scatter_add and\n  // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/state_ops.cc#L146\n\n  const int64_t num_indices =\n      CalculateTensorElementCount(op_info.inputs(1), &found_unknown_shapes);\n\n  int64_t num_elems_in_ref_per_index = 1;\n  auto ref_tensor_shape = MaybeGetMinimumShape(\n      op_info.inputs(0).shape(), op_info.inputs(0).shape().dim_size(),\n      &found_unknown_shapes);\n  for (int i = 1; i < ref_tensor_shape.dim().size(); ++i) {\n    num_elems_in_ref_per_index *= ref_tensor_shape.dim(i).size();\n  }\n  const int64_t op_count = num_indices * num_elems_in_ref_per_index;\n  node_costs->num_compute_ops = op_count;\n\n  // Sparsely access ref so input size depends on the number of operations\n  int64_t ref_input_size =\n      op_count * DataTypeSize(BaseType(op_info.inputs(0).dtype()));\n  int64_t indices_input_size =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  int64_t updates_input_size =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  node_costs->num_input_bytes_accessed = {ref_input_size, indices_input_size,\n                                          updates_input_size};\n\n  // Sparsely access ref so output size depends on the number of operations\n  int64_t output_size =\n      op_count * DataTypeSize(BaseType(op_info.outputs(0).dtype()));\n  node_costs->num_output_bytes_accessed = {output_size};\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictFusedOp(\n    const OpContext& op_context,\n    const std::vector<OpContext>& fused_op_contexts,\n    NodeCosts* node_costs) const {\n  // Note that PredictDefaultNodeCosts will get the correct memory costs from\n  // the node's inputs and outputs; but we don't want to have to re-implement\n  // the logic for computing the operation count of each of our component\n  // operations here; so we simply add the compute times of each component\n  // operation, then update the cost.\n  bool found_unknown_shapes = false;\n  Status s =\n      PredictDefaultNodeCosts(0, op_context, &found_unknown_shapes, node_costs);\n\n  for (auto& fused_op : fused_op_contexts) {\n    NodeCosts fused_node_costs;\n    s.Update(PredictNodeCosts(fused_op, &fused_node_costs));\n    node_costs->num_compute_ops += fused_node_costs.num_compute_ops;\n    node_costs->inaccurate |= fused_node_costs.inaccurate;\n    // Set, not increment. Note that we are predicting the cost of one fused\n    // node, not a function node composed of many nodes.\n    node_costs->num_nodes_with_unknown_shapes |=\n        fused_node_costs.num_nodes_with_unknown_shapes;\n    node_costs->num_nodes_with_unknown_op_type |=\n        fused_node_costs.num_nodes_with_unknown_op_type;\n    node_costs->num_nodes_with_pure_memory_op |=\n        fused_node_costs.num_nodes_with_pure_memory_op;\n  }\n\n  return Status::OK();\n}\n\n/* static */\nOpContext OpLevelCostEstimator::FusedChildContext(\n    const OpContext& parent, const std::string& op_name,\n    const OpInfo::TensorProperties& output,\n    const std::vector<OpInfo::TensorProperties>& inputs) {\n  // Setup the base parameters of our new context.\n  OpContext new_context;\n  new_context.name = op_name;\n  new_context.device_name = parent.device_name;\n  new_context.op_info = parent.op_info;\n  new_context.op_info.set_op(op_name);\n\n  // Setup the inputs of our new context.\n  new_context.op_info.mutable_inputs()->Clear();\n  for (const auto& input : inputs) {\n    *new_context.op_info.mutable_inputs()->Add() = input;\n  }\n\n  // Setup the output of our new context.\n  new_context.op_info.mutable_outputs()->Clear();\n  *new_context.op_info.mutable_outputs()->Add() = output;\n\n  return new_context;\n}\n\n/* static */\nOpInfo::TensorProperties OpLevelCostEstimator::DescribeTensor(\n    DataType type, const std::vector<int64_t>& dims) {\n  OpInfo::TensorProperties ret;\n  ret.set_dtype(type);\n\n  auto shape = ret.mutable_shape();\n  for (const int dim : dims) {\n    shape->add_dim()->set_size(dim);\n  }\n\n  return ret;\n}\n\n/* static */\nStatusOr<OpLevelCostEstimator::ConvolutionDimensions>\nOpLevelCostEstimator::OpDimensionsFromInputs(\n    const TensorShapeProto& original_image_shape, const OpInfo& op_info,\n    bool* found_unknown_shapes) {\n  VLOG(2) << \"op features: \" << op_info.DebugString();\n  VLOG(2) << \"Original image shape: \" << original_image_shape.DebugString();\n  auto image_shape =\n      MaybeGetMinimumShape(original_image_shape, 4, found_unknown_shapes);\n  VLOG(2) << \"Image shape: \" << image_shape.DebugString();\n\n  int x_index, y_index, channel_index;\n  const std::string& data_format = GetDataFormat(op_info);\n  if (data_format == \"NCHW\") {\n    channel_index = 1;\n    y_index = 2;\n    x_index = 3;\n  } else {\n    y_index = 1;\n    x_index = 2;\n    channel_index = 3;\n  }\n  int64_t batch = image_shape.dim(0).size();\n  int64_t ix = image_shape.dim(x_index).size();\n  int64_t iy = image_shape.dim(y_index).size();\n  int64_t iz = image_shape.dim(channel_index).size();\n\n  // Note that FusedBatchNorm doesn't have ksize attr, but GetKernelSize returns\n  // {1, 1, 1, 1} in that case.\n  std::vector<int64_t> ksize = GetKernelSize(op_info);\n  int64_t kx = ksize[x_index];\n  int64_t ky = ksize[y_index];\n  // These ops don't support groupwise operation, therefore kz == iz.\n  int64_t kz = iz;\n\n  std::vector<int64_t> strides = GetStrides(op_info);\n  int64_t sx = strides[x_index];\n  int64_t sy = strides[y_index];\n  if (sx == 0 || sy == 0) {\n    return errors::InvalidArgument(\n        \"Stride must be > 0 for Height and Width, but got (\", sy, \", \", sx,\n        \")\");\n  }\n  const auto padding = GetPadding(op_info);\n\n  int64_t ox = GetOutputSize(ix, kx, sx, padding);\n  int64_t oy = GetOutputSize(iy, ky, sy, padding);\n  int64_t oz = iz;\n\n  OpLevelCostEstimator::ConvolutionDimensions conv_dims = {\n      batch, ix, iy, iz, kx, ky, kz, oz, ox, oy, sx, sy, padding};\n  return conv_dims;\n}\n\nStatus OpLevelCostEstimator::PredictMaxPool(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n                                             &found_unknown_shapes));\n  // kx * ky - 1 comparisons per output (kx * xy > 1)\n  // or 1 copy per output (kx * k1 = 1).\n  int per_output_ops = dims.kx * dims.ky == 1 ? 1 : dims.kx * dims.ky - 1;\n  int64_t ops = dims.batch * dims.ox * dims.oy * dims.oz * per_output_ops;\n  node_costs->num_compute_ops = ops;\n\n  int64_t input_size = 0;\n  if (dims.ky >= dims.sy) {\n    input_size = CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  } else {  // dims.ky < dims.sy\n    // Vertical stride is larger than vertical kernel; assuming row-major\n    // format, skip unnecessary rows (or read every kx rows per sy rows, as the\n    // others are not used for output).\n    const auto data_size = DataTypeSize(BaseType(op_info.inputs(0).dtype()));\n    input_size = data_size * dims.batch * dims.ix * dims.ky * dims.oy * dims.iz;\n  }\n  node_costs->num_input_bytes_accessed = {input_size};\n  const int64_t output_size =\n      CalculateOutputSize(op_info, &found_unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n  node_costs->max_memory = output_size;\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictMaxPoolGrad(const OpContext& op_context,\n                                                NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  // y: op_info.inputs(1)\n  // y_grad: op_info.inputs(2)\n  if (op_info.inputs_size() < 3) {\n    return errors::InvalidArgument(\"MaxPoolGrad op has invalid inputs: \",\n                                   op_info.ShortDebugString());\n  }\n\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n                                             &found_unknown_shapes));\n\n  int64_t ops = 0;\n  if (dims.kx == 1 && dims.ky == 1) {\n    // 1x1 window. No need to know which input was max.\n    ops = dims.batch * dims.ix * dims.iy * dims.iz;\n  } else if (dims.kx <= dims.sx && dims.ky <= dims.sy) {\n    // Non-overlapping window: re-run maxpool, then assign zero or y_grad.\n    ops = dims.batch * dims.iz *\n          (dims.ox * dims.oy * (dims.kx * dims.ky - 1) + dims.ix * dims.iy);\n  } else {\n    // Overlapping window: initialize with zeros, re-run maxpool, then\n    // accumulate y_gad to proper x_grad locations.\n    ops = dims.batch * dims.iz *\n          (dims.ox * dims.oy * (dims.kx * dims.ky - 1) + dims.ix * dims.iy * 2);\n  }\n  node_costs->num_compute_ops = ops;\n\n  // Just read x and y_grad; no need to read y as we assume MaxPoolGrad re-run\n  // MaxPool internally.\n  const int64_t input0_size =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  const int64_t input2_size =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  node_costs->num_input_bytes_accessed = {input0_size, 0, input2_size};\n  // Write x_grad; size equal to x.\n  const int64_t output_size =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n  node_costs->max_memory = output_size;\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\n/* This predict function handles three types of tensorflow ops\n * AssignVariableOp/AssignAddVariableOp/AssignSubVariableOp, broadcasting\n * was not possible for these ops, therefore the input tensor's shapes is\n * enough to compute the cost */\nStatus OpLevelCostEstimator::PredictAssignVariableOps(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  /* First input of these ops are reference to the assignee. */\n  if (op_info.inputs_size() != 2) {\n    return errors::InvalidArgument(\"AssignVariable op has invalid input: \",\n                                   op_info.ShortDebugString());\n  }\n\n  const int64_t ops = op_info.op() == kAssignVariableOp\n                          ? 0\n                          : CalculateTensorElementCount(op_info.inputs(1),\n                                                        &found_unknown_shapes);\n  node_costs->num_compute_ops = ops;\n  const int64_t input_size = CalculateInputSize(op_info, &found_unknown_shapes);\n  node_costs->num_input_bytes_accessed = {input_size};\n  // TODO(dyoon): check these ops' behavior whether it writes data;\n  // Op itself doesn't have output tensor, but it may modify the input (ref or\n  // resource). Maybe use node_costs->internal_write_bytes.\n  node_costs->num_output_bytes_accessed = {0};\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictAvgPool(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n                                             &found_unknown_shapes));\n\n  // kx * ky - 1 additions and 1 multiplication per output.\n  int64_t ops = dims.batch * dims.ox * dims.oy * dims.oz * dims.kx * dims.ky;\n  node_costs->num_compute_ops = ops;\n\n  int64_t input_size;\n  if (dims.ky >= dims.sy) {\n    input_size = CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  } else {  // dims.ky < dims.sy\n    // vertical stride is larger than vertical kernel; assuming row-major\n    // format, skip unnecessary rows (or read every kx rows per sy rows, as the\n    // others are not used for output).\n    const auto data_size = DataTypeSize(BaseType(op_info.inputs(0).dtype()));\n    input_size = data_size * dims.batch * dims.ix * dims.ky * dims.oy * dims.iz;\n  }\n  node_costs->num_input_bytes_accessed = {input_size};\n\n  const int64_t output_size =\n      CalculateOutputSize(op_info, &found_unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n  node_costs->max_memory = output_size;\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictAvgPoolGrad(const OpContext& op_context,\n                                                NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x's shape: op_info.inputs(0)\n  // y_grad: op_info.inputs(1)\n\n  // Extract x_shape from op_info.inputs(0).value() or op_info.outputs(0).\n  bool shape_found = false;\n  TensorShapeProto x_shape;\n  if (op_info.inputs_size() >= 1 && op_info.inputs(0).has_value()) {\n    const TensorProto& value = op_info.inputs(0).value();\n    shape_found = GetTensorShapeProtoFromTensorProto(value, &x_shape);\n  }\n  if (!shape_found && op_info.outputs_size() > 0) {\n    x_shape = op_info.outputs(0).shape();\n    shape_found = true;\n  }\n  if (!shape_found) {\n    // Set the minimum shape that's feasible.\n    x_shape.Clear();\n    for (int i = 0; i < 4; ++i) {\n      x_shape.add_dim()->set_size(1);\n    }\n    found_unknown_shapes = true;\n  }\n\n  TF_ASSIGN_OR_RETURN(\n      ConvolutionDimensions dims,\n      OpDimensionsFromInputs(x_shape, op_info, &found_unknown_shapes));\n\n  int64_t ops = 0;\n  if (dims.kx <= dims.sx && dims.ky <= dims.sy) {\n    // Non-overlapping window.\n    ops = dims.batch * dims.iz * (dims.ix * dims.iy + dims.ox * dims.oy);\n  } else {\n    // Overlapping window.\n    ops = dims.batch * dims.iz *\n          (dims.ix * dims.iy + dims.ox * dims.oy * (dims.kx * dims.ky + 1));\n  }\n  auto s = PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                   node_costs);\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n  return s;\n}\n\nStatus OpLevelCostEstimator::PredictFusedBatchNorm(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  // scale: op_info.inputs(1)\n  // offset: op_info.inputs(2)\n  // mean: op_info.inputs(3)  --> only for inference\n  // variance: op_info.inputs(4) --> only for inference\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n                                             &found_unknown_shapes));\n  const bool is_training = IsTraining(op_info);\n\n  int64_t ops = 0;\n  const auto rsqrt_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_rsqrt_op<float>>::Cost;\n  if (is_training) {\n    ops = dims.iz * (dims.batch * dims.ix * dims.iy * 4 + 6 + rsqrt_cost);\n  } else {\n    ops = dims.batch * dims.ix * dims.iy * dims.iz * 2;\n  }\n  node_costs->num_compute_ops = ops;\n\n  const int64_t size_nhwc =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  const int64_t size_c =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  if (is_training) {\n    node_costs->num_input_bytes_accessed = {size_nhwc, size_c, size_c};\n    node_costs->num_output_bytes_accessed = {size_nhwc, size_c, size_c, size_c,\n                                             size_c};\n    // FusedBatchNorm in training mode internally re-reads the input tensor:\n    // one for mean/variance, and the 2nd internal read forthe actual scaling.\n    // Assume small intermediate data such as mean / variance (size_c) can be\n    // cached on-chip.\n    node_costs->internal_read_bytes = size_nhwc;\n  } else {\n    node_costs->num_input_bytes_accessed = {size_nhwc, size_c, size_c, size_c,\n                                            size_c};\n    node_costs->num_output_bytes_accessed = {size_nhwc};\n  }\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictFusedBatchNormGrad(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // y_backprop: op_info.inputs(0)\n  // x: op_info.inputs(1)\n  // scale: op_info.inputs(2)\n  // mean: op_info.inputs(3)\n  // variance or inverse of variance: op_info.inputs(4)\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(1).shape(), op_info,\n                                             &found_unknown_shapes));\n\n  int64_t ops = 0;\n  const auto rsqrt_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_rsqrt_op<float>>::Cost;\n  ops = dims.iz * (dims.batch * dims.ix * dims.iy * 11 + 5 + rsqrt_cost);\n  node_costs->num_compute_ops = ops;\n\n  const int64_t size_nhwc =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  const int64_t size_c =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  // TODO(dyoon): fix missing memory cost for variance input (size_c) and\n  // yet another read of y_backprop (size_nhwc) internally.\n  node_costs->num_input_bytes_accessed = {size_nhwc, size_nhwc, size_c, size_c};\n  node_costs->num_output_bytes_accessed = {size_nhwc, size_c, size_c};\n  // FusedBatchNormGrad has to read y_backprop internally.\n  node_costs->internal_read_bytes = size_nhwc;\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictNaryOp(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  // Calculate the largest known tensor size across all inputs and output.\n  int64_t op_count = CalculateLargestInputCount(op_info, &found_unknown_shapes);\n  // If output shape is available, try to use the element count calculated from\n  // that.\n  if (op_info.outputs_size() > 0) {\n    op_count = std::max(\n        op_count,\n        CalculateTensorElementCount(op_info.outputs(0), &found_unknown_shapes));\n  }\n  // Also calculate the output shape possibly resulting from broadcasting.\n  // Note that the some Nary ops (such as AddN) do not support broadcasting,\n  // but we're including this here for completeness.\n  if (op_info.inputs_size() >= 2) {\n    op_count = std::max(op_count, CwiseOutputElementCount(op_info));\n  }\n\n  // Nary ops perform one operation for every element in every input tensor.\n  op_count *= op_info.inputs_size() - 1;\n\n  const auto sum_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_sum_op<float>>::Cost;\n  return PredictDefaultNodeCosts(op_count * sum_cost, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\n// softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))\nStatus OpLevelCostEstimator::PredictSoftmax(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const int64_t logits_size = CalculateTensorElementCount(\n      op_context.op_info.inputs(0), &found_unknown_shapes);\n  // Softmax input rank should be >=1.\n  TensorShapeProto logits_shape = op_context.op_info.inputs(0).shape();\n  if (logits_shape.unknown_rank() || logits_shape.dim_size() == 0) {\n    return errors::InvalidArgument(\"Softmax op has invalid input: \",\n                                   op_context.op_info.ShortDebugString());\n  }\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n\n  // Every element of <logits> will be exponentiated, have that result included\n  // in a sum across j, and also have that result multiplied by the reciprocal\n  // of the sum_j. In addition, we'll compute 1/sum_j for every i.\n  auto ops =\n      (EIGEN_COST(scalar_exp_op<float>) + EIGEN_COST(scalar_sum_op<float>) +\n       EIGEN_COST(scalar_product_op<float>)) *\n          logits_size +\n      EIGEN_COST(scalar_inverse_op<float>) * logits_shape.dim(0).size();\n\n#undef EIGEN_COST\n  return PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictResizeBilinear(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n\n  if (op_context.op_info.outputs().empty() ||\n      op_context.op_info.inputs().empty()) {\n    return errors::InvalidArgument(\n        \"ResizeBilinear op has invalid input / output \",\n        op_context.op_info.ShortDebugString());\n  }\n\n  const int64_t output_elements = CalculateTensorElementCount(\n      op_context.op_info.outputs(0), &found_unknown_shapes);\n\n  const auto half_pixel_centers =\n      op_context.op_info.attr().find(\"half_pixel_centers\");\n  bool use_half_pixel_centers = false;\n  if (half_pixel_centers == op_context.op_info.attr().end()) {\n    LOG(WARNING) << \"half_pixel_centers attr not set for ResizeBilinear.\";\n    return PredictCostOfAnUnknownOp(op_context, node_costs);\n  } else {\n    use_half_pixel_centers = half_pixel_centers->second.b();\n  }\n\n  // Compose cost of bilinear interpolation.\n  int64_t ops = 0;\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n  const auto sub_cost_float = EIGEN_COST(scalar_difference_op<float>);\n  const auto sub_cost_int = EIGEN_COST(scalar_difference_op<int64_t>);\n  const auto add_cost = EIGEN_COST(scalar_sum_op<float>);\n  const auto mul_cost = EIGEN_COST(scalar_product_op<float>);\n  const auto floor_cost = EIGEN_COST(scalar_floor_op<float>);\n  const auto max_cost = EIGEN_COST(scalar_max_op<int64_t>);\n  const auto min_cost = EIGEN_COST(scalar_min_op<int64_t>);\n  const auto cast_to_int_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_cast_op<float, int64_t>>::Cost;\n  const auto cast_to_float_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_cast_op<int64_t, float>>::Cost;\n  const auto ceil_cost = EIGEN_COST(scalar_ceil_op<float>);\n#undef EIGEN_COST\n\n  // Ops calculated from tensorflow/core/kernels/image/resize_bilinear_op.cc.\n\n  // Op counts taken from resize_bilinear implementation on 07/21/2020.\n  // Computed op counts may become inaccurate if resize_bilinear implementation\n  // changes.\n\n  // resize_bilinear has an optimization where the interpolation weights are\n  // precomputed and cached. Given input tensors of size [B,H1,W1,C] and output\n  // tensors of size [B,H2,W2,C], the last dimension C that needs to be accessed\n  // in the input for interpolation are identical at every point in the output.\n  // These values are cached in the compute_interpolation_weights function. For\n  // a particular y in [0...H2-1], the rows to be accessed in the input are the\n  // same. Likewise, for a particular x in [0...H2-1], the columns to be accsed\n  // are the same. So the precomputation only needs to be done for H2 + W2\n  // values.\n  const auto output_shape = MaybeGetMinimumShape(\n      op_context.op_info.outputs(0).shape(), 4, &found_unknown_shapes);\n  // Assume H is dim 1 and W is dim 2 to match logic in resize_bilinear, which\n  // also makes this assumption.\n  const int64_t output_height = output_shape.dim(1).size();\n  const int64_t output_width = output_shape.dim(2).size();\n  // Add the ops done outside of the scaler function in\n  // compute_interpolation_weights.\n  int64_t interp_weight_cost = floor_cost + max_cost + min_cost +\n                               sub_cost_float + sub_cost_int + ceil_cost +\n                               cast_to_int_cost * 2;\n  // There are two options for computing the weight of each pixel in the\n  // interpolation. Algorithm can use pixel centers, or corners, for the\n  // weight. Ops depend on the scaler function passed into\n  // compute_interpolation_weights.\n  if (use_half_pixel_centers) {\n    // Ops for HalfPixelScalaer.\n    interp_weight_cost +=\n        add_cost + mul_cost + sub_cost_float + cast_to_float_cost;\n  } else {\n    // Ops for LegacyScaler.\n    interp_weight_cost += cast_to_float_cost + mul_cost;\n  }\n  // Cost for the interpolation is multiplied by (H2 + w2), as mentioned above.\n  ops += interp_weight_cost * (output_height + output_width);\n\n  // Ops for computing the new values, done for every element. Logic is from\n  // compute_lerp in the inner loop of resize_image which consists of:\n  //   const float top = top_left + (top_right - top_left) * x_lerp;\n  //   const float bottom = bottom_left + (bottom_right - bottom_left) * x_lerp;\n  //   return top + (bottom - top) * y_lerp;\n  ops += (add_cost * 3 + sub_cost_float * 3 + mul_cost * 3) * output_elements;\n\n  return PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictCropAndResize(const OpContext& op_context,\n                                                  NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n\n  const auto method = op_context.op_info.attr().find(\"method\");\n  bool use_bilinear_interp;\n  if (method == op_context.op_info.attr().end() ||\n      method->second.s() == \"bilinear\") {\n    use_bilinear_interp = true;\n  } else if (method->second.s() == \"nearest\") {\n    use_bilinear_interp = false;\n  } else {\n    LOG(WARNING) << \"method attr in CropAndResize invalid; expected bilinear \"\n                    \"or nearest.\";\n    return PredictCostOfAnUnknownOp(op_context, node_costs);\n  }\n\n  const int64_t num_boxes = op_context.op_info.inputs(1).shape().dim(0).size();\n  const auto crop_shape = MaybeGetMinimumShape(\n      op_context.op_info.outputs(0).shape(), 4, &found_unknown_shapes);\n  const int64_t crop_height = crop_shape.dim(1).size();\n  const int64_t crop_width = crop_shape.dim(2).size();\n  const int64_t output_elements = CalculateTensorElementCount(\n      op_context.op_info.outputs(0), &found_unknown_shapes);\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n  const auto sub_cost = EIGEN_COST(scalar_difference_op<float>);\n  const auto add_cost = EIGEN_COST(scalar_sum_op<float>);\n  const auto mul_cost = EIGEN_COST(scalar_product_op<float>);\n  auto div_cost = EIGEN_COST(scalar_div_cost<float>);\n  const auto floor_cost = EIGEN_COST(scalar_floor_op<float>);\n  const auto ceil_cost = EIGEN_COST(scalar_ceil_op<float>);\n  auto round_cost = EIGEN_COST(scalar_round_op<float>);\n  const auto cast_to_float_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_cast_op<int64_t, float>>::Cost;\n#undef EIGEN_COST\n\n  // Computing ops following\n  // tensorflow/core/kernels/image/crop_and_resize_op.cc at 08/25/2020. Op\n  // calculation differs from rough estimate in implementation, as it separates\n  // out cost per box from cost per pixel and cost per element.\n\n  // Since crop arguments are user controlled, check for overflow.\n  int64_t crop_area = MultiplyWithoutOverflow(crop_height, crop_width);\n  if (crop_area < 0)\n    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n                                   crop_height, \" with \", crop_width,\n                                   \" would overflow\");\n  int64_t crop_volume = MultiplyWithoutOverflow(crop_area, num_boxes);\n  if (crop_volume < 0)\n    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n                                   crop_area, \" with \", num_boxes,\n                                   \" would overflow\");\n  int64_t crop_depth = MultiplyWithoutOverflow(crop_height, num_boxes);\n  if (crop_depth < 0)\n    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n                                   crop_height, \" with \", num_boxes,\n                                   \" would overflow\");\n\n  // Ops for variables height_scale and width_scale.\n  int64_t ops = (sub_cost * 6 + mul_cost * 2 + div_cost * 2) * num_boxes;\n  // Ops for variable in_y.\n  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_depth;\n  // Ops for variable in_x (same computation across both branches).\n  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_volume;\n  // Specify op_cost based on the method.\n  if (use_bilinear_interp) {\n    // Ops for variables top_y_index, bottom_y_index, y_lerp.\n    ops += (floor_cost + ceil_cost + sub_cost) * crop_depth;\n    // Ops for variables left_x, right_x, x_lerp;\n    ops += (floor_cost + ceil_cost + sub_cost) * crop_volume;\n    // Ops for innermost loop across depth.\n    ops +=\n        (cast_to_float_cost * 4 + add_cost * 3 + sub_cost * 3 + mul_cost * 3) *\n        output_elements;\n  } else /* method == \"nearest\" */ {\n    // Ops for variables closest_x_index and closest_y_index.\n    ops += round_cost * 2 * crop_volume;\n    // Ops for innermost loop across depth.\n    ops += cast_to_float_cost * output_elements;\n  }\n  return PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\n}  // end namespace grappler\n}  // end namespace tensorflow\n", "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#ifndef TENSORFLOW_CORE_GRAPPLER_COSTS_OP_LEVEL_COST_ESTIMATOR_H_\n#define TENSORFLOW_CORE_GRAPPLER_COSTS_OP_LEVEL_COST_ESTIMATOR_H_\n\n#include <numeric>\n\n#include \"tensorflow/core/grappler/costs/cost_estimator.h\"\n#include \"tensorflow/core/grappler/costs/op_context.h\"\n#include \"tensorflow/core/grappler/costs/op_performance_data.pb.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/util/padding.h\"\n\nnamespace tensorflow {\nnamespace grappler {\n\nbool GetTensorShapeProtoFromTensorProto(const TensorProto& tensor_proto,\n                                        TensorShapeProto* tensor_shape_proto);\nTensorShapeProto MaybeGetMinimumShape(const TensorShapeProto& original_shape,\n                                      int rank, bool* found_unknown_shapes);\n\n// Node costs; an intermediate structure used within op level cost estimator.\nstruct NodeCosts {\n  // If this FLAG is true, override calculated compute time with a minimum\n  // value, instead of calculating it from num_compute_ops and compute ops/sec.\n  // For example, PredictIdentity, PredictVariable, PredictMetadata set this\n  // FLAG.\n  bool minimum_cost_op = false;\n\n  // Compute ops.\n  int64_t num_compute_ops = 0;\n\n  // Memory bytes accessed; note that these may be different to the size of\n  // tensors.\n  std::vector<int64_t> num_input_bytes_accessed;   // ordered by input tensors.\n  std::vector<int64_t> num_output_bytes_accessed;  // ordered by output ports.\n  int64_t internal_read_bytes = 0;\n  int64_t internal_write_bytes = 0;\n\n  // Convenience functions.\n  int64_t num_total_input_bytes() const {\n    return std::accumulate(num_input_bytes_accessed.begin(),\n                           num_input_bytes_accessed.end(), 0LL);\n  }\n  int64_t num_total_read_bytes() const {\n    return num_total_input_bytes() + internal_read_bytes;\n  }\n  int64_t num_total_output_bytes() const {\n    return std::accumulate(num_output_bytes_accessed.begin(),\n                           num_output_bytes_accessed.end(), 0LL);\n  }\n  int64_t num_total_write_bytes() const {\n    return num_total_output_bytes() + internal_write_bytes;\n  }\n  int64_t num_bytes_accessed() const {\n    return num_total_read_bytes() + num_total_write_bytes();\n  }\n\n  // Memory usage.\n  int64_t max_memory = 0;\n  int64_t persistent_memory = 0;\n  int64_t temporary_memory = 0;\n\n  // Stats.\n  int64_t num_nodes = 1;\n  int64_t num_nodes_with_unknown_shapes = 0;\n  int64_t num_nodes_with_unknown_op_type = 0;\n  int64_t num_nodes_with_pure_memory_op = 0;\n  bool inaccurate = false;\n\n  // TODO(dyoon): this is added for compatibility; some old code is hard to\n  // migrate; hence, using these as a backup. Once we clean up, we'll delete\n  // these fields. New code should not use these.\n  bool has_costs = false;\n  Costs costs;\n};\n\nclass OpLevelCostEstimator {\n public:\n  OpLevelCostEstimator();\n  virtual ~OpLevelCostEstimator() {}\n\n  virtual Costs PredictCosts(const OpContext& op_context) const;\n\n  // Returns basic device performance info.\n  virtual DeviceInfo GetDeviceInfo(const DeviceProperties& device) const;\n\n protected:\n  // TODO(dyoon): Consider to remove PredictOpCountBasedCosts() with OpInfo.\n  // Naive cost estimate based on the given operations count and total\n  // input/output tensor sizes of the given op_info combined.\n  Costs PredictOpCountBasedCost(double operations, const OpInfo& op_info) const;\n\n  // Naive cost estimate based on the given operations count and the given total\n  // io size in bytes. Sizes of op_info inputs and outputs are not taken into\n  // consideration.\n  Costs PredictOpCountBasedCost(double operations, double input_io_bytes,\n                                double output_io_bytes,\n                                const OpInfo& op_info) const;\n\n  // Top-level method cost function (PredictCosts calls this method to get\n  // NodeCosts, and then converts it to Costs). PredictNodeCosts() calls other\n  // Predict methods depending on op types.\n  Status PredictNodeCosts(const OpContext& op_context,\n                          NodeCosts* node_costs) const;\n\n  // Predict cost of an op for which no accurate estimator is defined.\n  Status PredictCostOfAnUnknownOp(const OpContext& op_context,\n                                  NodeCosts* node_costs) const;\n\n  // This family of routines predicts the costs to\n  // perform the specified TensorFlow Op on the\n  // device represented by a subclass. The default\n  // implementation just divides the operations to\n  // perform the op (from the \"Count\" routines,\n  // above) by the device peak operations per\n  // second.\n  // Implementation of costs other than\n  // execution_time is optional, depending on the\n  // device.\n  Status PredictNaryOp(const OpContext& op_context,\n                       NodeCosts* node_costs) const;\n  Status PredictConv2D(const OpContext& op_context,\n                       NodeCosts* node_costs) const;\n  Status PredictCwiseOp(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictConv2DBackpropInput(const OpContext& op_context,\n                                    NodeCosts* node_costs) const;\n  Status PredictConv2DBackpropFilter(const OpContext& op_context,\n                                     NodeCosts* node_costs) const;\n  Status PredictFusedConv2DBiasActivation(const OpContext& op_context,\n                                          NodeCosts* node_costs) const;\n  Status PredictMatMul(const OpContext& op_context,\n                       NodeCosts* node_costs) const;\n  Status PredictSparseTensorDenseMatMul(const OpContext& op_context,\n                                        NodeCosts* node_costs) const;\n  Status PredictNoOp(const OpContext& op_context, NodeCosts* node_costs) const;\n  Status PredictIdentity(const OpContext& op_context,\n                         NodeCosts* node_costs) const;\n  Status PredictVariable(const OpContext& op_context,\n                         NodeCosts* node_costs) const;\n  Status PredictBatchMatMul(const OpContext& op_context,\n                            NodeCosts* node_costs) const;\n  Status PredictMetadata(const OpContext& op_context,\n                         NodeCosts* node_costs) const;\n  Status PredictGatherOrSlice(const OpContext& op_context,\n                              NodeCosts* node_costs) const;\n  Status PredictScatter(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictMaxPool(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictMaxPoolGrad(const OpContext& op_context,\n                            NodeCosts* node_costs) const;\n  Status PredictAvgPool(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictAvgPoolGrad(const OpContext& op_context,\n                            NodeCosts* node_costs) const;\n  Status PredictFusedBatchNorm(const OpContext& op_context,\n                               NodeCosts* node_costs) const;\n  Status PredictFusedBatchNormGrad(const OpContext& op_context,\n                                   NodeCosts* node_costs) const;\n  Status PredictEinsum(const OpContext& op_context,\n                       NodeCosts* node_costs) const;\n  Status PredictAssignVariableOps(const OpContext& op_context,\n                                  NodeCosts* node_costs) const;\n  Status PredictPureMemoryOp(const OpContext& op_context,\n                             NodeCosts* node_costs) const;\n  Status PredictSoftmax(const OpContext& op_context,\n                        NodeCosts* node_costs) const;\n  Status PredictResizeBilinear(const OpContext& op_context,\n                               NodeCosts* node_costs) const;\n  Status PredictCropAndResize(const OpContext& op_context,\n                              NodeCosts* node_costs) const;\n\n  // Generic cost prediction method for fused operations.\n  Status PredictFusedOp(const OpContext& op_context,\n                        const std::vector<OpContext>& fused_op_contexts,\n                        NodeCosts* node_costs) const;\n\n  // Utility function for safe division. Returns 0\n  // if rhs is 0 or negative.\n  static double SafeDiv(const double lhs, const double rhs) {\n    if (rhs > 0) {\n      return lhs / rhs;\n    } else {\n      return 0.0;\n    }\n  }\n\n  // This family of routines counts the number of operations to perform the\n  // specified TensorFlow Op.\n  struct MatMulDimensions {\n    int m;\n    int n;\n    int k;\n  };\n  struct BatchMatMulDimensions {\n    std::vector<int> batch_dims;\n    MatMulDimensions matmul_dims;\n  };\n  struct ConvolutionDimensions {\n    int64_t batch;  // Batch size.\n    int64_t ix;     // Input size x.\n    int64_t iy;     // Input size y.\n    int64_t iz;     // Input depth.\n    int64_t kx;     // Kernel x.\n    int64_t ky;     // Kernel y.\n    int64_t kz;     // Kernel depth (in case of group convolution, this will be\n                    // smaller than input depth).\n    int64_t oz;     // Output depth.\n    int64_t ox;     // Output size x.\n    int64_t oy;     // Output size y.\n    int64_t sx;     // Stride x.\n    int64_t sy;     // Stride y.\n    Padding padding;  // SAME or VALID.\n  };\n  static int64_t CountConv2DOperations(const OpInfo& op_info,\n                                       bool* found_unknown_shapes);\n  static int64_t CountConv2DOperations(const OpInfo& op_info,\n                                       ConvolutionDimensions* conv_info,\n                                       bool* found_unknown_shapes);\n  static int64_t CountMatMulOperations(const OpInfo& op_info,\n                                       bool* found_unknown_shapes);\n  static int64_t CountMatMulOperations(const OpInfo& op_info,\n                                       MatMulDimensions* mat_mul,\n                                       bool* found_unknown_shapes);\n  bool GenerateBatchMatmulContextFromEinsum(const OpContext& einsum_context,\n                                            OpContext* batch_matmul_context,\n                                            bool* found_unknown_shapes) const;\n  static int64_t CountBatchMatMulOperations(const OpInfo& op_info,\n                                            bool* found_unknown_shapes);\n  static int64_t CountBatchMatMulOperations(\n      const OpInfo& op_info, BatchMatMulDimensions* batch_mat_mul,\n      bool* found_unknown_shapes);\n  static int64_t CountConv2DBackpropInputOperations(\n      const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n      bool* found_unknown_shapes);\n  static int64_t CountConv2DBackpropFilterOperations(\n      const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n      bool* found_unknown_shapes);\n\n  // Calculate the element count of an input/output tensor.\n  static int64_t CalculateTensorElementCount(\n      const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes);\n\n  // Calculate the total size in bytes of an input/output tensor.\n  static int64_t CalculateTensorSize(const OpInfo::TensorProperties& tensor,\n                                     bool* found_unknown_shapes);\n\n  // Calculate the element count of the largest\n  // input of specified TensorFlow op.\n  static int64_t CalculateLargestInputCount(const OpInfo& op_info,\n                                            bool* found_unknown_shapes);\n\n  // Calculate the total size in bytes of the all\n  // the inputs of specified TensorFlow op.\n  static int64_t CalculateInputSize(const OpInfo& op_info,\n                                    bool* found_unknown_shapes);\n\n  // Same, but a vector format: one for each input.\n  static std::vector<int64_t> CalculateInputTensorSize(\n      const OpInfo& op_info, bool* found_unknown_shapes);\n\n  // Calculate the total size in bytes of the all\n  // the outputs of specified TensorFlow op.\n  static int64_t CalculateOutputSize(const OpInfo& op_info,\n                                     bool* found_unknown_shapes);\n\n  // Same, but a vector format: one for each output.\n  static std::vector<int64_t> CalculateOutputTensorSize(\n      const OpInfo& op_info, bool* found_unknown_shapes);\n\n  // For convolution and its grad ops.\n  static ConvolutionDimensions ConvolutionDimensionsFromInputs(\n      const TensorShapeProto& original_image_shape,\n      const TensorShapeProto& original_filter_shape, const OpInfo& op_info,\n      bool* found_unknown_shapes);\n\n  // For Pooling, FusedBatchNorm, and their grad ops.\n  static StatusOr<ConvolutionDimensions> OpDimensionsFromInputs(\n      const TensorShapeProto& original_image_shape, const OpInfo& op_info,\n      bool* found_unknown_shapes);\n\n  // Helper to construct child operation contexts for the component operations\n  // of fused ops.\n  static OpContext FusedChildContext(\n      const OpContext& parent, const string& op_name,\n      const OpInfo::TensorProperties& output,\n      const std::vector<OpInfo::TensorProperties>& inputs);\n\n  // Helper to construct tensor shapes.\n  static OpInfo::TensorProperties DescribeTensor(\n      DataType type, const std::vector<int64_t>& dims);\n\n  // Helper method for building common case NodeCosts.\n  static Status PredictDefaultNodeCosts(const int64_t num_compute_ops,\n                                        const OpContext& op_context,\n                                        bool* found_unknown_shapes,\n                                        NodeCosts* node_costs);\n\n protected:\n  std::map<string, int> elementwise_ops_;\n  typedef std::function<Status(const OpContext& op_context, NodeCosts*)>\n      CostImpl;\n  std::map<string, CostImpl> device_cost_impl_;\n  // If true, assume compute and memory overlap; hence, the op cost is max of\n  // compute_time and memory_time, instead of sum of those two.\n  bool compute_memory_overlap_;\n  std::set<string> persistent_ops_;\n\n private:\n  friend class OpLevelCostEstimatorTest;\n};\n\n}  // end namespace grappler\n}  // end namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_GRAPPLER_COSTS_OP_LEVEL_COST_ESTIMATOR_H_\n", "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/grappler/costs/op_level_cost_estimator.h\"\n\n#include <unordered_set>\n\n#include \"tensorflow/core/framework/attr_value.pb.h\"\n#include \"tensorflow/core/framework/attr_value_util.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor.pb.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/platform/status_matchers.h\"\n#include \"tensorflow/core/platform/test.h\"\n#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n\nnamespace tensorflow {\nnamespace grappler {\n\nnamespace {\n\n// TODO(dyoon): Consider to use this Test class for all the test cases, and then\n// remove friend in the OpLevelCostEstimator class header.\nclass TestOpLevelCostEstimator : public OpLevelCostEstimator {\n public:\n  TestOpLevelCostEstimator() {\n    compute_memory_overlap_ = true;\n    device_info_ = DeviceInfo();\n  }\n  ~TestOpLevelCostEstimator() override {}\n\n  void SetDeviceInfo(const DeviceInfo& device_info) {\n    device_info_ = device_info;\n  }\n\n  void SetComputeMemoryOverlap(bool value) { compute_memory_overlap_ = value; }\n\n protected:\n  DeviceInfo GetDeviceInfo(const DeviceProperties& device) const override {\n    return device_info_;\n  }\n\n  DeviceInfo device_info_;\n};\n\nvoid ExpectZeroCost(const Costs& cost) {\n  EXPECT_TRUE(cost.inaccurate);\n  EXPECT_EQ(cost.compute_time, Costs::Duration::zero());\n  EXPECT_EQ(cost.execution_time, Costs::Duration::zero());\n  EXPECT_EQ(cost.memory_time, Costs::Duration::zero());\n}\n\n// Wrangles the minimum number of proto fields to set up a matrix.\nvoid DescribeMatrix(int rows, int columns, OpInfo* op_info) {\n  auto input = op_info->add_inputs();\n  auto shape = input->mutable_shape();\n  auto shape_rows = shape->add_dim();\n  shape_rows->set_size(rows);\n  auto shape_columns = shape->add_dim();\n  shape_columns->set_size(columns);\n  input->set_dtype(DT_FLOAT);\n}\n\nvoid SetCpuDevice(OpInfo* op_info) {\n  auto device = op_info->mutable_device();\n  device->set_type(\"CPU\");\n  device->set_num_cores(10);\n  device->set_bandwidth(10000000);  // 10000000 KB/s = 10 GB/s\n  device->set_frequency(1000);      // 1000 Mhz = 1 GHz\n}\n\n// Returns an OpInfo for MatMul with the minimum set of fields set up.\nOpContext DescribeMatMul(int m, int n, int l, int k) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"MatMul\");\n\n  DescribeMatrix(m, l, &op_context.op_info);\n  DescribeMatrix(k, n, &op_context.op_info);\n  return op_context;\n}\n\n// Wrangles the minimum number of proto fields to set up an input of\n// arbitrary rank and type.\nvoid DescribeArbitraryRankInput(const std::vector<int>& dims, DataType dtype,\n                                OpInfo* op_info) {\n  auto input = op_info->add_inputs();\n  input->set_dtype(dtype);\n  auto shape = input->mutable_shape();\n  for (auto d : dims) {\n    shape->add_dim()->set_size(d);\n  }\n}\n\n// Wrangles the minimum number of proto fields to set up an output of\n// arbitrary rank and type.\nvoid DescribeArbitraryRankOutput(const std::vector<int>& dims, DataType dtype,\n                                 OpInfo* op_info) {\n  auto output = op_info->add_outputs();\n  output->set_dtype(dtype);\n  auto shape = output->mutable_shape();\n  for (auto d : dims) {\n    shape->add_dim()->set_size(d);\n  }\n}\n\n// Returns an OpInfo for a SparseTensorDenseMatMul\nOpContext DescribeSparseTensorDenseMatMul(const int nnz_a,\n                                          const std::vector<int>& dims_b,\n                                          const std::vector<int>& dims_out) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"SparseTensorDenseMatMul\");\n\n  DescribeArbitraryRankInput({nnz_a, 2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput({nnz_a}, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput(dims_b, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankOutput(dims_out, DT_FLOAT, &op_context.op_info);\n  return op_context;\n}\n\n// Returns an OpInfo for an XlaEinsum\nOpContext DescribeXlaEinsum(const std::vector<int>& dims_a,\n                            const std::vector<int>& dims_b,\n                            const string& equation) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"XlaEinsum\");\n  AttrValue equation_attribute;\n  equation_attribute.set_s(equation);\n  (*op_context.op_info.mutable_attr())[\"equation\"] = equation_attribute;\n  if (!dims_a.empty())\n    DescribeArbitraryRankInput(dims_a, DT_FLOAT, &op_context.op_info);\n  if (!dims_b.empty())\n    DescribeArbitraryRankInput(dims_b, DT_FLOAT, &op_context.op_info);\n  return op_context;\n}\n\n// Returns an OpInfo for an Einsum\nOpContext DescribeEinsum(const std::vector<int>& dims_a,\n                         const std::vector<int>& dims_b,\n                         const string& equation) {\n  OpContext op_context = DescribeXlaEinsum(dims_a, dims_b, equation);\n  op_context.op_info.set_op(\"Einsum\");\n  return op_context;\n}\n\nvoid DescribeDummyTensor(OpInfo::TensorProperties* tensor) {\n  // Intentionally leave the tensor shape and type information missing.\n}\n\n// Wrangles the minimum number of proto fields to set up a 1D Tensor for cost\n// estimation purposes.\nvoid DescribeTensor1D(int dim0, OpInfo::TensorProperties* tensor) {\n  auto shape = tensor->mutable_shape();\n  shape->add_dim()->set_size(dim0);\n  tensor->set_dtype(DT_FLOAT);\n}\n\n// Wrangles the minimum number of proto fields to set up a 4D Tensor for cost\n// estimation purposes.\nvoid DescribeTensor4D(int dim0, int dim1, int dim2, int dim3,\n                      OpInfo::TensorProperties* tensor) {\n  auto shape = tensor->mutable_shape();\n  shape->add_dim()->set_size(dim0);\n  shape->add_dim()->set_size(dim1);\n  shape->add_dim()->set_size(dim2);\n  shape->add_dim()->set_size(dim3);\n  tensor->set_dtype(DT_FLOAT);\n}\n\n// Wrangles the minimum number of proto fields to set up a 4D Tensor for cost\n// estimation purposes.\nvoid DescribeTensor5D(int dim0, int dim1, int dim2, int dim3, int dim4,\n                      OpInfo::TensorProperties* tensor) {\n  auto shape = tensor->mutable_shape();\n  shape->add_dim()->set_size(dim0);\n  shape->add_dim()->set_size(dim1);\n  shape->add_dim()->set_size(dim2);\n  shape->add_dim()->set_size(dim3);\n  shape->add_dim()->set_size(dim4);\n  tensor->set_dtype(DT_FLOAT);\n}\n\n// DescribeConvolution constructs an OpContext for a Conv2D applied to an input\n// tensor with shape (batch, ix, iy, iz1) and a kernel tensor with shape\n// (kx, ky, iz2, oz).\nOpContext DescribeConvolution(int batch, int ix, int iy, int iz1, int iz2,\n                              int kx, int ky, int oz) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"Conv2D\");\n\n  DescribeTensor4D(batch, ix, iy, iz1, op_context.op_info.add_inputs());\n  DescribeTensor4D(kx, ky, iz2, oz, op_context.op_info.add_inputs());\n\n  return op_context;\n}\n\n// Describe DepthwiseConvolution constructs an OpContext for a\n// DepthwiseConv2dNative applied to an input\n// tensor with shape (batch, ix, iy, iz1) and a kernel tensor with shape\n// (kx, ky, iz2, cm). cm is channel multiplier\n\nOpContext DescribeDepthwiseConv2dNative(int batch, int ix, int iy, int iz1,\n                                        int iz2, int kx, int ky, int cm) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"DepthwiseConv2dNative\");\n\n  DescribeTensor4D(batch, ix, iy, iz1, op_context.op_info.add_inputs());\n  DescribeTensor4D(kx, ky, iz2, cm, op_context.op_info.add_inputs());\n\n  return op_context;\n}\n\n// DescribeFusedConv2DBiasActivation constructs an OpContext for a\n// FusedConv2DBiasActivation applied to a convolution input tensor with shape\n// (batch, ix, iy, iz1), a kernel tensor with shape (kx, ky, iz2, oz), a\n// bias tensor with shape (oz), a side input tensor with shape\n// (batch, ox, oy, oz) if has_side_input is set, and two scaling tensors with\n// shape (1). If a vectorized channel format is chosen (NCHW_VECT_C, e.g.) we'll\n// default to 4 (the vector size most often used with this format on NVIDIA\n// platforms) for the major channel size, and divide the input channel size by\n// that amount.\n//\n// Note that this assumes the NHWC data format.\nOpContext DescribeFusedConv2DBiasActivation(int batch, int ix, int iy, int iz1,\n                                            int iz2, int kx, int ky, int ox,\n                                            int oy, int oz, bool has_side_input,\n                                            const string& data_format,\n                                            const string& filter_format) {\n  const int kVecWidth = 4;\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"FusedConv2DBiasActivation\");\n  auto* attr_data_format = op_context.op_info.mutable_attr();\n  SetAttrValue(data_format, &(*attr_data_format)[\"data_format\"]);\n  auto* attr_filter_format = op_context.op_info.mutable_attr();\n  SetAttrValue(filter_format, &(*attr_filter_format)[\"filter_format\"]);\n  if (data_format == \"NHWC\") {\n    DescribeTensor4D(batch, ix, iy, iz1, op_context.op_info.add_inputs());\n  } else if (data_format == \"NCHW\") {\n    DescribeTensor4D(batch, iz1, ix, iy, op_context.op_info.add_inputs());\n  } else {\n    // Use the NCHW_VECT_C format.\n    EXPECT_EQ(data_format, \"NCHW_VECT_C\");\n    EXPECT_EQ(iz1 % kVecWidth, 0);\n    DescribeTensor5D(batch, iz1 / kVecWidth, ix, iy, kVecWidth,\n                     op_context.op_info.add_inputs());\n  }\n  if (filter_format == \"HWIO\") {\n    DescribeTensor4D(kx, ky, iz2, oz, op_context.op_info.add_inputs());\n  } else if (filter_format == \"OIHW\") {\n    DescribeTensor4D(oz, iz2, kx, ky, op_context.op_info.add_inputs());\n  } else {\n    EXPECT_EQ(filter_format, \"OIHW_VECT_I\");\n    EXPECT_EQ(iz2 % kVecWidth, 0);\n    // Use the OIHW_VECT_I format.\n    DescribeTensor5D(oz, iz2 / kVecWidth, kx, ky, kVecWidth,\n                     op_context.op_info.add_inputs());\n  }\n  DescribeTensor1D(oz, op_context.op_info.add_inputs());\n\n  // Add the side_input, if any.\n  auto side_input = op_context.op_info.add_inputs();\n  if (has_side_input) {\n    if (data_format == \"NHWC\") {\n      DescribeTensor4D(batch, ox, oy, oz, side_input);\n    } else if (data_format == \"NCHW\") {\n      DescribeTensor4D(batch, oz, ox, oy, side_input);\n    } else {\n      // Use the NCHW_VECT_C format.\n      EXPECT_EQ(data_format, \"NCHW_VECT_C\");\n      EXPECT_EQ(oz % kVecWidth, 0);\n      DescribeTensor5D(batch, oz / kVecWidth, ox, oy, kVecWidth, side_input);\n    }\n  }\n\n  // Add the scaling tensors.\n  DescribeTensor1D(1, op_context.op_info.add_inputs());\n  DescribeTensor1D(1, op_context.op_info.add_inputs());\n\n  return op_context;\n}\n\n// DescribeUnaryOp constructs an OpContext for the given operation applied to\n// a 4-tensor with shape (size1, 1, 1, 1).\nOpContext DescribeUnaryOp(const string& op, int size1) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(op);\n\n  DescribeTensor4D(size1, 1, 1, 1, op_context.op_info.add_inputs());\n  DescribeTensor4D(size1, 1, 1, 1, op_context.op_info.add_outputs());\n\n  return op_context;\n}\n\n// DescribeBinaryOp constructs an OpContext for the given operation applied to\n// a 4-tensor with dimensions (size1, 1, 1, 1) and a 4-tensor with dimensions\n// (2 * size1, size2, 1, 1).\n//\n// The choice of dimension here is arbitrary, and is used strictly to test the\n// cost model for applying elementwise operations to tensors with unequal\n// dimension values.\nOpContext DescribeBinaryOp(const string& op, int size1, int size2) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(op);\n\n  DescribeTensor4D(size1, 1, 1, 1, op_context.op_info.add_inputs());\n  DescribeTensor4D(2 * size1, size2, 1, 1, op_context.op_info.add_inputs());\n  DescribeTensor4D(2 * size1, size2, 1, 1, op_context.op_info.add_outputs());\n\n  return op_context;\n}\n\n// DescribeBiasAdd constructs an OpContext for a BiasAdd applied to a 4-tensor\n// with dimensions (1, 1, size2, size1) and a bias with dimension (size1),\n// according to the constraint that the bias must be 1D with size equal to that\n// of the last dimension of the input value.\nOpContext DescribeBiasAdd(int size1, int size2) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"BiasAdd\");\n\n  DescribeTensor4D(1, 1, size2, size1, op_context.op_info.add_inputs());\n  DescribeTensor1D(size1, op_context.op_info.add_inputs());\n  DescribeTensor4D(1, 1, size2, size1, op_context.op_info.add_outputs());\n\n  return op_context;\n}\n\nint GetOutputSize(const int x, const int k, const int s,\n                  const string& padding) {\n  if (padding == \"SAME\") {\n    return (x + s - 1) / s;\n  } else {\n    return (x - k + s) / s;\n  }\n}\n\nstd::vector<int> GetPoolingOutputSize(const std::vector<int>& input,\n                                      const std::vector<int>& ksize,\n                                      const std::vector<int>& strides,\n                                      const string& data_format,\n                                      const string& padding) {\n  // h, w, and c indices: default with NHWC.\n  int h_index = 1;\n  int w_index = 2;\n  int c_index = 3;\n  if (data_format == \"NCHW\") {\n    h_index = 2;\n    w_index = 3;\n    c_index = 1;\n  }\n  // Extract parameters.\n  int n = input[0];\n  int h = input[h_index];\n  int w = input[w_index];\n  int c = input[c_index];\n  int sx = strides[h_index];\n  int sy = strides[w_index];\n  int kx = ksize[h_index];\n  int ky = ksize[w_index];\n\n  // Output activation size: default with VALID padding.\n  int ho = GetOutputSize(h, kx, sx, padding);\n  int wo = GetOutputSize(w, ky, sy, padding);\n\n  std::vector<int> output;\n  if (data_format == \"NHWC\") {\n    output = {n, ho, wo, c};\n  } else {\n    output = {n, c, ho, wo};\n  }\n  return output;\n}\n\n// Helper functions for testing GetTensorShapeProtoFromTensorProto().\nvoid GetTensorProto(const DataType dtype, const std::vector<int64_t>& shape,\n                    const std::vector<int64_t> values,\n                    const bool tensor_content, TensorProto* tensor_proto) {\n  tensor_proto->Clear();\n  TensorProto temp_tensor_proto;\n  temp_tensor_proto.set_dtype(dtype);\n  for (const auto& x : shape) {\n    temp_tensor_proto.mutable_tensor_shape()->add_dim()->set_size(x);\n  }\n  for (const auto& x : values) {\n    if (dtype == DT_INT64) {\n      temp_tensor_proto.add_int64_val(x);\n    } else if (dtype == DT_INT32 || dtype == DT_INT16 || dtype == DT_INT8 ||\n               dtype == DT_UINT8) {\n      temp_tensor_proto.add_int_val(x);\n    } else if (dtype == DT_UINT32) {\n      temp_tensor_proto.add_uint32_val(x);\n    } else if (dtype == DT_UINT64) {\n      temp_tensor_proto.add_uint64_val(x);\n    } else {\n      CHECK(false) << \"Unsupported dtype: \" << dtype;\n    }\n  }\n  Tensor tensor(dtype);\n  CHECK(tensor.FromProto(temp_tensor_proto));\n  if (tensor_content) {\n    tensor.AsProtoTensorContent(tensor_proto);\n  } else {\n    tensor.AsProtoField(tensor_proto);\n  }\n}\n\nOpContext DescribePoolingOp(const string& op_name, const std::vector<int>& x,\n                            const std::vector<int>& ksize,\n                            const std::vector<int>& strides,\n                            const string& data_format, const string& padding) {\n  OpContext op_context;\n  auto& op_info = op_context.op_info;\n  SetCpuDevice(&op_info);\n  op_info.set_op(op_name);\n\n  const std::vector<int> y =\n      GetPoolingOutputSize(x, ksize, strides, data_format, padding);\n  if (op_name == \"AvgPool\" || op_name == \"MaxPool\") {\n    // input: x, output: y.\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_inputs());\n    DescribeTensor4D(y[0], y[1], y[2], y[3], op_info.add_outputs());\n  } else if (op_name == \"AvgPoolGrad\") {\n    // input: x's shape, y_grad, output: x_grad.\n    DescribeArbitraryRankInput({4}, DT_INT32, &op_info);\n    auto* tensor_proto = op_info.mutable_inputs(0)->mutable_value();\n    GetTensorProto(DT_INT32, {4}, {x[0], x[1], x[2], x[3]},\n                   /*tensor_content=*/false, tensor_proto);\n    DescribeTensor4D(y[0], y[1], y[2], y[3], op_info.add_inputs());\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_outputs());\n  } else if (op_name == \"MaxPoolGrad\") {\n    // input: x, y, y_grad, output: x_grad.\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_inputs());\n    DescribeTensor4D(y[0], y[1], y[2], y[3], op_info.add_inputs());\n    DescribeTensor4D(y[0], y[1], y[2], y[3], op_info.add_inputs());\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_outputs());\n  }\n  auto* attr = op_info.mutable_attr();\n  SetAttrValue(data_format, &(*attr)[\"data_format\"]);\n  SetAttrValue(padding, &(*attr)[\"padding\"]);\n  SetAttrValue(strides, &(*attr)[\"strides\"]);\n  SetAttrValue(ksize, &(*attr)[\"ksize\"]);\n  return op_context;\n}\n\nOpContext DescribeFusedBatchNorm(const bool is_training, const bool is_grad,\n                                 const std::vector<int>& x,\n                                 const string& data_format) {\n  // First, get MaxPool op info with unit stride and unit window.\n  OpContext op_context = DescribePoolingOp(\"MaxPool\", x, {1, 1, 1, 1},\n                                           {1, 1, 1, 1}, data_format, \"SAME\");\n  auto& op_info = op_context.op_info;\n  // Override op name.\n  if (is_grad) {\n    op_info.set_op(\"FusedBatchNormGrad\");\n  } else {\n    op_info.set_op(\"FusedBatchNorm\");\n  }\n\n  // Add additional input output tensors.\n  if (is_grad) {\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_inputs());\n  }\n  int num_1d_inputs = is_grad ? 3 : 4;\n  for (int i = 0; i < num_1d_inputs; i++) {\n    auto* tensor = op_info.add_inputs();\n    auto* shape = tensor->mutable_shape();\n    shape->add_dim()->set_size(x[3]);\n    tensor->set_dtype(DT_FLOAT);\n  }\n  for (int i = 0; i < 4; i++) {\n    auto* tensor = op_info.add_outputs();\n    auto* shape = tensor->mutable_shape();\n    shape->add_dim()->set_size(x[3]);\n    tensor->set_dtype(DT_FLOAT);\n  }\n\n  // Delete unnecessary attr.\n  auto* attr = op_context.op_info.mutable_attr();\n  attr->erase(\"ksize\");\n  attr->erase(\"strides\");\n  attr->erase(\"padding\");\n\n  // Additional attrs for FusedBatchNorm.\n  SetAttrValue(is_training, &(*attr)[\"is_training\"]);\n\n  return op_context;\n}\n}  // namespace\n\nclass OpLevelCostEstimatorTest : public ::testing::Test {\n protected:\n  using BatchMatMulDimensions = OpLevelCostEstimator::BatchMatMulDimensions;\n\n  Costs PredictCosts(const OpContext& op_context) const {\n    return estimator_.PredictCosts(op_context);\n  }\n\n  int64_t CountMatMulOperations(const OpInfo& op_info,\n                                bool* found_unknown_shapes) const {\n    return estimator_.CountMatMulOperations(op_info, found_unknown_shapes);\n  }\n\n  int64_t CountBatchMatMulOperations(const OpInfo& op_info,\n                                     bool* found_unknown_shapes) const {\n    return estimator_.CountBatchMatMulOperations(op_info, found_unknown_shapes);\n  }\n\n  int64_t CountBatchMatMulOperations(const OpInfo& op_info,\n                                     BatchMatMulDimensions* batch_mat_mul,\n                                     bool* found_unknown_shapes) const {\n    return estimator_.CountBatchMatMulOperations(op_info, batch_mat_mul,\n                                                 found_unknown_shapes);\n  }\n\n  void SetComputeMemoryOverlap(bool value) {\n    estimator_.compute_memory_overlap_ = value;\n  }\n\n  void ValidateOpDimensionsFromInputs(const int n, const int h, const int w,\n                                      const int c, const int kx, const int ky,\n                                      const int sx, const int sy,\n                                      const string& data_format,\n                                      const string& padding) {\n    OpContext op_context;\n    int ho;\n    int wo;\n    if (data_format == \"NHWC\") {\n      op_context = DescribePoolingOp(\"MaxPool\", {n, h, w, c}, {1, kx, ky, 1},\n                                     {1, sx, sy, 1}, \"NHWC\", padding);\n      ho = op_context.op_info.outputs(0).shape().dim(1).size();\n      wo = op_context.op_info.outputs(0).shape().dim(2).size();\n    } else {\n      op_context = DescribePoolingOp(\"MaxPool\", {n, c, h, w}, {1, 1, kx, ky},\n                                     {1, 1, sx, sy}, \"NCHW\", padding);\n      ho = op_context.op_info.outputs(0).shape().dim(2).size();\n      wo = op_context.op_info.outputs(0).shape().dim(3).size();\n    }\n\n    bool found_unknown_shapes;\n    TF_ASSERT_OK_AND_ASSIGN(\n        auto dims, OpLevelCostEstimator::OpDimensionsFromInputs(\n                       op_context.op_info.inputs(0).shape(), op_context.op_info,\n                       &found_unknown_shapes));\n    Padding padding_enum;\n    if (padding == \"VALID\") {\n      padding_enum = Padding::VALID;\n    } else {\n      padding_enum = Padding::SAME;\n    }\n    EXPECT_EQ(n, dims.batch);\n    EXPECT_EQ(h, dims.ix);\n    EXPECT_EQ(w, dims.iy);\n    EXPECT_EQ(c, dims.iz);\n    EXPECT_EQ(kx, dims.kx);\n    EXPECT_EQ(ky, dims.ky);\n    EXPECT_EQ(sx, dims.sx);\n    EXPECT_EQ(sy, dims.sy);\n    EXPECT_EQ(ho, dims.ox);\n    EXPECT_EQ(wo, dims.oy);\n    EXPECT_EQ(c, dims.oz);\n    EXPECT_EQ(padding_enum, dims.padding);\n  }\n\n  StatusOr<OpLevelCostEstimator::ConvolutionDimensions>\n  CallOpDimensionsFromInputs(const int n, const int h, const int w, const int c,\n                             const int kx, const int ky, const int sx,\n                             const int sy, const string& data_format,\n                             const string& padding) {\n    OpContext op_context;\n\n    const std::vector<int> x = {n, h, w, c};\n    const std::vector<int> ksize = {1, kx, ky, 1};\n    std::vector<int> strides;\n    if (data_format == \"NHWC\") {\n      strides = {1, sy, sx, 1};\n    } else {\n      strides = {1, 1, sy, sx};\n    }\n\n    auto& op_info = op_context.op_info;\n    SetCpuDevice(&op_info);\n    op_info.set_op(\"MaxPool\");\n\n    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_inputs());\n    auto* attr = op_info.mutable_attr();\n    SetAttrValue(data_format, &(*attr)[\"data_format\"]);\n    SetAttrValue(padding, &(*attr)[\"padding\"]);\n    SetAttrValue(strides, &(*attr)[\"strides\"]);\n    SetAttrValue(ksize, &(*attr)[\"ksize\"]);\n    bool found_unknown_shapes;\n    return OpLevelCostEstimator::OpDimensionsFromInputs(\n        op_context.op_info.inputs(0).shape(), op_context.op_info,\n        &found_unknown_shapes);\n  }\n\n  OpLevelCostEstimator estimator_;\n};\n\nclass OpLevelBatchMatMulCostEstimatorTest\n    : public OpLevelCostEstimatorTest,\n      public ::testing::WithParamInterface<const char*> {\n protected:\n  // Returns an OpInfo for a BatchMatMul\n  OpContext DescribeBatchMatMul(const std::vector<int>& dims_a,\n                                const std::vector<int>& dims_b) {\n    OpContext op_context;\n    SetCpuDevice(&op_context.op_info);\n    op_context.op_info.set_op(GetParam());\n\n    DescribeArbitraryRankInput(dims_a, DT_FLOAT, &op_context.op_info);\n    DescribeArbitraryRankInput(dims_b, DT_FLOAT, &op_context.op_info);\n    return op_context;\n  }\n\n  int64_t CountBatchMatMulOperations(const OpInfo& op_info,\n                                     bool* found_unknown_shapes) const {\n    return OpLevelCostEstimatorTest::CountBatchMatMulOperations(\n        op_info, found_unknown_shapes);\n  }\n\n  int64_t CountBatchMatMulDimProduct(const OpInfo& op_info,\n                                     bool* found_unknown_shapes) const {\n    BatchMatMulDimensions batch_mat_mul;\n\n    batch_mat_mul.matmul_dims.n = 0;\n    batch_mat_mul.matmul_dims.m = 0;\n    batch_mat_mul.matmul_dims.k = 0;\n\n    OpLevelCostEstimatorTest::CountBatchMatMulOperations(\n        op_info, &batch_mat_mul, found_unknown_shapes);\n    int dimension_product = 1;\n    for (auto dim : batch_mat_mul.batch_dims) dimension_product *= dim;\n\n    dimension_product *= batch_mat_mul.matmul_dims.n;\n    dimension_product *= batch_mat_mul.matmul_dims.m;\n    dimension_product *= batch_mat_mul.matmul_dims.k;\n\n    return dimension_product;\n  }\n};\n\nTEST_F(OpLevelCostEstimatorTest, TestPersistentOpCosts) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  std::unordered_set<string> persistent_ops = {\n      \"Const\",       \"Variable\",       \"VariableV2\", \"AutoReloadVariable\",\n      \"VarHandleOp\", \"ReadVariableOp\",\n  };\n  // Minimum cost for all persistent ops.\n  for (const auto& op : persistent_ops) {\n    op_context.op_info.set_op(op);\n    auto cost = estimator_.PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(0), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(1), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(1), cost.execution_time);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestGatherCosts) {\n  std::vector<std::string> gather_ops = {\"Gather\", \"GatherNd\", \"GatherV2\"};\n\n  for (const auto& op : gather_ops) {\n    OpContext op_context;\n    SetCpuDevice(&op_context.op_info);\n    op_context.op_info.set_op(op);\n\n    // Huge first input shouldn't affect Gather execution and memory costs.\n    DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n    DescribeArbitraryRankInput({16}, DT_INT64, &op_context.op_info);\n    DescribeArbitraryRankOutput({16, 10}, DT_FLOAT, &op_context.op_info);\n\n    auto cost = estimator_.PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(130), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(16), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(146), cost.execution_time);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestGatherCostsWithoutOutput) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"Gather\");\n\n  // Huge first input shouldn't affect Gather execution and memory costs.\n  DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankInput({16}, DT_INT64, &op_context.op_info);\n\n  auto cost = estimator_.PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(0), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(0), cost.execution_time);\n  EXPECT_EQ(1, cost.num_ops_total);\n  EXPECT_TRUE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestSliceCosts) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"Slice\");\n\n  // Huge first input shouldn't affect Slice execution and memory costs.\n  DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankOutput({10, 10}, DT_FLOAT, &op_context.op_info);\n\n  auto cost = estimator_.PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(81), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(10), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(91), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestStridedSliceCosts) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"StridedSlice\");\n\n  // Huge first input shouldn't affect StridedSlice execution and memory costs.\n  DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankInput({2}, DT_INT64, &op_context.op_info);\n  DescribeArbitraryRankOutput({10, 10}, DT_FLOAT, &op_context.op_info);\n\n  auto cost = estimator_.PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(81), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(10), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(91), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, TestScatterOps) {\n  std::vector<string> scatter_ops = {\"ScatterAdd\",   \"ScatterDiv\", \"ScatterMax\",\n                                     \"ScatterMin\",   \"ScatterMul\", \"ScatterSub\",\n                                     \"ScatterUpdate\"};\n  for (const auto& op : scatter_ops) {\n    // Test updates.shape = indices.shape + ref.shape[1:]\n    {\n      OpContext op_context;\n      SetCpuDevice(&op_context.op_info);\n      op_context.op_info.set_op(op);\n      // Huge first dimension in input shouldn't affect Scatter execution and\n      // memory costs.\n      DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n      DescribeArbitraryRankInput({16}, DT_INT64, &op_context.op_info);\n      DescribeArbitraryRankInput({16, 10}, DT_FLOAT, &op_context.op_info);\n      DescribeArbitraryRankOutput({10000000, 10}, DT_FLOAT,\n                                  &op_context.op_info);\n\n      auto cost = estimator_.PredictCosts(op_context);\n      EXPECT_EQ(Costs::Duration(205), cost.memory_time);\n      EXPECT_EQ(Costs::Duration(16), cost.compute_time);\n      EXPECT_EQ(Costs::Duration(221), cost.execution_time);\n      EXPECT_EQ(cost.num_ops_total, 1);\n      EXPECT_FALSE(cost.inaccurate);\n      EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n      EXPECT_EQ(cost.temporary_memory, 0);\n      EXPECT_EQ(cost.persistent_memory, 0);\n    }\n\n    // Test updates.shape = [] and INT32 indices\n    {\n      OpContext op_context;\n      SetCpuDevice(&op_context.op_info);\n      op_context.op_info.set_op(op);\n      // Huge first dimension in input shouldn't affect Scatter execution and\n      // memory costs.\n      DescribeArbitraryRankInput({10000000, 10}, DT_FLOAT, &op_context.op_info);\n      DescribeArbitraryRankInput({16}, DT_INT32, &op_context.op_info);\n      DescribeArbitraryRankInput({}, DT_FLOAT, &op_context.op_info);\n      DescribeArbitraryRankOutput({10000000, 10}, DT_FLOAT,\n                                  &op_context.op_info);\n\n      auto cost = estimator_.PredictCosts(op_context);\n      EXPECT_EQ(Costs::Duration(135), cost.memory_time);\n      EXPECT_EQ(Costs::Duration(16), cost.compute_time);\n      EXPECT_EQ(Costs::Duration(151), cost.execution_time);\n      EXPECT_EQ(1, cost.num_ops_total);\n      EXPECT_FALSE(cost.inaccurate);\n      EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n    }\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, BiasAddExecutionTime) {\n  auto cost = PredictCosts(DescribeBiasAdd(1000, 10));\n  EXPECT_EQ(Costs::Duration(8400), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(1000), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(9400), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, Conv2DExecutionTime) {\n  auto cost = PredictCosts(DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(233780), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(354877440), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(355111220), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, InvalidConv2DConfig) {\n  // Convolution ops.\n  const std::vector<std::string> conv_ops = {\n      \"Conv2D\",\n      \"Conv2DBackpropFilter\",\n      \"Conv2DBackpropInput\",\n      \"DepthwiseConv2dNative\",\n      \"DepthwiseConv2dNativeBackpropFilter\",\n      \"DepthwiseConv2dNativeBackpropInput\",\n  };\n  // A valid Conv2D config.\n  const std::vector<int> valid_conv_config = {16, 19, 19, 48, 48, 5, 5, 256};\n  for (const auto& op : conv_ops) {\n    // Test with setting one value in conv config to zero.\n    // PredictCosts() should return zero costs.\n    for (int i = 0; i < valid_conv_config.size(); ++i) {\n      std::vector<int> conv_config(valid_conv_config);\n      conv_config[i] = 0;\n      auto op_context = DescribeConvolution(\n          conv_config[0], conv_config[1], conv_config[2], conv_config[3],\n          conv_config[4], conv_config[5], conv_config[6], conv_config[7]);\n      op_context.op_info.set_op(op);\n      auto cost = PredictCosts(op_context);\n      EXPECT_EQ(Costs::Duration(0), cost.memory_time);\n      EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n      EXPECT_EQ(Costs::Duration(0), cost.execution_time);\n      EXPECT_EQ(1, cost.num_ops_total);\n      EXPECT_TRUE(cost.inaccurate);\n      EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n    }\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, DepthwiseConv2dNativeExecutionTime) {\n  auto cost =\n      PredictCosts(DescribeDepthwiseConv2dNative(16, 19, 19, 48, 48, 5, 5, 3));\n  EXPECT_EQ(Costs::Duration(112340), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(4158720), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(4271060), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, DummyExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"Dummy\", 1000, 1));\n  EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(2000), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_TRUE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, ExecutionTimeSumOrMax) {\n  SetComputeMemoryOverlap(true);\n  auto cost = PredictCosts(DescribeBinaryOp(\"Dummy\", 1000, 1));\n  EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(2000), cost.execution_time);  // max(2000, 200)\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_TRUE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n  SetComputeMemoryOverlap(false);  // Set it back to default.\n}\n\nTEST_F(OpLevelCostEstimatorTest,\n       FusedConv2DBiasActivationNCHW_HWIO_NoSideInput) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ false,\n      \"NCHW\", \"HWIO\"));\n  EXPECT_EQ(Costs::Duration(825345), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355321037), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(356146382), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNCHW_HWIO) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW\", \"HWIO\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNCHW_OIHW) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW\", \"OIHW\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNHWC_HWIO) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NHWC\", \"HWIO\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNHWC_OIHW) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NHWC\", \"OIHW\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNCHW_VECT_C_OIHW) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW_VECT_C\", \"OIHW\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, FusedConv2DBiasActivationNCHW_OIHW_VECT_I) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW\", \"OIHW_VECT_I\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest,\n       FusedConv2DBiasActivationNCHW_VECT_C_OIHW_VECT_I) {\n  auto cost = PredictCosts(DescribeFusedConv2DBiasActivation(\n      16, 19, 19, 48, 48, 5, 5, 19, 19, 256, /* has_side_input = */ true,\n      \"NCHW_VECT_C\", \"OIHW_VECT_I\"));\n  EXPECT_EQ(Costs::Duration(1416808), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(355616768), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(357033576), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, MulExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"Mul\", 1000, 1));\n  EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(200), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(2200), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, MulBroadcastExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"Mul\", 1000, 2));\n  EXPECT_EQ(Costs::Duration(3600), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(400), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(4000), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, ModExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"Mod\", 1000, 1));\n  EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(1600), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(3600), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, SquaredDifferenceExecutionTime) {\n  auto cost = PredictCosts(DescribeBinaryOp(\"SquaredDifference\", 1000, 2));\n  EXPECT_EQ(cost.memory_time, Costs::Duration(3600));\n  EXPECT_EQ(cost.compute_time, Costs::Duration(800));\n  EXPECT_EQ(cost.execution_time, Costs::Duration(4400));\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, UnaryOpExecutionTime) {\n  std::vector<std::pair<std::string, int>> unary_ops = {\n      {\"All\", 1},      {\"ArgMax\", 1}, {\"Cast\", 1},  {\"Max\", 1},\n      {\"Min\", 1},      {\"Prod\", 1},   {\"Relu\", 1},  {\"Relu6\", 1},\n      {\"Softmax\", 43}, {\"Sum\", 1},    {\"TopKV2\", 1}};\n\n  const int kTensorSize = 1000;\n  for (auto unary_op : unary_ops) {\n    OpContext op_context = DescribeUnaryOp(unary_op.first, kTensorSize);\n\n    const int kExpectedMemoryTime = 800;\n    int expected_compute_time = std::ceil(\n        unary_op.second * kTensorSize /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time))\n        << unary_op.first;\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(expected_compute_time + kExpectedMemoryTime));\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, BinaryOpExecutionTime) {\n  std::vector<std::pair<std::string, int>> binary_ops = {\n      {\"Select\", 1},\n      {\"SelectV2\", 1},\n      {\"SquaredDifference\", 2},\n      {\"Where\", 1},\n  };\n\n  const int kTensorSize1 = 1000;\n  const int kTensorSize2 = 2;\n  for (auto binary_op : binary_ops) {\n    OpContext op_context =\n        DescribeBinaryOp(binary_op.first, kTensorSize1, kTensorSize2);\n\n    const int kExpectedMemoryTime = 3600;\n    int expected_compute_time = std::ceil(\n        binary_op.second * kTensorSize1 * kTensorSize2 * 2 /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(kExpectedMemoryTime), cost.memory_time)\n        << binary_op.first;\n    EXPECT_EQ(Costs::Duration(expected_compute_time), cost.compute_time)\n        << binary_op.first;\n    EXPECT_EQ(Costs::Duration(expected_compute_time + kExpectedMemoryTime),\n              cost.execution_time)\n        << binary_op.first;\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, BroadcastAddExecutionTime) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"Add\");\n\n  DescribeTensor1D(100, op_context.op_info.add_inputs());\n  DescribeTensor4D(1, 10, 1, 1, op_context.op_info.add_inputs());\n\n  auto cost = PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(44), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(100), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(144), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, UnknownOrPartialShape) {\n  {\n    auto cost = PredictCosts(DescribeMatMul(2, 4, 7, 7));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeMatMul(-1, 4, 7, 7));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeMatMul(2, 4, -1, 7));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeConvolution(16, -1, 19, 48, 48, 5, 5, 256));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_P(OpLevelBatchMatMulCostEstimatorTest, TestBatchMatMul) {\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({}, {}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({2, 4}, {}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({2, 4}, {4, 2}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({1, 2, 4}, {1, 4, 2}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost = PredictCosts(DescribeBatchMatMul({2, 4}, {1, 3, 4, 2}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n  }\n  bool matmul_inaccurate = false;\n  bool batch_matmul_inaccurate = false;\n  EXPECT_EQ(\n      CountMatMulOperations(DescribeMatMul(2, 2, 4, 4).op_info,\n                            &matmul_inaccurate),\n      CountBatchMatMulOperations(DescribeBatchMatMul({2, 4}, {4, 2}).op_info,\n                                 &batch_matmul_inaccurate));\n  EXPECT_EQ(matmul_inaccurate, batch_matmul_inaccurate);\n  EXPECT_EQ(10 * CountMatMulOperations(DescribeMatMul(2, 2, 4, 4).op_info,\n                                       &matmul_inaccurate),\n            CountBatchMatMulOperations(\n                DescribeBatchMatMul({10, 2, 4}, {-1, 10, 4, 2}).op_info,\n                &batch_matmul_inaccurate));\n  EXPECT_NE(matmul_inaccurate, batch_matmul_inaccurate);\n  EXPECT_EQ(20 * CountMatMulOperations(DescribeMatMul(2, 2, 4, 4).op_info,\n                                       &matmul_inaccurate),\n            CountBatchMatMulOperations(\n                DescribeBatchMatMul({2, 10, 2, 4}, {-1, 10, 4, 2}).op_info,\n                &batch_matmul_inaccurate));\n  EXPECT_NE(matmul_inaccurate, batch_matmul_inaccurate);\n\n  // Test the count to make sure that they extracted the dimensions correctly\n  int prod = CountBatchMatMulDimProduct(\n      DescribeBatchMatMul({2, 4}, {1, 3, 4, 2}).op_info,\n      &batch_matmul_inaccurate);\n  EXPECT_EQ(prod, 16);\n  EXPECT_FALSE(batch_matmul_inaccurate);\n\n  // Exercise the bad cases of a batchMatMul.\n  OpContext bad_batch = DescribeBatchMatMul({2, 4}, {4, 2});\n  bad_batch.op_info.set_op(\"notBatchMatMul\");\n  prod =\n      CountBatchMatMulDimProduct(bad_batch.op_info, &batch_matmul_inaccurate);\n\n  EXPECT_EQ(prod, 0);\n  EXPECT_TRUE(batch_matmul_inaccurate);\n\n  // Exercise a transpose case of a batchMatMul\n  OpContext transpose_batch = DescribeBatchMatMul({2, 4, 3, 1}, {4, 2});\n  auto attr = transpose_batch.op_info.mutable_attr();\n  (*attr)[\"adj_x\"].set_b(true);\n  (*attr)[\"adj_y\"].set_b(true);\n\n  prod = CountBatchMatMulDimProduct(transpose_batch.op_info,\n                                    &batch_matmul_inaccurate);\n  EXPECT_EQ(prod, 12);\n}\nINSTANTIATE_TEST_SUITE_P(TestBatchMatMul, OpLevelBatchMatMulCostEstimatorTest,\n                         ::testing::Values(\"BatchMatMul\", \"BatchMatMulV2\"));\n\nTEST_F(OpLevelCostEstimatorTest, SparseTensorDenseMatMul) {\n  // Unknown shape cases\n  {\n    auto cost =\n        PredictCosts(DescribeSparseTensorDenseMatMul(-1, {1, 1}, {1, 1}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeSparseTensorDenseMatMul(1, {-1, 1}, {1, 1}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeSparseTensorDenseMatMul(1, {1, -1}, {1, -1}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  {\n    auto cost =\n        PredictCosts(DescribeSparseTensorDenseMatMul(1, {1, 1}, {-1, 1}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n  }\n  // Known shape cases\n  {\n    auto cost = PredictCosts(\n        DescribeSparseTensorDenseMatMul(10, {1000, 100}, {50, 100}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n    EXPECT_EQ(Costs::Duration(200), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(2422), cost.memory_time);\n  }\n  {\n    // Same cost as above case because cost does not depend on k_dim\n    auto cost = PredictCosts(\n        DescribeSparseTensorDenseMatMul(10, {100000, 100}, {50, 100}));\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n    EXPECT_EQ(Costs::Duration(200), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(2422), cost.memory_time);\n  }\n}\n\nvoid ExpectTensorShape(const std::vector<int64_t>& expected,\n                       const TensorShapeProto& tensor_shape_proto) {\n  TensorShape tensor_shape_expected(expected);\n  TensorShape tensor_shape(tensor_shape_proto);\n\n  EXPECT_EQ(tensor_shape_expected, tensor_shape);\n}\n\nTEST_F(OpLevelCostEstimatorTest, GetTensorShapeProtoFromTensorProto) {\n  TensorProto tensor_proto;\n  TensorShapeProto tensor_shape_proto;\n\n  // Dimension larger than max value; should fail while converting to\n  // Tensor class.\n  tensor_proto.mutable_tensor_shape()->add_dim()->set_size(255);\n  EXPECT_FALSE(\n      GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n\n  tensor_proto.Clear();\n  // Expect only 1D shape.\n  tensor_proto.mutable_tensor_shape()->add_dim()->set_size(1);\n  tensor_proto.mutable_tensor_shape()->add_dim()->set_size(2);\n  EXPECT_FALSE(\n      GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n\n  // Expect only handle integer data types.\n  GetTensorProto(DT_FLOAT, {}, {}, /*tensor_content=*/false, &tensor_proto);\n  EXPECT_FALSE(\n      GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n\n  // Check GetTensorShapeProtoFromTensorProto() returns correct values.\n  {\n    std::vector<int64_t> shape_expected = {10, 20, 30, 40};\n    GetTensorProto(DT_INT32, {4}, shape_expected,\n                   /*tensor_content=*/false, &tensor_proto);\n    EXPECT_TRUE(\n        GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n    ExpectTensorShape(shape_expected, tensor_shape_proto);\n  }\n\n  {\n    std::vector<int64_t> shape_expected = {40, 20, 90, 40};\n    GetTensorProto(DT_INT64, {4}, shape_expected,\n                   /*tensor_content=*/false, &tensor_proto);\n    EXPECT_TRUE(\n        GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n    ExpectTensorShape(shape_expected, tensor_shape_proto);\n  }\n\n  {\n    std::vector<int64_t> shape_expected = {10, 20, 30, 40};\n    GetTensorProto(DT_INT32, {4}, shape_expected,\n                   /*tensor_content=*/true, &tensor_proto);\n    EXPECT_TRUE(\n        GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n    ExpectTensorShape(shape_expected, tensor_shape_proto);\n  }\n\n  {\n    std::vector<int64_t> shape_expected = {40, 20, 90, 40};\n    GetTensorProto(DT_INT64, {4}, shape_expected,\n                   /*tensor_content=*/true, &tensor_proto);\n    EXPECT_TRUE(\n        GetTensorShapeProtoFromTensorProto(tensor_proto, &tensor_shape_proto));\n    ExpectTensorShape(shape_expected, tensor_shape_proto);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, OpDimensionsFromInputs) {\n  std::vector<string> paddings = {\"VALID\", \"SAME\"};\n  std::vector<string> formats = {\"NHWC\", \"NCHW\"};\n  for (const auto& p : paddings) {\n    for (const auto& f : formats) {\n      // n, h, w, c, kx, ky, sx, sy, data_format, padding.\n      ValidateOpDimensionsFromInputs(10, 20, 20, 100, 3, 3, 2, 2, f, p);\n      ValidateOpDimensionsFromInputs(10, 20, 20, 100, 1, 1, 3, 3, f, p);\n      ValidateOpDimensionsFromInputs(10, 200, 200, 100, 5, 5, 3, 3, f, p);\n      ValidateOpDimensionsFromInputs(10, 14, 14, 3840, 3, 3, 2, 2, f, p);\n    }\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, OpDimensionsFromInputsError) {\n  std::vector<string> paddings = {\"VALID\", \"SAME\"};\n  std::vector<string> formats = {\"NHWC\", \"NCHW\"};\n  for (const auto& p : paddings) {\n    for (const auto& f : formats) {\n      // n, h, w, c, kx, ky, sx, sy, data_format, padding.\n      ASSERT_THAT(\n          CallOpDimensionsFromInputs(10, 14, 14, 3840, 3, 3, 0, 2, f, p),\n          testing::StatusIs(\n              error::INVALID_ARGUMENT,\n              \"Stride must be > 0 for Height and Width, but got (2, 0)\"));\n      ASSERT_THAT(\n          CallOpDimensionsFromInputs(10, 14, 14, 3840, 3, 3, 2, 0, f, p),\n          testing::StatusIs(\n              error::INVALID_ARGUMENT,\n              \"Stride must be > 0 for Height and Width, but got (0, 2)\"));\n    }\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictMaxPool) {\n  auto predict_max_pool = [this](const int n, const int in, const int c,\n                                 const int k, const int s,\n                                 const string& padding) -> Costs {\n    OpContext op_context = DescribePoolingOp(\n        \"MaxPool\", {n, in, in, c}, {1, k, k, 1}, {1, s, s, 1}, \"NHWC\", padding);\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    // Typical 3xz3 window with 2x2 stride.\n    auto costs = predict_max_pool(10, 20, 384, 3, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1075200), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(307200), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(768000), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n  {\n    // 1x1 window with 2x2 stride: used for shortcut in resnet-50.\n    auto costs = predict_max_pool(10, 20, 384, 1, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(499200), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(38400), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(460800), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n  {\n    // 2x2 window with 3x3 stride.\n    auto costs = predict_max_pool(10, 20, 384, 2, 3, \"VALID\");\n    EXPECT_EQ(Costs::Duration(561792), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(56448), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(505344), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictMaxPoolGrad) {\n  auto predict_max_pool_grad = [this](const int n, const int in, const int c,\n                                      const int k, const int s,\n                                      const string& padding) -> Costs {\n    OpContext op_context =\n        DescribePoolingOp(\"MaxPoolGrad\", {n, in, in, c}, {1, k, k, 1},\n                          {1, s, s, 1}, \"NHWC\", padding);\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    // Typical 3x3 window with 2x2 stride.\n    auto costs = predict_max_pool_grad(10, 20, 384, 3, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1996800), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(614400), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(1382400), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n  {\n    // 1x1 window with 2x2 stride: used for shortcut in resnet-50.\n    auto costs = predict_max_pool_grad(10, 20, 384, 1, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1536000), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(153600), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(1382400), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n  {\n    // 2x2 window with 3x3 stride.\n    auto costs = predict_max_pool_grad(10, 20, 384, 2, 3, \"VALID\");\n    EXPECT_EQ(Costs::Duration(1514112), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(210048), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(1304064), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictAvgPool) {\n  auto predict_avg_pool = [this](const int n, const int in, const int c,\n                                 const int k, const int s,\n                                 const string& padding) -> Costs {\n    OpContext op_context = DescribePoolingOp(\n        \"AvgPool\", {n, in, in, c}, {1, k, k, 1}, {1, s, s, 1}, \"NHWC\", padding);\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    // Typical 3x3 window with 2x2 stride.\n    auto costs = predict_avg_pool(10, 20, 384, 3, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1113600), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(345600), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(768000), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n  {\n    // 1x1 window with 2x2 stride: used for shortcut in resnet-50.\n    auto costs = predict_avg_pool(10, 20, 384, 1, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(499200), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(38400), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(460800), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n  {\n    // 2x2 window with 3x3 stride.\n    auto costs = predict_avg_pool(10, 20, 384, 2, 3, \"VALID\");\n    EXPECT_EQ(Costs::Duration(580608), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(75264), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(505344), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictAvgPoolGrad) {\n  auto predict_avg_pool_grad = [this](const int n, const int in, const int c,\n                                      const int k, const int s,\n                                      const string& padding) -> Costs {\n    OpContext op_context =\n        DescribePoolingOp(\"AvgPoolGrad\", {n, in, in, c}, {1, k, k, 1},\n                          {1, s, s, 1}, \"NHWC\", padding);\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    // Typical 3xz3 window with 2x2 stride.\n    auto costs = predict_avg_pool_grad(10, 20, 384, 3, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(1305602), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(537600), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(768002), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n  {\n    // 1x1 window with 2x2 stride: used for shortcut in resnet-50.\n    auto costs = predict_avg_pool_grad(10, 20, 384, 1, 2, \"SAME\");\n    EXPECT_EQ(Costs::Duration(960002), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(192000), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(768002), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n  {\n    // 2x2 window with 3x3 stride.\n    auto costs = predict_avg_pool_grad(10, 20, 384, 2, 3, \"VALID\");\n    EXPECT_EQ(Costs::Duration(862082), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(172416), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(689666), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictFusedBatchNorm) {\n  auto predict_fused_bn = [this](const int n, const int in, const int c,\n                                 const bool is_training) -> Costs {\n    OpContext op_context = DescribeFusedBatchNorm(\n        is_training, /*is_grad=*/false, {n, in, in, c}, \"NHWC\");\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    auto costs = predict_fused_bn(10, 20, 96, /*is_training=*/true);\n    EXPECT_EQ(Costs::Duration(614737), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(153706), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(461031), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n\n  {\n    auto costs = predict_fused_bn(10, 20, 32, /*is_training=*/true);\n    EXPECT_EQ(Costs::Duration(204913), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(51236), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(153677), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n\n  {\n    auto costs = predict_fused_bn(10, 20, 96, /*is_training=*/false);\n    EXPECT_EQ(Costs::Duration(384154), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(76800), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(307354), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n\n  {\n    auto costs = predict_fused_bn(10, 20, 32, /*is_training=*/false);\n    EXPECT_EQ(Costs::Duration(128052), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(25600), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(102452), costs.memory_time);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictFusedBatchNormGrad) {\n  auto predict_fused_bn_grad = [this](const int n, const int in,\n                                      const int c) -> Costs {\n    OpContext op_context = DescribeFusedBatchNorm(\n        /*is_training=*/false, /*is_grad=*/true, {n, in, in, c}, \"NHWC\");\n    return estimator_.PredictCosts(op_context);\n  };\n\n  {\n    auto costs = predict_fused_bn_grad(10, 20, 96);\n    EXPECT_EQ(Costs::Duration(1037050), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(422496), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(614554), costs.memory_time);\n    EXPECT_EQ(costs.num_ops_total, 1);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(costs.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(costs.temporary_memory, 0);\n    EXPECT_EQ(costs.persistent_memory, 0);\n  }\n\n  {\n    auto costs = predict_fused_bn_grad(128, 7, 384);\n    EXPECT_EQ(Costs::Duration(6503809), costs.execution_time);\n    EXPECT_EQ(Costs::Duration(2649677), costs.compute_time);\n    EXPECT_EQ(Costs::Duration(3854132), costs.memory_time);\n    EXPECT_EQ(1, costs.num_ops_total);\n    EXPECT_FALSE(costs.inaccurate);\n    EXPECT_EQ(0, costs.num_ops_with_unknown_shapes);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, MaybeGetMinimumShape) {\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(true);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 4, &unknown_shapes);\n    EXPECT_TRUE(unknown_shapes);\n    ExpectTensorShape({1, 1, 1, 1}, y);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 1, &unknown_shapes);\n    EXPECT_FALSE(unknown_shapes);\n    ExpectTensorShape({1}, y);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 2, &unknown_shapes);\n    EXPECT_FALSE(unknown_shapes);\n    ExpectTensorShape({1, 1}, y);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    x.add_dim()->set_size(10);\n    x.add_dim()->set_size(20);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 2, &unknown_shapes);\n    EXPECT_FALSE(unknown_shapes);\n    ExpectTensorShape({10, 20}, y);\n\n    unknown_shapes = false;\n    TensorShapeProto z = MaybeGetMinimumShape(x, 4, &unknown_shapes);\n    EXPECT_TRUE(unknown_shapes);\n    EXPECT_EQ(4, z.dim_size());\n    ExpectTensorShape({10, 20, 1, 1}, z);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    x.add_dim()->set_size(10);\n    x.add_dim()->set_size(20);\n    x.add_dim()->set_size(-1);\n    x.add_dim()->set_size(20);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 4, &unknown_shapes);\n    EXPECT_TRUE(unknown_shapes);\n    ExpectTensorShape({10, 20, 1, 20}, y);\n  }\n\n  {\n    TensorShapeProto x;\n    x.set_unknown_rank(false);\n    x.add_dim()->set_size(10);\n    x.add_dim()->set_size(20);\n    x.add_dim()->set_size(30);\n    x.add_dim()->set_size(20);\n    bool unknown_shapes = false;\n    TensorShapeProto y = MaybeGetMinimumShape(x, 2, &unknown_shapes);\n    EXPECT_TRUE(unknown_shapes);\n    ExpectTensorShape({10, 20}, y);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, IntermediateRdWrBandwidth) {\n  TestOpLevelCostEstimator estimator;\n\n  // Compute limited.\n  estimator.SetDeviceInfo(DeviceInfo(/*gigaops=*/1,\n                                     /*gb_per_sec=*/1));\n  estimator.SetComputeMemoryOverlap(true);\n  auto cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(3548774400), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.compute_time);\n\n  estimator.SetComputeMemoryOverlap(false);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(3551112192), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.compute_time + cost.memory_time +\n                                     cost.intermediate_memory_time);\n\n  // Memory limited.\n  estimator.SetDeviceInfo(DeviceInfo(/*gigaops=*/99999,\n                                     /*gb_per_sec=*/1));\n  estimator.SetComputeMemoryOverlap(true);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(2337792), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.memory_time);\n\n  estimator.SetComputeMemoryOverlap(false);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(2373281), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.compute_time + cost.memory_time +\n                                     cost.intermediate_memory_time);\n\n  // Intermediate memory bandwidth limited.\n  estimator.SetDeviceInfo(DeviceInfo(/*gigaops=*/99999,\n                                     /*gb_per_sec=*/9999,\n                                     /*intermediate_read_gb_per_sec=*/1,\n                                     /*intermediate_write_gb_per_sec=*/1));\n  estimator.SetComputeMemoryOverlap(true);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(2337792), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.intermediate_memory_time);\n\n  estimator.SetComputeMemoryOverlap(false);\n  cost = estimator.PredictCosts(\n      DescribeConvolution(16, 19, 19, 48, 48, 5, 5, 256));\n  EXPECT_EQ(Costs::Duration(2373515), cost.execution_time);\n  EXPECT_EQ(cost.execution_time, cost.compute_time + cost.memory_time +\n                                     cost.intermediate_memory_time);\n}\n\nTEST_F(OpLevelCostEstimatorTest, Einsum) {\n  {  // Test a simple matrix multiplication.\n    auto cost = PredictCosts(DescribeEinsum({100, 50}, {100, 50}, \"ik,jk->ij\"));\n    EXPECT_EQ(Costs::Duration(104000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(4000), cost.memory_time);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(PredictCosts(DescribeEinsum({100, 50}, {100, 50}, \"ik,jk->ij\"))\n                  .execution_time,\n              PredictCosts(DescribeXlaEinsum({100, 50}, {100, 50}, \"ik,jk->ij\"))\n                  .execution_time);\n  }\n  {  // Test a simple batch matrix multiplication.\n    auto cost = PredictCosts(\n        DescribeEinsum({25, 100, 50}, {100, 50, 25}, \"Bik,jkB->Bij\"));\n    EXPECT_EQ(Costs::Duration(25 * 104000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(25 * 100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(25 * 4000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(PredictCosts(\n                  DescribeEinsum({25, 100, 50}, {100, 50, 25}, \"Bik,jkB->Bij\"))\n                  .execution_time,\n              PredictCosts(DescribeXlaEinsum({25, 100, 50}, {100, 50, 25},\n                                             \"Bik,jkB->Bij\"))\n                  .execution_time);\n  }\n  {  // Test multiple batch dimensions.\n    auto cost = PredictCosts(DescribeEinsum(\n        {25, 16, 100, 50}, {16, 100, 50, 25}, \"BNik,NjkB->BNij\"));\n    EXPECT_EQ(Costs::Duration(16 * 25 * 104000), cost.execution_time);\n    EXPECT_EQ(\n        Costs::Duration(16 * 25 * 100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n        cost.compute_time);\n    EXPECT_EQ(Costs::Duration(16 * 25 * 4000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({25, 16, 100, 50}, {16, 100, 50, 25},\n                                    \"BNik,NjkB->BNij\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({25, 16, 100, 50}, {16, 100, 50, 25},\n                                       \"BNik,NjkB->BNij\"))\n            .execution_time);\n  }\n  {  // Test multiple M dimensions.\n    auto cost =\n        PredictCosts(DescribeEinsum({25, 100, 50}, {100, 50}, \"Aik,jk->Aij\"));\n    EXPECT_EQ(Costs::Duration(2552000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(25 * 100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(52000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({25, 100, 50}, {100, 50}, \"Aik,jk->Aij\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({25, 100, 50}, {100, 50}, \"Aik,jk->Aij\"))\n            .execution_time);\n  }\n  {  // Test multiple N dimensions.\n    auto cost =\n        PredictCosts(DescribeEinsum({100, 50}, {25, 100, 50}, \"ik,Bjk->ijB\"));\n    EXPECT_EQ(Costs::Duration(2552000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(25 * 100 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(52000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50}, {25, 100, 50}, \"ik,Bjk->ijB\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50}, {25, 100, 50}, \"ik,Bjk->ijB\"))\n            .execution_time);\n  }\n  {  // Test multiple contracting dimensions.\n    auto cost = PredictCosts(\n        DescribeEinsum({100, 50, 25}, {100, 50, 25}, \"ikl,jkl->ij\"));\n    EXPECT_EQ(Costs::Duration(2600000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(100 * 50 * 25 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(100000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(PredictCosts(\n                  DescribeEinsum({100, 50, 25}, {100, 50, 25}, \"ikl,jkl->ij\"))\n                  .execution_time,\n              PredictCosts(DescribeXlaEinsum({100, 50, 25}, {100, 50, 25},\n                                             \"ikl,jkl->ij\"))\n                  .execution_time);\n  }\n  {  // Test a simple matrix transpose.\n    auto cost = PredictCosts(DescribeEinsum({100, 50}, {}, \"ij->ji\"));\n    EXPECT_EQ(Costs::Duration(2000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(2000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50}, {}, \"ij->ji\")).execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50}, {}, \"ij->ji\"))\n            .execution_time);\n  }\n  {  // Test a malformed Einsum equation: Mismatch between shapes and equation.\n    auto cost =\n        PredictCosts(DescribeEinsum({100, 50, 25}, {50, 100}, \"ik,kl->il\"));\n    EXPECT_EQ(Costs::Duration(52000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(52000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50, 25}, {50, 100}, \"ik,kl->il\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50, 25}, {50, 100}, \"ik,kl->il\"))\n            .execution_time);\n\n    cost = PredictCosts(DescribeEinsum({100, 50}, {50, 100, 25}, \"ik,kl->il\"));\n    EXPECT_EQ(Costs::Duration(52000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(52000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50}, {50, 100, 25}, \"ik,kl->il\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50}, {50, 100, 25}, \"ik,kl->il\"))\n            .execution_time);\n  }\n  {  // Test an unsupported Einsum: ellipsis\n    auto cost = PredictCosts(DescribeEinsum(\n        {100, 50, 25, 16}, {50, 100, 32, 12}, \"ik...,kl...->il...\"));\n    EXPECT_EQ(Costs::Duration(1568000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(1568000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 50, 25, 16}, {50, 100, 32, 12},\n                                    \"ik...,kl...->il...\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 50, 25, 16}, {50, 100, 32, 12},\n                                       \"ik...,kl...->il...\"))\n            .execution_time);\n  }\n  {  // Test a malformed/unsupported Einsum: repeated indices\n    auto cost =\n        PredictCosts(DescribeEinsum({100, 100, 50}, {50, 100}, \"iik,kl->il\"));\n    EXPECT_EQ(Costs::Duration(202000), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(202000), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(0, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(\n        PredictCosts(DescribeEinsum({100, 100, 50}, {50, 100}, \"iik,kl->il\"))\n            .execution_time,\n        PredictCosts(DescribeXlaEinsum({100, 100, 50}, {50, 100}, \"iik,kl->il\"))\n            .execution_time);\n  }\n  {  // Test missing shapes.\n    auto cost = PredictCosts(DescribeEinsum({-1, 50}, {100, 50}, \"ik,jk->ij\"));\n    EXPECT_EQ(Costs::Duration(3020), cost.execution_time);\n    EXPECT_EQ(Costs::Duration(1 * 50 * 100 * 2 / (1000 * 10 * 1e-3)),\n              cost.compute_time);\n    EXPECT_EQ(Costs::Duration(2020), cost.memory_time);\n    EXPECT_EQ(1, cost.num_ops_total);\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(1, cost.num_ops_with_unknown_shapes);\n\n    // Einsums and XlaEinsums should be estimated similarly.\n    EXPECT_EQ(PredictCosts(DescribeEinsum({-1, 50}, {100, 50}, \"ik,jk->ij\"))\n                  .execution_time,\n              PredictCosts(DescribeXlaEinsum({-1, 50}, {100, 50}, \"ik,jk->ij\"))\n                  .execution_time);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PredictResourceVariableOps) {\n  TestOpLevelCostEstimator estimator;\n  estimator.SetDeviceInfo(DeviceInfo(/*gigaops=*/1, /*gb_per_sec=*/1));\n\n  {\n    OpContext op_context;\n    op_context.op_info.set_op(\"AssignVariableOp\");\n    DescribeDummyTensor(op_context.op_info.add_inputs());\n    DescribeTensor1D(100, op_context.op_info.add_inputs());\n    auto cost = estimator.PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(400), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(0), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(400), cost.execution_time);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n\n  {\n    OpContext op_context;\n    op_context.op_info.set_op(\"AssignSubVariableOp\");\n    DescribeDummyTensor(op_context.op_info.add_inputs());\n    DescribeTensor1D(100, op_context.op_info.add_inputs());\n    auto cost = estimator.PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(400), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(100), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(400), cost.execution_time);\n    EXPECT_FALSE(cost.inaccurate);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, AddNExecutionTime) {\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"AddN\");\n\n  DescribeTensor4D(1, 10, 10, 10, op_context.op_info.add_inputs());\n  DescribeTensor4D(1, 10, 10, 10, op_context.op_info.add_inputs());\n  DescribeTensor4D(1, 10, 10, 10, op_context.op_info.add_inputs());\n\n  auto cost = PredictCosts(op_context);\n  EXPECT_EQ(Costs::Duration(1200), cost.memory_time);\n  EXPECT_EQ(Costs::Duration(200), cost.compute_time);\n  EXPECT_EQ(Costs::Duration(1400), cost.execution_time);\n  EXPECT_EQ(cost.num_ops_total, 1);\n  EXPECT_FALSE(cost.inaccurate);\n  EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  EXPECT_EQ(cost.temporary_memory, 0);\n  EXPECT_EQ(cost.persistent_memory, 0);\n}\n\nTEST_F(OpLevelCostEstimatorTest, IdentityOpExecutionTime) {\n  std::vector<std::string> identity_ops = {\n      \"_Recv\",         \"_Send\",        \"BitCast\",         \"Identity\",\n      \"Enter\",         \"Exit\",         \"IdentityN\",       \"Merge\",\n      \"NextIteration\", \"Placeholder\",  \"PreventGradient\", \"RefIdentity\",\n      \"Reshape\",       \"StopGradient\", \"Switch\"};\n\n  const int kTensorSize = 1000;\n  for (auto identity_op : identity_ops) {\n    OpContext op_context = DescribeUnaryOp(identity_op, kTensorSize);\n\n    const int kExpectedMemoryTime = 0;\n    const int kExpectedComputeTime = 1;\n\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(kExpectedMemoryTime), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(kExpectedComputeTime), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(kExpectedComputeTime + kExpectedMemoryTime),\n              cost.execution_time);\n    EXPECT_EQ(cost.max_memory, kTensorSize * 4);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, PureMemoryOpExecutionTime) {\n  std::vector<std::string> reshape_ops = {\n      \"ConcatV2\",     \"DataFormatVecPermute\",\n      \"DepthToSpace\", \"ExpandDims\",\n      \"Fill\",         \"OneHot\",\n      \"Pack\",         \"Range\",\n      \"SpaceToDepth\", \"Split\",\n      \"Squeeze\",      \"Transpose\",\n      \"Tile\",         \"Unpack\"};\n\n  const int kTensorSize = 1000;\n  for (auto reshape_op : reshape_ops) {\n    OpContext op_context = DescribeUnaryOp(reshape_op, kTensorSize);\n\n    const int kExpectedMemoryTime = 800;\n    const int kExpectedComputeTime = 0;\n\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(Costs::Duration(kExpectedMemoryTime), cost.memory_time);\n    EXPECT_EQ(Costs::Duration(kExpectedComputeTime), cost.compute_time);\n    EXPECT_EQ(Costs::Duration(kExpectedComputeTime + kExpectedMemoryTime),\n              cost.execution_time);\n    EXPECT_EQ(cost.max_memory, kTensorSize * 4);\n    EXPECT_EQ(cost.num_ops_total, 1);\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, ResizeBilinearExecutionTime) {\n  const int kImageDim = 255;\n  const int kChannelSize = 10;\n  const int kComputeLerpCost = 9;\n  {\n    OpContext op_context;\n    SetCpuDevice(&op_context.op_info);\n    op_context.op_info.set_op(\"ResizeBilinear\");\n    DescribeTensor4D(1, kImageDim, kImageDim, kChannelSize,\n                     op_context.op_info.add_inputs());\n    // Test with no output.\n    auto cost = PredictCosts(op_context);\n    ExpectZeroCost(cost);\n    op_context.op_info.clear_inputs();\n\n    DescribeTensor4D(0, 0, 0, 0, op_context.op_info.add_outputs());\n    // Test with no input.\n    cost = PredictCosts(op_context);\n    ExpectZeroCost(cost);\n  }\n  {\n    // Test with size 0 output.\n    OpContext op_context;\n    SetCpuDevice(&op_context.op_info);\n    op_context.op_info.set_op(\"ResizeBilinear\");\n\n    DescribeTensor4D(1, kImageDim, kImageDim, kChannelSize,\n                     op_context.op_info.add_inputs());\n    const int kExpectedMemoryTime = kImageDim * kImageDim * 4;\n    DescribeTensor4D(0, 0, 0, 0, op_context.op_info.add_outputs());\n\n    // As the half_pixel_centers attr was not set, cost should be inaccurate\n    // with 0 compute time.\n    auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(0));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_TRUE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n    EXPECT_EQ(cost.temporary_memory, 0);\n    EXPECT_EQ(cost.persistent_memory, 0);\n\n    AttrValue half_pixel_centers;\n    half_pixel_centers.set_b(false);\n    (*op_context.op_info.mutable_attr())[\"half_pixel_centers\"] =\n        half_pixel_centers;\n    cost = PredictCosts(op_context);\n    // Compute time depends only on output size, so compute time is 0.\n    EXPECT_EQ(cost.compute_time, Costs::Duration(0));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n\n  // Test with non-zero output size.\n  const int kOutputImageDim = 100;\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"ResizeBilinear\");\n  DescribeTensor4D(1, kImageDim, kImageDim, kChannelSize,\n                   op_context.op_info.add_inputs());\n  DescribeTensor4D(1, kOutputImageDim, kOutputImageDim, kChannelSize,\n                   op_context.op_info.add_outputs());\n  const int kExpectedMemoryTime =\n      (kImageDim * kImageDim + kOutputImageDim * kOutputImageDim) * 4;\n\n  {\n    // Cost of calculating weights without using half_pixel_centers.\n    AttrValue half_pixel_centers;\n    half_pixel_centers.set_b(false);\n    (*op_context.op_info.mutable_attr())[\"half_pixel_centers\"] =\n        half_pixel_centers;\n    const int kInterpWeightCost = 10;\n    const int num_ops =\n        kInterpWeightCost * (kOutputImageDim * 2) +\n        kComputeLerpCost * (kOutputImageDim * kOutputImageDim * kChannelSize);\n    const int expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(kExpectedMemoryTime + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n\n  {\n    // Cost of calculating weights using half_pixel_centers.\n    AttrValue half_pixel_centers;\n    half_pixel_centers.set_b(true);\n    (*op_context.op_info.mutable_attr())[\"half_pixel_centers\"] =\n        half_pixel_centers;\n    const int kInterpWeightCost = 12;\n    const int num_ops =\n        kInterpWeightCost * (kOutputImageDim * 2) +\n        kComputeLerpCost * (kOutputImageDim * kOutputImageDim * kChannelSize);\n    const int expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(kExpectedMemoryTime + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n\n  {\n    // Cost with very large tensor.\n    op_context.op_info.clear_outputs();\n    // Number of elements in tensor exceeds 2^32.\n    constexpr int64_t kLargeOutputImageDim = 40000;\n    DescribeTensor4D(1, kLargeOutputImageDim, kLargeOutputImageDim,\n                     kChannelSize, op_context.op_info.add_outputs());\n    const int64_t kInterpWeightCost = 12;\n    // Using half_pixel_centers.\n    AttrValue half_pixel_centers;\n    half_pixel_centers.set_b(true);\n    (*op_context.op_info.mutable_attr())[\"half_pixel_centers\"] =\n        half_pixel_centers;\n\n    const int64_t num_ops =\n        kInterpWeightCost * (kLargeOutputImageDim * 2) +\n        kComputeLerpCost *\n            (kLargeOutputImageDim * kLargeOutputImageDim * kChannelSize);\n    const int64_t expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const int64_t expected_memory_time =\n        (kImageDim * kImageDim + kLargeOutputImageDim * kLargeOutputImageDim) *\n        4;\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(expected_memory_time));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(expected_memory_time + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n}\n\nTEST_F(OpLevelCostEstimatorTest, CropAndResizeExecutionTime) {\n  const int kImageDim = 255;\n  const int kChannelSize = 10;\n  const int kOutputImageDim = 100;\n  const int kNumBoxes = 10;\n  const int kOutputElements =\n      kNumBoxes * kOutputImageDim * kOutputImageDim * kChannelSize;\n  OpContext op_context;\n  SetCpuDevice(&op_context.op_info);\n  op_context.op_info.set_op(\"CropAndResize\");\n  DescribeTensor4D(1, kImageDim, kImageDim, kChannelSize,\n                   op_context.op_info.add_inputs());\n  DescribeArbitraryRankInput({kNumBoxes, 4}, DT_INT64, &op_context.op_info);\n  DescribeTensor4D(kNumBoxes, kOutputImageDim, kOutputImageDim, kChannelSize,\n                   op_context.op_info.add_outputs());\n\n  // Note this is time [ns, default in Duration in Costs], not bytes;\n  // whereas memory bandwidth from SetCpuDevice() is 10GB/s.\n  const int kExpectedMemoryTime =\n      (kImageDim * kImageDim * 4 +  // input image in float.\n       kNumBoxes * 4 * 8 / 10 +     // boxes (kNumBoxes x 4) in int64.\n       kNumBoxes * kOutputImageDim * kOutputImageDim * 4);  // output in float.\n  // Note that input image and output image has kChannelSize dim, which is 10,\n  // hence, no need to divide it by 10 (bandwidth).\n\n  {\n    // Cost of CropAndResize with bilinear interpolation.\n    AttrValue method;\n    method.set_s(\"bilinear\");\n    (*op_context.op_info.mutable_attr())[\"method\"] = method;\n    int num_ops = 28 * kNumBoxes + 4 * kNumBoxes * kOutputImageDim +\n                  4 * kNumBoxes * kOutputImageDim * kOutputImageDim +\n                  3 * kNumBoxes * kOutputImageDim +\n                  3 * kNumBoxes * kOutputImageDim * kOutputImageDim +\n                  13 * kOutputElements;\n    const int expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(kExpectedMemoryTime + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n\n  {\n    // Cost of CropAndResize when nearest pixel is taken.\n    AttrValue method;\n    method.set_s(\"nearest\");\n    (*op_context.op_info.mutable_attr())[\"method\"] = method;\n    int num_ops = 28 * kNumBoxes + 4 * kNumBoxes * kOutputImageDim +\n                  4 * kNumBoxes * kOutputImageDim * kOutputImageDim +\n                  2 * kNumBoxes * kOutputImageDim * kOutputImageDim +\n                  kOutputElements;\n    const int expected_compute_time = std::ceil(\n        num_ops /\n        estimator_.GetDeviceInfo(op_context.op_info.device()).gigaops);\n\n    const auto cost = PredictCosts(op_context);\n    EXPECT_EQ(cost.compute_time, Costs::Duration(expected_compute_time));\n    EXPECT_EQ(cost.memory_time, Costs::Duration(kExpectedMemoryTime));\n    EXPECT_EQ(cost.execution_time,\n              Costs::Duration(kExpectedMemoryTime + expected_compute_time));\n    EXPECT_FALSE(cost.inaccurate);\n    EXPECT_EQ(cost.num_ops_with_unknown_shapes, 0);\n  }\n}\n\n}  // end namespace grappler\n}  // end namespace tensorflow\n"], "filenames": ["tensorflow/core/grappler/costs/BUILD", "tensorflow/core/grappler/costs/op_level_cost_estimator.cc", "tensorflow/core/grappler/costs/op_level_cost_estimator.h", "tensorflow/core/grappler/costs/op_level_cost_estimator_test.cc"], "buggy_code_start_loc": [357, 2156, 293, 26], "buggy_code_end_loc": [357, 2464, 294, 1385], "fixing_code_start_loc": [358, 2156, 293, 27], "fixing_code_end_loc": [359, 2475, 294, 1440], "type": "CWE-369", "message": "Tensorflow is an Open Source Machine Learning Framework. The estimator for the cost of some convolution operations can be made to execute a division by 0. The function fails to check that the stride argument is strictly positive. Hence, the fix is to add a check for the stride argument to ensure it is valid. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2022-21725", "sourceIdentifier": "security-advisories@github.com", "published": "2022-02-03T13:15:07.870", "lastModified": "2022-02-09T04:56:38.513", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Tensorflow is an Open Source Machine Learning Framework. The estimator for the cost of some convolution operations can be made to execute a division by 0. The function fails to check that the stride argument is strictly positive. Hence, the fix is to add a check for the stride argument to ensure it is valid. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range."}, {"lang": "es", "value": "Tensorflow es un marco de aprendizaje autom\u00e1tico de c\u00f3digo abierto. El estimador del coste de algunas operaciones de convoluci\u00f3n puede hacerse para ejecutar una divisi\u00f3n por 0. La funci\u00f3n no comprueba que el argumento stride sea estrictamente positivo. Por lo tanto, la correcci\u00f3n consiste en a\u00f1adir una comprobaci\u00f3n del argumento stride para asegurar que es v\u00e1lido. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.8.0. Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.7.1, TensorFlow versi\u00f3n 2.6.3, y TensorFlow versi\u00f3n 2.5.3, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-369"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.5.2", "matchCriteriaId": "688150BF-477C-48FC-9AEF-A79AC57A6DDC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndIncluding": "2.6.2", "matchCriteriaId": "C9E69B60-8C97-47E2-9027-9598B8392E5D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:*:*:*:*:*:*:*", "matchCriteriaId": "2EDFAAB8-799C-4259-9102-944D4760DA2C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/ffa202a17ab7a4a10182b746d230ea66f021fe16/tensorflow/core/grappler/costs/op_level_cost_estimator.cc#L189-L198", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/3218043d6d3a019756607643cf65574fbfef5d7a", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-v3f7-j968-4h5f", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/3218043d6d3a019756607643cf65574fbfef5d7a"}}