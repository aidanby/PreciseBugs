{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/graph.pb.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/summary.pb.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/lib/db/sqlite.h\"\n#include \"tensorflow/core/platform/protobuf.h\"\n#include \"tensorflow/core/summary/schema.h\"\n#include \"tensorflow/core/summary/summary_db_writer.h\"\n#include \"tensorflow/core/summary/summary_file_writer.h\"\n#include \"tensorflow/core/util/event.pb.h\"\n\nnamespace tensorflow {\n\nREGISTER_KERNEL_BUILDER(Name(\"SummaryWriter\").Device(DEVICE_CPU),\n                        ResourceHandleOp<SummaryWriterInterface>);\n\nclass CreateSummaryFileWriterOp : public OpKernel {\n public:\n  explicit CreateSummaryFileWriterOp(OpKernelConstruction* ctx)\n      : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"logdir\", &tmp));\n    const string logdir = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"max_queue\", &tmp));\n    const int32_t max_queue = tmp->scalar<int32>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"flush_millis\", &tmp));\n    const int32_t flush_millis = tmp->scalar<int32>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"filename_suffix\", &tmp));\n    const string filename_suffix = tmp->scalar<tstring>()();\n\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupOrCreateResource<SummaryWriterInterface>(\n                            ctx, HandleFromInput(ctx, 0), &s,\n                            [max_queue, flush_millis, logdir, filename_suffix,\n                             ctx](SummaryWriterInterface** s) {\n                              return CreateSummaryFileWriter(\n                                  max_queue, flush_millis, logdir,\n                                  filename_suffix, ctx->env(), s);\n                            }));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"CreateSummaryFileWriter\").Device(DEVICE_CPU),\n                        CreateSummaryFileWriterOp);\n\nclass CreateSummaryDbWriterOp : public OpKernel {\n public:\n  explicit CreateSummaryDbWriterOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"db_uri\", &tmp));\n    const string db_uri = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"experiment_name\", &tmp));\n    const string experiment_name = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"run_name\", &tmp));\n    const string run_name = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"user_name\", &tmp));\n    const string user_name = tmp->scalar<tstring>()();\n\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(\n        ctx,\n        LookupOrCreateResource<SummaryWriterInterface>(\n            ctx, HandleFromInput(ctx, 0), &s,\n            [db_uri, experiment_name, run_name, user_name,\n             ctx](SummaryWriterInterface** s) {\n              Sqlite* db;\n              TF_RETURN_IF_ERROR(Sqlite::Open(\n                  db_uri, SQLITE_OPEN_READWRITE | SQLITE_OPEN_CREATE, &db));\n              core::ScopedUnref unref(db);\n              TF_RETURN_IF_ERROR(SetupTensorboardSqliteDb(db));\n              TF_RETURN_IF_ERROR(CreateSummaryDbWriter(\n                  db, experiment_name, run_name, user_name, ctx->env(), s));\n              return Status::OK();\n            }));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"CreateSummaryDbWriter\").Device(DEVICE_CPU),\n                        CreateSummaryDbWriterOp);\n\nclass FlushSummaryWriterOp : public OpKernel {\n public:\n  explicit FlushSummaryWriterOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    OP_REQUIRES_OK(ctx, s->Flush());\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"FlushSummaryWriter\").Device(DEVICE_CPU),\n                        FlushSummaryWriterOp);\n\nclass CloseSummaryWriterOp : public OpKernel {\n public:\n  explicit CloseSummaryWriterOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, DeleteResource<SummaryWriterInterface>(\n                            ctx, HandleFromInput(ctx, 0)));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"CloseSummaryWriter\").Device(DEVICE_CPU),\n                        CloseSummaryWriterOp);\n\nclass WriteSummaryOp : public OpKernel {\n public:\n  explicit WriteSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"summary_metadata\", &tmp));\n    const string& serialized_metadata = tmp->scalar<tstring>()();\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n\n    OP_REQUIRES_OK(ctx, s->WriteTensor(step, *t, tag, serialized_metadata));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteSummary\").Device(DEVICE_CPU),\n                        WriteSummaryOp);\n\nclass WriteRawProtoSummaryOp : public OpKernel {\n public:\n  explicit WriteRawProtoSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tmp->shape()),\n                errors::InvalidArgument(\"step must be scalar, got shape \",\n                                        tmp->shape().DebugString()));\n    const int64_t step = tmp->scalar<int64_t>()();\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n    std::unique_ptr<Event> event{new Event};\n    event->set_step(step);\n    event->set_wall_time(static_cast<double>(ctx->env()->NowMicros()) / 1.0e6);\n    // Each Summary proto contains just one repeated field \"value\" of Value\n    // messages with the actual data, so repeated Merge() is equivalent to\n    // concatenating all the Value entries together into a single Event.\n    const auto summary_pbs = t->flat<tstring>();\n    for (int i = 0; i < summary_pbs.size(); ++i) {\n      if (!event->mutable_summary()->MergeFromString(summary_pbs(i))) {\n        ctx->CtxFailureWithWarning(errors::DataLoss(\n            \"Bad tf.compat.v1.Summary binary proto tensor string at index \",\n            i));\n        return;\n      }\n    }\n    OP_REQUIRES_OK(ctx, s->WriteEvent(std::move(event)));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteRawProtoSummary\").Device(DEVICE_CPU),\n                        WriteRawProtoSummaryOp);\n\nclass ImportEventOp : public OpKernel {\n public:\n  explicit ImportEventOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"event\", &t));\n    std::unique_ptr<Event> event{new Event};\n    if (!ParseProtoUnlimited(event.get(), t->scalar<tstring>()())) {\n      ctx->CtxFailureWithWarning(\n          errors::DataLoss(\"Bad tf.Event binary proto tensor string\"));\n      return;\n    }\n    OP_REQUIRES_OK(ctx, s->WriteEvent(std::move(event)));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"ImportEvent\").Device(DEVICE_CPU), ImportEventOp);\n\nclass WriteScalarSummaryOp : public OpKernel {\n public:\n  explicit WriteScalarSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"value\", &t));\n\n    OP_REQUIRES_OK(ctx, s->WriteScalar(step, *t, tag));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteScalarSummary\").Device(DEVICE_CPU),\n                        WriteScalarSummaryOp);\n\nclass WriteHistogramSummaryOp : public OpKernel {\n public:\n  explicit WriteHistogramSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"values\", &t));\n\n    OP_REQUIRES_OK(ctx, s->WriteHistogram(step, *t, tag));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteHistogramSummary\").Device(DEVICE_CPU),\n                        WriteHistogramSummaryOp);\n\nclass WriteImageSummaryOp : public OpKernel {\n public:\n  explicit WriteImageSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    int64_t max_images_tmp;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_images\", &max_images_tmp));\n    OP_REQUIRES(ctx, max_images_tmp < (1LL << 31),\n                errors::InvalidArgument(\"max_images must be < 2^31\"));\n    max_images_ = static_cast<int32>(max_images_tmp);\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n    const Tensor* bad_color;\n    OP_REQUIRES_OK(ctx, ctx->input(\"bad_color\", &bad_color));\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(bad_color->shape()),\n        errors::InvalidArgument(\"bad_color must be a vector, got shape \",\n                                bad_color->shape().DebugString()));\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n\n    OP_REQUIRES_OK(ctx, s->WriteImage(step, *t, tag, max_images_, *bad_color));\n  }\n\n private:\n  int32 max_images_;\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteImageSummary\").Device(DEVICE_CPU),\n                        WriteImageSummaryOp);\n\nclass WriteAudioSummaryOp : public OpKernel {\n public:\n  explicit WriteAudioSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_outputs\", &max_outputs_));\n    OP_REQUIRES(ctx, max_outputs_ > 0,\n                errors::InvalidArgument(\"max_outputs must be > 0\"));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"sample_rate\", &tmp));\n    const float sample_rate = tmp->scalar<float>()();\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n\n    OP_REQUIRES_OK(ctx,\n                   s->WriteAudio(step, *t, tag, max_outputs_, sample_rate));\n  }\n\n private:\n  int max_outputs_;\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteAudioSummary\").Device(DEVICE_CPU),\n                        WriteAudioSummaryOp);\n\nclass WriteGraphSummaryOp : public OpKernel {\n public:\n  explicit WriteGraphSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &t));\n    const int64_t step = t->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n    std::unique_ptr<GraphDef> graph{new GraphDef};\n    if (!ParseProtoUnlimited(graph.get(), t->scalar<tstring>()())) {\n      ctx->CtxFailureWithWarning(\n          errors::DataLoss(\"Bad tf.GraphDef binary proto tensor string\"));\n      return;\n    }\n    OP_REQUIRES_OK(ctx, s->WriteGraph(step, std::move(graph)));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteGraphSummary\").Device(DEVICE_CPU),\n                        WriteGraphSummaryOp);\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.python.summary.writer.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os.path\nimport shutil\nimport threading\nimport time\nimport warnings\n\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import summary_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import meta_graph_pb2\nfrom tensorflow.core.util import event_pb2\nfrom tensorflow.core.util.event_pb2 import SessionLog\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import meta_graph\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import summary_ops_v2\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.summary import plugin_asset\nfrom tensorflow.python.summary import summary_iterator\nfrom tensorflow.python.summary.writer import writer\nfrom tensorflow.python.summary.writer import writer_cache\nfrom tensorflow.python.util import compat\n\n\nclass FileWriterTestBase(object):\n\n  def _FileWriter(self, *args, **kwargs):\n    return writer.FileWriter(*args, **kwargs)\n\n  def _TestDir(self, test_name):\n    test_dir = os.path.join(self.get_temp_dir(), test_name)\n    return test_dir\n\n  def _CleanTestDir(self, test_name):\n    test_dir = self._TestDir(test_name)\n    if os.path.exists(test_dir):\n      shutil.rmtree(test_dir)\n    return test_dir\n\n  def _EventsReader(self, test_dir):\n    event_paths = glob.glob(os.path.join(test_dir, \"event*\"))\n    # If the tests runs multiple times in the same directory we can have\n    # more than one matching event file.  We only want to read the last one.\n    self.assertTrue(event_paths)\n    return summary_iterator.summary_iterator(event_paths[-1])\n\n  def _assertRecent(self, t):\n    self.assertTrue(abs(t - time.time()) < 5)\n\n  def _assertEventsWithGraph(self, test_dir, g, has_shapes):\n    meta_graph_def = meta_graph.create_meta_graph_def(\n        graph_def=g.as_graph_def(add_shapes=has_shapes))\n\n    rr = self._EventsReader(test_dir)\n\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n\n    # The next event should have the graph.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(0, ev.step)\n    ev_graph = graph_pb2.GraphDef()\n    ev_graph.ParseFromString(ev.graph_def)\n    self.assertProtoEquals(g.as_graph_def(add_shapes=has_shapes), ev_graph)\n\n    # The next event should have the metagraph.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(0, ev.step)\n    ev_meta_graph = meta_graph_pb2.MetaGraphDef()\n    ev_meta_graph.ParseFromString(ev.meta_graph_def)\n    self.assertProtoEquals(meta_graph_def, ev_meta_graph)\n\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testAddingSummaryGraphAndRunMetadata(self):\n    test_dir = self._CleanTestDir(\"basics\")\n    sw = self._FileWriter(test_dir)\n\n    sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 1)\n    sw.add_summary(\n        summary_pb2.Summary(\n            value=[summary_pb2.Summary.Value(\n                tag=\"mee\", simple_value=10.0)]),\n        10)\n    sw.add_summary(\n        summary_pb2.Summary(\n            value=[summary_pb2.Summary.Value(\n                tag=\"boo\", simple_value=20.0)]),\n        20)\n    with ops.Graph().as_default() as g:\n      constant_op.constant([0], name=\"zero\")\n    sw.add_graph(g, global_step=30)\n\n    run_metadata = config_pb2.RunMetadata()\n    device_stats = run_metadata.step_stats.dev_stats.add()\n    device_stats.device = \"test\"\n    sw.add_run_metadata(run_metadata, \"test run\", global_step=40)\n    sw.close()\n    rr = self._EventsReader(test_dir)\n\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n\n    # The next event should be the START message.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(1, ev.step)\n    self.assertEqual(SessionLog.START, ev.session_log.status)\n\n    # The next event should have the value 'mee=10.0'.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(10, ev.step)\n    self.assertProtoEquals(\"\"\"\n      value { tag: 'mee' simple_value: 10.0 }\n      \"\"\", ev.summary)\n\n    # The next event should have the value 'boo=20.0'.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(20, ev.step)\n    self.assertProtoEquals(\"\"\"\n      value { tag: 'boo' simple_value: 20.0 }\n      \"\"\", ev.summary)\n\n    # The next event should have the graph_def.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(30, ev.step)\n    ev_graph = graph_pb2.GraphDef()\n    ev_graph.ParseFromString(ev.graph_def)\n    self.assertProtoEquals(g.as_graph_def(add_shapes=True), ev_graph)\n\n    # The next event should have metadata for the run.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(40, ev.step)\n    self.assertEqual(\"test run\", ev.tagged_run_metadata.tag)\n    parsed_run_metadata = config_pb2.RunMetadata()\n    parsed_run_metadata.ParseFromString(ev.tagged_run_metadata.run_metadata)\n    self.assertProtoEquals(run_metadata, parsed_run_metadata)\n\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testGraphAsNamed(self):\n    test_dir = self._CleanTestDir(\"basics_named_graph\")\n    with ops.Graph().as_default() as g:\n      constant_op.constant([12], name=\"douze\")\n    sw = self._FileWriter(test_dir, graph=g)\n    sw.close()\n    self._assertEventsWithGraph(test_dir, g, True)\n\n  @test_util.run_deprecated_v1\n  def testGraphAsPositional(self):\n    test_dir = self._CleanTestDir(\"basics_positional_graph\")\n    with ops.Graph().as_default() as g:\n      constant_op.constant([12], name=\"douze\")\n    sw = self._FileWriter(test_dir, g)\n    sw.close()\n    self._assertEventsWithGraph(test_dir, g, True)\n\n  @test_util.run_deprecated_v1\n  def testGraphDefAsNamed(self):\n    test_dir = self._CleanTestDir(\"basics_named_graph_def\")\n    with ops.Graph().as_default() as g:\n      constant_op.constant([12], name=\"douze\")\n    gd = g.as_graph_def()\n    sw = self._FileWriter(test_dir, graph_def=gd)\n    sw.close()\n    self._assertEventsWithGraph(test_dir, g, False)\n\n  @test_util.run_deprecated_v1\n  def testGraphDefAsPositional(self):\n    test_dir = self._CleanTestDir(\"basics_positional_graph_def\")\n    with ops.Graph().as_default() as g:\n      constant_op.constant([12], name=\"douze\")\n    gd = g.as_graph_def()\n    sw = self._FileWriter(test_dir, gd)\n    sw.close()\n    self._assertEventsWithGraph(test_dir, g, False)\n\n  @test_util.run_deprecated_v1\n  def testGraphAndGraphDef(self):\n    with self.assertRaises(ValueError):\n      test_dir = self._CleanTestDir(\"basics_graph_and_graph_def\")\n      with ops.Graph().as_default() as g:\n        constant_op.constant([12], name=\"douze\")\n      gd = g.as_graph_def()\n      sw = self._FileWriter(test_dir, graph=g, graph_def=gd)\n      sw.close()\n\n  @test_util.run_deprecated_v1\n  def testNeitherGraphNorGraphDef(self):\n    with self.assertRaises(TypeError):\n      test_dir = self._CleanTestDir(\"basics_string_instead_of_graph\")\n      sw = self._FileWriter(test_dir, \"string instead of graph object\")\n      sw.close()\n\n  @test_util.run_deprecated_v1\n  def testCloseAndReopen(self):\n    test_dir = self._CleanTestDir(\"close_and_reopen\")\n    sw = self._FileWriter(test_dir)\n    sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 1)\n    sw.close()\n    # Sleep at least one second to make sure we get a new event file name.\n    time.sleep(1.2)\n    sw.reopen()\n    sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 2)\n    sw.close()\n\n    # We should now have 2 events files.\n    event_paths = sorted(glob.glob(os.path.join(test_dir, \"event*\")))\n    self.assertEqual(2, len(event_paths))\n\n    # Check the first file contents.\n    rr = summary_iterator.summary_iterator(event_paths[0])\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n    # The next event should be the START message.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(1, ev.step)\n    self.assertEqual(SessionLog.START, ev.session_log.status)\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n    # Check the second file contents.\n    rr = summary_iterator.summary_iterator(event_paths[1])\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n    # The next event should be the START message.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(2, ev.step)\n    self.assertEqual(SessionLog.START, ev.session_log.status)\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testNonBlockingClose(self):\n    test_dir = self._CleanTestDir(\"non_blocking_close\")\n    sw = self._FileWriter(test_dir)\n    # Sleep 1.2 seconds to make sure event queue is empty.\n    time.sleep(1.2)\n    time_before_close = time.time()\n    sw.close()\n    self._assertRecent(time_before_close)\n\n  @test_util.run_deprecated_v1\n  def testUseAfterClose(self):\n    test_dir = self._CleanTestDir(\"use_after_close\")\n    sw = self._FileWriter(test_dir)\n    sw.close()\n    with warnings.catch_warnings(record=True) as triggered:\n      warnings.simplefilter(\"always\")\n      self.assertFalse(triggered)\n      sw.add_summary(summary_pb2.Summary())\n      sw.add_session_log(event_pb2.SessionLog())\n      sw.add_graph(ops.Graph())\n\n    self.assertEqual(len(triggered), 3)\n    for w in triggered:\n      self.assertEqual(w.category, UserWarning)\n\n  @test_util.run_deprecated_v1\n  def testWithStatement(self):\n    test_dir = self._CleanTestDir(\"with_statement\")\n    with self._FileWriter(test_dir) as sw:\n      sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 1)\n    event_paths = sorted(glob.glob(os.path.join(test_dir, \"event*\")))\n    self.assertEqual(1, len(event_paths))\n\n  # Checks that values returned from session Run() calls are added correctly to\n  # summaries.  These are numpy types so we need to check they fit in the\n  # protocol buffers correctly.\n  @test_util.run_deprecated_v1\n  def testAddingSummariesFromSessionRunCalls(self):\n    test_dir = self._CleanTestDir(\"global_step\")\n    sw = self._FileWriter(test_dir)\n    with self.cached_session():\n      i = constant_op.constant(1, dtype=dtypes.int32, shape=[])\n      l = constant_op.constant(2, dtype=dtypes.int64, shape=[])\n      # Test the summary can be passed serialized.\n      summ = summary_pb2.Summary(\n          value=[summary_pb2.Summary.Value(\n              tag=\"i\", simple_value=1.0)])\n      sw.add_summary(summ.SerializeToString(), self.evaluate(i))\n      sw.add_summary(\n          summary_pb2.Summary(\n              value=[summary_pb2.Summary.Value(tag=\"l\", simple_value=2.0)]),\n          self.evaluate(l))\n      sw.close()\n\n    rr = self._EventsReader(test_dir)\n\n    # File_version.\n    ev = next(rr)\n    self.assertTrue(ev)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n\n    # Summary passed serialized.\n    ev = next(rr)\n    self.assertTrue(ev)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(1, ev.step)\n    self.assertProtoEquals(\"\"\"\n      value { tag: 'i' simple_value: 1.0 }\n      \"\"\", ev.summary)\n\n    # Summary passed as SummaryObject.\n    ev = next(rr)\n    self.assertTrue(ev)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(2, ev.step)\n    self.assertProtoEquals(\"\"\"\n      value { tag: 'l' simple_value: 2.0 }\n      \"\"\", ev.summary)\n\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testPluginMetadataStrippedFromSubsequentEvents(self):\n    test_dir = self._CleanTestDir(\"basics\")\n    sw = self._FileWriter(test_dir)\n\n    sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 1)\n\n    # We add 2 summaries with the same tags. They both have metadata. The writer\n    # should strip the metadata from the second one.\n    value = summary_pb2.Summary.Value(tag=\"foo\", simple_value=10.0)\n    value.metadata.plugin_data.plugin_name = \"bar\"\n    value.metadata.plugin_data.content = compat.as_bytes(\"... content ...\")\n    sw.add_summary(summary_pb2.Summary(value=[value]), 10)\n    value = summary_pb2.Summary.Value(tag=\"foo\", simple_value=10.0)\n    value.metadata.plugin_data.plugin_name = \"bar\"\n    value.metadata.plugin_data.content = compat.as_bytes(\"... content ...\")\n    sw.add_summary(summary_pb2.Summary(value=[value]), 10)\n\n    sw.close()\n    rr = self._EventsReader(test_dir)\n\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n\n    # The next event should be the START message.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(1, ev.step)\n    self.assertEqual(SessionLog.START, ev.session_log.status)\n\n    # This is the first event with tag foo. It should contain SummaryMetadata.\n    ev = next(rr)\n    self.assertProtoEquals(\"\"\"\n      value {\n        tag: \"foo\"\n        simple_value: 10.0\n        metadata {\n          plugin_data {\n            plugin_name: \"bar\"\n            content: \"... content ...\"\n          }\n        }\n      }\n      \"\"\", ev.summary)\n\n    # This is the second event with tag foo. It should lack SummaryMetadata\n    # because the file writer should have stripped it.\n    ev = next(rr)\n    self.assertProtoEquals(\"\"\"\n      value {\n        tag: \"foo\"\n        simple_value: 10.0\n      }\n      \"\"\", ev.summary)\n\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testFileWriterWithSuffix(self):\n    test_dir = self._CleanTestDir(\"test_suffix\")\n    sw = self._FileWriter(test_dir, filename_suffix=\"_test_suffix\")\n    for _ in range(10):\n      sw.add_summary(\n          summary_pb2.Summary(value=[\n              summary_pb2.Summary.Value(tag=\"float_ten\", simple_value=10.0)\n          ]),\n          10)\n      sw.close()\n      sw.reopen()\n    sw.close()\n    event_filenames = glob.glob(os.path.join(test_dir, \"event*\"))\n    for filename in event_filenames:\n      self.assertTrue(filename.endswith(\"_test_suffix\"))\n\n  def testPluginAssetSerialized(self):\n    class ExamplePluginAsset(plugin_asset.PluginAsset):\n      plugin_name = \"example\"\n\n      def assets(self):\n        return {\"foo.txt\": \"foo!\", \"bar.txt\": \"bar!\"}\n\n    with ops.Graph().as_default() as g:\n      plugin_asset.get_plugin_asset(ExamplePluginAsset)\n\n      logdir = self.get_temp_dir()\n      fw = self._FileWriter(logdir)\n      fw.add_graph(g)\n    plugin_dir = os.path.join(logdir, writer._PLUGINS_DIR, \"example\")\n\n    with gfile.Open(os.path.join(plugin_dir, \"foo.txt\"), \"r\") as f:\n      content = f.read()\n    self.assertEqual(content, \"foo!\")\n\n    with gfile.Open(os.path.join(plugin_dir, \"bar.txt\"), \"r\") as f:\n      content = f.read()\n    self.assertEqual(content, \"bar!\")\n\n\nclass FakeWriteError(Exception):\n  pass\n\n\nclass FileWriterTestCase(FileWriterTestBase, test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testWriterException_raisedFromFlush(self):\n    test_dir = self.get_temp_dir()\n    sw = self._FileWriter(test_dir)\n    writer_thread = sw.event_writer._worker\n    with test.mock.patch.object(\n        writer_thread, \"_ev_writer\", autospec=True) as mock_writer:\n      # Coordinate threads to ensure both events are added before the writer\n      # thread dies, to avoid the second add_event() failing instead of flush().\n      second_event_added = threading.Event()\n      def _FakeWriteEvent(event):\n        del event  # unused\n        second_event_added.wait()\n        raise FakeWriteError()\n      mock_writer.WriteEvent.side_effect = _FakeWriteEvent\n      sw.add_event(event_pb2.Event())\n      sw.add_event(event_pb2.Event())\n      second_event_added.set()\n      with self.assertRaises(FakeWriteError):\n        sw.flush()\n\n  @test_util.run_deprecated_v1\n  def testWriterException_raisedFromClose(self):\n    test_dir = self.get_temp_dir()\n    sw = self._FileWriter(test_dir)\n    writer_thread = sw.event_writer._worker\n    with test.mock.patch.object(\n        writer_thread, \"_ev_writer\", autospec=True) as mock_writer:\n      mock_writer.WriteEvent.side_effect = FakeWriteError()\n      sw.add_event(event_pb2.Event())\n      with self.assertRaises(FakeWriteError):\n        sw.close()\n\n  @test_util.run_deprecated_v1\n  def testWriterException_raisedFromAddEvent(self):\n    test_dir = self.get_temp_dir()\n    sw = self._FileWriter(test_dir)\n    writer_thread = sw.event_writer._worker\n    with test.mock.patch.object(\n        writer_thread, \"_ev_writer\", autospec=True) as mock_writer:\n      mock_writer.WriteEvent.side_effect = FakeWriteError()\n      sw.add_event(event_pb2.Event())\n      # Wait for writer thread to exit first, then try to add a new event.\n      writer_thread.join()\n      with self.assertRaises(FakeWriteError):\n        sw.add_event(event_pb2.Event())\n\n  @test_util.run_deprecated_v1\n  def testWriterException_raisedFromPendingAddEvent(self):\n    test_dir = self.get_temp_dir()\n    # Set max_queue=1 to allow the third add_event() call to block (first event\n    # is consumed immediately, the second fills the queue, the third blocks).\n    sw = self._FileWriter(test_dir, max_queue=1)\n    writer_thread = sw.event_writer._worker\n    with test.mock.patch.object(\n        writer_thread, \"_ev_writer\", autospec=True) as mock_writer:\n      # Coordinate threads to ensure the first two events are added and then\n      # the writer thread sleeps briefly before exiting, to maximize the chance\n      # that the third add_event() reaches the pending blocked state before the\n      # queue closes on writer thread exit, since that's what we want to test.\n      second_event_added = threading.Event()\n      def _FakeWriteEvent(event):\n        del event  # unused\n        second_event_added.wait()\n        time.sleep(0.1)\n        raise FakeWriteError()\n      mock_writer.WriteEvent.side_effect = _FakeWriteEvent\n      sw.add_event(event_pb2.Event())\n      sw.add_event(event_pb2.Event())\n      second_event_added.set()\n      with self.assertRaises(FakeWriteError):\n        sw.add_event(event_pb2.Event())\n\n\nclass SessionBasedFileWriterTestCase(FileWriterTestBase, test.TestCase):\n  \"\"\"Tests for FileWriter behavior when passed a Session argument.\"\"\"\n\n  def _FileWriter(self, *args, **kwargs):\n    if \"session\" not in kwargs:\n      # Pass in test_session() as the session. It will be cached during this\n      # test method invocation so that any other use of test_session() with no\n      # graph should result in re-using the same underlying Session.\n      with self.cached_session() as sess:\n        kwargs[\"session\"] = sess\n        return writer.FileWriter(*args, **kwargs)\n    return writer.FileWriter(*args, **kwargs)\n\n  def _createTaggedSummary(self, tag):\n    summary = summary_pb2.Summary()\n    summary.value.add(tag=tag)\n    return summary\n\n  def testSharing_withOtherSessionBasedFileWriters(self):\n    logdir = self.get_temp_dir()\n    with session.Session() as sess:\n      # Initial file writer\n      writer1 = writer.FileWriter(session=sess, logdir=logdir)\n      writer1.add_summary(self._createTaggedSummary(\"one\"), 1)\n      writer1.flush()\n\n      # File writer, should share file with writer1\n      writer2 = writer.FileWriter(session=sess, logdir=logdir)\n      writer2.add_summary(self._createTaggedSummary(\"two\"), 2)\n      writer2.flush()\n\n      # File writer with different logdir (shouldn't be in this logdir at all)\n      writer3 = writer.FileWriter(session=sess, logdir=logdir + \"-other\")\n      writer3.add_summary(self._createTaggedSummary(\"three\"), 3)\n      writer3.flush()\n\n      # File writer in a different session (should be in separate file)\n      time.sleep(1.1)  # Ensure filename has a different timestamp\n      with session.Session() as other_sess:\n        writer4 = writer.FileWriter(session=other_sess, logdir=logdir)\n        writer4.add_summary(self._createTaggedSummary(\"four\"), 4)\n        writer4.flush()\n\n      # One more file writer, should share file with writer1\n      writer5 = writer.FileWriter(session=sess, logdir=logdir)\n      writer5.add_summary(self._createTaggedSummary(\"five\"), 5)\n      writer5.flush()\n\n    event_paths = iter(sorted(glob.glob(os.path.join(logdir, \"event*\"))))\n\n    # First file should have tags \"one\", \"two\", and \"five\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"one\", next(events).summary.value[0].tag)\n    self.assertEqual(\"two\", next(events).summary.value[0].tag)\n    self.assertEqual(\"five\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # Second file should have just \"four\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"four\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # No more files\n    self.assertRaises(StopIteration, lambda: next(event_paths))\n\n    # Just check that the other logdir file exists to be sure we wrote it\n    self.assertTrue(glob.glob(os.path.join(logdir + \"-other\", \"event*\")))\n\n  def testSharing_withExplicitSummaryFileWriters(self):\n    logdir = self.get_temp_dir()\n    with session.Session() as sess:\n      # Initial file writer via FileWriter(session=?)\n      writer1 = writer.FileWriter(session=sess, logdir=logdir)\n      writer1.add_summary(self._createTaggedSummary(\"one\"), 1)\n      writer1.flush()\n\n      # Next one via create_file_writer(), should use same file\n      writer2 = summary_ops_v2.create_file_writer(logdir=logdir)\n      with summary_ops_v2.always_record_summaries(), writer2.as_default():\n        summary2 = summary_ops_v2.scalar(\"two\", 2.0, step=2)\n      sess.run(writer2.init())\n      sess.run(summary2)\n      sess.run(writer2.flush())\n\n      # Next has different shared name, should be in separate file\n      time.sleep(1.1)  # Ensure filename has a different timestamp\n      writer3 = summary_ops_v2.create_file_writer(logdir=logdir, name=\"other\")\n      with summary_ops_v2.always_record_summaries(), writer3.as_default():\n        summary3 = summary_ops_v2.scalar(\"three\", 3.0, step=3)\n      sess.run(writer3.init())\n      sess.run(summary3)\n      sess.run(writer3.flush())\n\n      # Next uses a second session, should be in separate file\n      time.sleep(1.1)  # Ensure filename has a different timestamp\n      with session.Session() as other_sess:\n        writer4 = summary_ops_v2.create_file_writer(logdir=logdir)\n        with summary_ops_v2.always_record_summaries(), writer4.as_default():\n          summary4 = summary_ops_v2.scalar(\"four\", 4.0, step=4)\n        other_sess.run(writer4.init())\n        other_sess.run(summary4)\n        other_sess.run(writer4.flush())\n\n        # Next via FileWriter(session=?) uses same second session, should be in\n        # same separate file. (This checks sharing in the other direction)\n        writer5 = writer.FileWriter(session=other_sess, logdir=logdir)\n        writer5.add_summary(self._createTaggedSummary(\"five\"), 5)\n        writer5.flush()\n\n      # One more via create_file_writer(), should use same file\n      writer6 = summary_ops_v2.create_file_writer(logdir=logdir)\n      with summary_ops_v2.always_record_summaries(), writer6.as_default():\n        summary6 = summary_ops_v2.scalar(\"six\", 6.0, step=6)\n      sess.run(writer6.init())\n      sess.run(summary6)\n      sess.run(writer6.flush())\n\n    event_paths = iter(sorted(glob.glob(os.path.join(logdir, \"event*\"))))\n\n    # First file should have tags \"one\", \"two\", and \"six\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"one\", next(events).summary.value[0].tag)\n    self.assertEqual(\"two\", next(events).summary.value[0].tag)\n    self.assertEqual(\"six\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # Second file should have just \"three\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"three\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # Third file should have \"four\" and \"five\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"four\", next(events).summary.value[0].tag)\n    self.assertEqual(\"five\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # No more files\n    self.assertRaises(StopIteration, lambda: next(event_paths))\n\n\nclass FileWriterCacheTest(test.TestCase):\n  \"\"\"FileWriterCache tests.\"\"\"\n\n  def _test_dir(self, test_name):\n    \"\"\"Create an empty dir to use for tests.\n\n    Args:\n      test_name: Name of the test.\n\n    Returns:\n      Absolute path to the test directory.\n    \"\"\"\n    test_dir = os.path.join(self.get_temp_dir(), test_name)\n    if os.path.isdir(test_dir):\n      for f in glob.glob(\"%s/*\" % test_dir):\n        os.remove(f)\n    else:\n      os.makedirs(test_dir)\n    return test_dir\n\n  def test_cache(self):\n    with ops.Graph().as_default():\n      dir1 = self._test_dir(\"test_cache_1\")\n      dir2 = self._test_dir(\"test_cache_2\")\n      sw1 = writer_cache.FileWriterCache.get(dir1)\n      sw2 = writer_cache.FileWriterCache.get(dir2)\n      sw3 = writer_cache.FileWriterCache.get(dir1)\n      self.assertEqual(sw1, sw3)\n      self.assertFalse(sw1 == sw2)\n      sw1.close()\n      sw2.close()\n      events1 = glob.glob(os.path.join(dir1, \"event*\"))\n      self.assertTrue(events1)\n      events2 = glob.glob(os.path.join(dir2, \"event*\"))\n      self.assertTrue(events2)\n      events3 = glob.glob(os.path.join(\"nowriter\", \"event*\"))\n      self.assertFalse(events3)\n\n  def test_clear(self):\n    with ops.Graph().as_default():\n      dir1 = self._test_dir(\"test_clear\")\n      sw1 = writer_cache.FileWriterCache.get(dir1)\n      writer_cache.FileWriterCache.clear()\n      sw2 = writer_cache.FileWriterCache.get(dir1)\n      self.assertFalse(sw1 == sw2)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/graph.pb.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/summary.pb.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/lib/db/sqlite.h\"\n#include \"tensorflow/core/platform/protobuf.h\"\n#include \"tensorflow/core/summary/schema.h\"\n#include \"tensorflow/core/summary/summary_db_writer.h\"\n#include \"tensorflow/core/summary/summary_file_writer.h\"\n#include \"tensorflow/core/util/event.pb.h\"\n\nnamespace tensorflow {\n\nREGISTER_KERNEL_BUILDER(Name(\"SummaryWriter\").Device(DEVICE_CPU),\n                        ResourceHandleOp<SummaryWriterInterface>);\n\nclass CreateSummaryFileWriterOp : public OpKernel {\n public:\n  explicit CreateSummaryFileWriterOp(OpKernelConstruction* ctx)\n      : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"logdir\", &tmp));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tmp->shape()),\n                errors::InvalidArgument(\"logdir must be a scalar\"));\n    const string logdir = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"max_queue\", &tmp));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tmp->shape()),\n                errors::InvalidArgument(\"max_queue must be a scalar\"));\n    const int32_t max_queue = tmp->scalar<int32>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"flush_millis\", &tmp));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tmp->shape()),\n                errors::InvalidArgument(\"flush_millis must be a scalar\"));\n    const int32_t flush_millis = tmp->scalar<int32>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"filename_suffix\", &tmp));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tmp->shape()),\n                errors::InvalidArgument(\"filename_suffix must be a scalar\"));\n    const string filename_suffix = tmp->scalar<tstring>()();\n\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupOrCreateResource<SummaryWriterInterface>(\n                            ctx, HandleFromInput(ctx, 0), &s,\n                            [max_queue, flush_millis, logdir, filename_suffix,\n                             ctx](SummaryWriterInterface** s) {\n                              return CreateSummaryFileWriter(\n                                  max_queue, flush_millis, logdir,\n                                  filename_suffix, ctx->env(), s);\n                            }));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"CreateSummaryFileWriter\").Device(DEVICE_CPU),\n                        CreateSummaryFileWriterOp);\n\nclass CreateSummaryDbWriterOp : public OpKernel {\n public:\n  explicit CreateSummaryDbWriterOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"db_uri\", &tmp));\n    const string db_uri = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"experiment_name\", &tmp));\n    const string experiment_name = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"run_name\", &tmp));\n    const string run_name = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"user_name\", &tmp));\n    const string user_name = tmp->scalar<tstring>()();\n\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(\n        ctx,\n        LookupOrCreateResource<SummaryWriterInterface>(\n            ctx, HandleFromInput(ctx, 0), &s,\n            [db_uri, experiment_name, run_name, user_name,\n             ctx](SummaryWriterInterface** s) {\n              Sqlite* db;\n              TF_RETURN_IF_ERROR(Sqlite::Open(\n                  db_uri, SQLITE_OPEN_READWRITE | SQLITE_OPEN_CREATE, &db));\n              core::ScopedUnref unref(db);\n              TF_RETURN_IF_ERROR(SetupTensorboardSqliteDb(db));\n              TF_RETURN_IF_ERROR(CreateSummaryDbWriter(\n                  db, experiment_name, run_name, user_name, ctx->env(), s));\n              return Status::OK();\n            }));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"CreateSummaryDbWriter\").Device(DEVICE_CPU),\n                        CreateSummaryDbWriterOp);\n\nclass FlushSummaryWriterOp : public OpKernel {\n public:\n  explicit FlushSummaryWriterOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    OP_REQUIRES_OK(ctx, s->Flush());\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"FlushSummaryWriter\").Device(DEVICE_CPU),\n                        FlushSummaryWriterOp);\n\nclass CloseSummaryWriterOp : public OpKernel {\n public:\n  explicit CloseSummaryWriterOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, DeleteResource<SummaryWriterInterface>(\n                            ctx, HandleFromInput(ctx, 0)));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"CloseSummaryWriter\").Device(DEVICE_CPU),\n                        CloseSummaryWriterOp);\n\nclass WriteSummaryOp : public OpKernel {\n public:\n  explicit WriteSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"summary_metadata\", &tmp));\n    const string& serialized_metadata = tmp->scalar<tstring>()();\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n\n    OP_REQUIRES_OK(ctx, s->WriteTensor(step, *t, tag, serialized_metadata));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteSummary\").Device(DEVICE_CPU),\n                        WriteSummaryOp);\n\nclass WriteRawProtoSummaryOp : public OpKernel {\n public:\n  explicit WriteRawProtoSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tmp->shape()),\n                errors::InvalidArgument(\"step must be scalar, got shape \",\n                                        tmp->shape().DebugString()));\n    const int64_t step = tmp->scalar<int64_t>()();\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n    std::unique_ptr<Event> event{new Event};\n    event->set_step(step);\n    event->set_wall_time(static_cast<double>(ctx->env()->NowMicros()) / 1.0e6);\n    // Each Summary proto contains just one repeated field \"value\" of Value\n    // messages with the actual data, so repeated Merge() is equivalent to\n    // concatenating all the Value entries together into a single Event.\n    const auto summary_pbs = t->flat<tstring>();\n    for (int i = 0; i < summary_pbs.size(); ++i) {\n      if (!event->mutable_summary()->MergeFromString(summary_pbs(i))) {\n        ctx->CtxFailureWithWarning(errors::DataLoss(\n            \"Bad tf.compat.v1.Summary binary proto tensor string at index \",\n            i));\n        return;\n      }\n    }\n    OP_REQUIRES_OK(ctx, s->WriteEvent(std::move(event)));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteRawProtoSummary\").Device(DEVICE_CPU),\n                        WriteRawProtoSummaryOp);\n\nclass ImportEventOp : public OpKernel {\n public:\n  explicit ImportEventOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"event\", &t));\n    std::unique_ptr<Event> event{new Event};\n    if (!ParseProtoUnlimited(event.get(), t->scalar<tstring>()())) {\n      ctx->CtxFailureWithWarning(\n          errors::DataLoss(\"Bad tf.Event binary proto tensor string\"));\n      return;\n    }\n    OP_REQUIRES_OK(ctx, s->WriteEvent(std::move(event)));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"ImportEvent\").Device(DEVICE_CPU), ImportEventOp);\n\nclass WriteScalarSummaryOp : public OpKernel {\n public:\n  explicit WriteScalarSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"value\", &t));\n\n    OP_REQUIRES_OK(ctx, s->WriteScalar(step, *t, tag));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteScalarSummary\").Device(DEVICE_CPU),\n                        WriteScalarSummaryOp);\n\nclass WriteHistogramSummaryOp : public OpKernel {\n public:\n  explicit WriteHistogramSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"values\", &t));\n\n    OP_REQUIRES_OK(ctx, s->WriteHistogram(step, *t, tag));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteHistogramSummary\").Device(DEVICE_CPU),\n                        WriteHistogramSummaryOp);\n\nclass WriteImageSummaryOp : public OpKernel {\n public:\n  explicit WriteImageSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    int64_t max_images_tmp;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_images\", &max_images_tmp));\n    OP_REQUIRES(ctx, max_images_tmp < (1LL << 31),\n                errors::InvalidArgument(\"max_images must be < 2^31\"));\n    max_images_ = static_cast<int32>(max_images_tmp);\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n    const Tensor* bad_color;\n    OP_REQUIRES_OK(ctx, ctx->input(\"bad_color\", &bad_color));\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(bad_color->shape()),\n        errors::InvalidArgument(\"bad_color must be a vector, got shape \",\n                                bad_color->shape().DebugString()));\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n\n    OP_REQUIRES_OK(ctx, s->WriteImage(step, *t, tag, max_images_, *bad_color));\n  }\n\n private:\n  int32 max_images_;\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteImageSummary\").Device(DEVICE_CPU),\n                        WriteImageSummaryOp);\n\nclass WriteAudioSummaryOp : public OpKernel {\n public:\n  explicit WriteAudioSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_outputs\", &max_outputs_));\n    OP_REQUIRES(ctx, max_outputs_ > 0,\n                errors::InvalidArgument(\"max_outputs must be > 0\"));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* tmp;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &tmp));\n    const int64_t step = tmp->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tag\", &tmp));\n    const string& tag = tmp->scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"sample_rate\", &tmp));\n    const float sample_rate = tmp->scalar<float>()();\n\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n\n    OP_REQUIRES_OK(ctx,\n                   s->WriteAudio(step, *t, tag, max_outputs_, sample_rate));\n  }\n\n private:\n  int max_outputs_;\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteAudioSummary\").Device(DEVICE_CPU),\n                        WriteAudioSummaryOp);\n\nclass WriteGraphSummaryOp : public OpKernel {\n public:\n  explicit WriteGraphSummaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    core::RefCountPtr<SummaryWriterInterface> s;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &s));\n    const Tensor* t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"step\", &t));\n    const int64_t step = t->scalar<int64_t>()();\n    OP_REQUIRES_OK(ctx, ctx->input(\"tensor\", &t));\n    std::unique_ptr<GraphDef> graph{new GraphDef};\n    if (!ParseProtoUnlimited(graph.get(), t->scalar<tstring>()())) {\n      ctx->CtxFailureWithWarning(\n          errors::DataLoss(\"Bad tf.GraphDef binary proto tensor string\"));\n      return;\n    }\n    OP_REQUIRES_OK(ctx, s->WriteGraph(step, std::move(graph)));\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"WriteGraphSummary\").Device(DEVICE_CPU),\n                        WriteGraphSummaryOp);\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.python.summary.writer.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os.path\nimport shutil\nimport threading\nimport time\nimport warnings\n\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import summary_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import meta_graph_pb2\nfrom tensorflow.core.util import event_pb2\nfrom tensorflow.core.util.event_pb2 import SessionLog\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import meta_graph\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import summary_ops_v2\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.summary import plugin_asset\nfrom tensorflow.python.summary import summary_iterator\nfrom tensorflow.python.summary.writer import writer\nfrom tensorflow.python.summary.writer import writer_cache\nfrom tensorflow.python.util import compat\n\n\nclass FileWriterTestBase(object):\n\n  def _FileWriter(self, *args, **kwargs):\n    return writer.FileWriter(*args, **kwargs)\n\n  def _TestDir(self, test_name):\n    test_dir = os.path.join(self.get_temp_dir(), test_name)\n    return test_dir\n\n  def _CleanTestDir(self, test_name):\n    test_dir = self._TestDir(test_name)\n    if os.path.exists(test_dir):\n      shutil.rmtree(test_dir)\n    return test_dir\n\n  def _EventsReader(self, test_dir):\n    event_paths = glob.glob(os.path.join(test_dir, \"event*\"))\n    # If the tests runs multiple times in the same directory we can have\n    # more than one matching event file.  We only want to read the last one.\n    self.assertTrue(event_paths)\n    return summary_iterator.summary_iterator(event_paths[-1])\n\n  def _assertRecent(self, t):\n    self.assertTrue(abs(t - time.time()) < 5)\n\n  def _assertEventsWithGraph(self, test_dir, g, has_shapes):\n    meta_graph_def = meta_graph.create_meta_graph_def(\n        graph_def=g.as_graph_def(add_shapes=has_shapes))\n\n    rr = self._EventsReader(test_dir)\n\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n\n    # The next event should have the graph.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(0, ev.step)\n    ev_graph = graph_pb2.GraphDef()\n    ev_graph.ParseFromString(ev.graph_def)\n    self.assertProtoEquals(g.as_graph_def(add_shapes=has_shapes), ev_graph)\n\n    # The next event should have the metagraph.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(0, ev.step)\n    ev_meta_graph = meta_graph_pb2.MetaGraphDef()\n    ev_meta_graph.ParseFromString(ev.meta_graph_def)\n    self.assertProtoEquals(meta_graph_def, ev_meta_graph)\n\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testAddingSummaryGraphAndRunMetadata(self):\n    test_dir = self._CleanTestDir(\"basics\")\n    sw = self._FileWriter(test_dir)\n\n    sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 1)\n    sw.add_summary(\n        summary_pb2.Summary(\n            value=[summary_pb2.Summary.Value(\n                tag=\"mee\", simple_value=10.0)]),\n        10)\n    sw.add_summary(\n        summary_pb2.Summary(\n            value=[summary_pb2.Summary.Value(\n                tag=\"boo\", simple_value=20.0)]),\n        20)\n    with ops.Graph().as_default() as g:\n      constant_op.constant([0], name=\"zero\")\n    sw.add_graph(g, global_step=30)\n\n    run_metadata = config_pb2.RunMetadata()\n    device_stats = run_metadata.step_stats.dev_stats.add()\n    device_stats.device = \"test\"\n    sw.add_run_metadata(run_metadata, \"test run\", global_step=40)\n    sw.close()\n    rr = self._EventsReader(test_dir)\n\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n\n    # The next event should be the START message.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(1, ev.step)\n    self.assertEqual(SessionLog.START, ev.session_log.status)\n\n    # The next event should have the value 'mee=10.0'.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(10, ev.step)\n    self.assertProtoEquals(\"\"\"\n      value { tag: 'mee' simple_value: 10.0 }\n      \"\"\", ev.summary)\n\n    # The next event should have the value 'boo=20.0'.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(20, ev.step)\n    self.assertProtoEquals(\"\"\"\n      value { tag: 'boo' simple_value: 20.0 }\n      \"\"\", ev.summary)\n\n    # The next event should have the graph_def.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(30, ev.step)\n    ev_graph = graph_pb2.GraphDef()\n    ev_graph.ParseFromString(ev.graph_def)\n    self.assertProtoEquals(g.as_graph_def(add_shapes=True), ev_graph)\n\n    # The next event should have metadata for the run.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(40, ev.step)\n    self.assertEqual(\"test run\", ev.tagged_run_metadata.tag)\n    parsed_run_metadata = config_pb2.RunMetadata()\n    parsed_run_metadata.ParseFromString(ev.tagged_run_metadata.run_metadata)\n    self.assertProtoEquals(run_metadata, parsed_run_metadata)\n\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testGraphAsNamed(self):\n    test_dir = self._CleanTestDir(\"basics_named_graph\")\n    with ops.Graph().as_default() as g:\n      constant_op.constant([12], name=\"douze\")\n    sw = self._FileWriter(test_dir, graph=g)\n    sw.close()\n    self._assertEventsWithGraph(test_dir, g, True)\n\n  @test_util.run_deprecated_v1\n  def testGraphAsPositional(self):\n    test_dir = self._CleanTestDir(\"basics_positional_graph\")\n    with ops.Graph().as_default() as g:\n      constant_op.constant([12], name=\"douze\")\n    sw = self._FileWriter(test_dir, g)\n    sw.close()\n    self._assertEventsWithGraph(test_dir, g, True)\n\n  @test_util.run_deprecated_v1\n  def testGraphDefAsNamed(self):\n    test_dir = self._CleanTestDir(\"basics_named_graph_def\")\n    with ops.Graph().as_default() as g:\n      constant_op.constant([12], name=\"douze\")\n    gd = g.as_graph_def()\n    sw = self._FileWriter(test_dir, graph_def=gd)\n    sw.close()\n    self._assertEventsWithGraph(test_dir, g, False)\n\n  @test_util.run_deprecated_v1\n  def testGraphDefAsPositional(self):\n    test_dir = self._CleanTestDir(\"basics_positional_graph_def\")\n    with ops.Graph().as_default() as g:\n      constant_op.constant([12], name=\"douze\")\n    gd = g.as_graph_def()\n    sw = self._FileWriter(test_dir, gd)\n    sw.close()\n    self._assertEventsWithGraph(test_dir, g, False)\n\n  @test_util.run_deprecated_v1\n  def testGraphAndGraphDef(self):\n    with self.assertRaises(ValueError):\n      test_dir = self._CleanTestDir(\"basics_graph_and_graph_def\")\n      with ops.Graph().as_default() as g:\n        constant_op.constant([12], name=\"douze\")\n      gd = g.as_graph_def()\n      sw = self._FileWriter(test_dir, graph=g, graph_def=gd)\n      sw.close()\n\n  @test_util.run_deprecated_v1\n  def testNeitherGraphNorGraphDef(self):\n    with self.assertRaises(TypeError):\n      test_dir = self._CleanTestDir(\"basics_string_instead_of_graph\")\n      sw = self._FileWriter(test_dir, \"string instead of graph object\")\n      sw.close()\n\n  @test_util.run_deprecated_v1\n  def testCloseAndReopen(self):\n    test_dir = self._CleanTestDir(\"close_and_reopen\")\n    sw = self._FileWriter(test_dir)\n    sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 1)\n    sw.close()\n    # Sleep at least one second to make sure we get a new event file name.\n    time.sleep(1.2)\n    sw.reopen()\n    sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 2)\n    sw.close()\n\n    # We should now have 2 events files.\n    event_paths = sorted(glob.glob(os.path.join(test_dir, \"event*\")))\n    self.assertEqual(2, len(event_paths))\n\n    # Check the first file contents.\n    rr = summary_iterator.summary_iterator(event_paths[0])\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n    # The next event should be the START message.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(1, ev.step)\n    self.assertEqual(SessionLog.START, ev.session_log.status)\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n    # Check the second file contents.\n    rr = summary_iterator.summary_iterator(event_paths[1])\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n    # The next event should be the START message.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(2, ev.step)\n    self.assertEqual(SessionLog.START, ev.session_log.status)\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testNonBlockingClose(self):\n    test_dir = self._CleanTestDir(\"non_blocking_close\")\n    sw = self._FileWriter(test_dir)\n    # Sleep 1.2 seconds to make sure event queue is empty.\n    time.sleep(1.2)\n    time_before_close = time.time()\n    sw.close()\n    self._assertRecent(time_before_close)\n\n  @test_util.run_deprecated_v1\n  def testUseAfterClose(self):\n    test_dir = self._CleanTestDir(\"use_after_close\")\n    sw = self._FileWriter(test_dir)\n    sw.close()\n    with warnings.catch_warnings(record=True) as triggered:\n      warnings.simplefilter(\"always\")\n      self.assertFalse(triggered)\n      sw.add_summary(summary_pb2.Summary())\n      sw.add_session_log(event_pb2.SessionLog())\n      sw.add_graph(ops.Graph())\n\n    self.assertEqual(len(triggered), 3)\n    for w in triggered:\n      self.assertEqual(w.category, UserWarning)\n\n  @test_util.run_deprecated_v1\n  def testWithStatement(self):\n    test_dir = self._CleanTestDir(\"with_statement\")\n    with self._FileWriter(test_dir) as sw:\n      sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 1)\n    event_paths = sorted(glob.glob(os.path.join(test_dir, \"event*\")))\n    self.assertEqual(1, len(event_paths))\n\n  # Checks that values returned from session Run() calls are added correctly to\n  # summaries.  These are numpy types so we need to check they fit in the\n  # protocol buffers correctly.\n  @test_util.run_deprecated_v1\n  def testAddingSummariesFromSessionRunCalls(self):\n    test_dir = self._CleanTestDir(\"global_step\")\n    sw = self._FileWriter(test_dir)\n    with self.cached_session():\n      i = constant_op.constant(1, dtype=dtypes.int32, shape=[])\n      l = constant_op.constant(2, dtype=dtypes.int64, shape=[])\n      # Test the summary can be passed serialized.\n      summ = summary_pb2.Summary(\n          value=[summary_pb2.Summary.Value(\n              tag=\"i\", simple_value=1.0)])\n      sw.add_summary(summ.SerializeToString(), self.evaluate(i))\n      sw.add_summary(\n          summary_pb2.Summary(\n              value=[summary_pb2.Summary.Value(tag=\"l\", simple_value=2.0)]),\n          self.evaluate(l))\n      sw.close()\n\n    rr = self._EventsReader(test_dir)\n\n    # File_version.\n    ev = next(rr)\n    self.assertTrue(ev)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n\n    # Summary passed serialized.\n    ev = next(rr)\n    self.assertTrue(ev)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(1, ev.step)\n    self.assertProtoEquals(\"\"\"\n      value { tag: 'i' simple_value: 1.0 }\n      \"\"\", ev.summary)\n\n    # Summary passed as SummaryObject.\n    ev = next(rr)\n    self.assertTrue(ev)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(2, ev.step)\n    self.assertProtoEquals(\"\"\"\n      value { tag: 'l' simple_value: 2.0 }\n      \"\"\", ev.summary)\n\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testPluginMetadataStrippedFromSubsequentEvents(self):\n    test_dir = self._CleanTestDir(\"basics\")\n    sw = self._FileWriter(test_dir)\n\n    sw.add_session_log(event_pb2.SessionLog(status=SessionLog.START), 1)\n\n    # We add 2 summaries with the same tags. They both have metadata. The writer\n    # should strip the metadata from the second one.\n    value = summary_pb2.Summary.Value(tag=\"foo\", simple_value=10.0)\n    value.metadata.plugin_data.plugin_name = \"bar\"\n    value.metadata.plugin_data.content = compat.as_bytes(\"... content ...\")\n    sw.add_summary(summary_pb2.Summary(value=[value]), 10)\n    value = summary_pb2.Summary.Value(tag=\"foo\", simple_value=10.0)\n    value.metadata.plugin_data.plugin_name = \"bar\"\n    value.metadata.plugin_data.content = compat.as_bytes(\"... content ...\")\n    sw.add_summary(summary_pb2.Summary(value=[value]), 10)\n\n    sw.close()\n    rr = self._EventsReader(test_dir)\n\n    # The first event should list the file_version.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(\"brain.Event:2\", ev.file_version)\n\n    # The next event should be the START message.\n    ev = next(rr)\n    self._assertRecent(ev.wall_time)\n    self.assertEqual(1, ev.step)\n    self.assertEqual(SessionLog.START, ev.session_log.status)\n\n    # This is the first event with tag foo. It should contain SummaryMetadata.\n    ev = next(rr)\n    self.assertProtoEquals(\"\"\"\n      value {\n        tag: \"foo\"\n        simple_value: 10.0\n        metadata {\n          plugin_data {\n            plugin_name: \"bar\"\n            content: \"... content ...\"\n          }\n        }\n      }\n      \"\"\", ev.summary)\n\n    # This is the second event with tag foo. It should lack SummaryMetadata\n    # because the file writer should have stripped it.\n    ev = next(rr)\n    self.assertProtoEquals(\"\"\"\n      value {\n        tag: \"foo\"\n        simple_value: 10.0\n      }\n      \"\"\", ev.summary)\n\n    # We should be done.\n    self.assertRaises(StopIteration, lambda: next(rr))\n\n  @test_util.run_deprecated_v1\n  def testFileWriterWithSuffix(self):\n    test_dir = self._CleanTestDir(\"test_suffix\")\n    sw = self._FileWriter(test_dir, filename_suffix=\"_test_suffix\")\n    for _ in range(10):\n      sw.add_summary(\n          summary_pb2.Summary(value=[\n              summary_pb2.Summary.Value(tag=\"float_ten\", simple_value=10.0)\n          ]),\n          10)\n      sw.close()\n      sw.reopen()\n    sw.close()\n    event_filenames = glob.glob(os.path.join(test_dir, \"event*\"))\n    for filename in event_filenames:\n      self.assertTrue(filename.endswith(\"_test_suffix\"))\n\n  def testPluginAssetSerialized(self):\n    class ExamplePluginAsset(plugin_asset.PluginAsset):\n      plugin_name = \"example\"\n\n      def assets(self):\n        return {\"foo.txt\": \"foo!\", \"bar.txt\": \"bar!\"}\n\n    with ops.Graph().as_default() as g:\n      plugin_asset.get_plugin_asset(ExamplePluginAsset)\n\n      logdir = self.get_temp_dir()\n      fw = self._FileWriter(logdir)\n      fw.add_graph(g)\n    plugin_dir = os.path.join(logdir, writer._PLUGINS_DIR, \"example\")\n\n    with gfile.Open(os.path.join(plugin_dir, \"foo.txt\"), \"r\") as f:\n      content = f.read()\n    self.assertEqual(content, \"foo!\")\n\n    with gfile.Open(os.path.join(plugin_dir, \"bar.txt\"), \"r\") as f:\n      content = f.read()\n    self.assertEqual(content, \"bar!\")\n\n\nclass FakeWriteError(Exception):\n  pass\n\n\nclass FileWriterTestCase(FileWriterTestBase, test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testWriterException_raisedFromFlush(self):\n    test_dir = self.get_temp_dir()\n    sw = self._FileWriter(test_dir)\n    writer_thread = sw.event_writer._worker\n    with test.mock.patch.object(\n        writer_thread, \"_ev_writer\", autospec=True) as mock_writer:\n      # Coordinate threads to ensure both events are added before the writer\n      # thread dies, to avoid the second add_event() failing instead of flush().\n      second_event_added = threading.Event()\n      def _FakeWriteEvent(event):\n        del event  # unused\n        second_event_added.wait()\n        raise FakeWriteError()\n      mock_writer.WriteEvent.side_effect = _FakeWriteEvent\n      sw.add_event(event_pb2.Event())\n      sw.add_event(event_pb2.Event())\n      second_event_added.set()\n      with self.assertRaises(FakeWriteError):\n        sw.flush()\n\n  @test_util.run_deprecated_v1\n  def testWriterException_raisedFromClose(self):\n    test_dir = self.get_temp_dir()\n    sw = self._FileWriter(test_dir)\n    writer_thread = sw.event_writer._worker\n    with test.mock.patch.object(\n        writer_thread, \"_ev_writer\", autospec=True) as mock_writer:\n      mock_writer.WriteEvent.side_effect = FakeWriteError()\n      sw.add_event(event_pb2.Event())\n      with self.assertRaises(FakeWriteError):\n        sw.close()\n\n  @test_util.run_deprecated_v1\n  def testWriterException_raisedFromAddEvent(self):\n    test_dir = self.get_temp_dir()\n    sw = self._FileWriter(test_dir)\n    writer_thread = sw.event_writer._worker\n    with test.mock.patch.object(\n        writer_thread, \"_ev_writer\", autospec=True) as mock_writer:\n      mock_writer.WriteEvent.side_effect = FakeWriteError()\n      sw.add_event(event_pb2.Event())\n      # Wait for writer thread to exit first, then try to add a new event.\n      writer_thread.join()\n      with self.assertRaises(FakeWriteError):\n        sw.add_event(event_pb2.Event())\n\n  @test_util.run_deprecated_v1\n  def testWriterException_raisedFromPendingAddEvent(self):\n    test_dir = self.get_temp_dir()\n    # Set max_queue=1 to allow the third add_event() call to block (first event\n    # is consumed immediately, the second fills the queue, the third blocks).\n    sw = self._FileWriter(test_dir, max_queue=1)\n    writer_thread = sw.event_writer._worker\n    with test.mock.patch.object(\n        writer_thread, \"_ev_writer\", autospec=True) as mock_writer:\n      # Coordinate threads to ensure the first two events are added and then\n      # the writer thread sleeps briefly before exiting, to maximize the chance\n      # that the third add_event() reaches the pending blocked state before the\n      # queue closes on writer thread exit, since that's what we want to test.\n      second_event_added = threading.Event()\n      def _FakeWriteEvent(event):\n        del event  # unused\n        second_event_added.wait()\n        time.sleep(0.1)\n        raise FakeWriteError()\n      mock_writer.WriteEvent.side_effect = _FakeWriteEvent\n      sw.add_event(event_pb2.Event())\n      sw.add_event(event_pb2.Event())\n      second_event_added.set()\n      with self.assertRaises(FakeWriteError):\n        sw.add_event(event_pb2.Event())\n\n\nclass SessionBasedFileWriterTestCase(FileWriterTestBase, test.TestCase):\n  \"\"\"Tests for FileWriter behavior when passed a Session argument.\"\"\"\n\n  def _FileWriter(self, *args, **kwargs):\n    if \"session\" not in kwargs:\n      # Pass in test_session() as the session. It will be cached during this\n      # test method invocation so that any other use of test_session() with no\n      # graph should result in re-using the same underlying Session.\n      with self.cached_session() as sess:\n        kwargs[\"session\"] = sess\n        return writer.FileWriter(*args, **kwargs)\n    return writer.FileWriter(*args, **kwargs)\n\n  def _createTaggedSummary(self, tag):\n    summary = summary_pb2.Summary()\n    summary.value.add(tag=tag)\n    return summary\n\n  def testSharing_withOtherSessionBasedFileWriters(self):\n    logdir = self.get_temp_dir()\n    with session.Session() as sess:\n      # Initial file writer\n      writer1 = writer.FileWriter(session=sess, logdir=logdir)\n      writer1.add_summary(self._createTaggedSummary(\"one\"), 1)\n      writer1.flush()\n\n      # File writer, should share file with writer1\n      writer2 = writer.FileWriter(session=sess, logdir=logdir)\n      writer2.add_summary(self._createTaggedSummary(\"two\"), 2)\n      writer2.flush()\n\n      # File writer with different logdir (shouldn't be in this logdir at all)\n      writer3 = writer.FileWriter(session=sess, logdir=logdir + \"-other\")\n      writer3.add_summary(self._createTaggedSummary(\"three\"), 3)\n      writer3.flush()\n\n      # File writer in a different session (should be in separate file)\n      time.sleep(1.1)  # Ensure filename has a different timestamp\n      with session.Session() as other_sess:\n        writer4 = writer.FileWriter(session=other_sess, logdir=logdir)\n        writer4.add_summary(self._createTaggedSummary(\"four\"), 4)\n        writer4.flush()\n\n      # One more file writer, should share file with writer1\n      writer5 = writer.FileWriter(session=sess, logdir=logdir)\n      writer5.add_summary(self._createTaggedSummary(\"five\"), 5)\n      writer5.flush()\n\n    event_paths = iter(sorted(glob.glob(os.path.join(logdir, \"event*\"))))\n\n    # First file should have tags \"one\", \"two\", and \"five\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"one\", next(events).summary.value[0].tag)\n    self.assertEqual(\"two\", next(events).summary.value[0].tag)\n    self.assertEqual(\"five\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # Second file should have just \"four\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"four\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # No more files\n    self.assertRaises(StopIteration, lambda: next(event_paths))\n\n    # Just check that the other logdir file exists to be sure we wrote it\n    self.assertTrue(glob.glob(os.path.join(logdir + \"-other\", \"event*\")))\n\n  def testSharing_withExplicitSummaryFileWriters(self):\n    logdir = self.get_temp_dir()\n    with session.Session() as sess:\n      # Initial file writer via FileWriter(session=?)\n      writer1 = writer.FileWriter(session=sess, logdir=logdir)\n      writer1.add_summary(self._createTaggedSummary(\"one\"), 1)\n      writer1.flush()\n\n      # Next one via create_file_writer(), should use same file\n      writer2 = summary_ops_v2.create_file_writer(logdir=logdir)\n      with summary_ops_v2.always_record_summaries(), writer2.as_default():\n        summary2 = summary_ops_v2.scalar(\"two\", 2.0, step=2)\n      sess.run(writer2.init())\n      sess.run(summary2)\n      sess.run(writer2.flush())\n\n      # Next has different shared name, should be in separate file\n      time.sleep(1.1)  # Ensure filename has a different timestamp\n      writer3 = summary_ops_v2.create_file_writer(logdir=logdir, name=\"other\")\n      with summary_ops_v2.always_record_summaries(), writer3.as_default():\n        summary3 = summary_ops_v2.scalar(\"three\", 3.0, step=3)\n      sess.run(writer3.init())\n      sess.run(summary3)\n      sess.run(writer3.flush())\n\n      # Next uses a second session, should be in separate file\n      time.sleep(1.1)  # Ensure filename has a different timestamp\n      with session.Session() as other_sess:\n        writer4 = summary_ops_v2.create_file_writer(logdir=logdir)\n        with summary_ops_v2.always_record_summaries(), writer4.as_default():\n          summary4 = summary_ops_v2.scalar(\"four\", 4.0, step=4)\n        other_sess.run(writer4.init())\n        other_sess.run(summary4)\n        other_sess.run(writer4.flush())\n\n        # Next via FileWriter(session=?) uses same second session, should be in\n        # same separate file. (This checks sharing in the other direction)\n        writer5 = writer.FileWriter(session=other_sess, logdir=logdir)\n        writer5.add_summary(self._createTaggedSummary(\"five\"), 5)\n        writer5.flush()\n\n      # One more via create_file_writer(), should use same file\n      writer6 = summary_ops_v2.create_file_writer(logdir=logdir)\n      with summary_ops_v2.always_record_summaries(), writer6.as_default():\n        summary6 = summary_ops_v2.scalar(\"six\", 6.0, step=6)\n      sess.run(writer6.init())\n      sess.run(summary6)\n      sess.run(writer6.flush())\n\n    event_paths = iter(sorted(glob.glob(os.path.join(logdir, \"event*\"))))\n\n    # First file should have tags \"one\", \"two\", and \"six\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"one\", next(events).summary.value[0].tag)\n    self.assertEqual(\"two\", next(events).summary.value[0].tag)\n    self.assertEqual(\"six\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # Second file should have just \"three\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"three\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # Third file should have \"four\" and \"five\"\n    events = summary_iterator.summary_iterator(next(event_paths))\n    self.assertEqual(\"brain.Event:2\", next(events).file_version)\n    self.assertEqual(\"four\", next(events).summary.value[0].tag)\n    self.assertEqual(\"five\", next(events).summary.value[0].tag)\n    self.assertRaises(StopIteration, lambda: next(events))\n\n    # No more files\n    self.assertRaises(StopIteration, lambda: next(event_paths))\n\n  def testSummaryFileWritersInvalidInput(self):\n    # Test case for GitHub issue 46909\n    logdir = self.get_temp_dir()\n    with session.Session() as sess:\n      with self.assertRaises(errors_impl.InvalidArgumentError):\n        writer = summary_ops_v2.create_file_writer(\n            logdir=logdir, flush_millis=[1, 2])\n        sess.run(writer.init())\n        sess.run(writer.flush())\n\n\nclass FileWriterCacheTest(test.TestCase):\n  \"\"\"FileWriterCache tests.\"\"\"\n\n  def _test_dir(self, test_name):\n    \"\"\"Create an empty dir to use for tests.\n\n    Args:\n      test_name: Name of the test.\n\n    Returns:\n      Absolute path to the test directory.\n    \"\"\"\n    test_dir = os.path.join(self.get_temp_dir(), test_name)\n    if os.path.isdir(test_dir):\n      for f in glob.glob(\"%s/*\" % test_dir):\n        os.remove(f)\n    else:\n      os.makedirs(test_dir)\n    return test_dir\n\n  def test_cache(self):\n    with ops.Graph().as_default():\n      dir1 = self._test_dir(\"test_cache_1\")\n      dir2 = self._test_dir(\"test_cache_2\")\n      sw1 = writer_cache.FileWriterCache.get(dir1)\n      sw2 = writer_cache.FileWriterCache.get(dir2)\n      sw3 = writer_cache.FileWriterCache.get(dir1)\n      self.assertEqual(sw1, sw3)\n      self.assertFalse(sw1 == sw2)\n      sw1.close()\n      sw2.close()\n      events1 = glob.glob(os.path.join(dir1, \"event*\"))\n      self.assertTrue(events1)\n      events2 = glob.glob(os.path.join(dir2, \"event*\"))\n      self.assertTrue(events2)\n      events3 = glob.glob(os.path.join(\"nowriter\", \"event*\"))\n      self.assertFalse(events3)\n\n  def test_clear(self):\n    with ops.Graph().as_default():\n      dir1 = self._test_dir(\"test_clear\")\n      sw1 = writer_cache.FileWriterCache.get(dir1)\n      writer_cache.FileWriterCache.clear()\n      sw2 = writer_cache.FileWriterCache.get(dir1)\n      self.assertFalse(sw1 == sw2)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/summary_kernels.cc", "tensorflow/python/summary/writer/writer_test.py"], "buggy_code_start_loc": [40, 36], "buggy_code_end_loc": [46, 687], "fixing_code_start_loc": [41, 37], "fixing_code_end_loc": [55, 699], "type": "CWE-617", "message": "TensorFlow is an open source platform for machine learning. In affected versions if `tf.summary.create_file_writer` is called with non-scalar arguments code crashes due to a `CHECK`-fail. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41200", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T20:15:08.037", "lastModified": "2021-11-09T17:40:42.607", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions if `tf.summary.create_file_writer` is called with non-scalar arguments code crashes due to a `CHECK`-fail. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas, si se llama a \"tf.summary.create_file_writer\" con argumentos no escalares, el c\u00f3digo es bloqueado debido a un fallo de \"CHECK\". La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n ser\u00e1 incluida este commit en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.4.4", "matchCriteriaId": "455FB550-4C9C-4BD6-9F76-A627B62AB332"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:*:*:*:*:*:*:*", "matchCriteriaId": "651EA851-E660-4E53-9F3E-B6B69D91326B"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/874bda09e6702cd50bac90b453b50bcc65b2769e", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/issues/46909", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-gh8h-7j2j-qv4f", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/874bda09e6702cd50bac90b453b50bcc65b2769e"}}