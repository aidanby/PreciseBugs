{"buggy_code": ["/*\n *  fs/userfaultfd.c\n *\n *  Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>\n *  Copyright (C) 2008-2009 Red Hat, Inc.\n *  Copyright (C) 2015  Red Hat, Inc.\n *\n *  This work is licensed under the terms of the GNU GPL, version 2. See\n *  the COPYING file in the top-level directory.\n *\n *  Some part derived from fs/eventfd.c (anon inode setup) and\n *  mm/ksm.c (mm hashing).\n */\n\n#include <linux/list.h>\n#include <linux/hashtable.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/file.h>\n#include <linux/bug.h>\n#include <linux/anon_inodes.h>\n#include <linux/syscalls.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mempolicy.h>\n#include <linux/ioctl.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n\nstatic struct kmem_cache *userfaultfd_ctx_cachep __read_mostly;\n\nenum userfaultfd_state {\n\tUFFD_STATE_WAIT_API,\n\tUFFD_STATE_RUNNING,\n};\n\n/*\n * Start with fault_pending_wqh and fault_wqh so they're more likely\n * to be in the same cacheline.\n */\nstruct userfaultfd_ctx {\n\t/* waitqueue head for the pending (i.e. not read) userfaults */\n\twait_queue_head_t fault_pending_wqh;\n\t/* waitqueue head for the userfaults */\n\twait_queue_head_t fault_wqh;\n\t/* waitqueue head for the pseudo fd to wakeup poll/read */\n\twait_queue_head_t fd_wqh;\n\t/* waitqueue head for events */\n\twait_queue_head_t event_wqh;\n\t/* a refile sequence protected by fault_pending_wqh lock */\n\tstruct seqcount refile_seq;\n\t/* pseudo fd refcounting */\n\tatomic_t refcount;\n\t/* userfaultfd syscall flags */\n\tunsigned int flags;\n\t/* features requested from the userspace */\n\tunsigned int features;\n\t/* state machine */\n\tenum userfaultfd_state state;\n\t/* released */\n\tbool released;\n\t/* memory mappings are changing because of non-cooperative event */\n\tbool mmap_changing;\n\t/* mm with one ore more vmas attached to this userfaultfd_ctx */\n\tstruct mm_struct *mm;\n};\n\nstruct userfaultfd_fork_ctx {\n\tstruct userfaultfd_ctx *orig;\n\tstruct userfaultfd_ctx *new;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_unmap_ctx {\n\tstruct userfaultfd_ctx *ctx;\n\tunsigned long start;\n\tunsigned long end;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_wait_queue {\n\tstruct uffd_msg msg;\n\twait_queue_entry_t wq;\n\tstruct userfaultfd_ctx *ctx;\n\tbool waken;\n};\n\nstruct userfaultfd_wake_range {\n\tunsigned long start;\n\tunsigned long len;\n};\n\nstatic int userfaultfd_wake_function(wait_queue_entry_t *wq, unsigned mode,\n\t\t\t\t     int wake_flags, void *key)\n{\n\tstruct userfaultfd_wake_range *range = key;\n\tint ret;\n\tstruct userfaultfd_wait_queue *uwq;\n\tunsigned long start, len;\n\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\n\tret = 0;\n\t/* len == 0 means wake all */\n\tstart = range->start;\n\tlen = range->len;\n\tif (len && (start > uwq->msg.arg.pagefault.address ||\n\t\t    start + len <= uwq->msg.arg.pagefault.address))\n\t\tgoto out;\n\tWRITE_ONCE(uwq->waken, true);\n\t/*\n\t * The Program-Order guarantees provided by the scheduler\n\t * ensure uwq->waken is visible before the task is woken.\n\t */\n\tret = wake_up_state(wq->private, mode);\n\tif (ret) {\n\t\t/*\n\t\t * Wake only once, autoremove behavior.\n\t\t *\n\t\t * After the effect of list_del_init is visible to the other\n\t\t * CPUs, the waitqueue may disappear from under us, see the\n\t\t * !list_empty_careful() in handle_userfault().\n\t\t *\n\t\t * try_to_wake_up() has an implicit smp_mb(), and the\n\t\t * wq->private is read before calling the extern function\n\t\t * \"wake_up_state\" (which in turns calls try_to_wake_up).\n\t\t */\n\t\tlist_del_init(&wq->entry);\n\t}\nout:\n\treturn ret;\n}\n\n/**\n * userfaultfd_ctx_get - Acquires a reference to the internal userfaultfd\n * context.\n * @ctx: [in] Pointer to the userfaultfd context.\n */\nstatic void userfaultfd_ctx_get(struct userfaultfd_ctx *ctx)\n{\n\tif (!atomic_inc_not_zero(&ctx->refcount))\n\t\tBUG();\n}\n\n/**\n * userfaultfd_ctx_put - Releases a reference to the internal userfaultfd\n * context.\n * @ctx: [in] Pointer to userfaultfd context.\n *\n * The userfaultfd context reference must have been previously acquired either\n * with userfaultfd_ctx_get() or userfaultfd_ctx_fdget().\n */\nstatic void userfaultfd_ctx_put(struct userfaultfd_ctx *ctx)\n{\n\tif (atomic_dec_and_test(&ctx->refcount)) {\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_pending_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_pending_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->event_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->event_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fd_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fd_wqh));\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n}\n\nstatic inline void msg_init(struct uffd_msg *msg)\n{\n\tBUILD_BUG_ON(sizeof(struct uffd_msg) != 32);\n\t/*\n\t * Must use memset to zero out the paddings or kernel data is\n\t * leaked to userland.\n\t */\n\tmemset(msg, 0, sizeof(struct uffd_msg));\n}\n\nstatic inline struct uffd_msg userfault_msg(unsigned long address,\n\t\t\t\t\t    unsigned int flags,\n\t\t\t\t\t    unsigned long reason,\n\t\t\t\t\t    unsigned int features)\n{\n\tstruct uffd_msg msg;\n\tmsg_init(&msg);\n\tmsg.event = UFFD_EVENT_PAGEFAULT;\n\tmsg.arg.pagefault.address = address;\n\tif (flags & FAULT_FLAG_WRITE)\n\t\t/*\n\t\t * If UFFD_FEATURE_PAGEFAULT_FLAG_WP was set in the\n\t\t * uffdio_api.features and UFFD_PAGEFAULT_FLAG_WRITE\n\t\t * was not set in a UFFD_EVENT_PAGEFAULT, it means it\n\t\t * was a read fault, otherwise if set it means it's\n\t\t * a write fault.\n\t\t */\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WRITE;\n\tif (reason & VM_UFFD_WP)\n\t\t/*\n\t\t * If UFFD_FEATURE_PAGEFAULT_FLAG_WP was set in the\n\t\t * uffdio_api.features and UFFD_PAGEFAULT_FLAG_WP was\n\t\t * not set in a UFFD_EVENT_PAGEFAULT, it means it was\n\t\t * a missing fault, otherwise if set it means it's a\n\t\t * write protect fault.\n\t\t */\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WP;\n\tif (features & UFFD_FEATURE_THREAD_ID)\n\t\tmsg.arg.pagefault.feat.ptid = task_pid_vnr(current);\n\treturn msg;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/*\n * Same functionality as userfaultfd_must_wait below with modifications for\n * hugepmd ranges.\n */\nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tpte_t *ptep, pte;\n\tbool ret = true;\n\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tptep = huge_pte_offset(mm, address, vma_mmu_pagesize(vma));\n\n\tif (!ptep)\n\t\tgoto out;\n\n\tret = false;\n\tpte = huge_ptep_get(ptep);\n\n\t/*\n\t * Lockless access: we're in a wait_event so it's ok if it\n\t * changes under us.\n\t */\n\tif (huge_pte_none(pte))\n\t\tret = true;\n\tif (!huge_pte_write(pte) && (reason & VM_UFFD_WP))\n\t\tret = true;\nout:\n\treturn ret;\n}\n#else\nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\treturn false;\t/* should never get here */\n}\n#endif /* CONFIG_HUGETLB_PAGE */\n\n/*\n * Verify the pagetables are still not ok after having reigstered into\n * the fault_pending_wqh to avoid userland having to UFFDIO_WAKE any\n * userfault that has already been resolved, if userfaultfd_read and\n * UFFDIO_COPY|ZEROPAGE are being run simultaneously on two different\n * threads.\n */\nstatic inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tbool ret = true;\n\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\tp4d = p4d_offset(pgd, address);\n\tif (!p4d_present(*p4d))\n\t\tgoto out;\n\tpud = pud_offset(p4d, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\tpmd = pmd_offset(pud, address);\n\t/*\n\t * READ_ONCE must function as a barrier with narrower scope\n\t * and it must be equivalent to:\n\t *\t_pmd = *pmd; barrier();\n\t *\n\t * This is to deal with the instability (as in\n\t * pmd_trans_unstable) of the pmd.\n\t */\n\t_pmd = READ_ONCE(*pmd);\n\tif (pmd_none(_pmd))\n\t\tgoto out;\n\n\tret = false;\n\tif (!pmd_present(_pmd))\n\t\tgoto out;\n\n\tif (pmd_trans_huge(_pmd))\n\t\tgoto out;\n\n\t/*\n\t * the pmd is stable (as in !pmd_trans_unstable) so we can re-read it\n\t * and use the standard pte_offset_map() instead of parsing _pmd.\n\t */\n\tpte = pte_offset_map(pmd, address);\n\t/*\n\t * Lockless access: we're in a wait_event so it's ok if it\n\t * changes under us.\n\t */\n\tif (pte_none(*pte))\n\t\tret = true;\n\tpte_unmap(pte);\n\nout:\n\treturn ret;\n}\n\n/*\n * The locking rules involved in returning VM_FAULT_RETRY depending on\n * FAULT_FLAG_ALLOW_RETRY, FAULT_FLAG_RETRY_NOWAIT and\n * FAULT_FLAG_KILLABLE are not straightforward. The \"Caution\"\n * recommendation in __lock_page_or_retry is not an understatement.\n *\n * If FAULT_FLAG_ALLOW_RETRY is set, the mmap_sem must be released\n * before returning VM_FAULT_RETRY only if FAULT_FLAG_RETRY_NOWAIT is\n * not set.\n *\n * If FAULT_FLAG_ALLOW_RETRY is set but FAULT_FLAG_KILLABLE is not\n * set, VM_FAULT_RETRY can still be returned if and only if there are\n * fatal_signal_pending()s, and the mmap_sem must be released before\n * returning it.\n */\nvm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)\n{\n\tstruct mm_struct *mm = vmf->vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue uwq;\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\tbool must_wait, return_to_userland;\n\tlong blocking_state;\n\n\t/*\n\t * We don't do userfault handling for the final child pid update.\n\t *\n\t * We also don't do userfault handling during\n\t * coredumping. hugetlbfs has the special\n\t * follow_hugetlb_page() to skip missing pages in the\n\t * FOLL_DUMP case, anon memory also checks for FOLL_DUMP with\n\t * the no_page_table() helper in follow_page_mask(), but the\n\t * shmem_vm_ops->fault method is invoked even during\n\t * coredumping without mmap_sem and it ends up here.\n\t */\n\tif (current->flags & (PF_EXITING|PF_DUMPCORE))\n\t\tgoto out;\n\n\t/*\n\t * Coredumping runs without mmap_sem so we can only check that\n\t * the mmap_sem is held, if PF_DUMPCORE was not set.\n\t */\n\tWARN_ON_ONCE(!rwsem_is_locked(&mm->mmap_sem));\n\n\tctx = vmf->vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx)\n\t\tgoto out;\n\n\tBUG_ON(ctx->mm != mm);\n\n\tVM_BUG_ON(reason & ~(VM_UFFD_MISSING|VM_UFFD_WP));\n\tVM_BUG_ON(!(reason & VM_UFFD_MISSING) ^ !!(reason & VM_UFFD_WP));\n\n\tif (ctx->features & UFFD_FEATURE_SIGBUS)\n\t\tgoto out;\n\n\t/*\n\t * If it's already released don't get it. This avoids to loop\n\t * in __get_user_pages if userfaultfd_release waits on the\n\t * caller of handle_userfault to release the mmap_sem.\n\t */\n\tif (unlikely(READ_ONCE(ctx->released))) {\n\t\t/*\n\t\t * Don't return VM_FAULT_SIGBUS in this case, so a non\n\t\t * cooperative manager can close the uffd after the\n\t\t * last UFFDIO_COPY, without risking to trigger an\n\t\t * involuntary SIGBUS if the process was starting the\n\t\t * userfaultfd while the userfaultfd was still armed\n\t\t * (but after the last UFFDIO_COPY). If the uffd\n\t\t * wasn't already closed when the userfault reached\n\t\t * this point, that would normally be solved by\n\t\t * userfaultfd_must_wait returning 'false'.\n\t\t *\n\t\t * If we were to return VM_FAULT_SIGBUS here, the non\n\t\t * cooperative manager would be instead forced to\n\t\t * always call UFFDIO_UNREGISTER before it can safely\n\t\t * close the uffd.\n\t\t */\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Check that we can return VM_FAULT_RETRY.\n\t *\n\t * NOTE: it should become possible to return VM_FAULT_RETRY\n\t * even if FAULT_FLAG_TRIED is set without leading to gup()\n\t * -EBUSY failures, if the userfaultfd is to be extended for\n\t * VM_UFFD_WP tracking and we intend to arm the userfault\n\t * without first stopping userland access to the memory. For\n\t * VM_UFFD_MISSING userfaults this is enough for now.\n\t */\n\tif (unlikely(!(vmf->flags & FAULT_FLAG_ALLOW_RETRY))) {\n\t\t/*\n\t\t * Validate the invariant that nowait must allow retry\n\t\t * to be sure not to return SIGBUS erroneously on\n\t\t * nowait invocations.\n\t\t */\n\t\tBUG_ON(vmf->flags & FAULT_FLAG_RETRY_NOWAIT);\n#ifdef CONFIG_DEBUG_VM\n\t\tif (printk_ratelimit()) {\n\t\t\tprintk(KERN_WARNING\n\t\t\t       \"FAULT_FLAG_ALLOW_RETRY missing %x\\n\",\n\t\t\t       vmf->flags);\n\t\t\tdump_stack();\n\t\t}\n#endif\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Handle nowait, not much to do other than tell it to retry\n\t * and wait.\n\t */\n\tret = VM_FAULT_RETRY;\n\tif (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)\n\t\tgoto out;\n\n\t/* take the reference before dropping the mmap_sem */\n\tuserfaultfd_ctx_get(ctx);\n\n\tinit_waitqueue_func_entry(&uwq.wq, userfaultfd_wake_function);\n\tuwq.wq.private = current;\n\tuwq.msg = userfault_msg(vmf->address, vmf->flags, reason,\n\t\t\tctx->features);\n\tuwq.ctx = ctx;\n\tuwq.waken = false;\n\n\treturn_to_userland =\n\t\t(vmf->flags & (FAULT_FLAG_USER|FAULT_FLAG_KILLABLE)) ==\n\t\t(FAULT_FLAG_USER|FAULT_FLAG_KILLABLE);\n\tblocking_state = return_to_userland ? TASK_INTERRUPTIBLE :\n\t\t\t TASK_KILLABLE;\n\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->fault_pending_wqh, &uwq.wq);\n\t/*\n\t * The smp_mb() after __set_current_state prevents the reads\n\t * following the spin_unlock to happen before the list_add in\n\t * __add_wait_queue.\n\t */\n\tset_current_state(blocking_state);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\tif (!is_vm_hugetlb_page(vmf->vma))\n\t\tmust_wait = userfaultfd_must_wait(ctx, vmf->address, vmf->flags,\n\t\t\t\t\t\t  reason);\n\telse\n\t\tmust_wait = userfaultfd_huge_must_wait(ctx, vmf->vma,\n\t\t\t\t\t\t       vmf->address,\n\t\t\t\t\t\t       vmf->flags, reason);\n\tup_read(&mm->mmap_sem);\n\n\tif (likely(must_wait && !READ_ONCE(ctx->released) &&\n\t\t   (return_to_userland ? !signal_pending(current) :\n\t\t    !fatal_signal_pending(current)))) {\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\t\tret |= VM_FAULT_MAJOR;\n\n\t\t/*\n\t\t * False wakeups can orginate even from rwsem before\n\t\t * up_read() however userfaults will wait either for a\n\t\t * targeted wakeup on the specific uwq waitqueue from\n\t\t * wake_userfault() or for signals or for uffd\n\t\t * release.\n\t\t */\n\t\twhile (!READ_ONCE(uwq.waken)) {\n\t\t\t/*\n\t\t\t * This needs the full smp_store_mb()\n\t\t\t * guarantee as the state write must be\n\t\t\t * visible to other CPUs before reading\n\t\t\t * uwq.waken from other CPUs.\n\t\t\t */\n\t\t\tset_current_state(blocking_state);\n\t\t\tif (READ_ONCE(uwq.waken) ||\n\t\t\t    READ_ONCE(ctx->released) ||\n\t\t\t    (return_to_userland ? signal_pending(current) :\n\t\t\t     fatal_signal_pending(current)))\n\t\t\t\tbreak;\n\t\t\tschedule();\n\t\t}\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\n\tif (return_to_userland) {\n\t\tif (signal_pending(current) &&\n\t\t    !fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * If we got a SIGSTOP or SIGCONT and this is\n\t\t\t * a normal userland page fault, just let\n\t\t\t * userland return so the signal will be\n\t\t\t * handled and gdb debugging works.  The page\n\t\t\t * fault code immediately after we return from\n\t\t\t * this function is going to release the\n\t\t\t * mmap_sem and it's not depending on it\n\t\t\t * (unlike gup would if we were not to return\n\t\t\t * VM_FAULT_RETRY).\n\t\t\t *\n\t\t\t * If a fatal signal is pending we still take\n\t\t\t * the streamlined VM_FAULT_RETRY failure path\n\t\t\t * and there's no need to retake the mmap_sem\n\t\t\t * in such case.\n\t\t\t */\n\t\t\tdown_read(&mm->mmap_sem);\n\t\t\tret = VM_FAULT_NOPAGE;\n\t\t}\n\t}\n\n\t/*\n\t * Here we race with the list_del; list_add in\n\t * userfaultfd_ctx_read(), however because we don't ever run\n\t * list_del_init() to refile across the two lists, the prev\n\t * and next pointers will never point to self. list_add also\n\t * would never let any of the two pointers to point to\n\t * self. So list_empty_careful won't risk to see both pointers\n\t * pointing to self at any time during the list refile. The\n\t * only case where list_del_init() is called is the full\n\t * removal in the wake function and there we don't re-list_add\n\t * and it's fine not to block on the spinlock. The uwq on this\n\t * kernel stack can be released after the list_del_init.\n\t */\n\tif (!list_empty_careful(&uwq.wq.entry)) {\n\t\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t\t/*\n\t\t * No need of list_del_init(), the uwq on the stack\n\t\t * will be freed shortly anyway.\n\t\t */\n\t\tlist_del(&uwq.wq.entry);\n\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\n\tuserfaultfd_ctx_put(ctx);\n\nout:\n\treturn ret;\n}\n\nstatic void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t      struct userfaultfd_wait_queue *ewq)\n{\n\tstruct userfaultfd_ctx *release_new_ctx;\n\n\tif (WARN_ON_ONCE(current->flags & PF_EXITING))\n\t\tgoto out;\n\n\tewq->ctx = ctx;\n\tinit_waitqueue_entry(&ewq->wq, current);\n\trelease_new_ctx = NULL;\n\n\tspin_lock(&ctx->event_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->event_wqh, &ewq->wq);\n\tfor (;;) {\n\t\tset_current_state(TASK_KILLABLE);\n\t\tif (ewq->msg.event == 0)\n\t\t\tbreak;\n\t\tif (READ_ONCE(ctx->released) ||\n\t\t    fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * &ewq->wq may be queued in fork_event, but\n\t\t\t * __remove_wait_queue ignores the head\n\t\t\t * parameter. It would be a problem if it\n\t\t\t * didn't.\n\t\t\t */\n\t\t\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n\t\t\tif (ewq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tstruct userfaultfd_ctx *new;\n\n\t\t\t\tnew = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tewq->msg.arg.reserved.reserved1;\n\t\t\t\trelease_new_ctx = new;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock(&ctx->event_wqh.lock);\n\n\tif (release_new_ctx) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct mm_struct *mm = release_new_ctx->mm;\n\n\t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n\t\tdown_write(&mm->mmap_sem);\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n\t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n\t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\t\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\t\t}\n\t\tup_write(&mm->mmap_sem);\n\n\t\tuserfaultfd_ctx_put(release_new_ctx);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\nout:\n\tWRITE_ONCE(ctx->mmap_changing, false);\n\tuserfaultfd_ctx_put(ctx);\n}\n\nstatic void userfaultfd_event_complete(struct userfaultfd_ctx *ctx,\n\t\t\t\t       struct userfaultfd_wait_queue *ewq)\n{\n\tewq->msg.event = 0;\n\twake_up_locked(&ctx->event_wqh);\n\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n}\n\nint dup_userfaultfd(struct vm_area_struct *vma, struct list_head *fcs)\n{\n\tstruct userfaultfd_ctx *ctx = NULL, *octx;\n\tstruct userfaultfd_fork_ctx *fctx;\n\n\toctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!octx || !(octx->features & UFFD_FEATURE_EVENT_FORK)) {\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(fctx, fcs, list)\n\t\tif (fctx->orig == octx) {\n\t\t\tctx = fctx->new;\n\t\t\tbreak;\n\t\t}\n\n\tif (!ctx) {\n\t\tfctx = kmalloc(sizeof(*fctx), GFP_KERNEL);\n\t\tif (!fctx)\n\t\t\treturn -ENOMEM;\n\n\t\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\t\tif (!ctx) {\n\t\t\tkfree(fctx);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tatomic_set(&ctx->refcount, 1);\n\t\tctx->flags = octx->flags;\n\t\tctx->state = UFFD_STATE_RUNNING;\n\t\tctx->features = octx->features;\n\t\tctx->released = false;\n\t\tctx->mmap_changing = false;\n\t\tctx->mm = vma->vm_mm;\n\t\tmmgrab(ctx->mm);\n\n\t\tuserfaultfd_ctx_get(octx);\n\t\tWRITE_ONCE(octx->mmap_changing, true);\n\t\tfctx->orig = octx;\n\t\tfctx->new = ctx;\n\t\tlist_add_tail(&fctx->list, fcs);\n\t}\n\n\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\treturn 0;\n}\n\nstatic void dup_fctx(struct userfaultfd_fork_ctx *fctx)\n{\n\tstruct userfaultfd_ctx *ctx = fctx->orig;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_FORK;\n\tewq.msg.arg.reserved.reserved1 = (unsigned long)fctx->new;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nvoid dup_userfaultfd_complete(struct list_head *fcs)\n{\n\tstruct userfaultfd_fork_ctx *fctx, *n;\n\n\tlist_for_each_entry_safe(fctx, n, fcs, list) {\n\t\tdup_fctx(fctx);\n\t\tlist_del(&fctx->list);\n\t\tkfree(fctx);\n\t}\n}\n\nvoid mremap_userfaultfd_prep(struct vm_area_struct *vma,\n\t\t\t     struct vm_userfaultfd_ctx *vm_ctx)\n{\n\tstruct userfaultfd_ctx *ctx;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (ctx && (ctx->features & UFFD_FEATURE_EVENT_REMAP)) {\n\t\tvm_ctx->ctx = ctx;\n\t\tuserfaultfd_ctx_get(ctx);\n\t\tWRITE_ONCE(ctx->mmap_changing, true);\n\t}\n}\n\nvoid mremap_userfaultfd_complete(struct vm_userfaultfd_ctx *vm_ctx,\n\t\t\t\t unsigned long from, unsigned long to,\n\t\t\t\t unsigned long len)\n{\n\tstruct userfaultfd_ctx *ctx = vm_ctx->ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tif (!ctx)\n\t\treturn;\n\n\tif (to & ~PAGE_MASK) {\n\t\tuserfaultfd_ctx_put(ctx);\n\t\treturn;\n\t}\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMAP;\n\tewq.msg.arg.remap.from = from;\n\tewq.msg.arg.remap.to = to;\n\tewq.msg.arg.remap.len = len;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nbool userfaultfd_remove(struct vm_area_struct *vma,\n\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_REMOVE))\n\t\treturn true;\n\n\tuserfaultfd_ctx_get(ctx);\n\tWRITE_ONCE(ctx->mmap_changing, true);\n\tup_read(&mm->mmap_sem);\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMOVE;\n\tewq.msg.arg.remove.start = start;\n\tewq.msg.arg.remove.end = end;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n\n\treturn false;\n}\n\nstatic bool has_unmap_ctx(struct userfaultfd_ctx *ctx, struct list_head *unmaps,\n\t\t\t  unsigned long start, unsigned long end)\n{\n\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\n\tlist_for_each_entry(unmap_ctx, unmaps, list)\n\t\tif (unmap_ctx->ctx == ctx && unmap_ctx->start == start &&\n\t\t    unmap_ctx->end == end)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nint userfaultfd_unmap_prep(struct vm_area_struct *vma,\n\t\t\t   unsigned long start, unsigned long end,\n\t\t\t   struct list_head *unmaps)\n{\n\tfor ( ; vma && vma->vm_start < end; vma = vma->vm_next) {\n\t\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\t\tstruct userfaultfd_ctx *ctx = vma->vm_userfaultfd_ctx.ctx;\n\n\t\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_UNMAP) ||\n\t\t    has_unmap_ctx(ctx, unmaps, start, end))\n\t\t\tcontinue;\n\n\t\tunmap_ctx = kzalloc(sizeof(*unmap_ctx), GFP_KERNEL);\n\t\tif (!unmap_ctx)\n\t\t\treturn -ENOMEM;\n\n\t\tuserfaultfd_ctx_get(ctx);\n\t\tWRITE_ONCE(ctx->mmap_changing, true);\n\t\tunmap_ctx->ctx = ctx;\n\t\tunmap_ctx->start = start;\n\t\tunmap_ctx->end = end;\n\t\tlist_add_tail(&unmap_ctx->list, unmaps);\n\t}\n\n\treturn 0;\n}\n\nvoid userfaultfd_unmap_complete(struct mm_struct *mm, struct list_head *uf)\n{\n\tstruct userfaultfd_unmap_ctx *ctx, *n;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tlist_for_each_entry_safe(ctx, n, uf, list) {\n\t\tmsg_init(&ewq.msg);\n\n\t\tewq.msg.event = UFFD_EVENT_UNMAP;\n\t\tewq.msg.arg.remove.start = ctx->start;\n\t\tewq.msg.arg.remove.end = ctx->end;\n\n\t\tuserfaultfd_event_wait_completion(ctx->ctx, &ewq);\n\n\t\tlist_del(&ctx->list);\n\t\tkfree(ctx);\n\t}\n}\n\nstatic int userfaultfd_release(struct inode *inode, struct file *file)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev;\n\t/* len == 0 means wake all */\n\tstruct userfaultfd_wake_range range = { .len = 0, };\n\tunsigned long new_flags;\n\n\tWRITE_ONCE(ctx->released, true);\n\n\tif (!mmget_not_zero(mm))\n\t\tgoto wakeup;\n\n\t/*\n\t * Flush page faults out of all CPUs. NOTE: all page faults\n\t * must be retried without returning VM_FAULT_SIGBUS if\n\t * userfaultfd_ctx_get() succeeds but vma->vma_userfault_ctx\n\t * changes while handle_userfault released the mmap_sem. So\n\t * it's critical that released is set to true (above), before\n\t * taking the mmap_sem for writing.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tprev = NULL;\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tcond_resched();\n\t\tBUG_ON(!!vma->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(vma->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\t\tif (vma->vm_userfaultfd_ctx.ctx != ctx) {\n\t\t\tprev = vma;\n\t\t\tcontinue;\n\t\t}\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, vma->vm_start, vma->vm_end,\n\t\t\t\t new_flags, vma->anon_vma,\n\t\t\t\t vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev)\n\t\t\tvma = prev;\n\t\telse\n\t\t\tprev = vma;\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t}\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nwakeup:\n\t/*\n\t * After no new page faults can wait on this fault_*wqh, flush\n\t * the last page faults that may have been already waiting on\n\t * the fault_*wqh.\n\t */\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL, &range);\n\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, &range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/* Flush pending events that may still wait on event_wqh */\n\twake_up_all(&ctx->event_wqh);\n\n\twake_up_poll(&ctx->fd_wqh, EPOLLHUP);\n\tuserfaultfd_ctx_put(ctx);\n\treturn 0;\n}\n\n/* fault_pending_wqh.lock must be hold by the caller */\nstatic inline struct userfaultfd_wait_queue *find_userfault_in(\n\t\twait_queue_head_t *wqh)\n{\n\twait_queue_entry_t *wq;\n\tstruct userfaultfd_wait_queue *uwq;\n\n\tVM_BUG_ON(!spin_is_locked(&wqh->lock));\n\n\tuwq = NULL;\n\tif (!waitqueue_active(wqh))\n\t\tgoto out;\n\t/* walk in reverse to provide FIFO behavior to read userfaults */\n\twq = list_last_entry(&wqh->head, typeof(*wq), entry);\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\nout:\n\treturn uwq;\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->fault_pending_wqh);\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault_evt(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->event_wqh);\n}\n\nstatic __poll_t userfaultfd_poll(struct file *file, poll_table *wait)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\t__poll_t ret;\n\n\tpoll_wait(file, &ctx->fd_wqh, wait);\n\n\tswitch (ctx->state) {\n\tcase UFFD_STATE_WAIT_API:\n\t\treturn EPOLLERR;\n\tcase UFFD_STATE_RUNNING:\n\t\t/*\n\t\t * poll() never guarantees that read won't block.\n\t\t * userfaults can be waken before they're read().\n\t\t */\n\t\tif (unlikely(!(file->f_flags & O_NONBLOCK)))\n\t\t\treturn EPOLLERR;\n\t\t/*\n\t\t * lockless access to see if there are pending faults\n\t\t * __pollwait last action is the add_wait_queue but\n\t\t * the spin_unlock would allow the waitqueue_active to\n\t\t * pass above the actual list_add inside\n\t\t * add_wait_queue critical section. So use a full\n\t\t * memory barrier to serialize the list_add write of\n\t\t * add_wait_queue() with the waitqueue_active read\n\t\t * below.\n\t\t */\n\t\tret = 0;\n\t\tsmp_mb();\n\t\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\t\tret = EPOLLIN;\n\t\telse if (waitqueue_active(&ctx->event_wqh))\n\t\t\tret = EPOLLIN;\n\n\t\treturn ret;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn EPOLLERR;\n\t}\n}\n\nstatic const struct file_operations userfaultfd_fops;\n\nstatic int resolve_userfault_fork(struct userfaultfd_ctx *ctx,\n\t\t\t\t  struct userfaultfd_ctx *new,\n\t\t\t\t  struct uffd_msg *msg)\n{\n\tint fd;\n\n\tfd = anon_inode_getfd(\"[userfaultfd]\", &userfaultfd_fops, new,\n\t\t\t      O_RDWR | (new->flags & UFFD_SHARED_FCNTL_FLAGS));\n\tif (fd < 0)\n\t\treturn fd;\n\n\tmsg->arg.reserved.reserved1 = 0;\n\tmsg->arg.fork.ufd = fd;\n\treturn 0;\n}\n\nstatic ssize_t userfaultfd_ctx_read(struct userfaultfd_ctx *ctx, int no_wait,\n\t\t\t\t    struct uffd_msg *msg)\n{\n\tssize_t ret;\n\tDECLARE_WAITQUEUE(wait, current);\n\tstruct userfaultfd_wait_queue *uwq;\n\t/*\n\t * Handling fork event requires sleeping operations, so\n\t * we drop the event_wqh lock, then do these ops, then\n\t * lock it back and wake up the waiter. While the lock is\n\t * dropped the ewq may go away so we keep track of it\n\t * carefully.\n\t */\n\tLIST_HEAD(fork_event);\n\tstruct userfaultfd_ctx *fork_nctx = NULL;\n\n\t/* always take the fd_wqh lock before the fault_pending_wqh lock */\n\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t__add_wait_queue(&ctx->fd_wqh, &wait);\n\tfor (;;) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t\tuwq = find_userfault(ctx);\n\t\tif (uwq) {\n\t\t\t/*\n\t\t\t * Use a seqcount to repeat the lockless check\n\t\t\t * in wake_userfault() to avoid missing\n\t\t\t * wakeups because during the refile both\n\t\t\t * waitqueue could become empty if this is the\n\t\t\t * only userfault.\n\t\t\t */\n\t\t\twrite_seqcount_begin(&ctx->refile_seq);\n\n\t\t\t/*\n\t\t\t * The fault_pending_wqh.lock prevents the uwq\n\t\t\t * to disappear from under us.\n\t\t\t *\n\t\t\t * Refile this userfault from\n\t\t\t * fault_pending_wqh to fault_wqh, it's not\n\t\t\t * pending anymore after we read it.\n\t\t\t *\n\t\t\t * Use list_del() by hand (as\n\t\t\t * userfaultfd_wake_function also uses\n\t\t\t * list_del_init() by hand) to be sure nobody\n\t\t\t * changes __remove_wait_queue() to use\n\t\t\t * list_del_init() in turn breaking the\n\t\t\t * !list_empty_careful() check in\n\t\t\t * handle_userfault(). The uwq->wq.head list\n\t\t\t * must never be empty at any time during the\n\t\t\t * refile, or the waitqueue could disappear\n\t\t\t * from under us. The \"wait_queue_head_t\"\n\t\t\t * parameter of __remove_wait_queue() is unused\n\t\t\t * anyway.\n\t\t\t */\n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\tadd_wait_queue(&ctx->fault_wqh, &uwq->wq);\n\n\t\t\twrite_seqcount_end(&ctx->refile_seq);\n\n\t\t\t/* careful to always initialize msg if ret == 0 */\n\t\t\t*msg = uwq->msg;\n\t\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t\tuwq = find_userfault_evt(ctx);\n\t\tif (uwq) {\n\t\t\t*msg = uwq->msg;\n\n\t\t\tif (uwq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tfork_nctx = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tuwq->msg.arg.reserved.reserved1;\n\t\t\t\tlist_move(&uwq->wq.entry, &fork_event);\n\t\t\t\t/*\n\t\t\t\t * fork_nctx can be freed as soon as\n\t\t\t\t * we drop the lock, unless we take a\n\t\t\t\t * reference on it.\n\t\t\t\t */\n\t\t\t\tuserfaultfd_ctx_get(fork_nctx);\n\t\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (no_wait) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\t\tschedule();\n\t\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t}\n\t__remove_wait_queue(&ctx->fd_wqh, &wait);\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\n\tif (!ret && msg->event == UFFD_EVENT_FORK) {\n\t\tret = resolve_userfault_fork(ctx, fork_nctx, msg);\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t\tif (!list_empty(&fork_event)) {\n\t\t\t/*\n\t\t\t * The fork thread didn't abort, so we can\n\t\t\t * drop the temporary refcount.\n\t\t\t */\n\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\n\t\t\tuwq = list_first_entry(&fork_event,\n\t\t\t\t\t       typeof(*uwq),\n\t\t\t\t\t       wq.entry);\n\t\t\t/*\n\t\t\t * If fork_event list wasn't empty and in turn\n\t\t\t * the event wasn't already released by fork\n\t\t\t * (the event is allocated on fork kernel\n\t\t\t * stack), put the event back to its place in\n\t\t\t * the event_wq. fork_event head will be freed\n\t\t\t * as soon as we return so the event cannot\n\t\t\t * stay queued there no matter the current\n\t\t\t * \"ret\" value.\n\t\t\t */\n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\t__add_wait_queue(&ctx->event_wqh, &uwq->wq);\n\n\t\t\t/*\n\t\t\t * Leave the event in the waitqueue and report\n\t\t\t * error to userland if we failed to resolve\n\t\t\t * the userfault fork.\n\t\t\t */\n\t\t\tif (likely(!ret))\n\t\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t} else {\n\t\t\t/*\n\t\t\t * Here the fork thread aborted and the\n\t\t\t * refcount from the fork thread on fork_nctx\n\t\t\t * has already been released. We still hold\n\t\t\t * the reference we took before releasing the\n\t\t\t * lock above. If resolve_userfault_fork\n\t\t\t * failed we've to drop it because the\n\t\t\t * fork_nctx has to be freed in such case. If\n\t\t\t * it succeeded we'll hold it because the new\n\t\t\t * uffd references it.\n\t\t\t */\n\t\t\tif (ret)\n\t\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\t\t}\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t userfaultfd_read(struct file *file, char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tssize_t _ret, ret = 0;\n\tstruct uffd_msg msg;\n\tint no_wait = file->f_flags & O_NONBLOCK;\n\n\tif (ctx->state == UFFD_STATE_WAIT_API)\n\t\treturn -EINVAL;\n\n\tfor (;;) {\n\t\tif (count < sizeof(msg))\n\t\t\treturn ret ? ret : -EINVAL;\n\t\t_ret = userfaultfd_ctx_read(ctx, no_wait, &msg);\n\t\tif (_ret < 0)\n\t\t\treturn ret ? ret : _ret;\n\t\tif (copy_to_user((__u64 __user *) buf, &msg, sizeof(msg)))\n\t\t\treturn ret ? ret : -EFAULT;\n\t\tret += sizeof(msg);\n\t\tbuf += sizeof(msg);\n\t\tcount -= sizeof(msg);\n\t\t/*\n\t\t * Allow to read more than one fault at time but only\n\t\t * block if waiting for the very first one.\n\t\t */\n\t\tno_wait = O_NONBLOCK;\n\t}\n}\n\nstatic void __wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t     struct userfaultfd_wake_range *range)\n{\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t/* wake all in the range and autoremove */\n\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL,\n\t\t\t\t     range);\n\tif (waitqueue_active(&ctx->fault_wqh))\n\t\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n}\n\nstatic __always_inline void wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t   struct userfaultfd_wake_range *range)\n{\n\tunsigned seq;\n\tbool need_wakeup;\n\n\t/*\n\t * To be sure waitqueue_active() is not reordered by the CPU\n\t * before the pagetable update, use an explicit SMP memory\n\t * barrier here. PT lock release or up_read(mmap_sem) still\n\t * have release semantics that can allow the\n\t * waitqueue_active() to be reordered before the pte update.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * Use waitqueue_active because it's very frequent to\n\t * change the address space atomically even if there are no\n\t * userfaults yet. So we take the spinlock only when we're\n\t * sure we've userfaults to wake.\n\t */\n\tdo {\n\t\tseq = read_seqcount_begin(&ctx->refile_seq);\n\t\tneed_wakeup = waitqueue_active(&ctx->fault_pending_wqh) ||\n\t\t\twaitqueue_active(&ctx->fault_wqh);\n\t\tcond_resched();\n\t} while (read_seqcount_retry(&ctx->refile_seq, seq));\n\tif (need_wakeup)\n\t\t__wake_userfault(ctx, range);\n}\n\nstatic __always_inline int validate_range(struct mm_struct *mm,\n\t\t\t\t\t  __u64 start, __u64 len)\n{\n\t__u64 task_size = mm->task_size;\n\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (len & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (!len)\n\t\treturn -EINVAL;\n\tif (start < mmap_min_addr)\n\t\treturn -EINVAL;\n\tif (start >= task_size)\n\t\treturn -EINVAL;\n\tif (len > task_size - start)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic inline bool vma_can_userfault(struct vm_area_struct *vma)\n{\n\treturn vma_is_anonymous(vma) || is_vm_hugetlb_page(vma) ||\n\t\tvma_is_shmem(vma);\n}\n\nstatic int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}\n\n/*\n * userfaultfd_wake may be used in combination with the\n * UFFDIO_*_MODE_DONTWAKE to wakeup userfaults in batches.\n */\nstatic int userfaultfd_wake(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\tint ret;\n\tstruct uffdio_range uffdio_wake;\n\tstruct userfaultfd_wake_range range;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_wake, buf, sizeof(uffdio_wake)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_wake.start, uffdio_wake.len);\n\tif (ret)\n\t\tgoto out;\n\n\trange.start = uffdio_wake.start;\n\trange.len = uffdio_wake.len;\n\n\t/*\n\t * len == 0 means wake all and we don't want to wake all here,\n\t * so check it again to be sure.\n\t */\n\tVM_BUG_ON(!range.len);\n\n\twake_userfault(ctx, &range);\n\tret = 0;\n\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_copy(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_copy uffdio_copy;\n\tstruct uffdio_copy __user *user_uffdio_copy;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_copy = (struct uffdio_copy __user *) arg;\n\n\tret = -EAGAIN;\n\tif (READ_ONCE(ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_copy, user_uffdio_copy,\n\t\t\t   /* don't copy \"copy\" last field */\n\t\t\t   sizeof(uffdio_copy)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_copy.dst, uffdio_copy.len);\n\tif (ret)\n\t\tgoto out;\n\t/*\n\t * double check for wraparound just in case. copy_from_user()\n\t * will later check uffdio_copy.src + uffdio_copy.len to fit\n\t * in the userland range.\n\t */\n\tret = -EINVAL;\n\tif (uffdio_copy.src + uffdio_copy.len <= uffdio_copy.src)\n\t\tgoto out;\n\tif (uffdio_copy.mode & ~UFFDIO_COPY_MODE_DONTWAKE)\n\t\tgoto out;\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mcopy_atomic(ctx->mm, uffdio_copy.dst, uffdio_copy.src,\n\t\t\t\t   uffdio_copy.len, &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_copy->copy)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\tBUG_ON(!ret);\n\t/* len == 0 would wake all */\n\trange.len = ret;\n\tif (!(uffdio_copy.mode & UFFDIO_COPY_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_copy.dst;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_copy.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_zeropage(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_zeropage uffdio_zeropage;\n\tstruct uffdio_zeropage __user *user_uffdio_zeropage;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_zeropage = (struct uffdio_zeropage __user *) arg;\n\n\tret = -EAGAIN;\n\tif (READ_ONCE(ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_zeropage, user_uffdio_zeropage,\n\t\t\t   /* don't copy \"zeropage\" last field */\n\t\t\t   sizeof(uffdio_zeropage)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t     uffdio_zeropage.range.len);\n\tif (ret)\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (uffdio_zeropage.mode & ~UFFDIO_ZEROPAGE_MODE_DONTWAKE)\n\t\tgoto out;\n\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mfill_zeropage(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t\t     uffdio_zeropage.range.len,\n\t\t\t\t     &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_zeropage->zeropage)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\t/* len == 0 would wake all */\n\tBUG_ON(!ret);\n\trange.len = ret;\n\tif (!(uffdio_zeropage.mode & UFFDIO_ZEROPAGE_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_zeropage.range.start;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_zeropage.range.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic inline unsigned int uffd_ctx_features(__u64 user_features)\n{\n\t/*\n\t * For the current set of features the bits just coincide\n\t */\n\treturn (unsigned int)user_features;\n}\n\n/*\n * userland asks for a certain API version and we return which bits\n * and ioctl commands are implemented in this kernel for such API\n * version or -EINVAL if unknown.\n */\nstatic int userfaultfd_api(struct userfaultfd_ctx *ctx,\n\t\t\t   unsigned long arg)\n{\n\tstruct uffdio_api uffdio_api;\n\tvoid __user *buf = (void __user *)arg;\n\tint ret;\n\t__u64 features;\n\n\tret = -EINVAL;\n\tif (ctx->state != UFFD_STATE_WAIT_API)\n\t\tgoto out;\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_api, buf, sizeof(uffdio_api)))\n\t\tgoto out;\n\tfeatures = uffdio_api.features;\n\tif (uffdio_api.api != UFFD_API || (features & ~UFFD_API_FEATURES)) {\n\t\tmemset(&uffdio_api, 0, sizeof(uffdio_api));\n\t\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\t\tgoto out;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\t/* report all available features and ioctls to userland */\n\tuffdio_api.features = UFFD_API_FEATURES;\n\tuffdio_api.ioctls = UFFD_API_IOCTLS;\n\tret = -EFAULT;\n\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\tgoto out;\n\tctx->state = UFFD_STATE_RUNNING;\n\t/* only enable the requested features for this uffd context */\n\tctx->features = uffd_ctx_features(features);\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic long userfaultfd_ioctl(struct file *file, unsigned cmd,\n\t\t\t      unsigned long arg)\n{\n\tint ret = -EINVAL;\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\n\tif (cmd != UFFDIO_API && ctx->state == UFFD_STATE_WAIT_API)\n\t\treturn -EINVAL;\n\n\tswitch(cmd) {\n\tcase UFFDIO_API:\n\t\tret = userfaultfd_api(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_REGISTER:\n\t\tret = userfaultfd_register(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_UNREGISTER:\n\t\tret = userfaultfd_unregister(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_WAKE:\n\t\tret = userfaultfd_wake(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_COPY:\n\t\tret = userfaultfd_copy(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_ZEROPAGE:\n\t\tret = userfaultfd_zeropage(ctx, arg);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic void userfaultfd_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct userfaultfd_ctx *ctx = f->private_data;\n\twait_queue_entry_t *wq;\n\tunsigned long pending = 0, total = 0;\n\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\tlist_for_each_entry(wq, &ctx->fault_pending_wqh.head, entry) {\n\t\tpending++;\n\t\ttotal++;\n\t}\n\tlist_for_each_entry(wq, &ctx->fault_wqh.head, entry) {\n\t\ttotal++;\n\t}\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/*\n\t * If more protocols will be added, there will be all shown\n\t * separated by a space. Like this:\n\t *\tprotocols: aa:... bb:...\n\t */\n\tseq_printf(m, \"pending:\\t%lu\\ntotal:\\t%lu\\nAPI:\\t%Lx:%x:%Lx\\n\",\n\t\t   pending, total, UFFD_API, ctx->features,\n\t\t   UFFD_API_IOCTLS|UFFD_API_RANGE_IOCTLS);\n}\n#endif\n\nstatic const struct file_operations userfaultfd_fops = {\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= userfaultfd_show_fdinfo,\n#endif\n\t.release\t= userfaultfd_release,\n\t.poll\t\t= userfaultfd_poll,\n\t.read\t\t= userfaultfd_read,\n\t.unlocked_ioctl = userfaultfd_ioctl,\n\t.compat_ioctl\t= userfaultfd_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic void init_once_userfaultfd_ctx(void *mem)\n{\n\tstruct userfaultfd_ctx *ctx = (struct userfaultfd_ctx *) mem;\n\n\tinit_waitqueue_head(&ctx->fault_pending_wqh);\n\tinit_waitqueue_head(&ctx->fault_wqh);\n\tinit_waitqueue_head(&ctx->event_wqh);\n\tinit_waitqueue_head(&ctx->fd_wqh);\n\tseqcount_init(&ctx->refile_seq);\n}\n\nSYSCALL_DEFINE1(userfaultfd, int, flags)\n{\n\tstruct userfaultfd_ctx *ctx;\n\tint fd;\n\n\tBUG_ON(!current->mm);\n\n\t/* Check the UFFD_* constants for consistency.  */\n\tBUILD_BUG_ON(UFFD_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(UFFD_NONBLOCK != O_NONBLOCK);\n\n\tif (flags & ~UFFD_SHARED_FCNTL_FLAGS)\n\t\treturn -EINVAL;\n\n\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tatomic_set(&ctx->refcount, 1);\n\tctx->flags = flags;\n\tctx->features = 0;\n\tctx->state = UFFD_STATE_WAIT_API;\n\tctx->released = false;\n\tctx->mmap_changing = false;\n\tctx->mm = current->mm;\n\t/* prevent the mm struct to be freed */\n\tmmgrab(ctx->mm);\n\n\tfd = anon_inode_getfd(\"[userfaultfd]\", &userfaultfd_fops, ctx,\n\t\t\t      O_RDWR | (flags & UFFD_SHARED_FCNTL_FLAGS));\n\tif (fd < 0) {\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n\treturn fd;\n}\n\nstatic int __init userfaultfd_init(void)\n{\n\tuserfaultfd_ctx_cachep = kmem_cache_create(\"userfaultfd_ctx_cache\",\n\t\t\t\t\t\tsizeof(struct userfaultfd_ctx),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tinit_once_userfaultfd_ctx);\n\treturn 0;\n}\n__initcall(userfaultfd_init);\n", "/*\n *  mm/userfaultfd.c\n *\n *  Copyright (C) 2015  Red Hat, Inc.\n *\n *  This work is licensed under the terms of the GNU GPL, version 2. See\n *  the COPYING file in the top-level directory.\n */\n\n#include <linux/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mmu_notifier.h>\n#include <linux/hugetlb.h>\n#include <linux/shmem_fs.h>\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\nstatic int mcopy_atomic_pte(struct mm_struct *dst_mm,\n\t\t\t    pmd_t *dst_pmd,\n\t\t\t    struct vm_area_struct *dst_vma,\n\t\t\t    unsigned long dst_addr,\n\t\t\t    unsigned long src_addr,\n\t\t\t    struct page **pagep)\n{\n\tstruct mem_cgroup *memcg;\n\tpte_t _dst_pte, *dst_pte;\n\tspinlock_t *ptl;\n\tvoid *page_kaddr;\n\tint ret;\n\tstruct page *page;\n\n\tif (!*pagep) {\n\t\tret = -ENOMEM;\n\t\tpage = alloc_page_vma(GFP_HIGHUSER_MOVABLE, dst_vma, dst_addr);\n\t\tif (!page)\n\t\t\tgoto out;\n\n\t\tpage_kaddr = kmap_atomic(page);\n\t\tret = copy_from_user(page_kaddr,\n\t\t\t\t     (const void __user *) src_addr,\n\t\t\t\t     PAGE_SIZE);\n\t\tkunmap_atomic(page_kaddr);\n\n\t\t/* fallback to copy_from_user outside mmap_sem */\n\t\tif (unlikely(ret)) {\n\t\t\tret = -ENOENT;\n\t\t\t*pagep = page;\n\t\t\t/* don't free the page */\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tpage = *pagep;\n\t\t*pagep = NULL;\n\t}\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceeding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\n\tret = -ENOMEM;\n\tif (mem_cgroup_try_charge(page, dst_mm, GFP_KERNEL, &memcg, false))\n\t\tgoto out_release;\n\n\t_dst_pte = mk_pte(page, dst_vma->vm_page_prot);\n\tif (dst_vma->vm_flags & VM_WRITE)\n\t\t_dst_pte = pte_mkwrite(pte_mkdirty(_dst_pte));\n\n\tret = -EEXIST;\n\tdst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\n\tif (!pte_none(*dst_pte))\n\t\tgoto out_release_uncharge_unlock;\n\n\tinc_mm_counter(dst_mm, MM_ANONPAGES);\n\tpage_add_new_anon_rmap(page, dst_vma, dst_addr, false);\n\tmem_cgroup_commit_charge(page, memcg, false, false);\n\tlru_cache_add_active_or_unevictable(page, dst_vma);\n\n\tset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\n\tpte_unmap_unlock(dst_pte, ptl);\n\tret = 0;\nout:\n\treturn ret;\nout_release_uncharge_unlock:\n\tpte_unmap_unlock(dst_pte, ptl);\n\tmem_cgroup_cancel_charge(page, memcg, false);\nout_release:\n\tput_page(page);\n\tgoto out;\n}\n\nstatic int mfill_zeropage_pte(struct mm_struct *dst_mm,\n\t\t\t      pmd_t *dst_pmd,\n\t\t\t      struct vm_area_struct *dst_vma,\n\t\t\t      unsigned long dst_addr)\n{\n\tpte_t _dst_pte, *dst_pte;\n\tspinlock_t *ptl;\n\tint ret;\n\n\t_dst_pte = pte_mkspecial(pfn_pte(my_zero_pfn(dst_addr),\n\t\t\t\t\t dst_vma->vm_page_prot));\n\tret = -EEXIST;\n\tdst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\n\tif (!pte_none(*dst_pte))\n\t\tgoto out_unlock;\n\tset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\tret = 0;\nout_unlock:\n\tpte_unmap_unlock(dst_pte, ptl);\n\treturn ret;\n}\n\nstatic pmd_t *mm_alloc_pmd(struct mm_struct *mm, unsigned long address)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\n\tpgd = pgd_offset(mm, address);\n\tp4d = p4d_alloc(mm, pgd, address);\n\tif (!p4d)\n\t\treturn NULL;\n\tpud = pud_alloc(mm, p4d, address);\n\tif (!pud)\n\t\treturn NULL;\n\t/*\n\t * Note that we didn't run this because the pmd was\n\t * missing, the *pmd may be already established and in\n\t * turn it may also be a trans_huge_pmd.\n\t */\n\treturn pmd_alloc(mm, pud, address);\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/*\n * __mcopy_atomic processing for HUGETLB vmas.  Note that this routine is\n * called with mmap_sem held, it will release mmap_sem before returning.\n */\nstatic __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,\n\t\t\t\t\t      struct vm_area_struct *dst_vma,\n\t\t\t\t\t      unsigned long dst_start,\n\t\t\t\t\t      unsigned long src_start,\n\t\t\t\t\t      unsigned long len,\n\t\t\t\t\t      bool zeropage)\n{\n\tint vm_alloc_shared = dst_vma->vm_flags & VM_SHARED;\n\tint vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tssize_t err;\n\tpte_t *dst_pte;\n\tunsigned long src_addr, dst_addr;\n\tlong copied;\n\tstruct page *page;\n\tstruct hstate *h;\n\tunsigned long vma_hpagesize;\n\tpgoff_t idx;\n\tu32 hash;\n\tstruct address_space *mapping;\n\n\t/*\n\t * There is no default zero huge page for all huge page sizes as\n\t * supported by hugetlb.  A PMD_SIZE huge pages may exist as used\n\t * by THP.  Since we can not reliably insert a zero page, this\n\t * feature is not supported.\n\t */\n\tif (zeropage) {\n\t\tup_read(&dst_mm->mmap_sem);\n\t\treturn -EINVAL;\n\t}\n\n\tsrc_addr = src_start;\n\tdst_addr = dst_start;\n\tcopied = 0;\n\tpage = NULL;\n\tvma_hpagesize = vma_kernel_pagesize(dst_vma);\n\n\t/*\n\t * Validate alignment based on huge page size\n\t */\n\terr = -EINVAL;\n\tif (dst_start & (vma_hpagesize - 1) || len & (vma_hpagesize - 1))\n\t\tgoto out_unlock;\n\nretry:\n\t/*\n\t * On routine entry dst_vma is set.  If we had to drop mmap_sem and\n\t * retry, dst_vma will be set to NULL and we must lookup again.\n\t */\n\tif (!dst_vma) {\n\t\terr = -ENOENT;\n\t\tdst_vma = find_vma(dst_mm, dst_start);\n\t\tif (!dst_vma || !is_vm_hugetlb_page(dst_vma))\n\t\t\tgoto out_unlock;\n\t\t/*\n\t\t * Only allow __mcopy_atomic_hugetlb on userfaultfd\n\t\t * registered ranges.\n\t\t */\n\t\tif (!dst_vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto out_unlock;\n\n\t\tif (dst_start < dst_vma->vm_start ||\n\t\t    dst_start + len > dst_vma->vm_end)\n\t\t\tgoto out_unlock;\n\n\t\terr = -EINVAL;\n\t\tif (vma_hpagesize != vma_kernel_pagesize(dst_vma))\n\t\t\tgoto out_unlock;\n\n\t\tvm_shared = dst_vma->vm_flags & VM_SHARED;\n\t}\n\n\tif (WARN_ON(dst_addr & (vma_hpagesize - 1) ||\n\t\t    (len - copied) & (vma_hpagesize - 1)))\n\t\tgoto out_unlock;\n\n\t/*\n\t * If not shared, ensure the dst_vma has a anon_vma.\n\t */\n\terr = -ENOMEM;\n\tif (!vm_shared) {\n\t\tif (unlikely(anon_vma_prepare(dst_vma)))\n\t\t\tgoto out_unlock;\n\t}\n\n\th = hstate_vma(dst_vma);\n\n\twhile (src_addr < src_start + len) {\n\t\tpte_t dst_pteval;\n\n\t\tBUG_ON(dst_addr >= dst_start + len);\n\t\tVM_BUG_ON(dst_addr & ~huge_page_mask(h));\n\n\t\t/*\n\t\t * Serialize via hugetlb_fault_mutex\n\t\t */\n\t\tidx = linear_page_index(dst_vma, dst_addr);\n\t\tmapping = dst_vma->vm_file->f_mapping;\n\t\thash = hugetlb_fault_mutex_hash(h, dst_mm, dst_vma, mapping,\n\t\t\t\t\t\t\t\tidx, dst_addr);\n\t\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\n\t\terr = -ENOMEM;\n\t\tdst_pte = huge_pte_alloc(dst_mm, dst_addr, huge_page_size(h));\n\t\tif (!dst_pte) {\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = -EEXIST;\n\t\tdst_pteval = huge_ptep_get(dst_pte);\n\t\tif (!huge_pte_none(dst_pteval)) {\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = hugetlb_mcopy_atomic_pte(dst_mm, dst_pte, dst_vma,\n\t\t\t\t\t\tdst_addr, src_addr, &page);\n\n\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\tvm_alloc_shared = vm_shared;\n\n\t\tcond_resched();\n\n\t\tif (unlikely(err == -ENOENT)) {\n\t\t\tup_read(&dst_mm->mmap_sem);\n\t\t\tBUG_ON(!page);\n\n\t\t\terr = copy_huge_page_from_user(page,\n\t\t\t\t\t\t(const void __user *)src_addr,\n\t\t\t\t\t\tpages_per_huge_page(h), true);\n\t\t\tif (unlikely(err)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdown_read(&dst_mm->mmap_sem);\n\n\t\t\tdst_vma = NULL;\n\t\t\tgoto retry;\n\t\t} else\n\t\t\tBUG_ON(page);\n\n\t\tif (!err) {\n\t\t\tdst_addr += vma_hpagesize;\n\t\t\tsrc_addr += vma_hpagesize;\n\t\t\tcopied += vma_hpagesize;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\terr = -EINTR;\n\t\t}\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\nout_unlock:\n\tup_read(&dst_mm->mmap_sem);\nout:\n\tif (page) {\n\t\t/*\n\t\t * We encountered an error and are about to free a newly\n\t\t * allocated huge page.\n\t\t *\n\t\t * Reservation handling is very subtle, and is different for\n\t\t * private and shared mappings.  See the routine\n\t\t * restore_reserve_on_error for details.  Unfortunately, we\n\t\t * can not call restore_reserve_on_error now as it would\n\t\t * require holding mmap_sem.\n\t\t *\n\t\t * If a reservation for the page existed in the reservation\n\t\t * map of a private mapping, the map was modified to indicate\n\t\t * the reservation was consumed when the page was allocated.\n\t\t * We clear the PagePrivate flag now so that the global\n\t\t * reserve count will not be incremented in free_huge_page.\n\t\t * The reservation map will still indicate the reservation\n\t\t * was consumed and possibly prevent later page allocation.\n\t\t * This is better than leaking a global reservation.  If no\n\t\t * reservation existed, it is still safe to clear PagePrivate\n\t\t * as no adjustments to reservation counts were made during\n\t\t * allocation.\n\t\t *\n\t\t * The reservation map for shared mappings indicates which\n\t\t * pages have reservations.  When a huge page is allocated\n\t\t * for an address with a reservation, no change is made to\n\t\t * the reserve map.  In this case PagePrivate will be set\n\t\t * to indicate that the global reservation count should be\n\t\t * incremented when the page is freed.  This is the desired\n\t\t * behavior.  However, when a huge page is allocated for an\n\t\t * address without a reservation a reservation entry is added\n\t\t * to the reservation map, and PagePrivate will not be set.\n\t\t * When the page is freed, the global reserve count will NOT\n\t\t * be incremented and it will appear as though we have leaked\n\t\t * reserved page.  In this case, set PagePrivate so that the\n\t\t * global reserve count will be incremented to match the\n\t\t * reservation map entry which was created.\n\t\t *\n\t\t * Note that vm_alloc_shared is based on the flags of the vma\n\t\t * for which the page was originally allocated.  dst_vma could\n\t\t * be different or NULL on error.\n\t\t */\n\t\tif (vm_alloc_shared)\n\t\t\tSetPagePrivate(page);\n\t\telse\n\t\t\tClearPagePrivate(page);\n\t\tput_page(page);\n\t}\n\tBUG_ON(copied < 0);\n\tBUG_ON(err > 0);\n\tBUG_ON(!copied && !err);\n\treturn copied ? copied : err;\n}\n#else /* !CONFIG_HUGETLB_PAGE */\n/* fail at build time if gcc attempts to use this */\nextern ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,\n\t\t\t\t      struct vm_area_struct *dst_vma,\n\t\t\t\t      unsigned long dst_start,\n\t\t\t\t      unsigned long src_start,\n\t\t\t\t      unsigned long len,\n\t\t\t\t      bool zeropage);\n#endif /* CONFIG_HUGETLB_PAGE */\n\nstatic __always_inline ssize_t mfill_atomic_pte(struct mm_struct *dst_mm,\n\t\t\t\t\t\tpmd_t *dst_pmd,\n\t\t\t\t\t\tstruct vm_area_struct *dst_vma,\n\t\t\t\t\t\tunsigned long dst_addr,\n\t\t\t\t\t\tunsigned long src_addr,\n\t\t\t\t\t\tstruct page **page,\n\t\t\t\t\t\tbool zeropage)\n{\n\tssize_t err;\n\n\t/*\n\t * The normal page fault path for a shmem will invoke the\n\t * fault, fill the hole in the file and COW it right away. The\n\t * result generates plain anonymous memory. So when we are\n\t * asked to fill an hole in a MAP_PRIVATE shmem mapping, we'll\n\t * generate anonymous memory directly without actually filling\n\t * the hole. For the MAP_PRIVATE case the robustness check\n\t * only happens in the pagetable (to verify it's still none)\n\t * and not in the radix tree.\n\t */\n\tif (!(dst_vma->vm_flags & VM_SHARED)) {\n\t\tif (!zeropage)\n\t\t\terr = mcopy_atomic_pte(dst_mm, dst_pmd, dst_vma,\n\t\t\t\t\t       dst_addr, src_addr, page);\n\t\telse\n\t\t\terr = mfill_zeropage_pte(dst_mm, dst_pmd,\n\t\t\t\t\t\t dst_vma, dst_addr);\n\t} else {\n\t\tif (!zeropage)\n\t\t\terr = shmem_mcopy_atomic_pte(dst_mm, dst_pmd,\n\t\t\t\t\t\t     dst_vma, dst_addr,\n\t\t\t\t\t\t     src_addr, page);\n\t\telse\n\t\t\terr = shmem_mfill_zeropage_pte(dst_mm, dst_pmd,\n\t\t\t\t\t\t       dst_vma, dst_addr);\n\t}\n\n\treturn err;\n}\n\nstatic __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,\n\t\t\t\t\t      unsigned long dst_start,\n\t\t\t\t\t      unsigned long src_start,\n\t\t\t\t\t      unsigned long len,\n\t\t\t\t\t      bool zeropage,\n\t\t\t\t\t      bool *mmap_changing)\n{\n\tstruct vm_area_struct *dst_vma;\n\tssize_t err;\n\tpmd_t *dst_pmd;\n\tunsigned long src_addr, dst_addr;\n\tlong copied;\n\tstruct page *page;\n\n\t/*\n\t * Sanitize the command parameters:\n\t */\n\tBUG_ON(dst_start & ~PAGE_MASK);\n\tBUG_ON(len & ~PAGE_MASK);\n\n\t/* Does the address range wrap, or is the span zero-sized? */\n\tBUG_ON(src_start + len <= src_start);\n\tBUG_ON(dst_start + len <= dst_start);\n\n\tsrc_addr = src_start;\n\tdst_addr = dst_start;\n\tcopied = 0;\n\tpage = NULL;\nretry:\n\tdown_read(&dst_mm->mmap_sem);\n\n\t/*\n\t * If memory mappings are changing because of non-cooperative\n\t * operation (e.g. mremap) running in parallel, bail out and\n\t * request the user to retry later\n\t */\n\terr = -EAGAIN;\n\tif (mmap_changing && READ_ONCE(*mmap_changing))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Make sure the vma is not shared, that the dst range is\n\t * both valid and fully within a single existing vma.\n\t */\n\terr = -ENOENT;\n\tdst_vma = find_vma(dst_mm, dst_start);\n\tif (!dst_vma)\n\t\tgoto out_unlock;\n\t/*\n\t * Be strict and only allow __mcopy_atomic on userfaultfd\n\t * registered ranges to prevent userland errors going\n\t * unnoticed. As far as the VM consistency is concerned, it\n\t * would be perfectly safe to remove this check, but there's\n\t * no useful usage for __mcopy_atomic ouside of userfaultfd\n\t * registered ranges. This is after all why these are ioctls\n\t * belonging to the userfaultfd and not syscalls.\n\t */\n\tif (!dst_vma->vm_userfaultfd_ctx.ctx)\n\t\tgoto out_unlock;\n\n\tif (dst_start < dst_vma->vm_start ||\n\t    dst_start + len > dst_vma->vm_end)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\t/*\n\t * shmem_zero_setup is invoked in mmap for MAP_ANONYMOUS|MAP_SHARED but\n\t * it will overwrite vm_ops, so vma_is_anonymous must return false.\n\t */\n\tif (WARN_ON_ONCE(vma_is_anonymous(dst_vma) &&\n\t    dst_vma->vm_flags & VM_SHARED))\n\t\tgoto out_unlock;\n\n\t/*\n\t * If this is a HUGETLB vma, pass off to appropriate routine\n\t */\n\tif (is_vm_hugetlb_page(dst_vma))\n\t\treturn  __mcopy_atomic_hugetlb(dst_mm, dst_vma, dst_start,\n\t\t\t\t\t\tsrc_start, len, zeropage);\n\n\tif (!vma_is_anonymous(dst_vma) && !vma_is_shmem(dst_vma))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Ensure the dst_vma has a anon_vma or this page\n\t * would get a NULL anon_vma when moved in the\n\t * dst_vma.\n\t */\n\terr = -ENOMEM;\n\tif (!(dst_vma->vm_flags & VM_SHARED) &&\n\t    unlikely(anon_vma_prepare(dst_vma)))\n\t\tgoto out_unlock;\n\n\twhile (src_addr < src_start + len) {\n\t\tpmd_t dst_pmdval;\n\n\t\tBUG_ON(dst_addr >= dst_start + len);\n\n\t\tdst_pmd = mm_alloc_pmd(dst_mm, dst_addr);\n\t\tif (unlikely(!dst_pmd)) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\tdst_pmdval = pmd_read_atomic(dst_pmd);\n\t\t/*\n\t\t * If the dst_pmd is mapped as THP don't\n\t\t * override it and just be strict.\n\t\t */\n\t\tif (unlikely(pmd_trans_huge(dst_pmdval))) {\n\t\t\terr = -EEXIST;\n\t\t\tbreak;\n\t\t}\n\t\tif (unlikely(pmd_none(dst_pmdval)) &&\n\t\t    unlikely(__pte_alloc(dst_mm, dst_pmd, dst_addr))) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\t/* If an huge pmd materialized from under us fail */\n\t\tif (unlikely(pmd_trans_huge(*dst_pmd))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tBUG_ON(pmd_none(*dst_pmd));\n\t\tBUG_ON(pmd_trans_huge(*dst_pmd));\n\n\t\terr = mfill_atomic_pte(dst_mm, dst_pmd, dst_vma, dst_addr,\n\t\t\t\t       src_addr, &page, zeropage);\n\t\tcond_resched();\n\n\t\tif (unlikely(err == -ENOENT)) {\n\t\t\tvoid *page_kaddr;\n\n\t\t\tup_read(&dst_mm->mmap_sem);\n\t\t\tBUG_ON(!page);\n\n\t\t\tpage_kaddr = kmap(page);\n\t\t\terr = copy_from_user(page_kaddr,\n\t\t\t\t\t     (const void __user *) src_addr,\n\t\t\t\t\t     PAGE_SIZE);\n\t\t\tkunmap(page);\n\t\t\tif (unlikely(err)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tgoto retry;\n\t\t} else\n\t\t\tBUG_ON(page);\n\n\t\tif (!err) {\n\t\t\tdst_addr += PAGE_SIZE;\n\t\t\tsrc_addr += PAGE_SIZE;\n\t\t\tcopied += PAGE_SIZE;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\terr = -EINTR;\n\t\t}\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\nout_unlock:\n\tup_read(&dst_mm->mmap_sem);\nout:\n\tif (page)\n\t\tput_page(page);\n\tBUG_ON(copied < 0);\n\tBUG_ON(err > 0);\n\tBUG_ON(!copied && !err);\n\treturn copied ? copied : err;\n}\n\nssize_t mcopy_atomic(struct mm_struct *dst_mm, unsigned long dst_start,\n\t\t     unsigned long src_start, unsigned long len,\n\t\t     bool *mmap_changing)\n{\n\treturn __mcopy_atomic(dst_mm, dst_start, src_start, len, false,\n\t\t\t      mmap_changing);\n}\n\nssize_t mfill_zeropage(struct mm_struct *dst_mm, unsigned long start,\n\t\t       unsigned long len, bool *mmap_changing)\n{\n\treturn __mcopy_atomic(dst_mm, start, 0, len, true, mmap_changing);\n}\n"], "fixing_code": ["/*\n *  fs/userfaultfd.c\n *\n *  Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>\n *  Copyright (C) 2008-2009 Red Hat, Inc.\n *  Copyright (C) 2015  Red Hat, Inc.\n *\n *  This work is licensed under the terms of the GNU GPL, version 2. See\n *  the COPYING file in the top-level directory.\n *\n *  Some part derived from fs/eventfd.c (anon inode setup) and\n *  mm/ksm.c (mm hashing).\n */\n\n#include <linux/list.h>\n#include <linux/hashtable.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/file.h>\n#include <linux/bug.h>\n#include <linux/anon_inodes.h>\n#include <linux/syscalls.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mempolicy.h>\n#include <linux/ioctl.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n\nstatic struct kmem_cache *userfaultfd_ctx_cachep __read_mostly;\n\nenum userfaultfd_state {\n\tUFFD_STATE_WAIT_API,\n\tUFFD_STATE_RUNNING,\n};\n\n/*\n * Start with fault_pending_wqh and fault_wqh so they're more likely\n * to be in the same cacheline.\n */\nstruct userfaultfd_ctx {\n\t/* waitqueue head for the pending (i.e. not read) userfaults */\n\twait_queue_head_t fault_pending_wqh;\n\t/* waitqueue head for the userfaults */\n\twait_queue_head_t fault_wqh;\n\t/* waitqueue head for the pseudo fd to wakeup poll/read */\n\twait_queue_head_t fd_wqh;\n\t/* waitqueue head for events */\n\twait_queue_head_t event_wqh;\n\t/* a refile sequence protected by fault_pending_wqh lock */\n\tstruct seqcount refile_seq;\n\t/* pseudo fd refcounting */\n\tatomic_t refcount;\n\t/* userfaultfd syscall flags */\n\tunsigned int flags;\n\t/* features requested from the userspace */\n\tunsigned int features;\n\t/* state machine */\n\tenum userfaultfd_state state;\n\t/* released */\n\tbool released;\n\t/* memory mappings are changing because of non-cooperative event */\n\tbool mmap_changing;\n\t/* mm with one ore more vmas attached to this userfaultfd_ctx */\n\tstruct mm_struct *mm;\n};\n\nstruct userfaultfd_fork_ctx {\n\tstruct userfaultfd_ctx *orig;\n\tstruct userfaultfd_ctx *new;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_unmap_ctx {\n\tstruct userfaultfd_ctx *ctx;\n\tunsigned long start;\n\tunsigned long end;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_wait_queue {\n\tstruct uffd_msg msg;\n\twait_queue_entry_t wq;\n\tstruct userfaultfd_ctx *ctx;\n\tbool waken;\n};\n\nstruct userfaultfd_wake_range {\n\tunsigned long start;\n\tunsigned long len;\n};\n\nstatic int userfaultfd_wake_function(wait_queue_entry_t *wq, unsigned mode,\n\t\t\t\t     int wake_flags, void *key)\n{\n\tstruct userfaultfd_wake_range *range = key;\n\tint ret;\n\tstruct userfaultfd_wait_queue *uwq;\n\tunsigned long start, len;\n\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\n\tret = 0;\n\t/* len == 0 means wake all */\n\tstart = range->start;\n\tlen = range->len;\n\tif (len && (start > uwq->msg.arg.pagefault.address ||\n\t\t    start + len <= uwq->msg.arg.pagefault.address))\n\t\tgoto out;\n\tWRITE_ONCE(uwq->waken, true);\n\t/*\n\t * The Program-Order guarantees provided by the scheduler\n\t * ensure uwq->waken is visible before the task is woken.\n\t */\n\tret = wake_up_state(wq->private, mode);\n\tif (ret) {\n\t\t/*\n\t\t * Wake only once, autoremove behavior.\n\t\t *\n\t\t * After the effect of list_del_init is visible to the other\n\t\t * CPUs, the waitqueue may disappear from under us, see the\n\t\t * !list_empty_careful() in handle_userfault().\n\t\t *\n\t\t * try_to_wake_up() has an implicit smp_mb(), and the\n\t\t * wq->private is read before calling the extern function\n\t\t * \"wake_up_state\" (which in turns calls try_to_wake_up).\n\t\t */\n\t\tlist_del_init(&wq->entry);\n\t}\nout:\n\treturn ret;\n}\n\n/**\n * userfaultfd_ctx_get - Acquires a reference to the internal userfaultfd\n * context.\n * @ctx: [in] Pointer to the userfaultfd context.\n */\nstatic void userfaultfd_ctx_get(struct userfaultfd_ctx *ctx)\n{\n\tif (!atomic_inc_not_zero(&ctx->refcount))\n\t\tBUG();\n}\n\n/**\n * userfaultfd_ctx_put - Releases a reference to the internal userfaultfd\n * context.\n * @ctx: [in] Pointer to userfaultfd context.\n *\n * The userfaultfd context reference must have been previously acquired either\n * with userfaultfd_ctx_get() or userfaultfd_ctx_fdget().\n */\nstatic void userfaultfd_ctx_put(struct userfaultfd_ctx *ctx)\n{\n\tif (atomic_dec_and_test(&ctx->refcount)) {\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_pending_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_pending_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->event_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->event_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fd_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fd_wqh));\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n}\n\nstatic inline void msg_init(struct uffd_msg *msg)\n{\n\tBUILD_BUG_ON(sizeof(struct uffd_msg) != 32);\n\t/*\n\t * Must use memset to zero out the paddings or kernel data is\n\t * leaked to userland.\n\t */\n\tmemset(msg, 0, sizeof(struct uffd_msg));\n}\n\nstatic inline struct uffd_msg userfault_msg(unsigned long address,\n\t\t\t\t\t    unsigned int flags,\n\t\t\t\t\t    unsigned long reason,\n\t\t\t\t\t    unsigned int features)\n{\n\tstruct uffd_msg msg;\n\tmsg_init(&msg);\n\tmsg.event = UFFD_EVENT_PAGEFAULT;\n\tmsg.arg.pagefault.address = address;\n\tif (flags & FAULT_FLAG_WRITE)\n\t\t/*\n\t\t * If UFFD_FEATURE_PAGEFAULT_FLAG_WP was set in the\n\t\t * uffdio_api.features and UFFD_PAGEFAULT_FLAG_WRITE\n\t\t * was not set in a UFFD_EVENT_PAGEFAULT, it means it\n\t\t * was a read fault, otherwise if set it means it's\n\t\t * a write fault.\n\t\t */\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WRITE;\n\tif (reason & VM_UFFD_WP)\n\t\t/*\n\t\t * If UFFD_FEATURE_PAGEFAULT_FLAG_WP was set in the\n\t\t * uffdio_api.features and UFFD_PAGEFAULT_FLAG_WP was\n\t\t * not set in a UFFD_EVENT_PAGEFAULT, it means it was\n\t\t * a missing fault, otherwise if set it means it's a\n\t\t * write protect fault.\n\t\t */\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WP;\n\tif (features & UFFD_FEATURE_THREAD_ID)\n\t\tmsg.arg.pagefault.feat.ptid = task_pid_vnr(current);\n\treturn msg;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/*\n * Same functionality as userfaultfd_must_wait below with modifications for\n * hugepmd ranges.\n */\nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tpte_t *ptep, pte;\n\tbool ret = true;\n\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tptep = huge_pte_offset(mm, address, vma_mmu_pagesize(vma));\n\n\tif (!ptep)\n\t\tgoto out;\n\n\tret = false;\n\tpte = huge_ptep_get(ptep);\n\n\t/*\n\t * Lockless access: we're in a wait_event so it's ok if it\n\t * changes under us.\n\t */\n\tif (huge_pte_none(pte))\n\t\tret = true;\n\tif (!huge_pte_write(pte) && (reason & VM_UFFD_WP))\n\t\tret = true;\nout:\n\treturn ret;\n}\n#else\nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\treturn false;\t/* should never get here */\n}\n#endif /* CONFIG_HUGETLB_PAGE */\n\n/*\n * Verify the pagetables are still not ok after having reigstered into\n * the fault_pending_wqh to avoid userland having to UFFDIO_WAKE any\n * userfault that has already been resolved, if userfaultfd_read and\n * UFFDIO_COPY|ZEROPAGE are being run simultaneously on two different\n * threads.\n */\nstatic inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tbool ret = true;\n\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\tp4d = p4d_offset(pgd, address);\n\tif (!p4d_present(*p4d))\n\t\tgoto out;\n\tpud = pud_offset(p4d, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\tpmd = pmd_offset(pud, address);\n\t/*\n\t * READ_ONCE must function as a barrier with narrower scope\n\t * and it must be equivalent to:\n\t *\t_pmd = *pmd; barrier();\n\t *\n\t * This is to deal with the instability (as in\n\t * pmd_trans_unstable) of the pmd.\n\t */\n\t_pmd = READ_ONCE(*pmd);\n\tif (pmd_none(_pmd))\n\t\tgoto out;\n\n\tret = false;\n\tif (!pmd_present(_pmd))\n\t\tgoto out;\n\n\tif (pmd_trans_huge(_pmd))\n\t\tgoto out;\n\n\t/*\n\t * the pmd is stable (as in !pmd_trans_unstable) so we can re-read it\n\t * and use the standard pte_offset_map() instead of parsing _pmd.\n\t */\n\tpte = pte_offset_map(pmd, address);\n\t/*\n\t * Lockless access: we're in a wait_event so it's ok if it\n\t * changes under us.\n\t */\n\tif (pte_none(*pte))\n\t\tret = true;\n\tpte_unmap(pte);\n\nout:\n\treturn ret;\n}\n\n/*\n * The locking rules involved in returning VM_FAULT_RETRY depending on\n * FAULT_FLAG_ALLOW_RETRY, FAULT_FLAG_RETRY_NOWAIT and\n * FAULT_FLAG_KILLABLE are not straightforward. The \"Caution\"\n * recommendation in __lock_page_or_retry is not an understatement.\n *\n * If FAULT_FLAG_ALLOW_RETRY is set, the mmap_sem must be released\n * before returning VM_FAULT_RETRY only if FAULT_FLAG_RETRY_NOWAIT is\n * not set.\n *\n * If FAULT_FLAG_ALLOW_RETRY is set but FAULT_FLAG_KILLABLE is not\n * set, VM_FAULT_RETRY can still be returned if and only if there are\n * fatal_signal_pending()s, and the mmap_sem must be released before\n * returning it.\n */\nvm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)\n{\n\tstruct mm_struct *mm = vmf->vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue uwq;\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\tbool must_wait, return_to_userland;\n\tlong blocking_state;\n\n\t/*\n\t * We don't do userfault handling for the final child pid update.\n\t *\n\t * We also don't do userfault handling during\n\t * coredumping. hugetlbfs has the special\n\t * follow_hugetlb_page() to skip missing pages in the\n\t * FOLL_DUMP case, anon memory also checks for FOLL_DUMP with\n\t * the no_page_table() helper in follow_page_mask(), but the\n\t * shmem_vm_ops->fault method is invoked even during\n\t * coredumping without mmap_sem and it ends up here.\n\t */\n\tif (current->flags & (PF_EXITING|PF_DUMPCORE))\n\t\tgoto out;\n\n\t/*\n\t * Coredumping runs without mmap_sem so we can only check that\n\t * the mmap_sem is held, if PF_DUMPCORE was not set.\n\t */\n\tWARN_ON_ONCE(!rwsem_is_locked(&mm->mmap_sem));\n\n\tctx = vmf->vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx)\n\t\tgoto out;\n\n\tBUG_ON(ctx->mm != mm);\n\n\tVM_BUG_ON(reason & ~(VM_UFFD_MISSING|VM_UFFD_WP));\n\tVM_BUG_ON(!(reason & VM_UFFD_MISSING) ^ !!(reason & VM_UFFD_WP));\n\n\tif (ctx->features & UFFD_FEATURE_SIGBUS)\n\t\tgoto out;\n\n\t/*\n\t * If it's already released don't get it. This avoids to loop\n\t * in __get_user_pages if userfaultfd_release waits on the\n\t * caller of handle_userfault to release the mmap_sem.\n\t */\n\tif (unlikely(READ_ONCE(ctx->released))) {\n\t\t/*\n\t\t * Don't return VM_FAULT_SIGBUS in this case, so a non\n\t\t * cooperative manager can close the uffd after the\n\t\t * last UFFDIO_COPY, without risking to trigger an\n\t\t * involuntary SIGBUS if the process was starting the\n\t\t * userfaultfd while the userfaultfd was still armed\n\t\t * (but after the last UFFDIO_COPY). If the uffd\n\t\t * wasn't already closed when the userfault reached\n\t\t * this point, that would normally be solved by\n\t\t * userfaultfd_must_wait returning 'false'.\n\t\t *\n\t\t * If we were to return VM_FAULT_SIGBUS here, the non\n\t\t * cooperative manager would be instead forced to\n\t\t * always call UFFDIO_UNREGISTER before it can safely\n\t\t * close the uffd.\n\t\t */\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Check that we can return VM_FAULT_RETRY.\n\t *\n\t * NOTE: it should become possible to return VM_FAULT_RETRY\n\t * even if FAULT_FLAG_TRIED is set without leading to gup()\n\t * -EBUSY failures, if the userfaultfd is to be extended for\n\t * VM_UFFD_WP tracking and we intend to arm the userfault\n\t * without first stopping userland access to the memory. For\n\t * VM_UFFD_MISSING userfaults this is enough for now.\n\t */\n\tif (unlikely(!(vmf->flags & FAULT_FLAG_ALLOW_RETRY))) {\n\t\t/*\n\t\t * Validate the invariant that nowait must allow retry\n\t\t * to be sure not to return SIGBUS erroneously on\n\t\t * nowait invocations.\n\t\t */\n\t\tBUG_ON(vmf->flags & FAULT_FLAG_RETRY_NOWAIT);\n#ifdef CONFIG_DEBUG_VM\n\t\tif (printk_ratelimit()) {\n\t\t\tprintk(KERN_WARNING\n\t\t\t       \"FAULT_FLAG_ALLOW_RETRY missing %x\\n\",\n\t\t\t       vmf->flags);\n\t\t\tdump_stack();\n\t\t}\n#endif\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Handle nowait, not much to do other than tell it to retry\n\t * and wait.\n\t */\n\tret = VM_FAULT_RETRY;\n\tif (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)\n\t\tgoto out;\n\n\t/* take the reference before dropping the mmap_sem */\n\tuserfaultfd_ctx_get(ctx);\n\n\tinit_waitqueue_func_entry(&uwq.wq, userfaultfd_wake_function);\n\tuwq.wq.private = current;\n\tuwq.msg = userfault_msg(vmf->address, vmf->flags, reason,\n\t\t\tctx->features);\n\tuwq.ctx = ctx;\n\tuwq.waken = false;\n\n\treturn_to_userland =\n\t\t(vmf->flags & (FAULT_FLAG_USER|FAULT_FLAG_KILLABLE)) ==\n\t\t(FAULT_FLAG_USER|FAULT_FLAG_KILLABLE);\n\tblocking_state = return_to_userland ? TASK_INTERRUPTIBLE :\n\t\t\t TASK_KILLABLE;\n\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->fault_pending_wqh, &uwq.wq);\n\t/*\n\t * The smp_mb() after __set_current_state prevents the reads\n\t * following the spin_unlock to happen before the list_add in\n\t * __add_wait_queue.\n\t */\n\tset_current_state(blocking_state);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\tif (!is_vm_hugetlb_page(vmf->vma))\n\t\tmust_wait = userfaultfd_must_wait(ctx, vmf->address, vmf->flags,\n\t\t\t\t\t\t  reason);\n\telse\n\t\tmust_wait = userfaultfd_huge_must_wait(ctx, vmf->vma,\n\t\t\t\t\t\t       vmf->address,\n\t\t\t\t\t\t       vmf->flags, reason);\n\tup_read(&mm->mmap_sem);\n\n\tif (likely(must_wait && !READ_ONCE(ctx->released) &&\n\t\t   (return_to_userland ? !signal_pending(current) :\n\t\t    !fatal_signal_pending(current)))) {\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\t\tret |= VM_FAULT_MAJOR;\n\n\t\t/*\n\t\t * False wakeups can orginate even from rwsem before\n\t\t * up_read() however userfaults will wait either for a\n\t\t * targeted wakeup on the specific uwq waitqueue from\n\t\t * wake_userfault() or for signals or for uffd\n\t\t * release.\n\t\t */\n\t\twhile (!READ_ONCE(uwq.waken)) {\n\t\t\t/*\n\t\t\t * This needs the full smp_store_mb()\n\t\t\t * guarantee as the state write must be\n\t\t\t * visible to other CPUs before reading\n\t\t\t * uwq.waken from other CPUs.\n\t\t\t */\n\t\t\tset_current_state(blocking_state);\n\t\t\tif (READ_ONCE(uwq.waken) ||\n\t\t\t    READ_ONCE(ctx->released) ||\n\t\t\t    (return_to_userland ? signal_pending(current) :\n\t\t\t     fatal_signal_pending(current)))\n\t\t\t\tbreak;\n\t\t\tschedule();\n\t\t}\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\n\tif (return_to_userland) {\n\t\tif (signal_pending(current) &&\n\t\t    !fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * If we got a SIGSTOP or SIGCONT and this is\n\t\t\t * a normal userland page fault, just let\n\t\t\t * userland return so the signal will be\n\t\t\t * handled and gdb debugging works.  The page\n\t\t\t * fault code immediately after we return from\n\t\t\t * this function is going to release the\n\t\t\t * mmap_sem and it's not depending on it\n\t\t\t * (unlike gup would if we were not to return\n\t\t\t * VM_FAULT_RETRY).\n\t\t\t *\n\t\t\t * If a fatal signal is pending we still take\n\t\t\t * the streamlined VM_FAULT_RETRY failure path\n\t\t\t * and there's no need to retake the mmap_sem\n\t\t\t * in such case.\n\t\t\t */\n\t\t\tdown_read(&mm->mmap_sem);\n\t\t\tret = VM_FAULT_NOPAGE;\n\t\t}\n\t}\n\n\t/*\n\t * Here we race with the list_del; list_add in\n\t * userfaultfd_ctx_read(), however because we don't ever run\n\t * list_del_init() to refile across the two lists, the prev\n\t * and next pointers will never point to self. list_add also\n\t * would never let any of the two pointers to point to\n\t * self. So list_empty_careful won't risk to see both pointers\n\t * pointing to self at any time during the list refile. The\n\t * only case where list_del_init() is called is the full\n\t * removal in the wake function and there we don't re-list_add\n\t * and it's fine not to block on the spinlock. The uwq on this\n\t * kernel stack can be released after the list_del_init.\n\t */\n\tif (!list_empty_careful(&uwq.wq.entry)) {\n\t\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t\t/*\n\t\t * No need of list_del_init(), the uwq on the stack\n\t\t * will be freed shortly anyway.\n\t\t */\n\t\tlist_del(&uwq.wq.entry);\n\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\n\tuserfaultfd_ctx_put(ctx);\n\nout:\n\treturn ret;\n}\n\nstatic void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t      struct userfaultfd_wait_queue *ewq)\n{\n\tstruct userfaultfd_ctx *release_new_ctx;\n\n\tif (WARN_ON_ONCE(current->flags & PF_EXITING))\n\t\tgoto out;\n\n\tewq->ctx = ctx;\n\tinit_waitqueue_entry(&ewq->wq, current);\n\trelease_new_ctx = NULL;\n\n\tspin_lock(&ctx->event_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->event_wqh, &ewq->wq);\n\tfor (;;) {\n\t\tset_current_state(TASK_KILLABLE);\n\t\tif (ewq->msg.event == 0)\n\t\t\tbreak;\n\t\tif (READ_ONCE(ctx->released) ||\n\t\t    fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * &ewq->wq may be queued in fork_event, but\n\t\t\t * __remove_wait_queue ignores the head\n\t\t\t * parameter. It would be a problem if it\n\t\t\t * didn't.\n\t\t\t */\n\t\t\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n\t\t\tif (ewq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tstruct userfaultfd_ctx *new;\n\n\t\t\t\tnew = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tewq->msg.arg.reserved.reserved1;\n\t\t\t\trelease_new_ctx = new;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock(&ctx->event_wqh.lock);\n\n\tif (release_new_ctx) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct mm_struct *mm = release_new_ctx->mm;\n\n\t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n\t\tdown_write(&mm->mmap_sem);\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n\t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n\t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\t\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\t\t}\n\t\tup_write(&mm->mmap_sem);\n\n\t\tuserfaultfd_ctx_put(release_new_ctx);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\nout:\n\tWRITE_ONCE(ctx->mmap_changing, false);\n\tuserfaultfd_ctx_put(ctx);\n}\n\nstatic void userfaultfd_event_complete(struct userfaultfd_ctx *ctx,\n\t\t\t\t       struct userfaultfd_wait_queue *ewq)\n{\n\tewq->msg.event = 0;\n\twake_up_locked(&ctx->event_wqh);\n\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n}\n\nint dup_userfaultfd(struct vm_area_struct *vma, struct list_head *fcs)\n{\n\tstruct userfaultfd_ctx *ctx = NULL, *octx;\n\tstruct userfaultfd_fork_ctx *fctx;\n\n\toctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!octx || !(octx->features & UFFD_FEATURE_EVENT_FORK)) {\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(fctx, fcs, list)\n\t\tif (fctx->orig == octx) {\n\t\t\tctx = fctx->new;\n\t\t\tbreak;\n\t\t}\n\n\tif (!ctx) {\n\t\tfctx = kmalloc(sizeof(*fctx), GFP_KERNEL);\n\t\tif (!fctx)\n\t\t\treturn -ENOMEM;\n\n\t\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\t\tif (!ctx) {\n\t\t\tkfree(fctx);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tatomic_set(&ctx->refcount, 1);\n\t\tctx->flags = octx->flags;\n\t\tctx->state = UFFD_STATE_RUNNING;\n\t\tctx->features = octx->features;\n\t\tctx->released = false;\n\t\tctx->mmap_changing = false;\n\t\tctx->mm = vma->vm_mm;\n\t\tmmgrab(ctx->mm);\n\n\t\tuserfaultfd_ctx_get(octx);\n\t\tWRITE_ONCE(octx->mmap_changing, true);\n\t\tfctx->orig = octx;\n\t\tfctx->new = ctx;\n\t\tlist_add_tail(&fctx->list, fcs);\n\t}\n\n\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\treturn 0;\n}\n\nstatic void dup_fctx(struct userfaultfd_fork_ctx *fctx)\n{\n\tstruct userfaultfd_ctx *ctx = fctx->orig;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_FORK;\n\tewq.msg.arg.reserved.reserved1 = (unsigned long)fctx->new;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nvoid dup_userfaultfd_complete(struct list_head *fcs)\n{\n\tstruct userfaultfd_fork_ctx *fctx, *n;\n\n\tlist_for_each_entry_safe(fctx, n, fcs, list) {\n\t\tdup_fctx(fctx);\n\t\tlist_del(&fctx->list);\n\t\tkfree(fctx);\n\t}\n}\n\nvoid mremap_userfaultfd_prep(struct vm_area_struct *vma,\n\t\t\t     struct vm_userfaultfd_ctx *vm_ctx)\n{\n\tstruct userfaultfd_ctx *ctx;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (ctx && (ctx->features & UFFD_FEATURE_EVENT_REMAP)) {\n\t\tvm_ctx->ctx = ctx;\n\t\tuserfaultfd_ctx_get(ctx);\n\t\tWRITE_ONCE(ctx->mmap_changing, true);\n\t}\n}\n\nvoid mremap_userfaultfd_complete(struct vm_userfaultfd_ctx *vm_ctx,\n\t\t\t\t unsigned long from, unsigned long to,\n\t\t\t\t unsigned long len)\n{\n\tstruct userfaultfd_ctx *ctx = vm_ctx->ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tif (!ctx)\n\t\treturn;\n\n\tif (to & ~PAGE_MASK) {\n\t\tuserfaultfd_ctx_put(ctx);\n\t\treturn;\n\t}\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMAP;\n\tewq.msg.arg.remap.from = from;\n\tewq.msg.arg.remap.to = to;\n\tewq.msg.arg.remap.len = len;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nbool userfaultfd_remove(struct vm_area_struct *vma,\n\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_REMOVE))\n\t\treturn true;\n\n\tuserfaultfd_ctx_get(ctx);\n\tWRITE_ONCE(ctx->mmap_changing, true);\n\tup_read(&mm->mmap_sem);\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMOVE;\n\tewq.msg.arg.remove.start = start;\n\tewq.msg.arg.remove.end = end;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n\n\treturn false;\n}\n\nstatic bool has_unmap_ctx(struct userfaultfd_ctx *ctx, struct list_head *unmaps,\n\t\t\t  unsigned long start, unsigned long end)\n{\n\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\n\tlist_for_each_entry(unmap_ctx, unmaps, list)\n\t\tif (unmap_ctx->ctx == ctx && unmap_ctx->start == start &&\n\t\t    unmap_ctx->end == end)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nint userfaultfd_unmap_prep(struct vm_area_struct *vma,\n\t\t\t   unsigned long start, unsigned long end,\n\t\t\t   struct list_head *unmaps)\n{\n\tfor ( ; vma && vma->vm_start < end; vma = vma->vm_next) {\n\t\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\t\tstruct userfaultfd_ctx *ctx = vma->vm_userfaultfd_ctx.ctx;\n\n\t\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_UNMAP) ||\n\t\t    has_unmap_ctx(ctx, unmaps, start, end))\n\t\t\tcontinue;\n\n\t\tunmap_ctx = kzalloc(sizeof(*unmap_ctx), GFP_KERNEL);\n\t\tif (!unmap_ctx)\n\t\t\treturn -ENOMEM;\n\n\t\tuserfaultfd_ctx_get(ctx);\n\t\tWRITE_ONCE(ctx->mmap_changing, true);\n\t\tunmap_ctx->ctx = ctx;\n\t\tunmap_ctx->start = start;\n\t\tunmap_ctx->end = end;\n\t\tlist_add_tail(&unmap_ctx->list, unmaps);\n\t}\n\n\treturn 0;\n}\n\nvoid userfaultfd_unmap_complete(struct mm_struct *mm, struct list_head *uf)\n{\n\tstruct userfaultfd_unmap_ctx *ctx, *n;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tlist_for_each_entry_safe(ctx, n, uf, list) {\n\t\tmsg_init(&ewq.msg);\n\n\t\tewq.msg.event = UFFD_EVENT_UNMAP;\n\t\tewq.msg.arg.remove.start = ctx->start;\n\t\tewq.msg.arg.remove.end = ctx->end;\n\n\t\tuserfaultfd_event_wait_completion(ctx->ctx, &ewq);\n\n\t\tlist_del(&ctx->list);\n\t\tkfree(ctx);\n\t}\n}\n\nstatic int userfaultfd_release(struct inode *inode, struct file *file)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev;\n\t/* len == 0 means wake all */\n\tstruct userfaultfd_wake_range range = { .len = 0, };\n\tunsigned long new_flags;\n\n\tWRITE_ONCE(ctx->released, true);\n\n\tif (!mmget_not_zero(mm))\n\t\tgoto wakeup;\n\n\t/*\n\t * Flush page faults out of all CPUs. NOTE: all page faults\n\t * must be retried without returning VM_FAULT_SIGBUS if\n\t * userfaultfd_ctx_get() succeeds but vma->vma_userfault_ctx\n\t * changes while handle_userfault released the mmap_sem. So\n\t * it's critical that released is set to true (above), before\n\t * taking the mmap_sem for writing.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tprev = NULL;\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tcond_resched();\n\t\tBUG_ON(!!vma->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(vma->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\t\tif (vma->vm_userfaultfd_ctx.ctx != ctx) {\n\t\t\tprev = vma;\n\t\t\tcontinue;\n\t\t}\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, vma->vm_start, vma->vm_end,\n\t\t\t\t new_flags, vma->anon_vma,\n\t\t\t\t vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev)\n\t\t\tvma = prev;\n\t\telse\n\t\t\tprev = vma;\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t}\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nwakeup:\n\t/*\n\t * After no new page faults can wait on this fault_*wqh, flush\n\t * the last page faults that may have been already waiting on\n\t * the fault_*wqh.\n\t */\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL, &range);\n\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, &range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/* Flush pending events that may still wait on event_wqh */\n\twake_up_all(&ctx->event_wqh);\n\n\twake_up_poll(&ctx->fd_wqh, EPOLLHUP);\n\tuserfaultfd_ctx_put(ctx);\n\treturn 0;\n}\n\n/* fault_pending_wqh.lock must be hold by the caller */\nstatic inline struct userfaultfd_wait_queue *find_userfault_in(\n\t\twait_queue_head_t *wqh)\n{\n\twait_queue_entry_t *wq;\n\tstruct userfaultfd_wait_queue *uwq;\n\n\tVM_BUG_ON(!spin_is_locked(&wqh->lock));\n\n\tuwq = NULL;\n\tif (!waitqueue_active(wqh))\n\t\tgoto out;\n\t/* walk in reverse to provide FIFO behavior to read userfaults */\n\twq = list_last_entry(&wqh->head, typeof(*wq), entry);\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\nout:\n\treturn uwq;\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->fault_pending_wqh);\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault_evt(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->event_wqh);\n}\n\nstatic __poll_t userfaultfd_poll(struct file *file, poll_table *wait)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\t__poll_t ret;\n\n\tpoll_wait(file, &ctx->fd_wqh, wait);\n\n\tswitch (ctx->state) {\n\tcase UFFD_STATE_WAIT_API:\n\t\treturn EPOLLERR;\n\tcase UFFD_STATE_RUNNING:\n\t\t/*\n\t\t * poll() never guarantees that read won't block.\n\t\t * userfaults can be waken before they're read().\n\t\t */\n\t\tif (unlikely(!(file->f_flags & O_NONBLOCK)))\n\t\t\treturn EPOLLERR;\n\t\t/*\n\t\t * lockless access to see if there are pending faults\n\t\t * __pollwait last action is the add_wait_queue but\n\t\t * the spin_unlock would allow the waitqueue_active to\n\t\t * pass above the actual list_add inside\n\t\t * add_wait_queue critical section. So use a full\n\t\t * memory barrier to serialize the list_add write of\n\t\t * add_wait_queue() with the waitqueue_active read\n\t\t * below.\n\t\t */\n\t\tret = 0;\n\t\tsmp_mb();\n\t\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\t\tret = EPOLLIN;\n\t\telse if (waitqueue_active(&ctx->event_wqh))\n\t\t\tret = EPOLLIN;\n\n\t\treturn ret;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn EPOLLERR;\n\t}\n}\n\nstatic const struct file_operations userfaultfd_fops;\n\nstatic int resolve_userfault_fork(struct userfaultfd_ctx *ctx,\n\t\t\t\t  struct userfaultfd_ctx *new,\n\t\t\t\t  struct uffd_msg *msg)\n{\n\tint fd;\n\n\tfd = anon_inode_getfd(\"[userfaultfd]\", &userfaultfd_fops, new,\n\t\t\t      O_RDWR | (new->flags & UFFD_SHARED_FCNTL_FLAGS));\n\tif (fd < 0)\n\t\treturn fd;\n\n\tmsg->arg.reserved.reserved1 = 0;\n\tmsg->arg.fork.ufd = fd;\n\treturn 0;\n}\n\nstatic ssize_t userfaultfd_ctx_read(struct userfaultfd_ctx *ctx, int no_wait,\n\t\t\t\t    struct uffd_msg *msg)\n{\n\tssize_t ret;\n\tDECLARE_WAITQUEUE(wait, current);\n\tstruct userfaultfd_wait_queue *uwq;\n\t/*\n\t * Handling fork event requires sleeping operations, so\n\t * we drop the event_wqh lock, then do these ops, then\n\t * lock it back and wake up the waiter. While the lock is\n\t * dropped the ewq may go away so we keep track of it\n\t * carefully.\n\t */\n\tLIST_HEAD(fork_event);\n\tstruct userfaultfd_ctx *fork_nctx = NULL;\n\n\t/* always take the fd_wqh lock before the fault_pending_wqh lock */\n\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t__add_wait_queue(&ctx->fd_wqh, &wait);\n\tfor (;;) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t\tuwq = find_userfault(ctx);\n\t\tif (uwq) {\n\t\t\t/*\n\t\t\t * Use a seqcount to repeat the lockless check\n\t\t\t * in wake_userfault() to avoid missing\n\t\t\t * wakeups because during the refile both\n\t\t\t * waitqueue could become empty if this is the\n\t\t\t * only userfault.\n\t\t\t */\n\t\t\twrite_seqcount_begin(&ctx->refile_seq);\n\n\t\t\t/*\n\t\t\t * The fault_pending_wqh.lock prevents the uwq\n\t\t\t * to disappear from under us.\n\t\t\t *\n\t\t\t * Refile this userfault from\n\t\t\t * fault_pending_wqh to fault_wqh, it's not\n\t\t\t * pending anymore after we read it.\n\t\t\t *\n\t\t\t * Use list_del() by hand (as\n\t\t\t * userfaultfd_wake_function also uses\n\t\t\t * list_del_init() by hand) to be sure nobody\n\t\t\t * changes __remove_wait_queue() to use\n\t\t\t * list_del_init() in turn breaking the\n\t\t\t * !list_empty_careful() check in\n\t\t\t * handle_userfault(). The uwq->wq.head list\n\t\t\t * must never be empty at any time during the\n\t\t\t * refile, or the waitqueue could disappear\n\t\t\t * from under us. The \"wait_queue_head_t\"\n\t\t\t * parameter of __remove_wait_queue() is unused\n\t\t\t * anyway.\n\t\t\t */\n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\tadd_wait_queue(&ctx->fault_wqh, &uwq->wq);\n\n\t\t\twrite_seqcount_end(&ctx->refile_seq);\n\n\t\t\t/* careful to always initialize msg if ret == 0 */\n\t\t\t*msg = uwq->msg;\n\t\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t\tuwq = find_userfault_evt(ctx);\n\t\tif (uwq) {\n\t\t\t*msg = uwq->msg;\n\n\t\t\tif (uwq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tfork_nctx = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tuwq->msg.arg.reserved.reserved1;\n\t\t\t\tlist_move(&uwq->wq.entry, &fork_event);\n\t\t\t\t/*\n\t\t\t\t * fork_nctx can be freed as soon as\n\t\t\t\t * we drop the lock, unless we take a\n\t\t\t\t * reference on it.\n\t\t\t\t */\n\t\t\t\tuserfaultfd_ctx_get(fork_nctx);\n\t\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (no_wait) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\t\tschedule();\n\t\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t}\n\t__remove_wait_queue(&ctx->fd_wqh, &wait);\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\n\tif (!ret && msg->event == UFFD_EVENT_FORK) {\n\t\tret = resolve_userfault_fork(ctx, fork_nctx, msg);\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t\tif (!list_empty(&fork_event)) {\n\t\t\t/*\n\t\t\t * The fork thread didn't abort, so we can\n\t\t\t * drop the temporary refcount.\n\t\t\t */\n\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\n\t\t\tuwq = list_first_entry(&fork_event,\n\t\t\t\t\t       typeof(*uwq),\n\t\t\t\t\t       wq.entry);\n\t\t\t/*\n\t\t\t * If fork_event list wasn't empty and in turn\n\t\t\t * the event wasn't already released by fork\n\t\t\t * (the event is allocated on fork kernel\n\t\t\t * stack), put the event back to its place in\n\t\t\t * the event_wq. fork_event head will be freed\n\t\t\t * as soon as we return so the event cannot\n\t\t\t * stay queued there no matter the current\n\t\t\t * \"ret\" value.\n\t\t\t */\n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\t__add_wait_queue(&ctx->event_wqh, &uwq->wq);\n\n\t\t\t/*\n\t\t\t * Leave the event in the waitqueue and report\n\t\t\t * error to userland if we failed to resolve\n\t\t\t * the userfault fork.\n\t\t\t */\n\t\t\tif (likely(!ret))\n\t\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t} else {\n\t\t\t/*\n\t\t\t * Here the fork thread aborted and the\n\t\t\t * refcount from the fork thread on fork_nctx\n\t\t\t * has already been released. We still hold\n\t\t\t * the reference we took before releasing the\n\t\t\t * lock above. If resolve_userfault_fork\n\t\t\t * failed we've to drop it because the\n\t\t\t * fork_nctx has to be freed in such case. If\n\t\t\t * it succeeded we'll hold it because the new\n\t\t\t * uffd references it.\n\t\t\t */\n\t\t\tif (ret)\n\t\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\t\t}\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t userfaultfd_read(struct file *file, char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tssize_t _ret, ret = 0;\n\tstruct uffd_msg msg;\n\tint no_wait = file->f_flags & O_NONBLOCK;\n\n\tif (ctx->state == UFFD_STATE_WAIT_API)\n\t\treturn -EINVAL;\n\n\tfor (;;) {\n\t\tif (count < sizeof(msg))\n\t\t\treturn ret ? ret : -EINVAL;\n\t\t_ret = userfaultfd_ctx_read(ctx, no_wait, &msg);\n\t\tif (_ret < 0)\n\t\t\treturn ret ? ret : _ret;\n\t\tif (copy_to_user((__u64 __user *) buf, &msg, sizeof(msg)))\n\t\t\treturn ret ? ret : -EFAULT;\n\t\tret += sizeof(msg);\n\t\tbuf += sizeof(msg);\n\t\tcount -= sizeof(msg);\n\t\t/*\n\t\t * Allow to read more than one fault at time but only\n\t\t * block if waiting for the very first one.\n\t\t */\n\t\tno_wait = O_NONBLOCK;\n\t}\n}\n\nstatic void __wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t     struct userfaultfd_wake_range *range)\n{\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t/* wake all in the range and autoremove */\n\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL,\n\t\t\t\t     range);\n\tif (waitqueue_active(&ctx->fault_wqh))\n\t\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n}\n\nstatic __always_inline void wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t   struct userfaultfd_wake_range *range)\n{\n\tunsigned seq;\n\tbool need_wakeup;\n\n\t/*\n\t * To be sure waitqueue_active() is not reordered by the CPU\n\t * before the pagetable update, use an explicit SMP memory\n\t * barrier here. PT lock release or up_read(mmap_sem) still\n\t * have release semantics that can allow the\n\t * waitqueue_active() to be reordered before the pte update.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * Use waitqueue_active because it's very frequent to\n\t * change the address space atomically even if there are no\n\t * userfaults yet. So we take the spinlock only when we're\n\t * sure we've userfaults to wake.\n\t */\n\tdo {\n\t\tseq = read_seqcount_begin(&ctx->refile_seq);\n\t\tneed_wakeup = waitqueue_active(&ctx->fault_pending_wqh) ||\n\t\t\twaitqueue_active(&ctx->fault_wqh);\n\t\tcond_resched();\n\t} while (read_seqcount_retry(&ctx->refile_seq, seq));\n\tif (need_wakeup)\n\t\t__wake_userfault(ctx, range);\n}\n\nstatic __always_inline int validate_range(struct mm_struct *mm,\n\t\t\t\t\t  __u64 start, __u64 len)\n{\n\t__u64 task_size = mm->task_size;\n\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (len & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (!len)\n\t\treturn -EINVAL;\n\tif (start < mmap_min_addr)\n\t\treturn -EINVAL;\n\tif (start >= task_size)\n\t\treturn -EINVAL;\n\tif (len > task_size - start)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic inline bool vma_can_userfault(struct vm_area_struct *vma)\n{\n\treturn vma_is_anonymous(vma) || is_vm_hugetlb_page(vma) ||\n\t\tvma_is_shmem(vma);\n}\n\nstatic int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * UFFDIO_COPY will fill file holes even without\n\t\t * PROT_WRITE. This check enforces that if this is a\n\t\t * MAP_SHARED, the process has write permission to the backing\n\t\t * file. If VM_MAYWRITE is set it also enforces that on a\n\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further\n\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.\n\t\t */\n\t\tret = -EPERM;\n\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}\n\n/*\n * userfaultfd_wake may be used in combination with the\n * UFFDIO_*_MODE_DONTWAKE to wakeup userfaults in batches.\n */\nstatic int userfaultfd_wake(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\tint ret;\n\tstruct uffdio_range uffdio_wake;\n\tstruct userfaultfd_wake_range range;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_wake, buf, sizeof(uffdio_wake)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_wake.start, uffdio_wake.len);\n\tif (ret)\n\t\tgoto out;\n\n\trange.start = uffdio_wake.start;\n\trange.len = uffdio_wake.len;\n\n\t/*\n\t * len == 0 means wake all and we don't want to wake all here,\n\t * so check it again to be sure.\n\t */\n\tVM_BUG_ON(!range.len);\n\n\twake_userfault(ctx, &range);\n\tret = 0;\n\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_copy(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_copy uffdio_copy;\n\tstruct uffdio_copy __user *user_uffdio_copy;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_copy = (struct uffdio_copy __user *) arg;\n\n\tret = -EAGAIN;\n\tif (READ_ONCE(ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_copy, user_uffdio_copy,\n\t\t\t   /* don't copy \"copy\" last field */\n\t\t\t   sizeof(uffdio_copy)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_copy.dst, uffdio_copy.len);\n\tif (ret)\n\t\tgoto out;\n\t/*\n\t * double check for wraparound just in case. copy_from_user()\n\t * will later check uffdio_copy.src + uffdio_copy.len to fit\n\t * in the userland range.\n\t */\n\tret = -EINVAL;\n\tif (uffdio_copy.src + uffdio_copy.len <= uffdio_copy.src)\n\t\tgoto out;\n\tif (uffdio_copy.mode & ~UFFDIO_COPY_MODE_DONTWAKE)\n\t\tgoto out;\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mcopy_atomic(ctx->mm, uffdio_copy.dst, uffdio_copy.src,\n\t\t\t\t   uffdio_copy.len, &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_copy->copy)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\tBUG_ON(!ret);\n\t/* len == 0 would wake all */\n\trange.len = ret;\n\tif (!(uffdio_copy.mode & UFFDIO_COPY_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_copy.dst;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_copy.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_zeropage(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_zeropage uffdio_zeropage;\n\tstruct uffdio_zeropage __user *user_uffdio_zeropage;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_zeropage = (struct uffdio_zeropage __user *) arg;\n\n\tret = -EAGAIN;\n\tif (READ_ONCE(ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_zeropage, user_uffdio_zeropage,\n\t\t\t   /* don't copy \"zeropage\" last field */\n\t\t\t   sizeof(uffdio_zeropage)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t     uffdio_zeropage.range.len);\n\tif (ret)\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (uffdio_zeropage.mode & ~UFFDIO_ZEROPAGE_MODE_DONTWAKE)\n\t\tgoto out;\n\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mfill_zeropage(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t\t     uffdio_zeropage.range.len,\n\t\t\t\t     &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_zeropage->zeropage)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\t/* len == 0 would wake all */\n\tBUG_ON(!ret);\n\trange.len = ret;\n\tif (!(uffdio_zeropage.mode & UFFDIO_ZEROPAGE_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_zeropage.range.start;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_zeropage.range.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic inline unsigned int uffd_ctx_features(__u64 user_features)\n{\n\t/*\n\t * For the current set of features the bits just coincide\n\t */\n\treturn (unsigned int)user_features;\n}\n\n/*\n * userland asks for a certain API version and we return which bits\n * and ioctl commands are implemented in this kernel for such API\n * version or -EINVAL if unknown.\n */\nstatic int userfaultfd_api(struct userfaultfd_ctx *ctx,\n\t\t\t   unsigned long arg)\n{\n\tstruct uffdio_api uffdio_api;\n\tvoid __user *buf = (void __user *)arg;\n\tint ret;\n\t__u64 features;\n\n\tret = -EINVAL;\n\tif (ctx->state != UFFD_STATE_WAIT_API)\n\t\tgoto out;\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_api, buf, sizeof(uffdio_api)))\n\t\tgoto out;\n\tfeatures = uffdio_api.features;\n\tif (uffdio_api.api != UFFD_API || (features & ~UFFD_API_FEATURES)) {\n\t\tmemset(&uffdio_api, 0, sizeof(uffdio_api));\n\t\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\t\tgoto out;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\t/* report all available features and ioctls to userland */\n\tuffdio_api.features = UFFD_API_FEATURES;\n\tuffdio_api.ioctls = UFFD_API_IOCTLS;\n\tret = -EFAULT;\n\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\tgoto out;\n\tctx->state = UFFD_STATE_RUNNING;\n\t/* only enable the requested features for this uffd context */\n\tctx->features = uffd_ctx_features(features);\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic long userfaultfd_ioctl(struct file *file, unsigned cmd,\n\t\t\t      unsigned long arg)\n{\n\tint ret = -EINVAL;\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\n\tif (cmd != UFFDIO_API && ctx->state == UFFD_STATE_WAIT_API)\n\t\treturn -EINVAL;\n\n\tswitch(cmd) {\n\tcase UFFDIO_API:\n\t\tret = userfaultfd_api(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_REGISTER:\n\t\tret = userfaultfd_register(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_UNREGISTER:\n\t\tret = userfaultfd_unregister(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_WAKE:\n\t\tret = userfaultfd_wake(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_COPY:\n\t\tret = userfaultfd_copy(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_ZEROPAGE:\n\t\tret = userfaultfd_zeropage(ctx, arg);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic void userfaultfd_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct userfaultfd_ctx *ctx = f->private_data;\n\twait_queue_entry_t *wq;\n\tunsigned long pending = 0, total = 0;\n\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\tlist_for_each_entry(wq, &ctx->fault_pending_wqh.head, entry) {\n\t\tpending++;\n\t\ttotal++;\n\t}\n\tlist_for_each_entry(wq, &ctx->fault_wqh.head, entry) {\n\t\ttotal++;\n\t}\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/*\n\t * If more protocols will be added, there will be all shown\n\t * separated by a space. Like this:\n\t *\tprotocols: aa:... bb:...\n\t */\n\tseq_printf(m, \"pending:\\t%lu\\ntotal:\\t%lu\\nAPI:\\t%Lx:%x:%Lx\\n\",\n\t\t   pending, total, UFFD_API, ctx->features,\n\t\t   UFFD_API_IOCTLS|UFFD_API_RANGE_IOCTLS);\n}\n#endif\n\nstatic const struct file_operations userfaultfd_fops = {\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= userfaultfd_show_fdinfo,\n#endif\n\t.release\t= userfaultfd_release,\n\t.poll\t\t= userfaultfd_poll,\n\t.read\t\t= userfaultfd_read,\n\t.unlocked_ioctl = userfaultfd_ioctl,\n\t.compat_ioctl\t= userfaultfd_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic void init_once_userfaultfd_ctx(void *mem)\n{\n\tstruct userfaultfd_ctx *ctx = (struct userfaultfd_ctx *) mem;\n\n\tinit_waitqueue_head(&ctx->fault_pending_wqh);\n\tinit_waitqueue_head(&ctx->fault_wqh);\n\tinit_waitqueue_head(&ctx->event_wqh);\n\tinit_waitqueue_head(&ctx->fd_wqh);\n\tseqcount_init(&ctx->refile_seq);\n}\n\nSYSCALL_DEFINE1(userfaultfd, int, flags)\n{\n\tstruct userfaultfd_ctx *ctx;\n\tint fd;\n\n\tBUG_ON(!current->mm);\n\n\t/* Check the UFFD_* constants for consistency.  */\n\tBUILD_BUG_ON(UFFD_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(UFFD_NONBLOCK != O_NONBLOCK);\n\n\tif (flags & ~UFFD_SHARED_FCNTL_FLAGS)\n\t\treturn -EINVAL;\n\n\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tatomic_set(&ctx->refcount, 1);\n\tctx->flags = flags;\n\tctx->features = 0;\n\tctx->state = UFFD_STATE_WAIT_API;\n\tctx->released = false;\n\tctx->mmap_changing = false;\n\tctx->mm = current->mm;\n\t/* prevent the mm struct to be freed */\n\tmmgrab(ctx->mm);\n\n\tfd = anon_inode_getfd(\"[userfaultfd]\", &userfaultfd_fops, ctx,\n\t\t\t      O_RDWR | (flags & UFFD_SHARED_FCNTL_FLAGS));\n\tif (fd < 0) {\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n\treturn fd;\n}\n\nstatic int __init userfaultfd_init(void)\n{\n\tuserfaultfd_ctx_cachep = kmem_cache_create(\"userfaultfd_ctx_cache\",\n\t\t\t\t\t\tsizeof(struct userfaultfd_ctx),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tinit_once_userfaultfd_ctx);\n\treturn 0;\n}\n__initcall(userfaultfd_init);\n", "/*\n *  mm/userfaultfd.c\n *\n *  Copyright (C) 2015  Red Hat, Inc.\n *\n *  This work is licensed under the terms of the GNU GPL, version 2. See\n *  the COPYING file in the top-level directory.\n */\n\n#include <linux/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mmu_notifier.h>\n#include <linux/hugetlb.h>\n#include <linux/shmem_fs.h>\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\nstatic int mcopy_atomic_pte(struct mm_struct *dst_mm,\n\t\t\t    pmd_t *dst_pmd,\n\t\t\t    struct vm_area_struct *dst_vma,\n\t\t\t    unsigned long dst_addr,\n\t\t\t    unsigned long src_addr,\n\t\t\t    struct page **pagep)\n{\n\tstruct mem_cgroup *memcg;\n\tpte_t _dst_pte, *dst_pte;\n\tspinlock_t *ptl;\n\tvoid *page_kaddr;\n\tint ret;\n\tstruct page *page;\n\n\tif (!*pagep) {\n\t\tret = -ENOMEM;\n\t\tpage = alloc_page_vma(GFP_HIGHUSER_MOVABLE, dst_vma, dst_addr);\n\t\tif (!page)\n\t\t\tgoto out;\n\n\t\tpage_kaddr = kmap_atomic(page);\n\t\tret = copy_from_user(page_kaddr,\n\t\t\t\t     (const void __user *) src_addr,\n\t\t\t\t     PAGE_SIZE);\n\t\tkunmap_atomic(page_kaddr);\n\n\t\t/* fallback to copy_from_user outside mmap_sem */\n\t\tif (unlikely(ret)) {\n\t\t\tret = -ENOENT;\n\t\t\t*pagep = page;\n\t\t\t/* don't free the page */\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tpage = *pagep;\n\t\t*pagep = NULL;\n\t}\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceeding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\n\tret = -ENOMEM;\n\tif (mem_cgroup_try_charge(page, dst_mm, GFP_KERNEL, &memcg, false))\n\t\tgoto out_release;\n\n\t_dst_pte = mk_pte(page, dst_vma->vm_page_prot);\n\tif (dst_vma->vm_flags & VM_WRITE)\n\t\t_dst_pte = pte_mkwrite(pte_mkdirty(_dst_pte));\n\n\tret = -EEXIST;\n\tdst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\n\tif (!pte_none(*dst_pte))\n\t\tgoto out_release_uncharge_unlock;\n\n\tinc_mm_counter(dst_mm, MM_ANONPAGES);\n\tpage_add_new_anon_rmap(page, dst_vma, dst_addr, false);\n\tmem_cgroup_commit_charge(page, memcg, false, false);\n\tlru_cache_add_active_or_unevictable(page, dst_vma);\n\n\tset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\n\tpte_unmap_unlock(dst_pte, ptl);\n\tret = 0;\nout:\n\treturn ret;\nout_release_uncharge_unlock:\n\tpte_unmap_unlock(dst_pte, ptl);\n\tmem_cgroup_cancel_charge(page, memcg, false);\nout_release:\n\tput_page(page);\n\tgoto out;\n}\n\nstatic int mfill_zeropage_pte(struct mm_struct *dst_mm,\n\t\t\t      pmd_t *dst_pmd,\n\t\t\t      struct vm_area_struct *dst_vma,\n\t\t\t      unsigned long dst_addr)\n{\n\tpte_t _dst_pte, *dst_pte;\n\tspinlock_t *ptl;\n\tint ret;\n\n\t_dst_pte = pte_mkspecial(pfn_pte(my_zero_pfn(dst_addr),\n\t\t\t\t\t dst_vma->vm_page_prot));\n\tret = -EEXIST;\n\tdst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\n\tif (!pte_none(*dst_pte))\n\t\tgoto out_unlock;\n\tset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\tret = 0;\nout_unlock:\n\tpte_unmap_unlock(dst_pte, ptl);\n\treturn ret;\n}\n\nstatic pmd_t *mm_alloc_pmd(struct mm_struct *mm, unsigned long address)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\n\tpgd = pgd_offset(mm, address);\n\tp4d = p4d_alloc(mm, pgd, address);\n\tif (!p4d)\n\t\treturn NULL;\n\tpud = pud_alloc(mm, p4d, address);\n\tif (!pud)\n\t\treturn NULL;\n\t/*\n\t * Note that we didn't run this because the pmd was\n\t * missing, the *pmd may be already established and in\n\t * turn it may also be a trans_huge_pmd.\n\t */\n\treturn pmd_alloc(mm, pud, address);\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/*\n * __mcopy_atomic processing for HUGETLB vmas.  Note that this routine is\n * called with mmap_sem held, it will release mmap_sem before returning.\n */\nstatic __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,\n\t\t\t\t\t      struct vm_area_struct *dst_vma,\n\t\t\t\t\t      unsigned long dst_start,\n\t\t\t\t\t      unsigned long src_start,\n\t\t\t\t\t      unsigned long len,\n\t\t\t\t\t      bool zeropage)\n{\n\tint vm_alloc_shared = dst_vma->vm_flags & VM_SHARED;\n\tint vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tssize_t err;\n\tpte_t *dst_pte;\n\tunsigned long src_addr, dst_addr;\n\tlong copied;\n\tstruct page *page;\n\tstruct hstate *h;\n\tunsigned long vma_hpagesize;\n\tpgoff_t idx;\n\tu32 hash;\n\tstruct address_space *mapping;\n\n\t/*\n\t * There is no default zero huge page for all huge page sizes as\n\t * supported by hugetlb.  A PMD_SIZE huge pages may exist as used\n\t * by THP.  Since we can not reliably insert a zero page, this\n\t * feature is not supported.\n\t */\n\tif (zeropage) {\n\t\tup_read(&dst_mm->mmap_sem);\n\t\treturn -EINVAL;\n\t}\n\n\tsrc_addr = src_start;\n\tdst_addr = dst_start;\n\tcopied = 0;\n\tpage = NULL;\n\tvma_hpagesize = vma_kernel_pagesize(dst_vma);\n\n\t/*\n\t * Validate alignment based on huge page size\n\t */\n\terr = -EINVAL;\n\tif (dst_start & (vma_hpagesize - 1) || len & (vma_hpagesize - 1))\n\t\tgoto out_unlock;\n\nretry:\n\t/*\n\t * On routine entry dst_vma is set.  If we had to drop mmap_sem and\n\t * retry, dst_vma will be set to NULL and we must lookup again.\n\t */\n\tif (!dst_vma) {\n\t\terr = -ENOENT;\n\t\tdst_vma = find_vma(dst_mm, dst_start);\n\t\tif (!dst_vma || !is_vm_hugetlb_page(dst_vma))\n\t\t\tgoto out_unlock;\n\t\t/*\n\t\t * Check the vma is registered in uffd, this is\n\t\t * required to enforce the VM_MAYWRITE check done at\n\t\t * uffd registration time.\n\t\t */\n\t\tif (!dst_vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto out_unlock;\n\n\t\tif (dst_start < dst_vma->vm_start ||\n\t\t    dst_start + len > dst_vma->vm_end)\n\t\t\tgoto out_unlock;\n\n\t\terr = -EINVAL;\n\t\tif (vma_hpagesize != vma_kernel_pagesize(dst_vma))\n\t\t\tgoto out_unlock;\n\n\t\tvm_shared = dst_vma->vm_flags & VM_SHARED;\n\t}\n\n\tif (WARN_ON(dst_addr & (vma_hpagesize - 1) ||\n\t\t    (len - copied) & (vma_hpagesize - 1)))\n\t\tgoto out_unlock;\n\n\t/*\n\t * If not shared, ensure the dst_vma has a anon_vma.\n\t */\n\terr = -ENOMEM;\n\tif (!vm_shared) {\n\t\tif (unlikely(anon_vma_prepare(dst_vma)))\n\t\t\tgoto out_unlock;\n\t}\n\n\th = hstate_vma(dst_vma);\n\n\twhile (src_addr < src_start + len) {\n\t\tpte_t dst_pteval;\n\n\t\tBUG_ON(dst_addr >= dst_start + len);\n\t\tVM_BUG_ON(dst_addr & ~huge_page_mask(h));\n\n\t\t/*\n\t\t * Serialize via hugetlb_fault_mutex\n\t\t */\n\t\tidx = linear_page_index(dst_vma, dst_addr);\n\t\tmapping = dst_vma->vm_file->f_mapping;\n\t\thash = hugetlb_fault_mutex_hash(h, dst_mm, dst_vma, mapping,\n\t\t\t\t\t\t\t\tidx, dst_addr);\n\t\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\n\t\terr = -ENOMEM;\n\t\tdst_pte = huge_pte_alloc(dst_mm, dst_addr, huge_page_size(h));\n\t\tif (!dst_pte) {\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = -EEXIST;\n\t\tdst_pteval = huge_ptep_get(dst_pte);\n\t\tif (!huge_pte_none(dst_pteval)) {\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = hugetlb_mcopy_atomic_pte(dst_mm, dst_pte, dst_vma,\n\t\t\t\t\t\tdst_addr, src_addr, &page);\n\n\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\tvm_alloc_shared = vm_shared;\n\n\t\tcond_resched();\n\n\t\tif (unlikely(err == -ENOENT)) {\n\t\t\tup_read(&dst_mm->mmap_sem);\n\t\t\tBUG_ON(!page);\n\n\t\t\terr = copy_huge_page_from_user(page,\n\t\t\t\t\t\t(const void __user *)src_addr,\n\t\t\t\t\t\tpages_per_huge_page(h), true);\n\t\t\tif (unlikely(err)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdown_read(&dst_mm->mmap_sem);\n\n\t\t\tdst_vma = NULL;\n\t\t\tgoto retry;\n\t\t} else\n\t\t\tBUG_ON(page);\n\n\t\tif (!err) {\n\t\t\tdst_addr += vma_hpagesize;\n\t\t\tsrc_addr += vma_hpagesize;\n\t\t\tcopied += vma_hpagesize;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\terr = -EINTR;\n\t\t}\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\nout_unlock:\n\tup_read(&dst_mm->mmap_sem);\nout:\n\tif (page) {\n\t\t/*\n\t\t * We encountered an error and are about to free a newly\n\t\t * allocated huge page.\n\t\t *\n\t\t * Reservation handling is very subtle, and is different for\n\t\t * private and shared mappings.  See the routine\n\t\t * restore_reserve_on_error for details.  Unfortunately, we\n\t\t * can not call restore_reserve_on_error now as it would\n\t\t * require holding mmap_sem.\n\t\t *\n\t\t * If a reservation for the page existed in the reservation\n\t\t * map of a private mapping, the map was modified to indicate\n\t\t * the reservation was consumed when the page was allocated.\n\t\t * We clear the PagePrivate flag now so that the global\n\t\t * reserve count will not be incremented in free_huge_page.\n\t\t * The reservation map will still indicate the reservation\n\t\t * was consumed and possibly prevent later page allocation.\n\t\t * This is better than leaking a global reservation.  If no\n\t\t * reservation existed, it is still safe to clear PagePrivate\n\t\t * as no adjustments to reservation counts were made during\n\t\t * allocation.\n\t\t *\n\t\t * The reservation map for shared mappings indicates which\n\t\t * pages have reservations.  When a huge page is allocated\n\t\t * for an address with a reservation, no change is made to\n\t\t * the reserve map.  In this case PagePrivate will be set\n\t\t * to indicate that the global reservation count should be\n\t\t * incremented when the page is freed.  This is the desired\n\t\t * behavior.  However, when a huge page is allocated for an\n\t\t * address without a reservation a reservation entry is added\n\t\t * to the reservation map, and PagePrivate will not be set.\n\t\t * When the page is freed, the global reserve count will NOT\n\t\t * be incremented and it will appear as though we have leaked\n\t\t * reserved page.  In this case, set PagePrivate so that the\n\t\t * global reserve count will be incremented to match the\n\t\t * reservation map entry which was created.\n\t\t *\n\t\t * Note that vm_alloc_shared is based on the flags of the vma\n\t\t * for which the page was originally allocated.  dst_vma could\n\t\t * be different or NULL on error.\n\t\t */\n\t\tif (vm_alloc_shared)\n\t\t\tSetPagePrivate(page);\n\t\telse\n\t\t\tClearPagePrivate(page);\n\t\tput_page(page);\n\t}\n\tBUG_ON(copied < 0);\n\tBUG_ON(err > 0);\n\tBUG_ON(!copied && !err);\n\treturn copied ? copied : err;\n}\n#else /* !CONFIG_HUGETLB_PAGE */\n/* fail at build time if gcc attempts to use this */\nextern ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,\n\t\t\t\t      struct vm_area_struct *dst_vma,\n\t\t\t\t      unsigned long dst_start,\n\t\t\t\t      unsigned long src_start,\n\t\t\t\t      unsigned long len,\n\t\t\t\t      bool zeropage);\n#endif /* CONFIG_HUGETLB_PAGE */\n\nstatic __always_inline ssize_t mfill_atomic_pte(struct mm_struct *dst_mm,\n\t\t\t\t\t\tpmd_t *dst_pmd,\n\t\t\t\t\t\tstruct vm_area_struct *dst_vma,\n\t\t\t\t\t\tunsigned long dst_addr,\n\t\t\t\t\t\tunsigned long src_addr,\n\t\t\t\t\t\tstruct page **page,\n\t\t\t\t\t\tbool zeropage)\n{\n\tssize_t err;\n\n\t/*\n\t * The normal page fault path for a shmem will invoke the\n\t * fault, fill the hole in the file and COW it right away. The\n\t * result generates plain anonymous memory. So when we are\n\t * asked to fill an hole in a MAP_PRIVATE shmem mapping, we'll\n\t * generate anonymous memory directly without actually filling\n\t * the hole. For the MAP_PRIVATE case the robustness check\n\t * only happens in the pagetable (to verify it's still none)\n\t * and not in the radix tree.\n\t */\n\tif (!(dst_vma->vm_flags & VM_SHARED)) {\n\t\tif (!zeropage)\n\t\t\terr = mcopy_atomic_pte(dst_mm, dst_pmd, dst_vma,\n\t\t\t\t\t       dst_addr, src_addr, page);\n\t\telse\n\t\t\terr = mfill_zeropage_pte(dst_mm, dst_pmd,\n\t\t\t\t\t\t dst_vma, dst_addr);\n\t} else {\n\t\tif (!zeropage)\n\t\t\terr = shmem_mcopy_atomic_pte(dst_mm, dst_pmd,\n\t\t\t\t\t\t     dst_vma, dst_addr,\n\t\t\t\t\t\t     src_addr, page);\n\t\telse\n\t\t\terr = shmem_mfill_zeropage_pte(dst_mm, dst_pmd,\n\t\t\t\t\t\t       dst_vma, dst_addr);\n\t}\n\n\treturn err;\n}\n\nstatic __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,\n\t\t\t\t\t      unsigned long dst_start,\n\t\t\t\t\t      unsigned long src_start,\n\t\t\t\t\t      unsigned long len,\n\t\t\t\t\t      bool zeropage,\n\t\t\t\t\t      bool *mmap_changing)\n{\n\tstruct vm_area_struct *dst_vma;\n\tssize_t err;\n\tpmd_t *dst_pmd;\n\tunsigned long src_addr, dst_addr;\n\tlong copied;\n\tstruct page *page;\n\n\t/*\n\t * Sanitize the command parameters:\n\t */\n\tBUG_ON(dst_start & ~PAGE_MASK);\n\tBUG_ON(len & ~PAGE_MASK);\n\n\t/* Does the address range wrap, or is the span zero-sized? */\n\tBUG_ON(src_start + len <= src_start);\n\tBUG_ON(dst_start + len <= dst_start);\n\n\tsrc_addr = src_start;\n\tdst_addr = dst_start;\n\tcopied = 0;\n\tpage = NULL;\nretry:\n\tdown_read(&dst_mm->mmap_sem);\n\n\t/*\n\t * If memory mappings are changing because of non-cooperative\n\t * operation (e.g. mremap) running in parallel, bail out and\n\t * request the user to retry later\n\t */\n\terr = -EAGAIN;\n\tif (mmap_changing && READ_ONCE(*mmap_changing))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Make sure the vma is not shared, that the dst range is\n\t * both valid and fully within a single existing vma.\n\t */\n\terr = -ENOENT;\n\tdst_vma = find_vma(dst_mm, dst_start);\n\tif (!dst_vma)\n\t\tgoto out_unlock;\n\t/*\n\t * Check the vma is registered in uffd, this is required to\n\t * enforce the VM_MAYWRITE check done at uffd registration\n\t * time.\n\t */\n\tif (!dst_vma->vm_userfaultfd_ctx.ctx)\n\t\tgoto out_unlock;\n\n\tif (dst_start < dst_vma->vm_start ||\n\t    dst_start + len > dst_vma->vm_end)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\t/*\n\t * shmem_zero_setup is invoked in mmap for MAP_ANONYMOUS|MAP_SHARED but\n\t * it will overwrite vm_ops, so vma_is_anonymous must return false.\n\t */\n\tif (WARN_ON_ONCE(vma_is_anonymous(dst_vma) &&\n\t    dst_vma->vm_flags & VM_SHARED))\n\t\tgoto out_unlock;\n\n\t/*\n\t * If this is a HUGETLB vma, pass off to appropriate routine\n\t */\n\tif (is_vm_hugetlb_page(dst_vma))\n\t\treturn  __mcopy_atomic_hugetlb(dst_mm, dst_vma, dst_start,\n\t\t\t\t\t\tsrc_start, len, zeropage);\n\n\tif (!vma_is_anonymous(dst_vma) && !vma_is_shmem(dst_vma))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Ensure the dst_vma has a anon_vma or this page\n\t * would get a NULL anon_vma when moved in the\n\t * dst_vma.\n\t */\n\terr = -ENOMEM;\n\tif (!(dst_vma->vm_flags & VM_SHARED) &&\n\t    unlikely(anon_vma_prepare(dst_vma)))\n\t\tgoto out_unlock;\n\n\twhile (src_addr < src_start + len) {\n\t\tpmd_t dst_pmdval;\n\n\t\tBUG_ON(dst_addr >= dst_start + len);\n\n\t\tdst_pmd = mm_alloc_pmd(dst_mm, dst_addr);\n\t\tif (unlikely(!dst_pmd)) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\tdst_pmdval = pmd_read_atomic(dst_pmd);\n\t\t/*\n\t\t * If the dst_pmd is mapped as THP don't\n\t\t * override it and just be strict.\n\t\t */\n\t\tif (unlikely(pmd_trans_huge(dst_pmdval))) {\n\t\t\terr = -EEXIST;\n\t\t\tbreak;\n\t\t}\n\t\tif (unlikely(pmd_none(dst_pmdval)) &&\n\t\t    unlikely(__pte_alloc(dst_mm, dst_pmd, dst_addr))) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\t/* If an huge pmd materialized from under us fail */\n\t\tif (unlikely(pmd_trans_huge(*dst_pmd))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tBUG_ON(pmd_none(*dst_pmd));\n\t\tBUG_ON(pmd_trans_huge(*dst_pmd));\n\n\t\terr = mfill_atomic_pte(dst_mm, dst_pmd, dst_vma, dst_addr,\n\t\t\t\t       src_addr, &page, zeropage);\n\t\tcond_resched();\n\n\t\tif (unlikely(err == -ENOENT)) {\n\t\t\tvoid *page_kaddr;\n\n\t\t\tup_read(&dst_mm->mmap_sem);\n\t\t\tBUG_ON(!page);\n\n\t\t\tpage_kaddr = kmap(page);\n\t\t\terr = copy_from_user(page_kaddr,\n\t\t\t\t\t     (const void __user *) src_addr,\n\t\t\t\t\t     PAGE_SIZE);\n\t\t\tkunmap(page);\n\t\t\tif (unlikely(err)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tgoto retry;\n\t\t} else\n\t\t\tBUG_ON(page);\n\n\t\tif (!err) {\n\t\t\tdst_addr += PAGE_SIZE;\n\t\t\tsrc_addr += PAGE_SIZE;\n\t\t\tcopied += PAGE_SIZE;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\terr = -EINTR;\n\t\t}\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\nout_unlock:\n\tup_read(&dst_mm->mmap_sem);\nout:\n\tif (page)\n\t\tput_page(page);\n\tBUG_ON(copied < 0);\n\tBUG_ON(err > 0);\n\tBUG_ON(!copied && !err);\n\treturn copied ? copied : err;\n}\n\nssize_t mcopy_atomic(struct mm_struct *dst_mm, unsigned long dst_start,\n\t\t     unsigned long src_start, unsigned long len,\n\t\t     bool *mmap_changing)\n{\n\treturn __mcopy_atomic(dst_mm, dst_start, src_start, len, false,\n\t\t\t      mmap_changing);\n}\n\nssize_t mfill_zeropage(struct mm_struct *dst_mm, unsigned long start,\n\t\t       unsigned long len, bool *mmap_changing)\n{\n\treturn __mcopy_atomic(dst_mm, start, 0, len, true, mmap_changing);\n}\n"], "filenames": ["fs/userfaultfd.c", "mm/userfaultfd.c"], "buggy_code_start_loc": [1363, 208], "buggy_code_end_loc": [1554, 469], "fixing_code_start_loc": [1364, 208], "fixing_code_end_loc": [1570, 466], "type": "CWE-863", "message": "The userfaultfd implementation in the Linux kernel before 4.19.7 mishandles access control for certain UFFDIO_ ioctl calls, as demonstrated by allowing local users to write data into holes in a tmpfs file (if the user has read-only access to that file, and that file contains holes), related to fs/userfaultfd.c and mm/userfaultfd.c.", "other": {"cve": {"id": "CVE-2018-18397", "sourceIdentifier": "cve@mitre.org", "published": "2018-12-12T10:29:00.240", "lastModified": "2020-08-24T17:37:01.140", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The userfaultfd implementation in the Linux kernel before 4.19.7 mishandles access control for certain UFFDIO_ ioctl calls, as demonstrated by allowing local users to write data into holes in a tmpfs file (if the user has read-only access to that file, and that file contains holes), related to fs/userfaultfd.c and mm/userfaultfd.c."}, {"lang": "es", "value": "La implementaci\u00f3n de userfaultfd en el kernel de Linux en versiones anteriores a la 4.17 gestiona de manera incorrecta para ciertas llamadas ioctl UFFDIO_, tal y como queda demostrado al permitir que usuarios locales escriban datos en huecos en un archivo tmpfs (si el usuario tiene acceso de solo lectura a dicho archivo que contiene huecos). Esto est\u00e1 relacionado con fs/userfaultfd.c y mm/userfaultfd.c."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:H/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:P/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-863"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.19.7", "matchCriteriaId": "9F04FA47-B72F-494A-A7C9-64B8A2B42F3D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:openshift_container_platform:3.11:*:*:*:*:*:*:*", "matchCriteriaId": "2F87326E-0B56-4356-A889-73D026DB1D4B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:virtualization_host:4.0:*:*:*:*:*:*:*", "matchCriteriaId": "BB28F9AF-3D06-4532-B397-96D7E4792503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_desktop:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "33C068A4-3780-4EAB-A937-6082DF847564"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "51EF4996-72F4-4FA4-814F-F5991E7A8318"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:7.4:*:*:*:*:*:*:*", "matchCriteriaId": "D99A687E-EAE6-417E-A88E-D0082BC194CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "B353CE99-D57C-465B-AAB0-73EF581127D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_eus:7.4:*:*:*:*:*:*:*", "matchCriteriaId": "9EC0D196-F7B8-4BDD-9050-779F7A7FBEE4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_eus:7.5:*:*:*:*:*:*:*", "matchCriteriaId": "A4E9DD8A-A68B-4A69-8B01-BFF92A2020A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_eus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "BF77CDCF-B9C9-427D-B2BF-36650FB2148C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.4:*:*:*:*:*:*:*", "matchCriteriaId": "D5F7E11E-FB34-4467-8919-2B6BEAABF665"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "B76AA310-FEC7-497F-AF04-C3EC1E76C4CC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_workstation:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "825ECE2D-E232-46E0-A047-074B34DB1E97"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.10:*:*:*:*:*:*:*", "matchCriteriaId": "07C312A0-CD2C-4B9C-B064-6409B25C278F"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=29ec90660d68bbdd69507c1c8b4e33aa299278b1", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHBA-2019:0327", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:0163", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:0202", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:0324", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:0831", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugs.chromium.org/p/project-zero/issues/detail?id=1700", "source": "cve@mitre.org", "tags": ["Exploit", "Issue Tracking", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.14.87", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.19.7", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/29ec90660d68bbdd69507c1c8b4e33aa299278b1", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3901-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3901-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3903-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3903-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/29ec90660d68bbdd69507c1c8b4e33aa299278b1"}}