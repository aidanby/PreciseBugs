{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tROUTE - implementation of the IP router.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tLinus Torvalds, <Linus.Torvalds@helsinki.fi>\n *\t\tAlexey Kuznetsov, <kuznet@ms2.inr.ac.ru>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tVerify area fixes.\n *\t\tAlan Cox\t:\tcli() protects routing changes\n *\t\tRui Oliveira\t:\tICMP routing table updates\n *\t\t(rco@di.uminho.pt)\tRouting table insertion and update\n *\t\tLinus Torvalds\t:\tRewrote bits to be sensible\n *\t\tAlan Cox\t:\tAdded BSD route gw semantics\n *\t\tAlan Cox\t:\tSuper /proc >4K\n *\t\tAlan Cox\t:\tMTU in route table\n *\t\tAlan Cox\t: \tMSS actually. Also added the window\n *\t\t\t\t\tclamper.\n *\t\tSam Lantinga\t:\tFixed route matching in rt_del()\n *\t\tAlan Cox\t:\tRouting cache support.\n *\t\tAlan Cox\t:\tRemoved compatibility cruft.\n *\t\tAlan Cox\t:\tRTF_REJECT support.\n *\t\tAlan Cox\t:\tTCP irtt support.\n *\t\tJonathan Naylor\t:\tAdded Metric support.\n *\tMiquel van Smoorenburg\t:\tBSD API fixes.\n *\tMiquel van Smoorenburg\t:\tMetrics.\n *\t\tAlan Cox\t:\tUse __u32 properly\n *\t\tAlan Cox\t:\tAligned routing errors more closely with BSD\n *\t\t\t\t\tour system is still very different.\n *\t\tAlan Cox\t:\tFaster /proc handling\n *\tAlexey Kuznetsov\t:\tMassive rework to support tree based routing,\n *\t\t\t\t\trouting caches and better behaviour.\n *\n *\t\tOlaf Erb\t:\tirtt wasn't being copied right.\n *\t\tBjorn Ekwall\t:\tKerneld route support.\n *\t\tAlan Cox\t:\tMulticast fixed (I hope)\n * \t\tPavel Krauz\t:\tLimited broadcast fixed\n *\t\tMike McLagan\t:\tRouting by source\n *\tAlexey Kuznetsov\t:\tEnd of old history. Split to fib.c and\n *\t\t\t\t\troute.c and rewritten from scratch.\n *\t\tAndi Kleen\t:\tLoad-limit warning messages.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year coma.\n *\tVitaly E. Lavrov\t:\tRace condition in ip_route_input_slow.\n *\tTobias Ringstrom\t:\tUninitialized res.type in ip_route_output_slow.\n *\tVladimir V. Ivanov\t:\tIP rule info (flowid) is really useful.\n *\t\tMarc Boucher\t:\trouting by fwmark\n *\tRobert Olsson\t\t:\tAdded rt_cache statistics\n *\tArnaldo C. Melo\t\t:\tConvert proc stuff to seq_file\n *\tEric Dumazet\t\t:\thashed spinlocks and rt_check_expire() fixes.\n * \tIlia Sotnikov\t\t:\tIgnore TOS on PMTUD and Redirect\n * \tIlia Sotnikov\t\t:\tRemoved TOS from hash calculations\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) \"IPv4: \" fmt\n\n#include <linux/module.h>\n#include <linux/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/string.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/proc_fs.h>\n#include <linux/init.h>\n#include <linux/skbuff.h>\n#include <linux/inetdevice.h>\n#include <linux/igmp.h>\n#include <linux/pkt_sched.h>\n#include <linux/mroute.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/random.h>\n#include <linux/rcupdate.h>\n#include <linux/times.h>\n#include <linux/slab.h>\n#include <linux/jhash.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/net_namespace.h>\n#include <net/protocol.h>\n#include <net/ip.h>\n#include <net/route.h>\n#include <net/inetpeer.h>\n#include <net/sock.h>\n#include <net/ip_fib.h>\n#include <net/arp.h>\n#include <net/tcp.h>\n#include <net/icmp.h>\n#include <net/xfrm.h>\n#include <net/lwtunnel.h>\n#include <net/netevent.h>\n#include <net/rtnetlink.h>\n#ifdef CONFIG_SYSCTL\n#include <linux/sysctl.h>\n#include <linux/kmemleak.h>\n#endif\n#include <net/secure_seq.h>\n#include <net/ip_tunnels.h>\n#include <net/l3mdev.h>\n\n#include \"fib_lookup.h\"\n\n#define RT_FL_TOS(oldflp4) \\\n\t((oldflp4)->flowi4_tos & (IPTOS_RT_MASK | RTO_ONLINK))\n\n#define RT_GC_TIMEOUT (300*HZ)\n\nstatic int ip_rt_max_size;\nstatic int ip_rt_redirect_number __read_mostly\t= 9;\nstatic int ip_rt_redirect_load __read_mostly\t= HZ / 50;\nstatic int ip_rt_redirect_silence __read_mostly\t= ((HZ / 50) << (9 + 1));\nstatic int ip_rt_error_cost __read_mostly\t= HZ;\nstatic int ip_rt_error_burst __read_mostly\t= 5 * HZ;\nstatic int ip_rt_mtu_expires __read_mostly\t= 10 * 60 * HZ;\nstatic int ip_rt_min_pmtu __read_mostly\t\t= 512 + 20 + 20;\nstatic int ip_rt_min_advmss __read_mostly\t= 256;\n\nstatic int ip_rt_gc_timeout __read_mostly\t= RT_GC_TIMEOUT;\n/*\n *\tInterface to generic destination cache.\n */\n\nstatic struct dst_entry *ipv4_dst_check(struct dst_entry *dst, u32 cookie);\nstatic unsigned int\t ipv4_default_advmss(const struct dst_entry *dst);\nstatic unsigned int\t ipv4_mtu(const struct dst_entry *dst);\nstatic struct dst_entry *ipv4_negative_advice(struct dst_entry *dst);\nstatic void\t\t ipv4_link_failure(struct sk_buff *skb);\nstatic void\t\t ip_rt_update_pmtu(struct dst_entry *dst, struct sock *sk,\n\t\t\t\t\t   struct sk_buff *skb, u32 mtu);\nstatic void\t\t ip_do_redirect(struct dst_entry *dst, struct sock *sk,\n\t\t\t\t\tstruct sk_buff *skb);\nstatic void\t\tipv4_dst_destroy(struct dst_entry *dst);\n\nstatic u32 *ipv4_cow_metrics(struct dst_entry *dst, unsigned long old)\n{\n\tWARN_ON(1);\n\treturn NULL;\n}\n\nstatic struct neighbour *ipv4_neigh_lookup(const struct dst_entry *dst,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   const void *daddr);\nstatic void ipv4_confirm_neigh(const struct dst_entry *dst, const void *daddr);\n\nstatic struct dst_ops ipv4_dst_ops = {\n\t.family =\t\tAF_INET,\n\t.check =\t\tipv4_dst_check,\n\t.default_advmss =\tipv4_default_advmss,\n\t.mtu =\t\t\tipv4_mtu,\n\t.cow_metrics =\t\tipv4_cow_metrics,\n\t.destroy =\t\tipv4_dst_destroy,\n\t.negative_advice =\tipv4_negative_advice,\n\t.link_failure =\t\tipv4_link_failure,\n\t.update_pmtu =\t\tip_rt_update_pmtu,\n\t.redirect =\t\tip_do_redirect,\n\t.local_out =\t\t__ip_local_out,\n\t.neigh_lookup =\t\tipv4_neigh_lookup,\n\t.confirm_neigh =\tipv4_confirm_neigh,\n};\n\n#define ECN_OR_COST(class)\tTC_PRIO_##class\n\nconst __u8 ip_tos2prio[16] = {\n\tTC_PRIO_BESTEFFORT,\n\tECN_OR_COST(BESTEFFORT),\n\tTC_PRIO_BESTEFFORT,\n\tECN_OR_COST(BESTEFFORT),\n\tTC_PRIO_BULK,\n\tECN_OR_COST(BULK),\n\tTC_PRIO_BULK,\n\tECN_OR_COST(BULK),\n\tTC_PRIO_INTERACTIVE,\n\tECN_OR_COST(INTERACTIVE),\n\tTC_PRIO_INTERACTIVE,\n\tECN_OR_COST(INTERACTIVE),\n\tTC_PRIO_INTERACTIVE_BULK,\n\tECN_OR_COST(INTERACTIVE_BULK),\n\tTC_PRIO_INTERACTIVE_BULK,\n\tECN_OR_COST(INTERACTIVE_BULK)\n};\nEXPORT_SYMBOL(ip_tos2prio);\n\nstatic DEFINE_PER_CPU(struct rt_cache_stat, rt_cache_stat);\n#define RT_CACHE_STAT_INC(field) raw_cpu_inc(rt_cache_stat.field)\n\n#ifdef CONFIG_PROC_FS\nstatic void *rt_cache_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tif (*pos)\n\t\treturn NULL;\n\treturn SEQ_START_TOKEN;\n}\n\nstatic void *rt_cache_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\treturn NULL;\n}\n\nstatic void rt_cache_seq_stop(struct seq_file *seq, void *v)\n{\n}\n\nstatic int rt_cache_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_printf(seq, \"%-127s\\n\",\n\t\t\t   \"Iface\\tDestination\\tGateway \\tFlags\\t\\tRefCnt\\tUse\\t\"\n\t\t\t   \"Metric\\tSource\\t\\tMTU\\tWindow\\tIRTT\\tTOS\\tHHRef\\t\"\n\t\t\t   \"HHUptod\\tSpecDst\");\n\treturn 0;\n}\n\nstatic const struct seq_operations rt_cache_seq_ops = {\n\t.start  = rt_cache_seq_start,\n\t.next   = rt_cache_seq_next,\n\t.stop   = rt_cache_seq_stop,\n\t.show   = rt_cache_seq_show,\n};\n\nstatic int rt_cache_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &rt_cache_seq_ops);\n}\n\nstatic const struct file_operations rt_cache_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open\t = rt_cache_seq_open,\n\t.read\t = seq_read,\n\t.llseek\t = seq_lseek,\n\t.release = seq_release,\n};\n\n\nstatic void *rt_cpu_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tint cpu;\n\n\tif (*pos == 0)\n\t\treturn SEQ_START_TOKEN;\n\n\tfor (cpu = *pos-1; cpu < nr_cpu_ids; ++cpu) {\n\t\tif (!cpu_possible(cpu))\n\t\t\tcontinue;\n\t\t*pos = cpu+1;\n\t\treturn &per_cpu(rt_cache_stat, cpu);\n\t}\n\treturn NULL;\n}\n\nstatic void *rt_cpu_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tint cpu;\n\n\tfor (cpu = *pos; cpu < nr_cpu_ids; ++cpu) {\n\t\tif (!cpu_possible(cpu))\n\t\t\tcontinue;\n\t\t*pos = cpu+1;\n\t\treturn &per_cpu(rt_cache_stat, cpu);\n\t}\n\treturn NULL;\n\n}\n\nstatic void rt_cpu_seq_stop(struct seq_file *seq, void *v)\n{\n\n}\n\nstatic int rt_cpu_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct rt_cache_stat *st = v;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_printf(seq, \"entries  in_hit in_slow_tot in_slow_mc in_no_route in_brd in_martian_dst in_martian_src  out_hit out_slow_tot out_slow_mc  gc_total gc_ignored gc_goal_miss gc_dst_overflow in_hlist_search out_hlist_search\\n\");\n\t\treturn 0;\n\t}\n\n\tseq_printf(seq,\"%08x  %08x %08x %08x %08x %08x %08x %08x \"\n\t\t   \" %08x %08x %08x %08x %08x %08x %08x %08x %08x \\n\",\n\t\t   dst_entries_get_slow(&ipv4_dst_ops),\n\t\t   0, /* st->in_hit */\n\t\t   st->in_slow_tot,\n\t\t   st->in_slow_mc,\n\t\t   st->in_no_route,\n\t\t   st->in_brd,\n\t\t   st->in_martian_dst,\n\t\t   st->in_martian_src,\n\n\t\t   0, /* st->out_hit */\n\t\t   st->out_slow_tot,\n\t\t   st->out_slow_mc,\n\n\t\t   0, /* st->gc_total */\n\t\t   0, /* st->gc_ignored */\n\t\t   0, /* st->gc_goal_miss */\n\t\t   0, /* st->gc_dst_overflow */\n\t\t   0, /* st->in_hlist_search */\n\t\t   0  /* st->out_hlist_search */\n\t\t);\n\treturn 0;\n}\n\nstatic const struct seq_operations rt_cpu_seq_ops = {\n\t.start  = rt_cpu_seq_start,\n\t.next   = rt_cpu_seq_next,\n\t.stop   = rt_cpu_seq_stop,\n\t.show   = rt_cpu_seq_show,\n};\n\n\nstatic int rt_cpu_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &rt_cpu_seq_ops);\n}\n\nstatic const struct file_operations rt_cpu_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open\t = rt_cpu_seq_open,\n\t.read\t = seq_read,\n\t.llseek\t = seq_lseek,\n\t.release = seq_release,\n};\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\nstatic int rt_acct_proc_show(struct seq_file *m, void *v)\n{\n\tstruct ip_rt_acct *dst, *src;\n\tunsigned int i, j;\n\n\tdst = kcalloc(256, sizeof(struct ip_rt_acct), GFP_KERNEL);\n\tif (!dst)\n\t\treturn -ENOMEM;\n\n\tfor_each_possible_cpu(i) {\n\t\tsrc = (struct ip_rt_acct *)per_cpu_ptr(ip_rt_acct, i);\n\t\tfor (j = 0; j < 256; j++) {\n\t\t\tdst[j].o_bytes   += src[j].o_bytes;\n\t\t\tdst[j].o_packets += src[j].o_packets;\n\t\t\tdst[j].i_bytes   += src[j].i_bytes;\n\t\t\tdst[j].i_packets += src[j].i_packets;\n\t\t}\n\t}\n\n\tseq_write(m, dst, 256 * sizeof(struct ip_rt_acct));\n\tkfree(dst);\n\treturn 0;\n}\n\nstatic int rt_acct_proc_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, rt_acct_proc_show, NULL);\n}\n\nstatic const struct file_operations rt_acct_proc_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= rt_acct_proc_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n#endif\n\nstatic int __net_init ip_rt_do_proc_init(struct net *net)\n{\n\tstruct proc_dir_entry *pde;\n\n\tpde = proc_create(\"rt_cache\", S_IRUGO, net->proc_net,\n\t\t\t  &rt_cache_seq_fops);\n\tif (!pde)\n\t\tgoto err1;\n\n\tpde = proc_create(\"rt_cache\", S_IRUGO,\n\t\t\t  net->proc_net_stat, &rt_cpu_seq_fops);\n\tif (!pde)\n\t\tgoto err2;\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tpde = proc_create(\"rt_acct\", 0, net->proc_net, &rt_acct_proc_fops);\n\tif (!pde)\n\t\tgoto err3;\n#endif\n\treturn 0;\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\nerr3:\n\tremove_proc_entry(\"rt_cache\", net->proc_net_stat);\n#endif\nerr2:\n\tremove_proc_entry(\"rt_cache\", net->proc_net);\nerr1:\n\treturn -ENOMEM;\n}\n\nstatic void __net_exit ip_rt_do_proc_exit(struct net *net)\n{\n\tremove_proc_entry(\"rt_cache\", net->proc_net_stat);\n\tremove_proc_entry(\"rt_cache\", net->proc_net);\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tremove_proc_entry(\"rt_acct\", net->proc_net);\n#endif\n}\n\nstatic struct pernet_operations ip_rt_proc_ops __net_initdata =  {\n\t.init = ip_rt_do_proc_init,\n\t.exit = ip_rt_do_proc_exit,\n};\n\nstatic int __init ip_rt_proc_init(void)\n{\n\treturn register_pernet_subsys(&ip_rt_proc_ops);\n}\n\n#else\nstatic inline int ip_rt_proc_init(void)\n{\n\treturn 0;\n}\n#endif /* CONFIG_PROC_FS */\n\nstatic inline bool rt_is_expired(const struct rtable *rth)\n{\n\treturn rth->rt_genid != rt_genid_ipv4(dev_net(rth->dst.dev));\n}\n\nvoid rt_cache_flush(struct net *net)\n{\n\trt_genid_bump_ipv4(net);\n}\n\nstatic struct neighbour *ipv4_neigh_lookup(const struct dst_entry *dst,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   const void *daddr)\n{\n\tstruct net_device *dev = dst->dev;\n\tconst __be32 *pkey = daddr;\n\tconst struct rtable *rt;\n\tstruct neighbour *n;\n\n\trt = (const struct rtable *) dst;\n\tif (rt->rt_gateway)\n\t\tpkey = (const __be32 *) &rt->rt_gateway;\n\telse if (skb)\n\t\tpkey = &ip_hdr(skb)->daddr;\n\n\tn = __ipv4_neigh_lookup(dev, *(__force u32 *)pkey);\n\tif (n)\n\t\treturn n;\n\treturn neigh_create(&arp_tbl, pkey, dev);\n}\n\nstatic void ipv4_confirm_neigh(const struct dst_entry *dst, const void *daddr)\n{\n\tstruct net_device *dev = dst->dev;\n\tconst __be32 *pkey = daddr;\n\tconst struct rtable *rt;\n\n\trt = (const struct rtable *)dst;\n\tif (rt->rt_gateway)\n\t\tpkey = (const __be32 *)&rt->rt_gateway;\n\telse if (!daddr ||\n\t\t (rt->rt_flags &\n\t\t  (RTCF_MULTICAST | RTCF_BROADCAST | RTCF_LOCAL)))\n\t\treturn;\n\n\t__ipv4_confirm_neigh(dev, *(__force u32 *)pkey);\n}\n\n#define IP_IDENTS_SZ 2048u\n\nstatic atomic_t *ip_idents __read_mostly;\nstatic u32 *ip_tstamps __read_mostly;\n\n/* In order to protect privacy, we add a perturbation to identifiers\n * if one generator is seldom used. This makes hard for an attacker\n * to infer how many packets were sent between two points in time.\n */\nu32 ip_idents_reserve(u32 hash, int segs)\n{\n\tu32 *p_tstamp = ip_tstamps + hash % IP_IDENTS_SZ;\n\tatomic_t *p_id = ip_idents + hash % IP_IDENTS_SZ;\n\tu32 old = ACCESS_ONCE(*p_tstamp);\n\tu32 now = (u32)jiffies;\n\tu32 new, delta = 0;\n\n\tif (old != now && cmpxchg(p_tstamp, old, now) == old)\n\t\tdelta = prandom_u32_max(now - old);\n\n\t/* Do not use atomic_add_return() as it makes UBSAN unhappy */\n\tdo {\n\t\told = (u32)atomic_read(p_id);\n\t\tnew = old + delta + segs;\n\t} while (atomic_cmpxchg(p_id, old, new) != old);\n\n\treturn new - segs;\n}\nEXPORT_SYMBOL(ip_idents_reserve);\n\nvoid __ip_select_ident(struct net *net, struct iphdr *iph, int segs)\n{\n\tstatic u32 ip_idents_hashrnd __read_mostly;\n\tu32 hash, id;\n\n\tnet_get_random_once(&ip_idents_hashrnd, sizeof(ip_idents_hashrnd));\n\n\thash = jhash_3words((__force u32)iph->daddr,\n\t\t\t    (__force u32)iph->saddr,\n\t\t\t    iph->protocol ^ net_hash_mix(net),\n\t\t\t    ip_idents_hashrnd);\n\tid = ip_idents_reserve(hash, segs);\n\tiph->id = htons(id);\n}\nEXPORT_SYMBOL(__ip_select_ident);\n\nstatic void __build_flow_key(const struct net *net, struct flowi4 *fl4,\n\t\t\t     const struct sock *sk,\n\t\t\t     const struct iphdr *iph,\n\t\t\t     int oif, u8 tos,\n\t\t\t     u8 prot, u32 mark, int flow_flags)\n{\n\tif (sk) {\n\t\tconst struct inet_sock *inet = inet_sk(sk);\n\n\t\toif = sk->sk_bound_dev_if;\n\t\tmark = sk->sk_mark;\n\t\ttos = RT_CONN_FLAGS(sk);\n\t\tprot = inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol;\n\t}\n\tflowi4_init_output(fl4, oif, mark, tos,\n\t\t\t   RT_SCOPE_UNIVERSE, prot,\n\t\t\t   flow_flags,\n\t\t\t   iph->daddr, iph->saddr, 0, 0,\n\t\t\t   sock_net_uid(net, sk));\n}\n\nstatic void build_skb_flow_key(struct flowi4 *fl4, const struct sk_buff *skb,\n\t\t\t       const struct sock *sk)\n{\n\tconst struct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tint oif = skb->dev->ifindex;\n\tu8 tos = RT_TOS(iph->tos);\n\tu8 prot = iph->protocol;\n\tu32 mark = skb->mark;\n\n\t__build_flow_key(net, fl4, sk, iph, oif, tos, prot, mark, 0);\n}\n\nstatic void build_sk_flow_key(struct flowi4 *fl4, const struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\tflowi4_init_output(fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t   inet_sk_flowi_flags(sk),\n\t\t\t   daddr, inet->inet_saddr, 0, 0, sk->sk_uid);\n\trcu_read_unlock();\n}\n\nstatic void ip_rt_build_flow_key(struct flowi4 *fl4, const struct sock *sk,\n\t\t\t\t const struct sk_buff *skb)\n{\n\tif (skb)\n\t\tbuild_skb_flow_key(fl4, skb, sk);\n\telse\n\t\tbuild_sk_flow_key(fl4, sk);\n}\n\nstatic DEFINE_SPINLOCK(fnhe_lock);\n\nstatic void fnhe_flush_routes(struct fib_nh_exception *fnhe)\n{\n\tstruct rtable *rt;\n\n\trt = rcu_dereference(fnhe->fnhe_rth_input);\n\tif (rt) {\n\t\tRCU_INIT_POINTER(fnhe->fnhe_rth_input, NULL);\n\t\tdst_dev_put(&rt->dst);\n\t\tdst_release(&rt->dst);\n\t}\n\trt = rcu_dereference(fnhe->fnhe_rth_output);\n\tif (rt) {\n\t\tRCU_INIT_POINTER(fnhe->fnhe_rth_output, NULL);\n\t\tdst_dev_put(&rt->dst);\n\t\tdst_release(&rt->dst);\n\t}\n}\n\nstatic struct fib_nh_exception *fnhe_oldest(struct fnhe_hash_bucket *hash)\n{\n\tstruct fib_nh_exception *fnhe, *oldest;\n\n\toldest = rcu_dereference(hash->chain);\n\tfor (fnhe = rcu_dereference(oldest->fnhe_next); fnhe;\n\t     fnhe = rcu_dereference(fnhe->fnhe_next)) {\n\t\tif (time_before(fnhe->fnhe_stamp, oldest->fnhe_stamp))\n\t\t\toldest = fnhe;\n\t}\n\tfnhe_flush_routes(oldest);\n\treturn oldest;\n}\n\nstatic inline u32 fnhe_hashfun(__be32 daddr)\n{\n\tstatic u32 fnhe_hashrnd __read_mostly;\n\tu32 hval;\n\n\tnet_get_random_once(&fnhe_hashrnd, sizeof(fnhe_hashrnd));\n\thval = jhash_1word((__force u32) daddr, fnhe_hashrnd);\n\treturn hash_32(hval, FNHE_HASH_SHIFT);\n}\n\nstatic void fill_route_from_fnhe(struct rtable *rt, struct fib_nh_exception *fnhe)\n{\n\trt->rt_pmtu = fnhe->fnhe_pmtu;\n\trt->dst.expires = fnhe->fnhe_expires;\n\n\tif (fnhe->fnhe_gw) {\n\t\trt->rt_flags |= RTCF_REDIRECTED;\n\t\trt->rt_gateway = fnhe->fnhe_gw;\n\t\trt->rt_uses_gateway = 1;\n\t}\n}\n\nstatic void update_or_create_fnhe(struct fib_nh *nh, __be32 daddr, __be32 gw,\n\t\t\t\t  u32 pmtu, unsigned long expires)\n{\n\tstruct fnhe_hash_bucket *hash;\n\tstruct fib_nh_exception *fnhe;\n\tstruct rtable *rt;\n\tunsigned int i;\n\tint depth;\n\tu32 hval = fnhe_hashfun(daddr);\n\n\tspin_lock_bh(&fnhe_lock);\n\n\thash = rcu_dereference(nh->nh_exceptions);\n\tif (!hash) {\n\t\thash = kzalloc(FNHE_HASH_SIZE * sizeof(*hash), GFP_ATOMIC);\n\t\tif (!hash)\n\t\t\tgoto out_unlock;\n\t\trcu_assign_pointer(nh->nh_exceptions, hash);\n\t}\n\n\thash += hval;\n\n\tdepth = 0;\n\tfor (fnhe = rcu_dereference(hash->chain); fnhe;\n\t     fnhe = rcu_dereference(fnhe->fnhe_next)) {\n\t\tif (fnhe->fnhe_daddr == daddr)\n\t\t\tbreak;\n\t\tdepth++;\n\t}\n\n\tif (fnhe) {\n\t\tif (gw)\n\t\t\tfnhe->fnhe_gw = gw;\n\t\tif (pmtu) {\n\t\t\tfnhe->fnhe_pmtu = pmtu;\n\t\t\tfnhe->fnhe_expires = max(1UL, expires);\n\t\t}\n\t\t/* Update all cached dsts too */\n\t\trt = rcu_dereference(fnhe->fnhe_rth_input);\n\t\tif (rt)\n\t\t\tfill_route_from_fnhe(rt, fnhe);\n\t\trt = rcu_dereference(fnhe->fnhe_rth_output);\n\t\tif (rt)\n\t\t\tfill_route_from_fnhe(rt, fnhe);\n\t} else {\n\t\tif (depth > FNHE_RECLAIM_DEPTH)\n\t\t\tfnhe = fnhe_oldest(hash);\n\t\telse {\n\t\t\tfnhe = kzalloc(sizeof(*fnhe), GFP_ATOMIC);\n\t\t\tif (!fnhe)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tfnhe->fnhe_next = hash->chain;\n\t\t\trcu_assign_pointer(hash->chain, fnhe);\n\t\t}\n\t\tfnhe->fnhe_genid = fnhe_genid(dev_net(nh->nh_dev));\n\t\tfnhe->fnhe_daddr = daddr;\n\t\tfnhe->fnhe_gw = gw;\n\t\tfnhe->fnhe_pmtu = pmtu;\n\t\tfnhe->fnhe_expires = expires;\n\n\t\t/* Exception created; mark the cached routes for the nexthop\n\t\t * stale, so anyone caching it rechecks if this exception\n\t\t * applies to them.\n\t\t */\n\t\trt = rcu_dereference(nh->nh_rth_input);\n\t\tif (rt)\n\t\t\trt->dst.obsolete = DST_OBSOLETE_KILL;\n\n\t\tfor_each_possible_cpu(i) {\n\t\t\tstruct rtable __rcu **prt;\n\t\t\tprt = per_cpu_ptr(nh->nh_pcpu_rth_output, i);\n\t\t\trt = rcu_dereference(*prt);\n\t\t\tif (rt)\n\t\t\t\trt->dst.obsolete = DST_OBSOLETE_KILL;\n\t\t}\n\t}\n\n\tfnhe->fnhe_stamp = jiffies;\n\nout_unlock:\n\tspin_unlock_bh(&fnhe_lock);\n}\n\nstatic void __ip_do_redirect(struct rtable *rt, struct sk_buff *skb, struct flowi4 *fl4,\n\t\t\t     bool kill_route)\n{\n\t__be32 new_gw = icmp_hdr(skb)->un.gateway;\n\t__be32 old_gw = ip_hdr(skb)->saddr;\n\tstruct net_device *dev = skb->dev;\n\tstruct in_device *in_dev;\n\tstruct fib_result res;\n\tstruct neighbour *n;\n\tstruct net *net;\n\n\tswitch (icmp_hdr(skb)->code & 7) {\n\tcase ICMP_REDIR_NET:\n\tcase ICMP_REDIR_NETTOS:\n\tcase ICMP_REDIR_HOST:\n\tcase ICMP_REDIR_HOSTTOS:\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (rt->rt_gateway != old_gw)\n\t\treturn;\n\n\tin_dev = __in_dev_get_rcu(dev);\n\tif (!in_dev)\n\t\treturn;\n\n\tnet = dev_net(dev);\n\tif (new_gw == old_gw || !IN_DEV_RX_REDIRECTS(in_dev) ||\n\t    ipv4_is_multicast(new_gw) || ipv4_is_lbcast(new_gw) ||\n\t    ipv4_is_zeronet(new_gw))\n\t\tgoto reject_redirect;\n\n\tif (!IN_DEV_SHARED_MEDIA(in_dev)) {\n\t\tif (!inet_addr_onlink(in_dev, new_gw, old_gw))\n\t\t\tgoto reject_redirect;\n\t\tif (IN_DEV_SEC_REDIRECTS(in_dev) && ip_fib_check_default(new_gw, dev))\n\t\t\tgoto reject_redirect;\n\t} else {\n\t\tif (inet_addr_type(net, new_gw) != RTN_UNICAST)\n\t\t\tgoto reject_redirect;\n\t}\n\n\tn = __ipv4_neigh_lookup(rt->dst.dev, new_gw);\n\tif (!n)\n\t\tn = neigh_create(&arp_tbl, &new_gw, rt->dst.dev);\n\tif (!IS_ERR(n)) {\n\t\tif (!(n->nud_state & NUD_VALID)) {\n\t\t\tneigh_event_send(n, NULL);\n\t\t} else {\n\t\t\tif (fib_lookup(net, fl4, &res, 0) == 0) {\n\t\t\t\tstruct fib_nh *nh = &FIB_RES_NH(res);\n\n\t\t\t\tupdate_or_create_fnhe(nh, fl4->daddr, new_gw,\n\t\t\t\t\t\t0, jiffies + ip_rt_gc_timeout);\n\t\t\t}\n\t\t\tif (kill_route)\n\t\t\t\trt->dst.obsolete = DST_OBSOLETE_KILL;\n\t\t\tcall_netevent_notifiers(NETEVENT_NEIGH_UPDATE, n);\n\t\t}\n\t\tneigh_release(n);\n\t}\n\treturn;\n\nreject_redirect:\n#ifdef CONFIG_IP_ROUTE_VERBOSE\n\tif (IN_DEV_LOG_MARTIANS(in_dev)) {\n\t\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\t\t__be32 daddr = iph->daddr;\n\t\t__be32 saddr = iph->saddr;\n\n\t\tnet_info_ratelimited(\"Redirect from %pI4 on %s about %pI4 ignored\\n\"\n\t\t\t\t     \"  Advised path = %pI4 -> %pI4\\n\",\n\t\t\t\t     &old_gw, dev->name, &new_gw,\n\t\t\t\t     &saddr, &daddr);\n\t}\n#endif\n\t;\n}\n\nstatic void ip_do_redirect(struct dst_entry *dst, struct sock *sk, struct sk_buff *skb)\n{\n\tstruct rtable *rt;\n\tstruct flowi4 fl4;\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct net *net = dev_net(skb->dev);\n\tint oif = skb->dev->ifindex;\n\tu8 tos = RT_TOS(iph->tos);\n\tu8 prot = iph->protocol;\n\tu32 mark = skb->mark;\n\n\trt = (struct rtable *) dst;\n\n\t__build_flow_key(net, &fl4, sk, iph, oif, tos, prot, mark, 0);\n\t__ip_do_redirect(rt, skb, &fl4, true);\n}\n\nstatic struct dst_entry *ipv4_negative_advice(struct dst_entry *dst)\n{\n\tstruct rtable *rt = (struct rtable *)dst;\n\tstruct dst_entry *ret = dst;\n\n\tif (rt) {\n\t\tif (dst->obsolete > 0) {\n\t\t\tip_rt_put(rt);\n\t\t\tret = NULL;\n\t\t} else if ((rt->rt_flags & RTCF_REDIRECTED) ||\n\t\t\t   rt->dst.expires) {\n\t\t\tip_rt_put(rt);\n\t\t\tret = NULL;\n\t\t}\n\t}\n\treturn ret;\n}\n\n/*\n * Algorithm:\n *\t1. The first ip_rt_redirect_number redirects are sent\n *\t   with exponential backoff, then we stop sending them at all,\n *\t   assuming that the host ignores our redirects.\n *\t2. If we did not see packets requiring redirects\n *\t   during ip_rt_redirect_silence, we assume that the host\n *\t   forgot redirected route and start to send redirects again.\n *\n * This algorithm is much cheaper and more intelligent than dumb load limiting\n * in icmp.c.\n *\n * NOTE. Do not forget to inhibit load limiting for redirects (redundant)\n * and \"frag. need\" (breaks PMTU discovery) in icmp.c.\n */\n\nvoid ip_rt_send_redirect(struct sk_buff *skb)\n{\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct in_device *in_dev;\n\tstruct inet_peer *peer;\n\tstruct net *net;\n\tint log_martians;\n\tint vif;\n\n\trcu_read_lock();\n\tin_dev = __in_dev_get_rcu(rt->dst.dev);\n\tif (!in_dev || !IN_DEV_TX_REDIRECTS(in_dev)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlog_martians = IN_DEV_LOG_MARTIANS(in_dev);\n\tvif = l3mdev_master_ifindex_rcu(rt->dst.dev);\n\trcu_read_unlock();\n\n\tnet = dev_net(rt->dst.dev);\n\tpeer = inet_getpeer_v4(net->ipv4.peers, ip_hdr(skb)->saddr, vif, 1);\n\tif (!peer) {\n\t\ticmp_send(skb, ICMP_REDIRECT, ICMP_REDIR_HOST,\n\t\t\t  rt_nexthop(rt, ip_hdr(skb)->daddr));\n\t\treturn;\n\t}\n\n\t/* No redirected packets during ip_rt_redirect_silence;\n\t * reset the algorithm.\n\t */\n\tif (time_after(jiffies, peer->rate_last + ip_rt_redirect_silence))\n\t\tpeer->rate_tokens = 0;\n\n\t/* Too many ignored redirects; do not send anything\n\t * set dst.rate_last to the last seen redirected packet.\n\t */\n\tif (peer->rate_tokens >= ip_rt_redirect_number) {\n\t\tpeer->rate_last = jiffies;\n\t\tgoto out_put_peer;\n\t}\n\n\t/* Check for load limit; set rate_last to the latest sent\n\t * redirect.\n\t */\n\tif (peer->rate_tokens == 0 ||\n\t    time_after(jiffies,\n\t\t       (peer->rate_last +\n\t\t\t(ip_rt_redirect_load << peer->rate_tokens)))) {\n\t\t__be32 gw = rt_nexthop(rt, ip_hdr(skb)->daddr);\n\n\t\ticmp_send(skb, ICMP_REDIRECT, ICMP_REDIR_HOST, gw);\n\t\tpeer->rate_last = jiffies;\n\t\t++peer->rate_tokens;\n#ifdef CONFIG_IP_ROUTE_VERBOSE\n\t\tif (log_martians &&\n\t\t    peer->rate_tokens == ip_rt_redirect_number)\n\t\t\tnet_warn_ratelimited(\"host %pI4/if%d ignores redirects for %pI4 to %pI4\\n\",\n\t\t\t\t\t     &ip_hdr(skb)->saddr, inet_iif(skb),\n\t\t\t\t\t     &ip_hdr(skb)->daddr, &gw);\n#endif\n\t}\nout_put_peer:\n\tinet_putpeer(peer);\n}\n\nstatic int ip_error(struct sk_buff *skb)\n{\n\tstruct in_device *in_dev = __in_dev_get_rcu(skb->dev);\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct inet_peer *peer;\n\tunsigned long now;\n\tstruct net *net;\n\tbool send;\n\tint code;\n\n\t/* IP on this device is disabled. */\n\tif (!in_dev)\n\t\tgoto out;\n\n\tnet = dev_net(rt->dst.dev);\n\tif (!IN_DEV_FORWARD(in_dev)) {\n\t\tswitch (rt->dst.error) {\n\t\tcase EHOSTUNREACH:\n\t\t\t__IP_INC_STATS(net, IPSTATS_MIB_INADDRERRORS);\n\t\t\tbreak;\n\n\t\tcase ENETUNREACH:\n\t\t\t__IP_INC_STATS(net, IPSTATS_MIB_INNOROUTES);\n\t\t\tbreak;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tswitch (rt->dst.error) {\n\tcase EINVAL:\n\tdefault:\n\t\tgoto out;\n\tcase EHOSTUNREACH:\n\t\tcode = ICMP_HOST_UNREACH;\n\t\tbreak;\n\tcase ENETUNREACH:\n\t\tcode = ICMP_NET_UNREACH;\n\t\t__IP_INC_STATS(net, IPSTATS_MIB_INNOROUTES);\n\t\tbreak;\n\tcase EACCES:\n\t\tcode = ICMP_PKT_FILTERED;\n\t\tbreak;\n\t}\n\n\tpeer = inet_getpeer_v4(net->ipv4.peers, ip_hdr(skb)->saddr,\n\t\t\t       l3mdev_master_ifindex(skb->dev), 1);\n\n\tsend = true;\n\tif (peer) {\n\t\tnow = jiffies;\n\t\tpeer->rate_tokens += now - peer->rate_last;\n\t\tif (peer->rate_tokens > ip_rt_error_burst)\n\t\t\tpeer->rate_tokens = ip_rt_error_burst;\n\t\tpeer->rate_last = now;\n\t\tif (peer->rate_tokens >= ip_rt_error_cost)\n\t\t\tpeer->rate_tokens -= ip_rt_error_cost;\n\t\telse\n\t\t\tsend = false;\n\t\tinet_putpeer(peer);\n\t}\n\tif (send)\n\t\ticmp_send(skb, ICMP_DEST_UNREACH, code, 0);\n\nout:\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic void __ip_rt_update_pmtu(struct rtable *rt, struct flowi4 *fl4, u32 mtu)\n{\n\tstruct dst_entry *dst = &rt->dst;\n\tstruct fib_result res;\n\n\tif (dst_metric_locked(dst, RTAX_MTU))\n\t\treturn;\n\n\tif (ipv4_mtu(dst) < mtu)\n\t\treturn;\n\n\tif (mtu < ip_rt_min_pmtu)\n\t\tmtu = ip_rt_min_pmtu;\n\n\tif (rt->rt_pmtu == mtu &&\n\t    time_before(jiffies, dst->expires - ip_rt_mtu_expires / 2))\n\t\treturn;\n\n\trcu_read_lock();\n\tif (fib_lookup(dev_net(dst->dev), fl4, &res, 0) == 0) {\n\t\tstruct fib_nh *nh = &FIB_RES_NH(res);\n\n\t\tupdate_or_create_fnhe(nh, fl4->daddr, 0, mtu,\n\t\t\t\t      jiffies + ip_rt_mtu_expires);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void ip_rt_update_pmtu(struct dst_entry *dst, struct sock *sk,\n\t\t\t      struct sk_buff *skb, u32 mtu)\n{\n\tstruct rtable *rt = (struct rtable *) dst;\n\tstruct flowi4 fl4;\n\n\tip_rt_build_flow_key(&fl4, sk, skb);\n\t__ip_rt_update_pmtu(rt, &fl4, mtu);\n}\n\nvoid ipv4_update_pmtu(struct sk_buff *skb, struct net *net, u32 mtu,\n\t\t      int oif, u32 mark, u8 protocol, int flow_flags)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\tif (!mark)\n\t\tmark = IP4_REPLY_MARK(net, skb->mark);\n\n\t__build_flow_key(net, &fl4, NULL, iph, oif,\n\t\t\t RT_TOS(iph->tos), protocol, mark, flow_flags);\n\trt = __ip_route_output_key(net, &fl4);\n\tif (!IS_ERR(rt)) {\n\t\t__ip_rt_update_pmtu(rt, &fl4, mtu);\n\t\tip_rt_put(rt);\n\t}\n}\nEXPORT_SYMBOL_GPL(ipv4_update_pmtu);\n\nstatic void __ipv4_sk_update_pmtu(struct sk_buff *skb, struct sock *sk, u32 mtu)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\t__build_flow_key(sock_net(sk), &fl4, sk, iph, 0, 0, 0, 0, 0);\n\n\tif (!fl4.flowi4_mark)\n\t\tfl4.flowi4_mark = IP4_REPLY_MARK(sock_net(sk), skb->mark);\n\n\trt = __ip_route_output_key(sock_net(sk), &fl4);\n\tif (!IS_ERR(rt)) {\n\t\t__ip_rt_update_pmtu(rt, &fl4, mtu);\n\t\tip_rt_put(rt);\n\t}\n}\n\nvoid ipv4_sk_update_pmtu(struct sk_buff *skb, struct sock *sk, u32 mtu)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tstruct dst_entry *odst = NULL;\n\tbool new = false;\n\tstruct net *net = sock_net(sk);\n\n\tbh_lock_sock(sk);\n\n\tif (!ip_sk_accept_pmtu(sk))\n\t\tgoto out;\n\n\todst = sk_dst_get(sk);\n\n\tif (sock_owned_by_user(sk) || !odst) {\n\t\t__ipv4_sk_update_pmtu(skb, sk, mtu);\n\t\tgoto out;\n\t}\n\n\t__build_flow_key(net, &fl4, sk, iph, 0, 0, 0, 0, 0);\n\n\trt = (struct rtable *)odst;\n\tif (odst->obsolete && !odst->ops->check(odst, 0)) {\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out;\n\n\t\tnew = true;\n\t}\n\n\t__ip_rt_update_pmtu((struct rtable *) rt->dst.path, &fl4, mtu);\n\n\tif (!dst_check(&rt->dst, 0)) {\n\t\tif (new)\n\t\t\tdst_release(&rt->dst);\n\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out;\n\n\t\tnew = true;\n\t}\n\n\tif (new)\n\t\tsk_dst_set(sk, &rt->dst);\n\nout:\n\tbh_unlock_sock(sk);\n\tdst_release(odst);\n}\nEXPORT_SYMBOL_GPL(ipv4_sk_update_pmtu);\n\nvoid ipv4_redirect(struct sk_buff *skb, struct net *net,\n\t\t   int oif, u32 mark, u8 protocol, int flow_flags)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\t__build_flow_key(net, &fl4, NULL, iph, oif,\n\t\t\t RT_TOS(iph->tos), protocol, mark, flow_flags);\n\trt = __ip_route_output_key(net, &fl4);\n\tif (!IS_ERR(rt)) {\n\t\t__ip_do_redirect(rt, skb, &fl4, false);\n\t\tip_rt_put(rt);\n\t}\n}\nEXPORT_SYMBOL_GPL(ipv4_redirect);\n\nvoid ipv4_sk_redirect(struct sk_buff *skb, struct sock *sk)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tstruct net *net = sock_net(sk);\n\n\t__build_flow_key(net, &fl4, sk, iph, 0, 0, 0, 0, 0);\n\trt = __ip_route_output_key(net, &fl4);\n\tif (!IS_ERR(rt)) {\n\t\t__ip_do_redirect(rt, skb, &fl4, false);\n\t\tip_rt_put(rt);\n\t}\n}\nEXPORT_SYMBOL_GPL(ipv4_sk_redirect);\n\nstatic struct dst_entry *ipv4_dst_check(struct dst_entry *dst, u32 cookie)\n{\n\tstruct rtable *rt = (struct rtable *) dst;\n\n\t/* All IPV4 dsts are created with ->obsolete set to the value\n\t * DST_OBSOLETE_FORCE_CHK which forces validation calls down\n\t * into this function always.\n\t *\n\t * When a PMTU/redirect information update invalidates a route,\n\t * this is indicated by setting obsolete to DST_OBSOLETE_KILL or\n\t * DST_OBSOLETE_DEAD by dst_free().\n\t */\n\tif (dst->obsolete != DST_OBSOLETE_FORCE_CHK || rt_is_expired(rt))\n\t\treturn NULL;\n\treturn dst;\n}\n\nstatic void ipv4_link_failure(struct sk_buff *skb)\n{\n\tstruct rtable *rt;\n\n\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_HOST_UNREACH, 0);\n\n\trt = skb_rtable(skb);\n\tif (rt)\n\t\tdst_set_expires(&rt->dst, 0);\n}\n\nstatic int ip_rt_bug(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tpr_debug(\"%s: %pI4 -> %pI4, %s\\n\",\n\t\t __func__, &ip_hdr(skb)->saddr, &ip_hdr(skb)->daddr,\n\t\t skb->dev ? skb->dev->name : \"?\");\n\tkfree_skb(skb);\n\tWARN_ON(1);\n\treturn 0;\n}\n\n/*\n   We do not cache source address of outgoing interface,\n   because it is used only by IP RR, TS and SRR options,\n   so that it out of fast path.\n\n   BTW remember: \"addr\" is allowed to be not aligned\n   in IP options!\n */\n\nvoid ip_rt_get_source(u8 *addr, struct sk_buff *skb, struct rtable *rt)\n{\n\t__be32 src;\n\n\tif (rt_is_output_route(rt))\n\t\tsrc = ip_hdr(skb)->saddr;\n\telse {\n\t\tstruct fib_result res;\n\t\tstruct flowi4 fl4;\n\t\tstruct iphdr *iph;\n\n\t\tiph = ip_hdr(skb);\n\n\t\tmemset(&fl4, 0, sizeof(fl4));\n\t\tfl4.daddr = iph->daddr;\n\t\tfl4.saddr = iph->saddr;\n\t\tfl4.flowi4_tos = RT_TOS(iph->tos);\n\t\tfl4.flowi4_oif = rt->dst.dev->ifindex;\n\t\tfl4.flowi4_iif = skb->dev->ifindex;\n\t\tfl4.flowi4_mark = skb->mark;\n\n\t\trcu_read_lock();\n\t\tif (fib_lookup(dev_net(rt->dst.dev), &fl4, &res, 0) == 0)\n\t\t\tsrc = FIB_RES_PREFSRC(dev_net(rt->dst.dev), res);\n\t\telse\n\t\t\tsrc = inet_select_addr(rt->dst.dev,\n\t\t\t\t\t       rt_nexthop(rt, iph->daddr),\n\t\t\t\t\t       RT_SCOPE_UNIVERSE);\n\t\trcu_read_unlock();\n\t}\n\tmemcpy(addr, &src, 4);\n}\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\nstatic void set_class_tag(struct rtable *rt, u32 tag)\n{\n\tif (!(rt->dst.tclassid & 0xFFFF))\n\t\trt->dst.tclassid |= tag & 0xFFFF;\n\tif (!(rt->dst.tclassid & 0xFFFF0000))\n\t\trt->dst.tclassid |= tag & 0xFFFF0000;\n}\n#endif\n\nstatic unsigned int ipv4_default_advmss(const struct dst_entry *dst)\n{\n\tunsigned int header_size = sizeof(struct tcphdr) + sizeof(struct iphdr);\n\tunsigned int advmss = max_t(unsigned int, dst->dev->mtu - header_size,\n\t\t\t\t    ip_rt_min_advmss);\n\n\treturn min(advmss, IPV4_MAX_PMTU - header_size);\n}\n\nstatic unsigned int ipv4_mtu(const struct dst_entry *dst)\n{\n\tconst struct rtable *rt = (const struct rtable *) dst;\n\tunsigned int mtu = rt->rt_pmtu;\n\n\tif (!mtu || time_after_eq(jiffies, rt->dst.expires))\n\t\tmtu = dst_metric_raw(dst, RTAX_MTU);\n\n\tif (mtu)\n\t\treturn mtu;\n\n\tmtu = READ_ONCE(dst->dev->mtu);\n\n\tif (unlikely(dst_metric_locked(dst, RTAX_MTU))) {\n\t\tif (rt->rt_uses_gateway && mtu > 576)\n\t\t\tmtu = 576;\n\t}\n\n\tmtu = min_t(unsigned int, mtu, IP_MAX_MTU);\n\n\treturn mtu - lwtunnel_headroom(dst->lwtstate, mtu);\n}\n\nstatic struct fib_nh_exception *find_exception(struct fib_nh *nh, __be32 daddr)\n{\n\tstruct fnhe_hash_bucket *hash = rcu_dereference(nh->nh_exceptions);\n\tstruct fib_nh_exception *fnhe;\n\tu32 hval;\n\n\tif (!hash)\n\t\treturn NULL;\n\n\thval = fnhe_hashfun(daddr);\n\n\tfor (fnhe = rcu_dereference(hash[hval].chain); fnhe;\n\t     fnhe = rcu_dereference(fnhe->fnhe_next)) {\n\t\tif (fnhe->fnhe_daddr == daddr)\n\t\t\treturn fnhe;\n\t}\n\treturn NULL;\n}\n\nstatic bool rt_bind_exception(struct rtable *rt, struct fib_nh_exception *fnhe,\n\t\t\t      __be32 daddr, const bool do_cache)\n{\n\tbool ret = false;\n\n\tspin_lock_bh(&fnhe_lock);\n\n\tif (daddr == fnhe->fnhe_daddr) {\n\t\tstruct rtable __rcu **porig;\n\t\tstruct rtable *orig;\n\t\tint genid = fnhe_genid(dev_net(rt->dst.dev));\n\n\t\tif (rt_is_input_route(rt))\n\t\t\tporig = &fnhe->fnhe_rth_input;\n\t\telse\n\t\t\tporig = &fnhe->fnhe_rth_output;\n\t\torig = rcu_dereference(*porig);\n\n\t\tif (fnhe->fnhe_genid != genid) {\n\t\t\tfnhe->fnhe_genid = genid;\n\t\t\tfnhe->fnhe_gw = 0;\n\t\t\tfnhe->fnhe_pmtu = 0;\n\t\t\tfnhe->fnhe_expires = 0;\n\t\t\tfnhe_flush_routes(fnhe);\n\t\t\torig = NULL;\n\t\t}\n\t\tfill_route_from_fnhe(rt, fnhe);\n\t\tif (!rt->rt_gateway)\n\t\t\trt->rt_gateway = daddr;\n\n\t\tif (do_cache) {\n\t\t\tdst_hold(&rt->dst);\n\t\t\trcu_assign_pointer(*porig, rt);\n\t\t\tif (orig) {\n\t\t\t\tdst_dev_put(&orig->dst);\n\t\t\t\tdst_release(&orig->dst);\n\t\t\t}\n\t\t\tret = true;\n\t\t}\n\n\t\tfnhe->fnhe_stamp = jiffies;\n\t}\n\tspin_unlock_bh(&fnhe_lock);\n\n\treturn ret;\n}\n\nstatic bool rt_cache_route(struct fib_nh *nh, struct rtable *rt)\n{\n\tstruct rtable *orig, *prev, **p;\n\tbool ret = true;\n\n\tif (rt_is_input_route(rt)) {\n\t\tp = (struct rtable **)&nh->nh_rth_input;\n\t} else {\n\t\tp = (struct rtable **)raw_cpu_ptr(nh->nh_pcpu_rth_output);\n\t}\n\torig = *p;\n\n\t/* hold dst before doing cmpxchg() to avoid race condition\n\t * on this dst\n\t */\n\tdst_hold(&rt->dst);\n\tprev = cmpxchg(p, orig, rt);\n\tif (prev == orig) {\n\t\tif (orig) {\n\t\t\tdst_dev_put(&orig->dst);\n\t\t\tdst_release(&orig->dst);\n\t\t}\n\t} else {\n\t\tdst_release(&rt->dst);\n\t\tret = false;\n\t}\n\n\treturn ret;\n}\n\nstruct uncached_list {\n\tspinlock_t\t\tlock;\n\tstruct list_head\thead;\n};\n\nstatic DEFINE_PER_CPU_ALIGNED(struct uncached_list, rt_uncached_list);\n\nstatic void rt_add_uncached_list(struct rtable *rt)\n{\n\tstruct uncached_list *ul = raw_cpu_ptr(&rt_uncached_list);\n\n\trt->rt_uncached_list = ul;\n\n\tspin_lock_bh(&ul->lock);\n\tlist_add_tail(&rt->rt_uncached, &ul->head);\n\tspin_unlock_bh(&ul->lock);\n}\n\nstatic void ipv4_dst_destroy(struct dst_entry *dst)\n{\n\tstruct dst_metrics *p = (struct dst_metrics *)DST_METRICS_PTR(dst);\n\tstruct rtable *rt = (struct rtable *) dst;\n\n\tif (p != &dst_default_metrics && atomic_dec_and_test(&p->refcnt))\n\t\tkfree(p);\n\n\tif (!list_empty(&rt->rt_uncached)) {\n\t\tstruct uncached_list *ul = rt->rt_uncached_list;\n\n\t\tspin_lock_bh(&ul->lock);\n\t\tlist_del(&rt->rt_uncached);\n\t\tspin_unlock_bh(&ul->lock);\n\t}\n}\n\nvoid rt_flush_dev(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\tstruct rtable *rt;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct uncached_list *ul = &per_cpu(rt_uncached_list, cpu);\n\n\t\tspin_lock_bh(&ul->lock);\n\t\tlist_for_each_entry(rt, &ul->head, rt_uncached) {\n\t\t\tif (rt->dst.dev != dev)\n\t\t\t\tcontinue;\n\t\t\trt->dst.dev = net->loopback_dev;\n\t\t\tdev_hold(rt->dst.dev);\n\t\t\tdev_put(dev);\n\t\t}\n\t\tspin_unlock_bh(&ul->lock);\n\t}\n}\n\nstatic bool rt_cache_valid(const struct rtable *rt)\n{\n\treturn\trt &&\n\t\trt->dst.obsolete == DST_OBSOLETE_FORCE_CHK &&\n\t\t!rt_is_expired(rt);\n}\n\nstatic void rt_set_nexthop(struct rtable *rt, __be32 daddr,\n\t\t\t   const struct fib_result *res,\n\t\t\t   struct fib_nh_exception *fnhe,\n\t\t\t   struct fib_info *fi, u16 type, u32 itag,\n\t\t\t   const bool do_cache)\n{\n\tbool cached = false;\n\n\tif (fi) {\n\t\tstruct fib_nh *nh = &FIB_RES_NH(*res);\n\n\t\tif (nh->nh_gw && nh->nh_scope == RT_SCOPE_LINK) {\n\t\t\trt->rt_gateway = nh->nh_gw;\n\t\t\trt->rt_uses_gateway = 1;\n\t\t}\n\t\tdst_init_metrics(&rt->dst, fi->fib_metrics->metrics, true);\n\t\tif (fi->fib_metrics != &dst_default_metrics) {\n\t\t\trt->dst._metrics |= DST_METRICS_REFCOUNTED;\n\t\t\tatomic_inc(&fi->fib_metrics->refcnt);\n\t\t}\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\t\trt->dst.tclassid = nh->nh_tclassid;\n#endif\n\t\trt->dst.lwtstate = lwtstate_get(nh->nh_lwtstate);\n\t\tif (unlikely(fnhe))\n\t\t\tcached = rt_bind_exception(rt, fnhe, daddr, do_cache);\n\t\telse if (do_cache)\n\t\t\tcached = rt_cache_route(nh, rt);\n\t\tif (unlikely(!cached)) {\n\t\t\t/* Routes we intend to cache in nexthop exception or\n\t\t\t * FIB nexthop have the DST_NOCACHE bit clear.\n\t\t\t * However, if we are unsuccessful at storing this\n\t\t\t * route into the cache we really need to set it.\n\t\t\t */\n\t\t\tif (!rt->rt_gateway)\n\t\t\t\trt->rt_gateway = daddr;\n\t\t\trt_add_uncached_list(rt);\n\t\t}\n\t} else\n\t\trt_add_uncached_list(rt);\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\n#ifdef CONFIG_IP_MULTIPLE_TABLES\n\tset_class_tag(rt, res->tclassid);\n#endif\n\tset_class_tag(rt, itag);\n#endif\n}\n\nstruct rtable *rt_dst_alloc(struct net_device *dev,\n\t\t\t    unsigned int flags, u16 type,\n\t\t\t    bool nopolicy, bool noxfrm, bool will_cache)\n{\n\tstruct rtable *rt;\n\n\trt = dst_alloc(&ipv4_dst_ops, dev, 1, DST_OBSOLETE_FORCE_CHK,\n\t\t       (will_cache ? 0 : DST_HOST) |\n\t\t       (nopolicy ? DST_NOPOLICY : 0) |\n\t\t       (noxfrm ? DST_NOXFRM : 0));\n\n\tif (rt) {\n\t\trt->rt_genid = rt_genid_ipv4(dev_net(dev));\n\t\trt->rt_flags = flags;\n\t\trt->rt_type = type;\n\t\trt->rt_is_input = 0;\n\t\trt->rt_iif = 0;\n\t\trt->rt_pmtu = 0;\n\t\trt->rt_gateway = 0;\n\t\trt->rt_uses_gateway = 0;\n\t\trt->rt_table_id = 0;\n\t\tINIT_LIST_HEAD(&rt->rt_uncached);\n\n\t\trt->dst.output = ip_output;\n\t\tif (flags & RTCF_LOCAL)\n\t\t\trt->dst.input = ip_local_deliver;\n\t}\n\n\treturn rt;\n}\nEXPORT_SYMBOL(rt_dst_alloc);\n\n/* called in rcu_read_lock() section */\nstatic int ip_route_input_mc(struct sk_buff *skb, __be32 daddr, __be32 saddr,\n\t\t\t\tu8 tos, struct net_device *dev, int our)\n{\n\tstruct rtable *rth;\n\tstruct in_device *in_dev = __in_dev_get_rcu(dev);\n\tunsigned int flags = RTCF_MULTICAST;\n\tu32 itag = 0;\n\tint err;\n\n\t/* Primary sanity checks. */\n\n\tif (!in_dev)\n\t\treturn -EINVAL;\n\n\tif (ipv4_is_multicast(saddr) || ipv4_is_lbcast(saddr) ||\n\t    skb->protocol != htons(ETH_P_IP))\n\t\tgoto e_inval;\n\n\tif (ipv4_is_loopback(saddr) && !IN_DEV_ROUTE_LOCALNET(in_dev))\n\t\tgoto e_inval;\n\n\tif (ipv4_is_zeronet(saddr)) {\n\t\tif (!ipv4_is_local_multicast(daddr))\n\t\t\tgoto e_inval;\n\t} else {\n\t\terr = fib_validate_source(skb, saddr, 0, tos, 0, dev,\n\t\t\t\t\t  in_dev, &itag);\n\t\tif (err < 0)\n\t\t\tgoto e_err;\n\t}\n\tif (our)\n\t\tflags |= RTCF_LOCAL;\n\n\trth = rt_dst_alloc(dev_net(dev)->loopback_dev, flags, RTN_MULTICAST,\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOPOLICY), false, false);\n\tif (!rth)\n\t\tgoto e_nobufs;\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\trth->dst.tclassid = itag;\n#endif\n\trth->dst.output = ip_rt_bug;\n\trth->rt_is_input= 1;\n\n#ifdef CONFIG_IP_MROUTE\n\tif (!ipv4_is_local_multicast(daddr) && IN_DEV_MFORWARD(in_dev))\n\t\trth->dst.input = ip_mr_input;\n#endif\n\tRT_CACHE_STAT_INC(in_slow_mc);\n\n\tskb_dst_set(skb, &rth->dst);\n\treturn 0;\n\ne_nobufs:\n\treturn -ENOBUFS;\ne_inval:\n\treturn -EINVAL;\ne_err:\n\treturn err;\n}\n\n\nstatic void ip_handle_martian_source(struct net_device *dev,\n\t\t\t\t     struct in_device *in_dev,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     __be32 daddr,\n\t\t\t\t     __be32 saddr)\n{\n\tRT_CACHE_STAT_INC(in_martian_src);\n#ifdef CONFIG_IP_ROUTE_VERBOSE\n\tif (IN_DEV_LOG_MARTIANS(in_dev) && net_ratelimit()) {\n\t\t/*\n\t\t *\tRFC1812 recommendation, if source is martian,\n\t\t *\tthe only hint is MAC header.\n\t\t */\n\t\tpr_warn(\"martian source %pI4 from %pI4, on dev %s\\n\",\n\t\t\t&daddr, &saddr, dev->name);\n\t\tif (dev->hard_header_len && skb_mac_header_was_set(skb)) {\n\t\t\tprint_hex_dump(KERN_WARNING, \"ll header: \",\n\t\t\t\t       DUMP_PREFIX_OFFSET, 16, 1,\n\t\t\t\t       skb_mac_header(skb),\n\t\t\t\t       dev->hard_header_len, true);\n\t\t}\n\t}\n#endif\n}\n\nstatic void ip_del_fnhe(struct fib_nh *nh, __be32 daddr)\n{\n\tstruct fnhe_hash_bucket *hash;\n\tstruct fib_nh_exception *fnhe, __rcu **fnhe_p;\n\tu32 hval = fnhe_hashfun(daddr);\n\n\tspin_lock_bh(&fnhe_lock);\n\n\thash = rcu_dereference_protected(nh->nh_exceptions,\n\t\t\t\t\t lockdep_is_held(&fnhe_lock));\n\thash += hval;\n\n\tfnhe_p = &hash->chain;\n\tfnhe = rcu_dereference_protected(*fnhe_p, lockdep_is_held(&fnhe_lock));\n\twhile (fnhe) {\n\t\tif (fnhe->fnhe_daddr == daddr) {\n\t\t\trcu_assign_pointer(*fnhe_p, rcu_dereference_protected(\n\t\t\t\tfnhe->fnhe_next, lockdep_is_held(&fnhe_lock)));\n\t\t\tfnhe_flush_routes(fnhe);\n\t\t\tkfree_rcu(fnhe, rcu);\n\t\t\tbreak;\n\t\t}\n\t\tfnhe_p = &fnhe->fnhe_next;\n\t\tfnhe = rcu_dereference_protected(fnhe->fnhe_next,\n\t\t\t\t\t\t lockdep_is_held(&fnhe_lock));\n\t}\n\n\tspin_unlock_bh(&fnhe_lock);\n}\n\nstatic void set_lwt_redirect(struct rtable *rth)\n{\n\tif (lwtunnel_output_redirect(rth->dst.lwtstate)) {\n\t\trth->dst.lwtstate->orig_output = rth->dst.output;\n\t\trth->dst.output = lwtunnel_output;\n\t}\n\n\tif (lwtunnel_input_redirect(rth->dst.lwtstate)) {\n\t\trth->dst.lwtstate->orig_input = rth->dst.input;\n\t\trth->dst.input = lwtunnel_input;\n\t}\n}\n\n/* called in rcu_read_lock() section */\nstatic int __mkroute_input(struct sk_buff *skb,\n\t\t\t   const struct fib_result *res,\n\t\t\t   struct in_device *in_dev,\n\t\t\t   __be32 daddr, __be32 saddr, u32 tos)\n{\n\tstruct fib_nh_exception *fnhe;\n\tstruct rtable *rth;\n\tint err;\n\tstruct in_device *out_dev;\n\tbool do_cache;\n\tu32 itag = 0;\n\n\t/* get a working reference to the output device */\n\tout_dev = __in_dev_get_rcu(FIB_RES_DEV(*res));\n\tif (!out_dev) {\n\t\tnet_crit_ratelimited(\"Bug in ip_route_input_slow(). Please report.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = fib_validate_source(skb, saddr, daddr, tos, FIB_RES_OIF(*res),\n\t\t\t\t  in_dev->dev, in_dev, &itag);\n\tif (err < 0) {\n\t\tip_handle_martian_source(in_dev->dev, in_dev, skb, daddr,\n\t\t\t\t\t saddr);\n\n\t\tgoto cleanup;\n\t}\n\n\tdo_cache = res->fi && !itag;\n\tif (out_dev == in_dev && err && IN_DEV_TX_REDIRECTS(out_dev) &&\n\t    skb->protocol == htons(ETH_P_IP) &&\n\t    (IN_DEV_SHARED_MEDIA(out_dev) ||\n\t     inet_addr_onlink(out_dev, saddr, FIB_RES_GW(*res))))\n\t\tIPCB(skb)->flags |= IPSKB_DOREDIRECT;\n\n\tif (skb->protocol != htons(ETH_P_IP)) {\n\t\t/* Not IP (i.e. ARP). Do not create route, if it is\n\t\t * invalid for proxy arp. DNAT routes are always valid.\n\t\t *\n\t\t * Proxy arp feature have been extended to allow, ARP\n\t\t * replies back to the same interface, to support\n\t\t * Private VLAN switch technologies. See arp.c.\n\t\t */\n\t\tif (out_dev == in_dev &&\n\t\t    IN_DEV_PROXY_ARP_PVLAN(in_dev) == 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\tfnhe = find_exception(&FIB_RES_NH(*res), daddr);\n\tif (do_cache) {\n\t\tif (fnhe) {\n\t\t\trth = rcu_dereference(fnhe->fnhe_rth_input);\n\t\t\tif (rth && rth->dst.expires &&\n\t\t\t    time_after(jiffies, rth->dst.expires)) {\n\t\t\t\tip_del_fnhe(&FIB_RES_NH(*res), daddr);\n\t\t\t\tfnhe = NULL;\n\t\t\t} else {\n\t\t\t\tgoto rt_cache;\n\t\t\t}\n\t\t}\n\n\t\trth = rcu_dereference(FIB_RES_NH(*res).nh_rth_input);\n\nrt_cache:\n\t\tif (rt_cache_valid(rth)) {\n\t\t\tskb_dst_set_noref(skb, &rth->dst);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\trth = rt_dst_alloc(out_dev->dev, 0, res->type,\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOPOLICY),\n\t\t\t   IN_DEV_CONF_GET(out_dev, NOXFRM), do_cache);\n\tif (!rth) {\n\t\terr = -ENOBUFS;\n\t\tgoto cleanup;\n\t}\n\n\trth->rt_is_input = 1;\n\tif (res->table)\n\t\trth->rt_table_id = res->table->tb_id;\n\tRT_CACHE_STAT_INC(in_slow_tot);\n\n\trth->dst.input = ip_forward;\n\n\trt_set_nexthop(rth, daddr, res, fnhe, res->fi, res->type, itag,\n\t\t       do_cache);\n\tset_lwt_redirect(rth);\n\tskb_dst_set(skb, &rth->dst);\nout:\n\terr = 0;\n cleanup:\n\treturn err;\n}\n\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n/* To make ICMP packets follow the right flow, the multipath hash is\n * calculated from the inner IP addresses.\n */\nstatic void ip_multipath_l3_keys(const struct sk_buff *skb,\n\t\t\t\t struct flow_keys *hash_keys)\n{\n\tconst struct iphdr *outer_iph = ip_hdr(skb);\n\tconst struct iphdr *inner_iph;\n\tconst struct icmphdr *icmph;\n\tstruct iphdr _inner_iph;\n\tstruct icmphdr _icmph;\n\n\thash_keys->addrs.v4addrs.src = outer_iph->saddr;\n\thash_keys->addrs.v4addrs.dst = outer_iph->daddr;\n\tif (likely(outer_iph->protocol != IPPROTO_ICMP))\n\t\treturn;\n\n\tif (unlikely((outer_iph->frag_off & htons(IP_OFFSET)) != 0))\n\t\treturn;\n\n\ticmph = skb_header_pointer(skb, outer_iph->ihl * 4, sizeof(_icmph),\n\t\t\t\t   &_icmph);\n\tif (!icmph)\n\t\treturn;\n\n\tif (icmph->type != ICMP_DEST_UNREACH &&\n\t    icmph->type != ICMP_REDIRECT &&\n\t    icmph->type != ICMP_TIME_EXCEEDED &&\n\t    icmph->type != ICMP_PARAMETERPROB)\n\t\treturn;\n\n\tinner_iph = skb_header_pointer(skb,\n\t\t\t\t       outer_iph->ihl * 4 + sizeof(_icmph),\n\t\t\t\t       sizeof(_inner_iph), &_inner_iph);\n\tif (!inner_iph)\n\t\treturn;\n\thash_keys->addrs.v4addrs.src = inner_iph->saddr;\n\thash_keys->addrs.v4addrs.dst = inner_iph->daddr;\n}\n\n/* if skb is set it will be used and fl4 can be NULL */\nint fib_multipath_hash(const struct fib_info *fi, const struct flowi4 *fl4,\n\t\t       const struct sk_buff *skb)\n{\n\tstruct net *net = fi->fib_net;\n\tstruct flow_keys hash_keys;\n\tu32 mhash;\n\n\tswitch (net->ipv4.sysctl_fib_multipath_hash_policy) {\n\tcase 0:\n\t\tmemset(&hash_keys, 0, sizeof(hash_keys));\n\t\thash_keys.control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t\tif (skb) {\n\t\t\tip_multipath_l3_keys(skb, &hash_keys);\n\t\t} else {\n\t\t\thash_keys.addrs.v4addrs.src = fl4->saddr;\n\t\t\thash_keys.addrs.v4addrs.dst = fl4->daddr;\n\t\t}\n\t\tbreak;\n\tcase 1:\n\t\t/* skb is currently provided only when forwarding */\n\t\tif (skb) {\n\t\t\tunsigned int flag = FLOW_DISSECTOR_F_STOP_AT_ENCAP;\n\t\t\tstruct flow_keys keys;\n\n\t\t\t/* short-circuit if we already have L4 hash present */\n\t\t\tif (skb->l4_hash)\n\t\t\t\treturn skb_get_hash_raw(skb) >> 1;\n\t\t\tmemset(&hash_keys, 0, sizeof(hash_keys));\n\t\t\tskb_flow_dissect_flow_keys(skb, &keys, flag);\n\t\t\thash_keys.addrs.v4addrs.src = keys.addrs.v4addrs.src;\n\t\t\thash_keys.addrs.v4addrs.dst = keys.addrs.v4addrs.dst;\n\t\t\thash_keys.ports.src = keys.ports.src;\n\t\t\thash_keys.ports.dst = keys.ports.dst;\n\t\t\thash_keys.basic.ip_proto = keys.basic.ip_proto;\n\t\t} else {\n\t\t\tmemset(&hash_keys, 0, sizeof(hash_keys));\n\t\t\thash_keys.control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t\t\thash_keys.addrs.v4addrs.src = fl4->saddr;\n\t\t\thash_keys.addrs.v4addrs.dst = fl4->daddr;\n\t\t\thash_keys.ports.src = fl4->fl4_sport;\n\t\t\thash_keys.ports.dst = fl4->fl4_dport;\n\t\t\thash_keys.basic.ip_proto = fl4->flowi4_proto;\n\t\t}\n\t\tbreak;\n\t}\n\tmhash = flow_hash_from_keys(&hash_keys);\n\n\treturn mhash >> 1;\n}\nEXPORT_SYMBOL_GPL(fib_multipath_hash);\n#endif /* CONFIG_IP_ROUTE_MULTIPATH */\n\nstatic int ip_mkroute_input(struct sk_buff *skb,\n\t\t\t    struct fib_result *res,\n\t\t\t    struct in_device *in_dev,\n\t\t\t    __be32 daddr, __be32 saddr, u32 tos)\n{\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\tif (res->fi && res->fi->fib_nhs > 1) {\n\t\tint h = fib_multipath_hash(res->fi, NULL, skb);\n\n\t\tfib_select_multipath(res, h);\n\t}\n#endif\n\n\t/* create a routing cache entry */\n\treturn __mkroute_input(skb, res, in_dev, daddr, saddr, tos);\n}\n\n/*\n *\tNOTE. We drop all the packets that has local source\n *\taddresses, because every properly looped back packet\n *\tmust have correct destination already attached by output routine.\n *\n *\tSuch approach solves two big problems:\n *\t1. Not simplex devices are handled properly.\n *\t2. IP spoofing attempts are filtered with 100% of guarantee.\n *\tcalled with rcu_read_lock()\n */\n\nstatic int ip_route_input_slow(struct sk_buff *skb, __be32 daddr, __be32 saddr,\n\t\t\t       u8 tos, struct net_device *dev,\n\t\t\t       struct fib_result *res)\n{\n\tstruct in_device *in_dev = __in_dev_get_rcu(dev);\n\tstruct ip_tunnel_info *tun_info;\n\tstruct flowi4\tfl4;\n\tunsigned int\tflags = 0;\n\tu32\t\titag = 0;\n\tstruct rtable\t*rth;\n\tint\t\terr = -EINVAL;\n\tstruct net    *net = dev_net(dev);\n\tbool do_cache;\n\n\t/* IP on this device is disabled. */\n\n\tif (!in_dev)\n\t\tgoto out;\n\n\t/* Check for the most weird martians, which can be not detected\n\t   by fib_lookup.\n\t */\n\n\ttun_info = skb_tunnel_info(skb);\n\tif (tun_info && !(tun_info->mode & IP_TUNNEL_INFO_TX))\n\t\tfl4.flowi4_tun_key.tun_id = tun_info->key.tun_id;\n\telse\n\t\tfl4.flowi4_tun_key.tun_id = 0;\n\tskb_dst_drop(skb);\n\n\tif (ipv4_is_multicast(saddr) || ipv4_is_lbcast(saddr))\n\t\tgoto martian_source;\n\n\tres->fi = NULL;\n\tres->table = NULL;\n\tif (ipv4_is_lbcast(daddr) || (saddr == 0 && daddr == 0))\n\t\tgoto brd_input;\n\n\t/* Accept zero addresses only to limited broadcast;\n\t * I even do not know to fix it or not. Waiting for complains :-)\n\t */\n\tif (ipv4_is_zeronet(saddr))\n\t\tgoto martian_source;\n\n\tif (ipv4_is_zeronet(daddr))\n\t\tgoto martian_destination;\n\n\t/* Following code try to avoid calling IN_DEV_NET_ROUTE_LOCALNET(),\n\t * and call it once if daddr or/and saddr are loopback addresses\n\t */\n\tif (ipv4_is_loopback(daddr)) {\n\t\tif (!IN_DEV_NET_ROUTE_LOCALNET(in_dev, net))\n\t\t\tgoto martian_destination;\n\t} else if (ipv4_is_loopback(saddr)) {\n\t\tif (!IN_DEV_NET_ROUTE_LOCALNET(in_dev, net))\n\t\t\tgoto martian_source;\n\t}\n\n\t/*\n\t *\tNow we are ready to route packet.\n\t */\n\tfl4.flowi4_oif = 0;\n\tfl4.flowi4_iif = dev->ifindex;\n\tfl4.flowi4_mark = skb->mark;\n\tfl4.flowi4_tos = tos;\n\tfl4.flowi4_scope = RT_SCOPE_UNIVERSE;\n\tfl4.flowi4_flags = 0;\n\tfl4.daddr = daddr;\n\tfl4.saddr = saddr;\n\tfl4.flowi4_uid = sock_net_uid(net, NULL);\n\terr = fib_lookup(net, &fl4, res, 0);\n\tif (err != 0) {\n\t\tif (!IN_DEV_FORWARD(in_dev))\n\t\t\terr = -EHOSTUNREACH;\n\t\tgoto no_route;\n\t}\n\n\tif (res->type == RTN_BROADCAST)\n\t\tgoto brd_input;\n\n\tif (res->type == RTN_LOCAL) {\n\t\terr = fib_validate_source(skb, saddr, daddr, tos,\n\t\t\t\t\t  0, dev, in_dev, &itag);\n\t\tif (err < 0)\n\t\t\tgoto martian_source;\n\t\tgoto local_input;\n\t}\n\n\tif (!IN_DEV_FORWARD(in_dev)) {\n\t\terr = -EHOSTUNREACH;\n\t\tgoto no_route;\n\t}\n\tif (res->type != RTN_UNICAST)\n\t\tgoto martian_destination;\n\n\terr = ip_mkroute_input(skb, res, in_dev, daddr, saddr, tos);\nout:\treturn err;\n\nbrd_input:\n\tif (skb->protocol != htons(ETH_P_IP))\n\t\tgoto e_inval;\n\n\tif (!ipv4_is_zeronet(saddr)) {\n\t\terr = fib_validate_source(skb, saddr, 0, tos, 0, dev,\n\t\t\t\t\t  in_dev, &itag);\n\t\tif (err < 0)\n\t\t\tgoto martian_source;\n\t}\n\tflags |= RTCF_BROADCAST;\n\tres->type = RTN_BROADCAST;\n\tRT_CACHE_STAT_INC(in_brd);\n\nlocal_input:\n\tdo_cache = false;\n\tif (res->fi) {\n\t\tif (!itag) {\n\t\t\trth = rcu_dereference(FIB_RES_NH(*res).nh_rth_input);\n\t\t\tif (rt_cache_valid(rth)) {\n\t\t\t\tskb_dst_set_noref(skb, &rth->dst);\n\t\t\t\terr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdo_cache = true;\n\t\t}\n\t}\n\n\trth = rt_dst_alloc(l3mdev_master_dev_rcu(dev) ? : net->loopback_dev,\n\t\t\t   flags | RTCF_LOCAL, res->type,\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOPOLICY), false, do_cache);\n\tif (!rth)\n\t\tgoto e_nobufs;\n\n\trth->dst.output= ip_rt_bug;\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\trth->dst.tclassid = itag;\n#endif\n\trth->rt_is_input = 1;\n\tif (res->table)\n\t\trth->rt_table_id = res->table->tb_id;\n\n\tRT_CACHE_STAT_INC(in_slow_tot);\n\tif (res->type == RTN_UNREACHABLE) {\n\t\trth->dst.input= ip_error;\n\t\trth->dst.error= -err;\n\t\trth->rt_flags \t&= ~RTCF_LOCAL;\n\t}\n\n\tif (do_cache) {\n\t\tstruct fib_nh *nh = &FIB_RES_NH(*res);\n\n\t\trth->dst.lwtstate = lwtstate_get(nh->nh_lwtstate);\n\t\tif (lwtunnel_input_redirect(rth->dst.lwtstate)) {\n\t\t\tWARN_ON(rth->dst.input == lwtunnel_input);\n\t\t\trth->dst.lwtstate->orig_input = rth->dst.input;\n\t\t\trth->dst.input = lwtunnel_input;\n\t\t}\n\n\t\tif (unlikely(!rt_cache_route(nh, rth)))\n\t\t\trt_add_uncached_list(rth);\n\t}\n\tskb_dst_set(skb, &rth->dst);\n\terr = 0;\n\tgoto out;\n\nno_route:\n\tRT_CACHE_STAT_INC(in_no_route);\n\tres->type = RTN_UNREACHABLE;\n\tres->fi = NULL;\n\tres->table = NULL;\n\tgoto local_input;\n\n\t/*\n\t *\tDo not cache martian addresses: they should be logged (RFC1812)\n\t */\nmartian_destination:\n\tRT_CACHE_STAT_INC(in_martian_dst);\n#ifdef CONFIG_IP_ROUTE_VERBOSE\n\tif (IN_DEV_LOG_MARTIANS(in_dev))\n\t\tnet_warn_ratelimited(\"martian destination %pI4 from %pI4, dev %s\\n\",\n\t\t\t\t     &daddr, &saddr, dev->name);\n#endif\n\ne_inval:\n\terr = -EINVAL;\n\tgoto out;\n\ne_nobufs:\n\terr = -ENOBUFS;\n\tgoto out;\n\nmartian_source:\n\tip_handle_martian_source(dev, in_dev, skb, daddr, saddr);\n\tgoto out;\n}\n\nint ip_route_input_noref(struct sk_buff *skb, __be32 daddr, __be32 saddr,\n\t\t\t u8 tos, struct net_device *dev)\n{\n\tstruct fib_result res;\n\tint err;\n\n\ttos &= IPTOS_RT_MASK;\n\trcu_read_lock();\n\terr = ip_route_input_rcu(skb, daddr, saddr, tos, dev, &res);\n\trcu_read_unlock();\n\n\treturn err;\n}\nEXPORT_SYMBOL(ip_route_input_noref);\n\n/* called with rcu_read_lock held */\nint ip_route_input_rcu(struct sk_buff *skb, __be32 daddr, __be32 saddr,\n\t\t       u8 tos, struct net_device *dev, struct fib_result *res)\n{\n\t/* Multicast recognition logic is moved from route cache to here.\n\t   The problem was that too many Ethernet cards have broken/missing\n\t   hardware multicast filters :-( As result the host on multicasting\n\t   network acquires a lot of useless route cache entries, sort of\n\t   SDR messages from all the world. Now we try to get rid of them.\n\t   Really, provided software IP multicast filter is organized\n\t   reasonably (at least, hashed), it does not result in a slowdown\n\t   comparing with route cache reject entries.\n\t   Note, that multicast routers are not affected, because\n\t   route cache entry is created eventually.\n\t */\n\tif (ipv4_is_multicast(daddr)) {\n\t\tstruct in_device *in_dev = __in_dev_get_rcu(dev);\n\t\tint our = 0;\n\t\tint err = -EINVAL;\n\n\t\tif (in_dev)\n\t\t\tour = ip_check_mc_rcu(in_dev, daddr, saddr,\n\t\t\t\t\t      ip_hdr(skb)->protocol);\n\n\t\t/* check l3 master if no match yet */\n\t\tif ((!in_dev || !our) && netif_is_l3_slave(dev)) {\n\t\t\tstruct in_device *l3_in_dev;\n\n\t\t\tl3_in_dev = __in_dev_get_rcu(skb->dev);\n\t\t\tif (l3_in_dev)\n\t\t\t\tour = ip_check_mc_rcu(l3_in_dev, daddr, saddr,\n\t\t\t\t\t\t      ip_hdr(skb)->protocol);\n\t\t}\n\n\t\tif (our\n#ifdef CONFIG_IP_MROUTE\n\t\t\t||\n\t\t    (!ipv4_is_local_multicast(daddr) &&\n\t\t     IN_DEV_MFORWARD(in_dev))\n#endif\n\t\t   ) {\n\t\t\terr = ip_route_input_mc(skb, daddr, saddr,\n\t\t\t\t\t\ttos, dev, our);\n\t\t}\n\t\treturn err;\n\t}\n\n\treturn ip_route_input_slow(skb, daddr, saddr, tos, dev, res);\n}\n\n/* called with rcu_read_lock() */\nstatic struct rtable *__mkroute_output(const struct fib_result *res,\n\t\t\t\t       const struct flowi4 *fl4, int orig_oif,\n\t\t\t\t       struct net_device *dev_out,\n\t\t\t\t       unsigned int flags)\n{\n\tstruct fib_info *fi = res->fi;\n\tstruct fib_nh_exception *fnhe;\n\tstruct in_device *in_dev;\n\tu16 type = res->type;\n\tstruct rtable *rth;\n\tbool do_cache;\n\n\tin_dev = __in_dev_get_rcu(dev_out);\n\tif (!in_dev)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (likely(!IN_DEV_ROUTE_LOCALNET(in_dev)))\n\t\tif (ipv4_is_loopback(fl4->saddr) &&\n\t\t    !(dev_out->flags & IFF_LOOPBACK) &&\n\t\t    !netif_is_l3_master(dev_out))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\tif (ipv4_is_lbcast(fl4->daddr))\n\t\ttype = RTN_BROADCAST;\n\telse if (ipv4_is_multicast(fl4->daddr))\n\t\ttype = RTN_MULTICAST;\n\telse if (ipv4_is_zeronet(fl4->daddr))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (dev_out->flags & IFF_LOOPBACK)\n\t\tflags |= RTCF_LOCAL;\n\n\tdo_cache = true;\n\tif (type == RTN_BROADCAST) {\n\t\tflags |= RTCF_BROADCAST | RTCF_LOCAL;\n\t\tfi = NULL;\n\t} else if (type == RTN_MULTICAST) {\n\t\tflags |= RTCF_MULTICAST | RTCF_LOCAL;\n\t\tif (!ip_check_mc_rcu(in_dev, fl4->daddr, fl4->saddr,\n\t\t\t\t     fl4->flowi4_proto))\n\t\t\tflags &= ~RTCF_LOCAL;\n\t\telse\n\t\t\tdo_cache = false;\n\t\t/* If multicast route do not exist use\n\t\t * default one, but do not gateway in this case.\n\t\t * Yes, it is hack.\n\t\t */\n\t\tif (fi && res->prefixlen < 4)\n\t\t\tfi = NULL;\n\t} else if ((type == RTN_LOCAL) && (orig_oif != 0) &&\n\t\t   (orig_oif != dev_out->ifindex)) {\n\t\t/* For local routes that require a particular output interface\n\t\t * we do not want to cache the result.  Caching the result\n\t\t * causes incorrect behaviour when there are multiple source\n\t\t * addresses on the interface, the end result being that if the\n\t\t * intended recipient is waiting on that interface for the\n\t\t * packet he won't receive it because it will be delivered on\n\t\t * the loopback interface and the IP_PKTINFO ipi_ifindex will\n\t\t * be set to the loopback interface as well.\n\t\t */\n\t\tfi = NULL;\n\t}\n\n\tfnhe = NULL;\n\tdo_cache &= fi != NULL;\n\tif (do_cache) {\n\t\tstruct rtable __rcu **prth;\n\t\tstruct fib_nh *nh = &FIB_RES_NH(*res);\n\n\t\tfnhe = find_exception(nh, fl4->daddr);\n\t\tif (fnhe) {\n\t\t\tprth = &fnhe->fnhe_rth_output;\n\t\t\trth = rcu_dereference(*prth);\n\t\t\tif (rth && rth->dst.expires &&\n\t\t\t    time_after(jiffies, rth->dst.expires)) {\n\t\t\t\tip_del_fnhe(nh, fl4->daddr);\n\t\t\t\tfnhe = NULL;\n\t\t\t} else {\n\t\t\t\tgoto rt_cache;\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(fl4->flowi4_flags &\n\t\t\t     FLOWI_FLAG_KNOWN_NH &&\n\t\t\t     !(nh->nh_gw &&\n\t\t\t       nh->nh_scope == RT_SCOPE_LINK))) {\n\t\t\tdo_cache = false;\n\t\t\tgoto add;\n\t\t}\n\t\tprth = raw_cpu_ptr(nh->nh_pcpu_rth_output);\n\t\trth = rcu_dereference(*prth);\n\nrt_cache:\n\t\tif (rt_cache_valid(rth) && dst_hold_safe(&rth->dst))\n\t\t\treturn rth;\n\t}\n\nadd:\n\trth = rt_dst_alloc(dev_out, flags, type,\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOPOLICY),\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOXFRM),\n\t\t\t   do_cache);\n\tif (!rth)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\trth->rt_iif\t= orig_oif ? : 0;\n\tif (res->table)\n\t\trth->rt_table_id = res->table->tb_id;\n\n\tRT_CACHE_STAT_INC(out_slow_tot);\n\n\tif (flags & (RTCF_BROADCAST | RTCF_MULTICAST)) {\n\t\tif (flags & RTCF_LOCAL &&\n\t\t    !(dev_out->flags & IFF_LOOPBACK)) {\n\t\t\trth->dst.output = ip_mc_output;\n\t\t\tRT_CACHE_STAT_INC(out_slow_mc);\n\t\t}\n#ifdef CONFIG_IP_MROUTE\n\t\tif (type == RTN_MULTICAST) {\n\t\t\tif (IN_DEV_MFORWARD(in_dev) &&\n\t\t\t    !ipv4_is_local_multicast(fl4->daddr)) {\n\t\t\t\trth->dst.input = ip_mr_input;\n\t\t\t\trth->dst.output = ip_mc_output;\n\t\t\t}\n\t\t}\n#endif\n\t}\n\n\trt_set_nexthop(rth, fl4->daddr, res, fnhe, fi, type, 0, do_cache);\n\tset_lwt_redirect(rth);\n\n\treturn rth;\n}\n\n/*\n * Major route resolver routine.\n */\n\nstruct rtable *ip_route_output_key_hash(struct net *net, struct flowi4 *fl4,\n\t\t\t\t\tconst struct sk_buff *skb)\n{\n\t__u8 tos = RT_FL_TOS(fl4);\n\tstruct fib_result res;\n\tstruct rtable *rth;\n\n\tres.tclassid\t= 0;\n\tres.fi\t\t= NULL;\n\tres.table\t= NULL;\n\n\tfl4->flowi4_iif = LOOPBACK_IFINDEX;\n\tfl4->flowi4_tos = tos & IPTOS_RT_MASK;\n\tfl4->flowi4_scope = ((tos & RTO_ONLINK) ?\n\t\t\t RT_SCOPE_LINK : RT_SCOPE_UNIVERSE);\n\n\trcu_read_lock();\n\trth = ip_route_output_key_hash_rcu(net, fl4, &res, skb);\n\trcu_read_unlock();\n\n\treturn rth;\n}\nEXPORT_SYMBOL_GPL(ip_route_output_key_hash);\n\nstruct rtable *ip_route_output_key_hash_rcu(struct net *net, struct flowi4 *fl4,\n\t\t\t\t\t    struct fib_result *res,\n\t\t\t\t\t    const struct sk_buff *skb)\n{\n\tstruct net_device *dev_out = NULL;\n\tint orig_oif = fl4->flowi4_oif;\n\tunsigned int flags = 0;\n\tstruct rtable *rth;\n\tint err = -ENETUNREACH;\n\n\tif (fl4->saddr) {\n\t\trth = ERR_PTR(-EINVAL);\n\t\tif (ipv4_is_multicast(fl4->saddr) ||\n\t\t    ipv4_is_lbcast(fl4->saddr) ||\n\t\t    ipv4_is_zeronet(fl4->saddr))\n\t\t\tgoto out;\n\n\t\t/* I removed check for oif == dev_out->oif here.\n\t\t   It was wrong for two reasons:\n\t\t   1. ip_dev_find(net, saddr) can return wrong iface, if saddr\n\t\t      is assigned to multiple interfaces.\n\t\t   2. Moreover, we are allowed to send packets with saddr\n\t\t      of another iface. --ANK\n\t\t */\n\n\t\tif (fl4->flowi4_oif == 0 &&\n\t\t    (ipv4_is_multicast(fl4->daddr) ||\n\t\t     ipv4_is_lbcast(fl4->daddr))) {\n\t\t\t/* It is equivalent to inet_addr_type(saddr) == RTN_LOCAL */\n\t\t\tdev_out = __ip_dev_find(net, fl4->saddr, false);\n\t\t\tif (!dev_out)\n\t\t\t\tgoto out;\n\n\t\t\t/* Special hack: user can direct multicasts\n\t\t\t   and limited broadcast via necessary interface\n\t\t\t   without fiddling with IP_MULTICAST_IF or IP_PKTINFO.\n\t\t\t   This hack is not just for fun, it allows\n\t\t\t   vic,vat and friends to work.\n\t\t\t   They bind socket to loopback, set ttl to zero\n\t\t\t   and expect that it will work.\n\t\t\t   From the viewpoint of routing cache they are broken,\n\t\t\t   because we are not allowed to build multicast path\n\t\t\t   with loopback source addr (look, routing cache\n\t\t\t   cannot know, that ttl is zero, so that packet\n\t\t\t   will not leave this host and route is valid).\n\t\t\t   Luckily, this hack is good workaround.\n\t\t\t */\n\n\t\t\tfl4->flowi4_oif = dev_out->ifindex;\n\t\t\tgoto make_route;\n\t\t}\n\n\t\tif (!(fl4->flowi4_flags & FLOWI_FLAG_ANYSRC)) {\n\t\t\t/* It is equivalent to inet_addr_type(saddr) == RTN_LOCAL */\n\t\t\tif (!__ip_dev_find(net, fl4->saddr, false))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\n\tif (fl4->flowi4_oif) {\n\t\tdev_out = dev_get_by_index_rcu(net, fl4->flowi4_oif);\n\t\trth = ERR_PTR(-ENODEV);\n\t\tif (!dev_out)\n\t\t\tgoto out;\n\n\t\t/* RACE: Check return value of inet_select_addr instead. */\n\t\tif (!(dev_out->flags & IFF_UP) || !__in_dev_get_rcu(dev_out)) {\n\t\t\trth = ERR_PTR(-ENETUNREACH);\n\t\t\tgoto out;\n\t\t}\n\t\tif (ipv4_is_local_multicast(fl4->daddr) ||\n\t\t    ipv4_is_lbcast(fl4->daddr) ||\n\t\t    fl4->flowi4_proto == IPPROTO_IGMP) {\n\t\t\tif (!fl4->saddr)\n\t\t\t\tfl4->saddr = inet_select_addr(dev_out, 0,\n\t\t\t\t\t\t\t      RT_SCOPE_LINK);\n\t\t\tgoto make_route;\n\t\t}\n\t\tif (!fl4->saddr) {\n\t\t\tif (ipv4_is_multicast(fl4->daddr))\n\t\t\t\tfl4->saddr = inet_select_addr(dev_out, 0,\n\t\t\t\t\t\t\t      fl4->flowi4_scope);\n\t\t\telse if (!fl4->daddr)\n\t\t\t\tfl4->saddr = inet_select_addr(dev_out, 0,\n\t\t\t\t\t\t\t      RT_SCOPE_HOST);\n\t\t}\n\t}\n\n\tif (!fl4->daddr) {\n\t\tfl4->daddr = fl4->saddr;\n\t\tif (!fl4->daddr)\n\t\t\tfl4->daddr = fl4->saddr = htonl(INADDR_LOOPBACK);\n\t\tdev_out = net->loopback_dev;\n\t\tfl4->flowi4_oif = LOOPBACK_IFINDEX;\n\t\tres->type = RTN_LOCAL;\n\t\tflags |= RTCF_LOCAL;\n\t\tgoto make_route;\n\t}\n\n\terr = fib_lookup(net, fl4, res, 0);\n\tif (err) {\n\t\tres->fi = NULL;\n\t\tres->table = NULL;\n\t\tif (fl4->flowi4_oif &&\n\t\t    (ipv4_is_multicast(fl4->daddr) ||\n\t\t    !netif_index_is_l3_master(net, fl4->flowi4_oif))) {\n\t\t\t/* Apparently, routing tables are wrong. Assume,\n\t\t\t   that the destination is on link.\n\n\t\t\t   WHY? DW.\n\t\t\t   Because we are allowed to send to iface\n\t\t\t   even if it has NO routes and NO assigned\n\t\t\t   addresses. When oif is specified, routing\n\t\t\t   tables are looked up with only one purpose:\n\t\t\t   to catch if destination is gatewayed, rather than\n\t\t\t   direct. Moreover, if MSG_DONTROUTE is set,\n\t\t\t   we send packet, ignoring both routing tables\n\t\t\t   and ifaddr state. --ANK\n\n\n\t\t\t   We could make it even if oif is unknown,\n\t\t\t   likely IPv6, but we do not.\n\t\t\t */\n\n\t\t\tif (fl4->saddr == 0)\n\t\t\t\tfl4->saddr = inet_select_addr(dev_out, 0,\n\t\t\t\t\t\t\t      RT_SCOPE_LINK);\n\t\t\tres->type = RTN_UNICAST;\n\t\t\tgoto make_route;\n\t\t}\n\t\trth = ERR_PTR(err);\n\t\tgoto out;\n\t}\n\n\tif (res->type == RTN_LOCAL) {\n\t\tif (!fl4->saddr) {\n\t\t\tif (res->fi->fib_prefsrc)\n\t\t\t\tfl4->saddr = res->fi->fib_prefsrc;\n\t\t\telse\n\t\t\t\tfl4->saddr = fl4->daddr;\n\t\t}\n\n\t\t/* L3 master device is the loopback for that domain */\n\t\tdev_out = l3mdev_master_dev_rcu(FIB_RES_DEV(*res)) ? :\n\t\t\tnet->loopback_dev;\n\t\tfl4->flowi4_oif = dev_out->ifindex;\n\t\tflags |= RTCF_LOCAL;\n\t\tgoto make_route;\n\t}\n\n\tfib_select_path(net, res, fl4, skb);\n\n\tdev_out = FIB_RES_DEV(*res);\n\tfl4->flowi4_oif = dev_out->ifindex;\n\n\nmake_route:\n\trth = __mkroute_output(res, fl4, orig_oif, dev_out, flags);\n\nout:\n\treturn rth;\n}\n\nstatic struct dst_entry *ipv4_blackhole_dst_check(struct dst_entry *dst, u32 cookie)\n{\n\treturn NULL;\n}\n\nstatic unsigned int ipv4_blackhole_mtu(const struct dst_entry *dst)\n{\n\tunsigned int mtu = dst_metric_raw(dst, RTAX_MTU);\n\n\treturn mtu ? : dst->dev->mtu;\n}\n\nstatic void ipv4_rt_blackhole_update_pmtu(struct dst_entry *dst, struct sock *sk,\n\t\t\t\t\t  struct sk_buff *skb, u32 mtu)\n{\n}\n\nstatic void ipv4_rt_blackhole_redirect(struct dst_entry *dst, struct sock *sk,\n\t\t\t\t       struct sk_buff *skb)\n{\n}\n\nstatic u32 *ipv4_rt_blackhole_cow_metrics(struct dst_entry *dst,\n\t\t\t\t\t  unsigned long old)\n{\n\treturn NULL;\n}\n\nstatic struct dst_ops ipv4_dst_blackhole_ops = {\n\t.family\t\t\t=\tAF_INET,\n\t.check\t\t\t=\tipv4_blackhole_dst_check,\n\t.mtu\t\t\t=\tipv4_blackhole_mtu,\n\t.default_advmss\t\t=\tipv4_default_advmss,\n\t.update_pmtu\t\t=\tipv4_rt_blackhole_update_pmtu,\n\t.redirect\t\t=\tipv4_rt_blackhole_redirect,\n\t.cow_metrics\t\t=\tipv4_rt_blackhole_cow_metrics,\n\t.neigh_lookup\t\t=\tipv4_neigh_lookup,\n};\n\nstruct dst_entry *ipv4_blackhole_route(struct net *net, struct dst_entry *dst_orig)\n{\n\tstruct rtable *ort = (struct rtable *) dst_orig;\n\tstruct rtable *rt;\n\n\trt = dst_alloc(&ipv4_dst_blackhole_ops, NULL, 1, DST_OBSOLETE_NONE, 0);\n\tif (rt) {\n\t\tstruct dst_entry *new = &rt->dst;\n\n\t\tnew->__use = 1;\n\t\tnew->input = dst_discard;\n\t\tnew->output = dst_discard_out;\n\n\t\tnew->dev = net->loopback_dev;\n\t\tif (new->dev)\n\t\t\tdev_hold(new->dev);\n\n\t\trt->rt_is_input = ort->rt_is_input;\n\t\trt->rt_iif = ort->rt_iif;\n\t\trt->rt_pmtu = ort->rt_pmtu;\n\n\t\trt->rt_genid = rt_genid_ipv4(net);\n\t\trt->rt_flags = ort->rt_flags;\n\t\trt->rt_type = ort->rt_type;\n\t\trt->rt_gateway = ort->rt_gateway;\n\t\trt->rt_uses_gateway = ort->rt_uses_gateway;\n\n\t\tINIT_LIST_HEAD(&rt->rt_uncached);\n\t}\n\n\tdst_release(dst_orig);\n\n\treturn rt ? &rt->dst : ERR_PTR(-ENOMEM);\n}\n\nstruct rtable *ip_route_output_flow(struct net *net, struct flowi4 *flp4,\n\t\t\t\t    const struct sock *sk)\n{\n\tstruct rtable *rt = __ip_route_output_key(net, flp4);\n\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\tif (flp4->flowi4_proto)\n\t\trt = (struct rtable *)xfrm_lookup_route(net, &rt->dst,\n\t\t\t\t\t\t\tflowi4_to_flowi(flp4),\n\t\t\t\t\t\t\tsk, 0);\n\n\treturn rt;\n}\nEXPORT_SYMBOL_GPL(ip_route_output_flow);\n\n/* called with rcu_read_lock held */\nstatic int rt_fill_info(struct net *net,  __be32 dst, __be32 src, u32 table_id,\n\t\t\tstruct flowi4 *fl4, struct sk_buff *skb, u32 portid,\n\t\t\tu32 seq)\n{\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct rtmsg *r;\n\tstruct nlmsghdr *nlh;\n\tunsigned long expires = 0;\n\tu32 error;\n\tu32 metrics[RTAX_MAX];\n\n\tnlh = nlmsg_put(skb, portid, seq, RTM_NEWROUTE, sizeof(*r), 0);\n\tif (!nlh)\n\t\treturn -EMSGSIZE;\n\n\tr = nlmsg_data(nlh);\n\tr->rtm_family\t = AF_INET;\n\tr->rtm_dst_len\t= 32;\n\tr->rtm_src_len\t= 0;\n\tr->rtm_tos\t= fl4->flowi4_tos;\n\tr->rtm_table\t= table_id < 256 ? table_id : RT_TABLE_COMPAT;\n\tif (nla_put_u32(skb, RTA_TABLE, table_id))\n\t\tgoto nla_put_failure;\n\tr->rtm_type\t= rt->rt_type;\n\tr->rtm_scope\t= RT_SCOPE_UNIVERSE;\n\tr->rtm_protocol = RTPROT_UNSPEC;\n\tr->rtm_flags\t= (rt->rt_flags & ~0xFFFF) | RTM_F_CLONED;\n\tif (rt->rt_flags & RTCF_NOTIFY)\n\t\tr->rtm_flags |= RTM_F_NOTIFY;\n\tif (IPCB(skb)->flags & IPSKB_DOREDIRECT)\n\t\tr->rtm_flags |= RTCF_DOREDIRECT;\n\n\tif (nla_put_in_addr(skb, RTA_DST, dst))\n\t\tgoto nla_put_failure;\n\tif (src) {\n\t\tr->rtm_src_len = 32;\n\t\tif (nla_put_in_addr(skb, RTA_SRC, src))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (rt->dst.dev &&\n\t    nla_put_u32(skb, RTA_OIF, rt->dst.dev->ifindex))\n\t\tgoto nla_put_failure;\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tif (rt->dst.tclassid &&\n\t    nla_put_u32(skb, RTA_FLOW, rt->dst.tclassid))\n\t\tgoto nla_put_failure;\n#endif\n\tif (!rt_is_input_route(rt) &&\n\t    fl4->saddr != src) {\n\t\tif (nla_put_in_addr(skb, RTA_PREFSRC, fl4->saddr))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (rt->rt_uses_gateway &&\n\t    nla_put_in_addr(skb, RTA_GATEWAY, rt->rt_gateway))\n\t\tgoto nla_put_failure;\n\n\texpires = rt->dst.expires;\n\tif (expires) {\n\t\tunsigned long now = jiffies;\n\n\t\tif (time_before(now, expires))\n\t\t\texpires -= now;\n\t\telse\n\t\t\texpires = 0;\n\t}\n\n\tmemcpy(metrics, dst_metrics_ptr(&rt->dst), sizeof(metrics));\n\tif (rt->rt_pmtu && expires)\n\t\tmetrics[RTAX_MTU - 1] = rt->rt_pmtu;\n\tif (rtnetlink_put_metrics(skb, metrics) < 0)\n\t\tgoto nla_put_failure;\n\n\tif (fl4->flowi4_mark &&\n\t    nla_put_u32(skb, RTA_MARK, fl4->flowi4_mark))\n\t\tgoto nla_put_failure;\n\n\tif (!uid_eq(fl4->flowi4_uid, INVALID_UID) &&\n\t    nla_put_u32(skb, RTA_UID,\n\t\t\tfrom_kuid_munged(current_user_ns(), fl4->flowi4_uid)))\n\t\tgoto nla_put_failure;\n\n\terror = rt->dst.error;\n\n\tif (rt_is_input_route(rt)) {\n#ifdef CONFIG_IP_MROUTE\n\t\tif (ipv4_is_multicast(dst) && !ipv4_is_local_multicast(dst) &&\n\t\t    IPV4_DEVCONF_ALL(net, MC_FORWARDING)) {\n\t\t\tint err = ipmr_get_route(net, skb,\n\t\t\t\t\t\t fl4->saddr, fl4->daddr,\n\t\t\t\t\t\t r, portid);\n\n\t\t\tif (err <= 0) {\n\t\t\t\tif (err == 0)\n\t\t\t\t\treturn 0;\n\t\t\t\tgoto nla_put_failure;\n\t\t\t}\n\t\t} else\n#endif\n\t\t\tif (nla_put_u32(skb, RTA_IIF, skb->dev->ifindex))\n\t\t\t\tgoto nla_put_failure;\n\t}\n\n\tif (rtnl_put_cacheinfo(skb, &rt->dst, 0, expires, error) < 0)\n\t\tgoto nla_put_failure;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n\nnla_put_failure:\n\tnlmsg_cancel(skb, nlh);\n\treturn -EMSGSIZE;\n}\n\nstatic int inet_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct rtmsg *rtm;\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct fib_result res = {};\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\t__be32 dst = 0;\n\t__be32 src = 0;\n\tu32 iif;\n\tint err;\n\tint mark;\n\tstruct sk_buff *skb;\n\tu32 table_id = RT_TABLE_MAIN;\n\tkuid_t uid;\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_ipv4_policy,\n\t\t\t  extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\trtm = nlmsg_data(nlh);\n\n\tskb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);\n\tif (!skb) {\n\t\terr = -ENOBUFS;\n\t\tgoto errout;\n\t}\n\n\t/* Reserve room for dummy headers, this skb can pass\n\t   through good chunk of routing engine.\n\t */\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\n\tsrc = tb[RTA_SRC] ? nla_get_in_addr(tb[RTA_SRC]) : 0;\n\tdst = tb[RTA_DST] ? nla_get_in_addr(tb[RTA_DST]) : 0;\n\tiif = tb[RTA_IIF] ? nla_get_u32(tb[RTA_IIF]) : 0;\n\tmark = tb[RTA_MARK] ? nla_get_u32(tb[RTA_MARK]) : 0;\n\tif (tb[RTA_UID])\n\t\tuid = make_kuid(current_user_ns(), nla_get_u32(tb[RTA_UID]));\n\telse\n\t\tuid = (iif ? INVALID_UID : current_uid());\n\n\t/* Bugfix: need to give ip_route_input enough of an IP header to\n\t * not gag.\n\t */\n\tip_hdr(skb)->protocol = IPPROTO_UDP;\n\tip_hdr(skb)->saddr = src;\n\tip_hdr(skb)->daddr = dst;\n\n\tskb_reserve(skb, MAX_HEADER + sizeof(struct iphdr));\n\n\tmemset(&fl4, 0, sizeof(fl4));\n\tfl4.daddr = dst;\n\tfl4.saddr = src;\n\tfl4.flowi4_tos = rtm->rtm_tos;\n\tfl4.flowi4_oif = tb[RTA_OIF] ? nla_get_u32(tb[RTA_OIF]) : 0;\n\tfl4.flowi4_mark = mark;\n\tfl4.flowi4_uid = uid;\n\n\trcu_read_lock();\n\n\tif (iif) {\n\t\tstruct net_device *dev;\n\n\t\tdev = dev_get_by_index_rcu(net, iif);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto errout_free;\n\t\t}\n\n\t\tskb->protocol\t= htons(ETH_P_IP);\n\t\tskb->dev\t= dev;\n\t\tskb->mark\t= mark;\n\t\terr = ip_route_input_rcu(skb, dst, src, rtm->rtm_tos,\n\t\t\t\t\t dev, &res);\n\n\t\trt = skb_rtable(skb);\n\t\tif (err == 0 && rt->dst.error)\n\t\t\terr = -rt->dst.error;\n\t} else {\n\t\trt = ip_route_output_key_hash_rcu(net, &fl4, &res, skb);\n\t\terr = 0;\n\t\tif (IS_ERR(rt))\n\t\t\terr = PTR_ERR(rt);\n\t\telse\n\t\t\tskb_dst_set(skb, &rt->dst);\n\t}\n\n\tif (err)\n\t\tgoto errout_free;\n\n\tif (rtm->rtm_flags & RTM_F_NOTIFY)\n\t\trt->rt_flags |= RTCF_NOTIFY;\n\n\tif (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)\n\t\ttable_id = rt->rt_table_id;\n\n\tif (rtm->rtm_flags & RTM_F_FIB_MATCH)\n\t\terr = fib_dump_info(skb, NETLINK_CB(in_skb).portid,\n\t\t\t\t    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,\n\t\t\t\t    rt->rt_type, res.prefix, res.prefixlen,\n\t\t\t\t    fl4.flowi4_tos, res.fi, 0);\n\telse\n\t\terr = rt_fill_info(net, dst, src, table_id, &fl4, skb,\n\t\t\t\t   NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);\n\tif (err < 0)\n\t\tgoto errout_free;\n\n\trcu_read_unlock();\n\n\terr = rtnl_unicast(skb, net, NETLINK_CB(in_skb).portid);\nerrout:\n\treturn err;\n\nerrout_free:\n\trcu_read_unlock();\n\tkfree_skb(skb);\n\tgoto errout;\n}\n\nvoid ip_rt_multicast_event(struct in_device *in_dev)\n{\n\trt_cache_flush(dev_net(in_dev->dev));\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int ip_rt_gc_interval __read_mostly  = 60 * HZ;\nstatic int ip_rt_gc_min_interval __read_mostly\t= HZ / 2;\nstatic int ip_rt_gc_elasticity __read_mostly\t= 8;\n\nstatic int ipv4_sysctl_rtcache_flush(struct ctl_table *__ctl, int write,\n\t\t\t\t\tvoid __user *buffer,\n\t\t\t\t\tsize_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = (struct net *)__ctl->extra1;\n\n\tif (write) {\n\t\trt_cache_flush(net);\n\t\tfnhe_genid_bump(net);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic struct ctl_table ipv4_route_table[] = {\n\t{\n\t\t.procname\t= \"gc_thresh\",\n\t\t.data\t\t= &ipv4_dst_ops.gc_thresh,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"max_size\",\n\t\t.data\t\t= &ip_rt_max_size,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t/*  Deprecated. Use gc_min_interval_ms */\n\n\t\t.procname\t= \"gc_min_interval\",\n\t\t.data\t\t= &ip_rt_gc_min_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"gc_min_interval_ms\",\n\t\t.data\t\t= &ip_rt_gc_min_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_ms_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"gc_timeout\",\n\t\t.data\t\t= &ip_rt_gc_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"gc_interval\",\n\t\t.data\t\t= &ip_rt_gc_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"redirect_load\",\n\t\t.data\t\t= &ip_rt_redirect_load,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"redirect_number\",\n\t\t.data\t\t= &ip_rt_redirect_number,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"redirect_silence\",\n\t\t.data\t\t= &ip_rt_redirect_silence,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"error_cost\",\n\t\t.data\t\t= &ip_rt_error_cost,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"error_burst\",\n\t\t.data\t\t= &ip_rt_error_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"gc_elasticity\",\n\t\t.data\t\t= &ip_rt_gc_elasticity,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"mtu_expires\",\n\t\t.data\t\t= &ip_rt_mtu_expires,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"min_pmtu\",\n\t\t.data\t\t= &ip_rt_min_pmtu,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"min_adv_mss\",\n\t\t.data\t\t= &ip_rt_min_advmss,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{ }\n};\n\nstatic struct ctl_table ipv4_route_flush_table[] = {\n\t{\n\t\t.procname\t= \"flush\",\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0200,\n\t\t.proc_handler\t= ipv4_sysctl_rtcache_flush,\n\t},\n\t{ },\n};\n\nstatic __net_init int sysctl_route_net_init(struct net *net)\n{\n\tstruct ctl_table *tbl;\n\n\ttbl = ipv4_route_flush_table;\n\tif (!net_eq(net, &init_net)) {\n\t\ttbl = kmemdup(tbl, sizeof(ipv4_route_flush_table), GFP_KERNEL);\n\t\tif (!tbl)\n\t\t\tgoto err_dup;\n\n\t\t/* Don't export sysctls to unprivileged users */\n\t\tif (net->user_ns != &init_user_ns)\n\t\t\ttbl[0].procname = NULL;\n\t}\n\ttbl[0].extra1 = net;\n\n\tnet->ipv4.route_hdr = register_net_sysctl(net, \"net/ipv4/route\", tbl);\n\tif (!net->ipv4.route_hdr)\n\t\tgoto err_reg;\n\treturn 0;\n\nerr_reg:\n\tif (tbl != ipv4_route_flush_table)\n\t\tkfree(tbl);\nerr_dup:\n\treturn -ENOMEM;\n}\n\nstatic __net_exit void sysctl_route_net_exit(struct net *net)\n{\n\tstruct ctl_table *tbl;\n\n\ttbl = net->ipv4.route_hdr->ctl_table_arg;\n\tunregister_net_sysctl_table(net->ipv4.route_hdr);\n\tBUG_ON(tbl == ipv4_route_flush_table);\n\tkfree(tbl);\n}\n\nstatic __net_initdata struct pernet_operations sysctl_route_ops = {\n\t.init = sysctl_route_net_init,\n\t.exit = sysctl_route_net_exit,\n};\n#endif\n\nstatic __net_init int rt_genid_init(struct net *net)\n{\n\tatomic_set(&net->ipv4.rt_genid, 0);\n\tatomic_set(&net->fnhe_genid, 0);\n\tatomic_set(&net->ipv4.dev_addr_genid, get_random_int());\n\treturn 0;\n}\n\nstatic __net_initdata struct pernet_operations rt_genid_ops = {\n\t.init = rt_genid_init,\n};\n\nstatic int __net_init ipv4_inetpeer_init(struct net *net)\n{\n\tstruct inet_peer_base *bp = kmalloc(sizeof(*bp), GFP_KERNEL);\n\n\tif (!bp)\n\t\treturn -ENOMEM;\n\tinet_peer_base_init(bp);\n\tnet->ipv4.peers = bp;\n\treturn 0;\n}\n\nstatic void __net_exit ipv4_inetpeer_exit(struct net *net)\n{\n\tstruct inet_peer_base *bp = net->ipv4.peers;\n\n\tnet->ipv4.peers = NULL;\n\tinetpeer_invalidate_tree(bp);\n\tkfree(bp);\n}\n\nstatic __net_initdata struct pernet_operations ipv4_inetpeer_ops = {\n\t.init\t=\tipv4_inetpeer_init,\n\t.exit\t=\tipv4_inetpeer_exit,\n};\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\nstruct ip_rt_acct __percpu *ip_rt_acct __read_mostly;\n#endif /* CONFIG_IP_ROUTE_CLASSID */\n\nint __init ip_rt_init(void)\n{\n\tint rc = 0;\n\tint cpu;\n\n\tip_idents = kmalloc(IP_IDENTS_SZ * sizeof(*ip_idents), GFP_KERNEL);\n\tif (!ip_idents)\n\t\tpanic(\"IP: failed to allocate ip_idents\\n\");\n\n\tprandom_bytes(ip_idents, IP_IDENTS_SZ * sizeof(*ip_idents));\n\n\tip_tstamps = kcalloc(IP_IDENTS_SZ, sizeof(*ip_tstamps), GFP_KERNEL);\n\tif (!ip_tstamps)\n\t\tpanic(\"IP: failed to allocate ip_tstamps\\n\");\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct uncached_list *ul = &per_cpu(rt_uncached_list, cpu);\n\n\t\tINIT_LIST_HEAD(&ul->head);\n\t\tspin_lock_init(&ul->lock);\n\t}\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tip_rt_acct = __alloc_percpu(256 * sizeof(struct ip_rt_acct), __alignof__(struct ip_rt_acct));\n\tif (!ip_rt_acct)\n\t\tpanic(\"IP: failed to allocate ip_rt_acct\\n\");\n#endif\n\n\tipv4_dst_ops.kmem_cachep =\n\t\tkmem_cache_create(\"ip_dst_cache\", sizeof(struct rtable), 0,\n\t\t\t\t  SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);\n\n\tipv4_dst_blackhole_ops.kmem_cachep = ipv4_dst_ops.kmem_cachep;\n\n\tif (dst_entries_init(&ipv4_dst_ops) < 0)\n\t\tpanic(\"IP: failed to allocate ipv4_dst_ops counter\\n\");\n\n\tif (dst_entries_init(&ipv4_dst_blackhole_ops) < 0)\n\t\tpanic(\"IP: failed to allocate ipv4_dst_blackhole_ops counter\\n\");\n\n\tipv4_dst_ops.gc_thresh = ~0;\n\tip_rt_max_size = INT_MAX;\n\n\tdevinet_init();\n\tip_fib_init();\n\n\tif (ip_rt_proc_init())\n\t\tpr_err(\"Unable to create route proc files\\n\");\n#ifdef CONFIG_XFRM\n\txfrm_init();\n\txfrm4_init();\n#endif\n\trtnl_register(PF_INET, RTM_GETROUTE, inet_rtm_getroute, NULL, NULL);\n\n#ifdef CONFIG_SYSCTL\n\tregister_pernet_subsys(&sysctl_route_ops);\n#endif\n\tregister_pernet_subsys(&rt_genid_ops);\n\tregister_pernet_subsys(&ipv4_inetpeer_ops);\n\treturn rc;\n}\n\n#ifdef CONFIG_SYSCTL\n/*\n * We really need to sanitize the damn ipv4 init order, then all\n * this nonsense will go away.\n */\nvoid __init ip_static_sysctl_init(void)\n{\n\tregister_net_sysctl(&init_net, \"net/ipv4/route\", ipv4_route_table);\n}\n#endif\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tROUTE - implementation of the IP router.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tLinus Torvalds, <Linus.Torvalds@helsinki.fi>\n *\t\tAlexey Kuznetsov, <kuznet@ms2.inr.ac.ru>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tVerify area fixes.\n *\t\tAlan Cox\t:\tcli() protects routing changes\n *\t\tRui Oliveira\t:\tICMP routing table updates\n *\t\t(rco@di.uminho.pt)\tRouting table insertion and update\n *\t\tLinus Torvalds\t:\tRewrote bits to be sensible\n *\t\tAlan Cox\t:\tAdded BSD route gw semantics\n *\t\tAlan Cox\t:\tSuper /proc >4K\n *\t\tAlan Cox\t:\tMTU in route table\n *\t\tAlan Cox\t: \tMSS actually. Also added the window\n *\t\t\t\t\tclamper.\n *\t\tSam Lantinga\t:\tFixed route matching in rt_del()\n *\t\tAlan Cox\t:\tRouting cache support.\n *\t\tAlan Cox\t:\tRemoved compatibility cruft.\n *\t\tAlan Cox\t:\tRTF_REJECT support.\n *\t\tAlan Cox\t:\tTCP irtt support.\n *\t\tJonathan Naylor\t:\tAdded Metric support.\n *\tMiquel van Smoorenburg\t:\tBSD API fixes.\n *\tMiquel van Smoorenburg\t:\tMetrics.\n *\t\tAlan Cox\t:\tUse __u32 properly\n *\t\tAlan Cox\t:\tAligned routing errors more closely with BSD\n *\t\t\t\t\tour system is still very different.\n *\t\tAlan Cox\t:\tFaster /proc handling\n *\tAlexey Kuznetsov\t:\tMassive rework to support tree based routing,\n *\t\t\t\t\trouting caches and better behaviour.\n *\n *\t\tOlaf Erb\t:\tirtt wasn't being copied right.\n *\t\tBjorn Ekwall\t:\tKerneld route support.\n *\t\tAlan Cox\t:\tMulticast fixed (I hope)\n * \t\tPavel Krauz\t:\tLimited broadcast fixed\n *\t\tMike McLagan\t:\tRouting by source\n *\tAlexey Kuznetsov\t:\tEnd of old history. Split to fib.c and\n *\t\t\t\t\troute.c and rewritten from scratch.\n *\t\tAndi Kleen\t:\tLoad-limit warning messages.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year coma.\n *\tVitaly E. Lavrov\t:\tRace condition in ip_route_input_slow.\n *\tTobias Ringstrom\t:\tUninitialized res.type in ip_route_output_slow.\n *\tVladimir V. Ivanov\t:\tIP rule info (flowid) is really useful.\n *\t\tMarc Boucher\t:\trouting by fwmark\n *\tRobert Olsson\t\t:\tAdded rt_cache statistics\n *\tArnaldo C. Melo\t\t:\tConvert proc stuff to seq_file\n *\tEric Dumazet\t\t:\thashed spinlocks and rt_check_expire() fixes.\n * \tIlia Sotnikov\t\t:\tIgnore TOS on PMTUD and Redirect\n * \tIlia Sotnikov\t\t:\tRemoved TOS from hash calculations\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) \"IPv4: \" fmt\n\n#include <linux/module.h>\n#include <linux/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/string.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/proc_fs.h>\n#include <linux/init.h>\n#include <linux/skbuff.h>\n#include <linux/inetdevice.h>\n#include <linux/igmp.h>\n#include <linux/pkt_sched.h>\n#include <linux/mroute.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/random.h>\n#include <linux/rcupdate.h>\n#include <linux/times.h>\n#include <linux/slab.h>\n#include <linux/jhash.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/net_namespace.h>\n#include <net/protocol.h>\n#include <net/ip.h>\n#include <net/route.h>\n#include <net/inetpeer.h>\n#include <net/sock.h>\n#include <net/ip_fib.h>\n#include <net/arp.h>\n#include <net/tcp.h>\n#include <net/icmp.h>\n#include <net/xfrm.h>\n#include <net/lwtunnel.h>\n#include <net/netevent.h>\n#include <net/rtnetlink.h>\n#ifdef CONFIG_SYSCTL\n#include <linux/sysctl.h>\n#include <linux/kmemleak.h>\n#endif\n#include <net/secure_seq.h>\n#include <net/ip_tunnels.h>\n#include <net/l3mdev.h>\n\n#include \"fib_lookup.h\"\n\n#define RT_FL_TOS(oldflp4) \\\n\t((oldflp4)->flowi4_tos & (IPTOS_RT_MASK | RTO_ONLINK))\n\n#define RT_GC_TIMEOUT (300*HZ)\n\nstatic int ip_rt_max_size;\nstatic int ip_rt_redirect_number __read_mostly\t= 9;\nstatic int ip_rt_redirect_load __read_mostly\t= HZ / 50;\nstatic int ip_rt_redirect_silence __read_mostly\t= ((HZ / 50) << (9 + 1));\nstatic int ip_rt_error_cost __read_mostly\t= HZ;\nstatic int ip_rt_error_burst __read_mostly\t= 5 * HZ;\nstatic int ip_rt_mtu_expires __read_mostly\t= 10 * 60 * HZ;\nstatic int ip_rt_min_pmtu __read_mostly\t\t= 512 + 20 + 20;\nstatic int ip_rt_min_advmss __read_mostly\t= 256;\n\nstatic int ip_rt_gc_timeout __read_mostly\t= RT_GC_TIMEOUT;\n/*\n *\tInterface to generic destination cache.\n */\n\nstatic struct dst_entry *ipv4_dst_check(struct dst_entry *dst, u32 cookie);\nstatic unsigned int\t ipv4_default_advmss(const struct dst_entry *dst);\nstatic unsigned int\t ipv4_mtu(const struct dst_entry *dst);\nstatic struct dst_entry *ipv4_negative_advice(struct dst_entry *dst);\nstatic void\t\t ipv4_link_failure(struct sk_buff *skb);\nstatic void\t\t ip_rt_update_pmtu(struct dst_entry *dst, struct sock *sk,\n\t\t\t\t\t   struct sk_buff *skb, u32 mtu);\nstatic void\t\t ip_do_redirect(struct dst_entry *dst, struct sock *sk,\n\t\t\t\t\tstruct sk_buff *skb);\nstatic void\t\tipv4_dst_destroy(struct dst_entry *dst);\n\nstatic u32 *ipv4_cow_metrics(struct dst_entry *dst, unsigned long old)\n{\n\tWARN_ON(1);\n\treturn NULL;\n}\n\nstatic struct neighbour *ipv4_neigh_lookup(const struct dst_entry *dst,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   const void *daddr);\nstatic void ipv4_confirm_neigh(const struct dst_entry *dst, const void *daddr);\n\nstatic struct dst_ops ipv4_dst_ops = {\n\t.family =\t\tAF_INET,\n\t.check =\t\tipv4_dst_check,\n\t.default_advmss =\tipv4_default_advmss,\n\t.mtu =\t\t\tipv4_mtu,\n\t.cow_metrics =\t\tipv4_cow_metrics,\n\t.destroy =\t\tipv4_dst_destroy,\n\t.negative_advice =\tipv4_negative_advice,\n\t.link_failure =\t\tipv4_link_failure,\n\t.update_pmtu =\t\tip_rt_update_pmtu,\n\t.redirect =\t\tip_do_redirect,\n\t.local_out =\t\t__ip_local_out,\n\t.neigh_lookup =\t\tipv4_neigh_lookup,\n\t.confirm_neigh =\tipv4_confirm_neigh,\n};\n\n#define ECN_OR_COST(class)\tTC_PRIO_##class\n\nconst __u8 ip_tos2prio[16] = {\n\tTC_PRIO_BESTEFFORT,\n\tECN_OR_COST(BESTEFFORT),\n\tTC_PRIO_BESTEFFORT,\n\tECN_OR_COST(BESTEFFORT),\n\tTC_PRIO_BULK,\n\tECN_OR_COST(BULK),\n\tTC_PRIO_BULK,\n\tECN_OR_COST(BULK),\n\tTC_PRIO_INTERACTIVE,\n\tECN_OR_COST(INTERACTIVE),\n\tTC_PRIO_INTERACTIVE,\n\tECN_OR_COST(INTERACTIVE),\n\tTC_PRIO_INTERACTIVE_BULK,\n\tECN_OR_COST(INTERACTIVE_BULK),\n\tTC_PRIO_INTERACTIVE_BULK,\n\tECN_OR_COST(INTERACTIVE_BULK)\n};\nEXPORT_SYMBOL(ip_tos2prio);\n\nstatic DEFINE_PER_CPU(struct rt_cache_stat, rt_cache_stat);\n#define RT_CACHE_STAT_INC(field) raw_cpu_inc(rt_cache_stat.field)\n\n#ifdef CONFIG_PROC_FS\nstatic void *rt_cache_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tif (*pos)\n\t\treturn NULL;\n\treturn SEQ_START_TOKEN;\n}\n\nstatic void *rt_cache_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\treturn NULL;\n}\n\nstatic void rt_cache_seq_stop(struct seq_file *seq, void *v)\n{\n}\n\nstatic int rt_cache_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_printf(seq, \"%-127s\\n\",\n\t\t\t   \"Iface\\tDestination\\tGateway \\tFlags\\t\\tRefCnt\\tUse\\t\"\n\t\t\t   \"Metric\\tSource\\t\\tMTU\\tWindow\\tIRTT\\tTOS\\tHHRef\\t\"\n\t\t\t   \"HHUptod\\tSpecDst\");\n\treturn 0;\n}\n\nstatic const struct seq_operations rt_cache_seq_ops = {\n\t.start  = rt_cache_seq_start,\n\t.next   = rt_cache_seq_next,\n\t.stop   = rt_cache_seq_stop,\n\t.show   = rt_cache_seq_show,\n};\n\nstatic int rt_cache_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &rt_cache_seq_ops);\n}\n\nstatic const struct file_operations rt_cache_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open\t = rt_cache_seq_open,\n\t.read\t = seq_read,\n\t.llseek\t = seq_lseek,\n\t.release = seq_release,\n};\n\n\nstatic void *rt_cpu_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tint cpu;\n\n\tif (*pos == 0)\n\t\treturn SEQ_START_TOKEN;\n\n\tfor (cpu = *pos-1; cpu < nr_cpu_ids; ++cpu) {\n\t\tif (!cpu_possible(cpu))\n\t\t\tcontinue;\n\t\t*pos = cpu+1;\n\t\treturn &per_cpu(rt_cache_stat, cpu);\n\t}\n\treturn NULL;\n}\n\nstatic void *rt_cpu_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tint cpu;\n\n\tfor (cpu = *pos; cpu < nr_cpu_ids; ++cpu) {\n\t\tif (!cpu_possible(cpu))\n\t\t\tcontinue;\n\t\t*pos = cpu+1;\n\t\treturn &per_cpu(rt_cache_stat, cpu);\n\t}\n\treturn NULL;\n\n}\n\nstatic void rt_cpu_seq_stop(struct seq_file *seq, void *v)\n{\n\n}\n\nstatic int rt_cpu_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct rt_cache_stat *st = v;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_printf(seq, \"entries  in_hit in_slow_tot in_slow_mc in_no_route in_brd in_martian_dst in_martian_src  out_hit out_slow_tot out_slow_mc  gc_total gc_ignored gc_goal_miss gc_dst_overflow in_hlist_search out_hlist_search\\n\");\n\t\treturn 0;\n\t}\n\n\tseq_printf(seq,\"%08x  %08x %08x %08x %08x %08x %08x %08x \"\n\t\t   \" %08x %08x %08x %08x %08x %08x %08x %08x %08x \\n\",\n\t\t   dst_entries_get_slow(&ipv4_dst_ops),\n\t\t   0, /* st->in_hit */\n\t\t   st->in_slow_tot,\n\t\t   st->in_slow_mc,\n\t\t   st->in_no_route,\n\t\t   st->in_brd,\n\t\t   st->in_martian_dst,\n\t\t   st->in_martian_src,\n\n\t\t   0, /* st->out_hit */\n\t\t   st->out_slow_tot,\n\t\t   st->out_slow_mc,\n\n\t\t   0, /* st->gc_total */\n\t\t   0, /* st->gc_ignored */\n\t\t   0, /* st->gc_goal_miss */\n\t\t   0, /* st->gc_dst_overflow */\n\t\t   0, /* st->in_hlist_search */\n\t\t   0  /* st->out_hlist_search */\n\t\t);\n\treturn 0;\n}\n\nstatic const struct seq_operations rt_cpu_seq_ops = {\n\t.start  = rt_cpu_seq_start,\n\t.next   = rt_cpu_seq_next,\n\t.stop   = rt_cpu_seq_stop,\n\t.show   = rt_cpu_seq_show,\n};\n\n\nstatic int rt_cpu_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &rt_cpu_seq_ops);\n}\n\nstatic const struct file_operations rt_cpu_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open\t = rt_cpu_seq_open,\n\t.read\t = seq_read,\n\t.llseek\t = seq_lseek,\n\t.release = seq_release,\n};\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\nstatic int rt_acct_proc_show(struct seq_file *m, void *v)\n{\n\tstruct ip_rt_acct *dst, *src;\n\tunsigned int i, j;\n\n\tdst = kcalloc(256, sizeof(struct ip_rt_acct), GFP_KERNEL);\n\tif (!dst)\n\t\treturn -ENOMEM;\n\n\tfor_each_possible_cpu(i) {\n\t\tsrc = (struct ip_rt_acct *)per_cpu_ptr(ip_rt_acct, i);\n\t\tfor (j = 0; j < 256; j++) {\n\t\t\tdst[j].o_bytes   += src[j].o_bytes;\n\t\t\tdst[j].o_packets += src[j].o_packets;\n\t\t\tdst[j].i_bytes   += src[j].i_bytes;\n\t\t\tdst[j].i_packets += src[j].i_packets;\n\t\t}\n\t}\n\n\tseq_write(m, dst, 256 * sizeof(struct ip_rt_acct));\n\tkfree(dst);\n\treturn 0;\n}\n\nstatic int rt_acct_proc_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, rt_acct_proc_show, NULL);\n}\n\nstatic const struct file_operations rt_acct_proc_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= rt_acct_proc_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n#endif\n\nstatic int __net_init ip_rt_do_proc_init(struct net *net)\n{\n\tstruct proc_dir_entry *pde;\n\n\tpde = proc_create(\"rt_cache\", S_IRUGO, net->proc_net,\n\t\t\t  &rt_cache_seq_fops);\n\tif (!pde)\n\t\tgoto err1;\n\n\tpde = proc_create(\"rt_cache\", S_IRUGO,\n\t\t\t  net->proc_net_stat, &rt_cpu_seq_fops);\n\tif (!pde)\n\t\tgoto err2;\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tpde = proc_create(\"rt_acct\", 0, net->proc_net, &rt_acct_proc_fops);\n\tif (!pde)\n\t\tgoto err3;\n#endif\n\treturn 0;\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\nerr3:\n\tremove_proc_entry(\"rt_cache\", net->proc_net_stat);\n#endif\nerr2:\n\tremove_proc_entry(\"rt_cache\", net->proc_net);\nerr1:\n\treturn -ENOMEM;\n}\n\nstatic void __net_exit ip_rt_do_proc_exit(struct net *net)\n{\n\tremove_proc_entry(\"rt_cache\", net->proc_net_stat);\n\tremove_proc_entry(\"rt_cache\", net->proc_net);\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tremove_proc_entry(\"rt_acct\", net->proc_net);\n#endif\n}\n\nstatic struct pernet_operations ip_rt_proc_ops __net_initdata =  {\n\t.init = ip_rt_do_proc_init,\n\t.exit = ip_rt_do_proc_exit,\n};\n\nstatic int __init ip_rt_proc_init(void)\n{\n\treturn register_pernet_subsys(&ip_rt_proc_ops);\n}\n\n#else\nstatic inline int ip_rt_proc_init(void)\n{\n\treturn 0;\n}\n#endif /* CONFIG_PROC_FS */\n\nstatic inline bool rt_is_expired(const struct rtable *rth)\n{\n\treturn rth->rt_genid != rt_genid_ipv4(dev_net(rth->dst.dev));\n}\n\nvoid rt_cache_flush(struct net *net)\n{\n\trt_genid_bump_ipv4(net);\n}\n\nstatic struct neighbour *ipv4_neigh_lookup(const struct dst_entry *dst,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   const void *daddr)\n{\n\tstruct net_device *dev = dst->dev;\n\tconst __be32 *pkey = daddr;\n\tconst struct rtable *rt;\n\tstruct neighbour *n;\n\n\trt = (const struct rtable *) dst;\n\tif (rt->rt_gateway)\n\t\tpkey = (const __be32 *) &rt->rt_gateway;\n\telse if (skb)\n\t\tpkey = &ip_hdr(skb)->daddr;\n\n\tn = __ipv4_neigh_lookup(dev, *(__force u32 *)pkey);\n\tif (n)\n\t\treturn n;\n\treturn neigh_create(&arp_tbl, pkey, dev);\n}\n\nstatic void ipv4_confirm_neigh(const struct dst_entry *dst, const void *daddr)\n{\n\tstruct net_device *dev = dst->dev;\n\tconst __be32 *pkey = daddr;\n\tconst struct rtable *rt;\n\n\trt = (const struct rtable *)dst;\n\tif (rt->rt_gateway)\n\t\tpkey = (const __be32 *)&rt->rt_gateway;\n\telse if (!daddr ||\n\t\t (rt->rt_flags &\n\t\t  (RTCF_MULTICAST | RTCF_BROADCAST | RTCF_LOCAL)))\n\t\treturn;\n\n\t__ipv4_confirm_neigh(dev, *(__force u32 *)pkey);\n}\n\n#define IP_IDENTS_SZ 2048u\n\nstatic atomic_t *ip_idents __read_mostly;\nstatic u32 *ip_tstamps __read_mostly;\n\n/* In order to protect privacy, we add a perturbation to identifiers\n * if one generator is seldom used. This makes hard for an attacker\n * to infer how many packets were sent between two points in time.\n */\nu32 ip_idents_reserve(u32 hash, int segs)\n{\n\tu32 *p_tstamp = ip_tstamps + hash % IP_IDENTS_SZ;\n\tatomic_t *p_id = ip_idents + hash % IP_IDENTS_SZ;\n\tu32 old = ACCESS_ONCE(*p_tstamp);\n\tu32 now = (u32)jiffies;\n\tu32 new, delta = 0;\n\n\tif (old != now && cmpxchg(p_tstamp, old, now) == old)\n\t\tdelta = prandom_u32_max(now - old);\n\n\t/* Do not use atomic_add_return() as it makes UBSAN unhappy */\n\tdo {\n\t\told = (u32)atomic_read(p_id);\n\t\tnew = old + delta + segs;\n\t} while (atomic_cmpxchg(p_id, old, new) != old);\n\n\treturn new - segs;\n}\nEXPORT_SYMBOL(ip_idents_reserve);\n\nvoid __ip_select_ident(struct net *net, struct iphdr *iph, int segs)\n{\n\tstatic u32 ip_idents_hashrnd __read_mostly;\n\tu32 hash, id;\n\n\tnet_get_random_once(&ip_idents_hashrnd, sizeof(ip_idents_hashrnd));\n\n\thash = jhash_3words((__force u32)iph->daddr,\n\t\t\t    (__force u32)iph->saddr,\n\t\t\t    iph->protocol ^ net_hash_mix(net),\n\t\t\t    ip_idents_hashrnd);\n\tid = ip_idents_reserve(hash, segs);\n\tiph->id = htons(id);\n}\nEXPORT_SYMBOL(__ip_select_ident);\n\nstatic void __build_flow_key(const struct net *net, struct flowi4 *fl4,\n\t\t\t     const struct sock *sk,\n\t\t\t     const struct iphdr *iph,\n\t\t\t     int oif, u8 tos,\n\t\t\t     u8 prot, u32 mark, int flow_flags)\n{\n\tif (sk) {\n\t\tconst struct inet_sock *inet = inet_sk(sk);\n\n\t\toif = sk->sk_bound_dev_if;\n\t\tmark = sk->sk_mark;\n\t\ttos = RT_CONN_FLAGS(sk);\n\t\tprot = inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol;\n\t}\n\tflowi4_init_output(fl4, oif, mark, tos,\n\t\t\t   RT_SCOPE_UNIVERSE, prot,\n\t\t\t   flow_flags,\n\t\t\t   iph->daddr, iph->saddr, 0, 0,\n\t\t\t   sock_net_uid(net, sk));\n}\n\nstatic void build_skb_flow_key(struct flowi4 *fl4, const struct sk_buff *skb,\n\t\t\t       const struct sock *sk)\n{\n\tconst struct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tint oif = skb->dev->ifindex;\n\tu8 tos = RT_TOS(iph->tos);\n\tu8 prot = iph->protocol;\n\tu32 mark = skb->mark;\n\n\t__build_flow_key(net, fl4, sk, iph, oif, tos, prot, mark, 0);\n}\n\nstatic void build_sk_flow_key(struct flowi4 *fl4, const struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\tflowi4_init_output(fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t   inet_sk_flowi_flags(sk),\n\t\t\t   daddr, inet->inet_saddr, 0, 0, sk->sk_uid);\n\trcu_read_unlock();\n}\n\nstatic void ip_rt_build_flow_key(struct flowi4 *fl4, const struct sock *sk,\n\t\t\t\t const struct sk_buff *skb)\n{\n\tif (skb)\n\t\tbuild_skb_flow_key(fl4, skb, sk);\n\telse\n\t\tbuild_sk_flow_key(fl4, sk);\n}\n\nstatic DEFINE_SPINLOCK(fnhe_lock);\n\nstatic void fnhe_flush_routes(struct fib_nh_exception *fnhe)\n{\n\tstruct rtable *rt;\n\n\trt = rcu_dereference(fnhe->fnhe_rth_input);\n\tif (rt) {\n\t\tRCU_INIT_POINTER(fnhe->fnhe_rth_input, NULL);\n\t\tdst_dev_put(&rt->dst);\n\t\tdst_release(&rt->dst);\n\t}\n\trt = rcu_dereference(fnhe->fnhe_rth_output);\n\tif (rt) {\n\t\tRCU_INIT_POINTER(fnhe->fnhe_rth_output, NULL);\n\t\tdst_dev_put(&rt->dst);\n\t\tdst_release(&rt->dst);\n\t}\n}\n\nstatic struct fib_nh_exception *fnhe_oldest(struct fnhe_hash_bucket *hash)\n{\n\tstruct fib_nh_exception *fnhe, *oldest;\n\n\toldest = rcu_dereference(hash->chain);\n\tfor (fnhe = rcu_dereference(oldest->fnhe_next); fnhe;\n\t     fnhe = rcu_dereference(fnhe->fnhe_next)) {\n\t\tif (time_before(fnhe->fnhe_stamp, oldest->fnhe_stamp))\n\t\t\toldest = fnhe;\n\t}\n\tfnhe_flush_routes(oldest);\n\treturn oldest;\n}\n\nstatic inline u32 fnhe_hashfun(__be32 daddr)\n{\n\tstatic u32 fnhe_hashrnd __read_mostly;\n\tu32 hval;\n\n\tnet_get_random_once(&fnhe_hashrnd, sizeof(fnhe_hashrnd));\n\thval = jhash_1word((__force u32) daddr, fnhe_hashrnd);\n\treturn hash_32(hval, FNHE_HASH_SHIFT);\n}\n\nstatic void fill_route_from_fnhe(struct rtable *rt, struct fib_nh_exception *fnhe)\n{\n\trt->rt_pmtu = fnhe->fnhe_pmtu;\n\trt->dst.expires = fnhe->fnhe_expires;\n\n\tif (fnhe->fnhe_gw) {\n\t\trt->rt_flags |= RTCF_REDIRECTED;\n\t\trt->rt_gateway = fnhe->fnhe_gw;\n\t\trt->rt_uses_gateway = 1;\n\t}\n}\n\nstatic void update_or_create_fnhe(struct fib_nh *nh, __be32 daddr, __be32 gw,\n\t\t\t\t  u32 pmtu, unsigned long expires)\n{\n\tstruct fnhe_hash_bucket *hash;\n\tstruct fib_nh_exception *fnhe;\n\tstruct rtable *rt;\n\tunsigned int i;\n\tint depth;\n\tu32 hval = fnhe_hashfun(daddr);\n\n\tspin_lock_bh(&fnhe_lock);\n\n\thash = rcu_dereference(nh->nh_exceptions);\n\tif (!hash) {\n\t\thash = kzalloc(FNHE_HASH_SIZE * sizeof(*hash), GFP_ATOMIC);\n\t\tif (!hash)\n\t\t\tgoto out_unlock;\n\t\trcu_assign_pointer(nh->nh_exceptions, hash);\n\t}\n\n\thash += hval;\n\n\tdepth = 0;\n\tfor (fnhe = rcu_dereference(hash->chain); fnhe;\n\t     fnhe = rcu_dereference(fnhe->fnhe_next)) {\n\t\tif (fnhe->fnhe_daddr == daddr)\n\t\t\tbreak;\n\t\tdepth++;\n\t}\n\n\tif (fnhe) {\n\t\tif (gw)\n\t\t\tfnhe->fnhe_gw = gw;\n\t\tif (pmtu) {\n\t\t\tfnhe->fnhe_pmtu = pmtu;\n\t\t\tfnhe->fnhe_expires = max(1UL, expires);\n\t\t}\n\t\t/* Update all cached dsts too */\n\t\trt = rcu_dereference(fnhe->fnhe_rth_input);\n\t\tif (rt)\n\t\t\tfill_route_from_fnhe(rt, fnhe);\n\t\trt = rcu_dereference(fnhe->fnhe_rth_output);\n\t\tif (rt)\n\t\t\tfill_route_from_fnhe(rt, fnhe);\n\t} else {\n\t\tif (depth > FNHE_RECLAIM_DEPTH)\n\t\t\tfnhe = fnhe_oldest(hash);\n\t\telse {\n\t\t\tfnhe = kzalloc(sizeof(*fnhe), GFP_ATOMIC);\n\t\t\tif (!fnhe)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tfnhe->fnhe_next = hash->chain;\n\t\t\trcu_assign_pointer(hash->chain, fnhe);\n\t\t}\n\t\tfnhe->fnhe_genid = fnhe_genid(dev_net(nh->nh_dev));\n\t\tfnhe->fnhe_daddr = daddr;\n\t\tfnhe->fnhe_gw = gw;\n\t\tfnhe->fnhe_pmtu = pmtu;\n\t\tfnhe->fnhe_expires = expires;\n\n\t\t/* Exception created; mark the cached routes for the nexthop\n\t\t * stale, so anyone caching it rechecks if this exception\n\t\t * applies to them.\n\t\t */\n\t\trt = rcu_dereference(nh->nh_rth_input);\n\t\tif (rt)\n\t\t\trt->dst.obsolete = DST_OBSOLETE_KILL;\n\n\t\tfor_each_possible_cpu(i) {\n\t\t\tstruct rtable __rcu **prt;\n\t\t\tprt = per_cpu_ptr(nh->nh_pcpu_rth_output, i);\n\t\t\trt = rcu_dereference(*prt);\n\t\t\tif (rt)\n\t\t\t\trt->dst.obsolete = DST_OBSOLETE_KILL;\n\t\t}\n\t}\n\n\tfnhe->fnhe_stamp = jiffies;\n\nout_unlock:\n\tspin_unlock_bh(&fnhe_lock);\n}\n\nstatic void __ip_do_redirect(struct rtable *rt, struct sk_buff *skb, struct flowi4 *fl4,\n\t\t\t     bool kill_route)\n{\n\t__be32 new_gw = icmp_hdr(skb)->un.gateway;\n\t__be32 old_gw = ip_hdr(skb)->saddr;\n\tstruct net_device *dev = skb->dev;\n\tstruct in_device *in_dev;\n\tstruct fib_result res;\n\tstruct neighbour *n;\n\tstruct net *net;\n\n\tswitch (icmp_hdr(skb)->code & 7) {\n\tcase ICMP_REDIR_NET:\n\tcase ICMP_REDIR_NETTOS:\n\tcase ICMP_REDIR_HOST:\n\tcase ICMP_REDIR_HOSTTOS:\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (rt->rt_gateway != old_gw)\n\t\treturn;\n\n\tin_dev = __in_dev_get_rcu(dev);\n\tif (!in_dev)\n\t\treturn;\n\n\tnet = dev_net(dev);\n\tif (new_gw == old_gw || !IN_DEV_RX_REDIRECTS(in_dev) ||\n\t    ipv4_is_multicast(new_gw) || ipv4_is_lbcast(new_gw) ||\n\t    ipv4_is_zeronet(new_gw))\n\t\tgoto reject_redirect;\n\n\tif (!IN_DEV_SHARED_MEDIA(in_dev)) {\n\t\tif (!inet_addr_onlink(in_dev, new_gw, old_gw))\n\t\t\tgoto reject_redirect;\n\t\tif (IN_DEV_SEC_REDIRECTS(in_dev) && ip_fib_check_default(new_gw, dev))\n\t\t\tgoto reject_redirect;\n\t} else {\n\t\tif (inet_addr_type(net, new_gw) != RTN_UNICAST)\n\t\t\tgoto reject_redirect;\n\t}\n\n\tn = __ipv4_neigh_lookup(rt->dst.dev, new_gw);\n\tif (!n)\n\t\tn = neigh_create(&arp_tbl, &new_gw, rt->dst.dev);\n\tif (!IS_ERR(n)) {\n\t\tif (!(n->nud_state & NUD_VALID)) {\n\t\t\tneigh_event_send(n, NULL);\n\t\t} else {\n\t\t\tif (fib_lookup(net, fl4, &res, 0) == 0) {\n\t\t\t\tstruct fib_nh *nh = &FIB_RES_NH(res);\n\n\t\t\t\tupdate_or_create_fnhe(nh, fl4->daddr, new_gw,\n\t\t\t\t\t\t0, jiffies + ip_rt_gc_timeout);\n\t\t\t}\n\t\t\tif (kill_route)\n\t\t\t\trt->dst.obsolete = DST_OBSOLETE_KILL;\n\t\t\tcall_netevent_notifiers(NETEVENT_NEIGH_UPDATE, n);\n\t\t}\n\t\tneigh_release(n);\n\t}\n\treturn;\n\nreject_redirect:\n#ifdef CONFIG_IP_ROUTE_VERBOSE\n\tif (IN_DEV_LOG_MARTIANS(in_dev)) {\n\t\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\t\t__be32 daddr = iph->daddr;\n\t\t__be32 saddr = iph->saddr;\n\n\t\tnet_info_ratelimited(\"Redirect from %pI4 on %s about %pI4 ignored\\n\"\n\t\t\t\t     \"  Advised path = %pI4 -> %pI4\\n\",\n\t\t\t\t     &old_gw, dev->name, &new_gw,\n\t\t\t\t     &saddr, &daddr);\n\t}\n#endif\n\t;\n}\n\nstatic void ip_do_redirect(struct dst_entry *dst, struct sock *sk, struct sk_buff *skb)\n{\n\tstruct rtable *rt;\n\tstruct flowi4 fl4;\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct net *net = dev_net(skb->dev);\n\tint oif = skb->dev->ifindex;\n\tu8 tos = RT_TOS(iph->tos);\n\tu8 prot = iph->protocol;\n\tu32 mark = skb->mark;\n\n\trt = (struct rtable *) dst;\n\n\t__build_flow_key(net, &fl4, sk, iph, oif, tos, prot, mark, 0);\n\t__ip_do_redirect(rt, skb, &fl4, true);\n}\n\nstatic struct dst_entry *ipv4_negative_advice(struct dst_entry *dst)\n{\n\tstruct rtable *rt = (struct rtable *)dst;\n\tstruct dst_entry *ret = dst;\n\n\tif (rt) {\n\t\tif (dst->obsolete > 0) {\n\t\t\tip_rt_put(rt);\n\t\t\tret = NULL;\n\t\t} else if ((rt->rt_flags & RTCF_REDIRECTED) ||\n\t\t\t   rt->dst.expires) {\n\t\t\tip_rt_put(rt);\n\t\t\tret = NULL;\n\t\t}\n\t}\n\treturn ret;\n}\n\n/*\n * Algorithm:\n *\t1. The first ip_rt_redirect_number redirects are sent\n *\t   with exponential backoff, then we stop sending them at all,\n *\t   assuming that the host ignores our redirects.\n *\t2. If we did not see packets requiring redirects\n *\t   during ip_rt_redirect_silence, we assume that the host\n *\t   forgot redirected route and start to send redirects again.\n *\n * This algorithm is much cheaper and more intelligent than dumb load limiting\n * in icmp.c.\n *\n * NOTE. Do not forget to inhibit load limiting for redirects (redundant)\n * and \"frag. need\" (breaks PMTU discovery) in icmp.c.\n */\n\nvoid ip_rt_send_redirect(struct sk_buff *skb)\n{\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct in_device *in_dev;\n\tstruct inet_peer *peer;\n\tstruct net *net;\n\tint log_martians;\n\tint vif;\n\n\trcu_read_lock();\n\tin_dev = __in_dev_get_rcu(rt->dst.dev);\n\tif (!in_dev || !IN_DEV_TX_REDIRECTS(in_dev)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlog_martians = IN_DEV_LOG_MARTIANS(in_dev);\n\tvif = l3mdev_master_ifindex_rcu(rt->dst.dev);\n\trcu_read_unlock();\n\n\tnet = dev_net(rt->dst.dev);\n\tpeer = inet_getpeer_v4(net->ipv4.peers, ip_hdr(skb)->saddr, vif, 1);\n\tif (!peer) {\n\t\ticmp_send(skb, ICMP_REDIRECT, ICMP_REDIR_HOST,\n\t\t\t  rt_nexthop(rt, ip_hdr(skb)->daddr));\n\t\treturn;\n\t}\n\n\t/* No redirected packets during ip_rt_redirect_silence;\n\t * reset the algorithm.\n\t */\n\tif (time_after(jiffies, peer->rate_last + ip_rt_redirect_silence))\n\t\tpeer->rate_tokens = 0;\n\n\t/* Too many ignored redirects; do not send anything\n\t * set dst.rate_last to the last seen redirected packet.\n\t */\n\tif (peer->rate_tokens >= ip_rt_redirect_number) {\n\t\tpeer->rate_last = jiffies;\n\t\tgoto out_put_peer;\n\t}\n\n\t/* Check for load limit; set rate_last to the latest sent\n\t * redirect.\n\t */\n\tif (peer->rate_tokens == 0 ||\n\t    time_after(jiffies,\n\t\t       (peer->rate_last +\n\t\t\t(ip_rt_redirect_load << peer->rate_tokens)))) {\n\t\t__be32 gw = rt_nexthop(rt, ip_hdr(skb)->daddr);\n\n\t\ticmp_send(skb, ICMP_REDIRECT, ICMP_REDIR_HOST, gw);\n\t\tpeer->rate_last = jiffies;\n\t\t++peer->rate_tokens;\n#ifdef CONFIG_IP_ROUTE_VERBOSE\n\t\tif (log_martians &&\n\t\t    peer->rate_tokens == ip_rt_redirect_number)\n\t\t\tnet_warn_ratelimited(\"host %pI4/if%d ignores redirects for %pI4 to %pI4\\n\",\n\t\t\t\t\t     &ip_hdr(skb)->saddr, inet_iif(skb),\n\t\t\t\t\t     &ip_hdr(skb)->daddr, &gw);\n#endif\n\t}\nout_put_peer:\n\tinet_putpeer(peer);\n}\n\nstatic int ip_error(struct sk_buff *skb)\n{\n\tstruct in_device *in_dev = __in_dev_get_rcu(skb->dev);\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct inet_peer *peer;\n\tunsigned long now;\n\tstruct net *net;\n\tbool send;\n\tint code;\n\n\t/* IP on this device is disabled. */\n\tif (!in_dev)\n\t\tgoto out;\n\n\tnet = dev_net(rt->dst.dev);\n\tif (!IN_DEV_FORWARD(in_dev)) {\n\t\tswitch (rt->dst.error) {\n\t\tcase EHOSTUNREACH:\n\t\t\t__IP_INC_STATS(net, IPSTATS_MIB_INADDRERRORS);\n\t\t\tbreak;\n\n\t\tcase ENETUNREACH:\n\t\t\t__IP_INC_STATS(net, IPSTATS_MIB_INNOROUTES);\n\t\t\tbreak;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tswitch (rt->dst.error) {\n\tcase EINVAL:\n\tdefault:\n\t\tgoto out;\n\tcase EHOSTUNREACH:\n\t\tcode = ICMP_HOST_UNREACH;\n\t\tbreak;\n\tcase ENETUNREACH:\n\t\tcode = ICMP_NET_UNREACH;\n\t\t__IP_INC_STATS(net, IPSTATS_MIB_INNOROUTES);\n\t\tbreak;\n\tcase EACCES:\n\t\tcode = ICMP_PKT_FILTERED;\n\t\tbreak;\n\t}\n\n\tpeer = inet_getpeer_v4(net->ipv4.peers, ip_hdr(skb)->saddr,\n\t\t\t       l3mdev_master_ifindex(skb->dev), 1);\n\n\tsend = true;\n\tif (peer) {\n\t\tnow = jiffies;\n\t\tpeer->rate_tokens += now - peer->rate_last;\n\t\tif (peer->rate_tokens > ip_rt_error_burst)\n\t\t\tpeer->rate_tokens = ip_rt_error_burst;\n\t\tpeer->rate_last = now;\n\t\tif (peer->rate_tokens >= ip_rt_error_cost)\n\t\t\tpeer->rate_tokens -= ip_rt_error_cost;\n\t\telse\n\t\t\tsend = false;\n\t\tinet_putpeer(peer);\n\t}\n\tif (send)\n\t\ticmp_send(skb, ICMP_DEST_UNREACH, code, 0);\n\nout:\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic void __ip_rt_update_pmtu(struct rtable *rt, struct flowi4 *fl4, u32 mtu)\n{\n\tstruct dst_entry *dst = &rt->dst;\n\tstruct fib_result res;\n\n\tif (dst_metric_locked(dst, RTAX_MTU))\n\t\treturn;\n\n\tif (ipv4_mtu(dst) < mtu)\n\t\treturn;\n\n\tif (mtu < ip_rt_min_pmtu)\n\t\tmtu = ip_rt_min_pmtu;\n\n\tif (rt->rt_pmtu == mtu &&\n\t    time_before(jiffies, dst->expires - ip_rt_mtu_expires / 2))\n\t\treturn;\n\n\trcu_read_lock();\n\tif (fib_lookup(dev_net(dst->dev), fl4, &res, 0) == 0) {\n\t\tstruct fib_nh *nh = &FIB_RES_NH(res);\n\n\t\tupdate_or_create_fnhe(nh, fl4->daddr, 0, mtu,\n\t\t\t\t      jiffies + ip_rt_mtu_expires);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void ip_rt_update_pmtu(struct dst_entry *dst, struct sock *sk,\n\t\t\t      struct sk_buff *skb, u32 mtu)\n{\n\tstruct rtable *rt = (struct rtable *) dst;\n\tstruct flowi4 fl4;\n\n\tip_rt_build_flow_key(&fl4, sk, skb);\n\t__ip_rt_update_pmtu(rt, &fl4, mtu);\n}\n\nvoid ipv4_update_pmtu(struct sk_buff *skb, struct net *net, u32 mtu,\n\t\t      int oif, u32 mark, u8 protocol, int flow_flags)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\tif (!mark)\n\t\tmark = IP4_REPLY_MARK(net, skb->mark);\n\n\t__build_flow_key(net, &fl4, NULL, iph, oif,\n\t\t\t RT_TOS(iph->tos), protocol, mark, flow_flags);\n\trt = __ip_route_output_key(net, &fl4);\n\tif (!IS_ERR(rt)) {\n\t\t__ip_rt_update_pmtu(rt, &fl4, mtu);\n\t\tip_rt_put(rt);\n\t}\n}\nEXPORT_SYMBOL_GPL(ipv4_update_pmtu);\n\nstatic void __ipv4_sk_update_pmtu(struct sk_buff *skb, struct sock *sk, u32 mtu)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\t__build_flow_key(sock_net(sk), &fl4, sk, iph, 0, 0, 0, 0, 0);\n\n\tif (!fl4.flowi4_mark)\n\t\tfl4.flowi4_mark = IP4_REPLY_MARK(sock_net(sk), skb->mark);\n\n\trt = __ip_route_output_key(sock_net(sk), &fl4);\n\tif (!IS_ERR(rt)) {\n\t\t__ip_rt_update_pmtu(rt, &fl4, mtu);\n\t\tip_rt_put(rt);\n\t}\n}\n\nvoid ipv4_sk_update_pmtu(struct sk_buff *skb, struct sock *sk, u32 mtu)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tstruct dst_entry *odst = NULL;\n\tbool new = false;\n\tstruct net *net = sock_net(sk);\n\n\tbh_lock_sock(sk);\n\n\tif (!ip_sk_accept_pmtu(sk))\n\t\tgoto out;\n\n\todst = sk_dst_get(sk);\n\n\tif (sock_owned_by_user(sk) || !odst) {\n\t\t__ipv4_sk_update_pmtu(skb, sk, mtu);\n\t\tgoto out;\n\t}\n\n\t__build_flow_key(net, &fl4, sk, iph, 0, 0, 0, 0, 0);\n\n\trt = (struct rtable *)odst;\n\tif (odst->obsolete && !odst->ops->check(odst, 0)) {\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out;\n\n\t\tnew = true;\n\t}\n\n\t__ip_rt_update_pmtu((struct rtable *) rt->dst.path, &fl4, mtu);\n\n\tif (!dst_check(&rt->dst, 0)) {\n\t\tif (new)\n\t\t\tdst_release(&rt->dst);\n\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out;\n\n\t\tnew = true;\n\t}\n\n\tif (new)\n\t\tsk_dst_set(sk, &rt->dst);\n\nout:\n\tbh_unlock_sock(sk);\n\tdst_release(odst);\n}\nEXPORT_SYMBOL_GPL(ipv4_sk_update_pmtu);\n\nvoid ipv4_redirect(struct sk_buff *skb, struct net *net,\n\t\t   int oif, u32 mark, u8 protocol, int flow_flags)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\t__build_flow_key(net, &fl4, NULL, iph, oif,\n\t\t\t RT_TOS(iph->tos), protocol, mark, flow_flags);\n\trt = __ip_route_output_key(net, &fl4);\n\tif (!IS_ERR(rt)) {\n\t\t__ip_do_redirect(rt, skb, &fl4, false);\n\t\tip_rt_put(rt);\n\t}\n}\nEXPORT_SYMBOL_GPL(ipv4_redirect);\n\nvoid ipv4_sk_redirect(struct sk_buff *skb, struct sock *sk)\n{\n\tconst struct iphdr *iph = (const struct iphdr *) skb->data;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tstruct net *net = sock_net(sk);\n\n\t__build_flow_key(net, &fl4, sk, iph, 0, 0, 0, 0, 0);\n\trt = __ip_route_output_key(net, &fl4);\n\tif (!IS_ERR(rt)) {\n\t\t__ip_do_redirect(rt, skb, &fl4, false);\n\t\tip_rt_put(rt);\n\t}\n}\nEXPORT_SYMBOL_GPL(ipv4_sk_redirect);\n\nstatic struct dst_entry *ipv4_dst_check(struct dst_entry *dst, u32 cookie)\n{\n\tstruct rtable *rt = (struct rtable *) dst;\n\n\t/* All IPV4 dsts are created with ->obsolete set to the value\n\t * DST_OBSOLETE_FORCE_CHK which forces validation calls down\n\t * into this function always.\n\t *\n\t * When a PMTU/redirect information update invalidates a route,\n\t * this is indicated by setting obsolete to DST_OBSOLETE_KILL or\n\t * DST_OBSOLETE_DEAD by dst_free().\n\t */\n\tif (dst->obsolete != DST_OBSOLETE_FORCE_CHK || rt_is_expired(rt))\n\t\treturn NULL;\n\treturn dst;\n}\n\nstatic void ipv4_link_failure(struct sk_buff *skb)\n{\n\tstruct rtable *rt;\n\n\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_HOST_UNREACH, 0);\n\n\trt = skb_rtable(skb);\n\tif (rt)\n\t\tdst_set_expires(&rt->dst, 0);\n}\n\nstatic int ip_rt_bug(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tpr_debug(\"%s: %pI4 -> %pI4, %s\\n\",\n\t\t __func__, &ip_hdr(skb)->saddr, &ip_hdr(skb)->daddr,\n\t\t skb->dev ? skb->dev->name : \"?\");\n\tkfree_skb(skb);\n\tWARN_ON(1);\n\treturn 0;\n}\n\n/*\n   We do not cache source address of outgoing interface,\n   because it is used only by IP RR, TS and SRR options,\n   so that it out of fast path.\n\n   BTW remember: \"addr\" is allowed to be not aligned\n   in IP options!\n */\n\nvoid ip_rt_get_source(u8 *addr, struct sk_buff *skb, struct rtable *rt)\n{\n\t__be32 src;\n\n\tif (rt_is_output_route(rt))\n\t\tsrc = ip_hdr(skb)->saddr;\n\telse {\n\t\tstruct fib_result res;\n\t\tstruct flowi4 fl4;\n\t\tstruct iphdr *iph;\n\n\t\tiph = ip_hdr(skb);\n\n\t\tmemset(&fl4, 0, sizeof(fl4));\n\t\tfl4.daddr = iph->daddr;\n\t\tfl4.saddr = iph->saddr;\n\t\tfl4.flowi4_tos = RT_TOS(iph->tos);\n\t\tfl4.flowi4_oif = rt->dst.dev->ifindex;\n\t\tfl4.flowi4_iif = skb->dev->ifindex;\n\t\tfl4.flowi4_mark = skb->mark;\n\n\t\trcu_read_lock();\n\t\tif (fib_lookup(dev_net(rt->dst.dev), &fl4, &res, 0) == 0)\n\t\t\tsrc = FIB_RES_PREFSRC(dev_net(rt->dst.dev), res);\n\t\telse\n\t\t\tsrc = inet_select_addr(rt->dst.dev,\n\t\t\t\t\t       rt_nexthop(rt, iph->daddr),\n\t\t\t\t\t       RT_SCOPE_UNIVERSE);\n\t\trcu_read_unlock();\n\t}\n\tmemcpy(addr, &src, 4);\n}\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\nstatic void set_class_tag(struct rtable *rt, u32 tag)\n{\n\tif (!(rt->dst.tclassid & 0xFFFF))\n\t\trt->dst.tclassid |= tag & 0xFFFF;\n\tif (!(rt->dst.tclassid & 0xFFFF0000))\n\t\trt->dst.tclassid |= tag & 0xFFFF0000;\n}\n#endif\n\nstatic unsigned int ipv4_default_advmss(const struct dst_entry *dst)\n{\n\tunsigned int header_size = sizeof(struct tcphdr) + sizeof(struct iphdr);\n\tunsigned int advmss = max_t(unsigned int, dst->dev->mtu - header_size,\n\t\t\t\t    ip_rt_min_advmss);\n\n\treturn min(advmss, IPV4_MAX_PMTU - header_size);\n}\n\nstatic unsigned int ipv4_mtu(const struct dst_entry *dst)\n{\n\tconst struct rtable *rt = (const struct rtable *) dst;\n\tunsigned int mtu = rt->rt_pmtu;\n\n\tif (!mtu || time_after_eq(jiffies, rt->dst.expires))\n\t\tmtu = dst_metric_raw(dst, RTAX_MTU);\n\n\tif (mtu)\n\t\treturn mtu;\n\n\tmtu = READ_ONCE(dst->dev->mtu);\n\n\tif (unlikely(dst_metric_locked(dst, RTAX_MTU))) {\n\t\tif (rt->rt_uses_gateway && mtu > 576)\n\t\t\tmtu = 576;\n\t}\n\n\tmtu = min_t(unsigned int, mtu, IP_MAX_MTU);\n\n\treturn mtu - lwtunnel_headroom(dst->lwtstate, mtu);\n}\n\nstatic struct fib_nh_exception *find_exception(struct fib_nh *nh, __be32 daddr)\n{\n\tstruct fnhe_hash_bucket *hash = rcu_dereference(nh->nh_exceptions);\n\tstruct fib_nh_exception *fnhe;\n\tu32 hval;\n\n\tif (!hash)\n\t\treturn NULL;\n\n\thval = fnhe_hashfun(daddr);\n\n\tfor (fnhe = rcu_dereference(hash[hval].chain); fnhe;\n\t     fnhe = rcu_dereference(fnhe->fnhe_next)) {\n\t\tif (fnhe->fnhe_daddr == daddr)\n\t\t\treturn fnhe;\n\t}\n\treturn NULL;\n}\n\nstatic bool rt_bind_exception(struct rtable *rt, struct fib_nh_exception *fnhe,\n\t\t\t      __be32 daddr, const bool do_cache)\n{\n\tbool ret = false;\n\n\tspin_lock_bh(&fnhe_lock);\n\n\tif (daddr == fnhe->fnhe_daddr) {\n\t\tstruct rtable __rcu **porig;\n\t\tstruct rtable *orig;\n\t\tint genid = fnhe_genid(dev_net(rt->dst.dev));\n\n\t\tif (rt_is_input_route(rt))\n\t\t\tporig = &fnhe->fnhe_rth_input;\n\t\telse\n\t\t\tporig = &fnhe->fnhe_rth_output;\n\t\torig = rcu_dereference(*porig);\n\n\t\tif (fnhe->fnhe_genid != genid) {\n\t\t\tfnhe->fnhe_genid = genid;\n\t\t\tfnhe->fnhe_gw = 0;\n\t\t\tfnhe->fnhe_pmtu = 0;\n\t\t\tfnhe->fnhe_expires = 0;\n\t\t\tfnhe_flush_routes(fnhe);\n\t\t\torig = NULL;\n\t\t}\n\t\tfill_route_from_fnhe(rt, fnhe);\n\t\tif (!rt->rt_gateway)\n\t\t\trt->rt_gateway = daddr;\n\n\t\tif (do_cache) {\n\t\t\tdst_hold(&rt->dst);\n\t\t\trcu_assign_pointer(*porig, rt);\n\t\t\tif (orig) {\n\t\t\t\tdst_dev_put(&orig->dst);\n\t\t\t\tdst_release(&orig->dst);\n\t\t\t}\n\t\t\tret = true;\n\t\t}\n\n\t\tfnhe->fnhe_stamp = jiffies;\n\t}\n\tspin_unlock_bh(&fnhe_lock);\n\n\treturn ret;\n}\n\nstatic bool rt_cache_route(struct fib_nh *nh, struct rtable *rt)\n{\n\tstruct rtable *orig, *prev, **p;\n\tbool ret = true;\n\n\tif (rt_is_input_route(rt)) {\n\t\tp = (struct rtable **)&nh->nh_rth_input;\n\t} else {\n\t\tp = (struct rtable **)raw_cpu_ptr(nh->nh_pcpu_rth_output);\n\t}\n\torig = *p;\n\n\t/* hold dst before doing cmpxchg() to avoid race condition\n\t * on this dst\n\t */\n\tdst_hold(&rt->dst);\n\tprev = cmpxchg(p, orig, rt);\n\tif (prev == orig) {\n\t\tif (orig) {\n\t\t\tdst_dev_put(&orig->dst);\n\t\t\tdst_release(&orig->dst);\n\t\t}\n\t} else {\n\t\tdst_release(&rt->dst);\n\t\tret = false;\n\t}\n\n\treturn ret;\n}\n\nstruct uncached_list {\n\tspinlock_t\t\tlock;\n\tstruct list_head\thead;\n};\n\nstatic DEFINE_PER_CPU_ALIGNED(struct uncached_list, rt_uncached_list);\n\nstatic void rt_add_uncached_list(struct rtable *rt)\n{\n\tstruct uncached_list *ul = raw_cpu_ptr(&rt_uncached_list);\n\n\trt->rt_uncached_list = ul;\n\n\tspin_lock_bh(&ul->lock);\n\tlist_add_tail(&rt->rt_uncached, &ul->head);\n\tspin_unlock_bh(&ul->lock);\n}\n\nstatic void ipv4_dst_destroy(struct dst_entry *dst)\n{\n\tstruct dst_metrics *p = (struct dst_metrics *)DST_METRICS_PTR(dst);\n\tstruct rtable *rt = (struct rtable *) dst;\n\n\tif (p != &dst_default_metrics && atomic_dec_and_test(&p->refcnt))\n\t\tkfree(p);\n\n\tif (!list_empty(&rt->rt_uncached)) {\n\t\tstruct uncached_list *ul = rt->rt_uncached_list;\n\n\t\tspin_lock_bh(&ul->lock);\n\t\tlist_del(&rt->rt_uncached);\n\t\tspin_unlock_bh(&ul->lock);\n\t}\n}\n\nvoid rt_flush_dev(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\tstruct rtable *rt;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct uncached_list *ul = &per_cpu(rt_uncached_list, cpu);\n\n\t\tspin_lock_bh(&ul->lock);\n\t\tlist_for_each_entry(rt, &ul->head, rt_uncached) {\n\t\t\tif (rt->dst.dev != dev)\n\t\t\t\tcontinue;\n\t\t\trt->dst.dev = net->loopback_dev;\n\t\t\tdev_hold(rt->dst.dev);\n\t\t\tdev_put(dev);\n\t\t}\n\t\tspin_unlock_bh(&ul->lock);\n\t}\n}\n\nstatic bool rt_cache_valid(const struct rtable *rt)\n{\n\treturn\trt &&\n\t\trt->dst.obsolete == DST_OBSOLETE_FORCE_CHK &&\n\t\t!rt_is_expired(rt);\n}\n\nstatic void rt_set_nexthop(struct rtable *rt, __be32 daddr,\n\t\t\t   const struct fib_result *res,\n\t\t\t   struct fib_nh_exception *fnhe,\n\t\t\t   struct fib_info *fi, u16 type, u32 itag,\n\t\t\t   const bool do_cache)\n{\n\tbool cached = false;\n\n\tif (fi) {\n\t\tstruct fib_nh *nh = &FIB_RES_NH(*res);\n\n\t\tif (nh->nh_gw && nh->nh_scope == RT_SCOPE_LINK) {\n\t\t\trt->rt_gateway = nh->nh_gw;\n\t\t\trt->rt_uses_gateway = 1;\n\t\t}\n\t\tdst_init_metrics(&rt->dst, fi->fib_metrics->metrics, true);\n\t\tif (fi->fib_metrics != &dst_default_metrics) {\n\t\t\trt->dst._metrics |= DST_METRICS_REFCOUNTED;\n\t\t\tatomic_inc(&fi->fib_metrics->refcnt);\n\t\t}\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\t\trt->dst.tclassid = nh->nh_tclassid;\n#endif\n\t\trt->dst.lwtstate = lwtstate_get(nh->nh_lwtstate);\n\t\tif (unlikely(fnhe))\n\t\t\tcached = rt_bind_exception(rt, fnhe, daddr, do_cache);\n\t\telse if (do_cache)\n\t\t\tcached = rt_cache_route(nh, rt);\n\t\tif (unlikely(!cached)) {\n\t\t\t/* Routes we intend to cache in nexthop exception or\n\t\t\t * FIB nexthop have the DST_NOCACHE bit clear.\n\t\t\t * However, if we are unsuccessful at storing this\n\t\t\t * route into the cache we really need to set it.\n\t\t\t */\n\t\t\tif (!rt->rt_gateway)\n\t\t\t\trt->rt_gateway = daddr;\n\t\t\trt_add_uncached_list(rt);\n\t\t}\n\t} else\n\t\trt_add_uncached_list(rt);\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\n#ifdef CONFIG_IP_MULTIPLE_TABLES\n\tset_class_tag(rt, res->tclassid);\n#endif\n\tset_class_tag(rt, itag);\n#endif\n}\n\nstruct rtable *rt_dst_alloc(struct net_device *dev,\n\t\t\t    unsigned int flags, u16 type,\n\t\t\t    bool nopolicy, bool noxfrm, bool will_cache)\n{\n\tstruct rtable *rt;\n\n\trt = dst_alloc(&ipv4_dst_ops, dev, 1, DST_OBSOLETE_FORCE_CHK,\n\t\t       (will_cache ? 0 : DST_HOST) |\n\t\t       (nopolicy ? DST_NOPOLICY : 0) |\n\t\t       (noxfrm ? DST_NOXFRM : 0));\n\n\tif (rt) {\n\t\trt->rt_genid = rt_genid_ipv4(dev_net(dev));\n\t\trt->rt_flags = flags;\n\t\trt->rt_type = type;\n\t\trt->rt_is_input = 0;\n\t\trt->rt_iif = 0;\n\t\trt->rt_pmtu = 0;\n\t\trt->rt_gateway = 0;\n\t\trt->rt_uses_gateway = 0;\n\t\trt->rt_table_id = 0;\n\t\tINIT_LIST_HEAD(&rt->rt_uncached);\n\n\t\trt->dst.output = ip_output;\n\t\tif (flags & RTCF_LOCAL)\n\t\t\trt->dst.input = ip_local_deliver;\n\t}\n\n\treturn rt;\n}\nEXPORT_SYMBOL(rt_dst_alloc);\n\n/* called in rcu_read_lock() section */\nstatic int ip_route_input_mc(struct sk_buff *skb, __be32 daddr, __be32 saddr,\n\t\t\t\tu8 tos, struct net_device *dev, int our)\n{\n\tstruct rtable *rth;\n\tstruct in_device *in_dev = __in_dev_get_rcu(dev);\n\tunsigned int flags = RTCF_MULTICAST;\n\tu32 itag = 0;\n\tint err;\n\n\t/* Primary sanity checks. */\n\n\tif (!in_dev)\n\t\treturn -EINVAL;\n\n\tif (ipv4_is_multicast(saddr) || ipv4_is_lbcast(saddr) ||\n\t    skb->protocol != htons(ETH_P_IP))\n\t\tgoto e_inval;\n\n\tif (ipv4_is_loopback(saddr) && !IN_DEV_ROUTE_LOCALNET(in_dev))\n\t\tgoto e_inval;\n\n\tif (ipv4_is_zeronet(saddr)) {\n\t\tif (!ipv4_is_local_multicast(daddr))\n\t\t\tgoto e_inval;\n\t} else {\n\t\terr = fib_validate_source(skb, saddr, 0, tos, 0, dev,\n\t\t\t\t\t  in_dev, &itag);\n\t\tif (err < 0)\n\t\t\tgoto e_err;\n\t}\n\tif (our)\n\t\tflags |= RTCF_LOCAL;\n\n\trth = rt_dst_alloc(dev_net(dev)->loopback_dev, flags, RTN_MULTICAST,\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOPOLICY), false, false);\n\tif (!rth)\n\t\tgoto e_nobufs;\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\trth->dst.tclassid = itag;\n#endif\n\trth->dst.output = ip_rt_bug;\n\trth->rt_is_input= 1;\n\n#ifdef CONFIG_IP_MROUTE\n\tif (!ipv4_is_local_multicast(daddr) && IN_DEV_MFORWARD(in_dev))\n\t\trth->dst.input = ip_mr_input;\n#endif\n\tRT_CACHE_STAT_INC(in_slow_mc);\n\n\tskb_dst_set(skb, &rth->dst);\n\treturn 0;\n\ne_nobufs:\n\treturn -ENOBUFS;\ne_inval:\n\treturn -EINVAL;\ne_err:\n\treturn err;\n}\n\n\nstatic void ip_handle_martian_source(struct net_device *dev,\n\t\t\t\t     struct in_device *in_dev,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     __be32 daddr,\n\t\t\t\t     __be32 saddr)\n{\n\tRT_CACHE_STAT_INC(in_martian_src);\n#ifdef CONFIG_IP_ROUTE_VERBOSE\n\tif (IN_DEV_LOG_MARTIANS(in_dev) && net_ratelimit()) {\n\t\t/*\n\t\t *\tRFC1812 recommendation, if source is martian,\n\t\t *\tthe only hint is MAC header.\n\t\t */\n\t\tpr_warn(\"martian source %pI4 from %pI4, on dev %s\\n\",\n\t\t\t&daddr, &saddr, dev->name);\n\t\tif (dev->hard_header_len && skb_mac_header_was_set(skb)) {\n\t\t\tprint_hex_dump(KERN_WARNING, \"ll header: \",\n\t\t\t\t       DUMP_PREFIX_OFFSET, 16, 1,\n\t\t\t\t       skb_mac_header(skb),\n\t\t\t\t       dev->hard_header_len, true);\n\t\t}\n\t}\n#endif\n}\n\nstatic void ip_del_fnhe(struct fib_nh *nh, __be32 daddr)\n{\n\tstruct fnhe_hash_bucket *hash;\n\tstruct fib_nh_exception *fnhe, __rcu **fnhe_p;\n\tu32 hval = fnhe_hashfun(daddr);\n\n\tspin_lock_bh(&fnhe_lock);\n\n\thash = rcu_dereference_protected(nh->nh_exceptions,\n\t\t\t\t\t lockdep_is_held(&fnhe_lock));\n\thash += hval;\n\n\tfnhe_p = &hash->chain;\n\tfnhe = rcu_dereference_protected(*fnhe_p, lockdep_is_held(&fnhe_lock));\n\twhile (fnhe) {\n\t\tif (fnhe->fnhe_daddr == daddr) {\n\t\t\trcu_assign_pointer(*fnhe_p, rcu_dereference_protected(\n\t\t\t\tfnhe->fnhe_next, lockdep_is_held(&fnhe_lock)));\n\t\t\tfnhe_flush_routes(fnhe);\n\t\t\tkfree_rcu(fnhe, rcu);\n\t\t\tbreak;\n\t\t}\n\t\tfnhe_p = &fnhe->fnhe_next;\n\t\tfnhe = rcu_dereference_protected(fnhe->fnhe_next,\n\t\t\t\t\t\t lockdep_is_held(&fnhe_lock));\n\t}\n\n\tspin_unlock_bh(&fnhe_lock);\n}\n\nstatic void set_lwt_redirect(struct rtable *rth)\n{\n\tif (lwtunnel_output_redirect(rth->dst.lwtstate)) {\n\t\trth->dst.lwtstate->orig_output = rth->dst.output;\n\t\trth->dst.output = lwtunnel_output;\n\t}\n\n\tif (lwtunnel_input_redirect(rth->dst.lwtstate)) {\n\t\trth->dst.lwtstate->orig_input = rth->dst.input;\n\t\trth->dst.input = lwtunnel_input;\n\t}\n}\n\n/* called in rcu_read_lock() section */\nstatic int __mkroute_input(struct sk_buff *skb,\n\t\t\t   const struct fib_result *res,\n\t\t\t   struct in_device *in_dev,\n\t\t\t   __be32 daddr, __be32 saddr, u32 tos)\n{\n\tstruct fib_nh_exception *fnhe;\n\tstruct rtable *rth;\n\tint err;\n\tstruct in_device *out_dev;\n\tbool do_cache;\n\tu32 itag = 0;\n\n\t/* get a working reference to the output device */\n\tout_dev = __in_dev_get_rcu(FIB_RES_DEV(*res));\n\tif (!out_dev) {\n\t\tnet_crit_ratelimited(\"Bug in ip_route_input_slow(). Please report.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = fib_validate_source(skb, saddr, daddr, tos, FIB_RES_OIF(*res),\n\t\t\t\t  in_dev->dev, in_dev, &itag);\n\tif (err < 0) {\n\t\tip_handle_martian_source(in_dev->dev, in_dev, skb, daddr,\n\t\t\t\t\t saddr);\n\n\t\tgoto cleanup;\n\t}\n\n\tdo_cache = res->fi && !itag;\n\tif (out_dev == in_dev && err && IN_DEV_TX_REDIRECTS(out_dev) &&\n\t    skb->protocol == htons(ETH_P_IP) &&\n\t    (IN_DEV_SHARED_MEDIA(out_dev) ||\n\t     inet_addr_onlink(out_dev, saddr, FIB_RES_GW(*res))))\n\t\tIPCB(skb)->flags |= IPSKB_DOREDIRECT;\n\n\tif (skb->protocol != htons(ETH_P_IP)) {\n\t\t/* Not IP (i.e. ARP). Do not create route, if it is\n\t\t * invalid for proxy arp. DNAT routes are always valid.\n\t\t *\n\t\t * Proxy arp feature have been extended to allow, ARP\n\t\t * replies back to the same interface, to support\n\t\t * Private VLAN switch technologies. See arp.c.\n\t\t */\n\t\tif (out_dev == in_dev &&\n\t\t    IN_DEV_PROXY_ARP_PVLAN(in_dev) == 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\tfnhe = find_exception(&FIB_RES_NH(*res), daddr);\n\tif (do_cache) {\n\t\tif (fnhe) {\n\t\t\trth = rcu_dereference(fnhe->fnhe_rth_input);\n\t\t\tif (rth && rth->dst.expires &&\n\t\t\t    time_after(jiffies, rth->dst.expires)) {\n\t\t\t\tip_del_fnhe(&FIB_RES_NH(*res), daddr);\n\t\t\t\tfnhe = NULL;\n\t\t\t} else {\n\t\t\t\tgoto rt_cache;\n\t\t\t}\n\t\t}\n\n\t\trth = rcu_dereference(FIB_RES_NH(*res).nh_rth_input);\n\nrt_cache:\n\t\tif (rt_cache_valid(rth)) {\n\t\t\tskb_dst_set_noref(skb, &rth->dst);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\trth = rt_dst_alloc(out_dev->dev, 0, res->type,\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOPOLICY),\n\t\t\t   IN_DEV_CONF_GET(out_dev, NOXFRM), do_cache);\n\tif (!rth) {\n\t\terr = -ENOBUFS;\n\t\tgoto cleanup;\n\t}\n\n\trth->rt_is_input = 1;\n\tif (res->table)\n\t\trth->rt_table_id = res->table->tb_id;\n\tRT_CACHE_STAT_INC(in_slow_tot);\n\n\trth->dst.input = ip_forward;\n\n\trt_set_nexthop(rth, daddr, res, fnhe, res->fi, res->type, itag,\n\t\t       do_cache);\n\tset_lwt_redirect(rth);\n\tskb_dst_set(skb, &rth->dst);\nout:\n\terr = 0;\n cleanup:\n\treturn err;\n}\n\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n/* To make ICMP packets follow the right flow, the multipath hash is\n * calculated from the inner IP addresses.\n */\nstatic void ip_multipath_l3_keys(const struct sk_buff *skb,\n\t\t\t\t struct flow_keys *hash_keys)\n{\n\tconst struct iphdr *outer_iph = ip_hdr(skb);\n\tconst struct iphdr *inner_iph;\n\tconst struct icmphdr *icmph;\n\tstruct iphdr _inner_iph;\n\tstruct icmphdr _icmph;\n\n\thash_keys->addrs.v4addrs.src = outer_iph->saddr;\n\thash_keys->addrs.v4addrs.dst = outer_iph->daddr;\n\tif (likely(outer_iph->protocol != IPPROTO_ICMP))\n\t\treturn;\n\n\tif (unlikely((outer_iph->frag_off & htons(IP_OFFSET)) != 0))\n\t\treturn;\n\n\ticmph = skb_header_pointer(skb, outer_iph->ihl * 4, sizeof(_icmph),\n\t\t\t\t   &_icmph);\n\tif (!icmph)\n\t\treturn;\n\n\tif (icmph->type != ICMP_DEST_UNREACH &&\n\t    icmph->type != ICMP_REDIRECT &&\n\t    icmph->type != ICMP_TIME_EXCEEDED &&\n\t    icmph->type != ICMP_PARAMETERPROB)\n\t\treturn;\n\n\tinner_iph = skb_header_pointer(skb,\n\t\t\t\t       outer_iph->ihl * 4 + sizeof(_icmph),\n\t\t\t\t       sizeof(_inner_iph), &_inner_iph);\n\tif (!inner_iph)\n\t\treturn;\n\thash_keys->addrs.v4addrs.src = inner_iph->saddr;\n\thash_keys->addrs.v4addrs.dst = inner_iph->daddr;\n}\n\n/* if skb is set it will be used and fl4 can be NULL */\nint fib_multipath_hash(const struct fib_info *fi, const struct flowi4 *fl4,\n\t\t       const struct sk_buff *skb)\n{\n\tstruct net *net = fi->fib_net;\n\tstruct flow_keys hash_keys;\n\tu32 mhash;\n\n\tswitch (net->ipv4.sysctl_fib_multipath_hash_policy) {\n\tcase 0:\n\t\tmemset(&hash_keys, 0, sizeof(hash_keys));\n\t\thash_keys.control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t\tif (skb) {\n\t\t\tip_multipath_l3_keys(skb, &hash_keys);\n\t\t} else {\n\t\t\thash_keys.addrs.v4addrs.src = fl4->saddr;\n\t\t\thash_keys.addrs.v4addrs.dst = fl4->daddr;\n\t\t}\n\t\tbreak;\n\tcase 1:\n\t\t/* skb is currently provided only when forwarding */\n\t\tif (skb) {\n\t\t\tunsigned int flag = FLOW_DISSECTOR_F_STOP_AT_ENCAP;\n\t\t\tstruct flow_keys keys;\n\n\t\t\t/* short-circuit if we already have L4 hash present */\n\t\t\tif (skb->l4_hash)\n\t\t\t\treturn skb_get_hash_raw(skb) >> 1;\n\t\t\tmemset(&hash_keys, 0, sizeof(hash_keys));\n\t\t\tskb_flow_dissect_flow_keys(skb, &keys, flag);\n\t\t\thash_keys.addrs.v4addrs.src = keys.addrs.v4addrs.src;\n\t\t\thash_keys.addrs.v4addrs.dst = keys.addrs.v4addrs.dst;\n\t\t\thash_keys.ports.src = keys.ports.src;\n\t\t\thash_keys.ports.dst = keys.ports.dst;\n\t\t\thash_keys.basic.ip_proto = keys.basic.ip_proto;\n\t\t} else {\n\t\t\tmemset(&hash_keys, 0, sizeof(hash_keys));\n\t\t\thash_keys.control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t\t\thash_keys.addrs.v4addrs.src = fl4->saddr;\n\t\t\thash_keys.addrs.v4addrs.dst = fl4->daddr;\n\t\t\thash_keys.ports.src = fl4->fl4_sport;\n\t\t\thash_keys.ports.dst = fl4->fl4_dport;\n\t\t\thash_keys.basic.ip_proto = fl4->flowi4_proto;\n\t\t}\n\t\tbreak;\n\t}\n\tmhash = flow_hash_from_keys(&hash_keys);\n\n\treturn mhash >> 1;\n}\nEXPORT_SYMBOL_GPL(fib_multipath_hash);\n#endif /* CONFIG_IP_ROUTE_MULTIPATH */\n\nstatic int ip_mkroute_input(struct sk_buff *skb,\n\t\t\t    struct fib_result *res,\n\t\t\t    struct in_device *in_dev,\n\t\t\t    __be32 daddr, __be32 saddr, u32 tos)\n{\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\tif (res->fi && res->fi->fib_nhs > 1) {\n\t\tint h = fib_multipath_hash(res->fi, NULL, skb);\n\n\t\tfib_select_multipath(res, h);\n\t}\n#endif\n\n\t/* create a routing cache entry */\n\treturn __mkroute_input(skb, res, in_dev, daddr, saddr, tos);\n}\n\n/*\n *\tNOTE. We drop all the packets that has local source\n *\taddresses, because every properly looped back packet\n *\tmust have correct destination already attached by output routine.\n *\n *\tSuch approach solves two big problems:\n *\t1. Not simplex devices are handled properly.\n *\t2. IP spoofing attempts are filtered with 100% of guarantee.\n *\tcalled with rcu_read_lock()\n */\n\nstatic int ip_route_input_slow(struct sk_buff *skb, __be32 daddr, __be32 saddr,\n\t\t\t       u8 tos, struct net_device *dev,\n\t\t\t       struct fib_result *res)\n{\n\tstruct in_device *in_dev = __in_dev_get_rcu(dev);\n\tstruct ip_tunnel_info *tun_info;\n\tstruct flowi4\tfl4;\n\tunsigned int\tflags = 0;\n\tu32\t\titag = 0;\n\tstruct rtable\t*rth;\n\tint\t\terr = -EINVAL;\n\tstruct net    *net = dev_net(dev);\n\tbool do_cache;\n\n\t/* IP on this device is disabled. */\n\n\tif (!in_dev)\n\t\tgoto out;\n\n\t/* Check for the most weird martians, which can be not detected\n\t   by fib_lookup.\n\t */\n\n\ttun_info = skb_tunnel_info(skb);\n\tif (tun_info && !(tun_info->mode & IP_TUNNEL_INFO_TX))\n\t\tfl4.flowi4_tun_key.tun_id = tun_info->key.tun_id;\n\telse\n\t\tfl4.flowi4_tun_key.tun_id = 0;\n\tskb_dst_drop(skb);\n\n\tif (ipv4_is_multicast(saddr) || ipv4_is_lbcast(saddr))\n\t\tgoto martian_source;\n\n\tres->fi = NULL;\n\tres->table = NULL;\n\tif (ipv4_is_lbcast(daddr) || (saddr == 0 && daddr == 0))\n\t\tgoto brd_input;\n\n\t/* Accept zero addresses only to limited broadcast;\n\t * I even do not know to fix it or not. Waiting for complains :-)\n\t */\n\tif (ipv4_is_zeronet(saddr))\n\t\tgoto martian_source;\n\n\tif (ipv4_is_zeronet(daddr))\n\t\tgoto martian_destination;\n\n\t/* Following code try to avoid calling IN_DEV_NET_ROUTE_LOCALNET(),\n\t * and call it once if daddr or/and saddr are loopback addresses\n\t */\n\tif (ipv4_is_loopback(daddr)) {\n\t\tif (!IN_DEV_NET_ROUTE_LOCALNET(in_dev, net))\n\t\t\tgoto martian_destination;\n\t} else if (ipv4_is_loopback(saddr)) {\n\t\tif (!IN_DEV_NET_ROUTE_LOCALNET(in_dev, net))\n\t\t\tgoto martian_source;\n\t}\n\n\t/*\n\t *\tNow we are ready to route packet.\n\t */\n\tfl4.flowi4_oif = 0;\n\tfl4.flowi4_iif = dev->ifindex;\n\tfl4.flowi4_mark = skb->mark;\n\tfl4.flowi4_tos = tos;\n\tfl4.flowi4_scope = RT_SCOPE_UNIVERSE;\n\tfl4.flowi4_flags = 0;\n\tfl4.daddr = daddr;\n\tfl4.saddr = saddr;\n\tfl4.flowi4_uid = sock_net_uid(net, NULL);\n\terr = fib_lookup(net, &fl4, res, 0);\n\tif (err != 0) {\n\t\tif (!IN_DEV_FORWARD(in_dev))\n\t\t\terr = -EHOSTUNREACH;\n\t\tgoto no_route;\n\t}\n\n\tif (res->type == RTN_BROADCAST)\n\t\tgoto brd_input;\n\n\tif (res->type == RTN_LOCAL) {\n\t\terr = fib_validate_source(skb, saddr, daddr, tos,\n\t\t\t\t\t  0, dev, in_dev, &itag);\n\t\tif (err < 0)\n\t\t\tgoto martian_source;\n\t\tgoto local_input;\n\t}\n\n\tif (!IN_DEV_FORWARD(in_dev)) {\n\t\terr = -EHOSTUNREACH;\n\t\tgoto no_route;\n\t}\n\tif (res->type != RTN_UNICAST)\n\t\tgoto martian_destination;\n\n\terr = ip_mkroute_input(skb, res, in_dev, daddr, saddr, tos);\nout:\treturn err;\n\nbrd_input:\n\tif (skb->protocol != htons(ETH_P_IP))\n\t\tgoto e_inval;\n\n\tif (!ipv4_is_zeronet(saddr)) {\n\t\terr = fib_validate_source(skb, saddr, 0, tos, 0, dev,\n\t\t\t\t\t  in_dev, &itag);\n\t\tif (err < 0)\n\t\t\tgoto martian_source;\n\t}\n\tflags |= RTCF_BROADCAST;\n\tres->type = RTN_BROADCAST;\n\tRT_CACHE_STAT_INC(in_brd);\n\nlocal_input:\n\tdo_cache = false;\n\tif (res->fi) {\n\t\tif (!itag) {\n\t\t\trth = rcu_dereference(FIB_RES_NH(*res).nh_rth_input);\n\t\t\tif (rt_cache_valid(rth)) {\n\t\t\t\tskb_dst_set_noref(skb, &rth->dst);\n\t\t\t\terr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdo_cache = true;\n\t\t}\n\t}\n\n\trth = rt_dst_alloc(l3mdev_master_dev_rcu(dev) ? : net->loopback_dev,\n\t\t\t   flags | RTCF_LOCAL, res->type,\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOPOLICY), false, do_cache);\n\tif (!rth)\n\t\tgoto e_nobufs;\n\n\trth->dst.output= ip_rt_bug;\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\trth->dst.tclassid = itag;\n#endif\n\trth->rt_is_input = 1;\n\tif (res->table)\n\t\trth->rt_table_id = res->table->tb_id;\n\n\tRT_CACHE_STAT_INC(in_slow_tot);\n\tif (res->type == RTN_UNREACHABLE) {\n\t\trth->dst.input= ip_error;\n\t\trth->dst.error= -err;\n\t\trth->rt_flags \t&= ~RTCF_LOCAL;\n\t}\n\n\tif (do_cache) {\n\t\tstruct fib_nh *nh = &FIB_RES_NH(*res);\n\n\t\trth->dst.lwtstate = lwtstate_get(nh->nh_lwtstate);\n\t\tif (lwtunnel_input_redirect(rth->dst.lwtstate)) {\n\t\t\tWARN_ON(rth->dst.input == lwtunnel_input);\n\t\t\trth->dst.lwtstate->orig_input = rth->dst.input;\n\t\t\trth->dst.input = lwtunnel_input;\n\t\t}\n\n\t\tif (unlikely(!rt_cache_route(nh, rth)))\n\t\t\trt_add_uncached_list(rth);\n\t}\n\tskb_dst_set(skb, &rth->dst);\n\terr = 0;\n\tgoto out;\n\nno_route:\n\tRT_CACHE_STAT_INC(in_no_route);\n\tres->type = RTN_UNREACHABLE;\n\tres->fi = NULL;\n\tres->table = NULL;\n\tgoto local_input;\n\n\t/*\n\t *\tDo not cache martian addresses: they should be logged (RFC1812)\n\t */\nmartian_destination:\n\tRT_CACHE_STAT_INC(in_martian_dst);\n#ifdef CONFIG_IP_ROUTE_VERBOSE\n\tif (IN_DEV_LOG_MARTIANS(in_dev))\n\t\tnet_warn_ratelimited(\"martian destination %pI4 from %pI4, dev %s\\n\",\n\t\t\t\t     &daddr, &saddr, dev->name);\n#endif\n\ne_inval:\n\terr = -EINVAL;\n\tgoto out;\n\ne_nobufs:\n\terr = -ENOBUFS;\n\tgoto out;\n\nmartian_source:\n\tip_handle_martian_source(dev, in_dev, skb, daddr, saddr);\n\tgoto out;\n}\n\nint ip_route_input_noref(struct sk_buff *skb, __be32 daddr, __be32 saddr,\n\t\t\t u8 tos, struct net_device *dev)\n{\n\tstruct fib_result res;\n\tint err;\n\n\ttos &= IPTOS_RT_MASK;\n\trcu_read_lock();\n\terr = ip_route_input_rcu(skb, daddr, saddr, tos, dev, &res);\n\trcu_read_unlock();\n\n\treturn err;\n}\nEXPORT_SYMBOL(ip_route_input_noref);\n\n/* called with rcu_read_lock held */\nint ip_route_input_rcu(struct sk_buff *skb, __be32 daddr, __be32 saddr,\n\t\t       u8 tos, struct net_device *dev, struct fib_result *res)\n{\n\t/* Multicast recognition logic is moved from route cache to here.\n\t   The problem was that too many Ethernet cards have broken/missing\n\t   hardware multicast filters :-( As result the host on multicasting\n\t   network acquires a lot of useless route cache entries, sort of\n\t   SDR messages from all the world. Now we try to get rid of them.\n\t   Really, provided software IP multicast filter is organized\n\t   reasonably (at least, hashed), it does not result in a slowdown\n\t   comparing with route cache reject entries.\n\t   Note, that multicast routers are not affected, because\n\t   route cache entry is created eventually.\n\t */\n\tif (ipv4_is_multicast(daddr)) {\n\t\tstruct in_device *in_dev = __in_dev_get_rcu(dev);\n\t\tint our = 0;\n\t\tint err = -EINVAL;\n\n\t\tif (in_dev)\n\t\t\tour = ip_check_mc_rcu(in_dev, daddr, saddr,\n\t\t\t\t\t      ip_hdr(skb)->protocol);\n\n\t\t/* check l3 master if no match yet */\n\t\tif ((!in_dev || !our) && netif_is_l3_slave(dev)) {\n\t\t\tstruct in_device *l3_in_dev;\n\n\t\t\tl3_in_dev = __in_dev_get_rcu(skb->dev);\n\t\t\tif (l3_in_dev)\n\t\t\t\tour = ip_check_mc_rcu(l3_in_dev, daddr, saddr,\n\t\t\t\t\t\t      ip_hdr(skb)->protocol);\n\t\t}\n\n\t\tif (our\n#ifdef CONFIG_IP_MROUTE\n\t\t\t||\n\t\t    (!ipv4_is_local_multicast(daddr) &&\n\t\t     IN_DEV_MFORWARD(in_dev))\n#endif\n\t\t   ) {\n\t\t\terr = ip_route_input_mc(skb, daddr, saddr,\n\t\t\t\t\t\ttos, dev, our);\n\t\t}\n\t\treturn err;\n\t}\n\n\treturn ip_route_input_slow(skb, daddr, saddr, tos, dev, res);\n}\n\n/* called with rcu_read_lock() */\nstatic struct rtable *__mkroute_output(const struct fib_result *res,\n\t\t\t\t       const struct flowi4 *fl4, int orig_oif,\n\t\t\t\t       struct net_device *dev_out,\n\t\t\t\t       unsigned int flags)\n{\n\tstruct fib_info *fi = res->fi;\n\tstruct fib_nh_exception *fnhe;\n\tstruct in_device *in_dev;\n\tu16 type = res->type;\n\tstruct rtable *rth;\n\tbool do_cache;\n\n\tin_dev = __in_dev_get_rcu(dev_out);\n\tif (!in_dev)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (likely(!IN_DEV_ROUTE_LOCALNET(in_dev)))\n\t\tif (ipv4_is_loopback(fl4->saddr) &&\n\t\t    !(dev_out->flags & IFF_LOOPBACK) &&\n\t\t    !netif_is_l3_master(dev_out))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\tif (ipv4_is_lbcast(fl4->daddr))\n\t\ttype = RTN_BROADCAST;\n\telse if (ipv4_is_multicast(fl4->daddr))\n\t\ttype = RTN_MULTICAST;\n\telse if (ipv4_is_zeronet(fl4->daddr))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (dev_out->flags & IFF_LOOPBACK)\n\t\tflags |= RTCF_LOCAL;\n\n\tdo_cache = true;\n\tif (type == RTN_BROADCAST) {\n\t\tflags |= RTCF_BROADCAST | RTCF_LOCAL;\n\t\tfi = NULL;\n\t} else if (type == RTN_MULTICAST) {\n\t\tflags |= RTCF_MULTICAST | RTCF_LOCAL;\n\t\tif (!ip_check_mc_rcu(in_dev, fl4->daddr, fl4->saddr,\n\t\t\t\t     fl4->flowi4_proto))\n\t\t\tflags &= ~RTCF_LOCAL;\n\t\telse\n\t\t\tdo_cache = false;\n\t\t/* If multicast route do not exist use\n\t\t * default one, but do not gateway in this case.\n\t\t * Yes, it is hack.\n\t\t */\n\t\tif (fi && res->prefixlen < 4)\n\t\t\tfi = NULL;\n\t} else if ((type == RTN_LOCAL) && (orig_oif != 0) &&\n\t\t   (orig_oif != dev_out->ifindex)) {\n\t\t/* For local routes that require a particular output interface\n\t\t * we do not want to cache the result.  Caching the result\n\t\t * causes incorrect behaviour when there are multiple source\n\t\t * addresses on the interface, the end result being that if the\n\t\t * intended recipient is waiting on that interface for the\n\t\t * packet he won't receive it because it will be delivered on\n\t\t * the loopback interface and the IP_PKTINFO ipi_ifindex will\n\t\t * be set to the loopback interface as well.\n\t\t */\n\t\tfi = NULL;\n\t}\n\n\tfnhe = NULL;\n\tdo_cache &= fi != NULL;\n\tif (do_cache) {\n\t\tstruct rtable __rcu **prth;\n\t\tstruct fib_nh *nh = &FIB_RES_NH(*res);\n\n\t\tfnhe = find_exception(nh, fl4->daddr);\n\t\tif (fnhe) {\n\t\t\tprth = &fnhe->fnhe_rth_output;\n\t\t\trth = rcu_dereference(*prth);\n\t\t\tif (rth && rth->dst.expires &&\n\t\t\t    time_after(jiffies, rth->dst.expires)) {\n\t\t\t\tip_del_fnhe(nh, fl4->daddr);\n\t\t\t\tfnhe = NULL;\n\t\t\t} else {\n\t\t\t\tgoto rt_cache;\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(fl4->flowi4_flags &\n\t\t\t     FLOWI_FLAG_KNOWN_NH &&\n\t\t\t     !(nh->nh_gw &&\n\t\t\t       nh->nh_scope == RT_SCOPE_LINK))) {\n\t\t\tdo_cache = false;\n\t\t\tgoto add;\n\t\t}\n\t\tprth = raw_cpu_ptr(nh->nh_pcpu_rth_output);\n\t\trth = rcu_dereference(*prth);\n\nrt_cache:\n\t\tif (rt_cache_valid(rth) && dst_hold_safe(&rth->dst))\n\t\t\treturn rth;\n\t}\n\nadd:\n\trth = rt_dst_alloc(dev_out, flags, type,\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOPOLICY),\n\t\t\t   IN_DEV_CONF_GET(in_dev, NOXFRM),\n\t\t\t   do_cache);\n\tif (!rth)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\trth->rt_iif\t= orig_oif ? : 0;\n\tif (res->table)\n\t\trth->rt_table_id = res->table->tb_id;\n\n\tRT_CACHE_STAT_INC(out_slow_tot);\n\n\tif (flags & (RTCF_BROADCAST | RTCF_MULTICAST)) {\n\t\tif (flags & RTCF_LOCAL &&\n\t\t    !(dev_out->flags & IFF_LOOPBACK)) {\n\t\t\trth->dst.output = ip_mc_output;\n\t\t\tRT_CACHE_STAT_INC(out_slow_mc);\n\t\t}\n#ifdef CONFIG_IP_MROUTE\n\t\tif (type == RTN_MULTICAST) {\n\t\t\tif (IN_DEV_MFORWARD(in_dev) &&\n\t\t\t    !ipv4_is_local_multicast(fl4->daddr)) {\n\t\t\t\trth->dst.input = ip_mr_input;\n\t\t\t\trth->dst.output = ip_mc_output;\n\t\t\t}\n\t\t}\n#endif\n\t}\n\n\trt_set_nexthop(rth, fl4->daddr, res, fnhe, fi, type, 0, do_cache);\n\tset_lwt_redirect(rth);\n\n\treturn rth;\n}\n\n/*\n * Major route resolver routine.\n */\n\nstruct rtable *ip_route_output_key_hash(struct net *net, struct flowi4 *fl4,\n\t\t\t\t\tconst struct sk_buff *skb)\n{\n\t__u8 tos = RT_FL_TOS(fl4);\n\tstruct fib_result res;\n\tstruct rtable *rth;\n\n\tres.tclassid\t= 0;\n\tres.fi\t\t= NULL;\n\tres.table\t= NULL;\n\n\tfl4->flowi4_iif = LOOPBACK_IFINDEX;\n\tfl4->flowi4_tos = tos & IPTOS_RT_MASK;\n\tfl4->flowi4_scope = ((tos & RTO_ONLINK) ?\n\t\t\t RT_SCOPE_LINK : RT_SCOPE_UNIVERSE);\n\n\trcu_read_lock();\n\trth = ip_route_output_key_hash_rcu(net, fl4, &res, skb);\n\trcu_read_unlock();\n\n\treturn rth;\n}\nEXPORT_SYMBOL_GPL(ip_route_output_key_hash);\n\nstruct rtable *ip_route_output_key_hash_rcu(struct net *net, struct flowi4 *fl4,\n\t\t\t\t\t    struct fib_result *res,\n\t\t\t\t\t    const struct sk_buff *skb)\n{\n\tstruct net_device *dev_out = NULL;\n\tint orig_oif = fl4->flowi4_oif;\n\tunsigned int flags = 0;\n\tstruct rtable *rth;\n\tint err = -ENETUNREACH;\n\n\tif (fl4->saddr) {\n\t\trth = ERR_PTR(-EINVAL);\n\t\tif (ipv4_is_multicast(fl4->saddr) ||\n\t\t    ipv4_is_lbcast(fl4->saddr) ||\n\t\t    ipv4_is_zeronet(fl4->saddr))\n\t\t\tgoto out;\n\n\t\t/* I removed check for oif == dev_out->oif here.\n\t\t   It was wrong for two reasons:\n\t\t   1. ip_dev_find(net, saddr) can return wrong iface, if saddr\n\t\t      is assigned to multiple interfaces.\n\t\t   2. Moreover, we are allowed to send packets with saddr\n\t\t      of another iface. --ANK\n\t\t */\n\n\t\tif (fl4->flowi4_oif == 0 &&\n\t\t    (ipv4_is_multicast(fl4->daddr) ||\n\t\t     ipv4_is_lbcast(fl4->daddr))) {\n\t\t\t/* It is equivalent to inet_addr_type(saddr) == RTN_LOCAL */\n\t\t\tdev_out = __ip_dev_find(net, fl4->saddr, false);\n\t\t\tif (!dev_out)\n\t\t\t\tgoto out;\n\n\t\t\t/* Special hack: user can direct multicasts\n\t\t\t   and limited broadcast via necessary interface\n\t\t\t   without fiddling with IP_MULTICAST_IF or IP_PKTINFO.\n\t\t\t   This hack is not just for fun, it allows\n\t\t\t   vic,vat and friends to work.\n\t\t\t   They bind socket to loopback, set ttl to zero\n\t\t\t   and expect that it will work.\n\t\t\t   From the viewpoint of routing cache they are broken,\n\t\t\t   because we are not allowed to build multicast path\n\t\t\t   with loopback source addr (look, routing cache\n\t\t\t   cannot know, that ttl is zero, so that packet\n\t\t\t   will not leave this host and route is valid).\n\t\t\t   Luckily, this hack is good workaround.\n\t\t\t */\n\n\t\t\tfl4->flowi4_oif = dev_out->ifindex;\n\t\t\tgoto make_route;\n\t\t}\n\n\t\tif (!(fl4->flowi4_flags & FLOWI_FLAG_ANYSRC)) {\n\t\t\t/* It is equivalent to inet_addr_type(saddr) == RTN_LOCAL */\n\t\t\tif (!__ip_dev_find(net, fl4->saddr, false))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\n\tif (fl4->flowi4_oif) {\n\t\tdev_out = dev_get_by_index_rcu(net, fl4->flowi4_oif);\n\t\trth = ERR_PTR(-ENODEV);\n\t\tif (!dev_out)\n\t\t\tgoto out;\n\n\t\t/* RACE: Check return value of inet_select_addr instead. */\n\t\tif (!(dev_out->flags & IFF_UP) || !__in_dev_get_rcu(dev_out)) {\n\t\t\trth = ERR_PTR(-ENETUNREACH);\n\t\t\tgoto out;\n\t\t}\n\t\tif (ipv4_is_local_multicast(fl4->daddr) ||\n\t\t    ipv4_is_lbcast(fl4->daddr) ||\n\t\t    fl4->flowi4_proto == IPPROTO_IGMP) {\n\t\t\tif (!fl4->saddr)\n\t\t\t\tfl4->saddr = inet_select_addr(dev_out, 0,\n\t\t\t\t\t\t\t      RT_SCOPE_LINK);\n\t\t\tgoto make_route;\n\t\t}\n\t\tif (!fl4->saddr) {\n\t\t\tif (ipv4_is_multicast(fl4->daddr))\n\t\t\t\tfl4->saddr = inet_select_addr(dev_out, 0,\n\t\t\t\t\t\t\t      fl4->flowi4_scope);\n\t\t\telse if (!fl4->daddr)\n\t\t\t\tfl4->saddr = inet_select_addr(dev_out, 0,\n\t\t\t\t\t\t\t      RT_SCOPE_HOST);\n\t\t}\n\t}\n\n\tif (!fl4->daddr) {\n\t\tfl4->daddr = fl4->saddr;\n\t\tif (!fl4->daddr)\n\t\t\tfl4->daddr = fl4->saddr = htonl(INADDR_LOOPBACK);\n\t\tdev_out = net->loopback_dev;\n\t\tfl4->flowi4_oif = LOOPBACK_IFINDEX;\n\t\tres->type = RTN_LOCAL;\n\t\tflags |= RTCF_LOCAL;\n\t\tgoto make_route;\n\t}\n\n\terr = fib_lookup(net, fl4, res, 0);\n\tif (err) {\n\t\tres->fi = NULL;\n\t\tres->table = NULL;\n\t\tif (fl4->flowi4_oif &&\n\t\t    (ipv4_is_multicast(fl4->daddr) ||\n\t\t    !netif_index_is_l3_master(net, fl4->flowi4_oif))) {\n\t\t\t/* Apparently, routing tables are wrong. Assume,\n\t\t\t   that the destination is on link.\n\n\t\t\t   WHY? DW.\n\t\t\t   Because we are allowed to send to iface\n\t\t\t   even if it has NO routes and NO assigned\n\t\t\t   addresses. When oif is specified, routing\n\t\t\t   tables are looked up with only one purpose:\n\t\t\t   to catch if destination is gatewayed, rather than\n\t\t\t   direct. Moreover, if MSG_DONTROUTE is set,\n\t\t\t   we send packet, ignoring both routing tables\n\t\t\t   and ifaddr state. --ANK\n\n\n\t\t\t   We could make it even if oif is unknown,\n\t\t\t   likely IPv6, but we do not.\n\t\t\t */\n\n\t\t\tif (fl4->saddr == 0)\n\t\t\t\tfl4->saddr = inet_select_addr(dev_out, 0,\n\t\t\t\t\t\t\t      RT_SCOPE_LINK);\n\t\t\tres->type = RTN_UNICAST;\n\t\t\tgoto make_route;\n\t\t}\n\t\trth = ERR_PTR(err);\n\t\tgoto out;\n\t}\n\n\tif (res->type == RTN_LOCAL) {\n\t\tif (!fl4->saddr) {\n\t\t\tif (res->fi->fib_prefsrc)\n\t\t\t\tfl4->saddr = res->fi->fib_prefsrc;\n\t\t\telse\n\t\t\t\tfl4->saddr = fl4->daddr;\n\t\t}\n\n\t\t/* L3 master device is the loopback for that domain */\n\t\tdev_out = l3mdev_master_dev_rcu(FIB_RES_DEV(*res)) ? :\n\t\t\tnet->loopback_dev;\n\t\tfl4->flowi4_oif = dev_out->ifindex;\n\t\tflags |= RTCF_LOCAL;\n\t\tgoto make_route;\n\t}\n\n\tfib_select_path(net, res, fl4, skb);\n\n\tdev_out = FIB_RES_DEV(*res);\n\tfl4->flowi4_oif = dev_out->ifindex;\n\n\nmake_route:\n\trth = __mkroute_output(res, fl4, orig_oif, dev_out, flags);\n\nout:\n\treturn rth;\n}\n\nstatic struct dst_entry *ipv4_blackhole_dst_check(struct dst_entry *dst, u32 cookie)\n{\n\treturn NULL;\n}\n\nstatic unsigned int ipv4_blackhole_mtu(const struct dst_entry *dst)\n{\n\tunsigned int mtu = dst_metric_raw(dst, RTAX_MTU);\n\n\treturn mtu ? : dst->dev->mtu;\n}\n\nstatic void ipv4_rt_blackhole_update_pmtu(struct dst_entry *dst, struct sock *sk,\n\t\t\t\t\t  struct sk_buff *skb, u32 mtu)\n{\n}\n\nstatic void ipv4_rt_blackhole_redirect(struct dst_entry *dst, struct sock *sk,\n\t\t\t\t       struct sk_buff *skb)\n{\n}\n\nstatic u32 *ipv4_rt_blackhole_cow_metrics(struct dst_entry *dst,\n\t\t\t\t\t  unsigned long old)\n{\n\treturn NULL;\n}\n\nstatic struct dst_ops ipv4_dst_blackhole_ops = {\n\t.family\t\t\t=\tAF_INET,\n\t.check\t\t\t=\tipv4_blackhole_dst_check,\n\t.mtu\t\t\t=\tipv4_blackhole_mtu,\n\t.default_advmss\t\t=\tipv4_default_advmss,\n\t.update_pmtu\t\t=\tipv4_rt_blackhole_update_pmtu,\n\t.redirect\t\t=\tipv4_rt_blackhole_redirect,\n\t.cow_metrics\t\t=\tipv4_rt_blackhole_cow_metrics,\n\t.neigh_lookup\t\t=\tipv4_neigh_lookup,\n};\n\nstruct dst_entry *ipv4_blackhole_route(struct net *net, struct dst_entry *dst_orig)\n{\n\tstruct rtable *ort = (struct rtable *) dst_orig;\n\tstruct rtable *rt;\n\n\trt = dst_alloc(&ipv4_dst_blackhole_ops, NULL, 1, DST_OBSOLETE_NONE, 0);\n\tif (rt) {\n\t\tstruct dst_entry *new = &rt->dst;\n\n\t\tnew->__use = 1;\n\t\tnew->input = dst_discard;\n\t\tnew->output = dst_discard_out;\n\n\t\tnew->dev = net->loopback_dev;\n\t\tif (new->dev)\n\t\t\tdev_hold(new->dev);\n\n\t\trt->rt_is_input = ort->rt_is_input;\n\t\trt->rt_iif = ort->rt_iif;\n\t\trt->rt_pmtu = ort->rt_pmtu;\n\n\t\trt->rt_genid = rt_genid_ipv4(net);\n\t\trt->rt_flags = ort->rt_flags;\n\t\trt->rt_type = ort->rt_type;\n\t\trt->rt_gateway = ort->rt_gateway;\n\t\trt->rt_uses_gateway = ort->rt_uses_gateway;\n\n\t\tINIT_LIST_HEAD(&rt->rt_uncached);\n\t}\n\n\tdst_release(dst_orig);\n\n\treturn rt ? &rt->dst : ERR_PTR(-ENOMEM);\n}\n\nstruct rtable *ip_route_output_flow(struct net *net, struct flowi4 *flp4,\n\t\t\t\t    const struct sock *sk)\n{\n\tstruct rtable *rt = __ip_route_output_key(net, flp4);\n\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\tif (flp4->flowi4_proto)\n\t\trt = (struct rtable *)xfrm_lookup_route(net, &rt->dst,\n\t\t\t\t\t\t\tflowi4_to_flowi(flp4),\n\t\t\t\t\t\t\tsk, 0);\n\n\treturn rt;\n}\nEXPORT_SYMBOL_GPL(ip_route_output_flow);\n\n/* called with rcu_read_lock held */\nstatic int rt_fill_info(struct net *net,  __be32 dst, __be32 src, u32 table_id,\n\t\t\tstruct flowi4 *fl4, struct sk_buff *skb, u32 portid,\n\t\t\tu32 seq)\n{\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct rtmsg *r;\n\tstruct nlmsghdr *nlh;\n\tunsigned long expires = 0;\n\tu32 error;\n\tu32 metrics[RTAX_MAX];\n\n\tnlh = nlmsg_put(skb, portid, seq, RTM_NEWROUTE, sizeof(*r), 0);\n\tif (!nlh)\n\t\treturn -EMSGSIZE;\n\n\tr = nlmsg_data(nlh);\n\tr->rtm_family\t = AF_INET;\n\tr->rtm_dst_len\t= 32;\n\tr->rtm_src_len\t= 0;\n\tr->rtm_tos\t= fl4->flowi4_tos;\n\tr->rtm_table\t= table_id < 256 ? table_id : RT_TABLE_COMPAT;\n\tif (nla_put_u32(skb, RTA_TABLE, table_id))\n\t\tgoto nla_put_failure;\n\tr->rtm_type\t= rt->rt_type;\n\tr->rtm_scope\t= RT_SCOPE_UNIVERSE;\n\tr->rtm_protocol = RTPROT_UNSPEC;\n\tr->rtm_flags\t= (rt->rt_flags & ~0xFFFF) | RTM_F_CLONED;\n\tif (rt->rt_flags & RTCF_NOTIFY)\n\t\tr->rtm_flags |= RTM_F_NOTIFY;\n\tif (IPCB(skb)->flags & IPSKB_DOREDIRECT)\n\t\tr->rtm_flags |= RTCF_DOREDIRECT;\n\n\tif (nla_put_in_addr(skb, RTA_DST, dst))\n\t\tgoto nla_put_failure;\n\tif (src) {\n\t\tr->rtm_src_len = 32;\n\t\tif (nla_put_in_addr(skb, RTA_SRC, src))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (rt->dst.dev &&\n\t    nla_put_u32(skb, RTA_OIF, rt->dst.dev->ifindex))\n\t\tgoto nla_put_failure;\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tif (rt->dst.tclassid &&\n\t    nla_put_u32(skb, RTA_FLOW, rt->dst.tclassid))\n\t\tgoto nla_put_failure;\n#endif\n\tif (!rt_is_input_route(rt) &&\n\t    fl4->saddr != src) {\n\t\tif (nla_put_in_addr(skb, RTA_PREFSRC, fl4->saddr))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (rt->rt_uses_gateway &&\n\t    nla_put_in_addr(skb, RTA_GATEWAY, rt->rt_gateway))\n\t\tgoto nla_put_failure;\n\n\texpires = rt->dst.expires;\n\tif (expires) {\n\t\tunsigned long now = jiffies;\n\n\t\tif (time_before(now, expires))\n\t\t\texpires -= now;\n\t\telse\n\t\t\texpires = 0;\n\t}\n\n\tmemcpy(metrics, dst_metrics_ptr(&rt->dst), sizeof(metrics));\n\tif (rt->rt_pmtu && expires)\n\t\tmetrics[RTAX_MTU - 1] = rt->rt_pmtu;\n\tif (rtnetlink_put_metrics(skb, metrics) < 0)\n\t\tgoto nla_put_failure;\n\n\tif (fl4->flowi4_mark &&\n\t    nla_put_u32(skb, RTA_MARK, fl4->flowi4_mark))\n\t\tgoto nla_put_failure;\n\n\tif (!uid_eq(fl4->flowi4_uid, INVALID_UID) &&\n\t    nla_put_u32(skb, RTA_UID,\n\t\t\tfrom_kuid_munged(current_user_ns(), fl4->flowi4_uid)))\n\t\tgoto nla_put_failure;\n\n\terror = rt->dst.error;\n\n\tif (rt_is_input_route(rt)) {\n#ifdef CONFIG_IP_MROUTE\n\t\tif (ipv4_is_multicast(dst) && !ipv4_is_local_multicast(dst) &&\n\t\t    IPV4_DEVCONF_ALL(net, MC_FORWARDING)) {\n\t\t\tint err = ipmr_get_route(net, skb,\n\t\t\t\t\t\t fl4->saddr, fl4->daddr,\n\t\t\t\t\t\t r, portid);\n\n\t\t\tif (err <= 0) {\n\t\t\t\tif (err == 0)\n\t\t\t\t\treturn 0;\n\t\t\t\tgoto nla_put_failure;\n\t\t\t}\n\t\t} else\n#endif\n\t\t\tif (nla_put_u32(skb, RTA_IIF, skb->dev->ifindex))\n\t\t\t\tgoto nla_put_failure;\n\t}\n\n\tif (rtnl_put_cacheinfo(skb, &rt->dst, 0, expires, error) < 0)\n\t\tgoto nla_put_failure;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n\nnla_put_failure:\n\tnlmsg_cancel(skb, nlh);\n\treturn -EMSGSIZE;\n}\n\nstatic int inet_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct rtmsg *rtm;\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct fib_result res = {};\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\t__be32 dst = 0;\n\t__be32 src = 0;\n\tu32 iif;\n\tint err;\n\tint mark;\n\tstruct sk_buff *skb;\n\tu32 table_id = RT_TABLE_MAIN;\n\tkuid_t uid;\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_ipv4_policy,\n\t\t\t  extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\trtm = nlmsg_data(nlh);\n\n\tskb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);\n\tif (!skb) {\n\t\terr = -ENOBUFS;\n\t\tgoto errout;\n\t}\n\n\t/* Reserve room for dummy headers, this skb can pass\n\t   through good chunk of routing engine.\n\t */\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\n\tsrc = tb[RTA_SRC] ? nla_get_in_addr(tb[RTA_SRC]) : 0;\n\tdst = tb[RTA_DST] ? nla_get_in_addr(tb[RTA_DST]) : 0;\n\tiif = tb[RTA_IIF] ? nla_get_u32(tb[RTA_IIF]) : 0;\n\tmark = tb[RTA_MARK] ? nla_get_u32(tb[RTA_MARK]) : 0;\n\tif (tb[RTA_UID])\n\t\tuid = make_kuid(current_user_ns(), nla_get_u32(tb[RTA_UID]));\n\telse\n\t\tuid = (iif ? INVALID_UID : current_uid());\n\n\t/* Bugfix: need to give ip_route_input enough of an IP header to\n\t * not gag.\n\t */\n\tip_hdr(skb)->protocol = IPPROTO_UDP;\n\tip_hdr(skb)->saddr = src;\n\tip_hdr(skb)->daddr = dst;\n\n\tskb_reserve(skb, MAX_HEADER + sizeof(struct iphdr));\n\n\tmemset(&fl4, 0, sizeof(fl4));\n\tfl4.daddr = dst;\n\tfl4.saddr = src;\n\tfl4.flowi4_tos = rtm->rtm_tos;\n\tfl4.flowi4_oif = tb[RTA_OIF] ? nla_get_u32(tb[RTA_OIF]) : 0;\n\tfl4.flowi4_mark = mark;\n\tfl4.flowi4_uid = uid;\n\n\trcu_read_lock();\n\n\tif (iif) {\n\t\tstruct net_device *dev;\n\n\t\tdev = dev_get_by_index_rcu(net, iif);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto errout_free;\n\t\t}\n\n\t\tskb->protocol\t= htons(ETH_P_IP);\n\t\tskb->dev\t= dev;\n\t\tskb->mark\t= mark;\n\t\terr = ip_route_input_rcu(skb, dst, src, rtm->rtm_tos,\n\t\t\t\t\t dev, &res);\n\n\t\trt = skb_rtable(skb);\n\t\tif (err == 0 && rt->dst.error)\n\t\t\terr = -rt->dst.error;\n\t} else {\n\t\trt = ip_route_output_key_hash_rcu(net, &fl4, &res, skb);\n\t\terr = 0;\n\t\tif (IS_ERR(rt))\n\t\t\terr = PTR_ERR(rt);\n\t\telse\n\t\t\tskb_dst_set(skb, &rt->dst);\n\t}\n\n\tif (err)\n\t\tgoto errout_free;\n\n\tif (rtm->rtm_flags & RTM_F_NOTIFY)\n\t\trt->rt_flags |= RTCF_NOTIFY;\n\n\tif (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)\n\t\ttable_id = rt->rt_table_id;\n\n\tif (rtm->rtm_flags & RTM_F_FIB_MATCH) {\n\t\tif (!res.fi) {\n\t\t\terr = fib_props[res.type].error;\n\t\t\tif (!err)\n\t\t\t\terr = -EHOSTUNREACH;\n\t\t\tgoto errout_free;\n\t\t}\n\t\terr = fib_dump_info(skb, NETLINK_CB(in_skb).portid,\n\t\t\t\t    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,\n\t\t\t\t    rt->rt_type, res.prefix, res.prefixlen,\n\t\t\t\t    fl4.flowi4_tos, res.fi, 0);\n\t} else {\n\t\terr = rt_fill_info(net, dst, src, table_id, &fl4, skb,\n\t\t\t\t   NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);\n\t}\n\tif (err < 0)\n\t\tgoto errout_free;\n\n\trcu_read_unlock();\n\n\terr = rtnl_unicast(skb, net, NETLINK_CB(in_skb).portid);\nerrout:\n\treturn err;\n\nerrout_free:\n\trcu_read_unlock();\n\tkfree_skb(skb);\n\tgoto errout;\n}\n\nvoid ip_rt_multicast_event(struct in_device *in_dev)\n{\n\trt_cache_flush(dev_net(in_dev->dev));\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int ip_rt_gc_interval __read_mostly  = 60 * HZ;\nstatic int ip_rt_gc_min_interval __read_mostly\t= HZ / 2;\nstatic int ip_rt_gc_elasticity __read_mostly\t= 8;\n\nstatic int ipv4_sysctl_rtcache_flush(struct ctl_table *__ctl, int write,\n\t\t\t\t\tvoid __user *buffer,\n\t\t\t\t\tsize_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = (struct net *)__ctl->extra1;\n\n\tif (write) {\n\t\trt_cache_flush(net);\n\t\tfnhe_genid_bump(net);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic struct ctl_table ipv4_route_table[] = {\n\t{\n\t\t.procname\t= \"gc_thresh\",\n\t\t.data\t\t= &ipv4_dst_ops.gc_thresh,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"max_size\",\n\t\t.data\t\t= &ip_rt_max_size,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t/*  Deprecated. Use gc_min_interval_ms */\n\n\t\t.procname\t= \"gc_min_interval\",\n\t\t.data\t\t= &ip_rt_gc_min_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"gc_min_interval_ms\",\n\t\t.data\t\t= &ip_rt_gc_min_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_ms_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"gc_timeout\",\n\t\t.data\t\t= &ip_rt_gc_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"gc_interval\",\n\t\t.data\t\t= &ip_rt_gc_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"redirect_load\",\n\t\t.data\t\t= &ip_rt_redirect_load,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"redirect_number\",\n\t\t.data\t\t= &ip_rt_redirect_number,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"redirect_silence\",\n\t\t.data\t\t= &ip_rt_redirect_silence,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"error_cost\",\n\t\t.data\t\t= &ip_rt_error_cost,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"error_burst\",\n\t\t.data\t\t= &ip_rt_error_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"gc_elasticity\",\n\t\t.data\t\t= &ip_rt_gc_elasticity,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"mtu_expires\",\n\t\t.data\t\t= &ip_rt_mtu_expires,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"min_pmtu\",\n\t\t.data\t\t= &ip_rt_min_pmtu,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"min_adv_mss\",\n\t\t.data\t\t= &ip_rt_min_advmss,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{ }\n};\n\nstatic struct ctl_table ipv4_route_flush_table[] = {\n\t{\n\t\t.procname\t= \"flush\",\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0200,\n\t\t.proc_handler\t= ipv4_sysctl_rtcache_flush,\n\t},\n\t{ },\n};\n\nstatic __net_init int sysctl_route_net_init(struct net *net)\n{\n\tstruct ctl_table *tbl;\n\n\ttbl = ipv4_route_flush_table;\n\tif (!net_eq(net, &init_net)) {\n\t\ttbl = kmemdup(tbl, sizeof(ipv4_route_flush_table), GFP_KERNEL);\n\t\tif (!tbl)\n\t\t\tgoto err_dup;\n\n\t\t/* Don't export sysctls to unprivileged users */\n\t\tif (net->user_ns != &init_user_ns)\n\t\t\ttbl[0].procname = NULL;\n\t}\n\ttbl[0].extra1 = net;\n\n\tnet->ipv4.route_hdr = register_net_sysctl(net, \"net/ipv4/route\", tbl);\n\tif (!net->ipv4.route_hdr)\n\t\tgoto err_reg;\n\treturn 0;\n\nerr_reg:\n\tif (tbl != ipv4_route_flush_table)\n\t\tkfree(tbl);\nerr_dup:\n\treturn -ENOMEM;\n}\n\nstatic __net_exit void sysctl_route_net_exit(struct net *net)\n{\n\tstruct ctl_table *tbl;\n\n\ttbl = net->ipv4.route_hdr->ctl_table_arg;\n\tunregister_net_sysctl_table(net->ipv4.route_hdr);\n\tBUG_ON(tbl == ipv4_route_flush_table);\n\tkfree(tbl);\n}\n\nstatic __net_initdata struct pernet_operations sysctl_route_ops = {\n\t.init = sysctl_route_net_init,\n\t.exit = sysctl_route_net_exit,\n};\n#endif\n\nstatic __net_init int rt_genid_init(struct net *net)\n{\n\tatomic_set(&net->ipv4.rt_genid, 0);\n\tatomic_set(&net->fnhe_genid, 0);\n\tatomic_set(&net->ipv4.dev_addr_genid, get_random_int());\n\treturn 0;\n}\n\nstatic __net_initdata struct pernet_operations rt_genid_ops = {\n\t.init = rt_genid_init,\n};\n\nstatic int __net_init ipv4_inetpeer_init(struct net *net)\n{\n\tstruct inet_peer_base *bp = kmalloc(sizeof(*bp), GFP_KERNEL);\n\n\tif (!bp)\n\t\treturn -ENOMEM;\n\tinet_peer_base_init(bp);\n\tnet->ipv4.peers = bp;\n\treturn 0;\n}\n\nstatic void __net_exit ipv4_inetpeer_exit(struct net *net)\n{\n\tstruct inet_peer_base *bp = net->ipv4.peers;\n\n\tnet->ipv4.peers = NULL;\n\tinetpeer_invalidate_tree(bp);\n\tkfree(bp);\n}\n\nstatic __net_initdata struct pernet_operations ipv4_inetpeer_ops = {\n\t.init\t=\tipv4_inetpeer_init,\n\t.exit\t=\tipv4_inetpeer_exit,\n};\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\nstruct ip_rt_acct __percpu *ip_rt_acct __read_mostly;\n#endif /* CONFIG_IP_ROUTE_CLASSID */\n\nint __init ip_rt_init(void)\n{\n\tint rc = 0;\n\tint cpu;\n\n\tip_idents = kmalloc(IP_IDENTS_SZ * sizeof(*ip_idents), GFP_KERNEL);\n\tif (!ip_idents)\n\t\tpanic(\"IP: failed to allocate ip_idents\\n\");\n\n\tprandom_bytes(ip_idents, IP_IDENTS_SZ * sizeof(*ip_idents));\n\n\tip_tstamps = kcalloc(IP_IDENTS_SZ, sizeof(*ip_tstamps), GFP_KERNEL);\n\tif (!ip_tstamps)\n\t\tpanic(\"IP: failed to allocate ip_tstamps\\n\");\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct uncached_list *ul = &per_cpu(rt_uncached_list, cpu);\n\n\t\tINIT_LIST_HEAD(&ul->head);\n\t\tspin_lock_init(&ul->lock);\n\t}\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tip_rt_acct = __alloc_percpu(256 * sizeof(struct ip_rt_acct), __alignof__(struct ip_rt_acct));\n\tif (!ip_rt_acct)\n\t\tpanic(\"IP: failed to allocate ip_rt_acct\\n\");\n#endif\n\n\tipv4_dst_ops.kmem_cachep =\n\t\tkmem_cache_create(\"ip_dst_cache\", sizeof(struct rtable), 0,\n\t\t\t\t  SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);\n\n\tipv4_dst_blackhole_ops.kmem_cachep = ipv4_dst_ops.kmem_cachep;\n\n\tif (dst_entries_init(&ipv4_dst_ops) < 0)\n\t\tpanic(\"IP: failed to allocate ipv4_dst_ops counter\\n\");\n\n\tif (dst_entries_init(&ipv4_dst_blackhole_ops) < 0)\n\t\tpanic(\"IP: failed to allocate ipv4_dst_blackhole_ops counter\\n\");\n\n\tipv4_dst_ops.gc_thresh = ~0;\n\tip_rt_max_size = INT_MAX;\n\n\tdevinet_init();\n\tip_fib_init();\n\n\tif (ip_rt_proc_init())\n\t\tpr_err(\"Unable to create route proc files\\n\");\n#ifdef CONFIG_XFRM\n\txfrm_init();\n\txfrm4_init();\n#endif\n\trtnl_register(PF_INET, RTM_GETROUTE, inet_rtm_getroute, NULL, NULL);\n\n#ifdef CONFIG_SYSCTL\n\tregister_pernet_subsys(&sysctl_route_ops);\n#endif\n\tregister_pernet_subsys(&rt_genid_ops);\n\tregister_pernet_subsys(&ipv4_inetpeer_ops);\n\treturn rc;\n}\n\n#ifdef CONFIG_SYSCTL\n/*\n * We really need to sanitize the damn ipv4 init order, then all\n * this nonsense will go away.\n */\nvoid __init ip_static_sysctl_init(void)\n{\n\tregister_net_sysctl(&init_net, \"net/ipv4/route\", ipv4_route_table);\n}\n#endif\n"], "filenames": ["net/ipv4/route.c"], "buggy_code_start_loc": [2766], "buggy_code_end_loc": [2773], "fixing_code_start_loc": [2766], "fixing_code_end_loc": [2781], "type": "CWE-476", "message": "net/ipv4/route.c in the Linux kernel 4.13-rc1 through 4.13-rc6 is too late to check for a NULL fi field when RTM_F_FIB_MATCH is set, which allows local users to cause a denial of service (NULL pointer dereference) or possibly have unspecified other impact via crafted system calls. NOTE: this does not affect any stable release.", "other": {"cve": {"id": "CVE-2017-13686", "sourceIdentifier": "cve@mitre.org", "published": "2017-08-24T22:29:00.237", "lastModified": "2017-08-30T01:16:25.703", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "net/ipv4/route.c in the Linux kernel 4.13-rc1 through 4.13-rc6 is too late to check for a NULL fi field when RTM_F_FIB_MATCH is set, which allows local users to cause a denial of service (NULL pointer dereference) or possibly have unspecified other impact via crafted system calls. NOTE: this does not affect any stable release."}, {"lang": "es", "value": "net/ipv4/route.c en el kernel Linux 4.13-rc1 en su versi\u00f3n 4.13-rc6 comprueba demasiado tarde el campo fi NULL cuando se establece RTM_F_FIB_MATCH, lo que permite que usuarios locales provoquen una denegaci\u00f3n de servicio (desreferencia de puntero NULL) o que tengan un impacto sin especificar mediante llamadas del sistema manipuladas. NOTA: Esto no afecta a ninguna distribuci\u00f3n estable."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:4.13:rc1:*:*:*:*:*:*", "matchCriteriaId": "00DC2A8C-1E8B-4799-BE91-771BB321246D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:4.13:rc2:*:*:*:*:*:*", "matchCriteriaId": "CCBE5190-4EFC-4C4E-BE91-5711E04F3D26"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:4.13:rc3:*:*:*:*:*:*", "matchCriteriaId": "CB7715C0-AB55-458B-9242-8BEE9446F6D4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:4.13:rc4:*:*:*:*:*:*", "matchCriteriaId": "BA601A37-DCB0-45D4-AE9A-637221D488DA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:4.13:rc5:*:*:*:*:*:*", "matchCriteriaId": "44295B9F-B686-4BD8-8C59-26CB590557B9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:4.13:rc6:*:*:*:*:*:*", "matchCriteriaId": "BBAEC1E0-FD67-4750-9816-975321F3EF01"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=bc3aae2bbac46dd894c89db5d5e98f7f0ef9e205", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/bc3aae2bbac46dd894c89db5d5e98f7f0ef9e205", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/bc3aae2bbac46dd894c89db5d5e98f7f0ef9e205"}}