{"buggy_code": ["package containerimage\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"path\"\n\t\"runtime\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/content\"\n\tcontainerderrors \"github.com/containerd/containerd/errdefs\"\n\t\"github.com/containerd/containerd/images\"\n\t\"github.com/containerd/containerd/platforms\"\n\tctdreference \"github.com/containerd/containerd/reference\"\n\t\"github.com/containerd/containerd/remotes\"\n\t\"github.com/containerd/containerd/remotes/docker\"\n\t\"github.com/containerd/containerd/remotes/docker/schema1\"\n\tdistreference \"github.com/docker/distribution/reference\"\n\t\"github.com/docker/docker/distribution\"\n\t\"github.com/docker/docker/distribution/metadata\"\n\t\"github.com/docker/docker/distribution/xfer\"\n\t\"github.com/docker/docker/image\"\n\t\"github.com/docker/docker/layer\"\n\tpkgprogress \"github.com/docker/docker/pkg/progress\"\n\t\"github.com/docker/docker/reference\"\n\t\"github.com/moby/buildkit/cache\"\n\t\"github.com/moby/buildkit/client/llb\"\n\t\"github.com/moby/buildkit/session\"\n\t\"github.com/moby/buildkit/solver\"\n\t\"github.com/moby/buildkit/source\"\n\t\"github.com/moby/buildkit/util/flightcontrol\"\n\t\"github.com/moby/buildkit/util/imageutil\"\n\t\"github.com/moby/buildkit/util/progress\"\n\t\"github.com/moby/buildkit/util/resolver\"\n\tdigest \"github.com/opencontainers/go-digest\"\n\t\"github.com/opencontainers/image-spec/identity\"\n\tocispec \"github.com/opencontainers/image-spec/specs-go/v1\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/time/rate\"\n)\n\n// SourceOpt is options for creating the image source\ntype SourceOpt struct {\n\tContentStore    content.Store\n\tCacheAccessor   cache.Accessor\n\tReferenceStore  reference.Store\n\tDownloadManager distribution.RootFSDownloadManager\n\tMetadataStore   metadata.V2MetadataService\n\tImageStore      image.Store\n\tRegistryHosts   docker.RegistryHosts\n\tLayerStore      layer.Store\n}\n\n// Source is the source implementation for accessing container images\ntype Source struct {\n\tSourceOpt\n\tg flightcontrol.Group\n}\n\n// NewSource creates a new image source\nfunc NewSource(opt SourceOpt) (*Source, error) {\n\treturn &Source{SourceOpt: opt}, nil\n}\n\n// ID returns image scheme identifier\nfunc (is *Source) ID() string {\n\treturn source.DockerImageScheme\n}\n\nfunc (is *Source) resolveLocal(refStr string) (*image.Image, error) {\n\tref, err := distreference.ParseNormalizedNamed(refStr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdgst, err := is.ReferenceStore.Get(ref)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\timg, err := is.ImageStore.Get(image.ID(dgst))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn img, nil\n}\n\nfunc (is *Source) resolveRemote(ctx context.Context, ref string, platform *ocispec.Platform, sm *session.Manager, g session.Group) (digest.Digest, []byte, error) {\n\ttype t struct {\n\t\tdgst digest.Digest\n\t\tdt   []byte\n\t}\n\tp := platforms.DefaultSpec()\n\tif platform != nil {\n\t\tp = *platform\n\t}\n\t// key is used to synchronize resolutions that can happen in parallel when doing multi-stage.\n\tkey := \"getconfig::\" + ref + \"::\" + platforms.Format(p)\n\tres, err := is.g.Do(ctx, key, func(ctx context.Context) (interface{}, error) {\n\t\tres := resolver.DefaultPool.GetResolver(is.RegistryHosts, ref, \"pull\", sm, g)\n\t\tdgst, dt, err := imageutil.Config(ctx, ref, res, is.ContentStore, nil, platform)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn &t{dgst: dgst, dt: dt}, nil\n\t})\n\tvar typed *t\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\ttyped = res.(*t)\n\treturn typed.dgst, typed.dt, nil\n}\n\n// ResolveImageConfig returns image config for an image\nfunc (is *Source) ResolveImageConfig(ctx context.Context, ref string, opt llb.ResolveImageConfigOpt, sm *session.Manager, g session.Group) (digest.Digest, []byte, error) {\n\tresolveMode, err := source.ParseImageResolveMode(opt.ResolveMode)\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\tswitch resolveMode {\n\tcase source.ResolveModeForcePull:\n\t\tdgst, dt, err := is.resolveRemote(ctx, ref, opt.Platform, sm, g)\n\t\t// TODO: pull should fallback to local in case of failure to allow offline behavior\n\t\t// the fallback doesn't work currently\n\t\treturn dgst, dt, err\n\t\t/*\n\t\t\tif err == nil {\n\t\t\t\treturn dgst, dt, err\n\t\t\t}\n\t\t\t// fallback to local\n\t\t\tdt, err = is.resolveLocal(ref)\n\t\t\treturn \"\", dt, err\n\t\t*/\n\n\tcase source.ResolveModeDefault:\n\t\t// default == prefer local, but in the future could be smarter\n\t\tfallthrough\n\tcase source.ResolveModePreferLocal:\n\t\timg, err := is.resolveLocal(ref)\n\t\tif err == nil {\n\t\t\tif opt.Platform != nil && !platformMatches(img, opt.Platform) {\n\t\t\t\tlogrus.WithField(\"ref\", ref).Debugf(\"Requested build platform %s does not match local image platform %s, checking remote\",\n\t\t\t\t\tpath.Join(opt.Platform.OS, opt.Platform.Architecture, opt.Platform.Variant),\n\t\t\t\t\tpath.Join(img.OS, img.Architecture, img.Variant),\n\t\t\t\t)\n\t\t\t} else {\n\t\t\t\treturn \"\", img.RawJSON(), err\n\t\t\t}\n\t\t}\n\t\t// fallback to remote\n\t\treturn is.resolveRemote(ctx, ref, opt.Platform, sm, g)\n\t}\n\t// should never happen\n\treturn \"\", nil, fmt.Errorf(\"builder cannot resolve image %s: invalid mode %q\", ref, opt.ResolveMode)\n}\n\n// Resolve returns access to pulling for an identifier\nfunc (is *Source) Resolve(ctx context.Context, id source.Identifier, sm *session.Manager, vtx solver.Vertex) (source.SourceInstance, error) {\n\timageIdentifier, ok := id.(*source.ImageIdentifier)\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"invalid image identifier %v\", id)\n\t}\n\n\tplatform := platforms.DefaultSpec()\n\tif imageIdentifier.Platform != nil {\n\t\tplatform = *imageIdentifier.Platform\n\t}\n\n\tp := &puller{\n\t\tsrc: imageIdentifier,\n\t\tis:  is,\n\t\t//resolver: is.getResolver(is.RegistryHosts, imageIdentifier.Reference.String(), sm, g),\n\t\tplatform: platform,\n\t\tsm:       sm,\n\t}\n\treturn p, nil\n}\n\ntype puller struct {\n\tis               *Source\n\tresolveLocalOnce sync.Once\n\tsrc              *source.ImageIdentifier\n\tdesc             ocispec.Descriptor\n\tref              string\n\tconfig           []byte\n\tplatform         ocispec.Platform\n\tsm               *session.Manager\n}\n\nfunc (p *puller) resolver(g session.Group) remotes.Resolver {\n\treturn resolver.DefaultPool.GetResolver(p.is.RegistryHosts, p.src.Reference.String(), \"pull\", p.sm, g)\n}\n\nfunc (p *puller) mainManifestKey(platform ocispec.Platform) (digest.Digest, error) {\n\tdt, err := json.Marshal(struct {\n\t\tDigest  digest.Digest\n\t\tOS      string\n\t\tArch    string\n\t\tVariant string `json:\",omitempty\"`\n\t}{\n\t\tDigest:  p.desc.Digest,\n\t\tOS:      platform.OS,\n\t\tArch:    platform.Architecture,\n\t\tVariant: platform.Variant,\n\t})\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn digest.FromBytes(dt), nil\n}\n\nfunc (p *puller) resolveLocal() {\n\tp.resolveLocalOnce.Do(func() {\n\t\tdgst := p.src.Reference.Digest()\n\t\tif dgst != \"\" {\n\t\t\tinfo, err := p.is.ContentStore.Info(context.TODO(), dgst)\n\t\t\tif err == nil {\n\t\t\t\tp.ref = p.src.Reference.String()\n\t\t\t\tdesc := ocispec.Descriptor{\n\t\t\t\t\tSize:   info.Size,\n\t\t\t\t\tDigest: dgst,\n\t\t\t\t}\n\t\t\t\tra, err := p.is.ContentStore.ReaderAt(context.TODO(), desc)\n\t\t\t\tif err == nil {\n\t\t\t\t\tmt, err := imageutil.DetectManifestMediaType(ra)\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\tdesc.MediaType = mt\n\t\t\t\t\t\tp.desc = desc\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif p.src.ResolveMode == source.ResolveModeDefault || p.src.ResolveMode == source.ResolveModePreferLocal {\n\t\t\tref := p.src.Reference.String()\n\t\t\timg, err := p.is.resolveLocal(ref)\n\t\t\tif err == nil {\n\t\t\t\tif !platformMatches(img, &p.platform) {\n\t\t\t\t\tlogrus.WithField(\"ref\", ref).Debugf(\"Requested build platform %s does not match local image platform %s, not resolving\",\n\t\t\t\t\t\tpath.Join(p.platform.OS, p.platform.Architecture, p.platform.Variant),\n\t\t\t\t\t\tpath.Join(img.OS, img.Architecture, img.Variant),\n\t\t\t\t\t)\n\t\t\t\t} else {\n\t\t\t\t\tp.config = img.RawJSON()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc (p *puller) resolve(ctx context.Context, g session.Group) error {\n\t// key is used to synchronize resolutions that can happen in parallel when doing multi-stage.\n\tkey := \"resolve::\" + p.ref + \"::\" + platforms.Format(p.platform)\n\t_, err := p.is.g.Do(ctx, key, func(ctx context.Context) (_ interface{}, err error) {\n\t\tresolveProgressDone := oneOffProgress(ctx, \"resolve \"+p.src.Reference.String())\n\t\tdefer func() {\n\t\t\tresolveProgressDone(err)\n\t\t}()\n\n\t\tref, err := distreference.ParseNormalizedNamed(p.src.Reference.String())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif p.desc.Digest == \"\" && p.config == nil {\n\t\t\torigRef, desc, err := p.resolver(g).Resolve(ctx, ref.String())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tp.desc = desc\n\t\t\tp.ref = origRef\n\t\t}\n\n\t\t// Schema 1 manifests cannot be resolved to an image config\n\t\t// since the conversion must take place after all the content\n\t\t// has been read.\n\t\t// It may be possible to have a mapping between schema 1 manifests\n\t\t// and the schema 2 manifests they are converted to.\n\t\tif p.config == nil && p.desc.MediaType != images.MediaTypeDockerSchema1Manifest {\n\t\t\tref, err := distreference.WithDigest(ref, p.desc.Digest)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\t_, dt, err := p.is.ResolveImageConfig(ctx, ref.String(), llb.ResolveImageConfigOpt{Platform: &p.platform, ResolveMode: resolveModeToString(p.src.ResolveMode)}, p.sm, g)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tp.config = dt\n\t\t}\n\t\treturn nil, nil\n\t})\n\treturn err\n}\n\nfunc (p *puller) CacheKey(ctx context.Context, g session.Group, index int) (string, solver.CacheOpts, bool, error) {\n\tp.resolveLocal()\n\n\tif p.desc.Digest != \"\" && index == 0 {\n\t\tdgst, err := p.mainManifestKey(p.platform)\n\t\tif err != nil {\n\t\t\treturn \"\", nil, false, err\n\t\t}\n\t\treturn dgst.String(), nil, false, nil\n\t}\n\n\tif p.config != nil {\n\t\tk := cacheKeyFromConfig(p.config).String()\n\t\tif k == \"\" {\n\t\t\treturn digest.FromBytes(p.config).String(), nil, true, nil\n\t\t}\n\t\treturn k, nil, true, nil\n\t}\n\n\tif err := p.resolve(ctx, g); err != nil {\n\t\treturn \"\", nil, false, err\n\t}\n\n\tif p.desc.Digest != \"\" && index == 0 {\n\t\tdgst, err := p.mainManifestKey(p.platform)\n\t\tif err != nil {\n\t\t\treturn \"\", nil, false, err\n\t\t}\n\t\treturn dgst.String(), nil, false, nil\n\t}\n\n\tk := cacheKeyFromConfig(p.config).String()\n\tif k == \"\" {\n\t\tdgst, err := p.mainManifestKey(p.platform)\n\t\tif err != nil {\n\t\t\treturn \"\", nil, false, err\n\t\t}\n\t\treturn dgst.String(), nil, true, nil\n\t}\n\n\treturn k, nil, true, nil\n}\n\nfunc (p *puller) getRef(ctx context.Context, diffIDs []layer.DiffID, opts ...cache.RefOption) (cache.ImmutableRef, error) {\n\tvar parent cache.ImmutableRef\n\tif len(diffIDs) > 1 {\n\t\tvar err error\n\t\tparent, err = p.getRef(ctx, diffIDs[:len(diffIDs)-1], opts...)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer parent.Release(context.TODO())\n\t}\n\treturn p.is.CacheAccessor.GetByBlob(ctx, ocispec.Descriptor{\n\t\tAnnotations: map[string]string{\n\t\t\t\"containerd.io/uncompressed\": diffIDs[len(diffIDs)-1].String(),\n\t\t},\n\t}, parent, opts...)\n}\n\nfunc (p *puller) Snapshot(ctx context.Context, g session.Group) (cache.ImmutableRef, error) {\n\tp.resolveLocal()\n\tif err := p.resolve(ctx, g); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif p.config != nil {\n\t\timg, err := p.is.ImageStore.Get(image.ID(digest.FromBytes(p.config)))\n\t\tif err == nil {\n\t\t\tif len(img.RootFS.DiffIDs) == 0 {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tl, err := p.is.LayerStore.Get(img.RootFS.ChainID())\n\t\t\tif err == nil {\n\t\t\t\tlayer.ReleaseAndLog(p.is.LayerStore, l)\n\t\t\t\tref, err := p.getRef(ctx, img.RootFS.DiffIDs, cache.WithDescription(fmt.Sprintf(\"from local %s\", p.ref)))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\treturn ref, nil\n\t\t\t}\n\t\t}\n\t}\n\n\tongoing := newJobs(p.ref)\n\n\tpctx, stopProgress := context.WithCancel(ctx)\n\n\tpw, _, ctx := progress.FromContext(ctx)\n\tdefer pw.Close()\n\n\tprogressDone := make(chan struct{})\n\tgo func() {\n\t\tshowProgress(pctx, ongoing, p.is.ContentStore, pw)\n\t\tclose(progressDone)\n\t}()\n\tdefer func() {\n\t\t<-progressDone\n\t}()\n\n\tfetcher, err := p.resolver(g).Fetcher(ctx, p.ref)\n\tif err != nil {\n\t\tstopProgress()\n\t\treturn nil, err\n\t}\n\n\tplatform := platforms.Only(p.platform)\n\n\tvar (\n\t\tschema1Converter *schema1.Converter\n\t\thandlers         []images.Handler\n\t)\n\tif p.desc.MediaType == images.MediaTypeDockerSchema1Manifest {\n\t\tschema1Converter = schema1.NewConverter(p.is.ContentStore, fetcher)\n\t\thandlers = append(handlers, schema1Converter)\n\n\t\t// TODO: Optimize to do dispatch and integrate pulling with download manager,\n\t\t// leverage existing blob mapping and layer storage\n\t} else {\n\n\t\t// TODO: need a wrapper snapshot interface that combines content\n\t\t// and snapshots as 1) buildkit shouldn't have a dependency on contentstore\n\t\t// or 2) cachemanager should manage the contentstore\n\t\thandlers = append(handlers, images.HandlerFunc(func(ctx context.Context, desc ocispec.Descriptor) ([]ocispec.Descriptor, error) {\n\t\t\tswitch desc.MediaType {\n\t\t\tcase images.MediaTypeDockerSchema2Manifest, ocispec.MediaTypeImageManifest,\n\t\t\t\timages.MediaTypeDockerSchema2ManifestList, ocispec.MediaTypeImageIndex,\n\t\t\t\timages.MediaTypeDockerSchema2Config, ocispec.MediaTypeImageConfig:\n\t\t\tdefault:\n\t\t\t\treturn nil, images.ErrSkipDesc\n\t\t\t}\n\t\t\tongoing.add(desc)\n\t\t\treturn nil, nil\n\t\t}))\n\n\t\t// Get all the children for a descriptor\n\t\tchildrenHandler := images.ChildrenHandler(p.is.ContentStore)\n\t\t// Set any children labels for that content\n\t\tchildrenHandler = images.SetChildrenLabels(p.is.ContentStore, childrenHandler)\n\t\t// Filter the children by the platform\n\t\tchildrenHandler = images.FilterPlatforms(childrenHandler, platform)\n\t\t// Limit manifests pulled to the best match in an index\n\t\tchildrenHandler = images.LimitManifests(childrenHandler, platform, 1)\n\n\t\thandlers = append(handlers,\n\t\t\tremotes.FetchHandler(p.is.ContentStore, fetcher),\n\t\t\tchildrenHandler,\n\t\t)\n\t}\n\n\tif err := images.Dispatch(ctx, images.Handlers(handlers...), nil, p.desc); err != nil {\n\t\tstopProgress()\n\t\treturn nil, err\n\t}\n\tdefer stopProgress()\n\n\tif schema1Converter != nil {\n\t\tp.desc, err = schema1Converter.Convert(ctx)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tmfst, err := images.Manifest(ctx, p.is.ContentStore, p.desc, platform)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tconfig, err := images.Config(ctx, p.is.ContentStore, p.desc, platform)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdt, err := content.ReadBlob(ctx, p.is.ContentStore, config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar img ocispec.Image\n\tif err := json.Unmarshal(dt, &img); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif len(mfst.Layers) != len(img.RootFS.DiffIDs) {\n\t\treturn nil, errors.Errorf(\"invalid config for manifest\")\n\t}\n\n\tpchan := make(chan pkgprogress.Progress, 10)\n\tdefer close(pchan)\n\n\tgo func() {\n\t\tm := map[string]struct {\n\t\t\tst      time.Time\n\t\t\tlimiter *rate.Limiter\n\t\t}{}\n\t\tfor p := range pchan {\n\t\t\tif p.Action == \"Extracting\" {\n\t\t\t\tst, ok := m[p.ID]\n\t\t\t\tif !ok {\n\t\t\t\t\tst.st = time.Now()\n\t\t\t\t\tst.limiter = rate.NewLimiter(rate.Every(100*time.Millisecond), 1)\n\t\t\t\t\tm[p.ID] = st\n\t\t\t\t}\n\t\t\t\tvar end *time.Time\n\t\t\t\tif p.LastUpdate || st.limiter.Allow() {\n\t\t\t\t\tif p.LastUpdate {\n\t\t\t\t\t\ttm := time.Now()\n\t\t\t\t\t\tend = &tm\n\t\t\t\t\t}\n\t\t\t\t\t_ = pw.Write(\"extracting \"+p.ID, progress.Status{\n\t\t\t\t\t\tAction:    \"extract\",\n\t\t\t\t\t\tStarted:   &st.st,\n\t\t\t\t\t\tCompleted: end,\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tif len(mfst.Layers) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tlayers := make([]xfer.DownloadDescriptor, 0, len(mfst.Layers))\n\n\tfor i, desc := range mfst.Layers {\n\t\tongoing.add(desc)\n\t\tlayers = append(layers, &layerDescriptor{\n\t\t\tdesc:    desc,\n\t\t\tdiffID:  layer.DiffID(img.RootFS.DiffIDs[i]),\n\t\t\tfetcher: fetcher,\n\t\t\tref:     p.src.Reference,\n\t\t\tis:      p.is,\n\t\t})\n\t}\n\n\tdefer func() {\n\t\t<-progressDone\n\t\tfor _, desc := range mfst.Layers {\n\t\t\tp.is.ContentStore.Delete(context.TODO(), desc.Digest)\n\t\t}\n\t}()\n\n\tr := image.NewRootFS()\n\trootFS, release, err := p.is.DownloadManager.Download(ctx, *r, runtime.GOOS, layers, pkgprogress.ChanOutput(pchan))\n\tstopProgress()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tref, err := p.getRef(ctx, rootFS.DiffIDs, cache.WithDescription(fmt.Sprintf(\"pulled from %s\", p.ref)))\n\trelease()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO: handle windows layers for cross platform builds\n\n\tif p.src.RecordType != \"\" && cache.GetRecordType(ref) == \"\" {\n\t\tif err := cache.SetRecordType(ref, p.src.RecordType); err != nil {\n\t\t\tref.Release(context.TODO())\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn ref, nil\n}\n\n// Fetch(ctx context.Context, desc ocispec.Descriptor) (io.ReadCloser, error)\ntype layerDescriptor struct {\n\tis      *Source\n\tfetcher remotes.Fetcher\n\tdesc    ocispec.Descriptor\n\tdiffID  layer.DiffID\n\tref     ctdreference.Spec\n}\n\nfunc (ld *layerDescriptor) Key() string {\n\treturn \"v2:\" + ld.desc.Digest.String()\n}\n\nfunc (ld *layerDescriptor) ID() string {\n\treturn ld.desc.Digest.String()\n}\n\nfunc (ld *layerDescriptor) DiffID() (layer.DiffID, error) {\n\treturn ld.diffID, nil\n}\n\nfunc (ld *layerDescriptor) Download(ctx context.Context, progressOutput pkgprogress.Output) (io.ReadCloser, int64, error) {\n\trc, err := ld.fetcher.Fetch(ctx, ld.desc)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\tdefer rc.Close()\n\n\trefKey := remotes.MakeRefKey(ctx, ld.desc)\n\n\tld.is.ContentStore.Abort(ctx, refKey)\n\n\tif err := content.WriteBlob(ctx, ld.is.ContentStore, refKey, rc, ld.desc); err != nil {\n\t\tld.is.ContentStore.Abort(ctx, refKey)\n\t\treturn nil, 0, err\n\t}\n\n\tra, err := ld.is.ContentStore.ReaderAt(ctx, ld.desc)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\treturn ioutil.NopCloser(content.NewReader(ra)), ld.desc.Size, nil\n}\n\nfunc (ld *layerDescriptor) Close() {\n\t// ld.is.ContentStore.Delete(context.TODO(), ld.desc.Digest))\n}\n\nfunc (ld *layerDescriptor) Registered(diffID layer.DiffID) {\n\t// Cache mapping from this layer's DiffID to the blobsum\n\tld.is.MetadataStore.Add(diffID, metadata.V2Metadata{Digest: ld.desc.Digest, SourceRepository: ld.ref.Locator})\n}\n\nfunc showProgress(ctx context.Context, ongoing *jobs, cs content.Store, pw progress.Writer) {\n\tvar (\n\t\tticker   = time.NewTicker(100 * time.Millisecond)\n\t\tstatuses = map[string]statusInfo{}\n\t\tdone     bool\n\t)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\tcase <-ctx.Done():\n\t\t\tdone = true\n\t\t}\n\n\t\tresolved := \"resolved\"\n\t\tif !ongoing.isResolved() {\n\t\t\tresolved = \"resolving\"\n\t\t}\n\t\tstatuses[ongoing.name] = statusInfo{\n\t\t\tRef:    ongoing.name,\n\t\t\tStatus: resolved,\n\t\t}\n\n\t\tactives := make(map[string]statusInfo)\n\n\t\tif !done {\n\t\t\tactive, err := cs.ListStatuses(ctx)\n\t\t\tif err != nil {\n\t\t\t\t// log.G(ctx).WithError(err).Error(\"active check failed\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// update status of active entries!\n\t\t\tfor _, active := range active {\n\t\t\t\tactives[active.Ref] = statusInfo{\n\t\t\t\t\tRef:       active.Ref,\n\t\t\t\t\tStatus:    \"downloading\",\n\t\t\t\t\tOffset:    active.Offset,\n\t\t\t\t\tTotal:     active.Total,\n\t\t\t\t\tStartedAt: active.StartedAt,\n\t\t\t\t\tUpdatedAt: active.UpdatedAt,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// now, update the items in jobs that are not in active\n\t\tfor _, j := range ongoing.jobs() {\n\t\t\trefKey := remotes.MakeRefKey(ctx, j.Descriptor)\n\t\t\tif a, ok := actives[refKey]; ok {\n\t\t\t\tstarted := j.started\n\t\t\t\t_ = pw.Write(j.Digest.String(), progress.Status{\n\t\t\t\t\tAction:  a.Status,\n\t\t\t\t\tTotal:   int(a.Total),\n\t\t\t\t\tCurrent: int(a.Offset),\n\t\t\t\t\tStarted: &started,\n\t\t\t\t})\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif !j.done {\n\t\t\t\tinfo, err := cs.Info(context.TODO(), j.Digest)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif containerderrors.IsNotFound(err) {\n\t\t\t\t\t\t// _ = pw.Write(j.Digest.String(), progress.Status{\n\t\t\t\t\t\t// \tAction: \"waiting\",\n\t\t\t\t\t\t// })\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tj.done = true\n\t\t\t\t}\n\n\t\t\t\tif done || j.done {\n\t\t\t\t\tstarted := j.started\n\t\t\t\t\tcreatedAt := info.CreatedAt\n\t\t\t\t\t_ = pw.Write(j.Digest.String(), progress.Status{\n\t\t\t\t\t\tAction:    \"done\",\n\t\t\t\t\t\tCurrent:   int(info.Size),\n\t\t\t\t\t\tTotal:     int(info.Size),\n\t\t\t\t\t\tCompleted: &createdAt,\n\t\t\t\t\t\tStarted:   &started,\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif done {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// jobs provides a way of identifying the download keys for a particular task\n// encountering during the pull walk.\n//\n// This is very minimal and will probably be replaced with something more\n// featured.\ntype jobs struct {\n\tname     string\n\tadded    map[digest.Digest]*job\n\tmu       sync.Mutex\n\tresolved bool\n}\n\ntype job struct {\n\tocispec.Descriptor\n\tdone    bool\n\tstarted time.Time\n}\n\nfunc newJobs(name string) *jobs {\n\treturn &jobs{\n\t\tname:  name,\n\t\tadded: make(map[digest.Digest]*job),\n\t}\n}\n\nfunc (j *jobs) add(desc ocispec.Descriptor) {\n\tj.mu.Lock()\n\tdefer j.mu.Unlock()\n\n\tif _, ok := j.added[desc.Digest]; ok {\n\t\treturn\n\t}\n\tj.added[desc.Digest] = &job{\n\t\tDescriptor: desc,\n\t\tstarted:    time.Now(),\n\t}\n}\n\nfunc (j *jobs) jobs() []*job {\n\tj.mu.Lock()\n\tdefer j.mu.Unlock()\n\n\tdescs := make([]*job, 0, len(j.added))\n\tfor _, j := range j.added {\n\t\tdescs = append(descs, j)\n\t}\n\treturn descs\n}\n\nfunc (j *jobs) isResolved() bool {\n\tj.mu.Lock()\n\tdefer j.mu.Unlock()\n\treturn j.resolved\n}\n\ntype statusInfo struct {\n\tRef       string\n\tStatus    string\n\tOffset    int64\n\tTotal     int64\n\tStartedAt time.Time\n\tUpdatedAt time.Time\n}\n\nfunc oneOffProgress(ctx context.Context, id string) func(err error) error {\n\tpw, _, _ := progress.FromContext(ctx)\n\tnow := time.Now()\n\tst := progress.Status{\n\t\tStarted: &now,\n\t}\n\t_ = pw.Write(id, st)\n\treturn func(err error) error {\n\t\t// TODO: set error on status\n\t\tnow := time.Now()\n\t\tst.Completed = &now\n\t\t_ = pw.Write(id, st)\n\t\t_ = pw.Close()\n\t\treturn err\n\t}\n}\n\n// cacheKeyFromConfig returns a stable digest from image config. If image config\n// is a known oci image we will use chainID of layers.\nfunc cacheKeyFromConfig(dt []byte) digest.Digest {\n\tvar img ocispec.Image\n\terr := json.Unmarshal(dt, &img)\n\tif err != nil {\n\t\treturn digest.FromBytes(dt)\n\t}\n\tif img.RootFS.Type != \"layers\" || len(img.RootFS.DiffIDs) == 0 {\n\t\treturn \"\"\n\t}\n\treturn identity.ChainID(img.RootFS.DiffIDs)\n}\n\n// resolveModeToString is the equivalent of github.com/moby/buildkit/solver/llb.ResolveMode.String()\n// FIXME: add String method on source.ResolveMode\nfunc resolveModeToString(rm source.ResolveMode) string {\n\tswitch rm {\n\tcase source.ResolveModeDefault:\n\t\treturn \"default\"\n\tcase source.ResolveModeForcePull:\n\t\treturn \"pull\"\n\tcase source.ResolveModePreferLocal:\n\t\treturn \"local\"\n\t}\n\treturn \"\"\n}\n\nfunc platformMatches(img *image.Image, p *ocispec.Platform) bool {\n\tif img.Architecture != p.Architecture {\n\t\treturn false\n\t}\n\tif img.Variant != \"\" && img.Variant != p.Variant {\n\t\treturn false\n\t}\n\treturn img.OS == p.OS\n}\n", "package distribution // import \"github.com/docker/docker/distribution\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime\"\n\t\"strings\"\n\n\t\"github.com/containerd/containerd/log\"\n\t\"github.com/containerd/containerd/platforms\"\n\t\"github.com/docker/distribution\"\n\t\"github.com/docker/distribution/manifest/manifestlist\"\n\t\"github.com/docker/distribution/manifest/ocischema\"\n\t\"github.com/docker/distribution/manifest/schema1\"\n\t\"github.com/docker/distribution/manifest/schema2\"\n\t\"github.com/docker/distribution/reference\"\n\t\"github.com/docker/distribution/registry/api/errcode\"\n\t\"github.com/docker/distribution/registry/client/auth\"\n\t\"github.com/docker/distribution/registry/client/transport\"\n\t\"github.com/docker/docker/distribution/metadata\"\n\t\"github.com/docker/docker/distribution/xfer\"\n\t\"github.com/docker/docker/image\"\n\tv1 \"github.com/docker/docker/image/v1\"\n\t\"github.com/docker/docker/layer\"\n\t\"github.com/docker/docker/pkg/ioutils\"\n\t\"github.com/docker/docker/pkg/progress\"\n\t\"github.com/docker/docker/pkg/stringid\"\n\t\"github.com/docker/docker/pkg/system\"\n\trefstore \"github.com/docker/docker/reference\"\n\t\"github.com/docker/docker/registry\"\n\tdigest \"github.com/opencontainers/go-digest\"\n\tspecs \"github.com/opencontainers/image-spec/specs-go/v1\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nvar (\n\terrRootFSMismatch = errors.New(\"layers from manifest don't match image configuration\")\n\terrRootFSInvalid  = errors.New(\"invalid rootfs in image configuration\")\n)\n\n// ImageConfigPullError is an error pulling the image config blob\n// (only applies to schema2).\ntype ImageConfigPullError struct {\n\tErr error\n}\n\n// Error returns the error string for ImageConfigPullError.\nfunc (e ImageConfigPullError) Error() string {\n\treturn \"error pulling image configuration: \" + e.Err.Error()\n}\n\ntype v2Puller struct {\n\tV2MetadataService metadata.V2MetadataService\n\tendpoint          registry.APIEndpoint\n\tconfig            *ImagePullConfig\n\trepoInfo          *registry.RepositoryInfo\n\trepo              distribution.Repository\n\t// confirmedV2 is set to true if we confirm we're talking to a v2\n\t// registry. This is used to limit fallbacks to the v1 protocol.\n\tconfirmedV2   bool\n\tmanifestStore *manifestStore\n}\n\nfunc (p *v2Puller) Pull(ctx context.Context, ref reference.Named, platform *specs.Platform) (err error) {\n\t// TODO(tiborvass): was ReceiveTimeout\n\tp.repo, p.confirmedV2, err = NewV2Repository(ctx, p.repoInfo, p.endpoint, p.config.MetaHeaders, p.config.AuthConfig, \"pull\")\n\tif err != nil {\n\t\tlogrus.Warnf(\"Error getting v2 registry: %v\", err)\n\t\treturn err\n\t}\n\n\tp.manifestStore.remote, err = p.repo.Manifests(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err = p.pullV2Repository(ctx, ref, platform); err != nil {\n\t\tif _, ok := err.(fallbackError); ok {\n\t\t\treturn err\n\t\t}\n\t\tif continueOnError(err, p.endpoint.Mirror) {\n\t\t\treturn fallbackError{\n\t\t\t\terr:         err,\n\t\t\t\tconfirmedV2: p.confirmedV2,\n\t\t\t\ttransportOK: true,\n\t\t\t}\n\t\t}\n\t}\n\treturn err\n}\n\nfunc (p *v2Puller) pullV2Repository(ctx context.Context, ref reference.Named, platform *specs.Platform) (err error) {\n\tvar layersDownloaded bool\n\tif !reference.IsNameOnly(ref) {\n\t\tlayersDownloaded, err = p.pullV2Tag(ctx, ref, platform)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\ttags, err := p.repo.Tags(ctx).All(ctx)\n\t\tif err != nil {\n\t\t\t// If this repository doesn't exist on V2, we should\n\t\t\t// permit a fallback to V1.\n\t\t\treturn allowV1Fallback(err)\n\t\t}\n\n\t\t// The v2 registry knows about this repository, so we will not\n\t\t// allow fallback to the v1 protocol even if we encounter an\n\t\t// error later on.\n\t\tp.confirmedV2 = true\n\n\t\tfor _, tag := range tags {\n\t\t\ttagRef, err := reference.WithTag(ref, tag)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tpulledNew, err := p.pullV2Tag(ctx, tagRef, platform)\n\t\t\tif err != nil {\n\t\t\t\t// Since this is the pull-all-tags case, don't\n\t\t\t\t// allow an error pulling a particular tag to\n\t\t\t\t// make the whole pull fall back to v1.\n\t\t\t\tif fallbackErr, ok := err.(fallbackError); ok {\n\t\t\t\t\treturn fallbackErr.err\n\t\t\t\t}\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t// pulledNew is true if either new layers were downloaded OR if existing images were newly tagged\n\t\t\t// TODO(tiborvass): should we change the name of `layersDownload`? What about message in WriteStatus?\n\t\t\tlayersDownloaded = layersDownloaded || pulledNew\n\t\t}\n\t}\n\n\twriteStatus(reference.FamiliarString(ref), p.config.ProgressOutput, layersDownloaded)\n\n\treturn nil\n}\n\ntype v2LayerDescriptor struct {\n\tdigest            digest.Digest\n\tdiffID            layer.DiffID\n\trepoInfo          *registry.RepositoryInfo\n\trepo              distribution.Repository\n\tV2MetadataService metadata.V2MetadataService\n\ttmpFile           *os.File\n\tverifier          digest.Verifier\n\tsrc               distribution.Descriptor\n}\n\nfunc (ld *v2LayerDescriptor) Key() string {\n\treturn \"v2:\" + ld.digest.String()\n}\n\nfunc (ld *v2LayerDescriptor) ID() string {\n\treturn stringid.TruncateID(ld.digest.String())\n}\n\nfunc (ld *v2LayerDescriptor) DiffID() (layer.DiffID, error) {\n\tif ld.diffID != \"\" {\n\t\treturn ld.diffID, nil\n\t}\n\treturn ld.V2MetadataService.GetDiffID(ld.digest)\n}\n\nfunc (ld *v2LayerDescriptor) Download(ctx context.Context, progressOutput progress.Output) (io.ReadCloser, int64, error) {\n\tlogrus.Debugf(\"pulling blob %q\", ld.digest)\n\n\tvar (\n\t\terr    error\n\t\toffset int64\n\t)\n\n\tif ld.tmpFile == nil {\n\t\tld.tmpFile, err = createDownloadFile()\n\t\tif err != nil {\n\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t}\n\t} else {\n\t\toffset, err = ld.tmpFile.Seek(0, io.SeekEnd)\n\t\tif err != nil {\n\t\t\tlogrus.Debugf(\"error seeking to end of download file: %v\", err)\n\t\t\toffset = 0\n\n\t\t\tld.tmpFile.Close()\n\t\t\tif err := os.Remove(ld.tmpFile.Name()); err != nil {\n\t\t\t\tlogrus.Errorf(\"Failed to remove temp file: %s\", ld.tmpFile.Name())\n\t\t\t}\n\t\t\tld.tmpFile, err = createDownloadFile()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\t\t} else if offset != 0 {\n\t\t\tlogrus.Debugf(\"attempting to resume download of %q from %d bytes\", ld.digest, offset)\n\t\t}\n\t}\n\n\ttmpFile := ld.tmpFile\n\n\tlayerDownload, err := ld.open(ctx)\n\tif err != nil {\n\t\tlogrus.Errorf(\"Error initiating layer download: %v\", err)\n\t\treturn nil, 0, retryOnError(err)\n\t}\n\n\tif offset != 0 {\n\t\t_, err := layerDownload.Seek(offset, io.SeekStart)\n\t\tif err != nil {\n\t\t\tif err := ld.truncateDownloadFile(); err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\t\t\treturn nil, 0, err\n\t\t}\n\t}\n\tsize, err := layerDownload.Seek(0, io.SeekEnd)\n\tif err != nil {\n\t\t// Seek failed, perhaps because there was no Content-Length\n\t\t// header. This shouldn't fail the download, because we can\n\t\t// still continue without a progress bar.\n\t\tsize = 0\n\t} else {\n\t\tif size != 0 && offset > size {\n\t\t\tlogrus.Debug(\"Partial download is larger than full blob. Starting over\")\n\t\t\toffset = 0\n\t\t\tif err := ld.truncateDownloadFile(); err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\t\t}\n\n\t\t// Restore the seek offset either at the beginning of the\n\t\t// stream, or just after the last byte we have from previous\n\t\t// attempts.\n\t\t_, err = layerDownload.Seek(offset, io.SeekStart)\n\t\tif err != nil {\n\t\t\treturn nil, 0, err\n\t\t}\n\t}\n\n\treader := progress.NewProgressReader(ioutils.NewCancelReadCloser(ctx, layerDownload), progressOutput, size-offset, ld.ID(), \"Downloading\")\n\tdefer reader.Close()\n\n\tif ld.verifier == nil {\n\t\tld.verifier = ld.digest.Verifier()\n\t}\n\n\t_, err = io.Copy(tmpFile, io.TeeReader(reader, ld.verifier))\n\tif err != nil {\n\t\tif err == transport.ErrWrongCodeForByteRange {\n\t\t\tif err := ld.truncateDownloadFile(); err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\t\t\treturn nil, 0, err\n\t\t}\n\t\treturn nil, 0, retryOnError(err)\n\t}\n\n\tprogress.Update(progressOutput, ld.ID(), \"Verifying Checksum\")\n\n\tif !ld.verifier.Verified() {\n\t\terr = fmt.Errorf(\"filesystem layer verification failed for digest %s\", ld.digest)\n\t\tlogrus.Error(err)\n\n\t\t// Allow a retry if this digest verification error happened\n\t\t// after a resumed download.\n\t\tif offset != 0 {\n\t\t\tif err := ld.truncateDownloadFile(); err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\n\t\t\treturn nil, 0, err\n\t\t}\n\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t}\n\n\tprogress.Update(progressOutput, ld.ID(), \"Download complete\")\n\n\tlogrus.Debugf(\"Downloaded %s to tempfile %s\", ld.ID(), tmpFile.Name())\n\n\t_, err = tmpFile.Seek(0, io.SeekStart)\n\tif err != nil {\n\t\ttmpFile.Close()\n\t\tif err := os.Remove(tmpFile.Name()); err != nil {\n\t\t\tlogrus.Errorf(\"Failed to remove temp file: %s\", tmpFile.Name())\n\t\t}\n\t\tld.tmpFile = nil\n\t\tld.verifier = nil\n\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t}\n\n\t// hand off the temporary file to the download manager, so it will only\n\t// be closed once\n\tld.tmpFile = nil\n\n\treturn ioutils.NewReadCloserWrapper(tmpFile, func() error {\n\t\ttmpFile.Close()\n\t\terr := os.RemoveAll(tmpFile.Name())\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed to remove temp file: %s\", tmpFile.Name())\n\t\t}\n\t\treturn err\n\t}), size, nil\n}\n\nfunc (ld *v2LayerDescriptor) Close() {\n\tif ld.tmpFile != nil {\n\t\tld.tmpFile.Close()\n\t\tif err := os.RemoveAll(ld.tmpFile.Name()); err != nil {\n\t\t\tlogrus.Errorf(\"Failed to remove temp file: %s\", ld.tmpFile.Name())\n\t\t}\n\t}\n}\n\nfunc (ld *v2LayerDescriptor) truncateDownloadFile() error {\n\t// Need a new hash context since we will be redoing the download\n\tld.verifier = nil\n\n\tif _, err := ld.tmpFile.Seek(0, io.SeekStart); err != nil {\n\t\tlogrus.Errorf(\"error seeking to beginning of download file: %v\", err)\n\t\treturn err\n\t}\n\n\tif err := ld.tmpFile.Truncate(0); err != nil {\n\t\tlogrus.Errorf(\"error truncating download file: %v\", err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (ld *v2LayerDescriptor) Registered(diffID layer.DiffID) {\n\t// Cache mapping from this layer's DiffID to the blobsum\n\tld.V2MetadataService.Add(diffID, metadata.V2Metadata{Digest: ld.digest, SourceRepository: ld.repoInfo.Name.Name()})\n}\n\nfunc (p *v2Puller) pullV2Tag(ctx context.Context, ref reference.Named, platform *specs.Platform) (tagUpdated bool, err error) {\n\n\tvar (\n\t\ttagOrDigest string // Used for logging/progress only\n\t\tdgst        digest.Digest\n\t\tmt          string\n\t\tsize        int64\n\t\ttagged      reference.NamedTagged\n\t\tisTagged    bool\n\t)\n\tif digested, isDigested := ref.(reference.Canonical); isDigested {\n\t\tdgst = digested.Digest()\n\t\ttagOrDigest = digested.String()\n\t} else if tagged, isTagged = ref.(reference.NamedTagged); isTagged {\n\t\ttagService := p.repo.Tags(ctx)\n\t\tdesc, err := tagService.Get(ctx, tagged.Tag())\n\t\tif err != nil {\n\t\t\treturn false, allowV1Fallback(err)\n\t\t}\n\n\t\tdgst = desc.Digest\n\t\ttagOrDigest = tagged.Tag()\n\t\tmt = desc.MediaType\n\t\tsize = desc.Size\n\t} else {\n\t\treturn false, fmt.Errorf(\"internal error: reference has neither a tag nor a digest: %s\", reference.FamiliarString(ref))\n\t}\n\n\tctx = log.WithLogger(ctx, logrus.WithFields(\n\t\tlogrus.Fields{\n\t\t\t\"digest\": dgst,\n\t\t\t\"remote\": ref,\n\t\t}))\n\n\tdesc := specs.Descriptor{\n\t\tMediaType: mt,\n\t\tDigest:    dgst,\n\t\tSize:      size,\n\t}\n\tmanifest, err := p.manifestStore.Get(ctx, desc)\n\tif err != nil {\n\t\tif isTagged && isNotFound(errors.Cause(err)) {\n\t\t\tlogrus.WithField(\"ref\", ref).WithError(err).Debug(\"Falling back to pull manifest by tag\")\n\n\t\t\tmsg := `%s Failed to pull manifest by the resolved digest. This registry does not\n\tappear to conform to the distribution registry specification; falling back to\n\tpull by tag.  This fallback is DEPRECATED, and will be removed in a future\n\trelease.  Please contact admins of %s. %s\n`\n\n\t\t\twarnEmoji := \"\\U000026A0\\U0000FE0F\"\n\t\t\tprogress.Messagef(p.config.ProgressOutput, \"WARNING\", msg, warnEmoji, p.endpoint.URL, warnEmoji)\n\n\t\t\t// Fetch by tag worked, but fetch by digest didn't.\n\t\t\t// This is a broken registry implementation.\n\t\t\t// We'll fallback to the old behavior and get the manifest by tag.\n\t\t\tvar ms distribution.ManifestService\n\t\t\tms, err = p.repo.Manifests(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\n\t\t\tmanifest, err = ms.Get(ctx, \"\", distribution.WithTag(tagged.Tag()))\n\t\t\terr = errors.Wrap(err, \"error after falling back to get manifest by tag\")\n\t\t}\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t}\n\n\tif manifest == nil {\n\t\treturn false, fmt.Errorf(\"image manifest does not exist for tag or digest %q\", tagOrDigest)\n\t}\n\n\tif m, ok := manifest.(*schema2.DeserializedManifest); ok {\n\t\tvar allowedMediatype bool\n\t\tfor _, t := range p.config.Schema2Types {\n\t\t\tif m.Manifest.Config.MediaType == t {\n\t\t\t\tallowedMediatype = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !allowedMediatype {\n\t\t\tconfigClass := mediaTypeClasses[m.Manifest.Config.MediaType]\n\t\t\tif configClass == \"\" {\n\t\t\t\tconfigClass = \"unknown\"\n\t\t\t}\n\t\t\treturn false, invalidManifestClassError{m.Manifest.Config.MediaType, configClass}\n\t\t}\n\t}\n\n\t// If manSvc.Get succeeded, we can be confident that the registry on\n\t// the other side speaks the v2 protocol.\n\tp.confirmedV2 = true\n\n\tlogrus.Debugf(\"Pulling ref from V2 registry: %s\", reference.FamiliarString(ref))\n\tprogress.Message(p.config.ProgressOutput, tagOrDigest, \"Pulling from \"+reference.FamiliarName(p.repo.Named()))\n\n\tvar (\n\t\tid             digest.Digest\n\t\tmanifestDigest digest.Digest\n\t)\n\n\tswitch v := manifest.(type) {\n\tcase *schema1.SignedManifest:\n\t\tif p.config.RequireSchema2 {\n\t\t\treturn false, fmt.Errorf(\"invalid manifest: not schema2\")\n\t\t}\n\n\t\t// give registries time to upgrade to schema2 and only warn if we know a registry has been upgraded long time ago\n\t\t// TODO: condition to be removed\n\t\tif reference.Domain(ref) == \"docker.io\" {\n\t\t\tmsg := fmt.Sprintf(\"Image %s uses outdated schema1 manifest format. Please upgrade to a schema2 image for better future compatibility. More information at https://docs.docker.com/registry/spec/deprecated-schema-v1/\", ref)\n\t\t\tlogrus.Warn(msg)\n\t\t\tprogress.Message(p.config.ProgressOutput, \"\", msg)\n\t\t}\n\n\t\tid, manifestDigest, err = p.pullSchema1(ctx, ref, v, platform)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\tcase *schema2.DeserializedManifest:\n\t\tid, manifestDigest, err = p.pullSchema2(ctx, ref, v, platform)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\tcase *ocischema.DeserializedManifest:\n\t\tid, manifestDigest, err = p.pullOCI(ctx, ref, v, platform)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\tcase *manifestlist.DeserializedManifestList:\n\t\tid, manifestDigest, err = p.pullManifestList(ctx, ref, v, platform)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\tdefault:\n\t\treturn false, invalidManifestFormatError{}\n\t}\n\n\tprogress.Message(p.config.ProgressOutput, \"\", \"Digest: \"+manifestDigest.String())\n\n\tif p.config.ReferenceStore != nil {\n\t\toldTagID, err := p.config.ReferenceStore.Get(ref)\n\t\tif err == nil {\n\t\t\tif oldTagID == id {\n\t\t\t\treturn false, addDigestReference(p.config.ReferenceStore, ref, manifestDigest, id)\n\t\t\t}\n\t\t} else if err != refstore.ErrDoesNotExist {\n\t\t\treturn false, err\n\t\t}\n\n\t\tif canonical, ok := ref.(reference.Canonical); ok {\n\t\t\tif err = p.config.ReferenceStore.AddDigest(canonical, id, true); err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t} else {\n\t\t\tif err = addDigestReference(p.config.ReferenceStore, ref, manifestDigest, id); err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\tif err = p.config.ReferenceStore.AddTag(ref, id, true); err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t}\n\t}\n\treturn true, nil\n}\n\nfunc (p *v2Puller) pullSchema1(ctx context.Context, ref reference.Reference, unverifiedManifest *schema1.SignedManifest, platform *specs.Platform) (id digest.Digest, manifestDigest digest.Digest, err error) {\n\tvar verifiedManifest *schema1.Manifest\n\tverifiedManifest, err = verifySchema1Manifest(unverifiedManifest, ref)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\trootFS := image.NewRootFS()\n\n\t// remove duplicate layers and check parent chain validity\n\terr = fixManifestLayers(verifiedManifest)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tvar descriptors []xfer.DownloadDescriptor\n\n\t// Image history converted to the new format\n\tvar history []image.History\n\n\t// Note that the order of this loop is in the direction of bottom-most\n\t// to top-most, so that the downloads slice gets ordered correctly.\n\tfor i := len(verifiedManifest.FSLayers) - 1; i >= 0; i-- {\n\t\tblobSum := verifiedManifest.FSLayers[i].BlobSum\n\n\t\tvar throwAway struct {\n\t\t\tThrowAway bool `json:\"throwaway,omitempty\"`\n\t\t}\n\t\tif err := json.Unmarshal([]byte(verifiedManifest.History[i].V1Compatibility), &throwAway); err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\n\t\th, err := v1.HistoryFromConfig([]byte(verifiedManifest.History[i].V1Compatibility), throwAway.ThrowAway)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\t\thistory = append(history, h)\n\n\t\tif throwAway.ThrowAway {\n\t\t\tcontinue\n\t\t}\n\n\t\tlayerDescriptor := &v2LayerDescriptor{\n\t\t\tdigest:            blobSum,\n\t\t\trepoInfo:          p.repoInfo,\n\t\t\trepo:              p.repo,\n\t\t\tV2MetadataService: p.V2MetadataService,\n\t\t}\n\n\t\tdescriptors = append(descriptors, layerDescriptor)\n\t}\n\n\t// The v1 manifest itself doesn't directly contain an OS. However,\n\t// the history does, but unfortunately that's a string, so search through\n\t// all the history until hopefully we find one which indicates the OS.\n\t// supertest2014/nyan is an example of a registry image with schemav1.\n\tconfigOS := runtime.GOOS\n\tif system.LCOWSupported() {\n\t\ttype config struct {\n\t\t\tOs string `json:\"os,omitempty\"`\n\t\t}\n\t\tfor _, v := range verifiedManifest.History {\n\t\t\tvar c config\n\t\t\tif err := json.Unmarshal([]byte(v.V1Compatibility), &c); err == nil {\n\t\t\t\tif c.Os != \"\" {\n\t\t\t\t\tconfigOS = c.Os\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// In the situation that the API call didn't specify an OS explicitly, but\n\t// we support the operating system, switch to that operating system.\n\t// eg FROM supertest2014/nyan with no platform specifier, and docker build\n\t// with no --platform= flag under LCOW.\n\trequestedOS := \"\"\n\tif platform != nil {\n\t\trequestedOS = platform.OS\n\t} else if system.IsOSSupported(configOS) {\n\t\trequestedOS = configOS\n\t}\n\n\t// Early bath if the requested OS doesn't match that of the configuration.\n\t// This avoids doing the download, only to potentially fail later.\n\tif !strings.EqualFold(configOS, requestedOS) {\n\t\treturn \"\", \"\", fmt.Errorf(\"cannot download image with operating system %q when requesting %q\", configOS, requestedOS)\n\t}\n\n\tresultRootFS, release, err := p.config.DownloadManager.Download(ctx, *rootFS, configOS, descriptors, p.config.ProgressOutput)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tdefer release()\n\n\tconfig, err := v1.MakeConfigFromV1Config([]byte(verifiedManifest.History[0].V1Compatibility), &resultRootFS, history)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\timageID, err := p.config.ImageStore.Put(ctx, config)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tmanifestDigest = digest.FromBytes(unverifiedManifest.Canonical)\n\n\treturn imageID, manifestDigest, nil\n}\n\nfunc (p *v2Puller) pullSchema2Layers(ctx context.Context, target distribution.Descriptor, layers []distribution.Descriptor, platform *specs.Platform) (id digest.Digest, err error) {\n\tif _, err := p.config.ImageStore.Get(ctx, target.Digest); err == nil {\n\t\t// If the image already exists locally, no need to pull\n\t\t// anything.\n\t\treturn target.Digest, nil\n\t}\n\n\tvar descriptors []xfer.DownloadDescriptor\n\n\t// Note that the order of this loop is in the direction of bottom-most\n\t// to top-most, so that the downloads slice gets ordered correctly.\n\tfor _, d := range layers {\n\t\tlayerDescriptor := &v2LayerDescriptor{\n\t\t\tdigest:            d.Digest,\n\t\t\trepo:              p.repo,\n\t\t\trepoInfo:          p.repoInfo,\n\t\t\tV2MetadataService: p.V2MetadataService,\n\t\t\tsrc:               d,\n\t\t}\n\n\t\tdescriptors = append(descriptors, layerDescriptor)\n\t}\n\n\tconfigChan := make(chan []byte, 1)\n\tconfigErrChan := make(chan error, 1)\n\tlayerErrChan := make(chan error, 1)\n\tdownloadsDone := make(chan struct{})\n\tvar cancel func()\n\tctx, cancel = context.WithCancel(ctx)\n\tdefer cancel()\n\n\t// Pull the image config\n\tgo func() {\n\t\tconfigJSON, err := p.pullSchema2Config(ctx, target.Digest)\n\t\tif err != nil {\n\t\t\tconfigErrChan <- ImageConfigPullError{Err: err}\n\t\t\tcancel()\n\t\t\treturn\n\t\t}\n\t\tconfigChan <- configJSON\n\t}()\n\n\tvar (\n\t\tconfigJSON       []byte          // raw serialized image config\n\t\tdownloadedRootFS *image.RootFS   // rootFS from registered layers\n\t\tconfigRootFS     *image.RootFS   // rootFS from configuration\n\t\trelease          func()          // release resources from rootFS download\n\t\tconfigPlatform   *specs.Platform // for LCOW when registering downloaded layers\n\t)\n\n\tlayerStoreOS := runtime.GOOS\n\tif platform != nil {\n\t\tlayerStoreOS = platform.OS\n\t}\n\n\t// https://github.com/docker/docker/issues/24766 - Err on the side of caution,\n\t// explicitly blocking images intended for linux from the Windows daemon. On\n\t// Windows, we do this before the attempt to download, effectively serialising\n\t// the download slightly slowing it down. We have to do it this way, as\n\t// chances are the download of layers itself would fail due to file names\n\t// which aren't suitable for NTFS. At some point in the future, if a similar\n\t// check to block Windows images being pulled on Linux is implemented, it\n\t// may be necessary to perform the same type of serialisation.\n\tif runtime.GOOS == \"windows\" {\n\t\tconfigJSON, configRootFS, configPlatform, err = receiveConfig(p.config.ImageStore, configChan, configErrChan)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tif configRootFS == nil {\n\t\t\treturn \"\", errRootFSInvalid\n\t\t}\n\t\tif err := checkImageCompatibility(configPlatform.OS, configPlatform.OSVersion); err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tif len(descriptors) != len(configRootFS.DiffIDs) {\n\t\t\treturn \"\", errRootFSMismatch\n\t\t}\n\t\tif platform == nil {\n\t\t\t// Early bath if the requested OS doesn't match that of the configuration.\n\t\t\t// This avoids doing the download, only to potentially fail later.\n\t\t\tif !system.IsOSSupported(configPlatform.OS) {\n\t\t\t\treturn \"\", fmt.Errorf(\"cannot download image with operating system %q when requesting %q\", configPlatform.OS, layerStoreOS)\n\t\t\t}\n\t\t\tlayerStoreOS = configPlatform.OS\n\t\t}\n\n\t\t// Populate diff ids in descriptors to avoid downloading foreign layers\n\t\t// which have been side loaded\n\t\tfor i := range descriptors {\n\t\t\tdescriptors[i].(*v2LayerDescriptor).diffID = configRootFS.DiffIDs[i]\n\t\t}\n\t}\n\n\tif p.config.DownloadManager != nil {\n\t\tgo func() {\n\t\t\tvar (\n\t\t\t\terr    error\n\t\t\t\trootFS image.RootFS\n\t\t\t)\n\t\t\tdownloadRootFS := *image.NewRootFS()\n\t\t\trootFS, release, err = p.config.DownloadManager.Download(ctx, downloadRootFS, layerStoreOS, descriptors, p.config.ProgressOutput)\n\t\t\tif err != nil {\n\t\t\t\t// Intentionally do not cancel the config download here\n\t\t\t\t// as the error from config download (if there is one)\n\t\t\t\t// is more interesting than the layer download error\n\t\t\t\tlayerErrChan <- err\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tdownloadedRootFS = &rootFS\n\t\t\tclose(downloadsDone)\n\t\t}()\n\t} else {\n\t\t// We have nothing to download\n\t\tclose(downloadsDone)\n\t}\n\n\tif configJSON == nil {\n\t\tconfigJSON, configRootFS, _, err = receiveConfig(p.config.ImageStore, configChan, configErrChan)\n\t\tif err == nil && configRootFS == nil {\n\t\t\terr = errRootFSInvalid\n\t\t}\n\t\tif err != nil {\n\t\t\tcancel()\n\t\t\tselect {\n\t\t\tcase <-downloadsDone:\n\t\t\tcase <-layerErrChan:\n\t\t\t}\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\tselect {\n\tcase <-downloadsDone:\n\tcase err = <-layerErrChan:\n\t\treturn \"\", err\n\t}\n\n\tif release != nil {\n\t\tdefer release()\n\t}\n\n\tif downloadedRootFS != nil {\n\t\t// The DiffIDs returned in rootFS MUST match those in the config.\n\t\t// Otherwise the image config could be referencing layers that aren't\n\t\t// included in the manifest.\n\t\tif len(downloadedRootFS.DiffIDs) != len(configRootFS.DiffIDs) {\n\t\t\treturn \"\", errRootFSMismatch\n\t\t}\n\n\t\tfor i := range downloadedRootFS.DiffIDs {\n\t\t\tif downloadedRootFS.DiffIDs[i] != configRootFS.DiffIDs[i] {\n\t\t\t\treturn \"\", errRootFSMismatch\n\t\t\t}\n\t\t}\n\t}\n\n\timageID, err := p.config.ImageStore.Put(ctx, configJSON)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn imageID, nil\n}\n\nfunc (p *v2Puller) pullSchema2(ctx context.Context, ref reference.Named, mfst *schema2.DeserializedManifest, platform *specs.Platform) (id digest.Digest, manifestDigest digest.Digest, err error) {\n\tmanifestDigest, err = schema2ManifestDigest(ref, mfst)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tid, err = p.pullSchema2Layers(ctx, mfst.Target(), mfst.Layers, platform)\n\treturn id, manifestDigest, err\n}\n\nfunc (p *v2Puller) pullOCI(ctx context.Context, ref reference.Named, mfst *ocischema.DeserializedManifest, platform *specs.Platform) (id digest.Digest, manifestDigest digest.Digest, err error) {\n\tmanifestDigest, err = schema2ManifestDigest(ref, mfst)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tid, err = p.pullSchema2Layers(ctx, mfst.Target(), mfst.Layers, platform)\n\treturn id, manifestDigest, err\n}\n\nfunc receiveConfig(s ImageConfigStore, configChan <-chan []byte, errChan <-chan error) ([]byte, *image.RootFS, *specs.Platform, error) {\n\tselect {\n\tcase configJSON := <-configChan:\n\t\trootfs, err := s.RootFSFromConfig(configJSON)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tplatform, err := s.PlatformFromConfig(configJSON)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\treturn configJSON, rootfs, platform, nil\n\tcase err := <-errChan:\n\t\treturn nil, nil, nil, err\n\t\t// Don't need a case for ctx.Done in the select because cancellation\n\t\t// will trigger an error in p.pullSchema2ImageConfig.\n\t}\n}\n\n// pullManifestList handles \"manifest lists\" which point to various\n// platform-specific manifests.\nfunc (p *v2Puller) pullManifestList(ctx context.Context, ref reference.Named, mfstList *manifestlist.DeserializedManifestList, pp *specs.Platform) (id digest.Digest, manifestListDigest digest.Digest, err error) {\n\tmanifestListDigest, err = schema2ManifestDigest(ref, mfstList)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tvar platform specs.Platform\n\tif pp != nil {\n\t\tplatform = *pp\n\t}\n\tlogrus.Debugf(\"%s resolved to a manifestList object with %d entries; looking for a %s/%s match\", ref, len(mfstList.Manifests), platforms.Format(platform), runtime.GOARCH)\n\n\tmanifestMatches := filterManifests(mfstList.Manifests, platform)\n\n\tif len(manifestMatches) == 0 {\n\t\terrMsg := fmt.Sprintf(\"no matching manifest for %s in the manifest list entries\", formatPlatform(platform))\n\t\tlogrus.Debugf(errMsg)\n\t\treturn \"\", \"\", errors.New(errMsg)\n\t}\n\n\tif len(manifestMatches) > 1 {\n\t\tlogrus.Debugf(\"found multiple matches in manifest list, choosing best match %s\", manifestMatches[0].Digest.String())\n\t}\n\tmatch := manifestMatches[0]\n\n\tif err := checkImageCompatibility(match.Platform.OS, match.Platform.OSVersion); err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tdesc := specs.Descriptor{\n\t\tDigest:    match.Digest,\n\t\tSize:      match.Size,\n\t\tMediaType: match.MediaType,\n\t}\n\tmanifest, err := p.manifestStore.Get(ctx, desc)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tmanifestRef, err := reference.WithDigest(reference.TrimNamed(ref), match.Digest)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tswitch v := manifest.(type) {\n\tcase *schema1.SignedManifest:\n\t\tmsg := fmt.Sprintf(\"[DEPRECATION NOTICE] v2 schema1 manifests in manifest lists are not supported and will break in a future release. Suggest author of %s to upgrade to v2 schema2. More information at https://docs.docker.com/registry/spec/deprecated-schema-v1/\", ref)\n\t\tlogrus.Warn(msg)\n\t\tprogress.Message(p.config.ProgressOutput, \"\", msg)\n\n\t\tplatform := toOCIPlatform(manifestMatches[0].Platform)\n\t\tid, _, err = p.pullSchema1(ctx, manifestRef, v, &platform)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\tcase *schema2.DeserializedManifest:\n\t\tplatform := toOCIPlatform(manifestMatches[0].Platform)\n\t\tid, _, err = p.pullSchema2(ctx, manifestRef, v, &platform)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\tcase *ocischema.DeserializedManifest:\n\t\tplatform := toOCIPlatform(manifestMatches[0].Platform)\n\t\tid, _, err = p.pullOCI(ctx, manifestRef, v, &platform)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\tdefault:\n\t\treturn \"\", \"\", errors.New(\"unsupported manifest format\")\n\t}\n\n\treturn id, manifestListDigest, err\n}\n\nfunc (p *v2Puller) pullSchema2Config(ctx context.Context, dgst digest.Digest) (configJSON []byte, err error) {\n\tblobs := p.repo.Blobs(ctx)\n\tconfigJSON, err = blobs.Get(ctx, dgst)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Verify image config digest\n\tverifier := dgst.Verifier()\n\tif _, err := verifier.Write(configJSON); err != nil {\n\t\treturn nil, err\n\t}\n\tif !verifier.Verified() {\n\t\terr := fmt.Errorf(\"image config verification failed for digest %s\", dgst)\n\t\tlogrus.Error(err)\n\t\treturn nil, err\n\t}\n\n\treturn configJSON, nil\n}\n\n// schema2ManifestDigest computes the manifest digest, and, if pulling by\n// digest, ensures that it matches the requested digest.\nfunc schema2ManifestDigest(ref reference.Named, mfst distribution.Manifest) (digest.Digest, error) {\n\t_, canonical, err := mfst.Payload()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// If pull by digest, then verify the manifest digest.\n\tif digested, isDigested := ref.(reference.Canonical); isDigested {\n\t\tverifier := digested.Digest().Verifier()\n\t\tif _, err := verifier.Write(canonical); err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tif !verifier.Verified() {\n\t\t\terr := fmt.Errorf(\"manifest verification failed for digest %s\", digested.Digest())\n\t\t\tlogrus.Error(err)\n\t\t\treturn \"\", err\n\t\t}\n\t\treturn digested.Digest(), nil\n\t}\n\n\treturn digest.FromBytes(canonical), nil\n}\n\n// allowV1Fallback checks if the error is a possible reason to fallback to v1\n// (even if confirmedV2 has been set already), and if so, wraps the error in\n// a fallbackError with confirmedV2 set to false. Otherwise, it returns the\n// error unmodified.\nfunc allowV1Fallback(err error) error {\n\tswitch v := err.(type) {\n\tcase errcode.Errors:\n\t\tif len(v) != 0 {\n\t\t\tif v0, ok := v[0].(errcode.Error); ok && shouldV2Fallback(v0) {\n\t\t\t\treturn fallbackError{\n\t\t\t\t\terr:         err,\n\t\t\t\t\tconfirmedV2: false,\n\t\t\t\t\ttransportOK: true,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\tcase errcode.Error:\n\t\tif shouldV2Fallback(v) {\n\t\t\treturn fallbackError{\n\t\t\t\terr:         err,\n\t\t\t\tconfirmedV2: false,\n\t\t\t\ttransportOK: true,\n\t\t\t}\n\t\t}\n\tcase *url.Error:\n\t\tif v.Err == auth.ErrNoBasicAuthCredentials {\n\t\t\treturn fallbackError{err: err, confirmedV2: false}\n\t\t}\n\t}\n\n\treturn err\n}\n\nfunc verifySchema1Manifest(signedManifest *schema1.SignedManifest, ref reference.Reference) (m *schema1.Manifest, err error) {\n\t// If pull by digest, then verify the manifest digest. NOTE: It is\n\t// important to do this first, before any other content validation. If the\n\t// digest cannot be verified, don't even bother with those other things.\n\tif digested, isCanonical := ref.(reference.Canonical); isCanonical {\n\t\tverifier := digested.Digest().Verifier()\n\t\tif _, err := verifier.Write(signedManifest.Canonical); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif !verifier.Verified() {\n\t\t\terr := fmt.Errorf(\"image verification failed for digest %s\", digested.Digest())\n\t\t\tlogrus.Error(err)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tm = &signedManifest.Manifest\n\n\tif m.SchemaVersion != 1 {\n\t\treturn nil, fmt.Errorf(\"unsupported schema version %d for %q\", m.SchemaVersion, reference.FamiliarString(ref))\n\t}\n\tif len(m.FSLayers) != len(m.History) {\n\t\treturn nil, fmt.Errorf(\"length of history not equal to number of layers for %q\", reference.FamiliarString(ref))\n\t}\n\tif len(m.FSLayers) == 0 {\n\t\treturn nil, fmt.Errorf(\"no FSLayers in manifest for %q\", reference.FamiliarString(ref))\n\t}\n\treturn m, nil\n}\n\n// fixManifestLayers removes repeated layers from the manifest and checks the\n// correctness of the parent chain.\nfunc fixManifestLayers(m *schema1.Manifest) error {\n\timgs := make([]*image.V1Image, len(m.FSLayers))\n\tfor i := range m.FSLayers {\n\t\timg := &image.V1Image{}\n\n\t\tif err := json.Unmarshal([]byte(m.History[i].V1Compatibility), img); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\timgs[i] = img\n\t\tif err := v1.ValidateID(img.ID); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif imgs[len(imgs)-1].Parent != \"\" && runtime.GOOS != \"windows\" {\n\t\t// Windows base layer can point to a base layer parent that is not in manifest.\n\t\treturn errors.New(\"invalid parent ID in the base layer of the image\")\n\t}\n\n\t// check general duplicates to error instead of a deadlock\n\tidmap := make(map[string]struct{})\n\n\tvar lastID string\n\tfor _, img := range imgs {\n\t\t// skip IDs that appear after each other, we handle those later\n\t\tif _, exists := idmap[img.ID]; img.ID != lastID && exists {\n\t\t\treturn fmt.Errorf(\"ID %+v appears multiple times in manifest\", img.ID)\n\t\t}\n\t\tlastID = img.ID\n\t\tidmap[lastID] = struct{}{}\n\t}\n\n\t// backwards loop so that we keep the remaining indexes after removing items\n\tfor i := len(imgs) - 2; i >= 0; i-- {\n\t\tif imgs[i].ID == imgs[i+1].ID { // repeated ID. remove and continue\n\t\t\tm.FSLayers = append(m.FSLayers[:i], m.FSLayers[i+1:]...)\n\t\t\tm.History = append(m.History[:i], m.History[i+1:]...)\n\t\t} else if imgs[i].Parent != imgs[i+1].ID {\n\t\t\treturn fmt.Errorf(\"invalid parent ID. Expected %v, got %v\", imgs[i+1].ID, imgs[i].Parent)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc createDownloadFile() (*os.File, error) {\n\treturn ioutil.TempFile(\"\", \"GetImageBlob\")\n}\n\nfunc toOCIPlatform(p manifestlist.PlatformSpec) specs.Platform {\n\treturn specs.Platform{\n\t\tOS:           p.OS,\n\t\tArchitecture: p.Architecture,\n\t\tVariant:      p.Variant,\n\t\tOSFeatures:   p.OSFeatures,\n\t\tOSVersion:    p.OSVersion,\n\t}\n}\n"], "fixing_code": ["package containerimage\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"path\"\n\t\"runtime\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/content\"\n\tcontainerderrors \"github.com/containerd/containerd/errdefs\"\n\t\"github.com/containerd/containerd/images\"\n\t\"github.com/containerd/containerd/platforms\"\n\tctdreference \"github.com/containerd/containerd/reference\"\n\t\"github.com/containerd/containerd/remotes\"\n\t\"github.com/containerd/containerd/remotes/docker\"\n\t\"github.com/containerd/containerd/remotes/docker/schema1\"\n\tdistreference \"github.com/docker/distribution/reference\"\n\t\"github.com/docker/docker/distribution\"\n\t\"github.com/docker/docker/distribution/metadata\"\n\t\"github.com/docker/docker/distribution/xfer\"\n\t\"github.com/docker/docker/image\"\n\t\"github.com/docker/docker/layer\"\n\tpkgprogress \"github.com/docker/docker/pkg/progress\"\n\t\"github.com/docker/docker/reference\"\n\t\"github.com/moby/buildkit/cache\"\n\t\"github.com/moby/buildkit/client/llb\"\n\t\"github.com/moby/buildkit/session\"\n\t\"github.com/moby/buildkit/solver\"\n\t\"github.com/moby/buildkit/source\"\n\t\"github.com/moby/buildkit/util/flightcontrol\"\n\t\"github.com/moby/buildkit/util/imageutil\"\n\t\"github.com/moby/buildkit/util/progress\"\n\t\"github.com/moby/buildkit/util/resolver\"\n\tdigest \"github.com/opencontainers/go-digest\"\n\t\"github.com/opencontainers/image-spec/identity\"\n\tocispec \"github.com/opencontainers/image-spec/specs-go/v1\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/time/rate\"\n)\n\n// SourceOpt is options for creating the image source\ntype SourceOpt struct {\n\tContentStore    content.Store\n\tCacheAccessor   cache.Accessor\n\tReferenceStore  reference.Store\n\tDownloadManager distribution.RootFSDownloadManager\n\tMetadataStore   metadata.V2MetadataService\n\tImageStore      image.Store\n\tRegistryHosts   docker.RegistryHosts\n\tLayerStore      layer.Store\n}\n\n// Source is the source implementation for accessing container images\ntype Source struct {\n\tSourceOpt\n\tg flightcontrol.Group\n}\n\n// NewSource creates a new image source\nfunc NewSource(opt SourceOpt) (*Source, error) {\n\treturn &Source{SourceOpt: opt}, nil\n}\n\n// ID returns image scheme identifier\nfunc (is *Source) ID() string {\n\treturn source.DockerImageScheme\n}\n\nfunc (is *Source) resolveLocal(refStr string) (*image.Image, error) {\n\tref, err := distreference.ParseNormalizedNamed(refStr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdgst, err := is.ReferenceStore.Get(ref)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\timg, err := is.ImageStore.Get(image.ID(dgst))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn img, nil\n}\n\nfunc (is *Source) resolveRemote(ctx context.Context, ref string, platform *ocispec.Platform, sm *session.Manager, g session.Group) (digest.Digest, []byte, error) {\n\ttype t struct {\n\t\tdgst digest.Digest\n\t\tdt   []byte\n\t}\n\tp := platforms.DefaultSpec()\n\tif platform != nil {\n\t\tp = *platform\n\t}\n\t// key is used to synchronize resolutions that can happen in parallel when doing multi-stage.\n\tkey := \"getconfig::\" + ref + \"::\" + platforms.Format(p)\n\tres, err := is.g.Do(ctx, key, func(ctx context.Context) (interface{}, error) {\n\t\tres := resolver.DefaultPool.GetResolver(is.RegistryHosts, ref, \"pull\", sm, g)\n\t\tdgst, dt, err := imageutil.Config(ctx, ref, res, is.ContentStore, nil, platform)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn &t{dgst: dgst, dt: dt}, nil\n\t})\n\tvar typed *t\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\ttyped = res.(*t)\n\treturn typed.dgst, typed.dt, nil\n}\n\n// ResolveImageConfig returns image config for an image\nfunc (is *Source) ResolveImageConfig(ctx context.Context, ref string, opt llb.ResolveImageConfigOpt, sm *session.Manager, g session.Group) (digest.Digest, []byte, error) {\n\tresolveMode, err := source.ParseImageResolveMode(opt.ResolveMode)\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\tswitch resolveMode {\n\tcase source.ResolveModeForcePull:\n\t\tdgst, dt, err := is.resolveRemote(ctx, ref, opt.Platform, sm, g)\n\t\t// TODO: pull should fallback to local in case of failure to allow offline behavior\n\t\t// the fallback doesn't work currently\n\t\treturn dgst, dt, err\n\t\t/*\n\t\t\tif err == nil {\n\t\t\t\treturn dgst, dt, err\n\t\t\t}\n\t\t\t// fallback to local\n\t\t\tdt, err = is.resolveLocal(ref)\n\t\t\treturn \"\", dt, err\n\t\t*/\n\n\tcase source.ResolveModeDefault:\n\t\t// default == prefer local, but in the future could be smarter\n\t\tfallthrough\n\tcase source.ResolveModePreferLocal:\n\t\timg, err := is.resolveLocal(ref)\n\t\tif err == nil {\n\t\t\tif opt.Platform != nil && !platformMatches(img, opt.Platform) {\n\t\t\t\tlogrus.WithField(\"ref\", ref).Debugf(\"Requested build platform %s does not match local image platform %s, checking remote\",\n\t\t\t\t\tpath.Join(opt.Platform.OS, opt.Platform.Architecture, opt.Platform.Variant),\n\t\t\t\t\tpath.Join(img.OS, img.Architecture, img.Variant),\n\t\t\t\t)\n\t\t\t} else {\n\t\t\t\treturn \"\", img.RawJSON(), err\n\t\t\t}\n\t\t}\n\t\t// fallback to remote\n\t\treturn is.resolveRemote(ctx, ref, opt.Platform, sm, g)\n\t}\n\t// should never happen\n\treturn \"\", nil, fmt.Errorf(\"builder cannot resolve image %s: invalid mode %q\", ref, opt.ResolveMode)\n}\n\n// Resolve returns access to pulling for an identifier\nfunc (is *Source) Resolve(ctx context.Context, id source.Identifier, sm *session.Manager, vtx solver.Vertex) (source.SourceInstance, error) {\n\timageIdentifier, ok := id.(*source.ImageIdentifier)\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"invalid image identifier %v\", id)\n\t}\n\n\tplatform := platforms.DefaultSpec()\n\tif imageIdentifier.Platform != nil {\n\t\tplatform = *imageIdentifier.Platform\n\t}\n\n\tp := &puller{\n\t\tsrc: imageIdentifier,\n\t\tis:  is,\n\t\t//resolver: is.getResolver(is.RegistryHosts, imageIdentifier.Reference.String(), sm, g),\n\t\tplatform: platform,\n\t\tsm:       sm,\n\t}\n\treturn p, nil\n}\n\ntype puller struct {\n\tis               *Source\n\tresolveLocalOnce sync.Once\n\tsrc              *source.ImageIdentifier\n\tdesc             ocispec.Descriptor\n\tref              string\n\tconfig           []byte\n\tplatform         ocispec.Platform\n\tsm               *session.Manager\n}\n\nfunc (p *puller) resolver(g session.Group) remotes.Resolver {\n\treturn resolver.DefaultPool.GetResolver(p.is.RegistryHosts, p.src.Reference.String(), \"pull\", p.sm, g)\n}\n\nfunc (p *puller) mainManifestKey(platform ocispec.Platform) (digest.Digest, error) {\n\tdt, err := json.Marshal(struct {\n\t\tDigest  digest.Digest\n\t\tOS      string\n\t\tArch    string\n\t\tVariant string `json:\",omitempty\"`\n\t}{\n\t\tDigest:  p.desc.Digest,\n\t\tOS:      platform.OS,\n\t\tArch:    platform.Architecture,\n\t\tVariant: platform.Variant,\n\t})\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn digest.FromBytes(dt), nil\n}\n\nfunc (p *puller) resolveLocal() {\n\tp.resolveLocalOnce.Do(func() {\n\t\tdgst := p.src.Reference.Digest()\n\t\tif dgst != \"\" {\n\t\t\tinfo, err := p.is.ContentStore.Info(context.TODO(), dgst)\n\t\t\tif err == nil {\n\t\t\t\tp.ref = p.src.Reference.String()\n\t\t\t\tdesc := ocispec.Descriptor{\n\t\t\t\t\tSize:   info.Size,\n\t\t\t\t\tDigest: dgst,\n\t\t\t\t}\n\t\t\t\tra, err := p.is.ContentStore.ReaderAt(context.TODO(), desc)\n\t\t\t\tif err == nil {\n\t\t\t\t\tmt, err := imageutil.DetectManifestMediaType(ra)\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\tdesc.MediaType = mt\n\t\t\t\t\t\tp.desc = desc\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif p.src.ResolveMode == source.ResolveModeDefault || p.src.ResolveMode == source.ResolveModePreferLocal {\n\t\t\tref := p.src.Reference.String()\n\t\t\timg, err := p.is.resolveLocal(ref)\n\t\t\tif err == nil {\n\t\t\t\tif !platformMatches(img, &p.platform) {\n\t\t\t\t\tlogrus.WithField(\"ref\", ref).Debugf(\"Requested build platform %s does not match local image platform %s, not resolving\",\n\t\t\t\t\t\tpath.Join(p.platform.OS, p.platform.Architecture, p.platform.Variant),\n\t\t\t\t\t\tpath.Join(img.OS, img.Architecture, img.Variant),\n\t\t\t\t\t)\n\t\t\t\t} else {\n\t\t\t\t\tp.config = img.RawJSON()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc (p *puller) resolve(ctx context.Context, g session.Group) error {\n\t// key is used to synchronize resolutions that can happen in parallel when doing multi-stage.\n\tkey := \"resolve::\" + p.ref + \"::\" + platforms.Format(p.platform)\n\t_, err := p.is.g.Do(ctx, key, func(ctx context.Context) (_ interface{}, err error) {\n\t\tresolveProgressDone := oneOffProgress(ctx, \"resolve \"+p.src.Reference.String())\n\t\tdefer func() {\n\t\t\tresolveProgressDone(err)\n\t\t}()\n\n\t\tref, err := distreference.ParseNormalizedNamed(p.src.Reference.String())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif p.desc.Digest == \"\" && p.config == nil {\n\t\t\torigRef, desc, err := p.resolver(g).Resolve(ctx, ref.String())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tp.desc = desc\n\t\t\tp.ref = origRef\n\t\t}\n\n\t\t// Schema 1 manifests cannot be resolved to an image config\n\t\t// since the conversion must take place after all the content\n\t\t// has been read.\n\t\t// It may be possible to have a mapping between schema 1 manifests\n\t\t// and the schema 2 manifests they are converted to.\n\t\tif p.config == nil && p.desc.MediaType != images.MediaTypeDockerSchema1Manifest {\n\t\t\tref, err := distreference.WithDigest(ref, p.desc.Digest)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\t_, dt, err := p.is.ResolveImageConfig(ctx, ref.String(), llb.ResolveImageConfigOpt{Platform: &p.platform, ResolveMode: resolveModeToString(p.src.ResolveMode)}, p.sm, g)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tp.config = dt\n\t\t}\n\t\treturn nil, nil\n\t})\n\treturn err\n}\n\nfunc (p *puller) CacheKey(ctx context.Context, g session.Group, index int) (string, solver.CacheOpts, bool, error) {\n\tp.resolveLocal()\n\n\tif p.desc.Digest != \"\" && index == 0 {\n\t\tdgst, err := p.mainManifestKey(p.platform)\n\t\tif err != nil {\n\t\t\treturn \"\", nil, false, err\n\t\t}\n\t\treturn dgst.String(), nil, false, nil\n\t}\n\n\tif p.config != nil {\n\t\tk := cacheKeyFromConfig(p.config).String()\n\t\tif k == \"\" {\n\t\t\treturn digest.FromBytes(p.config).String(), nil, true, nil\n\t\t}\n\t\treturn k, nil, true, nil\n\t}\n\n\tif err := p.resolve(ctx, g); err != nil {\n\t\treturn \"\", nil, false, err\n\t}\n\n\tif p.desc.Digest != \"\" && index == 0 {\n\t\tdgst, err := p.mainManifestKey(p.platform)\n\t\tif err != nil {\n\t\t\treturn \"\", nil, false, err\n\t\t}\n\t\treturn dgst.String(), nil, false, nil\n\t}\n\n\tk := cacheKeyFromConfig(p.config).String()\n\tif k == \"\" {\n\t\tdgst, err := p.mainManifestKey(p.platform)\n\t\tif err != nil {\n\t\t\treturn \"\", nil, false, err\n\t\t}\n\t\treturn dgst.String(), nil, true, nil\n\t}\n\n\treturn k, nil, true, nil\n}\n\nfunc (p *puller) getRef(ctx context.Context, diffIDs []layer.DiffID, opts ...cache.RefOption) (cache.ImmutableRef, error) {\n\tvar parent cache.ImmutableRef\n\tif len(diffIDs) > 1 {\n\t\tvar err error\n\t\tparent, err = p.getRef(ctx, diffIDs[:len(diffIDs)-1], opts...)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer parent.Release(context.TODO())\n\t}\n\treturn p.is.CacheAccessor.GetByBlob(ctx, ocispec.Descriptor{\n\t\tAnnotations: map[string]string{\n\t\t\t\"containerd.io/uncompressed\": diffIDs[len(diffIDs)-1].String(),\n\t\t},\n\t}, parent, opts...)\n}\n\nfunc (p *puller) Snapshot(ctx context.Context, g session.Group) (cache.ImmutableRef, error) {\n\tp.resolveLocal()\n\tif err := p.resolve(ctx, g); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif p.config != nil {\n\t\timg, err := p.is.ImageStore.Get(image.ID(digest.FromBytes(p.config)))\n\t\tif err == nil {\n\t\t\tif len(img.RootFS.DiffIDs) == 0 {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tl, err := p.is.LayerStore.Get(img.RootFS.ChainID())\n\t\t\tif err == nil {\n\t\t\t\tlayer.ReleaseAndLog(p.is.LayerStore, l)\n\t\t\t\tref, err := p.getRef(ctx, img.RootFS.DiffIDs, cache.WithDescription(fmt.Sprintf(\"from local %s\", p.ref)))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\treturn ref, nil\n\t\t\t}\n\t\t}\n\t}\n\n\tongoing := newJobs(p.ref)\n\n\tpctx, stopProgress := context.WithCancel(ctx)\n\n\tpw, _, ctx := progress.FromContext(ctx)\n\tdefer pw.Close()\n\n\tprogressDone := make(chan struct{})\n\tgo func() {\n\t\tshowProgress(pctx, ongoing, p.is.ContentStore, pw)\n\t\tclose(progressDone)\n\t}()\n\tdefer func() {\n\t\t<-progressDone\n\t}()\n\n\tfetcher, err := p.resolver(g).Fetcher(ctx, p.ref)\n\tif err != nil {\n\t\tstopProgress()\n\t\treturn nil, err\n\t}\n\n\tplatform := platforms.Only(p.platform)\n\n\tvar (\n\t\tschema1Converter *schema1.Converter\n\t\thandlers         []images.Handler\n\t)\n\tif p.desc.MediaType == images.MediaTypeDockerSchema1Manifest {\n\t\tschema1Converter = schema1.NewConverter(p.is.ContentStore, fetcher)\n\t\thandlers = append(handlers, schema1Converter)\n\n\t\t// TODO: Optimize to do dispatch and integrate pulling with download manager,\n\t\t// leverage existing blob mapping and layer storage\n\t} else {\n\n\t\t// TODO: need a wrapper snapshot interface that combines content\n\t\t// and snapshots as 1) buildkit shouldn't have a dependency on contentstore\n\t\t// or 2) cachemanager should manage the contentstore\n\t\thandlers = append(handlers, images.HandlerFunc(func(ctx context.Context, desc ocispec.Descriptor) ([]ocispec.Descriptor, error) {\n\t\t\tswitch desc.MediaType {\n\t\t\tcase images.MediaTypeDockerSchema2Manifest, ocispec.MediaTypeImageManifest,\n\t\t\t\timages.MediaTypeDockerSchema2ManifestList, ocispec.MediaTypeImageIndex,\n\t\t\t\timages.MediaTypeDockerSchema2Config, ocispec.MediaTypeImageConfig:\n\t\t\tdefault:\n\t\t\t\treturn nil, images.ErrSkipDesc\n\t\t\t}\n\t\t\tongoing.add(desc)\n\t\t\treturn nil, nil\n\t\t}))\n\n\t\t// Get all the children for a descriptor\n\t\tchildrenHandler := images.ChildrenHandler(p.is.ContentStore)\n\t\t// Set any children labels for that content\n\t\tchildrenHandler = images.SetChildrenLabels(p.is.ContentStore, childrenHandler)\n\t\t// Filter the children by the platform\n\t\tchildrenHandler = images.FilterPlatforms(childrenHandler, platform)\n\t\t// Limit manifests pulled to the best match in an index\n\t\tchildrenHandler = images.LimitManifests(childrenHandler, platform, 1)\n\n\t\thandlers = append(handlers,\n\t\t\tremotes.FetchHandler(p.is.ContentStore, fetcher),\n\t\t\tchildrenHandler,\n\t\t)\n\t}\n\n\tif err := images.Dispatch(ctx, images.Handlers(handlers...), nil, p.desc); err != nil {\n\t\tstopProgress()\n\t\treturn nil, err\n\t}\n\tdefer stopProgress()\n\n\tif schema1Converter != nil {\n\t\tp.desc, err = schema1Converter.Convert(ctx)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tmfst, err := images.Manifest(ctx, p.is.ContentStore, p.desc, platform)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tconfig, err := images.Config(ctx, p.is.ContentStore, p.desc, platform)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdt, err := content.ReadBlob(ctx, p.is.ContentStore, config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar img ocispec.Image\n\tif err := json.Unmarshal(dt, &img); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif len(mfst.Layers) != len(img.RootFS.DiffIDs) {\n\t\treturn nil, errors.Errorf(\"invalid config for manifest\")\n\t}\n\n\tpchan := make(chan pkgprogress.Progress, 10)\n\tdefer close(pchan)\n\n\tgo func() {\n\t\tm := map[string]struct {\n\t\t\tst      time.Time\n\t\t\tlimiter *rate.Limiter\n\t\t}{}\n\t\tfor p := range pchan {\n\t\t\tif p.Action == \"Extracting\" {\n\t\t\t\tst, ok := m[p.ID]\n\t\t\t\tif !ok {\n\t\t\t\t\tst.st = time.Now()\n\t\t\t\t\tst.limiter = rate.NewLimiter(rate.Every(100*time.Millisecond), 1)\n\t\t\t\t\tm[p.ID] = st\n\t\t\t\t}\n\t\t\t\tvar end *time.Time\n\t\t\t\tif p.LastUpdate || st.limiter.Allow() {\n\t\t\t\t\tif p.LastUpdate {\n\t\t\t\t\t\ttm := time.Now()\n\t\t\t\t\t\tend = &tm\n\t\t\t\t\t}\n\t\t\t\t\t_ = pw.Write(\"extracting \"+p.ID, progress.Status{\n\t\t\t\t\t\tAction:    \"extract\",\n\t\t\t\t\t\tStarted:   &st.st,\n\t\t\t\t\t\tCompleted: end,\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tif len(mfst.Layers) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tlayers := make([]xfer.DownloadDescriptor, 0, len(mfst.Layers))\n\n\tfor i, desc := range mfst.Layers {\n\t\tif err := desc.Digest.Validate(); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"layer digest could not be validated\")\n\t\t}\n\t\tongoing.add(desc)\n\t\tlayers = append(layers, &layerDescriptor{\n\t\t\tdesc:    desc,\n\t\t\tdiffID:  layer.DiffID(img.RootFS.DiffIDs[i]),\n\t\t\tfetcher: fetcher,\n\t\t\tref:     p.src.Reference,\n\t\t\tis:      p.is,\n\t\t})\n\t}\n\n\tdefer func() {\n\t\t<-progressDone\n\t\tfor _, desc := range mfst.Layers {\n\t\t\tp.is.ContentStore.Delete(context.TODO(), desc.Digest)\n\t\t}\n\t}()\n\n\tr := image.NewRootFS()\n\trootFS, release, err := p.is.DownloadManager.Download(ctx, *r, runtime.GOOS, layers, pkgprogress.ChanOutput(pchan))\n\tstopProgress()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tref, err := p.getRef(ctx, rootFS.DiffIDs, cache.WithDescription(fmt.Sprintf(\"pulled from %s\", p.ref)))\n\trelease()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO: handle windows layers for cross platform builds\n\n\tif p.src.RecordType != \"\" && cache.GetRecordType(ref) == \"\" {\n\t\tif err := cache.SetRecordType(ref, p.src.RecordType); err != nil {\n\t\t\tref.Release(context.TODO())\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn ref, nil\n}\n\n// Fetch(ctx context.Context, desc ocispec.Descriptor) (io.ReadCloser, error)\ntype layerDescriptor struct {\n\tis      *Source\n\tfetcher remotes.Fetcher\n\tdesc    ocispec.Descriptor\n\tdiffID  layer.DiffID\n\tref     ctdreference.Spec\n}\n\nfunc (ld *layerDescriptor) Key() string {\n\treturn \"v2:\" + ld.desc.Digest.String()\n}\n\nfunc (ld *layerDescriptor) ID() string {\n\treturn ld.desc.Digest.String()\n}\n\nfunc (ld *layerDescriptor) DiffID() (layer.DiffID, error) {\n\treturn ld.diffID, nil\n}\n\nfunc (ld *layerDescriptor) Download(ctx context.Context, progressOutput pkgprogress.Output) (io.ReadCloser, int64, error) {\n\trc, err := ld.fetcher.Fetch(ctx, ld.desc)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\tdefer rc.Close()\n\n\trefKey := remotes.MakeRefKey(ctx, ld.desc)\n\n\tld.is.ContentStore.Abort(ctx, refKey)\n\n\tif err := content.WriteBlob(ctx, ld.is.ContentStore, refKey, rc, ld.desc); err != nil {\n\t\tld.is.ContentStore.Abort(ctx, refKey)\n\t\treturn nil, 0, err\n\t}\n\n\tra, err := ld.is.ContentStore.ReaderAt(ctx, ld.desc)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\treturn ioutil.NopCloser(content.NewReader(ra)), ld.desc.Size, nil\n}\n\nfunc (ld *layerDescriptor) Close() {\n\t// ld.is.ContentStore.Delete(context.TODO(), ld.desc.Digest))\n}\n\nfunc (ld *layerDescriptor) Registered(diffID layer.DiffID) {\n\t// Cache mapping from this layer's DiffID to the blobsum\n\tld.is.MetadataStore.Add(diffID, metadata.V2Metadata{Digest: ld.desc.Digest, SourceRepository: ld.ref.Locator})\n}\n\nfunc showProgress(ctx context.Context, ongoing *jobs, cs content.Store, pw progress.Writer) {\n\tvar (\n\t\tticker   = time.NewTicker(100 * time.Millisecond)\n\t\tstatuses = map[string]statusInfo{}\n\t\tdone     bool\n\t)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\tcase <-ctx.Done():\n\t\t\tdone = true\n\t\t}\n\n\t\tresolved := \"resolved\"\n\t\tif !ongoing.isResolved() {\n\t\t\tresolved = \"resolving\"\n\t\t}\n\t\tstatuses[ongoing.name] = statusInfo{\n\t\t\tRef:    ongoing.name,\n\t\t\tStatus: resolved,\n\t\t}\n\n\t\tactives := make(map[string]statusInfo)\n\n\t\tif !done {\n\t\t\tactive, err := cs.ListStatuses(ctx)\n\t\t\tif err != nil {\n\t\t\t\t// log.G(ctx).WithError(err).Error(\"active check failed\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// update status of active entries!\n\t\t\tfor _, active := range active {\n\t\t\t\tactives[active.Ref] = statusInfo{\n\t\t\t\t\tRef:       active.Ref,\n\t\t\t\t\tStatus:    \"downloading\",\n\t\t\t\t\tOffset:    active.Offset,\n\t\t\t\t\tTotal:     active.Total,\n\t\t\t\t\tStartedAt: active.StartedAt,\n\t\t\t\t\tUpdatedAt: active.UpdatedAt,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// now, update the items in jobs that are not in active\n\t\tfor _, j := range ongoing.jobs() {\n\t\t\trefKey := remotes.MakeRefKey(ctx, j.Descriptor)\n\t\t\tif a, ok := actives[refKey]; ok {\n\t\t\t\tstarted := j.started\n\t\t\t\t_ = pw.Write(j.Digest.String(), progress.Status{\n\t\t\t\t\tAction:  a.Status,\n\t\t\t\t\tTotal:   int(a.Total),\n\t\t\t\t\tCurrent: int(a.Offset),\n\t\t\t\t\tStarted: &started,\n\t\t\t\t})\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif !j.done {\n\t\t\t\tinfo, err := cs.Info(context.TODO(), j.Digest)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif containerderrors.IsNotFound(err) {\n\t\t\t\t\t\t// _ = pw.Write(j.Digest.String(), progress.Status{\n\t\t\t\t\t\t// \tAction: \"waiting\",\n\t\t\t\t\t\t// })\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tj.done = true\n\t\t\t\t}\n\n\t\t\t\tif done || j.done {\n\t\t\t\t\tstarted := j.started\n\t\t\t\t\tcreatedAt := info.CreatedAt\n\t\t\t\t\t_ = pw.Write(j.Digest.String(), progress.Status{\n\t\t\t\t\t\tAction:    \"done\",\n\t\t\t\t\t\tCurrent:   int(info.Size),\n\t\t\t\t\t\tTotal:     int(info.Size),\n\t\t\t\t\t\tCompleted: &createdAt,\n\t\t\t\t\t\tStarted:   &started,\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif done {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// jobs provides a way of identifying the download keys for a particular task\n// encountering during the pull walk.\n//\n// This is very minimal and will probably be replaced with something more\n// featured.\ntype jobs struct {\n\tname     string\n\tadded    map[digest.Digest]*job\n\tmu       sync.Mutex\n\tresolved bool\n}\n\ntype job struct {\n\tocispec.Descriptor\n\tdone    bool\n\tstarted time.Time\n}\n\nfunc newJobs(name string) *jobs {\n\treturn &jobs{\n\t\tname:  name,\n\t\tadded: make(map[digest.Digest]*job),\n\t}\n}\n\nfunc (j *jobs) add(desc ocispec.Descriptor) {\n\tj.mu.Lock()\n\tdefer j.mu.Unlock()\n\n\tif _, ok := j.added[desc.Digest]; ok {\n\t\treturn\n\t}\n\tj.added[desc.Digest] = &job{\n\t\tDescriptor: desc,\n\t\tstarted:    time.Now(),\n\t}\n}\n\nfunc (j *jobs) jobs() []*job {\n\tj.mu.Lock()\n\tdefer j.mu.Unlock()\n\n\tdescs := make([]*job, 0, len(j.added))\n\tfor _, j := range j.added {\n\t\tdescs = append(descs, j)\n\t}\n\treturn descs\n}\n\nfunc (j *jobs) isResolved() bool {\n\tj.mu.Lock()\n\tdefer j.mu.Unlock()\n\treturn j.resolved\n}\n\ntype statusInfo struct {\n\tRef       string\n\tStatus    string\n\tOffset    int64\n\tTotal     int64\n\tStartedAt time.Time\n\tUpdatedAt time.Time\n}\n\nfunc oneOffProgress(ctx context.Context, id string) func(err error) error {\n\tpw, _, _ := progress.FromContext(ctx)\n\tnow := time.Now()\n\tst := progress.Status{\n\t\tStarted: &now,\n\t}\n\t_ = pw.Write(id, st)\n\treturn func(err error) error {\n\t\t// TODO: set error on status\n\t\tnow := time.Now()\n\t\tst.Completed = &now\n\t\t_ = pw.Write(id, st)\n\t\t_ = pw.Close()\n\t\treturn err\n\t}\n}\n\n// cacheKeyFromConfig returns a stable digest from image config. If image config\n// is a known oci image we will use chainID of layers.\nfunc cacheKeyFromConfig(dt []byte) digest.Digest {\n\tvar img ocispec.Image\n\terr := json.Unmarshal(dt, &img)\n\tif err != nil {\n\t\treturn digest.FromBytes(dt)\n\t}\n\tif img.RootFS.Type != \"layers\" || len(img.RootFS.DiffIDs) == 0 {\n\t\treturn \"\"\n\t}\n\treturn identity.ChainID(img.RootFS.DiffIDs)\n}\n\n// resolveModeToString is the equivalent of github.com/moby/buildkit/solver/llb.ResolveMode.String()\n// FIXME: add String method on source.ResolveMode\nfunc resolveModeToString(rm source.ResolveMode) string {\n\tswitch rm {\n\tcase source.ResolveModeDefault:\n\t\treturn \"default\"\n\tcase source.ResolveModeForcePull:\n\t\treturn \"pull\"\n\tcase source.ResolveModePreferLocal:\n\t\treturn \"local\"\n\t}\n\treturn \"\"\n}\n\nfunc platformMatches(img *image.Image, p *ocispec.Platform) bool {\n\tif img.Architecture != p.Architecture {\n\t\treturn false\n\t}\n\tif img.Variant != \"\" && img.Variant != p.Variant {\n\t\treturn false\n\t}\n\treturn img.OS == p.OS\n}\n", "package distribution // import \"github.com/docker/docker/distribution\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime\"\n\t\"strings\"\n\n\t\"github.com/containerd/containerd/log\"\n\t\"github.com/containerd/containerd/platforms\"\n\t\"github.com/docker/distribution\"\n\t\"github.com/docker/distribution/manifest/manifestlist\"\n\t\"github.com/docker/distribution/manifest/ocischema\"\n\t\"github.com/docker/distribution/manifest/schema1\"\n\t\"github.com/docker/distribution/manifest/schema2\"\n\t\"github.com/docker/distribution/reference\"\n\t\"github.com/docker/distribution/registry/api/errcode\"\n\t\"github.com/docker/distribution/registry/client/auth\"\n\t\"github.com/docker/distribution/registry/client/transport\"\n\t\"github.com/docker/docker/distribution/metadata\"\n\t\"github.com/docker/docker/distribution/xfer\"\n\t\"github.com/docker/docker/image\"\n\tv1 \"github.com/docker/docker/image/v1\"\n\t\"github.com/docker/docker/layer\"\n\t\"github.com/docker/docker/pkg/ioutils\"\n\t\"github.com/docker/docker/pkg/progress\"\n\t\"github.com/docker/docker/pkg/stringid\"\n\t\"github.com/docker/docker/pkg/system\"\n\trefstore \"github.com/docker/docker/reference\"\n\t\"github.com/docker/docker/registry\"\n\tdigest \"github.com/opencontainers/go-digest\"\n\tspecs \"github.com/opencontainers/image-spec/specs-go/v1\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n)\n\nvar (\n\terrRootFSMismatch = errors.New(\"layers from manifest don't match image configuration\")\n\terrRootFSInvalid  = errors.New(\"invalid rootfs in image configuration\")\n)\n\n// ImageConfigPullError is an error pulling the image config blob\n// (only applies to schema2).\ntype ImageConfigPullError struct {\n\tErr error\n}\n\n// Error returns the error string for ImageConfigPullError.\nfunc (e ImageConfigPullError) Error() string {\n\treturn \"error pulling image configuration: \" + e.Err.Error()\n}\n\ntype v2Puller struct {\n\tV2MetadataService metadata.V2MetadataService\n\tendpoint          registry.APIEndpoint\n\tconfig            *ImagePullConfig\n\trepoInfo          *registry.RepositoryInfo\n\trepo              distribution.Repository\n\t// confirmedV2 is set to true if we confirm we're talking to a v2\n\t// registry. This is used to limit fallbacks to the v1 protocol.\n\tconfirmedV2   bool\n\tmanifestStore *manifestStore\n}\n\nfunc (p *v2Puller) Pull(ctx context.Context, ref reference.Named, platform *specs.Platform) (err error) {\n\t// TODO(tiborvass): was ReceiveTimeout\n\tp.repo, p.confirmedV2, err = NewV2Repository(ctx, p.repoInfo, p.endpoint, p.config.MetaHeaders, p.config.AuthConfig, \"pull\")\n\tif err != nil {\n\t\tlogrus.Warnf(\"Error getting v2 registry: %v\", err)\n\t\treturn err\n\t}\n\n\tp.manifestStore.remote, err = p.repo.Manifests(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err = p.pullV2Repository(ctx, ref, platform); err != nil {\n\t\tif _, ok := err.(fallbackError); ok {\n\t\t\treturn err\n\t\t}\n\t\tif continueOnError(err, p.endpoint.Mirror) {\n\t\t\treturn fallbackError{\n\t\t\t\terr:         err,\n\t\t\t\tconfirmedV2: p.confirmedV2,\n\t\t\t\ttransportOK: true,\n\t\t\t}\n\t\t}\n\t}\n\treturn err\n}\n\nfunc (p *v2Puller) pullV2Repository(ctx context.Context, ref reference.Named, platform *specs.Platform) (err error) {\n\tvar layersDownloaded bool\n\tif !reference.IsNameOnly(ref) {\n\t\tlayersDownloaded, err = p.pullV2Tag(ctx, ref, platform)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\ttags, err := p.repo.Tags(ctx).All(ctx)\n\t\tif err != nil {\n\t\t\t// If this repository doesn't exist on V2, we should\n\t\t\t// permit a fallback to V1.\n\t\t\treturn allowV1Fallback(err)\n\t\t}\n\n\t\t// The v2 registry knows about this repository, so we will not\n\t\t// allow fallback to the v1 protocol even if we encounter an\n\t\t// error later on.\n\t\tp.confirmedV2 = true\n\n\t\tfor _, tag := range tags {\n\t\t\ttagRef, err := reference.WithTag(ref, tag)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tpulledNew, err := p.pullV2Tag(ctx, tagRef, platform)\n\t\t\tif err != nil {\n\t\t\t\t// Since this is the pull-all-tags case, don't\n\t\t\t\t// allow an error pulling a particular tag to\n\t\t\t\t// make the whole pull fall back to v1.\n\t\t\t\tif fallbackErr, ok := err.(fallbackError); ok {\n\t\t\t\t\treturn fallbackErr.err\n\t\t\t\t}\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t// pulledNew is true if either new layers were downloaded OR if existing images were newly tagged\n\t\t\t// TODO(tiborvass): should we change the name of `layersDownload`? What about message in WriteStatus?\n\t\t\tlayersDownloaded = layersDownloaded || pulledNew\n\t\t}\n\t}\n\n\twriteStatus(reference.FamiliarString(ref), p.config.ProgressOutput, layersDownloaded)\n\n\treturn nil\n}\n\ntype v2LayerDescriptor struct {\n\tdigest            digest.Digest\n\tdiffID            layer.DiffID\n\trepoInfo          *registry.RepositoryInfo\n\trepo              distribution.Repository\n\tV2MetadataService metadata.V2MetadataService\n\ttmpFile           *os.File\n\tverifier          digest.Verifier\n\tsrc               distribution.Descriptor\n}\n\nfunc (ld *v2LayerDescriptor) Key() string {\n\treturn \"v2:\" + ld.digest.String()\n}\n\nfunc (ld *v2LayerDescriptor) ID() string {\n\treturn stringid.TruncateID(ld.digest.String())\n}\n\nfunc (ld *v2LayerDescriptor) DiffID() (layer.DiffID, error) {\n\tif ld.diffID != \"\" {\n\t\treturn ld.diffID, nil\n\t}\n\treturn ld.V2MetadataService.GetDiffID(ld.digest)\n}\n\nfunc (ld *v2LayerDescriptor) Download(ctx context.Context, progressOutput progress.Output) (io.ReadCloser, int64, error) {\n\tlogrus.Debugf(\"pulling blob %q\", ld.digest)\n\n\tvar (\n\t\terr    error\n\t\toffset int64\n\t)\n\n\tif ld.tmpFile == nil {\n\t\tld.tmpFile, err = createDownloadFile()\n\t\tif err != nil {\n\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t}\n\t} else {\n\t\toffset, err = ld.tmpFile.Seek(0, io.SeekEnd)\n\t\tif err != nil {\n\t\t\tlogrus.Debugf(\"error seeking to end of download file: %v\", err)\n\t\t\toffset = 0\n\n\t\t\tld.tmpFile.Close()\n\t\t\tif err := os.Remove(ld.tmpFile.Name()); err != nil {\n\t\t\t\tlogrus.Errorf(\"Failed to remove temp file: %s\", ld.tmpFile.Name())\n\t\t\t}\n\t\t\tld.tmpFile, err = createDownloadFile()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\t\t} else if offset != 0 {\n\t\t\tlogrus.Debugf(\"attempting to resume download of %q from %d bytes\", ld.digest, offset)\n\t\t}\n\t}\n\n\ttmpFile := ld.tmpFile\n\n\tlayerDownload, err := ld.open(ctx)\n\tif err != nil {\n\t\tlogrus.Errorf(\"Error initiating layer download: %v\", err)\n\t\treturn nil, 0, retryOnError(err)\n\t}\n\n\tif offset != 0 {\n\t\t_, err := layerDownload.Seek(offset, io.SeekStart)\n\t\tif err != nil {\n\t\t\tif err := ld.truncateDownloadFile(); err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\t\t\treturn nil, 0, err\n\t\t}\n\t}\n\tsize, err := layerDownload.Seek(0, io.SeekEnd)\n\tif err != nil {\n\t\t// Seek failed, perhaps because there was no Content-Length\n\t\t// header. This shouldn't fail the download, because we can\n\t\t// still continue without a progress bar.\n\t\tsize = 0\n\t} else {\n\t\tif size != 0 && offset > size {\n\t\t\tlogrus.Debug(\"Partial download is larger than full blob. Starting over\")\n\t\t\toffset = 0\n\t\t\tif err := ld.truncateDownloadFile(); err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\t\t}\n\n\t\t// Restore the seek offset either at the beginning of the\n\t\t// stream, or just after the last byte we have from previous\n\t\t// attempts.\n\t\t_, err = layerDownload.Seek(offset, io.SeekStart)\n\t\tif err != nil {\n\t\t\treturn nil, 0, err\n\t\t}\n\t}\n\n\treader := progress.NewProgressReader(ioutils.NewCancelReadCloser(ctx, layerDownload), progressOutput, size-offset, ld.ID(), \"Downloading\")\n\tdefer reader.Close()\n\n\tif ld.verifier == nil {\n\t\tld.verifier = ld.digest.Verifier()\n\t}\n\n\t_, err = io.Copy(tmpFile, io.TeeReader(reader, ld.verifier))\n\tif err != nil {\n\t\tif err == transport.ErrWrongCodeForByteRange {\n\t\t\tif err := ld.truncateDownloadFile(); err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\t\t\treturn nil, 0, err\n\t\t}\n\t\treturn nil, 0, retryOnError(err)\n\t}\n\n\tprogress.Update(progressOutput, ld.ID(), \"Verifying Checksum\")\n\n\tif !ld.verifier.Verified() {\n\t\terr = fmt.Errorf(\"filesystem layer verification failed for digest %s\", ld.digest)\n\t\tlogrus.Error(err)\n\n\t\t// Allow a retry if this digest verification error happened\n\t\t// after a resumed download.\n\t\tif offset != 0 {\n\t\t\tif err := ld.truncateDownloadFile(); err != nil {\n\t\t\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t\t\t}\n\n\t\t\treturn nil, 0, err\n\t\t}\n\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t}\n\n\tprogress.Update(progressOutput, ld.ID(), \"Download complete\")\n\n\tlogrus.Debugf(\"Downloaded %s to tempfile %s\", ld.ID(), tmpFile.Name())\n\n\t_, err = tmpFile.Seek(0, io.SeekStart)\n\tif err != nil {\n\t\ttmpFile.Close()\n\t\tif err := os.Remove(tmpFile.Name()); err != nil {\n\t\t\tlogrus.Errorf(\"Failed to remove temp file: %s\", tmpFile.Name())\n\t\t}\n\t\tld.tmpFile = nil\n\t\tld.verifier = nil\n\t\treturn nil, 0, xfer.DoNotRetry{Err: err}\n\t}\n\n\t// hand off the temporary file to the download manager, so it will only\n\t// be closed once\n\tld.tmpFile = nil\n\n\treturn ioutils.NewReadCloserWrapper(tmpFile, func() error {\n\t\ttmpFile.Close()\n\t\terr := os.RemoveAll(tmpFile.Name())\n\t\tif err != nil {\n\t\t\tlogrus.Errorf(\"Failed to remove temp file: %s\", tmpFile.Name())\n\t\t}\n\t\treturn err\n\t}), size, nil\n}\n\nfunc (ld *v2LayerDescriptor) Close() {\n\tif ld.tmpFile != nil {\n\t\tld.tmpFile.Close()\n\t\tif err := os.RemoveAll(ld.tmpFile.Name()); err != nil {\n\t\t\tlogrus.Errorf(\"Failed to remove temp file: %s\", ld.tmpFile.Name())\n\t\t}\n\t}\n}\n\nfunc (ld *v2LayerDescriptor) truncateDownloadFile() error {\n\t// Need a new hash context since we will be redoing the download\n\tld.verifier = nil\n\n\tif _, err := ld.tmpFile.Seek(0, io.SeekStart); err != nil {\n\t\tlogrus.Errorf(\"error seeking to beginning of download file: %v\", err)\n\t\treturn err\n\t}\n\n\tif err := ld.tmpFile.Truncate(0); err != nil {\n\t\tlogrus.Errorf(\"error truncating download file: %v\", err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (ld *v2LayerDescriptor) Registered(diffID layer.DiffID) {\n\t// Cache mapping from this layer's DiffID to the blobsum\n\tld.V2MetadataService.Add(diffID, metadata.V2Metadata{Digest: ld.digest, SourceRepository: ld.repoInfo.Name.Name()})\n}\n\nfunc (p *v2Puller) pullV2Tag(ctx context.Context, ref reference.Named, platform *specs.Platform) (tagUpdated bool, err error) {\n\n\tvar (\n\t\ttagOrDigest string // Used for logging/progress only\n\t\tdgst        digest.Digest\n\t\tmt          string\n\t\tsize        int64\n\t\ttagged      reference.NamedTagged\n\t\tisTagged    bool\n\t)\n\tif digested, isDigested := ref.(reference.Canonical); isDigested {\n\t\tdgst = digested.Digest()\n\t\ttagOrDigest = digested.String()\n\t} else if tagged, isTagged = ref.(reference.NamedTagged); isTagged {\n\t\ttagService := p.repo.Tags(ctx)\n\t\tdesc, err := tagService.Get(ctx, tagged.Tag())\n\t\tif err != nil {\n\t\t\treturn false, allowV1Fallback(err)\n\t\t}\n\n\t\tdgst = desc.Digest\n\t\ttagOrDigest = tagged.Tag()\n\t\tmt = desc.MediaType\n\t\tsize = desc.Size\n\t} else {\n\t\treturn false, fmt.Errorf(\"internal error: reference has neither a tag nor a digest: %s\", reference.FamiliarString(ref))\n\t}\n\n\tctx = log.WithLogger(ctx, logrus.WithFields(\n\t\tlogrus.Fields{\n\t\t\t\"digest\": dgst,\n\t\t\t\"remote\": ref,\n\t\t}))\n\n\tdesc := specs.Descriptor{\n\t\tMediaType: mt,\n\t\tDigest:    dgst,\n\t\tSize:      size,\n\t}\n\tmanifest, err := p.manifestStore.Get(ctx, desc)\n\tif err != nil {\n\t\tif isTagged && isNotFound(errors.Cause(err)) {\n\t\t\tlogrus.WithField(\"ref\", ref).WithError(err).Debug(\"Falling back to pull manifest by tag\")\n\n\t\t\tmsg := `%s Failed to pull manifest by the resolved digest. This registry does not\n\tappear to conform to the distribution registry specification; falling back to\n\tpull by tag.  This fallback is DEPRECATED, and will be removed in a future\n\trelease.  Please contact admins of %s. %s\n`\n\n\t\t\twarnEmoji := \"\\U000026A0\\U0000FE0F\"\n\t\t\tprogress.Messagef(p.config.ProgressOutput, \"WARNING\", msg, warnEmoji, p.endpoint.URL, warnEmoji)\n\n\t\t\t// Fetch by tag worked, but fetch by digest didn't.\n\t\t\t// This is a broken registry implementation.\n\t\t\t// We'll fallback to the old behavior and get the manifest by tag.\n\t\t\tvar ms distribution.ManifestService\n\t\t\tms, err = p.repo.Manifests(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\n\t\t\tmanifest, err = ms.Get(ctx, \"\", distribution.WithTag(tagged.Tag()))\n\t\t\terr = errors.Wrap(err, \"error after falling back to get manifest by tag\")\n\t\t}\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t}\n\n\tif manifest == nil {\n\t\treturn false, fmt.Errorf(\"image manifest does not exist for tag or digest %q\", tagOrDigest)\n\t}\n\n\tif m, ok := manifest.(*schema2.DeserializedManifest); ok {\n\t\tvar allowedMediatype bool\n\t\tfor _, t := range p.config.Schema2Types {\n\t\t\tif m.Manifest.Config.MediaType == t {\n\t\t\t\tallowedMediatype = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !allowedMediatype {\n\t\t\tconfigClass := mediaTypeClasses[m.Manifest.Config.MediaType]\n\t\t\tif configClass == \"\" {\n\t\t\t\tconfigClass = \"unknown\"\n\t\t\t}\n\t\t\treturn false, invalidManifestClassError{m.Manifest.Config.MediaType, configClass}\n\t\t}\n\t}\n\n\t// If manSvc.Get succeeded, we can be confident that the registry on\n\t// the other side speaks the v2 protocol.\n\tp.confirmedV2 = true\n\n\tlogrus.Debugf(\"Pulling ref from V2 registry: %s\", reference.FamiliarString(ref))\n\tprogress.Message(p.config.ProgressOutput, tagOrDigest, \"Pulling from \"+reference.FamiliarName(p.repo.Named()))\n\n\tvar (\n\t\tid             digest.Digest\n\t\tmanifestDigest digest.Digest\n\t)\n\n\tswitch v := manifest.(type) {\n\tcase *schema1.SignedManifest:\n\t\tif p.config.RequireSchema2 {\n\t\t\treturn false, fmt.Errorf(\"invalid manifest: not schema2\")\n\t\t}\n\n\t\t// give registries time to upgrade to schema2 and only warn if we know a registry has been upgraded long time ago\n\t\t// TODO: condition to be removed\n\t\tif reference.Domain(ref) == \"docker.io\" {\n\t\t\tmsg := fmt.Sprintf(\"Image %s uses outdated schema1 manifest format. Please upgrade to a schema2 image for better future compatibility. More information at https://docs.docker.com/registry/spec/deprecated-schema-v1/\", ref)\n\t\t\tlogrus.Warn(msg)\n\t\t\tprogress.Message(p.config.ProgressOutput, \"\", msg)\n\t\t}\n\n\t\tid, manifestDigest, err = p.pullSchema1(ctx, ref, v, platform)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\tcase *schema2.DeserializedManifest:\n\t\tid, manifestDigest, err = p.pullSchema2(ctx, ref, v, platform)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\tcase *ocischema.DeserializedManifest:\n\t\tid, manifestDigest, err = p.pullOCI(ctx, ref, v, platform)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\tcase *manifestlist.DeserializedManifestList:\n\t\tid, manifestDigest, err = p.pullManifestList(ctx, ref, v, platform)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\tdefault:\n\t\treturn false, invalidManifestFormatError{}\n\t}\n\n\tprogress.Message(p.config.ProgressOutput, \"\", \"Digest: \"+manifestDigest.String())\n\n\tif p.config.ReferenceStore != nil {\n\t\toldTagID, err := p.config.ReferenceStore.Get(ref)\n\t\tif err == nil {\n\t\t\tif oldTagID == id {\n\t\t\t\treturn false, addDigestReference(p.config.ReferenceStore, ref, manifestDigest, id)\n\t\t\t}\n\t\t} else if err != refstore.ErrDoesNotExist {\n\t\t\treturn false, err\n\t\t}\n\n\t\tif canonical, ok := ref.(reference.Canonical); ok {\n\t\t\tif err = p.config.ReferenceStore.AddDigest(canonical, id, true); err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t} else {\n\t\t\tif err = addDigestReference(p.config.ReferenceStore, ref, manifestDigest, id); err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\tif err = p.config.ReferenceStore.AddTag(ref, id, true); err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t}\n\t}\n\treturn true, nil\n}\n\nfunc (p *v2Puller) pullSchema1(ctx context.Context, ref reference.Reference, unverifiedManifest *schema1.SignedManifest, platform *specs.Platform) (id digest.Digest, manifestDigest digest.Digest, err error) {\n\tvar verifiedManifest *schema1.Manifest\n\tverifiedManifest, err = verifySchema1Manifest(unverifiedManifest, ref)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\trootFS := image.NewRootFS()\n\n\t// remove duplicate layers and check parent chain validity\n\terr = fixManifestLayers(verifiedManifest)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tvar descriptors []xfer.DownloadDescriptor\n\n\t// Image history converted to the new format\n\tvar history []image.History\n\n\t// Note that the order of this loop is in the direction of bottom-most\n\t// to top-most, so that the downloads slice gets ordered correctly.\n\tfor i := len(verifiedManifest.FSLayers) - 1; i >= 0; i-- {\n\t\tblobSum := verifiedManifest.FSLayers[i].BlobSum\n\t\tif err = blobSum.Validate(); err != nil {\n\t\t\treturn \"\", \"\", errors.Wrapf(err, \"could not validate layer digest %q\", blobSum)\n\t\t}\n\n\t\tvar throwAway struct {\n\t\t\tThrowAway bool `json:\"throwaway,omitempty\"`\n\t\t}\n\t\tif err := json.Unmarshal([]byte(verifiedManifest.History[i].V1Compatibility), &throwAway); err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\n\t\th, err := v1.HistoryFromConfig([]byte(verifiedManifest.History[i].V1Compatibility), throwAway.ThrowAway)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\t\thistory = append(history, h)\n\n\t\tif throwAway.ThrowAway {\n\t\t\tcontinue\n\t\t}\n\n\t\tlayerDescriptor := &v2LayerDescriptor{\n\t\t\tdigest:            blobSum,\n\t\t\trepoInfo:          p.repoInfo,\n\t\t\trepo:              p.repo,\n\t\t\tV2MetadataService: p.V2MetadataService,\n\t\t}\n\n\t\tdescriptors = append(descriptors, layerDescriptor)\n\t}\n\n\t// The v1 manifest itself doesn't directly contain an OS. However,\n\t// the history does, but unfortunately that's a string, so search through\n\t// all the history until hopefully we find one which indicates the OS.\n\t// supertest2014/nyan is an example of a registry image with schemav1.\n\tconfigOS := runtime.GOOS\n\tif system.LCOWSupported() {\n\t\ttype config struct {\n\t\t\tOs string `json:\"os,omitempty\"`\n\t\t}\n\t\tfor _, v := range verifiedManifest.History {\n\t\t\tvar c config\n\t\t\tif err := json.Unmarshal([]byte(v.V1Compatibility), &c); err == nil {\n\t\t\t\tif c.Os != \"\" {\n\t\t\t\t\tconfigOS = c.Os\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// In the situation that the API call didn't specify an OS explicitly, but\n\t// we support the operating system, switch to that operating system.\n\t// eg FROM supertest2014/nyan with no platform specifier, and docker build\n\t// with no --platform= flag under LCOW.\n\trequestedOS := \"\"\n\tif platform != nil {\n\t\trequestedOS = platform.OS\n\t} else if system.IsOSSupported(configOS) {\n\t\trequestedOS = configOS\n\t}\n\n\t// Early bath if the requested OS doesn't match that of the configuration.\n\t// This avoids doing the download, only to potentially fail later.\n\tif !strings.EqualFold(configOS, requestedOS) {\n\t\treturn \"\", \"\", fmt.Errorf(\"cannot download image with operating system %q when requesting %q\", configOS, requestedOS)\n\t}\n\n\tresultRootFS, release, err := p.config.DownloadManager.Download(ctx, *rootFS, configOS, descriptors, p.config.ProgressOutput)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tdefer release()\n\n\tconfig, err := v1.MakeConfigFromV1Config([]byte(verifiedManifest.History[0].V1Compatibility), &resultRootFS, history)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\timageID, err := p.config.ImageStore.Put(ctx, config)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tmanifestDigest = digest.FromBytes(unverifiedManifest.Canonical)\n\n\treturn imageID, manifestDigest, nil\n}\n\nfunc (p *v2Puller) pullSchema2Layers(ctx context.Context, target distribution.Descriptor, layers []distribution.Descriptor, platform *specs.Platform) (id digest.Digest, err error) {\n\tif _, err := p.config.ImageStore.Get(ctx, target.Digest); err == nil {\n\t\t// If the image already exists locally, no need to pull\n\t\t// anything.\n\t\treturn target.Digest, nil\n\t}\n\n\tvar descriptors []xfer.DownloadDescriptor\n\n\t// Note that the order of this loop is in the direction of bottom-most\n\t// to top-most, so that the downloads slice gets ordered correctly.\n\tfor _, d := range layers {\n\t\tif err := d.Digest.Validate(); err != nil {\n\t\t\treturn \"\", errors.Wrapf(err, \"could not validate layer digest %q\", d.Digest)\n\t\t}\n\t\tlayerDescriptor := &v2LayerDescriptor{\n\t\t\tdigest:            d.Digest,\n\t\t\trepo:              p.repo,\n\t\t\trepoInfo:          p.repoInfo,\n\t\t\tV2MetadataService: p.V2MetadataService,\n\t\t\tsrc:               d,\n\t\t}\n\n\t\tdescriptors = append(descriptors, layerDescriptor)\n\t}\n\n\tconfigChan := make(chan []byte, 1)\n\tconfigErrChan := make(chan error, 1)\n\tlayerErrChan := make(chan error, 1)\n\tdownloadsDone := make(chan struct{})\n\tvar cancel func()\n\tctx, cancel = context.WithCancel(ctx)\n\tdefer cancel()\n\n\t// Pull the image config\n\tgo func() {\n\t\tconfigJSON, err := p.pullSchema2Config(ctx, target.Digest)\n\t\tif err != nil {\n\t\t\tconfigErrChan <- ImageConfigPullError{Err: err}\n\t\t\tcancel()\n\t\t\treturn\n\t\t}\n\t\tconfigChan <- configJSON\n\t}()\n\n\tvar (\n\t\tconfigJSON       []byte          // raw serialized image config\n\t\tdownloadedRootFS *image.RootFS   // rootFS from registered layers\n\t\tconfigRootFS     *image.RootFS   // rootFS from configuration\n\t\trelease          func()          // release resources from rootFS download\n\t\tconfigPlatform   *specs.Platform // for LCOW when registering downloaded layers\n\t)\n\n\tlayerStoreOS := runtime.GOOS\n\tif platform != nil {\n\t\tlayerStoreOS = platform.OS\n\t}\n\n\t// https://github.com/docker/docker/issues/24766 - Err on the side of caution,\n\t// explicitly blocking images intended for linux from the Windows daemon. On\n\t// Windows, we do this before the attempt to download, effectively serialising\n\t// the download slightly slowing it down. We have to do it this way, as\n\t// chances are the download of layers itself would fail due to file names\n\t// which aren't suitable for NTFS. At some point in the future, if a similar\n\t// check to block Windows images being pulled on Linux is implemented, it\n\t// may be necessary to perform the same type of serialisation.\n\tif runtime.GOOS == \"windows\" {\n\t\tconfigJSON, configRootFS, configPlatform, err = receiveConfig(p.config.ImageStore, configChan, configErrChan)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tif configRootFS == nil {\n\t\t\treturn \"\", errRootFSInvalid\n\t\t}\n\t\tif err := checkImageCompatibility(configPlatform.OS, configPlatform.OSVersion); err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tif len(descriptors) != len(configRootFS.DiffIDs) {\n\t\t\treturn \"\", errRootFSMismatch\n\t\t}\n\t\tif platform == nil {\n\t\t\t// Early bath if the requested OS doesn't match that of the configuration.\n\t\t\t// This avoids doing the download, only to potentially fail later.\n\t\t\tif !system.IsOSSupported(configPlatform.OS) {\n\t\t\t\treturn \"\", fmt.Errorf(\"cannot download image with operating system %q when requesting %q\", configPlatform.OS, layerStoreOS)\n\t\t\t}\n\t\t\tlayerStoreOS = configPlatform.OS\n\t\t}\n\n\t\t// Populate diff ids in descriptors to avoid downloading foreign layers\n\t\t// which have been side loaded\n\t\tfor i := range descriptors {\n\t\t\tdescriptors[i].(*v2LayerDescriptor).diffID = configRootFS.DiffIDs[i]\n\t\t}\n\t}\n\n\tif p.config.DownloadManager != nil {\n\t\tgo func() {\n\t\t\tvar (\n\t\t\t\terr    error\n\t\t\t\trootFS image.RootFS\n\t\t\t)\n\t\t\tdownloadRootFS := *image.NewRootFS()\n\t\t\trootFS, release, err = p.config.DownloadManager.Download(ctx, downloadRootFS, layerStoreOS, descriptors, p.config.ProgressOutput)\n\t\t\tif err != nil {\n\t\t\t\t// Intentionally do not cancel the config download here\n\t\t\t\t// as the error from config download (if there is one)\n\t\t\t\t// is more interesting than the layer download error\n\t\t\t\tlayerErrChan <- err\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tdownloadedRootFS = &rootFS\n\t\t\tclose(downloadsDone)\n\t\t}()\n\t} else {\n\t\t// We have nothing to download\n\t\tclose(downloadsDone)\n\t}\n\n\tif configJSON == nil {\n\t\tconfigJSON, configRootFS, _, err = receiveConfig(p.config.ImageStore, configChan, configErrChan)\n\t\tif err == nil && configRootFS == nil {\n\t\t\terr = errRootFSInvalid\n\t\t}\n\t\tif err != nil {\n\t\t\tcancel()\n\t\t\tselect {\n\t\t\tcase <-downloadsDone:\n\t\t\tcase <-layerErrChan:\n\t\t\t}\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\tselect {\n\tcase <-downloadsDone:\n\tcase err = <-layerErrChan:\n\t\treturn \"\", err\n\t}\n\n\tif release != nil {\n\t\tdefer release()\n\t}\n\n\tif downloadedRootFS != nil {\n\t\t// The DiffIDs returned in rootFS MUST match those in the config.\n\t\t// Otherwise the image config could be referencing layers that aren't\n\t\t// included in the manifest.\n\t\tif len(downloadedRootFS.DiffIDs) != len(configRootFS.DiffIDs) {\n\t\t\treturn \"\", errRootFSMismatch\n\t\t}\n\n\t\tfor i := range downloadedRootFS.DiffIDs {\n\t\t\tif downloadedRootFS.DiffIDs[i] != configRootFS.DiffIDs[i] {\n\t\t\t\treturn \"\", errRootFSMismatch\n\t\t\t}\n\t\t}\n\t}\n\n\timageID, err := p.config.ImageStore.Put(ctx, configJSON)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn imageID, nil\n}\n\nfunc (p *v2Puller) pullSchema2(ctx context.Context, ref reference.Named, mfst *schema2.DeserializedManifest, platform *specs.Platform) (id digest.Digest, manifestDigest digest.Digest, err error) {\n\tmanifestDigest, err = schema2ManifestDigest(ref, mfst)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tid, err = p.pullSchema2Layers(ctx, mfst.Target(), mfst.Layers, platform)\n\treturn id, manifestDigest, err\n}\n\nfunc (p *v2Puller) pullOCI(ctx context.Context, ref reference.Named, mfst *ocischema.DeserializedManifest, platform *specs.Platform) (id digest.Digest, manifestDigest digest.Digest, err error) {\n\tmanifestDigest, err = schema2ManifestDigest(ref, mfst)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tid, err = p.pullSchema2Layers(ctx, mfst.Target(), mfst.Layers, platform)\n\treturn id, manifestDigest, err\n}\n\nfunc receiveConfig(s ImageConfigStore, configChan <-chan []byte, errChan <-chan error) ([]byte, *image.RootFS, *specs.Platform, error) {\n\tselect {\n\tcase configJSON := <-configChan:\n\t\trootfs, err := s.RootFSFromConfig(configJSON)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tplatform, err := s.PlatformFromConfig(configJSON)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\treturn configJSON, rootfs, platform, nil\n\tcase err := <-errChan:\n\t\treturn nil, nil, nil, err\n\t\t// Don't need a case for ctx.Done in the select because cancellation\n\t\t// will trigger an error in p.pullSchema2ImageConfig.\n\t}\n}\n\n// pullManifestList handles \"manifest lists\" which point to various\n// platform-specific manifests.\nfunc (p *v2Puller) pullManifestList(ctx context.Context, ref reference.Named, mfstList *manifestlist.DeserializedManifestList, pp *specs.Platform) (id digest.Digest, manifestListDigest digest.Digest, err error) {\n\tmanifestListDigest, err = schema2ManifestDigest(ref, mfstList)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tvar platform specs.Platform\n\tif pp != nil {\n\t\tplatform = *pp\n\t}\n\tlogrus.Debugf(\"%s resolved to a manifestList object with %d entries; looking for a %s/%s match\", ref, len(mfstList.Manifests), platforms.Format(platform), runtime.GOARCH)\n\n\tmanifestMatches := filterManifests(mfstList.Manifests, platform)\n\n\tif len(manifestMatches) == 0 {\n\t\terrMsg := fmt.Sprintf(\"no matching manifest for %s in the manifest list entries\", formatPlatform(platform))\n\t\tlogrus.Debugf(errMsg)\n\t\treturn \"\", \"\", errors.New(errMsg)\n\t}\n\n\tif len(manifestMatches) > 1 {\n\t\tlogrus.Debugf(\"found multiple matches in manifest list, choosing best match %s\", manifestMatches[0].Digest.String())\n\t}\n\tmatch := manifestMatches[0]\n\n\tif err := checkImageCompatibility(match.Platform.OS, match.Platform.OSVersion); err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tdesc := specs.Descriptor{\n\t\tDigest:    match.Digest,\n\t\tSize:      match.Size,\n\t\tMediaType: match.MediaType,\n\t}\n\tmanifest, err := p.manifestStore.Get(ctx, desc)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tmanifestRef, err := reference.WithDigest(reference.TrimNamed(ref), match.Digest)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\tswitch v := manifest.(type) {\n\tcase *schema1.SignedManifest:\n\t\tmsg := fmt.Sprintf(\"[DEPRECATION NOTICE] v2 schema1 manifests in manifest lists are not supported and will break in a future release. Suggest author of %s to upgrade to v2 schema2. More information at https://docs.docker.com/registry/spec/deprecated-schema-v1/\", ref)\n\t\tlogrus.Warn(msg)\n\t\tprogress.Message(p.config.ProgressOutput, \"\", msg)\n\n\t\tplatform := toOCIPlatform(manifestMatches[0].Platform)\n\t\tid, _, err = p.pullSchema1(ctx, manifestRef, v, &platform)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\tcase *schema2.DeserializedManifest:\n\t\tplatform := toOCIPlatform(manifestMatches[0].Platform)\n\t\tid, _, err = p.pullSchema2(ctx, manifestRef, v, &platform)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\tcase *ocischema.DeserializedManifest:\n\t\tplatform := toOCIPlatform(manifestMatches[0].Platform)\n\t\tid, _, err = p.pullOCI(ctx, manifestRef, v, &platform)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", err\n\t\t}\n\tdefault:\n\t\treturn \"\", \"\", errors.New(\"unsupported manifest format\")\n\t}\n\n\treturn id, manifestListDigest, err\n}\n\nfunc (p *v2Puller) pullSchema2Config(ctx context.Context, dgst digest.Digest) (configJSON []byte, err error) {\n\tblobs := p.repo.Blobs(ctx)\n\tconfigJSON, err = blobs.Get(ctx, dgst)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Verify image config digest\n\tverifier := dgst.Verifier()\n\tif _, err := verifier.Write(configJSON); err != nil {\n\t\treturn nil, err\n\t}\n\tif !verifier.Verified() {\n\t\terr := fmt.Errorf(\"image config verification failed for digest %s\", dgst)\n\t\tlogrus.Error(err)\n\t\treturn nil, err\n\t}\n\n\treturn configJSON, nil\n}\n\n// schema2ManifestDigest computes the manifest digest, and, if pulling by\n// digest, ensures that it matches the requested digest.\nfunc schema2ManifestDigest(ref reference.Named, mfst distribution.Manifest) (digest.Digest, error) {\n\t_, canonical, err := mfst.Payload()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// If pull by digest, then verify the manifest digest.\n\tif digested, isDigested := ref.(reference.Canonical); isDigested {\n\t\tverifier := digested.Digest().Verifier()\n\t\tif _, err := verifier.Write(canonical); err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tif !verifier.Verified() {\n\t\t\terr := fmt.Errorf(\"manifest verification failed for digest %s\", digested.Digest())\n\t\t\tlogrus.Error(err)\n\t\t\treturn \"\", err\n\t\t}\n\t\treturn digested.Digest(), nil\n\t}\n\n\treturn digest.FromBytes(canonical), nil\n}\n\n// allowV1Fallback checks if the error is a possible reason to fallback to v1\n// (even if confirmedV2 has been set already), and if so, wraps the error in\n// a fallbackError with confirmedV2 set to false. Otherwise, it returns the\n// error unmodified.\nfunc allowV1Fallback(err error) error {\n\tswitch v := err.(type) {\n\tcase errcode.Errors:\n\t\tif len(v) != 0 {\n\t\t\tif v0, ok := v[0].(errcode.Error); ok && shouldV2Fallback(v0) {\n\t\t\t\treturn fallbackError{\n\t\t\t\t\terr:         err,\n\t\t\t\t\tconfirmedV2: false,\n\t\t\t\t\ttransportOK: true,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\tcase errcode.Error:\n\t\tif shouldV2Fallback(v) {\n\t\t\treturn fallbackError{\n\t\t\t\terr:         err,\n\t\t\t\tconfirmedV2: false,\n\t\t\t\ttransportOK: true,\n\t\t\t}\n\t\t}\n\tcase *url.Error:\n\t\tif v.Err == auth.ErrNoBasicAuthCredentials {\n\t\t\treturn fallbackError{err: err, confirmedV2: false}\n\t\t}\n\t}\n\n\treturn err\n}\n\nfunc verifySchema1Manifest(signedManifest *schema1.SignedManifest, ref reference.Reference) (m *schema1.Manifest, err error) {\n\t// If pull by digest, then verify the manifest digest. NOTE: It is\n\t// important to do this first, before any other content validation. If the\n\t// digest cannot be verified, don't even bother with those other things.\n\tif digested, isCanonical := ref.(reference.Canonical); isCanonical {\n\t\tverifier := digested.Digest().Verifier()\n\t\tif _, err := verifier.Write(signedManifest.Canonical); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif !verifier.Verified() {\n\t\t\terr := fmt.Errorf(\"image verification failed for digest %s\", digested.Digest())\n\t\t\tlogrus.Error(err)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tm = &signedManifest.Manifest\n\n\tif m.SchemaVersion != 1 {\n\t\treturn nil, fmt.Errorf(\"unsupported schema version %d for %q\", m.SchemaVersion, reference.FamiliarString(ref))\n\t}\n\tif len(m.FSLayers) != len(m.History) {\n\t\treturn nil, fmt.Errorf(\"length of history not equal to number of layers for %q\", reference.FamiliarString(ref))\n\t}\n\tif len(m.FSLayers) == 0 {\n\t\treturn nil, fmt.Errorf(\"no FSLayers in manifest for %q\", reference.FamiliarString(ref))\n\t}\n\treturn m, nil\n}\n\n// fixManifestLayers removes repeated layers from the manifest and checks the\n// correctness of the parent chain.\nfunc fixManifestLayers(m *schema1.Manifest) error {\n\timgs := make([]*image.V1Image, len(m.FSLayers))\n\tfor i := range m.FSLayers {\n\t\timg := &image.V1Image{}\n\n\t\tif err := json.Unmarshal([]byte(m.History[i].V1Compatibility), img); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\timgs[i] = img\n\t\tif err := v1.ValidateID(img.ID); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif imgs[len(imgs)-1].Parent != \"\" && runtime.GOOS != \"windows\" {\n\t\t// Windows base layer can point to a base layer parent that is not in manifest.\n\t\treturn errors.New(\"invalid parent ID in the base layer of the image\")\n\t}\n\n\t// check general duplicates to error instead of a deadlock\n\tidmap := make(map[string]struct{})\n\n\tvar lastID string\n\tfor _, img := range imgs {\n\t\t// skip IDs that appear after each other, we handle those later\n\t\tif _, exists := idmap[img.ID]; img.ID != lastID && exists {\n\t\t\treturn fmt.Errorf(\"ID %+v appears multiple times in manifest\", img.ID)\n\t\t}\n\t\tlastID = img.ID\n\t\tidmap[lastID] = struct{}{}\n\t}\n\n\t// backwards loop so that we keep the remaining indexes after removing items\n\tfor i := len(imgs) - 2; i >= 0; i-- {\n\t\tif imgs[i].ID == imgs[i+1].ID { // repeated ID. remove and continue\n\t\t\tm.FSLayers = append(m.FSLayers[:i], m.FSLayers[i+1:]...)\n\t\t\tm.History = append(m.History[:i], m.History[i+1:]...)\n\t\t} else if imgs[i].Parent != imgs[i+1].ID {\n\t\t\treturn fmt.Errorf(\"invalid parent ID. Expected %v, got %v\", imgs[i+1].ID, imgs[i].Parent)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc createDownloadFile() (*os.File, error) {\n\treturn ioutil.TempFile(\"\", \"GetImageBlob\")\n}\n\nfunc toOCIPlatform(p manifestlist.PlatformSpec) specs.Platform {\n\treturn specs.Platform{\n\t\tOS:           p.OS,\n\t\tArchitecture: p.Architecture,\n\t\tVariant:      p.Variant,\n\t\tOSFeatures:   p.OSFeatures,\n\t\tOSVersion:    p.OSVersion,\n\t}\n}\n"], "filenames": ["builder/builder-next/adapters/containerimage/pull.go", "distribution/pull_v2.go"], "buggy_code_start_loc": [526, 530], "buggy_code_end_loc": [526, 628], "fixing_code_start_loc": [527, 531], "fixing_code_end_loc": [530, 635], "type": "CWE-754", "message": "In Docker before versions 9.03.15, 20.10.3 there is a vulnerability in which pulling an intentionally malformed Docker image manifest crashes the dockerd daemon. Versions 20.10.3 and 19.03.15 contain patches that prevent the daemon from crashing.", "other": {"cve": {"id": "CVE-2021-21285", "sourceIdentifier": "security-advisories@github.com", "published": "2021-02-02T18:15:12.203", "lastModified": "2022-10-25T12:55:28.000", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In Docker before versions 9.03.15, 20.10.3 there is a vulnerability in which pulling an intentionally malformed Docker image manifest crashes the dockerd daemon. Versions 20.10.3 and 19.03.15 contain patches that prevent the daemon from crashing."}, {"lang": "es", "value": "En Docker versiones anteriores a 9.03.15, 20.10.3, se presenta una vulnerabilidad en la que al extraer un manifiesto de imagen de Docker malformado intencionalmente, bloquea al demonio dockerd.&#xa0;Las versiones 20.10.3 y 19.03.15 contienen parches que impiden al demonio bloquearse"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-754"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:docker:docker:*:*:*:*:*:*:*:*", "versionEndExcluding": "19.03.15", "matchCriteriaId": "C6B22016-AB78-4E82-9F65-AEC2526F3EDF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:docker:docker:*:*:*:*:*:*:*:*", "versionStartIncluding": "20.0.0", "versionEndExcluding": "20.10.3", "matchCriteriaId": "B4427D14-D219-490B-8467-40FE253775A1"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:e-series_santricity_os_controller:*:*:*:*:*:*:*:*", "versionStartIncluding": "11.0", "versionEndIncluding": "11.60.3", "matchCriteriaId": "7402489D-85E5-4662-BF87-259740DC72F8"}]}]}], "references": [{"url": "https://docs.docker.com/engine/release-notes/#20103", "source": "security-advisories@github.com", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://github.com/moby/moby/commit/8d3179546e79065adefa67cc697c09d0ab137d30", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/moby/moby/releases/tag/v19.03.15", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/moby/moby/releases/tag/v20.10.3", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/moby/moby/security/advisories/GHSA-6fj5-m822-rqx8", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://security.gentoo.org/glsa/202107-23", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20210226-0005/", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2021/dsa-4865", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/moby/moby/commit/8d3179546e79065adefa67cc697c09d0ab137d30"}}