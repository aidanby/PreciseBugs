{"buggy_code": ["/*\n * random.c -- A strong random number generator\n *\n * Copyright (C) 2017 Jason A. Donenfeld <Jason@zx2c4.com>. All\n * Rights Reserved.\n *\n * Copyright Matt Mackall <mpm@selenic.com>, 2003, 2004, 2005\n *\n * Copyright Theodore Ts'o, 1994, 1995, 1996, 1997, 1998, 1999.  All\n * rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, and the entire permission notice in its entirety,\n *    including the disclaimer of warranties.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. The name of the author may not be used to endorse or promote\n *    products derived from this software without specific prior\n *    written permission.\n *\n * ALTERNATIVELY, this product may be distributed under the terms of\n * the GNU General Public License, in which case the provisions of the GPL are\n * required INSTEAD OF the above restrictions.  (This clause is\n * necessary due to a potential bad interaction between the GPL and\n * the restrictions contained in a BSD-style copyright.)\n *\n * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, ALL OF\n * WHICH ARE HEREBY DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT\n * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n * USE OF THIS SOFTWARE, EVEN IF NOT ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n */\n\n/*\n * (now, with legal B.S. out of the way.....)\n *\n * This routine gathers environmental noise from device drivers, etc.,\n * and returns good random numbers, suitable for cryptographic use.\n * Besides the obvious cryptographic uses, these numbers are also good\n * for seeding TCP sequence numbers, and other places where it is\n * desirable to have numbers which are not only random, but hard to\n * predict by an attacker.\n *\n * Theory of operation\n * ===================\n *\n * Computers are very predictable devices.  Hence it is extremely hard\n * to produce truly random numbers on a computer --- as opposed to\n * pseudo-random numbers, which can easily generated by using a\n * algorithm.  Unfortunately, it is very easy for attackers to guess\n * the sequence of pseudo-random number generators, and for some\n * applications this is not acceptable.  So instead, we must try to\n * gather \"environmental noise\" from the computer's environment, which\n * must be hard for outside attackers to observe, and use that to\n * generate random numbers.  In a Unix environment, this is best done\n * from inside the kernel.\n *\n * Sources of randomness from the environment include inter-keyboard\n * timings, inter-interrupt timings from some interrupts, and other\n * events which are both (a) non-deterministic and (b) hard for an\n * outside observer to measure.  Randomness from these sources are\n * added to an \"entropy pool\", which is mixed using a CRC-like function.\n * This is not cryptographically strong, but it is adequate assuming\n * the randomness is not chosen maliciously, and it is fast enough that\n * the overhead of doing it on every interrupt is very reasonable.\n * As random bytes are mixed into the entropy pool, the routines keep\n * an *estimate* of how many bits of randomness have been stored into\n * the random number generator's internal state.\n *\n * When random bytes are desired, they are obtained by taking the SHA\n * hash of the contents of the \"entropy pool\".  The SHA hash avoids\n * exposing the internal state of the entropy pool.  It is believed to\n * be computationally infeasible to derive any useful information\n * about the input of SHA from its output.  Even if it is possible to\n * analyze SHA in some clever way, as long as the amount of data\n * returned from the generator is less than the inherent entropy in\n * the pool, the output data is totally unpredictable.  For this\n * reason, the routine decreases its internal estimate of how many\n * bits of \"true randomness\" are contained in the entropy pool as it\n * outputs random numbers.\n *\n * If this estimate goes to zero, the routine can still generate\n * random numbers; however, an attacker may (at least in theory) be\n * able to infer the future output of the generator from prior\n * outputs.  This requires successful cryptanalysis of SHA, which is\n * not believed to be feasible, but there is a remote possibility.\n * Nonetheless, these numbers should be useful for the vast majority\n * of purposes.\n *\n * Exported interfaces ---- output\n * ===============================\n *\n * There are four exported interfaces; two for use within the kernel,\n * and two or use from userspace.\n *\n * Exported interfaces ---- userspace output\n * -----------------------------------------\n *\n * The userspace interfaces are two character devices /dev/random and\n * /dev/urandom.  /dev/random is suitable for use when very high\n * quality randomness is desired (for example, for key generation or\n * one-time pads), as it will only return a maximum of the number of\n * bits of randomness (as estimated by the random number generator)\n * contained in the entropy pool.\n *\n * The /dev/urandom device does not have this limit, and will return\n * as many bytes as are requested.  As more and more random bytes are\n * requested without giving time for the entropy pool to recharge,\n * this will result in random numbers that are merely cryptographically\n * strong.  For many applications, however, this is acceptable.\n *\n * Exported interfaces ---- kernel output\n * --------------------------------------\n *\n * The primary kernel interface is\n *\n * \tvoid get_random_bytes(void *buf, int nbytes);\n *\n * This interface will return the requested number of random bytes,\n * and place it in the requested buffer.  This is equivalent to a\n * read from /dev/urandom.\n *\n * For less critical applications, there are the functions:\n *\n * \tu32 get_random_u32()\n * \tu64 get_random_u64()\n * \tunsigned int get_random_int()\n * \tunsigned long get_random_long()\n *\n * These are produced by a cryptographic RNG seeded from get_random_bytes,\n * and so do not deplete the entropy pool as much.  These are recommended\n * for most in-kernel operations *if the result is going to be stored in\n * the kernel*.\n *\n * Specifically, the get_random_int() family do not attempt to do\n * \"anti-backtracking\".  If you capture the state of the kernel (e.g.\n * by snapshotting the VM), you can figure out previous get_random_int()\n * return values.  But if the value is stored in the kernel anyway,\n * this is not a problem.\n *\n * It *is* safe to expose get_random_int() output to attackers (e.g. as\n * network cookies); given outputs 1..n, it's not feasible to predict\n * outputs 0 or n+1.  The only concern is an attacker who breaks into\n * the kernel later; the get_random_int() engine is not reseeded as\n * often as the get_random_bytes() one.\n *\n * get_random_bytes() is needed for keys that need to stay secret after\n * they are erased from the kernel.  For example, any key that will\n * be wrapped and stored encrypted.  And session encryption keys: we'd\n * like to know that after the session is closed and the keys erased,\n * the plaintext is unrecoverable to someone who recorded the ciphertext.\n *\n * But for network ports/cookies, stack canaries, PRNG seeds, address\n * space layout randomization, session *authentication* keys, or other\n * applications where the sensitive data is stored in the kernel in\n * plaintext for as long as it's sensitive, the get_random_int() family\n * is just fine.\n *\n * Consider ASLR.  We want to keep the address space secret from an\n * outside attacker while the process is running, but once the address\n * space is torn down, it's of no use to an attacker any more.  And it's\n * stored in kernel data structures as long as it's alive, so worrying\n * about an attacker's ability to extrapolate it from the get_random_int()\n * CRNG is silly.\n *\n * Even some cryptographic keys are safe to generate with get_random_int().\n * In particular, keys for SipHash are generally fine.  Here, knowledge\n * of the key authorizes you to do something to a kernel object (inject\n * packets to a network connection, or flood a hash table), and the\n * key is stored with the object being protected.  Once it goes away,\n * we no longer care if anyone knows the key.\n *\n * prandom_u32()\n * -------------\n *\n * For even weaker applications, see the pseudorandom generator\n * prandom_u32(), prandom_max(), and prandom_bytes().  If the random\n * numbers aren't security-critical at all, these are *far* cheaper.\n * Useful for self-tests, random error simulation, randomized backoffs,\n * and any other application where you trust that nobody is trying to\n * maliciously mess with you by guessing the \"random\" numbers.\n *\n * Exported interfaces ---- input\n * ==============================\n *\n * The current exported interfaces for gathering environmental noise\n * from the devices are:\n *\n *\tvoid add_device_randomness(const void *buf, unsigned int size);\n * \tvoid add_input_randomness(unsigned int type, unsigned int code,\n *                                unsigned int value);\n *\tvoid add_interrupt_randomness(int irq, int irq_flags);\n * \tvoid add_disk_randomness(struct gendisk *disk);\n *\n * add_device_randomness() is for adding data to the random pool that\n * is likely to differ between two devices (or possibly even per boot).\n * This would be things like MAC addresses or serial numbers, or the\n * read-out of the RTC. This does *not* add any actual entropy to the\n * pool, but it initializes the pool to different values for devices\n * that might otherwise be identical and have very little entropy\n * available to them (particularly common in the embedded world).\n *\n * add_input_randomness() uses the input layer interrupt timing, as well as\n * the event type information from the hardware.\n *\n * add_interrupt_randomness() uses the interrupt timing as random\n * inputs to the entropy pool. Using the cycle counters and the irq source\n * as inputs, it feeds the randomness roughly once a second.\n *\n * add_disk_randomness() uses what amounts to the seek time of block\n * layer request events, on a per-disk_devt basis, as input to the\n * entropy pool. Note that high-speed solid state drives with very low\n * seek times do not make for good sources of entropy, as their seek\n * times are usually fairly consistent.\n *\n * All of these routines try to estimate how many bits of randomness a\n * particular randomness source.  They do this by keeping track of the\n * first and second order deltas of the event timings.\n *\n * Ensuring unpredictability at system startup\n * ============================================\n *\n * When any operating system starts up, it will go through a sequence\n * of actions that are fairly predictable by an adversary, especially\n * if the start-up does not involve interaction with a human operator.\n * This reduces the actual number of bits of unpredictability in the\n * entropy pool below the value in entropy_count.  In order to\n * counteract this effect, it helps to carry information in the\n * entropy pool across shut-downs and start-ups.  To do this, put the\n * following lines an appropriate script which is run during the boot\n * sequence:\n *\n *\techo \"Initializing random number generator...\"\n *\trandom_seed=/var/run/random-seed\n *\t# Carry a random seed from start-up to start-up\n *\t# Load and then save the whole entropy pool\n *\tif [ -f $random_seed ]; then\n *\t\tcat $random_seed >/dev/urandom\n *\telse\n *\t\ttouch $random_seed\n *\tfi\n *\tchmod 600 $random_seed\n *\tdd if=/dev/urandom of=$random_seed count=1 bs=512\n *\n * and the following lines in an appropriate script which is run as\n * the system is shutdown:\n *\n *\t# Carry a random seed from shut-down to start-up\n *\t# Save the whole entropy pool\n *\techo \"Saving random seed...\"\n *\trandom_seed=/var/run/random-seed\n *\ttouch $random_seed\n *\tchmod 600 $random_seed\n *\tdd if=/dev/urandom of=$random_seed count=1 bs=512\n *\n * For example, on most modern systems using the System V init\n * scripts, such code fragments would be found in\n * /etc/rc.d/init.d/random.  On older Linux systems, the correct script\n * location might be in /etc/rcb.d/rc.local or /etc/rc.d/rc.0.\n *\n * Effectively, these commands cause the contents of the entropy pool\n * to be saved at shut-down time and reloaded into the entropy pool at\n * start-up.  (The 'dd' in the addition to the bootup script is to\n * make sure that /etc/random-seed is different for every start-up,\n * even if the system crashes without executing rc.0.)  Even with\n * complete knowledge of the start-up activities, predicting the state\n * of the entropy pool requires knowledge of the previous history of\n * the system.\n *\n * Configuring the /dev/random driver under Linux\n * ==============================================\n *\n * The /dev/random driver under Linux uses minor numbers 8 and 9 of\n * the /dev/mem major number (#1).  So if your system does not have\n * /dev/random and /dev/urandom created already, they can be created\n * by using the commands:\n *\n * \tmknod /dev/random c 1 8\n * \tmknod /dev/urandom c 1 9\n *\n * Acknowledgements:\n * =================\n *\n * Ideas for constructing this random number generator were derived\n * from Pretty Good Privacy's random number generator, and from private\n * discussions with Phil Karn.  Colin Plumb provided a faster random\n * number generator, which speed up the mixing function of the entropy\n * pool, taken from PGPfone.  Dale Worley has also contributed many\n * useful ideas and suggestions to improve this driver.\n *\n * Any flaws in the design are solely my responsibility, and should\n * not be attributed to the Phil, Colin, or any of authors of PGP.\n *\n * Further background information on this topic may be obtained from\n * RFC 1750, \"Randomness Recommendations for Security\", by Donald\n * Eastlake, Steve Crocker, and Jeff Schiller.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/utsname.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/major.h>\n#include <linux/string.h>\n#include <linux/fcntl.h>\n#include <linux/slab.h>\n#include <linux/random.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/genhd.h>\n#include <linux/interrupt.h>\n#include <linux/mm.h>\n#include <linux/nodemask.h>\n#include <linux/spinlock.h>\n#include <linux/kthread.h>\n#include <linux/percpu.h>\n#include <linux/fips.h>\n#include <linux/ptrace.h>\n#include <linux/workqueue.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/syscalls.h>\n#include <linux/completion.h>\n#include <linux/uuid.h>\n#include <crypto/chacha.h>\n#include <crypto/sha.h>\n\n#include <asm/processor.h>\n#include <linux/uaccess.h>\n#include <asm/irq.h>\n#include <asm/irq_regs.h>\n#include <asm/io.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/random.h>\n\n/* #define ADD_INTERRUPT_BENCH */\n\n/*\n * Configuration information\n */\n#define INPUT_POOL_SHIFT\t12\n#define INPUT_POOL_WORDS\t(1 << (INPUT_POOL_SHIFT-5))\n#define OUTPUT_POOL_SHIFT\t10\n#define OUTPUT_POOL_WORDS\t(1 << (OUTPUT_POOL_SHIFT-5))\n#define EXTRACT_SIZE\t\t10\n\n\n#define LONGS(x) (((x) + sizeof(unsigned long) - 1)/sizeof(unsigned long))\n\n/*\n * To allow fractional bits to be tracked, the entropy_count field is\n * denominated in units of 1/8th bits.\n *\n * 2*(ENTROPY_SHIFT + poolbitshift) must <= 31, or the multiply in\n * credit_entropy_bits() needs to be 64 bits wide.\n */\n#define ENTROPY_SHIFT 3\n#define ENTROPY_BITS(r) ((r)->entropy_count >> ENTROPY_SHIFT)\n\n/*\n * If the entropy count falls under this number of bits, then we\n * should wake up processes which are selecting or polling on write\n * access to /dev/random.\n */\nstatic int random_write_wakeup_bits = 28 * OUTPUT_POOL_WORDS;\n\n/*\n * Originally, we used a primitive polynomial of degree .poolwords\n * over GF(2).  The taps for various sizes are defined below.  They\n * were chosen to be evenly spaced except for the last tap, which is 1\n * to get the twisting happening as fast as possible.\n *\n * For the purposes of better mixing, we use the CRC-32 polynomial as\n * well to make a (modified) twisted Generalized Feedback Shift\n * Register.  (See M. Matsumoto & Y. Kurita, 1992.  Twisted GFSR\n * generators.  ACM Transactions on Modeling and Computer Simulation\n * 2(3):179-194.  Also see M. Matsumoto & Y. Kurita, 1994.  Twisted\n * GFSR generators II.  ACM Transactions on Modeling and Computer\n * Simulation 4:254-266)\n *\n * Thanks to Colin Plumb for suggesting this.\n *\n * The mixing operation is much less sensitive than the output hash,\n * where we use SHA-1.  All that we want of mixing operation is that\n * it be a good non-cryptographic hash; i.e. it not produce collisions\n * when fed \"random\" data of the sort we expect to see.  As long as\n * the pool state differs for different inputs, we have preserved the\n * input entropy and done a good job.  The fact that an intelligent\n * attacker can construct inputs that will produce controlled\n * alterations to the pool's state is not important because we don't\n * consider such inputs to contribute any randomness.  The only\n * property we need with respect to them is that the attacker can't\n * increase his/her knowledge of the pool's state.  Since all\n * additions are reversible (knowing the final state and the input,\n * you can reconstruct the initial state), if an attacker has any\n * uncertainty about the initial state, he/she can only shuffle that\n * uncertainty about, but never cause any collisions (which would\n * decrease the uncertainty).\n *\n * Our mixing functions were analyzed by Lacharme, Roeck, Strubel, and\n * Videau in their paper, \"The Linux Pseudorandom Number Generator\n * Revisited\" (see: http://eprint.iacr.org/2012/251.pdf).  In their\n * paper, they point out that we are not using a true Twisted GFSR,\n * since Matsumoto & Kurita used a trinomial feedback polynomial (that\n * is, with only three taps, instead of the six that we are using).\n * As a result, the resulting polynomial is neither primitive nor\n * irreducible, and hence does not have a maximal period over\n * GF(2**32).  They suggest a slight change to the generator\n * polynomial which improves the resulting TGFSR polynomial to be\n * irreducible, which we have made here.\n */\nstatic const struct poolinfo {\n\tint poolbitshift, poolwords, poolbytes, poolfracbits;\n#define S(x) ilog2(x)+5, (x), (x)*4, (x) << (ENTROPY_SHIFT+5)\n\tint tap1, tap2, tap3, tap4, tap5;\n} poolinfo_table[] = {\n\t/* was: x^128 + x^103 + x^76 + x^51 +x^25 + x + 1 */\n\t/* x^128 + x^104 + x^76 + x^51 +x^25 + x + 1 */\n\t{ S(128),\t104,\t76,\t51,\t25,\t1 },\n};\n\n/*\n * Static global variables\n */\nstatic DECLARE_WAIT_QUEUE_HEAD(random_write_wait);\nstatic struct fasync_struct *fasync;\n\nstatic DEFINE_SPINLOCK(random_ready_list_lock);\nstatic LIST_HEAD(random_ready_list);\n\nstruct crng_state {\n\t__u32\t\tstate[16];\n\tunsigned long\tinit_time;\n\tspinlock_t\tlock;\n};\n\nstatic struct crng_state primary_crng = {\n\t.lock = __SPIN_LOCK_UNLOCKED(primary_crng.lock),\n};\n\n/*\n * crng_init =  0 --> Uninitialized\n *\t\t1 --> Initialized\n *\t\t2 --> Initialized from input_pool\n *\n * crng_init is protected by primary_crng->lock, and only increases\n * its value (from 0->1->2).\n */\nstatic int crng_init = 0;\n#define crng_ready() (likely(crng_init > 1))\nstatic int crng_init_cnt = 0;\nstatic unsigned long crng_global_init_time = 0;\n#define CRNG_INIT_CNT_THRESH (2*CHACHA_KEY_SIZE)\nstatic void _extract_crng(struct crng_state *crng, __u8 out[CHACHA_BLOCK_SIZE]);\nstatic void _crng_backtrack_protect(struct crng_state *crng,\n\t\t\t\t    __u8 tmp[CHACHA_BLOCK_SIZE], int used);\nstatic void process_random_ready_list(void);\nstatic void _get_random_bytes(void *buf, int nbytes);\n\nstatic struct ratelimit_state unseeded_warning =\n\tRATELIMIT_STATE_INIT(\"warn_unseeded_randomness\", HZ, 3);\nstatic struct ratelimit_state urandom_warning =\n\tRATELIMIT_STATE_INIT(\"warn_urandom_randomness\", HZ, 3);\n\nstatic int ratelimit_disable __read_mostly;\n\nmodule_param_named(ratelimit_disable, ratelimit_disable, int, 0644);\nMODULE_PARM_DESC(ratelimit_disable, \"Disable random ratelimit suppression\");\n\n/**********************************************************************\n *\n * OS independent entropy store.   Here are the functions which handle\n * storing entropy in an entropy pool.\n *\n **********************************************************************/\n\nstruct entropy_store;\nstruct entropy_store {\n\t/* read-only data: */\n\tconst struct poolinfo *poolinfo;\n\t__u32 *pool;\n\tconst char *name;\n\n\t/* read-write data: */\n\tspinlock_t lock;\n\tunsigned short add_ptr;\n\tunsigned short input_rotate;\n\tint entropy_count;\n\tunsigned int initialized:1;\n\tunsigned int last_data_init:1;\n\t__u8 last_data[EXTRACT_SIZE];\n};\n\nstatic ssize_t extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t       size_t nbytes, int min, int rsvd);\nstatic ssize_t _extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t\tsize_t nbytes, int fips);\n\nstatic void crng_reseed(struct crng_state *crng, struct entropy_store *r);\nstatic __u32 input_pool_data[INPUT_POOL_WORDS] __latent_entropy;\n\nstatic struct entropy_store input_pool = {\n\t.poolinfo = &poolinfo_table[0],\n\t.name = \"input\",\n\t.lock = __SPIN_LOCK_UNLOCKED(input_pool.lock),\n\t.pool = input_pool_data\n};\n\nstatic __u32 const twist_table[8] = {\n\t0x00000000, 0x3b6e20c8, 0x76dc4190, 0x4db26158,\n\t0xedb88320, 0xd6d6a3e8, 0x9b64c2b0, 0xa00ae278 };\n\n/*\n * This function adds bytes into the entropy \"pool\".  It does not\n * update the entropy estimate.  The caller should call\n * credit_entropy_bits if this is appropriate.\n *\n * The pool is stirred with a primitive polynomial of the appropriate\n * degree, and then twisted.  We twist by three bits at a time because\n * it's cheap to do so and helps slightly in the expected case where\n * the entropy is concentrated in the low-order bits.\n */\nstatic void _mix_pool_bytes(struct entropy_store *r, const void *in,\n\t\t\t    int nbytes)\n{\n\tunsigned long i, tap1, tap2, tap3, tap4, tap5;\n\tint input_rotate;\n\tint wordmask = r->poolinfo->poolwords - 1;\n\tconst char *bytes = in;\n\t__u32 w;\n\n\ttap1 = r->poolinfo->tap1;\n\ttap2 = r->poolinfo->tap2;\n\ttap3 = r->poolinfo->tap3;\n\ttap4 = r->poolinfo->tap4;\n\ttap5 = r->poolinfo->tap5;\n\n\tinput_rotate = r->input_rotate;\n\ti = r->add_ptr;\n\n\t/* mix one byte at a time to simplify size handling and churn faster */\n\twhile (nbytes--) {\n\t\tw = rol32(*bytes++, input_rotate);\n\t\ti = (i - 1) & wordmask;\n\n\t\t/* XOR in the various taps */\n\t\tw ^= r->pool[i];\n\t\tw ^= r->pool[(i + tap1) & wordmask];\n\t\tw ^= r->pool[(i + tap2) & wordmask];\n\t\tw ^= r->pool[(i + tap3) & wordmask];\n\t\tw ^= r->pool[(i + tap4) & wordmask];\n\t\tw ^= r->pool[(i + tap5) & wordmask];\n\n\t\t/* Mix the result back in with a twist */\n\t\tr->pool[i] = (w >> 3) ^ twist_table[w & 7];\n\n\t\t/*\n\t\t * Normally, we add 7 bits of rotation to the pool.\n\t\t * At the beginning of the pool, add an extra 7 bits\n\t\t * rotation, so that successive passes spread the\n\t\t * input bits across the pool evenly.\n\t\t */\n\t\tinput_rotate = (input_rotate + (i ? 7 : 14)) & 31;\n\t}\n\n\tr->input_rotate = input_rotate;\n\tr->add_ptr = i;\n}\n\nstatic void __mix_pool_bytes(struct entropy_store *r, const void *in,\n\t\t\t     int nbytes)\n{\n\ttrace_mix_pool_bytes_nolock(r->name, nbytes, _RET_IP_);\n\t_mix_pool_bytes(r, in, nbytes);\n}\n\nstatic void mix_pool_bytes(struct entropy_store *r, const void *in,\n\t\t\t   int nbytes)\n{\n\tunsigned long flags;\n\n\ttrace_mix_pool_bytes(r->name, nbytes, _RET_IP_);\n\tspin_lock_irqsave(&r->lock, flags);\n\t_mix_pool_bytes(r, in, nbytes);\n\tspin_unlock_irqrestore(&r->lock, flags);\n}\n\nstruct fast_pool {\n\t__u32\t\tpool[4];\n\tunsigned long\tlast;\n\tunsigned short\treg_idx;\n\tunsigned char\tcount;\n};\n\n/*\n * This is a fast mixing routine used by the interrupt randomness\n * collector.  It's hardcoded for an 128 bit pool and assumes that any\n * locks that might be needed are taken by the caller.\n */\nstatic void fast_mix(struct fast_pool *f)\n{\n\t__u32 a = f->pool[0],\tb = f->pool[1];\n\t__u32 c = f->pool[2],\td = f->pool[3];\n\n\ta += b;\t\t\tc += d;\n\tb = rol32(b, 6);\td = rol32(d, 27);\n\td ^= a;\t\t\tb ^= c;\n\n\ta += b;\t\t\tc += d;\n\tb = rol32(b, 16);\td = rol32(d, 14);\n\td ^= a;\t\t\tb ^= c;\n\n\ta += b;\t\t\tc += d;\n\tb = rol32(b, 6);\td = rol32(d, 27);\n\td ^= a;\t\t\tb ^= c;\n\n\ta += b;\t\t\tc += d;\n\tb = rol32(b, 16);\td = rol32(d, 14);\n\td ^= a;\t\t\tb ^= c;\n\n\tf->pool[0] = a;  f->pool[1] = b;\n\tf->pool[2] = c;  f->pool[3] = d;\n\tf->count++;\n}\n\nstatic void process_random_ready_list(void)\n{\n\tunsigned long flags;\n\tstruct random_ready_callback *rdy, *tmp;\n\n\tspin_lock_irqsave(&random_ready_list_lock, flags);\n\tlist_for_each_entry_safe(rdy, tmp, &random_ready_list, list) {\n\t\tstruct module *owner = rdy->owner;\n\n\t\tlist_del_init(&rdy->list);\n\t\trdy->func(rdy);\n\t\tmodule_put(owner);\n\t}\n\tspin_unlock_irqrestore(&random_ready_list_lock, flags);\n}\n\n/*\n * Credit (or debit) the entropy store with n bits of entropy.\n * Use credit_entropy_bits_safe() if the value comes from userspace\n * or otherwise should be checked for extreme values.\n */\nstatic void credit_entropy_bits(struct entropy_store *r, int nbits)\n{\n\tint entropy_count, orig, has_initialized = 0;\n\tconst int pool_size = r->poolinfo->poolfracbits;\n\tint nfrac = nbits << ENTROPY_SHIFT;\n\n\tif (!nbits)\n\t\treturn;\n\nretry:\n\tentropy_count = orig = READ_ONCE(r->entropy_count);\n\tif (nfrac < 0) {\n\t\t/* Debit */\n\t\tentropy_count += nfrac;\n\t} else {\n\t\t/*\n\t\t * Credit: we have to account for the possibility of\n\t\t * overwriting already present entropy.\t Even in the\n\t\t * ideal case of pure Shannon entropy, new contributions\n\t\t * approach the full value asymptotically:\n\t\t *\n\t\t * entropy <- entropy + (pool_size - entropy) *\n\t\t *\t(1 - exp(-add_entropy/pool_size))\n\t\t *\n\t\t * For add_entropy <= pool_size/2 then\n\t\t * (1 - exp(-add_entropy/pool_size)) >=\n\t\t *    (add_entropy/pool_size)*0.7869...\n\t\t * so we can approximate the exponential with\n\t\t * 3/4*add_entropy/pool_size and still be on the\n\t\t * safe side by adding at most pool_size/2 at a time.\n\t\t *\n\t\t * The use of pool_size-2 in the while statement is to\n\t\t * prevent rounding artifacts from making the loop\n\t\t * arbitrarily long; this limits the loop to log2(pool_size)*2\n\t\t * turns no matter how large nbits is.\n\t\t */\n\t\tint pnfrac = nfrac;\n\t\tconst int s = r->poolinfo->poolbitshift + ENTROPY_SHIFT + 2;\n\t\t/* The +2 corresponds to the /4 in the denominator */\n\n\t\tdo {\n\t\t\tunsigned int anfrac = min(pnfrac, pool_size/2);\n\t\t\tunsigned int add =\n\t\t\t\t((pool_size - entropy_count)*anfrac*3) >> s;\n\n\t\t\tentropy_count += add;\n\t\t\tpnfrac -= anfrac;\n\t\t} while (unlikely(entropy_count < pool_size-2 && pnfrac));\n\t}\n\n\tif (WARN_ON(entropy_count < 0)) {\n\t\tpr_warn(\"negative entropy/overflow: pool %s count %d\\n\",\n\t\t\tr->name, entropy_count);\n\t\tentropy_count = 0;\n\t} else if (entropy_count > pool_size)\n\t\tentropy_count = pool_size;\n\tif (cmpxchg(&r->entropy_count, orig, entropy_count) != orig)\n\t\tgoto retry;\n\n\tif (has_initialized) {\n\t\tr->initialized = 1;\n\t\tkill_fasync(&fasync, SIGIO, POLL_IN);\n\t}\n\n\ttrace_credit_entropy_bits(r->name, nbits,\n\t\t\t\t  entropy_count >> ENTROPY_SHIFT, _RET_IP_);\n\n\tif (r == &input_pool) {\n\t\tint entropy_bits = entropy_count >> ENTROPY_SHIFT;\n\n\t\tif (crng_init < 2) {\n\t\t\tif (entropy_bits < 128)\n\t\t\t\treturn;\n\t\t\tcrng_reseed(&primary_crng, r);\n\t\t\tentropy_bits = ENTROPY_BITS(r);\n\t\t}\n\t}\n}\n\nstatic int credit_entropy_bits_safe(struct entropy_store *r, int nbits)\n{\n\tconst int nbits_max = r->poolinfo->poolwords * 32;\n\n\tif (nbits < 0)\n\t\treturn -EINVAL;\n\n\t/* Cap the value to avoid overflows */\n\tnbits = min(nbits,  nbits_max);\n\n\tcredit_entropy_bits(r, nbits);\n\treturn 0;\n}\n\n/*********************************************************************\n *\n * CRNG using CHACHA20\n *\n *********************************************************************/\n\n#define CRNG_RESEED_INTERVAL (300*HZ)\n\nstatic DECLARE_WAIT_QUEUE_HEAD(crng_init_wait);\n\n#ifdef CONFIG_NUMA\n/*\n * Hack to deal with crazy userspace progams when they are all trying\n * to access /dev/urandom in parallel.  The programs are almost\n * certainly doing something terribly wrong, but we'll work around\n * their brain damage.\n */\nstatic struct crng_state **crng_node_pool __read_mostly;\n#endif\n\nstatic void invalidate_batched_entropy(void);\nstatic void numa_crng_init(void);\n\nstatic bool trust_cpu __ro_after_init = IS_ENABLED(CONFIG_RANDOM_TRUST_CPU);\nstatic int __init parse_trust_cpu(char *arg)\n{\n\treturn kstrtobool(arg, &trust_cpu);\n}\nearly_param(\"random.trust_cpu\", parse_trust_cpu);\n\nstatic bool crng_init_try_arch(struct crng_state *crng)\n{\n\tint\t\ti;\n\tbool\t\tarch_init = true;\n\tunsigned long\trv;\n\n\tfor (i = 4; i < 16; i++) {\n\t\tif (!arch_get_random_seed_long(&rv) &&\n\t\t    !arch_get_random_long(&rv)) {\n\t\t\trv = random_get_entropy();\n\t\t\tarch_init = false;\n\t\t}\n\t\tcrng->state[i] ^= rv;\n\t}\n\n\treturn arch_init;\n}\n\nstatic bool __init crng_init_try_arch_early(struct crng_state *crng)\n{\n\tint\t\ti;\n\tbool\t\tarch_init = true;\n\tunsigned long\trv;\n\n\tfor (i = 4; i < 16; i++) {\n\t\tif (!arch_get_random_seed_long_early(&rv) &&\n\t\t    !arch_get_random_long_early(&rv)) {\n\t\t\trv = random_get_entropy();\n\t\t\tarch_init = false;\n\t\t}\n\t\tcrng->state[i] ^= rv;\n\t}\n\n\treturn arch_init;\n}\n\nstatic void __maybe_unused crng_initialize_secondary(struct crng_state *crng)\n{\n\tmemcpy(&crng->state[0], \"expand 32-byte k\", 16);\n\t_get_random_bytes(&crng->state[4], sizeof(__u32) * 12);\n\tcrng_init_try_arch(crng);\n\tcrng->init_time = jiffies - CRNG_RESEED_INTERVAL - 1;\n}\n\nstatic void __init crng_initialize_primary(struct crng_state *crng)\n{\n\tmemcpy(&crng->state[0], \"expand 32-byte k\", 16);\n\t_extract_entropy(&input_pool, &crng->state[4], sizeof(__u32) * 12, 0);\n\tif (crng_init_try_arch_early(crng) && trust_cpu) {\n\t\tinvalidate_batched_entropy();\n\t\tnuma_crng_init();\n\t\tcrng_init = 2;\n\t\tpr_notice(\"crng done (trusting CPU's manufacturer)\\n\");\n\t}\n\tcrng->init_time = jiffies - CRNG_RESEED_INTERVAL - 1;\n}\n\n#ifdef CONFIG_NUMA\nstatic void do_numa_crng_init(struct work_struct *work)\n{\n\tint i;\n\tstruct crng_state *crng;\n\tstruct crng_state **pool;\n\n\tpool = kcalloc(nr_node_ids, sizeof(*pool), GFP_KERNEL|__GFP_NOFAIL);\n\tfor_each_online_node(i) {\n\t\tcrng = kmalloc_node(sizeof(struct crng_state),\n\t\t\t\t    GFP_KERNEL | __GFP_NOFAIL, i);\n\t\tspin_lock_init(&crng->lock);\n\t\tcrng_initialize_secondary(crng);\n\t\tpool[i] = crng;\n\t}\n\tmb();\n\tif (cmpxchg(&crng_node_pool, NULL, pool)) {\n\t\tfor_each_node(i)\n\t\t\tkfree(pool[i]);\n\t\tkfree(pool);\n\t}\n}\n\nstatic DECLARE_WORK(numa_crng_init_work, do_numa_crng_init);\n\nstatic void numa_crng_init(void)\n{\n\tschedule_work(&numa_crng_init_work);\n}\n#else\nstatic void numa_crng_init(void) {}\n#endif\n\n/*\n * crng_fast_load() can be called by code in the interrupt service\n * path.  So we can't afford to dilly-dally.\n */\nstatic int crng_fast_load(const char *cp, size_t len)\n{\n\tunsigned long flags;\n\tchar *p;\n\n\tif (!spin_trylock_irqsave(&primary_crng.lock, flags))\n\t\treturn 0;\n\tif (crng_init != 0) {\n\t\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\t\treturn 0;\n\t}\n\tp = (unsigned char *) &primary_crng.state[4];\n\twhile (len > 0 && crng_init_cnt < CRNG_INIT_CNT_THRESH) {\n\t\tp[crng_init_cnt % CHACHA_KEY_SIZE] ^= *cp;\n\t\tcp++; crng_init_cnt++; len--;\n\t}\n\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\tif (crng_init_cnt >= CRNG_INIT_CNT_THRESH) {\n\t\tinvalidate_batched_entropy();\n\t\tcrng_init = 1;\n\t\tpr_notice(\"fast init done\\n\");\n\t}\n\treturn 1;\n}\n\n/*\n * crng_slow_load() is called by add_device_randomness, which has two\n * attributes.  (1) We can't trust the buffer passed to it is\n * guaranteed to be unpredictable (so it might not have any entropy at\n * all), and (2) it doesn't have the performance constraints of\n * crng_fast_load().\n *\n * So we do something more comprehensive which is guaranteed to touch\n * all of the primary_crng's state, and which uses a LFSR with a\n * period of 255 as part of the mixing algorithm.  Finally, we do\n * *not* advance crng_init_cnt since buffer we may get may be something\n * like a fixed DMI table (for example), which might very well be\n * unique to the machine, but is otherwise unvarying.\n */\nstatic int crng_slow_load(const char *cp, size_t len)\n{\n\tunsigned long\t\tflags;\n\tstatic unsigned char\tlfsr = 1;\n\tunsigned char\t\ttmp;\n\tunsigned\t\ti, max = CHACHA_KEY_SIZE;\n\tconst char *\t\tsrc_buf = cp;\n\tchar *\t\t\tdest_buf = (char *) &primary_crng.state[4];\n\n\tif (!spin_trylock_irqsave(&primary_crng.lock, flags))\n\t\treturn 0;\n\tif (crng_init != 0) {\n\t\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\t\treturn 0;\n\t}\n\tif (len > max)\n\t\tmax = len;\n\n\tfor (i = 0; i < max ; i++) {\n\t\ttmp = lfsr;\n\t\tlfsr >>= 1;\n\t\tif (tmp & 1)\n\t\t\tlfsr ^= 0xE1;\n\t\ttmp = dest_buf[i % CHACHA_KEY_SIZE];\n\t\tdest_buf[i % CHACHA_KEY_SIZE] ^= src_buf[i % len] ^ lfsr;\n\t\tlfsr += (tmp << 3) | (tmp >> 5);\n\t}\n\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\treturn 1;\n}\n\nstatic void crng_reseed(struct crng_state *crng, struct entropy_store *r)\n{\n\tunsigned long\tflags;\n\tint\t\ti, num;\n\tunion {\n\t\t__u8\tblock[CHACHA_BLOCK_SIZE];\n\t\t__u32\tkey[8];\n\t} buf;\n\n\tif (r) {\n\t\tnum = extract_entropy(r, &buf, 32, 16, 0);\n\t\tif (num == 0)\n\t\t\treturn;\n\t} else {\n\t\t_extract_crng(&primary_crng, buf.block);\n\t\t_crng_backtrack_protect(&primary_crng, buf.block,\n\t\t\t\t\tCHACHA_KEY_SIZE);\n\t}\n\tspin_lock_irqsave(&crng->lock, flags);\n\tfor (i = 0; i < 8; i++) {\n\t\tunsigned long\trv;\n\t\tif (!arch_get_random_seed_long(&rv) &&\n\t\t    !arch_get_random_long(&rv))\n\t\t\trv = random_get_entropy();\n\t\tcrng->state[i+4] ^= buf.key[i] ^ rv;\n\t}\n\tmemzero_explicit(&buf, sizeof(buf));\n\tcrng->init_time = jiffies;\n\tspin_unlock_irqrestore(&crng->lock, flags);\n\tif (crng == &primary_crng && crng_init < 2) {\n\t\tinvalidate_batched_entropy();\n\t\tnuma_crng_init();\n\t\tcrng_init = 2;\n\t\tprocess_random_ready_list();\n\t\twake_up_interruptible(&crng_init_wait);\n\t\tkill_fasync(&fasync, SIGIO, POLL_IN);\n\t\tpr_notice(\"crng init done\\n\");\n\t\tif (unseeded_warning.missed) {\n\t\t\tpr_notice(\"%d get_random_xx warning(s) missed due to ratelimiting\\n\",\n\t\t\t\t  unseeded_warning.missed);\n\t\t\tunseeded_warning.missed = 0;\n\t\t}\n\t\tif (urandom_warning.missed) {\n\t\t\tpr_notice(\"%d urandom warning(s) missed due to ratelimiting\\n\",\n\t\t\t\t  urandom_warning.missed);\n\t\t\turandom_warning.missed = 0;\n\t\t}\n\t}\n}\n\nstatic void _extract_crng(struct crng_state *crng,\n\t\t\t  __u8 out[CHACHA_BLOCK_SIZE])\n{\n\tunsigned long v, flags;\n\n\tif (crng_ready() &&\n\t    (time_after(crng_global_init_time, crng->init_time) ||\n\t     time_after(jiffies, crng->init_time + CRNG_RESEED_INTERVAL)))\n\t\tcrng_reseed(crng, crng == &primary_crng ? &input_pool : NULL);\n\tspin_lock_irqsave(&crng->lock, flags);\n\tif (arch_get_random_long(&v))\n\t\tcrng->state[14] ^= v;\n\tchacha20_block(&crng->state[0], out);\n\tif (crng->state[12] == 0)\n\t\tcrng->state[13]++;\n\tspin_unlock_irqrestore(&crng->lock, flags);\n}\n\nstatic void extract_crng(__u8 out[CHACHA_BLOCK_SIZE])\n{\n\tstruct crng_state *crng = NULL;\n\n#ifdef CONFIG_NUMA\n\tif (crng_node_pool)\n\t\tcrng = crng_node_pool[numa_node_id()];\n\tif (crng == NULL)\n#endif\n\t\tcrng = &primary_crng;\n\t_extract_crng(crng, out);\n}\n\n/*\n * Use the leftover bytes from the CRNG block output (if there is\n * enough) to mutate the CRNG key to provide backtracking protection.\n */\nstatic void _crng_backtrack_protect(struct crng_state *crng,\n\t\t\t\t    __u8 tmp[CHACHA_BLOCK_SIZE], int used)\n{\n\tunsigned long\tflags;\n\t__u32\t\t*s, *d;\n\tint\t\ti;\n\n\tused = round_up(used, sizeof(__u32));\n\tif (used + CHACHA_KEY_SIZE > CHACHA_BLOCK_SIZE) {\n\t\textract_crng(tmp);\n\t\tused = 0;\n\t}\n\tspin_lock_irqsave(&crng->lock, flags);\n\ts = (__u32 *) &tmp[used];\n\td = &crng->state[4];\n\tfor (i=0; i < 8; i++)\n\t\t*d++ ^= *s++;\n\tspin_unlock_irqrestore(&crng->lock, flags);\n}\n\nstatic void crng_backtrack_protect(__u8 tmp[CHACHA_BLOCK_SIZE], int used)\n{\n\tstruct crng_state *crng = NULL;\n\n#ifdef CONFIG_NUMA\n\tif (crng_node_pool)\n\t\tcrng = crng_node_pool[numa_node_id()];\n\tif (crng == NULL)\n#endif\n\t\tcrng = &primary_crng;\n\t_crng_backtrack_protect(crng, tmp, used);\n}\n\nstatic ssize_t extract_crng_user(void __user *buf, size_t nbytes)\n{\n\tssize_t ret = 0, i = CHACHA_BLOCK_SIZE;\n\t__u8 tmp[CHACHA_BLOCK_SIZE] __aligned(4);\n\tint large_request = (nbytes > 256);\n\n\twhile (nbytes) {\n\t\tif (large_request && need_resched()) {\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tschedule();\n\t\t}\n\n\t\textract_crng(tmp);\n\t\ti = min_t(int, nbytes, CHACHA_BLOCK_SIZE);\n\t\tif (copy_to_user(buf, tmp, i)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tnbytes -= i;\n\t\tbuf += i;\n\t\tret += i;\n\t}\n\tcrng_backtrack_protect(tmp, i);\n\n\t/* Wipe data just written to memory */\n\tmemzero_explicit(tmp, sizeof(tmp));\n\n\treturn ret;\n}\n\n\n/*********************************************************************\n *\n * Entropy input management\n *\n *********************************************************************/\n\n/* There is one of these per entropy source */\nstruct timer_rand_state {\n\tcycles_t last_time;\n\tlong last_delta, last_delta2;\n};\n\n#define INIT_TIMER_RAND_STATE { INITIAL_JIFFIES, };\n\n/*\n * Add device- or boot-specific data to the input pool to help\n * initialize it.\n *\n * None of this adds any entropy; it is meant to avoid the problem of\n * the entropy pool having similar initial state across largely\n * identical devices.\n */\nvoid add_device_randomness(const void *buf, unsigned int size)\n{\n\tunsigned long time = random_get_entropy() ^ jiffies;\n\tunsigned long flags;\n\n\tif (!crng_ready() && size)\n\t\tcrng_slow_load(buf, size);\n\n\ttrace_add_device_randomness(size, _RET_IP_);\n\tspin_lock_irqsave(&input_pool.lock, flags);\n\t_mix_pool_bytes(&input_pool, buf, size);\n\t_mix_pool_bytes(&input_pool, &time, sizeof(time));\n\tspin_unlock_irqrestore(&input_pool.lock, flags);\n}\nEXPORT_SYMBOL(add_device_randomness);\n\nstatic struct timer_rand_state input_timer_state = INIT_TIMER_RAND_STATE;\n\n/*\n * This function adds entropy to the entropy \"pool\" by using timing\n * delays.  It uses the timer_rand_state structure to make an estimate\n * of how many bits of entropy this call has added to the pool.\n *\n * The number \"num\" is also added to the pool - it should somehow describe\n * the type of event which just happened.  This is currently 0-255 for\n * keyboard scan codes, and 256 upwards for interrupts.\n *\n */\nstatic void add_timer_randomness(struct timer_rand_state *state, unsigned num)\n{\n\tstruct entropy_store\t*r;\n\tstruct {\n\t\tlong jiffies;\n\t\tunsigned cycles;\n\t\tunsigned num;\n\t} sample;\n\tlong delta, delta2, delta3;\n\n\tsample.jiffies = jiffies;\n\tsample.cycles = random_get_entropy();\n\tsample.num = num;\n\tr = &input_pool;\n\tmix_pool_bytes(r, &sample, sizeof(sample));\n\n\t/*\n\t * Calculate number of bits of randomness we probably added.\n\t * We take into account the first, second and third-order deltas\n\t * in order to make our estimate.\n\t */\n\tdelta = sample.jiffies - READ_ONCE(state->last_time);\n\tWRITE_ONCE(state->last_time, sample.jiffies);\n\n\tdelta2 = delta - READ_ONCE(state->last_delta);\n\tWRITE_ONCE(state->last_delta, delta);\n\n\tdelta3 = delta2 - READ_ONCE(state->last_delta2);\n\tWRITE_ONCE(state->last_delta2, delta2);\n\n\tif (delta < 0)\n\t\tdelta = -delta;\n\tif (delta2 < 0)\n\t\tdelta2 = -delta2;\n\tif (delta3 < 0)\n\t\tdelta3 = -delta3;\n\tif (delta > delta2)\n\t\tdelta = delta2;\n\tif (delta > delta3)\n\t\tdelta = delta3;\n\n\t/*\n\t * delta is now minimum absolute delta.\n\t * Round down by 1 bit on general principles,\n\t * and limit entropy estimate to 12 bits.\n\t */\n\tcredit_entropy_bits(r, min_t(int, fls(delta>>1), 11));\n}\n\nvoid add_input_randomness(unsigned int type, unsigned int code,\n\t\t\t\t unsigned int value)\n{\n\tstatic unsigned char last_value;\n\n\t/* ignore autorepeat and the like */\n\tif (value == last_value)\n\t\treturn;\n\n\tlast_value = value;\n\tadd_timer_randomness(&input_timer_state,\n\t\t\t     (type << 4) ^ code ^ (code >> 4) ^ value);\n\ttrace_add_input_randomness(ENTROPY_BITS(&input_pool));\n}\nEXPORT_SYMBOL_GPL(add_input_randomness);\n\nstatic DEFINE_PER_CPU(struct fast_pool, irq_randomness);\n\n#ifdef ADD_INTERRUPT_BENCH\nstatic unsigned long avg_cycles, avg_deviation;\n\n#define AVG_SHIFT 8     /* Exponential average factor k=1/256 */\n#define FIXED_1_2 (1 << (AVG_SHIFT-1))\n\nstatic void add_interrupt_bench(cycles_t start)\n{\n        long delta = random_get_entropy() - start;\n\n        /* Use a weighted moving average */\n        delta = delta - ((avg_cycles + FIXED_1_2) >> AVG_SHIFT);\n        avg_cycles += delta;\n        /* And average deviation */\n        delta = abs(delta) - ((avg_deviation + FIXED_1_2) >> AVG_SHIFT);\n        avg_deviation += delta;\n}\n#else\n#define add_interrupt_bench(x)\n#endif\n\nstatic __u32 get_reg(struct fast_pool *f, struct pt_regs *regs)\n{\n\t__u32 *ptr = (__u32 *) regs;\n\tunsigned int idx;\n\n\tif (regs == NULL)\n\t\treturn 0;\n\tidx = READ_ONCE(f->reg_idx);\n\tif (idx >= sizeof(struct pt_regs) / sizeof(__u32))\n\t\tidx = 0;\n\tptr += idx++;\n\tWRITE_ONCE(f->reg_idx, idx);\n\treturn *ptr;\n}\n\nvoid add_interrupt_randomness(int irq, int irq_flags)\n{\n\tstruct entropy_store\t*r;\n\tstruct fast_pool\t*fast_pool = this_cpu_ptr(&irq_randomness);\n\tstruct pt_regs\t\t*regs = get_irq_regs();\n\tunsigned long\t\tnow = jiffies;\n\tcycles_t\t\tcycles = random_get_entropy();\n\t__u32\t\t\tc_high, j_high;\n\t__u64\t\t\tip;\n\tunsigned long\t\tseed;\n\tint\t\t\tcredit = 0;\n\n\tif (cycles == 0)\n\t\tcycles = get_reg(fast_pool, regs);\n\tc_high = (sizeof(cycles) > 4) ? cycles >> 32 : 0;\n\tj_high = (sizeof(now) > 4) ? now >> 32 : 0;\n\tfast_pool->pool[0] ^= cycles ^ j_high ^ irq;\n\tfast_pool->pool[1] ^= now ^ c_high;\n\tip = regs ? instruction_pointer(regs) : _RET_IP_;\n\tfast_pool->pool[2] ^= ip;\n\tfast_pool->pool[3] ^= (sizeof(ip) > 4) ? ip >> 32 :\n\t\tget_reg(fast_pool, regs);\n\n\tfast_mix(fast_pool);\n\tadd_interrupt_bench(cycles);\n\n\tif (unlikely(crng_init == 0)) {\n\t\tif ((fast_pool->count >= 64) &&\n\t\t    crng_fast_load((char *) fast_pool->pool,\n\t\t\t\t   sizeof(fast_pool->pool))) {\n\t\t\tfast_pool->count = 0;\n\t\t\tfast_pool->last = now;\n\t\t}\n\t\treturn;\n\t}\n\n\tif ((fast_pool->count < 64) &&\n\t    !time_after(now, fast_pool->last + HZ))\n\t\treturn;\n\n\tr = &input_pool;\n\tif (!spin_trylock(&r->lock))\n\t\treturn;\n\n\tfast_pool->last = now;\n\t__mix_pool_bytes(r, &fast_pool->pool, sizeof(fast_pool->pool));\n\n\t/*\n\t * If we have architectural seed generator, produce a seed and\n\t * add it to the pool.  For the sake of paranoia don't let the\n\t * architectural seed generator dominate the input from the\n\t * interrupt noise.\n\t */\n\tif (arch_get_random_seed_long(&seed)) {\n\t\t__mix_pool_bytes(r, &seed, sizeof(seed));\n\t\tcredit = 1;\n\t}\n\tspin_unlock(&r->lock);\n\n\tfast_pool->count = 0;\n\n\t/* award one bit for the contents of the fast pool */\n\tcredit_entropy_bits(r, credit + 1);\n}\nEXPORT_SYMBOL_GPL(add_interrupt_randomness);\n\n#ifdef CONFIG_BLOCK\nvoid add_disk_randomness(struct gendisk *disk)\n{\n\tif (!disk || !disk->random)\n\t\treturn;\n\t/* first major is 1, so we get >= 0x200 here */\n\tadd_timer_randomness(disk->random, 0x100 + disk_devt(disk));\n\ttrace_add_disk_randomness(disk_devt(disk), ENTROPY_BITS(&input_pool));\n}\nEXPORT_SYMBOL_GPL(add_disk_randomness);\n#endif\n\n/*********************************************************************\n *\n * Entropy extraction routines\n *\n *********************************************************************/\n\n/*\n * This function decides how many bytes to actually take from the\n * given pool, and also debits the entropy count accordingly.\n */\nstatic size_t account(struct entropy_store *r, size_t nbytes, int min,\n\t\t      int reserved)\n{\n\tint entropy_count, orig, have_bytes;\n\tsize_t ibytes, nfrac;\n\n\tBUG_ON(r->entropy_count > r->poolinfo->poolfracbits);\n\n\t/* Can we pull enough? */\nretry:\n\tentropy_count = orig = READ_ONCE(r->entropy_count);\n\tibytes = nbytes;\n\t/* never pull more than available */\n\thave_bytes = entropy_count >> (ENTROPY_SHIFT + 3);\n\n\tif ((have_bytes -= reserved) < 0)\n\t\thave_bytes = 0;\n\tibytes = min_t(size_t, ibytes, have_bytes);\n\tif (ibytes < min)\n\t\tibytes = 0;\n\n\tif (WARN_ON(entropy_count < 0)) {\n\t\tpr_warn(\"negative entropy count: pool %s count %d\\n\",\n\t\t\tr->name, entropy_count);\n\t\tentropy_count = 0;\n\t}\n\tnfrac = ibytes << (ENTROPY_SHIFT + 3);\n\tif ((size_t) entropy_count > nfrac)\n\t\tentropy_count -= nfrac;\n\telse\n\t\tentropy_count = 0;\n\n\tif (cmpxchg(&r->entropy_count, orig, entropy_count) != orig)\n\t\tgoto retry;\n\n\ttrace_debit_entropy(r->name, 8 * ibytes);\n\tif (ibytes && ENTROPY_BITS(r) < random_write_wakeup_bits) {\n\t\twake_up_interruptible(&random_write_wait);\n\t\tkill_fasync(&fasync, SIGIO, POLL_OUT);\n\t}\n\n\treturn ibytes;\n}\n\n/*\n * This function does the actual extraction for extract_entropy and\n * extract_entropy_user.\n *\n * Note: we assume that .poolwords is a multiple of 16 words.\n */\nstatic void extract_buf(struct entropy_store *r, __u8 *out)\n{\n\tint i;\n\tunion {\n\t\t__u32 w[5];\n\t\tunsigned long l[LONGS(20)];\n\t} hash;\n\t__u32 workspace[SHA1_WORKSPACE_WORDS];\n\tunsigned long flags;\n\n\t/*\n\t * If we have an architectural hardware random number\n\t * generator, use it for SHA's initial vector\n\t */\n\tsha1_init(hash.w);\n\tfor (i = 0; i < LONGS(20); i++) {\n\t\tunsigned long v;\n\t\tif (!arch_get_random_long(&v))\n\t\t\tbreak;\n\t\thash.l[i] = v;\n\t}\n\n\t/* Generate a hash across the pool, 16 words (512 bits) at a time */\n\tspin_lock_irqsave(&r->lock, flags);\n\tfor (i = 0; i < r->poolinfo->poolwords; i += 16)\n\t\tsha1_transform(hash.w, (__u8 *)(r->pool + i), workspace);\n\n\t/*\n\t * We mix the hash back into the pool to prevent backtracking\n\t * attacks (where the attacker knows the state of the pool\n\t * plus the current outputs, and attempts to find previous\n\t * ouputs), unless the hash function can be inverted. By\n\t * mixing at least a SHA1 worth of hash data back, we make\n\t * brute-forcing the feedback as hard as brute-forcing the\n\t * hash.\n\t */\n\t__mix_pool_bytes(r, hash.w, sizeof(hash.w));\n\tspin_unlock_irqrestore(&r->lock, flags);\n\n\tmemzero_explicit(workspace, sizeof(workspace));\n\n\t/*\n\t * In case the hash function has some recognizable output\n\t * pattern, we fold it in half. Thus, we always feed back\n\t * twice as much data as we output.\n\t */\n\thash.w[0] ^= hash.w[3];\n\thash.w[1] ^= hash.w[4];\n\thash.w[2] ^= rol32(hash.w[2], 16);\n\n\tmemcpy(out, &hash, EXTRACT_SIZE);\n\tmemzero_explicit(&hash, sizeof(hash));\n}\n\nstatic ssize_t _extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t\tsize_t nbytes, int fips)\n{\n\tssize_t ret = 0, i;\n\t__u8 tmp[EXTRACT_SIZE];\n\tunsigned long flags;\n\n\twhile (nbytes) {\n\t\textract_buf(r, tmp);\n\n\t\tif (fips) {\n\t\t\tspin_lock_irqsave(&r->lock, flags);\n\t\t\tif (!memcmp(tmp, r->last_data, EXTRACT_SIZE))\n\t\t\t\tpanic(\"Hardware RNG duplicated output!\\n\");\n\t\t\tmemcpy(r->last_data, tmp, EXTRACT_SIZE);\n\t\t\tspin_unlock_irqrestore(&r->lock, flags);\n\t\t}\n\t\ti = min_t(int, nbytes, EXTRACT_SIZE);\n\t\tmemcpy(buf, tmp, i);\n\t\tnbytes -= i;\n\t\tbuf += i;\n\t\tret += i;\n\t}\n\n\t/* Wipe data just returned from memory */\n\tmemzero_explicit(tmp, sizeof(tmp));\n\n\treturn ret;\n}\n\n/*\n * This function extracts randomness from the \"entropy pool\", and\n * returns it in a buffer.\n *\n * The min parameter specifies the minimum amount we can pull before\n * failing to avoid races that defeat catastrophic reseeding while the\n * reserved parameter indicates how much entropy we must leave in the\n * pool after each pull to avoid starving other readers.\n */\nstatic ssize_t extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t\t size_t nbytes, int min, int reserved)\n{\n\t__u8 tmp[EXTRACT_SIZE];\n\tunsigned long flags;\n\n\t/* if last_data isn't primed, we need EXTRACT_SIZE extra bytes */\n\tif (fips_enabled) {\n\t\tspin_lock_irqsave(&r->lock, flags);\n\t\tif (!r->last_data_init) {\n\t\t\tr->last_data_init = 1;\n\t\t\tspin_unlock_irqrestore(&r->lock, flags);\n\t\t\ttrace_extract_entropy(r->name, EXTRACT_SIZE,\n\t\t\t\t\t      ENTROPY_BITS(r), _RET_IP_);\n\t\t\textract_buf(r, tmp);\n\t\t\tspin_lock_irqsave(&r->lock, flags);\n\t\t\tmemcpy(r->last_data, tmp, EXTRACT_SIZE);\n\t\t}\n\t\tspin_unlock_irqrestore(&r->lock, flags);\n\t}\n\n\ttrace_extract_entropy(r->name, nbytes, ENTROPY_BITS(r), _RET_IP_);\n\tnbytes = account(r, nbytes, min, reserved);\n\n\treturn _extract_entropy(r, buf, nbytes, fips_enabled);\n}\n\n#define warn_unseeded_randomness(previous) \\\n\t_warn_unseeded_randomness(__func__, (void *) _RET_IP_, (previous))\n\nstatic void _warn_unseeded_randomness(const char *func_name, void *caller,\n\t\t\t\t      void **previous)\n{\n#ifdef CONFIG_WARN_ALL_UNSEEDED_RANDOM\n\tconst bool print_once = false;\n#else\n\tstatic bool print_once __read_mostly;\n#endif\n\n\tif (print_once ||\n\t    crng_ready() ||\n\t    (previous && (caller == READ_ONCE(*previous))))\n\t\treturn;\n\tWRITE_ONCE(*previous, caller);\n#ifndef CONFIG_WARN_ALL_UNSEEDED_RANDOM\n\tprint_once = true;\n#endif\n\tif (__ratelimit(&unseeded_warning))\n\t\tprintk_deferred(KERN_NOTICE \"random: %s called from %pS \"\n\t\t\t\t\"with crng_init=%d\\n\", func_name, caller,\n\t\t\t\tcrng_init);\n}\n\n/*\n * This function is the exported kernel interface.  It returns some\n * number of good random numbers, suitable for key generation, seeding\n * TCP sequence numbers, etc.  It does not rely on the hardware random\n * number generator.  For random bytes direct from the hardware RNG\n * (when available), use get_random_bytes_arch(). In order to ensure\n * that the randomness provided by this function is okay, the function\n * wait_for_random_bytes() should be called and return 0 at least once\n * at any point prior.\n */\nstatic void _get_random_bytes(void *buf, int nbytes)\n{\n\t__u8 tmp[CHACHA_BLOCK_SIZE] __aligned(4);\n\n\ttrace_get_random_bytes(nbytes, _RET_IP_);\n\n\twhile (nbytes >= CHACHA_BLOCK_SIZE) {\n\t\textract_crng(buf);\n\t\tbuf += CHACHA_BLOCK_SIZE;\n\t\tnbytes -= CHACHA_BLOCK_SIZE;\n\t}\n\n\tif (nbytes > 0) {\n\t\textract_crng(tmp);\n\t\tmemcpy(buf, tmp, nbytes);\n\t\tcrng_backtrack_protect(tmp, nbytes);\n\t} else\n\t\tcrng_backtrack_protect(tmp, CHACHA_BLOCK_SIZE);\n\tmemzero_explicit(tmp, sizeof(tmp));\n}\n\nvoid get_random_bytes(void *buf, int nbytes)\n{\n\tstatic void *previous;\n\n\twarn_unseeded_randomness(&previous);\n\t_get_random_bytes(buf, nbytes);\n}\nEXPORT_SYMBOL(get_random_bytes);\n\n\n/*\n * Each time the timer fires, we expect that we got an unpredictable\n * jump in the cycle counter. Even if the timer is running on another\n * CPU, the timer activity will be touching the stack of the CPU that is\n * generating entropy..\n *\n * Note that we don't re-arm the timer in the timer itself - we are\n * happy to be scheduled away, since that just makes the load more\n * complex, but we do not want the timer to keep ticking unless the\n * entropy loop is running.\n *\n * So the re-arming always happens in the entropy loop itself.\n */\nstatic void entropy_timer(struct timer_list *t)\n{\n\tcredit_entropy_bits(&input_pool, 1);\n}\n\n/*\n * If we have an actual cycle counter, see if we can\n * generate enough entropy with timing noise\n */\nstatic void try_to_generate_entropy(void)\n{\n\tstruct {\n\t\tunsigned long now;\n\t\tstruct timer_list timer;\n\t} stack;\n\n\tstack.now = random_get_entropy();\n\n\t/* Slow counter - or none. Don't even bother */\n\tif (stack.now == random_get_entropy())\n\t\treturn;\n\n\ttimer_setup_on_stack(&stack.timer, entropy_timer, 0);\n\twhile (!crng_ready()) {\n\t\tif (!timer_pending(&stack.timer))\n\t\t\tmod_timer(&stack.timer, jiffies+1);\n\t\tmix_pool_bytes(&input_pool, &stack.now, sizeof(stack.now));\n\t\tschedule();\n\t\tstack.now = random_get_entropy();\n\t}\n\n\tdel_timer_sync(&stack.timer);\n\tdestroy_timer_on_stack(&stack.timer);\n\tmix_pool_bytes(&input_pool, &stack.now, sizeof(stack.now));\n}\n\n/*\n * Wait for the urandom pool to be seeded and thus guaranteed to supply\n * cryptographically secure random numbers. This applies to: the /dev/urandom\n * device, the get_random_bytes function, and the get_random_{u32,u64,int,long}\n * family of functions. Using any of these functions without first calling\n * this function forfeits the guarantee of security.\n *\n * Returns: 0 if the urandom pool has been seeded.\n *          -ERESTARTSYS if the function was interrupted by a signal.\n */\nint wait_for_random_bytes(void)\n{\n\tif (likely(crng_ready()))\n\t\treturn 0;\n\n\tdo {\n\t\tint ret;\n\t\tret = wait_event_interruptible_timeout(crng_init_wait, crng_ready(), HZ);\n\t\tif (ret)\n\t\t\treturn ret > 0 ? 0 : ret;\n\n\t\ttry_to_generate_entropy();\n\t} while (!crng_ready());\n\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_random_bytes);\n\n/*\n * Returns whether or not the urandom pool has been seeded and thus guaranteed\n * to supply cryptographically secure random numbers. This applies to: the\n * /dev/urandom device, the get_random_bytes function, and the get_random_{u32,\n * ,u64,int,long} family of functions.\n *\n * Returns: true if the urandom pool has been seeded.\n *          false if the urandom pool has not been seeded.\n */\nbool rng_is_initialized(void)\n{\n\treturn crng_ready();\n}\nEXPORT_SYMBOL(rng_is_initialized);\n\n/*\n * Add a callback function that will be invoked when the nonblocking\n * pool is initialised.\n *\n * returns: 0 if callback is successfully added\n *\t    -EALREADY if pool is already initialised (callback not called)\n *\t    -ENOENT if module for callback is not alive\n */\nint add_random_ready_callback(struct random_ready_callback *rdy)\n{\n\tstruct module *owner;\n\tunsigned long flags;\n\tint err = -EALREADY;\n\n\tif (crng_ready())\n\t\treturn err;\n\n\towner = rdy->owner;\n\tif (!try_module_get(owner))\n\t\treturn -ENOENT;\n\n\tspin_lock_irqsave(&random_ready_list_lock, flags);\n\tif (crng_ready())\n\t\tgoto out;\n\n\towner = NULL;\n\n\tlist_add(&rdy->list, &random_ready_list);\n\terr = 0;\n\nout:\n\tspin_unlock_irqrestore(&random_ready_list_lock, flags);\n\n\tmodule_put(owner);\n\n\treturn err;\n}\nEXPORT_SYMBOL(add_random_ready_callback);\n\n/*\n * Delete a previously registered readiness callback function.\n */\nvoid del_random_ready_callback(struct random_ready_callback *rdy)\n{\n\tunsigned long flags;\n\tstruct module *owner = NULL;\n\n\tspin_lock_irqsave(&random_ready_list_lock, flags);\n\tif (!list_empty(&rdy->list)) {\n\t\tlist_del_init(&rdy->list);\n\t\towner = rdy->owner;\n\t}\n\tspin_unlock_irqrestore(&random_ready_list_lock, flags);\n\n\tmodule_put(owner);\n}\nEXPORT_SYMBOL(del_random_ready_callback);\n\n/*\n * This function will use the architecture-specific hardware random\n * number generator if it is available.  The arch-specific hw RNG will\n * almost certainly be faster than what we can do in software, but it\n * is impossible to verify that it is implemented securely (as\n * opposed, to, say, the AES encryption of a sequence number using a\n * key known by the NSA).  So it's useful if we need the speed, but\n * only if we're willing to trust the hardware manufacturer not to\n * have put in a back door.\n *\n * Return number of bytes filled in.\n */\nint __must_check get_random_bytes_arch(void *buf, int nbytes)\n{\n\tint left = nbytes;\n\tchar *p = buf;\n\n\ttrace_get_random_bytes_arch(left, _RET_IP_);\n\twhile (left) {\n\t\tunsigned long v;\n\t\tint chunk = min_t(int, left, sizeof(unsigned long));\n\n\t\tif (!arch_get_random_long(&v))\n\t\t\tbreak;\n\n\t\tmemcpy(p, &v, chunk);\n\t\tp += chunk;\n\t\tleft -= chunk;\n\t}\n\n\treturn nbytes - left;\n}\nEXPORT_SYMBOL(get_random_bytes_arch);\n\n/*\n * init_std_data - initialize pool with system data\n *\n * @r: pool to initialize\n *\n * This function clears the pool's entropy count and mixes some system\n * data into the pool to prepare it for use. The pool is not cleared\n * as that can only decrease the entropy in the pool.\n */\nstatic void __init init_std_data(struct entropy_store *r)\n{\n\tint i;\n\tktime_t now = ktime_get_real();\n\tunsigned long rv;\n\n\tmix_pool_bytes(r, &now, sizeof(now));\n\tfor (i = r->poolinfo->poolbytes; i > 0; i -= sizeof(rv)) {\n\t\tif (!arch_get_random_seed_long(&rv) &&\n\t\t    !arch_get_random_long(&rv))\n\t\t\trv = random_get_entropy();\n\t\tmix_pool_bytes(r, &rv, sizeof(rv));\n\t}\n\tmix_pool_bytes(r, utsname(), sizeof(*(utsname())));\n}\n\n/*\n * Note that setup_arch() may call add_device_randomness()\n * long before we get here. This allows seeding of the pools\n * with some platform dependent data very early in the boot\n * process. But it limits our options here. We must use\n * statically allocated structures that already have all\n * initializations complete at compile time. We should also\n * take care not to overwrite the precious per platform data\n * we were given.\n */\nint __init rand_initialize(void)\n{\n\tinit_std_data(&input_pool);\n\tcrng_initialize_primary(&primary_crng);\n\tcrng_global_init_time = jiffies;\n\tif (ratelimit_disable) {\n\t\turandom_warning.interval = 0;\n\t\tunseeded_warning.interval = 0;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_BLOCK\nvoid rand_initialize_disk(struct gendisk *disk)\n{\n\tstruct timer_rand_state *state;\n\n\t/*\n\t * If kzalloc returns null, we just won't use that entropy\n\t * source.\n\t */\n\tstate = kzalloc(sizeof(struct timer_rand_state), GFP_KERNEL);\n\tif (state) {\n\t\tstate->last_time = INITIAL_JIFFIES;\n\t\tdisk->random = state;\n\t}\n}\n#endif\n\nstatic ssize_t\nurandom_read_nowarn(struct file *file, char __user *buf, size_t nbytes,\n\t\t    loff_t *ppos)\n{\n\tint ret;\n\n\tnbytes = min_t(size_t, nbytes, INT_MAX >> (ENTROPY_SHIFT + 3));\n\tret = extract_crng_user(buf, nbytes);\n\ttrace_urandom_read(8 * nbytes, 0, ENTROPY_BITS(&input_pool));\n\treturn ret;\n}\n\nstatic ssize_t\nurandom_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)\n{\n\tunsigned long flags;\n\tstatic int maxwarn = 10;\n\n\tif (!crng_ready() && maxwarn > 0) {\n\t\tmaxwarn--;\n\t\tif (__ratelimit(&urandom_warning))\n\t\t\tpr_notice(\"%s: uninitialized urandom read (%zd bytes read)\\n\",\n\t\t\t\t  current->comm, nbytes);\n\t\tspin_lock_irqsave(&primary_crng.lock, flags);\n\t\tcrng_init_cnt = 0;\n\t\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\t}\n\n\treturn urandom_read_nowarn(file, buf, nbytes, ppos);\n}\n\nstatic ssize_t\nrandom_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)\n{\n\tint ret;\n\n\tret = wait_for_random_bytes();\n\tif (ret != 0)\n\t\treturn ret;\n\treturn urandom_read_nowarn(file, buf, nbytes, ppos);\n}\n\nstatic __poll_t\nrandom_poll(struct file *file, poll_table * wait)\n{\n\t__poll_t mask;\n\n\tpoll_wait(file, &crng_init_wait, wait);\n\tpoll_wait(file, &random_write_wait, wait);\n\tmask = 0;\n\tif (crng_ready())\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\tif (ENTROPY_BITS(&input_pool) < random_write_wakeup_bits)\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\treturn mask;\n}\n\nstatic int\nwrite_pool(struct entropy_store *r, const char __user *buffer, size_t count)\n{\n\tsize_t bytes;\n\t__u32 t, buf[16];\n\tconst char __user *p = buffer;\n\n\twhile (count > 0) {\n\t\tint b, i = 0;\n\n\t\tbytes = min(count, sizeof(buf));\n\t\tif (copy_from_user(&buf, p, bytes))\n\t\t\treturn -EFAULT;\n\n\t\tfor (b = bytes ; b > 0 ; b -= sizeof(__u32), i++) {\n\t\t\tif (!arch_get_random_int(&t))\n\t\t\t\tbreak;\n\t\t\tbuf[i] ^= t;\n\t\t}\n\n\t\tcount -= bytes;\n\t\tp += bytes;\n\n\t\tmix_pool_bytes(r, buf, bytes);\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t random_write(struct file *file, const char __user *buffer,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tsize_t ret;\n\n\tret = write_pool(&input_pool, buffer, count);\n\tif (ret)\n\t\treturn ret;\n\n\treturn (ssize_t)count;\n}\n\nstatic long random_ioctl(struct file *f, unsigned int cmd, unsigned long arg)\n{\n\tint size, ent_count;\n\tint __user *p = (int __user *)arg;\n\tint retval;\n\n\tswitch (cmd) {\n\tcase RNDGETENTCNT:\n\t\t/* inherently racy, no point locking */\n\t\tent_count = ENTROPY_BITS(&input_pool);\n\t\tif (put_user(ent_count, p))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase RNDADDTOENTCNT:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (get_user(ent_count, p))\n\t\t\treturn -EFAULT;\n\t\treturn credit_entropy_bits_safe(&input_pool, ent_count);\n\tcase RNDADDENTROPY:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (get_user(ent_count, p++))\n\t\t\treturn -EFAULT;\n\t\tif (ent_count < 0)\n\t\t\treturn -EINVAL;\n\t\tif (get_user(size, p++))\n\t\t\treturn -EFAULT;\n\t\tretval = write_pool(&input_pool, (const char __user *)p,\n\t\t\t\t    size);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\t\treturn credit_entropy_bits_safe(&input_pool, ent_count);\n\tcase RNDZAPENTCNT:\n\tcase RNDCLEARPOOL:\n\t\t/*\n\t\t * Clear the entropy pool counters. We no longer clear\n\t\t * the entropy pool, as that's silly.\n\t\t */\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tinput_pool.entropy_count = 0;\n\t\treturn 0;\n\tcase RNDRESEEDCRNG:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (crng_init < 2)\n\t\t\treturn -ENODATA;\n\t\tcrng_reseed(&primary_crng, NULL);\n\t\tcrng_global_init_time = jiffies - 1;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int random_fasync(int fd, struct file *filp, int on)\n{\n\treturn fasync_helper(fd, filp, on, &fasync);\n}\n\nconst struct file_operations random_fops = {\n\t.read  = random_read,\n\t.write = random_write,\n\t.poll  = random_poll,\n\t.unlocked_ioctl = random_ioctl,\n\t.compat_ioctl = compat_ptr_ioctl,\n\t.fasync = random_fasync,\n\t.llseek = noop_llseek,\n};\n\nconst struct file_operations urandom_fops = {\n\t.read  = urandom_read,\n\t.write = random_write,\n\t.unlocked_ioctl = random_ioctl,\n\t.compat_ioctl = compat_ptr_ioctl,\n\t.fasync = random_fasync,\n\t.llseek = noop_llseek,\n};\n\nSYSCALL_DEFINE3(getrandom, char __user *, buf, size_t, count,\n\t\tunsigned int, flags)\n{\n\tint ret;\n\n\tif (flags & ~(GRND_NONBLOCK|GRND_RANDOM|GRND_INSECURE))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Requesting insecure and blocking randomness at the same time makes\n\t * no sense.\n\t */\n\tif ((flags & (GRND_INSECURE|GRND_RANDOM)) == (GRND_INSECURE|GRND_RANDOM))\n\t\treturn -EINVAL;\n\n\tif (count > INT_MAX)\n\t\tcount = INT_MAX;\n\n\tif (!(flags & GRND_INSECURE) && !crng_ready()) {\n\t\tif (flags & GRND_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\t\tret = wait_for_random_bytes();\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\treturn urandom_read_nowarn(NULL, buf, count, NULL);\n}\n\n/********************************************************************\n *\n * Sysctl interface\n *\n ********************************************************************/\n\n#ifdef CONFIG_SYSCTL\n\n#include <linux/sysctl.h>\n\nstatic int min_write_thresh;\nstatic int max_write_thresh = INPUT_POOL_WORDS * 32;\nstatic int random_min_urandom_seed = 60;\nstatic char sysctl_bootid[16];\n\n/*\n * This function is used to return both the bootid UUID, and random\n * UUID.  The difference is in whether table->data is NULL; if it is,\n * then a new UUID is generated and returned to the user.\n *\n * If the user accesses this via the proc interface, the UUID will be\n * returned as an ASCII string in the standard UUID format; if via the\n * sysctl system call, as 16 bytes of binary data.\n */\nstatic int proc_do_uuid(struct ctl_table *table, int write,\n\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table fake_table;\n\tunsigned char buf[64], tmp_uuid[16], *uuid;\n\n\tuuid = table->data;\n\tif (!uuid) {\n\t\tuuid = tmp_uuid;\n\t\tgenerate_random_uuid(uuid);\n\t} else {\n\t\tstatic DEFINE_SPINLOCK(bootid_spinlock);\n\n\t\tspin_lock(&bootid_spinlock);\n\t\tif (!uuid[8])\n\t\t\tgenerate_random_uuid(uuid);\n\t\tspin_unlock(&bootid_spinlock);\n\t}\n\n\tsprintf(buf, \"%pU\", uuid);\n\n\tfake_table.data = buf;\n\tfake_table.maxlen = sizeof(buf);\n\n\treturn proc_dostring(&fake_table, write, buffer, lenp, ppos);\n}\n\n/*\n * Return entropy available scaled to integral bits\n */\nstatic int proc_do_entropy(struct ctl_table *table, int write,\n\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table fake_table;\n\tint entropy_count;\n\n\tentropy_count = *(int *)table->data >> ENTROPY_SHIFT;\n\n\tfake_table.data = &entropy_count;\n\tfake_table.maxlen = sizeof(entropy_count);\n\n\treturn proc_dointvec(&fake_table, write, buffer, lenp, ppos);\n}\n\nstatic int sysctl_poolsize = INPUT_POOL_WORDS * 32;\nextern struct ctl_table random_table[];\nstruct ctl_table random_table[] = {\n\t{\n\t\t.procname\t= \"poolsize\",\n\t\t.data\t\t= &sysctl_poolsize,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"entropy_avail\",\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_entropy,\n\t\t.data\t\t= &input_pool.entropy_count,\n\t},\n\t{\n\t\t.procname\t= \"write_wakeup_threshold\",\n\t\t.data\t\t= &random_write_wakeup_bits,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_write_thresh,\n\t\t.extra2\t\t= &max_write_thresh,\n\t},\n\t{\n\t\t.procname\t= \"urandom_min_reseed_secs\",\n\t\t.data\t\t= &random_min_urandom_seed,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"boot_id\",\n\t\t.data\t\t= &sysctl_bootid,\n\t\t.maxlen\t\t= 16,\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_uuid,\n\t},\n\t{\n\t\t.procname\t= \"uuid\",\n\t\t.maxlen\t\t= 16,\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_uuid,\n\t},\n#ifdef ADD_INTERRUPT_BENCH\n\t{\n\t\t.procname\t= \"add_interrupt_avg_cycles\",\n\t\t.data\t\t= &avg_cycles,\n\t\t.maxlen\t\t= sizeof(avg_cycles),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"add_interrupt_avg_deviation\",\n\t\t.data\t\t= &avg_deviation,\n\t\t.maxlen\t\t= sizeof(avg_deviation),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n#endif\n\t{ }\n};\n#endif \t/* CONFIG_SYSCTL */\n\nstruct batched_entropy {\n\tunion {\n\t\tu64 entropy_u64[CHACHA_BLOCK_SIZE / sizeof(u64)];\n\t\tu32 entropy_u32[CHACHA_BLOCK_SIZE / sizeof(u32)];\n\t};\n\tunsigned int position;\n\tspinlock_t batch_lock;\n};\n\n/*\n * Get a random word for internal kernel use only. The quality of the random\n * number is good as /dev/urandom, but there is no backtrack protection, with\n * the goal of being quite fast and not depleting entropy. In order to ensure\n * that the randomness provided by this function is okay, the function\n * wait_for_random_bytes() should be called and return 0 at least once at any\n * point prior.\n */\nstatic DEFINE_PER_CPU(struct batched_entropy, batched_entropy_u64) = {\n\t.batch_lock\t= __SPIN_LOCK_UNLOCKED(batched_entropy_u64.lock),\n};\n\nu64 get_random_u64(void)\n{\n\tu64 ret;\n\tunsigned long flags;\n\tstruct batched_entropy *batch;\n\tstatic void *previous;\n\n\twarn_unseeded_randomness(&previous);\n\n\tbatch = raw_cpu_ptr(&batched_entropy_u64);\n\tspin_lock_irqsave(&batch->batch_lock, flags);\n\tif (batch->position % ARRAY_SIZE(batch->entropy_u64) == 0) {\n\t\textract_crng((u8 *)batch->entropy_u64);\n\t\tbatch->position = 0;\n\t}\n\tret = batch->entropy_u64[batch->position++];\n\tspin_unlock_irqrestore(&batch->batch_lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(get_random_u64);\n\nstatic DEFINE_PER_CPU(struct batched_entropy, batched_entropy_u32) = {\n\t.batch_lock\t= __SPIN_LOCK_UNLOCKED(batched_entropy_u32.lock),\n};\nu32 get_random_u32(void)\n{\n\tu32 ret;\n\tunsigned long flags;\n\tstruct batched_entropy *batch;\n\tstatic void *previous;\n\n\twarn_unseeded_randomness(&previous);\n\n\tbatch = raw_cpu_ptr(&batched_entropy_u32);\n\tspin_lock_irqsave(&batch->batch_lock, flags);\n\tif (batch->position % ARRAY_SIZE(batch->entropy_u32) == 0) {\n\t\textract_crng((u8 *)batch->entropy_u32);\n\t\tbatch->position = 0;\n\t}\n\tret = batch->entropy_u32[batch->position++];\n\tspin_unlock_irqrestore(&batch->batch_lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(get_random_u32);\n\n/* It's important to invalidate all potential batched entropy that might\n * be stored before the crng is initialized, which we can do lazily by\n * simply resetting the counter to zero so that it's re-extracted on the\n * next usage. */\nstatic void invalidate_batched_entropy(void)\n{\n\tint cpu;\n\tunsigned long flags;\n\n\tfor_each_possible_cpu (cpu) {\n\t\tstruct batched_entropy *batched_entropy;\n\n\t\tbatched_entropy = per_cpu_ptr(&batched_entropy_u32, cpu);\n\t\tspin_lock_irqsave(&batched_entropy->batch_lock, flags);\n\t\tbatched_entropy->position = 0;\n\t\tspin_unlock(&batched_entropy->batch_lock);\n\n\t\tbatched_entropy = per_cpu_ptr(&batched_entropy_u64, cpu);\n\t\tspin_lock(&batched_entropy->batch_lock);\n\t\tbatched_entropy->position = 0;\n\t\tspin_unlock_irqrestore(&batched_entropy->batch_lock, flags);\n\t}\n}\n\n/**\n * randomize_page - Generate a random, page aligned address\n * @start:\tThe smallest acceptable address the caller will take.\n * @range:\tThe size of the area, starting at @start, within which the\n *\t\trandom address must fall.\n *\n * If @start + @range would overflow, @range is capped.\n *\n * NOTE: Historical use of randomize_range, which this replaces, presumed that\n * @start was already page aligned.  We now align it regardless.\n *\n * Return: A page aligned address within [start, start + range).  On error,\n * @start is returned.\n */\nunsigned long\nrandomize_page(unsigned long start, unsigned long range)\n{\n\tif (!PAGE_ALIGNED(start)) {\n\t\trange -= PAGE_ALIGN(start) - start;\n\t\tstart = PAGE_ALIGN(start);\n\t}\n\n\tif (start > ULONG_MAX - range)\n\t\trange = ULONG_MAX - start;\n\n\trange >>= PAGE_SHIFT;\n\n\tif (range == 0)\n\t\treturn start;\n\n\treturn start + (get_random_long() % range << PAGE_SHIFT);\n}\n\n/* Interface for in-kernel drivers of true hardware RNGs.\n * Those devices may produce endless random bits and will be throttled\n * when our pool is full.\n */\nvoid add_hwgenerator_randomness(const char *buffer, size_t count,\n\t\t\t\tsize_t entropy)\n{\n\tstruct entropy_store *poolp = &input_pool;\n\n\tif (unlikely(crng_init == 0)) {\n\t\tcrng_fast_load(buffer, count);\n\t\treturn;\n\t}\n\n\t/* Suspend writing if we're above the trickle threshold.\n\t * We'll be woken up again once below random_write_wakeup_thresh,\n\t * or when the calling thread is about to terminate.\n\t */\n\twait_event_interruptible(random_write_wait, kthread_should_stop() ||\n\t\t\tENTROPY_BITS(&input_pool) <= random_write_wakeup_bits);\n\tmix_pool_bytes(poolp, buffer, count);\n\tcredit_entropy_bits(poolp, entropy);\n}\nEXPORT_SYMBOL_GPL(add_hwgenerator_randomness);\n\n/* Handle random seed passed by bootloader.\n * If the seed is trustworthy, it would be regarded as hardware RNGs. Otherwise\n * it would be regarded as device data.\n * The decision is controlled by CONFIG_RANDOM_TRUST_BOOTLOADER.\n */\nvoid add_bootloader_randomness(const void *buf, unsigned int size)\n{\n\tif (IS_ENABLED(CONFIG_RANDOM_TRUST_BOOTLOADER))\n\t\tadd_hwgenerator_randomness(buf, size, size * 8);\n\telse\n\t\tadd_device_randomness(buf, size);\n}\nEXPORT_SYMBOL_GPL(add_bootloader_randomness);\n", "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * include/linux/random.h\n *\n * Include file for the random number generator.\n */\n#ifndef _LINUX_RANDOM_H\n#define _LINUX_RANDOM_H\n\n#include <linux/bug.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/once.h>\n\n#include <uapi/linux/random.h>\n\nstruct random_ready_callback {\n\tstruct list_head list;\n\tvoid (*func)(struct random_ready_callback *rdy);\n\tstruct module *owner;\n};\n\nextern void add_device_randomness(const void *, unsigned int);\nextern void add_bootloader_randomness(const void *, unsigned int);\n\n#if defined(LATENT_ENTROPY_PLUGIN) && !defined(__CHECKER__)\nstatic inline void add_latent_entropy(void)\n{\n\tadd_device_randomness((const void *)&latent_entropy,\n\t\t\t      sizeof(latent_entropy));\n}\n#else\nstatic inline void add_latent_entropy(void) {}\n#endif\n\nextern void add_input_randomness(unsigned int type, unsigned int code,\n\t\t\t\t unsigned int value) __latent_entropy;\nextern void add_interrupt_randomness(int irq, int irq_flags) __latent_entropy;\n\nextern void get_random_bytes(void *buf, int nbytes);\nextern int wait_for_random_bytes(void);\nextern int __init rand_initialize(void);\nextern bool rng_is_initialized(void);\nextern int add_random_ready_callback(struct random_ready_callback *rdy);\nextern void del_random_ready_callback(struct random_ready_callback *rdy);\nextern int __must_check get_random_bytes_arch(void *buf, int nbytes);\n\n#ifndef MODULE\nextern const struct file_operations random_fops, urandom_fops;\n#endif\n\nu32 get_random_u32(void);\nu64 get_random_u64(void);\nstatic inline unsigned int get_random_int(void)\n{\n\treturn get_random_u32();\n}\nstatic inline unsigned long get_random_long(void)\n{\n#if BITS_PER_LONG == 64\n\treturn get_random_u64();\n#else\n\treturn get_random_u32();\n#endif\n}\n\n/*\n * On 64-bit architectures, protect against non-terminated C string overflows\n * by zeroing out the first byte of the canary; this leaves 56 bits of entropy.\n */\n#ifdef CONFIG_64BIT\n# ifdef __LITTLE_ENDIAN\n#  define CANARY_MASK 0xffffffffffffff00UL\n# else /* big endian, 64 bits: */\n#  define CANARY_MASK 0x00ffffffffffffffUL\n# endif\n#else /* 32 bits: */\n# define CANARY_MASK 0xffffffffUL\n#endif\n\nstatic inline unsigned long get_random_canary(void)\n{\n\tunsigned long val = get_random_long();\n\n\treturn val & CANARY_MASK;\n}\n\n/* Calls wait_for_random_bytes() and then calls get_random_bytes(buf, nbytes).\n * Returns the result of the call to wait_for_random_bytes. */\nstatic inline int get_random_bytes_wait(void *buf, int nbytes)\n{\n\tint ret = wait_for_random_bytes();\n\tget_random_bytes(buf, nbytes);\n\treturn ret;\n}\n\n#define declare_get_random_var_wait(var) \\\n\tstatic inline int get_random_ ## var ## _wait(var *out) { \\\n\t\tint ret = wait_for_random_bytes(); \\\n\t\tif (unlikely(ret)) \\\n\t\t\treturn ret; \\\n\t\t*out = get_random_ ## var(); \\\n\t\treturn 0; \\\n\t}\ndeclare_get_random_var_wait(u32)\ndeclare_get_random_var_wait(u64)\ndeclare_get_random_var_wait(int)\ndeclare_get_random_var_wait(long)\n#undef declare_get_random_var\n\nunsigned long randomize_page(unsigned long start, unsigned long range);\n\nu32 prandom_u32(void);\nvoid prandom_bytes(void *buf, size_t nbytes);\nvoid prandom_seed(u32 seed);\nvoid prandom_reseed_late(void);\n\nstruct rnd_state {\n\t__u32 s1, s2, s3, s4;\n};\n\nu32 prandom_u32_state(struct rnd_state *state);\nvoid prandom_bytes_state(struct rnd_state *state, void *buf, size_t nbytes);\nvoid prandom_seed_full_state(struct rnd_state __percpu *pcpu_state);\n\n#define prandom_init_once(pcpu_state)\t\t\t\\\n\tDO_ONCE(prandom_seed_full_state, (pcpu_state))\n\n/**\n * prandom_u32_max - returns a pseudo-random number in interval [0, ep_ro)\n * @ep_ro: right open interval endpoint\n *\n * Returns a pseudo-random number that is in interval [0, ep_ro). Note\n * that the result depends on PRNG being well distributed in [0, ~0U]\n * u32 space. Here we use maximally equidistributed combined Tausworthe\n * generator, that is, prandom_u32(). This is useful when requesting a\n * random index of an array containing ep_ro elements, for example.\n *\n * Returns: pseudo-random number in interval [0, ep_ro)\n */\nstatic inline u32 prandom_u32_max(u32 ep_ro)\n{\n\treturn (u32)(((u64) prandom_u32() * ep_ro) >> 32);\n}\n\n/*\n * Handle minimum values for seeds\n */\nstatic inline u32 __seed(u32 x, u32 m)\n{\n\treturn (x < m) ? x + m : x;\n}\n\n/**\n * prandom_seed_state - set seed for prandom_u32_state().\n * @state: pointer to state structure to receive the seed.\n * @seed: arbitrary 64-bit value to use as a seed.\n */\nstatic inline void prandom_seed_state(struct rnd_state *state, u64 seed)\n{\n\tu32 i = (seed >> 32) ^ (seed << 10) ^ seed;\n\n\tstate->s1 = __seed(i,   2U);\n\tstate->s2 = __seed(i,   8U);\n\tstate->s3 = __seed(i,  16U);\n\tstate->s4 = __seed(i, 128U);\n}\n\n#ifdef CONFIG_ARCH_RANDOM\n# include <asm/archrandom.h>\n#else\nstatic inline bool __must_check arch_get_random_long(unsigned long *v)\n{\n\treturn false;\n}\nstatic inline bool __must_check arch_get_random_int(unsigned int *v)\n{\n\treturn false;\n}\nstatic inline bool __must_check arch_get_random_seed_long(unsigned long *v)\n{\n\treturn false;\n}\nstatic inline bool __must_check arch_get_random_seed_int(unsigned int *v)\n{\n\treturn false;\n}\n#endif\n\n/*\n * Called from the boot CPU during startup; not valid to call once\n * secondary CPUs are up and preemption is possible.\n */\n#ifndef arch_get_random_seed_long_early\nstatic inline bool __init arch_get_random_seed_long_early(unsigned long *v)\n{\n\tWARN_ON(system_state != SYSTEM_BOOTING);\n\treturn arch_get_random_seed_long(v);\n}\n#endif\n\n#ifndef arch_get_random_long_early\nstatic inline bool __init arch_get_random_long_early(unsigned long *v)\n{\n\tWARN_ON(system_state != SYSTEM_BOOTING);\n\treturn arch_get_random_long(v);\n}\n#endif\n\n/* Pseudo random number generator from numerical recipes. */\nstatic inline u32 next_pseudo_random32(u32 seed)\n{\n\treturn seed * 1664525 + 1013904223;\n}\n\n#endif /* _LINUX_RANDOM_H */\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  Kernel internal timers\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  1997-01-28  Modified by Finn Arne Gangstad to make timers scale better.\n *\n *  1997-09-10  Updated NTP code according to technical memorandum Jan '96\n *              \"A Kernel Model for Precision Timekeeping\" by Dave Mills\n *  1998-12-24  Fixed a xtime SMP race (we need the xtime_lock rw spinlock to\n *              serialize accesses to xtime/lost_ticks).\n *                              Copyright (C) 1998  Andrea Arcangeli\n *  1999-03-10  Improved NTP compatibility by Ulrich Windl\n *  2002-05-31\tMove sys_sysinfo here and make its locking sane, Robert Love\n *  2000-10-05  Implemented scalable SMP per-CPU timer handling.\n *                              Copyright (C) 2000, 2001, 2002  Ingo Molnar\n *              Designed by David S. Miller, Alexey Kuznetsov and Ingo Molnar\n */\n\n#include <linux/kernel_stat.h>\n#include <linux/export.h>\n#include <linux/interrupt.h>\n#include <linux/percpu.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/pid_namespace.h>\n#include <linux/notifier.h>\n#include <linux/thread_info.h>\n#include <linux/time.h>\n#include <linux/jiffies.h>\n#include <linux/posix-timers.h>\n#include <linux/cpu.h>\n#include <linux/syscalls.h>\n#include <linux/delay.h>\n#include <linux/tick.h>\n#include <linux/kallsyms.h>\n#include <linux/irq_work.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/debug.h>\n#include <linux/slab.h>\n#include <linux/compat.h>\n\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/div64.h>\n#include <asm/timex.h>\n#include <asm/io.h>\n\n#include \"tick-internal.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/timer.h>\n\n__visible u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;\n\nEXPORT_SYMBOL(jiffies_64);\n\n/*\n * The timer wheel has LVL_DEPTH array levels. Each level provides an array of\n * LVL_SIZE buckets. Each level is driven by its own clock and therefor each\n * level has a different granularity.\n *\n * The level granularity is:\t\tLVL_CLK_DIV ^ lvl\n * The level clock frequency is:\tHZ / (LVL_CLK_DIV ^ level)\n *\n * The array level of a newly armed timer depends on the relative expiry\n * time. The farther the expiry time is away the higher the array level and\n * therefor the granularity becomes.\n *\n * Contrary to the original timer wheel implementation, which aims for 'exact'\n * expiry of the timers, this implementation removes the need for recascading\n * the timers into the lower array levels. The previous 'classic' timer wheel\n * implementation of the kernel already violated the 'exact' expiry by adding\n * slack to the expiry time to provide batched expiration. The granularity\n * levels provide implicit batching.\n *\n * This is an optimization of the original timer wheel implementation for the\n * majority of the timer wheel use cases: timeouts. The vast majority of\n * timeout timers (networking, disk I/O ...) are canceled before expiry. If\n * the timeout expires it indicates that normal operation is disturbed, so it\n * does not matter much whether the timeout comes with a slight delay.\n *\n * The only exception to this are networking timers with a small expiry\n * time. They rely on the granularity. Those fit into the first wheel level,\n * which has HZ granularity.\n *\n * We don't have cascading anymore. timers with a expiry time above the\n * capacity of the last wheel level are force expired at the maximum timeout\n * value of the last wheel level. From data sampling we know that the maximum\n * value observed is 5 days (network connection tracking), so this should not\n * be an issue.\n *\n * The currently chosen array constants values are a good compromise between\n * array size and granularity.\n *\n * This results in the following granularity and range levels:\n *\n * HZ 1000 steps\n * Level Offset  Granularity            Range\n *  0      0         1 ms                0 ms -         63 ms\n *  1     64         8 ms               64 ms -        511 ms\n *  2    128        64 ms              512 ms -       4095 ms (512ms - ~4s)\n *  3    192       512 ms             4096 ms -      32767 ms (~4s - ~32s)\n *  4    256      4096 ms (~4s)      32768 ms -     262143 ms (~32s - ~4m)\n *  5    320     32768 ms (~32s)    262144 ms -    2097151 ms (~4m - ~34m)\n *  6    384    262144 ms (~4m)    2097152 ms -   16777215 ms (~34m - ~4h)\n *  7    448   2097152 ms (~34m)  16777216 ms -  134217727 ms (~4h - ~1d)\n *  8    512  16777216 ms (~4h)  134217728 ms - 1073741822 ms (~1d - ~12d)\n *\n * HZ  300\n * Level Offset  Granularity            Range\n *  0\t   0         3 ms                0 ms -        210 ms\n *  1\t  64        26 ms              213 ms -       1703 ms (213ms - ~1s)\n *  2\t 128       213 ms             1706 ms -      13650 ms (~1s - ~13s)\n *  3\t 192      1706 ms (~1s)      13653 ms -     109223 ms (~13s - ~1m)\n *  4\t 256     13653 ms (~13s)    109226 ms -     873810 ms (~1m - ~14m)\n *  5\t 320    109226 ms (~1m)     873813 ms -    6990503 ms (~14m - ~1h)\n *  6\t 384    873813 ms (~14m)   6990506 ms -   55924050 ms (~1h - ~15h)\n *  7\t 448   6990506 ms (~1h)   55924053 ms -  447392423 ms (~15h - ~5d)\n *  8    512  55924053 ms (~15h) 447392426 ms - 3579139406 ms (~5d - ~41d)\n *\n * HZ  250\n * Level Offset  Granularity            Range\n *  0\t   0         4 ms                0 ms -        255 ms\n *  1\t  64        32 ms              256 ms -       2047 ms (256ms - ~2s)\n *  2\t 128       256 ms             2048 ms -      16383 ms (~2s - ~16s)\n *  3\t 192      2048 ms (~2s)      16384 ms -     131071 ms (~16s - ~2m)\n *  4\t 256     16384 ms (~16s)    131072 ms -    1048575 ms (~2m - ~17m)\n *  5\t 320    131072 ms (~2m)    1048576 ms -    8388607 ms (~17m - ~2h)\n *  6\t 384   1048576 ms (~17m)   8388608 ms -   67108863 ms (~2h - ~18h)\n *  7\t 448   8388608 ms (~2h)   67108864 ms -  536870911 ms (~18h - ~6d)\n *  8    512  67108864 ms (~18h) 536870912 ms - 4294967288 ms (~6d - ~49d)\n *\n * HZ  100\n * Level Offset  Granularity            Range\n *  0\t   0         10 ms               0 ms -        630 ms\n *  1\t  64         80 ms             640 ms -       5110 ms (640ms - ~5s)\n *  2\t 128        640 ms            5120 ms -      40950 ms (~5s - ~40s)\n *  3\t 192       5120 ms (~5s)     40960 ms -     327670 ms (~40s - ~5m)\n *  4\t 256      40960 ms (~40s)   327680 ms -    2621430 ms (~5m - ~43m)\n *  5\t 320     327680 ms (~5m)   2621440 ms -   20971510 ms (~43m - ~5h)\n *  6\t 384    2621440 ms (~43m) 20971520 ms -  167772150 ms (~5h - ~1d)\n *  7\t 448   20971520 ms (~5h) 167772160 ms - 1342177270 ms (~1d - ~15d)\n */\n\n/* Clock divisor for the next level */\n#define LVL_CLK_SHIFT\t3\n#define LVL_CLK_DIV\t(1UL << LVL_CLK_SHIFT)\n#define LVL_CLK_MASK\t(LVL_CLK_DIV - 1)\n#define LVL_SHIFT(n)\t((n) * LVL_CLK_SHIFT)\n#define LVL_GRAN(n)\t(1UL << LVL_SHIFT(n))\n\n/*\n * The time start value for each level to select the bucket at enqueue\n * time.\n */\n#define LVL_START(n)\t((LVL_SIZE - 1) << (((n) - 1) * LVL_CLK_SHIFT))\n\n/* Size of each clock level */\n#define LVL_BITS\t6\n#define LVL_SIZE\t(1UL << LVL_BITS)\n#define LVL_MASK\t(LVL_SIZE - 1)\n#define LVL_OFFS(n)\t((n) * LVL_SIZE)\n\n/* Level depth */\n#if HZ > 100\n# define LVL_DEPTH\t9\n# else\n# define LVL_DEPTH\t8\n#endif\n\n/* The cutoff (max. capacity of the wheel) */\n#define WHEEL_TIMEOUT_CUTOFF\t(LVL_START(LVL_DEPTH))\n#define WHEEL_TIMEOUT_MAX\t(WHEEL_TIMEOUT_CUTOFF - LVL_GRAN(LVL_DEPTH - 1))\n\n/*\n * The resulting wheel size. If NOHZ is configured we allocate two\n * wheels so we have a separate storage for the deferrable timers.\n */\n#define WHEEL_SIZE\t(LVL_SIZE * LVL_DEPTH)\n\n#ifdef CONFIG_NO_HZ_COMMON\n# define NR_BASES\t2\n# define BASE_STD\t0\n# define BASE_DEF\t1\n#else\n# define NR_BASES\t1\n# define BASE_STD\t0\n# define BASE_DEF\t0\n#endif\n\nstruct timer_base {\n\traw_spinlock_t\t\tlock;\n\tstruct timer_list\t*running_timer;\n#ifdef CONFIG_PREEMPT_RT\n\tspinlock_t\t\texpiry_lock;\n\tatomic_t\t\ttimer_waiters;\n#endif\n\tunsigned long\t\tclk;\n\tunsigned long\t\tnext_expiry;\n\tunsigned int\t\tcpu;\n\tbool\t\t\tis_idle;\n\tbool\t\t\tmust_forward_clk;\n\tDECLARE_BITMAP(pending_map, WHEEL_SIZE);\n\tstruct hlist_head\tvectors[WHEEL_SIZE];\n} ____cacheline_aligned;\n\nstatic DEFINE_PER_CPU(struct timer_base, timer_bases[NR_BASES]);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\nstatic DEFINE_STATIC_KEY_FALSE(timers_nohz_active);\nstatic DEFINE_MUTEX(timer_keys_mutex);\n\nstatic void timer_update_keys(struct work_struct *work);\nstatic DECLARE_WORK(timer_update_work, timer_update_keys);\n\n#ifdef CONFIG_SMP\nunsigned int sysctl_timer_migration = 1;\n\nDEFINE_STATIC_KEY_FALSE(timers_migration_enabled);\n\nstatic void timers_update_migration(void)\n{\n\tif (sysctl_timer_migration && tick_nohz_active)\n\t\tstatic_branch_enable(&timers_migration_enabled);\n\telse\n\t\tstatic_branch_disable(&timers_migration_enabled);\n}\n#else\nstatic inline void timers_update_migration(void) { }\n#endif /* !CONFIG_SMP */\n\nstatic void timer_update_keys(struct work_struct *work)\n{\n\tmutex_lock(&timer_keys_mutex);\n\ttimers_update_migration();\n\tstatic_branch_enable(&timers_nohz_active);\n\tmutex_unlock(&timer_keys_mutex);\n}\n\nvoid timers_update_nohz(void)\n{\n\tschedule_work(&timer_update_work);\n}\n\nint timer_migration_handler(struct ctl_table *table, int write,\n\t\t\t    void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\n\tmutex_lock(&timer_keys_mutex);\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (!ret && write)\n\t\ttimers_update_migration();\n\tmutex_unlock(&timer_keys_mutex);\n\treturn ret;\n}\n\nstatic inline bool is_timers_nohz_active(void)\n{\n\treturn static_branch_unlikely(&timers_nohz_active);\n}\n#else\nstatic inline bool is_timers_nohz_active(void) { return false; }\n#endif /* NO_HZ_COMMON */\n\nstatic unsigned long round_jiffies_common(unsigned long j, int cpu,\n\t\tbool force_up)\n{\n\tint rem;\n\tunsigned long original = j;\n\n\t/*\n\t * We don't want all cpus firing their timers at once hitting the\n\t * same lock or cachelines, so we skew each extra cpu with an extra\n\t * 3 jiffies. This 3 jiffies came originally from the mm/ code which\n\t * already did this.\n\t * The skew is done by adding 3*cpunr, then round, then subtract this\n\t * extra offset again.\n\t */\n\tj += cpu * 3;\n\n\trem = j % HZ;\n\n\t/*\n\t * If the target jiffie is just after a whole second (which can happen\n\t * due to delays of the timer irq, long irq off times etc etc) then\n\t * we should round down to the whole second, not up. Use 1/4th second\n\t * as cutoff for this rounding as an extreme upper bound for this.\n\t * But never round down if @force_up is set.\n\t */\n\tif (rem < HZ/4 && !force_up) /* round down */\n\t\tj = j - rem;\n\telse /* round up */\n\t\tj = j - rem + HZ;\n\n\t/* now that we have rounded, subtract the extra skew again */\n\tj -= cpu * 3;\n\n\t/*\n\t * Make sure j is still in the future. Otherwise return the\n\t * unmodified value.\n\t */\n\treturn time_is_after_jiffies(j) ? j : original;\n}\n\n/**\n * __round_jiffies - function to round jiffies to a full second\n * @j: the time in (absolute) jiffies that should be rounded\n * @cpu: the processor number on which the timeout will happen\n *\n * __round_jiffies() rounds an absolute time in the future (in jiffies)\n * up or down to (approximately) full seconds. This is useful for timers\n * for which the exact time they fire does not matter too much, as long as\n * they fire approximately every X seconds.\n *\n * By rounding these timers to whole seconds, all such timers will fire\n * at the same time, rather than at various times spread out. The goal\n * of this is to have the CPU wake up less, which saves power.\n *\n * The exact rounding is skewed for each processor to avoid all\n * processors firing at the exact same time, which could lead\n * to lock contention or spurious cache line bouncing.\n *\n * The return value is the rounded version of the @j parameter.\n */\nunsigned long __round_jiffies(unsigned long j, int cpu)\n{\n\treturn round_jiffies_common(j, cpu, false);\n}\nEXPORT_SYMBOL_GPL(__round_jiffies);\n\n/**\n * __round_jiffies_relative - function to round jiffies to a full second\n * @j: the time in (relative) jiffies that should be rounded\n * @cpu: the processor number on which the timeout will happen\n *\n * __round_jiffies_relative() rounds a time delta  in the future (in jiffies)\n * up or down to (approximately) full seconds. This is useful for timers\n * for which the exact time they fire does not matter too much, as long as\n * they fire approximately every X seconds.\n *\n * By rounding these timers to whole seconds, all such timers will fire\n * at the same time, rather than at various times spread out. The goal\n * of this is to have the CPU wake up less, which saves power.\n *\n * The exact rounding is skewed for each processor to avoid all\n * processors firing at the exact same time, which could lead\n * to lock contention or spurious cache line bouncing.\n *\n * The return value is the rounded version of the @j parameter.\n */\nunsigned long __round_jiffies_relative(unsigned long j, int cpu)\n{\n\tunsigned long j0 = jiffies;\n\n\t/* Use j0 because jiffies might change while we run */\n\treturn round_jiffies_common(j + j0, cpu, false) - j0;\n}\nEXPORT_SYMBOL_GPL(__round_jiffies_relative);\n\n/**\n * round_jiffies - function to round jiffies to a full second\n * @j: the time in (absolute) jiffies that should be rounded\n *\n * round_jiffies() rounds an absolute time in the future (in jiffies)\n * up or down to (approximately) full seconds. This is useful for timers\n * for which the exact time they fire does not matter too much, as long as\n * they fire approximately every X seconds.\n *\n * By rounding these timers to whole seconds, all such timers will fire\n * at the same time, rather than at various times spread out. The goal\n * of this is to have the CPU wake up less, which saves power.\n *\n * The return value is the rounded version of the @j parameter.\n */\nunsigned long round_jiffies(unsigned long j)\n{\n\treturn round_jiffies_common(j, raw_smp_processor_id(), false);\n}\nEXPORT_SYMBOL_GPL(round_jiffies);\n\n/**\n * round_jiffies_relative - function to round jiffies to a full second\n * @j: the time in (relative) jiffies that should be rounded\n *\n * round_jiffies_relative() rounds a time delta  in the future (in jiffies)\n * up or down to (approximately) full seconds. This is useful for timers\n * for which the exact time they fire does not matter too much, as long as\n * they fire approximately every X seconds.\n *\n * By rounding these timers to whole seconds, all such timers will fire\n * at the same time, rather than at various times spread out. The goal\n * of this is to have the CPU wake up less, which saves power.\n *\n * The return value is the rounded version of the @j parameter.\n */\nunsigned long round_jiffies_relative(unsigned long j)\n{\n\treturn __round_jiffies_relative(j, raw_smp_processor_id());\n}\nEXPORT_SYMBOL_GPL(round_jiffies_relative);\n\n/**\n * __round_jiffies_up - function to round jiffies up to a full second\n * @j: the time in (absolute) jiffies that should be rounded\n * @cpu: the processor number on which the timeout will happen\n *\n * This is the same as __round_jiffies() except that it will never\n * round down.  This is useful for timeouts for which the exact time\n * of firing does not matter too much, as long as they don't fire too\n * early.\n */\nunsigned long __round_jiffies_up(unsigned long j, int cpu)\n{\n\treturn round_jiffies_common(j, cpu, true);\n}\nEXPORT_SYMBOL_GPL(__round_jiffies_up);\n\n/**\n * __round_jiffies_up_relative - function to round jiffies up to a full second\n * @j: the time in (relative) jiffies that should be rounded\n * @cpu: the processor number on which the timeout will happen\n *\n * This is the same as __round_jiffies_relative() except that it will never\n * round down.  This is useful for timeouts for which the exact time\n * of firing does not matter too much, as long as they don't fire too\n * early.\n */\nunsigned long __round_jiffies_up_relative(unsigned long j, int cpu)\n{\n\tunsigned long j0 = jiffies;\n\n\t/* Use j0 because jiffies might change while we run */\n\treturn round_jiffies_common(j + j0, cpu, true) - j0;\n}\nEXPORT_SYMBOL_GPL(__round_jiffies_up_relative);\n\n/**\n * round_jiffies_up - function to round jiffies up to a full second\n * @j: the time in (absolute) jiffies that should be rounded\n *\n * This is the same as round_jiffies() except that it will never\n * round down.  This is useful for timeouts for which the exact time\n * of firing does not matter too much, as long as they don't fire too\n * early.\n */\nunsigned long round_jiffies_up(unsigned long j)\n{\n\treturn round_jiffies_common(j, raw_smp_processor_id(), true);\n}\nEXPORT_SYMBOL_GPL(round_jiffies_up);\n\n/**\n * round_jiffies_up_relative - function to round jiffies up to a full second\n * @j: the time in (relative) jiffies that should be rounded\n *\n * This is the same as round_jiffies_relative() except that it will never\n * round down.  This is useful for timeouts for which the exact time\n * of firing does not matter too much, as long as they don't fire too\n * early.\n */\nunsigned long round_jiffies_up_relative(unsigned long j)\n{\n\treturn __round_jiffies_up_relative(j, raw_smp_processor_id());\n}\nEXPORT_SYMBOL_GPL(round_jiffies_up_relative);\n\n\nstatic inline unsigned int timer_get_idx(struct timer_list *timer)\n{\n\treturn (timer->flags & TIMER_ARRAYMASK) >> TIMER_ARRAYSHIFT;\n}\n\nstatic inline void timer_set_idx(struct timer_list *timer, unsigned int idx)\n{\n\ttimer->flags = (timer->flags & ~TIMER_ARRAYMASK) |\n\t\t\tidx << TIMER_ARRAYSHIFT;\n}\n\n/*\n * Helper function to calculate the array index for a given expiry\n * time.\n */\nstatic inline unsigned calc_index(unsigned expires, unsigned lvl)\n{\n\texpires = (expires + LVL_GRAN(lvl)) >> LVL_SHIFT(lvl);\n\treturn LVL_OFFS(lvl) + (expires & LVL_MASK);\n}\n\nstatic int calc_wheel_index(unsigned long expires, unsigned long clk)\n{\n\tunsigned long delta = expires - clk;\n\tunsigned int idx;\n\n\tif (delta < LVL_START(1)) {\n\t\tidx = calc_index(expires, 0);\n\t} else if (delta < LVL_START(2)) {\n\t\tidx = calc_index(expires, 1);\n\t} else if (delta < LVL_START(3)) {\n\t\tidx = calc_index(expires, 2);\n\t} else if (delta < LVL_START(4)) {\n\t\tidx = calc_index(expires, 3);\n\t} else if (delta < LVL_START(5)) {\n\t\tidx = calc_index(expires, 4);\n\t} else if (delta < LVL_START(6)) {\n\t\tidx = calc_index(expires, 5);\n\t} else if (delta < LVL_START(7)) {\n\t\tidx = calc_index(expires, 6);\n\t} else if (LVL_DEPTH > 8 && delta < LVL_START(8)) {\n\t\tidx = calc_index(expires, 7);\n\t} else if ((long) delta < 0) {\n\t\tidx = clk & LVL_MASK;\n\t} else {\n\t\t/*\n\t\t * Force expire obscene large timeouts to expire at the\n\t\t * capacity limit of the wheel.\n\t\t */\n\t\tif (delta >= WHEEL_TIMEOUT_CUTOFF)\n\t\t\texpires = clk + WHEEL_TIMEOUT_MAX;\n\n\t\tidx = calc_index(expires, LVL_DEPTH - 1);\n\t}\n\treturn idx;\n}\n\n/*\n * Enqueue the timer into the hash bucket, mark it pending in\n * the bitmap and store the index in the timer flags.\n */\nstatic void enqueue_timer(struct timer_base *base, struct timer_list *timer,\n\t\t\t  unsigned int idx)\n{\n\thlist_add_head(&timer->entry, base->vectors + idx);\n\t__set_bit(idx, base->pending_map);\n\ttimer_set_idx(timer, idx);\n\n\ttrace_timer_start(timer, timer->expires, timer->flags);\n}\n\nstatic void\n__internal_add_timer(struct timer_base *base, struct timer_list *timer)\n{\n\tunsigned int idx;\n\n\tidx = calc_wheel_index(timer->expires, base->clk);\n\tenqueue_timer(base, timer, idx);\n}\n\nstatic void\ntrigger_dyntick_cpu(struct timer_base *base, struct timer_list *timer)\n{\n\tif (!is_timers_nohz_active())\n\t\treturn;\n\n\t/*\n\t * TODO: This wants some optimizing similar to the code below, but we\n\t * will do that when we switch from push to pull for deferrable timers.\n\t */\n\tif (timer->flags & TIMER_DEFERRABLE) {\n\t\tif (tick_nohz_full_cpu(base->cpu))\n\t\t\twake_up_nohz_cpu(base->cpu);\n\t\treturn;\n\t}\n\n\t/*\n\t * We might have to IPI the remote CPU if the base is idle and the\n\t * timer is not deferrable. If the other CPU is on the way to idle\n\t * then it can't set base->is_idle as we hold the base lock:\n\t */\n\tif (!base->is_idle)\n\t\treturn;\n\n\t/* Check whether this is the new first expiring timer: */\n\tif (time_after_eq(timer->expires, base->next_expiry))\n\t\treturn;\n\n\t/*\n\t * Set the next expiry time and kick the CPU so it can reevaluate the\n\t * wheel:\n\t */\n\tif (time_before(timer->expires, base->clk)) {\n\t\t/*\n\t\t * Prevent from forward_timer_base() moving the base->clk\n\t\t * backward\n\t\t */\n\t\tbase->next_expiry = base->clk;\n\t} else {\n\t\tbase->next_expiry = timer->expires;\n\t}\n\twake_up_nohz_cpu(base->cpu);\n}\n\nstatic void\ninternal_add_timer(struct timer_base *base, struct timer_list *timer)\n{\n\t__internal_add_timer(base, timer);\n\ttrigger_dyntick_cpu(base, timer);\n}\n\n#ifdef CONFIG_DEBUG_OBJECTS_TIMERS\n\nstatic struct debug_obj_descr timer_debug_descr;\n\nstatic void *timer_debug_hint(void *addr)\n{\n\treturn ((struct timer_list *) addr)->function;\n}\n\nstatic bool timer_is_static_object(void *addr)\n{\n\tstruct timer_list *timer = addr;\n\n\treturn (timer->entry.pprev == NULL &&\n\t\ttimer->entry.next == TIMER_ENTRY_STATIC);\n}\n\n/*\n * fixup_init is called when:\n * - an active object is initialized\n */\nstatic bool timer_fixup_init(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tdel_timer_sync(timer);\n\t\tdebug_object_init(timer, &timer_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Stub timer callback for improperly used timers. */\nstatic void stub_timer(struct timer_list *unused)\n{\n\tWARN_ON(1);\n}\n\n/*\n * fixup_activate is called when:\n * - an active object is activated\n * - an unknown non-static object is activated\n */\nstatic bool timer_fixup_activate(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_NOTAVAILABLE:\n\t\ttimer_setup(timer, stub_timer, 0);\n\t\treturn true;\n\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tWARN_ON(1);\n\t\t/* fall through */\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/*\n * fixup_free is called when:\n * - an active object is freed\n */\nstatic bool timer_fixup_free(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tdel_timer_sync(timer);\n\t\tdebug_object_free(timer, &timer_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/*\n * fixup_assert_init is called when:\n * - an untracked/uninit-ed object is found\n */\nstatic bool timer_fixup_assert_init(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_NOTAVAILABLE:\n\t\ttimer_setup(timer, stub_timer, 0);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic struct debug_obj_descr timer_debug_descr = {\n\t.name\t\t\t= \"timer_list\",\n\t.debug_hint\t\t= timer_debug_hint,\n\t.is_static_object\t= timer_is_static_object,\n\t.fixup_init\t\t= timer_fixup_init,\n\t.fixup_activate\t\t= timer_fixup_activate,\n\t.fixup_free\t\t= timer_fixup_free,\n\t.fixup_assert_init\t= timer_fixup_assert_init,\n};\n\nstatic inline void debug_timer_init(struct timer_list *timer)\n{\n\tdebug_object_init(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_activate(struct timer_list *timer)\n{\n\tdebug_object_activate(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_deactivate(struct timer_list *timer)\n{\n\tdebug_object_deactivate(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_free(struct timer_list *timer)\n{\n\tdebug_object_free(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_assert_init(struct timer_list *timer)\n{\n\tdebug_object_assert_init(timer, &timer_debug_descr);\n}\n\nstatic void do_init_timer(struct timer_list *timer,\n\t\t\t  void (*func)(struct timer_list *),\n\t\t\t  unsigned int flags,\n\t\t\t  const char *name, struct lock_class_key *key);\n\nvoid init_timer_on_stack_key(struct timer_list *timer,\n\t\t\t     void (*func)(struct timer_list *),\n\t\t\t     unsigned int flags,\n\t\t\t     const char *name, struct lock_class_key *key)\n{\n\tdebug_object_init_on_stack(timer, &timer_debug_descr);\n\tdo_init_timer(timer, func, flags, name, key);\n}\nEXPORT_SYMBOL_GPL(init_timer_on_stack_key);\n\nvoid destroy_timer_on_stack(struct timer_list *timer)\n{\n\tdebug_object_free(timer, &timer_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_timer_on_stack);\n\n#else\nstatic inline void debug_timer_init(struct timer_list *timer) { }\nstatic inline void debug_timer_activate(struct timer_list *timer) { }\nstatic inline void debug_timer_deactivate(struct timer_list *timer) { }\nstatic inline void debug_timer_assert_init(struct timer_list *timer) { }\n#endif\n\nstatic inline void debug_init(struct timer_list *timer)\n{\n\tdebug_timer_init(timer);\n\ttrace_timer_init(timer);\n}\n\nstatic inline void debug_deactivate(struct timer_list *timer)\n{\n\tdebug_timer_deactivate(timer);\n\ttrace_timer_cancel(timer);\n}\n\nstatic inline void debug_assert_init(struct timer_list *timer)\n{\n\tdebug_timer_assert_init(timer);\n}\n\nstatic void do_init_timer(struct timer_list *timer,\n\t\t\t  void (*func)(struct timer_list *),\n\t\t\t  unsigned int flags,\n\t\t\t  const char *name, struct lock_class_key *key)\n{\n\ttimer->entry.pprev = NULL;\n\ttimer->function = func;\n\ttimer->flags = flags | raw_smp_processor_id();\n\tlockdep_init_map(&timer->lockdep_map, name, key, 0);\n}\n\n/**\n * init_timer_key - initialize a timer\n * @timer: the timer to be initialized\n * @func: timer callback function\n * @flags: timer flags\n * @name: name of the timer\n * @key: lockdep class key of the fake lock used for tracking timer\n *       sync lock dependencies\n *\n * init_timer_key() must be done to a timer prior calling *any* of the\n * other timer functions.\n */\nvoid init_timer_key(struct timer_list *timer,\n\t\t    void (*func)(struct timer_list *), unsigned int flags,\n\t\t    const char *name, struct lock_class_key *key)\n{\n\tdebug_init(timer);\n\tdo_init_timer(timer, func, flags, name, key);\n}\nEXPORT_SYMBOL(init_timer_key);\n\nstatic inline void detach_timer(struct timer_list *timer, bool clear_pending)\n{\n\tstruct hlist_node *entry = &timer->entry;\n\n\tdebug_deactivate(timer);\n\n\t__hlist_del(entry);\n\tif (clear_pending)\n\t\tentry->pprev = NULL;\n\tentry->next = LIST_POISON2;\n}\n\nstatic int detach_if_pending(struct timer_list *timer, struct timer_base *base,\n\t\t\t     bool clear_pending)\n{\n\tunsigned idx = timer_get_idx(timer);\n\n\tif (!timer_pending(timer))\n\t\treturn 0;\n\n\tif (hlist_is_singular_node(&timer->entry, base->vectors + idx))\n\t\t__clear_bit(idx, base->pending_map);\n\n\tdetach_timer(timer, clear_pending);\n\treturn 1;\n}\n\nstatic inline struct timer_base *get_timer_cpu_base(u32 tflags, u32 cpu)\n{\n\tstruct timer_base *base = per_cpu_ptr(&timer_bases[BASE_STD], cpu);\n\n\t/*\n\t * If the timer is deferrable and NO_HZ_COMMON is set then we need\n\t * to use the deferrable base.\n\t */\n\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON) && (tflags & TIMER_DEFERRABLE))\n\t\tbase = per_cpu_ptr(&timer_bases[BASE_DEF], cpu);\n\treturn base;\n}\n\nstatic inline struct timer_base *get_timer_this_cpu_base(u32 tflags)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\t/*\n\t * If the timer is deferrable and NO_HZ_COMMON is set then we need\n\t * to use the deferrable base.\n\t */\n\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON) && (tflags & TIMER_DEFERRABLE))\n\t\tbase = this_cpu_ptr(&timer_bases[BASE_DEF]);\n\treturn base;\n}\n\nstatic inline struct timer_base *get_timer_base(u32 tflags)\n{\n\treturn get_timer_cpu_base(tflags, tflags & TIMER_CPUMASK);\n}\n\nstatic inline struct timer_base *\nget_target_base(struct timer_base *base, unsigned tflags)\n{\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\n\tif (static_branch_likely(&timers_migration_enabled) &&\n\t    !(tflags & TIMER_PINNED))\n\t\treturn get_timer_cpu_base(tflags, get_nohz_timer_target());\n#endif\n\treturn get_timer_this_cpu_base(tflags);\n}\n\nstatic inline void forward_timer_base(struct timer_base *base)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tunsigned long jnow;\n\n\t/*\n\t * We only forward the base when we are idle or have just come out of\n\t * idle (must_forward_clk logic), and have a delta between base clock\n\t * and jiffies. In the common case, run_timers will take care of it.\n\t */\n\tif (likely(!base->must_forward_clk))\n\t\treturn;\n\n\tjnow = READ_ONCE(jiffies);\n\tbase->must_forward_clk = base->is_idle;\n\tif ((long)(jnow - base->clk) < 2)\n\t\treturn;\n\n\t/*\n\t * If the next expiry value is > jiffies, then we fast forward to\n\t * jiffies otherwise we forward to the next expiry value.\n\t */\n\tif (time_after(base->next_expiry, jnow)) {\n\t\tbase->clk = jnow;\n\t} else {\n\t\tif (WARN_ON_ONCE(time_before(base->next_expiry, base->clk)))\n\t\t\treturn;\n\t\tbase->clk = base->next_expiry;\n\t}\n#endif\n}\n\n\n/*\n * We are using hashed locking: Holding per_cpu(timer_bases[x]).lock means\n * that all timers which are tied to this base are locked, and the base itself\n * is locked too.\n *\n * So __run_timers/migrate_timers can safely modify all timers which could\n * be found in the base->vectors array.\n *\n * When a timer is migrating then the TIMER_MIGRATING flag is set and we need\n * to wait until the migration is done.\n */\nstatic struct timer_base *lock_timer_base(struct timer_list *timer,\n\t\t\t\t\t  unsigned long *flags)\n\t__acquires(timer->base->lock)\n{\n\tfor (;;) {\n\t\tstruct timer_base *base;\n\t\tu32 tf;\n\n\t\t/*\n\t\t * We need to use READ_ONCE() here, otherwise the compiler\n\t\t * might re-read @tf between the check for TIMER_MIGRATING\n\t\t * and spin_lock().\n\t\t */\n\t\ttf = READ_ONCE(timer->flags);\n\n\t\tif (!(tf & TIMER_MIGRATING)) {\n\t\t\tbase = get_timer_base(tf);\n\t\t\traw_spin_lock_irqsave(&base->lock, *flags);\n\t\t\tif (timer->flags == tf)\n\t\t\t\treturn base;\n\t\t\traw_spin_unlock_irqrestore(&base->lock, *flags);\n\t\t}\n\t\tcpu_relax();\n\t}\n}\n\n#define MOD_TIMER_PENDING_ONLY\t\t0x01\n#define MOD_TIMER_REDUCE\t\t0x02\n#define MOD_TIMER_NOTPENDING\t\t0x04\n\nstatic inline int\n__mod_timer(struct timer_list *timer, unsigned long expires, unsigned int options)\n{\n\tstruct timer_base *base, *new_base;\n\tunsigned int idx = UINT_MAX;\n\tunsigned long clk = 0, flags;\n\tint ret = 0;\n\n\tBUG_ON(!timer->function);\n\n\t/*\n\t * This is a common optimization triggered by the networking code - if\n\t * the timer is re-modified to have the same timeout or ends up in the\n\t * same array bucket then just return:\n\t */\n\tif (!(options & MOD_TIMER_NOTPENDING) && timer_pending(timer)) {\n\t\t/*\n\t\t * The downside of this optimization is that it can result in\n\t\t * larger granularity than you would get from adding a new\n\t\t * timer with this expiry.\n\t\t */\n\t\tlong diff = timer->expires - expires;\n\n\t\tif (!diff)\n\t\t\treturn 1;\n\t\tif (options & MOD_TIMER_REDUCE && diff <= 0)\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * We lock timer base and calculate the bucket index right\n\t\t * here. If the timer ends up in the same bucket, then we\n\t\t * just update the expiry time and avoid the whole\n\t\t * dequeue/enqueue dance.\n\t\t */\n\t\tbase = lock_timer_base(timer, &flags);\n\t\tforward_timer_base(base);\n\n\t\tif (timer_pending(timer) && (options & MOD_TIMER_REDUCE) &&\n\t\t    time_before_eq(timer->expires, expires)) {\n\t\t\tret = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tclk = base->clk;\n\t\tidx = calc_wheel_index(expires, clk);\n\n\t\t/*\n\t\t * Retrieve and compare the array index of the pending\n\t\t * timer. If it matches set the expiry to the new value so a\n\t\t * subsequent call will exit in the expires check above.\n\t\t */\n\t\tif (idx == timer_get_idx(timer)) {\n\t\t\tif (!(options & MOD_TIMER_REDUCE))\n\t\t\t\ttimer->expires = expires;\n\t\t\telse if (time_after(timer->expires, expires))\n\t\t\t\ttimer->expires = expires;\n\t\t\tret = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else {\n\t\tbase = lock_timer_base(timer, &flags);\n\t\tforward_timer_base(base);\n\t}\n\n\tret = detach_if_pending(timer, base, false);\n\tif (!ret && (options & MOD_TIMER_PENDING_ONLY))\n\t\tgoto out_unlock;\n\n\tnew_base = get_target_base(base, timer->flags);\n\n\tif (base != new_base) {\n\t\t/*\n\t\t * We are trying to schedule the timer on the new base.\n\t\t * However we can't change timer's base while it is running,\n\t\t * otherwise del_timer_sync() can't detect that the timer's\n\t\t * handler yet has not finished. This also guarantees that the\n\t\t * timer is serialized wrt itself.\n\t\t */\n\t\tif (likely(base->running_timer != timer)) {\n\t\t\t/* See the comment in lock_timer_base() */\n\t\t\ttimer->flags |= TIMER_MIGRATING;\n\n\t\t\traw_spin_unlock(&base->lock);\n\t\t\tbase = new_base;\n\t\t\traw_spin_lock(&base->lock);\n\t\t\tWRITE_ONCE(timer->flags,\n\t\t\t\t   (timer->flags & ~TIMER_BASEMASK) | base->cpu);\n\t\t\tforward_timer_base(base);\n\t\t}\n\t}\n\n\tdebug_timer_activate(timer);\n\n\ttimer->expires = expires;\n\t/*\n\t * If 'idx' was calculated above and the base time did not advance\n\t * between calculating 'idx' and possibly switching the base, only\n\t * enqueue_timer() and trigger_dyntick_cpu() is required. Otherwise\n\t * we need to (re)calculate the wheel index via\n\t * internal_add_timer().\n\t */\n\tif (idx != UINT_MAX && clk == base->clk) {\n\t\tenqueue_timer(base, timer, idx);\n\t\ttrigger_dyntick_cpu(base, timer);\n\t} else {\n\t\tinternal_add_timer(base, timer);\n\t}\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&base->lock, flags);\n\n\treturn ret;\n}\n\n/**\n * mod_timer_pending - modify a pending timer's timeout\n * @timer: the pending timer to be modified\n * @expires: new timeout in jiffies\n *\n * mod_timer_pending() is the same for pending timers as mod_timer(),\n * but will not re-activate and modify already deleted timers.\n *\n * It is useful for unserialized use of timers.\n */\nint mod_timer_pending(struct timer_list *timer, unsigned long expires)\n{\n\treturn __mod_timer(timer, expires, MOD_TIMER_PENDING_ONLY);\n}\nEXPORT_SYMBOL(mod_timer_pending);\n\n/**\n * mod_timer - modify a timer's timeout\n * @timer: the timer to be modified\n * @expires: new timeout in jiffies\n *\n * mod_timer() is a more efficient way to update the expire field of an\n * active timer (if the timer is inactive it will be activated)\n *\n * mod_timer(timer, expires) is equivalent to:\n *\n *     del_timer(timer); timer->expires = expires; add_timer(timer);\n *\n * Note that if there are multiple unserialized concurrent users of the\n * same timer, then mod_timer() is the only safe way to modify the timeout,\n * since add_timer() cannot modify an already running timer.\n *\n * The function returns whether it has modified a pending timer or not.\n * (ie. mod_timer() of an inactive timer returns 0, mod_timer() of an\n * active timer returns 1.)\n */\nint mod_timer(struct timer_list *timer, unsigned long expires)\n{\n\treturn __mod_timer(timer, expires, 0);\n}\nEXPORT_SYMBOL(mod_timer);\n\n/**\n * timer_reduce - Modify a timer's timeout if it would reduce the timeout\n * @timer:\tThe timer to be modified\n * @expires:\tNew timeout in jiffies\n *\n * timer_reduce() is very similar to mod_timer(), except that it will only\n * modify a running timer if that would reduce the expiration time (it will\n * start a timer that isn't running).\n */\nint timer_reduce(struct timer_list *timer, unsigned long expires)\n{\n\treturn __mod_timer(timer, expires, MOD_TIMER_REDUCE);\n}\nEXPORT_SYMBOL(timer_reduce);\n\n/**\n * add_timer - start a timer\n * @timer: the timer to be added\n *\n * The kernel will do a ->function(@timer) callback from the\n * timer interrupt at the ->expires point in the future. The\n * current time is 'jiffies'.\n *\n * The timer's ->expires, ->function fields must be set prior calling this\n * function.\n *\n * Timers with an ->expires field in the past will be executed in the next\n * timer tick.\n */\nvoid add_timer(struct timer_list *timer)\n{\n\tBUG_ON(timer_pending(timer));\n\t__mod_timer(timer, timer->expires, MOD_TIMER_NOTPENDING);\n}\nEXPORT_SYMBOL(add_timer);\n\n/**\n * add_timer_on - start a timer on a particular CPU\n * @timer: the timer to be added\n * @cpu: the CPU to start it on\n *\n * This is not very scalable on SMP. Double adds are not possible.\n */\nvoid add_timer_on(struct timer_list *timer, int cpu)\n{\n\tstruct timer_base *new_base, *base;\n\tunsigned long flags;\n\n\tBUG_ON(timer_pending(timer) || !timer->function);\n\n\tnew_base = get_timer_cpu_base(timer->flags, cpu);\n\n\t/*\n\t * If @timer was on a different CPU, it should be migrated with the\n\t * old base locked to prevent other operations proceeding with the\n\t * wrong base locked.  See lock_timer_base().\n\t */\n\tbase = lock_timer_base(timer, &flags);\n\tif (base != new_base) {\n\t\ttimer->flags |= TIMER_MIGRATING;\n\n\t\traw_spin_unlock(&base->lock);\n\t\tbase = new_base;\n\t\traw_spin_lock(&base->lock);\n\t\tWRITE_ONCE(timer->flags,\n\t\t\t   (timer->flags & ~TIMER_BASEMASK) | cpu);\n\t}\n\tforward_timer_base(base);\n\n\tdebug_timer_activate(timer);\n\tinternal_add_timer(base, timer);\n\traw_spin_unlock_irqrestore(&base->lock, flags);\n}\nEXPORT_SYMBOL_GPL(add_timer_on);\n\n/**\n * del_timer - deactivate a timer.\n * @timer: the timer to be deactivated\n *\n * del_timer() deactivates a timer - this works on both active and inactive\n * timers.\n *\n * The function returns whether it has deactivated a pending timer or not.\n * (ie. del_timer() of an inactive timer returns 0, del_timer() of an\n * active timer returns 1.)\n */\nint del_timer(struct timer_list *timer)\n{\n\tstruct timer_base *base;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tdebug_assert_init(timer);\n\n\tif (timer_pending(timer)) {\n\t\tbase = lock_timer_base(timer, &flags);\n\t\tret = detach_if_pending(timer, base, true);\n\t\traw_spin_unlock_irqrestore(&base->lock, flags);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(del_timer);\n\n/**\n * try_to_del_timer_sync - Try to deactivate a timer\n * @timer: timer to delete\n *\n * This function tries to deactivate a timer. Upon successful (ret >= 0)\n * exit the timer is not queued and the handler is not running on any CPU.\n */\nint try_to_del_timer_sync(struct timer_list *timer)\n{\n\tstruct timer_base *base;\n\tunsigned long flags;\n\tint ret = -1;\n\n\tdebug_assert_init(timer);\n\n\tbase = lock_timer_base(timer, &flags);\n\n\tif (base->running_timer != timer)\n\t\tret = detach_if_pending(timer, base, true);\n\n\traw_spin_unlock_irqrestore(&base->lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(try_to_del_timer_sync);\n\n#ifdef CONFIG_PREEMPT_RT\nstatic __init void timer_base_init_expiry_lock(struct timer_base *base)\n{\n\tspin_lock_init(&base->expiry_lock);\n}\n\nstatic inline void timer_base_lock_expiry(struct timer_base *base)\n{\n\tspin_lock(&base->expiry_lock);\n}\n\nstatic inline void timer_base_unlock_expiry(struct timer_base *base)\n{\n\tspin_unlock(&base->expiry_lock);\n}\n\n/*\n * The counterpart to del_timer_wait_running().\n *\n * If there is a waiter for base->expiry_lock, then it was waiting for the\n * timer callback to finish. Drop expiry_lock and reaquire it. That allows\n * the waiter to acquire the lock and make progress.\n */\nstatic void timer_sync_wait_running(struct timer_base *base)\n{\n\tif (atomic_read(&base->timer_waiters)) {\n\t\tspin_unlock(&base->expiry_lock);\n\t\tspin_lock(&base->expiry_lock);\n\t}\n}\n\n/*\n * This function is called on PREEMPT_RT kernels when the fast path\n * deletion of a timer failed because the timer callback function was\n * running.\n *\n * This prevents priority inversion, if the softirq thread on a remote CPU\n * got preempted, and it prevents a life lock when the task which tries to\n * delete a timer preempted the softirq thread running the timer callback\n * function.\n */\nstatic void del_timer_wait_running(struct timer_list *timer)\n{\n\tu32 tf;\n\n\ttf = READ_ONCE(timer->flags);\n\tif (!(tf & TIMER_MIGRATING)) {\n\t\tstruct timer_base *base = get_timer_base(tf);\n\n\t\t/*\n\t\t * Mark the base as contended and grab the expiry lock,\n\t\t * which is held by the softirq across the timer\n\t\t * callback. Drop the lock immediately so the softirq can\n\t\t * expire the next timer. In theory the timer could already\n\t\t * be running again, but that's more than unlikely and just\n\t\t * causes another wait loop.\n\t\t */\n\t\tatomic_inc(&base->timer_waiters);\n\t\tspin_lock_bh(&base->expiry_lock);\n\t\tatomic_dec(&base->timer_waiters);\n\t\tspin_unlock_bh(&base->expiry_lock);\n\t}\n}\n#else\nstatic inline void timer_base_init_expiry_lock(struct timer_base *base) { }\nstatic inline void timer_base_lock_expiry(struct timer_base *base) { }\nstatic inline void timer_base_unlock_expiry(struct timer_base *base) { }\nstatic inline void timer_sync_wait_running(struct timer_base *base) { }\nstatic inline void del_timer_wait_running(struct timer_list *timer) { }\n#endif\n\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)\n/**\n * del_timer_sync - deactivate a timer and wait for the handler to finish.\n * @timer: the timer to be deactivated\n *\n * This function only differs from del_timer() on SMP: besides deactivating\n * the timer it also makes sure the handler has finished executing on other\n * CPUs.\n *\n * Synchronization rules: Callers must prevent restarting of the timer,\n * otherwise this function is meaningless. It must not be called from\n * interrupt contexts unless the timer is an irqsafe one. The caller must\n * not hold locks which would prevent completion of the timer's\n * handler. The timer's handler must not call add_timer_on(). Upon exit the\n * timer is not queued and the handler is not running on any CPU.\n *\n * Note: For !irqsafe timers, you must not hold locks that are held in\n *   interrupt context while calling this function. Even if the lock has\n *   nothing to do with the timer in question.  Here's why::\n *\n *    CPU0                             CPU1\n *    ----                             ----\n *                                     <SOFTIRQ>\n *                                       call_timer_fn();\n *                                       base->running_timer = mytimer;\n *    spin_lock_irq(somelock);\n *                                     <IRQ>\n *                                        spin_lock(somelock);\n *    del_timer_sync(mytimer);\n *    while (base->running_timer == mytimer);\n *\n * Now del_timer_sync() will never return and never release somelock.\n * The interrupt on the other CPU is waiting to grab somelock but\n * it has interrupted the softirq that CPU0 is waiting to finish.\n *\n * The function returns whether it has deactivated a pending timer or not.\n */\nint del_timer_sync(struct timer_list *timer)\n{\n\tint ret;\n\n#ifdef CONFIG_LOCKDEP\n\tunsigned long flags;\n\n\t/*\n\t * If lockdep gives a backtrace here, please reference\n\t * the synchronization rules above.\n\t */\n\tlocal_irq_save(flags);\n\tlock_map_acquire(&timer->lockdep_map);\n\tlock_map_release(&timer->lockdep_map);\n\tlocal_irq_restore(flags);\n#endif\n\t/*\n\t * don't use it in hardirq context, because it\n\t * could lead to deadlock.\n\t */\n\tWARN_ON(in_irq() && !(timer->flags & TIMER_IRQSAFE));\n\n\tdo {\n\t\tret = try_to_del_timer_sync(timer);\n\n\t\tif (unlikely(ret < 0)) {\n\t\t\tdel_timer_wait_running(timer);\n\t\t\tcpu_relax();\n\t\t}\n\t} while (ret < 0);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(del_timer_sync);\n#endif\n\nstatic void call_timer_fn(struct timer_list *timer,\n\t\t\t  void (*fn)(struct timer_list *),\n\t\t\t  unsigned long baseclk)\n{\n\tint count = preempt_count();\n\n#ifdef CONFIG_LOCKDEP\n\t/*\n\t * It is permissible to free the timer from inside the\n\t * function that is called from it, this we need to take into\n\t * account for lockdep too. To avoid bogus \"held lock freed\"\n\t * warnings as well as problems when looking into\n\t * timer->lockdep_map, make a copy and use that here.\n\t */\n\tstruct lockdep_map lockdep_map;\n\n\tlockdep_copy_map(&lockdep_map, &timer->lockdep_map);\n#endif\n\t/*\n\t * Couple the lock chain with the lock chain at\n\t * del_timer_sync() by acquiring the lock_map around the fn()\n\t * call here and in del_timer_sync().\n\t */\n\tlock_map_acquire(&lockdep_map);\n\n\ttrace_timer_expire_entry(timer, baseclk);\n\tfn(timer);\n\ttrace_timer_expire_exit(timer);\n\n\tlock_map_release(&lockdep_map);\n\n\tif (count != preempt_count()) {\n\t\tWARN_ONCE(1, \"timer: %pS preempt leak: %08x -> %08x\\n\",\n\t\t\t  fn, count, preempt_count());\n\t\t/*\n\t\t * Restore the preempt count. That gives us a decent\n\t\t * chance to survive and extract information. If the\n\t\t * callback kept a lock held, bad luck, but not worse\n\t\t * than the BUG() we had.\n\t\t */\n\t\tpreempt_count_set(count);\n\t}\n}\n\nstatic void expire_timers(struct timer_base *base, struct hlist_head *head)\n{\n\t/*\n\t * This value is required only for tracing. base->clk was\n\t * incremented directly before expire_timers was called. But expiry\n\t * is related to the old base->clk value.\n\t */\n\tunsigned long baseclk = base->clk - 1;\n\n\twhile (!hlist_empty(head)) {\n\t\tstruct timer_list *timer;\n\t\tvoid (*fn)(struct timer_list *);\n\n\t\ttimer = hlist_entry(head->first, struct timer_list, entry);\n\n\t\tbase->running_timer = timer;\n\t\tdetach_timer(timer, true);\n\n\t\tfn = timer->function;\n\n\t\tif (timer->flags & TIMER_IRQSAFE) {\n\t\t\traw_spin_unlock(&base->lock);\n\t\t\tcall_timer_fn(timer, fn, baseclk);\n\t\t\tbase->running_timer = NULL;\n\t\t\traw_spin_lock(&base->lock);\n\t\t} else {\n\t\t\traw_spin_unlock_irq(&base->lock);\n\t\t\tcall_timer_fn(timer, fn, baseclk);\n\t\t\tbase->running_timer = NULL;\n\t\t\ttimer_sync_wait_running(base);\n\t\t\traw_spin_lock_irq(&base->lock);\n\t\t}\n\t}\n}\n\nstatic int __collect_expired_timers(struct timer_base *base,\n\t\t\t\t    struct hlist_head *heads)\n{\n\tunsigned long clk = base->clk;\n\tstruct hlist_head *vec;\n\tint i, levels = 0;\n\tunsigned int idx;\n\n\tfor (i = 0; i < LVL_DEPTH; i++) {\n\t\tidx = (clk & LVL_MASK) + i * LVL_SIZE;\n\n\t\tif (__test_and_clear_bit(idx, base->pending_map)) {\n\t\t\tvec = base->vectors + idx;\n\t\t\thlist_move_list(vec, heads++);\n\t\t\tlevels++;\n\t\t}\n\t\t/* Is it time to look at the next level? */\n\t\tif (clk & LVL_CLK_MASK)\n\t\t\tbreak;\n\t\t/* Shift clock for the next level granularity */\n\t\tclk >>= LVL_CLK_SHIFT;\n\t}\n\treturn levels;\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * Find the next pending bucket of a level. Search from level start (@offset)\n * + @clk upwards and if nothing there, search from start of the level\n * (@offset) up to @offset + clk.\n */\nstatic int next_pending_bucket(struct timer_base *base, unsigned offset,\n\t\t\t       unsigned clk)\n{\n\tunsigned pos, start = offset + clk;\n\tunsigned end = offset + LVL_SIZE;\n\n\tpos = find_next_bit(base->pending_map, end, start);\n\tif (pos < end)\n\t\treturn pos - start;\n\n\tpos = find_next_bit(base->pending_map, start, offset);\n\treturn pos < start ? pos + LVL_SIZE - start : -1;\n}\n\n/*\n * Search the first expiring timer in the various clock levels. Caller must\n * hold base->lock.\n */\nstatic unsigned long __next_timer_interrupt(struct timer_base *base)\n{\n\tunsigned long clk, next, adj;\n\tunsigned lvl, offset = 0;\n\n\tnext = base->clk + NEXT_TIMER_MAX_DELTA;\n\tclk = base->clk;\n\tfor (lvl = 0; lvl < LVL_DEPTH; lvl++, offset += LVL_SIZE) {\n\t\tint pos = next_pending_bucket(base, offset, clk & LVL_MASK);\n\n\t\tif (pos >= 0) {\n\t\t\tunsigned long tmp = clk + (unsigned long) pos;\n\n\t\t\ttmp <<= LVL_SHIFT(lvl);\n\t\t\tif (time_before(tmp, next))\n\t\t\t\tnext = tmp;\n\t\t}\n\t\t/*\n\t\t * Clock for the next level. If the current level clock lower\n\t\t * bits are zero, we look at the next level as is. If not we\n\t\t * need to advance it by one because that's going to be the\n\t\t * next expiring bucket in that level. base->clk is the next\n\t\t * expiring jiffie. So in case of:\n\t\t *\n\t\t * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0\n\t\t *  0    0    0    0    0    0\n\t\t *\n\t\t * we have to look at all levels @index 0. With\n\t\t *\n\t\t * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0\n\t\t *  0    0    0    0    0    2\n\t\t *\n\t\t * LVL0 has the next expiring bucket @index 2. The upper\n\t\t * levels have the next expiring bucket @index 1.\n\t\t *\n\t\t * In case that the propagation wraps the next level the same\n\t\t * rules apply:\n\t\t *\n\t\t * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0\n\t\t *  0    0    0    0    F    2\n\t\t *\n\t\t * So after looking at LVL0 we get:\n\t\t *\n\t\t * LVL5 LVL4 LVL3 LVL2 LVL1\n\t\t *  0    0    0    1    0\n\t\t *\n\t\t * So no propagation from LVL1 to LVL2 because that happened\n\t\t * with the add already, but then we need to propagate further\n\t\t * from LVL2 to LVL3.\n\t\t *\n\t\t * So the simple check whether the lower bits of the current\n\t\t * level are 0 or not is sufficient for all cases.\n\t\t */\n\t\tadj = clk & LVL_CLK_MASK ? 1 : 0;\n\t\tclk >>= LVL_CLK_SHIFT;\n\t\tclk += adj;\n\t}\n\treturn next;\n}\n\n/*\n * Check, if the next hrtimer event is before the next timer wheel\n * event:\n */\nstatic u64 cmp_next_hrtimer_event(u64 basem, u64 expires)\n{\n\tu64 nextevt = hrtimer_get_next_event();\n\n\t/*\n\t * If high resolution timers are enabled\n\t * hrtimer_get_next_event() returns KTIME_MAX.\n\t */\n\tif (expires <= nextevt)\n\t\treturn expires;\n\n\t/*\n\t * If the next timer is already expired, return the tick base\n\t * time so the tick is fired immediately.\n\t */\n\tif (nextevt <= basem)\n\t\treturn basem;\n\n\t/*\n\t * Round up to the next jiffie. High resolution timers are\n\t * off, so the hrtimers are expired in the tick and we need to\n\t * make sure that this tick really expires the timer to avoid\n\t * a ping pong of the nohz stop code.\n\t *\n\t * Use DIV_ROUND_UP_ULL to prevent gcc calling __divdi3\n\t */\n\treturn DIV_ROUND_UP_ULL(nextevt, TICK_NSEC) * TICK_NSEC;\n}\n\n/**\n * get_next_timer_interrupt - return the time (clock mono) of the next timer\n * @basej:\tbase time jiffies\n * @basem:\tbase time clock monotonic\n *\n * Returns the tick aligned clock monotonic time of the next pending\n * timer or KTIME_MAX if no timer is pending.\n */\nu64 get_next_timer_interrupt(unsigned long basej, u64 basem)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\tu64 expires = KTIME_MAX;\n\tunsigned long nextevt;\n\tbool is_max_delta;\n\n\t/*\n\t * Pretend that there is no timer pending if the cpu is offline.\n\t * Possible pending timers will be migrated later to an active cpu.\n\t */\n\tif (cpu_is_offline(smp_processor_id()))\n\t\treturn expires;\n\n\traw_spin_lock(&base->lock);\n\tnextevt = __next_timer_interrupt(base);\n\tis_max_delta = (nextevt == base->clk + NEXT_TIMER_MAX_DELTA);\n\tbase->next_expiry = nextevt;\n\t/*\n\t * We have a fresh next event. Check whether we can forward the\n\t * base. We can only do that when @basej is past base->clk\n\t * otherwise we might rewind base->clk.\n\t */\n\tif (time_after(basej, base->clk)) {\n\t\tif (time_after(nextevt, basej))\n\t\t\tbase->clk = basej;\n\t\telse if (time_after(nextevt, base->clk))\n\t\t\tbase->clk = nextevt;\n\t}\n\n\tif (time_before_eq(nextevt, basej)) {\n\t\texpires = basem;\n\t\tbase->is_idle = false;\n\t} else {\n\t\tif (!is_max_delta)\n\t\t\texpires = basem + (u64)(nextevt - basej) * TICK_NSEC;\n\t\t/*\n\t\t * If we expect to sleep more than a tick, mark the base idle.\n\t\t * Also the tick is stopped so any added timer must forward\n\t\t * the base clk itself to keep granularity small. This idle\n\t\t * logic is only maintained for the BASE_STD base, deferrable\n\t\t * timers may still see large granularity skew (by design).\n\t\t */\n\t\tif ((expires - basem) > TICK_NSEC) {\n\t\t\tbase->must_forward_clk = true;\n\t\t\tbase->is_idle = true;\n\t\t}\n\t}\n\traw_spin_unlock(&base->lock);\n\n\treturn cmp_next_hrtimer_event(basem, expires);\n}\n\n/**\n * timer_clear_idle - Clear the idle state of the timer base\n *\n * Called with interrupts disabled\n */\nvoid timer_clear_idle(void)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\t/*\n\t * We do this unlocked. The worst outcome is a remote enqueue sending\n\t * a pointless IPI, but taking the lock would just make the window for\n\t * sending the IPI a few instructions smaller for the cost of taking\n\t * the lock in the exit from idle path.\n\t */\n\tbase->is_idle = false;\n}\n\nstatic int collect_expired_timers(struct timer_base *base,\n\t\t\t\t  struct hlist_head *heads)\n{\n\tunsigned long now = READ_ONCE(jiffies);\n\n\t/*\n\t * NOHZ optimization. After a long idle sleep we need to forward the\n\t * base to current jiffies. Avoid a loop by searching the bitfield for\n\t * the next expiring timer.\n\t */\n\tif ((long)(now - base->clk) > 2) {\n\t\tunsigned long next = __next_timer_interrupt(base);\n\n\t\t/*\n\t\t * If the next timer is ahead of time forward to current\n\t\t * jiffies, otherwise forward to the next expiry time:\n\t\t */\n\t\tif (time_after(next, now)) {\n\t\t\t/*\n\t\t\t * The call site will increment base->clk and then\n\t\t\t * terminate the expiry loop immediately.\n\t\t\t */\n\t\t\tbase->clk = now;\n\t\t\treturn 0;\n\t\t}\n\t\tbase->clk = next;\n\t}\n\treturn __collect_expired_timers(base, heads);\n}\n#else\nstatic inline int collect_expired_timers(struct timer_base *base,\n\t\t\t\t\t struct hlist_head *heads)\n{\n\treturn __collect_expired_timers(base, heads);\n}\n#endif\n\n/*\n * Called from the timer interrupt handler to charge one tick to the current\n * process.  user_tick is 1 if the tick is user time, 0 for system.\n */\nvoid update_process_times(int user_tick)\n{\n\tstruct task_struct *p = current;\n\n\t/* Note: this timer irq context must be accounted for as well. */\n\taccount_process_tick(p, user_tick);\n\trun_local_timers();\n\trcu_sched_clock_irq(user_tick);\n#ifdef CONFIG_IRQ_WORK\n\tif (in_irq())\n\t\tirq_work_tick();\n#endif\n\tscheduler_tick();\n\tif (IS_ENABLED(CONFIG_POSIX_TIMERS))\n\t\trun_posix_cpu_timers();\n}\n\n/**\n * __run_timers - run all expired timers (if any) on this CPU.\n * @base: the timer vector to be processed.\n */\nstatic inline void __run_timers(struct timer_base *base)\n{\n\tstruct hlist_head heads[LVL_DEPTH];\n\tint levels;\n\n\tif (!time_after_eq(jiffies, base->clk))\n\t\treturn;\n\n\ttimer_base_lock_expiry(base);\n\traw_spin_lock_irq(&base->lock);\n\n\t/*\n\t * timer_base::must_forward_clk must be cleared before running\n\t * timers so that any timer functions that call mod_timer() will\n\t * not try to forward the base. Idle tracking / clock forwarding\n\t * logic is only used with BASE_STD timers.\n\t *\n\t * The must_forward_clk flag is cleared unconditionally also for\n\t * the deferrable base. The deferrable base is not affected by idle\n\t * tracking and never forwarded, so clearing the flag is a NOOP.\n\t *\n\t * The fact that the deferrable base is never forwarded can cause\n\t * large variations in granularity for deferrable timers, but they\n\t * can be deferred for long periods due to idle anyway.\n\t */\n\tbase->must_forward_clk = false;\n\n\twhile (time_after_eq(jiffies, base->clk)) {\n\n\t\tlevels = collect_expired_timers(base, heads);\n\t\tbase->clk++;\n\n\t\twhile (levels--)\n\t\t\texpire_timers(base, heads + levels);\n\t}\n\traw_spin_unlock_irq(&base->lock);\n\ttimer_base_unlock_expiry(base);\n}\n\n/*\n * This function runs timers and the timer-tq in bottom half context.\n */\nstatic __latent_entropy void run_timer_softirq(struct softirq_action *h)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\t__run_timers(base);\n\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON))\n\t\t__run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));\n}\n\n/*\n * Called by the local, per-CPU timer interrupt on SMP.\n */\nvoid run_local_timers(void)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\thrtimer_run_queues();\n\t/* Raise the softirq only if required. */\n\tif (time_before(jiffies, base->clk)) {\n\t\tif (!IS_ENABLED(CONFIG_NO_HZ_COMMON))\n\t\t\treturn;\n\t\t/* CPU is awake, so check the deferrable base. */\n\t\tbase++;\n\t\tif (time_before(jiffies, base->clk))\n\t\t\treturn;\n\t}\n\traise_softirq(TIMER_SOFTIRQ);\n}\n\n/*\n * Since schedule_timeout()'s timer is defined on the stack, it must store\n * the target task on the stack as well.\n */\nstruct process_timer {\n\tstruct timer_list timer;\n\tstruct task_struct *task;\n};\n\nstatic void process_timeout(struct timer_list *t)\n{\n\tstruct process_timer *timeout = from_timer(timeout, t, timer);\n\n\twake_up_process(timeout->task);\n}\n\n/**\n * schedule_timeout - sleep until timeout\n * @timeout: timeout value in jiffies\n *\n * Make the current task sleep until @timeout jiffies have elapsed.\n * The function behavior depends on the current task state\n * (see also set_current_state() description):\n *\n * %TASK_RUNNING - the scheduler is called, but the task does not sleep\n * at all. That happens because sched_submit_work() does nothing for\n * tasks in %TASK_RUNNING state.\n *\n * %TASK_UNINTERRUPTIBLE - at least @timeout jiffies are guaranteed to\n * pass before the routine returns unless the current task is explicitly\n * woken up, (e.g. by wake_up_process()).\n *\n * %TASK_INTERRUPTIBLE - the routine may return early if a signal is\n * delivered to the current task or the current task is explicitly woken\n * up.\n *\n * The current task state is guaranteed to be %TASK_RUNNING when this\n * routine returns.\n *\n * Specifying a @timeout value of %MAX_SCHEDULE_TIMEOUT will schedule\n * the CPU away without a bound on the timeout. In this case the return\n * value will be %MAX_SCHEDULE_TIMEOUT.\n *\n * Returns 0 when the timer has expired otherwise the remaining time in\n * jiffies will be returned. In all cases the return value is guaranteed\n * to be non-negative.\n */\nsigned long __sched schedule_timeout(signed long timeout)\n{\n\tstruct process_timer timer;\n\tunsigned long expire;\n\n\tswitch (timeout)\n\t{\n\tcase MAX_SCHEDULE_TIMEOUT:\n\t\t/*\n\t\t * These two special cases are useful to be comfortable\n\t\t * in the caller. Nothing more. We could take\n\t\t * MAX_SCHEDULE_TIMEOUT from one of the negative value\n\t\t * but I' d like to return a valid offset (>=0) to allow\n\t\t * the caller to do everything it want with the retval.\n\t\t */\n\t\tschedule();\n\t\tgoto out;\n\tdefault:\n\t\t/*\n\t\t * Another bit of PARANOID. Note that the retval will be\n\t\t * 0 since no piece of kernel is supposed to do a check\n\t\t * for a negative retval of schedule_timeout() (since it\n\t\t * should never happens anyway). You just have the printk()\n\t\t * that will tell you if something is gone wrong and where.\n\t\t */\n\t\tif (timeout < 0) {\n\t\t\tprintk(KERN_ERR \"schedule_timeout: wrong timeout \"\n\t\t\t\t\"value %lx\\n\", timeout);\n\t\t\tdump_stack();\n\t\t\tcurrent->state = TASK_RUNNING;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\texpire = timeout + jiffies;\n\n\ttimer.task = current;\n\ttimer_setup_on_stack(&timer.timer, process_timeout, 0);\n\t__mod_timer(&timer.timer, expire, MOD_TIMER_NOTPENDING);\n\tschedule();\n\tdel_singleshot_timer_sync(&timer.timer);\n\n\t/* Remove the timer from the object tracker */\n\tdestroy_timer_on_stack(&timer.timer);\n\n\ttimeout = expire - jiffies;\n\n out:\n\treturn timeout < 0 ? 0 : timeout;\n}\nEXPORT_SYMBOL(schedule_timeout);\n\n/*\n * We can use __set_current_state() here because schedule_timeout() calls\n * schedule() unconditionally.\n */\nsigned long __sched schedule_timeout_interruptible(signed long timeout)\n{\n\t__set_current_state(TASK_INTERRUPTIBLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_interruptible);\n\nsigned long __sched schedule_timeout_killable(signed long timeout)\n{\n\t__set_current_state(TASK_KILLABLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_killable);\n\nsigned long __sched schedule_timeout_uninterruptible(signed long timeout)\n{\n\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_uninterruptible);\n\n/*\n * Like schedule_timeout_uninterruptible(), except this task will not contribute\n * to load average.\n */\nsigned long __sched schedule_timeout_idle(signed long timeout)\n{\n\t__set_current_state(TASK_IDLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_idle);\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic void migrate_timer_list(struct timer_base *new_base, struct hlist_head *head)\n{\n\tstruct timer_list *timer;\n\tint cpu = new_base->cpu;\n\n\twhile (!hlist_empty(head)) {\n\t\ttimer = hlist_entry(head->first, struct timer_list, entry);\n\t\tdetach_timer(timer, false);\n\t\ttimer->flags = (timer->flags & ~TIMER_BASEMASK) | cpu;\n\t\tinternal_add_timer(new_base, timer);\n\t}\n}\n\nint timers_prepare_cpu(unsigned int cpu)\n{\n\tstruct timer_base *base;\n\tint b;\n\n\tfor (b = 0; b < NR_BASES; b++) {\n\t\tbase = per_cpu_ptr(&timer_bases[b], cpu);\n\t\tbase->clk = jiffies;\n\t\tbase->next_expiry = base->clk + NEXT_TIMER_MAX_DELTA;\n\t\tbase->is_idle = false;\n\t\tbase->must_forward_clk = true;\n\t}\n\treturn 0;\n}\n\nint timers_dead_cpu(unsigned int cpu)\n{\n\tstruct timer_base *old_base;\n\tstruct timer_base *new_base;\n\tint b, i;\n\n\tBUG_ON(cpu_online(cpu));\n\n\tfor (b = 0; b < NR_BASES; b++) {\n\t\told_base = per_cpu_ptr(&timer_bases[b], cpu);\n\t\tnew_base = get_cpu_ptr(&timer_bases[b]);\n\t\t/*\n\t\t * The caller is globally serialized and nobody else\n\t\t * takes two locks at once, deadlock is not possible.\n\t\t */\n\t\traw_spin_lock_irq(&new_base->lock);\n\t\traw_spin_lock_nested(&old_base->lock, SINGLE_DEPTH_NESTING);\n\n\t\t/*\n\t\t * The current CPUs base clock might be stale. Update it\n\t\t * before moving the timers over.\n\t\t */\n\t\tforward_timer_base(new_base);\n\n\t\tBUG_ON(old_base->running_timer);\n\n\t\tfor (i = 0; i < WHEEL_SIZE; i++)\n\t\t\tmigrate_timer_list(new_base, old_base->vectors + i);\n\n\t\traw_spin_unlock(&old_base->lock);\n\t\traw_spin_unlock_irq(&new_base->lock);\n\t\tput_cpu_ptr(&timer_bases);\n\t}\n\treturn 0;\n}\n\n#endif /* CONFIG_HOTPLUG_CPU */\n\nstatic void __init init_timer_cpu(int cpu)\n{\n\tstruct timer_base *base;\n\tint i;\n\n\tfor (i = 0; i < NR_BASES; i++) {\n\t\tbase = per_cpu_ptr(&timer_bases[i], cpu);\n\t\tbase->cpu = cpu;\n\t\traw_spin_lock_init(&base->lock);\n\t\tbase->clk = jiffies;\n\t\ttimer_base_init_expiry_lock(base);\n\t}\n}\n\nstatic void __init init_timer_cpus(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tinit_timer_cpu(cpu);\n}\n\nvoid __init init_timers(void)\n{\n\tinit_timer_cpus();\n\topen_softirq(TIMER_SOFTIRQ, run_timer_softirq);\n}\n\n/**\n * msleep - sleep safely even with waitqueue interruptions\n * @msecs: Time in milliseconds to sleep for\n */\nvoid msleep(unsigned int msecs)\n{\n\tunsigned long timeout = msecs_to_jiffies(msecs) + 1;\n\n\twhile (timeout)\n\t\ttimeout = schedule_timeout_uninterruptible(timeout);\n}\n\nEXPORT_SYMBOL(msleep);\n\n/**\n * msleep_interruptible - sleep waiting for signals\n * @msecs: Time in milliseconds to sleep for\n */\nunsigned long msleep_interruptible(unsigned int msecs)\n{\n\tunsigned long timeout = msecs_to_jiffies(msecs) + 1;\n\n\twhile (timeout && !signal_pending(current))\n\t\ttimeout = schedule_timeout_interruptible(timeout);\n\treturn jiffies_to_msecs(timeout);\n}\n\nEXPORT_SYMBOL(msleep_interruptible);\n\n/**\n * usleep_range - Sleep for an approximate time\n * @min: Minimum time in usecs to sleep\n * @max: Maximum time in usecs to sleep\n *\n * In non-atomic context where the exact wakeup time is flexible, use\n * usleep_range() instead of udelay().  The sleep improves responsiveness\n * by avoiding the CPU-hogging busy-wait of udelay(), and the range reduces\n * power usage by allowing hrtimers to take advantage of an already-\n * scheduled interrupt instead of scheduling a new one just for this sleep.\n */\nvoid __sched usleep_range(unsigned long min, unsigned long max)\n{\n\tktime_t exp = ktime_add_us(ktime_get(), min);\n\tu64 delta = (u64)(max - min) * NSEC_PER_USEC;\n\n\tfor (;;) {\n\t\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\t\t/* Do not return before the requested sleep time has elapsed */\n\t\tif (!schedule_hrtimeout_range(&exp, delta, HRTIMER_MODE_ABS))\n\t\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL(usleep_range);\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * This is a maximally equidistributed combined Tausworthe generator\n * based on code from GNU Scientific Library 1.5 (30 Jun 2004)\n *\n * lfsr113 version:\n *\n * x_n = (s1_n ^ s2_n ^ s3_n ^ s4_n)\n *\n * s1_{n+1} = (((s1_n & 4294967294) << 18) ^ (((s1_n <<  6) ^ s1_n) >> 13))\n * s2_{n+1} = (((s2_n & 4294967288) <<  2) ^ (((s2_n <<  2) ^ s2_n) >> 27))\n * s3_{n+1} = (((s3_n & 4294967280) <<  7) ^ (((s3_n << 13) ^ s3_n) >> 21))\n * s4_{n+1} = (((s4_n & 4294967168) << 13) ^ (((s4_n <<  3) ^ s4_n) >> 12))\n *\n * The period of this generator is about 2^113 (see erratum paper).\n *\n * From: P. L'Ecuyer, \"Maximally Equidistributed Combined Tausworthe\n * Generators\", Mathematics of Computation, 65, 213 (1996), 203--213:\n * http://www.iro.umontreal.ca/~lecuyer/myftp/papers/tausme.ps\n * ftp://ftp.iro.umontreal.ca/pub/simulation/lecuyer/papers/tausme.ps\n *\n * There is an erratum in the paper \"Tables of Maximally Equidistributed\n * Combined LFSR Generators\", Mathematics of Computation, 68, 225 (1999),\n * 261--269: http://www.iro.umontreal.ca/~lecuyer/myftp/papers/tausme2.ps\n *\n *      ... the k_j most significant bits of z_j must be non-zero,\n *      for each j. (Note: this restriction also applies to the\n *      computer code given in [4], but was mistakenly not mentioned\n *      in that paper.)\n *\n * This affects the seeding procedure by imposing the requirement\n * s1 > 1, s2 > 7, s3 > 15, s4 > 127.\n */\n\n#include <linux/types.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/jiffies.h>\n#include <linux/random.h>\n#include <linux/sched.h>\n#include <asm/unaligned.h>\n\n#ifdef CONFIG_RANDOM32_SELFTEST\nstatic void __init prandom_state_selftest(void);\n#else\nstatic inline void prandom_state_selftest(void)\n{\n}\n#endif\n\nstatic DEFINE_PER_CPU(struct rnd_state, net_rand_state) __latent_entropy;\n\n/**\n *\tprandom_u32_state - seeded pseudo-random number generator.\n *\t@state: pointer to state structure holding seeded state.\n *\n *\tThis is used for pseudo-randomness with no outside seeding.\n *\tFor more random results, use prandom_u32().\n */\nu32 prandom_u32_state(struct rnd_state *state)\n{\n#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)\n\tstate->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);\n\tstate->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);\n\tstate->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);\n\tstate->s4 = TAUSWORTHE(state->s4,  3U, 12U, 4294967168U, 13U);\n\n\treturn (state->s1 ^ state->s2 ^ state->s3 ^ state->s4);\n}\nEXPORT_SYMBOL(prandom_u32_state);\n\n/**\n *\tprandom_u32 - pseudo random number generator\n *\n *\tA 32 bit pseudo-random number is generated using a fast\n *\talgorithm suitable for simulation. This algorithm is NOT\n *\tconsidered safe for cryptographic use.\n */\nu32 prandom_u32(void)\n{\n\tstruct rnd_state *state = &get_cpu_var(net_rand_state);\n\tu32 res;\n\n\tres = prandom_u32_state(state);\n\tput_cpu_var(net_rand_state);\n\n\treturn res;\n}\nEXPORT_SYMBOL(prandom_u32);\n\n/**\n *\tprandom_bytes_state - get the requested number of pseudo-random bytes\n *\n *\t@state: pointer to state structure holding seeded state.\n *\t@buf: where to copy the pseudo-random bytes to\n *\t@bytes: the requested number of bytes\n *\n *\tThis is used for pseudo-randomness with no outside seeding.\n *\tFor more random results, use prandom_bytes().\n */\nvoid prandom_bytes_state(struct rnd_state *state, void *buf, size_t bytes)\n{\n\tu8 *ptr = buf;\n\n\twhile (bytes >= sizeof(u32)) {\n\t\tput_unaligned(prandom_u32_state(state), (u32 *) ptr);\n\t\tptr += sizeof(u32);\n\t\tbytes -= sizeof(u32);\n\t}\n\n\tif (bytes > 0) {\n\t\tu32 rem = prandom_u32_state(state);\n\t\tdo {\n\t\t\t*ptr++ = (u8) rem;\n\t\t\tbytes--;\n\t\t\trem >>= BITS_PER_BYTE;\n\t\t} while (bytes > 0);\n\t}\n}\nEXPORT_SYMBOL(prandom_bytes_state);\n\n/**\n *\tprandom_bytes - get the requested number of pseudo-random bytes\n *\t@buf: where to copy the pseudo-random bytes to\n *\t@bytes: the requested number of bytes\n */\nvoid prandom_bytes(void *buf, size_t bytes)\n{\n\tstruct rnd_state *state = &get_cpu_var(net_rand_state);\n\n\tprandom_bytes_state(state, buf, bytes);\n\tput_cpu_var(net_rand_state);\n}\nEXPORT_SYMBOL(prandom_bytes);\n\nstatic void prandom_warmup(struct rnd_state *state)\n{\n\t/* Calling RNG ten times to satisfy recurrence condition */\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n}\n\nstatic u32 __extract_hwseed(void)\n{\n\tunsigned int val = 0;\n\n\t(void)(arch_get_random_seed_int(&val) ||\n\t       arch_get_random_int(&val));\n\n\treturn val;\n}\n\nstatic void prandom_seed_early(struct rnd_state *state, u32 seed,\n\t\t\t       bool mix_with_hwseed)\n{\n#define LCG(x)\t ((x) * 69069U)\t/* super-duper LCG */\n#define HWSEED() (mix_with_hwseed ? __extract_hwseed() : 0)\n\tstate->s1 = __seed(HWSEED() ^ LCG(seed),        2U);\n\tstate->s2 = __seed(HWSEED() ^ LCG(state->s1),   8U);\n\tstate->s3 = __seed(HWSEED() ^ LCG(state->s2),  16U);\n\tstate->s4 = __seed(HWSEED() ^ LCG(state->s3), 128U);\n}\n\n/**\n *\tprandom_seed - add entropy to pseudo random number generator\n *\t@entropy: entropy value\n *\n *\tAdd some additional entropy to the prandom pool.\n */\nvoid prandom_seed(u32 entropy)\n{\n\tint i;\n\t/*\n\t * No locking on the CPUs, but then somewhat random results are, well,\n\t * expected.\n\t */\n\tfor_each_possible_cpu(i) {\n\t\tstruct rnd_state *state = &per_cpu(net_rand_state, i);\n\n\t\tstate->s1 = __seed(state->s1 ^ entropy, 2U);\n\t\tprandom_warmup(state);\n\t}\n}\nEXPORT_SYMBOL(prandom_seed);\n\n/*\n *\tGenerate some initially weak seeding values to allow\n *\tto start the prandom_u32() engine.\n */\nstatic int __init prandom_init(void)\n{\n\tint i;\n\n\tprandom_state_selftest();\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rnd_state *state = &per_cpu(net_rand_state, i);\n\t\tu32 weak_seed = (i + jiffies) ^ random_get_entropy();\n\n\t\tprandom_seed_early(state, weak_seed, true);\n\t\tprandom_warmup(state);\n\t}\n\n\treturn 0;\n}\ncore_initcall(prandom_init);\n\nstatic void __prandom_timer(struct timer_list *unused);\n\nstatic DEFINE_TIMER(seed_timer, __prandom_timer);\n\nstatic void __prandom_timer(struct timer_list *unused)\n{\n\tu32 entropy;\n\tunsigned long expires;\n\n\tget_random_bytes(&entropy, sizeof(entropy));\n\tprandom_seed(entropy);\n\n\t/* reseed every ~60 seconds, in [40 .. 80) interval with slack */\n\texpires = 40 + prandom_u32_max(40);\n\tseed_timer.expires = jiffies + msecs_to_jiffies(expires * MSEC_PER_SEC);\n\n\tadd_timer(&seed_timer);\n}\n\nstatic void __init __prandom_start_seed_timer(void)\n{\n\tseed_timer.expires = jiffies + msecs_to_jiffies(40 * MSEC_PER_SEC);\n\tadd_timer(&seed_timer);\n}\n\nvoid prandom_seed_full_state(struct rnd_state __percpu *pcpu_state)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rnd_state *state = per_cpu_ptr(pcpu_state, i);\n\t\tu32 seeds[4];\n\n\t\tget_random_bytes(&seeds, sizeof(seeds));\n\t\tstate->s1 = __seed(seeds[0],   2U);\n\t\tstate->s2 = __seed(seeds[1],   8U);\n\t\tstate->s3 = __seed(seeds[2],  16U);\n\t\tstate->s4 = __seed(seeds[3], 128U);\n\n\t\tprandom_warmup(state);\n\t}\n}\nEXPORT_SYMBOL(prandom_seed_full_state);\n\n/*\n *\tGenerate better values after random number generator\n *\tis fully initialized.\n */\nstatic void __prandom_reseed(bool late)\n{\n\tunsigned long flags;\n\tstatic bool latch = false;\n\tstatic DEFINE_SPINLOCK(lock);\n\n\t/* Asking for random bytes might result in bytes getting\n\t * moved into the nonblocking pool and thus marking it\n\t * as initialized. In this case we would double back into\n\t * this function and attempt to do a late reseed.\n\t * Ignore the pointless attempt to reseed again if we're\n\t * already waiting for bytes when the nonblocking pool\n\t * got initialized.\n\t */\n\n\t/* only allow initial seeding (late == false) once */\n\tif (!spin_trylock_irqsave(&lock, flags))\n\t\treturn;\n\n\tif (latch && !late)\n\t\tgoto out;\n\n\tlatch = true;\n\tprandom_seed_full_state(&net_rand_state);\nout:\n\tspin_unlock_irqrestore(&lock, flags);\n}\n\nvoid prandom_reseed_late(void)\n{\n\t__prandom_reseed(true);\n}\n\nstatic int __init prandom_reseed(void)\n{\n\t__prandom_reseed(false);\n\t__prandom_start_seed_timer();\n\treturn 0;\n}\nlate_initcall(prandom_reseed);\n\n#ifdef CONFIG_RANDOM32_SELFTEST\nstatic struct prandom_test1 {\n\tu32 seed;\n\tu32 result;\n} test1[] = {\n\t{ 1U, 3484351685U },\n\t{ 2U, 2623130059U },\n\t{ 3U, 3125133893U },\n\t{ 4U,  984847254U },\n};\n\nstatic struct prandom_test2 {\n\tu32 seed;\n\tu32 iteration;\n\tu32 result;\n} test2[] = {\n\t/* Test cases against taus113 from GSL library. */\n\t{  931557656U, 959U, 2975593782U },\n\t{ 1339693295U, 876U, 3887776532U },\n\t{ 1545556285U, 961U, 1615538833U },\n\t{  601730776U, 723U, 1776162651U },\n\t{ 1027516047U, 687U,  511983079U },\n\t{  416526298U, 700U,  916156552U },\n\t{ 1395522032U, 652U, 2222063676U },\n\t{  366221443U, 617U, 2992857763U },\n\t{ 1539836965U, 714U, 3783265725U },\n\t{  556206671U, 994U,  799626459U },\n\t{  684907218U, 799U,  367789491U },\n\t{ 2121230701U, 931U, 2115467001U },\n\t{ 1668516451U, 644U, 3620590685U },\n\t{  768046066U, 883U, 2034077390U },\n\t{ 1989159136U, 833U, 1195767305U },\n\t{  536585145U, 996U, 3577259204U },\n\t{ 1008129373U, 642U, 1478080776U },\n\t{ 1740775604U, 939U, 1264980372U },\n\t{ 1967883163U, 508U,   10734624U },\n\t{ 1923019697U, 730U, 3821419629U },\n\t{  442079932U, 560U, 3440032343U },\n\t{ 1961302714U, 845U,  841962572U },\n\t{ 2030205964U, 962U, 1325144227U },\n\t{ 1160407529U, 507U,  240940858U },\n\t{  635482502U, 779U, 4200489746U },\n\t{ 1252788931U, 699U,  867195434U },\n\t{ 1961817131U, 719U,  668237657U },\n\t{ 1071468216U, 983U,  917876630U },\n\t{ 1281848367U, 932U, 1003100039U },\n\t{  582537119U, 780U, 1127273778U },\n\t{ 1973672777U, 853U, 1071368872U },\n\t{ 1896756996U, 762U, 1127851055U },\n\t{  847917054U, 500U, 1717499075U },\n\t{ 1240520510U, 951U, 2849576657U },\n\t{ 1685071682U, 567U, 1961810396U },\n\t{ 1516232129U, 557U,    3173877U },\n\t{ 1208118903U, 612U, 1613145022U },\n\t{ 1817269927U, 693U, 4279122573U },\n\t{ 1510091701U, 717U,  638191229U },\n\t{  365916850U, 807U,  600424314U },\n\t{  399324359U, 702U, 1803598116U },\n\t{ 1318480274U, 779U, 2074237022U },\n\t{  697758115U, 840U, 1483639402U },\n\t{ 1696507773U, 840U,  577415447U },\n\t{ 2081979121U, 981U, 3041486449U },\n\t{  955646687U, 742U, 3846494357U },\n\t{ 1250683506U, 749U,  836419859U },\n\t{  595003102U, 534U,  366794109U },\n\t{   47485338U, 558U, 3521120834U },\n\t{  619433479U, 610U, 3991783875U },\n\t{  704096520U, 518U, 4139493852U },\n\t{ 1712224984U, 606U, 2393312003U },\n\t{ 1318233152U, 922U, 3880361134U },\n\t{  855572992U, 761U, 1472974787U },\n\t{   64721421U, 703U,  683860550U },\n\t{  678931758U, 840U,  380616043U },\n\t{  692711973U, 778U, 1382361947U },\n\t{  677703619U, 530U, 2826914161U },\n\t{   92393223U, 586U, 1522128471U },\n\t{ 1222592920U, 743U, 3466726667U },\n\t{  358288986U, 695U, 1091956998U },\n\t{ 1935056945U, 958U,  514864477U },\n\t{  735675993U, 990U, 1294239989U },\n\t{ 1560089402U, 897U, 2238551287U },\n\t{   70616361U, 829U,   22483098U },\n\t{  368234700U, 731U, 2913875084U },\n\t{   20221190U, 879U, 1564152970U },\n\t{  539444654U, 682U, 1835141259U },\n\t{ 1314987297U, 840U, 1801114136U },\n\t{ 2019295544U, 645U, 3286438930U },\n\t{  469023838U, 716U, 1637918202U },\n\t{ 1843754496U, 653U, 2562092152U },\n\t{  400672036U, 809U, 4264212785U },\n\t{  404722249U, 965U, 2704116999U },\n\t{  600702209U, 758U,  584979986U },\n\t{  519953954U, 667U, 2574436237U },\n\t{ 1658071126U, 694U, 2214569490U },\n\t{  420480037U, 749U, 3430010866U },\n\t{  690103647U, 969U, 3700758083U },\n\t{ 1029424799U, 937U, 3787746841U },\n\t{ 2012608669U, 506U, 3362628973U },\n\t{ 1535432887U, 998U,   42610943U },\n\t{ 1330635533U, 857U, 3040806504U },\n\t{ 1223800550U, 539U, 3954229517U },\n\t{ 1322411537U, 680U, 3223250324U },\n\t{ 1877847898U, 945U, 2915147143U },\n\t{ 1646356099U, 874U,  965988280U },\n\t{  805687536U, 744U, 4032277920U },\n\t{ 1948093210U, 633U, 1346597684U },\n\t{  392609744U, 783U, 1636083295U },\n\t{  690241304U, 770U, 1201031298U },\n\t{ 1360302965U, 696U, 1665394461U },\n\t{ 1220090946U, 780U, 1316922812U },\n\t{  447092251U, 500U, 3438743375U },\n\t{ 1613868791U, 592U,  828546883U },\n\t{  523430951U, 548U, 2552392304U },\n\t{  726692899U, 810U, 1656872867U },\n\t{ 1364340021U, 836U, 3710513486U },\n\t{ 1986257729U, 931U,  935013962U },\n\t{  407983964U, 921U,  728767059U },\n};\n\nstatic void __init prandom_state_selftest(void)\n{\n\tint i, j, errors = 0, runs = 0;\n\tbool error = false;\n\n\tfor (i = 0; i < ARRAY_SIZE(test1); i++) {\n\t\tstruct rnd_state state;\n\n\t\tprandom_seed_early(&state, test1[i].seed, false);\n\t\tprandom_warmup(&state);\n\n\t\tif (test1[i].result != prandom_u32_state(&state))\n\t\t\terror = true;\n\t}\n\n\tif (error)\n\t\tpr_warn(\"prandom: seed boundary self test failed\\n\");\n\telse\n\t\tpr_info(\"prandom: seed boundary self test passed\\n\");\n\n\tfor (i = 0; i < ARRAY_SIZE(test2); i++) {\n\t\tstruct rnd_state state;\n\n\t\tprandom_seed_early(&state, test2[i].seed, false);\n\t\tprandom_warmup(&state);\n\n\t\tfor (j = 0; j < test2[i].iteration - 1; j++)\n\t\t\tprandom_u32_state(&state);\n\n\t\tif (test2[i].result != prandom_u32_state(&state))\n\t\t\terrors++;\n\n\t\truns++;\n\t\tcond_resched();\n\t}\n\n\tif (errors)\n\t\tpr_warn(\"prandom: %d/%d self tests failed\\n\", errors, runs);\n\telse\n\t\tpr_info(\"prandom: %d self tests passed\\n\", runs);\n}\n#endif\n"], "fixing_code": ["/*\n * random.c -- A strong random number generator\n *\n * Copyright (C) 2017 Jason A. Donenfeld <Jason@zx2c4.com>. All\n * Rights Reserved.\n *\n * Copyright Matt Mackall <mpm@selenic.com>, 2003, 2004, 2005\n *\n * Copyright Theodore Ts'o, 1994, 1995, 1996, 1997, 1998, 1999.  All\n * rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, and the entire permission notice in its entirety,\n *    including the disclaimer of warranties.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. The name of the author may not be used to endorse or promote\n *    products derived from this software without specific prior\n *    written permission.\n *\n * ALTERNATIVELY, this product may be distributed under the terms of\n * the GNU General Public License, in which case the provisions of the GPL are\n * required INSTEAD OF the above restrictions.  (This clause is\n * necessary due to a potential bad interaction between the GPL and\n * the restrictions contained in a BSD-style copyright.)\n *\n * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, ALL OF\n * WHICH ARE HEREBY DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT\n * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n * USE OF THIS SOFTWARE, EVEN IF NOT ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n */\n\n/*\n * (now, with legal B.S. out of the way.....)\n *\n * This routine gathers environmental noise from device drivers, etc.,\n * and returns good random numbers, suitable for cryptographic use.\n * Besides the obvious cryptographic uses, these numbers are also good\n * for seeding TCP sequence numbers, and other places where it is\n * desirable to have numbers which are not only random, but hard to\n * predict by an attacker.\n *\n * Theory of operation\n * ===================\n *\n * Computers are very predictable devices.  Hence it is extremely hard\n * to produce truly random numbers on a computer --- as opposed to\n * pseudo-random numbers, which can easily generated by using a\n * algorithm.  Unfortunately, it is very easy for attackers to guess\n * the sequence of pseudo-random number generators, and for some\n * applications this is not acceptable.  So instead, we must try to\n * gather \"environmental noise\" from the computer's environment, which\n * must be hard for outside attackers to observe, and use that to\n * generate random numbers.  In a Unix environment, this is best done\n * from inside the kernel.\n *\n * Sources of randomness from the environment include inter-keyboard\n * timings, inter-interrupt timings from some interrupts, and other\n * events which are both (a) non-deterministic and (b) hard for an\n * outside observer to measure.  Randomness from these sources are\n * added to an \"entropy pool\", which is mixed using a CRC-like function.\n * This is not cryptographically strong, but it is adequate assuming\n * the randomness is not chosen maliciously, and it is fast enough that\n * the overhead of doing it on every interrupt is very reasonable.\n * As random bytes are mixed into the entropy pool, the routines keep\n * an *estimate* of how many bits of randomness have been stored into\n * the random number generator's internal state.\n *\n * When random bytes are desired, they are obtained by taking the SHA\n * hash of the contents of the \"entropy pool\".  The SHA hash avoids\n * exposing the internal state of the entropy pool.  It is believed to\n * be computationally infeasible to derive any useful information\n * about the input of SHA from its output.  Even if it is possible to\n * analyze SHA in some clever way, as long as the amount of data\n * returned from the generator is less than the inherent entropy in\n * the pool, the output data is totally unpredictable.  For this\n * reason, the routine decreases its internal estimate of how many\n * bits of \"true randomness\" are contained in the entropy pool as it\n * outputs random numbers.\n *\n * If this estimate goes to zero, the routine can still generate\n * random numbers; however, an attacker may (at least in theory) be\n * able to infer the future output of the generator from prior\n * outputs.  This requires successful cryptanalysis of SHA, which is\n * not believed to be feasible, but there is a remote possibility.\n * Nonetheless, these numbers should be useful for the vast majority\n * of purposes.\n *\n * Exported interfaces ---- output\n * ===============================\n *\n * There are four exported interfaces; two for use within the kernel,\n * and two or use from userspace.\n *\n * Exported interfaces ---- userspace output\n * -----------------------------------------\n *\n * The userspace interfaces are two character devices /dev/random and\n * /dev/urandom.  /dev/random is suitable for use when very high\n * quality randomness is desired (for example, for key generation or\n * one-time pads), as it will only return a maximum of the number of\n * bits of randomness (as estimated by the random number generator)\n * contained in the entropy pool.\n *\n * The /dev/urandom device does not have this limit, and will return\n * as many bytes as are requested.  As more and more random bytes are\n * requested without giving time for the entropy pool to recharge,\n * this will result in random numbers that are merely cryptographically\n * strong.  For many applications, however, this is acceptable.\n *\n * Exported interfaces ---- kernel output\n * --------------------------------------\n *\n * The primary kernel interface is\n *\n * \tvoid get_random_bytes(void *buf, int nbytes);\n *\n * This interface will return the requested number of random bytes,\n * and place it in the requested buffer.  This is equivalent to a\n * read from /dev/urandom.\n *\n * For less critical applications, there are the functions:\n *\n * \tu32 get_random_u32()\n * \tu64 get_random_u64()\n * \tunsigned int get_random_int()\n * \tunsigned long get_random_long()\n *\n * These are produced by a cryptographic RNG seeded from get_random_bytes,\n * and so do not deplete the entropy pool as much.  These are recommended\n * for most in-kernel operations *if the result is going to be stored in\n * the kernel*.\n *\n * Specifically, the get_random_int() family do not attempt to do\n * \"anti-backtracking\".  If you capture the state of the kernel (e.g.\n * by snapshotting the VM), you can figure out previous get_random_int()\n * return values.  But if the value is stored in the kernel anyway,\n * this is not a problem.\n *\n * It *is* safe to expose get_random_int() output to attackers (e.g. as\n * network cookies); given outputs 1..n, it's not feasible to predict\n * outputs 0 or n+1.  The only concern is an attacker who breaks into\n * the kernel later; the get_random_int() engine is not reseeded as\n * often as the get_random_bytes() one.\n *\n * get_random_bytes() is needed for keys that need to stay secret after\n * they are erased from the kernel.  For example, any key that will\n * be wrapped and stored encrypted.  And session encryption keys: we'd\n * like to know that after the session is closed and the keys erased,\n * the plaintext is unrecoverable to someone who recorded the ciphertext.\n *\n * But for network ports/cookies, stack canaries, PRNG seeds, address\n * space layout randomization, session *authentication* keys, or other\n * applications where the sensitive data is stored in the kernel in\n * plaintext for as long as it's sensitive, the get_random_int() family\n * is just fine.\n *\n * Consider ASLR.  We want to keep the address space secret from an\n * outside attacker while the process is running, but once the address\n * space is torn down, it's of no use to an attacker any more.  And it's\n * stored in kernel data structures as long as it's alive, so worrying\n * about an attacker's ability to extrapolate it from the get_random_int()\n * CRNG is silly.\n *\n * Even some cryptographic keys are safe to generate with get_random_int().\n * In particular, keys for SipHash are generally fine.  Here, knowledge\n * of the key authorizes you to do something to a kernel object (inject\n * packets to a network connection, or flood a hash table), and the\n * key is stored with the object being protected.  Once it goes away,\n * we no longer care if anyone knows the key.\n *\n * prandom_u32()\n * -------------\n *\n * For even weaker applications, see the pseudorandom generator\n * prandom_u32(), prandom_max(), and prandom_bytes().  If the random\n * numbers aren't security-critical at all, these are *far* cheaper.\n * Useful for self-tests, random error simulation, randomized backoffs,\n * and any other application where you trust that nobody is trying to\n * maliciously mess with you by guessing the \"random\" numbers.\n *\n * Exported interfaces ---- input\n * ==============================\n *\n * The current exported interfaces for gathering environmental noise\n * from the devices are:\n *\n *\tvoid add_device_randomness(const void *buf, unsigned int size);\n * \tvoid add_input_randomness(unsigned int type, unsigned int code,\n *                                unsigned int value);\n *\tvoid add_interrupt_randomness(int irq, int irq_flags);\n * \tvoid add_disk_randomness(struct gendisk *disk);\n *\n * add_device_randomness() is for adding data to the random pool that\n * is likely to differ between two devices (or possibly even per boot).\n * This would be things like MAC addresses or serial numbers, or the\n * read-out of the RTC. This does *not* add any actual entropy to the\n * pool, but it initializes the pool to different values for devices\n * that might otherwise be identical and have very little entropy\n * available to them (particularly common in the embedded world).\n *\n * add_input_randomness() uses the input layer interrupt timing, as well as\n * the event type information from the hardware.\n *\n * add_interrupt_randomness() uses the interrupt timing as random\n * inputs to the entropy pool. Using the cycle counters and the irq source\n * as inputs, it feeds the randomness roughly once a second.\n *\n * add_disk_randomness() uses what amounts to the seek time of block\n * layer request events, on a per-disk_devt basis, as input to the\n * entropy pool. Note that high-speed solid state drives with very low\n * seek times do not make for good sources of entropy, as their seek\n * times are usually fairly consistent.\n *\n * All of these routines try to estimate how many bits of randomness a\n * particular randomness source.  They do this by keeping track of the\n * first and second order deltas of the event timings.\n *\n * Ensuring unpredictability at system startup\n * ============================================\n *\n * When any operating system starts up, it will go through a sequence\n * of actions that are fairly predictable by an adversary, especially\n * if the start-up does not involve interaction with a human operator.\n * This reduces the actual number of bits of unpredictability in the\n * entropy pool below the value in entropy_count.  In order to\n * counteract this effect, it helps to carry information in the\n * entropy pool across shut-downs and start-ups.  To do this, put the\n * following lines an appropriate script which is run during the boot\n * sequence:\n *\n *\techo \"Initializing random number generator...\"\n *\trandom_seed=/var/run/random-seed\n *\t# Carry a random seed from start-up to start-up\n *\t# Load and then save the whole entropy pool\n *\tif [ -f $random_seed ]; then\n *\t\tcat $random_seed >/dev/urandom\n *\telse\n *\t\ttouch $random_seed\n *\tfi\n *\tchmod 600 $random_seed\n *\tdd if=/dev/urandom of=$random_seed count=1 bs=512\n *\n * and the following lines in an appropriate script which is run as\n * the system is shutdown:\n *\n *\t# Carry a random seed from shut-down to start-up\n *\t# Save the whole entropy pool\n *\techo \"Saving random seed...\"\n *\trandom_seed=/var/run/random-seed\n *\ttouch $random_seed\n *\tchmod 600 $random_seed\n *\tdd if=/dev/urandom of=$random_seed count=1 bs=512\n *\n * For example, on most modern systems using the System V init\n * scripts, such code fragments would be found in\n * /etc/rc.d/init.d/random.  On older Linux systems, the correct script\n * location might be in /etc/rcb.d/rc.local or /etc/rc.d/rc.0.\n *\n * Effectively, these commands cause the contents of the entropy pool\n * to be saved at shut-down time and reloaded into the entropy pool at\n * start-up.  (The 'dd' in the addition to the bootup script is to\n * make sure that /etc/random-seed is different for every start-up,\n * even if the system crashes without executing rc.0.)  Even with\n * complete knowledge of the start-up activities, predicting the state\n * of the entropy pool requires knowledge of the previous history of\n * the system.\n *\n * Configuring the /dev/random driver under Linux\n * ==============================================\n *\n * The /dev/random driver under Linux uses minor numbers 8 and 9 of\n * the /dev/mem major number (#1).  So if your system does not have\n * /dev/random and /dev/urandom created already, they can be created\n * by using the commands:\n *\n * \tmknod /dev/random c 1 8\n * \tmknod /dev/urandom c 1 9\n *\n * Acknowledgements:\n * =================\n *\n * Ideas for constructing this random number generator were derived\n * from Pretty Good Privacy's random number generator, and from private\n * discussions with Phil Karn.  Colin Plumb provided a faster random\n * number generator, which speed up the mixing function of the entropy\n * pool, taken from PGPfone.  Dale Worley has also contributed many\n * useful ideas and suggestions to improve this driver.\n *\n * Any flaws in the design are solely my responsibility, and should\n * not be attributed to the Phil, Colin, or any of authors of PGP.\n *\n * Further background information on this topic may be obtained from\n * RFC 1750, \"Randomness Recommendations for Security\", by Donald\n * Eastlake, Steve Crocker, and Jeff Schiller.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/utsname.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/major.h>\n#include <linux/string.h>\n#include <linux/fcntl.h>\n#include <linux/slab.h>\n#include <linux/random.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/genhd.h>\n#include <linux/interrupt.h>\n#include <linux/mm.h>\n#include <linux/nodemask.h>\n#include <linux/spinlock.h>\n#include <linux/kthread.h>\n#include <linux/percpu.h>\n#include <linux/fips.h>\n#include <linux/ptrace.h>\n#include <linux/workqueue.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/syscalls.h>\n#include <linux/completion.h>\n#include <linux/uuid.h>\n#include <crypto/chacha.h>\n#include <crypto/sha.h>\n\n#include <asm/processor.h>\n#include <linux/uaccess.h>\n#include <asm/irq.h>\n#include <asm/irq_regs.h>\n#include <asm/io.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/random.h>\n\n/* #define ADD_INTERRUPT_BENCH */\n\n/*\n * Configuration information\n */\n#define INPUT_POOL_SHIFT\t12\n#define INPUT_POOL_WORDS\t(1 << (INPUT_POOL_SHIFT-5))\n#define OUTPUT_POOL_SHIFT\t10\n#define OUTPUT_POOL_WORDS\t(1 << (OUTPUT_POOL_SHIFT-5))\n#define EXTRACT_SIZE\t\t10\n\n\n#define LONGS(x) (((x) + sizeof(unsigned long) - 1)/sizeof(unsigned long))\n\n/*\n * To allow fractional bits to be tracked, the entropy_count field is\n * denominated in units of 1/8th bits.\n *\n * 2*(ENTROPY_SHIFT + poolbitshift) must <= 31, or the multiply in\n * credit_entropy_bits() needs to be 64 bits wide.\n */\n#define ENTROPY_SHIFT 3\n#define ENTROPY_BITS(r) ((r)->entropy_count >> ENTROPY_SHIFT)\n\n/*\n * If the entropy count falls under this number of bits, then we\n * should wake up processes which are selecting or polling on write\n * access to /dev/random.\n */\nstatic int random_write_wakeup_bits = 28 * OUTPUT_POOL_WORDS;\n\n/*\n * Originally, we used a primitive polynomial of degree .poolwords\n * over GF(2).  The taps for various sizes are defined below.  They\n * were chosen to be evenly spaced except for the last tap, which is 1\n * to get the twisting happening as fast as possible.\n *\n * For the purposes of better mixing, we use the CRC-32 polynomial as\n * well to make a (modified) twisted Generalized Feedback Shift\n * Register.  (See M. Matsumoto & Y. Kurita, 1992.  Twisted GFSR\n * generators.  ACM Transactions on Modeling and Computer Simulation\n * 2(3):179-194.  Also see M. Matsumoto & Y. Kurita, 1994.  Twisted\n * GFSR generators II.  ACM Transactions on Modeling and Computer\n * Simulation 4:254-266)\n *\n * Thanks to Colin Plumb for suggesting this.\n *\n * The mixing operation is much less sensitive than the output hash,\n * where we use SHA-1.  All that we want of mixing operation is that\n * it be a good non-cryptographic hash; i.e. it not produce collisions\n * when fed \"random\" data of the sort we expect to see.  As long as\n * the pool state differs for different inputs, we have preserved the\n * input entropy and done a good job.  The fact that an intelligent\n * attacker can construct inputs that will produce controlled\n * alterations to the pool's state is not important because we don't\n * consider such inputs to contribute any randomness.  The only\n * property we need with respect to them is that the attacker can't\n * increase his/her knowledge of the pool's state.  Since all\n * additions are reversible (knowing the final state and the input,\n * you can reconstruct the initial state), if an attacker has any\n * uncertainty about the initial state, he/she can only shuffle that\n * uncertainty about, but never cause any collisions (which would\n * decrease the uncertainty).\n *\n * Our mixing functions were analyzed by Lacharme, Roeck, Strubel, and\n * Videau in their paper, \"The Linux Pseudorandom Number Generator\n * Revisited\" (see: http://eprint.iacr.org/2012/251.pdf).  In their\n * paper, they point out that we are not using a true Twisted GFSR,\n * since Matsumoto & Kurita used a trinomial feedback polynomial (that\n * is, with only three taps, instead of the six that we are using).\n * As a result, the resulting polynomial is neither primitive nor\n * irreducible, and hence does not have a maximal period over\n * GF(2**32).  They suggest a slight change to the generator\n * polynomial which improves the resulting TGFSR polynomial to be\n * irreducible, which we have made here.\n */\nstatic const struct poolinfo {\n\tint poolbitshift, poolwords, poolbytes, poolfracbits;\n#define S(x) ilog2(x)+5, (x), (x)*4, (x) << (ENTROPY_SHIFT+5)\n\tint tap1, tap2, tap3, tap4, tap5;\n} poolinfo_table[] = {\n\t/* was: x^128 + x^103 + x^76 + x^51 +x^25 + x + 1 */\n\t/* x^128 + x^104 + x^76 + x^51 +x^25 + x + 1 */\n\t{ S(128),\t104,\t76,\t51,\t25,\t1 },\n};\n\n/*\n * Static global variables\n */\nstatic DECLARE_WAIT_QUEUE_HEAD(random_write_wait);\nstatic struct fasync_struct *fasync;\n\nstatic DEFINE_SPINLOCK(random_ready_list_lock);\nstatic LIST_HEAD(random_ready_list);\n\nstruct crng_state {\n\t__u32\t\tstate[16];\n\tunsigned long\tinit_time;\n\tspinlock_t\tlock;\n};\n\nstatic struct crng_state primary_crng = {\n\t.lock = __SPIN_LOCK_UNLOCKED(primary_crng.lock),\n};\n\n/*\n * crng_init =  0 --> Uninitialized\n *\t\t1 --> Initialized\n *\t\t2 --> Initialized from input_pool\n *\n * crng_init is protected by primary_crng->lock, and only increases\n * its value (from 0->1->2).\n */\nstatic int crng_init = 0;\n#define crng_ready() (likely(crng_init > 1))\nstatic int crng_init_cnt = 0;\nstatic unsigned long crng_global_init_time = 0;\n#define CRNG_INIT_CNT_THRESH (2*CHACHA_KEY_SIZE)\nstatic void _extract_crng(struct crng_state *crng, __u8 out[CHACHA_BLOCK_SIZE]);\nstatic void _crng_backtrack_protect(struct crng_state *crng,\n\t\t\t\t    __u8 tmp[CHACHA_BLOCK_SIZE], int used);\nstatic void process_random_ready_list(void);\nstatic void _get_random_bytes(void *buf, int nbytes);\n\nstatic struct ratelimit_state unseeded_warning =\n\tRATELIMIT_STATE_INIT(\"warn_unseeded_randomness\", HZ, 3);\nstatic struct ratelimit_state urandom_warning =\n\tRATELIMIT_STATE_INIT(\"warn_urandom_randomness\", HZ, 3);\n\nstatic int ratelimit_disable __read_mostly;\n\nmodule_param_named(ratelimit_disable, ratelimit_disable, int, 0644);\nMODULE_PARM_DESC(ratelimit_disable, \"Disable random ratelimit suppression\");\n\n/**********************************************************************\n *\n * OS independent entropy store.   Here are the functions which handle\n * storing entropy in an entropy pool.\n *\n **********************************************************************/\n\nstruct entropy_store;\nstruct entropy_store {\n\t/* read-only data: */\n\tconst struct poolinfo *poolinfo;\n\t__u32 *pool;\n\tconst char *name;\n\n\t/* read-write data: */\n\tspinlock_t lock;\n\tunsigned short add_ptr;\n\tunsigned short input_rotate;\n\tint entropy_count;\n\tunsigned int initialized:1;\n\tunsigned int last_data_init:1;\n\t__u8 last_data[EXTRACT_SIZE];\n};\n\nstatic ssize_t extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t       size_t nbytes, int min, int rsvd);\nstatic ssize_t _extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t\tsize_t nbytes, int fips);\n\nstatic void crng_reseed(struct crng_state *crng, struct entropy_store *r);\nstatic __u32 input_pool_data[INPUT_POOL_WORDS] __latent_entropy;\n\nstatic struct entropy_store input_pool = {\n\t.poolinfo = &poolinfo_table[0],\n\t.name = \"input\",\n\t.lock = __SPIN_LOCK_UNLOCKED(input_pool.lock),\n\t.pool = input_pool_data\n};\n\nstatic __u32 const twist_table[8] = {\n\t0x00000000, 0x3b6e20c8, 0x76dc4190, 0x4db26158,\n\t0xedb88320, 0xd6d6a3e8, 0x9b64c2b0, 0xa00ae278 };\n\n/*\n * This function adds bytes into the entropy \"pool\".  It does not\n * update the entropy estimate.  The caller should call\n * credit_entropy_bits if this is appropriate.\n *\n * The pool is stirred with a primitive polynomial of the appropriate\n * degree, and then twisted.  We twist by three bits at a time because\n * it's cheap to do so and helps slightly in the expected case where\n * the entropy is concentrated in the low-order bits.\n */\nstatic void _mix_pool_bytes(struct entropy_store *r, const void *in,\n\t\t\t    int nbytes)\n{\n\tunsigned long i, tap1, tap2, tap3, tap4, tap5;\n\tint input_rotate;\n\tint wordmask = r->poolinfo->poolwords - 1;\n\tconst char *bytes = in;\n\t__u32 w;\n\n\ttap1 = r->poolinfo->tap1;\n\ttap2 = r->poolinfo->tap2;\n\ttap3 = r->poolinfo->tap3;\n\ttap4 = r->poolinfo->tap4;\n\ttap5 = r->poolinfo->tap5;\n\n\tinput_rotate = r->input_rotate;\n\ti = r->add_ptr;\n\n\t/* mix one byte at a time to simplify size handling and churn faster */\n\twhile (nbytes--) {\n\t\tw = rol32(*bytes++, input_rotate);\n\t\ti = (i - 1) & wordmask;\n\n\t\t/* XOR in the various taps */\n\t\tw ^= r->pool[i];\n\t\tw ^= r->pool[(i + tap1) & wordmask];\n\t\tw ^= r->pool[(i + tap2) & wordmask];\n\t\tw ^= r->pool[(i + tap3) & wordmask];\n\t\tw ^= r->pool[(i + tap4) & wordmask];\n\t\tw ^= r->pool[(i + tap5) & wordmask];\n\n\t\t/* Mix the result back in with a twist */\n\t\tr->pool[i] = (w >> 3) ^ twist_table[w & 7];\n\n\t\t/*\n\t\t * Normally, we add 7 bits of rotation to the pool.\n\t\t * At the beginning of the pool, add an extra 7 bits\n\t\t * rotation, so that successive passes spread the\n\t\t * input bits across the pool evenly.\n\t\t */\n\t\tinput_rotate = (input_rotate + (i ? 7 : 14)) & 31;\n\t}\n\n\tr->input_rotate = input_rotate;\n\tr->add_ptr = i;\n}\n\nstatic void __mix_pool_bytes(struct entropy_store *r, const void *in,\n\t\t\t     int nbytes)\n{\n\ttrace_mix_pool_bytes_nolock(r->name, nbytes, _RET_IP_);\n\t_mix_pool_bytes(r, in, nbytes);\n}\n\nstatic void mix_pool_bytes(struct entropy_store *r, const void *in,\n\t\t\t   int nbytes)\n{\n\tunsigned long flags;\n\n\ttrace_mix_pool_bytes(r->name, nbytes, _RET_IP_);\n\tspin_lock_irqsave(&r->lock, flags);\n\t_mix_pool_bytes(r, in, nbytes);\n\tspin_unlock_irqrestore(&r->lock, flags);\n}\n\nstruct fast_pool {\n\t__u32\t\tpool[4];\n\tunsigned long\tlast;\n\tunsigned short\treg_idx;\n\tunsigned char\tcount;\n};\n\n/*\n * This is a fast mixing routine used by the interrupt randomness\n * collector.  It's hardcoded for an 128 bit pool and assumes that any\n * locks that might be needed are taken by the caller.\n */\nstatic void fast_mix(struct fast_pool *f)\n{\n\t__u32 a = f->pool[0],\tb = f->pool[1];\n\t__u32 c = f->pool[2],\td = f->pool[3];\n\n\ta += b;\t\t\tc += d;\n\tb = rol32(b, 6);\td = rol32(d, 27);\n\td ^= a;\t\t\tb ^= c;\n\n\ta += b;\t\t\tc += d;\n\tb = rol32(b, 16);\td = rol32(d, 14);\n\td ^= a;\t\t\tb ^= c;\n\n\ta += b;\t\t\tc += d;\n\tb = rol32(b, 6);\td = rol32(d, 27);\n\td ^= a;\t\t\tb ^= c;\n\n\ta += b;\t\t\tc += d;\n\tb = rol32(b, 16);\td = rol32(d, 14);\n\td ^= a;\t\t\tb ^= c;\n\n\tf->pool[0] = a;  f->pool[1] = b;\n\tf->pool[2] = c;  f->pool[3] = d;\n\tf->count++;\n}\n\nstatic void process_random_ready_list(void)\n{\n\tunsigned long flags;\n\tstruct random_ready_callback *rdy, *tmp;\n\n\tspin_lock_irqsave(&random_ready_list_lock, flags);\n\tlist_for_each_entry_safe(rdy, tmp, &random_ready_list, list) {\n\t\tstruct module *owner = rdy->owner;\n\n\t\tlist_del_init(&rdy->list);\n\t\trdy->func(rdy);\n\t\tmodule_put(owner);\n\t}\n\tspin_unlock_irqrestore(&random_ready_list_lock, flags);\n}\n\n/*\n * Credit (or debit) the entropy store with n bits of entropy.\n * Use credit_entropy_bits_safe() if the value comes from userspace\n * or otherwise should be checked for extreme values.\n */\nstatic void credit_entropy_bits(struct entropy_store *r, int nbits)\n{\n\tint entropy_count, orig, has_initialized = 0;\n\tconst int pool_size = r->poolinfo->poolfracbits;\n\tint nfrac = nbits << ENTROPY_SHIFT;\n\n\tif (!nbits)\n\t\treturn;\n\nretry:\n\tentropy_count = orig = READ_ONCE(r->entropy_count);\n\tif (nfrac < 0) {\n\t\t/* Debit */\n\t\tentropy_count += nfrac;\n\t} else {\n\t\t/*\n\t\t * Credit: we have to account for the possibility of\n\t\t * overwriting already present entropy.\t Even in the\n\t\t * ideal case of pure Shannon entropy, new contributions\n\t\t * approach the full value asymptotically:\n\t\t *\n\t\t * entropy <- entropy + (pool_size - entropy) *\n\t\t *\t(1 - exp(-add_entropy/pool_size))\n\t\t *\n\t\t * For add_entropy <= pool_size/2 then\n\t\t * (1 - exp(-add_entropy/pool_size)) >=\n\t\t *    (add_entropy/pool_size)*0.7869...\n\t\t * so we can approximate the exponential with\n\t\t * 3/4*add_entropy/pool_size and still be on the\n\t\t * safe side by adding at most pool_size/2 at a time.\n\t\t *\n\t\t * The use of pool_size-2 in the while statement is to\n\t\t * prevent rounding artifacts from making the loop\n\t\t * arbitrarily long; this limits the loop to log2(pool_size)*2\n\t\t * turns no matter how large nbits is.\n\t\t */\n\t\tint pnfrac = nfrac;\n\t\tconst int s = r->poolinfo->poolbitshift + ENTROPY_SHIFT + 2;\n\t\t/* The +2 corresponds to the /4 in the denominator */\n\n\t\tdo {\n\t\t\tunsigned int anfrac = min(pnfrac, pool_size/2);\n\t\t\tunsigned int add =\n\t\t\t\t((pool_size - entropy_count)*anfrac*3) >> s;\n\n\t\t\tentropy_count += add;\n\t\t\tpnfrac -= anfrac;\n\t\t} while (unlikely(entropy_count < pool_size-2 && pnfrac));\n\t}\n\n\tif (WARN_ON(entropy_count < 0)) {\n\t\tpr_warn(\"negative entropy/overflow: pool %s count %d\\n\",\n\t\t\tr->name, entropy_count);\n\t\tentropy_count = 0;\n\t} else if (entropy_count > pool_size)\n\t\tentropy_count = pool_size;\n\tif (cmpxchg(&r->entropy_count, orig, entropy_count) != orig)\n\t\tgoto retry;\n\n\tif (has_initialized) {\n\t\tr->initialized = 1;\n\t\tkill_fasync(&fasync, SIGIO, POLL_IN);\n\t}\n\n\ttrace_credit_entropy_bits(r->name, nbits,\n\t\t\t\t  entropy_count >> ENTROPY_SHIFT, _RET_IP_);\n\n\tif (r == &input_pool) {\n\t\tint entropy_bits = entropy_count >> ENTROPY_SHIFT;\n\n\t\tif (crng_init < 2) {\n\t\t\tif (entropy_bits < 128)\n\t\t\t\treturn;\n\t\t\tcrng_reseed(&primary_crng, r);\n\t\t\tentropy_bits = ENTROPY_BITS(r);\n\t\t}\n\t}\n}\n\nstatic int credit_entropy_bits_safe(struct entropy_store *r, int nbits)\n{\n\tconst int nbits_max = r->poolinfo->poolwords * 32;\n\n\tif (nbits < 0)\n\t\treturn -EINVAL;\n\n\t/* Cap the value to avoid overflows */\n\tnbits = min(nbits,  nbits_max);\n\n\tcredit_entropy_bits(r, nbits);\n\treturn 0;\n}\n\n/*********************************************************************\n *\n * CRNG using CHACHA20\n *\n *********************************************************************/\n\n#define CRNG_RESEED_INTERVAL (300*HZ)\n\nstatic DECLARE_WAIT_QUEUE_HEAD(crng_init_wait);\n\n#ifdef CONFIG_NUMA\n/*\n * Hack to deal with crazy userspace progams when they are all trying\n * to access /dev/urandom in parallel.  The programs are almost\n * certainly doing something terribly wrong, but we'll work around\n * their brain damage.\n */\nstatic struct crng_state **crng_node_pool __read_mostly;\n#endif\n\nstatic void invalidate_batched_entropy(void);\nstatic void numa_crng_init(void);\n\nstatic bool trust_cpu __ro_after_init = IS_ENABLED(CONFIG_RANDOM_TRUST_CPU);\nstatic int __init parse_trust_cpu(char *arg)\n{\n\treturn kstrtobool(arg, &trust_cpu);\n}\nearly_param(\"random.trust_cpu\", parse_trust_cpu);\n\nstatic bool crng_init_try_arch(struct crng_state *crng)\n{\n\tint\t\ti;\n\tbool\t\tarch_init = true;\n\tunsigned long\trv;\n\n\tfor (i = 4; i < 16; i++) {\n\t\tif (!arch_get_random_seed_long(&rv) &&\n\t\t    !arch_get_random_long(&rv)) {\n\t\t\trv = random_get_entropy();\n\t\t\tarch_init = false;\n\t\t}\n\t\tcrng->state[i] ^= rv;\n\t}\n\n\treturn arch_init;\n}\n\nstatic bool __init crng_init_try_arch_early(struct crng_state *crng)\n{\n\tint\t\ti;\n\tbool\t\tarch_init = true;\n\tunsigned long\trv;\n\n\tfor (i = 4; i < 16; i++) {\n\t\tif (!arch_get_random_seed_long_early(&rv) &&\n\t\t    !arch_get_random_long_early(&rv)) {\n\t\t\trv = random_get_entropy();\n\t\t\tarch_init = false;\n\t\t}\n\t\tcrng->state[i] ^= rv;\n\t}\n\n\treturn arch_init;\n}\n\nstatic void __maybe_unused crng_initialize_secondary(struct crng_state *crng)\n{\n\tmemcpy(&crng->state[0], \"expand 32-byte k\", 16);\n\t_get_random_bytes(&crng->state[4], sizeof(__u32) * 12);\n\tcrng_init_try_arch(crng);\n\tcrng->init_time = jiffies - CRNG_RESEED_INTERVAL - 1;\n}\n\nstatic void __init crng_initialize_primary(struct crng_state *crng)\n{\n\tmemcpy(&crng->state[0], \"expand 32-byte k\", 16);\n\t_extract_entropy(&input_pool, &crng->state[4], sizeof(__u32) * 12, 0);\n\tif (crng_init_try_arch_early(crng) && trust_cpu) {\n\t\tinvalidate_batched_entropy();\n\t\tnuma_crng_init();\n\t\tcrng_init = 2;\n\t\tpr_notice(\"crng done (trusting CPU's manufacturer)\\n\");\n\t}\n\tcrng->init_time = jiffies - CRNG_RESEED_INTERVAL - 1;\n}\n\n#ifdef CONFIG_NUMA\nstatic void do_numa_crng_init(struct work_struct *work)\n{\n\tint i;\n\tstruct crng_state *crng;\n\tstruct crng_state **pool;\n\n\tpool = kcalloc(nr_node_ids, sizeof(*pool), GFP_KERNEL|__GFP_NOFAIL);\n\tfor_each_online_node(i) {\n\t\tcrng = kmalloc_node(sizeof(struct crng_state),\n\t\t\t\t    GFP_KERNEL | __GFP_NOFAIL, i);\n\t\tspin_lock_init(&crng->lock);\n\t\tcrng_initialize_secondary(crng);\n\t\tpool[i] = crng;\n\t}\n\tmb();\n\tif (cmpxchg(&crng_node_pool, NULL, pool)) {\n\t\tfor_each_node(i)\n\t\t\tkfree(pool[i]);\n\t\tkfree(pool);\n\t}\n}\n\nstatic DECLARE_WORK(numa_crng_init_work, do_numa_crng_init);\n\nstatic void numa_crng_init(void)\n{\n\tschedule_work(&numa_crng_init_work);\n}\n#else\nstatic void numa_crng_init(void) {}\n#endif\n\n/*\n * crng_fast_load() can be called by code in the interrupt service\n * path.  So we can't afford to dilly-dally.\n */\nstatic int crng_fast_load(const char *cp, size_t len)\n{\n\tunsigned long flags;\n\tchar *p;\n\n\tif (!spin_trylock_irqsave(&primary_crng.lock, flags))\n\t\treturn 0;\n\tif (crng_init != 0) {\n\t\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\t\treturn 0;\n\t}\n\tp = (unsigned char *) &primary_crng.state[4];\n\twhile (len > 0 && crng_init_cnt < CRNG_INIT_CNT_THRESH) {\n\t\tp[crng_init_cnt % CHACHA_KEY_SIZE] ^= *cp;\n\t\tcp++; crng_init_cnt++; len--;\n\t}\n\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\tif (crng_init_cnt >= CRNG_INIT_CNT_THRESH) {\n\t\tinvalidate_batched_entropy();\n\t\tcrng_init = 1;\n\t\tpr_notice(\"fast init done\\n\");\n\t}\n\treturn 1;\n}\n\n/*\n * crng_slow_load() is called by add_device_randomness, which has two\n * attributes.  (1) We can't trust the buffer passed to it is\n * guaranteed to be unpredictable (so it might not have any entropy at\n * all), and (2) it doesn't have the performance constraints of\n * crng_fast_load().\n *\n * So we do something more comprehensive which is guaranteed to touch\n * all of the primary_crng's state, and which uses a LFSR with a\n * period of 255 as part of the mixing algorithm.  Finally, we do\n * *not* advance crng_init_cnt since buffer we may get may be something\n * like a fixed DMI table (for example), which might very well be\n * unique to the machine, but is otherwise unvarying.\n */\nstatic int crng_slow_load(const char *cp, size_t len)\n{\n\tunsigned long\t\tflags;\n\tstatic unsigned char\tlfsr = 1;\n\tunsigned char\t\ttmp;\n\tunsigned\t\ti, max = CHACHA_KEY_SIZE;\n\tconst char *\t\tsrc_buf = cp;\n\tchar *\t\t\tdest_buf = (char *) &primary_crng.state[4];\n\n\tif (!spin_trylock_irqsave(&primary_crng.lock, flags))\n\t\treturn 0;\n\tif (crng_init != 0) {\n\t\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\t\treturn 0;\n\t}\n\tif (len > max)\n\t\tmax = len;\n\n\tfor (i = 0; i < max ; i++) {\n\t\ttmp = lfsr;\n\t\tlfsr >>= 1;\n\t\tif (tmp & 1)\n\t\t\tlfsr ^= 0xE1;\n\t\ttmp = dest_buf[i % CHACHA_KEY_SIZE];\n\t\tdest_buf[i % CHACHA_KEY_SIZE] ^= src_buf[i % len] ^ lfsr;\n\t\tlfsr += (tmp << 3) | (tmp >> 5);\n\t}\n\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\treturn 1;\n}\n\nstatic void crng_reseed(struct crng_state *crng, struct entropy_store *r)\n{\n\tunsigned long\tflags;\n\tint\t\ti, num;\n\tunion {\n\t\t__u8\tblock[CHACHA_BLOCK_SIZE];\n\t\t__u32\tkey[8];\n\t} buf;\n\n\tif (r) {\n\t\tnum = extract_entropy(r, &buf, 32, 16, 0);\n\t\tif (num == 0)\n\t\t\treturn;\n\t} else {\n\t\t_extract_crng(&primary_crng, buf.block);\n\t\t_crng_backtrack_protect(&primary_crng, buf.block,\n\t\t\t\t\tCHACHA_KEY_SIZE);\n\t}\n\tspin_lock_irqsave(&crng->lock, flags);\n\tfor (i = 0; i < 8; i++) {\n\t\tunsigned long\trv;\n\t\tif (!arch_get_random_seed_long(&rv) &&\n\t\t    !arch_get_random_long(&rv))\n\t\t\trv = random_get_entropy();\n\t\tcrng->state[i+4] ^= buf.key[i] ^ rv;\n\t}\n\tmemzero_explicit(&buf, sizeof(buf));\n\tcrng->init_time = jiffies;\n\tspin_unlock_irqrestore(&crng->lock, flags);\n\tif (crng == &primary_crng && crng_init < 2) {\n\t\tinvalidate_batched_entropy();\n\t\tnuma_crng_init();\n\t\tcrng_init = 2;\n\t\tprocess_random_ready_list();\n\t\twake_up_interruptible(&crng_init_wait);\n\t\tkill_fasync(&fasync, SIGIO, POLL_IN);\n\t\tpr_notice(\"crng init done\\n\");\n\t\tif (unseeded_warning.missed) {\n\t\t\tpr_notice(\"%d get_random_xx warning(s) missed due to ratelimiting\\n\",\n\t\t\t\t  unseeded_warning.missed);\n\t\t\tunseeded_warning.missed = 0;\n\t\t}\n\t\tif (urandom_warning.missed) {\n\t\t\tpr_notice(\"%d urandom warning(s) missed due to ratelimiting\\n\",\n\t\t\t\t  urandom_warning.missed);\n\t\t\turandom_warning.missed = 0;\n\t\t}\n\t}\n}\n\nstatic void _extract_crng(struct crng_state *crng,\n\t\t\t  __u8 out[CHACHA_BLOCK_SIZE])\n{\n\tunsigned long v, flags;\n\n\tif (crng_ready() &&\n\t    (time_after(crng_global_init_time, crng->init_time) ||\n\t     time_after(jiffies, crng->init_time + CRNG_RESEED_INTERVAL)))\n\t\tcrng_reseed(crng, crng == &primary_crng ? &input_pool : NULL);\n\tspin_lock_irqsave(&crng->lock, flags);\n\tif (arch_get_random_long(&v))\n\t\tcrng->state[14] ^= v;\n\tchacha20_block(&crng->state[0], out);\n\tif (crng->state[12] == 0)\n\t\tcrng->state[13]++;\n\tspin_unlock_irqrestore(&crng->lock, flags);\n}\n\nstatic void extract_crng(__u8 out[CHACHA_BLOCK_SIZE])\n{\n\tstruct crng_state *crng = NULL;\n\n#ifdef CONFIG_NUMA\n\tif (crng_node_pool)\n\t\tcrng = crng_node_pool[numa_node_id()];\n\tif (crng == NULL)\n#endif\n\t\tcrng = &primary_crng;\n\t_extract_crng(crng, out);\n}\n\n/*\n * Use the leftover bytes from the CRNG block output (if there is\n * enough) to mutate the CRNG key to provide backtracking protection.\n */\nstatic void _crng_backtrack_protect(struct crng_state *crng,\n\t\t\t\t    __u8 tmp[CHACHA_BLOCK_SIZE], int used)\n{\n\tunsigned long\tflags;\n\t__u32\t\t*s, *d;\n\tint\t\ti;\n\n\tused = round_up(used, sizeof(__u32));\n\tif (used + CHACHA_KEY_SIZE > CHACHA_BLOCK_SIZE) {\n\t\textract_crng(tmp);\n\t\tused = 0;\n\t}\n\tspin_lock_irqsave(&crng->lock, flags);\n\ts = (__u32 *) &tmp[used];\n\td = &crng->state[4];\n\tfor (i=0; i < 8; i++)\n\t\t*d++ ^= *s++;\n\tspin_unlock_irqrestore(&crng->lock, flags);\n}\n\nstatic void crng_backtrack_protect(__u8 tmp[CHACHA_BLOCK_SIZE], int used)\n{\n\tstruct crng_state *crng = NULL;\n\n#ifdef CONFIG_NUMA\n\tif (crng_node_pool)\n\t\tcrng = crng_node_pool[numa_node_id()];\n\tif (crng == NULL)\n#endif\n\t\tcrng = &primary_crng;\n\t_crng_backtrack_protect(crng, tmp, used);\n}\n\nstatic ssize_t extract_crng_user(void __user *buf, size_t nbytes)\n{\n\tssize_t ret = 0, i = CHACHA_BLOCK_SIZE;\n\t__u8 tmp[CHACHA_BLOCK_SIZE] __aligned(4);\n\tint large_request = (nbytes > 256);\n\n\twhile (nbytes) {\n\t\tif (large_request && need_resched()) {\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tschedule();\n\t\t}\n\n\t\textract_crng(tmp);\n\t\ti = min_t(int, nbytes, CHACHA_BLOCK_SIZE);\n\t\tif (copy_to_user(buf, tmp, i)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tnbytes -= i;\n\t\tbuf += i;\n\t\tret += i;\n\t}\n\tcrng_backtrack_protect(tmp, i);\n\n\t/* Wipe data just written to memory */\n\tmemzero_explicit(tmp, sizeof(tmp));\n\n\treturn ret;\n}\n\n\n/*********************************************************************\n *\n * Entropy input management\n *\n *********************************************************************/\n\n/* There is one of these per entropy source */\nstruct timer_rand_state {\n\tcycles_t last_time;\n\tlong last_delta, last_delta2;\n};\n\n#define INIT_TIMER_RAND_STATE { INITIAL_JIFFIES, };\n\n/*\n * Add device- or boot-specific data to the input pool to help\n * initialize it.\n *\n * None of this adds any entropy; it is meant to avoid the problem of\n * the entropy pool having similar initial state across largely\n * identical devices.\n */\nvoid add_device_randomness(const void *buf, unsigned int size)\n{\n\tunsigned long time = random_get_entropy() ^ jiffies;\n\tunsigned long flags;\n\n\tif (!crng_ready() && size)\n\t\tcrng_slow_load(buf, size);\n\n\ttrace_add_device_randomness(size, _RET_IP_);\n\tspin_lock_irqsave(&input_pool.lock, flags);\n\t_mix_pool_bytes(&input_pool, buf, size);\n\t_mix_pool_bytes(&input_pool, &time, sizeof(time));\n\tspin_unlock_irqrestore(&input_pool.lock, flags);\n}\nEXPORT_SYMBOL(add_device_randomness);\n\nstatic struct timer_rand_state input_timer_state = INIT_TIMER_RAND_STATE;\n\n/*\n * This function adds entropy to the entropy \"pool\" by using timing\n * delays.  It uses the timer_rand_state structure to make an estimate\n * of how many bits of entropy this call has added to the pool.\n *\n * The number \"num\" is also added to the pool - it should somehow describe\n * the type of event which just happened.  This is currently 0-255 for\n * keyboard scan codes, and 256 upwards for interrupts.\n *\n */\nstatic void add_timer_randomness(struct timer_rand_state *state, unsigned num)\n{\n\tstruct entropy_store\t*r;\n\tstruct {\n\t\tlong jiffies;\n\t\tunsigned cycles;\n\t\tunsigned num;\n\t} sample;\n\tlong delta, delta2, delta3;\n\n\tsample.jiffies = jiffies;\n\tsample.cycles = random_get_entropy();\n\tsample.num = num;\n\tr = &input_pool;\n\tmix_pool_bytes(r, &sample, sizeof(sample));\n\n\t/*\n\t * Calculate number of bits of randomness we probably added.\n\t * We take into account the first, second and third-order deltas\n\t * in order to make our estimate.\n\t */\n\tdelta = sample.jiffies - READ_ONCE(state->last_time);\n\tWRITE_ONCE(state->last_time, sample.jiffies);\n\n\tdelta2 = delta - READ_ONCE(state->last_delta);\n\tWRITE_ONCE(state->last_delta, delta);\n\n\tdelta3 = delta2 - READ_ONCE(state->last_delta2);\n\tWRITE_ONCE(state->last_delta2, delta2);\n\n\tif (delta < 0)\n\t\tdelta = -delta;\n\tif (delta2 < 0)\n\t\tdelta2 = -delta2;\n\tif (delta3 < 0)\n\t\tdelta3 = -delta3;\n\tif (delta > delta2)\n\t\tdelta = delta2;\n\tif (delta > delta3)\n\t\tdelta = delta3;\n\n\t/*\n\t * delta is now minimum absolute delta.\n\t * Round down by 1 bit on general principles,\n\t * and limit entropy estimate to 12 bits.\n\t */\n\tcredit_entropy_bits(r, min_t(int, fls(delta>>1), 11));\n}\n\nvoid add_input_randomness(unsigned int type, unsigned int code,\n\t\t\t\t unsigned int value)\n{\n\tstatic unsigned char last_value;\n\n\t/* ignore autorepeat and the like */\n\tif (value == last_value)\n\t\treturn;\n\n\tlast_value = value;\n\tadd_timer_randomness(&input_timer_state,\n\t\t\t     (type << 4) ^ code ^ (code >> 4) ^ value);\n\ttrace_add_input_randomness(ENTROPY_BITS(&input_pool));\n}\nEXPORT_SYMBOL_GPL(add_input_randomness);\n\nstatic DEFINE_PER_CPU(struct fast_pool, irq_randomness);\n\n#ifdef ADD_INTERRUPT_BENCH\nstatic unsigned long avg_cycles, avg_deviation;\n\n#define AVG_SHIFT 8     /* Exponential average factor k=1/256 */\n#define FIXED_1_2 (1 << (AVG_SHIFT-1))\n\nstatic void add_interrupt_bench(cycles_t start)\n{\n        long delta = random_get_entropy() - start;\n\n        /* Use a weighted moving average */\n        delta = delta - ((avg_cycles + FIXED_1_2) >> AVG_SHIFT);\n        avg_cycles += delta;\n        /* And average deviation */\n        delta = abs(delta) - ((avg_deviation + FIXED_1_2) >> AVG_SHIFT);\n        avg_deviation += delta;\n}\n#else\n#define add_interrupt_bench(x)\n#endif\n\nstatic __u32 get_reg(struct fast_pool *f, struct pt_regs *regs)\n{\n\t__u32 *ptr = (__u32 *) regs;\n\tunsigned int idx;\n\n\tif (regs == NULL)\n\t\treturn 0;\n\tidx = READ_ONCE(f->reg_idx);\n\tif (idx >= sizeof(struct pt_regs) / sizeof(__u32))\n\t\tidx = 0;\n\tptr += idx++;\n\tWRITE_ONCE(f->reg_idx, idx);\n\treturn *ptr;\n}\n\nvoid add_interrupt_randomness(int irq, int irq_flags)\n{\n\tstruct entropy_store\t*r;\n\tstruct fast_pool\t*fast_pool = this_cpu_ptr(&irq_randomness);\n\tstruct pt_regs\t\t*regs = get_irq_regs();\n\tunsigned long\t\tnow = jiffies;\n\tcycles_t\t\tcycles = random_get_entropy();\n\t__u32\t\t\tc_high, j_high;\n\t__u64\t\t\tip;\n\tunsigned long\t\tseed;\n\tint\t\t\tcredit = 0;\n\n\tif (cycles == 0)\n\t\tcycles = get_reg(fast_pool, regs);\n\tc_high = (sizeof(cycles) > 4) ? cycles >> 32 : 0;\n\tj_high = (sizeof(now) > 4) ? now >> 32 : 0;\n\tfast_pool->pool[0] ^= cycles ^ j_high ^ irq;\n\tfast_pool->pool[1] ^= now ^ c_high;\n\tip = regs ? instruction_pointer(regs) : _RET_IP_;\n\tfast_pool->pool[2] ^= ip;\n\tfast_pool->pool[3] ^= (sizeof(ip) > 4) ? ip >> 32 :\n\t\tget_reg(fast_pool, regs);\n\n\tfast_mix(fast_pool);\n\tadd_interrupt_bench(cycles);\n\tthis_cpu_add(net_rand_state.s1, fast_pool->pool[cycles & 3]);\n\n\tif (unlikely(crng_init == 0)) {\n\t\tif ((fast_pool->count >= 64) &&\n\t\t    crng_fast_load((char *) fast_pool->pool,\n\t\t\t\t   sizeof(fast_pool->pool))) {\n\t\t\tfast_pool->count = 0;\n\t\t\tfast_pool->last = now;\n\t\t}\n\t\treturn;\n\t}\n\n\tif ((fast_pool->count < 64) &&\n\t    !time_after(now, fast_pool->last + HZ))\n\t\treturn;\n\n\tr = &input_pool;\n\tif (!spin_trylock(&r->lock))\n\t\treturn;\n\n\tfast_pool->last = now;\n\t__mix_pool_bytes(r, &fast_pool->pool, sizeof(fast_pool->pool));\n\n\t/*\n\t * If we have architectural seed generator, produce a seed and\n\t * add it to the pool.  For the sake of paranoia don't let the\n\t * architectural seed generator dominate the input from the\n\t * interrupt noise.\n\t */\n\tif (arch_get_random_seed_long(&seed)) {\n\t\t__mix_pool_bytes(r, &seed, sizeof(seed));\n\t\tcredit = 1;\n\t}\n\tspin_unlock(&r->lock);\n\n\tfast_pool->count = 0;\n\n\t/* award one bit for the contents of the fast pool */\n\tcredit_entropy_bits(r, credit + 1);\n}\nEXPORT_SYMBOL_GPL(add_interrupt_randomness);\n\n#ifdef CONFIG_BLOCK\nvoid add_disk_randomness(struct gendisk *disk)\n{\n\tif (!disk || !disk->random)\n\t\treturn;\n\t/* first major is 1, so we get >= 0x200 here */\n\tadd_timer_randomness(disk->random, 0x100 + disk_devt(disk));\n\ttrace_add_disk_randomness(disk_devt(disk), ENTROPY_BITS(&input_pool));\n}\nEXPORT_SYMBOL_GPL(add_disk_randomness);\n#endif\n\n/*********************************************************************\n *\n * Entropy extraction routines\n *\n *********************************************************************/\n\n/*\n * This function decides how many bytes to actually take from the\n * given pool, and also debits the entropy count accordingly.\n */\nstatic size_t account(struct entropy_store *r, size_t nbytes, int min,\n\t\t      int reserved)\n{\n\tint entropy_count, orig, have_bytes;\n\tsize_t ibytes, nfrac;\n\n\tBUG_ON(r->entropy_count > r->poolinfo->poolfracbits);\n\n\t/* Can we pull enough? */\nretry:\n\tentropy_count = orig = READ_ONCE(r->entropy_count);\n\tibytes = nbytes;\n\t/* never pull more than available */\n\thave_bytes = entropy_count >> (ENTROPY_SHIFT + 3);\n\n\tif ((have_bytes -= reserved) < 0)\n\t\thave_bytes = 0;\n\tibytes = min_t(size_t, ibytes, have_bytes);\n\tif (ibytes < min)\n\t\tibytes = 0;\n\n\tif (WARN_ON(entropy_count < 0)) {\n\t\tpr_warn(\"negative entropy count: pool %s count %d\\n\",\n\t\t\tr->name, entropy_count);\n\t\tentropy_count = 0;\n\t}\n\tnfrac = ibytes << (ENTROPY_SHIFT + 3);\n\tif ((size_t) entropy_count > nfrac)\n\t\tentropy_count -= nfrac;\n\telse\n\t\tentropy_count = 0;\n\n\tif (cmpxchg(&r->entropy_count, orig, entropy_count) != orig)\n\t\tgoto retry;\n\n\ttrace_debit_entropy(r->name, 8 * ibytes);\n\tif (ibytes && ENTROPY_BITS(r) < random_write_wakeup_bits) {\n\t\twake_up_interruptible(&random_write_wait);\n\t\tkill_fasync(&fasync, SIGIO, POLL_OUT);\n\t}\n\n\treturn ibytes;\n}\n\n/*\n * This function does the actual extraction for extract_entropy and\n * extract_entropy_user.\n *\n * Note: we assume that .poolwords is a multiple of 16 words.\n */\nstatic void extract_buf(struct entropy_store *r, __u8 *out)\n{\n\tint i;\n\tunion {\n\t\t__u32 w[5];\n\t\tunsigned long l[LONGS(20)];\n\t} hash;\n\t__u32 workspace[SHA1_WORKSPACE_WORDS];\n\tunsigned long flags;\n\n\t/*\n\t * If we have an architectural hardware random number\n\t * generator, use it for SHA's initial vector\n\t */\n\tsha1_init(hash.w);\n\tfor (i = 0; i < LONGS(20); i++) {\n\t\tunsigned long v;\n\t\tif (!arch_get_random_long(&v))\n\t\t\tbreak;\n\t\thash.l[i] = v;\n\t}\n\n\t/* Generate a hash across the pool, 16 words (512 bits) at a time */\n\tspin_lock_irqsave(&r->lock, flags);\n\tfor (i = 0; i < r->poolinfo->poolwords; i += 16)\n\t\tsha1_transform(hash.w, (__u8 *)(r->pool + i), workspace);\n\n\t/*\n\t * We mix the hash back into the pool to prevent backtracking\n\t * attacks (where the attacker knows the state of the pool\n\t * plus the current outputs, and attempts to find previous\n\t * ouputs), unless the hash function can be inverted. By\n\t * mixing at least a SHA1 worth of hash data back, we make\n\t * brute-forcing the feedback as hard as brute-forcing the\n\t * hash.\n\t */\n\t__mix_pool_bytes(r, hash.w, sizeof(hash.w));\n\tspin_unlock_irqrestore(&r->lock, flags);\n\n\tmemzero_explicit(workspace, sizeof(workspace));\n\n\t/*\n\t * In case the hash function has some recognizable output\n\t * pattern, we fold it in half. Thus, we always feed back\n\t * twice as much data as we output.\n\t */\n\thash.w[0] ^= hash.w[3];\n\thash.w[1] ^= hash.w[4];\n\thash.w[2] ^= rol32(hash.w[2], 16);\n\n\tmemcpy(out, &hash, EXTRACT_SIZE);\n\tmemzero_explicit(&hash, sizeof(hash));\n}\n\nstatic ssize_t _extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t\tsize_t nbytes, int fips)\n{\n\tssize_t ret = 0, i;\n\t__u8 tmp[EXTRACT_SIZE];\n\tunsigned long flags;\n\n\twhile (nbytes) {\n\t\textract_buf(r, tmp);\n\n\t\tif (fips) {\n\t\t\tspin_lock_irqsave(&r->lock, flags);\n\t\t\tif (!memcmp(tmp, r->last_data, EXTRACT_SIZE))\n\t\t\t\tpanic(\"Hardware RNG duplicated output!\\n\");\n\t\t\tmemcpy(r->last_data, tmp, EXTRACT_SIZE);\n\t\t\tspin_unlock_irqrestore(&r->lock, flags);\n\t\t}\n\t\ti = min_t(int, nbytes, EXTRACT_SIZE);\n\t\tmemcpy(buf, tmp, i);\n\t\tnbytes -= i;\n\t\tbuf += i;\n\t\tret += i;\n\t}\n\n\t/* Wipe data just returned from memory */\n\tmemzero_explicit(tmp, sizeof(tmp));\n\n\treturn ret;\n}\n\n/*\n * This function extracts randomness from the \"entropy pool\", and\n * returns it in a buffer.\n *\n * The min parameter specifies the minimum amount we can pull before\n * failing to avoid races that defeat catastrophic reseeding while the\n * reserved parameter indicates how much entropy we must leave in the\n * pool after each pull to avoid starving other readers.\n */\nstatic ssize_t extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t\t size_t nbytes, int min, int reserved)\n{\n\t__u8 tmp[EXTRACT_SIZE];\n\tunsigned long flags;\n\n\t/* if last_data isn't primed, we need EXTRACT_SIZE extra bytes */\n\tif (fips_enabled) {\n\t\tspin_lock_irqsave(&r->lock, flags);\n\t\tif (!r->last_data_init) {\n\t\t\tr->last_data_init = 1;\n\t\t\tspin_unlock_irqrestore(&r->lock, flags);\n\t\t\ttrace_extract_entropy(r->name, EXTRACT_SIZE,\n\t\t\t\t\t      ENTROPY_BITS(r), _RET_IP_);\n\t\t\textract_buf(r, tmp);\n\t\t\tspin_lock_irqsave(&r->lock, flags);\n\t\t\tmemcpy(r->last_data, tmp, EXTRACT_SIZE);\n\t\t}\n\t\tspin_unlock_irqrestore(&r->lock, flags);\n\t}\n\n\ttrace_extract_entropy(r->name, nbytes, ENTROPY_BITS(r), _RET_IP_);\n\tnbytes = account(r, nbytes, min, reserved);\n\n\treturn _extract_entropy(r, buf, nbytes, fips_enabled);\n}\n\n#define warn_unseeded_randomness(previous) \\\n\t_warn_unseeded_randomness(__func__, (void *) _RET_IP_, (previous))\n\nstatic void _warn_unseeded_randomness(const char *func_name, void *caller,\n\t\t\t\t      void **previous)\n{\n#ifdef CONFIG_WARN_ALL_UNSEEDED_RANDOM\n\tconst bool print_once = false;\n#else\n\tstatic bool print_once __read_mostly;\n#endif\n\n\tif (print_once ||\n\t    crng_ready() ||\n\t    (previous && (caller == READ_ONCE(*previous))))\n\t\treturn;\n\tWRITE_ONCE(*previous, caller);\n#ifndef CONFIG_WARN_ALL_UNSEEDED_RANDOM\n\tprint_once = true;\n#endif\n\tif (__ratelimit(&unseeded_warning))\n\t\tprintk_deferred(KERN_NOTICE \"random: %s called from %pS \"\n\t\t\t\t\"with crng_init=%d\\n\", func_name, caller,\n\t\t\t\tcrng_init);\n}\n\n/*\n * This function is the exported kernel interface.  It returns some\n * number of good random numbers, suitable for key generation, seeding\n * TCP sequence numbers, etc.  It does not rely on the hardware random\n * number generator.  For random bytes direct from the hardware RNG\n * (when available), use get_random_bytes_arch(). In order to ensure\n * that the randomness provided by this function is okay, the function\n * wait_for_random_bytes() should be called and return 0 at least once\n * at any point prior.\n */\nstatic void _get_random_bytes(void *buf, int nbytes)\n{\n\t__u8 tmp[CHACHA_BLOCK_SIZE] __aligned(4);\n\n\ttrace_get_random_bytes(nbytes, _RET_IP_);\n\n\twhile (nbytes >= CHACHA_BLOCK_SIZE) {\n\t\textract_crng(buf);\n\t\tbuf += CHACHA_BLOCK_SIZE;\n\t\tnbytes -= CHACHA_BLOCK_SIZE;\n\t}\n\n\tif (nbytes > 0) {\n\t\textract_crng(tmp);\n\t\tmemcpy(buf, tmp, nbytes);\n\t\tcrng_backtrack_protect(tmp, nbytes);\n\t} else\n\t\tcrng_backtrack_protect(tmp, CHACHA_BLOCK_SIZE);\n\tmemzero_explicit(tmp, sizeof(tmp));\n}\n\nvoid get_random_bytes(void *buf, int nbytes)\n{\n\tstatic void *previous;\n\n\twarn_unseeded_randomness(&previous);\n\t_get_random_bytes(buf, nbytes);\n}\nEXPORT_SYMBOL(get_random_bytes);\n\n\n/*\n * Each time the timer fires, we expect that we got an unpredictable\n * jump in the cycle counter. Even if the timer is running on another\n * CPU, the timer activity will be touching the stack of the CPU that is\n * generating entropy..\n *\n * Note that we don't re-arm the timer in the timer itself - we are\n * happy to be scheduled away, since that just makes the load more\n * complex, but we do not want the timer to keep ticking unless the\n * entropy loop is running.\n *\n * So the re-arming always happens in the entropy loop itself.\n */\nstatic void entropy_timer(struct timer_list *t)\n{\n\tcredit_entropy_bits(&input_pool, 1);\n}\n\n/*\n * If we have an actual cycle counter, see if we can\n * generate enough entropy with timing noise\n */\nstatic void try_to_generate_entropy(void)\n{\n\tstruct {\n\t\tunsigned long now;\n\t\tstruct timer_list timer;\n\t} stack;\n\n\tstack.now = random_get_entropy();\n\n\t/* Slow counter - or none. Don't even bother */\n\tif (stack.now == random_get_entropy())\n\t\treturn;\n\n\ttimer_setup_on_stack(&stack.timer, entropy_timer, 0);\n\twhile (!crng_ready()) {\n\t\tif (!timer_pending(&stack.timer))\n\t\t\tmod_timer(&stack.timer, jiffies+1);\n\t\tmix_pool_bytes(&input_pool, &stack.now, sizeof(stack.now));\n\t\tschedule();\n\t\tstack.now = random_get_entropy();\n\t}\n\n\tdel_timer_sync(&stack.timer);\n\tdestroy_timer_on_stack(&stack.timer);\n\tmix_pool_bytes(&input_pool, &stack.now, sizeof(stack.now));\n}\n\n/*\n * Wait for the urandom pool to be seeded and thus guaranteed to supply\n * cryptographically secure random numbers. This applies to: the /dev/urandom\n * device, the get_random_bytes function, and the get_random_{u32,u64,int,long}\n * family of functions. Using any of these functions without first calling\n * this function forfeits the guarantee of security.\n *\n * Returns: 0 if the urandom pool has been seeded.\n *          -ERESTARTSYS if the function was interrupted by a signal.\n */\nint wait_for_random_bytes(void)\n{\n\tif (likely(crng_ready()))\n\t\treturn 0;\n\n\tdo {\n\t\tint ret;\n\t\tret = wait_event_interruptible_timeout(crng_init_wait, crng_ready(), HZ);\n\t\tif (ret)\n\t\t\treturn ret > 0 ? 0 : ret;\n\n\t\ttry_to_generate_entropy();\n\t} while (!crng_ready());\n\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_random_bytes);\n\n/*\n * Returns whether or not the urandom pool has been seeded and thus guaranteed\n * to supply cryptographically secure random numbers. This applies to: the\n * /dev/urandom device, the get_random_bytes function, and the get_random_{u32,\n * ,u64,int,long} family of functions.\n *\n * Returns: true if the urandom pool has been seeded.\n *          false if the urandom pool has not been seeded.\n */\nbool rng_is_initialized(void)\n{\n\treturn crng_ready();\n}\nEXPORT_SYMBOL(rng_is_initialized);\n\n/*\n * Add a callback function that will be invoked when the nonblocking\n * pool is initialised.\n *\n * returns: 0 if callback is successfully added\n *\t    -EALREADY if pool is already initialised (callback not called)\n *\t    -ENOENT if module for callback is not alive\n */\nint add_random_ready_callback(struct random_ready_callback *rdy)\n{\n\tstruct module *owner;\n\tunsigned long flags;\n\tint err = -EALREADY;\n\n\tif (crng_ready())\n\t\treturn err;\n\n\towner = rdy->owner;\n\tif (!try_module_get(owner))\n\t\treturn -ENOENT;\n\n\tspin_lock_irqsave(&random_ready_list_lock, flags);\n\tif (crng_ready())\n\t\tgoto out;\n\n\towner = NULL;\n\n\tlist_add(&rdy->list, &random_ready_list);\n\terr = 0;\n\nout:\n\tspin_unlock_irqrestore(&random_ready_list_lock, flags);\n\n\tmodule_put(owner);\n\n\treturn err;\n}\nEXPORT_SYMBOL(add_random_ready_callback);\n\n/*\n * Delete a previously registered readiness callback function.\n */\nvoid del_random_ready_callback(struct random_ready_callback *rdy)\n{\n\tunsigned long flags;\n\tstruct module *owner = NULL;\n\n\tspin_lock_irqsave(&random_ready_list_lock, flags);\n\tif (!list_empty(&rdy->list)) {\n\t\tlist_del_init(&rdy->list);\n\t\towner = rdy->owner;\n\t}\n\tspin_unlock_irqrestore(&random_ready_list_lock, flags);\n\n\tmodule_put(owner);\n}\nEXPORT_SYMBOL(del_random_ready_callback);\n\n/*\n * This function will use the architecture-specific hardware random\n * number generator if it is available.  The arch-specific hw RNG will\n * almost certainly be faster than what we can do in software, but it\n * is impossible to verify that it is implemented securely (as\n * opposed, to, say, the AES encryption of a sequence number using a\n * key known by the NSA).  So it's useful if we need the speed, but\n * only if we're willing to trust the hardware manufacturer not to\n * have put in a back door.\n *\n * Return number of bytes filled in.\n */\nint __must_check get_random_bytes_arch(void *buf, int nbytes)\n{\n\tint left = nbytes;\n\tchar *p = buf;\n\n\ttrace_get_random_bytes_arch(left, _RET_IP_);\n\twhile (left) {\n\t\tunsigned long v;\n\t\tint chunk = min_t(int, left, sizeof(unsigned long));\n\n\t\tif (!arch_get_random_long(&v))\n\t\t\tbreak;\n\n\t\tmemcpy(p, &v, chunk);\n\t\tp += chunk;\n\t\tleft -= chunk;\n\t}\n\n\treturn nbytes - left;\n}\nEXPORT_SYMBOL(get_random_bytes_arch);\n\n/*\n * init_std_data - initialize pool with system data\n *\n * @r: pool to initialize\n *\n * This function clears the pool's entropy count and mixes some system\n * data into the pool to prepare it for use. The pool is not cleared\n * as that can only decrease the entropy in the pool.\n */\nstatic void __init init_std_data(struct entropy_store *r)\n{\n\tint i;\n\tktime_t now = ktime_get_real();\n\tunsigned long rv;\n\n\tmix_pool_bytes(r, &now, sizeof(now));\n\tfor (i = r->poolinfo->poolbytes; i > 0; i -= sizeof(rv)) {\n\t\tif (!arch_get_random_seed_long(&rv) &&\n\t\t    !arch_get_random_long(&rv))\n\t\t\trv = random_get_entropy();\n\t\tmix_pool_bytes(r, &rv, sizeof(rv));\n\t}\n\tmix_pool_bytes(r, utsname(), sizeof(*(utsname())));\n}\n\n/*\n * Note that setup_arch() may call add_device_randomness()\n * long before we get here. This allows seeding of the pools\n * with some platform dependent data very early in the boot\n * process. But it limits our options here. We must use\n * statically allocated structures that already have all\n * initializations complete at compile time. We should also\n * take care not to overwrite the precious per platform data\n * we were given.\n */\nint __init rand_initialize(void)\n{\n\tinit_std_data(&input_pool);\n\tcrng_initialize_primary(&primary_crng);\n\tcrng_global_init_time = jiffies;\n\tif (ratelimit_disable) {\n\t\turandom_warning.interval = 0;\n\t\tunseeded_warning.interval = 0;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_BLOCK\nvoid rand_initialize_disk(struct gendisk *disk)\n{\n\tstruct timer_rand_state *state;\n\n\t/*\n\t * If kzalloc returns null, we just won't use that entropy\n\t * source.\n\t */\n\tstate = kzalloc(sizeof(struct timer_rand_state), GFP_KERNEL);\n\tif (state) {\n\t\tstate->last_time = INITIAL_JIFFIES;\n\t\tdisk->random = state;\n\t}\n}\n#endif\n\nstatic ssize_t\nurandom_read_nowarn(struct file *file, char __user *buf, size_t nbytes,\n\t\t    loff_t *ppos)\n{\n\tint ret;\n\n\tnbytes = min_t(size_t, nbytes, INT_MAX >> (ENTROPY_SHIFT + 3));\n\tret = extract_crng_user(buf, nbytes);\n\ttrace_urandom_read(8 * nbytes, 0, ENTROPY_BITS(&input_pool));\n\treturn ret;\n}\n\nstatic ssize_t\nurandom_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)\n{\n\tunsigned long flags;\n\tstatic int maxwarn = 10;\n\n\tif (!crng_ready() && maxwarn > 0) {\n\t\tmaxwarn--;\n\t\tif (__ratelimit(&urandom_warning))\n\t\t\tpr_notice(\"%s: uninitialized urandom read (%zd bytes read)\\n\",\n\t\t\t\t  current->comm, nbytes);\n\t\tspin_lock_irqsave(&primary_crng.lock, flags);\n\t\tcrng_init_cnt = 0;\n\t\tspin_unlock_irqrestore(&primary_crng.lock, flags);\n\t}\n\n\treturn urandom_read_nowarn(file, buf, nbytes, ppos);\n}\n\nstatic ssize_t\nrandom_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)\n{\n\tint ret;\n\n\tret = wait_for_random_bytes();\n\tif (ret != 0)\n\t\treturn ret;\n\treturn urandom_read_nowarn(file, buf, nbytes, ppos);\n}\n\nstatic __poll_t\nrandom_poll(struct file *file, poll_table * wait)\n{\n\t__poll_t mask;\n\n\tpoll_wait(file, &crng_init_wait, wait);\n\tpoll_wait(file, &random_write_wait, wait);\n\tmask = 0;\n\tif (crng_ready())\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\tif (ENTROPY_BITS(&input_pool) < random_write_wakeup_bits)\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\treturn mask;\n}\n\nstatic int\nwrite_pool(struct entropy_store *r, const char __user *buffer, size_t count)\n{\n\tsize_t bytes;\n\t__u32 t, buf[16];\n\tconst char __user *p = buffer;\n\n\twhile (count > 0) {\n\t\tint b, i = 0;\n\n\t\tbytes = min(count, sizeof(buf));\n\t\tif (copy_from_user(&buf, p, bytes))\n\t\t\treturn -EFAULT;\n\n\t\tfor (b = bytes ; b > 0 ; b -= sizeof(__u32), i++) {\n\t\t\tif (!arch_get_random_int(&t))\n\t\t\t\tbreak;\n\t\t\tbuf[i] ^= t;\n\t\t}\n\n\t\tcount -= bytes;\n\t\tp += bytes;\n\n\t\tmix_pool_bytes(r, buf, bytes);\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t random_write(struct file *file, const char __user *buffer,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tsize_t ret;\n\n\tret = write_pool(&input_pool, buffer, count);\n\tif (ret)\n\t\treturn ret;\n\n\treturn (ssize_t)count;\n}\n\nstatic long random_ioctl(struct file *f, unsigned int cmd, unsigned long arg)\n{\n\tint size, ent_count;\n\tint __user *p = (int __user *)arg;\n\tint retval;\n\n\tswitch (cmd) {\n\tcase RNDGETENTCNT:\n\t\t/* inherently racy, no point locking */\n\t\tent_count = ENTROPY_BITS(&input_pool);\n\t\tif (put_user(ent_count, p))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase RNDADDTOENTCNT:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (get_user(ent_count, p))\n\t\t\treturn -EFAULT;\n\t\treturn credit_entropy_bits_safe(&input_pool, ent_count);\n\tcase RNDADDENTROPY:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (get_user(ent_count, p++))\n\t\t\treturn -EFAULT;\n\t\tif (ent_count < 0)\n\t\t\treturn -EINVAL;\n\t\tif (get_user(size, p++))\n\t\t\treturn -EFAULT;\n\t\tretval = write_pool(&input_pool, (const char __user *)p,\n\t\t\t\t    size);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\t\treturn credit_entropy_bits_safe(&input_pool, ent_count);\n\tcase RNDZAPENTCNT:\n\tcase RNDCLEARPOOL:\n\t\t/*\n\t\t * Clear the entropy pool counters. We no longer clear\n\t\t * the entropy pool, as that's silly.\n\t\t */\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tinput_pool.entropy_count = 0;\n\t\treturn 0;\n\tcase RNDRESEEDCRNG:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (crng_init < 2)\n\t\t\treturn -ENODATA;\n\t\tcrng_reseed(&primary_crng, NULL);\n\t\tcrng_global_init_time = jiffies - 1;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int random_fasync(int fd, struct file *filp, int on)\n{\n\treturn fasync_helper(fd, filp, on, &fasync);\n}\n\nconst struct file_operations random_fops = {\n\t.read  = random_read,\n\t.write = random_write,\n\t.poll  = random_poll,\n\t.unlocked_ioctl = random_ioctl,\n\t.compat_ioctl = compat_ptr_ioctl,\n\t.fasync = random_fasync,\n\t.llseek = noop_llseek,\n};\n\nconst struct file_operations urandom_fops = {\n\t.read  = urandom_read,\n\t.write = random_write,\n\t.unlocked_ioctl = random_ioctl,\n\t.compat_ioctl = compat_ptr_ioctl,\n\t.fasync = random_fasync,\n\t.llseek = noop_llseek,\n};\n\nSYSCALL_DEFINE3(getrandom, char __user *, buf, size_t, count,\n\t\tunsigned int, flags)\n{\n\tint ret;\n\n\tif (flags & ~(GRND_NONBLOCK|GRND_RANDOM|GRND_INSECURE))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Requesting insecure and blocking randomness at the same time makes\n\t * no sense.\n\t */\n\tif ((flags & (GRND_INSECURE|GRND_RANDOM)) == (GRND_INSECURE|GRND_RANDOM))\n\t\treturn -EINVAL;\n\n\tif (count > INT_MAX)\n\t\tcount = INT_MAX;\n\n\tif (!(flags & GRND_INSECURE) && !crng_ready()) {\n\t\tif (flags & GRND_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\t\tret = wait_for_random_bytes();\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\treturn urandom_read_nowarn(NULL, buf, count, NULL);\n}\n\n/********************************************************************\n *\n * Sysctl interface\n *\n ********************************************************************/\n\n#ifdef CONFIG_SYSCTL\n\n#include <linux/sysctl.h>\n\nstatic int min_write_thresh;\nstatic int max_write_thresh = INPUT_POOL_WORDS * 32;\nstatic int random_min_urandom_seed = 60;\nstatic char sysctl_bootid[16];\n\n/*\n * This function is used to return both the bootid UUID, and random\n * UUID.  The difference is in whether table->data is NULL; if it is,\n * then a new UUID is generated and returned to the user.\n *\n * If the user accesses this via the proc interface, the UUID will be\n * returned as an ASCII string in the standard UUID format; if via the\n * sysctl system call, as 16 bytes of binary data.\n */\nstatic int proc_do_uuid(struct ctl_table *table, int write,\n\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table fake_table;\n\tunsigned char buf[64], tmp_uuid[16], *uuid;\n\n\tuuid = table->data;\n\tif (!uuid) {\n\t\tuuid = tmp_uuid;\n\t\tgenerate_random_uuid(uuid);\n\t} else {\n\t\tstatic DEFINE_SPINLOCK(bootid_spinlock);\n\n\t\tspin_lock(&bootid_spinlock);\n\t\tif (!uuid[8])\n\t\t\tgenerate_random_uuid(uuid);\n\t\tspin_unlock(&bootid_spinlock);\n\t}\n\n\tsprintf(buf, \"%pU\", uuid);\n\n\tfake_table.data = buf;\n\tfake_table.maxlen = sizeof(buf);\n\n\treturn proc_dostring(&fake_table, write, buffer, lenp, ppos);\n}\n\n/*\n * Return entropy available scaled to integral bits\n */\nstatic int proc_do_entropy(struct ctl_table *table, int write,\n\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table fake_table;\n\tint entropy_count;\n\n\tentropy_count = *(int *)table->data >> ENTROPY_SHIFT;\n\n\tfake_table.data = &entropy_count;\n\tfake_table.maxlen = sizeof(entropy_count);\n\n\treturn proc_dointvec(&fake_table, write, buffer, lenp, ppos);\n}\n\nstatic int sysctl_poolsize = INPUT_POOL_WORDS * 32;\nextern struct ctl_table random_table[];\nstruct ctl_table random_table[] = {\n\t{\n\t\t.procname\t= \"poolsize\",\n\t\t.data\t\t= &sysctl_poolsize,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"entropy_avail\",\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_entropy,\n\t\t.data\t\t= &input_pool.entropy_count,\n\t},\n\t{\n\t\t.procname\t= \"write_wakeup_threshold\",\n\t\t.data\t\t= &random_write_wakeup_bits,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_write_thresh,\n\t\t.extra2\t\t= &max_write_thresh,\n\t},\n\t{\n\t\t.procname\t= \"urandom_min_reseed_secs\",\n\t\t.data\t\t= &random_min_urandom_seed,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"boot_id\",\n\t\t.data\t\t= &sysctl_bootid,\n\t\t.maxlen\t\t= 16,\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_uuid,\n\t},\n\t{\n\t\t.procname\t= \"uuid\",\n\t\t.maxlen\t\t= 16,\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_uuid,\n\t},\n#ifdef ADD_INTERRUPT_BENCH\n\t{\n\t\t.procname\t= \"add_interrupt_avg_cycles\",\n\t\t.data\t\t= &avg_cycles,\n\t\t.maxlen\t\t= sizeof(avg_cycles),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"add_interrupt_avg_deviation\",\n\t\t.data\t\t= &avg_deviation,\n\t\t.maxlen\t\t= sizeof(avg_deviation),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n#endif\n\t{ }\n};\n#endif \t/* CONFIG_SYSCTL */\n\nstruct batched_entropy {\n\tunion {\n\t\tu64 entropy_u64[CHACHA_BLOCK_SIZE / sizeof(u64)];\n\t\tu32 entropy_u32[CHACHA_BLOCK_SIZE / sizeof(u32)];\n\t};\n\tunsigned int position;\n\tspinlock_t batch_lock;\n};\n\n/*\n * Get a random word for internal kernel use only. The quality of the random\n * number is good as /dev/urandom, but there is no backtrack protection, with\n * the goal of being quite fast and not depleting entropy. In order to ensure\n * that the randomness provided by this function is okay, the function\n * wait_for_random_bytes() should be called and return 0 at least once at any\n * point prior.\n */\nstatic DEFINE_PER_CPU(struct batched_entropy, batched_entropy_u64) = {\n\t.batch_lock\t= __SPIN_LOCK_UNLOCKED(batched_entropy_u64.lock),\n};\n\nu64 get_random_u64(void)\n{\n\tu64 ret;\n\tunsigned long flags;\n\tstruct batched_entropy *batch;\n\tstatic void *previous;\n\n\twarn_unseeded_randomness(&previous);\n\n\tbatch = raw_cpu_ptr(&batched_entropy_u64);\n\tspin_lock_irqsave(&batch->batch_lock, flags);\n\tif (batch->position % ARRAY_SIZE(batch->entropy_u64) == 0) {\n\t\textract_crng((u8 *)batch->entropy_u64);\n\t\tbatch->position = 0;\n\t}\n\tret = batch->entropy_u64[batch->position++];\n\tspin_unlock_irqrestore(&batch->batch_lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(get_random_u64);\n\nstatic DEFINE_PER_CPU(struct batched_entropy, batched_entropy_u32) = {\n\t.batch_lock\t= __SPIN_LOCK_UNLOCKED(batched_entropy_u32.lock),\n};\nu32 get_random_u32(void)\n{\n\tu32 ret;\n\tunsigned long flags;\n\tstruct batched_entropy *batch;\n\tstatic void *previous;\n\n\twarn_unseeded_randomness(&previous);\n\n\tbatch = raw_cpu_ptr(&batched_entropy_u32);\n\tspin_lock_irqsave(&batch->batch_lock, flags);\n\tif (batch->position % ARRAY_SIZE(batch->entropy_u32) == 0) {\n\t\textract_crng((u8 *)batch->entropy_u32);\n\t\tbatch->position = 0;\n\t}\n\tret = batch->entropy_u32[batch->position++];\n\tspin_unlock_irqrestore(&batch->batch_lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(get_random_u32);\n\n/* It's important to invalidate all potential batched entropy that might\n * be stored before the crng is initialized, which we can do lazily by\n * simply resetting the counter to zero so that it's re-extracted on the\n * next usage. */\nstatic void invalidate_batched_entropy(void)\n{\n\tint cpu;\n\tunsigned long flags;\n\n\tfor_each_possible_cpu (cpu) {\n\t\tstruct batched_entropy *batched_entropy;\n\n\t\tbatched_entropy = per_cpu_ptr(&batched_entropy_u32, cpu);\n\t\tspin_lock_irqsave(&batched_entropy->batch_lock, flags);\n\t\tbatched_entropy->position = 0;\n\t\tspin_unlock(&batched_entropy->batch_lock);\n\n\t\tbatched_entropy = per_cpu_ptr(&batched_entropy_u64, cpu);\n\t\tspin_lock(&batched_entropy->batch_lock);\n\t\tbatched_entropy->position = 0;\n\t\tspin_unlock_irqrestore(&batched_entropy->batch_lock, flags);\n\t}\n}\n\n/**\n * randomize_page - Generate a random, page aligned address\n * @start:\tThe smallest acceptable address the caller will take.\n * @range:\tThe size of the area, starting at @start, within which the\n *\t\trandom address must fall.\n *\n * If @start + @range would overflow, @range is capped.\n *\n * NOTE: Historical use of randomize_range, which this replaces, presumed that\n * @start was already page aligned.  We now align it regardless.\n *\n * Return: A page aligned address within [start, start + range).  On error,\n * @start is returned.\n */\nunsigned long\nrandomize_page(unsigned long start, unsigned long range)\n{\n\tif (!PAGE_ALIGNED(start)) {\n\t\trange -= PAGE_ALIGN(start) - start;\n\t\tstart = PAGE_ALIGN(start);\n\t}\n\n\tif (start > ULONG_MAX - range)\n\t\trange = ULONG_MAX - start;\n\n\trange >>= PAGE_SHIFT;\n\n\tif (range == 0)\n\t\treturn start;\n\n\treturn start + (get_random_long() % range << PAGE_SHIFT);\n}\n\n/* Interface for in-kernel drivers of true hardware RNGs.\n * Those devices may produce endless random bits and will be throttled\n * when our pool is full.\n */\nvoid add_hwgenerator_randomness(const char *buffer, size_t count,\n\t\t\t\tsize_t entropy)\n{\n\tstruct entropy_store *poolp = &input_pool;\n\n\tif (unlikely(crng_init == 0)) {\n\t\tcrng_fast_load(buffer, count);\n\t\treturn;\n\t}\n\n\t/* Suspend writing if we're above the trickle threshold.\n\t * We'll be woken up again once below random_write_wakeup_thresh,\n\t * or when the calling thread is about to terminate.\n\t */\n\twait_event_interruptible(random_write_wait, kthread_should_stop() ||\n\t\t\tENTROPY_BITS(&input_pool) <= random_write_wakeup_bits);\n\tmix_pool_bytes(poolp, buffer, count);\n\tcredit_entropy_bits(poolp, entropy);\n}\nEXPORT_SYMBOL_GPL(add_hwgenerator_randomness);\n\n/* Handle random seed passed by bootloader.\n * If the seed is trustworthy, it would be regarded as hardware RNGs. Otherwise\n * it would be regarded as device data.\n * The decision is controlled by CONFIG_RANDOM_TRUST_BOOTLOADER.\n */\nvoid add_bootloader_randomness(const void *buf, unsigned int size)\n{\n\tif (IS_ENABLED(CONFIG_RANDOM_TRUST_BOOTLOADER))\n\t\tadd_hwgenerator_randomness(buf, size, size * 8);\n\telse\n\t\tadd_device_randomness(buf, size);\n}\nEXPORT_SYMBOL_GPL(add_bootloader_randomness);\n", "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * include/linux/random.h\n *\n * Include file for the random number generator.\n */\n#ifndef _LINUX_RANDOM_H\n#define _LINUX_RANDOM_H\n\n#include <linux/bug.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/once.h>\n#include <linux/percpu.h>\n\n#include <uapi/linux/random.h>\n\nstruct random_ready_callback {\n\tstruct list_head list;\n\tvoid (*func)(struct random_ready_callback *rdy);\n\tstruct module *owner;\n};\n\nextern void add_device_randomness(const void *, unsigned int);\nextern void add_bootloader_randomness(const void *, unsigned int);\n\n#if defined(LATENT_ENTROPY_PLUGIN) && !defined(__CHECKER__)\nstatic inline void add_latent_entropy(void)\n{\n\tadd_device_randomness((const void *)&latent_entropy,\n\t\t\t      sizeof(latent_entropy));\n}\n#else\nstatic inline void add_latent_entropy(void) {}\n#endif\n\nextern void add_input_randomness(unsigned int type, unsigned int code,\n\t\t\t\t unsigned int value) __latent_entropy;\nextern void add_interrupt_randomness(int irq, int irq_flags) __latent_entropy;\n\nextern void get_random_bytes(void *buf, int nbytes);\nextern int wait_for_random_bytes(void);\nextern int __init rand_initialize(void);\nextern bool rng_is_initialized(void);\nextern int add_random_ready_callback(struct random_ready_callback *rdy);\nextern void del_random_ready_callback(struct random_ready_callback *rdy);\nextern int __must_check get_random_bytes_arch(void *buf, int nbytes);\n\n#ifndef MODULE\nextern const struct file_operations random_fops, urandom_fops;\n#endif\n\nu32 get_random_u32(void);\nu64 get_random_u64(void);\nstatic inline unsigned int get_random_int(void)\n{\n\treturn get_random_u32();\n}\nstatic inline unsigned long get_random_long(void)\n{\n#if BITS_PER_LONG == 64\n\treturn get_random_u64();\n#else\n\treturn get_random_u32();\n#endif\n}\n\n/*\n * On 64-bit architectures, protect against non-terminated C string overflows\n * by zeroing out the first byte of the canary; this leaves 56 bits of entropy.\n */\n#ifdef CONFIG_64BIT\n# ifdef __LITTLE_ENDIAN\n#  define CANARY_MASK 0xffffffffffffff00UL\n# else /* big endian, 64 bits: */\n#  define CANARY_MASK 0x00ffffffffffffffUL\n# endif\n#else /* 32 bits: */\n# define CANARY_MASK 0xffffffffUL\n#endif\n\nstatic inline unsigned long get_random_canary(void)\n{\n\tunsigned long val = get_random_long();\n\n\treturn val & CANARY_MASK;\n}\n\n/* Calls wait_for_random_bytes() and then calls get_random_bytes(buf, nbytes).\n * Returns the result of the call to wait_for_random_bytes. */\nstatic inline int get_random_bytes_wait(void *buf, int nbytes)\n{\n\tint ret = wait_for_random_bytes();\n\tget_random_bytes(buf, nbytes);\n\treturn ret;\n}\n\n#define declare_get_random_var_wait(var) \\\n\tstatic inline int get_random_ ## var ## _wait(var *out) { \\\n\t\tint ret = wait_for_random_bytes(); \\\n\t\tif (unlikely(ret)) \\\n\t\t\treturn ret; \\\n\t\t*out = get_random_ ## var(); \\\n\t\treturn 0; \\\n\t}\ndeclare_get_random_var_wait(u32)\ndeclare_get_random_var_wait(u64)\ndeclare_get_random_var_wait(int)\ndeclare_get_random_var_wait(long)\n#undef declare_get_random_var\n\nunsigned long randomize_page(unsigned long start, unsigned long range);\n\nu32 prandom_u32(void);\nvoid prandom_bytes(void *buf, size_t nbytes);\nvoid prandom_seed(u32 seed);\nvoid prandom_reseed_late(void);\n\nstruct rnd_state {\n\t__u32 s1, s2, s3, s4;\n};\n\nDECLARE_PER_CPU(struct rnd_state, net_rand_state) __latent_entropy;\n\nu32 prandom_u32_state(struct rnd_state *state);\nvoid prandom_bytes_state(struct rnd_state *state, void *buf, size_t nbytes);\nvoid prandom_seed_full_state(struct rnd_state __percpu *pcpu_state);\n\n#define prandom_init_once(pcpu_state)\t\t\t\\\n\tDO_ONCE(prandom_seed_full_state, (pcpu_state))\n\n/**\n * prandom_u32_max - returns a pseudo-random number in interval [0, ep_ro)\n * @ep_ro: right open interval endpoint\n *\n * Returns a pseudo-random number that is in interval [0, ep_ro). Note\n * that the result depends on PRNG being well distributed in [0, ~0U]\n * u32 space. Here we use maximally equidistributed combined Tausworthe\n * generator, that is, prandom_u32(). This is useful when requesting a\n * random index of an array containing ep_ro elements, for example.\n *\n * Returns: pseudo-random number in interval [0, ep_ro)\n */\nstatic inline u32 prandom_u32_max(u32 ep_ro)\n{\n\treturn (u32)(((u64) prandom_u32() * ep_ro) >> 32);\n}\n\n/*\n * Handle minimum values for seeds\n */\nstatic inline u32 __seed(u32 x, u32 m)\n{\n\treturn (x < m) ? x + m : x;\n}\n\n/**\n * prandom_seed_state - set seed for prandom_u32_state().\n * @state: pointer to state structure to receive the seed.\n * @seed: arbitrary 64-bit value to use as a seed.\n */\nstatic inline void prandom_seed_state(struct rnd_state *state, u64 seed)\n{\n\tu32 i = (seed >> 32) ^ (seed << 10) ^ seed;\n\n\tstate->s1 = __seed(i,   2U);\n\tstate->s2 = __seed(i,   8U);\n\tstate->s3 = __seed(i,  16U);\n\tstate->s4 = __seed(i, 128U);\n}\n\n#ifdef CONFIG_ARCH_RANDOM\n# include <asm/archrandom.h>\n#else\nstatic inline bool __must_check arch_get_random_long(unsigned long *v)\n{\n\treturn false;\n}\nstatic inline bool __must_check arch_get_random_int(unsigned int *v)\n{\n\treturn false;\n}\nstatic inline bool __must_check arch_get_random_seed_long(unsigned long *v)\n{\n\treturn false;\n}\nstatic inline bool __must_check arch_get_random_seed_int(unsigned int *v)\n{\n\treturn false;\n}\n#endif\n\n/*\n * Called from the boot CPU during startup; not valid to call once\n * secondary CPUs are up and preemption is possible.\n */\n#ifndef arch_get_random_seed_long_early\nstatic inline bool __init arch_get_random_seed_long_early(unsigned long *v)\n{\n\tWARN_ON(system_state != SYSTEM_BOOTING);\n\treturn arch_get_random_seed_long(v);\n}\n#endif\n\n#ifndef arch_get_random_long_early\nstatic inline bool __init arch_get_random_long_early(unsigned long *v)\n{\n\tWARN_ON(system_state != SYSTEM_BOOTING);\n\treturn arch_get_random_long(v);\n}\n#endif\n\n/* Pseudo random number generator from numerical recipes. */\nstatic inline u32 next_pseudo_random32(u32 seed)\n{\n\treturn seed * 1664525 + 1013904223;\n}\n\n#endif /* _LINUX_RANDOM_H */\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  Kernel internal timers\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  1997-01-28  Modified by Finn Arne Gangstad to make timers scale better.\n *\n *  1997-09-10  Updated NTP code according to technical memorandum Jan '96\n *              \"A Kernel Model for Precision Timekeeping\" by Dave Mills\n *  1998-12-24  Fixed a xtime SMP race (we need the xtime_lock rw spinlock to\n *              serialize accesses to xtime/lost_ticks).\n *                              Copyright (C) 1998  Andrea Arcangeli\n *  1999-03-10  Improved NTP compatibility by Ulrich Windl\n *  2002-05-31\tMove sys_sysinfo here and make its locking sane, Robert Love\n *  2000-10-05  Implemented scalable SMP per-CPU timer handling.\n *                              Copyright (C) 2000, 2001, 2002  Ingo Molnar\n *              Designed by David S. Miller, Alexey Kuznetsov and Ingo Molnar\n */\n\n#include <linux/kernel_stat.h>\n#include <linux/export.h>\n#include <linux/interrupt.h>\n#include <linux/percpu.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/pid_namespace.h>\n#include <linux/notifier.h>\n#include <linux/thread_info.h>\n#include <linux/time.h>\n#include <linux/jiffies.h>\n#include <linux/posix-timers.h>\n#include <linux/cpu.h>\n#include <linux/syscalls.h>\n#include <linux/delay.h>\n#include <linux/tick.h>\n#include <linux/kallsyms.h>\n#include <linux/irq_work.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/debug.h>\n#include <linux/slab.h>\n#include <linux/compat.h>\n#include <linux/random.h>\n\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/div64.h>\n#include <asm/timex.h>\n#include <asm/io.h>\n\n#include \"tick-internal.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/timer.h>\n\n__visible u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;\n\nEXPORT_SYMBOL(jiffies_64);\n\n/*\n * The timer wheel has LVL_DEPTH array levels. Each level provides an array of\n * LVL_SIZE buckets. Each level is driven by its own clock and therefor each\n * level has a different granularity.\n *\n * The level granularity is:\t\tLVL_CLK_DIV ^ lvl\n * The level clock frequency is:\tHZ / (LVL_CLK_DIV ^ level)\n *\n * The array level of a newly armed timer depends on the relative expiry\n * time. The farther the expiry time is away the higher the array level and\n * therefor the granularity becomes.\n *\n * Contrary to the original timer wheel implementation, which aims for 'exact'\n * expiry of the timers, this implementation removes the need for recascading\n * the timers into the lower array levels. The previous 'classic' timer wheel\n * implementation of the kernel already violated the 'exact' expiry by adding\n * slack to the expiry time to provide batched expiration. The granularity\n * levels provide implicit batching.\n *\n * This is an optimization of the original timer wheel implementation for the\n * majority of the timer wheel use cases: timeouts. The vast majority of\n * timeout timers (networking, disk I/O ...) are canceled before expiry. If\n * the timeout expires it indicates that normal operation is disturbed, so it\n * does not matter much whether the timeout comes with a slight delay.\n *\n * The only exception to this are networking timers with a small expiry\n * time. They rely on the granularity. Those fit into the first wheel level,\n * which has HZ granularity.\n *\n * We don't have cascading anymore. timers with a expiry time above the\n * capacity of the last wheel level are force expired at the maximum timeout\n * value of the last wheel level. From data sampling we know that the maximum\n * value observed is 5 days (network connection tracking), so this should not\n * be an issue.\n *\n * The currently chosen array constants values are a good compromise between\n * array size and granularity.\n *\n * This results in the following granularity and range levels:\n *\n * HZ 1000 steps\n * Level Offset  Granularity            Range\n *  0      0         1 ms                0 ms -         63 ms\n *  1     64         8 ms               64 ms -        511 ms\n *  2    128        64 ms              512 ms -       4095 ms (512ms - ~4s)\n *  3    192       512 ms             4096 ms -      32767 ms (~4s - ~32s)\n *  4    256      4096 ms (~4s)      32768 ms -     262143 ms (~32s - ~4m)\n *  5    320     32768 ms (~32s)    262144 ms -    2097151 ms (~4m - ~34m)\n *  6    384    262144 ms (~4m)    2097152 ms -   16777215 ms (~34m - ~4h)\n *  7    448   2097152 ms (~34m)  16777216 ms -  134217727 ms (~4h - ~1d)\n *  8    512  16777216 ms (~4h)  134217728 ms - 1073741822 ms (~1d - ~12d)\n *\n * HZ  300\n * Level Offset  Granularity            Range\n *  0\t   0         3 ms                0 ms -        210 ms\n *  1\t  64        26 ms              213 ms -       1703 ms (213ms - ~1s)\n *  2\t 128       213 ms             1706 ms -      13650 ms (~1s - ~13s)\n *  3\t 192      1706 ms (~1s)      13653 ms -     109223 ms (~13s - ~1m)\n *  4\t 256     13653 ms (~13s)    109226 ms -     873810 ms (~1m - ~14m)\n *  5\t 320    109226 ms (~1m)     873813 ms -    6990503 ms (~14m - ~1h)\n *  6\t 384    873813 ms (~14m)   6990506 ms -   55924050 ms (~1h - ~15h)\n *  7\t 448   6990506 ms (~1h)   55924053 ms -  447392423 ms (~15h - ~5d)\n *  8    512  55924053 ms (~15h) 447392426 ms - 3579139406 ms (~5d - ~41d)\n *\n * HZ  250\n * Level Offset  Granularity            Range\n *  0\t   0         4 ms                0 ms -        255 ms\n *  1\t  64        32 ms              256 ms -       2047 ms (256ms - ~2s)\n *  2\t 128       256 ms             2048 ms -      16383 ms (~2s - ~16s)\n *  3\t 192      2048 ms (~2s)      16384 ms -     131071 ms (~16s - ~2m)\n *  4\t 256     16384 ms (~16s)    131072 ms -    1048575 ms (~2m - ~17m)\n *  5\t 320    131072 ms (~2m)    1048576 ms -    8388607 ms (~17m - ~2h)\n *  6\t 384   1048576 ms (~17m)   8388608 ms -   67108863 ms (~2h - ~18h)\n *  7\t 448   8388608 ms (~2h)   67108864 ms -  536870911 ms (~18h - ~6d)\n *  8    512  67108864 ms (~18h) 536870912 ms - 4294967288 ms (~6d - ~49d)\n *\n * HZ  100\n * Level Offset  Granularity            Range\n *  0\t   0         10 ms               0 ms -        630 ms\n *  1\t  64         80 ms             640 ms -       5110 ms (640ms - ~5s)\n *  2\t 128        640 ms            5120 ms -      40950 ms (~5s - ~40s)\n *  3\t 192       5120 ms (~5s)     40960 ms -     327670 ms (~40s - ~5m)\n *  4\t 256      40960 ms (~40s)   327680 ms -    2621430 ms (~5m - ~43m)\n *  5\t 320     327680 ms (~5m)   2621440 ms -   20971510 ms (~43m - ~5h)\n *  6\t 384    2621440 ms (~43m) 20971520 ms -  167772150 ms (~5h - ~1d)\n *  7\t 448   20971520 ms (~5h) 167772160 ms - 1342177270 ms (~1d - ~15d)\n */\n\n/* Clock divisor for the next level */\n#define LVL_CLK_SHIFT\t3\n#define LVL_CLK_DIV\t(1UL << LVL_CLK_SHIFT)\n#define LVL_CLK_MASK\t(LVL_CLK_DIV - 1)\n#define LVL_SHIFT(n)\t((n) * LVL_CLK_SHIFT)\n#define LVL_GRAN(n)\t(1UL << LVL_SHIFT(n))\n\n/*\n * The time start value for each level to select the bucket at enqueue\n * time.\n */\n#define LVL_START(n)\t((LVL_SIZE - 1) << (((n) - 1) * LVL_CLK_SHIFT))\n\n/* Size of each clock level */\n#define LVL_BITS\t6\n#define LVL_SIZE\t(1UL << LVL_BITS)\n#define LVL_MASK\t(LVL_SIZE - 1)\n#define LVL_OFFS(n)\t((n) * LVL_SIZE)\n\n/* Level depth */\n#if HZ > 100\n# define LVL_DEPTH\t9\n# else\n# define LVL_DEPTH\t8\n#endif\n\n/* The cutoff (max. capacity of the wheel) */\n#define WHEEL_TIMEOUT_CUTOFF\t(LVL_START(LVL_DEPTH))\n#define WHEEL_TIMEOUT_MAX\t(WHEEL_TIMEOUT_CUTOFF - LVL_GRAN(LVL_DEPTH - 1))\n\n/*\n * The resulting wheel size. If NOHZ is configured we allocate two\n * wheels so we have a separate storage for the deferrable timers.\n */\n#define WHEEL_SIZE\t(LVL_SIZE * LVL_DEPTH)\n\n#ifdef CONFIG_NO_HZ_COMMON\n# define NR_BASES\t2\n# define BASE_STD\t0\n# define BASE_DEF\t1\n#else\n# define NR_BASES\t1\n# define BASE_STD\t0\n# define BASE_DEF\t0\n#endif\n\nstruct timer_base {\n\traw_spinlock_t\t\tlock;\n\tstruct timer_list\t*running_timer;\n#ifdef CONFIG_PREEMPT_RT\n\tspinlock_t\t\texpiry_lock;\n\tatomic_t\t\ttimer_waiters;\n#endif\n\tunsigned long\t\tclk;\n\tunsigned long\t\tnext_expiry;\n\tunsigned int\t\tcpu;\n\tbool\t\t\tis_idle;\n\tbool\t\t\tmust_forward_clk;\n\tDECLARE_BITMAP(pending_map, WHEEL_SIZE);\n\tstruct hlist_head\tvectors[WHEEL_SIZE];\n} ____cacheline_aligned;\n\nstatic DEFINE_PER_CPU(struct timer_base, timer_bases[NR_BASES]);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\nstatic DEFINE_STATIC_KEY_FALSE(timers_nohz_active);\nstatic DEFINE_MUTEX(timer_keys_mutex);\n\nstatic void timer_update_keys(struct work_struct *work);\nstatic DECLARE_WORK(timer_update_work, timer_update_keys);\n\n#ifdef CONFIG_SMP\nunsigned int sysctl_timer_migration = 1;\n\nDEFINE_STATIC_KEY_FALSE(timers_migration_enabled);\n\nstatic void timers_update_migration(void)\n{\n\tif (sysctl_timer_migration && tick_nohz_active)\n\t\tstatic_branch_enable(&timers_migration_enabled);\n\telse\n\t\tstatic_branch_disable(&timers_migration_enabled);\n}\n#else\nstatic inline void timers_update_migration(void) { }\n#endif /* !CONFIG_SMP */\n\nstatic void timer_update_keys(struct work_struct *work)\n{\n\tmutex_lock(&timer_keys_mutex);\n\ttimers_update_migration();\n\tstatic_branch_enable(&timers_nohz_active);\n\tmutex_unlock(&timer_keys_mutex);\n}\n\nvoid timers_update_nohz(void)\n{\n\tschedule_work(&timer_update_work);\n}\n\nint timer_migration_handler(struct ctl_table *table, int write,\n\t\t\t    void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\n\tmutex_lock(&timer_keys_mutex);\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (!ret && write)\n\t\ttimers_update_migration();\n\tmutex_unlock(&timer_keys_mutex);\n\treturn ret;\n}\n\nstatic inline bool is_timers_nohz_active(void)\n{\n\treturn static_branch_unlikely(&timers_nohz_active);\n}\n#else\nstatic inline bool is_timers_nohz_active(void) { return false; }\n#endif /* NO_HZ_COMMON */\n\nstatic unsigned long round_jiffies_common(unsigned long j, int cpu,\n\t\tbool force_up)\n{\n\tint rem;\n\tunsigned long original = j;\n\n\t/*\n\t * We don't want all cpus firing their timers at once hitting the\n\t * same lock or cachelines, so we skew each extra cpu with an extra\n\t * 3 jiffies. This 3 jiffies came originally from the mm/ code which\n\t * already did this.\n\t * The skew is done by adding 3*cpunr, then round, then subtract this\n\t * extra offset again.\n\t */\n\tj += cpu * 3;\n\n\trem = j % HZ;\n\n\t/*\n\t * If the target jiffie is just after a whole second (which can happen\n\t * due to delays of the timer irq, long irq off times etc etc) then\n\t * we should round down to the whole second, not up. Use 1/4th second\n\t * as cutoff for this rounding as an extreme upper bound for this.\n\t * But never round down if @force_up is set.\n\t */\n\tif (rem < HZ/4 && !force_up) /* round down */\n\t\tj = j - rem;\n\telse /* round up */\n\t\tj = j - rem + HZ;\n\n\t/* now that we have rounded, subtract the extra skew again */\n\tj -= cpu * 3;\n\n\t/*\n\t * Make sure j is still in the future. Otherwise return the\n\t * unmodified value.\n\t */\n\treturn time_is_after_jiffies(j) ? j : original;\n}\n\n/**\n * __round_jiffies - function to round jiffies to a full second\n * @j: the time in (absolute) jiffies that should be rounded\n * @cpu: the processor number on which the timeout will happen\n *\n * __round_jiffies() rounds an absolute time in the future (in jiffies)\n * up or down to (approximately) full seconds. This is useful for timers\n * for which the exact time they fire does not matter too much, as long as\n * they fire approximately every X seconds.\n *\n * By rounding these timers to whole seconds, all such timers will fire\n * at the same time, rather than at various times spread out. The goal\n * of this is to have the CPU wake up less, which saves power.\n *\n * The exact rounding is skewed for each processor to avoid all\n * processors firing at the exact same time, which could lead\n * to lock contention or spurious cache line bouncing.\n *\n * The return value is the rounded version of the @j parameter.\n */\nunsigned long __round_jiffies(unsigned long j, int cpu)\n{\n\treturn round_jiffies_common(j, cpu, false);\n}\nEXPORT_SYMBOL_GPL(__round_jiffies);\n\n/**\n * __round_jiffies_relative - function to round jiffies to a full second\n * @j: the time in (relative) jiffies that should be rounded\n * @cpu: the processor number on which the timeout will happen\n *\n * __round_jiffies_relative() rounds a time delta  in the future (in jiffies)\n * up or down to (approximately) full seconds. This is useful for timers\n * for which the exact time they fire does not matter too much, as long as\n * they fire approximately every X seconds.\n *\n * By rounding these timers to whole seconds, all such timers will fire\n * at the same time, rather than at various times spread out. The goal\n * of this is to have the CPU wake up less, which saves power.\n *\n * The exact rounding is skewed for each processor to avoid all\n * processors firing at the exact same time, which could lead\n * to lock contention or spurious cache line bouncing.\n *\n * The return value is the rounded version of the @j parameter.\n */\nunsigned long __round_jiffies_relative(unsigned long j, int cpu)\n{\n\tunsigned long j0 = jiffies;\n\n\t/* Use j0 because jiffies might change while we run */\n\treturn round_jiffies_common(j + j0, cpu, false) - j0;\n}\nEXPORT_SYMBOL_GPL(__round_jiffies_relative);\n\n/**\n * round_jiffies - function to round jiffies to a full second\n * @j: the time in (absolute) jiffies that should be rounded\n *\n * round_jiffies() rounds an absolute time in the future (in jiffies)\n * up or down to (approximately) full seconds. This is useful for timers\n * for which the exact time they fire does not matter too much, as long as\n * they fire approximately every X seconds.\n *\n * By rounding these timers to whole seconds, all such timers will fire\n * at the same time, rather than at various times spread out. The goal\n * of this is to have the CPU wake up less, which saves power.\n *\n * The return value is the rounded version of the @j parameter.\n */\nunsigned long round_jiffies(unsigned long j)\n{\n\treturn round_jiffies_common(j, raw_smp_processor_id(), false);\n}\nEXPORT_SYMBOL_GPL(round_jiffies);\n\n/**\n * round_jiffies_relative - function to round jiffies to a full second\n * @j: the time in (relative) jiffies that should be rounded\n *\n * round_jiffies_relative() rounds a time delta  in the future (in jiffies)\n * up or down to (approximately) full seconds. This is useful for timers\n * for which the exact time they fire does not matter too much, as long as\n * they fire approximately every X seconds.\n *\n * By rounding these timers to whole seconds, all such timers will fire\n * at the same time, rather than at various times spread out. The goal\n * of this is to have the CPU wake up less, which saves power.\n *\n * The return value is the rounded version of the @j parameter.\n */\nunsigned long round_jiffies_relative(unsigned long j)\n{\n\treturn __round_jiffies_relative(j, raw_smp_processor_id());\n}\nEXPORT_SYMBOL_GPL(round_jiffies_relative);\n\n/**\n * __round_jiffies_up - function to round jiffies up to a full second\n * @j: the time in (absolute) jiffies that should be rounded\n * @cpu: the processor number on which the timeout will happen\n *\n * This is the same as __round_jiffies() except that it will never\n * round down.  This is useful for timeouts for which the exact time\n * of firing does not matter too much, as long as they don't fire too\n * early.\n */\nunsigned long __round_jiffies_up(unsigned long j, int cpu)\n{\n\treturn round_jiffies_common(j, cpu, true);\n}\nEXPORT_SYMBOL_GPL(__round_jiffies_up);\n\n/**\n * __round_jiffies_up_relative - function to round jiffies up to a full second\n * @j: the time in (relative) jiffies that should be rounded\n * @cpu: the processor number on which the timeout will happen\n *\n * This is the same as __round_jiffies_relative() except that it will never\n * round down.  This is useful for timeouts for which the exact time\n * of firing does not matter too much, as long as they don't fire too\n * early.\n */\nunsigned long __round_jiffies_up_relative(unsigned long j, int cpu)\n{\n\tunsigned long j0 = jiffies;\n\n\t/* Use j0 because jiffies might change while we run */\n\treturn round_jiffies_common(j + j0, cpu, true) - j0;\n}\nEXPORT_SYMBOL_GPL(__round_jiffies_up_relative);\n\n/**\n * round_jiffies_up - function to round jiffies up to a full second\n * @j: the time in (absolute) jiffies that should be rounded\n *\n * This is the same as round_jiffies() except that it will never\n * round down.  This is useful for timeouts for which the exact time\n * of firing does not matter too much, as long as they don't fire too\n * early.\n */\nunsigned long round_jiffies_up(unsigned long j)\n{\n\treturn round_jiffies_common(j, raw_smp_processor_id(), true);\n}\nEXPORT_SYMBOL_GPL(round_jiffies_up);\n\n/**\n * round_jiffies_up_relative - function to round jiffies up to a full second\n * @j: the time in (relative) jiffies that should be rounded\n *\n * This is the same as round_jiffies_relative() except that it will never\n * round down.  This is useful for timeouts for which the exact time\n * of firing does not matter too much, as long as they don't fire too\n * early.\n */\nunsigned long round_jiffies_up_relative(unsigned long j)\n{\n\treturn __round_jiffies_up_relative(j, raw_smp_processor_id());\n}\nEXPORT_SYMBOL_GPL(round_jiffies_up_relative);\n\n\nstatic inline unsigned int timer_get_idx(struct timer_list *timer)\n{\n\treturn (timer->flags & TIMER_ARRAYMASK) >> TIMER_ARRAYSHIFT;\n}\n\nstatic inline void timer_set_idx(struct timer_list *timer, unsigned int idx)\n{\n\ttimer->flags = (timer->flags & ~TIMER_ARRAYMASK) |\n\t\t\tidx << TIMER_ARRAYSHIFT;\n}\n\n/*\n * Helper function to calculate the array index for a given expiry\n * time.\n */\nstatic inline unsigned calc_index(unsigned expires, unsigned lvl)\n{\n\texpires = (expires + LVL_GRAN(lvl)) >> LVL_SHIFT(lvl);\n\treturn LVL_OFFS(lvl) + (expires & LVL_MASK);\n}\n\nstatic int calc_wheel_index(unsigned long expires, unsigned long clk)\n{\n\tunsigned long delta = expires - clk;\n\tunsigned int idx;\n\n\tif (delta < LVL_START(1)) {\n\t\tidx = calc_index(expires, 0);\n\t} else if (delta < LVL_START(2)) {\n\t\tidx = calc_index(expires, 1);\n\t} else if (delta < LVL_START(3)) {\n\t\tidx = calc_index(expires, 2);\n\t} else if (delta < LVL_START(4)) {\n\t\tidx = calc_index(expires, 3);\n\t} else if (delta < LVL_START(5)) {\n\t\tidx = calc_index(expires, 4);\n\t} else if (delta < LVL_START(6)) {\n\t\tidx = calc_index(expires, 5);\n\t} else if (delta < LVL_START(7)) {\n\t\tidx = calc_index(expires, 6);\n\t} else if (LVL_DEPTH > 8 && delta < LVL_START(8)) {\n\t\tidx = calc_index(expires, 7);\n\t} else if ((long) delta < 0) {\n\t\tidx = clk & LVL_MASK;\n\t} else {\n\t\t/*\n\t\t * Force expire obscene large timeouts to expire at the\n\t\t * capacity limit of the wheel.\n\t\t */\n\t\tif (delta >= WHEEL_TIMEOUT_CUTOFF)\n\t\t\texpires = clk + WHEEL_TIMEOUT_MAX;\n\n\t\tidx = calc_index(expires, LVL_DEPTH - 1);\n\t}\n\treturn idx;\n}\n\n/*\n * Enqueue the timer into the hash bucket, mark it pending in\n * the bitmap and store the index in the timer flags.\n */\nstatic void enqueue_timer(struct timer_base *base, struct timer_list *timer,\n\t\t\t  unsigned int idx)\n{\n\thlist_add_head(&timer->entry, base->vectors + idx);\n\t__set_bit(idx, base->pending_map);\n\ttimer_set_idx(timer, idx);\n\n\ttrace_timer_start(timer, timer->expires, timer->flags);\n}\n\nstatic void\n__internal_add_timer(struct timer_base *base, struct timer_list *timer)\n{\n\tunsigned int idx;\n\n\tidx = calc_wheel_index(timer->expires, base->clk);\n\tenqueue_timer(base, timer, idx);\n}\n\nstatic void\ntrigger_dyntick_cpu(struct timer_base *base, struct timer_list *timer)\n{\n\tif (!is_timers_nohz_active())\n\t\treturn;\n\n\t/*\n\t * TODO: This wants some optimizing similar to the code below, but we\n\t * will do that when we switch from push to pull for deferrable timers.\n\t */\n\tif (timer->flags & TIMER_DEFERRABLE) {\n\t\tif (tick_nohz_full_cpu(base->cpu))\n\t\t\twake_up_nohz_cpu(base->cpu);\n\t\treturn;\n\t}\n\n\t/*\n\t * We might have to IPI the remote CPU if the base is idle and the\n\t * timer is not deferrable. If the other CPU is on the way to idle\n\t * then it can't set base->is_idle as we hold the base lock:\n\t */\n\tif (!base->is_idle)\n\t\treturn;\n\n\t/* Check whether this is the new first expiring timer: */\n\tif (time_after_eq(timer->expires, base->next_expiry))\n\t\treturn;\n\n\t/*\n\t * Set the next expiry time and kick the CPU so it can reevaluate the\n\t * wheel:\n\t */\n\tif (time_before(timer->expires, base->clk)) {\n\t\t/*\n\t\t * Prevent from forward_timer_base() moving the base->clk\n\t\t * backward\n\t\t */\n\t\tbase->next_expiry = base->clk;\n\t} else {\n\t\tbase->next_expiry = timer->expires;\n\t}\n\twake_up_nohz_cpu(base->cpu);\n}\n\nstatic void\ninternal_add_timer(struct timer_base *base, struct timer_list *timer)\n{\n\t__internal_add_timer(base, timer);\n\ttrigger_dyntick_cpu(base, timer);\n}\n\n#ifdef CONFIG_DEBUG_OBJECTS_TIMERS\n\nstatic struct debug_obj_descr timer_debug_descr;\n\nstatic void *timer_debug_hint(void *addr)\n{\n\treturn ((struct timer_list *) addr)->function;\n}\n\nstatic bool timer_is_static_object(void *addr)\n{\n\tstruct timer_list *timer = addr;\n\n\treturn (timer->entry.pprev == NULL &&\n\t\ttimer->entry.next == TIMER_ENTRY_STATIC);\n}\n\n/*\n * fixup_init is called when:\n * - an active object is initialized\n */\nstatic bool timer_fixup_init(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tdel_timer_sync(timer);\n\t\tdebug_object_init(timer, &timer_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Stub timer callback for improperly used timers. */\nstatic void stub_timer(struct timer_list *unused)\n{\n\tWARN_ON(1);\n}\n\n/*\n * fixup_activate is called when:\n * - an active object is activated\n * - an unknown non-static object is activated\n */\nstatic bool timer_fixup_activate(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_NOTAVAILABLE:\n\t\ttimer_setup(timer, stub_timer, 0);\n\t\treturn true;\n\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tWARN_ON(1);\n\t\t/* fall through */\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/*\n * fixup_free is called when:\n * - an active object is freed\n */\nstatic bool timer_fixup_free(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tdel_timer_sync(timer);\n\t\tdebug_object_free(timer, &timer_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/*\n * fixup_assert_init is called when:\n * - an untracked/uninit-ed object is found\n */\nstatic bool timer_fixup_assert_init(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_NOTAVAILABLE:\n\t\ttimer_setup(timer, stub_timer, 0);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic struct debug_obj_descr timer_debug_descr = {\n\t.name\t\t\t= \"timer_list\",\n\t.debug_hint\t\t= timer_debug_hint,\n\t.is_static_object\t= timer_is_static_object,\n\t.fixup_init\t\t= timer_fixup_init,\n\t.fixup_activate\t\t= timer_fixup_activate,\n\t.fixup_free\t\t= timer_fixup_free,\n\t.fixup_assert_init\t= timer_fixup_assert_init,\n};\n\nstatic inline void debug_timer_init(struct timer_list *timer)\n{\n\tdebug_object_init(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_activate(struct timer_list *timer)\n{\n\tdebug_object_activate(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_deactivate(struct timer_list *timer)\n{\n\tdebug_object_deactivate(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_free(struct timer_list *timer)\n{\n\tdebug_object_free(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_assert_init(struct timer_list *timer)\n{\n\tdebug_object_assert_init(timer, &timer_debug_descr);\n}\n\nstatic void do_init_timer(struct timer_list *timer,\n\t\t\t  void (*func)(struct timer_list *),\n\t\t\t  unsigned int flags,\n\t\t\t  const char *name, struct lock_class_key *key);\n\nvoid init_timer_on_stack_key(struct timer_list *timer,\n\t\t\t     void (*func)(struct timer_list *),\n\t\t\t     unsigned int flags,\n\t\t\t     const char *name, struct lock_class_key *key)\n{\n\tdebug_object_init_on_stack(timer, &timer_debug_descr);\n\tdo_init_timer(timer, func, flags, name, key);\n}\nEXPORT_SYMBOL_GPL(init_timer_on_stack_key);\n\nvoid destroy_timer_on_stack(struct timer_list *timer)\n{\n\tdebug_object_free(timer, &timer_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_timer_on_stack);\n\n#else\nstatic inline void debug_timer_init(struct timer_list *timer) { }\nstatic inline void debug_timer_activate(struct timer_list *timer) { }\nstatic inline void debug_timer_deactivate(struct timer_list *timer) { }\nstatic inline void debug_timer_assert_init(struct timer_list *timer) { }\n#endif\n\nstatic inline void debug_init(struct timer_list *timer)\n{\n\tdebug_timer_init(timer);\n\ttrace_timer_init(timer);\n}\n\nstatic inline void debug_deactivate(struct timer_list *timer)\n{\n\tdebug_timer_deactivate(timer);\n\ttrace_timer_cancel(timer);\n}\n\nstatic inline void debug_assert_init(struct timer_list *timer)\n{\n\tdebug_timer_assert_init(timer);\n}\n\nstatic void do_init_timer(struct timer_list *timer,\n\t\t\t  void (*func)(struct timer_list *),\n\t\t\t  unsigned int flags,\n\t\t\t  const char *name, struct lock_class_key *key)\n{\n\ttimer->entry.pprev = NULL;\n\ttimer->function = func;\n\ttimer->flags = flags | raw_smp_processor_id();\n\tlockdep_init_map(&timer->lockdep_map, name, key, 0);\n}\n\n/**\n * init_timer_key - initialize a timer\n * @timer: the timer to be initialized\n * @func: timer callback function\n * @flags: timer flags\n * @name: name of the timer\n * @key: lockdep class key of the fake lock used for tracking timer\n *       sync lock dependencies\n *\n * init_timer_key() must be done to a timer prior calling *any* of the\n * other timer functions.\n */\nvoid init_timer_key(struct timer_list *timer,\n\t\t    void (*func)(struct timer_list *), unsigned int flags,\n\t\t    const char *name, struct lock_class_key *key)\n{\n\tdebug_init(timer);\n\tdo_init_timer(timer, func, flags, name, key);\n}\nEXPORT_SYMBOL(init_timer_key);\n\nstatic inline void detach_timer(struct timer_list *timer, bool clear_pending)\n{\n\tstruct hlist_node *entry = &timer->entry;\n\n\tdebug_deactivate(timer);\n\n\t__hlist_del(entry);\n\tif (clear_pending)\n\t\tentry->pprev = NULL;\n\tentry->next = LIST_POISON2;\n}\n\nstatic int detach_if_pending(struct timer_list *timer, struct timer_base *base,\n\t\t\t     bool clear_pending)\n{\n\tunsigned idx = timer_get_idx(timer);\n\n\tif (!timer_pending(timer))\n\t\treturn 0;\n\n\tif (hlist_is_singular_node(&timer->entry, base->vectors + idx))\n\t\t__clear_bit(idx, base->pending_map);\n\n\tdetach_timer(timer, clear_pending);\n\treturn 1;\n}\n\nstatic inline struct timer_base *get_timer_cpu_base(u32 tflags, u32 cpu)\n{\n\tstruct timer_base *base = per_cpu_ptr(&timer_bases[BASE_STD], cpu);\n\n\t/*\n\t * If the timer is deferrable and NO_HZ_COMMON is set then we need\n\t * to use the deferrable base.\n\t */\n\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON) && (tflags & TIMER_DEFERRABLE))\n\t\tbase = per_cpu_ptr(&timer_bases[BASE_DEF], cpu);\n\treturn base;\n}\n\nstatic inline struct timer_base *get_timer_this_cpu_base(u32 tflags)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\t/*\n\t * If the timer is deferrable and NO_HZ_COMMON is set then we need\n\t * to use the deferrable base.\n\t */\n\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON) && (tflags & TIMER_DEFERRABLE))\n\t\tbase = this_cpu_ptr(&timer_bases[BASE_DEF]);\n\treturn base;\n}\n\nstatic inline struct timer_base *get_timer_base(u32 tflags)\n{\n\treturn get_timer_cpu_base(tflags, tflags & TIMER_CPUMASK);\n}\n\nstatic inline struct timer_base *\nget_target_base(struct timer_base *base, unsigned tflags)\n{\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\n\tif (static_branch_likely(&timers_migration_enabled) &&\n\t    !(tflags & TIMER_PINNED))\n\t\treturn get_timer_cpu_base(tflags, get_nohz_timer_target());\n#endif\n\treturn get_timer_this_cpu_base(tflags);\n}\n\nstatic inline void forward_timer_base(struct timer_base *base)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tunsigned long jnow;\n\n\t/*\n\t * We only forward the base when we are idle or have just come out of\n\t * idle (must_forward_clk logic), and have a delta between base clock\n\t * and jiffies. In the common case, run_timers will take care of it.\n\t */\n\tif (likely(!base->must_forward_clk))\n\t\treturn;\n\n\tjnow = READ_ONCE(jiffies);\n\tbase->must_forward_clk = base->is_idle;\n\tif ((long)(jnow - base->clk) < 2)\n\t\treturn;\n\n\t/*\n\t * If the next expiry value is > jiffies, then we fast forward to\n\t * jiffies otherwise we forward to the next expiry value.\n\t */\n\tif (time_after(base->next_expiry, jnow)) {\n\t\tbase->clk = jnow;\n\t} else {\n\t\tif (WARN_ON_ONCE(time_before(base->next_expiry, base->clk)))\n\t\t\treturn;\n\t\tbase->clk = base->next_expiry;\n\t}\n#endif\n}\n\n\n/*\n * We are using hashed locking: Holding per_cpu(timer_bases[x]).lock means\n * that all timers which are tied to this base are locked, and the base itself\n * is locked too.\n *\n * So __run_timers/migrate_timers can safely modify all timers which could\n * be found in the base->vectors array.\n *\n * When a timer is migrating then the TIMER_MIGRATING flag is set and we need\n * to wait until the migration is done.\n */\nstatic struct timer_base *lock_timer_base(struct timer_list *timer,\n\t\t\t\t\t  unsigned long *flags)\n\t__acquires(timer->base->lock)\n{\n\tfor (;;) {\n\t\tstruct timer_base *base;\n\t\tu32 tf;\n\n\t\t/*\n\t\t * We need to use READ_ONCE() here, otherwise the compiler\n\t\t * might re-read @tf between the check for TIMER_MIGRATING\n\t\t * and spin_lock().\n\t\t */\n\t\ttf = READ_ONCE(timer->flags);\n\n\t\tif (!(tf & TIMER_MIGRATING)) {\n\t\t\tbase = get_timer_base(tf);\n\t\t\traw_spin_lock_irqsave(&base->lock, *flags);\n\t\t\tif (timer->flags == tf)\n\t\t\t\treturn base;\n\t\t\traw_spin_unlock_irqrestore(&base->lock, *flags);\n\t\t}\n\t\tcpu_relax();\n\t}\n}\n\n#define MOD_TIMER_PENDING_ONLY\t\t0x01\n#define MOD_TIMER_REDUCE\t\t0x02\n#define MOD_TIMER_NOTPENDING\t\t0x04\n\nstatic inline int\n__mod_timer(struct timer_list *timer, unsigned long expires, unsigned int options)\n{\n\tstruct timer_base *base, *new_base;\n\tunsigned int idx = UINT_MAX;\n\tunsigned long clk = 0, flags;\n\tint ret = 0;\n\n\tBUG_ON(!timer->function);\n\n\t/*\n\t * This is a common optimization triggered by the networking code - if\n\t * the timer is re-modified to have the same timeout or ends up in the\n\t * same array bucket then just return:\n\t */\n\tif (!(options & MOD_TIMER_NOTPENDING) && timer_pending(timer)) {\n\t\t/*\n\t\t * The downside of this optimization is that it can result in\n\t\t * larger granularity than you would get from adding a new\n\t\t * timer with this expiry.\n\t\t */\n\t\tlong diff = timer->expires - expires;\n\n\t\tif (!diff)\n\t\t\treturn 1;\n\t\tif (options & MOD_TIMER_REDUCE && diff <= 0)\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * We lock timer base and calculate the bucket index right\n\t\t * here. If the timer ends up in the same bucket, then we\n\t\t * just update the expiry time and avoid the whole\n\t\t * dequeue/enqueue dance.\n\t\t */\n\t\tbase = lock_timer_base(timer, &flags);\n\t\tforward_timer_base(base);\n\n\t\tif (timer_pending(timer) && (options & MOD_TIMER_REDUCE) &&\n\t\t    time_before_eq(timer->expires, expires)) {\n\t\t\tret = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tclk = base->clk;\n\t\tidx = calc_wheel_index(expires, clk);\n\n\t\t/*\n\t\t * Retrieve and compare the array index of the pending\n\t\t * timer. If it matches set the expiry to the new value so a\n\t\t * subsequent call will exit in the expires check above.\n\t\t */\n\t\tif (idx == timer_get_idx(timer)) {\n\t\t\tif (!(options & MOD_TIMER_REDUCE))\n\t\t\t\ttimer->expires = expires;\n\t\t\telse if (time_after(timer->expires, expires))\n\t\t\t\ttimer->expires = expires;\n\t\t\tret = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else {\n\t\tbase = lock_timer_base(timer, &flags);\n\t\tforward_timer_base(base);\n\t}\n\n\tret = detach_if_pending(timer, base, false);\n\tif (!ret && (options & MOD_TIMER_PENDING_ONLY))\n\t\tgoto out_unlock;\n\n\tnew_base = get_target_base(base, timer->flags);\n\n\tif (base != new_base) {\n\t\t/*\n\t\t * We are trying to schedule the timer on the new base.\n\t\t * However we can't change timer's base while it is running,\n\t\t * otherwise del_timer_sync() can't detect that the timer's\n\t\t * handler yet has not finished. This also guarantees that the\n\t\t * timer is serialized wrt itself.\n\t\t */\n\t\tif (likely(base->running_timer != timer)) {\n\t\t\t/* See the comment in lock_timer_base() */\n\t\t\ttimer->flags |= TIMER_MIGRATING;\n\n\t\t\traw_spin_unlock(&base->lock);\n\t\t\tbase = new_base;\n\t\t\traw_spin_lock(&base->lock);\n\t\t\tWRITE_ONCE(timer->flags,\n\t\t\t\t   (timer->flags & ~TIMER_BASEMASK) | base->cpu);\n\t\t\tforward_timer_base(base);\n\t\t}\n\t}\n\n\tdebug_timer_activate(timer);\n\n\ttimer->expires = expires;\n\t/*\n\t * If 'idx' was calculated above and the base time did not advance\n\t * between calculating 'idx' and possibly switching the base, only\n\t * enqueue_timer() and trigger_dyntick_cpu() is required. Otherwise\n\t * we need to (re)calculate the wheel index via\n\t * internal_add_timer().\n\t */\n\tif (idx != UINT_MAX && clk == base->clk) {\n\t\tenqueue_timer(base, timer, idx);\n\t\ttrigger_dyntick_cpu(base, timer);\n\t} else {\n\t\tinternal_add_timer(base, timer);\n\t}\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&base->lock, flags);\n\n\treturn ret;\n}\n\n/**\n * mod_timer_pending - modify a pending timer's timeout\n * @timer: the pending timer to be modified\n * @expires: new timeout in jiffies\n *\n * mod_timer_pending() is the same for pending timers as mod_timer(),\n * but will not re-activate and modify already deleted timers.\n *\n * It is useful for unserialized use of timers.\n */\nint mod_timer_pending(struct timer_list *timer, unsigned long expires)\n{\n\treturn __mod_timer(timer, expires, MOD_TIMER_PENDING_ONLY);\n}\nEXPORT_SYMBOL(mod_timer_pending);\n\n/**\n * mod_timer - modify a timer's timeout\n * @timer: the timer to be modified\n * @expires: new timeout in jiffies\n *\n * mod_timer() is a more efficient way to update the expire field of an\n * active timer (if the timer is inactive it will be activated)\n *\n * mod_timer(timer, expires) is equivalent to:\n *\n *     del_timer(timer); timer->expires = expires; add_timer(timer);\n *\n * Note that if there are multiple unserialized concurrent users of the\n * same timer, then mod_timer() is the only safe way to modify the timeout,\n * since add_timer() cannot modify an already running timer.\n *\n * The function returns whether it has modified a pending timer or not.\n * (ie. mod_timer() of an inactive timer returns 0, mod_timer() of an\n * active timer returns 1.)\n */\nint mod_timer(struct timer_list *timer, unsigned long expires)\n{\n\treturn __mod_timer(timer, expires, 0);\n}\nEXPORT_SYMBOL(mod_timer);\n\n/**\n * timer_reduce - Modify a timer's timeout if it would reduce the timeout\n * @timer:\tThe timer to be modified\n * @expires:\tNew timeout in jiffies\n *\n * timer_reduce() is very similar to mod_timer(), except that it will only\n * modify a running timer if that would reduce the expiration time (it will\n * start a timer that isn't running).\n */\nint timer_reduce(struct timer_list *timer, unsigned long expires)\n{\n\treturn __mod_timer(timer, expires, MOD_TIMER_REDUCE);\n}\nEXPORT_SYMBOL(timer_reduce);\n\n/**\n * add_timer - start a timer\n * @timer: the timer to be added\n *\n * The kernel will do a ->function(@timer) callback from the\n * timer interrupt at the ->expires point in the future. The\n * current time is 'jiffies'.\n *\n * The timer's ->expires, ->function fields must be set prior calling this\n * function.\n *\n * Timers with an ->expires field in the past will be executed in the next\n * timer tick.\n */\nvoid add_timer(struct timer_list *timer)\n{\n\tBUG_ON(timer_pending(timer));\n\t__mod_timer(timer, timer->expires, MOD_TIMER_NOTPENDING);\n}\nEXPORT_SYMBOL(add_timer);\n\n/**\n * add_timer_on - start a timer on a particular CPU\n * @timer: the timer to be added\n * @cpu: the CPU to start it on\n *\n * This is not very scalable on SMP. Double adds are not possible.\n */\nvoid add_timer_on(struct timer_list *timer, int cpu)\n{\n\tstruct timer_base *new_base, *base;\n\tunsigned long flags;\n\n\tBUG_ON(timer_pending(timer) || !timer->function);\n\n\tnew_base = get_timer_cpu_base(timer->flags, cpu);\n\n\t/*\n\t * If @timer was on a different CPU, it should be migrated with the\n\t * old base locked to prevent other operations proceeding with the\n\t * wrong base locked.  See lock_timer_base().\n\t */\n\tbase = lock_timer_base(timer, &flags);\n\tif (base != new_base) {\n\t\ttimer->flags |= TIMER_MIGRATING;\n\n\t\traw_spin_unlock(&base->lock);\n\t\tbase = new_base;\n\t\traw_spin_lock(&base->lock);\n\t\tWRITE_ONCE(timer->flags,\n\t\t\t   (timer->flags & ~TIMER_BASEMASK) | cpu);\n\t}\n\tforward_timer_base(base);\n\n\tdebug_timer_activate(timer);\n\tinternal_add_timer(base, timer);\n\traw_spin_unlock_irqrestore(&base->lock, flags);\n}\nEXPORT_SYMBOL_GPL(add_timer_on);\n\n/**\n * del_timer - deactivate a timer.\n * @timer: the timer to be deactivated\n *\n * del_timer() deactivates a timer - this works on both active and inactive\n * timers.\n *\n * The function returns whether it has deactivated a pending timer or not.\n * (ie. del_timer() of an inactive timer returns 0, del_timer() of an\n * active timer returns 1.)\n */\nint del_timer(struct timer_list *timer)\n{\n\tstruct timer_base *base;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tdebug_assert_init(timer);\n\n\tif (timer_pending(timer)) {\n\t\tbase = lock_timer_base(timer, &flags);\n\t\tret = detach_if_pending(timer, base, true);\n\t\traw_spin_unlock_irqrestore(&base->lock, flags);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(del_timer);\n\n/**\n * try_to_del_timer_sync - Try to deactivate a timer\n * @timer: timer to delete\n *\n * This function tries to deactivate a timer. Upon successful (ret >= 0)\n * exit the timer is not queued and the handler is not running on any CPU.\n */\nint try_to_del_timer_sync(struct timer_list *timer)\n{\n\tstruct timer_base *base;\n\tunsigned long flags;\n\tint ret = -1;\n\n\tdebug_assert_init(timer);\n\n\tbase = lock_timer_base(timer, &flags);\n\n\tif (base->running_timer != timer)\n\t\tret = detach_if_pending(timer, base, true);\n\n\traw_spin_unlock_irqrestore(&base->lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(try_to_del_timer_sync);\n\n#ifdef CONFIG_PREEMPT_RT\nstatic __init void timer_base_init_expiry_lock(struct timer_base *base)\n{\n\tspin_lock_init(&base->expiry_lock);\n}\n\nstatic inline void timer_base_lock_expiry(struct timer_base *base)\n{\n\tspin_lock(&base->expiry_lock);\n}\n\nstatic inline void timer_base_unlock_expiry(struct timer_base *base)\n{\n\tspin_unlock(&base->expiry_lock);\n}\n\n/*\n * The counterpart to del_timer_wait_running().\n *\n * If there is a waiter for base->expiry_lock, then it was waiting for the\n * timer callback to finish. Drop expiry_lock and reaquire it. That allows\n * the waiter to acquire the lock and make progress.\n */\nstatic void timer_sync_wait_running(struct timer_base *base)\n{\n\tif (atomic_read(&base->timer_waiters)) {\n\t\tspin_unlock(&base->expiry_lock);\n\t\tspin_lock(&base->expiry_lock);\n\t}\n}\n\n/*\n * This function is called on PREEMPT_RT kernels when the fast path\n * deletion of a timer failed because the timer callback function was\n * running.\n *\n * This prevents priority inversion, if the softirq thread on a remote CPU\n * got preempted, and it prevents a life lock when the task which tries to\n * delete a timer preempted the softirq thread running the timer callback\n * function.\n */\nstatic void del_timer_wait_running(struct timer_list *timer)\n{\n\tu32 tf;\n\n\ttf = READ_ONCE(timer->flags);\n\tif (!(tf & TIMER_MIGRATING)) {\n\t\tstruct timer_base *base = get_timer_base(tf);\n\n\t\t/*\n\t\t * Mark the base as contended and grab the expiry lock,\n\t\t * which is held by the softirq across the timer\n\t\t * callback. Drop the lock immediately so the softirq can\n\t\t * expire the next timer. In theory the timer could already\n\t\t * be running again, but that's more than unlikely and just\n\t\t * causes another wait loop.\n\t\t */\n\t\tatomic_inc(&base->timer_waiters);\n\t\tspin_lock_bh(&base->expiry_lock);\n\t\tatomic_dec(&base->timer_waiters);\n\t\tspin_unlock_bh(&base->expiry_lock);\n\t}\n}\n#else\nstatic inline void timer_base_init_expiry_lock(struct timer_base *base) { }\nstatic inline void timer_base_lock_expiry(struct timer_base *base) { }\nstatic inline void timer_base_unlock_expiry(struct timer_base *base) { }\nstatic inline void timer_sync_wait_running(struct timer_base *base) { }\nstatic inline void del_timer_wait_running(struct timer_list *timer) { }\n#endif\n\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)\n/**\n * del_timer_sync - deactivate a timer and wait for the handler to finish.\n * @timer: the timer to be deactivated\n *\n * This function only differs from del_timer() on SMP: besides deactivating\n * the timer it also makes sure the handler has finished executing on other\n * CPUs.\n *\n * Synchronization rules: Callers must prevent restarting of the timer,\n * otherwise this function is meaningless. It must not be called from\n * interrupt contexts unless the timer is an irqsafe one. The caller must\n * not hold locks which would prevent completion of the timer's\n * handler. The timer's handler must not call add_timer_on(). Upon exit the\n * timer is not queued and the handler is not running on any CPU.\n *\n * Note: For !irqsafe timers, you must not hold locks that are held in\n *   interrupt context while calling this function. Even if the lock has\n *   nothing to do with the timer in question.  Here's why::\n *\n *    CPU0                             CPU1\n *    ----                             ----\n *                                     <SOFTIRQ>\n *                                       call_timer_fn();\n *                                       base->running_timer = mytimer;\n *    spin_lock_irq(somelock);\n *                                     <IRQ>\n *                                        spin_lock(somelock);\n *    del_timer_sync(mytimer);\n *    while (base->running_timer == mytimer);\n *\n * Now del_timer_sync() will never return and never release somelock.\n * The interrupt on the other CPU is waiting to grab somelock but\n * it has interrupted the softirq that CPU0 is waiting to finish.\n *\n * The function returns whether it has deactivated a pending timer or not.\n */\nint del_timer_sync(struct timer_list *timer)\n{\n\tint ret;\n\n#ifdef CONFIG_LOCKDEP\n\tunsigned long flags;\n\n\t/*\n\t * If lockdep gives a backtrace here, please reference\n\t * the synchronization rules above.\n\t */\n\tlocal_irq_save(flags);\n\tlock_map_acquire(&timer->lockdep_map);\n\tlock_map_release(&timer->lockdep_map);\n\tlocal_irq_restore(flags);\n#endif\n\t/*\n\t * don't use it in hardirq context, because it\n\t * could lead to deadlock.\n\t */\n\tWARN_ON(in_irq() && !(timer->flags & TIMER_IRQSAFE));\n\n\tdo {\n\t\tret = try_to_del_timer_sync(timer);\n\n\t\tif (unlikely(ret < 0)) {\n\t\t\tdel_timer_wait_running(timer);\n\t\t\tcpu_relax();\n\t\t}\n\t} while (ret < 0);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(del_timer_sync);\n#endif\n\nstatic void call_timer_fn(struct timer_list *timer,\n\t\t\t  void (*fn)(struct timer_list *),\n\t\t\t  unsigned long baseclk)\n{\n\tint count = preempt_count();\n\n#ifdef CONFIG_LOCKDEP\n\t/*\n\t * It is permissible to free the timer from inside the\n\t * function that is called from it, this we need to take into\n\t * account for lockdep too. To avoid bogus \"held lock freed\"\n\t * warnings as well as problems when looking into\n\t * timer->lockdep_map, make a copy and use that here.\n\t */\n\tstruct lockdep_map lockdep_map;\n\n\tlockdep_copy_map(&lockdep_map, &timer->lockdep_map);\n#endif\n\t/*\n\t * Couple the lock chain with the lock chain at\n\t * del_timer_sync() by acquiring the lock_map around the fn()\n\t * call here and in del_timer_sync().\n\t */\n\tlock_map_acquire(&lockdep_map);\n\n\ttrace_timer_expire_entry(timer, baseclk);\n\tfn(timer);\n\ttrace_timer_expire_exit(timer);\n\n\tlock_map_release(&lockdep_map);\n\n\tif (count != preempt_count()) {\n\t\tWARN_ONCE(1, \"timer: %pS preempt leak: %08x -> %08x\\n\",\n\t\t\t  fn, count, preempt_count());\n\t\t/*\n\t\t * Restore the preempt count. That gives us a decent\n\t\t * chance to survive and extract information. If the\n\t\t * callback kept a lock held, bad luck, but not worse\n\t\t * than the BUG() we had.\n\t\t */\n\t\tpreempt_count_set(count);\n\t}\n}\n\nstatic void expire_timers(struct timer_base *base, struct hlist_head *head)\n{\n\t/*\n\t * This value is required only for tracing. base->clk was\n\t * incremented directly before expire_timers was called. But expiry\n\t * is related to the old base->clk value.\n\t */\n\tunsigned long baseclk = base->clk - 1;\n\n\twhile (!hlist_empty(head)) {\n\t\tstruct timer_list *timer;\n\t\tvoid (*fn)(struct timer_list *);\n\n\t\ttimer = hlist_entry(head->first, struct timer_list, entry);\n\n\t\tbase->running_timer = timer;\n\t\tdetach_timer(timer, true);\n\n\t\tfn = timer->function;\n\n\t\tif (timer->flags & TIMER_IRQSAFE) {\n\t\t\traw_spin_unlock(&base->lock);\n\t\t\tcall_timer_fn(timer, fn, baseclk);\n\t\t\tbase->running_timer = NULL;\n\t\t\traw_spin_lock(&base->lock);\n\t\t} else {\n\t\t\traw_spin_unlock_irq(&base->lock);\n\t\t\tcall_timer_fn(timer, fn, baseclk);\n\t\t\tbase->running_timer = NULL;\n\t\t\ttimer_sync_wait_running(base);\n\t\t\traw_spin_lock_irq(&base->lock);\n\t\t}\n\t}\n}\n\nstatic int __collect_expired_timers(struct timer_base *base,\n\t\t\t\t    struct hlist_head *heads)\n{\n\tunsigned long clk = base->clk;\n\tstruct hlist_head *vec;\n\tint i, levels = 0;\n\tunsigned int idx;\n\n\tfor (i = 0; i < LVL_DEPTH; i++) {\n\t\tidx = (clk & LVL_MASK) + i * LVL_SIZE;\n\n\t\tif (__test_and_clear_bit(idx, base->pending_map)) {\n\t\t\tvec = base->vectors + idx;\n\t\t\thlist_move_list(vec, heads++);\n\t\t\tlevels++;\n\t\t}\n\t\t/* Is it time to look at the next level? */\n\t\tif (clk & LVL_CLK_MASK)\n\t\t\tbreak;\n\t\t/* Shift clock for the next level granularity */\n\t\tclk >>= LVL_CLK_SHIFT;\n\t}\n\treturn levels;\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * Find the next pending bucket of a level. Search from level start (@offset)\n * + @clk upwards and if nothing there, search from start of the level\n * (@offset) up to @offset + clk.\n */\nstatic int next_pending_bucket(struct timer_base *base, unsigned offset,\n\t\t\t       unsigned clk)\n{\n\tunsigned pos, start = offset + clk;\n\tunsigned end = offset + LVL_SIZE;\n\n\tpos = find_next_bit(base->pending_map, end, start);\n\tif (pos < end)\n\t\treturn pos - start;\n\n\tpos = find_next_bit(base->pending_map, start, offset);\n\treturn pos < start ? pos + LVL_SIZE - start : -1;\n}\n\n/*\n * Search the first expiring timer in the various clock levels. Caller must\n * hold base->lock.\n */\nstatic unsigned long __next_timer_interrupt(struct timer_base *base)\n{\n\tunsigned long clk, next, adj;\n\tunsigned lvl, offset = 0;\n\n\tnext = base->clk + NEXT_TIMER_MAX_DELTA;\n\tclk = base->clk;\n\tfor (lvl = 0; lvl < LVL_DEPTH; lvl++, offset += LVL_SIZE) {\n\t\tint pos = next_pending_bucket(base, offset, clk & LVL_MASK);\n\n\t\tif (pos >= 0) {\n\t\t\tunsigned long tmp = clk + (unsigned long) pos;\n\n\t\t\ttmp <<= LVL_SHIFT(lvl);\n\t\t\tif (time_before(tmp, next))\n\t\t\t\tnext = tmp;\n\t\t}\n\t\t/*\n\t\t * Clock for the next level. If the current level clock lower\n\t\t * bits are zero, we look at the next level as is. If not we\n\t\t * need to advance it by one because that's going to be the\n\t\t * next expiring bucket in that level. base->clk is the next\n\t\t * expiring jiffie. So in case of:\n\t\t *\n\t\t * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0\n\t\t *  0    0    0    0    0    0\n\t\t *\n\t\t * we have to look at all levels @index 0. With\n\t\t *\n\t\t * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0\n\t\t *  0    0    0    0    0    2\n\t\t *\n\t\t * LVL0 has the next expiring bucket @index 2. The upper\n\t\t * levels have the next expiring bucket @index 1.\n\t\t *\n\t\t * In case that the propagation wraps the next level the same\n\t\t * rules apply:\n\t\t *\n\t\t * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0\n\t\t *  0    0    0    0    F    2\n\t\t *\n\t\t * So after looking at LVL0 we get:\n\t\t *\n\t\t * LVL5 LVL4 LVL3 LVL2 LVL1\n\t\t *  0    0    0    1    0\n\t\t *\n\t\t * So no propagation from LVL1 to LVL2 because that happened\n\t\t * with the add already, but then we need to propagate further\n\t\t * from LVL2 to LVL3.\n\t\t *\n\t\t * So the simple check whether the lower bits of the current\n\t\t * level are 0 or not is sufficient for all cases.\n\t\t */\n\t\tadj = clk & LVL_CLK_MASK ? 1 : 0;\n\t\tclk >>= LVL_CLK_SHIFT;\n\t\tclk += adj;\n\t}\n\treturn next;\n}\n\n/*\n * Check, if the next hrtimer event is before the next timer wheel\n * event:\n */\nstatic u64 cmp_next_hrtimer_event(u64 basem, u64 expires)\n{\n\tu64 nextevt = hrtimer_get_next_event();\n\n\t/*\n\t * If high resolution timers are enabled\n\t * hrtimer_get_next_event() returns KTIME_MAX.\n\t */\n\tif (expires <= nextevt)\n\t\treturn expires;\n\n\t/*\n\t * If the next timer is already expired, return the tick base\n\t * time so the tick is fired immediately.\n\t */\n\tif (nextevt <= basem)\n\t\treturn basem;\n\n\t/*\n\t * Round up to the next jiffie. High resolution timers are\n\t * off, so the hrtimers are expired in the tick and we need to\n\t * make sure that this tick really expires the timer to avoid\n\t * a ping pong of the nohz stop code.\n\t *\n\t * Use DIV_ROUND_UP_ULL to prevent gcc calling __divdi3\n\t */\n\treturn DIV_ROUND_UP_ULL(nextevt, TICK_NSEC) * TICK_NSEC;\n}\n\n/**\n * get_next_timer_interrupt - return the time (clock mono) of the next timer\n * @basej:\tbase time jiffies\n * @basem:\tbase time clock monotonic\n *\n * Returns the tick aligned clock monotonic time of the next pending\n * timer or KTIME_MAX if no timer is pending.\n */\nu64 get_next_timer_interrupt(unsigned long basej, u64 basem)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\tu64 expires = KTIME_MAX;\n\tunsigned long nextevt;\n\tbool is_max_delta;\n\n\t/*\n\t * Pretend that there is no timer pending if the cpu is offline.\n\t * Possible pending timers will be migrated later to an active cpu.\n\t */\n\tif (cpu_is_offline(smp_processor_id()))\n\t\treturn expires;\n\n\traw_spin_lock(&base->lock);\n\tnextevt = __next_timer_interrupt(base);\n\tis_max_delta = (nextevt == base->clk + NEXT_TIMER_MAX_DELTA);\n\tbase->next_expiry = nextevt;\n\t/*\n\t * We have a fresh next event. Check whether we can forward the\n\t * base. We can only do that when @basej is past base->clk\n\t * otherwise we might rewind base->clk.\n\t */\n\tif (time_after(basej, base->clk)) {\n\t\tif (time_after(nextevt, basej))\n\t\t\tbase->clk = basej;\n\t\telse if (time_after(nextevt, base->clk))\n\t\t\tbase->clk = nextevt;\n\t}\n\n\tif (time_before_eq(nextevt, basej)) {\n\t\texpires = basem;\n\t\tbase->is_idle = false;\n\t} else {\n\t\tif (!is_max_delta)\n\t\t\texpires = basem + (u64)(nextevt - basej) * TICK_NSEC;\n\t\t/*\n\t\t * If we expect to sleep more than a tick, mark the base idle.\n\t\t * Also the tick is stopped so any added timer must forward\n\t\t * the base clk itself to keep granularity small. This idle\n\t\t * logic is only maintained for the BASE_STD base, deferrable\n\t\t * timers may still see large granularity skew (by design).\n\t\t */\n\t\tif ((expires - basem) > TICK_NSEC) {\n\t\t\tbase->must_forward_clk = true;\n\t\t\tbase->is_idle = true;\n\t\t}\n\t}\n\traw_spin_unlock(&base->lock);\n\n\treturn cmp_next_hrtimer_event(basem, expires);\n}\n\n/**\n * timer_clear_idle - Clear the idle state of the timer base\n *\n * Called with interrupts disabled\n */\nvoid timer_clear_idle(void)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\t/*\n\t * We do this unlocked. The worst outcome is a remote enqueue sending\n\t * a pointless IPI, but taking the lock would just make the window for\n\t * sending the IPI a few instructions smaller for the cost of taking\n\t * the lock in the exit from idle path.\n\t */\n\tbase->is_idle = false;\n}\n\nstatic int collect_expired_timers(struct timer_base *base,\n\t\t\t\t  struct hlist_head *heads)\n{\n\tunsigned long now = READ_ONCE(jiffies);\n\n\t/*\n\t * NOHZ optimization. After a long idle sleep we need to forward the\n\t * base to current jiffies. Avoid a loop by searching the bitfield for\n\t * the next expiring timer.\n\t */\n\tif ((long)(now - base->clk) > 2) {\n\t\tunsigned long next = __next_timer_interrupt(base);\n\n\t\t/*\n\t\t * If the next timer is ahead of time forward to current\n\t\t * jiffies, otherwise forward to the next expiry time:\n\t\t */\n\t\tif (time_after(next, now)) {\n\t\t\t/*\n\t\t\t * The call site will increment base->clk and then\n\t\t\t * terminate the expiry loop immediately.\n\t\t\t */\n\t\t\tbase->clk = now;\n\t\t\treturn 0;\n\t\t}\n\t\tbase->clk = next;\n\t}\n\treturn __collect_expired_timers(base, heads);\n}\n#else\nstatic inline int collect_expired_timers(struct timer_base *base,\n\t\t\t\t\t struct hlist_head *heads)\n{\n\treturn __collect_expired_timers(base, heads);\n}\n#endif\n\n/*\n * Called from the timer interrupt handler to charge one tick to the current\n * process.  user_tick is 1 if the tick is user time, 0 for system.\n */\nvoid update_process_times(int user_tick)\n{\n\tstruct task_struct *p = current;\n\n\t/* Note: this timer irq context must be accounted for as well. */\n\taccount_process_tick(p, user_tick);\n\trun_local_timers();\n\trcu_sched_clock_irq(user_tick);\n#ifdef CONFIG_IRQ_WORK\n\tif (in_irq())\n\t\tirq_work_tick();\n#endif\n\tscheduler_tick();\n\tif (IS_ENABLED(CONFIG_POSIX_TIMERS))\n\t\trun_posix_cpu_timers();\n\n\t/* The current CPU might make use of net randoms without receiving IRQs\n\t * to renew them often enough. Let's update the net_rand_state from a\n\t * non-constant value that's not affine to the number of calls to make\n\t * sure it's updated when there's some activity (we don't care in idle).\n\t */\n\tthis_cpu_add(net_rand_state.s1, rol32(jiffies, 24) + user_tick);\n}\n\n/**\n * __run_timers - run all expired timers (if any) on this CPU.\n * @base: the timer vector to be processed.\n */\nstatic inline void __run_timers(struct timer_base *base)\n{\n\tstruct hlist_head heads[LVL_DEPTH];\n\tint levels;\n\n\tif (!time_after_eq(jiffies, base->clk))\n\t\treturn;\n\n\ttimer_base_lock_expiry(base);\n\traw_spin_lock_irq(&base->lock);\n\n\t/*\n\t * timer_base::must_forward_clk must be cleared before running\n\t * timers so that any timer functions that call mod_timer() will\n\t * not try to forward the base. Idle tracking / clock forwarding\n\t * logic is only used with BASE_STD timers.\n\t *\n\t * The must_forward_clk flag is cleared unconditionally also for\n\t * the deferrable base. The deferrable base is not affected by idle\n\t * tracking and never forwarded, so clearing the flag is a NOOP.\n\t *\n\t * The fact that the deferrable base is never forwarded can cause\n\t * large variations in granularity for deferrable timers, but they\n\t * can be deferred for long periods due to idle anyway.\n\t */\n\tbase->must_forward_clk = false;\n\n\twhile (time_after_eq(jiffies, base->clk)) {\n\n\t\tlevels = collect_expired_timers(base, heads);\n\t\tbase->clk++;\n\n\t\twhile (levels--)\n\t\t\texpire_timers(base, heads + levels);\n\t}\n\traw_spin_unlock_irq(&base->lock);\n\ttimer_base_unlock_expiry(base);\n}\n\n/*\n * This function runs timers and the timer-tq in bottom half context.\n */\nstatic __latent_entropy void run_timer_softirq(struct softirq_action *h)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\t__run_timers(base);\n\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON))\n\t\t__run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));\n}\n\n/*\n * Called by the local, per-CPU timer interrupt on SMP.\n */\nvoid run_local_timers(void)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\thrtimer_run_queues();\n\t/* Raise the softirq only if required. */\n\tif (time_before(jiffies, base->clk)) {\n\t\tif (!IS_ENABLED(CONFIG_NO_HZ_COMMON))\n\t\t\treturn;\n\t\t/* CPU is awake, so check the deferrable base. */\n\t\tbase++;\n\t\tif (time_before(jiffies, base->clk))\n\t\t\treturn;\n\t}\n\traise_softirq(TIMER_SOFTIRQ);\n}\n\n/*\n * Since schedule_timeout()'s timer is defined on the stack, it must store\n * the target task on the stack as well.\n */\nstruct process_timer {\n\tstruct timer_list timer;\n\tstruct task_struct *task;\n};\n\nstatic void process_timeout(struct timer_list *t)\n{\n\tstruct process_timer *timeout = from_timer(timeout, t, timer);\n\n\twake_up_process(timeout->task);\n}\n\n/**\n * schedule_timeout - sleep until timeout\n * @timeout: timeout value in jiffies\n *\n * Make the current task sleep until @timeout jiffies have elapsed.\n * The function behavior depends on the current task state\n * (see also set_current_state() description):\n *\n * %TASK_RUNNING - the scheduler is called, but the task does not sleep\n * at all. That happens because sched_submit_work() does nothing for\n * tasks in %TASK_RUNNING state.\n *\n * %TASK_UNINTERRUPTIBLE - at least @timeout jiffies are guaranteed to\n * pass before the routine returns unless the current task is explicitly\n * woken up, (e.g. by wake_up_process()).\n *\n * %TASK_INTERRUPTIBLE - the routine may return early if a signal is\n * delivered to the current task or the current task is explicitly woken\n * up.\n *\n * The current task state is guaranteed to be %TASK_RUNNING when this\n * routine returns.\n *\n * Specifying a @timeout value of %MAX_SCHEDULE_TIMEOUT will schedule\n * the CPU away without a bound on the timeout. In this case the return\n * value will be %MAX_SCHEDULE_TIMEOUT.\n *\n * Returns 0 when the timer has expired otherwise the remaining time in\n * jiffies will be returned. In all cases the return value is guaranteed\n * to be non-negative.\n */\nsigned long __sched schedule_timeout(signed long timeout)\n{\n\tstruct process_timer timer;\n\tunsigned long expire;\n\n\tswitch (timeout)\n\t{\n\tcase MAX_SCHEDULE_TIMEOUT:\n\t\t/*\n\t\t * These two special cases are useful to be comfortable\n\t\t * in the caller. Nothing more. We could take\n\t\t * MAX_SCHEDULE_TIMEOUT from one of the negative value\n\t\t * but I' d like to return a valid offset (>=0) to allow\n\t\t * the caller to do everything it want with the retval.\n\t\t */\n\t\tschedule();\n\t\tgoto out;\n\tdefault:\n\t\t/*\n\t\t * Another bit of PARANOID. Note that the retval will be\n\t\t * 0 since no piece of kernel is supposed to do a check\n\t\t * for a negative retval of schedule_timeout() (since it\n\t\t * should never happens anyway). You just have the printk()\n\t\t * that will tell you if something is gone wrong and where.\n\t\t */\n\t\tif (timeout < 0) {\n\t\t\tprintk(KERN_ERR \"schedule_timeout: wrong timeout \"\n\t\t\t\t\"value %lx\\n\", timeout);\n\t\t\tdump_stack();\n\t\t\tcurrent->state = TASK_RUNNING;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\texpire = timeout + jiffies;\n\n\ttimer.task = current;\n\ttimer_setup_on_stack(&timer.timer, process_timeout, 0);\n\t__mod_timer(&timer.timer, expire, MOD_TIMER_NOTPENDING);\n\tschedule();\n\tdel_singleshot_timer_sync(&timer.timer);\n\n\t/* Remove the timer from the object tracker */\n\tdestroy_timer_on_stack(&timer.timer);\n\n\ttimeout = expire - jiffies;\n\n out:\n\treturn timeout < 0 ? 0 : timeout;\n}\nEXPORT_SYMBOL(schedule_timeout);\n\n/*\n * We can use __set_current_state() here because schedule_timeout() calls\n * schedule() unconditionally.\n */\nsigned long __sched schedule_timeout_interruptible(signed long timeout)\n{\n\t__set_current_state(TASK_INTERRUPTIBLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_interruptible);\n\nsigned long __sched schedule_timeout_killable(signed long timeout)\n{\n\t__set_current_state(TASK_KILLABLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_killable);\n\nsigned long __sched schedule_timeout_uninterruptible(signed long timeout)\n{\n\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_uninterruptible);\n\n/*\n * Like schedule_timeout_uninterruptible(), except this task will not contribute\n * to load average.\n */\nsigned long __sched schedule_timeout_idle(signed long timeout)\n{\n\t__set_current_state(TASK_IDLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_idle);\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic void migrate_timer_list(struct timer_base *new_base, struct hlist_head *head)\n{\n\tstruct timer_list *timer;\n\tint cpu = new_base->cpu;\n\n\twhile (!hlist_empty(head)) {\n\t\ttimer = hlist_entry(head->first, struct timer_list, entry);\n\t\tdetach_timer(timer, false);\n\t\ttimer->flags = (timer->flags & ~TIMER_BASEMASK) | cpu;\n\t\tinternal_add_timer(new_base, timer);\n\t}\n}\n\nint timers_prepare_cpu(unsigned int cpu)\n{\n\tstruct timer_base *base;\n\tint b;\n\n\tfor (b = 0; b < NR_BASES; b++) {\n\t\tbase = per_cpu_ptr(&timer_bases[b], cpu);\n\t\tbase->clk = jiffies;\n\t\tbase->next_expiry = base->clk + NEXT_TIMER_MAX_DELTA;\n\t\tbase->is_idle = false;\n\t\tbase->must_forward_clk = true;\n\t}\n\treturn 0;\n}\n\nint timers_dead_cpu(unsigned int cpu)\n{\n\tstruct timer_base *old_base;\n\tstruct timer_base *new_base;\n\tint b, i;\n\n\tBUG_ON(cpu_online(cpu));\n\n\tfor (b = 0; b < NR_BASES; b++) {\n\t\told_base = per_cpu_ptr(&timer_bases[b], cpu);\n\t\tnew_base = get_cpu_ptr(&timer_bases[b]);\n\t\t/*\n\t\t * The caller is globally serialized and nobody else\n\t\t * takes two locks at once, deadlock is not possible.\n\t\t */\n\t\traw_spin_lock_irq(&new_base->lock);\n\t\traw_spin_lock_nested(&old_base->lock, SINGLE_DEPTH_NESTING);\n\n\t\t/*\n\t\t * The current CPUs base clock might be stale. Update it\n\t\t * before moving the timers over.\n\t\t */\n\t\tforward_timer_base(new_base);\n\n\t\tBUG_ON(old_base->running_timer);\n\n\t\tfor (i = 0; i < WHEEL_SIZE; i++)\n\t\t\tmigrate_timer_list(new_base, old_base->vectors + i);\n\n\t\traw_spin_unlock(&old_base->lock);\n\t\traw_spin_unlock_irq(&new_base->lock);\n\t\tput_cpu_ptr(&timer_bases);\n\t}\n\treturn 0;\n}\n\n#endif /* CONFIG_HOTPLUG_CPU */\n\nstatic void __init init_timer_cpu(int cpu)\n{\n\tstruct timer_base *base;\n\tint i;\n\n\tfor (i = 0; i < NR_BASES; i++) {\n\t\tbase = per_cpu_ptr(&timer_bases[i], cpu);\n\t\tbase->cpu = cpu;\n\t\traw_spin_lock_init(&base->lock);\n\t\tbase->clk = jiffies;\n\t\ttimer_base_init_expiry_lock(base);\n\t}\n}\n\nstatic void __init init_timer_cpus(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tinit_timer_cpu(cpu);\n}\n\nvoid __init init_timers(void)\n{\n\tinit_timer_cpus();\n\topen_softirq(TIMER_SOFTIRQ, run_timer_softirq);\n}\n\n/**\n * msleep - sleep safely even with waitqueue interruptions\n * @msecs: Time in milliseconds to sleep for\n */\nvoid msleep(unsigned int msecs)\n{\n\tunsigned long timeout = msecs_to_jiffies(msecs) + 1;\n\n\twhile (timeout)\n\t\ttimeout = schedule_timeout_uninterruptible(timeout);\n}\n\nEXPORT_SYMBOL(msleep);\n\n/**\n * msleep_interruptible - sleep waiting for signals\n * @msecs: Time in milliseconds to sleep for\n */\nunsigned long msleep_interruptible(unsigned int msecs)\n{\n\tunsigned long timeout = msecs_to_jiffies(msecs) + 1;\n\n\twhile (timeout && !signal_pending(current))\n\t\ttimeout = schedule_timeout_interruptible(timeout);\n\treturn jiffies_to_msecs(timeout);\n}\n\nEXPORT_SYMBOL(msleep_interruptible);\n\n/**\n * usleep_range - Sleep for an approximate time\n * @min: Minimum time in usecs to sleep\n * @max: Maximum time in usecs to sleep\n *\n * In non-atomic context where the exact wakeup time is flexible, use\n * usleep_range() instead of udelay().  The sleep improves responsiveness\n * by avoiding the CPU-hogging busy-wait of udelay(), and the range reduces\n * power usage by allowing hrtimers to take advantage of an already-\n * scheduled interrupt instead of scheduling a new one just for this sleep.\n */\nvoid __sched usleep_range(unsigned long min, unsigned long max)\n{\n\tktime_t exp = ktime_add_us(ktime_get(), min);\n\tu64 delta = (u64)(max - min) * NSEC_PER_USEC;\n\n\tfor (;;) {\n\t\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\t\t/* Do not return before the requested sleep time has elapsed */\n\t\tif (!schedule_hrtimeout_range(&exp, delta, HRTIMER_MODE_ABS))\n\t\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL(usleep_range);\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * This is a maximally equidistributed combined Tausworthe generator\n * based on code from GNU Scientific Library 1.5 (30 Jun 2004)\n *\n * lfsr113 version:\n *\n * x_n = (s1_n ^ s2_n ^ s3_n ^ s4_n)\n *\n * s1_{n+1} = (((s1_n & 4294967294) << 18) ^ (((s1_n <<  6) ^ s1_n) >> 13))\n * s2_{n+1} = (((s2_n & 4294967288) <<  2) ^ (((s2_n <<  2) ^ s2_n) >> 27))\n * s3_{n+1} = (((s3_n & 4294967280) <<  7) ^ (((s3_n << 13) ^ s3_n) >> 21))\n * s4_{n+1} = (((s4_n & 4294967168) << 13) ^ (((s4_n <<  3) ^ s4_n) >> 12))\n *\n * The period of this generator is about 2^113 (see erratum paper).\n *\n * From: P. L'Ecuyer, \"Maximally Equidistributed Combined Tausworthe\n * Generators\", Mathematics of Computation, 65, 213 (1996), 203--213:\n * http://www.iro.umontreal.ca/~lecuyer/myftp/papers/tausme.ps\n * ftp://ftp.iro.umontreal.ca/pub/simulation/lecuyer/papers/tausme.ps\n *\n * There is an erratum in the paper \"Tables of Maximally Equidistributed\n * Combined LFSR Generators\", Mathematics of Computation, 68, 225 (1999),\n * 261--269: http://www.iro.umontreal.ca/~lecuyer/myftp/papers/tausme2.ps\n *\n *      ... the k_j most significant bits of z_j must be non-zero,\n *      for each j. (Note: this restriction also applies to the\n *      computer code given in [4], but was mistakenly not mentioned\n *      in that paper.)\n *\n * This affects the seeding procedure by imposing the requirement\n * s1 > 1, s2 > 7, s3 > 15, s4 > 127.\n */\n\n#include <linux/types.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/jiffies.h>\n#include <linux/random.h>\n#include <linux/sched.h>\n#include <asm/unaligned.h>\n\n#ifdef CONFIG_RANDOM32_SELFTEST\nstatic void __init prandom_state_selftest(void);\n#else\nstatic inline void prandom_state_selftest(void)\n{\n}\n#endif\n\nDEFINE_PER_CPU(struct rnd_state, net_rand_state) __latent_entropy;\n\n/**\n *\tprandom_u32_state - seeded pseudo-random number generator.\n *\t@state: pointer to state structure holding seeded state.\n *\n *\tThis is used for pseudo-randomness with no outside seeding.\n *\tFor more random results, use prandom_u32().\n */\nu32 prandom_u32_state(struct rnd_state *state)\n{\n#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)\n\tstate->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);\n\tstate->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);\n\tstate->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);\n\tstate->s4 = TAUSWORTHE(state->s4,  3U, 12U, 4294967168U, 13U);\n\n\treturn (state->s1 ^ state->s2 ^ state->s3 ^ state->s4);\n}\nEXPORT_SYMBOL(prandom_u32_state);\n\n/**\n *\tprandom_u32 - pseudo random number generator\n *\n *\tA 32 bit pseudo-random number is generated using a fast\n *\talgorithm suitable for simulation. This algorithm is NOT\n *\tconsidered safe for cryptographic use.\n */\nu32 prandom_u32(void)\n{\n\tstruct rnd_state *state = &get_cpu_var(net_rand_state);\n\tu32 res;\n\n\tres = prandom_u32_state(state);\n\tput_cpu_var(net_rand_state);\n\n\treturn res;\n}\nEXPORT_SYMBOL(prandom_u32);\n\n/**\n *\tprandom_bytes_state - get the requested number of pseudo-random bytes\n *\n *\t@state: pointer to state structure holding seeded state.\n *\t@buf: where to copy the pseudo-random bytes to\n *\t@bytes: the requested number of bytes\n *\n *\tThis is used for pseudo-randomness with no outside seeding.\n *\tFor more random results, use prandom_bytes().\n */\nvoid prandom_bytes_state(struct rnd_state *state, void *buf, size_t bytes)\n{\n\tu8 *ptr = buf;\n\n\twhile (bytes >= sizeof(u32)) {\n\t\tput_unaligned(prandom_u32_state(state), (u32 *) ptr);\n\t\tptr += sizeof(u32);\n\t\tbytes -= sizeof(u32);\n\t}\n\n\tif (bytes > 0) {\n\t\tu32 rem = prandom_u32_state(state);\n\t\tdo {\n\t\t\t*ptr++ = (u8) rem;\n\t\t\tbytes--;\n\t\t\trem >>= BITS_PER_BYTE;\n\t\t} while (bytes > 0);\n\t}\n}\nEXPORT_SYMBOL(prandom_bytes_state);\n\n/**\n *\tprandom_bytes - get the requested number of pseudo-random bytes\n *\t@buf: where to copy the pseudo-random bytes to\n *\t@bytes: the requested number of bytes\n */\nvoid prandom_bytes(void *buf, size_t bytes)\n{\n\tstruct rnd_state *state = &get_cpu_var(net_rand_state);\n\n\tprandom_bytes_state(state, buf, bytes);\n\tput_cpu_var(net_rand_state);\n}\nEXPORT_SYMBOL(prandom_bytes);\n\nstatic void prandom_warmup(struct rnd_state *state)\n{\n\t/* Calling RNG ten times to satisfy recurrence condition */\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n\tprandom_u32_state(state);\n}\n\nstatic u32 __extract_hwseed(void)\n{\n\tunsigned int val = 0;\n\n\t(void)(arch_get_random_seed_int(&val) ||\n\t       arch_get_random_int(&val));\n\n\treturn val;\n}\n\nstatic void prandom_seed_early(struct rnd_state *state, u32 seed,\n\t\t\t       bool mix_with_hwseed)\n{\n#define LCG(x)\t ((x) * 69069U)\t/* super-duper LCG */\n#define HWSEED() (mix_with_hwseed ? __extract_hwseed() : 0)\n\tstate->s1 = __seed(HWSEED() ^ LCG(seed),        2U);\n\tstate->s2 = __seed(HWSEED() ^ LCG(state->s1),   8U);\n\tstate->s3 = __seed(HWSEED() ^ LCG(state->s2),  16U);\n\tstate->s4 = __seed(HWSEED() ^ LCG(state->s3), 128U);\n}\n\n/**\n *\tprandom_seed - add entropy to pseudo random number generator\n *\t@entropy: entropy value\n *\n *\tAdd some additional entropy to the prandom pool.\n */\nvoid prandom_seed(u32 entropy)\n{\n\tint i;\n\t/*\n\t * No locking on the CPUs, but then somewhat random results are, well,\n\t * expected.\n\t */\n\tfor_each_possible_cpu(i) {\n\t\tstruct rnd_state *state = &per_cpu(net_rand_state, i);\n\n\t\tstate->s1 = __seed(state->s1 ^ entropy, 2U);\n\t\tprandom_warmup(state);\n\t}\n}\nEXPORT_SYMBOL(prandom_seed);\n\n/*\n *\tGenerate some initially weak seeding values to allow\n *\tto start the prandom_u32() engine.\n */\nstatic int __init prandom_init(void)\n{\n\tint i;\n\n\tprandom_state_selftest();\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rnd_state *state = &per_cpu(net_rand_state, i);\n\t\tu32 weak_seed = (i + jiffies) ^ random_get_entropy();\n\n\t\tprandom_seed_early(state, weak_seed, true);\n\t\tprandom_warmup(state);\n\t}\n\n\treturn 0;\n}\ncore_initcall(prandom_init);\n\nstatic void __prandom_timer(struct timer_list *unused);\n\nstatic DEFINE_TIMER(seed_timer, __prandom_timer);\n\nstatic void __prandom_timer(struct timer_list *unused)\n{\n\tu32 entropy;\n\tunsigned long expires;\n\n\tget_random_bytes(&entropy, sizeof(entropy));\n\tprandom_seed(entropy);\n\n\t/* reseed every ~60 seconds, in [40 .. 80) interval with slack */\n\texpires = 40 + prandom_u32_max(40);\n\tseed_timer.expires = jiffies + msecs_to_jiffies(expires * MSEC_PER_SEC);\n\n\tadd_timer(&seed_timer);\n}\n\nstatic void __init __prandom_start_seed_timer(void)\n{\n\tseed_timer.expires = jiffies + msecs_to_jiffies(40 * MSEC_PER_SEC);\n\tadd_timer(&seed_timer);\n}\n\nvoid prandom_seed_full_state(struct rnd_state __percpu *pcpu_state)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rnd_state *state = per_cpu_ptr(pcpu_state, i);\n\t\tu32 seeds[4];\n\n\t\tget_random_bytes(&seeds, sizeof(seeds));\n\t\tstate->s1 = __seed(seeds[0],   2U);\n\t\tstate->s2 = __seed(seeds[1],   8U);\n\t\tstate->s3 = __seed(seeds[2],  16U);\n\t\tstate->s4 = __seed(seeds[3], 128U);\n\n\t\tprandom_warmup(state);\n\t}\n}\nEXPORT_SYMBOL(prandom_seed_full_state);\n\n/*\n *\tGenerate better values after random number generator\n *\tis fully initialized.\n */\nstatic void __prandom_reseed(bool late)\n{\n\tunsigned long flags;\n\tstatic bool latch = false;\n\tstatic DEFINE_SPINLOCK(lock);\n\n\t/* Asking for random bytes might result in bytes getting\n\t * moved into the nonblocking pool and thus marking it\n\t * as initialized. In this case we would double back into\n\t * this function and attempt to do a late reseed.\n\t * Ignore the pointless attempt to reseed again if we're\n\t * already waiting for bytes when the nonblocking pool\n\t * got initialized.\n\t */\n\n\t/* only allow initial seeding (late == false) once */\n\tif (!spin_trylock_irqsave(&lock, flags))\n\t\treturn;\n\n\tif (latch && !late)\n\t\tgoto out;\n\n\tlatch = true;\n\tprandom_seed_full_state(&net_rand_state);\nout:\n\tspin_unlock_irqrestore(&lock, flags);\n}\n\nvoid prandom_reseed_late(void)\n{\n\t__prandom_reseed(true);\n}\n\nstatic int __init prandom_reseed(void)\n{\n\t__prandom_reseed(false);\n\t__prandom_start_seed_timer();\n\treturn 0;\n}\nlate_initcall(prandom_reseed);\n\n#ifdef CONFIG_RANDOM32_SELFTEST\nstatic struct prandom_test1 {\n\tu32 seed;\n\tu32 result;\n} test1[] = {\n\t{ 1U, 3484351685U },\n\t{ 2U, 2623130059U },\n\t{ 3U, 3125133893U },\n\t{ 4U,  984847254U },\n};\n\nstatic struct prandom_test2 {\n\tu32 seed;\n\tu32 iteration;\n\tu32 result;\n} test2[] = {\n\t/* Test cases against taus113 from GSL library. */\n\t{  931557656U, 959U, 2975593782U },\n\t{ 1339693295U, 876U, 3887776532U },\n\t{ 1545556285U, 961U, 1615538833U },\n\t{  601730776U, 723U, 1776162651U },\n\t{ 1027516047U, 687U,  511983079U },\n\t{  416526298U, 700U,  916156552U },\n\t{ 1395522032U, 652U, 2222063676U },\n\t{  366221443U, 617U, 2992857763U },\n\t{ 1539836965U, 714U, 3783265725U },\n\t{  556206671U, 994U,  799626459U },\n\t{  684907218U, 799U,  367789491U },\n\t{ 2121230701U, 931U, 2115467001U },\n\t{ 1668516451U, 644U, 3620590685U },\n\t{  768046066U, 883U, 2034077390U },\n\t{ 1989159136U, 833U, 1195767305U },\n\t{  536585145U, 996U, 3577259204U },\n\t{ 1008129373U, 642U, 1478080776U },\n\t{ 1740775604U, 939U, 1264980372U },\n\t{ 1967883163U, 508U,   10734624U },\n\t{ 1923019697U, 730U, 3821419629U },\n\t{  442079932U, 560U, 3440032343U },\n\t{ 1961302714U, 845U,  841962572U },\n\t{ 2030205964U, 962U, 1325144227U },\n\t{ 1160407529U, 507U,  240940858U },\n\t{  635482502U, 779U, 4200489746U },\n\t{ 1252788931U, 699U,  867195434U },\n\t{ 1961817131U, 719U,  668237657U },\n\t{ 1071468216U, 983U,  917876630U },\n\t{ 1281848367U, 932U, 1003100039U },\n\t{  582537119U, 780U, 1127273778U },\n\t{ 1973672777U, 853U, 1071368872U },\n\t{ 1896756996U, 762U, 1127851055U },\n\t{  847917054U, 500U, 1717499075U },\n\t{ 1240520510U, 951U, 2849576657U },\n\t{ 1685071682U, 567U, 1961810396U },\n\t{ 1516232129U, 557U,    3173877U },\n\t{ 1208118903U, 612U, 1613145022U },\n\t{ 1817269927U, 693U, 4279122573U },\n\t{ 1510091701U, 717U,  638191229U },\n\t{  365916850U, 807U,  600424314U },\n\t{  399324359U, 702U, 1803598116U },\n\t{ 1318480274U, 779U, 2074237022U },\n\t{  697758115U, 840U, 1483639402U },\n\t{ 1696507773U, 840U,  577415447U },\n\t{ 2081979121U, 981U, 3041486449U },\n\t{  955646687U, 742U, 3846494357U },\n\t{ 1250683506U, 749U,  836419859U },\n\t{  595003102U, 534U,  366794109U },\n\t{   47485338U, 558U, 3521120834U },\n\t{  619433479U, 610U, 3991783875U },\n\t{  704096520U, 518U, 4139493852U },\n\t{ 1712224984U, 606U, 2393312003U },\n\t{ 1318233152U, 922U, 3880361134U },\n\t{  855572992U, 761U, 1472974787U },\n\t{   64721421U, 703U,  683860550U },\n\t{  678931758U, 840U,  380616043U },\n\t{  692711973U, 778U, 1382361947U },\n\t{  677703619U, 530U, 2826914161U },\n\t{   92393223U, 586U, 1522128471U },\n\t{ 1222592920U, 743U, 3466726667U },\n\t{  358288986U, 695U, 1091956998U },\n\t{ 1935056945U, 958U,  514864477U },\n\t{  735675993U, 990U, 1294239989U },\n\t{ 1560089402U, 897U, 2238551287U },\n\t{   70616361U, 829U,   22483098U },\n\t{  368234700U, 731U, 2913875084U },\n\t{   20221190U, 879U, 1564152970U },\n\t{  539444654U, 682U, 1835141259U },\n\t{ 1314987297U, 840U, 1801114136U },\n\t{ 2019295544U, 645U, 3286438930U },\n\t{  469023838U, 716U, 1637918202U },\n\t{ 1843754496U, 653U, 2562092152U },\n\t{  400672036U, 809U, 4264212785U },\n\t{  404722249U, 965U, 2704116999U },\n\t{  600702209U, 758U,  584979986U },\n\t{  519953954U, 667U, 2574436237U },\n\t{ 1658071126U, 694U, 2214569490U },\n\t{  420480037U, 749U, 3430010866U },\n\t{  690103647U, 969U, 3700758083U },\n\t{ 1029424799U, 937U, 3787746841U },\n\t{ 2012608669U, 506U, 3362628973U },\n\t{ 1535432887U, 998U,   42610943U },\n\t{ 1330635533U, 857U, 3040806504U },\n\t{ 1223800550U, 539U, 3954229517U },\n\t{ 1322411537U, 680U, 3223250324U },\n\t{ 1877847898U, 945U, 2915147143U },\n\t{ 1646356099U, 874U,  965988280U },\n\t{  805687536U, 744U, 4032277920U },\n\t{ 1948093210U, 633U, 1346597684U },\n\t{  392609744U, 783U, 1636083295U },\n\t{  690241304U, 770U, 1201031298U },\n\t{ 1360302965U, 696U, 1665394461U },\n\t{ 1220090946U, 780U, 1316922812U },\n\t{  447092251U, 500U, 3438743375U },\n\t{ 1613868791U, 592U,  828546883U },\n\t{  523430951U, 548U, 2552392304U },\n\t{  726692899U, 810U, 1656872867U },\n\t{ 1364340021U, 836U, 3710513486U },\n\t{ 1986257729U, 931U,  935013962U },\n\t{  407983964U, 921U,  728767059U },\n};\n\nstatic void __init prandom_state_selftest(void)\n{\n\tint i, j, errors = 0, runs = 0;\n\tbool error = false;\n\n\tfor (i = 0; i < ARRAY_SIZE(test1); i++) {\n\t\tstruct rnd_state state;\n\n\t\tprandom_seed_early(&state, test1[i].seed, false);\n\t\tprandom_warmup(&state);\n\n\t\tif (test1[i].result != prandom_u32_state(&state))\n\t\t\terror = true;\n\t}\n\n\tif (error)\n\t\tpr_warn(\"prandom: seed boundary self test failed\\n\");\n\telse\n\t\tpr_info(\"prandom: seed boundary self test passed\\n\");\n\n\tfor (i = 0; i < ARRAY_SIZE(test2); i++) {\n\t\tstruct rnd_state state;\n\n\t\tprandom_seed_early(&state, test2[i].seed, false);\n\t\tprandom_warmup(&state);\n\n\t\tfor (j = 0; j < test2[i].iteration - 1; j++)\n\t\t\tprandom_u32_state(&state);\n\n\t\tif (test2[i].result != prandom_u32_state(&state))\n\t\t\terrors++;\n\n\t\truns++;\n\t\tcond_resched();\n\t}\n\n\tif (errors)\n\t\tpr_warn(\"prandom: %d/%d self tests failed\\n\", errors, runs);\n\telse\n\t\tpr_info(\"prandom: %d self tests passed\\n\", runs);\n}\n#endif\n"], "filenames": ["drivers/char/random.c", "include/linux/random.h", "kernel/time/timer.c", "lib/random32.c"], "buggy_code_start_loc": [1279, 13, 45, 51], "buggy_code_end_loc": [1279, 121, 1744, 52], "fixing_code_start_loc": [1280, 14, 46, 51], "fixing_code_end_loc": [1281, 125, 1753, 52], "type": "CWE-330", "message": "The Linux kernel through 5.7.11 allows remote attackers to make observations that help to obtain sensitive information about the internal state of the network RNG, aka CID-f227e3ec3b5c. This is related to drivers/char/random.c and kernel/time/timer.c.", "other": {"cve": {"id": "CVE-2020-16166", "sourceIdentifier": "cve@mitre.org", "published": "2020-07-30T21:15:11.533", "lastModified": "2022-04-26T17:06:14.463", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The Linux kernel through 5.7.11 allows remote attackers to make observations that help to obtain sensitive information about the internal state of the network RNG, aka CID-f227e3ec3b5c. This is related to drivers/char/random.c and kernel/time/timer.c."}, {"lang": "es", "value": "El kernel de Linux versiones hasta 5.7.11, permite a atacantes remotos realizar observaciones que ayudan a obtener informaci\u00f3n confidencial sobre el estado interno de la red RNG, tambi\u00e9n se conoce como CID-f227e3ec3b5c. Esto est\u00e1 relacionado con los archivos drivers/char/random.c y kernel/time/timer.c"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 3.7, "baseSeverity": "LOW"}, "exploitabilityScore": 2.2, "impactScore": 1.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:N/A:N", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-330"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "5.7.11", "matchCriteriaId": "F999DECE-5A01-4DCC-9FE0-EFA5001598EE"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.1:*:*:*:*:*:*:*", "matchCriteriaId": "B620311B-34A3-48A6-82DF-6F078D7A4493"}, {"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.2:*:*:*:*:*:*:*", "matchCriteriaId": "B009C22E-30A4-4288-BCF6-C3E81DEAF45A"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:31:*:*:*:*:*:*:*", "matchCriteriaId": "80F0FA5D-8D3B-4C0E-81E2-87998286AF33"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:32:*:*:*:*:*:*:*", "matchCriteriaId": "36D96259-24BD-44E2-96D9-78CE1D41F956"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:esm:*:*:*", "matchCriteriaId": "815D70A8-47D3-459C-A32C-9FEACA0659D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:20.04:*:*:*:lts:*:*:*", "matchCriteriaId": "902B8056-9E37-443B-8905-8AA93E2447FB"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:active_iq_unified_manager:*:*:*:*:*:vmware_vsphere:*:*", "versionStartIncluding": "9.5", "matchCriteriaId": "0CB28AF5-5AF0-4475-A7B6-12E1795FFDCB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:cloud_volumes_ontap_mediator:-:*:*:*:*:*:*:*", "matchCriteriaId": "280AA828-6FA9-4260-8EC1-019423B966E1"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:e-series_santricity_os_controller:*:*:*:*:*:*:*:*", "versionStartIncluding": "11.0.0", "versionEndIncluding": "11.60.3", "matchCriteriaId": "BD1E9594-C46F-40D1-8BC2-6B16635B55C4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:hci_bootstrap_os:-:*:*:*:*:*:*:*", "matchCriteriaId": "84574629-EB00-4235-8962-45070F3C9F6A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:hci_management_node:-:*:*:*:*:*:*:*", "matchCriteriaId": "A3C19813-E823-456A-B1CE-EC0684CE1953"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:solidfire:-:*:*:*:*:*:*:*", "matchCriteriaId": "A6E9EF0C-AFA8-4F7B-9FDC-1E0F7C26E737"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:steelstore_cloud_integrated_storage:-:*:*:*:*:*:*:*", "matchCriteriaId": "E94F7F59-1785-493F-91A7-5F5EA5E87E4D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:storagegrid:*:*:*:*:*:*:*:*", "versionEndIncluding": "9.0.4", "matchCriteriaId": "73019FE2-F7CE-4B12-9DC1-8333F08A7D9C"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410c_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "234DEFE0-5CE5-4B0A-96B8-5D227CB8ED31"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410c:-:*:*:*:*:*:*:*", "matchCriteriaId": "CDDF61B7-EC5C-467C-B710-B89F502CD04F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:oracle:sd-wan_edge:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "78C99571-0F3C-43E6-84B3-7D80E045EF8E"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2020-08/msg00009.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2020-08/msg00047.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://arxiv.org/pdf/2012.07432.pdf", "source": "cve@mitre.org", "tags": ["Technical Description", "Third Party Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=f227e3ec3b5cad859ad15666874405e8c1bbc1d4", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=c51f8f88d705e06bd696d7510aff22b33eb8e638", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/f227e3ec3b5cad859ad15666874405e8c1bbc1d4", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2020/09/msg00025.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2020/10/msg00032.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2020/10/msg00034.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/AAPTLPAEKVAJYJ4LHN7VH4CN2W75R2YW/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/MFBCLQWJI5I4G25TVJNLXLAXJ4MERQNW/", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20200814-0004/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4525-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4526-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.oracle.com/security-alerts/cpuApr2021.html", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/f227e3ec3b5cad859ad15666874405e8c1bbc1d4"}}