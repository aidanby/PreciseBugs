{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * SN Platform GRU Driver\n *\n *              FAULT HANDLER FOR GRU DETECTED TLB MISSES\n *\n * This file contains code that handles TLB misses within the GRU.\n * These misses are reported either via interrupts or user polling of\n * the user CB.\n *\n *  Copyright (c) 2008 Silicon Graphics, Inc.  All Rights Reserved.\n */\n\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/spinlock.h>\n#include <linux/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/device.h>\n#include <linux/io.h>\n#include <linux/uaccess.h>\n#include <linux/security.h>\n#include <linux/sync_core.h>\n#include <linux/prefetch.h>\n#include \"gru.h\"\n#include \"grutables.h\"\n#include \"grulib.h\"\n#include \"gru_instructions.h\"\n#include <asm/uv/uv_hub.h>\n\n/* Return codes for vtop functions */\n#define VTOP_SUCCESS               0\n#define VTOP_INVALID               -1\n#define VTOP_RETRY                 -2\n\n\n/*\n * Test if a physical address is a valid GRU GSEG address\n */\nstatic inline int is_gru_paddr(unsigned long paddr)\n{\n\treturn paddr >= gru_start_paddr && paddr < gru_end_paddr;\n}\n\n/*\n * Find the vma of a GRU segment. Caller must hold mmap_lock.\n */\nstruct vm_area_struct *gru_find_vma(unsigned long vaddr)\n{\n\tstruct vm_area_struct *vma;\n\n\tvma = vma_lookup(current->mm, vaddr);\n\tif (vma && vma->vm_ops == &gru_vm_ops)\n\t\treturn vma;\n\treturn NULL;\n}\n\n/*\n * Find and lock the gts that contains the specified user vaddr.\n *\n * Returns:\n * \t- *gts with the mmap_lock locked for read and the GTS locked.\n *\t- NULL if vaddr invalid OR is not a valid GSEG vaddr.\n */\n\nstatic struct gru_thread_state *gru_find_lock_gts(unsigned long vaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tstruct gru_thread_state *gts = NULL;\n\n\tmmap_read_lock(mm);\n\tvma = gru_find_vma(vaddr);\n\tif (vma)\n\t\tgts = gru_find_thread_state(vma, TSID(vaddr, vma));\n\tif (gts)\n\t\tmutex_lock(&gts->ts_ctxlock);\n\telse\n\t\tmmap_read_unlock(mm);\n\treturn gts;\n}\n\nstatic struct gru_thread_state *gru_alloc_locked_gts(unsigned long vaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tstruct gru_thread_state *gts = ERR_PTR(-EINVAL);\n\n\tmmap_write_lock(mm);\n\tvma = gru_find_vma(vaddr);\n\tif (!vma)\n\t\tgoto err;\n\n\tgts = gru_alloc_thread_state(vma, TSID(vaddr, vma));\n\tif (IS_ERR(gts))\n\t\tgoto err;\n\tmutex_lock(&gts->ts_ctxlock);\n\tmmap_write_downgrade(mm);\n\treturn gts;\n\nerr:\n\tmmap_write_unlock(mm);\n\treturn gts;\n}\n\n/*\n * Unlock a GTS that was previously locked with gru_find_lock_gts().\n */\nstatic void gru_unlock_gts(struct gru_thread_state *gts)\n{\n\tmutex_unlock(&gts->ts_ctxlock);\n\tmmap_read_unlock(current->mm);\n}\n\n/*\n * Set a CB.istatus to active using a user virtual address. This must be done\n * just prior to a TFH RESTART. The new cb.istatus is an in-cache status ONLY.\n * If the line is evicted, the status may be lost. The in-cache update\n * is necessary to prevent the user from seeing a stale cb.istatus that will\n * change as soon as the TFH restart is complete. Races may cause an\n * occasional failure to clear the cb.istatus, but that is ok.\n */\nstatic void gru_cb_set_istatus_active(struct gru_instruction_bits *cbk)\n{\n\tif (cbk) {\n\t\tcbk->istatus = CBS_ACTIVE;\n\t}\n}\n\n/*\n * Read & clear a TFM\n *\n * The GRU has an array of fault maps. A map is private to a cpu\n * Only one cpu will be accessing a cpu's fault map.\n *\n * This function scans the cpu-private fault map & clears all bits that\n * are set. The function returns a bitmap that indicates the bits that\n * were cleared. Note that sense the maps may be updated asynchronously by\n * the GRU, atomic operations must be used to clear bits.\n */\nstatic void get_clear_fault_map(struct gru_state *gru,\n\t\t\t\tstruct gru_tlb_fault_map *imap,\n\t\t\t\tstruct gru_tlb_fault_map *dmap)\n{\n\tunsigned long i, k;\n\tstruct gru_tlb_fault_map *tfm;\n\n\ttfm = get_tfm_for_cpu(gru, gru_cpu_fault_map_id());\n\tprefetchw(tfm);\t\t/* Helps on hardware, required for emulator */\n\tfor (i = 0; i < BITS_TO_LONGS(GRU_NUM_CBE); i++) {\n\t\tk = tfm->fault_bits[i];\n\t\tif (k)\n\t\t\tk = xchg(&tfm->fault_bits[i], 0UL);\n\t\timap->fault_bits[i] = k;\n\t\tk = tfm->done_bits[i];\n\t\tif (k)\n\t\t\tk = xchg(&tfm->done_bits[i], 0UL);\n\t\tdmap->fault_bits[i] = k;\n\t}\n\n\t/*\n\t * Not functionally required but helps performance. (Required\n\t * on emulator)\n\t */\n\tgru_flush_cache(tfm);\n}\n\n/*\n * Atomic (interrupt context) & non-atomic (user context) functions to\n * convert a vaddr into a physical address. The size of the page\n * is returned in pageshift.\n * \treturns:\n * \t\t  0 - successful\n * \t\t< 0 - error code\n * \t\t  1 - (atomic only) try again in non-atomic context\n */\nstatic int non_atomic_pte_lookup(struct vm_area_struct *vma,\n\t\t\t\t unsigned long vaddr, int write,\n\t\t\t\t unsigned long *paddr, int *pageshift)\n{\n\tstruct page *page;\n\n#ifdef CONFIG_HUGETLB_PAGE\n\t*pageshift = is_vm_hugetlb_page(vma) ? HPAGE_SHIFT : PAGE_SHIFT;\n#else\n\t*pageshift = PAGE_SHIFT;\n#endif\n\tif (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0, &page, NULL) <= 0)\n\t\treturn -EFAULT;\n\t*paddr = page_to_phys(page);\n\tput_page(page);\n\treturn 0;\n}\n\n/*\n * atomic_pte_lookup\n *\n * Convert a user virtual address to a physical address\n * Only supports Intel large pages (2MB only) on x86_64.\n *\tZZZ - hugepage support is incomplete\n *\n * NOTE: mmap_lock is already held on entry to this function. This\n * guarantees existence of the page tables.\n */\nstatic int atomic_pte_lookup(struct vm_area_struct *vma, unsigned long vaddr,\n\tint write, unsigned long *paddr, int *pageshift)\n{\n\tpgd_t *pgdp;\n\tp4d_t *p4dp;\n\tpud_t *pudp;\n\tpmd_t *pmdp;\n\tpte_t pte;\n\n\tpgdp = pgd_offset(vma->vm_mm, vaddr);\n\tif (unlikely(pgd_none(*pgdp)))\n\t\tgoto err;\n\n\tp4dp = p4d_offset(pgdp, vaddr);\n\tif (unlikely(p4d_none(*p4dp)))\n\t\tgoto err;\n\n\tpudp = pud_offset(p4dp, vaddr);\n\tif (unlikely(pud_none(*pudp)))\n\t\tgoto err;\n\n\tpmdp = pmd_offset(pudp, vaddr);\n\tif (unlikely(pmd_none(*pmdp)))\n\t\tgoto err;\n#ifdef CONFIG_X86_64\n\tif (unlikely(pmd_large(*pmdp)))\n\t\tpte = *(pte_t *) pmdp;\n\telse\n#endif\n\t\tpte = *pte_offset_kernel(pmdp, vaddr);\n\n\tif (unlikely(!pte_present(pte) ||\n\t\t     (write && (!pte_write(pte) || !pte_dirty(pte)))))\n\t\treturn 1;\n\n\t*paddr = pte_pfn(pte) << PAGE_SHIFT;\n#ifdef CONFIG_HUGETLB_PAGE\n\t*pageshift = is_vm_hugetlb_page(vma) ? HPAGE_SHIFT : PAGE_SHIFT;\n#else\n\t*pageshift = PAGE_SHIFT;\n#endif\n\treturn 0;\n\nerr:\n\treturn 1;\n}\n\nstatic int gru_vtop(struct gru_thread_state *gts, unsigned long vaddr,\n\t\t    int write, int atomic, unsigned long *gpa, int *pageshift)\n{\n\tstruct mm_struct *mm = gts->ts_mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long paddr;\n\tint ret, ps;\n\n\tvma = find_vma(mm, vaddr);\n\tif (!vma)\n\t\tgoto inval;\n\n\t/*\n\t * Atomic lookup is faster & usually works even if called in non-atomic\n\t * context.\n\t */\n\trmb();\t/* Must/check ms_range_active before loading PTEs */\n\tret = atomic_pte_lookup(vma, vaddr, write, &paddr, &ps);\n\tif (ret) {\n\t\tif (atomic)\n\t\t\tgoto upm;\n\t\tif (non_atomic_pte_lookup(vma, vaddr, write, &paddr, &ps))\n\t\t\tgoto inval;\n\t}\n\tif (is_gru_paddr(paddr))\n\t\tgoto inval;\n\tpaddr = paddr & ~((1UL << ps) - 1);\n\t*gpa = uv_soc_phys_ram_to_gpa(paddr);\n\t*pageshift = ps;\n\treturn VTOP_SUCCESS;\n\ninval:\n\treturn VTOP_INVALID;\nupm:\n\treturn VTOP_RETRY;\n}\n\n\n/*\n * Flush a CBE from cache. The CBE is clean in the cache. Dirty the\n * CBE cacheline so that the line will be written back to home agent.\n * Otherwise the line may be silently dropped. This has no impact\n * except on performance.\n */\nstatic void gru_flush_cache_cbe(struct gru_control_block_extended *cbe)\n{\n\tif (unlikely(cbe)) {\n\t\tcbe->cbrexecstatus = 0;         /* make CL dirty */\n\t\tgru_flush_cache(cbe);\n\t}\n}\n\n/*\n * Preload the TLB with entries that may be required. Currently, preloading\n * is implemented only for BCOPY. Preload  <tlb_preload_count> pages OR to\n * the end of the bcopy tranfer, whichever is smaller.\n */\nstatic void gru_preload_tlb(struct gru_state *gru,\n\t\t\tstruct gru_thread_state *gts, int atomic,\n\t\t\tunsigned long fault_vaddr, int asid, int write,\n\t\t\tunsigned char tlb_preload_count,\n\t\t\tstruct gru_tlb_fault_handle *tfh,\n\t\t\tstruct gru_control_block_extended *cbe)\n{\n\tunsigned long vaddr = 0, gpa;\n\tint ret, pageshift;\n\n\tif (cbe->opccpy != OP_BCOPY)\n\t\treturn;\n\n\tif (fault_vaddr == cbe->cbe_baddr0)\n\t\tvaddr = fault_vaddr + GRU_CACHE_LINE_BYTES * cbe->cbe_src_cl - 1;\n\telse if (fault_vaddr == cbe->cbe_baddr1)\n\t\tvaddr = fault_vaddr + (1 << cbe->xtypecpy) * cbe->cbe_nelemcur - 1;\n\n\tfault_vaddr &= PAGE_MASK;\n\tvaddr &= PAGE_MASK;\n\tvaddr = min(vaddr, fault_vaddr + tlb_preload_count * PAGE_SIZE);\n\n\twhile (vaddr > fault_vaddr) {\n\t\tret = gru_vtop(gts, vaddr, write, atomic, &gpa, &pageshift);\n\t\tif (ret || tfh_write_only(tfh, gpa, GAA_RAM, vaddr, asid, write,\n\t\t\t\t\t  GRU_PAGESIZE(pageshift)))\n\t\t\treturn;\n\t\tgru_dbg(grudev,\n\t\t\t\"%s: gid %d, gts 0x%p, tfh 0x%p, vaddr 0x%lx, asid 0x%x, rw %d, ps %d, gpa 0x%lx\\n\",\n\t\t\tatomic ? \"atomic\" : \"non-atomic\", gru->gs_gid, gts, tfh,\n\t\t\tvaddr, asid, write, pageshift, gpa);\n\t\tvaddr -= PAGE_SIZE;\n\t\tSTAT(tlb_preload_page);\n\t}\n}\n\n/*\n * Drop a TLB entry into the GRU. The fault is described by info in an TFH.\n *\tInput:\n *\t\tcb    Address of user CBR. Null if not running in user context\n * \tReturn:\n * \t\t  0 = dropin, exception, or switch to UPM successful\n * \t\t  1 = range invalidate active\n * \t\t< 0 = error code\n *\n */\nstatic int gru_try_dropin(struct gru_state *gru,\n\t\t\t  struct gru_thread_state *gts,\n\t\t\t  struct gru_tlb_fault_handle *tfh,\n\t\t\t  struct gru_instruction_bits *cbk)\n{\n\tstruct gru_control_block_extended *cbe = NULL;\n\tunsigned char tlb_preload_count = gts->ts_tlb_preload_count;\n\tint pageshift = 0, asid, write, ret, atomic = !cbk, indexway;\n\tunsigned long gpa = 0, vaddr = 0;\n\n\t/*\n\t * NOTE: The GRU contains magic hardware that eliminates races between\n\t * TLB invalidates and TLB dropins. If an invalidate occurs\n\t * in the window between reading the TFH and the subsequent TLB dropin,\n\t * the dropin is ignored. This eliminates the need for additional locks.\n\t */\n\n\t/*\n\t * Prefetch the CBE if doing TLB preloading\n\t */\n\tif (unlikely(tlb_preload_count)) {\n\t\tcbe = gru_tfh_to_cbe(tfh);\n\t\tprefetchw(cbe);\n\t}\n\n\t/*\n\t * Error if TFH state is IDLE or FMM mode & the user issuing a UPM call.\n\t * Might be a hardware race OR a stupid user. Ignore FMM because FMM\n\t * is a transient state.\n\t */\n\tif (tfh->status != TFHSTATUS_EXCEPTION) {\n\t\tgru_flush_cache(tfh);\n\t\tsync_core();\n\t\tif (tfh->status != TFHSTATUS_EXCEPTION)\n\t\t\tgoto failnoexception;\n\t\tSTAT(tfh_stale_on_fault);\n\t}\n\tif (tfh->state == TFHSTATE_IDLE)\n\t\tgoto failidle;\n\tif (tfh->state == TFHSTATE_MISS_FMM && cbk)\n\t\tgoto failfmm;\n\n\twrite = (tfh->cause & TFHCAUSE_TLB_MOD) != 0;\n\tvaddr = tfh->missvaddr;\n\tasid = tfh->missasid;\n\tindexway = tfh->indexway;\n\tif (asid == 0)\n\t\tgoto failnoasid;\n\n\trmb();\t/* TFH must be cache resident before reading ms_range_active */\n\n\t/*\n\t * TFH is cache resident - at least briefly. Fail the dropin\n\t * if a range invalidate is active.\n\t */\n\tif (atomic_read(&gts->ts_gms->ms_range_active))\n\t\tgoto failactive;\n\n\tret = gru_vtop(gts, vaddr, write, atomic, &gpa, &pageshift);\n\tif (ret == VTOP_INVALID)\n\t\tgoto failinval;\n\tif (ret == VTOP_RETRY)\n\t\tgoto failupm;\n\n\tif (!(gts->ts_sizeavail & GRU_SIZEAVAIL(pageshift))) {\n\t\tgts->ts_sizeavail |= GRU_SIZEAVAIL(pageshift);\n\t\tif (atomic || !gru_update_cch(gts)) {\n\t\t\tgts->ts_force_cch_reload = 1;\n\t\t\tgoto failupm;\n\t\t}\n\t}\n\n\tif (unlikely(cbe) && pageshift == PAGE_SHIFT) {\n\t\tgru_preload_tlb(gru, gts, atomic, vaddr, asid, write, tlb_preload_count, tfh, cbe);\n\t\tgru_flush_cache_cbe(cbe);\n\t}\n\n\tgru_cb_set_istatus_active(cbk);\n\tgts->ustats.tlbdropin++;\n\ttfh_write_restart(tfh, gpa, GAA_RAM, vaddr, asid, write,\n\t\t\t  GRU_PAGESIZE(pageshift));\n\tgru_dbg(grudev,\n\t\t\"%s: gid %d, gts 0x%p, tfh 0x%p, vaddr 0x%lx, asid 0x%x, indexway 0x%x,\"\n\t\t\" rw %d, ps %d, gpa 0x%lx\\n\",\n\t\tatomic ? \"atomic\" : \"non-atomic\", gru->gs_gid, gts, tfh, vaddr, asid,\n\t\tindexway, write, pageshift, gpa);\n\tSTAT(tlb_dropin);\n\treturn 0;\n\nfailnoasid:\n\t/* No asid (delayed unload). */\n\tSTAT(tlb_dropin_fail_no_asid);\n\tgru_dbg(grudev, \"FAILED no_asid tfh: 0x%p, vaddr 0x%lx\\n\", tfh, vaddr);\n\tif (!cbk)\n\t\ttfh_user_polling_mode(tfh);\n\telse\n\t\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\treturn -EAGAIN;\n\nfailupm:\n\t/* Atomic failure switch CBR to UPM */\n\ttfh_user_polling_mode(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_upm);\n\tgru_dbg(grudev, \"FAILED upm tfh: 0x%p, vaddr 0x%lx\\n\", tfh, vaddr);\n\treturn 1;\n\nfailfmm:\n\t/* FMM state on UPM call */\n\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_fmm);\n\tgru_dbg(grudev, \"FAILED fmm tfh: 0x%p, state %d\\n\", tfh, tfh->state);\n\treturn 0;\n\nfailnoexception:\n\t/* TFH status did not show exception pending */\n\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tif (cbk)\n\t\tgru_flush_cache(cbk);\n\tSTAT(tlb_dropin_fail_no_exception);\n\tgru_dbg(grudev, \"FAILED non-exception tfh: 0x%p, status %d, state %d\\n\",\n\t\ttfh, tfh->status, tfh->state);\n\treturn 0;\n\nfailidle:\n\t/* TFH state was idle  - no miss pending */\n\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tif (cbk)\n\t\tgru_flush_cache(cbk);\n\tSTAT(tlb_dropin_fail_idle);\n\tgru_dbg(grudev, \"FAILED idle tfh: 0x%p, state %d\\n\", tfh, tfh->state);\n\treturn 0;\n\nfailinval:\n\t/* All errors (atomic & non-atomic) switch CBR to EXCEPTION state */\n\ttfh_exception(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_invalid);\n\tgru_dbg(grudev, \"FAILED inval tfh: 0x%p, vaddr 0x%lx\\n\", tfh, vaddr);\n\treturn -EFAULT;\n\nfailactive:\n\t/* Range invalidate active. Switch to UPM iff atomic */\n\tif (!cbk)\n\t\ttfh_user_polling_mode(tfh);\n\telse\n\t\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_range_active);\n\tgru_dbg(grudev, \"FAILED range active: tfh 0x%p, vaddr 0x%lx\\n\",\n\t\ttfh, vaddr);\n\treturn 1;\n}\n\n/*\n * Process an external interrupt from the GRU. This interrupt is\n * caused by a TLB miss.\n * Note that this is the interrupt handler that is registered with linux\n * interrupt handlers.\n */\nstatic irqreturn_t gru_intr(int chiplet, int blade)\n{\n\tstruct gru_state *gru;\n\tstruct gru_tlb_fault_map imap, dmap;\n\tstruct gru_thread_state *gts;\n\tstruct gru_tlb_fault_handle *tfh = NULL;\n\tstruct completion *cmp;\n\tint cbrnum, ctxnum;\n\n\tSTAT(intr);\n\n\tgru = &gru_base[blade]->bs_grus[chiplet];\n\tif (!gru) {\n\t\tdev_err(grudev, \"GRU: invalid interrupt: cpu %d, chiplet %d\\n\",\n\t\t\traw_smp_processor_id(), chiplet);\n\t\treturn IRQ_NONE;\n\t}\n\tget_clear_fault_map(gru, &imap, &dmap);\n\tgru_dbg(grudev,\n\t\t\"cpu %d, chiplet %d, gid %d, imap %016lx %016lx, dmap %016lx %016lx\\n\",\n\t\tsmp_processor_id(), chiplet, gru->gs_gid,\n\t\timap.fault_bits[0], imap.fault_bits[1],\n\t\tdmap.fault_bits[0], dmap.fault_bits[1]);\n\n\tfor_each_cbr_in_tfm(cbrnum, dmap.fault_bits) {\n\t\tSTAT(intr_cbr);\n\t\tcmp = gru->gs_blade->bs_async_wq;\n\t\tif (cmp)\n\t\t\tcomplete(cmp);\n\t\tgru_dbg(grudev, \"gid %d, cbr_done %d, done %d\\n\",\n\t\t\tgru->gs_gid, cbrnum, cmp ? cmp->done : -1);\n\t}\n\n\tfor_each_cbr_in_tfm(cbrnum, imap.fault_bits) {\n\t\tSTAT(intr_tfh);\n\t\ttfh = get_tfh_by_index(gru, cbrnum);\n\t\tprefetchw(tfh);\t/* Helps on hdw, required for emulator */\n\n\t\t/*\n\t\t * When hardware sets a bit in the faultmap, it implicitly\n\t\t * locks the GRU context so that it cannot be unloaded.\n\t\t * The gts cannot change until a TFH start/writestart command\n\t\t * is issued.\n\t\t */\n\t\tctxnum = tfh->ctxnum;\n\t\tgts = gru->gs_gts[ctxnum];\n\n\t\t/* Spurious interrupts can cause this. Ignore. */\n\t\tif (!gts) {\n\t\t\tSTAT(intr_spurious);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * This is running in interrupt context. Trylock the mmap_lock.\n\t\t * If it fails, retry the fault in user context.\n\t\t */\n\t\tgts->ustats.fmm_tlbmiss++;\n\t\tif (!gts->ts_force_cch_reload &&\n\t\t\t\t\tmmap_read_trylock(gts->ts_mm)) {\n\t\t\tgru_try_dropin(gru, gts, tfh, NULL);\n\t\t\tmmap_read_unlock(gts->ts_mm);\n\t\t} else {\n\t\t\ttfh_user_polling_mode(tfh);\n\t\t\tSTAT(intr_mm_lock_failed);\n\t\t}\n\t}\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t gru0_intr(int irq, void *dev_id)\n{\n\treturn gru_intr(0, uv_numa_blade_id());\n}\n\nirqreturn_t gru1_intr(int irq, void *dev_id)\n{\n\treturn gru_intr(1, uv_numa_blade_id());\n}\n\nirqreturn_t gru_intr_mblade(int irq, void *dev_id)\n{\n\tint blade;\n\n\tfor_each_possible_blade(blade) {\n\t\tif (uv_blade_nr_possible_cpus(blade))\n\t\t\tcontinue;\n\t\tgru_intr(0, blade);\n\t\tgru_intr(1, blade);\n\t}\n\treturn IRQ_HANDLED;\n}\n\n\nstatic int gru_user_dropin(struct gru_thread_state *gts,\n\t\t\t   struct gru_tlb_fault_handle *tfh,\n\t\t\t   void *cb)\n{\n\tstruct gru_mm_struct *gms = gts->ts_gms;\n\tint ret;\n\n\tgts->ustats.upm_tlbmiss++;\n\twhile (1) {\n\t\twait_event(gms->ms_wait_queue,\n\t\t\t   atomic_read(&gms->ms_range_active) == 0);\n\t\tprefetchw(tfh);\t/* Helps on hdw, required for emulator */\n\t\tret = gru_try_dropin(gts->ts_gru, gts, tfh, cb);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\t\tSTAT(call_os_wait_queue);\n\t}\n}\n\n/*\n * This interface is called as a result of a user detecting a \"call OS\" bit\n * in a user CB. Normally means that a TLB fault has occurred.\n * \tcb - user virtual address of the CB\n */\nint gru_handle_user_call_os(unsigned long cb)\n{\n\tstruct gru_tlb_fault_handle *tfh;\n\tstruct gru_thread_state *gts;\n\tvoid *cbk;\n\tint ucbnum, cbrnum, ret = -EINVAL;\n\n\tSTAT(call_os);\n\n\t/* sanity check the cb pointer */\n\tucbnum = get_cb_number((void *)cb);\n\tif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\n\t\treturn -EINVAL;\n\n\tgts = gru_find_lock_gts(cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\n\t\tgoto exit;\n\n\tgru_check_context_placement(gts);\n\n\t/*\n\t * CCH may contain stale data if ts_force_cch_reload is set.\n\t */\n\tif (gts->ts_gru && gts->ts_force_cch_reload) {\n\t\tgts->ts_force_cch_reload = 0;\n\t\tgru_update_cch(gts);\n\t}\n\n\tret = -EAGAIN;\n\tcbrnum = thread_cbr_number(gts, ucbnum);\n\tif (gts->ts_gru) {\n\t\ttfh = get_tfh_by_index(gts->ts_gru, cbrnum);\n\t\tcbk = get_gseg_base_address_cb(gts->ts_gru->gs_gru_base_vaddr,\n\t\t\t\tgts->ts_ctxnum, ucbnum);\n\t\tret = gru_user_dropin(gts, tfh, cbk);\n\t}\nexit:\n\tgru_unlock_gts(gts);\n\treturn ret;\n}\n\n/*\n * Fetch the exception detail information for a CB that terminated with\n * an exception.\n */\nint gru_get_exception_detail(unsigned long arg)\n{\n\tstruct control_block_extended_exc_detail excdet;\n\tstruct gru_control_block_extended *cbe;\n\tstruct gru_thread_state *gts;\n\tint ucbnum, cbrnum, ret;\n\n\tSTAT(user_exception);\n\tif (copy_from_user(&excdet, (void __user *)arg, sizeof(excdet)))\n\t\treturn -EFAULT;\n\n\tgts = gru_find_lock_gts(excdet.cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", excdet.cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\tucbnum = get_cb_number((void *)excdet.cb);\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE) {\n\t\tret = -EINVAL;\n\t} else if (gts->ts_gru) {\n\t\tcbrnum = thread_cbr_number(gts, ucbnum);\n\t\tcbe = get_cbe_by_index(gts->ts_gru, cbrnum);\n\t\tgru_flush_cache(cbe);\t/* CBE not coherent */\n\t\tsync_core();\t\t/* make sure we are have current data */\n\t\texcdet.opc = cbe->opccpy;\n\t\texcdet.exopc = cbe->exopccpy;\n\t\texcdet.ecause = cbe->ecause;\n\t\texcdet.exceptdet0 = cbe->idef1upd;\n\t\texcdet.exceptdet1 = cbe->idef3upd;\n\t\texcdet.cbrstate = cbe->cbrstate;\n\t\texcdet.cbrexecstatus = cbe->cbrexecstatus;\n\t\tgru_flush_cache_cbe(cbe);\n\t\tret = 0;\n\t} else {\n\t\tret = -EAGAIN;\n\t}\n\tgru_unlock_gts(gts);\n\n\tgru_dbg(grudev,\n\t\t\"cb 0x%lx, op %d, exopc %d, cbrstate %d, cbrexecstatus 0x%x, ecause 0x%x, \"\n\t\t\"exdet0 0x%lx, exdet1 0x%x\\n\",\n\t\texcdet.cb, excdet.opc, excdet.exopc, excdet.cbrstate, excdet.cbrexecstatus,\n\t\texcdet.ecause, excdet.exceptdet0, excdet.exceptdet1);\n\tif (!ret && copy_to_user((void __user *)arg, &excdet, sizeof(excdet)))\n\t\tret = -EFAULT;\n\treturn ret;\n}\n\n/*\n * User request to unload a context. Content is saved for possible reload.\n */\nstatic int gru_unload_all_contexts(void)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_state *gru;\n\tint gid, ctxnum;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tforeach_gid(gid) {\n\t\tgru = GID_TO_GRU(gid);\n\t\tspin_lock(&gru->gs_lock);\n\t\tfor (ctxnum = 0; ctxnum < GRU_NUM_CCH; ctxnum++) {\n\t\t\tgts = gru->gs_gts[ctxnum];\n\t\t\tif (gts && mutex_trylock(&gts->ts_ctxlock)) {\n\t\t\t\tspin_unlock(&gru->gs_lock);\n\t\t\t\tgru_unload_context(gts, 1);\n\t\t\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\t\t\tspin_lock(&gru->gs_lock);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&gru->gs_lock);\n\t}\n\treturn 0;\n}\n\nint gru_user_unload_context(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_unload_context_req req;\n\n\tSTAT(user_unload_context);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\n\tgru_dbg(grudev, \"gseg 0x%lx\\n\", req.gseg);\n\n\tif (!req.gseg)\n\t\treturn gru_unload_all_contexts();\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts)\n\t\treturn -EINVAL;\n\n\tif (gts->ts_gru)\n\t\tgru_unload_context(gts, 1);\n\tgru_unlock_gts(gts);\n\n\treturn 0;\n}\n\n/*\n * User request to flush a range of virtual addresses from the GRU TLB\n * (Mainly for testing).\n */\nint gru_user_flush_tlb(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_flush_tlb_req req;\n\tstruct gru_mm_struct *gms;\n\n\tSTAT(user_flush_tlb);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\n\tgru_dbg(grudev, \"gseg 0x%lx, vaddr 0x%lx, len 0x%lx\\n\", req.gseg,\n\t\treq.vaddr, req.len);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts)\n\t\treturn -EINVAL;\n\n\tgms = gts->ts_gms;\n\tgru_unlock_gts(gts);\n\tgru_flush_tlb_range(gms, req.vaddr, req.len);\n\n\treturn 0;\n}\n\n/*\n * Fetch GSEG statisticss\n */\nlong gru_get_gseg_statistics(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_get_gseg_statistics_req req;\n\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * The library creates arrays of contexts for threaded programs.\n\t * If no gts exists in the array, the context has never been used & all\n\t * statistics are implicitly 0.\n\t */\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (gts) {\n\t\tmemcpy(&req.stats, &gts->ustats, sizeof(gts->ustats));\n\t\tgru_unlock_gts(gts);\n\t} else {\n\t\tmemset(&req.stats, 0, sizeof(gts->ustats));\n\t}\n\n\tif (copy_to_user((void __user *)arg, &req, sizeof(req)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n/*\n * Register the current task as the user of the GSEG slice.\n * Needed for TLB fault interrupt targeting.\n */\nint gru_set_context_option(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_set_context_option_req req;\n\tint ret = 0;\n\n\tSTAT(set_context_option);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\tgru_dbg(grudev, \"op %d, gseg 0x%lx, value1 0x%lx\\n\", req.op, req.gseg, req.val1);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts) {\n\t\tgts = gru_alloc_locked_gts(req.gseg);\n\t\tif (IS_ERR(gts))\n\t\t\treturn PTR_ERR(gts);\n\t}\n\n\tswitch (req.op) {\n\tcase sco_blade_chiplet:\n\t\t/* Select blade/chiplet for GRU context */\n\t\tif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\n\t\t    req.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\n\t\t    (req.val1 >= 0 && !gru_base[req.val1])) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tgts->ts_user_blade_id = req.val1;\n\t\t\tgts->ts_user_chiplet_id = req.val0;\n\t\t\tgru_check_context_placement(gts);\n\t\t}\n\t\tbreak;\n\tcase sco_gseg_owner:\n \t\t/* Register the current task as the GSEG owner */\n\t\tgts->ts_tgid_owner = current->tgid;\n\t\tbreak;\n\tcase sco_cch_req_slice:\n \t\t/* Set the CCH slice option */\n\t\tgts->ts_cch_req_slice = req.val1 & 3;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tgru_unlock_gts(gts);\n\n\treturn ret;\n}\n", "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * SN Platform GRU Driver\n *\n *            DRIVER TABLE MANAGER + GRU CONTEXT LOAD/UNLOAD\n *\n *  Copyright (c) 2008 Silicon Graphics, Inc.  All Rights Reserved.\n */\n\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/spinlock.h>\n#include <linux/sched.h>\n#include <linux/device.h>\n#include <linux/list.h>\n#include <linux/err.h>\n#include <linux/prefetch.h>\n#include <asm/uv/uv_hub.h>\n#include \"gru.h\"\n#include \"grutables.h\"\n#include \"gruhandles.h\"\n\nunsigned long gru_options __read_mostly;\n\nstatic struct device_driver gru_driver = {\n\t.name = \"gru\"\n};\n\nstatic struct device gru_device = {\n\t.init_name = \"\",\n\t.driver = &gru_driver,\n};\n\nstruct device *grudev = &gru_device;\n\n/*\n * Select a gru fault map to be used by the current cpu. Note that\n * multiple cpus may be using the same map.\n *\tZZZ should be inline but did not work on emulator\n */\nint gru_cpu_fault_map_id(void)\n{\n#ifdef CONFIG_IA64\n\treturn uv_blade_processor_id() % GRU_NUM_TFM;\n#else\n\tint cpu = smp_processor_id();\n\tint id, core;\n\n\tcore = uv_cpu_core_number(cpu);\n\tid = core + UV_MAX_INT_CORES * uv_cpu_socket_number(cpu);\n\treturn id;\n#endif\n}\n\n/*--------- ASID Management -------------------------------------------\n *\n *  Initially, assign asids sequentially from MIN_ASID .. MAX_ASID.\n *  Once MAX is reached, flush the TLB & start over. However,\n *  some asids may still be in use. There won't be many (percentage wise) still\n *  in use. Search active contexts & determine the value of the first\n *  asid in use (\"x\"s below). Set \"limit\" to this value.\n *  This defines a block of assignable asids.\n *\n *  When \"limit\" is reached, search forward from limit+1 and determine the\n *  next block of assignable asids.\n *\n *  Repeat until MAX_ASID is reached, then start over again.\n *\n *  Each time MAX_ASID is reached, increment the asid generation. Since\n *  the search for in-use asids only checks contexts with GRUs currently\n *  assigned, asids in some contexts will be missed. Prior to loading\n *  a context, the asid generation of the GTS asid is rechecked. If it\n *  doesn't match the current generation, a new asid will be assigned.\n *\n *   \t0---------------x------------x---------------------x----|\n *\t  ^-next\t^-limit\t   \t\t\t\t^-MAX_ASID\n *\n * All asid manipulation & context loading/unloading is protected by the\n * gs_lock.\n */\n\n/* Hit the asid limit. Start over */\nstatic int gru_wrap_asid(struct gru_state *gru)\n{\n\tgru_dbg(grudev, \"gid %d\\n\", gru->gs_gid);\n\tSTAT(asid_wrap);\n\tgru->gs_asid_gen++;\n\treturn MIN_ASID;\n}\n\n/* Find the next chunk of unused asids */\nstatic int gru_reset_asid_limit(struct gru_state *gru, int asid)\n{\n\tint i, gid, inuse_asid, limit;\n\n\tgru_dbg(grudev, \"gid %d, asid 0x%x\\n\", gru->gs_gid, asid);\n\tSTAT(asid_next);\n\tlimit = MAX_ASID;\n\tif (asid >= limit)\n\t\tasid = gru_wrap_asid(gru);\n\tgru_flush_all_tlb(gru);\n\tgid = gru->gs_gid;\nagain:\n\tfor (i = 0; i < GRU_NUM_CCH; i++) {\n\t\tif (!gru->gs_gts[i] || is_kernel_context(gru->gs_gts[i]))\n\t\t\tcontinue;\n\t\tinuse_asid = gru->gs_gts[i]->ts_gms->ms_asids[gid].mt_asid;\n\t\tgru_dbg(grudev, \"gid %d, gts %p, gms %p, inuse 0x%x, cxt %d\\n\",\n\t\t\tgru->gs_gid, gru->gs_gts[i], gru->gs_gts[i]->ts_gms,\n\t\t\tinuse_asid, i);\n\t\tif (inuse_asid == asid) {\n\t\t\tasid += ASID_INC;\n\t\t\tif (asid >= limit) {\n\t\t\t\t/*\n\t\t\t\t * empty range: reset the range limit and\n\t\t\t\t * start over\n\t\t\t\t */\n\t\t\t\tlimit = MAX_ASID;\n\t\t\t\tif (asid >= MAX_ASID)\n\t\t\t\t\tasid = gru_wrap_asid(gru);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\n\t\tif ((inuse_asid > asid) && (inuse_asid < limit))\n\t\t\tlimit = inuse_asid;\n\t}\n\tgru->gs_asid_limit = limit;\n\tgru->gs_asid = asid;\n\tgru_dbg(grudev, \"gid %d, new asid 0x%x, new_limit 0x%x\\n\", gru->gs_gid,\n\t\t\t\t\tasid, limit);\n\treturn asid;\n}\n\n/* Assign a new ASID to a thread context.  */\nstatic int gru_assign_asid(struct gru_state *gru)\n{\n\tint asid;\n\n\tgru->gs_asid += ASID_INC;\n\tasid = gru->gs_asid;\n\tif (asid >= gru->gs_asid_limit)\n\t\tasid = gru_reset_asid_limit(gru, asid);\n\n\tgru_dbg(grudev, \"gid %d, asid 0x%x\\n\", gru->gs_gid, asid);\n\treturn asid;\n}\n\n/*\n * Clear n bits in a word. Return a word indicating the bits that were cleared.\n * Optionally, build an array of chars that contain the bit numbers allocated.\n */\nstatic unsigned long reserve_resources(unsigned long *p, int n, int mmax,\n\t\t\t\t       signed char *idx)\n{\n\tunsigned long bits = 0;\n\tint i;\n\n\twhile (n--) {\n\t\ti = find_first_bit(p, mmax);\n\t\tif (i == mmax)\n\t\t\tBUG();\n\t\t__clear_bit(i, p);\n\t\t__set_bit(i, &bits);\n\t\tif (idx)\n\t\t\t*idx++ = i;\n\t}\n\treturn bits;\n}\n\nunsigned long gru_reserve_cb_resources(struct gru_state *gru, int cbr_au_count,\n\t\t\t\t       signed char *cbmap)\n{\n\treturn reserve_resources(&gru->gs_cbr_map, cbr_au_count, GRU_CBR_AU,\n\t\t\t\t cbmap);\n}\n\nunsigned long gru_reserve_ds_resources(struct gru_state *gru, int dsr_au_count,\n\t\t\t\t       signed char *dsmap)\n{\n\treturn reserve_resources(&gru->gs_dsr_map, dsr_au_count, GRU_DSR_AU,\n\t\t\t\t dsmap);\n}\n\nstatic void reserve_gru_resources(struct gru_state *gru,\n\t\t\t\t  struct gru_thread_state *gts)\n{\n\tgru->gs_active_contexts++;\n\tgts->ts_cbr_map =\n\t    gru_reserve_cb_resources(gru, gts->ts_cbr_au_count,\n\t\t\t\t     gts->ts_cbr_idx);\n\tgts->ts_dsr_map =\n\t    gru_reserve_ds_resources(gru, gts->ts_dsr_au_count, NULL);\n}\n\nstatic void free_gru_resources(struct gru_state *gru,\n\t\t\t       struct gru_thread_state *gts)\n{\n\tgru->gs_active_contexts--;\n\tgru->gs_cbr_map |= gts->ts_cbr_map;\n\tgru->gs_dsr_map |= gts->ts_dsr_map;\n}\n\n/*\n * Check if a GRU has sufficient free resources to satisfy an allocation\n * request. Note: GRU locks may or may not be held when this is called. If\n * not held, recheck after acquiring the appropriate locks.\n *\n * Returns 1 if sufficient resources, 0 if not\n */\nstatic int check_gru_resources(struct gru_state *gru, int cbr_au_count,\n\t\t\t       int dsr_au_count, int max_active_contexts)\n{\n\treturn hweight64(gru->gs_cbr_map) >= cbr_au_count\n\t\t&& hweight64(gru->gs_dsr_map) >= dsr_au_count\n\t\t&& gru->gs_active_contexts < max_active_contexts;\n}\n\n/*\n * TLB manangment requires tracking all GRU chiplets that have loaded a GSEG\n * context.\n */\nstatic int gru_load_mm_tracker(struct gru_state *gru,\n\t\t\t\t\tstruct gru_thread_state *gts)\n{\n\tstruct gru_mm_struct *gms = gts->ts_gms;\n\tstruct gru_mm_tracker *asids = &gms->ms_asids[gru->gs_gid];\n\tunsigned short ctxbitmap = (1 << gts->ts_ctxnum);\n\tint asid;\n\n\tspin_lock(&gms->ms_asid_lock);\n\tasid = asids->mt_asid;\n\n\tspin_lock(&gru->gs_asid_lock);\n\tif (asid == 0 || (asids->mt_ctxbitmap == 0 && asids->mt_asid_gen !=\n\t\t\t  gru->gs_asid_gen)) {\n\t\tasid = gru_assign_asid(gru);\n\t\tasids->mt_asid = asid;\n\t\tasids->mt_asid_gen = gru->gs_asid_gen;\n\t\tSTAT(asid_new);\n\t} else {\n\t\tSTAT(asid_reuse);\n\t}\n\tspin_unlock(&gru->gs_asid_lock);\n\n\tBUG_ON(asids->mt_ctxbitmap & ctxbitmap);\n\tasids->mt_ctxbitmap |= ctxbitmap;\n\tif (!test_bit(gru->gs_gid, gms->ms_asidmap))\n\t\t__set_bit(gru->gs_gid, gms->ms_asidmap);\n\tspin_unlock(&gms->ms_asid_lock);\n\n\tgru_dbg(grudev,\n\t\t\"gid %d, gts %p, gms %p, ctxnum %d, asid 0x%x, asidmap 0x%lx\\n\",\n\t\tgru->gs_gid, gts, gms, gts->ts_ctxnum, asid,\n\t\tgms->ms_asidmap[0]);\n\treturn asid;\n}\n\nstatic void gru_unload_mm_tracker(struct gru_state *gru,\n\t\t\t\t\tstruct gru_thread_state *gts)\n{\n\tstruct gru_mm_struct *gms = gts->ts_gms;\n\tstruct gru_mm_tracker *asids;\n\tunsigned short ctxbitmap;\n\n\tasids = &gms->ms_asids[gru->gs_gid];\n\tctxbitmap = (1 << gts->ts_ctxnum);\n\tspin_lock(&gms->ms_asid_lock);\n\tspin_lock(&gru->gs_asid_lock);\n\tBUG_ON((asids->mt_ctxbitmap & ctxbitmap) != ctxbitmap);\n\tasids->mt_ctxbitmap ^= ctxbitmap;\n\tgru_dbg(grudev, \"gid %d, gts %p, gms %p, ctxnum %d, asidmap 0x%lx\\n\",\n\t\tgru->gs_gid, gts, gms, gts->ts_ctxnum, gms->ms_asidmap[0]);\n\tspin_unlock(&gru->gs_asid_lock);\n\tspin_unlock(&gms->ms_asid_lock);\n}\n\n/*\n * Decrement the reference count on a GTS structure. Free the structure\n * if the reference count goes to zero.\n */\nvoid gts_drop(struct gru_thread_state *gts)\n{\n\tif (gts && refcount_dec_and_test(&gts->ts_refcnt)) {\n\t\tif (gts->ts_gms)\n\t\t\tgru_drop_mmu_notifier(gts->ts_gms);\n\t\tkfree(gts);\n\t\tSTAT(gts_free);\n\t}\n}\n\n/*\n * Locate the GTS structure for the current thread.\n */\nstatic struct gru_thread_state *gru_find_current_gts_nolock(struct gru_vma_data\n\t\t\t    *vdata, int tsid)\n{\n\tstruct gru_thread_state *gts;\n\n\tlist_for_each_entry(gts, &vdata->vd_head, ts_next)\n\t    if (gts->ts_tsid == tsid)\n\t\treturn gts;\n\treturn NULL;\n}\n\n/*\n * Allocate a thread state structure.\n */\nstruct gru_thread_state *gru_alloc_gts(struct vm_area_struct *vma,\n\t\tint cbr_au_count, int dsr_au_count,\n\t\tunsigned char tlb_preload_count, int options, int tsid)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_mm_struct *gms;\n\tint bytes;\n\n\tbytes = DSR_BYTES(dsr_au_count) + CBR_BYTES(cbr_au_count);\n\tbytes += sizeof(struct gru_thread_state);\n\tgts = kmalloc(bytes, GFP_KERNEL);\n\tif (!gts)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tSTAT(gts_alloc);\n\tmemset(gts, 0, sizeof(struct gru_thread_state)); /* zero out header */\n\trefcount_set(&gts->ts_refcnt, 1);\n\tmutex_init(&gts->ts_ctxlock);\n\tgts->ts_cbr_au_count = cbr_au_count;\n\tgts->ts_dsr_au_count = dsr_au_count;\n\tgts->ts_tlb_preload_count = tlb_preload_count;\n\tgts->ts_user_options = options;\n\tgts->ts_user_blade_id = -1;\n\tgts->ts_user_chiplet_id = -1;\n\tgts->ts_tsid = tsid;\n\tgts->ts_ctxnum = NULLCTX;\n\tgts->ts_tlb_int_select = -1;\n\tgts->ts_cch_req_slice = -1;\n\tgts->ts_sizeavail = GRU_SIZEAVAIL(PAGE_SHIFT);\n\tif (vma) {\n\t\tgts->ts_mm = current->mm;\n\t\tgts->ts_vma = vma;\n\t\tgms = gru_register_mmu_notifier();\n\t\tif (IS_ERR(gms))\n\t\t\tgoto err;\n\t\tgts->ts_gms = gms;\n\t}\n\n\tgru_dbg(grudev, \"alloc gts %p\\n\", gts);\n\treturn gts;\n\nerr:\n\tgts_drop(gts);\n\treturn ERR_CAST(gms);\n}\n\n/*\n * Allocate a vma private data structure.\n */\nstruct gru_vma_data *gru_alloc_vma_data(struct vm_area_struct *vma, int tsid)\n{\n\tstruct gru_vma_data *vdata = NULL;\n\n\tvdata = kmalloc(sizeof(*vdata), GFP_KERNEL);\n\tif (!vdata)\n\t\treturn NULL;\n\n\tSTAT(vdata_alloc);\n\tINIT_LIST_HEAD(&vdata->vd_head);\n\tspin_lock_init(&vdata->vd_lock);\n\tgru_dbg(grudev, \"alloc vdata %p\\n\", vdata);\n\treturn vdata;\n}\n\n/*\n * Find the thread state structure for the current thread.\n */\nstruct gru_thread_state *gru_find_thread_state(struct vm_area_struct *vma,\n\t\t\t\t\tint tsid)\n{\n\tstruct gru_vma_data *vdata = vma->vm_private_data;\n\tstruct gru_thread_state *gts;\n\n\tspin_lock(&vdata->vd_lock);\n\tgts = gru_find_current_gts_nolock(vdata, tsid);\n\tspin_unlock(&vdata->vd_lock);\n\tgru_dbg(grudev, \"vma %p, gts %p\\n\", vma, gts);\n\treturn gts;\n}\n\n/*\n * Allocate a new thread state for a GSEG. Note that races may allow\n * another thread to race to create a gts.\n */\nstruct gru_thread_state *gru_alloc_thread_state(struct vm_area_struct *vma,\n\t\t\t\t\tint tsid)\n{\n\tstruct gru_vma_data *vdata = vma->vm_private_data;\n\tstruct gru_thread_state *gts, *ngts;\n\n\tgts = gru_alloc_gts(vma, vdata->vd_cbr_au_count,\n\t\t\t    vdata->vd_dsr_au_count,\n\t\t\t    vdata->vd_tlb_preload_count,\n\t\t\t    vdata->vd_user_options, tsid);\n\tif (IS_ERR(gts))\n\t\treturn gts;\n\n\tspin_lock(&vdata->vd_lock);\n\tngts = gru_find_current_gts_nolock(vdata, tsid);\n\tif (ngts) {\n\t\tgts_drop(gts);\n\t\tgts = ngts;\n\t\tSTAT(gts_double_allocate);\n\t} else {\n\t\tlist_add(&gts->ts_next, &vdata->vd_head);\n\t}\n\tspin_unlock(&vdata->vd_lock);\n\tgru_dbg(grudev, \"vma %p, gts %p\\n\", vma, gts);\n\treturn gts;\n}\n\n/*\n * Free the GRU context assigned to the thread state.\n */\nstatic void gru_free_gru_context(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru;\n\n\tgru = gts->ts_gru;\n\tgru_dbg(grudev, \"gts %p, gid %d\\n\", gts, gru->gs_gid);\n\n\tspin_lock(&gru->gs_lock);\n\tgru->gs_gts[gts->ts_ctxnum] = NULL;\n\tfree_gru_resources(gru, gts);\n\tBUG_ON(test_bit(gts->ts_ctxnum, &gru->gs_context_map) == 0);\n\t__clear_bit(gts->ts_ctxnum, &gru->gs_context_map);\n\tgts->ts_ctxnum = NULLCTX;\n\tgts->ts_gru = NULL;\n\tgts->ts_blade = -1;\n\tspin_unlock(&gru->gs_lock);\n\n\tgts_drop(gts);\n\tSTAT(free_context);\n}\n\n/*\n * Prefetching cachelines help hardware performance.\n * (Strictly a performance enhancement. Not functionally required).\n */\nstatic void prefetch_data(void *p, int num, int stride)\n{\n\twhile (num-- > 0) {\n\t\tprefetchw(p);\n\t\tp += stride;\n\t}\n}\n\nstatic inline long gru_copy_handle(void *d, void *s)\n{\n\tmemcpy(d, s, GRU_HANDLE_BYTES);\n\treturn GRU_HANDLE_BYTES;\n}\n\nstatic void gru_prefetch_context(void *gseg, void *cb, void *cbe,\n\t\t\t\tunsigned long cbrmap, unsigned long length)\n{\n\tint i, scr;\n\n\tprefetch_data(gseg + GRU_DS_BASE, length / GRU_CACHE_LINE_BYTES,\n\t\t      GRU_CACHE_LINE_BYTES);\n\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr) {\n\t\tprefetch_data(cb, 1, GRU_CACHE_LINE_BYTES);\n\t\tprefetch_data(cbe + i * GRU_HANDLE_STRIDE, 1,\n\t\t\t      GRU_CACHE_LINE_BYTES);\n\t\tcb += GRU_HANDLE_STRIDE;\n\t}\n}\n\nstatic void gru_load_context_data(void *save, void *grubase, int ctxnum,\n\t\t\t\t  unsigned long cbrmap, unsigned long dsrmap,\n\t\t\t\t  int data_valid)\n{\n\tvoid *gseg, *cb, *cbe;\n\tunsigned long length;\n\tint i, scr;\n\n\tgseg = grubase + ctxnum * GRU_GSEG_STRIDE;\n\tcb = gseg + GRU_CB_BASE;\n\tcbe = grubase + GRU_CBE_BASE;\n\tlength = hweight64(dsrmap) * GRU_DSR_AU_BYTES;\n\tgru_prefetch_context(gseg, cb, cbe, cbrmap, length);\n\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr) {\n\t\tif (data_valid) {\n\t\t\tsave += gru_copy_handle(cb, save);\n\t\t\tsave += gru_copy_handle(cbe + i * GRU_HANDLE_STRIDE,\n\t\t\t\t\t\tsave);\n\t\t} else {\n\t\t\tmemset(cb, 0, GRU_CACHE_LINE_BYTES);\n\t\t\tmemset(cbe + i * GRU_HANDLE_STRIDE, 0,\n\t\t\t\t\t\tGRU_CACHE_LINE_BYTES);\n\t\t}\n\t\t/* Flush CBE to hide race in context restart */\n\t\tmb();\n\t\tgru_flush_cache(cbe + i * GRU_HANDLE_STRIDE);\n\t\tcb += GRU_HANDLE_STRIDE;\n\t}\n\n\tif (data_valid)\n\t\tmemcpy(gseg + GRU_DS_BASE, save, length);\n\telse\n\t\tmemset(gseg + GRU_DS_BASE, 0, length);\n}\n\nstatic void gru_unload_context_data(void *save, void *grubase, int ctxnum,\n\t\t\t\t    unsigned long cbrmap, unsigned long dsrmap)\n{\n\tvoid *gseg, *cb, *cbe;\n\tunsigned long length;\n\tint i, scr;\n\n\tgseg = grubase + ctxnum * GRU_GSEG_STRIDE;\n\tcb = gseg + GRU_CB_BASE;\n\tcbe = grubase + GRU_CBE_BASE;\n\tlength = hweight64(dsrmap) * GRU_DSR_AU_BYTES;\n\n\t/* CBEs may not be coherent. Flush them from cache */\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr)\n\t\tgru_flush_cache(cbe + i * GRU_HANDLE_STRIDE);\n\tmb();\t\t/* Let the CL flush complete */\n\n\tgru_prefetch_context(gseg, cb, cbe, cbrmap, length);\n\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr) {\n\t\tsave += gru_copy_handle(save, cb);\n\t\tsave += gru_copy_handle(save, cbe + i * GRU_HANDLE_STRIDE);\n\t\tcb += GRU_HANDLE_STRIDE;\n\t}\n\tmemcpy(save, gseg + GRU_DS_BASE, length);\n}\n\nvoid gru_unload_context(struct gru_thread_state *gts, int savestate)\n{\n\tstruct gru_state *gru = gts->ts_gru;\n\tstruct gru_context_configuration_handle *cch;\n\tint ctxnum = gts->ts_ctxnum;\n\n\tif (!is_kernel_context(gts))\n\t\tzap_vma_ptes(gts->ts_vma, UGRUADDR(gts), GRU_GSEG_PAGESIZE);\n\tcch = get_cch(gru->gs_gru_base_vaddr, ctxnum);\n\n\tgru_dbg(grudev, \"gts %p, cbrmap 0x%lx, dsrmap 0x%lx\\n\",\n\t\tgts, gts->ts_cbr_map, gts->ts_dsr_map);\n\tlock_cch_handle(cch);\n\tif (cch_interrupt_sync(cch))\n\t\tBUG();\n\n\tif (!is_kernel_context(gts))\n\t\tgru_unload_mm_tracker(gru, gts);\n\tif (savestate) {\n\t\tgru_unload_context_data(gts->ts_gdata, gru->gs_gru_base_vaddr,\n\t\t\t\t\tctxnum, gts->ts_cbr_map,\n\t\t\t\t\tgts->ts_dsr_map);\n\t\tgts->ts_data_valid = 1;\n\t}\n\n\tif (cch_deallocate(cch))\n\t\tBUG();\n\tunlock_cch_handle(cch);\n\n\tgru_free_gru_context(gts);\n}\n\n/*\n * Load a GRU context by copying it from the thread data structure in memory\n * to the GRU.\n */\nvoid gru_load_context(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru = gts->ts_gru;\n\tstruct gru_context_configuration_handle *cch;\n\tint i, err, asid, ctxnum = gts->ts_ctxnum;\n\n\tcch = get_cch(gru->gs_gru_base_vaddr, ctxnum);\n\tlock_cch_handle(cch);\n\tcch->tfm_fault_bit_enable =\n\t    (gts->ts_user_options == GRU_OPT_MISS_FMM_POLL\n\t     || gts->ts_user_options == GRU_OPT_MISS_FMM_INTR);\n\tcch->tlb_int_enable = (gts->ts_user_options == GRU_OPT_MISS_FMM_INTR);\n\tif (cch->tlb_int_enable) {\n\t\tgts->ts_tlb_int_select = gru_cpu_fault_map_id();\n\t\tcch->tlb_int_select = gts->ts_tlb_int_select;\n\t}\n\tif (gts->ts_cch_req_slice >= 0) {\n\t\tcch->req_slice_set_enable = 1;\n\t\tcch->req_slice = gts->ts_cch_req_slice;\n\t} else {\n\t\tcch->req_slice_set_enable =0;\n\t}\n\tcch->tfm_done_bit_enable = 0;\n\tcch->dsr_allocation_map = gts->ts_dsr_map;\n\tcch->cbr_allocation_map = gts->ts_cbr_map;\n\n\tif (is_kernel_context(gts)) {\n\t\tcch->unmap_enable = 1;\n\t\tcch->tfm_done_bit_enable = 1;\n\t\tcch->cb_int_enable = 1;\n\t\tcch->tlb_int_select = 0;\t/* For now, ints go to cpu 0 */\n\t} else {\n\t\tcch->unmap_enable = 0;\n\t\tcch->tfm_done_bit_enable = 0;\n\t\tcch->cb_int_enable = 0;\n\t\tasid = gru_load_mm_tracker(gru, gts);\n\t\tfor (i = 0; i < 8; i++) {\n\t\t\tcch->asid[i] = asid + i;\n\t\t\tcch->sizeavail[i] = gts->ts_sizeavail;\n\t\t}\n\t}\n\n\terr = cch_allocate(cch);\n\tif (err) {\n\t\tgru_dbg(grudev,\n\t\t\t\"err %d: cch %p, gts %p, cbr 0x%lx, dsr 0x%lx\\n\",\n\t\t\terr, cch, gts, gts->ts_cbr_map, gts->ts_dsr_map);\n\t\tBUG();\n\t}\n\n\tgru_load_context_data(gts->ts_gdata, gru->gs_gru_base_vaddr, ctxnum,\n\t\t\tgts->ts_cbr_map, gts->ts_dsr_map, gts->ts_data_valid);\n\n\tif (cch_start(cch))\n\t\tBUG();\n\tunlock_cch_handle(cch);\n\n\tgru_dbg(grudev, \"gid %d, gts %p, cbrmap 0x%lx, dsrmap 0x%lx, tie %d, tis %d\\n\",\n\t\tgts->ts_gru->gs_gid, gts, gts->ts_cbr_map, gts->ts_dsr_map,\n\t\t(gts->ts_user_options == GRU_OPT_MISS_FMM_INTR), gts->ts_tlb_int_select);\n}\n\n/*\n * Update fields in an active CCH:\n * \t- retarget interrupts on local blade\n * \t- update sizeavail mask\n */\nint gru_update_cch(struct gru_thread_state *gts)\n{\n\tstruct gru_context_configuration_handle *cch;\n\tstruct gru_state *gru = gts->ts_gru;\n\tint i, ctxnum = gts->ts_ctxnum, ret = 0;\n\n\tcch = get_cch(gru->gs_gru_base_vaddr, ctxnum);\n\n\tlock_cch_handle(cch);\n\tif (cch->state == CCHSTATE_ACTIVE) {\n\t\tif (gru->gs_gts[gts->ts_ctxnum] != gts)\n\t\t\tgoto exit;\n\t\tif (cch_interrupt(cch))\n\t\t\tBUG();\n\t\tfor (i = 0; i < 8; i++)\n\t\t\tcch->sizeavail[i] = gts->ts_sizeavail;\n\t\tgts->ts_tlb_int_select = gru_cpu_fault_map_id();\n\t\tcch->tlb_int_select = gru_cpu_fault_map_id();\n\t\tcch->tfm_fault_bit_enable =\n\t\t  (gts->ts_user_options == GRU_OPT_MISS_FMM_POLL\n\t\t    || gts->ts_user_options == GRU_OPT_MISS_FMM_INTR);\n\t\tif (cch_start(cch))\n\t\t\tBUG();\n\t\tret = 1;\n\t}\nexit:\n\tunlock_cch_handle(cch);\n\treturn ret;\n}\n\n/*\n * Update CCH tlb interrupt select. Required when all the following is true:\n * \t- task's GRU context is loaded into a GRU\n * \t- task is using interrupt notification for TLB faults\n * \t- task has migrated to a different cpu on the same blade where\n * \t  it was previously running.\n */\nstatic int gru_retarget_intr(struct gru_thread_state *gts)\n{\n\tif (gts->ts_tlb_int_select < 0\n\t    || gts->ts_tlb_int_select == gru_cpu_fault_map_id())\n\t\treturn 0;\n\n\tgru_dbg(grudev, \"retarget from %d to %d\\n\", gts->ts_tlb_int_select,\n\t\tgru_cpu_fault_map_id());\n\treturn gru_update_cch(gts);\n}\n\n/*\n * Check if a GRU context is allowed to use a specific chiplet. By default\n * a context is assigned to any blade-local chiplet. However, users can\n * override this.\n * \tReturns 1 if assignment allowed, 0 otherwise\n */\nstatic int gru_check_chiplet_assignment(struct gru_state *gru,\n\t\t\t\t\tstruct gru_thread_state *gts)\n{\n\tint blade_id;\n\tint chiplet_id;\n\n\tblade_id = gts->ts_user_blade_id;\n\tif (blade_id < 0)\n\t\tblade_id = uv_numa_blade_id();\n\n\tchiplet_id = gts->ts_user_chiplet_id;\n\treturn gru->gs_blade_id == blade_id &&\n\t\t(chiplet_id < 0 || chiplet_id == gru->gs_chiplet_id);\n}\n\n/*\n * Unload the gru context if it is not assigned to the correct blade or\n * chiplet. Misassignment can occur if the process migrates to a different\n * blade or if the user changes the selected blade/chiplet.\n */\nvoid gru_check_context_placement(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru;\n\n\t/*\n\t * If the current task is the context owner, verify that the\n\t * context is correctly placed. This test is skipped for non-owner\n\t * references. Pthread apps use non-owner references to the CBRs.\n\t */\n\tgru = gts->ts_gru;\n\tif (!gru || gts->ts_tgid_owner != current->tgid)\n\t\treturn;\n\n\tif (!gru_check_chiplet_assignment(gru, gts)) {\n\t\tSTAT(check_context_unload);\n\t\tgru_unload_context(gts, 1);\n\t} else if (gru_retarget_intr(gts)) {\n\t\tSTAT(check_context_retarget_intr);\n\t}\n}\n\n\n/*\n * Insufficient GRU resources available on the local blade. Steal a context from\n * a process. This is a hack until a _real_ resource scheduler is written....\n */\n#define next_ctxnum(n)\t((n) <  GRU_NUM_CCH - 2 ? (n) + 1 : 0)\n#define next_gru(b, g)\t(((g) < &(b)->bs_grus[GRU_CHIPLETS_PER_BLADE - 1]) ?  \\\n\t\t\t\t ((g)+1) : &(b)->bs_grus[0])\n\nstatic int is_gts_stealable(struct gru_thread_state *gts,\n\t\tstruct gru_blade_state *bs)\n{\n\tif (is_kernel_context(gts))\n\t\treturn down_write_trylock(&bs->bs_kgts_sema);\n\telse\n\t\treturn mutex_trylock(&gts->ts_ctxlock);\n}\n\nstatic void gts_stolen(struct gru_thread_state *gts,\n\t\tstruct gru_blade_state *bs)\n{\n\tif (is_kernel_context(gts)) {\n\t\tup_write(&bs->bs_kgts_sema);\n\t\tSTAT(steal_kernel_context);\n\t} else {\n\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\tSTAT(steal_user_context);\n\t}\n}\n\nvoid gru_steal_context(struct gru_thread_state *gts)\n{\n\tstruct gru_blade_state *blade;\n\tstruct gru_state *gru, *gru0;\n\tstruct gru_thread_state *ngts = NULL;\n\tint ctxnum, ctxnum0, flag = 0, cbr, dsr;\n\tint blade_id;\n\n\tblade_id = gts->ts_user_blade_id;\n\tif (blade_id < 0)\n\t\tblade_id = uv_numa_blade_id();\n\tcbr = gts->ts_cbr_au_count;\n\tdsr = gts->ts_dsr_au_count;\n\n\tblade = gru_base[blade_id];\n\tspin_lock(&blade->bs_lock);\n\n\tctxnum = next_ctxnum(blade->bs_lru_ctxnum);\n\tgru = blade->bs_lru_gru;\n\tif (ctxnum == 0)\n\t\tgru = next_gru(blade, gru);\n\tblade->bs_lru_gru = gru;\n\tblade->bs_lru_ctxnum = ctxnum;\n\tctxnum0 = ctxnum;\n\tgru0 = gru;\n\twhile (1) {\n\t\tif (gru_check_chiplet_assignment(gru, gts)) {\n\t\t\tif (check_gru_resources(gru, cbr, dsr, GRU_NUM_CCH))\n\t\t\t\tbreak;\n\t\t\tspin_lock(&gru->gs_lock);\n\t\t\tfor (; ctxnum < GRU_NUM_CCH; ctxnum++) {\n\t\t\t\tif (flag && gru == gru0 && ctxnum == ctxnum0)\n\t\t\t\t\tbreak;\n\t\t\t\tngts = gru->gs_gts[ctxnum];\n\t\t\t\t/*\n\t\t\t \t* We are grabbing locks out of order, so trylock is\n\t\t\t \t* needed. GTSs are usually not locked, so the odds of\n\t\t\t \t* success are high. If trylock fails, try to steal a\n\t\t\t \t* different GSEG.\n\t\t\t \t*/\n\t\t\t\tif (ngts && is_gts_stealable(ngts, blade))\n\t\t\t\t\tbreak;\n\t\t\t\tngts = NULL;\n\t\t\t}\n\t\t\tspin_unlock(&gru->gs_lock);\n\t\t\tif (ngts || (flag && gru == gru0 && ctxnum == ctxnum0))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (flag && gru == gru0)\n\t\t\tbreak;\n\t\tflag = 1;\n\t\tctxnum = 0;\n\t\tgru = next_gru(blade, gru);\n\t}\n\tspin_unlock(&blade->bs_lock);\n\n\tif (ngts) {\n\t\tgts->ustats.context_stolen++;\n\t\tngts->ts_steal_jiffies = jiffies;\n\t\tgru_unload_context(ngts, is_kernel_context(ngts) ? 0 : 1);\n\t\tgts_stolen(ngts, blade);\n\t} else {\n\t\tSTAT(steal_context_failed);\n\t}\n\tgru_dbg(grudev,\n\t\t\"stole gid %d, ctxnum %d from gts %p. Need cb %d, ds %d;\"\n\t\t\" avail cb %ld, ds %ld\\n\",\n\t\tgru->gs_gid, ctxnum, ngts, cbr, dsr, hweight64(gru->gs_cbr_map),\n\t\thweight64(gru->gs_dsr_map));\n}\n\n/*\n * Assign a gru context.\n */\nstatic int gru_assign_context_number(struct gru_state *gru)\n{\n\tint ctxnum;\n\n\tctxnum = find_first_zero_bit(&gru->gs_context_map, GRU_NUM_CCH);\n\t__set_bit(ctxnum, &gru->gs_context_map);\n\treturn ctxnum;\n}\n\n/*\n * Scan the GRUs on the local blade & assign a GRU context.\n */\nstruct gru_state *gru_assign_gru_context(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru, *grux;\n\tint i, max_active_contexts;\n\tint blade_id = gts->ts_user_blade_id;\n\n\tif (blade_id < 0)\n\t\tblade_id = uv_numa_blade_id();\nagain:\n\tgru = NULL;\n\tmax_active_contexts = GRU_NUM_CCH;\n\tfor_each_gru_on_blade(grux, blade_id, i) {\n\t\tif (!gru_check_chiplet_assignment(grux, gts))\n\t\t\tcontinue;\n\t\tif (check_gru_resources(grux, gts->ts_cbr_au_count,\n\t\t\t\t\tgts->ts_dsr_au_count,\n\t\t\t\t\tmax_active_contexts)) {\n\t\t\tgru = grux;\n\t\t\tmax_active_contexts = grux->gs_active_contexts;\n\t\t\tif (max_active_contexts == 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (gru) {\n\t\tspin_lock(&gru->gs_lock);\n\t\tif (!check_gru_resources(gru, gts->ts_cbr_au_count,\n\t\t\t\t\t gts->ts_dsr_au_count, GRU_NUM_CCH)) {\n\t\t\tspin_unlock(&gru->gs_lock);\n\t\t\tgoto again;\n\t\t}\n\t\treserve_gru_resources(gru, gts);\n\t\tgts->ts_gru = gru;\n\t\tgts->ts_blade = gru->gs_blade_id;\n\t\tgts->ts_ctxnum = gru_assign_context_number(gru);\n\t\trefcount_inc(&gts->ts_refcnt);\n\t\tgru->gs_gts[gts->ts_ctxnum] = gts;\n\t\tspin_unlock(&gru->gs_lock);\n\n\t\tSTAT(assign_context);\n\t\tgru_dbg(grudev,\n\t\t\t\"gseg %p, gts %p, gid %d, ctx %d, cbr %d, dsr %d\\n\",\n\t\t\tgseg_virtual_address(gts->ts_gru, gts->ts_ctxnum), gts,\n\t\t\tgts->ts_gru->gs_gid, gts->ts_ctxnum,\n\t\t\tgts->ts_cbr_au_count, gts->ts_dsr_au_count);\n\t} else {\n\t\tgru_dbg(grudev, \"failed to allocate a GTS %s\\n\", \"\");\n\t\tSTAT(assign_context_failed);\n\t}\n\n\treturn gru;\n}\n\n/*\n * gru_nopage\n *\n * Map the user's GRU segment\n *\n * \tNote: gru segments alway mmaped on GRU_GSEG_PAGESIZE boundaries.\n */\nvm_fault_t gru_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct gru_thread_state *gts;\n\tunsigned long paddr, vaddr;\n\tunsigned long expires;\n\n\tvaddr = vmf->address;\n\tgru_dbg(grudev, \"vma %p, vaddr 0x%lx (0x%lx)\\n\",\n\t\tvma, vaddr, GSEG_BASE(vaddr));\n\tSTAT(nopfn);\n\n\t/* The following check ensures vaddr is a valid address in the VMA */\n\tgts = gru_find_thread_state(vma, TSID(vaddr, vma));\n\tif (!gts)\n\t\treturn VM_FAULT_SIGBUS;\n\nagain:\n\tmutex_lock(&gts->ts_ctxlock);\n\tpreempt_disable();\n\n\tgru_check_context_placement(gts);\n\n\tif (!gts->ts_gru) {\n\t\tSTAT(load_user_context);\n\t\tif (!gru_assign_gru_context(gts)) {\n\t\t\tpreempt_enable();\n\t\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tschedule_timeout(GRU_ASSIGN_DELAY);  /* true hack ZZZ */\n\t\t\texpires = gts->ts_steal_jiffies + GRU_STEAL_DELAY;\n\t\t\tif (time_before(expires, jiffies))\n\t\t\t\tgru_steal_context(gts);\n\t\t\tgoto again;\n\t\t}\n\t\tgru_load_context(gts);\n\t\tpaddr = gseg_physical_address(gts->ts_gru, gts->ts_ctxnum);\n\t\tremap_pfn_range(vma, vaddr & ~(GRU_GSEG_PAGESIZE - 1),\n\t\t\t\tpaddr >> PAGE_SHIFT, GRU_GSEG_PAGESIZE,\n\t\t\t\tvma->vm_page_prot);\n\t}\n\n\tpreempt_enable();\n\tmutex_unlock(&gts->ts_ctxlock);\n\n\treturn VM_FAULT_NOPAGE;\n}\n\n", "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n * SN Platform GRU Driver\n *\n *            GRU DRIVER TABLES, MACROS, externs, etc\n *\n *  Copyright (c) 2008 Silicon Graphics, Inc.  All Rights Reserved.\n */\n\n#ifndef __GRUTABLES_H__\n#define __GRUTABLES_H__\n\n/*\n * GRU Chiplet:\n *   The GRU is a user addressible memory accelerator. It provides\n *   several forms of load, store, memset, bcopy instructions. In addition, it\n *   contains special instructions for AMOs, sending messages to message\n *   queues, etc.\n *\n *   The GRU is an integral part of the node controller. It connects\n *   directly to the cpu socket. In its current implementation, there are 2\n *   GRU chiplets in the node controller on each blade (~node).\n *\n *   The entire GRU memory space is fully coherent and cacheable by the cpus.\n *\n *   Each GRU chiplet has a physical memory map that looks like the following:\n *\n *   \t+-----------------+\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t+-----------------+\n *   \t|  system control |\n *   \t+-----------------+        _______ +-------------+\n *   \t|/////////////////|       /        |             |\n *   \t|/////////////////|      /         |             |\n *   \t|/////////////////|     /          | instructions|\n *   \t|/////////////////|    /           |             |\n *   \t|/////////////////|   /            |             |\n *   \t|/////////////////|  /             |-------------|\n *   \t|/////////////////| /              |             |\n *   \t+-----------------+                |             |\n *   \t|   context 15    |                |  data       |\n *   \t+-----------------+                |             |\n *   \t|    ......       | \\              |             |\n *   \t+-----------------+  \\____________ +-------------+\n *   \t|   context 1     |\n *   \t+-----------------+\n *   \t|   context 0     |\n *   \t+-----------------+\n *\n *   Each of the \"contexts\" is a chunk of memory that can be mmaped into user\n *   space. The context consists of 2 parts:\n *\n *  \t- an instruction space that can be directly accessed by the user\n *  \t  to issue GRU instructions and to check instruction status.\n *\n *  \t- a data area that acts as normal RAM.\n *\n *   User instructions contain virtual addresses of data to be accessed by the\n *   GRU. The GRU contains a TLB that is used to convert these user virtual\n *   addresses to physical addresses.\n *\n *   The \"system control\" area of the GRU chiplet is used by the kernel driver\n *   to manage user contexts and to perform functions such as TLB dropin and\n *   purging.\n *\n *   One context may be reserved for the kernel and used for cross-partition\n *   communication. The GRU will also be used to asynchronously zero out\n *   large blocks of memory (not currently implemented).\n *\n *\n * Tables:\n *\n * \tVDATA-VMA Data\t\t- Holds a few parameters. Head of linked list of\n * \t\t\t\t  GTS tables for threads using the GSEG\n * \tGTS - Gru Thread State  - contains info for managing a GSEG context. A\n * \t\t\t\t  GTS is allocated for each thread accessing a\n * \t\t\t\t  GSEG.\n *     \tGTD - GRU Thread Data   - contains shadow copy of GRU data when GSEG is\n *     \t\t\t\t  not loaded into a GRU\n *\tGMS - GRU Memory Struct - Used to manage TLB shootdowns. Tracks GRUs\n *\t\t\t\t  where a GSEG has been loaded. Similar to\n *\t\t\t\t  an mm_struct but for GRU.\n *\n *\tGS  - GRU State \t- Used to manage the state of a GRU chiplet\n *\tBS  - Blade State\t- Used to manage state of all GRU chiplets\n *\t\t\t\t  on a blade\n *\n *\n *  Normal task tables for task using GRU.\n *  \t\t- 2 threads in process\n *  \t\t- 2 GSEGs open in process\n *  \t\t- GSEG1 is being used by both threads\n *  \t\t- GSEG2 is used only by thread 2\n *\n *       task -->|\n *       task ---+---> mm ->------ (notifier) -------+-> gms\n *                     |                             |\n *                     |--> vma -> vdata ---> gts--->|\t\tGSEG1 (thread1)\n *                     |                  |          |\n *                     |                  +-> gts--->|\t\tGSEG1 (thread2)\n *                     |                             |\n *                     |--> vma -> vdata ---> gts--->|\t\tGSEG2 (thread2)\n *                     .\n *                     .\n *\n *  GSEGs are marked DONTCOPY on fork\n *\n * At open\n * \tfile.private_data -> NULL\n *\n * At mmap,\n * \tvma -> vdata\n *\n * After gseg reference\n * \tvma -> vdata ->gts\n *\n * After fork\n *   parent\n * \tvma -> vdata -> gts\n *   child\n * \t(vma is not copied)\n *\n */\n\n#include <linux/refcount.h>\n#include <linux/rmap.h>\n#include <linux/interrupt.h>\n#include <linux/mutex.h>\n#include <linux/wait.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mm_types.h>\n#include \"gru.h\"\n#include \"grulib.h\"\n#include \"gruhandles.h\"\n\nextern struct gru_stats_s gru_stats;\nextern struct gru_blade_state *gru_base[];\nextern unsigned long gru_start_paddr, gru_end_paddr;\nextern void *gru_start_vaddr;\nextern unsigned int gru_max_gids;\n\n#define GRU_MAX_BLADES\t\tMAX_NUMNODES\n#define GRU_MAX_GRUS\t\t(GRU_MAX_BLADES * GRU_CHIPLETS_PER_BLADE)\n\n#define GRU_DRIVER_ID_STR\t\"SGI GRU Device Driver\"\n#define GRU_DRIVER_VERSION_STR\t\"0.85\"\n\n/*\n * GRU statistics.\n */\nstruct gru_stats_s {\n\tatomic_long_t vdata_alloc;\n\tatomic_long_t vdata_free;\n\tatomic_long_t gts_alloc;\n\tatomic_long_t gts_free;\n\tatomic_long_t gms_alloc;\n\tatomic_long_t gms_free;\n\tatomic_long_t gts_double_allocate;\n\tatomic_long_t assign_context;\n\tatomic_long_t assign_context_failed;\n\tatomic_long_t free_context;\n\tatomic_long_t load_user_context;\n\tatomic_long_t load_kernel_context;\n\tatomic_long_t lock_kernel_context;\n\tatomic_long_t unlock_kernel_context;\n\tatomic_long_t steal_user_context;\n\tatomic_long_t steal_kernel_context;\n\tatomic_long_t steal_context_failed;\n\tatomic_long_t nopfn;\n\tatomic_long_t asid_new;\n\tatomic_long_t asid_next;\n\tatomic_long_t asid_wrap;\n\tatomic_long_t asid_reuse;\n\tatomic_long_t intr;\n\tatomic_long_t intr_cbr;\n\tatomic_long_t intr_tfh;\n\tatomic_long_t intr_spurious;\n\tatomic_long_t intr_mm_lock_failed;\n\tatomic_long_t call_os;\n\tatomic_long_t call_os_wait_queue;\n\tatomic_long_t user_flush_tlb;\n\tatomic_long_t user_unload_context;\n\tatomic_long_t user_exception;\n\tatomic_long_t set_context_option;\n\tatomic_long_t check_context_retarget_intr;\n\tatomic_long_t check_context_unload;\n\tatomic_long_t tlb_dropin;\n\tatomic_long_t tlb_preload_page;\n\tatomic_long_t tlb_dropin_fail_no_asid;\n\tatomic_long_t tlb_dropin_fail_upm;\n\tatomic_long_t tlb_dropin_fail_invalid;\n\tatomic_long_t tlb_dropin_fail_range_active;\n\tatomic_long_t tlb_dropin_fail_idle;\n\tatomic_long_t tlb_dropin_fail_fmm;\n\tatomic_long_t tlb_dropin_fail_no_exception;\n\tatomic_long_t tfh_stale_on_fault;\n\tatomic_long_t mmu_invalidate_range;\n\tatomic_long_t mmu_invalidate_page;\n\tatomic_long_t flush_tlb;\n\tatomic_long_t flush_tlb_gru;\n\tatomic_long_t flush_tlb_gru_tgh;\n\tatomic_long_t flush_tlb_gru_zero_asid;\n\n\tatomic_long_t copy_gpa;\n\tatomic_long_t read_gpa;\n\n\tatomic_long_t mesq_receive;\n\tatomic_long_t mesq_receive_none;\n\tatomic_long_t mesq_send;\n\tatomic_long_t mesq_send_failed;\n\tatomic_long_t mesq_noop;\n\tatomic_long_t mesq_send_unexpected_error;\n\tatomic_long_t mesq_send_lb_overflow;\n\tatomic_long_t mesq_send_qlimit_reached;\n\tatomic_long_t mesq_send_amo_nacked;\n\tatomic_long_t mesq_send_put_nacked;\n\tatomic_long_t mesq_page_overflow;\n\tatomic_long_t mesq_qf_locked;\n\tatomic_long_t mesq_qf_noop_not_full;\n\tatomic_long_t mesq_qf_switch_head_failed;\n\tatomic_long_t mesq_qf_unexpected_error;\n\tatomic_long_t mesq_noop_unexpected_error;\n\tatomic_long_t mesq_noop_lb_overflow;\n\tatomic_long_t mesq_noop_qlimit_reached;\n\tatomic_long_t mesq_noop_amo_nacked;\n\tatomic_long_t mesq_noop_put_nacked;\n\tatomic_long_t mesq_noop_page_overflow;\n\n};\n\nenum mcs_op {cchop_allocate, cchop_start, cchop_interrupt, cchop_interrupt_sync,\n\tcchop_deallocate, tfhop_write_only, tfhop_write_restart,\n\ttghop_invalidate, mcsop_last};\n\nstruct mcs_op_statistic {\n\tatomic_long_t\tcount;\n\tatomic_long_t\ttotal;\n\tunsigned long\tmax;\n};\n\nextern struct mcs_op_statistic mcs_op_statistics[mcsop_last];\n\n#define OPT_DPRINT\t\t1\n#define OPT_STATS\t\t2\n\n\n#define IRQ_GRU\t\t\t110\t/* Starting IRQ number for interrupts */\n\n/* Delay in jiffies between attempts to assign a GRU context */\n#define GRU_ASSIGN_DELAY\t((HZ * 20) / 1000)\n\n/*\n * If a process has it's context stolen, min delay in jiffies before trying to\n * steal a context from another process.\n */\n#define GRU_STEAL_DELAY\t\t((HZ * 200) / 1000)\n\n#define STAT(id)\tdo {\t\t\t\t\t\t\\\n\t\t\t\tif (gru_options & OPT_STATS)\t\t\\\n\t\t\t\t\tatomic_long_inc(&gru_stats.id);\t\\\n\t\t\t} while (0)\n\n#ifdef CONFIG_SGI_GRU_DEBUG\n#define gru_dbg(dev, fmt, x...)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (gru_options & OPT_DPRINT)\t\t\t\t\\\n\t\t\tprintk(KERN_DEBUG \"GRU:%d %s: \" fmt, smp_processor_id(), __func__, x);\\\n\t} while (0)\n#else\n#define gru_dbg(x...)\n#endif\n\n/*-----------------------------------------------------------------------------\n * ASID management\n */\n#define MAX_ASID\t0xfffff0\n#define MIN_ASID\t8\n#define ASID_INC\t8\t/* number of regions */\n\n/* Generate a GRU asid value from a GRU base asid & a virtual address. */\n#define VADDR_HI_BIT\t\t64\n#define GRUREGION(addr)\t\t((addr) >> (VADDR_HI_BIT - 3) & 3)\n#define GRUASID(asid, addr)\t((asid) + GRUREGION(addr))\n\n/*------------------------------------------------------------------------------\n *  File & VMS Tables\n */\n\nstruct gru_state;\n\n/*\n * This structure is pointed to from the mmstruct via the notifier pointer.\n * There is one of these per address space.\n */\nstruct gru_mm_tracker {\t\t\t\t/* pack to reduce size */\n\tunsigned int\t\tmt_asid_gen:24;\t/* ASID wrap count */\n\tunsigned int\t\tmt_asid:24;\t/* current base ASID for gru */\n\tunsigned short\t\tmt_ctxbitmap:16;/* bitmap of contexts using\n\t\t\t\t\t\t   asid */\n} __attribute__ ((packed));\n\nstruct gru_mm_struct {\n\tstruct mmu_notifier\tms_notifier;\n\tspinlock_t\t\tms_asid_lock;\t/* protects ASID assignment */\n\tatomic_t\t\tms_range_active;/* num range_invals active */\n\twait_queue_head_t\tms_wait_queue;\n\tDECLARE_BITMAP(ms_asidmap, GRU_MAX_GRUS);\n\tstruct gru_mm_tracker\tms_asids[GRU_MAX_GRUS];\n};\n\n/*\n * One of these structures is allocated when a GSEG is mmaped. The\n * structure is pointed to by the vma->vm_private_data field in the vma struct.\n */\nstruct gru_vma_data {\n\tspinlock_t\t\tvd_lock;\t/* Serialize access to vma */\n\tstruct list_head\tvd_head;\t/* head of linked list of gts */\n\tlong\t\t\tvd_user_options;/* misc user option flags */\n\tint\t\t\tvd_cbr_au_count;\n\tint\t\t\tvd_dsr_au_count;\n\tunsigned char\t\tvd_tlb_preload_count;\n};\n\n/*\n * One of these is allocated for each thread accessing a mmaped GRU. A linked\n * list of these structure is hung off the struct gru_vma_data in the mm_struct.\n */\nstruct gru_thread_state {\n\tstruct list_head\tts_next;\t/* list - head at vma-private */\n\tstruct mutex\t\tts_ctxlock;\t/* load/unload CTX lock */\n\tstruct mm_struct\t*ts_mm;\t\t/* mm currently mapped to\n\t\t\t\t\t\t   context */\n\tstruct vm_area_struct\t*ts_vma;\t/* vma of GRU context */\n\tstruct gru_state\t*ts_gru;\t/* GRU where the context is\n\t\t\t\t\t\t   loaded */\n\tstruct gru_mm_struct\t*ts_gms;\t/* asid & ioproc struct */\n\tunsigned char\t\tts_tlb_preload_count; /* TLB preload pages */\n\tunsigned long\t\tts_cbr_map;\t/* map of allocated CBRs */\n\tunsigned long\t\tts_dsr_map;\t/* map of allocated DATA\n\t\t\t\t\t\t   resources */\n\tunsigned long\t\tts_steal_jiffies;/* jiffies when context last\n\t\t\t\t\t\t    stolen */\n\tlong\t\t\tts_user_options;/* misc user option flags */\n\tpid_t\t\t\tts_tgid_owner;\t/* task that is using the\n\t\t\t\t\t\t   context - for migration */\n\tshort\t\t\tts_user_blade_id;/* user selected blade */\n\tsigned char\t\tts_user_chiplet_id;/* user selected chiplet */\n\tunsigned short\t\tts_sizeavail;\t/* Pagesizes in use */\n\tint\t\t\tts_tsid;\t/* thread that owns the\n\t\t\t\t\t\t   structure */\n\tint\t\t\tts_tlb_int_select;/* target cpu if interrupts\n\t\t\t\t\t\t     enabled */\n\tint\t\t\tts_ctxnum;\t/* context number where the\n\t\t\t\t\t\t   context is loaded */\n\trefcount_t\t\tts_refcnt;\t/* reference count GTS */\n\tunsigned char\t\tts_dsr_au_count;/* Number of DSR resources\n\t\t\t\t\t\t   required for contest */\n\tunsigned char\t\tts_cbr_au_count;/* Number of CBR resources\n\t\t\t\t\t\t   required for contest */\n\tsigned char\t\tts_cch_req_slice;/* CCH packet slice */\n\tsigned char\t\tts_blade;\t/* If >= 0, migrate context if\n\t\t\t\t\t\t   ref from different blade */\n\tsigned char\t\tts_force_cch_reload;\n\tsigned char\t\tts_cbr_idx[GRU_CBR_AU];/* CBR numbers of each\n\t\t\t\t\t\t\t  allocated CB */\n\tint\t\t\tts_data_valid;\t/* Indicates if ts_gdata has\n\t\t\t\t\t\t   valid data */\n\tstruct gru_gseg_statistics ustats;\t/* User statistics */\n\tunsigned long\t\tts_gdata[];\t/* save area for GRU data (CB,\n\t\t\t\t\t\t   DS, CBE) */\n};\n\n/*\n * Threaded programs actually allocate an array of GSEGs when a context is\n * created. Each thread uses a separate GSEG. TSID is the index into the GSEG\n * array.\n */\n#define TSID(a, v)\t\t(((a) - (v)->vm_start) / GRU_GSEG_PAGESIZE)\n#define UGRUADDR(gts)\t\t((gts)->ts_vma->vm_start +\t\t\\\n\t\t\t\t\t(gts)->ts_tsid * GRU_GSEG_PAGESIZE)\n\n#define NULLCTX\t\t\t(-1)\t/* if context not loaded into GRU */\n\n/*-----------------------------------------------------------------------------\n *  GRU State Tables\n */\n\n/*\n * One of these exists for each GRU chiplet.\n */\nstruct gru_state {\n\tstruct gru_blade_state\t*gs_blade;\t\t/* GRU state for entire\n\t\t\t\t\t\t\t   blade */\n\tunsigned long\t\tgs_gru_base_paddr;\t/* Physical address of\n\t\t\t\t\t\t\t   gru segments (64) */\n\tvoid\t\t\t*gs_gru_base_vaddr;\t/* Virtual address of\n\t\t\t\t\t\t\t   gru segments (64) */\n\tunsigned short\t\tgs_gid;\t\t\t/* unique GRU number */\n\tunsigned short\t\tgs_blade_id;\t\t/* blade of GRU */\n\tunsigned char\t\tgs_chiplet_id;\t\t/* blade chiplet of GRU */\n\tunsigned char\t\tgs_tgh_local_shift;\t/* used to pick TGH for\n\t\t\t\t\t\t\t   local flush */\n\tunsigned char\t\tgs_tgh_first_remote;\t/* starting TGH# for\n\t\t\t\t\t\t\t   remote flush */\n\tspinlock_t\t\tgs_asid_lock;\t\t/* lock used for\n\t\t\t\t\t\t\t   assigning asids */\n\tspinlock_t\t\tgs_lock;\t\t/* lock used for\n\t\t\t\t\t\t\t   assigning contexts */\n\n\t/* -- the following are protected by the gs_asid_lock spinlock ---- */\n\tunsigned int\t\tgs_asid;\t\t/* Next availe ASID */\n\tunsigned int\t\tgs_asid_limit;\t\t/* Limit of available\n\t\t\t\t\t\t\t   ASIDs */\n\tunsigned int\t\tgs_asid_gen;\t\t/* asid generation.\n\t\t\t\t\t\t\t   Inc on wrap */\n\n\t/* --- the following fields are protected by the gs_lock spinlock --- */\n\tunsigned long\t\tgs_context_map;\t\t/* bitmap to manage\n\t\t\t\t\t\t\t   contexts in use */\n\tunsigned long\t\tgs_cbr_map;\t\t/* bitmap to manage CB\n\t\t\t\t\t\t\t   resources */\n\tunsigned long\t\tgs_dsr_map;\t\t/* bitmap used to manage\n\t\t\t\t\t\t\t   DATA resources */\n\tunsigned int\t\tgs_reserved_cbrs;\t/* Number of kernel-\n\t\t\t\t\t\t\t   reserved cbrs */\n\tunsigned int\t\tgs_reserved_dsr_bytes;\t/* Bytes of kernel-\n\t\t\t\t\t\t\t   reserved dsrs */\n\tunsigned short\t\tgs_active_contexts;\t/* number of contexts\n\t\t\t\t\t\t\t   in use */\n\tstruct gru_thread_state\t*gs_gts[GRU_NUM_CCH];\t/* GTS currently using\n\t\t\t\t\t\t\t   the context */\n\tint\t\t\tgs_irq[GRU_NUM_TFM];\t/* Interrupt irqs */\n};\n\n/*\n * This structure contains the GRU state for all the GRUs on a blade.\n */\nstruct gru_blade_state {\n\tvoid\t\t\t*kernel_cb;\t\t/* First kernel\n\t\t\t\t\t\t\t   reserved cb */\n\tvoid\t\t\t*kernel_dsr;\t\t/* First kernel\n\t\t\t\t\t\t\t   reserved DSR */\n\tstruct rw_semaphore\tbs_kgts_sema;\t\t/* lock for kgts */\n\tstruct gru_thread_state *bs_kgts;\t\t/* GTS for kernel use */\n\n\t/* ---- the following are used for managing kernel async GRU CBRs --- */\n\tint\t\t\tbs_async_dsr_bytes;\t/* DSRs for async */\n\tint\t\t\tbs_async_cbrs;\t\t/* CBRs AU for async */\n\tstruct completion\t*bs_async_wq;\n\n\t/* ---- the following are protected by the bs_lock spinlock ---- */\n\tspinlock_t\t\tbs_lock;\t\t/* lock used for\n\t\t\t\t\t\t\t   stealing contexts */\n\tint\t\t\tbs_lru_ctxnum;\t\t/* STEAL - last context\n\t\t\t\t\t\t\t   stolen */\n\tstruct gru_state\t*bs_lru_gru;\t\t/* STEAL - last gru\n\t\t\t\t\t\t\t   stolen */\n\n\tstruct gru_state\tbs_grus[GRU_CHIPLETS_PER_BLADE];\n};\n\n/*-----------------------------------------------------------------------------\n * Address Primitives\n */\n#define get_tfm_for_cpu(g, c)\t\t\t\t\t\t\\\n\t((struct gru_tlb_fault_map *)get_tfm((g)->gs_gru_base_vaddr, (c)))\n#define get_tfh_by_index(g, i)\t\t\t\t\t\t\\\n\t((struct gru_tlb_fault_handle *)get_tfh((g)->gs_gru_base_vaddr, (i)))\n#define get_tgh_by_index(g, i)\t\t\t\t\t\t\\\n\t((struct gru_tlb_global_handle *)get_tgh((g)->gs_gru_base_vaddr, (i)))\n#define get_cbe_by_index(g, i)\t\t\t\t\t\t\\\n\t((struct gru_control_block_extended *)get_cbe((g)->gs_gru_base_vaddr,\\\n\t\t\t(i)))\n\n/*-----------------------------------------------------------------------------\n * Useful Macros\n */\n\n/* Given a blade# & chiplet#, get a pointer to the GRU */\n#define get_gru(b, c)\t\t(&gru_base[b]->bs_grus[c])\n\n/* Number of bytes to save/restore when unloading/loading GRU contexts */\n#define DSR_BYTES(dsr)\t\t((dsr) * GRU_DSR_AU_BYTES)\n#define CBR_BYTES(cbr)\t\t((cbr) * GRU_HANDLE_BYTES * GRU_CBR_AU_SIZE * 2)\n\n/* Convert a user CB number to the actual CBRNUM */\n#define thread_cbr_number(gts, n) ((gts)->ts_cbr_idx[(n) / GRU_CBR_AU_SIZE] \\\n\t\t\t\t  * GRU_CBR_AU_SIZE + (n) % GRU_CBR_AU_SIZE)\n\n/* Convert a gid to a pointer to the GRU */\n#define GID_TO_GRU(gid)\t\t\t\t\t\t\t\\\n\t(gru_base[(gid) / GRU_CHIPLETS_PER_BLADE] ?\t\t\t\\\n\t\t(&gru_base[(gid) / GRU_CHIPLETS_PER_BLADE]->\t\t\\\n\t\t\tbs_grus[(gid) % GRU_CHIPLETS_PER_BLADE]) :\t\\\n\t NULL)\n\n/* Scan all active GRUs in a GRU bitmap */\n#define for_each_gru_in_bitmap(gid, map)\t\t\t\t\\\n\tfor_each_set_bit((gid), (map), GRU_MAX_GRUS)\n\n/* Scan all active GRUs on a specific blade */\n#define for_each_gru_on_blade(gru, nid, i)\t\t\t\t\\\n\tfor ((gru) = gru_base[nid]->bs_grus, (i) = 0;\t\t\t\\\n\t\t\t(i) < GRU_CHIPLETS_PER_BLADE;\t\t\t\\\n\t\t\t(i)++, (gru)++)\n\n/* Scan all GRUs */\n#define foreach_gid(gid)\t\t\t\t\t\t\\\n\tfor ((gid) = 0; (gid) < gru_max_gids; (gid)++)\n\n/* Scan all active GTSs on a gru. Note: must hold ss_lock to use this macro. */\n#define for_each_gts_on_gru(gts, gru, ctxnum)\t\t\t\t\\\n\tfor ((ctxnum) = 0; (ctxnum) < GRU_NUM_CCH; (ctxnum)++)\t\t\\\n\t\tif (((gts) = (gru)->gs_gts[ctxnum]))\n\n/* Scan each CBR whose bit is set in a TFM (or copy of) */\n#define for_each_cbr_in_tfm(i, map)\t\t\t\t\t\\\n\tfor_each_set_bit((i), (map), GRU_NUM_CBE)\n\n/* Scan each CBR in a CBR bitmap. Note: multiple CBRs in an allocation unit */\n#define for_each_cbr_in_allocation_map(i, map, k)\t\t\t\\\n\tfor_each_set_bit((k), (map), GRU_CBR_AU)\t\t\t\\\n\t\tfor ((i) = (k)*GRU_CBR_AU_SIZE;\t\t\t\t\\\n\t\t\t\t(i) < ((k) + 1) * GRU_CBR_AU_SIZE; (i)++)\n\n#define gseg_physical_address(gru, ctxnum)\t\t\t\t\\\n\t\t((gru)->gs_gru_base_paddr + ctxnum * GRU_GSEG_STRIDE)\n#define gseg_virtual_address(gru, ctxnum)\t\t\t\t\\\n\t\t((gru)->gs_gru_base_vaddr + ctxnum * GRU_GSEG_STRIDE)\n\n/*-----------------------------------------------------------------------------\n * Lock / Unlock GRU handles\n * \tUse the \"delresp\" bit in the handle as a \"lock\" bit.\n */\n\n/* Lock hierarchy checking enabled only in emulator */\n\n/* 0 = lock failed, 1 = locked */\nstatic inline int __trylock_handle(void *h)\n{\n\treturn !test_and_set_bit(1, h);\n}\n\nstatic inline void __lock_handle(void *h)\n{\n\twhile (test_and_set_bit(1, h))\n\t\tcpu_relax();\n}\n\nstatic inline void __unlock_handle(void *h)\n{\n\tclear_bit(1, h);\n}\n\nstatic inline int trylock_cch_handle(struct gru_context_configuration_handle *cch)\n{\n\treturn __trylock_handle(cch);\n}\n\nstatic inline void lock_cch_handle(struct gru_context_configuration_handle *cch)\n{\n\t__lock_handle(cch);\n}\n\nstatic inline void unlock_cch_handle(struct gru_context_configuration_handle\n\t\t\t\t     *cch)\n{\n\t__unlock_handle(cch);\n}\n\nstatic inline void lock_tgh_handle(struct gru_tlb_global_handle *tgh)\n{\n\t__lock_handle(tgh);\n}\n\nstatic inline void unlock_tgh_handle(struct gru_tlb_global_handle *tgh)\n{\n\t__unlock_handle(tgh);\n}\n\nstatic inline int is_kernel_context(struct gru_thread_state *gts)\n{\n\treturn !gts->ts_mm;\n}\n\n/*\n * The following are for Nehelem-EX. A more general scheme is needed for\n * future processors.\n */\n#define UV_MAX_INT_CORES\t\t8\n#define uv_cpu_socket_number(p)\t\t((cpu_physical_id(p) >> 5) & 1)\n#define uv_cpu_ht_number(p)\t\t(cpu_physical_id(p) & 1)\n#define uv_cpu_core_number(p)\t\t(((cpu_physical_id(p) >> 2) & 4) |\t\\\n\t\t\t\t\t((cpu_physical_id(p) >> 1) & 3))\n/*-----------------------------------------------------------------------------\n * Function prototypes & externs\n */\nstruct gru_unload_context_req;\n\nextern const struct vm_operations_struct gru_vm_ops;\nextern struct device *grudev;\n\nextern struct gru_vma_data *gru_alloc_vma_data(struct vm_area_struct *vma,\n\t\t\t\tint tsid);\nextern struct gru_thread_state *gru_find_thread_state(struct vm_area_struct\n\t\t\t\t*vma, int tsid);\nextern struct gru_thread_state *gru_alloc_thread_state(struct vm_area_struct\n\t\t\t\t*vma, int tsid);\nextern struct gru_state *gru_assign_gru_context(struct gru_thread_state *gts);\nextern void gru_load_context(struct gru_thread_state *gts);\nextern void gru_steal_context(struct gru_thread_state *gts);\nextern void gru_unload_context(struct gru_thread_state *gts, int savestate);\nextern int gru_update_cch(struct gru_thread_state *gts);\nextern void gts_drop(struct gru_thread_state *gts);\nextern void gru_tgh_flush_init(struct gru_state *gru);\nextern int gru_kservices_init(void);\nextern void gru_kservices_exit(void);\nextern irqreturn_t gru0_intr(int irq, void *dev_id);\nextern irqreturn_t gru1_intr(int irq, void *dev_id);\nextern irqreturn_t gru_intr_mblade(int irq, void *dev_id);\nextern int gru_dump_chiplet_request(unsigned long arg);\nextern long gru_get_gseg_statistics(unsigned long arg);\nextern int gru_handle_user_call_os(unsigned long address);\nextern int gru_user_flush_tlb(unsigned long arg);\nextern int gru_user_unload_context(unsigned long arg);\nextern int gru_get_exception_detail(unsigned long arg);\nextern int gru_set_context_option(unsigned long address);\nextern void gru_check_context_placement(struct gru_thread_state *gts);\nextern int gru_cpu_fault_map_id(void);\nextern struct vm_area_struct *gru_find_vma(unsigned long vaddr);\nextern void gru_flush_all_tlb(struct gru_state *gru);\nextern int gru_proc_init(void);\nextern void gru_proc_exit(void);\n\nextern struct gru_thread_state *gru_alloc_gts(struct vm_area_struct *vma,\n\t\tint cbr_au_count, int dsr_au_count,\n\t\tunsigned char tlb_preload_count, int options, int tsid);\nextern unsigned long gru_reserve_cb_resources(struct gru_state *gru,\n\t\tint cbr_au_count, signed char *cbmap);\nextern unsigned long gru_reserve_ds_resources(struct gru_state *gru,\n\t\tint dsr_au_count, signed char *dsmap);\nextern vm_fault_t gru_fault(struct vm_fault *vmf);\nextern struct gru_mm_struct *gru_register_mmu_notifier(void);\nextern void gru_drop_mmu_notifier(struct gru_mm_struct *gms);\n\nextern int gru_ktest(unsigned long arg);\nextern void gru_flush_tlb_range(struct gru_mm_struct *gms, unsigned long start,\n\t\t\t\t\tunsigned long len);\n\nextern unsigned long gru_options;\n\n#endif /* __GRUTABLES_H__ */\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * SN Platform GRU Driver\n *\n *              FAULT HANDLER FOR GRU DETECTED TLB MISSES\n *\n * This file contains code that handles TLB misses within the GRU.\n * These misses are reported either via interrupts or user polling of\n * the user CB.\n *\n *  Copyright (c) 2008 Silicon Graphics, Inc.  All Rights Reserved.\n */\n\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/spinlock.h>\n#include <linux/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/device.h>\n#include <linux/io.h>\n#include <linux/uaccess.h>\n#include <linux/security.h>\n#include <linux/sync_core.h>\n#include <linux/prefetch.h>\n#include \"gru.h\"\n#include \"grutables.h\"\n#include \"grulib.h\"\n#include \"gru_instructions.h\"\n#include <asm/uv/uv_hub.h>\n\n/* Return codes for vtop functions */\n#define VTOP_SUCCESS               0\n#define VTOP_INVALID               -1\n#define VTOP_RETRY                 -2\n\n\n/*\n * Test if a physical address is a valid GRU GSEG address\n */\nstatic inline int is_gru_paddr(unsigned long paddr)\n{\n\treturn paddr >= gru_start_paddr && paddr < gru_end_paddr;\n}\n\n/*\n * Find the vma of a GRU segment. Caller must hold mmap_lock.\n */\nstruct vm_area_struct *gru_find_vma(unsigned long vaddr)\n{\n\tstruct vm_area_struct *vma;\n\n\tvma = vma_lookup(current->mm, vaddr);\n\tif (vma && vma->vm_ops == &gru_vm_ops)\n\t\treturn vma;\n\treturn NULL;\n}\n\n/*\n * Find and lock the gts that contains the specified user vaddr.\n *\n * Returns:\n * \t- *gts with the mmap_lock locked for read and the GTS locked.\n *\t- NULL if vaddr invalid OR is not a valid GSEG vaddr.\n */\n\nstatic struct gru_thread_state *gru_find_lock_gts(unsigned long vaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tstruct gru_thread_state *gts = NULL;\n\n\tmmap_read_lock(mm);\n\tvma = gru_find_vma(vaddr);\n\tif (vma)\n\t\tgts = gru_find_thread_state(vma, TSID(vaddr, vma));\n\tif (gts)\n\t\tmutex_lock(&gts->ts_ctxlock);\n\telse\n\t\tmmap_read_unlock(mm);\n\treturn gts;\n}\n\nstatic struct gru_thread_state *gru_alloc_locked_gts(unsigned long vaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tstruct gru_thread_state *gts = ERR_PTR(-EINVAL);\n\n\tmmap_write_lock(mm);\n\tvma = gru_find_vma(vaddr);\n\tif (!vma)\n\t\tgoto err;\n\n\tgts = gru_alloc_thread_state(vma, TSID(vaddr, vma));\n\tif (IS_ERR(gts))\n\t\tgoto err;\n\tmutex_lock(&gts->ts_ctxlock);\n\tmmap_write_downgrade(mm);\n\treturn gts;\n\nerr:\n\tmmap_write_unlock(mm);\n\treturn gts;\n}\n\n/*\n * Unlock a GTS that was previously locked with gru_find_lock_gts().\n */\nstatic void gru_unlock_gts(struct gru_thread_state *gts)\n{\n\tmutex_unlock(&gts->ts_ctxlock);\n\tmmap_read_unlock(current->mm);\n}\n\n/*\n * Set a CB.istatus to active using a user virtual address. This must be done\n * just prior to a TFH RESTART. The new cb.istatus is an in-cache status ONLY.\n * If the line is evicted, the status may be lost. The in-cache update\n * is necessary to prevent the user from seeing a stale cb.istatus that will\n * change as soon as the TFH restart is complete. Races may cause an\n * occasional failure to clear the cb.istatus, but that is ok.\n */\nstatic void gru_cb_set_istatus_active(struct gru_instruction_bits *cbk)\n{\n\tif (cbk) {\n\t\tcbk->istatus = CBS_ACTIVE;\n\t}\n}\n\n/*\n * Read & clear a TFM\n *\n * The GRU has an array of fault maps. A map is private to a cpu\n * Only one cpu will be accessing a cpu's fault map.\n *\n * This function scans the cpu-private fault map & clears all bits that\n * are set. The function returns a bitmap that indicates the bits that\n * were cleared. Note that sense the maps may be updated asynchronously by\n * the GRU, atomic operations must be used to clear bits.\n */\nstatic void get_clear_fault_map(struct gru_state *gru,\n\t\t\t\tstruct gru_tlb_fault_map *imap,\n\t\t\t\tstruct gru_tlb_fault_map *dmap)\n{\n\tunsigned long i, k;\n\tstruct gru_tlb_fault_map *tfm;\n\n\ttfm = get_tfm_for_cpu(gru, gru_cpu_fault_map_id());\n\tprefetchw(tfm);\t\t/* Helps on hardware, required for emulator */\n\tfor (i = 0; i < BITS_TO_LONGS(GRU_NUM_CBE); i++) {\n\t\tk = tfm->fault_bits[i];\n\t\tif (k)\n\t\t\tk = xchg(&tfm->fault_bits[i], 0UL);\n\t\timap->fault_bits[i] = k;\n\t\tk = tfm->done_bits[i];\n\t\tif (k)\n\t\t\tk = xchg(&tfm->done_bits[i], 0UL);\n\t\tdmap->fault_bits[i] = k;\n\t}\n\n\t/*\n\t * Not functionally required but helps performance. (Required\n\t * on emulator)\n\t */\n\tgru_flush_cache(tfm);\n}\n\n/*\n * Atomic (interrupt context) & non-atomic (user context) functions to\n * convert a vaddr into a physical address. The size of the page\n * is returned in pageshift.\n * \treturns:\n * \t\t  0 - successful\n * \t\t< 0 - error code\n * \t\t  1 - (atomic only) try again in non-atomic context\n */\nstatic int non_atomic_pte_lookup(struct vm_area_struct *vma,\n\t\t\t\t unsigned long vaddr, int write,\n\t\t\t\t unsigned long *paddr, int *pageshift)\n{\n\tstruct page *page;\n\n#ifdef CONFIG_HUGETLB_PAGE\n\t*pageshift = is_vm_hugetlb_page(vma) ? HPAGE_SHIFT : PAGE_SHIFT;\n#else\n\t*pageshift = PAGE_SHIFT;\n#endif\n\tif (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0, &page, NULL) <= 0)\n\t\treturn -EFAULT;\n\t*paddr = page_to_phys(page);\n\tput_page(page);\n\treturn 0;\n}\n\n/*\n * atomic_pte_lookup\n *\n * Convert a user virtual address to a physical address\n * Only supports Intel large pages (2MB only) on x86_64.\n *\tZZZ - hugepage support is incomplete\n *\n * NOTE: mmap_lock is already held on entry to this function. This\n * guarantees existence of the page tables.\n */\nstatic int atomic_pte_lookup(struct vm_area_struct *vma, unsigned long vaddr,\n\tint write, unsigned long *paddr, int *pageshift)\n{\n\tpgd_t *pgdp;\n\tp4d_t *p4dp;\n\tpud_t *pudp;\n\tpmd_t *pmdp;\n\tpte_t pte;\n\n\tpgdp = pgd_offset(vma->vm_mm, vaddr);\n\tif (unlikely(pgd_none(*pgdp)))\n\t\tgoto err;\n\n\tp4dp = p4d_offset(pgdp, vaddr);\n\tif (unlikely(p4d_none(*p4dp)))\n\t\tgoto err;\n\n\tpudp = pud_offset(p4dp, vaddr);\n\tif (unlikely(pud_none(*pudp)))\n\t\tgoto err;\n\n\tpmdp = pmd_offset(pudp, vaddr);\n\tif (unlikely(pmd_none(*pmdp)))\n\t\tgoto err;\n#ifdef CONFIG_X86_64\n\tif (unlikely(pmd_large(*pmdp)))\n\t\tpte = *(pte_t *) pmdp;\n\telse\n#endif\n\t\tpte = *pte_offset_kernel(pmdp, vaddr);\n\n\tif (unlikely(!pte_present(pte) ||\n\t\t     (write && (!pte_write(pte) || !pte_dirty(pte)))))\n\t\treturn 1;\n\n\t*paddr = pte_pfn(pte) << PAGE_SHIFT;\n#ifdef CONFIG_HUGETLB_PAGE\n\t*pageshift = is_vm_hugetlb_page(vma) ? HPAGE_SHIFT : PAGE_SHIFT;\n#else\n\t*pageshift = PAGE_SHIFT;\n#endif\n\treturn 0;\n\nerr:\n\treturn 1;\n}\n\nstatic int gru_vtop(struct gru_thread_state *gts, unsigned long vaddr,\n\t\t    int write, int atomic, unsigned long *gpa, int *pageshift)\n{\n\tstruct mm_struct *mm = gts->ts_mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long paddr;\n\tint ret, ps;\n\n\tvma = find_vma(mm, vaddr);\n\tif (!vma)\n\t\tgoto inval;\n\n\t/*\n\t * Atomic lookup is faster & usually works even if called in non-atomic\n\t * context.\n\t */\n\trmb();\t/* Must/check ms_range_active before loading PTEs */\n\tret = atomic_pte_lookup(vma, vaddr, write, &paddr, &ps);\n\tif (ret) {\n\t\tif (atomic)\n\t\t\tgoto upm;\n\t\tif (non_atomic_pte_lookup(vma, vaddr, write, &paddr, &ps))\n\t\t\tgoto inval;\n\t}\n\tif (is_gru_paddr(paddr))\n\t\tgoto inval;\n\tpaddr = paddr & ~((1UL << ps) - 1);\n\t*gpa = uv_soc_phys_ram_to_gpa(paddr);\n\t*pageshift = ps;\n\treturn VTOP_SUCCESS;\n\ninval:\n\treturn VTOP_INVALID;\nupm:\n\treturn VTOP_RETRY;\n}\n\n\n/*\n * Flush a CBE from cache. The CBE is clean in the cache. Dirty the\n * CBE cacheline so that the line will be written back to home agent.\n * Otherwise the line may be silently dropped. This has no impact\n * except on performance.\n */\nstatic void gru_flush_cache_cbe(struct gru_control_block_extended *cbe)\n{\n\tif (unlikely(cbe)) {\n\t\tcbe->cbrexecstatus = 0;         /* make CL dirty */\n\t\tgru_flush_cache(cbe);\n\t}\n}\n\n/*\n * Preload the TLB with entries that may be required. Currently, preloading\n * is implemented only for BCOPY. Preload  <tlb_preload_count> pages OR to\n * the end of the bcopy tranfer, whichever is smaller.\n */\nstatic void gru_preload_tlb(struct gru_state *gru,\n\t\t\tstruct gru_thread_state *gts, int atomic,\n\t\t\tunsigned long fault_vaddr, int asid, int write,\n\t\t\tunsigned char tlb_preload_count,\n\t\t\tstruct gru_tlb_fault_handle *tfh,\n\t\t\tstruct gru_control_block_extended *cbe)\n{\n\tunsigned long vaddr = 0, gpa;\n\tint ret, pageshift;\n\n\tif (cbe->opccpy != OP_BCOPY)\n\t\treturn;\n\n\tif (fault_vaddr == cbe->cbe_baddr0)\n\t\tvaddr = fault_vaddr + GRU_CACHE_LINE_BYTES * cbe->cbe_src_cl - 1;\n\telse if (fault_vaddr == cbe->cbe_baddr1)\n\t\tvaddr = fault_vaddr + (1 << cbe->xtypecpy) * cbe->cbe_nelemcur - 1;\n\n\tfault_vaddr &= PAGE_MASK;\n\tvaddr &= PAGE_MASK;\n\tvaddr = min(vaddr, fault_vaddr + tlb_preload_count * PAGE_SIZE);\n\n\twhile (vaddr > fault_vaddr) {\n\t\tret = gru_vtop(gts, vaddr, write, atomic, &gpa, &pageshift);\n\t\tif (ret || tfh_write_only(tfh, gpa, GAA_RAM, vaddr, asid, write,\n\t\t\t\t\t  GRU_PAGESIZE(pageshift)))\n\t\t\treturn;\n\t\tgru_dbg(grudev,\n\t\t\t\"%s: gid %d, gts 0x%p, tfh 0x%p, vaddr 0x%lx, asid 0x%x, rw %d, ps %d, gpa 0x%lx\\n\",\n\t\t\tatomic ? \"atomic\" : \"non-atomic\", gru->gs_gid, gts, tfh,\n\t\t\tvaddr, asid, write, pageshift, gpa);\n\t\tvaddr -= PAGE_SIZE;\n\t\tSTAT(tlb_preload_page);\n\t}\n}\n\n/*\n * Drop a TLB entry into the GRU. The fault is described by info in an TFH.\n *\tInput:\n *\t\tcb    Address of user CBR. Null if not running in user context\n * \tReturn:\n * \t\t  0 = dropin, exception, or switch to UPM successful\n * \t\t  1 = range invalidate active\n * \t\t< 0 = error code\n *\n */\nstatic int gru_try_dropin(struct gru_state *gru,\n\t\t\t  struct gru_thread_state *gts,\n\t\t\t  struct gru_tlb_fault_handle *tfh,\n\t\t\t  struct gru_instruction_bits *cbk)\n{\n\tstruct gru_control_block_extended *cbe = NULL;\n\tunsigned char tlb_preload_count = gts->ts_tlb_preload_count;\n\tint pageshift = 0, asid, write, ret, atomic = !cbk, indexway;\n\tunsigned long gpa = 0, vaddr = 0;\n\n\t/*\n\t * NOTE: The GRU contains magic hardware that eliminates races between\n\t * TLB invalidates and TLB dropins. If an invalidate occurs\n\t * in the window between reading the TFH and the subsequent TLB dropin,\n\t * the dropin is ignored. This eliminates the need for additional locks.\n\t */\n\n\t/*\n\t * Prefetch the CBE if doing TLB preloading\n\t */\n\tif (unlikely(tlb_preload_count)) {\n\t\tcbe = gru_tfh_to_cbe(tfh);\n\t\tprefetchw(cbe);\n\t}\n\n\t/*\n\t * Error if TFH state is IDLE or FMM mode & the user issuing a UPM call.\n\t * Might be a hardware race OR a stupid user. Ignore FMM because FMM\n\t * is a transient state.\n\t */\n\tif (tfh->status != TFHSTATUS_EXCEPTION) {\n\t\tgru_flush_cache(tfh);\n\t\tsync_core();\n\t\tif (tfh->status != TFHSTATUS_EXCEPTION)\n\t\t\tgoto failnoexception;\n\t\tSTAT(tfh_stale_on_fault);\n\t}\n\tif (tfh->state == TFHSTATE_IDLE)\n\t\tgoto failidle;\n\tif (tfh->state == TFHSTATE_MISS_FMM && cbk)\n\t\tgoto failfmm;\n\n\twrite = (tfh->cause & TFHCAUSE_TLB_MOD) != 0;\n\tvaddr = tfh->missvaddr;\n\tasid = tfh->missasid;\n\tindexway = tfh->indexway;\n\tif (asid == 0)\n\t\tgoto failnoasid;\n\n\trmb();\t/* TFH must be cache resident before reading ms_range_active */\n\n\t/*\n\t * TFH is cache resident - at least briefly. Fail the dropin\n\t * if a range invalidate is active.\n\t */\n\tif (atomic_read(&gts->ts_gms->ms_range_active))\n\t\tgoto failactive;\n\n\tret = gru_vtop(gts, vaddr, write, atomic, &gpa, &pageshift);\n\tif (ret == VTOP_INVALID)\n\t\tgoto failinval;\n\tif (ret == VTOP_RETRY)\n\t\tgoto failupm;\n\n\tif (!(gts->ts_sizeavail & GRU_SIZEAVAIL(pageshift))) {\n\t\tgts->ts_sizeavail |= GRU_SIZEAVAIL(pageshift);\n\t\tif (atomic || !gru_update_cch(gts)) {\n\t\t\tgts->ts_force_cch_reload = 1;\n\t\t\tgoto failupm;\n\t\t}\n\t}\n\n\tif (unlikely(cbe) && pageshift == PAGE_SHIFT) {\n\t\tgru_preload_tlb(gru, gts, atomic, vaddr, asid, write, tlb_preload_count, tfh, cbe);\n\t\tgru_flush_cache_cbe(cbe);\n\t}\n\n\tgru_cb_set_istatus_active(cbk);\n\tgts->ustats.tlbdropin++;\n\ttfh_write_restart(tfh, gpa, GAA_RAM, vaddr, asid, write,\n\t\t\t  GRU_PAGESIZE(pageshift));\n\tgru_dbg(grudev,\n\t\t\"%s: gid %d, gts 0x%p, tfh 0x%p, vaddr 0x%lx, asid 0x%x, indexway 0x%x,\"\n\t\t\" rw %d, ps %d, gpa 0x%lx\\n\",\n\t\tatomic ? \"atomic\" : \"non-atomic\", gru->gs_gid, gts, tfh, vaddr, asid,\n\t\tindexway, write, pageshift, gpa);\n\tSTAT(tlb_dropin);\n\treturn 0;\n\nfailnoasid:\n\t/* No asid (delayed unload). */\n\tSTAT(tlb_dropin_fail_no_asid);\n\tgru_dbg(grudev, \"FAILED no_asid tfh: 0x%p, vaddr 0x%lx\\n\", tfh, vaddr);\n\tif (!cbk)\n\t\ttfh_user_polling_mode(tfh);\n\telse\n\t\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\treturn -EAGAIN;\n\nfailupm:\n\t/* Atomic failure switch CBR to UPM */\n\ttfh_user_polling_mode(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_upm);\n\tgru_dbg(grudev, \"FAILED upm tfh: 0x%p, vaddr 0x%lx\\n\", tfh, vaddr);\n\treturn 1;\n\nfailfmm:\n\t/* FMM state on UPM call */\n\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_fmm);\n\tgru_dbg(grudev, \"FAILED fmm tfh: 0x%p, state %d\\n\", tfh, tfh->state);\n\treturn 0;\n\nfailnoexception:\n\t/* TFH status did not show exception pending */\n\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tif (cbk)\n\t\tgru_flush_cache(cbk);\n\tSTAT(tlb_dropin_fail_no_exception);\n\tgru_dbg(grudev, \"FAILED non-exception tfh: 0x%p, status %d, state %d\\n\",\n\t\ttfh, tfh->status, tfh->state);\n\treturn 0;\n\nfailidle:\n\t/* TFH state was idle  - no miss pending */\n\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tif (cbk)\n\t\tgru_flush_cache(cbk);\n\tSTAT(tlb_dropin_fail_idle);\n\tgru_dbg(grudev, \"FAILED idle tfh: 0x%p, state %d\\n\", tfh, tfh->state);\n\treturn 0;\n\nfailinval:\n\t/* All errors (atomic & non-atomic) switch CBR to EXCEPTION state */\n\ttfh_exception(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_invalid);\n\tgru_dbg(grudev, \"FAILED inval tfh: 0x%p, vaddr 0x%lx\\n\", tfh, vaddr);\n\treturn -EFAULT;\n\nfailactive:\n\t/* Range invalidate active. Switch to UPM iff atomic */\n\tif (!cbk)\n\t\ttfh_user_polling_mode(tfh);\n\telse\n\t\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_range_active);\n\tgru_dbg(grudev, \"FAILED range active: tfh 0x%p, vaddr 0x%lx\\n\",\n\t\ttfh, vaddr);\n\treturn 1;\n}\n\n/*\n * Process an external interrupt from the GRU. This interrupt is\n * caused by a TLB miss.\n * Note that this is the interrupt handler that is registered with linux\n * interrupt handlers.\n */\nstatic irqreturn_t gru_intr(int chiplet, int blade)\n{\n\tstruct gru_state *gru;\n\tstruct gru_tlb_fault_map imap, dmap;\n\tstruct gru_thread_state *gts;\n\tstruct gru_tlb_fault_handle *tfh = NULL;\n\tstruct completion *cmp;\n\tint cbrnum, ctxnum;\n\n\tSTAT(intr);\n\n\tgru = &gru_base[blade]->bs_grus[chiplet];\n\tif (!gru) {\n\t\tdev_err(grudev, \"GRU: invalid interrupt: cpu %d, chiplet %d\\n\",\n\t\t\traw_smp_processor_id(), chiplet);\n\t\treturn IRQ_NONE;\n\t}\n\tget_clear_fault_map(gru, &imap, &dmap);\n\tgru_dbg(grudev,\n\t\t\"cpu %d, chiplet %d, gid %d, imap %016lx %016lx, dmap %016lx %016lx\\n\",\n\t\tsmp_processor_id(), chiplet, gru->gs_gid,\n\t\timap.fault_bits[0], imap.fault_bits[1],\n\t\tdmap.fault_bits[0], dmap.fault_bits[1]);\n\n\tfor_each_cbr_in_tfm(cbrnum, dmap.fault_bits) {\n\t\tSTAT(intr_cbr);\n\t\tcmp = gru->gs_blade->bs_async_wq;\n\t\tif (cmp)\n\t\t\tcomplete(cmp);\n\t\tgru_dbg(grudev, \"gid %d, cbr_done %d, done %d\\n\",\n\t\t\tgru->gs_gid, cbrnum, cmp ? cmp->done : -1);\n\t}\n\n\tfor_each_cbr_in_tfm(cbrnum, imap.fault_bits) {\n\t\tSTAT(intr_tfh);\n\t\ttfh = get_tfh_by_index(gru, cbrnum);\n\t\tprefetchw(tfh);\t/* Helps on hdw, required for emulator */\n\n\t\t/*\n\t\t * When hardware sets a bit in the faultmap, it implicitly\n\t\t * locks the GRU context so that it cannot be unloaded.\n\t\t * The gts cannot change until a TFH start/writestart command\n\t\t * is issued.\n\t\t */\n\t\tctxnum = tfh->ctxnum;\n\t\tgts = gru->gs_gts[ctxnum];\n\n\t\t/* Spurious interrupts can cause this. Ignore. */\n\t\tif (!gts) {\n\t\t\tSTAT(intr_spurious);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * This is running in interrupt context. Trylock the mmap_lock.\n\t\t * If it fails, retry the fault in user context.\n\t\t */\n\t\tgts->ustats.fmm_tlbmiss++;\n\t\tif (!gts->ts_force_cch_reload &&\n\t\t\t\t\tmmap_read_trylock(gts->ts_mm)) {\n\t\t\tgru_try_dropin(gru, gts, tfh, NULL);\n\t\t\tmmap_read_unlock(gts->ts_mm);\n\t\t} else {\n\t\t\ttfh_user_polling_mode(tfh);\n\t\t\tSTAT(intr_mm_lock_failed);\n\t\t}\n\t}\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t gru0_intr(int irq, void *dev_id)\n{\n\treturn gru_intr(0, uv_numa_blade_id());\n}\n\nirqreturn_t gru1_intr(int irq, void *dev_id)\n{\n\treturn gru_intr(1, uv_numa_blade_id());\n}\n\nirqreturn_t gru_intr_mblade(int irq, void *dev_id)\n{\n\tint blade;\n\n\tfor_each_possible_blade(blade) {\n\t\tif (uv_blade_nr_possible_cpus(blade))\n\t\t\tcontinue;\n\t\tgru_intr(0, blade);\n\t\tgru_intr(1, blade);\n\t}\n\treturn IRQ_HANDLED;\n}\n\n\nstatic int gru_user_dropin(struct gru_thread_state *gts,\n\t\t\t   struct gru_tlb_fault_handle *tfh,\n\t\t\t   void *cb)\n{\n\tstruct gru_mm_struct *gms = gts->ts_gms;\n\tint ret;\n\n\tgts->ustats.upm_tlbmiss++;\n\twhile (1) {\n\t\twait_event(gms->ms_wait_queue,\n\t\t\t   atomic_read(&gms->ms_range_active) == 0);\n\t\tprefetchw(tfh);\t/* Helps on hdw, required for emulator */\n\t\tret = gru_try_dropin(gts->ts_gru, gts, tfh, cb);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\t\tSTAT(call_os_wait_queue);\n\t}\n}\n\n/*\n * This interface is called as a result of a user detecting a \"call OS\" bit\n * in a user CB. Normally means that a TLB fault has occurred.\n * \tcb - user virtual address of the CB\n */\nint gru_handle_user_call_os(unsigned long cb)\n{\n\tstruct gru_tlb_fault_handle *tfh;\n\tstruct gru_thread_state *gts;\n\tvoid *cbk;\n\tint ucbnum, cbrnum, ret = -EINVAL;\n\n\tSTAT(call_os);\n\n\t/* sanity check the cb pointer */\n\tucbnum = get_cb_number((void *)cb);\n\tif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\n\t\treturn -EINVAL;\n\nagain:\n\tgts = gru_find_lock_gts(cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\n\t\tgoto exit;\n\n\tif (gru_check_context_placement(gts)) {\n\t\tgru_unlock_gts(gts);\n\t\tgru_unload_context(gts, 1);\n\t\tgoto again;\n\t}\n\n\t/*\n\t * CCH may contain stale data if ts_force_cch_reload is set.\n\t */\n\tif (gts->ts_gru && gts->ts_force_cch_reload) {\n\t\tgts->ts_force_cch_reload = 0;\n\t\tgru_update_cch(gts);\n\t}\n\n\tret = -EAGAIN;\n\tcbrnum = thread_cbr_number(gts, ucbnum);\n\tif (gts->ts_gru) {\n\t\ttfh = get_tfh_by_index(gts->ts_gru, cbrnum);\n\t\tcbk = get_gseg_base_address_cb(gts->ts_gru->gs_gru_base_vaddr,\n\t\t\t\tgts->ts_ctxnum, ucbnum);\n\t\tret = gru_user_dropin(gts, tfh, cbk);\n\t}\nexit:\n\tgru_unlock_gts(gts);\n\treturn ret;\n}\n\n/*\n * Fetch the exception detail information for a CB that terminated with\n * an exception.\n */\nint gru_get_exception_detail(unsigned long arg)\n{\n\tstruct control_block_extended_exc_detail excdet;\n\tstruct gru_control_block_extended *cbe;\n\tstruct gru_thread_state *gts;\n\tint ucbnum, cbrnum, ret;\n\n\tSTAT(user_exception);\n\tif (copy_from_user(&excdet, (void __user *)arg, sizeof(excdet)))\n\t\treturn -EFAULT;\n\n\tgts = gru_find_lock_gts(excdet.cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", excdet.cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\tucbnum = get_cb_number((void *)excdet.cb);\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE) {\n\t\tret = -EINVAL;\n\t} else if (gts->ts_gru) {\n\t\tcbrnum = thread_cbr_number(gts, ucbnum);\n\t\tcbe = get_cbe_by_index(gts->ts_gru, cbrnum);\n\t\tgru_flush_cache(cbe);\t/* CBE not coherent */\n\t\tsync_core();\t\t/* make sure we are have current data */\n\t\texcdet.opc = cbe->opccpy;\n\t\texcdet.exopc = cbe->exopccpy;\n\t\texcdet.ecause = cbe->ecause;\n\t\texcdet.exceptdet0 = cbe->idef1upd;\n\t\texcdet.exceptdet1 = cbe->idef3upd;\n\t\texcdet.cbrstate = cbe->cbrstate;\n\t\texcdet.cbrexecstatus = cbe->cbrexecstatus;\n\t\tgru_flush_cache_cbe(cbe);\n\t\tret = 0;\n\t} else {\n\t\tret = -EAGAIN;\n\t}\n\tgru_unlock_gts(gts);\n\n\tgru_dbg(grudev,\n\t\t\"cb 0x%lx, op %d, exopc %d, cbrstate %d, cbrexecstatus 0x%x, ecause 0x%x, \"\n\t\t\"exdet0 0x%lx, exdet1 0x%x\\n\",\n\t\texcdet.cb, excdet.opc, excdet.exopc, excdet.cbrstate, excdet.cbrexecstatus,\n\t\texcdet.ecause, excdet.exceptdet0, excdet.exceptdet1);\n\tif (!ret && copy_to_user((void __user *)arg, &excdet, sizeof(excdet)))\n\t\tret = -EFAULT;\n\treturn ret;\n}\n\n/*\n * User request to unload a context. Content is saved for possible reload.\n */\nstatic int gru_unload_all_contexts(void)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_state *gru;\n\tint gid, ctxnum;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tforeach_gid(gid) {\n\t\tgru = GID_TO_GRU(gid);\n\t\tspin_lock(&gru->gs_lock);\n\t\tfor (ctxnum = 0; ctxnum < GRU_NUM_CCH; ctxnum++) {\n\t\t\tgts = gru->gs_gts[ctxnum];\n\t\t\tif (gts && mutex_trylock(&gts->ts_ctxlock)) {\n\t\t\t\tspin_unlock(&gru->gs_lock);\n\t\t\t\tgru_unload_context(gts, 1);\n\t\t\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\t\t\tspin_lock(&gru->gs_lock);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&gru->gs_lock);\n\t}\n\treturn 0;\n}\n\nint gru_user_unload_context(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_unload_context_req req;\n\n\tSTAT(user_unload_context);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\n\tgru_dbg(grudev, \"gseg 0x%lx\\n\", req.gseg);\n\n\tif (!req.gseg)\n\t\treturn gru_unload_all_contexts();\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts)\n\t\treturn -EINVAL;\n\n\tif (gts->ts_gru)\n\t\tgru_unload_context(gts, 1);\n\tgru_unlock_gts(gts);\n\n\treturn 0;\n}\n\n/*\n * User request to flush a range of virtual addresses from the GRU TLB\n * (Mainly for testing).\n */\nint gru_user_flush_tlb(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_flush_tlb_req req;\n\tstruct gru_mm_struct *gms;\n\n\tSTAT(user_flush_tlb);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\n\tgru_dbg(grudev, \"gseg 0x%lx, vaddr 0x%lx, len 0x%lx\\n\", req.gseg,\n\t\treq.vaddr, req.len);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts)\n\t\treturn -EINVAL;\n\n\tgms = gts->ts_gms;\n\tgru_unlock_gts(gts);\n\tgru_flush_tlb_range(gms, req.vaddr, req.len);\n\n\treturn 0;\n}\n\n/*\n * Fetch GSEG statisticss\n */\nlong gru_get_gseg_statistics(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_get_gseg_statistics_req req;\n\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * The library creates arrays of contexts for threaded programs.\n\t * If no gts exists in the array, the context has never been used & all\n\t * statistics are implicitly 0.\n\t */\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (gts) {\n\t\tmemcpy(&req.stats, &gts->ustats, sizeof(gts->ustats));\n\t\tgru_unlock_gts(gts);\n\t} else {\n\t\tmemset(&req.stats, 0, sizeof(gts->ustats));\n\t}\n\n\tif (copy_to_user((void __user *)arg, &req, sizeof(req)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n/*\n * Register the current task as the user of the GSEG slice.\n * Needed for TLB fault interrupt targeting.\n */\nint gru_set_context_option(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_set_context_option_req req;\n\tint ret = 0;\n\n\tSTAT(set_context_option);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\tgru_dbg(grudev, \"op %d, gseg 0x%lx, value1 0x%lx\\n\", req.op, req.gseg, req.val1);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts) {\n\t\tgts = gru_alloc_locked_gts(req.gseg);\n\t\tif (IS_ERR(gts))\n\t\t\treturn PTR_ERR(gts);\n\t}\n\n\tswitch (req.op) {\n\tcase sco_blade_chiplet:\n\t\t/* Select blade/chiplet for GRU context */\n\t\tif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\n\t\t    req.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\n\t\t    (req.val1 >= 0 && !gru_base[req.val1])) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tgts->ts_user_blade_id = req.val1;\n\t\t\tgts->ts_user_chiplet_id = req.val0;\n\t\t\tif (gru_check_context_placement(gts)) {\n\t\t\t\tgru_unlock_gts(gts);\n\t\t\t\tgru_unload_context(gts, 1);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase sco_gseg_owner:\n \t\t/* Register the current task as the GSEG owner */\n\t\tgts->ts_tgid_owner = current->tgid;\n\t\tbreak;\n\tcase sco_cch_req_slice:\n \t\t/* Set the CCH slice option */\n\t\tgts->ts_cch_req_slice = req.val1 & 3;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tgru_unlock_gts(gts);\n\n\treturn ret;\n}\n", "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * SN Platform GRU Driver\n *\n *            DRIVER TABLE MANAGER + GRU CONTEXT LOAD/UNLOAD\n *\n *  Copyright (c) 2008 Silicon Graphics, Inc.  All Rights Reserved.\n */\n\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/spinlock.h>\n#include <linux/sched.h>\n#include <linux/device.h>\n#include <linux/list.h>\n#include <linux/err.h>\n#include <linux/prefetch.h>\n#include <asm/uv/uv_hub.h>\n#include \"gru.h\"\n#include \"grutables.h\"\n#include \"gruhandles.h\"\n\nunsigned long gru_options __read_mostly;\n\nstatic struct device_driver gru_driver = {\n\t.name = \"gru\"\n};\n\nstatic struct device gru_device = {\n\t.init_name = \"\",\n\t.driver = &gru_driver,\n};\n\nstruct device *grudev = &gru_device;\n\n/*\n * Select a gru fault map to be used by the current cpu. Note that\n * multiple cpus may be using the same map.\n *\tZZZ should be inline but did not work on emulator\n */\nint gru_cpu_fault_map_id(void)\n{\n#ifdef CONFIG_IA64\n\treturn uv_blade_processor_id() % GRU_NUM_TFM;\n#else\n\tint cpu = smp_processor_id();\n\tint id, core;\n\n\tcore = uv_cpu_core_number(cpu);\n\tid = core + UV_MAX_INT_CORES * uv_cpu_socket_number(cpu);\n\treturn id;\n#endif\n}\n\n/*--------- ASID Management -------------------------------------------\n *\n *  Initially, assign asids sequentially from MIN_ASID .. MAX_ASID.\n *  Once MAX is reached, flush the TLB & start over. However,\n *  some asids may still be in use. There won't be many (percentage wise) still\n *  in use. Search active contexts & determine the value of the first\n *  asid in use (\"x\"s below). Set \"limit\" to this value.\n *  This defines a block of assignable asids.\n *\n *  When \"limit\" is reached, search forward from limit+1 and determine the\n *  next block of assignable asids.\n *\n *  Repeat until MAX_ASID is reached, then start over again.\n *\n *  Each time MAX_ASID is reached, increment the asid generation. Since\n *  the search for in-use asids only checks contexts with GRUs currently\n *  assigned, asids in some contexts will be missed. Prior to loading\n *  a context, the asid generation of the GTS asid is rechecked. If it\n *  doesn't match the current generation, a new asid will be assigned.\n *\n *   \t0---------------x------------x---------------------x----|\n *\t  ^-next\t^-limit\t   \t\t\t\t^-MAX_ASID\n *\n * All asid manipulation & context loading/unloading is protected by the\n * gs_lock.\n */\n\n/* Hit the asid limit. Start over */\nstatic int gru_wrap_asid(struct gru_state *gru)\n{\n\tgru_dbg(grudev, \"gid %d\\n\", gru->gs_gid);\n\tSTAT(asid_wrap);\n\tgru->gs_asid_gen++;\n\treturn MIN_ASID;\n}\n\n/* Find the next chunk of unused asids */\nstatic int gru_reset_asid_limit(struct gru_state *gru, int asid)\n{\n\tint i, gid, inuse_asid, limit;\n\n\tgru_dbg(grudev, \"gid %d, asid 0x%x\\n\", gru->gs_gid, asid);\n\tSTAT(asid_next);\n\tlimit = MAX_ASID;\n\tif (asid >= limit)\n\t\tasid = gru_wrap_asid(gru);\n\tgru_flush_all_tlb(gru);\n\tgid = gru->gs_gid;\nagain:\n\tfor (i = 0; i < GRU_NUM_CCH; i++) {\n\t\tif (!gru->gs_gts[i] || is_kernel_context(gru->gs_gts[i]))\n\t\t\tcontinue;\n\t\tinuse_asid = gru->gs_gts[i]->ts_gms->ms_asids[gid].mt_asid;\n\t\tgru_dbg(grudev, \"gid %d, gts %p, gms %p, inuse 0x%x, cxt %d\\n\",\n\t\t\tgru->gs_gid, gru->gs_gts[i], gru->gs_gts[i]->ts_gms,\n\t\t\tinuse_asid, i);\n\t\tif (inuse_asid == asid) {\n\t\t\tasid += ASID_INC;\n\t\t\tif (asid >= limit) {\n\t\t\t\t/*\n\t\t\t\t * empty range: reset the range limit and\n\t\t\t\t * start over\n\t\t\t\t */\n\t\t\t\tlimit = MAX_ASID;\n\t\t\t\tif (asid >= MAX_ASID)\n\t\t\t\t\tasid = gru_wrap_asid(gru);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\n\t\tif ((inuse_asid > asid) && (inuse_asid < limit))\n\t\t\tlimit = inuse_asid;\n\t}\n\tgru->gs_asid_limit = limit;\n\tgru->gs_asid = asid;\n\tgru_dbg(grudev, \"gid %d, new asid 0x%x, new_limit 0x%x\\n\", gru->gs_gid,\n\t\t\t\t\tasid, limit);\n\treturn asid;\n}\n\n/* Assign a new ASID to a thread context.  */\nstatic int gru_assign_asid(struct gru_state *gru)\n{\n\tint asid;\n\n\tgru->gs_asid += ASID_INC;\n\tasid = gru->gs_asid;\n\tif (asid >= gru->gs_asid_limit)\n\t\tasid = gru_reset_asid_limit(gru, asid);\n\n\tgru_dbg(grudev, \"gid %d, asid 0x%x\\n\", gru->gs_gid, asid);\n\treturn asid;\n}\n\n/*\n * Clear n bits in a word. Return a word indicating the bits that were cleared.\n * Optionally, build an array of chars that contain the bit numbers allocated.\n */\nstatic unsigned long reserve_resources(unsigned long *p, int n, int mmax,\n\t\t\t\t       signed char *idx)\n{\n\tunsigned long bits = 0;\n\tint i;\n\n\twhile (n--) {\n\t\ti = find_first_bit(p, mmax);\n\t\tif (i == mmax)\n\t\t\tBUG();\n\t\t__clear_bit(i, p);\n\t\t__set_bit(i, &bits);\n\t\tif (idx)\n\t\t\t*idx++ = i;\n\t}\n\treturn bits;\n}\n\nunsigned long gru_reserve_cb_resources(struct gru_state *gru, int cbr_au_count,\n\t\t\t\t       signed char *cbmap)\n{\n\treturn reserve_resources(&gru->gs_cbr_map, cbr_au_count, GRU_CBR_AU,\n\t\t\t\t cbmap);\n}\n\nunsigned long gru_reserve_ds_resources(struct gru_state *gru, int dsr_au_count,\n\t\t\t\t       signed char *dsmap)\n{\n\treturn reserve_resources(&gru->gs_dsr_map, dsr_au_count, GRU_DSR_AU,\n\t\t\t\t dsmap);\n}\n\nstatic void reserve_gru_resources(struct gru_state *gru,\n\t\t\t\t  struct gru_thread_state *gts)\n{\n\tgru->gs_active_contexts++;\n\tgts->ts_cbr_map =\n\t    gru_reserve_cb_resources(gru, gts->ts_cbr_au_count,\n\t\t\t\t     gts->ts_cbr_idx);\n\tgts->ts_dsr_map =\n\t    gru_reserve_ds_resources(gru, gts->ts_dsr_au_count, NULL);\n}\n\nstatic void free_gru_resources(struct gru_state *gru,\n\t\t\t       struct gru_thread_state *gts)\n{\n\tgru->gs_active_contexts--;\n\tgru->gs_cbr_map |= gts->ts_cbr_map;\n\tgru->gs_dsr_map |= gts->ts_dsr_map;\n}\n\n/*\n * Check if a GRU has sufficient free resources to satisfy an allocation\n * request. Note: GRU locks may or may not be held when this is called. If\n * not held, recheck after acquiring the appropriate locks.\n *\n * Returns 1 if sufficient resources, 0 if not\n */\nstatic int check_gru_resources(struct gru_state *gru, int cbr_au_count,\n\t\t\t       int dsr_au_count, int max_active_contexts)\n{\n\treturn hweight64(gru->gs_cbr_map) >= cbr_au_count\n\t\t&& hweight64(gru->gs_dsr_map) >= dsr_au_count\n\t\t&& gru->gs_active_contexts < max_active_contexts;\n}\n\n/*\n * TLB manangment requires tracking all GRU chiplets that have loaded a GSEG\n * context.\n */\nstatic int gru_load_mm_tracker(struct gru_state *gru,\n\t\t\t\t\tstruct gru_thread_state *gts)\n{\n\tstruct gru_mm_struct *gms = gts->ts_gms;\n\tstruct gru_mm_tracker *asids = &gms->ms_asids[gru->gs_gid];\n\tunsigned short ctxbitmap = (1 << gts->ts_ctxnum);\n\tint asid;\n\n\tspin_lock(&gms->ms_asid_lock);\n\tasid = asids->mt_asid;\n\n\tspin_lock(&gru->gs_asid_lock);\n\tif (asid == 0 || (asids->mt_ctxbitmap == 0 && asids->mt_asid_gen !=\n\t\t\t  gru->gs_asid_gen)) {\n\t\tasid = gru_assign_asid(gru);\n\t\tasids->mt_asid = asid;\n\t\tasids->mt_asid_gen = gru->gs_asid_gen;\n\t\tSTAT(asid_new);\n\t} else {\n\t\tSTAT(asid_reuse);\n\t}\n\tspin_unlock(&gru->gs_asid_lock);\n\n\tBUG_ON(asids->mt_ctxbitmap & ctxbitmap);\n\tasids->mt_ctxbitmap |= ctxbitmap;\n\tif (!test_bit(gru->gs_gid, gms->ms_asidmap))\n\t\t__set_bit(gru->gs_gid, gms->ms_asidmap);\n\tspin_unlock(&gms->ms_asid_lock);\n\n\tgru_dbg(grudev,\n\t\t\"gid %d, gts %p, gms %p, ctxnum %d, asid 0x%x, asidmap 0x%lx\\n\",\n\t\tgru->gs_gid, gts, gms, gts->ts_ctxnum, asid,\n\t\tgms->ms_asidmap[0]);\n\treturn asid;\n}\n\nstatic void gru_unload_mm_tracker(struct gru_state *gru,\n\t\t\t\t\tstruct gru_thread_state *gts)\n{\n\tstruct gru_mm_struct *gms = gts->ts_gms;\n\tstruct gru_mm_tracker *asids;\n\tunsigned short ctxbitmap;\n\n\tasids = &gms->ms_asids[gru->gs_gid];\n\tctxbitmap = (1 << gts->ts_ctxnum);\n\tspin_lock(&gms->ms_asid_lock);\n\tspin_lock(&gru->gs_asid_lock);\n\tBUG_ON((asids->mt_ctxbitmap & ctxbitmap) != ctxbitmap);\n\tasids->mt_ctxbitmap ^= ctxbitmap;\n\tgru_dbg(grudev, \"gid %d, gts %p, gms %p, ctxnum %d, asidmap 0x%lx\\n\",\n\t\tgru->gs_gid, gts, gms, gts->ts_ctxnum, gms->ms_asidmap[0]);\n\tspin_unlock(&gru->gs_asid_lock);\n\tspin_unlock(&gms->ms_asid_lock);\n}\n\n/*\n * Decrement the reference count on a GTS structure. Free the structure\n * if the reference count goes to zero.\n */\nvoid gts_drop(struct gru_thread_state *gts)\n{\n\tif (gts && refcount_dec_and_test(&gts->ts_refcnt)) {\n\t\tif (gts->ts_gms)\n\t\t\tgru_drop_mmu_notifier(gts->ts_gms);\n\t\tkfree(gts);\n\t\tSTAT(gts_free);\n\t}\n}\n\n/*\n * Locate the GTS structure for the current thread.\n */\nstatic struct gru_thread_state *gru_find_current_gts_nolock(struct gru_vma_data\n\t\t\t    *vdata, int tsid)\n{\n\tstruct gru_thread_state *gts;\n\n\tlist_for_each_entry(gts, &vdata->vd_head, ts_next)\n\t    if (gts->ts_tsid == tsid)\n\t\treturn gts;\n\treturn NULL;\n}\n\n/*\n * Allocate a thread state structure.\n */\nstruct gru_thread_state *gru_alloc_gts(struct vm_area_struct *vma,\n\t\tint cbr_au_count, int dsr_au_count,\n\t\tunsigned char tlb_preload_count, int options, int tsid)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_mm_struct *gms;\n\tint bytes;\n\n\tbytes = DSR_BYTES(dsr_au_count) + CBR_BYTES(cbr_au_count);\n\tbytes += sizeof(struct gru_thread_state);\n\tgts = kmalloc(bytes, GFP_KERNEL);\n\tif (!gts)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tSTAT(gts_alloc);\n\tmemset(gts, 0, sizeof(struct gru_thread_state)); /* zero out header */\n\trefcount_set(&gts->ts_refcnt, 1);\n\tmutex_init(&gts->ts_ctxlock);\n\tgts->ts_cbr_au_count = cbr_au_count;\n\tgts->ts_dsr_au_count = dsr_au_count;\n\tgts->ts_tlb_preload_count = tlb_preload_count;\n\tgts->ts_user_options = options;\n\tgts->ts_user_blade_id = -1;\n\tgts->ts_user_chiplet_id = -1;\n\tgts->ts_tsid = tsid;\n\tgts->ts_ctxnum = NULLCTX;\n\tgts->ts_tlb_int_select = -1;\n\tgts->ts_cch_req_slice = -1;\n\tgts->ts_sizeavail = GRU_SIZEAVAIL(PAGE_SHIFT);\n\tif (vma) {\n\t\tgts->ts_mm = current->mm;\n\t\tgts->ts_vma = vma;\n\t\tgms = gru_register_mmu_notifier();\n\t\tif (IS_ERR(gms))\n\t\t\tgoto err;\n\t\tgts->ts_gms = gms;\n\t}\n\n\tgru_dbg(grudev, \"alloc gts %p\\n\", gts);\n\treturn gts;\n\nerr:\n\tgts_drop(gts);\n\treturn ERR_CAST(gms);\n}\n\n/*\n * Allocate a vma private data structure.\n */\nstruct gru_vma_data *gru_alloc_vma_data(struct vm_area_struct *vma, int tsid)\n{\n\tstruct gru_vma_data *vdata = NULL;\n\n\tvdata = kmalloc(sizeof(*vdata), GFP_KERNEL);\n\tif (!vdata)\n\t\treturn NULL;\n\n\tSTAT(vdata_alloc);\n\tINIT_LIST_HEAD(&vdata->vd_head);\n\tspin_lock_init(&vdata->vd_lock);\n\tgru_dbg(grudev, \"alloc vdata %p\\n\", vdata);\n\treturn vdata;\n}\n\n/*\n * Find the thread state structure for the current thread.\n */\nstruct gru_thread_state *gru_find_thread_state(struct vm_area_struct *vma,\n\t\t\t\t\tint tsid)\n{\n\tstruct gru_vma_data *vdata = vma->vm_private_data;\n\tstruct gru_thread_state *gts;\n\n\tspin_lock(&vdata->vd_lock);\n\tgts = gru_find_current_gts_nolock(vdata, tsid);\n\tspin_unlock(&vdata->vd_lock);\n\tgru_dbg(grudev, \"vma %p, gts %p\\n\", vma, gts);\n\treturn gts;\n}\n\n/*\n * Allocate a new thread state for a GSEG. Note that races may allow\n * another thread to race to create a gts.\n */\nstruct gru_thread_state *gru_alloc_thread_state(struct vm_area_struct *vma,\n\t\t\t\t\tint tsid)\n{\n\tstruct gru_vma_data *vdata = vma->vm_private_data;\n\tstruct gru_thread_state *gts, *ngts;\n\n\tgts = gru_alloc_gts(vma, vdata->vd_cbr_au_count,\n\t\t\t    vdata->vd_dsr_au_count,\n\t\t\t    vdata->vd_tlb_preload_count,\n\t\t\t    vdata->vd_user_options, tsid);\n\tif (IS_ERR(gts))\n\t\treturn gts;\n\n\tspin_lock(&vdata->vd_lock);\n\tngts = gru_find_current_gts_nolock(vdata, tsid);\n\tif (ngts) {\n\t\tgts_drop(gts);\n\t\tgts = ngts;\n\t\tSTAT(gts_double_allocate);\n\t} else {\n\t\tlist_add(&gts->ts_next, &vdata->vd_head);\n\t}\n\tspin_unlock(&vdata->vd_lock);\n\tgru_dbg(grudev, \"vma %p, gts %p\\n\", vma, gts);\n\treturn gts;\n}\n\n/*\n * Free the GRU context assigned to the thread state.\n */\nstatic void gru_free_gru_context(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru;\n\n\tgru = gts->ts_gru;\n\tgru_dbg(grudev, \"gts %p, gid %d\\n\", gts, gru->gs_gid);\n\n\tspin_lock(&gru->gs_lock);\n\tgru->gs_gts[gts->ts_ctxnum] = NULL;\n\tfree_gru_resources(gru, gts);\n\tBUG_ON(test_bit(gts->ts_ctxnum, &gru->gs_context_map) == 0);\n\t__clear_bit(gts->ts_ctxnum, &gru->gs_context_map);\n\tgts->ts_ctxnum = NULLCTX;\n\tgts->ts_gru = NULL;\n\tgts->ts_blade = -1;\n\tspin_unlock(&gru->gs_lock);\n\n\tgts_drop(gts);\n\tSTAT(free_context);\n}\n\n/*\n * Prefetching cachelines help hardware performance.\n * (Strictly a performance enhancement. Not functionally required).\n */\nstatic void prefetch_data(void *p, int num, int stride)\n{\n\twhile (num-- > 0) {\n\t\tprefetchw(p);\n\t\tp += stride;\n\t}\n}\n\nstatic inline long gru_copy_handle(void *d, void *s)\n{\n\tmemcpy(d, s, GRU_HANDLE_BYTES);\n\treturn GRU_HANDLE_BYTES;\n}\n\nstatic void gru_prefetch_context(void *gseg, void *cb, void *cbe,\n\t\t\t\tunsigned long cbrmap, unsigned long length)\n{\n\tint i, scr;\n\n\tprefetch_data(gseg + GRU_DS_BASE, length / GRU_CACHE_LINE_BYTES,\n\t\t      GRU_CACHE_LINE_BYTES);\n\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr) {\n\t\tprefetch_data(cb, 1, GRU_CACHE_LINE_BYTES);\n\t\tprefetch_data(cbe + i * GRU_HANDLE_STRIDE, 1,\n\t\t\t      GRU_CACHE_LINE_BYTES);\n\t\tcb += GRU_HANDLE_STRIDE;\n\t}\n}\n\nstatic void gru_load_context_data(void *save, void *grubase, int ctxnum,\n\t\t\t\t  unsigned long cbrmap, unsigned long dsrmap,\n\t\t\t\t  int data_valid)\n{\n\tvoid *gseg, *cb, *cbe;\n\tunsigned long length;\n\tint i, scr;\n\n\tgseg = grubase + ctxnum * GRU_GSEG_STRIDE;\n\tcb = gseg + GRU_CB_BASE;\n\tcbe = grubase + GRU_CBE_BASE;\n\tlength = hweight64(dsrmap) * GRU_DSR_AU_BYTES;\n\tgru_prefetch_context(gseg, cb, cbe, cbrmap, length);\n\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr) {\n\t\tif (data_valid) {\n\t\t\tsave += gru_copy_handle(cb, save);\n\t\t\tsave += gru_copy_handle(cbe + i * GRU_HANDLE_STRIDE,\n\t\t\t\t\t\tsave);\n\t\t} else {\n\t\t\tmemset(cb, 0, GRU_CACHE_LINE_BYTES);\n\t\t\tmemset(cbe + i * GRU_HANDLE_STRIDE, 0,\n\t\t\t\t\t\tGRU_CACHE_LINE_BYTES);\n\t\t}\n\t\t/* Flush CBE to hide race in context restart */\n\t\tmb();\n\t\tgru_flush_cache(cbe + i * GRU_HANDLE_STRIDE);\n\t\tcb += GRU_HANDLE_STRIDE;\n\t}\n\n\tif (data_valid)\n\t\tmemcpy(gseg + GRU_DS_BASE, save, length);\n\telse\n\t\tmemset(gseg + GRU_DS_BASE, 0, length);\n}\n\nstatic void gru_unload_context_data(void *save, void *grubase, int ctxnum,\n\t\t\t\t    unsigned long cbrmap, unsigned long dsrmap)\n{\n\tvoid *gseg, *cb, *cbe;\n\tunsigned long length;\n\tint i, scr;\n\n\tgseg = grubase + ctxnum * GRU_GSEG_STRIDE;\n\tcb = gseg + GRU_CB_BASE;\n\tcbe = grubase + GRU_CBE_BASE;\n\tlength = hweight64(dsrmap) * GRU_DSR_AU_BYTES;\n\n\t/* CBEs may not be coherent. Flush them from cache */\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr)\n\t\tgru_flush_cache(cbe + i * GRU_HANDLE_STRIDE);\n\tmb();\t\t/* Let the CL flush complete */\n\n\tgru_prefetch_context(gseg, cb, cbe, cbrmap, length);\n\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr) {\n\t\tsave += gru_copy_handle(save, cb);\n\t\tsave += gru_copy_handle(save, cbe + i * GRU_HANDLE_STRIDE);\n\t\tcb += GRU_HANDLE_STRIDE;\n\t}\n\tmemcpy(save, gseg + GRU_DS_BASE, length);\n}\n\nvoid gru_unload_context(struct gru_thread_state *gts, int savestate)\n{\n\tstruct gru_state *gru = gts->ts_gru;\n\tstruct gru_context_configuration_handle *cch;\n\tint ctxnum = gts->ts_ctxnum;\n\n\tif (!is_kernel_context(gts))\n\t\tzap_vma_ptes(gts->ts_vma, UGRUADDR(gts), GRU_GSEG_PAGESIZE);\n\tcch = get_cch(gru->gs_gru_base_vaddr, ctxnum);\n\n\tgru_dbg(grudev, \"gts %p, cbrmap 0x%lx, dsrmap 0x%lx\\n\",\n\t\tgts, gts->ts_cbr_map, gts->ts_dsr_map);\n\tlock_cch_handle(cch);\n\tif (cch_interrupt_sync(cch))\n\t\tBUG();\n\n\tif (!is_kernel_context(gts))\n\t\tgru_unload_mm_tracker(gru, gts);\n\tif (savestate) {\n\t\tgru_unload_context_data(gts->ts_gdata, gru->gs_gru_base_vaddr,\n\t\t\t\t\tctxnum, gts->ts_cbr_map,\n\t\t\t\t\tgts->ts_dsr_map);\n\t\tgts->ts_data_valid = 1;\n\t}\n\n\tif (cch_deallocate(cch))\n\t\tBUG();\n\tunlock_cch_handle(cch);\n\n\tgru_free_gru_context(gts);\n}\n\n/*\n * Load a GRU context by copying it from the thread data structure in memory\n * to the GRU.\n */\nvoid gru_load_context(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru = gts->ts_gru;\n\tstruct gru_context_configuration_handle *cch;\n\tint i, err, asid, ctxnum = gts->ts_ctxnum;\n\n\tcch = get_cch(gru->gs_gru_base_vaddr, ctxnum);\n\tlock_cch_handle(cch);\n\tcch->tfm_fault_bit_enable =\n\t    (gts->ts_user_options == GRU_OPT_MISS_FMM_POLL\n\t     || gts->ts_user_options == GRU_OPT_MISS_FMM_INTR);\n\tcch->tlb_int_enable = (gts->ts_user_options == GRU_OPT_MISS_FMM_INTR);\n\tif (cch->tlb_int_enable) {\n\t\tgts->ts_tlb_int_select = gru_cpu_fault_map_id();\n\t\tcch->tlb_int_select = gts->ts_tlb_int_select;\n\t}\n\tif (gts->ts_cch_req_slice >= 0) {\n\t\tcch->req_slice_set_enable = 1;\n\t\tcch->req_slice = gts->ts_cch_req_slice;\n\t} else {\n\t\tcch->req_slice_set_enable =0;\n\t}\n\tcch->tfm_done_bit_enable = 0;\n\tcch->dsr_allocation_map = gts->ts_dsr_map;\n\tcch->cbr_allocation_map = gts->ts_cbr_map;\n\n\tif (is_kernel_context(gts)) {\n\t\tcch->unmap_enable = 1;\n\t\tcch->tfm_done_bit_enable = 1;\n\t\tcch->cb_int_enable = 1;\n\t\tcch->tlb_int_select = 0;\t/* For now, ints go to cpu 0 */\n\t} else {\n\t\tcch->unmap_enable = 0;\n\t\tcch->tfm_done_bit_enable = 0;\n\t\tcch->cb_int_enable = 0;\n\t\tasid = gru_load_mm_tracker(gru, gts);\n\t\tfor (i = 0; i < 8; i++) {\n\t\t\tcch->asid[i] = asid + i;\n\t\t\tcch->sizeavail[i] = gts->ts_sizeavail;\n\t\t}\n\t}\n\n\terr = cch_allocate(cch);\n\tif (err) {\n\t\tgru_dbg(grudev,\n\t\t\t\"err %d: cch %p, gts %p, cbr 0x%lx, dsr 0x%lx\\n\",\n\t\t\terr, cch, gts, gts->ts_cbr_map, gts->ts_dsr_map);\n\t\tBUG();\n\t}\n\n\tgru_load_context_data(gts->ts_gdata, gru->gs_gru_base_vaddr, ctxnum,\n\t\t\tgts->ts_cbr_map, gts->ts_dsr_map, gts->ts_data_valid);\n\n\tif (cch_start(cch))\n\t\tBUG();\n\tunlock_cch_handle(cch);\n\n\tgru_dbg(grudev, \"gid %d, gts %p, cbrmap 0x%lx, dsrmap 0x%lx, tie %d, tis %d\\n\",\n\t\tgts->ts_gru->gs_gid, gts, gts->ts_cbr_map, gts->ts_dsr_map,\n\t\t(gts->ts_user_options == GRU_OPT_MISS_FMM_INTR), gts->ts_tlb_int_select);\n}\n\n/*\n * Update fields in an active CCH:\n * \t- retarget interrupts on local blade\n * \t- update sizeavail mask\n */\nint gru_update_cch(struct gru_thread_state *gts)\n{\n\tstruct gru_context_configuration_handle *cch;\n\tstruct gru_state *gru = gts->ts_gru;\n\tint i, ctxnum = gts->ts_ctxnum, ret = 0;\n\n\tcch = get_cch(gru->gs_gru_base_vaddr, ctxnum);\n\n\tlock_cch_handle(cch);\n\tif (cch->state == CCHSTATE_ACTIVE) {\n\t\tif (gru->gs_gts[gts->ts_ctxnum] != gts)\n\t\t\tgoto exit;\n\t\tif (cch_interrupt(cch))\n\t\t\tBUG();\n\t\tfor (i = 0; i < 8; i++)\n\t\t\tcch->sizeavail[i] = gts->ts_sizeavail;\n\t\tgts->ts_tlb_int_select = gru_cpu_fault_map_id();\n\t\tcch->tlb_int_select = gru_cpu_fault_map_id();\n\t\tcch->tfm_fault_bit_enable =\n\t\t  (gts->ts_user_options == GRU_OPT_MISS_FMM_POLL\n\t\t    || gts->ts_user_options == GRU_OPT_MISS_FMM_INTR);\n\t\tif (cch_start(cch))\n\t\t\tBUG();\n\t\tret = 1;\n\t}\nexit:\n\tunlock_cch_handle(cch);\n\treturn ret;\n}\n\n/*\n * Update CCH tlb interrupt select. Required when all the following is true:\n * \t- task's GRU context is loaded into a GRU\n * \t- task is using interrupt notification for TLB faults\n * \t- task has migrated to a different cpu on the same blade where\n * \t  it was previously running.\n */\nstatic int gru_retarget_intr(struct gru_thread_state *gts)\n{\n\tif (gts->ts_tlb_int_select < 0\n\t    || gts->ts_tlb_int_select == gru_cpu_fault_map_id())\n\t\treturn 0;\n\n\tgru_dbg(grudev, \"retarget from %d to %d\\n\", gts->ts_tlb_int_select,\n\t\tgru_cpu_fault_map_id());\n\treturn gru_update_cch(gts);\n}\n\n/*\n * Check if a GRU context is allowed to use a specific chiplet. By default\n * a context is assigned to any blade-local chiplet. However, users can\n * override this.\n * \tReturns 1 if assignment allowed, 0 otherwise\n */\nstatic int gru_check_chiplet_assignment(struct gru_state *gru,\n\t\t\t\t\tstruct gru_thread_state *gts)\n{\n\tint blade_id;\n\tint chiplet_id;\n\n\tblade_id = gts->ts_user_blade_id;\n\tif (blade_id < 0)\n\t\tblade_id = uv_numa_blade_id();\n\n\tchiplet_id = gts->ts_user_chiplet_id;\n\treturn gru->gs_blade_id == blade_id &&\n\t\t(chiplet_id < 0 || chiplet_id == gru->gs_chiplet_id);\n}\n\n/*\n * Unload the gru context if it is not assigned to the correct blade or\n * chiplet. Misassignment can occur if the process migrates to a different\n * blade or if the user changes the selected blade/chiplet.\n */\nint gru_check_context_placement(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru;\n\tint ret = 0;\n\n\t/*\n\t * If the current task is the context owner, verify that the\n\t * context is correctly placed. This test is skipped for non-owner\n\t * references. Pthread apps use non-owner references to the CBRs.\n\t */\n\tgru = gts->ts_gru;\n\t/*\n\t * If gru or gts->ts_tgid_owner isn't initialized properly, return\n\t * success to indicate that the caller does not need to unload the\n\t * gru context.The caller is responsible for their inspection and\n\t * reinitialization if needed.\n\t */\n\tif (!gru || gts->ts_tgid_owner != current->tgid)\n\t\treturn ret;\n\n\tif (!gru_check_chiplet_assignment(gru, gts)) {\n\t\tSTAT(check_context_unload);\n\t\tret = -EINVAL;\n\t} else if (gru_retarget_intr(gts)) {\n\t\tSTAT(check_context_retarget_intr);\n\t}\n\n\treturn ret;\n}\n\n\n/*\n * Insufficient GRU resources available on the local blade. Steal a context from\n * a process. This is a hack until a _real_ resource scheduler is written....\n */\n#define next_ctxnum(n)\t((n) <  GRU_NUM_CCH - 2 ? (n) + 1 : 0)\n#define next_gru(b, g)\t(((g) < &(b)->bs_grus[GRU_CHIPLETS_PER_BLADE - 1]) ?  \\\n\t\t\t\t ((g)+1) : &(b)->bs_grus[0])\n\nstatic int is_gts_stealable(struct gru_thread_state *gts,\n\t\tstruct gru_blade_state *bs)\n{\n\tif (is_kernel_context(gts))\n\t\treturn down_write_trylock(&bs->bs_kgts_sema);\n\telse\n\t\treturn mutex_trylock(&gts->ts_ctxlock);\n}\n\nstatic void gts_stolen(struct gru_thread_state *gts,\n\t\tstruct gru_blade_state *bs)\n{\n\tif (is_kernel_context(gts)) {\n\t\tup_write(&bs->bs_kgts_sema);\n\t\tSTAT(steal_kernel_context);\n\t} else {\n\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\tSTAT(steal_user_context);\n\t}\n}\n\nvoid gru_steal_context(struct gru_thread_state *gts)\n{\n\tstruct gru_blade_state *blade;\n\tstruct gru_state *gru, *gru0;\n\tstruct gru_thread_state *ngts = NULL;\n\tint ctxnum, ctxnum0, flag = 0, cbr, dsr;\n\tint blade_id;\n\n\tblade_id = gts->ts_user_blade_id;\n\tif (blade_id < 0)\n\t\tblade_id = uv_numa_blade_id();\n\tcbr = gts->ts_cbr_au_count;\n\tdsr = gts->ts_dsr_au_count;\n\n\tblade = gru_base[blade_id];\n\tspin_lock(&blade->bs_lock);\n\n\tctxnum = next_ctxnum(blade->bs_lru_ctxnum);\n\tgru = blade->bs_lru_gru;\n\tif (ctxnum == 0)\n\t\tgru = next_gru(blade, gru);\n\tblade->bs_lru_gru = gru;\n\tblade->bs_lru_ctxnum = ctxnum;\n\tctxnum0 = ctxnum;\n\tgru0 = gru;\n\twhile (1) {\n\t\tif (gru_check_chiplet_assignment(gru, gts)) {\n\t\t\tif (check_gru_resources(gru, cbr, dsr, GRU_NUM_CCH))\n\t\t\t\tbreak;\n\t\t\tspin_lock(&gru->gs_lock);\n\t\t\tfor (; ctxnum < GRU_NUM_CCH; ctxnum++) {\n\t\t\t\tif (flag && gru == gru0 && ctxnum == ctxnum0)\n\t\t\t\t\tbreak;\n\t\t\t\tngts = gru->gs_gts[ctxnum];\n\t\t\t\t/*\n\t\t\t \t* We are grabbing locks out of order, so trylock is\n\t\t\t \t* needed. GTSs are usually not locked, so the odds of\n\t\t\t \t* success are high. If trylock fails, try to steal a\n\t\t\t \t* different GSEG.\n\t\t\t \t*/\n\t\t\t\tif (ngts && is_gts_stealable(ngts, blade))\n\t\t\t\t\tbreak;\n\t\t\t\tngts = NULL;\n\t\t\t}\n\t\t\tspin_unlock(&gru->gs_lock);\n\t\t\tif (ngts || (flag && gru == gru0 && ctxnum == ctxnum0))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (flag && gru == gru0)\n\t\t\tbreak;\n\t\tflag = 1;\n\t\tctxnum = 0;\n\t\tgru = next_gru(blade, gru);\n\t}\n\tspin_unlock(&blade->bs_lock);\n\n\tif (ngts) {\n\t\tgts->ustats.context_stolen++;\n\t\tngts->ts_steal_jiffies = jiffies;\n\t\tgru_unload_context(ngts, is_kernel_context(ngts) ? 0 : 1);\n\t\tgts_stolen(ngts, blade);\n\t} else {\n\t\tSTAT(steal_context_failed);\n\t}\n\tgru_dbg(grudev,\n\t\t\"stole gid %d, ctxnum %d from gts %p. Need cb %d, ds %d;\"\n\t\t\" avail cb %ld, ds %ld\\n\",\n\t\tgru->gs_gid, ctxnum, ngts, cbr, dsr, hweight64(gru->gs_cbr_map),\n\t\thweight64(gru->gs_dsr_map));\n}\n\n/*\n * Assign a gru context.\n */\nstatic int gru_assign_context_number(struct gru_state *gru)\n{\n\tint ctxnum;\n\n\tctxnum = find_first_zero_bit(&gru->gs_context_map, GRU_NUM_CCH);\n\t__set_bit(ctxnum, &gru->gs_context_map);\n\treturn ctxnum;\n}\n\n/*\n * Scan the GRUs on the local blade & assign a GRU context.\n */\nstruct gru_state *gru_assign_gru_context(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru, *grux;\n\tint i, max_active_contexts;\n\tint blade_id = gts->ts_user_blade_id;\n\n\tif (blade_id < 0)\n\t\tblade_id = uv_numa_blade_id();\nagain:\n\tgru = NULL;\n\tmax_active_contexts = GRU_NUM_CCH;\n\tfor_each_gru_on_blade(grux, blade_id, i) {\n\t\tif (!gru_check_chiplet_assignment(grux, gts))\n\t\t\tcontinue;\n\t\tif (check_gru_resources(grux, gts->ts_cbr_au_count,\n\t\t\t\t\tgts->ts_dsr_au_count,\n\t\t\t\t\tmax_active_contexts)) {\n\t\t\tgru = grux;\n\t\t\tmax_active_contexts = grux->gs_active_contexts;\n\t\t\tif (max_active_contexts == 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (gru) {\n\t\tspin_lock(&gru->gs_lock);\n\t\tif (!check_gru_resources(gru, gts->ts_cbr_au_count,\n\t\t\t\t\t gts->ts_dsr_au_count, GRU_NUM_CCH)) {\n\t\t\tspin_unlock(&gru->gs_lock);\n\t\t\tgoto again;\n\t\t}\n\t\treserve_gru_resources(gru, gts);\n\t\tgts->ts_gru = gru;\n\t\tgts->ts_blade = gru->gs_blade_id;\n\t\tgts->ts_ctxnum = gru_assign_context_number(gru);\n\t\trefcount_inc(&gts->ts_refcnt);\n\t\tgru->gs_gts[gts->ts_ctxnum] = gts;\n\t\tspin_unlock(&gru->gs_lock);\n\n\t\tSTAT(assign_context);\n\t\tgru_dbg(grudev,\n\t\t\t\"gseg %p, gts %p, gid %d, ctx %d, cbr %d, dsr %d\\n\",\n\t\t\tgseg_virtual_address(gts->ts_gru, gts->ts_ctxnum), gts,\n\t\t\tgts->ts_gru->gs_gid, gts->ts_ctxnum,\n\t\t\tgts->ts_cbr_au_count, gts->ts_dsr_au_count);\n\t} else {\n\t\tgru_dbg(grudev, \"failed to allocate a GTS %s\\n\", \"\");\n\t\tSTAT(assign_context_failed);\n\t}\n\n\treturn gru;\n}\n\n/*\n * gru_nopage\n *\n * Map the user's GRU segment\n *\n * \tNote: gru segments alway mmaped on GRU_GSEG_PAGESIZE boundaries.\n */\nvm_fault_t gru_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct gru_thread_state *gts;\n\tunsigned long paddr, vaddr;\n\tunsigned long expires;\n\n\tvaddr = vmf->address;\n\tgru_dbg(grudev, \"vma %p, vaddr 0x%lx (0x%lx)\\n\",\n\t\tvma, vaddr, GSEG_BASE(vaddr));\n\tSTAT(nopfn);\n\n\t/* The following check ensures vaddr is a valid address in the VMA */\n\tgts = gru_find_thread_state(vma, TSID(vaddr, vma));\n\tif (!gts)\n\t\treturn VM_FAULT_SIGBUS;\n\nagain:\n\tmutex_lock(&gts->ts_ctxlock);\n\tpreempt_disable();\n\n\tif (gru_check_context_placement(gts)) {\n\t\tpreempt_enable();\n\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\tgru_unload_context(gts, 1);\n\t\treturn VM_FAULT_NOPAGE;\n\t}\n\n\tif (!gts->ts_gru) {\n\t\tSTAT(load_user_context);\n\t\tif (!gru_assign_gru_context(gts)) {\n\t\t\tpreempt_enable();\n\t\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tschedule_timeout(GRU_ASSIGN_DELAY);  /* true hack ZZZ */\n\t\t\texpires = gts->ts_steal_jiffies + GRU_STEAL_DELAY;\n\t\t\tif (time_before(expires, jiffies))\n\t\t\t\tgru_steal_context(gts);\n\t\t\tgoto again;\n\t\t}\n\t\tgru_load_context(gts);\n\t\tpaddr = gseg_physical_address(gts->ts_gru, gts->ts_ctxnum);\n\t\tremap_pfn_range(vma, vaddr & ~(GRU_GSEG_PAGESIZE - 1),\n\t\t\t\tpaddr >> PAGE_SHIFT, GRU_GSEG_PAGESIZE,\n\t\t\t\tvma->vm_page_prot);\n\t}\n\n\tpreempt_enable();\n\tmutex_unlock(&gts->ts_ctxlock);\n\n\treturn VM_FAULT_NOPAGE;\n}\n\n", "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n * SN Platform GRU Driver\n *\n *            GRU DRIVER TABLES, MACROS, externs, etc\n *\n *  Copyright (c) 2008 Silicon Graphics, Inc.  All Rights Reserved.\n */\n\n#ifndef __GRUTABLES_H__\n#define __GRUTABLES_H__\n\n/*\n * GRU Chiplet:\n *   The GRU is a user addressible memory accelerator. It provides\n *   several forms of load, store, memset, bcopy instructions. In addition, it\n *   contains special instructions for AMOs, sending messages to message\n *   queues, etc.\n *\n *   The GRU is an integral part of the node controller. It connects\n *   directly to the cpu socket. In its current implementation, there are 2\n *   GRU chiplets in the node controller on each blade (~node).\n *\n *   The entire GRU memory space is fully coherent and cacheable by the cpus.\n *\n *   Each GRU chiplet has a physical memory map that looks like the following:\n *\n *   \t+-----------------+\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t|/////////////////|\n *   \t+-----------------+\n *   \t|  system control |\n *   \t+-----------------+        _______ +-------------+\n *   \t|/////////////////|       /        |             |\n *   \t|/////////////////|      /         |             |\n *   \t|/////////////////|     /          | instructions|\n *   \t|/////////////////|    /           |             |\n *   \t|/////////////////|   /            |             |\n *   \t|/////////////////|  /             |-------------|\n *   \t|/////////////////| /              |             |\n *   \t+-----------------+                |             |\n *   \t|   context 15    |                |  data       |\n *   \t+-----------------+                |             |\n *   \t|    ......       | \\              |             |\n *   \t+-----------------+  \\____________ +-------------+\n *   \t|   context 1     |\n *   \t+-----------------+\n *   \t|   context 0     |\n *   \t+-----------------+\n *\n *   Each of the \"contexts\" is a chunk of memory that can be mmaped into user\n *   space. The context consists of 2 parts:\n *\n *  \t- an instruction space that can be directly accessed by the user\n *  \t  to issue GRU instructions and to check instruction status.\n *\n *  \t- a data area that acts as normal RAM.\n *\n *   User instructions contain virtual addresses of data to be accessed by the\n *   GRU. The GRU contains a TLB that is used to convert these user virtual\n *   addresses to physical addresses.\n *\n *   The \"system control\" area of the GRU chiplet is used by the kernel driver\n *   to manage user contexts and to perform functions such as TLB dropin and\n *   purging.\n *\n *   One context may be reserved for the kernel and used for cross-partition\n *   communication. The GRU will also be used to asynchronously zero out\n *   large blocks of memory (not currently implemented).\n *\n *\n * Tables:\n *\n * \tVDATA-VMA Data\t\t- Holds a few parameters. Head of linked list of\n * \t\t\t\t  GTS tables for threads using the GSEG\n * \tGTS - Gru Thread State  - contains info for managing a GSEG context. A\n * \t\t\t\t  GTS is allocated for each thread accessing a\n * \t\t\t\t  GSEG.\n *     \tGTD - GRU Thread Data   - contains shadow copy of GRU data when GSEG is\n *     \t\t\t\t  not loaded into a GRU\n *\tGMS - GRU Memory Struct - Used to manage TLB shootdowns. Tracks GRUs\n *\t\t\t\t  where a GSEG has been loaded. Similar to\n *\t\t\t\t  an mm_struct but for GRU.\n *\n *\tGS  - GRU State \t- Used to manage the state of a GRU chiplet\n *\tBS  - Blade State\t- Used to manage state of all GRU chiplets\n *\t\t\t\t  on a blade\n *\n *\n *  Normal task tables for task using GRU.\n *  \t\t- 2 threads in process\n *  \t\t- 2 GSEGs open in process\n *  \t\t- GSEG1 is being used by both threads\n *  \t\t- GSEG2 is used only by thread 2\n *\n *       task -->|\n *       task ---+---> mm ->------ (notifier) -------+-> gms\n *                     |                             |\n *                     |--> vma -> vdata ---> gts--->|\t\tGSEG1 (thread1)\n *                     |                  |          |\n *                     |                  +-> gts--->|\t\tGSEG1 (thread2)\n *                     |                             |\n *                     |--> vma -> vdata ---> gts--->|\t\tGSEG2 (thread2)\n *                     .\n *                     .\n *\n *  GSEGs are marked DONTCOPY on fork\n *\n * At open\n * \tfile.private_data -> NULL\n *\n * At mmap,\n * \tvma -> vdata\n *\n * After gseg reference\n * \tvma -> vdata ->gts\n *\n * After fork\n *   parent\n * \tvma -> vdata -> gts\n *   child\n * \t(vma is not copied)\n *\n */\n\n#include <linux/refcount.h>\n#include <linux/rmap.h>\n#include <linux/interrupt.h>\n#include <linux/mutex.h>\n#include <linux/wait.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mm_types.h>\n#include \"gru.h\"\n#include \"grulib.h\"\n#include \"gruhandles.h\"\n\nextern struct gru_stats_s gru_stats;\nextern struct gru_blade_state *gru_base[];\nextern unsigned long gru_start_paddr, gru_end_paddr;\nextern void *gru_start_vaddr;\nextern unsigned int gru_max_gids;\n\n#define GRU_MAX_BLADES\t\tMAX_NUMNODES\n#define GRU_MAX_GRUS\t\t(GRU_MAX_BLADES * GRU_CHIPLETS_PER_BLADE)\n\n#define GRU_DRIVER_ID_STR\t\"SGI GRU Device Driver\"\n#define GRU_DRIVER_VERSION_STR\t\"0.85\"\n\n/*\n * GRU statistics.\n */\nstruct gru_stats_s {\n\tatomic_long_t vdata_alloc;\n\tatomic_long_t vdata_free;\n\tatomic_long_t gts_alloc;\n\tatomic_long_t gts_free;\n\tatomic_long_t gms_alloc;\n\tatomic_long_t gms_free;\n\tatomic_long_t gts_double_allocate;\n\tatomic_long_t assign_context;\n\tatomic_long_t assign_context_failed;\n\tatomic_long_t free_context;\n\tatomic_long_t load_user_context;\n\tatomic_long_t load_kernel_context;\n\tatomic_long_t lock_kernel_context;\n\tatomic_long_t unlock_kernel_context;\n\tatomic_long_t steal_user_context;\n\tatomic_long_t steal_kernel_context;\n\tatomic_long_t steal_context_failed;\n\tatomic_long_t nopfn;\n\tatomic_long_t asid_new;\n\tatomic_long_t asid_next;\n\tatomic_long_t asid_wrap;\n\tatomic_long_t asid_reuse;\n\tatomic_long_t intr;\n\tatomic_long_t intr_cbr;\n\tatomic_long_t intr_tfh;\n\tatomic_long_t intr_spurious;\n\tatomic_long_t intr_mm_lock_failed;\n\tatomic_long_t call_os;\n\tatomic_long_t call_os_wait_queue;\n\tatomic_long_t user_flush_tlb;\n\tatomic_long_t user_unload_context;\n\tatomic_long_t user_exception;\n\tatomic_long_t set_context_option;\n\tatomic_long_t check_context_retarget_intr;\n\tatomic_long_t check_context_unload;\n\tatomic_long_t tlb_dropin;\n\tatomic_long_t tlb_preload_page;\n\tatomic_long_t tlb_dropin_fail_no_asid;\n\tatomic_long_t tlb_dropin_fail_upm;\n\tatomic_long_t tlb_dropin_fail_invalid;\n\tatomic_long_t tlb_dropin_fail_range_active;\n\tatomic_long_t tlb_dropin_fail_idle;\n\tatomic_long_t tlb_dropin_fail_fmm;\n\tatomic_long_t tlb_dropin_fail_no_exception;\n\tatomic_long_t tfh_stale_on_fault;\n\tatomic_long_t mmu_invalidate_range;\n\tatomic_long_t mmu_invalidate_page;\n\tatomic_long_t flush_tlb;\n\tatomic_long_t flush_tlb_gru;\n\tatomic_long_t flush_tlb_gru_tgh;\n\tatomic_long_t flush_tlb_gru_zero_asid;\n\n\tatomic_long_t copy_gpa;\n\tatomic_long_t read_gpa;\n\n\tatomic_long_t mesq_receive;\n\tatomic_long_t mesq_receive_none;\n\tatomic_long_t mesq_send;\n\tatomic_long_t mesq_send_failed;\n\tatomic_long_t mesq_noop;\n\tatomic_long_t mesq_send_unexpected_error;\n\tatomic_long_t mesq_send_lb_overflow;\n\tatomic_long_t mesq_send_qlimit_reached;\n\tatomic_long_t mesq_send_amo_nacked;\n\tatomic_long_t mesq_send_put_nacked;\n\tatomic_long_t mesq_page_overflow;\n\tatomic_long_t mesq_qf_locked;\n\tatomic_long_t mesq_qf_noop_not_full;\n\tatomic_long_t mesq_qf_switch_head_failed;\n\tatomic_long_t mesq_qf_unexpected_error;\n\tatomic_long_t mesq_noop_unexpected_error;\n\tatomic_long_t mesq_noop_lb_overflow;\n\tatomic_long_t mesq_noop_qlimit_reached;\n\tatomic_long_t mesq_noop_amo_nacked;\n\tatomic_long_t mesq_noop_put_nacked;\n\tatomic_long_t mesq_noop_page_overflow;\n\n};\n\nenum mcs_op {cchop_allocate, cchop_start, cchop_interrupt, cchop_interrupt_sync,\n\tcchop_deallocate, tfhop_write_only, tfhop_write_restart,\n\ttghop_invalidate, mcsop_last};\n\nstruct mcs_op_statistic {\n\tatomic_long_t\tcount;\n\tatomic_long_t\ttotal;\n\tunsigned long\tmax;\n};\n\nextern struct mcs_op_statistic mcs_op_statistics[mcsop_last];\n\n#define OPT_DPRINT\t\t1\n#define OPT_STATS\t\t2\n\n\n#define IRQ_GRU\t\t\t110\t/* Starting IRQ number for interrupts */\n\n/* Delay in jiffies between attempts to assign a GRU context */\n#define GRU_ASSIGN_DELAY\t((HZ * 20) / 1000)\n\n/*\n * If a process has it's context stolen, min delay in jiffies before trying to\n * steal a context from another process.\n */\n#define GRU_STEAL_DELAY\t\t((HZ * 200) / 1000)\n\n#define STAT(id)\tdo {\t\t\t\t\t\t\\\n\t\t\t\tif (gru_options & OPT_STATS)\t\t\\\n\t\t\t\t\tatomic_long_inc(&gru_stats.id);\t\\\n\t\t\t} while (0)\n\n#ifdef CONFIG_SGI_GRU_DEBUG\n#define gru_dbg(dev, fmt, x...)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (gru_options & OPT_DPRINT)\t\t\t\t\\\n\t\t\tprintk(KERN_DEBUG \"GRU:%d %s: \" fmt, smp_processor_id(), __func__, x);\\\n\t} while (0)\n#else\n#define gru_dbg(x...)\n#endif\n\n/*-----------------------------------------------------------------------------\n * ASID management\n */\n#define MAX_ASID\t0xfffff0\n#define MIN_ASID\t8\n#define ASID_INC\t8\t/* number of regions */\n\n/* Generate a GRU asid value from a GRU base asid & a virtual address. */\n#define VADDR_HI_BIT\t\t64\n#define GRUREGION(addr)\t\t((addr) >> (VADDR_HI_BIT - 3) & 3)\n#define GRUASID(asid, addr)\t((asid) + GRUREGION(addr))\n\n/*------------------------------------------------------------------------------\n *  File & VMS Tables\n */\n\nstruct gru_state;\n\n/*\n * This structure is pointed to from the mmstruct via the notifier pointer.\n * There is one of these per address space.\n */\nstruct gru_mm_tracker {\t\t\t\t/* pack to reduce size */\n\tunsigned int\t\tmt_asid_gen:24;\t/* ASID wrap count */\n\tunsigned int\t\tmt_asid:24;\t/* current base ASID for gru */\n\tunsigned short\t\tmt_ctxbitmap:16;/* bitmap of contexts using\n\t\t\t\t\t\t   asid */\n} __attribute__ ((packed));\n\nstruct gru_mm_struct {\n\tstruct mmu_notifier\tms_notifier;\n\tspinlock_t\t\tms_asid_lock;\t/* protects ASID assignment */\n\tatomic_t\t\tms_range_active;/* num range_invals active */\n\twait_queue_head_t\tms_wait_queue;\n\tDECLARE_BITMAP(ms_asidmap, GRU_MAX_GRUS);\n\tstruct gru_mm_tracker\tms_asids[GRU_MAX_GRUS];\n};\n\n/*\n * One of these structures is allocated when a GSEG is mmaped. The\n * structure is pointed to by the vma->vm_private_data field in the vma struct.\n */\nstruct gru_vma_data {\n\tspinlock_t\t\tvd_lock;\t/* Serialize access to vma */\n\tstruct list_head\tvd_head;\t/* head of linked list of gts */\n\tlong\t\t\tvd_user_options;/* misc user option flags */\n\tint\t\t\tvd_cbr_au_count;\n\tint\t\t\tvd_dsr_au_count;\n\tunsigned char\t\tvd_tlb_preload_count;\n};\n\n/*\n * One of these is allocated for each thread accessing a mmaped GRU. A linked\n * list of these structure is hung off the struct gru_vma_data in the mm_struct.\n */\nstruct gru_thread_state {\n\tstruct list_head\tts_next;\t/* list - head at vma-private */\n\tstruct mutex\t\tts_ctxlock;\t/* load/unload CTX lock */\n\tstruct mm_struct\t*ts_mm;\t\t/* mm currently mapped to\n\t\t\t\t\t\t   context */\n\tstruct vm_area_struct\t*ts_vma;\t/* vma of GRU context */\n\tstruct gru_state\t*ts_gru;\t/* GRU where the context is\n\t\t\t\t\t\t   loaded */\n\tstruct gru_mm_struct\t*ts_gms;\t/* asid & ioproc struct */\n\tunsigned char\t\tts_tlb_preload_count; /* TLB preload pages */\n\tunsigned long\t\tts_cbr_map;\t/* map of allocated CBRs */\n\tunsigned long\t\tts_dsr_map;\t/* map of allocated DATA\n\t\t\t\t\t\t   resources */\n\tunsigned long\t\tts_steal_jiffies;/* jiffies when context last\n\t\t\t\t\t\t    stolen */\n\tlong\t\t\tts_user_options;/* misc user option flags */\n\tpid_t\t\t\tts_tgid_owner;\t/* task that is using the\n\t\t\t\t\t\t   context - for migration */\n\tshort\t\t\tts_user_blade_id;/* user selected blade */\n\tsigned char\t\tts_user_chiplet_id;/* user selected chiplet */\n\tunsigned short\t\tts_sizeavail;\t/* Pagesizes in use */\n\tint\t\t\tts_tsid;\t/* thread that owns the\n\t\t\t\t\t\t   structure */\n\tint\t\t\tts_tlb_int_select;/* target cpu if interrupts\n\t\t\t\t\t\t     enabled */\n\tint\t\t\tts_ctxnum;\t/* context number where the\n\t\t\t\t\t\t   context is loaded */\n\trefcount_t\t\tts_refcnt;\t/* reference count GTS */\n\tunsigned char\t\tts_dsr_au_count;/* Number of DSR resources\n\t\t\t\t\t\t   required for contest */\n\tunsigned char\t\tts_cbr_au_count;/* Number of CBR resources\n\t\t\t\t\t\t   required for contest */\n\tsigned char\t\tts_cch_req_slice;/* CCH packet slice */\n\tsigned char\t\tts_blade;\t/* If >= 0, migrate context if\n\t\t\t\t\t\t   ref from different blade */\n\tsigned char\t\tts_force_cch_reload;\n\tsigned char\t\tts_cbr_idx[GRU_CBR_AU];/* CBR numbers of each\n\t\t\t\t\t\t\t  allocated CB */\n\tint\t\t\tts_data_valid;\t/* Indicates if ts_gdata has\n\t\t\t\t\t\t   valid data */\n\tstruct gru_gseg_statistics ustats;\t/* User statistics */\n\tunsigned long\t\tts_gdata[];\t/* save area for GRU data (CB,\n\t\t\t\t\t\t   DS, CBE) */\n};\n\n/*\n * Threaded programs actually allocate an array of GSEGs when a context is\n * created. Each thread uses a separate GSEG. TSID is the index into the GSEG\n * array.\n */\n#define TSID(a, v)\t\t(((a) - (v)->vm_start) / GRU_GSEG_PAGESIZE)\n#define UGRUADDR(gts)\t\t((gts)->ts_vma->vm_start +\t\t\\\n\t\t\t\t\t(gts)->ts_tsid * GRU_GSEG_PAGESIZE)\n\n#define NULLCTX\t\t\t(-1)\t/* if context not loaded into GRU */\n\n/*-----------------------------------------------------------------------------\n *  GRU State Tables\n */\n\n/*\n * One of these exists for each GRU chiplet.\n */\nstruct gru_state {\n\tstruct gru_blade_state\t*gs_blade;\t\t/* GRU state for entire\n\t\t\t\t\t\t\t   blade */\n\tunsigned long\t\tgs_gru_base_paddr;\t/* Physical address of\n\t\t\t\t\t\t\t   gru segments (64) */\n\tvoid\t\t\t*gs_gru_base_vaddr;\t/* Virtual address of\n\t\t\t\t\t\t\t   gru segments (64) */\n\tunsigned short\t\tgs_gid;\t\t\t/* unique GRU number */\n\tunsigned short\t\tgs_blade_id;\t\t/* blade of GRU */\n\tunsigned char\t\tgs_chiplet_id;\t\t/* blade chiplet of GRU */\n\tunsigned char\t\tgs_tgh_local_shift;\t/* used to pick TGH for\n\t\t\t\t\t\t\t   local flush */\n\tunsigned char\t\tgs_tgh_first_remote;\t/* starting TGH# for\n\t\t\t\t\t\t\t   remote flush */\n\tspinlock_t\t\tgs_asid_lock;\t\t/* lock used for\n\t\t\t\t\t\t\t   assigning asids */\n\tspinlock_t\t\tgs_lock;\t\t/* lock used for\n\t\t\t\t\t\t\t   assigning contexts */\n\n\t/* -- the following are protected by the gs_asid_lock spinlock ---- */\n\tunsigned int\t\tgs_asid;\t\t/* Next availe ASID */\n\tunsigned int\t\tgs_asid_limit;\t\t/* Limit of available\n\t\t\t\t\t\t\t   ASIDs */\n\tunsigned int\t\tgs_asid_gen;\t\t/* asid generation.\n\t\t\t\t\t\t\t   Inc on wrap */\n\n\t/* --- the following fields are protected by the gs_lock spinlock --- */\n\tunsigned long\t\tgs_context_map;\t\t/* bitmap to manage\n\t\t\t\t\t\t\t   contexts in use */\n\tunsigned long\t\tgs_cbr_map;\t\t/* bitmap to manage CB\n\t\t\t\t\t\t\t   resources */\n\tunsigned long\t\tgs_dsr_map;\t\t/* bitmap used to manage\n\t\t\t\t\t\t\t   DATA resources */\n\tunsigned int\t\tgs_reserved_cbrs;\t/* Number of kernel-\n\t\t\t\t\t\t\t   reserved cbrs */\n\tunsigned int\t\tgs_reserved_dsr_bytes;\t/* Bytes of kernel-\n\t\t\t\t\t\t\t   reserved dsrs */\n\tunsigned short\t\tgs_active_contexts;\t/* number of contexts\n\t\t\t\t\t\t\t   in use */\n\tstruct gru_thread_state\t*gs_gts[GRU_NUM_CCH];\t/* GTS currently using\n\t\t\t\t\t\t\t   the context */\n\tint\t\t\tgs_irq[GRU_NUM_TFM];\t/* Interrupt irqs */\n};\n\n/*\n * This structure contains the GRU state for all the GRUs on a blade.\n */\nstruct gru_blade_state {\n\tvoid\t\t\t*kernel_cb;\t\t/* First kernel\n\t\t\t\t\t\t\t   reserved cb */\n\tvoid\t\t\t*kernel_dsr;\t\t/* First kernel\n\t\t\t\t\t\t\t   reserved DSR */\n\tstruct rw_semaphore\tbs_kgts_sema;\t\t/* lock for kgts */\n\tstruct gru_thread_state *bs_kgts;\t\t/* GTS for kernel use */\n\n\t/* ---- the following are used for managing kernel async GRU CBRs --- */\n\tint\t\t\tbs_async_dsr_bytes;\t/* DSRs for async */\n\tint\t\t\tbs_async_cbrs;\t\t/* CBRs AU for async */\n\tstruct completion\t*bs_async_wq;\n\n\t/* ---- the following are protected by the bs_lock spinlock ---- */\n\tspinlock_t\t\tbs_lock;\t\t/* lock used for\n\t\t\t\t\t\t\t   stealing contexts */\n\tint\t\t\tbs_lru_ctxnum;\t\t/* STEAL - last context\n\t\t\t\t\t\t\t   stolen */\n\tstruct gru_state\t*bs_lru_gru;\t\t/* STEAL - last gru\n\t\t\t\t\t\t\t   stolen */\n\n\tstruct gru_state\tbs_grus[GRU_CHIPLETS_PER_BLADE];\n};\n\n/*-----------------------------------------------------------------------------\n * Address Primitives\n */\n#define get_tfm_for_cpu(g, c)\t\t\t\t\t\t\\\n\t((struct gru_tlb_fault_map *)get_tfm((g)->gs_gru_base_vaddr, (c)))\n#define get_tfh_by_index(g, i)\t\t\t\t\t\t\\\n\t((struct gru_tlb_fault_handle *)get_tfh((g)->gs_gru_base_vaddr, (i)))\n#define get_tgh_by_index(g, i)\t\t\t\t\t\t\\\n\t((struct gru_tlb_global_handle *)get_tgh((g)->gs_gru_base_vaddr, (i)))\n#define get_cbe_by_index(g, i)\t\t\t\t\t\t\\\n\t((struct gru_control_block_extended *)get_cbe((g)->gs_gru_base_vaddr,\\\n\t\t\t(i)))\n\n/*-----------------------------------------------------------------------------\n * Useful Macros\n */\n\n/* Given a blade# & chiplet#, get a pointer to the GRU */\n#define get_gru(b, c)\t\t(&gru_base[b]->bs_grus[c])\n\n/* Number of bytes to save/restore when unloading/loading GRU contexts */\n#define DSR_BYTES(dsr)\t\t((dsr) * GRU_DSR_AU_BYTES)\n#define CBR_BYTES(cbr)\t\t((cbr) * GRU_HANDLE_BYTES * GRU_CBR_AU_SIZE * 2)\n\n/* Convert a user CB number to the actual CBRNUM */\n#define thread_cbr_number(gts, n) ((gts)->ts_cbr_idx[(n) / GRU_CBR_AU_SIZE] \\\n\t\t\t\t  * GRU_CBR_AU_SIZE + (n) % GRU_CBR_AU_SIZE)\n\n/* Convert a gid to a pointer to the GRU */\n#define GID_TO_GRU(gid)\t\t\t\t\t\t\t\\\n\t(gru_base[(gid) / GRU_CHIPLETS_PER_BLADE] ?\t\t\t\\\n\t\t(&gru_base[(gid) / GRU_CHIPLETS_PER_BLADE]->\t\t\\\n\t\t\tbs_grus[(gid) % GRU_CHIPLETS_PER_BLADE]) :\t\\\n\t NULL)\n\n/* Scan all active GRUs in a GRU bitmap */\n#define for_each_gru_in_bitmap(gid, map)\t\t\t\t\\\n\tfor_each_set_bit((gid), (map), GRU_MAX_GRUS)\n\n/* Scan all active GRUs on a specific blade */\n#define for_each_gru_on_blade(gru, nid, i)\t\t\t\t\\\n\tfor ((gru) = gru_base[nid]->bs_grus, (i) = 0;\t\t\t\\\n\t\t\t(i) < GRU_CHIPLETS_PER_BLADE;\t\t\t\\\n\t\t\t(i)++, (gru)++)\n\n/* Scan all GRUs */\n#define foreach_gid(gid)\t\t\t\t\t\t\\\n\tfor ((gid) = 0; (gid) < gru_max_gids; (gid)++)\n\n/* Scan all active GTSs on a gru. Note: must hold ss_lock to use this macro. */\n#define for_each_gts_on_gru(gts, gru, ctxnum)\t\t\t\t\\\n\tfor ((ctxnum) = 0; (ctxnum) < GRU_NUM_CCH; (ctxnum)++)\t\t\\\n\t\tif (((gts) = (gru)->gs_gts[ctxnum]))\n\n/* Scan each CBR whose bit is set in a TFM (or copy of) */\n#define for_each_cbr_in_tfm(i, map)\t\t\t\t\t\\\n\tfor_each_set_bit((i), (map), GRU_NUM_CBE)\n\n/* Scan each CBR in a CBR bitmap. Note: multiple CBRs in an allocation unit */\n#define for_each_cbr_in_allocation_map(i, map, k)\t\t\t\\\n\tfor_each_set_bit((k), (map), GRU_CBR_AU)\t\t\t\\\n\t\tfor ((i) = (k)*GRU_CBR_AU_SIZE;\t\t\t\t\\\n\t\t\t\t(i) < ((k) + 1) * GRU_CBR_AU_SIZE; (i)++)\n\n#define gseg_physical_address(gru, ctxnum)\t\t\t\t\\\n\t\t((gru)->gs_gru_base_paddr + ctxnum * GRU_GSEG_STRIDE)\n#define gseg_virtual_address(gru, ctxnum)\t\t\t\t\\\n\t\t((gru)->gs_gru_base_vaddr + ctxnum * GRU_GSEG_STRIDE)\n\n/*-----------------------------------------------------------------------------\n * Lock / Unlock GRU handles\n * \tUse the \"delresp\" bit in the handle as a \"lock\" bit.\n */\n\n/* Lock hierarchy checking enabled only in emulator */\n\n/* 0 = lock failed, 1 = locked */\nstatic inline int __trylock_handle(void *h)\n{\n\treturn !test_and_set_bit(1, h);\n}\n\nstatic inline void __lock_handle(void *h)\n{\n\twhile (test_and_set_bit(1, h))\n\t\tcpu_relax();\n}\n\nstatic inline void __unlock_handle(void *h)\n{\n\tclear_bit(1, h);\n}\n\nstatic inline int trylock_cch_handle(struct gru_context_configuration_handle *cch)\n{\n\treturn __trylock_handle(cch);\n}\n\nstatic inline void lock_cch_handle(struct gru_context_configuration_handle *cch)\n{\n\t__lock_handle(cch);\n}\n\nstatic inline void unlock_cch_handle(struct gru_context_configuration_handle\n\t\t\t\t     *cch)\n{\n\t__unlock_handle(cch);\n}\n\nstatic inline void lock_tgh_handle(struct gru_tlb_global_handle *tgh)\n{\n\t__lock_handle(tgh);\n}\n\nstatic inline void unlock_tgh_handle(struct gru_tlb_global_handle *tgh)\n{\n\t__unlock_handle(tgh);\n}\n\nstatic inline int is_kernel_context(struct gru_thread_state *gts)\n{\n\treturn !gts->ts_mm;\n}\n\n/*\n * The following are for Nehelem-EX. A more general scheme is needed for\n * future processors.\n */\n#define UV_MAX_INT_CORES\t\t8\n#define uv_cpu_socket_number(p)\t\t((cpu_physical_id(p) >> 5) & 1)\n#define uv_cpu_ht_number(p)\t\t(cpu_physical_id(p) & 1)\n#define uv_cpu_core_number(p)\t\t(((cpu_physical_id(p) >> 2) & 4) |\t\\\n\t\t\t\t\t((cpu_physical_id(p) >> 1) & 3))\n/*-----------------------------------------------------------------------------\n * Function prototypes & externs\n */\nstruct gru_unload_context_req;\n\nextern const struct vm_operations_struct gru_vm_ops;\nextern struct device *grudev;\n\nextern struct gru_vma_data *gru_alloc_vma_data(struct vm_area_struct *vma,\n\t\t\t\tint tsid);\nextern struct gru_thread_state *gru_find_thread_state(struct vm_area_struct\n\t\t\t\t*vma, int tsid);\nextern struct gru_thread_state *gru_alloc_thread_state(struct vm_area_struct\n\t\t\t\t*vma, int tsid);\nextern struct gru_state *gru_assign_gru_context(struct gru_thread_state *gts);\nextern void gru_load_context(struct gru_thread_state *gts);\nextern void gru_steal_context(struct gru_thread_state *gts);\nextern void gru_unload_context(struct gru_thread_state *gts, int savestate);\nextern int gru_update_cch(struct gru_thread_state *gts);\nextern void gts_drop(struct gru_thread_state *gts);\nextern void gru_tgh_flush_init(struct gru_state *gru);\nextern int gru_kservices_init(void);\nextern void gru_kservices_exit(void);\nextern irqreturn_t gru0_intr(int irq, void *dev_id);\nextern irqreturn_t gru1_intr(int irq, void *dev_id);\nextern irqreturn_t gru_intr_mblade(int irq, void *dev_id);\nextern int gru_dump_chiplet_request(unsigned long arg);\nextern long gru_get_gseg_statistics(unsigned long arg);\nextern int gru_handle_user_call_os(unsigned long address);\nextern int gru_user_flush_tlb(unsigned long arg);\nextern int gru_user_unload_context(unsigned long arg);\nextern int gru_get_exception_detail(unsigned long arg);\nextern int gru_set_context_option(unsigned long address);\nextern int gru_check_context_placement(struct gru_thread_state *gts);\nextern int gru_cpu_fault_map_id(void);\nextern struct vm_area_struct *gru_find_vma(unsigned long vaddr);\nextern void gru_flush_all_tlb(struct gru_state *gru);\nextern int gru_proc_init(void);\nextern void gru_proc_exit(void);\n\nextern struct gru_thread_state *gru_alloc_gts(struct vm_area_struct *vma,\n\t\tint cbr_au_count, int dsr_au_count,\n\t\tunsigned char tlb_preload_count, int options, int tsid);\nextern unsigned long gru_reserve_cb_resources(struct gru_state *gru,\n\t\tint cbr_au_count, signed char *cbmap);\nextern unsigned long gru_reserve_ds_resources(struct gru_state *gru,\n\t\tint dsr_au_count, signed char *dsmap);\nextern vm_fault_t gru_fault(struct vm_fault *vmf);\nextern struct gru_mm_struct *gru_register_mmu_notifier(void);\nextern void gru_drop_mmu_notifier(struct gru_mm_struct *gms);\n\nextern int gru_ktest(unsigned long arg);\nextern void gru_flush_tlb_range(struct gru_mm_struct *gms, unsigned long start,\n\t\t\t\t\tunsigned long len);\n\nextern unsigned long gru_options;\n\n#endif /* __GRUTABLES_H__ */\n"], "filenames": ["drivers/misc/sgi-gru/grufault.c", "drivers/misc/sgi-gru/grumain.c", "drivers/misc/sgi-gru/grutables.h"], "buggy_code_start_loc": [650, 719, 635], "buggy_code_end_loc": [878, 938, 636], "fixing_code_start_loc": [651, 719, 635], "fixing_code_end_loc": [887, 952, 636], "type": "CWE-416", "message": "A use-after-free flaw was found in the Linux kernel\u2019s SGI GRU driver in the way the first gru_file_unlocked_ioctl function is called by the user, where a fail pass occurs in the gru_check_chiplet_assignment function. This flaw allows a local user to crash or potentially escalate their privileges on the system.", "other": {"cve": {"id": "CVE-2022-3424", "sourceIdentifier": "secalert@redhat.com", "published": "2023-03-06T23:15:10.853", "lastModified": "2023-05-19T16:51:36.780", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A use-after-free flaw was found in the Linux kernel\u2019s SGI GRU driver in the way the first gru_file_unlocked_ioctl function is called by the user, where a fail pass occurs in the gru_check_chiplet_assignment function. This flaw allows a local user to crash or potentially escalate their privileges on the system."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.33", "versionEndExcluding": "4.9.337", "matchCriteriaId": "06458313-3A6F-47E0-8BC4-51BC0D1C4BC4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.14.303", "matchCriteriaId": "1E7450AD-4739-46F0-B81B-C02E7B35A97B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.15", "versionEndExcluding": "4.19.270", "matchCriteriaId": "AE8904A3-99BE-4E49-9682-1F90A6373F4F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.20", "versionEndExcluding": "5.4.229", "matchCriteriaId": "A0C0D95E-414A-445E-941B-3EF6A4D3A093"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.5", "versionEndExcluding": "5.10.163", "matchCriteriaId": "D05D31FC-BD74-4F9E-B1D8-9CED62BE6F65"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.11", "versionEndExcluding": "5.15.86", "matchCriteriaId": "47237296-55D1-4ED4-8075-D00FC85A61EE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.16", "versionEndExcluding": "6.0.16", "matchCriteriaId": "C720A569-3D93-4D77-95F6-E2B3A3267D9F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "6.1", "versionEndExcluding": "6.1.2", "matchCriteriaId": "77239F4B-6BB2-4B9E-A654-36A52396116C"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "7F6FB57C-2BC7-487C-96DD-132683AEB35D"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2132640", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/643a16a0eb1d6ac23744bb6e90a00fc21148a9dc", "source": "secalert@redhat.com", "tags": ["Patch"]}, {"url": "https://lists.debian.org/debian-lts-announce/2023/05/msg00005.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2023/05/msg00006.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lore.kernel.org/all/20221019031445.901570-1-zyytlz.wz@163.com/", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch"]}, {"url": "https://security.netapp.com/advisory/ntap-20230406-0005/", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.spinics.net/lists/kernel/msg4518970.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/643a16a0eb1d6ac23744bb6e90a00fc21148a9dc"}}