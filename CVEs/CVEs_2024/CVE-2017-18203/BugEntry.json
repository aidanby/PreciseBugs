{"buggy_code": ["/*\n * Copyright (C) 2001, 2002 Sistina Software (UK) Limited.\n * Copyright (C) 2004-2008 Red Hat, Inc. All rights reserved.\n *\n * This file is released under the GPL.\n */\n\n#include \"dm-core.h\"\n#include \"dm-rq.h\"\n#include \"dm-uevent.h\"\n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/sched/signal.h>\n#include <linux/blkpg.h>\n#include <linux/bio.h>\n#include <linux/mempool.h>\n#include <linux/dax.h>\n#include <linux/slab.h>\n#include <linux/idr.h>\n#include <linux/uio.h>\n#include <linux/hdreg.h>\n#include <linux/delay.h>\n#include <linux/wait.h>\n#include <linux/pr.h>\n#include <linux/refcount.h>\n\n#define DM_MSG_PREFIX \"core\"\n\n/*\n * Cookies are numeric values sent with CHANGE and REMOVE\n * uevents while resuming, removing or renaming the device.\n */\n#define DM_COOKIE_ENV_VAR_NAME \"DM_COOKIE\"\n#define DM_COOKIE_LENGTH 24\n\nstatic const char *_name = DM_NAME;\n\nstatic unsigned int major = 0;\nstatic unsigned int _major = 0;\n\nstatic DEFINE_IDR(_minor_idr);\n\nstatic DEFINE_SPINLOCK(_minor_lock);\n\nstatic void do_deferred_remove(struct work_struct *w);\n\nstatic DECLARE_WORK(deferred_remove_work, do_deferred_remove);\n\nstatic struct workqueue_struct *deferred_remove_workqueue;\n\natomic_t dm_global_event_nr = ATOMIC_INIT(0);\nDECLARE_WAIT_QUEUE_HEAD(dm_global_eventq);\n\nvoid dm_issue_global_event(void)\n{\n\tatomic_inc(&dm_global_event_nr);\n\twake_up(&dm_global_eventq);\n}\n\n/*\n * One of these is allocated per bio.\n */\nstruct dm_io {\n\tstruct mapped_device *md;\n\tblk_status_t status;\n\tatomic_t io_count;\n\tstruct bio *bio;\n\tunsigned long start_time;\n\tspinlock_t endio_lock;\n\tstruct dm_stats_aux stats_aux;\n};\n\n#define MINOR_ALLOCED ((void *)-1)\n\n/*\n * Bits for the md->flags field.\n */\n#define DMF_BLOCK_IO_FOR_SUSPEND 0\n#define DMF_SUSPENDED 1\n#define DMF_FROZEN 2\n#define DMF_FREEING 3\n#define DMF_DELETING 4\n#define DMF_NOFLUSH_SUSPENDING 5\n#define DMF_DEFERRED_REMOVE 6\n#define DMF_SUSPENDED_INTERNALLY 7\n\n#define DM_NUMA_NODE NUMA_NO_NODE\nstatic int dm_numa_node = DM_NUMA_NODE;\n\n/*\n * For mempools pre-allocation at the table loading time.\n */\nstruct dm_md_mempools {\n\tmempool_t *io_pool;\n\tstruct bio_set *bs;\n};\n\nstruct table_device {\n\tstruct list_head list;\n\trefcount_t count;\n\tstruct dm_dev dm_dev;\n};\n\nstatic struct kmem_cache *_io_cache;\nstatic struct kmem_cache *_rq_tio_cache;\nstatic struct kmem_cache *_rq_cache;\n\n/*\n * Bio-based DM's mempools' reserved IOs set by the user.\n */\n#define RESERVED_BIO_BASED_IOS\t\t16\nstatic unsigned reserved_bio_based_ios = RESERVED_BIO_BASED_IOS;\n\nstatic int __dm_get_module_param_int(int *module_param, int min, int max)\n{\n\tint param = ACCESS_ONCE(*module_param);\n\tint modified_param = 0;\n\tbool modified = true;\n\n\tif (param < min)\n\t\tmodified_param = min;\n\telse if (param > max)\n\t\tmodified_param = max;\n\telse\n\t\tmodified = false;\n\n\tif (modified) {\n\t\t(void)cmpxchg(module_param, param, modified_param);\n\t\tparam = modified_param;\n\t}\n\n\treturn param;\n}\n\nunsigned __dm_get_module_param(unsigned *module_param,\n\t\t\t       unsigned def, unsigned max)\n{\n\tunsigned param = ACCESS_ONCE(*module_param);\n\tunsigned modified_param = 0;\n\n\tif (!param)\n\t\tmodified_param = def;\n\telse if (param > max)\n\t\tmodified_param = max;\n\n\tif (modified_param) {\n\t\t(void)cmpxchg(module_param, param, modified_param);\n\t\tparam = modified_param;\n\t}\n\n\treturn param;\n}\n\nunsigned dm_get_reserved_bio_based_ios(void)\n{\n\treturn __dm_get_module_param(&reserved_bio_based_ios,\n\t\t\t\t     RESERVED_BIO_BASED_IOS, DM_RESERVED_MAX_IOS);\n}\nEXPORT_SYMBOL_GPL(dm_get_reserved_bio_based_ios);\n\nstatic unsigned dm_get_numa_node(void)\n{\n\treturn __dm_get_module_param_int(&dm_numa_node,\n\t\t\t\t\t DM_NUMA_NODE, num_online_nodes() - 1);\n}\n\nstatic int __init local_init(void)\n{\n\tint r = -ENOMEM;\n\n\t/* allocate a slab for the dm_ios */\n\t_io_cache = KMEM_CACHE(dm_io, 0);\n\tif (!_io_cache)\n\t\treturn r;\n\n\t_rq_tio_cache = KMEM_CACHE(dm_rq_target_io, 0);\n\tif (!_rq_tio_cache)\n\t\tgoto out_free_io_cache;\n\n\t_rq_cache = kmem_cache_create(\"dm_old_clone_request\", sizeof(struct request),\n\t\t\t\t      __alignof__(struct request), 0, NULL);\n\tif (!_rq_cache)\n\t\tgoto out_free_rq_tio_cache;\n\n\tr = dm_uevent_init();\n\tif (r)\n\t\tgoto out_free_rq_cache;\n\n\tdeferred_remove_workqueue = alloc_workqueue(\"kdmremove\", WQ_UNBOUND, 1);\n\tif (!deferred_remove_workqueue) {\n\t\tr = -ENOMEM;\n\t\tgoto out_uevent_exit;\n\t}\n\n\t_major = major;\n\tr = register_blkdev(_major, _name);\n\tif (r < 0)\n\t\tgoto out_free_workqueue;\n\n\tif (!_major)\n\t\t_major = r;\n\n\treturn 0;\n\nout_free_workqueue:\n\tdestroy_workqueue(deferred_remove_workqueue);\nout_uevent_exit:\n\tdm_uevent_exit();\nout_free_rq_cache:\n\tkmem_cache_destroy(_rq_cache);\nout_free_rq_tio_cache:\n\tkmem_cache_destroy(_rq_tio_cache);\nout_free_io_cache:\n\tkmem_cache_destroy(_io_cache);\n\n\treturn r;\n}\n\nstatic void local_exit(void)\n{\n\tflush_scheduled_work();\n\tdestroy_workqueue(deferred_remove_workqueue);\n\n\tkmem_cache_destroy(_rq_cache);\n\tkmem_cache_destroy(_rq_tio_cache);\n\tkmem_cache_destroy(_io_cache);\n\tunregister_blkdev(_major, _name);\n\tdm_uevent_exit();\n\n\t_major = 0;\n\n\tDMINFO(\"cleaned up\");\n}\n\nstatic int (*_inits[])(void) __initdata = {\n\tlocal_init,\n\tdm_target_init,\n\tdm_linear_init,\n\tdm_stripe_init,\n\tdm_io_init,\n\tdm_kcopyd_init,\n\tdm_interface_init,\n\tdm_statistics_init,\n};\n\nstatic void (*_exits[])(void) = {\n\tlocal_exit,\n\tdm_target_exit,\n\tdm_linear_exit,\n\tdm_stripe_exit,\n\tdm_io_exit,\n\tdm_kcopyd_exit,\n\tdm_interface_exit,\n\tdm_statistics_exit,\n};\n\nstatic int __init dm_init(void)\n{\n\tconst int count = ARRAY_SIZE(_inits);\n\n\tint r, i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tr = _inits[i]();\n\t\tif (r)\n\t\t\tgoto bad;\n\t}\n\n\treturn 0;\n\n      bad:\n\twhile (i--)\n\t\t_exits[i]();\n\n\treturn r;\n}\n\nstatic void __exit dm_exit(void)\n{\n\tint i = ARRAY_SIZE(_exits);\n\n\twhile (i--)\n\t\t_exits[i]();\n\n\t/*\n\t * Should be empty by this point.\n\t */\n\tidr_destroy(&_minor_idr);\n}\n\n/*\n * Block device functions\n */\nint dm_deleting_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_DELETING, &md->flags);\n}\n\nstatic int dm_blk_open(struct block_device *bdev, fmode_t mode)\n{\n\tstruct mapped_device *md;\n\n\tspin_lock(&_minor_lock);\n\n\tmd = bdev->bd_disk->private_data;\n\tif (!md)\n\t\tgoto out;\n\n\tif (test_bit(DMF_FREEING, &md->flags) ||\n\t    dm_deleting_md(md)) {\n\t\tmd = NULL;\n\t\tgoto out;\n\t}\n\n\tdm_get(md);\n\tatomic_inc(&md->open_count);\nout:\n\tspin_unlock(&_minor_lock);\n\n\treturn md ? 0 : -ENXIO;\n}\n\nstatic void dm_blk_close(struct gendisk *disk, fmode_t mode)\n{\n\tstruct mapped_device *md;\n\n\tspin_lock(&_minor_lock);\n\n\tmd = disk->private_data;\n\tif (WARN_ON(!md))\n\t\tgoto out;\n\n\tif (atomic_dec_and_test(&md->open_count) &&\n\t    (test_bit(DMF_DEFERRED_REMOVE, &md->flags)))\n\t\tqueue_work(deferred_remove_workqueue, &deferred_remove_work);\n\n\tdm_put(md);\nout:\n\tspin_unlock(&_minor_lock);\n}\n\nint dm_open_count(struct mapped_device *md)\n{\n\treturn atomic_read(&md->open_count);\n}\n\n/*\n * Guarantees nothing is using the device before it's deleted.\n */\nint dm_lock_for_deletion(struct mapped_device *md, bool mark_deferred, bool only_deferred)\n{\n\tint r = 0;\n\n\tspin_lock(&_minor_lock);\n\n\tif (dm_open_count(md)) {\n\t\tr = -EBUSY;\n\t\tif (mark_deferred)\n\t\t\tset_bit(DMF_DEFERRED_REMOVE, &md->flags);\n\t} else if (only_deferred && !test_bit(DMF_DEFERRED_REMOVE, &md->flags))\n\t\tr = -EEXIST;\n\telse\n\t\tset_bit(DMF_DELETING, &md->flags);\n\n\tspin_unlock(&_minor_lock);\n\n\treturn r;\n}\n\nint dm_cancel_deferred_remove(struct mapped_device *md)\n{\n\tint r = 0;\n\n\tspin_lock(&_minor_lock);\n\n\tif (test_bit(DMF_DELETING, &md->flags))\n\t\tr = -EBUSY;\n\telse\n\t\tclear_bit(DMF_DEFERRED_REMOVE, &md->flags);\n\n\tspin_unlock(&_minor_lock);\n\n\treturn r;\n}\n\nstatic void do_deferred_remove(struct work_struct *w)\n{\n\tdm_deferred_remove();\n}\n\nsector_t dm_get_size(struct mapped_device *md)\n{\n\treturn get_capacity(md->disk);\n}\n\nstruct request_queue *dm_get_md_queue(struct mapped_device *md)\n{\n\treturn md->queue;\n}\n\nstruct dm_stats *dm_get_stats(struct mapped_device *md)\n{\n\treturn &md->stats;\n}\n\nstatic int dm_blk_getgeo(struct block_device *bdev, struct hd_geometry *geo)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\n\treturn dm_get_geometry(md, geo);\n}\n\nstatic int dm_grab_bdev_for_ioctl(struct mapped_device *md,\n\t\t\t\t  struct block_device **bdev,\n\t\t\t\t  fmode_t *mode)\n{\n\tstruct dm_target *tgt;\n\tstruct dm_table *map;\n\tint srcu_idx, r;\n\nretry:\n\tr = -ENOTTY;\n\tmap = dm_get_live_table(md, &srcu_idx);\n\tif (!map || !dm_table_get_size(map))\n\t\tgoto out;\n\n\t/* We only support devices that have a single target */\n\tif (dm_table_get_num_targets(map) != 1)\n\t\tgoto out;\n\n\ttgt = dm_table_get_target(map, 0);\n\tif (!tgt->type->prepare_ioctl)\n\t\tgoto out;\n\n\tif (dm_suspended_md(md)) {\n\t\tr = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tr = tgt->type->prepare_ioctl(tgt, bdev, mode);\n\tif (r < 0)\n\t\tgoto out;\n\n\tbdgrab(*bdev);\n\tdm_put_live_table(md, srcu_idx);\n\treturn r;\n\nout:\n\tdm_put_live_table(md, srcu_idx);\n\tif (r == -ENOTCONN && !fatal_signal_pending(current)) {\n\t\tmsleep(10);\n\t\tgoto retry;\n\t}\n\treturn r;\n}\n\nstatic int dm_blk_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t\tunsigned int cmd, unsigned long arg)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (r > 0) {\n\t\t/*\n\t\t * Target determined this ioctl is being issued against a\n\t\t * subset of the parent bdev; require extra privileges.\n\t\t */\n\t\tif (!capable(CAP_SYS_RAWIO)) {\n\t\t\tDMWARN_LIMIT(\n\t\"%s: sending ioctl %x to DM device without required privilege.\",\n\t\t\t\tcurrent->comm, cmd);\n\t\t\tr = -ENOIOCTLCMD;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr =  __blkdev_driver_ioctl(bdev, mode, cmd, arg);\nout:\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic struct dm_io *alloc_io(struct mapped_device *md)\n{\n\treturn mempool_alloc(md->io_pool, GFP_NOIO);\n}\n\nstatic void free_io(struct mapped_device *md, struct dm_io *io)\n{\n\tmempool_free(io, md->io_pool);\n}\n\nstatic void free_tio(struct dm_target_io *tio)\n{\n\tbio_put(&tio->clone);\n}\n\nint md_in_flight(struct mapped_device *md)\n{\n\treturn atomic_read(&md->pending[READ]) +\n\t       atomic_read(&md->pending[WRITE]);\n}\n\nstatic void start_io_acct(struct dm_io *io)\n{\n\tstruct mapped_device *md = io->md;\n\tstruct bio *bio = io->bio;\n\tint cpu;\n\tint rw = bio_data_dir(bio);\n\n\tio->start_time = jiffies;\n\n\tcpu = part_stat_lock();\n\tpart_round_stats(md->queue, cpu, &dm_disk(md)->part0);\n\tpart_stat_unlock();\n\tatomic_set(&dm_disk(md)->part0.in_flight[rw],\n\t\tatomic_inc_return(&md->pending[rw]));\n\n\tif (unlikely(dm_stats_used(&md->stats)))\n\t\tdm_stats_account_io(&md->stats, bio_data_dir(bio),\n\t\t\t\t    bio->bi_iter.bi_sector, bio_sectors(bio),\n\t\t\t\t    false, 0, &io->stats_aux);\n}\n\nstatic void end_io_acct(struct dm_io *io)\n{\n\tstruct mapped_device *md = io->md;\n\tstruct bio *bio = io->bio;\n\tunsigned long duration = jiffies - io->start_time;\n\tint pending;\n\tint rw = bio_data_dir(bio);\n\n\tgeneric_end_io_acct(md->queue, rw, &dm_disk(md)->part0, io->start_time);\n\n\tif (unlikely(dm_stats_used(&md->stats)))\n\t\tdm_stats_account_io(&md->stats, bio_data_dir(bio),\n\t\t\t\t    bio->bi_iter.bi_sector, bio_sectors(bio),\n\t\t\t\t    true, duration, &io->stats_aux);\n\n\t/*\n\t * After this is decremented the bio must not be touched if it is\n\t * a flush.\n\t */\n\tpending = atomic_dec_return(&md->pending[rw]);\n\tatomic_set(&dm_disk(md)->part0.in_flight[rw], pending);\n\tpending += atomic_read(&md->pending[rw^0x1]);\n\n\t/* nudge anyone waiting on suspend queue */\n\tif (!pending)\n\t\twake_up(&md->wait);\n}\n\n/*\n * Add the bio to the list of deferred io.\n */\nstatic void queue_io(struct mapped_device *md, struct bio *bio)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md->deferred_lock, flags);\n\tbio_list_add(&md->deferred, bio);\n\tspin_unlock_irqrestore(&md->deferred_lock, flags);\n\tqueue_work(md->wq, &md->work);\n}\n\n/*\n * Everyone (including functions in this file), should use this\n * function to access the md->map field, and make sure they call\n * dm_put_live_table() when finished.\n */\nstruct dm_table *dm_get_live_table(struct mapped_device *md, int *srcu_idx) __acquires(md->io_barrier)\n{\n\t*srcu_idx = srcu_read_lock(&md->io_barrier);\n\n\treturn srcu_dereference(md->map, &md->io_barrier);\n}\n\nvoid dm_put_live_table(struct mapped_device *md, int srcu_idx) __releases(md->io_barrier)\n{\n\tsrcu_read_unlock(&md->io_barrier, srcu_idx);\n}\n\nvoid dm_sync_table(struct mapped_device *md)\n{\n\tsynchronize_srcu(&md->io_barrier);\n\tsynchronize_rcu_expedited();\n}\n\n/*\n * A fast alternative to dm_get_live_table/dm_put_live_table.\n * The caller must not block between these two functions.\n */\nstatic struct dm_table *dm_get_live_table_fast(struct mapped_device *md) __acquires(RCU)\n{\n\trcu_read_lock();\n\treturn rcu_dereference(md->map);\n}\n\nstatic void dm_put_live_table_fast(struct mapped_device *md) __releases(RCU)\n{\n\trcu_read_unlock();\n}\n\n/*\n * Open a table device so we can use it as a map destination.\n */\nstatic int open_table_device(struct table_device *td, dev_t dev,\n\t\t\t     struct mapped_device *md)\n{\n\tstatic char *_claim_ptr = \"I belong to device-mapper\";\n\tstruct block_device *bdev;\n\n\tint r;\n\n\tBUG_ON(td->dm_dev.bdev);\n\n\tbdev = blkdev_get_by_dev(dev, td->dm_dev.mode | FMODE_EXCL, _claim_ptr);\n\tif (IS_ERR(bdev))\n\t\treturn PTR_ERR(bdev);\n\n\tr = bd_link_disk_holder(bdev, dm_disk(md));\n\tif (r) {\n\t\tblkdev_put(bdev, td->dm_dev.mode | FMODE_EXCL);\n\t\treturn r;\n\t}\n\n\ttd->dm_dev.bdev = bdev;\n\ttd->dm_dev.dax_dev = dax_get_by_host(bdev->bd_disk->disk_name);\n\treturn 0;\n}\n\n/*\n * Close a table device that we've been using.\n */\nstatic void close_table_device(struct table_device *td, struct mapped_device *md)\n{\n\tif (!td->dm_dev.bdev)\n\t\treturn;\n\n\tbd_unlink_disk_holder(td->dm_dev.bdev, dm_disk(md));\n\tblkdev_put(td->dm_dev.bdev, td->dm_dev.mode | FMODE_EXCL);\n\tput_dax(td->dm_dev.dax_dev);\n\ttd->dm_dev.bdev = NULL;\n\ttd->dm_dev.dax_dev = NULL;\n}\n\nstatic struct table_device *find_table_device(struct list_head *l, dev_t dev,\n\t\t\t\t\t      fmode_t mode) {\n\tstruct table_device *td;\n\n\tlist_for_each_entry(td, l, list)\n\t\tif (td->dm_dev.bdev->bd_dev == dev && td->dm_dev.mode == mode)\n\t\t\treturn td;\n\n\treturn NULL;\n}\n\nint dm_get_table_device(struct mapped_device *md, dev_t dev, fmode_t mode,\n\t\t\tstruct dm_dev **result) {\n\tint r;\n\tstruct table_device *td;\n\n\tmutex_lock(&md->table_devices_lock);\n\ttd = find_table_device(&md->table_devices, dev, mode);\n\tif (!td) {\n\t\ttd = kmalloc_node(sizeof(*td), GFP_KERNEL, md->numa_node_id);\n\t\tif (!td) {\n\t\t\tmutex_unlock(&md->table_devices_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttd->dm_dev.mode = mode;\n\t\ttd->dm_dev.bdev = NULL;\n\n\t\tif ((r = open_table_device(td, dev, md))) {\n\t\t\tmutex_unlock(&md->table_devices_lock);\n\t\t\tkfree(td);\n\t\t\treturn r;\n\t\t}\n\n\t\tformat_dev_t(td->dm_dev.name, dev);\n\n\t\trefcount_set(&td->count, 1);\n\t\tlist_add(&td->list, &md->table_devices);\n\t} else {\n\t\trefcount_inc(&td->count);\n\t}\n\tmutex_unlock(&md->table_devices_lock);\n\n\t*result = &td->dm_dev;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_get_table_device);\n\nvoid dm_put_table_device(struct mapped_device *md, struct dm_dev *d)\n{\n\tstruct table_device *td = container_of(d, struct table_device, dm_dev);\n\n\tmutex_lock(&md->table_devices_lock);\n\tif (refcount_dec_and_test(&td->count)) {\n\t\tclose_table_device(td, md);\n\t\tlist_del(&td->list);\n\t\tkfree(td);\n\t}\n\tmutex_unlock(&md->table_devices_lock);\n}\nEXPORT_SYMBOL(dm_put_table_device);\n\nstatic void free_table_devices(struct list_head *devices)\n{\n\tstruct list_head *tmp, *next;\n\n\tlist_for_each_safe(tmp, next, devices) {\n\t\tstruct table_device *td = list_entry(tmp, struct table_device, list);\n\n\t\tDMWARN(\"dm_destroy: %s still exists with %d references\",\n\t\t       td->dm_dev.name, refcount_read(&td->count));\n\t\tkfree(td);\n\t}\n}\n\n/*\n * Get the geometry associated with a dm device\n */\nint dm_get_geometry(struct mapped_device *md, struct hd_geometry *geo)\n{\n\t*geo = md->geometry;\n\n\treturn 0;\n}\n\n/*\n * Set the geometry of a device.\n */\nint dm_set_geometry(struct mapped_device *md, struct hd_geometry *geo)\n{\n\tsector_t sz = (sector_t)geo->cylinders * geo->heads * geo->sectors;\n\n\tif (geo->start > sz) {\n\t\tDMWARN(\"Start sector is beyond the geometry limits.\");\n\t\treturn -EINVAL;\n\t}\n\n\tmd->geometry = *geo;\n\n\treturn 0;\n}\n\n/*-----------------------------------------------------------------\n * CRUD START:\n *   A more elegant soln is in the works that uses the queue\n *   merge fn, unfortunately there are a couple of changes to\n *   the block layer that I want to make for this.  So in the\n *   interests of getting something for people to use I give\n *   you this clearly demarcated crap.\n *---------------------------------------------------------------*/\n\nstatic int __noflush_suspending(struct mapped_device *md)\n{\n\treturn test_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);\n}\n\n/*\n * Decrements the number of outstanding ios that a bio has been\n * cloned into, completing the original io if necc.\n */\nstatic void dec_pending(struct dm_io *io, blk_status_t error)\n{\n\tunsigned long flags;\n\tblk_status_t io_error;\n\tstruct bio *bio;\n\tstruct mapped_device *md = io->md;\n\n\t/* Push-back supersedes any I/O errors */\n\tif (unlikely(error)) {\n\t\tspin_lock_irqsave(&io->endio_lock, flags);\n\t\tif (!(io->status == BLK_STS_DM_REQUEUE &&\n\t\t\t\t__noflush_suspending(md)))\n\t\t\tio->status = error;\n\t\tspin_unlock_irqrestore(&io->endio_lock, flags);\n\t}\n\n\tif (atomic_dec_and_test(&io->io_count)) {\n\t\tif (io->status == BLK_STS_DM_REQUEUE) {\n\t\t\t/*\n\t\t\t * Target requested pushing back the I/O.\n\t\t\t */\n\t\t\tspin_lock_irqsave(&md->deferred_lock, flags);\n\t\t\tif (__noflush_suspending(md))\n\t\t\t\tbio_list_add_head(&md->deferred, io->bio);\n\t\t\telse\n\t\t\t\t/* noflush suspend was interrupted. */\n\t\t\t\tio->status = BLK_STS_IOERR;\n\t\t\tspin_unlock_irqrestore(&md->deferred_lock, flags);\n\t\t}\n\n\t\tio_error = io->status;\n\t\tbio = io->bio;\n\t\tend_io_acct(io);\n\t\tfree_io(md, io);\n\n\t\tif (io_error == BLK_STS_DM_REQUEUE)\n\t\t\treturn;\n\n\t\tif ((bio->bi_opf & REQ_PREFLUSH) && bio->bi_iter.bi_size) {\n\t\t\t/*\n\t\t\t * Preflush done for flush with data, reissue\n\t\t\t * without REQ_PREFLUSH.\n\t\t\t */\n\t\t\tbio->bi_opf &= ~REQ_PREFLUSH;\n\t\t\tqueue_io(md, bio);\n\t\t} else {\n\t\t\t/* done with normal IO or empty flush */\n\t\t\tbio->bi_status = io_error;\n\t\t\tbio_endio(bio);\n\t\t}\n\t}\n}\n\nvoid disable_write_same(struct mapped_device *md)\n{\n\tstruct queue_limits *limits = dm_get_queue_limits(md);\n\n\t/* device doesn't really support WRITE SAME, disable it */\n\tlimits->max_write_same_sectors = 0;\n}\n\nvoid disable_write_zeroes(struct mapped_device *md)\n{\n\tstruct queue_limits *limits = dm_get_queue_limits(md);\n\n\t/* device doesn't really support WRITE ZEROES, disable it */\n\tlimits->max_write_zeroes_sectors = 0;\n}\n\nstatic void clone_endio(struct bio *bio)\n{\n\tblk_status_t error = bio->bi_status;\n\tstruct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);\n\tstruct dm_io *io = tio->io;\n\tstruct mapped_device *md = tio->io->md;\n\tdm_endio_fn endio = tio->ti->type->end_io;\n\n\tif (unlikely(error == BLK_STS_TARGET)) {\n\t\tif (bio_op(bio) == REQ_OP_WRITE_SAME &&\n\t\t    !bio->bi_disk->queue->limits.max_write_same_sectors)\n\t\t\tdisable_write_same(md);\n\t\tif (bio_op(bio) == REQ_OP_WRITE_ZEROES &&\n\t\t    !bio->bi_disk->queue->limits.max_write_zeroes_sectors)\n\t\t\tdisable_write_zeroes(md);\n\t}\n\n\tif (endio) {\n\t\tint r = endio(tio->ti, bio, &error);\n\t\tswitch (r) {\n\t\tcase DM_ENDIO_REQUEUE:\n\t\t\terror = BLK_STS_DM_REQUEUE;\n\t\t\t/*FALLTHRU*/\n\t\tcase DM_ENDIO_DONE:\n\t\t\tbreak;\n\t\tcase DM_ENDIO_INCOMPLETE:\n\t\t\t/* The target will handle the io */\n\t\t\treturn;\n\t\tdefault:\n\t\t\tDMWARN(\"unimplemented target endio return value: %d\", r);\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tfree_tio(tio);\n\tdec_pending(io, error);\n}\n\n/*\n * Return maximum size of I/O possible at the supplied sector up to the current\n * target boundary.\n */\nstatic sector_t max_io_len_target_boundary(sector_t sector, struct dm_target *ti)\n{\n\tsector_t target_offset = dm_target_offset(ti, sector);\n\n\treturn ti->len - target_offset;\n}\n\nstatic sector_t max_io_len(sector_t sector, struct dm_target *ti)\n{\n\tsector_t len = max_io_len_target_boundary(sector, ti);\n\tsector_t offset, max_len;\n\n\t/*\n\t * Does the target need to split even further?\n\t */\n\tif (ti->max_io_len) {\n\t\toffset = dm_target_offset(ti, sector);\n\t\tif (unlikely(ti->max_io_len & (ti->max_io_len - 1)))\n\t\t\tmax_len = sector_div(offset, ti->max_io_len);\n\t\telse\n\t\t\tmax_len = offset & (ti->max_io_len - 1);\n\t\tmax_len = ti->max_io_len - max_len;\n\n\t\tif (len > max_len)\n\t\t\tlen = max_len;\n\t}\n\n\treturn len;\n}\n\nint dm_set_target_max_io_len(struct dm_target *ti, sector_t len)\n{\n\tif (len > UINT_MAX) {\n\t\tDMERR(\"Specified maximum size of target IO (%llu) exceeds limit (%u)\",\n\t\t      (unsigned long long)len, UINT_MAX);\n\t\tti->error = \"Maximum size of target IO is too large\";\n\t\treturn -EINVAL;\n\t}\n\n\tti->max_io_len = (uint32_t) len;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_set_target_max_io_len);\n\nstatic struct dm_target *dm_dax_get_live_target(struct mapped_device *md,\n\t\tsector_t sector, int *srcu_idx)\n{\n\tstruct dm_table *map;\n\tstruct dm_target *ti;\n\n\tmap = dm_get_live_table(md, srcu_idx);\n\tif (!map)\n\t\treturn NULL;\n\n\tti = dm_table_find_target(map, sector);\n\tif (!dm_target_is_valid(ti))\n\t\treturn NULL;\n\n\treturn ti;\n}\n\nstatic long dm_dax_direct_access(struct dax_device *dax_dev, pgoff_t pgoff,\n\t\tlong nr_pages, void **kaddr, pfn_t *pfn)\n{\n\tstruct mapped_device *md = dax_get_private(dax_dev);\n\tsector_t sector = pgoff * PAGE_SECTORS;\n\tstruct dm_target *ti;\n\tlong len, ret = -EIO;\n\tint srcu_idx;\n\n\tti = dm_dax_get_live_target(md, sector, &srcu_idx);\n\n\tif (!ti)\n\t\tgoto out;\n\tif (!ti->type->direct_access)\n\t\tgoto out;\n\tlen = max_io_len(sector, ti) / PAGE_SECTORS;\n\tif (len < 1)\n\t\tgoto out;\n\tnr_pages = min(len, nr_pages);\n\tif (ti->type->direct_access)\n\t\tret = ti->type->direct_access(ti, pgoff, nr_pages, kaddr, pfn);\n\n out:\n\tdm_put_live_table(md, srcu_idx);\n\n\treturn ret;\n}\n\nstatic size_t dm_dax_copy_from_iter(struct dax_device *dax_dev, pgoff_t pgoff,\n\t\tvoid *addr, size_t bytes, struct iov_iter *i)\n{\n\tstruct mapped_device *md = dax_get_private(dax_dev);\n\tsector_t sector = pgoff * PAGE_SECTORS;\n\tstruct dm_target *ti;\n\tlong ret = 0;\n\tint srcu_idx;\n\n\tti = dm_dax_get_live_target(md, sector, &srcu_idx);\n\n\tif (!ti)\n\t\tgoto out;\n\tif (!ti->type->dax_copy_from_iter) {\n\t\tret = copy_from_iter(addr, bytes, i);\n\t\tgoto out;\n\t}\n\tret = ti->type->dax_copy_from_iter(ti, pgoff, addr, bytes, i);\n out:\n\tdm_put_live_table(md, srcu_idx);\n\n\treturn ret;\n}\n\n/*\n * A target may call dm_accept_partial_bio only from the map routine.  It is\n * allowed for all bio types except REQ_PREFLUSH.\n *\n * dm_accept_partial_bio informs the dm that the target only wants to process\n * additional n_sectors sectors of the bio and the rest of the data should be\n * sent in a next bio.\n *\n * A diagram that explains the arithmetics:\n * +--------------------+---------------+-------+\n * |         1          |       2       |   3   |\n * +--------------------+---------------+-------+\n *\n * <-------------- *tio->len_ptr --------------->\n *                      <------- bi_size ------->\n *                      <-- n_sectors -->\n *\n * Region 1 was already iterated over with bio_advance or similar function.\n *\t(it may be empty if the target doesn't use bio_advance)\n * Region 2 is the remaining bio size that the target wants to process.\n *\t(it may be empty if region 1 is non-empty, although there is no reason\n *\t to make it empty)\n * The target requires that region 3 is to be sent in the next bio.\n *\n * If the target wants to receive multiple copies of the bio (via num_*bios, etc),\n * the partially processed part (the sum of regions 1+2) must be the same for all\n * copies of the bio.\n */\nvoid dm_accept_partial_bio(struct bio *bio, unsigned n_sectors)\n{\n\tstruct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);\n\tunsigned bi_size = bio->bi_iter.bi_size >> SECTOR_SHIFT;\n\tBUG_ON(bio->bi_opf & REQ_PREFLUSH);\n\tBUG_ON(bi_size > *tio->len_ptr);\n\tBUG_ON(n_sectors > bi_size);\n\t*tio->len_ptr -= bi_size - n_sectors;\n\tbio->bi_iter.bi_size = n_sectors << SECTOR_SHIFT;\n}\nEXPORT_SYMBOL_GPL(dm_accept_partial_bio);\n\n/*\n * The zone descriptors obtained with a zone report indicate\n * zone positions within the target device. The zone descriptors\n * must be remapped to match their position within the dm device.\n * A target may call dm_remap_zone_report after completion of a\n * REQ_OP_ZONE_REPORT bio to remap the zone descriptors obtained\n * from the target device mapping to the dm device.\n */\nvoid dm_remap_zone_report(struct dm_target *ti, struct bio *bio, sector_t start)\n{\n#ifdef CONFIG_BLK_DEV_ZONED\n\tstruct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);\n\tstruct bio *report_bio = tio->io->bio;\n\tstruct blk_zone_report_hdr *hdr = NULL;\n\tstruct blk_zone *zone;\n\tunsigned int nr_rep = 0;\n\tunsigned int ofst;\n\tstruct bio_vec bvec;\n\tstruct bvec_iter iter;\n\tvoid *addr;\n\n\tif (bio->bi_status)\n\t\treturn;\n\n\t/*\n\t * Remap the start sector of the reported zones. For sequential zones,\n\t * also remap the write pointer position.\n\t */\n\tbio_for_each_segment(bvec, report_bio, iter) {\n\t\taddr = kmap_atomic(bvec.bv_page);\n\n\t\t/* Remember the report header in the first page */\n\t\tif (!hdr) {\n\t\t\thdr = addr;\n\t\t\tofst = sizeof(struct blk_zone_report_hdr);\n\t\t} else\n\t\t\tofst = 0;\n\n\t\t/* Set zones start sector */\n\t\twhile (hdr->nr_zones && ofst < bvec.bv_len) {\n\t\t\tzone = addr + ofst;\n\t\t\tif (zone->start >= start + ti->len) {\n\t\t\t\thdr->nr_zones = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tzone->start = zone->start + ti->begin - start;\n\t\t\tif (zone->type != BLK_ZONE_TYPE_CONVENTIONAL) {\n\t\t\t\tif (zone->cond == BLK_ZONE_COND_FULL)\n\t\t\t\t\tzone->wp = zone->start + zone->len;\n\t\t\t\telse if (zone->cond == BLK_ZONE_COND_EMPTY)\n\t\t\t\t\tzone->wp = zone->start;\n\t\t\t\telse\n\t\t\t\t\tzone->wp = zone->wp + ti->begin - start;\n\t\t\t}\n\t\t\tofst += sizeof(struct blk_zone);\n\t\t\thdr->nr_zones--;\n\t\t\tnr_rep++;\n\t\t}\n\n\t\tif (addr != hdr)\n\t\t\tkunmap_atomic(addr);\n\n\t\tif (!hdr->nr_zones)\n\t\t\tbreak;\n\t}\n\n\tif (hdr) {\n\t\thdr->nr_zones = nr_rep;\n\t\tkunmap_atomic(hdr);\n\t}\n\n\tbio_advance(report_bio, report_bio->bi_iter.bi_size);\n\n#else /* !CONFIG_BLK_DEV_ZONED */\n\tbio->bi_status = BLK_STS_NOTSUPP;\n#endif\n}\nEXPORT_SYMBOL_GPL(dm_remap_zone_report);\n\n/*\n * Flush current->bio_list when the target map method blocks.\n * This fixes deadlocks in snapshot and possibly in other targets.\n */\nstruct dm_offload {\n\tstruct blk_plug plug;\n\tstruct blk_plug_cb cb;\n};\n\nstatic void flush_current_bio_list(struct blk_plug_cb *cb, bool from_schedule)\n{\n\tstruct dm_offload *o = container_of(cb, struct dm_offload, cb);\n\tstruct bio_list list;\n\tstruct bio *bio;\n\tint i;\n\n\tINIT_LIST_HEAD(&o->cb.list);\n\n\tif (unlikely(!current->bio_list))\n\t\treturn;\n\n\tfor (i = 0; i < 2; i++) {\n\t\tlist = current->bio_list[i];\n\t\tbio_list_init(&current->bio_list[i]);\n\n\t\twhile ((bio = bio_list_pop(&list))) {\n\t\t\tstruct bio_set *bs = bio->bi_pool;\n\t\t\tif (unlikely(!bs) || bs == fs_bio_set ||\n\t\t\t    !bs->rescue_workqueue) {\n\t\t\t\tbio_list_add(&current->bio_list[i], bio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tspin_lock(&bs->rescue_lock);\n\t\t\tbio_list_add(&bs->rescue_list, bio);\n\t\t\tqueue_work(bs->rescue_workqueue, &bs->rescue_work);\n\t\t\tspin_unlock(&bs->rescue_lock);\n\t\t}\n\t}\n}\n\nstatic void dm_offload_start(struct dm_offload *o)\n{\n\tblk_start_plug(&o->plug);\n\to->cb.callback = flush_current_bio_list;\n\tlist_add(&o->cb.list, &current->plug->cb_list);\n}\n\nstatic void dm_offload_end(struct dm_offload *o)\n{\n\tlist_del(&o->cb.list);\n\tblk_finish_plug(&o->plug);\n}\n\nstatic void __map_bio(struct dm_target_io *tio)\n{\n\tint r;\n\tsector_t sector;\n\tstruct dm_offload o;\n\tstruct bio *clone = &tio->clone;\n\tstruct dm_target *ti = tio->ti;\n\n\tclone->bi_end_io = clone_endio;\n\n\t/*\n\t * Map the clone.  If r == 0 we don't need to do\n\t * anything, the target has assumed ownership of\n\t * this io.\n\t */\n\tatomic_inc(&tio->io->io_count);\n\tsector = clone->bi_iter.bi_sector;\n\n\tdm_offload_start(&o);\n\tr = ti->type->map(ti, clone);\n\tdm_offload_end(&o);\n\n\tswitch (r) {\n\tcase DM_MAPIO_SUBMITTED:\n\t\tbreak;\n\tcase DM_MAPIO_REMAPPED:\n\t\t/* the bio has been remapped so dispatch it */\n\t\ttrace_block_bio_remap(clone->bi_disk->queue, clone,\n\t\t\t\t      bio_dev(tio->io->bio), sector);\n\t\tgeneric_make_request(clone);\n\t\tbreak;\n\tcase DM_MAPIO_KILL:\n\t\tdec_pending(tio->io, BLK_STS_IOERR);\n\t\tfree_tio(tio);\n\t\tbreak;\n\tcase DM_MAPIO_REQUEUE:\n\t\tdec_pending(tio->io, BLK_STS_DM_REQUEUE);\n\t\tfree_tio(tio);\n\t\tbreak;\n\tdefault:\n\t\tDMWARN(\"unimplemented target map return value: %d\", r);\n\t\tBUG();\n\t}\n}\n\nstruct clone_info {\n\tstruct mapped_device *md;\n\tstruct dm_table *map;\n\tstruct bio *bio;\n\tstruct dm_io *io;\n\tsector_t sector;\n\tunsigned sector_count;\n};\n\nstatic void bio_setup_sector(struct bio *bio, sector_t sector, unsigned len)\n{\n\tbio->bi_iter.bi_sector = sector;\n\tbio->bi_iter.bi_size = to_bytes(len);\n}\n\n/*\n * Creates a bio that consists of range of complete bvecs.\n */\nstatic int clone_bio(struct dm_target_io *tio, struct bio *bio,\n\t\t     sector_t sector, unsigned len)\n{\n\tstruct bio *clone = &tio->clone;\n\n\t__bio_clone_fast(clone, bio);\n\n\tif (unlikely(bio_integrity(bio) != NULL)) {\n\t\tint r;\n\n\t\tif (unlikely(!dm_target_has_integrity(tio->ti->type) &&\n\t\t\t     !dm_target_passes_integrity(tio->ti->type))) {\n\t\t\tDMWARN(\"%s: the target %s doesn't support integrity data.\",\n\t\t\t\tdm_device_name(tio->io->md),\n\t\t\t\ttio->ti->type->name);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tr = bio_integrity_clone(clone, bio, GFP_NOIO);\n\t\tif (r < 0)\n\t\t\treturn r;\n\t}\n\n\tif (bio_op(bio) != REQ_OP_ZONE_REPORT)\n\t\tbio_advance(clone, to_bytes(sector - clone->bi_iter.bi_sector));\n\tclone->bi_iter.bi_size = to_bytes(len);\n\n\tif (unlikely(bio_integrity(bio) != NULL))\n\t\tbio_integrity_trim(clone);\n\n\treturn 0;\n}\n\nstatic struct dm_target_io *alloc_tio(struct clone_info *ci,\n\t\t\t\t      struct dm_target *ti,\n\t\t\t\t      unsigned target_bio_nr)\n{\n\tstruct dm_target_io *tio;\n\tstruct bio *clone;\n\n\tclone = bio_alloc_bioset(GFP_NOIO, 0, ci->md->bs);\n\ttio = container_of(clone, struct dm_target_io, clone);\n\n\ttio->io = ci->io;\n\ttio->ti = ti;\n\ttio->target_bio_nr = target_bio_nr;\n\n\treturn tio;\n}\n\nstatic void __clone_and_map_simple_bio(struct clone_info *ci,\n\t\t\t\t       struct dm_target *ti,\n\t\t\t\t       unsigned target_bio_nr, unsigned *len)\n{\n\tstruct dm_target_io *tio = alloc_tio(ci, ti, target_bio_nr);\n\tstruct bio *clone = &tio->clone;\n\n\ttio->len_ptr = len;\n\n\t__bio_clone_fast(clone, ci->bio);\n\tif (len)\n\t\tbio_setup_sector(clone, ci->sector, *len);\n\n\t__map_bio(tio);\n}\n\nstatic void __send_duplicate_bios(struct clone_info *ci, struct dm_target *ti,\n\t\t\t\t  unsigned num_bios, unsigned *len)\n{\n\tunsigned target_bio_nr;\n\n\tfor (target_bio_nr = 0; target_bio_nr < num_bios; target_bio_nr++)\n\t\t__clone_and_map_simple_bio(ci, ti, target_bio_nr, len);\n}\n\nstatic int __send_empty_flush(struct clone_info *ci)\n{\n\tunsigned target_nr = 0;\n\tstruct dm_target *ti;\n\n\tBUG_ON(bio_has_data(ci->bio));\n\twhile ((ti = dm_table_get_target(ci->map, target_nr++)))\n\t\t__send_duplicate_bios(ci, ti, ti->num_flush_bios, NULL);\n\n\treturn 0;\n}\n\nstatic int __clone_and_map_data_bio(struct clone_info *ci, struct dm_target *ti,\n\t\t\t\t     sector_t sector, unsigned *len)\n{\n\tstruct bio *bio = ci->bio;\n\tstruct dm_target_io *tio;\n\tunsigned target_bio_nr;\n\tunsigned num_target_bios = 1;\n\tint r = 0;\n\n\t/*\n\t * Does the target want to receive duplicate copies of the bio?\n\t */\n\tif (bio_data_dir(bio) == WRITE && ti->num_write_bios)\n\t\tnum_target_bios = ti->num_write_bios(ti, bio);\n\n\tfor (target_bio_nr = 0; target_bio_nr < num_target_bios; target_bio_nr++) {\n\t\ttio = alloc_tio(ci, ti, target_bio_nr);\n\t\ttio->len_ptr = len;\n\t\tr = clone_bio(tio, bio, sector, *len);\n\t\tif (r < 0) {\n\t\t\tfree_tio(tio);\n\t\t\tbreak;\n\t\t}\n\t\t__map_bio(tio);\n\t}\n\n\treturn r;\n}\n\ntypedef unsigned (*get_num_bios_fn)(struct dm_target *ti);\n\nstatic unsigned get_num_discard_bios(struct dm_target *ti)\n{\n\treturn ti->num_discard_bios;\n}\n\nstatic unsigned get_num_write_same_bios(struct dm_target *ti)\n{\n\treturn ti->num_write_same_bios;\n}\n\nstatic unsigned get_num_write_zeroes_bios(struct dm_target *ti)\n{\n\treturn ti->num_write_zeroes_bios;\n}\n\ntypedef bool (*is_split_required_fn)(struct dm_target *ti);\n\nstatic bool is_split_required_for_discard(struct dm_target *ti)\n{\n\treturn ti->split_discard_bios;\n}\n\nstatic int __send_changing_extent_only(struct clone_info *ci,\n\t\t\t\t       get_num_bios_fn get_num_bios,\n\t\t\t\t       is_split_required_fn is_split_required)\n{\n\tstruct dm_target *ti;\n\tunsigned len;\n\tunsigned num_bios;\n\n\tdo {\n\t\tti = dm_table_find_target(ci->map, ci->sector);\n\t\tif (!dm_target_is_valid(ti))\n\t\t\treturn -EIO;\n\n\t\t/*\n\t\t * Even though the device advertised support for this type of\n\t\t * request, that does not mean every target supports it, and\n\t\t * reconfiguration might also have changed that since the\n\t\t * check was performed.\n\t\t */\n\t\tnum_bios = get_num_bios ? get_num_bios(ti) : 0;\n\t\tif (!num_bios)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (is_split_required && !is_split_required(ti))\n\t\t\tlen = min((sector_t)ci->sector_count, max_io_len_target_boundary(ci->sector, ti));\n\t\telse\n\t\t\tlen = min((sector_t)ci->sector_count, max_io_len(ci->sector, ti));\n\n\t\t__send_duplicate_bios(ci, ti, num_bios, &len);\n\n\t\tci->sector += len;\n\t} while (ci->sector_count -= len);\n\n\treturn 0;\n}\n\nstatic int __send_discard(struct clone_info *ci)\n{\n\treturn __send_changing_extent_only(ci, get_num_discard_bios,\n\t\t\t\t\t   is_split_required_for_discard);\n}\n\nstatic int __send_write_same(struct clone_info *ci)\n{\n\treturn __send_changing_extent_only(ci, get_num_write_same_bios, NULL);\n}\n\nstatic int __send_write_zeroes(struct clone_info *ci)\n{\n\treturn __send_changing_extent_only(ci, get_num_write_zeroes_bios, NULL);\n}\n\n/*\n * Select the correct strategy for processing a non-flush bio.\n */\nstatic int __split_and_process_non_flush(struct clone_info *ci)\n{\n\tstruct bio *bio = ci->bio;\n\tstruct dm_target *ti;\n\tunsigned len;\n\tint r;\n\n\tif (unlikely(bio_op(bio) == REQ_OP_DISCARD))\n\t\treturn __send_discard(ci);\n\telse if (unlikely(bio_op(bio) == REQ_OP_WRITE_SAME))\n\t\treturn __send_write_same(ci);\n\telse if (unlikely(bio_op(bio) == REQ_OP_WRITE_ZEROES))\n\t\treturn __send_write_zeroes(ci);\n\n\tti = dm_table_find_target(ci->map, ci->sector);\n\tif (!dm_target_is_valid(ti))\n\t\treturn -EIO;\n\n\tif (bio_op(bio) == REQ_OP_ZONE_REPORT)\n\t\tlen = ci->sector_count;\n\telse\n\t\tlen = min_t(sector_t, max_io_len(ci->sector, ti),\n\t\t\t    ci->sector_count);\n\n\tr = __clone_and_map_data_bio(ci, ti, ci->sector, &len);\n\tif (r < 0)\n\t\treturn r;\n\n\tci->sector += len;\n\tci->sector_count -= len;\n\n\treturn 0;\n}\n\n/*\n * Entry point to split a bio into clones and submit them to the targets.\n */\nstatic void __split_and_process_bio(struct mapped_device *md,\n\t\t\t\t    struct dm_table *map, struct bio *bio)\n{\n\tstruct clone_info ci;\n\tint error = 0;\n\n\tif (unlikely(!map)) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\tci.map = map;\n\tci.md = md;\n\tci.io = alloc_io(md);\n\tci.io->status = 0;\n\tatomic_set(&ci.io->io_count, 1);\n\tci.io->bio = bio;\n\tci.io->md = md;\n\tspin_lock_init(&ci.io->endio_lock);\n\tci.sector = bio->bi_iter.bi_sector;\n\n\tstart_io_acct(ci.io);\n\n\tif (bio->bi_opf & REQ_PREFLUSH) {\n\t\tci.bio = &ci.md->flush_bio;\n\t\tci.sector_count = 0;\n\t\terror = __send_empty_flush(&ci);\n\t\t/* dec_pending submits any data associated with flush */\n\t} else if (bio_op(bio) == REQ_OP_ZONE_RESET) {\n\t\tci.bio = bio;\n\t\tci.sector_count = 0;\n\t\terror = __split_and_process_non_flush(&ci);\n\t} else {\n\t\tci.bio = bio;\n\t\tci.sector_count = bio_sectors(bio);\n\t\twhile (ci.sector_count && !error)\n\t\t\terror = __split_and_process_non_flush(&ci);\n\t}\n\n\t/* drop the extra reference count */\n\tdec_pending(ci.io, errno_to_blk_status(error));\n}\n/*-----------------------------------------------------------------\n * CRUD END\n *---------------------------------------------------------------*/\n\n/*\n * The request function that just remaps the bio built up by\n * dm_merge_bvec.\n */\nstatic blk_qc_t dm_make_request(struct request_queue *q, struct bio *bio)\n{\n\tint rw = bio_data_dir(bio);\n\tstruct mapped_device *md = q->queuedata;\n\tint srcu_idx;\n\tstruct dm_table *map;\n\n\tmap = dm_get_live_table(md, &srcu_idx);\n\n\tgeneric_start_io_acct(q, rw, bio_sectors(bio), &dm_disk(md)->part0);\n\n\t/* if we're suspended, we have to queue this io for later */\n\tif (unlikely(test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags))) {\n\t\tdm_put_live_table(md, srcu_idx);\n\n\t\tif (!(bio->bi_opf & REQ_RAHEAD))\n\t\t\tqueue_io(md, bio);\n\t\telse\n\t\t\tbio_io_error(bio);\n\t\treturn BLK_QC_T_NONE;\n\t}\n\n\t__split_and_process_bio(md, map, bio);\n\tdm_put_live_table(md, srcu_idx);\n\treturn BLK_QC_T_NONE;\n}\n\nstatic int dm_any_congested(void *congested_data, int bdi_bits)\n{\n\tint r = bdi_bits;\n\tstruct mapped_device *md = congested_data;\n\tstruct dm_table *map;\n\n\tif (!test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags)) {\n\t\tif (dm_request_based(md)) {\n\t\t\t/*\n\t\t\t * With request-based DM we only need to check the\n\t\t\t * top-level queue for congestion.\n\t\t\t */\n\t\t\tr = md->queue->backing_dev_info->wb.state & bdi_bits;\n\t\t} else {\n\t\t\tmap = dm_get_live_table_fast(md);\n\t\t\tif (map)\n\t\t\t\tr = dm_table_any_congested(map, bdi_bits);\n\t\t\tdm_put_live_table_fast(md);\n\t\t}\n\t}\n\n\treturn r;\n}\n\n/*-----------------------------------------------------------------\n * An IDR is used to keep track of allocated minor numbers.\n *---------------------------------------------------------------*/\nstatic void free_minor(int minor)\n{\n\tspin_lock(&_minor_lock);\n\tidr_remove(&_minor_idr, minor);\n\tspin_unlock(&_minor_lock);\n}\n\n/*\n * See if the device with a specific minor # is free.\n */\nstatic int specific_minor(int minor)\n{\n\tint r;\n\n\tif (minor >= (1 << MINORBITS))\n\t\treturn -EINVAL;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&_minor_lock);\n\n\tr = idr_alloc(&_minor_idr, MINOR_ALLOCED, minor, minor + 1, GFP_NOWAIT);\n\n\tspin_unlock(&_minor_lock);\n\tidr_preload_end();\n\tif (r < 0)\n\t\treturn r == -ENOSPC ? -EBUSY : r;\n\treturn 0;\n}\n\nstatic int next_free_minor(int *minor)\n{\n\tint r;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&_minor_lock);\n\n\tr = idr_alloc(&_minor_idr, MINOR_ALLOCED, 0, 1 << MINORBITS, GFP_NOWAIT);\n\n\tspin_unlock(&_minor_lock);\n\tidr_preload_end();\n\tif (r < 0)\n\t\treturn r;\n\t*minor = r;\n\treturn 0;\n}\n\nstatic const struct block_device_operations dm_blk_dops;\nstatic const struct dax_operations dm_dax_ops;\n\nstatic void dm_wq_work(struct work_struct *work);\n\nvoid dm_init_md_queue(struct mapped_device *md)\n{\n\t/*\n\t * Request-based dm devices cannot be stacked on top of bio-based dm\n\t * devices.  The type of this dm device may not have been decided yet.\n\t * The type is decided at the first table loading time.\n\t * To prevent problematic device stacking, clear the queue flag\n\t * for request stacking support until then.\n\t *\n\t * This queue is new, so no concurrency on the queue_flags.\n\t */\n\tqueue_flag_clear_unlocked(QUEUE_FLAG_STACKABLE, md->queue);\n\n\t/*\n\t * Initialize data that will only be used by a non-blk-mq DM queue\n\t * - must do so here (in alloc_dev callchain) before queue is used\n\t */\n\tmd->queue->queuedata = md;\n\tmd->queue->backing_dev_info->congested_data = md;\n}\n\nvoid dm_init_normal_md_queue(struct mapped_device *md)\n{\n\tmd->use_blk_mq = false;\n\tdm_init_md_queue(md);\n\n\t/*\n\t * Initialize aspects of queue that aren't relevant for blk-mq\n\t */\n\tmd->queue->backing_dev_info->congested_fn = dm_any_congested;\n}\n\nstatic void cleanup_mapped_device(struct mapped_device *md)\n{\n\tif (md->wq)\n\t\tdestroy_workqueue(md->wq);\n\tif (md->kworker_task)\n\t\tkthread_stop(md->kworker_task);\n\tmempool_destroy(md->io_pool);\n\tif (md->bs)\n\t\tbioset_free(md->bs);\n\n\tif (md->dax_dev) {\n\t\tkill_dax(md->dax_dev);\n\t\tput_dax(md->dax_dev);\n\t\tmd->dax_dev = NULL;\n\t}\n\n\tif (md->disk) {\n\t\tspin_lock(&_minor_lock);\n\t\tmd->disk->private_data = NULL;\n\t\tspin_unlock(&_minor_lock);\n\t\tdel_gendisk(md->disk);\n\t\tput_disk(md->disk);\n\t}\n\n\tif (md->queue)\n\t\tblk_cleanup_queue(md->queue);\n\n\tcleanup_srcu_struct(&md->io_barrier);\n\n\tif (md->bdev) {\n\t\tbdput(md->bdev);\n\t\tmd->bdev = NULL;\n\t}\n\n\tdm_mq_cleanup_mapped_device(md);\n}\n\n/*\n * Allocate and initialise a blank device with a given minor.\n */\nstatic struct mapped_device *alloc_dev(int minor)\n{\n\tint r, numa_node_id = dm_get_numa_node();\n\tstruct dax_device *dax_dev;\n\tstruct mapped_device *md;\n\tvoid *old_md;\n\n\tmd = kvzalloc_node(sizeof(*md), GFP_KERNEL, numa_node_id);\n\tif (!md) {\n\t\tDMWARN(\"unable to allocate device, out of memory.\");\n\t\treturn NULL;\n\t}\n\n\tif (!try_module_get(THIS_MODULE))\n\t\tgoto bad_module_get;\n\n\t/* get a minor number for the dev */\n\tif (minor == DM_ANY_MINOR)\n\t\tr = next_free_minor(&minor);\n\telse\n\t\tr = specific_minor(minor);\n\tif (r < 0)\n\t\tgoto bad_minor;\n\n\tr = init_srcu_struct(&md->io_barrier);\n\tif (r < 0)\n\t\tgoto bad_io_barrier;\n\n\tmd->numa_node_id = numa_node_id;\n\tmd->use_blk_mq = dm_use_blk_mq_default();\n\tmd->init_tio_pdu = false;\n\tmd->type = DM_TYPE_NONE;\n\tmutex_init(&md->suspend_lock);\n\tmutex_init(&md->type_lock);\n\tmutex_init(&md->table_devices_lock);\n\tspin_lock_init(&md->deferred_lock);\n\tatomic_set(&md->holders, 1);\n\tatomic_set(&md->open_count, 0);\n\tatomic_set(&md->event_nr, 0);\n\tatomic_set(&md->uevent_seq, 0);\n\tINIT_LIST_HEAD(&md->uevent_list);\n\tINIT_LIST_HEAD(&md->table_devices);\n\tspin_lock_init(&md->uevent_lock);\n\n\tmd->queue = blk_alloc_queue_node(GFP_KERNEL, numa_node_id);\n\tif (!md->queue)\n\t\tgoto bad;\n\n\tdm_init_md_queue(md);\n\n\tmd->disk = alloc_disk_node(1, numa_node_id);\n\tif (!md->disk)\n\t\tgoto bad;\n\n\tatomic_set(&md->pending[0], 0);\n\tatomic_set(&md->pending[1], 0);\n\tinit_waitqueue_head(&md->wait);\n\tINIT_WORK(&md->work, dm_wq_work);\n\tinit_waitqueue_head(&md->eventq);\n\tinit_completion(&md->kobj_holder.completion);\n\tmd->kworker_task = NULL;\n\n\tmd->disk->major = _major;\n\tmd->disk->first_minor = minor;\n\tmd->disk->fops = &dm_blk_dops;\n\tmd->disk->queue = md->queue;\n\tmd->disk->private_data = md;\n\tsprintf(md->disk->disk_name, \"dm-%d\", minor);\n\n\tdax_dev = alloc_dax(md, md->disk->disk_name, &dm_dax_ops);\n\tif (!dax_dev)\n\t\tgoto bad;\n\tmd->dax_dev = dax_dev;\n\n\tadd_disk(md->disk);\n\tformat_dev_t(md->name, MKDEV(_major, minor));\n\n\tmd->wq = alloc_workqueue(\"kdmflush\", WQ_MEM_RECLAIM, 0);\n\tif (!md->wq)\n\t\tgoto bad;\n\n\tmd->bdev = bdget_disk(md->disk, 0);\n\tif (!md->bdev)\n\t\tgoto bad;\n\n\tbio_init(&md->flush_bio, NULL, 0);\n\tbio_set_dev(&md->flush_bio, md->bdev);\n\tmd->flush_bio.bi_opf = REQ_OP_WRITE | REQ_PREFLUSH | REQ_SYNC;\n\n\tdm_stats_init(&md->stats);\n\n\t/* Populate the mapping, nobody knows we exist yet */\n\tspin_lock(&_minor_lock);\n\told_md = idr_replace(&_minor_idr, md, minor);\n\tspin_unlock(&_minor_lock);\n\n\tBUG_ON(old_md != MINOR_ALLOCED);\n\n\treturn md;\n\nbad:\n\tcleanup_mapped_device(md);\nbad_io_barrier:\n\tfree_minor(minor);\nbad_minor:\n\tmodule_put(THIS_MODULE);\nbad_module_get:\n\tkvfree(md);\n\treturn NULL;\n}\n\nstatic void unlock_fs(struct mapped_device *md);\n\nstatic void free_dev(struct mapped_device *md)\n{\n\tint minor = MINOR(disk_devt(md->disk));\n\n\tunlock_fs(md);\n\n\tcleanup_mapped_device(md);\n\n\tfree_table_devices(&md->table_devices);\n\tdm_stats_cleanup(&md->stats);\n\tfree_minor(minor);\n\n\tmodule_put(THIS_MODULE);\n\tkvfree(md);\n}\n\nstatic void __bind_mempools(struct mapped_device *md, struct dm_table *t)\n{\n\tstruct dm_md_mempools *p = dm_table_get_md_mempools(t);\n\n\tif (md->bs) {\n\t\t/* The md already has necessary mempools. */\n\t\tif (dm_table_bio_based(t)) {\n\t\t\t/*\n\t\t\t * Reload bioset because front_pad may have changed\n\t\t\t * because a different table was loaded.\n\t\t\t */\n\t\t\tbioset_free(md->bs);\n\t\t\tmd->bs = p->bs;\n\t\t\tp->bs = NULL;\n\t\t}\n\t\t/*\n\t\t * There's no need to reload with request-based dm\n\t\t * because the size of front_pad doesn't change.\n\t\t * Note for future: If you are to reload bioset,\n\t\t * prep-ed requests in the queue may refer\n\t\t * to bio from the old bioset, so you must walk\n\t\t * through the queue to unprep.\n\t\t */\n\t\tgoto out;\n\t}\n\n\tBUG_ON(!p || md->io_pool || md->bs);\n\n\tmd->io_pool = p->io_pool;\n\tp->io_pool = NULL;\n\tmd->bs = p->bs;\n\tp->bs = NULL;\n\nout:\n\t/* mempool bind completed, no longer need any mempools in the table */\n\tdm_table_free_md_mempools(t);\n}\n\n/*\n * Bind a table to the device.\n */\nstatic void event_callback(void *context)\n{\n\tunsigned long flags;\n\tLIST_HEAD(uevents);\n\tstruct mapped_device *md = (struct mapped_device *) context;\n\n\tspin_lock_irqsave(&md->uevent_lock, flags);\n\tlist_splice_init(&md->uevent_list, &uevents);\n\tspin_unlock_irqrestore(&md->uevent_lock, flags);\n\n\tdm_send_uevents(&uevents, &disk_to_dev(md->disk)->kobj);\n\n\tatomic_inc(&md->event_nr);\n\twake_up(&md->eventq);\n\tdm_issue_global_event();\n}\n\n/*\n * Protected by md->suspend_lock obtained by dm_swap_table().\n */\nstatic void __set_size(struct mapped_device *md, sector_t size)\n{\n\tlockdep_assert_held(&md->suspend_lock);\n\n\tset_capacity(md->disk, size);\n\n\ti_size_write(md->bdev->bd_inode, (loff_t)size << SECTOR_SHIFT);\n}\n\n/*\n * Returns old map, which caller must destroy.\n */\nstatic struct dm_table *__bind(struct mapped_device *md, struct dm_table *t,\n\t\t\t       struct queue_limits *limits)\n{\n\tstruct dm_table *old_map;\n\tstruct request_queue *q = md->queue;\n\tsector_t size;\n\n\tlockdep_assert_held(&md->suspend_lock);\n\n\tsize = dm_table_get_size(t);\n\n\t/*\n\t * Wipe any geometry if the size of the table changed.\n\t */\n\tif (size != dm_get_size(md))\n\t\tmemset(&md->geometry, 0, sizeof(md->geometry));\n\n\t__set_size(md, size);\n\n\tdm_table_event_callback(t, event_callback, md);\n\n\t/*\n\t * The queue hasn't been stopped yet, if the old table type wasn't\n\t * for request-based during suspension.  So stop it to prevent\n\t * I/O mapping before resume.\n\t * This must be done before setting the queue restrictions,\n\t * because request-based dm may be run just after the setting.\n\t */\n\tif (dm_table_request_based(t)) {\n\t\tdm_stop_queue(q);\n\t\t/*\n\t\t * Leverage the fact that request-based DM targets are\n\t\t * immutable singletons and establish md->immutable_target\n\t\t * - used to optimize both dm_request_fn and dm_mq_queue_rq\n\t\t */\n\t\tmd->immutable_target = dm_table_get_immutable_target(t);\n\t}\n\n\t__bind_mempools(md, t);\n\n\told_map = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\trcu_assign_pointer(md->map, (void *)t);\n\tmd->immutable_target_type = dm_table_get_immutable_target_type(t);\n\n\tdm_table_set_restrictions(t, q, limits);\n\tif (old_map)\n\t\tdm_sync_table(md);\n\n\treturn old_map;\n}\n\n/*\n * Returns unbound table for the caller to free.\n */\nstatic struct dm_table *__unbind(struct mapped_device *md)\n{\n\tstruct dm_table *map = rcu_dereference_protected(md->map, 1);\n\n\tif (!map)\n\t\treturn NULL;\n\n\tdm_table_event_callback(map, NULL, NULL);\n\tRCU_INIT_POINTER(md->map, NULL);\n\tdm_sync_table(md);\n\n\treturn map;\n}\n\n/*\n * Constructor for a new device.\n */\nint dm_create(int minor, struct mapped_device **result)\n{\n\tstruct mapped_device *md;\n\n\tmd = alloc_dev(minor);\n\tif (!md)\n\t\treturn -ENXIO;\n\n\tdm_sysfs_init(md);\n\n\t*result = md;\n\treturn 0;\n}\n\n/*\n * Functions to manage md->type.\n * All are required to hold md->type_lock.\n */\nvoid dm_lock_md_type(struct mapped_device *md)\n{\n\tmutex_lock(&md->type_lock);\n}\n\nvoid dm_unlock_md_type(struct mapped_device *md)\n{\n\tmutex_unlock(&md->type_lock);\n}\n\nvoid dm_set_md_type(struct mapped_device *md, enum dm_queue_mode type)\n{\n\tBUG_ON(!mutex_is_locked(&md->type_lock));\n\tmd->type = type;\n}\n\nenum dm_queue_mode dm_get_md_type(struct mapped_device *md)\n{\n\treturn md->type;\n}\n\nstruct target_type *dm_get_immutable_target_type(struct mapped_device *md)\n{\n\treturn md->immutable_target_type;\n}\n\n/*\n * The queue_limits are only valid as long as you have a reference\n * count on 'md'.\n */\nstruct queue_limits *dm_get_queue_limits(struct mapped_device *md)\n{\n\tBUG_ON(!atomic_read(&md->holders));\n\treturn &md->queue->limits;\n}\nEXPORT_SYMBOL_GPL(dm_get_queue_limits);\n\n/*\n * Setup the DM device's queue based on md's type\n */\nint dm_setup_md_queue(struct mapped_device *md, struct dm_table *t)\n{\n\tint r;\n\tenum dm_queue_mode type = dm_get_md_type(md);\n\n\tswitch (type) {\n\tcase DM_TYPE_REQUEST_BASED:\n\t\tr = dm_old_init_request_queue(md, t);\n\t\tif (r) {\n\t\t\tDMERR(\"Cannot initialize queue for request-based mapped device\");\n\t\t\treturn r;\n\t\t}\n\t\tbreak;\n\tcase DM_TYPE_MQ_REQUEST_BASED:\n\t\tr = dm_mq_init_request_queue(md, t);\n\t\tif (r) {\n\t\t\tDMERR(\"Cannot initialize queue for request-based dm-mq mapped device\");\n\t\t\treturn r;\n\t\t}\n\t\tbreak;\n\tcase DM_TYPE_BIO_BASED:\n\tcase DM_TYPE_DAX_BIO_BASED:\n\t\tdm_init_normal_md_queue(md);\n\t\tblk_queue_make_request(md->queue, dm_make_request);\n\t\t/*\n\t\t * DM handles splitting bios as needed.  Free the bio_split bioset\n\t\t * since it won't be used (saves 1 process per bio-based DM device).\n\t\t */\n\t\tbioset_free(md->queue->bio_split);\n\t\tmd->queue->bio_split = NULL;\n\n\t\tif (type == DM_TYPE_DAX_BIO_BASED)\n\t\t\tqueue_flag_set_unlocked(QUEUE_FLAG_DAX, md->queue);\n\t\tbreak;\n\tcase DM_TYPE_NONE:\n\t\tWARN_ON_ONCE(true);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstruct mapped_device *dm_get_md(dev_t dev)\n{\n\tstruct mapped_device *md;\n\tunsigned minor = MINOR(dev);\n\n\tif (MAJOR(dev) != _major || minor >= (1 << MINORBITS))\n\t\treturn NULL;\n\n\tspin_lock(&_minor_lock);\n\n\tmd = idr_find(&_minor_idr, minor);\n\tif (md) {\n\t\tif ((md == MINOR_ALLOCED ||\n\t\t     (MINOR(disk_devt(dm_disk(md))) != minor) ||\n\t\t     dm_deleting_md(md) ||\n\t\t     test_bit(DMF_FREEING, &md->flags))) {\n\t\t\tmd = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tdm_get(md);\n\t}\n\nout:\n\tspin_unlock(&_minor_lock);\n\n\treturn md;\n}\nEXPORT_SYMBOL_GPL(dm_get_md);\n\nvoid *dm_get_mdptr(struct mapped_device *md)\n{\n\treturn md->interface_ptr;\n}\n\nvoid dm_set_mdptr(struct mapped_device *md, void *ptr)\n{\n\tmd->interface_ptr = ptr;\n}\n\nvoid dm_get(struct mapped_device *md)\n{\n\tatomic_inc(&md->holders);\n\tBUG_ON(test_bit(DMF_FREEING, &md->flags));\n}\n\nint dm_hold(struct mapped_device *md)\n{\n\tspin_lock(&_minor_lock);\n\tif (test_bit(DMF_FREEING, &md->flags)) {\n\t\tspin_unlock(&_minor_lock);\n\t\treturn -EBUSY;\n\t}\n\tdm_get(md);\n\tspin_unlock(&_minor_lock);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_hold);\n\nconst char *dm_device_name(struct mapped_device *md)\n{\n\treturn md->name;\n}\nEXPORT_SYMBOL_GPL(dm_device_name);\n\nstatic void __dm_destroy(struct mapped_device *md, bool wait)\n{\n\tstruct request_queue *q = dm_get_md_queue(md);\n\tstruct dm_table *map;\n\tint srcu_idx;\n\n\tmight_sleep();\n\n\tspin_lock(&_minor_lock);\n\tidr_replace(&_minor_idr, MINOR_ALLOCED, MINOR(disk_devt(dm_disk(md))));\n\tset_bit(DMF_FREEING, &md->flags);\n\tspin_unlock(&_minor_lock);\n\n\tblk_set_queue_dying(q);\n\n\tif (dm_request_based(md) && md->kworker_task)\n\t\tkthread_flush_worker(&md->kworker);\n\n\t/*\n\t * Take suspend_lock so that presuspend and postsuspend methods\n\t * do not race with internal suspend.\n\t */\n\tmutex_lock(&md->suspend_lock);\n\tmap = dm_get_live_table(md, &srcu_idx);\n\tif (!dm_suspended_md(md)) {\n\t\tdm_table_presuspend_targets(map);\n\t\tdm_table_postsuspend_targets(map);\n\t}\n\t/* dm_put_live_table must be before msleep, otherwise deadlock is possible */\n\tdm_put_live_table(md, srcu_idx);\n\tmutex_unlock(&md->suspend_lock);\n\n\t/*\n\t * Rare, but there may be I/O requests still going to complete,\n\t * for example.  Wait for all references to disappear.\n\t * No one should increment the reference count of the mapped_device,\n\t * after the mapped_device state becomes DMF_FREEING.\n\t */\n\tif (wait)\n\t\twhile (atomic_read(&md->holders))\n\t\t\tmsleep(1);\n\telse if (atomic_read(&md->holders))\n\t\tDMWARN(\"%s: Forcibly removing mapped_device still in use! (%d users)\",\n\t\t       dm_device_name(md), atomic_read(&md->holders));\n\n\tdm_sysfs_exit(md);\n\tdm_table_destroy(__unbind(md));\n\tfree_dev(md);\n}\n\nvoid dm_destroy(struct mapped_device *md)\n{\n\t__dm_destroy(md, true);\n}\n\nvoid dm_destroy_immediate(struct mapped_device *md)\n{\n\t__dm_destroy(md, false);\n}\n\nvoid dm_put(struct mapped_device *md)\n{\n\tatomic_dec(&md->holders);\n}\nEXPORT_SYMBOL_GPL(dm_put);\n\nstatic int dm_wait_for_completion(struct mapped_device *md, long task_state)\n{\n\tint r = 0;\n\tDEFINE_WAIT(wait);\n\n\twhile (1) {\n\t\tprepare_to_wait(&md->wait, &wait, task_state);\n\n\t\tif (!md_in_flight(md))\n\t\t\tbreak;\n\n\t\tif (signal_pending_state(task_state, current)) {\n\t\t\tr = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tio_schedule();\n\t}\n\tfinish_wait(&md->wait, &wait);\n\n\treturn r;\n}\n\n/*\n * Process the deferred bios\n */\nstatic void dm_wq_work(struct work_struct *work)\n{\n\tstruct mapped_device *md = container_of(work, struct mapped_device,\n\t\t\t\t\t\twork);\n\tstruct bio *c;\n\tint srcu_idx;\n\tstruct dm_table *map;\n\n\tmap = dm_get_live_table(md, &srcu_idx);\n\n\twhile (!test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags)) {\n\t\tspin_lock_irq(&md->deferred_lock);\n\t\tc = bio_list_pop(&md->deferred);\n\t\tspin_unlock_irq(&md->deferred_lock);\n\n\t\tif (!c)\n\t\t\tbreak;\n\n\t\tif (dm_request_based(md))\n\t\t\tgeneric_make_request(c);\n\t\telse\n\t\t\t__split_and_process_bio(md, map, c);\n\t}\n\n\tdm_put_live_table(md, srcu_idx);\n}\n\nstatic void dm_queue_flush(struct mapped_device *md)\n{\n\tclear_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);\n\tsmp_mb__after_atomic();\n\tqueue_work(md->wq, &md->work);\n}\n\n/*\n * Swap in a new table, returning the old one for the caller to destroy.\n */\nstruct dm_table *dm_swap_table(struct mapped_device *md, struct dm_table *table)\n{\n\tstruct dm_table *live_map = NULL, *map = ERR_PTR(-EINVAL);\n\tstruct queue_limits limits;\n\tint r;\n\n\tmutex_lock(&md->suspend_lock);\n\n\t/* device must be suspended */\n\tif (!dm_suspended_md(md))\n\t\tgoto out;\n\n\t/*\n\t * If the new table has no data devices, retain the existing limits.\n\t * This helps multipath with queue_if_no_path if all paths disappear,\n\t * then new I/O is queued based on these limits, and then some paths\n\t * reappear.\n\t */\n\tif (dm_table_has_no_data_devices(table)) {\n\t\tlive_map = dm_get_live_table_fast(md);\n\t\tif (live_map)\n\t\t\tlimits = md->queue->limits;\n\t\tdm_put_live_table_fast(md);\n\t}\n\n\tif (!live_map) {\n\t\tr = dm_calculate_queue_limits(table, &limits);\n\t\tif (r) {\n\t\t\tmap = ERR_PTR(r);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmap = __bind(md, table, &limits);\n\tdm_issue_global_event();\n\nout:\n\tmutex_unlock(&md->suspend_lock);\n\treturn map;\n}\n\n/*\n * Functions to lock and unlock any filesystem running on the\n * device.\n */\nstatic int lock_fs(struct mapped_device *md)\n{\n\tint r;\n\n\tWARN_ON(md->frozen_sb);\n\n\tmd->frozen_sb = freeze_bdev(md->bdev);\n\tif (IS_ERR(md->frozen_sb)) {\n\t\tr = PTR_ERR(md->frozen_sb);\n\t\tmd->frozen_sb = NULL;\n\t\treturn r;\n\t}\n\n\tset_bit(DMF_FROZEN, &md->flags);\n\n\treturn 0;\n}\n\nstatic void unlock_fs(struct mapped_device *md)\n{\n\tif (!test_bit(DMF_FROZEN, &md->flags))\n\t\treturn;\n\n\tthaw_bdev(md->bdev, md->frozen_sb);\n\tmd->frozen_sb = NULL;\n\tclear_bit(DMF_FROZEN, &md->flags);\n}\n\n/*\n * @suspend_flags: DM_SUSPEND_LOCKFS_FLAG and/or DM_SUSPEND_NOFLUSH_FLAG\n * @task_state: e.g. TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE\n * @dmf_suspended_flag: DMF_SUSPENDED or DMF_SUSPENDED_INTERNALLY\n *\n * If __dm_suspend returns 0, the device is completely quiescent\n * now. There is no request-processing activity. All new requests\n * are being added to md->deferred list.\n */\nstatic int __dm_suspend(struct mapped_device *md, struct dm_table *map,\n\t\t\tunsigned suspend_flags, long task_state,\n\t\t\tint dmf_suspended_flag)\n{\n\tbool do_lockfs = suspend_flags & DM_SUSPEND_LOCKFS_FLAG;\n\tbool noflush = suspend_flags & DM_SUSPEND_NOFLUSH_FLAG;\n\tint r;\n\n\tlockdep_assert_held(&md->suspend_lock);\n\n\t/*\n\t * DMF_NOFLUSH_SUSPENDING must be set before presuspend.\n\t * This flag is cleared before dm_suspend returns.\n\t */\n\tif (noflush)\n\t\tset_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);\n\telse\n\t\tpr_debug(\"%s: suspending with flush\\n\", dm_device_name(md));\n\n\t/*\n\t * This gets reverted if there's an error later and the targets\n\t * provide the .presuspend_undo hook.\n\t */\n\tdm_table_presuspend_targets(map);\n\n\t/*\n\t * Flush I/O to the device.\n\t * Any I/O submitted after lock_fs() may not be flushed.\n\t * noflush takes precedence over do_lockfs.\n\t * (lock_fs() flushes I/Os and waits for them to complete.)\n\t */\n\tif (!noflush && do_lockfs) {\n\t\tr = lock_fs(md);\n\t\tif (r) {\n\t\t\tdm_table_presuspend_undo_targets(map);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\t/*\n\t * Here we must make sure that no processes are submitting requests\n\t * to target drivers i.e. no one may be executing\n\t * __split_and_process_bio. This is called from dm_request and\n\t * dm_wq_work.\n\t *\n\t * To get all processes out of __split_and_process_bio in dm_request,\n\t * we take the write lock. To prevent any process from reentering\n\t * __split_and_process_bio from dm_request and quiesce the thread\n\t * (dm_wq_work), we set BMF_BLOCK_IO_FOR_SUSPEND and call\n\t * flush_workqueue(md->wq).\n\t */\n\tset_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);\n\tif (map)\n\t\tsynchronize_srcu(&md->io_barrier);\n\n\t/*\n\t * Stop md->queue before flushing md->wq in case request-based\n\t * dm defers requests to md->wq from md->queue.\n\t */\n\tif (dm_request_based(md)) {\n\t\tdm_stop_queue(md->queue);\n\t\tif (md->kworker_task)\n\t\t\tkthread_flush_worker(&md->kworker);\n\t}\n\n\tflush_workqueue(md->wq);\n\n\t/*\n\t * At this point no more requests are entering target request routines.\n\t * We call dm_wait_for_completion to wait for all existing requests\n\t * to finish.\n\t */\n\tr = dm_wait_for_completion(md, task_state);\n\tif (!r)\n\t\tset_bit(dmf_suspended_flag, &md->flags);\n\n\tif (noflush)\n\t\tclear_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);\n\tif (map)\n\t\tsynchronize_srcu(&md->io_barrier);\n\n\t/* were we interrupted ? */\n\tif (r < 0) {\n\t\tdm_queue_flush(md);\n\n\t\tif (dm_request_based(md))\n\t\t\tdm_start_queue(md->queue);\n\n\t\tunlock_fs(md);\n\t\tdm_table_presuspend_undo_targets(map);\n\t\t/* pushback list is already flushed, so skip flush */\n\t}\n\n\treturn r;\n}\n\n/*\n * We need to be able to change a mapping table under a mounted\n * filesystem.  For example we might want to move some data in\n * the background.  Before the table can be swapped with\n * dm_bind_table, dm_suspend must be called to flush any in\n * flight bios and ensure that any further io gets deferred.\n */\n/*\n * Suspend mechanism in request-based dm.\n *\n * 1. Flush all I/Os by lock_fs() if needed.\n * 2. Stop dispatching any I/O by stopping the request_queue.\n * 3. Wait for all in-flight I/Os to be completed or requeued.\n *\n * To abort suspend, start the request_queue.\n */\nint dm_suspend(struct mapped_device *md, unsigned suspend_flags)\n{\n\tstruct dm_table *map = NULL;\n\tint r = 0;\n\nretry:\n\tmutex_lock_nested(&md->suspend_lock, SINGLE_DEPTH_NESTING);\n\n\tif (dm_suspended_md(md)) {\n\t\tr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (dm_suspended_internally_md(md)) {\n\t\t/* already internally suspended, wait for internal resume */\n\t\tmutex_unlock(&md->suspend_lock);\n\t\tr = wait_on_bit(&md->flags, DMF_SUSPENDED_INTERNALLY, TASK_INTERRUPTIBLE);\n\t\tif (r)\n\t\t\treturn r;\n\t\tgoto retry;\n\t}\n\n\tmap = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\n\tr = __dm_suspend(md, map, suspend_flags, TASK_INTERRUPTIBLE, DMF_SUSPENDED);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tdm_table_postsuspend_targets(map);\n\nout_unlock:\n\tmutex_unlock(&md->suspend_lock);\n\treturn r;\n}\n\nstatic int __dm_resume(struct mapped_device *md, struct dm_table *map)\n{\n\tif (map) {\n\t\tint r = dm_table_resume_targets(map);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tdm_queue_flush(md);\n\n\t/*\n\t * Flushing deferred I/Os must be done after targets are resumed\n\t * so that mapping of targets can work correctly.\n\t * Request-based dm is queueing the deferred I/Os in its request_queue.\n\t */\n\tif (dm_request_based(md))\n\t\tdm_start_queue(md->queue);\n\n\tunlock_fs(md);\n\n\treturn 0;\n}\n\nint dm_resume(struct mapped_device *md)\n{\n\tint r;\n\tstruct dm_table *map = NULL;\n\nretry:\n\tr = -EINVAL;\n\tmutex_lock_nested(&md->suspend_lock, SINGLE_DEPTH_NESTING);\n\n\tif (!dm_suspended_md(md))\n\t\tgoto out;\n\n\tif (dm_suspended_internally_md(md)) {\n\t\t/* already internally suspended, wait for internal resume */\n\t\tmutex_unlock(&md->suspend_lock);\n\t\tr = wait_on_bit(&md->flags, DMF_SUSPENDED_INTERNALLY, TASK_INTERRUPTIBLE);\n\t\tif (r)\n\t\t\treturn r;\n\t\tgoto retry;\n\t}\n\n\tmap = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\tif (!map || !dm_table_get_size(map))\n\t\tgoto out;\n\n\tr = __dm_resume(md, map);\n\tif (r)\n\t\tgoto out;\n\n\tclear_bit(DMF_SUSPENDED, &md->flags);\nout:\n\tmutex_unlock(&md->suspend_lock);\n\n\treturn r;\n}\n\n/*\n * Internal suspend/resume works like userspace-driven suspend. It waits\n * until all bios finish and prevents issuing new bios to the target drivers.\n * It may be used only from the kernel.\n */\n\nstatic void __dm_internal_suspend(struct mapped_device *md, unsigned suspend_flags)\n{\n\tstruct dm_table *map = NULL;\n\n\tlockdep_assert_held(&md->suspend_lock);\n\n\tif (md->internal_suspend_count++)\n\t\treturn; /* nested internal suspend */\n\n\tif (dm_suspended_md(md)) {\n\t\tset_bit(DMF_SUSPENDED_INTERNALLY, &md->flags);\n\t\treturn; /* nest suspend */\n\t}\n\n\tmap = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\n\t/*\n\t * Using TASK_UNINTERRUPTIBLE because only NOFLUSH internal suspend is\n\t * supported.  Properly supporting a TASK_INTERRUPTIBLE internal suspend\n\t * would require changing .presuspend to return an error -- avoid this\n\t * until there is a need for more elaborate variants of internal suspend.\n\t */\n\t(void) __dm_suspend(md, map, suspend_flags, TASK_UNINTERRUPTIBLE,\n\t\t\t    DMF_SUSPENDED_INTERNALLY);\n\n\tdm_table_postsuspend_targets(map);\n}\n\nstatic void __dm_internal_resume(struct mapped_device *md)\n{\n\tBUG_ON(!md->internal_suspend_count);\n\n\tif (--md->internal_suspend_count)\n\t\treturn; /* resume from nested internal suspend */\n\n\tif (dm_suspended_md(md))\n\t\tgoto done; /* resume from nested suspend */\n\n\t/*\n\t * NOTE: existing callers don't need to call dm_table_resume_targets\n\t * (which may fail -- so best to avoid it for now by passing NULL map)\n\t */\n\t(void) __dm_resume(md, NULL);\n\ndone:\n\tclear_bit(DMF_SUSPENDED_INTERNALLY, &md->flags);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&md->flags, DMF_SUSPENDED_INTERNALLY);\n}\n\nvoid dm_internal_suspend_noflush(struct mapped_device *md)\n{\n\tmutex_lock(&md->suspend_lock);\n\t__dm_internal_suspend(md, DM_SUSPEND_NOFLUSH_FLAG);\n\tmutex_unlock(&md->suspend_lock);\n}\nEXPORT_SYMBOL_GPL(dm_internal_suspend_noflush);\n\nvoid dm_internal_resume(struct mapped_device *md)\n{\n\tmutex_lock(&md->suspend_lock);\n\t__dm_internal_resume(md);\n\tmutex_unlock(&md->suspend_lock);\n}\nEXPORT_SYMBOL_GPL(dm_internal_resume);\n\n/*\n * Fast variants of internal suspend/resume hold md->suspend_lock,\n * which prevents interaction with userspace-driven suspend.\n */\n\nvoid dm_internal_suspend_fast(struct mapped_device *md)\n{\n\tmutex_lock(&md->suspend_lock);\n\tif (dm_suspended_md(md) || dm_suspended_internally_md(md))\n\t\treturn;\n\n\tset_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);\n\tsynchronize_srcu(&md->io_barrier);\n\tflush_workqueue(md->wq);\n\tdm_wait_for_completion(md, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL_GPL(dm_internal_suspend_fast);\n\nvoid dm_internal_resume_fast(struct mapped_device *md)\n{\n\tif (dm_suspended_md(md) || dm_suspended_internally_md(md))\n\t\tgoto done;\n\n\tdm_queue_flush(md);\n\ndone:\n\tmutex_unlock(&md->suspend_lock);\n}\nEXPORT_SYMBOL_GPL(dm_internal_resume_fast);\n\n/*-----------------------------------------------------------------\n * Event notification.\n *---------------------------------------------------------------*/\nint dm_kobject_uevent(struct mapped_device *md, enum kobject_action action,\n\t\t       unsigned cookie)\n{\n\tchar udev_cookie[DM_COOKIE_LENGTH];\n\tchar *envp[] = { udev_cookie, NULL };\n\n\tif (!cookie)\n\t\treturn kobject_uevent(&disk_to_dev(md->disk)->kobj, action);\n\telse {\n\t\tsnprintf(udev_cookie, DM_COOKIE_LENGTH, \"%s=%u\",\n\t\t\t DM_COOKIE_ENV_VAR_NAME, cookie);\n\t\treturn kobject_uevent_env(&disk_to_dev(md->disk)->kobj,\n\t\t\t\t\t  action, envp);\n\t}\n}\n\nuint32_t dm_next_uevent_seq(struct mapped_device *md)\n{\n\treturn atomic_add_return(1, &md->uevent_seq);\n}\n\nuint32_t dm_get_event_nr(struct mapped_device *md)\n{\n\treturn atomic_read(&md->event_nr);\n}\n\nint dm_wait_event(struct mapped_device *md, int event_nr)\n{\n\treturn wait_event_interruptible(md->eventq,\n\t\t\t(event_nr != atomic_read(&md->event_nr)));\n}\n\nvoid dm_uevent_add(struct mapped_device *md, struct list_head *elist)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md->uevent_lock, flags);\n\tlist_add(elist, &md->uevent_list);\n\tspin_unlock_irqrestore(&md->uevent_lock, flags);\n}\n\n/*\n * The gendisk is only valid as long as you have a reference\n * count on 'md'.\n */\nstruct gendisk *dm_disk(struct mapped_device *md)\n{\n\treturn md->disk;\n}\nEXPORT_SYMBOL_GPL(dm_disk);\n\nstruct kobject *dm_kobject(struct mapped_device *md)\n{\n\treturn &md->kobj_holder.kobj;\n}\n\nstruct mapped_device *dm_get_from_kobject(struct kobject *kobj)\n{\n\tstruct mapped_device *md;\n\n\tmd = container_of(kobj, struct mapped_device, kobj_holder.kobj);\n\n\tif (test_bit(DMF_FREEING, &md->flags) ||\n\t    dm_deleting_md(md))\n\t\treturn NULL;\n\n\tdm_get(md);\n\treturn md;\n}\n\nint dm_suspended_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_SUSPENDED, &md->flags);\n}\n\nint dm_suspended_internally_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_SUSPENDED_INTERNALLY, &md->flags);\n}\n\nint dm_test_deferred_remove_flag(struct mapped_device *md)\n{\n\treturn test_bit(DMF_DEFERRED_REMOVE, &md->flags);\n}\n\nint dm_suspended(struct dm_target *ti)\n{\n\treturn dm_suspended_md(dm_table_get_md(ti->table));\n}\nEXPORT_SYMBOL_GPL(dm_suspended);\n\nint dm_noflush_suspending(struct dm_target *ti)\n{\n\treturn __noflush_suspending(dm_table_get_md(ti->table));\n}\nEXPORT_SYMBOL_GPL(dm_noflush_suspending);\n\nstruct dm_md_mempools *dm_alloc_md_mempools(struct mapped_device *md, enum dm_queue_mode type,\n\t\t\t\t\t    unsigned integrity, unsigned per_io_data_size)\n{\n\tstruct dm_md_mempools *pools = kzalloc_node(sizeof(*pools), GFP_KERNEL, md->numa_node_id);\n\tunsigned int pool_size = 0;\n\tunsigned int front_pad;\n\n\tif (!pools)\n\t\treturn NULL;\n\n\tswitch (type) {\n\tcase DM_TYPE_BIO_BASED:\n\tcase DM_TYPE_DAX_BIO_BASED:\n\t\tpool_size = dm_get_reserved_bio_based_ios();\n\t\tfront_pad = roundup(per_io_data_size, __alignof__(struct dm_target_io)) + offsetof(struct dm_target_io, clone);\n\t\n\t\tpools->io_pool = mempool_create_slab_pool(pool_size, _io_cache);\n\t\tif (!pools->io_pool)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase DM_TYPE_REQUEST_BASED:\n\tcase DM_TYPE_MQ_REQUEST_BASED:\n\t\tpool_size = dm_get_reserved_rq_based_ios();\n\t\tfront_pad = offsetof(struct dm_rq_clone_bio_info, clone);\n\t\t/* per_io_data_size is used for blk-mq pdu at queue allocation */\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tpools->bs = bioset_create(pool_size, front_pad, BIOSET_NEED_RESCUER);\n\tif (!pools->bs)\n\t\tgoto out;\n\n\tif (integrity && bioset_integrity_create(pools->bs, pool_size))\n\t\tgoto out;\n\n\treturn pools;\n\nout:\n\tdm_free_md_mempools(pools);\n\n\treturn NULL;\n}\n\nvoid dm_free_md_mempools(struct dm_md_mempools *pools)\n{\n\tif (!pools)\n\t\treturn;\n\n\tmempool_destroy(pools->io_pool);\n\n\tif (pools->bs)\n\t\tbioset_free(pools->bs);\n\n\tkfree(pools);\n}\n\nstruct dm_pr {\n\tu64\told_key;\n\tu64\tnew_key;\n\tu32\tflags;\n\tbool\tfail_early;\n};\n\nstatic int dm_call_pr(struct block_device *bdev, iterate_devices_callout_fn fn,\n\t\t      void *data)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tstruct dm_table *table;\n\tstruct dm_target *ti;\n\tint ret = -ENOTTY, srcu_idx;\n\n\ttable = dm_get_live_table(md, &srcu_idx);\n\tif (!table || !dm_table_get_size(table))\n\t\tgoto out;\n\n\t/* We only support devices that have a single target */\n\tif (dm_table_get_num_targets(table) != 1)\n\t\tgoto out;\n\tti = dm_table_get_target(table, 0);\n\n\tret = -EINVAL;\n\tif (!ti->type->iterate_devices)\n\t\tgoto out;\n\n\tret = ti->type->iterate_devices(ti, fn, data);\nout:\n\tdm_put_live_table(md, srcu_idx);\n\treturn ret;\n}\n\n/*\n * For register / unregister we need to manually call out to every path.\n */\nstatic int __dm_pr_register(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t    sector_t start, sector_t len, void *data)\n{\n\tstruct dm_pr *pr = data;\n\tconst struct pr_ops *ops = dev->bdev->bd_disk->fops->pr_ops;\n\n\tif (!ops || !ops->pr_register)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->pr_register(dev->bdev, pr->old_key, pr->new_key, pr->flags);\n}\n\nstatic int dm_pr_register(struct block_device *bdev, u64 old_key, u64 new_key,\n\t\t\t  u32 flags)\n{\n\tstruct dm_pr pr = {\n\t\t.old_key\t= old_key,\n\t\t.new_key\t= new_key,\n\t\t.flags\t\t= flags,\n\t\t.fail_early\t= true,\n\t};\n\tint ret;\n\n\tret = dm_call_pr(bdev, __dm_pr_register, &pr);\n\tif (ret && new_key) {\n\t\t/* unregister all paths if we failed to register any path */\n\t\tpr.old_key = new_key;\n\t\tpr.new_key = 0;\n\t\tpr.flags = 0;\n\t\tpr.fail_early = false;\n\t\tdm_call_pr(bdev, __dm_pr_register, &pr);\n\t}\n\n\treturn ret;\n}\n\nstatic int dm_pr_reserve(struct block_device *bdev, u64 key, enum pr_type type,\n\t\t\t u32 flags)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tconst struct pr_ops *ops;\n\tfmode_t mode;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tops = bdev->bd_disk->fops->pr_ops;\n\tif (ops && ops->pr_reserve)\n\t\tr = ops->pr_reserve(bdev, key, type, flags);\n\telse\n\t\tr = -EOPNOTSUPP;\n\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic int dm_pr_release(struct block_device *bdev, u64 key, enum pr_type type)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tconst struct pr_ops *ops;\n\tfmode_t mode;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tops = bdev->bd_disk->fops->pr_ops;\n\tif (ops && ops->pr_release)\n\t\tr = ops->pr_release(bdev, key, type);\n\telse\n\t\tr = -EOPNOTSUPP;\n\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic int dm_pr_preempt(struct block_device *bdev, u64 old_key, u64 new_key,\n\t\t\t enum pr_type type, bool abort)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tconst struct pr_ops *ops;\n\tfmode_t mode;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tops = bdev->bd_disk->fops->pr_ops;\n\tif (ops && ops->pr_preempt)\n\t\tr = ops->pr_preempt(bdev, old_key, new_key, type, abort);\n\telse\n\t\tr = -EOPNOTSUPP;\n\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic int dm_pr_clear(struct block_device *bdev, u64 key)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tconst struct pr_ops *ops;\n\tfmode_t mode;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tops = bdev->bd_disk->fops->pr_ops;\n\tif (ops && ops->pr_clear)\n\t\tr = ops->pr_clear(bdev, key);\n\telse\n\t\tr = -EOPNOTSUPP;\n\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic const struct pr_ops dm_pr_ops = {\n\t.pr_register\t= dm_pr_register,\n\t.pr_reserve\t= dm_pr_reserve,\n\t.pr_release\t= dm_pr_release,\n\t.pr_preempt\t= dm_pr_preempt,\n\t.pr_clear\t= dm_pr_clear,\n};\n\nstatic const struct block_device_operations dm_blk_dops = {\n\t.open = dm_blk_open,\n\t.release = dm_blk_close,\n\t.ioctl = dm_blk_ioctl,\n\t.getgeo = dm_blk_getgeo,\n\t.pr_ops = &dm_pr_ops,\n\t.owner = THIS_MODULE\n};\n\nstatic const struct dax_operations dm_dax_ops = {\n\t.direct_access = dm_dax_direct_access,\n\t.copy_from_iter = dm_dax_copy_from_iter,\n};\n\n/*\n * module hooks\n */\nmodule_init(dm_init);\nmodule_exit(dm_exit);\n\nmodule_param(major, uint, 0);\nMODULE_PARM_DESC(major, \"The major number of the device mapper\");\n\nmodule_param(reserved_bio_based_ios, uint, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(reserved_bio_based_ios, \"Reserved IOs in bio-based mempools\");\n\nmodule_param(dm_numa_node, int, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(dm_numa_node, \"NUMA node for DM device memory allocations\");\n\nMODULE_DESCRIPTION(DM_NAME \" driver\");\nMODULE_AUTHOR(\"Joe Thornber <dm-devel@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n"], "fixing_code": ["/*\n * Copyright (C) 2001, 2002 Sistina Software (UK) Limited.\n * Copyright (C) 2004-2008 Red Hat, Inc. All rights reserved.\n *\n * This file is released under the GPL.\n */\n\n#include \"dm-core.h\"\n#include \"dm-rq.h\"\n#include \"dm-uevent.h\"\n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/sched/signal.h>\n#include <linux/blkpg.h>\n#include <linux/bio.h>\n#include <linux/mempool.h>\n#include <linux/dax.h>\n#include <linux/slab.h>\n#include <linux/idr.h>\n#include <linux/uio.h>\n#include <linux/hdreg.h>\n#include <linux/delay.h>\n#include <linux/wait.h>\n#include <linux/pr.h>\n#include <linux/refcount.h>\n\n#define DM_MSG_PREFIX \"core\"\n\n/*\n * Cookies are numeric values sent with CHANGE and REMOVE\n * uevents while resuming, removing or renaming the device.\n */\n#define DM_COOKIE_ENV_VAR_NAME \"DM_COOKIE\"\n#define DM_COOKIE_LENGTH 24\n\nstatic const char *_name = DM_NAME;\n\nstatic unsigned int major = 0;\nstatic unsigned int _major = 0;\n\nstatic DEFINE_IDR(_minor_idr);\n\nstatic DEFINE_SPINLOCK(_minor_lock);\n\nstatic void do_deferred_remove(struct work_struct *w);\n\nstatic DECLARE_WORK(deferred_remove_work, do_deferred_remove);\n\nstatic struct workqueue_struct *deferred_remove_workqueue;\n\natomic_t dm_global_event_nr = ATOMIC_INIT(0);\nDECLARE_WAIT_QUEUE_HEAD(dm_global_eventq);\n\nvoid dm_issue_global_event(void)\n{\n\tatomic_inc(&dm_global_event_nr);\n\twake_up(&dm_global_eventq);\n}\n\n/*\n * One of these is allocated per bio.\n */\nstruct dm_io {\n\tstruct mapped_device *md;\n\tblk_status_t status;\n\tatomic_t io_count;\n\tstruct bio *bio;\n\tunsigned long start_time;\n\tspinlock_t endio_lock;\n\tstruct dm_stats_aux stats_aux;\n};\n\n#define MINOR_ALLOCED ((void *)-1)\n\n/*\n * Bits for the md->flags field.\n */\n#define DMF_BLOCK_IO_FOR_SUSPEND 0\n#define DMF_SUSPENDED 1\n#define DMF_FROZEN 2\n#define DMF_FREEING 3\n#define DMF_DELETING 4\n#define DMF_NOFLUSH_SUSPENDING 5\n#define DMF_DEFERRED_REMOVE 6\n#define DMF_SUSPENDED_INTERNALLY 7\n\n#define DM_NUMA_NODE NUMA_NO_NODE\nstatic int dm_numa_node = DM_NUMA_NODE;\n\n/*\n * For mempools pre-allocation at the table loading time.\n */\nstruct dm_md_mempools {\n\tmempool_t *io_pool;\n\tstruct bio_set *bs;\n};\n\nstruct table_device {\n\tstruct list_head list;\n\trefcount_t count;\n\tstruct dm_dev dm_dev;\n};\n\nstatic struct kmem_cache *_io_cache;\nstatic struct kmem_cache *_rq_tio_cache;\nstatic struct kmem_cache *_rq_cache;\n\n/*\n * Bio-based DM's mempools' reserved IOs set by the user.\n */\n#define RESERVED_BIO_BASED_IOS\t\t16\nstatic unsigned reserved_bio_based_ios = RESERVED_BIO_BASED_IOS;\n\nstatic int __dm_get_module_param_int(int *module_param, int min, int max)\n{\n\tint param = ACCESS_ONCE(*module_param);\n\tint modified_param = 0;\n\tbool modified = true;\n\n\tif (param < min)\n\t\tmodified_param = min;\n\telse if (param > max)\n\t\tmodified_param = max;\n\telse\n\t\tmodified = false;\n\n\tif (modified) {\n\t\t(void)cmpxchg(module_param, param, modified_param);\n\t\tparam = modified_param;\n\t}\n\n\treturn param;\n}\n\nunsigned __dm_get_module_param(unsigned *module_param,\n\t\t\t       unsigned def, unsigned max)\n{\n\tunsigned param = ACCESS_ONCE(*module_param);\n\tunsigned modified_param = 0;\n\n\tif (!param)\n\t\tmodified_param = def;\n\telse if (param > max)\n\t\tmodified_param = max;\n\n\tif (modified_param) {\n\t\t(void)cmpxchg(module_param, param, modified_param);\n\t\tparam = modified_param;\n\t}\n\n\treturn param;\n}\n\nunsigned dm_get_reserved_bio_based_ios(void)\n{\n\treturn __dm_get_module_param(&reserved_bio_based_ios,\n\t\t\t\t     RESERVED_BIO_BASED_IOS, DM_RESERVED_MAX_IOS);\n}\nEXPORT_SYMBOL_GPL(dm_get_reserved_bio_based_ios);\n\nstatic unsigned dm_get_numa_node(void)\n{\n\treturn __dm_get_module_param_int(&dm_numa_node,\n\t\t\t\t\t DM_NUMA_NODE, num_online_nodes() - 1);\n}\n\nstatic int __init local_init(void)\n{\n\tint r = -ENOMEM;\n\n\t/* allocate a slab for the dm_ios */\n\t_io_cache = KMEM_CACHE(dm_io, 0);\n\tif (!_io_cache)\n\t\treturn r;\n\n\t_rq_tio_cache = KMEM_CACHE(dm_rq_target_io, 0);\n\tif (!_rq_tio_cache)\n\t\tgoto out_free_io_cache;\n\n\t_rq_cache = kmem_cache_create(\"dm_old_clone_request\", sizeof(struct request),\n\t\t\t\t      __alignof__(struct request), 0, NULL);\n\tif (!_rq_cache)\n\t\tgoto out_free_rq_tio_cache;\n\n\tr = dm_uevent_init();\n\tif (r)\n\t\tgoto out_free_rq_cache;\n\n\tdeferred_remove_workqueue = alloc_workqueue(\"kdmremove\", WQ_UNBOUND, 1);\n\tif (!deferred_remove_workqueue) {\n\t\tr = -ENOMEM;\n\t\tgoto out_uevent_exit;\n\t}\n\n\t_major = major;\n\tr = register_blkdev(_major, _name);\n\tif (r < 0)\n\t\tgoto out_free_workqueue;\n\n\tif (!_major)\n\t\t_major = r;\n\n\treturn 0;\n\nout_free_workqueue:\n\tdestroy_workqueue(deferred_remove_workqueue);\nout_uevent_exit:\n\tdm_uevent_exit();\nout_free_rq_cache:\n\tkmem_cache_destroy(_rq_cache);\nout_free_rq_tio_cache:\n\tkmem_cache_destroy(_rq_tio_cache);\nout_free_io_cache:\n\tkmem_cache_destroy(_io_cache);\n\n\treturn r;\n}\n\nstatic void local_exit(void)\n{\n\tflush_scheduled_work();\n\tdestroy_workqueue(deferred_remove_workqueue);\n\n\tkmem_cache_destroy(_rq_cache);\n\tkmem_cache_destroy(_rq_tio_cache);\n\tkmem_cache_destroy(_io_cache);\n\tunregister_blkdev(_major, _name);\n\tdm_uevent_exit();\n\n\t_major = 0;\n\n\tDMINFO(\"cleaned up\");\n}\n\nstatic int (*_inits[])(void) __initdata = {\n\tlocal_init,\n\tdm_target_init,\n\tdm_linear_init,\n\tdm_stripe_init,\n\tdm_io_init,\n\tdm_kcopyd_init,\n\tdm_interface_init,\n\tdm_statistics_init,\n};\n\nstatic void (*_exits[])(void) = {\n\tlocal_exit,\n\tdm_target_exit,\n\tdm_linear_exit,\n\tdm_stripe_exit,\n\tdm_io_exit,\n\tdm_kcopyd_exit,\n\tdm_interface_exit,\n\tdm_statistics_exit,\n};\n\nstatic int __init dm_init(void)\n{\n\tconst int count = ARRAY_SIZE(_inits);\n\n\tint r, i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tr = _inits[i]();\n\t\tif (r)\n\t\t\tgoto bad;\n\t}\n\n\treturn 0;\n\n      bad:\n\twhile (i--)\n\t\t_exits[i]();\n\n\treturn r;\n}\n\nstatic void __exit dm_exit(void)\n{\n\tint i = ARRAY_SIZE(_exits);\n\n\twhile (i--)\n\t\t_exits[i]();\n\n\t/*\n\t * Should be empty by this point.\n\t */\n\tidr_destroy(&_minor_idr);\n}\n\n/*\n * Block device functions\n */\nint dm_deleting_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_DELETING, &md->flags);\n}\n\nstatic int dm_blk_open(struct block_device *bdev, fmode_t mode)\n{\n\tstruct mapped_device *md;\n\n\tspin_lock(&_minor_lock);\n\n\tmd = bdev->bd_disk->private_data;\n\tif (!md)\n\t\tgoto out;\n\n\tif (test_bit(DMF_FREEING, &md->flags) ||\n\t    dm_deleting_md(md)) {\n\t\tmd = NULL;\n\t\tgoto out;\n\t}\n\n\tdm_get(md);\n\tatomic_inc(&md->open_count);\nout:\n\tspin_unlock(&_minor_lock);\n\n\treturn md ? 0 : -ENXIO;\n}\n\nstatic void dm_blk_close(struct gendisk *disk, fmode_t mode)\n{\n\tstruct mapped_device *md;\n\n\tspin_lock(&_minor_lock);\n\n\tmd = disk->private_data;\n\tif (WARN_ON(!md))\n\t\tgoto out;\n\n\tif (atomic_dec_and_test(&md->open_count) &&\n\t    (test_bit(DMF_DEFERRED_REMOVE, &md->flags)))\n\t\tqueue_work(deferred_remove_workqueue, &deferred_remove_work);\n\n\tdm_put(md);\nout:\n\tspin_unlock(&_minor_lock);\n}\n\nint dm_open_count(struct mapped_device *md)\n{\n\treturn atomic_read(&md->open_count);\n}\n\n/*\n * Guarantees nothing is using the device before it's deleted.\n */\nint dm_lock_for_deletion(struct mapped_device *md, bool mark_deferred, bool only_deferred)\n{\n\tint r = 0;\n\n\tspin_lock(&_minor_lock);\n\n\tif (dm_open_count(md)) {\n\t\tr = -EBUSY;\n\t\tif (mark_deferred)\n\t\t\tset_bit(DMF_DEFERRED_REMOVE, &md->flags);\n\t} else if (only_deferred && !test_bit(DMF_DEFERRED_REMOVE, &md->flags))\n\t\tr = -EEXIST;\n\telse\n\t\tset_bit(DMF_DELETING, &md->flags);\n\n\tspin_unlock(&_minor_lock);\n\n\treturn r;\n}\n\nint dm_cancel_deferred_remove(struct mapped_device *md)\n{\n\tint r = 0;\n\n\tspin_lock(&_minor_lock);\n\n\tif (test_bit(DMF_DELETING, &md->flags))\n\t\tr = -EBUSY;\n\telse\n\t\tclear_bit(DMF_DEFERRED_REMOVE, &md->flags);\n\n\tspin_unlock(&_minor_lock);\n\n\treturn r;\n}\n\nstatic void do_deferred_remove(struct work_struct *w)\n{\n\tdm_deferred_remove();\n}\n\nsector_t dm_get_size(struct mapped_device *md)\n{\n\treturn get_capacity(md->disk);\n}\n\nstruct request_queue *dm_get_md_queue(struct mapped_device *md)\n{\n\treturn md->queue;\n}\n\nstruct dm_stats *dm_get_stats(struct mapped_device *md)\n{\n\treturn &md->stats;\n}\n\nstatic int dm_blk_getgeo(struct block_device *bdev, struct hd_geometry *geo)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\n\treturn dm_get_geometry(md, geo);\n}\n\nstatic int dm_grab_bdev_for_ioctl(struct mapped_device *md,\n\t\t\t\t  struct block_device **bdev,\n\t\t\t\t  fmode_t *mode)\n{\n\tstruct dm_target *tgt;\n\tstruct dm_table *map;\n\tint srcu_idx, r;\n\nretry:\n\tr = -ENOTTY;\n\tmap = dm_get_live_table(md, &srcu_idx);\n\tif (!map || !dm_table_get_size(map))\n\t\tgoto out;\n\n\t/* We only support devices that have a single target */\n\tif (dm_table_get_num_targets(map) != 1)\n\t\tgoto out;\n\n\ttgt = dm_table_get_target(map, 0);\n\tif (!tgt->type->prepare_ioctl)\n\t\tgoto out;\n\n\tif (dm_suspended_md(md)) {\n\t\tr = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tr = tgt->type->prepare_ioctl(tgt, bdev, mode);\n\tif (r < 0)\n\t\tgoto out;\n\n\tbdgrab(*bdev);\n\tdm_put_live_table(md, srcu_idx);\n\treturn r;\n\nout:\n\tdm_put_live_table(md, srcu_idx);\n\tif (r == -ENOTCONN && !fatal_signal_pending(current)) {\n\t\tmsleep(10);\n\t\tgoto retry;\n\t}\n\treturn r;\n}\n\nstatic int dm_blk_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t\tunsigned int cmd, unsigned long arg)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (r > 0) {\n\t\t/*\n\t\t * Target determined this ioctl is being issued against a\n\t\t * subset of the parent bdev; require extra privileges.\n\t\t */\n\t\tif (!capable(CAP_SYS_RAWIO)) {\n\t\t\tDMWARN_LIMIT(\n\t\"%s: sending ioctl %x to DM device without required privilege.\",\n\t\t\t\tcurrent->comm, cmd);\n\t\t\tr = -ENOIOCTLCMD;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr =  __blkdev_driver_ioctl(bdev, mode, cmd, arg);\nout:\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic struct dm_io *alloc_io(struct mapped_device *md)\n{\n\treturn mempool_alloc(md->io_pool, GFP_NOIO);\n}\n\nstatic void free_io(struct mapped_device *md, struct dm_io *io)\n{\n\tmempool_free(io, md->io_pool);\n}\n\nstatic void free_tio(struct dm_target_io *tio)\n{\n\tbio_put(&tio->clone);\n}\n\nint md_in_flight(struct mapped_device *md)\n{\n\treturn atomic_read(&md->pending[READ]) +\n\t       atomic_read(&md->pending[WRITE]);\n}\n\nstatic void start_io_acct(struct dm_io *io)\n{\n\tstruct mapped_device *md = io->md;\n\tstruct bio *bio = io->bio;\n\tint cpu;\n\tint rw = bio_data_dir(bio);\n\n\tio->start_time = jiffies;\n\n\tcpu = part_stat_lock();\n\tpart_round_stats(md->queue, cpu, &dm_disk(md)->part0);\n\tpart_stat_unlock();\n\tatomic_set(&dm_disk(md)->part0.in_flight[rw],\n\t\tatomic_inc_return(&md->pending[rw]));\n\n\tif (unlikely(dm_stats_used(&md->stats)))\n\t\tdm_stats_account_io(&md->stats, bio_data_dir(bio),\n\t\t\t\t    bio->bi_iter.bi_sector, bio_sectors(bio),\n\t\t\t\t    false, 0, &io->stats_aux);\n}\n\nstatic void end_io_acct(struct dm_io *io)\n{\n\tstruct mapped_device *md = io->md;\n\tstruct bio *bio = io->bio;\n\tunsigned long duration = jiffies - io->start_time;\n\tint pending;\n\tint rw = bio_data_dir(bio);\n\n\tgeneric_end_io_acct(md->queue, rw, &dm_disk(md)->part0, io->start_time);\n\n\tif (unlikely(dm_stats_used(&md->stats)))\n\t\tdm_stats_account_io(&md->stats, bio_data_dir(bio),\n\t\t\t\t    bio->bi_iter.bi_sector, bio_sectors(bio),\n\t\t\t\t    true, duration, &io->stats_aux);\n\n\t/*\n\t * After this is decremented the bio must not be touched if it is\n\t * a flush.\n\t */\n\tpending = atomic_dec_return(&md->pending[rw]);\n\tatomic_set(&dm_disk(md)->part0.in_flight[rw], pending);\n\tpending += atomic_read(&md->pending[rw^0x1]);\n\n\t/* nudge anyone waiting on suspend queue */\n\tif (!pending)\n\t\twake_up(&md->wait);\n}\n\n/*\n * Add the bio to the list of deferred io.\n */\nstatic void queue_io(struct mapped_device *md, struct bio *bio)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md->deferred_lock, flags);\n\tbio_list_add(&md->deferred, bio);\n\tspin_unlock_irqrestore(&md->deferred_lock, flags);\n\tqueue_work(md->wq, &md->work);\n}\n\n/*\n * Everyone (including functions in this file), should use this\n * function to access the md->map field, and make sure they call\n * dm_put_live_table() when finished.\n */\nstruct dm_table *dm_get_live_table(struct mapped_device *md, int *srcu_idx) __acquires(md->io_barrier)\n{\n\t*srcu_idx = srcu_read_lock(&md->io_barrier);\n\n\treturn srcu_dereference(md->map, &md->io_barrier);\n}\n\nvoid dm_put_live_table(struct mapped_device *md, int srcu_idx) __releases(md->io_barrier)\n{\n\tsrcu_read_unlock(&md->io_barrier, srcu_idx);\n}\n\nvoid dm_sync_table(struct mapped_device *md)\n{\n\tsynchronize_srcu(&md->io_barrier);\n\tsynchronize_rcu_expedited();\n}\n\n/*\n * A fast alternative to dm_get_live_table/dm_put_live_table.\n * The caller must not block between these two functions.\n */\nstatic struct dm_table *dm_get_live_table_fast(struct mapped_device *md) __acquires(RCU)\n{\n\trcu_read_lock();\n\treturn rcu_dereference(md->map);\n}\n\nstatic void dm_put_live_table_fast(struct mapped_device *md) __releases(RCU)\n{\n\trcu_read_unlock();\n}\n\n/*\n * Open a table device so we can use it as a map destination.\n */\nstatic int open_table_device(struct table_device *td, dev_t dev,\n\t\t\t     struct mapped_device *md)\n{\n\tstatic char *_claim_ptr = \"I belong to device-mapper\";\n\tstruct block_device *bdev;\n\n\tint r;\n\n\tBUG_ON(td->dm_dev.bdev);\n\n\tbdev = blkdev_get_by_dev(dev, td->dm_dev.mode | FMODE_EXCL, _claim_ptr);\n\tif (IS_ERR(bdev))\n\t\treturn PTR_ERR(bdev);\n\n\tr = bd_link_disk_holder(bdev, dm_disk(md));\n\tif (r) {\n\t\tblkdev_put(bdev, td->dm_dev.mode | FMODE_EXCL);\n\t\treturn r;\n\t}\n\n\ttd->dm_dev.bdev = bdev;\n\ttd->dm_dev.dax_dev = dax_get_by_host(bdev->bd_disk->disk_name);\n\treturn 0;\n}\n\n/*\n * Close a table device that we've been using.\n */\nstatic void close_table_device(struct table_device *td, struct mapped_device *md)\n{\n\tif (!td->dm_dev.bdev)\n\t\treturn;\n\n\tbd_unlink_disk_holder(td->dm_dev.bdev, dm_disk(md));\n\tblkdev_put(td->dm_dev.bdev, td->dm_dev.mode | FMODE_EXCL);\n\tput_dax(td->dm_dev.dax_dev);\n\ttd->dm_dev.bdev = NULL;\n\ttd->dm_dev.dax_dev = NULL;\n}\n\nstatic struct table_device *find_table_device(struct list_head *l, dev_t dev,\n\t\t\t\t\t      fmode_t mode) {\n\tstruct table_device *td;\n\n\tlist_for_each_entry(td, l, list)\n\t\tif (td->dm_dev.bdev->bd_dev == dev && td->dm_dev.mode == mode)\n\t\t\treturn td;\n\n\treturn NULL;\n}\n\nint dm_get_table_device(struct mapped_device *md, dev_t dev, fmode_t mode,\n\t\t\tstruct dm_dev **result) {\n\tint r;\n\tstruct table_device *td;\n\n\tmutex_lock(&md->table_devices_lock);\n\ttd = find_table_device(&md->table_devices, dev, mode);\n\tif (!td) {\n\t\ttd = kmalloc_node(sizeof(*td), GFP_KERNEL, md->numa_node_id);\n\t\tif (!td) {\n\t\t\tmutex_unlock(&md->table_devices_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttd->dm_dev.mode = mode;\n\t\ttd->dm_dev.bdev = NULL;\n\n\t\tif ((r = open_table_device(td, dev, md))) {\n\t\t\tmutex_unlock(&md->table_devices_lock);\n\t\t\tkfree(td);\n\t\t\treturn r;\n\t\t}\n\n\t\tformat_dev_t(td->dm_dev.name, dev);\n\n\t\trefcount_set(&td->count, 1);\n\t\tlist_add(&td->list, &md->table_devices);\n\t} else {\n\t\trefcount_inc(&td->count);\n\t}\n\tmutex_unlock(&md->table_devices_lock);\n\n\t*result = &td->dm_dev;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_get_table_device);\n\nvoid dm_put_table_device(struct mapped_device *md, struct dm_dev *d)\n{\n\tstruct table_device *td = container_of(d, struct table_device, dm_dev);\n\n\tmutex_lock(&md->table_devices_lock);\n\tif (refcount_dec_and_test(&td->count)) {\n\t\tclose_table_device(td, md);\n\t\tlist_del(&td->list);\n\t\tkfree(td);\n\t}\n\tmutex_unlock(&md->table_devices_lock);\n}\nEXPORT_SYMBOL(dm_put_table_device);\n\nstatic void free_table_devices(struct list_head *devices)\n{\n\tstruct list_head *tmp, *next;\n\n\tlist_for_each_safe(tmp, next, devices) {\n\t\tstruct table_device *td = list_entry(tmp, struct table_device, list);\n\n\t\tDMWARN(\"dm_destroy: %s still exists with %d references\",\n\t\t       td->dm_dev.name, refcount_read(&td->count));\n\t\tkfree(td);\n\t}\n}\n\n/*\n * Get the geometry associated with a dm device\n */\nint dm_get_geometry(struct mapped_device *md, struct hd_geometry *geo)\n{\n\t*geo = md->geometry;\n\n\treturn 0;\n}\n\n/*\n * Set the geometry of a device.\n */\nint dm_set_geometry(struct mapped_device *md, struct hd_geometry *geo)\n{\n\tsector_t sz = (sector_t)geo->cylinders * geo->heads * geo->sectors;\n\n\tif (geo->start > sz) {\n\t\tDMWARN(\"Start sector is beyond the geometry limits.\");\n\t\treturn -EINVAL;\n\t}\n\n\tmd->geometry = *geo;\n\n\treturn 0;\n}\n\n/*-----------------------------------------------------------------\n * CRUD START:\n *   A more elegant soln is in the works that uses the queue\n *   merge fn, unfortunately there are a couple of changes to\n *   the block layer that I want to make for this.  So in the\n *   interests of getting something for people to use I give\n *   you this clearly demarcated crap.\n *---------------------------------------------------------------*/\n\nstatic int __noflush_suspending(struct mapped_device *md)\n{\n\treturn test_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);\n}\n\n/*\n * Decrements the number of outstanding ios that a bio has been\n * cloned into, completing the original io if necc.\n */\nstatic void dec_pending(struct dm_io *io, blk_status_t error)\n{\n\tunsigned long flags;\n\tblk_status_t io_error;\n\tstruct bio *bio;\n\tstruct mapped_device *md = io->md;\n\n\t/* Push-back supersedes any I/O errors */\n\tif (unlikely(error)) {\n\t\tspin_lock_irqsave(&io->endio_lock, flags);\n\t\tif (!(io->status == BLK_STS_DM_REQUEUE &&\n\t\t\t\t__noflush_suspending(md)))\n\t\t\tio->status = error;\n\t\tspin_unlock_irqrestore(&io->endio_lock, flags);\n\t}\n\n\tif (atomic_dec_and_test(&io->io_count)) {\n\t\tif (io->status == BLK_STS_DM_REQUEUE) {\n\t\t\t/*\n\t\t\t * Target requested pushing back the I/O.\n\t\t\t */\n\t\t\tspin_lock_irqsave(&md->deferred_lock, flags);\n\t\t\tif (__noflush_suspending(md))\n\t\t\t\tbio_list_add_head(&md->deferred, io->bio);\n\t\t\telse\n\t\t\t\t/* noflush suspend was interrupted. */\n\t\t\t\tio->status = BLK_STS_IOERR;\n\t\t\tspin_unlock_irqrestore(&md->deferred_lock, flags);\n\t\t}\n\n\t\tio_error = io->status;\n\t\tbio = io->bio;\n\t\tend_io_acct(io);\n\t\tfree_io(md, io);\n\n\t\tif (io_error == BLK_STS_DM_REQUEUE)\n\t\t\treturn;\n\n\t\tif ((bio->bi_opf & REQ_PREFLUSH) && bio->bi_iter.bi_size) {\n\t\t\t/*\n\t\t\t * Preflush done for flush with data, reissue\n\t\t\t * without REQ_PREFLUSH.\n\t\t\t */\n\t\t\tbio->bi_opf &= ~REQ_PREFLUSH;\n\t\t\tqueue_io(md, bio);\n\t\t} else {\n\t\t\t/* done with normal IO or empty flush */\n\t\t\tbio->bi_status = io_error;\n\t\t\tbio_endio(bio);\n\t\t}\n\t}\n}\n\nvoid disable_write_same(struct mapped_device *md)\n{\n\tstruct queue_limits *limits = dm_get_queue_limits(md);\n\n\t/* device doesn't really support WRITE SAME, disable it */\n\tlimits->max_write_same_sectors = 0;\n}\n\nvoid disable_write_zeroes(struct mapped_device *md)\n{\n\tstruct queue_limits *limits = dm_get_queue_limits(md);\n\n\t/* device doesn't really support WRITE ZEROES, disable it */\n\tlimits->max_write_zeroes_sectors = 0;\n}\n\nstatic void clone_endio(struct bio *bio)\n{\n\tblk_status_t error = bio->bi_status;\n\tstruct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);\n\tstruct dm_io *io = tio->io;\n\tstruct mapped_device *md = tio->io->md;\n\tdm_endio_fn endio = tio->ti->type->end_io;\n\n\tif (unlikely(error == BLK_STS_TARGET)) {\n\t\tif (bio_op(bio) == REQ_OP_WRITE_SAME &&\n\t\t    !bio->bi_disk->queue->limits.max_write_same_sectors)\n\t\t\tdisable_write_same(md);\n\t\tif (bio_op(bio) == REQ_OP_WRITE_ZEROES &&\n\t\t    !bio->bi_disk->queue->limits.max_write_zeroes_sectors)\n\t\t\tdisable_write_zeroes(md);\n\t}\n\n\tif (endio) {\n\t\tint r = endio(tio->ti, bio, &error);\n\t\tswitch (r) {\n\t\tcase DM_ENDIO_REQUEUE:\n\t\t\terror = BLK_STS_DM_REQUEUE;\n\t\t\t/*FALLTHRU*/\n\t\tcase DM_ENDIO_DONE:\n\t\t\tbreak;\n\t\tcase DM_ENDIO_INCOMPLETE:\n\t\t\t/* The target will handle the io */\n\t\t\treturn;\n\t\tdefault:\n\t\t\tDMWARN(\"unimplemented target endio return value: %d\", r);\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tfree_tio(tio);\n\tdec_pending(io, error);\n}\n\n/*\n * Return maximum size of I/O possible at the supplied sector up to the current\n * target boundary.\n */\nstatic sector_t max_io_len_target_boundary(sector_t sector, struct dm_target *ti)\n{\n\tsector_t target_offset = dm_target_offset(ti, sector);\n\n\treturn ti->len - target_offset;\n}\n\nstatic sector_t max_io_len(sector_t sector, struct dm_target *ti)\n{\n\tsector_t len = max_io_len_target_boundary(sector, ti);\n\tsector_t offset, max_len;\n\n\t/*\n\t * Does the target need to split even further?\n\t */\n\tif (ti->max_io_len) {\n\t\toffset = dm_target_offset(ti, sector);\n\t\tif (unlikely(ti->max_io_len & (ti->max_io_len - 1)))\n\t\t\tmax_len = sector_div(offset, ti->max_io_len);\n\t\telse\n\t\t\tmax_len = offset & (ti->max_io_len - 1);\n\t\tmax_len = ti->max_io_len - max_len;\n\n\t\tif (len > max_len)\n\t\t\tlen = max_len;\n\t}\n\n\treturn len;\n}\n\nint dm_set_target_max_io_len(struct dm_target *ti, sector_t len)\n{\n\tif (len > UINT_MAX) {\n\t\tDMERR(\"Specified maximum size of target IO (%llu) exceeds limit (%u)\",\n\t\t      (unsigned long long)len, UINT_MAX);\n\t\tti->error = \"Maximum size of target IO is too large\";\n\t\treturn -EINVAL;\n\t}\n\n\tti->max_io_len = (uint32_t) len;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_set_target_max_io_len);\n\nstatic struct dm_target *dm_dax_get_live_target(struct mapped_device *md,\n\t\tsector_t sector, int *srcu_idx)\n{\n\tstruct dm_table *map;\n\tstruct dm_target *ti;\n\n\tmap = dm_get_live_table(md, srcu_idx);\n\tif (!map)\n\t\treturn NULL;\n\n\tti = dm_table_find_target(map, sector);\n\tif (!dm_target_is_valid(ti))\n\t\treturn NULL;\n\n\treturn ti;\n}\n\nstatic long dm_dax_direct_access(struct dax_device *dax_dev, pgoff_t pgoff,\n\t\tlong nr_pages, void **kaddr, pfn_t *pfn)\n{\n\tstruct mapped_device *md = dax_get_private(dax_dev);\n\tsector_t sector = pgoff * PAGE_SECTORS;\n\tstruct dm_target *ti;\n\tlong len, ret = -EIO;\n\tint srcu_idx;\n\n\tti = dm_dax_get_live_target(md, sector, &srcu_idx);\n\n\tif (!ti)\n\t\tgoto out;\n\tif (!ti->type->direct_access)\n\t\tgoto out;\n\tlen = max_io_len(sector, ti) / PAGE_SECTORS;\n\tif (len < 1)\n\t\tgoto out;\n\tnr_pages = min(len, nr_pages);\n\tif (ti->type->direct_access)\n\t\tret = ti->type->direct_access(ti, pgoff, nr_pages, kaddr, pfn);\n\n out:\n\tdm_put_live_table(md, srcu_idx);\n\n\treturn ret;\n}\n\nstatic size_t dm_dax_copy_from_iter(struct dax_device *dax_dev, pgoff_t pgoff,\n\t\tvoid *addr, size_t bytes, struct iov_iter *i)\n{\n\tstruct mapped_device *md = dax_get_private(dax_dev);\n\tsector_t sector = pgoff * PAGE_SECTORS;\n\tstruct dm_target *ti;\n\tlong ret = 0;\n\tint srcu_idx;\n\n\tti = dm_dax_get_live_target(md, sector, &srcu_idx);\n\n\tif (!ti)\n\t\tgoto out;\n\tif (!ti->type->dax_copy_from_iter) {\n\t\tret = copy_from_iter(addr, bytes, i);\n\t\tgoto out;\n\t}\n\tret = ti->type->dax_copy_from_iter(ti, pgoff, addr, bytes, i);\n out:\n\tdm_put_live_table(md, srcu_idx);\n\n\treturn ret;\n}\n\n/*\n * A target may call dm_accept_partial_bio only from the map routine.  It is\n * allowed for all bio types except REQ_PREFLUSH.\n *\n * dm_accept_partial_bio informs the dm that the target only wants to process\n * additional n_sectors sectors of the bio and the rest of the data should be\n * sent in a next bio.\n *\n * A diagram that explains the arithmetics:\n * +--------------------+---------------+-------+\n * |         1          |       2       |   3   |\n * +--------------------+---------------+-------+\n *\n * <-------------- *tio->len_ptr --------------->\n *                      <------- bi_size ------->\n *                      <-- n_sectors -->\n *\n * Region 1 was already iterated over with bio_advance or similar function.\n *\t(it may be empty if the target doesn't use bio_advance)\n * Region 2 is the remaining bio size that the target wants to process.\n *\t(it may be empty if region 1 is non-empty, although there is no reason\n *\t to make it empty)\n * The target requires that region 3 is to be sent in the next bio.\n *\n * If the target wants to receive multiple copies of the bio (via num_*bios, etc),\n * the partially processed part (the sum of regions 1+2) must be the same for all\n * copies of the bio.\n */\nvoid dm_accept_partial_bio(struct bio *bio, unsigned n_sectors)\n{\n\tstruct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);\n\tunsigned bi_size = bio->bi_iter.bi_size >> SECTOR_SHIFT;\n\tBUG_ON(bio->bi_opf & REQ_PREFLUSH);\n\tBUG_ON(bi_size > *tio->len_ptr);\n\tBUG_ON(n_sectors > bi_size);\n\t*tio->len_ptr -= bi_size - n_sectors;\n\tbio->bi_iter.bi_size = n_sectors << SECTOR_SHIFT;\n}\nEXPORT_SYMBOL_GPL(dm_accept_partial_bio);\n\n/*\n * The zone descriptors obtained with a zone report indicate\n * zone positions within the target device. The zone descriptors\n * must be remapped to match their position within the dm device.\n * A target may call dm_remap_zone_report after completion of a\n * REQ_OP_ZONE_REPORT bio to remap the zone descriptors obtained\n * from the target device mapping to the dm device.\n */\nvoid dm_remap_zone_report(struct dm_target *ti, struct bio *bio, sector_t start)\n{\n#ifdef CONFIG_BLK_DEV_ZONED\n\tstruct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);\n\tstruct bio *report_bio = tio->io->bio;\n\tstruct blk_zone_report_hdr *hdr = NULL;\n\tstruct blk_zone *zone;\n\tunsigned int nr_rep = 0;\n\tunsigned int ofst;\n\tstruct bio_vec bvec;\n\tstruct bvec_iter iter;\n\tvoid *addr;\n\n\tif (bio->bi_status)\n\t\treturn;\n\n\t/*\n\t * Remap the start sector of the reported zones. For sequential zones,\n\t * also remap the write pointer position.\n\t */\n\tbio_for_each_segment(bvec, report_bio, iter) {\n\t\taddr = kmap_atomic(bvec.bv_page);\n\n\t\t/* Remember the report header in the first page */\n\t\tif (!hdr) {\n\t\t\thdr = addr;\n\t\t\tofst = sizeof(struct blk_zone_report_hdr);\n\t\t} else\n\t\t\tofst = 0;\n\n\t\t/* Set zones start sector */\n\t\twhile (hdr->nr_zones && ofst < bvec.bv_len) {\n\t\t\tzone = addr + ofst;\n\t\t\tif (zone->start >= start + ti->len) {\n\t\t\t\thdr->nr_zones = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tzone->start = zone->start + ti->begin - start;\n\t\t\tif (zone->type != BLK_ZONE_TYPE_CONVENTIONAL) {\n\t\t\t\tif (zone->cond == BLK_ZONE_COND_FULL)\n\t\t\t\t\tzone->wp = zone->start + zone->len;\n\t\t\t\telse if (zone->cond == BLK_ZONE_COND_EMPTY)\n\t\t\t\t\tzone->wp = zone->start;\n\t\t\t\telse\n\t\t\t\t\tzone->wp = zone->wp + ti->begin - start;\n\t\t\t}\n\t\t\tofst += sizeof(struct blk_zone);\n\t\t\thdr->nr_zones--;\n\t\t\tnr_rep++;\n\t\t}\n\n\t\tif (addr != hdr)\n\t\t\tkunmap_atomic(addr);\n\n\t\tif (!hdr->nr_zones)\n\t\t\tbreak;\n\t}\n\n\tif (hdr) {\n\t\thdr->nr_zones = nr_rep;\n\t\tkunmap_atomic(hdr);\n\t}\n\n\tbio_advance(report_bio, report_bio->bi_iter.bi_size);\n\n#else /* !CONFIG_BLK_DEV_ZONED */\n\tbio->bi_status = BLK_STS_NOTSUPP;\n#endif\n}\nEXPORT_SYMBOL_GPL(dm_remap_zone_report);\n\n/*\n * Flush current->bio_list when the target map method blocks.\n * This fixes deadlocks in snapshot and possibly in other targets.\n */\nstruct dm_offload {\n\tstruct blk_plug plug;\n\tstruct blk_plug_cb cb;\n};\n\nstatic void flush_current_bio_list(struct blk_plug_cb *cb, bool from_schedule)\n{\n\tstruct dm_offload *o = container_of(cb, struct dm_offload, cb);\n\tstruct bio_list list;\n\tstruct bio *bio;\n\tint i;\n\n\tINIT_LIST_HEAD(&o->cb.list);\n\n\tif (unlikely(!current->bio_list))\n\t\treturn;\n\n\tfor (i = 0; i < 2; i++) {\n\t\tlist = current->bio_list[i];\n\t\tbio_list_init(&current->bio_list[i]);\n\n\t\twhile ((bio = bio_list_pop(&list))) {\n\t\t\tstruct bio_set *bs = bio->bi_pool;\n\t\t\tif (unlikely(!bs) || bs == fs_bio_set ||\n\t\t\t    !bs->rescue_workqueue) {\n\t\t\t\tbio_list_add(&current->bio_list[i], bio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tspin_lock(&bs->rescue_lock);\n\t\t\tbio_list_add(&bs->rescue_list, bio);\n\t\t\tqueue_work(bs->rescue_workqueue, &bs->rescue_work);\n\t\t\tspin_unlock(&bs->rescue_lock);\n\t\t}\n\t}\n}\n\nstatic void dm_offload_start(struct dm_offload *o)\n{\n\tblk_start_plug(&o->plug);\n\to->cb.callback = flush_current_bio_list;\n\tlist_add(&o->cb.list, &current->plug->cb_list);\n}\n\nstatic void dm_offload_end(struct dm_offload *o)\n{\n\tlist_del(&o->cb.list);\n\tblk_finish_plug(&o->plug);\n}\n\nstatic void __map_bio(struct dm_target_io *tio)\n{\n\tint r;\n\tsector_t sector;\n\tstruct dm_offload o;\n\tstruct bio *clone = &tio->clone;\n\tstruct dm_target *ti = tio->ti;\n\n\tclone->bi_end_io = clone_endio;\n\n\t/*\n\t * Map the clone.  If r == 0 we don't need to do\n\t * anything, the target has assumed ownership of\n\t * this io.\n\t */\n\tatomic_inc(&tio->io->io_count);\n\tsector = clone->bi_iter.bi_sector;\n\n\tdm_offload_start(&o);\n\tr = ti->type->map(ti, clone);\n\tdm_offload_end(&o);\n\n\tswitch (r) {\n\tcase DM_MAPIO_SUBMITTED:\n\t\tbreak;\n\tcase DM_MAPIO_REMAPPED:\n\t\t/* the bio has been remapped so dispatch it */\n\t\ttrace_block_bio_remap(clone->bi_disk->queue, clone,\n\t\t\t\t      bio_dev(tio->io->bio), sector);\n\t\tgeneric_make_request(clone);\n\t\tbreak;\n\tcase DM_MAPIO_KILL:\n\t\tdec_pending(tio->io, BLK_STS_IOERR);\n\t\tfree_tio(tio);\n\t\tbreak;\n\tcase DM_MAPIO_REQUEUE:\n\t\tdec_pending(tio->io, BLK_STS_DM_REQUEUE);\n\t\tfree_tio(tio);\n\t\tbreak;\n\tdefault:\n\t\tDMWARN(\"unimplemented target map return value: %d\", r);\n\t\tBUG();\n\t}\n}\n\nstruct clone_info {\n\tstruct mapped_device *md;\n\tstruct dm_table *map;\n\tstruct bio *bio;\n\tstruct dm_io *io;\n\tsector_t sector;\n\tunsigned sector_count;\n};\n\nstatic void bio_setup_sector(struct bio *bio, sector_t sector, unsigned len)\n{\n\tbio->bi_iter.bi_sector = sector;\n\tbio->bi_iter.bi_size = to_bytes(len);\n}\n\n/*\n * Creates a bio that consists of range of complete bvecs.\n */\nstatic int clone_bio(struct dm_target_io *tio, struct bio *bio,\n\t\t     sector_t sector, unsigned len)\n{\n\tstruct bio *clone = &tio->clone;\n\n\t__bio_clone_fast(clone, bio);\n\n\tif (unlikely(bio_integrity(bio) != NULL)) {\n\t\tint r;\n\n\t\tif (unlikely(!dm_target_has_integrity(tio->ti->type) &&\n\t\t\t     !dm_target_passes_integrity(tio->ti->type))) {\n\t\t\tDMWARN(\"%s: the target %s doesn't support integrity data.\",\n\t\t\t\tdm_device_name(tio->io->md),\n\t\t\t\ttio->ti->type->name);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tr = bio_integrity_clone(clone, bio, GFP_NOIO);\n\t\tif (r < 0)\n\t\t\treturn r;\n\t}\n\n\tif (bio_op(bio) != REQ_OP_ZONE_REPORT)\n\t\tbio_advance(clone, to_bytes(sector - clone->bi_iter.bi_sector));\n\tclone->bi_iter.bi_size = to_bytes(len);\n\n\tif (unlikely(bio_integrity(bio) != NULL))\n\t\tbio_integrity_trim(clone);\n\n\treturn 0;\n}\n\nstatic struct dm_target_io *alloc_tio(struct clone_info *ci,\n\t\t\t\t      struct dm_target *ti,\n\t\t\t\t      unsigned target_bio_nr)\n{\n\tstruct dm_target_io *tio;\n\tstruct bio *clone;\n\n\tclone = bio_alloc_bioset(GFP_NOIO, 0, ci->md->bs);\n\ttio = container_of(clone, struct dm_target_io, clone);\n\n\ttio->io = ci->io;\n\ttio->ti = ti;\n\ttio->target_bio_nr = target_bio_nr;\n\n\treturn tio;\n}\n\nstatic void __clone_and_map_simple_bio(struct clone_info *ci,\n\t\t\t\t       struct dm_target *ti,\n\t\t\t\t       unsigned target_bio_nr, unsigned *len)\n{\n\tstruct dm_target_io *tio = alloc_tio(ci, ti, target_bio_nr);\n\tstruct bio *clone = &tio->clone;\n\n\ttio->len_ptr = len;\n\n\t__bio_clone_fast(clone, ci->bio);\n\tif (len)\n\t\tbio_setup_sector(clone, ci->sector, *len);\n\n\t__map_bio(tio);\n}\n\nstatic void __send_duplicate_bios(struct clone_info *ci, struct dm_target *ti,\n\t\t\t\t  unsigned num_bios, unsigned *len)\n{\n\tunsigned target_bio_nr;\n\n\tfor (target_bio_nr = 0; target_bio_nr < num_bios; target_bio_nr++)\n\t\t__clone_and_map_simple_bio(ci, ti, target_bio_nr, len);\n}\n\nstatic int __send_empty_flush(struct clone_info *ci)\n{\n\tunsigned target_nr = 0;\n\tstruct dm_target *ti;\n\n\tBUG_ON(bio_has_data(ci->bio));\n\twhile ((ti = dm_table_get_target(ci->map, target_nr++)))\n\t\t__send_duplicate_bios(ci, ti, ti->num_flush_bios, NULL);\n\n\treturn 0;\n}\n\nstatic int __clone_and_map_data_bio(struct clone_info *ci, struct dm_target *ti,\n\t\t\t\t     sector_t sector, unsigned *len)\n{\n\tstruct bio *bio = ci->bio;\n\tstruct dm_target_io *tio;\n\tunsigned target_bio_nr;\n\tunsigned num_target_bios = 1;\n\tint r = 0;\n\n\t/*\n\t * Does the target want to receive duplicate copies of the bio?\n\t */\n\tif (bio_data_dir(bio) == WRITE && ti->num_write_bios)\n\t\tnum_target_bios = ti->num_write_bios(ti, bio);\n\n\tfor (target_bio_nr = 0; target_bio_nr < num_target_bios; target_bio_nr++) {\n\t\ttio = alloc_tio(ci, ti, target_bio_nr);\n\t\ttio->len_ptr = len;\n\t\tr = clone_bio(tio, bio, sector, *len);\n\t\tif (r < 0) {\n\t\t\tfree_tio(tio);\n\t\t\tbreak;\n\t\t}\n\t\t__map_bio(tio);\n\t}\n\n\treturn r;\n}\n\ntypedef unsigned (*get_num_bios_fn)(struct dm_target *ti);\n\nstatic unsigned get_num_discard_bios(struct dm_target *ti)\n{\n\treturn ti->num_discard_bios;\n}\n\nstatic unsigned get_num_write_same_bios(struct dm_target *ti)\n{\n\treturn ti->num_write_same_bios;\n}\n\nstatic unsigned get_num_write_zeroes_bios(struct dm_target *ti)\n{\n\treturn ti->num_write_zeroes_bios;\n}\n\ntypedef bool (*is_split_required_fn)(struct dm_target *ti);\n\nstatic bool is_split_required_for_discard(struct dm_target *ti)\n{\n\treturn ti->split_discard_bios;\n}\n\nstatic int __send_changing_extent_only(struct clone_info *ci,\n\t\t\t\t       get_num_bios_fn get_num_bios,\n\t\t\t\t       is_split_required_fn is_split_required)\n{\n\tstruct dm_target *ti;\n\tunsigned len;\n\tunsigned num_bios;\n\n\tdo {\n\t\tti = dm_table_find_target(ci->map, ci->sector);\n\t\tif (!dm_target_is_valid(ti))\n\t\t\treturn -EIO;\n\n\t\t/*\n\t\t * Even though the device advertised support for this type of\n\t\t * request, that does not mean every target supports it, and\n\t\t * reconfiguration might also have changed that since the\n\t\t * check was performed.\n\t\t */\n\t\tnum_bios = get_num_bios ? get_num_bios(ti) : 0;\n\t\tif (!num_bios)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (is_split_required && !is_split_required(ti))\n\t\t\tlen = min((sector_t)ci->sector_count, max_io_len_target_boundary(ci->sector, ti));\n\t\telse\n\t\t\tlen = min((sector_t)ci->sector_count, max_io_len(ci->sector, ti));\n\n\t\t__send_duplicate_bios(ci, ti, num_bios, &len);\n\n\t\tci->sector += len;\n\t} while (ci->sector_count -= len);\n\n\treturn 0;\n}\n\nstatic int __send_discard(struct clone_info *ci)\n{\n\treturn __send_changing_extent_only(ci, get_num_discard_bios,\n\t\t\t\t\t   is_split_required_for_discard);\n}\n\nstatic int __send_write_same(struct clone_info *ci)\n{\n\treturn __send_changing_extent_only(ci, get_num_write_same_bios, NULL);\n}\n\nstatic int __send_write_zeroes(struct clone_info *ci)\n{\n\treturn __send_changing_extent_only(ci, get_num_write_zeroes_bios, NULL);\n}\n\n/*\n * Select the correct strategy for processing a non-flush bio.\n */\nstatic int __split_and_process_non_flush(struct clone_info *ci)\n{\n\tstruct bio *bio = ci->bio;\n\tstruct dm_target *ti;\n\tunsigned len;\n\tint r;\n\n\tif (unlikely(bio_op(bio) == REQ_OP_DISCARD))\n\t\treturn __send_discard(ci);\n\telse if (unlikely(bio_op(bio) == REQ_OP_WRITE_SAME))\n\t\treturn __send_write_same(ci);\n\telse if (unlikely(bio_op(bio) == REQ_OP_WRITE_ZEROES))\n\t\treturn __send_write_zeroes(ci);\n\n\tti = dm_table_find_target(ci->map, ci->sector);\n\tif (!dm_target_is_valid(ti))\n\t\treturn -EIO;\n\n\tif (bio_op(bio) == REQ_OP_ZONE_REPORT)\n\t\tlen = ci->sector_count;\n\telse\n\t\tlen = min_t(sector_t, max_io_len(ci->sector, ti),\n\t\t\t    ci->sector_count);\n\n\tr = __clone_and_map_data_bio(ci, ti, ci->sector, &len);\n\tif (r < 0)\n\t\treturn r;\n\n\tci->sector += len;\n\tci->sector_count -= len;\n\n\treturn 0;\n}\n\n/*\n * Entry point to split a bio into clones and submit them to the targets.\n */\nstatic void __split_and_process_bio(struct mapped_device *md,\n\t\t\t\t    struct dm_table *map, struct bio *bio)\n{\n\tstruct clone_info ci;\n\tint error = 0;\n\n\tif (unlikely(!map)) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\tci.map = map;\n\tci.md = md;\n\tci.io = alloc_io(md);\n\tci.io->status = 0;\n\tatomic_set(&ci.io->io_count, 1);\n\tci.io->bio = bio;\n\tci.io->md = md;\n\tspin_lock_init(&ci.io->endio_lock);\n\tci.sector = bio->bi_iter.bi_sector;\n\n\tstart_io_acct(ci.io);\n\n\tif (bio->bi_opf & REQ_PREFLUSH) {\n\t\tci.bio = &ci.md->flush_bio;\n\t\tci.sector_count = 0;\n\t\terror = __send_empty_flush(&ci);\n\t\t/* dec_pending submits any data associated with flush */\n\t} else if (bio_op(bio) == REQ_OP_ZONE_RESET) {\n\t\tci.bio = bio;\n\t\tci.sector_count = 0;\n\t\terror = __split_and_process_non_flush(&ci);\n\t} else {\n\t\tci.bio = bio;\n\t\tci.sector_count = bio_sectors(bio);\n\t\twhile (ci.sector_count && !error)\n\t\t\terror = __split_and_process_non_flush(&ci);\n\t}\n\n\t/* drop the extra reference count */\n\tdec_pending(ci.io, errno_to_blk_status(error));\n}\n/*-----------------------------------------------------------------\n * CRUD END\n *---------------------------------------------------------------*/\n\n/*\n * The request function that just remaps the bio built up by\n * dm_merge_bvec.\n */\nstatic blk_qc_t dm_make_request(struct request_queue *q, struct bio *bio)\n{\n\tint rw = bio_data_dir(bio);\n\tstruct mapped_device *md = q->queuedata;\n\tint srcu_idx;\n\tstruct dm_table *map;\n\n\tmap = dm_get_live_table(md, &srcu_idx);\n\n\tgeneric_start_io_acct(q, rw, bio_sectors(bio), &dm_disk(md)->part0);\n\n\t/* if we're suspended, we have to queue this io for later */\n\tif (unlikely(test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags))) {\n\t\tdm_put_live_table(md, srcu_idx);\n\n\t\tif (!(bio->bi_opf & REQ_RAHEAD))\n\t\t\tqueue_io(md, bio);\n\t\telse\n\t\t\tbio_io_error(bio);\n\t\treturn BLK_QC_T_NONE;\n\t}\n\n\t__split_and_process_bio(md, map, bio);\n\tdm_put_live_table(md, srcu_idx);\n\treturn BLK_QC_T_NONE;\n}\n\nstatic int dm_any_congested(void *congested_data, int bdi_bits)\n{\n\tint r = bdi_bits;\n\tstruct mapped_device *md = congested_data;\n\tstruct dm_table *map;\n\n\tif (!test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags)) {\n\t\tif (dm_request_based(md)) {\n\t\t\t/*\n\t\t\t * With request-based DM we only need to check the\n\t\t\t * top-level queue for congestion.\n\t\t\t */\n\t\t\tr = md->queue->backing_dev_info->wb.state & bdi_bits;\n\t\t} else {\n\t\t\tmap = dm_get_live_table_fast(md);\n\t\t\tif (map)\n\t\t\t\tr = dm_table_any_congested(map, bdi_bits);\n\t\t\tdm_put_live_table_fast(md);\n\t\t}\n\t}\n\n\treturn r;\n}\n\n/*-----------------------------------------------------------------\n * An IDR is used to keep track of allocated minor numbers.\n *---------------------------------------------------------------*/\nstatic void free_minor(int minor)\n{\n\tspin_lock(&_minor_lock);\n\tidr_remove(&_minor_idr, minor);\n\tspin_unlock(&_minor_lock);\n}\n\n/*\n * See if the device with a specific minor # is free.\n */\nstatic int specific_minor(int minor)\n{\n\tint r;\n\n\tif (minor >= (1 << MINORBITS))\n\t\treturn -EINVAL;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&_minor_lock);\n\n\tr = idr_alloc(&_minor_idr, MINOR_ALLOCED, minor, minor + 1, GFP_NOWAIT);\n\n\tspin_unlock(&_minor_lock);\n\tidr_preload_end();\n\tif (r < 0)\n\t\treturn r == -ENOSPC ? -EBUSY : r;\n\treturn 0;\n}\n\nstatic int next_free_minor(int *minor)\n{\n\tint r;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&_minor_lock);\n\n\tr = idr_alloc(&_minor_idr, MINOR_ALLOCED, 0, 1 << MINORBITS, GFP_NOWAIT);\n\n\tspin_unlock(&_minor_lock);\n\tidr_preload_end();\n\tif (r < 0)\n\t\treturn r;\n\t*minor = r;\n\treturn 0;\n}\n\nstatic const struct block_device_operations dm_blk_dops;\nstatic const struct dax_operations dm_dax_ops;\n\nstatic void dm_wq_work(struct work_struct *work);\n\nvoid dm_init_md_queue(struct mapped_device *md)\n{\n\t/*\n\t * Request-based dm devices cannot be stacked on top of bio-based dm\n\t * devices.  The type of this dm device may not have been decided yet.\n\t * The type is decided at the first table loading time.\n\t * To prevent problematic device stacking, clear the queue flag\n\t * for request stacking support until then.\n\t *\n\t * This queue is new, so no concurrency on the queue_flags.\n\t */\n\tqueue_flag_clear_unlocked(QUEUE_FLAG_STACKABLE, md->queue);\n\n\t/*\n\t * Initialize data that will only be used by a non-blk-mq DM queue\n\t * - must do so here (in alloc_dev callchain) before queue is used\n\t */\n\tmd->queue->queuedata = md;\n\tmd->queue->backing_dev_info->congested_data = md;\n}\n\nvoid dm_init_normal_md_queue(struct mapped_device *md)\n{\n\tmd->use_blk_mq = false;\n\tdm_init_md_queue(md);\n\n\t/*\n\t * Initialize aspects of queue that aren't relevant for blk-mq\n\t */\n\tmd->queue->backing_dev_info->congested_fn = dm_any_congested;\n}\n\nstatic void cleanup_mapped_device(struct mapped_device *md)\n{\n\tif (md->wq)\n\t\tdestroy_workqueue(md->wq);\n\tif (md->kworker_task)\n\t\tkthread_stop(md->kworker_task);\n\tmempool_destroy(md->io_pool);\n\tif (md->bs)\n\t\tbioset_free(md->bs);\n\n\tif (md->dax_dev) {\n\t\tkill_dax(md->dax_dev);\n\t\tput_dax(md->dax_dev);\n\t\tmd->dax_dev = NULL;\n\t}\n\n\tif (md->disk) {\n\t\tspin_lock(&_minor_lock);\n\t\tmd->disk->private_data = NULL;\n\t\tspin_unlock(&_minor_lock);\n\t\tdel_gendisk(md->disk);\n\t\tput_disk(md->disk);\n\t}\n\n\tif (md->queue)\n\t\tblk_cleanup_queue(md->queue);\n\n\tcleanup_srcu_struct(&md->io_barrier);\n\n\tif (md->bdev) {\n\t\tbdput(md->bdev);\n\t\tmd->bdev = NULL;\n\t}\n\n\tdm_mq_cleanup_mapped_device(md);\n}\n\n/*\n * Allocate and initialise a blank device with a given minor.\n */\nstatic struct mapped_device *alloc_dev(int minor)\n{\n\tint r, numa_node_id = dm_get_numa_node();\n\tstruct dax_device *dax_dev;\n\tstruct mapped_device *md;\n\tvoid *old_md;\n\n\tmd = kvzalloc_node(sizeof(*md), GFP_KERNEL, numa_node_id);\n\tif (!md) {\n\t\tDMWARN(\"unable to allocate device, out of memory.\");\n\t\treturn NULL;\n\t}\n\n\tif (!try_module_get(THIS_MODULE))\n\t\tgoto bad_module_get;\n\n\t/* get a minor number for the dev */\n\tif (minor == DM_ANY_MINOR)\n\t\tr = next_free_minor(&minor);\n\telse\n\t\tr = specific_minor(minor);\n\tif (r < 0)\n\t\tgoto bad_minor;\n\n\tr = init_srcu_struct(&md->io_barrier);\n\tif (r < 0)\n\t\tgoto bad_io_barrier;\n\n\tmd->numa_node_id = numa_node_id;\n\tmd->use_blk_mq = dm_use_blk_mq_default();\n\tmd->init_tio_pdu = false;\n\tmd->type = DM_TYPE_NONE;\n\tmutex_init(&md->suspend_lock);\n\tmutex_init(&md->type_lock);\n\tmutex_init(&md->table_devices_lock);\n\tspin_lock_init(&md->deferred_lock);\n\tatomic_set(&md->holders, 1);\n\tatomic_set(&md->open_count, 0);\n\tatomic_set(&md->event_nr, 0);\n\tatomic_set(&md->uevent_seq, 0);\n\tINIT_LIST_HEAD(&md->uevent_list);\n\tINIT_LIST_HEAD(&md->table_devices);\n\tspin_lock_init(&md->uevent_lock);\n\n\tmd->queue = blk_alloc_queue_node(GFP_KERNEL, numa_node_id);\n\tif (!md->queue)\n\t\tgoto bad;\n\n\tdm_init_md_queue(md);\n\n\tmd->disk = alloc_disk_node(1, numa_node_id);\n\tif (!md->disk)\n\t\tgoto bad;\n\n\tatomic_set(&md->pending[0], 0);\n\tatomic_set(&md->pending[1], 0);\n\tinit_waitqueue_head(&md->wait);\n\tINIT_WORK(&md->work, dm_wq_work);\n\tinit_waitqueue_head(&md->eventq);\n\tinit_completion(&md->kobj_holder.completion);\n\tmd->kworker_task = NULL;\n\n\tmd->disk->major = _major;\n\tmd->disk->first_minor = minor;\n\tmd->disk->fops = &dm_blk_dops;\n\tmd->disk->queue = md->queue;\n\tmd->disk->private_data = md;\n\tsprintf(md->disk->disk_name, \"dm-%d\", minor);\n\n\tdax_dev = alloc_dax(md, md->disk->disk_name, &dm_dax_ops);\n\tif (!dax_dev)\n\t\tgoto bad;\n\tmd->dax_dev = dax_dev;\n\n\tadd_disk(md->disk);\n\tformat_dev_t(md->name, MKDEV(_major, minor));\n\n\tmd->wq = alloc_workqueue(\"kdmflush\", WQ_MEM_RECLAIM, 0);\n\tif (!md->wq)\n\t\tgoto bad;\n\n\tmd->bdev = bdget_disk(md->disk, 0);\n\tif (!md->bdev)\n\t\tgoto bad;\n\n\tbio_init(&md->flush_bio, NULL, 0);\n\tbio_set_dev(&md->flush_bio, md->bdev);\n\tmd->flush_bio.bi_opf = REQ_OP_WRITE | REQ_PREFLUSH | REQ_SYNC;\n\n\tdm_stats_init(&md->stats);\n\n\t/* Populate the mapping, nobody knows we exist yet */\n\tspin_lock(&_minor_lock);\n\told_md = idr_replace(&_minor_idr, md, minor);\n\tspin_unlock(&_minor_lock);\n\n\tBUG_ON(old_md != MINOR_ALLOCED);\n\n\treturn md;\n\nbad:\n\tcleanup_mapped_device(md);\nbad_io_barrier:\n\tfree_minor(minor);\nbad_minor:\n\tmodule_put(THIS_MODULE);\nbad_module_get:\n\tkvfree(md);\n\treturn NULL;\n}\n\nstatic void unlock_fs(struct mapped_device *md);\n\nstatic void free_dev(struct mapped_device *md)\n{\n\tint minor = MINOR(disk_devt(md->disk));\n\n\tunlock_fs(md);\n\n\tcleanup_mapped_device(md);\n\n\tfree_table_devices(&md->table_devices);\n\tdm_stats_cleanup(&md->stats);\n\tfree_minor(minor);\n\n\tmodule_put(THIS_MODULE);\n\tkvfree(md);\n}\n\nstatic void __bind_mempools(struct mapped_device *md, struct dm_table *t)\n{\n\tstruct dm_md_mempools *p = dm_table_get_md_mempools(t);\n\n\tif (md->bs) {\n\t\t/* The md already has necessary mempools. */\n\t\tif (dm_table_bio_based(t)) {\n\t\t\t/*\n\t\t\t * Reload bioset because front_pad may have changed\n\t\t\t * because a different table was loaded.\n\t\t\t */\n\t\t\tbioset_free(md->bs);\n\t\t\tmd->bs = p->bs;\n\t\t\tp->bs = NULL;\n\t\t}\n\t\t/*\n\t\t * There's no need to reload with request-based dm\n\t\t * because the size of front_pad doesn't change.\n\t\t * Note for future: If you are to reload bioset,\n\t\t * prep-ed requests in the queue may refer\n\t\t * to bio from the old bioset, so you must walk\n\t\t * through the queue to unprep.\n\t\t */\n\t\tgoto out;\n\t}\n\n\tBUG_ON(!p || md->io_pool || md->bs);\n\n\tmd->io_pool = p->io_pool;\n\tp->io_pool = NULL;\n\tmd->bs = p->bs;\n\tp->bs = NULL;\n\nout:\n\t/* mempool bind completed, no longer need any mempools in the table */\n\tdm_table_free_md_mempools(t);\n}\n\n/*\n * Bind a table to the device.\n */\nstatic void event_callback(void *context)\n{\n\tunsigned long flags;\n\tLIST_HEAD(uevents);\n\tstruct mapped_device *md = (struct mapped_device *) context;\n\n\tspin_lock_irqsave(&md->uevent_lock, flags);\n\tlist_splice_init(&md->uevent_list, &uevents);\n\tspin_unlock_irqrestore(&md->uevent_lock, flags);\n\n\tdm_send_uevents(&uevents, &disk_to_dev(md->disk)->kobj);\n\n\tatomic_inc(&md->event_nr);\n\twake_up(&md->eventq);\n\tdm_issue_global_event();\n}\n\n/*\n * Protected by md->suspend_lock obtained by dm_swap_table().\n */\nstatic void __set_size(struct mapped_device *md, sector_t size)\n{\n\tlockdep_assert_held(&md->suspend_lock);\n\n\tset_capacity(md->disk, size);\n\n\ti_size_write(md->bdev->bd_inode, (loff_t)size << SECTOR_SHIFT);\n}\n\n/*\n * Returns old map, which caller must destroy.\n */\nstatic struct dm_table *__bind(struct mapped_device *md, struct dm_table *t,\n\t\t\t       struct queue_limits *limits)\n{\n\tstruct dm_table *old_map;\n\tstruct request_queue *q = md->queue;\n\tsector_t size;\n\n\tlockdep_assert_held(&md->suspend_lock);\n\n\tsize = dm_table_get_size(t);\n\n\t/*\n\t * Wipe any geometry if the size of the table changed.\n\t */\n\tif (size != dm_get_size(md))\n\t\tmemset(&md->geometry, 0, sizeof(md->geometry));\n\n\t__set_size(md, size);\n\n\tdm_table_event_callback(t, event_callback, md);\n\n\t/*\n\t * The queue hasn't been stopped yet, if the old table type wasn't\n\t * for request-based during suspension.  So stop it to prevent\n\t * I/O mapping before resume.\n\t * This must be done before setting the queue restrictions,\n\t * because request-based dm may be run just after the setting.\n\t */\n\tif (dm_table_request_based(t)) {\n\t\tdm_stop_queue(q);\n\t\t/*\n\t\t * Leverage the fact that request-based DM targets are\n\t\t * immutable singletons and establish md->immutable_target\n\t\t * - used to optimize both dm_request_fn and dm_mq_queue_rq\n\t\t */\n\t\tmd->immutable_target = dm_table_get_immutable_target(t);\n\t}\n\n\t__bind_mempools(md, t);\n\n\told_map = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\trcu_assign_pointer(md->map, (void *)t);\n\tmd->immutable_target_type = dm_table_get_immutable_target_type(t);\n\n\tdm_table_set_restrictions(t, q, limits);\n\tif (old_map)\n\t\tdm_sync_table(md);\n\n\treturn old_map;\n}\n\n/*\n * Returns unbound table for the caller to free.\n */\nstatic struct dm_table *__unbind(struct mapped_device *md)\n{\n\tstruct dm_table *map = rcu_dereference_protected(md->map, 1);\n\n\tif (!map)\n\t\treturn NULL;\n\n\tdm_table_event_callback(map, NULL, NULL);\n\tRCU_INIT_POINTER(md->map, NULL);\n\tdm_sync_table(md);\n\n\treturn map;\n}\n\n/*\n * Constructor for a new device.\n */\nint dm_create(int minor, struct mapped_device **result)\n{\n\tstruct mapped_device *md;\n\n\tmd = alloc_dev(minor);\n\tif (!md)\n\t\treturn -ENXIO;\n\n\tdm_sysfs_init(md);\n\n\t*result = md;\n\treturn 0;\n}\n\n/*\n * Functions to manage md->type.\n * All are required to hold md->type_lock.\n */\nvoid dm_lock_md_type(struct mapped_device *md)\n{\n\tmutex_lock(&md->type_lock);\n}\n\nvoid dm_unlock_md_type(struct mapped_device *md)\n{\n\tmutex_unlock(&md->type_lock);\n}\n\nvoid dm_set_md_type(struct mapped_device *md, enum dm_queue_mode type)\n{\n\tBUG_ON(!mutex_is_locked(&md->type_lock));\n\tmd->type = type;\n}\n\nenum dm_queue_mode dm_get_md_type(struct mapped_device *md)\n{\n\treturn md->type;\n}\n\nstruct target_type *dm_get_immutable_target_type(struct mapped_device *md)\n{\n\treturn md->immutable_target_type;\n}\n\n/*\n * The queue_limits are only valid as long as you have a reference\n * count on 'md'.\n */\nstruct queue_limits *dm_get_queue_limits(struct mapped_device *md)\n{\n\tBUG_ON(!atomic_read(&md->holders));\n\treturn &md->queue->limits;\n}\nEXPORT_SYMBOL_GPL(dm_get_queue_limits);\n\n/*\n * Setup the DM device's queue based on md's type\n */\nint dm_setup_md_queue(struct mapped_device *md, struct dm_table *t)\n{\n\tint r;\n\tenum dm_queue_mode type = dm_get_md_type(md);\n\n\tswitch (type) {\n\tcase DM_TYPE_REQUEST_BASED:\n\t\tr = dm_old_init_request_queue(md, t);\n\t\tif (r) {\n\t\t\tDMERR(\"Cannot initialize queue for request-based mapped device\");\n\t\t\treturn r;\n\t\t}\n\t\tbreak;\n\tcase DM_TYPE_MQ_REQUEST_BASED:\n\t\tr = dm_mq_init_request_queue(md, t);\n\t\tif (r) {\n\t\t\tDMERR(\"Cannot initialize queue for request-based dm-mq mapped device\");\n\t\t\treturn r;\n\t\t}\n\t\tbreak;\n\tcase DM_TYPE_BIO_BASED:\n\tcase DM_TYPE_DAX_BIO_BASED:\n\t\tdm_init_normal_md_queue(md);\n\t\tblk_queue_make_request(md->queue, dm_make_request);\n\t\t/*\n\t\t * DM handles splitting bios as needed.  Free the bio_split bioset\n\t\t * since it won't be used (saves 1 process per bio-based DM device).\n\t\t */\n\t\tbioset_free(md->queue->bio_split);\n\t\tmd->queue->bio_split = NULL;\n\n\t\tif (type == DM_TYPE_DAX_BIO_BASED)\n\t\t\tqueue_flag_set_unlocked(QUEUE_FLAG_DAX, md->queue);\n\t\tbreak;\n\tcase DM_TYPE_NONE:\n\t\tWARN_ON_ONCE(true);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstruct mapped_device *dm_get_md(dev_t dev)\n{\n\tstruct mapped_device *md;\n\tunsigned minor = MINOR(dev);\n\n\tif (MAJOR(dev) != _major || minor >= (1 << MINORBITS))\n\t\treturn NULL;\n\n\tspin_lock(&_minor_lock);\n\n\tmd = idr_find(&_minor_idr, minor);\n\tif (md) {\n\t\tif ((md == MINOR_ALLOCED ||\n\t\t     (MINOR(disk_devt(dm_disk(md))) != minor) ||\n\t\t     dm_deleting_md(md) ||\n\t\t     test_bit(DMF_FREEING, &md->flags))) {\n\t\t\tmd = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tdm_get(md);\n\t}\n\nout:\n\tspin_unlock(&_minor_lock);\n\n\treturn md;\n}\nEXPORT_SYMBOL_GPL(dm_get_md);\n\nvoid *dm_get_mdptr(struct mapped_device *md)\n{\n\treturn md->interface_ptr;\n}\n\nvoid dm_set_mdptr(struct mapped_device *md, void *ptr)\n{\n\tmd->interface_ptr = ptr;\n}\n\nvoid dm_get(struct mapped_device *md)\n{\n\tatomic_inc(&md->holders);\n\tBUG_ON(test_bit(DMF_FREEING, &md->flags));\n}\n\nint dm_hold(struct mapped_device *md)\n{\n\tspin_lock(&_minor_lock);\n\tif (test_bit(DMF_FREEING, &md->flags)) {\n\t\tspin_unlock(&_minor_lock);\n\t\treturn -EBUSY;\n\t}\n\tdm_get(md);\n\tspin_unlock(&_minor_lock);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_hold);\n\nconst char *dm_device_name(struct mapped_device *md)\n{\n\treturn md->name;\n}\nEXPORT_SYMBOL_GPL(dm_device_name);\n\nstatic void __dm_destroy(struct mapped_device *md, bool wait)\n{\n\tstruct request_queue *q = dm_get_md_queue(md);\n\tstruct dm_table *map;\n\tint srcu_idx;\n\n\tmight_sleep();\n\n\tspin_lock(&_minor_lock);\n\tidr_replace(&_minor_idr, MINOR_ALLOCED, MINOR(disk_devt(dm_disk(md))));\n\tset_bit(DMF_FREEING, &md->flags);\n\tspin_unlock(&_minor_lock);\n\n\tblk_set_queue_dying(q);\n\n\tif (dm_request_based(md) && md->kworker_task)\n\t\tkthread_flush_worker(&md->kworker);\n\n\t/*\n\t * Take suspend_lock so that presuspend and postsuspend methods\n\t * do not race with internal suspend.\n\t */\n\tmutex_lock(&md->suspend_lock);\n\tmap = dm_get_live_table(md, &srcu_idx);\n\tif (!dm_suspended_md(md)) {\n\t\tdm_table_presuspend_targets(map);\n\t\tdm_table_postsuspend_targets(map);\n\t}\n\t/* dm_put_live_table must be before msleep, otherwise deadlock is possible */\n\tdm_put_live_table(md, srcu_idx);\n\tmutex_unlock(&md->suspend_lock);\n\n\t/*\n\t * Rare, but there may be I/O requests still going to complete,\n\t * for example.  Wait for all references to disappear.\n\t * No one should increment the reference count of the mapped_device,\n\t * after the mapped_device state becomes DMF_FREEING.\n\t */\n\tif (wait)\n\t\twhile (atomic_read(&md->holders))\n\t\t\tmsleep(1);\n\telse if (atomic_read(&md->holders))\n\t\tDMWARN(\"%s: Forcibly removing mapped_device still in use! (%d users)\",\n\t\t       dm_device_name(md), atomic_read(&md->holders));\n\n\tdm_sysfs_exit(md);\n\tdm_table_destroy(__unbind(md));\n\tfree_dev(md);\n}\n\nvoid dm_destroy(struct mapped_device *md)\n{\n\t__dm_destroy(md, true);\n}\n\nvoid dm_destroy_immediate(struct mapped_device *md)\n{\n\t__dm_destroy(md, false);\n}\n\nvoid dm_put(struct mapped_device *md)\n{\n\tatomic_dec(&md->holders);\n}\nEXPORT_SYMBOL_GPL(dm_put);\n\nstatic int dm_wait_for_completion(struct mapped_device *md, long task_state)\n{\n\tint r = 0;\n\tDEFINE_WAIT(wait);\n\n\twhile (1) {\n\t\tprepare_to_wait(&md->wait, &wait, task_state);\n\n\t\tif (!md_in_flight(md))\n\t\t\tbreak;\n\n\t\tif (signal_pending_state(task_state, current)) {\n\t\t\tr = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tio_schedule();\n\t}\n\tfinish_wait(&md->wait, &wait);\n\n\treturn r;\n}\n\n/*\n * Process the deferred bios\n */\nstatic void dm_wq_work(struct work_struct *work)\n{\n\tstruct mapped_device *md = container_of(work, struct mapped_device,\n\t\t\t\t\t\twork);\n\tstruct bio *c;\n\tint srcu_idx;\n\tstruct dm_table *map;\n\n\tmap = dm_get_live_table(md, &srcu_idx);\n\n\twhile (!test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags)) {\n\t\tspin_lock_irq(&md->deferred_lock);\n\t\tc = bio_list_pop(&md->deferred);\n\t\tspin_unlock_irq(&md->deferred_lock);\n\n\t\tif (!c)\n\t\t\tbreak;\n\n\t\tif (dm_request_based(md))\n\t\t\tgeneric_make_request(c);\n\t\telse\n\t\t\t__split_and_process_bio(md, map, c);\n\t}\n\n\tdm_put_live_table(md, srcu_idx);\n}\n\nstatic void dm_queue_flush(struct mapped_device *md)\n{\n\tclear_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);\n\tsmp_mb__after_atomic();\n\tqueue_work(md->wq, &md->work);\n}\n\n/*\n * Swap in a new table, returning the old one for the caller to destroy.\n */\nstruct dm_table *dm_swap_table(struct mapped_device *md, struct dm_table *table)\n{\n\tstruct dm_table *live_map = NULL, *map = ERR_PTR(-EINVAL);\n\tstruct queue_limits limits;\n\tint r;\n\n\tmutex_lock(&md->suspend_lock);\n\n\t/* device must be suspended */\n\tif (!dm_suspended_md(md))\n\t\tgoto out;\n\n\t/*\n\t * If the new table has no data devices, retain the existing limits.\n\t * This helps multipath with queue_if_no_path if all paths disappear,\n\t * then new I/O is queued based on these limits, and then some paths\n\t * reappear.\n\t */\n\tif (dm_table_has_no_data_devices(table)) {\n\t\tlive_map = dm_get_live_table_fast(md);\n\t\tif (live_map)\n\t\t\tlimits = md->queue->limits;\n\t\tdm_put_live_table_fast(md);\n\t}\n\n\tif (!live_map) {\n\t\tr = dm_calculate_queue_limits(table, &limits);\n\t\tif (r) {\n\t\t\tmap = ERR_PTR(r);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmap = __bind(md, table, &limits);\n\tdm_issue_global_event();\n\nout:\n\tmutex_unlock(&md->suspend_lock);\n\treturn map;\n}\n\n/*\n * Functions to lock and unlock any filesystem running on the\n * device.\n */\nstatic int lock_fs(struct mapped_device *md)\n{\n\tint r;\n\n\tWARN_ON(md->frozen_sb);\n\n\tmd->frozen_sb = freeze_bdev(md->bdev);\n\tif (IS_ERR(md->frozen_sb)) {\n\t\tr = PTR_ERR(md->frozen_sb);\n\t\tmd->frozen_sb = NULL;\n\t\treturn r;\n\t}\n\n\tset_bit(DMF_FROZEN, &md->flags);\n\n\treturn 0;\n}\n\nstatic void unlock_fs(struct mapped_device *md)\n{\n\tif (!test_bit(DMF_FROZEN, &md->flags))\n\t\treturn;\n\n\tthaw_bdev(md->bdev, md->frozen_sb);\n\tmd->frozen_sb = NULL;\n\tclear_bit(DMF_FROZEN, &md->flags);\n}\n\n/*\n * @suspend_flags: DM_SUSPEND_LOCKFS_FLAG and/or DM_SUSPEND_NOFLUSH_FLAG\n * @task_state: e.g. TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE\n * @dmf_suspended_flag: DMF_SUSPENDED or DMF_SUSPENDED_INTERNALLY\n *\n * If __dm_suspend returns 0, the device is completely quiescent\n * now. There is no request-processing activity. All new requests\n * are being added to md->deferred list.\n */\nstatic int __dm_suspend(struct mapped_device *md, struct dm_table *map,\n\t\t\tunsigned suspend_flags, long task_state,\n\t\t\tint dmf_suspended_flag)\n{\n\tbool do_lockfs = suspend_flags & DM_SUSPEND_LOCKFS_FLAG;\n\tbool noflush = suspend_flags & DM_SUSPEND_NOFLUSH_FLAG;\n\tint r;\n\n\tlockdep_assert_held(&md->suspend_lock);\n\n\t/*\n\t * DMF_NOFLUSH_SUSPENDING must be set before presuspend.\n\t * This flag is cleared before dm_suspend returns.\n\t */\n\tif (noflush)\n\t\tset_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);\n\telse\n\t\tpr_debug(\"%s: suspending with flush\\n\", dm_device_name(md));\n\n\t/*\n\t * This gets reverted if there's an error later and the targets\n\t * provide the .presuspend_undo hook.\n\t */\n\tdm_table_presuspend_targets(map);\n\n\t/*\n\t * Flush I/O to the device.\n\t * Any I/O submitted after lock_fs() may not be flushed.\n\t * noflush takes precedence over do_lockfs.\n\t * (lock_fs() flushes I/Os and waits for them to complete.)\n\t */\n\tif (!noflush && do_lockfs) {\n\t\tr = lock_fs(md);\n\t\tif (r) {\n\t\t\tdm_table_presuspend_undo_targets(map);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\t/*\n\t * Here we must make sure that no processes are submitting requests\n\t * to target drivers i.e. no one may be executing\n\t * __split_and_process_bio. This is called from dm_request and\n\t * dm_wq_work.\n\t *\n\t * To get all processes out of __split_and_process_bio in dm_request,\n\t * we take the write lock. To prevent any process from reentering\n\t * __split_and_process_bio from dm_request and quiesce the thread\n\t * (dm_wq_work), we set BMF_BLOCK_IO_FOR_SUSPEND and call\n\t * flush_workqueue(md->wq).\n\t */\n\tset_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);\n\tif (map)\n\t\tsynchronize_srcu(&md->io_barrier);\n\n\t/*\n\t * Stop md->queue before flushing md->wq in case request-based\n\t * dm defers requests to md->wq from md->queue.\n\t */\n\tif (dm_request_based(md)) {\n\t\tdm_stop_queue(md->queue);\n\t\tif (md->kworker_task)\n\t\t\tkthread_flush_worker(&md->kworker);\n\t}\n\n\tflush_workqueue(md->wq);\n\n\t/*\n\t * At this point no more requests are entering target request routines.\n\t * We call dm_wait_for_completion to wait for all existing requests\n\t * to finish.\n\t */\n\tr = dm_wait_for_completion(md, task_state);\n\tif (!r)\n\t\tset_bit(dmf_suspended_flag, &md->flags);\n\n\tif (noflush)\n\t\tclear_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);\n\tif (map)\n\t\tsynchronize_srcu(&md->io_barrier);\n\n\t/* were we interrupted ? */\n\tif (r < 0) {\n\t\tdm_queue_flush(md);\n\n\t\tif (dm_request_based(md))\n\t\t\tdm_start_queue(md->queue);\n\n\t\tunlock_fs(md);\n\t\tdm_table_presuspend_undo_targets(map);\n\t\t/* pushback list is already flushed, so skip flush */\n\t}\n\n\treturn r;\n}\n\n/*\n * We need to be able to change a mapping table under a mounted\n * filesystem.  For example we might want to move some data in\n * the background.  Before the table can be swapped with\n * dm_bind_table, dm_suspend must be called to flush any in\n * flight bios and ensure that any further io gets deferred.\n */\n/*\n * Suspend mechanism in request-based dm.\n *\n * 1. Flush all I/Os by lock_fs() if needed.\n * 2. Stop dispatching any I/O by stopping the request_queue.\n * 3. Wait for all in-flight I/Os to be completed or requeued.\n *\n * To abort suspend, start the request_queue.\n */\nint dm_suspend(struct mapped_device *md, unsigned suspend_flags)\n{\n\tstruct dm_table *map = NULL;\n\tint r = 0;\n\nretry:\n\tmutex_lock_nested(&md->suspend_lock, SINGLE_DEPTH_NESTING);\n\n\tif (dm_suspended_md(md)) {\n\t\tr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (dm_suspended_internally_md(md)) {\n\t\t/* already internally suspended, wait for internal resume */\n\t\tmutex_unlock(&md->suspend_lock);\n\t\tr = wait_on_bit(&md->flags, DMF_SUSPENDED_INTERNALLY, TASK_INTERRUPTIBLE);\n\t\tif (r)\n\t\t\treturn r;\n\t\tgoto retry;\n\t}\n\n\tmap = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\n\tr = __dm_suspend(md, map, suspend_flags, TASK_INTERRUPTIBLE, DMF_SUSPENDED);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tdm_table_postsuspend_targets(map);\n\nout_unlock:\n\tmutex_unlock(&md->suspend_lock);\n\treturn r;\n}\n\nstatic int __dm_resume(struct mapped_device *md, struct dm_table *map)\n{\n\tif (map) {\n\t\tint r = dm_table_resume_targets(map);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tdm_queue_flush(md);\n\n\t/*\n\t * Flushing deferred I/Os must be done after targets are resumed\n\t * so that mapping of targets can work correctly.\n\t * Request-based dm is queueing the deferred I/Os in its request_queue.\n\t */\n\tif (dm_request_based(md))\n\t\tdm_start_queue(md->queue);\n\n\tunlock_fs(md);\n\n\treturn 0;\n}\n\nint dm_resume(struct mapped_device *md)\n{\n\tint r;\n\tstruct dm_table *map = NULL;\n\nretry:\n\tr = -EINVAL;\n\tmutex_lock_nested(&md->suspend_lock, SINGLE_DEPTH_NESTING);\n\n\tif (!dm_suspended_md(md))\n\t\tgoto out;\n\n\tif (dm_suspended_internally_md(md)) {\n\t\t/* already internally suspended, wait for internal resume */\n\t\tmutex_unlock(&md->suspend_lock);\n\t\tr = wait_on_bit(&md->flags, DMF_SUSPENDED_INTERNALLY, TASK_INTERRUPTIBLE);\n\t\tif (r)\n\t\t\treturn r;\n\t\tgoto retry;\n\t}\n\n\tmap = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\tif (!map || !dm_table_get_size(map))\n\t\tgoto out;\n\n\tr = __dm_resume(md, map);\n\tif (r)\n\t\tgoto out;\n\n\tclear_bit(DMF_SUSPENDED, &md->flags);\nout:\n\tmutex_unlock(&md->suspend_lock);\n\n\treturn r;\n}\n\n/*\n * Internal suspend/resume works like userspace-driven suspend. It waits\n * until all bios finish and prevents issuing new bios to the target drivers.\n * It may be used only from the kernel.\n */\n\nstatic void __dm_internal_suspend(struct mapped_device *md, unsigned suspend_flags)\n{\n\tstruct dm_table *map = NULL;\n\n\tlockdep_assert_held(&md->suspend_lock);\n\n\tif (md->internal_suspend_count++)\n\t\treturn; /* nested internal suspend */\n\n\tif (dm_suspended_md(md)) {\n\t\tset_bit(DMF_SUSPENDED_INTERNALLY, &md->flags);\n\t\treturn; /* nest suspend */\n\t}\n\n\tmap = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\n\t/*\n\t * Using TASK_UNINTERRUPTIBLE because only NOFLUSH internal suspend is\n\t * supported.  Properly supporting a TASK_INTERRUPTIBLE internal suspend\n\t * would require changing .presuspend to return an error -- avoid this\n\t * until there is a need for more elaborate variants of internal suspend.\n\t */\n\t(void) __dm_suspend(md, map, suspend_flags, TASK_UNINTERRUPTIBLE,\n\t\t\t    DMF_SUSPENDED_INTERNALLY);\n\n\tdm_table_postsuspend_targets(map);\n}\n\nstatic void __dm_internal_resume(struct mapped_device *md)\n{\n\tBUG_ON(!md->internal_suspend_count);\n\n\tif (--md->internal_suspend_count)\n\t\treturn; /* resume from nested internal suspend */\n\n\tif (dm_suspended_md(md))\n\t\tgoto done; /* resume from nested suspend */\n\n\t/*\n\t * NOTE: existing callers don't need to call dm_table_resume_targets\n\t * (which may fail -- so best to avoid it for now by passing NULL map)\n\t */\n\t(void) __dm_resume(md, NULL);\n\ndone:\n\tclear_bit(DMF_SUSPENDED_INTERNALLY, &md->flags);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&md->flags, DMF_SUSPENDED_INTERNALLY);\n}\n\nvoid dm_internal_suspend_noflush(struct mapped_device *md)\n{\n\tmutex_lock(&md->suspend_lock);\n\t__dm_internal_suspend(md, DM_SUSPEND_NOFLUSH_FLAG);\n\tmutex_unlock(&md->suspend_lock);\n}\nEXPORT_SYMBOL_GPL(dm_internal_suspend_noflush);\n\nvoid dm_internal_resume(struct mapped_device *md)\n{\n\tmutex_lock(&md->suspend_lock);\n\t__dm_internal_resume(md);\n\tmutex_unlock(&md->suspend_lock);\n}\nEXPORT_SYMBOL_GPL(dm_internal_resume);\n\n/*\n * Fast variants of internal suspend/resume hold md->suspend_lock,\n * which prevents interaction with userspace-driven suspend.\n */\n\nvoid dm_internal_suspend_fast(struct mapped_device *md)\n{\n\tmutex_lock(&md->suspend_lock);\n\tif (dm_suspended_md(md) || dm_suspended_internally_md(md))\n\t\treturn;\n\n\tset_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);\n\tsynchronize_srcu(&md->io_barrier);\n\tflush_workqueue(md->wq);\n\tdm_wait_for_completion(md, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL_GPL(dm_internal_suspend_fast);\n\nvoid dm_internal_resume_fast(struct mapped_device *md)\n{\n\tif (dm_suspended_md(md) || dm_suspended_internally_md(md))\n\t\tgoto done;\n\n\tdm_queue_flush(md);\n\ndone:\n\tmutex_unlock(&md->suspend_lock);\n}\nEXPORT_SYMBOL_GPL(dm_internal_resume_fast);\n\n/*-----------------------------------------------------------------\n * Event notification.\n *---------------------------------------------------------------*/\nint dm_kobject_uevent(struct mapped_device *md, enum kobject_action action,\n\t\t       unsigned cookie)\n{\n\tchar udev_cookie[DM_COOKIE_LENGTH];\n\tchar *envp[] = { udev_cookie, NULL };\n\n\tif (!cookie)\n\t\treturn kobject_uevent(&disk_to_dev(md->disk)->kobj, action);\n\telse {\n\t\tsnprintf(udev_cookie, DM_COOKIE_LENGTH, \"%s=%u\",\n\t\t\t DM_COOKIE_ENV_VAR_NAME, cookie);\n\t\treturn kobject_uevent_env(&disk_to_dev(md->disk)->kobj,\n\t\t\t\t\t  action, envp);\n\t}\n}\n\nuint32_t dm_next_uevent_seq(struct mapped_device *md)\n{\n\treturn atomic_add_return(1, &md->uevent_seq);\n}\n\nuint32_t dm_get_event_nr(struct mapped_device *md)\n{\n\treturn atomic_read(&md->event_nr);\n}\n\nint dm_wait_event(struct mapped_device *md, int event_nr)\n{\n\treturn wait_event_interruptible(md->eventq,\n\t\t\t(event_nr != atomic_read(&md->event_nr)));\n}\n\nvoid dm_uevent_add(struct mapped_device *md, struct list_head *elist)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md->uevent_lock, flags);\n\tlist_add(elist, &md->uevent_list);\n\tspin_unlock_irqrestore(&md->uevent_lock, flags);\n}\n\n/*\n * The gendisk is only valid as long as you have a reference\n * count on 'md'.\n */\nstruct gendisk *dm_disk(struct mapped_device *md)\n{\n\treturn md->disk;\n}\nEXPORT_SYMBOL_GPL(dm_disk);\n\nstruct kobject *dm_kobject(struct mapped_device *md)\n{\n\treturn &md->kobj_holder.kobj;\n}\n\nstruct mapped_device *dm_get_from_kobject(struct kobject *kobj)\n{\n\tstruct mapped_device *md;\n\n\tmd = container_of(kobj, struct mapped_device, kobj_holder.kobj);\n\n\tspin_lock(&_minor_lock);\n\tif (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {\n\t\tmd = NULL;\n\t\tgoto out;\n\t}\n\tdm_get(md);\nout:\n\tspin_unlock(&_minor_lock);\n\n\treturn md;\n}\n\nint dm_suspended_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_SUSPENDED, &md->flags);\n}\n\nint dm_suspended_internally_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_SUSPENDED_INTERNALLY, &md->flags);\n}\n\nint dm_test_deferred_remove_flag(struct mapped_device *md)\n{\n\treturn test_bit(DMF_DEFERRED_REMOVE, &md->flags);\n}\n\nint dm_suspended(struct dm_target *ti)\n{\n\treturn dm_suspended_md(dm_table_get_md(ti->table));\n}\nEXPORT_SYMBOL_GPL(dm_suspended);\n\nint dm_noflush_suspending(struct dm_target *ti)\n{\n\treturn __noflush_suspending(dm_table_get_md(ti->table));\n}\nEXPORT_SYMBOL_GPL(dm_noflush_suspending);\n\nstruct dm_md_mempools *dm_alloc_md_mempools(struct mapped_device *md, enum dm_queue_mode type,\n\t\t\t\t\t    unsigned integrity, unsigned per_io_data_size)\n{\n\tstruct dm_md_mempools *pools = kzalloc_node(sizeof(*pools), GFP_KERNEL, md->numa_node_id);\n\tunsigned int pool_size = 0;\n\tunsigned int front_pad;\n\n\tif (!pools)\n\t\treturn NULL;\n\n\tswitch (type) {\n\tcase DM_TYPE_BIO_BASED:\n\tcase DM_TYPE_DAX_BIO_BASED:\n\t\tpool_size = dm_get_reserved_bio_based_ios();\n\t\tfront_pad = roundup(per_io_data_size, __alignof__(struct dm_target_io)) + offsetof(struct dm_target_io, clone);\n\t\n\t\tpools->io_pool = mempool_create_slab_pool(pool_size, _io_cache);\n\t\tif (!pools->io_pool)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase DM_TYPE_REQUEST_BASED:\n\tcase DM_TYPE_MQ_REQUEST_BASED:\n\t\tpool_size = dm_get_reserved_rq_based_ios();\n\t\tfront_pad = offsetof(struct dm_rq_clone_bio_info, clone);\n\t\t/* per_io_data_size is used for blk-mq pdu at queue allocation */\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tpools->bs = bioset_create(pool_size, front_pad, BIOSET_NEED_RESCUER);\n\tif (!pools->bs)\n\t\tgoto out;\n\n\tif (integrity && bioset_integrity_create(pools->bs, pool_size))\n\t\tgoto out;\n\n\treturn pools;\n\nout:\n\tdm_free_md_mempools(pools);\n\n\treturn NULL;\n}\n\nvoid dm_free_md_mempools(struct dm_md_mempools *pools)\n{\n\tif (!pools)\n\t\treturn;\n\n\tmempool_destroy(pools->io_pool);\n\n\tif (pools->bs)\n\t\tbioset_free(pools->bs);\n\n\tkfree(pools);\n}\n\nstruct dm_pr {\n\tu64\told_key;\n\tu64\tnew_key;\n\tu32\tflags;\n\tbool\tfail_early;\n};\n\nstatic int dm_call_pr(struct block_device *bdev, iterate_devices_callout_fn fn,\n\t\t      void *data)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tstruct dm_table *table;\n\tstruct dm_target *ti;\n\tint ret = -ENOTTY, srcu_idx;\n\n\ttable = dm_get_live_table(md, &srcu_idx);\n\tif (!table || !dm_table_get_size(table))\n\t\tgoto out;\n\n\t/* We only support devices that have a single target */\n\tif (dm_table_get_num_targets(table) != 1)\n\t\tgoto out;\n\tti = dm_table_get_target(table, 0);\n\n\tret = -EINVAL;\n\tif (!ti->type->iterate_devices)\n\t\tgoto out;\n\n\tret = ti->type->iterate_devices(ti, fn, data);\nout:\n\tdm_put_live_table(md, srcu_idx);\n\treturn ret;\n}\n\n/*\n * For register / unregister we need to manually call out to every path.\n */\nstatic int __dm_pr_register(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t    sector_t start, sector_t len, void *data)\n{\n\tstruct dm_pr *pr = data;\n\tconst struct pr_ops *ops = dev->bdev->bd_disk->fops->pr_ops;\n\n\tif (!ops || !ops->pr_register)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->pr_register(dev->bdev, pr->old_key, pr->new_key, pr->flags);\n}\n\nstatic int dm_pr_register(struct block_device *bdev, u64 old_key, u64 new_key,\n\t\t\t  u32 flags)\n{\n\tstruct dm_pr pr = {\n\t\t.old_key\t= old_key,\n\t\t.new_key\t= new_key,\n\t\t.flags\t\t= flags,\n\t\t.fail_early\t= true,\n\t};\n\tint ret;\n\n\tret = dm_call_pr(bdev, __dm_pr_register, &pr);\n\tif (ret && new_key) {\n\t\t/* unregister all paths if we failed to register any path */\n\t\tpr.old_key = new_key;\n\t\tpr.new_key = 0;\n\t\tpr.flags = 0;\n\t\tpr.fail_early = false;\n\t\tdm_call_pr(bdev, __dm_pr_register, &pr);\n\t}\n\n\treturn ret;\n}\n\nstatic int dm_pr_reserve(struct block_device *bdev, u64 key, enum pr_type type,\n\t\t\t u32 flags)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tconst struct pr_ops *ops;\n\tfmode_t mode;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tops = bdev->bd_disk->fops->pr_ops;\n\tif (ops && ops->pr_reserve)\n\t\tr = ops->pr_reserve(bdev, key, type, flags);\n\telse\n\t\tr = -EOPNOTSUPP;\n\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic int dm_pr_release(struct block_device *bdev, u64 key, enum pr_type type)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tconst struct pr_ops *ops;\n\tfmode_t mode;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tops = bdev->bd_disk->fops->pr_ops;\n\tif (ops && ops->pr_release)\n\t\tr = ops->pr_release(bdev, key, type);\n\telse\n\t\tr = -EOPNOTSUPP;\n\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic int dm_pr_preempt(struct block_device *bdev, u64 old_key, u64 new_key,\n\t\t\t enum pr_type type, bool abort)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tconst struct pr_ops *ops;\n\tfmode_t mode;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tops = bdev->bd_disk->fops->pr_ops;\n\tif (ops && ops->pr_preempt)\n\t\tr = ops->pr_preempt(bdev, old_key, new_key, type, abort);\n\telse\n\t\tr = -EOPNOTSUPP;\n\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic int dm_pr_clear(struct block_device *bdev, u64 key)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tconst struct pr_ops *ops;\n\tfmode_t mode;\n\tint r;\n\n\tr = dm_grab_bdev_for_ioctl(md, &bdev, &mode);\n\tif (r < 0)\n\t\treturn r;\n\n\tops = bdev->bd_disk->fops->pr_ops;\n\tif (ops && ops->pr_clear)\n\t\tr = ops->pr_clear(bdev, key);\n\telse\n\t\tr = -EOPNOTSUPP;\n\n\tbdput(bdev);\n\treturn r;\n}\n\nstatic const struct pr_ops dm_pr_ops = {\n\t.pr_register\t= dm_pr_register,\n\t.pr_reserve\t= dm_pr_reserve,\n\t.pr_release\t= dm_pr_release,\n\t.pr_preempt\t= dm_pr_preempt,\n\t.pr_clear\t= dm_pr_clear,\n};\n\nstatic const struct block_device_operations dm_blk_dops = {\n\t.open = dm_blk_open,\n\t.release = dm_blk_close,\n\t.ioctl = dm_blk_ioctl,\n\t.getgeo = dm_blk_getgeo,\n\t.pr_ops = &dm_pr_ops,\n\t.owner = THIS_MODULE\n};\n\nstatic const struct dax_operations dm_dax_ops = {\n\t.direct_access = dm_dax_direct_access,\n\t.copy_from_iter = dm_dax_copy_from_iter,\n};\n\n/*\n * module hooks\n */\nmodule_init(dm_init);\nmodule_exit(dm_exit);\n\nmodule_param(major, uint, 0);\nMODULE_PARM_DESC(major, \"The major number of the device mapper\");\n\nmodule_param(reserved_bio_based_ios, uint, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(reserved_bio_based_ios, \"Reserved IOs in bio-based mempools\");\n\nmodule_param(dm_numa_node, int, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(dm_numa_node, \"NUMA node for DM device memory allocations\");\n\nMODULE_DESCRIPTION(DM_NAME \" driver\");\nMODULE_AUTHOR(\"Joe Thornber <dm-devel@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n"], "filenames": ["drivers/md/dm.c"], "buggy_code_start_loc": [2714], "buggy_code_end_loc": [2718], "fixing_code_start_loc": [2714], "fixing_code_end_loc": [2723], "type": "CWE-362", "message": "The dm_get_from_kobject function in drivers/md/dm.c in the Linux kernel before 4.14.3 allow local users to cause a denial of service (BUG) by leveraging a race condition with __dm_destroy during creation and removal of DM devices.", "other": {"cve": {"id": "CVE-2017-18203", "sourceIdentifier": "cve@mitre.org", "published": "2018-02-27T20:29:00.260", "lastModified": "2018-06-20T01:29:00.883", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The dm_get_from_kobject function in drivers/md/dm.c in the Linux kernel before 4.14.3 allow local users to cause a denial of service (BUG) by leveraging a race condition with __dm_destroy during creation and removal of DM devices."}, {"lang": "es", "value": "La funci\u00f3n dm_get_from_kobject en drivers/md/dm.c en el kernel de Linux, en versiones anteriores a la 4.14.3, permite que usuarios locales provoquen una denegaci\u00f3n de servicio (bug) aprovechando una condici\u00f3n de carrera en __dm_destroy durante la creaci\u00f3n y eliminaci\u00f3n de dispositivos DM."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 1.9}, "baseSeverity": "LOW", "exploitabilityScore": 3.4, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.14.3", "matchCriteriaId": "9F3C9534-2349-43A8-A1B3-E683DCE46BE8"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b9a41d21dceadf8104812626ef85dc56ee8a60ed", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/103184", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:0676", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2018:1062", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2018:1854", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2019:4154", "source": "cve@mitre.org"}, {"url": "https://github.com/torvalds/linux/commit/b9a41d21dceadf8104812626ef85dc56ee8a60ed", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2018/05/msg00000.html", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3619-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3619-2/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3653-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3653-2/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3655-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3655-2/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3657-1/", "source": "cve@mitre.org"}, {"url": "https://www.debian.org/security/2018/dsa-4187", "source": "cve@mitre.org"}, {"url": "https://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.14.3", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/b9a41d21dceadf8104812626ef85dc56ee8a60ed"}}