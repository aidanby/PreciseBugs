{"buggy_code": ["#ifndef _LINUX_ERRQUEUE_H\n#define _LINUX_ERRQUEUE_H 1\n\n\n#include <net/ip.h>\n#if IS_ENABLED(CONFIG_IPV6)\n#include <linux/ipv6.h>\n#endif\n#include <uapi/linux/errqueue.h>\n\n#define SKB_EXT_ERR(skb) ((struct sock_exterr_skb *) ((skb)->cb))\n\nstruct sock_exterr_skb {\n\tunion {\n\t\tstruct inet_skb_parm\th4;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tstruct inet6_skb_parm\th6;\n#endif\n\t} header;\n\tstruct sock_extended_err\tee;\n\tu16\t\t\t\taddr_offset;\n\t__be16\t\t\t\tport;\n};\n\n#endif\n", "/*\n *\tRoutines having to do with the 'struct sk_buff' memory handlers.\n *\n *\tAuthors:\tAlan Cox <alan@lxorguk.ukuu.org.uk>\n *\t\t\tFlorian La Roche <rzsfl@rz.uni-sb.de>\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tFixed the worst of the load\n *\t\t\t\t\tbalancer bugs.\n *\t\tDave Platt\t:\tInterrupt stacking fix.\n *\tRichard Kooijman\t:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tChanged buffer format.\n *\t\tAlan Cox\t:\tdestructor hook for AF_UNIX etc.\n *\t\tLinus Torvalds\t:\tBetter skb_clone.\n *\t\tAlan Cox\t:\tAdded skb_copy.\n *\t\tAlan Cox\t:\tAdded all the changed routines Linus\n *\t\t\t\t\tonly put in the headers\n *\t\tRay VanTassle\t:\tFixed --skb->lock in free\n *\t\tAlan Cox\t:\tskb_copy copy arp field\n *\t\tAndi Kleen\t:\tslabified it.\n *\t\tRobert Olsson\t:\tRemoved skb_head_pool\n *\n *\tNOTE:\n *\t\tThe __skb_ routines should be called with interrupts\n *\tdisabled, or you better be *real* sure that the operation is atomic\n *\twith respect to whatever list is being frobbed (e.g. via lock_sock()\n *\tor via disabling bottom half handlers, etc).\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n/*\n *\tThe functions in this file will not compile correctly with gcc 2.4.x\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/kmemcheck.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/slab.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/sctp.h>\n#include <linux/netdevice.h>\n#ifdef CONFIG_NET_CLS_ACT\n#include <net/pkt_sched.h>\n#endif\n#include <linux/string.h>\n#include <linux/skbuff.h>\n#include <linux/splice.h>\n#include <linux/cache.h>\n#include <linux/rtnetlink.h>\n#include <linux/init.h>\n#include <linux/scatterlist.h>\n#include <linux/errqueue.h>\n#include <linux/prefetch.h>\n#include <linux/if_vlan.h>\n\n#include <net/protocol.h>\n#include <net/dst.h>\n#include <net/sock.h>\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n\n#include <linux/uaccess.h>\n#include <trace/events/skb.h>\n#include <linux/highmem.h>\n#include <linux/capability.h>\n#include <linux/user_namespace.h>\n\nstruct kmem_cache *skbuff_head_cache __read_mostly;\nstatic struct kmem_cache *skbuff_fclone_cache __read_mostly;\nint sysctl_max_skb_frags __read_mostly = MAX_SKB_FRAGS;\nEXPORT_SYMBOL(sysctl_max_skb_frags);\n\n/**\n *\tskb_panic - private function for out-of-line support\n *\t@skb:\tbuffer\n *\t@sz:\tsize\n *\t@addr:\taddress\n *\t@msg:\tskb_over_panic or skb_under_panic\n *\n *\tOut-of-line support for skb_put() and skb_push().\n *\tCalled via the wrapper skb_over_panic() or skb_under_panic().\n *\tKeep out of line to prevent kernel bloat.\n *\t__builtin_return_address is not used because it is not always reliable.\n */\nstatic void skb_panic(struct sk_buff *skb, unsigned int sz, void *addr,\n\t\t      const char msg[])\n{\n\tpr_emerg(\"%s: text:%p len:%d put:%d head:%p data:%p tail:%#lx end:%#lx dev:%s\\n\",\n\t\t msg, addr, skb->len, sz, skb->head, skb->data,\n\t\t (unsigned long)skb->tail, (unsigned long)skb->end,\n\t\t skb->dev ? skb->dev->name : \"<NULL>\");\n\tBUG();\n}\n\nstatic void skb_over_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\nstatic void skb_under_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\n/*\n * kmalloc_reserve is a wrapper around kmalloc_node_track_caller that tells\n * the caller if emergency pfmemalloc reserves are being used. If it is and\n * the socket is later found to be SOCK_MEMALLOC then PFMEMALLOC reserves\n * may be used. Otherwise, the packet data may be discarded until enough\n * memory is free\n */\n#define kmalloc_reserve(size, gfp, node, pfmemalloc) \\\n\t __kmalloc_reserve(size, gfp, node, _RET_IP_, pfmemalloc)\n\nstatic void *__kmalloc_reserve(size_t size, gfp_t flags, int node,\n\t\t\t       unsigned long ip, bool *pfmemalloc)\n{\n\tvoid *obj;\n\tbool ret_pfmemalloc = false;\n\n\t/*\n\t * Try a regular allocation, when that fails and we're not entitled\n\t * to the reserves, fail.\n\t */\n\tobj = kmalloc_node_track_caller(size,\n\t\t\t\t\tflags | __GFP_NOMEMALLOC | __GFP_NOWARN,\n\t\t\t\t\tnode);\n\tif (obj || !(gfp_pfmemalloc_allowed(flags)))\n\t\tgoto out;\n\n\t/* Try again but now we are using pfmemalloc reserves */\n\tret_pfmemalloc = true;\n\tobj = kmalloc_node_track_caller(size, flags, node);\n\nout:\n\tif (pfmemalloc)\n\t\t*pfmemalloc = ret_pfmemalloc;\n\n\treturn obj;\n}\n\n/* \tAllocate a new skbuff. We do this ourselves so we can fill in a few\n *\t'private' fields and also do memory statistics to find all the\n *\t[BEEP] leaks.\n *\n */\n\nstruct sk_buff *__alloc_skb_head(gfp_t gfp_mask, int node)\n{\n\tstruct sk_buff *skb;\n\n\t/* Get the HEAD */\n\tskb = kmem_cache_alloc_node(skbuff_head_cache,\n\t\t\t\t    gfp_mask & ~__GFP_DMA, node);\n\tif (!skb)\n\t\tgoto out;\n\n\t/*\n\t * Only clear those fields we need to clear, not those that we will\n\t * actually initialise below. Hence, don't put any more fields after\n\t * the tail pointer in struct sk_buff!\n\t */\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->head = NULL;\n\tskb->truesize = sizeof(struct sk_buff);\n\tatomic_set(&skb->users, 1);\n\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\nout:\n\treturn skb;\n}\n\n/**\n *\t__alloc_skb\t-\tallocate a network buffer\n *\t@size: size to allocate\n *\t@gfp_mask: allocation mask\n *\t@flags: If SKB_ALLOC_FCLONE is set, allocate from fclone cache\n *\t\tinstead of head cache and allocate a cloned (child) skb.\n *\t\tIf SKB_ALLOC_RX is set, __GFP_MEMALLOC will be used for\n *\t\tallocations in case the data is required for writeback\n *\t@node: numa node to allocate memory on\n *\n *\tAllocate a new &sk_buff. The returned buffer has no headroom and a\n *\ttail room of at least size bytes. The object has a reference count\n *\tof one. The return is the buffer. On a failure the return is %NULL.\n *\n *\tBuffers may only be allocated from interrupts using a @gfp_mask of\n *\t%GFP_ATOMIC.\n */\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,\n\t\t\t    int flags, int node)\n{\n\tstruct kmem_cache *cache;\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tu8 *data;\n\tbool pfmemalloc;\n\n\tcache = (flags & SKB_ALLOC_FCLONE)\n\t\t? skbuff_fclone_cache : skbuff_head_cache;\n\n\tif (sk_memalloc_socks() && (flags & SKB_ALLOC_RX))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t/* Get the HEAD */\n\tskb = kmem_cache_alloc_node(cache, gfp_mask & ~__GFP_DMA, node);\n\tif (!skb)\n\t\tgoto out;\n\tprefetchw(skb);\n\n\t/* We do our best to align skb_shared_info on a separate cache\n\t * line. It usually works because kmalloc(X > SMP_CACHE_BYTES) gives\n\t * aligned memory blocks, unless SLUB/SLAB debug is enabled.\n\t * Both skb->head and skb_shared_info are cache line aligned.\n\t */\n\tsize = SKB_DATA_ALIGN(size);\n\tsize += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tdata = kmalloc_reserve(size, gfp_mask, node, &pfmemalloc);\n\tif (!data)\n\t\tgoto nodata;\n\t/* kmalloc(size) might give us more room than requested.\n\t * Put skb_shared_info exactly at the end of allocated zone,\n\t * to allow max possible filling before reallocation.\n\t */\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\tprefetchw(data + size);\n\n\t/*\n\t * Only clear those fields we need to clear, not those that we will\n\t * actually initialise below. Hence, don't put any more fields after\n\t * the tail pointer in struct sk_buff!\n\t */\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t/* Account for allocated memory : skb + skb->head */\n\tskb->truesize = SKB_TRUESIZE(size);\n\tskb->pfmemalloc = pfmemalloc;\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\tif (flags & SKB_ALLOC_FCLONE) {\n\t\tstruct sk_buff_fclones *fclones;\n\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\t\tkmemcheck_annotate_bitfield(&fclones->skb2, flags1);\n\t\tskb->fclone = SKB_FCLONE_ORIG;\n\t\tatomic_set(&fclones->fclone_ref, 1);\n\n\t\tfclones->skb2.fclone = SKB_FCLONE_CLONE;\n\t}\nout:\n\treturn skb;\nnodata:\n\tkmem_cache_free(cache, skb);\n\tskb = NULL;\n\tgoto out;\n}\nEXPORT_SYMBOL(__alloc_skb);\n\n/**\n * __build_skb - build a network buffer\n * @data: data buffer provided by caller\n * @frag_size: size of data, or 0 if head was kmalloced\n *\n * Allocate a new &sk_buff. Caller provides space holding head and\n * skb_shared_info. @data must have been allocated by kmalloc() only if\n * @frag_size is 0, otherwise data should come from the page allocator\n *  or vmalloc()\n * The return is the new skb buffer.\n * On a failure the return is %NULL, and @data is not freed.\n * Notes :\n *  Before IO, driver allocates only data buffer where NIC put incoming frame\n *  Driver should add room at head (NET_SKB_PAD) and\n *  MUST add room at tail (SKB_DATA_ALIGN(skb_shared_info))\n *  After IO, driver calls build_skb(), to allocate sk_buff and populate it\n *  before giving packet to stack.\n *  RX rings only contains data buffers, not full skbs.\n */\nstruct sk_buff *__build_skb(void *data, unsigned int frag_size)\n{\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tunsigned int size = frag_size ? : ksize(data);\n\n\tskb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\tsize -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->truesize = SKB_TRUESIZE(size);\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\treturn skb;\n}\n\n/* build_skb() is wrapper over __build_skb(), that specifically\n * takes care of skb->head and skb->pfmemalloc\n * This means that if @frag_size is not zero, then @data must be backed\n * by a page fragment, not kmalloc() or vmalloc()\n */\nstruct sk_buff *build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb = __build_skb(data, frag_size);\n\n\tif (skb && frag_size) {\n\t\tskb->head_frag = 1;\n\t\tif (page_is_pfmemalloc(virt_to_head_page(data)))\n\t\t\tskb->pfmemalloc = 1;\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(build_skb);\n\n#define NAPI_SKB_CACHE_SIZE\t64\n\nstruct napi_alloc_cache {\n\tstruct page_frag_cache page;\n\tunsigned int skb_count;\n\tvoid *skb_cache[NAPI_SKB_CACHE_SIZE];\n};\n\nstatic DEFINE_PER_CPU(struct page_frag_cache, netdev_alloc_cache);\nstatic DEFINE_PER_CPU(struct napi_alloc_cache, napi_alloc_cache);\n\nstatic void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)\n{\n\tstruct page_frag_cache *nc;\n\tunsigned long flags;\n\tvoid *data;\n\n\tlocal_irq_save(flags);\n\tnc = this_cpu_ptr(&netdev_alloc_cache);\n\tdata = page_frag_alloc(nc, fragsz, gfp_mask);\n\tlocal_irq_restore(flags);\n\treturn data;\n}\n\n/**\n * netdev_alloc_frag - allocate a page fragment\n * @fragsz: fragment size\n *\n * Allocates a frag from a page for receive buffer.\n * Uses GFP_ATOMIC allocations.\n */\nvoid *netdev_alloc_frag(unsigned int fragsz)\n{\n\treturn __netdev_alloc_frag(fragsz, GFP_ATOMIC | __GFP_COLD);\n}\nEXPORT_SYMBOL(netdev_alloc_frag);\n\nstatic void *__napi_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\treturn page_frag_alloc(&nc->page, fragsz, gfp_mask);\n}\n\nvoid *napi_alloc_frag(unsigned int fragsz)\n{\n\treturn __napi_alloc_frag(fragsz, GFP_ATOMIC | __GFP_COLD);\n}\nEXPORT_SYMBOL(napi_alloc_frag);\n\n/**\n *\t__netdev_alloc_skb - allocate an skbuff for rx on a specific device\n *\t@dev: network device to receive on\n *\t@len: length to allocate\n *\t@gfp_mask: get_free_pages mask, passed to alloc_skb\n *\n *\tAllocate a new &sk_buff and assign it a usage count of one. The\n *\tbuffer has NET_SKB_PAD headroom built in. Users should allocate\n *\tthe headroom they think they need without accounting for the\n *\tbuilt in space. The built in space is used for optimisations.\n *\n *\t%NULL is returned if there is no free memory.\n */\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,\n\t\t\t\t   gfp_t gfp_mask)\n{\n\tstruct page_frag_cache *nc;\n\tunsigned long flags;\n\tstruct sk_buff *skb;\n\tbool pfmemalloc;\n\tvoid *data;\n\n\tlen += NET_SKB_PAD;\n\n\tif ((len > SKB_WITH_OVERHEAD(PAGE_SIZE)) ||\n\t    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA))) {\n\t\tskb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);\n\t\tif (!skb)\n\t\t\tgoto skb_fail;\n\t\tgoto skb_success;\n\t}\n\n\tlen += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tlen = SKB_DATA_ALIGN(len);\n\n\tif (sk_memalloc_socks())\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tlocal_irq_save(flags);\n\n\tnc = this_cpu_ptr(&netdev_alloc_cache);\n\tdata = page_frag_alloc(nc, len, gfp_mask);\n\tpfmemalloc = nc->pfmemalloc;\n\n\tlocal_irq_restore(flags);\n\n\tif (unlikely(!data))\n\t\treturn NULL;\n\n\tskb = __build_skb(data, len);\n\tif (unlikely(!skb)) {\n\t\tskb_free_frag(data);\n\t\treturn NULL;\n\t}\n\n\t/* use OR instead of assignment to avoid clearing of bits in mask */\n\tif (pfmemalloc)\n\t\tskb->pfmemalloc = 1;\n\tskb->head_frag = 1;\n\nskb_success:\n\tskb_reserve(skb, NET_SKB_PAD);\n\tskb->dev = dev;\n\nskb_fail:\n\treturn skb;\n}\nEXPORT_SYMBOL(__netdev_alloc_skb);\n\n/**\n *\t__napi_alloc_skb - allocate skbuff for rx in a specific NAPI instance\n *\t@napi: napi instance this buffer was allocated for\n *\t@len: length to allocate\n *\t@gfp_mask: get_free_pages mask, passed to alloc_skb and alloc_pages\n *\n *\tAllocate a new sk_buff for use in NAPI receive.  This buffer will\n *\tattempt to allocate the head from a special reserved region used\n *\tonly for NAPI Rx allocation.  By doing this we can save several\n *\tCPU cycles by avoiding having to disable and re-enable IRQs.\n *\n *\t%NULL is returned if there is no free memory.\n */\nstruct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,\n\t\t\t\t gfp_t gfp_mask)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\tstruct sk_buff *skb;\n\tvoid *data;\n\n\tlen += NET_SKB_PAD + NET_IP_ALIGN;\n\n\tif ((len > SKB_WITH_OVERHEAD(PAGE_SIZE)) ||\n\t    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA))) {\n\t\tskb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);\n\t\tif (!skb)\n\t\t\tgoto skb_fail;\n\t\tgoto skb_success;\n\t}\n\n\tlen += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tlen = SKB_DATA_ALIGN(len);\n\n\tif (sk_memalloc_socks())\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tdata = page_frag_alloc(&nc->page, len, gfp_mask);\n\tif (unlikely(!data))\n\t\treturn NULL;\n\n\tskb = __build_skb(data, len);\n\tif (unlikely(!skb)) {\n\t\tskb_free_frag(data);\n\t\treturn NULL;\n\t}\n\n\t/* use OR instead of assignment to avoid clearing of bits in mask */\n\tif (nc->page.pfmemalloc)\n\t\tskb->pfmemalloc = 1;\n\tskb->head_frag = 1;\n\nskb_success:\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\n\tskb->dev = napi->dev;\n\nskb_fail:\n\treturn skb;\n}\nEXPORT_SYMBOL(__napi_alloc_skb);\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize)\n{\n\tskb_fill_page_desc(skb, i, page, off, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_add_rx_frag);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\tskb_frag_size_add(frag, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_coalesce_rx_frag);\n\nstatic void skb_drop_list(struct sk_buff **listp)\n{\n\tkfree_skb_list(*listp);\n\t*listp = NULL;\n}\n\nstatic inline void skb_drop_fraglist(struct sk_buff *skb)\n{\n\tskb_drop_list(&skb_shinfo(skb)->frag_list);\n}\n\nstatic void skb_clone_fraglist(struct sk_buff *skb)\n{\n\tstruct sk_buff *list;\n\n\tskb_walk_frags(skb, list)\n\t\tskb_get(list);\n}\n\nstatic void skb_free_head(struct sk_buff *skb)\n{\n\tunsigned char *head = skb->head;\n\n\tif (skb->head_frag)\n\t\tskb_free_frag(head);\n\telse\n\t\tkfree(head);\n}\n\nstatic void skb_release_data(struct sk_buff *skb)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint i;\n\n\tif (skb->cloned &&\n\t    atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,\n\t\t\t      &shinfo->dataref))\n\t\treturn;\n\n\tfor (i = 0; i < shinfo->nr_frags; i++)\n\t\t__skb_frag_unref(&shinfo->frags[i]);\n\n\t/*\n\t * If skb buf is from userspace, we need to notify the caller\n\t * the lower device DMA has done;\n\t */\n\tif (shinfo->tx_flags & SKBTX_DEV_ZEROCOPY) {\n\t\tstruct ubuf_info *uarg;\n\n\t\tuarg = shinfo->destructor_arg;\n\t\tif (uarg->callback)\n\t\t\tuarg->callback(uarg, true);\n\t}\n\n\tif (shinfo->frag_list)\n\t\tkfree_skb_list(shinfo->frag_list);\n\n\tskb_free_head(skb);\n}\n\n/*\n *\tFree an skbuff by memory without cleaning the state.\n */\nstatic void kfree_skbmem(struct sk_buff *skb)\n{\n\tstruct sk_buff_fclones *fclones;\n\n\tswitch (skb->fclone) {\n\tcase SKB_FCLONE_UNAVAILABLE:\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t\treturn;\n\n\tcase SKB_FCLONE_ORIG:\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\t\t/* We usually free the clone (TX completion) before original skb\n\t\t * This test would have no chance to be true for the clone,\n\t\t * while here, branch prediction will be good.\n\t\t */\n\t\tif (atomic_read(&fclones->fclone_ref) == 1)\n\t\t\tgoto fastpath;\n\t\tbreak;\n\n\tdefault: /* SKB_FCLONE_CLONE */\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb2);\n\t\tbreak;\n\t}\n\tif (!atomic_dec_and_test(&fclones->fclone_ref))\n\t\treturn;\nfastpath:\n\tkmem_cache_free(skbuff_fclone_cache, fclones);\n}\n\nstatic void skb_release_head_state(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n#ifdef CONFIG_XFRM\n\tsecpath_put(skb->sp);\n#endif\n\tif (skb->destructor) {\n\t\tWARN_ON(in_irq());\n\t\tskb->destructor(skb);\n\t}\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tnf_conntrack_put(skb_nfct(skb));\n#endif\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\tnf_bridge_put(skb->nf_bridge);\n#endif\n}\n\n/* Free everything but the sk_buff shell. */\nstatic void skb_release_all(struct sk_buff *skb)\n{\n\tskb_release_head_state(skb);\n\tif (likely(skb->head))\n\t\tskb_release_data(skb);\n}\n\n/**\n *\t__kfree_skb - private function\n *\t@skb: buffer\n *\n *\tFree an sk_buff. Release anything attached to the buffer.\n *\tClean the state. This is an internal helper function. Users should\n *\talways call kfree_skb\n */\n\nvoid __kfree_skb(struct sk_buff *skb)\n{\n\tskb_release_all(skb);\n\tkfree_skbmem(skb);\n}\nEXPORT_SYMBOL(__kfree_skb);\n\n/**\n *\tkfree_skb - free an sk_buff\n *\t@skb: buffer to free\n *\n *\tDrop a reference to the buffer and free it if the usage count has\n *\thit zero.\n */\nvoid kfree_skb(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\ttrace_kfree_skb(skb, __builtin_return_address(0));\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(kfree_skb);\n\nvoid kfree_skb_list(struct sk_buff *segs)\n{\n\twhile (segs) {\n\t\tstruct sk_buff *next = segs->next;\n\n\t\tkfree_skb(segs);\n\t\tsegs = next;\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_list);\n\n/**\n *\tskb_tx_error - report an sk_buff xmit error\n *\t@skb: buffer that triggered an error\n *\n *\tReport xmit error if a device callback is tracking this skb.\n *\tskb must be freed afterwards.\n */\nvoid skb_tx_error(struct sk_buff *skb)\n{\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_DEV_ZEROCOPY) {\n\t\tstruct ubuf_info *uarg;\n\n\t\tuarg = skb_shinfo(skb)->destructor_arg;\n\t\tif (uarg->callback)\n\t\t\tuarg->callback(uarg, false);\n\t\tskb_shinfo(skb)->tx_flags &= ~SKBTX_DEV_ZEROCOPY;\n\t}\n}\nEXPORT_SYMBOL(skb_tx_error);\n\n/**\n *\tconsume_skb - free an skbuff\n *\t@skb: buffer to free\n *\n *\tDrop a ref to the buffer and free it if the usage count has hit zero\n *\tFunctions identically to kfree_skb, but kfree_skb assumes that the frame\n *\tis being dropped after a failure and notes that\n */\nvoid consume_skb(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\ttrace_consume_skb(skb);\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(consume_skb);\n\nvoid __kfree_skb_flush(void)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\t/* flush skb_cache if containing objects */\n\tif (nc->skb_count) {\n\t\tkmem_cache_free_bulk(skbuff_head_cache, nc->skb_count,\n\t\t\t\t     nc->skb_cache);\n\t\tnc->skb_count = 0;\n\t}\n}\n\nstatic inline void _kfree_skb_defer(struct sk_buff *skb)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\t/* drop skb->head and call any destructors for packet */\n\tskb_release_all(skb);\n\n\t/* record skb to CPU local list */\n\tnc->skb_cache[nc->skb_count++] = skb;\n\n#ifdef CONFIG_SLUB\n\t/* SLUB writes into objects when freeing */\n\tprefetchw(skb);\n#endif\n\n\t/* flush skb_cache if it is filled */\n\tif (unlikely(nc->skb_count == NAPI_SKB_CACHE_SIZE)) {\n\t\tkmem_cache_free_bulk(skbuff_head_cache, NAPI_SKB_CACHE_SIZE,\n\t\t\t\t     nc->skb_cache);\n\t\tnc->skb_count = 0;\n\t}\n}\nvoid __kfree_skb_defer(struct sk_buff *skb)\n{\n\t_kfree_skb_defer(skb);\n}\n\nvoid napi_consume_skb(struct sk_buff *skb, int budget)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\n\t/* Zero budget indicate non-NAPI context called us, like netpoll */\n\tif (unlikely(!budget)) {\n\t\tdev_consume_skb_any(skb);\n\t\treturn;\n\t}\n\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\t/* if reaching here SKB is ready to free */\n\ttrace_consume_skb(skb);\n\n\t/* if SKB is a clone, don't handle this case */\n\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\n\t_kfree_skb_defer(skb);\n}\nEXPORT_SYMBOL(napi_consume_skb);\n\n/* Make sure a field is enclosed inside headers_start/headers_end section */\n#define CHECK_SKB_FIELD(field) \\\n\tBUILD_BUG_ON(offsetof(struct sk_buff, field) <\t\t\\\n\t\t     offsetof(struct sk_buff, headers_start));\t\\\n\tBUILD_BUG_ON(offsetof(struct sk_buff, field) >\t\t\\\n\t\t     offsetof(struct sk_buff, headers_end));\t\\\n\nstatic void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\tnew->tstamp\t\t= old->tstamp;\n\t/* We do not copy old->sk */\n\tnew->dev\t\t= old->dev;\n\tmemcpy(new->cb, old->cb, sizeof(old->cb));\n\tskb_dst_copy(new, old);\n#ifdef CONFIG_XFRM\n\tnew->sp\t\t\t= secpath_get(old->sp);\n#endif\n\t__nf_copy(new, old, false);\n\n\t/* Note : this field could be in headers_start/headers_end section\n\t * It is not yet because we do not want to have a 16 bit hole\n\t */\n\tnew->queue_mapping = old->queue_mapping;\n\n\tmemcpy(&new->headers_start, &old->headers_start,\n\t       offsetof(struct sk_buff, headers_end) -\n\t       offsetof(struct sk_buff, headers_start));\n\tCHECK_SKB_FIELD(protocol);\n\tCHECK_SKB_FIELD(csum);\n\tCHECK_SKB_FIELD(hash);\n\tCHECK_SKB_FIELD(priority);\n\tCHECK_SKB_FIELD(skb_iif);\n\tCHECK_SKB_FIELD(vlan_proto);\n\tCHECK_SKB_FIELD(vlan_tci);\n\tCHECK_SKB_FIELD(transport_header);\n\tCHECK_SKB_FIELD(network_header);\n\tCHECK_SKB_FIELD(mac_header);\n\tCHECK_SKB_FIELD(inner_protocol);\n\tCHECK_SKB_FIELD(inner_transport_header);\n\tCHECK_SKB_FIELD(inner_network_header);\n\tCHECK_SKB_FIELD(inner_mac_header);\n\tCHECK_SKB_FIELD(mark);\n#ifdef CONFIG_NETWORK_SECMARK\n\tCHECK_SKB_FIELD(secmark);\n#endif\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tCHECK_SKB_FIELD(napi_id);\n#endif\n#ifdef CONFIG_XPS\n\tCHECK_SKB_FIELD(sender_cpu);\n#endif\n#ifdef CONFIG_NET_SCHED\n\tCHECK_SKB_FIELD(tc_index);\n#endif\n\n}\n\n/*\n * You should not add any new code to this function.  Add it to\n * __copy_skb_header above instead.\n */\nstatic struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)\n{\n#define C(x) n->x = skb->x\n\n\tn->next = n->prev = NULL;\n\tn->sk = NULL;\n\t__copy_skb_header(n, skb);\n\n\tC(len);\n\tC(data_len);\n\tC(mac_len);\n\tn->hdr_len = skb->nohdr ? skb_headroom(skb) : skb->hdr_len;\n\tn->cloned = 1;\n\tn->nohdr = 0;\n\tn->destructor = NULL;\n\tC(tail);\n\tC(end);\n\tC(head);\n\tC(head_frag);\n\tC(data);\n\tC(truesize);\n\tatomic_set(&n->users, 1);\n\n\tatomic_inc(&(skb_shinfo(skb)->dataref));\n\tskb->cloned = 1;\n\n\treturn n;\n#undef C\n}\n\n/**\n *\tskb_morph\t-\tmorph one skb into another\n *\t@dst: the skb to receive the contents\n *\t@src: the skb to supply the contents\n *\n *\tThis is identical to skb_clone except that the target skb is\n *\tsupplied by the user.\n *\n *\tThe target skb is returned upon exit.\n */\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src)\n{\n\tskb_release_all(dst);\n\treturn __skb_clone(dst, src);\n}\nEXPORT_SYMBOL_GPL(skb_morph);\n\n/**\n *\tskb_copy_ubufs\t-\tcopy userspace skb frags buffers to kernel\n *\t@skb: the skb to modify\n *\t@gfp_mask: allocation priority\n *\n *\tThis must be called on SKBTX_DEV_ZEROCOPY skb.\n *\tIt will copy all frags into kernel and drop the reference\n *\tto userspace pages.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n *\n *\tReturns 0 on success or a negative error code on failure\n *\tto allocate kernel memory to copy to.\n */\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint i;\n\tint num_frags = skb_shinfo(skb)->nr_frags;\n\tstruct page *page, *head = NULL;\n\tstruct ubuf_info *uarg = skb_shinfo(skb)->destructor_arg;\n\n\tfor (i = 0; i < num_frags; i++) {\n\t\tu8 *vaddr;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tpage = alloc_page(gfp_mask);\n\t\tif (!page) {\n\t\t\twhile (head) {\n\t\t\t\tstruct page *next = (struct page *)page_private(head);\n\t\t\t\tput_page(head);\n\t\t\t\thead = next;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tvaddr = kmap_atomic(skb_frag_page(f));\n\t\tmemcpy(page_address(page),\n\t\t       vaddr + f->page_offset, skb_frag_size(f));\n\t\tkunmap_atomic(vaddr);\n\t\tset_page_private(page, (unsigned long)head);\n\t\thead = page;\n\t}\n\n\t/* skb frags release userspace buffers */\n\tfor (i = 0; i < num_frags; i++)\n\t\tskb_frag_unref(skb, i);\n\n\tuarg->callback(uarg, false);\n\n\t/* skb frags point to kernel buffers */\n\tfor (i = num_frags - 1; i >= 0; i--) {\n\t\t__skb_fill_page_desc(skb, i, head, 0,\n\t\t\t\t     skb_shinfo(skb)->frags[i].size);\n\t\thead = (struct page *)page_private(head);\n\t}\n\n\tskb_shinfo(skb)->tx_flags &= ~SKBTX_DEV_ZEROCOPY;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_copy_ubufs);\n\n/**\n *\tskb_clone\t-\tduplicate an sk_buff\n *\t@skb: buffer to clone\n *\t@gfp_mask: allocation priority\n *\n *\tDuplicate an &sk_buff. The new one is not owned by a socket. Both\n *\tcopies share the same packet data but not structure. The new\n *\tbuffer has a reference count of 1. If the allocation fails the\n *\tfunction returns %NULL otherwise the new buffer is returned.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n */\n\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tstruct sk_buff_fclones *fclones = container_of(skb,\n\t\t\t\t\t\t       struct sk_buff_fclones,\n\t\t\t\t\t\t       skb1);\n\tstruct sk_buff *n;\n\n\tif (skb_orphan_frags(skb, gfp_mask))\n\t\treturn NULL;\n\n\tif (skb->fclone == SKB_FCLONE_ORIG &&\n\t    atomic_read(&fclones->fclone_ref) == 1) {\n\t\tn = &fclones->skb2;\n\t\tatomic_set(&fclones->fclone_ref, 2);\n\t} else {\n\t\tif (skb_pfmemalloc(skb))\n\t\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t\tn = kmem_cache_alloc(skbuff_head_cache, gfp_mask);\n\t\tif (!n)\n\t\t\treturn NULL;\n\n\t\tkmemcheck_annotate_bitfield(n, flags1);\n\t\tn->fclone = SKB_FCLONE_UNAVAILABLE;\n\t}\n\n\treturn __skb_clone(n, skb);\n}\nEXPORT_SYMBOL(skb_clone);\n\nstatic void skb_headers_offset_update(struct sk_buff *skb, int off)\n{\n\t/* Only adjust this if it actually is csum_start rather than csum */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tskb->csum_start += off;\n\t/* {transport,network,mac}_header and tail are relative to skb->head */\n\tskb->transport_header += off;\n\tskb->network_header   += off;\n\tif (skb_mac_header_was_set(skb))\n\t\tskb->mac_header += off;\n\tskb->inner_transport_header += off;\n\tskb->inner_network_header += off;\n\tskb->inner_mac_header += off;\n}\n\nstatic void copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\t__copy_skb_header(new, old);\n\n\tskb_shinfo(new)->gso_size = skb_shinfo(old)->gso_size;\n\tskb_shinfo(new)->gso_segs = skb_shinfo(old)->gso_segs;\n\tskb_shinfo(new)->gso_type = skb_shinfo(old)->gso_type;\n}\n\nstatic inline int skb_alloc_rx_flag(const struct sk_buff *skb)\n{\n\tif (skb_pfmemalloc(skb))\n\t\treturn SKB_ALLOC_RX;\n\treturn 0;\n}\n\n/**\n *\tskb_copy\t-\tcreate private copy of an sk_buff\n *\t@skb: buffer to copy\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data. This is used when the\n *\tcaller wishes to modify the data and needs a private copy of the\n *\tdata to alter. Returns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tAs by-product this function converts non-linear &sk_buff to linear\n *\tone, so that &sk_buff becomes completely private and caller is allowed\n *\tto modify all the data of returned buffer. This means that this\n *\tfunction is not recommended for use in circumstances when only\n *\theader is going to be modified. Use pskb_copy() instead.\n */\n\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint headerlen = skb_headroom(skb);\n\tunsigned int size = skb_end_offset(skb) + skb->data_len;\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\n\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\n\n\tif (!n)\n\t\treturn NULL;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headerlen);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\tif (skb_copy_bits(skb, -headerlen, n->head, headerlen + skb->len))\n\t\tBUG();\n\n\tcopy_skb_header(n, skb);\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy);\n\n/**\n *\t__pskb_copy_fclone\t-  create copy of an sk_buff with private head.\n *\t@skb: buffer to copy\n *\t@headroom: headroom of new skb\n *\t@gfp_mask: allocation priority\n *\t@fclone: if true allocate the copy of the skb from the fclone\n *\tcache instead of the head cache; it is recommended to set this\n *\tto true for the cases where the copy will likely be cloned\n *\n *\tMake a copy of both an &sk_buff and part of its data, located\n *\tin header. Fragmented data remain shared. This is used when\n *\tthe caller wishes to modify only header of &sk_buff and needs\n *\tprivate copy of the header to alter. Returns %NULL on failure\n *\tor the pointer to the buffer on success.\n *\tThe returned buffer has a reference count of 1.\n */\n\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone)\n{\n\tunsigned int size = skb_headlen(skb) + headroom;\n\tint flags = skb_alloc_rx_flag(skb) | (fclone ? SKB_ALLOC_FCLONE : 0);\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask, flags, NUMA_NO_NODE);\n\n\tif (!n)\n\t\tgoto out;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headroom);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb_headlen(skb));\n\t/* Copy the bytes */\n\tskb_copy_from_linear_data(skb, n->data, n->len);\n\n\tn->truesize += skb->data_len;\n\tn->data_len  = skb->data_len;\n\tn->len\t     = skb->len;\n\n\tif (skb_shinfo(skb)->nr_frags) {\n\t\tint i;\n\n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tkfree_skb(n);\n\t\t\tn = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_shinfo(n)->frags[i] = skb_shinfo(skb)->frags[i];\n\t\t\tskb_frag_ref(skb, i);\n\t\t}\n\t\tskb_shinfo(n)->nr_frags = i;\n\t}\n\n\tif (skb_has_frag_list(skb)) {\n\t\tskb_shinfo(n)->frag_list = skb_shinfo(skb)->frag_list;\n\t\tskb_clone_fraglist(n);\n\t}\n\n\tcopy_skb_header(n, skb);\nout:\n\treturn n;\n}\nEXPORT_SYMBOL(__pskb_copy_fclone);\n\n/**\n *\tpskb_expand_head - reallocate header of &sk_buff\n *\t@skb: buffer to reallocate\n *\t@nhead: room to add at head\n *\t@ntail: room to add at tail\n *\t@gfp_mask: allocation priority\n *\n *\tExpands (or creates identical copy, if @nhead and @ntail are zero)\n *\theader of @skb. &sk_buff itself is not changed. &sk_buff MUST have\n *\treference count of 1. Returns zero in the case of success or error,\n *\tif expansion failed. In the last case, &sk_buff is not changed.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,\n\t\t     gfp_t gfp_mask)\n{\n\tint i, osize = skb_end_offset(skb);\n\tint size = osize + nhead + ntail;\n\tlong off;\n\tu8 *data;\n\n\tBUG_ON(nhead < 0);\n\n\tif (skb_shared(skb))\n\t\tBUG();\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\tgoto nodata;\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\t/* Copy only real data... and, alas, header. This should be\n\t * optimized for the cases when header is void.\n\t */\n\tmemcpy(data + nhead, skb->head, skb_tail_pointer(skb) - skb->head);\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info, frags[skb_shinfo(skb)->nr_frags]));\n\n\t/*\n\t * if shinfo is shared we must drop the old head gracefully, but if it\n\t * is not we can just drop the old head and let the existing refcount\n\t * be since all we did is relocate the values\n\t */\n\tif (skb_cloned(skb)) {\n\t\t/* copy this zero copy skb frags */\n\t\tif (skb_orphan_frags(skb, gfp_mask))\n\t\t\tgoto nofrags;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\n\t\tskb_release_data(skb);\n\t} else {\n\t\tskb_free_head(skb);\n\t}\n\toff = (data + nhead) - skb->head;\n\n\tskb->head     = data;\n\tskb->head_frag = 0;\n\tskb->data    += off;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end      = size;\n\toff           = nhead;\n#else\n\tskb->end      = skb->head + size;\n#endif\n\tskb->tail\t      += off;\n\tskb_headers_offset_update(skb, nhead);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\n\t/* It is not generally safe to change skb->truesize.\n\t * For the moment, we really care of rx path, or\n\t * when skb is orphaned (not attached to a socket).\n\t */\n\tif (!skb->sk || skb->destructor == sock_edemux)\n\t\tskb->truesize += size - osize;\n\n\treturn 0;\n\nnofrags:\n\tkfree(data);\nnodata:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(pskb_expand_head);\n\n/* Make private copy of skb with writable head and some headroom */\n\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb, unsigned int headroom)\n{\n\tstruct sk_buff *skb2;\n\tint delta = headroom - skb_headroom(skb);\n\n\tif (delta <= 0)\n\t\tskb2 = pskb_copy(skb, GFP_ATOMIC);\n\telse {\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (skb2 && pskb_expand_head(skb2, SKB_DATA_ALIGN(delta), 0,\n\t\t\t\t\t     GFP_ATOMIC)) {\n\t\t\tkfree_skb(skb2);\n\t\t\tskb2 = NULL;\n\t\t}\n\t}\n\treturn skb2;\n}\nEXPORT_SYMBOL(skb_realloc_headroom);\n\n/**\n *\tskb_copy_expand\t-\tcopy and expand sk_buff\n *\t@skb: buffer to copy\n *\t@newheadroom: new free bytes at head\n *\t@newtailroom: new free bytes at tail\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data and while doing so\n *\tallocate additional space.\n *\n *\tThis is used when the caller wishes to modify the data and needs a\n *\tprivate copy of the data to alter as well as more space for new fields.\n *\tReturns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tYou must pass %GFP_ATOMIC as the allocation priority if this function\n *\tis called from an interrupt.\n */\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb,\n\t\t\t\tint newheadroom, int newtailroom,\n\t\t\t\tgfp_t gfp_mask)\n{\n\t/*\n\t *\tAllocate the copy buffer\n\t */\n\tstruct sk_buff *n = __alloc_skb(newheadroom + skb->len + newtailroom,\n\t\t\t\t\tgfp_mask, skb_alloc_rx_flag(skb),\n\t\t\t\t\tNUMA_NO_NODE);\n\tint oldheadroom = skb_headroom(skb);\n\tint head_copy_len, head_copy_off;\n\n\tif (!n)\n\t\treturn NULL;\n\n\tskb_reserve(n, newheadroom);\n\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\thead_copy_len = oldheadroom;\n\thead_copy_off = 0;\n\tif (newheadroom <= head_copy_len)\n\t\thead_copy_len = newheadroom;\n\telse\n\t\thead_copy_off = newheadroom - head_copy_len;\n\n\t/* Copy the linear header and data. */\n\tif (skb_copy_bits(skb, -head_copy_len, n->head + head_copy_off,\n\t\t\t  skb->len + head_copy_len))\n\t\tBUG();\n\n\tcopy_skb_header(n, skb);\n\n\tskb_headers_offset_update(n, newheadroom - oldheadroom);\n\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy_expand);\n\n/**\n *\tskb_pad\t\t\t-\tzero pad the tail of an skb\n *\t@skb: buffer to pad\n *\t@pad: space to pad\n *\n *\tEnsure that a buffer is followed by a padding area that is zero\n *\tfilled. Used by network drivers which may DMA or transfer data\n *\tbeyond the buffer end onto the wire.\n *\n *\tMay return error in out of memory cases. The skb is freed on error.\n */\n\nint skb_pad(struct sk_buff *skb, int pad)\n{\n\tint err;\n\tint ntail;\n\n\t/* If the skbuff is non linear tailroom is always zero.. */\n\tif (!skb_cloned(skb) && skb_tailroom(skb) >= pad) {\n\t\tmemset(skb->data+skb->len, 0, pad);\n\t\treturn 0;\n\t}\n\n\tntail = skb->data_len + pad - (skb->end - skb->tail);\n\tif (likely(skb_cloned(skb) || ntail > 0)) {\n\t\terr = pskb_expand_head(skb, 0, ntail, GFP_ATOMIC);\n\t\tif (unlikely(err))\n\t\t\tgoto free_skb;\n\t}\n\n\t/* FIXME: The use of this function with non-linear skb's really needs\n\t * to be audited.\n\t */\n\terr = skb_linearize(skb);\n\tif (unlikely(err))\n\t\tgoto free_skb;\n\n\tmemset(skb->data + skb->len, 0, pad);\n\treturn 0;\n\nfree_skb:\n\tkfree_skb(skb);\n\treturn err;\n}\nEXPORT_SYMBOL(skb_pad);\n\n/**\n *\tpskb_put - add data to the tail of a potentially fragmented buffer\n *\t@skb: start of the buffer to use\n *\t@tail: tail fragment of the buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the potentially\n *\tfragmented buffer. @tail must be the last fragment of @skb -- or\n *\t@skb itself. If this would exceed the total buffer size the kernel\n *\twill panic. A pointer to the first byte of the extra data is\n *\treturned.\n */\n\nunsigned char *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len)\n{\n\tif (tail != skb) {\n\t\tskb->data_len += len;\n\t\tskb->len += len;\n\t}\n\treturn skb_put(tail, len);\n}\nEXPORT_SYMBOL_GPL(pskb_put);\n\n/**\n *\tskb_put - add data to a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer. If this would\n *\texceed the total buffer size the kernel will panic. A pointer to the\n *\tfirst byte of the extra data is returned.\n */\nunsigned char *skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned char *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\tif (unlikely(skb->tail > skb->end))\n\t\tskb_over_panic(skb, len, __builtin_return_address(0));\n\treturn tmp;\n}\nEXPORT_SYMBOL(skb_put);\n\n/**\n *\tskb_push - add data to the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer at the buffer\n *\tstart. If this would exceed the total buffer headroom the kernel will\n *\tpanic. A pointer to the first byte of the extra data is returned.\n */\nunsigned char *skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\tif (unlikely(skb->data<skb->head))\n\t\tskb_under_panic(skb, len, __builtin_return_address(0));\n\treturn skb->data;\n}\nEXPORT_SYMBOL(skb_push);\n\n/**\n *\tskb_pull - remove data from the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to remove\n *\n *\tThis function removes data from the start of a buffer, returning\n *\tthe memory to the headroom. A pointer to the next data in the buffer\n *\tis returned. Once the data has been pulled future pushes will overwrite\n *\tthe old data.\n */\nunsigned char *skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn skb_pull_inline(skb, len);\n}\nEXPORT_SYMBOL(skb_pull);\n\n/**\n *\tskb_trim - remove end from a buffer\n *\t@skb: buffer to alter\n *\t@len: new length\n *\n *\tCut the length of a buffer down by removing data from the tail. If\n *\tthe buffer is already under the length specified it is not modified.\n *\tThe skb must be linear.\n */\nvoid skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->len > len)\n\t\t__skb_trim(skb, len);\n}\nEXPORT_SYMBOL(skb_trim);\n\n/* Trims skb to length len. It can change skb pointers.\n */\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tstruct sk_buff **fragp;\n\tstruct sk_buff *frag;\n\tint offset = skb_headlen(skb);\n\tint nfrags = skb_shinfo(skb)->nr_frags;\n\tint i;\n\tint err;\n\n\tif (skb_cloned(skb) &&\n\t    unlikely((err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC))))\n\t\treturn err;\n\n\ti = 0;\n\tif (offset >= len)\n\t\tgoto drop_pages;\n\n\tfor (; i < nfrags; i++) {\n\t\tint end = offset + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i++], len - offset);\n\ndrop_pages:\n\t\tskb_shinfo(skb)->nr_frags = i;\n\n\t\tfor (; i < nfrags; i++)\n\t\t\tskb_frag_unref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_drop_fraglist(skb);\n\t\tgoto done;\n\t}\n\n\tfor (fragp = &skb_shinfo(skb)->frag_list; (frag = *fragp);\n\t     fragp = &frag->next) {\n\t\tint end = offset + frag->len;\n\n\t\tif (skb_shared(frag)) {\n\t\t\tstruct sk_buff *nfrag;\n\n\t\t\tnfrag = skb_clone(frag, GFP_ATOMIC);\n\t\t\tif (unlikely(!nfrag))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tnfrag->next = frag->next;\n\t\t\tconsume_skb(frag);\n\t\t\tfrag = nfrag;\n\t\t\t*fragp = frag;\n\t\t}\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (end > len &&\n\t\t    unlikely((err = pskb_trim(frag, len - offset))))\n\t\t\treturn err;\n\n\t\tif (frag->next)\n\t\t\tskb_drop_list(&frag->next);\n\t\tbreak;\n\t}\n\ndone:\n\tif (len > skb_headlen(skb)) {\n\t\tskb->data_len -= skb->len - len;\n\t\tskb->len       = len;\n\t} else {\n\t\tskb->len       = len;\n\t\tskb->data_len  = 0;\n\t\tskb_set_tail_pointer(skb, len);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(___pskb_trim);\n\n/**\n *\t__pskb_pull_tail - advance tail of skb header\n *\t@skb: buffer to reallocate\n *\t@delta: number of bytes to advance tail\n *\n *\tThe function makes a sense only on a fragmented &sk_buff,\n *\tit expands header moving its tail forward and copying necessary\n *\tdata from fragmented part.\n *\n *\t&sk_buff MUST have reference count of 1.\n *\n *\tReturns %NULL (and &sk_buff does not change) if pull failed\n *\tor value of new tail of skb in the case of success.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\n/* Moves tail of skb head forward, copying data from fragmented part,\n * when it is necessary.\n * 1. It may fail due to malloc failure.\n * 2. It may change skb pointers.\n *\n * It is pretty complicated. Luckily, it is called only in exceptional cases.\n */\nunsigned char *__pskb_pull_tail(struct sk_buff *skb, int delta)\n{\n\t/* If skb has not enough free space at tail, get new one\n\t * plus 128 bytes for future expansions. If we have enough\n\t * room at tail, reallocate without expansion only if skb is cloned.\n\t */\n\tint i, k, eat = (skb->tail + delta) - skb->end;\n\n\tif (eat > 0 || skb_cloned(skb)) {\n\t\tif (pskb_expand_head(skb, 0, eat > 0 ? eat + 128 : 0,\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn NULL;\n\t}\n\n\tif (skb_copy_bits(skb, skb_headlen(skb), skb_tail_pointer(skb), delta))\n\t\tBUG();\n\n\t/* Optimization: no fragments, no reasons to preestimate\n\t * size of pulled pages. Superb.\n\t */\n\tif (!skb_has_frag_list(skb))\n\t\tgoto pull_pages;\n\n\t/* Estimate size of pulled pages. */\n\teat = delta;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size >= eat)\n\t\t\tgoto pull_pages;\n\t\teat -= size;\n\t}\n\n\t/* If we need update frag list, we are in troubles.\n\t * Certainly, it possible to add an offset to skb data,\n\t * but taking into account that pulling is expected to\n\t * be very rare operation, it is worth to fight against\n\t * further bloating skb head and crucify ourselves here instead.\n\t * Pure masohism, indeed. 8)8)\n\t */\n\tif (eat) {\n\t\tstruct sk_buff *list = skb_shinfo(skb)->frag_list;\n\t\tstruct sk_buff *clone = NULL;\n\t\tstruct sk_buff *insp = NULL;\n\n\t\tdo {\n\t\t\tBUG_ON(!list);\n\n\t\t\tif (list->len <= eat) {\n\t\t\t\t/* Eaten as whole. */\n\t\t\t\teat -= list->len;\n\t\t\t\tlist = list->next;\n\t\t\t\tinsp = list;\n\t\t\t} else {\n\t\t\t\t/* Eaten partially. */\n\n\t\t\t\tif (skb_shared(list)) {\n\t\t\t\t\t/* Sucks! We need to fork list. :-( */\n\t\t\t\t\tclone = skb_clone(list, GFP_ATOMIC);\n\t\t\t\t\tif (!clone)\n\t\t\t\t\t\treturn NULL;\n\t\t\t\t\tinsp = list->next;\n\t\t\t\t\tlist = clone;\n\t\t\t\t} else {\n\t\t\t\t\t/* This may be pulled without\n\t\t\t\t\t * problems. */\n\t\t\t\t\tinsp = list;\n\t\t\t\t}\n\t\t\t\tif (!pskb_pull(list, eat)) {\n\t\t\t\t\tkfree_skb(clone);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (eat);\n\n\t\t/* Free pulled out fragments. */\n\t\twhile ((list = skb_shinfo(skb)->frag_list) != insp) {\n\t\t\tskb_shinfo(skb)->frag_list = list->next;\n\t\t\tkfree_skb(list);\n\t\t}\n\t\t/* And insert new clone at head. */\n\t\tif (clone) {\n\t\t\tclone->next = list;\n\t\t\tskb_shinfo(skb)->frag_list = clone;\n\t\t}\n\t}\n\t/* Success! Now we may commit changes to skb data. */\n\npull_pages:\n\teat = delta;\n\tk = 0;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size <= eat) {\n\t\t\tskb_frag_unref(skb, i);\n\t\t\teat -= size;\n\t\t} else {\n\t\t\tskb_shinfo(skb)->frags[k] = skb_shinfo(skb)->frags[i];\n\t\t\tif (eat) {\n\t\t\t\tskb_shinfo(skb)->frags[k].page_offset += eat;\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb)->frags[k], eat);\n\t\t\t\teat = 0;\n\t\t\t}\n\t\t\tk++;\n\t\t}\n\t}\n\tskb_shinfo(skb)->nr_frags = k;\n\n\tskb->tail     += delta;\n\tskb->data_len -= delta;\n\n\treturn skb_tail_pointer(skb);\n}\nEXPORT_SYMBOL(__pskb_pull_tail);\n\n/**\n *\tskb_copy_bits - copy bits from skb to kernel buffer\n *\t@skb: source skb\n *\t@offset: offset in source\n *\t@to: destination buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source skb to the\n *\tdestination buffer.\n *\n *\tCAUTION ! :\n *\t\tIf its prototype is ever changed,\n *\t\tcheck arch/{*}/net/{*}.S files,\n *\t\tsince it is called from BPF assembly code.\n */\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\t/* Copy header. */\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_from_linear_data_offset(skb, offset, to, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tto     += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(f);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tvaddr = kmap_atomic(skb_frag_page(f));\n\t\t\tmemcpy(to,\n\t\t\t       vaddr + f->page_offset + offset - start,\n\t\t\t       copy);\n\t\t\tkunmap_atomic(vaddr);\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_copy_bits(frag_iter, offset - start, to, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_copy_bits);\n\n/*\n * Callback from splice_to_pipe(), if we need to release some pages\n * at the end of the spd in case we error'ed out in filling the pipe.\n */\nstatic void sock_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tput_page(spd->pages[i]);\n}\n\nstatic struct page *linear_to_page(struct page *page, unsigned int *len,\n\t\t\t\t   unsigned int *offset,\n\t\t\t\t   struct sock *sk)\n{\n\tstruct page_frag *pfrag = sk_page_frag(sk);\n\n\tif (!sk_page_frag_refill(sk, pfrag))\n\t\treturn NULL;\n\n\t*len = min_t(unsigned int, *len, pfrag->size - pfrag->offset);\n\n\tmemcpy(page_address(pfrag->page) + pfrag->offset,\n\t       page_address(page) + *offset, *len);\n\t*offset = pfrag->offset;\n\tpfrag->offset += *len;\n\n\treturn pfrag->page;\n}\n\nstatic bool spd_can_coalesce(const struct splice_pipe_desc *spd,\n\t\t\t     struct page *page,\n\t\t\t     unsigned int offset)\n{\n\treturn\tspd->nr_pages &&\n\t\tspd->pages[spd->nr_pages - 1] == page &&\n\t\t(spd->partial[spd->nr_pages - 1].offset +\n\t\t spd->partial[spd->nr_pages - 1].len == offset);\n}\n\n/*\n * Fill page/offset/length into spd, if it can hold more pages.\n */\nstatic bool spd_fill_page(struct splice_pipe_desc *spd,\n\t\t\t  struct pipe_inode_info *pipe, struct page *page,\n\t\t\t  unsigned int *len, unsigned int offset,\n\t\t\t  bool linear,\n\t\t\t  struct sock *sk)\n{\n\tif (unlikely(spd->nr_pages == MAX_SKB_FRAGS))\n\t\treturn true;\n\n\tif (linear) {\n\t\tpage = linear_to_page(page, len, &offset, sk);\n\t\tif (!page)\n\t\t\treturn true;\n\t}\n\tif (spd_can_coalesce(spd, page, offset)) {\n\t\tspd->partial[spd->nr_pages - 1].len += *len;\n\t\treturn false;\n\t}\n\tget_page(page);\n\tspd->pages[spd->nr_pages] = page;\n\tspd->partial[spd->nr_pages].len = *len;\n\tspd->partial[spd->nr_pages].offset = offset;\n\tspd->nr_pages++;\n\n\treturn false;\n}\n\nstatic bool __splice_segment(struct page *page, unsigned int poff,\n\t\t\t     unsigned int plen, unsigned int *off,\n\t\t\t     unsigned int *len,\n\t\t\t     struct splice_pipe_desc *spd, bool linear,\n\t\t\t     struct sock *sk,\n\t\t\t     struct pipe_inode_info *pipe)\n{\n\tif (!*len)\n\t\treturn true;\n\n\t/* skip this segment if already processed */\n\tif (*off >= plen) {\n\t\t*off -= plen;\n\t\treturn false;\n\t}\n\n\t/* ignore any bits we already processed */\n\tpoff += *off;\n\tplen -= *off;\n\t*off = 0;\n\n\tdo {\n\t\tunsigned int flen = min(*len, plen);\n\n\t\tif (spd_fill_page(spd, pipe, page, &flen, poff,\n\t\t\t\t  linear, sk))\n\t\t\treturn true;\n\t\tpoff += flen;\n\t\tplen -= flen;\n\t\t*len -= flen;\n\t} while (*len && plen);\n\n\treturn false;\n}\n\n/*\n * Map linear and fragment data from the skb to spd. It reports true if the\n * pipe is full or if we already spliced the requested length.\n */\nstatic bool __skb_splice_bits(struct sk_buff *skb, struct pipe_inode_info *pipe,\n\t\t\t      unsigned int *offset, unsigned int *len,\n\t\t\t      struct splice_pipe_desc *spd, struct sock *sk)\n{\n\tint seg;\n\tstruct sk_buff *iter;\n\n\t/* map the linear part :\n\t * If skb->head_frag is set, this 'linear' part is backed by a\n\t * fragment, and if the head is not shared with any clones then\n\t * we can avoid a copy since we own the head portion of this page.\n\t */\n\tif (__splice_segment(virt_to_page(skb->data),\n\t\t\t     (unsigned long) skb->data & (PAGE_SIZE - 1),\n\t\t\t     skb_headlen(skb),\n\t\t\t     offset, len, spd,\n\t\t\t     skb_head_is_locked(skb),\n\t\t\t     sk, pipe))\n\t\treturn true;\n\n\t/*\n\t * then map the fragments\n\t */\n\tfor (seg = 0; seg < skb_shinfo(skb)->nr_frags; seg++) {\n\t\tconst skb_frag_t *f = &skb_shinfo(skb)->frags[seg];\n\n\t\tif (__splice_segment(skb_frag_page(f),\n\t\t\t\t     f->page_offset, skb_frag_size(f),\n\t\t\t\t     offset, len, spd, false, sk, pipe))\n\t\t\treturn true;\n\t}\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (*offset >= iter->len) {\n\t\t\t*offset -= iter->len;\n\t\t\tcontinue;\n\t\t}\n\t\t/* __skb_splice_bits() only fails if the output has no room\n\t\t * left, so no point in going over the frag_list for the error\n\t\t * case.\n\t\t */\n\t\tif (__skb_splice_bits(iter, pipe, offset, len, spd, sk))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Map data from the skb to a pipe. Should handle both the linear part,\n * the fragments, and the frag list.\n */\nint skb_splice_bits(struct sk_buff *skb, struct sock *sk, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int tlen,\n\t\t    unsigned int flags)\n{\n\tstruct partial_page partial[MAX_SKB_FRAGS];\n\tstruct page *pages[MAX_SKB_FRAGS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages = pages,\n\t\t.partial = partial,\n\t\t.nr_pages_max = MAX_SKB_FRAGS,\n\t\t.flags = flags,\n\t\t.ops = &nosteal_pipe_buf_ops,\n\t\t.spd_release = sock_spd_release,\n\t};\n\tint ret = 0;\n\n\t__skb_splice_bits(skb, pipe, &offset, &tlen, &spd, sk);\n\n\tif (spd.nr_pages)\n\t\tret = splice_to_pipe(pipe, &spd);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(skb_splice_bits);\n\n/**\n *\tskb_store_bits - store bits from kernel buffer to skb\n *\t@skb: destination buffer\n *\t@offset: offset in destination\n *\t@from: source buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source buffer to the\n *\tdestination skb.  This function handles all the messy bits of\n *\ttraversing fragment lists and such.\n */\n\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_to_linear_data_offset(skb, offset, from, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tfrom += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tmemcpy(vaddr + frag->page_offset + offset - start,\n\t\t\t       from, copy);\n\t\t\tkunmap_atomic(vaddr);\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_store_bits(frag_iter, offset - start,\n\t\t\t\t\t   from, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_store_bits);\n\n/* Checksum skb data. */\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t/* Checksum header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = ops->update(skb->data + offset, copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tcsum2 = ops->update(vaddr + frag->page_offset +\n\t\t\t\t\t    offset - start, copy, 0);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tcsum = ops->combine(csum, csum2, pos, copy);\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = __skb_checksum(frag_iter, offset - start,\n\t\t\t\t\t       copy, 0, ops);\n\t\t\tcsum = ops->combine(csum, csum2, pos, copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\n\treturn csum;\n}\nEXPORT_SYMBOL(__skb_checksum);\n\n__wsum skb_checksum(const struct sk_buff *skb, int offset,\n\t\t    int len, __wsum csum)\n{\n\tconst struct skb_checksum_ops ops = {\n\t\t.update  = csum_partial_ext,\n\t\t.combine = csum_block_add_ext,\n\t};\n\n\treturn __skb_checksum(skb, offset, len, csum, &ops);\n}\nEXPORT_SYMBOL(skb_checksum);\n\n/* Both of above in one bottle. */\n\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset,\n\t\t\t\t    u8 *to, int len, __wsum csum)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t/* Copy header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = csum_partial_copy_nocheck(skb->data + offset, to,\n\t\t\t\t\t\t copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tto     += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tcsum2 = csum_partial_copy_nocheck(vaddr +\n\t\t\t\t\t\t\t  frag->page_offset +\n\t\t\t\t\t\t\t  offset - start, to,\n\t\t\t\t\t\t\t  copy, 0);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\t__wsum csum2;\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = skb_copy_and_csum_bits(frag_iter,\n\t\t\t\t\t\t       offset - start,\n\t\t\t\t\t\t       to, copy, 0);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn csum;\n}\nEXPORT_SYMBOL(skb_copy_and_csum_bits);\n\n /**\n *\tskb_zerocopy_headlen - Calculate headroom needed for skb_zerocopy()\n *\t@from: source buffer\n *\n *\tCalculates the amount of linear headroom needed in the 'to' skb passed\n *\tinto skb_zerocopy().\n */\nunsigned int\nskb_zerocopy_headlen(const struct sk_buff *from)\n{\n\tunsigned int hlen = 0;\n\n\tif (!from->head_frag ||\n\t    skb_headlen(from) < L1_CACHE_BYTES ||\n\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS)\n\t\thlen = skb_headlen(from);\n\n\tif (skb_has_frag_list(from))\n\t\thlen = from->len;\n\n\treturn hlen;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy_headlen);\n\n/**\n *\tskb_zerocopy - Zero copy skb to skb\n *\t@to: destination buffer\n *\t@from: source buffer\n *\t@len: number of bytes to copy from source buffer\n *\t@hlen: size of linear headroom in destination buffer\n *\n *\tCopies up to `len` bytes from `from` to `to` by creating references\n *\tto the frags in the source buffer.\n *\n *\tThe `hlen` as calculated by skb_zerocopy_headlen() specifies the\n *\theadroom in the `to` buffer.\n *\n *\tReturn value:\n *\t0: everything is OK\n *\t-ENOMEM: couldn't orphan frags of @from due to lack of memory\n *\t-EFAULT: skb_copy_bits() found some problem with skb geometry\n */\nint\nskb_zerocopy(struct sk_buff *to, struct sk_buff *from, int len, int hlen)\n{\n\tint i, j = 0;\n\tint plen = 0; /* length of skb->head fragment */\n\tint ret;\n\tstruct page *page;\n\tunsigned int offset;\n\n\tBUG_ON(!from->head_frag && !hlen);\n\n\t/* dont bother with small payloads */\n\tif (len <= skb_tailroom(to))\n\t\treturn skb_copy_bits(from, 0, skb_put(to, len), len);\n\n\tif (hlen) {\n\t\tret = skb_copy_bits(from, 0, skb_put(to, hlen), hlen);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\tlen -= hlen;\n\t} else {\n\t\tplen = min_t(int, skb_headlen(from), len);\n\t\tif (plen) {\n\t\t\tpage = virt_to_head_page(from->head);\n\t\t\toffset = from->data - (unsigned char *)page_address(page);\n\t\t\t__skb_fill_page_desc(to, 0, page, offset, plen);\n\t\t\tget_page(page);\n\t\t\tj = 1;\n\t\t\tlen -= plen;\n\t\t}\n\t}\n\n\tto->truesize += len + plen;\n\tto->len += len + plen;\n\tto->data_len += len + plen;\n\n\tif (unlikely(skb_orphan_frags(from, GFP_ATOMIC))) {\n\t\tskb_tx_error(from);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++) {\n\t\tif (!len)\n\t\t\tbreak;\n\t\tskb_shinfo(to)->frags[j] = skb_shinfo(from)->frags[i];\n\t\tskb_shinfo(to)->frags[j].size = min_t(int, skb_shinfo(to)->frags[j].size, len);\n\t\tlen -= skb_shinfo(to)->frags[j].size;\n\t\tskb_frag_ref(to, j);\n\t\tj++;\n\t}\n\tskb_shinfo(to)->nr_frags = j;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy);\n\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to)\n{\n\t__wsum csum;\n\tlong csstart;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tcsstart = skb_checksum_start_offset(skb);\n\telse\n\t\tcsstart = skb_headlen(skb);\n\n\tBUG_ON(csstart > skb_headlen(skb));\n\n\tskb_copy_from_linear_data(skb, to, csstart);\n\n\tcsum = 0;\n\tif (csstart != skb->len)\n\t\tcsum = skb_copy_and_csum_bits(skb, csstart, to + csstart,\n\t\t\t\t\t      skb->len - csstart, 0);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tlong csstuff = csstart + skb->csum_offset;\n\n\t\t*((__sum16 *)(to + csstuff)) = csum_fold(csum);\n\t}\n}\nEXPORT_SYMBOL(skb_copy_and_csum_dev);\n\n/**\n *\tskb_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The head item is\n *\treturned or %NULL if the list is empty.\n */\n\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue);\n\n/**\n *\tskb_dequeue_tail - remove from the tail of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the tail of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The tail item is\n *\treturned or %NULL if the list is empty.\n */\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue_tail(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue_tail);\n\n/**\n *\tskb_queue_purge - empty a list\n *\t@list: list to empty\n *\n *\tDelete all buffers on an &sk_buff list. Each buffer is removed from\n *\tthe list and one reference dropped. This function takes the list\n *\tlock and is atomic with respect to other list locking functions.\n */\nvoid skb_queue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\twhile ((skb = skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL(skb_queue_purge);\n\n/**\n *\tskb_rbtree_purge - empty a skb rbtree\n *\t@root: root of the rbtree to empty\n *\n *\tDelete all buffers on an &sk_buff rbtree. Each buffer is removed from\n *\tthe list and one reference dropped. This function does not take\n *\tany lock. Synchronization should be handled by the caller (e.g., TCP\n *\tout-of-order queue is protected by the socket lock).\n */\nvoid skb_rbtree_purge(struct rb_root *root)\n{\n\tstruct sk_buff *skb, *next;\n\n\trbtree_postorder_for_each_entry_safe(skb, next, root, rbnode)\n\t\tkfree_skb(skb);\n\n\t*root = RB_ROOT;\n}\n\n/**\n *\tskb_queue_head - queue a buffer at the list head\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the start of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_head(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_head);\n\n/**\n *\tskb_queue_tail - queue a buffer at the list tail\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the tail of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_tail(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_tail);\n\n/**\n *\tskb_unlink\t-\tremove a buffer from a list\n *\t@skb: buffer to remove\n *\t@list: list to use\n *\n *\tRemove a packet from a list. The list locks are taken and this\n *\tfunction is atomic with respect to other list locked calls\n *\n *\tYou must know what list the SKB is on.\n */\nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_unlink(skb, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_unlink);\n\n/**\n *\tskb_append\t-\tappend a buffer\n *\t@old: buffer to insert after\n *\t@newsk: buffer to insert\n *\t@list: list to use\n *\n *\tPlace a packet after a given packet in a list. The list locks are taken\n *\tand this function is atomic with respect to other list locked calls.\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_after(list, old, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_append);\n\n/**\n *\tskb_insert\t-\tinsert a buffer\n *\t@old: buffer to insert before\n *\t@newsk: buffer to insert\n *\t@list: list to use\n *\n *\tPlace a packet before a given packet in a list. The list locks are\n * \ttaken and this function is atomic with respect to other list locked\n *\tcalls.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_insert(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_insert(newsk, old->prev, old, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_insert);\n\nstatic inline void skb_split_inside_header(struct sk_buff *skb,\n\t\t\t\t\t   struct sk_buff* skb1,\n\t\t\t\t\t   const u32 len, const int pos)\n{\n\tint i;\n\n\tskb_copy_from_linear_data_offset(skb, len, skb_put(skb1, pos - len),\n\t\t\t\t\t pos - len);\n\t/* And move data appendix as is. */\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\tskb_shinfo(skb1)->frags[i] = skb_shinfo(skb)->frags[i];\n\n\tskb_shinfo(skb1)->nr_frags = skb_shinfo(skb)->nr_frags;\n\tskb_shinfo(skb)->nr_frags  = 0;\n\tskb1->data_len\t\t   = skb->data_len;\n\tskb1->len\t\t   += skb1->data_len;\n\tskb->data_len\t\t   = 0;\n\tskb->len\t\t   = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void skb_split_no_header(struct sk_buff *skb,\n\t\t\t\t       struct sk_buff* skb1,\n\t\t\t\t       const u32 len, int pos)\n{\n\tint i, k = 0;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\n\tskb_shinfo(skb)->nr_frags = 0;\n\tskb1->len\t\t  = skb1->data_len = skb->len - len;\n\tskb->len\t\t  = len;\n\tskb->data_len\t\t  = len - pos;\n\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + size > len) {\n\t\t\tskb_shinfo(skb1)->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < len) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tskb_frag_ref(skb, i);\n\t\t\t\tskb_shinfo(skb1)->frags[0].page_offset += len - pos;\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb1)->frags[0], len - pos);\n\t\t\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i], len - pos);\n\t\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\t\t}\n\t\t\tk++;\n\t\t} else\n\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\tpos += size;\n\t}\n\tskb_shinfo(skb1)->nr_frags = k;\n}\n\n/**\n * skb_split - Split fragmented skb to two parts at length len.\n * @skb: the buffer to split\n * @skb1: the buffer to receive the second part\n * @len: new length for skb\n */\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len)\n{\n\tint pos = skb_headlen(skb);\n\n\tskb_shinfo(skb1)->tx_flags = skb_shinfo(skb)->tx_flags & SKBTX_SHARED_FRAG;\n\tif (len < pos)\t/* Split line is inside header. */\n\t\tskb_split_inside_header(skb, skb1, len, pos);\n\telse\t\t/* Second chunk has no header, nothing to copy. */\n\t\tskb_split_no_header(skb, skb1, len, pos);\n}\nEXPORT_SYMBOL(skb_split);\n\n/* Shifting from/to a cloned skb is a no-go.\n *\n * Caller cannot keep skb_shinfo related pointers past calling here!\n */\nstatic int skb_prepare_for_shift(struct sk_buff *skb)\n{\n\treturn skb_cloned(skb) && pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\n\n/**\n * skb_shift - Shifts paged data partially from skb to another\n * @tgt: buffer into which tail data gets added\n * @skb: buffer from which the paged data comes from\n * @shiftlen: shift up to this many bytes\n *\n * Attempts to shift up to shiftlen worth of bytes, which may be less than\n * the length of the skb, from skb to tgt. Returns number bytes shifted.\n * It's up to caller to free skb if everything was shifted.\n *\n * If @tgt runs out of frags, the whole operation is aborted.\n *\n * Skb cannot include anything else but paged data while tgt is allowed\n * to have non-paged data as well.\n *\n * TODO: full sized shift could be optimized but that would need\n * specialized skb free'er to handle frags without up-to-date nr_frags.\n */\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen)\n{\n\tint from, to, merge, todo;\n\tstruct skb_frag_struct *fragfrom, *fragto;\n\n\tBUG_ON(shiftlen > skb->len);\n\n\tif (skb_headlen(skb))\n\t\treturn 0;\n\n\ttodo = shiftlen;\n\tfrom = 0;\n\tto = skb_shinfo(tgt)->nr_frags;\n\tfragfrom = &skb_shinfo(skb)->frags[from];\n\n\t/* Actual merge is delayed until the point when we know we can\n\t * commit all, so that we don't have to undo partial changes\n\t */\n\tif (!to ||\n\t    !skb_can_coalesce(tgt, to, skb_frag_page(fragfrom),\n\t\t\t      fragfrom->page_offset)) {\n\t\tmerge = -1;\n\t} else {\n\t\tmerge = to - 1;\n\n\t\ttodo -= skb_frag_size(fragfrom);\n\t\tif (todo < 0) {\n\t\t\tif (skb_prepare_for_shift(skb) ||\n\t\t\t    skb_prepare_for_shift(tgt))\n\t\t\t\treturn 0;\n\n\t\t\t/* All previous frag pointers might be stale! */\n\t\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\t\tskb_frag_size_add(fragto, shiftlen);\n\t\t\tskb_frag_size_sub(fragfrom, shiftlen);\n\t\t\tfragfrom->page_offset += shiftlen;\n\n\t\t\tgoto onlymerged;\n\t\t}\n\n\t\tfrom++;\n\t}\n\n\t/* Skip full, not-fitting skb to avoid expensive operations */\n\tif ((shiftlen == skb->len) &&\n\t    (skb_shinfo(skb)->nr_frags - from) > (MAX_SKB_FRAGS - to))\n\t\treturn 0;\n\n\tif (skb_prepare_for_shift(skb) || skb_prepare_for_shift(tgt))\n\t\treturn 0;\n\n\twhile ((todo > 0) && (from < skb_shinfo(skb)->nr_frags)) {\n\t\tif (to == MAX_SKB_FRAGS)\n\t\t\treturn 0;\n\n\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\tfragto = &skb_shinfo(tgt)->frags[to];\n\n\t\tif (todo >= skb_frag_size(fragfrom)) {\n\t\t\t*fragto = *fragfrom;\n\t\t\ttodo -= skb_frag_size(fragfrom);\n\t\t\tfrom++;\n\t\t\tto++;\n\n\t\t} else {\n\t\t\t__skb_frag_ref(fragfrom);\n\t\t\tfragto->page = fragfrom->page;\n\t\t\tfragto->page_offset = fragfrom->page_offset;\n\t\t\tskb_frag_size_set(fragto, todo);\n\n\t\t\tfragfrom->page_offset += todo;\n\t\t\tskb_frag_size_sub(fragfrom, todo);\n\t\t\ttodo = 0;\n\n\t\t\tto++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ready to \"commit\" this state change to tgt */\n\tskb_shinfo(tgt)->nr_frags = to;\n\n\tif (merge >= 0) {\n\t\tfragfrom = &skb_shinfo(skb)->frags[0];\n\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\tskb_frag_size_add(fragto, skb_frag_size(fragfrom));\n\t\t__skb_frag_unref(fragfrom);\n\t}\n\n\t/* Reposition in the original skb */\n\tto = 0;\n\twhile (from < skb_shinfo(skb)->nr_frags)\n\t\tskb_shinfo(skb)->frags[to++] = skb_shinfo(skb)->frags[from++];\n\tskb_shinfo(skb)->nr_frags = to;\n\n\tBUG_ON(todo > 0 && !skb_shinfo(skb)->nr_frags);\n\nonlymerged:\n\t/* Most likely the tgt won't ever need its checksum anymore, skb on\n\t * the other hand might need it if it needs to be resent\n\t */\n\ttgt->ip_summed = CHECKSUM_PARTIAL;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t/* Yak, is it really working this way? Some helper please? */\n\tskb->len -= shiftlen;\n\tskb->data_len -= shiftlen;\n\tskb->truesize -= shiftlen;\n\ttgt->len += shiftlen;\n\ttgt->data_len += shiftlen;\n\ttgt->truesize += shiftlen;\n\n\treturn shiftlen;\n}\n\n/**\n * skb_prepare_seq_read - Prepare a sequential read of skb data\n * @skb: the buffer to read\n * @from: lower offset of data to be read\n * @to: upper offset of data to be read\n * @st: state variable\n *\n * Initializes the specified state variable. Must be called before\n * invoking skb_seq_read() for the first time.\n */\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st)\n{\n\tst->lower_offset = from;\n\tst->upper_offset = to;\n\tst->root_skb = st->cur_skb = skb;\n\tst->frag_idx = st->stepped_offset = 0;\n\tst->frag_data = NULL;\n}\nEXPORT_SYMBOL(skb_prepare_seq_read);\n\n/**\n * skb_seq_read - Sequentially read skb data\n * @consumed: number of bytes consumed by the caller so far\n * @data: destination pointer for data to be returned\n * @st: state variable\n *\n * Reads a block of skb data at @consumed relative to the\n * lower offset specified to skb_prepare_seq_read(). Assigns\n * the head of the data block to @data and returns the length\n * of the block or 0 if the end of the skb data or the upper\n * offset has been reached.\n *\n * The caller is not required to consume all of the data\n * returned, i.e. @consumed is typically set to the number\n * of bytes already consumed and the next call to\n * skb_seq_read() will return the remaining part of the block.\n *\n * Note 1: The size of each block of data returned can be arbitrary,\n *       this limitation is the cost for zerocopy sequential\n *       reads of potentially non linear data.\n *\n * Note 2: Fragment lists within fragments are not implemented\n *       at the moment, state->root_skb could be replaced with\n *       a stack for this purpose.\n */\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st)\n{\n\tunsigned int block_limit, abs_offset = consumed + st->lower_offset;\n\tskb_frag_t *frag;\n\n\tif (unlikely(abs_offset >= st->upper_offset)) {\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\nnext_skb:\n\tblock_limit = skb_headlen(st->cur_skb) + st->stepped_offset;\n\n\tif (abs_offset < block_limit && !st->frag_data) {\n\t\t*data = st->cur_skb->data + (abs_offset - st->stepped_offset);\n\t\treturn block_limit - abs_offset;\n\t}\n\n\tif (st->frag_idx == 0 && !st->frag_data)\n\t\tst->stepped_offset += skb_headlen(st->cur_skb);\n\n\twhile (st->frag_idx < skb_shinfo(st->cur_skb)->nr_frags) {\n\t\tfrag = &skb_shinfo(st->cur_skb)->frags[st->frag_idx];\n\t\tblock_limit = skb_frag_size(frag) + st->stepped_offset;\n\n\t\tif (abs_offset < block_limit) {\n\t\t\tif (!st->frag_data)\n\t\t\t\tst->frag_data = kmap_atomic(skb_frag_page(frag));\n\n\t\t\t*data = (u8 *) st->frag_data + frag->page_offset +\n\t\t\t\t(abs_offset - st->stepped_offset);\n\n\t\t\treturn block_limit - abs_offset;\n\t\t}\n\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\n\t\tst->frag_idx++;\n\t\tst->stepped_offset += skb_frag_size(frag);\n\t}\n\n\tif (st->frag_data) {\n\t\tkunmap_atomic(st->frag_data);\n\t\tst->frag_data = NULL;\n\t}\n\n\tif (st->root_skb == st->cur_skb && skb_has_frag_list(st->root_skb)) {\n\t\tst->cur_skb = skb_shinfo(st->root_skb)->frag_list;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t} else if (st->cur_skb->next) {\n\t\tst->cur_skb = st->cur_skb->next;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_seq_read);\n\n/**\n * skb_abort_seq_read - Abort a sequential read of skb data\n * @st: state variable\n *\n * Must be called if skb_seq_read() was not called until it\n * returned 0.\n */\nvoid skb_abort_seq_read(struct skb_seq_state *st)\n{\n\tif (st->frag_data)\n\t\tkunmap_atomic(st->frag_data);\n}\nEXPORT_SYMBOL(skb_abort_seq_read);\n\n#define TS_SKB_CB(state)\t((struct skb_seq_state *) &((state)->cb))\n\nstatic unsigned int skb_ts_get_next_block(unsigned int offset, const u8 **text,\n\t\t\t\t\t  struct ts_config *conf,\n\t\t\t\t\t  struct ts_state *state)\n{\n\treturn skb_seq_read(offset, text, TS_SKB_CB(state));\n}\n\nstatic void skb_ts_finish(struct ts_config *conf, struct ts_state *state)\n{\n\tskb_abort_seq_read(TS_SKB_CB(state));\n}\n\n/**\n * skb_find_text - Find a text pattern in skb data\n * @skb: the buffer to look in\n * @from: search offset\n * @to: search limit\n * @config: textsearch configuration\n *\n * Finds a pattern in the skb data according to the specified\n * textsearch configuration. Use textsearch_next() to retrieve\n * subsequent occurrences of the pattern. Returns the offset\n * to the first occurrence or UINT_MAX if no match was found.\n */\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config)\n{\n\tstruct ts_state state;\n\tunsigned int ret;\n\n\tconfig->get_next_block = skb_ts_get_next_block;\n\tconfig->finish = skb_ts_finish;\n\n\tskb_prepare_seq_read(skb, from, to, TS_SKB_CB(&state));\n\n\tret = textsearch_find(config, &state);\n\treturn (ret <= to - from ? ret : UINT_MAX);\n}\nEXPORT_SYMBOL(skb_find_text);\n\n/**\n * skb_append_datato_frags - append the user data to a skb\n * @sk: sock  structure\n * @skb: skb structure to be appended with user data.\n * @getfrag: call back function to be used for getting the user data\n * @from: pointer to user message iov\n * @length: length of the iov message\n *\n * Description: This procedure append the user data in the fragment part\n * of the skb if any page alloc fails user this procedure returns  -ENOMEM\n */\nint skb_append_datato_frags(struct sock *sk, struct sk_buff *skb,\n\t\t\tint (*getfrag)(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length)\n{\n\tint frg_cnt = skb_shinfo(skb)->nr_frags;\n\tint copy;\n\tint offset = 0;\n\tint ret;\n\tstruct page_frag *pfrag = &current->task_frag;\n\n\tdo {\n\t\t/* Return error if we don't have space for new frag */\n\t\tif (frg_cnt >= MAX_SKB_FRAGS)\n\t\t\treturn -EMSGSIZE;\n\n\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\treturn -ENOMEM;\n\n\t\t/* copy the user data to page */\n\t\tcopy = min_t(int, length, pfrag->size - pfrag->offset);\n\n\t\tret = getfrag(from, page_address(pfrag->page) + pfrag->offset,\n\t\t\t      offset, copy, 0, skb);\n\t\tif (ret < 0)\n\t\t\treturn -EFAULT;\n\n\t\t/* copy was successful so update the size parameters */\n\t\tskb_fill_page_desc(skb, frg_cnt, pfrag->page, pfrag->offset,\n\t\t\t\t   copy);\n\t\tfrg_cnt++;\n\t\tpfrag->offset += copy;\n\t\tget_page(pfrag->page);\n\n\t\tskb->truesize += copy;\n\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\tskb->len += copy;\n\t\tskb->data_len += copy;\n\t\toffset += copy;\n\t\tlength -= copy;\n\n\t} while (length > 0);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_append_datato_frags);\n\nint skb_append_pagefrags(struct sk_buff *skb, struct page *page,\n\t\t\t int offset, size_t size)\n{\n\tint i = skb_shinfo(skb)->nr_frags;\n\n\tif (skb_can_coalesce(skb, i, page, offset)) {\n\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], size);\n\t} else if (i < MAX_SKB_FRAGS) {\n\t\tget_page(page);\n\t\tskb_fill_page_desc(skb, i, page, offset, size);\n\t} else {\n\t\treturn -EMSGSIZE;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_append_pagefrags);\n\n/**\n *\tskb_pull_rcsum - pull skb and update receive checksum\n *\t@skb: buffer to update\n *\t@len: length of data pulled\n *\n *\tThis function performs an skb_pull on the packet and updates\n *\tthe CHECKSUM_COMPLETE checksum.  It should be used on\n *\treceive path processing instead of skb_pull unless you know\n *\tthat the checksum difference is zero (e.g., a valid IP header)\n *\tor you are setting ip_summed to CHECKSUM_NONE.\n */\nunsigned char *skb_pull_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned char *data = skb->data;\n\n\tBUG_ON(len > skb->len);\n\t__skb_pull(skb, len);\n\tskb_postpull_rcsum(skb, data, len);\n\treturn skb->data;\n}\nEXPORT_SYMBOL_GPL(skb_pull_rcsum);\n\n/**\n *\tskb_segment - Perform protocol segmentation on skb.\n *\t@head_skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\n *\tThis function performs segmentation on the given skb.  It returns\n *\ta pointer to the first in a list of new skbs for the segments.\n *\tIn case of error it returns ERR_PTR(err).\n */\nstruct sk_buff *skb_segment(struct sk_buff *head_skb,\n\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = NULL;\n\tstruct sk_buff *tail = NULL;\n\tstruct sk_buff *list_skb = skb_shinfo(head_skb)->frag_list;\n\tskb_frag_t *frag = skb_shinfo(head_skb)->frags;\n\tunsigned int mss = skb_shinfo(head_skb)->gso_size;\n\tunsigned int doffset = head_skb->data - skb_mac_header(head_skb);\n\tstruct sk_buff *frag_skb = head_skb;\n\tunsigned int offset = doffset;\n\tunsigned int tnl_hlen = skb_tnl_header_len(head_skb);\n\tunsigned int partial_segs = 0;\n\tunsigned int headroom;\n\tunsigned int len = head_skb->len;\n\t__be16 proto;\n\tbool csum, sg;\n\tint nfrags = skb_shinfo(head_skb)->nr_frags;\n\tint err = -ENOMEM;\n\tint i = 0;\n\tint pos;\n\tint dummy;\n\n\t__skb_push(head_skb, doffset);\n\tproto = skb_network_protocol(head_skb, &dummy);\n\tif (unlikely(!proto))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tsg = !!(features & NETIF_F_SG);\n\tcsum = !!can_checksum_protocol(features, proto);\n\n\tif (sg && csum && (mss != GSO_BY_FRAGS))  {\n\t\tif (!(features & NETIF_F_GSO_PARTIAL)) {\n\t\t\tstruct sk_buff *iter;\n\n\t\t\tif (!list_skb ||\n\t\t\t    !net_gso_ok(features, skb_shinfo(head_skb)->gso_type))\n\t\t\t\tgoto normal;\n\n\t\t\t/* Split the buffer at the frag_list pointer.\n\t\t\t * This is based on the assumption that all\n\t\t\t * buffers in the chain excluding the last\n\t\t\t * containing the same amount of data.\n\t\t\t */\n\t\t\tskb_walk_frags(head_skb, iter) {\n\t\t\t\tif (skb_headlen(iter))\n\t\t\t\t\tgoto normal;\n\n\t\t\t\tlen -= iter->len;\n\t\t\t}\n\t\t}\n\n\t\t/* GSO partial only requires that we trim off any excess that\n\t\t * doesn't fit into an MSS sized block, so take care of that\n\t\t * now.\n\t\t */\n\t\tpartial_segs = len / mss;\n\t\tif (partial_segs > 1)\n\t\t\tmss *= partial_segs;\n\t\telse\n\t\t\tpartial_segs = 0;\n\t}\n\nnormal:\n\theadroom = skb_headroom(head_skb);\n\tpos = skb_headlen(head_skb);\n\n\tdo {\n\t\tstruct sk_buff *nskb;\n\t\tskb_frag_t *nskb_frag;\n\t\tint hsize;\n\t\tint size;\n\n\t\tif (unlikely(mss == GSO_BY_FRAGS)) {\n\t\t\tlen = list_skb->len;\n\t\t} else {\n\t\t\tlen = head_skb->len - offset;\n\t\t\tif (len > mss)\n\t\t\t\tlen = mss;\n\t\t}\n\n\t\thsize = skb_headlen(head_skb) - offset;\n\t\tif (hsize < 0)\n\t\t\thsize = 0;\n\t\tif (hsize > len || !sg)\n\t\t\thsize = len;\n\n\t\tif (!hsize && i >= nfrags && skb_headlen(list_skb) &&\n\t\t    (skb_headlen(list_skb) == len || sg)) {\n\t\t\tBUG_ON(skb_headlen(list_skb) > len);\n\n\t\t\ti = 0;\n\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\tfrag_skb = list_skb;\n\t\t\tpos += skb_headlen(list_skb);\n\n\t\t\twhile (pos < offset + len) {\n\t\t\t\tBUG_ON(i >= nfrags);\n\n\t\t\t\tsize = skb_frag_size(frag);\n\t\t\t\tif (pos + size > offset + len)\n\t\t\t\t\tbreak;\n\n\t\t\t\ti++;\n\t\t\t\tpos += size;\n\t\t\t\tfrag++;\n\t\t\t}\n\n\t\t\tnskb = skb_clone(list_skb, GFP_ATOMIC);\n\t\t\tlist_skb = list_skb->next;\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tif (unlikely(pskb_trim(nskb, len))) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\thsize = skb_end_offset(nskb);\n\t\t\tif (skb_cow_head(nskb, doffset + headroom)) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tnskb->truesize += skb_end_offset(nskb) - hsize;\n\t\t\tskb_release_head_state(nskb);\n\t\t\t__skb_push(nskb, doffset);\n\t\t} else {\n\t\t\tnskb = __alloc_skb(hsize + doffset + headroom,\n\t\t\t\t\t   GFP_ATOMIC, skb_alloc_rx_flag(head_skb),\n\t\t\t\t\t   NUMA_NO_NODE);\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tskb_reserve(nskb, headroom);\n\t\t\t__skb_put(nskb, doffset);\n\t\t}\n\n\t\tif (segs)\n\t\t\ttail->next = nskb;\n\t\telse\n\t\t\tsegs = nskb;\n\t\ttail = nskb;\n\n\t\t__copy_skb_header(nskb, head_skb);\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - headroom);\n\t\tskb_reset_mac_len(nskb);\n\n\t\tskb_copy_from_linear_data_offset(head_skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t doffset + tnl_hlen);\n\n\t\tif (nskb->len == len + doffset)\n\t\t\tgoto perform_csum_check;\n\n\t\tif (!sg) {\n\t\t\tif (!nskb->remcsum_offload)\n\t\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tSKB_GSO_CB(nskb)->csum =\n\t\t\t\tskb_copy_and_csum_bits(head_skb, offset,\n\t\t\t\t\t\t       skb_put(nskb, len),\n\t\t\t\t\t\t       len, 0);\n\t\t\tSKB_GSO_CB(nskb)->csum_start =\n\t\t\t\tskb_headroom(nskb) + doffset;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnskb_frag = skb_shinfo(nskb)->frags;\n\n\t\tskb_copy_from_linear_data_offset(head_skb, offset,\n\t\t\t\t\t\t skb_put(nskb, hsize), hsize);\n\n\t\tskb_shinfo(nskb)->tx_flags = skb_shinfo(head_skb)->tx_flags &\n\t\t\tSKBTX_SHARED_FRAG;\n\n\t\twhile (pos < offset + len) {\n\t\t\tif (i >= nfrags) {\n\t\t\t\tBUG_ON(skb_headlen(list_skb));\n\n\t\t\t\ti = 0;\n\t\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\t\tfrag_skb = list_skb;\n\n\t\t\t\tBUG_ON(!nfrags);\n\n\t\t\t\tlist_skb = list_skb->next;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_shinfo(nskb)->nr_frags >=\n\t\t\t\t     MAX_SKB_FRAGS)) {\n\t\t\t\tnet_warn_ratelimited(\n\t\t\t\t\t\"skb_segment: too many frags: %u %u\\n\",\n\t\t\t\t\tpos, mss);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_orphan_frags(frag_skb, GFP_ATOMIC)))\n\t\t\t\tgoto err;\n\n\t\t\t*nskb_frag = *frag;\n\t\t\t__skb_frag_ref(nskb_frag);\n\t\t\tsize = skb_frag_size(nskb_frag);\n\n\t\t\tif (pos < offset) {\n\t\t\t\tnskb_frag->page_offset += offset - pos;\n\t\t\t\tskb_frag_size_sub(nskb_frag, offset - pos);\n\t\t\t}\n\n\t\t\tskb_shinfo(nskb)->nr_frags++;\n\n\t\t\tif (pos + size <= offset + len) {\n\t\t\t\ti++;\n\t\t\t\tfrag++;\n\t\t\t\tpos += size;\n\t\t\t} else {\n\t\t\t\tskb_frag_size_sub(nskb_frag, pos + size - (offset + len));\n\t\t\t\tgoto skip_fraglist;\n\t\t\t}\n\n\t\t\tnskb_frag++;\n\t\t}\n\nskip_fraglist:\n\t\tnskb->data_len = len - hsize;\n\t\tnskb->len += nskb->data_len;\n\t\tnskb->truesize += nskb->data_len;\n\nperform_csum_check:\n\t\tif (!csum) {\n\t\t\tif (skb_has_shared_frag(nskb)) {\n\t\t\t\terr = __skb_linearize(nskb);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif (!nskb->remcsum_offload)\n\t\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tSKB_GSO_CB(nskb)->csum =\n\t\t\t\tskb_checksum(nskb, doffset,\n\t\t\t\t\t     nskb->len - doffset, 0);\n\t\t\tSKB_GSO_CB(nskb)->csum_start =\n\t\t\t\tskb_headroom(nskb) + doffset;\n\t\t}\n\t} while ((offset += len) < head_skb->len);\n\n\t/* Some callers want to get the end of the list.\n\t * Put it in segs->prev to avoid walking the list.\n\t * (see validate_xmit_skb_list() for example)\n\t */\n\tsegs->prev = tail;\n\n\tif (partial_segs) {\n\t\tstruct sk_buff *iter;\n\t\tint type = skb_shinfo(head_skb)->gso_type;\n\t\tunsigned short gso_size = skb_shinfo(head_skb)->gso_size;\n\n\t\t/* Update type to add partial and then remove dodgy if set */\n\t\ttype |= (features & NETIF_F_GSO_PARTIAL) / NETIF_F_GSO_PARTIAL * SKB_GSO_PARTIAL;\n\t\ttype &= ~SKB_GSO_DODGY;\n\n\t\t/* Update GSO info and prepare to start updating headers on\n\t\t * our way back down the stack of protocols.\n\t\t */\n\t\tfor (iter = segs; iter; iter = iter->next) {\n\t\t\tskb_shinfo(iter)->gso_size = gso_size;\n\t\t\tskb_shinfo(iter)->gso_segs = partial_segs;\n\t\t\tskb_shinfo(iter)->gso_type = type;\n\t\t\tSKB_GSO_CB(iter)->data_offset = skb_headroom(iter) + doffset;\n\t\t}\n\n\t\tif (tail->len - doffset <= gso_size)\n\t\t\tskb_shinfo(tail)->gso_size = 0;\n\t\telse if (tail != segs)\n\t\t\tskb_shinfo(tail)->gso_segs = DIV_ROUND_UP(tail->len - doffset, gso_size);\n\t}\n\n\t/* Following permits correct backpressure, for protocols\n\t * using skb_set_owner_w().\n\t * Idea is to tranfert ownership from head_skb to last segment.\n\t */\n\tif (head_skb->destructor == sock_wfree) {\n\t\tswap(tail->truesize, head_skb->truesize);\n\t\tswap(tail->destructor, head_skb->destructor);\n\t\tswap(tail->sk, head_skb->sk);\n\t}\n\treturn segs;\n\nerr:\n\tkfree_skb_list(segs);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(skb_segment);\n\nint skb_gro_receive(struct sk_buff **head, struct sk_buff *skb)\n{\n\tstruct skb_shared_info *pinfo, *skbinfo = skb_shinfo(skb);\n\tunsigned int offset = skb_gro_offset(skb);\n\tunsigned int headlen = skb_headlen(skb);\n\tunsigned int len = skb_gro_len(skb);\n\tstruct sk_buff *lp, *p = *head;\n\tunsigned int delta_truesize;\n\n\tif (unlikely(p->len + len >= 65536))\n\t\treturn -E2BIG;\n\n\tlp = NAPI_GRO_CB(p)->last;\n\tpinfo = skb_shinfo(lp);\n\n\tif (headlen <= offset) {\n\t\tskb_frag_t *frag;\n\t\tskb_frag_t *frag2;\n\t\tint i = skbinfo->nr_frags;\n\t\tint nr_frags = pinfo->nr_frags + i;\n\n\t\tif (nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\toffset -= headlen;\n\t\tpinfo->nr_frags = nr_frags;\n\t\tskbinfo->nr_frags = 0;\n\n\t\tfrag = pinfo->frags + nr_frags;\n\t\tfrag2 = skbinfo->frags + i;\n\t\tdo {\n\t\t\t*--frag = *--frag2;\n\t\t} while (--i);\n\n\t\tfrag->page_offset += offset;\n\t\tskb_frag_size_sub(frag, offset);\n\n\t\t/* all fragments truesize : remove (head size + sk_buff) */\n\t\tdelta_truesize = skb->truesize -\n\t\t\t\t SKB_TRUESIZE(skb_end_offset(skb));\n\n\t\tskb->truesize -= skb->data_len;\n\t\tskb->len -= skb->data_len;\n\t\tskb->data_len = 0;\n\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE;\n\t\tgoto done;\n\t} else if (skb->head_frag) {\n\t\tint nr_frags = pinfo->nr_frags;\n\t\tskb_frag_t *frag = pinfo->frags + nr_frags;\n\t\tstruct page *page = virt_to_head_page(skb->head);\n\t\tunsigned int first_size = headlen - offset;\n\t\tunsigned int first_offset;\n\n\t\tif (nr_frags + 1 + skbinfo->nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\tfirst_offset = skb->data -\n\t\t\t       (unsigned char *)page_address(page) +\n\t\t\t       offset;\n\n\t\tpinfo->nr_frags = nr_frags + 1 + skbinfo->nr_frags;\n\n\t\tfrag->page.p\t  = page;\n\t\tfrag->page_offset = first_offset;\n\t\tskb_frag_size_set(frag, first_size);\n\n\t\tmemcpy(frag + 1, skbinfo->frags, sizeof(*frag) * skbinfo->nr_frags);\n\t\t/* We dont need to clear skbinfo->nr_frags here */\n\n\t\tdelta_truesize = skb->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE_STOLEN_HEAD;\n\t\tgoto done;\n\t}\n\nmerge:\n\tdelta_truesize = skb->truesize;\n\tif (offset > headlen) {\n\t\tunsigned int eat = offset - headlen;\n\n\t\tskbinfo->frags[0].page_offset += eat;\n\t\tskb_frag_size_sub(&skbinfo->frags[0], eat);\n\t\tskb->data_len -= eat;\n\t\tskb->len -= eat;\n\t\toffset = headlen;\n\t}\n\n\t__skb_pull(skb, offset);\n\n\tif (NAPI_GRO_CB(p)->last == p)\n\t\tskb_shinfo(p)->frag_list = skb;\n\telse\n\t\tNAPI_GRO_CB(p)->last->next = skb;\n\tNAPI_GRO_CB(p)->last = skb;\n\t__skb_header_release(skb);\n\tlp = p;\n\ndone:\n\tNAPI_GRO_CB(p)->count++;\n\tp->data_len += len;\n\tp->truesize += delta_truesize;\n\tp->len += len;\n\tif (lp != p) {\n\t\tlp->data_len += len;\n\t\tlp->truesize += delta_truesize;\n\t\tlp->len += len;\n\t}\n\tNAPI_GRO_CB(skb)->same_flow = 1;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_gro_receive);\n\nvoid __init skb_init(void)\n{\n\tskbuff_head_cache = kmem_cache_create(\"skbuff_head_cache\",\n\t\t\t\t\t      sizeof(struct sk_buff),\n\t\t\t\t\t      0,\n\t\t\t\t\t      SLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t      NULL);\n\tskbuff_fclone_cache = kmem_cache_create(\"skbuff_fclone_cache\",\n\t\t\t\t\t\tsizeof(struct sk_buff_fclones),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tNULL);\n}\n\n/**\n *\tskb_to_sgvec - Fill a scatter-gather list from a socket buffer\n *\t@skb: Socket buffer containing the buffers to be mapped\n *\t@sg: The scatter-gather list to map into\n *\t@offset: The offset into the buffer's contents to start mapping\n *\t@len: Length of buffer space to be mapped\n *\n *\tFill the specified scatter-gather list with mappings/pointers into a\n *\tregion of the buffer space attached to a socket buffer.\n */\nstatic int\n__skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint elt = 0;\n\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tsg_set_buf(sg, skb->data + offset, copy);\n\t\telt++;\n\t\tif ((len -= copy) == 0)\n\t\t\treturn elt;\n\t\toffset += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tsg_set_page(&sg[elt], skb_frag_page(frag), copy,\n\t\t\t\t\tfrag->page_offset+offset-start);\n\t\t\telt++;\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\telt += __skb_to_sgvec(frag_iter, sg+elt, offset - start,\n\t\t\t\t\t      copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn elt;\n}\n\n/* As compared with skb_to_sgvec, skb_to_sgvec_nomark only map skb to given\n * sglist without mark the sg which contain last skb data as the end.\n * So the caller can mannipulate sg list as will when padding new data after\n * the first call without calling sg_unmark_end to expend sg list.\n *\n * Scenario to use skb_to_sgvec_nomark:\n * 1. sg_init_table\n * 2. skb_to_sgvec_nomark(payload1)\n * 3. skb_to_sgvec_nomark(payload2)\n *\n * This is equivalent to:\n * 1. sg_init_table\n * 2. skb_to_sgvec(payload1)\n * 3. sg_unmark_end\n * 4. skb_to_sgvec(payload2)\n *\n * When mapping mutilple payload conditionally, skb_to_sgvec_nomark\n * is more preferable.\n */\nint skb_to_sgvec_nomark(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\tint offset, int len)\n{\n\treturn __skb_to_sgvec(skb, sg, offset, len);\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec_nomark);\n\nint skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint nsg = __skb_to_sgvec(skb, sg, offset, len);\n\n\tsg_mark_end(&sg[nsg - 1]);\n\n\treturn nsg;\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec);\n\n/**\n *\tskb_cow_data - Check that a socket buffer's data buffers are writable\n *\t@skb: The socket buffer to check.\n *\t@tailbits: Amount of trailing space to be added\n *\t@trailer: Returned pointer to the skb where the @tailbits space begins\n *\n *\tMake sure that the data buffers attached to a socket buffer are\n *\twritable. If they are not, private copies are made of the data buffers\n *\tand the socket buffer is set to use these instead.\n *\n *\tIf @tailbits is given, make sure that there is space to write @tailbits\n *\tbytes of data beyond current end of socket buffer.  @trailer will be\n *\tset to point to the skb in which this space begins.\n *\n *\tThe number of scatterlist elements required to completely map the\n *\tCOW'd and extended socket buffer will be returned.\n */\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer)\n{\n\tint copyflag;\n\tint elt;\n\tstruct sk_buff *skb1, **skb_p;\n\n\t/* If skb is cloned or its head is paged, reallocate\n\t * head pulling out all the pages (pages are considered not writable\n\t * at the moment even if they are anonymous).\n\t */\n\tif ((skb_cloned(skb) || skb_shinfo(skb)->nr_frags) &&\n\t    __pskb_pull_tail(skb, skb_pagelen(skb)-skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\t/* Easy case. Most of packets will go this way. */\n\tif (!skb_has_frag_list(skb)) {\n\t\t/* A little of trouble, not enough of space for trailer.\n\t\t * This should not happen, when stack is tuned to generate\n\t\t * good frames. OK, on miss we reallocate and reserve even more\n\t\t * space, 128 bytes is fair. */\n\n\t\tif (skb_tailroom(skb) < tailbits &&\n\t\t    pskb_expand_head(skb, 0, tailbits-skb_tailroom(skb)+128, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\t/* Voila! */\n\t\t*trailer = skb;\n\t\treturn 1;\n\t}\n\n\t/* Misery. We are in troubles, going to mincer fragments... */\n\n\telt = 1;\n\tskb_p = &skb_shinfo(skb)->frag_list;\n\tcopyflag = 0;\n\n\twhile ((skb1 = *skb_p) != NULL) {\n\t\tint ntail = 0;\n\n\t\t/* The fragment is partially pulled by someone,\n\t\t * this can happen on input. Copy it and everything\n\t\t * after it. */\n\n\t\tif (skb_shared(skb1))\n\t\t\tcopyflag = 1;\n\n\t\t/* If the skb is the last, worry about trailer. */\n\n\t\tif (skb1->next == NULL && tailbits) {\n\t\t\tif (skb_shinfo(skb1)->nr_frags ||\n\t\t\t    skb_has_frag_list(skb1) ||\n\t\t\t    skb_tailroom(skb1) < tailbits)\n\t\t\t\tntail = tailbits + 128;\n\t\t}\n\n\t\tif (copyflag ||\n\t\t    skb_cloned(skb1) ||\n\t\t    ntail ||\n\t\t    skb_shinfo(skb1)->nr_frags ||\n\t\t    skb_has_frag_list(skb1)) {\n\t\t\tstruct sk_buff *skb2;\n\n\t\t\t/* Fuck, we are miserable poor guys... */\n\t\t\tif (ntail == 0)\n\t\t\t\tskb2 = skb_copy(skb1, GFP_ATOMIC);\n\t\t\telse\n\t\t\t\tskb2 = skb_copy_expand(skb1,\n\t\t\t\t\t\t       skb_headroom(skb1),\n\t\t\t\t\t\t       ntail,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\t\tif (unlikely(skb2 == NULL))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (skb1->sk)\n\t\t\t\tskb_set_owner_w(skb2, skb1->sk);\n\n\t\t\t/* Looking around. Are we still alive?\n\t\t\t * OK, link new skb, drop old one */\n\n\t\t\tskb2->next = skb1->next;\n\t\t\t*skb_p = skb2;\n\t\t\tkfree_skb(skb1);\n\t\t\tskb1 = skb2;\n\t\t}\n\t\telt++;\n\t\t*trailer = skb1;\n\t\tskb_p = &skb1->next;\n\t}\n\n\treturn elt;\n}\nEXPORT_SYMBOL_GPL(skb_cow_data);\n\nstatic void sock_rmem_free(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tatomic_sub(skb->truesize, &sk->sk_rmem_alloc);\n}\n\nstatic void skb_set_err_queue(struct sk_buff *skb)\n{\n\t/* pkt_type of skbs received on local sockets is never PACKET_OUTGOING.\n\t * So, it is safe to (mis)use it to mark skbs on the error queue.\n\t */\n\tskb->pkt_type = PACKET_OUTGOING;\n\tBUILD_BUG_ON(PACKET_OUTGOING == 0);\n}\n\n/*\n * Note: We dont mem charge error packets (no sk_forward_alloc changes)\n */\nint sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned int)sk->sk_rcvbuf)\n\t\treturn -ENOMEM;\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_rmem_free;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\tskb_set_err_queue(skb);\n\n\t/* before exiting rcu section, make sure dst is refcounted */\n\tskb_dst_force(skb);\n\n\tskb_queue_tail(&sk->sk_error_queue, skb);\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_queue_err_skb);\n\nstatic bool is_icmp_err_skb(const struct sk_buff *skb)\n{\n\treturn skb && (SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_ICMP ||\n\t\t       SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_ICMP6);\n}\n\nstruct sk_buff *sock_dequeue_err_skb(struct sock *sk)\n{\n\tstruct sk_buff_head *q = &sk->sk_error_queue;\n\tstruct sk_buff *skb, *skb_next = NULL;\n\tbool icmp_next = false;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\tskb = __skb_dequeue(q);\n\tif (skb && (skb_next = skb_peek(q)))\n\t\ticmp_next = is_icmp_err_skb(skb_next);\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\tif (is_icmp_err_skb(skb) && !icmp_next)\n\t\tsk->sk_err = 0;\n\n\tif (skb_next)\n\t\tsk->sk_error_report(sk);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(sock_dequeue_err_skb);\n\n/**\n * skb_clone_sk - create clone of skb, and take reference to socket\n * @skb: the skb to clone\n *\n * This function creates a clone of a buffer that holds a reference on\n * sk_refcnt.  Buffers created via this function are meant to be\n * returned using sock_queue_err_skb, or free via kfree_skb.\n *\n * When passing buffers allocated with this function to sock_queue_err_skb\n * it is necessary to wrap the call with sock_hold/sock_put in order to\n * prevent the socket from being released prior to being enqueued on\n * the sk_error_queue.\n */\nstruct sk_buff *skb_clone_sk(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sk_buff *clone;\n\n\tif (!sk || !atomic_inc_not_zero(&sk->sk_refcnt))\n\t\treturn NULL;\n\n\tclone = skb_clone(skb, GFP_ATOMIC);\n\tif (!clone) {\n\t\tsock_put(sk);\n\t\treturn NULL;\n\t}\n\n\tclone->sk = sk;\n\tclone->destructor = sock_efree;\n\n\treturn clone;\n}\nEXPORT_SYMBOL(skb_clone_sk);\n\nstatic void __skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t\t\tstruct sock *sk,\n\t\t\t\t\tint tstype)\n{\n\tstruct sock_exterr_skb *serr;\n\tint err;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;\n\tserr->ee.ee_info = tstype;\n\tif (sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID) {\n\t\tserr->ee.ee_data = skb_shinfo(skb)->tskey;\n\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\tserr->ee.ee_data -= sk->sk_tskey;\n\t}\n\n\terr = sock_queue_err_skb(sk, skb);\n\n\tif (err)\n\t\tkfree_skb(skb);\n}\n\nstatic bool skb_may_tx_timestamp(struct sock *sk, bool tsonly)\n{\n\tbool ret;\n\n\tif (likely(sysctl_tstamp_allow_data || tsonly))\n\t\treturn true;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tret = sk->sk_socket && sk->sk_socket->file &&\n\t      file_ns_capable(sk->sk_socket->file, &init_user_ns, CAP_NET_RAW);\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn ret;\n}\n\nvoid skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps)\n{\n\tstruct sock *sk = skb->sk;\n\n\tif (!skb_may_tx_timestamp(sk, false))\n\t\treturn;\n\n\t/* Take a reference to prevent skb_orphan() from freeing the socket,\n\t * but only if the socket refcount is not zero.\n\t */\n\tif (likely(atomic_inc_not_zero(&sk->sk_refcnt))) {\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\t\t__skb_complete_tx_timestamp(skb, sk, SCM_TSTAMP_SND);\n\t\tsock_put(sk);\n\t}\n}\nEXPORT_SYMBOL_GPL(skb_complete_tx_timestamp);\n\nvoid __skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t     struct sock *sk, int tstype)\n{\n\tstruct sk_buff *skb;\n\tbool tsonly;\n\n\tif (!sk)\n\t\treturn;\n\n\ttsonly = sk->sk_tsflags & SOF_TIMESTAMPING_OPT_TSONLY;\n\tif (!skb_may_tx_timestamp(sk, tsonly))\n\t\treturn;\n\n\tif (tsonly) {\n#ifdef CONFIG_INET\n\t\tif ((sk->sk_tsflags & SOF_TIMESTAMPING_OPT_STATS) &&\n\t\t    sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\tskb = tcp_get_timestamping_opt_stats(sk);\n\t\telse\n#endif\n\t\t\tskb = alloc_skb(0, GFP_ATOMIC);\n\t} else {\n\t\tskb = skb_clone(orig_skb, GFP_ATOMIC);\n\t}\n\tif (!skb)\n\t\treturn;\n\n\tif (tsonly) {\n\t\tskb_shinfo(skb)->tx_flags = skb_shinfo(orig_skb)->tx_flags;\n\t\tskb_shinfo(skb)->tskey = skb_shinfo(orig_skb)->tskey;\n\t}\n\n\tif (hwtstamps)\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\telse\n\t\tskb->tstamp = ktime_get_real();\n\n\t__skb_complete_tx_timestamp(skb, sk, tstype);\n}\nEXPORT_SYMBOL_GPL(__skb_tstamp_tx);\n\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t   struct skb_shared_hwtstamps *hwtstamps)\n{\n\treturn __skb_tstamp_tx(orig_skb, hwtstamps, orig_skb->sk,\n\t\t\t       SCM_TSTAMP_SND);\n}\nEXPORT_SYMBOL_GPL(skb_tstamp_tx);\n\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sock_exterr_skb *serr;\n\tint err = 1;\n\n\tskb->wifi_acked_valid = 1;\n\tskb->wifi_acked = acked;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TXSTATUS;\n\n\t/* Take a reference to prevent skb_orphan() from freeing the socket,\n\t * but only if the socket refcount is not zero.\n\t */\n\tif (likely(atomic_inc_not_zero(&sk->sk_refcnt))) {\n\t\terr = sock_queue_err_skb(sk, skb);\n\t\tsock_put(sk);\n\t}\n\tif (err)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_complete_wifi_ack);\n\n/**\n * skb_partial_csum_set - set up and verify partial csum values for packet\n * @skb: the skb to set\n * @start: the number of bytes after skb->data to start checksumming.\n * @off: the offset from start to place the checksum.\n *\n * For untrusted partially-checksummed packets, we need to make sure the values\n * for skb->csum_start and skb->csum_offset are valid so we don't oops.\n *\n * This function checks and sets those values and skb->ip_summed: if this\n * returns false you should drop the packet.\n */\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off)\n{\n\tif (unlikely(start > skb_headlen(skb)) ||\n\t    unlikely((int)start + off > skb_headlen(skb) - 2)) {\n\t\tnet_warn_ratelimited(\"bad partial csum: csum=%u/%u len=%u\\n\",\n\t\t\t\t     start, off, skb_headlen(skb));\n\t\treturn false;\n\t}\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = skb_headroom(skb) + start;\n\tskb->csum_offset = off;\n\tskb_set_transport_header(skb, start);\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(skb_partial_csum_set);\n\nstatic int skb_maybe_pull_tail(struct sk_buff *skb, unsigned int len,\n\t\t\t       unsigned int max)\n{\n\tif (skb_headlen(skb) >= len)\n\t\treturn 0;\n\n\t/* If we need to pullup then pullup to the max, so we\n\t * won't need to do it again.\n\t */\n\tif (max > skb->len)\n\t\tmax = skb->len;\n\n\tif (__pskb_pull_tail(skb, max - skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\tif (skb_headlen(skb) < len)\n\t\treturn -EPROTO;\n\n\treturn 0;\n}\n\n#define MAX_TCP_HDR_LEN (15 * 4)\n\nstatic __sum16 *skb_checksum_setup_ip(struct sk_buff *skb,\n\t\t\t\t      typeof(IPPROTO_IP) proto,\n\t\t\t\t      unsigned int off)\n{\n\tswitch (proto) {\n\t\tint err;\n\n\tcase IPPROTO_TCP:\n\t\terr = skb_maybe_pull_tail(skb, off + sizeof(struct tcphdr),\n\t\t\t\t\t  off + MAX_TCP_HDR_LEN);\n\t\tif (!err && !skb_partial_csum_set(skb, off,\n\t\t\t\t\t\t  offsetof(struct tcphdr,\n\t\t\t\t\t\t\t   check)))\n\t\t\terr = -EPROTO;\n\t\treturn err ? ERR_PTR(err) : &tcp_hdr(skb)->check;\n\n\tcase IPPROTO_UDP:\n\t\terr = skb_maybe_pull_tail(skb, off + sizeof(struct udphdr),\n\t\t\t\t\t  off + sizeof(struct udphdr));\n\t\tif (!err && !skb_partial_csum_set(skb, off,\n\t\t\t\t\t\t  offsetof(struct udphdr,\n\t\t\t\t\t\t\t   check)))\n\t\t\terr = -EPROTO;\n\t\treturn err ? ERR_PTR(err) : &udp_hdr(skb)->check;\n\t}\n\n\treturn ERR_PTR(-EPROTO);\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * maximally sized IP and TCP or UDP headers.\n */\n#define MAX_IP_HDR_LEN 128\n\nstatic int skb_checksum_setup_ipv4(struct sk_buff *skb, bool recalculate)\n{\n\tunsigned int off;\n\tbool fragment;\n\t__sum16 *csum;\n\tint err;\n\n\tfragment = false;\n\n\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t  sizeof(struct iphdr),\n\t\t\t\t  MAX_IP_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tif (ip_hdr(skb)->frag_off & htons(IP_OFFSET | IP_MF))\n\t\tfragment = true;\n\n\toff = ip_hdrlen(skb);\n\n\terr = -EPROTO;\n\n\tif (fragment)\n\t\tgoto out;\n\n\tcsum = skb_checksum_setup_ip(skb, ip_hdr(skb)->protocol, off);\n\tif (IS_ERR(csum))\n\t\treturn PTR_ERR(csum);\n\n\tif (recalculate)\n\t\t*csum = ~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t   skb->len - off,\n\t\t\t\t\t   ip_hdr(skb)->protocol, 0);\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * an IPv6 header, all options, and a maximal TCP or UDP header.\n */\n#define MAX_IPV6_HDR_LEN 256\n\n#define OPT_HDR(type, skb, off) \\\n\t(type *)(skb_network_header(skb) + (off))\n\nstatic int skb_checksum_setup_ipv6(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\tu8 nexthdr;\n\tunsigned int off;\n\tunsigned int len;\n\tbool fragment;\n\tbool done;\n\t__sum16 *csum;\n\n\tfragment = false;\n\tdone = false;\n\n\toff = sizeof(struct ipv6hdr);\n\n\terr = skb_maybe_pull_tail(skb, off, MAX_IPV6_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tnexthdr = ipv6_hdr(skb)->nexthdr;\n\n\tlen = sizeof(struct ipv6hdr) + ntohs(ipv6_hdr(skb)->payload_len);\n\twhile (off <= len && !done) {\n\t\tswitch (nexthdr) {\n\t\tcase IPPROTO_DSTOPTS:\n\t\tcase IPPROTO_HOPOPTS:\n\t\tcase IPPROTO_ROUTING: {\n\t\t\tstruct ipv6_opt_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ipv6_opt_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ipv6_opt_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_optlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_AH: {\n\t\t\tstruct ip_auth_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ip_auth_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ip_auth_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_authlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_FRAGMENT: {\n\t\t\tstruct frag_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct frag_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct frag_hdr, skb, off);\n\n\t\t\tif (hp->frag_off & htons(IP6_OFFSET | IP6_MF))\n\t\t\t\tfragment = true;\n\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += sizeof(struct frag_hdr);\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tdone = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\terr = -EPROTO;\n\n\tif (!done || fragment)\n\t\tgoto out;\n\n\tcsum = skb_checksum_setup_ip(skb, nexthdr, off);\n\tif (IS_ERR(csum))\n\t\treturn PTR_ERR(csum);\n\n\tif (recalculate)\n\t\t*csum = ~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t skb->len - off, nexthdr, 0);\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/**\n * skb_checksum_setup - set up partial checksum offset\n * @skb: the skb to set up\n * @recalculate: if true the pseudo-header checksum will be recalculated\n */\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\terr = skb_checksum_setup_ipv4(skb, recalculate);\n\t\tbreak;\n\n\tcase htons(ETH_P_IPV6):\n\t\terr = skb_checksum_setup_ipv6(skb, recalculate);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EPROTO;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(skb_checksum_setup);\n\n/**\n * skb_checksum_maybe_trim - maybe trims the given skb\n * @skb: the skb to check\n * @transport_len: the data length beyond the network header\n *\n * Checks whether the given skb has data beyond the given transport length.\n * If so, returns a cloned skb trimmed to this transport length.\n * Otherwise returns the provided skb. Returns NULL in error cases\n * (e.g. transport_len exceeds skb length or out-of-memory).\n *\n * Caller needs to set the skb transport header and free any returned skb if it\n * differs from the provided skb.\n */\nstatic struct sk_buff *skb_checksum_maybe_trim(struct sk_buff *skb,\n\t\t\t\t\t       unsigned int transport_len)\n{\n\tstruct sk_buff *skb_chk;\n\tunsigned int len = skb_transport_offset(skb) + transport_len;\n\tint ret;\n\n\tif (skb->len < len)\n\t\treturn NULL;\n\telse if (skb->len == len)\n\t\treturn skb;\n\n\tskb_chk = skb_clone(skb, GFP_ATOMIC);\n\tif (!skb_chk)\n\t\treturn NULL;\n\n\tret = pskb_trim_rcsum(skb_chk, len);\n\tif (ret) {\n\t\tkfree_skb(skb_chk);\n\t\treturn NULL;\n\t}\n\n\treturn skb_chk;\n}\n\n/**\n * skb_checksum_trimmed - validate checksum of an skb\n * @skb: the skb to check\n * @transport_len: the data length beyond the network header\n * @skb_chkf: checksum function to use\n *\n * Applies the given checksum function skb_chkf to the provided skb.\n * Returns a checked and maybe trimmed skb. Returns NULL on error.\n *\n * If the skb has data beyond the given transport length, then a\n * trimmed & cloned skb is checked and returned.\n *\n * Caller needs to set the skb transport header and free any returned skb if it\n * differs from the provided skb.\n */\nstruct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,\n\t\t\t\t     unsigned int transport_len,\n\t\t\t\t     __sum16(*skb_chkf)(struct sk_buff *skb))\n{\n\tstruct sk_buff *skb_chk;\n\tunsigned int offset = skb_transport_offset(skb);\n\t__sum16 ret;\n\n\tskb_chk = skb_checksum_maybe_trim(skb, transport_len);\n\tif (!skb_chk)\n\t\tgoto err;\n\n\tif (!pskb_may_pull(skb_chk, offset))\n\t\tgoto err;\n\n\tskb_pull_rcsum(skb_chk, offset);\n\tret = skb_chkf(skb_chk);\n\tskb_push_rcsum(skb_chk, offset);\n\n\tif (ret)\n\t\tgoto err;\n\n\treturn skb_chk;\n\nerr:\n\tif (skb_chk && skb_chk != skb)\n\t\tkfree_skb(skb_chk);\n\n\treturn NULL;\n\n}\nEXPORT_SYMBOL(skb_checksum_trimmed);\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb)\n{\n\tnet_warn_ratelimited(\"%s: received packets cannot be forwarded while LRO is enabled\\n\",\n\t\t\t     skb->dev->name);\n}\nEXPORT_SYMBOL(__skb_warn_lro_forwarding);\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen)\n{\n\tif (head_stolen) {\n\t\tskb_release_head_state(skb);\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t} else {\n\t\t__kfree_skb(skb);\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_partial);\n\n/**\n * skb_try_coalesce - try to merge skb to prior one\n * @to: prior buffer\n * @from: buffer to add\n * @fragstolen: pointer to boolean\n * @delta_truesize: how much more was allocated than was requested\n */\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize)\n{\n\tint i, delta, len = from->len;\n\n\t*fragstolen = false;\n\n\tif (skb_cloned(to))\n\t\treturn false;\n\n\tif (len <= skb_tailroom(to)) {\n\t\tif (len)\n\t\t\tBUG_ON(skb_copy_bits(from, 0, skb_put(to, len), len));\n\t\t*delta_truesize = 0;\n\t\treturn true;\n\t}\n\n\tif (skb_has_frag_list(to) || skb_has_frag_list(from))\n\t\treturn false;\n\n\tif (skb_headlen(from) != 0) {\n\t\tstruct page *page;\n\t\tunsigned int offset;\n\n\t\tif (skb_shinfo(to)->nr_frags +\n\t\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tif (skb_head_is_locked(from))\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\t\tpage = virt_to_head_page(from->head);\n\t\toffset = from->data - (unsigned char *)page_address(page);\n\n\t\tskb_fill_page_desc(to, skb_shinfo(to)->nr_frags,\n\t\t\t\t   page, offset, skb_headlen(from));\n\t\t*fragstolen = true;\n\t} else {\n\t\tif (skb_shinfo(to)->nr_frags +\n\t\t    skb_shinfo(from)->nr_frags > MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_TRUESIZE(skb_end_offset(from));\n\t}\n\n\tWARN_ON_ONCE(delta < len);\n\n\tmemcpy(skb_shinfo(to)->frags + skb_shinfo(to)->nr_frags,\n\t       skb_shinfo(from)->frags,\n\t       skb_shinfo(from)->nr_frags * sizeof(skb_frag_t));\n\tskb_shinfo(to)->nr_frags += skb_shinfo(from)->nr_frags;\n\n\tif (!skb_cloned(from))\n\t\tskb_shinfo(from)->nr_frags = 0;\n\n\t/* if the skb is not cloned this does nothing\n\t * since we set nr_frags to 0.\n\t */\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++)\n\t\tskb_frag_ref(from, i);\n\n\tto->truesize += delta;\n\tto->len += len;\n\tto->data_len += len;\n\n\t*delta_truesize = delta;\n\treturn true;\n}\nEXPORT_SYMBOL(skb_try_coalesce);\n\n/**\n * skb_scrub_packet - scrub an skb\n *\n * @skb: buffer to clean\n * @xnet: packet is crossing netns\n *\n * skb_scrub_packet can be used after encapsulating or decapsulting a packet\n * into/from a tunnel. Some information have to be cleared during these\n * operations.\n * skb_scrub_packet can also be used to clean a skb before injecting it in\n * another namespace (@xnet == true). We have to clear all information in the\n * skb that could impact namespace isolation.\n */\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet)\n{\n\tskb->tstamp = 0;\n\tskb->pkt_type = PACKET_HOST;\n\tskb->skb_iif = 0;\n\tskb->ignore_df = 0;\n\tskb_dst_drop(skb);\n\tsecpath_reset(skb);\n\tnf_reset(skb);\n\tnf_reset_trace(skb);\n\n\tif (!xnet)\n\t\treturn;\n\n\tskb_orphan(skb);\n\tskb->mark = 0;\n}\nEXPORT_SYMBOL_GPL(skb_scrub_packet);\n\n/**\n * skb_gso_transport_seglen - Return length of individual segments of a gso packet\n *\n * @skb: GSO skb\n *\n * skb_gso_transport_seglen is used to determine the real size of the\n * individual segments, including Layer4 headers (TCP/UDP).\n *\n * The MAC/L2 or network (IP, IPv6) headers are not accounted for.\n */\nunsigned int skb_gso_transport_seglen(const struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tunsigned int thlen = 0;\n\n\tif (skb->encapsulation) {\n\t\tthlen = skb_inner_transport_header(skb) -\n\t\t\tskb_transport_header(skb);\n\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n\t\t\tthlen += inner_tcp_hdrlen(skb);\n\t} else if (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) {\n\t\tthlen = tcp_hdrlen(skb);\n\t} else if (unlikely(shinfo->gso_type & SKB_GSO_SCTP)) {\n\t\tthlen = sizeof(struct sctphdr);\n\t}\n\t/* UFO sets gso_size to the size of the fragmentation\n\t * payload, i.e. the size of the L4 (UDP) header is already\n\t * accounted for.\n\t */\n\treturn thlen + shinfo->gso_size;\n}\nEXPORT_SYMBOL_GPL(skb_gso_transport_seglen);\n\n/**\n * skb_gso_validate_mtu - Return in case such skb fits a given MTU\n *\n * @skb: GSO skb\n * @mtu: MTU to validate against\n *\n * skb_gso_validate_mtu validates if a given skb will fit a wanted MTU\n * once split.\n */\nbool skb_gso_validate_mtu(const struct sk_buff *skb, unsigned int mtu)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tconst struct sk_buff *iter;\n\tunsigned int hlen;\n\n\thlen = skb_gso_network_seglen(skb);\n\n\tif (shinfo->gso_size != GSO_BY_FRAGS)\n\t\treturn hlen <= mtu;\n\n\t/* Undo this so we can re-use header sizes */\n\thlen -= GSO_BY_FRAGS;\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (hlen + skb_headlen(iter) > mtu)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(skb_gso_validate_mtu);\n\nstatic struct sk_buff *skb_reorder_vlan_header(struct sk_buff *skb)\n{\n\tif (skb_cow(skb, skb_headroom(skb)) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\tmemmove(skb->data - ETH_HLEN, skb->data - skb->mac_len - VLAN_HLEN,\n\t\t2 * ETH_ALEN);\n\tskb->mac_header += VLAN_HLEN;\n\treturn skb;\n}\n\nstruct sk_buff *skb_vlan_untag(struct sk_buff *skb)\n{\n\tstruct vlan_hdr *vhdr;\n\tu16 vlan_tci;\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\t/* vlan_tci is already set-up so leave this for another time */\n\t\treturn skb;\n\t}\n\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\tgoto err_free;\n\n\tif (unlikely(!pskb_may_pull(skb, VLAN_HLEN)))\n\t\tgoto err_free;\n\n\tvhdr = (struct vlan_hdr *)skb->data;\n\tvlan_tci = ntohs(vhdr->h_vlan_TCI);\n\t__vlan_hwaccel_put_tag(skb, skb->protocol, vlan_tci);\n\n\tskb_pull_rcsum(skb, VLAN_HLEN);\n\tvlan_set_encap_proto(skb, vhdr);\n\n\tskb = skb_reorder_vlan_header(skb);\n\tif (unlikely(!skb))\n\t\tgoto err_free;\n\n\tskb_reset_network_header(skb);\n\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\treturn skb;\n\nerr_free:\n\tkfree_skb(skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(skb_vlan_untag);\n\nint skb_ensure_writable(struct sk_buff *skb, int write_len)\n{\n\tif (!pskb_may_pull(skb, write_len))\n\t\treturn -ENOMEM;\n\n\tif (!skb_cloned(skb) || skb_clone_writable(skb, write_len))\n\t\treturn 0;\n\n\treturn pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\nEXPORT_SYMBOL(skb_ensure_writable);\n\n/* remove VLAN header from packet and update csum accordingly.\n * expects a non skb_vlan_tag_present skb with a vlan tag payload\n */\nint __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci)\n{\n\tstruct vlan_hdr *vhdr;\n\tint offset = skb->data - skb_mac_header(skb);\n\tint err;\n\n\tif (WARN_ONCE(offset,\n\t\t      \"__skb_vlan_pop got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t      offset)) {\n\t\treturn -EINVAL;\n\t}\n\n\terr = skb_ensure_writable(skb, VLAN_ETH_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tskb_postpull_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\n\tvhdr = (struct vlan_hdr *)(skb->data + ETH_HLEN);\n\t*vlan_tci = ntohs(vhdr->h_vlan_TCI);\n\n\tmemmove(skb->data + VLAN_HLEN, skb->data, 2 * ETH_ALEN);\n\t__skb_pull(skb, VLAN_HLEN);\n\n\tvlan_set_encap_proto(skb, vhdr);\n\tskb->mac_header += VLAN_HLEN;\n\n\tif (skb_network_offset(skb) < ETH_HLEN)\n\t\tskb_set_network_header(skb, ETH_HLEN);\n\n\tskb_reset_mac_len(skb);\n\n\treturn err;\n}\nEXPORT_SYMBOL(__skb_vlan_pop);\n\n/* Pop a vlan tag either from hwaccel or from payload.\n * Expects skb->data at mac header.\n */\nint skb_vlan_pop(struct sk_buff *skb)\n{\n\tu16 vlan_tci;\n\t__be16 vlan_proto;\n\tint err;\n\n\tif (likely(skb_vlan_tag_present(skb))) {\n\t\tskb->vlan_tci = 0;\n\t} else {\n\t\tif (unlikely(!eth_type_vlan(skb->protocol)))\n\t\t\treturn 0;\n\n\t\terr = __skb_vlan_pop(skb, &vlan_tci);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\t/* move next vlan tag to hw accel tag */\n\tif (likely(!eth_type_vlan(skb->protocol)))\n\t\treturn 0;\n\n\tvlan_proto = skb->protocol;\n\terr = __skb_vlan_pop(skb, &vlan_tci);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_vlan_pop);\n\n/* Push a vlan tag either into hwaccel or into payload (if hwaccel tag present).\n * Expects skb->data at mac header.\n */\nint skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci)\n{\n\tif (skb_vlan_tag_present(skb)) {\n\t\tint offset = skb->data - skb_mac_header(skb);\n\t\tint err;\n\n\t\tif (WARN_ONCE(offset,\n\t\t\t      \"skb_vlan_push got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t\t      offset)) {\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\terr = __vlan_insert_tag(skb, skb->vlan_proto,\n\t\t\t\t\tskb_vlan_tag_get(skb));\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tskb->protocol = skb->vlan_proto;\n\t\tskb->mac_len += VLAN_HLEN;\n\n\t\tskb_postpush_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\t}\n\t__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_vlan_push);\n\n/**\n * alloc_skb_with_frags - allocate skb with page frags\n *\n * @header_len: size of linear part\n * @data_len: needed length in frags\n * @max_page_order: max page order desired.\n * @errcode: pointer to error code if any\n * @gfp_mask: allocation mask\n *\n * This can be used to allocate a paged skb, given a maximal order for frags.\n */\nstruct sk_buff *alloc_skb_with_frags(unsigned long header_len,\n\t\t\t\t     unsigned long data_len,\n\t\t\t\t     int max_page_order,\n\t\t\t\t     int *errcode,\n\t\t\t\t     gfp_t gfp_mask)\n{\n\tint npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\n\tunsigned long chunk;\n\tstruct sk_buff *skb;\n\tstruct page *page;\n\tgfp_t gfp_head;\n\tint i;\n\n\t*errcode = -EMSGSIZE;\n\t/* Note this test could be relaxed, if we succeed to allocate\n\t * high order pages...\n\t */\n\tif (npages > MAX_SKB_FRAGS)\n\t\treturn NULL;\n\n\tgfp_head = gfp_mask;\n\tif (gfp_head & __GFP_DIRECT_RECLAIM)\n\t\tgfp_head |= __GFP_REPEAT;\n\n\t*errcode = -ENOBUFS;\n\tskb = alloc_skb(header_len, gfp_head);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb->truesize += npages << PAGE_SHIFT;\n\n\tfor (i = 0; npages > 0; i++) {\n\t\tint order = max_page_order;\n\n\t\twhile (order) {\n\t\t\tif (npages >= 1 << order) {\n\t\t\t\tpage = alloc_pages((gfp_mask & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t\t\t\t   __GFP_COMP |\n\t\t\t\t\t\t   __GFP_NOWARN |\n\t\t\t\t\t\t   __GFP_NORETRY,\n\t\t\t\t\t\t   order);\n\t\t\t\tif (page)\n\t\t\t\t\tgoto fill_page;\n\t\t\t\t/* Do not retry other high order allocations */\n\t\t\t\torder = 1;\n\t\t\t\tmax_page_order = 0;\n\t\t\t}\n\t\t\torder--;\n\t\t}\n\t\tpage = alloc_page(gfp_mask);\n\t\tif (!page)\n\t\t\tgoto failure;\nfill_page:\n\t\tchunk = min_t(unsigned long, data_len,\n\t\t\t      PAGE_SIZE << order);\n\t\tskb_fill_page_desc(skb, i, page, 0, chunk);\n\t\tdata_len -= chunk;\n\t\tnpages -= 1 << order;\n\t}\n\treturn skb;\n\nfailure:\n\tkfree_skb(skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_skb_with_frags);\n\n/* carve out the first off bytes from skb when off < headlen */\nstatic int pskb_carve_inside_header(struct sk_buff *skb, const u32 off,\n\t\t\t\t    const int headlen, gfp_t gfp_mask)\n{\n\tint i;\n\tint size = skb_end_offset(skb);\n\tint new_hlen = headlen - off;\n\tu8 *data;\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size +\n\t\t\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\t/* Copy real data, and all frags */\n\tskb_copy_from_linear_data_offset(skb, off, data, new_hlen);\n\tskb->len -= off;\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info,\n\t\t\tfrags[skb_shinfo(skb)->nr_frags]));\n\tif (skb_cloned(skb)) {\n\t\t/* drop the old head gracefully */\n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tkfree(data);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\t\tskb_release_data(skb);\n\t} else {\n\t\t/* we can reuse existing recount- all we did was\n\t\t * relocate values\n\t\t */\n\t\tskb_free_head(skb);\n\t}\n\n\tskb->head = data;\n\tskb->data = data;\n\tskb->head_frag = 0;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end = size;\n#else\n\tskb->end = skb->head + size;\n#endif\n\tskb_set_tail_pointer(skb, skb_headlen(skb));\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned = 0;\n\tskb->hdr_len = 0;\n\tskb->nohdr = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\n\treturn 0;\n}\n\nstatic int pskb_carve(struct sk_buff *skb, const u32 off, gfp_t gfp);\n\n/* carve out the first eat bytes from skb's frag_list. May recurse into\n * pskb_carve()\n */\nstatic int pskb_carve_frag_list(struct sk_buff *skb,\n\t\t\t\tstruct skb_shared_info *shinfo, int eat,\n\t\t\t\tgfp_t gfp_mask)\n{\n\tstruct sk_buff *list = shinfo->frag_list;\n\tstruct sk_buff *clone = NULL;\n\tstruct sk_buff *insp = NULL;\n\n\tdo {\n\t\tif (!list) {\n\t\t\tpr_err(\"Not enough bytes to eat. Want %d\\n\", eat);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (list->len <= eat) {\n\t\t\t/* Eaten as whole. */\n\t\t\teat -= list->len;\n\t\t\tlist = list->next;\n\t\t\tinsp = list;\n\t\t} else {\n\t\t\t/* Eaten partially. */\n\t\t\tif (skb_shared(list)) {\n\t\t\t\tclone = skb_clone(list, gfp_mask);\n\t\t\t\tif (!clone)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tinsp = list->next;\n\t\t\t\tlist = clone;\n\t\t\t} else {\n\t\t\t\t/* This may be pulled without problems. */\n\t\t\t\tinsp = list;\n\t\t\t}\n\t\t\tif (pskb_carve(list, eat, gfp_mask) < 0) {\n\t\t\t\tkfree_skb(clone);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t} while (eat);\n\n\t/* Free pulled out fragments. */\n\twhile ((list = shinfo->frag_list) != insp) {\n\t\tshinfo->frag_list = list->next;\n\t\tkfree_skb(list);\n\t}\n\t/* And insert new clone at head. */\n\tif (clone) {\n\t\tclone->next = list;\n\t\tshinfo->frag_list = clone;\n\t}\n\treturn 0;\n}\n\n/* carve off first len bytes from skb. Split line (off) is in the\n * non-linear part of skb\n */\nstatic int pskb_carve_inside_nonlinear(struct sk_buff *skb, const u32 off,\n\t\t\t\t       int pos, gfp_t gfp_mask)\n{\n\tint i, k = 0;\n\tint size = skb_end_offset(skb);\n\tu8 *data;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\tstruct skb_shared_info *shinfo;\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size +\n\t\t\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb), offsetof(struct skb_shared_info,\n\t\t\t\t\t frags[skb_shinfo(skb)->nr_frags]));\n\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\tshinfo = (struct skb_shared_info *)(data + size);\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint fsize = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + fsize > off) {\n\t\t\tshinfo->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < off) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tshinfo->frags[0].page_offset += off - pos;\n\t\t\t\tskb_frag_size_sub(&shinfo->frags[0], off - pos);\n\t\t\t}\n\t\t\tskb_frag_ref(skb, i);\n\t\t\tk++;\n\t\t}\n\t\tpos += fsize;\n\t}\n\tshinfo->nr_frags = k;\n\tif (skb_has_frag_list(skb))\n\t\tskb_clone_fraglist(skb);\n\n\tif (k == 0) {\n\t\t/* split line is in frag list */\n\t\tpskb_carve_frag_list(skb, shinfo, off - pos, gfp_mask);\n\t}\n\tskb_release_data(skb);\n\n\tskb->head = data;\n\tskb->head_frag = 0;\n\tskb->data = data;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end = size;\n#else\n\tskb->end = skb->head + size;\n#endif\n\tskb_reset_tail_pointer(skb);\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tskb->len -= off;\n\tskb->data_len = skb->len;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\treturn 0;\n}\n\n/* remove len bytes from the beginning of the skb */\nstatic int pskb_carve(struct sk_buff *skb, const u32 len, gfp_t gfp)\n{\n\tint headlen = skb_headlen(skb);\n\n\tif (len < headlen)\n\t\treturn pskb_carve_inside_header(skb, len, headlen, gfp);\n\telse\n\t\treturn pskb_carve_inside_nonlinear(skb, len, headlen, gfp);\n}\n\n/* Extract to_copy bytes starting at off from skb, and return this in\n * a new skb\n */\nstruct sk_buff *pskb_extract(struct sk_buff *skb, int off,\n\t\t\t     int to_copy, gfp_t gfp)\n{\n\tstruct sk_buff  *clone = skb_clone(skb, gfp);\n\n\tif (!clone)\n\t\treturn NULL;\n\n\tif (pskb_carve(clone, off, gfp) < 0 ||\n\t    pskb_trim(clone, to_copy)) {\n\t\tkfree_skb(clone);\n\t\treturn NULL;\n\t}\n\treturn clone;\n}\nEXPORT_SYMBOL(pskb_extract);\n\n/**\n * skb_condense - try to get rid of fragments/frag_list if possible\n * @skb: buffer\n *\n * Can be used to save memory before skb is added to a busy queue.\n * If packet has bytes in frags and enough tail room in skb->head,\n * pull all of them, so that we can free the frags right now and adjust\n * truesize.\n * Notes:\n *\tWe do not reallocate skb->head thus can not fail.\n *\tCaller must re-evaluate skb->truesize if needed.\n */\nvoid skb_condense(struct sk_buff *skb)\n{\n\tif (skb->data_len) {\n\t\tif (skb->data_len > skb->end - skb->tail ||\n\t\t    skb_cloned(skb))\n\t\t\treturn;\n\n\t\t/* Nice, we can free page frag(s) right now */\n\t\t__pskb_pull_tail(skb, skb->data_len);\n\t}\n\t/* At this point, skb->truesize might be over estimated,\n\t * because skb had a fragment, and fragments do not tell\n\t * their truesize.\n\t * When we pulled its content into skb->head, fragment\n\t * was freed, but __pskb_pull_tail() could not possibly\n\t * adjust skb->truesize, not knowing the frag truesize.\n\t */\n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n}\n", "/*\n * NET\t\tAn implementation of the SOCKET network access protocol.\n *\n * Version:\t@(#)socket.c\t1.1.93\t18/02/95\n *\n * Authors:\tOrest Zborowski, <obz@Kodak.COM>\n *\t\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\n * Fixes:\n *\t\tAnonymous\t:\tNOTSOCK/BADF cleanup. Error fix in\n *\t\t\t\t\tshutdown()\n *\t\tAlan Cox\t:\tverify_area() fixes\n *\t\tAlan Cox\t:\tRemoved DDI\n *\t\tJonathan Kamens\t:\tSOCK_DGRAM reconnect bug\n *\t\tAlan Cox\t:\tMoved a load of checks to the very\n *\t\t\t\t\ttop level.\n *\t\tAlan Cox\t:\tMove address structures to/from user\n *\t\t\t\t\tmode above the protocol layers.\n *\t\tRob Janssen\t:\tAllow 0 length sends.\n *\t\tAlan Cox\t:\tAsynchronous I/O support (cribbed from the\n *\t\t\t\t\ttty drivers).\n *\t\tNiibe Yutaka\t:\tAsynchronous I/O for writes (4.4BSD style)\n *\t\tJeff Uphoff\t:\tMade max number of sockets command-line\n *\t\t\t\t\tconfigurable.\n *\t\tMatti Aarnio\t:\tMade the number of sockets dynamic,\n *\t\t\t\t\tto be allocated when needed, and mr.\n *\t\t\t\t\tUphoff's max is used as max to be\n *\t\t\t\t\tallowed to allocate.\n *\t\tLinus\t\t:\tArgh. removed all the socket allocation\n *\t\t\t\t\taltogether: it's in the inode now.\n *\t\tAlan Cox\t:\tMade sock_alloc()/sock_release() public\n *\t\t\t\t\tfor NetROM and future kernel nfsd type\n *\t\t\t\t\tstuff.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg basics.\n *\t\tTom Dyas\t:\tExport net symbols.\n *\t\tMarcin Dalecki\t:\tFixed problems with CONFIG_NET=\"n\".\n *\t\tAlan Cox\t:\tAdded thread locking to sys_* calls\n *\t\t\t\t\tfor sockets. May have errors at the\n *\t\t\t\t\tmoment.\n *\t\tKevin Buhr\t:\tFixed the dumb errors in the above.\n *\t\tAndi Kleen\t:\tSome small cleanups, optimizations,\n *\t\t\t\t\tand fixed a copy_from_user() bug.\n *\t\tTigran Aivazian\t:\tsys_send(args) calls sys_sendto(args, NULL, 0)\n *\t\tTigran Aivazian\t:\tMade listen(2) backlog sanity checks\n *\t\t\t\t\tprotocol-independent\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\n *\tThis module is effectively the top level interface to the BSD socket\n *\tparadigm.\n *\n *\tBased upon Swansea University Computer Society NET3.039\n */\n\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/file.h>\n#include <linux/net.h>\n#include <linux/interrupt.h>\n#include <linux/thread_info.h>\n#include <linux/rcupdate.h>\n#include <linux/netdevice.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/mutex.h>\n#include <linux/if_bridge.h>\n#include <linux/if_frad.h>\n#include <linux/if_vlan.h>\n#include <linux/ptp_classify.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/cache.h>\n#include <linux/module.h>\n#include <linux/highmem.h>\n#include <linux/mount.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/compat.h>\n#include <linux/kmod.h>\n#include <linux/audit.h>\n#include <linux/wireless.h>\n#include <linux/nsproxy.h>\n#include <linux/magic.h>\n#include <linux/slab.h>\n#include <linux/xattr.h>\n\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n\n#include <net/compat.h>\n#include <net/wext.h>\n#include <net/cls_cgroup.h>\n\n#include <net/sock.h>\n#include <linux/netfilter.h>\n\n#include <linux/if_tun.h>\n#include <linux/ipv6_route.h>\n#include <linux/route.h>\n#include <linux/sockios.h>\n#include <linux/atalk.h>\n#include <net/busy_poll.h>\n#include <linux/errqueue.h>\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\nunsigned int sysctl_net_busy_read __read_mostly;\nunsigned int sysctl_net_busy_poll __read_mostly;\n#endif\n\nstatic ssize_t sock_read_iter(struct kiocb *iocb, struct iov_iter *to);\nstatic ssize_t sock_write_iter(struct kiocb *iocb, struct iov_iter *from);\nstatic int sock_mmap(struct file *file, struct vm_area_struct *vma);\n\nstatic int sock_close(struct inode *inode, struct file *file);\nstatic unsigned int sock_poll(struct file *file,\n\t\t\t      struct poll_table_struct *wait);\nstatic long sock_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\n#ifdef CONFIG_COMPAT\nstatic long compat_sock_ioctl(struct file *file,\n\t\t\t      unsigned int cmd, unsigned long arg);\n#endif\nstatic int sock_fasync(int fd, struct file *filp, int on);\nstatic ssize_t sock_sendpage(struct file *file, struct page *page,\n\t\t\t     int offset, size_t size, loff_t *ppos, int more);\nstatic ssize_t sock_splice_read(struct file *file, loff_t *ppos,\n\t\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\t\tunsigned int flags);\n\n/*\n *\tSocket files have a set of 'special' operations as well as the generic file ones. These don't appear\n *\tin the operation structures but are done directly via the socketcall() multiplexor.\n */\n\nstatic const struct file_operations socket_file_ops = {\n\t.owner =\tTHIS_MODULE,\n\t.llseek =\tno_llseek,\n\t.read_iter =\tsock_read_iter,\n\t.write_iter =\tsock_write_iter,\n\t.poll =\t\tsock_poll,\n\t.unlocked_ioctl = sock_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = compat_sock_ioctl,\n#endif\n\t.mmap =\t\tsock_mmap,\n\t.release =\tsock_close,\n\t.fasync =\tsock_fasync,\n\t.sendpage =\tsock_sendpage,\n\t.splice_write = generic_splice_sendpage,\n\t.splice_read =\tsock_splice_read,\n};\n\n/*\n *\tThe protocol list. Each protocol is registered in here.\n */\n\nstatic DEFINE_SPINLOCK(net_family_lock);\nstatic const struct net_proto_family __rcu *net_families[NPROTO] __read_mostly;\n\n/*\n *\tStatistics counters of the socket lists\n */\n\nstatic DEFINE_PER_CPU(int, sockets_in_use);\n\n/*\n * Support routines.\n * Move socket addresses back and forth across the kernel/user\n * divide and look after the messy bits.\n */\n\n/**\n *\tmove_addr_to_kernel\t-\tcopy a socket address into kernel space\n *\t@uaddr: Address in user space\n *\t@kaddr: Address in kernel space\n *\t@ulen: Length in user space\n *\n *\tThe address is copied into kernel space. If the provided address is\n *\ttoo long an error code of -EINVAL is returned. If the copy gives\n *\tinvalid addresses -EFAULT is returned. On a success 0 is returned.\n */\n\nint move_addr_to_kernel(void __user *uaddr, int ulen, struct sockaddr_storage *kaddr)\n{\n\tif (ulen < 0 || ulen > sizeof(struct sockaddr_storage))\n\t\treturn -EINVAL;\n\tif (ulen == 0)\n\t\treturn 0;\n\tif (copy_from_user(kaddr, uaddr, ulen))\n\t\treturn -EFAULT;\n\treturn audit_sockaddr(ulen, kaddr);\n}\n\n/**\n *\tmove_addr_to_user\t-\tcopy an address to user space\n *\t@kaddr: kernel space address\n *\t@klen: length of address in kernel\n *\t@uaddr: user space address\n *\t@ulen: pointer to user length field\n *\n *\tThe value pointed to by ulen on entry is the buffer length available.\n *\tThis is overwritten with the buffer space used. -EINVAL is returned\n *\tif an overlong buffer is specified or a negative buffer size. -EFAULT\n *\tis returned if either the buffer or the length field are not\n *\taccessible.\n *\tAfter copying the data up to the limit the user specifies, the true\n *\tlength of the data is written over the length limit the user\n *\tspecified. Zero is returned for a success.\n */\n\nstatic int move_addr_to_user(struct sockaddr_storage *kaddr, int klen,\n\t\t\t     void __user *uaddr, int __user *ulen)\n{\n\tint err;\n\tint len;\n\n\tBUG_ON(klen > sizeof(struct sockaddr_storage));\n\terr = get_user(len, ulen);\n\tif (err)\n\t\treturn err;\n\tif (len > klen)\n\t\tlen = klen;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\tif (len) {\n\t\tif (audit_sockaddr(klen, kaddr))\n\t\t\treturn -ENOMEM;\n\t\tif (copy_to_user(uaddr, kaddr, len))\n\t\t\treturn -EFAULT;\n\t}\n\t/*\n\t *      \"fromlen shall refer to the value before truncation..\"\n\t *                      1003.1g\n\t */\n\treturn __put_user(klen, ulen);\n}\n\nstatic struct kmem_cache *sock_inode_cachep __read_mostly;\n\nstatic struct inode *sock_alloc_inode(struct super_block *sb)\n{\n\tstruct socket_alloc *ei;\n\tstruct socket_wq *wq;\n\n\tei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL);\n\tif (!ei)\n\t\treturn NULL;\n\twq = kmalloc(sizeof(*wq), GFP_KERNEL);\n\tif (!wq) {\n\t\tkmem_cache_free(sock_inode_cachep, ei);\n\t\treturn NULL;\n\t}\n\tinit_waitqueue_head(&wq->wait);\n\twq->fasync_list = NULL;\n\twq->flags = 0;\n\tRCU_INIT_POINTER(ei->socket.wq, wq);\n\n\tei->socket.state = SS_UNCONNECTED;\n\tei->socket.flags = 0;\n\tei->socket.ops = NULL;\n\tei->socket.sk = NULL;\n\tei->socket.file = NULL;\n\n\treturn &ei->vfs_inode;\n}\n\nstatic void sock_destroy_inode(struct inode *inode)\n{\n\tstruct socket_alloc *ei;\n\tstruct socket_wq *wq;\n\n\tei = container_of(inode, struct socket_alloc, vfs_inode);\n\twq = rcu_dereference_protected(ei->socket.wq, 1);\n\tkfree_rcu(wq, rcu);\n\tkmem_cache_free(sock_inode_cachep, ei);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct socket_alloc *ei = (struct socket_alloc *)foo;\n\n\tinode_init_once(&ei->vfs_inode);\n}\n\nstatic void init_inodecache(void)\n{\n\tsock_inode_cachep = kmem_cache_create(\"sock_inode_cache\",\n\t\t\t\t\t      sizeof(struct socket_alloc),\n\t\t\t\t\t      0,\n\t\t\t\t\t      (SLAB_HWCACHE_ALIGN |\n\t\t\t\t\t       SLAB_RECLAIM_ACCOUNT |\n\t\t\t\t\t       SLAB_MEM_SPREAD | SLAB_ACCOUNT),\n\t\t\t\t\t      init_once);\n\tBUG_ON(sock_inode_cachep == NULL);\n}\n\nstatic const struct super_operations sockfs_ops = {\n\t.alloc_inode\t= sock_alloc_inode,\n\t.destroy_inode\t= sock_destroy_inode,\n\t.statfs\t\t= simple_statfs,\n};\n\n/*\n * sockfs_dname() is called from d_path().\n */\nstatic char *sockfs_dname(struct dentry *dentry, char *buffer, int buflen)\n{\n\treturn dynamic_dname(dentry, buffer, buflen, \"socket:[%lu]\",\n\t\t\t\td_inode(dentry)->i_ino);\n}\n\nstatic const struct dentry_operations sockfs_dentry_operations = {\n\t.d_dname  = sockfs_dname,\n};\n\nstatic int sockfs_xattr_get(const struct xattr_handler *handler,\n\t\t\t    struct dentry *dentry, struct inode *inode,\n\t\t\t    const char *suffix, void *value, size_t size)\n{\n\tif (value) {\n\t\tif (dentry->d_name.len + 1 > size)\n\t\t\treturn -ERANGE;\n\t\tmemcpy(value, dentry->d_name.name, dentry->d_name.len + 1);\n\t}\n\treturn dentry->d_name.len + 1;\n}\n\n#define XATTR_SOCKPROTONAME_SUFFIX \"sockprotoname\"\n#define XATTR_NAME_SOCKPROTONAME (XATTR_SYSTEM_PREFIX XATTR_SOCKPROTONAME_SUFFIX)\n#define XATTR_NAME_SOCKPROTONAME_LEN (sizeof(XATTR_NAME_SOCKPROTONAME)-1)\n\nstatic const struct xattr_handler sockfs_xattr_handler = {\n\t.name = XATTR_NAME_SOCKPROTONAME,\n\t.get = sockfs_xattr_get,\n};\n\nstatic int sockfs_security_xattr_set(const struct xattr_handler *handler,\n\t\t\t\t     struct dentry *dentry, struct inode *inode,\n\t\t\t\t     const char *suffix, const void *value,\n\t\t\t\t     size_t size, int flags)\n{\n\t/* Handled by LSM. */\n\treturn -EAGAIN;\n}\n\nstatic const struct xattr_handler sockfs_security_xattr_handler = {\n\t.prefix = XATTR_SECURITY_PREFIX,\n\t.set = sockfs_security_xattr_set,\n};\n\nstatic const struct xattr_handler *sockfs_xattr_handlers[] = {\n\t&sockfs_xattr_handler,\n\t&sockfs_security_xattr_handler,\n\tNULL\n};\n\nstatic struct dentry *sockfs_mount(struct file_system_type *fs_type,\n\t\t\t int flags, const char *dev_name, void *data)\n{\n\treturn mount_pseudo_xattr(fs_type, \"socket:\", &sockfs_ops,\n\t\t\t\t  sockfs_xattr_handlers,\n\t\t\t\t  &sockfs_dentry_operations, SOCKFS_MAGIC);\n}\n\nstatic struct vfsmount *sock_mnt __read_mostly;\n\nstatic struct file_system_type sock_fs_type = {\n\t.name =\t\t\"sockfs\",\n\t.mount =\tsockfs_mount,\n\t.kill_sb =\tkill_anon_super,\n};\n\n/*\n *\tObtains the first available file descriptor and sets it up for use.\n *\n *\tThese functions create file structures and maps them to fd space\n *\tof the current process. On success it returns file descriptor\n *\tand file struct implicitly stored in sock->file.\n *\tNote that another thread may close file descriptor before we return\n *\tfrom this function. We use the fact that now we do not refer\n *\tto socket after mapping. If one day we will need it, this\n *\tfunction will increment ref. count on file by 1.\n *\n *\tIn any case returned fd MAY BE not valid!\n *\tThis race condition is unavoidable\n *\twith shared fd spaces, we cannot solve it inside kernel,\n *\tbut we take care of internal coherence yet.\n */\n\nstruct file *sock_alloc_file(struct socket *sock, int flags, const char *dname)\n{\n\tstruct qstr name = { .name = \"\" };\n\tstruct path path;\n\tstruct file *file;\n\n\tif (dname) {\n\t\tname.name = dname;\n\t\tname.len = strlen(name.name);\n\t} else if (sock->sk) {\n\t\tname.name = sock->sk->sk_prot_creator->name;\n\t\tname.len = strlen(name.name);\n\t}\n\tpath.dentry = d_alloc_pseudo(sock_mnt->mnt_sb, &name);\n\tif (unlikely(!path.dentry))\n\t\treturn ERR_PTR(-ENOMEM);\n\tpath.mnt = mntget(sock_mnt);\n\n\td_instantiate(path.dentry, SOCK_INODE(sock));\n\n\tfile = alloc_file(&path, FMODE_READ | FMODE_WRITE,\n\t\t  &socket_file_ops);\n\tif (IS_ERR(file)) {\n\t\t/* drop dentry, keep inode */\n\t\tihold(d_inode(path.dentry));\n\t\tpath_put(&path);\n\t\treturn file;\n\t}\n\n\tsock->file = file;\n\tfile->f_flags = O_RDWR | (flags & O_NONBLOCK);\n\tfile->private_data = sock;\n\treturn file;\n}\nEXPORT_SYMBOL(sock_alloc_file);\n\nstatic int sock_map_fd(struct socket *sock, int flags)\n{\n\tstruct file *newfile;\n\tint fd = get_unused_fd_flags(flags);\n\tif (unlikely(fd < 0))\n\t\treturn fd;\n\n\tnewfile = sock_alloc_file(sock, flags, NULL);\n\tif (likely(!IS_ERR(newfile))) {\n\t\tfd_install(fd, newfile);\n\t\treturn fd;\n\t}\n\n\tput_unused_fd(fd);\n\treturn PTR_ERR(newfile);\n}\n\nstruct socket *sock_from_file(struct file *file, int *err)\n{\n\tif (file->f_op == &socket_file_ops)\n\t\treturn file->private_data;\t/* set in sock_map_fd */\n\n\t*err = -ENOTSOCK;\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_from_file);\n\n/**\n *\tsockfd_lookup - Go from a file number to its socket slot\n *\t@fd: file handle\n *\t@err: pointer to an error code return\n *\n *\tThe file handle passed in is locked and the socket it is bound\n *\ttoo is returned. If an error occurs the err pointer is overwritten\n *\twith a negative errno code and NULL is returned. The function checks\n *\tfor both invalid handles and passing a handle which is not a socket.\n *\n *\tOn a success the socket object pointer is returned.\n */\n\nstruct socket *sockfd_lookup(int fd, int *err)\n{\n\tstruct file *file;\n\tstruct socket *sock;\n\n\tfile = fget(fd);\n\tif (!file) {\n\t\t*err = -EBADF;\n\t\treturn NULL;\n\t}\n\n\tsock = sock_from_file(file, err);\n\tif (!sock)\n\t\tfput(file);\n\treturn sock;\n}\nEXPORT_SYMBOL(sockfd_lookup);\n\nstatic struct socket *sockfd_lookup_light(int fd, int *err, int *fput_needed)\n{\n\tstruct fd f = fdget(fd);\n\tstruct socket *sock;\n\n\t*err = -EBADF;\n\tif (f.file) {\n\t\tsock = sock_from_file(f.file, err);\n\t\tif (likely(sock)) {\n\t\t\t*fput_needed = f.flags;\n\t\t\treturn sock;\n\t\t}\n\t\tfdput(f);\n\t}\n\treturn NULL;\n}\n\nstatic ssize_t sockfs_listxattr(struct dentry *dentry, char *buffer,\n\t\t\t\tsize_t size)\n{\n\tssize_t len;\n\tssize_t used = 0;\n\n\tlen = security_inode_listsecurity(d_inode(dentry), buffer, size);\n\tif (len < 0)\n\t\treturn len;\n\tused += len;\n\tif (buffer) {\n\t\tif (size < used)\n\t\t\treturn -ERANGE;\n\t\tbuffer += len;\n\t}\n\n\tlen = (XATTR_NAME_SOCKPROTONAME_LEN + 1);\n\tused += len;\n\tif (buffer) {\n\t\tif (size < used)\n\t\t\treturn -ERANGE;\n\t\tmemcpy(buffer, XATTR_NAME_SOCKPROTONAME, len);\n\t\tbuffer += len;\n\t}\n\n\treturn used;\n}\n\nstatic int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n{\n\tint err = simple_setattr(dentry, iattr);\n\n\tif (!err && (iattr->ia_valid & ATTR_UID)) {\n\t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n\n\t\tsock->sk->sk_uid = iattr->ia_uid;\n\t}\n\n\treturn err;\n}\n\nstatic const struct inode_operations sockfs_inode_ops = {\n\t.listxattr = sockfs_listxattr,\n\t.setattr = sockfs_setattr,\n};\n\n/**\n *\tsock_alloc\t-\tallocate a socket\n *\n *\tAllocate a new inode and socket object. The two are bound together\n *\tand initialised. The socket is then returned. If we are out of inodes\n *\tNULL is returned.\n */\n\nstruct socket *sock_alloc(void)\n{\n\tstruct inode *inode;\n\tstruct socket *sock;\n\n\tinode = new_inode_pseudo(sock_mnt->mnt_sb);\n\tif (!inode)\n\t\treturn NULL;\n\n\tsock = SOCKET_I(inode);\n\n\tkmemcheck_annotate_bitfield(sock, type);\n\tinode->i_ino = get_next_ino();\n\tinode->i_mode = S_IFSOCK | S_IRWXUGO;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_op = &sockfs_inode_ops;\n\n\tthis_cpu_add(sockets_in_use, 1);\n\treturn sock;\n}\nEXPORT_SYMBOL(sock_alloc);\n\n/**\n *\tsock_release\t-\tclose a socket\n *\t@sock: socket to close\n *\n *\tThe socket is released from the protocol stack if it has a release\n *\tcallback, and the inode is then released if the socket is bound to\n *\tan inode not a file.\n */\n\nvoid sock_release(struct socket *sock)\n{\n\tif (sock->ops) {\n\t\tstruct module *owner = sock->ops->owner;\n\n\t\tsock->ops->release(sock);\n\t\tsock->ops = NULL;\n\t\tmodule_put(owner);\n\t}\n\n\tif (rcu_dereference_protected(sock->wq, 1)->fasync_list)\n\t\tpr_err(\"%s: fasync list not empty!\\n\", __func__);\n\n\tthis_cpu_sub(sockets_in_use, 1);\n\tif (!sock->file) {\n\t\tiput(SOCK_INODE(sock));\n\t\treturn;\n\t}\n\tsock->file = NULL;\n}\nEXPORT_SYMBOL(sock_release);\n\nvoid __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)\n{\n\tu8 flags = *tx_flags;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_HARDWARE)\n\t\tflags |= SKBTX_HW_TSTAMP;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_SOFTWARE)\n\t\tflags |= SKBTX_SW_TSTAMP;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_SCHED)\n\t\tflags |= SKBTX_SCHED_TSTAMP;\n\n\t*tx_flags = flags;\n}\nEXPORT_SYMBOL(__sock_tx_timestamp);\n\nstatic inline int sock_sendmsg_nosec(struct socket *sock, struct msghdr *msg)\n{\n\tint ret = sock->ops->sendmsg(sock, msg, msg_data_left(msg));\n\tBUG_ON(ret == -EIOCBQUEUED);\n\treturn ret;\n}\n\nint sock_sendmsg(struct socket *sock, struct msghdr *msg)\n{\n\tint err = security_socket_sendmsg(sock, msg,\n\t\t\t\t\t  msg_data_left(msg));\n\n\treturn err ?: sock_sendmsg_nosec(sock, msg);\n}\nEXPORT_SYMBOL(sock_sendmsg);\n\nint kernel_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t   struct kvec *vec, size_t num, size_t size)\n{\n\tiov_iter_kvec(&msg->msg_iter, WRITE | ITER_KVEC, vec, num, size);\n\treturn sock_sendmsg(sock, msg);\n}\nEXPORT_SYMBOL(kernel_sendmsg);\n\nstatic bool skb_is_err_queue(const struct sk_buff *skb)\n{\n\t/* pkt_type of skbs enqueued on the error queue are set to\n\t * PACKET_OUTGOING in skb_set_err_queue(). This is only safe to do\n\t * in recvmsg, since skbs received on a local socket will never\n\t * have a pkt_type of PACKET_OUTGOING.\n\t */\n\treturn skb->pkt_type == PACKET_OUTGOING;\n}\n\n/*\n * called from sock_recv_timestamp() if sock_flag(sk, SOCK_RCVTSTAMP)\n */\nvoid __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,\n\tstruct sk_buff *skb)\n{\n\tint need_software_tstamp = sock_flag(sk, SOCK_RCVTSTAMP);\n\tstruct scm_timestamping tss;\n\tint empty = 1;\n\tstruct skb_shared_hwtstamps *shhwtstamps =\n\t\tskb_hwtstamps(skb);\n\n\t/* Race occurred between timestamp enabling and packet\n\t   receiving.  Fill in the current time for now. */\n\tif (need_software_tstamp && skb->tstamp == 0)\n\t\t__net_timestamp(skb);\n\n\tif (need_software_tstamp) {\n\t\tif (!sock_flag(sk, SOCK_RCVTSTAMPNS)) {\n\t\t\tstruct timeval tv;\n\t\t\tskb_get_timestamp(skb, &tv);\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMP,\n\t\t\t\t sizeof(tv), &tv);\n\t\t} else {\n\t\t\tstruct timespec ts;\n\t\t\tskb_get_timestampns(skb, &ts);\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPNS,\n\t\t\t\t sizeof(ts), &ts);\n\t\t}\n\t}\n\n\tmemset(&tss, 0, sizeof(tss));\n\tif ((sk->sk_tsflags & SOF_TIMESTAMPING_SOFTWARE) &&\n\t    ktime_to_timespec_cond(skb->tstamp, tss.ts + 0))\n\t\tempty = 0;\n\tif (shhwtstamps &&\n\t    (sk->sk_tsflags & SOF_TIMESTAMPING_RAW_HARDWARE) &&\n\t    ktime_to_timespec_cond(shhwtstamps->hwtstamp, tss.ts + 2))\n\t\tempty = 0;\n\tif (!empty) {\n\t\tput_cmsg(msg, SOL_SOCKET,\n\t\t\t SCM_TIMESTAMPING, sizeof(tss), &tss);\n\n\t\tif (skb_is_err_queue(skb) && skb->len &&\n\t\t    (sk->sk_tsflags & SOF_TIMESTAMPING_OPT_STATS))\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPING_OPT_STATS,\n\t\t\t\t skb->len, skb->data);\n\t}\n}\nEXPORT_SYMBOL_GPL(__sock_recv_timestamp);\n\nvoid __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,\n\tstruct sk_buff *skb)\n{\n\tint ack;\n\n\tif (!sock_flag(sk, SOCK_WIFI_STATUS))\n\t\treturn;\n\tif (!skb->wifi_acked_valid)\n\t\treturn;\n\n\tack = skb->wifi_acked;\n\n\tput_cmsg(msg, SOL_SOCKET, SCM_WIFI_STATUS, sizeof(ack), &ack);\n}\nEXPORT_SYMBOL_GPL(__sock_recv_wifi_status);\n\nstatic inline void sock_recv_drops(struct msghdr *msg, struct sock *sk,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tif (sock_flag(sk, SOCK_RXQ_OVFL) && skb && SOCK_SKB_CB(skb)->dropcount)\n\t\tput_cmsg(msg, SOL_SOCKET, SO_RXQ_OVFL,\n\t\t\tsizeof(__u32), &SOCK_SKB_CB(skb)->dropcount);\n}\n\nvoid __sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,\n\tstruct sk_buff *skb)\n{\n\tsock_recv_timestamp(msg, sk, skb);\n\tsock_recv_drops(msg, sk, skb);\n}\nEXPORT_SYMBOL_GPL(__sock_recv_ts_and_drops);\n\nstatic inline int sock_recvmsg_nosec(struct socket *sock, struct msghdr *msg,\n\t\t\t\t     int flags)\n{\n\treturn sock->ops->recvmsg(sock, msg, msg_data_left(msg), flags);\n}\n\nint sock_recvmsg(struct socket *sock, struct msghdr *msg, int flags)\n{\n\tint err = security_socket_recvmsg(sock, msg, msg_data_left(msg), flags);\n\n\treturn err ?: sock_recvmsg_nosec(sock, msg, flags);\n}\nEXPORT_SYMBOL(sock_recvmsg);\n\n/**\n * kernel_recvmsg - Receive a message from a socket (kernel space)\n * @sock:       The socket to receive the message from\n * @msg:        Received message\n * @vec:        Input s/g array for message data\n * @num:        Size of input s/g array\n * @size:       Number of bytes to read\n * @flags:      Message flags (MSG_DONTWAIT, etc...)\n *\n * On return the msg structure contains the scatter/gather array passed in the\n * vec argument. The array is modified so that it consists of the unfilled\n * portion of the original array.\n *\n * The returned value is the total number of bytes received, or an error.\n */\nint kernel_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t   struct kvec *vec, size_t num, size_t size, int flags)\n{\n\tmm_segment_t oldfs = get_fs();\n\tint result;\n\n\tiov_iter_kvec(&msg->msg_iter, READ | ITER_KVEC, vec, num, size);\n\tset_fs(KERNEL_DS);\n\tresult = sock_recvmsg(sock, msg, flags);\n\tset_fs(oldfs);\n\treturn result;\n}\nEXPORT_SYMBOL(kernel_recvmsg);\n\nstatic ssize_t sock_sendpage(struct file *file, struct page *page,\n\t\t\t     int offset, size_t size, loff_t *ppos, int more)\n{\n\tstruct socket *sock;\n\tint flags;\n\n\tsock = file->private_data;\n\n\tflags = (file->f_flags & O_NONBLOCK) ? MSG_DONTWAIT : 0;\n\t/* more is a combination of MSG_MORE and MSG_SENDPAGE_NOTLAST */\n\tflags |= more;\n\n\treturn kernel_sendpage(sock, page, offset, size, flags);\n}\n\nstatic ssize_t sock_splice_read(struct file *file, loff_t *ppos,\n\t\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\t\tunsigned int flags)\n{\n\tstruct socket *sock = file->private_data;\n\n\tif (unlikely(!sock->ops->splice_read))\n\t\treturn -EINVAL;\n\n\treturn sock->ops->splice_read(sock, ppos, pipe, len, flags);\n}\n\nstatic ssize_t sock_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct socket *sock = file->private_data;\n\tstruct msghdr msg = {.msg_iter = *to,\n\t\t\t     .msg_iocb = iocb};\n\tssize_t res;\n\n\tif (file->f_flags & O_NONBLOCK)\n\t\tmsg.msg_flags = MSG_DONTWAIT;\n\n\tif (iocb->ki_pos != 0)\n\t\treturn -ESPIPE;\n\n\tif (!iov_iter_count(to))\t/* Match SYS5 behaviour */\n\t\treturn 0;\n\n\tres = sock_recvmsg(sock, &msg, msg.msg_flags);\n\t*to = msg.msg_iter;\n\treturn res;\n}\n\nstatic ssize_t sock_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct socket *sock = file->private_data;\n\tstruct msghdr msg = {.msg_iter = *from,\n\t\t\t     .msg_iocb = iocb};\n\tssize_t res;\n\n\tif (iocb->ki_pos != 0)\n\t\treturn -ESPIPE;\n\n\tif (file->f_flags & O_NONBLOCK)\n\t\tmsg.msg_flags = MSG_DONTWAIT;\n\n\tif (sock->type == SOCK_SEQPACKET)\n\t\tmsg.msg_flags |= MSG_EOR;\n\n\tres = sock_sendmsg(sock, &msg);\n\t*from = msg.msg_iter;\n\treturn res;\n}\n\n/*\n * Atomic setting of ioctl hooks to avoid race\n * with module unload.\n */\n\nstatic DEFINE_MUTEX(br_ioctl_mutex);\nstatic int (*br_ioctl_hook) (struct net *, unsigned int cmd, void __user *arg);\n\nvoid brioctl_set(int (*hook) (struct net *, unsigned int, void __user *))\n{\n\tmutex_lock(&br_ioctl_mutex);\n\tbr_ioctl_hook = hook;\n\tmutex_unlock(&br_ioctl_mutex);\n}\nEXPORT_SYMBOL(brioctl_set);\n\nstatic DEFINE_MUTEX(vlan_ioctl_mutex);\nstatic int (*vlan_ioctl_hook) (struct net *, void __user *arg);\n\nvoid vlan_ioctl_set(int (*hook) (struct net *, void __user *))\n{\n\tmutex_lock(&vlan_ioctl_mutex);\n\tvlan_ioctl_hook = hook;\n\tmutex_unlock(&vlan_ioctl_mutex);\n}\nEXPORT_SYMBOL(vlan_ioctl_set);\n\nstatic DEFINE_MUTEX(dlci_ioctl_mutex);\nstatic int (*dlci_ioctl_hook) (unsigned int, void __user *);\n\nvoid dlci_ioctl_set(int (*hook) (unsigned int, void __user *))\n{\n\tmutex_lock(&dlci_ioctl_mutex);\n\tdlci_ioctl_hook = hook;\n\tmutex_unlock(&dlci_ioctl_mutex);\n}\nEXPORT_SYMBOL(dlci_ioctl_set);\n\nstatic long sock_do_ioctl(struct net *net, struct socket *sock,\n\t\t\t\t unsigned int cmd, unsigned long arg)\n{\n\tint err;\n\tvoid __user *argp = (void __user *)arg;\n\n\terr = sock->ops->ioctl(sock, cmd, arg);\n\n\t/*\n\t * If this ioctl is unknown try to hand it down\n\t * to the NIC driver.\n\t */\n\tif (err == -ENOIOCTLCMD)\n\t\terr = dev_ioctl(net, cmd, argp);\n\n\treturn err;\n}\n\n/*\n *\tWith an ioctl, arg may well be a user mode pointer, but we don't know\n *\twhat to do with it - that's up to the protocol still.\n */\n\nstatic struct ns_common *get_net_ns(struct ns_common *ns)\n{\n\treturn &get_net(container_of(ns, struct net, ns))->ns;\n}\n\nstatic long sock_ioctl(struct file *file, unsigned cmd, unsigned long arg)\n{\n\tstruct socket *sock;\n\tstruct sock *sk;\n\tvoid __user *argp = (void __user *)arg;\n\tint pid, err;\n\tstruct net *net;\n\n\tsock = file->private_data;\n\tsk = sock->sk;\n\tnet = sock_net(sk);\n\tif (cmd >= SIOCDEVPRIVATE && cmd <= (SIOCDEVPRIVATE + 15)) {\n\t\terr = dev_ioctl(net, cmd, argp);\n\t} else\n#ifdef CONFIG_WEXT_CORE\n\tif (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST) {\n\t\terr = dev_ioctl(net, cmd, argp);\n\t} else\n#endif\n\t\tswitch (cmd) {\n\t\tcase FIOSETOWN:\n\t\tcase SIOCSPGRP:\n\t\t\terr = -EFAULT;\n\t\t\tif (get_user(pid, (int __user *)argp))\n\t\t\t\tbreak;\n\t\t\tf_setown(sock->file, pid, 1);\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\tcase FIOGETOWN:\n\t\tcase SIOCGPGRP:\n\t\t\terr = put_user(f_getown(sock->file),\n\t\t\t\t       (int __user *)argp);\n\t\t\tbreak;\n\t\tcase SIOCGIFBR:\n\t\tcase SIOCSIFBR:\n\t\tcase SIOCBRADDBR:\n\t\tcase SIOCBRDELBR:\n\t\t\terr = -ENOPKG;\n\t\t\tif (!br_ioctl_hook)\n\t\t\t\trequest_module(\"bridge\");\n\n\t\t\tmutex_lock(&br_ioctl_mutex);\n\t\t\tif (br_ioctl_hook)\n\t\t\t\terr = br_ioctl_hook(net, cmd, argp);\n\t\t\tmutex_unlock(&br_ioctl_mutex);\n\t\t\tbreak;\n\t\tcase SIOCGIFVLAN:\n\t\tcase SIOCSIFVLAN:\n\t\t\terr = -ENOPKG;\n\t\t\tif (!vlan_ioctl_hook)\n\t\t\t\trequest_module(\"8021q\");\n\n\t\t\tmutex_lock(&vlan_ioctl_mutex);\n\t\t\tif (vlan_ioctl_hook)\n\t\t\t\terr = vlan_ioctl_hook(net, argp);\n\t\t\tmutex_unlock(&vlan_ioctl_mutex);\n\t\t\tbreak;\n\t\tcase SIOCADDDLCI:\n\t\tcase SIOCDELDLCI:\n\t\t\terr = -ENOPKG;\n\t\t\tif (!dlci_ioctl_hook)\n\t\t\t\trequest_module(\"dlci\");\n\n\t\t\tmutex_lock(&dlci_ioctl_mutex);\n\t\t\tif (dlci_ioctl_hook)\n\t\t\t\terr = dlci_ioctl_hook(cmd, argp);\n\t\t\tmutex_unlock(&dlci_ioctl_mutex);\n\t\t\tbreak;\n\t\tcase SIOCGSKNS:\n\t\t\terr = -EPERM;\n\t\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\t\tbreak;\n\n\t\t\terr = open_related_ns(&net->ns, get_net_ns);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = sock_do_ioctl(net, sock, cmd, arg);\n\t\t\tbreak;\n\t\t}\n\treturn err;\n}\n\nint sock_create_lite(int family, int type, int protocol, struct socket **res)\n{\n\tint err;\n\tstruct socket *sock = NULL;\n\n\terr = security_socket_create(family, type, protocol, 1);\n\tif (err)\n\t\tgoto out;\n\n\tsock = sock_alloc();\n\tif (!sock) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tsock->type = type;\n\terr = security_socket_post_create(sock, family, type, protocol, 1);\n\tif (err)\n\t\tgoto out_release;\n\nout:\n\t*res = sock;\n\treturn err;\nout_release:\n\tsock_release(sock);\n\tsock = NULL;\n\tgoto out;\n}\nEXPORT_SYMBOL(sock_create_lite);\n\n/* No kernel lock held - perfect */\nstatic unsigned int sock_poll(struct file *file, poll_table *wait)\n{\n\tunsigned int busy_flag = 0;\n\tstruct socket *sock;\n\n\t/*\n\t *      We can't return errors to poll, so it's either yes or no.\n\t */\n\tsock = file->private_data;\n\n\tif (sk_can_busy_loop(sock->sk)) {\n\t\t/* this socket can poll_ll so tell the system call */\n\t\tbusy_flag = POLL_BUSY_LOOP;\n\n\t\t/* once, only if requested by syscall */\n\t\tif (wait && (wait->_key & POLL_BUSY_LOOP))\n\t\t\tsk_busy_loop(sock->sk, 1);\n\t}\n\n\treturn busy_flag | sock->ops->poll(file, sock, wait);\n}\n\nstatic int sock_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct socket *sock = file->private_data;\n\n\treturn sock->ops->mmap(file, sock, vma);\n}\n\nstatic int sock_close(struct inode *inode, struct file *filp)\n{\n\tsock_release(SOCKET_I(inode));\n\treturn 0;\n}\n\n/*\n *\tUpdate the socket async list\n *\n *\tFasync_list locking strategy.\n *\n *\t1. fasync_list is modified only under process context socket lock\n *\t   i.e. under semaphore.\n *\t2. fasync_list is used under read_lock(&sk->sk_callback_lock)\n *\t   or under socket lock\n */\n\nstatic int sock_fasync(int fd, struct file *filp, int on)\n{\n\tstruct socket *sock = filp->private_data;\n\tstruct sock *sk = sock->sk;\n\tstruct socket_wq *wq;\n\n\tif (sk == NULL)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\twq = rcu_dereference_protected(sock->wq, lockdep_sock_is_held(sk));\n\tfasync_helper(fd, filp, on, &wq->fasync_list);\n\n\tif (!wq->fasync_list)\n\t\tsock_reset_flag(sk, SOCK_FASYNC);\n\telse\n\t\tsock_set_flag(sk, SOCK_FASYNC);\n\n\trelease_sock(sk);\n\treturn 0;\n}\n\n/* This function may be called only under rcu_lock */\n\nint sock_wake_async(struct socket_wq *wq, int how, int band)\n{\n\tif (!wq || !wq->fasync_list)\n\t\treturn -1;\n\n\tswitch (how) {\n\tcase SOCK_WAKE_WAITD:\n\t\tif (test_bit(SOCKWQ_ASYNC_WAITDATA, &wq->flags))\n\t\t\tbreak;\n\t\tgoto call_kill;\n\tcase SOCK_WAKE_SPACE:\n\t\tif (!test_and_clear_bit(SOCKWQ_ASYNC_NOSPACE, &wq->flags))\n\t\t\tbreak;\n\t\t/* fall through */\n\tcase SOCK_WAKE_IO:\ncall_kill:\n\t\tkill_fasync(&wq->fasync_list, SIGIO, band);\n\t\tbreak;\n\tcase SOCK_WAKE_URG:\n\t\tkill_fasync(&wq->fasync_list, SIGURG, band);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_wake_async);\n\nint __sock_create(struct net *net, int family, int type, int protocol,\n\t\t\t struct socket **res, int kern)\n{\n\tint err;\n\tstruct socket *sock;\n\tconst struct net_proto_family *pf;\n\n\t/*\n\t *      Check protocol is in range\n\t */\n\tif (family < 0 || family >= NPROTO)\n\t\treturn -EAFNOSUPPORT;\n\tif (type < 0 || type >= SOCK_MAX)\n\t\treturn -EINVAL;\n\n\t/* Compatibility.\n\n\t   This uglymoron is moved from INET layer to here to avoid\n\t   deadlock in module load.\n\t */\n\tif (family == PF_INET && type == SOCK_PACKET) {\n\t\tpr_info_once(\"%s uses obsolete (PF_INET,SOCK_PACKET)\\n\",\n\t\t\t     current->comm);\n\t\tfamily = PF_PACKET;\n\t}\n\n\terr = security_socket_create(family, type, protocol, kern);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t *\tAllocate the socket and allow the family to set things up. if\n\t *\tthe protocol is 0, the family is instructed to select an appropriate\n\t *\tdefault.\n\t */\n\tsock = sock_alloc();\n\tif (!sock) {\n\t\tnet_warn_ratelimited(\"socket: no more sockets\\n\");\n\t\treturn -ENFILE;\t/* Not exactly a match, but its the\n\t\t\t\t   closest posix thing */\n\t}\n\n\tsock->type = type;\n\n#ifdef CONFIG_MODULES\n\t/* Attempt to load a protocol module if the find failed.\n\t *\n\t * 12/09/1996 Marcin: But! this makes REALLY only sense, if the user\n\t * requested real, full-featured networking support upon configuration.\n\t * Otherwise module support will break!\n\t */\n\tif (rcu_access_pointer(net_families[family]) == NULL)\n\t\trequest_module(\"net-pf-%d\", family);\n#endif\n\n\trcu_read_lock();\n\tpf = rcu_dereference(net_families[family]);\n\terr = -EAFNOSUPPORT;\n\tif (!pf)\n\t\tgoto out_release;\n\n\t/*\n\t * We will call the ->create function, that possibly is in a loadable\n\t * module, so we have to bump that loadable module refcnt first.\n\t */\n\tif (!try_module_get(pf->owner))\n\t\tgoto out_release;\n\n\t/* Now protected by module ref count */\n\trcu_read_unlock();\n\n\terr = pf->create(net, sock, protocol, kern);\n\tif (err < 0)\n\t\tgoto out_module_put;\n\n\t/*\n\t * Now to bump the refcnt of the [loadable] module that owns this\n\t * socket at sock_release time we decrement its refcnt.\n\t */\n\tif (!try_module_get(sock->ops->owner))\n\t\tgoto out_module_busy;\n\n\t/*\n\t * Now that we're done with the ->create function, the [loadable]\n\t * module can have its refcnt decremented\n\t */\n\tmodule_put(pf->owner);\n\terr = security_socket_post_create(sock, family, type, protocol, kern);\n\tif (err)\n\t\tgoto out_sock_release;\n\t*res = sock;\n\n\treturn 0;\n\nout_module_busy:\n\terr = -EAFNOSUPPORT;\nout_module_put:\n\tsock->ops = NULL;\n\tmodule_put(pf->owner);\nout_sock_release:\n\tsock_release(sock);\n\treturn err;\n\nout_release:\n\trcu_read_unlock();\n\tgoto out_sock_release;\n}\nEXPORT_SYMBOL(__sock_create);\n\nint sock_create(int family, int type, int protocol, struct socket **res)\n{\n\treturn __sock_create(current->nsproxy->net_ns, family, type, protocol, res, 0);\n}\nEXPORT_SYMBOL(sock_create);\n\nint sock_create_kern(struct net *net, int family, int type, int protocol, struct socket **res)\n{\n\treturn __sock_create(net, family, type, protocol, res, 1);\n}\nEXPORT_SYMBOL(sock_create_kern);\n\nSYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)\n{\n\tint retval;\n\tstruct socket *sock;\n\tint flags;\n\n\t/* Check the SOCK_* constants for consistency.  */\n\tBUILD_BUG_ON(SOCK_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON((SOCK_MAX | SOCK_TYPE_MASK) != SOCK_TYPE_MASK);\n\tBUILD_BUG_ON(SOCK_CLOEXEC & SOCK_TYPE_MASK);\n\tBUILD_BUG_ON(SOCK_NONBLOCK & SOCK_TYPE_MASK);\n\n\tflags = type & ~SOCK_TYPE_MASK;\n\tif (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))\n\t\treturn -EINVAL;\n\ttype &= SOCK_TYPE_MASK;\n\n\tif (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))\n\t\tflags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;\n\n\tretval = sock_create(family, type, protocol, &sock);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = sock_map_fd(sock, flags & (O_CLOEXEC | O_NONBLOCK));\n\tif (retval < 0)\n\t\tgoto out_release;\n\nout:\n\t/* It may be already another descriptor 8) Not kernel problem. */\n\treturn retval;\n\nout_release:\n\tsock_release(sock);\n\treturn retval;\n}\n\n/*\n *\tCreate a pair of connected sockets.\n */\n\nSYSCALL_DEFINE4(socketpair, int, family, int, type, int, protocol,\n\t\tint __user *, usockvec)\n{\n\tstruct socket *sock1, *sock2;\n\tint fd1, fd2, err;\n\tstruct file *newfile1, *newfile2;\n\tint flags;\n\n\tflags = type & ~SOCK_TYPE_MASK;\n\tif (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))\n\t\treturn -EINVAL;\n\ttype &= SOCK_TYPE_MASK;\n\n\tif (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))\n\t\tflags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;\n\n\t/*\n\t * Obtain the first socket and check if the underlying protocol\n\t * supports the socketpair call.\n\t */\n\n\terr = sock_create(family, type, protocol, &sock1);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = sock_create(family, type, protocol, &sock2);\n\tif (err < 0)\n\t\tgoto out_release_1;\n\n\terr = sock1->ops->socketpair(sock1, sock2);\n\tif (err < 0)\n\t\tgoto out_release_both;\n\n\tfd1 = get_unused_fd_flags(flags);\n\tif (unlikely(fd1 < 0)) {\n\t\terr = fd1;\n\t\tgoto out_release_both;\n\t}\n\n\tfd2 = get_unused_fd_flags(flags);\n\tif (unlikely(fd2 < 0)) {\n\t\terr = fd2;\n\t\tgoto out_put_unused_1;\n\t}\n\n\tnewfile1 = sock_alloc_file(sock1, flags, NULL);\n\tif (IS_ERR(newfile1)) {\n\t\terr = PTR_ERR(newfile1);\n\t\tgoto out_put_unused_both;\n\t}\n\n\tnewfile2 = sock_alloc_file(sock2, flags, NULL);\n\tif (IS_ERR(newfile2)) {\n\t\terr = PTR_ERR(newfile2);\n\t\tgoto out_fput_1;\n\t}\n\n\terr = put_user(fd1, &usockvec[0]);\n\tif (err)\n\t\tgoto out_fput_both;\n\n\terr = put_user(fd2, &usockvec[1]);\n\tif (err)\n\t\tgoto out_fput_both;\n\n\taudit_fd_pair(fd1, fd2);\n\n\tfd_install(fd1, newfile1);\n\tfd_install(fd2, newfile2);\n\t/* fd1 and fd2 may be already another descriptors.\n\t * Not kernel problem.\n\t */\n\n\treturn 0;\n\nout_fput_both:\n\tfput(newfile2);\n\tfput(newfile1);\n\tput_unused_fd(fd2);\n\tput_unused_fd(fd1);\n\tgoto out;\n\nout_fput_1:\n\tfput(newfile1);\n\tput_unused_fd(fd2);\n\tput_unused_fd(fd1);\n\tsock_release(sock2);\n\tgoto out;\n\nout_put_unused_both:\n\tput_unused_fd(fd2);\nout_put_unused_1:\n\tput_unused_fd(fd1);\nout_release_both:\n\tsock_release(sock2);\nout_release_1:\n\tsock_release(sock1);\nout:\n\treturn err;\n}\n\n/*\n *\tBind a name to a socket. Nothing much to do here since it's\n *\tthe protocol's responsibility to handle the local address.\n *\n *\tWe move the socket address to kernel space before we call\n *\tthe protocol layer (having also checked the address is ok).\n */\n\nSYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint err, fput_needed;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock) {\n\t\terr = move_addr_to_kernel(umyaddr, addrlen, &address);\n\t\tif (err >= 0) {\n\t\t\terr = security_socket_bind(sock,\n\t\t\t\t\t\t   (struct sockaddr *)&address,\n\t\t\t\t\t\t   addrlen);\n\t\t\tif (!err)\n\t\t\t\terr = sock->ops->bind(sock,\n\t\t\t\t\t\t      (struct sockaddr *)\n\t\t\t\t\t\t      &address, addrlen);\n\t\t}\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tPerform a listen. Basically, we allow the protocol to do anything\n *\tnecessary for a listen, and if that works, we mark the socket as\n *\tready for listening.\n */\n\nSYSCALL_DEFINE2(listen, int, fd, int, backlog)\n{\n\tstruct socket *sock;\n\tint err, fput_needed;\n\tint somaxconn;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock) {\n\t\tsomaxconn = sock_net(sock->sk)->core.sysctl_somaxconn;\n\t\tif ((unsigned int)backlog > somaxconn)\n\t\t\tbacklog = somaxconn;\n\n\t\terr = security_socket_listen(sock, backlog);\n\t\tif (!err)\n\t\t\terr = sock->ops->listen(sock, backlog);\n\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tFor accept, we attempt to create a new socket, set up the link\n *\twith the client, wake up the client, then return the new\n *\tconnected fd. We collect the address of the connector in kernel\n *\tspace and move it to user at the very end. This is unclean because\n *\twe open the socket then return an error.\n *\n *\t1003.1g adds the ability to recvmsg() to query connection pending\n *\tstatus to recvmsg. We need to add that support in a way thats\n *\tclean when we restucture accept also.\n */\n\nSYSCALL_DEFINE4(accept4, int, fd, struct sockaddr __user *, upeer_sockaddr,\n\t\tint __user *, upeer_addrlen, int, flags)\n{\n\tstruct socket *sock, *newsock;\n\tstruct file *newfile;\n\tint err, len, newfd, fput_needed;\n\tstruct sockaddr_storage address;\n\n\tif (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))\n\t\treturn -EINVAL;\n\n\tif (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))\n\t\tflags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\terr = -ENFILE;\n\tnewsock = sock_alloc();\n\tif (!newsock)\n\t\tgoto out_put;\n\n\tnewsock->type = sock->type;\n\tnewsock->ops = sock->ops;\n\n\t/*\n\t * We don't need try_module_get here, as the listening socket (sock)\n\t * has the protocol module (sock->ops->owner) held.\n\t */\n\t__module_get(newsock->ops->owner);\n\n\tnewfd = get_unused_fd_flags(flags);\n\tif (unlikely(newfd < 0)) {\n\t\terr = newfd;\n\t\tsock_release(newsock);\n\t\tgoto out_put;\n\t}\n\tnewfile = sock_alloc_file(newsock, flags, sock->sk->sk_prot_creator->name);\n\tif (IS_ERR(newfile)) {\n\t\terr = PTR_ERR(newfile);\n\t\tput_unused_fd(newfd);\n\t\tsock_release(newsock);\n\t\tgoto out_put;\n\t}\n\n\terr = security_socket_accept(sock, newsock);\n\tif (err)\n\t\tgoto out_fd;\n\n\terr = sock->ops->accept(sock, newsock, sock->file->f_flags, false);\n\tif (err < 0)\n\t\tgoto out_fd;\n\n\tif (upeer_sockaddr) {\n\t\tif (newsock->ops->getname(newsock, (struct sockaddr *)&address,\n\t\t\t\t\t  &len, 2) < 0) {\n\t\t\terr = -ECONNABORTED;\n\t\t\tgoto out_fd;\n\t\t}\n\t\terr = move_addr_to_user(&address,\n\t\t\t\t\tlen, upeer_sockaddr, upeer_addrlen);\n\t\tif (err < 0)\n\t\t\tgoto out_fd;\n\t}\n\n\t/* File flags are not inherited via accept() unlike another OSes. */\n\n\tfd_install(newfd, newfile);\n\terr = newfd;\n\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\nout_fd:\n\tfput(newfile);\n\tput_unused_fd(newfd);\n\tgoto out_put;\n}\n\nSYSCALL_DEFINE3(accept, int, fd, struct sockaddr __user *, upeer_sockaddr,\n\t\tint __user *, upeer_addrlen)\n{\n\treturn sys_accept4(fd, upeer_sockaddr, upeer_addrlen, 0);\n}\n\n/*\n *\tAttempt to connect to a socket with the server address.  The address\n *\tis in user space so we verify it is OK and move it to kernel space.\n *\n *\tFor 1003.1g we need to add clean support for a bind to AF_UNSPEC to\n *\tbreak bindings\n *\n *\tNOTE: 1003.1g draft 6.3 is broken with respect to AX.25/NetROM and\n *\tother SEQPACKET protocols that take time to connect() as it doesn't\n *\tinclude the -EINPROGRESS status for such sockets.\n */\n\nSYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr,\n\t\tint, addrlen)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint err, fput_needed;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\terr = move_addr_to_kernel(uservaddr, addrlen, &address);\n\tif (err < 0)\n\t\tgoto out_put;\n\n\terr =\n\t    security_socket_connect(sock, (struct sockaddr *)&address, addrlen);\n\tif (err)\n\t\tgoto out_put;\n\n\terr = sock->ops->connect(sock, (struct sockaddr *)&address, addrlen,\n\t\t\t\t sock->file->f_flags);\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\n/*\n *\tGet the local address ('name') of a socket object. Move the obtained\n *\tname to user space.\n */\n\nSYSCALL_DEFINE3(getsockname, int, fd, struct sockaddr __user *, usockaddr,\n\t\tint __user *, usockaddr_len)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint len, err, fput_needed;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\terr = security_socket_getsockname(sock);\n\tif (err)\n\t\tgoto out_put;\n\n\terr = sock->ops->getname(sock, (struct sockaddr *)&address, &len, 0);\n\tif (err)\n\t\tgoto out_put;\n\terr = move_addr_to_user(&address, len, usockaddr, usockaddr_len);\n\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\n/*\n *\tGet the remote address ('name') of a socket object. Move the obtained\n *\tname to user space.\n */\n\nSYSCALL_DEFINE3(getpeername, int, fd, struct sockaddr __user *, usockaddr,\n\t\tint __user *, usockaddr_len)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint len, err, fput_needed;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock != NULL) {\n\t\terr = security_socket_getpeername(sock);\n\t\tif (err) {\n\t\t\tfput_light(sock->file, fput_needed);\n\t\t\treturn err;\n\t\t}\n\n\t\terr =\n\t\t    sock->ops->getname(sock, (struct sockaddr *)&address, &len,\n\t\t\t\t       1);\n\t\tif (!err)\n\t\t\terr = move_addr_to_user(&address, len, usockaddr,\n\t\t\t\t\t\tusockaddr_len);\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tSend a datagram to a given address. We move the address into kernel\n *\tspace and check the user space data area is readable before invoking\n *\tthe protocol.\n */\n\nSYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint, addr_len)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint err;\n\tstruct msghdr msg;\n\tstruct iovec iov;\n\tint fput_needed;\n\n\terr = import_single_range(WRITE, buff, len, &iov, &msg.msg_iter);\n\tif (unlikely(err))\n\t\treturn err;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\tif (addr) {\n\t\terr = move_addr_to_kernel(addr, addr_len, &address);\n\t\tif (err < 0)\n\t\t\tgoto out_put;\n\t\tmsg.msg_name = (struct sockaddr *)&address;\n\t\tmsg.msg_namelen = addr_len;\n\t}\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\tmsg.msg_flags = flags;\n\terr = sock_sendmsg(sock, &msg);\n\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\n/*\n *\tSend a datagram down a socket.\n */\n\nSYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len,\n\t\tunsigned int, flags)\n{\n\treturn sys_sendto(fd, buff, len, flags, NULL, 0);\n}\n\n/*\n *\tReceive a frame from the socket and optionally record the address of the\n *\tsender. We verify the buffers are writable and if needed move the\n *\tsender address from kernel to user space.\n */\n\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\terr = import_single_range(READ, ubuf, size, &iov, &msg.msg_iter);\n\tif (unlikely(err))\n\t\treturn err;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\t/* Save some cycles and don't copy the address if not needed */\n\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg.msg_namelen = 0;\n\tmsg.msg_iocb = NULL;\n\tmsg.msg_flags = 0;\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\n/*\n *\tReceive a datagram from a socket.\n */\n\nSYSCALL_DEFINE4(recv, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags)\n{\n\treturn sys_recvfrom(fd, ubuf, size, flags, NULL, NULL);\n}\n\n/*\n *\tSet a socket option. Because we don't know the option lengths we have\n *\tto pass the user mode parameter for the protocols to sort out.\n */\n\nSYSCALL_DEFINE5(setsockopt, int, fd, int, level, int, optname,\n\t\tchar __user *, optval, int, optlen)\n{\n\tint err, fput_needed;\n\tstruct socket *sock;\n\n\tif (optlen < 0)\n\t\treturn -EINVAL;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock != NULL) {\n\t\terr = security_socket_setsockopt(sock, level, optname);\n\t\tif (err)\n\t\t\tgoto out_put;\n\n\t\tif (level == SOL_SOCKET)\n\t\t\terr =\n\t\t\t    sock_setsockopt(sock, level, optname, optval,\n\t\t\t\t\t    optlen);\n\t\telse\n\t\t\terr =\n\t\t\t    sock->ops->setsockopt(sock, level, optname, optval,\n\t\t\t\t\t\t  optlen);\nout_put:\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tGet a socket option. Because we don't know the option lengths we have\n *\tto pass a user mode parameter for the protocols to sort out.\n */\n\nSYSCALL_DEFINE5(getsockopt, int, fd, int, level, int, optname,\n\t\tchar __user *, optval, int __user *, optlen)\n{\n\tint err, fput_needed;\n\tstruct socket *sock;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock != NULL) {\n\t\terr = security_socket_getsockopt(sock, level, optname);\n\t\tif (err)\n\t\t\tgoto out_put;\n\n\t\tif (level == SOL_SOCKET)\n\t\t\terr =\n\t\t\t    sock_getsockopt(sock, level, optname, optval,\n\t\t\t\t\t    optlen);\n\t\telse\n\t\t\terr =\n\t\t\t    sock->ops->getsockopt(sock, level, optname, optval,\n\t\t\t\t\t\t  optlen);\nout_put:\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tShutdown a socket.\n */\n\nSYSCALL_DEFINE2(shutdown, int, fd, int, how)\n{\n\tint err, fput_needed;\n\tstruct socket *sock;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock != NULL) {\n\t\terr = security_socket_shutdown(sock, how);\n\t\tif (!err)\n\t\t\terr = sock->ops->shutdown(sock, how);\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/* A couple of helpful macros for getting the address of the 32/64 bit\n * fields which are the same type (int / unsigned) on our platforms.\n */\n#define COMPAT_MSG(msg, member)\t((MSG_CMSG_COMPAT & flags) ? &msg##_compat->member : &msg->member)\n#define COMPAT_NAMELEN(msg)\tCOMPAT_MSG(msg, msg_namelen)\n#define COMPAT_FLAGS(msg)\tCOMPAT_MSG(msg, msg_flags)\n\nstruct used_address {\n\tstruct sockaddr_storage name;\n\tunsigned int name_len;\n};\n\nstatic int copy_msghdr_from_user(struct msghdr *kmsg,\n\t\t\t\t struct user_msghdr __user *umsg,\n\t\t\t\t struct sockaddr __user **save_addr,\n\t\t\t\t struct iovec **iov)\n{\n\tstruct sockaddr __user *uaddr;\n\tstruct iovec __user *uiov;\n\tsize_t nr_segs;\n\tssize_t err;\n\n\tif (!access_ok(VERIFY_READ, umsg, sizeof(*umsg)) ||\n\t    __get_user(uaddr, &umsg->msg_name) ||\n\t    __get_user(kmsg->msg_namelen, &umsg->msg_namelen) ||\n\t    __get_user(uiov, &umsg->msg_iov) ||\n\t    __get_user(nr_segs, &umsg->msg_iovlen) ||\n\t    __get_user(kmsg->msg_control, &umsg->msg_control) ||\n\t    __get_user(kmsg->msg_controllen, &umsg->msg_controllen) ||\n\t    __get_user(kmsg->msg_flags, &umsg->msg_flags))\n\t\treturn -EFAULT;\n\n\tif (!uaddr)\n\t\tkmsg->msg_namelen = 0;\n\n\tif (kmsg->msg_namelen < 0)\n\t\treturn -EINVAL;\n\n\tif (kmsg->msg_namelen > sizeof(struct sockaddr_storage))\n\t\tkmsg->msg_namelen = sizeof(struct sockaddr_storage);\n\n\tif (save_addr)\n\t\t*save_addr = uaddr;\n\n\tif (uaddr && kmsg->msg_namelen) {\n\t\tif (!save_addr) {\n\t\t\terr = move_addr_to_kernel(uaddr, kmsg->msg_namelen,\n\t\t\t\t\t\t  kmsg->msg_name);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t} else {\n\t\tkmsg->msg_name = NULL;\n\t\tkmsg->msg_namelen = 0;\n\t}\n\n\tif (nr_segs > UIO_MAXIOV)\n\t\treturn -EMSGSIZE;\n\n\tkmsg->msg_iocb = NULL;\n\n\treturn import_iovec(save_addr ? READ : WRITE, uiov, nr_segs,\n\t\t\t    UIO_FASTIOV, iov, &kmsg->msg_iter);\n}\n\nstatic int ___sys_sendmsg(struct socket *sock, struct user_msghdr __user *msg,\n\t\t\t struct msghdr *msg_sys, unsigned int flags,\n\t\t\t struct used_address *used_address,\n\t\t\t unsigned int allowed_msghdr_flags)\n{\n\tstruct compat_msghdr __user *msg_compat =\n\t    (struct compat_msghdr __user *)msg;\n\tstruct sockaddr_storage address;\n\tstruct iovec iovstack[UIO_FASTIOV], *iov = iovstack;\n\tunsigned char ctl[sizeof(struct cmsghdr) + 20]\n\t\t\t\t__aligned(sizeof(__kernel_size_t));\n\t/* 20 is size of ipv6_pktinfo */\n\tunsigned char *ctl_buf = ctl;\n\tint ctl_len;\n\tssize_t err;\n\n\tmsg_sys->msg_name = &address;\n\n\tif (MSG_CMSG_COMPAT & flags)\n\t\terr = get_compat_msghdr(msg_sys, msg_compat, NULL, &iov);\n\telse\n\t\terr = copy_msghdr_from_user(msg_sys, msg, NULL, &iov);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = -ENOBUFS;\n\n\tif (msg_sys->msg_controllen > INT_MAX)\n\t\tgoto out_freeiov;\n\tflags |= (msg_sys->msg_flags & allowed_msghdr_flags);\n\tctl_len = msg_sys->msg_controllen;\n\tif ((MSG_CMSG_COMPAT & flags) && ctl_len) {\n\t\terr =\n\t\t    cmsghdr_from_user_compat_to_kern(msg_sys, sock->sk, ctl,\n\t\t\t\t\t\t     sizeof(ctl));\n\t\tif (err)\n\t\t\tgoto out_freeiov;\n\t\tctl_buf = msg_sys->msg_control;\n\t\tctl_len = msg_sys->msg_controllen;\n\t} else if (ctl_len) {\n\t\tBUILD_BUG_ON(sizeof(struct cmsghdr) !=\n\t\t\t     CMSG_ALIGN(sizeof(struct cmsghdr)));\n\t\tif (ctl_len > sizeof(ctl)) {\n\t\t\tctl_buf = sock_kmalloc(sock->sk, ctl_len, GFP_KERNEL);\n\t\t\tif (ctl_buf == NULL)\n\t\t\t\tgoto out_freeiov;\n\t\t}\n\t\terr = -EFAULT;\n\t\t/*\n\t\t * Careful! Before this, msg_sys->msg_control contains a user pointer.\n\t\t * Afterwards, it will be a kernel pointer. Thus the compiler-assisted\n\t\t * checking falls down on this.\n\t\t */\n\t\tif (copy_from_user(ctl_buf,\n\t\t\t\t   (void __user __force *)msg_sys->msg_control,\n\t\t\t\t   ctl_len))\n\t\t\tgoto out_freectl;\n\t\tmsg_sys->msg_control = ctl_buf;\n\t}\n\tmsg_sys->msg_flags = flags;\n\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tmsg_sys->msg_flags |= MSG_DONTWAIT;\n\t/*\n\t * If this is sendmmsg() and current destination address is same as\n\t * previously succeeded address, omit asking LSM's decision.\n\t * used_address->name_len is initialized to UINT_MAX so that the first\n\t * destination address never matches.\n\t */\n\tif (used_address && msg_sys->msg_name &&\n\t    used_address->name_len == msg_sys->msg_namelen &&\n\t    !memcmp(&used_address->name, msg_sys->msg_name,\n\t\t    used_address->name_len)) {\n\t\terr = sock_sendmsg_nosec(sock, msg_sys);\n\t\tgoto out_freectl;\n\t}\n\terr = sock_sendmsg(sock, msg_sys);\n\t/*\n\t * If this is sendmmsg() and sending to current destination address was\n\t * successful, remember it.\n\t */\n\tif (used_address && err >= 0) {\n\t\tused_address->name_len = msg_sys->msg_namelen;\n\t\tif (msg_sys->msg_name)\n\t\t\tmemcpy(&used_address->name, msg_sys->msg_name,\n\t\t\t       used_address->name_len);\n\t}\n\nout_freectl:\n\tif (ctl_buf != ctl)\n\t\tsock_kfree_s(sock->sk, ctl_buf, ctl_len);\nout_freeiov:\n\tkfree(iov);\n\treturn err;\n}\n\n/*\n *\tBSD sendmsg interface\n */\n\nlong __sys_sendmsg(int fd, struct user_msghdr __user *msg, unsigned flags)\n{\n\tint fput_needed, err;\n\tstruct msghdr msg_sys;\n\tstruct socket *sock;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\terr = ___sys_sendmsg(sock, msg, &msg_sys, flags, NULL, 0);\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\nSYSCALL_DEFINE3(sendmsg, int, fd, struct user_msghdr __user *, msg, unsigned int, flags)\n{\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\treturn __sys_sendmsg(fd, msg, flags);\n}\n\n/*\n *\tLinux sendmmsg interface\n */\n\nint __sys_sendmmsg(int fd, struct mmsghdr __user *mmsg, unsigned int vlen,\n\t\t   unsigned int flags)\n{\n\tint fput_needed, err, datagrams;\n\tstruct socket *sock;\n\tstruct mmsghdr __user *entry;\n\tstruct compat_mmsghdr __user *compat_entry;\n\tstruct msghdr msg_sys;\n\tstruct used_address used_address;\n\tunsigned int oflags = flags;\n\n\tif (vlen > UIO_MAXIOV)\n\t\tvlen = UIO_MAXIOV;\n\n\tdatagrams = 0;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\treturn err;\n\n\tused_address.name_len = UINT_MAX;\n\tentry = mmsg;\n\tcompat_entry = (struct compat_mmsghdr __user *)mmsg;\n\terr = 0;\n\tflags |= MSG_BATCH;\n\n\twhile (datagrams < vlen) {\n\t\tif (datagrams == vlen - 1)\n\t\t\tflags = oflags;\n\n\t\tif (MSG_CMSG_COMPAT & flags) {\n\t\t\terr = ___sys_sendmsg(sock, (struct user_msghdr __user *)compat_entry,\n\t\t\t\t\t     &msg_sys, flags, &used_address, MSG_EOR);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\terr = __put_user(err, &compat_entry->msg_len);\n\t\t\t++compat_entry;\n\t\t} else {\n\t\t\terr = ___sys_sendmsg(sock,\n\t\t\t\t\t     (struct user_msghdr __user *)entry,\n\t\t\t\t\t     &msg_sys, flags, &used_address, MSG_EOR);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\terr = put_user(err, &entry->msg_len);\n\t\t\t++entry;\n\t\t}\n\n\t\tif (err)\n\t\t\tbreak;\n\t\t++datagrams;\n\t\tif (msg_data_left(&msg_sys))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\n\tfput_light(sock->file, fput_needed);\n\n\t/* We only return an error if no datagrams were able to be sent */\n\tif (datagrams != 0)\n\t\treturn datagrams;\n\n\treturn err;\n}\n\nSYSCALL_DEFINE4(sendmmsg, int, fd, struct mmsghdr __user *, mmsg,\n\t\tunsigned int, vlen, unsigned int, flags)\n{\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\treturn __sys_sendmmsg(fd, mmsg, vlen, flags);\n}\n\nstatic int ___sys_recvmsg(struct socket *sock, struct user_msghdr __user *msg,\n\t\t\t struct msghdr *msg_sys, unsigned int flags, int nosec)\n{\n\tstruct compat_msghdr __user *msg_compat =\n\t    (struct compat_msghdr __user *)msg;\n\tstruct iovec iovstack[UIO_FASTIOV];\n\tstruct iovec *iov = iovstack;\n\tunsigned long cmsg_ptr;\n\tint len;\n\tssize_t err;\n\n\t/* kernel mode address */\n\tstruct sockaddr_storage addr;\n\n\t/* user mode address pointers */\n\tstruct sockaddr __user *uaddr;\n\tint __user *uaddr_len = COMPAT_NAMELEN(msg);\n\n\tmsg_sys->msg_name = &addr;\n\n\tif (MSG_CMSG_COMPAT & flags)\n\t\terr = get_compat_msghdr(msg_sys, msg_compat, &uaddr, &iov);\n\telse\n\t\terr = copy_msghdr_from_user(msg_sys, msg, &uaddr, &iov);\n\tif (err < 0)\n\t\treturn err;\n\n\tcmsg_ptr = (unsigned long)msg_sys->msg_control;\n\tmsg_sys->msg_flags = flags & (MSG_CMSG_CLOEXEC|MSG_CMSG_COMPAT);\n\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg_sys->msg_namelen = 0;\n\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = (nosec ? sock_recvmsg_nosec : sock_recvmsg)(sock, msg_sys, flags);\n\tif (err < 0)\n\t\tgoto out_freeiov;\n\tlen = err;\n\n\tif (uaddr != NULL) {\n\t\terr = move_addr_to_user(&addr,\n\t\t\t\t\tmsg_sys->msg_namelen, uaddr,\n\t\t\t\t\tuaddr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_freeiov;\n\t}\n\terr = __put_user((msg_sys->msg_flags & ~MSG_CMSG_COMPAT),\n\t\t\t COMPAT_FLAGS(msg));\n\tif (err)\n\t\tgoto out_freeiov;\n\tif (MSG_CMSG_COMPAT & flags)\n\t\terr = __put_user((unsigned long)msg_sys->msg_control - cmsg_ptr,\n\t\t\t\t &msg_compat->msg_controllen);\n\telse\n\t\terr = __put_user((unsigned long)msg_sys->msg_control - cmsg_ptr,\n\t\t\t\t &msg->msg_controllen);\n\tif (err)\n\t\tgoto out_freeiov;\n\terr = len;\n\nout_freeiov:\n\tkfree(iov);\n\treturn err;\n}\n\n/*\n *\tBSD recvmsg interface\n */\n\nlong __sys_recvmsg(int fd, struct user_msghdr __user *msg, unsigned flags)\n{\n\tint fput_needed, err;\n\tstruct msghdr msg_sys;\n\tstruct socket *sock;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\terr = ___sys_recvmsg(sock, msg, &msg_sys, flags, 0);\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\nSYSCALL_DEFINE3(recvmsg, int, fd, struct user_msghdr __user *, msg,\n\t\tunsigned int, flags)\n{\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\treturn __sys_recvmsg(fd, msg, flags);\n}\n\n/*\n *     Linux recvmmsg interface\n */\n\nint __sys_recvmmsg(int fd, struct mmsghdr __user *mmsg, unsigned int vlen,\n\t\t   unsigned int flags, struct timespec *timeout)\n{\n\tint fput_needed, err, datagrams;\n\tstruct socket *sock;\n\tstruct mmsghdr __user *entry;\n\tstruct compat_mmsghdr __user *compat_entry;\n\tstruct msghdr msg_sys;\n\tstruct timespec64 end_time;\n\tstruct timespec64 timeout64;\n\n\tif (timeout &&\n\t    poll_select_set_timeout(&end_time, timeout->tv_sec,\n\t\t\t\t    timeout->tv_nsec))\n\t\treturn -EINVAL;\n\n\tdatagrams = 0;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\treturn err;\n\n\terr = sock_error(sock->sk);\n\tif (err) {\n\t\tdatagrams = err;\n\t\tgoto out_put;\n\t}\n\n\tentry = mmsg;\n\tcompat_entry = (struct compat_mmsghdr __user *)mmsg;\n\n\twhile (datagrams < vlen) {\n\t\t/*\n\t\t * No need to ask LSM for more than the first datagram.\n\t\t */\n\t\tif (MSG_CMSG_COMPAT & flags) {\n\t\t\terr = ___sys_recvmsg(sock, (struct user_msghdr __user *)compat_entry,\n\t\t\t\t\t     &msg_sys, flags & ~MSG_WAITFORONE,\n\t\t\t\t\t     datagrams);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\terr = __put_user(err, &compat_entry->msg_len);\n\t\t\t++compat_entry;\n\t\t} else {\n\t\t\terr = ___sys_recvmsg(sock,\n\t\t\t\t\t     (struct user_msghdr __user *)entry,\n\t\t\t\t\t     &msg_sys, flags & ~MSG_WAITFORONE,\n\t\t\t\t\t     datagrams);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\terr = put_user(err, &entry->msg_len);\n\t\t\t++entry;\n\t\t}\n\n\t\tif (err)\n\t\t\tbreak;\n\t\t++datagrams;\n\n\t\t/* MSG_WAITFORONE turns on MSG_DONTWAIT after one packet */\n\t\tif (flags & MSG_WAITFORONE)\n\t\t\tflags |= MSG_DONTWAIT;\n\n\t\tif (timeout) {\n\t\t\tktime_get_ts64(&timeout64);\n\t\t\t*timeout = timespec64_to_timespec(\n\t\t\t\t\ttimespec64_sub(end_time, timeout64));\n\t\t\tif (timeout->tv_sec < 0) {\n\t\t\t\ttimeout->tv_sec = timeout->tv_nsec = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Timeout, return less than vlen datagrams */\n\t\t\tif (timeout->tv_nsec == 0 && timeout->tv_sec == 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/* Out of band data, return right away */\n\t\tif (msg_sys.msg_flags & MSG_OOB)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\n\tif (err == 0)\n\t\tgoto out_put;\n\n\tif (datagrams == 0) {\n\t\tdatagrams = err;\n\t\tgoto out_put;\n\t}\n\n\t/*\n\t * We may return less entries than requested (vlen) if the\n\t * sock is non block and there aren't enough datagrams...\n\t */\n\tif (err != -EAGAIN) {\n\t\t/*\n\t\t * ... or  if recvmsg returns an error after we\n\t\t * received some datagrams, where we record the\n\t\t * error to return on the next call or if the\n\t\t * app asks about it using getsockopt(SO_ERROR).\n\t\t */\n\t\tsock->sk->sk_err = -err;\n\t}\nout_put:\n\tfput_light(sock->file, fput_needed);\n\n\treturn datagrams;\n}\n\nSYSCALL_DEFINE5(recvmmsg, int, fd, struct mmsghdr __user *, mmsg,\n\t\tunsigned int, vlen, unsigned int, flags,\n\t\tstruct timespec __user *, timeout)\n{\n\tint datagrams;\n\tstruct timespec timeout_sys;\n\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\n\tif (!timeout)\n\t\treturn __sys_recvmmsg(fd, mmsg, vlen, flags, NULL);\n\n\tif (copy_from_user(&timeout_sys, timeout, sizeof(timeout_sys)))\n\t\treturn -EFAULT;\n\n\tdatagrams = __sys_recvmmsg(fd, mmsg, vlen, flags, &timeout_sys);\n\n\tif (datagrams > 0 &&\n\t    copy_to_user(timeout, &timeout_sys, sizeof(timeout_sys)))\n\t\tdatagrams = -EFAULT;\n\n\treturn datagrams;\n}\n\n#ifdef __ARCH_WANT_SYS_SOCKETCALL\n/* Argument list sizes for sys_socketcall */\n#define AL(x) ((x) * sizeof(unsigned long))\nstatic const unsigned char nargs[21] = {\n\tAL(0), AL(3), AL(3), AL(3), AL(2), AL(3),\n\tAL(3), AL(3), AL(4), AL(4), AL(4), AL(6),\n\tAL(6), AL(2), AL(5), AL(5), AL(3), AL(3),\n\tAL(4), AL(5), AL(4)\n};\n\n#undef AL\n\n/*\n *\tSystem call vectors.\n *\n *\tArgument checking cleaned up. Saved 20% in size.\n *  This function doesn't need to set the kernel lock because\n *  it is set by the callees.\n */\n\nSYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)\n{\n\tunsigned long a[AUDITSC_ARGS];\n\tunsigned long a0, a1;\n\tint err;\n\tunsigned int len;\n\n\tif (call < 1 || call > SYS_SENDMMSG)\n\t\treturn -EINVAL;\n\n\tlen = nargs[call];\n\tif (len > sizeof(a))\n\t\treturn -EINVAL;\n\n\t/* copy_from_user should be SMP safe. */\n\tif (copy_from_user(a, args, len))\n\t\treturn -EFAULT;\n\n\terr = audit_socketcall(nargs[call] / sizeof(unsigned long), a);\n\tif (err)\n\t\treturn err;\n\n\ta0 = a[0];\n\ta1 = a[1];\n\n\tswitch (call) {\n\tcase SYS_SOCKET:\n\t\terr = sys_socket(a0, a1, a[2]);\n\t\tbreak;\n\tcase SYS_BIND:\n\t\terr = sys_bind(a0, (struct sockaddr __user *)a1, a[2]);\n\t\tbreak;\n\tcase SYS_CONNECT:\n\t\terr = sys_connect(a0, (struct sockaddr __user *)a1, a[2]);\n\t\tbreak;\n\tcase SYS_LISTEN:\n\t\terr = sys_listen(a0, a1);\n\t\tbreak;\n\tcase SYS_ACCEPT:\n\t\terr = sys_accept4(a0, (struct sockaddr __user *)a1,\n\t\t\t\t  (int __user *)a[2], 0);\n\t\tbreak;\n\tcase SYS_GETSOCKNAME:\n\t\terr =\n\t\t    sys_getsockname(a0, (struct sockaddr __user *)a1,\n\t\t\t\t    (int __user *)a[2]);\n\t\tbreak;\n\tcase SYS_GETPEERNAME:\n\t\terr =\n\t\t    sys_getpeername(a0, (struct sockaddr __user *)a1,\n\t\t\t\t    (int __user *)a[2]);\n\t\tbreak;\n\tcase SYS_SOCKETPAIR:\n\t\terr = sys_socketpair(a0, a1, a[2], (int __user *)a[3]);\n\t\tbreak;\n\tcase SYS_SEND:\n\t\terr = sys_send(a0, (void __user *)a1, a[2], a[3]);\n\t\tbreak;\n\tcase SYS_SENDTO:\n\t\terr = sys_sendto(a0, (void __user *)a1, a[2], a[3],\n\t\t\t\t (struct sockaddr __user *)a[4], a[5]);\n\t\tbreak;\n\tcase SYS_RECV:\n\t\terr = sys_recv(a0, (void __user *)a1, a[2], a[3]);\n\t\tbreak;\n\tcase SYS_RECVFROM:\n\t\terr = sys_recvfrom(a0, (void __user *)a1, a[2], a[3],\n\t\t\t\t   (struct sockaddr __user *)a[4],\n\t\t\t\t   (int __user *)a[5]);\n\t\tbreak;\n\tcase SYS_SHUTDOWN:\n\t\terr = sys_shutdown(a0, a1);\n\t\tbreak;\n\tcase SYS_SETSOCKOPT:\n\t\terr = sys_setsockopt(a0, a1, a[2], (char __user *)a[3], a[4]);\n\t\tbreak;\n\tcase SYS_GETSOCKOPT:\n\t\terr =\n\t\t    sys_getsockopt(a0, a1, a[2], (char __user *)a[3],\n\t\t\t\t   (int __user *)a[4]);\n\t\tbreak;\n\tcase SYS_SENDMSG:\n\t\terr = sys_sendmsg(a0, (struct user_msghdr __user *)a1, a[2]);\n\t\tbreak;\n\tcase SYS_SENDMMSG:\n\t\terr = sys_sendmmsg(a0, (struct mmsghdr __user *)a1, a[2], a[3]);\n\t\tbreak;\n\tcase SYS_RECVMSG:\n\t\terr = sys_recvmsg(a0, (struct user_msghdr __user *)a1, a[2]);\n\t\tbreak;\n\tcase SYS_RECVMMSG:\n\t\terr = sys_recvmmsg(a0, (struct mmsghdr __user *)a1, a[2], a[3],\n\t\t\t\t   (struct timespec __user *)a[4]);\n\t\tbreak;\n\tcase SYS_ACCEPT4:\n\t\terr = sys_accept4(a0, (struct sockaddr __user *)a1,\n\t\t\t\t  (int __user *)a[2], a[3]);\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn err;\n}\n\n#endif\t\t\t\t/* __ARCH_WANT_SYS_SOCKETCALL */\n\n/**\n *\tsock_register - add a socket protocol handler\n *\t@ops: description of protocol\n *\n *\tThis function is called by a protocol handler that wants to\n *\tadvertise its address family, and have it linked into the\n *\tsocket interface. The value ops->family corresponds to the\n *\tsocket system call protocol family.\n */\nint sock_register(const struct net_proto_family *ops)\n{\n\tint err;\n\n\tif (ops->family >= NPROTO) {\n\t\tpr_crit(\"protocol %d >= NPROTO(%d)\\n\", ops->family, NPROTO);\n\t\treturn -ENOBUFS;\n\t}\n\n\tspin_lock(&net_family_lock);\n\tif (rcu_dereference_protected(net_families[ops->family],\n\t\t\t\t      lockdep_is_held(&net_family_lock)))\n\t\terr = -EEXIST;\n\telse {\n\t\trcu_assign_pointer(net_families[ops->family], ops);\n\t\terr = 0;\n\t}\n\tspin_unlock(&net_family_lock);\n\n\tpr_info(\"NET: Registered protocol family %d\\n\", ops->family);\n\treturn err;\n}\nEXPORT_SYMBOL(sock_register);\n\n/**\n *\tsock_unregister - remove a protocol handler\n *\t@family: protocol family to remove\n *\n *\tThis function is called by a protocol handler that wants to\n *\tremove its address family, and have it unlinked from the\n *\tnew socket creation.\n *\n *\tIf protocol handler is a module, then it can use module reference\n *\tcounts to protect against new references. If protocol handler is not\n *\ta module then it needs to provide its own protection in\n *\tthe ops->create routine.\n */\nvoid sock_unregister(int family)\n{\n\tBUG_ON(family < 0 || family >= NPROTO);\n\n\tspin_lock(&net_family_lock);\n\tRCU_INIT_POINTER(net_families[family], NULL);\n\tspin_unlock(&net_family_lock);\n\n\tsynchronize_rcu();\n\n\tpr_info(\"NET: Unregistered protocol family %d\\n\", family);\n}\nEXPORT_SYMBOL(sock_unregister);\n\nstatic int __init sock_init(void)\n{\n\tint err;\n\t/*\n\t *      Initialize the network sysctl infrastructure.\n\t */\n\terr = net_sysctl_init();\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t *      Initialize skbuff SLAB cache\n\t */\n\tskb_init();\n\n\t/*\n\t *      Initialize the protocols module.\n\t */\n\n\tinit_inodecache();\n\n\terr = register_filesystem(&sock_fs_type);\n\tif (err)\n\t\tgoto out_fs;\n\tsock_mnt = kern_mount(&sock_fs_type);\n\tif (IS_ERR(sock_mnt)) {\n\t\terr = PTR_ERR(sock_mnt);\n\t\tgoto out_mount;\n\t}\n\n\t/* The real protocol initialization is performed in later initcalls.\n\t */\n\n#ifdef CONFIG_NETFILTER\n\terr = netfilter_init();\n\tif (err)\n\t\tgoto out;\n#endif\n\n\tptp_classifier_init();\n\nout:\n\treturn err;\n\nout_mount:\n\tunregister_filesystem(&sock_fs_type);\nout_fs:\n\tgoto out;\n}\n\ncore_initcall(sock_init);\t/* early initcall */\n\n#ifdef CONFIG_PROC_FS\nvoid socket_seq_show(struct seq_file *seq)\n{\n\tint cpu;\n\tint counter = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t    counter += per_cpu(sockets_in_use, cpu);\n\n\t/* It can be negative, by the way. 8) */\n\tif (counter < 0)\n\t\tcounter = 0;\n\n\tseq_printf(seq, \"sockets: used %d\\n\", counter);\n}\n#endif\t\t\t\t/* CONFIG_PROC_FS */\n\n#ifdef CONFIG_COMPAT\nstatic int do_siocgstamp(struct net *net, struct socket *sock,\n\t\t\t unsigned int cmd, void __user *up)\n{\n\tmm_segment_t old_fs = get_fs();\n\tstruct timeval ktv;\n\tint err;\n\n\tset_fs(KERNEL_DS);\n\terr = sock_do_ioctl(net, sock, cmd, (unsigned long)&ktv);\n\tset_fs(old_fs);\n\tif (!err)\n\t\terr = compat_put_timeval(&ktv, up);\n\n\treturn err;\n}\n\nstatic int do_siocgstampns(struct net *net, struct socket *sock,\n\t\t\t   unsigned int cmd, void __user *up)\n{\n\tmm_segment_t old_fs = get_fs();\n\tstruct timespec kts;\n\tint err;\n\n\tset_fs(KERNEL_DS);\n\terr = sock_do_ioctl(net, sock, cmd, (unsigned long)&kts);\n\tset_fs(old_fs);\n\tif (!err)\n\t\terr = compat_put_timespec(&kts, up);\n\n\treturn err;\n}\n\nstatic int dev_ifname32(struct net *net, struct compat_ifreq __user *uifr32)\n{\n\tstruct ifreq __user *uifr;\n\tint err;\n\n\tuifr = compat_alloc_user_space(sizeof(struct ifreq));\n\tif (copy_in_user(uifr, uifr32, sizeof(struct compat_ifreq)))\n\t\treturn -EFAULT;\n\n\terr = dev_ioctl(net, SIOCGIFNAME, uifr);\n\tif (err)\n\t\treturn err;\n\n\tif (copy_in_user(uifr32, uifr, sizeof(struct compat_ifreq)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int dev_ifconf(struct net *net, struct compat_ifconf __user *uifc32)\n{\n\tstruct compat_ifconf ifc32;\n\tstruct ifconf ifc;\n\tstruct ifconf __user *uifc;\n\tstruct compat_ifreq __user *ifr32;\n\tstruct ifreq __user *ifr;\n\tunsigned int i, j;\n\tint err;\n\n\tif (copy_from_user(&ifc32, uifc32, sizeof(struct compat_ifconf)))\n\t\treturn -EFAULT;\n\n\tmemset(&ifc, 0, sizeof(ifc));\n\tif (ifc32.ifcbuf == 0) {\n\t\tifc32.ifc_len = 0;\n\t\tifc.ifc_len = 0;\n\t\tifc.ifc_req = NULL;\n\t\tuifc = compat_alloc_user_space(sizeof(struct ifconf));\n\t} else {\n\t\tsize_t len = ((ifc32.ifc_len / sizeof(struct compat_ifreq)) + 1) *\n\t\t\tsizeof(struct ifreq);\n\t\tuifc = compat_alloc_user_space(sizeof(struct ifconf) + len);\n\t\tifc.ifc_len = len;\n\t\tifr = ifc.ifc_req = (void __user *)(uifc + 1);\n\t\tifr32 = compat_ptr(ifc32.ifcbuf);\n\t\tfor (i = 0; i < ifc32.ifc_len; i += sizeof(struct compat_ifreq)) {\n\t\t\tif (copy_in_user(ifr, ifr32, sizeof(struct compat_ifreq)))\n\t\t\t\treturn -EFAULT;\n\t\t\tifr++;\n\t\t\tifr32++;\n\t\t}\n\t}\n\tif (copy_to_user(uifc, &ifc, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\terr = dev_ioctl(net, SIOCGIFCONF, uifc);\n\tif (err)\n\t\treturn err;\n\n\tif (copy_from_user(&ifc, uifc, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\tifr = ifc.ifc_req;\n\tifr32 = compat_ptr(ifc32.ifcbuf);\n\tfor (i = 0, j = 0;\n\t     i + sizeof(struct compat_ifreq) <= ifc32.ifc_len && j < ifc.ifc_len;\n\t     i += sizeof(struct compat_ifreq), j += sizeof(struct ifreq)) {\n\t\tif (copy_in_user(ifr32, ifr, sizeof(struct compat_ifreq)))\n\t\t\treturn -EFAULT;\n\t\tifr32++;\n\t\tifr++;\n\t}\n\n\tif (ifc32.ifcbuf == 0) {\n\t\t/* Translate from 64-bit structure multiple to\n\t\t * a 32-bit one.\n\t\t */\n\t\ti = ifc.ifc_len;\n\t\ti = ((i / sizeof(struct ifreq)) * sizeof(struct compat_ifreq));\n\t\tifc32.ifc_len = i;\n\t} else {\n\t\tifc32.ifc_len = i;\n\t}\n\tif (copy_to_user(uifc32, &ifc32, sizeof(struct compat_ifconf)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int ethtool_ioctl(struct net *net, struct compat_ifreq __user *ifr32)\n{\n\tstruct compat_ethtool_rxnfc __user *compat_rxnfc;\n\tbool convert_in = false, convert_out = false;\n\tsize_t buf_size = ALIGN(sizeof(struct ifreq), 8);\n\tstruct ethtool_rxnfc __user *rxnfc;\n\tstruct ifreq __user *ifr;\n\tu32 rule_cnt = 0, actual_rule_cnt;\n\tu32 ethcmd;\n\tu32 data;\n\tint ret;\n\n\tif (get_user(data, &ifr32->ifr_ifru.ifru_data))\n\t\treturn -EFAULT;\n\n\tcompat_rxnfc = compat_ptr(data);\n\n\tif (get_user(ethcmd, &compat_rxnfc->cmd))\n\t\treturn -EFAULT;\n\n\t/* Most ethtool structures are defined without padding.\n\t * Unfortunately struct ethtool_rxnfc is an exception.\n\t */\n\tswitch (ethcmd) {\n\tdefault:\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRLALL:\n\t\t/* Buffer size is variable */\n\t\tif (get_user(rule_cnt, &compat_rxnfc->rule_cnt))\n\t\t\treturn -EFAULT;\n\t\tif (rule_cnt > KMALLOC_MAX_SIZE / sizeof(u32))\n\t\t\treturn -ENOMEM;\n\t\tbuf_size += rule_cnt * sizeof(u32);\n\t\t/* fall through */\n\tcase ETHTOOL_GRXRINGS:\n\tcase ETHTOOL_GRXCLSRLCNT:\n\tcase ETHTOOL_GRXCLSRULE:\n\tcase ETHTOOL_SRXCLSRLINS:\n\t\tconvert_out = true;\n\t\t/* fall through */\n\tcase ETHTOOL_SRXCLSRLDEL:\n\t\tbuf_size += sizeof(struct ethtool_rxnfc);\n\t\tconvert_in = true;\n\t\tbreak;\n\t}\n\n\tifr = compat_alloc_user_space(buf_size);\n\trxnfc = (void __user *)ifr + ALIGN(sizeof(struct ifreq), 8);\n\n\tif (copy_in_user(&ifr->ifr_name, &ifr32->ifr_name, IFNAMSIZ))\n\t\treturn -EFAULT;\n\n\tif (put_user(convert_in ? rxnfc : compat_ptr(data),\n\t\t     &ifr->ifr_ifru.ifru_data))\n\t\treturn -EFAULT;\n\n\tif (convert_in) {\n\t\t/* We expect there to be holes between fs.m_ext and\n\t\t * fs.ring_cookie and at the end of fs, but nowhere else.\n\t\t */\n\t\tBUILD_BUG_ON(offsetof(struct compat_ethtool_rxnfc, fs.m_ext) +\n\t\t\t     sizeof(compat_rxnfc->fs.m_ext) !=\n\t\t\t     offsetof(struct ethtool_rxnfc, fs.m_ext) +\n\t\t\t     sizeof(rxnfc->fs.m_ext));\n\t\tBUILD_BUG_ON(\n\t\t\toffsetof(struct compat_ethtool_rxnfc, fs.location) -\n\t\t\toffsetof(struct compat_ethtool_rxnfc, fs.ring_cookie) !=\n\t\t\toffsetof(struct ethtool_rxnfc, fs.location) -\n\t\t\toffsetof(struct ethtool_rxnfc, fs.ring_cookie));\n\n\t\tif (copy_in_user(rxnfc, compat_rxnfc,\n\t\t\t\t (void __user *)(&rxnfc->fs.m_ext + 1) -\n\t\t\t\t (void __user *)rxnfc) ||\n\t\t    copy_in_user(&rxnfc->fs.ring_cookie,\n\t\t\t\t &compat_rxnfc->fs.ring_cookie,\n\t\t\t\t (void __user *)(&rxnfc->fs.location + 1) -\n\t\t\t\t (void __user *)&rxnfc->fs.ring_cookie) ||\n\t\t    copy_in_user(&rxnfc->rule_cnt, &compat_rxnfc->rule_cnt,\n\t\t\t\t sizeof(rxnfc->rule_cnt)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = dev_ioctl(net, SIOCETHTOOL, ifr);\n\tif (ret)\n\t\treturn ret;\n\n\tif (convert_out) {\n\t\tif (copy_in_user(compat_rxnfc, rxnfc,\n\t\t\t\t (const void __user *)(&rxnfc->fs.m_ext + 1) -\n\t\t\t\t (const void __user *)rxnfc) ||\n\t\t    copy_in_user(&compat_rxnfc->fs.ring_cookie,\n\t\t\t\t &rxnfc->fs.ring_cookie,\n\t\t\t\t (const void __user *)(&rxnfc->fs.location + 1) -\n\t\t\t\t (const void __user *)&rxnfc->fs.ring_cookie) ||\n\t\t    copy_in_user(&compat_rxnfc->rule_cnt, &rxnfc->rule_cnt,\n\t\t\t\t sizeof(rxnfc->rule_cnt)))\n\t\t\treturn -EFAULT;\n\n\t\tif (ethcmd == ETHTOOL_GRXCLSRLALL) {\n\t\t\t/* As an optimisation, we only copy the actual\n\t\t\t * number of rules that the underlying\n\t\t\t * function returned.  Since Mallory might\n\t\t\t * change the rule count in user memory, we\n\t\t\t * check that it is less than the rule count\n\t\t\t * originally given (as the user buffer size),\n\t\t\t * which has been range-checked.\n\t\t\t */\n\t\t\tif (get_user(actual_rule_cnt, &rxnfc->rule_cnt))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (actual_rule_cnt < rule_cnt)\n\t\t\t\trule_cnt = actual_rule_cnt;\n\t\t\tif (copy_in_user(&compat_rxnfc->rule_locs[0],\n\t\t\t\t\t &rxnfc->rule_locs[0],\n\t\t\t\t\t rule_cnt * sizeof(u32)))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int compat_siocwandev(struct net *net, struct compat_ifreq __user *uifr32)\n{\n\tvoid __user *uptr;\n\tcompat_uptr_t uptr32;\n\tstruct ifreq __user *uifr;\n\n\tuifr = compat_alloc_user_space(sizeof(*uifr));\n\tif (copy_in_user(uifr, uifr32, sizeof(struct compat_ifreq)))\n\t\treturn -EFAULT;\n\n\tif (get_user(uptr32, &uifr32->ifr_settings.ifs_ifsu))\n\t\treturn -EFAULT;\n\n\tuptr = compat_ptr(uptr32);\n\n\tif (put_user(uptr, &uifr->ifr_settings.ifs_ifsu.raw_hdlc))\n\t\treturn -EFAULT;\n\n\treturn dev_ioctl(net, SIOCWANDEV, uifr);\n}\n\nstatic int bond_ioctl(struct net *net, unsigned int cmd,\n\t\t\t struct compat_ifreq __user *ifr32)\n{\n\tstruct ifreq kifr;\n\tmm_segment_t old_fs;\n\tint err;\n\n\tswitch (cmd) {\n\tcase SIOCBONDENSLAVE:\n\tcase SIOCBONDRELEASE:\n\tcase SIOCBONDSETHWADDR:\n\tcase SIOCBONDCHANGEACTIVE:\n\t\tif (copy_from_user(&kifr, ifr32, sizeof(struct compat_ifreq)))\n\t\t\treturn -EFAULT;\n\n\t\told_fs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t\terr = dev_ioctl(net, cmd,\n\t\t\t\t(struct ifreq __user __force *) &kifr);\n\t\tset_fs(old_fs);\n\n\t\treturn err;\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n}\n\n/* Handle ioctls that use ifreq::ifr_data and just need struct ifreq converted */\nstatic int compat_ifr_data_ioctl(struct net *net, unsigned int cmd,\n\t\t\t\t struct compat_ifreq __user *u_ifreq32)\n{\n\tstruct ifreq __user *u_ifreq64;\n\tchar tmp_buf[IFNAMSIZ];\n\tvoid __user *data64;\n\tu32 data32;\n\n\tif (copy_from_user(&tmp_buf[0], &(u_ifreq32->ifr_ifrn.ifrn_name[0]),\n\t\t\t   IFNAMSIZ))\n\t\treturn -EFAULT;\n\tif (get_user(data32, &u_ifreq32->ifr_ifru.ifru_data))\n\t\treturn -EFAULT;\n\tdata64 = compat_ptr(data32);\n\n\tu_ifreq64 = compat_alloc_user_space(sizeof(*u_ifreq64));\n\n\tif (copy_to_user(&u_ifreq64->ifr_ifrn.ifrn_name[0], &tmp_buf[0],\n\t\t\t IFNAMSIZ))\n\t\treturn -EFAULT;\n\tif (put_user(data64, &u_ifreq64->ifr_ifru.ifru_data))\n\t\treturn -EFAULT;\n\n\treturn dev_ioctl(net, cmd, u_ifreq64);\n}\n\nstatic int dev_ifsioc(struct net *net, struct socket *sock,\n\t\t\t unsigned int cmd, struct compat_ifreq __user *uifr32)\n{\n\tstruct ifreq __user *uifr;\n\tint err;\n\n\tuifr = compat_alloc_user_space(sizeof(*uifr));\n\tif (copy_in_user(uifr, uifr32, sizeof(*uifr32)))\n\t\treturn -EFAULT;\n\n\terr = sock_do_ioctl(net, sock, cmd, (unsigned long)uifr);\n\n\tif (!err) {\n\t\tswitch (cmd) {\n\t\tcase SIOCGIFFLAGS:\n\t\tcase SIOCGIFMETRIC:\n\t\tcase SIOCGIFMTU:\n\t\tcase SIOCGIFMEM:\n\t\tcase SIOCGIFHWADDR:\n\t\tcase SIOCGIFINDEX:\n\t\tcase SIOCGIFADDR:\n\t\tcase SIOCGIFBRDADDR:\n\t\tcase SIOCGIFDSTADDR:\n\t\tcase SIOCGIFNETMASK:\n\t\tcase SIOCGIFPFLAGS:\n\t\tcase SIOCGIFTXQLEN:\n\t\tcase SIOCGMIIPHY:\n\t\tcase SIOCGMIIREG:\n\t\t\tif (copy_in_user(uifr32, uifr, sizeof(*uifr32)))\n\t\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn err;\n}\n\nstatic int compat_sioc_ifmap(struct net *net, unsigned int cmd,\n\t\t\tstruct compat_ifreq __user *uifr32)\n{\n\tstruct ifreq ifr;\n\tstruct compat_ifmap __user *uifmap32;\n\tmm_segment_t old_fs;\n\tint err;\n\n\tuifmap32 = &uifr32->ifr_ifru.ifru_map;\n\terr = copy_from_user(&ifr, uifr32, sizeof(ifr.ifr_name));\n\terr |= get_user(ifr.ifr_map.mem_start, &uifmap32->mem_start);\n\terr |= get_user(ifr.ifr_map.mem_end, &uifmap32->mem_end);\n\terr |= get_user(ifr.ifr_map.base_addr, &uifmap32->base_addr);\n\terr |= get_user(ifr.ifr_map.irq, &uifmap32->irq);\n\terr |= get_user(ifr.ifr_map.dma, &uifmap32->dma);\n\terr |= get_user(ifr.ifr_map.port, &uifmap32->port);\n\tif (err)\n\t\treturn -EFAULT;\n\n\told_fs = get_fs();\n\tset_fs(KERNEL_DS);\n\terr = dev_ioctl(net, cmd, (void  __user __force *)&ifr);\n\tset_fs(old_fs);\n\n\tif (cmd == SIOCGIFMAP && !err) {\n\t\terr = copy_to_user(uifr32, &ifr, sizeof(ifr.ifr_name));\n\t\terr |= put_user(ifr.ifr_map.mem_start, &uifmap32->mem_start);\n\t\terr |= put_user(ifr.ifr_map.mem_end, &uifmap32->mem_end);\n\t\terr |= put_user(ifr.ifr_map.base_addr, &uifmap32->base_addr);\n\t\terr |= put_user(ifr.ifr_map.irq, &uifmap32->irq);\n\t\terr |= put_user(ifr.ifr_map.dma, &uifmap32->dma);\n\t\terr |= put_user(ifr.ifr_map.port, &uifmap32->port);\n\t\tif (err)\n\t\t\terr = -EFAULT;\n\t}\n\treturn err;\n}\n\nstruct rtentry32 {\n\tu32\t\trt_pad1;\n\tstruct sockaddr rt_dst;         /* target address               */\n\tstruct sockaddr rt_gateway;     /* gateway addr (RTF_GATEWAY)   */\n\tstruct sockaddr rt_genmask;     /* target network mask (IP)     */\n\tunsigned short\trt_flags;\n\tshort\t\trt_pad2;\n\tu32\t\trt_pad3;\n\tunsigned char\trt_tos;\n\tunsigned char\trt_class;\n\tshort\t\trt_pad4;\n\tshort\t\trt_metric;      /* +1 for binary compatibility! */\n\t/* char * */ u32 rt_dev;        /* forcing the device at add    */\n\tu32\t\trt_mtu;         /* per route MTU/Window         */\n\tu32\t\trt_window;      /* Window clamping              */\n\tunsigned short  rt_irtt;        /* Initial RTT                  */\n};\n\nstruct in6_rtmsg32 {\n\tstruct in6_addr\t\trtmsg_dst;\n\tstruct in6_addr\t\trtmsg_src;\n\tstruct in6_addr\t\trtmsg_gateway;\n\tu32\t\t\trtmsg_type;\n\tu16\t\t\trtmsg_dst_len;\n\tu16\t\t\trtmsg_src_len;\n\tu32\t\t\trtmsg_metric;\n\tu32\t\t\trtmsg_info;\n\tu32\t\t\trtmsg_flags;\n\ts32\t\t\trtmsg_ifindex;\n};\n\nstatic int routing_ioctl(struct net *net, struct socket *sock,\n\t\t\t unsigned int cmd, void __user *argp)\n{\n\tint ret;\n\tvoid *r = NULL;\n\tstruct in6_rtmsg r6;\n\tstruct rtentry r4;\n\tchar devname[16];\n\tu32 rtdev;\n\tmm_segment_t old_fs = get_fs();\n\n\tif (sock && sock->sk && sock->sk->sk_family == AF_INET6) { /* ipv6 */\n\t\tstruct in6_rtmsg32 __user *ur6 = argp;\n\t\tret = copy_from_user(&r6.rtmsg_dst, &(ur6->rtmsg_dst),\n\t\t\t3 * sizeof(struct in6_addr));\n\t\tret |= get_user(r6.rtmsg_type, &(ur6->rtmsg_type));\n\t\tret |= get_user(r6.rtmsg_dst_len, &(ur6->rtmsg_dst_len));\n\t\tret |= get_user(r6.rtmsg_src_len, &(ur6->rtmsg_src_len));\n\t\tret |= get_user(r6.rtmsg_metric, &(ur6->rtmsg_metric));\n\t\tret |= get_user(r6.rtmsg_info, &(ur6->rtmsg_info));\n\t\tret |= get_user(r6.rtmsg_flags, &(ur6->rtmsg_flags));\n\t\tret |= get_user(r6.rtmsg_ifindex, &(ur6->rtmsg_ifindex));\n\n\t\tr = (void *) &r6;\n\t} else { /* ipv4 */\n\t\tstruct rtentry32 __user *ur4 = argp;\n\t\tret = copy_from_user(&r4.rt_dst, &(ur4->rt_dst),\n\t\t\t\t\t3 * sizeof(struct sockaddr));\n\t\tret |= get_user(r4.rt_flags, &(ur4->rt_flags));\n\t\tret |= get_user(r4.rt_metric, &(ur4->rt_metric));\n\t\tret |= get_user(r4.rt_mtu, &(ur4->rt_mtu));\n\t\tret |= get_user(r4.rt_window, &(ur4->rt_window));\n\t\tret |= get_user(r4.rt_irtt, &(ur4->rt_irtt));\n\t\tret |= get_user(rtdev, &(ur4->rt_dev));\n\t\tif (rtdev) {\n\t\t\tret |= copy_from_user(devname, compat_ptr(rtdev), 15);\n\t\t\tr4.rt_dev = (char __user __force *)devname;\n\t\t\tdevname[15] = 0;\n\t\t} else\n\t\t\tr4.rt_dev = NULL;\n\n\t\tr = (void *) &r4;\n\t}\n\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tset_fs(KERNEL_DS);\n\tret = sock_do_ioctl(net, sock, cmd, (unsigned long) r);\n\tset_fs(old_fs);\n\nout:\n\treturn ret;\n}\n\n/* Since old style bridge ioctl's endup using SIOCDEVPRIVATE\n * for some operations; this forces use of the newer bridge-utils that\n * use compatible ioctls\n */\nstatic int old_bridge_ioctl(compat_ulong_t __user *argp)\n{\n\tcompat_ulong_t tmp;\n\n\tif (get_user(tmp, argp))\n\t\treturn -EFAULT;\n\tif (tmp == BRCTL_GET_VERSION)\n\t\treturn BRCTL_VERSION + 1;\n\treturn -EINVAL;\n}\n\nstatic int compat_sock_ioctl_trans(struct file *file, struct socket *sock,\n\t\t\t unsigned int cmd, unsigned long arg)\n{\n\tvoid __user *argp = compat_ptr(arg);\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\n\tif (cmd >= SIOCDEVPRIVATE && cmd <= (SIOCDEVPRIVATE + 15))\n\t\treturn compat_ifr_data_ioctl(net, cmd, argp);\n\n\tswitch (cmd) {\n\tcase SIOCSIFBR:\n\tcase SIOCGIFBR:\n\t\treturn old_bridge_ioctl(argp);\n\tcase SIOCGIFNAME:\n\t\treturn dev_ifname32(net, argp);\n\tcase SIOCGIFCONF:\n\t\treturn dev_ifconf(net, argp);\n\tcase SIOCETHTOOL:\n\t\treturn ethtool_ioctl(net, argp);\n\tcase SIOCWANDEV:\n\t\treturn compat_siocwandev(net, argp);\n\tcase SIOCGIFMAP:\n\tcase SIOCSIFMAP:\n\t\treturn compat_sioc_ifmap(net, cmd, argp);\n\tcase SIOCBONDENSLAVE:\n\tcase SIOCBONDRELEASE:\n\tcase SIOCBONDSETHWADDR:\n\tcase SIOCBONDCHANGEACTIVE:\n\t\treturn bond_ioctl(net, cmd, argp);\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\t\treturn routing_ioctl(net, sock, cmd, argp);\n\tcase SIOCGSTAMP:\n\t\treturn do_siocgstamp(net, sock, cmd, argp);\n\tcase SIOCGSTAMPNS:\n\t\treturn do_siocgstampns(net, sock, cmd, argp);\n\tcase SIOCBONDSLAVEINFOQUERY:\n\tcase SIOCBONDINFOQUERY:\n\tcase SIOCSHWTSTAMP:\n\tcase SIOCGHWTSTAMP:\n\t\treturn compat_ifr_data_ioctl(net, cmd, argp);\n\n\tcase FIOSETOWN:\n\tcase SIOCSPGRP:\n\tcase FIOGETOWN:\n\tcase SIOCGPGRP:\n\tcase SIOCBRADDBR:\n\tcase SIOCBRDELBR:\n\tcase SIOCGIFVLAN:\n\tcase SIOCSIFVLAN:\n\tcase SIOCADDDLCI:\n\tcase SIOCDELDLCI:\n\tcase SIOCGSKNS:\n\t\treturn sock_ioctl(file, cmd, arg);\n\n\tcase SIOCGIFFLAGS:\n\tcase SIOCSIFFLAGS:\n\tcase SIOCGIFMETRIC:\n\tcase SIOCSIFMETRIC:\n\tcase SIOCGIFMTU:\n\tcase SIOCSIFMTU:\n\tcase SIOCGIFMEM:\n\tcase SIOCSIFMEM:\n\tcase SIOCGIFHWADDR:\n\tcase SIOCSIFHWADDR:\n\tcase SIOCADDMULTI:\n\tcase SIOCDELMULTI:\n\tcase SIOCGIFINDEX:\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCSIFHWBROADCAST:\n\tcase SIOCDIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCSIFPFLAGS:\n\tcase SIOCGIFPFLAGS:\n\tcase SIOCGIFTXQLEN:\n\tcase SIOCSIFTXQLEN:\n\tcase SIOCBRADDIF:\n\tcase SIOCBRDELIF:\n\tcase SIOCSIFNAME:\n\tcase SIOCGMIIPHY:\n\tcase SIOCGMIIREG:\n\tcase SIOCSMIIREG:\n\t\treturn dev_ifsioc(net, sock, cmd, argp);\n\n\tcase SIOCSARP:\n\tcase SIOCGARP:\n\tcase SIOCDARP:\n\tcase SIOCATMARK:\n\t\treturn sock_do_ioctl(net, sock, cmd, arg);\n\t}\n\n\treturn -ENOIOCTLCMD;\n}\n\nstatic long compat_sock_ioctl(struct file *file, unsigned int cmd,\n\t\t\t      unsigned long arg)\n{\n\tstruct socket *sock = file->private_data;\n\tint ret = -ENOIOCTLCMD;\n\tstruct sock *sk;\n\tstruct net *net;\n\n\tsk = sock->sk;\n\tnet = sock_net(sk);\n\n\tif (sock->ops->compat_ioctl)\n\t\tret = sock->ops->compat_ioctl(sock, cmd, arg);\n\n\tif (ret == -ENOIOCTLCMD &&\n\t    (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST))\n\t\tret = compat_wext_handle_ioctl(net, cmd, arg);\n\n\tif (ret == -ENOIOCTLCMD)\n\t\tret = compat_sock_ioctl_trans(file, sock, cmd, arg);\n\n\treturn ret;\n}\n#endif\n\nint kernel_bind(struct socket *sock, struct sockaddr *addr, int addrlen)\n{\n\treturn sock->ops->bind(sock, addr, addrlen);\n}\nEXPORT_SYMBOL(kernel_bind);\n\nint kernel_listen(struct socket *sock, int backlog)\n{\n\treturn sock->ops->listen(sock, backlog);\n}\nEXPORT_SYMBOL(kernel_listen);\n\nint kernel_accept(struct socket *sock, struct socket **newsock, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\n\terr = sock_create_lite(sk->sk_family, sk->sk_type, sk->sk_protocol,\n\t\t\t       newsock);\n\tif (err < 0)\n\t\tgoto done;\n\n\terr = sock->ops->accept(sock, *newsock, flags, true);\n\tif (err < 0) {\n\t\tsock_release(*newsock);\n\t\t*newsock = NULL;\n\t\tgoto done;\n\t}\n\n\t(*newsock)->ops = sock->ops;\n\t__module_get((*newsock)->ops->owner);\n\ndone:\n\treturn err;\n}\nEXPORT_SYMBOL(kernel_accept);\n\nint kernel_connect(struct socket *sock, struct sockaddr *addr, int addrlen,\n\t\t   int flags)\n{\n\treturn sock->ops->connect(sock, addr, addrlen, flags);\n}\nEXPORT_SYMBOL(kernel_connect);\n\nint kernel_getsockname(struct socket *sock, struct sockaddr *addr,\n\t\t\t int *addrlen)\n{\n\treturn sock->ops->getname(sock, addr, addrlen, 0);\n}\nEXPORT_SYMBOL(kernel_getsockname);\n\nint kernel_getpeername(struct socket *sock, struct sockaddr *addr,\n\t\t\t int *addrlen)\n{\n\treturn sock->ops->getname(sock, addr, addrlen, 1);\n}\nEXPORT_SYMBOL(kernel_getpeername);\n\nint kernel_getsockopt(struct socket *sock, int level, int optname,\n\t\t\tchar *optval, int *optlen)\n{\n\tmm_segment_t oldfs = get_fs();\n\tchar __user *uoptval;\n\tint __user *uoptlen;\n\tint err;\n\n\tuoptval = (char __user __force *) optval;\n\tuoptlen = (int __user __force *) optlen;\n\n\tset_fs(KERNEL_DS);\n\tif (level == SOL_SOCKET)\n\t\terr = sock_getsockopt(sock, level, optname, uoptval, uoptlen);\n\telse\n\t\terr = sock->ops->getsockopt(sock, level, optname, uoptval,\n\t\t\t\t\t    uoptlen);\n\tset_fs(oldfs);\n\treturn err;\n}\nEXPORT_SYMBOL(kernel_getsockopt);\n\nint kernel_setsockopt(struct socket *sock, int level, int optname,\n\t\t\tchar *optval, unsigned int optlen)\n{\n\tmm_segment_t oldfs = get_fs();\n\tchar __user *uoptval;\n\tint err;\n\n\tuoptval = (char __user __force *) optval;\n\n\tset_fs(KERNEL_DS);\n\tif (level == SOL_SOCKET)\n\t\terr = sock_setsockopt(sock, level, optname, uoptval, optlen);\n\telse\n\t\terr = sock->ops->setsockopt(sock, level, optname, uoptval,\n\t\t\t\t\t    optlen);\n\tset_fs(oldfs);\n\treturn err;\n}\nEXPORT_SYMBOL(kernel_setsockopt);\n\nint kernel_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t    size_t size, int flags)\n{\n\tif (sock->ops->sendpage)\n\t\treturn sock->ops->sendpage(sock, page, offset, size, flags);\n\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}\nEXPORT_SYMBOL(kernel_sendpage);\n\nint kernel_sock_ioctl(struct socket *sock, int cmd, unsigned long arg)\n{\n\tmm_segment_t oldfs = get_fs();\n\tint err;\n\n\tset_fs(KERNEL_DS);\n\terr = sock->ops->ioctl(sock, cmd, arg);\n\tset_fs(oldfs);\n\n\treturn err;\n}\nEXPORT_SYMBOL(kernel_sock_ioctl);\n\nint kernel_sock_shutdown(struct socket *sock, enum sock_shutdown_cmd how)\n{\n\treturn sock->ops->shutdown(sock, how);\n}\nEXPORT_SYMBOL(kernel_sock_shutdown);\n"], "fixing_code": ["#ifndef _LINUX_ERRQUEUE_H\n#define _LINUX_ERRQUEUE_H 1\n\n\n#include <net/ip.h>\n#if IS_ENABLED(CONFIG_IPV6)\n#include <linux/ipv6.h>\n#endif\n#include <uapi/linux/errqueue.h>\n\n#define SKB_EXT_ERR(skb) ((struct sock_exterr_skb *) ((skb)->cb))\n\nstruct sock_exterr_skb {\n\tunion {\n\t\tstruct inet_skb_parm\th4;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tstruct inet6_skb_parm\th6;\n#endif\n\t} header;\n\tstruct sock_extended_err\tee;\n\tu16\t\t\t\taddr_offset;\n\t__be16\t\t\t\tport;\n\tu8\t\t\t\topt_stats:1,\n\t\t\t\t\tunused:7;\n};\n\n#endif\n", "/*\n *\tRoutines having to do with the 'struct sk_buff' memory handlers.\n *\n *\tAuthors:\tAlan Cox <alan@lxorguk.ukuu.org.uk>\n *\t\t\tFlorian La Roche <rzsfl@rz.uni-sb.de>\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tFixed the worst of the load\n *\t\t\t\t\tbalancer bugs.\n *\t\tDave Platt\t:\tInterrupt stacking fix.\n *\tRichard Kooijman\t:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tChanged buffer format.\n *\t\tAlan Cox\t:\tdestructor hook for AF_UNIX etc.\n *\t\tLinus Torvalds\t:\tBetter skb_clone.\n *\t\tAlan Cox\t:\tAdded skb_copy.\n *\t\tAlan Cox\t:\tAdded all the changed routines Linus\n *\t\t\t\t\tonly put in the headers\n *\t\tRay VanTassle\t:\tFixed --skb->lock in free\n *\t\tAlan Cox\t:\tskb_copy copy arp field\n *\t\tAndi Kleen\t:\tslabified it.\n *\t\tRobert Olsson\t:\tRemoved skb_head_pool\n *\n *\tNOTE:\n *\t\tThe __skb_ routines should be called with interrupts\n *\tdisabled, or you better be *real* sure that the operation is atomic\n *\twith respect to whatever list is being frobbed (e.g. via lock_sock()\n *\tor via disabling bottom half handlers, etc).\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n/*\n *\tThe functions in this file will not compile correctly with gcc 2.4.x\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/kmemcheck.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/slab.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/sctp.h>\n#include <linux/netdevice.h>\n#ifdef CONFIG_NET_CLS_ACT\n#include <net/pkt_sched.h>\n#endif\n#include <linux/string.h>\n#include <linux/skbuff.h>\n#include <linux/splice.h>\n#include <linux/cache.h>\n#include <linux/rtnetlink.h>\n#include <linux/init.h>\n#include <linux/scatterlist.h>\n#include <linux/errqueue.h>\n#include <linux/prefetch.h>\n#include <linux/if_vlan.h>\n\n#include <net/protocol.h>\n#include <net/dst.h>\n#include <net/sock.h>\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n\n#include <linux/uaccess.h>\n#include <trace/events/skb.h>\n#include <linux/highmem.h>\n#include <linux/capability.h>\n#include <linux/user_namespace.h>\n\nstruct kmem_cache *skbuff_head_cache __read_mostly;\nstatic struct kmem_cache *skbuff_fclone_cache __read_mostly;\nint sysctl_max_skb_frags __read_mostly = MAX_SKB_FRAGS;\nEXPORT_SYMBOL(sysctl_max_skb_frags);\n\n/**\n *\tskb_panic - private function for out-of-line support\n *\t@skb:\tbuffer\n *\t@sz:\tsize\n *\t@addr:\taddress\n *\t@msg:\tskb_over_panic or skb_under_panic\n *\n *\tOut-of-line support for skb_put() and skb_push().\n *\tCalled via the wrapper skb_over_panic() or skb_under_panic().\n *\tKeep out of line to prevent kernel bloat.\n *\t__builtin_return_address is not used because it is not always reliable.\n */\nstatic void skb_panic(struct sk_buff *skb, unsigned int sz, void *addr,\n\t\t      const char msg[])\n{\n\tpr_emerg(\"%s: text:%p len:%d put:%d head:%p data:%p tail:%#lx end:%#lx dev:%s\\n\",\n\t\t msg, addr, skb->len, sz, skb->head, skb->data,\n\t\t (unsigned long)skb->tail, (unsigned long)skb->end,\n\t\t skb->dev ? skb->dev->name : \"<NULL>\");\n\tBUG();\n}\n\nstatic void skb_over_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\nstatic void skb_under_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\n/*\n * kmalloc_reserve is a wrapper around kmalloc_node_track_caller that tells\n * the caller if emergency pfmemalloc reserves are being used. If it is and\n * the socket is later found to be SOCK_MEMALLOC then PFMEMALLOC reserves\n * may be used. Otherwise, the packet data may be discarded until enough\n * memory is free\n */\n#define kmalloc_reserve(size, gfp, node, pfmemalloc) \\\n\t __kmalloc_reserve(size, gfp, node, _RET_IP_, pfmemalloc)\n\nstatic void *__kmalloc_reserve(size_t size, gfp_t flags, int node,\n\t\t\t       unsigned long ip, bool *pfmemalloc)\n{\n\tvoid *obj;\n\tbool ret_pfmemalloc = false;\n\n\t/*\n\t * Try a regular allocation, when that fails and we're not entitled\n\t * to the reserves, fail.\n\t */\n\tobj = kmalloc_node_track_caller(size,\n\t\t\t\t\tflags | __GFP_NOMEMALLOC | __GFP_NOWARN,\n\t\t\t\t\tnode);\n\tif (obj || !(gfp_pfmemalloc_allowed(flags)))\n\t\tgoto out;\n\n\t/* Try again but now we are using pfmemalloc reserves */\n\tret_pfmemalloc = true;\n\tobj = kmalloc_node_track_caller(size, flags, node);\n\nout:\n\tif (pfmemalloc)\n\t\t*pfmemalloc = ret_pfmemalloc;\n\n\treturn obj;\n}\n\n/* \tAllocate a new skbuff. We do this ourselves so we can fill in a few\n *\t'private' fields and also do memory statistics to find all the\n *\t[BEEP] leaks.\n *\n */\n\nstruct sk_buff *__alloc_skb_head(gfp_t gfp_mask, int node)\n{\n\tstruct sk_buff *skb;\n\n\t/* Get the HEAD */\n\tskb = kmem_cache_alloc_node(skbuff_head_cache,\n\t\t\t\t    gfp_mask & ~__GFP_DMA, node);\n\tif (!skb)\n\t\tgoto out;\n\n\t/*\n\t * Only clear those fields we need to clear, not those that we will\n\t * actually initialise below. Hence, don't put any more fields after\n\t * the tail pointer in struct sk_buff!\n\t */\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->head = NULL;\n\tskb->truesize = sizeof(struct sk_buff);\n\tatomic_set(&skb->users, 1);\n\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\nout:\n\treturn skb;\n}\n\n/**\n *\t__alloc_skb\t-\tallocate a network buffer\n *\t@size: size to allocate\n *\t@gfp_mask: allocation mask\n *\t@flags: If SKB_ALLOC_FCLONE is set, allocate from fclone cache\n *\t\tinstead of head cache and allocate a cloned (child) skb.\n *\t\tIf SKB_ALLOC_RX is set, __GFP_MEMALLOC will be used for\n *\t\tallocations in case the data is required for writeback\n *\t@node: numa node to allocate memory on\n *\n *\tAllocate a new &sk_buff. The returned buffer has no headroom and a\n *\ttail room of at least size bytes. The object has a reference count\n *\tof one. The return is the buffer. On a failure the return is %NULL.\n *\n *\tBuffers may only be allocated from interrupts using a @gfp_mask of\n *\t%GFP_ATOMIC.\n */\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,\n\t\t\t    int flags, int node)\n{\n\tstruct kmem_cache *cache;\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tu8 *data;\n\tbool pfmemalloc;\n\n\tcache = (flags & SKB_ALLOC_FCLONE)\n\t\t? skbuff_fclone_cache : skbuff_head_cache;\n\n\tif (sk_memalloc_socks() && (flags & SKB_ALLOC_RX))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t/* Get the HEAD */\n\tskb = kmem_cache_alloc_node(cache, gfp_mask & ~__GFP_DMA, node);\n\tif (!skb)\n\t\tgoto out;\n\tprefetchw(skb);\n\n\t/* We do our best to align skb_shared_info on a separate cache\n\t * line. It usually works because kmalloc(X > SMP_CACHE_BYTES) gives\n\t * aligned memory blocks, unless SLUB/SLAB debug is enabled.\n\t * Both skb->head and skb_shared_info are cache line aligned.\n\t */\n\tsize = SKB_DATA_ALIGN(size);\n\tsize += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tdata = kmalloc_reserve(size, gfp_mask, node, &pfmemalloc);\n\tif (!data)\n\t\tgoto nodata;\n\t/* kmalloc(size) might give us more room than requested.\n\t * Put skb_shared_info exactly at the end of allocated zone,\n\t * to allow max possible filling before reallocation.\n\t */\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\tprefetchw(data + size);\n\n\t/*\n\t * Only clear those fields we need to clear, not those that we will\n\t * actually initialise below. Hence, don't put any more fields after\n\t * the tail pointer in struct sk_buff!\n\t */\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t/* Account for allocated memory : skb + skb->head */\n\tskb->truesize = SKB_TRUESIZE(size);\n\tskb->pfmemalloc = pfmemalloc;\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\tif (flags & SKB_ALLOC_FCLONE) {\n\t\tstruct sk_buff_fclones *fclones;\n\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\t\tkmemcheck_annotate_bitfield(&fclones->skb2, flags1);\n\t\tskb->fclone = SKB_FCLONE_ORIG;\n\t\tatomic_set(&fclones->fclone_ref, 1);\n\n\t\tfclones->skb2.fclone = SKB_FCLONE_CLONE;\n\t}\nout:\n\treturn skb;\nnodata:\n\tkmem_cache_free(cache, skb);\n\tskb = NULL;\n\tgoto out;\n}\nEXPORT_SYMBOL(__alloc_skb);\n\n/**\n * __build_skb - build a network buffer\n * @data: data buffer provided by caller\n * @frag_size: size of data, or 0 if head was kmalloced\n *\n * Allocate a new &sk_buff. Caller provides space holding head and\n * skb_shared_info. @data must have been allocated by kmalloc() only if\n * @frag_size is 0, otherwise data should come from the page allocator\n *  or vmalloc()\n * The return is the new skb buffer.\n * On a failure the return is %NULL, and @data is not freed.\n * Notes :\n *  Before IO, driver allocates only data buffer where NIC put incoming frame\n *  Driver should add room at head (NET_SKB_PAD) and\n *  MUST add room at tail (SKB_DATA_ALIGN(skb_shared_info))\n *  After IO, driver calls build_skb(), to allocate sk_buff and populate it\n *  before giving packet to stack.\n *  RX rings only contains data buffers, not full skbs.\n */\nstruct sk_buff *__build_skb(void *data, unsigned int frag_size)\n{\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tunsigned int size = frag_size ? : ksize(data);\n\n\tskb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\tsize -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->truesize = SKB_TRUESIZE(size);\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\treturn skb;\n}\n\n/* build_skb() is wrapper over __build_skb(), that specifically\n * takes care of skb->head and skb->pfmemalloc\n * This means that if @frag_size is not zero, then @data must be backed\n * by a page fragment, not kmalloc() or vmalloc()\n */\nstruct sk_buff *build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb = __build_skb(data, frag_size);\n\n\tif (skb && frag_size) {\n\t\tskb->head_frag = 1;\n\t\tif (page_is_pfmemalloc(virt_to_head_page(data)))\n\t\t\tskb->pfmemalloc = 1;\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(build_skb);\n\n#define NAPI_SKB_CACHE_SIZE\t64\n\nstruct napi_alloc_cache {\n\tstruct page_frag_cache page;\n\tunsigned int skb_count;\n\tvoid *skb_cache[NAPI_SKB_CACHE_SIZE];\n};\n\nstatic DEFINE_PER_CPU(struct page_frag_cache, netdev_alloc_cache);\nstatic DEFINE_PER_CPU(struct napi_alloc_cache, napi_alloc_cache);\n\nstatic void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)\n{\n\tstruct page_frag_cache *nc;\n\tunsigned long flags;\n\tvoid *data;\n\n\tlocal_irq_save(flags);\n\tnc = this_cpu_ptr(&netdev_alloc_cache);\n\tdata = page_frag_alloc(nc, fragsz, gfp_mask);\n\tlocal_irq_restore(flags);\n\treturn data;\n}\n\n/**\n * netdev_alloc_frag - allocate a page fragment\n * @fragsz: fragment size\n *\n * Allocates a frag from a page for receive buffer.\n * Uses GFP_ATOMIC allocations.\n */\nvoid *netdev_alloc_frag(unsigned int fragsz)\n{\n\treturn __netdev_alloc_frag(fragsz, GFP_ATOMIC | __GFP_COLD);\n}\nEXPORT_SYMBOL(netdev_alloc_frag);\n\nstatic void *__napi_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\treturn page_frag_alloc(&nc->page, fragsz, gfp_mask);\n}\n\nvoid *napi_alloc_frag(unsigned int fragsz)\n{\n\treturn __napi_alloc_frag(fragsz, GFP_ATOMIC | __GFP_COLD);\n}\nEXPORT_SYMBOL(napi_alloc_frag);\n\n/**\n *\t__netdev_alloc_skb - allocate an skbuff for rx on a specific device\n *\t@dev: network device to receive on\n *\t@len: length to allocate\n *\t@gfp_mask: get_free_pages mask, passed to alloc_skb\n *\n *\tAllocate a new &sk_buff and assign it a usage count of one. The\n *\tbuffer has NET_SKB_PAD headroom built in. Users should allocate\n *\tthe headroom they think they need without accounting for the\n *\tbuilt in space. The built in space is used for optimisations.\n *\n *\t%NULL is returned if there is no free memory.\n */\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,\n\t\t\t\t   gfp_t gfp_mask)\n{\n\tstruct page_frag_cache *nc;\n\tunsigned long flags;\n\tstruct sk_buff *skb;\n\tbool pfmemalloc;\n\tvoid *data;\n\n\tlen += NET_SKB_PAD;\n\n\tif ((len > SKB_WITH_OVERHEAD(PAGE_SIZE)) ||\n\t    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA))) {\n\t\tskb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);\n\t\tif (!skb)\n\t\t\tgoto skb_fail;\n\t\tgoto skb_success;\n\t}\n\n\tlen += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tlen = SKB_DATA_ALIGN(len);\n\n\tif (sk_memalloc_socks())\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tlocal_irq_save(flags);\n\n\tnc = this_cpu_ptr(&netdev_alloc_cache);\n\tdata = page_frag_alloc(nc, len, gfp_mask);\n\tpfmemalloc = nc->pfmemalloc;\n\n\tlocal_irq_restore(flags);\n\n\tif (unlikely(!data))\n\t\treturn NULL;\n\n\tskb = __build_skb(data, len);\n\tif (unlikely(!skb)) {\n\t\tskb_free_frag(data);\n\t\treturn NULL;\n\t}\n\n\t/* use OR instead of assignment to avoid clearing of bits in mask */\n\tif (pfmemalloc)\n\t\tskb->pfmemalloc = 1;\n\tskb->head_frag = 1;\n\nskb_success:\n\tskb_reserve(skb, NET_SKB_PAD);\n\tskb->dev = dev;\n\nskb_fail:\n\treturn skb;\n}\nEXPORT_SYMBOL(__netdev_alloc_skb);\n\n/**\n *\t__napi_alloc_skb - allocate skbuff for rx in a specific NAPI instance\n *\t@napi: napi instance this buffer was allocated for\n *\t@len: length to allocate\n *\t@gfp_mask: get_free_pages mask, passed to alloc_skb and alloc_pages\n *\n *\tAllocate a new sk_buff for use in NAPI receive.  This buffer will\n *\tattempt to allocate the head from a special reserved region used\n *\tonly for NAPI Rx allocation.  By doing this we can save several\n *\tCPU cycles by avoiding having to disable and re-enable IRQs.\n *\n *\t%NULL is returned if there is no free memory.\n */\nstruct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,\n\t\t\t\t gfp_t gfp_mask)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\tstruct sk_buff *skb;\n\tvoid *data;\n\n\tlen += NET_SKB_PAD + NET_IP_ALIGN;\n\n\tif ((len > SKB_WITH_OVERHEAD(PAGE_SIZE)) ||\n\t    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA))) {\n\t\tskb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);\n\t\tif (!skb)\n\t\t\tgoto skb_fail;\n\t\tgoto skb_success;\n\t}\n\n\tlen += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tlen = SKB_DATA_ALIGN(len);\n\n\tif (sk_memalloc_socks())\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tdata = page_frag_alloc(&nc->page, len, gfp_mask);\n\tif (unlikely(!data))\n\t\treturn NULL;\n\n\tskb = __build_skb(data, len);\n\tif (unlikely(!skb)) {\n\t\tskb_free_frag(data);\n\t\treturn NULL;\n\t}\n\n\t/* use OR instead of assignment to avoid clearing of bits in mask */\n\tif (nc->page.pfmemalloc)\n\t\tskb->pfmemalloc = 1;\n\tskb->head_frag = 1;\n\nskb_success:\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\n\tskb->dev = napi->dev;\n\nskb_fail:\n\treturn skb;\n}\nEXPORT_SYMBOL(__napi_alloc_skb);\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize)\n{\n\tskb_fill_page_desc(skb, i, page, off, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_add_rx_frag);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\tskb_frag_size_add(frag, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_coalesce_rx_frag);\n\nstatic void skb_drop_list(struct sk_buff **listp)\n{\n\tkfree_skb_list(*listp);\n\t*listp = NULL;\n}\n\nstatic inline void skb_drop_fraglist(struct sk_buff *skb)\n{\n\tskb_drop_list(&skb_shinfo(skb)->frag_list);\n}\n\nstatic void skb_clone_fraglist(struct sk_buff *skb)\n{\n\tstruct sk_buff *list;\n\n\tskb_walk_frags(skb, list)\n\t\tskb_get(list);\n}\n\nstatic void skb_free_head(struct sk_buff *skb)\n{\n\tunsigned char *head = skb->head;\n\n\tif (skb->head_frag)\n\t\tskb_free_frag(head);\n\telse\n\t\tkfree(head);\n}\n\nstatic void skb_release_data(struct sk_buff *skb)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint i;\n\n\tif (skb->cloned &&\n\t    atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,\n\t\t\t      &shinfo->dataref))\n\t\treturn;\n\n\tfor (i = 0; i < shinfo->nr_frags; i++)\n\t\t__skb_frag_unref(&shinfo->frags[i]);\n\n\t/*\n\t * If skb buf is from userspace, we need to notify the caller\n\t * the lower device DMA has done;\n\t */\n\tif (shinfo->tx_flags & SKBTX_DEV_ZEROCOPY) {\n\t\tstruct ubuf_info *uarg;\n\n\t\tuarg = shinfo->destructor_arg;\n\t\tif (uarg->callback)\n\t\t\tuarg->callback(uarg, true);\n\t}\n\n\tif (shinfo->frag_list)\n\t\tkfree_skb_list(shinfo->frag_list);\n\n\tskb_free_head(skb);\n}\n\n/*\n *\tFree an skbuff by memory without cleaning the state.\n */\nstatic void kfree_skbmem(struct sk_buff *skb)\n{\n\tstruct sk_buff_fclones *fclones;\n\n\tswitch (skb->fclone) {\n\tcase SKB_FCLONE_UNAVAILABLE:\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t\treturn;\n\n\tcase SKB_FCLONE_ORIG:\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\t\t/* We usually free the clone (TX completion) before original skb\n\t\t * This test would have no chance to be true for the clone,\n\t\t * while here, branch prediction will be good.\n\t\t */\n\t\tif (atomic_read(&fclones->fclone_ref) == 1)\n\t\t\tgoto fastpath;\n\t\tbreak;\n\n\tdefault: /* SKB_FCLONE_CLONE */\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb2);\n\t\tbreak;\n\t}\n\tif (!atomic_dec_and_test(&fclones->fclone_ref))\n\t\treturn;\nfastpath:\n\tkmem_cache_free(skbuff_fclone_cache, fclones);\n}\n\nstatic void skb_release_head_state(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n#ifdef CONFIG_XFRM\n\tsecpath_put(skb->sp);\n#endif\n\tif (skb->destructor) {\n\t\tWARN_ON(in_irq());\n\t\tskb->destructor(skb);\n\t}\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tnf_conntrack_put(skb_nfct(skb));\n#endif\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\tnf_bridge_put(skb->nf_bridge);\n#endif\n}\n\n/* Free everything but the sk_buff shell. */\nstatic void skb_release_all(struct sk_buff *skb)\n{\n\tskb_release_head_state(skb);\n\tif (likely(skb->head))\n\t\tskb_release_data(skb);\n}\n\n/**\n *\t__kfree_skb - private function\n *\t@skb: buffer\n *\n *\tFree an sk_buff. Release anything attached to the buffer.\n *\tClean the state. This is an internal helper function. Users should\n *\talways call kfree_skb\n */\n\nvoid __kfree_skb(struct sk_buff *skb)\n{\n\tskb_release_all(skb);\n\tkfree_skbmem(skb);\n}\nEXPORT_SYMBOL(__kfree_skb);\n\n/**\n *\tkfree_skb - free an sk_buff\n *\t@skb: buffer to free\n *\n *\tDrop a reference to the buffer and free it if the usage count has\n *\thit zero.\n */\nvoid kfree_skb(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\ttrace_kfree_skb(skb, __builtin_return_address(0));\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(kfree_skb);\n\nvoid kfree_skb_list(struct sk_buff *segs)\n{\n\twhile (segs) {\n\t\tstruct sk_buff *next = segs->next;\n\n\t\tkfree_skb(segs);\n\t\tsegs = next;\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_list);\n\n/**\n *\tskb_tx_error - report an sk_buff xmit error\n *\t@skb: buffer that triggered an error\n *\n *\tReport xmit error if a device callback is tracking this skb.\n *\tskb must be freed afterwards.\n */\nvoid skb_tx_error(struct sk_buff *skb)\n{\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_DEV_ZEROCOPY) {\n\t\tstruct ubuf_info *uarg;\n\n\t\tuarg = skb_shinfo(skb)->destructor_arg;\n\t\tif (uarg->callback)\n\t\t\tuarg->callback(uarg, false);\n\t\tskb_shinfo(skb)->tx_flags &= ~SKBTX_DEV_ZEROCOPY;\n\t}\n}\nEXPORT_SYMBOL(skb_tx_error);\n\n/**\n *\tconsume_skb - free an skbuff\n *\t@skb: buffer to free\n *\n *\tDrop a ref to the buffer and free it if the usage count has hit zero\n *\tFunctions identically to kfree_skb, but kfree_skb assumes that the frame\n *\tis being dropped after a failure and notes that\n */\nvoid consume_skb(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\ttrace_consume_skb(skb);\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(consume_skb);\n\nvoid __kfree_skb_flush(void)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\t/* flush skb_cache if containing objects */\n\tif (nc->skb_count) {\n\t\tkmem_cache_free_bulk(skbuff_head_cache, nc->skb_count,\n\t\t\t\t     nc->skb_cache);\n\t\tnc->skb_count = 0;\n\t}\n}\n\nstatic inline void _kfree_skb_defer(struct sk_buff *skb)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\t/* drop skb->head and call any destructors for packet */\n\tskb_release_all(skb);\n\n\t/* record skb to CPU local list */\n\tnc->skb_cache[nc->skb_count++] = skb;\n\n#ifdef CONFIG_SLUB\n\t/* SLUB writes into objects when freeing */\n\tprefetchw(skb);\n#endif\n\n\t/* flush skb_cache if it is filled */\n\tif (unlikely(nc->skb_count == NAPI_SKB_CACHE_SIZE)) {\n\t\tkmem_cache_free_bulk(skbuff_head_cache, NAPI_SKB_CACHE_SIZE,\n\t\t\t\t     nc->skb_cache);\n\t\tnc->skb_count = 0;\n\t}\n}\nvoid __kfree_skb_defer(struct sk_buff *skb)\n{\n\t_kfree_skb_defer(skb);\n}\n\nvoid napi_consume_skb(struct sk_buff *skb, int budget)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\n\t/* Zero budget indicate non-NAPI context called us, like netpoll */\n\tif (unlikely(!budget)) {\n\t\tdev_consume_skb_any(skb);\n\t\treturn;\n\t}\n\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\t/* if reaching here SKB is ready to free */\n\ttrace_consume_skb(skb);\n\n\t/* if SKB is a clone, don't handle this case */\n\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\n\t_kfree_skb_defer(skb);\n}\nEXPORT_SYMBOL(napi_consume_skb);\n\n/* Make sure a field is enclosed inside headers_start/headers_end section */\n#define CHECK_SKB_FIELD(field) \\\n\tBUILD_BUG_ON(offsetof(struct sk_buff, field) <\t\t\\\n\t\t     offsetof(struct sk_buff, headers_start));\t\\\n\tBUILD_BUG_ON(offsetof(struct sk_buff, field) >\t\t\\\n\t\t     offsetof(struct sk_buff, headers_end));\t\\\n\nstatic void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\tnew->tstamp\t\t= old->tstamp;\n\t/* We do not copy old->sk */\n\tnew->dev\t\t= old->dev;\n\tmemcpy(new->cb, old->cb, sizeof(old->cb));\n\tskb_dst_copy(new, old);\n#ifdef CONFIG_XFRM\n\tnew->sp\t\t\t= secpath_get(old->sp);\n#endif\n\t__nf_copy(new, old, false);\n\n\t/* Note : this field could be in headers_start/headers_end section\n\t * It is not yet because we do not want to have a 16 bit hole\n\t */\n\tnew->queue_mapping = old->queue_mapping;\n\n\tmemcpy(&new->headers_start, &old->headers_start,\n\t       offsetof(struct sk_buff, headers_end) -\n\t       offsetof(struct sk_buff, headers_start));\n\tCHECK_SKB_FIELD(protocol);\n\tCHECK_SKB_FIELD(csum);\n\tCHECK_SKB_FIELD(hash);\n\tCHECK_SKB_FIELD(priority);\n\tCHECK_SKB_FIELD(skb_iif);\n\tCHECK_SKB_FIELD(vlan_proto);\n\tCHECK_SKB_FIELD(vlan_tci);\n\tCHECK_SKB_FIELD(transport_header);\n\tCHECK_SKB_FIELD(network_header);\n\tCHECK_SKB_FIELD(mac_header);\n\tCHECK_SKB_FIELD(inner_protocol);\n\tCHECK_SKB_FIELD(inner_transport_header);\n\tCHECK_SKB_FIELD(inner_network_header);\n\tCHECK_SKB_FIELD(inner_mac_header);\n\tCHECK_SKB_FIELD(mark);\n#ifdef CONFIG_NETWORK_SECMARK\n\tCHECK_SKB_FIELD(secmark);\n#endif\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tCHECK_SKB_FIELD(napi_id);\n#endif\n#ifdef CONFIG_XPS\n\tCHECK_SKB_FIELD(sender_cpu);\n#endif\n#ifdef CONFIG_NET_SCHED\n\tCHECK_SKB_FIELD(tc_index);\n#endif\n\n}\n\n/*\n * You should not add any new code to this function.  Add it to\n * __copy_skb_header above instead.\n */\nstatic struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)\n{\n#define C(x) n->x = skb->x\n\n\tn->next = n->prev = NULL;\n\tn->sk = NULL;\n\t__copy_skb_header(n, skb);\n\n\tC(len);\n\tC(data_len);\n\tC(mac_len);\n\tn->hdr_len = skb->nohdr ? skb_headroom(skb) : skb->hdr_len;\n\tn->cloned = 1;\n\tn->nohdr = 0;\n\tn->destructor = NULL;\n\tC(tail);\n\tC(end);\n\tC(head);\n\tC(head_frag);\n\tC(data);\n\tC(truesize);\n\tatomic_set(&n->users, 1);\n\n\tatomic_inc(&(skb_shinfo(skb)->dataref));\n\tskb->cloned = 1;\n\n\treturn n;\n#undef C\n}\n\n/**\n *\tskb_morph\t-\tmorph one skb into another\n *\t@dst: the skb to receive the contents\n *\t@src: the skb to supply the contents\n *\n *\tThis is identical to skb_clone except that the target skb is\n *\tsupplied by the user.\n *\n *\tThe target skb is returned upon exit.\n */\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src)\n{\n\tskb_release_all(dst);\n\treturn __skb_clone(dst, src);\n}\nEXPORT_SYMBOL_GPL(skb_morph);\n\n/**\n *\tskb_copy_ubufs\t-\tcopy userspace skb frags buffers to kernel\n *\t@skb: the skb to modify\n *\t@gfp_mask: allocation priority\n *\n *\tThis must be called on SKBTX_DEV_ZEROCOPY skb.\n *\tIt will copy all frags into kernel and drop the reference\n *\tto userspace pages.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n *\n *\tReturns 0 on success or a negative error code on failure\n *\tto allocate kernel memory to copy to.\n */\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint i;\n\tint num_frags = skb_shinfo(skb)->nr_frags;\n\tstruct page *page, *head = NULL;\n\tstruct ubuf_info *uarg = skb_shinfo(skb)->destructor_arg;\n\n\tfor (i = 0; i < num_frags; i++) {\n\t\tu8 *vaddr;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tpage = alloc_page(gfp_mask);\n\t\tif (!page) {\n\t\t\twhile (head) {\n\t\t\t\tstruct page *next = (struct page *)page_private(head);\n\t\t\t\tput_page(head);\n\t\t\t\thead = next;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tvaddr = kmap_atomic(skb_frag_page(f));\n\t\tmemcpy(page_address(page),\n\t\t       vaddr + f->page_offset, skb_frag_size(f));\n\t\tkunmap_atomic(vaddr);\n\t\tset_page_private(page, (unsigned long)head);\n\t\thead = page;\n\t}\n\n\t/* skb frags release userspace buffers */\n\tfor (i = 0; i < num_frags; i++)\n\t\tskb_frag_unref(skb, i);\n\n\tuarg->callback(uarg, false);\n\n\t/* skb frags point to kernel buffers */\n\tfor (i = num_frags - 1; i >= 0; i--) {\n\t\t__skb_fill_page_desc(skb, i, head, 0,\n\t\t\t\t     skb_shinfo(skb)->frags[i].size);\n\t\thead = (struct page *)page_private(head);\n\t}\n\n\tskb_shinfo(skb)->tx_flags &= ~SKBTX_DEV_ZEROCOPY;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_copy_ubufs);\n\n/**\n *\tskb_clone\t-\tduplicate an sk_buff\n *\t@skb: buffer to clone\n *\t@gfp_mask: allocation priority\n *\n *\tDuplicate an &sk_buff. The new one is not owned by a socket. Both\n *\tcopies share the same packet data but not structure. The new\n *\tbuffer has a reference count of 1. If the allocation fails the\n *\tfunction returns %NULL otherwise the new buffer is returned.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n */\n\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tstruct sk_buff_fclones *fclones = container_of(skb,\n\t\t\t\t\t\t       struct sk_buff_fclones,\n\t\t\t\t\t\t       skb1);\n\tstruct sk_buff *n;\n\n\tif (skb_orphan_frags(skb, gfp_mask))\n\t\treturn NULL;\n\n\tif (skb->fclone == SKB_FCLONE_ORIG &&\n\t    atomic_read(&fclones->fclone_ref) == 1) {\n\t\tn = &fclones->skb2;\n\t\tatomic_set(&fclones->fclone_ref, 2);\n\t} else {\n\t\tif (skb_pfmemalloc(skb))\n\t\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t\tn = kmem_cache_alloc(skbuff_head_cache, gfp_mask);\n\t\tif (!n)\n\t\t\treturn NULL;\n\n\t\tkmemcheck_annotate_bitfield(n, flags1);\n\t\tn->fclone = SKB_FCLONE_UNAVAILABLE;\n\t}\n\n\treturn __skb_clone(n, skb);\n}\nEXPORT_SYMBOL(skb_clone);\n\nstatic void skb_headers_offset_update(struct sk_buff *skb, int off)\n{\n\t/* Only adjust this if it actually is csum_start rather than csum */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tskb->csum_start += off;\n\t/* {transport,network,mac}_header and tail are relative to skb->head */\n\tskb->transport_header += off;\n\tskb->network_header   += off;\n\tif (skb_mac_header_was_set(skb))\n\t\tskb->mac_header += off;\n\tskb->inner_transport_header += off;\n\tskb->inner_network_header += off;\n\tskb->inner_mac_header += off;\n}\n\nstatic void copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\t__copy_skb_header(new, old);\n\n\tskb_shinfo(new)->gso_size = skb_shinfo(old)->gso_size;\n\tskb_shinfo(new)->gso_segs = skb_shinfo(old)->gso_segs;\n\tskb_shinfo(new)->gso_type = skb_shinfo(old)->gso_type;\n}\n\nstatic inline int skb_alloc_rx_flag(const struct sk_buff *skb)\n{\n\tif (skb_pfmemalloc(skb))\n\t\treturn SKB_ALLOC_RX;\n\treturn 0;\n}\n\n/**\n *\tskb_copy\t-\tcreate private copy of an sk_buff\n *\t@skb: buffer to copy\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data. This is used when the\n *\tcaller wishes to modify the data and needs a private copy of the\n *\tdata to alter. Returns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tAs by-product this function converts non-linear &sk_buff to linear\n *\tone, so that &sk_buff becomes completely private and caller is allowed\n *\tto modify all the data of returned buffer. This means that this\n *\tfunction is not recommended for use in circumstances when only\n *\theader is going to be modified. Use pskb_copy() instead.\n */\n\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint headerlen = skb_headroom(skb);\n\tunsigned int size = skb_end_offset(skb) + skb->data_len;\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\n\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\n\n\tif (!n)\n\t\treturn NULL;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headerlen);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\tif (skb_copy_bits(skb, -headerlen, n->head, headerlen + skb->len))\n\t\tBUG();\n\n\tcopy_skb_header(n, skb);\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy);\n\n/**\n *\t__pskb_copy_fclone\t-  create copy of an sk_buff with private head.\n *\t@skb: buffer to copy\n *\t@headroom: headroom of new skb\n *\t@gfp_mask: allocation priority\n *\t@fclone: if true allocate the copy of the skb from the fclone\n *\tcache instead of the head cache; it is recommended to set this\n *\tto true for the cases where the copy will likely be cloned\n *\n *\tMake a copy of both an &sk_buff and part of its data, located\n *\tin header. Fragmented data remain shared. This is used when\n *\tthe caller wishes to modify only header of &sk_buff and needs\n *\tprivate copy of the header to alter. Returns %NULL on failure\n *\tor the pointer to the buffer on success.\n *\tThe returned buffer has a reference count of 1.\n */\n\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone)\n{\n\tunsigned int size = skb_headlen(skb) + headroom;\n\tint flags = skb_alloc_rx_flag(skb) | (fclone ? SKB_ALLOC_FCLONE : 0);\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask, flags, NUMA_NO_NODE);\n\n\tif (!n)\n\t\tgoto out;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headroom);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb_headlen(skb));\n\t/* Copy the bytes */\n\tskb_copy_from_linear_data(skb, n->data, n->len);\n\n\tn->truesize += skb->data_len;\n\tn->data_len  = skb->data_len;\n\tn->len\t     = skb->len;\n\n\tif (skb_shinfo(skb)->nr_frags) {\n\t\tint i;\n\n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tkfree_skb(n);\n\t\t\tn = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_shinfo(n)->frags[i] = skb_shinfo(skb)->frags[i];\n\t\t\tskb_frag_ref(skb, i);\n\t\t}\n\t\tskb_shinfo(n)->nr_frags = i;\n\t}\n\n\tif (skb_has_frag_list(skb)) {\n\t\tskb_shinfo(n)->frag_list = skb_shinfo(skb)->frag_list;\n\t\tskb_clone_fraglist(n);\n\t}\n\n\tcopy_skb_header(n, skb);\nout:\n\treturn n;\n}\nEXPORT_SYMBOL(__pskb_copy_fclone);\n\n/**\n *\tpskb_expand_head - reallocate header of &sk_buff\n *\t@skb: buffer to reallocate\n *\t@nhead: room to add at head\n *\t@ntail: room to add at tail\n *\t@gfp_mask: allocation priority\n *\n *\tExpands (or creates identical copy, if @nhead and @ntail are zero)\n *\theader of @skb. &sk_buff itself is not changed. &sk_buff MUST have\n *\treference count of 1. Returns zero in the case of success or error,\n *\tif expansion failed. In the last case, &sk_buff is not changed.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,\n\t\t     gfp_t gfp_mask)\n{\n\tint i, osize = skb_end_offset(skb);\n\tint size = osize + nhead + ntail;\n\tlong off;\n\tu8 *data;\n\n\tBUG_ON(nhead < 0);\n\n\tif (skb_shared(skb))\n\t\tBUG();\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\tgoto nodata;\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\t/* Copy only real data... and, alas, header. This should be\n\t * optimized for the cases when header is void.\n\t */\n\tmemcpy(data + nhead, skb->head, skb_tail_pointer(skb) - skb->head);\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info, frags[skb_shinfo(skb)->nr_frags]));\n\n\t/*\n\t * if shinfo is shared we must drop the old head gracefully, but if it\n\t * is not we can just drop the old head and let the existing refcount\n\t * be since all we did is relocate the values\n\t */\n\tif (skb_cloned(skb)) {\n\t\t/* copy this zero copy skb frags */\n\t\tif (skb_orphan_frags(skb, gfp_mask))\n\t\t\tgoto nofrags;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\n\t\tskb_release_data(skb);\n\t} else {\n\t\tskb_free_head(skb);\n\t}\n\toff = (data + nhead) - skb->head;\n\n\tskb->head     = data;\n\tskb->head_frag = 0;\n\tskb->data    += off;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end      = size;\n\toff           = nhead;\n#else\n\tskb->end      = skb->head + size;\n#endif\n\tskb->tail\t      += off;\n\tskb_headers_offset_update(skb, nhead);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\n\t/* It is not generally safe to change skb->truesize.\n\t * For the moment, we really care of rx path, or\n\t * when skb is orphaned (not attached to a socket).\n\t */\n\tif (!skb->sk || skb->destructor == sock_edemux)\n\t\tskb->truesize += size - osize;\n\n\treturn 0;\n\nnofrags:\n\tkfree(data);\nnodata:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(pskb_expand_head);\n\n/* Make private copy of skb with writable head and some headroom */\n\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb, unsigned int headroom)\n{\n\tstruct sk_buff *skb2;\n\tint delta = headroom - skb_headroom(skb);\n\n\tif (delta <= 0)\n\t\tskb2 = pskb_copy(skb, GFP_ATOMIC);\n\telse {\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (skb2 && pskb_expand_head(skb2, SKB_DATA_ALIGN(delta), 0,\n\t\t\t\t\t     GFP_ATOMIC)) {\n\t\t\tkfree_skb(skb2);\n\t\t\tskb2 = NULL;\n\t\t}\n\t}\n\treturn skb2;\n}\nEXPORT_SYMBOL(skb_realloc_headroom);\n\n/**\n *\tskb_copy_expand\t-\tcopy and expand sk_buff\n *\t@skb: buffer to copy\n *\t@newheadroom: new free bytes at head\n *\t@newtailroom: new free bytes at tail\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data and while doing so\n *\tallocate additional space.\n *\n *\tThis is used when the caller wishes to modify the data and needs a\n *\tprivate copy of the data to alter as well as more space for new fields.\n *\tReturns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tYou must pass %GFP_ATOMIC as the allocation priority if this function\n *\tis called from an interrupt.\n */\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb,\n\t\t\t\tint newheadroom, int newtailroom,\n\t\t\t\tgfp_t gfp_mask)\n{\n\t/*\n\t *\tAllocate the copy buffer\n\t */\n\tstruct sk_buff *n = __alloc_skb(newheadroom + skb->len + newtailroom,\n\t\t\t\t\tgfp_mask, skb_alloc_rx_flag(skb),\n\t\t\t\t\tNUMA_NO_NODE);\n\tint oldheadroom = skb_headroom(skb);\n\tint head_copy_len, head_copy_off;\n\n\tif (!n)\n\t\treturn NULL;\n\n\tskb_reserve(n, newheadroom);\n\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\thead_copy_len = oldheadroom;\n\thead_copy_off = 0;\n\tif (newheadroom <= head_copy_len)\n\t\thead_copy_len = newheadroom;\n\telse\n\t\thead_copy_off = newheadroom - head_copy_len;\n\n\t/* Copy the linear header and data. */\n\tif (skb_copy_bits(skb, -head_copy_len, n->head + head_copy_off,\n\t\t\t  skb->len + head_copy_len))\n\t\tBUG();\n\n\tcopy_skb_header(n, skb);\n\n\tskb_headers_offset_update(n, newheadroom - oldheadroom);\n\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy_expand);\n\n/**\n *\tskb_pad\t\t\t-\tzero pad the tail of an skb\n *\t@skb: buffer to pad\n *\t@pad: space to pad\n *\n *\tEnsure that a buffer is followed by a padding area that is zero\n *\tfilled. Used by network drivers which may DMA or transfer data\n *\tbeyond the buffer end onto the wire.\n *\n *\tMay return error in out of memory cases. The skb is freed on error.\n */\n\nint skb_pad(struct sk_buff *skb, int pad)\n{\n\tint err;\n\tint ntail;\n\n\t/* If the skbuff is non linear tailroom is always zero.. */\n\tif (!skb_cloned(skb) && skb_tailroom(skb) >= pad) {\n\t\tmemset(skb->data+skb->len, 0, pad);\n\t\treturn 0;\n\t}\n\n\tntail = skb->data_len + pad - (skb->end - skb->tail);\n\tif (likely(skb_cloned(skb) || ntail > 0)) {\n\t\terr = pskb_expand_head(skb, 0, ntail, GFP_ATOMIC);\n\t\tif (unlikely(err))\n\t\t\tgoto free_skb;\n\t}\n\n\t/* FIXME: The use of this function with non-linear skb's really needs\n\t * to be audited.\n\t */\n\terr = skb_linearize(skb);\n\tif (unlikely(err))\n\t\tgoto free_skb;\n\n\tmemset(skb->data + skb->len, 0, pad);\n\treturn 0;\n\nfree_skb:\n\tkfree_skb(skb);\n\treturn err;\n}\nEXPORT_SYMBOL(skb_pad);\n\n/**\n *\tpskb_put - add data to the tail of a potentially fragmented buffer\n *\t@skb: start of the buffer to use\n *\t@tail: tail fragment of the buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the potentially\n *\tfragmented buffer. @tail must be the last fragment of @skb -- or\n *\t@skb itself. If this would exceed the total buffer size the kernel\n *\twill panic. A pointer to the first byte of the extra data is\n *\treturned.\n */\n\nunsigned char *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len)\n{\n\tif (tail != skb) {\n\t\tskb->data_len += len;\n\t\tskb->len += len;\n\t}\n\treturn skb_put(tail, len);\n}\nEXPORT_SYMBOL_GPL(pskb_put);\n\n/**\n *\tskb_put - add data to a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer. If this would\n *\texceed the total buffer size the kernel will panic. A pointer to the\n *\tfirst byte of the extra data is returned.\n */\nunsigned char *skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned char *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\tif (unlikely(skb->tail > skb->end))\n\t\tskb_over_panic(skb, len, __builtin_return_address(0));\n\treturn tmp;\n}\nEXPORT_SYMBOL(skb_put);\n\n/**\n *\tskb_push - add data to the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer at the buffer\n *\tstart. If this would exceed the total buffer headroom the kernel will\n *\tpanic. A pointer to the first byte of the extra data is returned.\n */\nunsigned char *skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\tif (unlikely(skb->data<skb->head))\n\t\tskb_under_panic(skb, len, __builtin_return_address(0));\n\treturn skb->data;\n}\nEXPORT_SYMBOL(skb_push);\n\n/**\n *\tskb_pull - remove data from the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to remove\n *\n *\tThis function removes data from the start of a buffer, returning\n *\tthe memory to the headroom. A pointer to the next data in the buffer\n *\tis returned. Once the data has been pulled future pushes will overwrite\n *\tthe old data.\n */\nunsigned char *skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn skb_pull_inline(skb, len);\n}\nEXPORT_SYMBOL(skb_pull);\n\n/**\n *\tskb_trim - remove end from a buffer\n *\t@skb: buffer to alter\n *\t@len: new length\n *\n *\tCut the length of a buffer down by removing data from the tail. If\n *\tthe buffer is already under the length specified it is not modified.\n *\tThe skb must be linear.\n */\nvoid skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->len > len)\n\t\t__skb_trim(skb, len);\n}\nEXPORT_SYMBOL(skb_trim);\n\n/* Trims skb to length len. It can change skb pointers.\n */\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tstruct sk_buff **fragp;\n\tstruct sk_buff *frag;\n\tint offset = skb_headlen(skb);\n\tint nfrags = skb_shinfo(skb)->nr_frags;\n\tint i;\n\tint err;\n\n\tif (skb_cloned(skb) &&\n\t    unlikely((err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC))))\n\t\treturn err;\n\n\ti = 0;\n\tif (offset >= len)\n\t\tgoto drop_pages;\n\n\tfor (; i < nfrags; i++) {\n\t\tint end = offset + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i++], len - offset);\n\ndrop_pages:\n\t\tskb_shinfo(skb)->nr_frags = i;\n\n\t\tfor (; i < nfrags; i++)\n\t\t\tskb_frag_unref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_drop_fraglist(skb);\n\t\tgoto done;\n\t}\n\n\tfor (fragp = &skb_shinfo(skb)->frag_list; (frag = *fragp);\n\t     fragp = &frag->next) {\n\t\tint end = offset + frag->len;\n\n\t\tif (skb_shared(frag)) {\n\t\t\tstruct sk_buff *nfrag;\n\n\t\t\tnfrag = skb_clone(frag, GFP_ATOMIC);\n\t\t\tif (unlikely(!nfrag))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tnfrag->next = frag->next;\n\t\t\tconsume_skb(frag);\n\t\t\tfrag = nfrag;\n\t\t\t*fragp = frag;\n\t\t}\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (end > len &&\n\t\t    unlikely((err = pskb_trim(frag, len - offset))))\n\t\t\treturn err;\n\n\t\tif (frag->next)\n\t\t\tskb_drop_list(&frag->next);\n\t\tbreak;\n\t}\n\ndone:\n\tif (len > skb_headlen(skb)) {\n\t\tskb->data_len -= skb->len - len;\n\t\tskb->len       = len;\n\t} else {\n\t\tskb->len       = len;\n\t\tskb->data_len  = 0;\n\t\tskb_set_tail_pointer(skb, len);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(___pskb_trim);\n\n/**\n *\t__pskb_pull_tail - advance tail of skb header\n *\t@skb: buffer to reallocate\n *\t@delta: number of bytes to advance tail\n *\n *\tThe function makes a sense only on a fragmented &sk_buff,\n *\tit expands header moving its tail forward and copying necessary\n *\tdata from fragmented part.\n *\n *\t&sk_buff MUST have reference count of 1.\n *\n *\tReturns %NULL (and &sk_buff does not change) if pull failed\n *\tor value of new tail of skb in the case of success.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\n/* Moves tail of skb head forward, copying data from fragmented part,\n * when it is necessary.\n * 1. It may fail due to malloc failure.\n * 2. It may change skb pointers.\n *\n * It is pretty complicated. Luckily, it is called only in exceptional cases.\n */\nunsigned char *__pskb_pull_tail(struct sk_buff *skb, int delta)\n{\n\t/* If skb has not enough free space at tail, get new one\n\t * plus 128 bytes for future expansions. If we have enough\n\t * room at tail, reallocate without expansion only if skb is cloned.\n\t */\n\tint i, k, eat = (skb->tail + delta) - skb->end;\n\n\tif (eat > 0 || skb_cloned(skb)) {\n\t\tif (pskb_expand_head(skb, 0, eat > 0 ? eat + 128 : 0,\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn NULL;\n\t}\n\n\tif (skb_copy_bits(skb, skb_headlen(skb), skb_tail_pointer(skb), delta))\n\t\tBUG();\n\n\t/* Optimization: no fragments, no reasons to preestimate\n\t * size of pulled pages. Superb.\n\t */\n\tif (!skb_has_frag_list(skb))\n\t\tgoto pull_pages;\n\n\t/* Estimate size of pulled pages. */\n\teat = delta;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size >= eat)\n\t\t\tgoto pull_pages;\n\t\teat -= size;\n\t}\n\n\t/* If we need update frag list, we are in troubles.\n\t * Certainly, it possible to add an offset to skb data,\n\t * but taking into account that pulling is expected to\n\t * be very rare operation, it is worth to fight against\n\t * further bloating skb head and crucify ourselves here instead.\n\t * Pure masohism, indeed. 8)8)\n\t */\n\tif (eat) {\n\t\tstruct sk_buff *list = skb_shinfo(skb)->frag_list;\n\t\tstruct sk_buff *clone = NULL;\n\t\tstruct sk_buff *insp = NULL;\n\n\t\tdo {\n\t\t\tBUG_ON(!list);\n\n\t\t\tif (list->len <= eat) {\n\t\t\t\t/* Eaten as whole. */\n\t\t\t\teat -= list->len;\n\t\t\t\tlist = list->next;\n\t\t\t\tinsp = list;\n\t\t\t} else {\n\t\t\t\t/* Eaten partially. */\n\n\t\t\t\tif (skb_shared(list)) {\n\t\t\t\t\t/* Sucks! We need to fork list. :-( */\n\t\t\t\t\tclone = skb_clone(list, GFP_ATOMIC);\n\t\t\t\t\tif (!clone)\n\t\t\t\t\t\treturn NULL;\n\t\t\t\t\tinsp = list->next;\n\t\t\t\t\tlist = clone;\n\t\t\t\t} else {\n\t\t\t\t\t/* This may be pulled without\n\t\t\t\t\t * problems. */\n\t\t\t\t\tinsp = list;\n\t\t\t\t}\n\t\t\t\tif (!pskb_pull(list, eat)) {\n\t\t\t\t\tkfree_skb(clone);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (eat);\n\n\t\t/* Free pulled out fragments. */\n\t\twhile ((list = skb_shinfo(skb)->frag_list) != insp) {\n\t\t\tskb_shinfo(skb)->frag_list = list->next;\n\t\t\tkfree_skb(list);\n\t\t}\n\t\t/* And insert new clone at head. */\n\t\tif (clone) {\n\t\t\tclone->next = list;\n\t\t\tskb_shinfo(skb)->frag_list = clone;\n\t\t}\n\t}\n\t/* Success! Now we may commit changes to skb data. */\n\npull_pages:\n\teat = delta;\n\tk = 0;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size <= eat) {\n\t\t\tskb_frag_unref(skb, i);\n\t\t\teat -= size;\n\t\t} else {\n\t\t\tskb_shinfo(skb)->frags[k] = skb_shinfo(skb)->frags[i];\n\t\t\tif (eat) {\n\t\t\t\tskb_shinfo(skb)->frags[k].page_offset += eat;\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb)->frags[k], eat);\n\t\t\t\teat = 0;\n\t\t\t}\n\t\t\tk++;\n\t\t}\n\t}\n\tskb_shinfo(skb)->nr_frags = k;\n\n\tskb->tail     += delta;\n\tskb->data_len -= delta;\n\n\treturn skb_tail_pointer(skb);\n}\nEXPORT_SYMBOL(__pskb_pull_tail);\n\n/**\n *\tskb_copy_bits - copy bits from skb to kernel buffer\n *\t@skb: source skb\n *\t@offset: offset in source\n *\t@to: destination buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source skb to the\n *\tdestination buffer.\n *\n *\tCAUTION ! :\n *\t\tIf its prototype is ever changed,\n *\t\tcheck arch/{*}/net/{*}.S files,\n *\t\tsince it is called from BPF assembly code.\n */\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\t/* Copy header. */\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_from_linear_data_offset(skb, offset, to, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tto     += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(f);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tvaddr = kmap_atomic(skb_frag_page(f));\n\t\t\tmemcpy(to,\n\t\t\t       vaddr + f->page_offset + offset - start,\n\t\t\t       copy);\n\t\t\tkunmap_atomic(vaddr);\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_copy_bits(frag_iter, offset - start, to, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_copy_bits);\n\n/*\n * Callback from splice_to_pipe(), if we need to release some pages\n * at the end of the spd in case we error'ed out in filling the pipe.\n */\nstatic void sock_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tput_page(spd->pages[i]);\n}\n\nstatic struct page *linear_to_page(struct page *page, unsigned int *len,\n\t\t\t\t   unsigned int *offset,\n\t\t\t\t   struct sock *sk)\n{\n\tstruct page_frag *pfrag = sk_page_frag(sk);\n\n\tif (!sk_page_frag_refill(sk, pfrag))\n\t\treturn NULL;\n\n\t*len = min_t(unsigned int, *len, pfrag->size - pfrag->offset);\n\n\tmemcpy(page_address(pfrag->page) + pfrag->offset,\n\t       page_address(page) + *offset, *len);\n\t*offset = pfrag->offset;\n\tpfrag->offset += *len;\n\n\treturn pfrag->page;\n}\n\nstatic bool spd_can_coalesce(const struct splice_pipe_desc *spd,\n\t\t\t     struct page *page,\n\t\t\t     unsigned int offset)\n{\n\treturn\tspd->nr_pages &&\n\t\tspd->pages[spd->nr_pages - 1] == page &&\n\t\t(spd->partial[spd->nr_pages - 1].offset +\n\t\t spd->partial[spd->nr_pages - 1].len == offset);\n}\n\n/*\n * Fill page/offset/length into spd, if it can hold more pages.\n */\nstatic bool spd_fill_page(struct splice_pipe_desc *spd,\n\t\t\t  struct pipe_inode_info *pipe, struct page *page,\n\t\t\t  unsigned int *len, unsigned int offset,\n\t\t\t  bool linear,\n\t\t\t  struct sock *sk)\n{\n\tif (unlikely(spd->nr_pages == MAX_SKB_FRAGS))\n\t\treturn true;\n\n\tif (linear) {\n\t\tpage = linear_to_page(page, len, &offset, sk);\n\t\tif (!page)\n\t\t\treturn true;\n\t}\n\tif (spd_can_coalesce(spd, page, offset)) {\n\t\tspd->partial[spd->nr_pages - 1].len += *len;\n\t\treturn false;\n\t}\n\tget_page(page);\n\tspd->pages[spd->nr_pages] = page;\n\tspd->partial[spd->nr_pages].len = *len;\n\tspd->partial[spd->nr_pages].offset = offset;\n\tspd->nr_pages++;\n\n\treturn false;\n}\n\nstatic bool __splice_segment(struct page *page, unsigned int poff,\n\t\t\t     unsigned int plen, unsigned int *off,\n\t\t\t     unsigned int *len,\n\t\t\t     struct splice_pipe_desc *spd, bool linear,\n\t\t\t     struct sock *sk,\n\t\t\t     struct pipe_inode_info *pipe)\n{\n\tif (!*len)\n\t\treturn true;\n\n\t/* skip this segment if already processed */\n\tif (*off >= plen) {\n\t\t*off -= plen;\n\t\treturn false;\n\t}\n\n\t/* ignore any bits we already processed */\n\tpoff += *off;\n\tplen -= *off;\n\t*off = 0;\n\n\tdo {\n\t\tunsigned int flen = min(*len, plen);\n\n\t\tif (spd_fill_page(spd, pipe, page, &flen, poff,\n\t\t\t\t  linear, sk))\n\t\t\treturn true;\n\t\tpoff += flen;\n\t\tplen -= flen;\n\t\t*len -= flen;\n\t} while (*len && plen);\n\n\treturn false;\n}\n\n/*\n * Map linear and fragment data from the skb to spd. It reports true if the\n * pipe is full or if we already spliced the requested length.\n */\nstatic bool __skb_splice_bits(struct sk_buff *skb, struct pipe_inode_info *pipe,\n\t\t\t      unsigned int *offset, unsigned int *len,\n\t\t\t      struct splice_pipe_desc *spd, struct sock *sk)\n{\n\tint seg;\n\tstruct sk_buff *iter;\n\n\t/* map the linear part :\n\t * If skb->head_frag is set, this 'linear' part is backed by a\n\t * fragment, and if the head is not shared with any clones then\n\t * we can avoid a copy since we own the head portion of this page.\n\t */\n\tif (__splice_segment(virt_to_page(skb->data),\n\t\t\t     (unsigned long) skb->data & (PAGE_SIZE - 1),\n\t\t\t     skb_headlen(skb),\n\t\t\t     offset, len, spd,\n\t\t\t     skb_head_is_locked(skb),\n\t\t\t     sk, pipe))\n\t\treturn true;\n\n\t/*\n\t * then map the fragments\n\t */\n\tfor (seg = 0; seg < skb_shinfo(skb)->nr_frags; seg++) {\n\t\tconst skb_frag_t *f = &skb_shinfo(skb)->frags[seg];\n\n\t\tif (__splice_segment(skb_frag_page(f),\n\t\t\t\t     f->page_offset, skb_frag_size(f),\n\t\t\t\t     offset, len, spd, false, sk, pipe))\n\t\t\treturn true;\n\t}\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (*offset >= iter->len) {\n\t\t\t*offset -= iter->len;\n\t\t\tcontinue;\n\t\t}\n\t\t/* __skb_splice_bits() only fails if the output has no room\n\t\t * left, so no point in going over the frag_list for the error\n\t\t * case.\n\t\t */\n\t\tif (__skb_splice_bits(iter, pipe, offset, len, spd, sk))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Map data from the skb to a pipe. Should handle both the linear part,\n * the fragments, and the frag list.\n */\nint skb_splice_bits(struct sk_buff *skb, struct sock *sk, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int tlen,\n\t\t    unsigned int flags)\n{\n\tstruct partial_page partial[MAX_SKB_FRAGS];\n\tstruct page *pages[MAX_SKB_FRAGS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages = pages,\n\t\t.partial = partial,\n\t\t.nr_pages_max = MAX_SKB_FRAGS,\n\t\t.flags = flags,\n\t\t.ops = &nosteal_pipe_buf_ops,\n\t\t.spd_release = sock_spd_release,\n\t};\n\tint ret = 0;\n\n\t__skb_splice_bits(skb, pipe, &offset, &tlen, &spd, sk);\n\n\tif (spd.nr_pages)\n\t\tret = splice_to_pipe(pipe, &spd);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(skb_splice_bits);\n\n/**\n *\tskb_store_bits - store bits from kernel buffer to skb\n *\t@skb: destination buffer\n *\t@offset: offset in destination\n *\t@from: source buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source buffer to the\n *\tdestination skb.  This function handles all the messy bits of\n *\ttraversing fragment lists and such.\n */\n\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_to_linear_data_offset(skb, offset, from, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tfrom += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tmemcpy(vaddr + frag->page_offset + offset - start,\n\t\t\t       from, copy);\n\t\t\tkunmap_atomic(vaddr);\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_store_bits(frag_iter, offset - start,\n\t\t\t\t\t   from, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_store_bits);\n\n/* Checksum skb data. */\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t/* Checksum header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = ops->update(skb->data + offset, copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tcsum2 = ops->update(vaddr + frag->page_offset +\n\t\t\t\t\t    offset - start, copy, 0);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tcsum = ops->combine(csum, csum2, pos, copy);\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = __skb_checksum(frag_iter, offset - start,\n\t\t\t\t\t       copy, 0, ops);\n\t\t\tcsum = ops->combine(csum, csum2, pos, copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\n\treturn csum;\n}\nEXPORT_SYMBOL(__skb_checksum);\n\n__wsum skb_checksum(const struct sk_buff *skb, int offset,\n\t\t    int len, __wsum csum)\n{\n\tconst struct skb_checksum_ops ops = {\n\t\t.update  = csum_partial_ext,\n\t\t.combine = csum_block_add_ext,\n\t};\n\n\treturn __skb_checksum(skb, offset, len, csum, &ops);\n}\nEXPORT_SYMBOL(skb_checksum);\n\n/* Both of above in one bottle. */\n\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset,\n\t\t\t\t    u8 *to, int len, __wsum csum)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t/* Copy header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = csum_partial_copy_nocheck(skb->data + offset, to,\n\t\t\t\t\t\t copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tto     += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tcsum2 = csum_partial_copy_nocheck(vaddr +\n\t\t\t\t\t\t\t  frag->page_offset +\n\t\t\t\t\t\t\t  offset - start, to,\n\t\t\t\t\t\t\t  copy, 0);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\t__wsum csum2;\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = skb_copy_and_csum_bits(frag_iter,\n\t\t\t\t\t\t       offset - start,\n\t\t\t\t\t\t       to, copy, 0);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn csum;\n}\nEXPORT_SYMBOL(skb_copy_and_csum_bits);\n\n /**\n *\tskb_zerocopy_headlen - Calculate headroom needed for skb_zerocopy()\n *\t@from: source buffer\n *\n *\tCalculates the amount of linear headroom needed in the 'to' skb passed\n *\tinto skb_zerocopy().\n */\nunsigned int\nskb_zerocopy_headlen(const struct sk_buff *from)\n{\n\tunsigned int hlen = 0;\n\n\tif (!from->head_frag ||\n\t    skb_headlen(from) < L1_CACHE_BYTES ||\n\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS)\n\t\thlen = skb_headlen(from);\n\n\tif (skb_has_frag_list(from))\n\t\thlen = from->len;\n\n\treturn hlen;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy_headlen);\n\n/**\n *\tskb_zerocopy - Zero copy skb to skb\n *\t@to: destination buffer\n *\t@from: source buffer\n *\t@len: number of bytes to copy from source buffer\n *\t@hlen: size of linear headroom in destination buffer\n *\n *\tCopies up to `len` bytes from `from` to `to` by creating references\n *\tto the frags in the source buffer.\n *\n *\tThe `hlen` as calculated by skb_zerocopy_headlen() specifies the\n *\theadroom in the `to` buffer.\n *\n *\tReturn value:\n *\t0: everything is OK\n *\t-ENOMEM: couldn't orphan frags of @from due to lack of memory\n *\t-EFAULT: skb_copy_bits() found some problem with skb geometry\n */\nint\nskb_zerocopy(struct sk_buff *to, struct sk_buff *from, int len, int hlen)\n{\n\tint i, j = 0;\n\tint plen = 0; /* length of skb->head fragment */\n\tint ret;\n\tstruct page *page;\n\tunsigned int offset;\n\n\tBUG_ON(!from->head_frag && !hlen);\n\n\t/* dont bother with small payloads */\n\tif (len <= skb_tailroom(to))\n\t\treturn skb_copy_bits(from, 0, skb_put(to, len), len);\n\n\tif (hlen) {\n\t\tret = skb_copy_bits(from, 0, skb_put(to, hlen), hlen);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\tlen -= hlen;\n\t} else {\n\t\tplen = min_t(int, skb_headlen(from), len);\n\t\tif (plen) {\n\t\t\tpage = virt_to_head_page(from->head);\n\t\t\toffset = from->data - (unsigned char *)page_address(page);\n\t\t\t__skb_fill_page_desc(to, 0, page, offset, plen);\n\t\t\tget_page(page);\n\t\t\tj = 1;\n\t\t\tlen -= plen;\n\t\t}\n\t}\n\n\tto->truesize += len + plen;\n\tto->len += len + plen;\n\tto->data_len += len + plen;\n\n\tif (unlikely(skb_orphan_frags(from, GFP_ATOMIC))) {\n\t\tskb_tx_error(from);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++) {\n\t\tif (!len)\n\t\t\tbreak;\n\t\tskb_shinfo(to)->frags[j] = skb_shinfo(from)->frags[i];\n\t\tskb_shinfo(to)->frags[j].size = min_t(int, skb_shinfo(to)->frags[j].size, len);\n\t\tlen -= skb_shinfo(to)->frags[j].size;\n\t\tskb_frag_ref(to, j);\n\t\tj++;\n\t}\n\tskb_shinfo(to)->nr_frags = j;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy);\n\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to)\n{\n\t__wsum csum;\n\tlong csstart;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tcsstart = skb_checksum_start_offset(skb);\n\telse\n\t\tcsstart = skb_headlen(skb);\n\n\tBUG_ON(csstart > skb_headlen(skb));\n\n\tskb_copy_from_linear_data(skb, to, csstart);\n\n\tcsum = 0;\n\tif (csstart != skb->len)\n\t\tcsum = skb_copy_and_csum_bits(skb, csstart, to + csstart,\n\t\t\t\t\t      skb->len - csstart, 0);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tlong csstuff = csstart + skb->csum_offset;\n\n\t\t*((__sum16 *)(to + csstuff)) = csum_fold(csum);\n\t}\n}\nEXPORT_SYMBOL(skb_copy_and_csum_dev);\n\n/**\n *\tskb_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The head item is\n *\treturned or %NULL if the list is empty.\n */\n\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue);\n\n/**\n *\tskb_dequeue_tail - remove from the tail of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the tail of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The tail item is\n *\treturned or %NULL if the list is empty.\n */\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue_tail(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue_tail);\n\n/**\n *\tskb_queue_purge - empty a list\n *\t@list: list to empty\n *\n *\tDelete all buffers on an &sk_buff list. Each buffer is removed from\n *\tthe list and one reference dropped. This function takes the list\n *\tlock and is atomic with respect to other list locking functions.\n */\nvoid skb_queue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\twhile ((skb = skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL(skb_queue_purge);\n\n/**\n *\tskb_rbtree_purge - empty a skb rbtree\n *\t@root: root of the rbtree to empty\n *\n *\tDelete all buffers on an &sk_buff rbtree. Each buffer is removed from\n *\tthe list and one reference dropped. This function does not take\n *\tany lock. Synchronization should be handled by the caller (e.g., TCP\n *\tout-of-order queue is protected by the socket lock).\n */\nvoid skb_rbtree_purge(struct rb_root *root)\n{\n\tstruct sk_buff *skb, *next;\n\n\trbtree_postorder_for_each_entry_safe(skb, next, root, rbnode)\n\t\tkfree_skb(skb);\n\n\t*root = RB_ROOT;\n}\n\n/**\n *\tskb_queue_head - queue a buffer at the list head\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the start of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_head(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_head);\n\n/**\n *\tskb_queue_tail - queue a buffer at the list tail\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the tail of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_tail(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_tail);\n\n/**\n *\tskb_unlink\t-\tremove a buffer from a list\n *\t@skb: buffer to remove\n *\t@list: list to use\n *\n *\tRemove a packet from a list. The list locks are taken and this\n *\tfunction is atomic with respect to other list locked calls\n *\n *\tYou must know what list the SKB is on.\n */\nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_unlink(skb, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_unlink);\n\n/**\n *\tskb_append\t-\tappend a buffer\n *\t@old: buffer to insert after\n *\t@newsk: buffer to insert\n *\t@list: list to use\n *\n *\tPlace a packet after a given packet in a list. The list locks are taken\n *\tand this function is atomic with respect to other list locked calls.\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_after(list, old, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_append);\n\n/**\n *\tskb_insert\t-\tinsert a buffer\n *\t@old: buffer to insert before\n *\t@newsk: buffer to insert\n *\t@list: list to use\n *\n *\tPlace a packet before a given packet in a list. The list locks are\n * \ttaken and this function is atomic with respect to other list locked\n *\tcalls.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_insert(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_insert(newsk, old->prev, old, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_insert);\n\nstatic inline void skb_split_inside_header(struct sk_buff *skb,\n\t\t\t\t\t   struct sk_buff* skb1,\n\t\t\t\t\t   const u32 len, const int pos)\n{\n\tint i;\n\n\tskb_copy_from_linear_data_offset(skb, len, skb_put(skb1, pos - len),\n\t\t\t\t\t pos - len);\n\t/* And move data appendix as is. */\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\tskb_shinfo(skb1)->frags[i] = skb_shinfo(skb)->frags[i];\n\n\tskb_shinfo(skb1)->nr_frags = skb_shinfo(skb)->nr_frags;\n\tskb_shinfo(skb)->nr_frags  = 0;\n\tskb1->data_len\t\t   = skb->data_len;\n\tskb1->len\t\t   += skb1->data_len;\n\tskb->data_len\t\t   = 0;\n\tskb->len\t\t   = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void skb_split_no_header(struct sk_buff *skb,\n\t\t\t\t       struct sk_buff* skb1,\n\t\t\t\t       const u32 len, int pos)\n{\n\tint i, k = 0;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\n\tskb_shinfo(skb)->nr_frags = 0;\n\tskb1->len\t\t  = skb1->data_len = skb->len - len;\n\tskb->len\t\t  = len;\n\tskb->data_len\t\t  = len - pos;\n\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + size > len) {\n\t\t\tskb_shinfo(skb1)->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < len) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tskb_frag_ref(skb, i);\n\t\t\t\tskb_shinfo(skb1)->frags[0].page_offset += len - pos;\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb1)->frags[0], len - pos);\n\t\t\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i], len - pos);\n\t\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\t\t}\n\t\t\tk++;\n\t\t} else\n\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\tpos += size;\n\t}\n\tskb_shinfo(skb1)->nr_frags = k;\n}\n\n/**\n * skb_split - Split fragmented skb to two parts at length len.\n * @skb: the buffer to split\n * @skb1: the buffer to receive the second part\n * @len: new length for skb\n */\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len)\n{\n\tint pos = skb_headlen(skb);\n\n\tskb_shinfo(skb1)->tx_flags = skb_shinfo(skb)->tx_flags & SKBTX_SHARED_FRAG;\n\tif (len < pos)\t/* Split line is inside header. */\n\t\tskb_split_inside_header(skb, skb1, len, pos);\n\telse\t\t/* Second chunk has no header, nothing to copy. */\n\t\tskb_split_no_header(skb, skb1, len, pos);\n}\nEXPORT_SYMBOL(skb_split);\n\n/* Shifting from/to a cloned skb is a no-go.\n *\n * Caller cannot keep skb_shinfo related pointers past calling here!\n */\nstatic int skb_prepare_for_shift(struct sk_buff *skb)\n{\n\treturn skb_cloned(skb) && pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\n\n/**\n * skb_shift - Shifts paged data partially from skb to another\n * @tgt: buffer into which tail data gets added\n * @skb: buffer from which the paged data comes from\n * @shiftlen: shift up to this many bytes\n *\n * Attempts to shift up to shiftlen worth of bytes, which may be less than\n * the length of the skb, from skb to tgt. Returns number bytes shifted.\n * It's up to caller to free skb if everything was shifted.\n *\n * If @tgt runs out of frags, the whole operation is aborted.\n *\n * Skb cannot include anything else but paged data while tgt is allowed\n * to have non-paged data as well.\n *\n * TODO: full sized shift could be optimized but that would need\n * specialized skb free'er to handle frags without up-to-date nr_frags.\n */\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen)\n{\n\tint from, to, merge, todo;\n\tstruct skb_frag_struct *fragfrom, *fragto;\n\n\tBUG_ON(shiftlen > skb->len);\n\n\tif (skb_headlen(skb))\n\t\treturn 0;\n\n\ttodo = shiftlen;\n\tfrom = 0;\n\tto = skb_shinfo(tgt)->nr_frags;\n\tfragfrom = &skb_shinfo(skb)->frags[from];\n\n\t/* Actual merge is delayed until the point when we know we can\n\t * commit all, so that we don't have to undo partial changes\n\t */\n\tif (!to ||\n\t    !skb_can_coalesce(tgt, to, skb_frag_page(fragfrom),\n\t\t\t      fragfrom->page_offset)) {\n\t\tmerge = -1;\n\t} else {\n\t\tmerge = to - 1;\n\n\t\ttodo -= skb_frag_size(fragfrom);\n\t\tif (todo < 0) {\n\t\t\tif (skb_prepare_for_shift(skb) ||\n\t\t\t    skb_prepare_for_shift(tgt))\n\t\t\t\treturn 0;\n\n\t\t\t/* All previous frag pointers might be stale! */\n\t\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\t\tskb_frag_size_add(fragto, shiftlen);\n\t\t\tskb_frag_size_sub(fragfrom, shiftlen);\n\t\t\tfragfrom->page_offset += shiftlen;\n\n\t\t\tgoto onlymerged;\n\t\t}\n\n\t\tfrom++;\n\t}\n\n\t/* Skip full, not-fitting skb to avoid expensive operations */\n\tif ((shiftlen == skb->len) &&\n\t    (skb_shinfo(skb)->nr_frags - from) > (MAX_SKB_FRAGS - to))\n\t\treturn 0;\n\n\tif (skb_prepare_for_shift(skb) || skb_prepare_for_shift(tgt))\n\t\treturn 0;\n\n\twhile ((todo > 0) && (from < skb_shinfo(skb)->nr_frags)) {\n\t\tif (to == MAX_SKB_FRAGS)\n\t\t\treturn 0;\n\n\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\tfragto = &skb_shinfo(tgt)->frags[to];\n\n\t\tif (todo >= skb_frag_size(fragfrom)) {\n\t\t\t*fragto = *fragfrom;\n\t\t\ttodo -= skb_frag_size(fragfrom);\n\t\t\tfrom++;\n\t\t\tto++;\n\n\t\t} else {\n\t\t\t__skb_frag_ref(fragfrom);\n\t\t\tfragto->page = fragfrom->page;\n\t\t\tfragto->page_offset = fragfrom->page_offset;\n\t\t\tskb_frag_size_set(fragto, todo);\n\n\t\t\tfragfrom->page_offset += todo;\n\t\t\tskb_frag_size_sub(fragfrom, todo);\n\t\t\ttodo = 0;\n\n\t\t\tto++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ready to \"commit\" this state change to tgt */\n\tskb_shinfo(tgt)->nr_frags = to;\n\n\tif (merge >= 0) {\n\t\tfragfrom = &skb_shinfo(skb)->frags[0];\n\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\tskb_frag_size_add(fragto, skb_frag_size(fragfrom));\n\t\t__skb_frag_unref(fragfrom);\n\t}\n\n\t/* Reposition in the original skb */\n\tto = 0;\n\twhile (from < skb_shinfo(skb)->nr_frags)\n\t\tskb_shinfo(skb)->frags[to++] = skb_shinfo(skb)->frags[from++];\n\tskb_shinfo(skb)->nr_frags = to;\n\n\tBUG_ON(todo > 0 && !skb_shinfo(skb)->nr_frags);\n\nonlymerged:\n\t/* Most likely the tgt won't ever need its checksum anymore, skb on\n\t * the other hand might need it if it needs to be resent\n\t */\n\ttgt->ip_summed = CHECKSUM_PARTIAL;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t/* Yak, is it really working this way? Some helper please? */\n\tskb->len -= shiftlen;\n\tskb->data_len -= shiftlen;\n\tskb->truesize -= shiftlen;\n\ttgt->len += shiftlen;\n\ttgt->data_len += shiftlen;\n\ttgt->truesize += shiftlen;\n\n\treturn shiftlen;\n}\n\n/**\n * skb_prepare_seq_read - Prepare a sequential read of skb data\n * @skb: the buffer to read\n * @from: lower offset of data to be read\n * @to: upper offset of data to be read\n * @st: state variable\n *\n * Initializes the specified state variable. Must be called before\n * invoking skb_seq_read() for the first time.\n */\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st)\n{\n\tst->lower_offset = from;\n\tst->upper_offset = to;\n\tst->root_skb = st->cur_skb = skb;\n\tst->frag_idx = st->stepped_offset = 0;\n\tst->frag_data = NULL;\n}\nEXPORT_SYMBOL(skb_prepare_seq_read);\n\n/**\n * skb_seq_read - Sequentially read skb data\n * @consumed: number of bytes consumed by the caller so far\n * @data: destination pointer for data to be returned\n * @st: state variable\n *\n * Reads a block of skb data at @consumed relative to the\n * lower offset specified to skb_prepare_seq_read(). Assigns\n * the head of the data block to @data and returns the length\n * of the block or 0 if the end of the skb data or the upper\n * offset has been reached.\n *\n * The caller is not required to consume all of the data\n * returned, i.e. @consumed is typically set to the number\n * of bytes already consumed and the next call to\n * skb_seq_read() will return the remaining part of the block.\n *\n * Note 1: The size of each block of data returned can be arbitrary,\n *       this limitation is the cost for zerocopy sequential\n *       reads of potentially non linear data.\n *\n * Note 2: Fragment lists within fragments are not implemented\n *       at the moment, state->root_skb could be replaced with\n *       a stack for this purpose.\n */\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st)\n{\n\tunsigned int block_limit, abs_offset = consumed + st->lower_offset;\n\tskb_frag_t *frag;\n\n\tif (unlikely(abs_offset >= st->upper_offset)) {\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\nnext_skb:\n\tblock_limit = skb_headlen(st->cur_skb) + st->stepped_offset;\n\n\tif (abs_offset < block_limit && !st->frag_data) {\n\t\t*data = st->cur_skb->data + (abs_offset - st->stepped_offset);\n\t\treturn block_limit - abs_offset;\n\t}\n\n\tif (st->frag_idx == 0 && !st->frag_data)\n\t\tst->stepped_offset += skb_headlen(st->cur_skb);\n\n\twhile (st->frag_idx < skb_shinfo(st->cur_skb)->nr_frags) {\n\t\tfrag = &skb_shinfo(st->cur_skb)->frags[st->frag_idx];\n\t\tblock_limit = skb_frag_size(frag) + st->stepped_offset;\n\n\t\tif (abs_offset < block_limit) {\n\t\t\tif (!st->frag_data)\n\t\t\t\tst->frag_data = kmap_atomic(skb_frag_page(frag));\n\n\t\t\t*data = (u8 *) st->frag_data + frag->page_offset +\n\t\t\t\t(abs_offset - st->stepped_offset);\n\n\t\t\treturn block_limit - abs_offset;\n\t\t}\n\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\n\t\tst->frag_idx++;\n\t\tst->stepped_offset += skb_frag_size(frag);\n\t}\n\n\tif (st->frag_data) {\n\t\tkunmap_atomic(st->frag_data);\n\t\tst->frag_data = NULL;\n\t}\n\n\tif (st->root_skb == st->cur_skb && skb_has_frag_list(st->root_skb)) {\n\t\tst->cur_skb = skb_shinfo(st->root_skb)->frag_list;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t} else if (st->cur_skb->next) {\n\t\tst->cur_skb = st->cur_skb->next;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_seq_read);\n\n/**\n * skb_abort_seq_read - Abort a sequential read of skb data\n * @st: state variable\n *\n * Must be called if skb_seq_read() was not called until it\n * returned 0.\n */\nvoid skb_abort_seq_read(struct skb_seq_state *st)\n{\n\tif (st->frag_data)\n\t\tkunmap_atomic(st->frag_data);\n}\nEXPORT_SYMBOL(skb_abort_seq_read);\n\n#define TS_SKB_CB(state)\t((struct skb_seq_state *) &((state)->cb))\n\nstatic unsigned int skb_ts_get_next_block(unsigned int offset, const u8 **text,\n\t\t\t\t\t  struct ts_config *conf,\n\t\t\t\t\t  struct ts_state *state)\n{\n\treturn skb_seq_read(offset, text, TS_SKB_CB(state));\n}\n\nstatic void skb_ts_finish(struct ts_config *conf, struct ts_state *state)\n{\n\tskb_abort_seq_read(TS_SKB_CB(state));\n}\n\n/**\n * skb_find_text - Find a text pattern in skb data\n * @skb: the buffer to look in\n * @from: search offset\n * @to: search limit\n * @config: textsearch configuration\n *\n * Finds a pattern in the skb data according to the specified\n * textsearch configuration. Use textsearch_next() to retrieve\n * subsequent occurrences of the pattern. Returns the offset\n * to the first occurrence or UINT_MAX if no match was found.\n */\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config)\n{\n\tstruct ts_state state;\n\tunsigned int ret;\n\n\tconfig->get_next_block = skb_ts_get_next_block;\n\tconfig->finish = skb_ts_finish;\n\n\tskb_prepare_seq_read(skb, from, to, TS_SKB_CB(&state));\n\n\tret = textsearch_find(config, &state);\n\treturn (ret <= to - from ? ret : UINT_MAX);\n}\nEXPORT_SYMBOL(skb_find_text);\n\n/**\n * skb_append_datato_frags - append the user data to a skb\n * @sk: sock  structure\n * @skb: skb structure to be appended with user data.\n * @getfrag: call back function to be used for getting the user data\n * @from: pointer to user message iov\n * @length: length of the iov message\n *\n * Description: This procedure append the user data in the fragment part\n * of the skb if any page alloc fails user this procedure returns  -ENOMEM\n */\nint skb_append_datato_frags(struct sock *sk, struct sk_buff *skb,\n\t\t\tint (*getfrag)(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length)\n{\n\tint frg_cnt = skb_shinfo(skb)->nr_frags;\n\tint copy;\n\tint offset = 0;\n\tint ret;\n\tstruct page_frag *pfrag = &current->task_frag;\n\n\tdo {\n\t\t/* Return error if we don't have space for new frag */\n\t\tif (frg_cnt >= MAX_SKB_FRAGS)\n\t\t\treturn -EMSGSIZE;\n\n\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\treturn -ENOMEM;\n\n\t\t/* copy the user data to page */\n\t\tcopy = min_t(int, length, pfrag->size - pfrag->offset);\n\n\t\tret = getfrag(from, page_address(pfrag->page) + pfrag->offset,\n\t\t\t      offset, copy, 0, skb);\n\t\tif (ret < 0)\n\t\t\treturn -EFAULT;\n\n\t\t/* copy was successful so update the size parameters */\n\t\tskb_fill_page_desc(skb, frg_cnt, pfrag->page, pfrag->offset,\n\t\t\t\t   copy);\n\t\tfrg_cnt++;\n\t\tpfrag->offset += copy;\n\t\tget_page(pfrag->page);\n\n\t\tskb->truesize += copy;\n\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\tskb->len += copy;\n\t\tskb->data_len += copy;\n\t\toffset += copy;\n\t\tlength -= copy;\n\n\t} while (length > 0);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_append_datato_frags);\n\nint skb_append_pagefrags(struct sk_buff *skb, struct page *page,\n\t\t\t int offset, size_t size)\n{\n\tint i = skb_shinfo(skb)->nr_frags;\n\n\tif (skb_can_coalesce(skb, i, page, offset)) {\n\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], size);\n\t} else if (i < MAX_SKB_FRAGS) {\n\t\tget_page(page);\n\t\tskb_fill_page_desc(skb, i, page, offset, size);\n\t} else {\n\t\treturn -EMSGSIZE;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_append_pagefrags);\n\n/**\n *\tskb_pull_rcsum - pull skb and update receive checksum\n *\t@skb: buffer to update\n *\t@len: length of data pulled\n *\n *\tThis function performs an skb_pull on the packet and updates\n *\tthe CHECKSUM_COMPLETE checksum.  It should be used on\n *\treceive path processing instead of skb_pull unless you know\n *\tthat the checksum difference is zero (e.g., a valid IP header)\n *\tor you are setting ip_summed to CHECKSUM_NONE.\n */\nunsigned char *skb_pull_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned char *data = skb->data;\n\n\tBUG_ON(len > skb->len);\n\t__skb_pull(skb, len);\n\tskb_postpull_rcsum(skb, data, len);\n\treturn skb->data;\n}\nEXPORT_SYMBOL_GPL(skb_pull_rcsum);\n\n/**\n *\tskb_segment - Perform protocol segmentation on skb.\n *\t@head_skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\n *\tThis function performs segmentation on the given skb.  It returns\n *\ta pointer to the first in a list of new skbs for the segments.\n *\tIn case of error it returns ERR_PTR(err).\n */\nstruct sk_buff *skb_segment(struct sk_buff *head_skb,\n\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = NULL;\n\tstruct sk_buff *tail = NULL;\n\tstruct sk_buff *list_skb = skb_shinfo(head_skb)->frag_list;\n\tskb_frag_t *frag = skb_shinfo(head_skb)->frags;\n\tunsigned int mss = skb_shinfo(head_skb)->gso_size;\n\tunsigned int doffset = head_skb->data - skb_mac_header(head_skb);\n\tstruct sk_buff *frag_skb = head_skb;\n\tunsigned int offset = doffset;\n\tunsigned int tnl_hlen = skb_tnl_header_len(head_skb);\n\tunsigned int partial_segs = 0;\n\tunsigned int headroom;\n\tunsigned int len = head_skb->len;\n\t__be16 proto;\n\tbool csum, sg;\n\tint nfrags = skb_shinfo(head_skb)->nr_frags;\n\tint err = -ENOMEM;\n\tint i = 0;\n\tint pos;\n\tint dummy;\n\n\t__skb_push(head_skb, doffset);\n\tproto = skb_network_protocol(head_skb, &dummy);\n\tif (unlikely(!proto))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tsg = !!(features & NETIF_F_SG);\n\tcsum = !!can_checksum_protocol(features, proto);\n\n\tif (sg && csum && (mss != GSO_BY_FRAGS))  {\n\t\tif (!(features & NETIF_F_GSO_PARTIAL)) {\n\t\t\tstruct sk_buff *iter;\n\n\t\t\tif (!list_skb ||\n\t\t\t    !net_gso_ok(features, skb_shinfo(head_skb)->gso_type))\n\t\t\t\tgoto normal;\n\n\t\t\t/* Split the buffer at the frag_list pointer.\n\t\t\t * This is based on the assumption that all\n\t\t\t * buffers in the chain excluding the last\n\t\t\t * containing the same amount of data.\n\t\t\t */\n\t\t\tskb_walk_frags(head_skb, iter) {\n\t\t\t\tif (skb_headlen(iter))\n\t\t\t\t\tgoto normal;\n\n\t\t\t\tlen -= iter->len;\n\t\t\t}\n\t\t}\n\n\t\t/* GSO partial only requires that we trim off any excess that\n\t\t * doesn't fit into an MSS sized block, so take care of that\n\t\t * now.\n\t\t */\n\t\tpartial_segs = len / mss;\n\t\tif (partial_segs > 1)\n\t\t\tmss *= partial_segs;\n\t\telse\n\t\t\tpartial_segs = 0;\n\t}\n\nnormal:\n\theadroom = skb_headroom(head_skb);\n\tpos = skb_headlen(head_skb);\n\n\tdo {\n\t\tstruct sk_buff *nskb;\n\t\tskb_frag_t *nskb_frag;\n\t\tint hsize;\n\t\tint size;\n\n\t\tif (unlikely(mss == GSO_BY_FRAGS)) {\n\t\t\tlen = list_skb->len;\n\t\t} else {\n\t\t\tlen = head_skb->len - offset;\n\t\t\tif (len > mss)\n\t\t\t\tlen = mss;\n\t\t}\n\n\t\thsize = skb_headlen(head_skb) - offset;\n\t\tif (hsize < 0)\n\t\t\thsize = 0;\n\t\tif (hsize > len || !sg)\n\t\t\thsize = len;\n\n\t\tif (!hsize && i >= nfrags && skb_headlen(list_skb) &&\n\t\t    (skb_headlen(list_skb) == len || sg)) {\n\t\t\tBUG_ON(skb_headlen(list_skb) > len);\n\n\t\t\ti = 0;\n\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\tfrag_skb = list_skb;\n\t\t\tpos += skb_headlen(list_skb);\n\n\t\t\twhile (pos < offset + len) {\n\t\t\t\tBUG_ON(i >= nfrags);\n\n\t\t\t\tsize = skb_frag_size(frag);\n\t\t\t\tif (pos + size > offset + len)\n\t\t\t\t\tbreak;\n\n\t\t\t\ti++;\n\t\t\t\tpos += size;\n\t\t\t\tfrag++;\n\t\t\t}\n\n\t\t\tnskb = skb_clone(list_skb, GFP_ATOMIC);\n\t\t\tlist_skb = list_skb->next;\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tif (unlikely(pskb_trim(nskb, len))) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\thsize = skb_end_offset(nskb);\n\t\t\tif (skb_cow_head(nskb, doffset + headroom)) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tnskb->truesize += skb_end_offset(nskb) - hsize;\n\t\t\tskb_release_head_state(nskb);\n\t\t\t__skb_push(nskb, doffset);\n\t\t} else {\n\t\t\tnskb = __alloc_skb(hsize + doffset + headroom,\n\t\t\t\t\t   GFP_ATOMIC, skb_alloc_rx_flag(head_skb),\n\t\t\t\t\t   NUMA_NO_NODE);\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tskb_reserve(nskb, headroom);\n\t\t\t__skb_put(nskb, doffset);\n\t\t}\n\n\t\tif (segs)\n\t\t\ttail->next = nskb;\n\t\telse\n\t\t\tsegs = nskb;\n\t\ttail = nskb;\n\n\t\t__copy_skb_header(nskb, head_skb);\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - headroom);\n\t\tskb_reset_mac_len(nskb);\n\n\t\tskb_copy_from_linear_data_offset(head_skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t doffset + tnl_hlen);\n\n\t\tif (nskb->len == len + doffset)\n\t\t\tgoto perform_csum_check;\n\n\t\tif (!sg) {\n\t\t\tif (!nskb->remcsum_offload)\n\t\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tSKB_GSO_CB(nskb)->csum =\n\t\t\t\tskb_copy_and_csum_bits(head_skb, offset,\n\t\t\t\t\t\t       skb_put(nskb, len),\n\t\t\t\t\t\t       len, 0);\n\t\t\tSKB_GSO_CB(nskb)->csum_start =\n\t\t\t\tskb_headroom(nskb) + doffset;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnskb_frag = skb_shinfo(nskb)->frags;\n\n\t\tskb_copy_from_linear_data_offset(head_skb, offset,\n\t\t\t\t\t\t skb_put(nskb, hsize), hsize);\n\n\t\tskb_shinfo(nskb)->tx_flags = skb_shinfo(head_skb)->tx_flags &\n\t\t\tSKBTX_SHARED_FRAG;\n\n\t\twhile (pos < offset + len) {\n\t\t\tif (i >= nfrags) {\n\t\t\t\tBUG_ON(skb_headlen(list_skb));\n\n\t\t\t\ti = 0;\n\t\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\t\tfrag_skb = list_skb;\n\n\t\t\t\tBUG_ON(!nfrags);\n\n\t\t\t\tlist_skb = list_skb->next;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_shinfo(nskb)->nr_frags >=\n\t\t\t\t     MAX_SKB_FRAGS)) {\n\t\t\t\tnet_warn_ratelimited(\n\t\t\t\t\t\"skb_segment: too many frags: %u %u\\n\",\n\t\t\t\t\tpos, mss);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_orphan_frags(frag_skb, GFP_ATOMIC)))\n\t\t\t\tgoto err;\n\n\t\t\t*nskb_frag = *frag;\n\t\t\t__skb_frag_ref(nskb_frag);\n\t\t\tsize = skb_frag_size(nskb_frag);\n\n\t\t\tif (pos < offset) {\n\t\t\t\tnskb_frag->page_offset += offset - pos;\n\t\t\t\tskb_frag_size_sub(nskb_frag, offset - pos);\n\t\t\t}\n\n\t\t\tskb_shinfo(nskb)->nr_frags++;\n\n\t\t\tif (pos + size <= offset + len) {\n\t\t\t\ti++;\n\t\t\t\tfrag++;\n\t\t\t\tpos += size;\n\t\t\t} else {\n\t\t\t\tskb_frag_size_sub(nskb_frag, pos + size - (offset + len));\n\t\t\t\tgoto skip_fraglist;\n\t\t\t}\n\n\t\t\tnskb_frag++;\n\t\t}\n\nskip_fraglist:\n\t\tnskb->data_len = len - hsize;\n\t\tnskb->len += nskb->data_len;\n\t\tnskb->truesize += nskb->data_len;\n\nperform_csum_check:\n\t\tif (!csum) {\n\t\t\tif (skb_has_shared_frag(nskb)) {\n\t\t\t\terr = __skb_linearize(nskb);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif (!nskb->remcsum_offload)\n\t\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tSKB_GSO_CB(nskb)->csum =\n\t\t\t\tskb_checksum(nskb, doffset,\n\t\t\t\t\t     nskb->len - doffset, 0);\n\t\t\tSKB_GSO_CB(nskb)->csum_start =\n\t\t\t\tskb_headroom(nskb) + doffset;\n\t\t}\n\t} while ((offset += len) < head_skb->len);\n\n\t/* Some callers want to get the end of the list.\n\t * Put it in segs->prev to avoid walking the list.\n\t * (see validate_xmit_skb_list() for example)\n\t */\n\tsegs->prev = tail;\n\n\tif (partial_segs) {\n\t\tstruct sk_buff *iter;\n\t\tint type = skb_shinfo(head_skb)->gso_type;\n\t\tunsigned short gso_size = skb_shinfo(head_skb)->gso_size;\n\n\t\t/* Update type to add partial and then remove dodgy if set */\n\t\ttype |= (features & NETIF_F_GSO_PARTIAL) / NETIF_F_GSO_PARTIAL * SKB_GSO_PARTIAL;\n\t\ttype &= ~SKB_GSO_DODGY;\n\n\t\t/* Update GSO info and prepare to start updating headers on\n\t\t * our way back down the stack of protocols.\n\t\t */\n\t\tfor (iter = segs; iter; iter = iter->next) {\n\t\t\tskb_shinfo(iter)->gso_size = gso_size;\n\t\t\tskb_shinfo(iter)->gso_segs = partial_segs;\n\t\t\tskb_shinfo(iter)->gso_type = type;\n\t\t\tSKB_GSO_CB(iter)->data_offset = skb_headroom(iter) + doffset;\n\t\t}\n\n\t\tif (tail->len - doffset <= gso_size)\n\t\t\tskb_shinfo(tail)->gso_size = 0;\n\t\telse if (tail != segs)\n\t\t\tskb_shinfo(tail)->gso_segs = DIV_ROUND_UP(tail->len - doffset, gso_size);\n\t}\n\n\t/* Following permits correct backpressure, for protocols\n\t * using skb_set_owner_w().\n\t * Idea is to tranfert ownership from head_skb to last segment.\n\t */\n\tif (head_skb->destructor == sock_wfree) {\n\t\tswap(tail->truesize, head_skb->truesize);\n\t\tswap(tail->destructor, head_skb->destructor);\n\t\tswap(tail->sk, head_skb->sk);\n\t}\n\treturn segs;\n\nerr:\n\tkfree_skb_list(segs);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(skb_segment);\n\nint skb_gro_receive(struct sk_buff **head, struct sk_buff *skb)\n{\n\tstruct skb_shared_info *pinfo, *skbinfo = skb_shinfo(skb);\n\tunsigned int offset = skb_gro_offset(skb);\n\tunsigned int headlen = skb_headlen(skb);\n\tunsigned int len = skb_gro_len(skb);\n\tstruct sk_buff *lp, *p = *head;\n\tunsigned int delta_truesize;\n\n\tif (unlikely(p->len + len >= 65536))\n\t\treturn -E2BIG;\n\n\tlp = NAPI_GRO_CB(p)->last;\n\tpinfo = skb_shinfo(lp);\n\n\tif (headlen <= offset) {\n\t\tskb_frag_t *frag;\n\t\tskb_frag_t *frag2;\n\t\tint i = skbinfo->nr_frags;\n\t\tint nr_frags = pinfo->nr_frags + i;\n\n\t\tif (nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\toffset -= headlen;\n\t\tpinfo->nr_frags = nr_frags;\n\t\tskbinfo->nr_frags = 0;\n\n\t\tfrag = pinfo->frags + nr_frags;\n\t\tfrag2 = skbinfo->frags + i;\n\t\tdo {\n\t\t\t*--frag = *--frag2;\n\t\t} while (--i);\n\n\t\tfrag->page_offset += offset;\n\t\tskb_frag_size_sub(frag, offset);\n\n\t\t/* all fragments truesize : remove (head size + sk_buff) */\n\t\tdelta_truesize = skb->truesize -\n\t\t\t\t SKB_TRUESIZE(skb_end_offset(skb));\n\n\t\tskb->truesize -= skb->data_len;\n\t\tskb->len -= skb->data_len;\n\t\tskb->data_len = 0;\n\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE;\n\t\tgoto done;\n\t} else if (skb->head_frag) {\n\t\tint nr_frags = pinfo->nr_frags;\n\t\tskb_frag_t *frag = pinfo->frags + nr_frags;\n\t\tstruct page *page = virt_to_head_page(skb->head);\n\t\tunsigned int first_size = headlen - offset;\n\t\tunsigned int first_offset;\n\n\t\tif (nr_frags + 1 + skbinfo->nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\tfirst_offset = skb->data -\n\t\t\t       (unsigned char *)page_address(page) +\n\t\t\t       offset;\n\n\t\tpinfo->nr_frags = nr_frags + 1 + skbinfo->nr_frags;\n\n\t\tfrag->page.p\t  = page;\n\t\tfrag->page_offset = first_offset;\n\t\tskb_frag_size_set(frag, first_size);\n\n\t\tmemcpy(frag + 1, skbinfo->frags, sizeof(*frag) * skbinfo->nr_frags);\n\t\t/* We dont need to clear skbinfo->nr_frags here */\n\n\t\tdelta_truesize = skb->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE_STOLEN_HEAD;\n\t\tgoto done;\n\t}\n\nmerge:\n\tdelta_truesize = skb->truesize;\n\tif (offset > headlen) {\n\t\tunsigned int eat = offset - headlen;\n\n\t\tskbinfo->frags[0].page_offset += eat;\n\t\tskb_frag_size_sub(&skbinfo->frags[0], eat);\n\t\tskb->data_len -= eat;\n\t\tskb->len -= eat;\n\t\toffset = headlen;\n\t}\n\n\t__skb_pull(skb, offset);\n\n\tif (NAPI_GRO_CB(p)->last == p)\n\t\tskb_shinfo(p)->frag_list = skb;\n\telse\n\t\tNAPI_GRO_CB(p)->last->next = skb;\n\tNAPI_GRO_CB(p)->last = skb;\n\t__skb_header_release(skb);\n\tlp = p;\n\ndone:\n\tNAPI_GRO_CB(p)->count++;\n\tp->data_len += len;\n\tp->truesize += delta_truesize;\n\tp->len += len;\n\tif (lp != p) {\n\t\tlp->data_len += len;\n\t\tlp->truesize += delta_truesize;\n\t\tlp->len += len;\n\t}\n\tNAPI_GRO_CB(skb)->same_flow = 1;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_gro_receive);\n\nvoid __init skb_init(void)\n{\n\tskbuff_head_cache = kmem_cache_create(\"skbuff_head_cache\",\n\t\t\t\t\t      sizeof(struct sk_buff),\n\t\t\t\t\t      0,\n\t\t\t\t\t      SLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t      NULL);\n\tskbuff_fclone_cache = kmem_cache_create(\"skbuff_fclone_cache\",\n\t\t\t\t\t\tsizeof(struct sk_buff_fclones),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tNULL);\n}\n\n/**\n *\tskb_to_sgvec - Fill a scatter-gather list from a socket buffer\n *\t@skb: Socket buffer containing the buffers to be mapped\n *\t@sg: The scatter-gather list to map into\n *\t@offset: The offset into the buffer's contents to start mapping\n *\t@len: Length of buffer space to be mapped\n *\n *\tFill the specified scatter-gather list with mappings/pointers into a\n *\tregion of the buffer space attached to a socket buffer.\n */\nstatic int\n__skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint elt = 0;\n\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tsg_set_buf(sg, skb->data + offset, copy);\n\t\telt++;\n\t\tif ((len -= copy) == 0)\n\t\t\treturn elt;\n\t\toffset += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tsg_set_page(&sg[elt], skb_frag_page(frag), copy,\n\t\t\t\t\tfrag->page_offset+offset-start);\n\t\t\telt++;\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\telt += __skb_to_sgvec(frag_iter, sg+elt, offset - start,\n\t\t\t\t\t      copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn elt;\n}\n\n/* As compared with skb_to_sgvec, skb_to_sgvec_nomark only map skb to given\n * sglist without mark the sg which contain last skb data as the end.\n * So the caller can mannipulate sg list as will when padding new data after\n * the first call without calling sg_unmark_end to expend sg list.\n *\n * Scenario to use skb_to_sgvec_nomark:\n * 1. sg_init_table\n * 2. skb_to_sgvec_nomark(payload1)\n * 3. skb_to_sgvec_nomark(payload2)\n *\n * This is equivalent to:\n * 1. sg_init_table\n * 2. skb_to_sgvec(payload1)\n * 3. sg_unmark_end\n * 4. skb_to_sgvec(payload2)\n *\n * When mapping mutilple payload conditionally, skb_to_sgvec_nomark\n * is more preferable.\n */\nint skb_to_sgvec_nomark(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\tint offset, int len)\n{\n\treturn __skb_to_sgvec(skb, sg, offset, len);\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec_nomark);\n\nint skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint nsg = __skb_to_sgvec(skb, sg, offset, len);\n\n\tsg_mark_end(&sg[nsg - 1]);\n\n\treturn nsg;\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec);\n\n/**\n *\tskb_cow_data - Check that a socket buffer's data buffers are writable\n *\t@skb: The socket buffer to check.\n *\t@tailbits: Amount of trailing space to be added\n *\t@trailer: Returned pointer to the skb where the @tailbits space begins\n *\n *\tMake sure that the data buffers attached to a socket buffer are\n *\twritable. If they are not, private copies are made of the data buffers\n *\tand the socket buffer is set to use these instead.\n *\n *\tIf @tailbits is given, make sure that there is space to write @tailbits\n *\tbytes of data beyond current end of socket buffer.  @trailer will be\n *\tset to point to the skb in which this space begins.\n *\n *\tThe number of scatterlist elements required to completely map the\n *\tCOW'd and extended socket buffer will be returned.\n */\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer)\n{\n\tint copyflag;\n\tint elt;\n\tstruct sk_buff *skb1, **skb_p;\n\n\t/* If skb is cloned or its head is paged, reallocate\n\t * head pulling out all the pages (pages are considered not writable\n\t * at the moment even if they are anonymous).\n\t */\n\tif ((skb_cloned(skb) || skb_shinfo(skb)->nr_frags) &&\n\t    __pskb_pull_tail(skb, skb_pagelen(skb)-skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\t/* Easy case. Most of packets will go this way. */\n\tif (!skb_has_frag_list(skb)) {\n\t\t/* A little of trouble, not enough of space for trailer.\n\t\t * This should not happen, when stack is tuned to generate\n\t\t * good frames. OK, on miss we reallocate and reserve even more\n\t\t * space, 128 bytes is fair. */\n\n\t\tif (skb_tailroom(skb) < tailbits &&\n\t\t    pskb_expand_head(skb, 0, tailbits-skb_tailroom(skb)+128, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\t/* Voila! */\n\t\t*trailer = skb;\n\t\treturn 1;\n\t}\n\n\t/* Misery. We are in troubles, going to mincer fragments... */\n\n\telt = 1;\n\tskb_p = &skb_shinfo(skb)->frag_list;\n\tcopyflag = 0;\n\n\twhile ((skb1 = *skb_p) != NULL) {\n\t\tint ntail = 0;\n\n\t\t/* The fragment is partially pulled by someone,\n\t\t * this can happen on input. Copy it and everything\n\t\t * after it. */\n\n\t\tif (skb_shared(skb1))\n\t\t\tcopyflag = 1;\n\n\t\t/* If the skb is the last, worry about trailer. */\n\n\t\tif (skb1->next == NULL && tailbits) {\n\t\t\tif (skb_shinfo(skb1)->nr_frags ||\n\t\t\t    skb_has_frag_list(skb1) ||\n\t\t\t    skb_tailroom(skb1) < tailbits)\n\t\t\t\tntail = tailbits + 128;\n\t\t}\n\n\t\tif (copyflag ||\n\t\t    skb_cloned(skb1) ||\n\t\t    ntail ||\n\t\t    skb_shinfo(skb1)->nr_frags ||\n\t\t    skb_has_frag_list(skb1)) {\n\t\t\tstruct sk_buff *skb2;\n\n\t\t\t/* Fuck, we are miserable poor guys... */\n\t\t\tif (ntail == 0)\n\t\t\t\tskb2 = skb_copy(skb1, GFP_ATOMIC);\n\t\t\telse\n\t\t\t\tskb2 = skb_copy_expand(skb1,\n\t\t\t\t\t\t       skb_headroom(skb1),\n\t\t\t\t\t\t       ntail,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\t\tif (unlikely(skb2 == NULL))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (skb1->sk)\n\t\t\t\tskb_set_owner_w(skb2, skb1->sk);\n\n\t\t\t/* Looking around. Are we still alive?\n\t\t\t * OK, link new skb, drop old one */\n\n\t\t\tskb2->next = skb1->next;\n\t\t\t*skb_p = skb2;\n\t\t\tkfree_skb(skb1);\n\t\t\tskb1 = skb2;\n\t\t}\n\t\telt++;\n\t\t*trailer = skb1;\n\t\tskb_p = &skb1->next;\n\t}\n\n\treturn elt;\n}\nEXPORT_SYMBOL_GPL(skb_cow_data);\n\nstatic void sock_rmem_free(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tatomic_sub(skb->truesize, &sk->sk_rmem_alloc);\n}\n\nstatic void skb_set_err_queue(struct sk_buff *skb)\n{\n\t/* pkt_type of skbs received on local sockets is never PACKET_OUTGOING.\n\t * So, it is safe to (mis)use it to mark skbs on the error queue.\n\t */\n\tskb->pkt_type = PACKET_OUTGOING;\n\tBUILD_BUG_ON(PACKET_OUTGOING == 0);\n}\n\n/*\n * Note: We dont mem charge error packets (no sk_forward_alloc changes)\n */\nint sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned int)sk->sk_rcvbuf)\n\t\treturn -ENOMEM;\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_rmem_free;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\tskb_set_err_queue(skb);\n\n\t/* before exiting rcu section, make sure dst is refcounted */\n\tskb_dst_force(skb);\n\n\tskb_queue_tail(&sk->sk_error_queue, skb);\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_queue_err_skb);\n\nstatic bool is_icmp_err_skb(const struct sk_buff *skb)\n{\n\treturn skb && (SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_ICMP ||\n\t\t       SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_ICMP6);\n}\n\nstruct sk_buff *sock_dequeue_err_skb(struct sock *sk)\n{\n\tstruct sk_buff_head *q = &sk->sk_error_queue;\n\tstruct sk_buff *skb, *skb_next = NULL;\n\tbool icmp_next = false;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\tskb = __skb_dequeue(q);\n\tif (skb && (skb_next = skb_peek(q)))\n\t\ticmp_next = is_icmp_err_skb(skb_next);\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\tif (is_icmp_err_skb(skb) && !icmp_next)\n\t\tsk->sk_err = 0;\n\n\tif (skb_next)\n\t\tsk->sk_error_report(sk);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(sock_dequeue_err_skb);\n\n/**\n * skb_clone_sk - create clone of skb, and take reference to socket\n * @skb: the skb to clone\n *\n * This function creates a clone of a buffer that holds a reference on\n * sk_refcnt.  Buffers created via this function are meant to be\n * returned using sock_queue_err_skb, or free via kfree_skb.\n *\n * When passing buffers allocated with this function to sock_queue_err_skb\n * it is necessary to wrap the call with sock_hold/sock_put in order to\n * prevent the socket from being released prior to being enqueued on\n * the sk_error_queue.\n */\nstruct sk_buff *skb_clone_sk(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sk_buff *clone;\n\n\tif (!sk || !atomic_inc_not_zero(&sk->sk_refcnt))\n\t\treturn NULL;\n\n\tclone = skb_clone(skb, GFP_ATOMIC);\n\tif (!clone) {\n\t\tsock_put(sk);\n\t\treturn NULL;\n\t}\n\n\tclone->sk = sk;\n\tclone->destructor = sock_efree;\n\n\treturn clone;\n}\nEXPORT_SYMBOL(skb_clone_sk);\n\nstatic void __skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t\t\tstruct sock *sk,\n\t\t\t\t\tint tstype,\n\t\t\t\t\tbool opt_stats)\n{\n\tstruct sock_exterr_skb *serr;\n\tint err;\n\n\tBUILD_BUG_ON(sizeof(struct sock_exterr_skb) > sizeof(skb->cb));\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;\n\tserr->ee.ee_info = tstype;\n\tserr->opt_stats = opt_stats;\n\tif (sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID) {\n\t\tserr->ee.ee_data = skb_shinfo(skb)->tskey;\n\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\tserr->ee.ee_data -= sk->sk_tskey;\n\t}\n\n\terr = sock_queue_err_skb(sk, skb);\n\n\tif (err)\n\t\tkfree_skb(skb);\n}\n\nstatic bool skb_may_tx_timestamp(struct sock *sk, bool tsonly)\n{\n\tbool ret;\n\n\tif (likely(sysctl_tstamp_allow_data || tsonly))\n\t\treturn true;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tret = sk->sk_socket && sk->sk_socket->file &&\n\t      file_ns_capable(sk->sk_socket->file, &init_user_ns, CAP_NET_RAW);\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn ret;\n}\n\nvoid skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps)\n{\n\tstruct sock *sk = skb->sk;\n\n\tif (!skb_may_tx_timestamp(sk, false))\n\t\treturn;\n\n\t/* Take a reference to prevent skb_orphan() from freeing the socket,\n\t * but only if the socket refcount is not zero.\n\t */\n\tif (likely(atomic_inc_not_zero(&sk->sk_refcnt))) {\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\t\t__skb_complete_tx_timestamp(skb, sk, SCM_TSTAMP_SND, false);\n\t\tsock_put(sk);\n\t}\n}\nEXPORT_SYMBOL_GPL(skb_complete_tx_timestamp);\n\nvoid __skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t     struct sock *sk, int tstype)\n{\n\tstruct sk_buff *skb;\n\tbool tsonly, opt_stats = false;\n\n\tif (!sk)\n\t\treturn;\n\n\ttsonly = sk->sk_tsflags & SOF_TIMESTAMPING_OPT_TSONLY;\n\tif (!skb_may_tx_timestamp(sk, tsonly))\n\t\treturn;\n\n\tif (tsonly) {\n#ifdef CONFIG_INET\n\t\tif ((sk->sk_tsflags & SOF_TIMESTAMPING_OPT_STATS) &&\n\t\t    sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM) {\n\t\t\tskb = tcp_get_timestamping_opt_stats(sk);\n\t\t\topt_stats = true;\n\t\t} else\n#endif\n\t\t\tskb = alloc_skb(0, GFP_ATOMIC);\n\t} else {\n\t\tskb = skb_clone(orig_skb, GFP_ATOMIC);\n\t}\n\tif (!skb)\n\t\treturn;\n\n\tif (tsonly) {\n\t\tskb_shinfo(skb)->tx_flags = skb_shinfo(orig_skb)->tx_flags;\n\t\tskb_shinfo(skb)->tskey = skb_shinfo(orig_skb)->tskey;\n\t}\n\n\tif (hwtstamps)\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\telse\n\t\tskb->tstamp = ktime_get_real();\n\n\t__skb_complete_tx_timestamp(skb, sk, tstype, opt_stats);\n}\nEXPORT_SYMBOL_GPL(__skb_tstamp_tx);\n\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t   struct skb_shared_hwtstamps *hwtstamps)\n{\n\treturn __skb_tstamp_tx(orig_skb, hwtstamps, orig_skb->sk,\n\t\t\t       SCM_TSTAMP_SND);\n}\nEXPORT_SYMBOL_GPL(skb_tstamp_tx);\n\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sock_exterr_skb *serr;\n\tint err = 1;\n\n\tskb->wifi_acked_valid = 1;\n\tskb->wifi_acked = acked;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TXSTATUS;\n\n\t/* Take a reference to prevent skb_orphan() from freeing the socket,\n\t * but only if the socket refcount is not zero.\n\t */\n\tif (likely(atomic_inc_not_zero(&sk->sk_refcnt))) {\n\t\terr = sock_queue_err_skb(sk, skb);\n\t\tsock_put(sk);\n\t}\n\tif (err)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_complete_wifi_ack);\n\n/**\n * skb_partial_csum_set - set up and verify partial csum values for packet\n * @skb: the skb to set\n * @start: the number of bytes after skb->data to start checksumming.\n * @off: the offset from start to place the checksum.\n *\n * For untrusted partially-checksummed packets, we need to make sure the values\n * for skb->csum_start and skb->csum_offset are valid so we don't oops.\n *\n * This function checks and sets those values and skb->ip_summed: if this\n * returns false you should drop the packet.\n */\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off)\n{\n\tif (unlikely(start > skb_headlen(skb)) ||\n\t    unlikely((int)start + off > skb_headlen(skb) - 2)) {\n\t\tnet_warn_ratelimited(\"bad partial csum: csum=%u/%u len=%u\\n\",\n\t\t\t\t     start, off, skb_headlen(skb));\n\t\treturn false;\n\t}\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = skb_headroom(skb) + start;\n\tskb->csum_offset = off;\n\tskb_set_transport_header(skb, start);\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(skb_partial_csum_set);\n\nstatic int skb_maybe_pull_tail(struct sk_buff *skb, unsigned int len,\n\t\t\t       unsigned int max)\n{\n\tif (skb_headlen(skb) >= len)\n\t\treturn 0;\n\n\t/* If we need to pullup then pullup to the max, so we\n\t * won't need to do it again.\n\t */\n\tif (max > skb->len)\n\t\tmax = skb->len;\n\n\tif (__pskb_pull_tail(skb, max - skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\tif (skb_headlen(skb) < len)\n\t\treturn -EPROTO;\n\n\treturn 0;\n}\n\n#define MAX_TCP_HDR_LEN (15 * 4)\n\nstatic __sum16 *skb_checksum_setup_ip(struct sk_buff *skb,\n\t\t\t\t      typeof(IPPROTO_IP) proto,\n\t\t\t\t      unsigned int off)\n{\n\tswitch (proto) {\n\t\tint err;\n\n\tcase IPPROTO_TCP:\n\t\terr = skb_maybe_pull_tail(skb, off + sizeof(struct tcphdr),\n\t\t\t\t\t  off + MAX_TCP_HDR_LEN);\n\t\tif (!err && !skb_partial_csum_set(skb, off,\n\t\t\t\t\t\t  offsetof(struct tcphdr,\n\t\t\t\t\t\t\t   check)))\n\t\t\terr = -EPROTO;\n\t\treturn err ? ERR_PTR(err) : &tcp_hdr(skb)->check;\n\n\tcase IPPROTO_UDP:\n\t\terr = skb_maybe_pull_tail(skb, off + sizeof(struct udphdr),\n\t\t\t\t\t  off + sizeof(struct udphdr));\n\t\tif (!err && !skb_partial_csum_set(skb, off,\n\t\t\t\t\t\t  offsetof(struct udphdr,\n\t\t\t\t\t\t\t   check)))\n\t\t\terr = -EPROTO;\n\t\treturn err ? ERR_PTR(err) : &udp_hdr(skb)->check;\n\t}\n\n\treturn ERR_PTR(-EPROTO);\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * maximally sized IP and TCP or UDP headers.\n */\n#define MAX_IP_HDR_LEN 128\n\nstatic int skb_checksum_setup_ipv4(struct sk_buff *skb, bool recalculate)\n{\n\tunsigned int off;\n\tbool fragment;\n\t__sum16 *csum;\n\tint err;\n\n\tfragment = false;\n\n\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t  sizeof(struct iphdr),\n\t\t\t\t  MAX_IP_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tif (ip_hdr(skb)->frag_off & htons(IP_OFFSET | IP_MF))\n\t\tfragment = true;\n\n\toff = ip_hdrlen(skb);\n\n\terr = -EPROTO;\n\n\tif (fragment)\n\t\tgoto out;\n\n\tcsum = skb_checksum_setup_ip(skb, ip_hdr(skb)->protocol, off);\n\tif (IS_ERR(csum))\n\t\treturn PTR_ERR(csum);\n\n\tif (recalculate)\n\t\t*csum = ~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t   skb->len - off,\n\t\t\t\t\t   ip_hdr(skb)->protocol, 0);\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * an IPv6 header, all options, and a maximal TCP or UDP header.\n */\n#define MAX_IPV6_HDR_LEN 256\n\n#define OPT_HDR(type, skb, off) \\\n\t(type *)(skb_network_header(skb) + (off))\n\nstatic int skb_checksum_setup_ipv6(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\tu8 nexthdr;\n\tunsigned int off;\n\tunsigned int len;\n\tbool fragment;\n\tbool done;\n\t__sum16 *csum;\n\n\tfragment = false;\n\tdone = false;\n\n\toff = sizeof(struct ipv6hdr);\n\n\terr = skb_maybe_pull_tail(skb, off, MAX_IPV6_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tnexthdr = ipv6_hdr(skb)->nexthdr;\n\n\tlen = sizeof(struct ipv6hdr) + ntohs(ipv6_hdr(skb)->payload_len);\n\twhile (off <= len && !done) {\n\t\tswitch (nexthdr) {\n\t\tcase IPPROTO_DSTOPTS:\n\t\tcase IPPROTO_HOPOPTS:\n\t\tcase IPPROTO_ROUTING: {\n\t\t\tstruct ipv6_opt_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ipv6_opt_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ipv6_opt_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_optlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_AH: {\n\t\t\tstruct ip_auth_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ip_auth_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ip_auth_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_authlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_FRAGMENT: {\n\t\t\tstruct frag_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct frag_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct frag_hdr, skb, off);\n\n\t\t\tif (hp->frag_off & htons(IP6_OFFSET | IP6_MF))\n\t\t\t\tfragment = true;\n\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += sizeof(struct frag_hdr);\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tdone = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\terr = -EPROTO;\n\n\tif (!done || fragment)\n\t\tgoto out;\n\n\tcsum = skb_checksum_setup_ip(skb, nexthdr, off);\n\tif (IS_ERR(csum))\n\t\treturn PTR_ERR(csum);\n\n\tif (recalculate)\n\t\t*csum = ~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t skb->len - off, nexthdr, 0);\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/**\n * skb_checksum_setup - set up partial checksum offset\n * @skb: the skb to set up\n * @recalculate: if true the pseudo-header checksum will be recalculated\n */\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\terr = skb_checksum_setup_ipv4(skb, recalculate);\n\t\tbreak;\n\n\tcase htons(ETH_P_IPV6):\n\t\terr = skb_checksum_setup_ipv6(skb, recalculate);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EPROTO;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(skb_checksum_setup);\n\n/**\n * skb_checksum_maybe_trim - maybe trims the given skb\n * @skb: the skb to check\n * @transport_len: the data length beyond the network header\n *\n * Checks whether the given skb has data beyond the given transport length.\n * If so, returns a cloned skb trimmed to this transport length.\n * Otherwise returns the provided skb. Returns NULL in error cases\n * (e.g. transport_len exceeds skb length or out-of-memory).\n *\n * Caller needs to set the skb transport header and free any returned skb if it\n * differs from the provided skb.\n */\nstatic struct sk_buff *skb_checksum_maybe_trim(struct sk_buff *skb,\n\t\t\t\t\t       unsigned int transport_len)\n{\n\tstruct sk_buff *skb_chk;\n\tunsigned int len = skb_transport_offset(skb) + transport_len;\n\tint ret;\n\n\tif (skb->len < len)\n\t\treturn NULL;\n\telse if (skb->len == len)\n\t\treturn skb;\n\n\tskb_chk = skb_clone(skb, GFP_ATOMIC);\n\tif (!skb_chk)\n\t\treturn NULL;\n\n\tret = pskb_trim_rcsum(skb_chk, len);\n\tif (ret) {\n\t\tkfree_skb(skb_chk);\n\t\treturn NULL;\n\t}\n\n\treturn skb_chk;\n}\n\n/**\n * skb_checksum_trimmed - validate checksum of an skb\n * @skb: the skb to check\n * @transport_len: the data length beyond the network header\n * @skb_chkf: checksum function to use\n *\n * Applies the given checksum function skb_chkf to the provided skb.\n * Returns a checked and maybe trimmed skb. Returns NULL on error.\n *\n * If the skb has data beyond the given transport length, then a\n * trimmed & cloned skb is checked and returned.\n *\n * Caller needs to set the skb transport header and free any returned skb if it\n * differs from the provided skb.\n */\nstruct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,\n\t\t\t\t     unsigned int transport_len,\n\t\t\t\t     __sum16(*skb_chkf)(struct sk_buff *skb))\n{\n\tstruct sk_buff *skb_chk;\n\tunsigned int offset = skb_transport_offset(skb);\n\t__sum16 ret;\n\n\tskb_chk = skb_checksum_maybe_trim(skb, transport_len);\n\tif (!skb_chk)\n\t\tgoto err;\n\n\tif (!pskb_may_pull(skb_chk, offset))\n\t\tgoto err;\n\n\tskb_pull_rcsum(skb_chk, offset);\n\tret = skb_chkf(skb_chk);\n\tskb_push_rcsum(skb_chk, offset);\n\n\tif (ret)\n\t\tgoto err;\n\n\treturn skb_chk;\n\nerr:\n\tif (skb_chk && skb_chk != skb)\n\t\tkfree_skb(skb_chk);\n\n\treturn NULL;\n\n}\nEXPORT_SYMBOL(skb_checksum_trimmed);\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb)\n{\n\tnet_warn_ratelimited(\"%s: received packets cannot be forwarded while LRO is enabled\\n\",\n\t\t\t     skb->dev->name);\n}\nEXPORT_SYMBOL(__skb_warn_lro_forwarding);\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen)\n{\n\tif (head_stolen) {\n\t\tskb_release_head_state(skb);\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t} else {\n\t\t__kfree_skb(skb);\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_partial);\n\n/**\n * skb_try_coalesce - try to merge skb to prior one\n * @to: prior buffer\n * @from: buffer to add\n * @fragstolen: pointer to boolean\n * @delta_truesize: how much more was allocated than was requested\n */\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize)\n{\n\tint i, delta, len = from->len;\n\n\t*fragstolen = false;\n\n\tif (skb_cloned(to))\n\t\treturn false;\n\n\tif (len <= skb_tailroom(to)) {\n\t\tif (len)\n\t\t\tBUG_ON(skb_copy_bits(from, 0, skb_put(to, len), len));\n\t\t*delta_truesize = 0;\n\t\treturn true;\n\t}\n\n\tif (skb_has_frag_list(to) || skb_has_frag_list(from))\n\t\treturn false;\n\n\tif (skb_headlen(from) != 0) {\n\t\tstruct page *page;\n\t\tunsigned int offset;\n\n\t\tif (skb_shinfo(to)->nr_frags +\n\t\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tif (skb_head_is_locked(from))\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\t\tpage = virt_to_head_page(from->head);\n\t\toffset = from->data - (unsigned char *)page_address(page);\n\n\t\tskb_fill_page_desc(to, skb_shinfo(to)->nr_frags,\n\t\t\t\t   page, offset, skb_headlen(from));\n\t\t*fragstolen = true;\n\t} else {\n\t\tif (skb_shinfo(to)->nr_frags +\n\t\t    skb_shinfo(from)->nr_frags > MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_TRUESIZE(skb_end_offset(from));\n\t}\n\n\tWARN_ON_ONCE(delta < len);\n\n\tmemcpy(skb_shinfo(to)->frags + skb_shinfo(to)->nr_frags,\n\t       skb_shinfo(from)->frags,\n\t       skb_shinfo(from)->nr_frags * sizeof(skb_frag_t));\n\tskb_shinfo(to)->nr_frags += skb_shinfo(from)->nr_frags;\n\n\tif (!skb_cloned(from))\n\t\tskb_shinfo(from)->nr_frags = 0;\n\n\t/* if the skb is not cloned this does nothing\n\t * since we set nr_frags to 0.\n\t */\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++)\n\t\tskb_frag_ref(from, i);\n\n\tto->truesize += delta;\n\tto->len += len;\n\tto->data_len += len;\n\n\t*delta_truesize = delta;\n\treturn true;\n}\nEXPORT_SYMBOL(skb_try_coalesce);\n\n/**\n * skb_scrub_packet - scrub an skb\n *\n * @skb: buffer to clean\n * @xnet: packet is crossing netns\n *\n * skb_scrub_packet can be used after encapsulating or decapsulting a packet\n * into/from a tunnel. Some information have to be cleared during these\n * operations.\n * skb_scrub_packet can also be used to clean a skb before injecting it in\n * another namespace (@xnet == true). We have to clear all information in the\n * skb that could impact namespace isolation.\n */\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet)\n{\n\tskb->tstamp = 0;\n\tskb->pkt_type = PACKET_HOST;\n\tskb->skb_iif = 0;\n\tskb->ignore_df = 0;\n\tskb_dst_drop(skb);\n\tsecpath_reset(skb);\n\tnf_reset(skb);\n\tnf_reset_trace(skb);\n\n\tif (!xnet)\n\t\treturn;\n\n\tskb_orphan(skb);\n\tskb->mark = 0;\n}\nEXPORT_SYMBOL_GPL(skb_scrub_packet);\n\n/**\n * skb_gso_transport_seglen - Return length of individual segments of a gso packet\n *\n * @skb: GSO skb\n *\n * skb_gso_transport_seglen is used to determine the real size of the\n * individual segments, including Layer4 headers (TCP/UDP).\n *\n * The MAC/L2 or network (IP, IPv6) headers are not accounted for.\n */\nunsigned int skb_gso_transport_seglen(const struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tunsigned int thlen = 0;\n\n\tif (skb->encapsulation) {\n\t\tthlen = skb_inner_transport_header(skb) -\n\t\t\tskb_transport_header(skb);\n\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n\t\t\tthlen += inner_tcp_hdrlen(skb);\n\t} else if (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) {\n\t\tthlen = tcp_hdrlen(skb);\n\t} else if (unlikely(shinfo->gso_type & SKB_GSO_SCTP)) {\n\t\tthlen = sizeof(struct sctphdr);\n\t}\n\t/* UFO sets gso_size to the size of the fragmentation\n\t * payload, i.e. the size of the L4 (UDP) header is already\n\t * accounted for.\n\t */\n\treturn thlen + shinfo->gso_size;\n}\nEXPORT_SYMBOL_GPL(skb_gso_transport_seglen);\n\n/**\n * skb_gso_validate_mtu - Return in case such skb fits a given MTU\n *\n * @skb: GSO skb\n * @mtu: MTU to validate against\n *\n * skb_gso_validate_mtu validates if a given skb will fit a wanted MTU\n * once split.\n */\nbool skb_gso_validate_mtu(const struct sk_buff *skb, unsigned int mtu)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tconst struct sk_buff *iter;\n\tunsigned int hlen;\n\n\thlen = skb_gso_network_seglen(skb);\n\n\tif (shinfo->gso_size != GSO_BY_FRAGS)\n\t\treturn hlen <= mtu;\n\n\t/* Undo this so we can re-use header sizes */\n\thlen -= GSO_BY_FRAGS;\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (hlen + skb_headlen(iter) > mtu)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(skb_gso_validate_mtu);\n\nstatic struct sk_buff *skb_reorder_vlan_header(struct sk_buff *skb)\n{\n\tif (skb_cow(skb, skb_headroom(skb)) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\tmemmove(skb->data - ETH_HLEN, skb->data - skb->mac_len - VLAN_HLEN,\n\t\t2 * ETH_ALEN);\n\tskb->mac_header += VLAN_HLEN;\n\treturn skb;\n}\n\nstruct sk_buff *skb_vlan_untag(struct sk_buff *skb)\n{\n\tstruct vlan_hdr *vhdr;\n\tu16 vlan_tci;\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\t/* vlan_tci is already set-up so leave this for another time */\n\t\treturn skb;\n\t}\n\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\tgoto err_free;\n\n\tif (unlikely(!pskb_may_pull(skb, VLAN_HLEN)))\n\t\tgoto err_free;\n\n\tvhdr = (struct vlan_hdr *)skb->data;\n\tvlan_tci = ntohs(vhdr->h_vlan_TCI);\n\t__vlan_hwaccel_put_tag(skb, skb->protocol, vlan_tci);\n\n\tskb_pull_rcsum(skb, VLAN_HLEN);\n\tvlan_set_encap_proto(skb, vhdr);\n\n\tskb = skb_reorder_vlan_header(skb);\n\tif (unlikely(!skb))\n\t\tgoto err_free;\n\n\tskb_reset_network_header(skb);\n\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\treturn skb;\n\nerr_free:\n\tkfree_skb(skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(skb_vlan_untag);\n\nint skb_ensure_writable(struct sk_buff *skb, int write_len)\n{\n\tif (!pskb_may_pull(skb, write_len))\n\t\treturn -ENOMEM;\n\n\tif (!skb_cloned(skb) || skb_clone_writable(skb, write_len))\n\t\treturn 0;\n\n\treturn pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\nEXPORT_SYMBOL(skb_ensure_writable);\n\n/* remove VLAN header from packet and update csum accordingly.\n * expects a non skb_vlan_tag_present skb with a vlan tag payload\n */\nint __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci)\n{\n\tstruct vlan_hdr *vhdr;\n\tint offset = skb->data - skb_mac_header(skb);\n\tint err;\n\n\tif (WARN_ONCE(offset,\n\t\t      \"__skb_vlan_pop got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t      offset)) {\n\t\treturn -EINVAL;\n\t}\n\n\terr = skb_ensure_writable(skb, VLAN_ETH_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tskb_postpull_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\n\tvhdr = (struct vlan_hdr *)(skb->data + ETH_HLEN);\n\t*vlan_tci = ntohs(vhdr->h_vlan_TCI);\n\n\tmemmove(skb->data + VLAN_HLEN, skb->data, 2 * ETH_ALEN);\n\t__skb_pull(skb, VLAN_HLEN);\n\n\tvlan_set_encap_proto(skb, vhdr);\n\tskb->mac_header += VLAN_HLEN;\n\n\tif (skb_network_offset(skb) < ETH_HLEN)\n\t\tskb_set_network_header(skb, ETH_HLEN);\n\n\tskb_reset_mac_len(skb);\n\n\treturn err;\n}\nEXPORT_SYMBOL(__skb_vlan_pop);\n\n/* Pop a vlan tag either from hwaccel or from payload.\n * Expects skb->data at mac header.\n */\nint skb_vlan_pop(struct sk_buff *skb)\n{\n\tu16 vlan_tci;\n\t__be16 vlan_proto;\n\tint err;\n\n\tif (likely(skb_vlan_tag_present(skb))) {\n\t\tskb->vlan_tci = 0;\n\t} else {\n\t\tif (unlikely(!eth_type_vlan(skb->protocol)))\n\t\t\treturn 0;\n\n\t\terr = __skb_vlan_pop(skb, &vlan_tci);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\t/* move next vlan tag to hw accel tag */\n\tif (likely(!eth_type_vlan(skb->protocol)))\n\t\treturn 0;\n\n\tvlan_proto = skb->protocol;\n\terr = __skb_vlan_pop(skb, &vlan_tci);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_vlan_pop);\n\n/* Push a vlan tag either into hwaccel or into payload (if hwaccel tag present).\n * Expects skb->data at mac header.\n */\nint skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci)\n{\n\tif (skb_vlan_tag_present(skb)) {\n\t\tint offset = skb->data - skb_mac_header(skb);\n\t\tint err;\n\n\t\tif (WARN_ONCE(offset,\n\t\t\t      \"skb_vlan_push got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t\t      offset)) {\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\terr = __vlan_insert_tag(skb, skb->vlan_proto,\n\t\t\t\t\tskb_vlan_tag_get(skb));\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tskb->protocol = skb->vlan_proto;\n\t\tskb->mac_len += VLAN_HLEN;\n\n\t\tskb_postpush_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\t}\n\t__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_vlan_push);\n\n/**\n * alloc_skb_with_frags - allocate skb with page frags\n *\n * @header_len: size of linear part\n * @data_len: needed length in frags\n * @max_page_order: max page order desired.\n * @errcode: pointer to error code if any\n * @gfp_mask: allocation mask\n *\n * This can be used to allocate a paged skb, given a maximal order for frags.\n */\nstruct sk_buff *alloc_skb_with_frags(unsigned long header_len,\n\t\t\t\t     unsigned long data_len,\n\t\t\t\t     int max_page_order,\n\t\t\t\t     int *errcode,\n\t\t\t\t     gfp_t gfp_mask)\n{\n\tint npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\n\tunsigned long chunk;\n\tstruct sk_buff *skb;\n\tstruct page *page;\n\tgfp_t gfp_head;\n\tint i;\n\n\t*errcode = -EMSGSIZE;\n\t/* Note this test could be relaxed, if we succeed to allocate\n\t * high order pages...\n\t */\n\tif (npages > MAX_SKB_FRAGS)\n\t\treturn NULL;\n\n\tgfp_head = gfp_mask;\n\tif (gfp_head & __GFP_DIRECT_RECLAIM)\n\t\tgfp_head |= __GFP_REPEAT;\n\n\t*errcode = -ENOBUFS;\n\tskb = alloc_skb(header_len, gfp_head);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb->truesize += npages << PAGE_SHIFT;\n\n\tfor (i = 0; npages > 0; i++) {\n\t\tint order = max_page_order;\n\n\t\twhile (order) {\n\t\t\tif (npages >= 1 << order) {\n\t\t\t\tpage = alloc_pages((gfp_mask & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t\t\t\t   __GFP_COMP |\n\t\t\t\t\t\t   __GFP_NOWARN |\n\t\t\t\t\t\t   __GFP_NORETRY,\n\t\t\t\t\t\t   order);\n\t\t\t\tif (page)\n\t\t\t\t\tgoto fill_page;\n\t\t\t\t/* Do not retry other high order allocations */\n\t\t\t\torder = 1;\n\t\t\t\tmax_page_order = 0;\n\t\t\t}\n\t\t\torder--;\n\t\t}\n\t\tpage = alloc_page(gfp_mask);\n\t\tif (!page)\n\t\t\tgoto failure;\nfill_page:\n\t\tchunk = min_t(unsigned long, data_len,\n\t\t\t      PAGE_SIZE << order);\n\t\tskb_fill_page_desc(skb, i, page, 0, chunk);\n\t\tdata_len -= chunk;\n\t\tnpages -= 1 << order;\n\t}\n\treturn skb;\n\nfailure:\n\tkfree_skb(skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_skb_with_frags);\n\n/* carve out the first off bytes from skb when off < headlen */\nstatic int pskb_carve_inside_header(struct sk_buff *skb, const u32 off,\n\t\t\t\t    const int headlen, gfp_t gfp_mask)\n{\n\tint i;\n\tint size = skb_end_offset(skb);\n\tint new_hlen = headlen - off;\n\tu8 *data;\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size +\n\t\t\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\t/* Copy real data, and all frags */\n\tskb_copy_from_linear_data_offset(skb, off, data, new_hlen);\n\tskb->len -= off;\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info,\n\t\t\tfrags[skb_shinfo(skb)->nr_frags]));\n\tif (skb_cloned(skb)) {\n\t\t/* drop the old head gracefully */\n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tkfree(data);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\t\tskb_release_data(skb);\n\t} else {\n\t\t/* we can reuse existing recount- all we did was\n\t\t * relocate values\n\t\t */\n\t\tskb_free_head(skb);\n\t}\n\n\tskb->head = data;\n\tskb->data = data;\n\tskb->head_frag = 0;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end = size;\n#else\n\tskb->end = skb->head + size;\n#endif\n\tskb_set_tail_pointer(skb, skb_headlen(skb));\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned = 0;\n\tskb->hdr_len = 0;\n\tskb->nohdr = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\n\treturn 0;\n}\n\nstatic int pskb_carve(struct sk_buff *skb, const u32 off, gfp_t gfp);\n\n/* carve out the first eat bytes from skb's frag_list. May recurse into\n * pskb_carve()\n */\nstatic int pskb_carve_frag_list(struct sk_buff *skb,\n\t\t\t\tstruct skb_shared_info *shinfo, int eat,\n\t\t\t\tgfp_t gfp_mask)\n{\n\tstruct sk_buff *list = shinfo->frag_list;\n\tstruct sk_buff *clone = NULL;\n\tstruct sk_buff *insp = NULL;\n\n\tdo {\n\t\tif (!list) {\n\t\t\tpr_err(\"Not enough bytes to eat. Want %d\\n\", eat);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (list->len <= eat) {\n\t\t\t/* Eaten as whole. */\n\t\t\teat -= list->len;\n\t\t\tlist = list->next;\n\t\t\tinsp = list;\n\t\t} else {\n\t\t\t/* Eaten partially. */\n\t\t\tif (skb_shared(list)) {\n\t\t\t\tclone = skb_clone(list, gfp_mask);\n\t\t\t\tif (!clone)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tinsp = list->next;\n\t\t\t\tlist = clone;\n\t\t\t} else {\n\t\t\t\t/* This may be pulled without problems. */\n\t\t\t\tinsp = list;\n\t\t\t}\n\t\t\tif (pskb_carve(list, eat, gfp_mask) < 0) {\n\t\t\t\tkfree_skb(clone);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t} while (eat);\n\n\t/* Free pulled out fragments. */\n\twhile ((list = shinfo->frag_list) != insp) {\n\t\tshinfo->frag_list = list->next;\n\t\tkfree_skb(list);\n\t}\n\t/* And insert new clone at head. */\n\tif (clone) {\n\t\tclone->next = list;\n\t\tshinfo->frag_list = clone;\n\t}\n\treturn 0;\n}\n\n/* carve off first len bytes from skb. Split line (off) is in the\n * non-linear part of skb\n */\nstatic int pskb_carve_inside_nonlinear(struct sk_buff *skb, const u32 off,\n\t\t\t\t       int pos, gfp_t gfp_mask)\n{\n\tint i, k = 0;\n\tint size = skb_end_offset(skb);\n\tu8 *data;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\tstruct skb_shared_info *shinfo;\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size +\n\t\t\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb), offsetof(struct skb_shared_info,\n\t\t\t\t\t frags[skb_shinfo(skb)->nr_frags]));\n\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\tshinfo = (struct skb_shared_info *)(data + size);\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint fsize = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + fsize > off) {\n\t\t\tshinfo->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < off) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tshinfo->frags[0].page_offset += off - pos;\n\t\t\t\tskb_frag_size_sub(&shinfo->frags[0], off - pos);\n\t\t\t}\n\t\t\tskb_frag_ref(skb, i);\n\t\t\tk++;\n\t\t}\n\t\tpos += fsize;\n\t}\n\tshinfo->nr_frags = k;\n\tif (skb_has_frag_list(skb))\n\t\tskb_clone_fraglist(skb);\n\n\tif (k == 0) {\n\t\t/* split line is in frag list */\n\t\tpskb_carve_frag_list(skb, shinfo, off - pos, gfp_mask);\n\t}\n\tskb_release_data(skb);\n\n\tskb->head = data;\n\tskb->head_frag = 0;\n\tskb->data = data;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end = size;\n#else\n\tskb->end = skb->head + size;\n#endif\n\tskb_reset_tail_pointer(skb);\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tskb->len -= off;\n\tskb->data_len = skb->len;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\treturn 0;\n}\n\n/* remove len bytes from the beginning of the skb */\nstatic int pskb_carve(struct sk_buff *skb, const u32 len, gfp_t gfp)\n{\n\tint headlen = skb_headlen(skb);\n\n\tif (len < headlen)\n\t\treturn pskb_carve_inside_header(skb, len, headlen, gfp);\n\telse\n\t\treturn pskb_carve_inside_nonlinear(skb, len, headlen, gfp);\n}\n\n/* Extract to_copy bytes starting at off from skb, and return this in\n * a new skb\n */\nstruct sk_buff *pskb_extract(struct sk_buff *skb, int off,\n\t\t\t     int to_copy, gfp_t gfp)\n{\n\tstruct sk_buff  *clone = skb_clone(skb, gfp);\n\n\tif (!clone)\n\t\treturn NULL;\n\n\tif (pskb_carve(clone, off, gfp) < 0 ||\n\t    pskb_trim(clone, to_copy)) {\n\t\tkfree_skb(clone);\n\t\treturn NULL;\n\t}\n\treturn clone;\n}\nEXPORT_SYMBOL(pskb_extract);\n\n/**\n * skb_condense - try to get rid of fragments/frag_list if possible\n * @skb: buffer\n *\n * Can be used to save memory before skb is added to a busy queue.\n * If packet has bytes in frags and enough tail room in skb->head,\n * pull all of them, so that we can free the frags right now and adjust\n * truesize.\n * Notes:\n *\tWe do not reallocate skb->head thus can not fail.\n *\tCaller must re-evaluate skb->truesize if needed.\n */\nvoid skb_condense(struct sk_buff *skb)\n{\n\tif (skb->data_len) {\n\t\tif (skb->data_len > skb->end - skb->tail ||\n\t\t    skb_cloned(skb))\n\t\t\treturn;\n\n\t\t/* Nice, we can free page frag(s) right now */\n\t\t__pskb_pull_tail(skb, skb->data_len);\n\t}\n\t/* At this point, skb->truesize might be over estimated,\n\t * because skb had a fragment, and fragments do not tell\n\t * their truesize.\n\t * When we pulled its content into skb->head, fragment\n\t * was freed, but __pskb_pull_tail() could not possibly\n\t * adjust skb->truesize, not knowing the frag truesize.\n\t */\n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n}\n", "/*\n * NET\t\tAn implementation of the SOCKET network access protocol.\n *\n * Version:\t@(#)socket.c\t1.1.93\t18/02/95\n *\n * Authors:\tOrest Zborowski, <obz@Kodak.COM>\n *\t\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\n * Fixes:\n *\t\tAnonymous\t:\tNOTSOCK/BADF cleanup. Error fix in\n *\t\t\t\t\tshutdown()\n *\t\tAlan Cox\t:\tverify_area() fixes\n *\t\tAlan Cox\t:\tRemoved DDI\n *\t\tJonathan Kamens\t:\tSOCK_DGRAM reconnect bug\n *\t\tAlan Cox\t:\tMoved a load of checks to the very\n *\t\t\t\t\ttop level.\n *\t\tAlan Cox\t:\tMove address structures to/from user\n *\t\t\t\t\tmode above the protocol layers.\n *\t\tRob Janssen\t:\tAllow 0 length sends.\n *\t\tAlan Cox\t:\tAsynchronous I/O support (cribbed from the\n *\t\t\t\t\ttty drivers).\n *\t\tNiibe Yutaka\t:\tAsynchronous I/O for writes (4.4BSD style)\n *\t\tJeff Uphoff\t:\tMade max number of sockets command-line\n *\t\t\t\t\tconfigurable.\n *\t\tMatti Aarnio\t:\tMade the number of sockets dynamic,\n *\t\t\t\t\tto be allocated when needed, and mr.\n *\t\t\t\t\tUphoff's max is used as max to be\n *\t\t\t\t\tallowed to allocate.\n *\t\tLinus\t\t:\tArgh. removed all the socket allocation\n *\t\t\t\t\taltogether: it's in the inode now.\n *\t\tAlan Cox\t:\tMade sock_alloc()/sock_release() public\n *\t\t\t\t\tfor NetROM and future kernel nfsd type\n *\t\t\t\t\tstuff.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg basics.\n *\t\tTom Dyas\t:\tExport net symbols.\n *\t\tMarcin Dalecki\t:\tFixed problems with CONFIG_NET=\"n\".\n *\t\tAlan Cox\t:\tAdded thread locking to sys_* calls\n *\t\t\t\t\tfor sockets. May have errors at the\n *\t\t\t\t\tmoment.\n *\t\tKevin Buhr\t:\tFixed the dumb errors in the above.\n *\t\tAndi Kleen\t:\tSome small cleanups, optimizations,\n *\t\t\t\t\tand fixed a copy_from_user() bug.\n *\t\tTigran Aivazian\t:\tsys_send(args) calls sys_sendto(args, NULL, 0)\n *\t\tTigran Aivazian\t:\tMade listen(2) backlog sanity checks\n *\t\t\t\t\tprotocol-independent\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\n *\tThis module is effectively the top level interface to the BSD socket\n *\tparadigm.\n *\n *\tBased upon Swansea University Computer Society NET3.039\n */\n\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/file.h>\n#include <linux/net.h>\n#include <linux/interrupt.h>\n#include <linux/thread_info.h>\n#include <linux/rcupdate.h>\n#include <linux/netdevice.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/mutex.h>\n#include <linux/if_bridge.h>\n#include <linux/if_frad.h>\n#include <linux/if_vlan.h>\n#include <linux/ptp_classify.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/cache.h>\n#include <linux/module.h>\n#include <linux/highmem.h>\n#include <linux/mount.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/compat.h>\n#include <linux/kmod.h>\n#include <linux/audit.h>\n#include <linux/wireless.h>\n#include <linux/nsproxy.h>\n#include <linux/magic.h>\n#include <linux/slab.h>\n#include <linux/xattr.h>\n\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n\n#include <net/compat.h>\n#include <net/wext.h>\n#include <net/cls_cgroup.h>\n\n#include <net/sock.h>\n#include <linux/netfilter.h>\n\n#include <linux/if_tun.h>\n#include <linux/ipv6_route.h>\n#include <linux/route.h>\n#include <linux/sockios.h>\n#include <linux/atalk.h>\n#include <net/busy_poll.h>\n#include <linux/errqueue.h>\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\nunsigned int sysctl_net_busy_read __read_mostly;\nunsigned int sysctl_net_busy_poll __read_mostly;\n#endif\n\nstatic ssize_t sock_read_iter(struct kiocb *iocb, struct iov_iter *to);\nstatic ssize_t sock_write_iter(struct kiocb *iocb, struct iov_iter *from);\nstatic int sock_mmap(struct file *file, struct vm_area_struct *vma);\n\nstatic int sock_close(struct inode *inode, struct file *file);\nstatic unsigned int sock_poll(struct file *file,\n\t\t\t      struct poll_table_struct *wait);\nstatic long sock_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\n#ifdef CONFIG_COMPAT\nstatic long compat_sock_ioctl(struct file *file,\n\t\t\t      unsigned int cmd, unsigned long arg);\n#endif\nstatic int sock_fasync(int fd, struct file *filp, int on);\nstatic ssize_t sock_sendpage(struct file *file, struct page *page,\n\t\t\t     int offset, size_t size, loff_t *ppos, int more);\nstatic ssize_t sock_splice_read(struct file *file, loff_t *ppos,\n\t\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\t\tunsigned int flags);\n\n/*\n *\tSocket files have a set of 'special' operations as well as the generic file ones. These don't appear\n *\tin the operation structures but are done directly via the socketcall() multiplexor.\n */\n\nstatic const struct file_operations socket_file_ops = {\n\t.owner =\tTHIS_MODULE,\n\t.llseek =\tno_llseek,\n\t.read_iter =\tsock_read_iter,\n\t.write_iter =\tsock_write_iter,\n\t.poll =\t\tsock_poll,\n\t.unlocked_ioctl = sock_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = compat_sock_ioctl,\n#endif\n\t.mmap =\t\tsock_mmap,\n\t.release =\tsock_close,\n\t.fasync =\tsock_fasync,\n\t.sendpage =\tsock_sendpage,\n\t.splice_write = generic_splice_sendpage,\n\t.splice_read =\tsock_splice_read,\n};\n\n/*\n *\tThe protocol list. Each protocol is registered in here.\n */\n\nstatic DEFINE_SPINLOCK(net_family_lock);\nstatic const struct net_proto_family __rcu *net_families[NPROTO] __read_mostly;\n\n/*\n *\tStatistics counters of the socket lists\n */\n\nstatic DEFINE_PER_CPU(int, sockets_in_use);\n\n/*\n * Support routines.\n * Move socket addresses back and forth across the kernel/user\n * divide and look after the messy bits.\n */\n\n/**\n *\tmove_addr_to_kernel\t-\tcopy a socket address into kernel space\n *\t@uaddr: Address in user space\n *\t@kaddr: Address in kernel space\n *\t@ulen: Length in user space\n *\n *\tThe address is copied into kernel space. If the provided address is\n *\ttoo long an error code of -EINVAL is returned. If the copy gives\n *\tinvalid addresses -EFAULT is returned. On a success 0 is returned.\n */\n\nint move_addr_to_kernel(void __user *uaddr, int ulen, struct sockaddr_storage *kaddr)\n{\n\tif (ulen < 0 || ulen > sizeof(struct sockaddr_storage))\n\t\treturn -EINVAL;\n\tif (ulen == 0)\n\t\treturn 0;\n\tif (copy_from_user(kaddr, uaddr, ulen))\n\t\treturn -EFAULT;\n\treturn audit_sockaddr(ulen, kaddr);\n}\n\n/**\n *\tmove_addr_to_user\t-\tcopy an address to user space\n *\t@kaddr: kernel space address\n *\t@klen: length of address in kernel\n *\t@uaddr: user space address\n *\t@ulen: pointer to user length field\n *\n *\tThe value pointed to by ulen on entry is the buffer length available.\n *\tThis is overwritten with the buffer space used. -EINVAL is returned\n *\tif an overlong buffer is specified or a negative buffer size. -EFAULT\n *\tis returned if either the buffer or the length field are not\n *\taccessible.\n *\tAfter copying the data up to the limit the user specifies, the true\n *\tlength of the data is written over the length limit the user\n *\tspecified. Zero is returned for a success.\n */\n\nstatic int move_addr_to_user(struct sockaddr_storage *kaddr, int klen,\n\t\t\t     void __user *uaddr, int __user *ulen)\n{\n\tint err;\n\tint len;\n\n\tBUG_ON(klen > sizeof(struct sockaddr_storage));\n\terr = get_user(len, ulen);\n\tif (err)\n\t\treturn err;\n\tif (len > klen)\n\t\tlen = klen;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\tif (len) {\n\t\tif (audit_sockaddr(klen, kaddr))\n\t\t\treturn -ENOMEM;\n\t\tif (copy_to_user(uaddr, kaddr, len))\n\t\t\treturn -EFAULT;\n\t}\n\t/*\n\t *      \"fromlen shall refer to the value before truncation..\"\n\t *                      1003.1g\n\t */\n\treturn __put_user(klen, ulen);\n}\n\nstatic struct kmem_cache *sock_inode_cachep __read_mostly;\n\nstatic struct inode *sock_alloc_inode(struct super_block *sb)\n{\n\tstruct socket_alloc *ei;\n\tstruct socket_wq *wq;\n\n\tei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL);\n\tif (!ei)\n\t\treturn NULL;\n\twq = kmalloc(sizeof(*wq), GFP_KERNEL);\n\tif (!wq) {\n\t\tkmem_cache_free(sock_inode_cachep, ei);\n\t\treturn NULL;\n\t}\n\tinit_waitqueue_head(&wq->wait);\n\twq->fasync_list = NULL;\n\twq->flags = 0;\n\tRCU_INIT_POINTER(ei->socket.wq, wq);\n\n\tei->socket.state = SS_UNCONNECTED;\n\tei->socket.flags = 0;\n\tei->socket.ops = NULL;\n\tei->socket.sk = NULL;\n\tei->socket.file = NULL;\n\n\treturn &ei->vfs_inode;\n}\n\nstatic void sock_destroy_inode(struct inode *inode)\n{\n\tstruct socket_alloc *ei;\n\tstruct socket_wq *wq;\n\n\tei = container_of(inode, struct socket_alloc, vfs_inode);\n\twq = rcu_dereference_protected(ei->socket.wq, 1);\n\tkfree_rcu(wq, rcu);\n\tkmem_cache_free(sock_inode_cachep, ei);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct socket_alloc *ei = (struct socket_alloc *)foo;\n\n\tinode_init_once(&ei->vfs_inode);\n}\n\nstatic void init_inodecache(void)\n{\n\tsock_inode_cachep = kmem_cache_create(\"sock_inode_cache\",\n\t\t\t\t\t      sizeof(struct socket_alloc),\n\t\t\t\t\t      0,\n\t\t\t\t\t      (SLAB_HWCACHE_ALIGN |\n\t\t\t\t\t       SLAB_RECLAIM_ACCOUNT |\n\t\t\t\t\t       SLAB_MEM_SPREAD | SLAB_ACCOUNT),\n\t\t\t\t\t      init_once);\n\tBUG_ON(sock_inode_cachep == NULL);\n}\n\nstatic const struct super_operations sockfs_ops = {\n\t.alloc_inode\t= sock_alloc_inode,\n\t.destroy_inode\t= sock_destroy_inode,\n\t.statfs\t\t= simple_statfs,\n};\n\n/*\n * sockfs_dname() is called from d_path().\n */\nstatic char *sockfs_dname(struct dentry *dentry, char *buffer, int buflen)\n{\n\treturn dynamic_dname(dentry, buffer, buflen, \"socket:[%lu]\",\n\t\t\t\td_inode(dentry)->i_ino);\n}\n\nstatic const struct dentry_operations sockfs_dentry_operations = {\n\t.d_dname  = sockfs_dname,\n};\n\nstatic int sockfs_xattr_get(const struct xattr_handler *handler,\n\t\t\t    struct dentry *dentry, struct inode *inode,\n\t\t\t    const char *suffix, void *value, size_t size)\n{\n\tif (value) {\n\t\tif (dentry->d_name.len + 1 > size)\n\t\t\treturn -ERANGE;\n\t\tmemcpy(value, dentry->d_name.name, dentry->d_name.len + 1);\n\t}\n\treturn dentry->d_name.len + 1;\n}\n\n#define XATTR_SOCKPROTONAME_SUFFIX \"sockprotoname\"\n#define XATTR_NAME_SOCKPROTONAME (XATTR_SYSTEM_PREFIX XATTR_SOCKPROTONAME_SUFFIX)\n#define XATTR_NAME_SOCKPROTONAME_LEN (sizeof(XATTR_NAME_SOCKPROTONAME)-1)\n\nstatic const struct xattr_handler sockfs_xattr_handler = {\n\t.name = XATTR_NAME_SOCKPROTONAME,\n\t.get = sockfs_xattr_get,\n};\n\nstatic int sockfs_security_xattr_set(const struct xattr_handler *handler,\n\t\t\t\t     struct dentry *dentry, struct inode *inode,\n\t\t\t\t     const char *suffix, const void *value,\n\t\t\t\t     size_t size, int flags)\n{\n\t/* Handled by LSM. */\n\treturn -EAGAIN;\n}\n\nstatic const struct xattr_handler sockfs_security_xattr_handler = {\n\t.prefix = XATTR_SECURITY_PREFIX,\n\t.set = sockfs_security_xattr_set,\n};\n\nstatic const struct xattr_handler *sockfs_xattr_handlers[] = {\n\t&sockfs_xattr_handler,\n\t&sockfs_security_xattr_handler,\n\tNULL\n};\n\nstatic struct dentry *sockfs_mount(struct file_system_type *fs_type,\n\t\t\t int flags, const char *dev_name, void *data)\n{\n\treturn mount_pseudo_xattr(fs_type, \"socket:\", &sockfs_ops,\n\t\t\t\t  sockfs_xattr_handlers,\n\t\t\t\t  &sockfs_dentry_operations, SOCKFS_MAGIC);\n}\n\nstatic struct vfsmount *sock_mnt __read_mostly;\n\nstatic struct file_system_type sock_fs_type = {\n\t.name =\t\t\"sockfs\",\n\t.mount =\tsockfs_mount,\n\t.kill_sb =\tkill_anon_super,\n};\n\n/*\n *\tObtains the first available file descriptor and sets it up for use.\n *\n *\tThese functions create file structures and maps them to fd space\n *\tof the current process. On success it returns file descriptor\n *\tand file struct implicitly stored in sock->file.\n *\tNote that another thread may close file descriptor before we return\n *\tfrom this function. We use the fact that now we do not refer\n *\tto socket after mapping. If one day we will need it, this\n *\tfunction will increment ref. count on file by 1.\n *\n *\tIn any case returned fd MAY BE not valid!\n *\tThis race condition is unavoidable\n *\twith shared fd spaces, we cannot solve it inside kernel,\n *\tbut we take care of internal coherence yet.\n */\n\nstruct file *sock_alloc_file(struct socket *sock, int flags, const char *dname)\n{\n\tstruct qstr name = { .name = \"\" };\n\tstruct path path;\n\tstruct file *file;\n\n\tif (dname) {\n\t\tname.name = dname;\n\t\tname.len = strlen(name.name);\n\t} else if (sock->sk) {\n\t\tname.name = sock->sk->sk_prot_creator->name;\n\t\tname.len = strlen(name.name);\n\t}\n\tpath.dentry = d_alloc_pseudo(sock_mnt->mnt_sb, &name);\n\tif (unlikely(!path.dentry))\n\t\treturn ERR_PTR(-ENOMEM);\n\tpath.mnt = mntget(sock_mnt);\n\n\td_instantiate(path.dentry, SOCK_INODE(sock));\n\n\tfile = alloc_file(&path, FMODE_READ | FMODE_WRITE,\n\t\t  &socket_file_ops);\n\tif (IS_ERR(file)) {\n\t\t/* drop dentry, keep inode */\n\t\tihold(d_inode(path.dentry));\n\t\tpath_put(&path);\n\t\treturn file;\n\t}\n\n\tsock->file = file;\n\tfile->f_flags = O_RDWR | (flags & O_NONBLOCK);\n\tfile->private_data = sock;\n\treturn file;\n}\nEXPORT_SYMBOL(sock_alloc_file);\n\nstatic int sock_map_fd(struct socket *sock, int flags)\n{\n\tstruct file *newfile;\n\tint fd = get_unused_fd_flags(flags);\n\tif (unlikely(fd < 0))\n\t\treturn fd;\n\n\tnewfile = sock_alloc_file(sock, flags, NULL);\n\tif (likely(!IS_ERR(newfile))) {\n\t\tfd_install(fd, newfile);\n\t\treturn fd;\n\t}\n\n\tput_unused_fd(fd);\n\treturn PTR_ERR(newfile);\n}\n\nstruct socket *sock_from_file(struct file *file, int *err)\n{\n\tif (file->f_op == &socket_file_ops)\n\t\treturn file->private_data;\t/* set in sock_map_fd */\n\n\t*err = -ENOTSOCK;\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_from_file);\n\n/**\n *\tsockfd_lookup - Go from a file number to its socket slot\n *\t@fd: file handle\n *\t@err: pointer to an error code return\n *\n *\tThe file handle passed in is locked and the socket it is bound\n *\ttoo is returned. If an error occurs the err pointer is overwritten\n *\twith a negative errno code and NULL is returned. The function checks\n *\tfor both invalid handles and passing a handle which is not a socket.\n *\n *\tOn a success the socket object pointer is returned.\n */\n\nstruct socket *sockfd_lookup(int fd, int *err)\n{\n\tstruct file *file;\n\tstruct socket *sock;\n\n\tfile = fget(fd);\n\tif (!file) {\n\t\t*err = -EBADF;\n\t\treturn NULL;\n\t}\n\n\tsock = sock_from_file(file, err);\n\tif (!sock)\n\t\tfput(file);\n\treturn sock;\n}\nEXPORT_SYMBOL(sockfd_lookup);\n\nstatic struct socket *sockfd_lookup_light(int fd, int *err, int *fput_needed)\n{\n\tstruct fd f = fdget(fd);\n\tstruct socket *sock;\n\n\t*err = -EBADF;\n\tif (f.file) {\n\t\tsock = sock_from_file(f.file, err);\n\t\tif (likely(sock)) {\n\t\t\t*fput_needed = f.flags;\n\t\t\treturn sock;\n\t\t}\n\t\tfdput(f);\n\t}\n\treturn NULL;\n}\n\nstatic ssize_t sockfs_listxattr(struct dentry *dentry, char *buffer,\n\t\t\t\tsize_t size)\n{\n\tssize_t len;\n\tssize_t used = 0;\n\n\tlen = security_inode_listsecurity(d_inode(dentry), buffer, size);\n\tif (len < 0)\n\t\treturn len;\n\tused += len;\n\tif (buffer) {\n\t\tif (size < used)\n\t\t\treturn -ERANGE;\n\t\tbuffer += len;\n\t}\n\n\tlen = (XATTR_NAME_SOCKPROTONAME_LEN + 1);\n\tused += len;\n\tif (buffer) {\n\t\tif (size < used)\n\t\t\treturn -ERANGE;\n\t\tmemcpy(buffer, XATTR_NAME_SOCKPROTONAME, len);\n\t\tbuffer += len;\n\t}\n\n\treturn used;\n}\n\nstatic int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n{\n\tint err = simple_setattr(dentry, iattr);\n\n\tif (!err && (iattr->ia_valid & ATTR_UID)) {\n\t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n\n\t\tsock->sk->sk_uid = iattr->ia_uid;\n\t}\n\n\treturn err;\n}\n\nstatic const struct inode_operations sockfs_inode_ops = {\n\t.listxattr = sockfs_listxattr,\n\t.setattr = sockfs_setattr,\n};\n\n/**\n *\tsock_alloc\t-\tallocate a socket\n *\n *\tAllocate a new inode and socket object. The two are bound together\n *\tand initialised. The socket is then returned. If we are out of inodes\n *\tNULL is returned.\n */\n\nstruct socket *sock_alloc(void)\n{\n\tstruct inode *inode;\n\tstruct socket *sock;\n\n\tinode = new_inode_pseudo(sock_mnt->mnt_sb);\n\tif (!inode)\n\t\treturn NULL;\n\n\tsock = SOCKET_I(inode);\n\n\tkmemcheck_annotate_bitfield(sock, type);\n\tinode->i_ino = get_next_ino();\n\tinode->i_mode = S_IFSOCK | S_IRWXUGO;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_op = &sockfs_inode_ops;\n\n\tthis_cpu_add(sockets_in_use, 1);\n\treturn sock;\n}\nEXPORT_SYMBOL(sock_alloc);\n\n/**\n *\tsock_release\t-\tclose a socket\n *\t@sock: socket to close\n *\n *\tThe socket is released from the protocol stack if it has a release\n *\tcallback, and the inode is then released if the socket is bound to\n *\tan inode not a file.\n */\n\nvoid sock_release(struct socket *sock)\n{\n\tif (sock->ops) {\n\t\tstruct module *owner = sock->ops->owner;\n\n\t\tsock->ops->release(sock);\n\t\tsock->ops = NULL;\n\t\tmodule_put(owner);\n\t}\n\n\tif (rcu_dereference_protected(sock->wq, 1)->fasync_list)\n\t\tpr_err(\"%s: fasync list not empty!\\n\", __func__);\n\n\tthis_cpu_sub(sockets_in_use, 1);\n\tif (!sock->file) {\n\t\tiput(SOCK_INODE(sock));\n\t\treturn;\n\t}\n\tsock->file = NULL;\n}\nEXPORT_SYMBOL(sock_release);\n\nvoid __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)\n{\n\tu8 flags = *tx_flags;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_HARDWARE)\n\t\tflags |= SKBTX_HW_TSTAMP;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_SOFTWARE)\n\t\tflags |= SKBTX_SW_TSTAMP;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_SCHED)\n\t\tflags |= SKBTX_SCHED_TSTAMP;\n\n\t*tx_flags = flags;\n}\nEXPORT_SYMBOL(__sock_tx_timestamp);\n\nstatic inline int sock_sendmsg_nosec(struct socket *sock, struct msghdr *msg)\n{\n\tint ret = sock->ops->sendmsg(sock, msg, msg_data_left(msg));\n\tBUG_ON(ret == -EIOCBQUEUED);\n\treturn ret;\n}\n\nint sock_sendmsg(struct socket *sock, struct msghdr *msg)\n{\n\tint err = security_socket_sendmsg(sock, msg,\n\t\t\t\t\t  msg_data_left(msg));\n\n\treturn err ?: sock_sendmsg_nosec(sock, msg);\n}\nEXPORT_SYMBOL(sock_sendmsg);\n\nint kernel_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t   struct kvec *vec, size_t num, size_t size)\n{\n\tiov_iter_kvec(&msg->msg_iter, WRITE | ITER_KVEC, vec, num, size);\n\treturn sock_sendmsg(sock, msg);\n}\nEXPORT_SYMBOL(kernel_sendmsg);\n\nstatic bool skb_is_err_queue(const struct sk_buff *skb)\n{\n\t/* pkt_type of skbs enqueued on the error queue are set to\n\t * PACKET_OUTGOING in skb_set_err_queue(). This is only safe to do\n\t * in recvmsg, since skbs received on a local socket will never\n\t * have a pkt_type of PACKET_OUTGOING.\n\t */\n\treturn skb->pkt_type == PACKET_OUTGOING;\n}\n\n/*\n * called from sock_recv_timestamp() if sock_flag(sk, SOCK_RCVTSTAMP)\n */\nvoid __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,\n\tstruct sk_buff *skb)\n{\n\tint need_software_tstamp = sock_flag(sk, SOCK_RCVTSTAMP);\n\tstruct scm_timestamping tss;\n\tint empty = 1;\n\tstruct skb_shared_hwtstamps *shhwtstamps =\n\t\tskb_hwtstamps(skb);\n\n\t/* Race occurred between timestamp enabling and packet\n\t   receiving.  Fill in the current time for now. */\n\tif (need_software_tstamp && skb->tstamp == 0)\n\t\t__net_timestamp(skb);\n\n\tif (need_software_tstamp) {\n\t\tif (!sock_flag(sk, SOCK_RCVTSTAMPNS)) {\n\t\t\tstruct timeval tv;\n\t\t\tskb_get_timestamp(skb, &tv);\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMP,\n\t\t\t\t sizeof(tv), &tv);\n\t\t} else {\n\t\t\tstruct timespec ts;\n\t\t\tskb_get_timestampns(skb, &ts);\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPNS,\n\t\t\t\t sizeof(ts), &ts);\n\t\t}\n\t}\n\n\tmemset(&tss, 0, sizeof(tss));\n\tif ((sk->sk_tsflags & SOF_TIMESTAMPING_SOFTWARE) &&\n\t    ktime_to_timespec_cond(skb->tstamp, tss.ts + 0))\n\t\tempty = 0;\n\tif (shhwtstamps &&\n\t    (sk->sk_tsflags & SOF_TIMESTAMPING_RAW_HARDWARE) &&\n\t    ktime_to_timespec_cond(shhwtstamps->hwtstamp, tss.ts + 2))\n\t\tempty = 0;\n\tif (!empty) {\n\t\tput_cmsg(msg, SOL_SOCKET,\n\t\t\t SCM_TIMESTAMPING, sizeof(tss), &tss);\n\n\t\tif (skb_is_err_queue(skb) && skb->len &&\n\t\t    SKB_EXT_ERR(skb)->opt_stats)\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPING_OPT_STATS,\n\t\t\t\t skb->len, skb->data);\n\t}\n}\nEXPORT_SYMBOL_GPL(__sock_recv_timestamp);\n\nvoid __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,\n\tstruct sk_buff *skb)\n{\n\tint ack;\n\n\tif (!sock_flag(sk, SOCK_WIFI_STATUS))\n\t\treturn;\n\tif (!skb->wifi_acked_valid)\n\t\treturn;\n\n\tack = skb->wifi_acked;\n\n\tput_cmsg(msg, SOL_SOCKET, SCM_WIFI_STATUS, sizeof(ack), &ack);\n}\nEXPORT_SYMBOL_GPL(__sock_recv_wifi_status);\n\nstatic inline void sock_recv_drops(struct msghdr *msg, struct sock *sk,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tif (sock_flag(sk, SOCK_RXQ_OVFL) && skb && SOCK_SKB_CB(skb)->dropcount)\n\t\tput_cmsg(msg, SOL_SOCKET, SO_RXQ_OVFL,\n\t\t\tsizeof(__u32), &SOCK_SKB_CB(skb)->dropcount);\n}\n\nvoid __sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,\n\tstruct sk_buff *skb)\n{\n\tsock_recv_timestamp(msg, sk, skb);\n\tsock_recv_drops(msg, sk, skb);\n}\nEXPORT_SYMBOL_GPL(__sock_recv_ts_and_drops);\n\nstatic inline int sock_recvmsg_nosec(struct socket *sock, struct msghdr *msg,\n\t\t\t\t     int flags)\n{\n\treturn sock->ops->recvmsg(sock, msg, msg_data_left(msg), flags);\n}\n\nint sock_recvmsg(struct socket *sock, struct msghdr *msg, int flags)\n{\n\tint err = security_socket_recvmsg(sock, msg, msg_data_left(msg), flags);\n\n\treturn err ?: sock_recvmsg_nosec(sock, msg, flags);\n}\nEXPORT_SYMBOL(sock_recvmsg);\n\n/**\n * kernel_recvmsg - Receive a message from a socket (kernel space)\n * @sock:       The socket to receive the message from\n * @msg:        Received message\n * @vec:        Input s/g array for message data\n * @num:        Size of input s/g array\n * @size:       Number of bytes to read\n * @flags:      Message flags (MSG_DONTWAIT, etc...)\n *\n * On return the msg structure contains the scatter/gather array passed in the\n * vec argument. The array is modified so that it consists of the unfilled\n * portion of the original array.\n *\n * The returned value is the total number of bytes received, or an error.\n */\nint kernel_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t   struct kvec *vec, size_t num, size_t size, int flags)\n{\n\tmm_segment_t oldfs = get_fs();\n\tint result;\n\n\tiov_iter_kvec(&msg->msg_iter, READ | ITER_KVEC, vec, num, size);\n\tset_fs(KERNEL_DS);\n\tresult = sock_recvmsg(sock, msg, flags);\n\tset_fs(oldfs);\n\treturn result;\n}\nEXPORT_SYMBOL(kernel_recvmsg);\n\nstatic ssize_t sock_sendpage(struct file *file, struct page *page,\n\t\t\t     int offset, size_t size, loff_t *ppos, int more)\n{\n\tstruct socket *sock;\n\tint flags;\n\n\tsock = file->private_data;\n\n\tflags = (file->f_flags & O_NONBLOCK) ? MSG_DONTWAIT : 0;\n\t/* more is a combination of MSG_MORE and MSG_SENDPAGE_NOTLAST */\n\tflags |= more;\n\n\treturn kernel_sendpage(sock, page, offset, size, flags);\n}\n\nstatic ssize_t sock_splice_read(struct file *file, loff_t *ppos,\n\t\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\t\tunsigned int flags)\n{\n\tstruct socket *sock = file->private_data;\n\n\tif (unlikely(!sock->ops->splice_read))\n\t\treturn -EINVAL;\n\n\treturn sock->ops->splice_read(sock, ppos, pipe, len, flags);\n}\n\nstatic ssize_t sock_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct socket *sock = file->private_data;\n\tstruct msghdr msg = {.msg_iter = *to,\n\t\t\t     .msg_iocb = iocb};\n\tssize_t res;\n\n\tif (file->f_flags & O_NONBLOCK)\n\t\tmsg.msg_flags = MSG_DONTWAIT;\n\n\tif (iocb->ki_pos != 0)\n\t\treturn -ESPIPE;\n\n\tif (!iov_iter_count(to))\t/* Match SYS5 behaviour */\n\t\treturn 0;\n\n\tres = sock_recvmsg(sock, &msg, msg.msg_flags);\n\t*to = msg.msg_iter;\n\treturn res;\n}\n\nstatic ssize_t sock_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct socket *sock = file->private_data;\n\tstruct msghdr msg = {.msg_iter = *from,\n\t\t\t     .msg_iocb = iocb};\n\tssize_t res;\n\n\tif (iocb->ki_pos != 0)\n\t\treturn -ESPIPE;\n\n\tif (file->f_flags & O_NONBLOCK)\n\t\tmsg.msg_flags = MSG_DONTWAIT;\n\n\tif (sock->type == SOCK_SEQPACKET)\n\t\tmsg.msg_flags |= MSG_EOR;\n\n\tres = sock_sendmsg(sock, &msg);\n\t*from = msg.msg_iter;\n\treturn res;\n}\n\n/*\n * Atomic setting of ioctl hooks to avoid race\n * with module unload.\n */\n\nstatic DEFINE_MUTEX(br_ioctl_mutex);\nstatic int (*br_ioctl_hook) (struct net *, unsigned int cmd, void __user *arg);\n\nvoid brioctl_set(int (*hook) (struct net *, unsigned int, void __user *))\n{\n\tmutex_lock(&br_ioctl_mutex);\n\tbr_ioctl_hook = hook;\n\tmutex_unlock(&br_ioctl_mutex);\n}\nEXPORT_SYMBOL(brioctl_set);\n\nstatic DEFINE_MUTEX(vlan_ioctl_mutex);\nstatic int (*vlan_ioctl_hook) (struct net *, void __user *arg);\n\nvoid vlan_ioctl_set(int (*hook) (struct net *, void __user *))\n{\n\tmutex_lock(&vlan_ioctl_mutex);\n\tvlan_ioctl_hook = hook;\n\tmutex_unlock(&vlan_ioctl_mutex);\n}\nEXPORT_SYMBOL(vlan_ioctl_set);\n\nstatic DEFINE_MUTEX(dlci_ioctl_mutex);\nstatic int (*dlci_ioctl_hook) (unsigned int, void __user *);\n\nvoid dlci_ioctl_set(int (*hook) (unsigned int, void __user *))\n{\n\tmutex_lock(&dlci_ioctl_mutex);\n\tdlci_ioctl_hook = hook;\n\tmutex_unlock(&dlci_ioctl_mutex);\n}\nEXPORT_SYMBOL(dlci_ioctl_set);\n\nstatic long sock_do_ioctl(struct net *net, struct socket *sock,\n\t\t\t\t unsigned int cmd, unsigned long arg)\n{\n\tint err;\n\tvoid __user *argp = (void __user *)arg;\n\n\terr = sock->ops->ioctl(sock, cmd, arg);\n\n\t/*\n\t * If this ioctl is unknown try to hand it down\n\t * to the NIC driver.\n\t */\n\tif (err == -ENOIOCTLCMD)\n\t\terr = dev_ioctl(net, cmd, argp);\n\n\treturn err;\n}\n\n/*\n *\tWith an ioctl, arg may well be a user mode pointer, but we don't know\n *\twhat to do with it - that's up to the protocol still.\n */\n\nstatic struct ns_common *get_net_ns(struct ns_common *ns)\n{\n\treturn &get_net(container_of(ns, struct net, ns))->ns;\n}\n\nstatic long sock_ioctl(struct file *file, unsigned cmd, unsigned long arg)\n{\n\tstruct socket *sock;\n\tstruct sock *sk;\n\tvoid __user *argp = (void __user *)arg;\n\tint pid, err;\n\tstruct net *net;\n\n\tsock = file->private_data;\n\tsk = sock->sk;\n\tnet = sock_net(sk);\n\tif (cmd >= SIOCDEVPRIVATE && cmd <= (SIOCDEVPRIVATE + 15)) {\n\t\terr = dev_ioctl(net, cmd, argp);\n\t} else\n#ifdef CONFIG_WEXT_CORE\n\tif (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST) {\n\t\terr = dev_ioctl(net, cmd, argp);\n\t} else\n#endif\n\t\tswitch (cmd) {\n\t\tcase FIOSETOWN:\n\t\tcase SIOCSPGRP:\n\t\t\terr = -EFAULT;\n\t\t\tif (get_user(pid, (int __user *)argp))\n\t\t\t\tbreak;\n\t\t\tf_setown(sock->file, pid, 1);\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\tcase FIOGETOWN:\n\t\tcase SIOCGPGRP:\n\t\t\terr = put_user(f_getown(sock->file),\n\t\t\t\t       (int __user *)argp);\n\t\t\tbreak;\n\t\tcase SIOCGIFBR:\n\t\tcase SIOCSIFBR:\n\t\tcase SIOCBRADDBR:\n\t\tcase SIOCBRDELBR:\n\t\t\terr = -ENOPKG;\n\t\t\tif (!br_ioctl_hook)\n\t\t\t\trequest_module(\"bridge\");\n\n\t\t\tmutex_lock(&br_ioctl_mutex);\n\t\t\tif (br_ioctl_hook)\n\t\t\t\terr = br_ioctl_hook(net, cmd, argp);\n\t\t\tmutex_unlock(&br_ioctl_mutex);\n\t\t\tbreak;\n\t\tcase SIOCGIFVLAN:\n\t\tcase SIOCSIFVLAN:\n\t\t\terr = -ENOPKG;\n\t\t\tif (!vlan_ioctl_hook)\n\t\t\t\trequest_module(\"8021q\");\n\n\t\t\tmutex_lock(&vlan_ioctl_mutex);\n\t\t\tif (vlan_ioctl_hook)\n\t\t\t\terr = vlan_ioctl_hook(net, argp);\n\t\t\tmutex_unlock(&vlan_ioctl_mutex);\n\t\t\tbreak;\n\t\tcase SIOCADDDLCI:\n\t\tcase SIOCDELDLCI:\n\t\t\terr = -ENOPKG;\n\t\t\tif (!dlci_ioctl_hook)\n\t\t\t\trequest_module(\"dlci\");\n\n\t\t\tmutex_lock(&dlci_ioctl_mutex);\n\t\t\tif (dlci_ioctl_hook)\n\t\t\t\terr = dlci_ioctl_hook(cmd, argp);\n\t\t\tmutex_unlock(&dlci_ioctl_mutex);\n\t\t\tbreak;\n\t\tcase SIOCGSKNS:\n\t\t\terr = -EPERM;\n\t\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\t\tbreak;\n\n\t\t\terr = open_related_ns(&net->ns, get_net_ns);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = sock_do_ioctl(net, sock, cmd, arg);\n\t\t\tbreak;\n\t\t}\n\treturn err;\n}\n\nint sock_create_lite(int family, int type, int protocol, struct socket **res)\n{\n\tint err;\n\tstruct socket *sock = NULL;\n\n\terr = security_socket_create(family, type, protocol, 1);\n\tif (err)\n\t\tgoto out;\n\n\tsock = sock_alloc();\n\tif (!sock) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tsock->type = type;\n\terr = security_socket_post_create(sock, family, type, protocol, 1);\n\tif (err)\n\t\tgoto out_release;\n\nout:\n\t*res = sock;\n\treturn err;\nout_release:\n\tsock_release(sock);\n\tsock = NULL;\n\tgoto out;\n}\nEXPORT_SYMBOL(sock_create_lite);\n\n/* No kernel lock held - perfect */\nstatic unsigned int sock_poll(struct file *file, poll_table *wait)\n{\n\tunsigned int busy_flag = 0;\n\tstruct socket *sock;\n\n\t/*\n\t *      We can't return errors to poll, so it's either yes or no.\n\t */\n\tsock = file->private_data;\n\n\tif (sk_can_busy_loop(sock->sk)) {\n\t\t/* this socket can poll_ll so tell the system call */\n\t\tbusy_flag = POLL_BUSY_LOOP;\n\n\t\t/* once, only if requested by syscall */\n\t\tif (wait && (wait->_key & POLL_BUSY_LOOP))\n\t\t\tsk_busy_loop(sock->sk, 1);\n\t}\n\n\treturn busy_flag | sock->ops->poll(file, sock, wait);\n}\n\nstatic int sock_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct socket *sock = file->private_data;\n\n\treturn sock->ops->mmap(file, sock, vma);\n}\n\nstatic int sock_close(struct inode *inode, struct file *filp)\n{\n\tsock_release(SOCKET_I(inode));\n\treturn 0;\n}\n\n/*\n *\tUpdate the socket async list\n *\n *\tFasync_list locking strategy.\n *\n *\t1. fasync_list is modified only under process context socket lock\n *\t   i.e. under semaphore.\n *\t2. fasync_list is used under read_lock(&sk->sk_callback_lock)\n *\t   or under socket lock\n */\n\nstatic int sock_fasync(int fd, struct file *filp, int on)\n{\n\tstruct socket *sock = filp->private_data;\n\tstruct sock *sk = sock->sk;\n\tstruct socket_wq *wq;\n\n\tif (sk == NULL)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\twq = rcu_dereference_protected(sock->wq, lockdep_sock_is_held(sk));\n\tfasync_helper(fd, filp, on, &wq->fasync_list);\n\n\tif (!wq->fasync_list)\n\t\tsock_reset_flag(sk, SOCK_FASYNC);\n\telse\n\t\tsock_set_flag(sk, SOCK_FASYNC);\n\n\trelease_sock(sk);\n\treturn 0;\n}\n\n/* This function may be called only under rcu_lock */\n\nint sock_wake_async(struct socket_wq *wq, int how, int band)\n{\n\tif (!wq || !wq->fasync_list)\n\t\treturn -1;\n\n\tswitch (how) {\n\tcase SOCK_WAKE_WAITD:\n\t\tif (test_bit(SOCKWQ_ASYNC_WAITDATA, &wq->flags))\n\t\t\tbreak;\n\t\tgoto call_kill;\n\tcase SOCK_WAKE_SPACE:\n\t\tif (!test_and_clear_bit(SOCKWQ_ASYNC_NOSPACE, &wq->flags))\n\t\t\tbreak;\n\t\t/* fall through */\n\tcase SOCK_WAKE_IO:\ncall_kill:\n\t\tkill_fasync(&wq->fasync_list, SIGIO, band);\n\t\tbreak;\n\tcase SOCK_WAKE_URG:\n\t\tkill_fasync(&wq->fasync_list, SIGURG, band);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_wake_async);\n\nint __sock_create(struct net *net, int family, int type, int protocol,\n\t\t\t struct socket **res, int kern)\n{\n\tint err;\n\tstruct socket *sock;\n\tconst struct net_proto_family *pf;\n\n\t/*\n\t *      Check protocol is in range\n\t */\n\tif (family < 0 || family >= NPROTO)\n\t\treturn -EAFNOSUPPORT;\n\tif (type < 0 || type >= SOCK_MAX)\n\t\treturn -EINVAL;\n\n\t/* Compatibility.\n\n\t   This uglymoron is moved from INET layer to here to avoid\n\t   deadlock in module load.\n\t */\n\tif (family == PF_INET && type == SOCK_PACKET) {\n\t\tpr_info_once(\"%s uses obsolete (PF_INET,SOCK_PACKET)\\n\",\n\t\t\t     current->comm);\n\t\tfamily = PF_PACKET;\n\t}\n\n\terr = security_socket_create(family, type, protocol, kern);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t *\tAllocate the socket and allow the family to set things up. if\n\t *\tthe protocol is 0, the family is instructed to select an appropriate\n\t *\tdefault.\n\t */\n\tsock = sock_alloc();\n\tif (!sock) {\n\t\tnet_warn_ratelimited(\"socket: no more sockets\\n\");\n\t\treturn -ENFILE;\t/* Not exactly a match, but its the\n\t\t\t\t   closest posix thing */\n\t}\n\n\tsock->type = type;\n\n#ifdef CONFIG_MODULES\n\t/* Attempt to load a protocol module if the find failed.\n\t *\n\t * 12/09/1996 Marcin: But! this makes REALLY only sense, if the user\n\t * requested real, full-featured networking support upon configuration.\n\t * Otherwise module support will break!\n\t */\n\tif (rcu_access_pointer(net_families[family]) == NULL)\n\t\trequest_module(\"net-pf-%d\", family);\n#endif\n\n\trcu_read_lock();\n\tpf = rcu_dereference(net_families[family]);\n\terr = -EAFNOSUPPORT;\n\tif (!pf)\n\t\tgoto out_release;\n\n\t/*\n\t * We will call the ->create function, that possibly is in a loadable\n\t * module, so we have to bump that loadable module refcnt first.\n\t */\n\tif (!try_module_get(pf->owner))\n\t\tgoto out_release;\n\n\t/* Now protected by module ref count */\n\trcu_read_unlock();\n\n\terr = pf->create(net, sock, protocol, kern);\n\tif (err < 0)\n\t\tgoto out_module_put;\n\n\t/*\n\t * Now to bump the refcnt of the [loadable] module that owns this\n\t * socket at sock_release time we decrement its refcnt.\n\t */\n\tif (!try_module_get(sock->ops->owner))\n\t\tgoto out_module_busy;\n\n\t/*\n\t * Now that we're done with the ->create function, the [loadable]\n\t * module can have its refcnt decremented\n\t */\n\tmodule_put(pf->owner);\n\terr = security_socket_post_create(sock, family, type, protocol, kern);\n\tif (err)\n\t\tgoto out_sock_release;\n\t*res = sock;\n\n\treturn 0;\n\nout_module_busy:\n\terr = -EAFNOSUPPORT;\nout_module_put:\n\tsock->ops = NULL;\n\tmodule_put(pf->owner);\nout_sock_release:\n\tsock_release(sock);\n\treturn err;\n\nout_release:\n\trcu_read_unlock();\n\tgoto out_sock_release;\n}\nEXPORT_SYMBOL(__sock_create);\n\nint sock_create(int family, int type, int protocol, struct socket **res)\n{\n\treturn __sock_create(current->nsproxy->net_ns, family, type, protocol, res, 0);\n}\nEXPORT_SYMBOL(sock_create);\n\nint sock_create_kern(struct net *net, int family, int type, int protocol, struct socket **res)\n{\n\treturn __sock_create(net, family, type, protocol, res, 1);\n}\nEXPORT_SYMBOL(sock_create_kern);\n\nSYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)\n{\n\tint retval;\n\tstruct socket *sock;\n\tint flags;\n\n\t/* Check the SOCK_* constants for consistency.  */\n\tBUILD_BUG_ON(SOCK_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON((SOCK_MAX | SOCK_TYPE_MASK) != SOCK_TYPE_MASK);\n\tBUILD_BUG_ON(SOCK_CLOEXEC & SOCK_TYPE_MASK);\n\tBUILD_BUG_ON(SOCK_NONBLOCK & SOCK_TYPE_MASK);\n\n\tflags = type & ~SOCK_TYPE_MASK;\n\tif (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))\n\t\treturn -EINVAL;\n\ttype &= SOCK_TYPE_MASK;\n\n\tif (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))\n\t\tflags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;\n\n\tretval = sock_create(family, type, protocol, &sock);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = sock_map_fd(sock, flags & (O_CLOEXEC | O_NONBLOCK));\n\tif (retval < 0)\n\t\tgoto out_release;\n\nout:\n\t/* It may be already another descriptor 8) Not kernel problem. */\n\treturn retval;\n\nout_release:\n\tsock_release(sock);\n\treturn retval;\n}\n\n/*\n *\tCreate a pair of connected sockets.\n */\n\nSYSCALL_DEFINE4(socketpair, int, family, int, type, int, protocol,\n\t\tint __user *, usockvec)\n{\n\tstruct socket *sock1, *sock2;\n\tint fd1, fd2, err;\n\tstruct file *newfile1, *newfile2;\n\tint flags;\n\n\tflags = type & ~SOCK_TYPE_MASK;\n\tif (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))\n\t\treturn -EINVAL;\n\ttype &= SOCK_TYPE_MASK;\n\n\tif (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))\n\t\tflags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;\n\n\t/*\n\t * Obtain the first socket and check if the underlying protocol\n\t * supports the socketpair call.\n\t */\n\n\terr = sock_create(family, type, protocol, &sock1);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = sock_create(family, type, protocol, &sock2);\n\tif (err < 0)\n\t\tgoto out_release_1;\n\n\terr = sock1->ops->socketpair(sock1, sock2);\n\tif (err < 0)\n\t\tgoto out_release_both;\n\n\tfd1 = get_unused_fd_flags(flags);\n\tif (unlikely(fd1 < 0)) {\n\t\terr = fd1;\n\t\tgoto out_release_both;\n\t}\n\n\tfd2 = get_unused_fd_flags(flags);\n\tif (unlikely(fd2 < 0)) {\n\t\terr = fd2;\n\t\tgoto out_put_unused_1;\n\t}\n\n\tnewfile1 = sock_alloc_file(sock1, flags, NULL);\n\tif (IS_ERR(newfile1)) {\n\t\terr = PTR_ERR(newfile1);\n\t\tgoto out_put_unused_both;\n\t}\n\n\tnewfile2 = sock_alloc_file(sock2, flags, NULL);\n\tif (IS_ERR(newfile2)) {\n\t\terr = PTR_ERR(newfile2);\n\t\tgoto out_fput_1;\n\t}\n\n\terr = put_user(fd1, &usockvec[0]);\n\tif (err)\n\t\tgoto out_fput_both;\n\n\terr = put_user(fd2, &usockvec[1]);\n\tif (err)\n\t\tgoto out_fput_both;\n\n\taudit_fd_pair(fd1, fd2);\n\n\tfd_install(fd1, newfile1);\n\tfd_install(fd2, newfile2);\n\t/* fd1 and fd2 may be already another descriptors.\n\t * Not kernel problem.\n\t */\n\n\treturn 0;\n\nout_fput_both:\n\tfput(newfile2);\n\tfput(newfile1);\n\tput_unused_fd(fd2);\n\tput_unused_fd(fd1);\n\tgoto out;\n\nout_fput_1:\n\tfput(newfile1);\n\tput_unused_fd(fd2);\n\tput_unused_fd(fd1);\n\tsock_release(sock2);\n\tgoto out;\n\nout_put_unused_both:\n\tput_unused_fd(fd2);\nout_put_unused_1:\n\tput_unused_fd(fd1);\nout_release_both:\n\tsock_release(sock2);\nout_release_1:\n\tsock_release(sock1);\nout:\n\treturn err;\n}\n\n/*\n *\tBind a name to a socket. Nothing much to do here since it's\n *\tthe protocol's responsibility to handle the local address.\n *\n *\tWe move the socket address to kernel space before we call\n *\tthe protocol layer (having also checked the address is ok).\n */\n\nSYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint err, fput_needed;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock) {\n\t\terr = move_addr_to_kernel(umyaddr, addrlen, &address);\n\t\tif (err >= 0) {\n\t\t\terr = security_socket_bind(sock,\n\t\t\t\t\t\t   (struct sockaddr *)&address,\n\t\t\t\t\t\t   addrlen);\n\t\t\tif (!err)\n\t\t\t\terr = sock->ops->bind(sock,\n\t\t\t\t\t\t      (struct sockaddr *)\n\t\t\t\t\t\t      &address, addrlen);\n\t\t}\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tPerform a listen. Basically, we allow the protocol to do anything\n *\tnecessary for a listen, and if that works, we mark the socket as\n *\tready for listening.\n */\n\nSYSCALL_DEFINE2(listen, int, fd, int, backlog)\n{\n\tstruct socket *sock;\n\tint err, fput_needed;\n\tint somaxconn;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock) {\n\t\tsomaxconn = sock_net(sock->sk)->core.sysctl_somaxconn;\n\t\tif ((unsigned int)backlog > somaxconn)\n\t\t\tbacklog = somaxconn;\n\n\t\terr = security_socket_listen(sock, backlog);\n\t\tif (!err)\n\t\t\terr = sock->ops->listen(sock, backlog);\n\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tFor accept, we attempt to create a new socket, set up the link\n *\twith the client, wake up the client, then return the new\n *\tconnected fd. We collect the address of the connector in kernel\n *\tspace and move it to user at the very end. This is unclean because\n *\twe open the socket then return an error.\n *\n *\t1003.1g adds the ability to recvmsg() to query connection pending\n *\tstatus to recvmsg. We need to add that support in a way thats\n *\tclean when we restucture accept also.\n */\n\nSYSCALL_DEFINE4(accept4, int, fd, struct sockaddr __user *, upeer_sockaddr,\n\t\tint __user *, upeer_addrlen, int, flags)\n{\n\tstruct socket *sock, *newsock;\n\tstruct file *newfile;\n\tint err, len, newfd, fput_needed;\n\tstruct sockaddr_storage address;\n\n\tif (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))\n\t\treturn -EINVAL;\n\n\tif (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))\n\t\tflags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\terr = -ENFILE;\n\tnewsock = sock_alloc();\n\tif (!newsock)\n\t\tgoto out_put;\n\n\tnewsock->type = sock->type;\n\tnewsock->ops = sock->ops;\n\n\t/*\n\t * We don't need try_module_get here, as the listening socket (sock)\n\t * has the protocol module (sock->ops->owner) held.\n\t */\n\t__module_get(newsock->ops->owner);\n\n\tnewfd = get_unused_fd_flags(flags);\n\tif (unlikely(newfd < 0)) {\n\t\terr = newfd;\n\t\tsock_release(newsock);\n\t\tgoto out_put;\n\t}\n\tnewfile = sock_alloc_file(newsock, flags, sock->sk->sk_prot_creator->name);\n\tif (IS_ERR(newfile)) {\n\t\terr = PTR_ERR(newfile);\n\t\tput_unused_fd(newfd);\n\t\tsock_release(newsock);\n\t\tgoto out_put;\n\t}\n\n\terr = security_socket_accept(sock, newsock);\n\tif (err)\n\t\tgoto out_fd;\n\n\terr = sock->ops->accept(sock, newsock, sock->file->f_flags, false);\n\tif (err < 0)\n\t\tgoto out_fd;\n\n\tif (upeer_sockaddr) {\n\t\tif (newsock->ops->getname(newsock, (struct sockaddr *)&address,\n\t\t\t\t\t  &len, 2) < 0) {\n\t\t\terr = -ECONNABORTED;\n\t\t\tgoto out_fd;\n\t\t}\n\t\terr = move_addr_to_user(&address,\n\t\t\t\t\tlen, upeer_sockaddr, upeer_addrlen);\n\t\tif (err < 0)\n\t\t\tgoto out_fd;\n\t}\n\n\t/* File flags are not inherited via accept() unlike another OSes. */\n\n\tfd_install(newfd, newfile);\n\terr = newfd;\n\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\nout_fd:\n\tfput(newfile);\n\tput_unused_fd(newfd);\n\tgoto out_put;\n}\n\nSYSCALL_DEFINE3(accept, int, fd, struct sockaddr __user *, upeer_sockaddr,\n\t\tint __user *, upeer_addrlen)\n{\n\treturn sys_accept4(fd, upeer_sockaddr, upeer_addrlen, 0);\n}\n\n/*\n *\tAttempt to connect to a socket with the server address.  The address\n *\tis in user space so we verify it is OK and move it to kernel space.\n *\n *\tFor 1003.1g we need to add clean support for a bind to AF_UNSPEC to\n *\tbreak bindings\n *\n *\tNOTE: 1003.1g draft 6.3 is broken with respect to AX.25/NetROM and\n *\tother SEQPACKET protocols that take time to connect() as it doesn't\n *\tinclude the -EINPROGRESS status for such sockets.\n */\n\nSYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr,\n\t\tint, addrlen)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint err, fput_needed;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\terr = move_addr_to_kernel(uservaddr, addrlen, &address);\n\tif (err < 0)\n\t\tgoto out_put;\n\n\terr =\n\t    security_socket_connect(sock, (struct sockaddr *)&address, addrlen);\n\tif (err)\n\t\tgoto out_put;\n\n\terr = sock->ops->connect(sock, (struct sockaddr *)&address, addrlen,\n\t\t\t\t sock->file->f_flags);\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\n/*\n *\tGet the local address ('name') of a socket object. Move the obtained\n *\tname to user space.\n */\n\nSYSCALL_DEFINE3(getsockname, int, fd, struct sockaddr __user *, usockaddr,\n\t\tint __user *, usockaddr_len)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint len, err, fput_needed;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\terr = security_socket_getsockname(sock);\n\tif (err)\n\t\tgoto out_put;\n\n\terr = sock->ops->getname(sock, (struct sockaddr *)&address, &len, 0);\n\tif (err)\n\t\tgoto out_put;\n\terr = move_addr_to_user(&address, len, usockaddr, usockaddr_len);\n\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\n/*\n *\tGet the remote address ('name') of a socket object. Move the obtained\n *\tname to user space.\n */\n\nSYSCALL_DEFINE3(getpeername, int, fd, struct sockaddr __user *, usockaddr,\n\t\tint __user *, usockaddr_len)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint len, err, fput_needed;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock != NULL) {\n\t\terr = security_socket_getpeername(sock);\n\t\tif (err) {\n\t\t\tfput_light(sock->file, fput_needed);\n\t\t\treturn err;\n\t\t}\n\n\t\terr =\n\t\t    sock->ops->getname(sock, (struct sockaddr *)&address, &len,\n\t\t\t\t       1);\n\t\tif (!err)\n\t\t\terr = move_addr_to_user(&address, len, usockaddr,\n\t\t\t\t\t\tusockaddr_len);\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tSend a datagram to a given address. We move the address into kernel\n *\tspace and check the user space data area is readable before invoking\n *\tthe protocol.\n */\n\nSYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint, addr_len)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint err;\n\tstruct msghdr msg;\n\tstruct iovec iov;\n\tint fput_needed;\n\n\terr = import_single_range(WRITE, buff, len, &iov, &msg.msg_iter);\n\tif (unlikely(err))\n\t\treturn err;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\tif (addr) {\n\t\terr = move_addr_to_kernel(addr, addr_len, &address);\n\t\tif (err < 0)\n\t\t\tgoto out_put;\n\t\tmsg.msg_name = (struct sockaddr *)&address;\n\t\tmsg.msg_namelen = addr_len;\n\t}\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\tmsg.msg_flags = flags;\n\terr = sock_sendmsg(sock, &msg);\n\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\n/*\n *\tSend a datagram down a socket.\n */\n\nSYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len,\n\t\tunsigned int, flags)\n{\n\treturn sys_sendto(fd, buff, len, flags, NULL, 0);\n}\n\n/*\n *\tReceive a frame from the socket and optionally record the address of the\n *\tsender. We verify the buffers are writable and if needed move the\n *\tsender address from kernel to user space.\n */\n\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\terr = import_single_range(READ, ubuf, size, &iov, &msg.msg_iter);\n\tif (unlikely(err))\n\t\treturn err;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\t/* Save some cycles and don't copy the address if not needed */\n\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg.msg_namelen = 0;\n\tmsg.msg_iocb = NULL;\n\tmsg.msg_flags = 0;\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\n/*\n *\tReceive a datagram from a socket.\n */\n\nSYSCALL_DEFINE4(recv, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags)\n{\n\treturn sys_recvfrom(fd, ubuf, size, flags, NULL, NULL);\n}\n\n/*\n *\tSet a socket option. Because we don't know the option lengths we have\n *\tto pass the user mode parameter for the protocols to sort out.\n */\n\nSYSCALL_DEFINE5(setsockopt, int, fd, int, level, int, optname,\n\t\tchar __user *, optval, int, optlen)\n{\n\tint err, fput_needed;\n\tstruct socket *sock;\n\n\tif (optlen < 0)\n\t\treturn -EINVAL;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock != NULL) {\n\t\terr = security_socket_setsockopt(sock, level, optname);\n\t\tif (err)\n\t\t\tgoto out_put;\n\n\t\tif (level == SOL_SOCKET)\n\t\t\terr =\n\t\t\t    sock_setsockopt(sock, level, optname, optval,\n\t\t\t\t\t    optlen);\n\t\telse\n\t\t\terr =\n\t\t\t    sock->ops->setsockopt(sock, level, optname, optval,\n\t\t\t\t\t\t  optlen);\nout_put:\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tGet a socket option. Because we don't know the option lengths we have\n *\tto pass a user mode parameter for the protocols to sort out.\n */\n\nSYSCALL_DEFINE5(getsockopt, int, fd, int, level, int, optname,\n\t\tchar __user *, optval, int __user *, optlen)\n{\n\tint err, fput_needed;\n\tstruct socket *sock;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock != NULL) {\n\t\terr = security_socket_getsockopt(sock, level, optname);\n\t\tif (err)\n\t\t\tgoto out_put;\n\n\t\tif (level == SOL_SOCKET)\n\t\t\terr =\n\t\t\t    sock_getsockopt(sock, level, optname, optval,\n\t\t\t\t\t    optlen);\n\t\telse\n\t\t\terr =\n\t\t\t    sock->ops->getsockopt(sock, level, optname, optval,\n\t\t\t\t\t\t  optlen);\nout_put:\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/*\n *\tShutdown a socket.\n */\n\nSYSCALL_DEFINE2(shutdown, int, fd, int, how)\n{\n\tint err, fput_needed;\n\tstruct socket *sock;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (sock != NULL) {\n\t\terr = security_socket_shutdown(sock, how);\n\t\tif (!err)\n\t\t\terr = sock->ops->shutdown(sock, how);\n\t\tfput_light(sock->file, fput_needed);\n\t}\n\treturn err;\n}\n\n/* A couple of helpful macros for getting the address of the 32/64 bit\n * fields which are the same type (int / unsigned) on our platforms.\n */\n#define COMPAT_MSG(msg, member)\t((MSG_CMSG_COMPAT & flags) ? &msg##_compat->member : &msg->member)\n#define COMPAT_NAMELEN(msg)\tCOMPAT_MSG(msg, msg_namelen)\n#define COMPAT_FLAGS(msg)\tCOMPAT_MSG(msg, msg_flags)\n\nstruct used_address {\n\tstruct sockaddr_storage name;\n\tunsigned int name_len;\n};\n\nstatic int copy_msghdr_from_user(struct msghdr *kmsg,\n\t\t\t\t struct user_msghdr __user *umsg,\n\t\t\t\t struct sockaddr __user **save_addr,\n\t\t\t\t struct iovec **iov)\n{\n\tstruct sockaddr __user *uaddr;\n\tstruct iovec __user *uiov;\n\tsize_t nr_segs;\n\tssize_t err;\n\n\tif (!access_ok(VERIFY_READ, umsg, sizeof(*umsg)) ||\n\t    __get_user(uaddr, &umsg->msg_name) ||\n\t    __get_user(kmsg->msg_namelen, &umsg->msg_namelen) ||\n\t    __get_user(uiov, &umsg->msg_iov) ||\n\t    __get_user(nr_segs, &umsg->msg_iovlen) ||\n\t    __get_user(kmsg->msg_control, &umsg->msg_control) ||\n\t    __get_user(kmsg->msg_controllen, &umsg->msg_controllen) ||\n\t    __get_user(kmsg->msg_flags, &umsg->msg_flags))\n\t\treturn -EFAULT;\n\n\tif (!uaddr)\n\t\tkmsg->msg_namelen = 0;\n\n\tif (kmsg->msg_namelen < 0)\n\t\treturn -EINVAL;\n\n\tif (kmsg->msg_namelen > sizeof(struct sockaddr_storage))\n\t\tkmsg->msg_namelen = sizeof(struct sockaddr_storage);\n\n\tif (save_addr)\n\t\t*save_addr = uaddr;\n\n\tif (uaddr && kmsg->msg_namelen) {\n\t\tif (!save_addr) {\n\t\t\terr = move_addr_to_kernel(uaddr, kmsg->msg_namelen,\n\t\t\t\t\t\t  kmsg->msg_name);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t} else {\n\t\tkmsg->msg_name = NULL;\n\t\tkmsg->msg_namelen = 0;\n\t}\n\n\tif (nr_segs > UIO_MAXIOV)\n\t\treturn -EMSGSIZE;\n\n\tkmsg->msg_iocb = NULL;\n\n\treturn import_iovec(save_addr ? READ : WRITE, uiov, nr_segs,\n\t\t\t    UIO_FASTIOV, iov, &kmsg->msg_iter);\n}\n\nstatic int ___sys_sendmsg(struct socket *sock, struct user_msghdr __user *msg,\n\t\t\t struct msghdr *msg_sys, unsigned int flags,\n\t\t\t struct used_address *used_address,\n\t\t\t unsigned int allowed_msghdr_flags)\n{\n\tstruct compat_msghdr __user *msg_compat =\n\t    (struct compat_msghdr __user *)msg;\n\tstruct sockaddr_storage address;\n\tstruct iovec iovstack[UIO_FASTIOV], *iov = iovstack;\n\tunsigned char ctl[sizeof(struct cmsghdr) + 20]\n\t\t\t\t__aligned(sizeof(__kernel_size_t));\n\t/* 20 is size of ipv6_pktinfo */\n\tunsigned char *ctl_buf = ctl;\n\tint ctl_len;\n\tssize_t err;\n\n\tmsg_sys->msg_name = &address;\n\n\tif (MSG_CMSG_COMPAT & flags)\n\t\terr = get_compat_msghdr(msg_sys, msg_compat, NULL, &iov);\n\telse\n\t\terr = copy_msghdr_from_user(msg_sys, msg, NULL, &iov);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = -ENOBUFS;\n\n\tif (msg_sys->msg_controllen > INT_MAX)\n\t\tgoto out_freeiov;\n\tflags |= (msg_sys->msg_flags & allowed_msghdr_flags);\n\tctl_len = msg_sys->msg_controllen;\n\tif ((MSG_CMSG_COMPAT & flags) && ctl_len) {\n\t\terr =\n\t\t    cmsghdr_from_user_compat_to_kern(msg_sys, sock->sk, ctl,\n\t\t\t\t\t\t     sizeof(ctl));\n\t\tif (err)\n\t\t\tgoto out_freeiov;\n\t\tctl_buf = msg_sys->msg_control;\n\t\tctl_len = msg_sys->msg_controllen;\n\t} else if (ctl_len) {\n\t\tBUILD_BUG_ON(sizeof(struct cmsghdr) !=\n\t\t\t     CMSG_ALIGN(sizeof(struct cmsghdr)));\n\t\tif (ctl_len > sizeof(ctl)) {\n\t\t\tctl_buf = sock_kmalloc(sock->sk, ctl_len, GFP_KERNEL);\n\t\t\tif (ctl_buf == NULL)\n\t\t\t\tgoto out_freeiov;\n\t\t}\n\t\terr = -EFAULT;\n\t\t/*\n\t\t * Careful! Before this, msg_sys->msg_control contains a user pointer.\n\t\t * Afterwards, it will be a kernel pointer. Thus the compiler-assisted\n\t\t * checking falls down on this.\n\t\t */\n\t\tif (copy_from_user(ctl_buf,\n\t\t\t\t   (void __user __force *)msg_sys->msg_control,\n\t\t\t\t   ctl_len))\n\t\t\tgoto out_freectl;\n\t\tmsg_sys->msg_control = ctl_buf;\n\t}\n\tmsg_sys->msg_flags = flags;\n\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tmsg_sys->msg_flags |= MSG_DONTWAIT;\n\t/*\n\t * If this is sendmmsg() and current destination address is same as\n\t * previously succeeded address, omit asking LSM's decision.\n\t * used_address->name_len is initialized to UINT_MAX so that the first\n\t * destination address never matches.\n\t */\n\tif (used_address && msg_sys->msg_name &&\n\t    used_address->name_len == msg_sys->msg_namelen &&\n\t    !memcmp(&used_address->name, msg_sys->msg_name,\n\t\t    used_address->name_len)) {\n\t\terr = sock_sendmsg_nosec(sock, msg_sys);\n\t\tgoto out_freectl;\n\t}\n\terr = sock_sendmsg(sock, msg_sys);\n\t/*\n\t * If this is sendmmsg() and sending to current destination address was\n\t * successful, remember it.\n\t */\n\tif (used_address && err >= 0) {\n\t\tused_address->name_len = msg_sys->msg_namelen;\n\t\tif (msg_sys->msg_name)\n\t\t\tmemcpy(&used_address->name, msg_sys->msg_name,\n\t\t\t       used_address->name_len);\n\t}\n\nout_freectl:\n\tif (ctl_buf != ctl)\n\t\tsock_kfree_s(sock->sk, ctl_buf, ctl_len);\nout_freeiov:\n\tkfree(iov);\n\treturn err;\n}\n\n/*\n *\tBSD sendmsg interface\n */\n\nlong __sys_sendmsg(int fd, struct user_msghdr __user *msg, unsigned flags)\n{\n\tint fput_needed, err;\n\tstruct msghdr msg_sys;\n\tstruct socket *sock;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\terr = ___sys_sendmsg(sock, msg, &msg_sys, flags, NULL, 0);\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\nSYSCALL_DEFINE3(sendmsg, int, fd, struct user_msghdr __user *, msg, unsigned int, flags)\n{\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\treturn __sys_sendmsg(fd, msg, flags);\n}\n\n/*\n *\tLinux sendmmsg interface\n */\n\nint __sys_sendmmsg(int fd, struct mmsghdr __user *mmsg, unsigned int vlen,\n\t\t   unsigned int flags)\n{\n\tint fput_needed, err, datagrams;\n\tstruct socket *sock;\n\tstruct mmsghdr __user *entry;\n\tstruct compat_mmsghdr __user *compat_entry;\n\tstruct msghdr msg_sys;\n\tstruct used_address used_address;\n\tunsigned int oflags = flags;\n\n\tif (vlen > UIO_MAXIOV)\n\t\tvlen = UIO_MAXIOV;\n\n\tdatagrams = 0;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\treturn err;\n\n\tused_address.name_len = UINT_MAX;\n\tentry = mmsg;\n\tcompat_entry = (struct compat_mmsghdr __user *)mmsg;\n\terr = 0;\n\tflags |= MSG_BATCH;\n\n\twhile (datagrams < vlen) {\n\t\tif (datagrams == vlen - 1)\n\t\t\tflags = oflags;\n\n\t\tif (MSG_CMSG_COMPAT & flags) {\n\t\t\terr = ___sys_sendmsg(sock, (struct user_msghdr __user *)compat_entry,\n\t\t\t\t\t     &msg_sys, flags, &used_address, MSG_EOR);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\terr = __put_user(err, &compat_entry->msg_len);\n\t\t\t++compat_entry;\n\t\t} else {\n\t\t\terr = ___sys_sendmsg(sock,\n\t\t\t\t\t     (struct user_msghdr __user *)entry,\n\t\t\t\t\t     &msg_sys, flags, &used_address, MSG_EOR);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\terr = put_user(err, &entry->msg_len);\n\t\t\t++entry;\n\t\t}\n\n\t\tif (err)\n\t\t\tbreak;\n\t\t++datagrams;\n\t\tif (msg_data_left(&msg_sys))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\n\tfput_light(sock->file, fput_needed);\n\n\t/* We only return an error if no datagrams were able to be sent */\n\tif (datagrams != 0)\n\t\treturn datagrams;\n\n\treturn err;\n}\n\nSYSCALL_DEFINE4(sendmmsg, int, fd, struct mmsghdr __user *, mmsg,\n\t\tunsigned int, vlen, unsigned int, flags)\n{\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\treturn __sys_sendmmsg(fd, mmsg, vlen, flags);\n}\n\nstatic int ___sys_recvmsg(struct socket *sock, struct user_msghdr __user *msg,\n\t\t\t struct msghdr *msg_sys, unsigned int flags, int nosec)\n{\n\tstruct compat_msghdr __user *msg_compat =\n\t    (struct compat_msghdr __user *)msg;\n\tstruct iovec iovstack[UIO_FASTIOV];\n\tstruct iovec *iov = iovstack;\n\tunsigned long cmsg_ptr;\n\tint len;\n\tssize_t err;\n\n\t/* kernel mode address */\n\tstruct sockaddr_storage addr;\n\n\t/* user mode address pointers */\n\tstruct sockaddr __user *uaddr;\n\tint __user *uaddr_len = COMPAT_NAMELEN(msg);\n\n\tmsg_sys->msg_name = &addr;\n\n\tif (MSG_CMSG_COMPAT & flags)\n\t\terr = get_compat_msghdr(msg_sys, msg_compat, &uaddr, &iov);\n\telse\n\t\terr = copy_msghdr_from_user(msg_sys, msg, &uaddr, &iov);\n\tif (err < 0)\n\t\treturn err;\n\n\tcmsg_ptr = (unsigned long)msg_sys->msg_control;\n\tmsg_sys->msg_flags = flags & (MSG_CMSG_CLOEXEC|MSG_CMSG_COMPAT);\n\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg_sys->msg_namelen = 0;\n\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = (nosec ? sock_recvmsg_nosec : sock_recvmsg)(sock, msg_sys, flags);\n\tif (err < 0)\n\t\tgoto out_freeiov;\n\tlen = err;\n\n\tif (uaddr != NULL) {\n\t\terr = move_addr_to_user(&addr,\n\t\t\t\t\tmsg_sys->msg_namelen, uaddr,\n\t\t\t\t\tuaddr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_freeiov;\n\t}\n\terr = __put_user((msg_sys->msg_flags & ~MSG_CMSG_COMPAT),\n\t\t\t COMPAT_FLAGS(msg));\n\tif (err)\n\t\tgoto out_freeiov;\n\tif (MSG_CMSG_COMPAT & flags)\n\t\terr = __put_user((unsigned long)msg_sys->msg_control - cmsg_ptr,\n\t\t\t\t &msg_compat->msg_controllen);\n\telse\n\t\terr = __put_user((unsigned long)msg_sys->msg_control - cmsg_ptr,\n\t\t\t\t &msg->msg_controllen);\n\tif (err)\n\t\tgoto out_freeiov;\n\terr = len;\n\nout_freeiov:\n\tkfree(iov);\n\treturn err;\n}\n\n/*\n *\tBSD recvmsg interface\n */\n\nlong __sys_recvmsg(int fd, struct user_msghdr __user *msg, unsigned flags)\n{\n\tint fput_needed, err;\n\tstruct msghdr msg_sys;\n\tstruct socket *sock;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\terr = ___sys_recvmsg(sock, msg, &msg_sys, flags, 0);\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}\n\nSYSCALL_DEFINE3(recvmsg, int, fd, struct user_msghdr __user *, msg,\n\t\tunsigned int, flags)\n{\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\treturn __sys_recvmsg(fd, msg, flags);\n}\n\n/*\n *     Linux recvmmsg interface\n */\n\nint __sys_recvmmsg(int fd, struct mmsghdr __user *mmsg, unsigned int vlen,\n\t\t   unsigned int flags, struct timespec *timeout)\n{\n\tint fput_needed, err, datagrams;\n\tstruct socket *sock;\n\tstruct mmsghdr __user *entry;\n\tstruct compat_mmsghdr __user *compat_entry;\n\tstruct msghdr msg_sys;\n\tstruct timespec64 end_time;\n\tstruct timespec64 timeout64;\n\n\tif (timeout &&\n\t    poll_select_set_timeout(&end_time, timeout->tv_sec,\n\t\t\t\t    timeout->tv_nsec))\n\t\treturn -EINVAL;\n\n\tdatagrams = 0;\n\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\treturn err;\n\n\terr = sock_error(sock->sk);\n\tif (err) {\n\t\tdatagrams = err;\n\t\tgoto out_put;\n\t}\n\n\tentry = mmsg;\n\tcompat_entry = (struct compat_mmsghdr __user *)mmsg;\n\n\twhile (datagrams < vlen) {\n\t\t/*\n\t\t * No need to ask LSM for more than the first datagram.\n\t\t */\n\t\tif (MSG_CMSG_COMPAT & flags) {\n\t\t\terr = ___sys_recvmsg(sock, (struct user_msghdr __user *)compat_entry,\n\t\t\t\t\t     &msg_sys, flags & ~MSG_WAITFORONE,\n\t\t\t\t\t     datagrams);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\terr = __put_user(err, &compat_entry->msg_len);\n\t\t\t++compat_entry;\n\t\t} else {\n\t\t\terr = ___sys_recvmsg(sock,\n\t\t\t\t\t     (struct user_msghdr __user *)entry,\n\t\t\t\t\t     &msg_sys, flags & ~MSG_WAITFORONE,\n\t\t\t\t\t     datagrams);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\terr = put_user(err, &entry->msg_len);\n\t\t\t++entry;\n\t\t}\n\n\t\tif (err)\n\t\t\tbreak;\n\t\t++datagrams;\n\n\t\t/* MSG_WAITFORONE turns on MSG_DONTWAIT after one packet */\n\t\tif (flags & MSG_WAITFORONE)\n\t\t\tflags |= MSG_DONTWAIT;\n\n\t\tif (timeout) {\n\t\t\tktime_get_ts64(&timeout64);\n\t\t\t*timeout = timespec64_to_timespec(\n\t\t\t\t\ttimespec64_sub(end_time, timeout64));\n\t\t\tif (timeout->tv_sec < 0) {\n\t\t\t\ttimeout->tv_sec = timeout->tv_nsec = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Timeout, return less than vlen datagrams */\n\t\t\tif (timeout->tv_nsec == 0 && timeout->tv_sec == 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/* Out of band data, return right away */\n\t\tif (msg_sys.msg_flags & MSG_OOB)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\n\tif (err == 0)\n\t\tgoto out_put;\n\n\tif (datagrams == 0) {\n\t\tdatagrams = err;\n\t\tgoto out_put;\n\t}\n\n\t/*\n\t * We may return less entries than requested (vlen) if the\n\t * sock is non block and there aren't enough datagrams...\n\t */\n\tif (err != -EAGAIN) {\n\t\t/*\n\t\t * ... or  if recvmsg returns an error after we\n\t\t * received some datagrams, where we record the\n\t\t * error to return on the next call or if the\n\t\t * app asks about it using getsockopt(SO_ERROR).\n\t\t */\n\t\tsock->sk->sk_err = -err;\n\t}\nout_put:\n\tfput_light(sock->file, fput_needed);\n\n\treturn datagrams;\n}\n\nSYSCALL_DEFINE5(recvmmsg, int, fd, struct mmsghdr __user *, mmsg,\n\t\tunsigned int, vlen, unsigned int, flags,\n\t\tstruct timespec __user *, timeout)\n{\n\tint datagrams;\n\tstruct timespec timeout_sys;\n\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\n\tif (!timeout)\n\t\treturn __sys_recvmmsg(fd, mmsg, vlen, flags, NULL);\n\n\tif (copy_from_user(&timeout_sys, timeout, sizeof(timeout_sys)))\n\t\treturn -EFAULT;\n\n\tdatagrams = __sys_recvmmsg(fd, mmsg, vlen, flags, &timeout_sys);\n\n\tif (datagrams > 0 &&\n\t    copy_to_user(timeout, &timeout_sys, sizeof(timeout_sys)))\n\t\tdatagrams = -EFAULT;\n\n\treturn datagrams;\n}\n\n#ifdef __ARCH_WANT_SYS_SOCKETCALL\n/* Argument list sizes for sys_socketcall */\n#define AL(x) ((x) * sizeof(unsigned long))\nstatic const unsigned char nargs[21] = {\n\tAL(0), AL(3), AL(3), AL(3), AL(2), AL(3),\n\tAL(3), AL(3), AL(4), AL(4), AL(4), AL(6),\n\tAL(6), AL(2), AL(5), AL(5), AL(3), AL(3),\n\tAL(4), AL(5), AL(4)\n};\n\n#undef AL\n\n/*\n *\tSystem call vectors.\n *\n *\tArgument checking cleaned up. Saved 20% in size.\n *  This function doesn't need to set the kernel lock because\n *  it is set by the callees.\n */\n\nSYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)\n{\n\tunsigned long a[AUDITSC_ARGS];\n\tunsigned long a0, a1;\n\tint err;\n\tunsigned int len;\n\n\tif (call < 1 || call > SYS_SENDMMSG)\n\t\treturn -EINVAL;\n\n\tlen = nargs[call];\n\tif (len > sizeof(a))\n\t\treturn -EINVAL;\n\n\t/* copy_from_user should be SMP safe. */\n\tif (copy_from_user(a, args, len))\n\t\treturn -EFAULT;\n\n\terr = audit_socketcall(nargs[call] / sizeof(unsigned long), a);\n\tif (err)\n\t\treturn err;\n\n\ta0 = a[0];\n\ta1 = a[1];\n\n\tswitch (call) {\n\tcase SYS_SOCKET:\n\t\terr = sys_socket(a0, a1, a[2]);\n\t\tbreak;\n\tcase SYS_BIND:\n\t\terr = sys_bind(a0, (struct sockaddr __user *)a1, a[2]);\n\t\tbreak;\n\tcase SYS_CONNECT:\n\t\terr = sys_connect(a0, (struct sockaddr __user *)a1, a[2]);\n\t\tbreak;\n\tcase SYS_LISTEN:\n\t\terr = sys_listen(a0, a1);\n\t\tbreak;\n\tcase SYS_ACCEPT:\n\t\terr = sys_accept4(a0, (struct sockaddr __user *)a1,\n\t\t\t\t  (int __user *)a[2], 0);\n\t\tbreak;\n\tcase SYS_GETSOCKNAME:\n\t\terr =\n\t\t    sys_getsockname(a0, (struct sockaddr __user *)a1,\n\t\t\t\t    (int __user *)a[2]);\n\t\tbreak;\n\tcase SYS_GETPEERNAME:\n\t\terr =\n\t\t    sys_getpeername(a0, (struct sockaddr __user *)a1,\n\t\t\t\t    (int __user *)a[2]);\n\t\tbreak;\n\tcase SYS_SOCKETPAIR:\n\t\terr = sys_socketpair(a0, a1, a[2], (int __user *)a[3]);\n\t\tbreak;\n\tcase SYS_SEND:\n\t\terr = sys_send(a0, (void __user *)a1, a[2], a[3]);\n\t\tbreak;\n\tcase SYS_SENDTO:\n\t\terr = sys_sendto(a0, (void __user *)a1, a[2], a[3],\n\t\t\t\t (struct sockaddr __user *)a[4], a[5]);\n\t\tbreak;\n\tcase SYS_RECV:\n\t\terr = sys_recv(a0, (void __user *)a1, a[2], a[3]);\n\t\tbreak;\n\tcase SYS_RECVFROM:\n\t\terr = sys_recvfrom(a0, (void __user *)a1, a[2], a[3],\n\t\t\t\t   (struct sockaddr __user *)a[4],\n\t\t\t\t   (int __user *)a[5]);\n\t\tbreak;\n\tcase SYS_SHUTDOWN:\n\t\terr = sys_shutdown(a0, a1);\n\t\tbreak;\n\tcase SYS_SETSOCKOPT:\n\t\terr = sys_setsockopt(a0, a1, a[2], (char __user *)a[3], a[4]);\n\t\tbreak;\n\tcase SYS_GETSOCKOPT:\n\t\terr =\n\t\t    sys_getsockopt(a0, a1, a[2], (char __user *)a[3],\n\t\t\t\t   (int __user *)a[4]);\n\t\tbreak;\n\tcase SYS_SENDMSG:\n\t\terr = sys_sendmsg(a0, (struct user_msghdr __user *)a1, a[2]);\n\t\tbreak;\n\tcase SYS_SENDMMSG:\n\t\terr = sys_sendmmsg(a0, (struct mmsghdr __user *)a1, a[2], a[3]);\n\t\tbreak;\n\tcase SYS_RECVMSG:\n\t\terr = sys_recvmsg(a0, (struct user_msghdr __user *)a1, a[2]);\n\t\tbreak;\n\tcase SYS_RECVMMSG:\n\t\terr = sys_recvmmsg(a0, (struct mmsghdr __user *)a1, a[2], a[3],\n\t\t\t\t   (struct timespec __user *)a[4]);\n\t\tbreak;\n\tcase SYS_ACCEPT4:\n\t\terr = sys_accept4(a0, (struct sockaddr __user *)a1,\n\t\t\t\t  (int __user *)a[2], a[3]);\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn err;\n}\n\n#endif\t\t\t\t/* __ARCH_WANT_SYS_SOCKETCALL */\n\n/**\n *\tsock_register - add a socket protocol handler\n *\t@ops: description of protocol\n *\n *\tThis function is called by a protocol handler that wants to\n *\tadvertise its address family, and have it linked into the\n *\tsocket interface. The value ops->family corresponds to the\n *\tsocket system call protocol family.\n */\nint sock_register(const struct net_proto_family *ops)\n{\n\tint err;\n\n\tif (ops->family >= NPROTO) {\n\t\tpr_crit(\"protocol %d >= NPROTO(%d)\\n\", ops->family, NPROTO);\n\t\treturn -ENOBUFS;\n\t}\n\n\tspin_lock(&net_family_lock);\n\tif (rcu_dereference_protected(net_families[ops->family],\n\t\t\t\t      lockdep_is_held(&net_family_lock)))\n\t\terr = -EEXIST;\n\telse {\n\t\trcu_assign_pointer(net_families[ops->family], ops);\n\t\terr = 0;\n\t}\n\tspin_unlock(&net_family_lock);\n\n\tpr_info(\"NET: Registered protocol family %d\\n\", ops->family);\n\treturn err;\n}\nEXPORT_SYMBOL(sock_register);\n\n/**\n *\tsock_unregister - remove a protocol handler\n *\t@family: protocol family to remove\n *\n *\tThis function is called by a protocol handler that wants to\n *\tremove its address family, and have it unlinked from the\n *\tnew socket creation.\n *\n *\tIf protocol handler is a module, then it can use module reference\n *\tcounts to protect against new references. If protocol handler is not\n *\ta module then it needs to provide its own protection in\n *\tthe ops->create routine.\n */\nvoid sock_unregister(int family)\n{\n\tBUG_ON(family < 0 || family >= NPROTO);\n\n\tspin_lock(&net_family_lock);\n\tRCU_INIT_POINTER(net_families[family], NULL);\n\tspin_unlock(&net_family_lock);\n\n\tsynchronize_rcu();\n\n\tpr_info(\"NET: Unregistered protocol family %d\\n\", family);\n}\nEXPORT_SYMBOL(sock_unregister);\n\nstatic int __init sock_init(void)\n{\n\tint err;\n\t/*\n\t *      Initialize the network sysctl infrastructure.\n\t */\n\terr = net_sysctl_init();\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t *      Initialize skbuff SLAB cache\n\t */\n\tskb_init();\n\n\t/*\n\t *      Initialize the protocols module.\n\t */\n\n\tinit_inodecache();\n\n\terr = register_filesystem(&sock_fs_type);\n\tif (err)\n\t\tgoto out_fs;\n\tsock_mnt = kern_mount(&sock_fs_type);\n\tif (IS_ERR(sock_mnt)) {\n\t\terr = PTR_ERR(sock_mnt);\n\t\tgoto out_mount;\n\t}\n\n\t/* The real protocol initialization is performed in later initcalls.\n\t */\n\n#ifdef CONFIG_NETFILTER\n\terr = netfilter_init();\n\tif (err)\n\t\tgoto out;\n#endif\n\n\tptp_classifier_init();\n\nout:\n\treturn err;\n\nout_mount:\n\tunregister_filesystem(&sock_fs_type);\nout_fs:\n\tgoto out;\n}\n\ncore_initcall(sock_init);\t/* early initcall */\n\n#ifdef CONFIG_PROC_FS\nvoid socket_seq_show(struct seq_file *seq)\n{\n\tint cpu;\n\tint counter = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t    counter += per_cpu(sockets_in_use, cpu);\n\n\t/* It can be negative, by the way. 8) */\n\tif (counter < 0)\n\t\tcounter = 0;\n\n\tseq_printf(seq, \"sockets: used %d\\n\", counter);\n}\n#endif\t\t\t\t/* CONFIG_PROC_FS */\n\n#ifdef CONFIG_COMPAT\nstatic int do_siocgstamp(struct net *net, struct socket *sock,\n\t\t\t unsigned int cmd, void __user *up)\n{\n\tmm_segment_t old_fs = get_fs();\n\tstruct timeval ktv;\n\tint err;\n\n\tset_fs(KERNEL_DS);\n\terr = sock_do_ioctl(net, sock, cmd, (unsigned long)&ktv);\n\tset_fs(old_fs);\n\tif (!err)\n\t\terr = compat_put_timeval(&ktv, up);\n\n\treturn err;\n}\n\nstatic int do_siocgstampns(struct net *net, struct socket *sock,\n\t\t\t   unsigned int cmd, void __user *up)\n{\n\tmm_segment_t old_fs = get_fs();\n\tstruct timespec kts;\n\tint err;\n\n\tset_fs(KERNEL_DS);\n\terr = sock_do_ioctl(net, sock, cmd, (unsigned long)&kts);\n\tset_fs(old_fs);\n\tif (!err)\n\t\terr = compat_put_timespec(&kts, up);\n\n\treturn err;\n}\n\nstatic int dev_ifname32(struct net *net, struct compat_ifreq __user *uifr32)\n{\n\tstruct ifreq __user *uifr;\n\tint err;\n\n\tuifr = compat_alloc_user_space(sizeof(struct ifreq));\n\tif (copy_in_user(uifr, uifr32, sizeof(struct compat_ifreq)))\n\t\treturn -EFAULT;\n\n\terr = dev_ioctl(net, SIOCGIFNAME, uifr);\n\tif (err)\n\t\treturn err;\n\n\tif (copy_in_user(uifr32, uifr, sizeof(struct compat_ifreq)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int dev_ifconf(struct net *net, struct compat_ifconf __user *uifc32)\n{\n\tstruct compat_ifconf ifc32;\n\tstruct ifconf ifc;\n\tstruct ifconf __user *uifc;\n\tstruct compat_ifreq __user *ifr32;\n\tstruct ifreq __user *ifr;\n\tunsigned int i, j;\n\tint err;\n\n\tif (copy_from_user(&ifc32, uifc32, sizeof(struct compat_ifconf)))\n\t\treturn -EFAULT;\n\n\tmemset(&ifc, 0, sizeof(ifc));\n\tif (ifc32.ifcbuf == 0) {\n\t\tifc32.ifc_len = 0;\n\t\tifc.ifc_len = 0;\n\t\tifc.ifc_req = NULL;\n\t\tuifc = compat_alloc_user_space(sizeof(struct ifconf));\n\t} else {\n\t\tsize_t len = ((ifc32.ifc_len / sizeof(struct compat_ifreq)) + 1) *\n\t\t\tsizeof(struct ifreq);\n\t\tuifc = compat_alloc_user_space(sizeof(struct ifconf) + len);\n\t\tifc.ifc_len = len;\n\t\tifr = ifc.ifc_req = (void __user *)(uifc + 1);\n\t\tifr32 = compat_ptr(ifc32.ifcbuf);\n\t\tfor (i = 0; i < ifc32.ifc_len; i += sizeof(struct compat_ifreq)) {\n\t\t\tif (copy_in_user(ifr, ifr32, sizeof(struct compat_ifreq)))\n\t\t\t\treturn -EFAULT;\n\t\t\tifr++;\n\t\t\tifr32++;\n\t\t}\n\t}\n\tif (copy_to_user(uifc, &ifc, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\terr = dev_ioctl(net, SIOCGIFCONF, uifc);\n\tif (err)\n\t\treturn err;\n\n\tif (copy_from_user(&ifc, uifc, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\tifr = ifc.ifc_req;\n\tifr32 = compat_ptr(ifc32.ifcbuf);\n\tfor (i = 0, j = 0;\n\t     i + sizeof(struct compat_ifreq) <= ifc32.ifc_len && j < ifc.ifc_len;\n\t     i += sizeof(struct compat_ifreq), j += sizeof(struct ifreq)) {\n\t\tif (copy_in_user(ifr32, ifr, sizeof(struct compat_ifreq)))\n\t\t\treturn -EFAULT;\n\t\tifr32++;\n\t\tifr++;\n\t}\n\n\tif (ifc32.ifcbuf == 0) {\n\t\t/* Translate from 64-bit structure multiple to\n\t\t * a 32-bit one.\n\t\t */\n\t\ti = ifc.ifc_len;\n\t\ti = ((i / sizeof(struct ifreq)) * sizeof(struct compat_ifreq));\n\t\tifc32.ifc_len = i;\n\t} else {\n\t\tifc32.ifc_len = i;\n\t}\n\tif (copy_to_user(uifc32, &ifc32, sizeof(struct compat_ifconf)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int ethtool_ioctl(struct net *net, struct compat_ifreq __user *ifr32)\n{\n\tstruct compat_ethtool_rxnfc __user *compat_rxnfc;\n\tbool convert_in = false, convert_out = false;\n\tsize_t buf_size = ALIGN(sizeof(struct ifreq), 8);\n\tstruct ethtool_rxnfc __user *rxnfc;\n\tstruct ifreq __user *ifr;\n\tu32 rule_cnt = 0, actual_rule_cnt;\n\tu32 ethcmd;\n\tu32 data;\n\tint ret;\n\n\tif (get_user(data, &ifr32->ifr_ifru.ifru_data))\n\t\treturn -EFAULT;\n\n\tcompat_rxnfc = compat_ptr(data);\n\n\tif (get_user(ethcmd, &compat_rxnfc->cmd))\n\t\treturn -EFAULT;\n\n\t/* Most ethtool structures are defined without padding.\n\t * Unfortunately struct ethtool_rxnfc is an exception.\n\t */\n\tswitch (ethcmd) {\n\tdefault:\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRLALL:\n\t\t/* Buffer size is variable */\n\t\tif (get_user(rule_cnt, &compat_rxnfc->rule_cnt))\n\t\t\treturn -EFAULT;\n\t\tif (rule_cnt > KMALLOC_MAX_SIZE / sizeof(u32))\n\t\t\treturn -ENOMEM;\n\t\tbuf_size += rule_cnt * sizeof(u32);\n\t\t/* fall through */\n\tcase ETHTOOL_GRXRINGS:\n\tcase ETHTOOL_GRXCLSRLCNT:\n\tcase ETHTOOL_GRXCLSRULE:\n\tcase ETHTOOL_SRXCLSRLINS:\n\t\tconvert_out = true;\n\t\t/* fall through */\n\tcase ETHTOOL_SRXCLSRLDEL:\n\t\tbuf_size += sizeof(struct ethtool_rxnfc);\n\t\tconvert_in = true;\n\t\tbreak;\n\t}\n\n\tifr = compat_alloc_user_space(buf_size);\n\trxnfc = (void __user *)ifr + ALIGN(sizeof(struct ifreq), 8);\n\n\tif (copy_in_user(&ifr->ifr_name, &ifr32->ifr_name, IFNAMSIZ))\n\t\treturn -EFAULT;\n\n\tif (put_user(convert_in ? rxnfc : compat_ptr(data),\n\t\t     &ifr->ifr_ifru.ifru_data))\n\t\treturn -EFAULT;\n\n\tif (convert_in) {\n\t\t/* We expect there to be holes between fs.m_ext and\n\t\t * fs.ring_cookie and at the end of fs, but nowhere else.\n\t\t */\n\t\tBUILD_BUG_ON(offsetof(struct compat_ethtool_rxnfc, fs.m_ext) +\n\t\t\t     sizeof(compat_rxnfc->fs.m_ext) !=\n\t\t\t     offsetof(struct ethtool_rxnfc, fs.m_ext) +\n\t\t\t     sizeof(rxnfc->fs.m_ext));\n\t\tBUILD_BUG_ON(\n\t\t\toffsetof(struct compat_ethtool_rxnfc, fs.location) -\n\t\t\toffsetof(struct compat_ethtool_rxnfc, fs.ring_cookie) !=\n\t\t\toffsetof(struct ethtool_rxnfc, fs.location) -\n\t\t\toffsetof(struct ethtool_rxnfc, fs.ring_cookie));\n\n\t\tif (copy_in_user(rxnfc, compat_rxnfc,\n\t\t\t\t (void __user *)(&rxnfc->fs.m_ext + 1) -\n\t\t\t\t (void __user *)rxnfc) ||\n\t\t    copy_in_user(&rxnfc->fs.ring_cookie,\n\t\t\t\t &compat_rxnfc->fs.ring_cookie,\n\t\t\t\t (void __user *)(&rxnfc->fs.location + 1) -\n\t\t\t\t (void __user *)&rxnfc->fs.ring_cookie) ||\n\t\t    copy_in_user(&rxnfc->rule_cnt, &compat_rxnfc->rule_cnt,\n\t\t\t\t sizeof(rxnfc->rule_cnt)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = dev_ioctl(net, SIOCETHTOOL, ifr);\n\tif (ret)\n\t\treturn ret;\n\n\tif (convert_out) {\n\t\tif (copy_in_user(compat_rxnfc, rxnfc,\n\t\t\t\t (const void __user *)(&rxnfc->fs.m_ext + 1) -\n\t\t\t\t (const void __user *)rxnfc) ||\n\t\t    copy_in_user(&compat_rxnfc->fs.ring_cookie,\n\t\t\t\t &rxnfc->fs.ring_cookie,\n\t\t\t\t (const void __user *)(&rxnfc->fs.location + 1) -\n\t\t\t\t (const void __user *)&rxnfc->fs.ring_cookie) ||\n\t\t    copy_in_user(&compat_rxnfc->rule_cnt, &rxnfc->rule_cnt,\n\t\t\t\t sizeof(rxnfc->rule_cnt)))\n\t\t\treturn -EFAULT;\n\n\t\tif (ethcmd == ETHTOOL_GRXCLSRLALL) {\n\t\t\t/* As an optimisation, we only copy the actual\n\t\t\t * number of rules that the underlying\n\t\t\t * function returned.  Since Mallory might\n\t\t\t * change the rule count in user memory, we\n\t\t\t * check that it is less than the rule count\n\t\t\t * originally given (as the user buffer size),\n\t\t\t * which has been range-checked.\n\t\t\t */\n\t\t\tif (get_user(actual_rule_cnt, &rxnfc->rule_cnt))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (actual_rule_cnt < rule_cnt)\n\t\t\t\trule_cnt = actual_rule_cnt;\n\t\t\tif (copy_in_user(&compat_rxnfc->rule_locs[0],\n\t\t\t\t\t &rxnfc->rule_locs[0],\n\t\t\t\t\t rule_cnt * sizeof(u32)))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int compat_siocwandev(struct net *net, struct compat_ifreq __user *uifr32)\n{\n\tvoid __user *uptr;\n\tcompat_uptr_t uptr32;\n\tstruct ifreq __user *uifr;\n\n\tuifr = compat_alloc_user_space(sizeof(*uifr));\n\tif (copy_in_user(uifr, uifr32, sizeof(struct compat_ifreq)))\n\t\treturn -EFAULT;\n\n\tif (get_user(uptr32, &uifr32->ifr_settings.ifs_ifsu))\n\t\treturn -EFAULT;\n\n\tuptr = compat_ptr(uptr32);\n\n\tif (put_user(uptr, &uifr->ifr_settings.ifs_ifsu.raw_hdlc))\n\t\treturn -EFAULT;\n\n\treturn dev_ioctl(net, SIOCWANDEV, uifr);\n}\n\nstatic int bond_ioctl(struct net *net, unsigned int cmd,\n\t\t\t struct compat_ifreq __user *ifr32)\n{\n\tstruct ifreq kifr;\n\tmm_segment_t old_fs;\n\tint err;\n\n\tswitch (cmd) {\n\tcase SIOCBONDENSLAVE:\n\tcase SIOCBONDRELEASE:\n\tcase SIOCBONDSETHWADDR:\n\tcase SIOCBONDCHANGEACTIVE:\n\t\tif (copy_from_user(&kifr, ifr32, sizeof(struct compat_ifreq)))\n\t\t\treturn -EFAULT;\n\n\t\told_fs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t\terr = dev_ioctl(net, cmd,\n\t\t\t\t(struct ifreq __user __force *) &kifr);\n\t\tset_fs(old_fs);\n\n\t\treturn err;\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n}\n\n/* Handle ioctls that use ifreq::ifr_data and just need struct ifreq converted */\nstatic int compat_ifr_data_ioctl(struct net *net, unsigned int cmd,\n\t\t\t\t struct compat_ifreq __user *u_ifreq32)\n{\n\tstruct ifreq __user *u_ifreq64;\n\tchar tmp_buf[IFNAMSIZ];\n\tvoid __user *data64;\n\tu32 data32;\n\n\tif (copy_from_user(&tmp_buf[0], &(u_ifreq32->ifr_ifrn.ifrn_name[0]),\n\t\t\t   IFNAMSIZ))\n\t\treturn -EFAULT;\n\tif (get_user(data32, &u_ifreq32->ifr_ifru.ifru_data))\n\t\treturn -EFAULT;\n\tdata64 = compat_ptr(data32);\n\n\tu_ifreq64 = compat_alloc_user_space(sizeof(*u_ifreq64));\n\n\tif (copy_to_user(&u_ifreq64->ifr_ifrn.ifrn_name[0], &tmp_buf[0],\n\t\t\t IFNAMSIZ))\n\t\treturn -EFAULT;\n\tif (put_user(data64, &u_ifreq64->ifr_ifru.ifru_data))\n\t\treturn -EFAULT;\n\n\treturn dev_ioctl(net, cmd, u_ifreq64);\n}\n\nstatic int dev_ifsioc(struct net *net, struct socket *sock,\n\t\t\t unsigned int cmd, struct compat_ifreq __user *uifr32)\n{\n\tstruct ifreq __user *uifr;\n\tint err;\n\n\tuifr = compat_alloc_user_space(sizeof(*uifr));\n\tif (copy_in_user(uifr, uifr32, sizeof(*uifr32)))\n\t\treturn -EFAULT;\n\n\terr = sock_do_ioctl(net, sock, cmd, (unsigned long)uifr);\n\n\tif (!err) {\n\t\tswitch (cmd) {\n\t\tcase SIOCGIFFLAGS:\n\t\tcase SIOCGIFMETRIC:\n\t\tcase SIOCGIFMTU:\n\t\tcase SIOCGIFMEM:\n\t\tcase SIOCGIFHWADDR:\n\t\tcase SIOCGIFINDEX:\n\t\tcase SIOCGIFADDR:\n\t\tcase SIOCGIFBRDADDR:\n\t\tcase SIOCGIFDSTADDR:\n\t\tcase SIOCGIFNETMASK:\n\t\tcase SIOCGIFPFLAGS:\n\t\tcase SIOCGIFTXQLEN:\n\t\tcase SIOCGMIIPHY:\n\t\tcase SIOCGMIIREG:\n\t\t\tif (copy_in_user(uifr32, uifr, sizeof(*uifr32)))\n\t\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn err;\n}\n\nstatic int compat_sioc_ifmap(struct net *net, unsigned int cmd,\n\t\t\tstruct compat_ifreq __user *uifr32)\n{\n\tstruct ifreq ifr;\n\tstruct compat_ifmap __user *uifmap32;\n\tmm_segment_t old_fs;\n\tint err;\n\n\tuifmap32 = &uifr32->ifr_ifru.ifru_map;\n\terr = copy_from_user(&ifr, uifr32, sizeof(ifr.ifr_name));\n\terr |= get_user(ifr.ifr_map.mem_start, &uifmap32->mem_start);\n\terr |= get_user(ifr.ifr_map.mem_end, &uifmap32->mem_end);\n\terr |= get_user(ifr.ifr_map.base_addr, &uifmap32->base_addr);\n\terr |= get_user(ifr.ifr_map.irq, &uifmap32->irq);\n\terr |= get_user(ifr.ifr_map.dma, &uifmap32->dma);\n\terr |= get_user(ifr.ifr_map.port, &uifmap32->port);\n\tif (err)\n\t\treturn -EFAULT;\n\n\told_fs = get_fs();\n\tset_fs(KERNEL_DS);\n\terr = dev_ioctl(net, cmd, (void  __user __force *)&ifr);\n\tset_fs(old_fs);\n\n\tif (cmd == SIOCGIFMAP && !err) {\n\t\terr = copy_to_user(uifr32, &ifr, sizeof(ifr.ifr_name));\n\t\terr |= put_user(ifr.ifr_map.mem_start, &uifmap32->mem_start);\n\t\terr |= put_user(ifr.ifr_map.mem_end, &uifmap32->mem_end);\n\t\terr |= put_user(ifr.ifr_map.base_addr, &uifmap32->base_addr);\n\t\terr |= put_user(ifr.ifr_map.irq, &uifmap32->irq);\n\t\terr |= put_user(ifr.ifr_map.dma, &uifmap32->dma);\n\t\terr |= put_user(ifr.ifr_map.port, &uifmap32->port);\n\t\tif (err)\n\t\t\terr = -EFAULT;\n\t}\n\treturn err;\n}\n\nstruct rtentry32 {\n\tu32\t\trt_pad1;\n\tstruct sockaddr rt_dst;         /* target address               */\n\tstruct sockaddr rt_gateway;     /* gateway addr (RTF_GATEWAY)   */\n\tstruct sockaddr rt_genmask;     /* target network mask (IP)     */\n\tunsigned short\trt_flags;\n\tshort\t\trt_pad2;\n\tu32\t\trt_pad3;\n\tunsigned char\trt_tos;\n\tunsigned char\trt_class;\n\tshort\t\trt_pad4;\n\tshort\t\trt_metric;      /* +1 for binary compatibility! */\n\t/* char * */ u32 rt_dev;        /* forcing the device at add    */\n\tu32\t\trt_mtu;         /* per route MTU/Window         */\n\tu32\t\trt_window;      /* Window clamping              */\n\tunsigned short  rt_irtt;        /* Initial RTT                  */\n};\n\nstruct in6_rtmsg32 {\n\tstruct in6_addr\t\trtmsg_dst;\n\tstruct in6_addr\t\trtmsg_src;\n\tstruct in6_addr\t\trtmsg_gateway;\n\tu32\t\t\trtmsg_type;\n\tu16\t\t\trtmsg_dst_len;\n\tu16\t\t\trtmsg_src_len;\n\tu32\t\t\trtmsg_metric;\n\tu32\t\t\trtmsg_info;\n\tu32\t\t\trtmsg_flags;\n\ts32\t\t\trtmsg_ifindex;\n};\n\nstatic int routing_ioctl(struct net *net, struct socket *sock,\n\t\t\t unsigned int cmd, void __user *argp)\n{\n\tint ret;\n\tvoid *r = NULL;\n\tstruct in6_rtmsg r6;\n\tstruct rtentry r4;\n\tchar devname[16];\n\tu32 rtdev;\n\tmm_segment_t old_fs = get_fs();\n\n\tif (sock && sock->sk && sock->sk->sk_family == AF_INET6) { /* ipv6 */\n\t\tstruct in6_rtmsg32 __user *ur6 = argp;\n\t\tret = copy_from_user(&r6.rtmsg_dst, &(ur6->rtmsg_dst),\n\t\t\t3 * sizeof(struct in6_addr));\n\t\tret |= get_user(r6.rtmsg_type, &(ur6->rtmsg_type));\n\t\tret |= get_user(r6.rtmsg_dst_len, &(ur6->rtmsg_dst_len));\n\t\tret |= get_user(r6.rtmsg_src_len, &(ur6->rtmsg_src_len));\n\t\tret |= get_user(r6.rtmsg_metric, &(ur6->rtmsg_metric));\n\t\tret |= get_user(r6.rtmsg_info, &(ur6->rtmsg_info));\n\t\tret |= get_user(r6.rtmsg_flags, &(ur6->rtmsg_flags));\n\t\tret |= get_user(r6.rtmsg_ifindex, &(ur6->rtmsg_ifindex));\n\n\t\tr = (void *) &r6;\n\t} else { /* ipv4 */\n\t\tstruct rtentry32 __user *ur4 = argp;\n\t\tret = copy_from_user(&r4.rt_dst, &(ur4->rt_dst),\n\t\t\t\t\t3 * sizeof(struct sockaddr));\n\t\tret |= get_user(r4.rt_flags, &(ur4->rt_flags));\n\t\tret |= get_user(r4.rt_metric, &(ur4->rt_metric));\n\t\tret |= get_user(r4.rt_mtu, &(ur4->rt_mtu));\n\t\tret |= get_user(r4.rt_window, &(ur4->rt_window));\n\t\tret |= get_user(r4.rt_irtt, &(ur4->rt_irtt));\n\t\tret |= get_user(rtdev, &(ur4->rt_dev));\n\t\tif (rtdev) {\n\t\t\tret |= copy_from_user(devname, compat_ptr(rtdev), 15);\n\t\t\tr4.rt_dev = (char __user __force *)devname;\n\t\t\tdevname[15] = 0;\n\t\t} else\n\t\t\tr4.rt_dev = NULL;\n\n\t\tr = (void *) &r4;\n\t}\n\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tset_fs(KERNEL_DS);\n\tret = sock_do_ioctl(net, sock, cmd, (unsigned long) r);\n\tset_fs(old_fs);\n\nout:\n\treturn ret;\n}\n\n/* Since old style bridge ioctl's endup using SIOCDEVPRIVATE\n * for some operations; this forces use of the newer bridge-utils that\n * use compatible ioctls\n */\nstatic int old_bridge_ioctl(compat_ulong_t __user *argp)\n{\n\tcompat_ulong_t tmp;\n\n\tif (get_user(tmp, argp))\n\t\treturn -EFAULT;\n\tif (tmp == BRCTL_GET_VERSION)\n\t\treturn BRCTL_VERSION + 1;\n\treturn -EINVAL;\n}\n\nstatic int compat_sock_ioctl_trans(struct file *file, struct socket *sock,\n\t\t\t unsigned int cmd, unsigned long arg)\n{\n\tvoid __user *argp = compat_ptr(arg);\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\n\tif (cmd >= SIOCDEVPRIVATE && cmd <= (SIOCDEVPRIVATE + 15))\n\t\treturn compat_ifr_data_ioctl(net, cmd, argp);\n\n\tswitch (cmd) {\n\tcase SIOCSIFBR:\n\tcase SIOCGIFBR:\n\t\treturn old_bridge_ioctl(argp);\n\tcase SIOCGIFNAME:\n\t\treturn dev_ifname32(net, argp);\n\tcase SIOCGIFCONF:\n\t\treturn dev_ifconf(net, argp);\n\tcase SIOCETHTOOL:\n\t\treturn ethtool_ioctl(net, argp);\n\tcase SIOCWANDEV:\n\t\treturn compat_siocwandev(net, argp);\n\tcase SIOCGIFMAP:\n\tcase SIOCSIFMAP:\n\t\treturn compat_sioc_ifmap(net, cmd, argp);\n\tcase SIOCBONDENSLAVE:\n\tcase SIOCBONDRELEASE:\n\tcase SIOCBONDSETHWADDR:\n\tcase SIOCBONDCHANGEACTIVE:\n\t\treturn bond_ioctl(net, cmd, argp);\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\t\treturn routing_ioctl(net, sock, cmd, argp);\n\tcase SIOCGSTAMP:\n\t\treturn do_siocgstamp(net, sock, cmd, argp);\n\tcase SIOCGSTAMPNS:\n\t\treturn do_siocgstampns(net, sock, cmd, argp);\n\tcase SIOCBONDSLAVEINFOQUERY:\n\tcase SIOCBONDINFOQUERY:\n\tcase SIOCSHWTSTAMP:\n\tcase SIOCGHWTSTAMP:\n\t\treturn compat_ifr_data_ioctl(net, cmd, argp);\n\n\tcase FIOSETOWN:\n\tcase SIOCSPGRP:\n\tcase FIOGETOWN:\n\tcase SIOCGPGRP:\n\tcase SIOCBRADDBR:\n\tcase SIOCBRDELBR:\n\tcase SIOCGIFVLAN:\n\tcase SIOCSIFVLAN:\n\tcase SIOCADDDLCI:\n\tcase SIOCDELDLCI:\n\tcase SIOCGSKNS:\n\t\treturn sock_ioctl(file, cmd, arg);\n\n\tcase SIOCGIFFLAGS:\n\tcase SIOCSIFFLAGS:\n\tcase SIOCGIFMETRIC:\n\tcase SIOCSIFMETRIC:\n\tcase SIOCGIFMTU:\n\tcase SIOCSIFMTU:\n\tcase SIOCGIFMEM:\n\tcase SIOCSIFMEM:\n\tcase SIOCGIFHWADDR:\n\tcase SIOCSIFHWADDR:\n\tcase SIOCADDMULTI:\n\tcase SIOCDELMULTI:\n\tcase SIOCGIFINDEX:\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCSIFHWBROADCAST:\n\tcase SIOCDIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCSIFPFLAGS:\n\tcase SIOCGIFPFLAGS:\n\tcase SIOCGIFTXQLEN:\n\tcase SIOCSIFTXQLEN:\n\tcase SIOCBRADDIF:\n\tcase SIOCBRDELIF:\n\tcase SIOCSIFNAME:\n\tcase SIOCGMIIPHY:\n\tcase SIOCGMIIREG:\n\tcase SIOCSMIIREG:\n\t\treturn dev_ifsioc(net, sock, cmd, argp);\n\n\tcase SIOCSARP:\n\tcase SIOCGARP:\n\tcase SIOCDARP:\n\tcase SIOCATMARK:\n\t\treturn sock_do_ioctl(net, sock, cmd, arg);\n\t}\n\n\treturn -ENOIOCTLCMD;\n}\n\nstatic long compat_sock_ioctl(struct file *file, unsigned int cmd,\n\t\t\t      unsigned long arg)\n{\n\tstruct socket *sock = file->private_data;\n\tint ret = -ENOIOCTLCMD;\n\tstruct sock *sk;\n\tstruct net *net;\n\n\tsk = sock->sk;\n\tnet = sock_net(sk);\n\n\tif (sock->ops->compat_ioctl)\n\t\tret = sock->ops->compat_ioctl(sock, cmd, arg);\n\n\tif (ret == -ENOIOCTLCMD &&\n\t    (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST))\n\t\tret = compat_wext_handle_ioctl(net, cmd, arg);\n\n\tif (ret == -ENOIOCTLCMD)\n\t\tret = compat_sock_ioctl_trans(file, sock, cmd, arg);\n\n\treturn ret;\n}\n#endif\n\nint kernel_bind(struct socket *sock, struct sockaddr *addr, int addrlen)\n{\n\treturn sock->ops->bind(sock, addr, addrlen);\n}\nEXPORT_SYMBOL(kernel_bind);\n\nint kernel_listen(struct socket *sock, int backlog)\n{\n\treturn sock->ops->listen(sock, backlog);\n}\nEXPORT_SYMBOL(kernel_listen);\n\nint kernel_accept(struct socket *sock, struct socket **newsock, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\n\terr = sock_create_lite(sk->sk_family, sk->sk_type, sk->sk_protocol,\n\t\t\t       newsock);\n\tif (err < 0)\n\t\tgoto done;\n\n\terr = sock->ops->accept(sock, *newsock, flags, true);\n\tif (err < 0) {\n\t\tsock_release(*newsock);\n\t\t*newsock = NULL;\n\t\tgoto done;\n\t}\n\n\t(*newsock)->ops = sock->ops;\n\t__module_get((*newsock)->ops->owner);\n\ndone:\n\treturn err;\n}\nEXPORT_SYMBOL(kernel_accept);\n\nint kernel_connect(struct socket *sock, struct sockaddr *addr, int addrlen,\n\t\t   int flags)\n{\n\treturn sock->ops->connect(sock, addr, addrlen, flags);\n}\nEXPORT_SYMBOL(kernel_connect);\n\nint kernel_getsockname(struct socket *sock, struct sockaddr *addr,\n\t\t\t int *addrlen)\n{\n\treturn sock->ops->getname(sock, addr, addrlen, 0);\n}\nEXPORT_SYMBOL(kernel_getsockname);\n\nint kernel_getpeername(struct socket *sock, struct sockaddr *addr,\n\t\t\t int *addrlen)\n{\n\treturn sock->ops->getname(sock, addr, addrlen, 1);\n}\nEXPORT_SYMBOL(kernel_getpeername);\n\nint kernel_getsockopt(struct socket *sock, int level, int optname,\n\t\t\tchar *optval, int *optlen)\n{\n\tmm_segment_t oldfs = get_fs();\n\tchar __user *uoptval;\n\tint __user *uoptlen;\n\tint err;\n\n\tuoptval = (char __user __force *) optval;\n\tuoptlen = (int __user __force *) optlen;\n\n\tset_fs(KERNEL_DS);\n\tif (level == SOL_SOCKET)\n\t\terr = sock_getsockopt(sock, level, optname, uoptval, uoptlen);\n\telse\n\t\terr = sock->ops->getsockopt(sock, level, optname, uoptval,\n\t\t\t\t\t    uoptlen);\n\tset_fs(oldfs);\n\treturn err;\n}\nEXPORT_SYMBOL(kernel_getsockopt);\n\nint kernel_setsockopt(struct socket *sock, int level, int optname,\n\t\t\tchar *optval, unsigned int optlen)\n{\n\tmm_segment_t oldfs = get_fs();\n\tchar __user *uoptval;\n\tint err;\n\n\tuoptval = (char __user __force *) optval;\n\n\tset_fs(KERNEL_DS);\n\tif (level == SOL_SOCKET)\n\t\terr = sock_setsockopt(sock, level, optname, uoptval, optlen);\n\telse\n\t\terr = sock->ops->setsockopt(sock, level, optname, uoptval,\n\t\t\t\t\t    optlen);\n\tset_fs(oldfs);\n\treturn err;\n}\nEXPORT_SYMBOL(kernel_setsockopt);\n\nint kernel_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t    size_t size, int flags)\n{\n\tif (sock->ops->sendpage)\n\t\treturn sock->ops->sendpage(sock, page, offset, size, flags);\n\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}\nEXPORT_SYMBOL(kernel_sendpage);\n\nint kernel_sock_ioctl(struct socket *sock, int cmd, unsigned long arg)\n{\n\tmm_segment_t oldfs = get_fs();\n\tint err;\n\n\tset_fs(KERNEL_DS);\n\terr = sock->ops->ioctl(sock, cmd, arg);\n\tset_fs(oldfs);\n\n\treturn err;\n}\nEXPORT_SYMBOL(kernel_sock_ioctl);\n\nint kernel_sock_shutdown(struct socket *sock, enum sock_shutdown_cmd how)\n{\n\treturn sock->ops->shutdown(sock, how);\n}\nEXPORT_SYMBOL(kernel_sock_shutdown);\n"], "filenames": ["include/linux/errqueue.h", "net/core/skbuff.c", "net/socket.c"], "buggy_code_start_loc": [22, 3796, 709], "buggy_code_end_loc": [22, 3892, 710], "fixing_code_start_loc": [23, 3796, 709], "fixing_code_end_loc": [25, 3897, 710], "type": "CWE-125", "message": "The TCP stack in the Linux kernel through 4.10.6 mishandles the SCM_TIMESTAMPING_OPT_STATS feature, which allows local users to obtain sensitive information from the kernel's internal socket data structures or cause a denial of service (out-of-bounds read) via crafted system calls, related to net/core/skbuff.c and net/socket.c.", "other": {"cve": {"id": "CVE-2017-7277", "sourceIdentifier": "cve@mitre.org", "published": "2017-03-28T06:59:00.143", "lastModified": "2017-03-31T17:02:01.357", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The TCP stack in the Linux kernel through 4.10.6 mishandles the SCM_TIMESTAMPING_OPT_STATS feature, which allows local users to obtain sensitive information from the kernel's internal socket data structures or cause a denial of service (out-of-bounds read) via crafted system calls, related to net/core/skbuff.c and net/socket.c."}, {"lang": "es", "value": "La pila TCP en el kernel de Linux hasta la versi\u00f3n 4.10.6 no maneja adecuadamente la funcionalidad SCM_TIMESTAMPING_OPT_STATS, lo que permite a usuarios locales obtener informaci\u00f3n sensible de la estructuras internas de datos del socket del kernel o provocar una denegaci\u00f3n de servicio (lectura fuera de l\u00edmites) a trav\u00e9s de llamadas al sistema manipuladas, relacionado con net/core/skbuff.c y net/socket.c."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 6.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 9.2, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.10.6", "matchCriteriaId": "355E978C-AFCC-4B61-A4BC-59CCD6CF35C9"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=4ef1b2869447411ad3ef91ad7d4891a83c1a509a", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=8605330aac5a5785630aec8f64378a54891937cc", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/97141", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/4ef1b2869447411ad3ef91ad7d4891a83c1a509a", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/8605330aac5a5785630aec8f64378a54891937cc", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://lkml.org/lkml/2017/3/15/485", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://patchwork.ozlabs.org/patch/740636/", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://patchwork.ozlabs.org/patch/740639/", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/4ef1b2869447411ad3ef91ad7d4891a83c1a509a"}}