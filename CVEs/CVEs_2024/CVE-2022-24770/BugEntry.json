{"buggy_code": ["from __future__ import annotations\n\nimport csv\nimport datetime\nimport io\nimport json\nimport os\nfrom abc import ABC, abstractmethod\nfrom typing import Any, List, Optional\n\nimport gradio as gr\nfrom gradio import encryptor\n\n\nclass FlaggingCallback(ABC):\n    \"\"\"\n    An abstract class for defining the methods that any FlaggingCallback should have.\n    \"\"\"\n\n    @abstractmethod\n    def setup(self, flagging_dir: str):\n        \"\"\"\n        This method should be overridden and ensure that everything is set up correctly for flag().\n        This method gets called once at the beginning of the Interface.launch() method.\n        Parameters:\n        flagging_dir: A string, typically containing the path to the directory where the flagging file should be storied (provided as an argument to Interface.__init__()).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def flag(\n        self,\n        interface: gr.Interface,\n        input_data: List[Any],\n        output_data: List[Any],\n        flag_option: Optional[str] = None,\n        flag_index: Optional[int] = None,\n        username: Optional[str] = None,\n    ) -> int:\n        \"\"\"\n        This method should be overridden by the FlaggingCallback subclass and may contain optional additional arguments.\n        This gets called every time the <flag> button is pressed.\n        Parameters:\n        interface: The Interface object that is being used to launch the flagging interface.\n        input_data: The input data to be flagged.\n        output_data: The output data to be flagged.\n        flag_option (optional): In the case that flagging_options are provided, the flag option that is being used.\n        flag_index (optional): The index of the sample that is being flagged.\n        username (optional): The username of the user that is flagging the data, if logged in.\n        Returns:\n        (int) The total number of samples that have been flagged.\n        \"\"\"\n        pass\n\n\nclass SimpleCSVLogger(FlaggingCallback):\n    \"\"\"\n    A simple example implementation of the FlaggingCallback abstract class\n    provided for illustrative purposes.\n    \"\"\"\n\n    def setup(self, flagging_dir: str):\n        self.flagging_dir = flagging_dir\n        os.makedirs(flagging_dir, exist_ok=True)\n\n    def flag(\n        self,\n        interface: gr.Interface,\n        input_data: List[Any],\n        output_data: List[Any],\n        flag_option: Optional[str] = None,\n        flag_index: Optional[int] = None,\n        username: Optional[str] = None,\n    ) -> int:\n        flagging_dir = self.flagging_dir\n        log_filepath = \"{}/log.csv\".format(flagging_dir)\n\n        csv_data = []\n        for i, input in enumerate(interface.input_components):\n            csv_data.append(\n                input.save_flagged(\n                    flagging_dir,\n                    interface.config[\"input_components\"][i][\"label\"],\n                    input_data[i],\n                    None,\n                )\n            )\n        for i, output in enumerate(interface.output_components):\n            csv_data.append(\n                output.save_flagged(\n                    flagging_dir,\n                    interface.config[\"output_components\"][i][\"label\"],\n                    output_data[i],\n                    None,\n                )\n                if output_data[i] is not None\n                else \"\"\n            )\n\n        with open(log_filepath, \"a\", newline=\"\") as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(csv_data)\n\n        with open(log_filepath, \"r\") as csvfile:\n            line_count = len([None for row in csv.reader(csvfile)]) - 1\n        return line_count\n\n\nclass CSVLogger(FlaggingCallback):\n    \"\"\"\n    The default implementation of the FlaggingCallback abstract class.\n    Logs the input and output data to a CSV file. Supports encryption.\n    \"\"\"\n\n    def setup(self, flagging_dir: str):\n        self.flagging_dir = flagging_dir\n        os.makedirs(flagging_dir, exist_ok=True)\n\n    def flag(\n        self,\n        interface: gr.Interface,\n        input_data: List[Any],\n        output_data: List[Any],\n        flag_option: Optional[str] = None,\n        flag_index: Optional[int] = None,\n        username: Optional[str] = None,\n    ) -> int:\n        flagging_dir = self.flagging_dir\n        log_fp = \"{}/log.csv\".format(flagging_dir)\n        encryption_key = interface.encryption_key if interface.encrypt else None\n        is_new = not os.path.exists(log_fp)\n        output_only_mode = input_data is None\n\n        if flag_index is None:\n            csv_data = []\n            if not output_only_mode:\n                for i, input in enumerate(interface.input_components):\n                    csv_data.append(\n                        input.save_flagged(\n                            flagging_dir,\n                            interface.config[\"input_components\"][i][\"label\"],\n                            input_data[i],\n                            encryption_key,\n                        )\n                    )\n            for i, output in enumerate(interface.output_components):\n                csv_data.append(\n                    output.save_flagged(\n                        flagging_dir,\n                        interface.config[\"output_components\"][i][\"label\"],\n                        output_data[i],\n                        encryption_key,\n                    )\n                    if output_data[i] is not None\n                    else \"\"\n                )\n            if not output_only_mode:\n                if flag_option is not None:\n                    csv_data.append(flag_option)\n                if username is not None:\n                    csv_data.append(username)\n                csv_data.append(str(datetime.datetime.now()))\n            if is_new:\n                headers = []\n                if not output_only_mode:\n                    headers += [\n                        interface[\"label\"]\n                        for interface in interface.config[\"input_components\"]\n                    ]\n                headers += [\n                    interface[\"label\"]\n                    for interface in interface.config[\"output_components\"]\n                ]\n                if not output_only_mode:\n                    if interface.flagging_options is not None:\n                        headers.append(\"flag\")\n                    if username is not None:\n                        headers.append(\"username\")\n                    headers.append(\"timestamp\")\n\n        def replace_flag_at_index(file_content):\n            file_content = io.StringIO(file_content)\n            content = list(csv.reader(file_content))\n            header = content[0]\n            flag_col_index = header.index(\"flag\")\n            content[flag_index][flag_col_index] = flag_option\n            output = io.StringIO()\n            writer = csv.writer(output)\n            writer.writerows(content)\n            return output.getvalue()\n\n        if interface.encrypt:\n            output = io.StringIO()\n            if not is_new:\n                with open(log_fp, \"rb\") as csvfile:\n                    encrypted_csv = csvfile.read()\n                    decrypted_csv = encryptor.decrypt(\n                        interface.encryption_key, encrypted_csv\n                    )\n                    file_content = decrypted_csv.decode()\n                    if flag_index is not None:\n                        file_content = replace_flag_at_index(file_content)\n                    output.write(file_content)\n            writer = csv.writer(output)\n            if flag_index is None:\n                if is_new:\n                    writer.writerow(headers)\n                writer.writerow(csv_data)\n            with open(log_fp, \"wb\") as csvfile:\n                csvfile.write(\n                    encryptor.encrypt(\n                        interface.encryption_key, output.getvalue().encode()\n                    )\n                )\n        else:\n            if flag_index is None:\n                with open(log_fp, \"a\", newline=\"\") as csvfile:\n                    writer = csv.writer(csvfile)\n                    if is_new:\n                        writer.writerow(headers)\n                    writer.writerow(csv_data)\n            else:\n                with open(log_fp) as csvfile:\n                    file_content = csvfile.read()\n                    file_content = replace_flag_at_index(file_content)\n                with open(\n                    log_fp, \"w\", newline=\"\"\n                ) as csvfile:  # newline parameter needed for Windows\n                    csvfile.write(file_content)\n        with open(log_fp, \"r\") as csvfile:\n            line_count = len([None for row in csv.reader(csvfile)]) - 1\n        return line_count\n\n\nclass HuggingFaceDatasetSaver(FlaggingCallback):\n    \"\"\"\n    A FlaggingCallback that saves flagged data to a HuggingFace dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        hf_foken: str,\n        dataset_name: str,\n        organization: Optional[str] = None,\n        private: bool = False,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Params:\n        hf_token (str): The token to use to access the huggingface API.\n        dataset_name (str): The name of the dataset to save the data to, e.g.\n            \"image-classifier-1\"\n        organization (str): The name of the organization to which to attach\n            the datasets. If None, the dataset attaches to the user only.\n        private (bool): If the dataset does not already exist, whether it\n            should be created as a private dataset or public. Private datasets\n            may require paid huggingface.co accounts\n        verbose (bool): Whether to print out the status of the dataset\n            creation.\n        \"\"\"\n        self.hf_foken = hf_foken\n        self.dataset_name = dataset_name\n        self.organization_name = organization\n        self.dataset_private = private\n        self.verbose = verbose\n\n    def setup(self, flagging_dir: str):\n        \"\"\"\n        Params:\n        flagging_dir (str): local directory where the dataset is cloned,\n        updated, and pushed from.\n        \"\"\"\n        try:\n            import huggingface_hub\n        except (ImportError, ModuleNotFoundError):\n            raise ImportError(\n                \"Package `huggingface_hub` not found is needed \"\n                \"for HuggingFaceDatasetSaver. Try 'pip install huggingface_hub'.\"\n            )\n        path_to_dataset_repo = huggingface_hub.create_repo(\n            name=self.dataset_name,\n            token=self.hf_foken,\n            private=self.dataset_private,\n            repo_type=\"dataset\",\n            exist_ok=True,\n        )\n        self.path_to_dataset_repo = path_to_dataset_repo  # e.g. \"https://huggingface.co/datasets/abidlabs/test-audio-10\"\n        self.flagging_dir = flagging_dir\n        self.dataset_dir = os.path.join(flagging_dir, self.dataset_name)\n        self.repo = huggingface_hub.Repository(\n            local_dir=self.dataset_dir,\n            clone_from=path_to_dataset_repo,\n            use_auth_token=self.hf_foken,\n        )\n        self.repo.git_pull()\n\n        # Should filename be user-specified?\n        self.log_file = os.path.join(self.dataset_dir, \"data.csv\")\n        self.infos_file = os.path.join(self.dataset_dir, \"dataset_infos.json\")\n\n    def flag(\n        self,\n        interface: gr.Interface,\n        input_data: List[Any],\n        output_data: List[Any],\n        flag_option: Optional[str] = None,\n        flag_index: Optional[int] = None,\n        username: Optional[str] = None,\n    ) -> int:\n        is_new = not os.path.exists(self.log_file)\n        infos = {\"flagged\": {\"features\": {}}}\n\n        with open(self.log_file, \"a\", newline=\"\") as csvfile:\n            writer = csv.writer(csvfile)\n\n            # File previews for certain input and output types\n            file_preview_types = {\n                gr.inputs.Audio: \"Audio\",\n                gr.outputs.Audio: \"Audio\",\n                gr.inputs.Image: \"Image\",\n                gr.outputs.Image: \"Image\",\n            }\n\n            # Generate the headers and dataset_infos\n            if is_new:\n                headers = []\n\n                for i, component in enumerate(interface.input_components):\n                    component_label = interface.config[\"input_components\"][i][\n                        \"label\"\n                    ] or \"Input_{}\".format(i)\n                    headers.append(component_label)\n                    infos[\"flagged\"][\"features\"][component_label] = {\n                        \"dtype\": \"string\",\n                        \"_type\": \"Value\",\n                    }\n                    if isinstance(component, tuple(file_preview_types)):\n                        headers.append(component_label + \" file\")\n                        for _component, _type in file_preview_types.items():\n                            if isinstance(component, _component):\n                                infos[\"flagged\"][\"features\"][\n                                    component_label + \" file\"\n                                ] = {\"_type\": _type}\n                                break\n\n                for i, component in enumerate(interface.output_components):\n                    component_label = interface.config[\"output_components\"][i][\n                        \"label\"\n                    ] or \"Output_{}\".format(i)\n                    headers.append(component_label)\n                    infos[\"flagged\"][\"features\"][component_label] = {\n                        \"dtype\": \"string\",\n                        \"_type\": \"Value\",\n                    }\n                    if isinstance(component, tuple(file_preview_types)):\n                        headers.append(component_label + \" file\")\n                        for _component, _type in file_preview_types.items():\n                            if isinstance(component, _component):\n                                infos[\"flagged\"][\"features\"][\n                                    component_label + \" file\"\n                                ] = {\"_type\": _type}\n                                break\n\n                if interface.flagging_options is not None:\n                    headers.append(\"flag\")\n                    infos[\"flagged\"][\"features\"][\"flag\"] = {\n                        \"dtype\": \"string\",\n                        \"_type\": \"Value\",\n                    }\n\n                writer.writerow(headers)\n\n            # Generate the row corresponding to the flagged sample\n            csv_data = []\n            for i, component in enumerate(interface.input_components):\n                label = interface.config[\"input_components\"][i][\n                    \"label\"\n                ] or \"Input_{}\".format(i)\n                filepath = component.save_flagged(\n                    self.dataset_dir, label, input_data[i], None\n                )\n                csv_data.append(filepath)\n                if isinstance(component, tuple(file_preview_types)):\n                    csv_data.append(\n                        \"{}/resolve/main/{}\".format(self.path_to_dataset_repo, filepath)\n                    )\n            for i, component in enumerate(interface.output_components):\n                label = interface.config[\"output_components\"][i][\n                    \"label\"\n                ] or \"Output_{}\".format(i)\n                filepath = (\n                    component.save_flagged(\n                        self.dataset_dir, label, output_data[i], None\n                    )\n                    if output_data[i] is not None\n                    else \"\"\n                )\n                csv_data.append(filepath)\n                if isinstance(component, tuple(file_preview_types)):\n                    csv_data.append(\n                        \"{}/resolve/main/{}\".format(self.path_to_dataset_repo, filepath)\n                    )\n            if flag_option is not None:\n                csv_data.append(flag_option)\n\n            writer.writerow(csv_data)\n\n        if is_new:\n            json.dump(infos, open(self.infos_file, \"w\"))\n\n        with open(self.log_file, \"r\") as csvfile:\n            line_count = len([None for row in csv.reader(csvfile)]) - 1\n\n        self.repo.push_to_hub(commit_message=\"Flagged sample #{}\".format(line_count))\n\n        return line_count\n", "\"\"\" Handy utility functions.\"\"\"\n\nfrom __future__ import annotations\n\nimport csv\nimport inspect\nimport json\nimport json.decoder\nimport os\nimport random\nimport warnings\nfrom distutils.version import StrictVersion\nfrom typing import TYPE_CHECKING, Any, Callable, Dict\n\nimport aiohttp\nimport analytics\nimport pkg_resources\nimport requests\n\nimport gradio\n\nif TYPE_CHECKING:  # Only import for type checking (is False at runtime).\n    from gradio import Interface\n\nanalytics_url = \"https://api.gradio.app/\"\nPKG_VERSION_URL = \"https://api.gradio.app/pkg-version\"\nanalytics.write_key = \"uxIFddIEuuUcFLf9VgH2teTEtPlWdkNy\"\nJSON_PATH = os.path.join(os.path.dirname(gradio.__file__), \"launches.json\")\n\n\ndef version_check():\n    try:\n        current_pkg_version = pkg_resources.require(\"gradio\")[0].version\n        latest_pkg_version = requests.get(url=PKG_VERSION_URL).json()[\"version\"]\n        if StrictVersion(latest_pkg_version) > StrictVersion(current_pkg_version):\n            print(\n                \"IMPORTANT: You are using gradio version {}, \"\n                \"however version {} \"\n                \"is available, please upgrade.\".format(\n                    current_pkg_version, latest_pkg_version\n                )\n            )\n            print(\"--------\")\n    except pkg_resources.DistributionNotFound:\n        warnings.warn(\n            \"gradio is not setup or installed properly. Unable to get version info.\"\n        )\n    except json.decoder.JSONDecodeError:\n        warnings.warn(\"unable to parse version details from package URL.\")\n    except KeyError:\n        warnings.warn(\"package URL does not contain version info.\")\n    except:\n        pass\n\n\ndef get_local_ip_address() -> str:\n    try:\n        ip_address = requests.get(\"https://api.ipify.org\", timeout=3).text\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        ip_address = \"No internet connection\"\n    return ip_address\n\n\ndef initiated_analytics(data: Dict[str:Any]) -> None:\n    try:\n        requests.post(\n            analytics_url + \"gradio-initiated-analytics/\", data=data, timeout=3\n        )\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        pass  # do not push analytics if no network\n\n\ndef launch_analytics(data: Dict[str, Any]) -> None:\n    try:\n        requests.post(\n            analytics_url + \"gradio-launched-analytics/\", data=data, timeout=3\n        )\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        pass  # do not push analytics if no network\n\n\ndef integration_analytics(data: Dict[str, Any]) -> None:\n    try:\n        requests.post(\n            analytics_url + \"gradio-integration-analytics/\", data=data, timeout=3\n        )\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        pass  # do not push analytics if no network\n\n\ndef error_analytics(ip_address: str, message: str) -> None:\n    \"\"\"\n    Send error analytics if there is network\n    :param type: RuntimeError or NameError\n    \"\"\"\n    data = {\"ip_address\": ip_address, \"error\": message}\n    try:\n        requests.post(analytics_url + \"gradio-error-analytics/\", data=data, timeout=3)\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        pass  # do not push analytics if no network\n\n\nasync def log_feature_analytics(ip_address: str, feature: str) -> None:\n    data = {\"ip_address\": ip_address, \"feature\": feature}\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.post(\n                analytics_url + \"gradio-feature-analytics/\", data=data\n            ):\n                pass\n        except (aiohttp.ClientError):\n            pass  # do not push analytics if no network\n\n\ndef colab_check() -> bool:\n    \"\"\"\n    Check if interface is launching from Google Colab\n    :return is_colab (bool): True or False\n    \"\"\"\n    is_colab = False\n    try:  # Check if running interactively using ipython.\n        from IPython import get_ipython\n\n        from_ipynb = get_ipython()\n        if \"google.colab\" in str(from_ipynb):\n            is_colab = True\n    except (ImportError, NameError):\n        pass\n    return is_colab\n\n\ndef ipython_check() -> bool:\n    \"\"\"\n    Check if interface is launching from iPython (not colab)\n    :return is_ipython (bool): True or False\n    \"\"\"\n    is_ipython = False\n    try:  # Check if running interactively using ipython.\n        from IPython import get_ipython\n\n        if get_ipython() is not None:\n            is_ipython = True\n    except (ImportError, NameError):\n        pass\n    return is_ipython\n\n\ndef readme_to_html(article: str) -> str:\n    try:\n        response = requests.get(article, timeout=3)\n        if response.status_code == requests.codes.ok:  # pylint: disable=no-member\n            article = response.text\n    except requests.exceptions.RequestException:\n        pass\n    return article\n\n\ndef show_tip(interface: Interface) -> None:\n    if interface.show_tips and random.random() < 1.5:\n        tip: str = random.choice(gradio.strings.en[\"TIPS\"])\n        print(f\"Tip: {tip}\")\n\n\ndef launch_counter() -> None:\n    try:\n        if not os.path.exists(JSON_PATH):\n            launches = {\"launches\": 1}\n            with open(JSON_PATH, \"w+\") as j:\n                json.dump(launches, j)\n        else:\n            with open(JSON_PATH) as j:\n                launches = json.load(j)\n            launches[\"launches\"] += 1\n            if launches[\"launches\"] in [25, 50]:\n                print(gradio.strings.en[\"BETA_INVITE\"])\n            with open(JSON_PATH, \"w\") as j:\n                j.write(json.dumps(launches))\n    except:\n        pass\n\n\ndef get_config_file(interface: Interface) -> Dict[str, Any]:\n    config = {\n        \"input_components\": [\n            iface.get_template_context() for iface in interface.input_components\n        ],\n        \"output_components\": [\n            iface.get_template_context() for iface in interface.output_components\n        ],\n        \"function_count\": len(interface.predict),\n        \"live\": interface.live,\n        \"examples_per_page\": interface.examples_per_page,\n        \"layout\": interface.layout,\n        \"show_input\": interface.show_input,\n        \"show_output\": interface.show_output,\n        \"title\": interface.title,\n        \"analytics_enabled\": interface.analytics_enabled,\n        \"description\": interface.description,\n        \"simple_description\": interface.simple_description,\n        \"article\": interface.article,\n        \"theme\": interface.theme,\n        \"css\": interface.css,\n        \"thumbnail\": interface.thumbnail,\n        \"allow_screenshot\": interface.allow_screenshot,\n        \"allow_flagging\": interface.allow_flagging,\n        \"flagging_options\": interface.flagging_options,\n        \"allow_interpretation\": interface.interpretation is not None,\n        \"queue\": interface.enable_queue,\n        \"cached_examples\": interface.cache_examples\n        if hasattr(interface, \"cache_examples\")\n        else False,\n        \"version\": pkg_resources.require(\"gradio\")[0].version,\n        \"favicon_path\": interface.favicon_path,\n    }\n    try:\n        param_names = inspect.getfullargspec(interface.predict[0])[0]\n        for iface, param in zip(config[\"input_components\"], param_names):\n            if not iface[\"label\"]:\n                iface[\"label\"] = param.replace(\"_\", \" \")\n        for i, iface in enumerate(config[\"output_components\"]):\n            outputs_per_function = int(\n                len(interface.output_components) / len(interface.predict)\n            )\n            function_index = i // outputs_per_function\n            component_index = i - function_index * outputs_per_function\n            ret_name = (\n                \"Output \" + str(component_index + 1)\n                if outputs_per_function > 1\n                else \"Output\"\n            )\n            if iface[\"label\"] is None:\n                iface[\"label\"] = ret_name\n            if len(interface.predict) > 1:\n                iface[\"label\"] = (\n                    interface.function_names[function_index].replace(\"_\", \" \")\n                    + \": \"\n                    + iface[\"label\"]\n                )\n\n    except ValueError:\n        pass\n    if interface.examples is not None:\n        if isinstance(interface.examples, str):\n            if not os.path.exists(interface.examples):\n                raise FileNotFoundError(\n                    \"Could not find examples directory: \" + interface.examples\n                )\n            log_file = os.path.join(interface.examples, \"log.csv\")\n            if not os.path.exists(log_file):\n                if len(interface.input_components) == 1:\n                    examples = [\n                        [os.path.join(interface.examples, item)]\n                        for item in os.listdir(interface.examples)\n                    ]\n                else:\n                    raise FileNotFoundError(\n                        \"Could not find log file (required for multiple inputs): \"\n                        + log_file\n                    )\n            else:\n                with open(log_file) as logs:\n                    examples = list(csv.reader(logs))\n                    examples = examples[1:]  # remove header\n            for i, example in enumerate(examples):\n                for j, (component, cell) in enumerate(\n                    zip(\n                        interface.input_components + interface.output_components,\n                        example,\n                    )\n                ):\n                    examples[i][j] = component.restore_flagged(\n                        interface.flagging_dir,\n                        cell,\n                        interface.encryption_key if interface.encrypt else None,\n                    )\n            config[\"examples\"] = examples\n            config[\"examples_dir\"] = interface.examples\n        else:\n            config[\"examples\"] = interface.examples\n    return config\n\n\ndef get_default_args(func: Callable) -> Dict[str, Any]:\n    signature = inspect.signature(func)\n    return [\n        v.default if v.default is not inspect.Parameter.empty else None\n        for v in signature.parameters.values()\n    ]\n", "import ipaddress\nimport os\nimport unittest\nimport unittest.mock as mock\nimport warnings\n\nimport pkg_resources\nimport requests\n\nfrom gradio.utils import (\n    colab_check,\n    error_analytics,\n    get_local_ip_address,\n    ipython_check,\n    json,\n    launch_analytics,\n    readme_to_html,\n    version_check,\n)\n\nos.environ[\"GRADIO_ANALYTICS_ENABLED\"] = \"False\"\n\n\nclass TestUtils(unittest.TestCase):\n    @mock.patch(\"pkg_resources.require\")\n    def test_should_fail_with_distribution_not_found(self, mock_require):\n\n        mock_require.side_effect = pkg_resources.DistributionNotFound()\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            version_check()\n            self.assertEqual(\n                str(w[-1].message),\n                \"gradio is not setup or installed properly. Unable to get version info.\",\n            )\n\n    @mock.patch(\"requests.get\")\n    def test_should_warn_with_unable_to_parse(self, mock_get):\n\n        mock_get.side_effect = json.decoder.JSONDecodeError(\"Expecting value\", \"\", 0)\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            version_check()\n            self.assertEqual(\n                str(w[-1].message), \"unable to parse version details from package URL.\"\n            )\n\n    @mock.patch(\"requests.Response.json\")\n    def test_should_warn_url_not_having_version(self, mock_json):\n\n        mock_json.return_value = {\"foo\": \"bar\"}\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            version_check()\n            self.assertEqual(\n                str(w[-1].message), \"package URL does not contain version info.\"\n            )\n\n    @mock.patch(\"requests.post\")\n    def test_error_analytics_doesnt_crash_on_connection_error(self, mock_post):\n\n        mock_post.side_effect = requests.ConnectionError()\n        error_analytics(\"placeholder\", \"placeholder\")\n        mock_post.assert_called()\n\n    @mock.patch(\"requests.post\")\n    def test_error_analytics_successful(self, mock_post):\n        error_analytics(\"placeholder\", \"placeholder\")\n        mock_post.assert_called()\n\n    @mock.patch(\"requests.post\")\n    def test_launch_analytics_doesnt_crash_on_connection_error(self, mock_post):\n        mock_post.side_effect = requests.ConnectionError()\n        launch_analytics(data={})\n        mock_post.assert_called()\n\n    @mock.patch(\"IPython.get_ipython\")\n    def test_colab_check_no_ipython(self, mock_get_ipython):\n        mock_get_ipython.return_value = None\n        assert colab_check() is False\n\n    @mock.patch(\"IPython.get_ipython\")\n    def test_ipython_check_import_fail(self, mock_get_ipython):\n        mock_get_ipython.side_effect = ImportError()\n        assert ipython_check() is False\n\n    @mock.patch(\"IPython.get_ipython\")\n    def test_ipython_check_no_ipython(self, mock_get_ipython):\n        mock_get_ipython.return_value = None\n        assert ipython_check() is False\n\n    @mock.patch(\"requests.get\")\n    def test_readme_to_html_doesnt_crash_on_connection_error(self, mock_get):\n        mock_get.side_effect = requests.ConnectionError()\n        readme_to_html(\"placeholder\")\n\n    def test_readme_to_html_correct_parse(self):\n        readme_to_html(\"https://github.com/gradio-app/gradio/blob/master/README.md\")\n\n\nclass TestIPAddress(unittest.TestCase):\n    def test_get_ip(self):\n        ip = get_local_ip_address()\n        try:  # check whether ip is valid\n            ipaddress.ip_address(ip)\n        except ValueError:\n            self.fail(\"Invalid IP address\")\n\n    @mock.patch(\"requests.get\")\n    def test_get_ip_without_internet(self, mock_get):\n        mock_get.side_effect = requests.ConnectionError()\n        ip = get_local_ip_address()\n        self.assertEqual(ip, \"No internet connection\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"], "fixing_code": ["from __future__ import annotations\n\nimport csv\nimport datetime\nimport io\nimport json\nimport os\nfrom abc import ABC, abstractmethod\nfrom typing import Any, List, Optional\n\nimport gradio as gr\nfrom gradio import encryptor, utils\n\n\nclass FlaggingCallback(ABC):\n    \"\"\"\n    An abstract class for defining the methods that any FlaggingCallback should have.\n    \"\"\"\n\n    @abstractmethod\n    def setup(self, flagging_dir: str):\n        \"\"\"\n        This method should be overridden and ensure that everything is set up correctly for flag().\n        This method gets called once at the beginning of the Interface.launch() method.\n        Parameters:\n        flagging_dir: A string, typically containing the path to the directory where the flagging file should be storied (provided as an argument to Interface.__init__()).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def flag(\n        self,\n        interface: gr.Interface,\n        input_data: List[Any],\n        output_data: List[Any],\n        flag_option: Optional[str] = None,\n        flag_index: Optional[int] = None,\n        username: Optional[str] = None,\n    ) -> int:\n        \"\"\"\n        This method should be overridden by the FlaggingCallback subclass and may contain optional additional arguments.\n        This gets called every time the <flag> button is pressed.\n        Parameters:\n        interface: The Interface object that is being used to launch the flagging interface.\n        input_data: The input data to be flagged.\n        output_data: The output data to be flagged.\n        flag_option (optional): In the case that flagging_options are provided, the flag option that is being used.\n        flag_index (optional): The index of the sample that is being flagged.\n        username (optional): The username of the user that is flagging the data, if logged in.\n        Returns:\n        (int) The total number of samples that have been flagged.\n        \"\"\"\n        pass\n\n\nclass SimpleCSVLogger(FlaggingCallback):\n    \"\"\"\n    A simple example implementation of the FlaggingCallback abstract class\n    provided for illustrative purposes.\n    \"\"\"\n\n    def setup(self, flagging_dir: str):\n        self.flagging_dir = flagging_dir\n        os.makedirs(flagging_dir, exist_ok=True)\n\n    def flag(\n        self,\n        interface: gr.Interface,\n        input_data: List[Any],\n        output_data: List[Any],\n        flag_option: Optional[str] = None,\n        flag_index: Optional[int] = None,\n        username: Optional[str] = None,\n    ) -> int:\n        flagging_dir = self.flagging_dir\n        log_filepath = \"{}/log.csv\".format(flagging_dir)\n\n        csv_data = []\n        for i, input in enumerate(interface.input_components):\n            csv_data.append(\n                input.save_flagged(\n                    flagging_dir,\n                    interface.config[\"input_components\"][i][\"label\"],\n                    input_data[i],\n                    None,\n                )\n            )\n        for i, output in enumerate(interface.output_components):\n            csv_data.append(\n                output.save_flagged(\n                    flagging_dir,\n                    interface.config[\"output_components\"][i][\"label\"],\n                    output_data[i],\n                    None,\n                )\n                if output_data[i] is not None\n                else \"\"\n            )\n\n        with open(log_filepath, \"a\", newline=\"\") as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(utils.santize_for_csv(csv_data))\n\n        with open(log_filepath, \"r\") as csvfile:\n            line_count = len([None for row in csv.reader(csvfile)]) - 1\n        return line_count\n\n\nclass CSVLogger(FlaggingCallback):\n    \"\"\"\n    The default implementation of the FlaggingCallback abstract class.\n    Logs the input and output data to a CSV file. Supports encryption.\n    \"\"\"\n\n    def setup(self, flagging_dir: str):\n        self.flagging_dir = flagging_dir\n        os.makedirs(flagging_dir, exist_ok=True)\n\n    def flag(\n        self,\n        interface: gr.Interface,\n        input_data: List[Any],\n        output_data: List[Any],\n        flag_option: Optional[str] = None,\n        flag_index: Optional[int] = None,\n        username: Optional[str] = None,\n    ) -> int:\n        flagging_dir = self.flagging_dir\n        log_fp = \"{}/log.csv\".format(flagging_dir)\n        encryption_key = interface.encryption_key if interface.encrypt else None\n        is_new = not os.path.exists(log_fp)\n        output_only_mode = input_data is None\n\n        if flag_index is None:\n            csv_data = []\n            if not output_only_mode:\n                for i, input in enumerate(interface.input_components):\n                    csv_data.append(\n                        input.save_flagged(\n                            flagging_dir,\n                            interface.config[\"input_components\"][i][\"label\"],\n                            input_data[i],\n                            encryption_key,\n                        )\n                    )\n            for i, output in enumerate(interface.output_components):\n                csv_data.append(\n                    output.save_flagged(\n                        flagging_dir,\n                        interface.config[\"output_components\"][i][\"label\"],\n                        output_data[i],\n                        encryption_key,\n                    )\n                    if output_data[i] is not None\n                    else \"\"\n                )\n            if not output_only_mode:\n                if flag_option is not None:\n                    csv_data.append(flag_option)\n                if username is not None:\n                    csv_data.append(username)\n                csv_data.append(str(datetime.datetime.now()))\n            if is_new:\n                headers = []\n                if not output_only_mode:\n                    headers += [\n                        interface[\"label\"]\n                        for interface in interface.config[\"input_components\"]\n                    ]\n                headers += [\n                    interface[\"label\"]\n                    for interface in interface.config[\"output_components\"]\n                ]\n                if not output_only_mode:\n                    if interface.flagging_options is not None:\n                        headers.append(\"flag\")\n                    if username is not None:\n                        headers.append(\"username\")\n                    headers.append(\"timestamp\")\n\n        def replace_flag_at_index(file_content):\n            file_content = io.StringIO(file_content)\n            content = list(csv.reader(file_content))\n            header = content[0]\n            flag_col_index = header.index(\"flag\")\n            content[flag_index][flag_col_index] = flag_option\n            output = io.StringIO()\n            writer = csv.writer(output)\n            writer.writerows(utils.santize_for_csv(content))\n            return output.getvalue()\n\n        if interface.encrypt:\n            output = io.StringIO()\n            if not is_new:\n                with open(log_fp, \"rb\") as csvfile:\n                    encrypted_csv = csvfile.read()\n                    decrypted_csv = encryptor.decrypt(\n                        interface.encryption_key, encrypted_csv\n                    )\n                    file_content = decrypted_csv.decode()\n                    if flag_index is not None:\n                        file_content = replace_flag_at_index(file_content)\n                    output.write(utils.santize_for_csv(file_content))\n            writer = csv.writer(output)\n            if flag_index is None:\n                if is_new:\n                    writer.writerow(headers)\n                writer.writerow(csv_data)\n            with open(log_fp, \"wb\") as csvfile:\n                csvfile.write(\n                    utils.santize_for_csv(\n                        encryptor.encrypt(\n                            interface.encryption_key, output.getvalue().encode()\n                        )\n                    )\n                )\n        else:\n            if flag_index is None:\n                with open(log_fp, \"a\", newline=\"\") as csvfile:\n                    writer = csv.writer(csvfile)\n                    if is_new:\n                        writer.writerow(utils.santize_for_csv(headers))\n                    writer.writerow(utils.santize_for_csv(csv_data))\n            else:\n                with open(log_fp) as csvfile:\n                    file_content = csvfile.read()\n                    file_content = replace_flag_at_index(file_content)\n                with open(\n                    log_fp, \"w\", newline=\"\"\n                ) as csvfile:  # newline parameter needed for Windows\n                    csvfile.write(utils.santize_for_csv(file_content))\n        with open(log_fp, \"r\") as csvfile:\n            line_count = len([None for row in csv.reader(csvfile)]) - 1\n        return line_count\n\n\nclass HuggingFaceDatasetSaver(FlaggingCallback):\n    \"\"\"\n    A FlaggingCallback that saves flagged data to a HuggingFace dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        hf_foken: str,\n        dataset_name: str,\n        organization: Optional[str] = None,\n        private: bool = False,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Params:\n        hf_token (str): The token to use to access the huggingface API.\n        dataset_name (str): The name of the dataset to save the data to, e.g.\n            \"image-classifier-1\"\n        organization (str): The name of the organization to which to attach\n            the datasets. If None, the dataset attaches to the user only.\n        private (bool): If the dataset does not already exist, whether it\n            should be created as a private dataset or public. Private datasets\n            may require paid huggingface.co accounts\n        verbose (bool): Whether to print out the status of the dataset\n            creation.\n        \"\"\"\n        self.hf_foken = hf_foken\n        self.dataset_name = dataset_name\n        self.organization_name = organization\n        self.dataset_private = private\n        self.verbose = verbose\n\n    def setup(self, flagging_dir: str):\n        \"\"\"\n        Params:\n        flagging_dir (str): local directory where the dataset is cloned,\n        updated, and pushed from.\n        \"\"\"\n        try:\n            import huggingface_hub\n        except (ImportError, ModuleNotFoundError):\n            raise ImportError(\n                \"Package `huggingface_hub` not found is needed \"\n                \"for HuggingFaceDatasetSaver. Try 'pip install huggingface_hub'.\"\n            )\n        path_to_dataset_repo = huggingface_hub.create_repo(\n            name=self.dataset_name,\n            token=self.hf_foken,\n            private=self.dataset_private,\n            repo_type=\"dataset\",\n            exist_ok=True,\n        )\n        self.path_to_dataset_repo = path_to_dataset_repo  # e.g. \"https://huggingface.co/datasets/abidlabs/test-audio-10\"\n        self.flagging_dir = flagging_dir\n        self.dataset_dir = os.path.join(flagging_dir, self.dataset_name)\n        self.repo = huggingface_hub.Repository(\n            local_dir=self.dataset_dir,\n            clone_from=path_to_dataset_repo,\n            use_auth_token=self.hf_foken,\n        )\n        self.repo.git_pull()\n\n        # Should filename be user-specified?\n        self.log_file = os.path.join(self.dataset_dir, \"data.csv\")\n        self.infos_file = os.path.join(self.dataset_dir, \"dataset_infos.json\")\n\n    def flag(\n        self,\n        interface: gr.Interface,\n        input_data: List[Any],\n        output_data: List[Any],\n        flag_option: Optional[str] = None,\n        flag_index: Optional[int] = None,\n        username: Optional[str] = None,\n    ) -> int:\n        is_new = not os.path.exists(self.log_file)\n        infos = {\"flagged\": {\"features\": {}}}\n\n        with open(self.log_file, \"a\", newline=\"\") as csvfile:\n            writer = csv.writer(csvfile)\n\n            # File previews for certain input and output types\n            file_preview_types = {\n                gr.inputs.Audio: \"Audio\",\n                gr.outputs.Audio: \"Audio\",\n                gr.inputs.Image: \"Image\",\n                gr.outputs.Image: \"Image\",\n            }\n\n            # Generate the headers and dataset_infos\n            if is_new:\n                headers = []\n\n                for i, component in enumerate(interface.input_components):\n                    component_label = interface.config[\"input_components\"][i][\n                        \"label\"\n                    ] or \"Input_{}\".format(i)\n                    headers.append(component_label)\n                    infos[\"flagged\"][\"features\"][component_label] = {\n                        \"dtype\": \"string\",\n                        \"_type\": \"Value\",\n                    }\n                    if isinstance(component, tuple(file_preview_types)):\n                        headers.append(component_label + \" file\")\n                        for _component, _type in file_preview_types.items():\n                            if isinstance(component, _component):\n                                infos[\"flagged\"][\"features\"][\n                                    component_label + \" file\"\n                                ] = {\"_type\": _type}\n                                break\n\n                for i, component in enumerate(interface.output_components):\n                    component_label = interface.config[\"output_components\"][i][\n                        \"label\"\n                    ] or \"Output_{}\".format(i)\n                    headers.append(component_label)\n                    infos[\"flagged\"][\"features\"][component_label] = {\n                        \"dtype\": \"string\",\n                        \"_type\": \"Value\",\n                    }\n                    if isinstance(component, tuple(file_preview_types)):\n                        headers.append(component_label + \" file\")\n                        for _component, _type in file_preview_types.items():\n                            if isinstance(component, _component):\n                                infos[\"flagged\"][\"features\"][\n                                    component_label + \" file\"\n                                ] = {\"_type\": _type}\n                                break\n\n                if interface.flagging_options is not None:\n                    headers.append(\"flag\")\n                    infos[\"flagged\"][\"features\"][\"flag\"] = {\n                        \"dtype\": \"string\",\n                        \"_type\": \"Value\",\n                    }\n\n                writer.writerow(utils.santize_for_csv(headers))\n\n            # Generate the row corresponding to the flagged sample\n            csv_data = []\n            for i, component in enumerate(interface.input_components):\n                label = interface.config[\"input_components\"][i][\n                    \"label\"\n                ] or \"Input_{}\".format(i)\n                filepath = component.save_flagged(\n                    self.dataset_dir, label, input_data[i], None\n                )\n                csv_data.append(filepath)\n                if isinstance(component, tuple(file_preview_types)):\n                    csv_data.append(\n                        \"{}/resolve/main/{}\".format(self.path_to_dataset_repo, filepath)\n                    )\n            for i, component in enumerate(interface.output_components):\n                label = interface.config[\"output_components\"][i][\n                    \"label\"\n                ] or \"Output_{}\".format(i)\n                filepath = (\n                    component.save_flagged(\n                        self.dataset_dir, label, output_data[i], None\n                    )\n                    if output_data[i] is not None\n                    else \"\"\n                )\n                csv_data.append(filepath)\n                if isinstance(component, tuple(file_preview_types)):\n                    csv_data.append(\n                        \"{}/resolve/main/{}\".format(self.path_to_dataset_repo, filepath)\n                    )\n            if flag_option is not None:\n                csv_data.append(flag_option)\n\n            writer.writerow(utils.santize_for_csv(csv_data))\n\n        if is_new:\n            json.dump(infos, open(self.infos_file, \"w\"))\n\n        with open(self.log_file, \"r\") as csvfile:\n            line_count = len([None for row in csv.reader(csvfile)]) - 1\n\n        self.repo.push_to_hub(commit_message=\"Flagged sample #{}\".format(line_count))\n\n        return line_count\n", "\"\"\" Handy utility functions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport csv\nimport inspect\nimport json\nimport json.decoder\nimport os\nimport random\nimport warnings\nfrom distutils.version import StrictVersion\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List\n\nimport aiohttp\nimport analytics\nimport pkg_resources\nimport requests\n\nimport gradio\n\nif TYPE_CHECKING:  # Only import for type checking (is False at runtime).\n    from gradio import Interface\n\nanalytics_url = \"https://api.gradio.app/\"\nPKG_VERSION_URL = \"https://api.gradio.app/pkg-version\"\nanalytics.write_key = \"uxIFddIEuuUcFLf9VgH2teTEtPlWdkNy\"\nJSON_PATH = os.path.join(os.path.dirname(gradio.__file__), \"launches.json\")\n\n\ndef version_check():\n    try:\n        current_pkg_version = pkg_resources.require(\"gradio\")[0].version\n        latest_pkg_version = requests.get(url=PKG_VERSION_URL).json()[\"version\"]\n        if StrictVersion(latest_pkg_version) > StrictVersion(current_pkg_version):\n            print(\n                \"IMPORTANT: You are using gradio version {}, \"\n                \"however version {} \"\n                \"is available, please upgrade.\".format(\n                    current_pkg_version, latest_pkg_version\n                )\n            )\n            print(\"--------\")\n    except pkg_resources.DistributionNotFound:\n        warnings.warn(\n            \"gradio is not setup or installed properly. Unable to get version info.\"\n        )\n    except json.decoder.JSONDecodeError:\n        warnings.warn(\"unable to parse version details from package URL.\")\n    except KeyError:\n        warnings.warn(\"package URL does not contain version info.\")\n    except:\n        pass\n\n\ndef get_local_ip_address() -> str:\n    try:\n        ip_address = requests.get(\"https://api.ipify.org\", timeout=3).text\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        ip_address = \"No internet connection\"\n    return ip_address\n\n\ndef initiated_analytics(data: Dict[str:Any]) -> None:\n    try:\n        requests.post(\n            analytics_url + \"gradio-initiated-analytics/\", data=data, timeout=3\n        )\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        pass  # do not push analytics if no network\n\n\ndef launch_analytics(data: Dict[str, Any]) -> None:\n    try:\n        requests.post(\n            analytics_url + \"gradio-launched-analytics/\", data=data, timeout=3\n        )\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        pass  # do not push analytics if no network\n\n\ndef integration_analytics(data: Dict[str, Any]) -> None:\n    try:\n        requests.post(\n            analytics_url + \"gradio-integration-analytics/\", data=data, timeout=3\n        )\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        pass  # do not push analytics if no network\n\n\ndef error_analytics(ip_address: str, message: str) -> None:\n    \"\"\"\n    Send error analytics if there is network\n    :param type: RuntimeError or NameError\n    \"\"\"\n    data = {\"ip_address\": ip_address, \"error\": message}\n    try:\n        requests.post(analytics_url + \"gradio-error-analytics/\", data=data, timeout=3)\n    except (requests.ConnectionError, requests.exceptions.ReadTimeout):\n        pass  # do not push analytics if no network\n\n\nasync def log_feature_analytics(ip_address: str, feature: str) -> None:\n    data = {\"ip_address\": ip_address, \"feature\": feature}\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.post(\n                analytics_url + \"gradio-feature-analytics/\", data=data\n            ):\n                pass\n        except (aiohttp.ClientError):\n            pass  # do not push analytics if no network\n\n\ndef colab_check() -> bool:\n    \"\"\"\n    Check if interface is launching from Google Colab\n    :return is_colab (bool): True or False\n    \"\"\"\n    is_colab = False\n    try:  # Check if running interactively using ipython.\n        from IPython import get_ipython\n\n        from_ipynb = get_ipython()\n        if \"google.colab\" in str(from_ipynb):\n            is_colab = True\n    except (ImportError, NameError):\n        pass\n    return is_colab\n\n\ndef ipython_check() -> bool:\n    \"\"\"\n    Check if interface is launching from iPython (not colab)\n    :return is_ipython (bool): True or False\n    \"\"\"\n    is_ipython = False\n    try:  # Check if running interactively using ipython.\n        from IPython import get_ipython\n\n        if get_ipython() is not None:\n            is_ipython = True\n    except (ImportError, NameError):\n        pass\n    return is_ipython\n\n\ndef readme_to_html(article: str) -> str:\n    try:\n        response = requests.get(article, timeout=3)\n        if response.status_code == requests.codes.ok:  # pylint: disable=no-member\n            article = response.text\n    except requests.exceptions.RequestException:\n        pass\n    return article\n\n\ndef show_tip(interface: Interface) -> None:\n    if interface.show_tips and random.random() < 1.5:\n        tip: str = random.choice(gradio.strings.en[\"TIPS\"])\n        print(f\"Tip: {tip}\")\n\n\ndef launch_counter() -> None:\n    try:\n        if not os.path.exists(JSON_PATH):\n            launches = {\"launches\": 1}\n            with open(JSON_PATH, \"w+\") as j:\n                json.dump(launches, j)\n        else:\n            with open(JSON_PATH) as j:\n                launches = json.load(j)\n            launches[\"launches\"] += 1\n            if launches[\"launches\"] in [25, 50]:\n                print(gradio.strings.en[\"BETA_INVITE\"])\n            with open(JSON_PATH, \"w\") as j:\n                j.write(json.dumps(launches))\n    except:\n        pass\n\n\ndef get_config_file(interface: Interface) -> Dict[str, Any]:\n    config = {\n        \"input_components\": [\n            iface.get_template_context() for iface in interface.input_components\n        ],\n        \"output_components\": [\n            iface.get_template_context() for iface in interface.output_components\n        ],\n        \"function_count\": len(interface.predict),\n        \"live\": interface.live,\n        \"examples_per_page\": interface.examples_per_page,\n        \"layout\": interface.layout,\n        \"show_input\": interface.show_input,\n        \"show_output\": interface.show_output,\n        \"title\": interface.title,\n        \"analytics_enabled\": interface.analytics_enabled,\n        \"description\": interface.description,\n        \"simple_description\": interface.simple_description,\n        \"article\": interface.article,\n        \"theme\": interface.theme,\n        \"css\": interface.css,\n        \"thumbnail\": interface.thumbnail,\n        \"allow_screenshot\": interface.allow_screenshot,\n        \"allow_flagging\": interface.allow_flagging,\n        \"flagging_options\": interface.flagging_options,\n        \"allow_interpretation\": interface.interpretation is not None,\n        \"queue\": interface.enable_queue,\n        \"cached_examples\": interface.cache_examples\n        if hasattr(interface, \"cache_examples\")\n        else False,\n        \"version\": pkg_resources.require(\"gradio\")[0].version,\n        \"favicon_path\": interface.favicon_path,\n    }\n    try:\n        param_names = inspect.getfullargspec(interface.predict[0])[0]\n        for iface, param in zip(config[\"input_components\"], param_names):\n            if not iface[\"label\"]:\n                iface[\"label\"] = param.replace(\"_\", \" \")\n        for i, iface in enumerate(config[\"output_components\"]):\n            outputs_per_function = int(\n                len(interface.output_components) / len(interface.predict)\n            )\n            function_index = i // outputs_per_function\n            component_index = i - function_index * outputs_per_function\n            ret_name = (\n                \"Output \" + str(component_index + 1)\n                if outputs_per_function > 1\n                else \"Output\"\n            )\n            if iface[\"label\"] is None:\n                iface[\"label\"] = ret_name\n            if len(interface.predict) > 1:\n                iface[\"label\"] = (\n                    interface.function_names[function_index].replace(\"_\", \" \")\n                    + \": \"\n                    + iface[\"label\"]\n                )\n\n    except ValueError:\n        pass\n    if interface.examples is not None:\n        if isinstance(interface.examples, str):\n            if not os.path.exists(interface.examples):\n                raise FileNotFoundError(\n                    \"Could not find examples directory: \" + interface.examples\n                )\n            log_file = os.path.join(interface.examples, \"log.csv\")\n            if not os.path.exists(log_file):\n                if len(interface.input_components) == 1:\n                    examples = [\n                        [os.path.join(interface.examples, item)]\n                        for item in os.listdir(interface.examples)\n                    ]\n                else:\n                    raise FileNotFoundError(\n                        \"Could not find log file (required for multiple inputs): \"\n                        + log_file\n                    )\n            else:\n                with open(log_file) as logs:\n                    examples = list(csv.reader(logs))\n                    examples = examples[1:]  # remove header\n            for i, example in enumerate(examples):\n                for j, (component, cell) in enumerate(\n                    zip(\n                        interface.input_components + interface.output_components,\n                        example,\n                    )\n                ):\n                    examples[i][j] = component.restore_flagged(\n                        interface.flagging_dir,\n                        cell,\n                        interface.encryption_key if interface.encrypt else None,\n                    )\n            config[\"examples\"] = examples\n            config[\"examples_dir\"] = interface.examples\n        else:\n            config[\"examples\"] = interface.examples\n    return config\n\n\ndef get_default_args(func: Callable) -> Dict[str, Any]:\n    signature = inspect.signature(func)\n    return [\n        v.default if v.default is not inspect.Parameter.empty else None\n        for v in signature.parameters.values()\n    ]\n\n\ndef santize_for_csv(data: str | List[str] | List[List[str]]):\n    \"\"\"Sanitizes data so that it can be safely written to a CSV file.\"\"\"\n\n    def sanitize(item):\n        return \"'\" + item\n\n    unsafe_prefixes = (\"+\", \"=\", \"-\", \"@\")\n    warning_message = \"Sanitizing flagged data by escaping cell contents that begin \"\n    \"with one of the following characters: '+', '=', '-', '@'.\"\n\n    if isinstance(data, str):\n        if data.startswith(unsafe_prefixes):\n            warnings.warn(warning_message)\n            return sanitize(data)\n        return data\n    elif isinstance(data, list) and isinstance(data[0], str):\n        sanitized_data = copy.deepcopy(data)\n        for index, item in enumerate(data):\n            if item.startswith(unsafe_prefixes):\n                warnings.warn(warning_message)\n                sanitized_data[index] = sanitize(item)\n        return sanitized_data\n    elif isinstance(data[0], list) and isinstance(data[0][0], str):\n        sanitized_data = copy.deepcopy(data)\n        for outer_index, sublist in enumerate(data):\n            for inner_index, item in enumerate(sublist):\n                if item.startswith(unsafe_prefixes):\n                    warnings.warn(warning_message)\n                    sanitized_data[outer_index][inner_index] = sanitize(item)\n        return sanitized_data\n    else:\n        raise ValueError(\"Unsupported data type: \" + str(type(data)))\n", "import ipaddress\nimport os\nimport unittest\nimport unittest.mock as mock\nimport warnings\n\nimport pkg_resources\nimport requests\n\nfrom gradio.utils import (\n    colab_check,\n    error_analytics,\n    get_local_ip_address,\n    ipython_check,\n    json,\n    launch_analytics,\n    readme_to_html,\n    santize_for_csv,\n    version_check,\n)\n\nos.environ[\"GRADIO_ANALYTICS_ENABLED\"] = \"False\"\n\n\nclass TestUtils(unittest.TestCase):\n    @mock.patch(\"pkg_resources.require\")\n    def test_should_fail_with_distribution_not_found(self, mock_require):\n\n        mock_require.side_effect = pkg_resources.DistributionNotFound()\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            version_check()\n            self.assertEqual(\n                str(w[-1].message),\n                \"gradio is not setup or installed properly. Unable to get version info.\",\n            )\n\n    @mock.patch(\"requests.get\")\n    def test_should_warn_with_unable_to_parse(self, mock_get):\n\n        mock_get.side_effect = json.decoder.JSONDecodeError(\"Expecting value\", \"\", 0)\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            version_check()\n            self.assertEqual(\n                str(w[-1].message), \"unable to parse version details from package URL.\"\n            )\n\n    @mock.patch(\"requests.Response.json\")\n    def test_should_warn_url_not_having_version(self, mock_json):\n\n        mock_json.return_value = {\"foo\": \"bar\"}\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            version_check()\n            self.assertEqual(\n                str(w[-1].message), \"package URL does not contain version info.\"\n            )\n\n    @mock.patch(\"requests.post\")\n    def test_error_analytics_doesnt_crash_on_connection_error(self, mock_post):\n\n        mock_post.side_effect = requests.ConnectionError()\n        error_analytics(\"placeholder\", \"placeholder\")\n        mock_post.assert_called()\n\n    @mock.patch(\"requests.post\")\n    def test_error_analytics_successful(self, mock_post):\n        error_analytics(\"placeholder\", \"placeholder\")\n        mock_post.assert_called()\n\n    @mock.patch(\"requests.post\")\n    def test_launch_analytics_doesnt_crash_on_connection_error(self, mock_post):\n        mock_post.side_effect = requests.ConnectionError()\n        launch_analytics(data={})\n        mock_post.assert_called()\n\n    @mock.patch(\"IPython.get_ipython\")\n    def test_colab_check_no_ipython(self, mock_get_ipython):\n        mock_get_ipython.return_value = None\n        assert colab_check() is False\n\n    @mock.patch(\"IPython.get_ipython\")\n    def test_ipython_check_import_fail(self, mock_get_ipython):\n        mock_get_ipython.side_effect = ImportError()\n        assert ipython_check() is False\n\n    @mock.patch(\"IPython.get_ipython\")\n    def test_ipython_check_no_ipython(self, mock_get_ipython):\n        mock_get_ipython.return_value = None\n        assert ipython_check() is False\n\n    @mock.patch(\"requests.get\")\n    def test_readme_to_html_doesnt_crash_on_connection_error(self, mock_get):\n        mock_get.side_effect = requests.ConnectionError()\n        readme_to_html(\"placeholder\")\n\n    def test_readme_to_html_correct_parse(self):\n        readme_to_html(\"https://github.com/gradio-app/gradio/blob/master/README.md\")\n\n\nclass TestIPAddress(unittest.TestCase):\n    def test_get_ip(self):\n        ip = get_local_ip_address()\n        try:  # check whether ip is valid\n            ipaddress.ip_address(ip)\n        except ValueError:\n            self.fail(\"Invalid IP address\")\n\n    @mock.patch(\"requests.get\")\n    def test_get_ip_without_internet(self, mock_get):\n        mock_get.side_effect = requests.ConnectionError()\n        ip = get_local_ip_address()\n        self.assertEqual(ip, \"No internet connection\")\n\n\nclass TestSanitizeForCSV(unittest.TestCase):\n    def test_safe(self):\n        safe_data = santize_for_csv(\"abc\")\n        self.assertEquals(safe_data, \"abc\")\n        safe_data = santize_for_csv([\"def\"])\n        self.assertEquals(safe_data, [\"def\"])\n        safe_data = santize_for_csv([[\"abc\"]])\n        self.assertEquals(safe_data, [[\"abc\"]])\n\n    def test_unsafe(self):\n        safe_data = santize_for_csv(\"=abc\")\n        self.assertEquals(safe_data, \"'=abc\")\n        safe_data = santize_for_csv([\"abc\", \"+abc\"])\n        self.assertEquals(safe_data, [\"abc\", \"'+abc\"])\n        safe_data = santize_for_csv([[\"abc\", \"=abc\"]])\n        self.assertEquals(safe_data, [[\"abc\", \"'=abc\"]])\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"], "filenames": ["gradio/flagging.py", "gradio/utils.py", "test/test_utils.py"], "buggy_code_start_loc": [12, 4, 17], "buggy_code_end_loc": [407, 288, 118], "fixing_code_start_loc": [12, 5, 18], "fixing_code_end_loc": [409, 324, 138], "type": "CWE-1236", "message": "`gradio` is an open source framework for building interactive machine learning models and demos. Prior to version 2.8.11, `gradio` suffers from Improper Neutralization of Formula Elements in a CSV File. The `gradio` library has a flagging functionality which saves input/output data into a CSV file on the developer's computer. This can allow a user to save arbitrary text into the CSV file, such as commands. If a program like MS Excel opens such a file, then it automatically runs these commands, which could lead to arbitrary commands running on the user's computer. The problem has been patched as of `2.8.11`, which escapes the saved csv with single quotes. As a workaround, avoid opening csv files generated by `gradio` with Excel or similar spreadsheet programs.", "other": {"cve": {"id": "CVE-2022-24770", "sourceIdentifier": "security-advisories@github.com", "published": "2022-03-17T21:15:08.133", "lastModified": "2022-03-24T13:00:22.403", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "`gradio` is an open source framework for building interactive machine learning models and demos. Prior to version 2.8.11, `gradio` suffers from Improper Neutralization of Formula Elements in a CSV File. The `gradio` library has a flagging functionality which saves input/output data into a CSV file on the developer's computer. This can allow a user to save arbitrary text into the CSV file, such as commands. If a program like MS Excel opens such a file, then it automatically runs these commands, which could lead to arbitrary commands running on the user's computer. The problem has been patched as of `2.8.11`, which escapes the saved csv with single quotes. As a workaround, avoid opening csv files generated by `gradio` with Excel or similar spreadsheet programs."}, {"lang": "es", "value": "\"gradio\" es un marco de trabajo de c\u00f3digo abierto para la construcci\u00f3n de modelos y demostraciones interactivas de aprendizaje autom\u00e1tico. En versiones anteriores a 2.8.11, \"gradio\" sufre de una Neutralizaci\u00f3n Inapropiada de Elementos de F\u00f3rmula en un Archivo CSV. La biblioteca \"gradio\" presenta una funcionalidad de marcado que guarda los datos de entrada/salida en un archivo CSV en el ordenador del desarrollador. Esto puede permitir a un usuario guardar texto arbitrario en el archivo CSV, como por ejemplo, comandos. Si un programa como MS Excel abre dicho archivo, entonces ejecuta autom\u00e1ticamente estos comandos, lo que podr\u00eda conllevara una ejecuci\u00f3n de comandos arbitrarios en el ordenador del usuario. El problema ha sido parcheado a partir de la versi\u00f3n \"2.8.11\", que escapa del csv guardado con comillas simples. Como medida de mitigaci\u00f3n, evite abrir los archivos csv generados por \"gradio\" con Excel o programas de hojas de c\u00e1lculo similares"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.8}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-1236"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:gradio_project:gradio:*:*:*:*:*:python:*:*", "versionEndExcluding": "2.8.11", "matchCriteriaId": "194AA77B-62C6-4BEF-80B1-5588BBEE3FD0"}]}]}], "references": [{"url": "https://github.com/gradio-app/gradio/commit/80fea89117358ee105973453fdc402398ae20239", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/gradio-app/gradio/pull/817", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/gradio-app/gradio/security/advisories/GHSA-f8xq-q7px-wg8c", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/gradio-app/gradio/commit/80fea89117358ee105973453fdc402398ae20239"}}