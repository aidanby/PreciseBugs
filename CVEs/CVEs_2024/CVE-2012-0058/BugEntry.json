{"buggy_code": ["/*\n *\tAn async IO implementation for Linux\n *\tWritten by Benjamin LaHaise <bcrl@kvack.org>\n *\n *\tImplements an efficient asynchronous io interface.\n *\n *\tCopyright 2000, 2001, 2002 Red Hat, Inc.  All Rights Reserved.\n *\n *\tSee ../COPYING for licensing terms.\n */\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/time.h>\n#include <linux/aio_abi.h>\n#include <linux/module.h>\n#include <linux/syscalls.h>\n#include <linux/backing-dev.h>\n#include <linux/uio.h>\n\n#define DEBUG 0\n\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/mmu_context.h>\n#include <linux/slab.h>\n#include <linux/timer.h>\n#include <linux/aio.h>\n#include <linux/highmem.h>\n#include <linux/workqueue.h>\n#include <linux/security.h>\n#include <linux/eventfd.h>\n#include <linux/blkdev.h>\n#include <linux/compat.h>\n\n#include <asm/kmap_types.h>\n#include <asm/uaccess.h>\n\n#if DEBUG > 1\n#define dprintk\t\tprintk\n#else\n#define dprintk(x...)\tdo { ; } while (0)\n#endif\n\n/*------ sysctl variables----*/\nstatic DEFINE_SPINLOCK(aio_nr_lock);\nunsigned long aio_nr;\t\t/* current system wide number of aio requests */\nunsigned long aio_max_nr = 0x10000; /* system wide maximum number of aio requests */\n/*----end sysctl variables---*/\n\nstatic struct kmem_cache\t*kiocb_cachep;\nstatic struct kmem_cache\t*kioctx_cachep;\n\nstatic struct workqueue_struct *aio_wq;\n\n/* Used for rare fput completion. */\nstatic void aio_fput_routine(struct work_struct *);\nstatic DECLARE_WORK(fput_work, aio_fput_routine);\n\nstatic DEFINE_SPINLOCK(fput_lock);\nstatic LIST_HEAD(fput_head);\n\nstatic void aio_kick_handler(struct work_struct *);\nstatic void aio_queue_work(struct kioctx *);\n\n/* aio_setup\n *\tCreates the slab caches used by the aio routines, panic on\n *\tfailure as this is done early during the boot sequence.\n */\nstatic int __init aio_setup(void)\n{\n\tkiocb_cachep = KMEM_CACHE(kiocb, SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\tkioctx_cachep = KMEM_CACHE(kioctx,SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\n\taio_wq = alloc_workqueue(\"aio\", 0, 1);\t/* used to limit concurrency */\n\tBUG_ON(!aio_wq);\n\n\tpr_debug(\"aio_setup: sizeof(struct page) = %d\\n\", (int)sizeof(struct page));\n\n\treturn 0;\n}\n__initcall(aio_setup);\n\nstatic void aio_free_ring(struct kioctx *ctx)\n{\n\tstruct aio_ring_info *info = &ctx->ring_info;\n\tlong i;\n\n\tfor (i=0; i<info->nr_pages; i++)\n\t\tput_page(info->ring_pages[i]);\n\n\tif (info->mmap_size) {\n\t\tdown_write(&ctx->mm->mmap_sem);\n\t\tdo_munmap(ctx->mm, info->mmap_base, info->mmap_size);\n\t\tup_write(&ctx->mm->mmap_sem);\n\t}\n\n\tif (info->ring_pages && info->ring_pages != info->internal_pages)\n\t\tkfree(info->ring_pages);\n\tinfo->ring_pages = NULL;\n\tinfo->nr = 0;\n}\n\nstatic int aio_setup_ring(struct kioctx *ctx)\n{\n\tstruct aio_ring *ring;\n\tstruct aio_ring_info *info = &ctx->ring_info;\n\tunsigned nr_events = ctx->max_reqs;\n\tunsigned long size;\n\tint nr_pages;\n\n\t/* Compensate for the ring buffer's head/tail overlap entry */\n\tnr_events += 2;\t/* 1 is required, 2 for good luck */\n\n\tsize = sizeof(struct aio_ring);\n\tsize += sizeof(struct io_event) * nr_events;\n\tnr_pages = (size + PAGE_SIZE-1) >> PAGE_SHIFT;\n\n\tif (nr_pages < 0)\n\t\treturn -EINVAL;\n\n\tnr_events = (PAGE_SIZE * nr_pages - sizeof(struct aio_ring)) / sizeof(struct io_event);\n\n\tinfo->nr = 0;\n\tinfo->ring_pages = info->internal_pages;\n\tif (nr_pages > AIO_RING_PAGES) {\n\t\tinfo->ring_pages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\t\tif (!info->ring_pages)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tinfo->mmap_size = nr_pages * PAGE_SIZE;\n\tdprintk(\"attempting mmap of %lu bytes\\n\", info->mmap_size);\n\tdown_write(&ctx->mm->mmap_sem);\n\tinfo->mmap_base = do_mmap(NULL, 0, info->mmap_size, \n\t\t\t\t  PROT_READ|PROT_WRITE, MAP_ANONYMOUS|MAP_PRIVATE,\n\t\t\t\t  0);\n\tif (IS_ERR((void *)info->mmap_base)) {\n\t\tup_write(&ctx->mm->mmap_sem);\n\t\tinfo->mmap_size = 0;\n\t\taio_free_ring(ctx);\n\t\treturn -EAGAIN;\n\t}\n\n\tdprintk(\"mmap address: 0x%08lx\\n\", info->mmap_base);\n\tinfo->nr_pages = get_user_pages(current, ctx->mm,\n\t\t\t\t\tinfo->mmap_base, nr_pages, \n\t\t\t\t\t1, 0, info->ring_pages, NULL);\n\tup_write(&ctx->mm->mmap_sem);\n\n\tif (unlikely(info->nr_pages != nr_pages)) {\n\t\taio_free_ring(ctx);\n\t\treturn -EAGAIN;\n\t}\n\n\tctx->user_id = info->mmap_base;\n\n\tinfo->nr = nr_events;\t\t/* trusted copy */\n\n\tring = kmap_atomic(info->ring_pages[0], KM_USER0);\n\tring->nr = nr_events;\t/* user copy */\n\tring->id = ctx->user_id;\n\tring->head = ring->tail = 0;\n\tring->magic = AIO_RING_MAGIC;\n\tring->compat_features = AIO_RING_COMPAT_FEATURES;\n\tring->incompat_features = AIO_RING_INCOMPAT_FEATURES;\n\tring->header_length = sizeof(struct aio_ring);\n\tkunmap_atomic(ring, KM_USER0);\n\n\treturn 0;\n}\n\n\n/* aio_ring_event: returns a pointer to the event at the given index from\n * kmap_atomic(, km).  Release the pointer with put_aio_ring_event();\n */\n#define AIO_EVENTS_PER_PAGE\t(PAGE_SIZE / sizeof(struct io_event))\n#define AIO_EVENTS_FIRST_PAGE\t((PAGE_SIZE - sizeof(struct aio_ring)) / sizeof(struct io_event))\n#define AIO_EVENTS_OFFSET\t(AIO_EVENTS_PER_PAGE - AIO_EVENTS_FIRST_PAGE)\n\n#define aio_ring_event(info, nr, km) ({\t\t\t\t\t\\\n\tunsigned pos = (nr) + AIO_EVENTS_OFFSET;\t\t\t\\\n\tstruct io_event *__event;\t\t\t\t\t\\\n\t__event = kmap_atomic(\t\t\t\t\t\t\\\n\t\t\t(info)->ring_pages[pos / AIO_EVENTS_PER_PAGE], km); \\\n\t__event += pos % AIO_EVENTS_PER_PAGE;\t\t\t\t\\\n\t__event;\t\t\t\t\t\t\t\\\n})\n\n#define put_aio_ring_event(event, km) do {\t\\\n\tstruct io_event *__event = (event);\t\\\n\t(void)__event;\t\t\t\t\\\n\tkunmap_atomic((void *)((unsigned long)__event & PAGE_MASK), km); \\\n} while(0)\n\nstatic void ctx_rcu_free(struct rcu_head *head)\n{\n\tstruct kioctx *ctx = container_of(head, struct kioctx, rcu_head);\n\tunsigned nr_events = ctx->max_reqs;\n\n\tkmem_cache_free(kioctx_cachep, ctx);\n\n\tif (nr_events) {\n\t\tspin_lock(&aio_nr_lock);\n\t\tBUG_ON(aio_nr - nr_events > aio_nr);\n\t\taio_nr -= nr_events;\n\t\tspin_unlock(&aio_nr_lock);\n\t}\n}\n\n/* __put_ioctx\n *\tCalled when the last user of an aio context has gone away,\n *\tand the struct needs to be freed.\n */\nstatic void __put_ioctx(struct kioctx *ctx)\n{\n\tBUG_ON(ctx->reqs_active);\n\n\tcancel_delayed_work(&ctx->wq);\n\tcancel_work_sync(&ctx->wq.work);\n\taio_free_ring(ctx);\n\tmmdrop(ctx->mm);\n\tctx->mm = NULL;\n\tpr_debug(\"__put_ioctx: freeing %p\\n\", ctx);\n\tcall_rcu(&ctx->rcu_head, ctx_rcu_free);\n}\n\nstatic inline void get_ioctx(struct kioctx *kioctx)\n{\n\tBUG_ON(atomic_read(&kioctx->users) <= 0);\n\tatomic_inc(&kioctx->users);\n}\n\nstatic inline int try_get_ioctx(struct kioctx *kioctx)\n{\n\treturn atomic_inc_not_zero(&kioctx->users);\n}\n\nstatic inline void put_ioctx(struct kioctx *kioctx)\n{\n\tBUG_ON(atomic_read(&kioctx->users) <= 0);\n\tif (unlikely(atomic_dec_and_test(&kioctx->users)))\n\t\t__put_ioctx(kioctx);\n}\n\n/* ioctx_alloc\n *\tAllocates and initializes an ioctx.  Returns an ERR_PTR if it failed.\n */\nstatic struct kioctx *ioctx_alloc(unsigned nr_events)\n{\n\tstruct mm_struct *mm;\n\tstruct kioctx *ctx;\n\tint did_sync = 0;\n\n\t/* Prevent overflows */\n\tif ((nr_events > (0x10000000U / sizeof(struct io_event))) ||\n\t    (nr_events > (0x10000000U / sizeof(struct kiocb)))) {\n\t\tpr_debug(\"ENOMEM: nr_events too high\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif ((unsigned long)nr_events > aio_max_nr)\n\t\treturn ERR_PTR(-EAGAIN);\n\n\tctx = kmem_cache_zalloc(kioctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tctx->max_reqs = nr_events;\n\tmm = ctx->mm = current->mm;\n\tatomic_inc(&mm->mm_count);\n\n\tatomic_set(&ctx->users, 1);\n\tspin_lock_init(&ctx->ctx_lock);\n\tspin_lock_init(&ctx->ring_info.ring_lock);\n\tinit_waitqueue_head(&ctx->wait);\n\n\tINIT_LIST_HEAD(&ctx->active_reqs);\n\tINIT_LIST_HEAD(&ctx->run_list);\n\tINIT_DELAYED_WORK(&ctx->wq, aio_kick_handler);\n\n\tif (aio_setup_ring(ctx) < 0)\n\t\tgoto out_freectx;\n\n\t/* limit the number of system wide aios */\n\tdo {\n\t\tspin_lock_bh(&aio_nr_lock);\n\t\tif (aio_nr + nr_events > aio_max_nr ||\n\t\t    aio_nr + nr_events < aio_nr)\n\t\t\tctx->max_reqs = 0;\n\t\telse\n\t\t\taio_nr += ctx->max_reqs;\n\t\tspin_unlock_bh(&aio_nr_lock);\n\t\tif (ctx->max_reqs || did_sync)\n\t\t\tbreak;\n\n\t\t/* wait for rcu callbacks to have completed before giving up */\n\t\tsynchronize_rcu();\n\t\tdid_sync = 1;\n\t\tctx->max_reqs = nr_events;\n\t} while (1);\n\n\tif (ctx->max_reqs == 0)\n\t\tgoto out_cleanup;\n\n\t/* now link into global list. */\n\tspin_lock(&mm->ioctx_lock);\n\thlist_add_head_rcu(&ctx->list, &mm->ioctx_list);\n\tspin_unlock(&mm->ioctx_lock);\n\n\tdprintk(\"aio: allocated ioctx %p[%ld]: mm=%p mask=0x%x\\n\",\n\t\tctx, ctx->user_id, current->mm, ctx->ring_info.nr);\n\treturn ctx;\n\nout_cleanup:\n\t__put_ioctx(ctx);\n\treturn ERR_PTR(-EAGAIN);\n\nout_freectx:\n\tmmdrop(mm);\n\tkmem_cache_free(kioctx_cachep, ctx);\n\tctx = ERR_PTR(-ENOMEM);\n\n\tdprintk(\"aio: error allocating ioctx %p\\n\", ctx);\n\treturn ctx;\n}\n\n/* aio_cancel_all\n *\tCancels all outstanding aio requests on an aio context.  Used \n *\twhen the processes owning a context have all exited to encourage \n *\tthe rapid destruction of the kioctx.\n */\nstatic void aio_cancel_all(struct kioctx *ctx)\n{\n\tint (*cancel)(struct kiocb *, struct io_event *);\n\tstruct io_event res;\n\tspin_lock_irq(&ctx->ctx_lock);\n\tctx->dead = 1;\n\twhile (!list_empty(&ctx->active_reqs)) {\n\t\tstruct list_head *pos = ctx->active_reqs.next;\n\t\tstruct kiocb *iocb = list_kiocb(pos);\n\t\tlist_del_init(&iocb->ki_list);\n\t\tcancel = iocb->ki_cancel;\n\t\tkiocbSetCancelled(iocb);\n\t\tif (cancel) {\n\t\t\tiocb->ki_users++;\n\t\t\tspin_unlock_irq(&ctx->ctx_lock);\n\t\t\tcancel(iocb, &res);\n\t\t\tspin_lock_irq(&ctx->ctx_lock);\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->ctx_lock);\n}\n\nstatic void wait_for_all_aios(struct kioctx *ctx)\n{\n\tstruct task_struct *tsk = current;\n\tDECLARE_WAITQUEUE(wait, tsk);\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tif (!ctx->reqs_active)\n\t\tgoto out;\n\n\tadd_wait_queue(&ctx->wait, &wait);\n\tset_task_state(tsk, TASK_UNINTERRUPTIBLE);\n\twhile (ctx->reqs_active) {\n\t\tspin_unlock_irq(&ctx->ctx_lock);\n\t\tio_schedule();\n\t\tset_task_state(tsk, TASK_UNINTERRUPTIBLE);\n\t\tspin_lock_irq(&ctx->ctx_lock);\n\t}\n\t__set_task_state(tsk, TASK_RUNNING);\n\tremove_wait_queue(&ctx->wait, &wait);\n\nout:\n\tspin_unlock_irq(&ctx->ctx_lock);\n}\n\n/* wait_on_sync_kiocb:\n *\tWaits on the given sync kiocb to complete.\n */\nssize_t wait_on_sync_kiocb(struct kiocb *iocb)\n{\n\twhile (iocb->ki_users) {\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tif (!iocb->ki_users)\n\t\t\tbreak;\n\t\tio_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n\treturn iocb->ki_user_data;\n}\nEXPORT_SYMBOL(wait_on_sync_kiocb);\n\n/* exit_aio: called when the last user of mm goes away.  At this point, \n * there is no way for any new requests to be submited or any of the \n * io_* syscalls to be called on the context.  However, there may be \n * outstanding requests which hold references to the context; as they \n * go away, they will call put_ioctx and release any pinned memory\n * associated with the request (held via struct page * references).\n */\nvoid exit_aio(struct mm_struct *mm)\n{\n\tstruct kioctx *ctx;\n\n\twhile (!hlist_empty(&mm->ioctx_list)) {\n\t\tctx = hlist_entry(mm->ioctx_list.first, struct kioctx, list);\n\t\thlist_del_rcu(&ctx->list);\n\n\t\taio_cancel_all(ctx);\n\n\t\twait_for_all_aios(ctx);\n\t\t/*\n\t\t * Ensure we don't leave the ctx on the aio_wq\n\t\t */\n\t\tcancel_work_sync(&ctx->wq.work);\n\n\t\tif (1 != atomic_read(&ctx->users))\n\t\t\tprintk(KERN_DEBUG\n\t\t\t\t\"exit_aio:ioctx still alive: %d %d %d\\n\",\n\t\t\t\tatomic_read(&ctx->users), ctx->dead,\n\t\t\t\tctx->reqs_active);\n\t\tput_ioctx(ctx);\n\t}\n}\n\n/* aio_get_req\n *\tAllocate a slot for an aio request.  Increments the users count\n * of the kioctx so that the kioctx stays around until all requests are\n * complete.  Returns NULL if no requests are free.\n *\n * Returns with kiocb->users set to 2.  The io submit code path holds\n * an extra reference while submitting the i/o.\n * This prevents races between the aio code path referencing the\n * req (after submitting it) and aio_complete() freeing the req.\n */\nstatic struct kiocb *__aio_get_req(struct kioctx *ctx)\n{\n\tstruct kiocb *req = NULL;\n\n\treq = kmem_cache_alloc(kiocb_cachep, GFP_KERNEL);\n\tif (unlikely(!req))\n\t\treturn NULL;\n\n\treq->ki_flags = 0;\n\treq->ki_users = 2;\n\treq->ki_key = 0;\n\treq->ki_ctx = ctx;\n\treq->ki_cancel = NULL;\n\treq->ki_retry = NULL;\n\treq->ki_dtor = NULL;\n\treq->private = NULL;\n\treq->ki_iovec = NULL;\n\tINIT_LIST_HEAD(&req->ki_run_list);\n\treq->ki_eventfd = NULL;\n\n\treturn req;\n}\n\n/*\n * struct kiocb's are allocated in batches to reduce the number of\n * times the ctx lock is acquired and released.\n */\n#define KIOCB_BATCH_SIZE\t32L\nstruct kiocb_batch {\n\tstruct list_head head;\n\tlong count; /* number of requests left to allocate */\n};\n\nstatic void kiocb_batch_init(struct kiocb_batch *batch, long total)\n{\n\tINIT_LIST_HEAD(&batch->head);\n\tbatch->count = total;\n}\n\nstatic void kiocb_batch_free(struct kiocb_batch *batch)\n{\n\tstruct kiocb *req, *n;\n\n\tlist_for_each_entry_safe(req, n, &batch->head, ki_batch) {\n\t\tlist_del(&req->ki_batch);\n\t\tkmem_cache_free(kiocb_cachep, req);\n\t}\n}\n\n/*\n * Allocate a batch of kiocbs.  This avoids taking and dropping the\n * context lock a lot during setup.\n */\nstatic int kiocb_batch_refill(struct kioctx *ctx, struct kiocb_batch *batch)\n{\n\tunsigned short allocated, to_alloc;\n\tlong avail;\n\tbool called_fput = false;\n\tstruct kiocb *req, *n;\n\tstruct aio_ring *ring;\n\n\tto_alloc = min(batch->count, KIOCB_BATCH_SIZE);\n\tfor (allocated = 0; allocated < to_alloc; allocated++) {\n\t\treq = __aio_get_req(ctx);\n\t\tif (!req)\n\t\t\t/* allocation failed, go with what we've got */\n\t\t\tbreak;\n\t\tlist_add(&req->ki_batch, &batch->head);\n\t}\n\n\tif (allocated == 0)\n\t\tgoto out;\n\nretry:\n\tspin_lock_irq(&ctx->ctx_lock);\n\tring = kmap_atomic(ctx->ring_info.ring_pages[0]);\n\n\tavail = aio_ring_avail(&ctx->ring_info, ring) - ctx->reqs_active;\n\tBUG_ON(avail < 0);\n\tif (avail == 0 && !called_fput) {\n\t\t/*\n\t\t * Handle a potential starvation case.  It is possible that\n\t\t * we hold the last reference on a struct file, causing us\n\t\t * to delay the final fput to non-irq context.  In this case,\n\t\t * ctx->reqs_active is artificially high.  Calling the fput\n\t\t * routine here may free up a slot in the event completion\n\t\t * ring, allowing this allocation to succeed.\n\t\t */\n\t\tkunmap_atomic(ring);\n\t\tspin_unlock_irq(&ctx->ctx_lock);\n\t\taio_fput_routine(NULL);\n\t\tcalled_fput = true;\n\t\tgoto retry;\n\t}\n\n\tif (avail < allocated) {\n\t\t/* Trim back the number of requests. */\n\t\tlist_for_each_entry_safe(req, n, &batch->head, ki_batch) {\n\t\t\tlist_del(&req->ki_batch);\n\t\t\tkmem_cache_free(kiocb_cachep, req);\n\t\t\tif (--allocated <= avail)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tbatch->count -= allocated;\n\tlist_for_each_entry(req, &batch->head, ki_batch) {\n\t\tlist_add(&req->ki_list, &ctx->active_reqs);\n\t\tctx->reqs_active++;\n\t}\n\n\tkunmap_atomic(ring);\n\tspin_unlock_irq(&ctx->ctx_lock);\n\nout:\n\treturn allocated;\n}\n\nstatic inline struct kiocb *aio_get_req(struct kioctx *ctx,\n\t\t\t\t\tstruct kiocb_batch *batch)\n{\n\tstruct kiocb *req;\n\n\tif (list_empty(&batch->head))\n\t\tif (kiocb_batch_refill(ctx, batch) == 0)\n\t\t\treturn NULL;\n\treq = list_first_entry(&batch->head, struct kiocb, ki_batch);\n\tlist_del(&req->ki_batch);\n\treturn req;\n}\n\nstatic inline void really_put_req(struct kioctx *ctx, struct kiocb *req)\n{\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\tif (req->ki_eventfd != NULL)\n\t\teventfd_ctx_put(req->ki_eventfd);\n\tif (req->ki_dtor)\n\t\treq->ki_dtor(req);\n\tif (req->ki_iovec != &req->ki_inline_vec)\n\t\tkfree(req->ki_iovec);\n\tkmem_cache_free(kiocb_cachep, req);\n\tctx->reqs_active--;\n\n\tif (unlikely(!ctx->reqs_active && ctx->dead))\n\t\twake_up_all(&ctx->wait);\n}\n\nstatic void aio_fput_routine(struct work_struct *data)\n{\n\tspin_lock_irq(&fput_lock);\n\twhile (likely(!list_empty(&fput_head))) {\n\t\tstruct kiocb *req = list_kiocb(fput_head.next);\n\t\tstruct kioctx *ctx = req->ki_ctx;\n\n\t\tlist_del(&req->ki_list);\n\t\tspin_unlock_irq(&fput_lock);\n\n\t\t/* Complete the fput(s) */\n\t\tif (req->ki_filp != NULL)\n\t\t\tfput(req->ki_filp);\n\n\t\t/* Link the iocb into the context's free list */\n\t\tspin_lock_irq(&ctx->ctx_lock);\n\t\treally_put_req(ctx, req);\n\t\tspin_unlock_irq(&ctx->ctx_lock);\n\n\t\tput_ioctx(ctx);\n\t\tspin_lock_irq(&fput_lock);\n\t}\n\tspin_unlock_irq(&fput_lock);\n}\n\n/* __aio_put_req\n *\tReturns true if this put was the last user of the request.\n */\nstatic int __aio_put_req(struct kioctx *ctx, struct kiocb *req)\n{\n\tdprintk(KERN_DEBUG \"aio_put(%p): f_count=%ld\\n\",\n\t\treq, atomic_long_read(&req->ki_filp->f_count));\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\treq->ki_users--;\n\tBUG_ON(req->ki_users < 0);\n\tif (likely(req->ki_users))\n\t\treturn 0;\n\tlist_del(&req->ki_list);\t\t/* remove from active_reqs */\n\treq->ki_cancel = NULL;\n\treq->ki_retry = NULL;\n\n\t/*\n\t * Try to optimize the aio and eventfd file* puts, by avoiding to\n\t * schedule work in case it is not final fput() time. In normal cases,\n\t * we would not be holding the last reference to the file*, so\n\t * this function will be executed w/out any aio kthread wakeup.\n\t */\n\tif (unlikely(!fput_atomic(req->ki_filp))) {\n\t\tget_ioctx(ctx);\n\t\tspin_lock(&fput_lock);\n\t\tlist_add(&req->ki_list, &fput_head);\n\t\tspin_unlock(&fput_lock);\n\t\tschedule_work(&fput_work);\n\t} else {\n\t\treq->ki_filp = NULL;\n\t\treally_put_req(ctx, req);\n\t}\n\treturn 1;\n}\n\n/* aio_put_req\n *\tReturns true if this put was the last user of the kiocb,\n *\tfalse if the request is still in use.\n */\nint aio_put_req(struct kiocb *req)\n{\n\tstruct kioctx *ctx = req->ki_ctx;\n\tint ret;\n\tspin_lock_irq(&ctx->ctx_lock);\n\tret = __aio_put_req(ctx, req);\n\tspin_unlock_irq(&ctx->ctx_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL(aio_put_req);\n\nstatic struct kioctx *lookup_ioctx(unsigned long ctx_id)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct kioctx *ctx, *ret = NULL;\n\tstruct hlist_node *n;\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(ctx, n, &mm->ioctx_list, list) {\n\t\t/*\n\t\t * RCU protects us against accessing freed memory but\n\t\t * we have to be careful not to get a reference when the\n\t\t * reference count already dropped to 0 (ctx->dead test\n\t\t * is unreliable because of races).\n\t\t */\n\t\tif (ctx->user_id == ctx_id && !ctx->dead && try_get_ioctx(ctx)){\n\t\t\tret = ctx;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/*\n * Queue up a kiocb to be retried. Assumes that the kiocb\n * has already been marked as kicked, and places it on\n * the retry run list for the corresponding ioctx, if it\n * isn't already queued. Returns 1 if it actually queued\n * the kiocb (to tell the caller to activate the work\n * queue to process it), or 0, if it found that it was\n * already queued.\n */\nstatic inline int __queue_kicked_iocb(struct kiocb *iocb)\n{\n\tstruct kioctx *ctx = iocb->ki_ctx;\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\tif (list_empty(&iocb->ki_run_list)) {\n\t\tlist_add_tail(&iocb->ki_run_list,\n\t\t\t&ctx->run_list);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/* aio_run_iocb\n *\tThis is the core aio execution routine. It is\n *\tinvoked both for initial i/o submission and\n *\tsubsequent retries via the aio_kick_handler.\n *\tExpects to be invoked with iocb->ki_ctx->lock\n *\talready held. The lock is released and reacquired\n *\tas needed during processing.\n *\n * Calls the iocb retry method (already setup for the\n * iocb on initial submission) for operation specific\n * handling, but takes care of most of common retry\n * execution details for a given iocb. The retry method\n * needs to be non-blocking as far as possible, to avoid\n * holding up other iocbs waiting to be serviced by the\n * retry kernel thread.\n *\n * The trickier parts in this code have to do with\n * ensuring that only one retry instance is in progress\n * for a given iocb at any time. Providing that guarantee\n * simplifies the coding of individual aio operations as\n * it avoids various potential races.\n */\nstatic ssize_t aio_run_iocb(struct kiocb *iocb)\n{\n\tstruct kioctx\t*ctx = iocb->ki_ctx;\n\tssize_t (*retry)(struct kiocb *);\n\tssize_t ret;\n\n\tif (!(retry = iocb->ki_retry)) {\n\t\tprintk(\"aio_run_iocb: iocb->ki_retry = NULL\\n\");\n\t\treturn 0;\n\t}\n\n\t/*\n\t * We don't want the next retry iteration for this\n\t * operation to start until this one has returned and\n\t * updated the iocb state. However, wait_queue functions\n\t * can trigger a kick_iocb from interrupt context in the\n\t * meantime, indicating that data is available for the next\n\t * iteration. We want to remember that and enable the\n\t * next retry iteration _after_ we are through with\n\t * this one.\n\t *\n\t * So, in order to be able to register a \"kick\", but\n\t * prevent it from being queued now, we clear the kick\n\t * flag, but make the kick code *think* that the iocb is\n\t * still on the run list until we are actually done.\n\t * When we are done with this iteration, we check if\n\t * the iocb was kicked in the meantime and if so, queue\n\t * it up afresh.\n\t */\n\n\tkiocbClearKicked(iocb);\n\n\t/*\n\t * This is so that aio_complete knows it doesn't need to\n\t * pull the iocb off the run list (We can't just call\n\t * INIT_LIST_HEAD because we don't want a kick_iocb to\n\t * queue this on the run list yet)\n\t */\n\tiocb->ki_run_list.next = iocb->ki_run_list.prev = NULL;\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\t/* Quit retrying if the i/o has been cancelled */\n\tif (kiocbIsCancelled(iocb)) {\n\t\tret = -EINTR;\n\t\taio_complete(iocb, ret, 0);\n\t\t/* must not access the iocb after this */\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Now we are all set to call the retry method in async\n\t * context.\n\t */\n\tret = retry(iocb);\n\n\tif (ret != -EIOCBRETRY && ret != -EIOCBQUEUED) {\n\t\t/*\n\t\t * There's no easy way to restart the syscall since other AIO's\n\t\t * may be already running. Just fail this IO with EINTR.\n\t\t */\n\t\tif (unlikely(ret == -ERESTARTSYS || ret == -ERESTARTNOINTR ||\n\t\t\t     ret == -ERESTARTNOHAND || ret == -ERESTART_RESTARTBLOCK))\n\t\t\tret = -EINTR;\n\t\taio_complete(iocb, ret, 0);\n\t}\nout:\n\tspin_lock_irq(&ctx->ctx_lock);\n\n\tif (-EIOCBRETRY == ret) {\n\t\t/*\n\t\t * OK, now that we are done with this iteration\n\t\t * and know that there is more left to go,\n\t\t * this is where we let go so that a subsequent\n\t\t * \"kick\" can start the next iteration\n\t\t */\n\n\t\t/* will make __queue_kicked_iocb succeed from here on */\n\t\tINIT_LIST_HEAD(&iocb->ki_run_list);\n\t\t/* we must queue the next iteration ourselves, if it\n\t\t * has already been kicked */\n\t\tif (kiocbIsKicked(iocb)) {\n\t\t\t__queue_kicked_iocb(iocb);\n\n\t\t\t/*\n\t\t\t * __queue_kicked_iocb will always return 1 here, because\n\t\t\t * iocb->ki_run_list is empty at this point so it should\n\t\t\t * be safe to unconditionally queue the context into the\n\t\t\t * work queue.\n\t\t\t */\n\t\t\taio_queue_work(ctx);\n\t\t}\n\t}\n\treturn ret;\n}\n\n/*\n * __aio_run_iocbs:\n * \tProcess all pending retries queued on the ioctx\n * \trun list.\n * Assumes it is operating within the aio issuer's mm\n * context.\n */\nstatic int __aio_run_iocbs(struct kioctx *ctx)\n{\n\tstruct kiocb *iocb;\n\tstruct list_head run_list;\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\tlist_replace_init(&ctx->run_list, &run_list);\n\twhile (!list_empty(&run_list)) {\n\t\tiocb = list_entry(run_list.next, struct kiocb,\n\t\t\tki_run_list);\n\t\tlist_del(&iocb->ki_run_list);\n\t\t/*\n\t\t * Hold an extra reference while retrying i/o.\n\t\t */\n\t\tiocb->ki_users++;       /* grab extra reference */\n\t\taio_run_iocb(iocb);\n\t\t__aio_put_req(ctx, iocb);\n \t}\n\tif (!list_empty(&ctx->run_list))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void aio_queue_work(struct kioctx * ctx)\n{\n\tunsigned long timeout;\n\t/*\n\t * if someone is waiting, get the work started right\n\t * away, otherwise, use a longer delay\n\t */\n\tsmp_mb();\n\tif (waitqueue_active(&ctx->wait))\n\t\ttimeout = 1;\n\telse\n\t\ttimeout = HZ/10;\n\tqueue_delayed_work(aio_wq, &ctx->wq, timeout);\n}\n\n/*\n * aio_run_all_iocbs:\n *\tProcess all pending retries queued on the ioctx\n *\trun list, and keep running them until the list\n *\tstays empty.\n * Assumes it is operating within the aio issuer's mm context.\n */\nstatic inline void aio_run_all_iocbs(struct kioctx *ctx)\n{\n\tspin_lock_irq(&ctx->ctx_lock);\n\twhile (__aio_run_iocbs(ctx))\n\t\t;\n\tspin_unlock_irq(&ctx->ctx_lock);\n}\n\n/*\n * aio_kick_handler:\n * \tWork queue handler triggered to process pending\n * \tretries on an ioctx. Takes on the aio issuer's\n *\tmm context before running the iocbs, so that\n *\tcopy_xxx_user operates on the issuer's address\n *      space.\n * Run on aiod's context.\n */\nstatic void aio_kick_handler(struct work_struct *work)\n{\n\tstruct kioctx *ctx = container_of(work, struct kioctx, wq.work);\n\tmm_segment_t oldfs = get_fs();\n\tstruct mm_struct *mm;\n\tint requeue;\n\n\tset_fs(USER_DS);\n\tuse_mm(ctx->mm);\n\tspin_lock_irq(&ctx->ctx_lock);\n\trequeue =__aio_run_iocbs(ctx);\n\tmm = ctx->mm;\n\tspin_unlock_irq(&ctx->ctx_lock);\n \tunuse_mm(mm);\n\tset_fs(oldfs);\n\t/*\n\t * we're in a worker thread already, don't use queue_delayed_work,\n\t */\n\tif (requeue)\n\t\tqueue_delayed_work(aio_wq, &ctx->wq, 0);\n}\n\n\n/*\n * Called by kick_iocb to queue the kiocb for retry\n * and if required activate the aio work queue to process\n * it\n */\nstatic void try_queue_kicked_iocb(struct kiocb *iocb)\n{\n \tstruct kioctx\t*ctx = iocb->ki_ctx;\n\tunsigned long flags;\n\tint run = 0;\n\n\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\t/* set this inside the lock so that we can't race with aio_run_iocb()\n\t * testing it and putting the iocb on the run list under the lock */\n\tif (!kiocbTryKick(iocb))\n\t\trun = __queue_kicked_iocb(iocb);\n\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n\tif (run)\n\t\taio_queue_work(ctx);\n}\n\n/*\n * kick_iocb:\n *      Called typically from a wait queue callback context\n *      to trigger a retry of the iocb.\n *      The retry is usually executed by aio workqueue\n *      threads (See aio_kick_handler).\n */\nvoid kick_iocb(struct kiocb *iocb)\n{\n\t/* sync iocbs are easy: they can only ever be executing from a \n\t * single context. */\n\tif (is_sync_kiocb(iocb)) {\n\t\tkiocbSetKicked(iocb);\n\t        wake_up_process(iocb->ki_obj.tsk);\n\t\treturn;\n\t}\n\n\ttry_queue_kicked_iocb(iocb);\n}\nEXPORT_SYMBOL(kick_iocb);\n\n/* aio_complete\n *\tCalled when the io request on the given iocb is complete.\n *\tReturns true if this is the last user of the request.  The \n *\tonly other user of the request can be the cancellation code.\n */\nint aio_complete(struct kiocb *iocb, long res, long res2)\n{\n\tstruct kioctx\t*ctx = iocb->ki_ctx;\n\tstruct aio_ring_info\t*info;\n\tstruct aio_ring\t*ring;\n\tstruct io_event\t*event;\n\tunsigned long\tflags;\n\tunsigned long\ttail;\n\tint\t\tret;\n\n\t/*\n\t * Special case handling for sync iocbs:\n\t *  - events go directly into the iocb for fast handling\n\t *  - the sync task with the iocb in its stack holds the single iocb\n\t *    ref, no other paths have a way to get another ref\n\t *  - the sync task helpfully left a reference to itself in the iocb\n\t */\n\tif (is_sync_kiocb(iocb)) {\n\t\tBUG_ON(iocb->ki_users != 1);\n\t\tiocb->ki_user_data = res;\n\t\tiocb->ki_users = 0;\n\t\twake_up_process(iocb->ki_obj.tsk);\n\t\treturn 1;\n\t}\n\n\tinfo = &ctx->ring_info;\n\n\t/* add a completion event to the ring buffer.\n\t * must be done holding ctx->ctx_lock to prevent\n\t * other code from messing with the tail\n\t * pointer since we might be called from irq\n\t * context.\n\t */\n\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\n\tif (iocb->ki_run_list.prev && !list_empty(&iocb->ki_run_list))\n\t\tlist_del_init(&iocb->ki_run_list);\n\n\t/*\n\t * cancelled requests don't get events, userland was given one\n\t * when the event got cancelled.\n\t */\n\tif (kiocbIsCancelled(iocb))\n\t\tgoto put_rq;\n\n\tring = kmap_atomic(info->ring_pages[0], KM_IRQ1);\n\n\ttail = info->tail;\n\tevent = aio_ring_event(info, tail, KM_IRQ0);\n\tif (++tail >= info->nr)\n\t\ttail = 0;\n\n\tevent->obj = (u64)(unsigned long)iocb->ki_obj.user;\n\tevent->data = iocb->ki_user_data;\n\tevent->res = res;\n\tevent->res2 = res2;\n\n\tdprintk(\"aio_complete: %p[%lu]: %p: %p %Lx %lx %lx\\n\",\n\t\tctx, tail, iocb, iocb->ki_obj.user, iocb->ki_user_data,\n\t\tres, res2);\n\n\t/* after flagging the request as done, we\n\t * must never even look at it again\n\t */\n\tsmp_wmb();\t/* make event visible before updating tail */\n\n\tinfo->tail = tail;\n\tring->tail = tail;\n\n\tput_aio_ring_event(event, KM_IRQ0);\n\tkunmap_atomic(ring, KM_IRQ1);\n\n\tpr_debug(\"added to ring %p at [%lu]\\n\", iocb, tail);\n\n\t/*\n\t * Check if the user asked us to deliver the result through an\n\t * eventfd. The eventfd_signal() function is safe to be called\n\t * from IRQ context.\n\t */\n\tif (iocb->ki_eventfd != NULL)\n\t\teventfd_signal(iocb->ki_eventfd, 1);\n\nput_rq:\n\t/* everything turned out well, dispose of the aiocb. */\n\tret = __aio_put_req(ctx, iocb);\n\n\t/*\n\t * We have to order our ring_info tail store above and test\n\t * of the wait list below outside the wait lock.  This is\n\t * like in wake_up_bit() where clearing a bit has to be\n\t * ordered with the unlocked test.\n\t */\n\tsmp_mb();\n\n\tif (waitqueue_active(&ctx->wait))\n\t\twake_up(&ctx->wait);\n\n\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(aio_complete);\n\n/* aio_read_evt\n *\tPull an event off of the ioctx's event ring.  Returns the number of \n *\tevents fetched (0 or 1 ;-)\n *\tFIXME: make this use cmpxchg.\n *\tTODO: make the ringbuffer user mmap()able (requires FIXME).\n */\nstatic int aio_read_evt(struct kioctx *ioctx, struct io_event *ent)\n{\n\tstruct aio_ring_info *info = &ioctx->ring_info;\n\tstruct aio_ring *ring;\n\tunsigned long head;\n\tint ret = 0;\n\n\tring = kmap_atomic(info->ring_pages[0], KM_USER0);\n\tdprintk(\"in aio_read_evt h%lu t%lu m%lu\\n\",\n\t\t (unsigned long)ring->head, (unsigned long)ring->tail,\n\t\t (unsigned long)ring->nr);\n\n\tif (ring->head == ring->tail)\n\t\tgoto out;\n\n\tspin_lock(&info->ring_lock);\n\n\thead = ring->head % info->nr;\n\tif (head != ring->tail) {\n\t\tstruct io_event *evp = aio_ring_event(info, head, KM_USER1);\n\t\t*ent = *evp;\n\t\thead = (head + 1) % info->nr;\n\t\tsmp_mb(); /* finish reading the event before updatng the head */\n\t\tring->head = head;\n\t\tret = 1;\n\t\tput_aio_ring_event(evp, KM_USER1);\n\t}\n\tspin_unlock(&info->ring_lock);\n\nout:\n\tkunmap_atomic(ring, KM_USER0);\n\tdprintk(\"leaving aio_read_evt: %d  h%lu t%lu\\n\", ret,\n\t\t (unsigned long)ring->head, (unsigned long)ring->tail);\n\treturn ret;\n}\n\nstruct aio_timeout {\n\tstruct timer_list\ttimer;\n\tint\t\t\ttimed_out;\n\tstruct task_struct\t*p;\n};\n\nstatic void timeout_func(unsigned long data)\n{\n\tstruct aio_timeout *to = (struct aio_timeout *)data;\n\n\tto->timed_out = 1;\n\twake_up_process(to->p);\n}\n\nstatic inline void init_timeout(struct aio_timeout *to)\n{\n\tsetup_timer_on_stack(&to->timer, timeout_func, (unsigned long) to);\n\tto->timed_out = 0;\n\tto->p = current;\n}\n\nstatic inline void set_timeout(long start_jiffies, struct aio_timeout *to,\n\t\t\t       const struct timespec *ts)\n{\n\tto->timer.expires = start_jiffies + timespec_to_jiffies(ts);\n\tif (time_after(to->timer.expires, jiffies))\n\t\tadd_timer(&to->timer);\n\telse\n\t\tto->timed_out = 1;\n}\n\nstatic inline void clear_timeout(struct aio_timeout *to)\n{\n\tdel_singleshot_timer_sync(&to->timer);\n}\n\nstatic int read_events(struct kioctx *ctx,\n\t\t\tlong min_nr, long nr,\n\t\t\tstruct io_event __user *event,\n\t\t\tstruct timespec __user *timeout)\n{\n\tlong\t\t\tstart_jiffies = jiffies;\n\tstruct task_struct\t*tsk = current;\n\tDECLARE_WAITQUEUE(wait, tsk);\n\tint\t\t\tret;\n\tint\t\t\ti = 0;\n\tstruct io_event\t\tent;\n\tstruct aio_timeout\tto;\n\tint\t\t\tretry = 0;\n\n\t/* needed to zero any padding within an entry (there shouldn't be \n\t * any, but C is fun!\n\t */\n\tmemset(&ent, 0, sizeof(ent));\nretry:\n\tret = 0;\n\twhile (likely(i < nr)) {\n\t\tret = aio_read_evt(ctx, &ent);\n\t\tif (unlikely(ret <= 0))\n\t\t\tbreak;\n\n\t\tdprintk(\"read event: %Lx %Lx %Lx %Lx\\n\",\n\t\t\tent.data, ent.obj, ent.res, ent.res2);\n\n\t\t/* Could we split the check in two? */\n\t\tret = -EFAULT;\n\t\tif (unlikely(copy_to_user(event, &ent, sizeof(ent)))) {\n\t\t\tdprintk(\"aio: lost an event due to EFAULT.\\n\");\n\t\t\tbreak;\n\t\t}\n\t\tret = 0;\n\n\t\t/* Good, event copied to userland, update counts. */\n\t\tevent ++;\n\t\ti ++;\n\t}\n\n\tif (min_nr <= i)\n\t\treturn i;\n\tif (ret)\n\t\treturn ret;\n\n\t/* End fast path */\n\n\t/* racey check, but it gets redone */\n\tif (!retry && unlikely(!list_empty(&ctx->run_list))) {\n\t\tretry = 1;\n\t\taio_run_all_iocbs(ctx);\n\t\tgoto retry;\n\t}\n\n\tinit_timeout(&to);\n\tif (timeout) {\n\t\tstruct timespec\tts;\n\t\tret = -EFAULT;\n\t\tif (unlikely(copy_from_user(&ts, timeout, sizeof(ts))))\n\t\t\tgoto out;\n\n\t\tset_timeout(start_jiffies, &to, &ts);\n\t}\n\n\twhile (likely(i < nr)) {\n\t\tadd_wait_queue_exclusive(&ctx->wait, &wait);\n\t\tdo {\n\t\t\tset_task_state(tsk, TASK_INTERRUPTIBLE);\n\t\t\tret = aio_read_evt(ctx, &ent);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tif (min_nr <= i)\n\t\t\t\tbreak;\n\t\t\tif (unlikely(ctx->dead)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (to.timed_out)\t/* Only check after read evt */\n\t\t\t\tbreak;\n\t\t\t/* Try to only show up in io wait if there are ops\n\t\t\t *  in flight */\n\t\t\tif (ctx->reqs_active)\n\t\t\t\tio_schedule();\n\t\t\telse\n\t\t\t\tschedule();\n\t\t\tif (signal_pending(tsk)) {\n\t\t\t\tret = -EINTR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/*ret = aio_read_evt(ctx, &ent);*/\n\t\t} while (1) ;\n\n\t\tset_task_state(tsk, TASK_RUNNING);\n\t\tremove_wait_queue(&ctx->wait, &wait);\n\n\t\tif (unlikely(ret <= 0))\n\t\t\tbreak;\n\n\t\tret = -EFAULT;\n\t\tif (unlikely(copy_to_user(event, &ent, sizeof(ent)))) {\n\t\t\tdprintk(\"aio: lost an event due to EFAULT.\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Good, event copied to userland, update counts. */\n\t\tevent ++;\n\t\ti ++;\n\t}\n\n\tif (timeout)\n\t\tclear_timeout(&to);\nout:\n\tdestroy_timer_on_stack(&to.timer);\n\treturn i ? i : ret;\n}\n\n/* Take an ioctx and remove it from the list of ioctx's.  Protects \n * against races with itself via ->dead.\n */\nstatic void io_destroy(struct kioctx *ioctx)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint was_dead;\n\n\t/* delete the entry from the list is someone else hasn't already */\n\tspin_lock(&mm->ioctx_lock);\n\twas_dead = ioctx->dead;\n\tioctx->dead = 1;\n\thlist_del_rcu(&ioctx->list);\n\tspin_unlock(&mm->ioctx_lock);\n\n\tdprintk(\"aio_release(%p)\\n\", ioctx);\n\tif (likely(!was_dead))\n\t\tput_ioctx(ioctx);\t/* twice for the list */\n\n\taio_cancel_all(ioctx);\n\twait_for_all_aios(ioctx);\n\n\t/*\n\t * Wake up any waiters.  The setting of ctx->dead must be seen\n\t * by other CPUs at this point.  Right now, we rely on the\n\t * locking done by the above calls to ensure this consistency.\n\t */\n\twake_up_all(&ioctx->wait);\n\tput_ioctx(ioctx);\t/* once for the lookup */\n}\n\n/* sys_io_setup:\n *\tCreate an aio_context capable of receiving at least nr_events.\n *\tctxp must not point to an aio_context that already exists, and\n *\tmust be initialized to 0 prior to the call.  On successful\n *\tcreation of the aio_context, *ctxp is filled in with the resulting \n *\thandle.  May fail with -EINVAL if *ctxp is not initialized,\n *\tif the specified nr_events exceeds internal limits.  May fail \n *\twith -EAGAIN if the specified nr_events exceeds the user's limit \n *\tof available events.  May fail with -ENOMEM if insufficient kernel\n *\tresources are available.  May fail with -EFAULT if an invalid\n *\tpointer is passed for ctxp.  Will fail with -ENOSYS if not\n *\timplemented.\n */\nSYSCALL_DEFINE2(io_setup, unsigned, nr_events, aio_context_t __user *, ctxp)\n{\n\tstruct kioctx *ioctx = NULL;\n\tunsigned long ctx;\n\tlong ret;\n\n\tret = get_user(ctx, ctxp);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (unlikely(ctx || nr_events == 0)) {\n\t\tpr_debug(\"EINVAL: io_setup: ctx %lu nr_events %u\\n\",\n\t\t         ctx, nr_events);\n\t\tgoto out;\n\t}\n\n\tioctx = ioctx_alloc(nr_events);\n\tret = PTR_ERR(ioctx);\n\tif (!IS_ERR(ioctx)) {\n\t\tret = put_user(ioctx->user_id, ctxp);\n\t\tif (!ret)\n\t\t\treturn 0;\n\n\t\tget_ioctx(ioctx); /* io_destroy() expects us to hold a ref */\n\t\tio_destroy(ioctx);\n\t}\n\nout:\n\treturn ret;\n}\n\n/* sys_io_destroy:\n *\tDestroy the aio_context specified.  May cancel any outstanding \n *\tAIOs and block on completion.  Will fail with -ENOSYS if not\n *\timplemented.  May fail with -EINVAL if the context pointed to\n *\tis invalid.\n */\nSYSCALL_DEFINE1(io_destroy, aio_context_t, ctx)\n{\n\tstruct kioctx *ioctx = lookup_ioctx(ctx);\n\tif (likely(NULL != ioctx)) {\n\t\tio_destroy(ioctx);\n\t\treturn 0;\n\t}\n\tpr_debug(\"EINVAL: io_destroy: invalid context id\\n\");\n\treturn -EINVAL;\n}\n\nstatic void aio_advance_iovec(struct kiocb *iocb, ssize_t ret)\n{\n\tstruct iovec *iov = &iocb->ki_iovec[iocb->ki_cur_seg];\n\n\tBUG_ON(ret <= 0);\n\n\twhile (iocb->ki_cur_seg < iocb->ki_nr_segs && ret > 0) {\n\t\tssize_t this = min((ssize_t)iov->iov_len, ret);\n\t\tiov->iov_base += this;\n\t\tiov->iov_len -= this;\n\t\tiocb->ki_left -= this;\n\t\tret -= this;\n\t\tif (iov->iov_len == 0) {\n\t\t\tiocb->ki_cur_seg++;\n\t\t\tiov++;\n\t\t}\n\t}\n\n\t/* the caller should not have done more io than what fit in\n\t * the remaining iovecs */\n\tBUG_ON(ret > 0 && iocb->ki_left == 0);\n}\n\nstatic ssize_t aio_rw_vect_retry(struct kiocb *iocb)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tssize_t (*rw_op)(struct kiocb *, const struct iovec *,\n\t\t\t unsigned long, loff_t);\n\tssize_t ret = 0;\n\tunsigned short opcode;\n\n\tif ((iocb->ki_opcode == IOCB_CMD_PREADV) ||\n\t\t(iocb->ki_opcode == IOCB_CMD_PREAD)) {\n\t\trw_op = file->f_op->aio_read;\n\t\topcode = IOCB_CMD_PREADV;\n\t} else {\n\t\trw_op = file->f_op->aio_write;\n\t\topcode = IOCB_CMD_PWRITEV;\n\t}\n\n\t/* This matches the pread()/pwrite() logic */\n\tif (iocb->ki_pos < 0)\n\t\treturn -EINVAL;\n\n\tdo {\n\t\tret = rw_op(iocb, &iocb->ki_iovec[iocb->ki_cur_seg],\n\t\t\t    iocb->ki_nr_segs - iocb->ki_cur_seg,\n\t\t\t    iocb->ki_pos);\n\t\tif (ret > 0)\n\t\t\taio_advance_iovec(iocb, ret);\n\n\t/* retry all partial writes.  retry partial reads as long as its a\n\t * regular file. */\n\t} while (ret > 0 && iocb->ki_left > 0 &&\n\t\t (opcode == IOCB_CMD_PWRITEV ||\n\t\t  (!S_ISFIFO(inode->i_mode) && !S_ISSOCK(inode->i_mode))));\n\n\t/* This means we must have transferred all that we could */\n\t/* No need to retry anymore */\n\tif ((ret == 0) || (iocb->ki_left == 0))\n\t\tret = iocb->ki_nbytes - iocb->ki_left;\n\n\t/* If we managed to write some out we return that, rather than\n\t * the eventual error. */\n\tif (opcode == IOCB_CMD_PWRITEV\n\t    && ret < 0 && ret != -EIOCBQUEUED && ret != -EIOCBRETRY\n\t    && iocb->ki_nbytes - iocb->ki_left)\n\t\tret = iocb->ki_nbytes - iocb->ki_left;\n\n\treturn ret;\n}\n\nstatic ssize_t aio_fdsync(struct kiocb *iocb)\n{\n\tstruct file *file = iocb->ki_filp;\n\tssize_t ret = -EINVAL;\n\n\tif (file->f_op->aio_fsync)\n\t\tret = file->f_op->aio_fsync(iocb, 1);\n\treturn ret;\n}\n\nstatic ssize_t aio_fsync(struct kiocb *iocb)\n{\n\tstruct file *file = iocb->ki_filp;\n\tssize_t ret = -EINVAL;\n\n\tif (file->f_op->aio_fsync)\n\t\tret = file->f_op->aio_fsync(iocb, 0);\n\treturn ret;\n}\n\nstatic ssize_t aio_setup_vectored_rw(int type, struct kiocb *kiocb, bool compat)\n{\n\tssize_t ret;\n\n#ifdef CONFIG_COMPAT\n\tif (compat)\n\t\tret = compat_rw_copy_check_uvector(type,\n\t\t\t\t(struct compat_iovec __user *)kiocb->ki_buf,\n\t\t\t\tkiocb->ki_nbytes, 1, &kiocb->ki_inline_vec,\n\t\t\t\t&kiocb->ki_iovec, 1);\n\telse\n#endif\n\t\tret = rw_copy_check_uvector(type,\n\t\t\t\t(struct iovec __user *)kiocb->ki_buf,\n\t\t\t\tkiocb->ki_nbytes, 1, &kiocb->ki_inline_vec,\n\t\t\t\t&kiocb->ki_iovec, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tkiocb->ki_nr_segs = kiocb->ki_nbytes;\n\tkiocb->ki_cur_seg = 0;\n\t/* ki_nbytes/left now reflect bytes instead of segs */\n\tkiocb->ki_nbytes = ret;\n\tkiocb->ki_left = ret;\n\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic ssize_t aio_setup_single_vector(struct kiocb *kiocb)\n{\n\tkiocb->ki_iovec = &kiocb->ki_inline_vec;\n\tkiocb->ki_iovec->iov_base = kiocb->ki_buf;\n\tkiocb->ki_iovec->iov_len = kiocb->ki_left;\n\tkiocb->ki_nr_segs = 1;\n\tkiocb->ki_cur_seg = 0;\n\treturn 0;\n}\n\n/*\n * aio_setup_iocb:\n *\tPerforms the initial checks and aio retry method\n *\tsetup for the kiocb at the time of io submission.\n */\nstatic ssize_t aio_setup_iocb(struct kiocb *kiocb, bool compat)\n{\n\tstruct file *file = kiocb->ki_filp;\n\tssize_t ret = 0;\n\n\tswitch (kiocb->ki_opcode) {\n\tcase IOCB_CMD_PREAD:\n\t\tret = -EBADF;\n\t\tif (unlikely(!(file->f_mode & FMODE_READ)))\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (unlikely(!access_ok(VERIFY_WRITE, kiocb->ki_buf,\n\t\t\tkiocb->ki_left)))\n\t\t\tbreak;\n\t\tret = security_file_permission(file, MAY_READ);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t\tret = aio_setup_single_vector(kiocb);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_read)\n\t\t\tkiocb->ki_retry = aio_rw_vect_retry;\n\t\tbreak;\n\tcase IOCB_CMD_PWRITE:\n\t\tret = -EBADF;\n\t\tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (unlikely(!access_ok(VERIFY_READ, kiocb->ki_buf,\n\t\t\tkiocb->ki_left)))\n\t\t\tbreak;\n\t\tret = security_file_permission(file, MAY_WRITE);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t\tret = aio_setup_single_vector(kiocb);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_write)\n\t\t\tkiocb->ki_retry = aio_rw_vect_retry;\n\t\tbreak;\n\tcase IOCB_CMD_PREADV:\n\t\tret = -EBADF;\n\t\tif (unlikely(!(file->f_mode & FMODE_READ)))\n\t\t\tbreak;\n\t\tret = security_file_permission(file, MAY_READ);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t\tret = aio_setup_vectored_rw(READ, kiocb, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_read)\n\t\t\tkiocb->ki_retry = aio_rw_vect_retry;\n\t\tbreak;\n\tcase IOCB_CMD_PWRITEV:\n\t\tret = -EBADF;\n\t\tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n\t\t\tbreak;\n\t\tret = security_file_permission(file, MAY_WRITE);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t\tret = aio_setup_vectored_rw(WRITE, kiocb, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_write)\n\t\t\tkiocb->ki_retry = aio_rw_vect_retry;\n\t\tbreak;\n\tcase IOCB_CMD_FDSYNC:\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_fsync)\n\t\t\tkiocb->ki_retry = aio_fdsync;\n\t\tbreak;\n\tcase IOCB_CMD_FSYNC:\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_fsync)\n\t\t\tkiocb->ki_retry = aio_fsync;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"EINVAL: io_submit: no operation provided\\n\");\n\t\tret = -EINVAL;\n\t}\n\n\tif (!kiocb->ki_retry)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic int io_submit_one(struct kioctx *ctx, struct iocb __user *user_iocb,\n\t\t\t struct iocb *iocb, struct kiocb_batch *batch,\n\t\t\t bool compat)\n{\n\tstruct kiocb *req;\n\tstruct file *file;\n\tssize_t ret;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(iocb->aio_reserved1 || iocb->aio_reserved2)) {\n\t\tpr_debug(\"EINVAL: io_submit: reserve field set\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* prevent overflows */\n\tif (unlikely(\n\t    (iocb->aio_buf != (unsigned long)iocb->aio_buf) ||\n\t    (iocb->aio_nbytes != (size_t)iocb->aio_nbytes) ||\n\t    ((ssize_t)iocb->aio_nbytes < 0)\n\t   )) {\n\t\tpr_debug(\"EINVAL: io_submit: overflow check\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tfile = fget(iocb->aio_fildes);\n\tif (unlikely(!file))\n\t\treturn -EBADF;\n\n\treq = aio_get_req(ctx, batch);  /* returns with 2 references to req */\n\tif (unlikely(!req)) {\n\t\tfput(file);\n\t\treturn -EAGAIN;\n\t}\n\treq->ki_filp = file;\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD) {\n\t\t/*\n\t\t * If the IOCB_FLAG_RESFD flag of aio_flags is set, get an\n\t\t * instance of the file* now. The file descriptor must be\n\t\t * an eventfd() fd, and will be signaled for each completed\n\t\t * event using the eventfd_signal() function.\n\t\t */\n\t\treq->ki_eventfd = eventfd_ctx_fdget((int) iocb->aio_resfd);\n\t\tif (IS_ERR(req->ki_eventfd)) {\n\t\t\tret = PTR_ERR(req->ki_eventfd);\n\t\t\treq->ki_eventfd = NULL;\n\t\t\tgoto out_put_req;\n\t\t}\n\t}\n\n\tret = put_user(req->ki_key, &user_iocb->aio_key);\n\tif (unlikely(ret)) {\n\t\tdprintk(\"EFAULT: aio_key\\n\");\n\t\tgoto out_put_req;\n\t}\n\n\treq->ki_obj.user = user_iocb;\n\treq->ki_user_data = iocb->aio_data;\n\treq->ki_pos = iocb->aio_offset;\n\n\treq->ki_buf = (char __user *)(unsigned long)iocb->aio_buf;\n\treq->ki_left = req->ki_nbytes = iocb->aio_nbytes;\n\treq->ki_opcode = iocb->aio_lio_opcode;\n\n\tret = aio_setup_iocb(req, compat);\n\n\tif (ret)\n\t\tgoto out_put_req;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\t/*\n\t * We could have raced with io_destroy() and are currently holding a\n\t * reference to ctx which should be destroyed. We cannot submit IO\n\t * since ctx gets freed as soon as io_submit() puts its reference.  The\n\t * check here is reliable: io_destroy() sets ctx->dead before waiting\n\t * for outstanding IO and the barrier between these two is realized by\n\t * unlock of mm->ioctx_lock and lock of ctx->ctx_lock.  Analogously we\n\t * increment ctx->reqs_active before checking for ctx->dead and the\n\t * barrier is realized by unlock and lock of ctx->ctx_lock. Thus if we\n\t * don't see ctx->dead set here, io_destroy() waits for our IO to\n\t * finish.\n\t */\n\tif (ctx->dead) {\n\t\tspin_unlock_irq(&ctx->ctx_lock);\n\t\tret = -EINVAL;\n\t\tgoto out_put_req;\n\t}\n\taio_run_iocb(req);\n\tif (!list_empty(&ctx->run_list)) {\n\t\t/* drain the run list */\n\t\twhile (__aio_run_iocbs(ctx))\n\t\t\t;\n\t}\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\taio_put_req(req);\t/* drop extra ref to req */\n\treturn 0;\n\nout_put_req:\n\taio_put_req(req);\t/* drop extra ref to req */\n\taio_put_req(req);\t/* drop i/o ref to req */\n\treturn ret;\n}\n\nlong do_io_submit(aio_context_t ctx_id, long nr,\n\t\t  struct iocb __user *__user *iocbpp, bool compat)\n{\n\tstruct kioctx *ctx;\n\tlong ret = 0;\n\tint i = 0;\n\tstruct blk_plug plug;\n\tstruct kiocb_batch batch;\n\n\tif (unlikely(nr < 0))\n\t\treturn -EINVAL;\n\n\tif (unlikely(nr > LONG_MAX/sizeof(*iocbpp)))\n\t\tnr = LONG_MAX/sizeof(*iocbpp);\n\n\tif (unlikely(!access_ok(VERIFY_READ, iocbpp, (nr*sizeof(*iocbpp)))))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx)) {\n\t\tpr_debug(\"EINVAL: io_submit: invalid context id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tkiocb_batch_init(&batch, nr);\n\n\tblk_start_plug(&plug);\n\n\t/*\n\t * AKPM: should this return a partial result if some of the IOs were\n\t * successfully submitted?\n\t */\n\tfor (i=0; i<nr; i++) {\n\t\tstruct iocb __user *user_iocb;\n\t\tstruct iocb tmp;\n\n\t\tif (unlikely(__get_user(user_iocb, iocbpp + i))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(copy_from_user(&tmp, user_iocb, sizeof(tmp)))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = io_submit_one(ctx, user_iocb, &tmp, &batch, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tblk_finish_plug(&plug);\n\n\tkiocb_batch_free(&batch);\n\tput_ioctx(ctx);\n\treturn i ? i : ret;\n}\n\n/* sys_io_submit:\n *\tQueue the nr iocbs pointed to by iocbpp for processing.  Returns\n *\tthe number of iocbs queued.  May return -EINVAL if the aio_context\n *\tspecified by ctx_id is invalid, if nr is < 0, if the iocb at\n *\t*iocbpp[0] is not properly initialized, if the operation specified\n *\tis invalid for the file descriptor in the iocb.  May fail with\n *\t-EFAULT if any of the data structures point to invalid data.  May\n *\tfail with -EBADF if the file descriptor specified in the first\n *\tiocb is invalid.  May fail with -EAGAIN if insufficient resources\n *\tare available to queue any iocbs.  Will return 0 if nr is 0.  Will\n *\tfail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr,\n\t\tstruct iocb __user * __user *, iocbpp)\n{\n\treturn do_io_submit(ctx_id, nr, iocbpp, 0);\n}\n\n/* lookup_kiocb\n *\tFinds a given iocb for cancellation.\n */\nstatic struct kiocb *lookup_kiocb(struct kioctx *ctx, struct iocb __user *iocb,\n\t\t\t\t  u32 key)\n{\n\tstruct list_head *pos;\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\t/* TODO: use a hash or array, this sucks. */\n\tlist_for_each(pos, &ctx->active_reqs) {\n\t\tstruct kiocb *kiocb = list_kiocb(pos);\n\t\tif (kiocb->ki_obj.user == iocb && kiocb->ki_key == key)\n\t\t\treturn kiocb;\n\t}\n\treturn NULL;\n}\n\n/* sys_io_cancel:\n *\tAttempts to cancel an iocb previously passed to io_submit.  If\n *\tthe operation is successfully cancelled, the resulting event is\n *\tcopied into the memory pointed to by result without being placed\n *\tinto the completion queue and 0 is returned.  May fail with\n *\t-EFAULT if any of the data structures pointed to are invalid.\n *\tMay fail with -EINVAL if aio_context specified by ctx_id is\n *\tinvalid.  May fail with -EAGAIN if the iocb specified was not\n *\tcancelled.  Will fail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE3(io_cancel, aio_context_t, ctx_id, struct iocb __user *, iocb,\n\t\tstruct io_event __user *, result)\n{\n\tint (*cancel)(struct kiocb *iocb, struct io_event *res);\n\tstruct kioctx *ctx;\n\tstruct kiocb *kiocb;\n\tu32 key;\n\tint ret;\n\n\tret = get_user(key, &iocb->aio_key);\n\tif (unlikely(ret))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx))\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tret = -EAGAIN;\n\tkiocb = lookup_kiocb(ctx, iocb, key);\n\tif (kiocb && kiocb->ki_cancel) {\n\t\tcancel = kiocb->ki_cancel;\n\t\tkiocb->ki_users ++;\n\t\tkiocbSetCancelled(kiocb);\n\t} else\n\t\tcancel = NULL;\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\tif (NULL != cancel) {\n\t\tstruct io_event tmp;\n\t\tpr_debug(\"calling cancel\\n\");\n\t\tmemset(&tmp, 0, sizeof(tmp));\n\t\ttmp.obj = (u64)(unsigned long)kiocb->ki_obj.user;\n\t\ttmp.data = kiocb->ki_user_data;\n\t\tret = cancel(kiocb, &tmp);\n\t\tif (!ret) {\n\t\t\t/* Cancellation succeeded -- copy the result\n\t\t\t * into the user's buffer.\n\t\t\t */\n\t\t\tif (copy_to_user(result, &tmp, sizeof(tmp)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\t} else\n\t\tret = -EINVAL;\n\n\tput_ioctx(ctx);\n\n\treturn ret;\n}\n\n/* io_getevents:\n *\tAttempts to read at least min_nr events and up to nr events from\n *\tthe completion queue for the aio_context specified by ctx_id. If\n *\tit succeeds, the number of read events is returned. May fail with\n *\t-EINVAL if ctx_id is invalid, if min_nr is out of range, if nr is\n *\tout of range, if timeout is out of range.  May fail with -EFAULT\n *\tif any of the memory specified is invalid.  May return 0 or\n *\t< min_nr if the timeout specified by timeout has elapsed\n *\tbefore sufficient events are available, where timeout == NULL\n *\tspecifies an infinite timeout. Note that the timeout pointed to by\n *\ttimeout is relative and will be updated if not NULL and the\n *\toperation blocks. Will fail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE5(io_getevents, aio_context_t, ctx_id,\n\t\tlong, min_nr,\n\t\tlong, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct timespec __user *, timeout)\n{\n\tstruct kioctx *ioctx = lookup_ioctx(ctx_id);\n\tlong ret = -EINVAL;\n\n\tif (likely(ioctx)) {\n\t\tif (likely(min_nr <= nr && min_nr >= 0))\n\t\t\tret = read_events(ioctx, min_nr, nr, events, timeout);\n\t\tput_ioctx(ioctx);\n\t}\n\n\tasmlinkage_protect(5, ret, ctx_id, min_nr, nr, events, timeout);\n\treturn ret;\n}\n"], "fixing_code": ["/*\n *\tAn async IO implementation for Linux\n *\tWritten by Benjamin LaHaise <bcrl@kvack.org>\n *\n *\tImplements an efficient asynchronous io interface.\n *\n *\tCopyright 2000, 2001, 2002 Red Hat, Inc.  All Rights Reserved.\n *\n *\tSee ../COPYING for licensing terms.\n */\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/time.h>\n#include <linux/aio_abi.h>\n#include <linux/module.h>\n#include <linux/syscalls.h>\n#include <linux/backing-dev.h>\n#include <linux/uio.h>\n\n#define DEBUG 0\n\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/mmu_context.h>\n#include <linux/slab.h>\n#include <linux/timer.h>\n#include <linux/aio.h>\n#include <linux/highmem.h>\n#include <linux/workqueue.h>\n#include <linux/security.h>\n#include <linux/eventfd.h>\n#include <linux/blkdev.h>\n#include <linux/compat.h>\n\n#include <asm/kmap_types.h>\n#include <asm/uaccess.h>\n\n#if DEBUG > 1\n#define dprintk\t\tprintk\n#else\n#define dprintk(x...)\tdo { ; } while (0)\n#endif\n\n/*------ sysctl variables----*/\nstatic DEFINE_SPINLOCK(aio_nr_lock);\nunsigned long aio_nr;\t\t/* current system wide number of aio requests */\nunsigned long aio_max_nr = 0x10000; /* system wide maximum number of aio requests */\n/*----end sysctl variables---*/\n\nstatic struct kmem_cache\t*kiocb_cachep;\nstatic struct kmem_cache\t*kioctx_cachep;\n\nstatic struct workqueue_struct *aio_wq;\n\n/* Used for rare fput completion. */\nstatic void aio_fput_routine(struct work_struct *);\nstatic DECLARE_WORK(fput_work, aio_fput_routine);\n\nstatic DEFINE_SPINLOCK(fput_lock);\nstatic LIST_HEAD(fput_head);\n\nstatic void aio_kick_handler(struct work_struct *);\nstatic void aio_queue_work(struct kioctx *);\n\n/* aio_setup\n *\tCreates the slab caches used by the aio routines, panic on\n *\tfailure as this is done early during the boot sequence.\n */\nstatic int __init aio_setup(void)\n{\n\tkiocb_cachep = KMEM_CACHE(kiocb, SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\tkioctx_cachep = KMEM_CACHE(kioctx,SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\n\taio_wq = alloc_workqueue(\"aio\", 0, 1);\t/* used to limit concurrency */\n\tBUG_ON(!aio_wq);\n\n\tpr_debug(\"aio_setup: sizeof(struct page) = %d\\n\", (int)sizeof(struct page));\n\n\treturn 0;\n}\n__initcall(aio_setup);\n\nstatic void aio_free_ring(struct kioctx *ctx)\n{\n\tstruct aio_ring_info *info = &ctx->ring_info;\n\tlong i;\n\n\tfor (i=0; i<info->nr_pages; i++)\n\t\tput_page(info->ring_pages[i]);\n\n\tif (info->mmap_size) {\n\t\tdown_write(&ctx->mm->mmap_sem);\n\t\tdo_munmap(ctx->mm, info->mmap_base, info->mmap_size);\n\t\tup_write(&ctx->mm->mmap_sem);\n\t}\n\n\tif (info->ring_pages && info->ring_pages != info->internal_pages)\n\t\tkfree(info->ring_pages);\n\tinfo->ring_pages = NULL;\n\tinfo->nr = 0;\n}\n\nstatic int aio_setup_ring(struct kioctx *ctx)\n{\n\tstruct aio_ring *ring;\n\tstruct aio_ring_info *info = &ctx->ring_info;\n\tunsigned nr_events = ctx->max_reqs;\n\tunsigned long size;\n\tint nr_pages;\n\n\t/* Compensate for the ring buffer's head/tail overlap entry */\n\tnr_events += 2;\t/* 1 is required, 2 for good luck */\n\n\tsize = sizeof(struct aio_ring);\n\tsize += sizeof(struct io_event) * nr_events;\n\tnr_pages = (size + PAGE_SIZE-1) >> PAGE_SHIFT;\n\n\tif (nr_pages < 0)\n\t\treturn -EINVAL;\n\n\tnr_events = (PAGE_SIZE * nr_pages - sizeof(struct aio_ring)) / sizeof(struct io_event);\n\n\tinfo->nr = 0;\n\tinfo->ring_pages = info->internal_pages;\n\tif (nr_pages > AIO_RING_PAGES) {\n\t\tinfo->ring_pages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\t\tif (!info->ring_pages)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tinfo->mmap_size = nr_pages * PAGE_SIZE;\n\tdprintk(\"attempting mmap of %lu bytes\\n\", info->mmap_size);\n\tdown_write(&ctx->mm->mmap_sem);\n\tinfo->mmap_base = do_mmap(NULL, 0, info->mmap_size, \n\t\t\t\t  PROT_READ|PROT_WRITE, MAP_ANONYMOUS|MAP_PRIVATE,\n\t\t\t\t  0);\n\tif (IS_ERR((void *)info->mmap_base)) {\n\t\tup_write(&ctx->mm->mmap_sem);\n\t\tinfo->mmap_size = 0;\n\t\taio_free_ring(ctx);\n\t\treturn -EAGAIN;\n\t}\n\n\tdprintk(\"mmap address: 0x%08lx\\n\", info->mmap_base);\n\tinfo->nr_pages = get_user_pages(current, ctx->mm,\n\t\t\t\t\tinfo->mmap_base, nr_pages, \n\t\t\t\t\t1, 0, info->ring_pages, NULL);\n\tup_write(&ctx->mm->mmap_sem);\n\n\tif (unlikely(info->nr_pages != nr_pages)) {\n\t\taio_free_ring(ctx);\n\t\treturn -EAGAIN;\n\t}\n\n\tctx->user_id = info->mmap_base;\n\n\tinfo->nr = nr_events;\t\t/* trusted copy */\n\n\tring = kmap_atomic(info->ring_pages[0], KM_USER0);\n\tring->nr = nr_events;\t/* user copy */\n\tring->id = ctx->user_id;\n\tring->head = ring->tail = 0;\n\tring->magic = AIO_RING_MAGIC;\n\tring->compat_features = AIO_RING_COMPAT_FEATURES;\n\tring->incompat_features = AIO_RING_INCOMPAT_FEATURES;\n\tring->header_length = sizeof(struct aio_ring);\n\tkunmap_atomic(ring, KM_USER0);\n\n\treturn 0;\n}\n\n\n/* aio_ring_event: returns a pointer to the event at the given index from\n * kmap_atomic(, km).  Release the pointer with put_aio_ring_event();\n */\n#define AIO_EVENTS_PER_PAGE\t(PAGE_SIZE / sizeof(struct io_event))\n#define AIO_EVENTS_FIRST_PAGE\t((PAGE_SIZE - sizeof(struct aio_ring)) / sizeof(struct io_event))\n#define AIO_EVENTS_OFFSET\t(AIO_EVENTS_PER_PAGE - AIO_EVENTS_FIRST_PAGE)\n\n#define aio_ring_event(info, nr, km) ({\t\t\t\t\t\\\n\tunsigned pos = (nr) + AIO_EVENTS_OFFSET;\t\t\t\\\n\tstruct io_event *__event;\t\t\t\t\t\\\n\t__event = kmap_atomic(\t\t\t\t\t\t\\\n\t\t\t(info)->ring_pages[pos / AIO_EVENTS_PER_PAGE], km); \\\n\t__event += pos % AIO_EVENTS_PER_PAGE;\t\t\t\t\\\n\t__event;\t\t\t\t\t\t\t\\\n})\n\n#define put_aio_ring_event(event, km) do {\t\\\n\tstruct io_event *__event = (event);\t\\\n\t(void)__event;\t\t\t\t\\\n\tkunmap_atomic((void *)((unsigned long)__event & PAGE_MASK), km); \\\n} while(0)\n\nstatic void ctx_rcu_free(struct rcu_head *head)\n{\n\tstruct kioctx *ctx = container_of(head, struct kioctx, rcu_head);\n\tunsigned nr_events = ctx->max_reqs;\n\n\tkmem_cache_free(kioctx_cachep, ctx);\n\n\tif (nr_events) {\n\t\tspin_lock(&aio_nr_lock);\n\t\tBUG_ON(aio_nr - nr_events > aio_nr);\n\t\taio_nr -= nr_events;\n\t\tspin_unlock(&aio_nr_lock);\n\t}\n}\n\n/* __put_ioctx\n *\tCalled when the last user of an aio context has gone away,\n *\tand the struct needs to be freed.\n */\nstatic void __put_ioctx(struct kioctx *ctx)\n{\n\tBUG_ON(ctx->reqs_active);\n\n\tcancel_delayed_work(&ctx->wq);\n\tcancel_work_sync(&ctx->wq.work);\n\taio_free_ring(ctx);\n\tmmdrop(ctx->mm);\n\tctx->mm = NULL;\n\tpr_debug(\"__put_ioctx: freeing %p\\n\", ctx);\n\tcall_rcu(&ctx->rcu_head, ctx_rcu_free);\n}\n\nstatic inline void get_ioctx(struct kioctx *kioctx)\n{\n\tBUG_ON(atomic_read(&kioctx->users) <= 0);\n\tatomic_inc(&kioctx->users);\n}\n\nstatic inline int try_get_ioctx(struct kioctx *kioctx)\n{\n\treturn atomic_inc_not_zero(&kioctx->users);\n}\n\nstatic inline void put_ioctx(struct kioctx *kioctx)\n{\n\tBUG_ON(atomic_read(&kioctx->users) <= 0);\n\tif (unlikely(atomic_dec_and_test(&kioctx->users)))\n\t\t__put_ioctx(kioctx);\n}\n\n/* ioctx_alloc\n *\tAllocates and initializes an ioctx.  Returns an ERR_PTR if it failed.\n */\nstatic struct kioctx *ioctx_alloc(unsigned nr_events)\n{\n\tstruct mm_struct *mm;\n\tstruct kioctx *ctx;\n\tint did_sync = 0;\n\n\t/* Prevent overflows */\n\tif ((nr_events > (0x10000000U / sizeof(struct io_event))) ||\n\t    (nr_events > (0x10000000U / sizeof(struct kiocb)))) {\n\t\tpr_debug(\"ENOMEM: nr_events too high\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif ((unsigned long)nr_events > aio_max_nr)\n\t\treturn ERR_PTR(-EAGAIN);\n\n\tctx = kmem_cache_zalloc(kioctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tctx->max_reqs = nr_events;\n\tmm = ctx->mm = current->mm;\n\tatomic_inc(&mm->mm_count);\n\n\tatomic_set(&ctx->users, 1);\n\tspin_lock_init(&ctx->ctx_lock);\n\tspin_lock_init(&ctx->ring_info.ring_lock);\n\tinit_waitqueue_head(&ctx->wait);\n\n\tINIT_LIST_HEAD(&ctx->active_reqs);\n\tINIT_LIST_HEAD(&ctx->run_list);\n\tINIT_DELAYED_WORK(&ctx->wq, aio_kick_handler);\n\n\tif (aio_setup_ring(ctx) < 0)\n\t\tgoto out_freectx;\n\n\t/* limit the number of system wide aios */\n\tdo {\n\t\tspin_lock_bh(&aio_nr_lock);\n\t\tif (aio_nr + nr_events > aio_max_nr ||\n\t\t    aio_nr + nr_events < aio_nr)\n\t\t\tctx->max_reqs = 0;\n\t\telse\n\t\t\taio_nr += ctx->max_reqs;\n\t\tspin_unlock_bh(&aio_nr_lock);\n\t\tif (ctx->max_reqs || did_sync)\n\t\t\tbreak;\n\n\t\t/* wait for rcu callbacks to have completed before giving up */\n\t\tsynchronize_rcu();\n\t\tdid_sync = 1;\n\t\tctx->max_reqs = nr_events;\n\t} while (1);\n\n\tif (ctx->max_reqs == 0)\n\t\tgoto out_cleanup;\n\n\t/* now link into global list. */\n\tspin_lock(&mm->ioctx_lock);\n\thlist_add_head_rcu(&ctx->list, &mm->ioctx_list);\n\tspin_unlock(&mm->ioctx_lock);\n\n\tdprintk(\"aio: allocated ioctx %p[%ld]: mm=%p mask=0x%x\\n\",\n\t\tctx, ctx->user_id, current->mm, ctx->ring_info.nr);\n\treturn ctx;\n\nout_cleanup:\n\t__put_ioctx(ctx);\n\treturn ERR_PTR(-EAGAIN);\n\nout_freectx:\n\tmmdrop(mm);\n\tkmem_cache_free(kioctx_cachep, ctx);\n\tctx = ERR_PTR(-ENOMEM);\n\n\tdprintk(\"aio: error allocating ioctx %p\\n\", ctx);\n\treturn ctx;\n}\n\n/* aio_cancel_all\n *\tCancels all outstanding aio requests on an aio context.  Used \n *\twhen the processes owning a context have all exited to encourage \n *\tthe rapid destruction of the kioctx.\n */\nstatic void aio_cancel_all(struct kioctx *ctx)\n{\n\tint (*cancel)(struct kiocb *, struct io_event *);\n\tstruct io_event res;\n\tspin_lock_irq(&ctx->ctx_lock);\n\tctx->dead = 1;\n\twhile (!list_empty(&ctx->active_reqs)) {\n\t\tstruct list_head *pos = ctx->active_reqs.next;\n\t\tstruct kiocb *iocb = list_kiocb(pos);\n\t\tlist_del_init(&iocb->ki_list);\n\t\tcancel = iocb->ki_cancel;\n\t\tkiocbSetCancelled(iocb);\n\t\tif (cancel) {\n\t\t\tiocb->ki_users++;\n\t\t\tspin_unlock_irq(&ctx->ctx_lock);\n\t\t\tcancel(iocb, &res);\n\t\t\tspin_lock_irq(&ctx->ctx_lock);\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->ctx_lock);\n}\n\nstatic void wait_for_all_aios(struct kioctx *ctx)\n{\n\tstruct task_struct *tsk = current;\n\tDECLARE_WAITQUEUE(wait, tsk);\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tif (!ctx->reqs_active)\n\t\tgoto out;\n\n\tadd_wait_queue(&ctx->wait, &wait);\n\tset_task_state(tsk, TASK_UNINTERRUPTIBLE);\n\twhile (ctx->reqs_active) {\n\t\tspin_unlock_irq(&ctx->ctx_lock);\n\t\tio_schedule();\n\t\tset_task_state(tsk, TASK_UNINTERRUPTIBLE);\n\t\tspin_lock_irq(&ctx->ctx_lock);\n\t}\n\t__set_task_state(tsk, TASK_RUNNING);\n\tremove_wait_queue(&ctx->wait, &wait);\n\nout:\n\tspin_unlock_irq(&ctx->ctx_lock);\n}\n\n/* wait_on_sync_kiocb:\n *\tWaits on the given sync kiocb to complete.\n */\nssize_t wait_on_sync_kiocb(struct kiocb *iocb)\n{\n\twhile (iocb->ki_users) {\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tif (!iocb->ki_users)\n\t\t\tbreak;\n\t\tio_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n\treturn iocb->ki_user_data;\n}\nEXPORT_SYMBOL(wait_on_sync_kiocb);\n\n/* exit_aio: called when the last user of mm goes away.  At this point, \n * there is no way for any new requests to be submited or any of the \n * io_* syscalls to be called on the context.  However, there may be \n * outstanding requests which hold references to the context; as they \n * go away, they will call put_ioctx and release any pinned memory\n * associated with the request (held via struct page * references).\n */\nvoid exit_aio(struct mm_struct *mm)\n{\n\tstruct kioctx *ctx;\n\n\twhile (!hlist_empty(&mm->ioctx_list)) {\n\t\tctx = hlist_entry(mm->ioctx_list.first, struct kioctx, list);\n\t\thlist_del_rcu(&ctx->list);\n\n\t\taio_cancel_all(ctx);\n\n\t\twait_for_all_aios(ctx);\n\t\t/*\n\t\t * Ensure we don't leave the ctx on the aio_wq\n\t\t */\n\t\tcancel_work_sync(&ctx->wq.work);\n\n\t\tif (1 != atomic_read(&ctx->users))\n\t\t\tprintk(KERN_DEBUG\n\t\t\t\t\"exit_aio:ioctx still alive: %d %d %d\\n\",\n\t\t\t\tatomic_read(&ctx->users), ctx->dead,\n\t\t\t\tctx->reqs_active);\n\t\tput_ioctx(ctx);\n\t}\n}\n\n/* aio_get_req\n *\tAllocate a slot for an aio request.  Increments the users count\n * of the kioctx so that the kioctx stays around until all requests are\n * complete.  Returns NULL if no requests are free.\n *\n * Returns with kiocb->users set to 2.  The io submit code path holds\n * an extra reference while submitting the i/o.\n * This prevents races between the aio code path referencing the\n * req (after submitting it) and aio_complete() freeing the req.\n */\nstatic struct kiocb *__aio_get_req(struct kioctx *ctx)\n{\n\tstruct kiocb *req = NULL;\n\n\treq = kmem_cache_alloc(kiocb_cachep, GFP_KERNEL);\n\tif (unlikely(!req))\n\t\treturn NULL;\n\n\treq->ki_flags = 0;\n\treq->ki_users = 2;\n\treq->ki_key = 0;\n\treq->ki_ctx = ctx;\n\treq->ki_cancel = NULL;\n\treq->ki_retry = NULL;\n\treq->ki_dtor = NULL;\n\treq->private = NULL;\n\treq->ki_iovec = NULL;\n\tINIT_LIST_HEAD(&req->ki_run_list);\n\treq->ki_eventfd = NULL;\n\n\treturn req;\n}\n\n/*\n * struct kiocb's are allocated in batches to reduce the number of\n * times the ctx lock is acquired and released.\n */\n#define KIOCB_BATCH_SIZE\t32L\nstruct kiocb_batch {\n\tstruct list_head head;\n\tlong count; /* number of requests left to allocate */\n};\n\nstatic void kiocb_batch_init(struct kiocb_batch *batch, long total)\n{\n\tINIT_LIST_HEAD(&batch->head);\n\tbatch->count = total;\n}\n\nstatic void kiocb_batch_free(struct kioctx *ctx, struct kiocb_batch *batch)\n{\n\tstruct kiocb *req, *n;\n\n\tif (list_empty(&batch->head))\n\t\treturn;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tlist_for_each_entry_safe(req, n, &batch->head, ki_batch) {\n\t\tlist_del(&req->ki_batch);\n\t\tlist_del(&req->ki_list);\n\t\tkmem_cache_free(kiocb_cachep, req);\n\t\tctx->reqs_active--;\n\t}\n\tspin_unlock_irq(&ctx->ctx_lock);\n}\n\n/*\n * Allocate a batch of kiocbs.  This avoids taking and dropping the\n * context lock a lot during setup.\n */\nstatic int kiocb_batch_refill(struct kioctx *ctx, struct kiocb_batch *batch)\n{\n\tunsigned short allocated, to_alloc;\n\tlong avail;\n\tbool called_fput = false;\n\tstruct kiocb *req, *n;\n\tstruct aio_ring *ring;\n\n\tto_alloc = min(batch->count, KIOCB_BATCH_SIZE);\n\tfor (allocated = 0; allocated < to_alloc; allocated++) {\n\t\treq = __aio_get_req(ctx);\n\t\tif (!req)\n\t\t\t/* allocation failed, go with what we've got */\n\t\t\tbreak;\n\t\tlist_add(&req->ki_batch, &batch->head);\n\t}\n\n\tif (allocated == 0)\n\t\tgoto out;\n\nretry:\n\tspin_lock_irq(&ctx->ctx_lock);\n\tring = kmap_atomic(ctx->ring_info.ring_pages[0]);\n\n\tavail = aio_ring_avail(&ctx->ring_info, ring) - ctx->reqs_active;\n\tBUG_ON(avail < 0);\n\tif (avail == 0 && !called_fput) {\n\t\t/*\n\t\t * Handle a potential starvation case.  It is possible that\n\t\t * we hold the last reference on a struct file, causing us\n\t\t * to delay the final fput to non-irq context.  In this case,\n\t\t * ctx->reqs_active is artificially high.  Calling the fput\n\t\t * routine here may free up a slot in the event completion\n\t\t * ring, allowing this allocation to succeed.\n\t\t */\n\t\tkunmap_atomic(ring);\n\t\tspin_unlock_irq(&ctx->ctx_lock);\n\t\taio_fput_routine(NULL);\n\t\tcalled_fput = true;\n\t\tgoto retry;\n\t}\n\n\tif (avail < allocated) {\n\t\t/* Trim back the number of requests. */\n\t\tlist_for_each_entry_safe(req, n, &batch->head, ki_batch) {\n\t\t\tlist_del(&req->ki_batch);\n\t\t\tkmem_cache_free(kiocb_cachep, req);\n\t\t\tif (--allocated <= avail)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tbatch->count -= allocated;\n\tlist_for_each_entry(req, &batch->head, ki_batch) {\n\t\tlist_add(&req->ki_list, &ctx->active_reqs);\n\t\tctx->reqs_active++;\n\t}\n\n\tkunmap_atomic(ring);\n\tspin_unlock_irq(&ctx->ctx_lock);\n\nout:\n\treturn allocated;\n}\n\nstatic inline struct kiocb *aio_get_req(struct kioctx *ctx,\n\t\t\t\t\tstruct kiocb_batch *batch)\n{\n\tstruct kiocb *req;\n\n\tif (list_empty(&batch->head))\n\t\tif (kiocb_batch_refill(ctx, batch) == 0)\n\t\t\treturn NULL;\n\treq = list_first_entry(&batch->head, struct kiocb, ki_batch);\n\tlist_del(&req->ki_batch);\n\treturn req;\n}\n\nstatic inline void really_put_req(struct kioctx *ctx, struct kiocb *req)\n{\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\tif (req->ki_eventfd != NULL)\n\t\teventfd_ctx_put(req->ki_eventfd);\n\tif (req->ki_dtor)\n\t\treq->ki_dtor(req);\n\tif (req->ki_iovec != &req->ki_inline_vec)\n\t\tkfree(req->ki_iovec);\n\tkmem_cache_free(kiocb_cachep, req);\n\tctx->reqs_active--;\n\n\tif (unlikely(!ctx->reqs_active && ctx->dead))\n\t\twake_up_all(&ctx->wait);\n}\n\nstatic void aio_fput_routine(struct work_struct *data)\n{\n\tspin_lock_irq(&fput_lock);\n\twhile (likely(!list_empty(&fput_head))) {\n\t\tstruct kiocb *req = list_kiocb(fput_head.next);\n\t\tstruct kioctx *ctx = req->ki_ctx;\n\n\t\tlist_del(&req->ki_list);\n\t\tspin_unlock_irq(&fput_lock);\n\n\t\t/* Complete the fput(s) */\n\t\tif (req->ki_filp != NULL)\n\t\t\tfput(req->ki_filp);\n\n\t\t/* Link the iocb into the context's free list */\n\t\tspin_lock_irq(&ctx->ctx_lock);\n\t\treally_put_req(ctx, req);\n\t\tspin_unlock_irq(&ctx->ctx_lock);\n\n\t\tput_ioctx(ctx);\n\t\tspin_lock_irq(&fput_lock);\n\t}\n\tspin_unlock_irq(&fput_lock);\n}\n\n/* __aio_put_req\n *\tReturns true if this put was the last user of the request.\n */\nstatic int __aio_put_req(struct kioctx *ctx, struct kiocb *req)\n{\n\tdprintk(KERN_DEBUG \"aio_put(%p): f_count=%ld\\n\",\n\t\treq, atomic_long_read(&req->ki_filp->f_count));\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\treq->ki_users--;\n\tBUG_ON(req->ki_users < 0);\n\tif (likely(req->ki_users))\n\t\treturn 0;\n\tlist_del(&req->ki_list);\t\t/* remove from active_reqs */\n\treq->ki_cancel = NULL;\n\treq->ki_retry = NULL;\n\n\t/*\n\t * Try to optimize the aio and eventfd file* puts, by avoiding to\n\t * schedule work in case it is not final fput() time. In normal cases,\n\t * we would not be holding the last reference to the file*, so\n\t * this function will be executed w/out any aio kthread wakeup.\n\t */\n\tif (unlikely(!fput_atomic(req->ki_filp))) {\n\t\tget_ioctx(ctx);\n\t\tspin_lock(&fput_lock);\n\t\tlist_add(&req->ki_list, &fput_head);\n\t\tspin_unlock(&fput_lock);\n\t\tschedule_work(&fput_work);\n\t} else {\n\t\treq->ki_filp = NULL;\n\t\treally_put_req(ctx, req);\n\t}\n\treturn 1;\n}\n\n/* aio_put_req\n *\tReturns true if this put was the last user of the kiocb,\n *\tfalse if the request is still in use.\n */\nint aio_put_req(struct kiocb *req)\n{\n\tstruct kioctx *ctx = req->ki_ctx;\n\tint ret;\n\tspin_lock_irq(&ctx->ctx_lock);\n\tret = __aio_put_req(ctx, req);\n\tspin_unlock_irq(&ctx->ctx_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL(aio_put_req);\n\nstatic struct kioctx *lookup_ioctx(unsigned long ctx_id)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct kioctx *ctx, *ret = NULL;\n\tstruct hlist_node *n;\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(ctx, n, &mm->ioctx_list, list) {\n\t\t/*\n\t\t * RCU protects us against accessing freed memory but\n\t\t * we have to be careful not to get a reference when the\n\t\t * reference count already dropped to 0 (ctx->dead test\n\t\t * is unreliable because of races).\n\t\t */\n\t\tif (ctx->user_id == ctx_id && !ctx->dead && try_get_ioctx(ctx)){\n\t\t\tret = ctx;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/*\n * Queue up a kiocb to be retried. Assumes that the kiocb\n * has already been marked as kicked, and places it on\n * the retry run list for the corresponding ioctx, if it\n * isn't already queued. Returns 1 if it actually queued\n * the kiocb (to tell the caller to activate the work\n * queue to process it), or 0, if it found that it was\n * already queued.\n */\nstatic inline int __queue_kicked_iocb(struct kiocb *iocb)\n{\n\tstruct kioctx *ctx = iocb->ki_ctx;\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\tif (list_empty(&iocb->ki_run_list)) {\n\t\tlist_add_tail(&iocb->ki_run_list,\n\t\t\t&ctx->run_list);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/* aio_run_iocb\n *\tThis is the core aio execution routine. It is\n *\tinvoked both for initial i/o submission and\n *\tsubsequent retries via the aio_kick_handler.\n *\tExpects to be invoked with iocb->ki_ctx->lock\n *\talready held. The lock is released and reacquired\n *\tas needed during processing.\n *\n * Calls the iocb retry method (already setup for the\n * iocb on initial submission) for operation specific\n * handling, but takes care of most of common retry\n * execution details for a given iocb. The retry method\n * needs to be non-blocking as far as possible, to avoid\n * holding up other iocbs waiting to be serviced by the\n * retry kernel thread.\n *\n * The trickier parts in this code have to do with\n * ensuring that only one retry instance is in progress\n * for a given iocb at any time. Providing that guarantee\n * simplifies the coding of individual aio operations as\n * it avoids various potential races.\n */\nstatic ssize_t aio_run_iocb(struct kiocb *iocb)\n{\n\tstruct kioctx\t*ctx = iocb->ki_ctx;\n\tssize_t (*retry)(struct kiocb *);\n\tssize_t ret;\n\n\tif (!(retry = iocb->ki_retry)) {\n\t\tprintk(\"aio_run_iocb: iocb->ki_retry = NULL\\n\");\n\t\treturn 0;\n\t}\n\n\t/*\n\t * We don't want the next retry iteration for this\n\t * operation to start until this one has returned and\n\t * updated the iocb state. However, wait_queue functions\n\t * can trigger a kick_iocb from interrupt context in the\n\t * meantime, indicating that data is available for the next\n\t * iteration. We want to remember that and enable the\n\t * next retry iteration _after_ we are through with\n\t * this one.\n\t *\n\t * So, in order to be able to register a \"kick\", but\n\t * prevent it from being queued now, we clear the kick\n\t * flag, but make the kick code *think* that the iocb is\n\t * still on the run list until we are actually done.\n\t * When we are done with this iteration, we check if\n\t * the iocb was kicked in the meantime and if so, queue\n\t * it up afresh.\n\t */\n\n\tkiocbClearKicked(iocb);\n\n\t/*\n\t * This is so that aio_complete knows it doesn't need to\n\t * pull the iocb off the run list (We can't just call\n\t * INIT_LIST_HEAD because we don't want a kick_iocb to\n\t * queue this on the run list yet)\n\t */\n\tiocb->ki_run_list.next = iocb->ki_run_list.prev = NULL;\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\t/* Quit retrying if the i/o has been cancelled */\n\tif (kiocbIsCancelled(iocb)) {\n\t\tret = -EINTR;\n\t\taio_complete(iocb, ret, 0);\n\t\t/* must not access the iocb after this */\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Now we are all set to call the retry method in async\n\t * context.\n\t */\n\tret = retry(iocb);\n\n\tif (ret != -EIOCBRETRY && ret != -EIOCBQUEUED) {\n\t\t/*\n\t\t * There's no easy way to restart the syscall since other AIO's\n\t\t * may be already running. Just fail this IO with EINTR.\n\t\t */\n\t\tif (unlikely(ret == -ERESTARTSYS || ret == -ERESTARTNOINTR ||\n\t\t\t     ret == -ERESTARTNOHAND || ret == -ERESTART_RESTARTBLOCK))\n\t\t\tret = -EINTR;\n\t\taio_complete(iocb, ret, 0);\n\t}\nout:\n\tspin_lock_irq(&ctx->ctx_lock);\n\n\tif (-EIOCBRETRY == ret) {\n\t\t/*\n\t\t * OK, now that we are done with this iteration\n\t\t * and know that there is more left to go,\n\t\t * this is where we let go so that a subsequent\n\t\t * \"kick\" can start the next iteration\n\t\t */\n\n\t\t/* will make __queue_kicked_iocb succeed from here on */\n\t\tINIT_LIST_HEAD(&iocb->ki_run_list);\n\t\t/* we must queue the next iteration ourselves, if it\n\t\t * has already been kicked */\n\t\tif (kiocbIsKicked(iocb)) {\n\t\t\t__queue_kicked_iocb(iocb);\n\n\t\t\t/*\n\t\t\t * __queue_kicked_iocb will always return 1 here, because\n\t\t\t * iocb->ki_run_list is empty at this point so it should\n\t\t\t * be safe to unconditionally queue the context into the\n\t\t\t * work queue.\n\t\t\t */\n\t\t\taio_queue_work(ctx);\n\t\t}\n\t}\n\treturn ret;\n}\n\n/*\n * __aio_run_iocbs:\n * \tProcess all pending retries queued on the ioctx\n * \trun list.\n * Assumes it is operating within the aio issuer's mm\n * context.\n */\nstatic int __aio_run_iocbs(struct kioctx *ctx)\n{\n\tstruct kiocb *iocb;\n\tstruct list_head run_list;\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\tlist_replace_init(&ctx->run_list, &run_list);\n\twhile (!list_empty(&run_list)) {\n\t\tiocb = list_entry(run_list.next, struct kiocb,\n\t\t\tki_run_list);\n\t\tlist_del(&iocb->ki_run_list);\n\t\t/*\n\t\t * Hold an extra reference while retrying i/o.\n\t\t */\n\t\tiocb->ki_users++;       /* grab extra reference */\n\t\taio_run_iocb(iocb);\n\t\t__aio_put_req(ctx, iocb);\n \t}\n\tif (!list_empty(&ctx->run_list))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void aio_queue_work(struct kioctx * ctx)\n{\n\tunsigned long timeout;\n\t/*\n\t * if someone is waiting, get the work started right\n\t * away, otherwise, use a longer delay\n\t */\n\tsmp_mb();\n\tif (waitqueue_active(&ctx->wait))\n\t\ttimeout = 1;\n\telse\n\t\ttimeout = HZ/10;\n\tqueue_delayed_work(aio_wq, &ctx->wq, timeout);\n}\n\n/*\n * aio_run_all_iocbs:\n *\tProcess all pending retries queued on the ioctx\n *\trun list, and keep running them until the list\n *\tstays empty.\n * Assumes it is operating within the aio issuer's mm context.\n */\nstatic inline void aio_run_all_iocbs(struct kioctx *ctx)\n{\n\tspin_lock_irq(&ctx->ctx_lock);\n\twhile (__aio_run_iocbs(ctx))\n\t\t;\n\tspin_unlock_irq(&ctx->ctx_lock);\n}\n\n/*\n * aio_kick_handler:\n * \tWork queue handler triggered to process pending\n * \tretries on an ioctx. Takes on the aio issuer's\n *\tmm context before running the iocbs, so that\n *\tcopy_xxx_user operates on the issuer's address\n *      space.\n * Run on aiod's context.\n */\nstatic void aio_kick_handler(struct work_struct *work)\n{\n\tstruct kioctx *ctx = container_of(work, struct kioctx, wq.work);\n\tmm_segment_t oldfs = get_fs();\n\tstruct mm_struct *mm;\n\tint requeue;\n\n\tset_fs(USER_DS);\n\tuse_mm(ctx->mm);\n\tspin_lock_irq(&ctx->ctx_lock);\n\trequeue =__aio_run_iocbs(ctx);\n\tmm = ctx->mm;\n\tspin_unlock_irq(&ctx->ctx_lock);\n \tunuse_mm(mm);\n\tset_fs(oldfs);\n\t/*\n\t * we're in a worker thread already, don't use queue_delayed_work,\n\t */\n\tif (requeue)\n\t\tqueue_delayed_work(aio_wq, &ctx->wq, 0);\n}\n\n\n/*\n * Called by kick_iocb to queue the kiocb for retry\n * and if required activate the aio work queue to process\n * it\n */\nstatic void try_queue_kicked_iocb(struct kiocb *iocb)\n{\n \tstruct kioctx\t*ctx = iocb->ki_ctx;\n\tunsigned long flags;\n\tint run = 0;\n\n\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\t/* set this inside the lock so that we can't race with aio_run_iocb()\n\t * testing it and putting the iocb on the run list under the lock */\n\tif (!kiocbTryKick(iocb))\n\t\trun = __queue_kicked_iocb(iocb);\n\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n\tif (run)\n\t\taio_queue_work(ctx);\n}\n\n/*\n * kick_iocb:\n *      Called typically from a wait queue callback context\n *      to trigger a retry of the iocb.\n *      The retry is usually executed by aio workqueue\n *      threads (See aio_kick_handler).\n */\nvoid kick_iocb(struct kiocb *iocb)\n{\n\t/* sync iocbs are easy: they can only ever be executing from a \n\t * single context. */\n\tif (is_sync_kiocb(iocb)) {\n\t\tkiocbSetKicked(iocb);\n\t        wake_up_process(iocb->ki_obj.tsk);\n\t\treturn;\n\t}\n\n\ttry_queue_kicked_iocb(iocb);\n}\nEXPORT_SYMBOL(kick_iocb);\n\n/* aio_complete\n *\tCalled when the io request on the given iocb is complete.\n *\tReturns true if this is the last user of the request.  The \n *\tonly other user of the request can be the cancellation code.\n */\nint aio_complete(struct kiocb *iocb, long res, long res2)\n{\n\tstruct kioctx\t*ctx = iocb->ki_ctx;\n\tstruct aio_ring_info\t*info;\n\tstruct aio_ring\t*ring;\n\tstruct io_event\t*event;\n\tunsigned long\tflags;\n\tunsigned long\ttail;\n\tint\t\tret;\n\n\t/*\n\t * Special case handling for sync iocbs:\n\t *  - events go directly into the iocb for fast handling\n\t *  - the sync task with the iocb in its stack holds the single iocb\n\t *    ref, no other paths have a way to get another ref\n\t *  - the sync task helpfully left a reference to itself in the iocb\n\t */\n\tif (is_sync_kiocb(iocb)) {\n\t\tBUG_ON(iocb->ki_users != 1);\n\t\tiocb->ki_user_data = res;\n\t\tiocb->ki_users = 0;\n\t\twake_up_process(iocb->ki_obj.tsk);\n\t\treturn 1;\n\t}\n\n\tinfo = &ctx->ring_info;\n\n\t/* add a completion event to the ring buffer.\n\t * must be done holding ctx->ctx_lock to prevent\n\t * other code from messing with the tail\n\t * pointer since we might be called from irq\n\t * context.\n\t */\n\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\n\tif (iocb->ki_run_list.prev && !list_empty(&iocb->ki_run_list))\n\t\tlist_del_init(&iocb->ki_run_list);\n\n\t/*\n\t * cancelled requests don't get events, userland was given one\n\t * when the event got cancelled.\n\t */\n\tif (kiocbIsCancelled(iocb))\n\t\tgoto put_rq;\n\n\tring = kmap_atomic(info->ring_pages[0], KM_IRQ1);\n\n\ttail = info->tail;\n\tevent = aio_ring_event(info, tail, KM_IRQ0);\n\tif (++tail >= info->nr)\n\t\ttail = 0;\n\n\tevent->obj = (u64)(unsigned long)iocb->ki_obj.user;\n\tevent->data = iocb->ki_user_data;\n\tevent->res = res;\n\tevent->res2 = res2;\n\n\tdprintk(\"aio_complete: %p[%lu]: %p: %p %Lx %lx %lx\\n\",\n\t\tctx, tail, iocb, iocb->ki_obj.user, iocb->ki_user_data,\n\t\tres, res2);\n\n\t/* after flagging the request as done, we\n\t * must never even look at it again\n\t */\n\tsmp_wmb();\t/* make event visible before updating tail */\n\n\tinfo->tail = tail;\n\tring->tail = tail;\n\n\tput_aio_ring_event(event, KM_IRQ0);\n\tkunmap_atomic(ring, KM_IRQ1);\n\n\tpr_debug(\"added to ring %p at [%lu]\\n\", iocb, tail);\n\n\t/*\n\t * Check if the user asked us to deliver the result through an\n\t * eventfd. The eventfd_signal() function is safe to be called\n\t * from IRQ context.\n\t */\n\tif (iocb->ki_eventfd != NULL)\n\t\teventfd_signal(iocb->ki_eventfd, 1);\n\nput_rq:\n\t/* everything turned out well, dispose of the aiocb. */\n\tret = __aio_put_req(ctx, iocb);\n\n\t/*\n\t * We have to order our ring_info tail store above and test\n\t * of the wait list below outside the wait lock.  This is\n\t * like in wake_up_bit() where clearing a bit has to be\n\t * ordered with the unlocked test.\n\t */\n\tsmp_mb();\n\n\tif (waitqueue_active(&ctx->wait))\n\t\twake_up(&ctx->wait);\n\n\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(aio_complete);\n\n/* aio_read_evt\n *\tPull an event off of the ioctx's event ring.  Returns the number of \n *\tevents fetched (0 or 1 ;-)\n *\tFIXME: make this use cmpxchg.\n *\tTODO: make the ringbuffer user mmap()able (requires FIXME).\n */\nstatic int aio_read_evt(struct kioctx *ioctx, struct io_event *ent)\n{\n\tstruct aio_ring_info *info = &ioctx->ring_info;\n\tstruct aio_ring *ring;\n\tunsigned long head;\n\tint ret = 0;\n\n\tring = kmap_atomic(info->ring_pages[0], KM_USER0);\n\tdprintk(\"in aio_read_evt h%lu t%lu m%lu\\n\",\n\t\t (unsigned long)ring->head, (unsigned long)ring->tail,\n\t\t (unsigned long)ring->nr);\n\n\tif (ring->head == ring->tail)\n\t\tgoto out;\n\n\tspin_lock(&info->ring_lock);\n\n\thead = ring->head % info->nr;\n\tif (head != ring->tail) {\n\t\tstruct io_event *evp = aio_ring_event(info, head, KM_USER1);\n\t\t*ent = *evp;\n\t\thead = (head + 1) % info->nr;\n\t\tsmp_mb(); /* finish reading the event before updatng the head */\n\t\tring->head = head;\n\t\tret = 1;\n\t\tput_aio_ring_event(evp, KM_USER1);\n\t}\n\tspin_unlock(&info->ring_lock);\n\nout:\n\tkunmap_atomic(ring, KM_USER0);\n\tdprintk(\"leaving aio_read_evt: %d  h%lu t%lu\\n\", ret,\n\t\t (unsigned long)ring->head, (unsigned long)ring->tail);\n\treturn ret;\n}\n\nstruct aio_timeout {\n\tstruct timer_list\ttimer;\n\tint\t\t\ttimed_out;\n\tstruct task_struct\t*p;\n};\n\nstatic void timeout_func(unsigned long data)\n{\n\tstruct aio_timeout *to = (struct aio_timeout *)data;\n\n\tto->timed_out = 1;\n\twake_up_process(to->p);\n}\n\nstatic inline void init_timeout(struct aio_timeout *to)\n{\n\tsetup_timer_on_stack(&to->timer, timeout_func, (unsigned long) to);\n\tto->timed_out = 0;\n\tto->p = current;\n}\n\nstatic inline void set_timeout(long start_jiffies, struct aio_timeout *to,\n\t\t\t       const struct timespec *ts)\n{\n\tto->timer.expires = start_jiffies + timespec_to_jiffies(ts);\n\tif (time_after(to->timer.expires, jiffies))\n\t\tadd_timer(&to->timer);\n\telse\n\t\tto->timed_out = 1;\n}\n\nstatic inline void clear_timeout(struct aio_timeout *to)\n{\n\tdel_singleshot_timer_sync(&to->timer);\n}\n\nstatic int read_events(struct kioctx *ctx,\n\t\t\tlong min_nr, long nr,\n\t\t\tstruct io_event __user *event,\n\t\t\tstruct timespec __user *timeout)\n{\n\tlong\t\t\tstart_jiffies = jiffies;\n\tstruct task_struct\t*tsk = current;\n\tDECLARE_WAITQUEUE(wait, tsk);\n\tint\t\t\tret;\n\tint\t\t\ti = 0;\n\tstruct io_event\t\tent;\n\tstruct aio_timeout\tto;\n\tint\t\t\tretry = 0;\n\n\t/* needed to zero any padding within an entry (there shouldn't be \n\t * any, but C is fun!\n\t */\n\tmemset(&ent, 0, sizeof(ent));\nretry:\n\tret = 0;\n\twhile (likely(i < nr)) {\n\t\tret = aio_read_evt(ctx, &ent);\n\t\tif (unlikely(ret <= 0))\n\t\t\tbreak;\n\n\t\tdprintk(\"read event: %Lx %Lx %Lx %Lx\\n\",\n\t\t\tent.data, ent.obj, ent.res, ent.res2);\n\n\t\t/* Could we split the check in two? */\n\t\tret = -EFAULT;\n\t\tif (unlikely(copy_to_user(event, &ent, sizeof(ent)))) {\n\t\t\tdprintk(\"aio: lost an event due to EFAULT.\\n\");\n\t\t\tbreak;\n\t\t}\n\t\tret = 0;\n\n\t\t/* Good, event copied to userland, update counts. */\n\t\tevent ++;\n\t\ti ++;\n\t}\n\n\tif (min_nr <= i)\n\t\treturn i;\n\tif (ret)\n\t\treturn ret;\n\n\t/* End fast path */\n\n\t/* racey check, but it gets redone */\n\tif (!retry && unlikely(!list_empty(&ctx->run_list))) {\n\t\tretry = 1;\n\t\taio_run_all_iocbs(ctx);\n\t\tgoto retry;\n\t}\n\n\tinit_timeout(&to);\n\tif (timeout) {\n\t\tstruct timespec\tts;\n\t\tret = -EFAULT;\n\t\tif (unlikely(copy_from_user(&ts, timeout, sizeof(ts))))\n\t\t\tgoto out;\n\n\t\tset_timeout(start_jiffies, &to, &ts);\n\t}\n\n\twhile (likely(i < nr)) {\n\t\tadd_wait_queue_exclusive(&ctx->wait, &wait);\n\t\tdo {\n\t\t\tset_task_state(tsk, TASK_INTERRUPTIBLE);\n\t\t\tret = aio_read_evt(ctx, &ent);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tif (min_nr <= i)\n\t\t\t\tbreak;\n\t\t\tif (unlikely(ctx->dead)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (to.timed_out)\t/* Only check after read evt */\n\t\t\t\tbreak;\n\t\t\t/* Try to only show up in io wait if there are ops\n\t\t\t *  in flight */\n\t\t\tif (ctx->reqs_active)\n\t\t\t\tio_schedule();\n\t\t\telse\n\t\t\t\tschedule();\n\t\t\tif (signal_pending(tsk)) {\n\t\t\t\tret = -EINTR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/*ret = aio_read_evt(ctx, &ent);*/\n\t\t} while (1) ;\n\n\t\tset_task_state(tsk, TASK_RUNNING);\n\t\tremove_wait_queue(&ctx->wait, &wait);\n\n\t\tif (unlikely(ret <= 0))\n\t\t\tbreak;\n\n\t\tret = -EFAULT;\n\t\tif (unlikely(copy_to_user(event, &ent, sizeof(ent)))) {\n\t\t\tdprintk(\"aio: lost an event due to EFAULT.\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Good, event copied to userland, update counts. */\n\t\tevent ++;\n\t\ti ++;\n\t}\n\n\tif (timeout)\n\t\tclear_timeout(&to);\nout:\n\tdestroy_timer_on_stack(&to.timer);\n\treturn i ? i : ret;\n}\n\n/* Take an ioctx and remove it from the list of ioctx's.  Protects \n * against races with itself via ->dead.\n */\nstatic void io_destroy(struct kioctx *ioctx)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint was_dead;\n\n\t/* delete the entry from the list is someone else hasn't already */\n\tspin_lock(&mm->ioctx_lock);\n\twas_dead = ioctx->dead;\n\tioctx->dead = 1;\n\thlist_del_rcu(&ioctx->list);\n\tspin_unlock(&mm->ioctx_lock);\n\n\tdprintk(\"aio_release(%p)\\n\", ioctx);\n\tif (likely(!was_dead))\n\t\tput_ioctx(ioctx);\t/* twice for the list */\n\n\taio_cancel_all(ioctx);\n\twait_for_all_aios(ioctx);\n\n\t/*\n\t * Wake up any waiters.  The setting of ctx->dead must be seen\n\t * by other CPUs at this point.  Right now, we rely on the\n\t * locking done by the above calls to ensure this consistency.\n\t */\n\twake_up_all(&ioctx->wait);\n\tput_ioctx(ioctx);\t/* once for the lookup */\n}\n\n/* sys_io_setup:\n *\tCreate an aio_context capable of receiving at least nr_events.\n *\tctxp must not point to an aio_context that already exists, and\n *\tmust be initialized to 0 prior to the call.  On successful\n *\tcreation of the aio_context, *ctxp is filled in with the resulting \n *\thandle.  May fail with -EINVAL if *ctxp is not initialized,\n *\tif the specified nr_events exceeds internal limits.  May fail \n *\twith -EAGAIN if the specified nr_events exceeds the user's limit \n *\tof available events.  May fail with -ENOMEM if insufficient kernel\n *\tresources are available.  May fail with -EFAULT if an invalid\n *\tpointer is passed for ctxp.  Will fail with -ENOSYS if not\n *\timplemented.\n */\nSYSCALL_DEFINE2(io_setup, unsigned, nr_events, aio_context_t __user *, ctxp)\n{\n\tstruct kioctx *ioctx = NULL;\n\tunsigned long ctx;\n\tlong ret;\n\n\tret = get_user(ctx, ctxp);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (unlikely(ctx || nr_events == 0)) {\n\t\tpr_debug(\"EINVAL: io_setup: ctx %lu nr_events %u\\n\",\n\t\t         ctx, nr_events);\n\t\tgoto out;\n\t}\n\n\tioctx = ioctx_alloc(nr_events);\n\tret = PTR_ERR(ioctx);\n\tif (!IS_ERR(ioctx)) {\n\t\tret = put_user(ioctx->user_id, ctxp);\n\t\tif (!ret)\n\t\t\treturn 0;\n\n\t\tget_ioctx(ioctx); /* io_destroy() expects us to hold a ref */\n\t\tio_destroy(ioctx);\n\t}\n\nout:\n\treturn ret;\n}\n\n/* sys_io_destroy:\n *\tDestroy the aio_context specified.  May cancel any outstanding \n *\tAIOs and block on completion.  Will fail with -ENOSYS if not\n *\timplemented.  May fail with -EINVAL if the context pointed to\n *\tis invalid.\n */\nSYSCALL_DEFINE1(io_destroy, aio_context_t, ctx)\n{\n\tstruct kioctx *ioctx = lookup_ioctx(ctx);\n\tif (likely(NULL != ioctx)) {\n\t\tio_destroy(ioctx);\n\t\treturn 0;\n\t}\n\tpr_debug(\"EINVAL: io_destroy: invalid context id\\n\");\n\treturn -EINVAL;\n}\n\nstatic void aio_advance_iovec(struct kiocb *iocb, ssize_t ret)\n{\n\tstruct iovec *iov = &iocb->ki_iovec[iocb->ki_cur_seg];\n\n\tBUG_ON(ret <= 0);\n\n\twhile (iocb->ki_cur_seg < iocb->ki_nr_segs && ret > 0) {\n\t\tssize_t this = min((ssize_t)iov->iov_len, ret);\n\t\tiov->iov_base += this;\n\t\tiov->iov_len -= this;\n\t\tiocb->ki_left -= this;\n\t\tret -= this;\n\t\tif (iov->iov_len == 0) {\n\t\t\tiocb->ki_cur_seg++;\n\t\t\tiov++;\n\t\t}\n\t}\n\n\t/* the caller should not have done more io than what fit in\n\t * the remaining iovecs */\n\tBUG_ON(ret > 0 && iocb->ki_left == 0);\n}\n\nstatic ssize_t aio_rw_vect_retry(struct kiocb *iocb)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tssize_t (*rw_op)(struct kiocb *, const struct iovec *,\n\t\t\t unsigned long, loff_t);\n\tssize_t ret = 0;\n\tunsigned short opcode;\n\n\tif ((iocb->ki_opcode == IOCB_CMD_PREADV) ||\n\t\t(iocb->ki_opcode == IOCB_CMD_PREAD)) {\n\t\trw_op = file->f_op->aio_read;\n\t\topcode = IOCB_CMD_PREADV;\n\t} else {\n\t\trw_op = file->f_op->aio_write;\n\t\topcode = IOCB_CMD_PWRITEV;\n\t}\n\n\t/* This matches the pread()/pwrite() logic */\n\tif (iocb->ki_pos < 0)\n\t\treturn -EINVAL;\n\n\tdo {\n\t\tret = rw_op(iocb, &iocb->ki_iovec[iocb->ki_cur_seg],\n\t\t\t    iocb->ki_nr_segs - iocb->ki_cur_seg,\n\t\t\t    iocb->ki_pos);\n\t\tif (ret > 0)\n\t\t\taio_advance_iovec(iocb, ret);\n\n\t/* retry all partial writes.  retry partial reads as long as its a\n\t * regular file. */\n\t} while (ret > 0 && iocb->ki_left > 0 &&\n\t\t (opcode == IOCB_CMD_PWRITEV ||\n\t\t  (!S_ISFIFO(inode->i_mode) && !S_ISSOCK(inode->i_mode))));\n\n\t/* This means we must have transferred all that we could */\n\t/* No need to retry anymore */\n\tif ((ret == 0) || (iocb->ki_left == 0))\n\t\tret = iocb->ki_nbytes - iocb->ki_left;\n\n\t/* If we managed to write some out we return that, rather than\n\t * the eventual error. */\n\tif (opcode == IOCB_CMD_PWRITEV\n\t    && ret < 0 && ret != -EIOCBQUEUED && ret != -EIOCBRETRY\n\t    && iocb->ki_nbytes - iocb->ki_left)\n\t\tret = iocb->ki_nbytes - iocb->ki_left;\n\n\treturn ret;\n}\n\nstatic ssize_t aio_fdsync(struct kiocb *iocb)\n{\n\tstruct file *file = iocb->ki_filp;\n\tssize_t ret = -EINVAL;\n\n\tif (file->f_op->aio_fsync)\n\t\tret = file->f_op->aio_fsync(iocb, 1);\n\treturn ret;\n}\n\nstatic ssize_t aio_fsync(struct kiocb *iocb)\n{\n\tstruct file *file = iocb->ki_filp;\n\tssize_t ret = -EINVAL;\n\n\tif (file->f_op->aio_fsync)\n\t\tret = file->f_op->aio_fsync(iocb, 0);\n\treturn ret;\n}\n\nstatic ssize_t aio_setup_vectored_rw(int type, struct kiocb *kiocb, bool compat)\n{\n\tssize_t ret;\n\n#ifdef CONFIG_COMPAT\n\tif (compat)\n\t\tret = compat_rw_copy_check_uvector(type,\n\t\t\t\t(struct compat_iovec __user *)kiocb->ki_buf,\n\t\t\t\tkiocb->ki_nbytes, 1, &kiocb->ki_inline_vec,\n\t\t\t\t&kiocb->ki_iovec, 1);\n\telse\n#endif\n\t\tret = rw_copy_check_uvector(type,\n\t\t\t\t(struct iovec __user *)kiocb->ki_buf,\n\t\t\t\tkiocb->ki_nbytes, 1, &kiocb->ki_inline_vec,\n\t\t\t\t&kiocb->ki_iovec, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tkiocb->ki_nr_segs = kiocb->ki_nbytes;\n\tkiocb->ki_cur_seg = 0;\n\t/* ki_nbytes/left now reflect bytes instead of segs */\n\tkiocb->ki_nbytes = ret;\n\tkiocb->ki_left = ret;\n\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic ssize_t aio_setup_single_vector(struct kiocb *kiocb)\n{\n\tkiocb->ki_iovec = &kiocb->ki_inline_vec;\n\tkiocb->ki_iovec->iov_base = kiocb->ki_buf;\n\tkiocb->ki_iovec->iov_len = kiocb->ki_left;\n\tkiocb->ki_nr_segs = 1;\n\tkiocb->ki_cur_seg = 0;\n\treturn 0;\n}\n\n/*\n * aio_setup_iocb:\n *\tPerforms the initial checks and aio retry method\n *\tsetup for the kiocb at the time of io submission.\n */\nstatic ssize_t aio_setup_iocb(struct kiocb *kiocb, bool compat)\n{\n\tstruct file *file = kiocb->ki_filp;\n\tssize_t ret = 0;\n\n\tswitch (kiocb->ki_opcode) {\n\tcase IOCB_CMD_PREAD:\n\t\tret = -EBADF;\n\t\tif (unlikely(!(file->f_mode & FMODE_READ)))\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (unlikely(!access_ok(VERIFY_WRITE, kiocb->ki_buf,\n\t\t\tkiocb->ki_left)))\n\t\t\tbreak;\n\t\tret = security_file_permission(file, MAY_READ);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t\tret = aio_setup_single_vector(kiocb);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_read)\n\t\t\tkiocb->ki_retry = aio_rw_vect_retry;\n\t\tbreak;\n\tcase IOCB_CMD_PWRITE:\n\t\tret = -EBADF;\n\t\tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (unlikely(!access_ok(VERIFY_READ, kiocb->ki_buf,\n\t\t\tkiocb->ki_left)))\n\t\t\tbreak;\n\t\tret = security_file_permission(file, MAY_WRITE);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t\tret = aio_setup_single_vector(kiocb);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_write)\n\t\t\tkiocb->ki_retry = aio_rw_vect_retry;\n\t\tbreak;\n\tcase IOCB_CMD_PREADV:\n\t\tret = -EBADF;\n\t\tif (unlikely(!(file->f_mode & FMODE_READ)))\n\t\t\tbreak;\n\t\tret = security_file_permission(file, MAY_READ);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t\tret = aio_setup_vectored_rw(READ, kiocb, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_read)\n\t\t\tkiocb->ki_retry = aio_rw_vect_retry;\n\t\tbreak;\n\tcase IOCB_CMD_PWRITEV:\n\t\tret = -EBADF;\n\t\tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n\t\t\tbreak;\n\t\tret = security_file_permission(file, MAY_WRITE);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t\tret = aio_setup_vectored_rw(WRITE, kiocb, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_write)\n\t\t\tkiocb->ki_retry = aio_rw_vect_retry;\n\t\tbreak;\n\tcase IOCB_CMD_FDSYNC:\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_fsync)\n\t\t\tkiocb->ki_retry = aio_fdsync;\n\t\tbreak;\n\tcase IOCB_CMD_FSYNC:\n\t\tret = -EINVAL;\n\t\tif (file->f_op->aio_fsync)\n\t\t\tkiocb->ki_retry = aio_fsync;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"EINVAL: io_submit: no operation provided\\n\");\n\t\tret = -EINVAL;\n\t}\n\n\tif (!kiocb->ki_retry)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic int io_submit_one(struct kioctx *ctx, struct iocb __user *user_iocb,\n\t\t\t struct iocb *iocb, struct kiocb_batch *batch,\n\t\t\t bool compat)\n{\n\tstruct kiocb *req;\n\tstruct file *file;\n\tssize_t ret;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(iocb->aio_reserved1 || iocb->aio_reserved2)) {\n\t\tpr_debug(\"EINVAL: io_submit: reserve field set\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* prevent overflows */\n\tif (unlikely(\n\t    (iocb->aio_buf != (unsigned long)iocb->aio_buf) ||\n\t    (iocb->aio_nbytes != (size_t)iocb->aio_nbytes) ||\n\t    ((ssize_t)iocb->aio_nbytes < 0)\n\t   )) {\n\t\tpr_debug(\"EINVAL: io_submit: overflow check\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tfile = fget(iocb->aio_fildes);\n\tif (unlikely(!file))\n\t\treturn -EBADF;\n\n\treq = aio_get_req(ctx, batch);  /* returns with 2 references to req */\n\tif (unlikely(!req)) {\n\t\tfput(file);\n\t\treturn -EAGAIN;\n\t}\n\treq->ki_filp = file;\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD) {\n\t\t/*\n\t\t * If the IOCB_FLAG_RESFD flag of aio_flags is set, get an\n\t\t * instance of the file* now. The file descriptor must be\n\t\t * an eventfd() fd, and will be signaled for each completed\n\t\t * event using the eventfd_signal() function.\n\t\t */\n\t\treq->ki_eventfd = eventfd_ctx_fdget((int) iocb->aio_resfd);\n\t\tif (IS_ERR(req->ki_eventfd)) {\n\t\t\tret = PTR_ERR(req->ki_eventfd);\n\t\t\treq->ki_eventfd = NULL;\n\t\t\tgoto out_put_req;\n\t\t}\n\t}\n\n\tret = put_user(req->ki_key, &user_iocb->aio_key);\n\tif (unlikely(ret)) {\n\t\tdprintk(\"EFAULT: aio_key\\n\");\n\t\tgoto out_put_req;\n\t}\n\n\treq->ki_obj.user = user_iocb;\n\treq->ki_user_data = iocb->aio_data;\n\treq->ki_pos = iocb->aio_offset;\n\n\treq->ki_buf = (char __user *)(unsigned long)iocb->aio_buf;\n\treq->ki_left = req->ki_nbytes = iocb->aio_nbytes;\n\treq->ki_opcode = iocb->aio_lio_opcode;\n\n\tret = aio_setup_iocb(req, compat);\n\n\tif (ret)\n\t\tgoto out_put_req;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\t/*\n\t * We could have raced with io_destroy() and are currently holding a\n\t * reference to ctx which should be destroyed. We cannot submit IO\n\t * since ctx gets freed as soon as io_submit() puts its reference.  The\n\t * check here is reliable: io_destroy() sets ctx->dead before waiting\n\t * for outstanding IO and the barrier between these two is realized by\n\t * unlock of mm->ioctx_lock and lock of ctx->ctx_lock.  Analogously we\n\t * increment ctx->reqs_active before checking for ctx->dead and the\n\t * barrier is realized by unlock and lock of ctx->ctx_lock. Thus if we\n\t * don't see ctx->dead set here, io_destroy() waits for our IO to\n\t * finish.\n\t */\n\tif (ctx->dead) {\n\t\tspin_unlock_irq(&ctx->ctx_lock);\n\t\tret = -EINVAL;\n\t\tgoto out_put_req;\n\t}\n\taio_run_iocb(req);\n\tif (!list_empty(&ctx->run_list)) {\n\t\t/* drain the run list */\n\t\twhile (__aio_run_iocbs(ctx))\n\t\t\t;\n\t}\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\taio_put_req(req);\t/* drop extra ref to req */\n\treturn 0;\n\nout_put_req:\n\taio_put_req(req);\t/* drop extra ref to req */\n\taio_put_req(req);\t/* drop i/o ref to req */\n\treturn ret;\n}\n\nlong do_io_submit(aio_context_t ctx_id, long nr,\n\t\t  struct iocb __user *__user *iocbpp, bool compat)\n{\n\tstruct kioctx *ctx;\n\tlong ret = 0;\n\tint i = 0;\n\tstruct blk_plug plug;\n\tstruct kiocb_batch batch;\n\n\tif (unlikely(nr < 0))\n\t\treturn -EINVAL;\n\n\tif (unlikely(nr > LONG_MAX/sizeof(*iocbpp)))\n\t\tnr = LONG_MAX/sizeof(*iocbpp);\n\n\tif (unlikely(!access_ok(VERIFY_READ, iocbpp, (nr*sizeof(*iocbpp)))))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx)) {\n\t\tpr_debug(\"EINVAL: io_submit: invalid context id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tkiocb_batch_init(&batch, nr);\n\n\tblk_start_plug(&plug);\n\n\t/*\n\t * AKPM: should this return a partial result if some of the IOs were\n\t * successfully submitted?\n\t */\n\tfor (i=0; i<nr; i++) {\n\t\tstruct iocb __user *user_iocb;\n\t\tstruct iocb tmp;\n\n\t\tif (unlikely(__get_user(user_iocb, iocbpp + i))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(copy_from_user(&tmp, user_iocb, sizeof(tmp)))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = io_submit_one(ctx, user_iocb, &tmp, &batch, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tblk_finish_plug(&plug);\n\n\tkiocb_batch_free(ctx, &batch);\n\tput_ioctx(ctx);\n\treturn i ? i : ret;\n}\n\n/* sys_io_submit:\n *\tQueue the nr iocbs pointed to by iocbpp for processing.  Returns\n *\tthe number of iocbs queued.  May return -EINVAL if the aio_context\n *\tspecified by ctx_id is invalid, if nr is < 0, if the iocb at\n *\t*iocbpp[0] is not properly initialized, if the operation specified\n *\tis invalid for the file descriptor in the iocb.  May fail with\n *\t-EFAULT if any of the data structures point to invalid data.  May\n *\tfail with -EBADF if the file descriptor specified in the first\n *\tiocb is invalid.  May fail with -EAGAIN if insufficient resources\n *\tare available to queue any iocbs.  Will return 0 if nr is 0.  Will\n *\tfail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr,\n\t\tstruct iocb __user * __user *, iocbpp)\n{\n\treturn do_io_submit(ctx_id, nr, iocbpp, 0);\n}\n\n/* lookup_kiocb\n *\tFinds a given iocb for cancellation.\n */\nstatic struct kiocb *lookup_kiocb(struct kioctx *ctx, struct iocb __user *iocb,\n\t\t\t\t  u32 key)\n{\n\tstruct list_head *pos;\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\t/* TODO: use a hash or array, this sucks. */\n\tlist_for_each(pos, &ctx->active_reqs) {\n\t\tstruct kiocb *kiocb = list_kiocb(pos);\n\t\tif (kiocb->ki_obj.user == iocb && kiocb->ki_key == key)\n\t\t\treturn kiocb;\n\t}\n\treturn NULL;\n}\n\n/* sys_io_cancel:\n *\tAttempts to cancel an iocb previously passed to io_submit.  If\n *\tthe operation is successfully cancelled, the resulting event is\n *\tcopied into the memory pointed to by result without being placed\n *\tinto the completion queue and 0 is returned.  May fail with\n *\t-EFAULT if any of the data structures pointed to are invalid.\n *\tMay fail with -EINVAL if aio_context specified by ctx_id is\n *\tinvalid.  May fail with -EAGAIN if the iocb specified was not\n *\tcancelled.  Will fail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE3(io_cancel, aio_context_t, ctx_id, struct iocb __user *, iocb,\n\t\tstruct io_event __user *, result)\n{\n\tint (*cancel)(struct kiocb *iocb, struct io_event *res);\n\tstruct kioctx *ctx;\n\tstruct kiocb *kiocb;\n\tu32 key;\n\tint ret;\n\n\tret = get_user(key, &iocb->aio_key);\n\tif (unlikely(ret))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx))\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tret = -EAGAIN;\n\tkiocb = lookup_kiocb(ctx, iocb, key);\n\tif (kiocb && kiocb->ki_cancel) {\n\t\tcancel = kiocb->ki_cancel;\n\t\tkiocb->ki_users ++;\n\t\tkiocbSetCancelled(kiocb);\n\t} else\n\t\tcancel = NULL;\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\tif (NULL != cancel) {\n\t\tstruct io_event tmp;\n\t\tpr_debug(\"calling cancel\\n\");\n\t\tmemset(&tmp, 0, sizeof(tmp));\n\t\ttmp.obj = (u64)(unsigned long)kiocb->ki_obj.user;\n\t\ttmp.data = kiocb->ki_user_data;\n\t\tret = cancel(kiocb, &tmp);\n\t\tif (!ret) {\n\t\t\t/* Cancellation succeeded -- copy the result\n\t\t\t * into the user's buffer.\n\t\t\t */\n\t\t\tif (copy_to_user(result, &tmp, sizeof(tmp)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\t} else\n\t\tret = -EINVAL;\n\n\tput_ioctx(ctx);\n\n\treturn ret;\n}\n\n/* io_getevents:\n *\tAttempts to read at least min_nr events and up to nr events from\n *\tthe completion queue for the aio_context specified by ctx_id. If\n *\tit succeeds, the number of read events is returned. May fail with\n *\t-EINVAL if ctx_id is invalid, if min_nr is out of range, if nr is\n *\tout of range, if timeout is out of range.  May fail with -EFAULT\n *\tif any of the memory specified is invalid.  May return 0 or\n *\t< min_nr if the timeout specified by timeout has elapsed\n *\tbefore sufficient events are available, where timeout == NULL\n *\tspecifies an infinite timeout. Note that the timeout pointed to by\n *\ttimeout is relative and will be updated if not NULL and the\n *\toperation blocks. Will fail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE5(io_getevents, aio_context_t, ctx_id,\n\t\tlong, min_nr,\n\t\tlong, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct timespec __user *, timeout)\n{\n\tstruct kioctx *ioctx = lookup_ioctx(ctx_id);\n\tlong ret = -EINVAL;\n\n\tif (likely(ioctx)) {\n\t\tif (likely(min_nr <= nr && min_nr >= 0))\n\t\t\tret = read_events(ioctx, min_nr, nr, events, timeout);\n\t\tput_ioctx(ioctx);\n\t}\n\n\tasmlinkage_protect(5, ret, ctx_id, min_nr, nr, events, timeout);\n\treturn ret;\n}\n"], "filenames": ["fs/aio.c"], "buggy_code_start_loc": [479], "buggy_code_end_loc": [1746], "fixing_code_start_loc": [479], "fixing_code_end_loc": [1753], "type": "CWE-400", "message": "The kiocb_batch_free function in fs/aio.c in the Linux kernel before 3.2.2 allows local users to cause a denial of service (OOPS) via vectors that trigger incorrect iocb management.", "other": {"cve": {"id": "CVE-2012-0058", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-17T11:00:36.227", "lastModified": "2020-07-29T16:56:55.787", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The kiocb_batch_free function in fs/aio.c in the Linux kernel before 3.2.2 allows local users to cause a denial of service (OOPS) via vectors that trigger incorrect iocb management."}, {"lang": "es", "value": "La funci\u00f3n kiocb_batch_free en fs/aio.c en el kernel de Linux antes de v3.2.2 permite a usuarios locales provocar una denegaci\u00f3n de servicio a trav\u00e9s de vectores que provocan una gesti\u00f3n incorrecta de IOCB."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.2.2", "matchCriteriaId": "D5AEDA47-7122-4C1A-A764-32500A089909"}]}]}], "references": [{"url": "http://marc.info/?l=bugtraq&m=139447903326211&w=2", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.2.2", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2012/01/18/7", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securitytracker.com/id?1027085", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=782696", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/802f43594d6e4d2ac61086d239153c17873a0428", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/802f43594d6e4d2ac61086d239153c17873a0428"}}