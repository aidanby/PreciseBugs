{"buggy_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <utility>\n#include <vector>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/variant.h\"\n#include \"tensorflow/core/framework/variant_encode_decode.h\"\n#include \"tensorflow/core/kernels/ragged_tensor_variant.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n\nnamespace tensorflow {\nnamespace {\n\nStatus RaggedComponentsFromVariant(\n    const Tensor& encoded_variant, int ragged_rank, DataType value_dtype,\n    DataType split_dtype, std::vector<RaggedTensorVariant>* decoded_ragged) {\n  const auto& flat_variants = encoded_variant.flat<Variant>();\n  decoded_ragged->reserve(flat_variants.size());\n\n  for (int i = 0; i < flat_variants.size(); i++) {\n    const auto& flat_variant = flat_variants(i);\n    const RaggedTensorVariant* decoded =\n        flat_variant.get<RaggedTensorVariant>();\n    if (decoded == nullptr) {\n      return errors::InvalidArgument(\n          \"Input Variant element at index \", i,\n          \" doesn't hold a RaggedTensorVariant: \", flat_variant.DebugString());\n    }\n    decoded_ragged->push_back(*decoded);\n    decoded = &decoded_ragged->back();\n    // Check ragged rank & types\n    if (decoded->ragged_rank() != ragged_rank) {\n      return errors::InvalidArgument(\n          \"Encoded input RaggedTensorVariant has ragged_rank=\",\n          decoded->ragged_rank(), \".  Expected ragged_rank=\", ragged_rank, \".\");\n    }\n    if (decoded->values().dtype() != value_dtype) {\n      return errors::InvalidArgument(\n          \"Expected values Tensor dtype: \", DataTypeString(value_dtype),\n          \", found: \", DataTypeString(decoded->values().dtype()));\n    }\n    if (decoded->values().dims() < 1) {\n      return errors::InvalidArgument(\n          \"Ragged values must have rank >= 1; encoded scalar element at index \",\n          i, \" has values Tensor: \", decoded->values().DebugString());\n    }\n    for (const auto& splits : decoded->nested_splits()) {\n      if (splits.dtype() != split_dtype) {\n        return errors::InvalidArgument(\n            \"Expected row_splits Tensor dtype: \", DataTypeString(split_dtype),\n            \", found: \", DataTypeString(splits.dtype()));\n      }\n      if (splits.dims() != 1) {\n        return errors::InvalidArgument(\n            \"Ragged splits must have rank 1; encoded scalar element at index \",\n            i, \" has splits Tensor \", splits.DebugString());\n      }\n    }\n  }\n  return Status::OK();\n}\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nStatus NestedStackRaggedTensors(\n    const std::vector<RaggedTensorVariant>& ragged_components,\n    const std::vector<int>& nested_dim_sizes, const int input_ragged_rank,\n    const int output_ragged_rank, RaggedTensorVariant* output_ragged) {\n  output_ragged->mutable_nested_splits()->reserve(output_ragged_rank);\n  const int dims = nested_dim_sizes.size();\n\n  // Populate first `dims - 1` splits.\n  for (int i = 0; i < dims - 1; i++) {\n    int dims_splits_size = nested_dim_sizes[i] + 1;\n    output_ragged->append_splits(Tensor(DataTypeToEnum<SPLIT_TYPE>::value,\n                                        TensorShape({dims_splits_size})));\n    auto splits_vec = output_ragged->mutable_splits(i)->vec<SPLIT_TYPE>();\n    int split_diff = nested_dim_sizes[i + 1];\n    for (int j = 0; j < dims_splits_size; j++) {\n      splits_vec(j) = j * split_diff;\n    }\n  }\n\n  // Populate `dims`-th split.\n  int splits_size = ragged_components.size() + 1;\n  output_ragged->append_splits(\n      Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({splits_size})));\n  auto dims_splits_vec =\n      output_ragged->mutable_splits(dims - 1)->vec<SPLIT_TYPE>();\n  dims_splits_vec(0) = 0;\n  for (int i = 0; i < ragged_components.size(); i++) {\n    int split_val = ragged_components[i].values().shape().dim_size(0);\n    if (input_ragged_rank != 0 && ragged_components[i].ragged_rank() > 0) {\n      split_val = ragged_components[i].splits(0).NumElements() - 1;\n    }\n    dims_splits_vec(i + 1) = dims_splits_vec(i) + split_val;\n  }\n\n  // Populate last `input_ragged_rank` splits.\n  for (int i = 0; i < input_ragged_rank; i++) {\n    int split_index = dims + i;\n    int split_size = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (!ragged_components[j].nested_splits().empty()) {\n        split_size += ragged_components[j].splits(i).NumElements() - 1;\n      }\n    }\n    output_ragged->append_splits(\n        Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({split_size})));\n    auto splits_vec =\n        output_ragged->mutable_splits(split_index)->vec<SPLIT_TYPE>();\n    splits_vec(0) = 0;\n    SPLIT_TYPE last_split_value = 0;\n    int index = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (ragged_components[j].nested_splits().empty()) {\n        // Corner case: empty row. e.g [ [[x], [x]], [] ]\n        continue;\n      }\n      auto component_splits_vec =\n          ragged_components[j].splits(i).vec<SPLIT_TYPE>();\n      for (int k = 1; k < component_splits_vec.size(); k++, index++) {\n        splits_vec(index) = component_splits_vec(k) + last_split_value;\n      }\n      last_split_value = splits_vec(index - 1);\n    }\n  }\n\n  // If the variant tensor input is empty, then we have no way to determine\n  // the correct shape for the dense_values.  (It must have rank>=1, and its\n  // outer dimension must be 0, but we don't know its shape beyond that.)\n  // For now, we just use a shape of `[0]` in this case.\n  // TODO(edloper): Update this op with an attribute containing information\n  // about dense_values shape.  If it's `None`, then we'll probably still have\n  // to use shape=[0] here, but if we have more info, then we can use it.\n  // E.g., in map_fn, we may have shape info from the RaggedTensorSpec.\n  TensorShape component_values_shape;\n  if (ragged_components.empty()) {\n    component_values_shape = TensorShape({0});\n  } else {\n    component_values_shape = ragged_components[0].values().shape();\n  }\n\n  // Populate values.\n  int values_size = component_values_shape.dim_size(0);\n  for (int i = 1; i < ragged_components.size(); i++) {\n    if (ragged_components[i].values().dims() != component_values_shape.dims()) {\n      return errors::InvalidArgument(\n          \"Rank of values must match for all \"\n          \"components; values shape at index 0: \",\n          component_values_shape.DebugString(), \", values shape at index \", i,\n          \": \", ragged_components[i].values().shape().DebugString());\n    }\n    values_size += ragged_components[i].values().shape().dim_size(0);\n  }\n  component_values_shape.set_dim(0, values_size);\n  output_ragged->set_values(\n      Tensor(DataTypeToEnum<VALUE_TYPE>::value, component_values_shape));\n  auto output_values_flat =\n      output_ragged->mutable_values()->flat_outer_dims<VALUE_TYPE, 2>();\n  int values_index = 0;\n  for (int i = 0; i < ragged_components.size(); i++) {\n    auto component_values_flat =\n        ragged_components[i].values().flat_outer_dims<VALUE_TYPE, 2>();\n    int num_inner_elements = ragged_components[i].values().NumElements();\n    if (ragged_components[i].values().dim_size(0) > 0) {\n      num_inner_elements /= ragged_components[i].values().dim_size(0);\n    }\n    for (int j = 0; j < ragged_components[i].values().dim_size(0);\n         j++, values_index++) {\n      for (int k = 0; k < num_inner_elements; k++) {\n        output_values_flat(values_index, k) = component_values_flat(j, k);\n      }\n    }\n  }\n  return Status::OK();\n}\n}  // namespace\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nclass RaggedTensorFromVariantOp : public OpKernel {\n public:\n  explicit RaggedTensorFromVariantOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"input_ragged_rank\",\n                                             &input_ragged_rank_attr_));\n    OP_REQUIRES_OK(\n        context, context->GetAttr(\"output_ragged_rank\", &output_ragged_rank_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Read input Tensor.\n    const Tensor& encoded_variant = context->input(0);\n    auto input_ragged_rank_ = input_ragged_rank_attr_;\n\n    if (input_ragged_rank_ == -1) {  // Infer input_ragged_rank_.\n      input_ragged_rank_ = output_ragged_rank_ - encoded_variant.dims();\n      OP_REQUIRES(context, input_ragged_rank_ >= 0,\n                  errors::InvalidArgument(\n                      \"Inferred input_ragged_rank (output_ragged_rank - \"\n                      \"encoded_variant.dims()) must be >= 0, found \"\n                      \"output_ragged_rank: \",\n                      output_ragged_rank_,\n                      \", encoded_variant.dims(): \", encoded_variant.dims(),\n                      \", inferred input_ragged_rank: \", input_ragged_rank_));\n    }\n    OP_REQUIRES(\n        context,\n        output_ragged_rank_ == encoded_variant.dims() + input_ragged_rank_,\n        errors::InvalidArgument(\n            \"output_ragged_rank must be equal to input_ragged_rank + \"\n            \"encoded_ragged.dims(); output_ragged_rank: \",\n            output_ragged_rank_, \", input_ragged_rank: \", input_ragged_rank_,\n            \", encoded_variant.dims(): \", encoded_variant.dims(), \".\"));\n\n    // Decode all variants.\n    const auto value_dtype = DataTypeToEnum<VALUE_TYPE>::v();\n    const auto split_dtype = DataTypeToEnum<SPLIT_TYPE>::v();\n    std::vector<RaggedTensorVariant> decoded_components;\n    OP_REQUIRES_OK(context, RaggedComponentsFromVariant(\n                                encoded_variant, input_ragged_rank_,\n                                value_dtype, split_dtype, &decoded_components));\n\n    // Corner case: input is a scalar.\n    if (encoded_variant.dims() == 0) {\n      ReturnRaggedTensor(context, decoded_components[0]);\n      return;\n    }\n\n    // Nested-Stack Ragged components into a batched RaggedTensor.\n    std::vector<int> encoded_dim_sizes(encoded_variant.dims(), 0);\n    for (int i = 0; i < encoded_variant.dims(); i++) {\n      encoded_dim_sizes[i] = encoded_variant.dim_size(i);\n    }\n    RaggedTensorVariant output_ragged;\n    OP_REQUIRES_OK(\n        context, NestedStackRaggedTensors<VALUE_TYPE, SPLIT_TYPE>(\n                     decoded_components, encoded_dim_sizes, input_ragged_rank_,\n                     output_ragged_rank_, &output_ragged));\n\n    // Set output.\n    ReturnRaggedTensor(context, output_ragged);\n  }\n\n private:\n  int input_ragged_rank_attr_;\n  int output_ragged_rank_;\n\n  void ReturnRaggedTensor(OpKernelContext* context,\n                          const RaggedTensorVariant& ragged_tensor) {\n    int ragged_rank = ragged_tensor.ragged_rank();\n    OpOutputList splits_out;\n    OP_REQUIRES_OK(context,\n                   context->output_list(\"output_nested_splits\", &splits_out));\n    for (int i = 0; i < ragged_rank; i++) {\n      splits_out.set(i, ragged_tensor.splits(i));\n    }\n    context->set_output(ragged_rank, ragged_tensor.values());\n  }\n};\n\n#define REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, split_type)      \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedTensorFromVariant\")             \\\n                              .Device(DEVICE_CPU)                     \\\n                              .TypeConstraint<value_type>(\"Tvalues\")  \\\n                              .TypeConstraint<split_type>(\"Tsplits\"), \\\n                          RaggedTensorFromVariantOp<value_type, split_type>);\n#define REGISTER_KERNELS(value_type)                  \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int32) \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int64)\nTF_CALL_POD_TYPES(REGISTER_KERNELS);\nTF_CALL_tstring(REGISTER_KERNELS);\nTF_CALL_QUANTIZED_TYPES(REGISTER_KERNELS);\nTF_CALL_quint16(REGISTER_KERNELS);\nTF_CALL_qint16(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n#undef REGISTER_KERNELS_WITH_SPLIT_TYPE\n}  // namespace tensorflow\n", "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for ragged_map_ops.map_fn.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops as mo\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_functional_ops\nfrom tensorflow.python.ops.ragged import ragged_map_ops\nfrom tensorflow.python.ops.ragged import ragged_math_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.platform import googletest\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RaggedMapOpTest(test_util.TensorFlowTestCase,\n                      parameterized.TestCase):\n\n  @parameterized.parameters([\n      # The following test sets map over a RaggedTensor and apply a\n      # transformation that returns with shape:\n      # [d1, (d2)] -> [d1]\n      dict(\n          fn=mo.reduce_mean,\n          elems=[[1, 2, 3], [4, 5], [6, 7]],\n          elems_dtype=dtypes.int32,\n          expected_output=[2, 4, 6],\n          result_dtype=dtypes.int32,\n      ),\n      dict(\n          fn=string_ops.reduce_join,\n          elems=[['foo', 'bar', 'baz'], ['a'], ['b', 'c']],\n          expected_output=[b'foobarbaz', b'a', b'bc'],\n          elems_dtype=dtypes.string,\n          result_dtype=dtypes.string,\n      ),\n      # [d1, (d2)] -> [d1, 2]\n      dict(\n          fn=lambda x: array_ops.stack([mo.reduce_mean(x), mo.reduce_sum(x)]),\n          # fn=self.stack_mean_and_sum,\n          elems=[[1, 2, 3], [4, 5], [6, 7]],\n          expected_output=[[2, 6], [4.5, 9], [6.5, 13]],\n          elems_dtype=dtypes.float32,\n          result_dtype=dtypes.float32,\n          expected_ragged_rank=0,\n      ),\n      # [d1, (d2)] -> [d1, (d2)]\n      dict(\n          fn=lambda x: x + np.int64(1),\n          elems=[[1, 2, 3], [4, 5], [6, 7]],\n          expected_output=[[2, 3, 4], [5, 6], [7, 8]],\n          elems_dtype=dtypes.int64,\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n      ),\n      # [d1, (d2), d3] -> [d1, (d2), d3]\n      dict(\n          fn=lambda x: x + np.int64(1),\n          elems=[[[1, 2], [3, 4]], [], [[5, 6], [7, 8], [9, 0]]],\n          elems_ragged_rank=1,\n          expected_ragged_rank=1,\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n          expected_output=[[[2, 3], [4, 5]], [], [[6, 7], [8, 9], [10, 1]]],\n      ),\n      # [d1, (d2)] -> [d1, (d2), (d3)]\n      dict(\n          fn=lambda x: ragged_tensor.RaggedTensor.from_row_starts(x, [0]),\n          elems=[[1, 2, 3], [4, 5], [6, 7]],\n          expected_output=[[[1, 2, 3]], [[4, 5]], [[6, 7]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2),\n      ),\n      # [d1, (d2), (d3)] -> [d1, (d2), (d3)]\n      dict(\n          fn=lambda x: ragged_functional_ops.map_flat_values(mo.add, x, 1),\n          elems=[[[1, 2, 3]], [[4, 5], [6, 7]]],\n          expected_output=[[[2, 3, 4]], [[5, 6], [7, 8]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2),\n      ),\n      # [d1, (d2), (d3)] -> [d1, (d2)]\n      dict(\n          fn=lambda x: ragged_math_ops.reduce_sum(x, axis=1),\n          elems=[[[1, 2, 3]], [[4, 5], [6, 7]]],\n          expected_output=[[6], [9, 13]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n      ),\n      # [d1, (d2), (d3)] -> [d1, (d3)]\n      dict(\n          fn=lambda x: ragged_math_ops.reduce_sum(x, axis=0),\n          elems=[[[1, 2, 3]], [[4, 5], [6, 7]]],\n          expected_output=[[1, 2, 3], [10, 12]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n      ),\n      # [d1, (d2), (d3)] -> [d1]\n      dict(\n          fn=ragged_math_ops.reduce_sum,\n          elems=[[[1, 2, 3]], [[4, 5], [6, 7]]],\n          expected_output=[6, 22],\n          result_dtype=dtypes.int64,\n      ),\n      # [d1] -> [d1, (d2)]\n      dict(\n          fn=mo.range,\n          elems=[4, 0, 2],\n          expected_output=[[0, 1, 2, 3], [], [0, 1]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n      ),\n      # [d1] -> [d1, (d2), (d3)]\n      dict(\n          fn=lambda x: ragged_math_ops.range(mo.range(x)),\n          elems=[5, 0, 3],\n          expected_output=[[[], [0], [0, 1], [0, 1, 2], [0, 1, 2, 3]], [],\n                           [[], [0], [0, 1]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2),\n      ),\n      # [d1, (d2), (d3), (d4a), (d5)] ->  [d1, (d2), (d3), (d4b), (d5)]\n      dict(\n          fn=lambda x: x + np.int64(1),\n          elems=[[[[[1, 2, 3]], [[4], [5]]]], [[[[6, 7]]], [[[8], []]]]],\n          expected_output=[[[[[2, 3, 4]], [[5], [6]]]], [[[[7, 8]]], [[[9],\n                                                                       []]]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=4),\n      ),\n      # [d1] -> [d1, (d2), (d3)]\n      dict(\n          fn=ragged_math_ops.range,\n          elems=np.array([1, 2, 3], np.int64),\n          expected_output=[[[0]], [[0, 1]], [[0, 1, 2]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2)),\n      # [0] -> [0, (d2), (d3)]  (github issue #36232)\n      dict(\n          fn=ragged_math_ops.range,\n          elems=np.zeros([0], np.int64),\n          expected_output=[],\n          expected_ragged_rank=2,\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2)),\n  ])\n\n  def testRaggedMap(\n      self,\n      fn,\n      elems,\n      expected_output,\n      expected_ragged_rank=None,\n      result_ragged_rank=None,\n      elems_ragged_rank=None,\n      elems_dtype=dtypes.int64,\n      result_dtype=None,\n      infer_shape=True,\n  ):\n    elems = ragged_factory_ops.constant(elems, elems_dtype, elems_ragged_rank)\n    output = ragged_map_ops.map_fn(\n        fn=fn, elems=elems, dtype=result_dtype, infer_shape=infer_shape)\n\n    expected_rt = ragged_factory_ops.constant(\n        expected_output, ragged_rank=expected_ragged_rank)\n    self.assertAllEqual(expected_rt, output)\n\n  def testRaggedMapOnStructure(self):\n    batman = ragged_factory_ops.constant([[1, 2, 3], [4], [5, 6, 7]])\n    # [[10, 20, 30], [40], [50, 60, 70]]\n    robin = ragged_functional_ops.map_flat_values(mo.multiply, batman, 10)\n\n    features = {'batman': batman, 'robin': robin}\n\n    def _reduce_sum_from_all(f):\n      return mo.reduce_sum(f['batman']) + mo.reduce_sum(f['robin'])\n\n    output = ragged_map_ops.map_fn(\n        fn=_reduce_sum_from_all,\n        elems=features,\n        dtype=dtypes.int32,\n    )\n\n    self.assertAllEqual(output, [66, 44, 198])\n\n  # Test mapping over a dict of RTs can produce a dict of RTs.\n  def testRaggedMapOnStructure_RaggedOutputs(self):\n    batman = ragged_factory_ops.constant([[1, 2, 3], [4], [5, 6, 7]])\n    # [[10, 20, 30], [40], [50, 60, 70]]\n    robin = ragged_functional_ops.map_flat_values(mo.multiply, batman, 10)\n\n    features = {'batman': batman, 'robin': robin}\n\n    def _increment(f):\n      return {\n          'batman': f['batman'] + 1,\n          'robin': f['robin'] + 1,\n      }\n\n    output = ragged_map_ops.map_fn(\n        fn=_increment,\n        elems=features,\n        infer_shape=False,\n        dtype={\n            'batman':\n                ragged_tensor.RaggedTensorType(\n                    dtype=dtypes.int32, ragged_rank=1),\n            'robin':\n                ragged_tensor.RaggedTensorType(\n                    dtype=dtypes.int32, ragged_rank=1)\n        },\n    )\n\n    self.assertAllEqual(output['batman'], [[2, 3, 4], [5], [6, 7, 8]])\n    self.assertAllEqual(output['robin'], [[11, 21, 31], [41], [51, 61, 71]])\n\n  def testZip(self):\n    x = ragged_factory_ops.constant(\n        [[10, 20], [30, 40], [50, 60], [70], [80, 90, 100]], dtypes.int64)\n    y = array_ops.expand_dims(mo.range(x.nrows(out_type=dtypes.int64)), axis=1)\n\n    def _zip(foo):\n      y_val, x_val = foo\n      bar = array_ops.tile(y_val, array_ops.shape(x_val))\n      return array_ops.stack([bar, x_val], axis=1)\n\n    output = ragged_map_ops.map_fn(\n        _zip, (y, x),\n        dtype=ragged_tensor.RaggedTensorType(dtype=dtypes.int64, ragged_rank=1),\n        infer_shape=False)\n\n    self.assertAllEqual(\n        output, [[[0, 10], [0, 20]], [[1, 30], [1, 40]], [[2, 50], [2, 60]],\n                 [[3, 70]], [[4, 80], [4, 90], [4, 100]]])\n\n  def testBatchGather(self):\n    tokens = ragged_factory_ops.constant([['hello', '.', 'there'], ['merhaba'],\n                                          ['bonjour', '.', 'ca va', '?']])\n    indices = ragged_factory_ops.constant([[0, 2], [0], [0, 2]])\n\n    def gather(x):\n      tokens_val, indices_val = x\n      return array_ops.gather(tokens_val, indices_val)\n\n    data = tokens, indices\n    out = ragged_map_ops.map_fn(\n        gather,\n        data,\n        dtype=ragged_tensor.RaggedTensorType(\n            dtype=dtypes.string, ragged_rank=1),\n        infer_shape=False)\n\n    self.assertAllEqual(\n        out, [[b'hello', b'there'], [b'merhaba'], [b'bonjour', b'ca va']])\n\n  def testMismatchRaggedRank(self):\n    elems = ragged_factory_ops.constant([[[1, 2, 3]], [[4, 5], [6, 7]]])\n    fn = lambda x: ragged_math_ops.reduce_sum(x, axis=0)\n    with self.assertRaisesRegex(\n        ValueError, r'(?s)Expected `fn` to return.*But it returned.*'):\n      _ = ragged_map_ops.map_fn(\n          fn,\n          elems,\n          dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=23))\n\n  def testMismatchRaggedRank2(self):\n    elems = ragged_factory_ops.constant([[1, 2, 3], [4, 5], [6, 7]])\n    fn = lambda x: ragged_tensor.RaggedTensor.from_row_starts(x, [0])\n    with self.assertRaisesRegex(\n        ValueError, r'(?s)Expected `fn` to return.*But it returned.*'):\n      _ = ragged_map_ops.map_fn(\n          fn,\n          elems,\n          dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=10))\n\n  def testMapOnSparseTensor(self):\n    s = sparse_tensor.SparseTensor(\n        indices=[[0, 0], [0, 1], [1, 0], [1, 1]],\n        values=[0, 5, 0, 4],\n        dense_shape=[2, 2],\n    )\n    t2 = ragged_tensor.RaggedTensor.from_sparse(s)\n    id_t2 = ragged_map_ops.map_fn(\n        lambda x: x, t2,\n    )\n    self.assertAllEqual(id_t2, [[0, 5], [0, 4]])\n\n\nif __name__ == '__main__':\n  googletest.main()\n"], "fixing_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <utility>\n#include <vector>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/variant.h\"\n#include \"tensorflow/core/framework/variant_encode_decode.h\"\n#include \"tensorflow/core/kernels/ragged_tensor_variant.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n\nnamespace tensorflow {\nnamespace {\n\nStatus RaggedComponentsFromVariant(\n    const Tensor& encoded_variant, int ragged_rank, DataType value_dtype,\n    DataType split_dtype, std::vector<RaggedTensorVariant>* decoded_ragged) {\n  const auto& flat_variants = encoded_variant.flat<Variant>();\n  decoded_ragged->reserve(flat_variants.size());\n\n  for (int i = 0; i < flat_variants.size(); i++) {\n    const auto& flat_variant = flat_variants(i);\n    const RaggedTensorVariant* decoded =\n        flat_variant.get<RaggedTensorVariant>();\n    if (decoded == nullptr) {\n      return errors::InvalidArgument(\n          \"Input Variant element at index \", i,\n          \" doesn't hold a RaggedTensorVariant: \", flat_variant.DebugString());\n    }\n    decoded_ragged->push_back(*decoded);\n    decoded = &decoded_ragged->back();\n    // Check ragged rank & types\n    if (decoded->ragged_rank() != ragged_rank) {\n      return errors::InvalidArgument(\n          \"Encoded input RaggedTensorVariant has ragged_rank=\",\n          decoded->ragged_rank(), \".  Expected ragged_rank=\", ragged_rank, \".\");\n    }\n    if (decoded->values().dtype() != value_dtype) {\n      return errors::InvalidArgument(\n          \"Expected values Tensor dtype: \", DataTypeString(value_dtype),\n          \", found: \", DataTypeString(decoded->values().dtype()));\n    }\n    if (decoded->values().dims() < 1) {\n      return errors::InvalidArgument(\n          \"Ragged values must have rank >= 1; encoded scalar element at index \",\n          i, \" has values Tensor: \", decoded->values().DebugString());\n    }\n    for (const auto& splits : decoded->nested_splits()) {\n      if (splits.dtype() != split_dtype) {\n        return errors::InvalidArgument(\n            \"Expected row_splits Tensor dtype: \", DataTypeString(split_dtype),\n            \", found: \", DataTypeString(splits.dtype()));\n      }\n      if (splits.dims() != 1) {\n        return errors::InvalidArgument(\n            \"Ragged splits must have rank 1; encoded scalar element at index \",\n            i, \" has splits Tensor \", splits.DebugString());\n      }\n    }\n  }\n  return Status::OK();\n}\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nStatus NestedStackRaggedTensors(\n    const std::vector<RaggedTensorVariant>& ragged_components,\n    const std::vector<int>& nested_dim_sizes, const int input_ragged_rank,\n    const int output_ragged_rank, RaggedTensorVariant* output_ragged) {\n  output_ragged->mutable_nested_splits()->reserve(output_ragged_rank);\n  const int dims = nested_dim_sizes.size();\n\n  // Populate first `dims - 1` splits.\n  for (int i = 0; i < dims - 1; i++) {\n    int dims_splits_size = nested_dim_sizes[i] + 1;\n    output_ragged->append_splits(Tensor(DataTypeToEnum<SPLIT_TYPE>::value,\n                                        TensorShape({dims_splits_size})));\n    auto splits_vec = output_ragged->mutable_splits(i)->vec<SPLIT_TYPE>();\n    int split_diff = nested_dim_sizes[i + 1];\n    for (int j = 0; j < dims_splits_size; j++) {\n      splits_vec(j) = j * split_diff;\n    }\n  }\n\n  // Populate `dims`-th split.\n  int splits_size = ragged_components.size() + 1;\n  output_ragged->append_splits(\n      Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({splits_size})));\n  auto dims_splits_vec =\n      output_ragged->mutable_splits(dims - 1)->vec<SPLIT_TYPE>();\n  dims_splits_vec(0) = 0;\n  for (int i = 0; i < ragged_components.size(); i++) {\n    int split_val = ragged_components[i].values().shape().dim_size(0);\n    if (input_ragged_rank != 0 && ragged_components[i].ragged_rank() > 0) {\n      split_val = ragged_components[i].splits(0).NumElements() - 1;\n    }\n    dims_splits_vec(i + 1) = dims_splits_vec(i) + split_val;\n  }\n\n  // Populate last `input_ragged_rank` splits.\n  for (int i = 0; i < input_ragged_rank; i++) {\n    int split_index = dims + i;\n    int split_size = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (!ragged_components[j].nested_splits().empty()) {\n        split_size += ragged_components[j].splits(i).NumElements() - 1;\n      }\n    }\n    output_ragged->append_splits(\n        Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({split_size})));\n    auto splits_vec =\n        output_ragged->mutable_splits(split_index)->vec<SPLIT_TYPE>();\n    splits_vec(0) = 0;\n    SPLIT_TYPE last_split_value = 0;\n    int index = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (ragged_components[j].nested_splits().empty()) {\n        // Corner case: empty row. e.g [ [[x], [x]], [] ]\n        continue;\n      }\n      auto component_splits_vec =\n          ragged_components[j].splits(i).vec<SPLIT_TYPE>();\n      for (int k = 1; k < component_splits_vec.size(); k++, index++) {\n        splits_vec(index) = component_splits_vec(k) + last_split_value;\n      }\n      last_split_value = splits_vec(index - 1);\n    }\n  }\n\n  // If the variant tensor input is empty, then we have no way to determine\n  // the correct shape for the dense_values.  (It must have rank>=1, and its\n  // outer dimension must be 0, but we don't know its shape beyond that.)\n  // For now, we just use a shape of `[0]` in this case.\n  // TODO(edloper): Update this op with an attribute containing information\n  // about dense_values shape.  If it's `None`, then we'll probably still have\n  // to use shape=[0] here, but if we have more info, then we can use it.\n  // E.g., in map_fn, we may have shape info from the RaggedTensorSpec.\n  TensorShape component_values_shape;\n  if (ragged_components.empty()) {\n    component_values_shape = TensorShape({0});\n  } else {\n    component_values_shape = ragged_components[0].values().shape();\n  }\n\n  // Populate values.\n  int values_size = component_values_shape.dim_size(0);\n  for (int i = 1; i < ragged_components.size(); i++) {\n    if (ragged_components[i].values().dims() != component_values_shape.dims()) {\n      return errors::InvalidArgument(\n          \"Rank of values must match for all \"\n          \"components; values shape at index 0: \",\n          component_values_shape.DebugString(), \", values shape at index \", i,\n          \": \", ragged_components[i].values().shape().DebugString());\n    }\n    values_size += ragged_components[i].values().shape().dim_size(0);\n  }\n  component_values_shape.set_dim(0, values_size);\n  output_ragged->set_values(\n      Tensor(DataTypeToEnum<VALUE_TYPE>::value, component_values_shape));\n  auto output_values_flat =\n      output_ragged->mutable_values()->flat_outer_dims<VALUE_TYPE, 2>();\n  int values_index = 0;\n\n  TensorShape expected_value_shape = component_values_shape;\n  expected_value_shape.RemoveDim(0);\n\n  for (int i = 0; i < ragged_components.size(); i++) {\n    // Check that the flat_values tensor shape is compatible.\n    TensorShape value_shape = ragged_components[i].values().shape();\n    value_shape.RemoveDim(0);\n    if (value_shape != expected_value_shape) {\n      return errors::InvalidArgument(\n          \"All flat_values must have compatible shapes.  Shape at index 0: \",\n          expected_value_shape, \".  Shape at index \", i, \": \", value_shape,\n          \".  If you are using tf.map_fn, then you may need to specify an \"\n          \"explicit fn_output_signature with appropriate ragged_rank, and/or \"\n          \"convert output tensors to RaggedTensors.\");\n    }\n\n    auto component_values_flat =\n        ragged_components[i].values().flat_outer_dims<VALUE_TYPE, 2>();\n    int num_inner_elements = ragged_components[i].values().NumElements();\n    if (ragged_components[i].values().dim_size(0) > 0) {\n      num_inner_elements /= ragged_components[i].values().dim_size(0);\n    }\n    for (int j = 0; j < ragged_components[i].values().dim_size(0);\n         j++, values_index++) {\n      for (int k = 0; k < num_inner_elements; k++) {\n        output_values_flat(values_index, k) = component_values_flat(j, k);\n      }\n    }\n  }\n  return Status::OK();\n}\n}  // namespace\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nclass RaggedTensorFromVariantOp : public OpKernel {\n public:\n  explicit RaggedTensorFromVariantOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"input_ragged_rank\",\n                                             &input_ragged_rank_attr_));\n    OP_REQUIRES_OK(\n        context, context->GetAttr(\"output_ragged_rank\", &output_ragged_rank_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Read input Tensor.\n    const Tensor& encoded_variant = context->input(0);\n    auto input_ragged_rank_ = input_ragged_rank_attr_;\n\n    if (input_ragged_rank_ == -1) {  // Infer input_ragged_rank_.\n      input_ragged_rank_ = output_ragged_rank_ - encoded_variant.dims();\n      OP_REQUIRES(context, input_ragged_rank_ >= 0,\n                  errors::InvalidArgument(\n                      \"Inferred input_ragged_rank (output_ragged_rank - \"\n                      \"encoded_variant.dims()) must be >= 0, found \"\n                      \"output_ragged_rank: \",\n                      output_ragged_rank_,\n                      \", encoded_variant.dims(): \", encoded_variant.dims(),\n                      \", inferred input_ragged_rank: \", input_ragged_rank_));\n    }\n    OP_REQUIRES(\n        context,\n        output_ragged_rank_ == encoded_variant.dims() + input_ragged_rank_,\n        errors::InvalidArgument(\n            \"output_ragged_rank must be equal to input_ragged_rank + \"\n            \"encoded_ragged.dims(); output_ragged_rank: \",\n            output_ragged_rank_, \", input_ragged_rank: \", input_ragged_rank_,\n            \", encoded_variant.dims(): \", encoded_variant.dims(), \".\"));\n\n    // Decode all variants.\n    const auto value_dtype = DataTypeToEnum<VALUE_TYPE>::v();\n    const auto split_dtype = DataTypeToEnum<SPLIT_TYPE>::v();\n    std::vector<RaggedTensorVariant> decoded_components;\n    OP_REQUIRES_OK(context, RaggedComponentsFromVariant(\n                                encoded_variant, input_ragged_rank_,\n                                value_dtype, split_dtype, &decoded_components));\n\n    // Corner case: input is a scalar.\n    if (encoded_variant.dims() == 0) {\n      ReturnRaggedTensor(context, decoded_components[0]);\n      return;\n    }\n\n    // Nested-Stack Ragged components into a batched RaggedTensor.\n    std::vector<int> encoded_dim_sizes(encoded_variant.dims(), 0);\n    for (int i = 0; i < encoded_variant.dims(); i++) {\n      encoded_dim_sizes[i] = encoded_variant.dim_size(i);\n    }\n    RaggedTensorVariant output_ragged;\n    OP_REQUIRES_OK(\n        context, NestedStackRaggedTensors<VALUE_TYPE, SPLIT_TYPE>(\n                     decoded_components, encoded_dim_sizes, input_ragged_rank_,\n                     output_ragged_rank_, &output_ragged));\n\n    // Set output.\n    ReturnRaggedTensor(context, output_ragged);\n  }\n\n private:\n  int input_ragged_rank_attr_;\n  int output_ragged_rank_;\n\n  void ReturnRaggedTensor(OpKernelContext* context,\n                          const RaggedTensorVariant& ragged_tensor) {\n    int ragged_rank = ragged_tensor.ragged_rank();\n    OpOutputList splits_out;\n    OP_REQUIRES_OK(context,\n                   context->output_list(\"output_nested_splits\", &splits_out));\n    for (int i = 0; i < ragged_rank; i++) {\n      splits_out.set(i, ragged_tensor.splits(i));\n    }\n    context->set_output(ragged_rank, ragged_tensor.values());\n  }\n};\n\n#define REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, split_type)      \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedTensorFromVariant\")             \\\n                              .Device(DEVICE_CPU)                     \\\n                              .TypeConstraint<value_type>(\"Tvalues\")  \\\n                              .TypeConstraint<split_type>(\"Tsplits\"), \\\n                          RaggedTensorFromVariantOp<value_type, split_type>);\n#define REGISTER_KERNELS(value_type)                  \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int32) \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int64)\nTF_CALL_POD_TYPES(REGISTER_KERNELS);\nTF_CALL_tstring(REGISTER_KERNELS);\nTF_CALL_QUANTIZED_TYPES(REGISTER_KERNELS);\nTF_CALL_quint16(REGISTER_KERNELS);\nTF_CALL_qint16(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n#undef REGISTER_KERNELS_WITH_SPLIT_TYPE\n}  // namespace tensorflow\n", "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for ragged_map_ops.map_fn.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import map_fn as map_fn_lib\nfrom tensorflow.python.ops import math_ops as mo\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_functional_ops\nfrom tensorflow.python.ops.ragged import ragged_map_ops\nfrom tensorflow.python.ops.ragged import ragged_math_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.platform import googletest\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RaggedMapOpTest(test_util.TensorFlowTestCase,\n                      parameterized.TestCase):\n\n  @parameterized.parameters([\n      # The following test sets map over a RaggedTensor and apply a\n      # transformation that returns with shape:\n      # [d1, (d2)] -> [d1]\n      dict(\n          fn=mo.reduce_mean,\n          elems=[[1, 2, 3], [4, 5], [6, 7]],\n          elems_dtype=dtypes.int32,\n          expected_output=[2, 4, 6],\n          result_dtype=dtypes.int32,\n      ),\n      dict(\n          fn=string_ops.reduce_join,\n          elems=[['foo', 'bar', 'baz'], ['a'], ['b', 'c']],\n          expected_output=[b'foobarbaz', b'a', b'bc'],\n          elems_dtype=dtypes.string,\n          result_dtype=dtypes.string,\n      ),\n      # [d1, (d2)] -> [d1, 2]\n      dict(\n          fn=lambda x: array_ops.stack([mo.reduce_mean(x), mo.reduce_sum(x)]),\n          # fn=self.stack_mean_and_sum,\n          elems=[[1, 2, 3], [4, 5], [6, 7]],\n          expected_output=[[2, 6], [4.5, 9], [6.5, 13]],\n          elems_dtype=dtypes.float32,\n          result_dtype=dtypes.float32,\n          expected_ragged_rank=0,\n      ),\n      # [d1, (d2)] -> [d1, (d2)]\n      dict(\n          fn=lambda x: x + np.int64(1),\n          elems=[[1, 2, 3], [4, 5], [6, 7]],\n          expected_output=[[2, 3, 4], [5, 6], [7, 8]],\n          elems_dtype=dtypes.int64,\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n      ),\n      # [d1, (d2), d3] -> [d1, (d2), d3]\n      dict(\n          fn=lambda x: x + np.int64(1),\n          elems=[[[1, 2], [3, 4]], [], [[5, 6], [7, 8], [9, 0]]],\n          elems_ragged_rank=1,\n          expected_ragged_rank=1,\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n          expected_output=[[[2, 3], [4, 5]], [], [[6, 7], [8, 9], [10, 1]]],\n      ),\n      # [d1, (d2)] -> [d1, (d2), (d3)]\n      dict(\n          fn=lambda x: ragged_tensor.RaggedTensor.from_row_starts(x, [0]),\n          elems=[[1, 2, 3], [4, 5], [6, 7]],\n          expected_output=[[[1, 2, 3]], [[4, 5]], [[6, 7]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2),\n      ),\n      # [d1, (d2), (d3)] -> [d1, (d2), (d3)]\n      dict(\n          fn=lambda x: ragged_functional_ops.map_flat_values(mo.add, x, 1),\n          elems=[[[1, 2, 3]], [[4, 5], [6, 7]]],\n          expected_output=[[[2, 3, 4]], [[5, 6], [7, 8]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2),\n      ),\n      # [d1, (d2), (d3)] -> [d1, (d2)]\n      dict(\n          fn=lambda x: ragged_math_ops.reduce_sum(x, axis=1),\n          elems=[[[1, 2, 3]], [[4, 5], [6, 7]]],\n          expected_output=[[6], [9, 13]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n      ),\n      # [d1, (d2), (d3)] -> [d1, (d3)]\n      dict(\n          fn=lambda x: ragged_math_ops.reduce_sum(x, axis=0),\n          elems=[[[1, 2, 3]], [[4, 5], [6, 7]]],\n          expected_output=[[1, 2, 3], [10, 12]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n      ),\n      # [d1, (d2), (d3)] -> [d1]\n      dict(\n          fn=ragged_math_ops.reduce_sum,\n          elems=[[[1, 2, 3]], [[4, 5], [6, 7]]],\n          expected_output=[6, 22],\n          result_dtype=dtypes.int64,\n      ),\n      # [d1] -> [d1, (d2)]\n      dict(\n          fn=mo.range,\n          elems=[4, 0, 2],\n          expected_output=[[0, 1, 2, 3], [], [0, 1]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=1),\n      ),\n      # [d1] -> [d1, (d2), (d3)]\n      dict(\n          fn=lambda x: ragged_math_ops.range(mo.range(x)),\n          elems=[5, 0, 3],\n          expected_output=[[[], [0], [0, 1], [0, 1, 2], [0, 1, 2, 3]], [],\n                           [[], [0], [0, 1]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2),\n      ),\n      # [d1, (d2), (d3), (d4a), (d5)] ->  [d1, (d2), (d3), (d4b), (d5)]\n      dict(\n          fn=lambda x: x + np.int64(1),\n          elems=[[[[[1, 2, 3]], [[4], [5]]]], [[[[6, 7]]], [[[8], []]]]],\n          expected_output=[[[[[2, 3, 4]], [[5], [6]]]], [[[[7, 8]]], [[[9],\n                                                                       []]]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=4),\n      ),\n      # [d1] -> [d1, (d2), (d3)]\n      dict(\n          fn=ragged_math_ops.range,\n          elems=np.array([1, 2, 3], np.int64),\n          expected_output=[[[0]], [[0, 1]], [[0, 1, 2]]],\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2)),\n      # [0] -> [0, (d2), (d3)]  (github issue #36232)\n      dict(\n          fn=ragged_math_ops.range,\n          elems=np.zeros([0], np.int64),\n          expected_output=[],\n          expected_ragged_rank=2,\n          result_dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=2)),\n  ])\n\n  def testRaggedMap(\n      self,\n      fn,\n      elems,\n      expected_output,\n      expected_ragged_rank=None,\n      result_ragged_rank=None,\n      elems_ragged_rank=None,\n      elems_dtype=dtypes.int64,\n      result_dtype=None,\n      infer_shape=True,\n  ):\n    elems = ragged_factory_ops.constant(elems, elems_dtype, elems_ragged_rank)\n    output = ragged_map_ops.map_fn(\n        fn=fn, elems=elems, dtype=result_dtype, infer_shape=infer_shape)\n\n    expected_rt = ragged_factory_ops.constant(\n        expected_output, ragged_rank=expected_ragged_rank)\n    self.assertAllEqual(expected_rt, output)\n\n  def testRaggedMapOnStructure(self):\n    batman = ragged_factory_ops.constant([[1, 2, 3], [4], [5, 6, 7]])\n    # [[10, 20, 30], [40], [50, 60, 70]]\n    robin = ragged_functional_ops.map_flat_values(mo.multiply, batman, 10)\n\n    features = {'batman': batman, 'robin': robin}\n\n    def _reduce_sum_from_all(f):\n      return mo.reduce_sum(f['batman']) + mo.reduce_sum(f['robin'])\n\n    output = ragged_map_ops.map_fn(\n        fn=_reduce_sum_from_all,\n        elems=features,\n        dtype=dtypes.int32,\n    )\n\n    self.assertAllEqual(output, [66, 44, 198])\n\n  # Test mapping over a dict of RTs can produce a dict of RTs.\n  def testRaggedMapOnStructure_RaggedOutputs(self):\n    batman = ragged_factory_ops.constant([[1, 2, 3], [4], [5, 6, 7]])\n    # [[10, 20, 30], [40], [50, 60, 70]]\n    robin = ragged_functional_ops.map_flat_values(mo.multiply, batman, 10)\n\n    features = {'batman': batman, 'robin': robin}\n\n    def _increment(f):\n      return {\n          'batman': f['batman'] + 1,\n          'robin': f['robin'] + 1,\n      }\n\n    output = ragged_map_ops.map_fn(\n        fn=_increment,\n        elems=features,\n        infer_shape=False,\n        dtype={\n            'batman':\n                ragged_tensor.RaggedTensorType(\n                    dtype=dtypes.int32, ragged_rank=1),\n            'robin':\n                ragged_tensor.RaggedTensorType(\n                    dtype=dtypes.int32, ragged_rank=1)\n        },\n    )\n\n    self.assertAllEqual(output['batman'], [[2, 3, 4], [5], [6, 7, 8]])\n    self.assertAllEqual(output['robin'], [[11, 21, 31], [41], [51, 61, 71]])\n\n  def testZip(self):\n    x = ragged_factory_ops.constant(\n        [[10, 20], [30, 40], [50, 60], [70], [80, 90, 100]], dtypes.int64)\n    y = array_ops.expand_dims(mo.range(x.nrows(out_type=dtypes.int64)), axis=1)\n\n    def _zip(foo):\n      y_val, x_val = foo\n      bar = array_ops.tile(y_val, array_ops.shape(x_val))\n      return array_ops.stack([bar, x_val], axis=1)\n\n    output = ragged_map_ops.map_fn(\n        _zip, (y, x),\n        dtype=ragged_tensor.RaggedTensorType(dtype=dtypes.int64, ragged_rank=1),\n        infer_shape=False)\n\n    self.assertAllEqual(\n        output, [[[0, 10], [0, 20]], [[1, 30], [1, 40]], [[2, 50], [2, 60]],\n                 [[3, 70]], [[4, 80], [4, 90], [4, 100]]])\n\n  def testBatchGather(self):\n    tokens = ragged_factory_ops.constant([['hello', '.', 'there'], ['merhaba'],\n                                          ['bonjour', '.', 'ca va', '?']])\n    indices = ragged_factory_ops.constant([[0, 2], [0], [0, 2]])\n\n    def gather(x):\n      tokens_val, indices_val = x\n      return array_ops.gather(tokens_val, indices_val)\n\n    data = tokens, indices\n    out = ragged_map_ops.map_fn(\n        gather,\n        data,\n        dtype=ragged_tensor.RaggedTensorType(\n            dtype=dtypes.string, ragged_rank=1),\n        infer_shape=False)\n\n    self.assertAllEqual(\n        out, [[b'hello', b'there'], [b'merhaba'], [b'bonjour', b'ca va']])\n\n  def testMismatchRaggedRank(self):\n    elems = ragged_factory_ops.constant([[[1, 2, 3]], [[4, 5], [6, 7]]])\n    fn = lambda x: ragged_math_ops.reduce_sum(x, axis=0)\n    with self.assertRaisesRegex(\n        ValueError, r'(?s)Expected `fn` to return.*But it returned.*'):\n      _ = ragged_map_ops.map_fn(\n          fn,\n          elems,\n          dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=23))\n\n  def testMismatchRaggedRank2(self):\n    elems = ragged_factory_ops.constant([[1, 2, 3], [4, 5], [6, 7]])\n    fn = lambda x: ragged_tensor.RaggedTensor.from_row_starts(x, [0])\n    with self.assertRaisesRegex(\n        ValueError, r'(?s)Expected `fn` to return.*But it returned.*'):\n      _ = ragged_map_ops.map_fn(\n          fn,\n          elems,\n          dtype=ragged_tensor.RaggedTensorType(\n              dtype=dtypes.int64, ragged_rank=10))\n\n  def testMapOnSparseTensor(self):\n    s = sparse_tensor.SparseTensor(\n        indices=[[0, 0], [0, 1], [1, 0], [1, 1]],\n        values=[0, 5, 0, 4],\n        dense_shape=[2, 2],\n    )\n    t2 = ragged_tensor.RaggedTensor.from_sparse(s)\n    id_t2 = ragged_map_ops.map_fn(\n        lambda x: x, t2,\n    )\n    self.assertAllEqual(id_t2, [[0, 5], [0, 4]])\n\n  def testRaggedMapWithIncorrectFnOutputSignature(self):\n    x = ragged_factory_ops.constant([[1, 2, 3, 4], [1]])\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                'All flat_values must have compatible shapes'):\n      y = map_fn_lib.map_fn(lambda r: map_fn_lib.map_fn(lambda y: r, r), x)\n      self.evaluate(y)\n\n  def testNestedRaggedMapWithFnOutputSignature(self):\n    ragged1d = ragged_tensor.RaggedTensorSpec([None], dtypes.int32)\n    ragged2d = ragged_tensor.RaggedTensorSpec([None, None], dtypes.int32)\n\n    x = ragged_factory_ops.constant([[1, 2, 3, 4], [1]])\n    # pylint: disable=g-long-lambda\n    y = map_fn_lib.map_fn(\n        lambda r: map_fn_lib.map_fn(\n            lambda y: r, r, fn_output_signature=ragged1d),\n        x,\n        fn_output_signature=ragged2d)\n    expected = [[[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]], [[1]]]\n    self.assertAllEqual(y, expected)\n\n\nif __name__ == '__main__':\n  googletest.main()\n"], "filenames": ["tensorflow/core/kernels/ragged_tensor_from_variant_op.cc", "tensorflow/python/ops/ragged/ragged_map_fn_op_test.py"], "buggy_code_start_loc": [176, 23], "buggy_code_end_loc": [177, 311], "fixing_code_start_loc": [177, 24], "fixing_code_end_loc": [194, 335], "type": "CWE-681", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions it is possible to nest a `tf.map_fn` within another `tf.map_fn` call. However, if the input tensor is a `RaggedTensor` and there is no function signature provided, code assumes the output is a fully specified tensor and fills output buffer with uninitialized contents from the heap. The `t` and `z` outputs should be identical, however this is not the case. The last row of `t` contains data from the heap which can be used to leak other memory information. The bug lies in the conversion from a `Variant` tensor to a `RaggedTensor`. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/ragged_tensor_from_variant_op.cc#L177-L190) does not check that all inner shapes match and this results in the additional dimensions. The same implementation can result in data loss, if input tensor is tweaked. We have patched the issue in GitHub commit 4e2565483d0ffcadc719bd44893fb7f609bb5f12. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-37679", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T23:15:08.287", "lastModified": "2021-08-19T13:55:26.060", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions it is possible to nest a `tf.map_fn` within another `tf.map_fn` call. However, if the input tensor is a `RaggedTensor` and there is no function signature provided, code assumes the output is a fully specified tensor and fills output buffer with uninitialized contents from the heap. The `t` and `z` outputs should be identical, however this is not the case. The last row of `t` contains data from the heap which can be used to leak other memory information. The bug lies in the conversion from a `Variant` tensor to a `RaggedTensor`. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/ragged_tensor_from_variant_op.cc#L177-L190) does not check that all inner shapes match and this results in the additional dimensions. The same implementation can result in data loss, if input tensor is tweaked. We have patched the issue in GitHub commit 4e2565483d0ffcadc719bd44893fb7f609bb5f12. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;En las versiones afectadas, es posible anidar un \"tf.map_fn\" dentro de otra llamada \"tf.map_fn\".&#xa0;Sin embargo, si el tensor de entrada es un \"RaggedTensor\" y no se proporciona una firma de funci\u00f3n, el c\u00f3digo asume que la salida es un tensor completamente especificado y llena el b\u00fafer de salida con contenido no inicializado de la pila.&#xa0;Las salidas \"t\" y\" z\" deben ser id\u00e9nticas, sin embargo, este no es el caso.&#xa0;La \u00faltima fila de \"t\" contiene datos de la pila que se pueden usar para filtrar otra informaci\u00f3n de la memoria.&#xa0;El error radica en la conversi\u00f3n de un tensor \"Variant\" a un\" RaggedTensor\".&#xa0;La [implementaci\u00f3n] (https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/ragged_tensor_from_variant_op.&#xa0;cc # L177-L190) no comprueba que todas las formas internas coincidan y esto da como resultado las dimensiones adicionales.&#xa0;La misma implementaci\u00f3n puede resultar en la p\u00e9rdida de datos, si se modifica el tensor de entrada.&#xa0;Hemos solucionado el problema en GitHub commit 4e2565483d0ffcadc719bd44893fb7f609bb5f12.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.6.0.&#xa0;Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.5.1, TensorFlow versi\u00f3n 2.4.3 y TensorFlow versi\u00f3n 2.3.4, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan se encuentran en el rango admitido."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-681"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.4", "matchCriteriaId": "0F83C081-51CC-415F-A8C0-0A44C75E2CD6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.3", "matchCriteriaId": "BD3F2BF8-EBA9-42BF-8F9B-D918B880B15A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D03E99A7-4E3D-427D-A156-C0713E9FB02A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "70FA6E48-6C57-40CA-809F-4E3D07CBF348"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "42187561-E491-434D-828C-F36701446634"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C66B61C8-450A-4C5E-9174-F970D6DEE778"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/4e2565483d0ffcadc719bd44893fb7f609bb5f12", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-g8wg-cjwc-xhhp", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/4e2565483d0ffcadc719bd44893fb7f609bb5f12"}}